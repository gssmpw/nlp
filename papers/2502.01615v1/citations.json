[
  {
    "index": 0,
    "papers": [
      {
        "key": "Crocker2010-cp",
        "author": "Crocker, Matthew W",
        "title": "Computational Psycholinguistics"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "marr1982",
        "author": "Marr, David",
        "title": "Vision: A Computational Investigation into the Human Representation and Processing of Visual Information"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hale-2001-probabilistic",
        "author": "Hale, John",
        "title": "A Probabilistic {Earley} Parser as a Psycholinguistic Model"
      },
      {
        "key": "Levy2008Expectation-basedComprehension",
        "author": "Levy, Roger",
        "title": "{Expectation-based syntactic comprehension}"
      },
      {
        "key": "Smith2013-ap",
        "author": "Smith, Nathaniel J and Levy, Roger",
        "title": "The effect of word predictability on reading time is logarithmic"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Smith2013-ap",
        "author": "Smith, Nathaniel J and Levy, Roger",
        "title": "The effect of word predictability on reading time is logarithmic"
      },
      {
        "key": "FRANK20151",
        "author": "Stefan L. Frank and Leun J. Otten and Giulia Galli and Gabriella Vigliocco",
        "title": "The ERP response to the amount of information conveyed by words in sentences"
      },
      {
        "key": "Shain2022-qv",
        "author": "Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger",
        "title": "Large-scale evidence for logarithmic effects of word predictability on reading time"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Van_Schijndel2021-sm",
        "author": "van Schijndel, Marten and Linzen, Tal",
        "title": "{Single-Stage} Prediction Models Do Not Explain the Magnitude of\nSyntactic Disambiguation Difficulty"
      },
      {
        "key": "Huang2024-qe",
        "author": "Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and\nMuxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen,\nTal",
        "title": "Large-scale benchmark yields no evidence that language model\nsurprisal explains syntactic disambiguation difficulty"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "Levy2008Expectation-basedComprehension",
        "author": "Levy, Roger",
        "title": "{Expectation-based syntactic comprehension}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Smith2013-ap",
        "author": "Smith, Nathaniel J and Levy, Roger",
        "title": "The effect of word predictability on reading time is logarithmic"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "frank2011insensitivity",
        "author": "Frank, Stefan L and Bod, Rens",
        "title": "Insensitivity of the Human Sentence-Processing System to Hierarchical Structure"
      },
      {
        "key": "Aurnhammer2019-fu",
        "author": "Aurnhammer, C and Frank, S L",
        "title": "{Comparing gated and simple recurrent neural network\narchitectures as models of human sentence processing}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Merkx2020ComparingData",
        "author": "Merkx, Danny  and\nFrank, Stefan L.",
        "title": "Human Sentence Processing: Recurrence or Attention?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Hale2018FindingSearch",
        "author": "Hale, John and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan R.",
        "title": "{Finding Syntax in Human Encephalography with Beam Search}"
      },
      {
        "key": "Yoshida2021-rc",
        "author": "Yoshida, Ryo  and\nNoji, Hiroshi  and\nOseki, Yohei",
        "title": "Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars"
      },
      {
        "key": "Oh2021-ln",
        "author": "Oh, Byung-Doh and Clark, Christian and Schuler, William",
        "title": "Surprisal Estimators for Human Reading Times Need Character\nModels"
      },
      {
        "key": "Oh2022-ss",
        "author": "Oh, Byung-Doh and Clark, Christian and Schuler, William",
        "title": "Comparison of structural parsers and neural language models as\nsurprisal estimators"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "frank2011insensitivity",
        "author": "Frank, Stefan L and Bod, Rens",
        "title": "Insensitivity of the Human Sentence-Processing System to Hierarchical Structure"
      },
      {
        "key": "Goodkind2018PredictiveQuality",
        "author": "Goodkind, Adam and Bicknell, Klinton",
        "title": "{Predictive power of word surprisal for reading times is a linear function of language model quality}"
      },
      {
        "key": "Wilcox2020OnBehavior",
        "author": "Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger",
        "title": "{On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior}"
      },
      {
        "key": "Wilcox2023-bb",
        "author": "Wilcox, Ethan and Meister, Clara and Cotterell, Ryan and\nPimentel, Tiago",
        "title": "Language Model Quality Correlates with Psychometric Predictive\nPower in Multiple Languages"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "kuribayashi-etal-2021-lower",
        "author": "Kuribayashi, Tatsuki  and\nOseki, Yohei  and\nIto, Takumi  and\nYoshida, Ryo  and\nAsahara, Masayuki  and\nInui, Kentaro",
        "title": "Lower Perplexity is Not Always Human-Like"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kuribayashi-etal-2022-context",
        "author": "Kuribayashi, Tatsuki  and\nOseki, Yohei  and\nBrassard, Ana  and\nInui, Kentaro",
        "title": "Context Limitations Make Neural Language Models More Human-Like"
      },
      {
        "key": "Shain2022-qv",
        "author": "Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger",
        "title": "Large-scale evidence for logarithmic effects of word predictability on reading time"
      },
      {
        "key": "Oh2023-zw",
        "author": "Oh, Byung-Doh and Schuler, William",
        "title": "Why does surprisal from larger Transformer-based language models\nprovide a poorer fit to human reading times?"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "Wilcox2024-qx",
        "author": "Wilcox, Ethan Gotlieb and Hu, Michael and Mueller, Aaron and\nLinzen, Tal and Warstadt, Alex and Choshen, Leshem and Zhuang,\nChengxu and Cotterell, Ryan and Williams, Adina",
        "title": "Bigger is not always better: The importance of human-scale\nlanguage modeling for psycholinguistics"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "kuribayashi-etal-2022-context",
        "author": "Kuribayashi, Tatsuki  and\nOseki, Yohei  and\nBrassard, Ana  and\nInui, Kentaro",
        "title": "Context Limitations Make Neural Language Models More Human-Like"
      },
      {
        "key": "Oh2023-hj",
        "author": "Oh, Byung-Doh and Schuler, William",
        "title": "Transformer-based language model surprisal predicts human reading\ntimes best with about two billion training tokens"
      },
      {
        "key": "Oh2023-zw",
        "author": "Oh, Byung-Doh and Schuler, William",
        "title": "Why does surprisal from larger Transformer-based language models\nprovide a poorer fit to human reading times?"
      },
      {
        "key": "Oh2024-cc",
        "author": "Oh, Byung-Doh and Yue, Shisen and Schuler, William",
        "title": "Frequency Explains the Inverse Correlation of Large Language\nModels{'} Size, Training Data Amount, and Surprisal{'}s Fit to\nReading Times"
      },
      {
        "key": "nair2023words",
        "author": "Nair, Sathvik and Resnik, Philip",
        "title": "Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?"
      },
      {
        "key": "kuribayashi-etal-2024-psychometric",
        "author": "Kuribayashi, Tatsuki  and\nOseki, Yohei  and\nBaldwin, Timothy",
        "title": "Psychometric Predictive Power of Large Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "michaelov2024revenge",
        "author": "James Michaelov and Catherine Arnett and Ben Bergen",
        "title": "Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics"
      },
      {
        "key": "aw2024instructiontuning",
        "author": "Khai Loong Aw and Syrielle Montariol and Badr AlKhamissi and Martin Schrimpf and Antoine Bosselut",
        "title": "Instruction-tuning Aligns {LLM}s to the Human Brain"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "Oh2023-zw",
        "author": "Oh, Byung-Doh and Schuler, William",
        "title": "Why does surprisal from larger Transformer-based language models\nprovide a poorer fit to human reading times?"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "Schrimpf2020-qa",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "michaelov2024revenge",
        "author": "James Michaelov and Catherine Arnett and Ben Bergen",
        "title": "Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics"
      },
      {
        "key": "Hosseini2024-jf",
        "author": "Hosseini, Eghbal A and Schrimpf, Martin and Zhang, Yian and\nBowman, Samuel and Zaslavsky, Noga and Fedorenko, Evelina",
        "title": "Artificial neural network language models predict human brain\nresponses to language even after a developmentally realistic\namount of training"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "aw2024instructiontuning",
        "author": "Khai Loong Aw and Syrielle Montariol and Badr AlKhamissi and Martin Schrimpf and Antoine Bosselut",
        "title": "Instruction-tuning Aligns {LLM}s to the Human Brain"
      },
      {
        "key": "kuribayashi-etal-2024-psychometric",
        "author": "Kuribayashi, Tatsuki  and\nOseki, Yohei  and\nBaldwin, Timothy",
        "title": "Psychometric Predictive Power of Large Language Models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "Witzel2012-nr",
        "author": "Witzel, Naoko and Witzel, Jeffrey and Forster, Kenneth",
        "title": "Comparisons of online reading paradigms: eye tracking,\nmoving-window, and maze"
      },
      {
        "key": "Lewis2005-hp",
        "author": "Lewis, Richard L and Vasishth, Shravan",
        "title": "An activation-based model of sentence processing as skilled\nmemory retrieval"
      },
      {
        "key": "Vani2021-aj",
        "author": "Vani, Pranali and Wilcox, Ethan Gotlieb and Levy, Roger",
        "title": "Using the Interpolated Maze Task to Assess Incremental Processing\nin English Relative Clauses"
      },
      {
        "key": "Caucheteux2023-tx",
        "author": "Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\\'e}mi",
        "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech"
      },
      {
        "key": "McCurdy2024-ix",
        "author": "McCurdy, Kate and Hahn, Michael",
        "title": "Lossy Context Surprisal Predicts Task-Dependent Patterns in\nRelative Clause Processing"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "Tenney2019-bb",
        "author": "Tenney, Ian and Das, Dipanjan and Pavlick, Ellie",
        "title": "{BERT} rediscovers the classical {NLP} pipeline"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "Dimigen2011-gt",
        "author": "Dimigen, Olaf and Sommer, Werner and Hohlfeld, Annette and Jacobs, Arthur M and Kliegl, Reinhold",
        "title": "Coregistration of eye movements and EEG in natural reading: analyses and review."
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "Rayner2009-aw",
        "author": "Rayner, Keith and Clifton, Jr, Charles",
        "title": "Language processing in reading and speech perception is fast and\nincremental: implications for event-related potential research"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "Rayner1998-rw",
        "author": "Rayner, Keith",
        "title": "Eye movements in reading and information processing: 20 years of\nresearch"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "Forster2009-dp",
        "author": "Forster, Kenneth I and Guerrera, Christine and Elliot, Lisa",
        "title": "The maze task: measuring forced incremental sentence processing\ntime"
      },
      {
        "key": "Boyce2023AmazeON",
        "author": "Veronica Boyce and Roger Philip Levy",
        "title": "A-maze of Natural Stories: Comprehension and surprisal in the Maze task"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "Toneva2019-ul",
        "author": "Toneva, Mariya and Wehbe, Leila",
        "title": "Interpreting and improving natural-language processing (in\nmachines) with natural language-processing (in the brain)"
      },
      {
        "key": "Schrimpf2020-qa",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "Caucheteux2023-tx",
        "author": "Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\\'e}mi",
        "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "Graves2016-dp",
        "author": "Graves, Alex",
        "title": "Adaptive Computation Time for Recurrent Neural Networks"
      },
      {
        "key": "Banino2021-oq",
        "author": "Banino, Andrea and Balaguer, Jan and Blundell, Charles",
        "title": "{PonderNet}: Learning to Ponder"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "icml19shallowdeepnetworks",
        "author": "Yi\\u{g}itcan Kaya and Sanghyun Hong and Tudor Dumitras",
        "title": "{Shallow-Deep Networks}: Understanding and Mitigating Network Overthinking"
      },
      {
        "key": "Zhou2020-vz",
        "author": "Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian\nand Xu, Ke and Wei, Furu",
        "title": "{BERT} loses Patience: Fast and robust inference with Early Exit"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "logitlens",
        "author": "nostalgebraist",
        "title": "interpreting gpt: the logit lens."
      },
      {
        "key": "dar-etal-2023-analyzing",
        "author": "Dar, Guy  and\nGeva, Mor  and\nGupta, Ankit  and\nBerant, Jonathan",
        "title": "Analyzing Transformers in Embedding Space"
      },
      {
        "key": "belrose2023eliciting",
        "author": "Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and McKinney, Lev and Ostrovsky, Igor and Biderman, Stella and Steinhardt, Jacob",
        "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "logitlens",
        "author": "nostalgebraist",
        "title": "interpreting gpt: the logit lens."
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "belrose2023eliciting",
        "author": "Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and McKinney, Lev and Ostrovsky, Igor and Biderman, Stella and Steinhardt, Jacob",
        "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"
      }
    ]
  }
]