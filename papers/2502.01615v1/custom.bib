@book{hawkins1994performance,
    title = {{A performance theory of order and constituency}},
    year = {1994},
    author = {Hawkins, John A},
    publisher = {Cambridge University Press},
    url = {https://psycnet.apa.org/record/1995-97959-000}
}

@inproceedings{hale-2001-probabilistic,
    title = {A Probabilistic {Earley} Parser as a Psycholinguistic Model},
    year = {2001},
    booktitle = {Proceedings of NAACL 2001},
    author = {Hale, John},
    pages = {159--166},
    url = {https://www.aclweb.org/anthology/N01-1021}
}

@inproceedings{hu2020systematic,
    title = {{A Systematic Assessment of Syntactic Generalization in Neural Language Models}},
    year = {2020},
    booktitle = {Proceedings of ACL},
    author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger},
    pages = {1725--1744},
    url = {https://www.aclweb.org/anthology/2020.acl-main.158/}
}

@article{brennan2016abstract,
    title = {{Abstract linguistic structure correlates with temporal activity during naturalistic comprehension}},
    year = {2016},
    journal = {Brain and language},
    author = {Brennan, Jonathan R and Stabler, Edward P and Van Wagenen, Sarah E and Luh, Wen-Ming and Hale, John T},
    pages = {81--94},
    volume = {157},
    publisher = {Elsevier},
    url = {https://www.sciencedirect.com/science/article/pii/S0093934X15300687?casa_token=2odXZdFCxGQAAAAA:dhneDpdfRQpK-TDzHKvKDTjIlSsArd3JDGNdoIKjM0L1sOsYbLjU5Oa2IYIT8D2Pa3rsphLuEQ}
}

@article{Uchida2014AnES,
    title = {{An ERP study of parsing and memory load in Japanese sentence processing -- A comparison between left-corner parsing and the Dependency Locality Theory}},
    year = {2014},
    journal = {Technical report of IEICE. Thought and language},
    author = {Uchida, Shodai and Miyamoto, E and Hirose, Yuki and Kobayashi, Yuki and Ito, Takane},
    pages = {101--106},
    volume = {114},
    url = {https://www.semanticscholar.org/paper/An-ERP-study-of-parsing-and-memory-load-in-Japanese-Uchida-Miyamoto/de8e7260ab5f191926bb7aebf8c72dce01302dde}
}

@inproceedings{van-schijndel-linzen-2019-entropy,
    title = {Can Entropy Explain Successor Surprisal Effects in Reading?},
    year = {2019},
    booktitle = {Proceedings of SCiL 2019},
    author = {van Schijndel, Marten and Linzen, Tal},
    pages = {1--7},
    url = {https://www.aclweb.org/anthology/W19-0101},
    doi = {10.7275/qtbb-9d05}
}

@inproceedings{Aurnhammer2019-ib,
    title = {{Comparing Gated and Simple Recurrent Neural Network Architectures as Models of Human Sentence Processing}},
    year = {2019},
    booktitle = {Proceedings of CogSci},
    author = {Aurnhammer, C and Frank, S L},
    pages = {112--118},
    url = {https://www.narcis.nl/publication/RecordID/oai:repository.ubn.ru.nl:2066%2F213724}
}

@inproceedings{roark-etal-2009-deriving,
    title = {{Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing}},
    year = {2009},
    booktitle = {Proceedings of EMNLP},
    author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
    month = {8},
    pages = {324--333},
    url = {https://www.aclweb.org/anthology/D09-1034},
    address = {Singapore}
}

@inproceedings{oseki2020design,
    title = {{Design of BCCWJ-EEG: Balanced Corpus with Human Electroencephalography}},
    year = {2020},
    booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference},
    author = {Oseki, Yohei and Asahara, Masayuki},
    pages = {189--194},
    url = {https://www.aclweb.org/anthology/2020.lrec-1.24}
}

@article{rayner1996effects,
    title = {{Effects of contextual constraint on eye movements in reading: A further examination}},
    year = {1996},
    journal = {Psychonomic Bulletin {\&} Review},
    author = {Rayner, Keith and Well, Arnold D},
    number = {4},
    pages = {504--509},
    volume = {3},
    publisher = {Springer},
    url = {https://link.springer.com/article/10.3758/BF03214555}
}

@inproceedings{genzel-charniak-2002-entropy,
    title = "Entropy Rate Constancy in Text",
    author = "Genzel, Dmitriy  and
      Charniak, Eugene",
    booktitle = "Proceedings of ACL",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1026",
    doi = "10.3115/1073083.1073117",
    pages = "199--206",
}

@inproceedings{linzen2020can,
    title = {{How Can We Accelerate Progress Towards Human-like Linguistic Generalization?}},
    year = {2020},
    booktitle = {Proceedings of ACL},
    author = {Linzen, Tal},
    pages = {5210--5217},
    url = {https://www.aclweb.org/anthology/2020.acl-main.465/}
}

@inproceedings{meister2020if,
    title = {{If Beam Search Is the Answer, What Was the Question?}},
    year = {2020},
    booktitle = {Proceedings of EMNLP},
    author = {Meister, Clara and Cotterell, Ryan and Vieira, Tim},
    pages = {2173--2185},
    url = {https://www.aclweb.org/anthology/2020.emnlp-main.170/}
}

@article{gotze2007information,
    title = {{Information structure}},
    year = {2007},
    journal = {Interdisciplinary studies on information structure},
    author = {G{\"{o}}tze, Michael and Weskott, Thomas and Endriss, Cornelia and Fiedler, Ines and Hinterwimmer, Stefan and Petrova, Svetlana and Schwarz, Anne and Skopeteas, Stavros and Stoel, Ruben},
    pages = {147--187},
    volume = {7},
    publisher = {Universit{\"{a}}tsverlag Potsdam}
}

@article{frank2011insensitivity,
    title = {Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
    year = {2011},
    journal = {Psychological Science},
    author = {Frank, Stefan L and Bod, Rens},
    number = {6},
    pages = {829--834},
    volume = {22},
    publisher = {Sage Publications Sage CA: Los Angeles, CA},
    url = {https://www.researchgate.net/publication/51140976_Insensitivity_of_the_Human_Sentence-Processing_System_to_Hierarchical_Structure}
}

@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
    title = {Language Models are Few-Shot Learners},
    year = {2020},
    booktitle = {Proceedings of NeurIPS},
    author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
    url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@article{tamkin2020language,
    title = {{Language Through a Prism: A Spectral Approach for Multiscale Language Representations}},
    year = {2020},
    journal = {Proceedings of NeurIPS},
    author = {Tamkin, Alex and Jurafsky, Dan and Goodman, Noah},
    volume = {33}
}

@article{kudo2006mecab,
    title = {{MeCab: Yet Another Part-of-speech and Morphological Analyzer}},
    year = {2006},
    journal = {http://mecab. sourceforge. jp},
    author = {Kudo, Taku},
    url = {https://ci.nii.ac.jp/naid/10027284215/}
}

@article{bender2011achieving,
    title = {{On Achieving and Evaluating Language-Independence in NLP}},
    year = {2011},
    journal = {Linguistic Issues in Language Technology},
    author = {Bender, Emily M},
    number = {3},
    pages = {1--26},
    volume = {6},
    url = {https://journals.linguisticsociety.org/elanguage/lilt/article/view/2624.html}
}

@inproceedings{brunner2019identifiability,
    title = {{On identifiability in transformers}},
    year = {2019},
    booktitle = {Proceedings of ICLR},
    author = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger}
}

@inproceedings{upadhye-etal-2020-predicting,
    title = {{Predicting Reference: What do Language Models Learn about Discourse Models?}},
    year = {2020},
    booktitle = {Proceedings of EMNLP},
    author = {Upadhye, Shiva and Bergen, Leon and Kehler, Andrew},
    month = {11},
    pages = {977--982},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.emnlp-main.70},
    address = {Online},
    doi = {10.18653/v1/2020.emnlp-main.70}
}

@book{levy2005probabilistic,
    title = {{Probabilistic models of word order and syntactic discontinuity}},
    year = {2005},
    author = {Levy, Roger},
    publisher = {stanford university},
    url = {https://www.semanticscholar.org/paper/Probabilistic-models-of-word-order-and-syntactic-Manning-Levy/fe5b91975137121ca28115031a52039ccc383c40}
}

@article{kaplan2020scaling,
    title = {{Scaling Laws for Neural Language Models}},
    year = {2020},
    journal = {arXiv preprint arXiv:2001.08361},
    author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
    url = {https://arxiv.org/abs/2001.08361}
}

@inproceedings{fossum-levy-2012-sequential,
    title = {{Sequential vs. Hierarchical Syntactic Models of Human Incremental Sentence Processing}},
    year = {2012},
    booktitle = {Proceedings of CMCL},
    author = {Fossum, Victoria and Levy, Roger},
    month = {6},
    pages = {61--69},
    url = {https://www.aclweb.org/anthology/W12-1706},
}

@inproceedings{NIPS2006_c6a01432,
    title = {{Speakers optimize information density through syntactic reduction}},
    year = {2007},
    booktitle = {NIPS},
    author = {Jaeger, T and Levy, Roger},
    editor = {Sch{\"{o}}lkopf, B and Platt, J and Hoffman, T},
    pages = {849--856},
    volume = {19},
    publisher = {MIT Press},
    url = {https://proceedings.neurips.cc/paper/2006/file/c6a01432c8138d46ba39957a8250e027-Paper.pdf}
}

@inproceedings{marvin-linzen-2018-targeted,
    title = {{Targeted Syntactic Evaluation of Language Models}},
    year = {2018},
    booktitle = {Proceedings of EMNLP},
    author = {Marvin, Rebecca and Linzen, Tal},
    pages = {1192--1202},
    url = {https://www.aclweb.org/anthology/D18-1151},
    doi = {10.18653/v1/D18-1151}
}

@inproceedings{kennedy2003dundee,
    title = {{The dundee corpus}},
    year = {2003},
    booktitle = {Proceedings of the 12th European conference on eye movement},
    author = {Kennedy, Alan and Hill, Robin and Pynte, Joël}
}

@article{FRANK20151,
title = {The ERP response to the amount of information conveyed by words in sentences},
journal = {Brain and Language},
volume = {140},
pages = {1-11},
year = {2015},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X14001515},
author = {Stefan L. Frank and Leun J. Otten and Giulia Galli and Gabriella Vigliocco},
keywords = {Information theory, Surprisal, Entropy, Event-related potentials, Sentence comprehension, Reading},
abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence’s hierarchical structure for generating expectations about the upcoming word.}
}
@inproceedings{joshi2020state,
    title = {{The State and Fate of Linguistic Diversity and Inclusion in the NLP World}},
    year = {2020},
    booktitle = {Proceedings of ACL},
    author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
    pages = {6282--6293},
    url = {https://www.aclweb.org/anthology/2020.acl-main.560/}
}

@article{prince1981towards,
    title = {{Towards a taxonomy of given-new information}},
    year = {1981},
    journal = {Journal of Radical pragmatics},
    author = {Prince, Ellen F},
    publisher = {Academic Press}
}

@article{DBLP:journals/corr/abs-1809-04179,
    title = {{What can linguistics and deep learning contribute to each other?}},
    year = {2019},
    journal = {Journal of Language},
    author = {Linzen, Tal},
    number = {1},
    pages = {99--108},
    volume = {95},
    url = {http://arxiv.org/abs/1809.04179},
    arxivId = {1809.04179}
}

@inproceedings{NIPS2010_0c74b7f7,
 author = {Maurits, Luke and Navarro, Dan and Perfors, Amy},
 booktitle = {NIPS},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Why are some word orders more common than others? A uniform information density account},
 url = {https://proceedings.neurips.cc/paper/2010/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},
 volume = {23},
 year = {2010}
}


@book{bunrui1964,
    title = {{分類語彙表}},
    year = {1964},
    author = {{国立国語研究所}},
    publisher = {秀英出版}
}

@book{bunrui2004,
    title = {{分類語彙表-増補改訂版}},
    year = {2004},
    author = {{国立国語研究所}},
    publisher = {大日本図書},
    url = {https://www.dainippon-tosho.co.jp/books/product/01661/},
    isbn = {978-4477016610}
}

@inproceedings{Shain2019AReading,
    title = {{A large-scale study of the effects of word frequency and predictability in naturalistic reading}},
    year = {2019},
    booktitle = {Proceedings of NAACL HLT},
    author = {Shain, Cory},
    isbn = {9781950737130},
    doi = {10.18653/v1/n19-1413}
}

@article{Shannon1948ACommunication,
    title = {{A Mathematical Theory of Communication}},
    year = {1948},
    journal = {Bell System Technical Journal},
    author = {Shannon, C. E.},
    number = {3},
    pages = {379--423},
    volume = {27},
    doi = {10.1002/j.1538-7305.1948.tb01338.x},
    issn = {15387305}
}

@inproceedings{Hewitt2019ARepresentations,
    title = {{A structural probe for finding syntax in word representations}},
    year = {2019},
    booktitle = {Proceedings of NAACL 2019},
    author = {Hewitt, John and Manning, Christopher D.},
    isbn = {9781950737130}
}

@article{Just1980AComprehension,
    title = {{A theory of reading: From eye fixations to comprehension}},
    year = {1980},
    journal = {Journal of Psychological Review},
    author = {Just, Marcel A. and Carpenter, Patricia A.},
    doi = {10.1037/0033-295X.87.4.329},
    issn = {0033295X},
    pmid = {7413885},
    keywords = {eye fixations {\&} cognitive processing, model of rea}
}

@article{Nakatani2010AnComplexity,
    title = {{An on-line study of Japanese nesting complexity}},
    year = {2010},
    journal = {Journal of Cognitive Science},
    author = {Nakatani, Kentaro and Gibson, Edward},
    doi = {10.1111/j.1551-6709.2009.01067.x},
    issn = {03640213},
    keywords = {Expectation-based parsing, Head-final language, Japanese, Nested structures, Retrieval, Sentence processing, Working memory}
}

@article{Goldberg2019AssessingAbilities,
    title = {{Assessing BERT's Syntactic Abilities}},
    year = {2019},
    author = {Goldberg, Yoav},
    pages = {2--5},
    url = {http://arxiv.org/abs/1901.05287},
    arxivId = {1901.05287}
}

@inproceedings{Vaswani2017AttentionNeed,
    title = {{Attention Is All You Need}},
    year = {2017},
    booktitle = {Proceedings of NIPS},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    pages = {5998--6008},
    url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
    issn = {10495258}
}

@inproceedings{Asahara2018BetweenLanguage,
    title = {{Between Reading Time and Clause Boundaries in Japanese - Wrap-up Effect in a Head-final Language}},
    year = {2018},
    booktitle = {Proceedings of PACLIC},
    author = {Asahara, Masayuki},
    pages = {19--27},
    url = {https://www.aclweb.org/anthology/Y18-1003}
}

@inproceedings{Asahara2017BetweenStructure,
    title = {{Between Reading Time and Information Structure}},
    year = {2017},
    booktitle = {Proceedings of PACLIC},
    author = {Asahara, Masayuki},
    pages = {15--24},
    url = {https://www.aclweb.org/anthology/Y17-1006}
}

@inproceedings{Asahara2017BetweenCategories,
    title = {{Between Reading Time and Syntactic / Semantic Categories}},
    year = {2017},
    booktitle = {Proceedings of IJCNLP},
    author = {Asahara, Masayuki and Kato, Sachi},
    pages = {404--412},
    url = {https://www.aclweb.org/anthology/I17-1041}
}

@article{Asahara2018BetweenPhrases,
    title = {{Between Reading Time and the Information Status of Noun Phrases}},
    year = {2018},
    journal = {Natural Language Processing},
    author = {Asahara, Masayuki},
    number = {5},
    pages = {527--554},
    volume = {25},
    publisher = {一般社団法人 言語処理学会},
    doi = {https://doi.org/10.5715/jnlp.25.527}
}

@inproceedings{Asahara2018BetweenJapanese,
    title = {{Between Reading Time and Zero Exophora in Japanese}},
    year = {2018},
    booktitle = {Proceedings of READ},
    author = {Asahara, Masayuki},
    pages = {34--36},
    url = {https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/C3-2.pdf}
}

@inproceedings{Ribeiro2020BeyondCheckList,
    title = {{Beyond Accuracy: Behavioral Testing of NLP Models with CheckList}},
    year = {2020},
    booktitle = {Proceedings of ACL},
    author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
    pages = {4902–4912},
    doi = {10.18653/v1/2020.acl-main.442},
    arxivId = {2005.04118}
}

@inproceedings{Merkx2020ComparingData,
    title = "Human Sentence Processing: Recurrence or Attention?",
    author = "Merkx, Danny  and
      Frank, Stefan L.",
    booktitle = "Proceedings of CMCL",
    month = jun,
    year = "2021",
    url = "https://aclanthology.org/2021.cmcl-1.2",
    doi = "10.18653/v1/2021.cmcl-1.2",
    pages = "12--22",
    abstract = "Recurrent neural networks (RNNs) have long been an architecture of interest for computational models of human sentence processing. The recently introduced Transformer architecture outperforms RNNs on many natural language processing tasks but little is known about its ability to model human language processing. We compare Transformer- and RNN-based language models{'} ability to account for measures of human reading effort. Our analysis shows Transformers to outperform RNNs in explaining self-paced reading times and neural activity during reading English sentences, challenging the widely held idea that human sentence processing involves recurrent and immediate processing and provides evidence for cue-based retrieval.",
}

@article{Futrell2020DependencyOrder,
    title = {{Dependency locality as an explanatory principle for word order}},
    year = {2020},
    journal = {Journal of Language},
    author = {Futrell, Richard and Levy, Roger P. and Gibson, Edward},
    doi = {10.1353/lan.2020.0024},
    issn = {15350665},
    keywords = {Corpus studies, Dependency-length minimization, Efficiency, Processing, Word-order universals}
}

@article{Nakatani2008DistinguishingJapanese,
    title = {{Distinguishing theories of syntactic expectation cost in sentence comprehension: Evidence from Japanese}},
    year = {2008},
    journal = {Journal of Linguistics},
    author = {Nakatani, Kentaro and Gibson, Edward},
    number = {1},
    pages = {63--87},
    volume = {46},
    doi = {10.1515/LING.2008.003},
    issn = {00243949}
}

@article{Nakatani2008DistinguishingJapaneseb,
    title = {{Distinguishing theories of syntactic expectation cost in sentence comprehension: Evidence from Japanese}},
    year = {2008},
    journal = {Journal of Linguistics},
    author = {Nakatani, Kentaro and Gibson, Edward},
    number = {1},
    pages = {63--87},
    volume = {46},
    doi = {10.1515/LING.2008.003},
    issn = {00243949}
}

@article{Levy2008Expectation-basedComprehension,
    title = {{Expectation-based syntactic comprehension}},
    year = {2008},
    journal = {Journal of Cognition},
    author = {Levy, Roger},
    number = {3},
    pages = {1126--1177},
    volume = {106},
    doi = {10.1016/j.cognition.2007.05.006},
    issn = {00100277},
    pmid = {17662975},
    keywords = {Frequency, Information theory, Parsing, Prediction, Sentence processing, Syntactic complexity, Syntax, Word order}
}


@article{DBLP:journals/corr/abs-2012-07805,
  author    = {Nicholas Carlini and
               Florian Tram{\`{e}}r and
               Eric Wallace and
               Matthew Jagielski and
               Ariel Herbert{-}Voss and
               Katherine Lee and
               Adam Roberts and
               Tom B. Brown and
               Dawn Song and
               {\'{U}}lfar Erlingsson and
               Alina Oprea and
               Colin Raffel},
  title     = {Extracting Training Data from Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2012.07805},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.07805},
  archivePrefix = {arXiv},
  eprint    = {2012.07805},
  timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-07805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hale2018FindingSearch,
    title = {{Finding Syntax in Human Encephalography with Beam Search}},
    year = {2018},
    booktitle = {Proceedings of ACL 2018},
    author = {Hale, John and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan R.},
    pages = {2727--2736},
    isbn = {9781948087322},
    doi = {10.18653/v1/p18-1254},
    arxivId = {1806.04127}
}

@article{Frank2011InsensitivityStructure,
    title = {{Insensitivity of the Human Sentence-Processing System to Hierarchical Structure}},
    year = {2011},
    journal = {Journal of Psychological Science},
    author = {Frank, Stefan L. and Bod, Rens},
    number = {6},
    pages = {829--834},
    volume = {22},
    url = {https://journals.sagepub.com/doi/abs/10.1177/0956797611409589},
    doi = {10.1177/0956797611409589},
    issn = {09567976},
    pmid = {21586764},
    keywords = {cognitive processes, computer simulation, language, neural networks, psycholinguistics}
}

@MISC{Radford_undated-nn,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  howpublished={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}



@inproceedings{Kuribayashi2020LanguageJapanese,
    title = {{Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese}},
    year = {2020},
    author = {Kuribayashi, Tatsuki and Ito, Takumi and Suzuki, Jun and Inui, Kentaro},
    doi = {10.18653/v1/2020.acl-main.47},
    arxivId = {2005.00842}
}

@article{Gibson1998LinguisticDependencies,
    title = {{Linguistic complexity: Locality of syntactic dependencies}},
    year = {1998},
    journal = {Journal of Cognition},
    author = {Gibson, Edward},
    number = {1},
    pages = {1--76},
    volume = {68},
    doi = {10.1016/S0010-0277(98)00034-1},
    issn = {00100277},
    pmid = {9775516},
    keywords = {Computational resources, Linguistic complexity, Sentence processing, Syntactic dependency}
}

@article{Konieczny2000LocalityComplexity,
    title = {{Locality and Parsing Complexity}},
    year = {2000},
    journal = {Journal of Psycholinguistic Research},
    author = {Konieczny, Lars},
    number = {6},
    pages = {627--645},
    volume = {29},
    doi = {10.1023/A:1026528912821},
    issn = {00906905},
    pmid = {11196066},
    keywords = {Anticipation, Extraposition, Locality, Parsiyol, Word-order}
}

@article{Lin2011LocalityComprehension,
    title = {{Locality versus Anti-locality Effects in Mandarin Sentence Comprehension}},
    year = {2011},
    journal = {Proceedings of the 23rd North American Conference on Chinese Linguistics},
    author = {Lin, Yowyu},
    number = {2000},
    pages = {200--214},
    volume = {1},
    url = {https://naccl.osu.edu/sites/naccl.osu.edu/files/NACCL-23_1_14.pdf}
}

@article{Hochreiter1997LongMemory,
    title = {{Long Short-Term Memory}},
    year = {1997},
    journal = {Journal of Neural Computation},
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    number = {8},
    pages = {1735--1780},
    volume = {9},
    doi = {10.1162/neco.1997.9.8.1735},
    issn = {08997667},
    pmid = {9377276}
}

@article{Futrell2020Lossy-ContextProcessing,
    title = {{Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing}},
    year = {2020},
    journal = {Journal of Cognitive Science},
    author = {Futrell, Richard and Gibson, Edward and Levy, Roger P.},
    doi = {10.1111/cogs.12814},
    issn = {15516709},
    keywords = {Information theory, Psycholinguistics, Sentence processing}
}

@inproceedings{Oseki2019ModelingProcessing,
    title = {{Modeling Hierarchical Syntactic Structures in Morphological Processing}},
    year = {2019},
    booktitle = {Proceedings of CMCL},
    author = {Oseki, Yohei and Yang, Charles and Marantz, Alec},
    pages = {43--52},
    doi = {10.18653/v1/w19-2905}
}

@inproceedings{Futrell2019NeuralState,
    title = {{Neural language models as psycholinguistic subjects: Representations of syntactic state}},
    year = {2019},
    booktitle = {Proceedings of NAACL HLT},
    author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Qian, Peng and Ballesteros, Miguel and Levy, Roger},
    isbn = {9781950737130},
    doi = {10.18653/v1/n19-1004},
    arxivId = {1903.03260}
}

@inproceedings{Sennrich2016NeuralUnits,
    title = {{Neural Machine Translation of Rare Words with Subword Units}},
    year = {2016},
    booktitle = {Proceedings of ACL},
    author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    pages = {1715--1725},
    isbn = {9781510827585},
    doi = {10.18653/v1/p16-1162}
}

@inproceedings{Wilcox2020OnBehavior,
    title = {{On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior}},
    year = {2020},
    booktitle = {Proceedings of CogSci},
    author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
    pages = {1707--1713},
    url = {http://arxiv.org/abs/2006.01912},
    arxivId = {2006.01912},
    keywords = {deep learning, eye-tracking, hension, language modeling, real-time language compre-, self-paced reading}
}

@inproceedings{Goodkind2018PredictiveQuality,
    title = {{Predictive power of word surprisal for reading times is a linear function of language model quality}},
    year = {2018},
    booktitle = {Proceedings of CMCL},
    author = {Goodkind, Adam and Bicknell, Klinton},
    pages = {10--18},
    doi = {10.18653/v1/w18-0102}
}

@inproceedings{Asahara2016Reading-TimeJapanese,
    title = {{Reading-Time Annotations for ``Balanced Corpus of Contemporary Written Japanese''}},
    year = {2016},
    booktitle = {Proceedings of COLING},
    author = {Asahara, Masayuki and Ono, Hajime and Miyamoto, Edson T},
    pages = {684--694},
    url = {https://www.aclweb.org/anthology/C16-1066}
}

@inproceedings{Dyer2016RecurrentGrammars,
    title = {{Recurrent neural network grammars}},
    year = {2016},
    publisher = "Association for Computational Linguistics",
    booktitle = {Proceedings of NAACL},
    author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
    isbn = {9781941643914},
    doi = {10.18653/v1/n16-1024},
    arxivId = {1602.07776}
}

@inproceedings{Davis2020RecurrentAttachment,
    title = {{Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment}},
    year = {2020},
    booktitle = {Proceedings of ACL},
    author = {Davis, Forrest and van Schijndel, Marten},
    pages = {1979--1990},
    doi = {10.18653/v1/2020.acl-main.179},
    arxivId = {2005.00165}
}

@inproceedings{Kudo2018SentencePiece:Processing,
    title = {{SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing}},
    year = {2018},
    booktitle = {Proceedings of EMNLP},
    author = {Kudo, Taku and Richardson, John},
    pages = {66--71},
    isbn = {9781948087858},
    doi = {10.18653/v1/d18-2012},
    arxivId = {1808.06226}
}

@inproceedings{Gauthier2020SyntaxGym:Models,
    title = {{SyntaxGym: An Online Platform for Targeted Evaluation of Language Models}},
    year = {2020},
    booktitle = {Proceedings of ACL (system demonstration)},
    author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
    pages = {70--76},
    doi = {10.18653/v1/2020.acl-demos.10}
}

@article{Gibson2000TheComplexity,
    title = {{The dependency locality theory: A distance-based theory of lingustic complexity}},
    year = {2000},
    journal = {Image, language, brain},
    author = {Gibson, Edward},
    pages = {95--126},
    volume = {2000},
    url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&cmd=Retrieve&dopt=AbstractPlus&list_uids=14407723415934217438related:3izWhByE8scJ},
    isbn = {0262133717},
    issn = {0010-0277},
    pmid = {9775516},
    keywords = {DLT, complexity, sentence processing}
}

@inproceedings{Barrett2015TheTreebank,
    title = {{The Dundee Treebank}},
    year = {2015},
    booktitle = {Fourteenth International Workshop on Treebanks and Linguistic Theories},
    author = {Barrett, Maria and Agi, Zeljko and S{\o}gaard, Anders},
    pages = {242--248},
    url={https://bib.irb.hr/datoteka/789398.barrett2015-dundee.pdf}
}

@article{Rayner2000TheReading,
    title = {{The Effect of Clause Wrap-Up on Eye Movements during Reading}},
    year = {2000},
    journal = {Quarterly Journal of Experimental Psychology Section A: Human Experimental Psychology},
    author = {Rayner, Keith and Kambe, Gretchen and Duffy, Susan A.},
    number = {4},
    pages = {1061--1080},
    volume = {53},
    doi = {10.1080/713755934},
    issn = {02724987},
    pmid = {11131813}
}

@article{Goldin-Meadow2008TheNonverbally,
    title = {{The natural order of events: How speakers of different languages represent events nonverbally}},
    year = {2008},
    journal = {Proceedings of the National Academy of Sciences of the United States of America},
    author = {Goldin-Meadow, Susan and So, Wing Chee and {\"{O}}zy{\"{u}}rek, Asli and Mylander, Carolyn},
    doi = {10.1073/pnas.0710060105},
    issn = {00278424},
    pmid = {18599445},
    keywords = {Gesture, Language genesis, Sign language, Word order}
}

@article{Ferrer-i-Cancho2017TheApproach,
    title = {{The placement of the head that maximizes predictability. An information theoretic approach}},
    year = {2017},
    journal = {Glottometrics},
    author = {Ferrer-i-Cancho, Ramon},
    issn = {16178351},
    arxivId = {1705.09932},
    keywords = {Compression, Gesture, Hilberg’s law, Information theory, Word order}
}

@inproceedings{Wilcox2019WhatDependencies,
    title = {{What do RNN Language Models Learn about Filler–Gap Dependencies?}},
    year = {2019},
    booktitle = {Proceedings of BlackboxNLP},
    author = {Wilcox, Ethan and Levy, Roger and Morita, Takashi and Futrell, Richard},
    pages = {211--221},
    doi = {10.18653/v1/w18-5423},
    arxivId = {1809.00042}
}

@inproceedings{ott-etal-2019-fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of NAACL (Demonstrations)",
    month = jun,
    year = "2019",
    url = "https://www.aclweb.org/anthology/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53",
    abstract = "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
}

@article{JSSv067i01,
   author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
   title = {Fitting Linear Mixed-Effects Models Using lme4},
   journal = {Journal of Statistical Software, Articles},
   volume = {67},
   number = {1},
   year = {2015},
   keywords = {sparse matrix methods; linear mixed models; penalized least squares; Cholesky decomposition},
   abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
   issn = {1548-7660},
   pages = {1--48},
   doi = {10.18637/jss.v067.i01},
   url = {https://www.jstatsoft.org/v067/i01}
}

@inproceedings{wei2021cognitive,
    title = "A Cognitive Regularizer for Language Modeling",
    author = "Wei, Jason  and
      Meister, Clara  and
      Cotterell, Ryan",
    booktitle = "Proceedings of ACL",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.404",
    doi = "10.18653/v1/2021.acl-long.404",
    pages = "5191--5202",
    abstract = "The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.",
}

%%%%%%%%%%%%%% ACL 2020 %%%%%%%%%%%%%%%%%%%%%%%

@book{nakagawa2016information,
  title={Information Structure in Spoken Japanese: Particles, word order, and intonation},
  author={Nakagawa, Natsuko},
  year={2016},
  publisher={Kyoto University}
}

@inproceedings{orita2017predicting,
  title={Predicting Japanese scrambling in the wild},
  author={Orita, Naho},
  booktitle={Proceedings of CMCL},
  pages={41--45},
  year={2017},
  url={https://www.aclweb.org/anthology/W17-0706/}
}

@InProceedings{armand-adaptive-softmax,
  title = 	 {{Efficient softmax approximation for GPUs}},
  author = 	 {{\'E}douard Grave and Armand Joulin and Moustapha Ciss{\'e} and David Grangier and Herv{\'e} J{\'e}gou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1302--1310},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/grave17a/grave17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/grave17a.html},
  abstract = 	 {We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.}
}

@article{saeki1960,
author="Saeki, Tetsuo",
title={{Gendaigo ni okeru gojun no keik{\=o} -- iwayuru hogo no baai [The trend of word order in modern writing-- in so-called complements]}},
journal="Gengo seikatsu [Language life]",
ISSN="04352955",
publisher="Chikuma Publishers",
year="1960",
month="dec",
volume="111",
pages="56-63",
URL="https://ci.nii.ac.jp/naid/40001065096/",
DOI="",
}

@article{vaswani2017,
  title = {{Attention is All you Need}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  journal = {NIPS},
  pages = {5998--6008},
  year = {2017},
  url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

%%%%%% 新しくciteするときはこの上に追加して
@article{ARNON201067,
title = {{More than words: Frequency effects for multi-word phrases}},
journal = {Journal of Memory and Language},
volume = {62},
number = {1},
pages = {67--82},
year = {2010},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2009.09.005},
url = {http://www.sciencedirect.com/science/article/pii/S0749596X09000965},
author = {Inbal Arnon and Neal Snider},
keywords = {Frequency, Usage-based models, Lexicon, Comprehension, Ngram},
abstract = {There is mounting evidence that language users are sensitive to distributional information at many grain-sizes. Much of this research has focused on the distributional properties of words, the units they consist of (morphemes, phonemes), and the syntactic structures they appear in (verb-categorization frames, syntactic constructions). In a series of studies we show that comprehenders are also sensitive to the frequencies of compositional four-word phrases (e.g. don’t have to worry): more frequent phrases are processed faster. The effect is not reducible to the frequency of the individual words or substrings and is observed across the entire frequency range (for low, mid and high frequency phrases). Comprehenders seem to learn and store frequency information about multi-word phrases. These findings call for processing models that can capture and predict phrase-frequency effects and support accounts where linguistic knowledge consists of patterns of varying sizes and levels of abstraction.}
}


@inproceedings{kann2018sentence,
  title={{Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!}},
  author={Kann, Katharina and Rothe, Sascha and Filippova, Katja},
  journal={arXiv preprint arXiv:1809.08731},
  year={2018},
  month = oct,
  booktitle={Proceedings of the 22nd Conference on Computational Natural Language Learning},
  url={https://www.aclweb.org/anthology/K18-1031},
  doi={10.18653/v1/K18-1031}, 
  address={Brussels, Belgium},
  publisher={Association for Computational Linguistics},
  pages={313--323},
}


@inproceedings{olteanu2006language,
  title={Language models and reranking for machine translation},
  author={Olteanu, Marian and Suriyentrakorn, Pasin and Moldovan, Dan},
  booktitle={Proceedings of the Workshop on Statistical Machine Translation},
  pages={150--153},
  year={2006},
  month=jun,
  address={New York City},
  publisher={Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/W06-3122},
}



@inproceedings{adaptive2018,
title={{Adaptive Input Representations for Neural Language Modeling}},
author={Alexei Baevski and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByxZX20qFQ},
}

@inproceedings{asahara-etal-2018-predicting,
  title={{Predicting Japanese Word Order in Double Object Constructions}},
  author={Asahara, Masayuki  and Nambu, Satoshi  and Sano, Shin-Ichiro},
  booktitle={Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing},
  month=jul,
  year={2018},
  address={Melbourne},
  publisher={Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/W18-2805},
  doi={10.18653/v1/W18-2805},
  pages={36--40},
}

@article{akhtar1999acquiring,
  title={{Acquiring basic word order: Evidence for data-driven learning of syntactic structure}},
  author={Akhtar, Nameera},
  journal={Journal of child language},
  volume={26},
  number={2},
  pages={339--356},
  year={1999},
  publisher={Cambridge University Press},
  doi={10.1017/S030500099900375X}
}

@article{bahlmann2007fmri,
  title={{An fMRI study of canonical and noncanonical word order in German}},
  author={Bahlmann, J{\"o}rg and Rodriguez-Fornells, Antoni and Rotte, Michael and M{\"u}nte, Thomas F},
  journal={Human brain mapping},
  volume={28},
  number={10},
  pages={940--949},
  year={2007},
  publisher={Wiley Online Library},
  doi = {10.1002/hbm.20318},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.20318},
}

@inproceedings{bloem2016testing,
  title={{Testing the Processing Hypothesis of word order variation using a probabilistic language model}},
  author={Bloem, Jelke},
  booktitle={Proceedings of CL4LC},
  pages={174--185},
  year={2016},
  month=dec,
  address={Osaka, Japan},
  url={https://www.aclweb.org/anthology/W16-4120},
}

@incollection{bresnan2007predicting,
  title={{Predicting the dative alternation}},
  author={Bresnan, Joan and Cueni, Anna and Nikitina, Tatiana and Baayen, R Harald},
  booktitle={Cognitive foundations of interpretation},
  pages={69--94},
  year={2007},
  publisher={KNAW},
  url={https://pure.mpg.de/pubman/item/item_58830_2}
}

@article{colleman2009verb,
  title={{Verb disposition in argument structure alternations: a corpus study of the dative alternation in Dutch}},
  author={Colleman, Timothy},
  journal={Language Sciences},
  volume={31},
  number={5},
  pages={593--611},
  year={2009},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/j.langsci.2008.01.001},
  url={http://www.sciencedirect.com/science/article/pii/S0388000108000028},
}



@book{comrie1989language,
  title={{Language universals and linguistic typology: Syntax and morphology}},
  author={Comrie, Bernard},
  year={1989},
  publisher={University of Chicago press},
  url={https://www.press.uchicago.edu/ucp/books/book/chicago/L/bo24426144.html},
  isbn={9780226114330},
  lccn={89040480},
}


@inproceedings{futrell2019rnns,
  title={{Do RNNs learn human-like abstract word order preferences?}},
  author={Futrell, Richard and Levy, Roger P},
  booktitle={Proceedings of SCiL},
  pages={50--59},
  year={2019},
  url={https://www.aclweb.org/anthology/W19-0106},
  doi={10.7275/jb34-9986},
}


@article{hoji1985logical,
  title={{Logical form constraints and configurational structures in Japanese.}},
  author={Hoji, Hajime},
  year={1985},
  journal={PHD Thesis. University of Washington},
  url={https://books.google.co.jp/books?id=HgTdnQEACAAJ},
}

@article{hovav2008english,
  title={{The English dative alternation: The case for verb sensitivity}},
  author={Hovav, Malka Rappaport and Levin, Beth},
  journal={Journal of linguistics},
  volume={44},
  number={1},
  pages={129--167},
  year={2008},
  publisher={Cambridge University Press},
  doi = {10.1017/S0022226707004975}
}

@inproceedings{imamura2014influence,
  title={{Influence of information structure on word order change and topic marker WA in Japanese}},
  author={Imamura, Satoshi and Sato, Yohei and Koizumi, Masatoshi},
  booktitle={Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing},
  pages={432--441},
  year={2014},
  month=dec,
  address={Phuket,Thailand},
  publisher={Department of Linguistics, Chulalongkorn University},
  url={https://www.aclweb.org/anthology/Y14-1050},
}

@article{nakamoto2006,
  title={{Preferred Word Orders Correlate with “Sentential” Meanings That Cannot Be Reduced to Verb Meanings:  A New Perspective on “Construction Effects” in Japanese}},
  author={Nakamoto, Keiko and Lee, Jae-ho and  Kuroda, Kow},
  journal={Cognitive Studies: Bulletin of the Japanese Cognitive Science Society},
  volume={13},
  number={3},
  pages={334--352},
  year={2006},
  doi={10.11225/jcss.13.334}
}

@article{kempen2004corpus,
  title={{A corpus study into word order variation in German subordinate clauses: Animacy affects}},
  author={Kempen, Gerard and Harbusch, Karin},
  journal={Multidisciplinary approaches to language production},
  pages={173-181},
  year={2004},
  publisher={Walter de Gruyter},
  doi={10.1515/9783110894028.173},
  url={https://www.degruyter.com/view/books/9783110894028/9783110894028.173/9783110894028.173.xml},
}


@article{koizumi2004cognitive,
  title={{Cognitive processing of Japanese sentences with ditransitive verbs}},
  author={Koizumi, Masatoshi and Tamaoka, Katsuo},
  journal={Gengo Kenkyu (Journal of the Linguistic Society of Japan)},
  volume={2004},
  number={125},
  pages={173--190},
  year={2004},
  publisher={The Linguistic Society of Japan},
  doi={10.11435/gengo1939.2004.125_173},
}

@article{koizumi2006,
  title={{The Canonical Positions of Adjuncts in the Processing of Japanese Sentence}},
  author={Masatoshi Koizumi and Katsuo Tamaoka},
  journal={Cognitive Studies: Bulletin of the Japanese Cognitive Science Society},
  volume={13},
  number={3},
  pages={392--403},
  year={2006},
  publisher={Japanese Cognitive Science Society},
  doi={10.11225/jcss.13.392},
}

@article{miyamoto2002sources,
  title={{Sources of difficulty in the processing of scrambling in Japanese}},
  author={Miyamoto, Edson T},
  journal={Sentence processing in East Asian languages},
  year={2002},
  publisher={CSLI},
  pages={167--188},
  url={https://ci.nii.ac.jp/naid/10015080761/en/},
}
% from https://www.aclweb.org/anthology/Y09-1043/
@book{noda1996, 
  title={{Wa to ga [Wa and ga]}},
  author={Noda, Hisashi},
  year={1996},
  publisher={Kurosio Publishers},
  url={http://www.9640.jp/book_view/?128}
}


@inproceedings{sasano2016corpus,
  title={{A Corpus-Based Analysis of Canonical Word Order of Japanese Double Object Constructions}},
  author={Sasano, Ryohei and Okumura, Manabu},
  booktitle={Proceedings ACL},
  pages={2236--2244},
  year={2016},
  month=aug,
  address={Berlin, Germany},
  publisher={Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/P16-1211},
  doi={10.18653/v1/P16-1211},
}

@article{shigenaga2014canonical,
  title={{Canonical Word Order of Japanese Ditransitive Sentences: A Preliminary Investigation through a Grammaticality Judgment Survey}},
  author={Shigenaga, Yasumasa},
  journal={Advances in Language and Literary Studies},
  volume={5},
  number={2},
  pages={35--45},
  year={2014},
  url={http://www.journals.aiac.org.au/index.php/alls/article/view/300},
  doi={10.7575/aiac.alls.v.5n.2p.35}
}

@article{slobin1982children,
  title={{Children use canonical sentence schemas: A crosslinguistic study of word order and inflections}},
  author={Slobin, Dan I and Bever, Thomas G},
  journal={Cognition},
  volume={12},
  number={3},
  pages={229--265},
  year={1982},
  publisher={Elsevier},
  doi={10.1016/0010-0277(82)90033-6},
  url={http://www.sciencedirect.com/science/article/pii/0010027782900336}
}


@article{alonso2000teaching,
  title={{Teaching English Word Order to ESL Spanish Students. A Functional Perspective}},
  author={Alonso Belmonte, Isabel and others},
  year={2000},
  journal={Encuentro. Revista de Investigaci{\'o}n e Innovaci{\'o}n en la clase de idiomas},
  url={http://hdl.handle.net/10017/952},
  volume={11},
  pages={1999--2000}
}

@article{matsuoka2003two,
  title={{Two Types of Ditransitive Consturctions in Japanese}},
  author={Matsuoka, Mikinari},
  journal={Journal of East Asian Linguistics},
  volume={12},
  number={2},
  pages={171--203},
  year={2003},
  publisher={Springer},
  url={https://doi.org/10.1023/A:1022472109327},
  doi={10.1023/A:1022472109327},
}

@inproceedings{cheng-etal-2014-chinese,
    title={{Chinese Word Ordering Errors Detection and Correction for Non-Native Chinese Language Learners}},
    author={Cheng, Shuk-Man and Yu, Chi-Hsin  and Chen, Hsin-Hsi},
    booktitle={Proceedings of COLING},
    month=aug,
    year={2014},
    address={Dublin, Ireland},
    publisher={Dublin City University and Association for Computational Linguistics},
    url={https://www.aclweb.org/anthology/C14-1028},
    pages={279--289},
}

@inproceedings{visweswariah-etal-2011-word,
    title={{A Word Reordering Model for Improved Machine Translation}},
    author={Visweswariah, Karthik  and
      Rajkumar, Rajakrishnan  and
      Gandhe, Ankur  and
      Ramanathan, Ananthakrishnan  and
      Navratil, Jiri},
    booktitle={Proceedings of EMNLP},
    month=jul,
    year={2011},
    address={Edinburgh, Scotland, UK.},
    publisher={Association for Computational Linguistics},
    url={https://www.aclweb.org/anthology/D11-1045},
    pages={486--496},
}



@book{ono1988nihongo,
  title={{Nihongo no bunpo{\=o} o kangaeru [Thinking about Japanese grammar]}},
  author={Ono, Susumu},
  year={1988},
  publisher={Iwanami Shoten},
  url={https://www.iwanami.co.jp/book/b267520.html}
}

@book{martin2004reference,
  title={{A reference grammar of Japanese}},
  author={Martin, Samuel Elmo},
  year={2004},
  publisher={University of Hawai'i Press},
  isbn={9780824828189},
  lccn={2003056702},
  url={https://books.google.co.jp/books?id=SszxbMtHbs8C},
}

@book{tsujimura2013introduction,
  title={{An introduction to Japanese linguistics}},
  author={Tsujimura, Natsuko},
  year={2013},
  publisher={John Wiley \& Sons},
  url={https://www.wiley.com/en-us/An+Introduction+to+Japanese+Linguistics%2C+3rd+Edition-p-9781444337730},
  isbn={9781118584309},
  lccn={2013017921},
}


@article{heycock1993syntactic,
  title={{Syntactic predication in Japanese}},
  author={Heycock, Caroline},
  journal={Journal of East Asian Linguistics},
  volume={2},
  number={2},
  pages={167--211},
  year={1993},
  month=jun,
  publisher={Springer},
  issn={1572-8560},
  doi={10.1007/BF01732503},
  url={https://doi.org/10.1007/BF01732503},
}

@book{fry2003ellipsis,
  title={{Ellipsis and wa-marking in Japanese conversation}},
  author={Fry, John},
  year={2003},
  publisher={Taylor \& Francis},
  url={https://doi.org/10.4324/9780203484036},
  doi={10.4324/9780203484036},
  isbn={9780203484036},
}

@book{saeki1998yosetsu,
  title={{Y{\=o}setsu Nihongo no Gojun [Essentials of Japanese word order]}},
  author={Saeki, Tetsuo},
  year={1998},
  publisher={Kurosio Publishers},
  isbn={9784874241653},
  url={http://www.9640.jp/book_view/?165},
}

% ACL2019 %
@inproceedings{kuribayashi-etal-2019-empirical,
    title = "An Empirical Study of Span Representations in Argumentation Structure Parsing",
    author = "Kuribayashi, Tatsuki  and
      Ouchi, Hiroki  and
      Inoue, Naoya  and
      Reisert, Paul  and
      Miyoshi, Toshinori  and
      Suzuki, Jun  and
      Inui, Kentaro",
    booktitle = "Proceedings of the ACL2019",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1464",
    doi = "10.18653/v1/P19-1464",
    pages = "4691--4698",
    abstract = "For several natural language processing (NLP) tasks, span representation design is attracting considerable attention as a promising new technique; a common basis for an effective design has been established. With such basis, exploring task-dependent extensions for argumentation structure parsing (ASP) becomes an interesting research direction. This study investigates (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. Our extensive experiments and analysis show that these representations yield high performance for ASP and provide some challenging types of instances to be parsed.",
}
@article{kuribayashi2017examining,
  title={Examining macro-level argumentative structure features for argumentative relation identification},
  author={Kuribayashi, Tatsuki and Reisert, Paul and Inoue, Naoya and Inui, Kentaro},
  year={2017},
  pages={1--6},
  volume={2017-NL-234},
  journal={IPSJ SIG Technical Report},
}
@article{afantenos2018comparing,
  title={Comparing decoding mechanisms for parsing argumentative structures},
  author={Afantenos, Stergos and Peldszus, Andreas and Stede, Manfred},
  journal={Argument \& Computation},
  volume={9},
  number={3},
  pages={177--192},
  year={2018},
  publisher={IOS Press}
}
@book{de1981introduction,
  title={Introduction to text linguistics},
  author={De Beaugrande, Robert and Dressler, Wolfgang U},
  year={1981},
  publisher={Routledge}
}
@book{halliday2014cohesion,
  title={Cohesion in english},
  author={Halliday, Michael Alexander Kirkwood and Hasan, Ruqaiya},
  year={2014},
  publisher={Routledge}
}
@inproceedings{stede2016parallel,
  title={Parallel Discourse Annotations on a Corpus of Short Texts},
  author={Stede, Manfred and Afantenos, Stergos and Peldszus, Andreas and Asher, Nicholas and Perret, J{\'e}r{\'e}my},
  booktitle={Proceedings of LREC},
  pages={1051--1058},
  year={2016}
}
@inproceedings{he-etal-2018-jointly,
    title = "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
    author = "He, Luheng  and
      Lee, Kenton  and
      Levy, Omer  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of ACL",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2058",
    doi = "10.18653/v1/P18-2058",
    pages = "364--369",
    abstract = "Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.",
}
@inproceedings{dror2018hitchhiker,
  title={The hitchhiker’s guide to testing statistical significance in natural language processing},
  author={Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
  booktitle={Proceedings of ACL},
  publisher = "Association for Computational Linguistics",
  pages={1383--1392},
  year={2018}
}
@article{wolf2019transformers,
  title={Transformers: State-of-the-art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}
@inproceedings{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Proceedings of NIPS},
  pages={5754--5764},
  year={2019}
}
@misc{liu2019roberta,
    title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of  NAACL},
  publisher = "Association for Computational Linguistics",
  pages={4171--4186},
  year={2019}
}
@InProceedings{Lin09,
  author = 	"Lin, Ziheng
		and Kan, Min-Yen
		and Ng, Hwee Tou",
  title = 	"Recognizing Implicit Discourse Relations in the Penn Discourse Treebank",
  booktitle = 	"Proceedings of EMNLP",
  year = 	"2009",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"343--351",
  location = 	"Singapore",
  url = 	"http://www.aclweb.org/anthology/D09-1036"
}
@inproceedings{Chen16,
author =        {Chen, Jifan and Zhang, Qi and Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
title =         {Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network},
booktitle =     {Proceedings of ACL},
year =  {2016},
pages =         {1726--1735}
}
@inproceedings{Li16,
  author = 	"Li, Qi
		and Li, Tianshi
		and Chang, Baobao",
  title = 	"Discourse Parsing with Attention-based Hierarchical Neural Networks",
  booktitle = 	"Proceedings of EMNLP",
  year = 	"2016",
  pages = 	"362--371",
  location = 	"Austin, Texas",
  doi = 	"10.18653/v1/D16-1035",
  url = 	"http://www.aclweb.org/anthology/D16-1035"
}
@article{Ji2015,
author = {Ji, Yangfeng and Eisenstein, Jacob},
journal = {TACL},
pages = {329--344},
title = {One Vector is Not Enough : Entity-Augmented Distributed Semantics for Discourse Relations},
year = {2015},
}
@INPROCEEDINGS{Li2017chinese,
author={M. {Li} and S. {Geng} and Y. {Gao} and S. {Peng} and H. {Liu} and H. {Wang}},
booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Crowdsourcing argumentation structures in Chinese hotel reviews},
year={2017},
volume={},
number={},
pages={87-92},
keywords={crowdsourcing;data mining;hotel industry;natural language processing;text analysis;customer reviews;argumentation annotation task;large-scale argumentation corpora;crowdsourcing technique;argumentation annotations;Chinese hotel reviews;Chinese argumentation dataset;crowdsourcing argumentation structures;argumentation mining;premises-claim discourse structures;natural language texts;Crowdsourcing;Guidelines;Reliability;Training;Labeling;Cameras},
doi={10.1109/SMC.2017.8122583},
ISSN={null},
month={Oct},}
@inproceedings{eger-etal-2018-cross,
    title = "Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!",
    author = "Eger, Steffen  and
      Daxenberger, Johannes  and
      Stab, Christian  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1071",
    pages = "831--844",
    abstract = "Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into German, French, Spanish, and Chinese. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/coling2018-xling{\_}argument{\_}mining.",
}
@book{freeman2011dialectics,
  title={Dialectics and the macrostructure of arguments: A theory of argument structure},
  author={Freeman, James B},
  volume={10},
  year={2011},
  publisher={Walter de Gruyter}
}
@book{toulmin1958uses,
  title={The uses of argument},
  author={Toulmin, Stephen Edelston},
  year={1958},
  publisher={Cambridge Univ. Press (Cambridge)}
}
@inproceedings{reed2006preliminary,
  title={Preliminary results from an argument corpus},
  author={Reed, Chris},
  booktitle={In Elo{\'\i}na Miyares Berm{\'u}dez \& Leonel Ruiz Miyares (Eds), Linguistics in the twenty-first century},
  year={2006},
  organization={Citeseer}
}
@article{Lawrence2019tutorial,
author = {Lawrence, John and Reed, Chris},
year = {2019},
month = {10},
pages = {765-818},
title = {Argument Mining: A Survey},
volume = {45},
journal = {Computational Linguistics},
doi = {10.1162/COLI_a_00364}
}
@article{mochales2011argumentation,
  title={Argumentation mining},
  author={Mochales, Raquel and Moens, Marie-Francine},
  journal={Artificial Intelligence and Law},
  volume={19},
  number={1},
  pages={1--22},
  year={2011},
  publisher={Springer}
}
@article{habernal2017argumentation,
  title={Argumentation mining in user-generated web discourse},
  author={Habernal, Ivan and Gurevych, Iryna},
  journal={Computational Linguistics},
  volume={43},
  number={1},
  pages={125--179},
  year={2017},
  publisher={MIT Press}
}
@inproceedings{reisert2019,
  title={An Annotation Protocol for Collecting User-Generated Counter-Arguments Using Crowdsourcing},
  author={Reisert, Paul and Vallejo, Gisela and Inoue, Naoya and Gurevych, Iryna and Inui, Kentaro},
  booktitle={Proceedings of the AIED 2019},
  pages={232--236},
  year={2019}
}
@inproceedings{nguyen2018,
  title={Argument mining for improving the automated scoring of persuasive essays},
  author={Nguyen, Huy V and Litman, Diane J},
  booktitle={Proceedings of AAAI},
  pages={5892--5899},
  year={2018}
}
@inproceedings{cabrio2018five,
  title={Five years of argument mining: a data-driven analysis},
  author={Cabrio, Elena and Villata, Serena},
  booktitle={Proceedings of IJCAI},
  pages={5427--5433},
  year={2018}
}
@inproceedings{ke2018learning,
  title={Learning to give feedback: modeling attributes affecting argument persuasiveness in student essays},
  author={Ke, Zixuan and Carlile, Winston and Gurrapadi, Nishant and Ng, Vincent},
  booktitle={Proceedings of IJCAI},
  pages={4130--4136},
  year={2018}
}
@inproceedings{reimers-etal-2019-classification,
    title = "Classification and Clustering of Arguments with Contextualized Word Embeddings",
    author = "Reimers, Nils  and
      Schiller, Benjamin  and
      Beck, Tilman  and
      Daxenberger, Johannes  and
      Stab, Christian  and
      Gurevych, Iryna",
    booktitle = "Proceedings of ACL",
    month = jul,
    year = "2019",
    url = "https://www.aclweb.org/anthology/P19-1054",
    doi = "10.18653/v1/P19-1054",
    pages = "567--578",
    abstract = "We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.",
}
@inproceedings{stab-etal-2018-cross,
    title = "Cross-topic Argument Mining from Heterogeneous Sources",
    author = "Stab, Christian  and
      Miller, Tristan  and
      Schiller, Benjamin  and
      Rai, Pranav  and
      Gurevych, Iryna",
    booktitle = "Proceedings of EMNLP",
    month = oct # "-" # nov,
    year = "2018",
    url = "https://www.aclweb.org/anthology/D18-1402",
    doi = "10.18653/v1/D18-1402",
    pages = "3664--3674",
}
@incollection{mann1987rhetorical,
 title={Rhetorical structure theory: Description and construction of text structures},
  author={Mann, William C and Thompson, Sandra A},
  booktitle={Natural language generation},
  pages={85--95},
  year={1987},
  publisher={Springer}
}
@inproceedings{prasad2008penn,
  title={The Penn Discourse TreeBank 2.0.},
  author={Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Miltsakaki, Eleni and Robaldo, Livio and Joshi, Aravind K and Webber, Bonnie L},
  booktitle={Proceedings of LREC},
  pages={2961--2968},
  year={2008},
}
@InProceedings{lee:17,
  author = 	"Lee, Kenton
		and He, Luheng
		and Lewis, Mike
		and Zettlemoyer, Luke",
  title = 	"End-to-end Neural Coreference Resolution",
  booktitle = 	"Proceedings of EMNLP",
  year = 	"2017",
  pages = 	"188--197",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1018",
  url = 	"http://aclweb.org/anthology/D17-1018"
}
@InProceedings{D18-1191,
  author = 	"Ouchi, Hiroki
		and Shindo, Hiroyuki
		and Matsumoto, Yuji",
  title = 	"A Span Selection Model for Semantic Role Labeling",
  booktitle = 	"Proceedings of EMNLP",
  year = 	"2018",
  pages = 	"1630--1642",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1191"
}
@InProceedings{P18-2058,
  author = 	"He, Luheng
		and Lee, Kenton
		and Levy, Omer
		and Zettlemoyer, Luke",
  title = 	"Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
  booktitle = 	"Proceedings of ACL",
  year = 	"2018",
  pages = 	"364--369",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-2058"
}
@article{Collins2004,
abstract = {This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.},
author = {Collins, Michael and Roark, Brian},
doi = {10.3115/1218955.1218970},
file = {:Users/kuribayashi/Desktop/essay/Incremental Parsing with the Perceptron Algorithm.pdf:pdf},
journal = {Proceedings of ACL},
pages = {111--es},
title = {{Incremental parsing with the perceptron algorithm}},
url = {http://portal.acm.org/citation.cfm?doid=1218955.1218970},
year = {2004}
}
@proceedings{Song2014,
author = {Song, Yi and Heilman, Michael and {Beigman Klebanov}, Beata and Deane, Paul},
file = {::},
booktitle = {Proceedings of the First Workshop on Argumentation Mining},
pages = {69--78},
title = {{Applying Argumentation Schemes for Essay Scoring}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2110},
year = {2014}
}
@inproceedings{Peldszus2016,
author = {Peldszus, Andreas and Stede, Manfred},
file = {::},
booktitle = {Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation},
title = {{An Annotated Corpus of Argumentative Microtexts}},
url = {http://www.ling.uni-potsdam.de/{~}peldszus/eca2015-preprint.pdf},
year = {2016},
pages={801--815}
}
@article{Teufel2002,
abstract = {In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges' agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.},
author = {Teufel, Simone and Moens, Marc},
doi = {10.1162/089120102762671936},
file = {::},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {4},
pages = {409--445},
title = {{Summarizing Scientific Articles: Experiments with Relevance and Rhetorical Status}},
url = {http://www.aclweb.org/anthology/J02-4002},
volume = {28},
year = {2002}
}
@article{Cocarascu2017,
author = {Cocarascu, Oana and Toni, Francesca},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Cocarascu, Toni - 2017 - Identifying attack and support argumentative relations using deep learning(2).pdf:pdf},
journal = {ACL},
pages = {1385--1390},
title = {{Identifying attack and support argumentative relations using deep learning}},
url = {http://aclweb.org/anthology/D17-1145},
year = {2017}
}
@article{Schneider2014a,
author = {Schneider, Jodi},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{An informatics perspective on argumentation mining}},
url = {http://ceur-ws.org/Vol-1341/paper11.pdf},
volume = {1341},
year = {2014}
}
@article{Moens2017,
author = {Moens, Marie-Francine},
doi = {10.3233/AAC-170025},
file = {::},
issn = {19462174},
journal = {Argument {\&} Computation},
keywords = {Natural language understanding,representation lea},
pages = {1--14},
title = {{Argumentation mining: How can a machine acquire common sense and world knowledge?}},
url = {http://www.medra.org/servlet/aliasResolver?alias=iospress{\&}doi=10.3233/AAC-170025},
year = {2017}
}

@inproceedings{stab2017,
  title={Parsing argumentation structures in persuasive essays},
  author={Stab, Christian and Gurevych, Iryna},
  booktitle={Proceedings of CL},
  year={2017},
  publisher={MIT Press},
volume = {43},
 pages = {619--659}
}

@article{Araszkiewicz2014,
author = {Araszkiewicz, Micha{\l} and {\L}opatkiewicz, Agata},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Legal argumentation concerning Almost Identical Expressions (AIE) in statutory texts}},
url = {http://ceur-ws.org/Vol-1341/paper8.pdf},
volume = {1341},
year = {2014}
}
@article{Ashley2014,
abstract = {Argument extraction techniques can likely improve legal information retrieval. Any effort to achieve that goal should take into account key features of legal reasoning such as the importance of legal rules and concepts, support and attack relations among claims, and citation of authoritative sources. Annotation types reflecting these key features will help identify the roles of textual elements in retrieved legal cases in order to better inform assessments of relevance for users' queries. As a result, legal argument models and argument schemes will likely play a central part in the text annotation type system.},
author = {Ashley, Kevin D.},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Applying argument extraction to improve legal information retrieval}},
url = {http://ceur-ws.org/Vol-1341/paper3.pdf},
volume = {1341},
year = {2014}
}
@article{Kiesel2015,
abstract = {This paper proposes a shared task for the identification of the argumentative structure in newspaper editorials. By the term “argumentative structure” we refer to the sequence of argumentative units in the text along with the relations between them. The main contribution is a large-scale dataset with more than 200 annotated editorials, which shall help argumentation mining researchers to evaluate and compare their systems in a standardized manner. The paper details how we model and manually identify argumentative structures in order to build this evaluation resource. Altogether, we consider the proposed task as a constructive step towards improving writing assistance systems and debating technologies.},
author = {Kiesel, Johannes and Al-Khatib, Khalid and Hagen, Matthias and Stein, Benno},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {2--5},
title = {{A Shared Task on Argumentation Mining in Newspaper Editorials}},
url = {http://aclweb.org/anthology/W/W15/W15-0505.pdf},
year = {2015}
}
@article{Bar-Haim2017,
abstract = {Stance classification is a core compo-nent in on-demand argument construction pipelines. Previous work on claim stance classification relied on background knowl-edge such as manually-composed senti-ment lexicons. We show that both accu-racy and coverage can be significantly im-proved through automatic expansion of the initial lexicon. We also developed a set of contextual features that further improves the state-of-the-art for this task.},
author = {Bar-Haim, Roy and Edelstein, Lilach and Jochim, Charles and Slonim, Noam},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Bar-Haim et al. - 2017 - Improving Claim Stance Classification with Lexical Knowledge Expansion and Context Utilization(2).pdf:pdf},
number = {2},
pages = {32--38},
title = {{Improving Claim Stance Classification with Lexical Knowledge Expansion and Context Utilization}},
url = {https://pdfs.semanticscholar.org/3213/a028427628e00ffa320f504b9195ad188e1b.pdf},
year = {2017}
}
@article{Wacholder2014,
author = {Wacholder, Nina and Muresan, Smaranda and Ghosh, Debanjan and Aakhus, Mark},
file = {::},
pages = {120--128},
title = {{Annotating Multiparty Discourse : Challenges for Agreement Metrics}},
url = {https://www.aclweb.org/anthology/W/W14/W14-4918.pdf},
year = {2014}
}
@article{Trevisan2014,
abstract = {[In this paper we present a new methodological approach for the analysis of public discourses aiming at the semi-automated identification of arguments by combining methods from discourse analysis with methods from Natural Language Processing. Discourses evolve over long periods of time and, consequently, form a broad database. Up to now, the analysis of discourses is hitherto performed primarily by hand, i.e., only small corpora or discourse fragments can be analyzed. Inevitably, this leads to lengthy and expensive annotation. Thus, there is a growing interest to overcome these methodological challenges by the use of computer-based methods and tools for the semi-automated analysis.},
author = {Trevisan, Bianka and Dickmeis, Eva and Jakobs, Eva-Maria and Niehr, Thomas},
file = {::},
isbn = {9781941643068},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {104--105},
title = {{Indicators of Argument-conclusion Relationships. An Approach for Argumentation Mining in German Discourses}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2116},
year = {2014}
}
@inproceedings{Nguyen2016,
  title={Context-aware argumentative relation mining},
  author={Nguyen, Huy and Litman, Diane},
  booktitle={Proceedings of ACL},
  pages={1127--1137},
  year={2016}
}
@article{Sardianos2015,
abstract = {Argument extraction is the task of identifying arguments, along with their components in text. Arguments can be usually decomposed into a claim and one or more premises justifying it. The proposed approach tries to identify seg-ments that represent argument elements (claims and premises) on social Web texts (mainly news and blogs) in the Greek language, for a small set of thematic domains, including articles on poli-tics, economics, culture, various social issues, and sports. The proposed approach exploits distributed representations of words, extracted from a large non-annotated corpus. Among the novel aspects of this work is the thematic do-main itself which relates to social Web, in con-trast to traditional research in the area, which concentrates mainly on law documents and sci-entific publications. The huge increase of so-cial web communities, along with their user ten-dency to debate, makes the identification of ar-guments in these texts a necessity. In addition, a new manually annotated corpus has been con-structed that can be used freely for research pur-poses. Evaluation results are quite promising, suggesting that distributed representations can contribute positively to the task of argument ex-traction.},
author = {Sardianos, Christos and Katakis, Ioannis Manousos and Petasis, Georgios and Karkaletsis, Vangelis},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {56--66},
title = {{Argument Extraction from News}},
url = {http://aclweb.org/anthology/W/W15/W15-0508.pdf},
year = {2015}
}
@article{Habernal2015,
abstract = {? 2015 Association for Computational Linguistics.Analyzing arguments in user-generated Web discourse has recently gained attention in argumentation mining, an evolving held of NLP. Current approaches, which employ fully-supervised machine learning, are usually domain dependent and suffer from the lack of large and diverse annotated corpora. However, annotating arguments in discourse is costly, errorprone, and highly context-dependent. We asked whether leveraging unlabeled data in a semi-supervised manner can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios.},
archivePrefix = {arXiv},
arxivId = {1401.6330},
author = {Habernal, Ivan and Gurevych, Iryna},
doi = {10.1162/COLI_a_00276},
eprint = {1401.6330},
isbn = {9781941643327},
issn = {0891-2017},
journal = {Proceedings of EMNLP},
number = {September},
pages = {2127--2137},
pmid = {791643259},
title = {{Exploiting Debate Portals for Semi-Supervised Argumentation Mining in User-Generated Web Discourse}},
url = {http://aclanthology.info/papers/exploiting-debate-portals-for-semi-supervised-argumentation-mining-in-user-generated-web-discourse},
year = {2015}
}
@article{Stab2014b,
abstract = {In this paper, we analyze and discuss ap- proaches to argumentation mining from the discourse structure perspective. We chose persuasive essays and scientific ar- ticles as our example domains. By an- alyzing several example arguments and providing an overview of previous work on argumentation mining, we derive im- portant tasks that are currently not ad- dressed by existing argumentation mining systems, most importantly, the identifica- tion of argumentation structures. We dis- cuss the relation of this task to automated discourse analysis and describe prelimi- nary results of two annotation studies fo- cusing on the annotation of argumentation structure. Based on our findings, we derive three challenges for encouraging future re- search on argumentation mining.},
author = {Stab, Christian and Kirschner, Christian and Eckle-Kohler, Judith and Gurevych, Iryna},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {1999},
title = {{Argumentation mining in persuasive essays and scientific articles from the discourse structure perspective}},
url = {http://ceur-ws.org/Vol-1341/paper5.pdf},
volume = {1341},
year = {2014}
}
@article{Kang2014,
author = {Kang, Juyeon and Saint-Dizier, Patrick},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {108--109},
title = {{Requirement Mining in Technical Documents}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2118},
year = {2014}
}
@article{Bex2014,
abstract = {We often try to teach people through stories and narratives instead of giving them explicit facts and rules. But how do these stories influence us, how do they persuade us to change our attitudes? In this paper, we aim to answer these questions by providing a computational model that offers an internal perspective on character motives in stories. This model allows us to represent the deliberations of the main characters and how they weighed their values and motives given their attitudes. We illustrate out model by discussing the well known fable of the Ant and the Grasshopper and the parable of the Prodigal Son.},
author = {Bex, Floris and Atkinson, Katie and Bench-Capon, Trevor},
doi = {10.1093/llc/fqu054},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Bex, Atkinson, Bench-Capon - 2014 - Arguments as a new perspective on character motive in stories(2).pdf:pdf},
issn = {14774615},
journal = {Literary and Linguistic Computing},
keywords = {arguments,character motive,stories},
number = {4},
pages = {467--487},
title = {{Arguments as a new perspective on character motive in stories}},
url = {http://www.florisbex.com/papers/LLC.pdf},
volume = {29},
year = {2014}
}
@article{IbnFaiz2014,
author = {{Ibn Faiz}, Syeed and Mercer, Robert E and Faiz, Syeed Ibn},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Ibn Faiz, Mercer, Faiz - 2014 - Extracting Higher Order Relations From Biomedical Text.pdf:pdf},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {100--101},
title = {{Extracting Higher Order Relations From Biomedical Text}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2114},
year = {2014}
}
@article{Lippi2015a,
abstract = {Argument mining has recently become a hot topic, attract- ing the interests of several and diverse research communities, ranging from artificial intelligence, to computational linguistics, natural language processing, social and philosophical sciences. In this paper, we attempt to describe the problems and challenges of argument mining from amachine learning angle. In particular, we advocate that machine learning tech- niques so far have been under-exploited, and that a more proper stan- dardization of the problem, also with regards to the underlying argument model, could provide a crucial element to develop better systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Lippi, Marco and Torroni, Paolo},
doi = {10.1007/978-3-319-28460-6_10},
eprint = {9809069v1},
file = {::},
isbn = {9783319284590},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {163--176},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Argument mining: A machine learning perspective}},
url = {http://homepages.abdn.ac.uk/n.oren/pages/TAFA-15/TAFA-15{\_}submission{\_}18.pdf},
volume = {9524},
year = {2015}
}
@article{Peldszus2015a,
abstract = {Argumentation mining obviously involves finding support relations between statements, but many interesting instances of argumentation also contain counter-considerations, which the author mentions in order to preempt possible objections by the readers. A counterconsideration in monologue text thus involves a switch of perspective toward an imaginary opponent. We present a classification approach to classifying counter-considerations and apply it to two different corpora: a selection of very short argumentative texts produced in a text generation experiment, and a set of newspaper commentaries. As expected, the latter pose more difficulties, which we investigate in a brief error anaylsis},
author = {Peldszus, Andreas and Stede, Manfred},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
number = {2011},
pages = {104--109},
title = {{Towards Detecting Counter-considerations in Text}},
url = {http://aclweb.org/anthology/W/W15/W15-0513.pdf},
year = {2015}
}
@article{Ong2014,
author = {Ong, Nathan and Litman, Diane and Brusilovsky, Alexandra},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {24--28},
title = {{Ontology-Based Argument Mining and Automatic Essay Scoring}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2104},
year = {2014}
}
@article{Boltuzic2015,
abstract = {Online debates sparkle argumentative discussions from which generally accepted arguments often emerge. We consider the task of unsupervised identification of prominent argument in online debates. As a first step, in this paper we perform a cluster analysis using semantic textual similarity to detect similar arguments. We perform a preliminary cluster evaluation and error analysis based on cluster-class matching against a manually labeled dataset},
author = {Boltu{\v{z}}i{\'{c}}, Filip and {\v{S}}najder, Jan},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
number = {2007},
pages = {110--115},
title = {{Identifying Prominent Arguments in Online Debates Using Semantic Textual Similarity}},
url = {http://aclweb.org/anthology/W/W15/W15-0514.pdf},
year = {2015}
}
@article{Houngbo2014,
author = {Houngbo, Hospice and Mercer, Robert},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {19--23},
title = {{An automated method to build a corpus of rhetorically-classified sentences in biomedical texts}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2103},
year = {2014}
}
@article{Lippi2015,
abstract = {Argumentation mining aims at automatically extracting structured arguments from unstructured textual documents. It has recently become a hot topic also due to its potential in processing information originating from the Web, and in particular from social media, in innovative ways. Recent advances in machine learning methods promise to enable breakthrough applications to social and economic sciences, policy making, and information technology: something that only a few years ago was unthinkable. In this survey article, we introduce argumentation models and methods, review existing systems and applications, and discuss challenges and perspectives of this exciting new research area.},
author = {Lippi, Marco and Torroni, Paolo},
doi = {10.1145/0000000.0000000},
file = {::},
issn = {15335399},
journal = {ACM Transactions on Internet Technology},
pages = {1--25},
title = {{A Argumentation Mining: State of the Art and Emerging Trends}},
url = {http://lia.disi.unibo.it/{~}ml/publications/ACMTOIT2015.pdf},
volume = {V},
year = {2015}
}
@article{Afantenos2014,
author = {Afantenos, Stergos and Asher, Nicholas},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {1999},
title = {{Counter-argumentation and discourse: A case study}},
url = {http://ceur-ws.org/Vol-1341/paper2.pdf},
volume = {1341},
year = {2014}
}
@article{Kirschner2015,
author = {Kirschner, Christian and Eckle-Kohler, Judith and Gurevych, Iryna},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {1--11},
title = {{Linking the Thoughts : Analysis of Argumentation Structures in Scientific Publications}},
url = {http://www.aclweb.org/anthology/W15-0501},
year = {2015}
}
@article{Hirst2014,
abstract = {In argumentative political speech, the way an issue is framed may indicate the unstated assumptions of the argument and hence the ideological position of the speaker. Our goal is to use and extend our prior work on discourse parsing and the identification of argumentation schemes to identify specific instances of issue framing and, more generally, ideological positions as they are expressed in text. We are using annotated historical and contemporary proceedings of the British, Canadian, and Dutch parliaments, looking in particular at speech on the topic of immigration.},
author = {Hirst, Graeme and Feng, Vanessa Wei and Cochrane, Christopher and Naderi, Nona},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Argumentation, ideology, and issue framing in parliamentary discourse}},
url = {http://ceur-ws.org/Vol-1341/paper6.pdf},
volume = {1341},
year = {2014}
}
@article{Stab2014a,
abstract = {In this paper, we present a novel approach for identifying argumentative discourse structures in persuasive essays. The structure of argumentation consists of several components (i.e. claims and premises) that are connected with argumentative relations. We consider this task in two consecutive steps. First, we identify the components of arguments using multiclass classification. Second, we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse. For both tasks, we evaluate several classifiers and propose novel feature sets including structural, lexical, syntactic and contextual features. In our experiments, we obtain a macro F1-score of 0.726 for identifying argument components and 0.722 for argumentative relations.},
author = {Stab, Christian and Gurevych, Iryna},
file = {::},
journal = {Proceedings of EMNLP},
pages = {46--56},
title = {{Identifying Argumentative Discourse Structures in Persuasive Essays}},
url = {http://www.aclweb.org/anthology/D14-1006},
year = {2014}
}
@article{Wei2016,
abstract = {In this paper we study how to identify per-suasive posts in the online forum discus-sions, using data from Change My View sub-Reddit. Our analysis confirms that the users' voting score for a comment is highly correlated with its metadata infor-mation such as published time and author reputation. In this work, we propose and evaluate other features to rank comments for their persuasive scores, including tex-tual information in the comments and so-cial interaction related features. Our ex-periments show that the surface textual features do not perform well compared to the argumentation based features, and the social interaction based features are effec-tive especially when more users partici-pate in the discussion.},
author = {Wei, Zhongyu and Liu, Yang and Li, Yi},
file = {::},
isbn = {9781510827592},
journal = {Acl},
pages = {195--200},
title = {{Is This Post Persuasive ? Ranking Argumentative Comments in the Online Forum}},
url = {http://aclweb.org/anthology/P16-2032},
year = {2016}
}
@article{Aharoni2014,
abstract = {We describe a novel and unique argumentative structure dataset. This corpus consists of data extracted from hundreds of Wikipedia articles using a meticulously monitored manual annotation process. The result is 2,683 argument elements, collected in the context of 33 controversial topics, organized under a simple claim-evidence structure. The obtained data are publicly available for academic research},
author = {Aharoni, Ehud and Polnarov, Anatoly and Lavee, Tamar and Hershcovich, Daniel and Levy, Ran and Rinott, Ruty and Gutfreund, Dan and Slonim, Noam},
file = {::},
isbn = {9781941643068},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {64--68},
title = {{A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of Controversial Topics}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2109},
year = {2014}
}
@article{Rooney2012,
abstract = {The area of argumentation theory is an increasingly impor- tant area of artificial intelligence and mechanisms that are able to automatically detect the argument structure provide a novel area of research. This paper considers the use of ker- nel methods for argumentation detection and classification. It shows that a classification accuracy of 65{\%}, can be attained using Natural Language Processing based kernel approaches, which do not require any heuristic choice of features.},
author = {Rooney, Niall and Wang, Hui and Browne, Fiona},
file = {::},
isbn = {9781577355588},
journal = {Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference Applying},
keywords = {Special Track on Applied Natural Language Processi},
pages = {272--275},
title = {{Applying Kernel Methods to Argumentation Mining}},
url = {http://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS12/paper/view/4366},
year = {2012}
}
@article{Peldszus2013,
abstract = {In this paper, we consider argument mining as the task of building a formal representation for an argumentative piece of text. Our goal is to provide a critical survey of the literature on both the resulting representations (i.e., argument diagramming techniques) and on the various aspects of the automatic analysis process. For representation, we also provide a synthesized proposal of a scheme that combines advantages from several of the earlier approaches; in addition, we discuss the relationship between representing argument structure and the rhetorical structure of texts in the sense of Mann and Thompsons (1988) RST. Then, for the argument mining problem, we also cover the literature on closely-related tasks that have been tackled in Computational Linguistics, because we think that these can contribute to more powerful argument mining systems than the first prototypes that were built in recent years. The paper concludes with our suggestions for the major challenges that should be addressed in the field of argument mining.},
author = {Peldszus, Andreas and Stede, Manfred},
doi = {10.4018/jcini.2013010101},
file = {::},
issn = {1557-3958},
journal = {International Journal of Cognitive Informatics and Natural Intelligence},
number = {1},
pages = {1--31},
title = {{From Argument Diagrams to Argumentation Mining in Texts: a survey}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/jcini.2013010101},
volume = {7},
year = {2013}
}
@article{Graves2014,
author = {Graves, Heather and Graves, Roger and Mercer, Robert and Akter, Mahzereen},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {98--99},
title = {{Titles That Announce Argumentative Claims in Biomedical Research Articles}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2113},
year = {2014}
}
@inproceedings{Niculae2017,
abstract = {We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20{\%} of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.},
author = {Niculae, Vlad and Park, Joonsuk and Cardie, Claire},
eprint = {1704.06869},
booktitle = {Proceedings of ACL},
file = {:Users/kuribayashi/Desktop/essay/Argument Minig with Structured SVMs and RNNs.pdf:pdf},
title = {{Argument Mining with Structured SVMs and RNNs}},
pages = {985--995},
year = {2017}
}
@article{Bex2014a,
abstract = {In this paper, we discuss how Value-based Argumentation can be used as a tool in human and computer story understanding, especially where understanding the story requires understanding of the motives of its characters. It is shown how arguments about motives can be extracted from stories, and how dialogues about these arguments can aid in story understanding.},
author = {Bex, Floris and Bench-Capon, Trevor},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Extracting and understanding arguments about motives from stories}},
url = {http://ceur-ws.org/Vol-1341/paper9.pdf},
volume = {1341},
year = {2014}
}
@article{Lawrence2014,
abstract = {In this paper we look at the manual analysis of arguments and how this compares to the current state of automatic argument analysis. These considerations are used to develop a new approach combining a machine learning algorithm to extract propositions from text, with a topic model to determine argument structure. The results of this method are compared to a manual analysis.},
author = {Lawrence, John and Reed, Chris and Allen, Colin and McAlister, Simon and Ravenscroft, Andrew},
file = {::},
isbn = {9781941643068},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {79--87},
title = {{Mining Arguments From 19th Century Philosophical Texts Using Topic Based Modelling}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2111},
year = {2014}
}
@article{Mao2014,
author = {Mao, Fiona and Mercer, Robert and Xiao, Lu},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {106--107},
title = {{Extracting Imperatives from Wikipedia Article for Deletion Discussions}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2117},
year = {2014}
}
@article{Lawrence2017,
author = {Lawrence, John and Reed, Chris},
file = {::},
journal = {Proceedings of the 4th Workshop on Argument Mining (ArgMin'17)},
pages = {39--48},
title = {{Mining Argumentative Structure from Natural Language Text Using Automatically Generated Premise-Conclusion Topic Models}},
url = {http://aclweb.org/anthology/W17-5105},
year = {2017}
}
@article{Cabrio2012,
abstract = {Many argumentation tools have been proposed nowadays to support the users in on-line social discussions. However, the main drawback of these tools is that they do not cope with the automatic generation of the arguments from the nat-ural language discussions of the users. In this paper, we propose to use a technique from computational linguistics, namely textual entailment, to generate in an auto-matic way the abstract arguments from the dialogues. The abstract arguments as well as their relationships are then structured in an argumentation graph to evaluate the dialogue as a whole. The success criteria of the proposed approach is that it is able to represent the dynamics of the dialogues among users allowing to find the use of argumentation natural enough to be really adopted.},
author = {Cabrio, Elena and Villata, Serena},
doi = {10.3233/978-1-61499-111-3-454},
file = {::},
isbn = {9781614991106},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {Abstract argumentation theory,Bipolar argumentation,Textual entailment},
number = {1},
pages = {454--461},
title = {{Generating abstract arguments: A natural language approach}},
url = {http://www-sop.inria.fr/members/Serena.Villata/Resources/comma2012.pdf},
volume = {245},
year = {2012}
}
@article{Biran2011,
abstract = {In written dialog, discourse participants need to justify claims they make, to convince the reader the claim is true and/or relevant to the discourse. This paper presents a new task (with an associated corpus), namely detecting such justifications. We investigate the nature of such justifications, and observe that the justifications themselves often contain discourse structure. We therefore develop a method to detect the existence of certain types of discourse relations, which helps us classify whether a segment is a justification or not. Our task is novel, and our work is novel in that it uses a large set of connectives (which we call indicators), and in that it uses a large set of discourse relations, without choosing among them.},
author = {Biran, Or and Rambow, Owen},
doi = {10.1109/ICSC.2011.41},
file = {::},
isbn = {9780769544922},
issn = {1793-351X},
journal = {Proceedings - 5th IEEE International Conference on Semantic Computing, ICSC 2011},
number = {1},
pages = {162--168},
title = {{Identifying justifications in written dialogs}},
url = {http://www.cs.columbia.edu/{~}orb/papers/biran{\_}rambow{\_}icsc2011.pdf},
year = {2011}
}
@article{Somasundaran2008,
author = {Somasundaran, Swapna},
title = {{Recognizing Stances in Ideological On-Line Debates}},
year = {2008}
}
@article{IvanHabernal2016,
abstract = {We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in nat- ural language texts. The architecture com- bines various linguistically-motivated clas- sification features in a Bayesian Network. We introduce novel ways for computing many of these features, and manually de- fine linguistically-motivated interrelationships among them, which the Bayesian network models. Our methodology is almost en- tirely unsupervised and completely language- independent; it relies on few language re- sources and is thus suitable for a large num- ber of languages. Furthermore, unlike much recent work, our approach can identify ex- pressions of various types and syntactic con- structions. We demonstrate a significant im- provement in identification accuracy, com- pared with less sophisticated baselines.},
archivePrefix = {arXiv},
arxivId = {1401.6330},
author = {{Ivan Habernal}, Gurevych Iryna},
doi = {10.1162/COLI},
eprint = {1401.6330},
file = {::},
isbn = {9781608459858},
issn = {04194217},
journal = {Dissertation Abstracts International, B: Sciences and Engineering},
number = {8},
pages = {4943},
pmid = {791643259},
title = {{Argumentation Mining in User-Generated Web Discourse}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00239},
volume = {70},
year = {2016}
}
@article{Giles,
author = {Giles, C Lee},
file = {::},
isbn = {9781450341448},
pages = {2890512},
title = {{Scholarly Big Data Knowledge and Semantics}},
url = {http://www2016.net/proceedings/companion/p371.pdf}
}
@article{Tratz2011,
abstract = {Dependency parsers are critical components within many NLP systems. However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of nonprojectivity support. Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation. In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it. We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing. The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93{\%} accuracy.},
author = {Tratz, Stephen and Hovy, Eduard},
file = {:Users/kuribayashi/Desktop/essay/A Fast, Accurate, Non-Projective, Semantically-Enriched Parser.pdf:pdf},
isbn = {978-1-937284-11-4},
journal = {Proceedings of EMNLP},
pages = {1257--1268},
title = {{A Fast, Accurate, Non-Projective, Semantically-Enriched Parser}},
url = {http://dl.acm.org/citation.cfm?id=2145564},
year = {2011}
}
@article{Carenini2006,
abstract = {Evaluative arguments are pervasive in natural human communication. In countless situations people attempt to advise or persuade their interlocutors that something is desirable (vs. undesirable) or right (vs. wrong). With the proliferation of on-line systems serving as personal advisors and assistants, there is a pressing need to develop general and testable computational models for generating and presenting evaluative arguments. Previous research on generating evaluative arguments has been characterized by two major limitations. First, researchers have tended to focus only on specific aspects of the generation process. Second, the proposed approaches were not empirically tested. The research presented in this paper addresses both limitations. We have designed and implemented a complete computational model for generating evaluative arguments. For content selection and organization, we devised an argumentation strategy based on guidelines from argumentation theory. For expressing the content in natural language, we extended and integrated previous work in computational linguistics on generating evaluative arguments. The key knowledge source for both tasks is a quantitative model of user preferences. To empirically test critical aspects of our generation model, we have devised and implemented an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. Within the framework, we have performed an experiment to test two basic hypotheses on which the design of the computational model is based; namely, that our proposal for tailoring an evaluative argument to the addressee's preferences increases its effectiveness, and that differences in conciseness significantly influence argument effectiveness. The second hypothesis was confirmed in the experiment. In contrast, the first hypothesis was only marginally confirmed. However, independent testing by other researchers has recently provided further support for this hypothesis. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Carenini, Giuseppe and Moore, Johanna D.},
doi = {10.1016/j.artint.2006.05.003},
file = {::},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Empirical evaluation,Natural language generation,Preferences,User tailoring},
number = {11},
pages = {925--952},
pmid = {21577044},
title = {{Generating and evaluating evaluative arguments}},
url = {https://www.cs.ubc.ca/{~}carenini/PAPERS/AIJ06.pdf},
volume = {170},
year = {2006}
}
@article{Nguyen2015,
author = {Nguyen, Huy V and Litman, Diane J},
file = {::},
journal = {Naacl Hlt 2015},
number = {1},
pages = {22},
title = {{Extracting argument and domain words for identifying argument components in texts}},
url = {http://aclweb.org/anthology/W/W15/W15-0503.pdf},
year = {2015}
}
@article{Teufel2009,
abstract = {Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scientific paper. It has been shown to be reliably used by independent human coders, and has proven useful for various information access tasks. Annotation experiments have however so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain specific rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics. {\textcopyright} 2009 ACL and AFNLP.},
author = {Teufel, S. and Siddharthan, A. and Batchelor, C.},
file = {::},
isbn = {978-1-932432-63-3},
journal = {Proceedings of EMNLP},
number = {August},
pages = {1493--1502},
title = {{Towards discipline-independent Argumentative Zoning: Evidence from chemistry and computational linguistics}},
url = {https://www.aclweb.org/anthology/D09-1155},
year = {2009}
}
@article{Oraby2015,
author = {Oraby, Shereen and Reed, Lena and Compton, Ryan and Riloff, Ellen and Walker, Marilyn and Whittaker, Steve},
file = {::},
journal = {Proceedings of NAACL},
pages = {116},
title = {{And That's A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue}},
url = {http://aclweb.org/anthology/W/W15/W15-0515.pdf},
year = {2015}
}
@article{Park2014,
abstract = {The ability to analyze the adequacy of supporting information is necessary for determining the strength of an argument.1 This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition as UNVERIFIABLE, VERIFIABLE NONEXPERIENTIAL, or VERIFIABLE EXPERIENTIAL2 , where the appropriate type of support is reason, evidence, and optional evidence, respectively3 . Once the existing support for propositions are identi- fied, this classification can provide an estimate of how adequately the arguments have been supported. We build a goldstandard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, achieving a macro-averaged F1 of 68.99{\%}},
author = {Park, Joonsuk and Cardie, Claire},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {29--38},
title = {{Identifying Appropriate Support for Propositions in Online User Comments}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2105},
year = {2014}
}
@article{Walton2016,
author = {Walton, Douglas and Macagno, Fabrizio},
doi = {10.1080/19462166.2015.1123772},
file = {::},
issn = {1946-2166},
journal = {Argument {\&} Computation},
keywords = {argument interpretation,argument representation,computational models of,natural language argument},
number = {April},
pages = {(in press)},
title = {{A classification system for argumentation schemes}},
url = {http://www.tandfonline.com/doi/full/10.1080/19462166.2015.1123772},
year = {2016}
}
@article{Ghosh2014,
abstract = {Argument mining of online interactions is in its infancy. One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential.},
author = {Ghosh, Debanjan and Muresan, Smaranda and Wacholder, Nina and Aakhus, Mark and Mitsui, Matthew},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {39--48},
title = {{Analyzing Argumentative Discourse Units in Online Interactions}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2106},
year = {2014}
}
@article{Habernal2014,
abstract = {In this paper, we argue that an annotation scheme for argumentation mining is a function of the task requirements and the corpus properties. There is no one-size- fits-all argumentation theory to be applied to realistic data on the Web. In two annotation studies, we experiment with 80 German newspaper editorials from the Web and about one thousand English documents from forums, comments, and blogs. Our example topics are taken from the educational domain. To formalize the problem of annotating arguments, in the first case, we apply a Claim-Premise scheme, and in the second case, we modify Toulmin's scheme. We find that the choice of the argument components to be annotated strongly depends on the register, the length of the document, and inherently on the literary devices and structures used for expressing argumentation. We hope that these findings will facilitate the creation of reliably annotated argumentation corpora for a wide range of tasks and corpus types and will help to bridge the gap between argumentation theories and actual application needs},
author = {Habernal, Ivan and Eckle-Kohler, Judith and Gurevych, Iryna},
file = {::},
issn = {16130073},
journal = {Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing},
pages = {26--39},
title = {{Argumentation Mining on the Web from Information Seeking Perspective}},
url = {http://ceur-ws.org/Vol-1341/},
year = {2014}
}
@article{Carstens2015,
abstract = {We advocate a relation based approach to Argumentation Mining. Our focus lies on the extraction of argumentative relations instead of the identification of arguments, themselves. By classifying pairs of sentences according to the relation that holds between them we are able to identify sentences that may be factual when considered in isolation, but carry argumentative meaning when read in context. We describe scenarios in which this is useful, as well as a corpus of annotated sentence pairs we are developing to provide a testbed for this approach},
author = {Carstens, Lucas and Toni, Francesca},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {29--34},
title = {{Towards relation based Argumentation Mining}},
url = {http://aclweb.org/anthology/W/W15/W15-0504.pdf},
year = {2015}
}
@article{Green2015,
abstract = {This paper presents preliminary work on identification of argumentation schemes, i.e., identifying premises, conclusion and name of argumentation scheme, in arguments for scientific claims in genetics research articles. The goal is to develop annotation guidelines for creating corpora for argumentation mining research. This paper gives the specification of ten semantically distinct argumentation schemes based on analysis of argumentation in several journal articles. In addition, it presents an empirical study on readers' ability to recognize some of the argumentation schemes},
author = {Green, Nancy L},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {12--21},
title = {{Identifying Argumentation Schemes in Genetics Research Articles}},
url = {http://aclweb.org/anthology/W/W15/W15-0502.pdf},
year = {2015}
}
@article{Wachsmuth2017c,
author = {Wachsmuth, Henning and Stein, Benno},
doi = {10.1145/2957757},
file = {:Users/kuribayashi/Desktop/essay/A Universal Model for Discourse-Level Argumentation Analysis.pdf:pdf},
issn = {15335399},
journal = {ACM Transactions on Internet Technology},
number = {3},
pages = {1--24},
title = {{A Universal Model for Discourse-Level Argumentation Analysis}},
url = {http://dl.acm.org/citation.cfm?doid=3106680.2957757},
volume = {17},
year = {2017}
}
@article{Sobhani2015,
abstract = {Argumentation mining and stance classification were recently introduced as interesting tasks in text mining. In this paper, a novel framework for argument tagging based on topic modeling is proposed. Unlike other machine learning approaches for argument tagging which often require large set of labeled data, the proposed model is minimally supervised and merely a one-to-one mapping between the pre-defined argument set and the extracted topics is required. These extracted arguments are subsequently exploited for stance classification. Additionally, a manuallyannotated corpus for stance classification and argument tagging of online news comments is introduced and made available. Experiments on our collected corpus demonstrate the benefits of using topic-modeling for argument tagging. We show that using Non-Negative Matrix Factorization instead of Latent Dirichlet Allocation achieves better results for argument classification, close to the results of a supervised classifier. Furthermore, the statistical model that leverages automatically-extracted arguments as features for stance classification shows promising results},
author = {Sobhani, Parinaz and Inkpen, Diana and Matwin, Stan},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {67--77},
title = {{From Argumentation Mining to Stance Classification}},
url = {http://aclweb.org/anthology/W/W15/W15-0509.pdf},
year = {2015}
}
@article{Stab2014,
abstract = {In this paper, we present a novel approach to model arguments, their components and relations in persuasive essays in English. We propose an annotation scheme that includes the annotation of claims and premises as well as support and attack relations for capturing the structure of argu- mentative discourse. $\backslash$n$\backslash$nWe further conduct a manual annotation study with three annotators on 90 persuasive essays. The obtained inter-rater agreement of $\alpha$U =0.72 for argument components and $\alpha$ =0.81 for argumentative relations indicates that the proposed annotation scheme success- fully guides annotators to substantial agreement. $\backslash$n$\backslash$nThe final corpus and the annotation guidelines are freely available to encourage future research in argument recognition. 1Introduction$\backslash$n$\backslash$nby Peldszus and Stede (2013) and define two directed relations between argument components:$\backslash$nsupport and attack.$\backslash$n$\backslash$nIn particular, the annotation guidelines consist of$\backslash$n$\backslash$n$\backslash$nthe following steps:$\backslash$n1. Topic and stance identification: Before starting with the annotation process, annotators identify the$\backslash$ntopic and the author's stance by reading the entire essay.$\backslash$n$\backslash$n$\backslash$n2. Annotation of argument components: In this step, the major claim is identified either in the intro-$\backslash$nduction or in the conclusion of an essay. Subsequently, annotators identify the claims and premises$\backslash$nin each paragraph. We instruct the annotators to annotate each argument component as a state-$\backslash$nment covering an entire sentence or less. We consolidate the annotations of all annotators before$\backslash$ncontinuing with the next step (section 5.4).$\backslash$n$\backslash$n3. Annotation of argumentative relations: Finally, the claims and premises are linked within each$\backslash$nparagraph, and the claims are linked to the major claim either with a support or attack relation$\backslash$n},
author = {Stab, Christian and Gurevych, Iryna},
file = {::},
isbn = {9781941643266},
journal = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
pages = {1501--1510},
title = {{Annotating Argument Components and Relations in Persuasive Essays}},
url = {http://www.aclweb.org/anthology/C14-1142},
year = {2014}
}
@inproceedings{Levy2014,
author = {Levy, Ran and Bilu, Yonatan and Hershcovich, Daniel and Aharoni, Ehud and Slonim, Noam},
booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin, Ireland, August 23-29 2014},
file = {::},
pages = {1489--1500},
title = {{Context Dependent Claim Detection}},
url = {http://www.aclweb.org/anthology/C14-1141},
year = {2014}
}
@article{Baroni2014,
abstract = {In existing literature, little attention has been paid to the problems of how the uncertainty reflected by natural language text (e.g. verbal and linguistic uncertainty) can be explicitly formulated in argumentation schemes, and how argumentation schemes enriched with various types of uncertainty can be exploited to support argumentation mining and evaluation. In this paper, we focus on the first problem, and introduce some preliminary ideas about how to classify and encompass uncertainty in argumentation schemes.},
author = {Baroni, Pietro and Giacomin, Massimiliano and Liao, Beishui and {Van Der Torre}, Leon},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Encompassing uncertainty in argumentation schemes}},
url = {http://ceur-ws.org/Vol-1341/paper10.pdf},
volume = {1341},
year = {2014}
}
@article{Daxenberger2017a,
abstract = {Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.},
archivePrefix = {arXiv},
arxivId = {1704.07203},
author = {Daxenberger, Johannes and Eger, Steffen and Habernal, Ivan and Stab, Christian and Gurevych, Iryna},
eprint = {1704.07203},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Daxenberger et al. - 2017 - What is the Essence of a Claim Cross-Domain Claim Identification.pdf:pdf},
journal = {Arxiv},
title = {{What is the Essence of a Claim? Cross-Domain Claim Identification}},
url = {http://arxiv.org/abs/1704.07203},
year = {2017}
}
@article{Falke2017,
abstract = {Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multi-document summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.},
archivePrefix = {arXiv},
arxivId = {1704.04452},
author = {Falke, Tobias and Gurevych, Iryna},
eprint = {1704.04452},
file = {::},
title = {{Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps}},
url = {http://arxiv.org/abs/1704.04452},
year = {2017}
}
@article{Habernal2016a,
author = {Habernal, Ivan and Gurevych, Iryna},
file = {::},
journal = {Proceedings of EMNLP},
pages = {1214--1223},
title = {{What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation}},
url = {https://aclweb.org/anthology/D/D16/D16-1129.pdf},
year = {2016}
}
@article{Rosenfeld2014,
author = {Rosenfeld, Ariel and Kraus, Sarit},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Argumentation theory in the field: An empirical study of fundamental notions}},
url = {http://ceur-ws.org/Vol-1341/paper7.pdf},
volume = {1341},
year = {2014}
}
@inproceedings{Peldszus2015,
author = {Peldszus, Andreas and Stede, Manfred},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Peldszus, Stede - 2015 - Joint prediction in MST-style discourse parsing for argumentation mining(2).pdf:pdf},
isbn = {9781941643327},
booktitle = {Proceedings of EMNLP},
pages = {938--948},
title = {{Joint prediction in MST-style discourse parsing for argumentation mining}},
url = {http://aclanthology.info/papers/joint-prediction-in-mst-style-discourse-parsing-for-argumentation-mining},
year = {2015}
}
@article{Wachsmuth2017b,
author = {Wachsmuth, Henning and Naderi, Nona and Hou, Yufang and Bilu, Yonatan and Prabhakaran, Vinodkumar and Thijmm, Tim Alberdingk and Hirst, Graeme and Stein, Benno},
file = {::},
isbn = {9781510838604},
journal = {Proceedings of the EACL2017},
pages = {176--187},
title = {{Computational Argumentation Quality Assessment in Natural Language}},
url = {http://www.aclweb.org/anthology/E17-1017},
year = {2017}
}
@article{Hidey2017,
author = {Hidey, Christopher and Musi, Elena and Hwang, Alyssa and Mckeown, Kathleen},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Hidey et al. - 2017 - Analyzing the Semantic Types of Claims and Premises in an Online Persuasive Forum(2).pdf:pdf},
pages = {11--21},
title = {{Analyzing the Semantic Types of Claims and Premises in an Online Persuasive Forum}},
url = {http://aclweb.org/anthology/W17-5102},
year = {2017}
}
@inproceedings{Potash2016,
abstract = {One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.},
booktitle = {Proceedings of EMNLP},
author = {Potash, Peter and Romanov, Alexey and Rumshisky, Anna},
eprint = {1612.08994},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Potash, Romanov, Rumshisky - 2016 - Here's My Point Joint Pointer Architecture for Argument Mining.pdf:pdf},
pages = {1375--1384},
title = {{Here's My Point: Joint Pointer Architecture for Argument Mining}},
url = {http://arxiv.org/abs/1612.08994},
year = {2017}
}
@article{Peldszus2014,
abstract = {Despite recent advances in discourse parsing and causality detection, the automatic recognition of argumentation structure of authentic texts is still a very challenging task. To approach this problem, we collected a small corpus of German microtexts in a text generation experiment, resulting in texts that are authentic but of controlled linguistic and rhetoric complexity. We show that trained annotators can determine the argumentation structure on these microtexts reliably. We experiment with different machine learning approaches for automatic argumentation structure recognition on various levels of granularity of the scheme. Given the complex nature of such a discourse understanding tasks, the first results presented here are promising, but invite for further investigation.},
author = {Peldszus, Andreas},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
number = {2012},
pages = {88--97},
title = {{Towards segment-based recognition of argumentation structure in short texts}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2112},
year = {2014}
}
@article{Tang2016,
author = {Tang, Jie},
file = {::},
isbn = {9781450341448},
pages = {2890513},
title = {{AMiner : Mining Deep Knowledge from Big Scholar Data Categories and Subject Descriptors}},
url = {http://www2016.net/proceedings/companion/p373.pdf},
year = {2016}
}
@article{Boltuzic2014a,
author = {Boltuzic, Filip and Snajder, Jan},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Boltuzic, Snajder - 2014 - Back up your Stance Recognizing Arguments in Online Discussions.pdf:pdf},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {49--58},
title = {{Back up your Stance : Recognizing Arguments in Online Discussions}},
url = {http://aclweb.org/anthology/W/W14/W14-2107.pdf},
year = {2014}
}
@article{Persing2016,
abstract = {Understanding the argumentative structure of a persuasive essay involves addressing two challenging tasks: identifying the components of the essay's argument and identifying the relations that occur between them. We ex-amine the under-investigated task of end-to-end argument mining in persuasive student es-says, where we (1) present the first results on end-to-end argument mining in student es-says using a pipeline approach; (2) address error propagation inherent in the pipeline ap-proach by performing joint inference over the outputs of the tasks in an Integer Linear Pro-gramming (ILP) framework; and (3) propose a novel objective function that enables F-score to be maximized directly by an ILP solver. We evaluate our joint-inference approach with our novel objective function on a publicly-available corpus of 90 essays, where it yields an 18.5{\%} relative error reduction in F-score over the pipeline system.},
author = {Persing, Isaac and Ng, Vincent},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Persing, Ng - 2016 - End-to-End Argumentation Mining in Student Essays.pdf:pdf},
isbn = {9781941643914},
booktitle = {Proceedings of NAACL},
pages = {1384--1394},
title = {{End-to-End Argumentation Mining in Student Essays}},
url = {http://www.aclweb.org/anthology/N16-1164},
year = {2016}
}
@article{Feng2011,
abstract = {Argumentation schemes are structures or tem- plates for various kinds of arguments. Given the text of an argument with premises and con- clusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. We achieve accura- cies of 63\UTF{2013}91{\%} in one-against-others classifi- cation and 80\UTF{2013}94{\%} in pairwise classification (baseline = 50{\%} in both cases).},
author = {Feng, Vanessa Wei and Hirst, Graeme},
file = {::},
isbn = {978-1-932432-87-9},
journal = {Computational Linguistics},
pages = {987--996},
title = {{Classifying Arguments by Scheme}},
url = {http://ftp.cs.toronto.edu/pub/gh/Feng+Hirst-2011.pdf},
year = {2011}
}
@article{Green2014,
author = {Green, Nancy},
file = {::},
isbn = {9781941643068},
journal = {Proceedings of the First Workshop on Argumentation Mining},
number = {1998},
pages = {11--18},
title = {{Towards Creation of a Corpus for Argumentation Mining the Biomedical Genetics Research Literature}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2102},
year = {2014}
}
@article{Lawrence2015,
abstract = {In this paper, we look at three different meth- ods of extracting the argumentative structure from a piece of natural language text. These methods cover linguistic features, changes in the topic being discussed and a supervised machine learning approach to identify the components of argumentation schemes, pat- terns of human reasoning which have been detailed extensively in philosophy and psy- chology. For each of these approaches we achieve results comparable to those previously reported, whilst at the same time achieving a more detailed argument structure. Finally, we use the results from these individual tech- niques to apply them in combination, further improving the argument structure identification.},
author = {Lawrence, John and Reed, Chris},
file = {::},
isbn = {9781941643341},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {127--136},
title = {{Combining Argument Mining Techniques}},
url = {http://aclweb.org/anthology/W/W15/W15-0516.pdf},
year = {2015}
}
@article{Green2017,
author = {Green, Nancy L},
file = {::},
pages = {73--78},
title = {{Manual Identification of Arguments with Implicit Conclusions Using Semantic Rules for Argument Mining}},
url = {http://aclweb.org/anthology/W17-5109},
year = {2017}
}
@article{Misra2015,
abstract = {More and more of the information available on the web is dialogic, and a significant portion of it takes place in online forum conversations about current social and political topics. We aim to develop tools to summarize what these conversations are about. What are the CENTRAL PROPOSITIONS associated with different stances on an issue; what are the abstract objects under discussion that are central to a speaker's argument? How can we recognize that two CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize that the CENTRAL PROPOSITIONS are exactly those arguments that people find most salient, and use human summarization as a probe for discovering them. We describe our corpus of human summaries of opinionated dialogs, then show how we can identify similar repeated arguments, and group them into FACETS across many discussions of a topic. We define a new task, ARGUMENT FACET SIMILARITY (AFS), and show that we can predict AFS with a .54 correlation score, versus an ngram system baseline of .39 and a semantic textual similarity system baseline of .45.},
author = {Misra, Amita and Anand, Pranav and Tree, Jean Fox and Walker, Marilyn},
file = {::},
isbn = {9781941643495},
journal = {Naacl2015},
pages = {430--440},
title = {{Using Summarization to Discover Argument Facets in Online Idealogical Dialog}},
url = {http://www.aclweb.org/anthology/N15-1046},
year = {2015}
}
@article{HernandezA.2014,
abstract = {In this paper we proposed a survey in sentiment, polarity and function analysis of citations. This is an interesting area that has had an increased development in recent years but still has plenty of room for growth and further research. The amount of scientific information in the web makes it necessary innovate the analysis of the influence of the work of peers and leaders in the scientific com- munity. We present an overview of gen- eral concepts, review contributions to the solution of related problems such as context identification, function and polarity classification, identify some trends and suggest possible future research directions.},
author = {{Hern{\'{a}}ndez A.}, Myriam and G{\'{o}}mez, Jos{\'{e}} M.},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {102--103},
title = {{Survey in sentiment, polarity and function analysis of citation}},
url = {http://acl2014.org/acl2014/W14-21/index.html},
year = {2014}
}
@article{Schneider2014b,
author = {Schneider, Jodi},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
number = {c},
pages = {59--63},
title = {{Automated argumentation mining to the rescue? Envisioning argumentation and decision-making support for debates in open online collaboration communities}},
url = {http://www.aclweb.org/anthology/W/W14/W14-2108},
year = {2014}
}
@article{Wyner2014,
abstract = {The paper discusses the architecture and development of an Argument Workbench, which supports an analyst in reconstructing arguments from across textual sources. The workbench takes a semi-automated, interactive approach searching in a corpus for fine-grained argument elements, which are concepts and conceptual patterns in expressions that are associated with argumentation schemes. The expressions can then be extracted from a corpus and reconstituted into instantiated argumentation schemes for and against a given conclusion. Such arguments can then be input to an argument evaluation tool.},
author = {Wyner, Adam},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Mining fine-grained argument elements}},
url = {http://ceur-ws.org/Vol-1341/paper12.pdf},
volume = {1341},
year = {2014}
}
@article{Schlichtkrull2017,
abstract = {Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.},
archivePrefix = {arXiv},
arxivId = {1703.06103},
author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
eprint = {1703.06103},
file = {::},
title = {{Modeling Relational Data with Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1703.06103},
year = {2017}
}
@article{Wei2017,
abstract = {For argumentation mining, there are several sub-tasks such as argumentation component type classification, relation classification. Existing research tends to solve such sub-tasks separately, but ignore the close relation between them. In this paper, we present a joint framework incorporating logical relation between sub-tasks to improve the performance of argumentation structure generation. We design an objective function to combine the predictions from individual models for each sub-task and solve the problem with some constraints constructed from background knowledge. We evaluate our proposed model on two public corpora and the experiment results show that our model can outperform the baseline that uses a separate model significantly for each sub-task. Our model also shows advantages on component-related sub-tasks compared to a state-of-the-art joint model based on the evidence graph.},
archivePrefix = {arXiv},
arxivId = {1701.05343},
author = {Wei, Zhongyu and Li, Chen and Liu, Yang},
eprint = {1701.05343},
file = {::},
title = {{A Joint Framework for Argumentative Text Analysis Incorporating Domain Knowledge}},
url = {http://arxiv.org/abs/1701.05343},
year = {2017}
}

@article{Bach2011,
abstract = {This paper presents a new task, learning logical structures of paragraphs in legal ar-ticles, which is studied in research on Le-gal Engineering (Katayama, 2007). The goals of this task are recognizing logi-cal parts of law sentences in a paragraph, and then grouping related logical parts into some logical structures of formulas, which describe logical relations between logical parts. We present a two-phase framework to learn logical structures of paragraphs in legal articles. In the first phase, we model the problem of recog-nizing logical parts in law sentences as a multi-layer sequence learning problem, and present a CRF-based model to recog-nize them. In the second phase, we pro-pose a graph-based method to group logi-cal parts into logical structures. We con-sider the problem of finding a subset of complete sub-graphs in a weighted-edge complete graph, where each node corre-sponds to a logical part, and a complete sub-graph corresponds to a logical struc-ture. We also present an integer linear pro-gramming formulation for this optimiza-tion problem. Our models achieve 74.37{\%} in recognizing logical parts, 79.59{\%} in recognizing logical structures, and 55.73{\%} in the whole task on the Japanese National Pension Law corpus.},
author = {Bach, Ngo Xuan and {Le Minh}, Nguyen and Oanh, Tran Thi and Shimazu, Akira},
doi = {10.1145/2425327.2425330},
file = {::},
issn = {15300226},
pages = {20--28},
title = {{Learning Logical Structures of Paragraphs in Legal Articles}},
url = {http://www.aclweb.org/anthology/I11-1003},
year = {2011}
}
@article{Teufel2014,
abstract = {We describe the task of intention-based text understanding for scientific argumentation. The model of scientific argumentation presented here is based on the recognition of 28 concrete rhetorical moves in text. These moves can in turn be associated with higherlevel intentions. The intentions we aim to model operate in the limited domain of scientific argumentation and justification; it is the limitation of the domain which makes our intentions predictable and enumerable, unlike general intentions. We explain how rhetorical moves relate to higher-level intentions. We also discuss work in progress towards a corpus annotated with limited-domain intentions, and speculate about the design of an automatic recognition system, for which many components already exist today},
author = {Teufel, Simone},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Scientific argumentation detection as limited-domain intention recognition}},
url = {http://ceur-ws.org/Vol-1341/paper14.pdf},
volume = {1341},
year = {2014}
}
@article{Palmer2014,
abstract = {In this paper we explore the relationship between the genre of a text and the types of situations introduced by the clauses of the text, working from the perspective of the theory of discourse modes (Smith, 2003). The typology of situation types distinguishes between, for example, events, states, generic statements, and speech acts. We analyze texts of different genres from two English text corpora, the Penn Discourse TreeBank (PDTB) and the Manually Annotated SubCorpus (MASC) of the Open American National Corpus. Texts of different types \UTF{2013} genres in the PDTB and subcorpora in MASC \UTF{2013} are segmented into clauses, and each clause is labeled with the type of situation it introduces to the discourse. We then compare the distribution of situation types across different text types, finding systematic differences across genres. Our findings support predictions of the discourse modes theory and offer new insights into the relationship between text types and situation type distributions.},
author = {Palmer, Alexis and Friedrich, Annemarie},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Genre distinctions and discourse modes: Text types differ in their situation type distributions}},
url = {http://ceur-ws.org/Vol-1341/paper13.pdf},
volume = {1341},
year = {2014}
}
@article{McDonald2005,
abstract = {We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.},
author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Haji{\v{c}}, Jan},
doi = {10.3115/1220575.1220641},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/McDonald et al. - 2005 - Non-projective dependency parsing using spanning tree algorithms.pdf:pdf},
journal = {Proceedings of EMNLP},
pages = {523--530},
title = {{Non-projective dependency parsing using spanning tree algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=1220575.1220641},
year = {2005}
}
@inproceedings{Hasan2014,
abstract = {Recent years have seen a surge of interest in stance classification in online debates. Oftentimes, however, it is important to determine not only the stance expressed by an author in her debate posts, but also the reasons behind her supporting or opposing the issue under debate. We therefore examine the new task of reason classification in this paper. Given the close interplay between stance classification and reason classification, we design computational models for examining how automatically computed stance information can be profitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts.},
author = {Hasan, Kazi Saidul and Ng, Vincent},
booktitle = {Proceedings of EMNLP},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Hasan, Ng - 2014 - Why are You Taking this Stance Identifying and Classifying Reasons in Ideological Debates.pdf:pdf},
isbn = {9781937284961},
pages = {751--762},
title = {{Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates}},
url = {http://www.aclweb.org/old{\_}anthology/D/D14/D14-1083.pdf},
year = {2014}
}
@article{Habernal2014a,
archivePrefix = {arXiv},
arxivId = {1707.06002},
author = {Habernal, Ivan and Hannemann, Raffael and Pollak, Christian and Klamm, Christopher and Pauli, Patrick and Gurevych, Iryna},
eprint = {1707.06002},
file = {::},
title = {{Argotario : Computational Argumentation Meets Serious Games}},
year = {2014}
}
@article{Bar-Haim2017a,
author = {Bar-Haim, Roy and Bhattacharya, Indrajit and Dinuzzo, Francesco and Saha, Amrita and Slonim, Noam},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Bar-Haim et al. - 2017 - Stance Classification of Context-Dependent Claims.pdf:pdf},
isbn = {9781510838604},
journal = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
pages = {251--261},
title = {{Stance Classification of Context-Dependent Claims}},
url = {http://www.aclweb.org/anthology/E17-1024},
volume = {1},
year = {2017}
}
@article{Madnani2012,
abstract = {Argumentative discourse contains not only language expressing claims and evidence, but also language used to organize these claims and pieces of evidence. Differentiating be- tween the two may be useful for many appli- cations, such as those that focus on the content (e.g., relation extraction) of arguments and those that focus on the structure of arguments (e.g., automated essay scoring). We propose an automated approach to detecting high-level organizational elements in argumentative dis- course that combines a rule-based system and a probabilistic sequence model in a principled manner. We present quantitative results on a dataset of human-annotated persuasive essays, and qualitative analyses of performance on es- says and on political debates.},
author = {Madnani, Nitin and Heilman, Michael and Tetreault, Joel and Chodorow, Martin},
file = {::},
isbn = {978-1-937284-20-6},
journal = {Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
pages = {20--28},
title = {{Identifying High-Level Organizational Elements in Argumentative Discourse}},
url = {http://www.aclweb.org/anthology/N12-1003},
year = {2012}
}
@article{Bilu2015,
author = {Bilu, Yonatan and Hershcovich, Daniel and Slonim, Noam},
file = {::},
isbn = {9781941643457},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
pages = {84--93},
title = {{Automatic Claim Negation: Why, How and When}},
url = {http://www.aclweb.org/anthology/W15-0511},
year = {2015}
}
@article{Walker2014,
author = {Walker, Vern R and Vazirova, Karina and Sanford, Cass},
file = {::},
journal = {Proceedings of the First Workshop on Argumentation Mining},
number = {2005},
pages = {1--10},
title = {{Annotating Patterns of Reasoning about Medical Theories of Causation in Vaccine Cases: Toward a Type System for Arguments}},
url = {http://aclweb.org/anthology/W/W14/W14-2101.pdf},
year = {2014}
}
@article{Wyner2015,
abstract = {The paper discusses the architecture and de- velopment of an Argument Workbench, which is a interactive, integrated, modular tool set to extract, reconstruct, and visualise arguments. We consider a corpora with dispersed infor- mation across texts, making it essential to con- ceptually search for argument elements, top- ics, and terminology. The Argument Work- bench is a processing cascade, developed in collaboration with DebateGraph. The tool supports an argument engineer to reconstruct arguments from textual sources, using infor- mation processed at one stage as input to a subsequent stage of analysis, and then build- ing an argument graph. We harvest and pre- process comments; highlight argument indi- cators, speech act and epistemic terminology; model topics; and identify domain terminol- ogy. We use conceptual semantic search over the corpus to extract sentences relative to argu- ment and domain terminology. The argument engineer uses the extracts for the construction of arguments in DebateGraph.},
author = {Wyner, Adam and Peters, Wim and Price, David},
file = {::},
journal = {Proceedings of the 2nd Workshop on Argumentation Mining},
number = {2012},
pages = {78--83},
title = {{Argument Discovery and Extraction with the Argument Workbench}},
url = {http://aclweb.org/anthology/W/W15/W15-0510.pdf},
year = {2015}
}
@article{Wachsmuth2017a,
author = {Wachsmuth, Henning and Naderi, Nona and Hou, Yufang and Bilu, Yonatan and Prabhakaran, Vinodkumar and Thijmm, Tim Alberdingk and Hirst, Graeme and Stein, Benno},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Wachsmuth et al. - 2017 - Computational Argumentation Quality Assessment in Natural Language(2).pdf:pdf},
journal = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL'17)},
pages = {176--187},
title = {{Computational Argumentation Quality Assessment in Natural Language}},
volume = {1},
year = {2017}
}
@article{Technology,
author = {Technology, Argument and Academy, Polish},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Technology, Academy - Unknown - Part II Mining arguments from dialogical context using Inference Anchoring Theory.pdf:pdf},
title = {{Part II: Mining arguments from dialogical context using Inference Anchoring Theory}}
}
@article{Stab2016a,
abstract = {In this paper, we propose a new task for assessing the quality of natural language arguments. The premises of a well-reasoned argument should provide enough evidence for accepting or rejecting its claim. Although this criterion, known as sufficiency, is widely adopted in argumentation theory, there are no empirical studies on its applicability to real arguments. In this work, we show that human annotators substantially agree on the sufficiency criterion and introduce a novel annotated corpus. Furthermore, we experiment with feature-rich SVMs and convolutional neural networks and achieve 84{\%} accuracy for automatically identifying insufficiently supported arguments. The final corpus as well as the annotation guideline are freely available for encouraging future research on argument quality.},
author = {Stab, Christian and Gurevych, Iryna},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Stab, Gurevych - 2016 - Recognizing Insufficiently Supported Arguments in Argumentative Essays.pdf:pdf},
isbn = {9781510838604},
journal = {Proceedings of EACL},
pages = {980--990},
title = {{Recognizing Insufficiently Supported Arguments in Argumentative Essays}},
year = {2016}
}
@article{Boltuzic2014,
abstract = {In online discussions, users often back up their stance with arguments. Their argu-ments are often vague, implicit, and poorly worded, yet they provide valuable insights into reasons underpinning users' opinions. In this paper, we make a first step towards argument-based opinion mining from on-line discussions and introduce a new task of argument recognition. We match user-created comments to a set of predefined topic-based arguments, which can be either attacked or supported in the comment. We present a manually-annotated corpus for ar-gument recognition in online discussions. We describe a supervised model based on comment-argument similarity and entail-ment features. Depending on problem for-mulation, model performance ranges from 70.5{\%} to 81.8{\%} F1-score, and decreases only marginally when applied to an unseen topic.},
author = {Boltuzic, Filip and Snajder, Jan},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Boltuzic, Snajder - 2014 - Back up your Stance Recognizing Arguments in Online Discussions.pdf:pdf},
journal = {Proceedings of the First Workshop on Argumentation Mining},
pages = {49--58},
title = {{Back up your Stance : Recognizing Arguments in Online Discussions}},
url = {http://aclweb.org/anthology/W/W14/W14-2107.pdf},
year = {2014}
}
@article{Habernal2016,
abstract = {We propose a new task in the field of computational argumentation in which we investigate qualitative properties of Web arguments, namely their convincingness. We cast the problem as relation classifica-tion, where a pair of arguments having the same stance to the same prompt is judged. We annotate a large datasets of 16k pairs of arguments over 32 topics and investi-gate whether the relation " A is more con-vincing than B " exhibits properties of total ordering; these findings are used as global constraints for cleaning the crowdsourced data. We propose two tasks: (1) predicting which argument from an argument pair is more convincing and (2) ranking all argu-ments to the topic based on their convinc-ingness. We experiment with feature-rich SVM and bidirectional LSTM and obtain 0.76-0.78 accuracy and 0.35-0.40 Spear-man's correlation in a cross-topic evalua-tion. We release the newly created corpus UKPConvArg1 and the experimental soft-ware under open licenses.},
author = {Habernal, Ivan and Gurevych, Iryna},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Habernal, Gurevych - 2016 - Which argument is more convincing Analyzing and predicting convincingness of Web arguments using bidirection.pdf:pdf},
isbn = {9781510827585},
journal = {Acl},
pages = {1589--1599},
title = {{Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM}},
year = {2016}
}
@article{Daxenberger2017,
abstract = {Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.},
archivePrefix = {arXiv},
arxivId = {1704.07203},
author = {Daxenberger, Johannes and Eger, Steffen and Habernal, Ivan and Stab, Christian and Gurevych, Iryna},
eprint = {1704.07203},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Daxenberger et al. - 2017 - What is the Essence of a Claim Cross-Domain Claim Identification.pdf:pdf},
journal = {Arxiv},
title = {{What is the Essence of a Claim? Cross-Domain Claim Identification}},
url = {http://arxiv.org/abs/1704.07203},
year = {2017}
}
@article{Zhang2015,
abstract = {Open domain targeted sentiment is the joint information extraction task that finds target mentions together with the senti- ment towards each mention from a text corpus. The task is typically modeled as a sequence labeling problem, and solved us- ing state-of-the-art labelers such as CRF. We empirically study the effect of word embeddings and automatic feature combi- nations on the task by extending a CRF baseline using neural networks, which have demonstrated large potentials for sentiment analysis. Results show that the neural model can give better results by significantly increasing the recall. In ad- dition, we propose a novel integration of neural and discrete features, which com- bines their relative advantages, leading to significantly higher results compared to both baselines},
author = {Zhang, Meishan and Zhang, Yue and Vo, Duy Tin},
doi = {10.18653/v1/D15-1073},
file = {:Users/kuribayashi/Desktop/EMNLP073.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of EMNLP},
number = {September},
pages = {612--621},
title = {{Neural Networks for Open Domain Targeted Sentiment}},
url = {http://aclweb.org/anthology/D15-1073},
year = {2015}
}
@article{Habernal2014,
abstract = {In this paper, we argue that an annotation scheme for argumentation mining is a function of the task requirements and the corpus properties. There is no one-size- fits-all argumentation theory to be applied to realistic data on the Web. In two annotation studies, we experiment with 80 German newspaper editorials from the Web and about one thousand English documents from forums, comments, and blogs. Our example topics are taken from the educational domain. To formalize the problem of annotating arguments, in the first case, we apply a Claim-Premise scheme, and in the second case, we modify Toulmin's scheme. We find that the choice of the argument components to be annotated strongly depends on the register, the length of the document, and inherently on the literary devices and structures used for expressing argumentation. We hope that these findings will facilitate the creation of reliably annotated argumentation corpora for a wide range of tasks and corpus types and will help to bridge the gap between argumentation theories and actual application needs},
author = {Habernal, Ivan and Eckle-Kohler, Judith and Gurevych, Iryna},
file = {:Users/kuribayashi/Documents/io-lab/AM/essay/harbenal2014.pdf:pdf},
issn = {16130073},
journal = {Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing},
pages = {26--39},
title = {{Argumentation Mining on the Web from Information Seeking Perspective}},
url = {http://ceur-ws.org/Vol-1341/},
year = {2014}
}
@article{Wachsmuth2017,
author = {Wachsmuth, Henning and Naderi, Nona and Hou, Yufang and Bilu, Yonatan and Prabhakaran, Vinodkumar and Thijmm, Tim Alberdingk and Hirst, Graeme and Stein, Benno},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Wachsmuth et al. - 2017 - Computational Argumentation Quality Assessment in Natural Language.pdf:pdf},
isbn = {9781510838604},
journal = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
pages = {176--187},
title = {{Computational Argumentation Quality Assessment in Natural Language}},
url = {http://www.aclweb.org/anthology/E17-1017},
year = {2017}
}
@article{Lippi2016,
abstract = {Argumentation mining aims at automatically extracting structured arguments from unstructured textual documents. It has recently become a hot topic also due to its potential in processing information originating from the Web, and in particular from social media, in innovative ways. Recent advances in machine learning methods promise to enable breakthrough applications to social and economic sciences, policy making, and information technology: something that only a few years ago was unthinkable. In this survey article, we introduce argumentation models and methods, review existing systems and applications, and discuss challenges and perspectives of this exciting new research area.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07526v1},
author = {Lippi, Marco and Torroni, Paolo},
doi = {10.1145/2850417},
eprint = {arXiv:1502.07526v1},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Lippi, Torroni - 2016 - Argumentation Mining State of the Art and Emerging Trends.pdf:pdf},
isbn = {9781577357384},
issn = {15335399},
journal = {ACM Transactions on Internet Technology},
number = {2},
pages = {10},
pmid = {1000285845},
title = {{Argumentation Mining: State of the Art and Emerging Trends}},
url = {http://doi.acm.org/10.1145/2850417},
volume = {16},
year = {2016}
}
@article{Schneider2014,
author = {Schneider, Jodi},
file = {::},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{An informatics perspective on argumentation mining}},
volume = {1341},
year = {2014}
}
@article{Category2016,
author = {Category, Aspect and Polarity, Opinion and Expression, Opinion Target and Village, East},
file = {:Users/kuribayashi/Desktop/absa2016{\_}annotationguidelines.pdf:pdf},
pages = {1--20},
title = {{SemEval 2016 Task 5 Aspect Based Sentiment Analysis ( ABSA-16 ) Annotation Guidelines}},
year = {2016}
}
@article{Stab2016,
abstract = {In this article, we present the first end-to-end approach for parsing argumentation structures in persuasive essays. We model the argumentation structure as a tree including several types of argument components connected with argumentative support and attack relations. We consider the identification of argumentation structures in several consecutive steps. First, we segment a persuasive essay in order to identify relevant argument components. Second, we jointly model the classification of argument components and the identification of argumentative relations using Integer Linear Programming. Third, we recognize the stance of each argument component in order to discriminate between argumentative support and attack relations. By evaluating the joint model using two corpora, we show that our approach not only considerably improves the identification of argument component types and argumentative relations but also significantly outperforms a challenging heuristic baseline. In addition, we introduce a novel corpus including 402 persuasive essays annotated with argumentation structures and show that our new annotation guideline successfully guides annotators to substantial agreement.},
year = {2016},
volume = {43},
number = {3},
archivePrefix = {arXiv},
arxivId = {1604.07370},
author = {Stab, Christian and Gurevych, Iryna},
eprint = {1604.07370},
file = {:Users/kuribayashi/Library/Application Support/Mendeley Desktop/Downloaded/Stab, Gurevych - 2016 - Parsing Argumentation Structures in Persuasive Essays.pdf:pdf},
journal = {arXiv preprint, under review},
pages = {619--659},
title = {{Parsing Argumentation Structures in Persuasive Essays}},
url = {http://arxiv.org/abs/1604.07370},

}

@article{Mitchell2013,
abstract = {Abstract$\backslash$r$\backslash$nWe propose a novel approach to sentiment$\backslash$r$\backslash$nanalysis for a low resource setting. The intuition$\backslash$r$\backslash$nbehind this work is that sentiment$\backslash$r$\backslash$nexpressed towards an entity, targeted sentiment,$\backslash$r$\backslash$nmay be viewed as a span of sentiment$\backslash$r$\backslash$nexpressed across the entity. This representation$\backslash$r$\backslash$nallows us to model sentiment detection$\backslash$r$\backslash$nas a sequence tagging problem, jointly$\backslash$r$\backslash$ndiscovering people and organizations along$\backslash$r$\backslash$nwith whether there is sentiment directed towards$\backslash$r$\backslash$nthem. We compare performance in$\backslash$r$\backslash$nboth Spanish and English on microblog data,$\backslash$r$\backslash$nusing only a sentiment lexicon as an external$\backslash$r$\backslash$nresource. By leveraging linguisticallyinformed$\backslash$r$\backslash$nfeatures within conditional random$\backslash$r$\backslash$nfields (CRFs) trained to minimize empirical$\backslash$r$\backslash$nrisk, our best models in Spanish signifi-$\backslash$r$\backslash$ncantly outperform a strong baseline, and reach$\backslash$r$\backslash$naround 90{\%} accuracy on the combined task of$\backslash$r$\backslash$nnamed entity recognition and sentiment prediction.$\backslash$r$\backslash$nOur models in English, trained on a$\backslash$r$\backslash$nmuch smaller dataset, are not yet statistically$\backslash$r$\backslash$nsignificant against their baselines.},
author = {Mitchell, Margaret and Aguilar, Jacqueline and Wilson, Theresa and {Van Durme}, Benjamin},
file = {:Users/kuribayashi/Desktop/open-domain-targeted-sentiment-emnlp-2013.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of EMNLP},
number = {October},
pages = {1643--1654},
title = {{Open Domain Targeted Sentiment}},
year = {2013}
}
@article{Rinott2015,
abstract = {Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life. A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion/claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects). In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim. This task has many practical applications in decision support and persuasion enhancement in a wide range of domains. We first introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance. Then, we suggest a system architecture based on supervised learning to address the evidence detection task. Finally, promising experimental results are reported},
author = {Rinott, Ruty and Dankin, Lena and Alzate, Carlos and Khapra, Mitesh M and Aharoni, Ehud and Slonim, Noam},
doi = {10.18653/v1/D15-1050},
file = {:Users/kuribayashi/Documents/io-lab/AM/essay/Evidence2015.pdf:pdf},
isbn = {9781941643327},
journal = {Emnlp},
number = {September},
pages = {440--450},
title = {{Show Me Your Evidence \UTF{2013} an Automatic Method for Context Dependent Evidence Detection}},
year = {2015}
}
@article{Lin2009,
abstract = {Sentiment analysis or opinion mining aims to use automated tools to detect subjective information such as opinions, at- titudes, and feelings expressed in text. This paper pro- poses a novel probabilistic modeling framework based on La- tentDirichlet Allocation (LDA), called joint sentiment/topic model (JST), which detects sentiment and topic simultane- ously from text. Unlike other machine learning approaches to sentiment classification which often require labeled cor- pora for classifier training, the proposed JST model is fully unsupervised. The model has been evaluated on the movie review dataset to classify the review sentiment polarity and minimum prior information have also been explored to fur- ther improve the sentiment classification accuracy. Prelimi- nary experiments have shown promising results achieved by JST.},
author = {Lin, Chenghua and Road, North Park and Ex, Exeter},
doi = {10.1145/1645953.1646003},
file = {:Users/kuribayashi/Desktop/Joint sentiment topic model for sentiment analysis.pdf:pdf},
isbn = {9781605585123},
issn = {9781605585123},
journal = {Cikm},
keywords = {cation,joint sentiment,latent dirichlet allo-,opinion mining,sentiment analysis,topic model},
pages = {375--384},
title = {{Joint Sentiment / Topic Model for Sentiment Analysis}},
year = {2009}
}
@incollection{Vinyals2015,
title = {Pointer Networks},
author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
booktitle = {NIPS},
pages = {2692--2700},
year = {2015},
url = {http://papers.nips.cc/paper/5866-pointer-networks.pdf}
}
@article{Hochreiter1997,
author = { Sepp Hochreiter  and  J\"{u}rgen Schmidhuber },
title = {Long Short-Term Memory},
journal = {Neural Computation},
volume = {9},
number = {8},
pages = {1735-1780},
year = {1997},

URL = { 
        https://doi.org/10.1162/neco.1997.9.8.1735
    
},
eprint = { 
        https://doi.org/10.1162/neco.1997.9.8.1735
    
}
,
    abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. },
}

@inproceedings{wachsmuthusing2016,
  title={Using Argument Mining to Assess the Argumentation Quality of Essays},
  author={Wachsmuth, Henning and Al-Khatib, Khalid and Stein, Benno},
  booktitle={Proceedings of COLING},
  pages={1680--1691},
  year={2016},
}

@article{Wei2017,
author = {Wei, Zhongyu and Li, Chen and Liu, Yang},
year = {2017},
month = {01},
pages = {},
title = {A Joint Framework for Argumentative Text Analysis Incorporating Domain Knowledge}
}

% Span Parsing %
@inproceedings{wang:16,
  title={Graph-based dependency parsing with bidirectional lstm},
  author={Wang, Wenhui and Chang, Baobao},
  booktitle={Proceedings of ACL},
  pages={2306--2315},
  year={2016}
}

@InProceedings{stern:17,
  author    = {Stern, Mitchell  and  Andreas, Jacob  and  Klein, Dan},
  title     = {A Minimal Span-Based Neural Constituency Parser},
  booktitle = {Proceedings of ACL},
  year      = {2017},
  pages     = {818--827},
}

@InProceedings{teranishi:17,
  author    = {Teranishi, Hiroki  and  Shindo, Hiroyuki  and  Matsumoto, Yuji},
  title     = {Coordination Boundary Identification with Similarity and Replaceability},
  booktitle = {Proceedings of IJCNLP},
  year      = {2017},
  pages     = {264--272},
}

@InProceedings{kitaev:18,
  author = 	"Kitaev, Nikita
		and Klein, Dan",
  title = 	"Constituency Parsing with a Self-Attentive Encoder",
  booktitle = 	"Proceedings of ACL",
  year = 	"2018",
  pages = 	"2676--2686",
}


% Span Coreference Resolution %
@InProceedings{lee:18,
  author = 	"Lee, Kenton
		and He, Luheng
		and Zettlemoyer, Luke",
  title = 	"Higher-Order Coreference Resolution with Coarse-to-Fine Inference",
  booktitle = 	"Proceedings of NAACL",
  year = 	"2018",
  pages = 	"687--692",
}


% Word Representation % 
@inproceedings{pennington:14,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Proceedings of EMNLP},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
}

@InProceedings{peters:18,
  author = 	"Peters, Matthew
		and Neumann, Mark
		and Iyyer, Mohit
		and Gardner, Matt
		and Clark, Christopher
		and Lee, Kenton
		and Zettlemoyer, Luke",
  title = 	"Deep Contextualized Word Representations",
  booktitle = 	"Proceedings of the NAACL-HLT 2018",
  year = 	"2018",
  pages = 	"2227--2237",
}

% NN Settings %
@misc{kingma:14,
  author    = {Kingma, D.P.  and  Ba, J.},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2014},
  howpublished = {\textit{arXiv preprint arXiv: 1412.6980}}
}

@inproceedings{glorot:10,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of AISTATS},
  pages={249--256},
  year={2010}
}

@misc{saxe:13,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  howpublished={\textit{arXiv preprint arXiv:1312.6120}},
  year={2013}
}

@InProceedings{collobert:11,
  author    = {Collobert, Ronan  and  Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  title     = {Natural Language Processing (Almost) from Scratch},
  booktitle = {Journal of Machine Learning Research},
  year      = {2011},
}

@article{srivastava:14,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
}


% LSTM %
@article{schuster:97,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE Transactions on Signal Processing},
  pages={2673--2681},
  year={1997},
}

@inproceedings{graves:05,
  title={Bidirectional {LSTM} networks for improved phoneme classification and recognition},
  author={Graves, Alex and Fern{\'a}ndez, Santiago and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of International Conference on Artificial Neural Networks},
  pages={799--804},
  year={2005},
}

@InProceedings{graves:13,
  author    = {Graves, Alan and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  title     = {Hybrid speech recognition with deep bidirectional {LSTM}},
  booktitle = {Proceedings of Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop},
  year      = {2013},
}

@InProceedings{Eger2017,
  author = 	"Eger, Steffen
		and Daxenberger, Johannes
		and Gurevych, Iryna",
  title = 	"Neural End-to-End Learning for Computational Argumentation Mining",
  publisher = "Association for Computational Linguistics",
  booktitle = 	"Proceedings of ACL",
  year = 	"2017",
  pages = 	"11--22",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-1002",
  url = 	"http://www.aclweb.org/anthology/P17-1002"
}
@inproceedings{Zhang2016,
  title={Using context to predict the purpose of argumentative writing revisions},
  author={Zhang, Fan and Litman, Diane},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1424--1430},
  year={2016}
}
@inproceedings{Palau2009,
  title={Argumentation mining: the detection, classification and structure of arguments in text},
  author={Palau, Raquel Mochales and Moens, Marie-Francine},
  booktitle={Proceedings of the 12th international conference on artificial intelligence and law},
  pages={98--107},
  year={2009},
  organization={ACM}
}
@inproceedings{Moens2007b,
  title={Automatic detection of arguments in legal texts},
  author={Moens, Marie-Francine and Boiy, Erik and Palau, Raquel Mochales and Reed, Chris},
  booktitle={Proceedings of the 11th international conference on Artificial intelligence and law},
  pages={225--230},
  year={2007},
  organization={ACM}
}
@inproceedings{Persing2015,
  title={Modeling argument strength in student essays},
  author={Persing, Isaac and Ng, Vincent},
  booktitle={Proceedings of ACL-IJCNLP)},
  volume={1},
  pages={543--552},
  year={2015}
}

@article{schuster:97,
  added-at = {2018-11-26T22:32:26.000+0100},
  author = {Schuster, Mike and Paliwal, Kuldip K.},
  biburl = {https://www.bibsonomy.org/bibtex/26026f083db110838a6b62c2eedfec9e9/habereder},
  citeulike-article-id = {2382095},
  interhash = {f38e944d43d0b3e008971c0a50c41d58},
  intrahash = {6026f083db110838a6b62c2eedfec9e9},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {final thema:seqtoseq},
  month = {November},
  pages = {2673--2681},
  priority = {2},
  timestamp = {2018-11-26T22:32:26.000+0100},
  title = {Bidirectional recurrent neural networks},
  volume = 45,
  year = 1997
}

@article{graves:05,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Graves, A. and Schmidhuber, J.},
  biburl = {https://www.bibsonomy.org/bibtex/28310590dd68887ad625378b0d094d293/schaul},
  citeulike-article-id = {2382119},
  description = {idsia},
  interhash = {9b4a3769df7574842de82665de117e9f},
  intrahash = {8310590dd68887ad625378b0d094d293},
  journal = {Neural Networks},
  keywords = {juergen},
  number = {5-6},
  pages = {602--610},
  priority = {2},
  timestamp = {2008-02-26T11:59:11.000+0100},
  title = {Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures},
  volume = 18,
  year = 2005
}

@article{graves:13,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Graves, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/2d3697b979a78e10841dcc7eaa1998466/dblp},
  ee = {http://arxiv.org/abs/1308.0850},
  interhash = {ec0258df377a752ce87a6f7c59dea06d},
  intrahash = {d3697b979a78e10841dcc7eaa1998466},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T15:13:07.000+0200},
  title = {Generating Sequences With Recurrent Neural Networks.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1308.html#Graves13},
  volume = {abs/1308.0850},
  year = 2013
}
@InProceedings{W04-3250,
  author = 	"Koehn, Philipp ",
  title = 	"Statistical Significance Tests for Machine Translation Evaluation",
  booktitle = 	"Proceedings of EMNLP ",
  pages = {388--395},
  year = 	"2004",
  url = 	"http://aclweb.org/anthology/W04-3250"
}
@inproceedings{Aker2017,
  title={What works and what does not: Classifier and feature analysis for argument mining},
  author={Aker, Ahmet and Sliwa, Alfred and Ma, Yuan and Lui, Ruishen and Borad, Niravkumar and Ziyaei, Seyedeh and Ghobadi, Mina},
  booktitle={Proceedings of the 4th Workshop on Argument Mining},
  pages={91--96},
  year={2017}
}
@inproceedings{Ghosh2016,
  title={Coarse-grained argumentation features for scoring persuasive essays},
  author={Ghosh, Debanjan and Khanam, Aquila and Han, Yubo and Muresan, Smaranda},
  booktitle={Proceedings of ACL},
  volume={2},
  pages={549--554},
  year={2016}
}
@article{Prasad2007,
  title={The penn discourse treebank 2.0 annotation manual},
  author={Prasad, Rashmi and Miltsakaki, Eleni and Dinesh, Nikhil and Lee, Alan and Joshi, Aravind and Robaldo, Livio and Webber, Bonnie L},
  year={2007}
}

@inproceedings{braud2016learning,
  title={Learning connective-based word representations for implicit discourse relation identification},
  author={Braud, Chlo{\'e} and Denis, Pascal},
  pages={203--213},
  booktitle={Proceedings of EMNLP},
  year={2016}
}

@inproceedings{braud-denis-2016-learning,
    title = "Learning Connective-based Word Representations for Implicit Discourse Relation Identification",
    author = "Braud, Chlo{\'e}  and
      Denis, Pascal",
    booktitle = "Proceedings of EMNLP",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1020",
    doi = "10.18653/v1/D16-1020",
    pages = "203--213",
}

@inproceedings{lawrence2015combining,
  title={Combining argument mining techniques},
  author={Lawrence, John and Reed, Chris},
  booktitle={Proceedings of the 2nd Workshop on Argumentation Mining},
  pages={127--136},
  year={2015}
}
@book{marcu2000theory,
  title={The theory and practice of discourse parsing and summarization},
  author={Marcu, Daniel},
  year={2000},
  publisher={MIT press}
}
@inproceedings{marcu2002unsupervised,
  title={An unsupervised approach to recognizing discourse relations},
  author={Marcu, Daniel and Echihabi, Abdessamad},
  booktitle={Proceedings of ACL},
  year={2002}
}
@inproceedings{stab2018,
  title={Cross-topic argument mining from heterogeneous sources},
  author={Stab, Christian and Miller, Tristan and Schiller, Benjamin and Rai, Pranav and Gurevych, Iryna},
  booktitle={Proceedings of EMNLP},
  pages={3664--3674},
  year={2018}
}

@inproceedings{Trautmann2019,
Author = {Dietrich Trautmann and Johannes Daxenberger and Christian Stab and Hinrich Schütze and Iryna Gurevych},
Title = {Fine-Grained Argument Unit Recognition and Classification},
booktitle={Proceedings of AAAI},
pages={9048-9056},
year={2020}
}
@inproceedings{sileo2019mining,
  author = 	"Sileo, Damien and Van-De-Cruys, Tim  and Pradel, Camille and Muller, Philippe",
  title = 	"Mining Discourse Markers for Unsupervised Sentence Representation Learning",
  booktitle = 	"Proceedings of the NAACL-HLT 2019",
  year = 	"2019",
  location = 	"Minneapolis, Minnesota",
  pages={3477--3486},
}
@inproceedings{pan2018discourse,
  title={Discourse marker augmented network with reinforcement learning for natural language inference},
  author={Pan, Boyuan and Yang, Yazheng and Zhao, Zhou and Zhuang, Yueting and Cai, Deng and He, Xiaofei},
  booktitle={Proceedings of ACL},
  pages={989--999},
  year={2018}
}
@book{freeman2011argument,
  title={Argument Structure:: Representation and Theory},
  author={Freeman, James B},
  volume={18},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@InProceedings{Pitler09,
  author    = {Pitler, Emily  and  Louis, Annie  and  Nenkova, Ani},
  title     = {Automatic sense prediction for implicit discourse relations in text},
  booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  month     = {August},
  year      = {2009},
  address   = {Suntec, Singapore},
  publisher = {Association for Computational Linguistics},
  pages     = {683--691},
  url       = {http://www.aclweb.org/anthology/P/P09/P09-1077}
}

@InProceedings{Lin09,
  author = 	"Lin, Ziheng
		and Kan, Min-Yen
		and Ng, Hwee Tou",
  title = 	"Recognizing Implicit Discourse Relations in the Penn Discourse Treebank",
  booktitle = 	"Proceedings of EMNLP",
  year = 	"2009",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"343--351",
  location = 	"Singapore",
  url = 	"http://www.aclweb.org/anthology/D09-1036"
}

@inproceedings{Chen16,
author =        {Chen, Jifan and Zhang, Qi and Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
title =         {Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network},
booktitle =     {Proceedings of ACL},
year =  {2016},
pages =         {1726--1735}
}

@article{Ji2015,
author = {Ji, Yangfeng and Eisenstein, Jacob},
journal = {TACL},
pages = {329--344},
title = {One Vector is Not Enough : Entity-Augmented Distributed Semantics for Discourse Relations},
year = {2015}
}

@book{kennedy2014introduction,
  title={An introduction to corpus linguistics},
  author={Kennedy, Graeme},
  year={2014},
  publisher={Routledge}
}
@ARTICLE{Brysbaert2018-ll,
  title    = "Corpus linguistics",
  author   = "Brysbaert, Marc and Mandera, Pawe{\l} and Keuleers, Emmanuel",
  abstract = "Psycholinguists are mostly familiar with corpus linguistics
              because the word frequency norms they use come from corpus
              linguistics. The measures provided by corpus linguistics are the
              most powerful when they can be combined with processing times for
              large numbers of words (obtained in megastudies) and subjective
              ratings for many words (obtained via crowdsourcing studies). The
              underlying assumptions of corpus linguistics differ slightly
              between studies depending on whether a researcher is interested
              in language production or language perception. The apparatus for
              corpus linguistics is becoming simple, as a result of the growing
              power of computers. The most frequently used measure derived from
              corpus linguistics is word frequency. Most of the time, a corpus
              will be downloaded from the internet. Indeed, the massive
              availability of language in digital form has been the driving
              force behind corpus linguistics. There are two ways to show the
              utility of the various measures provided by computational
              linguistics: either by setting up a new study that addresses a
              specific theoretical question or by reanalyzing an old study.
              (PsycInfo Database Record (c) 2020 APA, all rights reserved)",
  journal  = "Research methods in psycholinguistics and the neurobiology of
              language: A practical guide.",
  volume   =  371,
  pages    = "230--246",
  year     =  2018
}
@book{jurafsky2000speech,
  title={Speech \& language processing},
  author={Jurafsky, Dan},
  year={2000},
  publisher={Pearson Education India}
}

@article{Baroni2021-az,
  title         = "On the proper role of linguistically-oriented deep net
                   analysis in linguistic theorizing",
  author        = "Baroni, Marco",
  abstract      = "A lively research field has recently emerged that uses
                   experimental methods to probe the linguistic behavior of
                   modern deep networks. While work in this tradition often
                   reports intriguing results about the grammatical skills of
                   deep nets, it is not clear what their implications for
                   linguistic theorizing should be. As a consequence,
                   linguistically-oriented deep net analysis has had very
                   little impact on linguistics at large. In this chapter, I
                   suggest that deep networks should be treated as theories
                   making explicit predictions about the acceptability of
                   linguistic utterances. I argue that, if we overcome some
                   obstacles standing in the way of seriously pursuing this
                   idea, we will gain a powerful new theoretical tool,
                   complementary to mainstream algebraic approaches.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2106.08694"
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={NIPS},
  pages={3104--3112},
  year={2014}
}
@article{goldberg2017neural,
  title={Neural network methods for natural language processing},
  author={Goldberg, Yoav},
  journal={Synthesis lectures on human language technologies},
  volume={10},
  number={1},
  pages={1--309},
  year={2017},
  publisher={Morgan \& Claypool Publishers}
}

@ARTICLE{Linzen2020-kd,
  title         = "How Can We Accelerate Progress Towards Human-like Linguistic
                   Generalization?",
  author        = "Linzen, Tal",
  abstract      = "This position paper describes and critiques the
                   Pretraining-Agnostic Identically Distributed (PAID)
                   evaluation paradigm, which has become a central tool for
                   measuring progress in natural language understanding. This
                   paradigm consists of three stages: (1) pre-training of a
                   word prediction model on a corpus of arbitrary size; (2)
                   fine-tuning (transfer learning) on a training set
                   representing a classification task; (3) evaluation on a test
                   set drawn from the same distribution as that training set.
                   This paradigm favors simple, low-bias architectures, which,
                   first, can be scaled to process vast amounts of data, and
                   second, can capture the fine-grained statistical properties
                   of a particular data set, regardless of whether those
                   properties are likely to generalize to examples of the task
                   outside the data set. This contrasts with humans, who learn
                   language from several orders of magnitude less data than the
                   systems favored by this evaluation paradigm, and generalize
                   to new tasks in a consistent way. We advocate for
                   supplementing or replacing PAID with paradigms that reward
                   architectures that generalize as quickly and robustly as
                   humans.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.00955"
}

@article{Dupoux2016-ia,
  title={Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner},
  author={Dupoux, Emmanuel},
  journal={Cognition},
  volume={173},
  pages={43--59},
  year={2018},
  publisher={Elsevier}
}

@article{Crocker2010-cp,
  title={Computational Psycholinguistics},
  author={Crocker, Matthew W},
  journal={The Handbook of Computational Linguistics and Natural Language Processing},
  year={2007},
  url={https://www.coli.uni-saarland.de/~crocker/documents/crocker-nlp-handbook.pdf}
}

@ARTICLE{Linzen2019-vt,
  title   = "{What can linguistics and deep learning contribute to each other?}",
  author  = "Linzen, Tal",
  journal = "J. Lang. Soc. Psychol.",
  volume  =  95,
  number  =  1,
  pages   = "99--108",
  year    =  2019
}
@ARTICLE{Hovy2010-gg,
  title     = "Towards a `science'of corpus annotation: a new methodological
               challenge for corpus linguistics",
  author    = "Hovy, Eduard and Lavid, Julia",
  abstract  = "… over the past decade in the Computational Linguistics (CL) /
               Natural Language Processing (NLP) communities … While it seems
               natural to implement pull-down menus, annotation proceeds much …
               USA (LDC: www.ldc.upenn.edu) and European Language Resources
               Association …",
  journal   = "International journal of translation",
  publisher = "cs.cmu.edu",
  volume    =  22,
  number    =  1,
  pages     = "13--36",
  year      =  2010
}
@book{marr1982,
author = {Marr, David},
title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
year = {1982},
isbn = {0716715678},
publisher = {Henry Holt and Co., Inc.},
address = {USA},
url ={https://academic.oup.com/mit-press-scholarship-online/book/13528?sid=oup:oxfordacademic&genre=book&aulast=Marr&aufirst=David&title=Vision%3A+A+Computational+Investigation+into+the+Human+Representation+and+Processing+of+Visual+Information&date=2010-07-09}
}

@ARTICLE{Keller2010,
  title     = "Cognitively Plausible Models of Human Language Processing",
  author    = "Keller, Frank",
  publisher = "Association for Computational Linguistics",
  pages     = "60--67",
  month     =  jul,
  year      =  2010,
  address   = "Uppsala, Sweden"
}
@book{mcclelland1986parallel,
  title={Parallel distributed processing},
  author={McClelland, James L and Rumelhart, David E and PDP Research Group and others},
  volume={2},
  year={1986},
  publisher={MIT press Cambridge, MA}
}

@ARTICLE{Hopfield1982-hl,
  title     = "Neural networks and physical systems with emergent collective
               computational abilities",
  author    = "Hopfield, J J",
  abstract  = "Computational properties of use of biological organisms or to
               the construction of computers can emerge as collective
               properties of systems having a large number of simple equivalent
               components (or neurons). The physical meaning of
               content-addressable memory is described by an appropriate phase
               space flow of the state of a system. A model of such a system is
               given, based on aspects of neurobiology but readily adapted to
               integrated circuits. The collective properties of this model
               produce a content-addressable memory which correctly yields an
               entire memory from any subpart of sufficient size. The algorithm
               for the time evolution of the state of the system is based on
               asynchronous parallel processing. Additional emergent collective
               properties include some capacity for generalization, familiarity
               recognition, categorization, error correction, and time sequence
               retention. The collective properties are only weakly sensitive
               to details of the modeling or the failure of individual devices.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  79,
  number    =  8,
  pages     = "2554--2558",
  month     =  apr,
  year      =  1982,
  language  = "en"
}

@ARTICLE{Manning2020-kl,
  title    = "Emergent linguistic structure in artificial neural networks
              trained by self-supervision",
  author   = "Manning, Christopher D and Clark, Kevin and Hewitt, John and
              Khandelwal, Urvashi and Levy, Omer",
  abstract = "This paper explores the knowledge of linguistic structure learned
              by large artificial neural networks, trained via
              self-supervision, whereby the model simply tries to predict a
              masked word in a given context. Human language communication is
              via sequences of words, but language understanding requires
              constructing rich hierarchical structures that are never observed
              explicitly. The mechanisms for this have been a prime mystery of
              human language acquisition, while engineering work has mainly
              proceeded by supervised learning on treebanks of sentences hand
              labeled for this latent structure. However, we demonstrate that
              modern deep contextual language models learn major aspects of
              this structure, without any explicit supervision. We develop
              methods for identifying linguistic hierarchical structure
              emergent in artificial neural networks and demonstrate that
              components in these models focus on syntactic grammatical
              relationships and anaphoric coreference. Indeed, we show that a
              linear transformation of learned embeddings in these models
              captures parse tree distances to a surprising degree, allowing
              approximate reconstruction of the sentence tree structures
              normally assumed by linguists. These results help explain why
              these models have brought such large improvements across many
              language-understanding tasks.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  117,
  number   =  48,
  pages    = "30046--30054",
  month    =  dec,
  year     =  2020,
  keywords = "artificial neural netwok; learning; self-supervision; syntax",
  language = "en"
}

@ARTICLE{Toneva_undated-je,
  title    = "Interpreting and improving natural-language processing (in
              machines) with natural language-processing (in the brain)",
  author   = "Toneva, Mariya and Wehbe, Leila",
  abstract = "Neural networks models for NLP are typically implemented without
              the explicit encoding of language rules and yet they are able to
              break one performance record after another. This has generated a
              lot of research interest in interpreting the representations
              learned by these",
  journal  = "papers.neurips.cc",
  year = 2019
}


@BOOK{Rumelhart1985-xj,
  title={On learning the past tenses of English verbs},
  author={Rumelhart, DE and McClelland, JL},
  booktitle={Parallel distributed processing: explorations in the microstructure of cognition, vol. 2: psychological and biological models},
  pages={216--271},
  year={1986},
  publisher={ERIC}
}

@ARTICLE{Pinker1988-zb,
  title     = "On language and connectionism: analysis of a parallel
               distributed processing model of language acquisition",
  author    = "Pinker, S and Prince, A",
  abstract  = "Does knowledge of language consist of mentally-represented
               rules? Rumelhart and McClelland have described a connectionist
               (parallel distributed processing) model of the acquisition of
               the past tense in English which successfully maps many stems
               onto their past tense forms, both regular (walk/walked) and
               irregular (go/went), and which mimics some of the errors and
               sequences of development of children. Yet the model contains no
               explicit rules, only a set of neuronstyle units which stand for
               trigrams of phonetic features of the …",
  journal   = "Cognition",
  publisher = "Elsevier",
  volume    =  28,
  number    = "1-2",
  pages     = "73--193",
  month     =  mar,
  year      =  1988,
  language  = "en"
}

@ARTICLE{Chomsky1980-vx,
  title     = "Rules and representations",
  author    = "Chomsky, Noam",
  abstract  = "The book from which these sections are excerpted (N. Chomsky,
               Rules and Representations, Columbia University Press, 1980) is
               concerned with the prospects for assimilating the study of human
               intelligence and its products to the natural sciences through
               the investigation of cognitive structures, understood as systems
               of rules and representations that can be regarded as ``mental
               organs.'' These mental structui′es serve as the vehicles for the
               exercise of various capacities. They develop in the mind on the
               basis of an innate endowment that permits the growth of rich and
               highly articulated structures along an intrinsically determined
               course under the triggering and partially shaping effect of
               experience, which fixes parameters in an intricate system of
               predetermined form. It is argued that the mind is modular in
               character, with a diversity of cognitive structures, each with
               its specific properties arid principles. Knowledge of language,
               of the behavior of objects, and much else crucially involves
               these mental structures, and is thus not characterizable in
               terms of capacities, dispositions, or practical abilities, nor
               is it necessarily grounded in experience in the standard sense
               of this term.Various types of knowledge and modes of knowledge
               acquisition are discussed in these terms. Some of the properties
               of the language faculty are investigated. The basic cognitive
               relation is ``knowing a grammar''; knowledge of language is
               derivative and, correspondingly, raises further problems.
               Language as commonly understood is not a unitary phenomenon but
               involves a number of interacting systems: the ``computational''
               system of grammar, which provides the representations of sound
               and meaning that permit a rich range of expressive potential, is
               distinct from a conceptual system with its own properties;
               knowledge of language must be distinguished from knowledge of
               how to use a language; and the various systems that enter into
               the knowledge and use of language must be further analyzed into
               their specific subcomponents.",
  journal   = "Behav. Brain Sci.",
  publisher = "Cambridge University Press",
  volume    =  3,
  number    =  1,
  pages     = "1--15",
  month     =  mar,
  year      =  1980,
  keywords  = "evolution; grammar; knowledge; language; mental structures"
}
@ARTICLE{Pullum2002-xn,
  title    = "Empirical assessment of stimulus poverty arguments",
  author   = "Pullum, Geoffrey K and Scholz, Barbara C",
  abstract = "It is widely believed that linguists have developed an argument
              for linguistic nativism called the 'argument from poverty of the
              stimulus'. Linguistic nativism is the view that human infants
              have at least some linguistically specific innate knowledge. This
              article examines a type of argument for linguistic nativism that
              takes the following form: (i) a fact about some natural language
              is exhibited that allegedly could not be learned from experience
              without access to a certain kind of (positive) data; (ii) it is
              claimed that data of the type in question are not found in normal
              linguistic experience; hence (iii) it is concluded that people
              cannot be learning the language from mere exposure to language
              use. We analyze the components of this sort of argument
              carefully, and examine four exemplars, none of which hold up. We
              conclude that linguists have some additional work to do if they
              wish to sustain their claims about having provided support for
              linguistic nativism, and we offer some reasons for thinking that
              the relevant kind of future work on this issue is likely to
              further undermine the linguistic nativist position. (PsycINFO
              Database Record (c) 2016 APA, all rights reserved)",
  journal  = "The Linguistic Review",
  volume   =  19,
  number   =  1,
  pages    = "9--50",
  year     =  2002
}
@BOOK{Kennedy2014-or,
  title     = "An Introduction to Corpus Linguistics",
  author    = "Kennedy, Graeme",
  abstract  = "The use of large, computerized bodies of text for linguistic
               analysis and description has emerged in recent years as one of
               the most significant and rapidly-developing fields of activity
               in the study of language. This book provides a comprehensive
               introduction and guide to Corpus Linguistics. All aspects of the
               field are explored, from the various types of electronic corpora
               that are available to instructions on how to design and compile
               a corpus. Graeme Kennedy surveys the development of corpora for
               use in linguistic research, looking back to the pre-electronic
               age as well as to the massive growth of computer corpora in the
               electronic age.",
  publisher = "Routledge",
  month     =  sep,
  year      =  2014,
  language  = "en"
}

@ARTICLE{McCoy_undated-px,
  title    = "Revisiting the poverty of the stimulus: hierarchical
              generalization without a hierarchical bias in recurrent neural
              networks",
  author   = "McCoy, R Thomas and Frank, Robert and Linzen, Tal",
  abstract = "Syntactic rules in natural language typically need to make
              reference to hierarchical sentence structure. However, the simple
              examples that language learners receive are often equally
              compatible with linear rules. Children consistently ignore these
              linear explanations and …",
  journal  = "cogsci.mindmodeling.org",
  year=2018
}


@ARTICLE{McCoy2020-mc,
  title    = "Does Syntax Need to Grow on Trees? Sources of Hierarchical
              Inductive Bias in {Sequence-to-Sequence} Networks",
  author   = "McCoy, R Thomas and Frank, Robert and Linzen, Tal",
  abstract = "Learners that are exposed to the same training data might
              generalize differently due to differing inductive biases. In
              neural network models, inductive biases could in theory arise
              from any aspect of the model architecture. We investigate which
              architectural factors affect the generalization behavior of
              neural sequence-to-sequence models trained on two syntactic
              tasks, English question formation and English tense reinflection.
              For both tasks, the training set is consistent with a
              generalization based on hierarchical structure and a
              generalization based on linear order. All architectural factors
              that we investigated qualitatively affected how models
              generalized, including factors with no clear connection to
              hierarchical structure. For example, LSTMs and GRUs displayed
              qualitatively different inductive biases. However, the only
              factor that consistently contributed a hierarchical bias across
              tasks was the use of a tree-structured model rather than a model
              with sequential recurrence, suggesting that human-like syntactic
              generalization requires architectural syntactic structure.",
  journal  = "TACL",
  volume   =  8,
  pages    = "125--140",
  year     =  2020
}

@ARTICLE{McCoy2020-cy,
  title     = "Universal linguistic inductive biases via meta-learning",
  author    = "McCoy, R Thomas and Grant, Erin and Smolensky, Paul and
               Griffiths, Thomas L and Linzen, Tal",
  abstract  = "How do learners acquire languages from the limited data
               available to them? This process must involve some inductive
               biases---factors that affect how a learner generalizes---but it
               is unclear which inductive biases can explain observed patterns
               in language acquisition. To …",
  publisher = "cogsci.mindmodeling.org",
  year      =  2020
}

@BOOK{Rumelhart2017-dz,
  title     = "Schemata: The building blocks of cognition",
  author    = "Rumelhart, David E",
  abstract  = "Schemata are employed in the process of interpreting sensory
               data, in retrieving information from memory, in organizing
               actions, in determining goals and subgoals, in allocating
               resources, and, generally, in guiding the flow of processing in
               the system. Perhaps the central function of schemata is in the
               construction of an interpretation of an event, object, or
               situation---that is, in the process of comprehension. Schemata
               are active computational devices capable of evaluating the
               quality of their own fit to the available data. Schemata …",
  publisher = "Routledge",
  year      =  2017
}

@ARTICLE{Kintsch1974-ro,
  title     = "The representation of meaning in memory",
  author    = "Kintsch, Walter",
  abstract  = "Describes a series of empirical and theoretical investigations
               of the role of meaning in psychological processes. A theory is
               proposed for the representation of the meaning of texts that
               uses ordered lists of propositions. Topics include the role of
               lexical decomposition in comprehension and memory and
               propositions as the units of recall. (81/2 p ref) (PsycINFO
               Database Record (c) 2016 APA, all rights reserved)",
  publisher = "Lawrence Erlbaum The representation of meaning in memory.",
  volume    =  279,
  year      =  1974,
  address   = "Oxford, England"
}


@INPROCEEDINGS{Chaudhary2020-fg,
  title     = "Automatic Extraction of Rules Governing Morphological Agreement",
  booktitle = "Proceedings of EMNLP",
  author    = "Chaudhary, Aditi and Anastasopoulos, Antonios and Pratapa,
               Adithya and Mortensen, David R and Sheikh, Zaid and Tsvetkov,
               Yulia and Neubig, Graham",
  abstract  = "Creating a descriptive grammar of a language is an indispensable
               step for language documentation and preservation. However, at
               the same time it is a tedious, time-consuming task. In this
               paper, we take steps towards automating this process by devising
               an automated framework for extracting a first-pass grammatical
               specification from raw text in a concise, human- and
               machine-readable format. We focus on extracting rules describing
               agreement, a morphosyntactic phenomenon at the core of the
               grammars of many of the world's languages. We apply our
               framework to all languages included in the Universal
               Dependencies project, with promising results. Using
               cross-lingual transfer, even with no expert annotations in the
               language of interest, our framework extracts a grammatical
               specification which is nearly equivalent to those created with
               large amounts of gold-standard annotated data. We confirm this
               finding with human expert evaluations of the rules that our
               framework produces, which have an average accuracy of 78\%. We
               release an interface demonstrating the extracted rules at
               https://neulab.github.io/lase/",
  publisher = "Association for Computational Linguistics",
  pages     = "5212--5236",
  month     =  nov,
  year      =  2020,
  address   = "Online"
}


@ARTICLE{Harris1951-uy,
  title     = "Methods in structural linguistics",
  author    = "Harris, Zellig S",
  abstract  = "A general treatise of methods for analyzing the structure of a
               language. For each of the basic steps in analysis, the purpose,
               nature, and results of the procedure are completely described.
               The schedule of procedures is essentially as follows: First, the
               distinct phonologic elements are determined and the relations
               among them investigated. Then the distinct morphologic elements
               are determined and the relations among them investigated. In
               this way the regularities occurring in the speech flow in any
               language community can be stated, although the methods do not
               eliminate non-uniqueness in descriptions. (PsycINFO Database
               Record (c) 2016 APA, all rights reserved)",
  publisher = "University of Chicago Press Methods in structural linguistics.",
  volume    =  384,
  year      =  1951,
  address   = "Chicago, IL, US"
}

@BOOK{Brown2010-jv,
  title     = "Concise Encyclopedia of Languages of the World",
  author    = "Brown, K and Ogilvie, S",
  abstract  = "Concise Encyclopedia of Languages of the World is an
               authoritative single-volume reference resource comprehensively
               describing the major languages and language families of the
               world. It will provide full descriptions of the phonology,
               semantics, morphology, and syntax of the world's major
               languages, giving insights into their structure, history and
               development, sounds, meaning, structure, and language family,
               thereby both highlighting their diversity for comparative study,
               and contextualizing them according to their genetic
               relationships and regional distribution.Based on the highly
               acclaimed and award-winning Encyclopedia of Language and
               Linguistics, this volume will provide an edited collection of
               almost 400 articles throughout which a representative subset of
               the world's major languages are unfolded and explained in
               up-to-date terminology and authoritative interpretation, by the
               leading scholars in linguistics. In highlighting the diversity
               of the world's languages --- from the thriving to the endangered
               and extinct --- this work will be the first point of call to any
               language expert interested in this huge area. No other single
               volume will match the extent of language coverage or the
               authority of the contributors of Concise Encyclopedia of
               Languages of the World.* Extraordinary breadth of coverage: a
               comprehensive selection of just under 400 articles covering the
               world's major languages, language families, and classification
               structures, issues and disputes* Peerless quality: based on 20
               years of academic development on two editions of the leading
               reference resource in linguistics, Encyclopedia of Language and
               Linguistics * Unique authorship: 350 of the world's leading
               experts brought together for one purpose* Exceptional editorial
               selection, review and validation process: Keith Brown and Sarah
               Ogilvie act as first-tier guarantors for article quality and
               coverage* Compact and affordable: one-volume format makes this
               suitable for personal study at any institution interested in
               areal, descriptive, or comparative language study - and at a
               fraction of the cost of the full encyclopedia",
  publisher = "Elsevier",
  month     =  apr,
  year      =  2010,
  language  = "en"
}
@inproceedings{pimentel-etal-2021-non,
    title = "How (Non-)Optimal is the Lexicon?",
    author = "Pimentel, Tiago  and
      Nikkarinen, Irene  and
      Mahowald, Kyle  and
      Cotterell, Ryan  and
      Blasi, Dami{\'a}n",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.350",
    doi = "10.18653/v1/2021.naacl-main.350",
    pages = "4426--4438",
    abstract = "The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf{'}s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world{'}s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon{'}s optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes{---}as measured by code length.",
}

@ARTICLE{Pimentel2021-ue,
  title         = "On Homophony and R'enyi Entropy",
  author        = "Pimentel, Tiago and Meister, Clara and Teufel, Simone and
                   Cotterell, Ryan",
  abstract      = "Homophony's widespread presence in natural languages is a
                   controversial topic. Recent theories of language optimality
                   have tried to justify its prevalence, despite its negative
                   effects on cognitive processing time; e.g., Piantadosi et
                   al. (2012) argued homophony enables the reuse of efficient
                   wordforms and is thus beneficial for languages. This
                   hypothesis has recently been challenged by Trott and Bergen
                   (2020), who posit that good wordforms are more often
                   homophonous simply because they are more phonotactically
                   probable. In this paper, we join in on the debate. We first
                   propose a new information-theoretic quantification of a
                   language's homophony: the sample R\textbackslash'enyi
                   entropy. Then, we use this quantification to revisit Trott
                   and Bergen's claims. While their point is theoretically
                   sound, a specific methodological issue in their experiments
                   raises doubts about their results. After addressing this
                   issue, we find no clear pressure either towards or against
                   homophony -- a much more nuanced result than either
                   Piantadosi et al.'s or Trott and Bergen's findings.",
  month         =  sep,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.13766"
}
@INPROCEEDINGS{Nagata2013-cn,
  title     = "Reconstructing an {Indo-European} family tree from non-native
               English texts",
  booktitle = "Proceedings of ACL",
  author    = "Nagata, Ryo and Whittaker, Edward",
  abstract  = "Mother tongue interference is the phenomenon where linguistic
               systems of a mother tongue are transferred to another language.
               Although there has been plenty of work on mother tongue
               interference, very little is known about how strongly it is
               transferred to another language and about what relation there is
               across mother tongues. To address these questions, this paper
               explores and visualizes mother tongue interference preserved in
               English texts written by Indo-European language speakers. This
               paper further explores …",
  publisher = "aclweb.org",
  pages     = "1137--1147",
  year      =  2013
}

@INPROCEEDINGS{Gerz2018-dd,
  title     = "On the Relation between Linguistic Typology and (Limitations of)
               Multilingual Language Modeling",
  booktitle = "Proceedings of EMNLP",
  author    = "Gerz, Daniela and Vuli{\'c}, Ivan and Ponti, Edoardo Maria and
               Reichart, Roi and Korhonen, Anna",
  abstract  = "A key challenge in cross-lingual NLP is developing general
               language-independent architectures that are equally applicable
               to any language. However, this ambition is largely hampered by
               the variation in structural and semantic properties, i.e. the
               typological profiles of the world's languages. In this work, we
               analyse the implications of this variation on the language
               modeling (LM) task. We present a large-scale study of
               state-of-the art n-gram based and neural language models on 50
               typologically diverse languages covering a wide variety of
               morphological systems. Operating in the full vocabulary LM setup
               focused on word-level prediction, we demonstrate that a coarse
               typology of morphological systems is predictive of absolute LM
               performance. Moreover, fine-grained typological features such as
               exponence, flexivity, fusion, and inflectional synthesis are
               borne out to be responsible for the proliferation of
               low-frequency phenomena which are organically difficult to model
               by statistical architectures, or for the meaning ambiguity of
               character n-grams. Our study strongly suggests that these
               features have to be taken into consideration during the
               construction of next-level language-agnostic LM architectures,
               capable of handling morphologically complex languages such as
               Tamil or Korean.",
  publisher = "Association for Computational Linguistics",
  pages     = "316--327",
  year      =  2018,
  address   = "Brussels, Belgium"
}

@ARTICLE{Aina2021-pe,
  title         = "Does referent predictability affect the choice of
                   referential form? A computational approach using masked
                   coreference resolution",
  author        = "Aina, Laura and Liao, Xixian and Boleda, Gemma and Westera,
                   Matthijs",
  abstract      = "It is often posited that more predictable parts of a
                   speaker's meaning tend to be made less explicit, for
                   instance using shorter, less informative words. Studying
                   these dynamics in the domain of referring expressions has
                   proven difficult, with existing studies, both
                   psycholinguistic and corpus-based, providing contradictory
                   results. We test the hypothesis that speakers produce less
                   informative referring expressions (e.g., pronouns vs. full
                   noun phrases) when the context is more informative about the
                   referent, using novel computational estimates of referent
                   predictability. We obtain these estimates training an
                   existing coreference resolution system for English on a new
                   task, masked coreference resolution, giving us a probability
                   distribution over referents that is conditioned on the
                   context but not the referring expression. The resulting
                   system retains standard coreference resolution performance
                   while yielding a better estimate of human-derived referent
                   predictability than previous attempts. A statistical
                   analysis of the relationship between model output and
                   mention form supports the hypothesis that predictability
                   affects the form of a mention, both its morphosyntactic type
                   and its length.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.13105"
}
@inproceedings{Yoshida2021-rc,
    title = "Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars",
    author = "Yoshida, Ryo  and
      Noji, Hiroshi  and
      Oseki, Yohei",
    booktitle = "Proceedings of EMNLP 2021",
    month = nov,
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.235",
    doi = "10.18653/v1/2021.emnlp-main.235",
    pages = "2964--2973",
    abstract = "In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be discussed.",
}

@INPROCEEDINGS{Wilcox2021-gy,
  title     = "A Targeted Assessment of Incremental Processing in Neural
               Language Models and Humans",
  booktitle = "Proceedings of ACL",
  author    = "Wilcox, Ethan and Vani, Pranali and Levy, Roger",
  abstract  = "We present a targeted, scaled-up comparison of incremental
               processing in humans and neural language models by collecting
               by-word reaction time data for sixteen different syntactic test
               suites across a range of structural phenomena. Human reaction
               time data comes from a novel online experimental paradigm called
               the Interpolated Maze task. We compare human reaction times to
               by-word probabilities for four contemporary language models,
               with different architectures and trained on a range of data set
               sizes. We find that across many phenomena, both humans and
               language models show increased processing difficulty in
               ungrammatical sentence regions with human and model `accuracy'
               scores a la Marvin and Linzen (2018) about equal. However,
               although language model outputs match humans in direction, we
               show that models systematically under-predict the difference in
               magnitude of incremental processing difficulty between
               grammatical and ungrammatical sentences. Specifically, when
               models encounter syntactic violations they fail to accurately
               predict the longer reading times observed in the human data.
               These results call into question whether contemporary language
               models are approaching human-like performance for sensitivity to
               syntactic violations.",
  pages     = "939--952",
  month     =  aug,
  year      =  2021,
  url = "https://aclanthology.org/2021.acl-long.76/"
}

@ARTICLE{Abney1991-po,
  title    = "Memory requirements and local ambiguities of parsing strategies",
  author   = "Abney, Steven P and Johnson, Mark",
  abstract = "We present a method for calculating lower bounds on the space
              required and local ambiguities entailed by parsing strategies. A
              fast, compact natural language parser must implement a strategy
              with low space requirements and few local ambiguities. It is also
              widely assumed in the psycholinguistics literature that extremely
              limited short-term space is available to the human parser, and
              that sentences containing center-embedded constructions are
              incomprehensible because processing them requires more space than
              is available. However, we show that the parsing strategies most
              psycholinguists assume require less space for processing
              center-embedded constructions than for processing other perfectly
              comprehensible constructions. We present alternative strategies
              for which center-embedded constructions do require more space
              than other constructions.",
  journal  = "J. Psycholinguist. Res.",
  volume   =  20,
  number   =  3,
  pages    = "233--250",
  month    =  may,
  year     =  1991
}

@INPROCEEDINGS{Resnik1992-zw,
  title     = "Left-corner parsing and psychological plausibility",
  booktitle = "Proceedings of the 14th conference on Computational linguistics
               - Volume 1",
  author    = "Resnik, Philip",
  abstract  = "It is well known that even extremely limited centerembedding
               causes people to have difficulty in comprehension, but that
               left- and right-branching constructions produce no such effect.
               If the difficulty in comprehension is taken to be a result of
               processing load, as is widely assumed, then measuring the
               processing load induced by a parsing strategy on these
               constructions may help determine its plausibility as a
               psychological model. On this basis, it has been argued [AJ91,
               JL83] that by identifying processing load with space
               utilization, we can rule out both top-down and bottom-up parsing
               as viable candidates for the human sentence processing
               mechanism, and that left-corner parsing represents a plausible
               alternative.Examining their arguments in detail, we find
               difficulties with each presentation. In this paper we revise the
               argument and validate its central claim. In so doing, we
               discover that the key distinction between the parsing methods is
               not the form of prediction (top-down vs. bottom-up vs.
               left-corner), but rather the ability to instantiate the
               operation of composition.",
  publisher = "Association for Computational Linguistics",
  pages     = "191--197",
  series    = "COLING '92",
  month     =  aug,
  year      =  1992,
  address   = "USA",
  location  = "Nantes, France"
}

@ARTICLE{Noji2016-co,
  title   = "Left-corner Parsing for Dependency Grammar",
  author  = "Noji, Hiroshi and Miyao, Yusuke",
  journal = "Information and Media Technologies",
  volume  =  11,
  pages   = "116--153",
  year    =  2016
}

@ARTICLE{Clark2013-xs,
  title    = "Whatever next? Predictive brains, situated agents, and the future
              of cognitive science",
  author   = "Clark, Andy",
  abstract = "Brains, it has recently been argued, are essentially prediction
              machines. They are bundles of cells that support perception and
              action by constantly attempting to match incoming sensory inputs
              with top-down expectations or predictions. This is achieved using
              a hierarchical generative model that aims to minimize prediction
              error within a bidirectional cascade of cortical processing. Such
              accounts offer a unifying model of perception and action,
              illuminate the functional role of attention, and may neatly
              capture the special contribution of cortical processing to
              adaptive success. This target article critically examines this
              ``hierarchical prediction machine'' approach, concluding that it
              offers the best clue yet to the shape of a unified science of
              mind and action. Sections 1 and 2 lay out the key elements and
              implications of the approach. Section 3 explores a variety of
              pitfalls and challenges, spanning the evidential, the
              methodological, and the more properly conceptual. The paper ends
              (sections 4 and 5) by asking how such approaches might impact our
              more general vision of mind, experience, and agency.",
  journal  = "Behav. Brain Sci.",
  volume   =  36,
  number   =  3,
  pages    = "181--204",
  month    =  jun,
  year     =  2013,
  language = "en",
  url="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/whatever-next-predictive-brains-situated-agents-and-the-future-of-cognitive-science/33542C736E17E3D1D44E8D03BE5F4CD9"
}

@BOOK{Elman1996-hx,
  title     = "Rethinking Innateness: A Connectionist Perspective on
               Development",
  author    = "Elman, Jeffrey L and Bates, Elizabeth A and Johnson, Mark H",
  abstract  = "Rethinking Innateness asks the question, ``What does it really
               mean to say that a behavior is innate?'' The authors describe a
               new framework in which interactions, occurring at all levels,
               give rise to emergent forms and behaviors. These outcomes often
               may be highly constrained and universal, yet are not themselves
               directly contained in the genes in any domain-specific way. One
               of the key contributions of Rethinking Innateness is a taxonomy
               of ways in which a behavior can be innate. These include
               constraints at the level of representation, architecture, and
               timing; typically, behaviors arise through the interaction of
               constraints at several of these levels.The ideas are explored
               through dynamic models inspired by a new kind of ``developmental
               connectionism,'' a marriage of connectionist models and
               developmental neurobiology, forming a new theoretical framework
               for the study of behavioral development. While relying heavily
               on the conceptual and computational tools provided by
               connectionism, Rethinking Innateness also identifies ways in
               which these tools need to be enriched by closer attention to
               biology.",
  publisher = "MIT Press",
  year      =  1996,
  language  = "en"
}

@ARTICLE{Lewis2001-ex,
  title     = "Learnability and the statistical structure of language: Poverty
               of stimulus arguments revisited",
  author    = "Lewis, J D and Elman, J L",
  abstract  = "Statistical learning, and ``any account which assigns a
               fundamental role to segmentation, categorization, analogy, and
               generalization'' is rejected in Chomskyan linguistics as
               ``mistaken in principle''(Chomsky, 1975). Acquisition is viewed,
               rather, as a search through the set of possible grammars for
               natural language, guided by successive inputs; or alternatively,
               as a parameter setting process in which the inputs serve as
               triggers. The stochastic nature of the input is thus ignored
               supposedly the learner is oblivious to the …",
  journal   = "Proceedings of the 26th annual Boston University",
  publisher = "Citeseer",
  year      =  2001
}

@ARTICLE{Petty2021-kb,
  title         = "Transformers Generalize Linearly",
  author        = "Petty, Jackson and Frank, Robert",
  abstract      = "Natural language exhibits patterns of hierarchically
                   governed dependencies, in which relations between words are
                   sensitive to syntactic structure rather than linear
                   ordering. While re-current network models often fail to
                   generalize in a hierarchically sensitive way (McCoy et
                   al.,2020) when trained on ambiguous data, the improvement in
                   performance of newer Trans-former language models (Vaswani
                   et al., 2017)on a range of syntactic benchmarks trained on
                   large data sets (Goldberg, 2019; Warstadtet al., 2019) opens
                   the question of whether these models might exhibit
                   hierarchical generalization in the face of impoverished
                   data.In this paper we examine patterns of structural
                   generalization for Transformer sequence-to-sequence models
                   and find that not only do Transformers fail to generalize
                   hierarchically across a wide variety of grammatical mapping
                   tasks, but they exhibit an even stronger preference for
                   linear generalization than comparable recurrent networks",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.12036"
}

@inproceedings{
kharitonov2021what,
title={What they do when in doubt: a study of inductive biases in seq2seq learners},
author={Eugene Kharitonov and Rahma Chaabouni},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YmA86Zo-P_t}
}

@inproceedings{kanishka2020,
    title = "Exploring {BERT}{'}s Sensitivity to Lexical Cues using Tests from Semantic Priming",
    author = "Misra, Kanishka  and
      Ettinger, Allyson  and
      Rayz, Julia",
    booktitle = "Findings of EMNLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.415",
    doi = "10.18653/v1/2020.findings-emnlp.415",
    pages = "4625--4635",
    abstract = "Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows {``}priming{''}, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.",
}

@INPROCEEDINGS{Hollenstein2019-ep,
  title     = "{{C}ogni{V}al}: A Framework for Cognitive Word Embedding
               Evaluation",
  booktitle = "Proceedings of CoNLL",
  author    = "Hollenstein, Nora and de la Torre, Antonio and Langer, Nicolas
               and Zhang, Ce",
  abstract  = "An interesting method of evaluating word representations is by
               how much they reflect the semantic representations in the human
               brain. However, most, if not all, previous works only focus on
               small datasets and a single modality. In this paper, we present
               the first multi-modal framework for evaluating English word
               representations based on cognitive lexical semantics. Six types
               of word embeddings are evaluated by fitting them to 15 datasets
               of eye-tracking, EEG and fMRI signals recorded during language
               processing. To achieve a global score over all evaluation
               hypotheses, we apply statistical significance testing accounting
               for the multiple comparisons problem. This framework is easily
               extensible and available to include other intrinsic and
               extrinsic evaluation methods. We find strong correlations in the
               results between cognitive datasets, across recording modalities
               and to their performance on extrinsic NLP tasks.",
  publisher = "Association for Computational Linguistics",
  pages     = "538--549",
  month     =  nov,
  year      =  2019,
  address   = "Hong Kong, China"
}

@ARTICLE{Ettinger2020-mv,
  title     = "What {BERT} is not: Lessons from a new suite of psycholinguistic
               diagnostics for language models",
  author    = "Ettinger, Allyson",
  abstract  = "Pre-training by language modeling has become a popular and
               successful approach to NLP tasks, but we have yet to understand
               exactly what linguistic capacities these pre-training processes
               confer upon models. In this paper we introduce a suite of
               diagnostics drawn from human language experiments, which allow
               us to ask targeted questions about information used by language
               models for generating predictions in context. As a case study,
               we apply these diagnostics to the popular BERT model, finding
               that it can generally distinguish good from bad completions
               involving shared category or role reversal, albeit with less
               sensitivity than humans, and it robustly retrieves noun
               hypernyms, but it struggles with challenging inference and
               role-based event prediction--- and, in particular, it shows
               clear insensitivity to the contextual impacts of negation.",
  journal   = "TACL",
  publisher = "MIT Press - Journals",
  volume    =  8,
  pages     = "34--48",
  month     =  dec,
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Upadhye2020-ed,
  title     = "{Predicting Reference: What do Language Models Learn about
               Discourse Models?}",
  booktitle = "Proceedings of {EMNLP}",
  author    = "Upadhye, Shiva and Bergen, Leon and Kehler, Andrew",
  publisher = "Association for Computational Linguistics",
  pages     = "977--982",
  month     =  nov,
  year      =  2020,
  address   = "Online"
}

@INPROCEEDINGS{Sugawara2021-ty,
  title      = "Benchmarking machine reading comprehension: A psychological
                perspective",
  booktitle  = "Proceedings of the 16th Conference of the European Chapter of
                the Association for Computational Linguistics: Main Volume",
  author     = "Sugawara, Saku and Stenetorp, Pontus and Aizawa, Akiko",
  abstract   = "It is concluded that future datasets should evaluate the
                capability of the model for constructing a coherent and
                grounded representation to understand context-dependent
                situations and ensure substantive validity by shortcut-proof
                questions and explanation as a part of the task design. Machine
                reading comprehension (MRC) has received considerable attention
                as a benchmark for natural language understanding. However, the
                conventional task design of MRC lacks explainability beyond the
                model interpretation, i.e., reading comprehension by a model
                cannot be explained in human terms. To this end, this position
                paper provides a theoretical basis for the design of MRC
                datasets based on psychology as well as psychometrics, and
                summarizes it in terms of the prerequisites for benchmarking
                MRC. We conclude that future datasets should (i) evaluate the
                capability of the model for constructing a coherent and
                grounded representation to understand context-dependent
                situations and (ii) ensure substantive validity by
                shortcut-proof questions and explanation as a part of the task
                design.",
  publisher  = "Association for Computational Linguistics",
  year       =  2021,
  address    = "Stroudsburg, PA, USA",
  conference = "Proceedings of the 16th Conference of the European Chapter of
                the Association for Computational Linguistics: Main Volume",
  location   = "Online"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Barrett2020-nr,
  title     = "Sequence labelling and sequence classification with gaze: Novel
               uses of eye‐tracking data for Natural Language Processing",
  author    = "Barrett, Maria and Hollenstein, Nora",
  abstract  = "Abstract Eye-tracking data from reading provide a structured
               signal with a fine-grained temporal resolution which closely
               follows the sequential structure of the text. It is highly
               correlated with the cognitive load associated with different
               stages of human, cognitive text processing. While eye-tracking
               data have been extensively studied to understand human
               cognition, it has only recently been considered for Natural
               Language Processing (NLP). In this review, we provide a
               comprehensive overview of how gaze data are being used in
               data-driven NLP, in particular for sequence labelling and
               sequence classification tasks. We argue that eye-tracking may
               effectively counter one of the core challenges of
               machine-learning-based NLP: the scarcity of annotated data. We
               outline the recent advances in gaze-augmented NLP to discuss how
               the gaze signal from human readers can be leveraged while also
               considering the potentials and limitations of this data source.",
  journal   = "Lang. Linguist. Compass",
  publisher = "Wiley",
  volume    =  14,
  number    =  11,
  pages     = "1--16",
  month     =  nov,
  year      =  2020,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Klerke2019-nv,
  title     = "At a glance: The impact of gaze aggregation views on syntactic
               tagging",
  booktitle = "Proceedings of the Beyond Vision and {LANguage}: {inTEgrating}
               Real-world kNowledge ({LANTERN})",
  author    = "Klerke, Sigrid and Plank, Barbara",
  abstract  = "Readers' eye movements used as part of the training signal have
               been shown to improve performance in a wide range of Natural
               Language Processing (NLP) tasks. Previous work uses gaze data
               either at the type level or at the token level and mostly from a
               single eye …",
  publisher = "aclweb.org",
  pages     = "51--61",
  year      =  2019
}

@inproceedings{Hollenstein2019-os,
    title = "Entity Recognition at First Sight: {I}mproving {NER} with Eye Movement Information",
    author = "Hollenstein, Nora  and
      Zhang, Ce",
    booktitle = "Proceedings of NAACL",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1001",
    doi = "10.18653/v1/N19-1001",
    pages = "1--10",
    abstract = "Previous research shows that eye-tracking data contains information about the lexical and syntactic properties of text, which can be used to improve natural language processing models. In this work, we leverage eye movement features from three corpora with recorded gaze information to augment a state-of-the-art neural model for named entity recognition (NER) with gaze embeddings. These corpora were manually annotated with named entity labels. Moreover, we show how gaze features, generalized on word type level, eliminate the need for recorded eye-tracking data at test time. The gaze-augmented models for NER using token-level and type-level features outperform the baselines. We present the benefits of eye-tracking features by evaluating the NER models on both individual datasets as well as in cross-domain settings.",
}

@inproceedings{Mishra2017-hx,
    title = "Leveraging Cognitive Features for Sentiment Analysis",
    author = "Mishra, Abhijit  and
      Kanojia, Diptesh  and
      Nagar, Seema  and
      Dey, Kuntal  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of CoNLL",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1016",
    doi = "10.18653/v1/K16-1016",
    pages = "156--166",
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Barrett2016-px,
  title     = "Weakly supervised part-of-speech tagging using eye-tracking data",
  booktitle = "Proceedings of ACL",
  author    = "Barrett, Maria and Bingel, Joachim and Keller, Frank and
               S{\o}gaard, Anders",
  abstract  = "For many of the world's languages, there are no or very few
               linguistically annotated resources. On the other hand, raw text,
               and often also dictionaries, can be harvested from the web for
               many of these languages, and part-of-speech taggers can be
               trained with these resources. At the same time, previous
               research shows that eye-tracking data, which can be obtained
               without explicit annotation, contains clues to partof-speech
               information. In this work, we bring these two ideas together and
               show that given raw text, a dictionary, and …",
  publisher = "aclweb.org",
  pages     = "579--584",
  year      =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Vickers2021-qm,
  title     = "{CogNLP-Sheffield} at {CMCL} 2021 Shared Task: Blending
               Cognitively Inspired Features with Transformer-based Language
               Models for Predicting Eye Tracking Patterns",
  booktitle = "Proceedings of CMCL",
  author    = "Vickers, Peter and Wainwright, Rosa and Madabushi, Harish Tayyar
               and Villavicencio, Aline",
  abstract  = "Abstract The CogNLP-Sheffield submissions to the CMCL 2021
               Shared Task examine the value of a variety of cognitively and
               linguistically inspired features for predicting eye tracking
               patterns, as both standalone model inputs and as supplements to
               contextual word …",
  publisher = "aclweb.org",
  pages     = "125--133",
  year      =  2021
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Mathias2020-zz,
  title     = "A Survey on Using Gaze Behaviour for Natural Language Processing",
  booktitle = "{IJCAI}",
  author    = "Mathias, Sandeep and Kanojia, Diptesh and Mishra, Abhijit and
               Bhattacharyya, Pushpak",
  abstract  = "Gaze behaviour has been used as a way to gather cognitive
               information for a number of years. In this paper, we discuss the
               use of gaze behaviour in solving different tasks in natural
               language processing (NLP) without having to record it at test
               time. This is because the …",
  publisher = "researchgate.net",
  pages     = "4907--4913",
  year      =  2020
}

@INPROCEEDINGS{Mitsuda2013-ul,
  title     = "Detecting Missing Annotation Disagreement using Eye Gaze
               Information",
  booktitle = "Proceedings of the 11th Workshop on {A}sian Language Resources",
  author    = "Mitsuda, Koh and Iida, Ryu and Tokunaga, Takenobu",
  publisher = "Asian Federation of Natural Language Processing",
  pages     = "19--26",
  month     =  oct,
  year      =  2013,
  address   = "Nagoya, Japan"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Manning2015-mm,
  title     = "Last Words: Computational Linguistics and Deep Learning",
  author    = "Manning, Christopher D",
  abstract  = "Deep Learning waves have lapped at the shores of computational
               linguistics for several years now, but 2015 seems like the year
               when the full force of the tsunami hit the major Natural
               Language Processing (NLP) conferences. However, some pundits are
               predicting that the final damage will be even worse.
               Accompanying ICML 2015 in Lille, France, there was another,
               almost as big, event: the 2015 Deep Learning Workshop. The
               workshop ended with a panel discussion, and at it, Neil Lawrence
               said,``NLP is kind of like a rabbit in the …",
  journal   = "Comput. Linguist.",
  publisher = "aclanthology.org",
  volume    =  41,
  number    =  4,
  pages     = "701--707",
  month     =  dec,
  year      =  2015
}

@article{ang2014effects,
  title={The effects of discourse markers on the reading comprehension and speed of Chinese learners of English},
  author={Ang, Zhang},
  journal={International Journal Of English Language and Linguistics Studies},
  volume={2},
  number={2},
  pages={27--49},
  year={2014}
}
@article{rumelhart1980schemata,
  title={Schemata: The building blocks of cognition},
  author={Rumelhart, David E},
  journal={Theoretical Issues in Reading Comprehension},
  year={1980},
  publisher={Lawrence Erlbaum Associates}
}

@ARTICLE{Carrell1982-ay,
  title     = "Cohesion Is Not Coherence",
  author    = "Carrell, Patricia L",
  abstract  = "[The purpose of this paper is to criticize the concept of
               cohesion as a measure of the coherence of a text. The paper
               begins with a brief overview of Halliday and Hasan's (1976)
               cohesion concept as an index of textual coherence. Next, the
               paper criticizes the concept of cohesion as a measure of textual
               coherence in the light of schema-theoretical views of text
               processing (e.g. reading) as an interactive process between the
               text and the reader. This criticism, which is drawn from both
               theoretical and empirical work in schema theory, attempts to
               show that text-analytic procedures such as Halliday and Hasan's
               cohesion concept, which encourage the belief that coherence is
               located in the text and can be defined as a configuration of
               textual features, and which fail to take the contributions of
               the text's reader into account, are incapable of accounting for
               textual coherence. The paper concludes with a caution to second
               language (EFL/ESL) teachers and researchers not to expect
               cohesion theory to be the solution to EFL/ESL reading/writing
               coherence problems at the level of the text.]",
  journal   = "TESOL Quarterly",
  publisher = "[Wiley, Teachers of English to Speakers of Other Languages, Inc.
               (TESOL)]",
  volume    =  16,
  number    =  4,
  pages     = "479--488",
  year      =  1982
}

@BOOK{Carrell1988-bj,
  title     = "Interactive Approaches to Second Language Reading",
  author    = "Carrell, Patricia L and Devine, Joanne and Eskey, David E",
  abstract  = "This text presents up-to-date theory, research, and classroom
               applications in second language reading from an interactive
               perspective. The paperback edition presents up-to-date theory,
               research, and classroom applications in second language reading
               from an interactive perspective. The book supports the view that
               reading in a second language involves more than decoding;
               instead, reading is seen as an interactive process whereby the
               learner's own background and knowledge contribute to
               understanding reading material. These articles are written by
               experts in the reading field, who present process models and
               discuss classroom applications. The book examines traditional
               approaches that focus on text and decoding skills, as well as
               more contemporary theories that consider the reader's base
               knowledge.",
  publisher = "Cambridge University Press",
  month     =  apr,
  year      =  1988,
  language  = "en"
}

@BOOK{Chaudron1985-sn,
  title     = "The Effect of Discourse Markers on the Comprehension of Lectures",
  author    = "Chaudron, Craig and Richards, Jack C",
  abstract  = "The study examined the ways in which different categories of
               discourse marker affect how well foreign college students
               understand university lectures, and specifically, the effects of
               macro markers (those indicating overall organization) and micro
               markers (functioning as fillers, indicating links between
               sentences). Four versions of an American history lecture were
               developed: a baseline version without special signals of
               discourse organization, a version with micro markers, one with
               macro markers, and one combining micro and macro markers. The
               four versions were assigned at random to different classes of
               pre-university and university groups of students of English as a
               second language. The students' comprehension was measured by
               three instruments: a cloze recall test, a multiple-choice test,
               and a true-false test. It was found that macro markers led to
               better recall of the text material than micro markers, in either
               the micro or the micro-macro version. It is suggested that more",
  publisher = "ERIC",
  month     =  apr,
  year      =  1985,
  keywords  = "Classroom Communication; College Instruction; Discourse
               Analysis; Foreign Students; Higher Education; Lecture Method;
               Listening Comprehension",
  language  = "en"
}

@ARTICLE{Carrell1983-gw,
  title     = "Schema theory and {ESL} reading pedagogy",
  author    = "Carrell, Patricia L and Eisterhold, Joan C",
  abstract  = "This article discusses the important role of background
               knowledge in a psycholinguistic model of EFL/ESL reading and
               demonstrates the relevance of schema-theoretic views of reading
               to the teaching of reading to EFL/ESL students. According to
               schema theory, reading comprehension is an interactive process
               between the text and the reader's prior background knowledge
               (Adams and Collins 1979, Rumelhart 1980). Reading comprehension
               involves one's knowledge of the world, which may be culturally
               based and culturally biased. Classroom implications of the
               schema-theoretic view of reading for EFL/ESL reading pedagogy
               are discussed, with techniques suggested for bringing about
               reader-centered EFL/ESL reading.",
  journal   = "TESOL Q.",
  publisher = "JSTOR",
  volume    =  17,
  number    =  4,
  pages     = "553",
  month     =  dec,
  year      =  1983
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{An2013-dk,
  title     = "Schema Theory in Reading",
  author    = "An, Shuying",
  abstract  = "The term`` schema'' was first used in psychology with the
               meaning of`` an active organization of past reactions or
               experiences''. It assumes that written text does not carry
               meaning by itself. Rather, a text only provides directions for
               readers as to how they should retrieve or …",
  journal   = "Theory \& Practice in Language Studies",
  publisher = "academypublication.com",
  volume    =  3,
  number    =  1,
  year      =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Mitchell1980-nj,
  title     = "The need for biases in learning generalizations",
  author    = "Mitchell, Tom M",
  abstract  = "Learning involves the ability to generalize from past experience
               in order to deal with new situations that are'' related to''
               this experience. The inductive leaap needed to deal with new
               situations seems to be possible only under certain biases for
               choosing one generalization of …",
  publisher = "Department of Computer Science, Laboratory for Computer Science
               Research, Rutgers Univ.",
  year      =  1980
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kirov2018-yf,
  title     = "Recurrent Neural Networks in Linguistic Theory: Revisiting
               Pinker and Prince (1988) and the Past Tense Debate",
  author    = "Kirov, Christo and Cotterell, Ryan",
  abstract  = "Can advances in NLP help advance cognitive modeling? We examine
               the role of artificial neural networks, the current state of the
               art in many common NLP tasks, by returning to a classic case
               study. In 1986, Rumelhart and McClelland famously introduced a
               neural …",
  journal   = "TACL",
  publisher = "aclweb.org",
  volume    =  6,
  pages     = "651--665",
  year      =  2018
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Warstadt2020-tk,
  title     = "{CAN} {NEURAL} {NETWORKS} {ACQUIRE} A {STRUCTURAL} {BIAS} {FROM}
               {RAW} {LINGUISTIC} {DATA}?",
  booktitle = "Proceedings of ACL",
  author    = "Warstadt, Alex and Bowman, Samuel R",
  abstract  = "We evaluate whether BERT, a widely used neural network for
               sentence processing, acquires an inductive bias towards forming
               structural generalizations through pretraining on raw data. We
               conduct four experiments testing its preference for structural
               vs. linear …",
  publisher = "par.nsf.gov",
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Rumelhart1988-ju,
  title     = "Parallel distributed processing",
  author    = "Rumelhart, David E and McClelland, James L and Group, Pdp
               Research and {Others}",
  abstract  = "Figure 1 illustrates the basic aspects of these systems. There
               is a set of processing units generally indicated by circles in
               our diagrams; at each point in time, each unit 21,-has an
               activation value, denoted in the diagram as a,-(t); this
               activation value is passed through a …",
  publisher = "IEEE Massachusetts",
  volume    =  1,
  year      =  1988
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{McCurdy2020-ck,
  title     = "Inflecting When There's No Majority: Limitations of
               {Encoder-Decoder} Neural Networks as Cognitive Models for German
               Plurals",
  booktitle = "Proceedings of ACL",
  author    = "McCurdy, Kate and Goldwater, Sharon and Lopez, Adam",
  abstract  = "Can artificial neural networks learn to represent inflectional
               morphology and generalize to new words as human speakers do?
               Kirov and Cotterell (2018) argue that the answer is yes: modern
               Encoder-Decoder (ED) architectures learn human-like behavior
               when inflecting …",
  publisher = "aclweb.org",
  pages     = "1745--1756",
  year      =  2020
}

%%%%


@ARTICLE{Frank2016-so,
  title    = "Cross-Linguistic Differences in Processing Double-Embedded
              Relative Clauses: Working-Memory Constraints or Language
              Statistics?",
  author   = "Frank, Stefan L and Trompenaars, Thijs and Vasishth, Shravan",
  abstract = "An English double-embedded relative clause from which the middle
              verb is omitted can often be processed more easily than its
              grammatical counterpart, a phenomenon known as the grammaticality
              illusion. This effect has been found to be reversed in German,
              suggesting that the illusion is language specific rather than a
              consequence of universal working memory constraints. We present
              results from three self-paced reading experiments which show that
              Dutch native speakers also do not show the grammaticality
              illusion in Dutch, whereas both German and Dutch native speakers
              do show the illusion when reading English sentences. These
              findings provide evidence against working memory constraints as
              an explanation for the observed effect in English. We propose an
              alternative account based on the statistical patterns of the
              languages involved. In support of this alternative, a single
              recurrent neural network model that is trained on both Dutch and
              English sentences is shown to predict the cross-linguistic
              difference in the grammaticality effect.",
  journal  = "Cogn. Sci.",
  volume   =  40,
  number   =  3,
  pages    = "554--578",
  month    =  apr,
  year     =  2016,
  keywords = "Bilingualism; Centre embedding; Cross-linguistic differences;
              Grammaticality illusion; Recurrent neural network model; Relative
              clauses; Self-paced reading; Sentence comprehension",
  language = "en",
  url="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12247"
}


@ARTICLE{Husain2014-zr,
  title    = "Strong expectations cancel locality effects: evidence from Hindi",
  author   = "Husain, Samar and Vasishth, Shravan and Srinivasan, Narayanan",
  abstract = "Expectation-driven facilitation (Hale, 2001; Levy, 2008) and
              locality-driven retrieval difficulty (Gibson, 1998, 2000; Lewis
              \& Vasishth, 2005) are widely recognized to be two critical
              factors in incremental sentence processing; there is accumulating
              evidence that both can influence processing difficulty. However,
              it is unclear whether and how expectations and memory interact.
              We first confirm a key prediction of the expectation account: a
              Hindi self-paced reading study shows that when an expectation for
              an upcoming part of speech is dashed, building a rarer structure
              consumes more processing time than building a less rare
              structure. This is a strong validation of the expectation-based
              account. In a second study, we show that when expectation is
              strong, i.e., when a particular verb is predicted, strong
              facilitation effects are seen when the appearance of the verb is
              delayed; however, when expectation is weak, i.e., when only the
              part of speech ``verb'' is predicted but a particular verb is not
              predicted, the facilitation disappears and a tendency towards a
              locality effect is seen. The interaction seen between expectation
              strength and distance shows that strong expectations cancel
              locality effects, and that weak expectations allow locality
              effects to emerge.",
  journal  = "PLoS One",
  volume   =  9,
  number   =  7,
  pages    = "e100986",
  month    =  jul,
  year     =  2014,
  language = "en",
  url="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0100986"
}

@ARTICLE{Lewis2005-hp,
  title    = "An activation-based model of sentence processing as skilled
              memory retrieval",
  author   = "Lewis, Richard L and Vasishth, Shravan",
  abstract = "We present a detailed process theory of the moment-by-moment
              working-memory retrievals and associated control structure that
              subserve sentence comprehension. The theory is derived from the
              application of independently motivated principles of memory and
              cognitive skill to the specialized task of sentence parsing. The
              resulting theory construes sentence processing as a series of
              skilled associative memory retrievals modulated by
              similarity-based interference and fluctuating activation. The
              cognitive principles are formalized in computational form in the
              Adaptive Control of Thought-Rational (ACT-R) architecture, and
              our process model is realized in ACT-R. We present the results of
              6 sets of simulations: 5 simulation sets provide quantitative
              accounts of the effects of length and structural interference on
              both unambiguous and garden-path structures. A final simulation
              set provides a graded taxonomy of double center embeddings
              ranging from relatively easy to extremely difficult. The
              explanation of center-embedding difficulty is a novel one that
              derives from the model' complete reliance on discriminating
              retrieval cues in the absence of an explicit representation of
              serial order information. All fits were obtained with only 1 free
              scaling parameter fixed across the simulations; all other
              parameters were ACT-R defaults. The modeling results support the
              hypothesis that fluctuating activation and similarity-based
              interference are the key factors shaping working memory in
              sentence processing. We contrast the theory and empirical
              predictions with several related accounts of sentence-processing
              complexity.",
  journal  = "Cogn. Sci.",
  volume   =  29,
  number   =  3,
  pages    = "375--419",
  month    =  may,
  year     =  2005,
  language = "en",
  url="https://www.ling.uni-potsdam.de/~vasishth/pdfs/Lewis-VasishthCogSci2005.pdf"
}
@ARTICLE{Lewis2006-qs,
  title    = "Computational principles of working memory in sentence
              comprehension",
  author   = "Lewis, Richard L and Vasishth, Shravan and Van Dyke, Julie A",
  abstract = "Understanding a sentence requires a working memory of the partial
              products of comprehension, so that linguistic relations between
              temporally distal parts of the sentence can be rapidly computed.
              We describe an emerging theoretical framework for this working
              memory system that incorporates several independently motivated
              principles of memory: a sharply limited attentional focus, rapid
              retrieval of item (but not order) information subject to
              interference from similar items, and activation decay (forgetting
              over time). A computational model embodying these principles
              provides an explanation of the functional capacities and severe
              limitations of human processing, as well as accounts of reading
              times. The broad implication is that the detailed nature of
              cross-linguistic sentence processing emerges from the interaction
              of general principles of human memory with the specialized task
              of language comprehension.",
  journal  = "Trends Cogn. Sci.",
  volume   =  10,
  number   =  10,
  pages    = "447--454",
  month    =  oct,
  year     =  2006,
  language = "en",
  url="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2239011/"
}
@article{lohse2004domain,
  title={Domain minimization in English verb-particle constructions},
  author={Lohse, Barbara and Hawkins, John A and Wasow, Thomas},
  journal={Language},
  pages={238--261},
  year={2004},
  publisher={JSTOR}
}

@ARTICLE{Cowan2001-xj,
  title    = "The magical number 4 in short-term memory: a reconsideration of
              mental storage capacity",
  author   = "Cowan, N",
  abstract = "Miller (1956) summarized evidence that people can remember about
              seven chunks in short-term memory (STM) tasks. However, that
              number was meant more as a rough estimate and a rhetorical device
              than as a real capacity limit. Others have since suggested that
              there is a more precise capacity limit, but that it is only three
              to five chunks. The present target article brings together a wide
              variety of data on capacity limits suggesting that the smaller
              capacity limit is real. Capacity limits will be useful in
              analyses of information processing only if the boundary
              conditions for observing them can be carefully described. Four
              basic conditions in which chunks can be identified and capacity
              limits can accordingly be observed are: (1) when information
              overload limits chunks to individual stimulus items, (2) when
              other steps are taken specifically to block the recording of
              stimulus items into larger chunks, (3) in performance
              discontinuities caused by the capacity limit, and (4) in various
              indirect effects of the capacity limit. Under these conditions,
              rehearsal and long-term memory cannot be used to combine stimulus
              items into chunks of an unknown size; nor can storage mechanisms
              that are not capacity-limited, such as sensory memory, allow the
              capacity-limited storage mechanism to be refilled during recall.
              A single, central capacity limit averaging about four chunks is
              implicated along with other, noncapacity-limited sources. The
              pure STM capacity limit expressed in chunks is distinguished from
              compound STM limits obtained when the number of separately held
              chunks is unclear. Reasons why pure capacity estimates fall
              within a narrow range are discussed and a capacity limit for the
              focus of attention is proposed.",
  journal  = "Behav. Brain Sci.",
  volume   =  24,
  number   =  1,
  pages    = "87--114; discussion 114--85",
  month    =  feb,
  year     =  2001,
  language = "en"
}

@ARTICLE{Vasishth2010-ji,
  title     = "Short-term forgetting in sentence comprehension: Crosslinguistic
               evidence from verb-final structures",
  author    = "Vasishth, Shravan and Suckow, Katja and Lewis, Richard L and
               Kern, Sabine",
  abstract  = "Seven experiments using self-paced reading and eyetracking
               suggest that omitting the middle verb in a double centre
               embedding leads to easier processing in English but leads to
               greater difficulty in German. One commonly accepted explanation
               for the English pattern?based on data from offline acceptability
               ratings and due to Gibson and Thomas (1999)?is that
               working-memory overload leads the comprehender to forget the
               prediction of the upcoming verb phrase (VP), which reduces
               working-memory load. We show that this VP-forgetting hypothesis
               does an excellent job of explaining the English data, but cannot
               account for the German results. We argue that the English and
               German results can be explained by the parser's adaptation to
               the grammatical properties of the languages; in contrast to
               English, German subordinate clauses always have the verb in
               clause-final position, and this property of German may lead the
               German parser to maintain predictions of upcoming VPs more
               robustly compared to English. The evidence thus argues against
               language-independent forgetting effects in online sentence
               processing; working-memory constraints can be conditioned by
               countervailing influences deriving from grammatical properties
               of the language under study.",
  journal   = "Language and Cognitive Processes",
  publisher = "Routledge",
  volume    =  25,
  number    =  4,
  pages     = "533--567",
  month     =  may,
  year      =  2010,
  url = "https://www.tandfonline.com/doi/abs/10.1080/01690960903310587"
}


@ARTICLE{Miller1956-xz,
  title    = "The magical number seven plus or minus two: some limits on our
              capacity for processing information",
  author   = "Miller, G A",
  journal  = "Psychol. Rev.",
  volume   =  63,
  number   =  2,
  pages    = "81--97",
  month    =  mar,
  year     =  1956,
  keywords = "PSYCHOMETRICS; STATISTICS",
  language = "en"
}

@INPROCEEDINGS{OConnor2021-sf,
  title     = "What Context Features Can Transformer Language Models Use?",
  booktitle = "Proceedings of ACL",
  author    = "O'Connor, Joe and Andreas, Jacob",
  abstract  = "Transformer-based language models benefit from conditioning on
               contexts of hundreds to thousands of previous tokens. What
               aspects of these contexts contribute to accurate model
               prediction? We describe a series of experiments that measure
               usable information by selectively ablating lexical and
               structural information in transformer language models trained on
               English Wikipedia. In both mid- and long-range contexts, we find
               that several extremely destructive context
               manipulations---including shuffling word order within sentences
               and deleting all words other than nouns---remove less than 15\%
               of the usable information. Our results suggest that long
               contexts, but not their detailed syntactic and propositional
               content, are important for the low perplexity of current
               transformer language models.",
  pages     = "851--864",
  month     =  aug,
  year      =  2021,
}

@inproceedings{Khandelwal2018,
  title={Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context},
  author={Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={284--294},
  year={2018}
}


@ARTICLE{Elman1991-ld,
  title     = "Distributed representations, simple recurrent networks, and
               grammatical structure",
  author    = "Elman, Jeffrey L",
  abstract  = "In this paper three problems for a connectionist account of
               language are considered1.What is the nature of linguistic
               representations?2.How can complex structural relationships such
               as constituent be represented?3.How can the apparently
               open-ended nature of language be accommodated by a
               fixed-resource system?",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  7,
  number    =  2,
  pages     = "195--225",
  month     =  sep,
  year      =  1991
}

@ARTICLE{Murawaki2019-wl,
  title         = "On the Definition of Japanese Word",
  author        = "Murawaki, Yugo",
  abstract      = "The annotation guidelines for Universal Dependencies (UD)
                   stipulate that the basic units of dependency annotation are
                   syntactic words, but it is not clear what are syntactic
                   words in Japanese. Departing from the long tradition of
                   using phrasal units called bunsetsu for dependency parsing,
                   the current UD Japanese treebanks adopt the Short Unit
                   Words. However, we argue that they are not syntactic word as
                   specified by the annotation guidelines. Although we find
                   non-mainstream attempts to linguistically define Japanese
                   words, such definitions have never been applied to corpus
                   annotation. We discuss the costs and benefits of adopting
                   the rather unfamiliar criteria.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.09719"
}

@ARTICLE{Parker2017-zm,
  title     = "The cue-based retrieval theory of sentence comprehension: New
               findings and new challenges",
  author    = "Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A",
  journal   = "Language processing and disorders",
  publisher = "Cambridge Scholars Publishing Newcastle",
  pages     = "121--144",
  year      =  2017
}

@INCOLLECTION{Ferreira2016-oj,
  title     = "Prediction, Information Structure, and
               {Good-Enough} Language Processing",
  booktitle = "Psychology of Learning and Motivation",
  author    = "Ferreira, Fernanda and Lowder, Matthew W",
  chapter = 	 {6},
  editor    = "Ross, Brian H",
  abstract  = "The good-enough language processing approach emphasizes people's
               tendency to generate superficial and even inaccurate
               interpretations of sentences. At the same time, a number of
               researchers have argued that prediction plays a key role in
               comprehension, allowing people to anticipate features of the
               input and even specific upcoming words based on sentential
               constraint. In this chapter, we review evidence from our lab
               supporting both approaches, even though at least superficially
               these two perspectives seem incompatible. We then argue that
               what allows us to link good-enough processing and prediction is
               the concept of information structure, which states that
               sentences are organized to convey both given or presupposed
               information, and new or focused information. Our fundamental
               proposal is that given or presupposed information is processed
               in a good-enough manner, while new or focused information is the
               target of the comprehender's prediction efforts. The result is a
               theory that brings together three different literatures that
               have been treated almost entirely independently, and which can
               be evaluated using a combination of behavioral, computational,
               and neural methods.",
  publisher = "Academic Press",
  volume    =  65,
  pages     = "217--247",
  month     =  jan,
  year      =  2016,
  keywords  = "Comprehension; Good-enough processing; Information structure;
               Language processing; Prediction",
  url="https://ferreiralab.faculty.ucdavis.edu/wp-content/uploads/sites/222/2015/05/Ferreira-Lowder-2016_Psych-of-Learning-Motivation.pdf"
}

@ARTICLE{Maekawa2014-ze,
  title    = "Balanced corpus of contemporary written Japanese",
  author   = "Maekawa, Kikuo and Yamazaki, Makoto and Ogiso, Toshinobu and
              Maruyama, Takehiko and Ogura, Hideki and Kashino, Wakako and
              Koiso, Hanae and Yamaguchi, Masaya and Tanaka, Makiro and Den,
              Yasuharu",
  abstract = "The balanced corpus of contemporary written Japanese (BCCWJ) is
              Japan's first 100 million words balanced corpus. It consists of
              three subcorpora (publication subcorpus, library subcorpus, and
              special-purpose subcorpus) and covers a wide range of text
              registers including books in general, magazines, newspapers,
              governmental white papers, best-selling books, an internet
              bulletin-board, a blog, school textbooks, minutes of the national
              diet, publicity newsletters of local governments, laws, and
              poetry verses. A random sampling technique is utilized whenever
              possible in order to maximize the representativeness of the
              corpus. The corpus is annotated in terms of dual POS analysis,
              document structure, and bibliographical information. The BCCWJ is
              currently accessible in three different ways including Chunagon a
              web-based interface to the dual POS analysis data. Lastly,
              results of some pilot evaluation of the corpus with respect to
              the textual diversity are reported. The analyses include POS
              distribution, word-class distribution, entropy of orthography,
              sentence length, and variation of the adjective predicate. High
              textual diversity is observed in all these analyses.",
  journal  = "Language Resources and Evaluation",
  volume   =  48,
  number   =  2,
  pages    = "345--371",
  month    =  jun,
  year     =  2014,
  url="https://link.springer.com/article/10.1007/s10579-013-9261-0"
}

@ARTICLE{Yadav2020-lu,
  title    = "Word Order Typology Interacts With Linguistic Complexity: A
              {Cross-Linguistic} Corpus Study",
  author   = "Yadav, Himanshu and Vaidya, Ashwini and Shukla, Vishakha and
              Husain, Samar",
  abstract = "Much previous work has suggested that word order preferences
              across languages can be explained by the dependency distance
              minimization constraint (Ferrer-i Cancho, 2008, 2015; Hawkins,
              1994). Consistent with this claim, corpus studies have shown that
              the average distance between a head (e.g., verb) and its
              dependent (e.g., noun) tends to be short cross-linguistically
              (Ferrer-i Cancho, 2014; Futrell, Mahowald, \& Gibson, 2015; Liu,
              Xu, \& Liang, 2017). This implies that on average languages avoid
              inefficient or complex structures for simpler structures. But a
              number of studies in psycholinguistics (Konieczny, 2000; Levy \&
              Keller, 2013; Vasishth, Suckow, Lewis, \& Kern, 2010) show that
              the comprehension system can adapt to the typological properties
              of a language, for example, verb-final order, leading to more
              complex structures, for example, having longer linear distance
              between a head and its dependent. In this paper, we conduct a
              corpus study for a group of 38 languages, which were either
              Subject-Verb-Object (SVO) or Subject-Object-Verb (SOV), in order
              to investigate the role of word order typology in determining
              syntactic complexity. We present results aggregated across all
              dependency types, as well as for specific verbal (objects,
              indirect objects, and adjuncts) and nonverbal (nominal,
              adjectival, and adverbial) dependencies. The results suggest that
              dependency distance in a language is determined by the default
              word order of a language, and crucially, the direction of a
              dependency (whether the head precedes the dependent or follows
              it; e.g., whether the noun precedes the verb or follows it).
              Particularly we show that in SOV languages (e.g., Hindi, Korean)
              as well as SVO languages (e.g., English, Spanish), longer linear
              distance (measured as number of words) between head and dependent
              arises in structures when they mirror the default word order of
              the language. In addition to showing results on linear distance,
              we also investigate the influence of word order typology on
              hierarchical distance (HD; measured as number of heads between
              head and dependent). The results for HD are similar to that of
              linear distance. At the same time, in comparison to linear
              distance, the influence of adaptability on HD seems less strong.
              In particular, the results show that most languages tend to avoid
              greater structural depth. Together, these results show evidence
              for ``limited adaptability'' to the default word order
              preferences in a language. Our results support a large body of
              work in the processing literature that highlights the importance
              of linguistic exposure and its interaction with working memory
              constraints in determining sentence complexity. Our results also
              point to the possible role of other factors such as the
              morphological richness of a language and a multifactor account of
              sentence complexity remains a promising area for future
              investigation.",
  journal  = "Cogn. Sci.",
  volume   =  44,
  number   =  4,
  pages    = "e12822",
  month    =  apr,
  year     =  2020,
  keywords = "Corpus linguistics; Dependency treebanks; Language adaptation;
              Language typology; Quantitative linguistics; Working memory
              constraints",
  language = "en"
}

@ARTICLE{Warstadt2020-or,
  title    = "{BLiMP}: The Benchmark of Linguistic Minimal Pairs for English",
  author   = "Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey,
              Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel",
  abstract = "Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei
              Peng, Sheng-Fu Wang, Samuel R. Bowman. Transactions of the
              Association for Computational Linguistics, Volume 8. 2020.",
  journal  = "TACL",
  volume   =  8,
  pages    = "377--392",
  year     =  2020
}

@ARTICLE{Spivey-Knowlton1995-ga,
  title    = "Resolving attachment ambiguities with multiple constraints",
  author   = "Spivey-Knowlton, M and Sedivy, J C",
  abstract = "Different theories of syntactic ambiguity resolution argue for
              different sources of information determining initial parsing
              decisions (e.g., structurally defined parsing principles,
              lexically specific biases, or referential pragmatics). However, a
              ``constraint-based'' approach to syntactic ambiguity resolution
              proposes that both lexically specific biases and referential
              pragmatics are used in parallel by the comprehender. Analyses of
              text corpora, sentence fragment completions, and self-paced
              reading experiments were conducted to demonstrate that both local
              information (lexically specific biases) and contextual
              information (referential presupposition) contribute to the
              on-line resolution of prepositional phrase attachment
              ambiguities. There does not appear to be a role for purely
              structurally defined parsing principles (i.e., minimal
              attachment). Present and previous evidence is consistent with a
              developing framework in which multiple constraints (bottom-up and
              top-down) interact immediately to determine initial syntactic
              commitments.",
  journal  = "Cognition",
  volume   =  55,
  number   =  3,
  pages    = "227--267",
  month    =  jun,
  year     =  1995,
  language = "en"
}

@ARTICLE{Van_Schijndel2021-sm,
  title    = "{Single-Stage} Prediction Models Do Not Explain the Magnitude of
              Syntactic Disambiguation Difficulty",
  author   = "van Schijndel, Marten and Linzen, Tal",
  abstract = "The disambiguation of a syntactically ambiguous sentence in favor
              of a less preferred parse can lead to slower reading at the
              disambiguation point. This phenomenon, referred to as a
              garden-path effect, has motivated models in which readers
              initially maintain only a subset of the possible parses of the
              sentence, and subsequently require time-consuming reanalysis to
              reconstruct a discarded parse. A more recent proposal argues that
              the garden-path effect can be reduced to surprisal arising in a
              fully parallel parser: words consistent with the initially
              dispreferred but ultimately correct parse are simply less
              predictable than those consistent with the incorrect parse. Since
              predictability has pervasive effects in reading far beyond
              garden-path sentences, this account, which dispenses with
              reanalysis mechanisms, is more parsimonious. Crucially, it
              predicts a linear effect of surprisal: the garden-path effect is
              expected to be proportional to the difference in word surprisal
              between the ultimately correct and ultimately incorrect
              interpretations. To test this prediction, we used recurrent
              neural network language models to estimate word-by-word surprisal
              for three temporarily ambiguous constructions. We then estimated
              the slowdown attributed to each bit of surprisal from human
              self-paced reading times, and used that quantity to predict
              syntactic disambiguation difficulty. Surprisal successfully
              predicted the existence of garden-path effects, but drastically
              underpredicted their magnitude, and failed to predict their
              relative severity across constructions. We conclude that a full
              explanation of syntactic disambiguation difficulty may require
              recovery mechanisms beyond predictability.",
  journal  = "Cognitive Science",
  volume   =  45,
  number   =  6,
  pages    = "e12988",
  month    =  jun,
  year     =  2021,
  keywords = "Garden paths; Information theory; Neural networks; Self-paced
              reading; Surprisal",
  language = "en",
  url="https://pubmed.ncbi.nlm.nih.gov/34170031/"
}


@ARTICLE{Safavi2016-mw,
  title    = "Dependency Resolution Difficulty Increases with Distance in
              Persian Separable Complex Predicates: Evidence for Expectation
              and {Memory-Based} Accounts",
  author   = "Safavi, Molood S and Husain, Samar and Vasishth, Shravan",
  abstract = "Delaying the appearance of a verb in a noun-verb dependency tends
              to increase processing difficulty at the verb; one explanation
              for this locality effect is decay and/or interference of the noun
              in working memory. Surprisal, an expectation-based account,
              predicts that delaying the appearance of a verb either renders it
              no more predictable or more predictable, leading respectively to
              a prediction of no effect of distance or a facilitation.
              Recently, Husain et al. (2014) suggested that when the exact
              identity of the upcoming verb is predictable (strong
              predictability), increasing argument-verb distance leads to
              facilitation effects, which is consistent with surprisal; but
              when the exact identity of the upcoming verb is not predictable
              (weak predictability), locality effects are seen. We investigated
              Husain et al.'s proposal using Persian complex predicates (CPs),
              which consist of a non-verbal element-a noun in the current
              study-and a verb. In CPs, once the noun has been read, the exact
              identity of the verb is highly predictable (strong
              predictability); this was confirmed using a sentence completion
              study. In two self-paced reading (SPR) and two eye-tracking (ET)
              experiments, we delayed the appearance of the verb by interposing
              a relative clause (Experiments 1 and 3) or a long PP (Experiments
              2 and 4). We also included a simple Noun-Verb predicate
              configuration with the same distance manipulation; here, the
              exact identity of the verb was not predictable (weak
              predictability). Thus, the design crossed Predictability Strength
              and Distance. We found that, consistent with surprisal, the verb
              in the strong predictability conditions was read faster than in
              the weak predictability conditions. Furthermore, greater
              verb-argument distance led to slower reading times; strong
              predictability did not neutralize or attenuate the locality
              effects. As regards the effect of distance on dependency
              resolution difficulty, these four experiments present evidence in
              favor of working memory accounts of argument-verb dependency
              resolution, and against the surprisal-based expectation account
              of Levy (2008). However, another expectation-based measure,
              entropy, which was computed using the offline sentence completion
              data, predicts reading times in Experiment 1 but not in the other
              experiments. Because participants tend to produce more
              ungrammatical continuations in the long-distance condition in
              Experiment 1, we suggest that forgetting due to memory overload
              leads to greater entropy at the verb.",
  journal  = "Front. Psychol.",
  volume   =  7,
  pages    = "403",
  month    =  mar,
  year     =  2016,
  keywords = "Persian; complex predicates; entropy; expectation; eye-tracking;
              locality; self-paced reading; surprisal",
  language = "en"
}

@INCOLLECTION{Bader1998-cb,
  title     = "Prosodic Influences on Reading Syntactically Ambiguous Sentences",
  booktitle = "Reanalysis in Sentence Processing",
  author    = "Bader, Markus",
  editor    = "Fodor, Janet Dean and Ferreira, Fernanda",
  abstract  = "In this chapter, I will propose a new account for the question
               of why only some local syntactic ambiguities lead to strong
               garden-path effects whereas others do not. This account is based
               on the observation that readers do not only compute syntactic
               structures during reading but also prosodic structures. These
               prosodic structures are a product of the process of phonological
               coding which normally accompanies silent reading. Due to the
               lack of a one-to-one mapping between syntactic and prosodic
               structures, recovery from a syntactic misanalysis may be
               accompanied by the need to replace the original prosodic
               structure or not. According to the prosodic constraint on
               reanalysis, the need to revise the original prosodic structure
               makes recovery from a syntactic misanalysis difficult. Empirical
               evidence for this claim comes from a series of three self-paced
               reading experiments which investigated the German variant of the
               English her-ambiguity. These experiments manipulated the
               prosodic structure of sentences by inserting focus operators
               (focus particles, sentence adverbials) into locally ambiguous
               sentences. Thereby it became possible to independently determine
               the contributions of syntactic and prosodic factors to
               reanalysis. The results show that for the ambiguitiy examined,
               garden-path effects can be predicted on prosodic grounds but not
               on syntactic grounds. This finding supports the claim that for
               certain kinds of syntactic ambiguities reanalysis is
               prosodically constrained.",
  publisher = "Springer Netherlands",
  pages     = "1--46",
  year      =  1998,
  address   = "Dordrecht"
}


@ARTICLE{Pritchett1988-ha,
  title     = "Garden Path Phenomena and the Grammatical Basis of Language
               Processing",
  author    = "Pritchett, Bradley L",
  abstract  = "[A central issue in the field of language processing concerns
               how grammatical theory and parsing are related. Evidence from
               processing breakdown (GARDEN PATH phenomena) reveals the
               conditions under which local ambiguity results in unprocessable
               sentences. These data provide evidence that the processor
               operates by admitting structure which maximally satisfies the
               principles of Government and Binding Theory locally at every
               point during a parse, and that the constraints on syntactic
               reanalysis during processing are also derived from grammatical
               theory. Alternative approaches to parsing are demonstrated to be
               incapable of accounting for the wide range of garden path
               effects.]",
  journal   = "Language",
  publisher = "Linguistic Society of America",
  volume    =  64,
  number    =  3,
  pages     = "539--576",
  year      =  1988
}

@ARTICLE{King1995-ki,
  title    = "Who Did What and When? Using Word- and {Clause-Level} {ERPs} to
              Monitor Working Memory Usage in Reading",
  author   = "King, J W and Kutas, M",
  abstract = "ERPs were recorded from 24 undergraduates as they read sentences
              known to differ in syntactic complexity and working memory
              requirements, namely Object and Subject Relative sentences. Both
              the single-word and multiword analyses revealed significant
              differences due to sentence type, while multiword ERPs also
              showed that sentence type effects differed for Good and Poor
              comprehenders. At the single-word level, ERPs to both verbs in
              Object Relative sentences showed a left anterior negativity
              between 300 and 500 msec postword-onset relative to those to
              Subject Relative verbs. At the multiword level, a slow frontal
              positivity characterized Subject Relative sentences, but was
              absent for Object Relatives. This slow positivity appears to
              index ease of processing or integration. and was more robust in
              Good than in Poor comprehenders.",
  journal  = "J. Cogn. Neurosci.",
  volume   =  7,
  number   =  3,
  pages    = "376--395",
  year     =  1995,
  language = "en",
  url="http://kutaslab.ucsd.edu/people/kutas/pdfs/1995.JCN.376.pdf"
}

@ARTICLE{Frazier1987-es,
  title     = "Syntactic Processing: Evidence from Dutch",
  author    = "Frazier, Lyn",
  journal   = "Nat. Lang. Linguist. Theory",
  publisher = "Springer",
  volume    =  5,
  number    =  4,
  pages     = "519--559",
  year      =  1987,
  url = "https://www.jstor.org/stable/4047505"
}

@article{Shain2021-yk,
  title={Robust effects of working memory demand during naturalistic language comprehension in language-selective cortex},
  author={Shain, Cory and Blank, Idan A and Fedorenko, Evelina and Gibson, Edward and Schuler, William},
  journal={Journal of Neuroscience},
  volume={42},
  number={39},
  pages={7412--7430},
  year={2022},
  publisher={Soc Neuroscience},
  url={https://www.jneurosci.org/content/42/39/7412.abstract}
}

@inproceedings{kuribayashi-etal-2021-lower,
    title = "Lower Perplexity is Not Always Human-Like",
    author = "Kuribayashi, Tatsuki  and
      Oseki, Yohei  and
      Ito, Takumi  and
      Yoshida, Ryo  and
      Asahara, Masayuki  and
      Inui, Kentaro",
    booktitle = "Proceedings of ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.405",
    doi = "10.18653/v1/2021.acl-long.405",
    pages = "5203--5217",
    abstract = "In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization {---}\textit{the lower perplexity a language model has, the more human-like the language model is}{---} in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.",
}

@BOOK{Deng2018-nc,
  title     = "Deep Learning in Natural Language Processing",
  author    = "Deng, Li and Liu, Yang",
  abstract  = "In recent years, deep learning has fundamentally changed the
               landscapes of a number of areas in artificial intelligence,
               including speech, vision, natural language, robotics, and game
               playing. In particular, the striking success of deep learning in
               a wide variety of natural language processing (NLP) applications
               has served as a benchmark for the advances in one of the most
               important tasks in artificial intelligence. This book reviews
               the state of the art of deep learning research and its
               successful applications to major NLP tasks, including speech
               recognition and understanding, dialogue systems, lexical
               analysis, parsing, knowledge graphs, machine translation,
               question answering, sentiment analysis, social computing, and
               natural language generation from images. Outlining and analyzing
               various research frontiers of NLP in the deep learning era, it
               features self-contained, comprehensive chapters written by
               leading researchers in the field. A glossary of technical terms
               and commonly used acronyms in the intersection of deep learning
               and NLP is also provided. The book appeals to advanced
               undergraduate and graduate students, post-doctoral researchers,
               lecturers and industrial researchers, as well as anyone
               interested in deep learning and natural language processing.",
  publisher = "Springer",
  month     =  may,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Alishahi2019-mq,
  title     = "Analyzing and interpreting neural networks for {NLP}: A report
               on the first {BlackboxNLP} workshop",
  author    = "Alishahi, Afra and Chrupa{\l}a, Grzegorz and Linzen, Tal",
  abstract  = "The Empirical Methods in Natural Language Processing (EMNLP)
               2018 workshop BlackboxNLP was dedicated to resources and
               techniques specifically developed for analyzing and
               understanding the inner-workings and representations acquired by
               neural models of language. Approaches included: systematic
               manipulation of input to neural networks and investigating the
               impact on their performance, testing whether interpretable
               knowledge can be decoded from intermediate representations
               acquired by neural networks, proposing modifications to neural
               network architectures to make their knowledge state or generated
               output more explainable, and examining the performance of
               networks on simplified or formal languages. Here we review a
               number of representative studies in each category.",
  journal   = "Natural Language Engineering",
  publisher = "Cambridge University Press",
  volume    =  25,
  number    =  4,
  pages     = "543--557",
  month     =  jul,
  year      =  2019,
  keywords  = "neural networks; interpretability; natural language processing"
}

@ARTICLE{Rogers2020-wx,
  title     = "A Primer in {BERTology}: What We Know About How {BERT} Works",
  author    = "Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna",
  journal   = "TACL",
  publisher = "MIT Press",
  volume    =  8,
  pages     = "842--866",
  month     =  jan,
  year      =  2020
}


@INPROCEEDINGS{Belinkov2020-zr,
  title     = "Interpretability and analysis in neural {NLP}",
  booktitle = "Proceedings of ACL",
  author    = "Belinkov, Yonatan and Gehrmann, Sebastian and Pavlick, Ellie",
  abstract  = "While deep learning has transformed the natural language
               processing (NLP) field and impacted the larger computational
               linguistics community, the rise of neural networks is stained by
               their opaque nature: It is challenging to interpret the inner
               workings of neural …",
  publisher = "aclweb.org",
  pages     = "1--5",
  year      =  2020
}


@ARTICLE{Belinkov2019-ce,
  title     = "Analysis methods in neural language processing: A survey",
  author    = "Belinkov, Yonatan and Glass, James",
  abstract  = "Abstract The field of natural language processing has seen
               impressive progress in recent years, with neural network models
               replacing many of the traditional systems. A plethora of new
               models have been proposed, many of which are thought to be
               opaque compared to their feature-rich counterparts. This has led
               researchers to analyze, interpret, and evaluate neural networks
               in novel and more fine-grained ways. In this survey paper, we
               review analysis methods in neural language processing,
               categorize them according to prominent research trends,
               highlight existing limitations, and point to potential
               directions for future work.",
  journal   = "TACL",
  publisher = "MIT Press - Journals",
  volume    =  7,
  pages     = "49--72",
  month     =  apr,
  year      =  2019,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}


@ARTICLE{Warstadt2019-bq,
  title     = "Neural network acceptability judgments",
  author    = "Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R",
  abstract  = "This paper investigates the ability of artificial neural
               networks to judge the grammatical acceptability of a sentence,
               with the goal of testing their linguistic competence. We
               introduce the Corpus of Linguistic Acceptability (CoLA), a set
               of 10,657 English sentences labeled as …",
  journal   = "TACL",
  publisher = "MIT Press",
  volume    =  7,
  pages     = "625--641",
  year      =  2019
}

@INPROCEEDINGS{Wei2021-vu,
  title     = "Frequency Effects on Syntactic Rule Learning in Transformers",
  booktitle = "Proceedings of EMNLP",
  author    = "Wei, Jason and Garrette, Dan and Linzen, Tal and Pavlick, Ellie",
  abstract  = "Pre-trained language models perform well on a variety of
               linguistic tasks that require symbolic reasoning, raising the
               question of whether such models implicitly represent abstract
               symbols and rules. We investigate this question using the case
               study of BERT's performance on English subject--verb agreement.
               Unlike prior work, we train multiple instances of BERT from
               scratch, allowing us to perform a series of controlled
               interventions at pre-training time. We show that BERT often
               generalizes well to subject--verb pairs that never occurred in
               training, suggesting a degree of rule-governed behavior. We also
               find, however, that performance is heavily influenced by word
               frequency, with experiments showing that both the absolute
               frequency of a verb form, as well as the frequency relative to
               the alternate inflection, are causally implicated in the
               predictions BERT makes at inference time. Closer analysis of
               these frequency effects reveals that BERT's behavior is
               consistent with a system that correctly applies the SVA rule in
               general but struggles to overcome strong training priors and to
               estimate agreement features (singular vs. plural) on infrequent
               lexical items.",
  publisher = "Association for Computational Linguistics",
  pages     = "932--948",
  month     =  nov,
  year      =  2021,
  address   = "Online and Punta Cana, Dominican Republic"
}

@ARTICLE{Lau2017-aw,
  title     = "Grammaticality, Acceptability, and Probability: A Probabilistic
               View of Linguistic Knowledge",
  author    = "Lau, Jey Han and Clark, Alexander and Lappin, Shalom",
  abstract  = "The question of whether humans represent grammatical knowledge
               as a binary condition on membership in a set of well-formed
               sentences, or as a probabilistic property has been the subject
               of debate among linguists, psychologists, and cognitive
               scientists for many decades. Acceptability judgments present a
               serious problem for both classical binary and probabilistic
               theories of grammaticality. These judgements are gradient in
               nature, and so cannot be directly accommodated in a binary
               formal grammar. However, it is also not possible to simply
               reduce acceptability to probability. The acceptability of a
               sentence is not the same as the likelihood of its occurrence,
               which is, in part, determined by factors like sentence length
               and lexical frequency. In this paper, we present the results of
               a set of large-scale experiments using crowd-sourced
               acceptability judgments that demonstrate gradience to be a
               pervasive feature in acceptability judgments. We then show how
               one can predict acceptability judgments on the basis of
               probability by augmenting probabilistic language models with an
               acceptability measure. This is a function that normalizes
               probability values to eliminate the confounding factors of
               length and lexical frequency. We describe a sequence of modeling
               experiments with unsupervised language models drawn from
               state-of-the-art machine learning methods in natural language
               processing. Several of these models achieve very encouraging
               levels of accuracy in the acceptability prediction task, as
               measured by the correlation between the acceptability measure
               scores and mean human acceptability values. We consider the
               relevance of these results to the debate on the nature of
               grammatical competence, and we argue that they support the view
               that linguistic knowledge can be intrinsically probabilistic.",
  journal   = "Cogn. Sci.",
  publisher = "Wiley Online Library",
  volume    =  41,
  number    =  5,
  pages     = "1202--1241",
  month     =  jul,
  year      =  2017,
  keywords  = "Grammaticality; Probabilistic modeling; Syntactic knowledge",
  language  = "en"
}

@ARTICLE{noauthor_undated-gn,
  title     = "Noisy-context surprisal as a human sentence processing cost
               model",
  author    = "Futrell, Richard and Levy, Roger",
  abstract  = "We use the noisy-channel theory of human sentence comprehension
               to develop an incremental processing cost model that unifies and
               extends key features of expectation-based and memory-based
               models. In this model, which we call noisy-context surprisal,
               the processing cost of a word is the surprisal of the word given
               a noisy representation of the preceding context. We show that
               this model accounts for an outstanding puzzle in sentence
               comprehension, language-dependent structural forgetting effects
               (Gibson and Thomas, 1999; Vasishth et al., 2010; Frank et al.,
               2016), which are previously not well modeled by either
               expectation-based or memory-based approaches. Additionally, we
               show that this model derives and generalizes locality effects
               (Gibson, 1998; Demberg and Keller, 2008), a signature prediction
               of memory-based models. We give corpus-based evidence for a key
               assumption in this derivation.",
  publisher = "Association for Computational Linguistics",
  pages     = "688--698",
  month     =  apr,
  year      =  2017,
  address   = "Valencia, Spain"
}

@inproceedings{ueda2021,
  title={On the Relationship between Zipf's Law of Abbreviation and Interfering Noise in Emergent Languages},
  author={Ueda, Ryo and Washio, Koki},
  booktitle={Proceedings of ACL (SRW)},
  pages={60--70},
  year={2021},
  url={https://aclanthology.org/2021.acl-srw.6/}
}

@ARTICLE{Chaabouni2020,
  title     = "Compositionality and Generalization In Emergent Languages",
  author    = "Chaabouni, Rahma and Kharitonov, Eugene and Bouchacourt, Diane
               and Dupoux, Emmanuel and Baroni, Marco",
  abstract  = "Natural language allows us to refer to novel composite concepts
               by combining expressions denoting their parts according to
               systematic rules, a property known as compositionality. In this
               paper, we study whether the language emerging in deep
               multi-agent simulations possesses a similar ability to refer to
               novel primitive combinations, and whether it accomplishes this
               feat by strategies akin to human-language compositionality.
               Equipped with new ways to measure compositionality in emergent
               languages inspired by disentanglement in representation
               learning, we establish three main results: First, given
               sufficiently large input spaces, the emergent language will
               naturally develop the ability to refer to novel composite
               concepts. Second, there is no correlation between the degree of
               compositionality of an emergent language and its ability to
               generalize. Third, while compositionality is not necessary for
               generalization, it provides an advantage in terms of language
               transmission: The more compositional a language is, the more
               easily it will be picked up by new learners, even when the
               latter differ in architecture from the original agents. We
               conclude that compositionality does not arise from simple
               generalization pressure, but if an emergent language does chance
               upon it, it will be more likely to survive and thrive.",
  publisher = "Association for Computational Linguistics",
  pages     = "4427--4442",
  month     =  jul,
  year      =  2020,
  address   = "Online"
}

@INPROCEEDINGS{Kottur2017-uf,
  title     = "Natural Language Does Not Emerge {`}Naturally{'} in
               {Multi-Agent} Dialog",
  booktitle = "Proceedings of EMNLP",
  author    = "Kottur, Satwik and Moura, Jos{\'e} and Lee, Stefan and Batra,
               Dhruv",
  abstract  = "A number of recent works have proposed techniques for end-to-end
               learning of communication protocols among cooperative
               multi-agent populations, and have simultaneously found the
               emergence of grounded human-interpretable language in the
               protocols developed by the agents, learned without any human
               supervision! In this paper, using a Task \& Talk reference game
               between two agents as a testbed, we present a sequence of
               `negative' results culminating in a `positive' one -- showing
               that while most agent-invented languages are effective (i.e.
               achieve near-perfect task rewards), they are decidedly not
               interpretable or compositional. In essence, we find that natural
               language does not emerge `naturally',despite the semblance of
               ease of natural-language-emergence that one may gather from
               recent literature. We discuss how it is possible to coax the
               invented languages to become more and more human-like and
               compositional by increasing restrictions on how two agents may
               communicate.",
  publisher = "Association for Computational Linguistics",
  pages     = "2962--2967",
  month     =  sep,
  year      =  2017,
  address   = "Copenhagen, Denmark"
}

@INCOLLECTION{Shapiro2003-pw,
  title     = "Artificial intelligence ({AI})",
  booktitle = "Encyclopedia of Computer Science",
  author    = "Shapiro, Stuart C",
  abstract  = "Artificial Intelligence (AI) is a field of computer science and
               engineering concerned with the computational understanding of
               what is commonly called intelligent behavior, and with the
               creation of artifacts that exhibit such behavior. This
               definition may be examined more closely by considering the field
               from three points of view: computational psychology,
               computational philosophy, and machine intelligence.",
  publisher = "John Wiley and Sons Ltd.",
  pages     = "89--93",
  month     =  jan,
  year      =  2003,
  address   = "GBR",
  url = "https://dl.acm.org/doi/abs/10.5555/1074100.1074138",
}

@inproceedings{Futrell2017noisy,
  title={Noisy-context surprisal as a human sentence processing cost model},
  author={Futrell, Richard and Levy, Roger},
  booktitle={Proceedings of EACL},
  pages={688--698},
  year={2017},
  url={https://aclanthology.org/E17-1065/}
}
@ARTICLE{Li1984-ql,
  title     = "Subject and {Topic:A} New Typology of Language",
  author    = "Li, C N and Thompson, S A",
  journal   = "Contemporary Linguistics",
  publisher = "en.cnki.com.cn",
  pages = "457--489",
  year      =  1984
}
@inproceedings{kiela-etal-2021-dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of NAACL",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@inproceedings{warstadt-etal-2020-learning,
    title = "Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)",
    author = "Warstadt, Alex  and
      Zhang, Yian  and
      Li, Xiaocheng  and
      Liu, Haokun  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of EMNLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.16",
    doi = "10.18653/v1/2020.emnlp-main.16",
    pages = "217--235",
    abstract = "One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa{\_}BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa{\_}BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",
}


@ARTICLE{Chaabouni2019-rk,
  title    = "Anti-efficient encoding in emergent communication",
  author   = "Chaabouni, Rahma and Kharitonov, E and Dupoux, Emmanuel and
              Baroni, Marco",
  abstract = "Surprisingly, networks develop an anti-efficient encoding scheme,
              in which the most frequent inputs are associated to the longest
              messages, and messages in general are skewed towards the maximum
              length threshold. Despite renewed interest in emergent language
              simulations with neural networks, little is known about the basic
              properties of the induced code, and how they compare to human
              language. One fundamental characteristic of the latter, known as
              Zipf's Law of Abbreviation (ZLA), is that more frequent words are
              efficiently associated to shorter strings. We study whether the
              same pattern emerges when two neural networks, a ``speaker'' and
              a ``listener'', are trained to play a signaling game.
              Surprisingly, we find that networks develop an
              \textbackslashemph\{anti-efficient\} encoding scheme, in which
              the most frequent inputs are associated to the longest messages,
              and messages in general are skewed towards the maximum length
              threshold. This anti-efficient code appears easier to
              discriminate for the listener, and, unlike in human
              communication, the speaker does not impose a contrasting
              least-effort pressure towards brevity. Indeed, when the cost
              function includes a penalty for longer messages, the resulting
              message distribution starts respecting ZLA. Our analysis stresses
              the importance of studying the basic features of emergent
              communication in a highly controlled setup, to ensure the latter
              will not strand too far from human language. Moreover, we present
              a concrete illustration of how different functional pressures can
              lead to successful communication codes that lack basic properties
              of human language, thus highlighting the role such pressures play
              in the latter.",
  journal  = "NeurIPS",
  year     =  2019,
  language = "en"
}


@ARTICLE{Fum2007-st,
  title   = "The cognitive modeling of human behavior: Why a model is
             (sometimes) better than 10,000 words",
  author  = "Fum, Danilo and Missier, Fabio Del and Stocco, Andrea",
  journal = "Cogn. Syst. Res.",
  volume  =  8,
  number  =  3,
  pages   = "135--142",
  month   =  sep,
  year    =  2007
}


@INPROCEEDINGS{Oh2021-ln,
  title     = "Surprisal Estimators for Human Reading Times Need Character
               Models",
  booktitle = "Proceedings of ACL-IJCNLP 2021",
  author    = "Oh, Byung-Doh and Clark, Christian and Schuler, William",
  abstract  = "While the use of character models has been popular in NLP
               applications, it has not been explored much in the context of
               psycholinguistic modeling. This paper presents a character model
               that can be applied to a structural parser-based processing
               model to calculate word generation probabilities. Experimental
               results show that surprisal estimates from a structural
               processing model using this character model deliver
               substantially better fits to self-paced reading, eye-tracking,
               and fMRI data than those from large-scale language models
               trained on much more data. This may suggest that the proposed
               processing model provides a more humanlike account of sentence
               processing, which assumes a larger role of morphology,
               phonotactics, and orthographic complexity than was previously
               thought.",
  pages     = "3746--3757",
  month     =  aug,
  year      =  2021,
  url = "https://aclanthology.org/2021.acl-long.290/"
}

@ARTICLE{Chaabouni2021,
  title    = "Emergent Communication at Scale",
  author   = "Chaabouni, Rahma and Strub, Florian and Altch{\'e}, Florent and
              Tarassov, Eugene and Tallec, Corentin and Davoodi, Elnaz and
              Mathewson, Kory Wallace and Tieleman, Olivier and Lazaridou,
              Angeliki and Piot, Bilal",
  abstract = "Emergent communication aims for a better understanding of human
              language evolution and building more efficient representations.
              We posit that reaching these goals will require scaling up, in
              contrast to a significant amount of literature that focuses on
              setting up small-scale problems to tease out desired properties
              of the emergent languages. We focus on three independent aspects
              to scale up, namely the dataset, task complexity, and population
              size. We provide a first set of results for large populations
              solving complex tasks on realistic large-scale datasets, as well
              as an easy-to-use codebase to enable further experimentation. In
              more complex tasks and datasets, we find that RL training can
              become unstable, but responds well to established stabilization
              techniques. We also identify the need for a different metric than
              topographic similarity, which does not correlate with the
              generalization performances when working with natural images. In
              this context, we probe ease-of-learnability and transfer methods
              to assess emergent languages. Finally, we observe that larger
              populations do not induce robust emergent protocols with high
              generalization performance, leading us to explore different ways
              to leverage population, through voting and imitation learning.",
  month    =  sep,
  year     =  2021
}

@INPROCEEDINGS{Gauthier2019-nj,
  title     = "Linking artificial and human neural representations of language",
  booktitle = "Proceedings of EMNLP-IJCNLP",
  author    = "Gauthier, Jon and Levy, Roger",
  abstract  = "What information from an act of sentence understanding is
               robustly represented in the human brain? We investigate this
               question by comparing sentence encoding models on a brain
               decoding task, where the sentence that an experimental
               participant has seen must be predicted from the fMRI signal
               evoked by the sentence. We take a pre-trained BERT architecture
               as a baseline sentence encoding model and fine-tune it on a
               variety of natural language understanding (NLU) tasks, asking
               which lead to improvements in brain-decoding performance. We
               find that none of the sentence encoding tasks tested yield
               significant increases in brain decoding performance. Through
               further task ablations and representational analyses, we find
               that tasks which produce syntax-light representations yield
               significant improvements in brain decoding performance. Our
               results constrain the space of NLU models that could best
               account for human neural representations of language, but also
               suggest limits on the possibility of decoding fine-grained
               syntactic information from fMRI human neuroimaging.",
  pages     = "529--539",
  month     =  nov,
  year      =  2019,
  url="https://aclanthology.org/D19-1050/"
}

@inproceedings{Michaelov2021-cx,
  title     = "Different kinds of cognitive plausibility: why are transformers
               better than {RNNs} at predicting {N400} amplitude?",
  author    = "Michaelov, James A and Bardolph, Megan D and Coulson, Seana and
               Bergen, Benjamin",
  abstract  = "Author(s): Michaelov, James A; Bardolph, Megan D; Coulson,
               Seana; Bergen, Benjamin | Abstract: Despite being designed for
               performance rather than cognitive plausibility, transformer
               language models have been found to be better at predicting
               metrics used to assess human language comprehension than
               language models with other architectures, such as recurrent
               neural networks. Based on how well they predict the N400, a
               neural signal associated with processing difficulty, we propose
               and provide evidence for one possible explanation---their
               predictions are affected by the preceding context in a way
               analogous to the effect of semantic facilitation in humans.",
  booktitle   = "Proceedings of CogSci 2021",
  volume    =  43,
  year      =  2021,
  keywords  = "Social and Behavioral Sciences",
  url="https://arxiv.org/abs/2107.09648"
}

@inproceedings{Merity2016-rb,
  title={Pointer sentinel mixture models},
  author={Stephen, Merity and Caiming, Xiong and James, Bradbury and Richard, Socher and others},
  booktitle={Proceedings of ICLR 2017},
  year={2017},
  url={https://openreview.net/forum?id=Byj72udxe}
}

@INPROCEEDINGS{Barrett2015-nd,
  title     = "{The Dundee Treebank}",
  booktitle = "Fourteenth International Workshop on Treebanks and Linguistic
               Theories",
  author    = "Barrett, Maria and Agi, {\v Z}eljko and S{\o}gaard, Anders",
  pages     = "242--248",
  year      =  2015
}

@ARTICLE{Van_Schijndel2013-sn,
  title    = "A model of language processing as hierarchic sequential
              prediction",
  author   = "van Schijndel, Marten and Exley, Andy and Schuler, William",
  abstract = "Computational models of memory are often expressed as hierarchic
              sequence models, but the hierarchies in these models are
              typically fairly shallow, reflecting the tendency for memories of
              superordinate sequence states to become increasingly conflated.
              This article describes a broad-coverage probabilistic sentence
              processing model that uses a variant of a left-corner parsing
              strategy to flatten sentence processing operations in parsing
              into a similarly shallow hierarchy of learned sequences. The main
              result of this article is that a broad-coverage model with
              constraints on hierarchy depth can process large newspaper
              corpora with the same accuracy as a state-of-the-art parser not
              defined in terms of sequential working memory operations.",
  journal  = "Top. Cogn. Sci.",
  volume   =  5,
  number   =  3,
  pages    = "522--540",
  month    =  jul,
  year     =  2013,
  keywords = "Computational linguistics; Memory models; Parsing; Sequence
              models; Working memory",
  language = "en",
  url="https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12034"
}

@ARTICLE{Galke2021,
  title    = "Emergent Communication for Understanding Human Language
              Evolution: What's Missing?",
  author   = "Galke, Lukas and Ram, Yoav and Raviv, Limor",
  abstract = "Emergent communication protocols among humans and artificial
              neural network agents do not yet share the same properties and
              show some critical mismatches in results. We describe three
              important phenomena with respect to the emergence and benefits of
              compositionality: ease-of-learning, generalization, and group
              size effects (i.e., larger groups create more systematic
              languages). The latter two are not fully replicated with neural
              agents, which hinders the use of neural emergent communication
              for language evolution research. We argue that one possible
              reason for these mismatches is that key cognitive and
              communicative constraints of humans are not yet integrated.
              Specifically, in humans, memory constraints and the alternation
              between the roles of speaker and listener underlie the emergence
              of linguistic structure, yet these constraints are typically
              absent in neural simulations. We suggest that introducing such
              communicative and cognitive constraints would promote more
              linguistically plausible behaviors with neural agents.",
  journal   = "Emergent Communication Workshop at ICLR 2022",
  month    =  mar,
  year     =  2022,
  url="https://openreview.net/forum?id=rqUGZQ-0XZ5"
}
@inproceedings{Rita2021,
 title={“LazImpa”: Lazy and Impatient neural agents learn to communicate efficiently},
  author={Rita, Mathieu and Chaabouni, Rahma and Dupoux, Emmanuel},
  booktitle={Proceedings of CoNLL},
  pages={335--343},
  year={2020},
  url={https://aclanthology.org/2020.conll-1.26/}
}

@inproceedings{Ambati2016,
  title={Assessing Relative Sentence Complexity using an Incremental CCG Parser},
  author={Ambati, Bharat Ram and Reddy, Siva and Steedman, Mark},
  booktitle={Proceedings of NAACL},
  pages={1051--1057},
  year={2016},
  url={https://aclanthology.org/N16-1120/}
}

@ARTICLE{Gibson2019-oe,
  title    = "How Efficiency Shapes Human Language",
  author   = "Gibson, Edward and Futrell, Richard and Piantadosi, Steven P and
              Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy,
              Roger",
  abstract = "Cognitive science applies diverse tools and perspectives to study
              human language. Recently, an exciting body of work has examined
              linguistic phenomena through the lens of efficiency in usage:
              what otherwise puzzling features of language find explanation in
              formal accounts of how language might be optimized for
              communication and learning? Here, we review studies that deploy
              formal tools from probability and information theory to
              understand how and why language works the way that it does,
              focusing on phenomena ranging from the lexicon through syntax.
              These studies show how a pervasive pressure for efficiency guides
              the forms of natural language and indicate that a rich future for
              language research lies in connecting linguistics to cognitive
              psychology and mathematical theories of communication and
              inference.",
  journal  = "Trends Cogn. Sci.",
  volume   =  23,
  number   =  5,
  pages    = "389--407",
  month    =  may,
  year     =  2019,
  keywords = "communication; cross-linguistic universals; language complexity;
              language efficiency; language evolution; language learnability",
  language = "en",
  url="http://colala.berkeley.edu/papers/gibson2019how.pdf"
}

@INPROCEEDINGS{Omura2018-xx,
  title     = "{{UD}-{J}apanese} {{BCCWJ}}: {U}niversal {D}ependencies
               Annotation for the {B}alanced {C}orpus of {C}ontemporary
               {W}ritten {J}apanese",
  booktitle = "Proceedings of the Second Workshop on Universal Dependencies
               ({{UDW}} 2018)",
  author    = "Omura, Mai and Asahara, Masayuki",
  abstract  = "In this paper, we describe a corpus UD Japanese-BCCWJ that was
               created by converting the Balanced Corpus of Contemporary
               Written Japanese (BCCWJ), a Japanese language corpus, to adhere
               to the UD annotation schema. The BCCWJ already assigns
               dependency information at the level of the bunsetsu (a Japanese
               syntactic unit comparable to the phrase). We developed a program
               to convert the BCCWJ to UD based on this dependency structure,
               and this corpus is the result of completely automatic conversion
               using the program. UD Japanese-BCCWJ is the largest-scale UD
               Japanese corpus and the second-largest of all UD corpora,
               including 1,980 documents, 57,109 sentences, and 1,273k words
               across six distinct domains.",
  pages     = "117--125",
  month     =  nov,
  year      =  2018,
  url = "https://aclanthology.org/W18-6014/"
}

@ARTICLE{Zaheer2020-mj,
  title   = "Big bird: Transformers for longer sequences",
  author  = "Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and
             Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham,
             Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and
             {Others}",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  33,
  pages   = "17283--17297",
  year    =  2020
}

@ARTICLE{Roy2021-jp,
  title     = "Efficient content-based sparse attention with routing
               transformers",
  author    = "Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and
               Grangier, David",
  abstract  = "Self-attention has recently been adopted for a wide range of
               sequence modeling problems. Despite its effectiveness,
               self-attention suffers from quadratic computation and memory
               requirements with respect to sequence length. Successful
               approaches to reduce this complexity focused on attending to
               local sliding windows or a small set of locations independent of
               content. Our work proposes to learn dynamic sparse attention
               patterns that avoid allocating computation and memory to attend
               to content unrelated to the query of interest. This work builds
               upon two lines of research: It combines the modeling flexibility
               of prior work on content-based sparse attention with the
               efficiency gains from approaches based on local, temporal sparse
               attention. Our model, the Routing Transformer, endows
               self-attention with a sparse routing module based on online
               k-means while reducing the overall complexity of attention to O(
               n1.5 d) from O( n2 d) for sequence length n and hidden dimension
               d. We show that our model outperforms comparable sparse
               attention models on language modeling on Wikitext-103 (15.8 vs
               18.3 perplexity), as well as on image generation on ImageNet-64
               (3.43 vs 3.44 bits/dim) while using fewer self-attention layers.
               Additionally, we set a new state-of-the-art on the newly
               released PG-19 data-set, obtaining a test perplexity of 33.2
               with a 22 layer Routing Transformer model trained on sequences
               of length 8192. We open-source the code for Routing Transformer
               in Tensorflow. 1",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  9,
  pages     = "53--68",
  month     =  feb,
  year      =  2021,
  language  = "en"
}


@INPROCEEDINGS{Sun2021-ob,
  title     = "Do {Long-Range} Language Models Actually Use {Long-Range}
               Context?",
  booktitle = "Proceedings of EMNLP",
  author    = "Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew
               and Iyyer, Mohit",
  abstract  = "Language models are generally trained on short, truncated input
               sequences, which limits their ability to use discourse-level
               information present in long-range context to improve their
               predictions. Recent efforts to improve the efficiency of
               self-attention have led to a proliferation of long-range
               Transformer language models, which can process much longer
               sequences than models of the past. However, the ways in which
               such models take advantage of the long-range context remain
               unclear. In this paper, we perform a fine-grained analysis of
               two long-range Transformer language models (including the
               Routing Transformer, which achieves state-of-the-art perplexity
               on the PG-19 long-sequence LM benchmark dataset) that accept
               input sequences of up to 8K tokens. Our results reveal that
               providing long-range context (i.e., beyond the previous 2K
               tokens) to these models only improves their predictions on a
               small set of tokens (e.g., those that can be copied from the
               distant context) and does not help at all for sentence-level
               prediction tasks. Finally, we discover that PG-19 contains a
               variety of different document types and domains, and that
               long-range context helps most for literary novels (as opposed to
               textbooks or magazines).",
  pages     = "807--822",
  month     =  nov,
  year      =  2021,
}
@INPROCEEDINGS{Hao2020-wo,
  title     = "Probabilistic Predictions of People Perusing: Evaluating Metrics
               of Language Model Performance for Psycholinguistic Modeling",
  booktitle = "Proceedings of CMCL",
  author    = "Hao, Yiding and Mendelsohn, Simon and Sterneck, Rachel and
               Martinez, Randi and Frank, Robert",
  abstract  = "By positing a relationship between naturalistic reading times
               and information-theoretic surprisal, surprisal theory (Hale,
               2001; Levy, 2008) provides a natural interface between language
               models and psycholinguistic models. This paper re-evaluates a
               claim due to Goodkind and Bicknell (2018) that a language
               model's ability to model reading times is a linear function of
               its perplexity. By extending Goodkind and Bicknell's analysis to
               modern neural architectures, we show that the proposed relation
               does not always hold for Long Short-Term Memory networks,
               Transformers, and pre-trained models. We introduce an alternate
               measure of language modeling performance called predictability
               norm correlation based on Cloze probabilities measured from
               human subjects. Our new metric yields a more robust relationship
               between language model quality and psycholinguistic modeling
               performance that allows for comparison between models with
               different training configurations.",
  pages     = "75--86",
  month     =  nov,
  year      =  2020,
  address   = "Online",
  url="https://aclanthology.org/2020.cmcl-1.10/"
}

@INPROCEEDINGS{Jaeger2007-wq,
  title     = "{Speakers optimize information density through syntactic
               reduction}",
  booktitle = "Proceedings of NIPS",
  author    = "Jaeger, T and Levy, Roger",
  editor    = "Sch{\"o}lkopf, B and Platt, J and Hoffman, T",
  publisher = "MIT Press",
  volume    =  19,
  pages     = "849--856",
  year      =  2007,
  url="https://papers.nips.cc/paper/2006/hash/c6a01432c8138d46ba39957a8250e027-Abstract.html"
}


@ARTICLE{Gildea2015-uv,
  title    = "Human languages order information efficiently",
  author   = "Gildea, D and Jaeger, T",
  abstract = "Using Monte Carlo simulations over data from five languages, it
              is found that their word orders are efficient for processing in
              terms of both dependency length and local lexical probability,
              suggesting that biases originating in how the brain understands
              language strongly constrain how human languages change over
              generations. Most languages use the relative order between words
              to encode meaning relations. Languages differ, however, in what
              orders they use and how these orders are mapped onto different
              meanings. We test the hypothesis that, despite these differences,
              human languages might constitute different `solutions' to common
              pressures of language use. Using Monte Carlo simulations over
              data from five languages, we find that their word orders are
              efficient for processing in terms of both dependency length and
              local lexical probability. This suggests that biases originating
              in how the brain understands language strongly constrain how
              human languages change over generations.",
  journal  = "ArXiv",
  year     =  2015,
  language = "en"
}

@ARTICLE{Hahn2020-dq,
  title    = "Universals of word order reflect optimization of grammars for
              efficient communication",
  author   = "Hahn, Michael and Jurafsky, Dan and Futrell, Richard",
  abstract = "The universal properties of human languages have been the subject
              of intense study across the language sciences. We report
              computational and corpus evidence for the hypothesis that a
              prominent subset of these universal properties-those related to
              word order-result from a process of optimization for efficient
              communication among humans, trading off the need to reduce
              complexity with the need to reduce ambiguity. We formalize these
              two pressures with information-theoretic and neural-network
              models of complexity and ambiguity and simulate grammars with
              optimized word-order parameters on large-scale data from 51
              languages. Evolution of grammars toward efficiency results in
              word-order patterns that predict a large subset of the major
              word-order correlations across languages.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  117,
  number   =  5,
  pages    = "2347--2353",
  month    =  feb,
  year     =  2020,
  keywords = "computational linguistics; language processing; language
              universals",
  language = "en",
  url="https://www.pnas.org/doi/pdf/10.1073/pnas.1910923117"
}

@ARTICLE{Chomsky2005-he,
  title     = "Three factors in language design",
  author    = "Chomsky, Noam",
  abstract  = "The biolinguistic perspective regards the language faculty as an
               ``organ of the body,'' along with other cognitive systems.
               Adopting it, we expect to find three factors that interact to
               determine (I-) languages attained: genetic endowment (the topic
               of Universal Grammar), experience, and principles that are
               language- or even organism-independent. Research has naturally
               focused on I-languages and UG, the problems of descriptive and
               explanatory adequacy. The Principles-and-Parameters approach
               opened the possibility for serious investigation of the third
               factor, and the attempt to account for properties of language in
               terms of general considerations of computational efficiency,
               eliminating some of the technology postulated as specific to
               language and providing more principled explanation of linguistic
               phenomena",
  journal   = "Linguist. Inq.",
  publisher = "MIT Press - Journals",
  volume    =  36,
  number    =  1,
  pages     = "1--22",
  month     =  jan,
  year      =  2005,
  language  = "en",
  url="https://www.semanticscholar.org/paper/Three-Factors-in-Language-Design-Chomsky/4ae45ba21f294566d55b9149185425e1f7cc2a60"
}
@inproceedings{heafield-etal-2013-scalable,
  title = {Scalable Modified {K}neser-{N}ey Language Model Estimation},
  author = {Heafield, Kenneth and Pouzyrevsky, Ivan and Clark, Jonathan H. and Koehn, Philipp},
  booktitle = {Proceedings of ACL},
  month = aug,
  year = {2013},
  url = {https://www.aclweb.org/anthology/P13-2121},
  pages = {690--696},
  month_numeric = {8}
}

@ARTICLE{Hahn2020-sb,
  title    = "Modeling word and morpheme order in natural language as an
              efficient tradeoff of memory and surprisal",
  author   = "Hahn, Michael and Degen, Judith and Futrell, Richard",
  abstract = "Memory limitations are known to constrain language comprehension
              and production, and have been argued to account for
              crosslinguistic word order regularities. However, a systematic
              assessment of the role of memory limitations in language
              structure has proven elusive, in part because it is hard to
              extract precise large-scale quantitative generalizations about
              language from existing mechanistic models of memory use in
              sentence processing. We provide an architecture-independent
              information-theoretic formalization of memory limitations which
              enables a simple calculation of the memory efficiency of
              languages. Our notion of memory efficiency is based on the idea
              of a memory--surprisal tradeoff : a certain level of average
              surprisal per word can only be achieved at the cost of storing
              some amount of information about past context. Based on this
              notion of memory usage, we advance the Efficient Tradeoff
              Hypothesis: the order of elements in natural language is under
              pressure to enable favorable memory-surprisal tradeoffs. We
              derive that languages enable more efficient tradeoffs when they
              exhibit information locality: when predictive information about
              an element is concentrated in its recent past. We provide
              empirical evidence from three test domains in support of the
              Efficient Tradeoff Hypothesis: a reanalysis of a miniature
              artificial language learning experiment, a large-scale study of
              word order in corpora of 54 languages, and an analysis of
              morpheme order in two agglutinative languages. These results
              suggest that principles of order in natural language can be
              explained via highly generic cognitively motivated principles and
              lend support to efficiency-based models of the structure of human
              language.",
  month    =  jun,
  year     =  2020,
  journal   = "Psychological Review",
  url="https://psyarxiv.com/nu4qz/"
  
}

@MISC{Greenberg1963-rm,
  title        = "Some universals of grammar with particular reference to the
                  order of meaningful elements",
  author       = "Greenberg, Joseph H",
  abstract     = "4. Morphology 5. Conclusion: Some General Principles Appendix
                  I: Basic Data on the 30- Language Sample Appendix II:
                  Distribution of Basic Order Types Appendix III: Universals
                  Restated Aufgabe 2. Wieweit erf{\"u}llt eine Sprache wie
                  Deutsch oder Englisch, die Greenberg als SVO typologisch
                  kennzeichnet, diese Charakterisierung? {\"U}berpr{\"u}fen Sie
                  Universal 1-25 und erl{\"a}utern Sie mit Beispielen! 1.
                  Introduction* The tentative nature of the conclusions set
                  forth here should be evident to the reader. Without much more
                  complete sampling of the …",
  publisher    = "tushik.org",
  year         =  1963,
  howpublished = "\url{http://tushik.org/wp-content/uploads/GRE-order.pdf}",
  note         = "Accessed: 2022-10-18"
}

@article{pimentel-etal-2022-effect,
  title={On the effect of anticipation on reading times},
  author={Pimentel, Tiago and Meister, Clara and Wilcox, Ethan G and Levy, Roger P and Cotterell, Ryan},
  journal={TACL},
  volume={11},
  pages={1624--1642},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
  url={https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00603/118720}
}
@inproceedings{futrell-etal-2018-natural,
    title = "The {Natural Stories Corpus}",
    author = "Futrell, Richard  and
      Gibson, Edward  and
      Tily, Harry J.  and
      Blank, Idan  and
      Vishnevetsky, Anastasia  and
      Piantadosi, Steven  and
      Fedorenko, Evelina",
    booktitle = "Proceedings of LREC 2018",
    month = may,
    year = "2018",
    pages = "76--82",
    url = "https://aclanthology.org/L18-1012",
}
@inproceedings{hu2023promptbased,
      title={Prompting is not a substitute for probability measurements in large language models}, 
      author={Jennifer Hu and Roger Levy},
      year={2023},
      booktitle={Proceedings of EMNLP 2023 (to appear)},
      url = "https://arxiv.org/abs/2305.13264",
}
@inproceedings{kuribayashi-etal-2022-context,
    title = "Context Limitations Make Neural Language Models More Human-Like",
    author = "Kuribayashi, Tatsuki  and
      Oseki, Yohei  and
      Brassard, Ana  and
      Inui, Kentaro",
    booktitle = "Proceedings of EMNLP 2022",
    month = dec,
    year = "2022",
    url = "https://aclanthology.org/2022.emnlp-main.712",
    doi = "10.18653/v1/2022.emnlp-main.712",
    pages = "10421--10436",
    abstract = "Language models (LMs) have been used in cognitive modeling as well as engineering studies{---}they compute information-theoretic complexity metrics that simulate humans{'} cognitive load during reading. This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans. Our results showed that constraining the LMs{'} context access improved their simulation of human reading behavior. We also showed that LM-human gaps in context access were associated with specific syntactic constructions; incorporating syntactic biases into LMs{'} context access might enhance their cognitive plausibility.",
}
@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
  howpublished = {arXiv cs.CL/2203.02155},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{Roark2009-si,
  title     = "Deriving lexical and syntactic expectation-based measures for
               psycholinguistic modeling via incremental top-down parsing",
  booktitle = "Proceedings of EMNLP 2009",
  author    = "Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and
               Pallier, Christophe",
  pages     = "324--333",
  month     =  aug,
  year      =  2009,
  url = "https://aclanthology.org/D09-1034/",
}
@article{opt,
  author       = {Susan Zhang and
                  Stephen Roller and
                  Naman Goyal and
                  Mikel Artetxe and
                  Moya Chen and
                  Shuohui Chen and
                  Christopher Dewan and
                  Mona T. Diab and
                  Xian Li and
                  Xi Victoria Lin and
                  Todor Mihaylov and
                  Myle Ott and
                  Sam Shleifer and
                  Kurt Shuster and
                  Daniel Simig and
                  Punit Singh Koura and
                  Anjali Sridhar and
                  Tianlu Wang and
                  Luke Zettlemoyer},
  title        = {{OPT: Open Pre-trained Transformer Language Models}},
  year         = {2022},
  url          = {https://arxiv.org/abs/2205.01068v4},
  journal = {arXiv preprint},
  volume = {cs.CL/2205.01068v4},
}
@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023},
  journal={(not published)},
  url="https://huggingface.co/tiiuae/falcon-40b"
}
@article{gpt4,
  author       = {OpenAI},
  title        = {{GPT-4 Technical Report}},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/arXiv.2303.08774},
  journal      = {arXiv preprint},
  volume       = {cs.CL/2303.08774v3},
}
@article{Shain2022-qv,
  title={Large-scale evidence for logarithmic effects of word predictability on reading time},
  author={Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={10},
  pages={e2307876121},
  year={2024},
  publisher={National Acad Sciences},
  url={https://www.pnas.org/doi/abs/10.1073/pnas.2307876121}
}
@inproceedings{de-varda-marelli-2023-scaling,
    title = "Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times",
    author = "de Varda, Andrea  and
      Marelli, Marco",
    booktitle = "Proceedings of ACL 2023",
    month = jul,
    year = "2023",
    url = "https://aclanthology.org/2023.acl-short.14",
    doi = "10.18653/v1/2023.acl-short.14",
    pages = "139--149",
    abstract = "Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters.",
}

@ARTICLE{Oh2023-zw,
  title     = "Why does surprisal from larger Transformer-based language models
               provide a poorer fit to human reading times?",
  author    = "Oh, Byung-Doh and Schuler, William",
  abstract  = "Abstract This work presents a linguistic analysis into why
               larger Transformer-based pre-trained language models with more
               parameters and lower perplexity nonetheless yield surprisal
               estimates that are less predictive of human reading times.
               First, regression analyses show a strictly monotonic, positive
               log-linear relationship between perplexity and fit to reading
               times for the more recently released five GPT-Neo variants and
               eight OPT variants on two separate datasets, replicating earlier
               results limited to just GPT-2 (Oh et al., 2022). Subsequently,
               analysis of residual errors reveals a systematic deviation of
               the larger variants, such as underpredicting reading times of
               named entities and making compensatory overpredictions for
               reading times of function words such as modals and conjunctions.
               These results suggest that the propensity of larger
               Transformer-based models to `memorize' sequences during training
               makes their surprisal estimates diverge from humanlike
               expectations, which warrants caution in using pre-trained
               language models to study human language processing.",
  journal   = "TACL",
  publisher = "MIT Press",
  volume    =  11,
  pages     = "336--350",
  month     =  mar,
  year      =  2023,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en",
  url = "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00548/115371/Why-Does-Surprisal-From-Larger-Transformer-Based"
}

@INPROCEEDINGS{Aurnhammer2019-fu,
  title     = "{Comparing gated and simple recurrent neural network
               architectures as models of human sentence processing}",
  booktitle = "Proceedings of {CogSci}",
  author    = "Aurnhammer, C and Frank, S L",
  pages     = "112--118",
  year      =  2019,
  url = "https://repository.ubn.ru.nl/bitstream/handle/2066/213724/213724.pdf?sequence=1&isAllowed=y"
}

@ARTICLE{Demberg2008-fd,
  title    = "Data from eye-tracking corpora as evidence for theories of
              syntactic processing complexity",
  author   = "Demberg, Vera and Keller, Frank",
  abstract = "We evaluate the predictions of two theories of syntactic
              processing complexity, dependency locality theory (DLT) and
              surprisal, against the Dundee Corpus, which contains the
              eye-tracking record of 10 participants reading 51,000 words of
              newspaper text. Our results show that DLT integration cost is not
              a significant predictor of reading times for arbitrary words in
              the corpus. However, DLT successfully predicts reading times for
              nouns. We also find evidence for integration cost effects at
              auxiliaries, not predicted by DLT. For surprisal, we demonstrate
              that an unlexicalized formulation of surprisal can predict
              reading times for arbitrary words in the corpus. Comparing DLT
              integration cost and surprisal, we find that the two measures are
              uncorrelated, which suggests that a complete theory will need to
              incorporate both aspects of processing complexity. We conclude
              that eye-tracking corpora, which provide reading time data for
              naturally occurring, contextualized sentences, can complement
              experimental evidence as a basis for theories of processing
              complexity.",
  journal  = "Cognition",
  volume   =  109,
  number   =  2,
  pages    = "193--210",
  month    =  nov,
  year     =  2008,
  language = "en",
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0010027708001741}
}

@INCOLLECTION{Renyi1961-sw,
  title     = "On Measures of Entropy and Information",
  booktitle = "Proceedings of the Fourth Berkeley Symposium on Mathematical
               Statistics and Probability, Volume 1: Contributions to the
               Theory of Statistics",
  author    = "R{\'e}nyi, Alfr{\'e}d",
  publisher = "University of California Press",
  volume    = "4.1",
  pages     = "547--562",
  month     =  jan,
  year      =  1961,
  language  = "en",
  url = "https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/On-Measures-of-Entropy-and-Information/bsmsp/1200512181",
}

@ARTICLE{Shannon1948-jg,
  title   = "{A Mathematical Theory of Communication}",
  author  = "Shannon, C E",
  journal = "Bell System Technical Journal",
  volume  =  27,
  number  =  3,
  pages   = "379--423",
  year    =  1948,
  url = "https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf"
}
@inproceedings{wolf-etal-2020-transformers,
    title = "{Transformers: State-of-the-Art Natural Language Processing}",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of EMNLP 2020 : System Demonstrations",
    month = oct,
    year = "2020",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}

@article{Wilcox2023-pi,
    author = {Wilcox, Ethan G. and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},
    title = "{Testing the Predictions of Surprisal Theory in 11 Languages}",
    journal = {TACL},
    volume = {11},
    pages = {1451-1470},
    year = {2023},
    month = {12},
    abstract = "{Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00612},
    url = {https://doi.org/10.1162/tacl\_a\_00612},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00612/2196877/tacl\_a\_00612.pdf},
}

@ARTICLE{Henderson2016-bf,
  title    = "Language structure in the brain: A fixation-related {fMRI} study
              of syntactic surprisal in reading",
  author   = "Henderson, John M and Choi, Wonil and Lowder, Matthew W and
              Ferreira, Fernanda",
  abstract = "How is syntactic analysis implemented by the human brain during
              language comprehension? The current study combined methods from
              computational linguistics, eyetracking, and fMRI to address this
              question. Subjects read passages of text presented as paragraphs
              while their eye movements were recorded in an MRI scanner. We
              parsed the text using a probabilistic context-free grammar to
              isolate syntactic difficulty. Syntactic difficulty was quantified
              as syntactic surprisal, which is related to the expectedness of a
              given word's syntactic category given its preceding context. We
              compared words with high and low syntactic surprisal values that
              were equated for length, frequency, and lexical surprisal, and
              used fixation-related (FIRE) fMRI to measure neural activity
              associated with syntactic surprisal for each fixated word. We
              observed greater neural activity for high than low syntactic
              surprisal in two predicted cortical regions previously identified
              with syntax: left inferior frontal gyrus (IFG) and less robustly,
              left anterior superior temporal lobe (ATL). These results support
              the hypothesis that left IFG and ATL play a central role in
              syntactic analysis during language comprehension. More generally,
              the results suggest a broader cortical network associated with
              syntactic prediction that includes increased activity in
              bilateral IFG and insula, as well as fusiform and right lingual
              gyri.",
  journal  = "Neuroimage",
  volume   =  132,
  pages    = "293--300",
  month    =  may,
  year     =  2016,
  keywords = "Eye movements; Language; Reading; Surprisal; Syntax; fMRI",
  language = "en",
  url = "https://pubmed.ncbi.nlm.nih.gov/26908322/"
}

@MISC{Katzir2023,
  title        = "Why large language models are poor theories of human
                  linguistic cognition. A reply to Piantadosi (2023).",
  author = "Katzir, Roni ",
  abstract     = "In a recent manuscript entitled ``Modern language models
                  refute Chomsky's approach to language'', Steven Piantadosi
                  proposes that large language models such as GPT-3 can serve
                  as serious theories of hum - lingbuzz, the linguistics
                  archive",
  howpublished = "\url{https://ling.auf.net/lingbuzz/007190}",
  note         = "Accessed: 2023-10-25",
  year          =  2023,
  language     = "en"
}

@MISC{Begus2023,
  title        = "Large Linguistic Models: Analyzing theoretical linguistic
                  abilities of {LLMs}",
  author       = "Begu\v{s}, Ga\v{s}per and Dąbkowski, Maksymilian and Rhodes, Ryan",
  abstract     = "The performance of large language models (LLMs) has recently
                  improved to the point where the models can perform well on
                  many language tasks. We show here that for the first time,
                  the models can also g - lingbuzz, the linguistics archive",
  howpublished = "\url{https://ling.auf.net/lingbuzz/007269}",
  note         = "Accessed: 2023-10-25",
  year          =  2023,
  language     = "en"
}


@article{Dentella2023-jg,
  title={Testing {AI} performance on less frequent aspects of language reveals insensitivity to underlying meaning},
  author={Dentella, Vittoria and Murphy, Elliot and Marcus, Gary and Leivada, Evelina},
  journal={arXiv preprint arXiv:2302.12313},
  year={2023},
  url={https://arxiv.org/abs/2302.12313}
}
@inproceedings{blevins-etal-2023-prompting,
    title = "Prompting Language Models for Linguistic Structure",
    author = "Blevins, Terra  and
      Gonen, Hila  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of ACL 2023",
    month = jul,
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.367",
    doi = "10.18653/v1/2023.acl-long.367",
    pages = "6649--6663",
}

@INPROCEEDINGS{Li2022-hm,
  title     = "Probing via Prompting",
  booktitle = "Proceedings of NAACL 2022",
  author    = "Li, Jiaoda and Cotterell, Ryan and Sachan, Mrinmaya",
  abstract  = "Probing is a popular approach to understand what linguistic
               information is contained in the representations of pre-trained
               language models. However, the mechanism of selecting the probe
               model has recently been subject to intense debate, as it is not
               clear if the probes are merely extracting information or
               modelling the linguistic property themselves. To address this
               challenge, this paper introduces a novel model-free approach to
               probing via prompting, which formulates probing as a prompting
               task. We conduct experiments on five probing tasks and show that
               PP is comparable or better at extracting information than
               diagnostic probes while learning much less on its own. We
               further combine the probing via prompting approach with pruning
               to analyze where the model stores the linguistic information in
               its architecture. Finally, we apply the probing via prompting
               approach to examine the usefulness of a linguistic property for
               pre-training by removing the heads that are essential to it and
               evaluating the resulting model's performance on language
               modeling.",
  pages     = "1144--1157",
  month     =  jul,
  year      =  2022,
  url = "https://aclanthology.org/2022.naacl-main.84/",
}
@book{taylor2012mental,
  title={The mental corpus: How language is represented in the mind},
  author={Taylor, John R},
  year={2012},
  publisher={Oxford University Press}
}

@INPROCEEDINGS{Lior2023-jb,
  title         = "Comparing Humans and Models on a Similar Scale: Towards
                   Cognitive Gender Bias Evaluation in Coreference Resolution",
  author        = "Lior, Gili and Stanovsky, Gabriel",
  abstract      = "Spurious correlations were found to be an important factor
                   explaining model performance in various NLP tasks (e.g.,
                   gender or racial artifacts), often considered to be
                   ''shortcuts'' to the actual task. However, humans tend to
                   similarly make quick (and sometimes wrong) predictions based
                   on societal and cognitive presuppositions. In this work we
                   address the question: can we quantify the extent to which
                   model biases reflect human behaviour? Answering this
                   question will help shed light on model performance and
                   provide meaningful comparisons against humans. We approach
                   this question through the lens of the dual-process theory
                   for human decision-making. This theory differentiates
                   between an automatic unconscious (and sometimes biased)
                   ''fast system'' and a ''slow system'', which when triggered
                   may revisit earlier automatic reactions. We make several
                   observations from two crowdsourcing experiments of gender
                   bias in coreference resolution, using self-paced reading to
                   study the ''fast'' system, and question answering to study
                   the ''slow'' system under a constrained time setting. On
                   real-world data humans make $\sim$3\% more gender-biased
                   decisions compared to models, while on synthetic data models
                   are $\sim$12\% more biased.",
  month         =  may,
  year          =  2023,
  booktitle = "Proceedings of CogSci 2023",
  volume  =  45,
URL = "https://escholarship.org/uc/item/5wz4869k#main"
}
@inproceedings{meister-etal-2022-analyzing,
    title = "Analyzing Wrap-Up Effects through an Information-Theoretic Lens",
    author = "Meister, Clara  and
      Pimentel, Tiago  and
      Clark, Thomas  and
      Cotterell, Ryan  and
      Levy, Roger",
    booktitle = "Proceedings of ACL 2022",
    month = may,
    year = "2022",
    url = "https://aclanthology.org/2022.acl-short.3",
    doi = "10.18653/v1/2022.acl-short.3",
    pages = "20--28",
}
@article{lin2022teaching,
  title={Teaching Models to Express Their Uncertainty in Words},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={Transactions on Machine Learning Research},
  year={2022},
  url={https://openreview.net/pdf?id=8s8K2UZGTZ}
}
@misc{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free {Dolly}: Introducing the World's First Truly Open Instruction-Tuned {LLM}},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford {Alpaca}: An Instruction-following {LLaMA} model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@inproceedings{noji-oseki-2021-effective,
    title = "Effective Batching for Recurrent Neural Network Grammars",
    author = "Noji, Hiroshi  and
      Oseki, Yohei",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.findings-acl.380",
    doi = "10.18653/v1/2021.findings-acl.380",
    pages = "4340--4352",
}
@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021},
  url={https://arxiv.org/abs/2112.00861}
}
@incollection{grice1975logic,
  title={Logic and conversation},
  author={Grice, Herbert P},
  booktitle={Speech acts},
  pages={41--58},
  year={1975},
  publisher={Brill},
  url={https://edge.edx.org/asset-v1:Brown+CSCI2951-K+2015_T2+type@asset+block/grice75.pdf}
}
@inproceedings{nair2023words,
  title={Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?},
  author={Nair, Sathvik and Resnik, Philip},
  booktitle={Findings of EMNLP2023},
  year={2023},
  url={https://arxiv.org/abs/2310.17774}
}
@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and Raja, Arun and others},
  booktitle={ICLR 2022-Tenth International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=9Vrb9D0WI4}
}
@inproceedings{wei2021finetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=gEZrGCozdqR}
}
@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022},
  url={https://arxiv.org/abs/2209.14375}
}
@article{LlaMa,
  title={{LLaMA 2}: Open foundation and fine-tuned chat models},
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
  url={https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/}
}
@book{chomsky1965aspects,
  title={Aspects of the Theory of Syntax},
  author={Chomsky, Noam},
  publisher={MIT Press},
  year={1965}
}
@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022},
  url={https://arxiv.org/abs/2207.05221}
}
@misc{robyn_speer_2022_7199437,
  author       = {Robyn Speer},
  title        = {rspeer/wordfreq: v3.0},
  month        = sep,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v3.0.2},
  doi          = {10.5281/zenodo.7199437},
  url          = {https://doi.org/10.5281/zenodo.7199437}
}
@inproceedings{pouw-etal-2023-cross,
    title = "Cross-Lingual Transfer of Cognitive Processing Complexity",
    author = "Pouw, Charlotte  and
      Hollenstein, Nora  and
      Beinborn, Lisa",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of EACL 2023",
    month = may,
    year = "2023",
    url = "https://aclanthology.org/2023.findings-eacl.49",
    doi = "10.18653/v1/2023.findings-eacl.49",
    pages = "655--669",
}
@article{siegelman2022expanding,
  title={Expanding horizons of cross-linguistic research on reading: The Multilingual Eye-movement Corpus (MECO)},
  author={Siegelman, Noam and Schroeder, Sascha and Acart{\"u}rk, Cengiz and Ahn, Hee-Don and Alexeeva, Svetlana and Amenta, Simona and Bertram, Raymond and Bonandrini, Rolando and Brysbaert, Marc and Chernova, Daria and others},
  journal={Behavior research methods},
  volume={54},
  number={6},
  pages={2843--2863},
  year={2022},
  publisher={Springer},
  url={https://link.springer.com/epdf/10.3758/s13428-021-01772-6?sharing_token=As4e3osuA15IaUCKtCvDT5AH0g46feNdnc402WrhzyoEtpF3alySPm1lAWocS1ewk9OZlpPc3CqibACC23iBC_nacc6BD4_GPYLuUZJAvfWHoa8e0hjmhhFn9fLIgIRd3VzSfjlcpQ3gS4EiUY2YpRXjDSh3hB5Zx5kZpkk4yIQ%3D}
}
@inproceedings{seabold2010statsmodels,
  title={statsmodels: Econometric and statistical modeling with {Python}},
  author={Seabold, Skipper and Perktold, Josef},
  booktitle={9th Python in Science Conference},
  year={2010},
url={https://www.statsmodels.org/stable/index.html}
}

@inproceedings{Timkey2023-dy,
  title         = "A Language Model with Limited Memory Capacity Captures
                   Interference in Human Sentence Processing",
  author        = "Timkey, William and Linzen, Tal",
  abstract      = "Two of the central factors believed to underpin human
                   sentence processing difficulty are expectations and
                   retrieval from working memory. A recent attempt to create a
                   unified cognitive model integrating these two factors relied
                   on the parallels between the self-attention mechanism of
                   transformer language models and cue-based retrieval theories
                   of working memory in human sentence processing (Ryu and
                   Lewis 2021). While Ryu and Lewis show that attention
                   patterns in specialized attention heads of GPT-2 are
                   consistent with similarity-based interference, a key
                   prediction of cue-based retrieval models, their method
                   requires identifying syntactically specialized attention
                   heads, and makes the cognitively implausible assumption that
                   hundreds of memory retrieval operations take place in
                   parallel. In the present work, we develop a recurrent neural
                   language model with a single self-attention head, which more
                   closely parallels the memory system assumed by cognitive
                   theories. We show that our model's single attention head
                   captures semantic and syntactic interference effects
                   observed in human experiments.",
  month         =  oct,
  year          =  2023,
  booktitle = "Findings of EMNLP 2023",
  archivePrefix = "arXiv (to appear in EMNLP 2023 Findings)",
  primaryClass  = "cs.CL",
  eprint        = "2310.16142",
  url = "https://arxiv.org/abs/2310.16142"
}

@misc{Liu2023-ib,
  title         = "Improving fit to human reading times via temperature-scaled
                   surprisal",
  author        = "Liu, Tong and {\v S}krjanec, Iza and Demberg, Vera",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  howpublished = {arXiv cs.CL/2311.09325},
  primaryClass  = "cs.CL",
  eprint        = "2311.09325",
  URL   =   "https://arxiv.org/abs/2311.09325"
}

@book{beinborn2023cognitive,
  title={Cognitive Plausibility in Natural Language Processing},
  author={Beinborn, Lisa and Hollenstein, Nora},
  year={2023},
  publisher={Springer Nature}
}

@misc{Aw2023-tx,
  title         = "Instruction-tuning Aligns {LLMs} to the Human Brain",
  author        = "Aw, Khai Loong and Montariol, Syrielle and AlKhamissi, Badr
                   and Schrimpf, Martin and Bosselut, Antoine",
  month         =  dec,
  year          =  2023,
  howpublished  = {arXiv cs.CL/2312.00575},
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.00575",
  URL = "https://arxiv.org/abs/2312.00575"
}

@misc{Kauf2024-lx,
  title         = "Comparing Plausibility Estimates in Base and
                   {Instruction-Tuned} Large Language Models",
  author        = "Kauf, Carina and Chersoni, Emmanuele and Lenci, Alessandro
                   and Fedorenko, Evelina and Ivanova, Anna A",
  month         =  mar,
  year          =  2024,
  howpublished  = {arXiv cs.CL/2403.14859},
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2403.14859",
  URL = "https://arxiv.org/abs/2403.14859"
}

@INPROCEEDINGS{Su2023-kc,
  title     = "One Embedder, Any Task: {Instruction-Finetuned} Text Embeddings",
  booktitle = "Findings of ACL 2023",
  author    = "Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong
               and Hu, Yushi and Ostendorf, Mari and Yih, Wen-Tau and Smith,
               Noah A and Zettlemoyer, Luke and Yu, Tao",
  editor    = "Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki",
  pages     = "1102--1121",
  month     =  jul,
  year      =  2023,
}
@misc{spacy,
title="spaCy: Industrial-strength Natural Language Processing in Python",
author = "Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane",
year = 2020,
}


@InProceedings{pmlr-v202-zhou23g,
  title = 	 {Controlled Text Generation with Natural Language Instructions},
  author =       {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Wilcox, Ethan and Cotterell, Ryan and Sachan, Mrinmaya},
  booktitle = 	 {Proceedings of ICML 2023},
  pages = 	 {42602--42613},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  month = 	 {23--29 Jul},
  pdf = 	 {https://proceedings.mlr.press/v202/zhou23g/zhou23g.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zhou23g.html},
}
@inproceedings{xu-etal-2023-baize,
    title = "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
    author = "Xu, Canwen  and
      Guo, Daya  and
      Duan, Nan  and
      McAuley, Julian",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of EMNLP 2023",
    month = dec,
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.385",
    doi = "10.18653/v1/2023.emnlp-main.385",
    pages = "6268--6278",
}
@book{alishahi2010computational,
  title={Computational modeling of human language acquisition},
  author={Alishahi, Afra},
  year={2010},
  publisher={Morgan \& Claypool Publishers},
  url={https://link.springer.com/book/10.1007/978-3-031-02140-4}
}
@article{zhang2023instruction,
  title={Instruction Tuning for Large Language Models: A Survey},
      author={Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023},
  url={https://arxiv.org/abs/2308.10792}
}
@inproceedings{wilcox-etal-2023-language,
    title = "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",
    author = "Wilcox, Ethan  and
      Meister, Clara  and
      Cotterell, Ryan  and
      Pimentel, Tiago",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of EMNLP 2023",
    month = dec,
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-main.466",
    doi = "10.18653/v1/2023.emnlp-main.466",
    pages = "7503--7511",
}
@inproceedings{kuribayashi-etal-2024-psychometric,
    title = "Psychometric Predictive Power of Large Language Models",
    author = "Kuribayashi, Tatsuki  and
      Oseki, Yohei  and
      Baldwin, Timothy",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of NAACL 2024",
    month = jun,
    year = "2024",
    url = "https://aclanthology.org/2024.findings-naacl.129",
    doi = "10.18653/v1/2024.findings-naacl.129",
    pages = "1983--2005",
}
@ARTICLE{Wilcox2024-qx,
  title    = "Bigger is not always better: The importance of human-scale
              language modeling for psycholinguistics",
  author   = "Wilcox, Ethan Gotlieb and Hu, Michael and Mueller, Aaron and
              Linzen, Tal and Warstadt, Alex and Choshen, Leshem and Zhuang,
              Chengxu and Cotterell, Ryan and Williams, Adina",
  journal  = "PsyArXiv",
  abstract = "Neural network language models can learn a surprising amount about
              language by predicting upcoming words in a corpus. Recent language
              technologies work has demonstrated that large performance
              improvements can arise from simply increasing (``scaling'') the
              size of the data sets they are trained on (and, correspondingly,
              the number of parameters in those models); accordingly, many
              contemporary systems are trained on trillions of words. While
              largely beneficial to performance on language applications,
              scaling has several downsides for both computational
              psycholinguistics and natural language processing research. We
              discuss the scientific challenges presented by scaling, as well as
              the benefits that would result from human-scale language modeling
              research. In the second half of this paper, we report on takeaways
              from two efforts to bring about human-scale language model
              pretraining. First, we report on the first iteration of the BabyLM
              Challenge, a shared task organized by the authors that asked
              participants to train a language model on 100 million words or
              less. Second, we present experiments to answer open questions from
              the findings of the BabyLM Challenge: namely, are a significant
              amount of computational resources required to achieve high
              performance, even at such small scales? We find that high
              performance can be achieved at small data scales and with typical
              academic-scale computational resources.",
  month    =  jul,
  year     =  2024,
  language = "en",
  url="https://osf.io/preprints/psyarxiv/rfwgd"
}
@inproceedings{
michaelov2024revenge,
title={Revenge of the Fallen? Recurrent Models Match Transformers at Predicting Human Language Comprehension Metrics},
author={James Michaelov and Catherine Arnett and Ben Bergen},
booktitle={COLM 2024},
year={2024},
url={https://openreview.net/forum?id=amhPBLFYWv}
}
@article{michaelov2023strong,
    author = {Michaelov, James A. and Bardolph, Megan D. and Van Petten, Cyma K. and Bergen, Benjamin K. and Coulson, Seana},
    title = "{Strong Prediction: Language Model Surprisal Explains Multiple N400 Effects}",
    journal = {Neurobiology of Language},
    volume = {5},
    number = {1},
    pages = {107-135},
    year = {2024},
    month = {04},
    abstract = "{Theoretical accounts of the N400 are divided as to whether the amplitude of the N400 response to a stimulus reflects the extent to which the stimulus was predicted, the extent to which the stimulus is semantically similar to its preceding context, or both. We use state-of-the-art machine learning tools to investigate which of these three accounts is best supported by the evidence. GPT-3, a neural language model trained to compute the conditional probability of any word based on the words that precede it, was used to operationalize contextual predictability. In particular, we used an information-theoretic construct known as surprisal (the negative logarithm of the conditional probability). Contextual semantic similarity was operationalized by using two high-quality co-occurrence-derived vector-based meaning representations for words: GloVe and fastText. The cosine between the vector representation of the sentence frame and final word was used to derive contextual cosine similarity estimates. A series of regression models were constructed, where these variables, along with cloze probability and plausibility ratings, were used to predict single trial N400 amplitudes recorded from healthy adults as they read sentences whose final word varied in its predictability, plausibility, and semantic relationship to the likeliest sentence completion. Statistical model comparison indicated GPT-3 surprisal provided the best account of N400 amplitude and suggested that apparently disparate N400 effects of expectancy, plausibility, and contextual semantic similarity can be reduced to variation in the predictability of words. The results are argued to support predictive coding in the human language network.}",
    issn = {2641-4368},
    doi = {10.1162/nol_a_00105},
    url = {https://doi.org/10.1162/nol\_a\_00105},
    eprint = {https://direct.mit.edu/nol/article-pdf/5/1/107/2361099/nol\_a\_00105.pdf},
}
@article{schrimpf2020-qa,
  title={The neural architecture of language: Integrative modeling converges on predictive processing},
  author={Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={45},
  pages={e2105646118},
  year={2021},
  publisher={National Acad Sciences},
  url={https://www.pnas.org/doi/abs/10.1073/pnas.2105646118}
}
@ARTICLE{Forster2009-dp,
  title     = "The maze task: measuring forced incremental sentence processing
               time",
  author    = "Forster, Kenneth I and Guerrera, Christine and Elliot, Lisa",
  journal   = "Behavior research methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  41,
  number    =  1,
  pages     = "163--171",
  abstract  = "The maze task is an online measure of sentence processing time
               that provides an alternative to the standard moving window
               version of self-paced reading. Rather than each word of the
               sentence being presented in succession, two words are presented
               at the same time, and the participant must choose which word is a
               grammatical continuation of the sentence. This procedure forces
               the reader into an incremental mode of processing in which each
               word must be fully integrated with the preceding context before
               the next word can be considered. Previous research with this
               technique has not considered whether it is sufficiently sensitive
               to syntactic complexity effects or to garden path effects. Four
               experiments are reported demonstrating that reliable differences
               in processing time for subject relatives and object relatives can
               be obtained, and that this technique generates garden path
               effects that correspond closely with the data from eyetracking
               experiments, but without the spillover effects that are sometimes
               obtained with eyetracking. It is also shown that the task is
               sensitive to word frequency effects, producing estimates well in
               excess of those found with eyetracking.",
  month     =  feb,
  year      =  2009,
  language  = "en",
  URL = "https://link.springer.com/article/10.3758/brm.41.1.163"
}

@inproceedings{icml19shallowdeepnetworks,
  Title = {{Shallow-Deep Networks}: Understanding and Mitigating Network Overthinking},
  Author = {Yi\u{g}itcan Kaya and Sanghyun Hong and Tudor Dumitras},
  Booktitle = {Proceedings of ICML 2019},
  Month = {Jun},
  Year = {2019},
  url = {https://proceedings.mlr.press/v97/kaya19a.html}
}
                 
@ARTICLE{Zhou2020-vz,
  title    = "{BERT} loses Patience: Fast and robust inference with Early Exit",
  author   = "Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian
              and Xu, Ke and Wei, Furu",
  journal  = "NeurIPS 2020",
  volume   = "abs/2006.04152",
  abstract = "In this paper, we propose Patience-based Early Exit, a
              straightforward yet effective inference method that can be used as
              a plug-and-play technique to simultaneously improve the efficiency
              and robustness of a pretrained language model (PLM). To achieve
              this, our approach couples an internal-classifier with each layer
              of a PLM and dynamically stops inference when the intermediate
              predictions of the internal classifiers remain unchanged for a
              pre-defined number of steps. Our approach improves inference
              efficiency as it allows the model to make a prediction with fewer
              layers. Meanwhile, experimental results with an ALBERT model show
              that our method can improve the accuracy and robustness of the
              model by preventing it from overthinking and exploiting multiple
              classifiers for prediction, yielding a better accuracy-speed
              trade-off compared to existing early exit methods.",
  month    =  jun,
  year     =  2020,
  url = "https://proceedings.neurips.cc/paper/2020/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html"
}

@misc{logitlens,
title= "interpreting gpt: the logit lens.",
author="nostalgebraist",
year="2020",
url="https://www.lesswrong.
com/posts/AcKRB8wDpdaN6v6ru/
interpreting-gpt-the-logit-lens"
}
@article{frazier1982making,
  title={Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences},
  author={Frazier, Lyn and Rayner, Keith},
  journal={Cognitive psychology},
  volume={14},
  number={2},
  pages={178--210},
  year={1982},
  publisher={Elsevier}
}
@ARTICLE{Hahn2022-ib,
  title    = "A resource-rational model of human processing of recursive
              linguistic structure",
  author   = "Hahn, Michael and Futrell, Richard and Levy, Roger and Gibson,
              Edward",
  journal  = "Proceedings of the National Academy of Sciences of the United States of America",
  volume   =  119,
  number   =  43,
  pages    = "e2122602119",
  abstract = "A major goal of psycholinguistic theory is to account for the
              cognitive constraints limiting the speed and ease of language
              comprehension and production. Wide-ranging evidence demonstrates a
              key role for linguistic expectations: A word's predictability, as
              measured by the information-theoretic quantity of surprisal, is a
              major determinant of processing difficulty. But surprisal, under
              standard theories, fails to predict the difficulty profile of an
              important class of linguistic patterns: the nested hierarchical
              structures made possible by recursion in human language. These
              nested structures are better accounted for by psycholinguistic
              theories of constrained working memory capacity. However, progress
              on theory unifying expectation-based and memory-based accounts has
              been limited. Here we present a unified theory of a rational
              trade-off between precision of memory representations with ease of
              prediction, a scaled-up computational implementation using
              contemporary machine learning methods, and experimental evidence
              in support of the theory's distinctive predictions. We show that
              the theory makes nuanced and distinctive predictions for
              difficulty patterns in nested recursive structures predicted by
              neither expectation-based nor memory-based theories alone. These
              predictions are confirmed 1) in two language comprehension
              experiments in English, and 2) in sentence completions in English,
              Spanish, and German. More generally, our framework offers
              computationally explicit theory and methods for understanding how
              memory constraints and prediction interact in human language
              comprehension and production.",
  month    =  oct,
  year     =  2022,
  keywords = "language processing; resource-rationality; surprisal",
  language = "en",
  url="https://pubmed.ncbi.nlm.nih.gov/36260742/"
}
@article{Boyce2023AmazeON,
  title={A-maze of Natural Stories: Comprehension and surprisal in the Maze task},
  author={Veronica Boyce and Roger Philip Levy},
  journal={Glossa Psycholinguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258086581}
}
@ARTICLE{Federmeier2007-qg,
  title     = "Multiple effects of sentential constraint on word processing",
  author    = "Federmeier, Kara D and Wlotko, Edward W and De Ochoa-Dewald,
               Esmeralda and Kutas, Marta",
  journal   = "Brain Research",
  publisher = "Elsevier BV",
  volume    =  1146,
  pages     = "75--84",
  abstract  = "Behavioral and electrophysiological studies have uncovered
               different patterns of constraint effects on the processing of
               words in sentences. Whereas response time measures have indicated
               a reduced scope of facilitation from strongly constraining
               contexts, event-related brain potential (ERP) measures have
               instead revealed enhanced facilitation for semantically related
               endings in such sentences. Given this disparity, and the
               concomitant possibility of functionally separable stages of
               context effects, the current study jointly examined expectancy
               (cloze probability) and constraint effects on the ERP response to
               words. Expected and unexpected (but plausible) words completed
               strongly and weakly constraining sentences; unexpected items were
               matched for contextual fit across the two levels of constraint
               and were semantically unrelated to the most expected endings.
               N400 amplitudes were graded by expectancy but unaffected by
               constraint and seemed to index the benefit of contextual
               information. However, a later effect, in the form of increased
               frontal positivity from 500 to 900 ms post-stimulus-onset,
               indicated a possible cost associated with the processing of
               unexpected words in strongly constraining contexts.",
  month     =  may,
  year      =  2007,
  language  = "en",
  URL = "https://www.sciencedirect.com/science/article/abs/pii/S0006899306019986"
}
@ARTICLE{Hubbard2019-vz,
  title     = "Downstream behavioral and electrophysiological consequences of
               word prediction on recognition memory",
  author    = "Hubbard, Ryan J and Rommers, Joost and Jacobs, Cassandra L and
               Federmeier, Kara D",
  journal   = "Frontiers in human neuroscience",
  publisher = "Frontiers Media SA",
  volume    =  13,
  pages     =  291,
  abstract  = "When people process language, they can use context to predict
               upcoming information, influencing processing and comprehension as
               seen in both behavioral and neural measures. Although numerous
               studies have shown immediate facilitative effects of confirmed
               predictions, the downstream consequences of prediction have been
               less explored. In the current study, we examined those
               consequences by probing participants' recognition memory for
               words after they read sets of sentences. Participants read
               strongly and weakly constraining sentences with expected or
               unexpected endings (``I added my name to the list/basket''), and
               later were tested on their memory for the sentence endings while
               EEG was recorded. Critically, the memory test contained words
               that were predictable (``list'') but were never read
               (participants saw ``basket''). Behaviorally, participants showed
               successful discrimination between old and new items, but false
               alarmed to the expected-item lures more often than to new items,
               showing that predicted words or concepts can linger, even when
               predictions are disconfirmed. Although false alarm rates did not
               differ by constraint, event-related potentials (ERPs) differed
               between false alarms to strongly and weakly predictable words.
               Additionally, previously unexpected (compared to previously
               expected) endings that appeared on the memory test elicited
               larger N1 and LPC amplitudes, suggesting greater attention and
               episodic recollection. In contrast, highly predictable sentence
               endings that had been read elicited reduced LPC amplitudes during
               the memory test. Thus, prediction can facilitate processing in
               the moment, but can also lead to false memory and reduced
               recollection for predictable information.",
  month     =  aug,
  year      =  2019,
  keywords  = "ERP; false memory; language comprehension; prediction;
               recognition",
  language  = "en",
  url = "https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2019.00291/full"
}
@ARTICLE{Wlotko2012-eh,
  title     = "So that's what you meant! Event-related potentials reveal
               multiple aspects of context use during construction of
               message-level meaning",
  author    = "Wlotko, Edward W and Federmeier, Kara D",
  journal   = "Neuroimage",
  publisher = "Elsevier BV",
  volume    =  62,
  number    =  1,
  pages     = "356--366",
  abstract  = "Factors that modulate the influence of contextual information on
               semantic processing in language comprehension have been
               thoroughly investigated with the N400 component of the
               event-related potential (ERP), a direct measure of initial
               contact with semantic memory. Although context has a strong and
               immediate impact on processing, multiple mechanisms contribute to
               the construction of message-level representations during normal
               comprehension. Some of these may be engaged after or concurrent
               with the formation of an initial meaning representation, and can
               then serve to revise or reshape meaning. In this study, ERPs were
               recorded while participants read plausible sentences that
               continuously varied in the amount of contextual constraint for
               the sentence-final word, defined via extensive norming data
               including the range of possible alternative completions for the
               contexts. Consistent with numerous past studies, the amplitude of
               the N400 was graded with expectancy, as amplitudes decreased with
               increasing constraint. Additionally, a left-lateralized, broad,
               slow negativity onsetting around 400-500 ms was largest for
               sentences with moderately strong constraint. Within this range of
               constraint, the negativity was larger for sentences with fewer
               alternative completions compared to those with many different
               ones. The timing and scalp distribution of the effect resemble
               brain responses linked to engagement of working memory resources,
               ambiguity resolution, and comprehension of jokes. Similar to
               cases of ``frame-shifting'' in non-literal language, this effect
               may reflect processing associated with reinterpretation or
               reconsideration of contextual material when multiple
               interpretations of a sentence were likely.",
  month     =  aug,
  year      =  2012,
  language  = "en",
  url="https://www.sciencedirect.com/science/article/abs/pii/S1053811912004508"
}
@ARTICLE{Szewczyk2022-cd,
  title     = "The power of ``good'': Can adjectives rapidly decrease as well as
               increase the availability of the upcoming noun?",
  author    = "Szewczyk, Jakub M and Mech, Emily N and Federmeier, Kara D",
  journal   = "Journal of Experimental Psychology: Learning, Memory, and Cognition",
  publisher = "American Psychological Association (APA)",
  volume    =  48,
  number    =  6,
  pages     = "856--875",
  abstract  = "Can a single adjective immediately influence message-building
               during sentence processing? We presented participants with 168
               sentence contexts, such as ``His skin was red from spending the
               day at the …'' Sentences ended with either the most expected word
               (``beach'') or a low cloze probability completion (``pool'').
               Nouns were preceded by adjectives that changed their relative
               likelihood (e.g., ``neighborhood'' increases the cloze
               probability of pool whereas ``sandy'' promotes beach). We asked
               if participants' online processing can be rapidly updated by the
               adjective, changing the resulting pattern of facilitation at the
               noun, and, if so, whether updates unfold symmetrically-not only
               increasing, but also decreasing, the fit of particular nouns. We
               measured event-related potentials (ERPs) to the adjective and the
               noun and modeled these with respect to (a) the overall amount of
               updating promoted by the adjective, (b) the preadjectival cloze
               probability of the noun and, (c) the amount of cloze probability
               change for the obtained noun after the adjective. Bayesian
               mixed-effects analysis of N400 amplitude at the noun revealed
               that adjectives rapidly influenced semantic processing of the
               noun, but did so asymmetrically, with positive updating (reducing
               N400 amplitudes) having a greater effect than negative updating
               (increasing N400s). At the adjective, the amount of (possible)
               updating was not associated with any discernible ERP modulation.
               Overall, these results suggest the information provided by
               adjectives is buffered until a head noun is encountered, at which
               point the access of the noun's semantics is shaped in parallel by
               both the adjective and the sentence-level representation.
               (PsycInfo Database Record (c) 2022 APA, all rights reserved).",
  month     =  jun,
  year      =  2022,
  language  = "en",
  url="https://pmc.ncbi.nlm.nih.gov/articles/PMC9059672/"
}
@ARTICLE{Szewczyk2022-ds,
  title     = "Context-based facilitation of semantic access follows both
               logarithmic and linear functions of stimulus probability",
  author    = "Szewczyk, Jakub M and Federmeier, Kara D",
  journal   = "Journal of Memory and Language",
  publisher = "Elsevier BV",
  volume    =  123,
  number    =  104311,
  pages     =  104311,
  abstract  = "Stimuli are easier to process when context makes them
               predictable, but does context-based facilitation arise from
               preactivation of a limited set of relatively probable upcoming
               stimuli (with facilitation then linearly related to probability)
               or, instead, because the system maintains and updates a
               probability distribution across all items (with facilitation
               logarithmically related to probability)? We measured the N400, an
               index of semantic access, to words of varying probability,
               including unpredictable words. Word predictability was measured
               using both cloze probabilities and a state-of-the-art machine
               learning language model (GPT-2). We reanalyzed five datasets (n =
               138) to demonstrate and then replicate that context-based
               facilitation on the N400 is graded, even among unpredictable
               words. Furthermore, we established that the relationship between
               word predictability and context-based facilitation combines
               linear and logarithmic functions. We argue that this composite
               function reveals properties of the mapping between words and
               semantic features and how feature- and word-related information
               is activated on-line.",
  month     =  apr,
  year      =  2022,
  keywords  = "Context-based facilitation; GPT-2; N400; Semantic access",
  language  = "en",
  url="https://www.sciencedirect.com/science/article/pii/S0749596X21000942"
}
@inproceedings{dar-etal-2023-analyzing,
    title = "Analyzing Transformers in Embedding Space",
    author = "Dar, Guy  and
      Geva, Mor  and
      Gupta, Ankit  and
      Berant, Jonathan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of ACL 2023",
    month = jul,
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.893",
    doi = "10.18653/v1/2023.acl-long.893",
    pages = "16124--16170",
}
@article{frank2013reading,
  title={Reading time data for evaluating broad-coverage models of English sentence processing},
  author={Frank, Stefan L and Fernandez Monsalve, Irene and Thompson, Robin L and Vigliocco, Gabriella},
  journal={Behavior research methods},
  volume={45},
  pages={1182--1190},
  year={2013},
  publisher={Springer},
  url={https://link.springer.com/article/10.3758/s13428-012-0313-y}
}
@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={ICML 2023},
  pages={2397--2430},
  year={2023},
  organization={PMLR},
  url={https://arxiv.org/abs/2304.01373}
}
@ARTICLE{Futrell2021-wr,
  title     = "The Natural Stories corpus: a reading-time corpus of English
               texts containing rare syntactic constructions",
  author    = "Futrell, Richard and Gibson, Edward and Tily, Harry J and Blank,
               Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T and
               Fedorenko, Evelina",
  journal   = "Language Resource and Evaluation",
  publisher = "Springer Science and Business Media LLC",
  volume    =  55,
  number    =  1,
  pages     = "63--77",
  abstract  = "It is now a common practice to compare models of human language
               processing by comparing how well they predict behavioral and
               neural measures of processing difficulty, such as reading times,
               on corpora of rich naturalistic linguistic materials. However,
               many of these corpora, which are based on naturally-occurring
               text, do not contain many of the low-frequency syntactic
               constructions that are often required to distinguish between
               processing theories. Here we describe a new corpus consisting of
               English texts edited to contain many low-frequency syntactic
               constructions while still sounding fluent to native speakers. The
               corpus is annotated with hand-corrected Penn Treebank-style parse
               trees and includes self-paced reading time data and aligned audio
               recordings. We give an overview of the content of the corpus,
               review recent work using the corpus, and release the data.",
  year      =  2021,
  keywords  = "Cognitive modeling; Psycholinguistics; Reading time",
  language  = "en",
  URL = "https://link.springer.com/article/10.1007/s10579-020-09503-7"
}
@ARTICLE{Smith2013-ap,
  title     = "The effect of word predictability on reading time is logarithmic",
  author    = "Smith, Nathaniel J and Levy, Roger",
  journal   = "Cognition",
  publisher = "Elsevier BV",
  volume    =  128,
  number    =  3,
  pages     = "302--319",
  abstract  = "It is well known that real-time human language processing is
               highly incremental and context-driven, and that the strength of a
               comprehender's expectation for each word encountered is a key
               determinant of the difficulty of integrating that word into the
               preceding context. In reading, this differential difficulty is
               largely manifested in the amount of time taken to read each word.
               While numerous studies over the past thirty years have shown
               expectation-based effects on reading times driven by lexical,
               syntactic, semantic, pragmatic, and other information sources,
               there has been little progress in establishing the quantitative
               relationship between expectation (or prediction) and reading
               times. Here, by combining a state-of-the-art computational
               language model, two large behavioral data-sets, and
               non-parametric statistical techniques, we establish for the first
               time the quantitative form of this relationship, finding that it
               is logarithmic over six orders of magnitude in estimated
               predictability. This result is problematic for a number of
               established models of eye movement control in reading, but lends
               partial support to an optimal perceptual discrimination account
               of word recognition. We also present a novel model in which
               language processing is highly incremental well below the level of
               the individual word, and show that it predicts both the shape and
               time-course of this effect. At a more general level, this result
               provides challenges for both anticipatory processing and semantic
               integration accounts of lexical predictability effects. And
               finally, this result provides evidence that comprehenders are
               highly sensitive to relative differences in predictability - even
               for differences between highly unpredictable words - and thus
               helps bring theoretical unity to our understanding of the role of
               prediction at multiple levels of linguistic structure in
               real-time language comprehension.",
  month     =  sep,
  year      =  2013,
  keywords  = "Expectation; Information theory; Probabilistic models of
               cognition; Psycholinguistics; Reading",
  language  = "en",
  url = "https://www.sciencedirect.com/science/article/pii/S0010027713000413"
}
@article{belrose2023eliciting,
  title={Eliciting Latent Predictions from Transformers with the Tuned Lens},
  author={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and McKinney, Lev and Ostrovsky, Igor and Biderman, Stella and Steinhardt, Jacob},
  year={2023},
  journal={arXiv preprint.},
  URL={https://arxiv.org/abs/2303.08112}
}
@INPROCEEDINGS{Oh2023-hj,
  title     = "Transformer-based language model surprisal predicts human reading
               times best with about two billion training tokens",
  author    = "Oh, Byung-Doh and Schuler, William",
  booktitle = "Findings of EMNLP 2023",
  pages     = "1915--1921",
  abstract  = "Byung-Doh Oh, William Schuler. Findings of the Association for
               Computational Linguistics: EMNLP 2023. 2023.",
  month     =  dec,
  year      =  2023,
  url = "https://aclanthology.org/2023.findings-emnlp.128/"
}
@INPROCEEDINGS{Oh2024-cc,
  title     = "Frequency Explains the Inverse Correlation of Large Language
               Models{'} Size, Training Data Amount, and Surprisal{'}s Fit to
               Reading Times",
  author    = "Oh, Byung-Doh and Yue, Shisen and Schuler, William",
  editor    = "Graham, Yvette and Purver, Matthew",
  booktitle = "Proceedings of EACL 2024",
  pages     = "2644--2663",
  abstract  = "Recent studies have shown that as Transformer-based language
               models become larger and are trained on very large amounts of
               data, the fit of their surprisal estimates to naturalistic human
               reading times degrades. The current work presents a series of
               analyses showing that word frequency is a key explanatory factor
               underlying these two trends. First, residual errors from four
               language model families on four corpora show that the inverse
               correlation between model size and fit to reading times is the
               strongest on the subset of least frequent words, which is driven
               by excessively accurate predictions of larger model variants.
               Additionally, training dynamics reveal that during later training
               steps, all model variants learn to predict rare words and that
               larger model variants do so more accurately, which explains the
               detrimental effect of both training data amount and model size on
               fit to reading times. Finally, a feature attribution analysis
               demonstrates that larger model variants are able to accurately
               predict rare words based on both an effectively longer context
               window size as well as stronger local associations compared to
               smaller model variants. Taken together, these results indicate
               that Transformer-based language models' surprisal estimates
               diverge from human-like expectations due to the superhumanly
               complex associations they learn for predicting rare words.",
  month     =  mar,
  year      =  2024,
  url = "https://aclanthology.org/2024.eacl-long.162/"
}
@ARTICLE{Rayner1998-rw,
  title     = "Eye movements in reading and information processing: 20 years of
               research",
  author    = "Rayner, Keith",
  journal   = "Psychological bulletin",
  publisher = "American Psychological Association (APA)",
  volume    =  124,
  number    =  3,
  pages     = "372--422",
  abstract  = "APA PsycNet DoiLanding page",
  year      =  1998,
  language  = "en",
  URL = "https://psycnet.apa.org/record/1998-11174-004"
}
@inproceedings{
aw2024instructiontuning,
title={Instruction-tuning Aligns {LLM}s to the Human Brain},
author={Khai Loong Aw and Syrielle Montariol and Badr AlKhamissi and Martin Schrimpf and Antoine Bosselut},
booktitle={COLM 2024},
year={2024},
url={https://openreview.net/forum?id=nXNN0x4wbl}
}
@ARTICLE{Wilcox2024-yt,
  title     = "An information-theoretic analysis of targeted regressions during
               reading",
  author    = "Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and
               Cotterell, Ryan",
  journal   = "Cognition",
  publisher = "Elsevier BV",
  volume    =  249,
  number    =  105765,
  pages     =  105765,
  abstract  = "Regressions, or backward saccades, are common during reading,
               accounting for between 5\% and 20\% of all saccades. And yet,
               relatively little is known about what causes them. We provide an
               information-theoretic operationalization for two previous
               qualitative hypotheses about regressions, which we dub
               reactivation and reanalysis. We argue that these hypotheses make
               different predictions about the pointwise mutual information or
               pmi between a regression's source and target. Intuitively, the
               pmi between two words measures how much more (or less) likely one
               word is to be present given the other. On one hand, the
               reactivation hypothesis predicts that regressions occur between
               words that are associated, implying high positive values of pmi.
               On the other hand, the reanalysis hypothesis predicts that
               regressions should occur between words that are not associated
               with each other, implying negative, low values of pmi. As a
               second theoretical contribution, we expand on previous theories
               by considering not only pmi but also expected values of pmi,
               E[pmi], where the expectation is taken over all possible
               realizations of the regression's target. The rationale for this
               is that language processing involves making inferences under
               uncertainty, and readers may be uncertain about what they have
               read, especially if a previous word was skipped. To test both
               theories, we use contemporary language models to estimate
               pmi-based statistics over word pairs in three corpora of eye
               tracking data in English, as well as in six languages across
               three language families (Indo-European, Uralic, and Turkic). Our
               results are consistent across languages and models tested:
               Positive values of pmi and E[pmi] consistently help to predict
               the patterns of regressions during reading, whereas negative
               values of pmi and E[pmi] do not. Our information-theoretic
               interpretation increases the predictive scope of both theories
               and our studies present the first systematic crosslinguistic
               analysis of regressions in the literature. Our results support
               the reactivation hypothesis and, more broadly, they expand the
               number of language processing behaviors that can be linked to
               information-theoretic principles.",
  month     =  aug,
  year      =  2024,
  keywords  = "Eye tracking; Information theory; Language processing; Mutual
               information; Reading; Regressions",
  language  = "en"
}
@INPROCEEDINGS{McCurdy2024-ix,
  title     = "Lossy Context Surprisal Predicts Task-Dependent Patterns in
               Relative Clause Processing",
  author    = "McCurdy, Kate and Hahn, Michael",
  booktitle = "Proceedings of CoNLL 2024",
  pages     = "36--45",
  abstract  = "Kate McCurdy, Michael Hahn. Proceedings of the 28th Conference on
               Computational Natural Language Learning. 2024.",
  month     =  nov,
  year      =  2024,
  url = "https://aclanthology.org/2024.conll-1.4/"
}
@INPROCEEDINGS{Wendler2024-wr,
  title     = "Do llamas work in English? On the latent language of multilingual
               transformers",
  author    = "Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and
               West, Robert",
  booktitle = "Proceedings of ACL 2024",
  pages     = "15366--15394",
  abstract  = "Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West.
               Proceedings of the 62nd Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers). 2024.",
  year      =  2024,
  url = "https://aclanthology.org/2024.acl-long.820/"
}
@inproceedings{lin-etal-2022-shot,
    title = "Few-shot Learning with Multilingual Generative Language Models",
    author = "Lin, Xi Victoria  and
      Mihaylov, Todor  and
      Artetxe, Mikel  and
      Wang, Tianlu  and
      Chen, Shuohui  and
      Simig, Daniel  and
      Ott, Myle  and
      Goyal, Naman  and
      Bhosale, Shruti  and
      Du, Jingfei  and
      Pasunuru, Ramakanth  and
      Shleifer, Sam  and
      Koura, Punit Singh  and
      Chaudhary, Vishrav  and
      O{'}Horo, Brian  and
      Wang, Jeff  and
      Zettlemoyer, Luke  and
      Kozareva, Zornitsa  and
      Diab, Mona  and
      Stoyanov, Veselin  and
      Li, Xian",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of EMNLP 2022",
    month = dec,
    year = "2022",
    url = "https://aclanthology.org/2022.emnlp-main.616",
    doi = "10.18653/v1/2022.emnlp-main.616",
    pages = "9019--9052",
    abstract = "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4{\%} absolute accuracy improvement in 0-shot settings and +9.4{\%} in 4-shot settings) and natural language inference (+5.4{\%} in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.",
}
@ARTICLE{Kutas2011-dt,
  title     = "Thirty years and counting: finding meaning in the {N400}
               component of the event-related brain potential ({ERP})",
  author    = "Kutas, Marta and Federmeier, Kara D",
  journal   = "Annual review of psychology",
  publisher = "Annual Reviews",
  volume    =  62,
  number    =  1,
  pages     = "621--647",
  abstract  = "We review the discovery, characterization, and evolving use of
               the N400, an event-related brain potential response linked to
               meaning processing. We describe the elicitation of N400s by an
               impressive range of stimulus types--including written, spoken,
               and signed words or pseudowords; drawings, photos, and videos of
               faces, objects, and actions; sounds; and mathematical
               symbols--and outline the sensitivity of N400 amplitude (as its
               latency is remarkably constant) to linguistic and nonlinguistic
               manipulations. We emphasize the effectiveness of the N400 as a
               dependent variable for examining almost every aspect of language
               processing and highlight its expanding use to probe semantic
               memory and to determine how the neurocognitive system dynamically
               and flexibly uses bottom-up and top-down information to make
               sense of the world. We conclude with different theories of the
               N400's functional significance and offer an N400-inspired
               reconceptualization of how meaning processing might unfold.",
  year      =  2011,
  language  = "en",
  url = "https://www.annualreviews.org/content/journals/10.1146/annurev.psych.093008.131123"
}
@article{Calvo2002-nx,
  title={Eye movements and processing stages in reading: Relative contribution of visual, lexical, and contextual factors},
  author={Calvo, Manuel G and Meseguer, Enrique},
  journal={The Spanish Journal of Psychology},
  volume={5},
  number={1},
  pages={66--77},
  year={2002},
  publisher={Cambridge University Press},
  url={https://www.cambridge.org/core/journals/spanish-journal-of-psychology/article/abs/eye-movements-and-processing-stages-in-reading-relative-contribution-of-visual-lexical-and-contextual-factors/A85DD970ED70A24BB97F28488DCD63E2}
}
@INPROCEEDINGS{Tenney2019-bb,
  title     = "{BERT} rediscovers the classical {NLP} pipeline",
  author    = "Tenney, Ian and Das, Dipanjan and Pavlick, Ellie",
  booktitle = "Proceedings of ACL 2019",
  pages     = "4593--4601",
  abstract  = "Ian Tenney, Dipanjan Das, Ellie Pavlick. Proceedings of the 57th
               Annual Meeting of the Association for Computational Linguistics.
               2019.",
  year      =  2019,
  url = "https://aclanthology.org/P19-1452/"
}
@INPROCEEDINGS{Brunner2019-so,
  title     = "On Identifiability in Transformers",
  author    = "Brunner, Gino and Liu, Yang and Pascual, Damian and Richter,
               Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger",
  booktitle = "ICLR 2019",
  abstract  = "In this paper we delve deep in the Transformer architecture by
               investigating two of its core components: self-attention and
               contextual embeddings. In particular, we study the
               identifiability of attention weights and token embeddings, and
               the aggregation of context into hidden tokens. We show that, for
               sequences longer than the attention head dimension, attention
               weights are not identifiable. We propose effective attention as a
               complementary tool for improving explanatory interpretations
               based on attention. Furthermore, we show that input tokens retain
               to a large degree their identity across the model. We also find
               evidence suggesting that identity information is mainly encoded
               in the angle of the embeddings and gradually decreases with
               depth. Finally, we demonstrate strong mixing of input information
               in the generation of contextual embeddings by means of a novel
               quantification method based on gradient attribution. Overall, we
               show that self-attention distributions are not directly
               interpretable and present tools to better understand and further
               investigate Transformer models.",
  month     =  sep,
  year      =  2019,
  URL = "https://openreview.net/forum?id=BJg1f6EFDB"
}
@ARTICLE{de-Varda2024-yh,
  title     = "Cloze probability, predictability ratings, and computational
               estimates for 205 English sentences, aligned with existing {EEG}
               and reading time data",
  author    = "de Varda, Andrea Gregor and Marelli, Marco and Amenta, Simona",
  journal   = "Behavior Research Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  56,
  number    =  5,
  pages     = "5190--5213",
  abstract  = "We release a database of cloze probability values, predictability
               ratings, and computational estimates for a sample of 205 English
               sentences (1726 words), aligned with previously released
               word-by-word reading time data (both self-paced reading and
               eye-movement records; Frank et al., Behavior Research Methods,
               45(4), 1182-1190. 2013) and EEG responses (Frank et al., Brain
               and Language, 140, 1-11. 2015). Our analyses show that
               predictability ratings are the best predictors of the EEG signal
               (N400, P600, LAN) self-paced reading times, and eye movement
               patterns, when spillover effects are taken into account. The
               computational estimates are particularly effective at explaining
               variance in the eye-tracking data without spillover. Cloze
               probability estimates have decent overall psychometric accuracy
               and are the best predictors of early fixation patterns (first
               fixation duration). Our results indicate that the choice of the
               best measurement of word predictability in context critically
               depends on the processing index being considered.",
  month     =  aug,
  year      =  2024,
  keywords  = "Cloze probability; Predictability ratings; Prediction; Surprisal
               estimates",
  language  = "en",
  url="https://pubmed.ncbi.nlm.nih.gov/37880511/"
}
@article{Dimigen2011-gt,
  title={Coregistration of eye movements and EEG in natural reading: analyses and review.},
  author={Dimigen, Olaf and Sommer, Werner and Hohlfeld, Annette and Jacobs, Arthur M and Kliegl, Reinhold},
  journal={Journal of experimental psychology: General},
  volume={140},
  number={4},
  pages={552},
  year={2011},
  publisher={American Psychological Association},
  URL = {https://psycnet.apa.org/record/2011-14094-001}
}
@ARTICLE{Rayner2009-aw,
  title     = "Language processing in reading and speech perception is fast and
               incremental: implications for event-related potential research",
  author    = "Rayner, Keith and Clifton, Jr, Charles",
  journal   = "Biological Psychology",
  publisher = "Elsevier BV",
  volume    =  80,
  number    =  1,
  pages     = "4--9",
  abstract  = "An overview of language processing during reading and listening
               is provided. Evidence is reviewed indicating that language
               processing in both domains is fast and incremental. We also
               discuss some aspects of normal reading and listening that are
               often obscured in event-related potential (ERP) research. We also
               discuss some apparent limitations of ERP techniques, as well as
               some recent indications that electroencephalographic (EEG)
               measures can be used to probe how lexical knowledge and lexical
               or structural expectations can contribute to the incremental
               process of language comprehension.",
  month     =  jan,
  year      =  2009,
  language  = "en",
  url = "https://www.sciencedirect.com/science/article/abs/pii/S0301051108001245"
}
@INPROCEEDINGS{Vasishth2004-qr,
  title     = "Modeling Sentence Processing in {ACT}-{R}",
  author    = "Vasishth, Shravan and Lewis, Richard L",
  booktitle = "Proceedings of the Workshop on Incremental Parsing: Bringing
               Engineering and Cognition Together",
  pages     = "82--87",
  abstract  = "Shravan Vasishth, Richard L. Lewis. Proceedings of the Workshop
               on Incremental Parsing: Bringing Engineering and Cognition
               Together. 2004.",
  year      =  2004
}
@ARTICLE{Huang2024-qe,
  title    = "Large-scale benchmark yields no evidence that language model
              surprisal explains syntactic disambiguation difficulty",
  author   = "Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and
              Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen,
              Tal",
  journal  = "Journal of Memory and Language",
  volume   =  137,
  pages    =  104510,
  abstract = "Prediction has been proposed as an overarching principle that
              explains human information processing in language and beyond. To
              what degree can processing difficulty in syntactically complex
              sentences – one of the major concerns of psycholinguistics – be
              explained by predictability, as estimated using computational
              language models, and operationalized as surprisal (negative log
              probability)? A precise, quantitative test of this question
              requires a much larger scale data collection effort than has been
              done in the past. We present the Syntactic Ambiguity Processing
              Benchmark, a dataset of self-paced reading times from 2000
              participants, who read a diverse set of complex English sentences.
              This dataset makes it possible to measure processing difficulty
              associated with individual syntactic constructions, and even
              individual sentences, precisely enough to rigorously test the
              predictions of computational models of language comprehension. By
              estimating the function that relates surprisal to reading times
              from filler items included in the experiment, we find that the
              predictions of language models with two different architectures
              sharply diverge from the empirical reading time data, dramatically
              underpredicting processing difficulty, failing to predict relative
              difficulty among different syntactic ambiguous constructions, and
              only partially explaining item-wise variability. These findings
              suggest that next-word prediction is most likely insufficient on
              its own to explain human syntactic processing.",
  month    =  aug,
  year     =  2024,
  keywords = "Sentence processing; Prediction; Surprisal; Language models",
  url = "https://www.sciencedirect.com/science/article/abs/pii/S0749596X24000135"
}
@ARTICLE{Lieder2019-xo,
  title     = "Resource-rational analysis: Understanding human cognition as the
               optimal use of limited computational resources",
  author    = "Lieder, Falk and Griffiths, Thomas L",
  journal   = "Behavioral and Brain Sciences",
  publisher = "Cambridge University Press (CUP)",
  volume    =  43,
  number    = "e1",
  pages     = "e1",
  abstract  = "Modeling human cognition is challenging because there are
               infinitely many mechanisms that can generate any given
               observation. Some researchers address this by constraining the
               hypothesis space through assumptions about what the human mind
               can and cannot do, while others constrain it through principles
               of rationality and adaptation. Recent work in economics,
               psychology, neuroscience, and linguistics has begun to integrate
               both approaches by augmenting rational models with cognitive
               constraints, incorporating rational principles into cognitive
               architectures, and applying optimality principles to
               understanding neural representations. We identify the rational
               use of limited resources as a unifying principle underlying these
               diverse approaches, expressing it in a new cognitive modeling
               paradigm called resource-rational analysis. The integration of
               rational principles with realistic cognitive constraints makes
               resource-rational analysis a promising framework for
               reverse-engineering cognitive mechanisms and representations. It
               has already shed new light on the debate about human rationality
               and can be leveraged to revisit classic questions of cognitive
               psychology within a principled computational framework. We
               demonstrate that resource-rational models can reconcile the
               mind's most impressive cognitive skills with people's ostensive
               irrationality. Resource-rational analysis also provides a new way
               to connect psychological theory more deeply with artificial
               intelligence, economics, neuroscience, and linguistics.",
  month     =  feb,
  year      =  2019,
  keywords  = "bounded rationality; cognitive biases; cognitive mechanisms;
               cognitive modeling; representations; resource rationality",
  language  = "en",
  url = "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/resourcerational-analysis-understanding-human-cognition-as-the-optimal-use-of-limited-computational-resources/586866D9AD1D1EA7A1EECE217D392F4A"
}
@INPROCEEDINGS{Oh2024-cf,
  title     = "Leading Whitespaces of Language Models’ Subword Vocabulary Pose a
               Confound for Calculating Word Probabilities",
  author    = "Oh, Byung-Doh and Schuler, William",
  booktitle = "Proceedings of EMNLP 2024",
  pages     = "3464--3472",
  abstract  = "Byung-Doh Oh, William Schuler. Proceedings of the 2024 Conference
               on Empirical Methods in Natural Language Processing. 2024.",
  month     =  nov,
  year      =  2024,
  url = "https://aclanthology.org/2024.emnlp-main.202/"
}
@INPROCEEDINGS{Giulianelli2024-wd,
  title     = "Generalized Measures of Anticipation and Responsivity in Online
               Language Processing",
  author    = "Giulianelli, Mario and Opedal, Andreas and Cotterell, Ryan",
  booktitle = "Findings of EMNLP 2024",
  pages     = "11648--11669",
  abstract  = "Mario Giulianelli, Andreas Opedal, Ryan Cotterell. Findings of
               the Association for Computational Linguistics: EMNLP 2024. 2024.",
  month     =  nov,
  year      =  2024,
  URL = "https://aclanthology.org/2024.findings-emnlp.682/"
}
@INPROCEEDINGS{Meister2024-vq,
  title     = "Towards a Similarity-adjusted Surprisal Theory",
  author    = "Meister, Clara and Giulianelli, Mario and Pimentel, Tiago",
  booktitle = "Proceedings of EMNLP 2024",
  pages     = "16485--16498",
  abstract  = "Clara Meister, Mario Giulianelli, Tiago Pimentel. Proceedings of
               the 2024 Conference on Empirical Methods in Natural Language
               Processing. 2024.",
  month     =  nov,
  year      =  2024,
  url = "https://aclanthology.org/2024.emnlp-main.921/"
}
@INPROCEEDINGS{Giulianelli2024-ms,
  title     = "On the Proper Treatment of Tokenization in Psycholinguistics",
  author    = "Giulianelli, Mario and Malagutti, Luca and Gastaldi, Juan Luis
               and DuSell, Brian and Vieira, Tim and Cotterell, Ryan",
  booktitle = "Proceedings of EMNLP 2024",
  pages     = "18556--18572",
  abstract  = "Mario Giulianelli, Luca Malagutti, Juan Luis Gastaldi, Brian
               DuSell, Tim Vieira, Ryan Cotterell. Proceedings of the 2024
               Conference on Empirical Methods in Natural Language Processing.
               2024.",
  month     =  nov,
  year      =  2024,
  URL = "https://aclanthology.org/2024.emnlp-main.1032/"
}
@INPROCEEDINGS{Opedal2024-si,
  title     = "On the Role of Context in Reading Time Prediction",
  author    = "Opedal, Andreas and Chodroff, Eleanor and Cotterell, Ryan and
               Wilcox, Ethan",
  booktitle = "Proceedings of EMNLP 2024",
  pages     = "3042--3058",
  abstract  = "Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, Ethan Wilcox.
               Proceedings of the 2024 Conference on Empirical Methods in
               Natural Language Processing. 2024.",
  month     =  nov,
  year      =  2024,
  url = "https://aclanthology.org/2024.emnlp-main.179/"
}
@INPROCEEDINGS{Prasad2024-yu,
  title     = "{SPAWNing} Structural Priming Predictions from a Cognitively
               Motivated Parser",
  author    = "Prasad, Grusha and Linzen, Tal",
  booktitle = "Proceedings of CoNLL 2024",
  pages     = "178--197",
  abstract  = "Grusha Prasad, Tal Linzen. Proceedings of the 28th Conference on
               Computational Natural Language Learning. 2024.",
  month     =  nov,
  year      =  2024,
  url="https://aclanthology.org/2024.conll-1.14/"
}
@INPROCEEDINGS{Wang2024-po,
  title     = "How can large language models become more human?",
  author    = "Wang, Daphne and Sadrzadeh, Mehrnoosh and Stanojević, Miloš and
               Chow, Wing-Yee and Breheny, Richard",
  booktitle = "Proceedings of CMCL 2024",
  pages     = "166--176",
  abstract  = "Daphne Wang, Mehrnoosh Sadrzadeh, Miloš Stanojević, Wing-Yee
               Chow, Richard Breheny. Proceedings of the Workshop on Cognitive
               Modeling and Computational Linguistics. 2024.",
  year      =  2024,
  url = "https://aclanthology.org/2024.cmcl-1.14/"
}
@article{Hu2024-qp,
  title={Language models align with human judgments on key grammatical constructions},
  author={Hu, Jennifer and Mahowald, Kyle and Lupyan, Gary and Ivanova, Anna and Levy, Roger},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={36},
  pages={e2400917121},
  year={2024},
  publisher={National Academy of Sciences},
  url={https://www.pnas.org/doi/abs/10.1073/pnas.2400917121}
}
@ARTICLE{Nour-Eddine2024-ub,
  title     = "A predictive coding model of the {N400}",
  author    = "Nour Eddine, Samer and Brothers, Trevor and Wang, Lin and
               Spratling, Michael and Kuperberg, Gina R",
  journal   = "Cognition",
  publisher = "Elsevier BV",
  volume    =  246,
  number    =  105755,
  pages     =  105755,
  abstract  = "The N400 event-related component has been widely used to
               investigate the neural mechanisms underlying real-time language
               comprehension. However, despite decades of research, there is
               still no unifying theory that can explain both its temporal
               dynamics and functional properties. In this work, we show that
               predictive coding - a biologically plausible algorithm for
               approximating Bayesian inference - offers a promising framework
               for characterizing the N400. Using an implemented predictive
               coding computational model, we demonstrate how the N400 can be
               formalized as the lexico-semantic prediction error produced as
               the brain infers meaning from the linguistic form of incoming
               words. We show that the magnitude of lexico-semantic prediction
               error mirrors the functional sensitivity of the N400 to various
               lexical variables, priming, contextual effects, as well as their
               higher-order interactions. We further show that the dynamics of
               the predictive coding algorithm provides a natural explanation
               for the temporal dynamics of the N400, and a biologically
               plausible link to neural activity. Together, these findings
               directly situate the N400 within the broader context of
               predictive coding research. More generally, they raise the
               possibility that the brain may use the same computational
               mechanism for inference across linguistic and non-linguistic
               domains.",
  month     =  may,
  year      =  2024,
  keywords  = "Bayesian inference; Language comprehension; Orthographic;
               Prediction; Prediction error; Semantic",
  language  = "en",
  url = "https://www.sciencedirect.com/science/article/abs/pii/S0010027724000416"
}
@ARTICLE{Mei2024-eb,
  title     = "A Turing test of whether {AI} chatbots are behaviorally similar
               to humans",
  author    = "Mei, Qiaozhu and Xie, Yutong and Yuan, Walter and Jackson,
               Matthew O",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "Proceedings of the National Academy of Sciences",
  volume    =  121,
  number    =  9,
  pages     = "e2313925121",
  abstract  = "We administer a Turing test to AI chatbots. We examine how
               chatbots behave in a suite of classic behavioral games that are
               designed to elicit characteristics such as trust, fairness,
               risk-aversion, cooperation, etc., as well as how they respond to
               a traditional Big-5 psychological survey that measures
               personality traits. ChatGPT-4 exhibits behavioral and personality
               traits that are statistically indistinguishable from a random
               human from tens of thousands of human subjects from more than 50
               countries. Chatbots also modify their behavior based on previous
               experience and contexts ``as if'' they were learning from the
               interactions and change their behavior in response to different
               framings of the same strategic situation. Their behaviors are
               often distinct from average and modal human behaviors, in which
               case they tend to behave on the more altruistic and cooperative
               end of the distribution. We estimate that they act as if they are
               maximizing an average of their own and partner's payoffs.",
  month     =  feb,
  year      =  2024,
  keywords  = "AI; Turing test; behavioral games; chatbot; personality",
  language  = "en"
}
@ARTICLE{Witzel2012-nr,
  title     = "Comparisons of online reading paradigms: eye tracking,
               moving-window, and maze",
  author    = "Witzel, Naoko and Witzel, Jeffrey and Forster, Kenneth",
  journal   = "Journal of Psycholinguistic Research",
  publisher = "Springer Science and Business Media LLC",
  volume    =  41,
  number    =  2,
  pages     = "105--128",
  abstract  = "This study compares four methodologies used to examine online
               sentence processing during reading. Specifically, self-paced,
               non-cumulative, moving-window reading (Just et al. in J Exp
               Psychol Gen 111:228-238, 1982), eye tracking (see e.g., Rayner in
               Q J Exp Psychol 62:1457-1506, 2009), and two versions of the maze
               task (Forster et al. in Behav Res Methods 41:163-171, 2009)--the
               lexicality maze and the grammaticality maze--were used to
               investigate the processing of sentences containing temporary
               structural ambiguities. Of particular interest were (i) whether
               each task was capable of revealing processing differences on
               these sentences and (ii) whether these effects were indicated
               precisely at the predicted word/region. Although there was
               considerable overlap in the general pattern of results from the
               four tasks, there were also clear differences among them in terms
               of the strength and timing of the observed effects. In
               particular, excepting sentences that tap into clause-closure
               commitments, both maze task versions provided robust,
               ``localized'' indications of incremental sentence processing
               difficulty relative to self-paced reading and eye tracking.",
  month     =  apr,
  year      =  2012,
  language  = "en",
  url = "https://link.springer.com/article/10.1007/s10936-011-9179-x"
}
@ARTICLE{Vani2021-aj,
  title    = "Using the Interpolated Maze Task to Assess Incremental Processing
              in English Relative Clauses",
  author   = "Vani, Pranali and Wilcox, Ethan Gotlieb and Levy, Roger",
  journal  = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  volume   =  43,
  number   =  43,
  abstract = "Author(s): Vani, Pranali; Wilcox, Ethan Gotlieb; Levy, Roger |
              Abstract: In English, Subject Relative Clauses are processed more
              quickly than Object Relative Clauses, but open questions remain
              about where in the clause slowdown occurs. The surprisal theory of
              incremental processing, under which processing difficulty
              corresponds to probabilistic expectations about upcoming material,
              predicts that slowdown should occur immediately on material that
              disambiguates the subject from object relative clause. However,
              evidence from eye tracking and self-paced reading studies suggests
              that slowdown occurs downstream of RC-disambiguating material, on
              the relative clause verb. These methods, however, suffer from
              well-known spillover effects which makes their results difficult
              to interpret. To address these issues, we introduce and deploy a
              novel variant of the Maze task for reading times (Forster,
              Guerrera, \& Elliot, 2009), called the Interpolated Maze in two
              English web-based experiments. In Experiment 1, we find that the
              locus of reading-time differences between SRCs and ORCs falls on
              immediate disambiguating definite determiner. Experiment 2
              provides a control, showing that ORCs are read more slowly than
              lexically-matching, non-anomalous material. These results provide
              new evidence for the locus of processing difficulty in relative
              clauses and support the surprisal theory of incremental
              processing.",
  year     =  2021,
  keywords = "Social and Behavioral Sciences",
  url="https://escholarship.org/uc/item/3x34x7dz"
}
@ARTICLE{Kaplan2020-es,
  title         = "Scaling laws for neural language models",
  author        = "Kaplan, Jared and McCandlish, Sam and Henighan, Tom and
                   Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray,
                   Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario",
  journal       = "arXiv [cs.LG]",
  abstract      = "We study empirical scaling laws for language model
                   performance on the cross-entropy loss. The loss scales as a
                   power-law with model size, dataset size, and the amount of
                   compute used for training, with some trends spanning more
                   than seven orders of magnitude. Other architectural details
                   such as network width or depth have minimal effects within a
                   wide range. Simple equations govern the dependence of
                   overfitting on model/dataset size and the dependence of
                   training speed on model size. These relationships allow us to
                   determine the optimal allocation of a fixed compute budget.
                   Larger models are significantly more sample-efficient, such
                   that optimally compute-efficient training involves training
                   very large models on a relatively modest amount of data and
                   stopping significantly before convergence.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}
@ARTICLE{Frank2024-xu,
  title     = "An eye-tracking-with-{EEG} coregistration corpus of narrative
               sentences",
  author    = "Frank, Stefan L and Aumeistere, Anna",
  journal   = "Lang. Resour. Eval.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  58,
  number    =  2,
  pages     = "641--657",
  abstract  = "AbstractWe present the Radboud Coregistration Corpus of Narrative
               Sentences (RaCCooNS), the first freely available corpus of
               eye-tracking-with-EEG data collected while participants read
               narrative sentences in Dutch. The corpus is intended for studying
               human sentence comprehension and for evaluating the cognitive
               validity of computational language models. RaCCooNS contains data
               from 37 participants (3 of which eye tracking only) reading 200
               Dutch sentences each. Less predictable words resulted in
               significantly longer reading times and larger N400 sizes,
               replicating well-known surprisal effects in eye tracking and EEG
               simultaneously. We release the raw eye-tracking data, the
               preprocessed eye-tracking data at the fixation, word, and trial
               levels, the raw EEG after merger with eye-tracking data, and the
               preprocessed EEG data both before and after ICA-based ocular
               artifact correction.",
  month     =  jun,
  year      =  2024,
  language  = "en"
}
@ARTICLE{Hollenstein2018-rm,
  title    = "{ZuCo}, a simultaneous {EEG} and eye-tracking resource for natural
              sentence reading",
  author   = "Hollenstein, Nora and Rotsztejn, Jonathan and Troendle, Marius and
              Pedroni, Andreas and Zhang, Ce and Langer, Nicolas",
  journal  = "Scientific Data",
  volume   =  5,
  pages    =  180291,
  abstract = "We present the Zurich Cognitive Language Processing Corpus (ZuCo),
              a dataset combining electroencephalography (EEG) and eye-tracking
              recordings from subjects reading natural sentences. ZuCo includes
              high-density EEG and eye-tracking data of 12 healthy adult native
              English speakers, each reading natural English text for 4-6 hours.
              The recordings span two normal reading tasks and one task-specific
              reading task, resulting in a dataset that encompasses EEG and
              eye-tracking data of 21,629 words in 1107 sentences and 154,173
              fixations. We believe that this dataset represents a valuable
              resource for natural language processing (NLP). The EEG and
              eye-tracking signals lend themselves to train improved
              machine-learning models for various tasks, in particular for
              information extraction tasks such as entity and relation
              extraction and sentiment analysis. Moreover, this dataset is
              useful for advancing research into the human reading and language
              understanding process at the level of brain activity and
              eye-movement.",
  month    =  dec,
  year     =  2018,
  language = "en",
  url = "https://www.nature.com/articles/sdata2018291"
}
@ARTICLE{Graves2016-dp,
  title    = "Adaptive Computation Time for Recurrent Neural Networks",
  author   = "Graves, Alex",
  abstract = "This paper introduces Adaptive Computation Time (ACT), an
              algorithm that allows recurrent neural networks to learn how many
              computational steps to take between receiving an input and
              emitting an output. ACT requires minimal changes to the network
              architecture, is deterministic and differentiable, and does not
              add any noise to the parameter gradients. Experimental results are
              provided for four synthetic problems: determining the parity of
              binary vectors, applying binary logic operations, adding integers,
              and sorting real numbers. Overall, performance is dramatically
              improved by the use of ACT, which successfully adapts the number
              of computational steps to the requirements of the problem. We also
              present character-level language modelling results on the Hutter
              prize Wikipedia dataset. In this case ACT does not yield large
              gains in performance; however it does provide intriguing insight
              into the structure of the data, with more computation allocated to
              harder-to-predict transitions, such as spaces between words and
              ends of sentences. This suggests that ACT or other adaptive
              computation methods could provide a generic method for inferring
              segment boundaries in sequence data.",
  month    =  mar,
  year     =  2016,
  journal = "arXiv preprint",
  URL = "https://arxiv.org/abs/1603.08983"
}
@INPROCEEDINGS{Banino2021-oq,
  title     = "{PonderNet}: Learning to Ponder",
  author    = "Banino, Andrea and Balaguer, Jan and Blundell, Charles",
  booktitle = "8th ICML Workshop on Automated Machine Learning (AutoML)",
  abstract  = "In standard neural networks the amount of computation used grows
               with the size of the inputs, but not with the complexity of the
               problem being learnt. To overcome this limitation we introduce
               PonderNet, a new algorithm that learns to adapt the amount of
               computation based on the complexity of the problem at hand.
               PonderNet learns end-to-end the number of computational steps to
               achieve an effective compromise between training prediction
               accuracy, computational cost and generalization. On a complex
               synthetic problem, PonderNet dramatically improves performance
               over previous adaptive computation methods and additionally
               succeeds at extrapolation tests where traditional neural networks
               fail. Also, our method matched the current state of the art
               results on a real world question and answering dataset, but using
               less compute. Finally, PonderNet reached state of the art results
               on a complex task designed to test the reasoning capabilities of
               neural networks.",
  month     =  jul,
  year      =  2021,
  URL   =   "https://openreview.net/forum?id=1EuxRTe0WN"
}
@article{Waldis2024-rf,
  title={Holmes A Benchmark to Assess the Linguistic Competence of Language Models},
  author={Waldis, Andreas and Perlitz, Yotam and Choshen, Leshem and Hou, Yufang and Gurevych, Iryna},
  journal={TACL},
  volume={12},
  pages={1616--1647},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…},
  url={https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00718/125534}
}
@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}
@INPROCEEDINGS{Heafield2011-ce,
  title     = "{K}en{LM}: Faster and Smaller Language Model Queries",
  author    = "Heafield, Kenneth",
  booktitle = "Proceedings of the sixth workshop on statistical machine translation",
  pages     = "187--197",
  month     =  jul,
  year      =  2011,
  url = "https://aclanthology.org/W11-2123/"
}
@ARTICLE{Hosseini2024-jf,
  title     = "Artificial neural network language models predict human brain
               responses to language even after a developmentally realistic
               amount of training",
  author    = "Hosseini, Eghbal A and Schrimpf, Martin and Zhang, Yian and
               Bowman, Samuel and Zaslavsky, Noga and Fedorenko, Evelina",
  journal   = "Neurobiology of Language",
  publisher = "MIT Press",
  volume    =  5,
  number    =  1,
  pages     = "43--63",
  abstract  = "Artificial neural networks have emerged as computationally
               plausible models of human language processing. A major criticism
               of these models is that the amount of training data they receive
               far exceeds that of humans during language learning. Here, we use
               two complementary approaches to ask how the models' ability to
               capture human fMRI responses to sentences is affected by the
               amount of training data. First, we evaluate GPT-2 models trained
               on 1 million, 10 million, 100 million, or 1 billion words against
               an fMRI benchmark. We consider the 100-million-word model to be
               developmentally plausible in terms of the amount of training data
               given that this amount is similar to what children are estimated
               to be exposed to during the first 10 years of life. Second, we
               test the performance of a GPT-2 model trained on a
               9-billion-token dataset to reach state-of-the-art next-word
               prediction performance on the human benchmark at different stages
               during training. Across both approaches, we find that (i) the
               models trained on a developmentally plausible amount of data
               already achieve near-maximal performance in capturing fMRI
               responses to sentences. Further, (ii) lower perplexity-a measure
               of next-word prediction performance-is associated with stronger
               alignment with human data, suggesting that models that have
               received enough training to achieve sufficiently high next-word
               prediction performance also acquire representations of sentences
               that are predictive of human fMRI responses. In tandem, these
               findings establish that although some training is necessary for
               the models' predictive ability, a developmentally realistic
               amount of training (∼100 million words) may suffice.",
  month     =  apr,
  year      =  2024,
  keywords  = "ANN-neural data alignment; artificial neural network;
               development; human behavior; language network",
  language  = "en",
  url = "https://direct.mit.edu/nol/article/5/1/43/119156/Artificial-Neural-Network-Language-Models-Predict"
}
@inproceedings{kitaev-etal-2019-multilingual,
    title = "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    author = "Kitaev, Nikita  and
      Cao, Steven  and
      Klein, Dan",
    booktitle = "Proceedings of ACL 2019",
    month = jul,
    year = "2019",
    url = "https://www.aclweb.org/anthology/P19-1340",
    doi = "10.18653/v1/P19-1340",
    pages = "3499--3505",
}

@inproceedings{kitaev-klein-2018-constituency,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of ACL 2018",
    month = jul,
    year = "2018",
    url = "https://www.aclweb.org/anthology/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
}
@ARTICLE{Lau2008-pb,
  title     = "A cortical network for semantics: (de)constructing the {N400}",
  author    = "Lau, Ellen F and Phillips, Colin and Poeppel, David",
  journal   = "Nature Reviews Neuroscience",
  publisher = "Springer Science and Business Media LLC",
  volume    =  9,
  number    =  12,
  pages     = "920--933",
  abstract  = "Measuring event-related potentials (ERPs) has been fundamental to
               our understanding of how language is encoded in the brain. One
               particular ERP response, the N400 response, has been especially
               influential as an index of lexical and semantic processing.
               However, there remains a lack of consensus on the interpretation
               of this component. Resolving this issue has important
               consequences for neural models of language comprehension. Here we
               show that evidence bearing on where the N400 response is
               generated provides key insights into what it reflects. A
               neuroanatomical model of semantic processing is used as a guide
               to interpret the pattern of activated regions in functional MRI,
               magnetoencephalography and intracranial recordings that are
               associated with contextual semantic manipulations that lead to
               N400 effects.",
  month     =  dec,
  year      =  2008,
  language  = "en",
  url = "https://www.nature.com/articles/nrn2532"
}
@INPROCEEDINGS{Ethayarajh2019-zy,
  title     = "How contextual are contextualized word representations? Comparing
               the geometry of {BERT}, {ELMo}, and {GPT}-2 embeddings",
  author    = "Ethayarajh, Kawin",
  booktitle = "Proceedings of EMNLP-IJCNLP 2019",
  pages     = "55--65",
  abstract  = "Kawin Ethayarajh. Proceedings of the 2019 Conference on Empirical
               Methods in Natural Language Processing and the 9th International
               Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
               2019.",
  month     =  nov,
  year      =  2019,
  URL = "https://aclanthology.org/D19-1006/"
}
@article{Caucheteux2023-tx,
  title={Evidence of a predictive coding hierarchy in the human brain listening to speech},
  author={Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  journal={Nature human behaviour},
  volume={7},
  number={3},
  pages={430--441},
  year={2023},
  publisher={Nature Publishing Group UK London},
  url={https://www.nature.com/articles/s41562-022-01516-2}
}
@ARTICLE{Toneva2019-ul,
  title    = "Interpreting and improving natural-language processing (in
              machines) with natural language-processing (in the brain)",
  author   = "Toneva, Mariya and Wehbe, Leila",
  journal  = "NeurIPS 2019",
  pages    = "14928--14938",
  abstract = "Neural networks models for NLP are typically implemented without
              the explicit encoding of language rules and yet they are able to
              break one performance record after another. This has generated a
              lot of research interest in interpreting the representations
              learned by these networks. We propose here a novel interpretation
              approach that relies on the only processing system we have that
              does understand language: the human brain. We use brain imaging
              recordings of subjects reading complex natural text to interpret
              word and sequence embeddings from 4 recent NLP models - ELMo, USE,
              BERT and Transformer-XL. We study how their representations differ
              across layer depth, context length, and attention type. Our
              results reveal differences in the context-related representations
              across these models. Further, in the transformer models, we find
              an interaction between layer depth and context length, and between
              layer depth and attention type. We finally hypothesize that
              altering BERT to better align with brain recordings would enable
              it to also better understand language. Probing the altered BERT
              using syntactic NLP tasks reveals that the model with increased
              brain-alignment outperforms the original model. Cognitive
              neuroscientists have already begun using NLP networks to study the
              brain, and this work closes the loop to allow the interaction
              between NLP and cognitive neuroscience to be a true
              cross-pollination.",
  month    =  may,
  year     =  2019,
  url = "https://proceedings.neurips.cc/paper/2019/hash/749a8e6c231831ef7756db230b4359c8-Abstract.html"
}
@INPROCEEDINGS{Wilcox2023-bb,
  title     = "Language Model Quality Correlates with Psychometric Predictive
               Power in Multiple Languages",
  author    = "Wilcox, Ethan and Meister, Clara and Cotterell, Ryan and
               Pimentel, Tiago",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  booktitle = "Proceedings of EMNLP 2023",
  pages     = "7503--7511",
  abstract  = "Surprisal theory (Hale, 2001; Levy, 2008) posits that a word's
               reading time is proportional to its surprisal (i.e., to its
               negative log probability given the proceeding context). Since we
               are unable to access a word's ground-truth probability, surprisal
               theory has been empirically tested using surprisal estimates from
               language models (LMs). Under the premise that surprisal theory
               holds, we would expect that higher quality language models
               provide more powerful predictors of human reading behavior---a
               conjecture we dub the quality--power (QP) hypothesis.
               Unfortunately, empirical support for the QP hypothesis is mixed.
               Some studies in English have found correlations between LM
               quality and predictive power, but other studies using Japanese
               data, as well as using larger English LMs, find no such
               correlations. In this work, we conduct a systematic
               crosslinguistic assessment of the QP hypothesis. We train LMs
               from scratch on small- and medium-sized datasets from 13
               languages (across five language families) and assess their
               ability to predict eye tracking data. We find correlations
               between LM quality and power in eleven of these thirteen
               languages, suggesting that, within the range of model classes and
               sizes tested, better language models are indeed better predictors
               of human language processing behaviors.",
  month     =  dec,
  year      =  2023,
  url = "https://aclanthology.org/2023.emnlp-main.466/"
}
@ARTICLE{Oh2022-ss,
  title     = "Comparison of structural parsers and neural language models as
               surprisal estimators",
  author    = "Oh, Byung-Doh and Clark, Christian and Schuler, William",
  journal   = "Frontiers in Artificial Intelligence",
  publisher = "Frontiers Media SA",
  volume    =  5,
  pages     =  777963,
  abstract  = "Expectation-based theories of sentence processing posit that
               processing difficulty is determined by predictability in context.
               While predictability quantified via surprisal has gained
               empirical support, this representation-agnostic measure leaves
               open the question of how to best approximate the human
               comprehender's latent probability model. This article first
               describes an incremental left-corner parser that incorporates
               information about common linguistic abstractions such as
               syntactic categories, predicate-argument structure, and
               morphological rules as a computational-level model of sentence
               processing. The article then evaluates a variety of structural
               parsers and deep neural language models as cognitive models of
               sentence processing by comparing the predictive power of their
               surprisal estimates on self-paced reading, eye-tracking, and fMRI
               data collected during real-time language processing. The results
               show that surprisal estimates from the proposed left-corner
               processing model deliver comparable and often superior fits to
               self-paced reading and eye-tracking data when compared to those
               from neural language models trained on much more data. This may
               suggest that the strong linguistic generalizations made by the
               proposed processing model may help predict humanlike processing
               costs that manifest in latency-based measures, even when the
               amount of training data is limited. Additionally, experiments
               using Transformer-based language models sharing the same primary
               architecture and training data show a surprising negative
               correlation between parameter count and fit to self-paced reading
               and eye-tracking data. These findings suggest that large-scale
               neural language models are making weaker generalizations based on
               patterns of lexical items rather than stronger, more humanlike
               generalizations based on linguistic structure.",
  month     =  mar,
  year      =  2022,
  keywords  = "eye-tracking; fMRI; incremental parsers; language models;
               self-paced reading; sentence processing; surprisal theory",
  language  = "en",
  url = "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.777963/full"
}
@BOOK{Beinborn2024-lw,
  title     = "Cognitive plausibility in natural language processing",
  author    = "Beinborn, Lisa and Hollenstein, Nora",
  publisher = "Springer International Publishing",
  address   = "Cham",
  series    = "Synthesis lectures on human language technologies",
  year      =  2024,
  language  = "en"
}
