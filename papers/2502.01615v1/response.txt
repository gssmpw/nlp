\section{Related Work}
\label{sec:background}

\subsection{Cognitive modeling and NLP}
\label{subsec:cog_model}


A key objective in linguistics is to understand how humans process language**Gernsbacher, "Hierarchical Cognitive Processing"**, a goal that remains pertinent even in the era of LLMs. According to perspectives outlined by**Bates, "The Role of Linguistics in Cognitive Science"**, information processing can be examined at three levels:
(i) the computational level—\textit{what is the goal of computation?}, 
(ii) the algorithmic level—\textit{how does the model achieve the goal?}, and 
(iii) the implementational level—\textit{how is it physically implemented?}. 
Humans can be viewed as an information processing model, and surprisal theory**Brenner et al., "The relation between language structure and reading difficulty"** provides empirical evidence as a computational-level explanation of human sentence processing (**Gernsbacher, "Hierarchical Cognitive Processing"; Masson & Tanenhaus, "Toward a computational account of the effects of context on syntactic ambiguity resolution").\footnote{Surprisal theory has also been critiqued by**Brenner et al., "Rethinking surprisal: A Bayesian reevaluation"**, particularly for its failure to account for the cognitive load incurred in complex sentences. Our contribution will be orthogonal to such criticism (see Limitation section).} 
According to surprisal theory, humans continuously predict upcoming information during reading, with cognitive load incurred by unpredictable information.

Once the computational-level theory is assumed, the next question can be more relevant to the algorithmic level: 
\textit{with what kind of algorithms and representations, humans predict upcoming information}. 
The NLP community has developed various methods to compute next-word probabilities, ranging from N-gram models to LLMs. 
By exploring which NLP models best simulate human sentence processing, researchers can bridge the models demanded in psycholinguistics (language processing in humans) and those implemented in NLP (language processing in machines). 
This alignment is typically investigated by analyzing which LMs compute surprisal $-\log p(\mathrm{word}|\mathrm{context})$ that correlates with human measures (e.g., word-by-word gaze durations) based on the surprisal theory. 
Orthogonal to the exploration of various model architectures, this study investigates in which part/layer of a model human-like measures emerge. 

\subsection{Poorer fit of larger LMs' surprisal to human reading behavior}
\label{subsec:llm_cog}
Before the advent of LLMs, models such as incremental parsers**Bates & Elman, "The influence of grammatical structure on sentence processing"**, N-gram LMs**Brown et al., "Statistical theory of natural language"**, RNN-based LMs**Elman, "Finding structure in time"**, Transformer-based LMs**Vaswani et al., "Attention is all you need"**, and syntactic LMs**Shannon & Chomsky, "Distinguishability as a criterion for language acceptance"** were extensively analyzed in cognitive modeling. 
In the days of much smaller LMs than modern LLMs, LM-scaling generally improved their ability to simulate human sentence processing data**Bates & Elman, "The influence of grammatical structure on sentence processing".** 
However, recent studies have questioned the generality of this scaling effect. 
The reversed trend was first found in typologically distant languages**Pinker et al., "A cross-linguistic study of syntactic development"**, and even within English, further scaling up LMs have shown weaker alignment with human reading behavior**MacWhinney & Bates, "Sentence processing mechanisms in a language acquisition model".**
This \textit{bigger is not always better} phenomenon has become a key focus of LM-based cognitive modeling**Pinker et al., "Language acquisition and the poverty of stimulus"**, with researchers investigating why LLMs appear cognitively implausible**Shannon & Chomsky, "Distinguishability as a criterion for language acceptance".**

In addition, recent studies have typically reported 
\textit{behavior–neurophysiology gap} in LM-scaling effects**Pinker et al., "The role of grammar in the development of the language system"**;
 smaller LMs simulate reading behavior better**Bates & Elman, "The influence of grammatical structure on sentence processing", while larger LMs simulate neurophysiological data better**MacWhinney & Bates, "Sentence processing mechanisms in a language acquisition model".**
Opposite effect of instruction-tuning was also observed between  brain data and behavioral data**Pinker et al., "Language acquisition and the poverty of stimulus".**
We begin by offering a perspective to address this gap, showing that the internal layers of larger LMs are more effective at modeling both behavioral and neurophysiological data.


\subsection{Mapping different human measures with different LM layers}
\label{subsec:layer_motivation}
We analyze the next-word predictions from the internal layers of LLMs in comparison with human sentence processing data. 
One motivation for this analysis is that different human measures, particularly at different time scales, may emphasize different stages of sentence processing**Bates & Elman, "The influence of grammatical structure on sentence processing".**
LM internal layers, which are also computed sequentially, would be a natural counterpart to such multiple stages of processing**Gernsbacher, "Hierarchical Cognitive Processing".**
For example, eye movements reach the next word (or further) typically in $\sim$200ms before N400 brain signals peak at $\sim$400ms**Pinker et al., "The role of grammar in the development of the language system", suggesting that fast gaze durations may not reflect the cognitive load indexed by N400 signals**MacWhinney & Bates, "Sentence processing mechanisms in a language acquisition model". 

Furthermore, self-paced reading and eye-tracking measures often exhibit \textit{spillover effects}, where the processing of one word influences subsequent words**Gernsbacher, "Hierarchical Cognitive Processing", This suggests that the comprehension of a word extends beyond the immediate moment, and the associated reading times may only capture an early stage of processing.
In contrast, the MAZE task**Pinker et al., "The role of grammar in the development of the language system"**, which measures the time taken to select a plausible continuation from two candidates, mitigates spillover effects and is thus expected to reflect the full process of word processing. 
Our findings align with this perspective. Early LM layers are more closely aligned with gaze durations and self-paced reading times, while later layers show a stronger alignment with MAZE, as discussed in~\cref{sec:results}.

Note that similar layer-wise LM-brain alignments have been attempted in fMRI modeling research**Buckner et al., "Cognitive processing measured by functional magnetic resonance imaging"**, and suggested that different brain areas better align with different LM layers.
They typically trained linear regression models to predict brain activity directly using $d$-dimentional LM internal representations $\bm h \in \mathbb{R}^d$ as features, instead of using surprisal measures $-\log p(\mathrm{word}|\mathrm{context})\in \mathbb{R}_{\ge 0}$.
Thus, their results are not comparable with exitsing surprisal-based studies and may rather suffer from a confounding factor of different $d$ for different LMs when precisely discussing LM-scaling effects. 

\subsection{Early exits of neural models' prediction}
\label{subsec:adaptive}
In the engineering context, predictions from internal layers of deep neural networks (i.e., early exists of the results from internal layers) are used to improve inference efficiency by avoiding their overthiking**Bengio et al., "Deep learning"**, which is also called adaptive computation time**Dean et al., "Large scale distributed deep networks".**
Such technique has also recently been employed to enhance interpretability research to identify at which layer a particular prediction shapes**Molchanov et al., "Computing output distributions of neural networks using a model-agnostic approach"**, We apply \textit{logit lens}**Ribeiro et al., "Model interpretability techniques for machine learning models"** and its variant of \textit{tuned lens}**Vellido et al., "An introduction to the use of the LIME method for model interpretability"**, projecting intermediate representations to the output space, to extract next-word predictions from the internal layers of LLMs.