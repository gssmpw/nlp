

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}


   \usepackage[acceptedWithA]{tacl2021v1}

\usepackage{tacl2021v1}

\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse %
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

\iftaclpubformat %
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi


\usepackage{amsmath,amsfonts,bm,mathtools,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow,longtable}
\usepackage{nccmath}
\usepackage{makecell}
\usepackage{footmisc}
\usepackage{cleveref}
\usepackage{xcolor,colortbl}
\usepackage{subcaption}  %
\usepackage{multirow, makecell}

\Crefname{section}{\S}{\S\S}



\title{Large Language Models Are Human-Like Internally}


\author{Tatsuki Kuribayashi${}^1$\, Yohei Oseki${}^2$\, Souhaib Ben Taieb${}^{1, 3}$ \\ \textbf{Kentaro Inui${}^{1,4,5}$} \, \textbf{Timothy Baldwin${}^{1,6}$} \\
        ${}^{1}$MBZUAI
        ${}^{2}$The University of Tokyo  \,
        ${}^{3}$University of Mons  \\
        ${}^{4}$Tohoku University \,
        ${}^{5}$RIKEN  \,
        ${}^{6}$The University of Melbourne \\
  \texttt{\{tatsuki.kuribayashi,souhaib.bentaieb,}\\ \texttt{kentaro.inui,timothy.baldwin\}@mbzuai.ac.ae} \\
  \texttt{oseki@g.ecc.u-tokyo.ac.jp}
}


\begin{document} 
\maketitle
\begin{abstract}

Recent cognitive modeling studies have reported that larger language models (LMs) exhibit a poorer fit to human reading behavior~\cite{Oh2023-zw,Shain2022-qv,kuribayashi-etal-2024-psychometric}, leading to claims of their cognitive implausibility. In this paper, we revisit this argument through the lens of mechanistic interpretability and argue that prior conclusions were skewed by an exclusive focus on the final layers of LMs. Our analysis reveals that next-word probabilities derived from internal layers of larger LMs align with human sentence processing data as well as, or better than, those from smaller LMs. This alignment holds consistently across behavioral (self-paced reading times, gaze durations, MAZE task processing times) and neurophysiological (N400 brain potentials) measures, challenging earlier mixed results and suggesting that the cognitive plausibility of larger LMs has been underestimated. Furthermore, we first identify an intriguing relationship between LM layers and human measures: earlier layers correspond more closely with fast gaze durations, while later layers better align with relatively slower signals such as N400 potentials and MAZE processing times.
Our work opens new avenues for interdisciplinary research at the intersection of mechanistic interpretability and cognitive modeling.


\end{abstract}


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/fig1.pdf}
    \caption{Different measures of human sentence processing align with surprisal from different layers of language models (LMs), and the best layer is typically not the final one. Larger LMs can better simulate human reading data with their internal layer than smaller LMs.
    }
    \label{fig:overview}
\end{figure}


\section{Introduction}
\label{sec:intro}

Understanding human sentence processing has long been a fundamental goal in linguistics. This goal is typically approached by investigating \textit{what computational models can simulate human sentence processing data}, such as eye movement patterns during reading, in the field of computational psycholinguistics~\cite{Crocker2010-cp,Beinborn2024-lw}.  Natural language processing (NLP) models, such as neural language models (LMs), have played a crucial role in this endeavor, serving as tools to test linguistic hypotheses.  Specifically, the theory of expectation-based human sentence processing~\cite{hale-2001-probabilistic,Levy2008Expectation-basedComprehension,Smith2013-ap} --- which posits that humans continuously predict upcoming linguistic information during reading --- naturally raises the following questions:  how well do word probabilities (i.e., surprisal, $-\log p(\mathrm{word}|\mathrm{context})$) derived from LMs align with human sentence processing behavior?  What kind of LMs produce the most human-like surprisal?

Previous studies have provided substantial evidence supporting expectation-based accounts of human sentence processing (\citealt{Shain2022-qv}; \textit{inter alia}).  However, they reveal an intriguing trend: surprisal estimates from large language models (LLMs) often deviate from human reading behavior, and rather smaller models, such as GPT-2 small, offer better simulations of human behavior~\cite{Shain2022-qv,Oh2023-zw,kuribayashi-etal-2022-context,kuribayashi-etal-2024-psychometric}.
This observation -- \textit{larger LMs are less human-like} --- has sparked intriguing linguistic questions~\cite{Wilcox2024-qx} as well as a fair amount of confusion within the community. 
Why do smaller LMs appear more human-like, despite their generally poorer linguistic competence~\cite{Waldis2024-rf}? 

In this work, we highlight the cognitively \textit{plausible} aspects of LLMs, challenging existing conclusions. 
Specifically, we show that \textbf{surprisal derived from the internal layers of larger LMs aligns with human sentence processing data as well as, or even better than, that from smaller LMs }. 
Previous studies, focusing exclusively on final-layers' surprisal, have overlooked this critical insight. 
These results could be drawn with techniques from mechanistic interpretability~\cite{dar-etal-2023-analyzing,belrose2023eliciting,Wendler2024-wr}, \textit{logit lens}~\cite{logitlens} or dubbed \textit{early exits}~\cite{icml19shallowdeepnetworks}; we compute next-word surprisals directly from internal layers of LMs by projecting intermediate representations into the output vocabulary space, bypassing subsequent layers. 
We additionally revealed that surprisal from earlier layers fits better with fast human responses (first-pass gaze durations and self-paced reading time), while surprisal from later layers aligns more closely with slower measures (N400 and MAZE task data) (Figure~\ref{fig:overview}). 
This also resolves the previously suggested \textit{behavior–neurophysiology gap} in LM-based cognitive modeling: smaller LMs predict reading behavior better~\cite{Oh2023-zw}, while larger LMs excel in modeling neurophysiological data~\cite{Schrimpf2020-qa,michaelov2024revenge,Hosseini2024-jf}.
We would suggest that this gap stemmed from the inconsistent treatment of internal layers (e.g., exclusive reliance on final layers, or inconsistent inclusion of intermediate layers).
If all the internal layers were focused on, even larger LMs have human-like layers through the lens of both behavioral and neurophysiological data.

Our exploration aligns with the common view that different human measures, operating on distinct timescales, reflect somewhat different stages of sentence processing.
For example, fast responses, such as first-pass gaze durations ($\sim$200ms), capture early-stage lexical processing~\cite{Calvo2002-nx}, while slower responses, such as N400 event-related potentials ($\sim$400ms), correspond to deeper semantic integration~\cite{Lau2008-pb,Kutas2011-dt,Nour-Eddine2024-ub}. 
Analogously, internal layers of LLMs may encode these temporal distinctions: earlier layers align with fast, shallow processes, while later layers correspond to slower, richer processes~\cite{Tenney2019-bb}.


In summary, our results suggest that larger LMs 
 provide superior cognitive plausibility in modeling both human behavior and neurophysiology data internally. 
In other words, shallower, cognitively plausible LMs are ``nested'' within LLMs. 
Broadly, these findings advocate the integration of cognitive modeling and mechanistic interpretability, encouraging a focus on layer-wise alignment with human measures. 









\section{Related Work}
\label{sec:background}

\subsection{Cognitive modeling and NLP}
\label{subsec:cog_model}


A key objective in linguistics is to understand how humans process language~\cite{Crocker2010-cp}, a goal that remains pertinent even in the era of LLMs. According to perspectives outlined by \citet{marr1982}, information processing can be examined at three levels:
(i) the computational level—\textit{what is the goal of computation?}, 
(ii) the algorithmic level—\textit{how does the model achieve the goal?}, and 
(iii) the implementational level—\textit{how is it physically implemented?}. 
Humans can be viewed as an information processing model, and surprisal theory~\cite{hale-2001-probabilistic,Levy2008Expectation-basedComprehension,Smith2013-ap} provides empirical evidence as a computational-level explanation of human sentence processing (\citealt{Smith2013-ap,FRANK20151,Shain2022-qv}; \textit{inter alia}).\footnote{Surprisal theory has also been critiqued~\cite{Van_Schijndel2021-sm,Huang2024-qe}, particularly for its failure to account for the cognitive load incurred in complex sentences. Our contribution will be orthogonal to such criticism (see Limitation section).} 
According to surprisal theory, humans continuously predict upcoming information during reading, with cognitive load incurred by unpredictable information.

Once the computational-level theory is assumed, the next question can be more relevant to the algorithmic level: 
\textit{with what kind of algorithms and representations, humans predict upcoming information}. 
The NLP community has developed various methods to compute next-word probabilities, ranging from N-gram models to LLMs. 
By exploring which NLP models best simulate human sentence processing, researchers can bridge the models demanded in psycholinguistics (language processing in humans) and those implemented in NLP (language processing in machines). 
This alignment is typically investigated by analyzing which LMs compute surprisal $-\log p(\mathrm{word}|\mathrm{context})$ that correlates with human measures (e.g., word-by-word gaze durations) based on the surprisal theory. 
Orthogonal to the exploration of various model architectures, this study investigates in which part/layer of a model human-like measures emerge. 

\subsection{Poorer fit of larger LMs' surprisal to human reading behavior}
\label{subsec:llm_cog}
Before the advent of LLMs, models such as incremental parsers~\cite{Levy2008Expectation-basedComprehension}, N-gram LMs~\cite{Smith2013-ap}, RNN-based LMs~\cite{frank2011insensitivity,Aurnhammer2019-fu}, Transformer-based LMs~\cite{Merkx2020ComparingData}, and syntactic LMs~\cite{Hale2018FindingSearch,Yoshida2021-rc,Oh2021-ln,Oh2022-ss} were extensively analyzed in cognitive modeling. 
In the days of much smaller LMs than modern LLMs, LM-scaling generally improved their ability to simulate human sentence processing data~\cite{frank2011insensitivity,Goodkind2018PredictiveQuality,Wilcox2020OnBehavior,Wilcox2023-bb}. 
However, recent studies have questioned the generality of this scaling effect. 
The reversed trend was first found in typologically distant languages~\cite{kuribayashi-etal-2021-lower}, and even within English, further scaling up LMs have shown weaker alignment with human reading behavior~\cite{kuribayashi-etal-2022-context,Shain2022-qv,Oh2023-zw}. 
This \textit{bigger is not always better} phenomenon has become a key focus of LM-based cognitive modeling~\cite{Wilcox2024-qx}, with researchers investigating why LLMs appear cognitively implausible~\cite{kuribayashi-etal-2022-context,Oh2023-hj,Oh2023-zw,Oh2024-cc,nair2023words,kuribayashi-etal-2024-psychometric}.
This study contributes to this discussion from a different perspective: 
We show the cognitively plausible properties of LLMs rather than solely identifying their limitations. 

In addition, recent studies have typically reported 
\textit{behavior–neurophysiology gap} in LM-scaling effects~\cite{michaelov2024revenge,aw2024instructiontuning};
 smaller LMs simulate reading behavior better~\cite{Oh2023-zw}, while larger LMs simulate neurophysiological data better~\cite{Schrimpf2020-qa,michaelov2024revenge,Hosseini2024-jf}. 
Opposite effect of instruction-tuning was also observed between  brain data and behavioral data~\cite{aw2024instructiontuning,kuribayashi-etal-2024-psychometric}. 
We begin by offering a perspective to address this gap, showing that the internal layers of larger LMs are more effective at modeling both behavioral and neurophysiological data.


\subsection{Mapping different human measures with different LM layers}
\label{subsec:layer_motivation}
We analyze the next-word predictions from the internal layers of LLMs in comparison with human sentence processing data. 
One motivation for this analysis is that different human measures, particularly at different time scales, may emphasize different stages of sentence processing~\cite{Witzel2012-nr,Lewis2005-hp,Vani2021-aj,Caucheteux2023-tx,McCurdy2024-ix}. 
LM internal layers, which are also computed sequentially, would be a natural counterpart to such multiple stages of processing~\cite{Tenney2019-bb}.
For example, eye movements reach the next word (or further) typically in $\sim$200ms before N400 brain signals peak at $\sim$400ms~\cite{Dimigen2011-gt}, suggesting that fast gaze durations may not reflect the cognitive load indexed by N400 signals~\cite{Rayner2009-aw}. 

Furthermore, self-paced reading and eye-tracking measures often exhibit \textit{spillover effects}, where the processing of one word influences subsequent words~\cite{Rayner1998-rw}. This suggests that the comprehension of a word extends beyond the immediate moment, and the associated reading times may only capture an early stage of processing.
In contrast, the MAZE task~\cite{Forster2009-dp,Boyce2023AmazeON}, which measures the time taken to select a plausible continuation from two candidates, mitigates spillover effects and is thus expected to reflect the full process of word processing. 
Our findings align with this perspective. Early LM layers are more closely aligned with gaze durations and self-paced reading times, while later layers show a stronger alignment with MAZE, as discussed in~\cref{sec:results}.

Note that similar layer-wise LM-brain alignments have been attempted in fMRI modeling research~\cite{Toneva2019-ul,Schrimpf2020-qa,Caucheteux2023-tx} and suggested that different brain areas better align with different LM layers.
They typically trained linear regression models to predict brain activity directly using $d$-dimentional LM internal representations $\bm h \in \mathbb{R}^d$ as features, instead of using surprisal measures $-\log p(\mathrm{word}|\mathrm{context})\in \mathbb{R}_{\ge 0}$.
Thus, their results are not comparable with exitsing surprisal-based studies and may rather suffer from a confounding factor of different $d$ for different LMs when precisely discussing LM-scaling effects. 

\subsection{Early exits of neural models' prediction}
\label{subsec:adaptive}
In the engineering context, predictions from internal layers of deep neural networks (i.e., early exists of the results from internal layers) are used to improve inference efficiency by avoiding their overthiking~\cite{Graves2016-dp,Banino2021-oq}, which is also called adaptive computation time~\cite{icml19shallowdeepnetworks,Zhou2020-vz}. Such technique has also recently been employed to enhance interpretability research to identify at which layer a particular prediction shapes~\cite{logitlens,dar-etal-2023-analyzing,belrose2023eliciting}. 
We apply \textit{logit lens}~\cite{logitlens} and its variant of \textit{tuned lens}~\cite{belrose2023eliciting}, projecting intermediate representations to the output space, to extract next-word predictions from the internal layers of LLMs.


\section{Experimental Settings}
\label{sec:settings}

\subsection{Human Data}
\label{subsec:data}
Table~\ref{tbl:results} lists the 15 human reading datasets used in our study (we additionally use MECO in~\cref{subsec:cross-lingual}), which include human measurements from various methods: self-paced reading time (SPR), first-pass gaze duration (FPGD), Maze task processing time (MAZE), and electroencephalography (EEG; specifically the N400 component). 
The datasets share a common format: each word $w_t$ is annotated with $\mathrm{Cost}(w_t) \in \mathbb{R}$ representing the human cognitive load associated with it. 
Our corpus selection aligns with recent studies~\cite{kuribayashi-etal-2024-psychometric,michaelov2024revenge,de-Varda2024-yh,McCurdy2024-ix}.\footnote{We applied the same preprocessing as~\citet{kuribayashi-etal-2024-psychometric} (DC, NS), \citet{de-Varda2024-yh} (UCL), \citet{Hahn2022-ib} (Fillers), and \citet{michaelov2024revenge} (N400). For ZuCO, we only used the naturalistic reading part, and for its N400, we averaged the values at the central electrode between 300-500ms during the first pass over a word.}

SPR is measured by presenting sentences through a sliding word-by-word window, with participants pressing a button to advance. 
FPGD, a key eye-tracking measure, represents the total time from first fixating on a word to moving to another word. 
Maze processing time is measured during a task requiring participants to select the plausible continuation of a sentence, offering a controlled alternative to naturalistic reading. 
EEG measures brain activity, with N400 reflecting the negative brain potential peaking around 400ms after word presentation. 
These are the common measures employed to study expectation-based sentence processing.
SPR, FPGD, and MAZE are categorized as human \textit{behavioral} data, while EEG falls under \textit{neurophysiological} data.

To minimize confounding factors between stimuli data and human measures, we included datasets with multi-layered annotations across multiple human measures. 
These include the Natural Stories Corpus~\cite{Futrell2021-wr} with SPR and MAZE data~\cite{Boyce2023AmazeON}, ZuCO corpus~\cite{Hollenstein2018-rm} with FPGD and N400 data, UCL Corpus~\cite{frank2013reading} annotated with SPR, FPGD, and N400 data~\cite{FRANK20151}, and filler sentences from~\citet{Hahn2022-ib} annotated with SPR, FPGD, and MAZE data~\cite{Vasishth2010-ji,Hahn2022-ib}. In particular, the FPGD and N400 data in ZuCO were simultaneously recorded from the same human subjects, which likely minimized confounding factors.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/main_results.pdf}
    \caption{Relationships between layer depth (x-axis) and $\Delta$LL (y-axis) for each LM in two datasets: FPGD in DC and SPR in NS. The graphs are separated by model families and data. The best/last layer is indicated with a red/black edge line. The graph starts at the first layer, not at the embedding layer.}
    \label{fig:reading_time}
\end{figure*}


\begin{table*}[t]
    \centering
    \scriptsize
    \tabcolsep0.03cm
    \begin{tabular}{llrrrrrp{0.2cm}rrrrr}
    \toprule
 &  & \multicolumn{5}{c}{Logit-lens ($\Delta$LL)} && \multicolumn{5}{c}{Tuned-lens ($\Delta$LL)} \\
 \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
Stimuli & Measure & 0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1.0 & & 0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1.0 \\
 \cmidrule(r){1-1} \cmidrule(r){2-2}  \cmidrule(lr){3-3} \cmidrule(lr){4-4}  \cmidrule(lr){5-5} \cmidrule(lr){6-6}  \cmidrule(lr){7-7}   \cmidrule(lr){9-9}  \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12}  \cmidrule(lr){13-13}  
DC (Dundee corpus) & FPGD~\cite{kennedy2003dundee} & 16.49 & \textbf{17.75} & 17.41 & 15.13 & 10.44 && \textbf{17.10} & 16.32 & 15.39 & 13.53 & 10.49 \\
\cmidrule(r){1-2} \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirowcell{2}[0pt][l]{NS \\ (Natural Stories Corpus)}   & SPR~\cite{Futrell2021-wr} & 23.59 & \textbf{23.75} & 21.16 & 13.37 & 7.11 && \textbf{16.88} & 14.53 & 11.36 & 8.14 & 6.32 \\
& MAZE~\cite{Boyce2023AmazeON} & 1.63 & 4.26 & 8.22 & 18.19 & \textbf{31.81} && 9.70 & 17.56 & 24.15 & 32.86 & \textbf{39.63} \\
\cmidrule(r){1-2} \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
 \multirow[c]{2}{*}{ZuCO} & FPGD~\cite{Hollenstein2018-rm} & 34.84 & \textbf{35.39} & 31.97 & 23.08 & 9.75 & & \textbf{30.48} & 27.16 & 22.56 & 17.29 & 8.77 \\
 & N400~\cite{Hollenstein2018-rm} & 0.06 & 0.10 & 0.15 & \textbf{0.20} & 0.15 & & 0.20 & 0.32 & \textbf{0.34} & 0.29 & 0.18 \\
\cmidrule(r){1-2}  \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirow[c]{3}{*}{UCL}   & SPR~\cite{frank2013reading} & \textbf{24.51} & 23.35 & 18.84 & 7.80 & 1.81 && \textbf{15.78} & 8.92 & 4.87 & 2.53 & 1.27 \\
& FPGD~\cite{frank2013reading} & 22.62 & \textbf{26.24} & 25.12 & 15.55 & 5.02 && \textbf{16.28} & 14.48 & 11.87 & 9.47 & 5.57 \\
 & N400~\cite{FRANK20151} & \textbf{57.45} & 33.30 & 14.01 & 12.89 & 32.26 && 11.31 & 6.12 & 16.19 & 29.49 & \textbf{37.11} \\
 \cmidrule(r){1-2} \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirowcell{3}[0pt][l]{Fillers in \\ \citet{Vasishth2010-ji}}  & SPR~\cite{Vasishth2010-ji} & 7.37 & 12.11 & 15.31 & \textbf{15.82} & 15.75 && 8.60 & 10.47 & 11.36 & 11.86 & \textbf{13.33} \\
& FPGD~\cite{Vasishth2010-ji} & 8.93 & 8.07 & 8.44 & 8.98 & \textbf{12.01} && 8.94 & 10.91 & 12.91 & 13.81 & \textbf{14.00} \\
& MAZE~\cite{Hahn2022-ib} & 4.72 & 2.99 & 7.53 & 35.86 & \textbf{83.16} && 9.96 & 28.27 & 52.00 & 73.38 & \textbf{88.64} \\
  \cmidrule(r){1-2} \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
Michaelov+,
2024 & N400~\cite{michaelov2023strong} & 1.07 & 1.79 & \textbf{2.37} & 2.10 & 1.13 && 0.95 & 1.51 & \textbf{1.70} & 1.38 & 0.99 \\
 \cmidrule(r){1-2} \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
 Federmeier+, 2007 & N400~\cite{Federmeier2007-qg} & 0.85 & 3.31 & 9.87 & 23.22 & \textbf{27.72} & & 1.49 & 5.22 & 13.06 & 24.48 & \textbf{28.71} \\
W\&F, 2012 & N400~\cite{Wlotko2012-eh} & \textbf{0.44} & 0.24 & 0.09 & 0.08 & 0.13 && \textbf{0.51} & 0.27 & 0.12 & 0.05 & 0.11 \\
Hubbard+, 2019 & N400~\cite{Hubbard2019-vz} & 0.18 & 0.19 & 0.24 & \textbf{0.30} & 0.22 && 0.11 & 0.12 & 0.22 & \textbf{0.36} & 0.33 \\
S\&F,2022 & N400~\cite{Szewczyk2022-ds} & 0.13 & 0.18 & 0.39 & 1.02 & \textbf{1.36} && 0.16 & 0.33 & 0.77 & 1.29 & \textbf{1.42} \\
Szewczyk+, 2022 & N400~\cite{Szewczyk2022-cd} & 1.61 & 3.32 & 5.07 & 7.96 & \textbf{8.40} && 2.12 & 3.58 & 5.52 & 8.10 & \textbf{8.93} \\

\bottomrule
\end{tabular}
        \caption{All the results. The $\Delta$LL scores are averaged by the layer relative depth, e.g., first 20\% of layers as ``0-0.2,'' across models, and the best relative layer range for each data is highlighted in bold. $\Delta$LLs are multiplied by 1000 for brevity.} 
        \label{tbl:results}
\end{table*}


\subsection{Language Models}
\label{subsec:model}
We evaluated 21 open-source LMs including bilion-scale ones: GPT-2 (124M, 355M, 774M, and 1.5B parameters)~\cite{Radford_undated-nn}, OPT (125M, 1.3B, 2.7B, 6.7B, 13B, 30B, and 66B parameters)~\cite{opt}, and Pythia (14M, 31M, 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B parameters)~\cite{biderman2023pythia}. 
For tuned-lens experiments (\cref{subsec:lens}), we used 14 of these models based on the availability of pre-trained tuned lenses.\footnote{GPT-2 124M, 774M, 1.5B; OPT 125M, 1.3B, 6.7B; and Pythia 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B.} The number of internal layers in our models ranges from 6 to 64. Results for surprisal from the embedding layer are excluded. %

\subsection{Psychometric Predictive Power}
\label{subsec:ppp}
We evaluated the ability of surprisal values $\mathrm{surprisal}(w_t) \in \mathbb{R}_{\ge 0}$ to predict word-by-word human reading data $\mathrm{Cost}(w_t) \in \mathbb{R}$ using regression models with several baseline features:

{\small
\begin{align}
\nonumber
    \mathrm{Cost}(w_t) &\sim \mathrm{surprisal}(w_t) + \mathrm{surprisal}(w_{t-1}) \\
    \nonumber
    &+ \mathrm{surprisal}(w_{t-2}) \\
    \nonumber
    &+ \mathrm{length}(w_t) + \mathrm{freq}(w_t) \\ 
    \nonumber
    &+ \mathrm{length}(w_{t-1}) + \mathrm{freq}(w_{t-1}) \\
    \label{eq:main_regression}
    &+ \mathrm{length}(w_{t-2}) + \mathrm{freq}(w_{t-2}) \;\;\mathrm{.} \\ 
\label{eq:surprisal}
\mathrm{surprisal}(w_t) &= -\log p(w_t|\bm w_{<t}) \;\;\mathrm{.}
\end{align}
}

\noindent
Here, $w_t$ is $t$-th word in a text, and  $\bm w_{<t}=[w_1, \cdots, w_{t-1}]$ is its context. 
Length and unigram frequency of words are included as baseline factors,\footnote{Word length follows character number, and word frequency is estimated with \texttt{word\_freq}~\cite{robyn_speer_2022_7199437}.} and  $\mathrm{surprisal}(w_{t-1}) \in \mathbb{R}_{\ge 0}$ and $\mathrm{surprisal}(w_{t-2}) \in \mathbb{R}_{\ge 0}$ are also added to account for spillover effects.
This model design follows recent studies~\cite{wilcox-etal-2023-language,pimentel-etal-2022-effect},\footnote{For N400 data, we added baseline amplitude feature.}. 
We used statsmodels package~\cite{seabold2010statsmodels}.

We trained linear regression models both with and without $\mathrm{surprisal}(w_t)$, and we report the difference in the goodness of fit measured by log-likelihood ($\Delta$ log-likelihood; $\Delta$LL).
This indicates how strongly surprisal contributes to predicting human sentence processing data, i.e., higher  $\Delta$LL is better.
The key question is which LM layer computes surprisal with better $\Delta$LL.


Note that we used the same baseline factors across all datasets/models for a fair comparison, except for electrode random effects for \citet{michaelov2023strong}'s EEG data. 
As is common in preprocessing, we exclude data points with zero SPR/FPGD/MAZE.
Human data for each token in the corpus were averaged across subjects prior to analysis, following recent practices \cite{pimentel-etal-2022-effect,Oh2023-zw,kuribayashi-etal-2024-psychometric,de-Varda2024-yh}.



\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/params_ppp_all.pdf}
    \caption{
Scaling effect between model size (parameter counts in log scale) and $\Delta$LL. Each marker corresponds to each LM's $\Delta$LL score from its best layer (red edge) or the last layer (black edge).
The regression lines show the scaling effects, and the red line is for best-layer's $\Delta$LL while the grey one is for the last layer's. 
The maker type (shape/size/color) follows the legend in Figure~\ref{fig:overview}. 
Pythia 14M, 31M, and 70M models are excluded as outliers in the X-axis, but including them does not alter the conclusion.
    }
    \label{fig:params_ppp}
\end{figure*}



\subsection{Probabilities from internal layers}
\label{subsec:lens}
Let us begin with the simplest method of \textit{logit-lens}~\cite{logitlens} to extract the probability of a word $w_t$ from model internals. 
Given a $d$-dimentaional internal representation $\bm h_{l,t}  \in \mathbb{R}^d$ at the $l$-th layer and time step $t$, the probability of a word $w_t$ in its context  $\bm w_{<t}$ is obtained as follows:

{\small
\begin{align}
    \nonumber
    &p(w_t|\bm w_{<t}; \bm h_{l,t}) = \mathrm{LogitLens}(\bm h_{l,t})[\mathrm{id}(w_t)] \\
    &= \mathrm{softmax}(\mathrm{LayerNorm}(\bm h_{l,t})\bm W_U)[\mathrm{id}(w_t)]\mathrm{,}
    \label{eq:logit-lens}
\end{align}
}%
where $\bm W_U \in \mathbb{R}^{d \times |\mathcal{V}|}$ is an unembedding matrix obtained from LM's output layer, and $|\mathcal{V}| \in \mathbb{R}$ is model's vocabulary size. 
Simply put, the internal representaion $\bm h_{l,t}$ is mapped into output vocabulary space by applying $\bm W_U$ (i.e., skipping subsequent layers [$\bm h_{l+1, t}, \cdots, \bm h_{\mathrm{last}, t}$]), and next-word probability is obtained in that space.
The obtained probability (Eq.~\ref{eq:logit-lens}) is first transformed to surprisal (Eq.~\ref{eq:surprisal}), and then used in the regression model to predict human reading data (Eq.~\ref{eq:main_regression}).
$\mathrm{LayerNorm}(\cdot)$ in Eq.~\ref{eq:logit-lens} is the layer normalization at the last layer, and [$\mathrm{id}(w_t)$] extracts the probability for $w_t$\footnote{
If a word is split into multiple subwords, accumulated surprisal is used. See Eq.2 in \citet{kuribayashi-etal-2021-lower}.} from the probability distribution over $\mathcal{V}$, obtained through the $\mathrm{softmax}(\cdot): \mathbb{R}^{|\mathcal{V}|}\to {[0,1]}^{|\mathcal{V}|}$ function.

Building on the logit-lens, \citet{belrose2023eliciting} extended the method into \textit{tuned-lens} to handle the potential \textit{representational drifts} through layers. 
This technique introduces an additional linear transformation for each layer $l$ to mitigate the mismatch between the representation spaces of the $l$-th layer and the last layer:

{\small
\begin{align}
    \nonumber
    &p(w|\bm w_{<t}; \bm h_{l,t}) \\ &= \mathrm{LogitLens}(\bm h_{l,t}\bm W_l  + \bm b_l)[\mathrm{id}(w)]\mathrm{,}
    \label{eq:tuned}
\end{align}
}%
where $\bm W_l \in \mathbb{R}^{d\times d}$ and $\bm b_l \in \mathbb{R}^{d}$ are additionally trained to align the output of logit-lens with the last layer's next-word probability distribution on additional LM pretraining data.\footnote{We used publicly available tuned-lens: \url{https://huggingface.co/spaces/AlignmentResearch/tuned-lens/tree/main}. Their actual implementation used the representation of $\bm h_{l,t}\bm W'_l + \bm h_{l,t} + \bm b_l=\bm h_{l,t}(\bm W'_l + \bm 1) \bm b_l$, but we omit the identity matrix $\bm 1 \in \mathbb{R}^{d\times d}$ in Eq.~\ref{eq:tuned} by overriding $\bm W_l=\bm W'_l + \bm 1$.}
Notably, we do not fine-tune any part of the LMs for human data; instead, we observe the emerging correlations between next-word probabilities and human reading measures.





\section{Experimental results}
\label{sec:results}

Figures~\ref{fig:reading_time} (\cref{subsec:best_layer}) and~\ref{fig:params_ppp} (\cref{subsec:params_ppp}) summarize our main findings, with comprehensive results in Table~\ref{tbl:results}.


\subsection{Last layer does not yield the best $\Delta$LL}
\label{subsec:best_layer}


We first revisit the experimental settings (DC and NS datasets) of \citet{Oh2023-zw}. 
The line graphs in Figure~\ref{fig:reading_time} depict the layer-wise $\Delta$LL for each LM. 
A consistent pattern emerges in the reading behavior (FPGD/SPR) modeling: the $\Delta$LL decreases toward the final layer (the rightmost, less-transparent, large markers), indicating that the last layer often yields the lowest score compared to the internal layers of the same model. 
These results challenge the assumption, widely adopted in existing studies, that the last layer is the most reliable indicator of an LM's cognitive plausibility. 

Table~\ref{tbl:results} presents a detailed breakdown of all the datasets, averaging $\Delta$LL scores across relative layer positions (e.g., 0--0.2 for the first 20\% of layers). As shown in Figure~\ref{fig:reading_time}, the best-performing layer is often not the final one (0.8--1.0). The optimal layer varies based on the type of measurement and stimuli. For instance, SPR and FPGD data are best modeled by earlier layers, whereas MAZE processing times and N400 signals are better captured by later layers (as motivated in \cref{subsec:layer_motivation}). Furthermore, the optimal layer for the UCL dataset is among the earlier layers, while for the Fillers dataset, it lies among the later layers (perhaps associated with the complexity of stimuli sentences). The layer--measure intearction are further analyzed in~\cref{subsec:layer_and_measure}.
Notably, logit-lens and tuned-lens yilded geenrally consistent patterns.


\subsection{Revisiting LM-scaling effects in cognitive modeling with internal layers}
\label{subsec:params_ppp}

We revisit the question with our extended focus on model internals: what kind of LMs yield the best $\Delta$LL from their internals?  As the field is particularly interested in the relationship with model scaling~\cite{Goodkind2018PredictiveQuality,Oh2023-zw}, we examine the relationship between LM parameter size (x-axis) and $\Delta$LL (y-axis) for two scenarios:  (1) using the last layer's $\Delta$LL (grey lines), reproducing previous findings, and (2) considering the best $\Delta$LL layer identified in this study (red lines).


Figure~\ref{fig:params_ppp} illustrates these two relationships. The grey lines align with prior findings replying on the last layer~\cite{Oh2023-zw,michaelov2024revenge}, showing mixed scaling effects, where larger LMs do not consistently outperform smaller ones. However, the red lines reveal a positive scaling trend: larger LMs achieve equal or better $\Delta$LL compared to smaller LMs when internal layers are taken into account. 
The Pearson correlation coefficients between parameter numbers and $\Delta$LL from the best layers was significantly larger than zero on average, across settings.\footnote{We collected the correlation coefficients between params. and $\Delta$LL from 34 settings of $\{\mathrm{dataset}\}\times\{\mathrm{lens}\}$, and one-sample t-test shows that these coeffiencts are, on average, significantly larger than zero ($\text{p-value}<0.05$).}
This suggests that when the analysis extends to internal layers, the $\Delta$LL ranking flips, revealing that larger LMs are seemingly more cognitively plausible. In other words, larger LMs embed cognitively plausible, smaller LMs within their internal. One notable exception is the MAZE processing time in the NS dataset~\cite{Boyce2023AmazeON}, where a strictly negative scaling effect persists, even when internal layers are considered. 
PPL–$\Delta$LL relationships\footnote{Perplexity (PPL), a general quality measure of LMs, is a geometric mean of next-word probabilities over data $L$: $\prod_{t=1}^{|L|} p(w_t|\bm w_{<t})^{1/|L|}$. The PPL--$\Delta$LL relationship has long been investigated~\cite{frank2011insensitivity,Goodkind2018PredictiveQuality,kuribayashi-etal-2021-lower,Oh2023-zw}.} are additionally shown in Figure~\ref{fig:ppl_ppp} in the Appendix, which also show that the poor $\Delta$LL of larger, more accurate LMs is mitigated.






\section{Analyses}
\label{sec:analysis}

\begin{table*}[t]
    \centering
    \scriptsize
    \tabcolsep0.02cm
\begin{tabular}{lrrrrrrrrrrrrrrrrrrr}
\toprule
 & \multicolumn{12}{c}{Logit-lens (win rate)} & \multicolumn{7}{c}{Tuned-lens (win rate)} \\
 \cmidrule(r){2-13} \cmidrule(r){14-20}
 & GPT2 & OPT& OPT& OPT & OPT& OPT & OPT & PT & PT & PT & PT & PT & GPT2 & OPT & OPT & PT & PT & PT & PT \\
 Data & XL & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 1B & 1.4B & 2.8B & 6.9B & 12B & XL & 1.3B & 6.7B & 1.4B & 2.8B & 6.9B & 12B \\
  \cmidrule(r){1-1} \cmidrule(r){2-2}  \cmidrule(lr){3-3} \cmidrule(lr){4-4}  \cmidrule(lr){5-5} \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8}  \cmidrule(lr){9-9}  \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12}  \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20} 
DC FPGD~\cite{kennedy2003dundee} & 0.80 & 0.80 & 0.82 & 0.76 & 0.73 & 0.76 & 0.78 & 0.00 & 0.32 & 0.36 & 0.73 & 0.73 & 0.73 & 0.80 & 0.67 & 0.64 & 0.58 & 0.55 & 0.54 \\
NS SPR~\cite{Futrell2021-wr} & 0.82 & 0.80 & 0.85 & 0.76 & 0.73 & 0.76 & 0.85 & 0.47 & 0.52 & 0.45 & 0.21 & 0.41 & 0.55 & 0.76 & 0.70 & 0.56 & 0.39 & 0.36 & 0.41 \\
ZuCO FPGD~\cite{Hollenstein2018-rm} & 0.80 & 0.84 & 0.88 & 0.79 & 0.73 & 0.78 & 0.86 & 0.76 & 0.72 & 0.70 & 0.85 & 0.84 & 0.80 & 0.84 & 0.76 & 0.72 & 0.67 & 0.61 & 0.65 \\
 UCL SPR~\cite{frank2013reading} & 0.78 & 0.80 & 0.79 & 0.76 & 0.76 & 0.78 & 0.80 & 0.71 & 0.52 & 0.64 & 0.70 & 0.70 & 0.73 & 0.80 & 0.61 & 0.52 & 0.36 & 0.42 & 0.35 \\
UCL FPGD~\cite{frank2013reading} & 0.94 & 0.88 & 0.82 & 0.79 & 0.83 & 0.88 & 0.83 & 0.59 & 0.56 & 0.58 & 0.70 & 0.73 & 0.90 & 0.92 & 0.91 & 0.96 & 0.48 & 0.73 & 0.73 \\
\bottomrule
\end{tabular}
    \caption{How likely $\Delta$LL from internal layers outperformed the previous best $\Delta$LL (achieved within the same model family, relying on their last layers). The results are focused on billion-scale models and behavioral data with somewhat drastic flips in LM-scaling effects for $\Delta$LLs. ``PT'' denotes ``Pythia.''}
    \label{tab:win_rate}
\end{table*}



\subsection{How easily can good layers be found?}
\label{subsec:layer_selection}
To assess how many layers yield a good $\Delta$LL, we analyze the amount of internal layers that outperform the previously best $\Delta$LL achieved by the last layer within the same model family. 
Table~\ref{tab:win_rate} presents the win rate of internal layers' $\Delta$LL against the respective previous best score.
The win rate is typically around 80\%, indicating a significant proportion of internal layers achieved good $\Delta$LL scores. 
These findings suggest that the cognitive plausibility of LLMs has been underestimated and that our argument (Figure~\ref{fig:params_ppp}) was not based on specific outlier layers but reflected a broader trend across many internal layers.









\subsection{Layer depth and human measures}
\label{subsec:layer_and_measure}
We observed systematic tendencies in the relationship between layer depth and human measurement methods. 
For instance, FPGD aligns better with earlier layers, whereas N400 aligns better with later layers, as summarized in Table~\ref{tbl:results}. 
To statistically validate this relationship, we trained a linear regression model to explain $\Delta$LL scores from our 15,833 experimental settings $s \in \{\mathrm{dataset}\}\times \{\mathrm{model}\} \times \{\mathrm{layer}\}$, using the following formula:

{\small
\begin{align}
    \nonumber
    \Delta \mathrm{LL}(s) &\sim \mathrm{stimuli}(s) + \mathrm{model}(s) + \mathrm{lens}(s) \\ 
    \nonumber
    +& \mathrm{layer\_depth}(s) + \mathrm{measure}(s) \\ 
    +& \mathrm{layer\_depth}(s) \times \mathrm{measure}(s),
    \label{eq:interaction}
\end{align}
}%

\noindent
where $\mathrm{stimuli}$ represents the source stimuli of the data (``Stimuli'' column in Table~\ref{tbl:results}), $\mathrm{model}$ encodes the model name, $\mathrm{layer\_depth}$ is the depth of the layer where the $\Delta$LL is obtained, $\mathrm{measure}$ encodes the human measurement method (``Measure'' column in Table~\ref{tbl:results}), and $\mathrm{lens}$ indicates whether the logit-lens or tuned-lens was used. The term $\mathrm{layer\_depth} \times \mathrm{measure}$ captures the interaction between effective layer depth and human measures, which is of interest. Note that $\mathrm{measure}$ is a categorical variable, with $\mathrm{FPGD}$ serving as the dummy category.

The coefficients for $\mathrm{layer\_depth} \times \mathrm{N400}$ and $\mathrm{layer\_depth} \times  \mathrm{MAZE}$ were significantly larger than zero ($\text{p-value}<0.001$), while that for $\mathrm{layer\_depth} \times \mathrm{SPR}$ does not significantly differ with $\mathrm{layer\_depth} \times \mathrm{FPGD}$ (see full regression results in  Table~\ref{tbl:coefficient_interaction} in the Appendix).
This confirms that SPR and FPGD align with shallower layers than those yielding a good fit with N400 and MAZE. 
We also visualize the relationships between $\Delta$LL and relative layer depth for each human measure in Figure~\ref{fig:diff} (polynomial fit using 2nd-order term). 
Here, we use corrected $\Delta$LLs that are computed by subtracting variances explained by other factors than $\mathrm{measure}$ identified in Eq.~\ref{eq:interaction}. 
The lines also indicate the differences across different human measures, e.g., good $\Delta$LL for MAZE is clearly associated with latter layers. 

It is worth noting that we also conducted an exploratory analysis with other human measures (e.g., P600, second-pass gaze duration) on the UCL corpus (Table~\ref{tbl:results_other_variables} in Appendix). 
For such an extended scope, we cannot confirm the implied relationship between fast–slow human responses and early–late LM layers, and this appears to hold primarily for well-established human measures of SPR, FPGD, N400, and MAZE.




\begin{table*}[t]
    \centering
    \scriptsize
    \tabcolsep=0.1cm
\begin{tabular}{llrrrrrp{0.2cm}rrrrr}
\toprule
& & \multicolumn{5}{c}{Logit-lens ($\Delta$LL)} && \multicolumn{5}{c}{Tuned-lens ($\Delta$LL)} \\
 \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
Stimuli & Measure & 0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1.0 & & 0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1.0 \\
 \cmidrule(r){1-1} \cmidrule(r){2-2}  \cmidrule(lr){3-3} \cmidrule(lr){4-4}  \cmidrule(lr){5-5} \cmidrule(lr){6-6}  \cmidrule(lr){7-7}   \cmidrule(lr){9-9}  \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12}  \cmidrule(lr){13-13}
 DC & FPGD~\cite{kennedy2003dundee} & 1.49 & 1.71 & \textbf{1.76} & 1.63 & 1.20 && 1.59 & 1.60 & \textbf{1.65} & 1.58 & 1.30 \\
\cmidrule(r){1-2}  \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirow[c]{2}{*}{NS}  & SPR~\cite{Futrell2021-wr} & \textbf{0.72} & 0.68 & 0.67 & 0.55 & 0.46 && 0.24 & 0.24 & 0.30 & 0.41 & \textbf{0.53} \\
 & MAZE~\cite{Boyce2023AmazeON} & 0.45 & 0.46 & 0.55 & 0.85 & \textbf{1.25} && 0.67 & 0.70 & 0.76 & 1.02 & \textbf{1.30} \\
\cmidrule(r){1-2}  \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirow[c]{2}{*}{ZuCO}  & FPGD~\cite{Hollenstein2018-rm} & 34.84 & \textbf{35.39} & 31.97 & 23.08 & 9.75 && \textbf{30.48} & 27.16 & 22.56 & 17.29 & 8.77 \\
 & N400~\cite{Hollenstein2018-rm} & 0.06 & 0.10 & 0.15 & \textbf{0.20} & 0.15 && 0.20 & 0.32 & \textbf{0.34} & 0.29 & 0.18 \\
\cmidrule(r){1-2}  \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirow[c]{3}{*}{UCL}  & SPR~\cite{frank2013reading} & \textbf{3.40} & 2.73 & 2.13 & 1.06 & 0.23 && \textbf{0.52} & 0.20 & 0.10 & 0.11 & 0.15 \\
& FPGD~\cite{frank2013reading} & 7.96 & \textbf{9.21} & 9.07 & 6.61 & 2.86 && \textbf{2.46} & 2.29 & 2.22 & 2.04 & 1.21 \\
 & N400~\cite{FRANK20151} & 3.77 & 1.96 & 2.64 & 8.07 & \textbf{16.31} && 1.34 & 2.60 & 6.30 & 9.52 & \textbf{13.26} \\
 \cmidrule(r){1-2}  \cmidrule(lr){3-7} \cmidrule(lr){9-13} 
\multirow[c]{3}{*}{Fillers}  & SPR~\cite{Vasishth2010-ji} & 0.37 & 0.40 & 0.89 & 2.89 & \textbf{8.74} && 0.55 & 1.19 & 1.31 & 1.68 & \textbf{2.94} \\
& FPGD~\cite{Vasishth2010-ji} & 0.26 & 0.22 & 0.21 & 0.62 & \textbf{1.50} && 0.37 & 0.32 & 0.27 & 0.71 & \textbf{1.19} \\
 & MAZE~\cite{Hahn2022-ib} & 4.78 & 3.46 & 2.05 & 5.77 & \textbf{12.19} && 0.49 & 0.94 & 3.43 & 5.33 & \textbf{6.13} \\
\bottomrule
\end{tabular}
\caption{Results for the same settings as Table~\ref{tbl:results}, except that only sentence/clause-final tokens are targeted. Some N400 data are omitted because they initially targeted  only sentence/clause-final tokens. $\Delta$LLs are multiplied by 1000 for brevity.} 
\label{tbl:results_clause_final}
\end{table*}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/layer_diff.pdf}
        \caption{All tokens.}
        \label{fig:diff}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/layer_diff_last_token.pdf}
        \caption{Clause-final tokens.}
        \label{fig:diff_last_token}
    \end{subfigure}
    \caption{Relationship between $\Delta$LL and relative layer depth (lower is shallower) for each human measure. Different measures are associated with different layers; for example, good $\Delta$LLs for FPGD are achieved in earlier layers, while those for MAZE are in the latter layers.}
    \label{fig:layer_diff}
\end{figure}


\subsection{Are results biased by targeted tokens?}
\label{subsec:clause_final}

A potential confound in~\cref{subsec:layer_and_measure} stems from differences in targeted tokens for different human measures. For instance, N400 data are typically recorded only at sentence-final tokens to preprocess continuous, time-series EEG data, potentially introducing biases specific to sentence-final tokens, such as wrap-up effects~\cite{Just1980AComprehension,Rayner2000TheReading,meister-etal-2022-analyzing}. In contrast, behavioral measures are recorded across tokens within a sentence. To address this potential confound, we conducted additional experiments targeting sentence/clause-final tokens for all measures, even for SPR, FPGD, and MAZE. Sentence/clause-final tokens are ideitified using a constituency parser~\cite{kitaev-etal-2019-multilingual,kitaev-klein-2018-constituency} and punctuations (e.g., ``.'', ``,''), following~\citet{meister-etal-2022-analyzing}.


Table~\ref{tbl:results_clause_final} presents the results. Even when analyses were restricted to sentence-final tokens, earlier layers continued to align better with SPR and FPGD data. Regression analysis (same as \cref{subsec:layer_and_measure}) confirmed that the coefficients for $\mathrm{layer\_depth} \times \mathrm{N400}$ and $\mathrm{layer\_depth} \times \mathrm{MAZE}$ remained significantly larger than those for $\mathrm{layer\_depth} \times \mathrm{FPGD}$. 
The coefficient for $\mathrm{layer\_depth} \times \mathrm{SPR}$ was also larger than that for $\mathrm{layer\_depth} \times \mathrm{FPGD}$ (see full regression results in Table~\ref{tbl:coefficient_interaction_last_token} in the Appendix). These patterns are visualized in Figure~\ref{fig:diff_last_token}.


\subsection{Cross-lingual experiments}
\label{subsec:cross-lingual}
We also examined the generality of our findings across languages by following the experiments of \citet{de-varda-marelli-2023-scaling}, using FPGD data in 13 languages, recorded in MECO~\cite{siegelman2022expanding} and five variants of multilingual XGLMs~\cite{lin-etal-2022-shot}.
See detailed settings and results in Appendix~\ref{app:cross_lingual} (Table~\ref{tbl:meco} and Figure~\ref{fig:params_ppp_meco}).

In summary, the best layer was typically not exactly the last layer (85\%=55/66 of the settings), aligned with our main results.
However, compared to our main results in Table~\ref{tbl:results}, the best layers leaned toward later depths. 
In addition, LM-scaling effects are still negative in several langauges.
These mixed results might be biased by the report that multilingual LMs process languages within the English subspace in middle layers~\cite{Wendler2024-wr}, leading to reduced alignment with target languages. 
Future work will explore scaling experiments with monolingual LMs, as suitable models to test scaling effects across languages are currently unavailable.

\section{When are earlier layers advantageous?}
\label{subsec:context}

We lastly explore when and why earlier layers' sueprisal aligns better with human reading data.

\subsection{Residual error analysis}
Following~\citet{Oh2023-zw}, we analyze by-token squared residual errors from regression models predicting human data (\cref{subsec:ppp}). 
We specifically use the largest data of DC (FPGD) and identify tokens where the use of the best internal layer, rather than the last layer, notably reduces errors.\footnote{\citet{Oh2023-zw} reported a misalignment between MSEs (residual errors) and log-likelihood scores due to the Euclidean norm penalty adopted in the \texttt{lme4} package. This did not arise in our analysis with \texttt{statsmodels}.} 
To address this question, we fit a linear regression model to predic the decreases in squared residual errors observed in each data point $w_t$ with an LM $\theta$, with by-token linguistic properties as features:
\begin{align}
    \nonumber
    &\mathrm{error\_decrease}(w_t, \theta) \sim \mathrm{model}(\theta) + \mathrm{length}(w_t)  \\ 
    &+ \mathrm{freq}(w_t) + \mathrm{position}(w_t) + \mathrm{POS}(w_t) \;\;\mathrm{.}
\end{align}

Results show that decreases in modeling error are associated with less frequent, longer words (see Table~\ref{tab:error_regression} in Appendix for full regression results). 
This aligns with prior observations~\cite{Oh2024-cc} that LLMs tend to predict infrequent tokens with overly confident surprisals, and surprisal from internal layers mitigates this issue.



\subsection{Contextualization in internal layers}
We propose an additional perspective linking our findings to the context sensitivity of humans and LMs during sentence processing. 
Earlier layers may better model human reading behavior because they are less contextualized, reflecting the human-like tendency to process sentences under working memory constraints.

\paragraph{Working memory limitations in humans}
Human sentence processing is constrained by limited cognitive resources, relying on selective and efficient use of context~\cite{Lewis2005-hp,Lieder2019-xo,Futrell2020Lossy-ContextProcessing,Hahn2022-ib}. 
Recent studies indicate that the MAZE task imposes greater working memory demands, requiring more extensive context use than SPR or FPGD~\cite{Hahn2022-ib,McCurdy2024-ix}. 
This aligns with our observation that MAZE processing times are better modeled by later, more context-sensitive layers (see the next paragraph), whereas SPR and FPGD align with earlier, less contextualized layers.

\paragraph{Working memory limitations in LMs}
Modern neural LMs are not optimized to conserve cognitive resources and often rely excessively on context information, resulting in superhuman predictions~\cite{kuribayashi-etal-2022-context,Oh2024-cc}. 
However, internal layers may exhibit a human-like moderation of context use. 
The NLP community has observed that LMs gradually enhance contextualization across layers, from shallow representations in early layers to deeply contextualized representations in later layers (\citealt{Brunner2019-so,Ethayarajh2019-zy,Toneva2019-ul}).

We also confirmed the by-layer context-sensitivity of surprisal by analyzing the correlation between (i) intermediate layers' surprisal and less-contextualized bigram surprisal\footnote{Bigram LM is trained on OpenWebText~\cite{Gokaslan2019OpenWeb} with KenLM toolkit~\cite{Heafield2011-ce}} and (ii) intermediate layers' surprisal and more contextualized surprisal from the LLM with the lowest PPL (OPT-66B). 
Figure~\ref{fig:contextualization} shows the above two types of correlations for each model's layer with the x-axis as the relative layer depth.
This shows that earlier layers correlate more strongly with bigram surprisal (Pearson correlation coefficient $r$ for this relationship is $-$0.92), while later layers align with more accurate, well-contextualized surprisal ($r$=0.95).
These findings reinforce the idea that earlier layers exhibit limited context sensitivity, while later layers are more contextualized and better suited for modeling data like MAZE, which demands higher contextualization.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{./figures/contextualization.pdf}
    \caption{
    The markers correspond to all the internal layers of our targeted LMs, which are sorted by relative layer depth (x-axis).  Two types of scores (y-axis) are plotted: (i) Pearson correlation coefficient between each layer's surprisal vs. less-contextualized bigram surprisal (blue), and (ii) each layer's surprisal vs. well-contextualized LLM surprisal (red). We used tuned-lens results.
    }
    \label{fig:contextualization}
\end{figure}




\section{Conclusions}
Recent cognitive modeling studies have demonstrated the worse fit of surprisal from larger LMs to human reading time.
In this paper, we argue that these negative results stem from the exclusive focus on the \textit{final layers} of LMs.
Instead, we observe that those from \textit{internal layers} comparably or even better fit with human behavior and neurophysiology data, suggesting that the cognitive plausibility of larger LMs has been underestimated.
Furthermore, different human measurements align with different layers, implying an intriguing parallel between temporal dynamics in human sentence processing and LM internal layers.

\section*{Limitations}
The experiments can be extended to more types of human measures, such as first fixation time, go-past reading time, eye regressions, total fixation time, EEG data other than the N400 potential~\cite{Federmeier2007-qg} as well as FMRI data~\cite{Shain2021-yk}.
As a first step, we have begun with the measures of SPR, FPGD, N400, and MAZE, as they are typically used in cognitive modeling.


Recent studies have raised several issues, orthogonal to our study, on LM-based cognitive modeling, for example, on tokenizations~\cite{nair2023words,Giulianelli2024-ms,Oh2024-cf}, LM training scenario~\cite{Oh2023-hj}, as well as more refined indicators of word predictability~\cite{pimentel-etal-2022-effect,Giulianelli2024-wd,Opedal2024-si,Meister2024-vq}.
More critically, the reliance solely on surprisal obtained from (not necessarily cognitively motivated) LMs will not tell the whole story of human sentence processing~\cite{Van_Schijndel2021-sm,Huang2024-qe,Prasad2024-yu,Wang2024-po}.
We believe that the combination of such more linguistically motivated perspectives and our analysis will enrich the findings (e.g., we can also obtain probabilities from internal layers of neural parsers). 
This study itself contributes to clarifying to what extent surprisal from modern LMs, which are seemingly behaving similarly to humans~\cite{Hu2024-qp}, can explain online human sentence processing.
More specifically, this study also opens up several new questions.
For example, how can the best layer for cognitive modeling be automatically found from a number of LLM layers? What are the information-theoretical insights for cognitive modeling and surprisal theory?

\bibliography{custom}
\bibliographystyle{acl_natbib}

\clearpage
\appendix

\section{Cross-lingual experiments}
\label{app:cross_lingual}

We target FPGD data from MECO in 13 languages~\cite{siegelman2022expanding} using five multilingual XGLMs~\cite{lin-etal-2022-shot} (564M, 1.7B, 2.9B, 4.5B, 7.5B).
We analyze the $\Delta$LL of surprisal from their internal layers. 
Due to the unavailability of tuned-lenses for XGLMs, only logit-lenses were used.
Table~\ref{tbl:meco} shows $\Delta$LLs per-layer-range, and Fugure~\ref{fig:params_ppp_meco} shows the relationship between paramter numbers and $\Delta$LL.


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/ppl_ppp_vertical.pdf}
    \caption{
Scaling effect between $\Delta$LL and PPL (measured on respective datasets with final layer), instead of model parameter counts, as adopted in Figure~\ref{fig:params_ppp}.
    }
    \label{fig:ppl_ppp}
\end{figure}

\begin{table}[ht]
    \centering
    \tiny
    \tabcolsep=0.01cm
    \begin{tabular}{lrrrrrp{0.1cm}rrrrr}
        \toprule
 & \multicolumn{5}{c}{Logit-lens ($\Delta$LL)} && \multicolumn{5}{c}{Tuned-lens ($\Delta$LL)} \\
 \cmidrule(lr){2-6}  \cmidrule(lr){8-12}
Measure & 0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1 & & 0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1 \\
    \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){8-8}   \cmidrule(lr){9-9}  \cmidrule(lr){10-10}  \cmidrule(lr){11-11} \cmidrule(lr){12-12} 
SPR & \textbf{24.51} & 23.35 & 18.84 & 7.80 & 1.81 && \textbf{15.78} & 8.92 & 4.87 & 2.53 & 1.27 \\
FPGD & 22.62 & \textbf{26.24} & 25.12 & 15.55 & 5.02& & \textbf{16.28} & 14.48 & 11.87 & 9.47 & 5.57 \\
SPGD & \textbf{241.76} & 191.77 & 124.82 & 30.13 & 3.97 && \textbf{68.43} & 20.05 & 5.28 & 1.71 & 3.87 \\
TOTAL & \textbf{211.73} & 175.06 & 119.29 & 32.39 & 3.06 && \textbf{67.39} & 23.51 & 7.82 & 1.61 & 1.56 \\
ELAN (125--175ms) & \textbf{0.79} & 0.38 & 0.18 & 0.30 & 0.49 && 0.22 & 0.38 & \textbf{0.77} & 0.67 & 0.50 \\
LAN (300--400ms) & \textbf{70.32} & 48.69 & 25.78 & 5.05 & 6.14 && \textbf{19.94} & 2.74 & 1.72 & 5.02 & 8.76 \\
N400 (300--500ms) & \textbf{57.45} & 33.30 & 14.01 & 12.89 & 32.26 && 11.31 & 6.12 & 16.19 & 29.49 & \textbf{37.11} \\
EPNP (400--600ms) & \textbf{78.22} & 58.65 & 35.25 & 7.76 & 3.09 && \textbf{29.31} & 6.67 & 1.29 & 1.56 & 4.45 \\
P600 (500--700ms) & \textbf{69.52} & 50.43 & 29.25 & 6.37 & 7.39 && \textbf{17.70} & 3.30 & 1.80 & 5.83 & 11.36 \\
PNP (600--700ms) & \textbf{60.47} & 46.76 & 29.58 & 7.32 & 1.56 && \textbf{25.38} & 8.09 & 2.30 & 0.73 & 1.61 \\
        \bottomrule
        \end{tabular}
        \caption{Results for other human measures recorded on the UCL corpus. $\Delta$LLs are multiplied by 1000.} 
        \label{tbl:results_other_variables}
\end{table}

\begin{table}[ht]
    \centering
    \tiny
    \tabcolsep=0.02cm
\begin{tabular}{lcccccc}
\toprule
Features & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                         &       0.0204  &        0.001     &    28.757  &         0.000        &        0.019    &        0.022     \\
\textbf{stimuli[T.Federmeier+,
2007]}      &       0.0025  &        0.001     &     3.945  &         0.000        &        0.001    &        0.004     \\
\textbf{stimuli[T.Fillers]}                &       0.0011  &        0.000     &     2.281  &         0.023        &        0.000    &        0.002     \\
\textbf{stimuli[T.Hubbard+,
2019]}         &      -0.0109  &        0.001     &   -17.559  &         0.000        &       -0.012    &       -0.010     \\
\textbf{stimuli[T.Michaelov+,
2024]}       &      -0.0096  &        0.001     &   -15.458  &         0.000        &       -0.011    &       -0.008     \\
\textbf{stimuli[T.NS]}                     &      -0.0038  &        0.001     &    -6.927  &         0.000        &       -0.005    &       -0.003     \\
\textbf{stimuli[T.S\&F,2022]}              &      -0.0105  &        0.001     &   -16.822  &         0.000        &       -0.012    &       -0.009     \\
\textbf{stimuli[T.Szewczyk+,
2022]}        &      -0.0057  &        0.001     &    -9.240  &         0.000        &       -0.007    &       -0.005     \\
\textbf{stimuli[T.UCL]}                    &       0.0054  &        0.000     &    11.807  &         0.000        &        0.004    &        0.006     \\
\textbf{stimuli[T.W\&F,2012]}              &      -0.0109  &        0.001     &   -17.593  &         0.000        &       -0.012    &       -0.010     \\
\textbf{stimuli[T.ZuCO]}                   &      -0.0007  &        0.000     &    -1.450  &         0.147        &       -0.002    &        0.000     \\
\textbf{model[T.gpt2-large]}               &       0.0010  &        0.001     &     1.633  &         0.103        &       -0.000    &        0.002     \\
\textbf{model[T.gpt2-medium]}              &      -0.0004  &        0.001     &    -0.542  &         0.588        &       -0.002    &        0.001     \\
\textbf{model[T.gpt2-xl]}                  &       0.0013  &        0.001     &     2.199  &         0.028        &        0.000    &        0.002     \\
\textbf{model[T.opt-1.3b]}                 &       0.0017  &        0.001     &     2.544  &         0.011        &        0.000    &        0.003     \\
\textbf{model[T.opt-125m]}                 &       0.0015  &        0.001     &     1.964  &         0.049        &     3.21e-06    &        0.003     \\
\textbf{model[T.opt-13b]}                  &       0.0017  &        0.001     &     2.505  &         0.012        &        0.000    &        0.003     \\
\textbf{model[T.opt-2.7b]}                 &       0.0019  &        0.001     &     2.735  &         0.006        &        0.001    &        0.003     \\
\textbf{model[T.opt-30b]}                  &       0.0011  &        0.001     &     1.644  &         0.100        &       -0.000    &        0.002     \\
\textbf{model[T.opt-6.7b]}                 &       0.0017  &        0.001     &     2.662  &         0.008        &        0.000    &        0.003     \\
\textbf{model[T.opt-66b]}                  &       0.0019  &        0.001     &     2.981  &         0.003        &        0.001    &        0.003     \\
\textbf{model[T.pythia-1.4b-deduped]}      &   -1.555e-05  &        0.001     &    -0.024  &         0.981        &       -0.001    &        0.001     \\
\textbf{model[T.pythia-12b-deduped]}       &       0.0012  &        0.001     &     1.993  &         0.046        &     2.03e-05    &        0.002     \\
\textbf{model[T.pythia-14m]}               &      -0.0033  &        0.001     &    -2.714  &         0.007        &       -0.006    &       -0.001     \\
\textbf{model[T.pythia-160m-deduped]}      &      -0.0026  &        0.001     &    -3.468  &         0.001        &       -0.004    &       -0.001     \\
\textbf{model[T.pythia-1b-deduped]}        &    8.195e-06  &        0.001     &     0.010  &         0.992        &       -0.002    &        0.002     \\
\textbf{model[T.pythia-1b-deduped-v0]}     &       0.0012  &        0.002     &     0.737  &         0.461        &       -0.002    &        0.004     \\
\textbf{model[T.pythia-2.8b-deduped]}      &       0.0005  &        0.001     &     0.814  &         0.416        &       -0.001    &        0.002     \\
\textbf{model[T.pythia-31m]}               &      -0.0031  &        0.001     &    -2.567  &         0.010        &       -0.005    &       -0.001     \\
\textbf{model[T.pythia-410m-deduped]}      &      -0.0003  &        0.001     &    -0.486  &         0.627        &       -0.002    &        0.001     \\
\textbf{model[T.pythia-6.9b-deduped]}      &       0.0007  &        0.001     &     1.173  &         0.241        &       -0.000    &        0.002     \\
\textbf{model[T.pythia-70m-deduped]}       &      -0.0027  &        0.001     &    -3.060  &         0.002        &       -0.004    &       -0.001     \\
\textbf{measure[T.MAZE]}                   &      -0.0261  &        0.001     &   -39.073  &         0.000        &       -0.027    &       -0.025     \\
\textbf{measure[T.N400]}                   &      -0.0124  &        0.001     &   -24.251  &         0.000        &       -0.013    &       -0.011     \\
\textbf{measure[T.SPR]}                    &      -0.0030  &        0.001     &    -5.303  &         0.000        &       -0.004    &       -0.002     \\
\textbf{method[T.tuned-lens]}              &    7.998e-05  &        0.000     &     0.389  &         0.697        &       -0.000    &        0.000     \\
\textbf{normalized\_layer}                 &      -0.0124  &        0.001     &   -20.423  &         0.000        &       -0.014    &       -0.011     \\
\cmidrule(lr){1-7}
\textbf{measure[T.MAZE]:normalized\_layer} &       0.0782  &        0.001     &    74.603  &         0.000        &        0.076    &        0.080     \\
\textbf{measure[T.N400]:normalized\_layer} &       0.0170  &        0.001     &    22.907  &         0.000        &        0.016    &        0.018     \\
\textbf{measure[T.SPR]:normalized\_layer}  &       0.0009  &        0.001     &     0.978  &         0.328        &       -0.001    &        0.003     \\
\bottomrule
\end{tabular}
    \caption{Coeffecients of the regression results in Figure~\ref{fig:diff}. The coefficients for the interaction terms listed in the last three lines are of interst.}
    \label{tbl:coefficient_interaction}
\end{table}


\begin{table}[t]
    \centering
    \tiny
    \tabcolsep=0.01cm
    \begin{tabular}{lcccccc}
    \toprule
Features & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                         &       0.0054  &        0.000     &    20.302  &         0.000        &        0.005    &        0.006     \\
\textbf{stimuli[T.Federmeier+,
2007]}      &       0.0366  &        0.000     &   137.401  &         0.000        &        0.036    &        0.037     \\
\textbf{stimuli[T.Fillers]}                &       0.0007  &        0.000     &     3.936  &         0.000        &        0.000    &        0.001     \\
\textbf{stimuli[T.Hubbard+,
2019]}         &       0.0232  &        0.000     &    87.206  &         0.000        &        0.023    &        0.024     \\
\textbf{stimuli[T.Michaelov+,
2024]}       &       0.0245  &        0.000     &    92.110  &         0.000        &        0.024    &        0.025     \\
\textbf{stimuli[T.NS]}                     &      -0.0014  &        0.000     &    -6.827  &         0.000        &       -0.002    &       -0.001     \\
\textbf{stimuli[T.S\&F,2022]}              &       0.0237  &        0.000     &    88.926  &         0.000        &        0.023    &        0.024     \\
\textbf{stimuli[T.Szewczyk+,
2022]}        &       0.0284  &        0.000     &   106.623  &         0.000        &        0.028    &        0.029     \\
\textbf{stimuli[T.UCL]}                    &       0.0020  &        0.000     &    11.513  &         0.000        &        0.002    &        0.002     \\
\textbf{stimuli[T.W\&F,2012]}              &       0.0232  &        0.000     &    87.126  &         0.000        &        0.023    &        0.024     \\
\textbf{stimuli[T.ZuCO]}                   &       0.0232  &        0.000     &   123.112  &         0.000        &        0.023    &        0.024     \\
\textbf{model[T.gpt2-large]}               &       0.0004  &        0.000     &     1.702  &         0.089        &    -5.95e-05    &        0.001     \\
\textbf{model[T.gpt2-medium]}              &      -0.0002  &        0.000     &    -0.561  &         0.575        &       -0.001    &        0.000     \\
\textbf{model[T.gpt2-xl]}                  &       0.0008  &        0.000     &     3.477  &         0.001        &        0.000    &        0.001     \\
\textbf{model[T.opt-1.3b]}                 &       0.0008  &        0.000     &     3.115  &         0.002        &        0.000    &        0.001     \\
\textbf{model[T.opt-125m]}                 &       0.0003  &        0.000     &     1.231  &         0.218        &       -0.000    &        0.001     \\
\textbf{model[T.opt-13b]}                  &       0.0008  &        0.000     &     2.952  &         0.003        &        0.000    &        0.001     \\
\textbf{model[T.opt-2.7b]}                 &       0.0009  &        0.000     &     3.425  &         0.001        &        0.000    &        0.001     \\
\textbf{model[T.opt-30b]}                  &       0.0006  &        0.000     &     2.412  &         0.016        &        0.000    &        0.001     \\
\textbf{model[T.opt-6.7b]}                 &       0.0005  &        0.000     &     2.184  &         0.029        &     5.24e-05    &        0.001     \\
\textbf{model[T.opt-66b]}                  &       0.0009  &        0.000     &     3.748  &         0.000        &        0.000    &        0.001     \\
\textbf{model[T.pythia-1.4b-deduped]}      &       0.0004  &        0.000     &     1.449  &         0.147        &       -0.000    &        0.001     \\
\textbf{model[T.pythia-12b-deduped]}       &       0.0010  &        0.000     &     4.172  &         0.000        &        0.001    &        0.001     \\
\textbf{model[T.pythia-14m]}               &      -0.0016  &        0.000     &    -3.566  &         0.000        &       -0.003    &       -0.001     \\
\textbf{model[T.pythia-160m-deduped]}      &      -0.0003  &        0.000     &    -1.082  &         0.279        &       -0.001    &        0.000     \\
\textbf{model[T.pythia-1b-deduped]}        &    1.503e-05  &        0.000     &     0.047  &         0.962        &       -0.001    &        0.001     \\
\textbf{model[T.pythia-1b-deduped-v0]}     &       0.0008  &        0.001     &     1.385  &         0.166        &       -0.000    &        0.002     \\
\textbf{model[T.pythia-2.8b-deduped]}      &       0.0003  &        0.000     &     1.218  &         0.223        &       -0.000    &        0.001     \\
\textbf{model[T.pythia-31m]}               &      -0.0014  &        0.000     &    -2.983  &         0.003        &       -0.002    &       -0.000     \\
\textbf{model[T.pythia-410m-deduped]}      &   -8.252e-05  &        0.000     &    -0.337  &         0.736        &       -0.001    &        0.000     \\
\textbf{model[T.pythia-6.9b-deduped]}      &       0.0008  &        0.000     &     3.247  &         0.001        &        0.000    &        0.001     \\
\textbf{model[T.pythia-70m-deduped]}       &      -0.0008  &        0.000     &    -2.471  &         0.013        &       -0.001    &       -0.000     \\
\textbf{measure[T.MAZE]}                   &      -0.0046  &        0.000     &   -18.669  &         0.000        &       -0.005    &       -0.004     \\
\textbf{measure[T.N400]}                   &      -0.0318  &        0.000     &  -135.362  &         0.000        &       -0.032    &       -0.031     \\
\textbf{measure[T.SPR]}                    &      -0.0053  &        0.000     &   -24.991  &         0.000        &       -0.006    &       -0.005     \\
\textbf{method[T.tuned-lens]}              &      -0.0009  &     7.74e-05     &   -11.156  &         0.000        &       -0.001    &       -0.001     \\
\textbf{normalized\_layer}                 &      -0.0079  &        0.000     &   -35.798  &         0.000        &       -0.008    &       -0.008     \\
\cmidrule(lr){1-7}
\textbf{measure[T.MAZE]:normalized\_layer} &       0.0124  &        0.000     &    32.324  &         0.000        &        0.012    &        0.013     \\
\textbf{measure[T.N400]:normalized\_layer} &       0.0145  &        0.000     &    52.124  &         0.000        &        0.014    &        0.015     \\
\textbf{measure[T.SPR]:normalized\_layer}  &       0.0094  &        0.000     &    27.774  &         0.000        &        0.009    &        0.010     \\
\bottomrule
\end{tabular}
    \caption{Coeffecients of the regression results in Figure~\ref{fig:diff_last_token} (only sentence/clause-final tokens are targeted).}
    \label{tbl:coefficient_interaction_last_token}
\end{table}

\begin{table}[ht]
    \centering
    \scriptsize
    \tabcolsep=0.2cm
\begin{tabular}{lrrrrr}
\toprule
 & \multicolumn{5}{c}{Logit-lens ($\Delta$LL)} \\
 \cmidrule(r){2-6}
Lang. &0-0.2 & 0.2-0.4 & 0.4-0.6 & 0.6-0.8 & 0.8-1 \\
      \cmidrule(r){1-1} \cmidrule(r){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6}
Du & 5.62 & 5.58 & 7.66 & 11.60 & \textbf{20.77} \\
Ee & 2.16 & 2.34 & 2.74 & \textbf{3.37}& 3.26 \\
En & 4.51 & 12.49 & 21.43 & \textbf{23.53} & 20.04 \\
Fi & 7.61 & 8.72 & 10.44 & 15.59 & \textbf{16.25} \\
Ge & 18.20 & 22.83 & 28.66 & \textbf{34.41} & 30.57 \\
Gr & 15.05 & 19.75 & 21.20 & \textbf{21.61} & 18.19 \\
He & \textbf{2.12} & 1.95 & 1.25 & 1.96 & 1.98 \\
It & 16.55 & 18.85 & 24.19 & 29.50 & \textbf{32.42} \\
Ko & 3.09 & 4.02 & 4.74 & 5.73 & \textbf{5.96} \\
No & 22.80 & 21.56 & 23.43 & 25.29 & \textbf{27.27} \\
Ru & \textbf{0.51} & 0.47 & 0.37 & 0.28 & 0.33 \\
Sp & 13.58 & 16.29 & 18.68 & \textbf{20.00} & 16.83 \\
Tr & 2.99 & 3.91 & 6.03 & 10.97 & \textbf{15.44} \\
\bottomrule
\end{tabular}
        \caption{Results in MECO. $\Delta$LLs are multiplied by 1000 for brevity.} 
        \label{tbl:meco}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/params_ppp_meco.pdf}
    \caption{
Scaling effect between $\Delta$LL and parameter counts in MECO. The grey lines are results replying on the last layer's $\Delta$LLs and, the red lines rely on the best internal layers' $\Delta$LLs.
In four languages (Fi, Gr, Ru, Sp) out of the targeted 13 languages, the scaling effect flipped from negative to positive, and we never observed the case of flipping from positive to negative.
    }
    \label{fig:params_ppp_meco}
\end{figure*}


\begin{table}[t]
    \centering
    \tiny
    \tabcolsep=0.02cm
    \begin{tabular}{lrrrrrr}
    \toprule
Features   & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{Intercept}                     &       0.0801  &        0.430     &     0.186  &         0.852        &       -0.764    &        0.924     \\
\textbf{model[T.gpt2-large]}           &       0.1510  &        0.061     &     2.490  &         0.013        &        0.032    &        0.270     \\
\textbf{model[T.gpt2-xl]}              &       0.1647  &        0.061     &     2.716  &         0.007        &        0.046    &        0.284     \\
\textbf{model[T.opt-1.3b]}             &       0.1293  &        0.061     &     2.132  &         0.033        &        0.010    &        0.248     \\
\textbf{model[T.opt-125m]}             &       0.0182  &        0.061     &     0.301  &         0.764        &       -0.101    &        0.137     \\
\textbf{model[T.opt-6.7b]}             &       0.1898  &        0.061     &     3.131  &         0.002        &        0.071    &        0.309     \\
\textbf{model[T.pythia-1.4b-deduped]}  &       0.0754  &        0.061     &     1.243  &         0.214        &       -0.043    &        0.194     \\
\textbf{model[T.pythia-12b-deduped]}   &       0.1649  &        0.061     &     2.719  &         0.007        &        0.046    &        0.284     \\
\textbf{model[T.pythia-160m-deduped]}  &       0.1237  &        0.061     &     2.040  &         0.041        &        0.005    &        0.243     \\
\textbf{model[T.pythia-1b-deduped-v0]} &       0.0695  &        0.061     &     1.146  &         0.252        &       -0.049    &        0.188     \\
\textbf{model[T.pythia-2.8b-deduped]}  &       0.1212  &        0.061     &     1.998  &         0.046        &        0.002    &        0.240     \\
\textbf{model[T.pythia-410m-deduped]}  &       0.0607  &        0.061     &     1.001  &         0.317        &       -0.058    &        0.180     \\
\textbf{model[T.pythia-6.9b-deduped]}  &       0.1534  &        0.061     &     2.529  &         0.011        &        0.035    &        0.272     \\
\textbf{model[T.pythia-70m-deduped]}   &      -0.0478  &        0.061     &    -0.788  &         0.431        &       -0.167    &        0.071     \\
\cmidrule(lr){1-7}
\textbf{pos[T.\$]}                     &      -0.1911  &        0.776     &    -0.246  &         0.805        &       -1.711    &        1.329     \\
\textbf{pos[T.,]}                      &      -4.1266  &        0.985     &    -4.188  &         0.000        &       -6.058    &       -2.195     \\
\textbf{pos[T..]}                      &      -2.5070  &        1.327     &    -1.890  &         0.059        &       -5.107    &        0.093     \\
\textbf{pos[T.CC]}                     &      -0.7410  &        0.431     &    -1.719  &         0.086        &       -1.586    &        0.104     \\
\textbf{pos[T.CD]}                     &      -1.6381  &        0.443     &    -3.699  &         0.000        &       -2.506    &       -0.770     \\
\textbf{pos[T.DT]}                     &      -0.9570  &        0.428     &    -2.238  &         0.025        &       -1.795    &       -0.119     \\
\textbf{pos[T.EX]}                     &      -1.5530  &        0.489     &    -3.176  &         0.001        &       -2.511    &       -0.595     \\
\textbf{pos[T.FW]}                     &       4.2889  &        0.580     &     7.395  &         0.000        &        3.152    &        5.426     \\
\textbf{pos[T.IN]}                     &      -1.1206  &        0.427     &    -2.625  &         0.009        &       -1.957    &       -0.284     \\
\textbf{pos[T.JJ]}                     &      -1.4790  &        0.427     &    -3.461  &         0.001        &       -2.317    &       -0.641     \\
\textbf{pos[T.JJR]}                    &      -1.6572  &        0.463     &    -3.580  &         0.000        &       -2.565    &       -0.750     \\
\textbf{pos[T.JJS]}                    &      -2.1918  &        0.488     &    -4.489  &         0.000        &       -3.149    &       -1.235     \\
\textbf{pos[T.MD]}                     &      -1.6324  &        0.435     &    -3.749  &         0.000        &       -2.486    &       -0.779     \\
\textbf{pos[T.NN]}                     &      -2.0801  &        0.427     &    -4.876  &         0.000        &       -2.916    &       -1.244     \\
\textbf{pos[T.NNP]}                    &      -1.0809  &        0.428     &    -2.523  &         0.012        &       -1.921    &       -0.241     \\
\textbf{pos[T.NNPS]}                   &      -5.2269  &        0.484     &   -10.804  &         0.000        &       -6.175    &       -4.279     \\
\textbf{pos[T.NNS]}                    &      -2.1715  &        0.428     &    -5.071  &         0.000        &       -3.011    &       -1.332     \\
\textbf{pos[T.PDT]}                    &      -2.2381  &        0.535     &    -4.183  &         0.000        &       -3.287    &       -1.189     \\
\textbf{pos[T.POS]}                    &      -2.0896  &        0.449     &    -4.651  &         0.000        &       -2.970    &       -1.209     \\
\textbf{pos[T.PRP]}                    &      -1.0883  &        0.430     &    -2.529  &         0.011        &       -1.932    &       -0.245     \\
\textbf{pos[T.PRP\$]}                  &      -1.4344  &        0.435     &    -3.296  &         0.001        &       -2.287    &       -0.581     \\
\textbf{pos[T.RB]}                     &      -1.7242  &        0.428     &    -4.026  &         0.000        &       -2.564    &       -0.885     \\
\textbf{pos[T.RBR]}                    &      -1.8143  &        0.482     &    -3.761  &         0.000        &       -2.760    &       -0.869     \\
\textbf{pos[T.RBS]}                    &      -3.0363  &        0.529     &    -5.743  &         0.000        &       -4.073    &       -2.000     \\
\textbf{pos[T.RP]}                     &      -1.4753  &        0.456     &    -3.235  &         0.001        &       -2.369    &       -0.582     \\
\textbf{pos[T.SYM]}                    &       2.6044  &        0.901     &     2.889  &         0.004        &        0.838    &        4.371     \\
\textbf{pos[T.TO]}                     &      -0.6908  &        0.431     &    -1.602  &         0.109        &       -1.536    &        0.155     \\
\textbf{pos[T.UH]}                     &      -7.8480  &        0.759     &   -10.338  &         0.000        &       -9.336    &       -6.360     \\
\textbf{pos[T.VB]}                     &      -1.2679  &        0.429     &    -2.955  &         0.003        &       -2.109    &       -0.427     \\
\textbf{pos[T.VBD]}                    &      -1.6735  &        0.431     &    -3.885  &         0.000        &       -2.518    &       -0.829     \\
\textbf{pos[T.VBG]}                    &      -2.0877  &        0.433     &    -4.818  &         0.000        &       -2.937    &       -1.238     \\
\textbf{pos[T.VBN]}                    &      -1.7750  &        0.431     &    -4.118  &         0.000        &       -2.620    &       -0.930     \\
\textbf{pos[T.VBP]}                    &      -1.1485  &        0.432     &    -2.659  &         0.008        &       -1.995    &       -0.302     \\
\textbf{pos[T.VBZ]}                    &      -1.2311  &        0.431     &    -2.860  &         0.004        &       -2.075    &       -0.387     \\
\textbf{pos[T.WDT]}                    &      -1.2174  &        0.449     &    -2.714  &         0.007        &       -2.097    &       -0.338     \\
\textbf{pos[T.WP]}                     &      -1.0289  &        0.455     &    -2.264  &         0.024        &       -1.920    &       -0.138     \\
\textbf{pos[T.WP\$]}                   &       0.1774  &        0.869     &     0.204  &         0.838        &       -1.525    &        1.880     \\
\textbf{pos[T.WRB]}                    &      -0.8454  &        0.456     &    -1.853  &         0.064        &       -1.740    &        0.049     \\
\textbf{has\_punct[T.True]}            &      -1.6396  &        0.038     &   -43.221  &         0.000        &       -1.714    &       -1.565     \\
\textbf{has\_num[T.True]}              &       2.3217  &        0.171     &    13.602  &         0.000        &        1.987    &        2.656     \\
\textbf{freq}              &       0.0142  &        0.007     &     1.930  &         0.054        &       -0.000    &        0.029     \\
\textbf{length}                        &       0.3629  &        0.007     &    51.621  &         0.000        &        0.349    &        0.377     \\
\textbf{token\_position\_in\_sentence}              &      -0.0042  &        0.001     &    -3.958  &         0.000        &       -0.006    &       -0.002     \\
\bottomrule
    \end{tabular}
    \caption{The results of regression model to predict the per-token squared residual errors. Note that word frequency and length are confounded (Pearson's $r=-0.7$), and once the length factor is excluded, the coefficient for the word frequency feature becomes significantly negative and has a high t-score. That is, a large decrease in regression error is generally associated with infrequent, long words.}
    \label{tab:error_regression}
\end{table}




\end{document}
