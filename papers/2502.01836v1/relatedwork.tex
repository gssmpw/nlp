\section{Related Work}
\label{sec-related-work}

\noindent \textbf{\textit{Data series indexes.}}
\label{sec-lit-index}
The most prominent data series indexing techniques can be categorized into graph-based indexes~\cite{DBLP:journals/pami/MalkovY20}, 
quantization~\cite{DBLP:journals/pami/GeHK014} and inverted indexes~\cite{DBLP:journals/pami/BabenkoL15}, 
locality-sensitive hashes~\cite{DBLP:journals/pvldb/HuangFZFN15},
and tree-based indexes~\cite{DBLP:conf/kdd/ShiehK08,DBLP:journals/pvldb/WangWPWH13}. 
Recent studies~\cite{DBLP:journals/pvldb/EchihabiZPB18, DBLP:journals/pvldb/EchihabiZPB19} have demonstrated that tree-based indexes~\cite{DBLP:conf/icde/PengFP20} achieve SOTA performance under several conditions (e.g., large-scale dataset).

iSAX~\cite{DBLP:conf/kdd/ShiehK08} and DSTree~\cite{DBLP:journals/pvldb/WangWPWH13} are two SOTA tree-based indexes for series similarity search of different strengths~\cite{DBLP:journals/pvldb/EchihabiZPB18, DBLP:journals/pvldb/EchihabiZPB19}.
iSAX is based on Symbolic aggregate approximation (SAX)~\cite{DBLP:conf/kdd/ShiehK08}, a discretized series summarization based on piecewise aggregate approximation (PAA)~\cite{DBLP:conf/sigmod/KeoghCMP01}.
PAA first transforms the data series into $l$ real values, and then SAX quantizes each PAA value using discrete symbols. 
iSAX (indexable SAX)~\cite{DBLP:conf/kdd/ShiehK08} enables the comparison of SAXs of different cardinalities, that makes SAX indexable through a prefix trie~\cite{DBLP:conf/aieeire/Briandais59}. 
DSTree~\cite{DBLP:journals/pvldb/WangWPWH13} is a dynamic splitting tree based on the adaptive piecewise constant approximation (EAPCA).
Furthermore, 
ADS+~\cite{DBLP:conf/sigmod/ZoumpatianosIP14} makes iSAX continuously adaptive to queries, 
ULISSE\cite{DBLP:journals/pvldb/LinardiP18,DBLP:journals/vldb/LinardiP20} supports variable-length queries, 
Coconut~\cite{DBLP:journals/pvldb/KondylakisDZP18} delivers a sortable iSAX variant, 
DPiSAX~\cite{DBLP:conf/icdm/YagoubiAMP17} and Odyssey~\cite{DBLP:journals/pvldb/ChatzakisFKPP23} make iSAX distributed, 
ParIS~\cite{DBLP:journals/tkde/PengFP21}, MESSI~\cite{DBLP:conf/icde/PengFP20,messijournal} and SING~\cite{DBLP:conf/icde/PengFP21} bring in modern hardware, FreSh~\cite{DBLP:conf/srds/FatourouKPP23} adds lock-freedom, 
Dumpy~\cite{DBLP:journals/pacmmod/Wang0WP023} and DumpyOS~\cite{DBLP:journals/vldb/WangWWPW24} introduce a data-adaptive multi-ary structure, 
while Hercules~\cite{DBLP:journals/pvldb/EchihabiFZPB22} and Elpis~\cite{DBLP:journals/pvldb/AziziEP23} combine the iSAX and EAPCA~\cite{DBLP:journals/pvldb/WangWPWH13} summarizations.

The proposed LeaFi framework is index agnostic.
We instantiate and evaluate it on both MESSI~\cite{messijournal} and DSTree~\cite{DBLP:journals/pvldb/WangWPWH13}, making its improvements translatable to most tree-based indexes.

\noindent \textbf{\textit{Machine learning applications in series indexes.}}
\label{sec-lit-learned-index}
Machine learning techniques have proven effective in enhancing various components of databases~\cite{DBLP:conf/sigmod/0001ZC21, DBLP:conf/sigmod/SaxenaRCLCCMKPN23}, such as indexes~\cite{DBLP:conf/sigmod/KraskaBCDP18, DBLP:conf/sigmod/NathanDAK20, DBLP:journals/pvldb/DingNAK20, DBLP:conf/icde/LiZSWT020, DBLP:conf/kdd/WangP21}, cardinality estimators~\cite{DBLP:conf/sigmod/Sun0021, DBLP:conf/sigmod/KimJSHCC22, DBLP:journals/pacmmod/WuNAKM23}, etc.
A few existing works are also motivated by the fact that there is waste in search time~\cite{DBLP:conf/sigmod/GogolouTEBP20, DBLP:journals/vldb/EchihabiTGBP23}. 
These works can be divided into two categories, early stopping approaches~\cite{DBLP:journals/pvldb/EchihabiZPB19, DBLP:conf/sigmod/GogolouTEBP20, DBLP:journals/vldb/EchihabiTGBP23} and leaf node reordering approaches~\cite{SC:kang2021case}.

In the context of data series similarity search, $\epsilon$-search identifies heuristic stopping criteria when best-so-far results are in the $\epsilon$ neighborhood of nearest neighbor results~\cite{DBLP:journals/pvldb/EchihabiZPB19}.
$\delta\epsilon$-search extends $\epsilon$-search by supporting a confidence level $\delta$, based on estimated pairwise distance distribution~\cite{DBLP:journals/pvldb/EchihabiZPB19}.
Progressive Search (ProS) incorporates machine learning models to estimate when the exact results are retrieved, using the query and best-so-far distances~\cite{DBLP:conf/sigmod/GogolouTEBP20, DBLP:journals/vldb/EchihabiTGBP23}.
Learned Reordering (LR) determines the visiting order of the leaf nodes by predicting their probabilities of containing the nearest neighbor results for a given query~\cite{SC:kang2021case}.

\begin{figure}[tb]
  \centering
  
  \subfloat{
  \includegraphics[width=0.66\linewidth]{legend-optimal-astro.pdf}}\\[-2ex]
  
  \setcounter{subfigure}{0}
  \subfloat[Early stopping
    \label{fig:liter-optimal-early-stopping}]{
    \includegraphics[width=.22\linewidth]{optimal-early-stop.pdf}}
  \subfloat[Reordering
    \label{fig:liter-optimal-reordering}]{
    \includegraphics[width=.22\linewidth]{optimal-reordering.pdf}}
  \subfloat[LeaFi
    \label{fig:liter-optimal-leafi}]{
    \includegraphics[width=.22\linewidth]{optimal-leafi.pdf}}
    
  \vspace*{-0.2cm}
  \caption{The optimal search time that can be possibly achieved by early-stopping approaches~\cite{DBLP:journals/pvldb/EchihabiZPB19, DBLP:conf/sigmod/GogolouTEBP20, DBLP:journals/vldb/EchihabiTGBP23}, leaf node reordering approaches~\cite{SC:kang2021case} and LeaFi for DSTree index on Astro dataset.
  The axes are the same as Figure~\ref{fig:intro-motiv-node-nn} (x-axis in Figure~\ref{fig:liter-optimal-leafi} has a different scale).
  }
  \label{fig:liter-comparison}
\end{figure}

Figure~\ref{fig:liter-comparison} illustrates the improvement potential for DSTree index on Astro dataset, as in Figure~\ref{fig:intro-motiv-num-nodes}, of early stopping approaches ($\epsilon$-search, $\delta\epsilon$-search, ProS and FLT), leaf node reordering approaches (LR), and our proposed learned filter approach (LeaFi).
These optimal performance is simulated by assuming all machine learning models make no mistake.
Leaf node reordering approaches help series indexes find the nearest neighbor results in the first node being visited and employ the tightest best-so-far distance.
However, as shown in Figure~\ref{fig:intro-motiv-node-nn}, a tight best-so-far distance provide marginal help in pruning more leaf nodes - most of the summarization-based lower bounds are still smaller than that. 
Early stopping approaches can terminate search right after the nearest neighbor results are retrieved, but it cannot reduce the search time before the retrieval. 

On the other hand, LeaFi helps series indexes to search only those leaf nodes that can update the best-so-far distances, as the tightest lower bounds are employed in pruning decision, hence attaining a significant and consistent improvement potential.
Moreover, as trained using the node-wise distance information instead of index-wise leaf node searching information, LeaFi can be efficiently trained using 2k examples (0.002\% of the collection), compared to 100k to 1m examples (0.1\% of the collection) in the literature~\cite{DBLP:conf/sigmod/LiZAH20, SC:kang2021case}.
As far as we are aware, LeaFi represents the first framework that incorporates learned filters in data series indexes and provides substantial improvement in pruning ratio and search time.

FAISS Learned Termination (FLT) is an early-termination technique proposed for vector similarity search using kNN graphs~\cite{DBLP:conf/sigmod/LiZAH20}.
Its stopping criterion is predicted by a nontrivial expert-crafted feature set, which cannot be directly applied to tree-based series indexes, and, in contrast to LeaFi, it does not offer a mechanism to set search quality targets.



\noindent \textbf{\textit{Conformal regressions.}}
\label{sec-lit-conformal}
Conformal regression is a statistical approach that enhances existing regression methods by providing predictive intervals with a guarantee on their coverage probability~\cite{GS:vovk2005algorithmic}. 
It involves fitting a regression model to a dataset and then generating predictions that include an interval which, with a specified level of confidence (e.g., 95\%), is expected to cover the true target values. 
This process relies on the computation of nonconformity scores that measure how unusual new observations are compared to training data~\cite{DBLP:conf/ecml/PapadopoulosPVG02, DBLP:journals/ftml/AngelopoulosB23}.

However, the direct application of existing conformal regression techniques cannot help LeaFi support the user-requested search quality.
This is because the conformal prediction intervals are derived independently for each model, whereas achieving an expected target of a LeaFi-enhanced search result, e.g., 99\% recall, requires tuning all learned filters collaboratively at the same time.
Hence, in LeaFi, we further design our auto-tuning approach based on the conformal regression framework.
To the best of our knowledge, LeaFi demonstrates the first effort to introduce conformal regression techniques into the domain of learned indexes.