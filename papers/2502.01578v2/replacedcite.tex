\section{Related Work}
There has been a great surge of research to design efficient variants of softmax attention or propose other alternatives for sequence modeling directly. The efficient variants broadly include two categories: \textit{sparsified attention} and \textit{linear attention}. {Sparsified Attention}____ computes attention maps with pre-defined or learnable masks. For instance, Slide Window Attention (SWA)____ limits each query input only attend to a certain number of preceding tokens. Another efficient variant is linear Attention____, which exchanges the computation order by decomposing the softmax function with \textit{randomized} or \textit{learnable} feature functions. 

Alternatively, ____ propose modeling sequential data with state-space models (SSMs) and show surprisingly good performance on a benchmark for comparing Transformers over long sequence data____. The H3 model____ expanded SSMs with gated connections and a conventional local convolution layer. They also show SSM can work in tandem with attention mechanism in a hybrid manner.  ____ propose to substitute the SSM layer with a global convolution parameterized by MLP. ____ incorporates data-dependent gating to SSMs and show comparable performance as transformer-based language models. ____ develops RWKV architecture which absorbs insights from RNN and Attention-free transformer____. The RetNet model ____ and TransformerLLM____ apply a decay factor to the current hidden state before incorporating the current input information and achieving impressive improvements. ____ and ____ develop chunkwise forms of GLA and DeltaNet respectively to parallelize the computation of gated linear recurrence models and provide a triton-based library to accelerate the training speed of linear attention model____.
%Although these new architectures show comparable performance of transformer, they typically require significant modify the model structure and often need training from scratch.

Another interesting line of work dedicated to substituting the softmax attention in a pre-trained model with linear attention and performing continual training to bridge their performance gap. ____ take a pre-trained SA transformer, swap the SA modules with linear Attention, and continue training the entire model on the same task. ____ adopts the same procedure by optimizing it with the fast decay rules and removing the $\text{ReLU}$ function in the feature maps, namely, a simple identity map. ____ linearized existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest continual pre-training budget to recover their performance. ____ improve hybrid models by applying knowledge distillation____ from pre-trained transformers to mamba, enhancing efficiency and inference speed.