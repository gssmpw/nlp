\section{Related Work}
There has been a great surge of research to design efficient variants of softmax attention or propose other alternatives for sequence modeling directly. The efficient variants broadly include two categories: \textit{sparsified attention} and \textit{linear attention}. {Sparsified Attention}~\cite{longformer, BigBird, SinkhornAttn, reformer} computes attention maps with pre-defined or learnable masks. For instance, Slide Window Attention (SWA)~\cite{longformer} limits each query input only attend to a certain number of preceding tokens. Another efficient variant is linear Attention~\cite{LA_RN, performer, RFA, LLA, Transnormer, log_normal_attn}, which exchanges the computation order by decomposing the softmax function with \textit{randomized} or \textit{learnable} feature functions. 

Alternatively, ~\citet{s4d} propose modeling sequential data with state-space models (SSMs) and show surprisingly good performance on a benchmark for comparing Transformers over long sequence data~\citep{LRA, ssm_pooler}. The H3 model~\cite{H3} expanded SSMs with gated connections and a conventional local convolution layer. They also show SSM can work in tandem with attention mechanism in a hybrid manner.  ~\citet{Hyena} propose to substitute the SSM layer with a global convolution parameterized by MLP. ~\citet{mamba} incorporates data-dependent gating to SSMs and show comparable performance as transformer-based language models. ~\citet{RWKV} develops RWKV architecture which absorbs insights from RNN and Attention-free transformer~\cite{atten-free}. The RetNet model ~\cite{retnet} and TransformerLLM~\cite{qin2023transnormerllm} apply a decay factor to the current hidden state before incorporating the current input information and achieving impressive improvements. \citet{GLA_hardware} and \citet{deltanet_yang} develop chunkwise forms of GLA and DeltaNet respectively to parallelize the computation of gated linear recurrence models and provide a triton-based library to accelerate the training speed of linear attention model~\citep{triton}.
%Although these new architectures show comparable performance of transformer, they typically require significant modify the model structure and often need training from scratch.

Another interesting line of work dedicated to substituting the softmax attention in a pre-trained model with linear attention and performing continual training to bridge their performance gap. \citet{post_linearize_attn} take a pre-trained SA transformer, swap the SA modules with linear Attention, and continue training the entire model on the same task. ~\citet{Mao_fast_decay_weight} adopts the same procedure by optimizing it with the fast decay rules and removing the $\text{ReLU}$ function in the feature maps, namely, a simple identity map. ~\citet{DiJiang, uptraining_llm} linearized existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest continual pre-training budget to recover their performance. ~\citet{mambainllama} improve hybrid models by applying knowledge distillation~\citep{KD, RW-KD} from pre-trained transformers to mamba, enhancing efficiency and inference speed.