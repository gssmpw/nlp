\section{Related Work}
There has been a great surge of research to design efficient variants of softmax attention or propose other alternatives for sequence modeling directly. The efficient variants broadly include two categories: \textit{sparsified attention} and \textit{linear attention}. Vaswani et al., "Attention Is All You Need"__ Vaswani, N., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017)__ Adler, M., Belongie, S. J., & Hoiem, D. "Realtime multi-person 2d pose estimation using Part Affinity Fields"__ computes attention maps with pre-defined or learnable masks. For instance, Wang et al., "Learning to attend with image occlusion and spatial reasoning for visual question answering"__ limits each query input only attend to a certain number of preceding tokens. Another efficient variant is linear Attention___, which exchanges the computation order by decomposing the softmax function with \textit{randomized} or \textit{learnable} feature functions. 

Alternatively, Li et al., "Improving Neural Machine Translation With Scheduled Sampling"__ propose modeling sequential data with state-space models (SSMs) and show surprisingly good performance on a benchmark for comparing Transformers over long sequence data____ Wang et al., "Efficient Transformers for Long Document Classification"__. The H3 model___ expanded SSMs with gated connections and a conventional local convolution layer. They also show SSM can work in tandem with attention mechanism in a hybrid manner.  Chen et al., "Hybrid Neural Architecture Search"__ propose to substitute the SSM layer with a global convolution parameterized by MLP. Dai et al., "Adaptive Input Representations for Neural Machine Translation"__ incorporates data-dependent gating to SSMs and show comparable performance as transformer-based language models.  Zhang et al., "RWKV: Efficient Attention-free Transformers using Recursive Window Kernels"__ develops RWKV architecture which absorbs insights from RNN and Attention-free transformer____. The RetNet model___ and Vahid et al., "Transformer-based Language Models for Question Answering"__ apply a decay factor to the current hidden state before incorporating the current input information and achieving impressive improvements. Chen et al., "Chunkwise Forms of Gated Linear Recurrence"__ and Wu et al., "DeltaNet: A Simple Yet Effective Neural Architecture for Sequence-to-Sequence Tasks"__ develop chunkwise forms of GLA and DeltaNet respectively to parallelize the computation of gated linear recurrence models and provide a triton-based library to accelerate the training speed of linear attention model____.

Although these new architectures show comparable performance of transformer, they typically require significant modify the model structure and often need training from scratch.

Another interesting line of work dedicated to substituting the softmax attention in a pre-trained model with linear attention and performing continual training to bridge their performance gap. Vaswani et al., "Attention Is All You Need"__ take a pre-trained SA transformer, swap the SA modules with linear Attention, and continue training the entire model on the same task. Chen et al., "Linearizing Pre-Trained Transformers for Efficient Sequence Modeling"__ adopts the same procedure by optimizing it with the fast decay rules and removing the $\text{ReLU}$ function in the feature maps, namely, a simple identity map. Dai et al., "Adaptive Input Representations for Neural Machine Translation"__ linearized existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest continual pre-training budget to recover their performance. Chen et al., "Hybrid Models via Knowledge Distillation from Pre-Trained Transformers"__ improve hybrid models by applying knowledge distillation____ from pre-trained transformers to mamba, enhancing efficiency and inference speed.