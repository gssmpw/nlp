@article{BoxPierce1970,
  author    = {George E. P. Box and David A. Pierce},
  title     = {Distribution of residual autocorrelations in autoregressive-integrated moving average time series models},
  journal   = {Journal of the American Statistical Association},
  volume    = {65},
  number    = {332},
  year      = {1970},
  pages     = {1509--1526},
}

@inproceedings{Oreshkin2020,
    title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
    author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=r1ecqn4YwB}
}

@article{ansari2024chronos,
  title={Chronos: Learning the Language of Time Series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Sundar and Arango, Sebastian Pineda and Kapoor, Shubham and others},
  journal={arXiv preprint arXiv:2403.07815},
  year={2024}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer},
  doi={10.1023/A:1010933404324}
}

@article{cao2023tempo,
  title={Tempo: Prompt-based generative pre-trained transformer for time series forecasting},
  author={Cao, Defu and Jia, Furong and Arik, Sercan O and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
  journal={arXiv preprint arXiv:2310.04948},
  year={2023}
}

@inproceedings{challu2023nhits,
  title={Nhits: Neural hierarchical interpolation for time series forecasting},
  author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={6989--6997},
  year={2023}
}

@article{chang2023llm4ts,
  title={Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms},
  author={Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu},
  journal={arXiv preprint arXiv:2308.08469},
  year={2023}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{ekambaram2024ttms,
  title={TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series},
  author={Ekambaram, Vijay and Jati, Arindam and Nguyen, Nam H and Dayama, Pankaj and Reddy, Chandra and Gifford, Wesley M and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2401.03955},
  year={2024}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{garza2023timegpt,
  title={TimeGPT-1},
  author={Garza, Azul and Mergenthaler-Canseco, Max},
  journal={arXiv preprint arXiv:2310.03589},
  year={2023}
}

@article{gruver2024large,
  title={Large language models are zero-shot time series forecasters},
  author={Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hochreiter1997long,
  author    = {Sepp Hochreiter and JÃ¼rgen Schmidhuber},
  title     = {Long Short-Term Memory},
  journal   = {Neural Computation},
  volume    = {9},
  number    = {8},
  year      = {1997},
  pages     = {1735--1780},
  doi       = {10.1162/neco.1997.9.8.1735}
}

@book{hyndman2008forecasting,
  title={Forecasting with exponential smoothing: the state space approach},
  author={Hyndman, Rob and Koehler, Anne B and Ord, J Keith and Snyder, Ralph D},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{koopmans1995spectral,
  title={The spectral analysis of time series},
  author={Koopmans, Lambert H},
  year={1995},
  publisher={Elsevier},
  doi={https://doi.org/10.1016/B978-0-12-419251-5.X5000-5}
}

@inproceedings{liu2022pyraformer,
title={Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting},
author={Shizhan Liu and Hang Yu and Cong Liao and Jianguo Li and Weiyao Lin and Alex X. Liu and Schahram Dustdar},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0EXmFzUn5I}
}

@article{rasul2023lag,
  title={Lag-llama: Towards foundation models for time series forecasting},
  author={Rasul, Kashif and Ashok, Arjun and Williams, Andrew Robert and Khorasani, Arian and Adamopoulos, George and Bhagwatkar, Rishika and Bilo{\v{s}}, Marin and Ghonia, Hena and Hassen, Nadhir Vincent and Schneider, Anderson and others},
  journal={arXiv preprint arXiv:2310.08278},
  year={2023}
}

@article{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  journal={arXiv preprint arXiv:2308.08241},
  year={2023}
}

@article{toda1993,
  author={Hiro Y. Toda and Peter C. B. Phillips},
  title={Vector Autoregressions and Causality},
  journal={Econometrica},
  volume={61},
  number={6},
  year={1993},
  pages={1367--1393},
  doi={10.2307/2951647}
}

@article{woo2024unified,
  title={Unified training of universal time series forecasting transformers},
  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2402.02592},
  year={2024}
}

@article{wu2021autoformer,
  title={Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting},
  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={22419--22430},
  year={2021}
}

@misc{wu_timesnet_2023,
	title = {{TimesNet}: {Temporal} {2D}-{Variation} {Modeling} for {General} {Time} {Series} {Analysis}},
	shorttitle = {{TimesNet}},
	url = {http://arxiv.org/abs/2210.02186},
	doi = {10.48550/arXiv.2210.02186},
	language = {en},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Wu, Haixu and Hu, Tengge and Liu, Yong and Zhou, Hang and Wang, Jianmin and Long, Mingsheng},
	month = apr,
	year = {2023},
	note = {arXiv:2210.02186 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Wu et al. - 2023 - TimesNet Temporal 2D-Variation Modeling for Gener.pdf:C\:\\Users\\rafae\\Zotero\\storage\\M7MRGFAS\\Wu et al. - 2023 - TimesNet Temporal 2D-Variation Modeling for Gener.pdf:application/pdf},
}

@article{xue2023promptcast,
  title={Promptcast: A new prompt-based learning paradigm for time series forecasting},
  author={Xue, Hao and Salim, Flora D},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@misc{zeng_are_2022,
	title = {Are {Transformers} {Effective} for {Time} {Series} {Forecasting}?},
	url = {http://arxiv.org/abs/2205.13504},
	doi = {10.48550/arXiv.2205.13504},
	language = {en},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
	month = aug,
	year = {2022},
	note = {arXiv:2205.13504 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Zeng et al. - 2022 - Are Transformers Effective for Time Series Forecas.pdf:C\:\\Users\\rafae\\Zotero\\storage\\IBFG99L2\\Zeng et al. - 2022 - Are Transformers Effective for Time Series Forecas.pdf:application/pdf},
}

@inproceedings{zhang2023crossformer,
  title={Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting},
  author={Zhang, Yunhao and Yan, Junchi},
  booktitle={The eleventh international conference on learning representations},
  year={2023}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={11106--11115},
  year={2021}
}

@inproceedings{zhou2022fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}

