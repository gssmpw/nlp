\section{Related work}
\label{sec:related_work}

\subsection{Classical approaches}
Traditional statistical methods, such as AutoRegressive Integrated Moving Average \cite{BoxPierce1970}, Vector Autoregression \cite{toda1993}, Exponential Smoothing \cite{hyndman2008forecasting}, and Spectral Analysis \cite{koopmans1995spectral} were widely used in TS forecasting. 
Progressively, machine learning models such as XGBoost \cite{chen2016xgboost}, Random Forest \cite{breiman2001random}, Gradient Boosting Regression Trees \cite{friedman2001greedy}, and LightGBM \cite{ke2017lightgbm} have shown improvements in the forecast due to their ability to handle non-linear patterns.

\subsection{Deep learning models}

Deep learning models have advanced TS forecasting, starting with Recurrent Neural Networks (RNNs), specifically designed to model sequential data.
In particular, advanced variants such as RNNs with Long Short-Term Memory units, widely adopted within the TS community, have seen significantly increased usage \cite{hochreiter1997long}.
Additionally, MLP-based models, such as DLinear \cite{zeng_are_2022}, N-BEATS \cite{Oreshkin2020}, and N-Hits \cite{challu2023nhits} use MLP to learn the coefficients that produce both backcast and forecast outputs from their structure.

Originally from NLP, the Transformer architecture is increasingly adapted for time series forecasting, often with modified attention layers to capture temporal dependencies, as seen in \Cref{sec:all_champs} and other prior works, which we describe in the following.
Informer \cite{zhou2021informer} and Pyaformer \cite{liu2022pyraformer} are transformer-based models that modify the attention mechanism. 
Informer designs a ProbSparse self-attention mechanism to replace the standard self-attention. 
Pyaformer, on the other hand, presents a pyramidal attention module, where the inter-scale tree structure captures features at different resolutions, and the intra-scale neighboring connections model the temporal dependencies across different ranges.
Wu et al. \cite{wu2021autoformer} introduced the Autoformer with an Auto-Correlation mechanism to capture the series-wise temporal dependencies based on the learned periods.
Following, FEDformer \cite{zhou2022fedformer} utilizes a mixture-of-expert framework to improve seasonal-trend decomposition and integrates Fourier and Wavelet-enhanced blocks to capture key structures in the TS.
\cite{zhang2023crossformer} presented Crossformer, a transformer-based model utilizing cross-dimension dependency for multivariate TS forecasting.
Another recent approach is TimesNet \cite{wu_timesnet_2023}, which is a univariate 2D CNN that segments 1D time series according to Fourier decomposition. 
The segments are then stacked to build a 2D series. 
This enables the convolutions to simultaneously look at the local structure of the signal at $t_i$ and $t_{i-T}$ simultaneously, where $T$ denotes a dominant signal period.  


\subsection{Large Language Models}

The success of Large Language Models (LLMs) like BERT and GPT in natural language processing has inspired researchers to apply these models to TS tasks.
One significant approach involves transforming numerical TS data into natural language prompts to leverage pre-trained language models without modifications. 
PromptCast \cite{xue2023promptcast} and \cite{gruver2024large} present this method, demonstrating effective generalization in zero-shot settings and often outperforming traditional numerical models. 
Moving to few-shot training strategies, TEST \cite{sun2023test} adapts TS data for pre-trained LLMs by tokenizing the data and aligning the embedding space, particularly in few-shot and generalization scenarios.
Several frameworks focus on enhancing TS forecasting through specialized fine-tuning strategies such as LLM4TS \cite{chang2023llm4ts} and TEMPO \cite{cao2023tempo}.


\subsection{Foundation Models}

There is a growing interest in foundation models designed explicitly for TS tasks. 
Tiny Time Mixers \cite{ekambaram2024ttms} introduce a compact model for multivariate TS forecasting.
Timer-XL is a foundation model for unified time series forecasting, supporting univariate and multivariate data by extending next-token prediction for causal generation \cite{liu2024timer}.
The model introduced a universal TimeAttention mechanism to capture fine-grained intra- and inter-series dependencies.
MOIRAI \cite{woo2024unified} addresses challenges like cross-frequency learning and varied distributional properties in large-scale data, achieving competitive zero-shot forecasting performance.
TimeGPT-1 \cite{garza2023timegpt} and Lag-LLama \cite{rasul2023lag}, utilizing decoder-only transformer architectures and achieving strong zero-shot generalization.
Chronos \cite{ansari2024chronos} trains transformer-based models on discrete tokens processed from TS data, demonstrating superior performance on diverse datasets.
% Foundation models based on LLMs on TS forecasting such as Chronos \cite{ansari2024chronos} trains transformer-based language models on tokenized TS data, demonstrating superior performance on diverse datasets.