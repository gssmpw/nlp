\section{Related work}
\label{sec:related_work}

\subsection{Classical approaches}
Traditional statistical methods, such as AutoRegressive Integrated Moving Average ____, Vector Autoregression ____, Exponential Smoothing ____, and Spectral Analysis ____ were widely used in TS forecasting. 
Progressively, machine learning models such as XGBoost ____, Random Forest ____, Gradient Boosting Regression Trees ____, and LightGBM ____ have shown improvements in the forecast due to their ability to handle non-linear patterns.

\subsection{Deep learning models}

Deep learning models have advanced TS forecasting, starting with Recurrent Neural Networks (RNNs), specifically designed to model sequential data.
In particular, advanced variants such as RNNs with Long Short-Term Memory units, widely adopted within the TS community, have seen significantly increased usage ____.
Additionally, MLP-based models, such as DLinear ____, N-BEATS ____, and N-Hits ____ use MLP to learn the coefficients that produce both backcast and forecast outputs from their structure.

Originally from NLP, the Transformer architecture is increasingly adapted for time series forecasting, often with modified attention layers to capture temporal dependencies, as seen in \Cref{sec:all_champs} and other prior works, which we describe in the following.
Informer ____ and Pyaformer ____ are transformer-based models that modify the attention mechanism. 
Informer designs a ProbSparse self-attention mechanism to replace the standard self-attention. 
Pyaformer, on the other hand, presents a pyramidal attention module, where the inter-scale tree structure captures features at different resolutions, and the intra-scale neighboring connections model the temporal dependencies across different ranges.
Wu et al. ____ introduced the Autoformer with an Auto-Correlation mechanism to capture the series-wise temporal dependencies based on the learned periods.
Following, FEDformer ____ utilizes a mixture-of-expert framework to improve seasonal-trend decomposition and integrates Fourier and Wavelet-enhanced blocks to capture key structures in the TS.
____ presented Crossformer, a transformer-based model utilizing cross-dimension dependency for multivariate TS forecasting.
Another recent approach is TimesNet ____, which is a univariate 2D CNN that segments 1D time series according to Fourier decomposition. 
The segments are then stacked to build a 2D series. 
This enables the convolutions to simultaneously look at the local structure of the signal at $t_i$ and $t_{i-T}$ simultaneously, where $T$ denotes a dominant signal period.  


\subsection{Large Language Models}

The success of Large Language Models (LLMs) like BERT and GPT in natural language processing has inspired researchers to apply these models to TS tasks.
One significant approach involves transforming numerical TS data into natural language prompts to leverage pre-trained language models without modifications. 
PromptCast ____ and ____ present this method, demonstrating effective generalization in zero-shot settings and often outperforming traditional numerical models. 
Moving to few-shot training strategies, TEST ____ adapts TS data for pre-trained LLMs by tokenizing the data and aligning the embedding space, particularly in few-shot and generalization scenarios.
Several frameworks focus on enhancing TS forecasting through specialized fine-tuning strategies such as LLM4TS ____ and TEMPO ____.


\subsection{Foundation Models}

There is a growing interest in foundation models designed explicitly for TS tasks. 
Tiny Time Mixers ____ introduce a compact model for multivariate TS forecasting.
Timer-XL is a foundation model for unified time series forecasting, supporting univariate and multivariate data by extending next-token prediction for causal generation ____.
The model introduced a universal TimeAttention mechanism to capture fine-grained intra- and inter-series dependencies.
MOIRAI ____ addresses challenges like cross-frequency learning and varied distributional properties in large-scale data, achieving competitive zero-shot forecasting performance.
TimeGPT-1 ____ and Lag-LLama ____, utilizing decoder-only transformer architectures and achieving strong zero-shot generalization.
Chronos ____ trains transformer-based models on discrete tokens processed from TS data, demonstrating superior performance on diverse datasets.
% Foundation models based on LLMs on TS forecasting such as Chronos ____ trains transformer-based language models on tokenized TS data, demonstrating superior performance on diverse datasets.