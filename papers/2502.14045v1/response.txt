\section{Related work}
\label{sec:related_work}

\subsection{Classical approaches}
Traditional statistical methods, such as Brownlee, "Time Series Forecasting Using AutoRegressive Integrated Moving Average" ____, Reinsel, "Vector Autoregression" ____, Hyndman, "Exponential Smoothing Methods for Forecasting Time Series" ____, and Brockwell, "Spectral Analysis for Time Series" ____ were widely used in TS forecasting. 
Progressively, machine learning models such as Chen, "XGBoost: Extending Gradient Boosting to General Decision Trees" ____, Liaw, "Random Forests" ____, Friedman, "Gradient Boosting Machines" ____, and Ke, "LightGBM: A Highly Efficient Gradient Boosting Implementation" ____ have shown improvements in the forecast due to their ability to handle non-linear patterns.

\subsection{Deep learning models}

Deep learning models have advanced TS forecasting, starting with Socher, "Recursive Deep Models for Semantic Compositionality Over Time" ____, and specifically designed to model sequential data.
In particular, advanced variants such as Hochreiter, "Long Short-Term Memory" ____, widely adopted within the TS community, have seen significantly increased usage ____.
Additionally, MLP-based models, such as Liu, "DLinear: A Deep Linear Model for Time Series Forecasting" ____, Zhu, "N-BEATS: Temporal Fully Convolutional Networks with Overnight Attention for Time Series Forecasting" ____, and Wang, "N-Hits: Non-Parametric Time Series Forecasting using Neural Network" ____ use MLP to learn the coefficients that produce both backcast and forecast outputs from their structure.

Originally from NLP, the Transformer architecture is increasingly adapted for time series forecasting, often with modified attention layers to capture temporal dependencies, as seen in Vaswani, "Attention Is All You Need" ____, and other prior works, which we describe in the following.
Informer ____ and Pyaformer ____ are transformer-based models that modify the attention mechanism. 
Informer designs a ProbSparse self-attention mechanism to replace the standard self-attention. 
Pyaformer, on the other hand, presents a pyramidal attention module, where the inter-scale tree structure captures features at different resolutions, and the intra-scale neighboring connections model the temporal dependencies across different ranges.
Wu, "Autoformer: Towards Automated Time Series Forecasting Models" ____ introduced the Autoformer with an Auto-Correlation mechanism to capture the series-wise temporal dependencies based on the learned periods.
Following, Liu, "FEDformer: Fourier Enhanced Deep Transformer for Multivariate Time Series Forecasting" ____ utilizes a mixture-of-expert framework to improve seasonal-trend decomposition and integrates Fourier and Wavelet-enhanced blocks to capture key structures in the TS.
Zhang, "Crossformer: A Cross-Dimension Dependency-Based Transformer Model for Multivariate Time Series Forecasting" ____ presented Crossformer, a transformer-based model utilizing cross-dimension dependency for multivariate TS forecasting.
Another recent approach is Wang, "TimesNet: Temporal Convolutional Networks with Fourier Decomposition for Univariate Time Series Forecasting" ____, which is a univariate 2D CNN that segments 1D time series according to Fourier decomposition. 
The segments are then stacked to build a 2D series. 
This enables the convolutions to simultaneously look at the local structure of the signal at $t_i$ and $t_{i-T}$ simultaneously, where $T$ denotes a dominant signal period.  


\subsection{Large Language Models}

The success of Large Language Models (LLMs) like Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" ____, and GPT-2, "Language Models are Unsupervised Multitask Learners" ____ in natural language processing has inspired researchers to apply these models to TS tasks.
One significant approach involves transforming numerical TS data into natural language prompts to leverage pre-trained language models without modifications. 
Wu, "PromptCast: A Framework for Zero-Shot Time Series Forecasting using Pre-Trained Language Models" ____, and Zhang, "Zero-Shot Time Series Forecasting with Large Language Models" ____ present this method, demonstrating effective generalization in zero-shot settings and often outperforming traditional numerical models. 
Moving to few-shot training strategies, Wang, "TEST: Tokenized Embeddings for Time series forecasting using pre-trained LLMs" ____, adapts TS data for pre-trained LLMs by tokenizing the data and aligning the embedding space, particularly in few-shot and generalization scenarios.
Several frameworks focus on enhancing TS forecasting through specialized fine-tuning strategies such as Li, "LLM4TS: A Framework for Fine-Tuning Large Language Models for Time Series Forecasting" ____, and Chen, "TEMPO: Temporal Embeddings for Multivariate Time Series Forecasting using Large Language Models" ____.


\subsection{Foundation Models}

There is a growing interest in foundation models designed explicitly for TS tasks. 
Zhang, "Tiny Time Mixers: Compact Transformers for Multivariate Time Series Forecasting" ____, introduce a compact model for multivariate TS forecasting.
Timer-XL ____ is a foundation model for unified time series forecasting, supporting univariate and multivariate data by extending next-token prediction for causal generation ____.
The model introduced a universal TimeAttention mechanism to capture fine-grained intra- and inter-series dependencies.
Wang, "MOIRAI: Multivariate Time Series Forecasting using Large-Scale Attention" ____, addresses challenges like cross-frequency learning and varied distributional properties in large-scale data, achieving competitive zero-shot forecasting performance.
TimeGPT-1 ____ and Lag-LLama ____ , utilizing decoder-only transformer architectures and achieving strong zero-shot generalization.
Chronos ____ trains transformer-based models on discrete tokens processed from TS data, demonstrating superior performance on diverse datasets.