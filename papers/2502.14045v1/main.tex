%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
% !TEX TS-program = pdflatex

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[table,xcdraw]{xcolor}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{colortbl}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% For tables
\usepackage{multirow}

\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}

\usepackage{multirow}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Position: There are no Champions in Long-Term Time Series Forecasting}

\begin{document}

\twocolumn[
\icmltitle{Position: There are no Champions in Long-Term Time Series Forecasting}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lorenzo Brigato}{equal,yyy}
\icmlauthor{Rafael Morand}{equal,yyy,gcb,comp}
\icmlauthor{Knut Strømmen}{equal,yyy,gcb}
\\
\icmlauthor{Maria Panagiotou}{yyy,gcb}
\icmlauthor{Markus Schmidt}{comp}
\icmlauthor{Stavroula Mougiakakou}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{ARTORG Center, University of Bern}
\icmlaffiliation{gcb}{Graduate School for Cellular and Biomedical Sciences, University of Bern}
\icmlaffiliation{comp}{Center for Experimental Neurology, Department of Neurology, Bern University Hospital}


\icmlcorrespondingauthor{Lorenzo Brigato}{lorenzo.brigato@unibe.ch}
\icmlcorrespondingauthor{Rafael Morand}{rafael.morand@unibe.ch}
\icmlcorrespondingauthor{Knut Strømmen}{knut.stroemmen@unibe.ch}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Recent advances in long-term time series forecasting have introduced numerous complex prediction models that consistently outperform previously published architectures.
However, this rapid progression raises concerns regarding inconsistent benchmarking and reporting practices, which may undermine the reliability of these comparisons.
\textbf{Our position emphasizes the need to shift focus away from pursuing ever-more complex models and towards enhancing benchmarking practices through rigorous and standardized evaluation methods.}
To support our claim, we first perform a broad, thorough, and reproducible evaluation of the top-performing models on the most popular benchmark by training 3,500+ networks over 14 datasets.
Then, through a comprehensive analysis, we find that slight changes to experimental setups or current evaluation metrics drastically shift the common belief that newly published results are advancing the state of the art.
Our findings suggest the need for rigorous and standardized evaluation methods that enable more substantiated claims, including reproducible hyperparameter setups and statistical testing. 

\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=0.85\columnwidth]{plots/radarplot.pdf}}
        \caption{\textbf{Performance comparison of recent models.}
        We summarize results in terms of relative MSE averaged over all forecast horizons for the leaderboard models of the TSLib benchmark.}
        \label{fig:radarplot}
    \end{center}
\end{figure}

Long-term time series forecasting (LTSF) is critical across various domains, including energy management \cite{WERON20141030}, financial planning \cite{SEZER2020106181}, and environmental modeling \cite{10825393}. 
Accurately predicting future values in time series data enables better decision-making and resource allocation. 
LTSF remains challenging due to the complex temporal dynamics, including trends, seasonality, irregular fluctuations, and significant variability across datasets \cite{shao2024exploring, qiu2024tfb}.

Recent advancements in deep learning have improved LTSF capabilities, and the field is currently witnessing an exponential surge in publication rates \cite{kim2024comprehensive}. 
Transformer models have been adapted to time series forecasting with innovative modifications, such as univariate patching \cite{nie_time_2023} and attention mechanisms tailored to exploit inter-variate dependencies \cite{liu_itransformer_2024,wang_timexer_2024}.
Additionally, models such as TimeMixer \cite{wang_timemixer_2024}, leveraging multiscale signal mixing, and TimesNet \cite{wu_timesnet_2023}, using Fourier-based 2D decomposition, have expanded the field.

However, we claim that the field is facing significant fair-benchmarking challenges despite the introduction of extensive benchmarks such as TSLib \cite{wang_deep_2024}, BasicTS+ \cite{shao2024exploring}, and TFB \cite{qiu2024tfb}. 
Inconsistencies in test setups across different benchmarks, biased comparisons, and challenges with reproducibility are investigated as factors that hinder fair performance assessment in the field.
Moreover, marginal performance gains in recent literature cast doubt on the practical value of increasingly complex model architectures \cite{zeng_are_2022}.
To support our hypothesis, we conduct a comprehensive, rigorous, and reproducible evaluation of the top-performing models on the most widely used benchmark, training over 3,500 networks across 14 datasets.
As shown in \Cref{fig:radarplot}, our findings reveal that no single model consistently outperforms all the baselines.
This result directly challenges the prevailing narrative of new architectures consistently surpassing competing models across all domains \cite{nie_time_2023,wu_timesnet_2023,liu_itransformer_2024,wang_timemixer_2024,wang_timexer_2024}.
We provide insights into the potential reasons behind this phenomenon and offer recommendations to help the field progress.
To facilitate ongoing research in LTSF, we will make our code publicly available upon acceptance.
In summary, the contributions of our paper are as follows:

\begin{itemize}
    \item We question the current narrative of consistently dominated LTSF benchmarks (\Cref{sec:all_champs}) supported by results obtained over a comprehensive and reproducible experimental setup (\Cref{sec:all_similar}).
    \item We provide possible reasons why this is happening (\Cref{sec:y_all_champ}) and propose guidelines to reduce the likelihood of repeating such overstated claims (\Cref{sec:real_claims}).
    \item We challenge previous claims that dataset characteristics may guide model selection, as our results show similar performance across networks, highlighting the need for further research in this direction (\Cref{sec:model_sel}).
\end{itemize}

\section{Field overview}
\label{sec:all_champs}

We provide an overview of recent advancements in LTSF, focusing on current benchmarks (\Cref{sec:recent_bench}) and emerging champions (\Cref{sec:rel_champ}).
Due to space limitations, additional related work on recent time series forecasting models and paradigms is included in \Cref{sec:related_work}.

\subsection{Benchmarks}
\label{sec:recent_bench}

\paragraph{TSLib} \cite{wang_deep_2024} compares 12 deep learning models across five tasks: classification, imputation, anomaly detection, and long-/short-term forecasting. 
For long-term forecasting, nine datasets from four domains are used.
Results are presented for two settings: unified hyperparameters (HPs) and an HP search per model, but details on parameters, context length, forecast horizon, or the search process are missing. 
The evaluation metric is the mean squared error (MSE) averaged across datasets.
The top models in the HP search setting are PatchTST \cite{zeng_are_2022} (MSE 0.305), N-Beats \cite{Oreshkin2020} (MSE 0.313), and iTransformer \cite{liu_itransformer_2024} (MSE 0.317), and the top performing models for the unified HP setting are iTransformer (MSE 0.342), N-Beats (MSE 0.371), and PatchTST (MSE 0.373).
The authors claim that their results clearly show the superior forecasting capabilities of transformer models, particularly iTransformer and PatchTST, despite arguably marginal improvements. 
They stress the importance of continued exploration of methods that use temporal tokens in time series analysis.

\paragraph{TFB} \cite{qiu2024tfb} evaluates 22 statistical, classical machine learning, and deep learning methods using 25 multivariate and 8,068 univariate datasets. 
For multivariate forecasting, shorter datasets use forecast horizons of 24–60 and look-back windows of 36–104, while longer datasets use forecast horizons of 96–720 and look-back windows of 96–512. 
Univariate forecasting applies fixed forecasting horizons of 6–48 with a look-back window set to 1.25 times the forecasting horizon. 
The experiments adhere to each method's HPs specified in the original works. HP searches are performed across up to eight sets, and the best result is selected from these evaluations. 
Based on these results, the authors claim that linear models outperform deep learning methods in datasets with increased trends and distribution shifts.
Conversely, transformers excel in datasets with marked patterns (e.g., seasonality).  
PatchTST and DLinear \cite{zeng_are_2022} consistently perform well without notable weaknesses.


\paragraph{BasicTS+} \cite{shao2024exploring} incorporates 28 forecasting models, including 17 short-term forecasting (STF) and 11 LTSF models, across 14 widely used datasets. 
STF models encompass prior-graph-based, latent-graph-based, and non-graph-based methods, while LTSF models consist of transformer-based and linear-layer-based architectures.
In STF, both the context length and forecast horizon are fixed at 12. 
For LTSF, the forecast horizon is set to 336, while context length varies across 96, 192, 336, and 720, with the best performance across these lengths being reported. 
Models are implemented following publicly available architectures and HPs, with further tuning of parameters like learning rate and batch size via grid search to ensure performance is at least as good as reported in the original paper.
Upon analyzing the results, the authors argue that dataset characteristics play a major role in determining model performance. 
They claim that Transformer models excel on datasets with clear, stable patterns, whereas simpler models like DLinear perform comparably on datasets without such patterns. 
The authors emphasize the need to address data distribution drift and unclear patterns instead of focusing solely on increasing model complexity.
They suggest that this may indicate potential overfitting to commonly used datasets like \textit{ETT*}, \textit{Electricity}, \textit{Weather}, and \textit{Exchange}, which risks creating a misleading impression of progress. 
Consequently, they conclude that careful dataset selection and curation are essential for advancing MTS forecasting meaningfully.

\subsection{Emergent LTSF champions}
\label{sec:rel_champ}

Recent models have made a leap in LTSF performance \citep{zeng_are_2022, nie_time_2023, wu_timesnet_2023, wang_timemixer_2024, liu_itransformer_2024, wang_timexer_2024}. 
One widely accepted benchmark for LTSF is TSLib \cite{wang_deep_2024}. 
Using this library, there has been a series of new models in 2024 that reportedly dominates the field. 
Sophisticated and elegant changes to established model architectures resulted in improved forecasting performance.
The current leaderboard in TSlib includes five models, which we introduce in chronological order in \Cref{tab:winners}.  

\cite{zeng_are_2022} introduced a collection of comparably simple multi-layer perceptron (MLP) baselines called LTFS-Linear. 
The authors challenged the utility of transformer-based models since, collectively, the linear baselines beat all previous transformers \cite{pmlr-v162-zhou22g,wu2021autoformer,zhou2021informer,liu2022pyraformer,li_enhancing_2019}. 
The DLinear model has since been used as a competitive baseline in LTSF benchmarks. 
PatchTST \cite{nie_time_2023} is a transformer model that introduced univariate patching of time series for tokenization.
PatchTST beat all previous transformer models but was occasionally beaten by DLinear. 
PatchTST has since been used as a competitive baseline in LTSF benchmarks. 
TimeMixer \cite{wang_timemixer_2024} is an MLP mixer model that introduces past-decomposable mixing and future-predictor mixing. 
The authors used two experimental settings. 
In setting A), all models were trained and tested using unified HPs, and in B) with HP search. 
TimeMixer dominated in setting A where unified HPs were used. 
Although still the clear winner in setting B, TimeMixer took the position as runner-up in some datasets and forecast horizons, being on most occasions second to PatchTST. 
The iTransformer \cite{liu_itransformer_2024} is a modification of the original transformer model with attention across variates instead of samples.
This allows for capturing interactions between variates. 
It was the top model in 5/9 datasets.
Since the remaining four datasets are similar (\textit{ETTh1}, \textit{ETTh2}, \textit{ETTm1}, \textit{ETTm2}), the authors also reported results for an average of these datasets. 
This improved the ranking of iTransformer, making it the top-performing model at the time. 
Finally, TimeXer \cite{wang_timexer_2024} is a transformer model that uses two attention stages, one for interactions between patches and one for interactions between variates.
TimeXer won in 6/7 datasets, of which four are similar (\textit{ETT*}).
We summarize the striking winning percentages of these models in \Cref{tab:winners}.

\newcommand{\cross}[1][1pt]{\ooalign{%
  \rule[1ex]{1ex}{#1}\cr% Horizontal bar
  \hss\rule{#1}{.7em}\hss\cr}}% Vertical bar

\begin{table}[]
\caption[caption]{\textbf{Win percentages.} Reported winners in LTSF for prediction lengths $T \in \{96, 192, 336, 720\}$. The percentage of reported wins is according to each prediction length without averaging. TimeMixer reported unified parameters (A) and HP search (B) results. (\cross[.4pt]avg. of all prediction horizons and avg. of ETT* datasets)}
\centering
    % \small
    % \begin{tabular}{l|cc}
    %     \toprule 
    %     {\multirow{2}{*}{Model}}  & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\% best\end{tabular}}                            \\
    %     \multicolumn{1}{c}{}                        & \multicolumn{1}{|c}{MSE}                                            & \multicolumn{1}{c}{MAE}         \\  \midrule \noalign{\vskip 0.5mm}
    %     DLinear \citeyearpar{zeng_are_2022}                       & 50.0\%                                                               & 16.7\%                                                                                                                        \\ \noalign{\vskip 1mm}
    %     PatchTST \citeyearpar{nie_time_2023}                       & 87.5\%                                                            & 59.4\%                                                                                                                      \\ \noalign{\vskip 1mm}
    %     % TimesNet \citeyearpar{wu_timesnet_2023}                       & \begin{tabular}[t]{@{}r@{}}44.4\%\\      \cross[.4pt](55.5\%)\end{tabular}    & \begin{tabular}[t]{@{}r@{}}66.6\%\\      \cross[.4pt](66.6\%)\end{tabular}                                                             \\ \noalign{\vskip 1mm}
    %     TimeMixer \citeyearpar{wang_timemixer_2024}                      & \begin{tabular}[t]{@{}r@{}}A) 93.8\%\\      B) 81.2\%\end{tabular} & \begin{tabular}[t]{@{}r@{}}A) 100.\% \\      B) 81.2\%\end{tabular}  \\ \noalign{\vskip 1mm}
    %     iTransformer \citeyearpar{liu_itransformer_2024}                       & \begin{tabular}[t]{@{}r@{}}33.3\%\\      \cross[.4pt](71.4\%)\end{tabular}   & \begin{tabular}[t]{@{}r@{}} 47.2\%\\      \cross[.4pt](85.7\%)\end{tabular}                                                          \\ \noalign{\vskip 1mm}
    %     TimeXer \citeyearpar{wang_timexer_2024}                    & 85.7\%                                                            & 60.7\%                                                            \\ \bottomrule
    % \end{tabular}

\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc}
\toprule 
\multirow{2}{*}{\textbf{Win \%}} & DLinear & PatchTST & TimeMixer & iTransformer & TimeXer \\ 
& \citeyearpar{zeng_are_2022} & \citeyearpar{nie_time_2023} & \citeyearpar{wang_timemixer_2024} & \citeyearpar{liu_itransformer_2024} & \citeyearpar{wang_timexer_2024} \\ 
\midrule 
\multirow{2}{*}{MSE} & 50.0 & 87.5 & \begin{tabular}[t]{@{}r@{}}A) 93.8\\ B) 81.2\end{tabular} & \begin{tabular}[t]{@{}r@{}}33.3\\ \cross[.4pt](71.4)\end{tabular} & 85.7 \\ 
\midrule
\multirow{2}{*}{MAE} & 16.7 & 59.4 & \begin{tabular}[t]{@{}l@{}} A) 100\\ B) 81.2\end{tabular} & \begin{tabular}[t]{@{}r@{}}47.2\\ \cross[.4pt](85.7)\end{tabular} & 60.7 \\ 
\bottomrule
\end{tabular}
}

\label{tab:winners}

\end{table}


\section{Who is the real champion?}
\label{sec:all_similar}

As seen in the previous section, recent papers often suggest that newly proposed architectures outperform others across almost all tested datasets. 
However, the variability in results for the same algorithms and reliance on prior studies with different HP optimizations raise questions about their consistent superiority. 
To investigate this, we selected the previously introduced five top-performing models from TSLib \cite{wang_deep_2024} and performed a comprehensive HP search across 14 datasets from various domains.

\begin{table*}[]
\caption{\textbf{Main results.}
Mean and best (i.e., Min) values averaged over prediction lengths. \textbf{\textcolor{red}{Best}} and {\textcolor{blue}{\underline{second-best}}} are highlighted.
}
\centering
\resizebox{\textwidth}{!}{

\begin{tabular}{@{}c|cccc|cccc|cccc|cccc|cccc@{}}
\toprule
Model                & \multicolumn{4}{c|}{DLinear}                                                                                                                                      & \multicolumn{4}{c|}{PatchTST}                                                                                                                                     & \multicolumn{4}{c|}{iTransformer}                                                                                                                                 & \multicolumn{4}{c|}{TimeMixer}                                                                                                                                    & \multicolumn{4}{c}{TimeXer}                                                                                                                                       \\ \midrule
Metric               & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c}{MSE}                                                         \\ \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7}\cmidrule(lr){8-9} \cmidrule(lr){10-11}\cmidrule(lr){12-13} \cmidrule(lr){14-15}\cmidrule(lr){16-17}\cmidrule(lr){18-19}\cmidrule(lr){20-21}
Statistic            & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                \\ \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20} \cmidrule(lr){21-21}

\multicolumn{1}{c|}{ETTh1}                & 0.4772                                 & 0.4761                                 & 0.4741                                 & 0.4725                                 & {\color[HTML]{FF0000} \textbf{0.4319}} & {\color[HTML]{FF0000} \textbf{0.4279}} & {\color[HTML]{FF0000} \textbf{0.4143}} & {\color[HTML]{FF0000} \textbf{0.4076}} & 0.4428                                 & 0.4408                                 & {\color[HTML]{0000FF} {\ul 0.4244}}    & 0.4217                                 & 0.4429                                 & 0.4386                                 & 0.429                                  & 0.4245                                 & {\color[HTML]{0000FF} {\ul 0.44}}      & {\color[HTML]{0000FF} {\ul 0.4361}}    & 0.4252                                 & {\color[HTML]{0000FF} {\ul 0.4204}}    \\
\multicolumn{1}{c|}{ETTm1}                & {\color[HTML]{FF0000} \textbf{0.4218}} & {\color[HTML]{FF0000} \textbf{0.4208}} & {\color[HTML]{0000FF} {\ul 0.4031}}    & {\color[HTML]{0000FF} {\ul 0.4023}}    & {\color[HTML]{0000FF} {\ul 0.4314}}    & {\color[HTML]{0000FF} {\ul 0.4292}}    & {\color[HTML]{FF0000} \textbf{0.3936}} & {\color[HTML]{FF0000} \textbf{0.3915}} & 0.4373                                 & 0.4333                                 & 0.408                                  & 0.4029                                 & 0.4487                                 & 0.4385                                 & 0.4315                                 & 0.4157                                 & 0.4446                                 & 0.4398                                 & 0.4121                                 & 0.4054                                 \\
\multicolumn{1}{c|}{ETTh2}                & 0.4743                                 & 0.4736                                 & 0.4849                                 & 0.4841                                 & 0.4099                                 & 0.4062                                 & 0.3793                                 & {\color[HTML]{0000FF} {\ul 0.3736}}    & {\color[HTML]{0000FF} {\ul 0.4054}}    & {\color[HTML]{0000FF} {\ul 0.4028}}    & 0.3784                                 & 0.3742                                 & {\color[HTML]{FF0000} \textbf{0.4044}} & {\color[HTML]{FF0000} \textbf{0.4011}} & {\color[HTML]{FF0000} \textbf{0.3744}} & {\color[HTML]{FF0000} \textbf{0.3684}} & 0.4063                                 & 0.4038                                 & {\color[HTML]{0000FF} {\ul 0.377}}     & 0.3746                                 \\
\multicolumn{1}{c|}{ETTm2}                & {\color[HTML]{FF0000} \textbf{0.2611}} & {\color[HTML]{FF0000} \textbf{0.261}}  & {\color[HTML]{FF0000} \textbf{0.1511}} & {\color[HTML]{FF0000} \textbf{0.1511}} & {\color[HTML]{0000FF} {\ul 0.2691}}    & {\color[HTML]{0000FF} {\ul 0.2676}}    & {\color[HTML]{0000FF} {\ul 0.1563}}    & {\color[HTML]{0000FF} {\ul 0.1552}}    & 0.2759                                 & 0.2749                                 & 0.1661                                 & 0.165                                  & 0.2704                                 & 0.2686                                 & 0.1618                                 & 0.1598                                 & 0.2767                                 & 0.2744                                 & 0.1682                                 & 0.1654                                 \\
\multicolumn{1}{c|}{Electricity}          & {\color[HTML]{0000FF} {\ul 0.2585}}    & 0.2583                                 & {\color[HTML]{0000FF} {\ul 0.1616}}    & {\color[HTML]{0000FF} {\ul 0.1615}}    & 0.2613                                 & 0.2608                                 & 0.164                                  & 0.1634                                 & 0.2622                                 & {\color[HTML]{0000FF} {\ul 0.2577}}    & 0.1655                                 & 0.162                                  & {\color[HTML]{FF0000} \textbf{0.2704}} & {\color[HTML]{FF0000} \textbf{0.2686}} & {\color[HTML]{FF0000} \textbf{0.1618}} & {\color[HTML]{FF0000} \textbf{0.1598}} & 0.2676                                 & 0.2635                                 & 0.1695                                 & 0.1645                                 \\
\multicolumn{1}{c|}{Weather}              & 0.2984                                 & 0.2984                                 & 0.2442                                 & 0.2441                                 & {\color[HTML]{FF0000} \textbf{0.2633}} & {\color[HTML]{0000FF} {\ul 0.2623}}    & {\color[HTML]{0000FF} {\ul 0.2245}}    & {\color[HTML]{0000FF} {\ul 0.2235}}    & 0.2791                                 & 0.2779                                 & 0.2385                                 & 0.2367                                 & 0.2707                                 & 0.2639                                 & 0.2313                                 & 0.2248                                 & {\color[HTML]{0000FF} {\ul 0.2639}}    & {\color[HTML]{FF0000} \textbf{0.2616}} & {\color[HTML]{FF0000} \textbf{0.2237}} & {\color[HTML]{FF0000} \textbf{0.2217}} \\
\multicolumn{1}{c|}{Exchange}             & 0.5905                                 & 0.5833                                 & 0.7754                                 & 0.7553                                 & 0.5746                                 & 0.5671                                 & 0.7379                                 & 0.7169                                 & {\color[HTML]{FF0000} \textbf{0.5572}} & {\color[HTML]{FF0000} \textbf{0.5394}} & {\color[HTML]{0000FF} {\ul 0.6995}}    & {\color[HTML]{FF0000} \textbf{0.6621}} & 31.4575                                & 0.7354                                 & 33705.21                               & 1.3975                                 & {\color[HTML]{0000FF} {\ul 0.5616}}    & {\color[HTML]{0000FF} {\ul 0.5483}}    & {\color[HTML]{FF0000} \textbf{0.6973}} & {\color[HTML]{0000FF} {\ul 0.6667}}    \\
\multicolumn{1}{c|}{MotorImagery}         & 1.1834                                 & 1.1821                                 & 4.5924                                 & 4.5831                                 & 1.006                                  & 1.0004                                 & 3.7749                                 & 3.7446                                 & {\color[HTML]{FF0000} \textbf{0.3835}} & {\color[HTML]{FF0000} \textbf{0.3378}} & {\color[HTML]{FF0000} \textbf{1.6916}} & {\color[HTML]{FF0000} \textbf{1.4959}} & 1.0113                                 & 0.9958                                 & 3.8691                                 & 3.8126                                 & {\color[HTML]{0000FF} {\ul 0.9155}}    & {\color[HTML]{0000FF} {\ul 0.8933}}    & {\color[HTML]{0000FF} {\ul 3.649}}     & {\color[HTML]{0000FF} {\ul 3.5762}}    \\
\multicolumn{1}{c|}{TDBrain}              & 0.8016                                 & 0.8014                                 & 1.1513                                 & 1.1505                                 & 0.7248                                 & 0.7223                                 & 0.9821                                 & 0.9757                                 & {\color[HTML]{0000FF} {\ul 0.7224}}    & 0.7206                                 & {\color[HTML]{0000FF} {\ul 0.9783}}    & {\color[HTML]{0000FF} {\ul 0.9742}}    & 0.7228                                 & {\color[HTML]{0000FF} {\ul 0.7199}}    & 0.9812                                 & {\color[HTML]{0000FF} {\ul 0.9742}}    & {\color[HTML]{FF0000} \textbf{0.7187}} & {\color[HTML]{FF0000} \textbf{0.7168}} & {\color[HTML]{FF0000} \textbf{0.97}}   & {\color[HTML]{FF0000} \textbf{0.9651}} \\
\multicolumn{1}{c|}{BeijingAir}           & 0.4716                                 & 0.4708                                 & 0.5829                                 & 0.5823                                 & 0.5472                                 & {\color[HTML]{FF0000} \textbf{0.4584}} & 0.8643                                 & {\color[HTML]{0000FF} {\ul 0.5758}}    & {\color[HTML]{0000FF} {\ul 0.4641}}    & 0.4612                                 & {\color[HTML]{FF0000} \textbf{0.5801}} & {\color[HTML]{FF0000} \textbf{0.5719}} & 0.4712                                 & 0.467                                  & 0.5917                                 & 0.5804                                 & {\color[HTML]{FF0000} \textbf{0.4618}} & {\color[HTML]{0000FF} {\ul 0.4591}}    & {\color[HTML]{0000FF} {\ul 0.5818}}    & 0.5765                                 \\
\multicolumn{1}{c|}{BenzeneConcentration} & {\color[HTML]{FF0000} \textbf{0.0196}} & {\color[HTML]{FF0000} \textbf{0.0178}} & {\color[HTML]{FF0000} \textbf{0.0082}} & {\color[HTML]{0000FF} {\ul 0.0081}}    & 0.0419                                 & 0.0383                                 & {\color[HTML]{0000FF} {\ul 0.0111}}    & 0.0107                                 & 0.0529                                 & 0.0479                                 & 0.0117                                 & 0.0108                                 & {\color[HTML]{0000FF} {\ul 0.0225}}    & {\color[HTML]{0000FF} {\ul 0.0188}}    & {\color[HTML]{FF0000} \textbf{0.0082}} & {\color[HTML]{FF0000} \textbf{0.008}}  & 0.1213                                 & 0.0383                                 & 0.0915                                 & 0.0119                                 \\
\multicolumn{1}{c|}{AustraliaRainfall}    & {\color[HTML]{FF0000} \textbf{0.7513}} & {\color[HTML]{FF0000} \textbf{0.7507}} & {\color[HTML]{FF0000} \textbf{0.8382}} & {\color[HTML]{FF0000} \textbf{0.8376}} & 0.7566                                 & 0.7556                                 & 0.8548                                 & 0.8524                                 & 0.7537                                 & 0.7531                                 & 0.8493                                 & 0.8482                                 & 0.7548                                 & 0.7539                                 & 0.853                                  & 0.8503                                 & {\color[HTML]{0000FF} {\ul 0.7528}}    & {\color[HTML]{0000FF} {\ul 0.7522}}    & {\color[HTML]{0000FF} {\ul 0.8468}}    & {\color[HTML]{0000FF} {\ul 0.8457}}    \\
\multicolumn{1}{c|}{KDDCup2018}           & {\color[HTML]{FF0000} \textbf{0.6302}} & {\color[HTML]{0000FF} {\ul 0.6274}}    & {\color[HTML]{FF0000} \textbf{0.9967}} & {\color[HTML]{FF0000} \textbf{0.9887}} & 0.6474                                 & 0.6442                                 & 1.0856                                 & 1.0757                                 & 0.6485                                 & 0.6461                                 & 1.0882                                 & 1.0839                                 & {\color[HTML]{0000FF} {\ul 0.631}}     & {\color[HTML]{FF0000} \textbf{0.6273}} & {\color[HTML]{0000FF} {\ul 1.0346}}    & 1.0302                                 & 0.6396                                 & 0.633                                  & 1.045                                  & {\color[HTML]{0000FF} {\ul 1.0297}}    \\
\multicolumn{1}{c|}{PedestrianCounts}     & 0.2892                                 & 0.2888                                 & 0.2983                                 & 0.2979                                 & 0.2929                                 & 0.2912                                 & {\color[HTML]{FF0000} \textbf{0.2915}} & {\color[HTML]{0000FF} {\ul 0.2885}}    & {\color[HTML]{0000FF} {\ul 0.2854}}    & {\color[HTML]{0000FF} {\ul 0.2827}}    & 0.2947                                 & 0.2911                                 & {\color[HTML]{FF0000} \textbf{0.2845}} & {\color[HTML]{FF0000} \textbf{0.2789}} & {\color[HTML]{0000FF} {\ul 0.2923}}    & {\color[HTML]{FF0000} \textbf{0.2859}} & 0.3066                                 & 0.2948                                 & 0.3112                                 & 0.2951                                 \\ \midrule
\multicolumn{1}{c|}{Average}              & 0.4949                                 & 0.4936                                 & 0.7973                                 & \multicolumn{1}{c|}{0.7942}            & 0.4756                                 & 0.4665                                 & 0.7382                                 & \multicolumn{1}{c|}{0.7111}                               & {\color[HTML]{FF0000} \textbf{0.4265}} & {\color[HTML]{FF0000} \textbf{0.4197}} & {\color[HTML]{FF0000} \textbf{0.5696}} & \multicolumn{1}{c|}{{\color[HTML]{FF0000} \textbf{0.5501}}} & 2.6748                                  & 0.4757                                  & 2408.1878                               & \multicolumn{1}{c|}{0.7633}                            & {\color[HTML]{0000FF} {\ul 0.4698}}    & {\color[HTML]{0000FF} {\ul 0.4582}}    & {\color[HTML]{0000FF} {\ul 0.712}}     & {\color[HTML]{0000FF} {\ul 0.6942}}                                   \\
\multicolumn{1}{c|}{Rank}              & 3.07                                   & 3.29                                   & 3.21                                   & \multicolumn{1}{c|}{3.43}              & 3.14                                   & 3.07                                   & {\color[HTML]{FF0000} \textbf{2.93}}   & \multicolumn{1}{c|}{{\color[HTML]{FF0000} \textbf{2.71}}} & {\color[HTML]{FF0000} \textbf{2.86}}   & 3.07                                   & {\color[HTML]{FF0000} \textbf{2.93}}   & \multicolumn{1}{c|}{{\color[HTML]{0000FF} {\ul 2.93}}}      & {\color[HTML]{0000FF} {\ul 2.93}}      & {\color[HTML]{FF0000} \textbf{2.64}}   & {\color[HTML]{0000FF} {\ul 3.00}}         & \multicolumn{1}{c|}{{\color[HTML]{0000FF} {\ul 2.93}}} & 3.00                                      & {\color[HTML]{0000FF} {\ul 2.93}}      & {\color[HTML]{FF0000} \textbf{2.93}}   & 3.00                                      \\ \bottomrule

\end{tabular}

}

\label{tab:main_results}
\end{table*}

\subsection{Hyperparameter search}

For HP tuning, we focused on optimizing parameters aligned with those described in TimeMixer \cite{wang_timemixer_2024}. 
Specifically, we searched for an input length between 96 and 512, model size \(d_{m}\) from 16 to 512, learning rate ranging from \(10^{-5}\) to \(0.1\), and encoder layers between 1 and 5.
We refer the reader to \Cref{tab:exact_hyperparameters} for exact values.
TimeMixer was limited to a maximum of 3 layers and a model size of 128 due to high memory demands. 
However, this is unlikely to affect performance, as the original HP search for all datasets in this study yielded results within these limits.
We used the Optuna framework \cite{optuna_2019} with a budget of 40 trials to optimize the HPs.
We employed the default \texttt{TPEsampler} for HP sampling and applied the \texttt{SuccessiveHalvingPruner}, configured with a minimum of three epochs and a reduction factor of two to prune unpromising trials.  
The search was conducted with a batch size of 8, a maximum of 15 epochs, and early stopping with a patience of 3 epochs.
All models were optimized with the Adam optimizer and an exponentially decaying scheduler following the default TSLib configuration. 
The optimal HPs, determined by the minimum validation loss in the trials, were used to train and evaluate the final model across three random seeds to ensure robust results.

We set the dimensions of the fully connected layers \(d_{f}\) equal to \(d_{m}\).
The patch length for transformer models using patching (i.e., PatchTST and TimeXer) was set to roughly match one period for each dataset, with the stride equal to the patch size (\Cref{tab:dataset_statistics}).
Original works concluded that variations in patch length have minimal effects \cite{nie_time_2023, wang_timexer_2024}.
For the rest of the model HPs, we default to the configurations provided in TSLib.


\subsection{Datasets}

We evaluate models on 14 datasets spanning five domains: Energy, Economy, Transport, Health, and Environment. 
The datasets vary significantly, containing between 7,588 and 72.58 million time points. 
They range from univariate to multivariate with up to 321 channels.
Sampling frequencies differ from 1 Hz to hourly and daily intervals.
Additionally, the datasets exhibit diversity in stationarity, complexity, trends, seasonality, and entropy (\Cref{sec:dataset_statistics}).
The dataset selection is designed to minimize bias, preventing undue advantages for any model while accounting for specific model strengths.


\subsection{Results: There is no champion}

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}
        {\input{plots/exclude_ds_absperf.pgf}}
    \caption{\textbf{Impact of datasets.}
    We evaluate the influence of removing a dataset from the evaluation, in particular \textit{MotorImagery}. We observe that this experiment leads to a significantly different conclusion, as iTransformer remains the best model, albeit with only a modest net average improvement over the other baselines.
    }
    \label{fig:exclude_ds_absperf}
\end{figure}

We follow the benchmark TSlib and use MSE and mean absolute error (MAE) to evaluate model performance. \Cref{tab:main_results} presents the MSE and MAE for each dataset, averaged over the most common forecast horizons \cite{wang_timexer_2024, liu_itransformer_2024, nie_time_2023, wang_timexer_2024}, revealing results that differ substantially from recent papers where proposed algorithms often dominate.
Instead, our findings indicate no definitive best-performing model across all datasets and forecast horizons (\Cref{tab:full_results}).
To assess reliability, we also compared our results with the best-reported outcomes from the original studies for three common datasets: \textit{ETT*}, \textit{Electricity}, and \textit{Weather}.
As shown in \Cref{tab:setup_reliability}, our HP search performed similar or better than the original papers' results.
While this is not a one-to-one comparison due to differences in HP search procedures and context lengths, achieving comparable or better scores confirms the proper implementation and tuning of our baselines.

To present a comprehensive view that highlights both the optimal outcomes and the realistic performance variability, we report the minimum values (best MSE/MAE) and the averages.
We observe that TimeMixer is the only model facing convergence challenges, specifically on \textit{Exchange}, impacting its overall average performance.
We further analyze run-to-run variability in \Cref{sec:hp_variability}.


\section{Why are they all champions?}
\label{sec:y_all_champ}

This section provides strong empirical evidence on possible reasons for how each model can become a champion.
We demonstrate how a few changes in the experimental setting, such as the exclusion/inclusion of an additional dataset, prediction horizon, or baseline model, plus HP tuning, may drastically change the final conclusions.
Additionally, we illustrate how specific visualizations can influence the perceived performance differences between models.
First, we interpret the results from our extensive experiments.
Second, to support our hypothesis, we analyze cases in recent literature in which such scenarios occur.

\begin{figure*}[t]
    \centering
    \resizebox{\linewidth}{!}
        {\input{plots/exclude_horizon_absperf.pgf}}
    \caption{\textbf{Impact of horizons.}
    We evaluate the influence of removing a forecast horizon from the evaluation. We observe that this experiment leads to three different benchmark champions underling the brittleness of conclusions regarding the best available models.
    }
    \label{fig:exclude_horizon_absperf}
\end{figure*}


\subsection{Impact of datasets} 

\paragraph{From our results} This experiment provides a practical sense of how much the addition/removal of a single dataset may affect conclusions. 
The most impactful case in our experimental setup corresponds to removing only the \textit{MotorImagery} dataset from the full pool.
As visible in \Cref{fig:exclude_ds_absperf} (left), when all datasets are evaluated, iTransformer (red bar) seems to clearly be the best model in terms of MSE.
However, when we remove it (\Cref{fig:exclude_ds_absperf}, right), we get a much closer scenario where all models are basically equivalent.
We report an additional view of this experiment in \Cref{sec:full_results} (\Cref{fig:all_ds_violin}), in which we show the MSE per dataset and that the sharp drop of difference is only due to the much better performance of iTransformer on \textit{MotorImagery}, which biases the perception of the overall rankings.

\paragraph{From literature} We observe that subsets of the full benchmark were used occasionally, which may be justified (e.g., too small datasets like ILI). 
\cite{liu_itransformer_2024}  averaged the performance over the four ETT datasets, which the authors justified by the similarity of the datasets.
However, this increased the percentage of wins of their proposed method from $33.3\%$ to $71.4\%$.

\subsection{Impact of prediction horizons} 

\paragraph{From our results} Similarly to the case of datasets, also prediction horizons may play a major role in biasing the perception of overall champions.
In this small experiment, we select $6$ datasets, more precisely \textit{ETT*}, \textit{Weather}, and \textit{Exchange}, and remove one prediction horizon at a time.
In such a manner, we simulate a slightly less broad evaluation yet reasonably acceptable considering that models are evaluated on $18$ total scenarios ($6$ datasets and $3$ prediction horizons). 
In \Cref{fig:exclude_horizon_absperf}, we find that we can have three different champions in terms of MSE (red bars) out of four different cases.
In particular, TimeXer wins the benchmark when prediction horizons $96$ and $192$ are removed, DLinear if the prediction is $336$, and iTransformer if the forecast length is $720$.
Although the differences are less pronounced than in the dataset case, this serves as further empirical evidence of the fragility of such conclusions.

\paragraph{From literature} \cite{shao2024exploring} focuses solely on a forecast horizon of 336, whereas \cite{qiu2024tfb} bases its analysis of the impact of different data characteristics on a horizon of 96 — despite reporting performance for all four forecast horizons. Our experiments demonstrate that this reporting can lead to differing interpretations of model performance, underscoring the critical need for more consistent evaluation practices in LTSF.


\subsection{Impact of evaluated models} 

\begin{table}[t]
\caption{\textbf{Impact of HP tuning.} We report the MSE of DLinear for \textit{Weather} at prediction length 96 when HP tuning is or is not performed both in our and previous papers, along with the relative improvement (when possible).
``$-$" indicates a missing analysis. 
}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccccc}
\toprule
 \textbf{MSE (DLinear)} & \citeyear{zeng_are_2022} & \citeyear{nie_time_2023} & \citeyear{wu_timesnet_2023} & \citeyear{liu_itransformer_2024} & \citeyear{wang_timemixer_2024} & \citeyear{wang_timexer_2024} & Ours \\
 \midrule
Unified HP & -- & -- & 0.196 & 0.196 & 0.195 & 0.196 & 0.198\\
HP tuning & 0.176 & 0.176 & -- & -- & 0.176 & --  & 0.168\\
\cmidrule(lr){2-8}
Rel. Improv. & -- & -- & -- & -- & +9.7\% & -- & +15.1\%\\
\bottomrule
\end{tabular}
}
\label{tab:impact_hp_tuning}
\end{table}

\paragraph{From our results}While less surprising, we stress that excluding the top model from a benchmark may automatically crown the second-best as a champion.
For instance, from the full experimental setup evaluated in \Cref{fig:exclude_ds_absperf} (left), removing iTransformer would automatically champion TimeXer as the best available model. 

\paragraph{From literature} We believe that the reports always include the best-performing complex models from the benchmarks (\Cref{sec:baselines}).
However, we notice the overall lack of inclusion of baseline models like N-Beats in the publications of the recent ``champions", although it is a top-3 method in \cite{wang_deep_2024}.
In addition, we identified cases where the best-performing model was excluded from discussions without clear justification.
For example, \cite{shao2024exploring, qiu2024tfb} claim recent transformers underperform compared to earlier methods, but their experiments show the most recent LTSF transformer at the time (PatchTST) outperformed competitors, contradicting this claim.
In addition, \cite{shao2024exploring} claims linear models are better for LTSF on datasets with unclear patterns or distribution shifts. However, the claim is based on results where PatchTST was excluded, although it performed similarly according to the full results, weakening their assertion.

\begin{table}[t]
\caption{\textbf{iPatch as an unreliable champion.}
%We summarize metrics over all datasets and prediction horizons.
Although iPatch scores the \textbf{\textcolor{red}{best}} average rank and the {\textcolor{blue}{\underline{second-best}}} MAE/MSE averaged over all datasets, it does not statistically differ from all the other baselines under a Friedman test (\Cref{sec:ipatch_not_champ}).
}
%\renewcommand{\arraystretch}{0.5}
%\setlength{\tabcolsep}{15pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc}
\toprule
 \textbf{Avg.}  & DLinear & PatchTST & iTransformer & TimeMixer & TimeXer & iPatch \\
 \midrule
MSE & 0.797 & 0.738 & \textbf{\textcolor{red}{0.570}} & 2408 & 0.712 & \textcolor{blue}{\underline{0.604}} \\
MAE & 0.494 & 0.476 & \textbf{\textcolor{red}{0.426}} & 2.675 & 0.470 & \textcolor{blue}{\underline{0.431}} \\
Rank & 3.71 & 3.50 & 3.71 & 3.57 & 3.43 & \textbf{\textcolor{red}{3.07}} \\
\bottomrule
\end{tabular}
}
\label{tab:ipatch_champ}
\end{table}


\subsection{Impact of HP tuning} 

\paragraph{From our results} The impact of HP tuning on the absolute performance of machine learning models in benchmarks is becoming increasingly evident \cite{Brigato_2021_ICCV, mcelfresh2024neural}.
In \Cref{tab:impact_hp_tuning}, we investigate whether this may also be the case in LTSF via a proof-of-concept example.
We report the evaluation in terms of MSE for DLinear on the \textit{Weather} dataset at prediction horizon 96.
HP tuning brings a relative performance boost of $\sim$15\% in our setup and an $\sim$10\% in \cite{wang_deep_2024}.
Similarly, building on the HP search details in \cite{wang_timexer_2024}, we found comparable performance between TimeMixer, iTransformer, and PatchTST, unlike the original work where TimeMixer consistently ranks first.
This underscores how close the actual performance of models is, making outcomes and conclusions sensitive to slight variations in HP search.
%Given the close performance of forecaster models, omitting HP tuning can lead to biased results where any model might incorrectly appear to be the benchmark champion.

\paragraph{From literature} In TSLib, the models are usually based on the implementation of the original publications.
However, in \cite{wu_timesnet_2023}, for a fair comparison, they changed the input embeddings and the final projections to be the same for all models. 
Specifically, the sequence length was set to 96 for all models by default.
This is critical since DLinear, after a broad ablation study, explicitly states that short input sequences ($<336$) lead to underfitting \cite{zeng_are_2022}.
We show the progression of the reported MSE of DLinear in \Cref{tab:impact_hp_tuning} and underline the sharp improvement (+9.7\%) if HP search, including context length, is performed.
This concludes one example where efforts towards a fair comparison put baseline models at a known disadvantage, and therefore, we recommend a fair HP search for all baselines.

\subsection{Impact of visualizations}

\paragraph{From our results}
Visualizations are a strong tool to convey a message. 
In \Cref{fig:bias_in_visualizations}, we investigate the impact of scales to visualize performance.
We observe that using an absolute scale exaggerates differences between models that are not perceived in the relative scale with uniform axes. 
Conversely, it can obscure substantial differences as in the example of \textit{MotorImagery} and \textit{BenzeneConcentration} (\Cref{fig:bias_in_visualizations}, right, b and e). 
Hence, for a fair comparison, we stress that researchers must carefully select appropriate styles and scales when visualizing results.

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=1.0\columnwidth]{plots/bias_in_visualizations.pdf}}
        \caption{\textbf{Bias in visualizations.}
        The radar plots show the same data represented at different scales (MSE for various datasets).}
        \label{fig:bias_in_visualizations}
    \end{center}
\end{figure}

\paragraph{From literature} Radar plots are commonly used in the literature. 
One example is \cite{liu_itransformer_2024}, which used a radar plot to visualize the relative performance between models, and \cite{qiu2024tfb}, which used a radar plot to show model performance across different dataset characteristics. 
In both cases, they used absolute scales. 
These choices can create a misleading impression of the models’ actual performance and lead to false conclusions. 
This is just one example of how visualization choices can shape result interpretation. 
Other plots may also create biased impressions through axis scaling or selective metric representation.


\section{How to make substantiated claims?}
\label{sec:real_claims}

This section provides guidelines and a proof-of-concept example of how to make substantiated claims supported by robust statistical evidence regarding models' superiority in a broad experimental setup.
First, we introduce recommended non-parametric statistical tests \cite{demvsar2006statistical}.
Second, we upgrade the existing iTransformer to emulate current model-design proposals and rigorously evaluate it in our setup (\Cref{sec:all_similar}). 
While not an overall champion, the upgraded model statistically outperforms its predecessor, demonstrating the idea that reliable claims can be made.

\subsection{Rigorous statistical testing}
\label{subsec:rig_testing}

Following good practices for reliable evaluations in machine learning, we claim that statistical tests should be used to decrease the chances of making unreliable claims.
\cite{demvsar2006statistical} studied various statistical tests for comparing classifiers from both theoretical and empirical perspectives. 
The study recommended a set of simple, reliable, and robust tests for such comparisons.
In particular, the sign test \cite{salzberg1997comparing} compares two classifiers over multiple datasets, and the Friedman test compares various classifiers over multiple datasets (see \Cref{sec:stat_tests} for details). 

The TSLib benchmark \cite{wang_deep_2024} employs averaging for presenting aggregated results. However, averages are susceptible to outliers \cite{demvsar2006statistical}.
A classifier's strong performance on one dataset can mask weaknesses elsewhere, so we prioritize consistent performance across problems, making dataset averaging unsuitable for evaluation (\Cref{sec:y_all_champ}).
Both the BasicTS+ \cite{shao2024exploring} and TFB \cite{qiu2024tfb} benchmarks focus on the number of wins achieved by each model but do not incorporate any statistical testing, making conclusions less reliable and hard to communicate in a concise manner.

\subsection{A proof-of-concept model for substantiated claims}

The iPatch model is introduced as a hybrid architecture combining principles from iTransformer and PatchTST to better capture variate and temporal-specific dynamics in multivariate time-series data.
By structuring the input into cycles and modeling both variate and cycle-level dependencies, iPatch provides a hierarchical approach to multivariate time-series forecasting.

\paragraph{Embedding} To prepare the input for its two-stage attention mechanism, we modify the embedding layer by structuring the temporal information into cycles for temporal attention.
Let the input be denoted as $\mathbf{x} \in \mathbb{R}^{B \times C \times L}$, where $B$ is the batch size, $C$ is the number of variates, and $L$ is the lookback length.
Following the iTransformer design, the input is tokenized into an embedding dimension $d_m$, resulting in $\mathbf{x} \in \mathbb{R}^{B \times C \times d_m}$.
In iPatch, however, we first divide the sequence into $N$ cycles of length $P$, such that $L = N \cdot P$.
This restructuring reshapes the input from $\mathbb{R}^{B \times C \times L}$ to $\mathbb{R}^{B \times C \times (N \cdot P)}$, then further into $\mathbb{R}^{B \times (C \cdot N) \times P}$ for preparing attention over the temporal dimension as in PatchTST.
Each cycle is subsequently embedded to $d_m$, resulting in $\mathbf{x} \in \mathbb{R}^{B \times (C \cdot N) \times d_m}$.  

\paragraph{Transformer layer} We enhance the attention module as a sequence of two attentions over variates and cycles.
First, the input $\mathbf{x} \in \mathbb{R}^{B \times (C \cdot N) \times d_m}$ is reshaped to $\mathbb{R}^{(B \cdot N) \times C \times d_m}$ to isolate the $C$ variates for each cycle.
Attention is applied over the variates similarly to iTransformer.
The result is reshaped back to $\mathbb{R}^{B \times (C \cdot N) \times d_m}$.
Next, the output of the variate attention is reshaped to $\mathbb{R}^{(B \cdot C) \times N \times d_m}$ to isolate temporal cycles.
We then apply the second attention mechanism over the $N$ cycles for each variate.
In the iTransformer, the MLP operates on univariate data, hypothesized to capture intrinsic time series properties like amplitude, periodicity, and frequency spectra \cite{liu_itransformer_2024}, for which we finally reshape the series from $\mathbb{R}^{(B \cdot C) \times N \times d_m}$ to \(\mathbb{R}^{(B \cdot N) \times C \times d_m}\) before applying the MLP layers.

\subsection{iPatch is not a champion although it may look so}
\label{sec:ipatch_not_champ}

First, we evaluate the performance of iPatch following either average MSE/MAE \cite{wang_deep_2024} or average ranks.
\Cref{tab:ipatch_champ} shows that iPatch achieves the best average rank, the second-best MSE, and MAE.
Complete results for iPatch are available in \Cref{tab:ipatch_results}.
In line with these outcomes and common practices in the field of LTSF, iPatch may ``almost" seem the best-performing model.
However, we then analyze more rigorously the results following the statistical analysis introduced in \Cref{subsec:rig_testing}.
In particular, we perform a Friedman test to compare the results of all the classifiers over all datasets from a statistical perspective.
We obtain a Friedman statistic of 1.14 and a p-value of 0.95, demonstrating that there is actually no real champion from this analysis. 

\subsection{iPatch statistically outperforms iTransformer}


Given the lack of a true champion, we proceed with pairwise comparisons among models by applying the sign test \cite{demvsar2006statistical}.
We first count the number of wins per dataset for each model and organize them in a 2D matrix, visible in \Cref{tab:sign_test}.
If the two algorithms compared are, as assumed under the null hypothesis, equivalent, each should win on approximately half of the datasets.
We then compute the corresponding p-values for rejecting the null hypothesis, given that the number of wins follows a binomial distribution. 
There are no statistically significant differences among all pairwise comparisons except for iPatch and iTransformer, with the first model winning on 11/14 datasets and scoring a significant p-value of 0.05.
This analysis provides two important takeaways: 1) The absence of statistical significance among pairwise comparisons re-iterates the lack of true champions among state-of-the-art models, and 2) with ad-hoc adjustments of existing architectures, it is still feasible to improve performance without claiming to be excellent overall.
It is also essential to consider other factors that contribute to a model's superiority, such as the trade-offs introduced by architectural modifications between performance and efficiency (e.g., speed or memory consumption) or the actual net improvements that are practically relevant.
However, these aspects fall beyond the scope of this paper.


\begin{table}[t]
\caption{\textbf{iPatch outperforms iTransformer.} 
We show pairwise win comparisons among models and indicate significant p-values following the sign test ($p < 0.05^{*}$). iPatch statistically outperforms iTransformer, scoring 11 wins and a p-value of 0.05. 
}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccc}
\toprule
\textbf{\#wins ($\rightarrow$)} & DLinear & PatchTST & iTransformer & TimeMixer & TimeXer & iPatch \\
\midrule
DLinear      & --  & 6  & 6  & 6  & 7  & 7  \\
PatchTST     & 8   & -- & 8  & 7  & 6  & 6  \\
iTransformer & 8   & 6  & -- & 7  & 8  & 3  \\
TimeMixer    & 8   & 7  & 7  & -- & 6  & 6  \\
TimeXer      & 7   & 8  & 6  & 8  & -- & 7  \\
iPatch       & 7   & 8  & 11$^{*}$ & 8  & 7  & -- \\
\bottomrule
\end{tabular}
}
\label{tab:sign_test}

\end{table}


\section{Model selection based on dataset features}
\label{sec:model_sel}

Identifying the appropriate model for a given dataset remains a challenging and nuanced task despite the availability of LTSF benchmarks.
%Characteristics such as seasonality, trend, distribution shift, stationarity, and inter-variate similarities are often used to describe datasets. 
% Although forecastability is often reported, determining the appropriate metric based on dataset characteristics remains an open question \cite{bezbochina_enhancing_2023}. 
As anticipated in \Cref{sec:recent_bench}, BasicTS+ and TFB provide guidelines for this fundamental challenge \cite{shao2024exploring, qiu2024tfb}.
In the following, we analyze whether our results align with previous observations.
As we stated earlier, the tested models perform similarly across most datasets, regardless of architecture or characteristics, with MSE variations between datasets far exceeding those between models.
For the following analyses, we make comparisons at a prediction length of 96 steps, as proposed in TFB.


\subsection{Linear vs. transformer}

BasicTS+ and TFB recommend using linear models when the data lacks clear patterns, has an increasing trend, or has a marked distribution shift.
Conversely, transformers are recommended for datasets with clear patterns, strong internal similarities, or nonlinear structures.
We revisit the example by \cite{shao2024exploring} and assess the performance of a linear model (DLinear) versus transformer model (PatchTST) on data with clear and unclear patterns, respectively. 
We use the same dataset with an unclear pattern (\textit{Exchange}) and replace their previously used \textit{PEMS} with a clear pattern by \textit{PedestrianCounts} (\Cref{fig:data_patterns}).
PatchTST outperforms DLinear on both occasions (\Cref{tab:full_results}), contradicting the previous claims by \cite{shao2024exploring}.
We highlight that PatchTST, a transformer model, was excluded from their analysis for unknown reasons.
However, both studies rely on limited datasets, urging caution before drawing broad conclusions.

\subsection{Univariate vs. multivariate}
Multivariate models should be preferred if -- and only if -- the dataset has strong inter-variate similarities \cite{shao2024exploring, qiu2024tfb}.
To assess this guideline, we compare the performance of PatchTST (univariate) against iTransformer (multivariate) on all datasets. 
We use the explained variance from principal component analysis as a proxy for inter-variate similarity (\Cref{sec:dataset_statistics}).
Indeed, also for this comparison, we observe that neither model performs increasingly better depending on the inter-variate similarity as the ranks fluctuate across the full spectrum (\Cref{fig:comp_variate_models}). %However, it is important to note that these analyses focus solely on inter-variable correlations and do not account for causal dependencies between variables. %This highlights the need for future studies to explore how causal relationships between variables impact the performance of multivariate models.

\subsection{Future direction}


% We are particularly interested in the \textit{MotorImagery} dataset because we observe the highest performance variance across models (\Cref{fig:radarplot}).
% This dataset has various characteristics on the far ends of the distributions (\Cref{tab:dataset_statistics}).
% The MSE of the models can arguably be grouped into three tiers.
% There are two models in the top tier (iTransformer, iPatch), three models in the bottom tier (DLinear, PatchTST, TimeMixer), and one model in the intermediate tier.
% Interestingly, this categorization aligns with the multivariate models at the top, univariate models at the bottom, and the model with a univariate/multivariate mix in between.
% We recommend that the community prioritize identifying and analyzing datasets that differentiate performance across models, as such datasets are crucial for drawing meaningful conclusions.
% Extensive research is needed to establish clear model selection guidelines, addressing challenges like defining baselines, acquiring representative datasets, and analyzing experiments in a rapidly evolving architecture landscape.
From \Cref{fig:radarplot}, we only observe interesting and substantial differences in model performance in two datasets (\textit{BenzeneConcentration} and \textit{MotorImagery}). 
Hence, we recommend that the community prioritize identifying and analyzing datasets that differentiate performance across models, as such datasets are crucial for drawing meaningful conclusions.
Extensive research is needed to establish clear model selection guidelines, addressing challenges like defining baselines, acquiring representative datasets, and analyzing experiments in a rapidly evolving architecture landscape.


\section{Alternative view}

\begin{figure}[t]
    \begin{center}
        \centerline{\includegraphics[width=1.0\columnwidth]{plots/comp_variate_models.pdf}}
        \caption{\textbf{Univariate vs. multivariate}
        PatchTST and iTransformer perform comparably as a function of explained variance.}
        \label{fig:comp_variate_models}
    \end{center}
\end{figure}

Critics might argue that our results are suboptimal due to experimental limitations, that focusing on recent models excludes influential earlier architectures and introduces bias, or that testing selected models overlooks the field's diversity.
Others may contend that some models consistently perform best on specific datasets, challenging our claim that no single model currently dominates in LTSF.

While these concerns are valid, our goal is not to establish exhaustive benchmarks or definitive rankings but to show that recent advancements often provide minimal improvement over earlier methods when experimental inconsistencies are addressed.
The limitations in observed progress among the latest models underscore the need for the field to prioritize standardized and transparent testing practices over introducing increasingly complex architectures.
Although our results may not be universally optimal, this further supports our position that small changes in experimental setups can significantly shift model rankings, making claims of superiority unreliable without standardized benchmarks and rigorous testing.
By focusing on recent models, we intentionally highlight the current state of the field and its challenges in reliably evaluating and comparing new methods.
Moreover, while some models may excel in narrow, context-specific scenarios, such isolated successes do not translate into universal applicability, further supporting our argument against the ``champion" narrative.

\section{Conclusions}

In this work, we evaluated LTSF research, highlighting the need for rigorous and standardized benchmarking practices.
Through an extensive and reproducible evaluation of 3,500+ models across 14 datasets, we demonstrated that claims of consistent performance improvements in newly published models often rely on specific experimental setups and evaluation methods.
Our findings question the idea of universal advancements, revealing that no single model consistently excels across our experiments.
We identified issues in the LTSF domain, such as non-standardized evaluation frameworks, biased comparisons, and limited reproducibility that hinder fair assessment and delay real progress.
To address these challenges, we recommend adopting standardized evaluation protocols, prioritizing benchmarking robustness over architectural complexity, and further investigating the link between dataset characteristics and model performance.

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\appendix
\onecolumn

\section{Related work}
\label{sec:related_work}

\subsection{Classical approaches}
Traditional statistical methods, such as AutoRegressive Integrated Moving Average \cite{BoxPierce1970}, Vector Autoregression \cite{toda1993}, Exponential Smoothing \cite{hyndman2008forecasting}, and Spectral Analysis \cite{koopmans1995spectral} were widely used in TS forecasting. 
Progressively, machine learning models such as XGBoost \cite{chen2016xgboost}, Random Forest \cite{breiman2001random}, Gradient Boosting Regression Trees \cite{friedman2001greedy}, and LightGBM \cite{ke2017lightgbm} have shown improvements in the forecast due to their ability to handle non-linear patterns.

\subsection{Deep learning models}

Deep learning models have advanced TS forecasting, starting with Recurrent Neural Networks (RNNs), specifically designed to model sequential data.
In particular, advanced variants such as RNNs with Long Short-Term Memory units, widely adopted within the TS community, have seen significantly increased usage \cite{hochreiter1997long}.
Additionally, MLP-based models, such as DLinear \cite{zeng_are_2022}, N-BEATS \cite{Oreshkin2020}, and N-Hits \cite{challu2023nhits} use MLP to learn the coefficients that produce both backcast and forecast outputs from their structure.

Originally from NLP, the Transformer architecture is increasingly adapted for time series forecasting, often with modified attention layers to capture temporal dependencies, as seen in \Cref{sec:all_champs} and other prior works, which we describe in the following.
Informer \cite{zhou2021informer} and Pyaformer \cite{liu2022pyraformer} are transformer-based models that modify the attention mechanism. 
Informer designs a ProbSparse self-attention mechanism to replace the standard self-attention. 
Pyaformer, on the other hand, presents a pyramidal attention module, where the inter-scale tree structure captures features at different resolutions, and the intra-scale neighboring connections model the temporal dependencies across different ranges.
Wu et al. \cite{wu2021autoformer} introduced the Autoformer with an Auto-Correlation mechanism to capture the series-wise temporal dependencies based on the learned periods.
Following, FEDformer \cite{zhou2022fedformer} utilizes a mixture-of-expert framework to improve seasonal-trend decomposition and integrates Fourier and Wavelet-enhanced blocks to capture key structures in the TS.
\cite{zhang2023crossformer} presented Crossformer, a transformer-based model utilizing cross-dimension dependency for multivariate TS forecasting.
Another recent approach is TimesNet \cite{wu_timesnet_2023}, which is a univariate 2D CNN that segments 1D time series according to Fourier decomposition. 
The segments are then stacked to build a 2D series. 
This enables the convolutions to simultaneously look at the local structure of the signal at $t_i$ and $t_{i-T}$ simultaneously, where $T$ denotes a dominant signal period.  


\subsection{Large Language Models}

The success of Large Language Models (LLMs) like BERT and GPT in natural language processing has inspired researchers to apply these models to TS tasks.
One significant approach involves transforming numerical TS data into natural language prompts to leverage pre-trained language models without modifications. 
PromptCast \cite{xue2023promptcast} and \cite{gruver2024large} present this method, demonstrating effective generalization in zero-shot settings and often outperforming traditional numerical models. 
Moving to few-shot training strategies, TEST \cite{sun2023test} adapts TS data for pre-trained LLMs by tokenizing the data and aligning the embedding space, particularly in few-shot and generalization scenarios.
Several frameworks focus on enhancing TS forecasting through specialized fine-tuning strategies such as LLM4TS \cite{chang2023llm4ts} and TEMPO \cite{cao2023tempo}.


\subsection{Foundation Models}

There is a growing interest in foundation models designed explicitly for TS tasks. 
Tiny Time Mixers \cite{ekambaram2024ttms} introduce a compact model for multivariate TS forecasting.
Timer-XL is a foundation model for unified time series forecasting, supporting univariate and multivariate data by extending next-token prediction for causal generation \cite{liu2024timer}.
The model introduced a universal TimeAttention mechanism to capture fine-grained intra- and inter-series dependencies.
MOIRAI \cite{woo2024unified} addresses challenges like cross-frequency learning and varied distributional properties in large-scale data, achieving competitive zero-shot forecasting performance.
TimeGPT-1 \cite{garza2023timegpt} and Lag-LLama \cite{rasul2023lag}, utilizing decoder-only transformer architectures and achieving strong zero-shot generalization.
Chronos \cite{ansari2024chronos} trains transformer-based models on discrete tokens processed from TS data, demonstrating superior performance on diverse datasets.
% Foundation models based on LLMs on TS forecasting such as Chronos \cite{ansari2024chronos} trains transformer-based language models on tokenized TS data, demonstrating superior performance on diverse datasets.


\section{Datasets}
\label{sec:dataset_statistics}

We include a commonly used set of small datasets (\textit{ETT*}, \textit{Electricity}, \textit{Weather}, \textit{Exchange}) and a set of larger datasets (\textit{MotorImagery}, \textit{TDBrain}, \textit{BeijingAir}, \textit{BenzeneConcentration}, \textit{AustraliaRainfall}, \textit{KDDCup2018}, \textit{PedestrianCounts}) which represents a subset of the Unified Time Series Dataset (UTSD) \cite{liu2024timer}.

This section provides a summary of descriptive statistics about the used datasets, an example of two contrastive datasets with clear and unclear patterns, respectively, and dataset-specific preprocessing steps.

\subsection{Dataset statistics}
\begin{table*}[h]
\renewcommand{\arraystretch}{1.8}
\centering
\caption{\textbf{Dataset statistics.}}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
Domain      & Dataset              & \# Timesteps & \# Channels & Shannon Entropy & Spectral Entropy & Sample Entropy & Stationarity & Complexity & Explained Variance (PC1) & Source \\ \midrule
Energy      & ETTh1                & 17420        & 7           & 0.775           & 0.669            & 0.769          & -5.909       & 0.497      & 0.344                    &    \cite{wang_deep_2024}    \\
Energy      & ETTm1                & 69680        & 7           & 0.789           & 0.548            & 0.430           & -14.985      & 0.485      & 0.346                    &    \cite{wang_deep_2024}    \\
Energy      & ETTh2                & 17420        & 7           & 0.813           & 0.639            & 0.526          & -4.136       & 0.397      & 0.431                    &    \cite{wang_deep_2024}    \\
Energy      & ETTm2                & 69680        & 7           & 0.817           & 0.527            & 0.319          & -5.664       & 0.425      & 0.431                    &    \cite{wang_deep_2024}    \\
Energy      & Electricity          & 26304        & 321         & 0.516           & 0.497            & 0.714          & -8.445       & 0.673      & 0.547                    &    \cite{wang_deep_2024}    \\
Environment & Weather              & 52696        & 21          & 0.453           & 0.57             & 0.110           & -26.681      & 0.632      & 0.424                    &    \cite{wang_deep_2024}    \\
Economic    & Exchange             & 7588         & 8           & 0.805           & 0.347            & 0.066          & -1.902       & 0.529      & 0.618                    &    \cite{wang_deep_2024}    \\
Health      & MotorImagery         & 1134000      & 64          & 0.719           & 0.519            & 0.326          & -3.133       & 0.763      & 0.305                    &    \cite{liu2024timer}    \\
Health      & TDBrain              & 2221212      & 33          & 0.823           & 0.749            & 0.987          & -3.167       & 0.404      & 0.475                    &    \cite{liu2024timer}    \\
Environment & BeijingAir           & 407184       & 9           & 0.493           & 0.686            & 0.951          & -13.253      & 0.165      & 0.383                    &    \cite{liu2024timer}    \\
Environment & BenzeneConcentration & 2042880      & 8           & 0.799           & 0.701            & 1.938          & -3.114       & -0.049     & 0.534                    &    \cite{liu2024timer}    \\
Environment & AustraliaRainfall    & 3846408      & 3           & 0.838           & 0.604            & 2.215          & -31.734      & -0.013     & 0.996                    &    \cite{liu2024timer}    \\
Environment & KDDCup2018           & 2942364      & 1           & 0.569           & 0.665            & 0.410           & -9.379       & 0.530       & 1.000                        &    \cite{liu2024timer}    \\
Transport   & PedestrianCounts     & 3132346      & 1           & 0.687           & 0.522            & 0.412          & -4.590        & 0.630       & 1.000                        &    \cite{liu2024timer}    \\ \bottomrule
\end{tabular}
}
\label{tab:dataset_statistics}
\end{table*}

In \Cref{tab:dataset_statistics}, we provide a comprehensive description of the datasets employed in this work and their corresponding statistics.
In the following, we describe in detail the various analyses employed to derive such dataset features.

\paragraph{Time steps and channels} We counted the \textit{number of time steps} and \textit{number of channels} as the size of the respective dimension.

\paragraph{Shannon entropy and spectral entropy}Shannon entropy quantifies the average level of uncertainty or information content associated with the outcomes in a discrete variable $X$.
Spectral entropy, a related concept, applies this measure to the frequency domain, using the normalized power spectral density as the probability distribution.
Entropy is calculated as 
\[
H(X) = -\sum_{x\in \chi} p(x) \log p(x)
\]
where $\chi$ is the set of all possible outcomes, and $p(x)$ is the probability of outcome $x$. 

\paragraph{Sample Entropy}Sample entropy is a statistical measure used to quantify the complexity or regularity of a time series. 
Unlike Shannon entropy, which evaluates uncertainty in discrete probability distributions, sample entropy assesses the likelihood that similar patterns in the time series remain similar at the next time step. 
It is defined as the negative natural logarithm of the conditional probability that two sequences of length $m$ that match within a tolerance $r$ will still match when extended to $m+1$. 
A lower sample entropy indicates more regularity or predictability in the time series, while a higher value suggests greater randomness or complexity.
Sample entropy is calculated as
\[
\text{SampEn}(m, r, N) = -\ln \frac{A}{B}
\]
where $m$ is the embedding dimension, $r$ is the tolerance, $N$ is the total length of the time series, $A$ is the number of matching pairs of length $m+1$, and $B$ is the number of matching pairs of length $m$.

\paragraph{Stationarity}In LTSF, stationarity is an important characteristic of time series, where the statistical characteristics, such as mean and variance, remain constant over time. 
We used the Augmented Dickey-Fuller (ADF) test to assess stationarity. 
This test evaluates the null hypothesis that a unit root is present and confirms stationarity if $\gamma < 0$ and the result is statistically significant.
We report $\gamma$ since it scales with stationarity.

\paragraph{Complexity} In time series analysis, complexity refers to the irregularity or unpredictability in the data.
We quantify complexity by using Higuchi's fractal dimension. 
A higher fractal dimension indicates greater complexity, while a lower value suggests more regularity or predictability in the data.
Higuchi’s fractal dimension is calculated as  
\[
D = \lim_{k \to 0} \frac{\log \left( \sum_{i=1}^n \left( \frac{|x_{i+k} - x_i|}{k} \right) \right)}{\log k}
\]
where $x_i$ represents the data points, $k$ is the time scale, and the sum is taken over different segments of the time series. 


\paragraph{Inter-variate similarity}As a proxy for inter-variate similarity, we provide the explained variance of the first principal component (PC1) obtained through principal component analysis (PCA). 
PC1 represents the direction of maximum variance in the data, capturing the dominant shared variation among the variables. 
The explained variance of PC1 quantifies the proportion of the total variance that is accounted for by this component. 
A higher explained variance indicates stronger similarity and shared dynamics among the variables, while a lower value suggests more independent behavior.

\subsection{Clear vs. unclear patterns}

In \Cref{sec:model_sel}, we assessed the performance of a linear model versus a transformer model on datasets with clear and unclear patterns, respectively.
We use the same dataset as BasicTS+ \cite{shao2024exploring} with an unclear pattern (\textit{Exchange}) and replace their previously used PEMS with a clear pattern by \textit{PedestrianCounts} (Fig. 4).
The plots of the time series highlight contrasting characteristics: \textit{Exchange} displays seemingly random trends, whereas \textit{PedestrianCounts} exhibits evident cyclic behavior.
To further emphasize this distinction, we provide butterfly plots, which reveal pronounced periodic patterns in \textit{PedestrianCounts} and irregular, stochastic-like trends in \textit{Exchange}.
Additionally, the power spectrum analysis underscores this contrast, showing a dominant peak for \textit{PedestrianCounts} and an absence of such peaks for \textit{Exchange}.

\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.7]{plots/data_patterns.pdf}
    \caption{\textbf{Distinct temporal patterns in two datasets.}
    }
    \label{fig:data_patterns}
\end{figure*}

\subsection{Preprocessing}

We followed the preprocessing steps from TSlib \cite{wang_deep_2024}. 
Furthermore, we added functionality to import data from UTSD as curated by \cite{liu2024timer}. 
Since the UTSD datasets are magnitudes larger, we modified the respective dataloader to return sequences at a stride length $S=100$ to accelerate the training.

\section{Exact hyperparameters}

To ensure a fair comparison, we performed an extensive hyperparameter search for all models on all the datasets.
We searched for input length, learning rate, number of layers, and model dimension.
The specific ranges are presented in\Cref{tab:exact_hyperparameters}.
Note that the range was the same for all models except for TimeMixer.
In that case, we limited the range for the number of layers and model dimensions due to increasingly high memory demands ($>49$GB  of VRAM on an RTX A6000 GPU if $d_{m} > 128$ and $L = 720$).
However, this is unlikely to affect performance, as the original HP search for all datasets in TimeMixer yielded results within these limits.

\begin{table}[H]
\caption{\textbf{Exact values of hyperparameters searched.} }
\centering
\begin{tabular}{@{}
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c @{}}
\toprule
Hyperparameter         & TimeMixer                & Other Models             \\ \midrule
Input Length           & \{96, 192, 336, 720\}                   & \{96, 192, 336, 720\}                   \\
Learning Rate          & \{\(10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\)\} & \{\(10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\)\} \\
\#Layers & \{1, 2, 3\}                             & \{1, 2, 3, 4\}                           \\
\(d_{m}\)               & \{16, 32, 64, 128\}                     & \{16, 32, 64, 128, 256, 512\}           \\ \bottomrule
\end{tabular}
\label{tab:exact_hyperparameters}
\end{table}


The patch length, a parameter used in PatchTST and TimeXer (and by extension also iPatch), was set based on the characteristics of the datasets (\Cref{tab:patch_length}).
As visualized in \Cref{fig:data_patterns}, there are datasets with dominant periodic behavior.
We set the patch length $P \approx \text{argmax}(\text{FFT}(X))$ wherever we observed such a natural pattern.
Unsurprisingly, the patch length resulted in a span of one day in all datasets with a pattern. 
In the remaining cases, we set the patch length manually (\textit{Exchange}, \textit{MotorImagery}, \textit{TDBrain}).
We aligned with the idea of TimesNet, which introduced series segmentation based on dominant frequencies to enhance performance \cite{wu_timesnet_2023}.
Moreover, the original works concluded that variations in patch length have minimal effects \cite{nie_time_2023, wang_timexer_2024}.

\begin{table*}[h]
\caption{\textbf{Patch lengths.} 
Details of the employed patch lengths for PatchTST, TimeXer, and iPatch models. 
}
\renewcommand{\arraystretch}{1.8}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
Dataset      & ETTh* & ETTm* & Electricity & Weather & Exchange & MotorImagery & TDBrain & BeijingAir & BenzeneConcentration & AustraliaRainfall & KDDCup2018 & PedestrianCounts \\ \midrule
Patch Length & 24    & 96    & 24          & 24      & 96       & 96           & 48      & 24         & 24                   & 24                & 24         & 24               \\ \bottomrule
\end{tabular}
}
\label{tab:patch_length}
\end{table*}

\section{Results reliability}

% \begin{table*}[]
% \caption{\textbf{Results reliability.}
% To assess the reliability of our results, we compare our implementation (best and average results), against the original works on popular datasets.
% Despite not being directly comparable for slight differences in setups, we align with previous values.
% }
% \centering
% \resizebox{\textwidth}{!}{

% \begin{tabular}{@{}cccccccccccccccc@{}}
% \toprule
% Dataset                             & \multicolumn{3}{c}{PatchTST}                                    & \multicolumn{3}{c}{TimeMixer}                                      & \multicolumn{3}{c}{iTransformer}                             & \multicolumn{3}{c}{TimeXer}                                  & \multicolumn{3}{c}{DLinear}                     \\ \midrule
% \multicolumn{1}{c|}{Implementation} & Original    & Ours Min       & \multicolumn{1}{c|}{Ours Mean}   & Original       & Ours Min       & \multicolumn{1}{c|}{Ours Mean}   & Original & Ours Min       & \multicolumn{1}{c|}{Ours Mean}   & Original & Ours Min       & \multicolumn{1}{c|}{Ours Mean}   & Original      & Ours Min       & Ours Mean      \\ \midrule
% \multicolumn{1}{c|}{ETT*}           & 0.338       & \textbf{0.332} & \multicolumn{1}{c|}{{\ul 0.336}} & \textbf{0.333} & {\ul 0.342}    & \multicolumn{1}{c|}{0.349}       & 0.383    & \textbf{0.342} & \multicolumn{1}{c|}{{\ul 0.344}} & 0.363    & \textbf{0.341} & \multicolumn{1}{c|}{0.346}       & \textbf{0.37} & {\ul 0.378}    & {\ul 0.378}    \\
% \multicolumn{1}{c|}{Electricity}    & 0.159       &                & \multicolumn{1}{c|}{}            & 0.156          &                & \multicolumn{1}{c|}{}            & 0.178    &                & \multicolumn{1}{c|}{}            & 0.171    &                & \multicolumn{1}{c|}{}            & 0.166         &                &                \\
% \multicolumn{1}{c|}{Weather}        & {\ul 0.225} & \textbf{0.224} & \multicolumn{1}{c|}{{\ul 0.225}} & 0.241          & \textbf{0.225} & \multicolumn{1}{c|}{{\ul 0.231}} & 0.258    & \textbf{0.237} & \multicolumn{1}{c|}{{\ul 0.239}} & 0.241    & \textbf{0.222} & \multicolumn{1}{c|}{{\ul 0.224}} & 0.246         & \textbf{0.244} & \textbf{0.244} \\ \bottomrule
% \end{tabular}

% }
% \label{tab:setup_reliability}
% \end{table*}

\begin{table*}[h]
\caption{\textbf{Results reliability.}
To assess the reliability of our results, we compare our implementation (best and average MSE) against the original works on popular datasets.
Despite not being directly comparable for slight differences in setups, we align with previous values.
}
\centering
\resizebox{\textwidth}{!}{

\begin{tabular}{cccccccccccccccc}
\toprule
Dataset                             & \multicolumn{3}{c}{DLinear}                          & \multicolumn{3}{c}{PatchTST}                         & \multicolumn{3}{c}{iTransformer}                     & \multicolumn{3}{c}{TimeMixer}                        & \multicolumn{3}{c}{TimeXer}     \\ \midrule
\multicolumn{1}{c|}{Implementation} & Original & Ours Min & \multicolumn{1}{c|}{Ours Mean} & Original & Ours Min & \multicolumn{1}{c|}{Ours Mean} & Original & Ours Min & \multicolumn{1}{c|}{Ours Mean} & Original & Ours Min & \multicolumn{1}{c|}{Ours Mean} & Original & Ours Min & Ours Mean \\ \midrule
\multicolumn{1}{c|}{ETT*}           & 0.37     & 0.378    & \multicolumn{1}{c|}{0.378}     & 0.338    & 0.332    & \multicolumn{1}{c|}{0.336}     & 0.383    & 0.342    & \multicolumn{1}{c|}{0.344}     & 0.333    & 0.342    & \multicolumn{1}{c|}{0.349}     & 0.363    & 0.341    & 0.346     \\
\multicolumn{1}{c|}{Electricity}    & 0.166    & 0.162    & \multicolumn{1}{c|}{0.162}     & 0.159    & 0.163    & \multicolumn{1}{c|}{0.164}     & 0.178    & 0.162    & \multicolumn{1}{c|}{0.166}     & 0.156    & 0.154    & \multicolumn{1}{c|}{0.156}     & 0.171    & 0.165    & 0.170     \\
\multicolumn{1}{c|}{Weather}        & 0.246    & 0.244    & \multicolumn{1}{c|}{0.244}     & 0.225    & 0.224    & \multicolumn{1}{c|}{0.225}     & 0.258    & 0.237    & \multicolumn{1}{c|}{0.239}     & 0.241    & 0.225    & \multicolumn{1}{c|}{0.231}     & 0.241    & 0.222    & 0.224     \\ \bottomrule
\end{tabular}

}
\label{tab:setup_reliability}
\end{table*}



\section{Stability of HP search}
\label{sec:hp_variability}

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \resizebox{\linewidth}{!}{\input{plots/hp_stability.pgf}}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \centering
        \resizebox{\linewidth}{!}{\input{plots/hp_stability_best.pgf}}
    \end{minipage}
    \caption{\textbf{HP search variability.}
    Comparison among two independent HP search results in terms of MSE for forecast horizon 96.
    The average of the final three models shows minimal variability except for a few cases (left), while the best model is even more stable (right).
    }
    \label{fig:hp_stability}
\end{figure*}


In this section, we perform a small proof-of-concept experiment to show the reliability of our HP search. 
For a subset of datasets and models, we performed two independent HP runs and consequent training of three models with the found optimal HPs to analyze the stability of the performed HP search.
We focused on prediction horizon 96.
In \Cref{fig:hp_stability}, we plot the MSE of the two searches over two opposing axes, meaning that points on the diagonal indicate a very stable search that leads to the same final result.
To provide an even more comprehensive analysis, we show the average of the three final models (left) and also the best model out of the final three (right).
The averages show a few cases with slight variability, namely PatchTST on \textit{BejingAir} and TimeXer on \textit{BenzeneConcentration}, but overall, the experiment proves a reliable and stable HP search across independent runs.
In the case of best results (\Cref{fig:hp_stability}, right), the variability is even less noticeable.


\begin{figure*}[t]
    \centering
    \resizebox{\linewidth}{!}
        {\input{plots/all_ds_violin.pgf}}
    \caption{\textbf{Impact of datasets.}
    We provide an alternative view of \Cref{fig:exclude_ds_absperf} and clearly visualize that the difference in average scores solely depends on the \textit{MotorImagery} dataset All baselines score similar results over the rest of the datasets.
    }
    \label{fig:all_ds_violin}
\end{figure*}

\section{Statistical tests}
\label{sec:stat_tests}

\subsection{Friedman test}

The Friedman test \cite{friedman1937use, friedman1940comparison} is a non-parametric statistical method designed as an alternative to repeated-measures ANOVA.
It enables the comparison of multiple algorithms across multiple datasets when the assumptions of parametric tests may not hold.
The test works by ranking the algorithms on each dataset separately, with the best-performing algorithm assigned a rank of 1, the second-best a rank of 2, and so forth.
In cases of tied performance, average ranks are assigned across the tied algorithms.

Let \( r^j_i \) denote the rank of the \( j \)-th algorithm out of \( k \) algorithms on the \( i \)-th dataset out of \( N \) datasets.
The Friedman test evaluates the average ranks of the algorithms, calculated as \(R^j = \frac{1}{N} \sum_{i=1}^N r^j_i\).
Under the null hypothesis, which assumes that all algorithms are equivalent in performance and thus their ranks \( R^j \) should be approximately equal, the Friedman statistic is given by:

\[
\chi^2_F = \frac{12N}{k(k+1)} \left[ \sum_{j=1}^k (R^j)^2 - \frac{k(k+1)^2}{4} \right]
\]

For sufficiently large values of \( N \) and \( k \), as a rule of thumb \( N > 10 \) and  \( k > 5 \) \cite{demvsar2006statistical}, this statistic follows a \( \chi^2 \) distribution with \( k-1 \) degrees of freedom.
Note our experimental setup aligns with these conditions.

The Friedman test, though less powerful than parametric repeated-measures ANOVA when its assumptions are met, is more robust in handling violations of these assumptions, with \cite{friedman1940comparison} observing largely consistent results between the two tests across 56 independent problems.
 
\subsection{Sign test}

The sign test \cite{salzberg1997comparing} is a non-parametric statistical method commonly used to compare the performance of two algorithms across multiple datasets.
It operates by evaluating the number of datasets on which each algorithm outperforms the other, assuming that the outcomes are independent and identically distributed.
Contrary to popular belief, counting only significant wins and losses actually makes the tests less reliable, as it imposes an arbitrary threshold of \( p < 0.05 \) to determine meaningfulness \cite{demvsar2006statistical}.

Under the null hypothesis, it is assumed that the two algorithms are equivalent in their performance, and thus, each algorithm has an equal probability (0.5) of outperforming the other on a given dataset. This leads to the number of wins for either algorithm following a binomial distribution with parameters \( N \) (the total number of datasets) and \( p = 0.5 \).
The null hypothesis is rejected if the observed number of wins for one algorithm is significantly different from \(\frac{N}{2}\), indicating that one algorithm systematically outperforms the other.

For small sample sizes, as in our case, with a total number of datasets being equal to 14,  critical values can be determined directly from the cumulative distribution function of the binomial distribution.
For larger sample sizes, the central limit theorem allows for an approximation using the normal distribution.
Specifically, the number of wins under the null hypothesis can be approximated by a normal distribution with mean \( \mu = N/2 \) and standard deviation \( \sigma = \sqrt{N}/2 \).
In cases where there are ties in performance, these ties are treated as supporting evidence for the null hypothesis.
To account for this, ties are split evenly between the two algorithms.
If the number of tied datasets is odd, one tie is disregarded to ensure that only whole numbers are assigned to each algorithm.


\section{Baseline comparisons}
\label{sec:baselines}

We provide an overview of included baselines for the top-performing LTSF models (\Cref{tab:baselines}).
We observe that previous models were replaced by recent ones as the field progressed.
We highlight that TimeMixer was not included as a baseline in TimeXer, although it was available at the time.

\begin{table*}[h]\centering
    \caption[caption]{\textbf{Included baseline models from top-performing models in long-term forecasting}. x: included; \textbf{o}: introduced}
        \small
    \begin{tabular}{l|ccccc|cccccccccccc}
        \toprule \noalign{\vskip 1mm}  
        Model                      & \rotatebox{90}{DLinear \citeyearpar{zeng_are_2022}}    & \rotatebox{90}{PatchTST \citeyearpar{zeng_are_2022}}    & \rotatebox{90}{TimeMixer \citeyearpar{wang_timemixer_2024}}    & \rotatebox{90}{iTransformer \citeyearpar{liu_itransformer_2024}}    & \rotatebox{90}{TimeXer \citeyearpar{wang_timexer_2024}}    & \rotatebox{90}{FEDformer \citeyearpar{zhou2022fedformer}} & \rotatebox{90}{Autoformer \citeyearpar{wu2021autoformer}} & \rotatebox{90}{Informer \citeyearpar{zhou2021informer}} & \rotatebox{90}{Pyraformer \citeyearpar{liu2022pyraformer}} & \rotatebox{90}{LogTrans \citeyearpar{li_enhancing_2019}} & \rotatebox{90}{Stationary \citeyearpar{liu2022stationary}} & \rotatebox{90}{Crossformer \citeyearpar{zhang2023crossformer}} & \rotatebox{90}{TimesNet \citeyearpar{wu_timesnet_2023}}   & \rotatebox{90}{SCINet \citeyearpar{liu2022scinet}} & \rotatebox{90}{Rlinear \citeyearpar{li_rlinear_2023}} & \rotatebox{90}{TiDE \citeyearpar{das_long-term_2024}} & \rotatebox{90}{\textit{others}} \\ \midrule 
        DLinear \citeyearpar{zeng_are_2022}                      & \textbf{o} &            &            &            &            & x       & x       & x       & x        & x        &          &          &          &                              &                              &                                     \\
         PatchTST \citeyearpar{nie_time_2023}                       & x          & \textbf{o} &            &            &            & x       & x       & x       & x        & x        &          &          &          &                              &                              &                                     \\
        TimeMixer \citeyearpar{wang_timemixer_2024}                       & x          & x          & \textbf{o} &            &            & x       & x       & x       &          &          & x        & x        & x        &          &                              &                              & x                                   \\
        iTransformer \citeyearpar{liu_itransformer_2024}                       & x          & x          &            & \textbf{o} &            & x       & x       &         &          &          & x        & x        & x        & x        & x                            & x                            & \multicolumn{1}{l}{}                \\
    TimeXer \citeyearpar{wang_timexer_2024}                    & x          & x          &            & x          & \textbf{o} &         & x       &         & x        &          & x        & x        & x        & x        & x                            & x                            &    \\ \bottomrule     \end{tabular}
    \label{tab:baselines}
\end{table*}

\section{Full results}
\label{sec:full_results}
\Cref{tab:full_results} shows the full results from our extensive experiments. 
We present the MSE and MAE for all forecast horizons $T \in \{96, 192, 336, 720\}$ and their average, respectively.
To provide a comprehensive view, we show the best and the average values over three random seeds.
\Cref{tab:ipatch_results} presents the corresponding full results for iPatch.

\begin{table*}[t]
\renewcommand{\arraystretch}{1.15}
\caption{\textbf{Full results.}
Mean and best (i.e., Min) values for all prediction lengths. \textbf{\textcolor{red}{Best}} and {\textcolor{blue}{\underline{second-best}}} are highlighted.
}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}c|c|cccc|cccc|cccc|cccc|cccc@{}}
\toprule
\multirow{+4}{*}{Dataset}              & Model                & \multicolumn{4}{c|}{DLinear}                                                                                                                                      & \multicolumn{4}{c|}{PatchTST}                                                                                                                                     & \multicolumn{4}{c|}{iTransformer}                                                                                                                                 & \multicolumn{4}{c|}{TimeMixer}                                                                                                                                    & \multicolumn{4}{c}{TimeXer}                                                                                                                                       \\ \cmidrule{2-22}
                     & Metric               & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c|}{MSE}                                                        & \multicolumn{2}{c}{MAE}                                                         & \multicolumn{2}{c}{MSE}                                                         \\ \cmidrule(lr){2-2}\cmidrule(lr){3-4}\cmidrule(lr){5-6} \cmidrule(lr){7-8}\cmidrule(lr){9-10} \cmidrule(lr){11-12}\cmidrule(lr){13-14}\cmidrule(lr){15-16}\cmidrule(lr){17-18}\cmidrule(lr){19-20}\cmidrule(lr){21-22}
                     & Statistic            & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c|}{Min}               & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                & \multicolumn{1}{c}{Mean}               & \multicolumn{1}{c}{Min}                \\ 
                     \midrule
                                       & 96        & {\color[HTML]{FF0000} \textbf{0.3938}} & {\color[HTML]{FF0000} \textbf{0.3932}} & {\color[HTML]{0000FF} {\ul 0.3703}}    & {\color[HTML]{0000FF} {\ul 0.3698}}    & 0.4044                                 & 0.4034                                 & 0.3798                                 & 0.3779                                 & 0.4031                                 & 0.4002                                 & 0.3806                                 & 0.3787                                 & {\color[HTML]{0000FF} {\ul 0.3967}}    & {\color[HTML]{0000FF} {\ul 0.3958}}    & {\color[HTML]{FF0000} \textbf{0.3702}} & {\color[HTML]{FF0000} \textbf{0.3682}} & 0.4001                                 & 0.3976                                 & 0.3758                                 & 0.3712                                 \\
                                       & 192       & 0.4366                                 & 0.4366                                 & 0.4225                                 & 0.4225                                 & {\color[HTML]{FF0000} \textbf{0.4277}} & {\color[HTML]{0000FF} {\ul 0.4231}}    & 0.4176                                 & 0.4125                                 & 0.4384                                 & 0.437                                  & {\color[HTML]{0000FF} {\ul 0.4133}}    & 0.4109                                 & 0.4349                                 & 0.4261                                 & {\color[HTML]{FF0000} \textbf{0.411}}  & {\color[HTML]{FF0000} \textbf{0.4045}} & {\color[HTML]{0000FF} {\ul 0.4297}}    & {\color[HTML]{FF0000} \textbf{0.4225}} & 0.4171                                 & {\color[HTML]{0000FF} {\ul 0.4075}}    \\
                                       & 336       & 0.5320                                  & 0.532                                  & 0.5427                                 & 0.5426                                 & {\color[HTML]{FF0000} \textbf{0.4329}} & {\color[HTML]{FF0000} \textbf{0.4306}} & {\color[HTML]{FF0000} \textbf{0.4221}} & {\color[HTML]{FF0000} \textbf{0.4170}}  & 0.4423                                 & 0.4413                                 & 0.4351                                 & 0.4327                                 & 0.4416                                 & 0.4381                                 & 0.438                                  & 0.4343                                 & {\color[HTML]{0000FF} {\ul 0.4345}}    & {\color[HTML]{0000FF} {\ul 0.4308}}    & {\color[HTML]{0000FF} {\ul 0.4247}}    & {\color[HTML]{0000FF} {\ul 0.4217}}    \\
                                       & 720       & 0.5464                                 & 0.5428                                 & 0.5608                                 & 0.5551                                 & {\color[HTML]{FF0000} \textbf{0.4626}} & {\color[HTML]{FF0000} \textbf{0.4543}} & {\color[HTML]{FF0000} \textbf{0.4378}} & {\color[HTML]{FF0000} \textbf{0.4231}} & {\color[HTML]{0000FF} {\ul 0.4876}}    & {\color[HTML]{0000FF} {\ul 0.4847}}    & {\color[HTML]{0000FF} {\ul 0.4684}}    & {\color[HTML]{0000FF} {\ul 0.4646}}    & 0.4985                                 & 0.4944                                 & 0.4968                                 & 0.4909                                 & 0.4957                                 & 0.4934                                 & 0.4831                                 & 0.4812                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{ETTh1}                & Average   & 0.4772                                 & 0.4761                                 & 0.4741                                 & 0.4725                                 & {\color[HTML]{FF0000} \textbf{0.4319}} & {\color[HTML]{FF0000} \textbf{0.4279}} & {\color[HTML]{FF0000} \textbf{0.4143}} & {\color[HTML]{FF0000} \textbf{0.4076}} & 0.4428                                 & 0.4408                                 & {\color[HTML]{0000FF} {\ul 0.4244}}    & 0.4217                                 & 0.4429                                 & 0.4386                                 & 0.429                                  & 0.4245                                 & {\color[HTML]{0000FF} {\ul 0.4400}}      & {\color[HTML]{0000FF} {\ul 0.4361}}    & 0.4252                                 & {\color[HTML]{0000FF} {\ul 0.4204}}    \\ \midrule
                                       & 96        & {\color[HTML]{FF0000} \textbf{0.3736}} & {\color[HTML]{0000FF} {\ul 0.3736}}    & 0.3415                                 & 0.3415                                 & {\color[HTML]{0000FF} {\ul 0.3737}}    & {\color[HTML]{FF0000} \textbf{0.3705}} & {\color[HTML]{FF0000} \textbf{0.3263}} & {\color[HTML]{FF0000} \textbf{0.3236}} & 0.387                                  & 0.3801                                 & {\color[HTML]{0000FF} {\ul 0.3374}}    & 0.3294                                 & 0.3869                                 & 0.3777                                 & 0.3410                                  & {\color[HTML]{0000FF} {\ul 0.3292}}    & 0.3961                                 & 0.3902                                 & 0.3482                                 & 0.3393                                 \\
                                       & 192       & {\color[HTML]{FF0000} \textbf{0.4070}}  & {\color[HTML]{FF0000} \textbf{0.4070}}  & 0.4015                                 & 0.4015                                 & {\color[HTML]{0000FF} {\ul 0.4120}}     & {\color[HTML]{0000FF} {\ul 0.4097}}    & {\color[HTML]{FF0000} \textbf{0.3618}} & {\color[HTML]{FF0000} \textbf{0.3613}} & 0.4254                                 & 0.4243                                 & 0.4028                                 & 0.4008                                 & 0.4272                                 & 0.4126                                 & {\color[HTML]{0000FF} {\ul 0.3972}}    & {\color[HTML]{0000FF} {\ul 0.3692}}    & 0.4279                                 & 0.422                                  & 0.4010                                  & 0.3934                                 \\
                                       & 336       & {\color[HTML]{FF0000} \textbf{0.4349}} & {\color[HTML]{FF0000} \textbf{0.4325}} & {\color[HTML]{0000FF} {\ul 0.4116}}    & {\color[HTML]{0000FF} {\ul 0.4093}}    & {\color[HTML]{0000FF} {\ul 0.4445}}    & {\color[HTML]{0000FF} {\ul 0.4431}}    & {\color[HTML]{FF0000} \textbf{0.4062}} & {\color[HTML]{FF0000} \textbf{0.4019}} & 0.4519                                 & 0.4469                                 & 0.4232                                 & 0.4157                                 & 0.4808                                 & 0.4733                                 & 0.4976                                 & 0.487                                  & 0.4612                                 & 0.4575                                 & 0.4277                                 & 0.4246                                 \\
                                       & 720       & {\color[HTML]{FF0000} \textbf{0.4716}} & {\color[HTML]{FF0000} \textbf{0.4701}} & {\color[HTML]{FF0000} \textbf{0.4578}} & {\color[HTML]{FF0000} \textbf{0.4567}} & 0.4955                                 & 0.4937                                 & 0.4799                                 & 0.4792                                 & {\color[HTML]{0000FF} {\ul 0.4850}}     & {\color[HTML]{0000FF} {\ul 0.482}}     & {\color[HTML]{0000FF} {\ul 0.4687}}    & 0.4657                                 & 0.4997                                 & 0.4905                                 & 0.4904                                 & 0.4773                                 & 0.4931                                 & 0.4894                                 & 0.4715                                 & {\color[HTML]{0000FF} {\ul 0.4645}}    \\ \cmidrule{2-22} 
\multirow{-5}{*}{ETTm1}                & Average   & {\color[HTML]{FF0000} \textbf{0.4218}} & {\color[HTML]{FF0000} \textbf{0.4208}} & {\color[HTML]{0000FF} {\ul 0.4031}}    & {\color[HTML]{0000FF} {\ul 0.4023}}    & {\color[HTML]{0000FF} {\ul 0.4314}}    & {\color[HTML]{0000FF} {\ul 0.4292}}    & {\color[HTML]{FF0000} \textbf{0.3936}} & {\color[HTML]{FF0000} \textbf{0.3915}} & 0.4373                                 & 0.4333                                 & 0.408                                  & 0.4029                                 & 0.4487                                 & 0.4385                                 & 0.4315                                 & 0.4157                                 & 0.4446                                 & 0.4398                                 & 0.4121                                 & 0.4054                                 \\ \midrule
                                       & 96        & 0.3677                                 & 0.3655                                 & 0.3042                                 & 0.3014                                 & {\color[HTML]{FF0000} \textbf{0.3414}} & {\color[HTML]{FF0000} \textbf{0.3389}} & {\color[HTML]{FF0000} \textbf{0.2833}} & {\color[HTML]{FF0000} \textbf{0.2807}} & 0.3515                                 & 0.348                                  & 0.3026                                 & 0.2964                                 & {\color[HTML]{0000FF} {\ul 0.3439}}    & {\color[HTML]{0000FF} {\ul 0.3413}}    & {\color[HTML]{0000FF} {\ul 0.287}}     & {\color[HTML]{0000FF} {\ul 0.2836}}    & 0.3489                                 & 0.3460                                  & 0.2965                                 & 0.2948                                 \\
                                       & 192       & 0.4429                                 & 0.4428                                 & 0.4232                                 & 0.4231                                 & {\color[HTML]{FF0000} \textbf{0.3919}} & {\color[HTML]{FF0000} \textbf{0.3902}} & {\color[HTML]{FF0000} \textbf{0.3602}} & {\color[HTML]{FF0000} \textbf{0.3563}} & 0.4016                                 & 0.3972                                 & 0.3837                                 & 0.379                                  & 0.4055                                 & 0.4041                                 & 0.3876                                 & 0.3843                                 & {\color[HTML]{0000FF} {\ul 0.3971}}    & {\color[HTML]{0000FF} {\ul 0.3954}}    & {\color[HTML]{0000FF} {\ul 0.3658}}    & {\color[HTML]{0000FF} {\ul 0.3624}}    \\
                                       & 336       & 0.4875                                 & 0.4874                                 & 0.5127                                 & 0.5126                                 & {\color[HTML]{0000FF} {\ul 0.4263}}    & 0.4243                                 & 0.406                                  & 0.4026                                 & {\color[HTML]{FF0000} \textbf{0.4248}} & 0.4226                                 & {\color[HTML]{0000FF} {\ul 0.403}}     & {\color[HTML]{0000FF} {\ul 0.3977}}    & 0.4288                                 & {\color[HTML]{FF0000} \textbf{0.4201}} & 0.4144                                 & {\color[HTML]{FF0000} \textbf{0.3975}} & 0.4266                                 & {\color[HTML]{0000FF} {\ul 0.4225}}    & {\color[HTML]{FF0000} \textbf{0.4016}} & {\color[HTML]{0000FF} {\ul 0.3977}}    \\
                                       & 720       & 0.5990                                  & 0.5988                                 & 0.6997                                 & 0.6991                                 & 0.48                                   & 0.4715                                 & 0.4677                                 & 0.4548                                 & {\color[HTML]{0000FF} {\ul 0.4436}}    & {\color[HTML]{0000FF} {\ul 0.4432}}    & {\color[HTML]{0000FF} {\ul 0.4245}}    & {\color[HTML]{0000FF} {\ul 0.4237}}    & {\color[HTML]{FF0000} \textbf{0.4395}} & {\color[HTML]{FF0000} \textbf{0.4388}} & {\color[HTML]{FF0000} \textbf{0.4086}} & {\color[HTML]{FF0000} \textbf{0.4081}} & 0.4524                                 & 0.4515                                 & 0.4441                                 & 0.4435                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{ETTh2}                & Average   & 0.4743                                 & 0.4736                                 & 0.4849                                 & 0.4841                                 & 0.4099                                 & 0.4062                                 & 0.3793                                 & {\color[HTML]{0000FF} {\ul 0.3736}}    & {\color[HTML]{0000FF} {\ul 0.4054}}    & {\color[HTML]{0000FF} {\ul 0.4028}}    & 0.3784                                 & 0.3742                                 & {\color[HTML]{FF0000} \textbf{0.4044}} & {\color[HTML]{FF0000} \textbf{0.4011}} & {\color[HTML]{FF0000} \textbf{0.3744}} & {\color[HTML]{FF0000} \textbf{0.3684}} & 0.4063                                 & 0.4038                                 & {\color[HTML]{0000FF} {\ul 0.3770}}     & 0.3746                                 \\ \midrule
                                       & 96        & {\color[HTML]{FF0000} \textbf{0.2215}} & {\color[HTML]{FF0000} \textbf{0.2215}} & {\color[HTML]{FF0000} \textbf{0.1081}} & {\color[HTML]{FF0000} \textbf{0.1081}} & {\color[HTML]{0000FF} {\ul 0.2243}}    & {\color[HTML]{0000FF} {\ul 0.2234}}    & {\color[HTML]{0000FF} {\ul 0.1093}}    & {\color[HTML]{0000FF} {\ul 0.1089}}    & 0.2325                                 & 0.2322                                 & 0.1179                                 & 0.1176                                 & 0.2253                                 & 0.224                                  & 0.1102                                 & {\color[HTML]{0000FF} {\ul 0.1089}}    & 0.2275                                 & 0.2273                                 & 0.1136                                 & 0.1131                                 \\
                                       & 192       & {\color[HTML]{FF0000} \textbf{0.2471}} & {\color[HTML]{FF0000} \textbf{0.247}}  & {\color[HTML]{FF0000} \textbf{0.134}}  & {\color[HTML]{FF0000} \textbf{0.1339}} & 0.2561                                 & 0.2547                                 & {\color[HTML]{0000FF} {\ul 0.1396}}    & {\color[HTML]{0000FF} {\ul 0.1384}}    & 0.2617                                 & 0.2608                                 & 0.1483                                 & 0.1473                                 & {\color[HTML]{0000FF} {\ul 0.2532}}    & {\color[HTML]{0000FF} {\ul 0.2527}}    & 0.1399                                 & 0.1395                                 & 0.2689                                 & 0.2636                                 & 0.1562                                 & 0.1505                                 \\
                                       & 336       & {\color[HTML]{FF0000} \textbf{0.2713}} & {\color[HTML]{FF0000} \textbf{0.2712}} & {\color[HTML]{FF0000} \textbf{0.1616}} & {\color[HTML]{FF0000} \textbf{0.1616}} & {\color[HTML]{0000FF} {\ul 0.2811}}    & {\color[HTML]{0000FF} {\ul 0.279}}     & {\color[HTML]{0000FF} {\ul 0.1675}}    & {\color[HTML]{0000FF} {\ul 0.1658}}    & 0.2899                                 & 0.2891                                 & 0.1826                                 & 0.1816                                 & 0.2848                                 & 0.2826                                 & 0.1792                                 & 0.1746                                 & 0.287                                  & 0.2853                                 & 0.1785                                 & 0.1771                                 \\
                                       & 720       & {\color[HTML]{FF0000} \textbf{0.3044}} & {\color[HTML]{FF0000} \textbf{0.3044}} & {\color[HTML]{FF0000} \textbf{0.2008}} & {\color[HTML]{FF0000} \textbf{0.2008}} & {\color[HTML]{0000FF} {\ul 0.3148}}    & {\color[HTML]{0000FF} {\ul 0.3133}}    & {\color[HTML]{0000FF} {\ul 0.2089}}    & {\color[HTML]{0000FF} {\ul 0.2079}}    & 0.3195                                 & 0.3175                                 & 0.2157                                 & 0.2135                                 & 0.3183                                 & 0.315                                  & 0.2179                                 & 0.2161                                 & 0.3235                                 & 0.3213                                 & 0.2246                                 & 0.2211                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{ETTm2}                & Average   & {\color[HTML]{FF0000} \textbf{0.2611}} & {\color[HTML]{FF0000} \textbf{0.261}}  & {\color[HTML]{FF0000} \textbf{0.1511}} & {\color[HTML]{FF0000} \textbf{0.1511}} & {\color[HTML]{0000FF} {\ul 0.2691}}    & {\color[HTML]{0000FF} {\ul 0.2676}}    & {\color[HTML]{0000FF} {\ul 0.1563}}    & {\color[HTML]{0000FF} {\ul 0.1552}}    & 0.2759                                 & 0.2749                                 & 0.1661                                 & 0.1650                                  & 0.2704                                 & 0.2686                                 & 0.1618                                 & 0.1598                                 & 0.2767                                 & 0.2744                                 & 0.1682                                 & 0.1654                                 \\ \midrule
                                       & 96        & 0.2298                                 & 0.2298                                 & 0.1330                                  & 0.1330                                  & {\color[HTML]{FF0000} \textbf{0.2274}} & {\color[HTML]{FF0000} \textbf{0.227}}  & {\color[HTML]{FF0000} \textbf{0.1301}} & {\color[HTML]{0000FF} {\ul 0.1300}}      & 0.2300                                   & {\color[HTML]{0000FF} {\ul 0.2290}}     & 0.1350                                  & 0.134                                  & {\color[HTML]{0000FF} {\ul 0.2280}}     & {\color[HTML]{FF0000} \textbf{0.2270}}  & {\color[HTML]{0000FF} {\ul 0.1311}}    & {\color[HTML]{FF0000} \textbf{0.1299}} & 0.2398                                 & 0.2393                                 & 0.1375                                 & 0.1372                                 \\
                                       & 192       & 0.275                                  & 0.2749                                 & 0.2156                                       & \multicolumn{1}{c|}{0.2156}                                 & {\color[HTML]{0000FF} {\ul 0.2465}}          & 0.2458                                       & {\color[HTML]{0000FF} {\ul 0.1972}}                      & 0.1962                                                    & 0.2582                                                   & 0.2579                                 & 0.2076                                                   & 0.2071                                                 & 0.2545                                                & {\color[HTML]{0000FF} {\ul 0.2437}}                      & 0.2044                                             & {\color[HTML]{0000FF} {\ul 0.1938}}                    & {\color[HTML]{FF0000} \textbf{0.2415}} & {\color[HTML]{FF0000} \textbf{0.2392}}                & {\color[HTML]{FF0000} \textbf{0.192}}                    & {\color[HTML]{FF0000} \textbf{0.1892}}       \\
                                       & 336       & {\color[HTML]{0000FF} {\ul 0.2621}}    & 0.2612                                 & {\color[HTML]{0000FF} {\ul 0.1633}}    & {\color[HTML]{0000FF} {\ul 0.1627}}    & 0.2718                                 & 0.2713                                 & 0.1725                                 & 0.1710                                  & 0.2686                                 & {\color[HTML]{0000FF} {\ul 0.2602}}    & 0.1696                                 & 0.1629                                 & 0.2645                                 & 0.2618                                 & 0.1653                                 & 0.1628                                 & {\color[HTML]{FF0000} \textbf{0.2611}} & {\color[HTML]{FF0000} \textbf{0.2575}} & {\color[HTML]{FF0000} \textbf{0.1626}} & {\color[HTML]{FF0000} \textbf{0.1599}} \\
                                       & 720       & 0.2991                                 & 0.2991                                 & {\color[HTML]{0000FF} {\ul 0.2031}}    & 0.2031                                 & 0.3045                                 & 0.3038                                 & 0.207                                  & 0.2062                                 & {\color[HTML]{0000FF} {\ul 0.2975}}    & {\color[HTML]{0000FF} {\ul 0.2957}}    & 0.2046                                 & {\color[HTML]{0000FF} {\ul 0.2024}}    & {\color[HTML]{FF0000} \textbf{0.2811}} & {\color[HTML]{FF0000} \textbf{0.2785}} & {\color[HTML]{FF0000} \textbf{0.1826}} & {\color[HTML]{FF0000} \textbf{0.1785}} & 0.321                                  & 0.3106                                 & 0.2293                                 & 0.2135                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{Electricity}          & Average   & {\color[HTML]{0000FF} {\ul 0.2585}}    & 0.2583                                 & {\color[HTML]{0000FF} {\ul 0.1616}}    & {\color[HTML]{0000FF} {\ul 0.1615}}    & 0.2613                                 & 0.2608                                 & 0.164                                  & 0.1634                                 & 0.2622                                 & {\color[HTML]{0000FF} {\ul 0.2577}}    & 0.1655                                 & 0.162                                  & {\color[HTML]{FF0000} \textbf{0.2704}} & {\color[HTML]{FF0000} \textbf{0.2686}} & {\color[HTML]{FF0000} \textbf{0.1618}} & {\color[HTML]{FF0000} \textbf{0.1598}} & 0.2676                                 & 0.2635                                 & 0.1695                                 & 0.1645                                 \\ \midrule
                                       & 96        & 0.2262                                 & 0.226                                  & 0.1678                                 & 0.1678                                 & {\color[HTML]{FF0000} \textbf{0.1952}} & {\color[HTML]{FF0000} \textbf{0.1947}} & {\color[HTML]{FF0000} \textbf{0.1455}} & {\color[HTML]{FF0000} \textbf{0.1451}} & 0.2158                                 & 0.2141                                 & 0.164                                  & 0.1616                                 & 0.2114                                 & 0.2012                                 & 0.1567                                 & 0.1487                                 & {\color[HTML]{0000FF} {\ul 0.1991}}    & {\color[HTML]{0000FF} {\ul 0.1975}}    & {\color[HTML]{0000FF} {\ul 0.1472}}    & {\color[HTML]{0000FF} {\ul 0.1459}}    \\
                                       & 192       & 0.2750                                  & 0.2749                                 & 0.2156                                 & 0.2156                                 & {\color[HTML]{0000FF} {\ul 0.2465}}    & 0.2458                                 & {\color[HTML]{0000FF} {\ul 0.1972}}    & 0.1962                                 & 0.2582                                 & 0.2579                                 & 0.2076                                 & 0.2071                                 & 0.2545                                 & {\color[HTML]{0000FF} {\ul 0.2437}}    & 0.2044                                 & {\color[HTML]{0000FF} {\ul 0.1938}}    & {\color[HTML]{FF0000} \textbf{0.2415}} & {\color[HTML]{FF0000} \textbf{0.2392}} & {\color[HTML]{FF0000} \textbf{0.192}}  & {\color[HTML]{FF0000} \textbf{0.1892}} \\
                                       & 336       & 0.3158                                 & 0.3157                                 & 0.2625                                 & 0.2625                                 & {\color[HTML]{FF0000} \textbf{0.2817}} & {\color[HTML]{FF0000} \textbf{0.2806}} & {\color[HTML]{FF0000} \textbf{0.2431}} & {\color[HTML]{FF0000} \textbf{0.2415}} & 0.2999                                 & 0.2987                                 & 0.2601                                 & 0.258                                  & 0.2888                                 & 0.2849                                 & 0.2535                                 & 0.2496                                 & {\color[HTML]{0000FF} {\ul 0.2858}}    & {\color[HTML]{0000FF} {\ul 0.2817}}    & {\color[HTML]{0000FF} {\ul 0.2451}}    & {\color[HTML]{0000FF} {\ul 0.2426}}    \\
                                       & 720       & 0.3767                                 & 0.3767                                 & 0.3307                                 & 0.3307                                 & 0.3298                                 & 0.3279                                 & 0.3123                                 & 0.3111                                 & 0.3427                                 & 0.3408                                 & 0.322                                  & 0.3202                                 & {\color[HTML]{FF0000} \textbf{0.3284}} & {\color[HTML]{FF0000} \textbf{0.3259}} & {\color[HTML]{0000FF} {\ul 0.3108}}    & {\color[HTML]{FF0000} \textbf{0.3069}} & {\color[HTML]{0000FF} {\ul 0.3291}}    & {\color[HTML]{0000FF} {\ul 0.3278}}    & {\color[HTML]{FF0000} \textbf{0.3103}} & {\color[HTML]{0000FF} {\ul 0.309}}     \\ \cmidrule{2-22} 
\multirow{-5}{*}{Weather}              & Average   & 0.2984                                 & 0.2984                                 & 0.2442                                 & 0.2441                                 & {\color[HTML]{FF0000} \textbf{0.2633}} & {\color[HTML]{0000FF} {\ul 0.2623}}    & {\color[HTML]{0000FF} {\ul 0.2245}}    & {\color[HTML]{0000FF} {\ul 0.2235}}    & 0.2791                                 & 0.2779                                 & 0.2385                                 & 0.2367                                 & 0.2707                                 & 0.2639                                 & 0.2313                                 & 0.2248                                 & {\color[HTML]{0000FF} {\ul 0.2639}}    & {\color[HTML]{FF0000} \textbf{0.2616}} & {\color[HTML]{FF0000} \textbf{0.2237}} & {\color[HTML]{FF0000} \textbf{0.2217}} \\ \midrule
                                       & 96        & 0.2166                                 & 0.2166                                 & 0.0963                                 & 0.0963                                 & {\color[HTML]{0000FF} {\ul 0.2153}}    & {\color[HTML]{FF0000} \textbf{0.2112}} & {\color[HTML]{0000FF} {\ul 0.0953}}    & {\color[HTML]{FF0000} \textbf{0.0913}} & {\color[HTML]{FF0000} \textbf{0.2134}} & 0.2131                                 & {\color[HTML]{FF0000} \textbf{0.0929}} & 0.0922                                 & 0.2363                                 & {\color[HTML]{0000FF} {\ul 0.2116}}    & 0.1074                                 & {\color[HTML]{0000FF} {\ul 0.0920}}     & 0.2158                                 & 0.2148                                 & 0.0954                                 & 0.0947                                 \\
                                       & 192       & 0.3385                                 & 0.337                                  & 0.2271                                 & 0.2249                                 & 0.308                                  & {\color[HTML]{0000FF} {\ul 0.3031}}    & 0.1888                                 & 0.183                                  & {\color[HTML]{FF0000} \textbf{0.3023}} & {\color[HTML]{FF0000} \textbf{0.3017}} & {\color[HTML]{FF0000} \textbf{0.1804}} & {\color[HTML]{FF0000} \textbf{0.18}}   & 0.3334                                 & 0.3138                                 & 0.2173                                 & 0.192                                  & {\color[HTML]{0000FF} {\ul 0.3043}}    & 0.3033                                 & {\color[HTML]{0000FF} {\ul 0.184}}     & {\color[HTML]{0000FF} {\ul 0.1828}}    \\
                                       & 336       & 1.2155                                 & 1.204                                  & 2.2101                                 & 2.1704                                 & 0.8419                                 & 0.8405                                 & 1.1522                                 & 1.1326                                 & {\color[HTML]{FF0000} \textbf{0.7177}} & {\color[HTML]{FF0000} \textbf{0.6595}} & {\color[HTML]{FF0000} \textbf{0.8852}} & {\color[HTML]{FF0000} \textbf{0.7692}} & 123.74                               & 1.0005                                 & 134815                            & 1.5782                                 & {\color[HTML]{0000FF} {\ul 0.7949}}    & {\color[HTML]{0000FF} {\ul 0.7489}}    & {\color[HTML]{0000FF} {\ul 1.0473}}    & {\color[HTML]{0000FF} {\ul 0.9525}}    \\
                                       & 720       & {\color[HTML]{FF0000} \textbf{0.5913}} & {\color[HTML]{FF0000} \textbf{0.5756}} & {\color[HTML]{FF0000} \textbf{0.5681}} & {\color[HTML]{FF0000} \textbf{0.5296}} & 0.9331                                 & {\color[HTML]{0000FF} {\ul 0.9137}}    & 1.5152                                 & 1.4605                                 & 0.9954                                 & 0.9834                                 & 1.6394                                 & 1.607                                  & 1.5125                                 & 1.4158                                 & 5.0646                                 & 3.7279                                 & {\color[HTML]{0000FF} {\ul 0.9313}}    & 0.9261                                 & {\color[HTML]{0000FF} {\ul 1.4625}}    & {\color[HTML]{0000FF} {\ul 1.437}}     \\ \cmidrule{2-22} 
\multirow{-5}{*}{Exchange}             & Average   & 0.5905                                 & 0.5833                                 & 0.7754                                 & 0.7553                                 & 0.5746                                 & 0.5671                                 & 0.7379                                 & 0.7169                                 & {\color[HTML]{FF0000} \textbf{0.5572}} & {\color[HTML]{FF0000} \textbf{0.5394}} & {\color[HTML]{0000FF} {\ul 0.6995}}    & {\color[HTML]{FF0000} \textbf{0.6621}} & 31.457                                & 0.7354                                 & 33705                             & 1.3975                                 & {\color[HTML]{0000FF} {\ul 0.5616}}    & {\color[HTML]{0000FF} {\ul 0.5483}}    & {\color[HTML]{FF0000} \textbf{0.6973}} & {\color[HTML]{0000FF} {\ul 0.6667}}    \\ \midrule
                                       & 96        & 0.9128                                 & 0.9106                                 & 2.7345                                 & 2.7281                                 & 0.7809                                 & 0.779                                  & 2.0489                                 & 2.0277                                 & {\color[HTML]{FF0000} \textbf{0.1654}} & {\color[HTML]{FF0000} \textbf{0.159}}  & {\color[HTML]{FF0000} \textbf{0.4718}} & {\color[HTML]{FF0000} \textbf{0.438}}  & 0.754                                  & 0.7213                                 & 2.0469                                 & 1.9562                                 & {\color[HTML]{0000FF} {\ul 0.3444}}    & {\color[HTML]{0000FF} {\ul 0.3064}}    & {\color[HTML]{0000FF} {\ul 1.2702}}    & {\color[HTML]{0000FF} {\ul 1.1502}}    \\
                                       & 192       & 1.1518                                 & 1.1498                                 & 4.5688                                 & 4.5506                                 & 0.9406                                 & 0.9345                                 & 3.3327                                 & 3.289                                  & {\color[HTML]{FF0000} \textbf{0.2719}} & {\color[HTML]{FF0000} \textbf{0.2382}} & {\color[HTML]{FF0000} \textbf{0.8407}} & {\color[HTML]{FF0000} \textbf{0.7093}} & 0.9481                                 & 0.9367                                 & 3.5042                                 & 3.4629                                 & {\color[HTML]{0000FF} {\ul 0.8772}}    & {\color[HTML]{0000FF} {\ul 0.8567}}    & {\color[HTML]{0000FF} {\ul 3.1613}}    & {\color[HTML]{0000FF} {\ul 3.1102}}    \\
                                       & 336       & 1.3276                                 & 1.3268                                 & 5.9655                                 & 5.9559                                 & {\color[HTML]{0000FF} {\ul 1.0953}}    & {\color[HTML]{0000FF} {\ul 1.0875}}    & {\color[HTML]{0000FF} {\ul 4.8131}}    & {\color[HTML]{0000FF} {\ul 4.7769}}    & {\color[HTML]{FF0000} \textbf{0.4366}} & {\color[HTML]{FF0000} \textbf{0.3848}} & {\color[HTML]{FF0000} \textbf{2.3733}} & {\color[HTML]{FF0000} \textbf{2.0705}} & 1.1189                                 & 1.1051                                 & 4.995                                  & 4.9205                                 & 1.1727                                 & 1.1459                                 & 5.1769                                 & 5.0606                                 \\
                                       & 720       & 1.3412                                 & 1.3410                                  & 5.1007                                 & 5.0979                                 & {\color[HTML]{0000FF} {\ul 1.2071}}    & {\color[HTML]{0000FF} {\ul 1.2005}}    & {\color[HTML]{0000FF} {\ul 4.9048}}    & {\color[HTML]{0000FF} {\ul 4.8849}}    & {\color[HTML]{FF0000} \textbf{0.66}}   & {\color[HTML]{FF0000} \textbf{0.5693}} & {\color[HTML]{FF0000} \textbf{3.0805}} & {\color[HTML]{FF0000} \textbf{2.7657}} & 1.2243                                 & 1.2203                                 & 4.9304                                 & 4.9106                                 & 1.2676                                 & 1.2643                                 & 4.9876                                 & 4.9838                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{MotorImagery}         & Average   & 1.1834                                 & 1.1821                                 & 4.5924                                 & 4.5831                                 & 1.006                                  & 1.0004                                 & 3.7749                                 & 3.7446                                 & {\color[HTML]{FF0000} \textbf{0.3835}} & {\color[HTML]{FF0000} \textbf{0.3378}} & {\color[HTML]{FF0000} \textbf{1.6916}} & {\color[HTML]{FF0000} \textbf{1.4959}} & 1.0113                                 & 0.9958                                 & 3.8691                                 & 3.8126                                 & {\color[HTML]{0000FF} {\ul 0.9155}}    & {\color[HTML]{0000FF} {\ul 0.8933}}    & {\color[HTML]{0000FF} {\ul 3.6490}}     & {\color[HTML]{0000FF} {\ul 3.5762}}    \\ \midrule
                                       & 96        & 0.6500                                   & 0.6499                                 & 0.7693                                 & 0.7692                                 & {\color[HTML]{0000FF} {\ul 0.5958}}    & 0.5951                                 & {\color[HTML]{0000FF} {\ul 0.6641}}    & {\color[HTML]{0000FF} {\ul 0.662}}     & 0.5979                                 & 0.5966                                 & 0.6726                                 & 0.6704                                 & 0.5959                                 & {\color[HTML]{0000FF} {\ul 0.5916}}    & 0.673                                  & 0.6636                                 & {\color[HTML]{FF0000} \textbf{0.5911}} & {\color[HTML]{FF0000} \textbf{0.5906}} & {\color[HTML]{FF0000} \textbf{0.6574}} & {\color[HTML]{FF0000} \textbf{0.6566}} \\
                                       & 192       & 0.7463                                 & 0.7463                                 & 1.0022                                 & 1.0021                                 & 0.6695                                 & 0.6689                                 & 0.8348                                 & 0.8335                                 & {\color[HTML]{0000FF} {\ul 0.6671}}    & {\color[HTML]{0000FF} {\ul 0.6653}}    & {\color[HTML]{0000FF} {\ul 0.8314}}    & {\color[HTML]{0000FF} {\ul 0.8277}}    & 0.6688                                 & 0.6657                                 & 0.8374                                 & 0.8296                                 & {\color[HTML]{FF0000} \textbf{0.6633}} & {\color[HTML]{FF0000} \textbf{0.6626}} & {\color[HTML]{FF0000} \textbf{0.824}}  & {\color[HTML]{FF0000} \textbf{0.8224}} \\
                                       & 336       & 0.8460                                  & 0.8460                                  & 1.2863                                 & 1.2862                                 & 0.7543                                 & 0.7467                                 & 1.066                                  & 1.0467                                 & {\color[HTML]{0000FF} {\ul 0.7488}}    & {\color[HTML]{0000FF} {\ul 0.7451}}    & {\color[HTML]{0000FF} {\ul 1.0531}}    & {\color[HTML]{0000FF} {\ul 1.0438}}    & 0.7498                                 & 0.7469                                 & 1.0568                                 & 1.0493                                 & {\color[HTML]{FF0000} \textbf{0.7482}} & {\color[HTML]{FF0000} \textbf{0.743}}  & {\color[HTML]{FF0000} \textbf{1.0505}} & {\color[HTML]{FF0000} \textbf{1.0373}} \\
                                       & 720       & 0.9643                                 & 0.9633                                 & 1.5473                                 & 1.5443                                 & 0.8798                                 & 0.8786                                 & 1.3633                                 & 1.3606                                 & {\color[HTML]{0000FF} {\ul 0.8757}}    & {\color[HTML]{0000FF} {\ul 0.8753}}    & {\color[HTML]{0000FF} {\ul 1.3561}}    & 1.3551                                 & 0.8767                                 & 0.8755                                 & 1.3575                                 & {\color[HTML]{0000FF} {\ul 1.3545}}    & {\color[HTML]{FF0000} \textbf{0.8722}} & {\color[HTML]{FF0000} \textbf{0.871}}  & {\color[HTML]{FF0000} \textbf{1.3479}} & {\color[HTML]{FF0000} \textbf{1.3441}} \\ \cmidrule{2-22} 
\multirow{-5}{*}{TDBrain}              & Average   & 0.8016                                 & 0.8014                                 & 1.1513                                 & 1.1505                                 & 0.7248                                 & 0.7223                                 & 0.9821                                 & 0.9757                                 & {\color[HTML]{0000FF} {\ul 0.7224}}    & 0.7206                                 & {\color[HTML]{0000FF} {\ul 0.9783}}    & {\color[HTML]{0000FF} {\ul 0.9742}}    & 0.7228                                 & {\color[HTML]{0000FF} {\ul 0.7199}}    & 0.9812                                 & {\color[HTML]{0000FF} {\ul 0.9742}}    & {\color[HTML]{FF0000} \textbf{0.7187}} & {\color[HTML]{FF0000} \textbf{0.7168}} & {\color[HTML]{FF0000} \textbf{0.97}}   & {\color[HTML]{FF0000} \textbf{0.9651}} \\ \midrule
                                       & 96        & {\color[HTML]{0000FF} {\ul 0.4409}}    & 0.4399                                 & {\color[HTML]{0000FF} {\ul 0.5290}}     & 0.5279                                 & 0.7806                                 & {\color[HTML]{FF0000} \textbf{0.428}}  & 1.6671                                 & {\color[HTML]{0000FF} {\ul 0.5181}}    & 0.4415                                 & 0.4326                                 & 0.5399                                 & {\color[HTML]{FF0000} \textbf{0.5145}} & 0.4516                                 & 0.4411                                 & 0.5515                                 & 0.5275                                 & {\color[HTML]{FF0000} \textbf{0.4321}} & {\color[HTML]{0000FF} {\ul 0.4300}}      & {\color[HTML]{FF0000} \textbf{0.5261}} & 0.5219                                 \\
                                       & 192       & 0.4635                                 & 0.4634                                 & {\color[HTML]{FF0000} \textbf{0.5690}}  & 0.5687                                 & {\color[HTML]{0000FF} {\ul 0.4543}}    & {\color[HTML]{0000FF} {\ul 0.4539}}    & 0.5723                                 & 0.5716                                 & 0.4584                                 & 0.4578                                 & {\color[HTML]{0000FF} {\ul 0.5710}}     & {\color[HTML]{0000FF} {\ul 0.5677}}    & 0.4717                                 & 0.4674                                 & 0.5955                                 & 0.5763                                 & {\color[HTML]{FF0000} \textbf{0.4538}} & {\color[HTML]{FF0000} \textbf{0.4483}} & 0.5737                                 & {\color[HTML]{FF0000} \textbf{0.5627}} \\
                                       & 336       & 0.4732                                 & 0.4731                                 & 0.5912                                 & 0.5910                                  & {\color[HTML]{0000FF} {\ul 0.466}}     & {\color[HTML]{0000FF} {\ul 0.466}}     & 0.5908                                 & 0.5905                                 & 0.4682                                 & 0.4673                                 & 0.5948                                 & 0.5933                                 & 0.4667                                 & 0.4664                                 & {\color[HTML]{0000FF} {\ul 0.5889}}    & {\color[HTML]{0000FF} {\ul 0.5884}}    & {\color[HTML]{FF0000} \textbf{0.4652}} & {\color[HTML]{FF0000} \textbf{0.4639}} & {\color[HTML]{FF0000} \textbf{0.5881}} & {\color[HTML]{FF0000} \textbf{0.5862}} \\
                                       & 720       & 0.5088                                 & 0.5068                                 & 0.6426                                 & 0.6415                                 & {\color[HTML]{FF0000} \textbf{0.4879}} & {\color[HTML]{FF0000} \textbf{0.4858}} & {\color[HTML]{0000FF} {\ul 0.6272}}    & {\color[HTML]{0000FF} {\ul 0.6231}}    & {\color[HTML]{0000FF} {\ul 0.4883}}    & {\color[HTML]{0000FF} {\ul 0.487}}     & {\color[HTML]{FF0000} \textbf{0.6147}} & {\color[HTML]{FF0000} \textbf{0.6123}} & 0.4948                                 & 0.4931                                 & 0.6309                                 & 0.6292                                 & 0.4961                                 & 0.4944                                 & 0.6393                                 & 0.6354                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{BeijingAir}           & Average   & 0.4716                                 & 0.4708                                 & 0.5829                                 & 0.5823                                 & 0.5472                                 & {\color[HTML]{FF0000} \textbf{0.4584}} & 0.8643                                 & {\color[HTML]{0000FF} {\ul 0.5758}}    & {\color[HTML]{0000FF} {\ul 0.4641}}    & 0.4612                                 & {\color[HTML]{FF0000} \textbf{0.5801}} & {\color[HTML]{FF0000} \textbf{0.5719}} & 0.4712                                 & 0.467                                  & 0.5917                                 & 0.5804                                 & {\color[HTML]{FF0000} \textbf{0.4618}} & {\color[HTML]{0000FF} {\ul 0.4591}}    & {\color[HTML]{0000FF} {\ul 0.5818}}    & 0.5765                                 \\ \midrule
                                       & 96        & {\color[HTML]{FF0000} \textbf{0.0162}} & {\color[HTML]{0000FF} {\ul 0.0144}}    & {\color[HTML]{0000FF} {\ul 0.0067}}    & {\color[HTML]{0000FF} {\ul 0.0065}}    & 0.0407                                 & 0.0393                                 & 0.0095                                 & 0.0093                                 & 0.0582                                 & 0.0482                                 & 0.0105                                 & 0.0088                                 & {\color[HTML]{0000FF} {\ul 0.0163}}    & {\color[HTML]{FF0000} \textbf{0.0132}} & {\color[HTML]{FF0000} \textbf{0.0059}} & {\color[HTML]{FF0000} \textbf{0.0059}} & 0.3782                                 & 0.0625                                 & 0.3288                                 & 0.0122                                 \\
                                       & 192       & {\color[HTML]{FF0000} \textbf{0.0148}} & {\color[HTML]{FF0000} \textbf{0.0134}} & {\color[HTML]{0000FF} {\ul 0.0066}}    & {\color[HTML]{0000FF} {\ul 0.0065}}    & 0.027                                  & 0.0237                                 & 0.0097                                 & 0.0095                                 & 0.0367                                 & 0.0354                                 & 0.0077                                 & 0.0076                                 & {\color[HTML]{0000FF} {\ul 0.0185}}    & {\color[HTML]{0000FF} {\ul 0.0164}}    & {\color[HTML]{FF0000} \textbf{0.0063}} & {\color[HTML]{FF0000} \textbf{0.0062}} & 0.0374                                 & 0.0329                                 & 0.0103                                 & 0.0101                                 \\
                                       & 336       & {\color[HTML]{FF0000} \textbf{0.0192}} & {\color[HTML]{FF0000} \textbf{0.0173}} & {\color[HTML]{0000FF} {\ul 0.0082}}    & {\color[HTML]{0000FF} {\ul 0.008}}     & 0.0415                                 & 0.0396                                 & 0.0094                                 & 0.0093                                 & 0.0586                                 & 0.0551                                 & 0.0133                                 & 0.0126                                 & {\color[HTML]{0000FF} {\ul 0.0285}}    & {\color[HTML]{0000FF} {\ul 0.0196}}    & {\color[HTML]{FF0000} \textbf{0.0079}} & {\color[HTML]{FF0000} \textbf{0.0075}} & 0.0321                                 & 0.03                                   & 0.0114                                 & 0.0113                                 \\
                                       & 720       & {\color[HTML]{0000FF} {\ul 0.0282}}    & {\color[HTML]{0000FF} {\ul 0.026}}     & {\color[HTML]{FF0000} \textbf{0.0115}} & {\color[HTML]{FF0000} \textbf{0.0114}} & 0.0583                                 & 0.0504                                 & 0.0157                                 & 0.0147                                 & 0.0581                                 & 0.0529                                 & 0.0153                                 & 0.0143                                 & {\color[HTML]{FF0000} \textbf{0.0266}} & {\color[HTML]{FF0000} \textbf{0.0258}} & {\color[HTML]{0000FF} {\ul 0.0126}}    & {\color[HTML]{0000FF} {\ul 0.0125}}    & 0.0376                                 & 0.0279                                 & 0.0156                                 & 0.0141                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{BenzeneConcentration} & Average   & {\color[HTML]{FF0000} \textbf{0.0196}} & {\color[HTML]{FF0000} \textbf{0.0178}} & {\color[HTML]{FF0000} \textbf{0.0082}} & {\color[HTML]{0000FF} {\ul 0.0081}}    & 0.0419                                 & 0.0383                                 & {\color[HTML]{0000FF} {\ul 0.0111}}    & 0.0107                                 & 0.0529                                 & 0.0479                                 & 0.0117                                 & 0.0108                                 & {\color[HTML]{0000FF} {\ul 0.0225}}    & {\color[HTML]{0000FF} {\ul 0.0188}}    & {\color[HTML]{FF0000} \textbf{0.0082}} & {\color[HTML]{FF0000} \textbf{0.008}}  & 0.1213                                 & 0.0383                                 & 0.0915                                 & 0.0119                                 \\ \midrule
                                       & 96        & 0.7298                                 & 0.7288                                 & {\color[HTML]{FF0000} \textbf{0.8063}} & {\color[HTML]{FF0000} \textbf{0.805}}  & 0.7317                                 & 0.7309                                 & 0.8154                                 & 0.8139                                 & {\color[HTML]{FF0000} \textbf{0.7271}} & {\color[HTML]{0000FF} {\ul 0.7265}}    & 0.809                                  & 0.8084                                 & 0.7286                                 & 0.7276                                 & 0.8135                                 & 0.8105                                 & {\color[HTML]{0000FF} {\ul 0.7277}}    & {\color[HTML]{FF0000} \textbf{0.7263}} & {\color[HTML]{0000FF} {\ul 0.8075}}    & {\color[HTML]{0000FF} {\ul 0.8061}}    \\
                                       & 192       & {\color[HTML]{FF0000} \textbf{0.7508}} & {\color[HTML]{FF0000} \textbf{0.7505}} & {\color[HTML]{FF0000} \textbf{0.8388}} & {\color[HTML]{FF0000} \textbf{0.8387}} & 0.7583                                 & 0.7562                                 & 0.8623                                 & 0.8554                                 & 0.7548                                 & 0.7536                                 & 0.8507                                 & 0.8489                                 & 0.7555                                 & 0.7543                                 & 0.8546                                 & 0.8516                                 & {\color[HTML]{0000FF} {\ul 0.7527}}    & {\color[HTML]{0000FF} {\ul 0.7524}}    & {\color[HTML]{0000FF} {\ul 0.8475}}    & {\color[HTML]{0000FF} {\ul 0.8469}}    \\
                                       & 336       & {\color[HTML]{FF0000} \textbf{0.7600}}   & {\color[HTML]{FF0000} \textbf{0.7592}} & {\color[HTML]{FF0000} \textbf{0.8502}} & {\color[HTML]{FF0000} \textbf{0.8495}} & 0.7641                                 & 0.7635                                 & 0.8644                                 & 0.8635                                 & 0.7631                                 & 0.7622                                 & 0.8633                                 & 0.8616                                 & 0.7647                                 & 0.7639                                 & 0.8678                                 & 0.8653                                 & {\color[HTML]{0000FF} {\ul 0.762}}     & {\color[HTML]{0000FF} {\ul 0.7614}}    & {\color[HTML]{0000FF} {\ul 0.8614}}    & {\color[HTML]{0000FF} {\ul 0.8593}}    \\
                                       & 720       & {\color[HTML]{FF0000} \textbf{0.7645}} & {\color[HTML]{FF0000} \textbf{0.7642}} & {\color[HTML]{FF0000} \textbf{0.8576}} & {\color[HTML]{FF0000} \textbf{0.8573}} & 0.7721                                 & 0.7717                                 & 0.8773                                 & 0.8769                                 & 0.7701                                 & 0.7699                                 & 0.8743                                 & 0.8739                                 & 0.7707                                 & 0.7699                                 & 0.8761                                 & 0.8737                                 & {\color[HTML]{0000FF} {\ul 0.7689}}    & {\color[HTML]{0000FF} {\ul 0.7688}}    & {\color[HTML]{0000FF} {\ul 0.8709}}    & {\color[HTML]{0000FF} {\ul 0.8707}}    \\ \cmidrule{2-22} 
\multirow{-5}{*}{AustraliaRainfall}    & Average   & {\color[HTML]{FF0000} \textbf{0.7513}} & {\color[HTML]{FF0000} \textbf{0.7507}} & {\color[HTML]{FF0000} \textbf{0.8382}} & {\color[HTML]{FF0000} \textbf{0.8376}} & 0.7566                                 & 0.7556                                 & 0.8548                                 & 0.8524                                 & 0.7537                                 & 0.7531                                 & 0.8493                                 & 0.8482                                 & 0.7548                                 & 0.7539                                 & 0.853                                  & 0.8503                                 & {\color[HTML]{0000FF} {\ul 0.7528}}    & {\color[HTML]{0000FF} {\ul 0.7522}}    & {\color[HTML]{0000FF} {\ul 0.8468}}    & {\color[HTML]{0000FF} {\ul 0.8457}}    \\ \midrule
                                       & 96        & {\color[HTML]{FF0000} \textbf{0.6273}} & {\color[HTML]{FF0000} \textbf{0.6265}} & {\color[HTML]{FF0000} \textbf{1.0863}} & {\color[HTML]{FF0000} \textbf{1.0816}} & 0.6524                                 & 0.6472                                 & 1.1694                                 & 1.1398                                 & 0.6547                                 & 0.6537                                 & 1.1861                                 & 1.1833                                 & {\color[HTML]{0000FF} {\ul 0.6301}}    & {\color[HTML]{0000FF} {\ul 0.627}}     & {\color[HTML]{0000FF} {\ul 1.1202}}    & 1.1126                                 & 0.6558                                 & 0.645                                  & 1.1455                                 & {\color[HTML]{0000FF} {\ul 1.1106}}    \\
                                       & 192       & {\color[HTML]{FF0000} \textbf{0.6266}} & {\color[HTML]{FF0000} \textbf{0.6263}} & {\color[HTML]{FF0000} \textbf{0.9782}} & {\color[HTML]{FF0000} \textbf{0.9778}} & 0.6452                                 & 0.6452                                 & 1.0903                                 & 1.0900                                   & 0.6510                                  & 0.6462                                 & 1.0941                                 & 1.0868                                 & 0.6407                                 & 0.6361                                 & 1.0484                                 & 1.0437                                 & {\color[HTML]{0000FF} {\ul 0.6289}}    & {\color[HTML]{0000FF} {\ul 0.6281}}    & {\color[HTML]{0000FF} {\ul 1.0342}}    & {\color[HTML]{0000FF} {\ul 1.0340}}     \\
                                       & 336       & 0.6482                                 & 0.6409                                 & 1.0208                                 & {\color[HTML]{0000FF} {\ul 1.0060}}     & 0.6576                                 & 0.6538                                 & 1.0924                                 & 1.0847                                 & 0.6533                                 & 0.6495                                 & 1.0649                                 & 1.0587                                 & {\color[HTML]{FF0000} \textbf{0.6344}} & {\color[HTML]{FF0000} \textbf{0.6286}} & {\color[HTML]{0000FF} {\ul 1.0111}}    & 1.0081                                 & {\color[HTML]{0000FF} {\ul 0.6377}}    & {\color[HTML]{0000FF} {\ul 0.6303}}    & {\color[HTML]{FF0000} \textbf{1.0101}} & {\color[HTML]{FF0000} \textbf{0.9986}} \\
                                       & 720       & {\color[HTML]{FF0000} \textbf{0.6185}} & {\color[HTML]{FF0000} \textbf{0.6159}} & {\color[HTML]{FF0000} \textbf{0.9014}} & {\color[HTML]{FF0000} \textbf{0.8894}} & 0.6342                                 & 0.6308                                 & 0.9905                                 & 0.9885                                 & 0.6350                                  & 0.6348                                 & 1.0076                                 & 1.0067                                 & {\color[HTML]{0000FF} {\ul 0.6188}}    & {\color[HTML]{0000FF} {\ul 0.6174}}    & {\color[HTML]{0000FF} {\ul 0.9589}}    & {\color[HTML]{0000FF} {\ul 0.9563}}    & 0.6359                                 & 0.6286                                 & 0.9901                                 & 0.9754                                 \\ \cmidrule{2-22} 
\multirow{-5}{*}{KDDCup2018}           & Average   & {\color[HTML]{FF0000} \textbf{0.6302}} & {\color[HTML]{0000FF} {\ul 0.6274}}    & {\color[HTML]{FF0000} \textbf{0.9967}} & {\color[HTML]{FF0000} \textbf{0.9887}} & 0.6474                                 & 0.6442                                 & 1.0856                                 & 1.0757                                 & 0.6485                                 & 0.6461                                 & 1.0882                                 & 1.0839                                 & {\color[HTML]{0000FF} {\ul 0.631}}     & {\color[HTML]{FF0000} \textbf{0.6273}} & {\color[HTML]{0000FF} {\ul 1.0346}}    & 1.0302                                 & 0.6396                                 & 0.6330                                  & 1.0450                                  & {\color[HTML]{0000FF} {\ul 1.0297}}    \\ \midrule
                                       & 96        & 0.2578                                 & 0.2572                                 & 0.2388                                 & 0.2383                                 & 0.2541                                 & 0.2519                                 & {\color[HTML]{0000FF} {\ul 0.2203}}    & {\color[HTML]{0000FF} {\ul 0.2182}}    & {\color[HTML]{0000FF} {\ul 0.2463}}    & {\color[HTML]{0000FF} {\ul 0.2415}}    & 0.2258                                 & 0.2199                                 & 0.2482                                 & {\color[HTML]{FF0000} \textbf{0.2391}} & 0.2246                                 & 0.2188                                 & {\color[HTML]{FF0000} \textbf{0.2449}} & {\color[HTML]{0000FF} {\ul 0.2415}}    & {\color[HTML]{FF0000} \textbf{0.219}}  & {\color[HTML]{FF0000} \textbf{0.2157}} \\
                                       & 192       & 0.2720                                  & 0.2715                                 & 0.2659                                 & 0.2654                                 & 0.2742                                 & 0.2731                                 & {\color[HTML]{0000FF} {\ul 0.2565}}    & 0.2555                                 & {\color[HTML]{FF0000} \textbf{0.2553}} & {\color[HTML]{FF0000} \textbf{0.2545}} & {\color[HTML]{FF0000} \textbf{0.255}}  & {\color[HTML]{FF0000} \textbf{0.2529}} & {\color[HTML]{0000FF} {\ul 0.2673}}    & {\color[HTML]{0000FF} {\ul 0.2656}}    & 0.2586                                 & {\color[HTML]{0000FF} {\ul 0.2547}}    & 0.3674                                 & 0.3308                                 & 0.3771                                 & 0.3223                                 \\
                                       & 336       & 0.2950                                  & 0.2945                                 & 0.3072                                 & 0.3068                                 & 0.3026                                 & 0.3008                                 & 0.3046                                 & 0.3026                                 & 0.3057                                 & 0.3023                                 & 0.3122                                 & 0.3086                                 & {\color[HTML]{FF0000} \textbf{0.2844}} & {\color[HTML]{0000FF} {\ul 0.2827}}    & {\color[HTML]{0000FF} {\ul 0.2973}}    & {\color[HTML]{0000FF} {\ul 0.2945}}    & {\color[HTML]{0000FF} {\ul 0.2871}}    & {\color[HTML]{FF0000} \textbf{0.2825}} & {\color[HTML]{FF0000} \textbf{0.2886}} & {\color[HTML]{FF0000} \textbf{0.2839}} \\
                                       & 720       & {\color[HTML]{0000FF} {\ul 0.3322}}    & 0.3318                                 & {\color[HTML]{0000FF} {\ul 0.3815}}    & 0.3812                                 & 0.3407                                 & 0.3388                                 & 0.3844                                 & 0.3778                                 & 0.3341                                 & 0.3324                                 & 0.3859                                 & 0.383                                  & 0.3381                                 & {\color[HTML]{0000FF} {\ul 0.328}}     & 0.3888                                 & {\color[HTML]{0000FF} {\ul 0.3755}}    & {\color[HTML]{FF0000} \textbf{0.3271}} & {\color[HTML]{FF0000} \textbf{0.3245}} & {\color[HTML]{FF0000} \textbf{0.3601}} & {\color[HTML]{FF0000} \textbf{0.3585}} \\ \cmidrule{2-22} 
\multirow{-5}{*}{PedestrianCounts}     & Average   & 0.2892                                 & 0.2888                                 & 0.2983                                 & 0.2979                                 & 0.2929                                 & 0.2912                                 & {\color[HTML]{FF0000} \textbf{0.2915}} & {\color[HTML]{0000FF} {\ul 0.2885}}    & {\color[HTML]{0000FF} {\ul 0.2854}}    & {\color[HTML]{0000FF} {\ul 0.2827}}    & 0.2947                                 & 0.2911                                 & {\color[HTML]{FF0000} \textbf{0.2845}} & {\color[HTML]{FF0000} \textbf{0.2789}} & {\color[HTML]{0000FF} {\ul 0.2923}}    & {\color[HTML]{FF0000} \textbf{0.2859}} & 0.3066                                 & 0.2948                                 & 0.3112                                 & 0.2951                                 \\ \midrule
\multicolumn{1}{c|}{Average} & \multicolumn{1}{c|}{Average}              & 0.4949                                 & 0.4936                                 & 0.7973                                 & \multicolumn{1}{c|}{0.7942}            & 0.4756                                 & 0.4665                                 & 0.7382                                 & \multicolumn{1}{c|}{0.7111}                               & {\color[HTML]{FF0000} \textbf{0.4265}} & {\color[HTML]{FF0000} \textbf{0.4197}} & {\color[HTML]{FF0000} \textbf{0.5696}} & \multicolumn{1}{c|}{{\color[HTML]{FF0000} \textbf{0.5501}}} & 2.6748                                  & 0.4757                                  & 2408.1878                               & \multicolumn{1}{c|}{0.7633}                            & {\color[HTML]{0000FF} {\ul 0.4698}}    & {\color[HTML]{0000FF} {\ul 0.4582}}    & {\color[HTML]{0000FF} {\ul 0.712}}     & {\color[HTML]{0000FF} {\ul 0.6942}}                                   \\ \midrule
\multicolumn{1}{c|}{Rank}    & \multicolumn{1}{c|}{Average}              & 3.07                                   & 3.29                                   & 3.21                                   & \multicolumn{1}{c|}{3.43}              & 3.14                                   & 3.07                                   & {\color[HTML]{FF0000} \textbf{2.93}}   & \multicolumn{1}{c|}{{\color[HTML]{FF0000} \textbf{2.71}}} & {\color[HTML]{FF0000} \textbf{2.86}}   & 3.07                                   & {\color[HTML]{FF0000} \textbf{2.93}}   & \multicolumn{1}{c|}{{\color[HTML]{0000FF} {\ul 2.93}}}      & {\color[HTML]{0000FF} {\ul 2.93}}      & {\color[HTML]{FF0000} \textbf{2.64}}   & {\color[HTML]{0000FF} {\ul 3.00}}         & \multicolumn{1}{c|}{{\color[HTML]{0000FF} {\ul 2.93}}} & 3.00                                      & {\color[HTML]{0000FF} {\ul 2.93}}      & {\color[HTML]{FF0000} \textbf{2.93}}   & 3.00                                      \\ \bottomrule
\end{tabular}

}

\label{tab:full_results}
\end{table*}

\begin{table*}[]
\caption{\textbf{Full results iPatch.} We present TSLib (left) and UTSD datasets (right). See \Cref{tab:ipatch_champ} for average MSE, MAE, and Rank.
}
\centering
\resizebox{\textwidth}{!}{

\begin{tabular}{c|c|cccc|c|c|cccc}
\toprule
\multirow{3}{*}{Dataset}     & Model     & \multicolumn{4}{c|}{iPatch}                                    & \multirow{3}{*}{Dataset}              & Model     & \multicolumn{4}{c}{iPatch}                        \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Metric    & \multicolumn{2}{c}{MAE} & \multicolumn{2}{c|}{MSE}             &                                       & Metric    & \multicolumn{2}{c}{MAE} & \multicolumn{2}{c}{MSE} \\ \cmidrule(lr){2-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6}\cmidrule(lr){8-8}\cmidrule(lr){9-10} \cmidrule(lr){11-12} 
                             & Statistic & Mean       & Min        & Mean                        & Min    &                                       & Statistic & Mean       & Min        & Mean       & Min        \\ \midrule
\multirow{5}{*}{ETTh1}       & 96        & 0.4074     & 0.3995     & 0.3877                      & 0.379  & \multirow{5}{*}{MotorImagery}         & 96        & 0.1433     & 0.1287     & 0.4238     & 0.3541     \\
                             & 192       & 0.4241     & 0.4221     & 0.415                       & 0.413  &                                       & 192       & 0.2633     & 0.2079     & 1.0616     & 0.7352     \\
                             & 336       & 0.4301     & 0.4284     & 0.4241                      & 0.4198 &                                       & 336       & 0.3999     & 0.3458     & 2.183      & 1.8659     \\
                             & 720       & 0.4635     & 0.4599     & 0.45                        & 0.4451 &                                       & 720       & 1.1959     & 1.1879     & 4.8701     & 4.8506     \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.4313     & 0.4275     & 0.4192                      & 0.4142 &                                       & Average   & 0.5006     & 0.4676     & 2.1346     & 1.9515     \\ \midrule
\multirow{5}{*}{ETTm1}       & 96        & 0.383      & 0.3726     & 0.3229                      & 0.3115 & \multirow{5}{*}{TDBrain}              & 96        & 0.5985     & 0.5909     & 0.6747     & 0.6592     \\
                             & 192       & 0.418      & 0.4171     & 0.3749                      & 0.3719 &                                       & 192       & 0.6704     & 0.665      & 0.8383     & 0.824      \\
                             & 336       & 0.4478     & 0.446      & 0.4289                      & 0.4259 &                                       & 336       & 0.7483     & 0.7423     & 1.0502     & 1.036      \\
                             & 720       & 0.4963     & 0.4919     & 0.4937                      & 0.4862 &                                       & 720       & 0.871      & 0.8704     & 1.3459     & 1.3451     \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.4363     & 0.4319     & 0.4051                      & 0.3989 &                                       & Average   & 0.722      & 0.7171     & 0.9773     & 0.9661     \\ \midrule
\multirow{5}{*}{ETTh2}       & 96        & 0.3445     & 0.3423     & 0.2935                      & 0.2902 & \multirow{5}{*}{BeijingAir}           & 96        & 0.4345     & 0.4302     & 0.5334     & 0.5231     \\
                             & 192       & 0.3953     & 0.3931     & 0.3759                      & 0.3711 &                                       & 192       & 0.4559     & 0.455      & 0.569      & 0.5643     \\
                             & 336       & 0.4337     & 0.4323     & 0.4226                      & 0.4205 &                                       & 336       & 0.4665     & 0.4658     & 0.5962     & 0.5941     \\
                             & 720       & 0.4443     & 0.4419     & 0.4197                      & 0.4172 &                                       & 720       & 0.4721     & 0.4685     & 0.5909     & 0.5843     \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.4045     & 0.4024     & 0.3779                      & 0.3747 &                                       & Average   & 0.4573     & 0.4549     & 0.5724     & 0.5665     \\ \midrule
\multirow{5}{*}{ETTm2}       & 96        & 0.2282     & 0.2281     & 0.1145                      & 0.1134 & \multirow{5}{*}{BenzeneConcentration} & 96        & 0.0197     & 0.0192     & 0.0059     & 0.0059     \\
                             & 192       & 0.2595     & 0.2593     & 0.146                       & 0.1457 &                                       & 192       & 0.0247     & 0.0244     & 0.0112     & 0.0111     \\
                             & 336       & 0.2865     & 0.285      & 0.18                        & 0.1786 &                                       & 336       & 0.0188     & 0.0166     & 0.0077     & 0.0076     \\
                             & 720       & 0.3181     & 0.3148     & 0.214                       & 0.2099 &                                       & 720       & 0.0315     & 0.0302     & 0.0152     & 0.0149     \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.2731     & 0.2718     & \multicolumn{1}{c|}{0.1636} & 0.1619 &                                       & Average   & 0.0237     & 0.0226     & 0.01       & 0.0098     \\ \midrule
\multirow{5}{*}{Electricity} & 96        & 0.2315     & 0.2304     & 0.1333                      & 0.133  & \multirow{5}{*}{AustraliaRainfall}    & 96        & 0.7284     & 0.7274     & 0.8115     & 0.8093     \\
                             & 192       & 0.2471     & 0.2456     & 0.1511                      & 0.15   &                                       & 192       & 0.7533     & 0.7524     & 0.85       & 0.8484     \\
                             & 336       & 0.2808     & 0.2782     & 0.1798                      & 0.1769 &                                       & 336       & 0.7623     & 0.7608     & 0.8624     & 0.8583     \\
                             & 720       & 0.3196     & 0.3159     & 0.2295                      & 0.2268 &                                       & 720       & 0.7691     & 0.7683     & 0.8719     & 0.8702     \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.2698     & 0.2675     & 0.1734                      & 0.1717 &                                       & Average   & 0.7533     & 0.7522     & 0.8489     & 0.8466     \\ \midrule
\multirow{5}{*}{Weather}     & 96        & 0.204      & 0.2031     & 0.1532                      & 0.1525 & \multirow{5}{*}{KDDCup2018}           & 96        & 0.6556     & 0.6539     & 1.1853     & 1.1828     \\
                             & 192       & 0.2501     & 0.2494     & 0.2032                      & 0.2016 &                                       & 192       & 0.6455     & 0.6454     & 1.0951     & 1.0948     \\
                             & 336       & 0.2879     & 0.2878     & 0.2516                      & 0.2511 &                                       & 336       & 0.6397     & 0.6366     & 1.0313     & 1.0267     \\
                             & 720       & 0.3348     & 0.3337     & 0.3202                      & 0.3192 &                                       & 720       & 0.6504     & 0.6478     & 1.0057     & 0.9874     \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.2692     & 0.2685     & 0.232                       & 0.2311 &                                       & Average   & 0.6478     & 0.6459     & 1.0793     & 1.0729     \\ \midrule
\multirow{5}{*}{Exchange}    & 96        & 0.2075     & 0.2041     & 0.0877                      & 0.0849 & \multirow{5}{*}{PedestrianCounts}     & 96        & 0.2316     & 0.2309     & 0.2139     & 0.213      \\
                             & 192       & 0.3101     & 0.3014     & 0.1891                      & 0.1805 &                                       & 192       & 0.2628     & 0.2498     & 0.2564     & 0.2475     \\
                             & 336       & 0.7651     & 0.7414     & 1.0303                      & 0.9813 &                                       & 336       & 0.2797     & 0.274      & 0.2939     & 0.2894     \\
                             & 720       & 1.0439     & 1.0229     & 1.8326                      & 1.7492 &                                       & 720       & 0.3196     & 0.318      & 0.3726     & 0.372      \\ \cmidrule{2-6} \cmidrule{8-12} 
                             & Average   & 0.5817     & 0.5674     & 0.7849                      & 0.749  &                                       & Average   & 0.2734     & 0.2682     & 0.2842     & 0.2805    \\
\bottomrule
\end{tabular}

}

\label{tab:ipatch_results}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
