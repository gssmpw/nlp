\section{Related Work}
We review the topics of empirical observations and theoretical studies of memorization.

\textbf{Empirical observations of memorization.}
A lot of recent empirical studies have shown that memorization inevitably happens in modern over-parameterized neural networks.
For instance, \citet{feldman2020neural,garg2023memorization} studied the memorization of training examples and found that neural networks tend to memorize visually atypical examples, i.e., those are rich in data-specific noise. 
These observations motivate us to study the impact of data-specific information on benign overfitting.
In this paper, our results provide a theoretical justification for these empirical observations in neural networks. 

\textbf{Theoretical analyses for memorization.}
A body of work theoretically examined memorization within classical machine learning frameworks, demonstrating its significance for achieving near-optimal generalization performance across various contexts, including high-dimensional linear regression \citep{cheng2022memorize}, prediction models \citep{brown2021memorization}, and generalization on discrete distributions and mixture models \citep{feldman2020does}.
This line of research failed to explain learning dynamics and generalization performance in non-convex and non-smooth neural networks.

Another line of work theoretically studied memorization in neural networks by analyzing the feature learning process during training, providing analytical frameworks that extend beyond the \ac{ntk} regime \citep{jacot2018neural,allen2019convergence,du2018gradient}.
For example, studies (e.g. \cite{cao2022benign} and \cite{kou2023benign}) explored benign overfitting in two-layer neural networks using the feature-noise model with homogeneous data noise distributions and showed a sharp phase transition between benign and harmful overfitting.
However, their results all show that it is harmful to memorize data-specific information and thus fail to explain the empirical observations with long-tailed data.