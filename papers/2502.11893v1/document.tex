\documentclass{article} 
\usepackage{times}

\usepackage{natbib}

\usepackage[T1]{fontenc}

\usepackage{acronym}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{booktabs} 
\usepackage{CJK}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{color}
\usepackage{enumitem}
\usepackage{float}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{svg}
\usepackage{tabularx}
\usepackage{url}
\usepackage{wrapfig}
\usepackage[accepted]{icml2025}

\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{condition}{Condition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}


\acrodef{cnn}[CNN]{convolutional neural network}
\acrodef{gd}[GD]{gradient descent}
\acrodef{iid}[IID]{independent and identically distributed}
\acrodef{mad}[MAD]{mean absolute derivation}
\acrodef{non-iid}[non-IID]{not independent and identically distributed}
\acrodef{ntk}[NTK]{neural tangent kernel}
\acrodef{sgd}[SGD]{stochastic gradient descent}
\acrodef{cnn}[CNN]{convolutional neural network}

\title{Rethinking Benign Overfitting of Long-Tailed Data Classification in Two-layer Neural Networks
}

\author{Ruichen~Xu\\
\texttt{rx10@rice.edu}\\
\And Kexin~Chen\\
\texttt{kxchen9819@gmail.com}
}


\let\oldfrac\frac
\renewcommand{\frac}[2]{
	\mathchoice
	{\oldfrac{#1}{#2}}
	{#1/#2}
	{#1/#2}
	{#1/#2}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\begin{document}

\twocolumn[
\icmltitle{Rethinking Benign Overfitting in Two-Layer Neural Networks
	}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
	\icmlauthor{Ruichen Xu\quad}{}%{yyy}
	\icmlauthor{\quad Kexin Chen}{}%{yyy,comp}
\end{icmlauthorlist}

%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}

\icmlcorrespondingauthor{Ruichen Xu}{rx10@rice.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
	
\printAffiliationsAndNotice{} 
\begin{abstract}
Recent theoretical studies \citep{kou2023benign,cao2022benign} have revealed a sharp phase transition from benign to harmful overfitting when the noise-to-feature ratio exceeds a thresholdâ€”a situation common in long-tailed data distributions where atypical data is prevalent.
However, harmful overfitting rarely happens in overparameterized neural networks. 
Further experimental results suggested that memorization is necessary for achieving near-optimal generalization error in long-tailed data distributions \citep{feldman2020neural}.
We argue that this discrepancy between theoretical predictions and empirical observations arises because previous feature-noise data models overlook the heterogeneous nature of noise across different data classes.
In this paper, we refine the feature-noise data model by incorporating class-dependent heterogeneous noise and re-examine the overfitting phenomenon in neural networks.
Through a comprehensive analysis of the training dynamics, we establish test loss bounds for the refined model.
Our findings reveal that neural networks can leverage "data noise", previously deemed harmful, to learn implicit features that improve the classification accuracy for long-tailed data. 
Experimental validation on both synthetic and real-world datasets supports our theoretical results.
\end{abstract}

\section{Introduction}
Overfitting, also known as memorization, has long been considered detrimental to model generalization performance \citep{hastie2009elements}.
However, with the advent of over-parameterized neural networks, models can perfectly fit the training data while still exhibiting improved generalization as model complexity increases.
When and why this \emph{benign overfitting} phenomenon happens have garnered significant interest within the learning theory community.
Recent works, e.g., \citep{frei2022benign,cao2022benign,kou2023benign} showed a sharp phase transition between benign and harmful overfitting in two-layer neural networks with the feature-noise data model \citep{allen2020towards}, which assumes data is composed of a feature vector as its mean and a random Gaussian vector as its data-specific noise.
Specifically, when the magnitude of data-specific noise exceeds a threshold, the neural networks memorize the data noise, leading to harmful overfitting.
Nevertheless, such harmful overfitting is rarely observed in modern over-parameterized neural networks.

Empirical evidence indicates that memorization can, in fact, enhance generalization, especially in long-tailed data distributions characterized by substantial data-specific noise \citep{feldman2020neural,hartley2022measuring,wang2024memorization,garg2023memorization}.
These findings contradict current theories of the phase transition to harmful overfitting.
Consequently, the following problem remains open.
\begin{center}
	\emph{How can we theoretically explain benign overfitting in overparameterized neural networks?}
\end{center}
Inspired by the heterogeneous intra-class distributions in real datasets, we refine the feature-noise data model by incorporating class-dependent heterogeneous noise and re-examine the benign overfitting phenomenon in two-layer ReLU \acp{cnn}.
In this paper, we make the following contributions:
\begin{itemize}
    \item We establish an enhanced feature-noise model by considering class-dependent heterogeneous noise across classes. Our results with this model theoretically explain how memorization of long-tailed data boosts model performance, which cannot be predicted by existing theoretical frameworks for neural networks. 
    Our results also show a probably counterintuitive result that a class' classification accuracy for long-tailed data may decrease with the dataset sizes of other classes, giving an explanation to the observations in \citet{sagawa2020investigation}.
    \item We derive general theoretical phase transition results between benign and harmful overfitting by analyzing the test error bound of the refined model. 
    Our results demonstrate that although the trained neural networks can classify data through explicit features, they can additionally utilize implicit features learned through memorization of data noise to classify long-tailed data.  
    The findings are well-supported by real datasets.
    \item We derive new proof techniques to tackle the randomness involved in feature learning.
    Unlike explicit feature learning, which exhibits stable activation states and magnitudes, data-specific noise has random activation states and magnitudes.
    Specifically, to tackle the random activation states, we explore the singular value distributions of neural networks to characterize the variability.
    Moreover, we demonstrate that the output strength of neurons randomly activated by data-specific noise is influenced by intra-class covariance matrices.
   	\item Our analysis provides a simple model-training-free metric for evaluating data memorization, unlike previous metrics that rely on training or storing multiple models. 
The data with high scores on our metric correspond to visually long-tailed samples, which are memorized to benefit model generalization, aligning with \citet{feldman2020neural,garg2023memorization}.
\end{itemize}

\subsection{Related Work}
We review the topics of empirical observations and theoretical studies of memorization.

\textbf{Empirical observations of memorization.}
A lot of recent empirical studies have shown that memorization inevitably happens in modern over-parameterized neural networks.
For instance, \citet{feldman2020neural,garg2023memorization} studied the memorization of training examples and found that neural networks tend to memorize visually atypical examples, i.e., those are rich in data-specific noise. 
These observations motivate us to study the impact of data-specific information on benign overfitting.
In this paper, our results provide a theoretical justification for these empirical observations in neural networks. 

\textbf{Theoretical analyses for memorization.}
A body of work theoretically examined memorization within classical machine learning frameworks, demonstrating its significance for achieving near-optimal generalization performance across various contexts, including high-dimensional linear regression \citep{cheng2022memorize}, prediction models \citep{brown2021memorization}, and generalization on discrete distributions and mixture models \citep{feldman2020does}.
This line of research failed to explain learning dynamics and generalization performance in non-convex and non-smooth neural networks.

Another line of work theoretically studied memorization in neural networks by analyzing the feature learning process during training, providing analytical frameworks that extend beyond the \ac{ntk} regime \citep{jacot2018neural,allen2019convergence,du2018gradient}.
For example, studies (e.g. \cite{cao2022benign} and \cite{kou2023benign}) explored benign overfitting in two-layer neural networks using the feature-noise model with homogeneous data noise distributions and showed a sharp phase transition between benign and harmful overfitting.
However, their results all show that it is harmful to memorize data-specific information and thus fail to explain the empirical observations with long-tailed data.

\subsection{Notation}
We use lowercase letters, lowercase boldface letters, and uppercase boldface letters to denote scalars, vectors, and matrices, respectively. 
We use $[m]$ to denote the set $\{1,\cdots,m\}$.
Given two sequences $\{x_n\}$ and $\{y_n\}$, we denote $x_n = \mathcal{O}(y_n)$ if $|x_n|\le C_1|y_n|$ for some positive constant $C_1$ and $x_n = \Omega(y_n)$ if $|x_n|\ge C_2|y_n|$ for some positive constant $C_2$.
We use $x_n = \Theta(y_n)$ if $x_n = \mathcal{O}(y_n)$ and $x_n = \Omega(y_n)$ both hold.
We use $\tilde{\mathcal{O}}(\cdot), \tilde{\Theta}(\cdot)$, and $\tilde{\Omega}(\cdot)$ to hide the logarithmic factors in these notations.
Given a matrix $\mathbf{A}$, we use $\left\|\mathbf{A}\right\|_F$ to denote its Frobenius norm, $\left\|\mathbf{A}\right\|_\text{op}$ to denote its operator norm, $\mathrm{Tr}(\mathbf{A})$ to denote its trace, $\lambda_i(\mathbf{A})$ to denote its $i^{th}$ largest singular value, $\lambda^+_{\max}(\mathbf{A})$ and $\lambda^+_{\min}(\mathbf{A})$ to denote its maximum and minimum absolute values of non-zero singular values, and $\text{rank}(\mathbf{A})$ to denote its rank.
We use the notation $(\mathbf{x},y)\sim\mathcal{D}$ to denote that the data sample $(\mathbf{x},y)$ is generated from a distribution $\mathcal{D}$.

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{Figure/digits_tsne}
	\caption{\textbf{Left:} Examples of MNIST long-tailed atypical data, which are tended to be memorized by neural networks \citep{feldman2020neural}. 
	\textbf{Right:} t-SNE visualization of MNIST. 
	The heterogeneous shapes imply real datasets, such as MNIST, practically show heterogeneously correlated data noise structures, providing experimental evidence for our theory.} 
	\label{fig: digits_corr}
\end{figure}

\section{Problem Setup}\label{sec: model}
In this section, we introduce the problem setup including the data model, 
\acp{cnn}, and the training algorithm. 

Given the evidence that real-world datasets have heterogeneous class-dependent data noise (as shown in Figure \ref{fig: digits_corr}), we consider a data distribution as follows.

\textbf{Data distribution.}
We define a data distribution $\mathcal{D}$ that each sample $(\mathbf{x},y)$ is generated as

1. Sample the label $y$ following a distribution $\mathcal{K}$, whose support is $[K] \ (K = \Theta(1))$.

2. The data $\mathbf{x} = \left(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}\right)$, where $\mathbf{x}^{(1)}, \mathbf{x}^{(2)}\in \mathbb{R}^d$ contains either feature or data-specific noise patch:
\begin{itemize}
\item Feature patch: One patch is randomly selected as the feature patch, containing a feature vector $\mathbf{u}_y \in \mathbb{R}^d$.
\item Noise patch: The remaining patch $\boldsymbol{\xi}$ is generated as $\mathbf{A}_y\boldsymbol{\zeta}$, where each coordinate of $\boldsymbol{\zeta}$ is i.i.d. drawn from $\mathcal{D}_\zeta$, a symmetric $\sigma_p = \Theta(1)$\footnote{Here, we simple refer a $\alpha$ sub-Gaussian variable as a sub-Gaussian variable with variance proxy $\alpha$.} sub-Gaussian distribution with variance 1, and $\mathbf{A}_y\in\mathbb{R}^{d\times d}$ satisfies $\mathbf{u}_k^\top\mathbf{A}_y=0$, for any $k\in[K]$. \footnote{This condition ensures that the noise patch is orthogonal to the feature patch. Our data distribution includes the widely adopted feature-noise data distribution with homogeneous data noise \citep{cao2022benign,zou2023benefits,jelassi2022towards} as a special case.
By setting $K=2$ and $\mathbf{A}_k\mathbf{A}^\top_k = \mathbf{I}-\sum_{k=1}^{K}\frac{\mathbf{u}_k\mathbf{u}_k^\top}{\left\|\mathbf{u}_k\right\|_2^2}$ for all $k,y\in[K]$, our data distribution becomes the same as theirs.}
\end{itemize}
Without loss of generality, we assume that the feature vectors $\mathbf{u}_1, \cdots, \mathbf{u}_K$ are orthogonal and their norms are bounded, i.e., $\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0, \left\|\mathbf{u}_i\right\|_2 = \Theta(1)$ for all $i,j \in [K]$ and $i \neq j$.

\begin{remark}
	The sub-Gaussian distribution of noise patches is general. In practice, pixel magnitudes in the classification tasks are bounded and thus are sub-Gaussian. 
\end{remark}

\textbf{Learner model.}
We consider a two-layer \ac{cnn} with ReLU activation as the learner model.
Given an input $\mathbf{x} = \left(\mathbf{x}^{(1)},\mathbf{x}^{(2)}\right)$, the model with weights $\mathbf{W}$ outputs a $K$-length vector $[F_1,\cdots, F_K]$ whose elements are
\begin{align}
	F_{k}\left(\mathbf{W},\mathbf{x}\right) = \frac{1}{m}\sum_{r=1}^{m}\sum_{j=1}^{2}\sigma\left(\left\langle \mathbf{w}_{k,r},\mathbf{x}^{(j)} \right\rangle\right),
\end{align}
where $\sigma(z) = \max\{0,z\}$ denotes the ReLU activation function, $\mathbf{w}_{k,r}$ denotes the weight vector for the $r^{th}$ neuron (totally $m$ neurons) associated with $F_k(\mathbf{W},\mathbf{x})$.

\textbf{Training objective.}
Given a training dataset with $n$ samples $\mathcal{S} = \{(\mathbf{x}_i,y_i)\}_{i=1}^n$ drawn from the distribution $\mathcal{D}$,
we train the neural network by minimizing the empirical risk with the cross-entropy loss, i.e.,
\begin{align}
	\mathcal{L}_\mathcal{S}\left(\mathbf{W}\right) = \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}(\mathbf{W},\mathbf{x}_i,y_i),
\end{align}
where 
	$\mathcal{L}(\mathbf{W},\mathbf{x},y) = -\log(\text{logit}_{y}(\mathbf{W},\mathbf{x}))$,
and $\text{logit}(\cdot)$ represents the output probability of the neural network:
\begin{align}
	\text{logit}_y(\mathbf{W},\mathbf{x}) = \frac{\exp(F_y(\mathbf{W},\mathbf{x}))}{\sum_{k=1}^{K}\exp(F_k(\mathbf{W},\mathbf{x}))}.
\end{align}

\textbf{Initialization.}
The initial weights of the neural network's parameters are generated i.i.d. from Gaussian distribution, i.e., $\mathbf{w}^{(0)}_{j,r}\sim \mathcal{N}(\mathbf{0},\sigma_0^2\mathbf{I})$, for all $j\in[K], r\in[m]$.

\textbf{Training algorithm.}
We train the neural network by \ac{gd} with a learning rate $\eta$, i.e.,
\begin{align}
	\mathbf{W}^{(t+1)} = \mathbf{W}^{(t)} - \frac{\eta}{n}\sum_{(\mathbf{x},y)\in\mathcal{S}}\nabla \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y).
\end{align}

\section{Main Results}\label{sec: learning}
In this section, we present our main theoretical results.
We start by introducing some conditions for our theory.

Our analyses rely on the following conditions on noise patch distributions, training dataset size $n$, network width $m$, dimension $d$, learning rate $\eta$, and initialization $\sigma_0$.
\begin{condition}\label{condition}
	Suppose there exists a sufficiently large
	constant $C$. For certain probability parameter $\delta \in (0,1)$, the following conditions hold:   
	\begin{enumerate}[label=(\alph*)]
		\item To ensure that the neurons can learn the data patterns\footnote{Conditions (a) on the noisy patches generalize the conditions on dimension $d$ in \cite{kou2023benign}. Setting $\mathbf{A}_k\mathbf{A}_k^\top = \mathbf{I} - \sum_{k=1}^{K}\mathbf{u}_k\mathbf{u}_k^\top/\left\|\mathbf{u}_k\right\|_2^2$ for all $k\in[K]$, Condition (a) is similar to the dimension condition in \cite{kou2023benign}.}, the noise patch distributions satisfy, for any $ i,j \in[K]$,
        \begin{equation*}
            \begin{cases}
                \mathrm{Tr}(\mathbf{A}_i^\top\mathbf{A}_i) \ge Cn\left\|\mathbf{A}_i^\top\mathbf{A}_j\right\|_F\log(\frac{n^2}{\delta}),\\
                \frac{\left\|\mathbf{A}_i^\top \mathbf{A}_j\right\|_F}{\left\|\mathbf{A}_i^\top \mathbf{A}_j\right\|_\textnormal{op}}\ge C\sqrt{\log(\frac{K}{\delta})},\\
                \left\|\mathbf{A}_i^\top\mathbf{A}_j\right\|_F\ge C^{-1}\max_{k\neq j}\{\left\|\mathbf{A}_i^\top\mathbf{A}_k\right\|_F\}.
            \end{cases}
        \end{equation*}
        Moreover, there exists a threshold $c'>0$ such that $\mathbb{P}[\zeta> c'] \ge 0.4$.
		\item To ensure that the learning problem is in a sufficiently over-parameterized setting, the training dataset size $n$, network width $m$, and dimension $d$ satisfy 
        \begin{equation*}
            \begin{cases}
                m\ge C\log(n/\delta)\max_i\{\frac{(\lambda_{\max}^+(\mathbf{A}_i))^2}{(\lambda_{\min}^+(\mathbf{A}_i))^2}\},\\
                n\ge C\log(m/\delta), m \ge \Omega\left( \frac{\log(\frac{n}{\delta})\log(T)^2}{n\sigma_0^2}\right),\\
                \min\{m,d,\text{rank}(\mathbf{A}_j)\}-0.9m \ge Cn,\\
                d \ge C\log(\frac{mn}{\delta}).
            \end{cases}
        \end{equation*}
		\item To ensure that gradient descent can minimize the training loss, the learning rate $\eta$ and initialization $\sigma_0$ satisfy, for any $j\in[K]$,
        \begin{equation*}
            \begin{cases}
                \!\eta \!\le\! \!\left(\!\!C\!\left(\!\max_{k\in[K]}\!\left(\!\left\|\mathbf{u}_k\right\|_2 \!+\!\! \sqrt{\!1.5\mathrm{Tr}\left(\mathbf{A}_k^\top\mathbf{A}_k\right)}\!\right)\!\right)^2\!\right)^{-1},\\
                \eta \le \frac{mn\log(T)}{\mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)},\\
                \sigma_0 \le\!C^{-1}n^{-1}\phi\cdot$ $\!\\
                \!\!\left(\!\max_{k\in[K]}\!\{\!\sqrt{\log(\!Km/\!\delta)}\!\!\left\|\mathbf{u}_k\right\|_2\!,\!\!\log(\!Km/\!\delta)\!\left\|\mathbf{A}_k\right\|\!\!_F\!\}\!\!\right)^{-1}\!\!,
            \end{cases}
        \end{equation*}
        where $\phi:= \min_{k_1,k_2\in[K]}\{\left\|\mathbf{A}_{k_1}^\top\mathbf{A}_{k_2}\right\|_F,\left\|\mathbf{u}_{k_1}\right\|_2^2\}$.
	\end{enumerate}
\end{condition}

Based on Condition \ref{condition}, we study the model convergence and generalization performance by bounding the training loss and the zero-one test loss (accuracy) of the trained model $\mathbf{W}^{(T)}$ on distribution $\mathcal{D}_k$ whose probability density function is $\mathbb{P}_{\mathcal{D}_k}[(\mathbf{x},y)] = \mathbb{P}_{\mathcal{D}}[(\mathbf{x},y)|y=k]$, i.e., for all $k\in[K]$,
\begin{equation}\nonumber
\begin{aligned}
	&\mathcal{L}^{0-1}_{\mathcal{D}_k}(\mathbf{W}^{(T)})\\
	=& \mathbb{P}_{(\mathbf{x},y)\sim \mathcal{D}_k}\left[F_y(\mathbf{W}^{(T)},\mathbf{x}) \neq \max_{j\in[K]}\{F_j(\mathbf{W}^{(T)},\mathbf{x})\}\right].
\end{aligned}
\end{equation}

Before presenting the main theorem, we define the set of long-tailed data with respect to the trained model $\mathbf{W}^{(T)}$.
\begin{definition}[$L$-Long-tailed data set]\label{def: lt}
    The $L$-long-tailed data distribution $\mathcal{T}_j$ for each $j\in[K]$ with model $ \mathbf{W}^{(T)}$ is defined as
    \begin{equation}\nonumber
    \begin{aligned}
        &\mathbb{P}_{\mathcal{T}_j}[(\mathbf{x},y)]=\\
        & \mathbb{P}_{\mathcal{D}_j}\left[(\mathbf{x},y)|\langle\sum_{r\in\mathcal{R}(\boldsymbol{\xi})}\mathbf{w}^{(T)}_{y,r}, \boldsymbol{\xi}\rangle \!=\!\Omega(L\|\mathbf{A}_y^\top\!\sum_{r\in\mathcal{R}(\boldsymbol{\xi})}\mathbf{w}^{(T)}_{y,r}\|_2)\right],
    \end{aligned}
\end{equation}
    where 
        $\mathcal{R}(\boldsymbol{\xi}) = \{r\in[m]:\langle\mathbf{w}^{(T)}_{y,r}, \boldsymbol{\xi}\rangle>0\}$.
\end{definition}
Definition \ref{def: lt} identifies data whose equivalent noise $\zeta'$ exceeds a threshold.
Specifically, for data $(\mathbf{x},y)\sim \mathcal{D}$, the inner product term $\langle\sum_{r\in\mathcal{R}(\boldsymbol{\xi})}\mathbf{w}^{(T)}_{y,r}, \boldsymbol{\xi}\rangle$ satisfies $\langle\sum_{r\in\mathcal{R}(\boldsymbol{\xi})}\mathbf{w}^{(T)}_{y,r}, \boldsymbol{\xi}\rangle = \Theta\left(\left\|\mathbf{A}_y^\top\sum_{r\in\mathcal{R}(\boldsymbol{\xi})}\mathbf{w}^{(T)}_{y,r}\right\|_2\zeta'\right)$, where $\zeta'$ is an equivalent random sub-Gaussian variable with variance 1.

\begin{remark}
	Model-dependent long-tailed definitions are common. \citet{feldman2020neural} defined the long-tailed data using an influence score that measures the training loss difference on datasets with and without a data sample. 
	\citet{garg2023memorization} defined the long-tailedness of data using a curvature metric that approximates a loss Hessian matrix-related quantity of trained models averaged over all training epochs.
\end{remark}
We denote $\mathcal{S}_j$ as the set containing training data with label $j$ in $\mathcal{S}$.
We present our main result in the following theorem.

\begin{theorem}\label{theorem: feature learning}
	For any $\epsilon >0$ and $k\in[K]$, under Condition \ref{condition}, there exists 
    $T = \tilde{\mathcal{O}}(\eta^{-1} \epsilon^{-1}\!\max\{n\max_{j\in[K]}\{\mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)\}, \sqrt{md}\sigma_0,\!\sqrt{m K}\})$, with probability at least $1-\delta$, the following holds:
	\begin{enumerate}
		\item The training loss satisfies $\mathcal{L}_\mathcal{S}(\mathbf{W}^{(T)})\le \epsilon$.
		\item Benign overfitting:
            \begin{enumerate}
                \item \underline{(For all data)} When the signal-to-noise ratio is large, i.e., $|\mathcal{S}_k|^2\left\|\mathbf{u}_k\right\|_2^4 \ge C_1\cdot\max_{j\neq k}\{|\mathcal{S}_j|\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F^2\}$, the zero-one test loss satisfies $$\mathcal{L}^{0-1}_{\mathcal{D}_k}(\mathbf{W}^{(T)})\!\le\! \sum_{j\neq k}\exp\!\left(\!-c_1\cdot\underbrace{\frac{|\mathcal{S}_k|^2\left\|\mathbf{u}_k\right\|_2^4}{|\mathcal{S}_j|\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F^2}}_\text{signal-to-noise ratio}\right)\!.$$
		\item \underline{(Only for long-tailed data)} When the noise correlation ratio is large, i.e.,$|\mathcal{S}_k|\left\|\mathbf{A}_k^\top\mathbf{A}_k\right\|_F^2 \ge C_2\cdot\max_{j\neq k}\{|\mathcal{S}_j|\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F^2\}$, the zero-one test loss satisfies
		\begin{equation}\nonumber
		\begin{aligned} &\mathcal{L}^{0-1}_{ \mathcal{T}_k}(\mathbf{W}^{(T)})\\
		\le& \sum_{j\neq k}\exp\left(-c_2L^2\cdot\underbrace{\frac{|\mathcal{S}_k|\left\|\mathbf{A}_k^\top\mathbf{A}_k\right\|_F^2}{|\mathcal{S}_j|\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F^2}}_{\text{noise correlation ratio }\Gamma_{k,j}} \right).
		\end{aligned}
		\end{equation}
            \end{enumerate}
		\item Harmful overfitting: When the signal-to-noise ratio and noise correlation ratio are small, i.e., $|\mathcal{S}_k|^2\left\|\mathbf{u}_k\right\|_2^4 \le C_3\cdot\max_{j\neq k}\{|\mathcal{S}_j|\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F^2\}$ and $|\mathcal{S}_k|\left\|\mathbf{A}_k^\top\mathbf{A}_k\right\|_F^2 \le C_3\cdot\max_{j\neq k}\{|\mathcal{S}_j|\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F^2\}$, the zero-one test loss satisfies $\mathcal{L}^{0-1}_{\mathcal{D}_k}(\mathbf{W}^{(T)}) \ge c_3$.
	\end{enumerate}
	Here $C_1,C_2,C_3, c_1, c_2, c_3$ are some absolute constants.
\end{theorem}

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{Figure/overfitting3}
	\caption{Illustration of the generalized phased transition between benign and harmful overfitting (memorization). 
	The blue region represents a benign overfitting regime where the test loss is small. 
	The orange region represents a harmful overfitting regime where the test loss is at a constant order.
	The gray band region is the setting where the test loss is not well characterized.}
	\label{fig:overfitting}
\end{figure}

\begin{remark}
	Theorem \ref{theorem: feature learning} shows that after $T$ iterations, \acp{cnn} converge to training loss $\epsilon$ (Statement 1).
	When the signal-to-noise ratio is high, the trained \acp{cnn} can achieve optimal test loss by effectively detecting explicit features $\mathbf{u}_k$ (Statement 2(a)).
	Furthermore, \acp{cnn} also classify long-tailed data by seeking to detect data-specific features so that the performance benefits from high noise correlation ratios (Statement 2(b)). 
	Conversely, when both noise correlation ratios and signal-to-noise ratios are small, the trained \acp{cnn} incur test loss at least a constant (Statement 3). 
    Notably, longer-tailed data (with a larger $L$) benefits more from the data-specific noise.
\end{remark}
\begin{remark}
	Theorem \ref{theorem: feature learning} characterizes a sharp phase transition between benign and harmful overfitting (memorization), visualized in Figure \ref{fig:overfitting}.
	Statement 2(a) in Theorem \ref{theorem: feature learning} extends the previous benign overfitting results, which characterize the horizontal dimension in Figure. \ref{fig:overfitting}.
	Statement 2(b) in Theorem \ref{theorem: feature learning} at the first time characterizes the vertical dimension in Figure. \ref{fig:overfitting}, showing that the model can also leverage data-specific noise to achieve benign overfitting.
\end{remark}
\begin{remark}
	By choosing $\mathbf{A}_j\mathbf{A}_j^\top = \mathbf{I} - \sum_{k=1}^{K}\mathbf{u}_k\mathbf{u}_k^\top/\left\|\mathbf{u}_k\right\|_2^2$ for all $j\in[K]$, our Statement 2(a) recovers the same convergence orders in the standard benign overfitting results \citep{kou2023benign}.
	However, our results are more general as our results cover the whole class of sub-Gaussian data noise, $K>2$ number of classes, and data imbalance, which is common in modern image classification tasks.
\end{remark}


The results of Theorem \ref{theorem: feature learning} theoretically explain the following two empirical observations in neural networks for the first time.

\textbf{Long-tailed (atypical) data is important for generalization.}
In Statement 2(b) of Theorem \ref{theorem: feature learning}, we show that the classification accuracy of long-tailed data increases with the noise correlation ratio.
This result implies that incorporating more long-tailed data into the training dataset enhances test accuracy, providing a theoretical explanation for the empirical observation in \citet{feldman2020neural} that including long-tailed data in the training dataset is necessary for neural networks to achieve near-optimal generalization performance.

\textbf{Increasing majority hurts minority.}
Statement 2(b) of Theorem \ref{theorem: feature learning} implies that as the dataset size of class $j$, $|\mathcal{S}_j|$ increases, the upper bound on test loss of other classes $k\neq j$ increases.
This leads to a possibly surprising result: the classification accuracy for long-tailed data of a specific class may decline when the sizes of other classes increase. 
The reason is that during training, the memorization of majority data-specific noise (classes with more data) dominates the memorization of minority data-specific noise.
Our result theoretically explains a counter-intuitive observation in neural networks that subsampling the majority group empirically achieves low minority error \citep{sagawa2020investigation}. 

\section{Proof Overview}
In this section, we present a proof sketch of Statement 1 and Statement 2(b) in Theorem \ref{theorem: feature learning} (Statement 2(a) uses a similar but simpler proof).
Due to the space limit, we defer the complete proofs to Appendices A, B, and C.  

\subsection{Proof Sketch of Statement 1 in Theorem \ref{theorem: feature learning}}
We analyze the training loss in two stages.
In training Stage 1, the training loss of each example decreases exponentially and stays at a constant order $\Theta(1)$.
In Stage 2, the model converges to an arbitrarily small constant.

\paragraph{Stage 1.} By the nature of cross-entropy, the training loss decreases exponentially, characterized by the following lemma.
\begin{lemma}\label{lemma: stage 1}
	Under Condition \ref{condition}, there exists an iteration number $T_0 = \tilde{\mathcal{O}}(\frac{mn}{\eta})$, so that for any $(\mathbf{x},y)\in \mathcal{S}$ and $t\le T_0$, the following holds:
	\begin{equation}
		\begin{aligned}
			&\mathcal{L}(\mathbf{W}^{(t+1)},\mathbf{x},y)\\
			 \le& \left(1-\Theta\left(\frac{\eta}{mn}|\mathcal{S}_y|\left\|\mathbf{u}_y\right\|_2^2\right)\right)\mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y).
		\end{aligned}
	\end{equation}
\end{lemma}
\paragraph{Stage 2.} We show that the training loss converges to an arbitrarily small constant $\epsilon$ with a rate of $\mathcal{O}(\frac{1}{T})$.
\begin{lemma}\label{lemma: stage 2}
		Let $\mathbf{W}^*$ be the network parameters with each neuron $\mathbf{w}_{j,r}^* = 2.5\log\left(\frac{4(K-1)}{\epsilon}\right)\frac{\mathbf{u}_j}{\left\|\mathbf{u}_j\right\|_2^2}$.
		Under Condition \ref{condition}, for any $t\in [T_0,T]$ and $(\mathbf{x},y)\in\mathcal{S}$, the following result holds:
	\begin{equation}\nonumber
		\begin{aligned}
			\frac{1}{t-T_0+1}\sum_{s=T_0}^{t}\mathcal{L}_S(\mathbf{W}^{(t)})\le& \frac{\left\|\mathbf{W}^{(T_0)}-\mathbf{W}^*\right\|_F^2}{\eta (t-T_0+1)} + \frac{\epsilon}{2}.
		\end{aligned}
	\end{equation}
\end{lemma}
After $T = \tilde{\mathcal{O}}(\eta^{-1} \epsilon^{-1}\!\max\{n\max_{j\in[K]}\{\mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)\},$ $ \sqrt{md}\sigma_0,\!\sqrt{m K}\})$ iterations, the training loss converges to $\epsilon$.
Our analysis extends that of \citet{kou2023benign}\footnote{\citet{kou2023benign} provided convergence analysis for training loss in two-layer \acp{cnn} with binary classification and logistic loss.} by providing the convergence rate for Stage 1 and a comprehensive convergence analysis for $K$-class classification problems using cross-entropy loss.


\subsection{Proof Sketch of Statement 2(b) in Theorem \ref{theorem: feature learning}}
By the definition of zero-one test loss, for any long-tailed data $(\mathbf{x},y)\in\mathcal{T}_i, i\in[K]$, its test loss satisfies:
\begin{equation}\nonumber
	\begin{aligned}
		&\mathcal{L}^{0-1}_{\mathcal{T}_i}(\mathbf{W}^{(T)})\le\sum_{j\neq i}\\ & \!\mathbb{P}_{(\mathbf{x},y)\sim \mathcal{T}_i}\!\!\!\left[\sum_{r=1}^{m}\!\sigma(\!\langle\mathbf{w}_{y,r}^{(T)}\!,\boldsymbol{\xi}\rangle\!)\!\le\!\!\sum_{r=1}^{m}\! \sigma(\!\langle \mathbf{w}_{j,r}^{(T)}\!, \!\mathbf{u}_y\rangle\!) \!+\! \sigma(\!\langle \mathbf{w}_{j,r}^{(T)}\!, \boldsymbol{\xi}\rangle\!) \!\right]\!\!.
	\end{aligned}
\end{equation}

Roughly speaking, since the neurons for a class do not learn explicit features for other classes, i.e., $\sum_{r=1}^{m}\! \sigma(\langle \mathbf{w}_{j,r}^{(T)}, \mathbf{u}_y\rangle)$ is small, the key to quantifying the test loss is to compare the correlation coefficients $\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle)$ and $\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle)$.
Firstly, we need to quantify the number of neurons that can be activated by the random data-specific noise $\boldsymbol{
\xi}$.
We leverage the properties of the rank and singular values of the neurons corresponding to each class $j\in[K]$, i.e., $\mathbf{W}_j^{(T)} = [\mathbf{w}_{j,1}^{(T)},\cdots,\mathbf{w}_{j,m}^{(T)}]^\top$, in the following lemma.
\begin{lemma}\label{lemma: rank_ev}
    Under condition \ref{condition}, the matrix $\mathbf{W}_j^{(T)}$ for all $j\in[K]$ satisfies
    \begin{equation}\nonumber
    \begin{aligned}
        &\textnormal{rank}(\mathbf{W}_j^{(T)}\mathbf{A}_j)\ge 0.9m, \ \lambda_1(\mathbf{W}_j^{(T)}\mathbf{A}_j) \le \mathcal{O}(\sqrt{m}\log(T)), \\
        &\lambda_{\min\{m,d,\textnormal{rank}(\mathbf{A}_j)\}-n}(\mathbf{W}_j^{(T)}\mathbf{A}_j) \ge 0.1 \sqrt{m}\sigma_0.
    \end{aligned}
    \end{equation}
\end{lemma}
With Lemma \ref{lemma: rank_ev}, we can prove that the probability that $\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle$ remains in an orthant decreases exponentially with the number of neurons $m$.
Consequently, we derive the following lemma for the number of activated neurons.
\begin{lemma}\label{lemma: activation}
	Under condition \ref{condition}, for any $j\in [K]$ and any $(\mathbf{x},y)\sim\mathcal{D}_j$, with probability at least $1-\exp(-\Omega(m))$, the trained model $\mathbf{W}^{(T)}$ satisfies
	\begin{align}
		\sum_{r=1}^{m}\mathbb{I}\left(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle\right) \ge 0.1m.
	\end{align}
\end{lemma}

Lemma \ref{lemma: activation} indicates that, with high probability, at least a non-negligible amount of neurons are able to detect the data-specific noise in the test data.
Next, we quantify the activation magnitude of the correlation coefficients $\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle)$ and $\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle)$.
 In the following lemma, we prove that the neurons for class $j\in[K]$ mainly learn the data noise from class $j$.
\begin{lemma}\label{lemma: learn_order}
	Under Condition \ref{condition}, for any $j\in [K], l\in [K]\backslash \{j\},(\mathbf{x}_q,y_q)\in\mathcal{S}_j, (\mathbf{x}_a,y_a)\in\mathcal{S}_l$, existing a sufficiently large constant $C'>0$, we have
	\begin{equation}
		\begin{aligned}
			\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}_q\rangle) \ge C' \sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}_a\rangle).
		\end{aligned}
	\end{equation}
\end{lemma}
As a result, the order of activation magnitudes is controlled by the intra-class correlations and inter-class correlations, quantified by $\|\mathbf{A}_y^\top\mathbf{A}_y\|_F$ and $\|\mathbf{A}_y^\top\mathbf{A}_j\|_F$.
\begin{lemma}\label{lemma: order}
	Under Condition \ref{condition}, for any $(\mathbf{x},y)\sim \mathcal{T}_i$ and $i\in[K],j\in[K]\backslash\{y\}$, the following holds:
	\begin{align}\nonumber
		& \sum_{r=1}^{m}\sigma\left(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle\right) = \Theta\left(\frac{\eta m}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F Lb(\mathcal{S}_{y}) \right),\\\nonumber
		&\sum_{r=1}^{m}\sigma\left(\langle\mathbf{w}^{(T)}_{j,r},\boldsymbol{\xi}\rangle\right) = \mathcal{O}\left(\frac{\eta m}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_Fb(\mathcal{S}_{j})|\zeta_1|\right),
	\end{align}
	where $b(\cdot)$ is a function defined as
	\begin{align}
		&b(\mathcal{A}) = \sqrt{\sum_{(\mathbf{x},y)\in \mathcal{A}}\left(\sum_{t'=0}^{T-1}(1-\textnormal{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\right)^2},
	\end{align}
	and $\zeta_1$ is a sub-Gaussian variable with variance 1.
\end{lemma}
Finally, to compare $b(\mathcal{S}_{y})$ and $b(\mathcal{S}_{j})$, we prove that the loss gradient coefficients are balanced.
\begin{lemma}\label{lemma: ratio}
	For all $t\in [T]$ and for any $i,j\in[n]$, there exists a positive constant $\kappa$ such that
	\begin{align}
		\frac{1-\textnormal{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)}{1-\textnormal{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)} \le \kappa.
	\end{align}
\end{lemma}

Leveraging Hoeffding's inequality, Lemma \ref{lemma: order}, and \ref{lemma: ratio}, we derive Statement 2(b) in Theorem \ref{theorem: feature learning}.

\section{Experiments}
In this section, we first validate our theory in Theorem \ref{theorem: feature learning} by constructing datasets and models following our problem setup in Section \ref{sec: model}.
We further verify our conclusions with real-world datasets MNIST \citep{lecun1998gradient} and CIFAR-10 \citep{krizhevsky2009learning}. 
\subsection{Synthetic Datasets}
\textbf{Synthetic data generation.}
We generate a synthetic dataset following the data distribution outlined in Section \ref{sec: model}. 
The dataset comprises a total of $K = 5$ classes with a dimensionality of $d=1000$.
We set the feature patches for each $k\in[K]$ as $\mathbf{u}_k=\mathbf{U}\cdot\left\|\mathbf{u}_k\right\|_2\cdot\mathbf{z}_k$, where $\mathbf{z}_k$ keeps the $k^{th}$ entry to be one and other entries zeros, and  $\mathbf{U}$ is a randomly generated unitary matrix.
Without loss of generality, we consider $\mathbf{A}_k$ to be matrices with all but one non-zero eigenvalue fixed as 0.5.
Then, we tune the not-fixed non-zero eigenvalue to change the noise correlation ratio accordingly.

\textbf{Setup.}
We train a two-layer neural network as defined in Section \ref{sec: model} with $m=100$ neurons.
We use the default initialization method in PyTorch to initialize the neurons' parameters.
We train the neural networks using \ac{gd} with a learning rate $\eta = 0.05$ over $20$ epochs.
To explore the effects of feature strength $\left\|\mathbf{u}_i\right\|_2$, noise correlation ratio $\Gamma_{i,j}$, and dataset size $|\mathcal{S}_i|$, we simply fix $\left\|\mathbf{u}_i\right\|_2 = \left\|\mathbf{u}\right\|_2$, $\Gamma_{i,j} = \Gamma$, and $|\mathcal{S}_i|=s$, where $\left\|\mathbf{u}\right\|_2$, $\Gamma$, and $s$ are tunable parameters for all class $i,j\in[K]\ (j\neq i)$.

\textbf{Effects of noise correlation and feature strength.} 
We simulate with feature strength $\left\|\mathbf{u}\right\|_2$ ranging from $0.001$ to $3.8$ and  noise correlation ratio $\Gamma$ ranging from $1$ to $1400$.
We fix the dataset size of each class as $|\mathcal{S}_i| = 100$, for all $i\in[K]$.
The resulting heatmap of test accuracy in relation to data feature size and noise correlation is presented in Figure \ref{fig: corr_fea}.
As illustrated, the test loss decreases not only with increasing feature strength, but also with higher noise correlation ratios.
Notably, with high noise correlation ratios, we can achieve near-optimal test accuracy even when feature strength approaches nearly zero. 
\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
        \centering
		\includegraphics[width=0.7\linewidth]{Figure/heatmap_ga}
		\caption{Test accuracy heatmap under Gaussian data noise. Each entry of $\zeta$ follows $\mathcal{N}(0,1)$.}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=0.7\linewidth]{Figure/heatmap_un}
		\caption{Test accuracy heatmap under uniform data noise. Each entry of $\zeta$ follows $\mathcal{U}(-\sqrt{3},\sqrt{3})$.}
	\end{subfigure}
    \caption{Impacts of feature strength and noise correlation ratio on test accuracy.}
	\label{fig: corr_fea}
\end{figure}

\textbf{Effect of dataset size.}
We verify the impact of the dataset sizes in the two types of benign overfitting in Statement (a) and (b) of Theorem \ref{theorem: feature learning}.
We fix the noise correlation ratio as $\Gamma=1.2$ with $\zeta$ following Gaussian distribution and vary the feature size ranging from $0.01$ to 4 and dataset size ranging from $10$ to $200$ to explore the effect of dataset size versus feature strength on test accuracy (Figure \ref{fig: num_fea}).
We fix the feature strength as $\left\|\mathbf{u}\right\|_2=1e-6$ and tune noise correlation ratio $\Gamma$ (with $\zeta$ following Gaussian distribution) ranging from 1 to 800 and dataset size ranging from $10$ to $200$ to explore the effect of dataset size versus noise correlation ratio (Figure \ref{fig: num_corr}).
These results verify our theory.
Specifically, for classification utilizing explicit features (Figure. \ref{fig: num_fea}), the test accuracy increases with increasing dataset size, matching our Statement 2(a) in Theorem \ref{theorem: feature learning}. 
In contrast, for classification utilizing implicit covariance features (Figure \ref{fig: num_corr}), the test accuracy remains in a constant order with the dataset size, matching Statement 2(b) in Theorem \ref{theorem: feature learning}.
\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
        \centering
		\includegraphics[width=0.7\linewidth]{Figure/num_fea_ga}
		\caption{Test accuracy heatmap with feature strength and dataset size. Each entry of $\zeta$ follows $\mathcal{N}(0,1)$.}
		\label{fig: num_fea}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
        \centering
		\includegraphics[width=0.7\linewidth]{Figure/num_corr_ga}
		\caption{Test accuracy heatmap with noise correlation ratio and dataset size. Each entry of $\zeta$ follows $\mathcal{N}(0,1)$.}
		\label{fig: num_corr}
	\end{subfigure}
	\caption{Impacts of data size on test accuracy.}
	\label{fig:num}
\end{figure}
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{Figure/atypical}
	\caption{Visualization of images with different influence scores.}
	\label{fig:atypical}
\end{figure*}
\begin{figure}
	\centering
\includegraphics[width=0.7\linewidth]{Figure/corr}
	\caption{Visualization of the squared Frobenius norm among classes in MNIST.}
	\label{fig:corr}
\end{figure}


\subsection{Real-World Datasets}
We verify the effects of noise correlation on MNIST \citep{lecun1998gradient} and CIFAR-10 \citep{krizhevsky2009learning}.
\paragraph{Noise correlation ratio verifications.} To verify our results in Theorem \ref{theorem: feature learning}, we compute the squared Frobenius norm, i.e., $\|\mathbf{A}_i^\top\mathbf{A}_j\|_F$, among different classes in MNIST.
As shown in Figure. \ref{fig:corr}, real datasets such as MNIST indeed have large noise correlation ratios, satisfying the conditions for long-tailed data benign overfitting predicted in our theory (Statement 2(b) of Theorem \ref{theorem: feature learning}).

\paragraph{Data influence score.} We quantify the impact of a single data $(\mathbf{x},y)$ by measuring its impact on $\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F$ (A determining factor of the long-tailed data loss as in Statement 2(b) of Theorem \ref{theorem: feature learning}).
We observe that $\|\mathbf{A}_y^\top\mathbf{A}_y\|_F$ is, in fact, the same as the Frobenius norm of the covariance matrix of class $y$'s distribution and hence we consider an influence score of an image $(\mathbf{x},y)\in\mathcal{S}$ as
\begin{align}\label{equ: infl}
	\text{infl}(\mathbf{x}) = \left\|\hat{\Sigma}(\mathcal{S}_y)\right\|_F^2 - \left\|\hat{\Sigma}(\mathcal{S}_y\backslash \{\mathbf{x}\})\right\|_F^2,
\end{align}
where $\hat{\Sigma}(\mathcal{S}_y)$ denotes the estimated\footnote{We estimate the covariance matrix using the sample covariance matrix.} covariance of the underlying data distribution within the dataset $\mathcal{S}_y$.



\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{Figure/remove}
	\caption{Effect on the test accuracy of removing a fraction of examples with high and low influence scores.}
	\label{fig:remove}
\end{figure}

\textbf{What influence scores indicate.}
We visualize the data with high and low influence scores in Figure \ref{fig:atypical}. 
The data with high influence scores are atypical data, which can be interpreted by scrawly written digits and hard-to-classify objects in the experimental datasets.


\textbf{Inspiration from influence scores.}
We sort the data from MNIST and CIFAR-10 based on the influence score defined in (\ref{equ: infl}).
In Figure \ref{fig:remove}, we remove a portion of data with high and low influence scores and train the remaining data on LeNet \citep{lecun1998gradient} and ResNet-18 \citep{he2016deep} to assess their impact on model accuracy.
We observe that the accuracy drop of removing data with high influence scores is significantly larger than that of removing data with low influence scores.
This observation verifies our theory (Statement 2(b) in Theorem \ref{theorem: feature learning}) that reducing the squared Frobenius norm significantly hurts the model's test performance as it reduces accuracy on long-tailed data.

\section{Conclusions}
In this paper, we rethink the overfitting phenomenon in overparameterized neural networks.
Specifically, we enhance the widely used feature-noise data model by incorporating heterogeneous data noise across classes. 
Our findings reveal that neural networks can learn implicit features from data noise, previously deemed harmful, and leverage these features to enhance classification accuracy in long-tailed data distributions. 
These findings align with the practical observations that memorization can enhance generalization performance.
Experiments on both synthetic and real-world datasets further validate our theory.

\bibliography{ref}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section{Key lemmas}\label{appendix:lemmas}
In this section, we present some important lemmas that illustrate the key properties of the data and neural networks.

\begin{lemma}\label{lemma: Gaussian_eigen}
	Let $\mathbf{Z}\in\mathbb{R}^{m\times n} (m>n)$ be a matrix whose entries are independent and identically distributed Gaussian variables, i.e., $\mathbf{Z}_{i,j}\sim \mathcal{N}(0,1)$ for all $i\in[m], j\in[n]$.
	With probability at least $1-\delta$, all singular values of $\mathbf{Z}$, $\lambda_i(\mathbf{Z})$, for all $i\in[n]$ satisfies 
	\begin{align}
		\lambda_{i}(\mathbf{Z}) \ge \sqrt{m} - 2\sqrt{8\log(\frac{2}{\delta})+8\log(9)n}.
	\end{align}
\end{lemma}
Lemma \ref{lemma: Gaussian_eigen} follows from the concentration inequality of Gaussian random matrices \citep{vershynin2018high}.

\begin{lemma}\label{lemma: sg}
    For $n$ random $\sigma_p$ sub-Gaussian variable $x_1,\cdots,x_n$, with probability $1-\delta$, we have
    \begin{align}
        \mathbb{P}[|x|\ge t] \le 2\exp\left(-\frac{t^2}{2\sigma_p^2}\right).
    \end{align}
\end{lemma}
\begin{proof}
    Based on the definition of sub-Gaussian distribution, with probability of $1-\frac{\delta}{n}$, we have
    \begin{align}
        |x_i| \ge \sqrt{2\sigma_p^2\log(\frac{2n}{\delta})}.
    \end{align}
    By Union bound, we finishes the proof.
\end{proof}

\begin{lemma}\label{lemma: inner product}
		Suppose two zero-mean random vectors $\mathbf{x},\mathbf{y}\in \mathbb{R}^d$ are generated as $\mathbf{x} = \mathbf{A}\boldsymbol{\zeta}_1, \mathbf{y} = \mathbf{A}\boldsymbol{\zeta}_2$, where $\boldsymbol{\zeta}_i$'s each coordinate is independent, symmetric, and $\sigma_p$ sub-Gaussian with $\mathbb{E}[\zeta^2_{i,j}]=1$, for any $i\in[2], j\in[d]$. 
		Then, $\mathbf{x}$ and $\mathbf{y}$ satisfy
		\begin{align}
			\mathbb{P}\left[|\langle \mathbf{x}, \mathbf{y}\rangle|\ge t \right] \le 2\exp\left( -\Omega\left(\min\left\{\frac{t^2}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_F^2\sigma_p^4}, \frac{t}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_\textnormal{op}\sigma^2_p}\right\}\right)\right).
		\end{align}
\end{lemma}
\begin{proof}
	We have
	\begin{equation}
	\begin{aligned}
		\left\langle \mathbf{x},\mathbf{y} \right\rangle= \left\langle \mathbf{A}\boldsymbol{\zeta}_1,\mathbf{A}\boldsymbol{\zeta}_2 \right\rangle = \boldsymbol{\zeta}_1^\top\mathbf{A}^\top\mathbf{A}\boldsymbol{\zeta}_2
		=\boldsymbol{\zeta}_1^\top\mathbf{U}^\top\Lambda\mathbf{U}\boldsymbol{\zeta}_2 = \tilde{\boldsymbol{\zeta}}_1^\top\Lambda\tilde{\boldsymbol{\zeta}}_2 = \mathrm{Tr}\left(\Lambda\tilde{\boldsymbol{\zeta}}_2\tilde{\boldsymbol{\zeta}}_1^\top\right).
	\end{aligned}
	\end{equation}

	As $\tilde{\boldsymbol{\zeta}}_1$ and $\tilde{\boldsymbol{\zeta}}_2$ are isotropic, by Bernstein's inequality, we have
	\begin{equation}
	\begin{aligned}
		\mathbb{P}\left[\mathrm{Tr}\left(\Lambda\tilde{\boldsymbol{\zeta}}_2\tilde{\boldsymbol{\zeta}}_1^\top\right)\ge  t \right]\le& 2\exp\left( -\Omega\left(\min\left\{\frac{t^2}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_F^2\sigma_p^4}, \frac{t}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_\text{op}\sigma_p^2}\right\}\right)\right).
	\end{aligned}
	\end{equation}
\end{proof}


\begin{lemma}\label{lemma: x squared}
	Suppose a zero-mean random vector $\mathbf{x} \in \mathbb{R}^d$ is generated as $\mathbf{x} = \mathbf{A}\boldsymbol{\zeta}$, where $\boldsymbol{\zeta}$'s each coordinate is independent, symmetric, and $\sigma_p$ sub-Gaussian with $\mathbb{E}\left[\zeta^2_{j}\right]=1$, for any $j\in[d]$. 
	Then, $\mathbf{x}$ satisfies
	\begin{align}
		\mathbb{P}\left[\left\|\mathbf{x}\right\|_2^2-\mathrm{Tr}\left(\mathbf{A}^\top\mathbf{A}\right)\ge t\right]\le& 2\exp\left( -\Omega\left(\min\left\{\frac{t^2}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_F^2\sigma_p^4}, \frac{t}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_\textnormal{op}\sigma_p^2}\right\}\right)\right).
	\end{align}
\end{lemma}
\begin{proof}
	We have
	\begin{equation}
		\begin{aligned}
			\left\|\mathbf{x}\right\|_2^2 = \left\langle \mathbf{A}\boldsymbol{\zeta}_1,\mathbf{A}\boldsymbol{\zeta}_1 \right\rangle = \boldsymbol{\zeta}_1^\top\mathbf{A}^\top\mathbf{A}\boldsymbol{\zeta}_1
			=\mathrm{Tr}\left(\mathbf{A}^\top\mathbf{A}\boldsymbol{\zeta}_1\boldsymbol{\zeta}_1^\top\right).
		\end{aligned}
	\end{equation}
	Then, expectation of $\left\|\mathbf{x}\right\|_2$ satisfies
	\begin{align}
		\mathbb{E}\left[\left\|\mathbf{x}\right\|_2^2\right] = \mathrm{Tr}\left(\mathbf{A}^\top\mathbf{A}\right).
	\end{align}
	By Bernstein's inequality, we have
	\begin{equation}
	\begin{aligned}
		\mathbb{P}\left[\left\|\mathbf{x}\right\|_2^2-\mathrm{Tr}\left(\mathbf{A}^\top\mathbf{A}\right)\ge t\right]\le& 2\exp\left( -\Omega\left(\min\left\{\frac{t^2}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_F^2\sigma_p^4}, \frac{t}{\left\|\mathbf{A}^\top\mathbf{A}\right\|_\text{op}\sigma_p^2}\right\}\right)\right).
	\end{aligned}
	\end{equation}
\end{proof}

\begin{lemma}\label{lemma: cross inner product}
	Suppose two zero-mean random vectors $\mathbf{x}_1,\mathbf{x}_2\in \mathbb{R}^d$ are generated as $\mathbf{x}_1 = \mathbf{A}\boldsymbol{\zeta}_1, \mathbf{x}_2 = \mathbf{B}\boldsymbol{\zeta}_2$, where $\boldsymbol{\zeta}_i$'s each coordinate is independent, symmetric, and $\sigma_p$ sub-Gaussian with $\mathbb{E}[\zeta^2_{i,j}]=1$, for any $i\in[2], j\in[d]$. 
	Then, $\mathbf{x}_1$ and $\mathbf{x}_2$ satisfy
\begin{equation}
	\begin{aligned}
		\mathbb{P}\left[|\left\langle\mathbf{x}_1,\mathbf{x}_2\right\rangle|\ge t\right]\le& 2\exp\left( -\Omega\left(\min\left\{\frac{t^2}{\left\|\mathbf{A}^\top\mathbf{B}\right\|_F^2}, \frac{t}{\left\|\mathbf{A}^\top\mathbf{B}\right\|_\textnormal{op}}\right\}\right)\right).
	\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
	We have
	\begin{equation}
		\begin{aligned}
			\left\langle \mathbf{x},\mathbf{y} \right\rangle= \left\langle \mathbf{A}\boldsymbol{\zeta}_1,\mathbf{B}\boldsymbol{\zeta}_2 \right\rangle = \boldsymbol{\zeta}_1^\top\mathbf{A}^\top\mathbf{B}\boldsymbol{\zeta}_2
			=\mathrm{Tr}\left(\mathbf{A}^\top\mathbf{B}\boldsymbol{\zeta}_2\boldsymbol{\zeta}_1^\top\right).
		\end{aligned}
	\end{equation}
	Then, by Bernstein's inequality, we have
\begin{equation}
	\begin{aligned}
		\mathbb{P}\left[|\left\langle\mathbf{x}_1,\mathbf{x}_2\right\rangle|\ge t\right]=&\mathbb{P}\left[\mathrm{Tr}\left(\mathbf{A}^\top\mathbf{B}\boldsymbol{\zeta}_2\boldsymbol{\zeta}_1^\top\right)\ge  t \right]\\
		\le& 2\exp\left( -\Omega\left(\min\left\{\frac{t^2}{\left\|\mathbf{A}^\top\mathbf{B}\right\|_F^2\sigma_p^4}, \frac{t}{\left\|\mathbf{A}^\top\mathbf{B}\right\|_\text{op}\sigma_p^2}\right\}\right)\right).
	\end{aligned}
\end{equation}
\end{proof}

\begin{lemma}\label{lemma: num concentration}
	Let $x_1, \cdots, x_m$ be $m$ independent zero-mean Gaussian variables.
	Denote $z_i$ as indicators for signs of $x_i$, \textit{i.e.}, for all $i \in [m]$,
	\begin{align}
		z_i = 
		\begin{cases}
			1, & x_i>0,\\
			0, & x_i\le0.
		\end{cases}
	\end{align}
	Then, we have
	\begin{align}
		\Pr\left[\sum_{i=1}^{m}z_i \ge 0.4m\right] \ge 1-\exp\left(-\frac{8}{25}m\right).
	\end{align}
\end{lemma}

\begin{proof}
	Because $z_i, i\in[m]$ are bounded in $[0,1]$, $z_i, i\in[m]$ are sub-Gaussian variables.
	By Hoeffdingâ€™s inequality, we have
	\begin{align}
		\Pr\left[m\left(\frac{1}{m}\sum_{i=1}^{m}z_i\right)\le m\left(\frac{1}{2}-\epsilon\right)\right] \le \exp\left(-\frac{2m^2\epsilon^2}{m(\frac{1}{16})}\right).
	\end{align}
	Let $\epsilon = 0.1$, we have
	\begin{align}
		\Pr\left[\sum_{i=1}^{m}z_i \le 0.4m\right] \le \exp\left(-\frac{8}{25}m\right).
	\end{align}
	Therefore, we have
	\begin{align}
		\Pr\left[\sum_{i=1}^{m}z_i \ge 0.4m\right] \ge 1 - \exp\left(-\frac{8}{25}m\right).
	\end{align}
	This completes the proof.
\end{proof}


\begin{lemma}\label{lemma: bound_increment}
	For any constant $t\in (0,1]$ and $x \in [-a,b], a,b >0$, we have 
	\begin{align}
		\log(1+t(\exp(x)-1)) \le \Gamma(x) x,
	\end{align}
	where $\Gamma(x) = \mathbb{I}(x\ge0) + \left[\frac{\log(1+t(\exp(-a)-1))}{-a}\right]\mathbb{I}(x<0)$.
\end{lemma}

\begin{proof}
	First, considering $x\ge0$, we have
	\begin{align}
		\frac{\partial\log(1+t(\exp(x)-1))}{\partial t} =& \frac{\exp(x)-1}{1+t(\exp(x)-1)}\ge0.
	\end{align}
	Thus, $\log\left(1+t(\exp(x)-1)\right) \le x, \forall x>0$.
	Second, considering $x<0$, we have
	\begin{align}
		\frac{\partial^2 \log(1+t(\exp(x)-1))}{\partial x^2} = \frac{(1-t)t\exp(x)}{[1+t(\exp(x)-1)]^2}\ge 0.
	\end{align}
	So $\log(1+t(\exp(x)-1))$ is a convex function of $x$. 
	We can conclude that
	\begin{align}
		\log(1+t(\exp(x)-1)) \le \frac{\log(1+t(\exp(-a)-1))}{-a} x, \forall x<0.
	\end{align}
	This completes the proof.
\end{proof}

\begin{lemma}\label{lemma: convex}
	With input $\mathbf{x}\in \mathbb{R}^K$, the function $f(\mathbf{x}) = -\log(\frac{\exp(x_i)}{\sum_{j\in[K]}\exp(x_j)})$ with any $i\in[K]$ is convex.
\end{lemma}
\begin{proof}
	For any $\mathbf{x}_1,\mathbf{x}_2\in\mathbb{R}^K$ and $\alpha\in [0,1]$, we have
	\begin{equation}
	\begin{aligned}
		\alpha f(\mathbf{x}_1) + (1-\alpha) f(\mathbf{x}_2) =&  -\alpha\log\left(\frac{\exp(x_{1,i})}{\sum_{j\in[K]}\exp(x_{1,j})}\right) - (1-\alpha)\log\left(\frac{\exp(x_{2,i})}{\sum_{j\in[K]}\exp(x_{2,j})}\right)\\
		=& -\log\left(\left(\frac{\exp(x_{1,i})}{\sum_{j\in[K]}\exp(x_{1,j})}\right)^\alpha \left(\frac{\exp(x_{2,i})}{\sum_{j\in[K]}\exp(x_{2,j})}\right)^{1-\alpha}\right)\\
		=& -\log\left(\frac{\exp(\alpha x_{1,i})}{(\sum_{j\in[K]}\exp(x_{1,j}))^\alpha} \frac{\exp((1-\alpha)x_{2,i})}{(\sum_{j\in[K]}\exp(x_{2,j}))^{1-\alpha}}\right)\\
		=&-\log\left(\frac{\exp(\alpha x_{1,i}+(1-\alpha)x_{2,i})}{(\sum_{j\in[K]}\exp(x_{1,j}))^\alpha(\sum_{j\in[K]}\exp(x_{2,j}))^{1-\alpha}}\right) \\
		\ge&  -\log\left(\frac{\exp(\alpha x_{1,i} + (1-\alpha)x_{2,i})}{\sum_{j\in[K]}\exp(\alpha x_{1,j}+ (1-\alpha)x_{2,j})}\right)\\
		=& f(\alpha\mathbf{x}_1 + (1-\alpha)\mathbf{x}_2).
	\end{aligned}
	\end{equation}
	This finishes the proof.
\end{proof}


\begin{lemma}\label{lemma: direction}
	A vector $\mathbf{z}$ uniformly sampled from $\mathbb{S}^{d-1}$ satisfies 
	\begin{align}
		\mathbb{P}\left[\left|\frac{1}{d}\sum_{i=1}^d\mathbb{I}(z_i)-\frac{1}{2}\right|\ge t\right] \le \exp\left(-2dt^2\right).
	\end{align}
\end{lemma}
This lemma directly follows from Hoeffding's inequality.

\begin{lemma}\label{lemma: subgaussian tail lower bound}
	For a constant $0<t<1$, a $\sigma_p$ sub-Gaussian variable $x$ with variance 1 satisfies
	\begin{align}
		\mathbb{P}\left[|x| > t\right] \ge \Omega((1-t^2)^2).
	\end{align}
\end{lemma}
\begin{proof}
Since $x$ is a sub-Gaussian variable with variance 1, we have $\mathbb{E}[x] = 1$.
Applying Paleyâ€“Zygmund inequality to $x^2$, we have
\begin{align}
	\mathbb{P}[x^2\ge t] \ge (1-t)^2 \frac{1}{\mathbb{E}[x^4]}.
\end{align}
As the fourth moment of sub-Gaussian variable $\mathbb{E}[x^4]$ is bounded by $\mathcal{O}(\sigma_p^4)$ \citep{vershynin2018high}, we have
\begin{align}
	\mathbb{P}[x^2\ge t] \ge \frac{C_4}{\sigma_p^4}(1-t)^2,
\end{align}
where $C_4$ is a constant.
This completes the proof.
\end{proof}
\begin{lemma}\label{lemma: orthant}
	Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ be a matrix with rank $M$.
	Suppose $\delta>0$ and $m \ge \Omega( \frac{\log(\frac{n}{\delta})(\lambda_{\max}^+(\mathbf{A}))^2}{((\lambda_{\min}^+(\mathbf{A}))^2)})$.
	With probability $1-\delta$, for any orthant $\mathcal{T}\in\mathbb{R}^m$ and vector $\mathbf{x}\in \mathbb{R}^n$ is a random sub-Gaussian vector with each coordinate follows $\mathcal{D}_{\xi}$, we have 
	\begin{align}
		\mathbb{P}\left[\mathbf{Ax}\in \mathcal{T}\right] \le& (0.6)^M.
	\end{align}
\end{lemma}
\begin{proof}
	Using singular value decomposition for $\mathbf{A}$, for any orthant $\mathcal{T}\in\mathbb{R}^m$, we have
        \begin{equation}
		\begin{aligned}
			\mathbb{P}\left[\mathbf{Ax}\in\mathcal{T}\right] =& \mathbb{P}\left[\mathbf{U}\mathbf{\Sigma}\mathbf{x}\in\mathcal{T}\right].
		\end{aligned}
	\end{equation}
	Without loss of generality, we assume the orthant $\mathcal{T}$ is that with all positive entries.
	Then, we have
	\begin{equation}
		\begin{aligned}
			\mathbb{P}\left[\mathbf{Ax}\in\mathcal{T}\right] \le& \mathbb{P}\left[\tilde{\mathbf{U}}\tilde{\mathbf{\Sigma}}\mathbf{x}\in\mathcal{T}\right],
		\end{aligned}
	\end{equation}
	where 
	\begin{align}
		\tilde{\mathbf{U}} = 
		\begin{bmatrix}
			\mathbf{1}\cdot \frac{1}{\sqrt{m}} &\tilde{\mathbf{U}}_2 &\cdots & \tilde{\mathbf{U}}_m
		\end{bmatrix},
	\end{align}
	and
	\begin{align}
		\tilde{\mathbf{\Sigma}} = 
		\begin{bmatrix}
			\lambda_{\max}^+(\mathbf{A})&0&\cdots & 0 & \\
			0&\lambda_{\min}^+(\mathbf{A})&\cdots&0 & \\
			\vdots&\vdots& &\vdots & \cdots\\
			0&0& \cdots & \lambda^+_{\min}(\mathbf{A})& \\
			&  & \cdots & &0
		\end{bmatrix},
	\end{align}
	Here, $\tilde{\mathbf{\Sigma}}$ is generated by replacing the non-zero singular values other than the largest one with $\lambda^+_{\min}(\mathbf{A})$.
	$\mathbf{1}$ is a vector with all one entries.
	In the following, we abbreviate $\lambda^+_{\max}(\mathbf{A})$ and $\lambda^+_{\min}(\mathbf{A})$ as $\lambda^+_{\max}$ and $\lambda^+_{\min}$ for convenience.
	Then, we have
	\begin{align}
		\tilde{\mathbf{U}}\tilde{\mathbf{\Sigma}}\mathbf{x} = \lambda_{\max}^+\cdot \frac{1}{\sqrt{m}}\cdot\mathbf{1}\cdot x_1 + \lambda_{\min}^+\sum_{i=2}^{M}x_i\tilde{\mathbf{U}}_i.
	\end{align}
	Here, as each coordinate of $\mathbf{x}$ is generate from $\mathcal{D}_{\xi}$, by Lemma \ref{lemma: sg}, we have
	\begin{align}
		|x_i|\le \sqrt{2\sigma_p^2\log(\frac{2n}{\delta})},
	\end{align}
	 with probability $1-\delta$.
	Then, we have
	\begin{equation}
		\begin{aligned}
			\mathbb{P}\left[\tilde{\mathbf{U}}\tilde{\mathbf{\Sigma}}\mathbf{x}\in\mathcal{T}\right]=& \mathbb{P}\left[ (\lambda_{\max}^+-\lambda_{\min}^+)\cdot \frac{1}{\sqrt{m}}\cdot\mathbf{1}\cdot x_1 + \lambda_{\min}^+\sum_{i=1}^{M}x_i\tilde{\mathbf{U}}_i\in \mathcal{T}\right]\\
			 \le& \mathbb{P}\left[\lambda_{\max}^+\cdot \frac{1}{\sqrt{m}}\cdot\mathbf{1}\cdot \sqrt{2\sigma_p^2\log(\frac{2n}{\delta})} + \lambda_{\min}^+\sum_{i=1}^{M}x_i\tilde{\mathbf{U}}_i\in \mathcal{T}\right]\\
			=&\mathbb{P}\left[\mathbf{1}\cdot \frac{\sqrt{2\sigma_p^2\log(\frac{2n}{\delta})}\lambda^+_{\max}}{\sqrt{m}} + \lambda^+_{\min}\mathbf{x}\in \mathcal{T}\right].
		\end{aligned}
	\end{equation}
	As the entries of $\mathbf{x}$ are independent, we can bound each entry independently.
	Then, when $m \ge \Omega( \frac{\log(\frac{n}{\delta})(\lambda_{\max}^+)^2}{((\lambda_{\min}^+)^2)})$, we have
	\begin{align}
		&\mathbb{P}\left[\lambda_{\min}^+x_i +  \frac{\sqrt{2\sigma_p^2\log(\frac{2n}{\delta})}\lambda^+_{\max}}{\sqrt{m}}<0\right]
		\ge 0.4,
	\end{align}
	by the property of $\mathcal{D}_{\xi}$, resulting in
	\begin{align}
		\mathbb{P}\left[\tilde{\mathbf{U}}\tilde{\mathbf{\Sigma}}\mathbf{x}\in\mathcal{T}\right] \le& (0.6)^M.
	\end{align}
	This completes the proof.
\end{proof}
\begin{lemma}\label{lemma: num_orthrant}
	Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ be a matrix with rank $M$. 
	Suppose $m \ge \Omega(\frac{\log(\frac{n}{\delta})(\lambda_{\max}^+(\mathbf{A}))^2}{((\lambda_{\min}^+(\mathbf{A}))^2)})$.
	With probability $1-\delta$, we have 
	\begin{align}
		\mathbb{P}\left[\sum_{i=1}^{m}\mathbb{I}((\mathbf{Ax})_{i}\le0)\ge 0.9m\right]\le \exp(-0.07m).
	\end{align}
\end{lemma}
\begin{proof}
	By Lemma \ref{lemma: direction}, the number $N_o$ of orthants that have more than $0.9m$ negative entries satisfies

	\begin{align}
		N_o \le 2^m \cdot \exp(-0.32m) = \exp((\log2-0.32)m).
	\end{align}
	Then, by Lemma \ref{lemma: orthant}, we have
	\begin{equation}
	\begin{aligned}
		\mathbb{P}\left[\sum_{i=1}^{m}\mathbb{I}((\mathbf{Ax})_{i}\le0)\ge 0.9m\right] \le& 0.6^M \cdot \exp((\log2-0.32)m)\\
		=&\exp(\log(0.6)M+(\log 2-0.32)m)\\
		\le& \exp(-0.51M+0.38m).
	\end{aligned}
	\end{equation}
As long as $M\ge 0.9m$, we have 
\begin{align}
	\mathbb{P}\left[\sum_{i=1}^{m}\mathbb{I}((\mathbf{Ax})_{i}\le0)\ge 0.9m\right] \le&\exp\left(-0.07m\right).
\end{align}
This completes the proof.
\end{proof}
\begin{lemma}\label{lemma: self and cross inner product}
	Suppose that $\delta > 0$ and $\mathrm{Tr}(\mathbf{A}_i^\top\mathbf{A}_i) = \Omega\left(\max\left\{\left(\left\|\mathbf{A}^\top_i\mathbf{A}_j\right\|_F^2\sigma_p^4\log(\frac{6n}{\delta})\right)^{1/2}, \left\|\mathbf{A}_i^\top\mathbf{A}_j\right\|_\textnormal{op}\sigma_p^2\log(\frac{6n}{\delta})\right\}\right)$. 
	For all $i,j\in[K]$, with probability $1-\delta$, we have
	\begin{align}
		\frac{1}{2}\mathrm{Tr}(\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}) \le& \left\|\boldsymbol{\xi}_i\right\|_2^2 \le \frac{3}{2}\mathrm{Tr}(\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}),\\
		|\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}_j\rangle|\le& \mathcal{O}\left(\max\left\{\left(\left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_j}\right\|_F^2\log(\frac{6n^2}{\delta})\right)^{1/2}, \left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_j}\right\|_\textnormal{op}\log(\frac{6n^2}{\delta})\right\}\right).
	\end{align}
\end{lemma}
\begin{proof}
	By Lemma \ref{lemma: x squared}, with probability at least $1-\frac{\delta}{3n}$, there exists a constant $C_1 > 0$ such that
	\begin{align}\nonumber
		\left\|\boldsymbol{\xi}_i\right\|_2^2 \le \mathrm{Tr}(\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}) +  \max\left\{\left(\left\|\mathbf{A}^\top_{y_i}\mathbf{A}_{y_i}\right\|_F^2\sigma_p^4\frac{1}{C_1}\log(\frac{6n}{\delta})\right)^{1/2}, \left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}\right\|_\text{op}\sigma_p^2\frac{1}{C_1}\log(\frac{6n}{\delta})\right\},
	\end{align}
	and 
	\begin{align}\nonumber
		\left\|\boldsymbol{\xi}_{y_i}\right\|_2^2 \ge \mathrm{Tr}(\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}) - \max\left\{\left(\left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}\right\|_F^2\sigma_p^4\frac{1}{C_1}\log(\frac{6n}{\delta})\right)^{1/2}, \left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i}\right\|_\text{op}\sigma_p^2\frac{1}{C_1}\log(\frac{6n}{\delta})\right\}.
	\end{align}
	In addition, by Lemmas \ref{lemma: cross inner product} and \ref{lemma: inner product}, with probability at least $1-\frac{\delta}{3n^2}$, there exists a constant $C_2 >0$ such that
	\begin{align}\nonumber
		|\langle \boldsymbol{\xi}_{y_i}, \boldsymbol{\xi}_{y_j} \rangle| \le \max\left\{\left(\left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_j}\right\|_F^2\sigma_p^4\frac{1}{C_2}\log(\frac{6n^2}{\delta})\right)^{1/2}, \left\|\mathbf{A}_{y_i}^\top\mathbf{A}_{y_j}\right\|_\text{op}\sigma_p^2\frac{1}{C_2}\log(\frac{6n^2}{\delta})\right\}.
	\end{align}
	Applying $\sigma_p=\Theta(1)$ finishes the proof.
\end{proof}
\begin{lemma}\label{lemma: init order}
	Suppose that $d = \Omega(\log(\frac{mn}{\delta}))$ and $m = \Omega(\log(1/\delta))$. 
	With probability at least $1-\delta$, for all $r\in[m], j \in [2], i\in[n]$, 
	\begin{align}\label{equ: inner_wu}
		&|\langle \mathbf{w}_{j,r}^{(0)}, \mathbf{u}_k\rangle| \le \sqrt{2\log\left(\frac{4Km}{\delta}\right)}\left\|\mathbf{u}_k\right\|_2\sigma_0,\\\label{equ: inner_wxi}
		&|\langle \mathbf{w}_{j,r}^{(0)}, \boldsymbol{\xi}_i\rangle| \le \mathcal{O}\left(\log\left(\frac{Km}{\delta}\right)\left\|\mathbf{A}_{y_i}\right\|_F\sigma_0\right).
	\end{align} 
\end{lemma}
\begin{proof}
	We prove the first bound (\ref{equ: inner_wu}) with Hoeffding's inequality.
	For all $j\in[K], r\in[m], i\in[n]$, with probability $1-\frac{\delta}{(2Km)}$,
	\begin{align}
		|\langle \mathbf{w}_{j,r}^{(0)}, \mathbf{u}_k\rangle| \le \sqrt{2\log\left(\frac{4Km}{\delta}\right)}\left\|\mathbf{u}_k\right\|_2\sigma_0.
	\end{align}
	Then, we prove the second bound (\ref{equ: inner_wxi}) leveraging Lemma \ref{lemma: cross inner product}.
	For all $j\in[K], r\in[m], i\in[n]$, with probability $1-\frac{\delta}{(2Km)}$,
	\begin{equation}
	\begin{aligned}
		|\langle\mathbf{w}^{(0)}_{j,r}, \boldsymbol{\xi}_i\rangle| \le& \mathcal{O}\left(\max\left\{\sqrt{\log\left(\frac{Km}{\delta}\right)}\left\|\mathbf{A}_{y_i}\right\|_F,\log\left(\frac{Km}{\delta}\right)\left\|\mathbf{A}_{y_i}\right\|_\text{op}\right\}\sigma_0\right)\\
		\le&\mathcal{O}\left(\log\left(\frac{Km}{\delta}\right)\left\|\mathbf{A}_{y_i}\right\|_F\sigma_0\right).
	\end{aligned}
	\end{equation}
\end{proof}
\begin{lemma}\label{lemma: set init}
	Suppose $\delta > 0$ and $m \ge \Omega(\log(\frac{n}{\delta}))$.
	With probability at least $1-\delta$, we have
	\begin{align}
		|\mathcal{S}_i^{(0)}| \ge 0.4m.
	\end{align}
\end{lemma}
Lemma \ref{lemma: set init} follows from Lemma \ref{lemma: num concentration} and union bound.
\begin{lemma}\label{lemma: data correlation}
	Suppose $\delta>0$ and $n\ge \Omega(\log(\frac{m}{\delta}))$.
	For any neuron $\mathbf{w}_{j,r}^{(0)}, j\in[2], r\in[m]$, with probability at least $1-\delta$, we have
	\begin{align}
		\sum_{i=1}^{n}\mathbb{I}(\langle\mathbf{w}_{j,r}^{(0)}, \boldsymbol{\xi}_i\rangle)\ge 0.4n.
	\end{align}
\end{lemma}
Lemma \ref{lemma: data correlation} follows from Lemma \ref{lemma: num concentration} and union bound.

\section{Two-stage training loss analysis}\label{appendix:training_loss}
In this section, we analyze the training loss. 
These results are based on the high probability conclusions in Appendix \ref{appendix:lemmas}.                                                         
We divide the convergence of neural networks trained on each data into two stages.
The training dynamics is at stage 1 at first and then enters stage 2 afterward.
In Stage 1, we consider that the training data $(\mathbf{x},y)\in\mathcal{S}$ satisfies $1-\text{logit}_{y}(\mathbf{W}, \mathbf{x}) = \Theta(1)$.

\subsection{Network gradient}
The full-batch gradient on the neuron $\mathbf{w}_{k,r}$ at iteration $t$ is 
\begin{equation}
	\begin{aligned}
		\nabla_{\mathbf{w}_{k,r}^{(t)}} \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)=& -\frac{1}{mn}\sum_{(\mathbf{x},y)\in \mathcal{S}}\left[\mathbb{I}\left(y=k\right)(1-\text{logit}_k(\mathbf{W}^{(t)},\mathbf{x}))\sum_{j=1}^{2}\sigma'\left(\langle \mathbf{w}_{k,r}^{(t)}, \mathbf{x}^{(j)}\rangle\right)\mathbf{x}^{(j)}\right]\\
		&+ \frac{1}{mn}\sum_{(\mathbf{x},y)\in \mathcal{S}}\left[\mathbb{I}\left(y\neq k\right)\text{logit}_k(\mathbf{W}^{(t)},\mathbf{x})\sum_{j=1}^{2}\sigma'\left(\langle \mathbf{w}_{k,r}^{(t)}, \mathbf{x}^{(j)}\rangle\right)\mathbf{x}^{(j)}\right].
	\end{aligned}
\end{equation}

\subsection{Training loss bounds at stage 1}
First, we consider the training loss dynamics of stage 1 :  
For any data $(\mathbf{x},y) \in \mathcal{S}$, we have $\mathcal{L}(\mathbf{W}^{(t')},\mathbf{x},y) = \Theta(1)$.
First, we characterize the training loss.
\begin{equation}\label{equ: loss}
	\begin{aligned}
		&\mathcal{L}(\mathbf{W}^{(t+1)},\mathbf{x},y) - \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)\\
		=&-\log\left(\text{prob}_y\left(\mathbf{W}^{(t+1)},\mathbf{x}\right)\right) + \log\left(\text{prob}_y\left(\mathbf{W}^{(t)},\mathbf{x}\right)\right)\\
		=& \log\left(\frac{\text{prob}_y\left(\mathbf{W}^{(t)},\mathbf{x}\right)}{\text{prob}_y\left(\mathbf{W}^{(t+1)},\mathbf{x}\right)}\right)\\
		=& \log\left(\frac{\frac{\exp\left(F_y^{(t)}\left(\mathbf{x}\right)\right)}{\left(\exp\left(F_{y}^{(t)}\left(\mathbf{x}\right)\right)+\sum_{j\neq y}\exp\left(F_{j}^{(t)}\left(\mathbf{x}\right)\right)\right)}}{\frac{\exp\left(F_y^{(t+1)}\left(\mathbf{x}\right)\right)}{\left(\exp\left(F_y^{(t+1)}\left(\mathbf{x}\right)\right)+\sum_{j\neq y}\exp\left(F_{j}^{(t+1)}\left(\mathbf{x}\right)\right)\right)}}\right)\\
		=& \log\left(\frac{1+\sum_{j\neq y}\exp\left(F_{j}^{(t)}\left(\mathbf{x}\right)-F_y^{(t)}\left(\mathbf{x}\right)\right)\exp\left(\Delta_{j}^{(t)}\left(\mathbf{x}\right)-\Delta_y^{(t)}\left(\mathbf{x}\right)\right)}{1+\sum_{j\neq y}\exp\left(F_{j}^{(t)}\left(\mathbf{x}\right)-F_y^{(t)}\left(\mathbf{x}\right)\right)}\right)\\
		=& \log\left(1+\frac{\sum_{j\neq y}\exp\left(F_{j}^{(t)}\left(\mathbf{x}\right)-F_y^{(t)}\left(\mathbf{x}\right)\right)\left(\exp\left(\Delta_{j}^{(t)}\left(\mathbf{x}\right)-\Delta_y^{(t)}\left(\mathbf{x}\right)\right)-1\right)}{1+\sum_{j\neq y}\exp\left(F_{j}^{(t)}\left(\mathbf{x}\right)-F_y^{(t)}\left(\mathbf{x}\right)\right)}\right)\\
		\le& \log\left(1+ \left(1-\text{prob}_y\left(\mathbf{W}^{(t)},\mathbf{x}\right)\right)\max_{j\neq y}\left(\exp\left(\Delta_{j}^{(t)}\left(\mathbf{x}\right)-\Delta_y^{(t)}\left(\mathbf{x}\right)\right)-1\right)\right),
	\end{aligned}
\end{equation}
where $\Delta_y^{(t)}\left(\mathbf{x}\right) = F_y^{(t+1)}\left(\mathbf{x}\right) - F_y^{(t)}\left(\mathbf{x}\right), \Delta_{j}^{(t)}\left(\mathbf{x}\right) = F_{j}^{(t+1)}\left(\mathbf{x}\right) - F_{j}^{(t)}\left(\mathbf{x}\right)$.

By Lemma \ref{lemma: bound_increment}, we have
\begin{align}\label{equ: loss_increment}
	\mathcal{L}\left(\mathbf{W}^{(t+1)},\mathbf{x},y\right) - \mathcal{L}\left(\mathbf{W}^{(t)},\mathbf{x},y\right) \le \Theta (1)\cdot \max_{j\neq y}\left(\Delta_{j}^{(t)}(\mathbf{x})-\Delta_{y}^{(t)}(\mathbf{x})\right).
\end{align}

To measure how training loss changes over iterations, we need to characterize the change of $\Delta^{(t)}_{j}(\mathbf{x})-\Delta_y^{(t)}(\mathbf{x})$.

\subsubsection{Upper bound of $\Delta_{j}^{(t)}(\mathbf{x})-\Delta_y^{(t)}(\mathbf{x})$}
We first rearrange $\Delta_{j}^{(t)}(\mathbf{x}) - \Delta_y^{(t)}(\mathbf{x})$ as follows.
\begin{equation}\label{equ: delta}
	\begin{aligned}
		&\Delta_{j}^{(t)}\left(\mathbf{x}\right)-\Delta_y^{(t)}\left(\mathbf{x}\right)\\ =& \frac{1}{m}\sum_{r=1}^{m} \sum_{i=1}^{2}\left[\sigma\left(
		\left\langle \mathbf{w}^{(t+1)}_{j,r},\mathbf{x}^{(i)} \right\rangle\right) - \sigma\left(\left\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}^{(j)} \right\rangle\right)\right]\\
		&- \frac{1}{m}\sum_{r=1}^{m}\sum_{i=1}^{2} \left[\sigma\left(
		\left\langle \mathbf{w}^{(t+1)}_{y,r},\mathbf{x}^{(i)} \right\rangle\right) - \sigma\left(\left\langle\mathbf{w}^{(t)}_{y,r},\mathbf{x}^{(i)} \right\rangle\right)\right]\\
		=& \underbrace{\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(
			\left\langle \mathbf{w}^{(t+1)}_{j,r},\mathbf{u}_{y}\right\rangle\right) - \sigma\left(
			\left\langle \mathbf{w}^{(t)}_{j,r},\mathbf{u}_{y}\right\rangle\right)\right]}_{A} \!+\! \underbrace{\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(
			\left\langle \mathbf{w}^{(t+1)}_{j,r},\boldsymbol{\xi}\right\rangle \right) - \sigma\left(
			\left\langle \mathbf{w}^{(t)}_{j,r},\boldsymbol{\xi}\right\rangle \right)\right]}_{B}\\
		&-\! \underbrace{\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(
			\left\langle \mathbf{w}^{(t+1)}_{y,r},\mathbf{u}_{y}\right\rangle\right) \!-\! \sigma\left(
			\left\langle \mathbf{w}^{(t)}_{y,r},\mathbf{u}_{y}\right\rangle\right)\right]}_{C} \!-\! \underbrace{\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(
			\left\langle \mathbf{w}^{(t+1)}_{y,r},\boldsymbol{\xi}\right\rangle \right) \!-\! \sigma\left(
			\left\langle \mathbf{w}^{(t)}_{y,r},\boldsymbol{\xi}\right\rangle \right)\right]}_{D},
	\end{aligned}
\end{equation}
We then bound $A, B, C, D$ in Stage 1.

\paragraph{Bound of $A$}
\begin{equation}\label{equ: bound_A}
	\begin{aligned}
		A =& \frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{j,r}^{(t)}\!-\!\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{y}}\sigma'(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{u}_y\rangle)\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)\mathbf{u}_y,\mathbf{u}_y\rangle\right)\right]\\
		&-\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{u}_y\rangle\right)\right]\\
		\le& 0,
	\end{aligned}
\end{equation}
where the inequality is by the fact that $\langle\mathbf{u}_y,\mathbf{u}_y\rangle \ge 0$.
\paragraph{Bound of $B$}
\begin{equation}\label{equ: bound_B}
	\begin{aligned}
		B =& \frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\left\langle\mathbf{w}_{j,r}^{(t)}-\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{j}}\sigma'\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)\boldsymbol{\xi}_i\right.\right.\right.\\
		&+\left.\left.\left.\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{j}}\sigma'\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\left(1-\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle\right)\right]-\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}\rangle\right)\right]\\
		\le& \frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\left\langle\mathbf{w}_{j,r}^{(t)}, \boldsymbol{\xi}\right\rangle-\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{j}}\sigma'\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)\boldsymbol{\xi}_i\right.\right.\\
		&+\left.\left.\left.\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{j}}\sigma'\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\left(1-\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle\right)\right]-\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}\rangle\right)\right]\\
		\le& \frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{j}}\sigma'\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)|\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\rangle|\\
		&+ \frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_j\backslash (\mathbf{x},y)}\sigma'\left(\langle\mathbf{w}_{j,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\left(1-\text{logit}_{j}(\mathbf{W}^{(t)},\mathbf{x}_i)\right)|\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\rangle|,
	\end{aligned}
\end{equation}
where the inequality is by the 1-Lipschitz property of the ReLU function.
\paragraph{Bound of $C$}
\begin{equation}\label{equ: bound_C}
	\begin{aligned}
		C =& \frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\left\langle\mathbf{w}_{y,r}^{(t)}+\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{y}}\sigma'(\langle\mathbf{w}_{y,r}^{(t)},\mathbf{u}_y\rangle)\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\mathbf{u}_y, \mathbf{u}_y\right\rangle\right)\right]\\
		&-\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{y,r}^{(t)},\mathbf{u}_y\rangle\right)\right]\\
		\ge& \frac{2\eta}{5mn}\sum_{(\mathbf{x},y)\in \mathcal{S}_y} \left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\left\|\mathbf{u}_y\right\|_2^2,
	\end{aligned}
\end{equation}
where the inequality is by the fact that $\left\|\mathbf{u}_y\right\|_2^2\ge 0$.

\paragraph{Bound of $D$}
\begin{equation}\label{equ: bound_D}
	\begin{aligned}
		D =& \frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\left\langle\mathbf{w}_{y,r}^{(t)}-\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{y}}\sigma'(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}_i\rangle)\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\boldsymbol{\xi}_i\right.\right.\right.\\
		&+\left.\left.\left.\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{y}}\sigma'\left(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle\right)\right]\\
		&-\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}\rangle\right)\right]\\
		\ge& \frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\left\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}\right\rangle-\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{y}}\sigma'(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}_i\rangle)\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle|\right.\right.\\
		&+\frac{\eta}{mn}\sigma'\left(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}\rangle\right)\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x})\right)\left\|\boldsymbol{\xi}\right\|_2^2\\
		&-\left.\left.\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{y}\backslash(\mathbf{x},y)}\sigma'\left(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}_i\rangle\right)\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle|\right)\right]\\
		&-\frac{1}{m}\sum_{r=1}^{m}\left[\sigma\left(\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}_j\rangle\right)\right] \\
 		\ge& -\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{y}}\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle|+ \frac{2\eta}{5mn}\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x})\right)\left\|\boldsymbol{\xi}\right\|_2^2\\
 		&-\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{y}\backslash(\mathbf{x},y)}\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle|\\
        \ge& 0,
	\end{aligned}
\end{equation}
where the last inequality is by that fact that the incremental term is larger than zero if a neuron is activated, by Lemmas \ref{lemma: ratio}, \ref{lemma: set init}, at least 0.4$m$ neurons are activated, and $\sigma'(\cdot)\le 1$.

Substituting $(\ref{equ: bound_A}), (\ref{equ: bound_B}), (\ref{equ: bound_C}), (\ref{equ: bound_D})$ into $(\ref{equ: delta})$, by Lemma \ref{lemma: self and cross inner product} and Condition \ref{condition}, we have
\begin{equation}\label{equ: delta_total}
\begin{aligned}
	&\Delta_{j}^{(t)}(\mathbf{x}) - \Delta_{y}^{(t)}(\mathbf{x})\\
    \le& - \frac{2\eta}{5mn} \sum_{(\mathbf{x},y)\in \mathcal{S}_y} \left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\left\|\mathbf{u}_y\right\|_2^2\\
    &+ \frac{2\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash \mathcal{S}_{y}}\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle| - \frac{2\eta}{5mn}\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x})\right)\left\|\boldsymbol{\xi}\right\|_2^2\\
 		&+\frac{2\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}_{y}\backslash(\mathbf{x},y)}\left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}\right\rangle|\\
        \le& - \frac{2\eta}{5mn} \sum_{(\mathbf{x},y)\in \mathcal{S}_y} \left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\left\|\mathbf{u}_y\right\|_2^2.
\end{aligned}
\end{equation}

Combining $(\ref{equ: delta_total})$ and $(\ref{equ: loss_increment})$, for any $(\mathbf{x},y)\in \mathcal{S}$ we have
\begin{equation}
	\begin{aligned}
		\mathcal{L}(\mathbf{W}^{(t+1)},\mathbf{x},y) \le& \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y) - \frac{2\eta}{5mn} \sum_{(\mathbf{x}_i,y_i)\in \mathcal{S}_y} \left(1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}_i)\right)\left\|\mathbf{u}_y\right\|_2^2\\
		\overset{(a)}{=}&\left(1-\Theta\left(\frac{\eta}{mn}|\mathcal{S}_y|\left\|\mathbf{u}_y\right\|_2^2\right)\right)\mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y),
	\end{aligned}
\end{equation}
where $(a)$ is obtained from Lemma \ref{lemma: ratio}.
This finishes the training loss analysis in Stage 1.

\subsubsection{Analysis of stage 2}
In stage 2, the loss of certain data is no longer $\Theta(1)$.

First, let $\mathbf{W}^*$ be 
\begin{align}\label{equ:W_star}
	\mathbf{w}_{j,r}^* = 2.5\log\left(\frac{4(K-1)}{\epsilon}\right)\frac{\mathbf{u}_j}{\left\|\mathbf{u}_j\right\|_2^2},
\end{align}
where $\epsilon > 0$ is a small constant.
We have
\begin{equation}
	\begin{aligned}
		&\left\|\mathbf{W}^{(t)}-\mathbf{W}^*\right\|_F^2 - \left\|\mathbf{W}^{(t+1)}-\mathbf{W}^*\right\|_F^2
		= \underbrace{2\eta\left\langle \nabla \mathcal{L}_S(\mathbf{W}^{(t)}), \mathbf{W}^{(t)}-\mathbf{W}^*\right\rangle}_{A} - \underbrace{\eta^2\left\|\nabla \mathcal{L}_S(\mathbf{W}^{(t)})\right\|_F^2}_{B}.
	\end{aligned}
\end{equation}

For the first term $A$, we have
\begin{equation}
	\begin{aligned}
		A=&2\eta\left\langle \nabla \mathcal{L}_S(\mathbf{W}^{(t)}), \mathbf{W}^{(t)}-\mathbf{W}^*\right\rangle\\
		=& \frac{2\eta}{n}\sum_{(\mathbf{x},y)\in \mathcal{S}}  \left\langle\nabla \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y), \mathbf{W}^{(t)}-\mathbf{W}^*\right\rangle\\
		=& \frac{2\eta}{n} \sum_{(\mathbf{x},y)\in \mathcal{S}} \sum_{k\in[K]}\left\langle \frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k} \nabla F_k(\mathbf{W}^{(t)},\mathbf{x},y), \mathbf{W}^{(t)} - \mathbf{W}^*\right\rangle\\
		=& \frac{2\eta}{n} \sum_{(\mathbf{x},y)\in \mathcal{S}}\sum_{k\in[K]}\frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k}\left\langle\nabla F_k(\mathbf{W}^{(t)},\mathbf{x},y), \mathbf{W}^{(t)}\right\rangle\\
		&- \frac{2\eta}{n} \sum_{(\mathbf{x},y)\in \mathcal{S}}\sum_{k\in [K]}\frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k}\left\langle\nabla F_k(\mathbf{W}^{(t)}, \mathbf{x}, y), \mathbf{W}^*\right\rangle\\
		=& \frac{2\eta}{n}\sum_{(\mathbf{x},y)\in \mathcal{S}}\sum_{k\in [K]} \frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k} \left(F_k(\mathbf{W}^{(t)},\mathbf{x},y) - \underbrace{\left\langle \nabla F_k(\mathbf{W}^{(t)}, \mathbf{x}, y) , \mathbf{W}^* \right\rangle}_{C} \right),
	\end{aligned}
\end{equation}
where the last equality is based on the homogeneity of $F_k(\cdot)$ for any $k \in[K]$.
Then, we need to bound $C$.
First, for any $i,k\in[K]$, we have
\begin{equation}
	\begin{aligned}
		\nabla_{\mathbf{w}_{k,r}^{(t)}} F_i(\mathbf{W}^{(t)},\mathbf{x},y) =& \frac{1}{m}\left(\sigma'\left(\left\langle \mathbf{w}_{k,r}^{(t)}, \boldsymbol{\xi} \right\rangle\right)\boldsymbol{\xi} + \sigma'\left(\left\langle\mathbf{w}_{k,r}^{(t)}, \mathbf{u}_y\right\rangle
		\right)\mathbf{u}_y\right)\mathbb{I}(k=i).
	\end{aligned}
\end{equation}
By the definition of $\mathbf{W}^*$ in (\ref{equ:W_star}), we can get the components of $C$ as
\begin{align}\label{equ:inner product_C}
	\left\langle \nabla F_i(\mathbf{W}^{(t)},\mathbf{x},y), \mathbf{W}^*\right\rangle =
	\begin{cases} 				\oldfrac{2.5\log\left(\frac{4(K-1)}{\epsilon}\right)}{m}\sum_{r=1}^{m}\sigma'\left(\left\langle\mathbf{w}_{i,r}^{(t)},\mathbf{u}_y\right\rangle\right)\left\|\mathbf{u}_y\right\|_2^2  &\text{if } i=y,\\
	0 &\text{if } i\neq y.
	\end{cases}
\end{align}

Next, we bound $B$.
\begin{equation}\label{equ: B1}
	\begin{aligned}
		B =& \eta^2 \left\|\nabla \mathcal{L}_S(\mathbf{W}^{(t)})\right\|_F^2 \le \eta^2 \left[ \frac{1}{n} \sum_{i=1}^{n} \left\|\sum_{k\in[K]}\frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k} \nabla F_k(\mathbf{W}^{(t)},\mathbf{x},y)\right\|_F\right]^2\\
		\le& \eta^2 \left[ \frac{1}{n} \sum_{i=1}^{n}\sum_{k\in[K]}\left|\frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k}\right| \left\| \nabla F_k(\mathbf{W}^{(t)},\mathbf{x},y)\right\|_F\right]^2,
	\end{aligned}
\end{equation}
where the first and the second inequalities are all based on triangle inequality.
For any $k\in[K]$ and $(\mathbf{x},y)\in\mathcal{S}$, we have
\begin{equation}\label{equ: B2}
	\begin{aligned}
		\left\| \nabla F_k(\mathbf{W}^{(t)},\mathbf{x},y)\right\|_F =& \left\| \sigma'\left(\left\langle \mathbf{w}_{k,r}^{(t)}, \mathbf{u}_y \right\rangle\right)\mathbf{u}_y + \sigma'\left(\left\langle \mathbf{w}^{(t)}_{k,r}, \boldsymbol{\xi}\right\rangle\right)\boldsymbol{\xi}\right\|_2\\
		\le& \left\|\mathbf{u}_y\right\|_2 + \left\|\boldsymbol{\xi}\right\|_2\\
		\le& \left\|\mathbf{u}_y\right\|_2 + \sqrt{\frac{3}{2}\mathrm{Tr}\left(\mathbf{A}_y^\top\mathbf{A}_y\right)},
	\end{aligned}
\end{equation}
where the first inequality is by triangle inequality and the fact that $\sigma'\left(\left\langle \mathbf{w}_{k,r}^{(t)}, \mathbf{u}_y \right\rangle\right), \sigma'\left(\left\langle \mathbf{w}^{(t)}_{k,r}, \boldsymbol{\xi}\right\rangle\right) \le 1$ and the second inequality is obtained from Lemma \ref{lemma: self and cross inner product}.

Then, we can bound $B$ as
\begin{equation}
	\begin{aligned}
		B \le&\eta^2 \left[ \frac{1}{n} \sum_{i=1}^{n}\sum_{k\in[K]}\left|\frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_k}\right| \left\| \nabla F_k(\mathbf{W}^{(t)},\mathbf{x},y)\right\|_F\right]^2\\
		\le& \eta^2 \left(\max_{k\in[K]}\left(\left\|\mathbf{u}_k\right\|_2 + \sqrt{\frac{3}{2}\mathrm{Tr}\left(\mathbf{A}_k^\top\mathbf{A}_k\right)}\right)\right)^2\cdot 4\left[\frac{1}{n}\sum_{i=1}^{n} \left(1-\text{logit}_y\left(\mathbf{W}^{(t)},\mathbf{x}\right)\right)\right]^2\\
		\le& 4\eta^2 \left(\max_{k\in[K]}\left(\left\|\mathbf{u}_k\right\|_2 + \sqrt{\frac{3}{2}\mathrm{Tr}\left(\mathbf{A}_k^\top\mathbf{A}_k\right)}\right)\right)^2 \cdot \mathcal{L}_S(\mathbf{W}^{(t)}),
	\end{aligned}
\end{equation}
where the first inequality is by (\ref{equ: B1}), the second inequality is by $(\ref{equ: B2})$ and the third inequality is by Jensen's inequality.

Combining them together, we have
\begin{equation}
	\begin{aligned}
		A =& \frac{2\eta}{n}\sum_{(\mathbf{x},y)\in \mathcal{S}} \sum_{k\in[K]}\frac{\partial \mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y)}{\partial F_i} \left(F_i(\mathbf{W}^{(t)},\mathbf{x},y) - \left\langle \nabla F_i(\mathbf{W}^{(t)},\mathbf{x},y), \mathbf{W}^*\right\rangle \right)\\
		\ge& \frac{2\eta}{n} \sum_{(\mathbf{x},y)\in \mathcal{S}} \left(\mathcal{L}(\mathbf{W}^{(t)},\mathbf{x},y) - \frac{\epsilon}{4}\right)\\
		=& 2\eta \left(\mathcal{L}_S(\mathbf{W}^{(t)}) - \frac{\epsilon}{4}\right),
	\end{aligned}
\end{equation}
where the first inequality is based on the convex property (Lemma \ref{lemma: convex}) of the cross-entropy function and (\ref{equ:inner product_C}).
Hence, we have
\begin{equation}
	\begin{aligned}
		&\left\|\mathbf{W}^{(t)}-\mathbf{W}^*\right\|_F^2 - \left\|\mathbf{W}^{(t+1)}-\mathbf{W}^*\right\|_F^2\\
		\ge& 2\eta \left(\mathcal{L}_S(\mathbf{W}^{(t)}) - \frac{\epsilon}{4}\right) -4\eta^2 \left(\max_{k\in[K]}\left(\left\|\mathbf{u}_k\right\|_2 + \sqrt{\frac{3}{2}\mathrm{Tr}\left(\mathbf{A}_k^\top\mathbf{A}_k\right)}\right)\right)^2 \cdot \mathcal{L}_S(\mathbf{W}^{(t)})\\
		\ge& \eta\mathcal{L}_S(\mathbf{W}^{(t)}) -\frac{\eta\epsilon}{2},
	\end{aligned}
\end{equation}
where the first inequality is by the bounds of $A$ and $B$ and the second inequality is obtained by the condition that $\eta \le \left(4\left(\max_{k\in[K]}\left(\left\|\mathbf{u}_k\right\|_2 + \sqrt{1.5\mathrm{Tr}\left(\mathbf{A}_k^\top\mathbf{A}_k\right)}\right)\right)^2\right)^{-1}$.

Taking summation over all the iterations yields
\begin{equation}
	\begin{aligned}		\sum_{t=T_0}^{T}\eta\mathcal{L}_S(\mathbf{W}^{(t)}) \le& \left\|\mathbf{W}^{(T_0)}-\mathbf{W}^*\right\|_F^2 + \frac{(T-T_0+1)\eta\epsilon}{2}\\
		\frac{1}{T-T_0+1}\sum_{t=T_0}^{T}\mathcal{L}_S(\mathbf{W}^{(t)})\le& \frac{\left\|\mathbf{W}^{(T_0)}-\mathbf{W}^*\right\|_F^2}{\eta (T-T_0+1)} + \frac{\epsilon}{2}.
	\end{aligned}
\end{equation}
By the definition of $\mathbf{W}^*$, we have
\begin{equation}
\begin{aligned}
    \left\|\mathbf{W}^{(T_1)}-\mathbf{W}^*\right\|_F \le& \left\|\mathbf{W}^{(T_1)}-\mathbf{W}^{(0)}\right\|_F + \left\|\mathbf{0}-\mathbf{W}^{(0)}\right\|_F + \left\|\mathbf{0} - \mathbf{W}^*\right\|_F.
\end{aligned}
\end{equation}
For $\left\|\mathbf{W}^{(T_1)}-\mathbf{W}^{(0)}\right\|_F$, we have
\begin{align}
	\left\|\mathbf{W}^{(T_1)}-\mathbf{W}^{(0)}\right\|_F \le  \mathcal{O}\left(T_1\frac{\eta}{m}\max_{i,j\in[K]}\{\left\|\mathbf{u}_i\right\|_2 + \mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)\}\right) = \tilde{\mathcal{O}}\left(n\max_{j\in[K]}\{\mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)\}\right).
\end{align}
Therefore, we have
\begin{equation}
	\begin{aligned}
    \left\|\mathbf{W}^{(T_1)}-\mathbf{W}^*\right\|_F \le& \tilde{\mathcal{O}}\left(n\max_{j\in[K]}\{\mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)\}\right) + \mathcal{O}(\sqrt{md}\sigma_0)+ \mathcal{O}\left(\sqrt{m K}\log\left(\frac{4(K-1)}{\epsilon}\right)\right)\\
    =&\tilde{\mathcal{O}}\left(\max\{n\max_{j\in[K]}\{\mathrm{Tr}(\mathbf{A}_j^\top\mathbf{A}_j)\}, \sqrt{md}\sigma_0,\sqrt{m K}\}\right).
\end{aligned}
\end{equation}
This finishes the training loss analysis in Stage 2.

\section{Test loss analysis}
In this section, we analyze the test loss.
Similar to the proof in Appendix \ref{appendix:training_loss}, the results are based on the high probability conclusions in Appendix \ref{appendix:lemmas}.  
In order to characterize the test loss, we first prove the following key lemmas.

\subsection{Key lemmas for test loss analysis}
\begin{lemma}\label{lemma: property}
	Define 
	\begin{align}
		\mathcal{S}_i^{(t)} = \{r\in[m]:\langle \mathbf{w}_{y_i,r}^{(t)}, \boldsymbol{\xi}_i\rangle>0\},
	\end{align}
	for all $(\mathbf{x}_i, y_i) \in \mathcal{S}$.
	For any $(\mathbf{x}_i,y_i),(\mathbf{x}_j,y_j)\in \mathcal{S}, t\in[T]$, we have
	\begin{align}\label{equ: ratio condition}
		\frac{1-\textnormal{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)}{1-\textnormal{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)} \le \kappa,
	\end{align}
	for a constant $\kappa>1$ and
	\begin{align}
		\mathcal{S}^{(t)}_i \subseteq \mathcal{S}^{(t+1)}_i.
	\end{align}
\end{lemma}
\begin{proof}
We prove the first statement by induction.
First, we show the conclusions hold at iteration 0.
At iteration 0, with probability at least $1-\delta$, for all $(\mathbf{x},y) \in \mathcal{S}$, by Condition \ref{condition} and Lemma \ref{lemma: init order}, we have
\begin{align}
	0 \le F_k(\mathbf{W}^{(0)},\mathbf{x}) \le C,
\end{align}
where $C > 0$ is a constant.
Therefore, there exists a constant $\kappa>0$ such that
\begin{align}\label{equ: balance ratio}
	\frac{1-\textnormal{logit}_{y_i}(\mathbf{W}^{(0)},\mathbf{x}_i)}{1-\textnormal{logit}_{y_j}(\mathbf{W}^{(0)},\mathbf{x}_j)} \le \kappa.
\end{align}

Suppose there exists $\bar{t}$ such that the conditions hold for any $0\le t\le \bar{t}$. 
We aim to prove that the conclusions also hold for $t=\bar{t}+1$.
  	We consider the following two cases.
	
	Case 1: 	$\oldfrac{1-\textnormal{logit}_{y_i}\left(\mathbf{W}^{(t)},\mathbf{x}_i\right)}{1-\textnormal{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)}<0.9\kappa$.
	First, we have
	\begin{equation}
		\begin{aligned}
			&\frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t+1)},\mathbf{x}_i\right)}{1-\text{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)}
			= \frac{\sum_{k\neq y_i}\exp(F_k^{(t)}(\mathbf{x}_i))\exp(\Delta_k^{(t)}(\mathbf{x}_i))}{\sum_{k\neq y_i}\exp(F_k^{(t)}(\mathbf{x}_i))}
			\frac{\sum_{k\in[K]}\exp\left(F_{k}^{(t)}(\mathbf{x}_i)\right)}{\sum_{k\in[K]}\exp\left(F_{k}^{(t)}(\mathbf{x}_i)\right)\exp\left(\Delta_k^{(t)}(\mathbf{x}_i)\right)},
		\end{aligned}
	\end{equation}
	indicating that
	\begin{align}
		\frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t+1)},\mathbf{x}_i\right)}{1-\text{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)} \le \frac{\max_{k\in[K]} \exp\left(\Delta_k^{(t)}(\mathbf{x}_i)\right) }{\min_{k\in[K]}\exp\left(\Delta_k^{(t)}(\mathbf{x}_i)\right)}.
	\end{align}
	Moreover, we have
	\begin{align}
		|\Delta_k^{(t)}(\mathbf{x}_i)| \le \eta\max_{k\in[K]}\left\{ \left\|\mathbf{u}_k\right\|_2^2+\left\| \boldsymbol{\xi}\right\|^2_2\right\} \le \eta\max_{k\in[K]}\left\{ \left\|\mathbf{u}_k\right\|_2^2+\frac{3}{2}\mathrm{Tr}\left(\mathbf{A}_k^\top\mathbf{A}_{k}\right)\right\} \le 0.02,
	\end{align}
	where the inequalities are by Lemma \ref{lemma: self and cross inner product} and Condition \ref{condition}.
	Then, we have
	\begin{equation}
		\begin{aligned}
			&\frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t+1)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t+1)},\mathbf{x}_j)}\\
			=&\frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t+1)},\mathbf{x}_i\right)}{1-\text{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)}\frac{1-\text{logit}_{y_j}\left(\mathbf{W}^{(t)},\mathbf{x}_j\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t+1)},\mathbf{x}_j)}\frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)}\\
			\le& \frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)}\cdot \exp(0.09)\\
			\le& \kappa.
		\end{aligned}
	\end{equation}

	Case 2: 	$\oldfrac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)}> 0.9\kappa>1$.
	We have
	\begin{equation}
		\begin{aligned}
			&\frac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t+1)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t+1)},\mathbf{x}_j)}\\
			\le&\frac{\sum_{k\neq y_j}\exp(F_k^{(t)}(\mathbf{x}_j))+ \exp\left(F_{y_j}^{(t)}(\mathbf{x}_j)\right)\exp\left(\Delta_{y_j}^{(t)}(\mathbf{x}_j)-\min_{k\neq y_j}\Delta_k^{(t)}(\mathbf{x}_j)\right)}{\sum_{k\neq y_i}\exp(F_k^{(t)}(\mathbf{x}_i))+\exp\left(F_{y_i}^{(t)}(\mathbf{x}_i)\right)\exp\left(\Delta_{y_i}^{(t)}(\mathbf{x}_i) - \max_{k\neq y_i}\Delta_{k}^{(t)}(\mathbf{x}_i)\right)}\\
			&\frac{\sum_{k\neq y_i}\exp(F_k^{(t)}(\mathbf{x}_i))}{\sum_{k\neq y_j}\exp(F_k^{(t)}(\mathbf{x}_j))}\\
            =& \frac{1 + \left(\oldfrac{1}{1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)}-1\right)\exp\left(\Delta_{y_j}^{(t)}(\mathbf{x}_j)-\min_{k\neq y_j}\Delta_k^{(t)}(\mathbf{x}_j)\right)}{1 + \left(\oldfrac{1}{1-\text{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)}-1\right)\exp\left(\Delta_{y_i}^{(t)}(\mathbf{x}_i) - \max_{k\neq y_i}\Delta_{k}^{(t)}(\mathbf{x}_i)\right)}\\
            \le& \max\{1, \kappa \exp(\Delta_{y_j}^{(t)}(\mathbf{x}_j)-\min_{k\neq y_j}\Delta_k^{(t)}(\mathbf{x}_j)-\Delta_{y_i}^{(t)}(\mathbf{x}_i) + \max_{k\neq y_i}\Delta_{k}^{(t)}(\mathbf{x}_i))\}
		\end{aligned}
	\end{equation}
	Denote $l_1 = \arg\min_{k\neq y_j}\Delta_k^{(t)}(\mathbf{x}_j)$ and $l_2 = \arg\max_{k\neq y_i}\Delta_k^{(t)}(\mathbf{x}_i)$. 
	We have
	\begin{equation}
	\begin{aligned}
		&\Delta_{l_2}^{(t)}(\mathbf{x}_i) - \Delta_{y_i}^{(t)}(\mathbf{x}_i) - \Delta_{l_1}^{(t)}(\mathbf{x}_j) + \Delta_{y_j}^{(t)}(\mathbf{x}_j)\\
		\le& \frac{\eta}{n}\sum_{k\neq i} (1-\text{logit}_{y_k}(\mathbf{W}^{(t)},\mathbf{x}_k))|\langle \boldsymbol{\xi}_i, \boldsymbol{\xi}_k\rangle| - \frac{2\eta}{5n} (1-\text{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i))\left\|\boldsymbol{\xi}_i\right\|_2^2\\
		&+ \frac{\eta}{n}\sum_{k\neq j} (1-\text{logit}_{y_k}(\mathbf{W}^{(t)},\mathbf{x}_k))|\langle \boldsymbol{\xi}_j, \boldsymbol{\xi}_k\rangle| + \frac{\eta}{n} (1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j))\left\|\boldsymbol{\xi}_j\right\|_2^2\\
		& - \frac{2\eta}{5n}(1-\text{logit}_{y_i}(\mathbf{W}^{(t)},\mathbf{x}_i)) \left\|\mathbf{u}_i\right\|_2^2 + \frac{\eta}{n} (1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j))\left\|\mathbf{u}_j\right\|_2^2\\
		\le& 0,
	\end{aligned}
	\end{equation}
        by letting $\kappa > \Theta(\max\{\frac{\mathrm{Tr}(\mathbf{A}_{y_j}^\top\mathbf{A}_{y_j})}{\mathrm{Tr}(\mathbf{A}_{y_i}^\top\mathbf{A}_{y_i})}, \frac{\left\|\mathbf{u}_j\right\|_2^2}{\left\|\mathbf{u}_i\right\|_2^2}\})$.
	Then the last inequality is by $\oldfrac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t)},\mathbf{x}_j)}> 0.9\kappa$ and Lemma \ref{lemma: self and cross inner product}. 
	Therefore, we have
	\begin{align}
		\oldfrac{1-\text{logit}_{y_i}\left(\mathbf{W}^{(t+1)},\mathbf{x}_i\right)}{1-\text{logit}_{y_j}(\mathbf{W}^{(t+1)},\mathbf{x}_j)} \le \kappa.
	\end{align}
	This completes the proof of the induction.
	
	Next, we prove the second statement.
	For a data sample $(\mathbf{x}_i,y_i)\in\mathcal{S}$ and a neuron in $\mathcal{S}_i^{(t)}$, we have
	\begin{equation}
		\begin{aligned}
			\langle \mathbf{w}_{y_i,r}^{(t+1)}, \boldsymbol{\xi}_i \rangle =& \langle \mathbf{w}_{y_i,r}^{(t)}, \boldsymbol{\xi}_i \rangle + \frac{\eta}{mn}  (1-\text{logit}_{y_i}(\mathbf{x}_i)) \sigma'(\langle \mathbf{w}_{y_i,r}^{(t)}, \boldsymbol{\xi}_i\rangle)\left\|\boldsymbol{\xi}_i\right\|_2^2\\
			&- \frac{\eta}{mn}\sum_{i'\neq i} \text{logit}_{y_{i}}(\mathbf{x}_i)\sigma'(\langle \mathbf{w}_{y_i,r}^{(t)}, \boldsymbol{\xi}_{i'}\rangle)\langle\boldsymbol{\xi}_{i'},\boldsymbol{\xi}_i\rangle\\
			\ge& \langle \mathbf{w}_{y_i,r}^{(t)}, \boldsymbol{\xi}_i \rangle,
		\end{aligned}
	\end{equation}
	where the inequality is by Lemma \ref{lemma: self and cross inner product}, Condition \ref{condition} and (\ref{equ: balance ratio}).
	Thus, we have $\mathcal{S}_i^{(t)} \subseteq \mathcal{S}_i^{(t+1)}$.
	This completes the proof.
\end{proof}
\begin{lemma}\label{lemma: negative correlation and order}
	Under Condition \ref{condition}, for any $j\in [K], l\in [K]\backslash \{j\}, (\mathbf{x}_q,y_q)\in\mathcal{S}_j, (\mathbf{x}_a,y_a)\in\mathcal{S}_l$, and $r\in\mathcal{S}_q^{(0)}$,
	\begin{equation}
	\begin{aligned}
		&\sum_{t'=0}^{t-1}(1-\textnormal{logit}_{y_q}(\mathbf{W}^{(t')},\mathbf{x}_q))\sigma'(\langle \mathbf{w}_{j,r}^{(t')},\boldsymbol{\xi}_q \rangle)\\
		\ge& \Omega\left(\frac{1}{n}\sum_{t'=0}^{t-1} (\textnormal{logit}_j(\mathbf{W}^{(t')}, \mathbf{x}_a))\sigma'(\langle \mathbf{w}_{j,r}^{(t')}, \boldsymbol{\xi}_a \rangle)\frac{\mathrm{Tr}(\mathbf{A}_l^\top\mathbf{A}_l)}{\max_{l_l,l_2\in[K]}\left\|\mathbf{A}_{l_1}^\top\mathbf{A}_{l_2}\right\|_F^{1/2}\log(\frac{3n^2}{\delta})^{1/2}}\right).
	\end{aligned}
\end{equation}
\end{lemma}
\begin{proof}
	By the update rule of gradient descent, we have
	\begin{equation}
	\begin{aligned}
		\langle \mathbf{w}_{j,r}^{(t+1)}-\mathbf{w}_{j,r}^{(t)}, \boldsymbol{\xi}_{a} \rangle \le &\frac{\eta}{mn}\sum_{(\mathbf{x}_i,y_i)\in\mathcal{S}\backslash(\mathbf{x}_a,y_a)} \left(1-\text{logit}_j(\mathbf{W}^{(t)},\boldsymbol{\xi}_i)\right)\sigma'(\langle \mathbf{w}^{(t)}_{j,r},\boldsymbol{\xi}_i\rangle)|\left\langle\boldsymbol{\xi}_i,\boldsymbol{\xi}_a\right\rangle|\\
		&+ \frac{\eta}{mn} (-\text{logit}_{y}(\mathbf{W}^{(t)},\mathbf{x}))\sigma'(\langle \mathbf{w}^{(t)}_{j,r},\boldsymbol{\xi}_a\rangle)\left\|\boldsymbol{\xi}_{a}\right\|_2^2.
	\end{aligned}
	\end{equation}
	By Lemma \ref{lemma: self and cross inner product}, we have 
	\begin{equation}\label{equ: aux3}
	\begin{aligned}
		&\langle \mathbf{w}_{j,r}^{(t+1)}-\mathbf{w}_{j,r}^{(0)}, \boldsymbol{\xi}_{a} \rangle +\frac{\eta}{mn}\sum_{t'=0}^{t-1} (\text{logit}_j(\mathbf{W}^{(t')}, \mathbf{x}_a))\sigma'(\langle \mathbf{w}_{j,r}^{(t')}, \boldsymbol{\xi}_a \rangle)\left\|\boldsymbol{\xi}_{a}\right\|_2^2\\
		\le& \frac{\eta}{mn} \sum_{(\mathbf{x},y)\in  \mathcal{S}\backslash(\mathbf{x}_a,y_a)}\sum_{t'=0}^{t}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\underbrace{\max_{l,y\in[K]}\left\|\mathbf{A}_l^\top\mathbf{A}_y\right\|_F^{1/2}\log(\frac{3n^2}{\delta})^{1/2}}_{E_1}.
	\end{aligned}
\end{equation}
Additionally, by the nature of ReLU activation function, the magnitude of $\langle \mathbf{w}_{j,r}^{(t+1)}-\mathbf{w}_{j,r}^{(0)}, \boldsymbol{\xi}_{a} \rangle$ satisfies
	\begin{align}\label{equ: aux2}
		\langle \mathbf{w}_{j,r}^{(t+1)}-\mathbf{w}_{j,r}^{(0)}, \boldsymbol{\xi}_{a} \rangle \ge -\frac{\eta}{mn} \sum_{(\mathbf{x},y)\in  \mathcal{S}\backslash(\mathbf{x}_a,y_a)}\sum_{t'=0}^{t}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))E_1 - \frac{\eta}{mn} \|\boldsymbol{\xi}_a\|_2^2.
	\end{align}
	As the learnign rate $\eta$ is small (by Condition \ref{condition}), combining (\ref{equ: aux3}) and (\ref{equ: aux2}), for any $(\mathbf{x},y)\sim\mathcal{D}$, we have
	\begin{equation}
	\begin{aligned}
		&\frac{\eta}{mn}\sum_{t'=0}^{t-1} (\text{logit}_j(\mathbf{W}^{(t')}, \mathbf{x}_a))\sigma'(\langle \mathbf{w}_{j,r}^{(t')}, \boldsymbol{\xi}_a \rangle)\left\|\boldsymbol{\xi}_{a}\right\|_2^2\\
		=&  \mathcal{O}\left(\frac{\eta}{mn} \sum_{(\mathbf{x},y)\in  \mathcal{S}\backslash(\mathbf{x}_a,y_a)}\sum_{t'=0}^{t}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))E_1\right),
	\end{aligned}
\end{equation}
	By Lemma \ref{lemma: property}, for $r\in\mathcal{S}^{(0)}_q$, we have
	\begin{equation}\label{equ: order compare}
	\begin{aligned}
		&\sum_{t'=0}^{t-1}(1-\text{logit}_{y_q}(\mathbf{W}^{(t')},\mathbf{x}_q))\sigma'(\langle \mathbf{w}_{j,r}^{(t')},\boldsymbol{\xi}_q \rangle)\\
		 =& \Omega\left(\frac{1}{n}\sum_{t'=0}^{t-1} (\text{logit}_j(\mathbf{W}^{(t')}, \mathbf{x}_a))\sigma'(\langle \mathbf{w}_{j,r}^{(t')}, \boldsymbol{\xi}_a \rangle)\frac{\left\|\boldsymbol{\xi}_{a}\right\|_2^2}{E_1}\right).
	\end{aligned}
	\end{equation}
	By Lemma \ref{lemma: self and cross inner product}, we have
	\begin{align}\label{equ: ratio order}
		\frac{\left\|\boldsymbol{\xi}_{a}\right\|_2^2}{E_1} = \Omega\left(\frac{\mathrm{Tr}(\mathbf{A}_l^\top\mathbf{A}_l)}{\max_{l_l,l_2\in[K]}\left\|\mathbf{A}_{l_1}^\top\mathbf{A}_{l_2}\right\|_F^{1/2}\log(\frac{3n^2}{\delta})^{1/2}}\right).
	\end{align}
	Combining (\ref{equ: order compare}) and (\ref{equ: ratio order}) yields the conclusion.
	This completes the proof.
\end{proof}
\begin{lemma}\label{lemma: logT}
	Under Condition \ref{condition}, for any $j\in[K]$, we have
	\begin{align}
		\left\|\left(\mathbf{W}_j^{(T)}\right)^\top\mathbf{A}_j\right\|_2 \le \mathcal{O}(\sqrt{m}\log(T)).
	\end{align}
\end{lemma}
\begin{proof}
	We bound it with two phases.
	The first phase $t\in[0,\tilde{T}_1]$ is before the loss of data $(\mathbf{x},y)\in\mathcal{S}$ satisfying
	\begin{align}\label{equ: logit assumption}
		\text{logit}_y(\mathbf{x}) \le c_5,
	\end{align} 
	where $c_5$ is a constant.
	Then, the loss satisfies
	\begin{equation}
	\begin{aligned}
		&(1-c_5)\exp\left( \sigma\left(\left\langle\mathbf{w}_{y,r}^{(\tilde{T}_1)}, \boldsymbol{\xi}\right\rangle\right)\right)\\
		\le&(1-c_5)\exp\left(\sigma\left(\left\langle\mathbf{w}_{y,r}^{(\tilde{T}_1)}, \mathbf{u}_y\right\rangle\right) + \sigma\left(\left\langle\mathbf{w}_{y,r}^{(\tilde{T}_1)}, \boldsymbol{\xi}\right\rangle\right)\right)\\
		\le& c_5\sum_{j\neq y} \exp\left(\sigma\left(\left\langle\mathbf{w}_{j,r}^{(\tilde{T}_1)}, \mathbf{u}_j\right\rangle\right) + \sigma\left(\left\langle\mathbf{w}_{j,r}^{(\tilde{T}_1)}, \boldsymbol{\xi}\right\rangle\right)\right)\\
		\le& c_5 K \exp\left( \sqrt{2\log\left(\frac{4Km}{\delta}\right)}\left\|\mathbf{u}_k\right\|_2\sigma_0 +\mathcal{O}\left(\log\left(\frac{Km}{\delta}\right)\left\|\mathbf{A}_{y}\right\|_F\sigma_0\right)+\frac{\eta}{mn}\left\|\boldsymbol{\mathbf{\xi}}\right\|_2^2\right)\\
		\le& 1.1c_5 K \exp\left(\frac{3\eta}{2mn}\mathrm{Tr}\left(\mathbf{A}^\top_y\mathbf{A}_y\right)\right),
	\end{aligned}
	\end{equation}
	where the first inequality is by the fact that $\sigma\left(\left\langle\mathbf{w}_{y,r}^{(\tilde{T}_1)}, \mathbf{u}_y\right\rangle\right) \ge 0$, the second inequality is by the definition of softmax function and (\ref{equ: logit assumption}), the third inequality is by Lemma \ref{lemma: init order} and the fourth inequality is by Lemma \ref{lemma: self and cross inner product} and Condition \ref{condition}.
	
	Letting $c_5$ be $\frac{T}{(1.1K+T)}$ yields 
	\begin{align}\label{equ: logT}
		\langle\mathbf{w}_{y,r}^{(\tilde{T}_1)}, \boldsymbol{\xi}\rangle \le \log\left(\frac{c_5}{1-c_5}K\right)+\frac{3\eta}{2mn}\mathrm{Tr}\left(\mathbf{A}^\top_y\mathbf{A}_y\right) \le \mathcal{O}\left(\log(T)\right).
	\end{align}
	Moreover, for $T>\tilde{T}_1$, we have
	\begin{equation}\label{equ: logt}
	\begin{aligned}
		\langle\mathbf{w}_{y,r}^{(T)}, \boldsymbol{\xi}\rangle =& \langle\mathbf{w}_{y,r}^{(\tilde{T}_1)}, \boldsymbol{\xi}\rangle + \langle\mathbf{w}_{y,r}^{(T)} - \mathbf{w}_{y,r}^{(\tilde{T}_1)}, \boldsymbol{\xi}\rangle\\
		\le& \mathcal{O}\left(\log(T)\right) + 2\sum_{t=\tilde{T}_1+1}^{T}\frac{\eta}{mn} (1-\text{logit}_y(\mathbf{W}^{(t)},\mathbf{x}))\left\|\boldsymbol{\xi}\right\|_2^2\\
		\le&  \mathcal{O}\left(\log(T)\right) + \frac{3\eta T}{mn} \frac{1}{T} \mathrm{Tr}\left(\mathbf{A}_y^\top\mathbf{A}_y\right)\\
		\le& \mathcal{O}(\log(T)),
	\end{aligned}
	\end{equation}
	where the first inequality is by (\ref{equ: logT}), the second inequality is by Lemma \ref{lemma: self and cross inner product} and condition of the second phase that $\text{logit}_y(\mathbf{x}) > c_5$.
	With probability of $1-\delta$, for $n$ randomly sampled data $(\mathbf{x}',y)\sim\mathcal{D}_y$, we have
        \begin{equation}
	\begin{aligned}
	\langle\mathbf{W}^{(T)}_{y,r},\boldsymbol{\xi}'\rangle\le& \mathcal{O}\left(\frac{\eta}{mm}\sum_{i\in\mathcal{S}}\sum_{t=0}^{T-1}(1-\text{logit}(\mathbf{W}^{(t}),\mathbf{x}) |\langle \boldsymbol{\xi}_i,\boldsymbol{\xi}' \rangle|\right)\\
    \le& \mathcal{O}\left(\frac{\eta}{mn}\sum_{t=0}^{T-1}(1-\text{logit}(\mathbf{W}^{(t)},\mathbf{x})) ||\boldsymbol{\xi}||^2_2\right) =\Theta(\langle\mathbf{w}_{y,r}^{(T)}, \boldsymbol{\xi}\rangle)= \left(\mathbf{w}_{y,r}^{(T)}\right)^\top\mathbf{A}_y\boldsymbol{\zeta},
	\end{aligned}
        \end{equation}
    With probability $1-\delta$, at least one sample $(\mathbf{x}',y)$ satisfies $\langle\mathbf{w}_{y,r}^{(T)}, \boldsymbol{\xi}'\rangle = \Theta\left(\left\|\left(\mathbf{w}_{y,r}^{(T)}\right)^\top\mathbf{A}_y\right\|_2\right)$ by the property of $\mathcal{D}_{\zeta}$.
    Therefore, we have
    \begin{align}\label{equ: inner product order}
        \langle\mathbf{w}_{y,r}^{(T)}, \boldsymbol{\xi}\rangle = \Omega\left(\left\|\left(\mathbf{w}_{y,r}^{(T)}\right)^\top\mathbf{A}_y\right\|_2\right).
    \end{align}
    Combining (\ref{equ: logt}) and (\ref{equ: inner product order}) completes the proof.
\end{proof}


\begin{lemma}
	Under condition \ref{condition}, for a random vector $\boldsymbol{\xi}$ generated from $\mathbf{A}_j\boldsymbol{\zeta}$, $\boldsymbol{\zeta} \sim \mathcal{D}_{\zeta}$ for any $j\in [K]$, with probability at least $1-\delta$, we have
	\begin{align}
		\sum_{r=1}^{m}\mathbb{I}\left(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle\right) \ge 0.1m.
	\end{align}
\end{lemma}
\begin{proof}
	First, we can concatenate the neuron weights for class $j \in [K]$ as
	\begin{align}
		\mathbf{W}_{j}^{(T)}\mathbf{A}_j =
		\begin{bmatrix}
			\left(\mathbf{w}_{j,1}^{(T)}\right)^\top\\
				\vdots\\
			\left(\mathbf{w}_{j,m}^{(T)}\right)^\top
		\end{bmatrix}\mathbf{A}_j
	=\underbrace{\begin{bmatrix}
		\left(\mathbf{w}_{j,1}^{(0)}\right)^\top\\
		\vdots\\
		\left(\mathbf{w}_{j,m}^{(0)}\right)^\top
	\end{bmatrix}}_{\mathbf{D}_1}\mathbf{A}_j + 
\underbrace{\begin{bmatrix}
	\beta_{j,1,1}&\cdots&\beta_{j,1,n}\\
	\vdots & \vdots &\vdots\\
	\beta_{j,m,1}&\cdots&\beta_{j,m,n}
\end{bmatrix}
\begin{bmatrix}
	\boldsymbol{\xi}_1^\top\\
	\vdots\\
	\boldsymbol{\xi}_n^\top
\end{bmatrix}}_{\mathbf{D}_2}\mathbf{A}_j,
	\end{align}
where $\beta_{j,r,i} := \sum_{t'=0}^{T-1}\sigma'(\langle\mathbf{w}_{j,r}^{(t')},\boldsymbol{\xi}_i\rangle)\text{logit}(\mathbf{W}^{(t')},\mathbf{x}_i)$.
The rank of matrix $\mathbf{D}_2\mathbf{A}_j$ satisfies
\begin{align}
	\text{rank}(\mathbf{D}_2\mathbf{A}_j) \le \min\{m,n,d, \text{rank}(\mathbf{A}_j)\} = n.
\end{align}
In addition, as the matrix $\mathbf{D}_1$ is a Gaussian random matrix, it has full rank almost surely.
We have 
\begin{align}
	\text{rank}(\mathbf{D}_1\mathbf{A}_j)=\min\{m,d,\text{rank}(\mathbf{A}_j)\},
\end{align}
almost surely.
Then by Condition \ref{condition}, the rank of $\mathbf{W}_{j}^{(T)}\mathbf{A}_j$ satisfies 
\begin{align}
	\text{rank}(\mathbf{W}_{j}^{(T)}\mathbf{A}_j) \ge \min\{m,d,\text{rank}(\mathbf{A}_j)\} - n \ge 0.9m.
\end{align}
In addition, by Lemma \ref{lemma: Gaussian_eigen} and Condition \ref{condition}, the singular value of $\mathbf{W}_{j}^{(T)}\mathbf{A}_j$ satisfies
\begin{align}
	\lambda_{\min\{m,d,\text{rank}(\mathbf{A}_j)\} - n}(\mathbf{W}_{j}^{(T)}\mathbf{A}_j) \ge 0.1\sqrt{m}\sigma_0.
\end{align}
Moreover, by Lemma \ref{lemma: logT}, we have
\begin{align}
	\lambda_{1}(\mathbf{W}_{j}^{(T)}\mathbf{A}_j) \le \left\|\mathbf{W}_j^{(T)}\mathbf{A}_j\right\|_F \le \mathcal{O}(\sqrt{m}\log(T)).
\end{align}
Therefore, according to Condition \ref{condition}, we have
\begin{align}
	m \ge \Omega\left( \frac{\log(\frac{n}{\delta})\log(T)^2}{n\sigma_0^2}\right).
\end{align}
By Lemma \ref{lemma: num_orthrant} and Condition \ref{condition}, with probability at least $1-\delta$, we have
	\begin{align}
		\sum_{r=1}^{m}\mathbb{I}\left(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle\right) \ge 0.1m.
	\end{align}
This completes the proof.
\end{proof}

\subsubsection{Proof of Statement 2(a) in Theorem \ref{theorem: feature learning}}
In this part, we prove Statement 2(a) in Theorem \ref{theorem: feature learning}.
For data samples following $(\mathbf{x},y)\sim \mathcal{D}$, the test loss satisfies
\begin{equation}
\begin{aligned}
	L_\mathcal{D}(\mathbf{W}^{(t)}) =& \mathbb{P}\left[\arg\max_k F_k(\mathbf{W}^{(T)},\mathbf{x},y) \neq y\right]\\
	 \le& \sum_{j\neq y}\mathbb{P}\left[F_y(\mathbf{W}^{(T)},\mathbf{x},y) \le F_{j}(\mathbf{W}^{(T)},\mathbf{x},y)\right]\\
	=& \sum_{j\neq y}\mathbb{P}\left[\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\mathbf{u}_y\rangle) + \sigma(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle)\le \sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_{y}\rangle) + \sigma(\langle\mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle)\right].
\end{aligned}
\end{equation}

For the features, we bound the loss for any $(\mathbf{x},y)\sim \mathcal{D}$ through
\begin{equation}\label{equ: short tail upper}
\begin{aligned}
	\mathcal{L}(\mathbf{W}^{(T)},\mathbf{x},y) \le& \sum_{j\neq y}\mathbb{P}\left[\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\mathbf{u}_y\rangle)\le \sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_{y}\rangle) + \sigma(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle) \right].
\end{aligned}
\end{equation}
Then, we can bound the model outputs as
\begin{equation}\label{equ: inner_product_feature}
\begin{aligned}
	&\sum_{r=1}^{m}\sigma\left(\left\langle\mathbf{w}_{y,r}^{(T)},\mathbf{u}_y\right\rangle\right)\\
	 =& \sum_{r=1}^{m} \sigma\left(\left\langle \mathbf{w}_{y,r}^{(0)} + \frac{\eta}{n}\sum_{t'=1}^{T}\sum_{(\mathbf{x}_l,y_l)\in\mathcal{S}_y}(1-\text{logit}_{y}(\mathbf{W}^{(t')},\mathbf{x}_l))\sigma'(\langle \mathbf{w}_{y,r}^{(t')},\mathbf{u}_y\rangle) \mathbf{u}_y ,\mathbf{u}_y \right\rangle\right)\\
	\ge& \frac{3m}{10}\frac{\eta}{n}\sum_{t'=1}^{T} \sum_{(\mathbf{x}_l,y_l)\in\mathcal{S}_y}(1-\text{logit}_{y}(\mathbf{W}^{(t')},\mathbf{x}_l))\left\|\mathbf{u}_y\right\|_2^2,
\end{aligned}
\end{equation}
and
\begin{equation}\label{equ: inner_product_noise}
	\begin{aligned}
		\sum_{r=1}^{m}\sigma(\langle \mathbf{w}_{j,r}^{(T)}, \boldsymbol{\xi}\rangle) \le& \sum_{r=1}^{m}|\langle\mathbf{w}^{(T)}_{j,r},\boldsymbol{\xi}\rangle|=\sum_{r=1}^{m}\left\|\mathbf{A}_k^\top\mathbf{w}_{j,r}^{(T)}\right\|_2|\zeta'|,
	\end{aligned}
\end{equation}
where $\zeta'$ is a sub-Gaussian variable.
Suppose that $\delta >0 $ and $\frac{\left\|\mathbf{A}_y^\top \mathbf{A}_j\right\|_F}{\left\|\mathbf{A}_y^\top \mathbf{A}_j\right\|_\text{op}}\ge \Theta(\sqrt{\log(\frac{K^2}{\delta})})$.
For the term $\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2$, with probability at least $1-\delta$, we have
\begin{equation}\label{equ: A_yw_j}
\begin{aligned}
	&\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2\\
	 =& \left\|\mathbf{A}_y^\top \left(\mathbf{w}_{j,r}^{(0)}+\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in \mathcal{S}_j}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\sigma'(\langle \mathbf{w}^{(t')}_{j,r},\boldsymbol{\xi}\rangle)\boldsymbol{\xi}\right.\right.\\ &\left.\left.-\frac{\eta}{n}\sum_{(\mathbf{x},y)\in\mathcal{S}\backslash\mathcal{S}_j}\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}) \sigma'(\langle \mathbf{w}^{(t')}_{j,r},\boldsymbol{\xi}\rangle)\boldsymbol{\xi}\right)\right\|_2\\
	 \le& \left\|\mathbf{A}_y^\top\mathbf{A}_j\left(\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in\mathcal{S}_j}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\sigma'(\langle\mathbf{w}_{j,r}^{(t')},\boldsymbol{\xi}\rangle)\boldsymbol{\zeta}\right)\right\|_2\\
	 &+ \left\|\sum_{k\neq j}\mathbf{A}_y^\top\mathbf{A}_k\left(\frac{\eta}{n	}\sum_{(\mathbf{x},y)\in\mathcal{S}_k}\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}) \sigma'(\langle \mathbf{w}^{(t')}_{j,r},\boldsymbol{\xi}\rangle)\boldsymbol{\zeta}\right)\right\|_2\\
	\overset{(a)}{=}& \Theta\left(\left\|\mathbf{A}_y^\top\left(\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in \mathcal{S}_j}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\sigma'(\langle \mathbf{w}^{(t')}_{j,r},\boldsymbol{\xi}\rangle)\boldsymbol{\xi}\right) \right\|_2\right)\\
	 \overset{(b)}{=}& \Theta\left(\frac{\eta}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_j}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right),
\end{aligned}
\end{equation}
where the inequality is by Lemma \ref{lemma: negative correlation and order} and Condition \ref{condition}, $(a)$ is based on Lemma \ref{lemma: negative correlation and order}, and $(b)$ is based on the concentration of random vectors (Theorem 6.2.6 in \cite{vershynin2018high}).
Substituting (\ref{equ: inner_product_feature}) and (\ref{equ: inner_product_noise}) into (\ref{equ: short tail upper}), for any $(\mathbf{x},y)\sim\mathcal{D}$ we have
\begin{equation}
	\begin{aligned}
		&\mathcal{L}(\mathbf{W}^{(T)},\mathbf{x},y)\\
		\le& \sum_{j\neq y}\mathbb{P}\left[\frac{m}{5}\frac{\eta}{n}\sum_{t'=0}^{T-1} \sum_{(\mathbf{x},y)\in\mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\left\|\mathbf{u}_y\right\|_2^2 \le \sum_{r=1}^{m}|\langle\mathbf{w}^{(T)}_{j,r},\boldsymbol{\xi}\rangle| + |\langle\mathbf{w}_{j,r}^{(T)}, \mathbf{u}_y\rangle|\right]\\
		=& \sum_{j\neq y}\mathbb{P}\left[\frac{m}{5}\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in\mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\left\|\mathbf{u}_y\right\|_2^2 \le \sum_{r=1}^{m}\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2|\zeta'| + |\langle\mathbf{w}_{j,r}^{(0)}, \mathbf{u}_y\rangle|\right]\\
		\le& \sum_{j\neq y}\mathbb{P}\left[\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in\mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\left\|\mathbf{u}_y\right\|_2^2\right.\\
		&\le \left.c\left\|\mathbf{A}_y^\top\mathbf{A}_j	\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_j}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}|\zeta'| + \mathcal{O}\left(\frac{n}{\eta}\sqrt{\log\left(\frac{4Km}{\delta}\right)}\left\|\mathbf{u}_y\right\|_2\sigma_0\right)\right]\\
		\le& \sum_{j\neq y}\exp\left(-c_1\cdot\frac{|\mathcal{S}_y|^2\left\|\mathbf{u}_y\right\|_2^4}{|\mathcal{S}_j|\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F^2}\right),
	\end{aligned}
\end{equation}
where $c,c_1>0$ are some constants and the last inequality is obtained from Hoeffding's inequality.
\subsubsection{Proof of Statement 2(b) in Theorem \ref{theorem: feature learning}}
Furthermore, for long-tailed data distribution, by Lemma \ref{lemma: negative correlation and order} and union bound, we have
\begin{equation}
\begin{aligned}
		\mathcal{L}^{0-1}(\mathbf{W}^{(t)},\mathbf{x},y) \le\sum_{j\neq y}\mathbb{P}\left[\sum_{r=1}^{m}\sigma\left(\left\langle\mathbf{w}_{y,r}^{(t)},\boldsymbol{\xi}\right\rangle\right)\le\sum_{r=1}^{m}\sigma\left(\left\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_y\right\rangle\right) + \sigma\left(\left\langle \mathbf{w}_{j,r}^{(t)}, \boldsymbol{\xi}\right\rangle\right) \right].
\end{aligned}
\end{equation}
Suppose $\delta>0$, by Condition \ref{condition}, w have $\frac{\left\|\mathbf{A}_y^\top \mathbf{A}_y\right\|_F}{\left\|\mathbf{A}_y^\top \mathbf{A}_y\right\|_\text{op}}\ge \Theta(\sqrt{\log(\frac{K^2}{\delta})})$, for any $y\in[K]$.
With probability at least $1-\delta$, for all $y\in[K]$, we have 
\begin{equation}\label{equ: A_yw_y}
	\begin{aligned}
		&\left\|\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2\\
		=& \left\|\mathbf{A}_y^\top \left(\mathbf{w}_{y,r}^{(0)}+\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in \mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\sigma'(\langle \mathbf{w}^{(t')}_{y,r},\boldsymbol{\xi}\rangle)\boldsymbol{\xi}\right.\right.\\
		&\left.\left. -\frac{\eta}{n	}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in\mathcal{S}\backslash\mathcal{S}_y}\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}) \sigma'(\langle \mathbf{w}^{(t')}_{y,r},\boldsymbol{\xi}\rangle)\boldsymbol{\xi}\right)\right\|_2\\
		\ge& \left\|\mathbf{A}_y^\top\mathbf{A}_y\left(\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in\mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\sigma'(\langle\mathbf{w}_{y,r}^{(t')},\boldsymbol{\xi}\rangle)\boldsymbol{\zeta}\right)\right\|_2\\
		&- \left\|\sum_{j\neq y}\mathbf{A}_y^\top\mathbf{A}_j\left(\frac{\eta}{n}\sum_{t'=0}^{T-1}\sum_{(\mathbf{x},y)\in\mathcal{S}_j}\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}) \sigma'(\langle \mathbf{w}^{(t')}_{y,r},\boldsymbol{\xi}\rangle)\boldsymbol{\zeta}\right)\right\|_2\\
		\overset{(a)}{=}& \Omega\left(\frac{\eta}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_y}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right),
	\end{aligned}
\end{equation}
where $(a)$ is obtained by the condition $\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F =\Omega\left(\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F\right)$ for all $j,y\in [K]$ and $j\neq y$, $K = \Theta(1)$, and $(\ref{equ: ratio condition})$.
Similarly, we also have
	$\left\|\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2 \le \mathcal{O}\left(\frac{\eta}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_y}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right)$.
Then, we have
\begin{align}\label{equ: lt_aux1}
	\left\|\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2 = \Theta\left(\frac{\eta}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_y}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right).
\end{align}
Similar to the proof of (\ref{equ: A_yw_j}), we also have
\begin{equation}\label{equ: lt_aux2}
\begin{aligned}
	&\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2 = \Theta\left(\frac{\eta}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_j}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right),
\end{aligned}
\end{equation}
with probability at least $1-\delta$.
Moreover, we have
\begin{align}
    &\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2 = \Theta\left(\frac{\eta}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_j}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right).
\end{align}

Denote $\bar{\mathcal{S}}_j(\boldsymbol{\xi}) = \{r\in[m]:\langle \mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle>0 \}$.
We have
\begin{align}
	\left\|\sum_{r\in \bar{\mathcal{S}}_y(\boldsymbol{\xi})}\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2 = \Theta\left(\frac{\eta m}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_j}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right),
\end{align}
because Lemma \ref{lemma: activation} holds and each $r\in \bar{\mathcal{S}}$ can be activated by as least $0.4n/K$ samples.
 
We have
\begin{equation}
\begin{aligned}
	&\mathcal{L}^{0-1}(\mathbf{W}^{(T)},\mathbf{x},y)\le \sum_{j\neq y}\mathbb{P}\left[\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle) \le \frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_{y}\rangle) +\sigma(\langle \mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi} \rangle) \right]\\
	=&\sum_{j\neq y}\mathbb{P}\left[|\sum_{r\in \bar{\mathcal{S}}_y(\boldsymbol{\xi})}\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle|\le \sum_{r=1}^{m}|\langle \mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle| + |\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_y\rangle|\right]\\
	\le& \sum_{j\neq y}\mathbb{P}\left[c\cdot\left\|\sum_{r\in \bar{\mathcal{S}_y}(\boldsymbol{\xi})}\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2 - \sum_{r=1}^{m}\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2\le \sum_{r=1}^{m}|\langle \mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle|-\sum_{r=1}^{m}\mathbb{E}|\langle \mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle| + |\langle\mathbf{w}_{j,r}^{(0)},\mathbf{u}_y\rangle|\right]\\
	\le& \sum_{j\neq y}\exp\left(-c'\cdot\frac{\left(cL\cdot\left\|\sum_{r\in \bar{\mathcal{S}}_y(\boldsymbol{\xi})}\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2 - \sum_{r=1}^{m}\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2-\mathcal{O}\left(\sqrt{\log\left(\frac{4Km}{\delta}\right)}\left\|\mathbf{u}_y\right\|_2\sigma_0\right)\right)^2}{\sum_{r=1}^{m}\left\|\mathbf{A}_y^\top\mathbf{w}_{j,r}^{(T)}\right\|_2^2}\right)\\
	\le&\sum_{j\neq y}\exp\left(-c_2L^2\cdot\frac{|\mathcal{S}_y|\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F^2}{|\mathcal{S}_j|\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F^2}\right),
\end{aligned}
\end{equation}
where $c,c'>0$ are constants, the first inequality is by the condition for long-tailed data that $|\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle|\ge c_5 \left\|\mathbf{A}_y^\top\mathbf{w}_{y,r}^{(T)}\right\|_2$, the third inequality is by Hoeffding's inequality and the last inequality is by (\ref{equ: lt_aux1}) and (\ref{equ: lt_aux2}).
 
\subsubsection{Test loss lower bound (Statement 3 in Theorem \ref{theorem: feature learning})}
For any $(\mathbf{x},y)\sim \mathcal{D}$, we have
\begin{equation}
\begin{aligned}
	\mathcal{L}^{0-1}(\mathbf{W}^{(T)}, \mathbf{x},y) \ge& \max_{j\neq y}\mathbb{P}\left[\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\mathbf{u}_y\rangle) + \sigma(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle)\le \sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_{y}\rangle) + \sigma(\langle\mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle)\right]\\
	=& \max_{j\neq y}\mathbb{P}\left[\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y,r}^{(T)},\mathbf{u}_y\rangle) + \sigma(\langle\mathbf{w}_{y,r}^{(T)},\boldsymbol{\xi}\rangle)\le \sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{j,r}^{(T)},\mathbf{u}_{y}\rangle) + \sigma(\langle\mathbf{w}_{j,r}^{(T)},\boldsymbol{\xi}\rangle)\right]\\
	\overset{(a)}{\ge}& \max_{j\neq y}\mathbb{P}\left[\frac{2\eta m}{5n}\sum_{t'=0}^{T-1} \sum_{(\mathbf{x},y)\in\mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\left\|\mathbf{u}_y\right\|_2^2 \right.\\
	&+\underbrace{\Theta\left(\frac{\eta m}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_y}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right)}_{B_1}|\zeta'|\\
	&\le\left. \underbrace{\Theta\left(\frac{\eta m}{n}\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F\cdot\sqrt{\sum_{(\mathbf{x},y)\in \mathcal{S}_j}\left(\sum_{t'=0}^{T-1}(1-\text{logit}_j(\mathbf{W}^{(t')},\mathbf{x}))\right)^2}\right)}_{B_2}|\zeta''|\right],
\end{aligned}
\end{equation}
where $(a)$ is based on (\ref{equ: A_yw_y}) and (\ref{equ: A_yw_j}). $\zeta'$ and $\zeta''$ are sub-Gaussain variables with variance 1.
For any $(\mathbf{x},y)\sim \mathcal{D}$, when there exists $j\in[K]$ and a constant $C_3$ satisfying $n\left\|\mathbf{u}_k\right\|_2^2\le C_3\left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F$ and $\left\|\mathbf{A}_k^\top\mathbf{A}_k\right\|_F\le C_3 \left\|\mathbf{A}_k^\top\mathbf{A}_j\right\|_F$, by Lemma \ref{lemma: subgaussian tail lower bound}, we have
\begin{equation}
\begin{aligned}
	\mathcal{L}^{0-1}(\mathbf{W}^{(T)}, \mathbf{x},y) \ge& \max_{j\neq y} \mathbb{P}\left[\frac{2\eta m}{5n}\sum_{(\mathbf{x},y)\in\mathcal{S}_y}(1-\text{logit}_y(\mathbf{W}^{(t')},\mathbf{x}))\left\|\mathbf{u}_y\right\|_2^2\le B_2|\zeta''|-B_1|\zeta'|\right]\\
\end{aligned}
\end{equation}
Choosing a constant $c_5= \Theta(\sigma_p)$, we have $|\zeta'| \le c_5$ with probability $1-\exp(\Omega(c_5^2/\sigma_p^2))$, resulting in
\begin{equation}
\begin{aligned}   
   \mathcal{L}^{0-1}(\mathbf{W}^{(T)}, \mathbf{x},y) \ge& \max_{j\neq y} \mathbb{P}\left[\Theta\left(\sum_{t'=0}^{T-1} \sqrt{|\mathcal{S}_y|}\left\|\mathbf{u}_y\right\|_2^2\right)\le \Theta(\left\|\mathbf{A}_y^\top\mathbf{A}_j\right\|_F)|\zeta''|-\Theta(\left\|\mathbf{A}_y^\top\mathbf{A}_y\right\|_F)c_5)\right] - \exp(\Omega(c_5^2/\sigma_p^2))\\
	\ge& \Omega(1).
\end{aligned}
\end{equation}

\end{document}