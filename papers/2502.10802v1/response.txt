\section{Related Work}
\label{sec:related-work}
\subsection{Enhancing Code Generation Accuracy with Test Case}

The integration of test cases has emerged as a pivotal strategy for improving the quality of code generated by LLMs. 
Numerous studies leverage test cases to filter, evaluate, and refine generated programs, leading to higher-performing and more reliable code. 
Below, we provide an overview of notable methodologies in this domain.


Li et al., "Sampling+Filtering for Improved Code Generation" introduce a Sampling+Filtering approach, 
where a large set of candidate programs is sampled and scored based on performance against a provided test set.
Chen et al., "Dual-Agreement Framework for Simultaneous Code-Test Generation" propose a framework in which both code and test cases are generated by LLMs simultaneously, using a dual agreement metric to identify and select better programs. 
Shi et al., "Assessing Semantic Similarity through Test Execution" assess semantic similarity by executing generated programs on test cases and leveraging the execution results to guide program selection.

Beyond evaluation, test cases play a critical role in iterative repair and refinement of generated code. 
Execution outcomes from test cases can be used to repair faulty programs. 
For instance, Shinn et al., "Reflexion: Using Failure Messages for Program Repair" introduce the Reflexion framework, 
where failure messages from test executions are converted into verbal feedback for program repair. 
Similarly, Olausson et al., "Repairing Programs using Failed Test Feedback" evaluate LLMs' capabilities to repair programs using failed test feedback. 
Zhang et al., "Analyzing Test Execution Results for Code Revision" allow LLMs to analyze test execution results to revise erroneous code.
Zhong et al., "Step-by-Step Debugging through LLM-Assisted Test Case Generation" extend this idea by enabling step-by-step debugging.
Huang et al., "Chain-of-Thought Prompting for Improved Code and Test Generation" adopt a chain-of-thought prompting approach that generates code and tests sequentially, iteratively improving the code based on test feedback.
Zhang et al., "Pair Programming with LLM-Assisted Repair Strategies" introduce a pair programming framework where test feedback informs repair strategies and planning.

The generation of test cases differs across these methodologies.
For example, CodeT, "Automated Test Case Generation for Improved Code Quality" and CodeCoT, "Code Completion through Test-Driven Learning" rely entirely on LLMs to generate test cases, 
whereas Reflexion employs LLMs primarily for simple programming tasks. 
Most other methods, however, assume pre-defined test cases as part of their evaluation frameworks, 
leveraging them for filtering, refinement, and program selection.



\subsection{Automated Test Case Generation}
Early works, such as "Search-Based Methods for Automated Test Case Generation" by Zhang et al., leveraged search-based methods to generate test cases.
More recently, there has been a growing trend toward utilizing LLMs for test case generation.
Studies such as "LLMs for Automating Program Testing", "Generating Effective Test Cases using LLMs", and "Exploring the Effectiveness of LLMs in Automated Test Generation" by Zhang et al., have explored the effectiveness of LLMs in automating program testing.
Similarly, "Enhancing Existing Test Cases with LLMs" investigates methods for enhancing existing test cases using LLMs.
Other approaches, such as TOGA, "Test-Oracle Generation using Deep Learning", TOGLL, "Transforming Test Oracles into LLM-Assisted Generation", and the method described in "LLM-Based Test Case Generation for Improved Code Quality" focus on generating test oracles based on predefined templates.
Meanwhile, "Deep Reinforcement Learning for Text-to-Test Case Generation" introduces a model trained with deep reinforcement learning for text-to-test case generation.
The application of LLMs in software development extends to generating test cases for various scenarios.
For instance, "Generating Security Tests using GPT-4" employs GPT-4 to generate security tests for identifying vulnerabilities, while "LLMs-Assisted Unit Test Generation" uses LLMs to assist in generating unit tests for automatic software testing.
Similarly, "Creating General Software Bug Reproduction Test Cases with LLMs" demonstrates the use of LLMs to create test cases aimed at reproducing general software bugs.
LLMs are also integrated into code generation studies to improve the accuracy of generated code through test case generation.
CodeT, "Automated Test Case Generation for Improved Code Quality", utilizes LLMs with zero-shot prompts to directly generate test cases. Reflexion, "Reflexion: Using Failure Messages for Program Repair" introduces a Test Agent powered by LLMs for test case generation.


\subsection{Evolution with LLMs for Code Generation}
The integration of LLMs with evolutionary algorithms has been extensively explored to enhance code generation.
For example, Bradley et al., "OpenELM: An Open-Source Library for Evolutionary Algorithm Design" introduced OpenELM, an open-source Python library that designs specialized evolutionary operators for code generation.
Chen et al., "Adaptive Mutation and Crossover Operators using LLMs" utilized LLMs as adaptive mutation and crossover operators in evolutionary neural architecture search (NAS),
optimizing neural network design by merging LLM capabilities with evolutionary processes.
Liu et al., "Automated Approach for Evolving Optimization Algorithms" proposed an automated approach for evolving optimization algorithms using LLMs, which eliminates the need for manual design by applying standard evolutionary operations such as initialization, selection, and mutation.
In a related study, Liu et al., "Evolving Heuristics with LLMs" developed a framework for evolving heuristics by integrating LLMs with evolutionary computation to co-evolve both natural language descriptions and executable code for heuristic design, further extending previous methodologies.

Ma et al., "Reward Reflection Mechanism using Evolutionary Search" introduced an algorithm combining LLMs with evolutionary search to generate and optimize reward functions for reinforcement learning tasks, incorporating a reward reflection mechanism for refinement. 
Meyerson et al., "Few-Shot Prompting for General Crossover Operator" demonstrated the use of few-shot prompting with LLMs to generate offspring in evolutionary algorithms, enabling a general crossover operator by concatenating parent solutions into a single prompt, which can adapt to various domains.
Similarly, Romera et al., "Context-Aware Program Mutations using LLMs" integrated LLMs with evolutionary algorithms to create context-aware program mutations, eliminating the need for manually defined mutation operators. 
Ye et al., "ReEvo: Combining Evolutionary Search and LLM-Driven Reflections" proposed ReEvo, which combines evolutionary search with LLM-driven reflections to generate optimization heuristics, efficiently exploring heuristic spaces while offering verbal gradients for optimization. 
Lastly, AutoTest, "Automated Test Case Generation using Evolutionary Framework" combines automated test case generation with code execution in an evolutionary framework, utilizing LLMs to co-generate code solutions and their corresponding test cases for enhanced optimization.

Additionally, various LLM-based co-evolution methods have been proposed. 
For instance,
Chi et al., "Identifying Outdated Test Cases through Co-Evolution" identifies outdated test cases by co-evolving production and test code.
Ruan et al., "Constructing Test Oracles from Bug Reports" constructs test oracles from bug reports using predefined templates.
Chi et al., "Dynamic Validation for Automated Co-Evolution between Production and Test Code" leveraged LLMs with dynamic validation to automate co-evolution between production and test code, identifying obsolete test cases to improve software quality and reduce maintenance overhead.
TestART, "Enhancing Unit Testing through Template-Based Fixes and Coverage Feedback" enhances unit testing for LLMs by combining automated test generation with repair, using template-based fixes and coverage feedback to improve test accuracy.