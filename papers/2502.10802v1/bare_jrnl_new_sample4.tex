\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
\usepackage{algpseudocode}

\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\usepackage{tablefootnote}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage[table]{xcolor} % 加载xcolor包，并启用table选项

\usepackage{latexsym}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021


% for tarbular
% \newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}


\begin{document}

\title{
CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation
}


% \author{\IEEEauthorblockN{Anonymous Authors}}


\author{
Kefan Li,
Hongyue Yu,
Tingyu Guo,
Shijie Cao,
Yuan Yuan~\IEEEmembership{Member,~IEEE}
\thanks{
The authors are with the Beihang University. Email:
kefanli@buaa.edu.cn, Natt1e@buaa.edu.cn, tingyuguo@buaa.edu.cn, cls1277@buaa.edu.cn, yuan21@buaa.edu.cn.
}
\thanks{
Corresponding author: Yuan Yuan
}
}



% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% % The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Large Language Models (LLMs) have shown remarkable performance in automated code generation.
However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable.
While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases.
To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases.
CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers.
The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with a test case generation operator for test case evolution.
Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection.
Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, 
achieving state-of-the-art performance in automated code generation and testing.
These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.
\end{abstract}

\begin{IEEEkeywords}
Large Language Models, Code Generation, Test Case Generation, Co-Evolution
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{I}{n} recent years, Large Language Models (LLMs) have undergone rapid development and have been widely applied in automated software development tasks such as code generation and test case generation.
Models like GPT-4 \cite{hurst2024gpt}, Llama3 \cite{dubey2024llama}, Qwen2.5 \cite{hui2024qwen2}, and DeepSeek-V3 \cite{liu2024deepseek} have demonstrated exceptional code generation capabilities, significantly advancing the field of automated software development.

Despite their impressive performance, 
LLMs do not always generate accurate or correct programs.
To enhance the reliability of program generation, 
test cases are often utilized to evaluate code correctness and provide feedback. 
Existing approaches frequently rely on pre-defined test cases for evaluation. 
For instance, 
Sampling+Filtering \cite{li2022competition} first generates a batch of candidate codes, evaluates them using pre-defined test cases, and selects the code with the highest score as the final solution. 
Other methods, such as Reflexion \cite{shinn2024reflexion}, Self-Repair \cite{olausson2023self}, INTERVENOR \cite{wang2023intervenor}, LDB \cite{Zhong0S24ldb}, and PairCoder \cite{zhang2024pair}, 
utilize execution feedback on pre-defined test cases to refine or repair generated code.
% Specifically, Reflexion uses self-generated test cases for simpler datasets while leveraging pre-defined test cases for more challenging ones.
These methods generally assume that pre-defined test cases are accurate and can be fully trusted.

However, pre-defined test cases are not always available.
In test-driven development (TDD) \cite{beck2002test}, 
test cases are defined before program implementation.
In real-world scenarios, software development frequently relies on natural language requirements, with no pre-existing test cases or example programs.
Furthermore, creating pre-defined test cases can be labor-intensive, requiring significant manual effort.
In situations where LLMs must generate test cases independently, 
the effectiveness of existing methods remains uncertain.

Some approaches do not rely on pre-defined test cases. 
For instance, methods like CodeT \cite{chen2022codet} and MBR-Exec \cite{shi2022natural} generate both code and test cases using LLMs and filter the code based on the execution results of these test cases.
Similarly, CodeCoT \cite{huang2023codecot}, 
which utilizes chain-of-thought prompting \cite{brown2020language}, 
enables LLMs to generate code and test cases simultaneously and evaluate and refine the code based on these cases. 
Additionally, AgentCoder \cite{huang2023agentcoder} introduces a multi-agent system with a dedicated Tester agent to generate test cases for evaluating and repairing code.
However, studies such as \cite{chen2022codet, li2024large} highlight that LLM-generated test cases can contain significant errors, 
potentially leading to unreliable evaluations and incorrect feedback.
Moreover, many of these methods neglect to verify the quality of the generated test cases.
While approaches like CodeT and MBR-Exec implement basic filtering mechanisms for test cases,
other methods often use these generated test cases directly for code evaluation without ensuring their correctness.

There are also previous co-evolution methods based on code and test cases, 
which have primarily focused on code maintenance and program repair.
For example, \cite{shimmi2022leveraging} recommends new test cases based on source code similarity, \cite{chi2024reaccept} identifies outdated test cases, and \cite{ruan2024evolutionary} constructs test oracles from bug reports using predefined templates.
Similarly, \cite{arcuri2007coevolving} generates test cases based on predefined specifications to drive software evolution.
However, these approaches depend heavily on pre-existing programs, test specifications, or templates, limiting their applicability in scenarios lacking such resources.


\begin{figure}[!t]
\centering
\includegraphics[width=3.4in]{figures/coevo_framework.pdf}
\caption{
An overview of CoCoEvo, where programs and test cases are generated from task descriptions and evolve iteratively.
}
\label{fig:coevo_framework}
\end{figure}


To address these limitations, we propose \textbf{CoCoEvo}, an LLM-based co-evolution framework that enables the simultaneous evolution of programs and test cases to produce more accurate outcomes (illustrated in Fig. \ref{fig:coevo_framework}).
In CoCoEvo, both test cases and programs are generated by LLMs,
requiring only natural language problem descriptions and function headers. 
No pre-defined programs, test cases, or templates are needed. 
The CoCoEvo framework consists of two alternating steps: program evolution and test case evolution.
In program evolution, we introduce LLM-based crossover and mutation operators to generate program offspring.
The current population of test cases is used to evaluate program fitness.
In test case evolution, we design an additional test case generation operator to produce test case offspring.
Conversely, the current population of programs is used to evaluate test case fitness.
For programs, we implement a fitness calculation method inspired by CodeT \cite{chen2022codet},
which assesses agreement between programs and test cases.
For test cases, we adopt a multi-objective optimization approach to select those with high accuracy and strong discriminatory power,
using the Pareto method \cite{deb2000fast}.
Additionally, we introduce a dynamic crossover rate scheduling mechanism, 
enabling broader exploration during early evolution stages and faster convergence later.

To evaluate the effectiveness of CoCoEvo,
we collected a LeetCode-Contest dataset comprising relatively new programming problems that are unlikely to have been included in the training data of the evaluated LLMs.
We conducted experiments on four state-of-the-art models: GPT-4o-mini \cite{hurst2024gpt}, Qwen2.5-Coder-32B \cite{hui2024qwen2}, Llama-3.1-70B \cite{dubey2024llama}, and DeepSeek-V3 \cite{liu2024deepseek}.
We compared with both basic code generation methods such as Sampling \cite{li2022competition} and Sampling+Filtering \cite{li2022competition}, repair-based methods, and agreement-based methods.
The results demonstrate that CoCoEvo outperforms all existing methods.
Additionally, the experiments revealed that methods relying on pre-defined test cases for feedback experience significant performance degradation when adapted to self-generated test cases. 
This highlights the limitations of such methods in scenarios lacking pre-defined test cases.
Furthermore, we conducted in-depth comparative experiments and discussions, which validated the effectiveness of the optimization strategies employed in CoCoEvo.

In summary, our main contributions are as follows:
\begin{enumerate}
\item{
Co-evolution framework.
We propose a novel LLM-based co-evolution framework for programs and test cases, 
including specialized LLM-based operators for offspring generation.
}
\item{
Optimized Evolution Techniques.
For program evolution, 
we design a dynamic crossover rate scheduler that enhances exploration early on and accelerates convergence later. 
For test cases, we develop a multi-objective optimization method for fitness evaluation and selection.
}
\item{
Experimental Validation.
We introduce the LeetCode-Contest dataset and conduct experiments using four state-of-the-art LLMs: GPT-4o-mini, Qwen2.5-Coder-32b, Llama-3.1-70B, and DeepSeek-V3.
The experimental results demonstrate that CoCoEvo achieves state-of-the-art performance, surpassing existing methods.
Additionally, we conduct ablation studies to evaluate the utility of each module within the framework, further highlighting the effectiveness of our approach.
}
\end{enumerate}
The remainder of this article is organized as follows.
Section \ref{sec:related-work} discusses the related work.
In Section \ref{sec:method}, the design details and implementation methods of the CoCoEvo are described. 
Sections \ref{sec:experiments} and \ref{sec:results} present the experimental settings and discussions, respectively.
Finally, Section \ref{sec:conclusion} concludes this article and outlines future work.






\section{Related Work}
\label{sec:related-work}
\subsection{Enhancing Code Generation Accuracy with Test Case}

The integration of test cases has emerged as a pivotal strategy for improving the quality of code generated by LLMs. 
Numerous studies leverage test cases to filter, evaluate, and refine generated programs, leading to higher-performing and more reliable code. 
Below, we provide an overview of notable methodologies in this domain.


Li et al. \cite{li2022competition} introduce a Sampling+Filtering approach, 
where a large set of candidate programs is sampled and scored based on performance against a provided test set.
Chen et al. \cite{chen2022codet} propose a framework in which both code and test cases are generated by LLMs simultaneously, using a dual agreement metric to identify and select better programs. 
Shi et al. \cite{shi2022natural} assess semantic similarity by executing generated programs on test cases and leveraging the execution results to guide program selection.

Beyond evaluation, test cases play a critical role in iterative repair and refinement of generated code. 
Execution outcomes from test cases can be used to repair faulty programs. 
For instance, Shinn et al. \cite{shinn2024reflexion} introduce the Reflexion framework, 
where failure messages from test executions are converted into verbal feedback for program repair. 
Similarly, Olausson et al. \cite{olausson2023self} evaluate LLMs' capabilities to repair programs using failed test feedback. 
Zhang et al. \cite{zhang2023self} allow LLMs to analyze test execution results to revise erroneous code.
Zhong et al. \cite{Zhong0S24ldb} extend this idea by enabling step-by-step debugging.
Huang et al. \cite{huang2023codecot} adopt a chain-of-thought prompting \cite{brown2020language} approach that generates code and tests sequentially, iteratively improving the code based on test feedback.
Zhang et al. \cite{zhang2024pair} introduce a pair programming framework where test feedback informs repair strategies and planning.

The generation of test cases differs across these methodologies.
For example, CodeT \cite{chen2022codet} and CodeCoT \cite{huang2023codecot} rely entirely on LLMs to generate test cases, 
whereas Reflexion employs LLMs primarily for simple programming tasks. 
Most other methods, however, assume pre-defined test cases as part of their evaluation frameworks, 
leveraging them for filtering, refinement, and program selection.



\subsection{Automated Test Case Generation}
Early works, such as~\cite{fraser2011evosuite}, leveraged search-based methods to generate test cases.
More recently, there has been a growing trend toward utilizing LLMs for test case generation.
Studies such as ~\cite{siddiq2024using}, ~\cite{guilherme2023initial}, and ~\cite{schafer2023empirical} have explored the effectiveness of LLMs in automating program testing.
Similarly, ~\cite{alshahwan2024automated} investigates methods for enhancing existing test cases using LLMs.
Other approaches, such as TOGA~\cite{dinella2022toga}, TOGLL~\cite{hossain2024togll}, and the method described in~\cite{hossain2023neural}, 
focus on generating test oracles based on predefined templates.
Meanwhile, ~\cite{takerngsaksiri2024tdd} introduces a model trained with deep reinforcement learning for text-to-test case generation.
The application of LLMs in software development extends to generating test cases for various scenarios.
For instance, ~\cite{zhang2023well} employs GPT-4 to generate security tests for identifying vulnerabilities, while ~\cite{schafer2023empirical} uses LLMs to assist in generating unit tests for automatic software testing.
Similarly, ~\cite{kang2023large} demonstrates the use of LLMs to create test cases aimed at reproducing general software bugs.
LLMs are also integrated into code generation studies to improve the accuracy of generated code through test case generation.
CodeT~\cite{chen2022codet}, for example, utilizes LLMs with zero-shot prompts to directly generate test cases. Reflexion~\cite{shinn2024reflexion} introduces a Test Agent powered by LLMs for test case generation.


\subsection{Evolution with LLMs for Code Generation}
The integration of LLMs with evolutionary algorithms has been extensively explored to enhance code generation.
For example, Bradley et al. \cite{bradley2024openelm} introduced OpenELM, an open-source Python library that designs specialized evolutionary operators for code generation.
Chen et al. \cite{chen2023evoprompting} utilized LLMs as adaptive mutation and crossover operators in evolutionary neural architecture search (NAS),
optimizing neural network design by merging LLM capabilities with evolutionary processes.
Liu et al. \cite{liu2023algorithm} proposed an automated approach for evolving optimization algorithms using LLMs, which eliminates the need for manual design by applying standard evolutionary operations such as initialization, selection, and mutation.
In a related study, Liu et al. \cite{liu2024evolution} developed a framework for evolving heuristics by integrating LLMs with evolutionary computation to co-evolve both natural language descriptions and executable code for heuristic design, further extending previous methodologies.

Ma et al. \cite{ma2023eureka} introduced an algorithm combining LLMs with evolutionary search to generate and optimize reward functions for reinforcement learning tasks, incorporating a reward reflection mechanism for refinement. 
Meyerson et al. \cite{meyerson2024language} demonstrated the use of few-shot prompting with LLMs to generate offspring in evolutionary algorithms, enabling a general crossover operator by concatenating parent solutions into a single prompt, which can adapt to various domains.
Similarly, Romera et al. \cite{romera2024mathematical} integrated LLMs with evolutionary algorithms to create context-aware program mutations, eliminating the need for manually defined mutation operators. 
Ye et al. \cite{ye2024reevo} proposed ReEvo, which combines evolutionary search with LLM-driven reflections to generate optimization heuristics, efficiently exploring heuristic spaces while offering verbal gradients for optimization. 
Lastly, AutoTest \cite{duan2024autotest} combines automated test case generation with code execution in an evolutionary framework, utilizing LLMs to co-generate code solutions and their corresponding test cases for enhanced optimization.

Additionally, various LLM-based co-evolution methods have been proposed. 
For instance,
Chi et al. \cite{chi2024reaccept} identifies outdated test cases by co-evolving production and test code.
Ruan et al. \cite{ruan2024evolutionary} constructs test oracles from bug reports using predefined templates.
Chi et al. \cite{chi2024reaccept} leveraged LLMs with dynamic validation to automate co-evolution between production and test code, identifying obsolete test cases to improve software quality and reduce maintenance overhead.
TestART \cite{gu2024testart} enhances unit testing for LLMs by combining automated test generation with repair, using template-based fixes and coverage feedback to improve test accuracy.




\section{Methods}
\label{sec:method}
\subsection{Framework Overview}


\begin{figure*}[!t]
\centering
\includegraphics[width=7.0in]{figures/coevo_loop.pdf}
\caption{Illustration of the co-evolution loop.}
\label{fig:coevo_loop}
\end{figure*}



An overview of CoCoEvo is presented in Fig. \ref{fig:coevo_loop}.
The evolutionary process begins with the random initialization of both the program population and the test case population,
achieved through the random generating programs and test cases using LLMs.
The co-evolution process follows a loop consisting of two alternating steps: program evolution and test case evolution, each of which includes a \emph{Generation-Evaluation-Selection} sequence.
During program evolution, the test case population is used to evaluate programs and calculate their fitness. 
Conversely, during test case evolution, the program population is utilized to evaluate test cases.

For programs, we introduce LLM-based crossover and LLM-based mutation operators for offspring generation.
For test cases, we implement an LLM-based additional test case generation operator.
Once the preset number of iterations is reached, the co-evolution loop terminates, and the program with the highest fitness score is returned as the final solution.
Algorithm \ref{alg:cocoevo} provides the pseudo-code detailing the CoCoEvo process.
Further implementation details for each algorithm step are elaborated in subsequent sections.



\begin{algorithm}[H]
\caption{Pseudocode of CoCoEvo}
\label{alg:cocoevo}
\begin{algorithmic}[1]
\Require Problem description $prompt$, Program population size $size$, max iterations $max\_iter$, crossover rate scheduler $cosine\_scheduler$
\Ensure $p_{best}$
\State program population $P \gets LLM_{program}(prompt)$
\State test case population $T \gets LLM_{test\_case}(prompt)$
\State evaluate $P$ on $T$ , calculate confidence $Conf_{P}$

\State // Co-evolution loop
\For{$r = 1$ \textbf{to} $max\_iter$}
    \State // Program evolution
    \State crossover rate $x \gets cosine\_scheduler(r)$
    \State the number of crossover operations $N_c \gets \lfloor size \times x \rfloor$
    \State the number of mutation operations $N_m \gets size - N_c$

    \State program offspring $P' \gets \emptyset$
    \For{$i = 1$ \textbf{to} $N_c$}
        \State select crossover parents $p_1, p_2 \gets \text{binary\_tournament}(P, Conf_P)$
        \State crossover offspring $p' \gets LLM_{crossover}(p_1, p_2)$
        \State $P' \gets P' \cup \{p'\}$
    \EndFor

    \For{$i = 1$ \textbf{to} $N_m$}
        \State select mutation parent $p \gets \text{random}(P, Conf_P)$
        \State mutation offspring $p' \gets LLM_{mutation}(p)$
        \State $P' \gets P' \cup \{p'\}$
    \EndFor
    \State evaluate $P'$ on $T$ , calculate confidence $Conf_{P'}$
    \State $P \gets \text{greedy}(P \cup P', Conf_{P,P'})$

    \State // Test case evolution
    \State program feedback $f \gets \text{execute}(T, P_{best})$
    \State additional test cases $T' \gets LLM_{test}(T, f)$
    \State $T \gets T \cup T'$
    \State evaluate $T$ on $P$, calculate confidence $Conf_{T}$, discrimination $disc_{T}$
    \State $T \gets \text{Pareto}(T, Conf_{T}, Disc_{T})$
\EndFor
\Return $p_{best}$
\end{algorithmic}
\end{algorithm}






\subsection{Program Evolution}
This section provides a detailed description of strategies employed in program evolution. 

\begin{figure}[!t]
\centering
\includegraphics[width=1.4in]{figures/cross.pdf}
\caption{
Cross-evaluation of programs and test cases, where each combination is assessed for pass/fail outcomes.
}
\label{fig:cross}
\end{figure}

\textbf{Program fitness function.} 
During evaluation, programs and test cases undergo cross-evaluation, as illustrated in Fig. \ref{fig:cross}. 
The outcomes are represented by a matrix $M$:
\begin{equation}
\label{eq:cross}
M_{i,j} = \begin{cases} 
1, & \text{\(program_i\) passed \(test_j\)} \\
0, & \text{\(program_i\) failed \(test_j\)}.
\end{cases}
\end{equation}
We compute the confidence score of each program,
adopting the approach in \cite{chen2022codet}. 
Programs that pass the same set of test cases are grouped into a set $P_{s}$,
while the corresponding test cases are grouped into a set $T_{s}$.
The confidence of program $i$ is then defined as
\begin{equation}
\label{eq:code-codet}
Conf_{P,i} = \sqrt{|P_{s}|} \times |T_{s}|
\end{equation}
where $P_{s}$ represents the set containing program $i$, and $T_{s}$ represents the associated test set.
The confidence score evaluates the likelihood of a program being correct.
The fitness of program $i$ is defined as
\begin{equation}
\label{eq:code-fitness}
F_{P,i} = Conf_{P,i}
\end{equation}
where a higher fitness score indicates a higher probability of correctness for the program.



\textbf{Crossover Rate Scheduler}
To address the multimodal nature of the program search space and mitigate the risk of premature convergence,
a dynamic scheduler is employed to regulate crossover and mutation rates. 
A higher mutation rate is applied in the early stages to promote exploration,
while the mutation rate decreases, and the crossover rate increases in later stages to refine convergence.

Unlike traditional random mutation methods, 
the LLM-based program mutation operator leverages LLMs for code generation, which could retain key features of high-quality programs.
This allows for a higher mutation rate without significant loss of desirable traits.
We adopt the cosine annealing method \cite{LoshchilovH17cosine} for scheduling crossover rate:
\begin{equation}
\label{eq:cosine}
x(r) = x_{final} + \frac{1}{2}( x_{initial} - x_{final})(1+cos(\frac{\pi r}{R})).
\end{equation}

This formulation ensures that the crossover rate increases smoothly from $0$ to $1$.
The number of crossover operations and the number of mutation operations are defined as follows:
\begin{equation}
\label{eq:rate}
\begin{cases} 
N_{r,c} = x(r)\\
N_{r,m} = N_P - N_{r,c}
\end{cases}
\end{equation}
where $N_P$ is the program population size,
$N_{r,c}$ and $N_{r,m}$ represent the number of program crossover and mutation operations in iteration round $r$, respectively.
When the initialization is completed, 
the iteration round $r$ is set to $1$.
After initialization, the iteration round $r$ starts at $1$ and increments by $1$ after a co-evolution loop.


\begin{figure}[!t]
\centering
\includegraphics[width=2.0in]{figures/cosine.pdf}
\caption{
Illustration of the variation of the crossover rate scheduled based on the cosine annealing method.
}
\label{fig:cosine}
\end{figure}

Fig. \ref{fig:cosine} illustrates the variation in crossover rates over $9$ evolutionary rounds.
This approach ensures a smooth transition between exploration (early stages) and convergence (later stages), 
enabling effective optimization of the solution space.






\subsection{Test Case Evolution}
\label{sec:test-evo}

\textbf{Test fitness function.}
The evaluation of test cases involves two key metrics: \emph{confidence} and \emph{discrimination}.
The confidence of test case $j$ is calculated as
\begin{equation}
\label{eq:test-conf}
Conf_{T,j} = \frac {1} {N_{P}} \sum_{i=1}^{N_{P}} {M_{i,j} F_{P,i}}
\end{equation}
where $N_P$ represents the total number of programs,
$F_{P,i}$ represents the fitness of program $i$, and $M_{i,j}$ is the evaluation matrix indicating whether $program_i$ passed $test_j$.
Intuitively, confidence measures the degree of agreement between the program population and the test cases,
with higher program fitness contributing more weight to the degree of agreement.

Simple test cases may fail to differentiate between correct and erroneous programs, as both types may pass them. 
To address this, we introduce a discrimination metric.
First, the pass rate of test case $j$ across the program population is defined as
\begin{equation}
\label{eq:test-p}
p_j = \frac {1} {N_{P}} \sum_{j=1}^{N_{P}} {M_{i,j}},
\end{equation}
then the discrimination of test case $j$ is represented by the entropy of the pass rate:
\begin{equation}
\label{eq:test-disc}
Disc_{T,j} = -p_j log_{2} p_j - (1-p_j) log_{2} p_j (1-p_j).
\end{equation}

Test cases achieve higher discrimination when their pass rate approaches $0.5$, as they better differentiate among the program population. 
This indicator assesses whether the test cases can exert evolutionary pressure on the program population.


\begin{figure}[!t]
\centering
\includegraphics[width=2.2in]{figures/pareto.pdf}
\caption{
Illustration of the Pareto front constructed based on test case confidence and discrimination.  
}
\label{fig:pareto}
\end{figure}


The selection of test cases is performed using the Pareto approach for multi-objective optimization \cite{deb2000fast}, 
as illustrated in Fig. \ref{fig:pareto}.
The Pareto front is constructed based on the two optimization metrics, confidence and discrimination.
Test cases with high confidence and low discrimination represent basic test cases that cover common scenarios, 
with a high probability of being correct.
Test cases with high discrimination represent more challenging cases that are effective at exposing errors.
We noticed that there are some test cases with very low confidence levels that sometimes have a very high degree of discrimination.
To avoid this type of test case rom being selected, the test cases with confidence lower than the average in the selection results will be filtered out. 





\subsection{LLM-based evolution operators}

\textbf{Program crossover operator.} 

We utilize LLMs to perform program crossover,
as illustrated in Fig. \ref{fig:crossover}. 
The process involves the following steps:
\begin{enumerate}
\item{
Selection. Two programs are selected as crossover parents using the binary tournament selection method based on their fitness scores.
}
\item{
Crossover. The selected programs are combined into a prompt. The LLM is instructed to analyze their similarities and differences and generate a new program that merges useful elements from both.
}
\end{enumerate}

\begin{figure}[!t]
\centering
\includegraphics[width=3.4in]{figures/code_crossover.pdf}
\caption{Illustration of LLM-based program crossover. }
\label{fig:crossover}
\end{figure}

For example, in Fig. \ref{fig:crossover},
two incorrect programs are sent to the LLM.
By analyzing their structures, the LLM combines useful fragments from each and generates a new program that resolves the errors.


\textbf{Program mutation operator.}
The LLM-based mutation operator intelligently rewrites a program to achieve the same functionality using a different approach.
The steps are as follows:
\begin{enumerate}
\item{
Selection. A program is randomly selected from the current population as the mutation parent.
}
\item{
Mutation. The parent program is included in a prompt,
and the LLM is instructed to rewrite it using an alternative implementation method.
}
\end{enumerate}

\begin{figure}[!t]
\centering
\includegraphics[width=3.4in]{figures/code_mutation.pdf}
\caption{Illustration of LLM-based program mutation.}
\label{fig:mutation}
\end{figure}


For example, Fig. \ref{fig:mutation} demonstrates a mutation scenario where the task is to find the maximum difference between adjacent elements in a 2D grid.
The parent program has a flaw—it only considers elements in the same row or column.
The LLM analyzes the problem and rewrites the program to include diagonal relationships, 
successfully resolving the flaw.





\textbf{Additional test cases generation operator.}
LLMs are also employed to generate additional test cases,
as shown in Fig. \ref{fig:test_gen}. 
The process includes the following steps:
\begin{enumerate}
\item {
Information Construct.
The prompt contains the following information:
The entire existing population of test cases,
the best program the program population and the line coverage information on it, where covered lines are marked as ``[+]'' and uncovered lines as ``[-]''.
}
\item {
Rethinking and Generation.
Based on the information, the LLMs are required to generate additional test cases:
If there are uncovered lines, the LLMs generate test cases to achieve full line coverage.
If all lines are already covered, the LLMs analyze the existing test cases to identify untested boundary conditions that could reveal potential program flaws.
}
\end{enumerate}

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/test_offspring.pdf}
\caption{
Illustration of LLM-based rethinking and additional test case generation.
}
\label{fig:test_gen}
\end{figure}

For instance, in Fig. \ref{fig:test_gen}, the LLM identifies an untested boundary scenario: the input list $nums2$ is not in ascending order.
It then generates a new test case with a non-ascending $nums2$, exposing the flaw in the provided program.









\section{Experiments}
\label{sec:experiments}
\subsection{Dataset}


We collected a dataset of $80$ programming problems from the weekly and biweekly contests hosted on the LeetCode platform \footnote{\url{https://leetcode.com/}}, forming the LeetCode-Contest dataset.
In the dataset, each problem includes a prompt comprising the function header and a natural language description in the form of a docstring,
along with test cases and reference solutions.
To ensure data integrity and avoid potential leakage, all selected problems were released after March 2024.

To more accurately assess the ability of LLMs to solve programming challenges, 
we took measures to prevent plagiarism of example test inputs and outputs in test case generation.
Specifically, we removed all test input-output samples from the problem descriptions, 
retaining only the natural language problem statements, the function header, and the associated constraints.

For each problem, we collected Python solution codes and a substantial set of test cases available on the website.
The detailed information of the LeetCode-Contest dataset is summarized in Table \ref{tab:leetcode}.

\begin{table}[!t]
\caption{
Illustration of the detailed information of the LeetCode-Contest dataset.
\label{tab:leetcode}
}
\centering
\begin{tabular}{lc}
\toprule

Problems            & 80 \\

\midrule

Average Test Cases  & 644.40 \\

\midrule

Selected Contests   & 
\makecell{
weekly-contest-402, weekly-contest-401, \\
biweekly-contest-132, weekly-contest-400, \\
weekly-contest-399, biweekly-contest-131, \\
weekly-contest-398, weekly-contest-397, \\
biweekly-contest-130, weekly-contest-396, \\
weekly-contest-395, biweekly-contest-129, \\
weekly-contest-394, weekly-contest-393, \\
biweekly-contest-128, weekly-contest-392, \\
weekly-contest-391, biweekly-contest-127, \\
weekly-contest-390, weekly-contest-389
} \\

\bottomrule
\end{tabular}
\end{table}


\subsection{Models}

We utilized four widely adopted and powerful models for our experiments: GPT-4o-mini \cite{hurst2024gpt}, Qwen2.5-Coder-32B \cite{hui2024qwen2}, Llama-3.1-70B \cite{dubey2024llama}, and DeepSeek-V3 \cite{liu2024deepseek}.
For GPT-4o-mini, we accessed the API via the OpenAI platform \footnote{\url{https://platform.openai.com/}}.
For Qwen2.5-Coder-32B and Llama-3.1-70B, we used the APIs available on the DeepInfra platform \footnote{\url{https://deepinfra.com/}}.
For DeepSeek-V3, we utilized the API provided by the DeepSeek platform \footnote{\url{https://platform.deepseek.com/}}.


\subsection{Compared Baselines}
We selected several of the most prominent and effective methods in the field of code generation with LLMs as our comparison baselines:
\begin{enumerate}
\item{
Sampling \cite{li2022competition}.
A basic method that generates a large number of code samples and randomly selects one as the final answer.
}
\item{
Sampling+Filtering \cite{li2022competition}. 
An enhancement of the Sampling method, where the generated codes are evaluated using test cases.
The code with the highest score is selected as the final answer.
}
\item{
Self-Repair \cite{olausson2023self}.
This method generates code randomly, and then feeds execution results on test cases back to the LLM, which repairs the code accordingly. 
}
\item{
Reflexion \cite{shinn2024reflexion}.
Error information from test case executions is used to guide the LLM in providing reflection and repair suggestions. The code is then repaired based on these suggestions.
}
\item{
INTERVENOR \cite{wang2023intervenor}.
Utilizes a Teacher-Student multi-agent framework. The Teacher agent analyzes error information from test case execution and provides repair suggestions, while the Student agent applies the fixes.
}
\item{
CodeCoT \cite{huang2023codecot}.
Based on the chain-of-thought paradigm \cite{brown2020language}, this method requires the LLM to generate both code and test cases simultaneously. Feedback from execution errors is then used to repair the code.
}
\item{
AgentCoder \cite{huang2023agentcoder}.
Implements two LLM-based agents: a Programming agent for code generation and repair, and a Testing agent for test case generation.
}
\item{
MBR-Exec \cite{shi2022natural}.
Generates both code and test cases using LLMs.
A loss function evaluates execution similarity based on the outputs of the generated code when run on test inputs.
Codes with higher execution divergence receive higher losses, and the code with the lowest loss is selected as the final answer.
}
\item{
CodeT \cite{chen2022codet}.
Generates code and test cases with LLMs, 
then performs clustering based on how the codes perform on the test cases.
The clustering score is calculated as the product of the number of codes in a cluster and the number of test cases passed.
}
\end{enumerate}

For all baselines,
if the method requires test cases for evaluation or feedback generation,
the test cases are generated using the same LLMs that perform code generation.


\subsection{Performance Indicators}
The ground truth test cases in the dataset are used to evaluate the accuracy of the generated code.
A code is considered correct if it passes all the test cases.
The performance of the approaches on the dataset is measured using the pass@1 metric \cite{kulal2019spoc}.
To calculate the pass@1 metric, each method submits a single program as the final answer for each problem.
The pass@1 metric represents the percentage of problems successfully solved.

Additionally, for further analysis of the CoCoEvo approach,
the ground truth program solutions are used to evaluate the correctness of the generated test cases.
A generated test case is considered correct if it executes successfully on the ground truth solution.




\subsection{Experimental Settings}
Given that invoking LLMs for code generation or repair consumes significantly more time and resources compared to executing code,
we evaluated the performance of all methods under the condition that the number of LLM calls for code generation or repair is held constant.
To ensure practical usability and align with the parameter settings of most code generation methods,
we set the number of LLM calls for code generation to $100$ iterations.
For methods requiring LLM-generated test cases,
we limited test case generation to $10$ iterations, with a maximum of $10$ test cases extracted per iteration.

The parameter settings of the CoCoEvo method can be found in Table \ref{tab:param}.
\begin{table}[!t]
\caption{
Illustration of the parameter settings of CoCoEvo.
\label{tab:param}
}
\centering
\begin{tabular}{lc}
\toprule
Program Population Size         & 10   \\
\midrule
Iterations                      & 10   \\
\midrule
Min Test Case Population Size   & 10   \\
\bottomrule
\end{tabular}
\end{table}
We observed that when calculating the pass@1 metric,
if multiple codes achieve the same score, one code will be randomly selected as the final result.
This introduces a certain degree of randomness to the evaluation process. 
To mitigate this, for all methods, we independently and randomly repeated the ``selection $\rightarrow$ evaluation'' process $5$ times and reported the averaged results.




\section{Results and Discussion}
\label{sec:results}
\subsection{Comparison of Accuracy}

\begin{table*}[!t]
\caption{
Illustration of pass@1 performance across all methods on the LeetCode-Contest dataset.
\label{tab:pass1}
}
\centering
\begin{threeparttable}
\begin{tabular}{lcccccc}
\toprule

Method & GPT-4o-mini & Qwen2.5-Coder-32B & Llama-3.1-70B & DeepSeek-V3 \\

\midrule

Sampling
& 35.25 & 44.00 & 32.00 & 69.00  \\
Sampling+Filtering\tnote{*}
& 37.50 & 44.50 & 31.00 & 68.25  \\

\midrule

Self-Repair\tnote{*}
& 33.00 & 42.00 & 29.25 & 60.75  \\

Reflexion\tnote{*}
& 37.50 & 48.75 & 25.00 & 62.50  \\

INTERVENOR\tnote{*}
& 27.75 & 20.25 & 20.25 & 47.50  \\


\midrule

CodeCoT
& 31.25 & 36.50 & 19.50 & 63.50 \\

AgentCoder
& 31.25 & 41.75 & 22.00 & 60.25  \\

\midrule

MBR-Exec
& 33.75 & 45.00 & 38.75 & 70.00 \\

CodeT
& 46.25 & 47.50 & 41.25 & 72.50  \\

\midrule

\textbf{CoCoEvo}
& \textbf{49.75} & \textbf{55.75} & \textbf{45.00} & \textbf{76.25} \\

\bottomrule
\end{tabular}
\begin{tablenotes}
\item[*] Denotes that the methods were originally designed to operate with pre-defined test cases.
For this work, we adapted these methods to utilize test cases generated by the LLMs.
\end{tablenotes}
\end{threeparttable}

\end{table*}


In this section, we present the results of the accuracy comparison for the methods.
Table \ref{tab:pass1} reports the pass@1 metric for each method across different LLMs on the LeetCode-Contest dataset.
As shown in Table \ref{tab:pass1}, our proposed CoCoEvo method achieves the highest performance across all LLMs, demonstrating its superior ability to generate accurate code.
This success can be attributed to the co-evolution framework employed by CoCoEvo, which facilitates a more effective iterative refinement between programs and test cases.

An interesting observation concerns the repair-based methods such as Self-Repair, Reflexion, and INTERVENOR.
When these methods utilize test cases generated by LLMs instead of pre-defined ones,
their effectiveness diminishes significantly.
In some instances, their performance is even worse than that of the basic Sampling approach.
This is because the erroneous test cases generated by LLMs can mislead these methods by providing inaccurate feedback, causing the repair process to make suboptimal adjustments to the code.
Consequently, this results in lower accuracy and poorer overall performance.
Similarly, for methods like Sampling+Filtering,
where test cases are used to filter and select the best code, performance remains comparable to the Sampling method when the test cases are generated by LLMs.
This highlights a critical limitation: these methods struggle to adapt effectively in scenarios where reliable pre-defined test cases are unavailable.

Furthermore, methods that leverage the agreement between programs and test cases, such as MBR-Exec and CodeT, exhibit higher performance compared to Sampling and repair-based approaches.
Among these, CodeT stands out, delivering consistent improvements in pass@1 across all LLMs.
This suggests that considering the alignment and mutual validation of programs and test cases is a promising strategy for enhancing code generation accuracy.



\begin{figure*}[!t]
\centering
\includegraphics[width=6.8in]{figures/gens.pdf}
\caption{
The graph illustrates the pass@1 performance during the evolutionary process.
The shaded region in the graph indicates the data distribution, expressed as the standard deviation.
The gray dashed line represents the final performance achieved by the CodeT method.
}
\label{fig:gens}
\end{figure*}



Fig. \ref{fig:gens} illustrates the changes of pass@1 during the evolution process.
While fluctuations in performance are observed,
the method demonstrates consistently strong performance on average.
The figure shows that although the CoCoEvo method starts with a relatively low pass@1,
its performance steadily improves over the course of evolution iterations, eventually surpassing that of the CodeT method.
This demonstrates the effectiveness of the CoCoEvo method.
By alternating the evolution of programs and test cases, the program population progressively converges toward the correct solution.


\begin{figure*}[!t]
\centering
\includegraphics[width=6.8in]{figures/tests.pdf}
\caption{
The graph illustrates the test case accuracy during the evolutionary process.
The gray dashed line represents the baseline accuracy achieved by randomly generated test cases.
}
\label{fig:tests}
\end{figure*}


Fig. \ref{fig:tests} illustrates the changes in the accuracy of the test population throughout the evolution process.
At initialization, the CoCoEvo method achieves a slightly higher test accuracy than the randomly generated method.
Specifically, the CoCoEvo method generates test cases once and extracts $10$ test cases, whereas the random method involves generating test cases $10$ times and extracting $10$ test cases each time.
This indicates that as the number of test case generations by LLMs increases, test accuracy tends to decrease slightly.

Interestingly, For the CoCoEvo method, the peak test accuracy is observed in the second iteration.
In subsequent iterations, the accuracy declines slightly, potentially due to the Pareto selection process, 
where test cases with higher discrimination but lower confidence are accepted.
This may inadvertently introduce some incorrect test cases.
Nevertheless, the decline is gradual, and the test accuracy remains consistently high, making this behavior acceptable.





\subsection{Comparison of Program Evolution Scheduler}

\begin{table}[!t]
\caption{
Comparison of pass@1 performance across different program crossover rate schedulers.
\label{tab:cmp-p-sche}
}
\centering
\begin{tabular}{lc}
\toprule

Crossover Rate Scheduler & Pass@1 \\

\midrule

Constant & 51.00 \\

\midrule

\textbf{Cosine} & \textbf{55.75} \\

\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/cp_p_sche.pdf}
\caption{
Program pass@1 performance during the evolutionary process for different crossover rate schedulers.
}
\label{fig:cmp-p-sche}
\end{figure}


To assess the effectiveness of the crossover rate scheduler, we compare the cosine scheduler with a constant scheduler in our experiments, using a mutation rate of $0.2$ and a crossover rate of $0.8$.
These experiments were performed on the Qwen2.5-Coder-32B and the LeetCode-Contest dataset.
The results, summarized in Table~\ref{tab:cmp-p-sche} and Figure~\ref{fig:cmp-p-sche}, highlight key differences between the cosine scheduler and the constant rate approach.

At the early stages of the evolution process, the constant method, characterized by a fixed crossover and mutation rate, demonstrates faster convergence and initially outperforms the cosine scheduler due to its higher crossover rate.
However, this rapid convergence causes the Constant method to stagnate and become trapped in a local optimum, ultimately limiting its overall performance.

In contrast, the cosine scheduler, despite its lower initial performance, maintains consistent improvement throughout the evolution process.
By adapting the crossover rate dynamically, the scheduler avoids premature convergence and achieves superior results in later stages.
Notably, it surpasses the performance of the Constant method after the fifth iteration, ultimately delivering a higher pass@1 rate.

This comparison underscores the importance of dynamic scheduling in optimizing program evolution, particularly in scenarios prone to local optima.







\subsection{Comparison of Program Fitness Function}
\begin{table}[!t]
\caption{
Comparison of pass@1 performance across different program fitness functions.
\label{tab:p-fitness}
}
\centering
\begin{tabular}{lc}
\toprule

Program Fitness Function & Pass@1 \\
\midrule

Pass Rate & 53.50 \\

\midrule

\textbf{CodeT Score} & \textbf{55.75} \\

\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/cp_p_fitness.pdf}
\caption{
Program pass@1 performance during the evolutionary process for different program fitness functions.
}
\label{fig:cmp-p-fitness}
\end{figure}

To evaluate the impact of different program fitness functions, we conducted a comparative analysis using the CodeT score and the simple pass rate as fitness functions.
The base model for these experiments was Qwen2.5-Coder-32B with the LeetCode-Contest dataset serving as the benchmark.
The results are summarized in Table~\ref{tab:p-fitness} and illustrated in Figure~\ref{fig:cmp-p-fitness}.

The findings reveal the insight that calculating the pass rate on test cases directly as the fitness function yields a lower pass@1 value compared to utilizing the CodeT score.
This behavior can be attributed to the advantages of the CodeT score, which incorporates the congruence between programs and test cases more effectively.
By accounting for this alignment, the CodeT score mitigates the negative effects of erroneous or poorly designed test cases on program fitness, leading to more stable and improved performance.
These results underscore the superiority of the CodeT score as a program fitness function, especially in scenarios where robustness and convergence are critical.





\subsection{Comparison of Test Fitness Function}


\begin{table}[!t]
\caption{
Comparison of pass@1 performance across different test case fitness functions.
\label{tab:t-fitness}
}
\centering
\begin{tabular}{lc}
\toprule

Test Fitness Function & Pass@1 \\
\midrule

Failure Rate & 37.00 \\

\midrule

Pass Rate & 51.50 \\

\midrule

Confidence & 51.50 \\

\midrule

\textbf{Pareto} & \textbf{55.75} \\

\bottomrule
\end{tabular}
\end{table}


\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/cp_t_fitness.pdf}
\caption{
Program pass@1 performance during the evolutionary process for different test case fitness functions.
}
\label{fig:cmp-t-fitness}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/cp_t_acc.pdf}
\caption{
Test case accuracy during the evolutionary process for different test case fitness functions.
}
\label{fig:cmp-t-acc}
\end{figure}

In this section, we present the results of comparing various test fitness functions.
Specifically, we evaluate Failure Rate, Pass Rate, Confidence, and Pareto as methods for test fitness calculation.
The experiments are conducted using the Qwen2.5-Coder-32B model on the LeetCode-Contest dataset.
The detailed results are depicted in Fig. \ref{fig:cmp-t-fitness}, and additional information on test case accuracy is provided in Fig. \ref{fig:cmp-t-acc}.

Among the test fitness functions, Pass Rate directly calculates the proportion of test cases successfully passed by a program, while Confidence represents a weighted pass rate, as described in Section \ref{sec:test-evo}.
Similarly, Failure Rate measures the failure rate of a program on the test cases.
For these three fitness calculation methods, greedy selection is employed as the offspring selection method.

Our findings indicate that employing the Pareto method as the selection mechanism yields the best performance.
The Pareto approach prioritizes test cases with both high discrimination and high confidence levels, thereby demonstrating superior effectiveness.
In contrast, using Failure Rate as the fitness function results in a sharp decline in pass@1 performance.
Previous studies on co-evolution have generally argued that the confidence level of a test case increases as more programs fail on it.
However, we discovered that if the test cases contain errors, this method can lead to a population dominated by faulty test cases, which fail to provide accurate feedback to the programs.

Furthermore, Fig. \ref{fig:cmp-t-fitness} reveals that employing Confidence as the test fitness function outperforms using Pass Rate by a small margin.
The Confidence function weights the fitness of test cases based on program performance, with programs demonstrating higher fitness values contributing more heavily to the confidence score.
This approach ensures that more reliable programs have a greater influence on test case evaluation.








\subsection{Ablation Study}



\begin{table}[!t]
\caption{
Illustrates the results of the ablation experiments.
Where ``w/o test evolution'' represents the method of removing the test case evolution module in the CoCoEvo approach.
\label{tab:ablation}
}
\centering
\begin{tabular}{lc}
\toprule

Method & Pass@1 \\
\midrule

Sampling & 44.00 \\

\midrule

w/o test evolution & 51.75 \\

\midrule 

\textbf{CoCoEvo} & \textbf{55.75} \\

\bottomrule
\end{tabular}
\end{table}



\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{figures/ablation.pdf}
\caption{
Illustrates the evolutionary process in the ablation study.
}
\label{fig:ablation}
\end{figure}

We conducted ablation experiments using the Qwen2.5-Coder-32B model on the LeetCode-Contest dataset to evaluate the effectiveness of the test case evolution module.
In these experiments, we removed the test case evolution module and directly utilized the test cases generated by the LLMs to evaluate the code.
The comparison results are presented in Table \ref{tab:ablation} and Fig. \ref{fig:ablation}.

The experimental results demonstrate that using only the program evolution module, without incorporating the test case evolution module, still outperforms the baseline sample-based method. However, after integrating the test case evolution module, the performance of the CoCoEvo method is further enhanced.
This highlights the critical role of the test case evolution module in the co-evolution process.

Through the co-evolution of test cases, two key benefits are achieved.
First, the accuracy of test cases within the population improves, as they are refined based on the agreement among programs.
Second, by applying the Pareto method, discriminative and challenging test cases are preserved.
These refined test cases provide more accurate and meaningful feedback to the program evolution process, ultimately improving the program’s pass rate in the final results.



\section{Conclusion}
\label{sec:conclusion}
This paper introduced CoCoEvo, an LLM-based co-evolution framework that advances automated code generation by integrating simultaneous program and test case evolution.
By eliminating the reliance on pre-defined test cases, CoCoEvo addresses a critical limitation in existing approaches, particularly in scenarios lacking comprehensive testing resources.
The dynamic crossover rate scheduling mechanism and the multi-objective optimization for test case selection were key to the framework's success, enabling effective exploration and refinement throughout the evolution process. 
Experimental results on the LeetCode-Contest dataset, utilizing four leading LLMs, demonstrated that CoCoEvo achieved superior performance compared to traditional methods.

Beyond its practical implications in software development, CoCoEvo opens new avenues for integrating evolutionary algorithms with LLMs.
Future work will focus on enhancing the scalability of CoCoEvo for project-level code generation and incorporating additional quality metrics for test case evaluation.
Furthermore, exploring its application in other domains, such as automated debugging and software maintenance, could further expand its impact.

















% \section{Other Resources}
% See \cite{ref1,ref2,ref3,ref4,ref5} for resources on formatting math into text and additional help in working with \LaTeX .

% \section{Text}


% \section*{Acknowledgments}
% This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.


% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}



 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%



% \begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibliography{paper}

% \end{thebibliography}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


