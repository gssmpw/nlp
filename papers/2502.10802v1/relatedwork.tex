\section{Related Work}
\label{sec:related-work}
\subsection{Enhancing Code Generation Accuracy with Test Case}

The integration of test cases has emerged as a pivotal strategy for improving the quality of code generated by LLMs. 
Numerous studies leverage test cases to filter, evaluate, and refine generated programs, leading to higher-performing and more reliable code. 
Below, we provide an overview of notable methodologies in this domain.


Li et al. \cite{li2022competition} introduce a Sampling+Filtering approach, 
where a large set of candidate programs is sampled and scored based on performance against a provided test set.
Chen et al. \cite{chen2022codet} propose a framework in which both code and test cases are generated by LLMs simultaneously, using a dual agreement metric to identify and select better programs. 
Shi et al. \cite{shi2022natural} assess semantic similarity by executing generated programs on test cases and leveraging the execution results to guide program selection.

Beyond evaluation, test cases play a critical role in iterative repair and refinement of generated code. 
Execution outcomes from test cases can be used to repair faulty programs. 
For instance, Shinn et al. \cite{shinn2024reflexion} introduce the Reflexion framework, 
where failure messages from test executions are converted into verbal feedback for program repair. 
Similarly, Olausson et al. \cite{olausson2023self} evaluate LLMs' capabilities to repair programs using failed test feedback. 
Zhang et al. \cite{zhang2023self} allow LLMs to analyze test execution results to revise erroneous code.
Zhong et al. \cite{Zhong0S24ldb} extend this idea by enabling step-by-step debugging.
Huang et al. \cite{huang2023codecot} adopt a chain-of-thought prompting \cite{brown2020language} approach that generates code and tests sequentially, iteratively improving the code based on test feedback.
Zhang et al. \cite{zhang2024pair} introduce a pair programming framework where test feedback informs repair strategies and planning.

The generation of test cases differs across these methodologies.
For example, CodeT \cite{chen2022codet} and CodeCoT \cite{huang2023codecot} rely entirely on LLMs to generate test cases, 
whereas Reflexion employs LLMs primarily for simple programming tasks. 
Most other methods, however, assume pre-defined test cases as part of their evaluation frameworks, 
leveraging them for filtering, refinement, and program selection.



\subsection{Automated Test Case Generation}
Early works, such as~\cite{fraser2011evosuite}, leveraged search-based methods to generate test cases.
More recently, there has been a growing trend toward utilizing LLMs for test case generation.
Studies such as ~\cite{siddiq2024using}, ~\cite{guilherme2023initial}, and ~\cite{schafer2023empirical} have explored the effectiveness of LLMs in automating program testing.
Similarly, ~\cite{alshahwan2024automated} investigates methods for enhancing existing test cases using LLMs.
Other approaches, such as TOGA~\cite{dinella2022toga}, TOGLL~\cite{hossain2024togll}, and the method described in~\cite{hossain2023neural}, 
focus on generating test oracles based on predefined templates.
Meanwhile, ~\cite{takerngsaksiri2024tdd} introduces a model trained with deep reinforcement learning for text-to-test case generation.
The application of LLMs in software development extends to generating test cases for various scenarios.
For instance, ~\cite{zhang2023well} employs GPT-4 to generate security tests for identifying vulnerabilities, while ~\cite{schafer2023empirical} uses LLMs to assist in generating unit tests for automatic software testing.
Similarly, ~\cite{kang2023large} demonstrates the use of LLMs to create test cases aimed at reproducing general software bugs.
LLMs are also integrated into code generation studies to improve the accuracy of generated code through test case generation.
CodeT~\cite{chen2022codet}, for example, utilizes LLMs with zero-shot prompts to directly generate test cases. Reflexion~\cite{shinn2024reflexion} introduces a Test Agent powered by LLMs for test case generation.


\subsection{Evolution with LLMs for Code Generation}
The integration of LLMs with evolutionary algorithms has been extensively explored to enhance code generation.
For example, Bradley et al. \cite{bradley2024openelm} introduced OpenELM, an open-source Python library that designs specialized evolutionary operators for code generation.
Chen et al. \cite{chen2023evoprompting} utilized LLMs as adaptive mutation and crossover operators in evolutionary neural architecture search (NAS),
optimizing neural network design by merging LLM capabilities with evolutionary processes.
Liu et al. \cite{liu2023algorithm} proposed an automated approach for evolving optimization algorithms using LLMs, which eliminates the need for manual design by applying standard evolutionary operations such as initialization, selection, and mutation.
In a related study, Liu et al. \cite{liu2024evolution} developed a framework for evolving heuristics by integrating LLMs with evolutionary computation to co-evolve both natural language descriptions and executable code for heuristic design, further extending previous methodologies.

Ma et al. \cite{ma2023eureka} introduced an algorithm combining LLMs with evolutionary search to generate and optimize reward functions for reinforcement learning tasks, incorporating a reward reflection mechanism for refinement. 
Meyerson et al. \cite{meyerson2024language} demonstrated the use of few-shot prompting with LLMs to generate offspring in evolutionary algorithms, enabling a general crossover operator by concatenating parent solutions into a single prompt, which can adapt to various domains.
Similarly, Romera et al. \cite{romera2024mathematical} integrated LLMs with evolutionary algorithms to create context-aware program mutations, eliminating the need for manually defined mutation operators. 
Ye et al. \cite{ye2024reevo} proposed ReEvo, which combines evolutionary search with LLM-driven reflections to generate optimization heuristics, efficiently exploring heuristic spaces while offering verbal gradients for optimization. 
Lastly, AutoTest \cite{duan2024autotest} combines automated test case generation with code execution in an evolutionary framework, utilizing LLMs to co-generate code solutions and their corresponding test cases for enhanced optimization.

Additionally, various LLM-based co-evolution methods have been proposed. 
For instance,
Chi et al. \cite{chi2024reaccept} identifies outdated test cases by co-evolving production and test code.
Ruan et al. \cite{ruan2024evolutionary} constructs test oracles from bug reports using predefined templates.
Chi et al. \cite{chi2024reaccept} leveraged LLMs with dynamic validation to automate co-evolution between production and test code, identifying obsolete test cases to improve software quality and reduce maintenance overhead.
TestART \cite{gu2024testart} enhances unit testing for LLMs by combining automated test generation with repair, using template-based fixes and coverage feedback to improve test accuracy.