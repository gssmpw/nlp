\section{Generative Psycho-Lexical Approach for Constructing LLM Value System}
\label{sec:approach}

This section presents the Generative Psycho-Lexical Approach (\our{}), illustrated schematically in \cref{fig:test} and algorithmically in \cref{alg:approach}, as the foundation for constructing the LLM value system. \our{} adopts an agentic framework, comprising three LLM agents and four sequential steps, to systematically collect, filter, measure, and structure the LLM value system.

\begin{figure*}[t]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/GPLA-agents.pdf}
    \caption{LLM agents in \our{}: (1) Perception Parser $M_{P}$, (2) Value Generator $M_{G}$, and (3) Value Evaluator $M_{E}$.}
    \label{fig:sub1}
  \end{subfigure}
  
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/GPLA-diagram.pdf}
    \caption{The pipeline of \our{} for constructing LLM value system: 1) extracting perceptions from the corpus, 2) identifying values behind perceptions, 3) filtering values, 4) measuring value orientations of LLM test subjects, and 5) computing the structure of the value system.}
    \label{fig:sub2}
  \end{subfigure}
  
  \caption{Generative Psycho-lexical Approach (\our{}) for Constructing LLM Value System.}
  \label{fig:test}
\end{figure*}


\begin{algorithm*}
  \caption{Generative Psycho-lexical Approach for Constructing LLM Value System}
  \label{alg:approach}
  \begin{algorithmic}[1]
  \STATE \textbf{Input:} Corpus $\mathcal{C}$ of LLM-generated text, LLMs as value measurement subjects $M = \{M_i\}_{i=1}^{n}$, and three LLM agents $M_{P}$, $M_{G}$, $M_{E}$.
  
  \STATE \textbf{Output:} Value system $\mathcal{VS} = (V, V_H, \mathbf{\Lambda})$ of LLMs.
  
  \STATE $P = M_{P}(\mathcal{C})$ \COMMENT{Extract perceptions $P$ from corpus $\mathcal{C}$}
  \STATE $\{V, Freq\} = M_{G}(P)$ \COMMENT{Generate values $V$ behind perceptions $P$ and collect their frequencies}
  \STATE $V = \text{LexiconFiltering}(V, Freq)$ \COMMENT{Remove duplicate and less prioritized values}
  \STATE $\mathbf{X}_{M} = \text{GPV}(V, M; M_{P}, M_{E})$  \COMMENT{Measure the value orientations of LLM subjects}
  \STATE $V_H, \mathbf{\Lambda} = \text{PCA}(\mathbf{X}_{M})$ \COMMENT{Compute the hidden factors and structure of the system}
  \end{algorithmic}
\end{algorithm*}


\paragraph{Data Collection.} In line with the established psycho-lexical approach \cite{de2016values}, we begin by comprehensively identifying atomic values that LLMs may endorse. We draw upon a diverse, value-laden corpus of LLM outputs \(\mathcal{C}\) from ValueBench \cite{ren2024valuebench}, GPV \cite{ye2025gpv}, BeaverTails \cite{ji2024beavertails}, and ValueLex \cite{biedma2024beyond}. These corpora provide a broad spectrum of value-laden LLM generations. Detailed statistics and further explanations on these datasets are provided in \cref{app:data_statistics}.

\paragraph{Identifying Value Lexicons.} Then, we extract perceptions \( P \) from the collected corpus using the Perception Parser \( M_{P} \). Perceptions are atomic value-rich expressions that reflect values behind free-form LLM outputs and are akin to stimuli in traditional psychometric tools \cite{ye2025gpv}. These perceptions are then mapped to underlying values \( V \) through the Value Generator \( M_{G} \), with their frequency of occurrence \( Freq \) recorded. We instantiate \( M_{P} \) following GPV \cite{ye2025gpv} and \( M_{G} \) using Kaleido model \cite{sorensen2024value}. In \cref{app:comprehensiveness}, we confirm the comprehensive coverage of the generated values.

\paragraph{Value Filtering.} To ensure the value lexicons are concise and representative, we filter out duplicates using Rouge scores and embedding similarity following the validated setup in \cite{sorensen2024value}. When two values exhibit significant semantic overlap, we retain the one more frequently observed, as higher frequency signifies greater priority \cite{ponizovskiy2020development}. This process results in value lexicons \( V \) that minimize redundancy while preserving semantic coverage.

\paragraph{Value Measurement and System Construction.}
Structuring the LLM value system requires
identifying correlations between values derived from LLM value measurements.
To this end, we adopt GPV \cite{ye2025gpv}, instantiated with \( M_P \) and \( M_E \), to measure the value orientations \( \mathbf{X}_{M} \) of LLM subjects \( M \) on values \( V \). GPV dynamically extracts psychometric stimuli from LLM outputs and reveals value orientations therein. We measure a set of 693 LLM subjects, consisting of 33 LLMs paired with 21 profiling prompts (\cref{app:llm_subjects}). The involved LLMs are instruction-tuned models, as they are more relevant for real-world, public-facing applications.
The value system \( \mathcal{VS} = (V, V_H, \mathbf{\Lambda}) \) is then constructed through Principal Component Analysis (PCA) \cite{ponizovskiy2020development}, which uncovers the underlying value factors \( V_H \) and hierarchical relationships \( \mathbf{\Lambda} \) (loadings of \( V \)  on \( V_H \)).


\section{Validation and Advantages of \our{}}

\subsection{Are LLMs Capable for \our{}?}
LLMs assume three roles in \our{}: Perception Parser, Value Generator, and Value Evaluator. This section validates the capabilities of LLMs in these roles.

Perceptions are value-laden expressions dynamically extracted from LLM outputs. They resemble the stimuli in traditional psychometric tools \cite{ye2025gpv}. The Perception Parser \( M_{P} \) extracts such perceptions from the LLM outputs. As validated by \citet{ye2025gpv}, \( M_{P} \) can extract high-quality perceptions suitable for subsequent analysis. In addition, related research confirms that LLMs demonstrate abilities beyond human golden standards in value-related generation tasks \cite{ren2024valuebench, sorensen2024value, ziems2024can}.

The Value Generator \( M_{G} \) identifies values from the extracted perceptions. As validated by \citet{sorensen2024value}, \( M_{G} \), instantiated with the Kaleido model, can generate values that are both comprehensive and high-quality, with 91\% of the generations marked as good by all three annotators, and missing values detected 0.35\% of the time.

The Value Evaluator \( M_{E} \) measures the value orientations of LLMs given the extracted perceptions. When instantiated with ValueLlama \cite{ye2025gpv}, \( M_{E} \) can approximate the psychologists' judgments on held-out values with about 90\% accuracy.


\subsection{Advantages of \our{} over Prior Psycho-Lexical Approaches}

Here we discuss the main advantages of \our{} over the traditional psycho-lexical approach in constructing value systems. We defer the comparison with ValueLex \cite{biedma2024beyond}, a recent study that constructs LLM value systems, to \cref{app:against_bhn}.

\paragraph{Full Automation.} Traditional psycho-lexical approaches involve extensive manual labor in compiling and refining lexicons, collecting self-reports, and analyzing data. In contrast, given pre-collected corpora, \our{} automates the entire process, from value lexicon extraction and value filtering, to non-reactive value measurement and system construction.

\paragraph{Lexicon Collection.} The traditional approach relies on values lexicons compiled from dictionaries and thesauruses, which lack comprehensive coverage in diverse linguistic forms, criteria for prioritizing values, and adaptability to evolving values or specific contexts. In contrast, \our{} utilizes LLMs to dynamically extract perceptions and generate corresponding values, offering the following advantages.
\begin{itemize}
  \item \textbf{Comprehensive Coverage.} LLMs can generate values in diverse linguistic forms, beyond words in dictionaries and thesauruses, and thus provide more comprehensive coverage of values. Please refer to \cref{app:comprehensiveness} for the empirical evidence.

  \item \textbf{Prioritization Criteria.} The scalable collection of value lexicons by LLMs enables the concurrent collection of their occurrence frequencies. Since these frequencies reflect the salience of values \cite{ponizovskiy2020development}, they can serve as criteria for prioritizing values during the filtering process.

  \item \textbf{Adaptability.} Since LLMs are trained on Internet-scale data with evolving language patterns, they effectively encode up-to-date knowledge of values and are readily adaptable to changing values in changing corpora distributions.

  \item \textbf{Context Specificity.} \our{} constructs value systems given corpora from specific contexts, which can capture the unique values and value structures in those contexts. It enables the construction of LLM value systems.
\end{itemize}

\paragraph{Value Measurement.} Traditional methods of value measurement typically rely on self-reports or peer ratings, which are prone to response biases, resource-intensive, and often fail to capture authentic behaviors or account for historical and subjective data \cite{ponizovskiy2020development}. In contrast, \our{} utilizes non-reactive and LLM-driven value measurement, which is proven to be more scalable and reliable \cite{ye2025gpv}.


\section{Benchmarking LLM Value Systems}

The power of a value system lies in its validity and utility \cite{schwartz2012overview}. This work introduces, for the first time, three benchmarking tasks for LLM value systems, rooted in both psychological theories and AI priorities: (1) Confirmatory Factor Analysis, which evaluates their structure validity given LLM value measurements; (2) LLM Safety Prediction, which assesses their predictive validity for LLM safety; and (3) LLM Value Alignment, which examines their representation power in aligning LLMs with human values.


\subsection{Confirmatory Factor Analysis for Evaluating Structure Validity}

Confirmatory Factor Analysis (CFA) is a statistical technique used to test whether a set of observed variables (atomic values) accurately represents and loads onto a smaller number of underlying latent factors (high-level values) \cite{schwartz2004cfa}.
Mathematically, the CFA model is represented as:
\begin{equation}
    \mathbf{X} = \mathbf{F} \mathbf{\Lambda}^T + \mathbf{\epsilon},    
\end{equation}
where $\mathbf{X} \in \mathbb{R}^{n \times |V|}$ is the observed data matrix (measuring atomic values); $\mathbf{F} \in \mathbb{R}^{n \times |V_H|}$ is the matrix of latent factors (aggregated measurements of high-level value factors); $\mathbf{\Lambda} \in \mathbb{R}^{|V| \times |V_H|}$ is the matrix of factor loadings (the contribution of each atomic value to each latent factor); and $\mathbf{\epsilon} \in \mathbb{R}^{n \times |V|}$ is the matrix of error terms.

This task evaluates the model fit on held-out value measurement data. Better model fit indicates higher structure validity of the system.


\subsection{LLM Safety Prediction for Evaluating Predictive Validity}\label{sec:safety_prediction}

Predictive validity refers to how well measurement results based on a particular value system can anticipate future decisions, actions, or outcomes \cite{bardi2003values}. In the context of human values, evaluating predictive validity often involves examining the correlations between values and other psychological constructs, such as personality traits and attitudes. For LLMs, we propose to evaluate the predictive validity of a value system by examining the relationship between its values and LLM safety, arguably one of the most critical concerns and "psychological constructs" for LLM deployment.

To this end, we draw prompts and safety scores from the established safety benchmark SALAD-Bench \cite{li2024salad}. We administer the prompts to LLMs, collect their responses, and measure the revealed values using GPV \cite{ye2025gpv}. We follow the linear probing protocol \cite{chen2020simple} to evaluate the predictive validity of the value representations under different value systems. Using the Bradley-Terry model \cite{bradley1952rank}, we train a linear classifier with pairwise cross-entropy loss to predict the relative safety of LLM pairs:
\begin{equation}
  \begin{aligned}
    s_{M_i} &= \text{Linear}(\mathbf{x}_{M_i}),\\
    P(M_i \succ M_j) &= \frac{\exp(s_{M_i})}{\exp(s_{M_i}) + \exp(s_{M_j})}.
  \end{aligned}
\end{equation}
Here, $s_{M_i} \in \mathbb{R}$ is the predicted LLM safety score, and $\mathbf{x}_{M_i} \in \mathbb{R}^{|V_H|}$ is the vector of LLM values under a particular value system. In this case, we only care about the relative safety of LLM pairs, not absolute safety scores. Therefore, the predictive validity is indicated by the binary classification accuracy of a well-trained linear classifier.


\subsection{LLM Value Alignment for Evaluating Representation Power}\label{sec:value_alignment}

Values are cognitive representations of motivational goals \cite{schwartz2012overview}. A well-constructed value system should be able to fully represent the broad motivations behind LLM outputs, therefore enabling effective transituational alignment of LLMs with human values.

Our evaluation builds on the BaseAlign algorithm \cite{yao2023value_fulcra}, originally designed to align LLM outputs with Schwartz's basic human values. This approach employs a target human value vector, either heuristically customized or derived from human survey data. We extend BaseAlign to arbitrary value systems by distilling the target vector from the human preference:
\begin{equation}
  \begin{aligned}
  \mathbf{x}_{V}^{*} & = \arg\min_{\mathbf{x}} f(\mathbf{x}; \mathcal{D}, M_E, V) \\
  & = \arg\min_{\mathbf{x}} \sum_{(p, r_w, r_l) \in \mathcal{D}} \left[\left| \mathbf{x} - M_E(p, r_w, V)\right| - \left| \mathbf{x} - M_E(p, r_l, V ) \right| \right].
  \end{aligned}
\end{equation}
Here, $\mathbf{x}_{V}^{*} \in \mathbb{R}^{|V|}$ is the target value vector for values $V$ under evaluation. It is distilled from dataset $\mathcal{D}$ \cite{ouyang2022training} using open-vocabulary value evaluator $M_E$ \cite{ye2025gpv}. $\mathcal{D}$ contains triplets of prompt $p$, winning response $r_w$, and losing response $r_l$. 
The distillation objective is to minimize the L1 distance between the target value and the values of winning responses, while simultaneously maximizing the distance with those of losing responses. Given that the objective function is piecewise linear, the optimal solution can be derived by iteratively examining each value dimension \( V_i \) and the corresponding value measurements \( \mathcal{M}_{V, i} = \{M_E(p, r_w, V_i)\}_\mathcal{D} \cup \{M_E(p, r_l, V_i)\}_\mathcal{D} \):
\begin{equation}
  \mathbf{x}_{V, i}^{*} = \arg\min_{\mathbf{x}_{V, i} \in \mathcal{M}_{V, i}} f(\mathbf{x}_{V, i}; \mathcal{D}, M_E, V_i), 
  \forall i = 1, \ldots, |V|.
\end{equation}

Next, we adopt PPO \cite{schulman2017proximal} to align the LLM with the target value under \( V \) \cite{yao2023value_fulcra}:
\begin{equation}
  \min_{\theta} \mathbb{E}_{p \sim \mathcal{D}_p, r \sim \pi_{\theta}(p)}
  \left|   
  \mathbf{x}_{V}^{*} - M_E(p, r, V)
  \right|,
\end{equation}
where \( \theta \) is the LLM policy parameters, \( \mathcal{D}_p \) is the dataset of prompts \cite{dai2023safe}, and \( \pi_{\theta}(p) \) is the parameterized LLM response distribution.
The representation power of \( V \) is evaluated by the helpfulness and harmlessness of the aligned LLMs \cite{bai2022training, yao2023value_fulcra}.