\section{Case Study 1: Analysis of HCI Paper Abstracts}

In this case study, we utilized Schemex to analyze the abstracts of 20 CHI'24 Best Papers to evaluate its capability for deriving actionable schemas. 
For simplicity, we accepted AI-generated answers without edits and conducted only one iteration cycle.

The workflow began with DeepSeek-R1 performing clustering, which revealed four distinct categories: Empirical Studies (6 examples), Theoretical Contributions (4), System Design \& Evaluation (6), and Ethnographic Insights (4). 
Validation against authors' pre-tagged ACM CCS concepts showed 95\% alignment (19 out of 20 papers). 
The single outlier was a paper that blended the technical design of robotics with ethnographic observations of human-animal interactions, which did not fit neatly into any cluster.

To evaluate the utility of the output schema, we conducted a blinded comparison using two randomly selected examples (one from each cluster: Empirical Studies and System Design \& Evaluation). 
GPT-4 generated paired abstracts using both the initial Stage 2 schemas and the refined Stage 3 schemas, guided by the titles of the original papers. 
An HCI expert with over five years of publication experience evaluated these abstracts through randomized pairwise comparisons and provided qualitative feedback.

The results (see Figure~\ref{fig:study1}) show marked improvements in abstracts guided by the iterated schema.
For the empirical study cluster, the iterated schema highlights that Implication should avoid ambitious claims unsupported by the study findings.
The abstract created using the iterated schema (V2) anchors the implication (``anonymity-first community archiving'') directly to findings about participants repurposing mobile money alerts to bypass monitored platforms, creating a causal link between observed behaviors and design recommendations.
In contrast, the abstract generated with the initial schema (V1) proposes ``trauma-awareness training'' as a solution but fails to connect it to the reported findings about ``spatial avoidance strategies'' and ``paradoxical trust in platforms.'' 
This creates a logical gap between findings and implications.

For the system design and evaluation cluster, the iterated schema highlights the importance of integrating qualitative context with quantitative data for effective Evaluation sentence writing.
While V1 (with initial schema) reports isolated performance statistics (22\% faster completion, 31\% reduced cognitive load, 65\% fewer corrective queries), it lacks explanatory depth about user experience drivers.
V2 (with iterated schema) presents evaluation findings by balancing quantitative metrics (88\% preference rate) with qualitative insights explaining the behavioral shift and reasons for preference.




