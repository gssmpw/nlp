\section{Schemex Workflow}

The Schemex workflow (see Figure \ref{fig:teaser}) assists users in transforming a set of examples into actionable schemas through three AI-assisted stages.
For instance, to obtain schemas for writing HCI paper abstracts, the input can be 20 CHI’24 best paper abstracts.  

Initially, the workflow clusters the 20 abstracts into different groups, where each group has distinguishable features—for instance, separating empirical studies from system papers.  
Next, for each cluster, the workflow examines the examples and derives schemas specific to each cluster, identifying initial schemas like ``\textit{Motivation}-\textit{Problem}-\textit{Method}-\textit{Findings}-\textit{Implications}" for empirical studies and ``\textit{Motivation}-\textit{Problem}-\textit{System}-\textit{Evaluation}-\textit{Design Recommendations}'' for system papers.  
Finally, we refine the schema through contrastive learning: we take the initial schema and ask AI to write abstracts based on it, then ask AI to compare the generated abstracts with the original human-authored ones and iterate the schema.  

The resulting schemas are actionable and comprehensive, covering a diverse set of examples and providing guidance on how to write abstracts for different genres of HCI papers.
Users can utilize the resulting schemas as a guide to write their own abstracts. 


\subsection{Stage 1: Clustering}
Clustering identifies subclasses of examples and is crucial in preventing schema overgeneralization.  
When given a set of examples, it is often unclear if they share the same schema.  
Generalizing across structurally different examples (e.g., empirical study and system papers), can lead to weak and bland schemas.  
Therefore, we must perform clustering first to identify examples that are similar to each other and then find the schema specific to each cluster.

We instruct DeepSeek-R1 to cluster the examples and provide reasoning.
We choose DeepSeek-R1 for its advanced reasoning capabilities.
For our CHI abstract dataset, the model processes all 20 examples simultaneously, producing clusters and explaining the common features that examples within each cluster share.
Users can then easily verify the clustering results.
They can refine the clustering if they find any errors and can also merge clusters or request further division within a cluster if needed.

For instance, DeepSeek-R1 maps the 20 abstracts into 4 clusters:
\begin{itemize}[noitemsep]
    \item Empirical Studies (6 examples)
    \item Theoretical Contributions (4 examples)
    \item System Design and Evaluation (6 examples)
    \item Ethnographic Insights (4 examples)
\end{itemize}
It also provides reasoning to help users understand the differences between the clusters.
For example, the common features that empirical study papers share are their focus on the study method, findings, and implications.
While system papers do not have that focus: they usually concentrate on tool design, evaluation, and design recommendations instead.
Users are allowed to refine the clustering as they see fit.
For example, one might merge the ethnographic cluster with empirical studies, as they share many common features, with ethnographic study being a special case of empirical study.


\subsection{Stage 2: Abstraction}
Abstraction identifies structural commonalities among examples within each cluster.  
It synthesizes multiple examples and extracts generalizable insights, a task that can often be overwhelming for individuals due to the high cognitive load involved.

For each cluster, we instruct DeepSeek-R1 to analyze the examples and abstract the schema, which covers:
\begin{itemize}[noitemsep]
\item Components: Core elements (e.g., Method and Findings for empirical study papers).
\item Attributes: Content norms (e.g., Method should describe the empirical approach).
\item Relationships: Causal links (e.g., Findings → Implications: implications should be translated from findings).
\end{itemize}
This approach distills executable guidelines for the specific cluster, ensuring that the output not only captures the key structural dimensions (Components) but also details what makes each component effective (Attributes) and how to integrate these components into a cohesive output (Relationships).
Here is a schema that DeepSeek-R1 derives for examples within the empirical study cluster:

\begin{itemize}[noitemsep]
    \item Motivation: Introduce the research area, its relevance to HCI, and real-world significance.
        \begin{itemize}
            \item \textit{Motivation → Problem: Establishes why the research is important and what needs addressing.}
        \end{itemize}
    \item Problem: Identify what is missing in prior work or current practice.
        \begin{itemize}
            \item \textit{Problem → Method: The identified problem justifies the chosen method.}
        \end{itemize}
    \item Method: Describe the empirical approach, such as experiments or mixed methods.
        \begin{itemize}
            \item \textit{Method → Findings: The method directly leads to the results.}
        \end{itemize}
    \item Findings: Present key results, often with statistical evidence or qualitative insights.
        \begin{itemize}
            \item \textit{Findings → Implications: Results are translated into actionable insights for HCI.}
        \end{itemize}
    \item Implications: Discuss contributions to HCI research, design, policy, or practice.
\end{itemize}

Furthermore, DeepSeek-R1 is instructed to generate a table that illustrates how each example conforms to the schema. 
This enables users to assess the schema’s coherence by mapping it back to the examples. 
Users can then conveniently review the schema-example alignment and, if necessary, regenerate or modify the schema.

\begin{figure*}
\centering
\includegraphics[width=0.63\textwidth]
{figures/study1.jpg}
    \caption{Case study 1 - Comparison of HCI paper abstracts generated by AI with the initial and iterated schemas.}
    \label{fig:study1}
\end{figure*}


\subsection{Stage 3: Refinement via Contrasting Examples}
The schema derived from Stage 2 is not perfect. 
For instance, while it specifies that Method should describe the empirical approach, it is not clear how to write the descriptions. 
To make the schema more precise and actionable, we employ an apply-and-test approach, iterating on the schema using contrastive learning.

Specifically, we take the initial schema from Stage 2 and prompt GPT-4 to generate HCI paper abstracts based on that schema and the paper title.
We then utilize DeepSeek-R1 to compare these generated abstracts with original human-authored ones, iterating the schema accordingly. 
For example, after one iteration, DeepSeek-R1 provided a more detailed schema for writing Method, advising users to name the empirical approach, describe the sample and duration, and connect the method to the research questions. 
As another example, the iterated schema also captures additional insights for Findings, such as emphasizing unexpected results.


\begin{itemize}[noitemsep]
    \item Method
            \begin{itemize}[noitemsep]
                \item Approach: Name the empirical approach 
                \item Sample/Duration: Include sample size, duration, and tools 
                \item Design: Link the method to the research question 
            \end{itemize}
    \item Findings
            \begin{itemize}[noitemsep]
                \item Unexpected Results: Note surprises
                \item Quantitative: Report statistical significance
                \item Qualitative: Highlight themes
            \end{itemize}
\end{itemize}

The workflow provides a compare-and-contrast view of before-and-after examples, allowing users to easily identify differences and assess the quality of the generated content. 
Users are involved in the process by evaluating whether the generated content meets their standards. 
The cycle repeats until users are satisfied.


Ultimately, users can utilize the resulting schemas as a guide to write their own HCI paper abstracts.