@article{wang2024understanding,
  title={Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling},
  author={Wang, Mingze and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}
@article{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}
@article{everett2024scaling,
  title={Scaling exponents across parameterizations and optimizers},
  author={Everett, Katie and Xiao, Lechao and Wortsman, Mitchell and Alemi, Alexander A and Novak, Roman and Liu, Peter J and Gur, Izzeddin and Sohl-Dickstein, Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and others},
  journal={arXiv preprint arXiv:2407.05872},
  year={2024}
}


@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{lei2016layer,
  title={Layer normalization},
  author={Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={ArXiv e-prints},
  pages={arXiv--1607},
  year={2016}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@article{popel2018training,
  title={Training tips for the transformer model},
  author={Popel, Martin and Bojar, Ond{\v{r}}ej},
  journal={arXiv preprint arXiv:1804.00247},
  year={2018}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}
@misc{
shin2024initializing,
title={Initializing the Layer-wise Learning Rate},
author={Kwang Yong Shin and Suhyun Kim and Soo-Mook Moon},
year={2024},
url={https://openreview.net/forum?id=mSSi0zYkEA}
}
@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}
@article{yuan2024mars,
  title={MARS: Unleashing the Power of Variance Reduction for Training Large Models},
  author={Yuan, Huizhuo and Liu, Yifeng and Wu, Shuang and Zhou, Xun and Gu, Quanquan},
  journal={arXiv preprint arXiv:2411.10438},
  year={2024}
}
@misc{jordan2024muon,  
	title={Muon optimizer},
	author={Jordan Keller and others},
	howpublished={\url{https://github.com/KellerJordan/Muon?tab=readme-ov-file}}, 
	year={2024}
}
@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}
@article{roulet2024stepping,
  title={Stepping on the edge: Curvature aware learning rate tuners},
  author={Roulet, Vincent and Agarwala, Atish and Grill, Jean-Bastien and Swirszcz, Grzegorz and Blondel, Mathieu and Pedregosa, Fabian},
  journal={arXiv preprint arXiv:2407.06183},
  year={2024}
}
@article{song2023trajectory,
  title={Trajectory alignment: understanding the edge of stability phenomenon via bifurcation theory},
  author={Song, Minhak and Yun, Chulhee},
  journal={arXiv preprint arXiv:2307.04204},
  year={2023}
}
@inproceedings{chen2023beyond,
  title={Beyond the edge of stability via two-step gradient updates},
  author={Chen, Lei and Bruna, Joan},
  booktitle={International Conference on Machine Learning},
  pages={4330--4391},
  year={2023},
  organization={PMLR}
}
@article{zhu2022understanding,
  title={Understanding edge-of-stability training dynamics with a minimalist example},
  author={Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
  journal={arXiv preprint arXiv:2210.03294},
  year={2022}
}
@article{wang2022analyzing,
  title={Analyzing sharpness along GD trajectory: Progressive sharpening and edge of stability},
  author={Wang, Zixuan and Li, Zhouzi and Li, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9983--9994},
  year={2022}
}
@article{park2022vision,
  title={How do vision transformers work?},
  author={Park, Namuk and Kim, Songkuk},
  journal={arXiv preprint arXiv:2202.06709},
  year={2022}
}
@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={International Conference on Learning Representations},
  year={2022}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}
@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
@article{wang2024improving,
  title={Improving generalization and convergence by enhancing implicit regularization},
  author={Wang, Mingze and Wang, Jinbo and He, Haotian and Wang, Zilin and Huang, Guanhua and Xiong, Feiyu and Li, Zhiyu and Wu, Lei and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}
@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}
@article{cohen2024understanding,
  title={Understanding Optimization in Deep Learning with Central Flows},
  author={Cohen, Jeremy M and Damian, Alex and Talwalkar, Ameet and Kolter, Zico and Lee, Jason D},
  journal={arXiv preprint arXiv:2410.24206},
  year={2024}
}
@article{song2024does,
  title={Does SGD really happen in tiny subspaces?},
  author={Song, Minhak and Ahn, Kwangjun and Yun, Chulhee},
  journal={arXiv preprint arXiv:2405.16002},
  year={2024}
}
@article{wen2024understanding,
  title={Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective},
  author={Wen, Kaiyue and Li, Zhiyuan and Wang, Jason and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2410.05192},
  year={2024}
}

@article{cohen2022adaptive,
  title={Adaptive gradient methods at the edge of stability},
  author={Cohen, Jeremy M and Ghorbani, Behrooz and Krishnan, Shankar and Agarwal, Naman and Medapati, Sourabh and Badura, Michal and Suo, Daniel and Cardoze, David and Nado, Zachary and Dahl, George E and others},
  journal={arXiv preprint arXiv:2207.14484},
  year={2022}
}

@article{pan2023toward,
  title={Toward understanding why adam converges faster than sgd for transformers},
  author={Pan, Yan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2306.00204},
  year={2023}
}

@article{kunstner2024heavy,
  title={Heavy-tailed class imbalance and why adam outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{ormaniec2024does,
  title={What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis},
  author={Ormaniec, Weronika and Dangel, Felix and Singh, Sidak Pal},
  journal={arXiv preprint arXiv:2410.10986},
  year={2024}
}
@article{zhang2024transformers,
  title={Why transformers need adam: A hessian perspective},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}
@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}
@article{zhao2024deconstructing,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}
@article{damian2022self,
  title={Self-stabilization: The implicit bias of gradient descent at the edge of stability},
  author={Damian, Alex and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2209.15594},
  year={2022}
}
@inproceedings{ahn2022understanding,
  title={Understanding the unstable convergence of gradient descent},
  author={Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={247--257},
  year={2022},
  organization={PMLR}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch {SGD}: Training {ImageNet} in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}



@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}


@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{even2024s,
  title={{(S) GD} over Diagonal Linear Networks: Implicit bias, Large Stepsizes and Edge of Stability},
  author={Even, Mathieu and Pesme, Scott and Gunasekar, Suriya and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{pascanu2012understanding,
  title={Understanding the exploding gradient problem},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  journal={CoRR, abs/1211.5063},
  volume={2},
  number={417},
  pages={1},
  year={2012}
}
@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International conference on machine learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}
@article{merity2017regularizing,
  title={Regularizing and optimizing LSTM language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}

@article{shazeer2020glu,
  title={{GLU} variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@article{dai2019transformer,
  title={Transformer-{XL}: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}
@inproceedings{liu2023same,
  title={Same pre-training loss, better downstream: Implicit bias matters for language models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={22188--22214},
  year={2023},
  organization={PMLR}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the {Hessian}},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE international conference on big data (Big data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}
@article{gao2020pile,
  title={The {Pile}: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{kaddour2023minipile,
  title={The {MiniPile} challenge for data-efficient language models},
  author={Kaddour, Jean},
  journal={arXiv preprint arXiv:2304.08442},
  year={2023}
}
@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}
@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{loshchilov2016sgdr,
  title={{SGDR}: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}
@article{wang2023achieving,
  title={Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling},
  author={Wang, Mingze and Min, Zeping and Wu, Lei},
  journal={International Conference on Machine Learning},
  year={2024}
}
@article{mueller2024normalization,
  title={Normalization layers are all that sharpness-aware minimization needs},
  author={Mueller, Maximilian and Vlaar, Tiffany and Rolnick, David and Hein, Matthias},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{xie2022adaptive,
  title={Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum},
  author={Xie, Zeke and Wang, Xinrui and Zhang, Huishuai and Sato, Issei and Sugiyama, Masashi},
  booktitle={International conference on machine learning},
  pages={24430--24459},
  year={2022},
  organization={PMLR}
}
@article{xie2022adan,
  title={Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models},
  author={Xie, Xingyu and Zhou, Pan and Li, Huan and Lin, Zhouchen and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2208.06677},
  year={2022}
}
@article{heo2020adamp,
  title={Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights},
  author={Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Kim, Gyuwan and Uh, Youngjung and Ha, Jung-Woo},
  journal={arXiv preprint arXiv:2006.08217},
  year={2020}
}
@article{zhuang2020adabelief,
  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18795--18806},
  year={2020}
}
@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}
@article{du2021efficient,
  title={Efficient sharpness-aware minimization for improved training of neural networks},
  author={Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent YF},
  journal={arXiv preprint arXiv:2110.03141},
  year={2021}
}
@inproceedings{kwon2021asam,
  title={{ASAM}: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}
@article{nesterov1983method,
  title={A method of solving a convex programming problem with convergence rate $O(1/k^2)$},
  author={Nesterov, Yurii},
  journal={Doklady Akademii Nauk SSSR},
  volume={269},
  number={3},
  pages={543},
  year={1983}
}
@article{graves2013generating,
  title={Generating sequences with recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1308.0850},
  year={2013}
}
@article{zeiler2012adadelta,
  title={{ADADELTA}: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{chen2024symbolic,
  title={Symbolic discovery of optimization algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{zhu2023decentralized,
  title={Decentralized {SGD} and average-direction {SAM} are asymptotically equivalent},
  author={Zhu, Tongtian and He, Fengxiang and Chen, Kaixuan and Song, Mingli and Tao, Dacheng},
  booktitle={International Conference on Machine Learning},
  pages={43005--43036},
  year={2023},
  organization={PMLR}
}
@article{ujvary2022rethinking,
  title={Rethinking sharpness-aware minimization as variational inference},
  author={Ujv{\'a}ry, Szilvia and Telek, Zsigmond and Kerekes, Anna and M{\'e}sz{\'a}ros, Anna and Husz{\'a}r, Ferenc},
  journal={arXiv preprint arXiv:2210.10452},
  year={2022}
}

@inproceedings{liu2022towards,
  title={Towards efficient and scalable sharpness-aware minimization},
  author={Liu, Yong and Mai, Siqi and Chen, Xiangning and Hsieh, Cho-Jui and You, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12360--12370},
  year={2022}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}
@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}
@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with {K}ronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}
@inproceedings{grosse2016kronecker,
  title={A {K}ronecker-factored approximate {F}isher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016},
  organization={PMLR}
}
@article{george2018fast,
  title={Fast approximate natural gradient descent in a {K}ronecker-factored eigenbasis},
  author={George, Thomas and Laurent, C{\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{liu2023sophia,
  title={Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{mi2022make,
  title={Make sharpness-aware minimization stronger: A sparsified perturbation approach},
  author={Mi, Peng and Shen, Li and Ren, Tianhe and Zhou, Yiyi and Sun, Xiaoshuai and Ji, Rongrong and Tao, Dacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30950--30962},
  year={2022}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@article{damian2021label,
  title={Label noise {SGD} provably prefers flat global minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27449--27461},
  year={2021}
}

@inproceedings{arora2022understanding,
  title={Understanding gradient descent on the edge of stability in deep learning},
  author={Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  booktitle={International Conference on Machine Learning},
  pages={948--1024},
  year={2022},
  organization={PMLR}
}

@article{wang2023faster,
  title={Faster Margin Maximization Rates for Generic Optimization Methods},
  author={Wang, Guanghui and Hu, Zihao and Muthukumar, Vidya and Abernethy, Jacob},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}
@article{wang2022accelerated,
  title={On Accelerated Perceptrons and Beyond},
  author={Wang, Guanghui and Hanashiro, Rafael and Guha, Etash and Abernethy, Jacob},
  journal={arXiv preprint arXiv:2210.09371},
  year={2022}
}
@article{li2020reconciling,
  title={Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14544--14555},
  year={2020}
}


@article{andriushchenko2023we,
  title={Why Do We Need Weight Decay in Modern Deep Learning?},
  author={Andriushchenko, Maksym and D'Angelo, Francesco and Varre, Aditya and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2310.04415},
  year={2023}
}


@article{dai2023crucial,
  title={The Crucial Role of Normalization in Sharpness-Aware Minimization},
  author={Dai, Yan and Ahn, Kwangjun and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}


@inproceedings{wen2023how,
title={How Sharpness-Aware Minimization Minimizes Sharpness?},
author={Kaiyue Wen and Tengyu Ma and Zhiyuan Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}


@article{long2023sharpness,
  title={Sharpness-Aware Minimization and the Edge of Stability},
  author={Long, Philip M and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2309.12488},
  year={2023}
}

@inproceedings{jiang2019fantastic,
  title={Fantastic Generalization Measures and Where to Find Them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{wen2023sharpness,
  title={Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}
@article{wang2023noise,
  title={The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization},
  author={Wang, Mingze and Wu, Lei},
  journal={arXiv preprint arXiv:2310.00692},
  year={2023}
}
@article{wu2020implicit,
  title={Implicit regularization and convergence for weight normalization},
  author={Wu, Xiaoxia and Dobriban, Edgar and Ren, Tongzheng and Wu, Shanshan and Li, Zhiyuan and Gunasekar, Suriya and Ward, Rachel and Liu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2835--2847},
  year={2020}
}
@article{li2022implicit,
  title={Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent},
  author={Li, Zhiyuan and Wang, Tianhao and Lee, Jason D and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34626--34640},
  year={2022}
}
@article{vardi2022margin,
  title={On margin maximization in linear and {ReLU} networks},
  author={Vardi, Gal and Shamir, Ohad and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37024--37036},
  year={2022}
}
@article{wang2023understanding,
  title={Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of {ReLU} Networks},
  author={Wang, Mingze and Ma, Chao},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}
@article{lyu2022understanding,
  title={Understanding the generalization benefit of normalization layers: Sharpness reduction},
  author={Lyu, Kaifeng and Li, Zhiyuan and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34689--34708},
  year={2022}
}
@article{pesme2021implicit,
  title={Implicit bias of {SGD} for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29218--29230},
  year={2021}
}
@article{pesme2023saddle,
  title={Saddle-to-Saddle Dynamics in Diagonal Linear Networks},
  author={Pesme, Scott and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}
@article{even2023s,
  title={{(S)GD} over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability},
  author={Even, Mathieu and Pesme, Scott and Gunasekar, Suriya and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2302.08982},
  year={2023}
}
@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}
@Article{ma2022beyond,
author = {Ma, Chao and Kunin , Daniel and Wu, Lei and Ying, Lexing},
title = {Beyond the Quadratic Approximation: The Multiscale Structure of Neural Network Loss Landscapes},
journal = {Journal of Machine Learning},
year = {2022},
volume = {1},
number = {3},
pages = {247--267},
}
@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an {Ornstein-Uhlenbeck} like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@inproceedings{Jastrzebski2020The,
title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho and Krzysztof Geras},
booktitle={International Conference on Learning Representations},
year={2020}
}


@InProceedings{wu2023implicitstability,
  title =    {The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent},
  author =       {Wu, Lei and Su, Weijie J},
  booktitle =    {The 40th International Conference on Machine Learning},
  pages =    {37656--37684},
  year =   {2023},
  volume =   {202},
  series =   {Proceedings of Machine Learning Research},
  publisher =    {PMLR}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}

@article{vardi2023implicit,
  title={On the implicit bias in deep-learning algorithms},
  author={Vardi, Gal},
  journal={Communications of the ACM},
  volume={66},
  number={6},
  pages={86--93},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{mulayoff2021implicit,
  title={The implicit bias of minima stability: A view from function space},
  author={Mulayoff, Rotem and Michaeli, Tomer and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17749--17761},
  year={2021}
}

@article{ma2021linear,
  title={On linear stability of {SGD} and input-smoothness of neural networks},
  author={Ma, Chao and Ying, Lexing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16805--16817},
  year={2021}
}

@article{gatmiry2023inductive,
  title={The Inductive Bias of Flatness Regularization for Deep Matrix Factorization},
  author={Gatmiry, Khashayar and Li, Zhiyuan and Chuang, Ching-Yao and Reddi, Sashank and Ma, Tengyu and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:2306.13239},
  year={2023}
}

@article{li2021happens,
  title={What Happens after {SGD} Reaches Zero Loss?--A Mathematical Framework},
  author={Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  journal={International Conference on Learning Representations},
  year={2022}
}
@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}
@inproceedings{vgg,
	author = {Karen Simonyan and Andrew Zisserman},
	booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015},
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	year = {2015}
}

@misc{zhang2024tinyllama,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{krizhevsky2009learning,
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	publisher = {Citeseer},
	title = {Learning multiple layers of features from tiny images},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	year = {2009},
	Bdsk-Url-1 = {https://www.cs.toronto.edu/~kriz/cifar.html}}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={International Conference on Learning Representations},
  year={2021}
}
@article{hastie2004entire,
  title={The entire regularization path for the support vector machine},
  author={Hastie, Trevor and Rosset, Saharon and Tibshirani, Robert and Zhu, Ji},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Oct},
  pages={1391--1415},
  year={2004}
}

@inproceedings{keskar2016large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{wu2023implicit,
  title={Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability},
  author={Wu, Jingfeng and Braverman, Vladimir and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{boser1992training,
  title={A training algorithm for optimal margin classifiers},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}


@article{wu2022does,
  author = {Wu, Lei and Wang, Mingze and Su, Weijie J},
  journal = {Advances in Neural Information Processing Systems},
  pages = {4680--4693},
  title = {The alignment property of {SGD} noise and how it helps select flat minima: A stability analysis},
  volume = {35},
  year = {2022}}

@inproceedings{nacson2022implicit,
  title={Implicit bias of the step size in linear diagonal neural networks},
  author={Nacson, Mor Shpigel and Ravichandran, Kavya and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={16270--16295},
  year={2022},
  organization={PMLR}
}
@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@article{kunin2022asymmetric,
  title={The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks},
  author={Kunin, Daniel and Yamamura, Atsushi and Ma, Chao and Ganguli, Surya},
  journal={International Conference on Learning Representations},
  year={2023}
}
@inproceedings{nacson2019lexicographic,
  title={Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4683--4692},
  year={2019},
  organization={PMLR}
}
@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}


@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={International Conference on Learning Representations},
  year={2019}
}
@article{sun2022mirror,
  title={Mirror descent maximizes generalized margin and can be implemented efficiently},
  author={Sun, Haoyuan and Ahn, Kwangjun and Thrampoulidis, Christos and Azizan, Navid},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}
@article{wang2021momentum,
  title={Momentum Doesn't Change The Implicit Bias},
  author={Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming},
  journal={Advances in Neural Information Processing Systems},
  year={2022},
}
@inproceedings{nacson2019stochastic,
  title={Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate},
  author={Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3051--3059},
  year={2019},
  organization={PMLR}
}
@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}
@inproceedings{ji2020gradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={2109--2136},
  year={2020},
  organization={PMLR}
}
@inproceedings{ji2021fast,
  title={Fast margin maximization via dual acceleration},
  author={Ji, Ziwei and Srebro, Nathan and Telgarsky, Matus},
  booktitle={International Conference on Machine Learning},
  pages={4860--4869},
  year={2021},
  organization={PMLR}
}
@inproceedings{ji2021characterizing,
  title={Characterizing the implicit bias via a primal-dual analysis},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Algorithmic Learning Theory},
  pages={772--804},
  year={2021},
  organization={PMLR}
}
@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Conference on Learning Theory},
  year={2019}
}
@inproceedings{nacson2019convergence,
  title={Convergence of gradient descent on separable data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019},
  organization={PMLR}
}


@article{bahri2021sharpness,
  title={Sharpness-aware minimization improves language model generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={arXiv preprint arXiv:2110.08529},
  year={2021}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}
@book{filippov2013differential,
  title={Differential equations with discontinuous righthand sides: control systems},
  author={Filippov, Aleksei Fedorovich},
  volume={18},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{lyu2021gradient,
  title={Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}
@article{luo2019theory,
  title={Theory of the frequency principle for general deep neural networks},
  author={Luo, Tao and Ma, Zheng and Xu, Zhi-Qin John and Zhang, Yaoyu},
  journal={arXiv preprint arXiv:1906.09235},
  year={2019}
}
@article{li2020curse,
  title={On the curse of memory in recurrent neural networks: Approximation and optimization analysis},
  author={Li, Zhong and Han, Jiequn and Li, Qianxiao and others},
  journal={arXiv preprint arXiv:2009.07799},
  year={2020}
}
@article{li2022approximation,
  title={Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks},
  author={Li, Zhong and Han, Jiequn and Weinan, E and Li, Qianxiao},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={42},
  pages={1--85},
  year={2022},
  publisher={Microtome Publishing}
}
@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}
@article{chatterji2021doesA,
  title={When does gradient descent with logistic loss find interpolating two-layer networks?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={159},
  pages={1--48},
  year={2021}
}



@article{jastrzkebski2017three,
  title={Three factors influencing minima in {SGD}},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{wu2017towards,
  title={Towards understanding generalization of deep learning: Perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and E, Weinan},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@inproceedings{chatterji2021doesB,
  title={When does gradient descent with logistic loss interpolate using deep networks with smoothed {ReLU} activations?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter},
  booktitle={Conference on Learning Theory},
  pages={927--1027},
  year={2021},
  organization={PMLR}
}
@inproceedings{zhou2021local,
  title={A local convergence theory for mildly over-parameterized two-layer neural network},
  author={Zhou, Mo and Ge, Rong and Jin, Chi},
  booktitle={Conference on Learning Theory},
  pages={4577--4632},
  year={2021},
  organization={PMLR}
}
@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International conference on machine learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}
@inproceedings{safran2021effects,
  title={The effects of mild over-parameterization on the optimization landscape of shallow {ReLU} neural networks},
  author={Safran, Itay M and Yehudai, Gilad and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3889--3934},
  year={2021},
  organization={PMLR}
}
@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}
@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={International Conference on Learning Representations},
  year={2021}
}
@article{bassily2020stability,
  title={Stability of stochastic gradient descent on nonsmooth convex losses},
  author={Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal},
  journal={arXiv preprint arXiv:2006.06914},
  year={2020}
}
@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}
@article{10.1214/aos/1176344196,
author = {W. H. Rogers and T. J. Wagner},
title = {{A Finite Sample Distribution-Free Performance Bound for Local Discrimination Rules}},
volume = {6},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {506 -- 514},
keywords = {deleted estimate, discrimination, distribution-free bound, error rate estimate, finite sample bound, local inference, nearest neighbor rules},
year = {1978},
doi = {10.1214/aos/1176344196},
URL = {https://doi.org/10.1214/aos/1176344196}
}
}
@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter and Foster, Dylan J and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}@article{ma2020towards,
  title={Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't},
  author={Ma, Chao and Wojtowytsch, Stephan and Wu, Lei and others},
  journal={arXiv preprint arXiv:2009.10713},
  year={2020}
}@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}


@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms {I}: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1474--1520},
  year={2019},
  publisher={JMLR. org}
}@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, L{\'e}on and others},
  journal={Proceedings of Neuro-N{\i}mes},
  volume={91},
  number={8},
  pages={12},
  year={1991},
  publisher={Nimes}
}@article{wu2019global,
  title={Global convergence of gradient descent for deep linear residual networks},
  author={Wu, Lei and Wang, Qingcan and Ma, Chao},
  journal={arXiv preprint arXiv:1911.00645},
  year={2019}
}@article{asadi2018chaining,
  title={Chaining mutual information and tightening generalization bounds},
  author={Asadi, Amir R and Abbe, Emmanuel and Verd{\'u}, Sergio},
  journal={arXiv preprint arXiv:1806.03803},
  year={2018}
}@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}@article{sun2020global,
  title={The global landscape of neural networks: An overview},
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={95--108},
  year={2020},
  publisher={IEEE}
}@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}@inproceedings{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1674--1703},
  year={2017},
  organization={PMLR}
}@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}@inproceedings{zhang2017hitting,
  title={A hitting time analysis of stochastic gradient langevin dynamics},
  author={Zhang, Yuchen and Liang, Percy and Charikar, Moses},
  booktitle={Conference on Learning Theory},
  pages={1980--2022},
  year={2017},
  organization={PMLR}
}@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}@article{vapnik1994measuring,
  title={Measuring the {VC}-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={arXiv preprint arXiv:1706.08947},
  year={2017}
}@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}@inproceedings{steinke2020reasoning,
  title={Reasoning about generalization via conditional mutual information},
  author={Steinke, Thomas and Zakynthinou, Lydia},
  booktitle={Conference on Learning Theory},
  pages={3437--3452},
  year={2020},
  organization={PMLR}
}@article{haghifam2020sharpened,
  title={Sharpened generalization bounds based on conditional mutual information and an application to noisy, iterative algorithms},
  author={Haghifam, Mahdi and Negrea, Jeffrey and Khisti, Ashish and Roy, Daniel M and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2004.12983},
  year={2020}
}@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1705.07809},
  year={2017}
}@article{bu2020tightening,
  title={Tightening Mutual Information-Based Bounds on Generalization Error},
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={121--130},
  year={2020},
  publisher={IEEE}
}

@inproceedings{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{tu2019understanding,
  title={Understanding generalization in recurrent neural networks},
  author={Tu, Zhuozhuo and He, Fengxiang and Tao, Dacheng},
  booktitle={International Conference on Learning Representations},
  year={2019}
}@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}
@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}
@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  year={2019}
}
@article{weinan2020comparative,
  title={A comparative analysis of optimization and generalization properties oftwo-layer neural network and random feature models under gradient descent dynamics},
  author={Weinan, E and Chao, Ma and Lei, Wu},
  journal={Science China Mathematics},
  volume={63},
  number={7},
  pages={1235},
  year={2020},
  publisher={Science China Press}
}
@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}
@inproceedings{liang2019fisher,
  title={{Fisher-Rao} metric, geometry, and complexity of neural networks},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={888--896},
  year={2019},
  organization={PMLR}
}
@article{wu2018sgd,
  title={How {SGD} selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and E, Weinan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={8279--8288},
  year={2018}
}

@article{weinan2021barron,
  title={The {Barron} Space and the Flow-Induced Function Spaces for Neural Network Models},
  author={E, Weinan and Ma, Chao and Wu, Lei},
  journal={Constructive Approximation},
  pages={1--38},
  year={2021},
  publisher={Springer}
}@article{zhou2020towards,
  title={Towards theoretically understanding why {SGD} generalizes better than {Adam} in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven and others},
  journal={arXiv preprint arXiv:2010.05627},
  year={2020}
}@article{li2020complexity,
  title={Complexity measures for neural networks with general activation functions using path-based norms},
  author={Li, Zhong and Ma, Chao and Wu, Lei},
  journal={arXiv preprint arXiv:2009.06132},
  year={2020}
}@article{ma2019priori,
  title={A priori estimates of the population risk for residual networks},
  author={Ma, Chao and Wang, Qingcan and others},
  journal={arXiv preprint arXiv:1903.02154},
  year={2019}
}@article{ma2018priori,
  title={A priori estimates of the population risk for two-layer neural networks},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1810.06397},
  year={2018}
}@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={Journal of Machine Learning Research},
  volume={21},
  year={2020},
  publisher={MICROTOME PUBL}
  }
@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}
@article{ma2021sobolev,
  title={The {Sobolev} Regularization Effect of Stochastic Gradient Descent},
  author={Ma, Chao and Ying, Lexing},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{jentzen2021proof,
  title={A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with {ReLU} activation for piecewise linear target functions},
  author={Jentzen, Arnulf and Riekert, Adrian},
  journal={arXiv preprint arXiv:2108.04620},
  year={2021}
}
@article{mustafa2021fine,
  title={Fine-grained Generalization Analysis of Structured Output Prediction},
  author={Mustafa, Waleed and Lei, Yunwen and Ledent, Antoine and Kloft, Marius},
  journal={arXiv preprint arXiv:2106.00115},
  year={2021}
}
@article{li2018tighter,
  title={On tighter generalization bound for deep neural networks: Cnns, resnets, and beyond},
  author={Li, Xingguo and Lu, Junwei and Wang, Zhaoran and Haupt, Jarvis and Zhao, Tuo},
  journal={arXiv preprint arXiv:1806.05159},
  year={2018}
}
@article{ledent2019norm,
  title={Norm-based generalisation bounds for multi-class convolutional neural networks},
  author={Ledent, Antoine and Mustafa, Waleed and Lei, Yunwen and Kloft, Marius},
  journal={arXiv preprint arXiv:1905.12430},
  year={2019},
  publisher={Technical report}
}
@inproceedings{zhou2018understanding,
  title={Understanding generalization and optimization performance of deep CNNs},
  author={Zhou, Pan and Feng, Jiashi},
  booktitle={International Conference on Machine Learning},
  pages={5960--5969},
  year={2018},
  organization={PMLR}
}
@article{long2019generalization,
  title={Generalization bounds for deep convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}
@inproceedings{liang2019fisher,
  title={{Fisher-Rao} metric, geometry, and complexity of neural networks},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={888--896},
  year={2019},
  organization={PMLR}
}
@inproceedings{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018},
  organization={PMLR}
}
@inproceedings{mou2018generalization,
  title={Generalization bounds of {SGLD} for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  booktitle={Conference on Learning Theory},
  pages={605--638},
  year={2018},
  organization={PMLR}
}
@article{weinan2021barron,
  title={The Barron Space and the Flow-Induced Function Spaces for Neural Network Models},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={Constructive Approximation},
  pages={1--38},
  year={2021},
  publisher={Springer}
}
@article{asadi2020chaining,
  title={Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks.},
  author={Asadi, Amir R and Abbe, Emmanuel},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={139--1},
  year={2020}
}
@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1705.07809},
  year={2017}
}
@article{bu2020tightening,
  title={Tightening Mutual Information-Based Bounds on Generalization Error},
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={121--130},
  year={2020},
  publisher={IEEE}
}
@inproceedings{pensia2018generalization,
  title={Generalization error bounds for noisy, iterative algorithms},
  author={Pensia, Ankit and Jog, Varun and Loh, Po-Ling},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)},
  pages={546--550},
  year={2018},
  organization={IEEE}
}
@inproceedings{fang2019sharp,
  title={Sharp analysis for nonconvex {SGD} escaping from saddle points},
  author={Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle={Conference on Learning Theory},
  pages={1192--1234},
  year={2019},
  organization={PMLR}
}
@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}
@article{kingma2014adam,
  title={{Adam}: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{reddi2019convergence,
  title={On the convergence of {Adam} and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle pointsâ€”online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on learning theory},
  pages={797--842},
  year={2015},
  organization={PMLR}
}
@inproceedings{kleinberg2018alternative,
  title={An alternative view: When does {SGD} escape local minima?},
  author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={2698--2707},
  year={2018},
  organization={PMLR}
}
@article{ma2020qualitative,
  title={A qualitative study of the dynamic behavior of adaptive gradient algorithms},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:2009.06125},
  year={2020}
}
@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}
@article{li2020accelerated,
author = {Li, Huan and Fang, Cong and Lin, Zhouchen},
year = {2020},
month = {07},
pages = {1-16},
title = {Accelerated First-Order Optimization Algorithms for Machine Learning},
volume = {PP},
journal = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2020.3007634}
}
@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@article{cai2019gram,
  title={{Gram-Gauss-Newton} Method: Learning Overparameterized Neural Networks for Regression Problems},
  author={Cai, Tianle and Gao, Ruiqi and Hou, Jikai and Chen, Siyu and Wang, Dong and He, Di and Zhang, Zhihua and Wang, Liwei},
  journal={arXiv preprint arXiv:1905.11675},
  year={2019}
}
@article{zhang2019fast,
  title={Fast convergence of natural gradient descent for overparameterized neural networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}
@article{fang2018spider,
  title={{SPIDER}: Near-optimal non-convex optimization via stochastic path integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={arXiv preprint arXiv:1807.01695},
  year={2018}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={315--323},
  year={2013},
  publisher={Citeseer}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26},
  year={2012}
}


@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of {Adam} and {RMSProp}},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}
@article{barakat2019convergence,
  title={Convergence analysis of a momentum algorithm with adaptive step size for non convex optimization},
  author={Barakat, Anas and Bianchi, Pascal},
  journal={arXiv preprint arXiv:1911.07596},
  year={2019}
}
@inproceedings{staib2019escaping,
  title={Escaping saddle points with adaptive gradient methods},
  author={Staib, Matthew and Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={5956--5965},
  year={2019},
  organization={PMLR}
}
@article{zhou2020towards,
  title={Towards Theoretically Understanding Why {SGD} Generalizes Better Than ADAM in Deep Learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and HOI, Steven and others},
  journal={arXiv preprint arXiv:2010.05627},
  year={2020}
}
@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{ainslie-etal-2023-gqa,
    title = "{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    author = "Ainslie, Joshua  and
      Lee-Thorp, James  and
      de Jong, Michiel  and
      Zemlyanskiy, Yury  and
      Lebron, Federico  and
      Sanghai, Sumit",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.298/",
    doi = "10.18653/v1/2023.emnlp-main.298",
    pages = "4895--4901",
    abstract = "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5{\%} of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA."
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@misc{Karpathy2022,
  author = {Andrej Karpathy},
  title = {\text{NanoGPT}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
  commit = {325be85d9be8c81b436728a420e85796c57dba7e}
}