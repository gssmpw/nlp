@inproceedings{Jastrzebski2020The,
title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho and Krzysztof Geras},
booktitle={International Conference on Learning Representations},
year={2020}
}

@inproceedings{ahn2022understanding,
  title={Understanding the unstable convergence of gradient descent},
  author={Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={247--257},
  year={2022},
  organization={PMLR}
}

@inproceedings{arora2022understanding,
  title={Understanding gradient descent on the edge of stability in deep learning},
  author={Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  booktitle={International Conference on Machine Learning},
  pages={948--1024},
  year={2022},
  organization={PMLR}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an {Ornstein-Uhlenbeck} like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{chen2024symbolic,
  title={Symbolic discovery of optimization algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={International Conference on Learning Representations},
  year={2021}
}

@article{cohen2022adaptive,
  title={Adaptive gradient methods at the edge of stability},
  author={Cohen, Jeremy M and Ghorbani, Behrooz and Krishnan, Shankar and Agarwal, Naman and Medapati, Sourabh and Badura, Michal and Suo, Daniel and Cardoze, David and Nado, Zachary and Dahl, George E and others},
  journal={arXiv preprint arXiv:2207.14484},
  year={2022}
}

@article{cohen2024understanding,
  title={Understanding Optimization in Deep Learning with Central Flows},
  author={Cohen, Jeremy M and Damian, Alex and Talwalkar, Ameet and Kolter, Zico and Lee, Jason D},
  journal={arXiv preprint arXiv:2410.24206},
  year={2024}
}

@article{damian2022self,
  title={Self-stabilization: The implicit bias of gradient descent at the edge of stability},
  author={Damian, Alex and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2209.15594},
  year={2022}
}

@misc{jordan2024muon,  
	title={Muon optimizer},
	author={Jordan Keller and others},
	howpublished={\url{https://github.com/KellerJordan/Muon?tab=readme-ov-file}}, 
	year={2024}
}

@article{liu2023sophia,
  title={Sophia: A scalable stochastic second-order optimizer for language model pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{lyu2022understanding,
  title={Understanding the generalization benefit of normalization layers: Sharpness reduction},
  author={Lyu, Kaifeng and Li, Zhiyuan and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34689--34708},
  year={2022}
}

@Article{ma2022beyond,
author = {Ma, Chao and Kunin , Daniel and Wu, Lei and Ying, Lexing},
title = {Beyond the Quadratic Approximation: The Multiscale Structure of Neural Network Loss Landscapes},
journal = {Journal of Machine Learning},
year = {2022},
volume = {1},
number = {3},
pages = {247--267},
}

@article{ormaniec2024does,
  title={What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis},
  author={Ormaniec, Weronika and Dangel, Felix and Singh, Sidak Pal},
  journal={arXiv preprint arXiv:2410.10986},
  year={2024}
}

@article{park2022vision,
  title={How do vision transformers work?},
  author={Park, Namuk and Kim, Songkuk},
  journal={arXiv preprint arXiv:2202.06709},
  year={2022}
}

@article{song2023trajectory,
  title={Trajectory alignment: understanding the edge of stability phenomenon via bifurcation theory},
  author={Song, Minhak and Yun, Chulhee},
  journal={arXiv preprint arXiv:2307.04204},
  year={2023}
}

@article{song2024does,
  title={Does SGD really happen in tiny subspaces?},
  author={Song, Minhak and Ahn, Kwangjun and Yun, Chulhee},
  journal={arXiv preprint arXiv:2405.16002},
  year={2024}
}

@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}

@article{wang2024improving,
  title={Improving generalization and convergence by enhancing implicit regularization},
  author={Wang, Mingze and Wang, Jinbo and He, Haotian and Wang, Zilin and Huang, Guanhua and Xiong, Feiyu and Li, Zhiyu and Wu, Lei and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{wen2024understanding,
  title={Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective},
  author={Wen, Kaiyue and Li, Zhiyuan and Wang, Jason and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2410.05192},
  year={2024}
}

@article{wu2018sgd,
  title={How {SGD} selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and E, Weinan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={8279--8288},
  year={2018}
}

@article{xie2022adan,
  title={Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models},
  author={Xie, Xingyu and Zhou, Pan and Li, Huan and Lin, Zhouchen and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2208.06677},
  year={2022}
}

@article{yuan2024mars,
  title={MARS: Unleashing the Power of Variance Reduction for Training Large Models},
  author={Yuan, Huizhuo and Liu, Yifeng and Wu, Shuang and Zhou, Xun and Gu, Quanquan},
  journal={arXiv preprint arXiv:2411.10438},
  year={2024}
}

@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}

@article{zhang2024transformers,
  title={Why transformers need adam: A hessian perspective},
  author={Zhang, Yushun and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv preprint arXiv:2402.16788},
  year={2024}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

