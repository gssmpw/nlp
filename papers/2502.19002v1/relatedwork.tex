\section{Related Works}
\label{section: related works}

% \vspace{-.1cm}

% {\bf Hessian structure of transformers.}
% \citet{park2022vision} provide empirical evidence that the \SA in vision transformers leads to a more non-convex but smoother loss landscape than that of CNNs. 
% ~\citet{zhang2024transformers} empirically demonstrated that Hessian blocks in transformers exhibit much more  disparity than CNNs, suggesting that this could be a potential cause for the reliance on adaptive optimizers. However, their work does not provide a clear principle regarding the sharpness distinction across different transformer blocks.





{\bf Sharpness structures in transformers.} Recent work has started to investigate blockwise sharpness patterns in transformer models through Hessian-based analyses.
For example,~\citet{zhang2024transformers} empirically observed the sharpness' blockwise heterogeneity  but did not establish a clear principle regarding the sharpness disparity among different blocks. Meanwhile, \citet{ormaniec2024does} provided a Hessian analysis for a single self-attention (\SA) layer, focusing only on the  sharpness disparity between the query-key (\QK) and value-output (\VO) blocks within the same layer. 

% \vspace{-.05cm}

In contrast, we examine sharpness at the block-type level across the entire transformer architecture, rather than focusing on individual blocks (as in \citet{zhang2024transformers}) or a single layer (as in \citet{ormaniec2024does}). This coarse-grained perspective reveals a consistent  disparity, as formalized by \textbf{Principle}~\eqref{equ: main findings},  which persists throughout most of the training processâ€”except during the initial steps.

% \vspace{-.05cm}

{\bf Efficient optimizers for LLM pre-training.} AdamW (Adam with decoupled weight decay)~\citep{loshchilov2017decoupled} has become the default optimizer in LLM pre-training. 
Efforts to design more efficient optimizers generally fall into two main categories: accelerating convergence and reducing memory footprint.
Accelerations have been developed using techniques such as Nesterov momentum~\citep{xie2022adan}, diagonal second-order estimates~\citep{liu2023sophia,wang2024improving}, variance reduction~\citep{yuan2024mars}, and matrix-based preconditioners~\citep{jordan2024muon,vyas2024soap}. 
Memory-efficient optimizers utilize sign-based methods~\citep{chen2024symbolic}, reduced usage of second moments in Adam~\citep{zhang2024adam}, and gradient low-rank projection~\citep{zhao2024galore}.
The closest work to our Blockwise LR is~\citet{wang2024improving}, which also increases the LR along low-sharpness directions. A detailed comparison is deferred to Section~\ref{section: application}.

% \vspace{-.05cm}

{\bf The edge of stability (EoS) phenomenon.} Neural network training typically occurs at the EoS stage~\citep{wu2018sgd,Jastrzebski2020The,cohen2021gradient,cohen2022adaptive}, where the optimizer exhibits oscillatory behavior along sharp directions without diverging, while steadily progressing along flat directions, leading to loss reduction. Several works \citep{wen2024understanding,song2024does,cohen2024understanding,wang2024improving} have highlighted the crucial role of the dynamics along flat directions (referred to as river directions by \citet{wen2024understanding}, bulk directions by \citet{song2024does}, and stable direction in \citet{wang2024improving}) in reducing total loss. Notably, \citet{wen2024understanding} further demonstrated that this picture is essential for understanding LLM pre-training. Building on these insights, our Blockwise LR approach is designed to accelerate training by amplifying the dynamics particularly along the flat river directions.
% For theoretical explanations of this fascinating phenomenon, we refer to recent studies~\citep{ma2022beyond,ahn2022understanding,song2023trajectory,blanc2020implicit,arora2022understanding,lyu2022understanding,damian2022self,cohen2024understanding}. 




% Inspired by EoS, \citet{wang2024improving} proposed an optimizer that accelerates LLM pre-training by increasing the LRs along low-sharpness directions.  


% \vspace{-.2cm}