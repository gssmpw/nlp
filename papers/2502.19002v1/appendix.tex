
\vspace{1.cm}

\section{Experimental Details}
\label{sec:experimental_details}


{\bf Models.} We utilize two popular classes of LLM models for our pre-training experiments:
 \begin{itemize}[leftmargin=2em]
    \item {\bf GPT-2.} We use GPT-2 (small)  model~\citep{radford2019language}, implemented via the nanoGPT code base ~\citep{Karpathy2022}. 
    Following nanoGPT, the model employs Gaussian Error Linear Unit (GELU) activations and standard Layer Normalization (LayerNorm). Detailed model configurations are provided in Table~\ref{table: model config and max lrs}.
    
    \item {\bf LLaMA.} LLaMA~\citep{touvron2023LLaMA} is another popular decoder-only Transformer architecture, incorporating Rotary Positional Encoding (RoPE)~\citep{su2024roformer}, Swish-Gated Linear Unit (SwiGLU), and Root mean square layer normalization (RMSNorm). 
    We pre-train LLaMA models of sizes ranging from 0.13B to 1.1B parameters. 
    For implementation, for 0.13B, 0.25B, 0.5B, 0.75B models, we utilize the LLaMA code from HuggingFace Transformers Library~\citep{wolf-etal-2020-transformers}. For the 1.1B model configuration, we follow TinyLlama~\citep{zhang2024tinyllama}, which employs grouped-query attention~\citep{ainslie-etal-2023-gqa}. Additional model configurations are detailed in Table~\ref{table: model config and max lrs}.
\end{itemize}


{\bf Datasets.} Models are pre-trained on the following datasets:

\begin{itemize}[leftmargin=2em]
    \item {\bf OpenWebText}~\citep{Gokaslan2019OpenWeb}. It is an opensource recreation of the WebText corpus, is extensively utilized for LLM pre-training such as RoBERTa~\citep{liu2019roberta} and GPT-2. 
    \item {\bf MiniPile.}~\citep{kaddour2023minipile}. It is a 6GB subset of the deduplicated Pile (825GB)~\citep{gao2020pile} presents a highly diverse text corpus. Given its diversity, training on minipile poses challenges and potential instabilities.
\end{itemize}


All experiments are conducted on 4 A800/H800 80G GPUs.% (80G).



\subsection{Training Configurations for AdamW Baselines}
\label{appendix: experiments for baselines}

\begin{table}[!ht]
	% \vspace{-20pt}
		\centering
		%\vspace{-18pt}
        \renewcommand{\arraystretch}{1.25}
		\caption{\small Model configurations and optimally-tuned peak learning rates.}
		\label{table: model config and max lrs}
		\begin{small}
	 % \addtolength{\tabcolsep}{-3pt} 
		\begin{tabular}{l|c|c|c|c|c|c|c}
		\hline 
		Acronym & Size & $d_{\mathrm{model}}$ & $d_{\mathrm{FF}}$ & n$\_$head & depth &  \texttt{lr\_max} on OpenWebText & \texttt{lr\_max} on MiniPile \\
		\hline\hline 
        GPT-2 (small) & 124M & 768 & 3072 & 12 & 12 & 6e-4 & 6e-4 \\
	LLaMA (0.13B) & 134M &  768 & 3072 & 12 & 6 & -- & 1.2e-3 \\
	LLaMA (0.25B) & 237M & 1024 & 4096 & 16 & 8 & 8e-4 & 7.5e-4 \\
	LLaMA (0.5B) & 522M & 1280 & 5120 & 20 & 15 & 8e-4 & 4.5e-4 \\
	LLaMA (0.75B) & 743M & 1664 & 6656 & 26 & 13 & 6e-4 & -- \\
	LLaMA (1.1B) & 1175M & 2048 & 5632 & 32 & 22 & 4e-4 & -- \\ 
	 \hline 
		\end{tabular}
		\end{small}
		% \vspace{-5pt}
\end{table}

As a baseline optimizer, we use the default AdamW for LLM pre-training, configured with the hyperparameters $\beta_1=0.9,\beta_2=0.95$ and weight decay $\lambda=0.1$. To ensure training stability, gradient clipping is applied by norm with threshold $1.0$. These settings align with the training protocols used in nanoGPT and LLaMA models ~\citep{touvron2023LLaMA}.
The LR strategy integrates a linear warm-up
phase, followed by a cosine decay scheduler with the peak learning rate \texttt{lr\_max} and the final learning rate \texttt{lr\_min}=\texttt{lr\_max}$/20$. Additionally,

\begin{itemize}[leftmargin=2em]
    \item {\bf OpenWebText pre-training.} The (max) sequence length is set to 1024, and the batch size is set to 480, following nanoGPT and~\citet{liu2023sophia}. The total training duration is 50,000 or 100,000 steps, including 1,000 warm-up steps.
    The grid search for \texttt{lr\_max} is performed over $\{$\texttt{2e-4}, \texttt{4e-4}, \texttt{6e-4}, \texttt{8e-4}, \texttt{1e-3}$\}$. Optimal learning rates for each model are detailed in Table~\ref{table: model config and max lrs}.

    \item {\bf MiniPile pre-training.} The (max) sequence length is set to 512, and the batch size is set to 300, following~\citet{wang2024improving}. The total training duration is 30,000 or 60,000 steps, including 600 warm-up steps. The grid search for \texttt{lr\_max} is performed over $\{$\texttt{3e-4}, \texttt{4.5e-4}, \texttt{6e-4}, \texttt{7.5e-4}, \texttt{9e-4}, \texttt{1.2e-3}, \texttt{1.5e-3}$\}$. Optimal learning rates for each model are detailed in Table~\ref{table: model config and max lrs}.
\end{itemize}

{\bf Baselines}: models are pre-trained using AdamW with the respective tuned \texttt{lr\_max} for each dataset and model configuration.



{\bf Related Experiments.}
\begin{itemize}[leftmargin=2em]
    \item {\bf Blockwise LR Experiments.} The baseline results in {\bf Figure~\ref{fig: main results: blockwise lr}}, {\bf Figure~\ref{fig: introduction} (right)}, and {\bf Table~\ref{tab: ablation studies} (the w/o line)} are trained following the configurations above.
    
    \item {\bf Sharpness Principle Experiments.} Models for {\bf Figure~\ref{fig: introduction} (left)}, {\bf Figure~\ref{fig: law final}}, {\bf Figure~\ref{fig: law gpt process}}, are trained using the baseline configurations for GPT-2 (small) or LLaMA (0.25B) on OpenWebText, with a total training duration 50,000 steps.
    In these experiments, the sharpness is estimated using $\bh(\btheta)$ in Eq.~\eqref{equ: fisher estimate}, with $B$ set to 1024. The sharpness distributions and average sharpness values for different blocks ($\bullet$) are calculated on a logarithmic scale, i.e., $\log \bh(\btheta[\bullet])$.

    Additionally, the experiment in Figure~\ref{fig: layerwise no law} employs the same model and sharpness estimator.
    
    \item {\bf Theoretical Analysis Support.} To support our theoretical insights in Section~\ref{section: theoretical insights}, {\bf Figure~\ref{fig: blockwise norm}} shows the evaluation of the parameter norms across different blocks during training. The model used is LlaMa (0.25B), trained on OpenWebText. The model is LLaMA (0.25B), trained on OpenWebText following the baseline configurations.
\end{itemize}



\begin{figure}[!ht]
    \centering
    \subfloat[
    (To illustrate Theorem~\ref{thm: FFN vs Norm}) Norms of input/output weight parameters in $\FFN$ and the weight parameters of $\Norm$ before $\FFN$, averaged by the number of layers.]{\includegraphics[width=0.25\linewidth]{figures/norm_ffn_ln.pdf}}
    \hspace{.4cm}
    \subfloat[To illustrate Theorem~\ref{thm: QK VO vs Norm}) Norms of query/key/value/output parameters in $\SA$ and the weight parameters of \Norm\ before $\SA$, averaged by the number of layers.]{\includegraphics[width=0.25\linewidth]{figures/norm_qkvo_ln.pdf}}
    \hspace{.4cm}
    \subfloat[(To illustrate Theorem~\ref{thm: Embed vs Norm}) Norms of weight parameters in $\Embed$ and the weight parameters in the adjoint \Norm\ layer after $\Embed$.]
    {\includegraphics[width=0.25\linewidth]{figures/norm_embed_ln.pdf}}
    \caption{Evolution of parameter norms across different blocks during pre-training LLaMA (0.25B) on OpenWebText.}
    \label{fig: blockwise norm}
\end{figure}




\begin{figure}[!ht]
    \centering
    \subfloat[Average sharpness across different layers. Layer $0$ corresponds to the \Embed\ layer. Layers $1,\cdots,8$ correspond to the \SA-\FFN\ layers.]{\includegraphics[width=0.3\linewidth]{figures/sharpness_layer_mean.pdf}}
    \hspace{.4cm}
    \subfloat[Average sharpness of the blocks ($\bullet\in\{\QK,\FFN,\VO,\Norm\}$) across different layers ($l=1,\cdots,8$).]
    {\includegraphics[width=0.3\linewidth]{figures/sharpness_layer_block.pdf}}
    \caption{In a pre-trained LLaMA (0.25B) (with $L=8$ layer), there is no clear disparity for the average sharpness across the {\bf layers}. This is in stark contrast to our our sharpness disparity {\bf Principle}~\eqref{equ: main findings} across the {\bf blocks}.}
    \label{fig: layerwise no law}
\end{figure}




% \subsection{Training configuration For Section~\ref{section: principle} and Section~\ref{section: application}}

% Unless stated otherwise, we follow each modelâ€™s standard configuration. Our model configuration for Llama is shown in Table~\ref{table3}, where 1.1B setting follows TinyLlama\cite{zhang2024tinyllama}, which utilizes grouped-query attention\cite{ainslie-etal-2023-gqa}. Our model configuration for GPT-2 follows GPT-2\cite{radford2019language}.

% \paragraph{OpenWebText pre-training.} We use the nanoGPT codebase to pre-train a GPT-2(117M parameters) and Llama models (0.25B, 0.5B, 0.75B, and 1.1B parameters) on the OpenWebText dataset. The key hyperparameters for all models are as follows: \texttt{seq\_len} $ = $ 1024, batch size = 480, weight decay coefficient $\lambda = 0.1$, $\epsilon =$ 1e-8, $\beta_1 = 0.9, \beta_2 =0.95$.  We use cosine-decay learning rate schedule with 1000 iterations of warm-up and total 50000 iterations. We perform grid search for learning rate in [2e-4, 4e-4, 6e-4, 8e-4, 10e-4]. The chosen peak learning rates are 6e-4, 8e-4, 8e-4, 6e-4, 4e-4 for GPT2, Llama-0.25B, Llama-0.5B, Llama-0.75B and Llama-1.1B respectively. The minimal learning rate is chosen as 3e-5, 4e-5, 4e-5, 3e-5, 2e-5 for these models. 

% \paragraph{MiniPile pre-training.}  We also use the nanoGPT codebase to train GPT-2 (117M parameters) and Llama (0.13B, 0.25B, and 0.5B parameters) models on the MiniPile dataset. The key hyperparameters for all models are: \texttt{seq\_len} $ = $ 512, batch size = 300, weight decay coefficient $\lambda = 0.1$, $\epsilon =$ 1e-8, $\beta_1 = 0.9, \beta_2 =0.95$.  We use cosine-decay learning rate schedule with 600 iterations of warm-up and total 30000 iterations. 
% We perform grid search for learning rate in [3e-4, 4.5e-4, 6e-4, 7.5e-4, 9e-4, 1.2e-3, 1.5e-3].
% The chosen peak learning rates are 6e-4, 1.2e-3, 7.5e-4, 4.5e-4 for GPT2, Llama-0.13B, Llama-0.25B and Llama-0.5B respectively. The minimal learning rate is chosen as 3e-5, 6e-5, 3.75e-5, 2.25e-5 for these models.



\subsection{Experimental Details for Blockwise LR on AdamW}
\label{appendix: experiments, blockwise LR}


{\bf Switching Time.} The principle of blockwise sharpness heterogeneity emerges clearly after the initial training phase, as shown in Figure~\ref{fig: law gpt process}. To leverage this principle, in our experiments of AdamW using Blockwise LR, we {\bf switch} from standard AdamW to AdamW with Blockwise LR {\bf at the end of LR warmup phase}. 
% This approach ensures that the use of Blockwise LR can aligns well with the sharpness principle.


{\bf Experiments in Figure~\ref{fig: main results: blockwise lr}.} We adopt the adjusting ratios~\eqref{equ: tuned hyperparameters, blockwise lr} as the default adjusting ratios for {\bf} all experiments of AdamW with Blockwise LR.


{\bf Experiment on Hyper-parameter Tuning.} We {\bf only} tune the four adjusting ratios $r(\bullet)$ ($\bullet\in\{\Embed,\QK,\VO,\FFN\}$) in a single small-scale experiment: pre-training LLaMA (0.25B) on Minipile. Specifically, we compare the results under the following configurations of ratios:
\begin{gather*}
r(\Embed)=6, r(\QK)=4, r(\FFN)=3, r(\VO)=2;\\
r(\Embed)=8, r(\QK)=6, r(\FFN)=4, r(\VO)=3;\\
r(\Embed)=10, r(\QK)=8, r(\FFN)=6, r(\VO)=4.
\end{gather*}
The results for the tuning experiments are presented in {\bf Figure~\ref{fig: robust llama web}}. One can see that the configuration $r(\Embed)=10, r(\QK)=8, r(\FFN)=6, r(\VO)=4$ (Eq.~\eqref{equ: tuned hyperparameters, blockwise lr}) achieves the largest improvement in terminal loss.
Additionally, Blockwise LR demonstrates robustness to these ratios, consistently accelerating pre-training across all tested configurations.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.3\linewidth]{figures/robust_web_0_25B.pdf}
    \caption{Pre-training LLaMA (0.25B) on Minipile using AdamW with Blockwise LR across three configurations of adjusting ratios.}
    \label{fig: robust llama web}
\end{figure}


{\bf Experiments in Table~\ref{tab: ablation studies}.} We pre-train LLaMA (0.25B) on
OpenWebText with a focusing on the three comparisons: (i) applying Blockwise LR exclusively to \Embed; (ii) applying Blockwise LR to both \Embed\ and \FFN; (iii) applying Blockwise LR to blocks of all the four types (\Embed, \FFN, \QK, and \VO). The adjusting ratios are maintained as per the tuned in Eq.~\eqref{equ: tuned hyperparameters, blockwise lr}.




\subsection{Experimental details for Adam-mini}
\label{appendix: experiments for adam-mini}


{\bf Adam-mini Baseline.} In the baseline experiments in Figure~\ref{fig: blockwise lr on adam-mini}, following~\citet{zhang2024adam}, we adopt the same peak learning rate \texttt{lr\_max} tuned for AdamW as the \texttt{lr\_max} of Adam-mini.


{\bf Hyperparameter tuning.} Since Adam-mini uses SGD within each blocks, its dynamics {\bf differs significantly} from those of AdamW. Thus, for Adam-mini with Blockwise LR, we re-tune the ratios $r(\bullet)\in\{1,2,4\}$ for $\bullet\in\{\Embed,\QK,\FFN,\VO\}$. The tuned ratios are $r(\Embed)=4, r(\QK)=1, r(\FFN)=4, r(\VO)=4$, which are used in the experiments in {\bf Figure~\ref{fig: blockwise lr on adam-mini}}.
Note that these ratios do not satisfy $r(\bullet)\propto\frac{\cS(\Norm)}{\cS(\bullet)}$. This discrepancy may stem from the unique dynamics of Adam-mini, particularly its SGD-like behavior within blocks. We leave further investigation for future work.

















\vspace{1.cm}

\section{Proofs in Section~\ref{section: principle}}
\label{appendix: proof: section principle}

\subsection{Proof of Theorem~\ref{thm: FFN vs Norm}}
\label{appendix: proof: thm: FFN vs Norm}

We focus on the transformation from $\bX^{(l-1)}$ to $\bX^{(l-1/2)}$:
$$\bX^{(l)}=\bX^{(l-1/2)}+\FFN^{(l)}\left(\LN^{l}\left(\bX^{(l-1/2)};\bgamma^{(l)}\right);\bW_1^{(l)},\bW_2^{(l)}\right).$$
From the chain rule, it follows that:
\begin{align*}
    \frac{\partial \cQ}{\partial \bW_{\bullet}^{(l)}}&=\frac{\partial \cQ}{\partial \bX^{(l)}}\frac{\partial \bX^{(l)}}{\partial \bW_{\bullet}^{(l)}},\quad \bullet\in\{1,2\};
    \\
    \frac{\partial \cQ}{\partial \bgamma^{(l)}}&=\frac{\partial \cQ}{\partial \bX^{(l)}}\frac{\partial \bX^{(l)}}{\partial\bgamma^{(l)}}.
\end{align*}


Thus, it suffices to compute $\frac{\partial \bX^{(l)}}{\partial\bW_{\bullet}^{(l)}}$ and $\frac{\partial \bX^{(l)}}{\partial\bgamma^{(l)}}$.
For simplicity, we define:
\begin{gather*}
    \bX:=\bX^{(l-1/2)},\quad
    \bX_{\rm std}=\frac{\bX-\bbE_r[\bX]}{\sqrt{\bbV_r[\bX]}},\quad
    \bX_\LN:=\LN(\bX;\bgamma)=\bX_{\rm std}\odot(\bone_{n\times 1}\otimes\bgamma),
    \\
    \MSA:=\bX_{\LN}\bW_1,\quad
    \ASA:=\sigma(\MSA),\quad
    \FFFN:=\ASA\bW_2,
    \quad
    \bY:=\bX^{(l)}=\bX+\FFFN,
\end{gather*}

where $\sigma(\cdot)$ represents the ReLU or Leacky ReLU activation function.
We now compute $\frac{\partial \bY}{\partial\bW_{\bullet}}$ and $\frac{\partial \bY}{\partial\bgamma}$.

It is straightforward that:
\begin{align*}
    \frac{\partial \bY}{\partial\bW_{1}}=&\frac{\partial\FFFN}{\partial\bW_{1}}=\frac{\partial\FFFN}{\partial\ASA}\frac{\partial\ASA}{\partial\MSA}\frac{\partial\MSA}{\partial\bW_{1}}=\left(\bI_{n}\otimes\bW_2^\top\right)\frac{\partial\ASA}{\partial\MSA}\left(\bX_{\LN}\otimes\bI_M\right);
\end{align*}
\begin{align*}
    \frac{\partial \bY}{\partial\bgamma}=&\frac{\partial\FFFN}{\partial\bX_{\LN}}\frac{\partial\bX_{\LN}}{\partial\bgamma}=\frac{\partial\FFFN}{\partial\ASA}\frac{\partial\ASA}{\partial\MSA}\frac{\partial\MSA}{\partial\bX_{\LN}}\frac{\partial\bX_{\LN}}{\partial\bgamma}
    \\=&\left(\bI_{n}\otimes\bW_2^\top\right)\frac{\partial\ASA}{\partial\MSA}\left(\bI_n\otimes\bW_1^\top\right)\Big(\diag\big(\vec(\bX_{\rm std})\big)\big(\bone_{n\times 1}\otimes \bI_{D}\big)\Big).
\end{align*}


For the (Leaky) ReLU, it holds that $\sigma(z)=z\sigma'(z)$. Thus, for $\frac{\partial \bY}{\partial\bW_2}$, we have: 
\begin{align*}
    \frac{\partial \bY}{\partial\bW_{2}}=&\frac{\partial\FFFN}{\partial\bW_{2}}=\ASA\otimes\bI_D=\left(\bX_{\LN}\bW_1\odot\frac{\partial\ASA}{\partial\MSA}\right)\otimes\bI_D.
\end{align*}

Now we derive the upper bounds. First, notice that:
\begin{align*}
    \norm{\bX_{\rm std}}_\rF=\left(\sum_{i=1}^n \left(\frac{\bX_{i,:}-\bbE[\bX_{i,:}]}{\sqrt{\bbV[\bX_{i,:}]}}\right)^2\right)^{1/2}=\left(\sum_{i=1}^n D\right)^{1/2}=\sqrt{nD};
\end{align*}
\begin{align*}
    &\norm{\bX_\LN}_\rF
    =\norm{\bX_{\rm std}\odot(\bone_{n\times 1}\otimes\bgamma)}_\rF
    \leq\norm{\bX_{\rm std}}_\rF\norm{\bone_{n\times 1}\otimes\bgamma}_\rF\leq\sqrt{nD}\norm{\bone_{n\times 1}}_\rF\norm{\bgamma}_\rF\leq n\sqrt{D}\norm{\bgamma}_\rF.
\end{align*}

Consequently, we have the following estimates:
\begin{align*}
    &\norm{\frac{\partial \cQ}{\partial\bW_{1}}}_\rF
    \leq\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial \bY}{\partial\bW_{1}}}_\rF
    =\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\left(\bI_{n}\otimes\bW_2^\top\right)\frac{\partial\ASA}{\partial\MSA}\left(\bX_{\LN}\otimes\bI_M\right)}_\rF\\
    \leq&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bI_{n}\otimes\bW_2^\top}_2\norm{\bX_{\LN}\otimes\bI_M}_2
    \leq\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bI_{n}}_2\norm{\bI_M}_2\norm{\bW_2^\top}_\rF\norm{\bX_{\LN}}_\rF
    \\\leq&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_2}_\rF\norm{\bX_{\LN}}_\rF\leq n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_2}_\rF\norm{\bgamma}_\rF;
\end{align*}
\begin{align*}
    &\norm{\frac{\partial \cQ}{\partial\bW_{2}}}_\rF
    \leq\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial \bY}{\partial\bW_{2}}}_\rF
    =\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\left(\bX_{\LN}\bW_1\odot\frac{\partial\ASA}{\partial\MSA}\right)\otimes\bI_D}_\rF\\
    \leq&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\left(\bX_{\LN}\bW_1\odot\frac{\partial\ASA}{\partial\MSA}\right)}_\rF\norm{\bI_D}_2
    \leq\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bX_{\LN}\bW_1}_\rF
    \\\leq&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_1}_\rF\norm{\bX_{\LN}}_\rF\leq n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_1}_\rF\norm{\bgamma}_\rF;
\end{align*}
\begin{align*}
    &\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF
    \leq\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial \bY}{\partial\bgamma}}_\rF
    \\=&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\left(\bI_{n}\otimes\bW_2^\top\right)\frac{\partial\ASA}{\partial\MSA}\left(\bI_n\otimes\bW_1^\top\right)\Big(\diag\big(\vec(\bX_{\rm std})\big)\big(\bone_{n\times 1}\otimes \bI_{D}\big)\Big)}_\rF
    \\\leq&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bI_{n}\otimes\bW_2^\top}_\rF\norm{\bI_{n}\otimes\bW_1^\top}_\rF\norm{\diag\big(\vec(\bX_{\rm std})\big)\big(\bone_{n\times 1}\otimes \bI_{D}\big)}_\rF
    \\\leq&
    \norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bI_{n}}_2\norm{\bW_2}_\rF\norm{\bI_{n}}_2\norm{\bW_1}_\rF\norm{\diag\big(\vec(\bX_{\rm std})\big)}_\rF\norm{\bone_{n\times 1}\otimes \bI_{D}}_\rF
    \\\leq&
    \norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_1}_\rF\norm{\bW_2}_\rF\norm{\bX_{\rm std}}_\rF\norm{\bone_{n\times 1}}_\rF\norm{\bI_{D}}_2
    \\\leq& n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_1}_\rF\norm{\bW_2}_\rF.
\end{align*}

Thus, if we define
$$\Psi:=n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_1}_\rF\norm{\bW_2}_\rF\norm{\bgamma}_\rF,$$

then it holds that:
\begin{gather*}
\norm{\frac{\partial \cQ}{\partial\bW_1}}_\rF\leq\frac{\Psi}{\norm{\bW_1}_{\rF}};\quad
\norm{\frac{\partial \cQ}{\partial\bW_2}}_\rF\leq\frac{\Psi}{\norm{\bW_2}_{\rF}};\quad
\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF\leq\frac{\Psi}{\norm{\bgamma}_{\rF}}
\end{gather*}

Therefore,
\begin{gather*}
    \cS(\bW_{\bullet})=\frac{1}{\#(\bW_{\bullet})}\norm{\frac{\partial \cQ}{\partial\bW_{\bullet}}}_\rF^2=\cO\left(\frac{\Psi^2}{D^2\norm{\bW_\bullet}_\rF^2}\right),\quad\bullet\in\{1,2\};
    \\
    \cS(\bgamma)=\frac{1}{\#(\bgamma)}\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF^2=\cO\left(\frac{\Psi^2}{D\norm{\bgamma}_\rF^2}\right).
\end{gather*}


\subsection{Proof of Theorem~\ref{thm: QK VO vs Norm}}
\label{appendix: proof: thm: QK VO vs Norm}



We focus on the transformation from $\bX^{(l-1)}$ to $\bX^{(l-1/2)}$: 
$$\bX^{(l-1/2)}=\bX^{(l-1)}+\SA^{(l)}\Big(\LN^{(l-1/2)}\left(\bX^{(l-1)};\bgamma^{(l-1/2)}\right);\bW_K^{(l)},\bW_Q^{(l)},\bW_V^{(l)},\bW_O^{(l)}\Big).$$

From the chain rule, it follows that:
\begin{align*}
    \frac{\partial \cQ}{\partial \bW_{\bullet}^{(l)}}&=\frac{\partial \cQ}{\partial \bX^{(l-1/2)}}\frac{\partial \bX^{(l-1/2)}}{\partial \bW_{\bullet}^{(l)}},\quad \bullet\in\{K,Q,V,O\};
    \\
    \frac{\partial \cQ}{\partial \bgamma^{(l-1/2)}}&=\frac{\partial \cQ}{\partial \bX^{(l-1/2)}}\frac{\partial \bX^{(l-1/2)}}{\partial\bgamma^{(l-1/2)}}.
\end{align*}


Thus, it suffices to compute $\frac{\partial \bX^{(l-1/2)}}{\partial\bW_{\bullet}^{(l-1/2)}}$ and $\frac{\partial \bX^{(l-1/2)}}{\partial\bgamma^{(l-1/2)}}$. For simplicity, we define:
\begin{gather*}
    \bX:=\bX^{(l-1)},\quad
    \bX_{\rm std}=\frac{\bX-\bbE_r[\bX]}{\sqrt{\bbV_r[\bX]}},\quad
    \bX_\LN:=\LN(\bX;\bgamma)=\bX_{\rm std}\odot\bgamma,
    \\
    \MSA:=\frac{\bX_{\LN}\bW_Q\bW_K^\top\bX_{\LN}^\top}{\sqrt{D}},\quad
    \ASA:=\sm\left(\MSA\right),\quad
    \SSA:=\ASA\bX_{\LN}\bW_V\bW_O,
    \\
    \bY:=\bX^{(l-1/2)}=\bX+\SSA.
\end{gather*}
 
We now compute $\frac{\partial \bY}{\partial\bW_{\bullet}}$ and $\frac{\partial \bY}{\partial\bgamma}$:
\begin{align*}
    \frac{\partial \bY}{\partial\bW_{Q}}=&\frac{\partial\SSA}{\partial\bW_{Q}}=\frac{\partial\SSA}{\partial\ASA}\frac{\partial\ASA}{\partial\MSA}\frac{\partial\MSA}{\partial\bW_{Q}}
    =\bracket{\bI_n\otimes \bW_O^\top\bW_V^\top\bX_{\LN}^\top}\frac{\partial\ASA}{\partial\MSA}\bracket{\frac{\bX_{\LN}\otimes \bX_{\LN} \bW_K}{\sqrt{D}}};
\end{align*}
\begin{align*}
    \frac{\partial \bY}{\partial\bW_{K}}=\frac{\partial\SSA}{\partial\bW_{K}}=\frac{\partial\SSA}{\partial\ASA}\frac{\partial\ASA}{\partial\MSA}\frac{\partial\MSA}{\partial\bW_{K}}
    =\bracket{\bI_n\otimes \bW_O^\top\bW_V^\top\bX_{\LN}^\top}\frac{\partial\ASA}{\partial\MSA}\bracket{\frac{\bX_{\LN}\otimes \bX_{\LN} \bW_Q}{\sqrt{D}}};
\end{align*}
\begin{align*}
    \frac{\partial \bY}{\partial\bW_{V}}=\frac{\partial\SSA}{\partial\bW_{V}}=\ASA\bX_{\LN}\otimes\bW_O^\top;
\end{align*}
\begin{align*}
    \frac{\partial \bY}{\partial\bW_{O}}=\frac{\partial\SSA}{\partial\bW_{O}}=\ASA\bX_{\LN}\bW_V\otimes\bI_D.
\end{align*}

Moreover,
\begin{align*}
    &\frac{\partial \bY}{\partial\bgamma}=\frac{\partial \bY}{\partial\bX_{\LN}}\frac{\partial \bX_{\LN}}{\partial\bgamma}
    =\frac{\partial\SSA}{\partial\bX_{\LN}}\frac{\partial \bX_{\LN}}{\partial\bgamma}
    \\=&\Bigg(\frac{1}{\sqrt{D}}\Big(\bI_n\otimes\bW_O^\top\bW_V^\top\bX_{\LN}^\top\Big)\frac{\partial\ASA}{\partial\MSA}\left(\Big(\bI_n\otimes\bX_{\LN}\bW_K\bW_Q^\top\right)+\bK_{n,n} \left(\bI_n\otimes\bX_{\LN}\bW_Q\bW_K^\top\right)\Big)
    \\&\quad+\ASA\otimes\bW_O^\top\bW_V^\top\Bigg)\Big(\diag\big(\vec(\bX_{\rm std})\big)\big(\bone_{n\times 1}\otimes \bI_{d}\big)\Big),
\end{align*}
where $\bK_{n,n}$ is the commutation matrix\footnote{The commutation matrix $\bK_{m,n}$ transforms column-wise vectorization into row-wise vectorization.}.


Recalling the proof in Appendix~\ref{appendix: proof: thm: FFN vs Norm}, we have:
\begin{align*}
    \norm{\bX_{\rm std}}_\rF=\sqrt{nD},
    \quad\norm{\bX_\LN}_\rF\leq n\sqrt{D}\norm{\bgamma}_\rF.
\end{align*}


Then, similar to the proof in Appendix~\ref{appendix: proof: thm: FFN vs Norm}, we have the following upper bounds:
\begin{align*}
    \norm{\frac{\partial \cQ}{\partial\bW_{Q}}}_\rF\leq&
    \frac{1}{\sqrt{D}}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF \norm{\bW_K}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bX_{\LN}}_\rF^3
    \\\leq&
    \frac{(n\sqrt{D})^3}{\sqrt{D}}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF \norm{\bW_K}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bgamma}_\rF^3;
\end{align*}
\begin{align*}
    \norm{\frac{\partial \cQ}{\partial\bW_{K}}}_\rF\leq&
    \frac{1}{\sqrt{D}}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF \norm{\bW_Q}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bX_{\LN}}_\rF^3
    \\\leq&
    \frac{(n\sqrt{D})^3}{\sqrt{D}}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF \norm{\bW_Q}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bgamma}_\rF^3;
\end{align*}
\begin{align*}
    \norm{\frac{\partial \cQ}{\partial\bW_{V}}}_\rF\leq\norm{\frac{\partial \cQ}{\partial\bY}}_\rF
    \norm{\ASA}_\rF\norm{\bW_O}_\rF\norm{\bX_{\LN}}_\rF
    \leq n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF
    \norm{\ASA}_\rF\norm{\bW_O}_\rF\norm{\bgamma}_\rF;
\end{align*}
\begin{align*}
    \norm{\frac{\partial \cQ}{\partial\bW_{O}}}_\rF\leq
    \norm{\frac{\partial \cQ}{\partial\bY}}_\rF
    \norm{\ASA}_\rF\norm{\bW_V}_\rF\norm{\bX_{\LN}}_\rF
    \leq n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF
    \norm{\ASA}_\rF\norm{\bW_V}_\rF\norm{\bgamma}_\rF;
\end{align*}
\begin{align*}
    \norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF
    \leq&\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\sqrt{n}\norm{\bX_{\rm std}}_\rF\left(\frac{2}{\sqrt{D}}\norm{\Big(\bI_n\otimes\bW_O^\top\bW_V^\top\bX_{\LN}^\top\Big)\frac{\partial\ASA}{\partial\MSA}\left(\bI_n\otimes\bX_{\LN}\bW_K\bW_Q^\top\right)}_\rF+\norm{\ASA\otimes\bW_O^\top\bW_V^\top}_\rF\right)
    \\\leq& n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\left(\frac{2}{\sqrt{D}}
    \norm{\frac{\partial\ASA}{\partial\MSA}}_\rF \norm{\bW_K}_\rF\norm{\bW_Q}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bX_{\LN}}_\rF^2+\norm{\ASA}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF
    \right)
    \\\leq&
    \norm{\frac{\partial \cQ}{\partial\bY}}_\rF\left(\frac{2(n\sqrt{D})^3}{\sqrt{D}}\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_K}_\rF\norm{\bW_Q}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bgamma}_\rF^2 + n\sqrt{D}\norm{\ASA}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\right).
\end{align*}

Therefore, if we define:
\begin{align*}
    \Phi:=&\frac{(n\sqrt{D})^3}{\sqrt{D}}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\frac{\partial\ASA}{\partial\MSA}}_\rF\norm{\bW_K}_\rF\norm{\bW_Q}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bgamma}_\rF^3,\\
    \Psi:=&n\sqrt{D}\norm{\frac{\partial \cQ}{\partial\bY}}_\rF\norm{\ASA}_\rF\norm{\bW_V}_\rF\norm{\bW_O}_\rF\norm{\bgamma}_\rF,
\end{align*}
then it holds that:
\begin{gather*}
    \norm{\frac{\partial \cQ}{\partial\bW_{K}}}_\rF\leq\frac{\Phi}{\norm{\bW_K}_\rF};\quad\quad
    \norm{\frac{\partial \cQ}{\partial\bW_{Q}}}_\rF\leq\frac{\Phi}{\norm{\bW_Q}_\rF};
    \\
    \norm{\frac{\partial \cQ}{\partial\bW_{V}}}_\rF\leq\frac{\Psi}{\norm{\bW_V}_\rF};\quad\quad
    \norm{\frac{\partial \cQ}{\partial\bW_{O}}}_\rF\leq\frac{\Psi}{\norm{\bW_O}_\rF};
    \\
    \norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF\leq\frac{2\Phi + \Psi}{\norm{\bgamma}_\rF}.
\end{gather*}


Therefore,
\begin{gather*}
    \cS(\bW_{\bullet})=\frac{1}{\#(\bW_{\bullet})}\norm{\frac{\partial \cQ}{\partial\bW_{\bullet}}}_\rF^2=\cO\left(\frac{\Phi^2}{D^2\norm{\bW_\bullet}_\rF^2}\right),\quad\bullet\in\{K,Q\};
    \\
    \cS(\bW_{\bullet})=\frac{1}{\#(\bW_{\bullet})}\norm{\frac{\partial \cQ}{\partial\bW_{\bullet}}}_\rF^2=\cO\left(\frac{\Psi^2}{D^2\norm{\bW_\bullet}_\rF^2}\right),\quad\bullet\in\{V,O\};
    \\
    \cS(\bgamma)=\frac{1}{\#(\bgamma)}\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF^2=\cO\left(\frac{\Phi^2+\Psi^2}{D\norm{\bgamma}_\rF^2}\right).
\end{gather*}




\subsection{Proof of Theorem~\ref{thm: Embed vs Norm}}
\label{appendix: proof: thm: Embed vs Norm}

We focus on the transformation from $\bX$ to $\bY:=\LN(\bX\bW_E;\bgamma^{(1/2)})$.
For simplicity, we define:
\begin{align*}
    \bZ:=\bX\bW_E,\quad
    \bZ_{\rm std}:=\frac{\bZ-\bbE_r[\bZ]}{\sqrt{\bbZ_r[\bZ]}},\quad
    \bY=\LN(\bZ;\bgamma)=\bZ_{\rm std}\odot(\bone_{n\times 1}\otimes\bgamma).
\end{align*}

It is straightforward that:
\begin{align*}
    \frac{\partial\bY}{\partial\bgamma}=\diag\big(\vec(\bZ_{\rm std})\big)\big(\bone_{n\times 1}\otimes \bI_{D}\big).
\end{align*}
Recalling the proof in Appendix~\ref{appendix: proof: thm: FFN vs Norm}, we have:
\begin{align*}
    \norm{\frac{\partial\bY}{\partial\bgamma}}_\rF
    \leq n\sqrt{D}.
\end{align*}

Then we calculate $\frac{\partial\bY}{\partial\bW_E}$. For simplicity, we denote 
\begin{gather*}
    \tilde{\bZ}:=\bZ-\bbE_{r}[\bZ],\quad \bZ=\begin{pmatrix}
    \tilde{\bz}_1 \\ ... \\ \tilde{\bz}_d
\end{pmatrix}\in\bbR^{d\times D},\\
\tilde{\bW}_E:=\bW_E-\bbE_{r}[\bW_E],\quad
\bW_E=\begin{pmatrix}
    \bw_{E_1} \\ ... \\ \bw_{E_d}
\end{pmatrix}\in\bbR^{d\times D},\quad
\tilde{\bW}_E=\begin{pmatrix}
    \tilde{\bw}_{E_1} \\ ... \\ \tilde{\bw}_{E_d}
\end{pmatrix}\in\bbR^{d\times D}
\end{gather*} 

By the proof in~\cite{xiong2020layer}, for a vector $\bx\in\bbR^{1\times D}$, denoted by $\tilde{\bx}:=\bx-\bbE[\bx]$, then $\frac{\partial \bx_{\rm std}}{\partial \bx}=\frac{\sqrt{D}}{\norm{\tilde{\bx}}_2}\left(\bI-\frac{\tilde{\bx}^\top\tilde{\bx}}{\norm{\tilde{\bx}}_2^2}\right)\left(\bI-\frac{1}{d}\bone_{1\times D}^\top\bone_{1\times D}\right)$. Thus, we have:
\begin{align*}
    &\frac{\partial\bY}{\partial\bW_E}=\frac{\partial\bY
    }{\partial \bZ_{\rm std}}\frac{\partial \bZ_{\rm std}}{\partial\bZ}\frac{\partial \bZ}{\partial\bW_E}
    \\=&\left(\bI_n\otimes\diag\left(\vec(\bgamma)\right)\right)\diag\left(\left\{\frac{\sqrt{D}}{\norm{\tilde{\bz}_{i}}_2}\left(\bI-\frac{\tilde{\bz}_{i}^\top\tilde{\bz}_{i}}{\norm{\tilde{\bz}_{i}}_2^2}\right)\left(\bI-\frac{1}{D}\bone_{1\times D}^\top\bone_{1\times D}\right)\right\}_{i\in[n]}\right)
    \left(\bX\otimes\bI_D\right).
\end{align*}

Recalling the relationship $z_{i,j}=\sum_{k=1}^d x_{i,k} w_{k,j}$, we have $\bbE[\bz_i]=\sum_{k=1}^d x_{i,k}\bbE[\bw_k]$, which implies
\begin{align*}
    \tilde{\bz}_i=\sum_{k=1}^d x_{i,k} \tilde{\bw}_k.
\end{align*}

Combining this property with the that are one-hot fact of the inputs $\bX$, we have:
\begin{align*}
    \min_{i\in[n]}\norm{\tilde{\bz}_{i}}_2\geq \min_{k\in[d]}\norm{\tilde{\bw}_{k}}_2.
\end{align*}

Additionally, the one-hot encoding ensures:
\begin{align*}
    \norm{\bX}_\rF=\left(\sum_{i=1}^n x_{i,j}^2\right)^{1/2}=\sqrt{n}.
\end{align*}

Now we have the following bound:
\begin{align*}
    &\norm{\frac{\partial\bY}{\partial\bW_E}}_\rF
    \\\leq&\norm{\bI_n\otimes\diag\left(\vec(\bgamma)\right)}_\rF\norm{\diag\left(\left\{\frac{\sqrt{D}}{\norm{\tilde{\bz}_{i}}_2}\left(\bI-\frac{\tilde{\bz}_{i}^\top\tilde{\bz}_{i}}{\norm{\tilde{\bz}_{i}}_2^2}\right)\left(\bI-\frac{1}{D}\bone_{1\times D}^\top\bone_{1\times D}\right)\right\}_{i\in[n]}\right)}_2\norm{\bX\otimes\bI_D}_2
    \\\leq& \sqrt{n}\norm{\bgamma}_\rF\frac{\sqrt{D}}{\min_{i\in[n]}\norm{\tilde{\bz}_i}_2}\norm{\bX}_2\leq n\sqrt{D}\frac{\norm{\bgamma}_\rF}{\min_{i\in[n]}\norm{\tilde{\bz}_i}_2}
    \leq n\sqrt{D}\frac{\norm{\bgamma}_\rF}{\min_{i\in[d]}\norm{\tilde{\bw}_i}_2}.
\end{align*}

If we choose $\Psi:=n\sqrt{D}\norm{\bgamma}_\rF $, then we have:
\begin{align*}
    \norm{\frac{\partial\bY}{\partial\bgamma}}_\rF\leq\frac{\Psi}{\norm{\bgamma}_\rF}
    ,\quad
    \norm{\frac{\partial\bY}{\partial\bW_E}}_\rF\leq\frac{\Psi}{\min_{i\in[d]}\norm{\tilde{\bw}_i}_2}.
\end{align*}


Therefore,
\begin{gather*}
    \cS(\bW_E)=\frac{1}{\#(\bW_E)}\norm{\frac{\partial \cQ}{\partial\bW_E}}_\rF^2=\cO\left(\frac{\Psi^2}{Dd\min_{i\in[d]}\norm{\tilde{\bw}_i}_2^2}\right);
    \\
    \cS(\bgamma)=\frac{1}{\#(\bgamma)}\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF^2=\cO\left(\frac{\Psi^2}{D\norm{\bgamma}_\rF^2}\right).
\end{gather*}




% \vspace{1.cm}

% \section{Proofs in Section~\ref{section: application}}