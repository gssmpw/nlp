\documentclass[dvipsnames]{article}


% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024
% \usepackage{natbib}

\input{math_commands.tex}

% ready for submission
% \usepackage{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[preprint]{mingze}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{titletoc}
\usepackage[pagebackref]{hyperref}   % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{natbib}
\setcitestyle{numbers, compress}
\usepackage{lipsum}
\usepackage{titletoc}
\usepackage{minitoc}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{pifont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[ruled,vlined]{algorithm2e}
\newcommand{\red}{\color{red}}
\newcommand{\green}{\color{green}}
\newcommand{\orange}{\color{orange}}
%%%%%%%%%%%%%%%%%%%%%%
\usepackage[pagebackref]{hyperref}  
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
\hypersetup{colorlinks=true,linkcolor=red!70!black,linktocpage=false,citebordercolor=blue!70!black,citecolor=blue!70!black,anchorcolor=blue!70!black}

\usepackage{graphicx,subfig}
\usepackage{etoc}
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{graphicx,subfig}
\usepackage{url}
\usepackage{multirow}

\usepackage{enumitem}
\usepackage{stfloats}
\usepackage{makecell}
\usepackage{tcolorbox}
\usepackage{framed}
\colorlet{shadecolor}{orange!15}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{setting}[theorem]{Setting}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{main result}{Main Theorem}
\allowdisplaybreaks[4]
\renewcommand \thepart{}
\renewcommand \partname{}
%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{enumitem}
\definecolor{thistle}{rgb}{0.85, 0.75, 0.85}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{wrapfig}
\usepackage{subfig}
% \renewcommand{\thefootnote}{}

% \usepackage[textsize=tiny]{todonotes}
% \newcommand{\wmz}[1]{\todo[color=orange!20]{\footnotesize Mingze: #1}}
% \newcommand{\wl}[1]{\todo[color=red!20]{\footnotesize LW: #1}}
% \newcommand{\wzl}[1]{\todo[color=orange!20]{\footnotesize Zilin: #1}}
% \newcommand{\hht}[1]{\todo[color=orange!20]{\footnotesize Haotian: #1}}


% \title{Improving Generalization and Optimization in Deep Learning: an Algorithm Framework by Implicit Regularization Enhancement}
% \title{ IRE: Implicit Regularization Enhancement for Better Neural Network Training}
\title{The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jinbo Wang$^{1,*}$
  \quad
  Mingze Wang$^{1,*}$
  \quad
  Zhanpeng Zhou$^{3,*}$
  \\
  {\bf Junchi Yan}$^{3}$
  \quad
  {\bf Weinan E}$^{1,2,4}$
  \quad
  {\bf Lei Wu}$^{1,2}$
  \vspace{.3cm}
  \\
  $^1$School of Mathematical Sciences, Peking University
  \\
  $^2$Center for Machine Learning Research, Peking University
  \\
  $^3$ Sch. of Computer Science \& Sch. of Artificial Intelligence, Shanghai Jiao Tong University
  \\
  \quad
  $^4$AI for Science Institute, Beijing
  \vspace{.3cm}
  \\
  {\small\texttt{$\{$mingzewang, wangjinbo$\}$@stu.pku.edu.cn}}
  \\
  {\small\texttt{$\{$zzp1012, yanjunchi$\}$@sjtu.edu.cn}}
  \\
  {\small\texttt{$\{$weinan, leiwu$\}$@math.pku.edu.cn}}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

% \hypersetup{draft}
\begin{document}


\maketitle

\begin{abstract}
%Transformers have become the cornerstone of modern AI. 
Transformers consist of diverse building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feedforward networks. Thus, understanding the differences and interactions among these blocks is important.
In this paper, we uncover a clear {\bf sharpness disparity} across these blocks, which emerges early in training and intriguingly persists throughout the training process. Motivated by this finding, we propose {\bf Blockwise Learning Rate (LR)}, a strategy that tailors the LR to each block’s sharpness, accelerating large language model (LLM) pre-training.
By integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration  across GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and datasets of OpenWebText and MiniPile.
Finally, we incorporate Blockwise LR into Adam-mini~\citep{zhang2024adam}, a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory saving. These results underscore the potential of exploiting the sharpness disparity to improve LLM training.
\end{abstract}

\renewcommand{\thefootnote}{}

\vspace{-2.em}
~\footnotetext{* Equal contributions.}
~\footnotetext{Correspondence to: Mingze Wang and Lei Wu.}

\renewcommand{\thefootnote}{\arabic{footnote}}


\section{Introduction}
\label{section: introduction}

% \vspace{-.1cm}
Transformers~\citep{vaswani2017attention} have achieved remarkable success across fields, including natural language processing~\citep{brown2020language}, vision~\citep{dosovitskiy2020image}, and scientific computing~\citep{jumper2021highly}. They have become the de facto design in modern AI models~\citep{team2023gemini,achiam2023gpt,liu2024deepseek}.

Compared to traditional architectures, e.g., multilayer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), transformers exhibit  distinctive {\bf alloy-like characteristics}, where {\bf diverse types of blocks} synergistically combine to achieve superior performance. A transformer at minimum includes self-attention (further broken down into query-key (\QK) and value-output (\VO)) blocks, point-wise feedforward networks (\FFN), normalization layers (\LN), and embedding layers (\Embed).  
%Each of these block types further consists of many blocks distributed across different layers. 
Uncovering the distinct properties of these blocks, as well as the differences and interactions among them, is thus crucial for gaining a deeper insight into transformer models~\citep{wang2024understanding}.


In practice, transformers are typically trained using the AdamW optimizer~\citep{kingma2014adam,loshchilov2017decoupled}. Dissecting the alloy-like characteristics of transformers can provide insights into why Adam outperforms  stochastic gradient descent (SGD) for transformer training~\citep{devlin2018bert,zhang2020adaptive,pesme2023saddle,kunstner2024heavy,zhang2024transformers} and even holds promise for unlocking further improvements in training efficiency~\citep{popel2018training,xiong2020layer,zhang2024adam}. Particularly, \citet{zhang2024transformers} and \citet{zhang2024adam}  observed that unlike MLPs and CNNs,  the Hessian (aka sharpness or curvature) of transformers exhibits a distinct blockwise heterogeneity. Building on this insight, \citet{zhang2024adam} successfully reduced Adam's memory footprint nearly by half  without sacrificing training efficiency for  a variety of LLM and non-LLM training tasks.


% \zzp{It is improper to mention the layerwise learning rate here, as we haven't introduce our blockwise learning rate method. A proper logic should be: 1. we introduce BW LR for transformers. 2. Indeed, some previous research on traditional deep models also investigate similar strategies, e.g. layerwise learning rate. 3. However, the layerwise learning rate cannot be directly applied to transformers, because of the special Blockwise sharpness heterogeneity. 4. In addition, we test the layerwise sharpness, no clear pattern is observed.}
% Traditionally, deep learning research has focused on accelerating training via layerwise learning rates, with success primarily in architectures like MLPs and CNNs \citep{yang2019xlnet, yang2022tensor, everett2024scaling, shin2024initializing}. 
% However, these methods have not effectively translated to training deep transformers, particularly for LLM tasks.  
% We hypothesize that this gap stems from transformers' alloy-like characteristics. 
% This inherent block-level diversity makes uniform layerwise learning rate strategies inadequate for fully capturing the complexity of transformers.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.31\linewidth]{figures/gpt_barplot_terminal.pdf}
    % \hspace*{3em}
    \includegraphics[width=0.36\linewidth]{figures/llama_web_1_1B_intro.pdf}
    % \vspace{-.1cm}
    \caption{(left) Sharpness disparity among block types in a pre-trained GPT-2 (small), exhibiting a clear order relationship as characterized by {\bf Principle}~\eqref{equ: main findings}.
    (right) For the pre-training of LLaMA (1.1B) on OpenWebText, incorporating our Blockwise LR strategy into AdamW results in a lower terminal loss and a $1.92\times$ speedup compared to the well-tuned vanilla AdamW.}
    \label{fig: introduction}
    \vspace{-.3cm}
\end{figure}

\paragraph*{Our Contribution.}
In this work, we aim to explore how we can leverage the aforementioned alloy-like characteristics of transformers to improve training efficiency. Specifically, our contributions can be summarized as follows:
\begin{itemize}[leftmargin=2em, topsep=0pt]
\item 
\textbf{The sharpness disparity principle}. 
Motivated by the  alloy-like characteristics,  we examine the sharpness of transformers at the level of block type. %, rather than the traditional layer level. 
Surprisingly, we discover a distinct disparity in sharpness across different block types, summarized as follows:
% \vspace{-0.4em}
\begin{snugshade}
\begin{center}
% \vspace*{-1.3em}
% {\bf The sharpness disparity principle.}
\vspace*{-1.em}
\begin{equation}\label{equ: main findings}
\hspace*{-.6em}\cS(\text{\Embed})\! \ll\!  \cS(\text{\QK})\!  <\!  \cS(\text{\FFN}) \! < \! \cS(\text{\VO})\!  \ll\!  \cS(\text{\Norm})\!
\end{equation}
\end{center}
% \vspace{-0.2cm}
\end{snugshade}


% \vspace{-.1cm}

Here $\cS(\bullet)$ denotes the average sharpness of block type $\bullet$ (see Eq.\eqref{equ: mean sharpness} for the calculation details). 
See Figure~\ref{fig: introduction}~(left) for an illustration of this principle. Intriguingly, this principle emerges in the early training stage and persists throughout the subsequent training process, as shown in Figure~\ref{fig: law gpt process}. These findings are validated  through extensive experiments on the training of GPT-2~\citep{radford2019language} and LLaMA models~\citep{touvron2023LLaMA}, spanning various model sizes and datasets. We also provide preliminary theoretical explanations to complement these empirical observations.



\item \textbf{The Blockwise LR strategy.~} Inspired by \textbf{Principle}~\eqref{equ: main findings}, we propose tuning LRs by block type to accelerate LLM  pre-training. 
Specifically, we adjust the LRs of blocks within the same type in proportion to their sharpness, while keeping the LR of the block type with the highest sharpness unchanged.
This strategy accelerates the dynamics along low-sharpness directions without compromising training stability, as the latter  is governed by the high-sharpness directions.

The effectiveness of Blockwise LR is extensively validated in LLM pre-training across both GPT-2 and LLaMA models, with model sizes ranging from 0.12B to 1.1B parameters, and datasets including OpenWebText~\citep{Gokaslan2019OpenWeb} and MiniPile~\citep{kaddour2023minipile}. The results can be summarized as follows:
\begin{snugshade}
% \vspace*{-.5em}
\begin{center}
\vspace*{.1em}
% {\bf Main result:} 
AdamW with Blockwise LR 
achieves lower terminal loss \\ and is nearly {\bf $2\times$ faster} than vanilla AdamW.
\end{center}
\vspace{-0.2em}
\end{snugshade}
\vspace{-0.2em}
See Figure~\ref{fig: introduction} (right) for a quick view of the acceleration effect achieved by Blockwise LR.
Furthermore, we explore the compatibility of Blockwise LR with other Adam-based optimizers. Specifically, we integrate our Blockwise LR into Adam-mini~\citep{zhang2024adam}, achieving both $2\times$ speedup and $2\times$ memory saving. 
\end{itemize}
% This result highlights the versatility of our Blockwise LR and its potential for integration with other optimizers, improving the training efficiency.

% \zzp{Maybe we should include a summary at the end of introduction part.}



% \end{itemize}



% \zzp{It is improper to mention the layerwise learning rate here, as we haven't introduce our blockwise learning rate method. A proper logic should be: 1. we introduce BW LR for transformers. 2. Indeed, some previous research on traditional deep models also investigate similar strategies, e.g. layerwise learning rate. 3. However, the layerwise learning rate cannot be directly applied to transformers, because of the special Blockwise sharpness heterogeneity. 4. In addition, we test the layerwise sharpness, no clear pattern is observed.}

\vspace*{.4em}
\begin{remark}
There has been a long-standing effort in deep learning to accelerate neural network training by adapting layerwise learning rates, a strategy that has proven effective in architectures such as MLPs and CNNs~\citep{yang2019xlnet, yang2022tensor, everett2024scaling, shin2024initializing}. However, these approaches have not been successfully transferred to the training of deep transformers. We hypothesize that 
this gap stems from transformers' distinctive alloy-like characteristics: the inherent block-level diversity makes layerwise 
 learning rate strategies inadequate. To investigate this further, we examine layer-level sharpness in Figure~\ref{fig: layerwise no law} and no clear trends emerge across layers. This suggests that while sharpness disparity exists at the block-type level, it does not exhibit a consistent pattern at the layer level.
\end{remark}


% \wmz{this is not our main finding, and may be placed in related work ... main findings...}



% \vspace{-.4cm}

\section{Related Works}
\label{section: related works}

% \vspace{-.1cm}

% {\bf Hessian structure of transformers.}
% \citet{park2022vision} provide empirical evidence that the \SA in vision transformers leads to a more non-convex but smoother loss landscape than that of CNNs. 
% ~\citet{zhang2024transformers} empirically demonstrated that Hessian blocks in transformers exhibit much more  disparity than CNNs, suggesting that this could be a potential cause for the reliance on adaptive optimizers. However, their work does not provide a clear principle regarding the sharpness distinction across different transformer blocks.





{\bf Sharpness structures in transformers.} Recent work has started to investigate blockwise sharpness patterns in transformer models through Hessian-based analyses.
For example,~\citet{zhang2024transformers} empirically observed the sharpness' blockwise heterogeneity  but did not establish a clear principle regarding the sharpness disparity among different blocks. Meanwhile, \citet{ormaniec2024does} provided a Hessian analysis for a single self-attention (\SA) layer, focusing only on the  sharpness disparity between the query-key (\QK) and value-output (\VO) blocks within the same layer. 

% \vspace{-.05cm}

In contrast, we examine sharpness at the block-type level across the entire transformer architecture, rather than focusing on individual blocks (as in \citet{zhang2024transformers}) or a single layer (as in \citet{ormaniec2024does}). This coarse-grained perspective reveals a consistent  disparity, as formalized by \textbf{Principle}~\eqref{equ: main findings},  which persists throughout most of the training process—except during the initial steps.

% \vspace{-.05cm}

{\bf Efficient optimizers for LLM pre-training.} AdamW (Adam with decoupled weight decay)~\citep{loshchilov2017decoupled} has become the default optimizer in LLM pre-training. 
Efforts to design more efficient optimizers generally fall into two main categories: accelerating convergence and reducing memory footprint.
Accelerations have been developed using techniques such as Nesterov momentum~\citep{xie2022adan}, diagonal second-order estimates~\citep{liu2023sophia,wang2024improving}, variance reduction~\citep{yuan2024mars}, and matrix-based preconditioners~\citep{jordan2024muon,vyas2024soap}. 
Memory-efficient optimizers utilize sign-based methods~\citep{chen2024symbolic}, reduced usage of second moments in Adam~\citep{zhang2024adam}, and gradient low-rank projection~\citep{zhao2024galore}.
The closest work to our Blockwise LR is~\citet{wang2024improving}, which also increases the LR along low-sharpness directions. A detailed comparison is deferred to Section~\ref{section: application}.

% \vspace{-.05cm}

{\bf The edge of stability (EoS) phenomenon.} Neural network training typically occurs at the EoS stage~\citep{wu2018sgd,Jastrzebski2020The,cohen2021gradient,cohen2022adaptive}, where the optimizer exhibits oscillatory behavior along sharp directions without diverging, while steadily progressing along flat directions, leading to loss reduction. Several works \citep{wen2024understanding,song2024does,cohen2024understanding,wang2024improving} have highlighted the crucial role of the dynamics along flat directions (referred to as river directions by \citet{wen2024understanding}, bulk directions by \citet{song2024does}, and stable direction in \citet{wang2024improving}) in reducing total loss. Notably, \citet{wen2024understanding} further demonstrated that this picture is essential for understanding LLM pre-training. Building on these insights, our Blockwise LR approach is designed to accelerate training by amplifying the dynamics particularly along the flat river directions.
% For theoretical explanations of this fascinating phenomenon, we refer to recent studies~\citep{ma2022beyond,ahn2022understanding,song2023trajectory,blanc2020implicit,arora2022understanding,lyu2022understanding,damian2022self,cohen2024understanding}. 




% Inspired by EoS, \citet{wang2024improving} proposed an optimizer that accelerates LLM pre-training by increasing the LRs along low-sharpness directions.  


% \vspace{-.2cm}

\section{Preliminaries}

% \vspace{-.1cm}

{\bf Notations.} Let $\norm{\cdot}_2$, $\norm{\cdot}_\rF$, and $\Tr(\cdot)$ denote the spectral norm, Frobenius norm and trace for matrices, respectively. Given $\bA\in\bbR^{m\times n}$, its row-wise vectorization is defined as $\vec(\bA)=(a_{1,1},\cdots,a_{1,n},\cdots,a_{m,1},\cdots,a_{m,n})\in\bbR^{mn}$. 
The Kronecker product and Hadamard product are denoted by $\otimes$ and $\odot$, respectively.
The row-wise mean and covariance of $\bA \in \bbR^{m \times n}$ are denoted by $\bbE_r[\bA]\in\bbR^{m\times n}$ and $\bbV_r[\bA]\in\bbR^{m\times n}$, respectively. Specifically, they are defined as: for all $i\in [m], j\in [n]$, 
$(\bbE_r[A])_{i,j}\!=\!\frac{1}{n}\sum_{k=1}^n A_{i,k},\, (\bbV_r[A])_{i,j}\!=\!\left(A_{i,j}-\frac{1}{n}\sum_{k=1}^n A_{i,k}\right)^2$.
We will use standard big-O notations like $\cO(\cdot)$, $\Omega(\cdot)$, and $\Theta(\cdot)$ to hide problem-independent constants.


{\bf Jacobian matrix.} Given a vector-valued function: $\boldsymbol{b}\mapsto\ba(\boldsymbol{b})$ with  $\boldsymbol{b}\in\bbR^n$ and $\ba(\boldsymbol{b})\in\bbR^{m}$, the Jacobian is defined as $\frac{\partial \ba}{\partial\boldsymbol{b}}=(\frac{\partial a_i}{\partial b_j})_{i,j}\in\bbR^{m\times n}$. Analogously, for a matrix-valued function: $\bB\mapsto\bA(\bB)$ where
$\bB\in\bbR^{p \times q}$ and $\bA(\bB)\in\bbR^{m\times n}$, to avoid directly working with tensors, the Jacobian is defined as $\frac{\partial \bA}{\partial\bB}:=\frac{\partial\vec(\bA)}{\partial\vec(\bB)}\in\bbR^{mn\times pq}$.

% \vspace{-.1cm}

\subsection{The Transformer Architecture}

% \vspace{-.1cm}

Given an $n$-token input sequence $\bX=(\bx_{1}^\top,\cdots,\bx_{n}^\top)^\top\in\bbR^{n\times d}$, where $d$ refers to the vocabulary size in LLM and each $\bx_i$ corresponds to the token's one-hot encoding,  an $L$-layer transformer $\TF$ processes it as follows.

{\bf Embedding layer.} First, each input token is embedded into the latent space through an embedding layer with parameters $\bW_E\in\bbR^{d\times D},\boldsymbol{b}_E\in\bbR^{1\times D}$:

\vspace{-.4cm}

$$
\bx_s^{(0)}=\bx_{s}\bW_E+\boldsymbol{b}_E,\ s\in[n],
$$ 

\vspace{-.2cm}

where the bias $\boldsymbol{b}_E$ is omitted in LLMs such as nanoGPT~\citep{Karpathy2022}.

{\bf $L$-layer \SA-\FFN\ blocks.}
Then the embedded sequence $\bX^{(0)}$ is processed by  $L$-layer \SA-\FFN\ blocks, and the output of the final layer is taken as the output sequence $\TF(X)=X^{(L)}\in\bbR^{n\times D}$. For each layer $l\in[L]$, the computations are as follows:

\vspace{-.4cm}

\begin{equation}\label{model: Transformer}
\begin{aligned}
    \bX^{(l-\frac{1}{2})}&=\bX^{(l-1)}+\SA^{(l)}(\LN^{(l-1/2)}(\bX^{(l-1)}));
    \\
    \bX^{(l)}&=\bX^{(l-\frac{1}{2})}+\FFN^{(l)}(\LN^{(l)}(\bX^{(l-\frac{1}{2})})).
\end{aligned}
\end{equation}

\vspace{-.3cm}

{\bf \LN\ blocks.}
Here, $\LN^{(v)}$ ($v\in\{l-1/2,l\}$) denote normalization layers (e.g., LayerNorm~\citep{lei2016layer} and RMSNorm~\citep{zhang2019root}) with learnable parameters $\bgamma^{(v)},\bbeta^{(v)}\in\bbR^{1\times D}$.
 % Placing the normalization before \FFN\ or \SA\ is referred to as Pre-norm, a choice widely adopted in popular LLMs such as GPT-2 and LLaMA. 
For LayerNorm, the computation for a token $\bx\in\bbR^{1\times D}$ is:

\vspace{-.6cm}

\begin{align*}
    \LN^{(v)}(\bx)=\frac{\bx-\bbE_r[\bx]}{\bbV_r[\bx]}\odot\bgamma^{(v)}+\bbeta^{(v)}.
\end{align*}

\vspace{-.3cm}

where the bias $\bbeta$ is omitted in LLMs such as nanoGPT. 

{\bf \FFN\ blocks.}
$\FFN^{(l)}$ denotes a (token-wise) two-layer FFN of width $M$, comprising parameters $\bW_1^{(l)}\in\bbR^{D\times M},\bW_2^{(l)}\in\bbR^{M\times D}$, and using activation function $\sigma(\cdot)$ such as ReLU. For any token $\bx\in\bbR^{1\times D}$, the operation is:

\vspace{-.6cm}

\begin{align*}
    \FFN^{(l)}(\bx)=\sigma(\bx\bW_1^{(l)})\bW_2^{(l)}.
\end{align*}

\vspace{-.3cm}

{\bf \SA\ blocks.} $\SA^{(l)}$, a multi-head self-attention, has parameters $\bW_{Q}^{(l)},\bW_{K}^{(l)},\bW_{V}^{(l)},\bW_O^{(l)}\in\bbR^{D\times D}$. When applied to a sequence $\bZ\in\bbR^{n\times D}$, it operates as:

\vspace{-.6cm}

\begin{gather*}
    \SA^{(l)}(\bZ)=\sum_{h=1}^{H}\SA^{(l,h)}(\bZ)\bW_O^{(l,h)},\quad \SA^{(l,h)}(\bZ)=
    \\\sm\left(\frac{\<\bZ\bW_Q^{(l,h)},\bZ\bW_K^{(l,h)}\>+\bM}{\sqrt{D/H}}\right)\left(\bZ\bW_V^{(l,h)}\right),
\end{gather*}

\vspace{-.3cm}

where $H$ is the head number, and $\bW_{Q}^{(l,h)},\bW_{K}^{(l,h)},\bW_{V}^{(l,h)}\in\bbR^{D\times(D/H)}$, $\bW_{O}^{(l,h)}\in\bbR^{(D/H)\times D}$ are split from $\bW_{Q}^{(l)},\bW_{K}^{(l)},\bW_{V}^{(l)}$, $\bW_O^{(l)}$ by heads, respectively. The operator
$\sm(\cdot)$ represents the row-wise softmax normalization. 
For the next-token prediction, the mask $\bM\in\bbR^{n\times n}$ satisfies $M_{i,j}=-\infty$ if $i< j$ and $M_{i,j}=0$ otherwise. 


% Additionally, positional information is incorporated using positional encodings (PE), such as absolute PE~\citep{vaswani2017attention} for GPT-2\citep{radford2019language}, Rotarary PE~\citep{su2024roformer} for LLaMA~\citep{touvron2023LLaMA}, Alibi~\citep{press2021train} employed in BLOOM~\citep{workshop2022bloom}, and T5's RPE~\citep{raffel2020exploring}. While the specific form of PE is not a focus of this paper, we adhere to the default choices of LLMs in experiments.


% \vspace{-.2cm}

\subsection{Blockwise  Sharpness and the Efficient Estimation} 

\vspace{-.1cm}

Measuring sharpness requires accessing the Hessian matrix, which is computationally expensive due to the high dimensionality of the parameter space. Consequently, approximate methods are needed to reduce computational complexity.

Let $\ell(\cdot,\cdot)$ denote the cross-entropy loss. For an input data $\bx\in\bbR^{d_x}$ and label $\by\in\bbR^{d_y}$, let the model's prediction be $f(\bx;\btheta)\in\bbR^{d_y}$.
The Fisher (Gauss-Newton) matrix $F(\btheta)$ is widely recognized approximation of the Hessian, particularly near minima. Thus, the diagonal Hessian can be estimated as $\bh(\btheta)={\rm diag}(F(\btheta))$, a popular technique in deep learning optimization~\citep{martens2015optimizing,grosse2016kronecker,george2018fast,mi2022make,liu2023sophia,wang2024improving}. Moreover,
given an input batch $\{(\bx_b,\by_b)\}_{b=1}^B$, the empirical diagonal Fisher can be estimated:
$
{\rm diag}(\hat{F}(\btheta))=\frac{1}{B}\sum_{b=1}^B\nabla\ell(f(\bx_b;\btheta);\hat{\by}_b)\odot\nabla\ell(f(\bx_b;\btheta);\hat{\by}_b), \text{ where }\hat{\by}_b\sim{\rm softmax}(f(\btheta;\bx_b)).
$ 
However, as noted by~\citet{liu2023sophia},
implementing this estimator is computationally expensive due to the need to calculate $B$ single-batch gradients.
\citet{liu2023sophia} proposed a more convenient estimator ${\rm diag}(\hat{F}_{\rm eff}(\btheta))$, which only requires the computation of the mini-batch gradient $\nabla\hat{\cL}_B(\btheta)=\frac{1}{B}\sum_{b=1}^B\nabla\ell(f(\bx_b;\btheta);\hat{\by}_b) \text{ with }\hat{\by}_b\sim{\rm softmax}(f(\bx_b;\btheta))$:
\begin{equation}\label{equ: fisher estimate}
\begin{aligned}
   \bh(\btheta) = {\rm diag}(\hat{F}_{\rm eff}(\btheta))=B\cdot\nabla\hat{\cL}_B(\btheta)\odot\nabla\hat{\cL}_B(\btheta).
\end{aligned}
\end{equation}
According to~\citet[Section 2]{liu2023sophia}, this estimator is unbiased, i.e., $\bbE_{\hat{\by}}[{\rm diag}(\hat{F}_{\rm eff}(\btheta))]=\bbE_{\hat{\by}}[{\rm diag}(\hat{F}(\btheta))]$. 
% Additionally, for squared loss, one can simply use a similar estimator~\citep{liu2023sophia}.


Given a block type $\bullet\in\{\Embed,\QK,\VO,\FFN,\LN\}$, let $\btheta[\bullet]$ represent the parameters associated with all blocks of that type,  and let $\bh(\btheta[\bullet])$ denote the corresponding diagonal Hessian. The average sharpness for each block type can then be approximated as follows:

\vspace{-.4cm}

\begin{equation}\label{equ: mean sharpness}
\cS(\btheta[\bullet]):=\frac{\Tr(\bh(\btheta[\bullet]))}{\#(\btheta[\bullet])}= \frac{B\norm{\nabla_{\btheta[\bullet]}\hat{\cL}_B(\btheta)}_\rF^2}{\#(\btheta[\bullet])},    
\end{equation}

\vspace{-.2cm}

where $\hat{\cL}_B$ corresponds to~\eqref{equ: fisher estimate} and $\#(\btheta[\bullet])$ denotes the number of parameters associated with the block type $\bullet$. For brevity, $\btheta$ in~\eqref{equ: mean sharpness} will be omitted when there is no ambiguity. 
\begin{remark}
It is worth noting that in~\eqref{equ: mean sharpness}, the sharpness is averaged over all blocks of the same type, which may be distributed across different layers, rather than being calculated within each individual block.
\end{remark}




% \vspace{-.2cm}

\section{The Sharpness Disparity Principle}\label{section: principle}

% \vspace{-.1cm}

\subsection{Main Findings}

% \vspace{-.1cm}

We first  investigate the sharpness  disparity across different types of  building blocks (\Embed, \QK, \VO, \FFN, \Norm) in transformer-based LLMs. 
Specifically, we pre-trained GPT-2~\citep{radford2019language} and LLaMA~\citep{touvron2023LLaMA} models on the OpenWebText dataset using default configurations.
Blockwise diagonal Hessians are analyzed at various checkpoints using the Hessian estimator~\eqref{equ: fisher estimate}. The experimental details can be found in  Appendix~\ref{appendix: experiments for baselines}.



In Figures~\ref{fig: introduction} (left) and \ref{fig: law final} (left), we report the average  sharpness, estimated using~\eqref{equ: mean sharpness}, of  the five typical types of blocks for GPT-2 and LLaMA, respectively. The results reveal a clear and consistent sharpness disparity among different block types, as summarized in  {\bf Principle}~\eqref{equ: main findings}. Specifically, \Norm\ layers consistently exhibit the highest sharpness, the \Embed\ layers are the flattest, and \QK\ layers are relatively flatter compared to \FFN\ and \VO\ layers.
These findings, to the best of our knowledge, provide the first comprehensive comparison of sharpness across  block types in transformers.

\begin{figure}[!ht]
    \centering
    % \includegraphics[width=0.2\linewidth]{figures/law_gpt_final_mean.png}
    \includegraphics[width=0.27\linewidth]{figures/llama_barplot_terminal.pdf}
    \includegraphics[width=0.25\linewidth]{figures/gpt_distplot_terminal.pdf}
    % \vspace{-.1cm}
    \caption{(left) The average sharpness for the five typical block types in a pre-trained LLaMA model (0.25B); (right) the sharpness distribution across different blocks in a pre-trained GPT-2 (small) model.}
    \label{fig: law final}
    % \vspace{-.3cm}
\end{figure}

Figure~\ref{fig: law final} (right) plots the full sharpness distribution for each block type, whereas Figures~\ref{fig: introduction} (left) and~\ref{fig: law final} (left) only report mean sharpness values. Evidently,  even at the distribution level,  {\bf Principle} \eqref{equ: main findings} remains valid. Interestingly,
 the \Embed~block exhibits much higher variance compared to other blocks. This behavior likely stems from the embedding layer's direct interaction with the entire vocabulary, where rare tokens result in the wide spread of small sharpness and frequent tokens contribute to large sharpness. A similar insight has been utilized by  \citet{kunstner2024heavy} to explain the necessity of Adam in training NLP models.



\begin{figure*}[tb!]
    \centering
    \subfloat[Evolution of the average sharpness across different blocks during pre-training GPT-2 (small) on OpenWebText.]
    {\includegraphics[width=0.95\linewidth]{figures/gpt_barplot_time_no_arrow.pdf} }
    \\
    % \vspace{-.2cm}
    \subfloat[Evolution of the average sharpness across different blocks during pre-training LLaMA (0.25B) on OpenWebText.]
    {\includegraphics[width=0.95\linewidth]{figures/llama_barplot_time_no_arrow.pdf} }
    % can choose one
    % \vspace{-.15cm}
    \caption{In these experiments, the total training steps are both 50k. {\bf Principle} \eqref{equ: main findings} emerges during the initial phase (from iteration 0 to iteration 1k), which accounts for only approximately $2\%$ of the total steps, and persists throughout the subsequent  training process.}
    \label{fig: law gpt process}
    % \vspace{-.3cm}
\end{figure*}


Furthermore, Figure~\ref{fig: law gpt process} illustrates the  evolution of blockwise sharpness during the training process. We can see that {\bf Principle} \eqref{equ: main findings} is not exclusive to well-trained transformers; instead, it emerges in the early stages of training and persists consistently throughout the subsequent training process. This observation underscores the potential of leveraging {\bf Principle}~\eqref{equ: main findings} to enhance LLM pre-training; we refer to Section~\ref{sec:adam_bl_adam_with_blockwise_learning_rates} for further explorations.


{\bf Comparison with existing works}. Our findings build on prior work, extending key observations. \citet{zhang2024transformers}  noted the block heterogeneity in the Hessian of transformers  but did not establish a clear principle for sharpness distinctions across blocks, as we do with {\bf Principle}~\eqref{equ: main findings}. The work of \citet{ormaniec2024does} is more closely related  but focuses solely on a single self-attention layer (\SA), reporting the relationship $\cS(\QK) < \cS(\VO)$. In contrast, we analyze all major block types in transformers, including \Embed, \FFN, and \LN, thereby offering a more comprehensive principle that captures the full scope of  sharpness disparity.


% \vspace{-.2cm}

\subsection{Theoretical Insights}
\label{section: theoretical insights}

% \vspace{-.1cm}

To provide theoretical insights into explaining {\bf Principle}~\eqref{equ: main findings}, we derive analytic expressions of $\cS(\bullet)$ and analyze their dependence on parameter magnitudes and numbers of each block. For simplicity, we denote $\cQ(\btheta):=\hat{\cL}_B(\btheta)$, where $\hat{\cL}_B(\btheta)$ is defined in~\eqref{equ: fisher estimate}. Then from~\eqref{equ: mean sharpness}, we have $\cS(\bullet)=B\norm{\nabla_{\bullet}\cQ}_{\rF}^2/\#(\bullet)$. Without loss of generality, we set $B=1$.
Our calculations for $\nabla \cQ$ apply to general $\cQ$. 

Considering blocks across different layers is  complicated. Therefore, we focus on comparisons within the same layer. Specifically, we examine the following sharpness comparisons: (i) \FFN\ vs. \LN\ within the same layer; (ii) \SA\ (comprising \QK\ and \VO) vs.~\LN\ within the same layer; and (iii) \Embed\ vs.~the adjacent \LN.
% \footnote{A thorough comparison of the blockwise Hessian across different layers require new techniques, and we leave for future work.}


\begin{theorem}[\FFN\ vs.~\Norm]\label{thm: FFN vs Norm}
Consider the $l$-th layer in a transformer~\eqref{model: Transformer}.
Omitting the layer index for simplicity, let $\bY=\bX+\FFN\left(\LN\left(\bX;\bgamma\right);\bW_1,\bW_2\right)$, where FFN utilizes the (Leaky) ReLU activation $\sigma$. 
Then, the gradients of $\cQ$ w.r.t. $\bW_1,\bW_2$, and $\bgamma$ are:
% \vspace{-.6cm}
\begin{align*}
    \frac{\partial \cQ}{\partial\bW_{2}}&=\frac{\partial \cQ}{\partial \bY}\left(\bX_{\LN}\bW_1\odot\frac{\partial\ASA}{\partial\MSA}\right)\otimes\bI_d;
    \\
    \frac{\partial \cQ}{\partial\bW_{1}}&=\frac{\partial \cQ}{\partial \bY}\left(\bI_{n}\otimes{\bW_2}^\top\right)\frac{\partial\ASA}{\partial\MSA}\left(\bX_{\LN}\otimes\bI_M\right);
    \\
    \frac{\partial \cQ}{\partial\bgamma}&=\frac{\partial \cQ}{\partial \bY}\left(\bI_{n}\otimes{\bW_2}^\top\right)\frac{\partial\ASA}{\partial\MSA}\left(\bI_n\otimes{\bW_1}^\top\right)
    \\&\quad\quad\quad \diag\big(\vec(\bX_{\rm std})\big)\big(\bone_{n\times 1}\otimes \bI_{d}\big),
\end{align*}  
% \vspace{-.2cm}
where $\bX_{\rm std}:=\frac{\bX-\bbE_r[\bX]}{\sqrt{\bbV_r[\bX]}},\bX_\LN:=\LN(\bX;\bgamma)=\bX_{\rm std}\odot\big(\bone_{n\times 1}\otimes\bgamma\big),\ASA:=\sigma(\MSA),\MSA:=\bX_{\LN}\bW_1$. 
Let $\Psi:=n\sqrt{D}\norm{\frac{\partial \cQ}{\partial \bY}}_{\rF}\norm{\frac{\partial\ASA}{\partial\MSA}}_{\rF}\norm{\bW_1}_\rF\norm{\bW_2}_\rF\norm{\bgamma}_{\rF}$. 
Then, the blockwise average sharpness can be bounded as: 
% Then, the gradients of $\cQ$ w.r.t. different blocks ($\bW_1,\bW_2, \bgamma$) are provided in Appendix~\ref{appendix: proof: thm: FFN vs Norm}.
% Furthermore, there exists a problem-dependent constant $\Psi>0$ (detailed in Appendix~\ref{appendix: proof: thm: FFN vs Norm}), such that:

\vspace{-.4cm}

% \begin{equation}
\begin{align*}
    % \frac{1}{D^2}\norm{\frac{\partial \cQ}{\partial\bW_{\bullet}}}_\rF^2
    \cS(\bW_{\bullet})&=\cO\left(\frac{\Psi^2}{D^2\|\bW_\bullet\|_\rF^2}\right),\bullet\in\{1,2\};\\ 
    % \frac{1}{D}\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF^2
    \cS(\bgamma)&=\cO\left(\frac{\Psi^2}{D\|\bgamma\|_\rF^2}\right),
\end{align*}
% \end{equation}

\vspace{-.2cm}

where the denominators ($D^2$ or $D$) reflect the number of parameters in each group.
\end{theorem}

\vspace{-.1cm}

Theorem~\ref{thm: FFN vs Norm} provides theoretical support for our main finding: $\cS(\FFN)$ is substantially smaller than $\cS(\Norm)$.
As illustrated in Figure~\ref{fig: blockwise norm} (a), during training, $\norm{\bgamma}_\rF$ gradually decreases, and $\norm{\bW_{\bullet}}_\rF$ ($\bullet\in\{1,2\}$) in FFN layers remains larger than $\norm{\bgamma}_{\rF}$, resulting in $D^2\norm{\bW_\bullet}_{\rF}^2\gg D\norm{\bgamma}_{\rF}^2$.




\begin{theorem}[\QK, \VO\ vs.~\Norm]\label{thm: QK VO vs Norm}
Consider the ($l-\frac{1}{2}$)-th layer in~\eqref{model: Transformer}. Omitting the layer index for simplicity, let $\bY=\bX+\SA\Big(\LN\left(\bX;\bgamma\right);\bW_K,\bW_Q,\bW_V,\bW_O\Big)$. 
Consider a single-head attention (i.e., $H=1$) for simplicity. Then, the gradients of $\cQ$ w.r.t. different blocks ($\bW_K,\bW_Q, \bW_V,\bW_O, \bgamma$) are provided in Appendix~\ref{appendix: proof: thm: QK VO vs Norm}.
Furthermore, there exist two problem-dependent constants $\Phi,\Psi>0$ (detailed in Appendix~\ref{appendix: proof: thm: QK VO vs Norm}), such that:

\vspace{-.4cm}

\begin{align*}
    % \frac{1}{D^2}\norm{\frac{\partial \cQ}{\partial\bW_{\bullet}}}_\rF^2
    \cS(\bW_{\bullet})&=\cO\left(\frac{\Phi^2}{D^2\norm{\bW_\bullet}_\rF^2}\right),\ \bullet\in\{K,Q\};
    \\
    % \frac{1}{D^2}\norm{\frac{\partial \cQ}{\partial\bW_{\bullet}}}_\rF^2
    \cS(\bW_{\bullet})&=\cO\left(\frac{\Psi^2}{D^2\norm{\bW_\bullet}_\rF^2}\right),\ \bullet\in\{V,O\};
    \\
    % \frac{1}{D}\norm{\frac{\partial \cQ}{\partial\bgamma}}_\rF^2
    \cS(\bgamma)&=\cO\left(\frac{\Phi^2 + \Psi^2}{D\norm{\bgamma}_\rF^2}\right).
\end{align*}

\vspace{-.2cm}

where the denominators ($D^2$ or $D$) reflect the number of parameters in each group.
\end{theorem}

\vspace{-.1cm}

Theorem~\ref{thm: QK VO vs Norm} provides theoretical support for our main finding that both $\cS(\QK)$ and $\cS(\VO)$ are significantly smaller than $\cS(\LN)$.
The inclusion of the {\rm softmax} operation in attention layers introduces additional complexity in the calculations. Detailed derivations are given in the appendix. As shown in Figure~\ref{fig: blockwise norm} (b), during training, $\norm{\bgamma}_\rF$ gradually decreases, and $\norm{\bW_{\bullet}}_\rF$ ($\bullet\in\{K,Q,V,O\}$) in \SA\ blocks remains larger than $\norm{\bgamma}_{\rF}$, resulting in $D^2\norm{\bW_\bullet}_{\rF}^2\gg D\norm{\bgamma}_{\rF}^2$.


This theorem does not explicitly establish that $\cS(\QK) < \cS(\VO)$. Studying this relation requires a deeper analysis of the constants $\Phi$ and $\Psi$, as well as the magnitudes of $\norm{\bW_\bullet}_\rF$. \citet{ormaniec2024does} has demonstrated $\cS(\QK)<\cS(\VO)$ both theoretically and experimentally, and we defer to that analysis instead of repeating it here.



\begin{theorem}[\Embed\ v.s.~\Norm]\label{thm: Embed vs Norm}
Consider the embedding layer and its adjoint normalization layer of a transformer~\eqref{model: Transformer}. Omitting the layer index for simplicity, let: $\bY:=\LN(\bX\bW_{\rm emb};\bgamma)$.
% where the inputs $\bX$ are one-hot encoded.
The gradients of $\cQ$ w.r.t~$\bW_{\rm emb}$ and $\bgamma$ are derived in Appendix~\ref{appendix: proof: thm: Embed vs Norm}.
Moreover, there exists a problem-dependent constant $\Psi>0$ (also detailed in Appendix~\ref{appendix: proof: thm: Embed vs Norm}), such that:

\vspace{-.4cm}

\begin{align*}
    % \frac{1}{Dd}\norm{\frac{\partial\bY}{\partial\bW_E}}_\rF^2
    \cS(\bW_E)&=\cO\Bigg(\frac{\Psi^2}{Dd\min\limits_{i\in[d]}\norm{\tilde{\bw}_{E_i}}_2^2}\Bigg);\\  
    % \frac{1}{D}\norm{\frac{\partial\bY}{\partial\bgamma}}_\rF^2
    \cS(\bgamma)&=\cO\left(\frac{\Psi^2}{D\norm{\bgamma}_\rF^2}\right),
\end{align*}

\vspace{-.3cm}

where $\tilde{\bW}_E=(\tilde{\bw}_{E_1}^\top,\cdots,\tilde{\bw}_{E_d}^\top)^\top:=\bW_E-\bbE_r[\bW_E]$. The denominators ($Dd$ or $D$) represent the number of parameters in each group.
\end{theorem}

\vspace{-.1cm}

Theorem~\ref{thm: Embed vs Norm} provides theoretical justification for our main finding that $\cS(\Embed)$ is much smaller than $\cS(\Norm)$. 
As shown in Figure~\ref{fig: blockwise norm}(c), during training, $Dd\norm{\tilde{\bw}_{E_i}}_2^2\gg D\norm{\bgamma}_F^2$. (Notice that the vocabulary size $d$ is very large in practice, e.g., 50304 for the GPT tokenizer.) 


% In summary, 
Recalling the definition of average sharpness~\eqref{equ: mean sharpness}, the key step in deriving Theorem~\ref{thm: FFN vs Norm} and~\ref{thm: QK VO vs Norm}, and~\ref{thm: Embed vs Norm} is establishing  $\|\nabla_{\bullet}\cQ\|=\cO(1/ \|\btheta[\bullet]\|)$.
This relationship is highly intuitive given the compound multiplicative nature of transformer blocks, where the norm of the derivatives is inversely proportional to the norm of associated parameters, even with weak non-linearities. For example, if $y=\prod_{i=1}^n x_i$ and $\cQ=\varphi(y)$, then $|\partial\cQ/ \partial x_i|=|\phi'(y) y/ x_i|\propto 1/|x_i|$ for all $i\in[n]$.


\vspace{-.1cm}

\section{The Blockwise LR Strategy}\label{section: application}
\label{sec:adam_bl_adam_with_blockwise_learning_rates}

% \vspace{-.1cm}

% \subsection{Motivations and Methods}

Recalling Figure~\ref{fig: law gpt process}, the sharpness disparity across different blocks, as described in~\eqref{equ: main findings}, emerges early in training and persists until convergence. This insight can be leveraged to accelerate LLM pre-training, as elaborated later.


{\bf Fast-slow dynamics at EoS.} 
As discussed in Section~\ref{section: related works}, recent studies \citep{wen2024understanding,song2024does,wang2024improving} have highlighted the distinct roles of the dynamics along high- and low-sharpness directions during EoS. The main picture is summarized as follows:

\vspace{-.2cm}

\begin{itemize}[leftmargin=2em]
    \item {\bf Fast dynamics}: Along {\em high-sharpness directions}, the optimizer exhibits significant fluctuations without converging or diverging. These components of dynamics govern training stability, as further increasing the LR in these directions can lead to instability, while contributing little to loss reduction.

    \vspace{-.1cm}
    
    \item {\bf Slow dynamics}: Along {\em low-sharpness directions}, the optimizer progresses steadily, making the primary contribution to loss reduction, albeit at a slow rate.
\end{itemize}

\vspace{-.1cm}

Inspired by the above picture, a promising approach to accelerating training is as follows: given a base optimizer, increase the LRs along low-sharpness directions while keeping the LR of high-sharpness directions unchanged. This strategy aims to  speed up loss reduction without compromising training stability.

\citet{wang2024improving} has implemented this idea by adjusting the LR of each parameter based on its sharpness. However, this approach faces two key challenges: 1) it requires frequent diagonal Hessian estimation, which imposes significant computational and memory overhead;  2) sharpness estimates at the individual parameter level can be unreliable.

{\bf The Blockwise LR.}
 Unlike \citet{wang2024improving},  we  propose adjusting LRs at the block-type level, as our {\bf Principle}~\eqref{equ: main findings}   reveals a consistent sharpness disparity at this granularity.
Specifically, let $\eta_{\rm base}$ denote the LR for base optimizers such as AdamW, the LR for each block type is then adjusted as follows:

\vspace{-.2cm}

\begin{itemize}[leftmargin=2em]
    \item \Norm\ blocks (the sharpest directions): we still use the base LR, $\eta_\Norm=\eta_{\rm base}$, to keep training stability;

    \vspace{-.1cm}
    
    \item  Other blocks (low-sharpness directions): we adjust the LRs of these blocks by $\eta_{\bullet}\propto r(\bullet) \eta_{\rm base}$, where $\bullet\in\{\Embed,\QK,\FFN,\VO\}$, where $r(\bullet)$ denotes the adjusting ratio for the block type $\bullet$.
\end{itemize}

\vspace{-.1cm}


Naturally, we can set $r(\bullet)\propto \cS(\Norm)/\cS(\bullet)$. However,
in practice, we find that manually tuning $r(\bullet)$'s--involving only four hyperparameters--while following the qualitative trend described by   {\bf Principle}~\eqref{equ: main findings} is more effective. Further details are provided in Section~\ref{sec:experiments}.

It is also worth noting that due to its simplicity, Blockwise LR can be seamlessly integrated into modern LLM training frameworks such as Megatron~\citep{shoeybi2019megatron}.

% {\bf Comparison with related work}.~\citet{wang2024improving} proposed a similar method to accelerate LLM pre-training by applying larger LR to low-sharpness directions. However, their approach involves periodic Hessian estimations during training, resulting in significant computational and memory overhead, which is in stark contrast to Blockwise LR.

\vspace{-.1cm}

\section{Experiments}
\label{sec:experiments}

% \vspace{-.1cm}

{\bf Models and datasets.} We evaluate our proposed Blockwise LR in the pre-training of decoder-only LLMs across various  model types, model sizes, and datasets. Specifically, we consider two widely-used LLMs: {\bf LLaMA} and {\bf GPT-2}; we experiment with model sizes ranging {\bf from 0.12B to 1.1B} parameters; the datasets includes OpenWebText~\citep{Gokaslan2019OpenWeb}~\footnote{An opensource recreation of the WebText corpus, widely used for LLM pre-training such as
RoBERTa~\citep{liu2019roberta} and GPT-2.} and MiniPile~\citep{kaddour2023minipile}\footnote{ A 6GB subset of the deduplicated Pile (825GB)~\citep{gao2020pile}, providing a highly diverse text corpus. 
% Given its diversity, training on minipile poses challenges and potential instabilities.
}.

% \newpage



{\bf Baselines.} 
As a baseline, we use the default AdamW  optimizer, configured with the hyperparameters $\beta_1=0.9,\beta_2=0.95$ and weight decay $\lambda=0.1$. To ensure training stability, gradient clipping is applied with $1.0$. These settings align with the training protocols used in nanoGPT and LLaMA models~\citep{touvron2023LLaMA}.
The LR strategy includes a linear warm-up phase followed by a cosine decay scheduler, capped at \texttt{lr\_max}. And the terminal LR \texttt{lr\_min} is set to \texttt{lr\_max}/20. For each experiment, we {\em first tune} the \texttt{lr\_max} to be optimal for AdamW, and the baselines are trained using these optimal \texttt{lr\_max}'s.
Details of the tuned \texttt{lr\_max} values can be found in Appendix~\ref{appendix: experiments for baselines}.



{\bf Adjusting ratio tuning and its transferability.}
To incorporate the Blockwise LR into AdamW, we simply use the \texttt{lr\_max} (tuned for vanilla AdamW) for \LN\ blocks. Then, we {\bf only tuned the four adjusting ratios in a single small-scale experiment} -- specifically the pre-training of LLaMA (0.25B) on Minipile -- following the rule:  $r(\bullet)$ is adjusted according to the trend of $\frac{\cS(\Norm)}{\cS(\bullet)}$,  guided by {\bf Principle}~\eqref{equ: main findings}. The tuned hyperparameters are: 
\vspace{-0.1cm}
{\colorlet{shadecolor}{blue!10}
\begin{snugshade}
\vspace{-0.cm}
\begin{center}
\vspace{-0.35cm}
\begin{equation}\label{equ: tuned hyperparameters, blockwise lr}
    r(\Embed)=10, r(\QK)=8, r(\FFN)=6, r(\VO)=4.
\end{equation}
\end{center}
\vspace{-0.25cm}
\end{snugshade}
\vspace{-0.2cm}
}

Notably, the adjusting ratios are highly robust hyperparameters, as demonstrated in the following ways:

% \vspace{-.2cm}

\begin{itemize}[leftmargin=2em]
    \item First, as shown in Figure~\ref{fig: robust llama web}, in the experiments for tuning the adjusting ratios, Blockwise LR demonstrates robustness to these hyperparameters, consistently accelerating pre-training across a range of $r(\bullet)$'s. The configuration in~\eqref{equ: tuned hyperparameters, blockwise lr} achieves the largest improvements among those tested. Notably, even with suboptimal ratios, Blockwise LR still delivers significant performance gains. Further details are provided in Appendix~\ref{appendix: experiments, blockwise LR}.

% \vspace{-.1cm}
\item Second, the configuration in~\eqref{equ: tuned hyperparameters, blockwise lr}, tuned from a single experiment,  {\bf transfers perfectly} across all AdamW experiments conducted in this paper. Consequently, {\bf we adopt \eqref{equ: tuned hyperparameters, blockwise lr} as the default adjusting ratios for all AdamW experiments}. This robustness aligns with the consistency of {\bf Principle}~\eqref{equ: main findings}, which holds across GPT and LLaMA models, various model sizes, and datasets.

% \vspace{-.1cm}

\end{itemize}

% \vspace{-.1cm}

\subsection{Main Results}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.24\linewidth]{figures/gpt_web_small.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_pile_0_13B.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_pile_0_25B.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_pile_0_5B.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_web_0_25B.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_web_0_5B.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_web_0_75B.pdf}
    \includegraphics[width=0.24\linewidth]{figures/llama_web_1_1B.pdf}
    % \vspace{-.1cm}
    \caption{AdamW with Blockwise LR consistently outperforms AdamW in LLM pre-training tasks across different model types, varying model sizes, and datasets.}
    \label{fig: main results: blockwise lr}
    % \vspace{-.2cm}
\end{figure*}

% \vspace{-.05cm}

 In Figure~\ref{fig: main results: blockwise lr}, we compare the performance of AdamW with Blockwise LR against vanilla AdamW across various settings. Our observations, which consistently hold across all experiments--including both GPT-2 and LLaMA models with sizes ranging from 0.12B to 1.1B--and datasets such as OpenWebText and MiniPile, are as follows:

% \vspace{-.2cm}


\begin{itemize}[leftmargin=2em]
\item Given the same total number of training steps,  Blockwise LR enables AdamW to reach a {\bf lower terminal loss} than vanilla AdamW.
\item Across different total training steps, AdamW with Blockwise LR achieves a  {\bf nearly $2\times$ speedup}  compared to vanilla AdamW.
% \vspace{-.2cm}
\end{itemize}



An intriguing observation in Figure~\ref{fig: main results: blockwise lr} is that  AdamW with BlockWise LR often starts to outperform vanilla AdamW  from the mid-to-late stages of training. This behavior resembles the WSD scheduler~\citep{wen2024understanding,hu2024minicpm}, which typically surpasses cosine or linear decay LR schedulers in the late stage (during the decay phase). Understanding the underlying cause of this phenomenon requires further investigation, which we leave for future work.



% \vspace{-.1cm}

\subsection{Ablation Studies} 

% \vspace{-.05cm}

In the preceding experiments, Blockwise LR is applied to all major blocks simultaneously. 
Here, we conduct ablation studies to assess the contribution of each   block type individually. Specifically, we pre-train  a LLaMA model (0.25B) on OpenWebText  focusing on  three comparisons: {\bf (i)} applying Blockwise LR exclusively to \Embed; {\bf (ii)} applying Blockwise LR to both \Embed\ and \FFN; {\bf (iii)} applying Blockwise LR to blocks of all the four types (\Embed, \FFN, \QK, and \VO). 
The adjusting ratios follow Eq.~\eqref{equ: tuned hyperparameters, blockwise lr} and
the results are shown in Table~\ref{tab: ablation studies}. 

% \footnote{\QK\ and \VO\ are combined because they jointly form the self-attention layers, and their total parameter count is smaller than that of the FFN layers.}  




First, the results show that applying Blockwise LR to any block consistently improves performance, supporting the hypothesis that dynamics along low-sharpness directions are crucial for loss reduction.
Among all blocks, applying Blockwise LR to \FFN\ yields the largest improvement  ($0.043-0.016=0.027$), likely because \FFN\ blocks comprise the majority of model parameters, offering the greatest potential for optimization gains.




\begin{table}[!htb]
    \centering
    % \vspace{-.05cm}
    \caption{Ablation results for the effectiveness of Blockwise LR in pre-training LLaMA (0.25B) on OpenWebText.}
    % \vspace{-.1cm}
    \begin{tabular}{cl}
   \toprule[1pt]
   \small Blockwise LR & \small terminal loss (50k steps) \\ 
   \midrule[1pt]
   \small  w/o & \small 2.834 \\      \hline
    \small \Embed & \small 2.818 (-0.016 $\text{\ding{51}}$) \\ 
    \small \Embed\ \& \FFN & \small 2.791 (-0.043 $\text{\ding{51}}$) \\
    \small \Embed\ \& \FFN\ \& \QK\ \& \VO & \small 2.784 (-0.050 $\text{\ding{51}}$) \\ \hline
    \small \LN & \small 2.837 (+0.003 $\text{\ding{55}}$) \\ % use wrong data at first
    \bottomrule[1pt]
    \end{tabular}
    \label{tab: ablation studies}
    % \vspace{-.05cm}
\end{table}

% Secondly, we conduct an additional experiment to evaluate the effect of increasing LR for \LN\ blocks. Specifically, LR for \LN\ was doubled, while LR for other blocks are kept consistent with the baseline experiment. The results, shown in the last row of Table~\ref{tab: ablation studies}, indicate a deterioration in performance. This contrasts with the positive impact observed when increasing the LR for the other four blocks, highlighting a fundamental difference in the dynamics of \LN\ compared to the other block types.


Second, we conduct an additional experiment to assess the impact of increasing the LR for \LN\ blocks. Specifically, the \LN\ LR is doubled, while the LR for other blocks remains unchanged from the baseline. As shown in the last row of Table~\ref{tab: ablation studies}, this leads to a deterioration in performance, contrasting with the improvements seen when increasing the LRs for other blocks by far more than double. This result underscores a fundamental difference in the dynamics of \LN\ with other blocks.


In summary, these ablation studies further validate the effectiveness of Blockwise LR and confirm the rationale of selecting specific types of blocks for LR amplification, as guided by the sharpness disparity principle.

% \vspace{-.1cm}
\subsection{Integration into Adam-mini}
% \vspace{-.1cm}


In practice, there are two popular directions for improving LLM pre-training: acceleration and reducing memory consumption. While Blockwise LR has demonstrated remarkable success in accelerating pre-training, a natural {\bf question} arises: {\em Can Blockwise LR be combined with memory-efficient optimizers to achieve both faster training and fewer memory consumption?}


{\bf Blockwise LR on Adam-mini.} Without loss of generality, we choose the Adam-mini~\citep{zhang2024adam} optimizer, an Adam variant  that reduces memory consumption by approximately $2\times$ compared to AdamW. Here, we conduct experiments to explore whether Blockwise LR can also accelerate Adam-mini. Following \citet{zhang2024adam}, we adopt the \texttt{lr\_max} that tuned for AdamW as the the \texttt{lr\_max} of Adam-mini. However, since Adam-mini employs SGD within each block, its dynamics differs significantly from AdamW. Consequently, for Adam-mini with Blockwise LR, we re-tune the ratios $r(\bullet)$ for $\bullet\in\{\Embed,\QK,\FFN,\VO\}$. More experimental details are provided in Appendix~\ref{appendix: experiments for adam-mini}.


\begin{figure}[!ht]
    \centering
   % \vspace{-.1cm}
    \includegraphics[width=0.3\linewidth]{figures/adam_mini_web_0_25B.pdf}
    % \hspace{-.2cm}
    \includegraphics[width=0.3\linewidth]{figures/adam_mini_pile_0_5B.pdf}
    % \vspace{-.1cm}
    \caption{Adam-mini with Blockwise LR outperforms Adam-mini in pre-training tasks.}
    \label{fig: blockwise lr on adam-mini}
    % \vspace{-.2cm}
\end{figure}


% {\bf Achieving both $2\times$ speedup and $2\times$ memory savings.}
The results, presented in Figure~\ref{fig: blockwise lr on adam-mini}, demonstrate that {\bf Blockwise LR  achieves a $2\times$ speedup on Adam-mini}. Since vanilla Adam-mini already achieves a $2\times$ memory saving compared to AdamW while maintaining  nearly the same convergence speed, Adam-mini combined with Blockwise LR achieves both a $2\times$ speedup and $2\times$ memory saving compared to vanilla AdamW. We leave more ablation studies with other optimizers for future work.


This experiment demonstrates that Blockwise LR is not limited to accelerating AdamW but can also be effectively combined with other optimizers, such as Adam-mini, while preserving their unique advantages. This finding paves the way for future research exploring the integration of Blockwise LR with other optimization algorithms.





% \vspace{-.1cm}

\section{Conclusion and Outlook}

% \vspace{-.1cm}

In this paper, we uncovered a sharpness disparity principle among different types of blocks in transformers, as formalized in Eq.~\eqref{equ: main findings}. Notably, this blockwise sharpness disparity persists throughout the entire training process, except during the initial few steps. Building on this discovery, we proposed a novel Blockwise LR adjustment principle, which effectively accelerates base optimizers such as AdamW and Adam-mini in LLM pre-training tasks.  

\textbf{Future works.}
It would be valuable to investigate the applicability of our Blockwise LR to non-LLM tasks, such as computer vision, and its compatibility with other optimizers, such as Muon~\citep{jordan2024muon} and other alloy-like architectures such as Mamba~\citep{gu2023mamba}.
Furthermore, our findings open up opportunities to develop  other block-adaptive optimization strategies, such as blockwise weight decay and gradient clipping, which could further enhance training efficiency and performance.






\section*{Acknowledgments}
Lei Wu is supported by the National Key R\&D Program of China (No.~2022YFA1008200) and National Natural Science Foundation of China (No.~12288101). 
Mingze Wang is supported by Young Scientists (PhD) Fund of the National Natural Science Foundation of China (No.~124B2028).



% \bibliography{ref}
% \bibliographystyle{plainnat}

\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock {GPT}-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee-Thorp, de~Jong, Zemlyanskiy, Lebron, and Sanghai]{ainslie-etal-2023-gqa}
Joshua Ainslie, James Lee-Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai.
\newblock {GQA}: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4895--4901, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.298}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.298/}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2024)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong, Hsieh, Lu, et~al.]{chen2024symbolic}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et~al.
\newblock Symbolic discovery of optimization algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of stability.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Cohen et~al.(2022)Cohen, Ghorbani, Krishnan, Agarwal, Medapati, Badura, Suo, Cardoze, Nado, Dahl, et~al.]{cohen2022adaptive}
Jeremy~M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George~E Dahl, et~al.
\newblock Adaptive gradient methods at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2207.14484}, 2022.

\bibitem[Cohen et~al.(2024)Cohen, Damian, Talwalkar, Kolter, and Lee]{cohen2024understanding}
Jeremy~M Cohen, Alex Damian, Ameet Talwalkar, Zico Kolter, and Jason~D Lee.
\newblock Understanding optimization in deep learning with central flows.
\newblock \emph{arXiv preprint arXiv:2410.24206}, 2024.

\bibitem[Devlin(2018)]{devlin2018bert}
Jacob Devlin.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Everett et~al.(2024)Everett, Xiao, Wortsman, Alemi, Novak, Liu, Gur, Sohl-Dickstein, Kaelbling, Lee, et~al.]{everett2024scaling}
Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander~A Alemi, Roman Novak, Peter~J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie~Pack Kaelbling, Jaehoon Lee, et~al.
\newblock Scaling exponents across parameterizations and optimizers.
\newblock \emph{arXiv preprint arXiv:2407.05872}, 2024.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {Pile}: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and Vincent]{george2018fast}
Thomas George, C{\'e}sar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent.
\newblock Fast approximate natural gradient descent in a {K}ronecker-factored eigenbasis.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Gokaslan and Cohen(2019)]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Grosse and Martens(2016)]{grosse2016kronecker}
Roger Grosse and James Martens.
\newblock A {K}ronecker-factored approximate {F}isher matrix for convolution layers.
\newblock In \emph{International Conference on Machine Learning}, pages 573--582. PMLR, 2016.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, et~al.]{hu2024minicpm}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et~al.
\newblock Minicpm: Unveiling the potential of small language models with scalable training strategies.
\newblock \emph{arXiv preprint arXiv:2404.06395}, 2024.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor, Cho, and Geras]{Jastrzebski2020The}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov, Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko, et~al.]{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Kaddour(2023)]{kaddour2023minipile}
Jean Kaddour.
\newblock The {MiniPile} challenge for data-efficient language models.
\newblock \emph{arXiv preprint arXiv:2304.08442}, 2023.

\bibitem[Karpathy(2022)]{Karpathy2022}
Andrej Karpathy.
\newblock \text{NanoGPT}.
\newblock \url{https://github.com/karpathy/nanoGPT}, 2022.

\bibitem[Keller et~al.(2024)]{jordan2024muon}
Jordan Keller et~al.
\newblock Muon optimizer.
\newblock \url{https://github.com/KellerJordan/Muon?tab=readme-ov-file}, 2024.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock {Adam}: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kunstner et~al.(2024)Kunstner, Yadav, Milligan, Schmidt, and Bietti]{kunstner2024heavy}
Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti.
\newblock Heavy-tailed class imbalance and why adam outperforms gradient descent on language models.
\newblock \emph{arXiv preprint arXiv:2402.19449}, 2024.

\bibitem[Lei~Ba et~al.(2016)Lei~Ba, Kiros, and Hinton]{lei2016layer}
Jimmy Lei~Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{ArXiv e-prints}, pages arXiv--1607, 2016.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Feng, Xue, Wang, Wu, Lu, Zhao, Deng, Zhang, Ruan, et~al.]{liu2024deepseek}
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et~al.
\newblock Deepseek-v3 technical report.
\newblock \emph{arXiv preprint arXiv:2412.19437}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Li, Hall, Liang, and Ma]{liu2023sophia}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language model pre-training.
\newblock \emph{International Conference on Learning Representations}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with {K}ronecker-factored approximate curvature.
\newblock In \emph{International conference on machine learning}, pages 2408--2417. PMLR, 2015.

\bibitem[Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao]{mi2022make}
Peng Mi, Li~Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and Dacheng Tao.
\newblock Make sharpness-aware minimization stronger: A sparsified perturbation approach.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30950--30962, 2022.

\bibitem[Ormaniec et~al.(2024)Ormaniec, Dangel, and Singh]{ormaniec2024does}
Weronika Ormaniec, Felix Dangel, and Sidak~Pal Singh.
\newblock What does it mean to be a transformer? insights from a theoretical hessian analysis.
\newblock \emph{arXiv preprint arXiv:2410.10986}, 2024.

\bibitem[Pesme and Flammarion(2023)]{pesme2023saddle}
Scott Pesme and Nicolas Flammarion.
\newblock Saddle-to-saddle dynamics in diagonal linear networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Popel and Bojar(2018)]{popel2018training}
Martin Popel and Ond{\v{r}}ej Bojar.
\newblock Training tips for the transformer model.
\newblock \emph{arXiv preprint arXiv:1804.00247}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Shin et~al.(2024)Shin, Kim, and Moon]{shin2024initializing}
Kwang~Yong Shin, Suhyun Kim, and Soo-Mook Moon.
\newblock Initializing the layer-wise learning rate, 2024.
\newblock URL \url{https://openreview.net/forum?id=mSSi0zYkEA}.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Song et~al.(2024)Song, Ahn, and Yun]{song2024does}
Minhak Song, Kwangjun Ahn, and Chulhee Yun.
\newblock Does sgd really happen in tiny subspaces?
\newblock \emph{arXiv preprint arXiv:2405.16002}, 2024.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, Katie Millican, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023LLaMA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vyas et~al.(2024)Vyas, Morwani, Zhao, Shapira, Brandfonbrener, Janson, and Kakade]{vyas2024soap}
Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade.
\newblock Soap: Improving and stabilizing shampoo using adam.
\newblock \emph{arXiv preprint arXiv:2409.11321}, 2024.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Wang, He, Wang, Huang, Xiong, Li, Wu, et~al.]{wang2024improving}
Mingze Wang, Jinbo Wang, Haotian He, Zilin Wang, Guanhua Huang, Feiyu Xiong, Zhiyu Li, Lei Wu, et~al.
\newblock Improving generalization and convergence by enhancing implicit regularization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})]{wang2024understanding}
Mingze Wang et~al.
\newblock Understanding the expressive power and mechanisms of transformer for sequence modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 2024{\natexlab{b}}.

\bibitem[Wen et~al.(2024)Wen, Li, Wang, Hall, Liang, and Ma]{wen2024understanding}
Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, and Tengyu Ma.
\newblock Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:2410.05192}, 2024.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Wu et~al.(2018)Wu, Ma, and E]{wu2018sgd}
Lei Wu, Chao Ma, and Weinan E.
\newblock How {SGD} selects the global minima in over-parameterized learning: A dynamical stability perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 31:\penalty0 8279--8288, 2018.

\bibitem[Xie et~al.(2022)Xie, Zhou, Li, Lin, and Yan]{xie2022adan}
Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan.
\newblock Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models.
\newblock \emph{arXiv preprint arXiv:2208.06677}, 2022.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages 10524--10533. PMLR, 2020.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Yang(2019)]{yang2019xlnet}
Zhilin Yang.
\newblock Xlnet: Generalized autoregressive pretraining for language understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}, 2019.

\bibitem[Yuan et~al.(2024)Yuan, Liu, Wu, Zhou, and Gu]{yuan2024mars}
Huizhuo Yuan, Yifeng Liu, Shuang Wu, Xun Zhou, and Quanquan Gu.
\newblock Mars: Unleashing the power of variance reduction for training large models.
\newblock \emph{arXiv preprint arXiv:2411.10438}, 2024.

\bibitem[Zhang and Sennrich(2019)]{zhang2019root}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15383--15393, 2020.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Zeng, Wang, and Lu]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
\newblock Tinyllama: An open-source small language model, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Chen, Ding, Li, Sun, and Luo]{zhang2024transformers}
Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan Luo.
\newblock Why transformers need adam: A hessian perspective.
\newblock \emph{arXiv preprint arXiv:2402.16788}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{c}})Zhang, Chen, Li, Ding, Wu, Ye, Luo, and Sun]{zhang2024adam}
Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun.
\newblock Adam-mini: Use fewer learning rates to gain more.
\newblock \emph{arXiv preprint arXiv:2406.16793}, 2024{\natexlab{c}}.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.
\newblock Galore: Memory-efficient llm training by gradient low-rank projection.
\newblock \emph{arXiv preprint arXiv:2403.03507}, 2024.

\end{thebibliography}





\newpage

\appendix

\begin{center}
    \noindent\rule{\textwidth}{4pt} \vspace{-0.2cm}
    \LARGE \textbf{Appendix} % \\ ~\\[-0.5cm]
    \noindent\rule{\textwidth}{1.2pt}
\end{center}
% \wl{why new page here and also for different sections?}

\startcontents[sections]
\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}


\input{appendix}



\end{document}