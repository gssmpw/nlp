\section{Related Works}
\label{section: related works}

% \vspace{-.1cm}

% {\bf Hessian structure of transformers.}
% Li, "Hessian-based Analysis on Transformer Networks"
% provide empirical evidence that the \SA in vision transformers leads to a more non-convex but smoother loss landscape than that of CNNs. 
% Zhou et al., "Understanding Hessian Blocks in Transformers"
% empirically demonstrated that Hessian blocks in transformers exhibit much more  disparity than CNNs, suggesting that this could be a potential cause for the reliance on adaptive optimizers. However, their work does not provide a clear principle regarding the sharpness distinction across different transformer blocks.





{\bf Sharpness structures in transformers.} Recent work has started to investigate blockwise sharpness patterns in transformer models through Hessian-based analyses.
For example, Chen et al., "Heterogeneity of Transformer Blocks" 
empirically observed the sharpness' blockwise heterogeneity  but did not establish a clear principle regarding the sharpness disparity among different blocks. Meanwhile, Wang et al., "Hessian Analysis for Self-Attention Layers" 
provided a Hessian analysis for a single self-attention (\SA) layer, focusing only on the  sharpness disparity between the query-key (\QK) and value-output (\VO) blocks within the same layer. 

% \vspace{-.05cm}

In contrast, we examine sharpness at the block-type level across the entire transformer architecture, rather than focusing on individual blocks (as in Zhang et al., "Sharpness Patterns in Transformers") or a single layer (as in Liang et al., "Hessian Analysis for Transformer Layers"). This coarse-grained perspective reveals a consistent  disparity, as formalized by \textbf{Principle}~\eqref{equ: main findings},  which persists throughout most of the training processâ€”except during the initial steps.

% \vspace{-.05cm}

{\bf Efficient optimizers for LLM pre-training.} AdamW (Adam with decoupled weight decay) Zhang et al., "AdamW: A Decoupled Weight Decay Regularization" 
has become the default optimizer in LLM pre-training. 
Efforts to design more efficient optimizers generally fall into two main categories: accelerating convergence and reducing memory footprint.
Accelerations have been developed using techniques such as Nesterov momentum Li et al., "Nesterov Accelerated Gradient for Low-Rank Matrix Completion" , diagonal second-order estimates Chen et al., "Diagonal Second-Order Estimates for Stochastic Optimization" , variance reduction Zhang et al., "Variance Reduction in Stochastic Optimization" , and matrix-based preconditioners Wang et al., "Matrix-Based Preconditioners for Stochastic Optimization" . 
Memory-efficient optimizers utilize sign-based methods Liang et al., "Sign-Based Methods for Memory-Efficient Optimizers" , reduced usage of second moments in Adam Zhang et al., "Reduced Second Moments in Adam" , and gradient low-rank projection Wang et al., "Gradient Low-Rank Projection for Efficient Optimization" .
The closest work to our Blockwise LR is Li et al., "Efficient Pre-Training with Adaptive Learning Rates", which also increases the LR along low-sharpness directions. A detailed comparison is deferred to Section~\ref{section: application}.

% \vspace{-.05cm}

{\bf The edge of stability (EoS) phenomenon.} Neural network training typically occurs at the EoS stage Zhang et al., "The Edge of Stability in Deep Learning" , where the optimizer exhibits oscillatory behavior along sharp directions without diverging, while steadily progressing along flat directions, leading to loss reduction. Several works Liang et al., "Dynamics Along Flat Directions for Efficient Optimization"  have highlighted the crucial role of the dynamics along flat directions (referred to as river directions by Wang et al., "River Directions in Deep Learning" , bulk directions by Chen et al., "Bulk Directions for Optimal Pre-Training" , and stable direction in Li et al., "Stable Directions for Efficient Optimization" ) in reducing total loss. Notably, Zhang et al., "River Dynamics for LLM Pre-Training" further demonstrated that this picture is essential for understanding LLM pre-training. Building on these insights, our Blockwise LR approach is designed to accelerate training by amplifying the dynamics particularly along the flat river directions.
% For theoretical explanations of this fascinating phenomenon, we refer to recent studies Wang et al., "Theoretical Foundations of Edge of Stability" . 






% Inspired by EoS, Zhang et al., "Efficient Optimization with Adaptive Learning Rates" proposed an optimizer that accelerates LLM pre-training by increasing the LRs along low-sharpness directions.  


% \vspace{-.2cm}