\section{Related Works}
\label{section: related works}

% \vspace{-.1cm}

% {\bf Hessian structure of transformers.}
% ____ provide empirical evidence that the \SA in vision transformers leads to a more non-convex but smoother loss landscape than that of CNNs. 
% ____ empirically demonstrated that Hessian blocks in transformers exhibit much more  disparity than CNNs, suggesting that this could be a potential cause for the reliance on adaptive optimizers. However, their work does not provide a clear principle regarding the sharpness distinction across different transformer blocks.





{\bf Sharpness structures in transformers.} Recent work has started to investigate blockwise sharpness patterns in transformer models through Hessian-based analyses.
For example,____ empirically observed the sharpness' blockwise heterogeneity  but did not establish a clear principle regarding the sharpness disparity among different blocks. Meanwhile, ____ provided a Hessian analysis for a single self-attention (\SA) layer, focusing only on the  sharpness disparity between the query-key (\QK) and value-output (\VO) blocks within the same layer. 

% \vspace{-.05cm}

In contrast, we examine sharpness at the block-type level across the entire transformer architecture, rather than focusing on individual blocks (as in ____) or a single layer (as in ____). This coarse-grained perspective reveals a consistent  disparity, as formalized by \textbf{Principle}~\eqref{equ: main findings},  which persists throughout most of the training processâ€”except during the initial steps.

% \vspace{-.05cm}

{\bf Efficient optimizers for LLM pre-training.} AdamW (Adam with decoupled weight decay)____ has become the default optimizer in LLM pre-training. 
Efforts to design more efficient optimizers generally fall into two main categories: accelerating convergence and reducing memory footprint.
Accelerations have been developed using techniques such as Nesterov momentum____, diagonal second-order estimates____, variance reduction____, and matrix-based preconditioners____. 
Memory-efficient optimizers utilize sign-based methods____, reduced usage of second moments in Adam____, and gradient low-rank projection____.
The closest work to our Blockwise LR is____, which also increases the LR along low-sharpness directions. A detailed comparison is deferred to Section~\ref{section: application}.

% \vspace{-.05cm}

{\bf The edge of stability (EoS) phenomenon.} Neural network training typically occurs at the EoS stage____, where the optimizer exhibits oscillatory behavior along sharp directions without diverging, while steadily progressing along flat directions, leading to loss reduction. Several works ____ have highlighted the crucial role of the dynamics along flat directions (referred to as river directions by ____, bulk directions by ____, and stable direction in ____) in reducing total loss. Notably, ____ further demonstrated that this picture is essential for understanding LLM pre-training. Building on these insights, our Blockwise LR approach is designed to accelerate training by amplifying the dynamics particularly along the flat river directions.
% For theoretical explanations of this fascinating phenomenon, we refer to recent studies____. 




% Inspired by EoS, ____ proposed an optimizer that accelerates LLM pre-training by increasing the LRs along low-sharpness directions.  


% \vspace{-.2cm}