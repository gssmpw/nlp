The first result is to evaluate the effectiveness of the TAP in automatically extracting SDL labels from trajectory data. 
Specifically, we evaluate if the cross-entropy optimization can effectively learn the values for thresholds for yaw rate (\(\Theta_\omega\)), acceleration (\(\Theta_a\)), and velocity (\(\Theta_v\)) from the corresponding data distributions. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/CEMResults.pdf}
   \caption{(a) Data distributions with initial seeds shown as dotted lines. (b) Initial subcategory averages for each seed. (c) Objective function  $J(\Theta)$ convergence per epoch. (d) Final subcategory averages after optimization; some skew remains due to inherent data imbalances, but all seeds converge to the same thresholds}
    \label{fig:cem_results}
\end{figure}

% \begin{table}[b]
% \centering
% \begin{tabular}{ |c|c|c| }
%  \hline
%  Objective & $\epsilon$ & $\eta$ \\
%  \hline
%  $J(\Theta_{a})$ & 0.05 & 0.05 \\
%  $J(\Theta_{v})$ & 0.05 & 0.2 \\
%  $J(\Theta_{\omega})$ & 0.005 & 0.01 \\
%  \hline
% \end{tabular}
% \caption{The hyper-parameters $\epsilon$ and learning rate used for each category of the cross-entropy method.}
% \label{table:hyperparameters}
% \end{table}

Sensor noise in trajectory data complicates distinguishing between truly stopped vehicles and those moving at very low speeds.
Setting the stop threshold $\theta_{v,\text{stop}}$ too low risks misclassifying stationary vehicles, while a threshold set too high may incorrectly label slow-moving vehicles as stopped. 
To mitigate this, we manually define $\theta_{v,\text{stop}} = 0.1 m/s$, reducing the impact of sensor fluctuations while maintaining accurate classification. 
The remaining thresholds in $\Theta$ are optimized using the cross-entropy method.
Figure~\ref{fig:cem_results} presents the results of threshold optimization, with Figure~\ref{fig:cem_results}(a), showing the 1-second data distributions for $D_a$, $D_v$, and $D_{\omega}$.
%To run the cross-entropy method, it is essential to begin with an initial threshold value $\Theta_0$ as a seed.

\noindent \textbf{Hyperparameters and Ablation:}
The cross-entropy method requires initial threshold seeds $\Theta^1_0, \Theta^2_0, \Theta^3_0$ to ensure effective convergence.
% We selected these seeds by analyzing the data distribution as well as using domain knowledge to achieve a balanced dataset partitioning starting point guess for cross-entropy.
We selected these seeds by analyzing the data distribution as well as using domain knowledge to achieve a starting point guess at balanced dataset partitions for cross-entropy.
To enhance robustness and avoid local minima, three distinct seeds were used for each threshold.
This approach improves data separability and ensures more reliable SDL extraction. 
The hyperparameters are listed in Table~\ref{table:hyperparameters}.
Figure~\ref{fig:cem_results}(a) also shows these initial threshold guesses plotted for their respective distributions.
These thresholds define initial intra-partition separation values (Figure~\ref{fig:cem_results}(b)), $\mu_{part}$, which measure the distance between samples within each partition with higher values indicating greater separation. 

\noindent \textbf{Evaluation:}
The cross-entropy method aims to optimize these thresholds for balanced separation across partitions.
Figure~\ref{fig:cem_results}(c) illustrates the convergence of the objective function \( J(\Theta) \) over multiple epochs, with similar final values for each seed indicating optimal thresholds. 
As \( J(\Theta) \) decreases, it demonstrates effective learning by the method. 
Figure~\ref{fig:cem_results}(d) shows the final intra-partition separations, where $\mu_{part}$ is more balanced across partitions compared to the initial values (Figure~\ref{fig:cem_results}(b)). 
% The lateral thresholds of $\Theta_{\omega}$ converged to values of $\{\theta_{\omega,\text{str}} = 0.0283,\theta_{\omega,\text{grad}} = 0.0754,\theta_{\omega,\text{med}} = 0.1541\}$ rad/s. The longitudinal thresholds converged to $\Theta_{a} = \{\theta_{a,\text{dec}} = -1.3715,\theta_{a,\text{acc}} = 1.5557\}$ m/s$^2$, and $\Theta_{v} = \{\theta_{v,\text{stop}} = 0.1, \theta_{v,\text{slow}} = 10.2140,\theta_{v,\text{med}} = 24.4046\}$ m/s.
The lateral thresholds converged to values of $\Theta_{\omega} = \{0.0283, 0.0754, 0.1541\}$ rad/s. The longitudinal thresholds converged to $\Theta_{a} = \{-1.3715, 1.5557\}$ m/s$^2$ and $\Theta_{v} = \{0.1, 10.2140, 24.4046\}$ m/s.
Consistent convergence across all seeds confirms the cross-entropy methodâ€™s reliability in optimizing partition balance. The complete table of initial seeds and converged values is included in Appendix~\ref{sec:appendix_ce_seeds}.
% Table~\ref{table:threshold_results} presents the final values of  $J(\Theta)$ after the optimization. Notably, the convergence of all seeds to the same final value suggests that the cross-entropy method consistently identifies the thresholds that best partition the data.
\begin{table}[b]
\centering
\renewcommand{\arraystretch}{1}
\begin{tabular}{ |c|c|c|c| }
 \hline
 \noalign{\vspace{0.1em}} 
$\downarrow$ \textbf{Hyperparameter} / Objective $\rightarrow$ & $\hat{J(\Theta_{a})}$ & $\hat{J(\Theta_{v})}$ & $\hat{J(\Theta_{\omega})}$ \\
 \hline
 \textbf{$\epsilon$} & 0.05 & 0.05 & 0.005 \\
\hline
 \textbf{$\eta$} & 0.05 & 0.2 & 0.01 \\
 \hline
\end{tabular}
\caption{The hyper-parameters $\epsilon$ and learning rate used for each category of the cross-entropy method.}
\label{table:hyperparameters}
\end{table}
% \begin{table}[b]
% \centering
% \begin{tabular}{ |cc|c|c|c|c| }
%  \hline
%  % Threshold & Seed 1 & Seed 2 & Seed 3 & Final Value \\
% \multicolumn{2}{|c|}{Threshold} & Seed 1 & Seed 2 & Seed 3 & Final Value \\
%  \hline
%  $\Theta_{a,\text{dec}}$ & m/s$^2$ & -0.5 & -1.0 & -2.0 & -1.3715 \\
%  $\Theta_{a,\text{acc}}$ & & 0.5 & 1.0 & 2.0 & 1.5557 \\
%  \hline
%  $\Theta_{v,\text{slow}}$ & m/s & 7.0 & 10.0 & 12.0 & 10.2140 \\
%  $\Theta_{v,\text{med}}$ & & 22.0 & 24.0 & 26.0 & 24.4046 \\
%  \hline
%  $\Theta_{\omega,\text{str}}$ & rad/s & 0.01 & 0.02 & 0.05 & 0.0283 \\
%  $\Theta_{\omega,\text{grad}}$ & & 0.05 & 0.07 & 0.1 & 0.0754 \\
%  $\Theta_{\omega,\text{med}}$ & & 0.1 & 0.15 & 0.2 & 0.1541 \\
%  \hline
% \end{tabular}
% \caption{Initial seeds and final optimized thresholds. 
% %In each case, Seed 1 and Seed 3 are on either side of the final threshold. This implies that the final thresholds calculated by the cross-entropy method are the ones that best partition the data.
% }
% \label{table:threshold_results}
% \end{table}

\begin{comment}
To demonstrate that the final thresholds result in more balanced partitions than the initial seeds, we analyzed a representative sample from the acceleration data distribution  $D_a$. 
For each partition (Decelerate, Maintain Speed, Accelerate), we uniformly selected 50 samples. 
Figure~\ref{fig:heatmap} shows a heatmap comparing the distances of these samples from their partition mean, where green indicates values close to the mean and red indicates greater deviations. 
The top row represents the initial seeds, while the bottom row shows the optimized thresholds. 
The final thresholds result in more data points being closer to the mean, particularly in the Decelerate and Accelerate partitions, as indicated by the increase in green cells. 
This further confirms that the cross-entropy method produces more balanced partitions, enhancing the accuracy of SDL extraction.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap.pdf}
    \caption{Heatmap of acceleration data showing the distance from the mean for each partition (Decelerate, Maintain Speed, Accelerate). Top row: initial seeds. Bottom row: optimized thresholds. More green cells in the bottom row indicate that the final thresholds partitions have data closer to their mean}
    \label{fig:heatmap}
\end{figure}
\end{comment}
