% Our work stands at the crossroads of scenario description languages, scene understanding, and trajectory similarity.
% We will discuss each topic individually and highlight how our paper differs from previous work.
Our research intersects with scenario description languages, scene understanding, and trajectory similarity. 
%In this section, we discuss each of these areas and highlight how our approach advances the current state of the art.

%\subsection{Scenario Description Languages}
\noindent \textbf{Scenario Description Languages}
% Scenario Desecription Languages (SDLs) are high-level embeddings for scenarios.
% There are many existing SDLs, such as Fortellix's Measurable Scenario Description Language~\cite{M-SDL}, Scenic~\cite{Scenic}, and OpenScenario~\cite{OpenScenario}.
% These SDLs are used to describe an entire scenario including things like pedestrians and traffic lights.
% For our work, we want to compare vehicle behavior, so we will use an SDL that focuses on describing vehicle motion.
% Methods have been proposed that use SDLs to measure scenario similarity, such as Scenario2Vector~\cite{aron_date24,aron_iccps21}.
% The novelty of our approach is in the pipeline used for automatic SDL extraction.
Scenario Description Languages (SDLs) serve as high-level embeddings to describe traffic scenarios, capturing key elements like vehicles, pedestrians, and traffic lights. Existing SDLs such as Fortellix’s MSDL~\cite{M-SDL}, Scenic~\cite{Scenic}, and OpenScenario~\cite{OpenScenario} focus on comprehensive scene descriptions of environment, road geometry etc.
However, our work diverges by concentrating specifically on describing autonomous vehicle motion to facilitate behavior comparison.
Previous methods, such as Scenario2Vector~\cite{aron_date24, 
aron_iccps21}, have explored using SDLs for scenario similarity using video and natural language data. 
In contrast, our approach differs by introducing an automated pipeline for extracting vehicle motion - focused SDL labels, enhancing scalability and precision.
Alternative approaches include graph-based representations like 
Roadscene2vec and GraphAD~\cite{Roadscene2vec, GraphAD, PreGSU}, 
which model scenes using nodes for agents and edges for their 
interactions. 
While effective for capturing vehicle interactions, these methods struggle to accurately represent vehicle motion dynamics.
% Another approach to scenario embeddings is to use a graph representation~\cite{Roadscene2vec,GraphAD,PreGSU}.
% This graph-based approach depicts agents and the environment as nodes and relationships as edges to model scenes.
% This kind of representation works well for modeling vehicle interaction and can be analyzed easily for similarity~\cite{scenegraph_sim}, but does not capture information about the actual motion of the vehicle well.
Recently, natural language descriptions~\cite{scene_understanding_captions, 
explainable_cars} have been used to capture complex scene interactions. 
Although expressive, this approach requires extensive training data and includes extraneous linguistic elements, making it less efficient for scenario comparison. 
In contrast, our method achieves a high information density while maintaining interpretability, focusing on the essential motion characteristics relevant to vehicle behavior.

% An increasingly common approach is to describe the scene using natural language~\cite{scene_understanding_captions,explainable_cars}.
% Natural language has the advantage of being able to convey very complex actions.
% However, it requires a lot of training data, and includes unnecessary grammar that wastes information and makes it difficult to compare embeddings.
% Instead, we want an approach that both communicates complex interactions and contains a high information density.

\noindent \textbf{Scene Understanding}
Scene understanding is a vast field within computer vision, of which traffic scene understanding is a small portion.
In \cite{nlp_video_retrieval1}, researchers use dashcam video for scenario retrieval.
Other scene understanding work~\cite{SpatialCNN_SceneUnderstanding,RoadSceneUnderstanding,Vid2Bev} focuses on modeling the relationship between road agents and their environment features like lanes, etc.  
For instance, Vid2Bev~\cite{Vid2Bev} converts camera footage into a bird’s-eye view, enabling AVs to create a vectorized world for planning and control.
These algorithms are important for AVs to create a vectorized view of the world in which their planning and control modules operate.
Since AV planning and control modules already operate in this vectorized space, we leverage the existing data available to the vehicle. This allows us to analyze vehicle behavior as perceived by the vehicle itself, rather than relying on camera-based perception. Thus, our focus is on analyzing vectorized vehicle data, bypassing the need to re-implement perception algorithms.
% Because AV planning and control modules operate in this vectorized world, we can assume that they already have this data.
% As a result, we are able to analyze vehicle behavior as the vehicle sees the world, rather than from the viewpoint of a camera.
% Therefore, this paper will focus on providing analysis based on the vectorized vehicle data, rather than spending resources on re-implementing perception algorithms.

% Scene understanding is often used with dashcam video for scenario similarity tasks~\cite{nlp_video_retrieval1}.
% Our work looks at scenario similarity using vectorized vehicle data.


\noindent \textbf{Trajectory Similarity}
% In the vectorized worldview used by AVs, vehicle behavior is often represented by a trajectory showing the history of its positions.
% % A wide variety of methods for comparing position histories have been proposed~\cite{}
% Many methods exist for comparing trajectory similarity~\cite{traj_sim_overview1,traj_sim_overview2}.
% Traditional methods include Average Displacement Error (ADE)~\cite{ADE} and Dynamic Time Warping (DTW)~\cite{DTW}, and more recently deep learning methods~\cite{dnn_traj_sim} have been used for trajectory similarity.
% ADE and DTW are traditional methods that compare trajectory similarity by calculating the distance between corresponding points in the trajectories.
% These methods are not as effective on data of different sizes, and require re-sampling or padding to make the trajectories equivalent length.
% However, for our work these methods of resizing the trajectory would remove vital information about vehicle velocity and acceleration.
% The deep learning methods are more robust to trajectories of different sizes, but are still vulnerable to early deviations which can cause large distances as the trajectories diverge.
% We want to identify similar behavior, not merely similar paths.
% In this paper we will compare to our method against ADE.
In the vectorized world used by AVs, vehicle behavior is represented by trajectories of positional history. 
Traditional methods for trajectory similarity, such as Average Displacement Error (ADE)~\cite{ADE} and Dynamic Time Warping (DTW)~\cite{DTW}, compare distances between corresponding points. 
However, these methods struggle with varying trajectory lengths, requiring resampling, which can strip away crucial information. 
Deep learning methods~\cite{dnn_traj_sim} are more robust to length differences but remain sensitive to early deviations, leading to accumulated errors. 
In contrast, our Trajectory-to-Action Pipeline (TAP) uses Scenario Description Language (SDL) labels to capture interpretable action-level behaviors, preserving essential motion characteristics and ensuring robustness to trajectory variations. 

