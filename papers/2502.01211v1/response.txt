\section{Related Work}
\label{sec:related-work}

The proposed PS are intended for addressing a (presumed to be) non-neutral status quo.
We position our work relative to other fairML works that aim at changing such status quo by implementing, according to **Calmon et al., "Fairness in Learning"**, ``bias-transforming'' methods **Hardt et al., "Equality of Opportunity in Supervised Learning"**.
**Dwork et al., "A Study of Fairness and Bias in AI Models"** famously introduce counterfactual fairness, which compares the factual distribution to its counterfactual counterpart **Kusner et al., "Counterfactual Fairness"** in which the downstream influence of the PA is accounted for all other attributes.
**Feldman et al., "Certifying and Removing Disparate Impact"** and **Hardt et al., "Equality of Opportunity in Supervised Learning"**, respectively, test for individual discrimination by comparing the observed profiles to generated ones in which the seemingly neutral attributes are updated conditional on the effect of the PA.
In terms of fairML methods, our work relates mainly to pre-processing methods such as those proposed by **Calmon et al., "Fairness in Learning"** and **Hardt et al., "Equality of Opportunity in Supervised Learning"**.
We differ from all these works by explicitly formulating what drives the non-neutrality of the status quo in the form of privilege.

Noteworthy are the ongoing fairML discussions on achieving long-term fairness **Kearns et al., "Fairness and Bias in Algorithmic Decision-Making"** and addressing the accuracy-fairness trade-off  **Dwork et al., "A Study of Fairness and Bias in AI Models"**, respectively.
The former argues that fairness interventions need to consider their impact over time. 
The latter argues that the trade-off is trivial as long as the data used for training is biased. 
Together with **Hardt et al., "Equality of Opportunity in Supervised Learning"** and its bias-preserving versus bias-transforming distinction, 
both discussions are examples of a more explicit direction within fairML of using algorithmic tools to intervene in the status quo.
Our work adds privilege to the discussion, viewing it as a consequence of the non-neutral status quo.