
@article{dandl_multi-objective_2020,
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	url = {http://link.springer.com/10.1007/978-3-030-58112-1_31},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of ‘what-if scenarios’. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally diﬃcult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-oﬀs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	language = {en},
	urldate = {2022-02-16},
	journal = {Parallel Problem Solving from Nature – PPSN XVI},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	year = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {448--469},
	file = {Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:/Users/ludwigbothmann/Zotero/storage/VML4W8KJ/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:application/pdf},
}

@inproceedings{kim_multiaccuracy_2019,
	address = {New York, NY, USA},
	title = {Multiaccuracy: {Black}-box {Post}-processing for {Fairness} in {Classification}},
	doi = {10.1145/3306618.3314287},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Michael P and Ghorbani, Amirata and Zou, James},
	year = {2019},
	pages = {247--254},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/QD9NTH6R/Kim et al. - 2018 - Multiaccuracy Black-Box Post-Processing for Fairn.pdf:application/pdf},
}

@inproceedings{dwork_fairness_2012,
	address = {New York, NY, USA},
	title = {Fairness {Through} {Awareness}},
	doi = {10.1145/2090236.2090255},
	booktitle = {Proceedings of the 3rd {Innovations} in {Theoretical} {Computer} {Science} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	year = {2012},
	pages = {214--226},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/ZIMF4AEZ/Dwork et al. - 2011 - Fairness Through Awareness.pdf:application/pdf},
}

@inproceedings{hebert-johnson_multicalibration_2018,
	title = {Multicalibration: {Calibration} for the ({Computationally}-{Identifiable}) {Masses}},
	url = {https://proceedings.mlr.press/v80/hebert-johnson18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hebert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
	editor = {Dy, Jennifer and Krause, Andreas},
	year = {2018},
	pages = {1939--1948},
	file = {Hébert-Johnson et al. - Multicalibration Calibration for the (Computation.pdf:/Users/ludwigbothmann/Zotero/storage/JLT3V73Z/Hébert-Johnson et al. - Multicalibration Calibration for the (Computation.pdf:application/pdf},
}

@inproceedings{kusner_counterfactual_2017,
	title = {Counterfactual {Fairness}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	file = {Kusner et al. - Counterfactual Fairness.pdf:/Users/ludwigbothmann/Zotero/storage/S6WBBP7N/Kusner et al. - Counterfactual Fairness.pdf:application/pdf},
}

@inproceedings{kleinberg_inherent_2017,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Inherent {Trade}-{Offs} in the {Fair} {Determination} of {Risk} {Scores}},
	volume = {67},
	doi = {10.4230/LIPIcs.ITCS.2017.43},
url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2017.43},
	booktitle = {8th {Innovations} in {Theoretical} {Computer} {Science} {Conference} ({ITCS} 2017)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	editor = {Papadimitriou, Christos H.},
	year = {2017},
	pages = {43:1--43:23},
}

@inproceedings{chikahara_learning_2021,
	title = {Learning {Individually} {Fair} {Classifier} with {Path}-{Specific} {Causal}-{Effect} {Constraint}},
	url = {https://proceedings.mlr.press/v130/chikahara21a.html},
	abstract = {Machine learning is used to make decisions for individuals in various fields, which require us to achieve good prediction accuracy while ensuring fairness with respect to sensitive features (e.g., race and gender). This problem, however, remains difficult in complex real-world scenarios. To quantify unfairness under such situations, existing methods utilize path-specific causal effects. However, none of them can ensure fairness for each individual without making impractical functional assumptions about the data. In this paper, we propose a far more practical framework for learning an individually fair classifier. To avoid restrictive functional assumptions, we define the probability of individual unfairness (PIU) and solve an optimization problem where PIU’s upper bound, which can be estimated from data, is controlled to be close to zero. We elucidate why our method can guarantee fairness for each individual. Experimental results show that our method can learn an individually fair classifier at a slight cost of accuracy.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Chikahara, Yoichi and Sakaue, Shinsaku and Fujino, Akinori and Kashima, Hisashi},
	year = {2021},
	pages = {145--153},
	file = {chikahara21a.pdf:/Users/ludwigbothmann/Zotero/storage/WA4QKQP9/chikahara21a.pdf:application/pdf;Full Text PDF:/Users/ludwigbothmann/Zotero/storage/2IC6MEFX/Chikahara et al. - 2021 - Learning Individually Fair Classifier with Path-Sp.pdf:application/pdf;Supplementary PDF:/Users/ludwigbothmann/Zotero/storage/V37AM9XX/Chikahara et al. - 2021 - Learning Individually Fair Classifier with Path-Sp.pdf:application/pdf},
}

@inproceedings{pan_explaining_2021,
	address = {Virtual Event Singapore},
	title = {Explaining {Algorithmic} {Fairness} {Through} {Fairness}-{Aware} {Causal} {Path} {Decomposition}},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467258},
	doi = {10.1145/3447548.3467258},
	abstract = {Algorithmic fairness has aroused considerable interests in data mining and machine learning communities recently. So far the existing research has been mostly focusing on the development of quantitative metrics to measure algorithm disparities across different protected groups, and approaches for adjusting the algorithm output to reduce such disparities. In this paper, we propose to study the problem of identification of the source of model disparities. Unlike existing interpretation methods which typically learn feature importance, we consider the causal relationships among feature variables and propose a novel framework to decompose the disparity into the sum of contributions from fairness-aware causal paths, which are paths linking the sensitive attribute and the final predictions, on the graph. We also consider the scenario when the directions on certain edges within those paths cannot be determined. Our framework is also model agnostic and applicable to a variety of quantitative disparity measures. Empirical evaluations on both synthetic and real-world data sets are provided to show that our method can provide precise and comprehensive explanations to the model disparities.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Pan, Weishen and Cui, Sen and Bian, Jiang and Zhang, Changshui and Wang, Fei},
	month = aug,
	year = {2021},
	pages = {1287--1297},
	file = {Pan et al. - 2021 - Explaining Algorithmic Fairness Through Fairness-A.pdf:/Users/ludwigbothmann/Zotero/storage/Y9Y5GENW/Pan et al. - 2021 - Explaining Algorithmic Fairness Through Fairness-A.pdf:application/pdf},
}

@inproceedings{calmon_optimized_2017,
	title = {Optimized {Pre}-{Processing} for {Discrimination} {Prevention}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html},
	abstract = {Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.},
	urldate = {2022-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
	year = {2017},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/QQ3YHIPU/Calmon et al. - 2017 - Optimized Pre-Processing for Discrimination Preven.pdf:application/pdf},
}

@inproceedings{nabi_learning_2019,
	title = {Learning {Optimal} {Fair} {Policies}},
	url = {https://proceedings.mlr.press/v97/nabi19a.html},
	abstract = {Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi \& Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4674--4682},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/T5WJNJYF/Nabi et al. - 2019 - Learning Optimal Fair Policies.pdf:application/pdf;Supplementary PDF:/Users/ludwigbothmann/Zotero/storage/T6VUCCNJ/Nabi et al. - 2019 - Learning Optimal Fair Policies.pdf:application/pdf},
}

@inproceedings{nabi_optimal_2022,
	title = {Optimal {Training} of {Fair} {Predictive} {Models}},
	url = {https://proceedings.mlr.press/v177/nabi22a.html},
	abstract = {Recently there has been sustained interest in modifying prediction algorithms to satisfy fairness constraints. These constraints are typically complex nonlinear functionals of the observed data distribution. Focusing on the path-specific causal constraints, we introduce new theoretical results and optimization techniques to make model training easier and more accurate. Specifically, we show how to reparameterize the observed data likelihood such that fairness constraints correspond directly to parameters that appear in the likelihood, transforming a complex constrained optimization objective into a simple optimization problem with box constraints. We also exploit methods from empirical likelihood theory in statistics to improve predictive performance by constraining baseline covariates, without requiring parametric models. We combine the merits of both proposals to optimize a hybrid reparameterized likelihood. The techniques presented here should be applicable more broadly to fair prediction proposals that impose constraints on predictive models.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Proceedings of the {First} {Conference} on {Causal} {Learning} and {Reasoning}},
	publisher = {PMLR},
	author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {594--617},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/FYWXN2X4/Nabi et al. - 2022 - Optimal Training of Fair Predictive Models.pdf:application/pdf},
}

@inproceedings{nabi_fair_2018,
	address = {New Orleans, Louisiana, USA},
	series = {{AAAI}'18/{IAAI}'18/{EAAI}'18},
	title = {Fair inference on outcomes},
	doi = {10.5555/3504035.3504270},
	abstract = {In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are "sensitive," in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.},
	urldate = {2023-01-31},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirtieth} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference} and {Eighth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Nabi, Razieh and Shpitser, Ilya},
	year = {2018},
	pages = {1931--1940},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/US5XDTJA/Nabi und Shpitser - 2018 - Fair inference on outcomes.pdf:application/pdf},
}

@article{plecko_fair_2020,
	title = {Fair {Data} {Adaptation} with {Quantile} {Preservation}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/19-966.html},
	abstract = {Fairness of classification and regression has received much attention recently and various, partially non-compatible, criteria have been proposed. The fairness criteria can be enforced for a given classifier or, alternatively, the data can be adapted to ensure that every classifier trained on the data will adhere to desired fairness criteria. We present a practical data adaption method based on quantile preservation in causal structural equation models. The data adaptation is based on a presumed counterfactual model for the data. While the counterfactual model itself cannot be verified experimentally, we show that certain population notions of fairness are still guaranteed even if the counterfactual model is misspecified. The nature of the fulfilled observational non-causal fairness notion (such as demographic parity, separation or sufficiency) depends on the structure of the underlying causal model and the choice of resolving variables. We describe an implementation of the proposed data adaptation procedure based on Random Forests (Breiman, 2001) and demonstrate its practical use on simulated and real-world data.},
	journal = {Journal of Machine Learning Research},
	author = {Plečko, Drago and Meinshausen, Nicolai},
	year = {2020},
	keywords = {graphical models, supervised learning, causality, counterfactual fairness, fairness in machine learning},
	pages = {1--44},
	file = {Pleˇcko und Meinshausen - Fair Data Adaptation with Quantile Preservation.pdf:/Users/ludwigbothmann/Zotero/storage/BULF2QM8/Pleˇcko und Meinshausen - Fair Data Adaptation with Quantile Preservation.pdf:application/pdf},
}

@inproceedings{bothmann_causal_2023,
	title = {Causal {Fair} {Machine} {Learning} via {Rank}-{Preserving} {Interventional} {Distributions}},
	url = {https://ceur-ws.org/Vol-3523/},
	abstract = {A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.},
	language = {en},
	booktitle = {Proceedings of the 1st {Workshop} on {Fairness} and {Bias} in {AI} co-located with 26th {European} {Conference} on {Artificial} {Intelligence} ({ECAI} 2023)},
	publisher = {CEUR Workshop Proceedings},
	author = {Bothmann, Ludwig and Dandl, Susanne and Schomaker, Michael},
	month = oct,
	year = {2023},
	file = {Bothmann et al. - Causal Fair Machine Learning via Rank-Preserving I.pdf:/Users/ludwigbothmann/Zotero/storage/CZGFP4L7/Bothmann et al. - Causal Fair Machine Learning via Rank-Preserving I.pdf:application/pdf;Bothmann et al. - Causal Fair Machine Learning via Rank-Preserving I.pdf:/Users/ludwigbothmann/Zotero/storage/RIWNVZ97/Bothmann et al. - Causal Fair Machine Learning via Rank-Preserving I.pdf:application/pdf},
}

@incollection{dandl_interpretable_2023,
	address = {Cham},
	title = {Interpretable {Regional} {Descriptors}: {Hyperbox}-{Based} {Local} {Explanations}},
	volume = {14171},
	shorttitle = {Interpretable {Regional} {Descriptors}},
	url = {https://link.springer.com/10.1007/978-3-031-43418-1_29},
	abstract = {This work introduces interpretable regional descriptors, or IRDs, for local, model-agnostic interpretations. IRDs are hyperboxes that describe how an observation’s feature values can be changed without aﬀecting its prediction. They justify a prediction by providing a set of “even if” arguments (semi-factual explanations), and they indicate which features aﬀect a prediction and whether pointwise biases or implausibilities exist. A concrete use case shows that this is valuable for both machine learning modelers and persons subject to a decision. We formalize the search for IRDs as an optimization problem and introduce a unifying framework for computing IRDs that covers desiderata, initialization techniques, and a post-processing method. We show how existing hyperbox methods can be adapted to ﬁt into this uniﬁed framework. A benchmark study compares the methods based on several quality measures and identiﬁes two strategies to improve IRDs.},
	language = {en},
	urldate = {2023-10-24},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}: {Research} {Track}},
	publisher = {Springer Nature Switzerland},
	author = {Dandl, Susanne and Casalicchio, Giuseppe and Bischl, Bernd and Bothmann, Ludwig},
	year = {2023},
	doi = {10.1007/978-3-031-43418-1_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {479--495},
	file = {Dandl et al. - 2023 - Interpretable Regional Descriptors Hyperbox-Based.pdf:/Users/ludwigbothmann/Zotero/storage/QUS6W633/Dandl et al. - 2023 - Interpretable Regional Descriptors Hyperbox-Based.pdf:application/pdf},
}

@article{chouldechova_fair_2017,
	title = {Fair {Prediction} with {Disparate} {Impact}: {A} {Study} of {Bias} in {Recidivism} {Prediction} {Instruments}},
	volume = {5},
	shorttitle = {Fair {Prediction} with {Disparate} {Impact}},
	url = {https://www.liebertpub.com/doi/10.1089/big.2016.0047},
	doi = {10.1089/big.2016.0047},
	number = {2},
	journal = {Big Data},
	author = {Chouldechova, Alexandra},
	year = {2017},
	keywords = {risk assessment, bias, disparate impact, fair machine learning, recidivism prediction},
	pages = {153--163},
}

@article{kauermann_berucksichtigung_2023,
	title = {Die {Berücksichtigung} von außergesetzlichen {Merkmalen} bei der {Mietspiegelerstellung} – {Kausalität} versus {Vorhersage}},
	volume = {17},
	issn = {1863-8163},
	url = {https://doi.org/10.1007/s11943-023-00321-1},
	doi = {10.1007/s11943-023-00321-1},
	abstract = {Das neue Mietspiegelgesetz erlaubt die Berücksichtigung von sogenannten außergesetzlichen Merkmalen wie Mietdauer und Vermietertyp bei der Erstellung von Mietspiegeln. Diese außergesetzlichen Merkmale dürfen in zukünftigen Mietspiegeln bei deren Erstellung und Modellwahl Einfluss finden, nicht aber im konkreten Mietspiegelmodell. Diese gesetzliche Vorgabe lässt viel Spielraum, der in diesem Beitrag aus statistischer Sicht beleuchtet wird. Anhand von konkreten Daten werden die Konsequenzen quantifiziert und aufgezeigt.},
	language = {de},
	urldate = {2023-11-30},
	journal = {AStA Wirtschafts- und Sozialstatistisches Archiv},
	author = {Kauermann, Göran and Windmann, Michael},
	month = jun,
	year = {2023},
	keywords = {außergesetzliche Merkmale, Mietspiegel, Regressionsmietspiegel},
	pages = {145--160},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/K6I68HBJ/Kauermann und Windmann - 2023 - Die Berücksichtigung von außergesetzlichen Merkmal.pdf:application/pdf},
}

@article{le_quy_survey_2022,
	title = {A survey on datasets for fairness-aware machine learning},
	volume = {12},
	copyright = {© 2022 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1452},
	doi = {10.1002/widm.1452},
	abstract = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues {\textgreater} Fairness in Data Mining Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Technologies {\textgreater} Data Preprocessing},
	language = {en},
	number = {3},
	urldate = {2024-02-28},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1452},
	keywords = {discrimination, bias, benchmark datasets, datasets for fairness, fairness-aware machine learning},
	pages = {e1452},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/ZKUMQYBX/Le Quy et al. - 2022 - A survey on datasets for fairness-aware machine le.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/8DT6D5QC/widm.html:text/html},
}

@article{bothmann_what_2024,
	title = {What {Is} {Fairness}? {On} the {Role} of {Protected} {Attributes} and {Fictitious} {Worlds}},
	shorttitle = {What {Is} {Fairness}?},
	url = {http://arxiv.org/abs/2205.09622},
	doi = {10.48550/arXiv.2205.09622},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Bothmann, Ludwig and Peters, Kristina and Bischl, Bernd},
	year = {2024},
}

@inproceedings{alvarez_counterfactual_2023,
	address = {New York, NY, USA},
	series = {{EAAMO} '23},
	title = {Counterfactual {Situation} {Testing}: {Uncovering} {Discrimination} under {Fairness} given the {Difference}},
	shorttitle = {Counterfactual {Situation} {Testing}},
	url = {https://dl.acm.org/doi/10.1145/3617694.3623222},
	doi = {10.1145/3617694.3623222},
	abstract = {We present counterfactual situation testing (CST), a causal data mining framework for detecting individual discrimination in a dataset of classifier decisions. CST answers the question “what would have been the model outcome had the individual, or complainant, been of a different protected status?” in an actionable and meaningful way. It extends the legally-grounded situation testing of Thanh et al. [62] by operationalizing the notion of fairness given the difference of Kohler-Hausmann [38] using counterfactual reasoning. In standard situation testing we find for each complainant similar protected and non-protected instances in the dataset; construct respectively a control and test group; and compare the groups such that a difference in decision outcomes implies a case of potential individual discrimination. In CST we avoid this idealized comparison by establishing the test group on the complainant’s counterfactual generated via the steps of abduction, action, and prediction. The counterfactual reflects how the protected attribute, when changed, affects the other seemingly neutral attributes of the complainant. Under CST we, thus, test for discrimination by comparing similar individuals within each group but dissimilar individuals across both groups for each complainant. Evaluating it on two classification scenarios, CST uncovers a greater number of cases than ST, even when the classifier is counterfactually fair.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 3rd {ACM} {Conference} on {Equity} and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Alvarez, Jose Manuel and Ruggieri, Salvatore},
	month = oct,
	year = {2023},
	keywords = {fairness-diagnostics},
	pages = {1--11},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/EHHFAGXB/Alvarez und Ruggieri - 2023 - Counterfactual Situation Testing Uncovering Discr.pdf:application/pdf},
}

@incollection{chiappa_causal_2019,
	address = {Cham},
	title = {A {Causal} {Bayesian} {Networks} {Viewpoint} on {Fairness}},
	volume = {547},
	url = {https://link.springer.com/10.1007/978-3-030-16744-8_1},
	abstract = {We oﬀer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal eﬀect of the sensitive attribute in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios.},
	language = {en},
	urldate = {2024-03-25},
	booktitle = {Privacy and {Identity} {Management}. {Fairness}, {Accountability}, and {Transparency} in the {Age} of {Big} {Data}},
	publisher = {Springer International Publishing},
	author = {Chiappa, Silvia and Isaac, William S.},
	year = {2019},
	doi = {10.1007/978-3-030-16744-8_1},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {3--20},
	file = {Chiappa und Isaac - 2019 - A Causal Bayesian Networks Viewpoint on Fairness.pdf:/Users/ludwigbothmann/Zotero/storage/78I534SB/Chiappa und Isaac - 2019 - A Causal Bayesian Networks Viewpoint on Fairness.pdf:application/pdf;Eingereichte Version:/Users/ludwigbothmann/Zotero/storage/C77FXPF2/Chiappa und Isaac - 2019 - A Causal Bayesian Networks Viewpoint on Fairness.pdf:application/pdf},
}


@InProceedings{ewald_guide_2024,
author="Ewald, Fiona Katharina
and Bothmann, Ludwig
and Wright, Marvin N.
and Bischl, Bernd
and Casalicchio, Giuseppe
and K{\"o}nig, Gunnar",
editor="Longo, Luca
and Lapuschkin, Sebastian
and Seifert, Christin",
title="A Guide to Feature Importance Methods for Scientific Inference",
booktitle="Explainable Artificial Intelligence",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="440--464",
doi="10.1007/978-3-031-63797-1_22",
url="https://link.springer.com/chapter/10.1007/978-3-031-63797-1_22",
}


@misc{chzhen_minimax_2022,
	title = {A minimax framework for quantifying risk-fairness trade-off in regression},
	url = {http://arxiv.org/abs/2007.14265},
	doi = {10.48550/arXiv.2007.14265},
	abstract = {We propose a theoretical framework for the problem of learning a real-valued function which meets fairness requirements. This framework is built upon the notion of \${\textbackslash}alpha\$-relative (fairness) improvement of the regression function which we introduce using the theory of optimal transport. Setting \${\textbackslash}alpha = 0\$ corresponds to the regression problem under the Demographic Parity constraint, while \${\textbackslash}alpha = 1\$ corresponds to the classical regression problem without any constraints. For \${\textbackslash}alpha {\textbackslash}in (0, 1)\$ the proposed framework allows to continuously interpolate between these two extreme cases and to study partially fair predictors. Within this framework we precisely quantify the cost in risk induced by the introduction of the fairness constraint. We put forward a statistical minimax setup and derive a general problem-dependent lower bound on the risk of any estimator satisfying \${\textbackslash}alpha\$-relative improvement constraint. We illustrate our framework on a model of linear regression with Gaussian design and systematic group-dependent bias, deriving matching (up to absolute constants) upper and lower bounds on the minimax risk under the introduced constraint. We provide a general post-processing strategy which enjoys fairness, risk guarantees and can be applied on top of any black-box algorithm. Finally, we perform a simulation study of the linear model and numerical experiments of benchmark data, validating our theoretical contributions.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Chzhen, Evgenii and Schreuder, Nicolas},
	month = jan,
	year = {2022},
	note = {arXiv:2007.14265 [math, stat]},
	keywords = {\_to\_read},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/3SPPY5A2/Chzhen und Schreuder - 2022 - A minimax framework for quantifying risk-fairness .pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/9XCF2IDJ/2007.html:text/html},
}

@misc{chzhen_fair_2020,
	title = {Fair {Regression} with {Wasserstein} {Barycenters}},
	url = {http://arxiv.org/abs/2006.07286},
	doi = {10.48550/arXiv.2006.07286},
	abstract = {We study the problem of learning a real-valued function that satisfies the Demographic Parity constraint. It demands the distribution of the predicted output to be independent of the sensitive attribute. We consider the case that the sensitive attribute is available for prediction. We establish a connection between fair regression and optimal transport theory, based on which we derive a close form expression for the optimal fair predictor. Specifically, we show that the distribution of this optimum is the Wasserstein barycenter of the distributions induced by the standard regression function on the sensitive groups. This result offers an intuitive interpretation of the optimal fair prediction and suggests a simple post-processing algorithm to achieve fairness. We establish risk and distribution-free fairness guarantees for this procedure. Numerical experiments indicate that our method is very effective in learning fair models, with a relative increase in error rate that is inferior to the relative gain in fairness.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Chzhen, Evgenii and Denis, Christophe and Hebiri, Mohamed and Oneto, Luca and Pontil, Massimiliano},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07286 [cs, math, stat]},
	keywords = {\_to\_read},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/Q66MAFGE/Chzhen et al. - 2020 - Fair Regression with Wasserstein Barycenters.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/B7F92DAY/2006.html:text/html},
}

@inproceedings{black_fliptest_2020,
	series = {{FAT}* '20},
	title = {{FlipTest}: fairness testing via optimal transport},
	shorttitle = {{FlipTest}},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372845},
	doi = {10.1145/3351095.3372845},
	urldate = {2024-05-05},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Black, Emily and Yeom, Samuel and Fredrikson, Matt},
	year = {2020},
	keywords = {\_to\_read},
	pages = {111--121},
}

@article{wachter_bias_2021,
	title = {Bias {Preservation} in {Machine} {Learning}: {The} {Legality} of {Fairness} {Metrics} {Under} {EU} {Non}-{Discrimination} {Law}},
	volume = {123},
	shorttitle = {Bias {Preservation} in {Machine} {Learning}},
	url = {https://papers.ssrn.com/abstract=3792772},
	doi = {10.2139/ssrn.3792772},
	language = {en},
	number = {3},
	urldate = {2024-05-06},
	journal = {West Virginia Law Review},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2021},
	keywords = {\_to\_read},
	pages = {735--790},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/6IFMHRAE/Wachter et al. - 2021 - Bias Preservation in Machine Learning The Legalit.pdf:application/pdf},
}

@inproceedings{weerts_algorithmic_2023,
	series = {{FAccT} '23},
	title = {Algorithmic {Unfairness} through the {Lens} of {EU} {Non}-{Discrimination} {Law}: {Or} {Why} the {Law} is not a {Decision} {Tree}},
	shorttitle = {Algorithmic {Unfairness} through the {Lens} of {EU} {Non}-{Discrimination} {Law}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594044},
	doi = {10.1145/3593013.3594044},
	urldate = {2024-05-05},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Weerts, Hilde and Xenidis, Raphaële and Tarissan, Fabien and Olsen, Henrik Palmer and Pechenizkiy, Mykola},
	year = {2023},
	pages = {805--816},
}

@misc{cooper_arbitrariness_2024,
	title = {Arbitrariness and {Social} {Prediction}: {The} {Confounding} {Role} of {Variance} in {Fair} {Classification}},
	shorttitle = {Arbitrariness and {Social} {Prediction}},
	url = {http://arxiv.org/abs/2301.11562},
	doi = {10.48550/arXiv.2301.11562},
	abstract = {Variance in predictions across different trained models is a significant, under-explored source of error in fair binary classification. In practice, the variance on some data examples is so large that decisions can be effectively arbitrary. To investigate this problem, we take an experimental approach and make four overarching contributions: We: 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair binary classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our experiments reveal shocking insights about the reliability of conclusions on benchmark datasets. Most fair binary classification benchmarks are close-to-fair when taking into account the amount of arbitrariness present in predictions -- before we even try to apply any fairness interventions. This finding calls into question the practical utility of common algorithmic fairness methods, and in turn suggests that we should reconsider how we choose to measure fairness in binary classification.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Cooper, A. Feder and Lee, Katherine and Choksi, Madiha Zahrah and Barocas, Solon and De Sa, Christopher and Grimmelmann, James and Kleinberg, Jon and Sen, Siddhartha and Zhang, Baobao},
	month = mar,
	year = {2024},
	note = {arXiv:2301.11562 [cs, stat]},
	file = {arXiv Fulltext PDF:/Users/ludwigbothmann/Zotero/storage/D5YXQEKY/Cooper et al. - 2024 - Arbitrariness and Social Prediction The Confoundi.pdf:application/pdf;arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/YGA8WUXD/2301.html:text/html},
}

@article{issa_kohler-hausmann_eddie_2019,
	title = {Eddie {Murphy} and the {Dangers} of {Counterfactual} {Causal} {Thinking} {About} {Detecting} {Racial} {Discrimination}},
	volume = {113},
	url = {https://scholarlycommons.law.northwestern.edu/nulr/vol113/iss5/6},
	number = {5},
	journal = {Northwestern University Law Review},
	author = {Issa Kohler-Hausmann},
	year = {2019},
	pages = {1163--1228},
}

@inproceedings{javaloy_causal_2023,
	title = {Causal normalizing flows: from theory to practice},
	abstract = {In this work, we deepen on the use of normalizing flows for causal inference. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyse different design and learning choices for causal normalizing flows to capture the underlying causal data-generating process. Third, we describe how to implement the do-operator in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems—where mixed discretecontinuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows.},
	language = {en},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Javaloy, Adrian and Sanchez-Martın, Pablo and Valera, Isabel},
	year = {2023},
	file = {Javaloy et al. - Causal normalizing flows from theory to practice.pdf:/Users/ludwigbothmann/Zotero/storage/JXLM5YZ9/Javaloy et al. - Causal normalizing flows from theory to practice.pdf:application/pdf},
}

@article{hothorn_conditional_2014,
	title = {Conditional {Transformation} {Models}},
	volume = {76},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/rssb.12017},
	doi = {10.1111/rssb.12017},
	number = {1},
	urldate = {2024-07-19},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Hothorn, Torsten and Kneib, Thomas and Bühlmann, Peter},
	year = {2014},
	pages = {3--27},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/C24RZ3GW/Hothorn et al. - 2014 - Conditional Transformation Models.pdf:application/pdf;Snapshot:/Users/ludwigbothmann/Zotero/storage/WX7895VZ/7075933.html:text/html},
}

@misc{goethals_beyond_2024,
	title = {Beyond {Accuracy}-{Fairness}: {Stop} evaluating bias mitigation methods solely on between-group metrics},
	shorttitle = {Beyond {Accuracy}-{Fairness}},
	url = {http://arxiv.org/abs/2401.13391},
	abstract = {Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a paradigm shift: initially, we should focus on generating the most precise ranking for each subgroup. Following this, individuals should be chosen from these rankings to meet both fairness standards and practical considerations.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Goethals, Sofie and Calders, Toon and Martens, David},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13391 [cs]},
	keywords = {Computer Science - Machine Learning, wichtig},
	file = {arXiv.org Snapshot:/Users/ludwigbothmann/Zotero/storage/JDL4RZLB/2401.html:text/html;Full Text PDF:/Users/ludwigbothmann/Zotero/storage/4FXK588Y/Goethals et al. - 2024 - Beyond Accuracy-Fairness Stop evaluating bias mit.pdf:application/pdf},
}

@article{lang_mlr3_2019,
	title = {mlr3: {A} modern object-oriented machine learning framework in {R}},
	volume = {4},
	url = {https://doi.org/10.21105/joss.01903},
	doi = {10.21105/joss.01903},
	number = {44},
	journal = {Journal of Open Source Software},
	author = {Lang, Michel and Binder, Martin and Richter, Jakob and Schratz, Patrick and Pfisterer, Florian and Coors, Stefan and Au, Quay and Casalicchio, Giuseppe and Kotthoff, Lars and Bischl, Bernd},
	year = {2019},
	note = {Publisher: The Open Journal},
	pages = {1903},
}

@inproceedings{simson_lazy_2024,
	address = {New York, NY, USA},
	series = {{FAccT} '24},
	title = {Lazy {Data} {Practices} {Harm} {Fairness} {Research}},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658931},
	doi = {10.1145/3630106.3658931},
	abstract = {Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications. Our analyses identify three main areas of concern: (1) a lack of representation for certain protected attributes in both data and evaluations; (2) the widespread exclusion of minorities during data preprocessing; and (3) opaque data processing threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.},
	urldate = {2024-08-28},
	booktitle = {Proceedings of the 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Simson, Jan and Fabris, Alessandro and Kern, Christoph},
	month = jun,
	year = {2024},
	pages = {642--659},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/K5SJKVFZ/Simson et al. - 2024 - Lazy Data Practices Harm Fairness Research.pdf:application/pdf},
}

@article{fabris_algorithmic_2022,
	title = {Algorithmic fairness datasets: the story so far},
	volume = {36},
	issn = {1573-756X},
	shorttitle = {Algorithmic fairness datasets},
	url = {https://doi.org/10.1007/s10618-022-00854-z},
	doi = {10.1007/s10618-022-00854-z},
	abstract = {Data-driven algorithms are studied and deployed in diverse domains to support critical decisions, directly impacting people’s well-being. As a result, a growing community of researchers has been investigating the equity of existing algorithms and proposing novel ones, advancing the understanding of risks and opportunities of automated decision-making for historically disadvantaged populations. Progress in fair machine learning and equitable algorithm design hinges on data, which can be appropriately used only if adequately documented. Unfortunately, the algorithmic fairness community, as a whole, suffers from a collective data documentation debt caused by a lack of information on specific resources (opacity) and scatteredness of available information (sparsity). In this work, we target this data documentation debt by surveying over two hundred datasets employed in algorithmic fairness research, and producing standardized and searchable documentation for each of them. Moreover we rigorously identify the three most popular fairness datasets, namely Adult, COMPAS, and German Credit, for which we compile in-depth documentation. This unifying documentation effort supports multiple contributions. Firstly, we summarize the merits and limitations of Adult, COMPAS, and German Credit, adding to and unifying recent scholarship, calling into question their suitability as general-purpose fairness benchmarks. Secondly, we document hundreds of available alternatives, annotating their domain and supported fairness tasks, along with additional properties of interest for fairness practitioners and researchers, including their format, cardinality, and the sensitive attributes they encode. We summarize this information, zooming in on the tasks, domains, and roles of these resources. Finally, we analyze these datasets from the perspective of five important data curation topics: anonymization, consent, inclusivity, labeling of sensitive attributes, and transparency. We discuss different approaches and levels of attention to these topics, making them tangible, and distill them into a set of best practices for the curation of novel resources.},
	language = {en},
	number = {6},
	urldate = {2024-08-28},
	journal = {Data Mining and Knowledge Discovery},
	author = {Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
	month = nov,
	year = {2022},
	pages = {2074--2152},
	file = {fabris22-appendix.pdf:/Users/ludwigbothmann/Zotero/storage/3JCQCYJQ/10618_2022_854_MOESM1_ESM.pdf:application/pdf;Full Text PDF:/Users/ludwigbothmann/Zotero/storage/7NALA7VP/Fabris et al. - 2022 - Algorithmic fairness datasets the story so far.pdf:application/pdf},
}

@article{gromping_south_nodate,
	title = {South {German} {Credit} {Data}: {Correcting} a {Widely} {Used} {Data} {Set}},
	language = {en},
	author = {Grömping, Ulrike},
	file = {Grömping - South German Credit Data Correcting a Widely Used.pdf:/Users/ludwigbothmann/Zotero/storage/EQ244739/Grömping - South German Credit Data Correcting a Widely Used.pdf:application/pdf},
}

@inproceedings{dandl_multi-objective_2022,
	address = {New York, NY, USA},
	series = {{GECCO} '22},
	title = {Multi-objective counterfactual fairness},
	url = {https://dl.acm.org/doi/10.1145/3520304.3528779},
	doi = {10.1145/3520304.3528779},
	abstract = {When machine learning is used to automate judgments, e.g. in areas like lending or crime prediction, incorrect decisions can lead to adverse effects for affected individuals. This occurs, e.g., if the data used to train these models is based on prior decisions that are unfairly skewed against specific subpopulations. If models should automate decision-making, they must account for these biases to prevent perpetuating or creating discriminatory practices. Counter-factual fairness audits models with respect to a notion of fairness that asks for equal outcomes between a decision made in the real world and a counterfactual world where the individual subject to a decision comes from a different protected demographic group. In this work, we propose a method to conduct such audits without access to the underlying causal structure of the data generating process by framing it as a multi-objective optimization task that can be efficiently solved using a genetic algorithm.},
	urldate = {2024-09-09},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Dandl, Susanne and Pfisterer, Florian and Bischl, Bernd},
	month = jul,
	year = {2022},
	pages = {328--331},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/VYHMMJXD/Dandl et al. - 2022 - Multi-objective counterfactual fairness.pdf:application/pdf},
}

@article{lara_transport-based_2024,
	title = {Transport-based {Counterfactual} {Models}},
	volume = {25},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v25/21-1440.html},
	abstract = {Counterfactual frameworks have grown popular in machine learning for both explaining algorithmic decisions but also defining individual notions of fairness, more intuitive than typical group fairness conditions. However, state-of-the-art models to compute counterfactuals are either unrealistic or unfeasible. In particular, while Pearl's causal inference provides appealing rules to calculate counterfactuals, it relies on a model that is unknown and hard to discover in practice. We address the problem of designing realistic and feasible counterfactuals in the absence of a causal model. We define transport-based counterfactual models as collections of joint probability distributions between observable distributions, and show their connection to causal counterfactuals. More specifically, we argue that optimal-transport theory defines relevant transport-based counterfactual models, as they are numerically feasible, statistically-faithful, and can coincide under some assumptions with causal counterfactual models. Finally, these models make counterfactual approaches to fairness feasible, and we illustrate their practicality and efficiency on fair learning. With this paper, we aim at laying out the theoretical foundations for a new, implementable approach to counterfactual thinking.},
	number = {136},
	urldate = {2024-09-10},
	journal = {Journal of Machine Learning Research},
	author = {Lara, Lucas De and González-Sanz, Alberto and Asher, Nicholas and Risser, Laurent and Loubes, Jean-Michel},
	year = {2024},
	pages = {1--59},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/QHAD4MH6/Lara et al. - 2024 - Transport-based Counterfactual Models.pdf:application/pdf;Source Code:/Users/ludwigbothmann/Zotero/storage/H4VTKH8R/PI-Fair.html:text/html},
}

@article{chiappa_general_2020,
	title = {A {General} {Approach} to {Fairness} with {Optimal} {Transport}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5771},
	doi = {10.1609/aaai.v34i04.5771},
	abstract = {We propose a general approach to fairness based on transporting distributions corresponding to different sensitive attributes to a common distribution. We use optimal transport theory to derive target distributions and methods that allow us to achieve fairness with minimal changes to the unfair model. Our approach is applicable to both classification and regression problems, can enforce different notions of fairness, and enable us to achieve a Pareto-optimal trade-off between accuracy and fairness. We demonstrate that it outperforms previous approaches in several benchmark fairness datasets.},
	language = {en},
	number = {04},
	urldate = {2024-09-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chiappa, Silvia and Jiang, Ray and Stepleton, Tom and Pacchiano, Aldo and Heinrich, Jiang and Aslanides, John},
	month = apr,
	year = {2020},
	note = {Number: 04},
	pages = {3633--3640},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/ZRH7YA2T/Silvia et al. - 2020 - A General Approach to Fairness with Optimal Transp.pdf:application/pdf},
}

@article{caton_fairness_2024,
	title = {Fairness in {Machine} {Learning}: {A} {Survey}},
	volume = {56},
	issn = {0360-0300},
	shorttitle = {Fairness in {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3616865},
	doi = {10.1145/3616865},
	abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.},
	number = {7},
	urldate = {2024-09-10},
	journal = {ACM Comput. Surv.},
	author = {Caton, Simon and Haas, Christian},
	month = apr,
	year = {2024},
	pages = {166:1--166:38},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/AZ33XGXL/Caton und Haas - 2024 - Fairness in Machine Learning A Survey.pdf:application/pdf},
}

@article{komanduri_identifiable_2023,
	title = {From {Identifiable} {Causal} {Representations} to {Controllable} {Counterfactual} {Generation}: {A} {Survey} on {Causal} {Generative} {Modeling}},
	issn = {2835-8856},
	shorttitle = {From {Identifiable} {Causal} {Representations} to {Controllable} {Counterfactual} {Generation}},
	url = {https://openreview.net/forum?id=PUpZXvNqmb},
	abstract = {Deep generative models have shown tremendous capability in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, tendency to induce spurious correlations, and poor out-of-distribution extrapolation. To remedy such challenges, recent work has proposed a shift toward causal generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interpretability. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual generation methods. We focus on fundamental theory, methodology, drawbacks, datasets, and metrics. Then, we cover applications of causal generative models in fairness, privacy, out-of-distribution generalization, precision medicine, and biological sciences. Lastly, we discuss open problems and fruitful research directions for future work in the field.},
	language = {en},
	urldate = {2024-09-10},
	journal = {Transactions on Machine Learning Research},
	author = {Komanduri, Aneesh and Wu, Xintao and Wu, Yongkai and Chen, Feng},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/G5N5ZI87/Komanduri et al. - 2023 - From Identifiable Causal Representations to Contro.pdf:application/pdf},
}

@inproceedings{fabris_algorithmic_2021,
	address = {New York, NY, USA},
	series = {{AIES} '21},
	title = {Algorithmic {Audit} of {Italian} {Car} {Insurance}: {Evidence} of {Unfairness} in {Access} and {Pricing}},
	shorttitle = {Algorithmic {Audit} of {Italian} {Car} {Insurance}},
	url = {https://dl.acm.org/doi/10.1145/3461702.3462569},
	doi = {10.1145/3461702.3462569},
	abstract = {We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.},
	urldate = {2024-09-13},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Fabris, Alessandro and Mishler, Alan and Gottardi, Stefano and Carletti, Mattia and Daicampi, Matteo and Susto, Gian Antonio and Silvello, Gianmaria},
	month = jul,
	year = {2021},
	pages = {458--468},
	file = {Full Text PDF:/Users/ludwigbothmann/Zotero/storage/GEZQBN8P/Fabris et al. - 2021 - Algorithmic Audit of Italian Car Insurance Eviden.pdf:application/pdf},
}

@article{rondina_testing_nodate,
	title = {Testing software for non-discrimination: an updated and extended audit in the {Italian} car insurance domain},
	abstract = {Context. As software systems become more integrated into society’s infrastructure, the responsibility of software professionals to ensure compliance with various non-functional requirements increases. These requirements include security, safety, privacy, and, increasingly, non-discrimination.},
	language = {en},
	author = {Rondina, Marco and Vetrò, Antonio and Coppola, Riccardo and Regragrui, Oumaima and Fabris, Alessandro and Silvello, Gianmaria and Susto, Gian Antonio and Martin, Juan Carlos De},
	file = {Rondina et al. - Testing software for non-discrimination an update.pdf:/Users/ludwigbothmann/Zotero/storage/XTSKF4LV/Rondina et al. - Testing software for non-discrimination an update.pdf:application/pdf},
}

@incollection{shapley_value_1953,
	address = {Princeton},
	title = {A {Value} for n-{Person} {Games}},
	url = {https://doi.org/10.1515/9781400881970-018},
	urldate = {2025-01-29},
	booktitle = {Contributions to the {Theory} of {Games}, {Volume} {II}},
	publisher = {Princeton University Press},
	author = {Shapley, L. S.},
	editor = {Kuhn, Harold William and Tucker, Albert William},
	year = {1953},
	doi = {10.1515/9781400881970-018},
	pages = {307--318},
}

@article{strumbelj_explaining_2014,
	title = {Explaining prediction models and individual predictions with feature contributions},
	volume = {41},
	copyright = {2013 Springer-Verlag London},
	url = {https://link.springer.com/article/10.1007/s10115-013-0679-x},
	doi = {10.1007/s10115-013-0679-x},
	language = {en},
	number = {3},
	urldate = {2025-01-29},
	journal = {Knowledge and Information Systems},
author = {{\v{S}}trumbelj, Erik and Kononenko, Igor},
	year = {2014},
	pages = {647--665},
}

@article{lundberg_local_2020,
	title = {From local explanations to global understanding with explainable {AI} for trees},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	url = {https://www.nature.com/articles/s42256-019-0138-9},
	doi = {10.1038/s42256-019-0138-9},
	language = {en},
	number = {1},
	urldate = {2025-01-29},
	journal = {Nature Machine Intelligence},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	year = {2020},
	pages = {56--67},
}

@article{friedman_bump_1999,
	title = {Bump hunting in high-dimensional data},
	volume = {9},
	url = {https://link.springer.com/article/10.1023/A:1008894516817},
	doi = {10.1023/A:1008894516817},
	language = {en},
	number = {2},
	urldate = {2025-01-29},
	journal = {Statistics and Computing},
	author = {Friedman, Jerome H. and Fisher, Nicholas I.},
	year = {1999},
	pages = {123--143},
}
