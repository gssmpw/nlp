\section{Related Work}
\label{sec:related-work}

The proposed PS are intended for addressing a (presumed to be) non-neutral status quo.
We position our work relative to other fairML works that aim at changing such status quo by implementing, according to 
\citet{wachter_bias_2021}, ``bias-transforming'' methods \citep[e.g.,][]{kusner_counterfactual_2017, black_fliptest_2020, alvarez_counterfactual_2023}.
\citet{kusner_counterfactual_2017} famously introduce counterfactual fairness, which compares the factual distribution to its counterfactual counterpart \citep[using the steps of abduction, action, and prediction of][]{PearlCausality2009} in which the downstream influence of the PA is accounted for all other attributes.
\citet{black_fliptest_2020} and \citet{alvarez_counterfactual_2023}, respectively, test for individual discrimination by comparing the observed profiles to generated ones in which the seemingly neutral attributes are updated conditional on the effect of the PA.
In terms of fairML methods, our work relates mainly to pre-processing methods such as those proposed by \citet{plecko_fair_2020} and \citet{bothmann_causal_2023}.
%We come back to these methods in the next section.
We differ from all these works by explicitly formulating what drives the non-neutrality of the status quo in the form of privilege.

Noteworthy are the ongoing fairML discussions on achieving long-term fairness \citep[e.g.,][]{DBLP:conf/www/HuC18, DBLP:conf/fat/DAmourSABSH20, DBLP:conf/fat/SchwobelR22} and addressing the accuracy-fairness trade-off  \citep[e.g.,][]{DBLP:conf/nips/WickpT19, DBLP:journals/corr/abs-2011-03173, DBLP:journals/natmi/RodolfaLG21, leininger-tradeoffs-2025}, respectively.
The former argues that fairness interventions need to consider their impact over time. 
The latter argues that the trade-off is trivial as long as the data used for training is biased. 
Together with \citet{wachter_bias_2021} and its bias-preserving versus bias-transforming distinction, 
both discussions are examples of a more explicit direction within fairML of using algorithmic tools to intervene in the status quo.
Our work adds privilege to the discussion, viewing it as a consequence of the non-neutral status quo.


% \iffalse
% %SUGGESTION LISA FOR REORDERED RELATED WORK SECTION
% In what sparked a strand of seminal work in fairML, \citet{kusner_counterfactual_2017} introduced counterfactual fairness as a divergence of the factual distribution from its counterfactual counterpart \citep[using the steps of abduction, action, and prediction of][]{PearlCausality2009} in which the downstream influence of the PA on all other attributes is accounted for.
% A number of recent contributions have promoted the explicit use of algorithmic tools to intervene on a non-neutral status quo.
% Besides \citet{wachter_bias_2021} and their aforementioned distinction between bias preservation and bias transformation, discussions of long-term fairness \citep[e.g.,][]{DBLP:conf/www/HuC18, DBLP:conf/fat/DAmourSABSH20, DBLP:conf/fat/SchwobelR22} and the accuracy-fairness trade-off  \citep[e.g.,][]{DBLP:conf/nips/WickpT19, DBLP:journals/corr/abs-2011-03173, DBLP:journals/natmi/RodolfaLG21, plecko_trade_2024, leininger-tradeoffs-2025} are particularly noteworthy.
% The former advocates for considering the impact of fairness interventions over time, while the latter argues out that the trade-off is trivial in tasks with biased training data.
% Our contribution formalizes the discussion by introducing the notion of privilege as a consequence of the non-neutral status quo.
% Concurrent work that aims at changing the undesirable status quo by implementing bias-transforming methods includes \citet{kusner_counterfactual_2017, black_fliptest_2020, alvarez_counterfactual_2023}.
% From a more technical perspective, our work relates mainly to pre-processing methods such as those proposed by \citet{plecko_fair_2020} and \citet{bothmann_causal_2023}.
% %We will revisit these methods in the next section (WHY? that's something I don't enjoy as a reader).
% While the previous works have greatly contributed to the progress of fairML, they remain vague on what drives the non-neutrality of the status quo.
% We close this gap by formalizing the causes in the form of privilege.
% The resulting PS provide a crucial opportunity for identifying individual-level discrimination and informing global bias-transforming policies.
% \fi