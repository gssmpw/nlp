\section{Related Work}\label{sec:related-work}

We discuss relevant work on human-robot collaboration (HRC), our target domain of human-robot collaborative transport, and our technical foundation of implicit communication.


\subsection{Human-Robot Collaboration}

HRC, defined broadly as the integration of humans and robots working together towards a set of (possibly joint) goals, has attracted considerable attention in recent years~\citep{mainprice2013collaborative,Wilcox-RSS-12,gopinath2017sharedautonomy,carlson2013wheelchair-shared}. An important challenge in HRC involves determining a fluent meshing between human and robot actions that leverages the unique competences of both in an effective way~\citep{hoffman2019evaluating}. This has motivated research on the design of planning and control frameworks that smoothly blend human and robot control inputs~\citep{dragan2013blending,nikolaidis2017adaptation,unhelkar2020bidirectional,aronson2024intentional}. For many applications, the human and the robot goals are well-defined; e.g., in shared control for wheelchair navigation, the role of the robot is to lead the user to a desired waypoint~\citep{carlson2013wheelchair-shared}. However, in other applications, the richness of the context results in a multiplicity of ways in which a task could be achieved. In those cases, a robot has to reason about how its own strategy for satisfying its objectives meshes with the strategy of its human partner~\citep{zhao2022coordination,pandya2024multi,peters2020inference,wang2022co} or how its actions contribute towards a joint strategy~\citep{xie2021learning}. An underlying challenge involves determining an appropriate space of strategies. \citet{nikolaidis2013human} describe an interactive planning framework that iteratively switches role assignment to humans and robots with the goal of converging on a shared strategy. \citet{zhao2022coordination} and \citet{xie2021learning} learn spaces of strategies from observations of low-level action sequences whereas \citet{wang2022co} learn a latent space of strategies that enables a robot to adapt to its collaborator.

Our approach is motivated by the physical HRC task of human-robot collaborative transport. The execution of this task admits multiple possible solutions in realistic environments with obstacle obstructions.
Recent work in HRC emphasizes the extraction of these solutions from team strategies observed in human teamwork. In contrast, we exploit the mathematical structure of the domain, identifying strategies of collaborative transport as classes of homotopy-equivalent trajectories of workspace traversal~\citep{knepper2012equivalence,vernaza2013winding,kretzschmar2016irl}. This allows for methodical enumeration of strategies in an interpretable form and in line with insights from recent studies on human-human object transport~\citep{freeman2023classification}. Additionally, while a body of work considers validation in virtual benchmarks, we emphasize \emph{physical}, \emph{explicit}, and \emph{concurrent} human-robot collaboration~\citep{selvaggio2021survey} treating the robot as a fully embodied partner with mobile manipulation capabilities.


\subsection{Human-Robot Collaborative Transport}

Physical HRC can support applications like personal robotics, manufacturing, and construction~\citep{losey2018review-shared,ogenyi2019physical,selvaggio2021survey}. Much of the recent work has focused on collaborative manipulation tasks involving a user and a manipulator, such as the rearrangement of large or heavy objects~\citep{gienger2018contactchanges,kim2018anticipatory,stouraitis2020bilevel,al2023resolving,kucukyilmaz2019online,agravante2014collaborative,Zheng2022SafeHC,Shao2024comanipulation} and the collaborative use of tools for sawing or bolt screwing~\citep{peternel2017comanipulation}.

Many real-world applications motivate the integration of manipulation and robot mobility. An important task of interest is the \emph{collaborative transport} of an object by a user and a mobile manipulator. This task is especially challenging because in addition to human-robot coordination, it requires considerations like collision avoidance, object stability, and human ergonomics. A large body of work assumes a fixed role assignment: the human intent is estimated and the robot follows it~\citep{bussy2012transportation,bussy2012proactive,mielke2024comanipulation,lima2023assistive,solanes2018human}. This can be too restrictive, especially when the robot has the capability to contribute meaningfully to the task as an independent agent. To account for such situations, \citet{nikolaidis2017mutual} develop a probabilistic controller that estimates the user's preferences and generates corrective actions to guide them to an efficient strategy. However, this implies that the robot is more equipped to guide the interaction, which might not always be the case. Instead, \citet{ng2023takes} allow leadership to be an emergent property of following a policy extracted from human demonstrations. \citet{mortl2012role} engineer a dynamic leadership negotiation through actions executed on the transported object by the two agents.
While mobility is integrated across these works, it is often oversimplified through the assumption of predefined paths~\citep{mortl2012role} or collision-free workspaces~\citep{bussy2012transportation,bussy2012proactive}.

In this work, we leverage the full mobility of a mobile base and explicitly account for obstacles while negotiating a strategy of workspace traversal. We develop a control framework that enables the robot to flexibly negotiate with the user using implicit communication, realized via subtle signals encoded in velocities transmitted to the transported object. In contrast to approaches directly learning traversal policies from human demonstrations~\citep{ng2023takes}, our approach has the potential to handle diverse maps by reasoning about the mapping between team actions and traversal strategies. While prior work assumes heavily instrumented setups including sensorized objects~\citep{mielke2024comanipulation,bussy2012proactive,bussy2012transportation,gienger2018contactchanges}, we use a commodity mobile manipulator (Hello Robot Stretch) and limited additional instrumentation (a motion capture system to eliminate the influence of localization errors). Finally, we move beyond prior work in terms of validation depth by conducting an extensive lab study ($N=24$) with in-depth insights via objective and subjective measures.


\subsection{Implicit Communication}

Any deviation from a robot's expected behavior has the potential to convey important, task-related information to an observer~\citep{knepper2017implicit}. This idea, rooted in conversational implicature~\citep{grice1975logic} has deep relevance to HRI~\citep{urakami2023nonverbal,hoffman2024inferring,venture2019expressive}. The community has long been exploring the expressive power of modalities like motion~\citep{takayama2011expressing}, gaze~\citep{admoni2014deliberate}, or gestures~\citep{carter2014catch}. Researchers have been building algorithmic frameworks that harness implicit communication to engineer fluent human-robot teamwork. \citet{knepper2017implicit} formalize implicit communication as the minimization of the information entropy over a probability distribution of possible goals. Considering a social navigation domain, \citet{mavrogiannis2022socialmomentum} implement implicit communication into a navigation controller that conveys the robot's intended passing side to co-navigating users. Considering a coverage task, \citet{walker2021influencing} implement implicit communication into the generation of robot motion that influences an observer to summarize robot motion in a desirable way. In a manipulation domain, \citet{DraganAuR14} instantiate implicit communication as \emph{Legibility}, a property of robot motion that enables an observer to infer the target object of a manipulator's reaching motion. Considering the collaborative game Hanabi, \citet{liang2019implicit} demonstrate that implicit communication can be a powerful tool for teams of humans and artificial agents working on a joint task.

Prior algorithmic instantiations of implicit communication assume simplified teamwork settings. For instance, the user is often passively observing the robot~\citep{DraganAuR14,walker2021influencing}, navigating next to the robot~\citep{mavrogiannis2022socialmomentum}, or working with a non-embodied agent~\citep{liang2019implicit}. In contrast, this work integrates implicit communication into physical human-robot collaboration through the task of collaborative transport. This task uniquely integrates the user and the robot as embodied, dynamic observers and actors, whose actions influence each other and impact the quality of the teamwork. We define a space of goals, corresponding to strategies of workspace traversal, which we identify using a notion of topological invariance~\citep{mavrogiannis2023winding}. We then develop a probabilistic inference that maps joint human-robot actions to strategies of workspace traversal. By tracking the entropy of the distribution over strategies, the robot may monitor the state of uncertainty over the unfolding traversal. By introducing entropy as a cost into a model predictive controller (IC-MPC), the robot is capable of balancing functional and communicative actions resulting in efficient, fluent, and positively perceived human-robot teamwork.
