\clearpage

\setcounter{page}{1}
\appendix
\onecolumn % Switch to single-column mode
%maketitlesupplementary
\begin{center}
\Large
\textbf{HuViDPO: Enhancing Video Generation through Direct Preference
Optimization for Human-Centric Alignment} \vspace{0.5em} Supplementary Material 
\end{center}



\section{Formula Derivation for DPO}
This section provides a structured derivation of the Direct Preference Optimization (DPO) formula. First, the technical background necessary to understand the DPO approach is presented, focusing on the concepts and preliminary models used. Next, the objective function of DPO is detailed, deriving it based on the reward function and KL divergence to establish a preference-based optimization framework.

\subsection{Technical Background}
The core idea of Direct Preference Optimization (DPO) is to utilize preference data directly for policy optimization, avoiding the dependency on reward model-based methods. Given a prompt \( x \), two responses \( (y_1, y_2) \sim \Pi_{\text{SFT}}(y | x) \) are generated. By manually annotating, the preferences of \( y_1 \) and \( y_2 \) are compared, resulting in a preference outcome \( y_w \succ y_l | x \), where \( w \) and \( l \) denote the "win" and "lose" responses, respectively.

To formalize preference probabilities, a reward model \( r \) is introduced such that for two generated responses \( y_1 \) and \( y_2 \), the probability of \( y_1 \) being preferred over \( y_2 \) is expressed as:

\begin{equation}
p(y_1 > y_2) = \frac{r^*(x, y_1)}{r^*(x, y_1) + r^*(x, y_2)}.
\end{equation}

Here, the reward function \( r^*(x, y) \) quantifies the quality of response \( y \) relative to prompt \( x \), and the probability \( p(y_1 > y_2) \) reflects the likelihood of choosing \( y_1 \) over \( y_2 \) based on this reward.

To ensure positive values in the reward model, the Bradley-Terry model is applied, representing the preference probability \( p(y_w \succ y_l | x) \) as:

\begin{equation}
p^*(y_w \succ y_l | x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))}.
\end{equation}

This exponential form stabilizes the reward values and maintains positive preference probabilities.

\subsection{DPO Objective Function}
The goal of DPO is to maximize rewards while aligning the policy with a baseline model. The objective function is defined as:

\begin{equation}
\max_\pi \mathbb{E}_{x \in X, y \in \pi} \left[ r(x, y) \right] - \beta \cdot \mathbb{D}_{\text{KL}} \left[ \pi(y | x) || \pi_{\text{ref}}(y | x) \right],
\end{equation}

where \( D_{\text{KL}} \) denotes the Kullback-Leibler divergence between the learned policy \( \pi \) and a reference policy \( \pi_{\text{ref}} \), enforcing consistency with the baseline model. To simplify, this objective is reformulated as a minimization problem:

\begin{equation}
\min_\pi \mathbb{E}_{x \in X, y \in \pi} \left[ \log \frac{\pi(y | x)}{\pi^*(y | x)} - \log Z(x) \right],
\end{equation}

where \( Z(x) \) is defined as:

\begin{equation}
Z(x) = \sum_y \pi_{\text{ref}}(y | x) \exp \left( \frac{1}{\beta} r(x, y) \right).
\end{equation}

This reformulation leads to the final optimization objective:

\begin{equation}
\min_\pi \mathbb{E}_{x \sim D} \left[ \mathbb{D}_{\text{KL}}(\pi(y | x) || \pi^*(y | x)) \right].
\end{equation}

Under the minimization of KL divergence, the policy \( \pi(y | x) \) adheres to the following form:

\begin{equation}
\pi(y | x) = \pi^*(y | x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y | x) \cdot \exp \left( \frac{1}{\beta} r(x, y) \right).
\end{equation}

Reversing this equation yields the reward function:

\begin{equation}
r^*(x, y) = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}.
\end{equation}

Incorporating the Bradley-Terry model, the cross-entropy loss function \(\mathcal{L}\) is defined, which quantifies the difference between the preferred and non-preferred responses. This loss function is essential for deriving the gradient necessary to optimize the DPO objective:

\begin{equation}
\mathcal{L} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \ln \sigma \left( \beta \log \frac{\pi(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right],
\end{equation}

where \( \sigma \) denotes the sigmoid function, which maps the difference in log-probabilities to a range of \([0, 1]\). Differentiating \(\mathcal{L}\) provides the gradient needed to optimize the DPO objective with respect to the preference data.


\section{Applying DPO Strategy into T2I tasks}

In adapting Dynamic Preference Optimization (DPO) to text-to-image tasks (T2I tasks), we consider a setting with a fixed dataset \( D = \{(c, x_0^w, x_0^l)\} \). In this dataset, each example contains a prompt \( c \) and a pair of images \( (x_0^w, x_0^l) \) generated by a reference model \( p_{\text{ref}} \), where \( x_0^w \succ x_0^l \) indicates that humans prefer \( x_0^w \) over \( x_0^l \). The goal is to train a new model \( p_\theta \) so that its generated images align with human preferences, rather than merely imitating the reference model. However, directly computing the distribution \( p_\theta(x_0 | c) \) is highly complex, as it requires marginalizing over all possible generation paths \( (x_1, \ldots, x_T) \) to produce \( x_0 \), which is practically infeasible.

To address this challenge, researchers leverage Evidence Lower Bound (ELBO) by introducing latent variables \( x_{1:T} \). The reward function \( R(c, x_{0:T}) \) is defined to measure the quality of the entire generation path, allowing the expected reward \( r(c, x_0) \) for given \( c \) and \( x_0 \) to be formulated as:

\begin{equation}
r(c, x_0) = \mathbb{E}_{p_\theta(x_{1:T} | x_0, c)} [R(c, x_{0:T})]
\end{equation}

In DPO, a KL regularization term is also included to constrain the generated distribution relative to the reference distribution. Here, an upper bound on the KL divergence is used, converting it to a joint KL divergence:

\begin{equation}
\mathbb{D}_{KL}[p_\theta(x_{0:T} | c) \parallel p_{\text{ref}}(x_{0:T} | c)]
\end{equation}

This upper bound ensures that the distribution of the generated model \( p_\theta(x_{0:T} | c) \) remains consistent with the reference model \( p_{\text{ref}}(x_{0:T} | c) \), preserving the modelâ€™s generation capabilities while optimizing human preference alignment. Plugging in this KL divergence upper bound and the reward function \( r(c, x_0) \) into the objective function, we obtain:

\begin{equation}
\max_{p_\theta} \mathbb{E}_{c \sim \mathcal{D}_c, x_{0:T} \sim p_\theta(x_{0:T} | c)} [r(c, x_0)] - \beta \mathbb{D}_{KL} [p_\theta(x_{0:T} | c) \parallel p_{\text{ref}}(x_{0:T} | c)]
\end{equation}

This objective function resembles the structure of Equation (5) but is defined over the path \( x_{0:T} \). Its primary goal is to maximize the reward for the reverse process \( p_\theta(x_{0:T}) \) while maintaining distributional consistency with the original reference reverse process. To optimize this objective, the conditional distribution \( p_\theta(x_{0:T}) \) is directly used. The final loss function \( L_{\text{DPO-T2I}}(\theta) \) is expressed as follows:

\begin{equation}
\mathcal{L}_{\text{Image}}(\theta) = -\mathbb{E}_{(x_0^w, x_0^l) \sim D} \log \sigma \left( \beta \mathbb{E}_{x_{1:T}^w \sim p_\theta(x_{1:T} | x_0^w), \, x_{1:T}^l \sim p_\theta(x_{1:T} | x_0^l)} \left[ \log \frac{p_\theta(x_{0:T}^w)}{p_{\text{ref}}(x_{0:T}^w)} - \log \frac{p_\theta(x_{0:T}^l)}{p_{\text{ref}}(x_{0:T}^l)} \right] \right)
\end{equation}

By applying Jensen's inequality, the expectation can be moved outside of the \(\log \sigma\) function, resulting in an upper bound. This simplifies the formula and can facilitate optimization. After applying Jensen's inequality, the loss function's upper bound is given by:

\begin{equation}
\mathcal{L}_{\text{Image}}(\theta) \leq -\mathbb{E}_{(x_0^w, x_0^l) \sim D} \, \mathbb{E}_{x_{1:T}^w \sim p_\theta(x_{1:T} | x_0^w), \, x_{1:T}^l \sim p_\theta(x_{1:T} | x_0^l)} \, \log \sigma \left( \beta \left[ \log \frac{p_\theta(x_{0:T}^w)}{p_{\text{ref}}(x_{0:T}^w)} - \log \frac{p_\theta(x_{0:T}^l)}{p_{\text{ref}}(x_{0:T}^l)} \right] \right)
\end{equation}


The final loss function maximizes the reward \( r(c, x_0) \) aligned with human preference while ensuring that \( p_\theta \) stays consistent with the reference distribution \( p_{\text{ref}} \) through the KL regularization term. This effectively aligns human preferences in the T2I task's DPO process, enabling the model not only to generate high-quality images but also to reflect human visual preferences and intuitive aesthetics.



\section{Details of User Study}

\subsection{Objective}
To evaluate the performance of the proposed T2V method, we conduct this user study and compare the generated results with three classic open-source T2V methods: LVDM, AnimateDiff, and LAMP. The evaluation focuses on three key criteria: Video-Text Alignment, Visual Quality, and Frame Consistency.

\subsection{Methodology}

\textbf{Video Generation.} We select a set of 14 prompts to generate videos, including dynamic content featuring fireworks, waterfall, smile, horse, guitar, birds flying, helicopter, and rain, covering a total of 8 types. For each prompt, videos are produced using four models. This process resulted in a total of 56 videos.

\vspace{0.1cm}
\noindent\textbf{Study Procedure.} Each participant is asked to evaluate a series of videos. For each video, participants are instructed to provide three separate scores (each ranging from 1 to 5, with 1 being the lowest and 5 being the highest) based on the following criteria:

\begin{itemize}
    \item \textbf{Video-Text Alignment:} Does the video accurately reflect the description?
    \begin{itemize}
        \item  1 point: The video does not align with the description; it is almost impossible to recognize elements from the text in the video.
        \item  3 points: The video partially aligns with the description, showing some relevant content, but with noticeable mismatches or missing details.
        \item  5 points: The video perfectly aligns with the description, accurately reflecting all key elements of the text.
    \end{itemize}

    \item \textbf{Visual Quality:} How clear and visually appealing is the video?
    \begin{itemize}
        \item  1 point: The video is blurry, with severe pixelation or visual defects.
        \item  3 points: The video quality is acceptable; the main content is recognizable, though some details are rough.
        \item  5 points: The video quality is excellent, with high clarity and well-represented details.
    \end{itemize}

    \item \textbf{Frame Consistency:} How smooth and seamless are the transitions between frames?
    \begin{itemize}
        \item  1 point: Frames are highly inconsistent, appearing jumpy or choppy, disrupting the viewing experience.
        \item  3 points: Frames are somewhat consistent, but transitions have some roughness or slight jumps.
        \item  5 points: Frames are consistently smooth, delivering a natural and seamless viewing experience.
    \end{itemize}
\end{itemize}

Participants are instructed to score each video independently and provide honest feedback.

\textbf{Data Collection.} We collect test data using an online form. Scores for each criterion are averaged across participants and videos. Statistical analysis is then conducted based on the data.

\subsection{Results Interpretation}
The results are shown in ~\cref{tab:example2}. Due to file size limitations for submissions, we are temporarily unable to publicly share the specific video examples and their scoring details involved in the user study.

\section{Experiment Result}

This section demonstrates the powerful generative capabilities of the HuViDPO method, which can produce high-quality, human-preferred videos across various visual styles, including pixel art, Claymation, cartoon, and realistic styles. \textcolor[HTML]{94CB6B}{\textit{\textbf{Green}}} highlights indicate the subject category, \textcolor[HTML]{18D8FA}{\textit{\textbf{blue}}} highlights represent motion-related information, and \textcolor[HTML]{E3954F}{\textit{\textbf{orange}}} highlights denote the background.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Appendix1.png}
    \caption{Pixel-art-style video generated by our method.}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Appendix2.png}
    \caption{Claymation-style video generated by our method.}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Appendix3.png}
    \caption{Realistic-style video generated by our method.}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Appendix4.png}
    \caption{Cartoon-style video generated by our method.}
\end{figure*}

