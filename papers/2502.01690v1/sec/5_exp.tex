\section{Experiment}


%-------------------------------------------------------------------------
\subsection{Implementation Detail}

Our training was conducted on a single 24G RTX4090 GPU across eight small-scale human preference datasets, including categories like waterfall, smile and guitar. We fine-tuned the SparseCausal-Attention module, Temporal-Attention module and Temporal-Spatial layers with a learning rate of 3e-5, batch size of 1, and training iters of 25000. Based on~\cref{lab:2}, the loss was calculated with a learning rate of 1e-4, along with weight decay and epsilon adjustments to prevent gradient issues. 

In comparing with baselines, we use standard evaluation metrics such as CLIPscore~\cite{radford2021learning}, SSIM~\cite{wang2004image}, and MSE, benchmarking our results against LVDM~\cite{he2022latent}, AnimateDiff~\cite{guo2023animatediff}, and LAMP~\cite{wu2023lamp}. To further highlight our approach's superiority, we visually present the quality of videos generated by different methods under identical prompts, allowing a direct comparison that underscores our methodâ€™s advantages. Additionally, we conducted a user study comparing our approach with baselines to showcase its effectiveness. Finally, an ablation study tested the importance of each module designed in ~\cref{sec:2}, reinforcing the scientific rigor of our approach.


%-------------------------------------------------------------------------


\begin{table}[]
\centering
    {
    \setlength{\extrarowheight}{2pt}
    \scalebox{0.72}{
        \begin{threeparttable}
        \begin{tabular}{@{}l@{\hskip 4pt}l@{\hskip 2pt}c@{\hskip 2pt}c@{\hskip 2pt}c@{\hskip 2pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{}}
            \toprule
                Metric                & Method                  & waterfall            & smile         & helicopter      & rain          & firework      & guitar      & All        \\ \midrule
                \multirow{4}{*}{CLIPscore $\mathrel{\uparrow}$} 
                & LVDM~\cite{he2022latent}              & 31.1         & 26.1          & 31.8       & 30.4            & 29.3          & 33.0       & 30.3          \\
                & AnimateDiff~\cite{guo2023animatediff}           & \underline{34.1}          & 27.7          & 29.4        & 31.3            & 31.8            & 31.8        & 31.0          \\
                & LAMP~\cite{wu2023lamp}             & 33.6          & \textbf{28.9}          & \underline{33.0}        & \textbf{33.2}            & \textbf{34.0}         & \underline{35.8}        & \textbf{33.1}          \\
                & \textbf{Ours}    & \textbf{34.9}          & \underline{28.1}       & \textbf{33.1}            & \underline{31.9}            & \underline{32.5}        & \textbf{37.2}  & \underline{33.0}        \\ \midrule

                \multirow{4}{*}{SSIM $\mathrel{\uparrow}$} 
                & LVDM~\cite{he2022latent}     & 73.1          & 54.4          & 52.0          & 67.8            & 55.1          & 65.4          & 61.3            \\
                & AnimateDiff~\cite{guo2023animatediff}     & 82.1          & 84.8          & 71.2          & 82.0          & 65.7          & 87.7          & 78.9          \\
                & LAMP~\cite{wu2023lamp}    & \textbf{95.2}          & \underline{88.1}          & \underline{81.3}          & \textbf{93.3}          & \textbf{88.0}          & \underline{93.9}            & \underline{89.9}            \\
                & \textbf{Ours} & \underline{89.3} & \textbf{92.2} & \textbf{91.6} & \underline{91.0} &\underline{86.6} & \textbf{95.8} & \textbf{91.1} \\ \midrule

                \multirow{4}{*}{MSE $\mathrel{\downarrow}$} 
                & LVDM~\cite{he2022latent}     & 62.0          & 68.0          & 66.6            & 62.7          & 82.0          & 61.3          & 67.1          \\
                & AnimateDiff~\cite{guo2023animatediff}     & 33.8          & 32.2          & 51.6          & 32.5          & 52.5          & 20.1          & 37.1          \\
                & LAMP~\cite{wu2023lamp}    & \textbf{12.2}          & \underline{25.1}            & \underline{31.5}          & \underline{12.1}          & \underline{25.9}          & \underline{14.4}            & \underline{20.2}          \\
                & \textbf{Ours} & \underline{24.3} & \textbf{17.1} & \textbf{24.1} & \textbf{8.1} & \textbf{24.8} &  \textbf{13.1} & \textbf{18.6} \\

            \bottomrule
        \end{tabular}
        
        \end{threeparttable}
        }
    }
\caption{Quantitative comparisons with baselines. Our method surpasses all baselines in SSIM and MSE, demonstrating superior inter-frame consistency. Bold font indicates the best performance, while underlined font indicates the second best.}
\label{tab:4}
\end{table}


\begin{table}
  \centering
  \setlength{\extrarowheight}{1pt}
  \scalebox{0.85}{
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Method &  Aesthetics $\mathrel{\uparrow}$ & Consistency $\mathrel{\uparrow}$ & Alignment $\mathrel{\uparrow}$ \\
    \midrule
    LVDM~\cite{he2022latent} & 45.4& 48.3& 55.8\\
    AnimatedDiff~\cite{guo2023animatediff} & 67.6& \underline{61.1}& 72.4\\
    LAMP~\cite{wu2023lamp} & \underline{72.5}& 60.8& \underline{75.6}\\
    \textbf{Ours} & \textbf{83.9}& \textbf{75.1}& \textbf{85.1}\\
    \bottomrule
  \end{tabular}
  
  }
  \caption{User preference comparison. This table demonstrates our significant superiority in aspects such as Aesthetics, Consistency, and Alignment, in comparison to LVDM~\cite{he2022latent}, AnimateDiff~\cite{guo2023animatediff}, and LAMP~\cite{wu2023lamp}. Bold font indicates the best performance, while underlined font indicates the second best.}
  \label{tab:example2}
\end{table}


\begin{figure*}[t]

  \centering
  \includegraphics[width=\textwidth]{vision2.png}

   \caption{Qualitative comparison with SD-v1.4~\cite{rombach2022high}, SD-v1.5 and SD-XL~\cite{podell2023sdxl}. The videos generated by our method offer richer visual effects and stronger text-to-video alignment. In the left image, the generated video aligns better with the prompt, and the waterfall exhibits enhanced fluidity and structural consistency. In the right image, the video appears more realistic, natural, and smooth.}
   \label{fig:7}
\end{figure*}



\subsection{Comparison with Baseline}

\noindent\textbf{Quantitative Result.} In this experiment, We set up 10 identical prompts for each category, generating 60 videos per baseline for evaluation. CLIPscore was used to measure alignment with prompts, where higher values indicate better alignment. We also calculated SSIM and MSE for spatiotemporal consistency, with higher SSIM and lower MSE scores preferred. As shown in~\cref{tab:4}, our method outperforms LVDM and AnimateDiff on CLIPscore, indicating strong text alignment. Additionally, HuViDPO achieved higher SSIM (1.2\% above LAMP, 29.8\% and 12.2\% over LVDM and AnimateDiff) and lower MSE (1.6\% below LAMP, 48.5\% and 18.5\% below LVDM and AnimateDiff), confirming its robust spatiotemporal consistency.

\begin{figure}[!h]
  \centering
    \includegraphics[width=\linewidth, height=150px]{table.png}

   \caption{Comparison between our First-Frame Generation strategy and the SD-v1.4~\cite{rombach2022high}, SD-v1.5, and SD-XL~\cite{podell2023sdxl} methods. HuViDPO outperforms SD-v1.4~\cite{rombach2022high} and SD-v1.5 on both metrics, but scores slightly lower on MSE compared to SD-XL~\cite{podell2023sdxl}. The slight discrepancy in MSE is primarily due to the larger motion amplitudes in the videos, but this does not affect the overall visual effectiveness of the videos.}
   \label{fig:6}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=\linewidth]{vision4.png}
  \caption{Qualitative comparison with methods without DPO Fine-tune. Details marked by the red box are magnified and displayed on the right side of the image. The above example shows that without DPO fine-tuning, the horse's legs appear significantly deformed. The below example demonstrates that with DPO fine-tuning, the man's smile process is more natural, revealing details of his teeth compared to without DPO fine-tuning.}
  \label{fig:9}
\end{figure}


\noindent\textbf{Qualitative Result.} First, we used a unified prompt input across different methods to generate videos, with example outputs shown in~\cref{fig:5}. These outputs clearly demonstrate that videos generated by HuViDPO exhibit greater flexibility, higher visual quality, and align more closely with human aesthetic preferences. Next, to address the limitations of existing evaluation metrics and further validate that videos generated by HuViDPO better meet human aesthetic standards, we conducted a large-scale user study. In total, 58 participants, including both experts and non-experts, evaluated videos from four different methods across 14 diverse cases. The participants rated each video on aesthetics, spatiotemporal consistency, and text alignment from 1 to 5, with the final aggregated scores converted to percentages (see Appendix for details on experimental setup). As shown in~\cref{table:2}, our method outperformed all baselines across every criterion, strongly indicating that the video animations generated by our approach better align with human preferences.


%-------------------------------------------------------------------------
\begin{figure}[!h]
  \centering
  
  \includegraphics[width=\linewidth]{vision3.png}
  \caption{Qualitative comparison with unmodified SparseCausal-Attention. Red boxed details are shown magnified on the right. In the upper example, unmodified SparseCausal-Attention makes the horse's legs vanish, while our method preserves natural leg motion. In the lower example, the same unmodified mechanism distorts the road surface, ruining structural coherence, whereas our method keeps the road smooth.}
  \label{fig:8}
\end{figure}


\subsection{Ablation Study}


\noindent\textbf{Effect of the DPO-Based Fine-tuning Strategy.}
DPO-Based Fine-tuning is a crucial part of our overall strategy. Our method significantly reduces abrupt transitions between different video frames across most video categories. As shown in~\cref{fig:9}, this experiment selected two categories, smile and horse running, to visually illustrate the advantages of DPO-Based Fine-tuning. This improvement enhances the fluidity and aesthetic quality of the videos, providing a more cohesive viewing experience. Additionally, the fine-tuning process effectively aligns the generated content with human preferences, making the video transitions smoother and visually more appealing. This alignment not only enhances the visual experience but also ensures that the content resonates more strongly with viewers.


\noindent\textbf{Effect of the First-Frame-Conditioned Strategy.}
The first frame of a video contains essential semantic and content information that effectively guides the generation of subsequent frames, playing a significant role in video generation tasks. By using DPO-SDXL for first-frame generation, we ensure that the resulting videos exhibit greater flexibility and diversity. The superiority of our method is evident in~\cref{fig:6}. Notably, while our approach scores slightly worse than SD-XL on the MSE metric, this is primarily due to the larger motion amplitudes in videos generated by our method, which means this metric does not fully capture the aesthetic quality. To further substantiate and provide an intuitive assessment of the effectiveness of our First-Frame Generation strategy, we selected two specific examples in~\cref{fig:7} to visually showcase the differences between SD-v1.4, SD-v1.5, the SD-XL method, and our first-frame generation strategy. The results indicate that our strategy outperforms the others in visual quality. Additionally, to further demonstrate the flexibility, diversity, and adaptability of our generated videos, we invite you to explore more examples in the supplementary material for a comprehensive understanding.



\vspace{0.05cm}
\noindent\textbf{Effect of the Improved SparseCausal-Attention Mechanism.} 
SparseCausal-Attention is a key module in our model, effectively enhancing the spatiotemporal consistency and quality of the generated videos. This module is crucial for maintaining continuity between frames. As shown in~\cref{fig:8}, compared to the SparseCausal-Attention in the base model LAMP, our improved module further strengthens information retention between frames. This ensures smoother transitions that align better with human aesthetic preferences. As a result, the generated videos are not only more visually appealing but also exhibit higher quality.


\section{Conclusion}

In this study, we are the first to introduce DPO strategies into T2V tasks. By designing a comprehensive loss function, we utilize human feedback to align video generation with human aesthetic preferences, naming it \textbf{HuViDPO}. Additionally, we constructed small-scale human preference datasets for each action category and employed LoRA to fine-tune the model, significantly improving the aesthetic quality of the generated videos while reducing training costs. Regarding resource efficiency, our setup allows each fine-tuning process for an action category to complete in under a day on a single 24GB RTX4090 GPU. Finally, to enhance the quality and flexibility of the generated videos, we adopted a First-Frame-Conditioned strategy, utilizing the rich information provided by the first frame to guide the generation of subsequent frames, and integrated a SparseCausal-Attention mechanism to further improve video quality and spatiotemporal consistency, ensuring a smoother and more coherent video generation process.