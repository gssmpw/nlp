\section{Method}
\label{sec:2}

In this section, we first introduce the derivation of the loss function, which is used to apply the DPO strategy to T2V tasks in~\cref{sec:q}. Next, we provide a detailed description of the DPO-Based Fine-tuning strategy by using small-scale human preference datasets in~\cref{sec:w}. Then, we detail the First-Frame-Conditioned strategy and the proposed SparseCausal-Attention mechanism in~\cref{sec:e}. Finally, in~\cref{sec:r}, we introduce the inference process of HuViDPO and its powerful generation capabilities.

\begin{figure*}[t]
  \centering
    \includegraphics[width=\textwidth,height=0.4\textheight]{pipeline.png}

   \caption{Training pipeline of our HuViDPO. Training process can be divided into two stages: (a) Training the Attention Block and Temporal-Spatial layers using basic training settings to improve the spatiotemporal consistency. (b) Fine-tuning the model, with LoRA added and other layers frozen, using small-scale human preference datasets and DPO strategy to enhance its alignment with human preferences. In phase (b), $loss_w$ and $loss_l$ denote the loss values computed by inputting winning and losing videos into the fine-tuned model, while $loss_{wref}$ and $loss_{lref}$ are the loss values obtained by inputting the same videos into the reference model.}
   \label{fig:2}
\end{figure*}

\subsection{Integrating DPO into T2V Tasks via Loss Function Design}
\label{sec:q}

%\textbf{Preliminaries for HuViDPO.}
In this section, we provide a detailed derivation of our unique loss function. We extend the DPO strategy's loss function from image generation to video generation. The detailed derivation of this strategy for T2I tasks is provided in the appendix, and the loss is defined as:

\begin{equation}
\label{eq:1}
\mathcal{L}_{\text{Image}}(\theta) = -\mathbb{E}_{(x_{0}^w, x_{0}^l) \sim \mathcal{D}} \log \sigma \left( \beta \left( \Delta(x_{0:T}^w) - \Delta(x_{0:T}^l) \right) \right),
\end{equation}

where \(x_0^w\) and \(x_0^l\) are the winning and losing image samples, \(\beta\) is a hyperparameter, and  indicates the corresponding time step, and \(\Delta(x)\) is defined as:

\begin{equation}
\label{eq}
\Delta(x) = \log \frac{p_\theta(x)}{p_{\text{ref}}(x)}.
\end{equation}


Based on~\cref{eq:1,eq}, the DPO loss function for the T2V task, where \( v_0^w \) and \( v_0^l \) are the winning and losing video samples, is defined as:

\begin{equation}
\label{eq:3}
\mathcal{L}_{\text{Video}}(\theta) = -\mathbb{E}_{(v_0^w, v_0^l) \sim \mathcal{D}} \log \sigma \left( \beta \left( \Delta(v_{0:T}^w) - \Delta(v_{0:T}^l) \right) \right).
\end{equation}


To handle the complexity of calculating high-dimensional video sequence probabilities with a total of \( T = 1000 \) time steps, we employ an approximation approach. We introduce an approximate posterior \( q(v_{1:T} | v_0) \) for the subsequent time steps and utilize the Evidence Lower Bound (ELBO)~\cite{kingma2013auto,hoffman2013stochastic} to approximate $\log p_\theta(v_{0:T})$. Then, by expressing \( p_\theta(v_{0:T}) \) and \( q(v_{1:T} | v_0) \) as products of conditional probabilities at each time step, we achieve a stepwise sampling approach. The final approximate expression is:

\begin{equation}
\label{KL1}
\begin{split}
\log p_\theta(v_{0:T}) \approx \mathbb{E}_{q(v_t | v_{t-1}), t \sim \{1..T\}} 
\Bigg[ \log \frac{p_\theta(v_0)}{q(v_0)} \\
+ \log \frac{p_\theta(v_t | v_{t-1})}{q(v_t | v_{t-1})} \Bigg].
\end{split}
\end{equation}


Since \( q(v_t | v_{t-1}) \) is a conditional probability distribution that generally sums to 1, the KL divergence~\cite{kingma2013auto,blei2017variational} can be expressed as:

\begin{equation}
\label{KL2}
\mathbb{D}_{KL}\left( q(v_t | v_{t-1}) \, \| \, p_\theta(v_t | v_{t-1}) \right) =  \log \frac{q(v_t | v_{t-1})}{p_\theta(v_t | v_{t-1})}.
\end{equation}


Based on~\cref{KL1,KL2}, we rewrite \( \log p_\theta(v_{0:T}) \) as:

\begin{equation}
\label{eq:6}
\begin{split}
\log p_\theta(v_{0:T}) \approx \mathbb{E}_{q(v_{1:T} | v_0), t \sim \{1..T\}} \left[ \log \frac{p_\theta(v_0)}{q(v_0)} \right] \\ 
- \mathbb{D}_{KL}\left( q(v_t | v_{t-1}) \, \| \, p_\theta(v_t | v_{t-1}) \right).
\end{split}
\end{equation}



Moreover, the derivation of \( \log p_{\text{ref}}(v_{0:T}) \) is consistent with that of \( \log p_\theta(v_{0:T}) \). Based o~\cref{eq,eq:6}, we can rewrite \( \Delta(v_{0:T}) \) as

\begin{equation}
\label{eq:7}
\Delta(v_{0:T}) = -\mathbb{D}_{KL}^{\theta} + \mathbb{D}_{KL}^{\text{ref}} + C.
\end{equation}

By rewriting the KL divergence in terms of noise prediction, we can express it as follows:

\begin{equation}
\label{eq:9}
\mathbb{D}_{KL}^{\theta} \propto ||\epsilon - \epsilon_\theta(v_t, t)||^2 
\quad \mathbb{D}_{KL}^{\text{ref}} \propto ||\epsilon - \epsilon_\text{ref}(v_t, t)||^2.
\end{equation}


%where $\epsilon$ is the true noise and $\epsilon_\theta$ is the predicted noise.

Finally, based on~\cref{eq:3,eq:7,eq:9}, the complete form of the DPO loss function for T2V task is:

\begin{equation}
\label{lab:2}
\begin{aligned}
\mathcal{L}_{\text{Video}}(\theta) = & \, \mathbb{E}_{(v_{0}^w, v_{0}^l) \sim \mathcal{D}, \, t \sim \{1..T\}} \Bigg[\beta \log \sigma \Bigg( \\
& \quad \left( ||\epsilon_w - \epsilon_\theta(v^w_t, t)||^2 - ||\epsilon_w - \epsilon_{\text{ref}}(v^w_t, t)||^2 \right) \\
& \quad - \left( ||\epsilon_l - \epsilon_\theta(v^l_t, t)||^2 - ||\epsilon_l - \epsilon_{\text{ref}}(v^l_t, t)||^2 \right) \Bigg) \Bigg].
\end{aligned}
\end{equation}





\subsection{DPO-Based Fine-tuning strategy}
\label{sec:w}

Our training process can be roughly divided into two steps, as shown in~\cref{fig:2}. First, within the standard VDM architecture, we train using a small video dataset and fine-tuning the Attention Block and Temporal-Spatial layers to ensure spatiotemporal consistency in the generated videos. It is important to note that during training, the first frame is not subjected to noise injection, while the other frames are generated through DDIM inversion~\cite{song2020denoising} from the original training data. Seen as~\cref{fig:7}, since the first frame contains most of the key information in the short video, it plays a crucial role in guiding the generation of subsequent frames, ensuring diversity in video generation, while our method enhances the visual aesthetics of the generated images.

Next, we conduct human preference scoring on videos used for DPO strategy training to evaluate each video against aesthetic standards. After scoring, the dataset is shuffled, noted as \( V_{\text{shuffle}} = \text{shuffle}(V) \), and a random pair \( (V_a, V_b) \) is selected as a training sample. The loss function, defined as~\cref{lab:2}, incorporates the DPO strategy into the VDM loss to align video generation with human preferences. During training, a reference model with frozen weights and a training model with an added LoRA module are used. Both models generate video sequences guided by the same prompt and first-frame image, and the loss from~\cref{lab:2} updates the LoRA weights through backpropagation, enhancing video alignment with human preferences and improving quality.

It is worth noting that, due to our DPO-Based Fine-tuning strategy, training time and computational resources are greatly reduced, with each action category requiring less than a day for the above two training processes.



\subsection{Enhancing Video Flexibility and Quality}
\label{sec:e}

\noindent\textbf{First-Frame-Conditioned Generation using DPO-SDXL.}
Due to the limited paired human preference video training data, the model risks losing flexibility in video generation and lacking rich semantic expression. To address this issue, we decouple content and motion. From our general understanding, the first frame of a video contains the majority of the content information of a short video. Therefore, guiding the generation of a short video with a high-quality first frame image that contains rich semantic information can effectively address the problems of losing flexibility and lacking rich semantic expression in video generation. In our method setup, we use DPO-SDXL~\cite{wallace2024diffusion} to generate the first frame of the video to guide the generation of subsequent video frames, as shown in~\cref{fig:4}. This method not only allows for the generation of videos based on prompts of various styles, ensuring flexibility in generation, but also aligns the video production more closely with human aesthetic standards, thereby achieving an optimal balance between maintaining flexibility and the lack of datasets.


\begin{figure}[t]
  \centering
    % 绘制矩形框
    \includegraphics[width=\linewidth]{pipeline3.png}

   \caption{The details of the proposed SparseCausal-Attention Mechanism. We extract $K/V$ tokens from the first frame and the $i-1$ frame, and compute the attention mechanism with the $Q$ of the $i$ frame.}
   \label{fig:3}
\end{figure}


\vspace{0.1cm}
\noindent\textbf{Improved SparseCausal-Attention Mechanism.}
To further address the spatiotemporal inconsistency issue in the few-shot T2V generation tasks and to improve the overall quality of video generation, our method designs and enhances the SparseCausal-Attention module. In our approach, for each frame $i$ in the video, we extract key/value \((K/V)\) information from both the first frame and the previous frame $i-1$, using the query \((Q)\) information of the current frame $i$ to compute the required attention mechanism~\cite{xing2024simda,wang2023crossformer++}. The attention calculation is formulated as follows:

\begin{equation}
\text{Attention}(Q_i, K, V) = \text{Softmax}\left(\frac{Q_i K_{\text{concat}}^\top}{\sqrt{d_k}}\right) V_{\text{concat}},
\label{eq:attention_calculation}
\end{equation}

where \(K_{\text{concat}}\) and \(V_{\text{concat}}\) represent the concatenated keys and values, defined as:

\begin{equation}
K_{\text{concat}} = [K_0; K_{i-1}] \quad V_{\text{concat}} = [V_0; V_{i-1}].
\label{eq:concat_key}
\end{equation}

As shown in~\cref{fig:8}, compared to methods without the improved SparseCausal-Attention module, this enhancement maintains frame consistency in the generated videos under equivalent conditions, leading to outputs that better align with human aesthetic standards and improve video quality. 
%Additionally, this method significantly improves computational efficiency, enabling efficient training even in resource-constrained environments.



\begin{figure}[!h]
  \centering
    % 绘制矩形框
    \includegraphics[width=\linewidth]{pipeline2.png}
    % 图片占位符
    % \includegraphics[width=\linewidth]{table.png}

   \caption{Inference Process of our HuViDPO. We first employ DPO-SDXL to create a diverse style-first frame, then concatenate it with other noise frames, which are then inputted into a trained model to produce the video output.}
   \label{fig:4}
\end{figure}


\subsection{Inference Process of HuViDPO}
\label{sec:r}

Similar to the training process, the inference process can also be divided into two steps. As shown in~\cref{fig:4}, we first use DPO-SDXL to generate the first-frame image based on the corresponding text prompt. This image aligns better with human preferences and provides a richer diversity of styles. Next, the generated first-frame image is combined with other frames composed of Gaussian noise to form a video sequence. This sequence, along with the same text prompt, is then input into the model that has been fine-tuned twice as described in~\cref{sec:w}, to generate the final video output. The cases presented in this paper primarily come from eight different action categories and various video styles. ~\cref{fig:1} and the appendix provide an initial demonstration of the high quality and strong alignment with human aesthetics of videos generated by HuViDPO. Of course, more examples are available in the supplementary material we provided, allowing readers to intuitively experience the video quality. It is worth noting that HuViDPO offers a high degree of generation flexibility, allowing users to customize different action categories and randomly select desired video styles, making it highly practical.



\begin{figure*}[t]
  \centering
    \includegraphics[width=\textwidth,height=188pt]{baseline.png}

   \caption{Qualitative comparison with LVDM~\cite{he2022latent}, AnimateDiff~\cite{guo2023animatediff}, and LAMP~\cite{wu2023lamp}. The above images clearly demonstrate that the videos generated by our method exhibit better spatiotemporal consistency and are more visually aligned with human preferences. In example (a) of this image, video generated by our method exhibits richer content and stronger spatiotemporal consistency, such as the rainbow and the shape of the bridge remaining largely unchanged. In example (b) of this image, video generated by our method features a more reasonable layout and richer main subjects, such as more detailed character features and a softer background.} %Please refer to \href{https://tankowa.github.io/HuViDPO.github.io/}{\textit{Website}} for the best view.}
   \label{fig:5}
\end{figure*}


