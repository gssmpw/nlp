\section{Related work}
\label{sec:formatting}

\subsection{Text-to-Video Generation}

T2V generation has made notable progress, evolving from early GAN-based models \cite{saito2017temporal,tulyakov2018mocogan,fu2023tell,li2018video,wu2022nuwa,yu2022generating} to newer transformer \cite{yan2021videogpt,arnab2021vivit,esser2021taming,ramesh2021zero,yu2022scaling} and diffusion models \cite{kirkpatrick2017overcoming,sohl2015deep,song2020denoising,zhang2022gddim}. Early efforts like MoCoGAN~\cite{tulyakov2018mocogan} focused on short video clips but faced issues with stability and coherence. The introduction of transformers improved sequential data handling, enhancing video generation, while diffusion models further improved video quality by progressively denoising the input. 
Despite these advances, T2V models still struggle to reflect human preferences, with the generated videos generally lacking aesthetic quality. Additionally, the scarcity of paired video preference data hinders effective model training and may lead to insufficient flexibility and poor quality in the generated videos.


\subsection{RLHF}

\iffalse
Aligning LLMs \cite{dai1901transformer,radford2019language,zhang2023opt} typically involves two steps: supervised fine-tuning followed by Reinforcement Learning with Human Feedback (RLHF) \cite{gao2023scaling,stiennon2020learning,rafailov2024direct}. Although effective, RLHF is computationally expensive and can lead to issues like reward hacking. Methods like DPO have streamlined alignment by leveraging feedback data directly, improving efficiency.

In contrast, diffusion model alignment is still evolving, focusing mainly on enhancing visual quality through curated datasets. Techniques like DOODL \cite{wallace2023end} and AlignProp \cite{prabhudesai2023aligning} target aesthetic improvements but face challenges with complex tasks such as text-image alignment. Reinforcement learning methods like DPOK \cite{fan2024reinforcement} and DDPO \cite{black2023training} improve reward optimization but struggle with scalability. DPO-SDXL integrates DPO into T2I generation, boosting both alignment and aesthetics.

However, aligning video generation remains a largely unaddressed challenge, especially when dealing with motion consistency and semantic coherence across frames.
\fi

RLHF \cite{gao2023scaling,stiennon2020learning,rafailov2024direct} is a method that utilizes human feedback to guide machine learning models. Early RLHF algorithms, such as DDPG~\cite{lillicrap2015continuous} and PPO~\cite{schulman2017proximal}, typically relied on complex reward models to quantify human feedback. These reward models require a large amount of annotated data and face challenges during tuning. As research has progressed, more efficient preference learning methods have emerged, among which DPO has become a new framework. DPO does not depend on a separate reward model; instead, it obtains human preferences through pairwise comparisons and directly optimizes these preferences. This shift not only simplifies the application of RLHF but also enhances the alignment of models with human values. Furthermore, DPO has been successfully introduced into T2I tasks~\cite{wallace2024diffusion,yang2024using}, providing new insights for generative models in addressing the alignment of human preferences and showcasing DPO's potential in the field of AIGC~\cite{shi2024instantbooth,
qing2024hierarchical,menapace2024snap,koley2024s}. However, there remains a gap in current research regarding the application of DPO strategies to T2V tasks. Effectively integrating DPO into T2V tasks presents a challenging endeavor.

