\section{Introduction}
\label{sec:intro}

The rise of diffusion models has significantly impacted generative models, especially in T2I generation \cite{nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}. These T2I technologies have also driven the development of T2V generation \cite{he2022latent,guo2023animatediff,wu2021godiva,yu2022generating,wu2023lamp}, which aims to generate coherent video sequences from text.

Recently, there has been a surge of work that introduces Reinforcement Learning from Human Feedback (RLHF) into T2I tasks~\cite{lee2023aligning,fan2024reinforcement}, aligning the generated images with human preferences. Notably, some works \cite{wallace2024diffusion,yang2024using} have incorporated DPO strategies into T2I generation, eliminating the constraints of reward models and enhancing training efficiency.
However, current T2V generation methods face a fundamental issue: the lack of an effective and feasible loss function to guide the alignment of generated videos with human preferences during the training phase using DPO strategy. To address this issue, we rigorously derive the loss function and integrate DPO strategy into the T2V tasks for the first time. This allows us to fine-tune~\cite{an2023latent,ge2023preserve,ho2022imagen,luo2023videofusion} the model to generate videos that align with human aesthetics without the need for a separate reward model. This strategy reduces the reliance on computational resources and enhances the aesthetic quality of video generation, and we refer to it as \textbf{HuViDPO}.

Building and training a general-purpose, robust pipeline from scratch is an extremely challenging task. One of the most difficult issues is the lack of paired video preference data. Constructing a comprehensive paired video preference dataset clearly requires a substantial amount of manpower and time, but our approach cleverly circumvents this problem. For each small action category, we create corresponding small-scale human preference datasets through random pairings based on 8 to 16 human-rated videos. Subsequently, each category is fine-tuned separately using its respective dataset. Experiments show that our approach can reduce the total training duration for each category to under one day while ensuring that the model successfully learns the human preference information embedded in the small-scale human preference datasets.

The above dataset construction method, while addressing the problem, also introduces new challenges. Due to the limited data, issues such as a lack of flexibility in generated content and low video quality may arise, potentially affecting the model's generalization ability and the temporal consistency between frames. To address this, we build on the LAMP~\cite{wu2023lamp} framework and use DPO-SDXL~\cite{wallace2024diffusion} model to guide the generation of the first frame of the video, enhancing both human preference alignment and generation flexibility. Furthermore, we have improved the SparseCausal-Attention mechanism in our model, which experimental results show effectively enhances spatiotemporal consistency and improves overall video quality.

We evaluated HuViDPO using eight action categories. The experiments showed that, after simple fine-tuning, HuViDPO can generate videos corresponding to each action category and effectively transfer to different styles of video generation tasks. Compared to other baselines~\cite{he2022latent,guo2023animatediff,wu2023lamp}, the videos generated by our method align more closely with human aesthetic preferences. Some of the high-quality videos we generated are shown in \cref{fig:1}, with more of our generated videos available in the supplementary material. Our main contributions can be summarized as follows:

\begin{itemize}
\item
Integration of the DPO strategy into T2V tasks is achieved by designing a feasible loss function. The derivation process of this loss function can be found in~\cref{sec:q}. This integration enables the model to generate videos that align with human aesthetic preferences without the need for a separate reward model, thereby enhancing the aesthetic quality of the videos.

\item
Efficient DPO-Based Fine-tuning strategy using small-scale human preference datasets. This strategy not only enables efficient training of each action category within a day on a 24GB RTX4090 GPU using a small video dataset but also enhances alignment with human aesthetic preferences through DPO-Based Fine-tuning strategy, reducing costs and significantly improving preference consistency in generated videos.

\item
Enhanced video flexibility and quality with First-Frame-Conditioned strategy and SparseCausal-Attention mechanism. Leveraging DPO-SDXL for first-frame guidance aligns with human preferences and maintains flexibility, while SparseCausal-Attention mechanism enhancements improve spatiotemporal consistency and elevate video quality, creating a more coherent viewing experience.
\end{itemize}