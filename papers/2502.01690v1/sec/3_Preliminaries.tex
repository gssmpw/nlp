\section{Preliminaries}

\subsection{Preliminaries of Stable Diffusion}

Diffusion model generates data by reversing a gradual noising process. In the forward diffusion process, we start with real data $x_0$ and progressively add Gaussian noise at each timestep $t$ according to a predefined variance schedule. The process is defined as:

\begin{equation}
q(x_t \mid x_{t-1}) = \mathcal{N}\left( x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I \right).
\end{equation}

The objective of the diffusion model is to learn the reverse of the forward process, effectively denoising the data step by step. To achieve this, a neural network $\boldsymbol{\epsilon}_\theta(x_t, t)$ is trained to predict the noise $\boldsymbol{\epsilon}$ added at each timestep $t$. The simplified training loss function is given by:

\begin{equation}
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta\left( x_t, t \right) \right\|^2 \right].
\label{lab:1}
\end{equation}

where $x_t$ is computed using:

\subsection{Direct Preference Optimization}

In DPO, the objective is to derive an optimal reward function by maximizing the log-likelihood of preference pairs, thereby optimizing the policy to better match human preferences. Given a sample pair $(x, y_w, y_l)$, where $y_w$ represents the preferred sample and $y_l$ represents the less preferred sample, the probability of preference can be expressed as:

\begin{equation}
p(y_w \succ y_l \mid x) = \sigma(r^*(x, y_w) - r^*(x, y_l)).
\end{equation}

To align the generated samples with human preferences, we maximize the log-likelihood of the preference pairs. The corresponding loss function is defined as:

\begin{equation}
\mathcal{L}= -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \ln \sigma(r^*(x, y_w) - r^*(x, y_l)) \right].
\end{equation}

In the reverse derivation of the reward function, we assume that the optimal reward function takes the form of the log-ratio between the reference policy $\pi_{\text{ref}}$ and the current policy $\pi$:

\begin{equation}
r^*(x, y) = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}.
\end{equation}

By substituting this into the loss function, we obtain the final DPO loss function:

\begin{equation}
\begin{aligned}
\mathcal{L} = -\mathbb{E}_{(x, y_w, y_l) \sim D} \Big[ &\ln \sigma\Big( \beta \log \frac{\pi(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} \\
&- \beta \log \frac{\pi(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \Big) \Big].
\end{aligned}
\end{equation}

This loss function optimizes the difference in rewards between preference pairs, allowing the model to learn behaviors that better align with human preferences.


