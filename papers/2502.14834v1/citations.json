[
  {
    "index": 0,
    "papers": [
      {
        "key": "ge2024v2pe",
        "author": "Ge, Junqi and Chen, Ziyi and Lin, Jintao and Zhu, Jinguo and Liu, Xihui and Dai, Jifeng and Zhu, Xizhou",
        "title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding"
      },
      {
        "key": "li2024giraffe",
        "author": "Li, Mukai and Li, Lei and Gong, Shansan and Liu, Qi",
        "title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models"
      },
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ma2024mmlongbench",
        "author": "Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others",
        "title": "Mmlongbench-doc: Benchmarking long-context document understanding with visualizations"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "deng2024longdocurl",
        "author": "Deng, Chao and Yuan, Jiale and Bu, Pi and Wang, Peijie and Li, Zhong-Zhi and Xu, Jian and Li, Xiao-Hui and Gao, Yuan and Song, Jun and Zheng, Bo and others",
        "title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2025longvitu",
        "author": "Wu, Rujie and Ma, Xiaojian and Ci, Hai and Fan, Yue and Wang, Yuxuan and Zhao, Haozhe and Li, Qing and Wang, Yizhou",
        "title": "LongViTU: Instruction Tuning for Long-Form Video Understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2024sharegpt4video",
        "author": "Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others",
        "title": "Sharegpt4video: Improving video understanding and generation with better captions"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wu2024longvideobench",
        "author": "Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan",
        "title": "Longvideobench: A benchmark for long-context interleaved video-language understanding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2024lvbench",
        "author": "Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and others",
        "title": "Lvbench: An extreme long video understanding benchmark"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wu2024longgenbench",
        "author": "Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqing and Lee, Roy Ka-Wei",
        "title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yu2024rlhf",
        "author": "Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others",
        "title": "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yu2024rlaif",
        "author": "Yu, Tianyu and Zhang, Haoye and Yao, Yuan and Dang, Yunkai and Chen, Da and Lu, Xiaoman and Cui, Ganqu and He, Taiwen and Liu, Zhiyuan and Chua, Tat-Seng and others",
        "title": "Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhou2024aligning",
        "author": "Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu",
        "title": "Aligning modalities in vision large language models via preference fine-tuning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2024mia",
        "author": "Liu, Ziyu and Zang, Yuhang and Dong, Xiaoyi and Zhang, Pan and Cao, Yuhang and Duan, Haodong and He, Conghui and Xiong, Yuanjun and Lin, Dahua and Wang, Jiaqi",
        "title": "Mia-dpo: Multi-image augmented direct preference optimization for large vision-language models"
      }
    ]
  }
]