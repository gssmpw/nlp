\section{Related Work}
Recent advancements in Vision-Language Models have focused on enhancing their ability to process long-context inputs____. There are abundant benchmarks and datasets that designed for multimodal long context understanding including MMLongBench-Doc____, LongDocURL____, LongViTU____, ShareGPT4Video____, LongVideoBench ____ and LVBench____. However, the long-output generation abilities of VLMs have been less explored. In our work, we find that current VLMs struggle to generate an output with over 1000 tokens, which is much shorter than their max input context length (>16,000 tokens)____. To fill this gap, we explore how to extend the maximum output length of VLMs.


Although we show that supervised fine-tuning can align VLMs with user's instructions on length requirements, it is also important to improve the quality of long output____. Previous works mainly focus on how to improve VLMs' generation quality on short output tasks via post training methods such as RLHF-V____, RLAIF-V____, POVID____ and MIA-DPO____. However, none of these methods have explored how to effectively use human correctional feedback on long output for aligning VLMs. We propose to iteratively use each segment of the revised long output as the preferred response, which extends the number of preference pairs and successfully improves the long generation quality of VLM.


\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{$S$} & \textbf{$S_l$} & \textbf{$S_q$}   & \textbf{$S_{PPT}$} \\
\midrule
\texttt{LongWriter-V-7B} & 81.8 & 82.5 & 81.1 & 83.1 \\
\quad w/o single-image data & 79.6 & 79.5 & 79.6 & 83.4  \\
\quad w/o multi-image data & 66.5 & 60.3 & 72.7 & 29.3   \\
\quad w/o backtranslation & 80.7 & 80.0 & 81.3 & 82.4 \\
\midrule
\texttt{LongWriter-V-7B-DPO} & 84.6 & 86.3 & 82.9 & 85.8 \\
\quad w/o iterative pairs & 84.6 & 87.4 & 81.8 & 83.3  \\
\quad w/o 1.4k gpt4o feedback & 80.7 & 78.7 & 82.7 & 71.7 \\
\bottomrule
\end{tabular}
}
\caption{Scores (\%) on MMLongBench-Write for models trained under different conditions, where $S$, $S_l$ and $S_q$ is the overall, length and quality score on all tasks and $S_{PPT}$ is the overall score on the PPT script task.}
\label{tb:mem}
\end{table}