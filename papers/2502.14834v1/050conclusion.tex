\section{Related Work}

Recent advancements in Vision-Language Models have focused on enhancing their ability to process long-context inputs~\cite{ge2024v2pe,li2024giraffe,chen2024internvl}. There are abundant benchmarks and datasets that designed for multimodal long context understanding including MMLongBench-Doc~\cite{ma2024mmlongbench}, LongDocURL~\cite{deng2024longdocurl}, LongViTU~\cite{wu2025longvitu}, ShareGPT4Video~\cite{chen2024sharegpt4video}, LongVideoBench ~\cite{wu2024longvideobench} and LVBench~\cite{wang2024lvbench}. However, the long-output generation abilities of VLMs have been less explored. In our work, we find that current VLMs struggle to generate an output with over 1000 tokens, which is much shorter than their max input context length (>16,000 tokens)~\cite{wang2024qwen2}. To fill this gap, we explore how to extend the maximum output length of VLMs.


Although we show that supervised fine-tuning can align VLMs with user's instructions on length requirements, it is also important to improve the quality of long output~\cite{wu2024longgenbench}. Previous works mainly focus on how to improve VLMs' generation quality on short output tasks via post training methods such as RLHF-V~\cite{yu2024rlhf}, RLAIF-V~\cite{yu2024rlaif}, POVID~\cite{zhou2024aligning} and MIA-DPO~\cite{liu2024mia}. However, none of these methods have explored how to effectively use human correctional feedback on long output for aligning VLMs. We propose to iteratively use each segment of the revised long output as the preferred response, which extends the number of preference pairs and successfully improves the long generation quality of VLM.


\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{$S$} & \textbf{$S_l$} & \textbf{$S_q$}   & \textbf{$S_{PPT}$} \\
\midrule
\texttt{LongWriter-V-7B} & 81.8 & 82.5 & 81.1 & 83.1 \\
\quad w/o single-image data & 79.6 & 79.5 & 79.6 & 83.4  \\
\quad w/o multi-image data & 66.5 & 60.3 & 72.7 & 29.3   \\
\quad w/o backtranslation & 80.7 & 80.0 & 81.3 & 82.4 \\
\midrule
\texttt{LongWriter-V-7B-DPO} & 84.6 & 86.3 & 82.9 & 85.8 \\
\quad w/o iterative pairs & 84.6 & 87.4 & 81.8 & 83.3  \\
\quad w/o 1.4k gpt4o feedback & 80.7 & 78.7 & 82.7 & 71.7 \\
\bottomrule
\end{tabular}
}
\caption{Scores (\%) on MMLongBench-Write for models trained under different conditions, where $S$, $S_l$ and $S_q$ is the overall, length and quality score on all tasks and $S_{PPT}$ is the overall score on the PPT script task.}
\label{tb:mem}
\end{table}
\section{Conclusion}

Our work introduces MMLongBench-Write, a comprehensive benchmark for evaluating long-generation tasks with visual inputs, and LongWriter-V-22k, a novel supervised fine-tuning dataset designed to enhance the long-output capabilities of VLMs. Furthermore, our proposed IterDPO method effectively leverages human feedback to improve the fidelity of long outputs, addressing issues such as hallucination. Future research may explore more efficient training strategies and larger datasets to further push the boundaries of long-output generation in VLMs.