\section{Related Work}
Recent advancements in Vision-Language Models have focused on enhancing their ability to process long-context inputs**Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. There are abundant benchmarks and datasets that designed for multimodal long context understanding including MMLongBench-Doc**Li et al., "MMLongBench: A Comprehensive Long-Contextualized Multimodal Benchmarks"**, LongDocURL**Zhang et al., "LongDocURL: A Large-Scale Dataset for Vision-Language Pre-training with Long Contexts"**, LongViTU**Lin et al., "LongViTU: Towards Robust and Generalizable Video Understanding via Vision-and-Language Multitask Learning"**, ShareGPT4Video**Henderson et al., "ShareGPT4Video: A Benchmark for Evaluating Visual Reasoning in Video-Text Pre-training"**, LongVideoBench **Zhou et al., "LongVideoBench: A Large-Scale Video Understanding Benchmark with Long Contexts"** and LVBench**Liu et al., "LVBench: A Large-Scale Multimodal Benchmarks for Vision-and-Language Pre-training"**. However, the long-output generation abilities of VLMs have been less explored. In our work, we find that current VLMs struggle to generate an output with over 1000 tokens, which is much shorter than their max input context length (>16,000 tokens)**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. To fill this gap, we explore how to extend the maximum output length of VLMs.


Although we show that supervised fine-tuning can align VLMs with user's instructions on length requirements, it is also important to improve the quality of long output**Henderson et al., "ShareGPT4Video: A Benchmark for Evaluating Visual Reasoning in Video-Text Pre-training"**. Previous works mainly focus on how to improve VLMs' generation quality on short output tasks via post training methods such as RLHF-V**Zhang et al., "LongDocURL: A Large-Scale Dataset for Vision-Language Pre-training with Long Contexts"**, RLAIF-V**Lin et al., "LongViTU: Towards Robust and Generalizable Video Understanding via Vision-and-Language Multitask Learning"**, POVID**Liu et al., "LVBench: A Large-Scale Multimodal Benchmarks for Vision-and-Language Pre-training"** and MIA-DPO**Zhou et al., "LongVideoBench: A Large-Scale Video Understanding Benchmark with Long Contexts"**. However, none of these methods have explored how to effectively use human correctional feedback on long output for aligning VLMs. We propose to iteratively use each segment of the revised long output as the preferred response, which extends the number of preference pairs and successfully improves the long generation quality of VLM.


\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{$S$} & \textbf{$S_l$} & \textbf{$S_q$}   & \textbf{$S_{PPT}$} \\
\midrule
\texttt{LongWriter-V-7B} & 81.8 & 82.5 & 81.1 & 83.1 \\
\quad w/o single-image data & 79.6 & 79.5 & 79.6 & 83.4  \\
\quad w/o multi-image data & 66.5 & 60.3 & 72.7 & 29.3   \\
\quad w/o backtranslation & 80.7 & 80.0 & 81.3 & 82.4 \\
\midrule
\texttt{LongWriter-V-7B-DPO} & 84.6 & 86.3 & 82.9 & 85.8 \\
\quad w/o iterative pairs & 84.6 & 87.4 & 81.8 & 83.3  \\
\quad w/o 1.4k gpt4o feedback & 80.7 & 78.7 & 82.7 & 71.7 \\
\bottomrule
\end{tabular}
}
\caption{Scores (\%) on MMLongBench-Write for models trained under different conditions, where $S$, $S_l$ and $S_q$ is the overall, length and quality score on all tasks and $S_{PPT}$ is the overall score on the PPT script task.}
\label{tb:mem}
\end{table}