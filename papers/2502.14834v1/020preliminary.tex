\section{Preliminaries}
\label{sec:preliminary}
In the preliminary experiments, we first collect MMLongBench-Write, a benchmark with visual inputs and long-output requirements. Then, we conduct an evaluation on this benchmark to explore the maximum output length of VLMs.  Besides, we reveal that the main reason for bounded output length lies in the length distribution of SFT data.

\xhdr{MMLongBench-Write}
% Writing a long text for given images is a widely-used skill in daily lives, which can be roughly divided into professional writing and creative writing based on whether it involves professional knowledge~\cite{taavitsainen2000conventions}.
The ability to write long texts based on visual inputs is a fundamental skill in various real-world applications and can be broadly categorized into professional writing and creative writing, depending on whether specialized knowledge is required~\cite{taavitsainen2000conventions}. 
To evaluate how well that VLMs master the two skills, we design three specific tasks for each skill. For each task, we curate 20 representative instructions with input images as test data. To ensure diversity, half instructions are in English and half are in Chinese. Figure~\ref{fig:intro} shows six examples of the benchmark and data distribution. It highlights that professional writing tasks typically involve more input images and require longer output lengths. 
%compared to creative writing tasks.

% It demonstrates that professional writing data has more input images and longer expected output length. %than creative writing.

\xhdr{LongWrite-V-Ruler}
To explore the maximum output length of VLMs, we select 8 examples from MMLongBench-Write benchmark, with four samples in English and four in Chinese. As depicted in Figure~\ref{fig:intro}, each instruction is in the form of \textit{"Write an $L$-word article for the given pictures"}. We construct a diverse test set by changing the length requirement $L$. This test set uses $L \in \{500, 1000, 2000, 4000\}$, which consists of 32 test prompts in total. 


\xhdr{Evaluation Result}
We conduct the LongWrite-V-Ruler test on three open-source VLMs and three proprietary models. In Figure~\ref{fig:ruler_res}, we plot the required output length (x-axis) and the corresponding average output length (y-axis) for 12 instructions. We can observe that there exists an upper bound of 1000 output length for all models.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/ruler/length_comparison_model_v2.pdf}
    \caption{LongWriter-V-Ruler test across different output length requirements. The horizontal line show the overall upper bound for current VLMs.}
    \label{fig:ruler_res}
\end{figure}

\xhdr{Preliminary Experiment}
As the controlled experiments in LongWriter~\cite{bai2024longwriter} has revealed that the maximum output length of LLM is correlated with the maximum output length of SFT data, we further explore how the average output length of SFT data can influence the long-generation capabilities of VLM. We fine-tune Qwen2-VL-7B-Instruct~\cite{wang2024qwen2} on three visual instruction datasets sampled from our final SFT data. Each dataset has 10k examples with different average output length respectively (0.8k, 1.8k and 2.8k). Figure~\ref{fig:ruler_res_sft} shows the trained models' performance on LongWrite-V-Ruler, we observe that the model's maximum output length increases with the average output length of SFT data. Besides, we find that \textbf{the number of long-output examples} is crucial for extending the output length of VLMs. For example, the training set with an average length of 1.8k contains 1\% data exceeding 4k output length, but the model trained on it fails to generate 4k tokens (orange line). In contrast, the model trained with 21\% data exceeding 4k output length is able to do that (blue line). This result indicates that the main reason that limits the VLM's output length is lack of enough long-output examples in SFT data.

% This finding underscores that the primary limitation of VLMs' output length is the insufficient number of long-output examples in the SFT data.
