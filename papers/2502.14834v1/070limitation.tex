\section*{Limitations}
We acknowledge some limitations in our work, which are listed below:
1. \textbf{Dataset Size}: The size of our LongWriter-V-22k dataset may not be sufficiently large to fully capture the diversity of long-output generation tasks. While this dataset size is adequate for initial exploration and training, it may limit the robustness of our findings and the generalizability of our model's performance. Expanding the dataset to include more examples would require significant additional resources, both in terms of data collection and annotation costs.
2. \textbf{Language Limitation}: The current dataset and benchmark are limited to English and Chinese only. This restricts our ability to evaluate the performance of VLMs across multiple languages, which is crucial for real-world applications where multilingual support is often required. Future work should consider expanding the dataset to include other languages to provide a more comprehensive evaluation of VLMs' long-output capabilities. 
3. \textbf{Human Feedback Efficiency}: While our IterDPO method significantly improves the efficiency of utilizing human feedback for long outputs, the process of collecting high-quality human corrections remains time-consuming and costly. This limits the scalability of our approach and the frequency with which we can update and refine the training data. Future work should explore more efficient methods for obtaining and incorporating human feedback to further enhance model performance.
