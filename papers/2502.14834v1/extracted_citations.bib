@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@article{chen2024sharegpt4video,
  title={Sharegpt4video: Improving video understanding and generation with better captions},
  author={Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others},
  journal={arXiv preprint arXiv:2406.04325},
  year={2024}
}

@article{deng2024longdocurl,
  title={LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating},
  author={Deng, Chao and Yuan, Jiale and Bu, Pi and Wang, Peijie and Li, Zhong-Zhi and Xu, Jian and Li, Xiao-Hui and Gao, Yuan and Song, Jun and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2412.18424},
  year={2024}
}

@article{ge2024v2pe,
  title={V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding},
  author={Ge, Junqi and Chen, Ziyi and Lin, Jintao and Zhu, Jinguo and Liu, Xihui and Dai, Jifeng and Zhu, Xizhou},
  journal={arXiv preprint arXiv:2412.09616},
  year={2024}
}

@article{li2024giraffe,
  title={GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models},
  author={Li, Mukai and Li, Lei and Gong, Shansan and Liu, Qi},
  journal={arXiv preprint arXiv:2412.12735},
  year={2024}
}

@article{liu2024mia,
  title={Mia-dpo: Multi-image augmented direct preference optimization for large vision-language models},
  author={Liu, Ziyu and Zang, Yuhang and Dong, Xiaoyi and Zhang, Pan and Cao, Yuhang and Duan, Haodong and He, Conghui and Xiong, Yuanjun and Lin, Dahua and Wang, Jiaqi},
  journal={arXiv preprint arXiv:2410.17637},
  year={2024}
}

@article{ma2024mmlongbench,
  title={Mmlongbench-doc: Benchmarking long-context document understanding with visualizations},
  author={Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others},
  journal={arXiv preprint arXiv:2407.01523},
  year={2024}
}

@article{wang2024lvbench,
  title={Lvbench: An extreme long video understanding benchmark},
  author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2406.08035},
  year={2024}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{wu2024longgenbench,
  title={LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs},
  author={Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqing and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2409.02076},
  year={2024}
}

@article{wu2024longvideobench,
  title={Longvideobench: A benchmark for long-context interleaved video-language understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2407.15754},
  year={2024}
}

@article{wu2025longvitu,
  title={LongViTU: Instruction Tuning for Long-Form Video Understanding},
  author={Wu, Rujie and Ma, Xiaojian and Ci, Hai and Fan, Yue and Wang, Yuxuan and Zhao, Haozhe and Li, Qing and Wang, Yizhou},
  journal={arXiv preprint arXiv:2501.05037},
  year={2025}
}

@article{yu2024rlaif,
  title={Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness},
  author={Yu, Tianyu and Zhang, Haoye and Yao, Yuan and Dang, Yunkai and Chen, Da and Lu, Xiaoman and Cui, Ganqu and He, Taiwen and Liu, Zhiyuan and Chua, Tat-Seng and others},
  journal={arXiv preprint arXiv:2405.17220},
  year={2024}
}

@inproceedings{yu2024rlhf,
  title={Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
  author={Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13807--13816},
  year={2024}
}

@article{zhou2024aligning,
  title={Aligning modalities in vision large language models via preference fine-tuning},
  author={Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2402.11411},
  year={2024}
}

