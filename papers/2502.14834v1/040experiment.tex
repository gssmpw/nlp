
\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{5.7cm}|m{0.7cm}m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}}
\toprule
 &  \multicolumn{3}{|c}{\textbf{Overall}} &  \multicolumn{2}{|c}{\textbf{[0,1500)}} &  \multicolumn{2}{|c}{\textbf{[1500,2000)}} &  \multicolumn{2}{|c}{\textbf{[2000,3000)}} &  \multicolumn{2}{|c}{\textbf{[3000,4000)}} \\
\cmidrule(r){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
 \textbf{Model} & $\overline{S}$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ \\
\midrule
\multicolumn{12}{l}{\emph{Caption + LLMs}} \\
\texttt{GLM-4-9B-Chat} & 71.3 & 62.0 & 80.6 & 87.9 & 72.2 & 65.7 & 82.4 & 44.7 & 76.7 & 24.2 & 93.5 \\
\texttt{GPT-4o-2024-08-06} & 77.1 & 66.6 & 87.5 & 86.7 & 81.2 & 68.9 & 88.3 & 58.7 & 85.8 & 33.5 & 97.2 \\
\texttt{Mistral-Large-Instruct-2407} & 78.9 & 69.6 & 88.2 & \textbf{89.7} & 84.7 & 70.9 & 89.9 & 58.4 & 83.0 & 47.2 & 94.9 \\
\texttt{DeepSeek-R1} & 82.4 & 70.3 & \textbf{94.5} & 87.2 & 92.4 & 73.4 & \textbf{95.7} & 59.8 & \textbf{92.0} & 38.1 & \textbf{95.8} \\    
\midrule
\multicolumn{12}{l}{\emph{Open-source VLMs}} \\
\texttt{MiniCPM-V2.6} & 54.1 & 30.3 & 77.8 & 56.1 & 68.9 & 31.3 & 81.7 & 15.0 & 69.4 & 4.5 & 86.1 \\
\texttt{Qwen2.5-VL-7B-Instruct} & 54.4 & 45.3 & 63.5 & 62.9 & 51.1 & 46.6 & 70.5 & 37.6 & 50.6 & 16.1 & 67.6 \\
\texttt{Qwen2.5-VL-72B-Instruct} & 83.3 & 79.9 & 86.7 & 80.0 & 78.4 & 84.5 & 90.3 & 71.6 & 79.7 & 65.3 & 91.7 \\
\midrule
\multicolumn{12}{l}{\emph{Proprietary VLMs}} \\
\texttt{Claude-3-Opus-20240229} & 61.7 & 41.5 & 82.0 & 52.0 & 64.7 & 42.8 & 87.5 & 36.1 & 74.6 & 23.3 & 89.8 \\
\texttt{GPT-4o-2024-08-06} & 62.7 & 42.7 & 82.6 & 86.6 & 91.2 & 37.7 & 83.1 & 34.2 & 71.6 & 14.2 & 88.4 \\
\texttt{Gemini-1.5-Pro} & 83.0 & 74.8 & 91.2 & 88.7 & \textbf{93.0} & 78.1 & 91.8 & 62.2 & 86.2 & 50.5 & 95.4   \\

\midrule
\multicolumn{12}{l}{\emph{Our trained VLMs}} \\
\texttt{LongWriter-V-7B} & 81.8 & 82.5 & 81.1 & 63.3 & 72.8 & 87.8 & 86.4 & 81.2 & 69.2 & 86.8 & 87.5 \\
\texttt{LongWriter-V-7B-DPO} & 84.6 & \textbf{86.2} & 82.9 & 69.5 & 82.5 & \textbf{90.5} & 86.9 & 87.1 & 69.0 & \textbf{87.4} & 85.2 \\
\texttt{LongWriter-V-72B} & \textbf{84.9} & 84.3 & 85.5 & 73.2 & 83.3 & 86.2 & 89.3 & \textbf{88.4} & 75.8 & 81.4 & 85.2 \\
\bottomrule
\end{tabular}
}
\caption{Evaluation results (\%) on MMLongBench-Write. Note that LLMs are tested with input images transformed into captions. We report scores on different subsets of the benchmark, where [0,1000) means the expected output length falls within 0 to 1000 tokens. $\overline{S}$, $S_l$, $S_q$ is the overall score, length score and quality score respectively. }
\label{tb:exp}
\end{table*}

% \begin{table*}[t]
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{p{5.7cm}|m{0.7cm}m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}}
% \toprule
%  &  \multicolumn{3}{|c}{\textbf{Overall}} &  \multicolumn{2}{|c}{\textbf{[0,1500)}} &  \multicolumn{2}{|c}{\textbf{[1500,2000)}} &  \multicolumn{2}{|c}{\textbf{[2000,3000)}} &  \multicolumn{2}{|c}{\textbf{[3000,4000)}} \\
% \cmidrule(r){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
%  \textbf{Model} & $\overline{S}$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ & $S_l$ & $S_q$ \\
% \midrule
% \multicolumn{12}{l}{\emph{Caption + LLMs}} \\
% \texttt{GLM-4-9B-Chat} & 71.3 & 62.0 & 80.6 & 87.9 & 72.2 & 65.7 & 82.4 & 44.7 & 76.7 & 24.2 & 93.5 \\
% \texttt{GPT-4o-2024-08-06} & 77.1 & 66.6 & 87.5 & 86.7 & 81.2 & 68.9 & 88.3 & 58.7 & 85.8 & 33.5 & 97.2 \\
% \texttt{Mistral-Large-Instruct-2407} 
% \texttt{DeepSeek-R1} 
% \midrule
% \multicolumn{12}{l}{\emph{Open-source VLMs}} \\
% \texttt{MiniCPM-V2.6} 
% \texttt{Qwen2.5-VL-7B-Instruct} 
% \texttt{Qwen2.5-VL-72B-Instruct} 
% \midrule
% \multicolumn{12}{l}{\emph{Proprietary VLMs}} \\
% \texttt{Claude-3-Opus-20240229} 
% \texttt{GPT-4o-2024-08-06} & 62.7 & 42.7 & 82.6 & 86.6 & 91.2 & 37.7 & 83.1 & 34.2 & 71.6 & 14.2 & 88.4 \\
% \texttt{Gemini-2.0-Flash}  \\

% \midrule
% \multicolumn{12}{l}{\emph{Our trained VLMs}} \\
% \texttt{LongWriter-V-7B} & 82.0 & 85.4 & 78.6 & 68.4 & 77.0 & \textbf{89.1} & 80.8 & 88.0 & 73.1 & 86.3 & 77.8 \\
% \texttt{LongWriter-V-72B} & 82.7 & 81.6 & 83.7 & 67.3 & 86.4 & 85.4 & 84.8 & 78.3 & 76.9 & 90.8 & 86.6 \\
% \texttt{LongWriter-V-7B-DPO} & \textbf{83.3} & \textbf{85.8} & 80.7 & 67.6 & 75.9 & 88.8 & 84.5 & \textbf{89.2} & 75.4 & \textbf{92.6} & 75.0 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Evaluation results (\%) on MMLongBench-Write. Note that LLMs are tested with input images transformed into captions. We report scores on different subsets of the benchmark, where [0,1000) means the expected output length falls within 0 to 1000 tokens. $\overline{S}$, $S_l$, $S_q$ is the overall score, length score and quality score respectively. }
% \label{tb:exp}
% \end{table*}

\section{Experiments}


\subsection{Experimental Setup}

\xhdr{Metric} Following~\citet{bai2024longwriter}, we evaluate the VLM's output length and quality using two metrics: $S_l$ and $S_q$. $S_l$ is the output score that measures how close that the VLM's output length $l_{v}$ is to the required length $l_{r}$:
\begin{equation}
S_l = 
\begin{cases} 
100 \cdot \max \left(0, 1 - \frac{(l_{v}/l_{r} - 1)}{3}\right) & \text{if } l_{v} > l_{r}, \\
100 \cdot \max \left(0, 1 - \frac{(l_{r}/l_{v} - 1)}{2}\right) & \text{if } l_{v} \leq l_{r}.
\end{cases}
\end{equation}
We also use gpt-4o-2024-08-06 to assign the quality score $S_q$ for six aspects: Relevance, Accuracy, Coherence, Clarity,
Breadth and Depth, and Reading Experience. We list the scoring prompt in Appendix~\ref{sec:setup}. Note that we have asked gpt-4o not to take the output length into account so that the quality score is independent with the length score. The overall score $\overline{S}$ is the mean of $S_l$ and $S_q$.

\xhdr{Baselines} We evaluate 3 proprietary VLMs, 3 open-source VLMs and 4 LLMs on MMLongBench-Write (model details about models are listed in Table~\ref{tb:model_card}). Given that LLMs can also process visual instructions via reading the image caption~\cite{ma2024mmlongbench}, we first use gpt-4o to describe the input images and then feed the caption and writing instruction to the LLM.



\subsection{Main Results}
We report the performance of baselines and our trained models in Table~\ref{tb:exp}. To study the effective output length of models, we divide the MMLongBench-Write benchmark into four subsets based on the instruction's required word count: 0-1500 words, 1500-2000 words, 2000-3000 words, and over 3000 words. The highest length and quality scores for each subset among models are in bold.
We have three observations on the results: (1) Most existing models struggle to satisfy the length requirement over 2000 words, while LongWriter-V models can generate enough words for such instructions. By checking the length score $S_l$ across different length intervals, we find that most models perform poorly on the [2000, 3000) range, with their $S_l$ below 70. In contrast, our LongWriter-V models can generate outputs with effective length and high quality even on the range of [3000, 4000).
(2) The scaling law effect on our benchmark is striking: smaller models like Qwen2.5-VL-7B-Instruct perform poorly in our evaluation with an overall score of 54.4, while its larger counterpart Qwen2.5-VL-72B-Instruct achieves a notably higher score of 83.3. Besides, after training the two VLMs on our LongWrite-V-22k data, both models improve significantly on long generation. The performance gap between the two sizes' models is narrowed after SFT (LongWriter-V-7B's 81.8 vs. LongWriter-V-72B's 84.9).
(3) DPO can improve both the VLM's output quality and the ability to follow the length requirements of long generation. LongWriter-V-7B-DPO, which is the model trained on LongWriter-V-7B with 2,844 preference pairs, achieves improvement on both $S_l$ (+3.7) and $S_q$ (+1.8),  showing that DPO is effective for boosting the long generation capabilities of VLMs.

\subsection{Human Evaluation}

As the quality score $S_q$ is assigned by the GPT-4o automatically, the evaluation results may have bias as LLM tends to favor the responses generated by itself~\cite{wang2023large,li2024llms}. To get a more fair quality comparison for the models, we conduct a human evaluation to capture the actual human preferences on model responses. Specifically, we select responses from four models: the three models trained by us and the GPT-4o-2024-08-06  baseline. We ask two human annotators to vote for their preferred response between two selected models on the $120$ responses of MMLongBench-Write. For each annotator, we collect $720$  votes and calculate the average win rate among models using two annotators' feedback. 

The results are shown in Figure~\ref{fig:human_eval}, where we surprisingly find that two of our trained models receive more votes from humans in the comparison with the GPT-4o-2024-08-06 baseline. While in the automatic quality score comparison, the two models also surpass the GPT-4o on the quality score. This indicates that our trained models have gained some advantages over the GPT-4o baseline in the human preference, which is consistent with the automatic evaluation on the quality score of responses.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/train/winrate_heatmap_v1.pdf}
    \caption{Human evaluation results on MMLongBench-Write, where each block of the matrix represents the model of the row's win rate over the model of the column. The win rate is voted by two annotators.}
    \label{fig:human_eval}
\end{figure}

\subsection{Ablation Study}

We conduct ablation experiments on both the SFT and DPO process of LongWriter-V models. For the LongWriter-V-7B model trained on LongWrite-V-22k data, we control the three data sources of LongWrite-V-22k to observe how they contribute to the final performance of the SFT model.  We run the SFT process on Qwen2.5-VL-7B-Instruct without (w/o) single-image, multi-image or backtranslation data respectively and evaluate the trained models on MMLongBench-Write. As shown in Table~\ref{tb:mem}, removing any of these data sources may lead to a decline in the overall score, where multi-image data is the most essential one, causing a decrease of 15.3 overall score. These results indicate that these sources are useful for training long output VLMs.




To explore the effectiveness of our iterative DPO strategy over the small size preference data on long-output VLM alignment, we run the DPO process without those extra pairs extended by the iterative strategy. Results in Table~\ref{tb:mem} demonstrates that the model gains +1.1 length score but -1.1 quality score and -2.5 PPT task score over the DPO model with full data, which means the extended data is useful for the generation quality and the PPT script task. To examine the effectiveness of mixing AI preference pairs, we then train the SFT model with the human revised preference pairs only, resulting in a even worse performance (-1.1 overall score against the SFT model). This suggests that incorporating AI-generated pairs can improve model performance by providing additional training signals.




