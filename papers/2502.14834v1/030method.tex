
\section{LongWriter-V: Data and Training}
In this section, we will introduce the data collection and training process for unlocking the long generation capability of vision-language models.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/ruler/ruler_plot_trained.pdf}
    \caption{LongWriter-V-Ruler test for Qwen2-VL-7B-Instruct trained on 10k SFT data samples with different average output lengths. }
    \label{fig:ruler_res_sft}
\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/pipeline_v0.pdf}
    \caption{SFT and DPO data collection pipeline of LongWriter-V. The SFT data includes both single-image and multi-image input for long text output. The DPO data contains human revision over each paragraph of VLM's long output. We conduct iterative direct preference optimization to learn the fine-grained human feedback.}
    %\vspace{-2mm}
    \label{fig:pipeline}
\end{figure*}

\subsection{Data Collection}
\label{sec:data_collection}
Figure~\ref{fig:pipeline} depicts the overall pipeline of our data collection process, which consists of two phases: SFT and DPO data collection. %We gather user instruction that require long output from existing visual instruction datasets and develop an agent-based method to generate long texts for SFT.


\subsubsection{SFT Data Collection}
\label{sec:sft_data_collection}
Existing VLMs fail to directly generate texts exceeding 1k tokens, so we develop a two-stage method to generate long texts as SFT data.


\xhdr{LongWrite Agent-V} Before introducing our method, we first formalize the task's objective. Given several input images $v$ and an user instruction $x$, our goal is to generate a text $y^*$ that aligned with user's length and quality requirements:
\begin{equation}
   y^* = \mathop{\arg\max}\limits_{y}(s_l(y) + s_q(y) )P_{\theta}(y|v,x) 
\end{equation}
where $s_l$ and $s_q$ is the scoring function for judging the length and quality of the output, respectively. $P_{\theta}$ is the function representing the end-to-end solution, while existing VLMs may not be directly applied as their maximum output lengths are below 1k.
To utilize off-the-shelf VLMs, we propose a two-stage method for generating long texts. Inspired by the plan-and-write method from LongWriter~\cite{bai2024longwriter}, we first prompt the VLM to generate an outline $o$ that structures the output, plans the content, and specifies the word count for each paragraph. This outline breaks down the complex long-output task into manageable sub-tasks. Next, we use the VLM to fill in each paragraph and concatenate them to form the final output:
\begin{equation}
y^* = \mathop{\arg\max}\limits_{o}P_{1}(o|v,x)  \mathop{\arg\max}\limits_{y}(s_l(y) + s_q(y) )P_{2}
\end{equation}
\begin{equation}
P_{2}(y|v,x,o) = \prod \limits_{i=0}^n p(y_i|v,x,o_i,y_{<i})
\end{equation}
where $P_{1}$ is the modeling function for first stage, which takes input images and instruction to write an n-paragraph outline $o = \{o_{i}, i =1,...,n\}$. $P_{2}$ refers to the second stage, where the VLM outputs the content $y_i$ paragraph by paragraph based on the input information, outline $o_{i}$ and previous paragraphs $y_{<i}$. In practice, we design two detailed prompts for guiding VLM to implement the two stages, which are listed in Appendix~\ref{app:prompt_lav}.
% \begin{aligned}
% y^* &= \mathop{\arg\max}\limits_{o}P_{1}(o|v,x) \\
% &\quad \mathop{\arg\max}\limits_{y}(s_l(y) + s_q(y) )P_{2}(y|v,x,o)
% \end{aligned}


% which is an open-domain image-text instruction dataset with 480k examples. It contains diverse instructions and responses derived from previous datasets like LLaVA-Instruct~\cite{liu2024visual} and ShareGPT4V~\cite{chen2024sharegpt4v}.As this dataset is for general purpose, the average output length is just 54.85. We perform two automatic checks to pick those long-output instructions.

\xhdr{Visual Instruction Collection}
To collect long-output visual instructions for SFT, we choose MMEvol~\cite{luo2024mmevol} as our primary data source. MMEvol is a large-scale, open-domain dataset containing 480k image-text instruction pairs, sourced from diverse datasets such as LLaVA-Instruct~\cite{liu2024visual} and ShareGPT4V~\cite{chen2024sharegpt4v}. However, the average output length in this dataset is relatively short (54.85 tokens), necessitating a filtering process to identify long-output instructions. We first check the original response length of e ach example and select those with output length over 128, yielding 55,835 valid data. Next, we utilized GPT-4o to verify whether each instruction genuinely requires a long output and whether the associated image was sufficiently relevant to the instruction. Finally, we get 8,115 single-image instructions.

% Then we use gpt-4o to double check if the instruction requires long output and if the image is enough related with the instruction. 



\xhdr{Multi-image Instruction Generation} As the original data in MMEvol only has one image for each instruction, we synthesize some multi-image instructions to increase the diversity of SFT data.  We select three subsets of MMEvol: wikiart, web-landmark, web-celebrity. Each subset contains hundreds of images in the same category. For example, images in web-landmark are all landmark pictures taken from different world attractions. We randomly sample 2 or 4 same-category images and then ask gpt-4o to generate an instruction that require long output for these images. We obtain 6,313 multi-image instructions in this way. Apart from synthetic data, we also collect natural multi-image data from an open-source PPT dataset, Zenodo10K ~\cite{zheng2025pptagent}. We transform these slides into images to use them as visual inputs and set the instruction as "Write a lecture script for these slides". We choose those slides that has at least 2 pages and at most 30 pages, resulting in 7,730 data.







\xhdr{Backtranslation}
Through above processes, we collect 22,158 single-image and multi-image instructions in total. Using the LongWrite Agent-V pipeline, we generate long output for each visual instruction as SFT data. We call this training data LongWrite-V-22k.  But most instructions don't specify the exact word count requirement, models trained on these data may lack the ability to follow the writing instruction with word count requirements. Therefore, we sample 5,000 data from LongWrite-V-22k and calculate the length of the output $L$ then add a requirement "Please write L-word in total." to the end of the instruction and use gpt-4o-mini for rephrasing the instruction to maintain consistency. This is inspired by previous backtranslation~\cite{li2023self} method on training long-output LLMs~\cite{pham2024suri}.

\subsubsection{DPO Data Collection}
\label{sec:dpo_data_collection}

The SFT data aims to extend VLMs' output length. But the longer outputs may bring more hallucinations and repetitions. So the follow up question is: \textit{how to improve the generation fidelity of long output VLM?} Previous works often adapt direct preference optimization~\cite{rafailov2024direct,liu2024mia} to correct the hallucinations of VLMs. We follow the data format in RLHF-V~\cite{yu2024rlhf} which utilizes the human-annotated segment-level corrections on VLM's outputs as feedback.


\xhdr{VLM Output Collection} To collect long responses, we select 100 slides that were not included in LongWrite-V-22k for VLM to generate scripts. These slides were previously used for teaching on MOOC platforms~\cite{yu2020mooccube} and cover 10 subjects such as Computer Science, Math and Physics. Each subject may contain 4 to 16 slides and each slide may have 10 to 30 pages. We use LongWriter-V-7B, the VLM trained on our SFT data, to generate scripts for each slide. The long scripts are segmented by sections and aligned with each page of the given slide. We find that LongWriter-V-7B tends to output fewer sections than the number of total pages, which is one of the issues that we would ask human annotators to fix.



\xhdr{Human Revision Collection} To get high-quality feedback on the flawed output of SFT model, we hire 10 college students from 10 different majors corresponding to the subjects of our slides. We required annotators to have a GPA above 3.8 to ensure their expertise. To facilitate the annotation process, we build an online platform (See Appendix~\ref{sec:platform}). Each annotator will get slides that match with their major.The platform displays each slide page alongside the corresponding script segment generated by the SFT model. We ask annotators to check and revise each page's script for the following error types: factual errors, missing information, relevance to the image, coherence of sentences, and repetition of words. After completing the annotation of a slide, our authors will review the annotation quality. Ultimately, we get 72 valid scripts with fine-grained human corrections.




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/train/length_distribution_22k.pdf}
    \caption{Output length statistics of LongWrite-V-22k.}
    \label{fig:stats}
\end{figure}


\subsection{Training}
\xhdr{Supervised Fine-tuning} We conduct model training based on two open-source VLMs with different parameter sizes: Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-72B-Instruct~\cite{Qwen2.5-VL}. We choose Qwen2.5-VL series as base model because they support a context window of 32k tokens. By resizing the input image's width and height to 280x280, the Qwen2.5-VL models can process up to 30 images. As shown in Figure~\ref{fig:stats}, the output length in LongWrite-V-22k are distributed between 0 and 10k with two peaks around 0 and 1.5k. The peak at 0 indicates some short output data is mixed in the LongWrite-V-22k, which are mainly the results of those simple instructions. To get a better length distribution, we sample 10k data from LongWrite-V-22k with an average output length of 2.8k as training data. We then fine-tune the two models for 3 epochs with a learning rate of 1e-5 for Qwen2.5-VL-7B-Instruct and 7e-6 for Qwen2.5-VL-72B-Instruct, resulting in two SFT models: LongWriter-V-7B and LongWriter-V-72B.


\xhdr{Iterative Direct Preference Optimization} 
%Reinforcement learning (RL) aligns LLMs with human preference by maximizing the average reward of model outputs, where a reward model $r(x,y)$ assigns a scalar reward to each input-output pair $(x, y)$ to represent its desirability. 
After SFT phase, DPO~\cite{rafailov2024direct} is a widely-used method to optimize VLM's output quality, which learns from a dataset of preference pairs $\mathcal{D}=\{(v,x, y_w, y_l)\}$, where the winning output $y_w$ is preferred over the losing output $y_l$ given the same visual input $v$ and text input $x$. The optimization objective of DPO is to maximize the difference between likelihood of preference pairs:  
\begin{equation}
\begin{aligned}
&\mathcal{L}_\text{DPO}(\pi_\theta; \pi_\text{ref})= -\mathbb{E}_{(v,x, y_w, y_l)\sim\mathcal{D}} \\
&[\log\sigma(\beta\log\frac{\pi_\theta(y_w|v,x)}{\pi_\text{ref}(y_w|v,x)}-\beta\log\frac{\pi_\theta(y_l|v,x)}{\pi_\text{ref}(y_l|v,x)})]
\end{aligned}
\end{equation}
In our annotation process, $v$ represents the images of a slide, $x$ is the instruction for generating scripts, $y_l$ is the flawed output script of VLM and $y_w$ is the slide's lecture after human revision. However, collecting human feedback on long output is very time-consuming and expensive. As mentioned in Section~\ref{sec:dpo_data_collection}, we gather 72 preference pairs on the scripts, which costs one week and around 1,000 \$ to finish.  To make most use of these data, we propose to iteratively learn the fine-grained human correctional feedback on the long output. As the $y_w = \{y_w^{i}, i=1,...N\}$ is a revised script for an $N$ page slide, we increasingly view each page's script $y_w^{i}$ as a winning segment over the flawed script:

\begin{small} 
\begin{equation}
\begin{aligned}
&\mathcal{L}_\text{IterDPO}(\pi_\theta; \pi_\text{ref})= -\mathbb{E}_{(v,x, y_w, y_l)\sim\mathcal{D}} \sum_{i=1}^{N}\\
&[\log\sigma(\beta\log\frac{\pi_\theta(y_w^{\le i}|v_{\le i},x)}{\pi_\text{ref}(y_w^{\le i}|v_{\le i},x)}-\beta\log\frac{\pi_\theta(y_l^{\le i}|v_{\le i},x)}{\pi_\text{ref}(y_l^{\le i}|v_{\le i},x)})]
\end{aligned}
\end{equation}
\end{small}
where $y_w^{\le i}$,  $y_l^{\le i}$ is the revised and unrevised scripts until page $i$, and $v_{\le i}$ are the corresponding images. We view $y_w^{\le i}$ as a new wining response over the flawed output $y_l^{\le i}$, this can help VLM learn the fine-grained feedback on the long output and extend the number preference pairs for $N$ times. In this way, we get 1,477 iterative pairs for training. Apart from human feedback, we also utilize AI feedback by employing the gpt4o as the reward model. Following RLAIF~\cite{yu2024rlaif}, we sample responses from the SFT model for 1,367 long-output instructions and use GPT-4o for assigning length and quality scores for the responses to construct preference pairs. Our final DPO model is trained with 2,844 mixed preference pairs,


% by sampling responses from the SFT model for 1,367 long-output instructions. Following~\cite{bai2024longwriter}, we use GPT-4o for assigning length and quality scores for the responses to construct preference pairs. Our final DPO model is trained with 2,844 mixed preference pairs,