\section{Introduction}

% Over the past year, research and products on long-context large language models (LLMs) have made remarkable progress: in terms of context window length, advancing from the initial 8k to the current 128k and even 1M tokens~\cite{GPT-4o,claude-3-5,reid2024gemini,glm2024chatglm}; and achieving promising performance on long-context benchmarks. However, beneath these advancements lies an urgent and practical question: \textbf{Do these models truly comprehend the long texts they process, i.e., are they capable of deeply understanding, learning, and reasoning based on the information contained in these long texts?}

% Critically, existing long-context understanding benchmarks~\cite{bai2024longbench,zhang2024infty,hsieh2024ruler} fail to reflect the long-context LLMs' \emph{deep} understanding capabilities across diverse tasks.
% They often focus on extractive questions, where answers are directly found in the material, a challenge easily handled by modern long-context models and RAG systems, as evidenced by their perfect recall in the Needle-in-a-Haystack test~\cite{needleinhaystack}.
% Furthermore, many of these benchmarks rely on synthetic tasks, which limits their applicability to real-world scenarios, and their adopted metrics like F1 and ROUGE are unreliable.

% To address these issues, we aim to build a benchmark with the following features: 
% (1) \textbf{Length}: Context length ranging from 8k to 2M words, with the majority under 128k.
% (2) \textbf{Difficulty}: Challenging enough that even human experts, using search tools within the document, cannot answer correctly in a short time.
% (3) \textbf{Coverage}: Cover various realistic scenarios.
% (4) \textbf{Reliability}: All in a multiple-choice question format for reliable evaluation.

% 
% With the above goal in mind, we present \emph{LongBench v2}.
% LongBench v2 contains 503 multiple-choice questions and is made up of 6 major task categories and 20 subtasks to cover as many realistic deep comprehension scenarios as possible, including \emph{single-document QA}, \emph{multi-document QA}, \emph{long in-context learning}, \emph{long-dialogue history understanding}, \emph{code repository understanding}, and \emph{long structured data understanding} .

% The integration of visual encoders with large language models (LLMs) has enabled these models to effectively handle multimodal information.

Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their capabilities in processing visual and textual inputs~\cite{alayrac2022flamingo,zhang2024vision}.  Notably, there have been substantial breakthroughs in the long-context capabilities of VLMs~\cite{xue2024longvila,shu2024video}. For instance, Qwen2-VL~\cite{wang2024qwen2} can now understand videos up to 20 minutes, with a context window of 32k tokens. This progress has significantly expanded the scope of tasks that VLMs can handle, making them more applicable to real-world scenarios.

However, despite the increased input context window, the effective output length of VLMs remains limited. To verify this limitation, we collect a benchmark comprising six tasks that require VLMs to generate long texts based on visual inputs (as shown in Figure~\ref{fig:intro}). By adjusting the required output length in the instructions, we found that all existing models struggle to generate outputs exceeding 1,000 words (Section~\ref{sec:preliminary}). In real-world scenarios, such long-output queries are common user demands~\cite{chou2024visionarena}. For example, (1) creative writing tasks may require generating detailed stories or essays based on visual prompts~\cite{hong2023visual}, and (2) professional writing tasks may involve writing comprehensive reports or analyses from visual data~\cite{hartsock2024vision}. To meet these practical needs, it is essential to enhance the long-output capabilities of VLMs.


To investigate the reasons behind the limited long-output capability of VLMs, we are inspired by the LongWriter~\cite{bai2024longwriter}, which adjusts the output length distribution of the supervised fine-tuning (SFT) data to observe changes in model output length. Our experiments revealed that the proportion of long-output examples in the SFT data determines the model's output length. This finding explains why VLMs typically have an output length limit of around 1,000 words. Existing visual instruction tuning datasets~\cite{schuhmann2022laion}, such as LLaVA~\cite{liu2024visual}, mainly contain tasks like grounding~\cite{liu2024grounding} and caption generation~\cite{wang2022git}, with most outputs being less than 300 words~\cite{lin2014microsoft}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/intro_v5.pdf}
    \caption{Left: Six examples for each type of task in MMLongBench-Write. They are divided into two categories: professional writing and creative writing. The former requires professional knowledge, while the latter does not. Right: The joint distribution of the number of input images and the expected output length for data in both categories. Most data requires a 1000+ word output with given images, challenging the long-generation capabilities of VLMs.}
    %\vspace{-2mm}
    \label{fig:intro}
\end{figure*}

% To ensure the quality and difficulty of test data, we combine automated and manual reviews during data collection. 
% We first recruit 97 data annotators with diverse academic backgrounds and grades from top universities and then select 24 data reviewers from this group.
% Annotators provide data including long documents, questions, options, answers, and evidence.
% We then leverage three long-context LLMs for an automated review, where a question is considered too easy if all three LLMs answer it correctly.
% Data passing the automated review are assigned to the reviewers, who answer the questions and determine whether the questions are appropriate (meet our requirements) and if the answers are correct.
% In our criteria, a qualified data point should have (1) an appropriate question with an objective, correct answer; (2) sufficient difficulty, such that all three LLMs cannot answer correctly at the same time, and the human reviewer cannot answer correctly within 3 minutes, even with searching tools within the document.
% If data do not meet these criteria, we request modifications from the annotator.
% We also set length and difficulty incentives to encourage longer and harder test data.

% Overall, our data shows a median word count of 54k and an average of 104k words. 
% Human experts are able to achieve an accuracy of only 53.7\% within 15 minutes, compared to 25\% accuracy with random guessing, highlighting the challenging nature of the test.
% In the evaluation, the best-performing model achieves only 50.1\% accuracy when directly outputting the answer. In contrast, the o1-preview model, which incorporates longer reasoning during inference, reaches 57.7\%, surpassing human experts. This implies that LongBench v2 places greater demands on the reasoning ability of current models, and incorporating more inference-time thinking and reasoning appears to be a natural and crucial step in addressing such long-context reasoning challenges.
% We hope LongBench v2 will accelerate the exploration of how scaling inference-time compute will affect deep understanding and reasoning in long-context scenarios.


To fill the gap, we select long-output instruction-image pairs from MMEvol~\cite{luo2024mmevol} as inputs. In addition to single-image inputs, we also constructed other forms of data, including multi-image inputs and backtranslated instructions~\cite{wang2024weaver}, to enrich the diversity of the input data. To generate long outputs, we propose a plan-and-write approach: LongWriter-Agent-V. This method involves providing input images and writing instructions to GPT-4o to first generate an outline and then sequentially write the text in segments. Through this approach, we collect LongWrite-V-22k, a dataset of 22k long-output examples. 


Using LongWrite-V-22k for SFT, the output length of Qwen2.5-VL-7B-Instruct~\cite{Qwen2.5-VL} can be extended beyond 3,000 words. However, longer outputs often introduce issues such as repetition and hallucination~\cite{favero2024multi}. To improve the fidelity of long outputs, we adapted the approach from RLHF-V~\cite{yu2024rlhf}, where human experts revise the model's outputs to form preference pairs for Direct Preference Optimization (DPO). Since traditional DPO~\cite{rafailov2024direct} is typically performed on short texts of around 300 words, and LongWriter-V's output length can exceed 3,000 words, the annotation cost is extremely high. To enhance the efficiency of preference data utilization, we proposed IterDPO, which divides long outputs into N segments, treating each segment's revision as a preference pair. This method allows the model to learn fine-grained human corrections for each segment and effectively multiplies the use of a single long-output preference pair by N times. Through LongWriter-V-22k SFT and IterDPO, our 7B model achieves impressive performance in both output length and quality, surpassing powerful VLMs like GPT-4o.


In summary, our contributions are as follows:
\begin{itemize}
\item We construct MMLongBench-Write to evaluate the long-output capabilities of VLMs and find that the output length limit of existing VLMs is around 1,000 words.
\item We collect the SFT dataset LongWrite-V-22k, enabling VLMs for 3,000+ word generation.
\item We propose IterDPO, which effectively improves the text quality of long-output VLM.
\end{itemize}