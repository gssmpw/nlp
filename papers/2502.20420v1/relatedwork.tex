\section{Related Work}
\label{sec:related-work}
% All the three tasks within the scope of this workshop: Neural Machine Translation (NMT), Image Captioning and Multimodal Machine Translation (MMT) broadly follow the same approach, that is \textit{Embed-Encode-Fuse/Attend-Decode}. 
% The exact encoders, fusing mechanisms, and decoders depend on the task and are described in the following sections.
% In this section, we discuss prior work related to our research and recent advancements along the lines of modern Vision Language Models (VLMs). 

Early Neural Machine Translation (NMT) and Image captioning systems~\cite{show2015tell,gao2018image} were based on Recurrent Neural Networks (RNNs) and their variants ~\cite{cho-etal-2014-learning, sutskever2014sequence,cho2014learning,hochreiter1997long}, often incorporating attention mechanisms ~\cite{bahdanau2014neural}. The seminal work of transformers~\cite{vaswani2017attention} paved the way for the development of high-quality image captioning ~\cite{chen2021captioning} as well as translation systems ~\cite{lewis2019bart}, even for low-resource languages~\cite{dabre2021indicbart,gala2023indictrans2}. Multimodal Machine Translation (MMT) systems witnessed a similar shift in their approrach ~\cite{caglayan2016multimodal,yao2020multimodal,guo2023layer}. 
% Prior submissions to the MMT of WAT
% Early Neural Machine Translation (NMT) and Image captioning systems~\cite{show2015tell,gao2018image} were based on Recurrent Neural Networks (RNNs) and their variants ~\cite{cho-etal-2014-learning, sutskever2014sequence,cho2014learning,hochreiter1997long}, often incorporating attention mechanisms ~\cite{bahdanau2014neural}. The seminal work of transformers~\cite{vaswani2017attention} paved the way for the development of high-quality image captioning ~\cite{chen2021captioning} as well as translation systems ~\cite{lewis2019bart}, even for low-resource languages~\cite{dabre2021indicbart,costa2022no,gala2023indictrans2}. Multimodal Machine Translation (MMT) systems witnessed a similar shift in their approrach ~\cite{caglayan2016multimodal,yao2020multimodal,guo2023layer}. 
Prior submissions to the MMT task at Workshop on Asian Translation~\cite{gain2021iitp, gupta2021vita, parida2022silo, dash2023bits, shahid2023odiagenai} also fall in this category. 
% OpenHathi tuned 
% Gemma based

The next generation of Multimodal LLMs~\cite{lu2024deepseek,laurenccon2024matters,tong2024cambrian,xue2024xgen} can handle a variety of complex tasks, including machine translation and captioning, by utilizing cutting-edge architectures as an unified general purpose agent. These models often rely on pre-trained LLMs, with an exception of few, which train the models from scratch ~\cite{team2024chameleon,lu2024unified}. Most of these Vision Language Models (VLMs) follow the architecture of \cite{liu2024visual} where a CLIP \cite{radford2021learning} or a similar encoder is used to encode the image and projected into LLM's representation space using an adapter layer. Notably, \citet{wang2023cogvlm} offers a departure from conventional architectures by using distinct matrices and Feed Forward Networks for image modalities. Recent developments replace the image encoder with SigLIP \cite{zhai2023sigmoid} and the single-layer MLP projector with attention-based pooling \cite{laurenccon2024matters}. 

Advanced backbone LLMs ~\cite{brown2020language,touvron2023llama,achiam2023gpt,team2023gemini,jiang2024mixtral, team2024gemma} however have a primary focus for English and European languages. There have been relatively few LLMs for Indic languages, such as Airavata \cite{gala2024airavata},  Navarsa \cite{NavarasaTeluguLLMLabs}, Kannada LLaMA, Tamil LLaMA \cite{balachandran2023tamilllama}, Odia LLaMA \cite{kohli2023building}, to name a few. However, most of these LLMs are an extension and finetuned version of LLaMA/Gemma for Indic languages, which don't fully capture the nuances of the language. This could be attributed to the fact that Indic languages are under-represented in Common Crawl (which majorly forms the training corpus of LLMs), despite India constituting 18\% of the global population. Hindi, for example, does not show-up in the top 20 languages despite being the 3rd most spoken ~\cite{buck2014n,penedo2023refinedweb}. Closed-source models such as Krutrim~\cite{team2024krutrim} and Sutra~\cite{bendale2024sutra} represent exceptions, as they are trained from scratch. Currently, PALO~\cite{maaz2024palo} is a multimodal LLM that supports only Hindi and Bengali. However, to the best of our knowledge, there are no other open-source multimodal LLMs trained specifically for low-resource Indic languages. In contrast, we developed a multilingual multimodal translation system that supports 10 Indic languages based on our general purpose multimodal model Chitrarth \cite{khan2025chitrarth}.
% TODO
%