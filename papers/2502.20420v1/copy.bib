@incollection{sen2022bengali-visgen,
  title={Bengali Visual Genome: A Multimodal Dataset for Machine Translation and Image Captioning},
  author={Sen, Arghyadeep and Parida, Shantipriya and Kotwal, Ketan and Panda, Subhadarshi and Bojar, Ond{\v{r}}ej and Dash, Satya Ranjan},
  booktitle={Intelligent Data Engineering and Analytics},
  pages={63--70},
  year={2022},
  publisher={Springer}
}

@article{hindi-visual-genome:2019,
  title={{Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine Translation}},
  author={Parida, Shantipriya and Bojar, Ond{\v{r}}ej and Dash, Satya Ranjan},
  journal={Computaci{\'o}n y Sistemas},
  note={Presented at CICLing 2019, La Rochelle, France},
  year = {2019},
  volume = {23},
  number = {4},
  pages = {1499--1505},
  issn = {1405-5546},
}
 @misc{11234/1-3533-malayalam-visgen,
 title = {Malayalam Visual Genome 1.0},
 author = {Parida, Shantipriya and Bojar, Ond{\v r}ej},
 url = {http://hdl.handle.net/11234/1-3533},
 note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Creative Commons - Attribution-{NonCommercial}-{ShareAlike} 4.0 International ({CC} {BY}-{NC}-{SA} 4.0)},
 year = {2021} }

 @article{shi2022adding-visual-info,
  title={Adding Visual Information to Improve Multimodal Machine Translation for Low-Resource Language},
  author={Shi, Xiayang and Yu, Zhenqiang},
  journal={Mathematical Problems in Engineering},
  volume={2022},
  year={2022},
  publisher={Hindawi}
}
@inproceedings{50650-vision-transformer,
title	= {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author	= {Alexander Kolesnikov and Alexey Dosovitskiy and Dirk Weissenborn and Georg Heigold and Jakob Uszkoreit and Lucas Beyer and Matthias Minderer and Mostafa Dehghani and Neil Houlsby and Sylvain Gelly and Thomas Unterthiner and Xiaohua Zhai},
year	= {2021}
}

@article{ZHAO20221-region-attentive,
title = {Region-attentive multimodal neural machine translation},
journal = {Neurocomputing},
volume = {476},
pages = {1-13},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.12.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221019317},
author = {Yuting Zhao and Mamoru Komachi and Tomoyuki Kajiwara and Chenhui Chu},
keywords = {Multimodal neural machine translation, Recurrent neural network, Self-attention network, Object detection, Semantic image regions},
abstract = {We propose a multimodal neural machine translation (MNMT) method with semantic image regions called region-attentive multimodal neural machine translation (RA-NMT). Existing studies on MNMT have mainly focused on employing global visual features or equally sized grid local visual features extracted by convolutional neural networks (CNNs) to improve translation performance. However, they neglect the effect of semantic information captured inside the visual features. This study utilizes semantic image regions extracted by object detection for MNMT and integrates visual and textual features using two modality-dependent attention mechanisms. The proposed method was implemented and verified on two neural architectures of neural machine translation (NMT): recurrent neural network (RNN) and self-attention network (SAN). Experimental results on different language pairs of Multi30k dataset show that our proposed method improves over baselines and outperforms most of the state-of-the-art MNMT methods. Further analysis demonstrates that the proposed method can achieve better translation performance because of its better visual feature use.}
}

@inproceedings{attention-is-all-you-need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{LASKAR2023979-english-assamese-mmt-transliteration,
title = {English-Assamese Multimodal Neural Machine Translation using Transliteration-based Phrase Augmentation Approach},
journal = {Procedia Computer Science},
volume = {218},
pages = {979-988},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.078},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000789},
author = {Sahinur Rahman Laskar and Bishwaraj Paul and Partha Pakray and Sivaji Bandyopadhyay},
keywords = {English-Assamese, MNMT, Multimodal, Phrase Pairs, Transliteration},
abstract = {Neural machine translation (NMT) is a popular machine translation method due to its contextual analyzing ability and end-to-end process flexibility. However, NMT suffers poor translation quality in low-resource contexts, particularly for diverse language pairs. To overcome this issue, multimodal concept has been introduced in NMT, wherein leverage information from different modalities like image or speech in addition to text to enhance automatic translation quality. In this paper, we have investigated multimodal NMT for a low-resource language diverse pair, English-Assamese, by addressing data scarcity and word-order divergence issues. To tackle such issues, a transliteration-based phrase augmentation approach is proposed, that leverages the sub-word level tokens sharing among source-target sequences in the training process via transliteration and provides more word alignment information by the addition of phrase pairs. Also, the relevant image features corresponding to the phrase pairs are augmented by considering a filtering step. With the proposed approach, state-of-the-art multimodal NMT results are attained for both directions of English-Assamese pair translation.}
}

@INPROCEEDINGS{9752181-mmt-for-english-assaemse,
  author={Laskar, Sahinur Rahman and Paul, Bishwaraj and Paudwal, Siddharth and Gautam, Pranjit and Biswas, Nirmita and Pakray, Partha},
  booktitle={2021 International Conference on Computational Performance Evaluation (ComPE)}, 
  title={Multimodal Neural Machine Translation for English–Assamese Pair}, 
  year={2021},
  volume={},
  number={},
  pages={387-392},
  doi={10.1109/ComPE53109.2021.9752181}}

@article{10.1145/3445974-mizo-multimodal-improved,
author = {Lalrempuii, Candy and Soni, Badal and Pakray, Partha},
title = {An Improved English-to-Mizo Neural Machine Translation},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3445974},
doi = {10.1145/3445974},
abstract = {Machine Translation is an effort to bridge language barriers and misinterpretations, making communication more convenient through the automatic translation of languages. The quality of translations produced by corpus-based approaches predominantly depends on the availability of a large parallel corpus. Although machine translation of many Indian languages has progressively gained attention, there is very limited research on machine translation and the challenges of using various machine translation techniques for a low-resource language such as Mizo. In this article, we have implemented and compared statistical-based approaches with modern neural-based approaches for the English–Mizo language pair. We have experimented with different tokenization methods, architectures, and configurations. The performance of translations predicted by the trained models has been evaluated using automatic and human evaluation measures. Furthermore, we have analyzed the prediction errors of the models and the quality of predictions based on variations in sentence length and compared the model performance with the existing baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {may},
articleno = {61},
numpages = {21},
keywords = {TER, Neural machine translation, transformer, low-resource language, Mizo, BLEU, METEOR}
}
@INPROCEEDINGS{mizo-vis-genome,
  author={Khenglawt, Vanlalmuansangi and Laskar, Sahinur Rahman and Manna, Riyanka and Pakray, Partha and Khan, Ajoy Kumar},
  booktitle={2022 IEEE Silchar Subsection Conference (SILCON)}, 
  title={Mizo Visual Genome 1.0 : A Dataset for English-Mizo Multimodal Neural Machine Translation}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/SILCON55242.2022.10028882}}
@article{Pathak2018EnglishMizoMT-english-mizo-neural-statistical,
  title={English–Mizo Machine Translation using neural and statistical approaches},
  author={Amarnath Pathak and Partha Pakray and Jereemi Bentham},
  journal={Neural Computing and Applications},
  year={2018},
  pages={1-17}
}

@INPROCEEDINGS{9500022-mt-english-to-mizo,
  author={Thihlum, Zaitinkhuma and Khenglawt, Vanlalmuansangi and Debnath, Somen},
  booktitle={2020 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)}, 
  title={Machine Translation of English Language to Mizo Language}, 
  year={2020},
  volume={},
  number={},
  pages={92-97},
  doi={10.1109/CCEM50674.2020.00028}}

  
@article{MEETEI20232102-hindi-english-news,
title = {Hindi to English Multimodal Machine Translation on News Dataset in Low Resource Setting},
journal = {Procedia Computer Science},
volume = {218},
pages = {2102-2109},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.186},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001862},
author = {Loitongbam Sanayai Meetei and Salam Michael Singh and Alok Singh and Ringki Das and Thoudam Doren Singh and Sivaji Bandyopadhyay},
keywords = {Multimodal machine translation, Visual attention, Low resource, Hindi, News domain},
abstract = {This work proposes a multimodal Hindi-to-English machine translation on a news corpus by integrating multiple input modalities. The experimental dataset comprises of an image from a news article and its caption in the English-Hindi language. The parallel English-Hindi dataset is constructed by translating the image caption from English-language news articles into Hindi. Unlike standard images used for NLP tasks, images in news articles frequently depict significant events, locations, or individuals. The caption provides a more extensive description of the image, resulting in the inclusion of multiple identified entities. Our methodology consists of encoders for each input modality, allowing for simultaneous consideration of both text and image. We undertake two tests to examine how effectively the image content improves MT systems when 1) the entire image is considered first and 2) the largest object recognized in the image is considered. We employ Byte Pair Encoding (BPE) to represent the text, and the visual attributes are extracted from the image with VGG-19, a pre-trained CNN model. In terms of BLEU and chrF, our Multimodal Machine Translation (MMT) systems are superior than the baseline unimodal NMT system. Our MMT system outperforms the NMT model by +1.8 BLEU and +0.03 chrF.}
}

@InProceedings{10.1007/978-981-19-4831-2_5-caption-translation-object,
author="Bisht, Paritosh
and Solanki, Arun",
editor="Unhelker, Bhuvan
and Pandey, Hari Mohan
and Raj, Gaurav",
title="Exploring Practical Deep Learning Approaches for English-to-Hindi Image Caption Translation Using Transformers and Object Detectors",
booktitle="Applications of Artificial Intelligence and Machine Learning",
year="2022",
publisher="Springer Nature Singapore",
address="Singapore",
pages="47--60",
abstract="Most of the captions available for images are only present in a few languages prominent on the internet. The task of machine translation of image captions aims to democratize this information for other low resource languages through automatic translation. Compared to regular machine translation, image information can also be utilized to improve translated caption quality. The proposed work aims to demonstrate various deep learning techniques and approaches that can be used for optimal and efficient translation of captions from English to Hindi. Results show that transformer-based approaches outperform sequence to sequence approaches across all metrics (around 5--20{\%} higher accuracy scores). Further, pre-trained transformer-based approaches are also able to resolve ambiguity very easily. Results also show that for such low resource scenarios, text only approaches are sufficient enough while multimodal approaches are unable to improve the translation quality. So, text only pre-trained transformers are recommended for most English-to-Hindi image caption translation applications.",
isbn="978-981-19-4831-2"
}

@misc{belinkov2018synthetic,
      title={Synthetic and Natural Noise Both Break Neural Machine Translation}, 
      author={Yonatan Belinkov and Yonatan Bisk},
      year={2018},
      eprint={1711.02173},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
lample2018unsupervised,
title={Unsupervised Machine Translation Using Monolingual Corpora Only},
author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkYTTf-AZ},
}

@inproceedings{
xie2017data-noising,
title={Data Noising as Smoothing in Neural Network Language Models},
author={Ziang Xie and Sida I. Wang and Jiwei Li and Daniel L{\'e}vy and Aiming Nie and Dan Jurafsky and Andrew Y. Ng},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=H1VyHY9gg}
}

@misc{kunchukuttan2020indicnlp,
author = "Anoop Kunchukuttan",
title = "{The IndicNLP Library}",
year = "2020",
howpublished={\url{https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf}}
}
@book{comrie2009world,
  title={The world's major languages},
  author={Comrie, Bernard},
  year={2009},
  publisher={Routledge}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{db-ccppf-83
, author = "D. Dori and M. Ben-Bassat"
, title = "Circumscribing a Convex Polygon by a Polygon of Fewer Sides with Minimal Area Addition"
, journal = "Comput. Vision Graph. Image Process."
, volume = 24
, year = 1983
, pages = "131--159"
, keywords = "polygons, extremal figures, area"
, update = "95.09 korneenko, 93.09 held"
, annote = "erratic"
}

@inproceedings{NIPS2017_3f5ee243-vaswani-transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{shi2022adding-visual-info-multimodal-transformer-hindi,
  title={Adding Visual Information to Improve Multimodal Machine Translation for Low-Resource Language},
  author={Shi, Xiayang and Yu, Zhenqiang},
  journal={Mathematical Problems in Engineering},
  volume={2022},
  year={2022},
  publisher={Hindawi}
}

@article{He2015-resnet,
	author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title = {Deep Residual Learning for Image Recognition},
	journal = {arXiv preprint arXiv:1512.03385},
	year = {2015}
}

@inbook{10.5555/303568.303704-convolutional-neural-net-cnn-lecun,
author = {LeCun, Yann and Bengio, Yoshua},
title = {Convolutional Networks for Images, Speech, and Time Series},
year = {1998},
isbn = {0262511029},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {The Handbook of Brain Theory and Neural Networks},
pages = {255–258},
numpages = {4}
}
@misc{https://doi.org/10.48550/arxiv.1409.1556-vgg,
  doi = {10.48550/ARXIV.1409.1556},
  
  url = {https://arxiv.org/abs/1409.1556},
  
  author = {Simonyan, Karen and Zisserman, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{10.1162-mbart,
    author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
    title = "{Multilingual Denoising Pre-training for Neural Machine Translation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {726-742},
    year = {2020},
    month = {11},
    abstract = "{This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl\_a\_00343},
    url = {https://doi.org/10.1162/tacl\_a\_00343},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00343/1923401/tacl\_a\_00343.pdf},
}




@ARTICLE{och03:asc-giza-plus-plus,
AUTHOR = {Franz Josef Och and Hermann Ney},
TITLE = {A Systematic Comparison of Various Statistical Alignment Models},
JOURNAL= {Computational Linguistics}, 
NUMBER = 1,
VOLUME = 29,
YEAR = 2003,
PAGES = {19--51}}
@inproceedings{10.5555/2969033.2969173-sequence-to-sequence-lstm-sutskever,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@misc{https://doi.org/10.48550/arxiv.1409.0473-bahdanau-rnn,
  doi = {10.48550/ARXIV.1409.0473},
  
  url = {https://arxiv.org/abs/1409.0473},
  
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@ARTICLE{9664333-word-region-alignment,  author={Zhao, Yuting and Komachi, Mamoru and Kajiwara, Tomoyuki and Chu, Chenhui},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Word-Region Alignment-Guided Multimodal Neural Machine Translation},   year={2022},  volume={30},  number={},  pages={244-259},  doi={10.1109/TASLP.2021.3138719}}

@article{10.1007/s10489-022-03331-8-dual-level-mixup,
author = {Ye, Junjie and Guo, Junjun},
title = {Dual-Level Interactive Multimodal-Mixup Encoder for Multi-Modal Neural Machine Translation},
year = {2022},
issue_date = {Sep 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {12},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-022-03331-8},
doi = {10.1007/s10489-022-03331-8},
abstract = {Multi-modal neural machine translation (MNMT), which mainly focuses on the use of image information to guide text translation. Recent MNMT approaches have been shown that incorporating visual features into textual translation framework is helpful to improve machine translation. However, visual features always contain textual unrelated information, but the noisy visual feature fusion problem is rarely considered for traditional MNMT methods. How to extract the useful visual features to enhance textual machine translation is the key point need to be considered for MNMT. In this paper, we propose a novel Dual-level Interactive Multimodal-Mixup Encoder (DLMulMix) based on multimodal-mixup for MNMT, which can extract the useful visual features to enhance textual-level machine translation. We first employ the Textual-visual Gating to extract text related visual features, which we believe that regional features are crucial for MNMT. Then visual grid features are employed in order to establish the image context of the effective regional features. Moreover, an effective visual-textual multimodal-mixup is adopted to align textual features and visual features into multi-modal common space to improve textual-level machine translation. We evaluate our proposed method on the Multi30K dataset. The experimental results show that the proposed approach outperforms the previous efforts for both EN-DE and EN-FR tasks regarding BLEU and METEOR scores.},
journal = {Applied Intelligence},
month = {sep},
pages = {14194–14203},
numpages = {10},
keywords = {Feature fusion, Multi-modal neural machine translation, Dual-level interactive multimodal-mixup encoder, Transformer}
}
@inproceedings{as-facle-87 
, author = "A. Aggarwal and Subhash Suri" 
, title = "Fast algorithms for computing the largest empty rectangle" 
, booktitle = "Proc. 3rd Annu. ACM Sympos. Comput. Geom." 
, year = 1987 
, pages = "278--290" 
, keywords = "orthogon, extremal figures, empty rectangle" 
, cites = "akmsw-gamsa-86, af-nfmer-86, cdl-cler-86, mos-fmrio-85, nhl-merp-84, ZZZ" 
, update = "98.03 bibrelex+mitchell, 95.05 korneenko" 
} 

@book{ps-cgi-90 
, author = "F. P. Preparata and M. I. Shamos" 
, title = "Computational Geometry: An Introduction" 
, edition = "3rd" 
, publisher = "Springer-Verlag" 
, month = oct 
, year = 1990 
, isbn = "3-540-96131-3" 
, succeeds = "ps-cgi-85" 
, update = "98.11 bibrelex" 
}

@unpublished{mr-leir-01
, author = "Asish Mukhopadhya and S. V. Rao" 
, title = "Largest empty isothetic rectangle" 
, note = manuscript
, year = 2001
} 

@incollection{hm-fwecs-88
, author = "M. Houle and A. Maciel"
, title = "Finding the widest empty corridor through a set of points"
, booktitle = "Snapshots of computational and discrete geometry"
, publisher = "Dept. of Computer Science, McGill University"
, address = "Montreal, Canada"
, year = 1988
, pages = "201--213"
, note = "Technical Report SOCS-88.11"
, update = "98.11 bibrelex"
} 

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}

@article{chen2023sharegpt4v,
            title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions},
            author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
            journal={arXiv preprint arXiv:2311.12793},
            year={2023}
          }

@misc{zhai2023sigmoidlosslanguageimage,
      title={Sigmoid Loss for Language Image Pre-Training}, 
      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},
      year={2023},
      eprint={2303.15343},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.15343}, 
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}
 @misc{11234/1-2997-hindi-visgenome,
 title = {Hindi Visual Genome 1.0},
 author = {Parida, Shantipriya and Bojar, Ond{\v r}ej},
 url = {http://hdl.handle.net/11234/1-2997},
 note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Creative Commons - Attribution-{NonCommercial}-{ShareAlike} 4.0 International ({CC} {BY}-{NC}-{SA} 4.0)},
 year = {2019} }

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@INPROCEEDINGS{gain-et-al-not-all,
  author={Gain, Baban and Haque, Rejwanul and Ekbal, Asif},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Not All Contexts are Important: The Impact of Effective Context in Conversational Neural Machine Translation}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN52387.2021.9534444}}

@misc{appicharla2023case,
      title={A Case Study on Context Encoding in Multi-Encoder based Document-Level Neural Machine Translation}, 
      author={Ramakrishna Appicharla and Baban Gain and Santanu Pal and Asif Ekbal},
      year={2023},
      eprint={2308.06063},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}