[
  {
    "index": 0,
    "papers": [
      {
        "key": "show2015tell",
        "author": "Show, Attend",
        "title": "Tell: Neural image caption generation with visual attention kelvin xu"
      },
      {
        "key": "gao2018image",
        "author": "Gao, Lizhao and Wang, Bo and Wang, Wenmin",
        "title": "Image captioning with scene-graph based semantic concepts"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cho-etal-2014-learning",
        "author": "Cho, Kyunghyun  and\nvan Merri{\\\"e}nboer, Bart  and\nGulcehre, Caglar  and\nBahdanau, Dzmitry  and\nBougares, Fethi  and\nSchwenk, Holger  and\nBengio, Yoshua",
        "title": "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation"
      },
      {
        "key": "sutskever2014sequence",
        "author": "Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V",
        "title": "Sequence to sequence learning with neural networks"
      },
      {
        "key": "cho2014learning",
        "author": "Cho, Kyunghyun",
        "title": "Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation"
      },
      {
        "key": "hochreiter1997long",
        "author": "Hochreiter, Sepp and Schmidhuber, J{\\\"u}rgen",
        "title": "Long short-term memory"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bahdanau2014neural",
        "author": "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
        "title": "Neural machine translation by jointly learning to align and translate"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2021captioning",
        "author": "Chen, Haishun and Wang, Ying and Yang, Xin and Li, Jie",
        "title": "Captioning transformer with scene graph guiding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lewis2019bart",
        "author": "Lewis, M",
        "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dabre2021indicbart",
        "author": "Dabre, Raj and Shrotriya, Himani and Kunchukuttan, Anoop and Puduppully, Ratish and Khapra, Mitesh M and Kumar, Pratyush",
        "title": "IndicBART: A pre-trained model for indic natural language generation"
      },
      {
        "key": "gala2023indictrans2",
        "author": "Gala, Jay and Chitale, Pranjal A and AK, Raghavan and Gumma, Varun and Doddapaneni, Sumanth and Kumar, Aswanth and Nawale, Janki and Sujatha, Anupama and Puduppully, Ratish and Raghavan, Vivek and others",
        "title": "Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "caglayan2016multimodal",
        "author": "Caglayan, Ozan and Barrault, Lo{\\\"\\i}c and Bougares, Fethi",
        "title": "Multimodal attention for neural machine translation"
      },
      {
        "key": "yao2020multimodal",
        "author": "Yao, Shaowei and Wan, Xiaojun",
        "title": "Multimodal transformer for multimodal machine translation"
      },
      {
        "key": "guo2023layer",
        "author": "Guo, Junjun and Ye, Junjie and Xiang, Yan and Yu, Zhengtao",
        "title": "Layer-level progressive transformer with modality difference awareness for multi-modal neural machine translation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "show2015tell",
        "author": "Show, Attend",
        "title": "Tell: Neural image caption generation with visual attention kelvin xu"
      },
      {
        "key": "gao2018image",
        "author": "Gao, Lizhao and Wang, Bo and Wang, Wenmin",
        "title": "Image captioning with scene-graph based semantic concepts"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cho-etal-2014-learning",
        "author": "Cho, Kyunghyun  and\nvan Merri{\\\"e}nboer, Bart  and\nGulcehre, Caglar  and\nBahdanau, Dzmitry  and\nBougares, Fethi  and\nSchwenk, Holger  and\nBengio, Yoshua",
        "title": "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation"
      },
      {
        "key": "sutskever2014sequence",
        "author": "Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V",
        "title": "Sequence to sequence learning with neural networks"
      },
      {
        "key": "cho2014learning",
        "author": "Cho, Kyunghyun",
        "title": "Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation"
      },
      {
        "key": "hochreiter1997long",
        "author": "Hochreiter, Sepp and Schmidhuber, J{\\\"u}rgen",
        "title": "Long short-term memory"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "bahdanau2014neural",
        "author": "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
        "title": "Neural machine translation by jointly learning to align and translate"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chen2021captioning",
        "author": "Chen, Haishun and Wang, Ying and Yang, Xin and Li, Jie",
        "title": "Captioning transformer with scene graph guiding"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lewis2019bart",
        "author": "Lewis, M",
        "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dabre2021indicbart",
        "author": "Dabre, Raj and Shrotriya, Himani and Kunchukuttan, Anoop and Puduppully, Ratish and Khapra, Mitesh M and Kumar, Pratyush",
        "title": "IndicBART: A pre-trained model for indic natural language generation"
      },
      {
        "key": "costa2022no",
        "author": "Costa-juss{\\`a}, Marta R and Cross, James and {\\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others",
        "title": "No language left behind: Scaling human-centered machine translation"
      },
      {
        "key": "gala2023indictrans2",
        "author": "Gala, Jay and Chitale, Pranjal A and AK, Raghavan and Gumma, Varun and Doddapaneni, Sumanth and Kumar, Aswanth and Nawale, Janki and Sujatha, Anupama and Puduppully, Ratish and Raghavan, Vivek and others",
        "title": "Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "caglayan2016multimodal",
        "author": "Caglayan, Ozan and Barrault, Lo{\\\"\\i}c and Bougares, Fethi",
        "title": "Multimodal attention for neural machine translation"
      },
      {
        "key": "yao2020multimodal",
        "author": "Yao, Shaowei and Wan, Xiaojun",
        "title": "Multimodal transformer for multimodal machine translation"
      },
      {
        "key": "guo2023layer",
        "author": "Guo, Junjun and Ye, Junjie and Xiang, Yan and Yu, Zhengtao",
        "title": "Layer-level progressive transformer with modality difference awareness for multi-modal neural machine translation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "gain2021iitp",
        "author": "Gain, Baban and Bandyopadhyay, Dibyanayan and Ekbal, Asif",
        "title": "IITP at WAT 2021: System description for English-Hindi multimodal translation task"
      },
      {
        "key": "gupta2021vita",
        "author": "Gupta, Kshitij and Gautam, Devansh and Mamidi, Radhika",
        "title": "ViTA: Visual-linguistic translation by aligning object tags"
      },
      {
        "key": "parida2022silo",
        "author": "Parida, Shantipriya and Panda, Subhadarshi and Gr{\\\"o}nroos, Stig-Arne and Granroth-Wilding, Mark and Koistinen, Mika",
        "title": "Silo NLP's Participation at WAT2022"
      },
      {
        "key": "dash2023bits",
        "author": "Dash, Amulya and Gupta, Hrithik Raj and Sharma, Yashvardhan",
        "title": "BITS-P at WAT 2023: Improving Indic Language Multimodal Translation by Image Augmentation using Diffusion Models"
      },
      {
        "key": "shahid2023odiagenai",
        "author": "Shahid, Sk and Kohli, Guneet Singh and Sekhar, Sambit and Dhal, Debasish and Sharma, Adit and Kushwaha, Shubhendra and Parida, Shantipriya and Gr{\\\"o}nroos, Stig-Arne and Dash, Satya Ranjan",
        "title": "OdiaGenAI\u2019s Participation at WAT2023"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "lu2024deepseek",
        "author": "Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others",
        "title": "Deepseek-vl: towards real-world vision-language understanding"
      },
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      },
      {
        "key": "tong2024cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others",
        "title": "Cambrian-1: A fully open, vision-centric exploration of multimodal llms"
      },
      {
        "key": "xue2024xgen",
        "author": "Xue, Le and Shu, Manli and Awadalla, Anas and Wang, Jun and Yan, An and Purushwalkam, Senthil and Zhou, Honglu and Prabhu, Viraj and Dai, Yutong and Ryoo, Michael S and others",
        "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "team2024chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      },
      {
        "key": "lu2024unified",
        "author": "Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha",
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wang2023cogvlm",
        "author": "Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others",
        "title": "Cogvlm: Visual expert for pretrained language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "zhai2023sigmoid",
        "author": "Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas",
        "title": "Sigmoid loss for language image pre-training"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "laurenccon2024matters",
        "author": "Lauren{\\c{c}}on, Hugo and Tronchon, L{\\'e}o and Cord, Matthieu and Sanh, Victor",
        "title": "What matters when building vision-language models?"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "key": "jiang2024mixtral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others",
        "title": "Mixtral of experts"
      },
      {
        "key": "team2024gemma",
        "author": "Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others",
        "title": "Gemma: Open models based on gemini research and technology"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "gala2024airavata",
        "author": "Jay Gala and Thanmay Jayakumar and Jaavid Aktar Husain and Aswanth Kumar M and Mohammed Safi Ur Rahman Khan and Diptesh Kanojia and Ratish Puduppully and Mitesh M. Khapra and Raj Dabre and Rudra Murthy and Anoop Kunchukuttan",
        "title": "Airavata: Introducing Hindi Instruction-tuned LLM"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "NavarasaTeluguLLMLabs",
        "author": "Telugu Labs",
        "title": "Navarsa: Indic LLMs based on Gemmma"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "balachandran2023tamilllama",
        "author": "Abhinand Balachandran",
        "title": "Tamil-Llama: A New Tamil Language Model Based on Llama 2"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "kohli2023building",
        "author": "Guneet Singh Kohli and Shantipriya Parida and Sambit Sekhar and Samirit Saha and Nipun B Nair and Parul Agarwal and Sonal Khosla and Kusumlata Patiyal and Debasish Dhal",
        "title": "Building a Llama2-finetuned LLM for Odia Language Utilizing Domain Knowledge Instruction Set"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "buck2014n",
        "author": "Buck, Christian and Heafield, Kenneth and Van Ooyen, Bas",
        "title": "N-gram Counts and Language Models from the Common Crawl."
      },
      {
        "key": "penedo2023refinedweb",
        "author": "Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien",
        "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "team2024krutrim",
        "author": "Aditya Kallappa and Palash Kamble and Abhinav Ravi and Akshat Patidar and Vinayak Dhruv and Deepak Kumar and Raghav Awasthi and Arveti Manjunath and Shubham Agarwal and Kumar Ashish and Gautam Bhargava and Chandra Khatri and Krutrim Team",
        "title": "Krutrim {LLM}: Multilingual Foundational Model for over a Billion People"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "bendale2024sutra",
        "author": "Bendale, Abhijit and Sapienza, Michael and Ripplinger, Steven and Gibbs, Simon and Lee, Jaewon and Mistry, Pranav",
        "title": "SUTRA: Scalable Multilingual Language Model Architecture"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "maaz2024palo",
        "author": "Maaz, Muhammad and Rasheed, Hanoona and Shaker, Abdelrahman and Khan, Salman and Cholakal, Hisham and Anwer, Rao M and Baldwin, Tim and Felsberg, Michael and Khan, Fahad S",
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "khan2025chitrarth",
        "author": "Khan, Shaharukh and Tarun, Ayush and Ravi, Abhinav and Faraz, Ali and Pokala, Praveen Kumar and Bhangare, Anagha and Kolla, Raja and Khatri, Chandra and Agarwal, Shubham and Krutrim, AI",
        "title": "Chitrarth: Bridging Vision and Language for a Billion People"
      }
    ]
  }
]