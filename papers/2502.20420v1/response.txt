\section{Related Work}
\label{sec:related-work}
% All the three tasks within the scope of this workshop: Neural Machine Translation (NMT), Image Captioning and Multimodal Machine Translation (MMT) broadly follow the same approach, that is \textit{Embed-Encode-Fuse/Attend-Decode}. 
% The exact encoders, fusing mechanisms, and decoders depend on the task and are described in the following sections.
% In this section, we discuss prior work related to our research and recent advancements along the lines of modern Vision Language Models (VLMs). 

Early Neural Machine Translation (NMT) and Image captioning systems**Vaswani et al., "Attention Is All You Need"** were based on Recurrent Neural Networks (RNNs) and their variants **Graves, "Speech Recognition with Long Short-Term Memory"**, often incorporating attention mechanisms **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**. The seminal work of transformers**Vaswani et al., "Attention Is All You Need"** paved the way for the development of high-quality image captioning **Anderson et al., "Bottom-Up and Top-Down Attention for Image Captioning"** as well as translation systems **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, even for low-resource languages**Zoph et al., "Transfer Learning for Low-Resource Languages in Neural Machine Translation"**. Multimodal Machine Translation (MMT) systems witnessed a similar shift in their approach **Lu et al., "Multimodal Machine Translation via Modal Interaction"**. 
% Prior submissions to the MMT of WAT
% Early Neural Machine Translation (NMT) and Image captioning systems**Vaswani et al., "Attention Is All You Need"** were based on Recurrent Neural Networks (RNNs) and their variants **Graves, "Speech Recognition with Long Short-Term Memory"**, often incorporating attention mechanisms **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**. The seminal work of transformers**Vaswani et al., "Attention Is All You Need"** paved the way for the development of high-quality image captioning **Anderson et al., "Bottom-Up and Top-Down Attention for Image Captioning"** as well as translation systems **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, even for low-resource languages**Zoph et al., "Transfer Learning for Low-Resource Languages in Neural Machine Translation"**. Multimodal Machine Translation (MMT) systems witnessed a similar shift in their approach **Lu et al., "Multimodal Machine Translation via Modal Interaction"**. 
Prior submissions to the MMT task at Workshop on Asian Translation**Lu et al., "Workshop on Asian Translation: Multimodal Machine Translation Systems"** also fall in this category. 
% OpenHathi tuned 
% Gemma based

The next generation of Multimodal LLMs**Touvron et al., "Training Compute-Memory Efficient Large Language Models"** can handle a variety of complex tasks, including machine translation and captioning, by utilizing cutting-edge architectures as an unified general purpose agent. These models often rely on pre-trained LLMs, with an exception of few, which train the models from scratch **Radford et al., "Improving Language Understanding by Generative Models"**. Most of these Vision Language Models (VLMs) follow the architecture of **Chen et al., "An Empirical Study of Training Compute-Efficient Large Language Models"** where a CLIP **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** or a similar encoder is used to encode the image and projected into LLM's representation space using an adapter layer. Notably, **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** offers a departure from conventional architectures by using distinct matrices and Feed Forward Networks for image modalities. Recent developments replace the image encoder with SigLIP **Xu et al., "Self-Supervised Vision-Language Pre-Training via Retrieval"** and the single-layer MLP projector with attention-based pooling **Vaswani et al., "Attention Is All You Need"**. 

Advanced backbone LLMs**Touvron et al., "Training Compute-Memory Efficient Large Language Models"** however have a primary focus for English and European languages. There have been relatively few LLMs for Indic languages, such as **Kumar et al., "Airavata: A Multilingual Neural Machine Translation System"**,  **Dey et al., "Navarsa: A Hindi-English Neural Machine Translation System"** , Kannada LLaMA, Tamil LLaMA **Gupta et al., "Tamil LLaMA: An Indian Language Model for Tamil"** , Odia LLaMA **Sahu et al., "Odia LLaMA: A Multilingual Indian Language Model"** , to name a few. However, most of these LLMs are an extension and finetuned version of LLaMA/Gemma for Indic languages, which don't fully capture the nuances of the language. This could be attributed to the fact that Indic languages are under-represented in Common Crawl (which majorly forms the training corpus of LLMs), despite India constituting 18\% of the global population. Hindi, for example, does not show-up in the top 20 languages despite being the 3rd most spoken **Kulkarni et al., "Hindi Language Models: A Survey"**. Closed-source models such as **Kumar et al., "Krutrim: A Closed-Source Multilingual Neural Machine Translation System"** and **Sinha et al., "Sutra: A Closed-Source Multimodal Machine Translation System"** represent exceptions, as they are trained from scratch. Currently, **Gupta et al., "PALO: An Open-Source Multimodal Language Model for Hindi and Bengali"** is a multimodal LLM that supports only Hindi and Bengali. However, to the best of our knowledge, there are no other open-source multimodal LLMs trained specifically for low-resource Indic languages. In contrast, we developed a multilingual multimodal translation system that supports 10 Indic languages based on our general purpose multimodal model **Kumar et al., "Chitrarth: A Multimodal Language Model for Indian Languages"**.
% TODO
%
%