\section{Related Work}
\label{sec:related-work}
% All the three tasks within the scope of this workshop: Neural Machine Translation (NMT), Image Captioning and Multimodal Machine Translation (MMT) broadly follow the same approach, that is \textit{Embed-Encode-Fuse/Attend-Decode}. 
% The exact encoders, fusing mechanisms, and decoders depend on the task and are described in the following sections.
% In this section, we discuss prior work related to our research and recent advancements along the lines of modern Vision Language Models (VLMs). 

Early Neural Machine Translation (NMT) and Image captioning systems____ were based on Recurrent Neural Networks (RNNs) and their variants ____, often incorporating attention mechanisms ____. The seminal work of transformers____ paved the way for the development of high-quality image captioning ____ as well as translation systems ____, even for low-resource languages____. Multimodal Machine Translation (MMT) systems witnessed a similar shift in their approrach ____. 
% Prior submissions to the MMT of WAT
% Early Neural Machine Translation (NMT) and Image captioning systems____ were based on Recurrent Neural Networks (RNNs) and their variants ____, often incorporating attention mechanisms ____. The seminal work of transformers____ paved the way for the development of high-quality image captioning ____ as well as translation systems ____, even for low-resource languages____. Multimodal Machine Translation (MMT) systems witnessed a similar shift in their approrach ____. 
Prior submissions to the MMT task at Workshop on Asian Translation____ also fall in this category. 
% OpenHathi tuned 
% Gemma based

The next generation of Multimodal LLMs____ can handle a variety of complex tasks, including machine translation and captioning, by utilizing cutting-edge architectures as an unified general purpose agent. These models often rely on pre-trained LLMs, with an exception of few, which train the models from scratch ____. Most of these Vision Language Models (VLMs) follow the architecture of ____ where a CLIP ____ or a similar encoder is used to encode the image and projected into LLM's representation space using an adapter layer. Notably, ____ offers a departure from conventional architectures by using distinct matrices and Feed Forward Networks for image modalities. Recent developments replace the image encoder with SigLIP ____ and the single-layer MLP projector with attention-based pooling ____. 

Advanced backbone LLMs ____ however have a primary focus for English and European languages. There have been relatively few LLMs for Indic languages, such as Airavata ____,  Navarsa ____, Kannada LLaMA, Tamil LLaMA ____, Odia LLaMA ____, to name a few. However, most of these LLMs are an extension and finetuned version of LLaMA/Gemma for Indic languages, which don't fully capture the nuances of the language. This could be attributed to the fact that Indic languages are under-represented in Common Crawl (which majorly forms the training corpus of LLMs), despite India constituting 18\% of the global population. Hindi, for example, does not show-up in the top 20 languages despite being the 3rd most spoken ____. Closed-source models such as Krutrim____ and Sutra____ represent exceptions, as they are trained from scratch. Currently, PALO____ is a multimodal LLM that supports only Hindi and Bengali. However, to the best of our knowledge, there are no other open-source multimodal LLMs trained specifically for low-resource Indic languages. In contrast, we developed a multilingual multimodal translation system that supports 10 Indic languages based on our general purpose multimodal model Chitrarth ____.
% TODO
%