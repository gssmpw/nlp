\section{Related Work}
\textbf{RAG Systems. } RAG systems are characterized as a "Retrieve-then-Read" framework **Gao, et al., "Learning Dual-Encoder Question Embeddings for Bidirectional and Unidirectional Transformers"**. The development of Naive RAG has primarily focused on retriever optimization, evolving from discrete retrievers such as BM25 **Robertson, et al., "The Probability Ranking Principle in IR"** to more sophisticated and domain-specific dense retrievers, including DPR **Karpukhin, et al., "Dense Passage Retrieval for Open-Domain Question Answering"** and MedCPT **Guo, et al., "MedCPT: A Medical Concept-Based Dense Passage Retrieval Model for Biomedical Question Answering"**, which demonstrate superior performance.

In recent years, numerous advanced RAG systems have emerged. Advanced RAG systems focus on designing multi-round retrieval structures, including iterative retrieval **Xiong, et al., "Multitask Learning for Multiple Query Retrieval"** , recursive retrieval **Wang, et al., "Recursive Neural Networks for Open-Domain Question Answering"**, and adaptive retrieval **Ren, et al., "Adaptive Multi-Round Retrieval Model with Iterative Refinement"**. A notable work in medical QA is MedRAG **Wang, et al., "MedRAG: A Medical Question Answering System using Dense Passage Retrieval"**, which analyzes retrievers, corpora, and LLMs, offering practical guidelines. Follow-up work, $i$-MedRAG **Zhang, et al., "Iterative MedRAG: An Improved Version of the MedRAG System for Medical Question Answering"** , improved performance through multi-round decomposition and iteration, albeit with significant computational costs.

These approaches focus solely on optimizing the retrieval process, overlooking the retrievability of \textit{factual knowledge}. In contrast, RGAR introduces a recurrent structure, enabling continuous query optimization through dual-end retrieval and extraction from EHRs and professional knowledge corpora, thereby enhancing access to both knowledge types.

\textbf{Query Optimization. } As the core interface in human-AI interaction, query optimization (also known as prompt optimization) is the key to improving AI system performance. It is widely applied in tasks such as text-to-image generation **Henderson, et al., "A Neural Algorithm for Artistic Style"** and code generation **Dong, et al., "CoCoNet: Deep Learning Filters and Memory-Augmented Generative Models of Codes"**.

In the era of large language models, query optimization for retrieval tasks has gained increasing attention. Representative work includes GAR **Gao, et al., "Graph-Based Attention Model for Query Expansion in Open-Domain Question Answering"**, which improves retrieval performance through query expansion using fine-tuned BERT models **Devlin, et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. GENREAD **Guo, et al., "GENREAD: Generating Document-Style Contextualized Reasoning Evidence for Open-Domain Question Answering"** further explored whether LLM-generated contexts could replace retrieved professional documents as reasoning evidence. MedGENIE **Zhang, et al., "MedGENIE: A Medical GENeralization and Extraction Framework for Open-Domain Question Answering"** extended this approach to medical QA.

Another line of work focuses on query transformation and decomposition, breaking down original queries into multiple sub-queries tailored to specific tasks, enhancing retrieval alignment with model needs **Yang, et al., "Query Decomposition via Dependency Tree"**. Subsequent work has reinforced the effectiveness of query decomposition through fine-tuning **Hou, et al., "Efficient Fine-Tuning of Pre-Trained Transformers for Question Answering Tasks"**.

Using expanded queries directly as reasoning evidence lacks the transparency of RAG, as RAG relies on retrievable documents that provide traceable and trustworthy reasoning, which is crucial in the medical field.
Besides, the effectiveness of query expansion and query decomposition approaches is heavily dependent on fine-tuning LLMs, which limits scalability.

%Additionally, domain-specific LLMs that generate reasoning evidence face challenges in knowledge updating **Zhu, et al., "Knowledge Updating for Domain-Specific Large Language Models"**, making RAG a more robust solution.

In contrast, our work focuses on query optimization without fine-tuning LLMs. Specifically, retrieval from EHRs can be seen as query filtering that eliminates irrelevant information, thereby obtaining pertinent \textit{factual knowledge}. Extracting factual knowledge enhances the effectiveness of retrieval from the corpus.

%\subsection{Medical Question Answering}

%Recent medical QA datasets such as MMLU-Med (Measuring Massive Multitask Language Understanding), PubMedQA (PubMedQA: A Dataset for Biomedical Research Question Answering), and BioASQ-Y/N (An Overview of the BIOASQ Large-Scale Biomedical Semantic Indexing and Question Answering Competition) require models to master vast amounts of medical knowledge not provided within the question context, exemplifying the challenges of open-domain question answering. The MIRAGE benchmark adopts a Question-Only Retrieval (QOR) paradigm, aligning with real-world cases of medical QA, where answer options should not be presented as input during retrieval.

%To better approximate clinical diagnosis scenarios, some datasets, such as MedQA-US (What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams) and MedMCQA (MedMCQA: A Large-Scale Multi-Subject Multi-Choice Dataset for Medical Domain Question Answering), incorporate specific patient cases within their questions, demanding that models apply medical knowledge to resolve practical issues. This represents a simplified form of factual-aware medical question answering. The latest dataset, EHRNoteQA, utilizes original EHR data from MIMIC-IV, necessitating that models accurately identify which factual information within the EHR aligns with the posed question and leverage specialized knowledge to formulate answers.

%Our approach adopts the MIRAGE benchmark's framework, focusing on enhancing models' capabilities in factual-aware medical question answering.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{pipeline.pdf}
    \caption{The Overall Framework of RGAR. a) The Recurrence Pipeline in ยง \ref{sec:pipeline}; b) Conceptual Knowledge Retrieval in ยง \ref{sec:Train-free}; c) Factual Knowledge Extraction in ยง \ref{sec:Extraction}; d) Response Template in ยง \ref{sec:pipeline}.}
    \label{fig:pipeline}
\end{figure*}