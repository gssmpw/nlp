\section{Related Work}
\textbf{RAG Systems. } RAG systems are characterized as a "Retrieve-then-Read" framework \cite{gao2023retrieval}. The development of Naive RAG has primarily focused on retriever optimization, evolving from discrete retrievers such as BM25 \cite{friedman1977algorithm} to more sophisticated and domain-specific dense retrievers, including DPR \cite{karpukhin-etal-2020-dense} and MedCPT \cite{jin2023medcpt}, which demonstrate superior performance.

In recent years, numerous advanced RAG systems have emerged. Advanced RAG systems focus on designing multi-round retrieval structures, including iterative retrieval \cite{sun2019pullnet}, recursive retrieval \cite{sarthi2024raptor}, and adaptive retrieval \cite{jeong-etal-2024-adaptive}. A notable work in medical QA is MedRAG \cite{xiong-etal-2024-benchmarking}, which analyzes retrievers, corpora, and LLMs, offering practical guidelines. Follow-up work, $i$-MedRAG \cite{xiong2024improving}, improved performance through multi-round decomposition and iteration, albeit with significant computational costs.

These approaches focus solely on optimizing the retrieval process, overlooking the retrievability of \textit{factual knowledge}. In contrast, RGAR introduces a recurrent structure, enabling continuous query optimization through dual-end retrieval and extraction from EHRs and professional knowledge corpora, thereby enhancing access to both knowledge types.

\textbf{Query Optimization. } As the core interface in human-AI interaction, query optimization (also known as prompt optimization) is the key to improving AI system performance. It is widely applied in tasks such as text-to-image generation \cite{liu2022compositional, wu-etal-2024-universal} and code generation \cite{nazzal2024promsec}.

In the era of large language models, query optimization for retrieval tasks has gained increasing attention. Representative work includes GAR \cite{mao-etal-2021-generation}, which improves retrieval performance through query expansion using fine-tuned BERT models \cite{devlin-etal-2019-bert}. GENREAD \cite{yu2023generate} further explored whether LLM-generated contexts could replace retrieved professional documents as reasoning evidence. MedGENIE \cite{frisoni-etal-2024-generate} extended this approach to medical QA.

Another line of work focuses on query transformation and decomposition, breaking down original queries into multiple sub-queries tailored to specific tasks, enhancing retrieval alignment with model needs \cite{dhuliawala2023chain}. Subsequent work has reinforced the effectiveness of query decomposition through fine-tuning \cite{ma2023query}.

Using expanded queries directly as reasoning evidence lacks the transparency of RAG, as RAG relies on retrievable documents that provide traceable and trustworthy reasoning, which is crucial in the medical field.
Besides, the effectiveness of query expansion and query decomposition approaches is heavily dependent on fine-tuning LLMs, which limits scalability.

%Additionally, domain-specific LLMs that generate reasoning evidence face challenges in knowledge updating \cite{wang2024knowledge}, making RAG a more robust solution.

In contrast, our work focuses on query optimization without fine-tuning LLMs. Specifically, retrieval from EHRs can be seen as query filtering that eliminates irrelevant information, thereby obtaining pertinent \textit{factual knowledge}. Extracting factual knowledge enhances the effectiveness of retrieval from the corpus.

%\subsection{Medical Question Answering}

%Recent medical QA datasets such as MMLU-Med (Measuring Massive Multitask Language Understanding), PubMedQA (PubMedQA: A Dataset for Biomedical Research Question Answering), and BioASQ-Y/N (An Overview of the BIOASQ Large-Scale Biomedical Semantic Indexing and Question Answering Competition) require models to master vast amounts of medical knowledge not provided within the question context, exemplifying the challenges of open-domain question answering. The MIRAGE benchmark adopts a Question-Only Retrieval (QOR) paradigm, aligning with real-world cases of medical QA, where answer options should not be presented as input during retrieval.

%To better approximate clinical diagnosis scenarios, some datasets, such as MedQA-US (What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams) and MedMCQA (MedMCQA: A Large-Scale Multi-Subject Multi-Choice Dataset for Medical Domain Question Answering), incorporate specific patient cases within their questions, demanding that models apply medical knowledge to resolve practical issues. This represents a simplified form of factual-aware medical question answering. The latest dataset, EHRNoteQA, utilizes original EHR data from MIMIC-IV, necessitating that models accurately identify which factual information within the EHR aligns with the posed question and leverage specialized knowledge to formulate answers.

%Our approach adopts the MIRAGE benchmark's framework, focusing on enhancing models' capabilities in factual-aware medical question answering.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{pipeline.pdf}
    \caption{The Overall Framework of RGAR. a) The Recurrence Pipeline in ยง \ref{sec:pipeline}; b) Conceptual Knowledge Retrieval in ยง \ref{sec:Train-free}; c) Factual Knowledge Extraction in ยง \ref{sec:Extraction}; d) Response Template in ยง \ref{sec:pipeline}.}
    \label{fig:pipeline}
\end{figure*}