% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{colortbl}
\usepackage{graphicx} % 用于插入图片
\usepackage{subcaption} % 用于分割小图和添加子标题
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Sichu Liang \textsuperscript{1}}\thanks{Equal Contribution},
 \textbf{Linhai Zhang \textsuperscript{2}}\footnotemark[1],
 \textbf{Hongyu Zhu \textsuperscript{3}}\footnotemark[1],
 \textbf{Wenwen Wang\textsuperscript{4}},
 \textbf{Yulan He\textsuperscript{2, 5}},
 \textbf{Deyu Zhou \textsuperscript{1}}\thanks{Corresponding author} 
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1}School of Computer Science and Engineering, Key Laboratory of New Generation Artificial Intelligence \\Technology and Its Interdisciplinary Applications, Southeast University, Ministry of Education, China
 \\
\textsuperscript{2}Department of Informatics, King's College London, UK\\
 \textsuperscript{3}	School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China\\
 \textsuperscript{4}School of Electrical and Computer Engineering, Carnegie Mellon University, USA\\
 \textsuperscript{5}The Alan Turing Insitute, UK\\
}

\begin{document}
\maketitle
\begin{abstract}
%Medical question answering demands substantial access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires medical knowledge through large-scale corpus retrieval and transfers this knowledge to a general-purpose large language model (LLM) for generating answers. 
Medical question answering requires extensive access to specialized \textit{conceptual knowledge}. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. 
%However, existing retrieval approaches lack dedicated attention to and consideration of factual knowledge, limiting the relevance and effectiveness of conceptual knowledge retrieval and hindering applications in real-world scenarios such as clinical decision-making based on Electronic Health Records (EHRs).
% However, existing retrieval approaches lack dedicated consideration of \textit{factual knowledge}, limiting the relevance of retrieved conceptual knowledge and hindering applications in real-world scenarios such as clinical decision-making based on Electronic Health Records (EHRs).
However, existing retrieval approaches often overlook the importance of \textit{factual knowledge}, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs).
%This paper presents RGAR, a recurrence generation-augmented retrieval framework that retrieve relevant factual knowledge and conceptual knowledge from dual ends, allowing them to interact and update one another.
This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant \textit{factual} and \textit{conceptual} knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another.
% This paper presents RGAR, a recurrence generation-augmented retrieval framework that leverages retrieved medical knowledge to continuously extract question-relevant factual knowledge from queries and transform it into retrieval-optimized representations, ultimately facilitating more relevant medical knowledge retrieval. 
% Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR sets a new state-of-the-art performance among medical RAG systems.
Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems.
Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. 
%Our findings reveal that extracting factual knowledge significantly enhances system performance, consistently yielding improved retrieval accuracy.
Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities in general question answering (QA) tasks, achieving impressive performance across diverse scenarios \cite{achiam2023gpt}. However, when facing domain-specific questions that require specialized expertise, from medical diagnosis \cite{jin2021disease} to legal charge prediction \cite{wei-etal-2024-mud}, these models face significant challenges, often generating unreliable conclusions due to both hallucinations \cite{ji2023survey} and potentially stale knowledge embedded in their parameters \cite{wang2024knowledge}. % These issues can be especially dangerous in high-stakes domains such as healthcare, where incorrect information could lead to serious consequences \cite{tian2024opportunities}.

% Addressing the task of question answering (QA) presents significant challenges, as it demands intricate reasoning involving both the explicit constraints articulated in the questions and the implicit domain knowledge \cite{frisoni-etal-2024-generate}. Such difficult tasks effectively reflect the complexities of real-life scenarios and are prevalent in fields requiring specialized knowledge, ranging from medical diagnostics \cite{jin2021disease} to predictions of criminal charges \cite{wei-etal-2024-mud}. 

% Open-domain question answering (OpenQA) \cite{chen-etal-2017-reading} aims to deal with real-world queries without relying on expert knowledge from any predefined domain. This approach effectively reflects the complexities of real-life scenarios, where each potential question lacks a pre-labeled body of text containing the answer. As one of the most challenging forms of question answering, OpenQA has been widely applied in professional knowledge reasoning tasks, ranging from medical diagnostics \cite{jin2021disease} to criminal charge predictions \cite{wei-etal-2024-mud}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{small.pdf}
    \caption{a) Medical AI Systems from the Perspective of Bloom's Taxonomy. b) Two Types of Medical Question Answering Tasks.}
    \label{fig:enter-label}
\end{figure}

\textbf{Retrieval-Augmented Generation (RAG)} \cite{lewis2020retrieval} has emerged as a promising approach to address these challenges by leveraging extensive, trustworthy knowledge bases to support LLM reasoning. The effectiveness of this approach, however, heavily depends on the relevance of retrieved documents. Recent advances, such as \textbf{Generation-Augmented Retrieval (GAR)} \cite{mao-etal-2021-generation}, focus on enhancing retrieval performance by generating relevant context for query expansion.

% By retrieving relevant document chunks from extensive, trustworthy knowledge bases to assist LLMs, \textbf{Retrieval-Augmented Generation (RAG)} \cite{lewis2020retrieval} has shown promise in tackling the above challenges. However, the relevance of the retrieved documents plays a crucial role in the model's reasoning capability. Recent approaches, such as \textbf{Generation-Augmented Retrieval (GAR)} \cite{mao-etal-2021-generation}, 
% focus on query formulation and propose query expansion methods to enhance generation performance.
% focus on how query formulation influences document relevance, proposing methods expending queries into multiple variations to enhance generation performance.

In the medical domain, current RAG approaches concatenate all available contextual information from a given example into a single basic query for retrieval, aiming to provide comprehensive context for model reasoning \cite{xiong-etal-2024-benchmarking}. While this method has demonstrated substantial improvements on early \textit{knowledge-intensive} medical QA datasets such as PubMedQA \cite{jin-etal-2019-pubmedqa}, its limitations have become increasingly apparent with the emergence of EHR-integrated datasets that better reflect real-world clinical practices \cite{kweon2024ehrnoteqa}. Electronic Health Records (EHRs) typically contain extensive patient data, including comprehensive diagnostic test results and medical histories \cite{pang2021cehr}. However, for any specific medical query, only a small subset of this information is typically relevant, and retrieval performance can be significantly degraded when queries are diluted with extraneous EHR content \cite{johnson2023mimic, lovon-melgarejo-etal-2024-revisiting}.

% The current RAG approach to solving medical problems concatenates all contextual information from a given example into a basic query for retrieval, aiming to capture the most comprehensive content for model reasoning \cite{xiong-etal-2024-benchmarking}. This method has achieved significant improvements on early knowledge-intensive medical QA datasets like PubMedQA \cite{jin-etal-2019-pubmedqa}.
% However, the emergence of EHR-integrated datasets, which better align with real-world clinical practices \cite{kweon2024ehrnoteqa}, reveals critical limitations of this paradigm. Electronic Health Records (EHRs) typically contain extensive patient data, including all diagnostic test results and medical histories \cite{pang2021cehr}, yet only a small fraction of this data is relevant to a specific question. Retrieval performance can be impaired when queries contain lengthy, irrelevant texts from EHRs \cite{johnson2023mimic, lovon-melgarejo-etal-2024-revisiting}.

% We highlight that current \textit{retrieval methods} often fail to adequately consider factual information. Real-world medical scenarios are inherently \textbf{factual-aware}, emphasizing the importance of factual information, such as EHRs, which are crucial for providing personalized and accurate medical advice for a specific query.

We highlight that current \textit{retrieval methods} often fail to adequately consider \textit{factual information} in real-world medical scenarios. Crucially, even when applying query expansion with GAR, the persistent oversight of factual information fundamentally limits their ability to retrieve real relevant documents.

% In \textbf{real-world} medical QA scenarios, it is crucial to consider not only \textit{professionally knowledge} but also \textit{factual-aware information}. \textbf{Factual-aware information} consists of essential factual content, such as electronic health records (EHRs), which are essential for delivering personalized and accurate medical advice. Early medical QA datasets primarily focused on professional knowledge \cite{jin-etal-2019-pubmedqa}, while more recent ones have recognized the significance of factual information, designing resources that better align with real-world clinical practice \cite{kweon2024ehrnoteqa}.


% However, current \textit{retrieval methods} often fail to adequately account for factual information. Existing studies simply concatenate EHR data with the query, assuming that retrieving relevant professional knowledge is sufficient for LLMs to solve the corresponding problem \cite{xiong-etal-2024-benchmarking}. This approach relies on the oversimplified assumption that all EHR content is relevant to the specific question, which is rarely the case in practice. EHRs typically contain comprehensive patient data, including all diagnostic test results and medical histories \cite{pang2021cehr}, of which only a small fraction is relevant to any specific question. Furthermore, the verbosity of EHRs can hinder retrieval performance when irrelevant, lengthy texts are included \cite{johnson2023mimic, lovon-melgarejo-etal-2024-revisiting}.

Inspired by \textbf{Bloom's taxonomy} \cite{forehand2010bloom,markus2001toward}, we categorize the knowledge required to address real-world medical QA problems into four types: \textit{Factual Knowledge}, \textit{Conceptual Knowledge}, \textit{Procedural Knowledge}, and \textit{Metacognitive Knowledge}.
The latter two represent higher-order knowledge typically embedded within advanced RAG systems. Specifically, \textit{Procedural Knowledge} refers to the processes and strategies required to solve problems, such as problem decomposition and retrieval \cite{wei2022chain, zhou2023leasttomost}, while \textit{Metacognitive Knowledge} pertains to an LLM's ability to assess whether it has sufficient knowledge or evidence to perform effective reasoning \cite{kim-etal-2023-tree, wang-etal-2023-self-knowledge}.

\textit{Factual Knowledge} and \textit{Conceptual Knowledge} require retrieval from large databases containing substantial amounts of irrelevant content, corresponding to the EHRs of patients and medical corpora in answering medical questions. Unfortunately, current RAG systems do not differentiate between these types of \textit{retrieval targets}, overlooking the necessity of retrieval from EHRs.

% \textit{Factual Knowledge} and \textit{Conceptual Knowledge} involve processing information from extensive databases that contain significant amounts of irrelevant content, corresponding to the EHRs of patients and the corpora of medical knowledge, to answer medical questions. Unfortunately, current RAG systems do not distinguish between these types of \textit{retrieval targets}, overlooking the necessity of retrieving information specifically from EHRs.

To overcome this limitation, we propose \textbf{RGAR}, a system designed to simultaneously retrieves \textit{Factual Knowledge} and \textit{Conceptual Knowledge} through a recurrent query generation and interaction mechanism. This approach iteratively refines queries to enhance the relevance of retrieved professional and factual knowledge, thereby improving performance on \textit{knowledge-intensive} and \textit{factual-aware} medical QA tasks.

Our key contributions are listed as follows:
\begin{itemize}
\item We are the first to analyze RAG systems through the lens of Bloom's taxonomy, addressing the current underrepresentation of \textit{Factual Knowledge} in existing frameworks.
\item We introduce RGAR, a dual-end retrieval system that facilitates recurrent interactions between \textit{Factual} and \textit{Conceptual} Knowledge, bridging the gap between LLMs and real-world clinical applications.
\item Through extensive experiments on three medical QA datasets involving \textit{Factual Knowledge}, we demonstrate that RGAR achieves superior average performance compared to state-of-the-art (SOTA) methods, enabling Llama-3.1-8B-Instruct model to outperform the considerably larger RAG-enhanced GPT-3.5-turbo.
\end{itemize}

\section{Related Work}
\textbf{RAG Systems. } RAG systems are characterized as a "Retrieve-then-Read" framework \cite{gao2023retrieval}. The development of Naive RAG has primarily focused on retriever optimization, evolving from discrete retrievers such as BM25 \cite{friedman1977algorithm} to more sophisticated and domain-specific dense retrievers, including DPR \cite{karpukhin-etal-2020-dense} and MedCPT \cite{jin2023medcpt}, which demonstrate superior performance.

In recent years, numerous advanced RAG systems have emerged. Advanced RAG systems focus on designing multi-round retrieval structures, including iterative retrieval \cite{sun2019pullnet}, recursive retrieval \cite{sarthi2024raptor}, and adaptive retrieval \cite{jeong-etal-2024-adaptive}. A notable work in medical QA is MedRAG \cite{xiong-etal-2024-benchmarking}, which analyzes retrievers, corpora, and LLMs, offering practical guidelines. Follow-up work, $i$-MedRAG \cite{xiong2024improving}, improved performance through multi-round decomposition and iteration, albeit with significant computational costs.

These approaches focus solely on optimizing the retrieval process, overlooking the retrievability of \textit{factual knowledge}. In contrast, RGAR introduces a recurrent structure, enabling continuous query optimization through dual-end retrieval and extraction from EHRs and professional knowledge corpora, thereby enhancing access to both knowledge types.

\textbf{Query Optimization. } As the core interface in human-AI interaction, query optimization (also known as prompt optimization) is the key to improving AI system performance. It is widely applied in tasks such as text-to-image generation \cite{liu2022compositional, wu-etal-2024-universal} and code generation \cite{nazzal2024promsec}.

In the era of large language models, query optimization for retrieval tasks has gained increasing attention. Representative work includes GAR \cite{mao-etal-2021-generation}, which improves retrieval performance through query expansion using fine-tuned BERT models \cite{devlin-etal-2019-bert}. GENREAD \cite{yu2023generate} further explored whether LLM-generated contexts could replace retrieved professional documents as reasoning evidence. MedGENIE \cite{frisoni-etal-2024-generate} extended this approach to medical QA.

Another line of work focuses on query transformation and decomposition, breaking down original queries into multiple sub-queries tailored to specific tasks, enhancing retrieval alignment with model needs \cite{dhuliawala2023chain}. Subsequent work has reinforced the effectiveness of query decomposition through fine-tuning \cite{ma2023query}.

Using expanded queries directly as reasoning evidence lacks the transparency of RAG, as RAG relies on retrievable documents that provide traceable and trustworthy reasoning, which is crucial in the medical field.
Besides, the effectiveness of query expansion and query decomposition approaches is heavily dependent on fine-tuning LLMs, which limits scalability.

%Additionally, domain-specific LLMs that generate reasoning evidence face challenges in knowledge updating \cite{wang2024knowledge}, making RAG a more robust solution.

In contrast, our work focuses on query optimization without fine-tuning LLMs. Specifically, retrieval from EHRs can be seen as query filtering that eliminates irrelevant information, thereby obtaining pertinent \textit{factual knowledge}. Extracting factual knowledge enhances the effectiveness of retrieval from the corpus.

%\subsection{Medical Question Answering}

%Recent medical QA datasets such as MMLU-Med (Measuring Massive Multitask Language Understanding), PubMedQA (PubMedQA: A Dataset for Biomedical Research Question Answering), and BioASQ-Y/N (An Overview of the BIOASQ Large-Scale Biomedical Semantic Indexing and Question Answering Competition) require models to master vast amounts of medical knowledge not provided within the question context, exemplifying the challenges of open-domain question answering. The MIRAGE benchmark adopts a Question-Only Retrieval (QOR) paradigm, aligning with real-world cases of medical QA, where answer options should not be presented as input during retrieval.

%To better approximate clinical diagnosis scenarios, some datasets, such as MedQA-US (What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams) and MedMCQA (MedMCQA: A Large-Scale Multi-Subject Multi-Choice Dataset for Medical Domain Question Answering), incorporate specific patient cases within their questions, demanding that models apply medical knowledge to resolve practical issues. This represents a simplified form of factual-aware medical question answering. The latest dataset, EHRNoteQA, utilizes original EHR data from MIMIC-IV, necessitating that models accurately identify which factual information within the EHR aligns with the posed question and leverage specialized knowledge to formulate answers.

%Our approach adopts the MIRAGE benchmark's framework, focusing on enhancing models' capabilities in factual-aware medical question answering.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{pipeline.pdf}
    \caption{The Overall Framework of RGAR. a) The Recurrence Pipeline in § \ref{sec:pipeline}; b) Conceptual Knowledge Retrieval in § \ref{sec:Train-free}; c) Factual Knowledge Extraction in § \ref{sec:Extraction}; d) Response Template in § \ref{sec:pipeline}.}
    \label{fig:pipeline}
\end{figure*}

\section{Methodology}
% 开头整段都要改
% 已经改过了
In this section, we introduce RGAR framework, as illustrated in Figure \ref{fig:pipeline}. It begins by prompting a general-purpose LLM to generate multiple queries from an initial basic query. These multiple queries are then used to \textbf{retrieve conceptual knowledge} from the corpus (§ \ref{sec:Train-free}). Then retrieved conceptual knowledge is subsequently used to \textbf{extract factual knowledge} from the electronic health records (EHRs) and transform it into retrieval-optimized representations (§ \ref{sec:Extraction}). The \textbf{recurrence pipeline} continuously updates the basic query and iteratively executes the two aforementioned components. This process optimizes the retrieved results, ultimately improving the quality of responses.(§ \ref{sec:pipeline}).
\subsection{Task Formulation}
In \textit{factual-aware} medical QA, each data sample comprises the following elements: a patient's natural language query $\mathcal{Q}$, the electronic health record (EHR) as factual knowledge $\mathcal{F}$, and a set of candidate answer $\mathcal{A} = \{a_1, ..., a_{|\mathcal{A}|}\}$. The overall goal is to identify the correct answer $\hat{a}$ from $\mathcal{A}$.

A \textit{non-retrieval} approach directly prompts an LLM to act as a \textbf{reader}, processing the entire context and generating an answer, formulated as:

\begin{equation}
\hat{a}=\textbf{LLM}(\mathcal{F},\mathcal{Q},\mathcal{A}|\mathcal{T}_r)
\end{equation}

where $\mathcal{T}_r$ is the prompts. However, this approach relies exclusively on the conceptual knowledge encoded within LLM, without leveraging external, trustworthy medical knowledge sources.

To overcome this limitation, recent studies have explored \textit{retrieval-based} approaches, which enhance the model’s knowledge by retrieving a specified number $N$ of chunks, denoted as $\mathcal{C} = \{c_1, ..., c_N\}$, from a chunked corpus (knowledge base) $\mathcal{K}$. This answering process is expressed as:

\begin{equation}
\hat{a}=\textbf{LLM}(\mathcal{F},\mathcal{Q},\mathcal{A},\mathcal{C}|\mathcal{T}_r).
\label{eq:retrieval-augmented}
\end{equation}

\subsection{Conceptual Knowledge Retrieval (CKR)}
\label{sec:Train-free}
To maintain consistency with the \textit{option-free retrieval approach} proposed by \cite{xiong-etal-2024-benchmarking}, we do not incorporate the answer options $\mathcal{A}$ during retrieval. This design is in line with real-world medical quality assurance scenarios, where answer choices are typically not available in advance.

Following their method, we construct the \textbf{basic query} by concatenating the EHR and the patient's query, formally defined as $q_b = \mathcal{Q} \oplus \mathcal{F}$, where $\oplus$ denotes text concatenation.

Traditional dense retrievers, such as Dense Passage Retrieval (DPR) \cite{karpukhin-etal-2020-dense}, identify the top-$N$ relevant chunks $C$ from the knowledge base $\mathcal{K}$ by computing similarity scores using an encoder $E$:

\begin{equation}
\begin{split}
    &\text{sim}(q_b, c_i) = E(q_b)^\top E(c_i), \\
    &\mathcal{C} = \text{top-}N(\{\text{sim}(q_b, c_i)\}).
\end{split}
\end{equation}


Vanilla GAR \cite{mao-etal-2021-generation} expands $q_b$ using a fine-tuned BERT \cite{devlin-etal-2019-bert} to produce three types of content that enhance retrieval: potential answers $q_e^a$, contexts $q_e^c$, and titles $q_e^t$.
With the growing zero-shot generation capabilities of LLMs \cite{kojima2022large}, a common practice is to prompt LLMs to serve as train-free query \textbf{generators}, producing expanded content $\tilde{q}_e$ using prompt templates $\mathcal{T}_g$ \cite{frisoni-etal-2024-generate}. The three types of content generation process can be formulated as:

\begin{equation}
\label{eq:query-generation}
\begin{array}{l}
\tilde{q}_e^a = \textbf{LLM}(q_b |\mathcal{T}^a_g), \\[1ex]
\tilde{q}_e^c = \textbf{LLM}(q_b |\mathcal{T}^c_g), \\[1ex]
\tilde{q}_e^t = \textbf{LLM}(q_b |\mathcal{T}^t_g).
\end{array}
\end{equation}

% 这样改了一下，不知道合不合适
%非常合适
%The final score $Sc$ to get retrieved $\mathcal{C}$ is then obtained by normalizing and averaging the similarities of these expanded queries:
The final score $Sc$ for retrieving $\mathcal{C}$ is then computed by normalizing and averaging the similarities of these expanded queries:
\begin{equation}
\label{eq:normalized-retrieval-score}
\text{Sc}(c_i) = \sum_{\tilde{q}_e \in \{\tilde{q}_e^a, \tilde{q}_e^c, \tilde{q}_e^t\}} \frac{\exp(\text{sim}(\tilde{q}_e, c_i))}{\sum_{c_j} \exp(\text{sim}(\tilde{q}_e, c_j))}.
\end{equation}


\subsection{Factual Knowledge Extraction (FKE)}
\label{sec:Extraction}

In EHR, only a small portion of necessary information constitutes problem-relevant factual knowledge \cite{d2004evaluation}. Direct input of lengthy EHR content containing substantial irrelevant information into dense retrievers can degrade retrieval performance \cite{ren-etal-2023-thorough}. While a straightforward approach would be to retrieve EHR content based on question $\mathcal{Q}$ \cite{factual_aware}, this fails to fully utilize conceptual knowledge obtained from previous Conceptual Knowledge Retrieval Stage. Furthermore, the necessary chunking of EHR for retrieval introduces content discontinuity \cite{luo-etal-2024-landmark}.

Given that EHRs more closely resemble long passages from the Needle in a Haystack task \cite{kamradt2024needle} rather than necessarily chunked corpus, and inspired by large language models' capability to precisely locate answer spans in reading comprehension tasks \cite{cheng2024adapting}, we propose leveraging LLMs for text span tasks \cite{rajpurkar-etal-2016-squad} on EHR to filter relevant factual knowledge efficiently and effectively using conceptual knowledge. We define this filtered factual knowledge as $\mathcal{F}_s$, with prompts $\mathcal{T}_s$, expressed as:
\begin{equation}
    \mathcal{F}_s=\textbf{LLM}(\mathcal{F},\mathcal{Q},\mathcal{C}|\mathcal{T}_s).  
\end{equation}


In addition, EHRs often contain numerical report results \cite{lovon-melgarejo-etal-2024-revisiting} that require conceptual knowledge to interpret their significance. Furthermore, medical QA involves multi-hop questions \cite{pal2022medmcqa}, where retrieved conceptual knowledge can generate explainable new factual knowledge conducive to reasoning. Drawing from LLM zero-shot summarization prompting strategies \cite{wu2025towards}, we analyze and summarize the filtered EHR $\mathcal{F}_s$ with prompts $\mathcal{T}_e$, yielding an enriched representation $\mathcal{F}_e$:
\begin{equation}
    \mathcal{F}_e=\textbf{LLM}(\mathcal{F}_s,\mathcal{Q},\mathcal{C}|\mathcal{T}_e).  
\end{equation}



This process, which we refer to as the LLM \textbf{Extractor}, completes the extraction of original EHR information. In practice, RGAR implements these two phases using single-stage prompting to reduce time overhead. 

% This new reliable factual knowledge enables deeper reasoning and analysis in the GAR, generating multi-queries for multi-hop knowledge retrieval.
% The length and complexity of EHR documents often pose significant challenges when it comes to efficiently extracting relevant information \cite{d2004evaluation}. Feeding lengthy, question-irrelevant EHR content directly into dense retrievers can degrade retrieval performance. A straightforward solution is to segment the content into chunks and use $\mathcal{Q}$ to retrieve necessary information from these chunks. However, dense retrievers primarily measure textual similarity, and the query often lacks the direct semantic links needed to connect $\mathcal{Q}$ with the underlying medical concepts in the original EHR content $\mathcal{F}$. This highlights the importance of the retrieved conceptual knowledge $C = \{c_1, c_2, \dots, c_N\}$, which serves as a vital bridge between $\mathcal{Q}$ and $\mathcal{F}$.

% Consider two straightforward query construction strategies:

% 1. Single unified query:  
%    \begin{equation}
%    \label{eq:single-query}
%    q_{\text{all}} = \mathcal{Q} \oplus c_1 \oplus c_2 \oplus \cdots \oplus c_N
%    \end{equation}  
%    In this approach, all conceptual knowledge is concatenated directly to the query. However, since each \( c_i \) corresponds to a distinct medical concept, the resulting embeddings blend multiple types of information, making it difficult for the retriever to focus on the single relevant signal.

% 2. Separate queries for each concept \( c_i \):  
%    \begin{equation}
%    \label{eq:separate-query}
%    q_{c_i} = \mathcal{Q} \oplus c_i
%    \end{equation}  
%    Here, each \( c_i \) is used to create a separate query. Retrieval is performed for each query independently, and the final result is obtained by averaging the normalized similarity scores of all \( q_{c_i} \). While this approach more precisely captures the relationship between each concept and the corresponding EHR segments, it introduces substantial computational overhead, requiring \( N \) independent retrieval operations. As such, it is impractical for real-world deployment scenarios.

% Inspired by the ability of large language models (LLMs) to locate answers within lengthy text passages \cite{cheng2024adapting} and their role in relevance assessment in RAG systems \cite{es-etal-2024-ragas}, we propose leveraging LLMs’ conditional generation capabilities to \textit{approximate} the retrieval task. Specifically, we design a set of retrieval prompts $\mathcal{T}^r$ that guide the LLM to produce an output distribution approximating the results of traditional dense retrievers:  
% \begin{equation}
% \label{eq:conceptual-retrieval}
% p_\theta(\mathcal{F}_r \mid \mathcal{F}, \mathcal{Q}, \mathcal{T}^r) \approx p_r(\mathcal{F}_r \mid \mathcal{Q}, \mathcal{F}),
% \end{equation}
% where $\mathcal{F}_r \subseteq \mathcal{F}$ represents EHR fragments relevant to $\mathcal{Q}$, and $\mathcal{T}^r$ represents prompts specifically designed for the retrieval task.

% Since large language models possess strong contextual retrieval and understanding capabilities, they can mitigate the information loss that may occur when embedding all $C$ into a single query. Thus, we approximate:  
% \begin{equation}
% \begin{split}
% p_\theta(\mathcal{F}_r \mid \mathcal{F}, Q, C, \mathcal{T}^r) &\approx \frac{1}{N}\sum_{i=1}^{N} \\
% &p_\theta\Big(\mathcal{F}_r \,\big|\, \mathcal{F}, Q \oplus c_i, \mathcal{T}^r\Big).
% \end{split}
% \end{equation}

% In other words, the performance of a single concatenated query approximately matches the normalized average results of individual queries $q_{c_i}=Q\oplus c_i$. The accuracy of this approximation depends on $p_\theta$.

% EHR documents often contain numerous numerical test results, which can be difficult for retrievers to match conceptually. For example, “Platelet count 14,200/mm³” might correspond to “low platelet count” in medical literature. To address this, we use the concept knowledge $C$ as a supplementary condition and employ specialized rewriting prompts $\mathcal{T}^{\text{rew}}$ to guide the LLM in rewriting retrieved EHR fragments. The process is formalized as:
% \begin{equation}
% p_\theta(\mathcal{F}_{re} \mid \mathcal{F}_r, \mathcal{Q}, \mathcal{T}^{\text{rew}})
% \end{equation}
% The model is expected to achieve the following objectives under the guidance of the prompt template and the provided concept knowledge $C$:  

% \textbf{First}, normalization of numerical information: Transform numerical expressions into standardized text descriptions. For instance, rewriting “Platelet count 14,200/mm³” into “low platelet count” facilitates retrieving truly relevant articles.  
% \textbf{Second}, information fusion: When certain indicators are scattered across multiple document fragments, the model can integrate them into a more comprehensive interpretation. For example, if one fragment mentions “elevated white blood cell count” and another mentions “decreased platelet count,” the model might generate “the patient shows signs of an inflammatory response accompanied by thrombocytopenia.” This provides a more complete context for generating question-relevant contexts or answers in the GAR process.


\subsection{The Recurrence Pipeline and Response}
\label{sec:pipeline}
% The extracted EHR information serves as question-relevant Factual knowledge $\mathcal{F}_e$, updating the Basic query $q_b$ through $\mathcal{Q} \oplus \mathcal{F}_e$.
% Building on the $\mathcal{F}_r$ obtained in the previous stage, we update the basic query $q_b = \mathcal{Q} \oplus \mathcal{F}_r$. \textit{Training-free Generation-augmented Retrieval} and \textit{Conditional Generating Retrieved and Rewritten EHR} stages are then iteratively performed until a predetermined number of iterations is reached. Ultimately, this iterative optimization yields the final retrieved conceptual knowledge $C^*$. 

Building on the \(\mathcal{F}_e\), we \textbf{update} the basic query for Conceptual Knowledge Retrieval as \(q_b = \mathcal{Q} \oplus \mathcal{F}_e\). This establishes a \textbf{recurrence interaction} between factual and conceptual knowledge, guiding next retrieval toward more relevant content. Iterative execution enhances the stability of both retrieval and extraction. The entire pipeline recurs for a predefined number of iterations, ultimately yielding the final retrieved conceptual knowledge $\mathcal{C}^*$.

% 这里改了一下
% During the response phase, we adhere to the approach outlined in Equation \ref{eq:retrieval-augmented} to produce answers. In particular, the $\mathcal{F}_e$ are confined to the retrieval phase and are not utilized in the response phase. The only difference lies in the retrieved chunks, which allows us to clearly demonstrate the impact of retrieval quality on the response phase.
During the response phase, we follow the approach in Equation \ref{eq:retrieval-augmented} to generate answers. Notably, the $\mathcal{F}_e$ are restricted to the retrieval phase and are not used in the response phase. The sole difference lies in the retrieved chunks, highlighting the impact of retrieval quality on the responses.

\section{Experiments}
\subsection{Experimental Setup}
\subsubsection{Benchmark Datasets}

We evaluated RGAR on three \textit{factual-aware} medical QA benchmarks featuring multiple-choice questions that require human-level reading comprehension and expert reasoning to analyze patients' clinical conditions.

% We evaluated RGAR on three \textit{factual-aware} medical QA benchmarks featuring multiple-choice questions that require multi-hop reasoning and human-level reading comprehension.  

% We evaluated RGAR on three \textit{factual-aware} medical QA benchmarks. Table \ref{tab:qa_benchmarks} illustrates the statistical differences between these factual-aware questions and traditional question types. All three datasets are multiple-choice OpenQA benchmarks that require multi-hop reasoning and human-level reading comprehension capabilities.


\textbf{MedQA-USMLE} \cite{jin2021disease} and \textbf{MedMCQA} \cite{pal2022medmcqa} consist of questions derived from professional medical exams, evaluating specialized expertise such as disease symptom diagnosis and medication dosage requirements. The problems frequently involve patient histories, vital signs (e.g., blood pressure, temperature), and final diagnostic evaluations (e.g., CT scans), making it necessary to retrieve relevant medical knowledge tailored to the patient’s specific circumstances. However, due to their exam-oriented format, the provided information has already been filtered, reducing the difficulty of extracting factual knowledge from EHR.

\textbf{EHRNoteQA} \cite{kweon2024ehrnoteqa} is a recently introduced benchmark that provides authentic, complex EHR data derived from MIMIC-IV \cite{johnson2023mimic}. This dataset encompasses a wide range of topics and demands that models emulate genuine clinical consultations, ultimately generating accurate discharge recommendations. Consequently, EHRNoteQA challenges models to identify which \textit{factual details} within the EHR are relevant to the questions at hand and apply domain-specific knowledge to address them.

\begin{table}[htbp]
  \centering
  \caption{Medical QA Benchmark Statistics.}
  \resizebox{\linewidth}{!}{ % 让表格适应页面宽度
    \begin{tabular}{lccc}
      \toprule
      Benchmarks & Max. Len & Avg. Len & Min. Len \\
            \midrule
      \rowcolor{gray!20} \multicolumn{4}{c}{Non-EHR QA Benchmarks} \\
      \midrule
        BioASQ-Y/N & 52 & 17 & 9  \\
      PubMedQA & 57 & 23 & 10  \\
      \midrule
      \rowcolor{gray!20} \multicolumn{4}{c}{EHR QA Benchmarks} \\
      \midrule
      MedMCQA & 207 & 41 & 11  \\
      MedQA-USMLE & 872 & 197 & 50  \\
      EHRNoteQA & 5782 & 3061 & 667  \\

      \bottomrule
    \end{tabular}
  }
  \label{tab:qa_benchmarks}
\end{table}

Table \ref{tab:qa_benchmarks} highlights that the chosen datasets, which include EHR information, tend to have significantly \textbf{longer} content compared to datasets without EHRs. Notably, the EHRNoteQA dataset has a maximum length exceeding 4,000 tokens. This raises concerns about the reasonableness of directly employing these EHRs for retrieval.

\begin{table*}[htbp]
  \centering
  \caption{Comparison of RGAR with Other Methods on Three Factual-Aware Datasets. $\Delta$ Indicates Improvement Over Custom, \textbf{Bold} Represents the Best, and \underline{Underline} Indicates the Second-Best.}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{llcccccc|cc}
      \toprule
      \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{MedQA-USMLE (\# 1273)} & \multicolumn{2}{c}{MedMCQA(\# 4183)} & \multicolumn{2}{c|}{EHRNoteQA(\# 962)} & \multicolumn{2}{c}{Average(↓)} \\
      \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
      \multicolumn{2}{c}{} & Acc. & $\Delta$ & Acc. & $\Delta$ & Acc. & $\Delta$ & Acc. & $\Delta$ \\
      \midrule
      \multirow{2}{*}{w/o Retrieval} & Custom  & 50.20 & 0.00  & 50.01 & 0.00  & 47.19 & 0.00  & 49.13 & 0.00  \\
                               & CoT     & 51.45 & 1.25  & 44.53 & -5.48 & 62.89 & 15.70 & 52.96 & 3.82  \\
      \midrule
      \multirow{5}{*}{w/ Retrieval}  & RAG     & 53.50 & 3.30  & \underline{50.54} & \underline{0.53}  & 61.12 & 13.93 & 55.05 & 5.92  \\
                               & MedRAG  & 50.27 & 0.07  & 47.53 & -2.48 & 70.58 & 23.39 & 56.13 & 6.99  \\
                               & GAR     & \underline{57.97} & \underline{7.77}  & 50.42 & 0.41  & 65.48 & 18.29 & 57.96 & 8.82  \\
                               & $i$-MedRAG & 56.24 & 6.04  & 44.94 & -5.07 & \textbf{74.22} & \textbf{27.03} & \underline{58.47} & \underline{9.33}  \\
                               & RGAR    & \textbf{58.83} & \textbf{8.63}  & \textbf{51.02} & \textbf{1.01}  & \underline{73.28} & \underline{26.09} & \textbf{61.04} & \textbf{11.91} \\
      \bottomrule
    \end{tabular}%
  }
  \label{tab:mian_results}
\end{table*}


% In line with MIRAGE \cite{xiong-etal-2024-benchmarking}, we implement the following evaluation framework:
% \begin{itemize}
%     \item \textbf{Option-Free Retrieval:} As mentioned in § \ref{sec:Train-free}, to replicate real-world medical QA conditions, no answer options are provided as input during retrieval.
%     \item \textbf{Zero-Shot Learning:} Given that real-world medical questions often lack similar exemplars, our benchmark evaluates RAG systems in a zero-shot setting, without in-context few-shot learning.
%     \item \textbf{Metrics:} We use Accuracy—the proportion of questions correctly answered—as the main evaluation metric across all benchmarks. We extract model outputs through regular expression matching applied to complete generated answers \cite{wang-etal-2024-answer-c}.
% \end{itemize}

\subsubsection{Retriever and Corpus}
To ensure a fair comparison, we adopt the same retriever, corpus, and parameter settings as previous work \cite{xiong-etal-2024-benchmarking}. We use MedCPT \cite{jin2023medcpt}, a dense retriever specialized for the biomedical domain, configured to retrieve 32 chunks by default. For the corpus, we employ the Textbooks dataset \cite{jin-etal-2019-pubmedqa}, a lightweight collection of 125.8k chunks derived from medical textbooks, with an average length of 182 tokens.
% To ensure a fair comparison, we follow MIRAGE \cite{xiong-etal-2024-benchmarking} in terms of the retrievers, corpus, and parameter settings. We employ MedCPT \cite{jin2023medcpt}, a dense retriever tailored to the biomedical domain, configured to retrieve 32 chunks by default. For the corpus, we use the Textbooks dataset \cite{jin-etal-2019-pubmedqa}, a lightweight collection derived from medical textbooks, consisting of 125.8k chunks with an average length of 182 tokens.




\subsubsection{LLMs and Baselines}
We focus on the effect of RGAR on general-purpose LLMs without domain-specific knowledge. Therefore, we exclude LLMs fine-tuned on the medical domain, such as PMC-Llama \cite{wu2024pmc}. %We focus on models with fewer than 8 billion parameters. 
Our primary experiments utilize Llama-3.2-3B-Instruct, while ablation studies include a range of models from the Llama-3.1/3.2 \cite{dubey2024llama} and Qwen-2.5 \cite{yang2024qwen2} families, ranging from 1.5B to 8B parameters. All selected models feature a context length of approximately 128K tokens.
Temperatures are set to zero to ensure reproducibility through greedy decoding. % To mitigate repetitive generation in smaller models, we use a repetition penalty of 1.2 and limit the maximum generation length to 8K tokens.

For \textit{non-retrieval methods}, we consider a zero-shot approach Custom \cite{kojima2022large} as a baseline and evaluate improvements relative to it. To fully exploit the reasoning capabilities of the LLMs, we incorporate chain-of-thought (CoT) reasoning \cite{wei2022chain}.
For \textit{retrieval-based methods}, we evaluate the classic RAG model \cite{lewis2020retrieval}, the domain-adapted MedRAG \cite{xiong-etal-2024-benchmarking}, and $i$-MedRAG \cite{xiong2024improving}, a medical-domain RAG system designed to decompose questions and iteratively provide answers.

We adopt GAR \cite{mao-etal-2021-generation} as a representative \textit{query-optimized RAG method}, implemented train-free in accordance with § \ref{sec:Train-free}. RGAR defaults to \textbf{2} rounds of recurrence.


\subsubsection{Evaluation Settings}
Following MIRAGE \cite{xiong-etal-2024-benchmarking}, we adopt the following evaluation framework. In \textbf{Option-Free Retrieval}, no answer options are provided for retrieval (§\ref{sec:Train-free}), ensuring a more realistic medical QA scenario. In \textbf{Zero-Shot Learning}, RAG systems are evaluated without in-context few-shot learning, reflecting the lack of similar exemplars in real-world medical questions. For \textbf{Metrics}, we employ Accuracy, defined as the proportion of correctly answered questions, and we extract model outputs by applying regular expression matching to the entire generated responses \cite{wang-etal-2024-answer-c}.


% We adopt GAR \cite{mao-etal-2021-generation} as a representative \textit{query-optimized RAG approach}. Rather than the original strategy of training a BERT model to generate queries, we instead leverage prompt-based query generation with LLMs \cite{yu2023generate}. Our RGAR approach maintains this same strategy and defaults to two retrieval–generation cycles.
% 我文章读错了，确实是要微调BERT，但是它也是答案是生成的而非原始的Notably, the original GAR retrieves ground-truth answer options; in our comparison, these options are also generated via prompts.















\subsection{Main Results}
\subsubsection{Cross-Dataset Performance Improvement}
\label{cross-dataset}
\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{line_1.pdf}
        \caption{Effect of Using Original Options.}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{line_2.pdf}
        \caption{Effect of RGAR's Two Components.}
        \label{fig:sub2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{line_3.pdf}
        \caption{Effect of Rounds in RGAR.}
        \label{fig:sub3}
    \end{subfigure}
    \caption{Accuracy with Different Numbers of Retrieved Chunks on EHRNoteQA Dataset.}
    \label{fig:three_sub_figures}
\end{figure*}

We evaluate RGAR with the Llama-3.2-3B-Instruct across three factual-aware medical datasets, comparing it with several competitive baselines. Table~\ref{tab:mian_results} presents the results of all methods, along with their relative improvements over the Custom baseline. RGAR achieves the highest average performance across the three datasets, surpassing the second-best method, $i$-MedRAG, by 2\%. The retrieval-based methods, even the lowest-performing RAG, consistently outperform the non-retrieval methods Custom and CoT. This highlights the importance of retrieving specialized medical knowledge when using general-purpose LLMs to answer professional medical queries. Comparing different retrieval methods, GAR outperforms vanilla RAG by approximately 3\% on average, with a maximum improvement of 4.37\% across datasets. This indicates that generating multiple queries for retrieval provides consistent benefits. However, while performing well on EHRNoteQA, MedRAG demonstrates a negative effect on the other two datasets compared to vanilla RAG.

Notably, the improvements achieved by our RGAR over GAR exhibit a positive correlation with the average length of the dataset’s context. On EHRNoteQA, which has an average context length exceeding 3000 tokens, our approach achieved a 7.8\% improvement. This validates the advantage of our \textit{Factual knowledge Extraction} in enhancing retrieval effectiveness. Consequently, our method is particularly well-suited to real-world scenarios where complete electronic health records must be analyzed to provide medical advice. This indicates that our approach is promising for real-life applications in assisting physicians with clinical recommendations.

When analyzing performance across different datasets, we find that retrieval-based methods perform significantly better on MedQA-USMLE and EHRNoteQA, while MedMCQA showa a negative effect—consistent with results reported by MedRAG \cite{xiong-etal-2024-benchmarking}. A closer analysis reveals that MedMCQA incorporates arithmetic reasoning questions (roughly 7\% of the total), and the addition of extensive retrieved contexts diminishes the model’s numerical reasoning capabilities, which could potentially be fixed with larger base LLMs \cite{mirzadeh2025gsmsymbolic}. Nonetheless, among retrieval-based methods, our RGAR stands out as the only approach that outperforms vanilla RAG on this dataset, delivering an improvement of more than 1\% over Custom.
On EHRNoteQA, while RGAR’s performance is slightly below that of $i$-MedRAG, \textbf{the latter’s inference time is approximately 4 times longer, establishing RGAR as a more efficient and cost-effective alternative}.


\subsubsection{Base LLMs with Different Sizes and Model Families}
\begin{table}[htbp]
  \centering
  \caption{Comparison of LLMs on MedQA-USMLE.}
  \resizebox{\linewidth}{!}{ % 调整表格宽度适应页面
    \begin{tabular}{lcccc}
      \toprule
      Model & \multicolumn{1}{c}{Custom} & \multicolumn{1}{c}{RAG} & \multicolumn{1}{c}{GAR} & \multicolumn{1}{c}{RGAR} \\
      \midrule

      Llama-3.2-1B-Instruct & 38.96 & 29.30 & 30.79 & 29.85 \\
      Llama-3.2-3B-Instruct & 50.20 & 53.50 & 57.97 & 58.83 \\
      Llama-3.1-8B-Instruct & 60.80 & 62.14 & 67.39 & 69.52 \\
        \midrule
      Qwen2.5-1.5B-Instruct & 43.99 & 41.48 & 43.42 & 42.58 \\
      Qwen2.5-3B-Instruct & 48.23 & 49.96 & 53.50 & 54.28 \\
      Qwen2.5-7B-Instruct & 59.46 & 58.83 & 63.39 & 63.86 \\
      \midrule
      Average   & 50.27 & 49.20 & 52.74 & 53.15 \\
      \bottomrule
    \end{tabular}
  }
  \label{tab:performance}
\end{table}
To further assess the versatility of RGAR, we conduct evaluations on MedQA-USMLE, a widely used medical dataset, by utilizing base LLMs of various sizes and model families, specifically from Llama and Qwen. The results in Table \ref{tab:performance} show that RGAR consistently achieves the best average performance.

When considering model size, we find that retrieval-based approaches fall short of the non-retrieval Custom baseline for smaller models, such as Llama-3.2-1B-Instruct and Qwen2.5-1.5B-Instruct. These smaller models, constrained by their weaker performance, are not well-suited to leverage retrieval-enhanced information. As the model size increases, however, all retrieval-enhanced approaches exhibit notable performance gains, with RGAR yielding the most significant improvements. This trend becomes particularly pronounced for larger models. For example, RGAR achieves a 7.38\% improvement over RAG on Llama-8B, 5.33\% on Llama-3B, 5.03\% on Qwen-8B, and 4.32\% on Qwen-3B.

%While we did not test commercial closed-source models like GPT due to their high API costs
Moreover, we find that under the same experimental conditions, \textbf{Llama-3.1-8B-Instruct achieves a performance of 69.52\% with RGAR, surpassing the 66.22\% reported by MedRAG for GPT-3.5-16k-0613} \cite{achiam2023gpt}. This significant improvement underscores the practicality of using well-optimized retrieval methods with smaller models, enabling performance rivals those of proprietary large-scale foundational models in real-world medical recommendation tasks.

\subsection{Ablation Study}
% Due to the absence of ground-truth retrieval chunks across all three datasets, it is not feasible to evaluate retrieval performance using metrics such as nDCG@10 or Recall@100 like the BEIR benchmark \cite{thakur2021beir}. Instead, we assess retrieval effectiveness through QA performance, varying the number of retrieved items \(N\) from 4 to 32. A lower retrieval count more rigorously tests retrieval quality. We investigate three primary factors: the effect of options generated by GAR versus those originally provided by the dataset, the contributions of GAR and enhanced EHR components, and the impact of RGAR’s iterative rounds.

Due to the absence of ground-truth retrieval chunks, we evaluate retrieval effectiveness through QA performance, systematically varying the number of retrieved chunks \(N\) from 4 to 32. A reduced retrieval number serves as a more stringent assessment of retrieval quality. We investigate three primary factors in Figure \ref{fig:three_sub_figures}: the effect of options generated by GAR versus those originally provided by the dataset, the contributions of CKR and FKE components, and the impact of RGAR’s recurrence rounds.

We first compare the retrieval performance between LLM-generated options and original dataset options. Figure \ref{fig:sub1} shows how RGAR and GAR perform across different values of \(N\). Both approaches maintain stable performance across different \(N\), indicating reliable retrieval quality. While using original options shows slightly higher average Accuracy, the difference is minimal. This suggests that even when GAR generates options that differ from the originals, it achieves similar retrieval results as long as the core topics align. 

We then examine the impact of RGAR's two main components—CKR and FKE—as shown in Figure \ref{fig:sub2}. When we remove the conceptual knowledge interaction from the FKE phase, the system shows only moderate improvements when extracting factual knowledge from EHR without conceptual knowledge, demonstrating the importance of integrating both types of knowledge. %When we remove the multi-query generation step from CKR, performance decreases as \(N\) increases, indicating unstable retrieval quality. This highlights the necessity of generating multiple queries during the CKR phase to maintain stable retrieval.
% 这里也改了一下
Removing the multi-query generation step from CKR causes performance to degrade as \(N\) increases, indicating that multiple queries are necessary to maintain stable retrieval.

Finally, we analyze the effect of rounds in RGAR (Round 0 means GAR), as illustrated in Figure \ref{fig:sub3}. Our results show that even a single iteration significantly improves performance by enabling interaction between factual and conceptual knowledge. Multiple rounds work similarly to a reranking mechanism \cite{mao-etal-2021-reader}, improving the ranking of important chunks and showing substantial gains even with relatively small \(N\). With \(N = 8\) , the default two-round setup achieves a performance of 75.78\%, almost 1\% better than using a single round. However, adding more rounds shows no clear benefits, as they tend to generate multi-hop factual knowledge during the FKE phase, leading CKR to retrieve multi-hop conceptual knowledge, which may cause LLMs to over-infer ~\cite{yang-etal-2024-large-language-models}. Given that each round involves one reasoning step from both the LLM extractor and LLM query generator, two rounds sufficiently support multi-hop reasoning needs \cite{lv-etal-2021-multi}.

% We begin by comparing the retrieval performance of using LLM-generated options to that of directly using the original dataset-provided options. Figure \ref{fig:sub1} illustrates how the performance of RGAR and GAR changes with different values of \(N\). Both configurations exhibit relatively stable performance across the range of \(N\), indicating consistent retrieval quality. While the approach relying on original options shows slightly higher average accuracy (Acc) at various \(N\), the difference is negligible. Even when GAR generates options that differ in content from the originals, it achieves similar retrieval outcomes as long as the underlying topics are aligned. This suggests that GAR-generated options, despite their differences, remain conducive to effective retrieval.

% Next, we analyze the role of RGAR’s two main components in Figure \ref{fig:sub2}: GAR itself and the enhanced EHR retrieval process. We examine the impact of using original EHR data directly for retrieval instead of leveraging our LLM-generated approach. When relying solely on original EHR data, even with multiple iterations, the performance shows only modest improvements and remains capped. This is because such approaches can only enhance the relevance of concept-level information and key content, without encouraging the synthesis of new information or exploration beyond the original data. This limitation is especially pronounced for medical queries requiring multi-hop reasoning. When GAR is removed entirely, we observe a performance decline as \(N\) increases, highlighting the instability of retrieval quality. While the most relevant information may still be retrieved, the absence of auxiliary context hampers reasoning, and the introduction of irrelevant information as \(N\) grows leads to further performance degradation.

% Finally, we assess the effect of iterative rounds in RGAR in Figure \ref{fig:sub3}.The experimental results demonstrate that implementing even a single recurrence iteration yields significant performance improvements, as it facilitates the interaction between factual and conceptual knowledge domains. Multiple iterations function analogously to a reranking mechanism, elevating the relevance of truly pertinent chunks and achieving substantial enhancement when operating with relatively modest N. Notably, with N=8, the default two-iteration configuration achieved a performance of 75.78\%, representing nearly a 1\% improvement over the single-iteration baseline. However, excessive iteration rounds demonstrate no discernible advantages, as they tend to generate multi-hop factual knowledge during the FKE phase, potentially leading to over-inference by LLMs ~\cite{yang-etal-2024-large-language-models}. 

% Finally, we assess the effect of iterative retrieval rounds in RGAR. Setting the iteration count to zero effectively results in the GAR approach. Figure~X depicts how performance evolves as the number of retrieval rounds increases from zero to three. The results show that even a single round yields notable improvements, with subsequent rounds offering diminishing returns. We adopt two retrieval rounds, based on the assumption that most questions can be answered within three reasoning steps~\cite{lv-etal-2021-multi}, and excessive rounds provide no clear advantage~\cite{yang-etal-2024-large-language-models}. RGAR’s approach resembles bidirectional breadth-first search, simultaneously exploring from both the original context and possible answers. With two rounds, RGAR allows for up to four multi-hop steps, which proves to be sufficient.

\subsection{Fine-Grained Performance Analysis}

While the previous sections examined overall dataset performance and established preliminary findings, this section provides a detailed analysis of specific aspects of our results. In § \ref{cross-dataset}, we showed that RGAR performs better on real-world medical recommendation tasks involving comprehensive EHRs. To verify this finding, we conduct a detailed analysis of EHRNoteQA by grouping questions based on context length and dividing them into four bins. Within each bin, we compare the performance of RGAR, GAR, and Custom. As shown in Figure \ref{fig:bar}, Custom shows decreasing accuracy with increasing context length. GAR improves accuracy across all bins, with RGAR achieving further performance gains. Notably, the improvements are more significant in the three bins with longer contexts compared to the first bin. The results show that RGAR maintains consistent average performance across different context length.

% The previous sections focused on overall dataset performance and provided some preliminary conclusions. Here, we delve deeper into specific aspects of those findings. In earlier sections, we concluded that our approach is better suited to real-world medical recommendation tasks involving comprehensive EHRs. To validate this, we further analyze EHRNoteQA by sorting questions based on average context length and dividing them into four bins. Within each bin, we compare the performance of our approach and GAR against the Custom baseline. As shown in Figure \ref{fig:bar}, Acc declines with increasing question length for the Custom baseline. GAR improves Acc across all bins, and RGAR further enhances performance. Notably, the improvement is more pronounced in the three bins with longer contexts compared to the first bin. Overall, the average performance across all bins is similar.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{bar.pdf}
    \caption{Fine-Grained Accuracy of EHRNoteQA After Sorting by Length and Dividing into Four Equal Parts.}
    \label{fig:bar}
\end{figure}

%We also revisit the findings, which suggest that GAR stabilizes retrieval. 
It is also important to note that generating multiple queries from different aspects within RGAR helps stabilize retrieval.
Figure \ref{fig:tsne} presents a t-SNE visualization of different queries and their individually retrieved chunks for a sample question (details provided in Appendix~\ref{case}). The basic query shows limited suitability for retrieval, as its coverage area differs from that of the three queries generated by RGAR. RGAR clearly introduces some variation in retrieval content. Although the regions corresponding to the three generated queries overlap, the specific chunks retrieved do not overlap significantly. This underscores the need to average the retrieval similarities of these three queries to achieve more stable retrieval results.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{t-sne.pdf}
    \caption{t-SNE Visualization of Different Queries and the Retrieved Chunks.}
    \label{fig:tsne}
\end{figure}

% We also revisit the conclusion from our ablation study, which suggested that GAR stabilizes retrieval. Figure \ref{fig:tsne} presents a t-SNE visualization of different queries and their individually retrieved chunks for a sample question (details provided in Appendix~\ref{case}). The basic query shows limited suitability for retrieval, as its coverage area differs from that of the three queries generated by GAR. GAR clearly introduces some variation in retrieval content. Although the regions corresponding to the three GAR-generated queries overlap, the specific chunks retrieved do not overlap significantly. This underscores the need to average the retrieval similarities of these three queries to achieve more stable retrieval results.

\section{Conclusion}
In this work, we propose RGAR, a novel RAG system that distinguishes two types of retrievable knowledge. Through comprehensive evaluation across three factual-aware medical benchmarks, RGAR demonstrates substantial improvements over existing methods, emphasizing the significant impact of in-depth factual knowledge extraction and its interaction with conceptual knowledge on enhancing retrieval performance. %Notably, our system enables 8B parameter models to outperform proprietary large-scale commercial models with retrieval capabilities.
Notably, our RGAR enables the Llama-3.1-8B-Instruct model to outperform the considerably larger, RAG-enhanced proprietary GPT-3.5.
%From a broader perspective, RGAR represents a promising approach for enhancing general LLMs in real-world clinical diagnostic scenarios that demand extensive factual knowledge processing. This framework shows potential for extension to other professional domains where factual awareness is crucial, offering a viable solution for specialized applications requiring precise factual knowledge management.
From a broader perspective, RGAR offers a promising approach for enhancing general-purpose LLMs in clinical diagnostic scenarios where extensive factual knowledge is crucial, with potential for extension to other professional domains demanding precise factual awareness. 
\newpage
\section*{Limitations}
%Despite RGAR achieving superior average performance, several limitations warrant discussion. Our RGAR necessitates corpus retrieval, with time complexity scaling proportionally with corpus size, this is a problem inherent in the RAG paradigm. Approaches that generate reasoning evidence directly through domain-specified LLMs \cite{yu2023generate, frisoni-etal-2024-generate} avoid the inference-time computational issue, however, they are illed in updating LLMs to follow new medical knowledge, which induces frequency updation and training costs. %Additionally, while the multiple LLM generations required by RGAR's retrieval process showed negligible additional time overhead on Llama3.2-3B in our primary experiments, this overhead becomes significant when scaling to larger models.
Despite RGAR achieving superior average performance, several limitations warrant discussion. Our RGAR requires corpus retrieval, and its time complexity scales proportionally with the size of the corpus, which is an inherent issue within the RAG paradigm. Approaches that generate reasoning evidence directly through domain-specific LLMs \cite{yu2023generate, frisoni-etal-2024-generate} avoid the computational challenges at inference time. However, they face difficulties in updating LLMs to incorporate new medical knowledge, which results in frequent updates and training costs.

Comparative approaches such as MedRAG \cite{xiong-etal-2024-benchmarking} and $i$-MedRAG \cite{xiong2024improving} explore integration possibilities with prompting techniques like Chain-of-Thought \cite{wei2022chain} and Self-Consistency \cite{wang2023selfconsistency} to enhance reasoning capabilities. Our investigation focused specifically on validating how additional factual knowledge processing improves retrieval performance, without examining the impact of these prompting strategies. %Furthermore, unlike multi-round methods such as %Adaptive RAG \cite{jeong-etal-2024-adaptive} and 
%$i$-MedRAG \cite{xiong2024improving} that implement LLM-based early stopping to reduce computational costs, our system operates with fixed time complexity.
%However, it is noteworthy that由于i-medrag每轮都要分解若干query检索并回答再汇总，RGAR的实际时间开销远小于i-medrag。
Furthermore, unlike multi-round methods such as $i$-MedRAG \cite{xiong2024improving} that implement LLM-based early stopping to reduce computational costs, our system operates with fixed time complexity. However, it is noteworthy that, because $i$-MedRAG requires multiple rounds of query decomposition, retrieval, and answer aggregation, the actual time overhead of RGAR is significantly smaller than that of $i$-MedRAG.


Our EHR extraction approach assumes LLMs can process complete EHR contextual input, justified by current mainstream LLMs exceeding 128K context windows with anticipated growth. However, in extreme cases where EHR content exceeds LLM context limits, integration with chunk-free approaches may be necessary \cite{luo-etal-2024-landmark, qian-etal-2024-grounding}. Finally, as RGAR operates in a zero-shot setting without instruction fine-tuning, its effectiveness is partially contingent on the model's instruction-following capabilities—which we cannot fully mitigate.

\section*{Ethical Statement}
This research adheres to the ACL Code of Ethics. All medical datasets utilized in this study are either open access or obtained through credentialed access protocols. To ensure patient privacy protection, all datasets have undergone comprehensive anonymization procedures.
While Large Language Models (LLMs) present considerable societal benefits, particularly in healthcare applications, they also introduce potential risks that warrant careful consideration. Although our work advances the relevance of retrieved content for medical queries, we acknowledge that LLM-generated responses based on retrieved information may still be susceptible to errors or perpetuate existing biases.
Given the critical nature of medical information and its potential impact on healthcare decisions, we strongly advocate for a conservative implementation approach. Specifically, we recommend that all system outputs undergo rigorous validation by qualified medical professionals before any practical application. This stringent verification process is essential to maintain the integrity of clinical and scientific discourse and prevent the propagation of inaccurate or potentially harmful information in healthcare settings.
These ethical safeguards reflect our commitment to responsible AI development in the medical domain, where the stakes of misinformation are particularly high and the need for reliability is paramount.

\bibliography{custom}

\appendix

\section{Implementation Details}
\subsection{Hardware Configuration}
All experiments were conducted on an in-house workstation equipped with \textit{dual} NVIDIA GeForce RTX 4090 GPUs, % (24GB VRAM \textit{each}),
128GB RAM, and an Intel® Core i9-13900K CPU.

Time cost across all methods on EHRNoteQA are shown in Table \ref{tab:method_time_comparison}.

\begin{table}[htbp]
  \centering
  \caption{Comparison of different methods in terms of execution time (hours).}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lccccccc}
    \toprule
    Method & Custom & CoT & RAG & MedRAG & GAR & $i$-MedRAG & RGAR \\
    \midrule
    Time (h) & 0.5 & 0.5 & 1 & 1 & 2 & 22 & 6 \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:method_time_comparison}
\end{table}

\subsection{Code and Results}
The core implementation of the RGAR framework and the output json files can be accessed via the \textbf{Anonymous Repository}: \url{https://anonymous.4open.science/r/RGAR-C613}


\section{Prompt Template and Case Study}
\label{case}
For simplicity, we merged EHR and question in the prompt words of the answer and treated them as question in the prompt words.
Table \ref{tab:prompts} shows the prompts template of RGAR and compared work (Using CoT ones). Table \ref{tab:input} shows the input of a sample, Table \ref{tab:output} shows the final output of RGAR.

\begin{table*}[h]
\centering
\begin{tabular}{p{0.3\textwidth}|p{0.7\textwidth}}
\toprule
Type & Prompt Template \\
\midrule
System prompts for Non-CoT & You are a helpful medical expert, and your task is to answer a multi-choice medical question using the relevant documents. Organize your output in a json formatted as Dict \{"answer\_choice": Str\{A/B/C/...\}\}. Your responses will be used for research purposes only, so please have a definite answer. Please just give me the json of the answer. \\
\midrule
System prompts for using CoT  & You are a helpful medical expert, and your task is to answer a multi-choice medical question. Please first think step-by-step and then choose the answer from the provided options. Organize your output in a json formatted as Dict\{"step\_by\_step\_thinking": Str(explanation), "answer\_choice": Str\{A/B/C/...\}\}. Your responses will be used for research purposes only, so please have a definite answer. Please just give me the json of the answer. \\
\midrule
Answer prompts for Non-CoT &Here are the relevant documents:
\{\{context\}\}
\newline
Here is the question:
\{\{question\}\}
\newline
Here are the potential choices:
\{\{options\}\}
\newline
Please just give me the json of the answer. Generate your output in json:\\

\midrule
Answer prompts for Using CoT &Here are the relevant documents:
\{\{context\}\}
\newline
Here is the question:
\{\{question\}\}
\newline
Here are the potential choices:
\{\{options\}\}
\newline
Please think step-by-step and generate your output in one json:\\
\midrule
Extracting EHR prompts & Here are the relevant knowledge sources:
\{\{context\}\}
\newline
Here are the electronic health records:
\{\{ehr\}\}
\newline
Here is the question:
\{\{question\}\}
\newline
Please analyze and extract the key factual information in the electronic health records relevant to solving this question and present it as a Python list. 
Use concise descriptions for each item, formatted as ["key detail 1", ..., "key detail N"]. Please only give me the list. Here is the list: \\
\midrule
Generating Possible Answer prompts & Please give 4 options for the question. Each option should be a concise description of a key detail, formatted as: A. "key detail 1" B. "key detail 2" C. "key detail 3" D. "key detail 4\\
\midrule
Generating Possible Title prompts & Please generate some titles of references that might address the above question. Please give me only the titles, formatted as: ["title 1", "title 2", ..., "title N"]. Please be careful not to give specific content and analysis, just the title.\\
\midrule
Generating Possible Contexts prompts & Please generate some knowledge that might address the above question. please give me only the knowledge. \\
\bottomrule
\end{tabular}
\caption{Prompt templates used in RGAR and Compared Methods.}
\label{tab:prompts}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{0.3\textwidth}|p{0.7\textwidth}}
\toprule
Type & Texts \\
\midrule
EHR & A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1°C (102.3°F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show: Platelet count 14,200/mm3 Fibrinogen 83 mg/mL (N = 200–430 mg/dL) D-dimer 965 ng/mL (N < 500 ng/mL)\\
\midrule
Questions & When phenol is applied to a sample of the patient's blood at 90°C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following? \\
\midrule
Options & A. Coagulase-positive, gram-positive cocci forming mauve-colored colonies on methicillin-containing agar \newline
B. Encapsulated, gram-negative coccobacilli forming grey-colored colonies on charcoal blood agar \newline
C. Spore-forming, gram-positive bacilli forming yellow colonies on casein agar \newline
\textbf{D. Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar} \\
\bottomrule
\end{tabular}
\caption{Case Study of a Sample Input.}
\label{tab:input}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{0.3\textwidth}|p{0.7\textwidth}}
\toprule
Type & Texts \\
\midrule
Generated Title & "Identification of Streptococcus pneumoniae in Clinical Specimens",\newline "Phosphorylcholine-Binding Protein A of Staphylococcus aureus",\newline "D-Dimer Assay for Diagnosing Deep Vein Thrombosis",\newline "The Role of Fibrin Degradation Products in Hemostasis",\newline "Infection Control Measures for Hospital-Acquired Bloodstream Infections" \\
\midrule
Generate Context & The description suggests Neisseria gonorrhoeae infection. Neisseria gonorrhoeae produces lipopolysaccharides in its outer membrane, specifically LPS or endotoxin. When heated, these lipopolysaccharides undergo cross-linking reactions resulting in the formation of phosphorylcholine-attached oligomers such as those described in the question stem. These structures can be detected by ELISA using antibodies against specific epitopes on the surface of N. gonorrhoeae cells. The presence of this structure would suggest an active infection due to N. gonorrhoeae. Blood cultures for N. gonorrhoeae typically require specialized media containing antibiotics like vancomycin, colistin, and nystatin to inhibit competing bacteria \\
\midrule
Generate Answer & Option 1: Streptococcus pneumoniae The presence of a high fever, tachycardia, hypotension, and low platelets suggests sepsis or severe infection. The laboratory findings also indicate disseminated intravascular coagulation (DIC). The identification of a phosphorylated N-acetylglucosane dimer with 6 fatty acid chains attached to a polysaccharide side chain after applying phenol at 90°C is consistent with pneumococcal capsular polysacarcharides.\newline
Option 2: Staphylococcus aureus Staphylococci can cause various infections but do not typically produce this type of capsule structure when exposed to heat.\newline
Option 3: Escherichia coli E. coli does have capsules that may be detected by certain methods such as latex agglutination tests; however, they don't match the description given in the prompt.\newline
Option 4: Neisseria meningitidis Neisseria species like N. meningitidis would more commonly present with symptoms related to central nervous system involvement rather than pelvic signs described in the scenario.\\
\midrule
Extract EHR & ['Patient age: 39 years', 'Symptoms: fever, chills, left lower quadrant pain', 'Vital signs: \textbf{high} temperature (39.1°C), tachycardia (126/min), tachypnea (28/min), hypotension (80/50 mmHg)', 'Physical exam findings: mucopurulent discharge from cervix, left adnexal tenderness', 'Laboratory results: \textbf{low} platelet count (14,200/mm\^{}3), \textbf{elevated} D-dimer (965 ng/mL)', 'Phenol test result: phosphorylated N-acetylglucosame dimmer with 6 fatty acids attached to a polysaccharide side chain'] \\
\midrule
Output Json & \{"answer\_choice": "D"\}\\
\bottomrule
\end{tabular}
\caption{Case Study of output in RGAR.}
\label{tab:output}
\end{table*}

\section{Framework Insight}
\subsection{Another View of the Recurrence Pipeline}
We conceptualize the Recurrence Pipeline as an exploration-exploitation process within the reinforcement learning framework \cite{10.1023/A:1013689704352}. In GAR, even when generated content is only partially accurate (or potentially inaccurate), it remains valuable for retrieval if it correlates with passages containing correct information (e.g., co-occurrence with correct answers), thus representing an exploratory phase. Conversely, EHR extraction serves as an exploitation phase, thoroughly utilizing explored knowledge by selecting relevant components and synthesizing new evidence (factual knowledge). Based on this newly derived evidence, subsequent iterations can initiate fresh exploration-exploitation cycles, creating a continuous knowledge transmission process \cite{10446501}.

In scenarios where additional factual knowledge is not required, the retrieved content tends to remain relatively constant, and utilizing this content under identical prompting conditions would likely yield similar factual knowledge through extraction and summarization. However, when conceptual knowledge is needed to derive new factual knowledge through reasoning from existing factual information, the updated basic query facilitates easier retrieval of conceptual knowledge supporting current reasoned factual knowledge, thereby maintaining the integrity of reasoning chains. Furthermore, leveraging current factual knowledge for retrieval enables the exploration and discovery of novel knowledge domains.

\subsection{Why No Flexible Stopping Criteria}
Similar multiround RAG systems have adopted more flexible stopping criteria. For instance, Adaptive RAG \cite{jeong-etal-2024-adaptive} determines whether to retrieve further % or how many rounds of retrieval are needed
by consulting the model itself. $i$-MedRAG \cite{xiong2024improving}, while setting a maximum number of retrieval iterations, also supports early stopping.

In our RGAR framework, we do not adopt such settings. On the one hand, we focus on evaluating how additional processing of \textit{factual knowledge} enhances retrieval performance, raising awareness of this often-overlooked type of knowledge in previous RAG systems, while flexible stopping criteria mainly showcase procedural knowledge and metacognitive knowledge. On the other hand, the metacognitive capabilities of current LLMs remain under question, as a model’s self-evaluation of the need for additional retrieval information often does not match actual requirements \cite{kumar-etal-2024-confidence}.

\subsection{Future Work}
% % 我们的RGAR framework利用检索到的medical domain专业知识，在medical OpenQA任务重提供了卓越的回答质量。但是，我们担忧这种强大的生成能力一旦被恶意利用，也可能带来安全隐患。比如，当被检索的corpus包含私隐私信息或版权内容时，恶意的提问者可能利用LLM的回答提取并泄露corpus中的敏感信息 \cite{carlini2021extracting}。此外，恶意的提问者可能通过收集大量提问-问答对来尝试replicate我们的base LLM \cite{tramer2016stealing, zhu2024efficient}, 或推断我们检索生成框架的内部信息 \cite{carlinistealing} 作为泄露的商业秘密或未来攻击的基石。我们将在未来尽最大努力阻止这些恶意攻击，比如检查query是否合法 \cite{inan2023llama} 和通过水印标识RGAR所使用的模型 \cite{zhu2024reliable}, 从而保证RAGR被合理、合法地使用。
Our RGAR framework leverages retrieved medical domain knowledge to deliver exceptional answer quality% in medical OpenQA tasks
. However, we are concerned that such powerful generative capabilities, if maliciously exploited, could pose security risks. For instance, when the retrieved corpus contains private or copyrighted information, malicious users could exploit the LLM's responses to extract and disclose sensitive data from the corpus \cite{carlini2021extracting}. 
% Additionally, malicious users might attempt to replicate our base LLM \cite{tramer2016stealing, zhu2024efficient} by collecting large volumes of question-answer pairs or infer internal details of our retrieval-based generation framework \cite{carlinistealing}. 
Additionally, malicious users might attempt to replicate our base LLM \cite{tramer2016stealing, zhu2024efficient} by collecting large volumes of question-answer pairs or infer internal details of our retrieval-based generation framework \cite{carlinistealing}. 
%, potentially exposing proprietary information or providing a foundation for future attacks. 
% %To mitigate these risks, 
We will make every effort to mitigate these risks, such as verifying the legitimacy of queries \cite{inan2023llama} and watermarking the models used in RGAR \cite{zhu2024reliable}.
, ensuring that RGAR is used responsibly and legally.

\section{Comparative Analysis of Dataset Length Distributions}
In this section, we present additional visualizations comparing the two categories of datasets we described, and explain our rationale for excluding the MMLU-med dataset \cite{hendrycks2021measuring}. We plotted smoothed Kernel Density Estimation (KDE) curves for these datasets, as shown in Figure \ref{fig:kde}. Our analysis confirms that datasets containing Electronic Health Records (EHR) consistently demonstrate greater length compared to those without EHR content. However, certain datasets exhibit complex question sources and types. For instance, while the MMLU dataset shows a considerable mean length of 84 tokens and a maximum length of 961 tokens, as illustrated in the figure, the vast majority of its questions lack EHR content and are predominantly shorter in length. This characteristic led to our decision to exclude it from our experimental evaluation.
\ref{fig:kde}
\begin{figure*}[htbp]
        \centering
    \includegraphics[width=1\linewidth]{KDE.pdf}
    \caption{Length Distribution Analysis of Medical QA Datasets with and without EHR.}
    \label{fig:kde}
\end{figure*}
\end{document}
