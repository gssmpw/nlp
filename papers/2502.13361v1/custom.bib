% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@inproceedings{xiong-etal-2024-benchmarking,
    title = "Benchmarking Retrieval-Augmented Generation for Medicine",
    author = "Xiong, Guangzhi  and
      Jin, Qiao  and
      Lu, Zhiyong  and
      Zhang, Aidong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.372/",
    doi = "10.18653/v1/2024.findings-acl.372",
    pages = "6233--6251",
    abstract = "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18{\%} over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the {\textquotedblleft}lost-in-the-middle{\textquotedblright} effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine."
}
@inproceedings{xiong2024improving,
  title={Improving retrieval-augmented generation in medicine with iterative follow-up questions},
  author={Xiong, Guangzhi and Jin, Qiao and Wang, Xiao and Zhang, Minjia and Lu, Zhiyong and Zhang, Aidong},
  booktitle={Biocomputing 2025: Proceedings of the Pacific Symposium},
  pages={199--214},
  year={2024},
  organization={World Scientific}
}

@inproceedings{sun2019pullnet,
  title={PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text},
  author={Sun, Haitian and Bedrax-Weiss, Tania and Cohen, William},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2380--2390},
  year={2019}
}


@inproceedings{
sarthi2024raptor,
title={{RAPTOR}: Recursive Abstractive Processing for Tree-Organized Retrieval},
author={Parth Sarthi and Salman Abdullah and Aditi Tuli and Shubh Khanna and Anna Goldie and Christopher D Manning},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=GN921JHCRw}
}

@inproceedings{jeong-etal-2024-adaptive,
    title = "Adaptive-{RAG}: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity",
    author = "Jeong, Soyeong  and
      Baek, Jinheon  and
      Cho, Sukmin  and
      Hwang, Sung Ju  and
      Park, Jong",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.389/",
    doi = "10.18653/v1/2024.naacl-long.389",
    pages = "7036--7050",
    abstract = "Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG."
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@inproceedings{
yu2023generate,
title={Generate rather than Retrieve: Large Language Models are Strong Context Generators},
author={Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=fB0hRu9GZUS}
}

@inproceedings{frisoni-etal-2024-generate,
    title = "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering",
    author = "Frisoni, Giacomo  and
      Cocchieri, Alessio  and
      Presepi, Alex  and
      Moro, Gianluca  and
      Meng, Zaiqiao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.533/",
    doi = "10.18653/v1/2024.acl-long.533",
    pages = "9878--9919",
    abstract = "Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, {\textquotedblleft}to generate or to retrieve{\textquotedblright} is the modern equivalent of Hamlet`s dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706x fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy."
}

@article{wang2024knowledge,
  title={Knowledge editing for large language models: A survey},
  author={Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
  journal={ACM Computing Surveys},
  volume={57},
  number={3},
  pages={1--37},
  year={2024},
  publisher={ACM New York, NY}
}


@inproceedings{jin-etal-2019-pubmedqa,
    title = "{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering",
    author = "Jin, Qiao  and
      Dhingra, Bhuwan  and
      Liu, Zhengping  and
      Cohen, William  and
      Lu, Xinghua",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1259/",
    doi = "10.18653/v1/D19-1259",
    pages = "2567--2577",
    abstract = "We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1{\%} accuracy, compared to single human performance of 78.0{\%} accuracy and majority-baseline of 55.2{\%} accuracy, leaving much room for improvement. PubMedQA is publicly available at \url{https://pubmedqa.github.io}."
}


@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@inproceedings{pal2022medmcqa,
  title={Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle={Conference on health, inference, and learning},
  pages={248--260},
  year={2022},
  organization={PMLR}
}


@article{tsatsaronis2015overview,
  title={An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition},
  author={Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and others},
  journal={BMC bioinformatics},
  volume={16},
  pages={1--28},
  year={2015},
  publisher={Springer}
}


@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}

@inproceedings{liu2022compositional,
  title={Compositional visual generation with composable diffusion models},
  author={Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B},
  booktitle={European Conference on Computer Vision},
  pages={423--439},
  year={2022},
  organization={Springer}
}

@inproceedings{wu-etal-2024-universal,
    title = "Universal Prompt Optimizer for Safe Text-to-Image Generation",
    author = "Wu, Zongyu  and
      Gao, Hongcheng  and
      Wang, Yueze  and
      Zhang, Xiang  and
      Wang, Suhang",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.351/",
    doi = "10.18653/v1/2024.naacl-long.351",
    pages = "6340--6354",
    abstract = "Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, we propose the first universal **p**rompt **o**ptimizer for **s**afe T2**I** (**POSI**) generation in black-box scenario. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance. Our code is available at [https://github.com/wzongyu/POSI](https://github.com/wzongyu/POSI)."
}

@inproceedings{zhao-etal-2024-optimizing,
    title = "Optimizing {LLM} Based Retrieval Augmented Generation Pipelines in the Financial Domain",
    author = "Zhao, Yiyun  and
      Singh, Prateek  and
      Bhathena, Hanoz  and
      Ramos, Bernardo  and
      Joshi, Aviral  and
      Gadiyaram, Swaroop  and
      Sharma, Saket",
    editor = "Yang, Yi  and
      Davani, Aida  and
      Sil, Avi  and
      Kumar, Anoop",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-industry.23/",
    doi = "10.18653/v1/2024.naacl-industry.23",
    pages = "279--294",
    abstract = "Retrieval Augmented Generation (RAG) is a prominent approach in real-word applications for grounding large language model (LLM) generations in up to date and domain-specific knowledge. However, there is a lack of systematic investigations of the impact of each component (retrieval quality, prompts, generation models) on the generation quality of a RAG pipeline in real world scenarios. In this study, we benchmark 6 LLMs in 15 retrieval scenarios exploring 9 prompts over 2 real world financial domain datasets. We thoroughly discuss the impact of each component in RAG pipeline on answer generation quality and formulate specific recommendations for the design of RAG systems."
}

@inproceedings{nazzal2024promsec,
  title={PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)},
  author={Nazzal, Mahmoud and Khalil, Issa and Khreishah, Abdallah and Phan, NhatHai},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={2266--2280},
  year={2024}
}

@inproceedings{kim-etal-2023-tree,
    title = "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
    author = "Kim, Gangwoo  and
      Kim, Sungdong  and
      Jeon, Byeongguk  and
      Park, Joonsuk  and
      Kang, Jaewoo",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.63/",
    doi = "10.18653/v1/2023.emnlp-main.63",
    pages = "996--1009",
    abstract = "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ{---}via few-shot prompting leveraging external knowledge{---}and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications."
}

@inproceedings{
zhou2023leasttomost,
title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
author={Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=WZH7099tgfM}
}

@inproceedings{wei-etal-2024-mud,
    title = "Through the {MUD}: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements",
    author = "Wei, Xiao  and
      Xu, Qi  and
      Yu, Hang  and
      Liu, Qian  and
      Cambria, Erik",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.158/",
    doi = "10.18653/v1/2024.acl-long.158",
    pages = "2864--2878",
    abstract = "The current charge prediction datasets mostly focus on single-defendant criminal cases.However, real-world criminal cases usually involve multiple defendants whose criminal facts are intertwined. In an early attempt to fill this gap, we introduce a new benchmark that encompasses legal cases involving multiple defendants, where each defendant is labeled with a charge and four types of crime elements, \textit{i.e.,} \textit{Object Element}, \textit{Objective Element}, \textit{Subject Element}, and \textit{Subjective Element}. Based on the dataset, we further develop an interpretable model called EJudge that incorporates crime elements and legal rules to infer charges. We observe that predicting crime charges while providing corresponding rationales benefits the interpretable AI system. Extensive experiments show that EJudge significantly surpasses state-of-the-art methods, which verify the importance of crime elements and legal rules in multi-defendant charge prediction. The source code and dataset are available at \url{https://anonymous.4open.science/r/MCP_1-6010}."
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{johnson2023mimic,
  title={MIMIC-IV, a freely accessible electronic health record dataset},
  author={Johnson, Alistair EW and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J and Hao, Sicheng and Moody, Benjamin and Gow, Brian and others},
  journal={Scientific data},
  volume={10},
  number={1},
  pages={1},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{pang2021cehr,
  title={CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks},
  author={Pang, Chao and Jiang, Xinzhuo and Kalluri, Krishna S and Spotnitz, Matthew and Chen, RuiJun and Perotte, Adler and Natarajan, Karthik},
  booktitle={Machine Learning for Health},
  pages={239--260},
  year={2021},
  organization={PMLR}
}

@inproceedings{lovon-melgarejo-etal-2024-revisiting,
    title = "Revisiting the {MIMIC}-{IV} Benchmark: Experiments Using Language Models for Electronic Health Records",
    author = "Lovon-Melgarejo, Jesus  and
      Ben-Haddi, Thouria  and
      Di Scala, Jules  and
      Moreno, Jose G.  and
      Tamine, Lynda",
    editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Thompson, Paul  and
      Ondov, Brian",
    booktitle = "Proceedings of the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.cl4health-1.23/",
    pages = "189--196",
    abstract = "The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an openly available MIMIC-IV benchmark for electronic health records (EHRs) to address this issue. First, we integrate the MIMIC-IV data within the Hugging Face datasets library to allow an easy share and use of this collection. Second, we investigate the application of templates to convert EHR tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the mortality of patients task show that fine-tuned text-based models are competitive against robust tabular classifiers. In contrast, zero-shot LLMs struggle to leverage EHR representations. This study underlines the potential of text-based approaches in the medical field and highlights areas for further improvement."
}

@inproceedings{chen-etal-2017-reading,
    title = "Reading {W}ikipedia to Answer Open-Domain Questions",
    author = "Chen, Danqi  and
      Fisch, Adam  and
      Weston, Jason  and
      Bordes, Antoine",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1171/",
    doi = "10.18653/v1/P17-1171",
    pages = "1870--1879",
    abstract = "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task."
}

@inproceedings{mao-etal-2021-generation,
    title = "Generation-Augmented Retrieval for Open-Domain Question Answering",
    author = "Mao, Yuning  and
      He, Pengcheng  and
      Liu, Xiaodong  and
      Shen, Yelong  and
      Gao, Jianfeng  and
      Han, Jiawei  and
      Chen, Weizhu",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.316/",
    doi = "10.18653/v1/2021.acl-long.316",
    pages = "4089--4100",
    abstract = "We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used."
}

@inproceedings{kweon2024ehrnoteqa,
  title={EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries},
  author={Kweon, Sunjun and Kim, Jiyoun and Kwak, Heeyoung and Cha, Dongchul and Yoon, Hangyul and Kim, Kwang Hyun and Yang, Jeewon and Won, Seunghyun and Choi, Edward},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@article{forehand2010bloom,
  title={Bloom’s taxonomy},
  author={Forehand, Mary},
  journal={Emerging perspectives on learning, teaching, and technology},
  volume={41},
  number={4},
  pages={47--56},
  year={2010}
}

@inproceedings{wang-etal-2023-self-knowledge,
    title = "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
    author = "Wang, Yile  and
      Li, Peng  and
      Sun, Maosong  and
      Liu, Yang",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.691/",
    doi = "10.18653/v1/2023.findings-emnlp.691",
    pages = "10303--10315",
    abstract = "Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model`s ability to recognize what they know and do not know (which is also called {\textquotedblleft}self-knowledge{\textquotedblright}) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT."
}

@article{dhuliawala2023chain,
  title={Chain-of-verification reduces hallucination in large language models},
  author={Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  journal={arXiv preprint arXiv:2309.11495},
  year={2023}
}

@inproceedings{ma2023query,
  title={Query Rewriting in Retrieval-Augmented Large Language Models},
  author={Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5303--5315},
  year={2023}
}

@article{markus2001toward,
  title={Toward a theory of knowledge reuse: Types of knowledge reuse situations and factors in reuse success},
  author={Markus, Lynne M},
  journal={Journal of management information systems},
  volume={18},
  number={1},
  pages={57--93},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{jin2023medcpt,
  title={MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval},
  author={Jin, Qiao and Kim, Won and Chen, Qingyu and Comeau, Donald C and Yeganova, Lana and Wilbur, W John and Lu, Zhiyong},
  journal={Bioinformatics},
  volume={39},
  number={11},
  pages={btad651},
  year={2023},
  publisher={Oxford University Press}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wu2024pmc,
  title={PMC-LLaMA: toward building open-source language models for medicine},
  author={Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Xie, Weidi and Wang, Yanfeng},
  journal={Journal of the American Medical Informatics Association},
  pages={ocae045},
  year={2024},
  publisher={Oxford University Press}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
}

@article{d2004evaluation,
  title={An evaluation of information-seeking behaviors of general pediatricians},
  author={D’Alessandro, Donna M and Kreiter, Clarence D and Peterson, Michael W},
  journal={Pediatrics},
  volume={113},
  number={1},
  pages={64--69},
  year={2004},
  publisher={American Academy of Pediatrics}
}

@inproceedings{es-etal-2024-ragas,
    title = "{RAGA}s: Automated Evaluation of Retrieval Augmented Generation",
    author = "Es, Shahul  and
      James, Jithin  and
      Espinosa Anke, Luis  and
      Schockaert, Steven",
    editor = "Aletras, Nikolaos  and
      De Clercq, Orphee",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-demo.16/",
    pages = "150--158",
    abstract = "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs."
}

@inproceedings{
cheng2024adapting,
title={Adapting Large Language Models via Reading Comprehension},
author={Daixuan Cheng and Shaohan Huang and Furu Wei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=y886UXPEZ0}
}

@inproceedings{shen-etal-2024-retrieval,
    title = "Retrieval-Augmented Retrieval: Large Language Models are Strong Zero-Shot Retriever",
    author = "Shen, Tao  and
      Long, Guodong  and
      Geng, Xiubo  and
      Tao, Chongyang  and
      Lei, Yibin  and
      Zhou, Tianyi  and
      Blumenstein, Michael  and
      Jiang, Daxin",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.943/",
    doi = "10.18653/v1/2024.findings-acl.943",
    pages = "15933--15946",
    abstract = "We propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Large language model as Retriever (LameR), is built upon no other neural models but an LLM in a retrieval-augmented retrieval fashion, while breaking brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query`s in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. As a part of the prompts, they are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever, the LLM-based query augmentation becomes less effective as the retriever bottlenecks the whole pipeline. Therefore, we propose to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion. As such, LameR makes the retrieval procedure transparent to the LLM, thus circumventing the bottleneck."
}

@inproceedings{kumar-etal-2024-confidence,
    title = "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models",
    author = "Kumar, Abhishek  and
      Morabito, Robert  and
      Umbet, Sanzhar  and
      Kabbara, Jad  and
      Emami, Ali",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.20/",
    doi = "10.18653/v1/2024.acl-long.20",
    pages = "315--334",
    abstract = "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM`s internal confidence, quantified by token probabilities, to the confidence conveyed in the model`s response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model`s confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI`s GPT-4 showed the strongest confidence-probability alignment, with an average Spearman`s $\hat{\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness."
}

@inproceedings{
mirzadeh2025gsmsymbolic,
title={{GSM}-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
author={Seyed Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=AjXkRZIvjB}
}

@inproceedings{zhang-etal-2024-arl2,
    title = "{ARL}2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
    author = "Zhang, LingXi  and
      Yu, Yue  and
      Wang, Kuan  and
      Zhang, Chao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.203/",
    doi = "10.18653/v1/2024.acl-long.203",
    pages = "3708--3719",
    abstract = "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to separate training processes and the inherent black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score adaptive relevance evidence, enabling the retriever to learn from robust LLM supervision. Furthermore, ARL2 incorporates a self-training strategy to minimize the cost of API calls. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4{\%} on NQ and 4.6{\%} on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities."
}

@INPROCEEDINGS{factual_aware,
  author={Lu, Fengyu and Duan, Jiaxin and Liu, Junfei},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Factual Aware Two-Stage Model for Medical Dialogue Summarization}, 
  year={2023},
  volume={},
  number={},
  pages={2859-2866},
  keywords={Adaptation models;Maximum likelihood estimation;Terminology;Semantics;Medical services;Generators;Recording;Medical Text Mining;Medical Dialogue Summarization;Abstractive Summarization},
  doi={10.1109/BIBM58861.2023.10385609}}

@inproceedings{
    thakur2021beir,
    title={{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
    author={Nandan Thakur and Nils Reimers and Andreas R{\"u}ckl{\'e} and Abhishek Srivastava and Iryna Gurevych},
    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
    year={2021},
    url={https://openreview.net/forum?id=wCu6T5xFjeJ}
}

@inproceedings{lv-etal-2021-multi,
    title = "Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability",
    author = "Lv, Xin  and
      Cao, Yixin  and
      Hou, Lei  and
      Li, Juanzi  and
      Liu, Zhiyuan  and
      Zhang, Yichi  and
      Dai, Zelin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.700/",
    doi = "10.18653/v1/2021.emnlp-main.700",
    pages = "8899--8911",
    abstract = "Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics, including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate these metrics using the interpretability scores of rules. We manually annotate all possible rules and establish a benchmark. In experiments, we verify the effectiveness of our benchmark. Besides, we run nine representative baselines on our benchmark, and the experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is 51.7{\%} lower than the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., how to better incorporate rule information into the multi-hop reasoning model. We will publish our codes and datasets upon acceptance."
}

@inproceedings{yang-etal-2024-large-language-models,
    title = "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
    author = "Yang, Sohee  and
      Gribovskaya, Elena  and
      Kassner, Nora  and
      Geva, Mor  and
      Riedel, Sebastian",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.550/",
    doi = "10.18653/v1/2024.acl-long.550",
    pages = "10210--10229",
    abstract = "We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as {\textquotedblleft}The mother of the singer of {\textquoteleft}Superstition' is{\textquotedblright}. We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies {\textquotedblleft}the singer of {\textquoteleft}Superstition'{\textquotedblright} as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder`s mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM`s internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80{\%} of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs."
}

@article{lievin2024can,
  title={Can large language models reason about medical questions?},
  author={Li{\'e}vin, Valentin and Hother, Christoffer Egeberg and Motzfeldt, Andreas Geert and Winther, Ole},
  journal={Patterns},
  volume={5},
  number={3},
  year={2024},
  publisher={Elsevier}
}

@article{nori2023capabilities,
  title={Capabilities of gpt-4 on medical challenge problems},
  author={Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
  journal={arXiv preprint arXiv:2303.13375},
  year={2023}
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264/",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392"
}

@article{liu-etal-2024-lost,
    title = "Lost in the Middle: How Language Models Use Long Contexts",
    author = "Liu, Nelson F.  and
      Lin, Kevin  and
      Hewitt, John  and
      Paranjape, Ashwin  and
      Bevilacqua, Michele  and
      Petroni, Fabio  and
      Liang, Percy",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.9/",
    doi = "10.1162/tacl_a_00638",
    pages = "157--173",
    abstract = "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models."
}

@inproceedings{parashar-etal-2023-prompting,
    title = "Prompting Scientific Names for Zero-Shot Species Recognition",
    author = "Parashar, Shubham  and
      Lin, Zhiqiu  and
      Li, Yanan  and
      Kong, Shu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.610/",
    doi = "10.18653/v1/2023.emnlp-main.610",
    pages = "9856--9861",
    abstract = "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., {\textquotedblleft}a photo of Lepus Timidus{\textquotedblright} (which is a scientific name in Latin). This is because these names are usually not included in CLIP`s training set. To improve performance, we explore using large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. However, this method improves only marginally. Instead, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP`s training set, and prompting them achieves 2{\textasciitilde}5 times higher accuracy on benchmarking datasets of fine-grained species recognition."
}

@article{wu2025towards,
  title={Towards evaluating and building versatile large language models for medicine},
  author={Wu, Chaoyi and Qiu, Pengcheng and Liu, Jinxin and Gu, Hongfei and Li, Na and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={npj Digital Medicine},
  volume={8},
  number={1},
  pages={58},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{ren-etal-2023-thorough,
    title = "A Thorough Examination on Zero-shot Dense Retrieval",
    author = "Ren, Ruiyang  and
      Qu, Yingqi  and
      Liu, Jing  and
      Zhao, Xin  and
      Wu, Qifei  and
      Ding, Yuchen  and
      Wu, Hua  and
      Wang, Haifeng  and
      Wen, Ji-Rong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1057/",
    doi = "10.18653/v1/2023.findings-emnlp.1057",
    pages = "15783--15796",
    abstract = "Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting. However, in the related literature, there still lacks a detailed and comprehensive study on zero-shot retrieval. In this paper, we present the first thorough examination of the zero-shot capability of DR models. We aim to identify the key factors and analyze how they affect zero-shot retrieval performance. In particular, we discuss the effect of several key factors related to source training set, analyze the potential bias from the target dataset, and review and compare existing zero-shot DR models. Our findings provide important evidence to better understand and develop zero-shot DR models."
}

@inproceedings{luo-etal-2024-landmark,
    title = "Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models",
    author = "Luo, Kun  and
      Liu, Zheng  and
      Xiao, Shitao  and
      Zhou, Tong  and
      Chen, Yubo  and
      Zhao, Jun  and
      Liu, Kang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.180/",
    doi = "10.18653/v1/2024.acl-long.180",
    pages = "3268--3281",
    abstract = "Retrieval augmentation is a promising approach to handle long-context language modeling. However, the existing retrieval methods usually work with the chunked context, which is prone to inferior quality of semantic representation and incomplete retrieval of useful information. In this work, we propose a new method for the retrieval augmentation of long-context language modeling, called Landmark Embedding. Our method is characterized by threefold technical contributions. Firstly, we introduce a \textit{chunking-free architecture}, which keeps the long context coherent such that high-quality embeddings can be generated for the fine-grained units within the context. Secondly, we present a position-aware objective function, which prioritizes the ultimate boundary for a consecutive span of information. By learning to discriminate such a special position, the useful information can be comprehensively retrieved for the query. Thirdly, we design a novel multi-stage learning algorithm, which makes the best use of readily available data and synthetic data for cost-effective training of the landmark embedding. In our experimental study, landmark embedding is able to substantially improve the performance for both LLaMA-2 and ChatGPT in a variety of long-context tasks; meanwhile, it also outperforms the existing retrieval methods with a notable advantage. Our model and source code will be made publicly available."
}

@misc{kamradt2024needle,
  author = {Kamradt, Greg},
  title = "LLMTest\_NeedleInAHaystack: Evaluating Long-Context Capabilities of Large Language Models",
  url = "https://github.com/gkamradt/LLMTest_NeedleInAHaystack",
  note = {Accessed: 2025-02-13}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{friedman1977algorithm,
  title={An algorithm for finding best matches in logarithmic expected time},
  author={Friedman, Jerome H and Bentley, Jon Louis and Finkel, Raphael Ari},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={3},
  number={3},
  pages={209--226},
  year={1977},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang-etal-2024-answer-c,
    title = "{\textquotedblleft}My Answer is {C}{\textquotedblright}: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",
    author = {Wang, Xinpeng  and
      Ma, Bolei  and
      Hu, Chengzhi  and
      Weber-Genzel, Leon  and
      R{\"o}ttger, Paul  and
      Kreuter, Frauke  and
      Hovy, Dirk  and
      Plank, Barbara},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.441/",
    doi = "10.18653/v1/2024.findings-acl.441",
    pages = "7407--7416",
    abstract = "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model`s diverse response styles such as starting with {\textquotedblleft}Sure{\textquotedblright} or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned \textit{on all dimensions}, reaching mismatch rates over 60{\%}. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation."
}

@inproceedings{
wang2023selfconsistency,
title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=1PL1NIMMrw}
}

@inproceedings{qian-etal-2024-grounding,
    title = "Grounding Language Model with Chunking-Free In-Context Retrieval",
    author = "Qian, Hongjin  and
      Liu, Zheng  and
      Mao, Kelong  and
      Zhou, Yujia  and
      Dou, Zhicheng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.71/",
    doi = "10.18653/v1/2024.acl-long.71",
    pages = "1298--1311",
    abstract = "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.The CFIC approach addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two innovative decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.Our evaluations of CFIC on a range of open question answering datasets demonstrate its superiority in retrieving relevant and accurate information, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems."
}

@inproceedings{
berglund2024the,
title={The Reversal Curse: {LLM}s trained on {\textquotedblleft}A is B{\textquotedblright} fail to learn {\textquotedblleft}B is A{\textquotedblright}},
author={Lukas Berglund and Meg Tong and Maximilian Kaufmann and Mikita Balesni and Asa Cooper Stickland and Tomasz Korbak and Owain Evans},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=GPKTIktA0k}
}

@article{10.1023/A:1013689704352,
author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul},
title = {Finite-time Analysis of the Multiarmed Bandit Problem},
year = {2002},
issue_date = {May-June 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1013689704352},
doi = {10.1023/A:1013689704352},
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
journal = {Mach. Learn.},
month = may,
pages = {235–256},
numpages = {22},
keywords = {finite horizon regret, bandit problems, adaptive allocation rules}
}

@INPROCEEDINGS{10446501,
  author={Zhu, Hongyu and Liang, Sichu and Hu, Wentao and Li, Fang-Qi and Yuan, Yali and Wang, Shi-Lin and Cheng, Guang},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Improve Deep Forest with Learnable Layerwise Augmentation Policy Schedules}, 
  year={2024},
  volume={},
  number={},
  pages={6660-6664},
  keywords={Deep learning;Schedules;Pattern classification;Signal processing algorithms;Benchmark testing;Data augmentation;Regression analysis;Deep Forest;Data Augmentation;Tabular Signal Classification},
  doi={10.1109/ICASSP48485.2024.10446501}}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction $\{$APIs$\}$},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th USENIX security symposium (USENIX Security 16)},
  pages={601--618},
  year={2016}
}

@article{zhu2024efficient,
  title={Efficient and Effective Model Extraction},
  author={Zhu, Hongyu and Hu, Wentao and Liang, Sichu and Li, Fangqi and Wang, Wenwen and Wang, Shilin},
  journal={arXiv preprint arXiv:2409.14122},
  year={2024}
}

@inproceedings{carlinistealing,
  title={Stealing part of a production language model},
  author={Carlini, Nicholas and Paleka, Daniel and Dvijotham, Krishnamurthy Dj and Steinke, Thomas and Hayase, Jonathan and Cooper, A Feder and Lee, Katherine and Jagielski, Matthew and Nasr, Milad and Conmy, Arthur and others},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@inproceedings{zhu2024reliable,
  title={Reliable Model Watermarking: Defending Against Theft without Compromising on Evasion},
  author={Zhu, Hongyu and Liang, Sichu and Hu, Wentao and Fangqi, Li and Jia, Ju and Wang, Shi-Lin},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={10124--10133},
  year={2024}
}

@inproceedings{mao-etal-2021-reader,
    title = "Reader-Guided Passage Reranking for Open-Domain Question Answering",
    author = "Mao, Yuning  and
      He, Pengcheng  and
      Liu, Xiaodong  and
      Shen, Yelong  and
      Gao, Jianfeng  and
      Han, Jiawei  and
      Chen, Weizhu",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.29/",
    doi = "10.18653/v1/2021.findings-acl.29",
    pages = "344--350"
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{tian2024opportunities,
  title={Opportunities and challenges for ChatGPT and large language models in biomedicine and health},
  author={Tian, Shubo and Jin, Qiao and Yeganova, Lana and Lai, Po-Ting and Zhu, Qingqing and Chen, Xiuying and Yang, Yifan and Chen, Qingyu and Kim, Won and Comeau, Donald C and others},
  journal={Briefings in Bioinformatics},
  volume={25},
  number={1},
  pages={bbad493},
  year={2024},
  publisher={Oxford University Press}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{10.1093/jamia/ocae014,
    author = {Hersh, William},
    title = {Search still matters: information retrieval in the era of generative AI},
    journal = {Journal of the American Medical Informatics Association},
    volume = {31},
    number = {9},
    pages = {2159-2161},
    year = {2024},
    month = {01},
    abstract = {Information retrieval (IR, also known as search) systems are ubiquitous in modern times. How does the emergence of generative artificial intelligence (AI), based on large language models (LLMs), fit into the IR process?This perspective explores the use of generative AI in the context of the motivations, considerations, and outcomes of the IR process with a focus on the academic use of such systems.There are many information needs, from simple to complex, that motivate use of IR. Users of such systems, particularly academics, have concerns for authoritativeness, timeliness, and contextualization of search. While LLMs may provide functionality that aids the IR process, the continued need for search systems, and research into their improvement, remains essential.},
    issn = {1527-974X},
    doi = {10.1093/jamia/ocae014},
    url = {https://doi.org/10.1093/jamia/ocae014},
    eprint = {https://academic.oup.com/jamia/article-pdf/31/9/2159/58868002/ocae014.pdf},
}

@article{li2024acecoder,
  title={Acecoder: An effective prompting technique specialized in code generation},
  author={Li, Jia and Zhao, Yunfei and Li, Yongmin and Li, Ge and Jin, Zhi},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={8},
  pages={1--26},
  year={2024},
  publisher={ACM New York, NY}
}



