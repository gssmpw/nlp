\section{Related Work}
\subsection{Scaling Test-time Compute}
Scaling test-time compute recently garners wide attention in LLM reasoning \cite{snell2024scalingllmtesttimecompute,wu2024empirical,brown2024large}. Existing studies have explored various methods for scaling up test-time compute, including: (1) \textbf{\textit{Aggregation-based methods}} that samples multiple responses for each question and obtains the final answer with self-consistency \cite{selfconsistency} or by selecting best-of-N answer using a verifier or reward model \cite{mathshepherd,zhang2024generative,lightman2023lets,havrilla2024glore}; (2) \textit{\textbf{Search-based methods}} that apply search algorithms such as Monte Carlo Tree Search \cite{alphallm,wang2024qimprovingmultistepreasoning,restmcts,qi2024mutual}, beam search \cite{snell2024scalingllmtesttimecompute}, or other effective algorithms \cite{feng2023alphazero,yao2023tree} to search for correct trajectories; (3) \textit{\textbf{Iterative-refine-based  methods}} that iteratively improve test performance through self-refinement \cite{selfrefine,shinn2024reflexion,chen2024magicore,chen2025sets}. Recently, there has been a growing focus on training LLMs to perform test-time search on their own, typically by conducting longer and deeper thinking \cite{o1,guo2025deepseek}. 
These test-time scaling efforts not only directly benefit LLM reasoning, but can also be integrated back into training time, enabling iterative improvement for LLM reasoning \cite{qin2024o1,feng2023alphazero,snell2024scalingllmtesttimecompute,luong2024reft}. 
In this work, we also present an efficient framework for training LLMs to perform effective test-time scaling through self-verification and self-correction iterations. This approach is achieved without extensive efforts, and the performance of \modelx can also be consistently promoted via iterative training.

\subsection{Self-verification and Self-correction}
Enabling LLMs to perform effective self-verification and self-correction is a promising solution for achieving robust reasoning for LLMs \cite{madaan2024self,shinn2023reflexion,paul2023refiner,lightman2023let}, and these abilities are also critical for performing deep reasoning.
Previous studies have shown that direct prompting of LLMs for self-verification or self-correction is suboptimal in most scenarios \cite{huang2023large,tyen2023llms,ma2024large,zhang2024understanding}. As a result, recent studies have explored various approaches to enhance these capabilities during post-training \cite{saunders2022self,rosset2024direct,kumar2024training}. These methods highlight the potential of using human-annotated or LLM-generated data to equip LLMs with self-verification or self-correction capabilities \cite{zhang2024small,jiang2024towards}, while also indicating that behavior imitation via supervised fine-tuning alone is insufficient for achieving valid self-verification or self-correction  \cite{kumar2024training,qu2025recursive,kamoi2024can}. In this work, we propose effective methods to enhance LLMs' self-verification and self-correction abilities through principled imitation data construction and RL training, and demonstrate the effectiveness of our approach with in-depth analysis.





\subsection{RL for LLM Reasoning}
Reinforcement learning has proven effective in enhancing LLM performance across various tasks \cite{ziegler2019fine,stiennon2020learning,bai2022training,ouyang2022training,setlur2025rl}. In LLM reasoning, previous studies typically employ RL in an actor-critic framework \cite{lightman2024lets,tajwar2024preference,havrilla2024teaching}, and research on developing accurate reward models for RL training has been a long-standing focus, particularly in reward modeling for Process-level RL \cite{lightman2024lets,setlur2024rewarding,setlur2025rl,luo2024improve}. Recently, several studies have demonstrate that simplified reward modeling and advantage estimation \cite{ahmadian2024back,deepseekmath,team2025kimi,guo2025deepseek} in RL training can also effectively enhance LLM reasoning. Recent advances in improving LLMs' deep thinking \cite{guo2025deepseek,team2025kimi} further highlight the effectiveness of utilizing unhackable rewards \cite{gao2023scaling,everitt2021reward} to consistently enhance LLM reasoning.
In this work, we also show that simplified advantage estimation and RL framework enable effective improvements on LLM reasoning. Additionally, we conducted an analysis on process-level RL, outcome-level RL and offline RL, providing insights for future work in RL for LLM reasoning.