\begin{table*}[h]
	\small 
	\centering
	\resizebox{0.97\textwidth}{!}{
		\begin{tabular}%
			{@{\hskip0pt}l@{\hskip4pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip0pt}}
			\toprule[1.5pt]
			&  \multicolumn{7}{c}{\bf Datasets} & \multirow{3}{*}{\makecell{\textbf{\phantom{xx}Average\phantom{xx}}}} \\
			\cmidrule{2-8} 
			\bfseries Model&\makecell{MATH\\500} &\makecell{AIME\\2024} & \makecell{AMC\\2023}& \makecell{College\\Math}& \makecell{Olympiad \\Bench}&GSM8K&\makecell{GaokaoEn\\2023}\\
			\midrule[1pt]
		  \multicolumn{9}{l}{\textit{Frontier LLMs }}\\
      GPT-4o$^{\star}$&76.6&9.3&47.5&48.5&43.3&92.9&67.5&55.1\\
		  Claude3.5-Sonnet$^{\star}$& 78.3& 16.0& -&-&-&96.4&-&- \\
		  GPT-o1-preview$^{\star}$& 85.5  &44.6&90.0&-& -& -&-&- \\
		  GPT-o1-mini$^{\star}$&90.0&56.7&95.0&57.8&65.3 &94.8&78.4&76.9 \\
		  	\midrule[1pt]
	  \multicolumn{9}{l}{\textit{Top-tier Open-source Reasoning LLMs }}\\
		  Mathstral-7B-v0.1$^{\star}$&57.8&0.0&37.5&33.7&21.5&84.9&46.0&40.2\\
		 NuminaMath-72B-CoT$^{\star}$&64.0&3.3&70.0&39.7&32.6&90.8&58.4&51.3\\
		  LLaMA3.1-70B-Instruct$^{\star}$ &65.4&23.3&50.0&42.5&27.7&94.1&54.0&51.0\\
		   Qwen2.5-Math-72B-Instruct$^{\star}$&85.6&30.0&70.0&49.5&49.0&95.9&71.9&64.6\\
		  	\midrule[1pt]
			  \multicolumn{9}{l}{\textit{General Model: Llama-3.1-8B-Instruct}}\\
		  Llama-3.1-8B-Instruct &48.0 &\underline{6.7}&\underline{30.0}&30.8 &15.6&84.4&41.0&36.6 \\
          Llama-3.1-8B-Instruct +  Original Solution SFT &31.0	&3.3	&7.5	&22.0	&8.0	&58.7&28.3&22.7\\
		  Llama-3.1-8B-Instruct + Long CoT SFT&51.4 &\underline{6.7} &27.5 &36.3  &\underline{19.0} &\underline{87.0}&\bfseries48.3&\underline{39.5} \\
         \rowcolor[rgb]{0.9, 1.0, 1.0}  \textbf{Llama-3.1-8B-\model-BI (\textit{ours})}&49.6&\bfseries10.0&20.0& 33.3&17.6&85.3&41.0&36.7 \\
              \rowcolor[rgb]{0.9, 1.0, 1.0}   \textbf{Llama-3.1-8B-\model-PRL (\textit{ours})}&\underline{53.6}&\underline{6.7}&25.0&\underline{33.7}&18.5&{86.7}&43.1& 38.2\\
	 \rowcolor[rgb]{0.9, 1.0, 1.0}   \textbf{Llama-3.1-8B-\model-ORL (\textit{ours})}&\bfseries 55.0&\underline{6.7}&\bfseries 32.5&\bfseries  34.7&\bfseries 20.7&\bfseries 87.3&\underline{45.2}&\bfseries40.3\\
		\midrule[1pt]
		\multicolumn{9}{l}{\textit{General Model: Qwen2-7B-Instruct}}\\
		Qwen2-7B-Instruct &51.2 &3.3&30.0 &18.2 &19.1&86.4&39.0&35.3 \\
        Qwen2-7B-Instruct + Original Solution SFT&  41.2	&0.0	&25.0	&30.1	&10.2	&74.5&34.8&30.8\\
        Qwen2-7B-Instruct + Long CoT SFT&60.4 &\underline{6.7} &32.5 & 36.3 &23.4 &81.2&53.5&42.0 \\
	\rowcolor[rgb]{0.9, 1.0, 1.0} 	\textbf{Qwen2-7B-\model-BI (\textit{ours})}&61.2 &3.3&27.5&\bfseries 41.1&\bfseries 27.1&\underline{ 87.4}&49.1&42.4 \\
            \rowcolor[rgb]{0.9, 1.0, 1.0}  \textbf{Qwen2-7B-\model-PRL (\textit{ours})}&\bfseries 65.4&\underline{6.7}&\underline{35.0}& \underline{36.7}&\underline{27.0}&\textbf{89.0}&\underline{49.9}&\textbf{44.2}\\
		\rowcolor[rgb]{0.9, 1.0, 1.0}  \textbf{Qwen2-7B-\model-ORL (\textit{ours})}&\underline{64.8}&3.3&\bfseries 42.5&34.7 &26.2&86.4&\bfseries 50.9&\underline{44.1}\\
		\midrule[1pt]
		\multicolumn{9}{l}{\textit{Math-Specialized Model: Qwen2.5-Math-7B}}\\
		Qwen2.5-Math-7B & 51.0 &16.7 &45.0 &21.5  &16.7&58.3&39.7&35.6 \\
		Qwen2.5-Math-7B-Instruct&83.2&13.3&72.5&47.0&40.4&\bfseries95.6&67.5&59.9 \\
        Eurus-2-7B-PRIME$^{\star}$\cite{cui2025process}&79.2&\underline{26.7}&57.8&45.0&42.1&88.0&57.1&56.6\\
        rStar-Math-7B$^{\star}$\footnotemark[2]\cite{guan2025rstar} &78.4&\underline{26.7}&47.5&\bfseries 52.5&\bfseries47.1&89.7&65.7&58.2\\
        Qwen2.5-7B-SimpleRL$^{\star}$\cite{zeng2025simplerl} &82.4&\underline{26.7}&62.5&-&43.3&-&-&-\\
        Qwen2.5-Math-7B + Original Solution SFT&58.0 &6.7 &42.5 & 35.8 &20.0 &79.5&51.9&42.1 \\
        Qwen2.5-Math-7B + Long CoT SFT&80.2 &16.7 &60.0 &\underline{49.6} &42.1 &91.4&69.1&58.4 \\
	 \rowcolor[rgb]{0.9, 1.0, 1.0}   \textbf{Qwen2.5-Math-7B-\model-BI (\textit{ours})}&81.6 &23.3 &60.0 & 43.9 &44.4&91.9&\underline{70.1}&59.3 \\
             \rowcolor[rgb]{0.9, 1.0, 1.0}  \textbf{Qwen2.5-Math-7B-\model-PRL (\textit{ours})}&\underline{83.4} &\underline{26.7}&\underline{70.0}&43.8&\underline{46.4} &\underline{93.2}&\textbf{70.4}&\underline{62.0} \\
	 \rowcolor[rgb]{0.9, 1.0, 1.0}  \textbf{Qwen2.5-Math-7B-\model-ORL (\textit{ours})}&\bfseries 84.4 &23.3&\bfseries 77.5&43.8&44.9 &92.9&\underline{70.1}&\textbf{62.4}\\
				\midrule[1pt]
\end{tabular}}


\caption{The performance of \modelx and other strong baselines on the most challenging math benchmarks is presented. \textbf{BI} refers to the behavior-initialized models through supervised fine-tuning, \textbf{ORL} denotes models trained with outcome-level RL, and \textbf{PRL} refers to models trained with process-level RL. The highest results are highlighted in \textbf{bold} and the second-best results are marked with \underline{underline}. For some baselines, we use the results from their original reports or from \citet{guan2025rstar}, denoted by $^*$.}
\label{tab:mainresults}
\vspace{-0.3cm}
\end{table*}






