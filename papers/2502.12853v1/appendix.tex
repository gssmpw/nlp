\section{Implementation Details}
\subsection{Verification Processing and SFT Data Construction}
\label{ap:veri_implement}
Given the responses sampled from the original LLM policy, we prompt frontier LLMs for initial verifications. In order to construct more valid verification, we force the LLMs to ``verify without re-solving the problem'' and filter out invalid verifications during data processing. We found that despite being instructed to "verify without re-solving the problem", most existing LLMs still biased to solve the problem again, as shown in Table \ref{tab:veri_follow}. Finally, we collected the verification data by querying gpt-4-preview-1106\footnote{\url{https://openai.com/api/}}
, which shows strong instruction-following ability to "verify without re-solving the problem" and can perform plausible verification such as adopting reverse thinking, inductive reasoning and other methods.


For these collected prompts, we refine the remaining verifications using gpt-4o to improve fluency and clarity. During this refinement, we instruct gpt-4o to append a conclusion at the end of each verification based on its stance—for example: ``Therefore, the answer is correct/incorrect/cannot verify.'' Finally, we discard any verifications where the judgment does not align with the actual correctness of the answer. The prompts we used during the whole process are provided in Appendix \S\ref{ap:prompts}.

With the refined and filtered verifications, we construct the SFT data as follows. For each problem, we determine the number of answer attempts required to eventually obtain a correct answer based on the accuracy from the initial sampling. The lower the accuracy, the more rounds of responses are generated. In our implementation, we categorize all problems into four difficulty levels and construct answer sequences with 1, 2, 3, or 4 rounds, according to descending accuracy. Then, after an incorrect answer, we append ``Wait, let me recheck my solution'' along with the corresponding verification. If that answer is not the final attempt, we further append ``Let me try again.'' We ensure that the last answer in the sequence is correct. Additionally, we ensure that the answers in each round for a given problem are distinct. Figure \ref{fig:sft_case} is an example of SFT data constructed with 4 rounds of responses.

\subsection{Baseline Details}\label{ap:baseline}
\subsubsection{Baseline Implementations}

In Table \ref{tab:mainresults}, the reported results for Frontier LLMs and Top-tier Open-source Reasoning LLMs are sourced from the original reports and \citet{guan2025rstar}. We evaluate Llama-3.1-8B-Instruct \cite{llama3.1}, Qwen2-7B-Instruct \cite{qwen2}, Qwen2.5-Math-7B, Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct\cite{qwen2.5} using the same process described in Section \S \ref{sec:e_metric}. For Eurus-7B-PRIME \cite{cui2025process}, rStar-Math-7B \cite{guan2025rstar}, and Qwen2.5-7B-SimpleRL \cite{zeng2025simplerl}, we report results directly from the original papers.

In Table \ref{tab:transfer-results}, the results for Llama-3.1-70B-Instruct and QwQ-32B-Preview are taken from \citet{shen2025satori}. For the remaining baselines, we follow the official evaluation protocol of the dataset project\footnote{%
  \begin{tabular}[t]{@{}l@{}}
    \href{https://github.com/Yale-LILY/FOLIO}{https://github.com/Yale-LILY/FOLIO}\\[0.5ex]
    \href{https://github.com/facebookresearch/cruxeval}{https://github.com/facebookresearch/cruxeval}\\[0.5ex]
    \href{https://github.com/eladsegal/strategyqa}{https://github.com/eladsegal/strategyqa}\\[0.5ex]
    \href{https://github.com/TIGER-AI-Lab/MMLU-Pro}{https://github.com/TIGER-AI-Lab/MMLU-Pro}
  \end{tabular}
}.
\subsubsection{Baseline License}

In this work, we utilize the Llama-3.1-8B-Instruct model, whose license can be reviewed at \url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/LICENSE}. In addition, the models Qwen2-7B-Instruct, Qwen2.5-Math-7B, Eurus-2-7B-PRIME, and project vLLM are distributed under the Apache License 2.0. We gratefully acknowledge the contributions of the open-source community and strictly adhere to the terms of the respective licenses.


\subsubsection{Baseline SFT Data Construction}
\label{ap:sft_base}
\paragraph{Original Solution SFT Data}
In this setting, we use the solution from the original dataset as sft data. To ensure a fair comparison, we maintain the same training data volume as our behavior initialization approaches.
\paragraph{Long CoT SFT Data}

We also introduce a baseline by fine-tuning on Long CoT responses generated by QwQ-32B-Preview \cite{qwq-32b-preview}. Specifically, we instruct QwQ to generate responses to given problems and filter out those with incorrect answers. The remaining high-quality responses are then used for supervised fine-tuning. Importantly, we ensure that the total training data volume remains consistent with that used in our behavior initialization approach. The prompt we use for QwQ is provided in Appendix \S\ref{ap:prompts}.



\subsection{Prompts}
\label{ap:prompts}
The prompts we use in all experiments are as follows:
\input{tables/prompts}

\section{Detailed Experiment Settings}
\input{tables/verification_compare}
\subsection{Datasets}
\label{ap:datasets}
Details of each test dataset we used as benchmark are as follows:

\subsubsection{In-domain Datasets}
\textbf{MATH500} \cite{lightman2023lets} offers a streamlined slice of the broader MATH \cite{MATH} dataset, comprising 500 test problems selected through uniform sampling. Despite its smaller scope, it maintains a distribution of topics and difficulty levels that mirrors the larger MATH corpus.

\textbf{GSM8K} \cite{cobbe2021gsm8k} features around 8,500 grade-school math word problems. The dataset focuses on simple arithmetic through early algebra and includes 1,319 distinct tasks in its test set.

\textbf{OlympiadBench} \cite{he2024olympiadbench} collects 8,476 advanced math and physics questions drawn from Olympiad contexts, with some originating from the Chinese college entrance exam. We use the subset of 674 text-only competition questions, providing open-ended math challenges.

\textbf{AMC2023} \cite{amc} and \textbf{AIME} \cite{aime} each supply a set of challenging exam-style problems: 40 questions from AMC 2023 and 30 from AIME 2024, all in text-only format.

\textbf{CollegeMath} \cite{tang2024mathscale} is a dataset targeting advanced college-level mathematics, drawn from nine textbooks spanning seven major fields—algebra, pre-calculus, calculus, vector calculus, probability, linear algebra, and differential equations. The final collection comprises 1,281 training examples and 2,818 test examples.

\textbf{Gaokao2023en} \cite{liao2024mario} is a dataset consisting of 385 mathematics problems sourced from the 2023 Chinese higher education entrance examination, which have been professionally translated into English. 

\subsubsection{Cross-domain Datasets}
\textbf{FOLIO} \cite{han2022folio} is meticulously annotated to assess intricate logical reasoning in natural language. It pairs 1,430 conclusions with 487 sets of premises—each verified using first-order logic (FOL)—and contains 203 unique problems in its test portion.

\textbf{CRUXEval} \cite{gu2024cruxeval} tests code comprehension and reasoning through 800 concise Python functions (spanning 3–13 lines). Each function is accompanied by one or more input-output examples. The goal is to predict the correct outputs given the function body and a specific input. The test partition encompasses all 800 problems.

\textbf{StrategyQA} \cite{geva2021strategyqa} targets multi-hop reasoning questions where the necessary intermediate steps are not explicit. Each of its 2,780 items includes a strategic query, a breakdown of the reasoning steps, and supporting evidence drawn from Wikipedia.

\textbf{MMLUProSTEM} is extracted from \textbf{MMLU-Pro} \cite{wang2024mmlu}. Following Satori \cite{shen2025satori}, we conduct evaluations on six STEM subsets—physics, chemistry, computer science, engineering, biology, and economics.



\subsection{Hyperparameters Setting}
\label{ap:hyper}
\input{tables/hyper}
During behavior initialization with SFT, we use a batch size of \textit{32} and adopt a learning rate of \textit{5e-6}. We set the maximum sequence length \textit{8000} to accommodate long responses and verifications. To balance stability and convergence during training, we add a KL punishment to the training loss, and the KL coefficient is set to \textit{0.1}. 

During reinforcement learning, 
for each training batch, we use a training batch size of 64, and sample $n$ responses for each question in a batch, resulting a forward batch size of $64n$. For each forward batch, we update the model for $n$ step with the training batch size 64. Specifically, for both process-level and outcome-level RL, we adopt $n=4$ (i.e., for RLOO, the sample number is also $4$). More hyperparameters of the RL training are presented in Table \ref{tab:hyper_rl}. We use the BF16 model precision in all experiments.

Main hyperparameters used in the experiments are illustrated in Table \ref{tab:hyper_sft} and \ref{tab:hyper_rl}.

\subsection{Experiment Environment}

All experiments are implemented using the PyTorch framework on 32 NVIDIA H20 (96GB) GPUs or 32 NVIDIA A100Pro (40GB) GPUs. Our training code is built upon Hugging Face TRL\footnote{\url{https://github.com/huggingface/trl}}. For inference, we use a single NVIDIA A100 (40GB) GPU with vLLM-0.5.4\footnote{\url{https://github.com/vllm-project/vllm}}. 
We utilize transformers version 4.39.3 for fine-tuning Qwen2-7B-Instruct and Qwen2.5-Math-7B, version 4.44.0 for fine-tuning Llama-3.1-8B, and version 4.46.3 for reinforcement learning. We use PyTorch 2.1.1 across our training pipeline.
Our evaluation code is built upon Qwen Math's evaluation codebase\footnote{\url{https://github.com/QwenLM/Qwen2.5-Math}}.

\section{Metrics Definition}
\label{ap:metric}
We include the formal definition of metrics we use for analyzing self-verification and self-correction behaviors of the post-trained models as follows.

\subsection{Notations}
We first present the main notations used in our formulation in Table \ref{tab:variable_lookup}.

\input{tables/var_lookup}

\subsection{Self-Verification Metrics}

\subsubsection{Verification Accuracy (VA)}
Verification Accuracy measures how often the verification prediction matches the ground-truth correctness ($N$ is the total number of verifications in the responses to the test set):
\begin{equation}
\resizebox{\linewidth}{!}{
$\text{VA} = \frac{1}{N}\sum_{t=1}^{N} \mathbb{I}\Bigl(\text{Parser}(v_t) = V_{golden}(s_t)\Bigr).$
}
\end{equation}

\subsubsection{Error Recall (ER)}
Error Recall measures the recall of detecting incorrect answers (i.e., the fraction of actually incorrect answers that are successfully identified as incorrect):
\begin{equation}
\label{eq:error_recall}
\resizebox{\linewidth}{!}{$
\text{ER} = \frac{\sum_y\sum_{t=1}^\frac{|y|_a}{2} 
    \mathbb{I}\Bigl(R(s_{t})=-1\Bigr)\,
    \mathbb{I}\Bigl(\text{Parser}(v_t)=\texttt{incorrect}\Bigr)}
  {\sum_y\sum_{t=1}^{\frac{|y|_a}{2}}
    \mathbb{I}\Bigl(R(s_{t})=-1\Bigr)}.
$}
\end{equation}
where $|y|_a$ is the total number of actions in $y$ and $\frac{|y|_a}{2}$ is the total number of attempts to solve the problem ($y=\{a_1, a_2,\cdots,a_{|y|_a}\}=\{s_1, v_1,\cdots,s_\frac{|y|_a}{2},v_\frac{|y|_a}{2} \}$). 

\subsubsection{Correct Precision (CP)}
Correct Precision measures the precision when the verification model predicts an answer to be correct (i.e., among all ``correct'' predictions, how many are truly correct):
\begin{equation}
\resizebox{\linewidth}{!}{$
\text{CP} = \frac{\sum_y\sum_{t=1}^\frac{|y|_a}{2}
    \mathbb{I}\Bigl(\text{Parser}(v_t)=\texttt{correct}\Bigr)\,
    \mathbb{I}\Bigl(R(s_{t})=1\Bigr)}
  {\sum_y\sum_{t=1}^\frac{|y|_a}{2}
    \mathbb{I}\Bigl(\text{Parser}(v_t)=\texttt{correct}\Bigr)}.
$}
\end{equation}

\subsection{Self-Correction Metrics}

\subsubsection{Incorrect to Correct Rate (ICR)}
The rate at which the model successfully corrects an initially incorrect answer ($R(s_1)=-1$) into a correct final answer ($R(s_{T_y})=1$), where $T_y=|y|_a/2$ is the total number of attempts to
solve the problem in each $y$. Formally:
\begin{equation}
\text{ICR} 
= \frac{\sum_y \mathbb{I}\bigl(R(s_1) = -1\bigr)\,\mathbb{I}\bigl(R(s_{T_y}) = 1\bigr)}
       {\sum_y \mathbb{I}\bigl(R(s_1) = -1\bigr)}.
\end{equation}

\subsubsection{Correct to Incorrect Rate (CIR)}
The rate at which the model incorrectly alters an initially correct answer ($R(s_1)=1$) into an incorrect final answer ($R(s_{T_y})=-1$), where $T_y=|y|_a/2$ is the total number of attempts to
solve the problem in each $y$. Formally:
\begin{equation}
\label{eq:correct_to_incorrect}
\text{CIR} 
= \frac{\sum_y \mathbb{I}\bigl(R(s_1) = 1\bigr)\,\mathbb{I}\bigl(R(s_{T_y}) = -1\bigr)}
       {\sum_y \mathbb{I}\bigl(R(s_1) = 1\bigr)}.
\end{equation}
% where $N$ is the total number of responses to the test set.


\section{Offline RL Training Details}

In this section, we provide additional details on the offline reinforcement learning training process, including formal definition, ablation studies, and implementation details.


\subsection{Accuracy-Grouped Baseline Definition}
\label{ap:acc_baseline}
To fully leverage the advantages of offline RL, which does not require real-time sampling, we explore more appropriate baseline selection by further grouping trajectories based on problem difficulty. Intuitively, for two trajectories $y^{(1)}$ and $y^{(2)}$ sampled under questions of different difficulty levels, and their corresponding actions $a^{(1)}_t$ and $a^{(2)}_t$ at the same position, even if they share identical reward contexts, their expected returns (baselines) should differ, i.e., the expected return is typically lower for more challenging problems. 

We measure a problem’s difficulty by estimating how often it is solved correctly under the current sampling policy. Concretely, we sample multiple trajectories in parallel for each problem. The fraction of these trajectories that yield a correct final answer serves as the problem’s accuracy. We then discretize this accuracy into separate bins, effectively grouping the problems according to their estimated difficulty. All trajectories belonging to problems within the same accuracy bin form a common subset.

Compared to using direct reward contexts alone, this accuracy-based grouping offers a more robust estimate of expected returns, problems in the same bin share similar success rates. Moreover, unlike a pre-defined difficulty grouping, these bins adjust dynamically as the model’s capabilities evolve. Building on this approach, we propose two accuracy-based baseline estimation methods for offline RL as follows.

\subsubsection{Accuracy-Grouped Baseline With Position Group}
\label{ap:acc_baseline_position}
Within each accuracy bin, we further split actions based on their position in the trajectory. Concretely, we consider all actions occurring at the same step index across trajectories in the same bin to be comparable, and we compute their average return to serve as the baseline. Thus, when we look up the baseline for a particular action at a given step in a trajectory, we use the average return of all actions taken at that same step index in all trajectories belonging to the same accuracy bin.

\subsubsection{Accuracy-Grouped Baseline With Reward Context}
We also propose combining accuracy-based grouping with reward-context grouping. The underlying assumption is that even if two actions share the same immediate reward context, their expected returns can differ if they originate from different difficulty bins. Generally, problems that are harder to solve exhibit lower expected returns. Consequently, we first bin the trajectories by accuracy, then further group them by common reward context. Within each sub-group, we average the returns of all relevant actions to obtain the baseline.

\subsection{Offline RL Implementation Details}
\label{ap:offline_rl_details}
In each iteration of offline RL training, we generate multiple trajectories (e.g., eight) per prompt in parallel. We then apply prompt filtering, rejection sampling, accuracy-based baseline estimation, advantage computation, and policy updates. Implementation details follow.

\subsubsection{Prompt Filtering}
\input{tables/acc_range_ablation}
As we sample multiple trajectories for each prompt, we compute the accuracy of each prompt. We retain prompts whose accuracy falls within a predefined range.

Our ablation study on Qwen2.5-Math-7B shown in Table \ref{tab:acc_ablation} confirms that filtering improves performance. The most stable results are obtained with an accuracy range of \([0.1,0.7]\), suggesting that including moderately difficult samples enhances the model's reasoning capabilities.

\subsubsection{Rejection Sampling}
We discard any trajectory that does not follow the alternation pattern of solution and verification:
$y = (s_1, v_1, \dots, s_k, v_k)$.
Additionally, we remove malformed trajectories such as $y = (s_1, s_2, v_1)$. To mitigate reward hacking due to excessively long outputs, we eliminate trajectories where $R(s_t) = 1$ and $R(v_t) = 1$ at timestep $t$, but further actions are taken at $t+1$. Moreover, we discard trajectories containing more than 20 actions, as excessive action sequences can introduce instability and deviate from expected solution structures.

\subsubsection{Loss Function}
\input{tables/baseline_ablation}
To determine the best offline baseline method, we conducted ablation studies on Qwen2.5-Math-7B shown in Table \ref{tab:baseline_ablation}. We found that using the accuracy-grouped baseline with an additional division by position provides the most stable results. When computing advantages, we subtract both the baseline and a scaled relative policy term like Equation 5. Notably, we fix $\pi_{\text{ref}}$ as the reference policy instead of being updated at each iteration.

\subsubsection{Training Hyperparameter Settings}
We use a batch size of 64, a maximum learning rate of $5\times10^{-7}$, and a KL penalty coefficient of 0.1. The maximum training sequence length is set to 8192. We apply a warm-up phase of 5 steps and a clipping range parameter of 0.2. We use BF16 model precision in all experiments.





\section{Demo Cases}
% \subsection{Demo Cases}
\label{ap:case}
\input{tables/case}
To intuitively demonstrate the effectiveness of our proposed method, we present the model's inference examples after RL on the MATH500 and StrategyQA datasets in the Figure \ref{fig:math_case} and Figure \ref{fig:sqa_case}.


\section{Other Discussion}
\subsection{Discussion on Potential Risk}
We have carefully considered potential risks associated with our work and found no significant concerns. Our approach, focused on enhancing LLM reasoning through self-verification and self-correction, does not introduce malicious or harmful effects, privacy issues, or security threats. Additionally, it does not contribute to biases, fairness concerns, or environmental impact. We believe our work is safe for responsible use in research.

\subsection{Use of AI Assistant}
In this work, we utilized an AI assistant solely for the purpose of refining and polishing the language of the manuscript. The AI assistant was employed to improve clarity, flow, and overall readability, ensuring the text adhered to academic writing standards. It was not involved in any data analysis, experimentation, or formulation of ideas. All research design, methodology, results, and conclusions were developed independently by the authors. The use of the AI assistant was limited to language enhancement and did not influence the content or scientific integrity of the work.