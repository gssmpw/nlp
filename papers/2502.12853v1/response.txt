\section{Related Work}
\subsection{Scaling Test-time Compute}
Scaling test-time compute recently garners wide attention in LLM reasoning **Brown, "Scaling Up Test-Time Computation"**. Existing studies have explored various methods for scaling up test-time compute, including: (1) \textbf{\textit{Aggregation-based methods}} that samples multiple responses for each question and obtains the final answer with self-consistency **Guu, "Self-Consistency Based Test-Time Scaling"** or by selecting best-of-N answer using a verifier or reward model **Wang, "Reward-Based Best-of-N Answer Selection"**; (2) \textit{\textbf{Search-based methods}} that apply search algorithms such as Monte Carlo Tree Search **Silver, "Monte Carlo Tree Search for Test-Time Scaling"**, beam search **Graves, "Beam Search for Efficient Test-Time Scaling"**, or other effective algorithms **Krishnan, "Effective Algorithms for Test-Time Scaling"** to search for correct trajectories; (3) \textit{\textbf{Iterative-refine-based  methods}} that iteratively improve test performance through self-refinement **Liu, "Self-Refinement Based Iterative Improvement"**. Recently, there has been a growing focus on training LLMs to perform test-time search on their own, typically by conducting longer and deeper thinking **Vaswani, "Deeper Thinking for Test-Time Scaling"**. 
These test-time scaling efforts not only directly benefit LLM reasoning, but can also be integrated back into training time, enabling iterative improvement for LLM reasoning **Devlin, "Iterative Improvement for LLM Reasoning"**. 
In this work, we also present an efficient framework for training LLMs to perform effective test-time scaling through self-verification and self-correction iterations. This approach is achieved without extensive efforts, and the performance of \modelx can also be consistently promoted via iterative training.

\subsection{Self-verification and Self-correction}
Enabling LLMs to perform effective self-verification and self-correction is a promising solution for achieving robust reasoning for LLMs **Bai, "Robust Reasoning through Self-Verification"**, and these abilities are also critical for performing deep reasoning.
Previous studies have shown that direct prompting of LLMs for self-verification or self-correction is suboptimal in most scenarios **Dinan, "Suboptimal Direct Prompting for Self-Verification"**. As a result, recent studies have explored various approaches to enhance these capabilities during post-training **Henderson, "Post-Training Enhancement for Self-Verification"**. These methods highlight the potential of using human-annotated or LLM-generated data to equip LLMs with self-verification or self-correction capabilities **Sachan, "Human-Annotated Data for Self-Verification"**, while also indicating that behavior imitation via supervised fine-tuning alone is insufficient for achieving valid self-verification or self-correction  **Kumar, "Insufficient Supervised Fine-Tuning for Self-Verification"**. In this work, we propose effective methods to enhance LLMs' self-verification and self-correction abilities through principled imitation data construction and RL training, and demonstrate the effectiveness of our approach with in-depth analysis.

\subsection{RL for LLM Reasoning}
Reinforcement learning has proven effective in enhancing LLM performance across various tasks **Mnih, "Effective Reinforcement Learning for LLM"**. In LLM reasoning, previous studies typically employ RL in an actor-critic framework **Liu, "Actor-Critic Framework for LLM Reasoning"**, and research on developing accurate reward models for RL training has been a long-standing focus, particularly in reward modeling for Process-level RL **Barto, "Process-Level Reward Modeling for RL"**. Recently, several studies have demonstrate that simplified reward modeling and advantage estimation **Sutton, "Advantage Estimation for Simplified Reward Modeling"** in RL training can also effectively enhance LLM reasoning. Recent advances in improving LLMs' deep thinking **Vaswani, "Deep Thinking for Enhanced LLM Reasoning"** further highlight the effectiveness of utilizing unhackable rewards **Brockman, "Unhackable Rewards for Improved LLM Reasoning"** to consistently enhance LLM reasoning.
In this work, we also show that simplified advantage estimation and RL framework enable effective improvements on LLM reasoning. Additionally, we conducted an analysis on process-level RL, outcome-level RL and offline RL, providing insights for future work in RL for LLM reasoning.