\section{Related Work}
\subsection{Scaling Test-time Compute}
Scaling test-time compute recently garners wide attention in LLM reasoning ____. Existing studies have explored various methods for scaling up test-time compute, including: (1) \textbf{\textit{Aggregation-based methods}} that samples multiple responses for each question and obtains the final answer with self-consistency ____ or by selecting best-of-N answer using a verifier or reward model ____; (2) \textit{\textbf{Search-based methods}} that apply search algorithms such as Monte Carlo Tree Search ____, beam search ____, or other effective algorithms ____ to search for correct trajectories; (3) \textit{\textbf{Iterative-refine-based  methods}} that iteratively improve test performance through self-refinement ____. Recently, there has been a growing focus on training LLMs to perform test-time search on their own, typically by conducting longer and deeper thinking ____. 
These test-time scaling efforts not only directly benefit LLM reasoning, but can also be integrated back into training time, enabling iterative improvement for LLM reasoning ____. 
In this work, we also present an efficient framework for training LLMs to perform effective test-time scaling through self-verification and self-correction iterations. This approach is achieved without extensive efforts, and the performance of \modelx can also be consistently promoted via iterative training.

\subsection{Self-verification and Self-correction}
Enabling LLMs to perform effective self-verification and self-correction is a promising solution for achieving robust reasoning for LLMs ____, and these abilities are also critical for performing deep reasoning.
Previous studies have shown that direct prompting of LLMs for self-verification or self-correction is suboptimal in most scenarios ____. As a result, recent studies have explored various approaches to enhance these capabilities during post-training ____. These methods highlight the potential of using human-annotated or LLM-generated data to equip LLMs with self-verification or self-correction capabilities ____, while also indicating that behavior imitation via supervised fine-tuning alone is insufficient for achieving valid self-verification or self-correction  ____. In this work, we propose effective methods to enhance LLMs' self-verification and self-correction abilities through principled imitation data construction and RL training, and demonstrate the effectiveness of our approach with in-depth analysis.





\subsection{RL for LLM Reasoning}
Reinforcement learning has proven effective in enhancing LLM performance across various tasks ____. In LLM reasoning, previous studies typically employ RL in an actor-critic framework ____, and research on developing accurate reward models for RL training has been a long-standing focus, particularly in reward modeling for Process-level RL ____. Recently, several studies have demonstrate that simplified reward modeling and advantage estimation ____ in RL training can also effectively enhance LLM reasoning. Recent advances in improving LLMs' deep thinking ____ further highlight the effectiveness of utilizing unhackable rewards ____ to consistently enhance LLM reasoning.
In this work, we also show that simplified advantage estimation and RL framework enable effective improvements on LLM reasoning. Additionally, we conducted an analysis on process-level RL, outcome-level RL and offline RL, providing insights for future work in RL for LLM reasoning.