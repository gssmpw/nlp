@article{ahmadian2024back,
  title={Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms},
  author={Ahmadian, Arash and Cremer, Chris and Gall{\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Pietquin, Olivier and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2402.14740},
  year={2024}
}

@article{alphallm,
	title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
	author={Tian, Ye and Peng, Baolin and Song, Linfeng and Jin, Lifeng and Yu, Dian and Mi, Haitao and Yu, Dong},
	journal={arXiv preprint arXiv:2404.12253},
	year={2024}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{brown2024large,
	title={Large language monkeys: Scaling inference compute with repeated sampling},
	author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
	journal={arXiv preprint arXiv:2407.21787},
	year={2024}
}

@article{chen2024magicore,
  title={MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning},
  author={Chen, Justin Chih-Yao and Prasad, Archiki and Saha, Swarnadeep and Stengel-Eskin, Elias and Bansal, Mohit},
  journal={arXiv preprint arXiv:2409.12147},
  year={2024}
}

@article{chen2025sets,
  title={SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling},
  author={Chen, Jiefeng and Ren, Jie and Chen, Xinyun and Yang, Chengrun and Sun, Ruoxi and Ar{\i}k, Sercan {\"O}},
  journal={arXiv preprint arXiv:2501.19306},
  year={2025}
}

@article{deepseekmath,
	title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
	author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
	journal={arXiv preprint arXiv:2402.03300},
	year={2024}
}

@article{everitt2021reward,
  title={Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective},
  author={Everitt, Tom and Hutter, Marcus and Kumar, Ramana and Krakovna, Victoria},
  journal={Synthese},
  volume={198},
  number={Suppl 27},
  pages={6435--6467},
  year={2021},
  publisher={Springer}
}

@article{feng2023alphazero,
	title={Alphazero-like tree-search can guide large language model decoding and training},
	author={Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun},
	journal={arXiv preprint arXiv:2309.17179},
	year={2023}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{havrilla2024glore,
  title={Glore: When, where, and how to improve llm reasoning via global and local refinements},
  author={Havrilla, Alex and Raparthy, Sharath and Nalmpantis, Christoforus and Dwivedi-Yu, Jane and Zhuravinskyi, Maksym and Hambro, Eric and Raileanu, Roberta},
  journal={arXiv preprint arXiv:2402.10963},
  year={2024}
}

@article{havrilla2024teaching,
  title={Teaching large language models to reason with reinforcement learning},
  author={Havrilla, Alex and Du, Yuqing and Raparthy, Sharath Chandra and Nalmpantis, Christoforos and Dwivedi-Yu, Jane and Zhuravinskyi, Maksym and Hambro, Eric and Sukhbaatar, Sainbayar and Raileanu, Roberta},
  journal={arXiv preprint arXiv:2403.04642},
  year={2024}
}

@article{huang2023large,
	title={Large language models cannot self-correct reasoning yet},
	author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
	journal={arXiv preprint arXiv:2310.01798},
	year={2023}
}

@article{jiang2024towards,
  title={Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning},
  author={Jiang, Huchen and Ma, Yangyang and Ding, Chaofan and Luan, Kexin and Di, Xinhan},
  journal={arXiv preprint arXiv:2412.17397},
  year={2024}
}

@article{kamoi2024can,
  title={When can llms actually correct their own mistakes? a critical survey of self-correction of llms},
  author={Kamoi, Ryo and Zhang, Yusen and Zhang, Nan and Han, Jiawei and Zhang, Rui},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1417--1440},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{kumar2024training,
	title={Training language models to self-correct via reinforcement learning},
	author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
	journal={arXiv preprint arXiv:2409.12917},
	year={2024}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
      journal={arXiv preprint arXiv:2305.20050},
      year={2023}
}

@article{luo2024improve,
  title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  year={2024}
}

@article{luong2024reft,
  title={Reft: Reasoning with reinforced fine-tuning},
  author={Luong, Trung Quoc and Zhang, Xinbo and Jie, Zhanming and Sun, Peng and Jin, Xiaoran and Li, Hang},
  journal={arXiv preprint arXiv:2401.08967},
  year={2024}
}

@article{ma2024large,
  title={Are Large Language Models Good Prompt Optimizers?},
  author={Ma, Ruotian and Wang, Xiaolei and Zhou, Xin and Li, Jian and Du, Nan and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.02101},
  year={2024}
}

@article{madaan2024self,
	title={Self-refine: Iterative refinement with self-feedback},
	author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@misc{mathshepherd,
	title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
	author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
	year={2024},
	eprint={2312.08935},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2312.08935}, 
}

@article{o1,
	title   = {Openai o1 system card},
	author  = {OpenAI},
	year    = {2024},
	journal = {preprint}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{paul2023refiner,
  title={Refiner: Reasoning feedback on intermediate representations},
  author={Paul, Debjit and Ismayilzada, Mete and Peyrard, Maxime and Borges, Beatriz and Bosselut, Antoine and West, Robert and Faltings, Boi},
  journal={arXiv preprint arXiv:2304.01904},
  year={2023}
}

@article{qi2024mutual,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  year={2024}
}

@article{qin2024o1,
  title={O1 Replication Journey: A Strategic Progress Report--Part 1},
  author={Qin, Yiwei and Li, Xuefeng and Zou, Haoyang and Liu, Yixiu and Xia, Shijie and Huang, Zhen and Ye, Yixin and Yuan, Weizhe and Liu, Hector and Li, Yuanzhi and others},
  journal={arXiv preprint arXiv:2410.18982},
  year={2024}
}

@article{qu2025recursive,
  title={Recursive introspection: Teaching language model agents how to self-improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={55249--55285},
  year={2025}
}

@article{restmcts,
	title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search},
	author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
	journal={arXiv preprint arXiv:2406.03816},
	year={2024}
}

@article{rosset2024direct,
  title={Direct nash optimization: Teaching language models to self-improve with general preferences},
  author={Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang},
  journal={arXiv preprint arXiv:2404.03715},
  year={2024}
}

@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}

@article{selfrefine,
	title={Self-refine: Iterative refinement with self-feedback},
	author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@article{setlur2024rewarding,
  title={Rewarding progress: Scaling automated process verifiers for llm reasoning},
  author={Setlur, Amrith and Nagpal, Chirag and Fisch, Adam and Geng, Xinyang and Eisenstein, Jacob and Agarwal, Rishabh and Agarwal, Alekh and Berant, Jonathan and Kumar, Aviral},
  journal={arXiv preprint arXiv:2410.08146},
  year={2024}
}

@article{setlur2025rl,
  title={Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold},
  author={Setlur, Amrith and Garg, Saurabh and Geng, Xinyang and Garg, Naman and Smith, Virginia and Kumar, Aviral},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={43000--43031},
  year={2025}
}

@article{shinn2023reflexion,
	title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
	author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
	journal={arXiv preprint arXiv:2303.11366},
	year={2023}
}

@article{shinn2024reflexion,
	title={Reflexion: Language agents with verbal reinforcement learning},
	author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@misc{snell2024scalingllmtesttimecompute,
	title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
	author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
	year={2024},
	eprint={2408.03314},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2408.03314}, 
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{tajwar2024preference,
  title={Preference fine-tuning of llms should leverage suboptimal, on-policy data},
  author={Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar, Aviral},
  journal={arXiv preprint arXiv:2404.14367},
  year={2024}
}

@article{team2025kimi,
  title={Kimi k1. 5: Scaling Reinforcement Learning with LLMs},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

@article{tyen2023llms,
  title={LLMs cannot find reasoning errors, but can correct them!},
  author={Tyen, Gladys and Mansoor, Hassan and Chen, Peter and Mak, Tony and C{\u{a}}rbune, Victor},
  journal={arXiv preprint arXiv:2311.08516},
  year={2023}
}

@misc{wang2024qimprovingmultistepreasoning,
	title={Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning}, 
	author={Chaojie Wang and Yanchen Deng and Zhiyi Lv and Shuicheng Yan and An Bo},
	year={2024},
	eprint={2406.14283},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2406.14283}, 
}

@article{wu2024empirical,
	title={An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models},
	author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
	journal={arXiv preprint arXiv:2408.00724},
	year={2024}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={11809--11822},
  year={2023}
}

@article{zhang2024small,
  title={Small Language Models Need Strong Verifiers to Self-Correct Reasoning},
  author={Zhang, Yunxiang and Khalifa, Muhammad and Logeswaran, Lajanugen and Kim, Jaekyeom and Lee, Moontae and Lee, Honglak and Wang, Lu},
  journal={arXiv preprint arXiv:2404.17140},
  year={2024}
}

@article{zhang2024understanding,
  title={Understanding the Dark Side of LLMs' Intrinsic Self-Correction},
  author={Zhang, Qingjie and Qiu, Han and Wang, Di and Qian, Haoting and Li, Yiming and Zhang, Tianwei and Huang, Minlie},
  journal={arXiv preprint arXiv:2412.14959},
  year={2024}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

