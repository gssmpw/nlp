\section{Analysis}

In this section, we provide various analyses to study \modelName's superior performance.
Unless specified, we utilize Llama3-8B-Instruct as the base LLM for the analyses.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{ACE} & \textbf{SPEED} & \textbf{GENIA} \\
        \midrule
        \modelName & 50.2 & 31.5 & 28.9 \\
        \quad -- Trigger Generation & 43.2 & 27.8 & 28.2 \\
        \quad -- Data Refinement & 47.4 & 23.3 & 22.0 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study for \modelName's various components measured as Tri-C F1 performance across datasets.}
    \label{tab:ablation}
\end{table}

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{l|cc|cc|cc}
        \toprule
        \textbf{Method} & \multicolumn{2}{c|}{\textbf{ACE}} & \multicolumn{2}{c|}{\textbf{SPEED}} & \multicolumn{2}{c}{\textbf{GENIA}} \\
        & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} \\
        \midrule
        \modelName & \textbf{57.4} & \textbf{50.2} & \textbf{44.6} & \textbf{31.5} & \textbf{35.2} & \textbf{28.9} \\
        \extracttrain{} + \starName & 46.9 & 38.9 & 44.5 & 29.5 & 30.2 & 24.3 \\
        \bottomrule
    \end{tabular}
    \caption{Comparing \modelName{}'s hybrid approach with data-mixing of forward and inverse generation.}
    \label{tab:results-data-mixing}
\end{table}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{ACE} & \textbf{SPEED} & \textbf{GENIA} \\
        \midrule
        \starName & 9.6\% & 15.3\% & 15.1\% \\
        \extracttrain & 19.1\% & 38.4\% & 44.8\% \\
        \modelName & \textbf{23.2\%} & \textbf{49.4\%} & \textbf{52.5\%} \\
        \bottomrule
    \end{tabular}
    \caption{Reporting the hit rate of synthetic data triggers relative to gold test triggers for the three methods.}
    \label{tab:trigger-quality}
\end{table}

\subsection{Ablation study}
\label{sec:analysis-ablation}

Table~\ref{tab:ablation} shows the ablation study for each of our forward generation components.
We observe how forward generation is critical in both trigger generation and data refinement stages with average performance reductions of 3.8\% F1 and 6\% F1 on removing the respective components from \modelName.

% \begin{table}[t]
%     \centering
%     \small
%     \begin{tabular}{l|cc|cc|cc}
%         \toprule
%         \textbf{Method} & \multicolumn{2}{c|}{\textbf{ACE}} & \multicolumn{2}{c|}{\textbf{SPEED}} & \multicolumn{2}{c}{\textbf{GENIA}} \\
%         & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} & \textbf{P} & \textbf{R} \\
%         \midrule
%         \starName & 9.6 & 29.5 & 15.3 & 9.1 & 15.1 & 28.9 \\
%         \extracttrain & 19.1 & 55.6 & 38.4 & \textbf{44.5} & 44.8 & \textbf{47.7} \\
%         \modelName & \textbf{23.2} & \textbf{58.3} & \textbf{49.4} & 37.7 & \textbf{52.5} & 45.4 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Reporting precision (P) and recall (R) between synthetic data triggers and gold test triggers for the three methods using Llama3-8B-Instruct as the base LLM.}
%     \label{tab:trigger-quality}
% \end{table}
% \tanmay{Recall doesn't make sense. Change precision to HIT Rate/Perc. - I-Hung}

\subsection{Comparison with Data mixing}
Data-mixing \cite{DBLP:conf/nips/HoffmannBMBCRCH22, DBLP:conf/nips/XieS0L23} is a widely used technique for leveraging complementary information across datasets to promote robust downstream model training.
We mix forward (\extracttrain) and inverse generation (\starName) as a hybrid baseline to compare with \modelName.
To keep comparisons fair, we consider $N/2$ data instances per event type from each dataset.
Results from Table~\ref{tab:results-data-mixing} demonstrate how \modelName{} beats the data-mixing based hybrid model by 5-6\% F1, highlighting the need for explicit model design to combine the benefits of forward and inverse generation.

\subsection{Analyzing trigger quality and drift}
\label{sec:analysis-trigger-quality}

ED models have a strong tendency to over-rely on lexical relations between triggers and events \cite{tong-etal-2022-docee, star}.
Thus, we compare the synthetic data triggers with the gold test triggers as a raw study on the quality and drift of triggers in the synthesized data.
Specifically, we extract triggers per event type in both datasets and measure the hit rate of the synthesized triggers on the gold set, as reported in Table~\ref{tab:trigger-quality}.
As observed, the poor hit rate shows the poor overlap of \starName's triggers with the gold triggers, which is a primary reason for its domain drift.
Furthermore, the consistently stronger precision of \modelName{} relative to \extracttrain{} demonstrate \modelName's better trigger annotation quality.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Naturalness} & \textbf{Event} & \textbf{Annotation} \\
        & & \textbf{Relevance} & \textbf{Quality} \\
        \midrule
        \starName & 3.1 & 3.4 & 3.1 \\
        \extracttrain & \textbf{4.2} & - & 2.9 \\
        \modelName & 3.6 & \textbf{4.0} & \textbf{3.6} \\
        \bottomrule
    \end{tabular}
    \caption{Human evaluation for sentence naturalness, relevance of event in generated sentence, and the annotation quality. 1 = worst, 5 = best.}
    \label{tab:human-scoring-results}
\end{table}

\begin{table*}[ht]
    \centering
    \small
    % Use a p{} column if you want to wrap text; adjust width as needed
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l l l l p{9.8cm}}
        \toprule
        \textbf{Dataset} & \textbf{Event} & \textbf{Method} & \textbf{Trigger} & \textbf{Sentence} \\
        \midrule
        
        % -- ACE: Conflict:Attack --
        \multirow{4}{*}{ACE} 
          & \multirow{4}{*}{Attack} 
          & \starName  
            & raid      
            & As the rebels embarked on a daring trek across the desert, they launched a surprise \textbf{raid} on the heavily guarded fortress, catching the enemy off guard. \\
        &  & \modelName 
            & shooting  
            & As the rival businessman signed the contract, a sudden \textbf{shooting} erupted outside, causing chaos in the midst of the transaction. \\
        \midrule
        
        % -- SPEED: Death --
        \multirow{4}{*}{SPEED} 
          & \multirow{4}{*}{Death} 
          & \starName  
            & asphyxiation 
            & The hiker's life was tragically cut short as \textbf{asphyxiation} occurred after she became stuck in the narrow cave crevice. \\
        &  & \modelName 
            & killed       
            & The patient's feverish state was triggered when they tested positive for the virus, which ultimately led to their being \textbf{killed} by the rapidly spreading infection. \\
        \midrule
        
        % -- GENIA: Binding --
        \multirow{4}{*}{GENIA} 
          & \multirow{4}{*}{Binding} 
          & \starName  
            & merge     
            & The regulatory protein's ability to activate a specific region of the DNA triggers the \textbf{merge} of two proteins, leading to the modification of gene expression. \\
        &  & \modelName 
            & bound     
            & During the phosphorylation of the enzyme, it \textbf{bound} to the DNA sequence, initiating the transcription process. \\
        
        \bottomrule
    \end{tabular}
    \caption{Qualitative examples demonstrating \starName{} and \modelName{}'s trigger and sentence generation quality.}
    \label{tab:qual-analysis}
\end{table*}

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|cc|cc|cc}
        \toprule
        \textbf{Method} & \multicolumn{2}{c|}{\textbf{ACE}} & \multicolumn{2}{c|}{\textbf{SPEED}} & \multicolumn{2}{c}{\textbf{GENIA}} \\
        & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} \\
        \midrule
        \modelName & \textbf{57.4} & 50.2 & 44.6 & 31.5 & 35.2 & 28.9 \\
        \quad + SFT LLM & 55.2 & \textbf{51.7} & \textbf{46.9} & \textbf{35.8} & \textbf{36.7} & \textbf{29.1} \\
        \bottomrule
    \end{tabular}
    \caption{Measuring model performance improvement using a LLM fine-tuned on unlabeled train data (SFT LLM) for \modelName. EI: Event Identification F1, TC: Trigger Classification F1.}
    \label{tab:results-fine-tuning}
\end{table}

To further study the label quality and relevance, we conduct a human evaluation.
Specifically, a human expert ED annotator is tasked to score the generations (between 1-5) on the naturalness of the sentence for the domain, the correct relevance of the event mentioned in the generated sentence, and the annotation quality (more details in \S~\ref{sec:appendix-human-study}).
We provide the averaged scores across the three datasets for 90 samples in Table~\ref{tab:human-scoring-results}.
\extracttrain{} has high sentence quality but poor label annotations; while \starName{} suffers from poor event relevance indicating domain drift.
Overall, \modelName{} performs the best with high annotation quality and event relevance.

\subsection{Domain-adapted LLM Fine-tuning}
\label{sec:exgen-target-sft}

We fine-tune the base LLM used for passage generation on the unlabeled target-domain data $D_T'$ to better align the generated passages to the target domain.
Naturally, this can be applied only for smaller LLMs owing to fine-tuning costs.
We present the results of fine-tuning Llama3-8B-Instruct on the unlabeled train data in Table~\ref{tab:results-fine-tuning}.
On average, we observe how target data fine-tuning improves \modelName{} by a slight 0.5-2\% F1, suggesting that target-domain passage generation may help, but it is not a highly influencing factor to improve downstream model performance.

% \begin{table}[t]
%     \centering
%     \small
%     \setlength{\tabcolsep}{3.5pt}
%     \begin{tabular}{l|cc|cc|cc}
%         \toprule
%         \textbf{Method} & \multicolumn{2}{c|}{\textbf{ACE}} & \multicolumn{2}{c|}{\textbf{SPEED}} & \multicolumn{2}{c}{\textbf{GENIA}} \\
%         & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} \\
%         \midrule
%         \modelName & \textbf{57.4} & \textbf{50.2} & 44.6 & 31.5 & \textbf{35.2} & \textbf{28.9} \\
%         \extracttrain{} + \starName & 46.9 & 38.9 & 44.5 & 29.5 & 30.2 & 24.3 \\
%         \extracttrain{} + \modelName & 53.4 & 48.7 & \textbf{48.0} & \textbf{32.4} & 30.5 & 24.4 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Measuring model performance improvement using a LLM fine-tuned on unlabeled train data (SFT LLM) for \modelName. EI: Event Identification F1, TC: Trigger Classification F1.}
%     \label{tab:results-data-mixing}
% \end{table}
% \tanmay{Remove last line - not adding much value.}

\subsection{Qualitative analysis of generated data}
\label{sec:qual-analysis}

We provide qualitative evidence for \modelName's domain-aligned triggers compared to \starName{} in Table~\ref{tab:qual-analysis} (more examples in Table~\ref{tab:compelte-qual-analysis}).
Owing to lack of domain grounding in \starName{}, the resulting triggers often appear misaligned (e.g. \emph{asphyxiation} for \textit{death} event related to pandemics).
This misalignment carries over to the generated sentences, further reducing their quality and alignment.
In contrast, \modelName's triggers are better aligned to the target domain corpus, resulting in better quality data.

% \subsection{Qualitative analysis of passage generations}
% \label{sec:qual-analysis-passages}

% \subsection{Generated data size ablation}
% Can also try this with few-shot examples (like a 2-d grid)

