\section{Related Works}
\label{sec:related-works}

\paragraph{Low-resource Event Detection}
Event Detection (ED) has been studied extensively \cite{sundheim-1992-overview, grishman-sundheim-1996-message}, leading to diverse datasets in news \cite{doddington-etal-2004-automatic, song-etal-2015-light, DBLP:conf/tac/EllisGFKSBS15}, Wikipedia \cite{li-etal-2021-document, pouran-ben-veyseh-etal-2022-mee}, and general domains \cite{wang-etal-2020-maven, parekh-etal-2023-geneva}, as well as niche areas like biomedical \cite{mlee, kim-etal-2011-overview-genia, kim-etal-2013-genia}, multimedia \cite{li-etal-2020-cross}, cybersecurity \cite{casie}, epidemiology \cite{parekh-etal-2024-speed, parekh-etal-2024-event}, and pharmacovigilance \cite{sun-etal-2022-phee}.
To address the growing need for event detection across expanding domains, better low-resource domain-specific techniques are essential.
Prior works have explored transfer learning via Abstract Meaning Representation \cite{huang-etal-2018-zero}, Semantic Role Labeling \cite{zhang-etal-2021-zero}, and Question Answering \cite{lyu-etal-2021-zero}.
Reformulating ED as a conditional generation task has also aided low-resource training \cite{hsu-etal-2022-degree, hsu-etal-2023-ampere, huang-etal-2022-multilingual-generative}.
Recently, LLM-based reasoning \cite{DBLP:journals/corr/abs-2304-11633, DBLP:journals/corr/abs-2303-03836, wang-etal-2023-code4struct} and transfer-learning \cite{cai-etal-2024-improving-event} has been explored for low-resource ED  and transfer learning, but performance remains inferior to supervised models \cite{huang-etal-2024-textee}. 
This motivates efforts in synthetic data generation for low-resource ED.

\paragraph{Data Generation for Information Extraction}
LLM-powered synthetic data generation has been successful for various NLP tasks \cite{li-etal-2023-synthetic, wang-etal-2023-self-instruct, DBLP:journals/www/WuZQWGSQZZLXC24, shao-etal-2025-case2code}.
For information extraction, works have explored knowledge retrieval \cite{chen2023chain, amalvy-etal-2023-learning}, translation \cite{parekh-etal-2024-contextual, DBLP:conf/iclr/LeCR024}, and data re-editing \cite{DBLP:journals/corr/abs-2102-01335, hu-etal-2023-entity}.
Some works in the directions we focus in our work include forward generation \cite{chia-etal-2022-relationprompt, ye-etal-2022-zerogen, wang-etal-2023-improving-unsupervised, DBLP:journals/corr/abs-2303-04360} and inverse generation \cite{josifoski-etal-2023-exploiting, star}.
We improve on existing forward and inverse generation works via our hybrid generation approach \modelName.
