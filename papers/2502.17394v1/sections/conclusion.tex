\section{Conclusion and Future Work}

We introduce \modelName{}, a hybrid forward-inverse generation approach for better domain-specific synthetic data in low-resource ED.
Experiments on three diverse datasets reveal that forward generation suffers from noisy labels due to poor LLM reasoning, while inverse generation faces domain drift.
\modelName{} mitigates these issues, achieving superior domain alignment and cleaner data, leading to the best downstream ED performance in zero and few-shot settings.
Future works can explore enhancing stronger domain alignment and extending \modelName{} for multilingual generation.
