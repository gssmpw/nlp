\section{Experimental Setup}
\label{sec:expt}

\begin{table*}[ht]
    \centering
    \small
    \setlength{\tabcolsep}{4.5pt}
    \begin{tabular}{lll|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Base LLM}} & \multirow{2}{*}{\textbf{Method}} & \textbf{Unlabeled} & \multicolumn{2}{c|}{\textbf{ACE}} & \multicolumn{2}{c|}{\textbf{SPEED}} & \multicolumn{2}{c|}{\textbf{GENIA}} & \multicolumn{2}{c}{\textbf{Average}} \\
         & & \textbf{Data Source} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Eve-I} & \textbf{Tri-C} \\
        \midrule
        \multirow{5}{*}{Llama3-8B} & Inference & - & 30.2 & 23.8 & 39.8 & 25.4 & 21.9 & 17.2 & 30.6 & 22.1 \\
        & \starName & - & 44.9 & 35.0 & 21.0 & 10.1 & 25.9 & 19.0 & 30.6 & 21.4 \\
        & \extracttrain & train & 41.7 & 37.8 & 45.6 & 31.5 & 26.9 & 21.4 & 38.1 & 30.2 \\
        & \textbf{\modelName (ours)} & train & 57.4 & 50.2 & 44.6 & 31.5 & \textbf{35.2} & \textbf{28.9} & 45.7 & \textbf{36.9} \\
        % & \quad + Target SFT & train &  \\
        & \textbf{\modelName{} (ours)} & external & \textbf{57.7} & \textbf{52.6} & \textbf{47.8} & \textbf{32.9} & 33.6 & 24.6 & \textbf{46.4} & 36.7 \\
        % & \quad + Target SFT & external & \\
        \midrule
        \multirow{5}{*}{Llama3-70B} & Inference & - & 46.9 & 41.3 & 46.9 & 35.6 & 34.2 & 28.2 & 42.7 & 35.0 \\
        & \starName & - & 50.0 & 42.3 & 18.3 & 13.8 & 23.3 & 16.9 & 30.5 & 24.3 \\
        & \extracttrain & train & 53.2 & 48.0 & \textbf{52.8} & \textbf{39.6} & 36.2 & 29.1 & 47.4 & 38.9 \\
        & \textbf{\modelName{} (ours)} & train & 58.1 & 53.8 & 49.9 & 38.7 & 38.0 & 29.7 & 48.7 & 40.7 \\
        & \textbf{\modelName{} (ours)} & external & \textbf{59.7} & \textbf{55.6} & 50.1 & 39.2 & \textbf{39.2} & \textbf{31.5} & \textbf{49.7} & \textbf{42.1} \\
        \midrule
        \multirow{5}{*}{GPT-3.5} & Inference & - & 33.0 & 26.2 & 44.2 & 32.9 & 31.2 & 24.7 & 36.1 & 27.9 \\
        & \starName & - & 45.0 & 36.6 & 21.3 & 14.6 & 21.8 & 14.3 & 29.4 & 21.8 \\
        & \extracttrain & train & 49.7 & 44.6 & \textbf{50.7} & \textbf{37.5} & 37.7 & 30.1 & 46.1 & 37.4 \\
        & \textbf{\modelName{} (ours)} & train & \textbf{54.8} & 48.3 & 50.3 & 36.8 & \textbf{39.3} & \textbf{31.1} & \textbf{48.1} & \textbf{38.7} \\
        & \textbf{\modelName{} (ours)} & external & 54.0 & \textbf{48.5} & 50.1 & 36.1 & 38.7 & 29.4 & 47.6 & 38.0 \\
        \midrule
        % \multicolumn{2}{l}{Gold Training data SFT of DEGREE} & \\
        - & Gold Data & - & 64.6 & 61.6 & 64.0 & 53.5 & 51.3 & 44.0 & 60.0 & 53.0 \\
        \bottomrule
    \end{tabular}
    \caption{Zero-shot results comparing \modelName{} with other baselines across three datasets and three base LLMs. Except for Inference, all other evaluations are performances of downstream DEGREE \cite{hsu-etal-2022-degree} model trained on data generated by each technique. Eve-I: Event Identification F1, Tri-C: Trigger Classification F1.}
    \label{tab:main-results}
\end{table*}

\paragraph{Datasets:}
We consider three ED datasets from diverse domains for our experiments:
(1) ACE \cite{doddington-etal-2004-automatic}, in the news domain,
(2) SPEED \cite{parekh-etal-2024-event}, in the social media domain, and
(3) GENIA \cite{kim-etal-2011-overview-genia}, in the biomedical domain. We simplify GENIA by converting the original document level annotations to sentence-level annotations.
For the few-shot setting, we compile $k$ data instances from the training data as the few-shot examplers.

For our unlabeled data, we consider two sources:
(1) \textbf{Train} - annotation-free training splits (i.e. only the text) of each dataset and
(2) \textbf{External} - unlabeled data from other external sources.
For ACE, we utilize News Category Dataset \cite{huffpost-data} comprising Huffpost news articles from 2012-2022 as the external data source. We filter articles corresponding to political, financial, and business articles.
For SPEED, we utilize COVIDKB \cite{zong-etal-2022-extracting} mining tweets from the Twitter COVID-19 Endpoint released in April 2020 as the external data source.
Finally, we utilize GENIA2013 dataset \cite{kim-etal-2013-genia} as the external data for GENIA.
We provide statistics about these datasets in Table~\ref{tab:data-statistics}.

\paragraph{Baseline methods:}
We consider three LLM-based techniques for low-resource ED as the baselines for our work.
(1) Inference \cite{DBLP:journals/corr/abs-2303-03836}: LLMs are used to directly infer on the target test data using their reasoning capability.
(2) \starName{} \cite{star}: This model is the state-of-the-art inverse generation model for ED. It utilizes trigger generation, passage generation, and data refinement steps without using any unlabeled data,
(3) Weak Supervision (\extracttrain): This model acts as the forward generation baseline. We utilize the Inference model to provide labels for the unlabeled data.
For an upper bound reference, we also include a human generation baseline (Human) wherein we sample $N$ data instances from the gold training data of each dataset to train the downstream ED model.

\paragraph{Base models:}
For our base LLMs, we consider three instruction-tuned LLMs of varying sizes, namely Llama3-8B-Instruct (8B model), Llama3-70B-Instruct (70B model) \cite{llama3}, and GPT-3.5 (175B model) \cite{gpt}.
For our downstream ED model, we consider a specialized low-resource model DEGREE \cite{hsu-etal-2022-degree}, a generative model prompted to fill event templates powered on a BART-large pre-trained language model \cite{lewis-etal-2020-bart}.

\paragraph{Evaluation:}
Our primary evaluation metric is the synthesized data-trained model's ED performance on the final test splits of each dataset.
We consider two low-resource settings - zero-shot (no labeled data) and few-shot ($k$ datapoints per event type are used).
Note this is different cross-dataset works \cite{cai-etal-2024-improving-event} which train and test on an exclusive set of event types.
For Inference, the LLM is directly run on the test set to procure model predictions.
We report the F1 scores for two metrics \cite{ahn-2006-stages} for measuring model performance:
(1) Event Identification (Eve-I) - correct identification of events, and
(2) Trigger Classification (Tri-C) - correct identification of trigger-event pairs.

\paragraph{Implementation Details:}
We follow \starName{} for the implementation of the baseline models and the majority of hyperparameter settings.
For \modelName's passage generation, we select the top $t=10$ triggers (except $t=8$ for GENIA) for passage generation.
We generate $N=50$ datapoints per event type for each generation strategy.
All our experimental results are reported over an average of three runs.
Additional implementation details are provided in Appendix~\ref{sec:appendix-implementation-details}.
% We utilize $N=50$
% For our downstream supervised model, we consider DEGREE \cite{hsu-etal-2022-degree}, one of the strong supervised generative model.
