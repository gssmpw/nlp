\appendix

\section{Additional details about \modelName}
\label{sec:appendix-exgen-prompts}

We provide illustrations about the various prompts used in \modelName{}.
For target structure extraction forward generation, we consider a two-prompt approach.
The first prompt aims to identify if any events of interest are mentioned in the text as illustrated in Figure~\ref{fig:ed_prompt}.
It comprises the task definition, full event ontology with definitions, the task instructions, and the query sentence.
The second prompt identifies the trigger corresponding to the event of interest, as illustrated in Figure~\ref{fig:te_prompt}.
Here, we specify the task definition, the event ontology details, and the query text with the task instructions.
To aid inverse generation for passage generation, we provide the task definition, the event ontology with event definitions, and the query comprising the sampled target structure.
We illustrate this prompt in Figure~\ref{fig:dg_prompt}.
Finally, we provide the simplified one-prompt setup for forward generation utilized for data refinement in Figure~\ref{fig:dv_prompt}.

\section{Additional Experimental Setup Details}

\subsection{Data Statistics}
We discuss details about our dataset in \S~\ref{sec:expt}.
Our test target domain data includes the test data splits of (1) ACE \cite{doddington-etal-2004-automatic} in the news domain,
(2) SPEED \cite{parekh-etal-2024-event} in the social media domain, and
(3) GENIA \cite{kim-etal-2011-overview-genia} in the biomedical domain
For unlabeled data, we utilize the training data of each dataset as one data source.
For the other data source, we utilize data from external sources, specifically:
(1) News Category Dataset (HuffPost) \cite{huffpost-data} comprising Huffpost news articles from 2012-2022 for ACE. We filter articles corresponding to political, financial, and business articles,
(2) COVIDKB \cite{zong-etal-2022-extracting} mining tweets from the Twitter COVID-19 Endpoint released in April 2020 as the external data source,
(3) GENIA2013 dataset \cite{kim-etal-2013-genia} as the external data for GENIA.
We provide statistics about this data in Table~\ref{tab:data-statistics}.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lccc}
        \toprule
        \textbf{Data} & \textbf{\# Sents} & \textbf{\# Event} & \textbf{Average} \\
        \textbf{Source} & & \textbf{Mentions} & \textbf{Length} \\
        \midrule
        \multicolumn{4}{c}{\textbf{Test Data}} \\
        \midrule
        ACE - test & 832 & 403 & 22.9 \\
        SPEED - test & 586 & 672 & 28.1 \\
        GENIA - test & 2,151 & 1,805 & 29.7 \\
        \midrule
        \multicolumn{4}{c}{\textbf{Unlabeled Train Data}} \\
        \midrule
        ACE - train & 17,172 & - & 15.6 \\
        SPEED - train & 1,601 & - & 33.5 \\
        GENIA - train & 6,431 & - & 30.1 \\
        \midrule
        \multicolumn{4}{c}{\textbf{Unlabeled External Data}} \\
        \midrule
        HuffPost & 43,350 & - & 17.4 \\
        COVIDKB & 7,311 & - & 30.6 \\
        GENIA2013 & 6,542 & - & 17.4 \\
        \bottomrule
    \end{tabular}
    \caption{Data Statistics for the various test and unlabeled datasets used in our work.}
    \label{tab:data-statistics}
\end{table}

\section{Implementation Details}
\label{sec:appendix-implementation-details}

Here, we provide detailed implementation details for each component and models used in our work.
We run most of our experiments on NVIDIA RTX A6000/A100 machines with support for 8 GPUs, while for GPT3.5, we make API calls through OpenAI.

\subsection{LLM-based Generation}

We provide details about the various hyperparameters for using LLMs in all the components of \starName{} and \modelName.
For Llama3-8B-Instruct and Llama3-70B-Instruct, we present the hyperparameters in Table~\ref{tab:hyper-llama}; while Table~\ref{tab:hyper-gpt} presents the hyperparameters for GPT3.5.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        Batch Size & 32 \\
        Temperature & 0.6 \\
        Top-p & 0.9 \\
        Max Generation Length & 250 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for decoding using Llama3-8B/70B model.}
    \label{tab:hyper-llama}
\end{table}

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        Base LLM & gpt-3.5-turbo-0125 \\
        Temperature & 1.0 \\
        Top-p & 1.0 \\
        Max Generation Length & 500 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for decoding using GPT3.5 model.}
    \label{tab:hyper-gpt}
\end{table}

\subsection{Few-shot Implementation Details}

For the few-shot setting, we can access additional $k$ datapoints per event type to aid better performance.
For LLM-based prompting, we simply add these examples in the prompt as in-context examples to help the model do better reasoning/generation.
For inverse generation methods (\starName, \modelName), we do not add the $k$ triggers to the extracted/generated trigger list, as it leads to a drop in model performance.
This can be attributed to the presence of duplicate information as the trigger generation/extraction already accounts for the $k$ triggers.
For passage generation/\extracttrain, we append the $k$ datapoints to the synthetically generated data to provide signals from the gold data.

\subsection{Downstream Model Training}

We choose DEGREE \cite{hsu-etal-2022-degree} as our downstream model for evaluation, a generation-based prompting model that utilizes natural language templates.
We implemented the DEGREE model under TextEE framework \cite{huang-etal-2024-textee}.
Table \ref{tab:hyper-degree} presents the primary hyperparameters for this model.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        Pre-trained LM & BART-Large \\
        Training Epochs & 25 \\
        Warmup Epochs & 5 \\
        Training Batch Size & 32 \\
        Eval Batch Size & 32 \\
        Learning Rate & 0.00001 \\
        Weight Decay & 0.00001 \\
        Gradient Clipping & 5 \\
        Beam Size & 1 \\
        Negative Samples & 15 \\
        Max Sequence Length & 250 \\
        Max Output Length & 20 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for DEGREE model.}
    \label{tab:hyper-degree}
\end{table}

\subsection{LLM Fine-tuning}

We discuss domain-adapted passage generation through LLM fine-tuning in \S~\ref{sec:exgen-target-sft}.
Specifically, we conduct a low-rank finetuning (LoRA) \cite{DBLP:journals/corr/abs-2106-09685} to reduce computational overhead to fine-tune Llama3-8B-Instruct. We implement LoRA using the \texttt{peft} and \texttt{trl} packages \cite{peft, vonwerra2022trl}. We choose the task of causal language modeling (i.e. continual pre-training) to perform domain adaptation on unlabeled in-domain sentences. 
We utilize cross-entropy loss on the dev split of the unlabeled data to select the best model.
We provide additional details about the hyperparameters for this fine-tuning for each dataset in Table~\ref{tab:hyper-sft} below.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lr}
        \toprule
        \multicolumn{2}{c}{\textbf{ACE}} \\
        \midrule
        Lora Rank & 32 \\
        Lora Alpha & 16 \\
        Lora Dropout & 0.1 \\
        Learning Rate & 0.0001 \\
        Weight Decay & 0.05 \\
        Training Batch Size & 32 \\
        Training Epochs & 3 \\
        Eval Steps & 20 \\
        \midrule
        \multicolumn{2}{c}{\textbf{SPEED}} \\
        \midrule
        Lora Rank & 32 \\
        Lora Alpha & 16 \\
        Lora Dropout & 0.1 \\
        Learning Rate & 0.00008 \\
        Weight Decay & 0.05 \\
        Training Batch Size & 32 \\
        Training Epochs & 10 \\
        Eval Steps & 20 \\
        \midrule
        \multicolumn{2}{c}{\textbf{GENIA}} \\
        \midrule
        Lora Rank & 32 \\
        Lora Alpha & 16 \\
        Lora Dropout & 0.1 \\
        Learning Rate & 0.00008 \\
        Weight Decay & 0.05 \\
        Training Batch Size & 32 \\
        Training Epochs & 6 \\
        Eval Steps & 20 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for LoRA fine-tuning Llama3-8B-Instruct.}
    \label{tab:hyper-sft}
\end{table}

\section{Additional analyses}

In this section, we provide additional analyses to support our main experiments.

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|cc|cc|cc}
        \toprule
        \textbf{Method} & \multicolumn{2}{c|}{\textbf{ACE}} & \multicolumn{2}{c|}{\textbf{SPEED}} & \multicolumn{2}{c}{\textbf{GENIA}} \\
        & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} & \textbf{EI} & \textbf{TC} \\
        \midrule
        \starName & \textbf{44.9} & \textbf{35.0} & \textbf{21.0} & 10.1 & 25.9 & 19.0 \\
        \quad + mention & 44.1 & 32.9 & 17.1 & \textbf{10.3} & \textbf{28.7} & \textbf{20.4} \\
        \quad + references & 35.5 & 27.3 & 19.0 & 9.2 & 25.8 & 18.1 \\
        \bottomrule
    \end{tabular}
    \caption{Measuring model performance improvement providing domain-specific cues in the form of domain-mention (mention) or domain sentence references (references) to the LLM for \starName. EI: Event Identification F1, TC: Trigger Classification F1.}
    \label{tab:star-domain-results}
\end{table}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{p{7cm}}
        \toprule
        \multicolumn{1}{c}{\textbf{ACE}} \\
        \midrule
        A 35-year-old cyclist was hit by a speeding car while riding to work, leaving her with severe injuries, while in a separate incident, a local retail giant filed a petition to restructure its debt, sparking concerns about its financial stability. \\
        As the war on terror raged on, the Mujahideen Advisory Council distributed a statement inviting Arab and foreign media reporters to enter Fallujah and cover the battles, while simultaneously, the ownership of the ancient artifacts was transferred to the museum, with the landlord demanding rent on the premises. \\
        \midrule
        \multicolumn{1}{c}{\textbf{SPEED}} \\
        \midrule
        As the influencer's viral challenge went viral, her followers were suddenly struck with a mysterious illness after the splash of a contaminated drink, leading to a shocking explosion of fatalities on social media. \\
        As the community struggled to come to terms with the devastating accident that had claimed the lives of several residents, the authorities swiftly implemented a strict quarantine to prevent the spread of the infectious disease, hoping to mitigate the tragedy. \\
        \midrule
        \multicolumn{1}{c}{\textbf{GENIA}} \\
        \midrule
        The specific transcription factor was elevated by the presence of the hormone, thereby increasing the expression of the target gene, while the inhibitory protein curbed the activity of a competing transcription factor, preventing the expression of a repressor gene. \\
        The binding of PEBP2/CBF to the promoter region boosts the expression of the gene, which turns on the production of a crucial cytokine in response to the immune response. \\
        \bottomrule
    \end{tabular}
    \caption{Example passages of overly long and more stereotypical sentences generated when the domain is mentioned or references are added to the LLM prompt for \starName.}
    \label{tab:star-domain-examples}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/num-samples_3data.pdf}
    \caption{Model performance for \modelName{} as keep change the number of generated datapoints $N$ using Llama3-8B-Instruct for the three datasets.}
    \label{fig:appendix-numsamples-analysis}
\end{figure*}

\subsection{\starName{} with domain-specific prompt}
\label{sec:appendix-analysis-star-domain}

A simple way to infuse domain specific information in inverse generation pipelines like \starName{} would be to add domain-related information in the prompts to the LLM.
We experiment with two such methods:
(1) domain-mention, where we provide the target domain information in the prompt and ask the model to generate accordingly, and
(2) domain-reference, where we use some examples from the unlabeled data in the prompt as reference sentences to better guide the passage generation.
We provide results for these explorations using the Llama3-8B-Insturct model in Table~\ref{tab:star-domain-results}.
As observed, the results are generally poor, with an average drop of 0.1-0.6\% F1 for domain-mention and 3.1-3.8\% F1 for domain-reference.
This is majorly because LLMs over-compensate, producing longer and more stereotypical information in their generations which hurts the naturalness of the sentence and causes further domain drift.
Furthermore, the LLM makes more errors in mentioning the event as a part of its reasoning, which is utilized to make the generation in the domain style.
We provide some qualitative examples for such generations in Table~\ref{tab:star-domain-examples}.
In some ways, it also puts into light and amplifies the gains obtained by doing target domain SFT for \modelName{} as discussed in \S~\ref{sec:exgen-target-sft}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/ed_prompt.pdf}
    \caption{Prompt for stage 1 of forward generation for target trigger extraction.}
    \label{fig:ed_prompt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/te_prompt.pdf}
    \caption{Prompt for stage 2 of forward generation for target trigger extraction.}
    \label{fig:te_prompt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/dg_prompt.pdf}
    \caption{Prompt for inverse generation for passage generation.}
    \label{fig:dg_prompt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/dg_prompt.pdf}
    \caption{Prompt for forward generation for data refinement.}
    \label{fig:dv_prompt}
\end{figure}

\subsection{Impact of different number of training samples}
\label{sec:appendix-num-samples-analysis}

We conduct a small analysis to study the impact of changing the number of generated samples on the downstream model performance for \modelName.
We present the results for Llama3-8B-Instruct in Figure~\ref{fig:appendix-numsamples-analysis}.
As observed, the performance continues to increase as we increase the data from $N=10$ to $N=100$ datapoints per event type.
This promises that inverse generation will provide continued improvements by having larger control over data distribution.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/human-eval-interface.pdf}
    \caption{Illustration of the interface for the human evaluation of the synthetically generated data. Short instructions are provided at the top. Each query comprises the sentence, annotation, and dataset. The human annotator is expected to score 1-5 for each of the three metrics on the right.}
    \label{fig:human-annotation-interface}
\end{figure*}

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{p{5cm}p{1cm}}
        \toprule
        \textbf{Sentence} & \textbf{Score} \\
        \midrule
        The sudden crash of the ambulance sent shockwaves through the hospital as medical staff rushed to the scene to monitor the patient's life signs, but it was too late, as the patient succumbed to the infectious disease. & SN: 2 ER: 1 AQ: 1 \\ \hline
        The wealthy entrepreneur transferred ownership of the struggling tech company to her trusted business partner, relinquishing control and financial responsibility & SN: 5 ER: 5 AQ: 5 \\ \hline
        Taken together, these data suggest that Id1 could be a possible target gene for mediating the effects of BMP-6 in human B cells, whereas Id2 and Id3 not seem to be involved. & SN: 4 ER: 3 AQ: 2\\
        \bottomrule
    \end{tabular}
    \caption{Illustration examples for the human evaluation metrics. SN: sentence naturalness, ER: event relevance, AQ: annotation quality.}
    \label{tab:human-annotation-examples}
\end{table}

\subsection{Human Evaluation Details}
\label{sec:appendix-human-study}

We conduct a small human evaluation to judge the quality of the synthetic data in \S~\ref{sec:analysis-trigger-quality}.
Here, we provide additional details about the human study and evaluation.
Since the evaluation is conducted on three diverse and niche domains, we only utilize a single human annotator who is an ED expert and has previously worked on all three datasets as the primary annotator.

We majorly evaluate on three dimensions:
(1) Sentence naturalness (SN): this metric judges whether the sentence seems grammatical, natural, and fits the domain of the target data.
(2) Event Relevance (ER): this metric is computed only for inverse/hybrid generation methods. This evaluation judges whether the sampled event and trigger are appropriately used to generate a sensible alignment with the target domain. Furthermore, it is verified if the right event definition is used.
(3) Annotation Quality (AQ): this metric judges if the right trigger is used for each event mentioned in the synthetic output. If there are any missing events, then this score is penalized.
For each metric, a score is given on a Likert scale \cite{likert1932technique} from 1 (worst) to 5 (best).
We also provide event definitions for each event in each dataset as a reference for better judgment.
We illustrate the annotation interface in Figure~\ref{fig:human-annotation-interface} and provide some sample examples in Table~\ref{tab:human-annotation-examples}.

\begin{table*}[ht]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{l l l l p{9.5cm}}
        \toprule
        \textbf{Dataset} & \textbf{Event} & \textbf{Method} & \textbf{Trigger} & \textbf{Sentence} \\
        \midrule
        
        % ===== ACE: Conflict:Attack (STAR + ModelName) =====
        \multirow{13}{*}{ACE}
            & \multirow{13}{*}{Attack}
            & \multirow{7}{*}{\starName}
                & raid
                & As the rebels embarked on a daring trek across the desert, they launched a surprise \textbf{raid} on the heavily guarded fortress, catching the enemy off guard. \\
            &
            &
                & siege
                & The government issued fines to the rebel group, while the city was under \textbf{siege}, with gunfire erupting throughout the streets. \\
            &
            &
                & slay
                & The wealthy businessman filed a complaint against his former partner, leading to a court proceeding, just as the rival gang leader \textbf{slays} his arch-nemesis in a brutal battle. \\
          \cmidrule(lr){3-5}
            &
            & \multirow{6}{*}{\modelName}
                & shooting
                & As the rival businessman signed the contract, a sudden \textbf{shooting} erupted outside, causing chaos in the midst of the transaction. \\
            &
            &
                & bombing
                & As the city was rocked by a devastating \textbf{bombing}, thousands of protesters took to the streets to demand justice and peace. \\
            &
            &
                & fight
                & The rival gangs engaged in a brutal \textbf{fight}, sparking a wave of gunfire that left several people injured and buildings damaged. \\
        \midrule

        % ===== SPEED: Death (STAR + ModelName) =====
        \multirow{13}{*}{SPEED}
            & \multirow{13}{*}{Death}
            & \multirow{6}{*}{\starName}
                & asphyxiation
                & The hiker's life was tragically cut short as \textbf{asphyxiation} occurred after she became stuck in the narrow cave crevice. \\
            &
            &
                & overdose
                & A young musician's \textbf{overdose} became the tragic finale of a life cut short in the midst of chaos. \\
            &
            &
                & drowning
                & As she struggled to stay afloat, her heart stopped beating, and she succumbed to \textbf{drowning} in the icy waters. \\
          \cmidrule(lr){3-5}
            &
            & \multirow{7}{*}{\modelName}
                & killed
                & The patient's feverish state was triggered when they tested positive for the virus, which ultimately led to their being \textbf{killed} by the rapidly spreading infection. \\
            &
            &
                & lost
                & As the pandemic spread rapidly across the globe, thousands of people \textbf{lost} their lives due to the deadly virus. \\
            &
            &
                & died
                & The elderly man, who had been suffering from a severe case of tuberculosis, \textbf{died} in his sleep. \\
        \midrule
        
        % ===== GENIA: Binding (STAR + ModelName) =====
        \multirow{14}{*}{GENIA}
            & \multirow{14}{*}{Binding}
            & \multirow{8}{*}{\starName}
                & merge
                & The regulatory protein's ability to activate a specific region of the DNA molecule triggers the \textbf{merge} of two proteins, leading to the modification of gene expression. \\
            &
            &
                & fuse
                & When the proteins \textbf{fuse} together, the activity of the transcription factor is inhibited, preventing the gene expression from proceeding. \\
            &
            &
                & snap
                & When the two proteins \textbf{snap} together, the binding of the complex inhibits the expression of the target gene by deactivating a specific region of the DNA molecule. \\
          \cmidrule(lr){3-5}
            &
            & \multirow{6}{*}{\modelName}
                & bound
                & During the phosphorylation of the enzyme, it \textbf{bound} to the DNA sequence, initiating the transcription process. \\
            &
            &
                & translocation
                & The protein \textbf{translocation} to the nucleus triggers the induction of gene expression. \\
            &
            &
                & binds
                & When the enzyme \textbf{binds} to the substrate, it activates the addition of a phosphate group to the target molecule, marking a crucial change in its function. \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of generated triggers and sentences from {\starName} and {\modelName} methods}
    \label{tab:compelte-qual-analysis}
\end{table*}

\subsection{Additional Qualitative Examples}
\label{sec:appendix-qual-analysis}

In \S~\ref{sec:qual-analysis}, we discussed how \modelName{} improves domain drift qualitatively relative to \starName and provided some examples.
Here, we provide more examples to further support that study in Table~\ref{tab:compelte-qual-analysis}.
This table further demonstrates how \starName{} can have a domain drift without a lack of domain-specific cues, while \modelName{} is better here.

