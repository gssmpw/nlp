% \begin{table*}[ht]
%     \centering
%     \small
%     \setlength{\tabcolsep}{3.5pt}
%     \begin{tabular}{lll|ccc|ccc|ccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Base LLM}} & \multirow{2}{*}{\textbf{Method}} & \textbf{Unlabeled} & \multicolumn{3}{c|}{\textbf{ACE}} & \multicolumn{3}{c|}{\textbf{SPEED}} & \multicolumn{3}{c}{\textbf{GENIA}} \\
%          & & \textbf{Data} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} \\
%         \midrule
%         \multirow{8}{*}{Llama3-8B} & Inference & - & 35.2 & 30.2 & 23.8 & 33.9 & 39.8 & 25.4 & 33.4 & 21.9 & 17.2 \\
%         & \starName & - & 40.4 & 44.9 & 35.0 & 11.5 & 21.0 & 10.1 & 20.9 & 25.9 & 19.0 \\
%         & \modelName{} (ours) & train & 50.6 & 52.5 & 47.5 & 22.9 & 34.0 & 19.8 & 20.3 & 22.3 & 16.3 \\
%         & \quad + Target SFT & train & 52.6 & 51.8 & 47.8 & & & & 31.9 & 28.7 & 23.6 \\
%         & \extracttrain & train & 47.9 & 41.7 & 37.8 & 40.5 & 45.6 & 31.5 & 33.3 & 26.9 & 21.4 \\
%         & \modelName{} (ours) & external & 44.4 & 49.5 & 40.6 & 22.9 & 33.5 & 20.2 & 24.1 & 23.3 & 19.0 \\
%         & \quad + Target SFT & external & \\
%         & \extracttrain & external & 52.1 & 48.4 & 43.6 & 34.8 & 43.7 & 29.9 & 33.7 & 22.7 & 17.9 \\
%         \midrule
%         \multirow{6}{*}{Llama3-70B} & Inference & - & 48.0 & 46.9 & 41.3 & 48.8 & 49.9 & 39.6 & 34.6 & 34.2 & 28.2 \\
%         & \starName & - &  47.5 & 50.0 & 42.3 & 15.7 & 13.8 & 18.3 & 23.1 & 23.3 & 16.9 \\
%         & \modelName{} (ours) & train & 54.7 & 53.9 & 49.8 & 31.7 & 36.7 & 26.5 & 27.8 & 29.2 & 22.4 \\
%         & \extracttrain & train & 55.5 & 53.2 & 48.0 & 52.5 & 56.8 & 43.6 & 36.1 & 36.2 & 29.4 \\
%         & \modelName{} (ours) & external &  \\
%         & \extracttrain & external & & & & 48.6 & 53.0 & 42.7 & 35.4 & 36.2 & 29.5 \\
%         \midrule
%         \multirow{6}{*}{GPT-3.5} & Inference & - & 32.6 & 33.0 & 26.2 & 44.2 & 47.2 & 34.9 & 33.3 & 31.2 & 24.7 \\
%         & \starName & - & 33.4 & 39.0 & 29.6 & 17.0 & 21.3 & 14.6 & 17.7 & 21.8 & 14.3 \\
%         & \modelName{} (ours) & train & 48.1 & 49.2 & 43.6 & 29.4 & 35.4 & 25.2 & 26.4 & 28.6 & 21.6 \\
%         & \extracttrain & train & 52.4 & 49.7 & 44.6 & 49.9 & 54.7 & 41.2 & 37.7 & 38.7 & 31.1 \\
%         & \modelName{} (ours) & external & a6-2 & & & 25.6 & 32.6 & 23.1 & 20.4 & 23.8 & 16.9 \\
%         & \extracttrain & external & 50.1 & 49.4 & 43.3 & 38.8	& 41.4 & 34.4 & 38.3 & 38.2 & 29.6 \\
%         \midrule
%         % \multicolumn{2}{l}{Gold Training data SFT of DEGREE} & \\
%         BART-large & DEGREE (gold) & - & 74.8 & 73.7 & 71.6 & 63.7 & 68.4 & 58.2 & 67.4 & 70.4 & 64.8 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:main-results}
% \end{table*}

% \begin{table*}[ht]
%     \centering
%     \small
%     \setlength{\tabcolsep}{3.5pt}
%     \begin{tabular}{lll|ccc|ccc|ccc|ccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Base LLM}} & \multirow{2}{*}{\textbf{Method}} & \textbf{Unlabeled} & \multicolumn{3}{c|}{\textbf{ACE}} & \multicolumn{3}{c|}{\textbf{SPEED}} & \multicolumn{3}{c}{\textbf{GENIA}} & \multicolumn{3}{c}{\textbf{Average}} \\
%          & & \textbf{Data} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} \\
%         \midrule
%         \multirow{8}{*}{Llama3-8B} & Inference & - & 35.2 & 30.2 & 23.8 & 33.9 & 39.8 & 25.4 & 33.4 & 21.9 & 17.2 & \\
%         & \starName & - & 40.4 & 44.9 & 35.0 & 11.5 & 21.0 & 10.1 & 20.9 & 25.9 & 19.0 \\
%         & \extracttrain & train & 47.9 & 41.7 & 37.8 & 40.5 & 45.6 & 31.5 & 33.3 & 26.9 & 21.4 \\
%         & \modelName (ours) & train & 51.4 & 54.9 & 47.4 & 37.1 & 44.6 & 31.5 & 25.8 & 25.5 & 22.0 \\
%         & \quad + Target SFT & train &  \\
%         & \modelName{} (ours) & external & & & & 40.2 & 47.8 & 32.9 \\
%         & \quad + Target SFT & external & \\
%         \midrule
%         \multirow{6}{*}{Llama3-70B} & Inference & - & 48.0 & 46.9 & 41.3 & 47.8 & 46.9 & 35.6 & 34.6 & 34.2 & 28.2 \\
%         & \starName & - &  47.5 & 50.0 & 42.3 & 15.7 & 13.8 & 18.3 & 23.1 & 23.3 & 16.9 \\
%         & \extracttrain & train & 55.5 & 53.2 & 48.0 & 49.5 & 52.8 & 39.6 & 36.1 & 36.2 & 29.4 \\
%         & \modelName{} (ours) & train & 54.7 & 53.9 & 49.8 & a6-1 & 36.7 & 26.5 & 27.8 & 29.2 & 22.4 \\
%         & \modelName{} (ours) & external &  \\
%         \midrule
%         \multirow{6}{*}{GPT-3.5} & Inference & - & 32.6 & 33.0 & 26.2 & 43.2 & 44.2 & 32.9 & 33.3 & 31.2 & 24.7 \\
%         & \starName & - & 33.4 & 39.0 & 29.6 & 17.0 & 21.3 & 14.6 & 17.7 & 21.8 & 14.3 \\
%         & \extracttrain & train & 52.4 & 49.7 & 44.6 & 46.9 & 51.7 & 38.2 & 37.7 & 38.7 & 31.1 \\
%         & \modelName{} (ours) & train & 48.1 & 49.2 & 43.6 & 29.4 & 35.4 & 25.2 & 26.4 & 28.6 & 21.6 \\
%         & \modelName{} (ours) & external & a6-2 & & & 25.6 & 32.6 & 23.1 & 20.4 & 23.8 & 16.9 \\
%         \midrule
%         % \multicolumn{2}{l}{Gold Training data SFT of DEGREE} & \\
%         BART-large & DEGREE (gold) & - & 74.8 & 73.7 & 71.6 & 63.7 & 68.4 & 58.2 & 67.4 & 70.4 & 64.8 \\
%         \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:main-results}
% \end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/tri-c_3data.pdf}
    \caption{Few-shot results comparing \modelName{} with other baselines across three datasets using Llama3-8B-Instruct as the base LLM. Except for Inference, all other evaluations are performances of downstream DEGREE \cite{hsu-etal-2022-degree} model trained on data generated by each technique. Tri-C: Trigger Classification F1, \#: Number of.}
    \label{fig:few-shot_tri-c_3data}
\end{figure*}

\section{Results}
\label{sec:zero-shot-results}

We present the results and insights for our zero-shot and few-shot settings below.

\subsection{Zero-shot Results}

We present the main results comparing the various methods across different LLMs and datasets in Table~\ref{tab:main-results}.
For \modelName, we show results using both the \textit{train} data as well as the \textit{external} data as the external data source.
We highlight some of our main findings below.

\paragraph{Forward generation > Inverse generation:}
Although \starName{} performs decently for ACE, its performances for SPEED and GENIA are quite poor (even below the Inference baseline).
Since SPEED (social media) and GENIA (biomedical) are more domain-specific than ACE (news), we attribute this poor performance to \starName's inability to generate domain-specific data instances (further validated in \S~\ref{sec:analysis-trigger-quality}).
On the other hand, \extracttrain{} outperforms \starName{}, providing average gains of 13\% Tri-C F1, thus establishing the superiority of forward generation-based synthetic data.
% \paragraph{Pure DG shows poor results on other domains. Also commenting on the difficulty of different datasets}

\paragraph{\modelName{} performs the best:}
Our hybrid approach combines the benefits from both forward and inverse generation and provides the best performance.
On average, \modelName{} beats \starName{} by 17.3\% Eve-I F1 and 16.3\% Tri-C F1 points - proving how domain-specific cues from unlabeled data can help generate better-aligned inverse generated data.
Compared to forward generation \extracttrain{} baseline, \modelName{} provides average gains of 3.6\% Eve-I F1 and 3.3\% Tri-C F1 - suggesting how data diversity and cleaner label quality can help improve model performance.

\paragraph{External data source is effective:}
Assuming access to the training data as the unlabeled data source can be a strong assumption and bias for \modelName.
To cross-validate this assumption's impact, we also consider \modelName{} with external data sources.
Surprisingly, as seen from Table~\ref{tab:main-results}, \modelName{} with external data provides similar gains of 4\% Eve-I F1 and 3.4\% Tri-C F1 over the best baseline.
This demonstrates that instead of getting biased by external data, \modelName{} utilizes the useful domain-specific signals (in the form of extracted triggers) from the external data and provides similar/better gains as with using the training data.

\paragraph{Generation ability doesn't scale up as reasoning ability:}
Although \modelName{} performs better across all LLMs, the gains are much higher for Llama3-8B model relative to the Llama3-70B or GPT3.5 models.
Comparing the improvements in model performance when scaling up from Llama3-8B to LLama3-70B, inverse generation-centric methods (\starName, \modelName) improve by an average of 2.5\% F1 points while reasoning-centric methods (Inference, \extracttrain) improve considerably better by an average of 10.8\% F1 points.
A similar disparity in performance improvements is observed for GPT3.5 as well.
This sheds light on how the generation capabilities of LLMs do not scale compared to reasoning capabilities.
Simultaneously, it shows that \modelName{} is particularly effective when used with smaller LLMs whose the reasoning capability are poor.

% \begin{table}[t]
%     \centering
%     \small
%     \begin{tabular}{lccc}
%         \toprule
%         \textbf{Model/Method} & \textbf{Tri-I} & \textbf{Eve-I} & \textbf{Tri-C} \\
%         \midrule
%         \multicolumn{4}{c}{ACE} \\
%         \midrule
%         \modelName{} + train & 50.6 & 52.5 & 47.5 \\
%         \modelName{} + ext & 44.4 & 49.5 & 40.6 \\
%         \extracttrain{} + train & 59.0 & 55.5 & 51.5 \\
%         \extracttrain{} + ext & 59.1 & 58.4 & 52.8 \\
%         \midrule
%         \multicolumn{4}{c}{SPEED} \\
%         \midrule
%         \modelName{} + train & 22.9 & 34.0 & 19.8 \\
%         \modelName{} + ext1 & 22.9 & 33.5 & 20.2 \\
%         \modelName{} + ext2 & 24.8 & 36.5 & 21.3 \\
%         \extracttrain{} + train & & \\
%         \extracttrain{} + ext1 & 43.7 & 45.2 & 36.8 \\
%         \extracttrain{} + ext2 & 41.6 & 45.6 & 36.9\\
%         \midrule
%         \multicolumn{4}{c}{GENIA} \\
%         \midrule
%         \modelName{} + train & 20.3 & 22.3 & 16.3 \\
%         \modelName{} + ext & 24.1 & 23.3 & 19.0 \\
%         \extracttrain{} + train & 34.9 & 32.6 & 25.2 \\
%         \extracttrain{} + ext &  \\
%         \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:general-data-source-results}
% \end{table}

\subsection{Few-shot Results}

We also study the various methods in the presence of small annotated data as part of our few-shot experiments.
Specifically, we study the $k=2$ and $k=5$ few-shot settings, where $k$ annotated examples per event type are utilized.
Majorly, we utilize the $k$ shots as in-context examples in the LLM prompts where applicable and append these few-shot examples to the synthesized training data as well.
Additionally, we consider another baseline (Supervised) of downstream models trained only on the $k$ shot examples.
We present the Tri-C results for all the datasets for the Llama3-8B model in Figure~\ref{fig:few-shot_tri-c_3data} and summarize our major findings below.

\paragraph{\modelName{} consistently performs the best:}
Similar to zero-shot results, we observe that \modelName{} consistently beats all other baseline models.
On an average, \modelName{} outperforms \starName{} and \extracttrain{} by 5.4\% Tri-C F1 and 7\% Tri-C F1 respectively.

\paragraph{Generation abilities improve significantly:}
Contrary to our findings of small improvements when scaling up LLM model size in \S~\ref{sec:zero-shot-results}, we observe that inverse generation-centric methods (\starName, \modelName{}) consistently improve as we increase the number of shots.
Contrastingly, reasoning-centric models stagnate and don't improve as much.
This highlights how inverse generation becomes particularly effective in the presence of few exemplars.

