\documentclass{article}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{arxiv}
\usepackage{graphicx}   % For including images
\usepackage{hyperref}   % For PDF metadata and hyperlinks
\usepackage{ifpdf}      % To handle both LaTeX and PDFLaTeX
\usepackage{caption}    % Better figure captions
% \usepackage[a4paper,margin=1in]{geometry}  % Adjust page layout
\usepackage{float}      % To control figure placement
\usepackage{amsmath}    % For cases environment and math alignment
\usepackage{booktabs}

\usepackage{pgfplots}
\usepackage{csvsimple}
\usepackage{subcaption}

% Set PDFLaTeX output flag
\ifpdf
    \pdfoutput=1
\fi

\title{Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models}

\author{
Amartaivan Sanjjamts\\
Department of Information and Physical Sciences\\
Graduate School of Information Science and Technology\\
Osaka University, Osaka, Japan\\
\texttt{s.amaru@ist.osaka-u.ac.jp}\\
\And
Morita Hiroshi\\
Department of Information and Physical Sciences\\
Graduate School of Information Science and Technology\\
Osaka University, Osaka, Japan\\
\texttt{morita@ist.osaka-u.ac.jp}\\
\And
Enkhtogtokh Togootogtokh\\
Department of Research and Development\\
Voizzr Technologies\\
Bavaria, Germany\\
\texttt{enkhtogtokh.java@gmail.com}\\
}


\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Understanding collective pedestrian movement is crucial for applications in crowd management, autonomous navigation, and human-robot interaction. This paper investigates the use of sequential deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers, for real-time flock detection in multi-pedestrian trajectories. Our proposed approach consists of a two-stage process: first, a pre-trained binary classification model is used for pairwise trajectory classification, and second, the learned representations are applied to identify multi-agent flocks dynamically. 

We validate our method using real-world group movement datasets, demonstrating its robustness across varying sequence lengths and diverse movement patterns. Experimental results indicate that our model consistently detects pedestrian flocks with high accuracy and stability, even in dynamic and noisy environments. Furthermore, we extend our approach to identify other forms of collective motion, such as convoys and swarms, paving the way for more comprehensive multi-agent behavior analysis. 

\end{abstract}


\section{Introduction}

The analysis of pedestrian trajectories has become an essential aspect of understanding human mobility patterns in various environments such as urban spaces, transportation systems, and public gatherings. In particular, the identification of pedestrian groups or "flocks" moving together in real-time is a challenging but crucial task. A flock can be defined as a group of individuals whose movements are highly correlated over time, often indicating a shared goal or destination. Detecting such flocks is not only important for crowd management and safety but also for enhancing the effectiveness of autonomous systems, such as self-driving cars, and improving human-robot interaction.

Collective motion in trajectory data can be categorized into different formats, including \textbf{flocks}, \textbf{convoys}, and \textbf{swarms} \cite{wang2020big}. A \textbf{flock} is a set of agents moving together within a limited spatial region over a specific time interval. A \textbf{convoy} extends this definition by maintaining the same group structure over longer periods, making it more stable in dynamic environments. A \textbf{swarm} represents a more loosely connected group, where individuals exhibit similar movement patterns but do not necessarily maintain fixed spatial relationships. In this study, we focus on \textbf{moving flock detection}, where groups of pedestrians dynamically form and dissolve while moving together over short time intervals.

Traditional approaches to flock detection have largely relied on density-based methods, such as clustering algorithms and graph-based representations, to identify groupings based on spatial proximity or movement patterns. While effective in some scenarios, these methods often struggle in dynamic environments with changing densities, irregular movement, or occlusions.

In contrast, modern data-driven approaches, particularly deep learning models, offer more flexibility and robustness in detecting real-time flocks. Techniques such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers have shown great promise in capturing the temporal dependencies in trajectory data, making them well-suited for flock detection in dynamic, multi-agent settings. These models can learn complex patterns from raw trajectory data and make predictions on the likelihood of group formation, even in challenging environments.

This paper investigates the use of deep learning models for real-time flock detection in multiple pedestrian trajectories, comparing the performance of modern sequential deep learning models like RNNs, LSTMs, and Transformers for pair detection and applying the pre-trained model to multiple trajectories to identify flock size and members.


\section{Literature Review}

Understanding collective pedestrian movement has been an active area of research, with various methodologies proposed to detect and analyze groups of individuals moving together. These approaches can broadly be categorized into traditional model-based methods and modern data-driven techniques, with recent advancements exploring hybrid strategies to improve detection accuracy.

\subsection{Traditional Approaches}

Early research in flock detection primarily relied on model-based and clustering techniques. One of the most widely used methods is Density-Based Spatial Clustering of Applications with Noise (DBSCAN), which identifies pedestrian groups based on spatial proximity and density \cite{ester1996density}. While effective in many cases, DBSCAN and other density-based techniques face challenges in dynamic environments where pedestrian group sizes and movement patterns change over time.

Graph-based methods have also been extensively used, where pedestrians are represented as nodes and edges denote spatial or temporal interactions \cite{helbing2000social}. These methods often employ social force models to simulate pedestrian dynamics, considering factors like inter-agent distance and collision avoidance. While useful for understanding local interactions, such approaches struggle with long-range dependencies and occlusions, limiting their applicability in real-world, large-scale crowd scenarios.

Another line of research involves rule-based and statistical models, such as Hidden Markov Models (HMMs) and Kalman filters, to estimate pedestrian movement patterns. These models typically assume linear motion patterns and rely on handcrafted features, making them less adaptable to complex and unstructured pedestrian flows \cite{li2008learning}. 

\subsection{Modern Data-Driven Approaches}

With the rise of deep learning, trajectory-based modeling has shifted towards data-driven methods that learn patterns directly from large-scale pedestrian datasets. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks have been widely used for pedestrian trajectory prediction and group detection due to their ability to capture sequential dependencies \cite{zhang2016deep}. These models have demonstrated improved accuracy in detecting pedestrian flocks by learning motion tendencies over time.

More recently, Transformer-based architectures have emerged as state-of-the-art models in sequence learning tasks. Unlike RNNs and LSTMs, Transformers rely on self-attention mechanisms, allowing them to model long-range dependencies more effectively without suffering from vanishing gradient issues \cite{vaswani2017attention}. This has led to improved performance in large-scale pedestrian behavior analysis, where long-term dependencies and complex interactions are prevalent.

Additionally, attention-based models, such as Social-LSTM and Social-GAN, have been proposed to model social interactions among pedestrians, dynamically adapting their predictions based on surrounding agents \cite{gupta2018social}. These models leverage learned representations to classify pedestrian groups, offering a more flexible alternative to traditional rule-based approaches.

\subsection{Hybrid Approaches}

Recent studies have explored the integration of traditional and deep learning-based methods to enhance flock detection capabilities. Hybrid models, such as DBSCAN combined with LSTMs, have been used to detect groups first through clustering and then refine predictions using sequential learning models \cite{jia2020trajectory}. Similarly, Graph Neural Networks (GNNs) have been applied to pedestrian movement analysis, capturing both spatial and temporal dependencies by integrating graph structures with deep learning architectures \cite{zhang2020graph}.

The combination of these approaches has demonstrated promising results, particularly in dynamic and large-scale scenarios where traditional clustering methods alone may fail. By leveraging both spatial clustering techniques and deep sequence modeling, hybrid models provide a robust framework for real-time multi-agent flock detection.



\section{Methodology}

In this study, we leverage sequential deep learning models to classify trajectory pairs as either forming a flock or not. The task is based on the premise that identifying patterns of movement within groups of pedestrians can offer valuable insights into pedestrian dynamics, such as crowd behavior, space utilization, and evacuation strategies. The problem is framed as a binary classification task, where each trajectory pair is classified as belonging to a flock (pair) or not (non-pair).

To achieve this, we employ a variety of sequential deep learning models. Specifically, we focus on architectures such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based models. Each of these models is trained to predict pairwise relationships between pedestrians based on their movement characteristics. The core idea is to capture temporal dependencies in the movement data, allowing the models to understand how pairs of pedestrians interact over time and how these interactions evolve.

\begin{figure}[H] 
    \centering
    \ifpdf
        \includegraphics[width=0.75\textwidth]{figure1.jpg}  % Use PDFLaTeX-compatible formats
    \fi
    \caption{Proposed Method Overview}
    \label{fig:proposed_method}
\end{figure}

As shown in Figure \ref{fig:proposed_method}, our proposed method consists of the following steps, each contributing to a comprehensive approach for accurate flock detection:

\begin{itemize} \item \textbf{Binary Classification:} We train a deep learning model specifically designed to predict whether a given pair of pedestrians belongs to a flock or not. This model is trained using trajectory data, where each pedestrian's movement features such as position, velocity, motion angle, and inter-agent distance are considered to make the classification decision. The binary classification framework allows the model to focus on detecting pairwise interactions, essential for identifying potential group behaviors. \end{itemize}

\begin{itemize} \item \textbf{Pairwise Evaluation:} Once the model is trained, it is applied across the entire dataset, evaluating each possible pair of pedestrians. By performing pairwise classification, we assess the temporal and spatial interactions between all pedestrians in the scene. This step is crucial because it helps identify potential pairings that might form a larger group or flock. It effectively transforms the problem of group detection into multiple binary decisions, enabling the model to efficiently handle dynamic pedestrian environments with varying numbers of individuals. \end{itemize}

\begin{itemize} \item \textbf{Flock Filtering:} After the pairwise evaluations are completed, a filtering mechanism is applied to refine the detected flocks. This process involves using predefined criteria such as a minimum flock size (e.g., a minimum of two pedestrians) and temporal consistency (e.g., ensuring that pairs that are detected as a flock maintain their relationship over time). Flock filtering ensures that the detected groups represent meaningful pedestrian formations rather than isolated or spurious pairings. The filtering mechanism also helps in removing false positives and reduces the likelihood of detecting non-flocking pairs, thereby improving the accuracy of the model's output. \end{itemize}

\begin{itemize} \item \textbf{Flock Aggregation:} After filtering, the final step involves aggregating the results of pairwise evaluations into larger flock formations. By clustering pairs that are consistently detected as belonging to the same flock, we can estimate the size and structure of the group. This aggregation step is key in translating the pairwise predictions into actionable insights, allowing us to identify the overall structure of pedestrian groups, such as convoys, clusters, or swarms, in the scene. The resulting groups are then analyzed for their behavior, density, and movement patterns. \end{itemize}

\subsection{Binary Classification Problem}

We formulate real-time flock detection as a binary classification problem. Given a pair of pedestrian trajectories, our goal is to determine whether the two pedestrians form a flock at a given time.  

Let $\mathbf{T}_i = \{(x_t^i, y_t^i) \mid t = 1, \dots, T\}$ and $\mathbf{T}_j = \{(x_t^j, y_t^j) \mid t = 1, \dots, T\}$ represent the trajectories of pedestrians $i$ and $j$, where $(x_t, y_t)$ denotes the position of a pedestrian at timestamp $t$.  

We define a binary function $\mathcal{F}$ to classify whether the pair $(i, j)$ belongs to the same flock:  

\begin{equation}
    \mathcal{F}(\mathbf{T}_i, \mathbf{T}_j) =
    \begin{cases}
      1, & \text{if } (i, j) \text{ belong to the same flock}, \\
      0, & \text{otherwise}.
    \end{cases}
\end{equation}

To learn this function, we train a deep learning model $\mathcal{M}_{\theta}$ with parameters $\theta$ such that:  

\begin{equation}
    \hat{y}_{i,j} = \mathcal{M}_{\theta}(\mathbf{T}_i, \mathbf{T}_j),
\end{equation}

where $\hat{y}_{i,j} \in \{0,1\}$ represents the predicted label indicating whether the pair belongs to a flock.
\subsection{Group Data Preparation}\label{Group_Data_Preperation}


To analyze pedestrian flocking behavior, we preprocess trajectory data by classifying trajectories based on their starting points under the assumption that pedestrians who start moving at similar times may belong to the same flock. The preparation process consists of two main steps: time-bin classification and sequence data selection.

\subsubsection{Time-Bin Classification}
First, the function read the preprocessed pedestrian trajectory data, which includes fields for timestamp, agent ID, position $(X,Y)$, velocity, motion angle, and face angle. To ensure uniform timestamp representation, the timestamps are converted into datetime format and adjusted to the appropriate time zone.

Next, we filter out agents with fewer trajectory points than the required sequence length ($L$). For each remaining agent, we identify the first recorded trajectory point (i.e., the minimum timestamp for each agent). These first trajectory points are then grouped into discrete time bins of size $T$, using:

\begin{equation}
\text{time\_bin} = \left\lfloor \frac{\text{TIMESTAMP} - \text{min}(\text{TIMESTAMP})}{T} \right\rfloor.
\end{equation}

Each bin represents a cohort of agents that started moving within the same time interval. The number of agents in each bin is recorded, and bins with zero agents are excluded. The resulting groups of agent IDs are stored in a file for further processing.

\subsubsection{Sequential Data Selection}
Once the agents are grouped into time bins, the trajectory data for each agent is extracted to form sequences of length $L$. For each agent, we ensure that a sufficient number of consecutive data points exist to meet the sequence length requirement. The data is then sorted by timestamp and agent ID.

The trajectory sequences for all agents within a given time bin are structured into blocks, where each block contains multiple agents' trajectories for a shared time interval. These structured blocks of sequential data are then exported as scene data for further analysis.


This method enables the identification of potential pedestrian flocks by examining agents who start moving at similar times, facilitating a deeper analysis of collective movement behaviors.


\subsection{Multi-agent flock identification}\label{multi-agent}

To identify multi-agent flocks from pairwise classification results, we employ a union-find-based clustering approach. This method efficiently groups pedestrians into cohesive flocks based on their classified pairwise relationships. The algorithm processes an input list of pedestrian pairs that have been determined to be part of the same flock and iteratively merges them into larger clusters. By leveraging the union-find data structure, we ensure that the clustering process remains computationally efficient, even for large-scale pedestrian datasets. The final output is a set of detected flocks, where each flock represents a cohesive group of individuals moving together within the observed environment.

\subsubsection{Input and Output}

\begin{itemize}
    \item \textbf{Input:} A list of tuples, where each tuple contains:
    \begin{itemize}
        \item A pair of pedestrian identifiers $(p_1, p_2)$.
        \item A binary classification result: 1 if the pair belongs to the same flock, 0 otherwise.
    \end{itemize}
    \item \textbf{Output:} A list of detected flocks, each represented as:
    \begin{itemize}
        \item The number of pedestrians in the flock.
        \item A sorted list of pedestrian identifiers forming the flock.
    \end{itemize}
\end{itemize}

\subsubsection{Algorithm Description}

The algorithm operates as follows:

\begin{enumerate}
    \item \textbf{Union-Find Initialization:} Each pedestrian is initially considered its own flock.
    \item \textbf{Pairwise Flock Merging:} For each detected flock pair:
    \begin{enumerate}
        \item Find the representative (root) of each pedestrian's flock using the \textit{find\_root} function.
        \item If the pedestrians belong to different flocks, merge them by updating their root.
    \end{enumerate}
    \item \textbf{Flock Organization:} After processing all pairs, pedestrians belonging to the same root are grouped into final flock structures.
    \item \textbf{Output Generation:} The function returns a list where each detected flock is represented by its size and the sorted list of pedestrian identifiers.
\end{enumerate}

\subsubsection{Union-Find with Path Compression}

To efficiently manage flock merging, we implement a union-find data structure with path compression. Given a pedestrian $p$, the function \texttt{find\_root(p)} recursively finds the root representative:

\begin{equation}
    \text{flocks}[p] =
    \begin{cases}
      p, & \text{if } p \text{ is its own root}, \\
      \text{find\_root}(\text{flocks}[p]), & \text{otherwise}.
    \end{cases}
\end{equation}

Path compression ensures that subsequent searches for the same pedestrian are efficient, reducing the overall complexity of the algorithm to nearly $\mathcal{O}(N)$ in practice.



\section{Experiments and Results}

We compare RNN, LSTM, and Transformer models in terms of accuracy and performance.

\subsection{Data and Model Preperation}

\subsubsection{Dataset}

We use a real-world pedestrian group-identified trajectory dataset \cite{zanlungo2015pedestrian} from an indoor environment recorded at the Asian Trade Center in Osaka, Japan.

Each row in the CSV file corresponds to a single tracked individual at a specific time instant and includes the following fields: \textit{time [ms] (Unix time + milliseconds/1000)}, \textit{person ID}, \textit{position x [mm]}, \textit{position y [mm]}, \textit{velocity [mm/s]}, \textit{angle of motion [rad]}, and \textit{facing angle [rad]}.


The group files contain annotations for the groups present on a given day. Only pedestrians in groups are listed, while those walking alone are excluded. Each row represents a single tracked pedestrian within a group and includes the following fields (space-separated values): $PEDESTRIAN-ID$, \textit{$GROUP-SIZE$}, \textit{$PARTNER-ID-1$}... (list of IDs of all other pedestrians in the group), \textit{$NUMBER-OF-INTERACTING-PARTNERS$}, and \textit{$INTERACTING-PARTNER-ID-1$}... (list of all socially interacting partners).

For the binary classification model training, we select only data where \textit{$GROUP-SIZE$} = 2, focusing on pairs of pedestrians from a single day. The pre-trained model is then tested on data from different days, ensuring that the model is evaluated on entirely unseen data.

\subsubsection*{Addressing Class Imbalance}

One challenge in training the binary classification model is class imbalance, as natural pedestrian movement data may contain significantly more non-flocking samples than flocking ones. To mitigate this issue, we apply a combination of:

Weighted Loss Functions: Assigning higher penalties to misclassified minority class samples.
Oversampling \& Undersampling: Augmenting minority class sequences and reducing redundant majority class samples.
Synthetic Data Generation: Creating additional flocking sequences by interpolating existing trajectories.
By employing these techniques, we ensure that the model does not develop a bias toward the majority class and maintains high predictive accuracy across both categories.
We split the dataset into training and testing sets with an 80/20 ratio. Depending on the sequence length (the number of records in a single sample), the number of samples vary.


\begin{table}[H]
   
    \label{tab:data_summary}
    \centering
    
    \begin{tabular}{@{}cccc@{}} 
     \toprule
     \textbf{Sequence Length} & \textbf{\# of total samples} & 
     \textbf{\# of training samples} & \textbf{\# of excluded agents} \\
     \midrule
    30 & 2794 & 2235  & 102 \\
    60 & 2758 &  2206 &  138\\
    100 & 2684 & 2147 &  212\\
    150 & 2612 & 2090 &  284\\
    200 & 2542 & 2034 & 354\\
    300 & 2404 & 1923 & 492\\
    500 & 2022 & 1618 &  874\\
     \bottomrule
    \end{tabular}  
    \caption{Sample Sequence Length vs. Number of Samples on 2013/05/05}
    \label{tab:seq_vs_sample}
\end{table}

As shown in Table \ref{tab:seq_vs_sample}, we extract sequence lengths of 30, 60, 100, 150, 200, 300, and 500. Based on the prepared pair pedestrian IDs, we select trajectory data where the current pedestrian meets the required sequence length, with a label of '1' indicating a flock. We then randomly select two single pedestrian datasets and add them to the dataset, labeling them '0' to indicate they are not part of a flock. As a result, the \textit{\# of total samples} include both pair and non-pair data, maintaining a 50-50 ratio. 


\subsubsection{Deep Learning Model Preparation}

\subsubsection*{Feature Engineering}

In accordance with the definition of a flock, we extract features that capture spatial and behavioral interactions between pedestrians. These include the center of the pair, the inter-distance between pedestrians, and absolute differences in motion properties such as velocity, motion angle, and facing angle. Additionally, we incorporate trajectory-based similarity metrics like Dynamic Time Warping (DTW) to enhance the model's ability to distinguish between temporary co-movement and sustained flocking behavior.

To ensure robust performance, we preprocess the extracted features through normalization techniques, such as min-max scaling and standardization, preventing bias due to differences in measurement units. Furthermore, we explore additional features related to pedestrian interaction, such as relative velocity angle, trajectory curvature, and acceleration differences.

For our binary classification sequential model, each sample consists of multiple sequential data points, where each instance corresponds to a label of '1' (indicating that the pair forms a flock) or '0' (indicating that they do not). The final set of features, selected based on their predictive importance, is listed in Table \ref{tab:feature_table}

\begin{table}[H]
    
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}llp{0.6\textwidth}@{}} % Adjust column widths as needed
    \toprule
    \textbf{Input (Features)} & \textbf{Feature Name}     & \textbf{Description} \\ 
    \midrule
    1 & \texttt{centerX}           & X-coordinate of the center between the two pedestrians in the pair. \\
    2 & \texttt{centerY}     & Y-coordinate of the center between the two pedestrians in the pair. \\
    3 & \texttt{interDistance}            & Euclidean distance between the two pedestrians in the pair. \\
    4 & \texttt{timeDifference}       & Absolute difference in timestamps between the two pedestrians in the pair. \\
    5 & \texttt{velocityDifference}       & Absolute difference in velocity between the two pedestrians in the pair. \\
    6 & \texttt{motionAngleDifference} & Absolute difference in motion angles between the two pedestrians in the pair. \\    
    7  & \texttt{faceAngleDifference}     & Absolute difference in facing angles between the two pedestrians in the pair. \\
    8  & \texttt{dtwValues}     & Pair trajectory similarity metric calculated using Dynamic Time Warping (DTW). \\
    
    \bottomrule
    \textbf{Output (Label)} & \texttt{label} &  Predicted label indicating whether the pair is a flock (1) or not (0). \\ 
    \bottomrule
    \end{tabular}
    }    
    \caption{Feature Description for Binary Classification Sequential Models (Inputs and Output)}
    \label{tab:feature_table}
      
\end{table}


\subsubsection*{Binary Model Parameter Selection}

In the process of selecting the optimal hyperparameters for our binary classification sequential model, we primarily draw from established best practices in the field. The model's hyperparameters, including batch size and hidden layer sizes, play a critical role in ensuring effective learning and generalization. We initially follow guidelines found in key research papers, which focus on the importance of carefully tuning these parameters to achieve robust performance.

For batch sizes and hidden layer sizes, we refer to studies such as \cite{deepsurrogate} and \cite{batchsizepaper}, which explore the impact of various batch sizes on model stability and convergence rates. Another important paper is \cite{hiddenlayers}, which discusses the effects of hidden layer sizes and their impact on model expressiveness and training efficiency.

To select the best-performing hyperparameters, we conduct tests across different combinations of batch sizes and hidden layer sizes. Specifically, we test the following combinations:
\begin{itemize}
    \item \textbf{Batch Sizes}: [64, 32, 16, 8]
    \item \textbf{Hidden Layer Sizes}: [256, 128, 64, 32, 16]
\end{itemize}

These combinations are chosen to balance the trade-off between computational efficiency and model performance. Larger batch sizes may speed up training but could lead to poorer generalization, while smaller batch sizes might improve generalization at the cost of longer training times. Similarly, different hidden layer sizes can affect the model's ability to capture complex patterns in the data while maintaining computational feasibility.

The result sections will describe the results from conducting experiments with these parameter combinations, comparing their performance based on validation accuracy and other relevant metrics.


\subsubsection{Multi-agent group data preparation}

As described in Section \ref{Group_Data_Preperation}, we prepare timestamp bin data by aligning the starting point of each pedestrian within the current timestamp range. 


\begin{figure}[H] 
    \centering
    \ifpdf
        \includegraphics[width=0.75\textwidth]{figure2.jpg}  % Use PDFLaTeX-compatible formats
    \fi
    \caption{Sample Member Count for Each Bin on 2013/05/05}
    \label{fig:member_count_chart}
\end{figure}

The bar chart shown in Figure \ref{fig:member_count_chart} visualizes the \textbf{member count} across distinct \textbf{time ranges} from the dataset. The data is grouped into four primary time periods that represent significant hours of pedestrian activity, as described in \url{https://dil.atr.jp/sets/groups/}:

\begin{itemize} \item 10:00 - 11:00 \item 12:00 - 13:00 \item 15:00 - 16:00 \item 19:00 - 20:00 \end{itemize}

Each time range is subdivided into 1-minute intervals, with the height of the bars indicating the number of pedestrians observed in each interval. The distribution of pedestrians across these time ranges allows for a more detailed analysis of pedestrian activity within each hour. Given that the sequence length used in the analysis was 100, the data is processed in 1-minute bins to capture a finer temporal resolution of pedestrian movement.

The total number of bins in the dataset is calculated to be 255. These bins serve as the basis for further analysis, including the detection of flocking behavior and the execution of pairwise interactions, which are crucial for understanding collective pedestrian dynamics. 

Depending on the duration of the time interval and the sequence length of the sample, the number of bins and the member count in each bin may vary. In this study, we set the sequence length to 100 and the time interval to 1-minute for the experiment.

\subsection{Computational Environment}

The computational environment for the present study consists of a high-performance workstation equipped with an Intel(R) Core(TM) i9-13900 processor (32 CPUs), 64 GB of RAM, and a powerful 32GB NVIDIA GeForce RTX 4070 Ti GPU. The system is configured to support intensive computational tasks and is running on Python 3.12. For deep learning tasks, we leverage PyTorch, with TabNet-4.1 being employed for regression and classification tasks.

The following software libraries and frameworks were utilized in this study:

\begin{itemize}
    \item \textbf{Deep Learning Models}: We implemented various models using \textit{PyTorch}, including Recurrent Neural Networks (RNN), Long Short-Term Memory networks (LSTM), and Transformer-based models. Key hyperparameters include \texttt{learning\_rate} = 0.001 and \texttt{num\_epochs} = 1000.
\end{itemize}

\begin{itemize}
    \item \textbf{Similarity Measurement}: For calculating the similarity between trajectories, we utilized the \textit{fastdtw} package, which efficiently computes the Euclidean distance between time-series data.
\end{itemize}

\begin{itemize}
    \item \textbf{Visualization}: We used the \textit{Matplotlib} library for high-quality visualizations of the flocking patterns, allowing for clear and efficient presentation of the agent trajectories and group dynamics.
\end{itemize}


\subsection{Main Results}

In this section, we present the results of the binary classification model by evaluating and comparing the performance of RNN, LSTM, and Transformer models in flock detection. The experiments were conducted on multiple datasets with varying sequence lengths (i.e., number of samples) to assess the impact of sequence size on model performance. The results indicate that both runtime and classification accuracy vary significantly depending on the model architecture, even when applied to the same dataset.



\subsubsection{Binary Classification Accuracy by Data Length and Models}

The following reported accuracy values represent the \textbf{average of 20-independent runs} (combination of batch  and hidden layer size) for each model. The results highlight the increasing performance of all three models (RNN, LSTM, Transformer) as sequence length increases.


\begin{figure}[H] 
    \centering
    \ifpdf
        \includegraphics[width=0.75\textwidth]{figure4.jpg}  % Use PDFLaTeX-compatible formats
    \fi
    \caption{Sequential models accuracy for different sequences}
    \label{fig:model_accuracy}
\end{figure}

\begin{itemize}
    \item Transformer Model Performance. 
    The Transformer consistently outperforms both RNN and LSTM across all sequence lengths. Notably, at \textbf{sequence length = 100}, the Transformer model exceeds \textbf{90\% accuracy} in some runs, averaging \textbf{88.06\%}. It continues to improve with longer sequences, reaching \textbf{95.70\% at sequence length = 500}.
    \item LSTM Model Performance.
    The LSTM model demonstrates \textbf{strong performance for longer sequences}, surpassing \textbf{80\% accuracy} at \textbf{150 sequences} and achieving \textbf{86.96\% at 500 sequences}. It consistently outperforms RNN across all sequence lengths, showing its ability to better capture long-term dependencies.
    \item RNN Model Performance.
    The RNN model lags behind the LSTM and Transformer, especially for longer sequences. While it shows improvement with increased sequence length, it struggles to exceed \textbf{70\% accuracy} even at \textbf{500 sequences}, peaking at \textbf{71.43\%}.
    
    
\end{itemize}

\textbf{Key Observations}
\begin{itemize}
    \item \textbf{Longer sequences benefit all models}, with the Transformer making the most significant gains.
    \item \textbf{LSTM handles longer sequences better than RNN}, suggesting that its gating mechanisms effectively capture dependencies.
    \item \textbf{The Transformer is the best-performing model} across all tested sequence lengths, achieving over \textbf{90\% accuracy} for sequences \textbf{$\geq$ 150}.
\end{itemize}


\subsubsection{RunTime Comparison}

The reported runtime values represent the \textbf{average of 20 independent runs}, considering different combinations of batch size and hidden layer size for each model. The results highlight the increasing runtime of all three models (RNN, LSTM, Transformer) as sequence length increases. Since actual training involved early stopping, the runtime could vary across runs. To ensure a fair comparison of model training efficiency, we set the number of epochs to 1000 for all training instances. 

\begin{figure}[H] 
    \centering
    \ifpdf
        \includegraphics[width=0.75\textwidth]{figure5.jpg}  % Use PDFLaTeX-compatible formats
    \fi
    \caption{Model training time for different sequences}
    \label{fig:model_training_runtime}
\end{figure}

The runtime comparison graph indicates that the RNN model has the shortest training time, making it the fastest among the three models due to its simpler structure. In contrast, the LSTM model has the longest training time across all sequence lengths, likely due to its more complex memory mechanisms.

The training time also varies with sequence length. Interestingly, the fastest training occurred at sequence lengths 100 and 300, while sequence length 150 took the longest to train. This suggests that certain sequence lengths may introduce computational inefficiencies.

\textbf{Key Observations}
\begin{itemize}
    \item RNN is the fastest model across all sequence lengths due to its simpler architecture.
    \item LSTM has the longest training time, peaking at 57.1 minutes for sequence length 150.
    \item Transformer runtimes closely follow LSTM but are slightly shorter in most cases.
    \item Sequence lengths 100 and 300 resulted in the shortest runtimes across all models, while 150 took the longest to train.

\end{itemize}

These results highlight the trade-offs between model complexity and training efficiency. While LSTM and Transformers are more powerful for sequence learning, they require significantly longer training times than RNNs. Choosing the right sequence length can also impact training efficiency, with some lengths (e.g., 100, 300) being computationally favorable.

\subsubsection{Comparison of Accuracy and Runtime Across Models}

\begin{figure}[H] 
    \centering
    \ifpdf
        \includegraphics[width=0.75\textwidth]{figure3.jpg}  % Use PDFLaTeX-compatible formats
    \fi
    \caption{Model training time for different sequences}
    \label{fig:model_training_accuracy_runtime}
\end{figure}

The figure illustrates the trade-off between \textbf{accuracy (\%)} and \textbf{runtime (minutes)} for RNN, LSTM, and Transformer models across different sequence lengths.

\begin{itemize}
    \item \textbf{Accuracy:} Transformer achieves the highest accuracy, followed by LSTM, while RNN performs the worst.
    \item \textbf{Runtime:} RNN is the fastest, while LSTM takes the longest time. Transformer is slightly faster than LSTM.
    \item \textbf{Sequence Length:} Training is more efficient at sequence lengths \textbf{100 and 300}, while \textbf{150 takes the longest}.
    \item \textbf{Key Trade-off:} RNN is best for speed, while Transformer offers a better balance between accuracy and runtime.
\end{itemize}


\subsubsection{Sample Screens for pair prediction}

Below are sample pair detection results for a sequence length of 100. The model accurately predicts complex trajectories as pairs in many cases. However, in some instances, it may incorrectly detect two individual pedestrians moving in similar directions as a pair.

\begin{figure}[H]
    \centering
    % First row of images
    \ifpdf
        \begin{subfigure}[c]{0.45\textwidth} 
            \centering
            \includegraphics[width=\textwidth]{figure6.jpg}
            \caption{The pair prediction result for straight trajectories.}
            \label{fig:pair1}
        \end{subfigure}
    \fi
    \hfill
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure7.jpg}
        \caption{The pair prediction result for complex trajectories.}
        \label{fig:pair2}
    \end{subfigure}

    \caption{Sample pair detection results for a sequence length of 100.}
    \label{fig:pair_prediction}
\end{figure}

\subsubsection{Flock Validation}

We validate our flock detection algorithm using the group files described in \cite{zanlungo2015pedestrian}. These group files contain group-labeled data for 6 days. After performing data preprocessing and cleaning, we found that most flocks have sizes of 2, 3, at the most between 11 and 12 across all the group files. We validated our method by training the model on data from different days, introducing unseen data to assess the model's generalization ability.

As we described in \ref{multi-agent} our flock detection algorithm iterates through all combinations to predict pairs and then filters the results to determine the flock size based on pair predictions. 

We further describe an example of flock validation results for the groups on 2013/02/17, using a pre-trained Transformer model with a sequence length = 100, batch size = 32, hidden layer size=64 and model accuracy 91.35\%. We set the confidence score for pair prediction to 0.9. As a result, all the flocks were correctly detected, with the following "group size":"count" distribution:

\textit{{"2": 1528, "4": 143, "3": 448, "5": 36, "7": 4, "6": 6, "11": 1, "9": 1, "8": 2}}


\subsubsection{Sample Screens for flock detection}

\begin{figure}[H]
    \centering
    % First row of images
    \ifpdf
        \begin{subfigure}[c]{0.45\textwidth} 
            \centering
            \includegraphics[width=\textwidth]{figure8.jpg}
            \caption{Detected flock size with 2.}
            \label{fig:flock2}
        \end{subfigure}
    \fi
    \hfill
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure9.jpg}
        \caption{Detected flock size with 3.}
        \label{fig:flock3}
    \end{subfigure}

    % Second row of images
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure10.jpg}
        \caption{Detected flock size with 5.}
        \label{fig:flock5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure11.jpg}
        \caption{Detected flock size with 7.}
        \label{fig:flock7}
    \end{subfigure}

    \caption{Sample flock detection screen steps with 4 different sizes}
    \label{fig:flock_detection}
\end{figure}


\section{Discussion}

In this section, we discuss the results of our flock detection algorithm and highlight key observations regarding the impact of data sequence length and the dependency of features used in the model.

\subsection*{Impact of Data Sequence Length}
The performance of our model demonstrates that using a sequence length of 100 steps results in over 90\% accuracy for pair detection with a set of 8 features. While this sequence length yields high accuracy, it is worth noting that 100 steps might seem relatively long from a human perspective when identifying pairs in a trajectory. Human perception of trajectories typically involves recognizing patterns and interactions over shorter durations. Despite this, our results indicate that using longer sequences provides the model with more comprehensive information about the agents' movements, which is crucial for accurately predicting pairings. However, a balance between sequence length and model performance must be considered, as excessively long sequences could lead to increased computational complexity and diminishing returns in predictive accuracy beyond a certain point.

\subsection*{Feature Dependency}
The current feature set for the binary classification model, consisting of center coordinates (centerx, centery), inter-agent distance, absolute values of the timestamp, velocity, motion angle, and face angle, has shown promising results in detecting pairs. With a total of 8 features, the model achieved high performance. Interestingly, in some instances, reducing the number of features to 6 (excluding centerx and centery) still yielded similar accuracy results. This suggests that the model can effectively detect pairs based on more dynamic features, such as inter-agent distance, velocity, and motion angles. The less significant contribution of static features like center coordinates in certain instances could inform future efforts to optimize the feature set, potentially simplifying the model without sacrificing performance. In practice, reducing the feature space could improve computational efficiency while maintaining high accuracy, especially for real-time applications.

In both of these cases, we further analyze and conduct additional experiments to explore the optimal balance between sequence length and feature set for improving model efficiency and generalization across various scenarios.

\section{Conclusion}

We proposed a robust flock detection method leveraging a pre-trained binary classification model, which excels in identifying flocks in dynamic environments with varying group sizes. One of the key advantages of our approach is its ability to handle highly dynamic scenes, where the number of groups can fluctuate over time. This flexibility is essential for accurately detecting groups under different conditions, making it well-suited for real-world applications. Our method has shown its potential in real-time flock detection, with applications spanning pedestrian movement analysis, space utilization, crowd management, and surveillance systems.

Additionally, the method demonstrates versatility by extending to more complex group behaviors, such as the identification of convoys or swarms. This enhancement is particularly valuable in scenarios where agent interactions form intricate patterns that need to be understood for effective crowd control or behavioral analysis.

Our current implementation uses a sequence length of 100, which delivers high accuracy in group detection. However, we recognize that in real-time systems, reducing the sequence length could be beneficial to enhance computational efficiency and allow for faster decision-making processes. Future work will explore ways to optimize this sequence length to achieve a balance between accuracy and efficiency. Shorter sequence lengths will enable broader applications, especially in environments that demand quick responses, such as smart city traffic management or disaster response systems.

Furthermore, the method can be enhanced by experimenting with different sequential deep learning models, such as bidirectional LSTMs or attention-based models like the Transformer. These models could potentially improve accuracy in identifying group dynamics by leveraging both past and future trajectory data. Exploring different model architectures and incorporating hybrid models could also enhance the systemâ€™s adaptability to various environments and increase its ability to generalize across unseen data. In addition to sequence length optimization, the integration of more sophisticated model structures may further contribute to the development of scalable and efficient flock detection methods.

In summary, while our current model performs effectively with the existing feature set and sequence length, further research will focus on optimizing both the model architecture and input data to ensure scalability and efficiency for a wider range of real-time applications.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
