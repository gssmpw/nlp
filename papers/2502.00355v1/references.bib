
@inproceedings{pardoux_backward_1998,
	address = {Boston, MA},
	series = {Progress in {Probability}},
	title = {Backward {Stochastic} {Differential} {Equations} and {Viscosity} {Solutions} of {Systems} of {Semilinear} {Parabolic} and {Elliptic} {PDEs} of {Second} {Order}},
	isbn = {978-1-4612-2022-0},
	doi = {10.1007/978-1-4612-2022-0_2},
	abstract = {The aim of this article is to present the theory of backward stochastic differential equations, in short BSDEs, and its connections with viscosity solutions of systems of semilinear second order partial differential equations of parabolic and elliptic type, in short PDEs. Linear BSDEs appeared long time ago, both as the equations for the adjoint process in stochastic control, as well as the model behind the Black and Scholes formula for the pricing and hedging of options in mathematical finance. These linear BSDEs can be solved more or less explicitly (see proof of Theorem 1.4).},
	language = {en},
	booktitle = {Stochastic {Analysis} and {Related} {Topics} {VI}},
	publisher = {Birkhäuser},
	author = {Pardoux, Etienne},
	editor = {Decreusefond, Laurent and Øksendal, Bernt and Gjerde, Jon and Üstünel, Ali Süleyman},
	year = {1998},
	keywords = {Elliptic PDEs, Parabolic Partial Differential Equation, Stochastic Differential Equation, Viscosity Solution, Viscosity Subsolution},
	pages = {79--127},
}

@misc{loaiza-ganem_deep_2024,
	title = {Deep {Generative} {Models} through the {Lens} of the {Manifold} {Hypothesis}: {A} {Survey} and {New} {Connections}},
	shorttitle = {Deep {Generative} {Models} through the {Lens} of the {Manifold} {Hypothesis}},
	url = {http://arxiv.org/abs/2404.02954},
	doi = {10.48550/arXiv.2404.02954},
	abstract = {In recent years there has been increased interest in understanding the interplay between deep generative models (DGMs) and the manifold hypothesis. Research in this area focuses on understanding the reasons why commonly-used DGMs succeed or fail at learning distributions supported on unknown low-dimensional manifolds, as well as developing new models explicitly designed to account for manifold-supported data. This manifold lens provides both clarity as to why some DGMs (e.g. diffusion models and some generative adversarial networks) empirically surpass others (e.g. likelihood-based models such as variational autoencoders, normalizing flows, or energy-based models) at sample generation, and guidance for devising more performant DGMs. We carry out the first survey of DGMs viewed through this lens, making two novel contributions along the way. First, we formally establish that numerical instability of likelihoods in high ambient dimensions is unavoidable when modelling data with low intrinsic dimension. We then show that DGMs on learned representations of autoencoders can be interpreted as approximately minimizing Wasserstein distance: this result, which applies to latent diffusion models, helps justify their outstanding empirical results. The manifold lens provides a rich perspective from which to understand DGMs, and we aim to make this perspective more accessible and widespread.},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Loaiza-Ganem, Gabriel and Ross, Brendan Leigh and Hosseinzadeh, Rasa and Caterini, Anthony L. and Cresswell, Jesse C.},
	month = sep,
	year = {2024},
	note = {arXiv:2404.02954 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{loaiza-ganem_deep_2024-1,
	title = {Deep {Generative} {Models} through the {Lens} of the {Manifold} {Hypothesis}: {A} {Survey} and {New} {Connections}},
	shorttitle = {Deep {Generative} {Models} through the {Lens} of the {Manifold} {Hypothesis}},
	url = {http://arxiv.org/abs/2404.02954},
	doi = {10.48550/arXiv.2404.02954},
	abstract = {In recent years there has been increased interest in understanding the interplay between deep generative models (DGMs) and the manifold hypothesis. Research in this area focuses on understanding the reasons why commonly-used DGMs succeed or fail at learning distributions supported on unknown low-dimensional manifolds, as well as developing new models explicitly designed to account for manifold-supported data. This manifold lens provides both clarity as to why some DGMs (e.g. diffusion models and some generative adversarial networks) empirically surpass others (e.g. likelihood-based models such as variational autoencoders, normalizing flows, or energy-based models) at sample generation, and guidance for devising more performant DGMs. We carry out the first survey of DGMs viewed through this lens, making two novel contributions along the way. First, we formally establish that numerical instability of likelihoods in high ambient dimensions is unavoidable when modelling data with low intrinsic dimension. We then show that DGMs on learned representations of autoencoders can be interpreted as approximately minimizing Wasserstein distance: this result, which applies to latent diffusion models, helps justify their outstanding empirical results. The manifold lens provides a rich perspective from which to understand DGMs, and we aim to make this perspective more accessible and widespread.},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Loaiza-Ganem, Gabriel and Ross, Brendan Leigh and Hosseinzadeh, Rasa and Caterini, Anthony L. and Cresswell, Jesse C.},
	month = sep,
	year = {2024},
	note = {arXiv:2404.02954 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{carlini_extracting_2021,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	isbn = {978-1-939133-24-3},
	language = {en},
	urldate = {2025-01-26},
	author = {Carlini, Nicholas and Tramèr, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Úlfar and Oprea, Alina and Raffel, Colin},
	year = {2021},
	pages = {2633--2650},
}

@inproceedings{benton_nearly_2023,
	title = {Nearly \$d\$-{Linear} {Convergence} {Bounds} for {Diffusion} {Models} via {Stochastic} {Localization}},
	url = {https://openreview.net/forum?id=r5njV3BsuD},
	abstract = {Denoising diffusions are a powerful method to generate approximate samples from high-dimensional data distributions. Recent results provide polynomial bounds on their convergence rate, assuming \$L{\textasciicircum}2\$-accurate scores. Until now, the tightest bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most \${\textbackslash}tilde O({\textbackslash}frac\{d {\textbackslash}log{\textasciicircum}2(1/{\textbackslash}delta)\}\{{\textbackslash}varepsilon{\textasciicircum}2\})\$ steps to approximate an arbitrary distribution on \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ corrupted with Gaussian noise of variance \${\textbackslash}delta\$ to within \${\textbackslash}varepsilon{\textasciicircum}2\$ in KL divergence. Our proof extends the Girsanov-based methods of previous works. We introduce a refined treatment of the error from discretizing the reverse SDE inspired by stochastic localization.},
	language = {en},
	urldate = {2025-01-25},
	author = {Benton, Joe and Bortoli, Valentin De and Doucet, Arnaud and Deligiannidis, George},
	month = oct,
	year = {2023},
}

@article{bodin_model_nodate,
	title = {Model, sample, and epoch-wise descents: exact solution of gradient ﬂow in the random feature model},
	abstract = {Recent evidence has shown the existence of a so-called double-descent and even triple-descent behavior for the generalization error of deep-learning models. This important phenomenon commonly appears in implemented neural network architectures, and also seems to emerge in epoch-wise curves during the training process. A recent line of research has highlighted that random matrix tools can be used to obtain precise analytical asymptotics of the generalization (and training) errors of the random feature model. In this contribution, we analyze the whole temporal behavior of the generalization and training errors under gradient ﬂow for the random feature model. We show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. This allows us to observe how the double and triple descents develop over time, if and when early stopping is an option, and also observe time-wise descent structures. Our techniques are based on Cauchy complex integral representations of the errors together with recent random matrix methods based on linear pencils.},
	language = {en},
	author = {Bodin, Antoine and Macris, Nicolas},
}

@misc{li_critical_2024,
	title = {Critical windows: non-asymptotic theory for feature emergence in diffusion models},
	shorttitle = {Critical windows},
	url = {http://arxiv.org/abs/2403.01633},
	doi = {10.48550/arXiv.2403.01633},
	abstract = {We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Meng et al., 2022; Choi et al., 2022; Raya \& Ambrogioni, 2023; Georgiev et al., 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of diffusion models as hierarchical samplers that progressively "decide" output features over a discrete sequence of times. We validate our bounds with synthetic experiments. Additionally, preliminary experiments on Stable Diffusion suggest critical windows may serve as a useful tool for diagnosing fairness and privacy violations in real-world diffusion models.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Li, Marvin and Chen, Sitan},
	month = may,
	year = {2024},
	note = {arXiv:2403.01633 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_critical_2024-1,
	title = {Critical windows: non-asymptotic theory for feature emergence in diffusion models},
	shorttitle = {Critical windows},
	url = {http://arxiv.org/abs/2403.01633},
	doi = {10.48550/arXiv.2403.01633},
	abstract = {We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Meng et al., 2022; Choi et al., 2022; Raya \& Ambrogioni, 2023; Georgiev et al., 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of diffusion models as hierarchical samplers that progressively "decide" output features over a discrete sequence of times. We validate our bounds with synthetic experiments. Additionally, preliminary experiments on Stable Diffusion suggest critical windows may serve as a useful tool for diagnosing fairness and privacy violations in real-world diffusion models.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Li, Marvin and Chen, Sitan},
	month = may,
	year = {2024},
	note = {arXiv:2403.01633 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_critical_2024-2,
	title = {Critical windows: non-asymptotic theory for feature emergence in diffusion models},
	shorttitle = {Critical windows},
	url = {http://arxiv.org/abs/2403.01633},
	doi = {10.48550/arXiv.2403.01633},
	abstract = {We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Meng et al., 2022; Choi et al., 2022; Raya \& Ambrogioni, 2023; Georgiev et al., 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of diffusion models as hierarchical samplers that progressively "decide" output features over a discrete sequence of times. We validate our bounds with synthetic experiments. Additionally, preliminary experiments on Stable Diffusion suggest critical windows may serve as a useful tool for diagnosing fairness and privacy violations in real-world diffusion models.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Li, Marvin and Chen, Sitan},
	month = may,
	year = {2024},
	note = {arXiv:2403.01633 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{macris_quantum_nodate,
	title = {Quantum information processing},
	author = {Macris, Nicolas},
}

@misc{cui_precise_2025,
	title = {A precise asymptotic analysis of learning diffusion models: theory and insights},
	shorttitle = {A precise asymptotic analysis of learning diffusion models},
	url = {http://arxiv.org/abs/2501.03937},
	doi = {10.48550/arXiv.2501.03937},
	abstract = {In this manuscript, we consider the problem of learning a flow or diffusion-based generative model parametrized by a two-layer auto-encoder, trained with online stochastic gradient descent, on a high-dimensional target density with an underlying low-dimensional manifold structure. We derive a tight asymptotic characterization of low-dimensional projections of the distribution of samples generated by the learned model, ascertaining in particular its dependence on the number of training samples. Building on this analysis, we discuss how mode collapse can arise, and lead to model collapse when the generative model is re-trained on generated synthetic data.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Cui, Hugo and Pehlevan, Cengiz and Lu, Yue M.},
	month = jan,
	year = {2025},
	note = {arXiv:2501.03937 [cs]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
}

@article{karras_elucidating_nodate,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
	language = {en},
	author = {Karras, Tero and Aila, Timo and Aittala, Miika and Laine, Samuli},
}

@misc{wang_evaluating_2024,
	title = {Evaluating the design space of diffusion-based generative models},
	url = {http://arxiv.org/abs/2406.12839},
	doi = {10.48550/arXiv.2406.12839},
	abstract = {Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in [Karras et al., 2022]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in [Song et al., 2021] is more preferable, but when it is less trained, the design in [Karras et al., 2022] becomes more preferable.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Wang, Yuqing and He, Ye and Tao, Molei},
	month = oct,
	year = {2024},
	note = {arXiv:2406.12839 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{bach_information_2022,
	title = {Information {Theory} with {Kernel} {Methods}},
	url = {http://arxiv.org/abs/2202.08545},
	doi = {10.48550/arXiv.2202.08545},
	abstract = {We consider the analysis of probability distributions through their associated covariance operators from reproducing kernel Hilbert spaces. We show that the von Neumann entropy and relative entropy of these operators are intimately related to the usual notions of Shannon entropy and relative entropy, and share many of their properties. They come together with efficient estimation algorithms from various oracles on the probability distributions. We also consider product spaces and show that for tensor product kernels, we can define notions of mutual information and joint entropies, which can then characterize independence perfectly, but only partially conditional independence. We finally show how these new notions of relative entropy lead to new upper-bounds on log partition functions, that can be used together with convex optimization within variational inference methods, providing a new family of probabilistic inference methods.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Bach, Francis},
	month = aug,
	year = {2022},
	note = {arXiv:2202.08545 [cs]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@book{gohberg_basic_2003,
	address = {Basel},
	title = {Basic {Classes} of {Linear} {Operators}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-7643-6930-9 978-3-0348-7980-4},
	url = {http://link.springer.com/10.1007/978-3-0348-7980-4},
	language = {en},
	urldate = {2024-12-19},
	publisher = {Birkhäuser},
	author = {Gohberg, Israel and Goldberg, Seymour and Kaashoek, Marinus A.},
	year = {2003},
	doi = {10.1007/978-3-0348-7980-4},
	keywords = {Hilbert space, Operator theory, Singular integral, functional analysis, operational calculus},
}

@incollection{gohberg_spectral_2003,
	address = {Basel},
	title = {Spectral {Theory} of {Integral} {Operators}},
	isbn = {978-3-0348-7980-4},
	url = {https://doi.org/10.1007/978-3-0348-7980-4_5},
	abstract = {Using the theory developed in Chapter IV, we now present some fundamental theorems concerning the spectral theory of compact self adjoint integral operators. In general, the spectral series representations of these operators converge in the L2-norm which is not strong enough for many applications. Therefore we prove the Hilbert-Schmidt theorem and Mercer’s theorem since each of these theorems gives conditions for a uniform convergence of the spectral decomposition of the integral operators. As a corollary of Mercer’s theorem we obtain the trace formula for positive integral operators with continuous kernel function.},
	language = {en},
	urldate = {2024-12-19},
	booktitle = {Basic {Classes} of {Linear} {Operators}},
	publisher = {Birkhäuser},
	author = {Gohberg, Israel and Goldberg, Seymour and Kaashoek, Marinus A.},
	editor = {Gohberg, Israel and Goldberg, Seymour and Kaashoek, Marinus A.},
	year = {2003},
	doi = {10.1007/978-3-0348-7980-4_5},
	pages = {193--202},
}

@article{pastur_spectra_1973,
	title = {{SPECTRA} {OF} {RANDOM} {SELF} {ADJOINT} {OPERATORS}},
	volume = {28},
	issn = {0036-0279},
	url = {https://iopscience.iop.org/article/10.1070/RM1973v028n01ABEH001396/meta},
	doi = {10.1070/RM1973v028n01ABEH001396},
	abstract = {SPECTRA OF RANDOM SELF ADJOINT OPERATORS, L A Pastur},
	language = {en},
	number = {1},
	urldate = {2024-12-15},
	journal = {Russian Mathematical Surveys},
	author = {Pastur, L. A.},
	month = feb,
	year = {1973},
	note = {Publisher: IOP Publishing},
	pages = {1},
}

@book{carmona_spectral_1990,
	address = {Boston, MA},
	title = {Spectral {Theory} of {Random} {Schrödinger} {Operators}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-8841-1 978-1-4612-4488-2},
	url = {http://link.springer.com/10.1007/978-1-4612-4488-2},
	language = {en},
	urldate = {2024-12-15},
	publisher = {Birkhäuser},
	author = {Carmona, René and Lacroix, Jean},
	year = {1990},
	doi = {10.1007/978-1-4612-4488-2},
	keywords = {Finite, Hölder condition, Identity, Smooth function, differential equation, function, operator theory, partial differential equations, proof, spectral theorem, subharmonic function, theorem},
}

@misc{mei_generalization_2021,
	title = {Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration},
	shorttitle = {Generalization error of random features and kernel methods},
	url = {http://arxiv.org/abs/2101.10588},
	doi = {10.48550/arXiv.2101.10588},
	abstract = {Consider the classical supervised learning problem: we are given data \$(y\_i,\{{\textbackslash}boldsymbol x\}\_i)\$, \$i{\textbackslash}le n\$, with \$y\_i\$ a response and \$\{{\textbackslash}boldsymbol x\}\_i{\textbackslash}in \{{\textbackslash}mathcal X\}\$ a covariates vector, and try to learn a model \$f:\{{\textbackslash}mathcal X\}{\textbackslash}to\{{\textbackslash}mathbb R\}\$ to predict future responses. Random features methods map the covariates vector \$\{{\textbackslash}boldsymbol x\}\_i\$ to a point \$\{{\textbackslash}boldsymbol {\textbackslash}phi\}(\{{\textbackslash}boldsymbol x\}\_i)\$ in a higher dimensional space \$\{{\textbackslash}mathbb R\}{\textasciicircum}N\$, via a random featurization map \$\{{\textbackslash}boldsymbol {\textbackslash}phi\}\$. We study the use of random features methods in conjunction with ridge regression in the feature space \$\{{\textbackslash}mathbb R\}{\textasciicircum}N\$. This can be viewed as a finite-dimensional approximation of kernel ridge regression (KRR), or as a stylized model for neural networks in the so called lazy training regime. We define a class of problems satisfying certain spectral conditions on the underlying kernels, and a hypercontractivity assumption on the associated eigenfunctions. These conditions are verified by classical high-dimensional examples. Under these conditions, we prove a sharp characterization of the error of random features ridge regression. In particular, we address two fundamental questions: \$(1)\${\textasciitilde}What is the generalization error of KRR? \$(2)\${\textasciitilde}How big \$N\$ should be for the random features approximation to achieve the same error as KRR? In this setting, we prove that KRR is well approximated by a projection onto the top \${\textbackslash}ell\$ eigenfunctions of the kernel, where \${\textbackslash}ell\$ depends on the sample size \$n\$. We show that the test error of random features ridge regression is dominated by its approximation error and is larger than the error of KRR as long as \$N{\textbackslash}le n{\textasciicircum}\{1-{\textbackslash}delta\}\$ for some \${\textbackslash}delta{\textgreater}0\$. We characterize this gap. For \$N{\textbackslash}ge n{\textasciicircum}\{1+{\textbackslash}delta\}\$, random features achieve the same error as the corresponding KRR, and further increasing \$N\$ does not lead to a significant change in test error.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = jan,
	year = {2021},
	note = {arXiv:2101.10588},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@techreport{caponnetto_fast_2005,
	address = {Fort Belvoir, VA},
	title = {Fast {Rates} for {Regularized} {Least}-{Squares} {Algorithm}:},
	shorttitle = {Fast {Rates} for {Regularized} {Least}-{Squares} {Algorithm}},
	url = {http://www.dtic.mil/docs/citations/ADA454989},
	language = {en},
	urldate = {2024-12-12},
	institution = {Defense Technical Information Center},
	author = {Caponnetto, Andrea and De Vito, Ernesto},
	month = apr,
	year = {2005},
	doi = {10.21236/ADA454989},
}

@techreport{de_vito_risk_2005,
	address = {Fort Belvoir, VA},
	title = {Risk {Bounds} for {Regularized} {Least}-{Squares} {Algorithm} with {Operator}-{Value} {Kernels}:},
	shorttitle = {Risk {Bounds} for {Regularized} {Least}-{Squares} {Algorithm} with {Operator}-{Value} {Kernels}},
	url = {http://www.dtic.mil/docs/citations/ADA466779},
	language = {en},
	urldate = {2024-12-12},
	institution = {Defense Technical Information Center},
	author = {De Vito, Ernesto and Caponnetto, Andrea},
	month = may,
	year = {2005},
	doi = {10.21236/ADA466779},
}

@article{caponnetto_optimal_2007,
	title = {Optimal {Rates} for the {Regularized} {Least}-{Squares} {Algorithm}},
	volume = {7},
	issn = {1615-3383},
	url = {https://doi.org/10.1007/s10208-006-0196-8},
	doi = {10.1007/s10208-006-0196-8},
	abstract = {We develop a theoretical analysis of the performance of the regularized least-square algorithm on a reproducing kernel Hilbert space in the supervised learning setting. The presented results hold in the general framework of vector-valued functions; therefore they can be applied to multitask problems. In particular, we observe that the concept of effective dimension plays a central role in the definition of a criterion for the choice of the regularization parameter as a function of the number of samples. Moreover, a complete minimax analysis of the problem is described, showing that the convergence rates obtained by regularized least-squares estimators are indeed optimal over a suitable class of priors defined by the considered kernel. Finally, we give an improved lower rate result describing worst asymptotic behavior on individual probability measures rather than over classes of priors.},
	language = {en},
	number = {3},
	urldate = {2024-12-12},
	journal = {Foundations of Computational Mathematics},
	author = {Caponnetto, A. and De Vito, E.},
	month = jul,
	year = {2007},
	keywords = {Effective Dimension, Marginal Distribution, Optimal Rate, Polish Space, Regularization Parameter},
	pages = {331--368},
}

@article{cucker_best_2002,
	title = {Best {Choices} for {Regularization} {Parameters} in {Learning} {Theory}: {On} the {Bias}—{Variance} {Problem}},
	volume = {2},
	issn = {1615-3375},
	shorttitle = {Best {Choices} for {Regularization} {Parameters} in {Learning} {Theory}},
	url = {https://doi.org/10.1007/s102080010030},
	doi = {10.1007/s102080010030},
	abstract = {Abstract.  No abstract.},
	language = {en},
	number = {4},
	urldate = {2024-12-11},
	journal = {Foundations of Computational Mathematics},
	author = {{Cucker} and {Smale}},
	month = oct,
	year = {2002},
	keywords = {AMS Classification. 68T05, 62J02.},
	pages = {413--428},
}

@article{vito_properties_nodate,
	title = {Some {Properties} of {Regularized} {Kernel} {Methods}},
	abstract = {In regularized kernel methods, the solution of a learning problem is found by minimizing functionals consisting of the sum of a data and a complexity term. In this paper we investigate some properties of a more general form of the above functionals in which the data term corresponds to the expected risk. First, we prove a quantitative version of the representer theorem holding for both regression and classiﬁcation, for both differentiable and non-differentiable loss functions, and for arbitrary offset terms. Second, we show that the case in which the offset space is non trivial corresponds to solving a standard problem of regularization in a Reproducing Kernel Hilbert Space in which the penalty term is given by a seminorm. Finally, we discuss the issues of existence and uniqueness of the solution. From the specialization of our analysis to the discrete setting it is immediate to establish a connection between the solution properties of sparsity and coefﬁcient boundedness and some properties of the loss function. For the case of Support Vector Machines for classiﬁcation, we also obtain a complete characterization of the whole method in terms of the Khun-Tucker conditions with no need to introduce the dual formulation.},
	language = {en},
	author = {Vito, Ernesto De},
}

@misc{christmann_consistency_2007,
	title = {Consistency and robustness of kernel-based regression in convex risk minimization},
	url = {http://arxiv.org/abs/0709.0626},
	doi = {10.48550/arXiv.0709.0626},
	abstract = {We investigate statistical properties for a broad class of modern kernel-based regression (KBR) methods. These kernel methods were developed during the last decade and are inspired by convex risk minimization in infinite-dimensional Hilbert spaces. One leading example is support vector regression. We first describe the relationship between the loss function \$L\$ of the KBR method and the tail of the response variable. We then establish the \$L\$-risk consistency for KBR which gives the mathematical justification for the statement that these methods are able to ``learn''. Then we consider robustness properties of such kernel methods. In particular, our results allow us to choose the loss function and the kernel to obtain computationally tractable and consistent KBR methods that have bounded influence functions. Furthermore, bounds for the bias and for the sensitivity curve, which is a finite sample version of the influence function, are developed, and the relationship between KBR and classical \$M\$ estimators is discussed.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Christmann, Andreas and Steinwart, Ingo},
	month = sep,
	year = {2007},
	note = {arXiv:0709.0626},
	keywords = {Mathematics - Statistics Theory, Statistics - Statistics Theory},
}

@misc{noauthor_reproducing_nodate,
	title = {Reproducing {Kernel} {Hilbert} {Spaces} in {Probability} and {Statistics} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/book/10.1007/978-1-4419-9096-9},
	urldate = {2024-12-06},
}

@misc{ghorbani_linearized_2020,
	title = {Linearized two-layers neural networks in high dimension},
	url = {http://arxiv.org/abs/1904.12191},
	doi = {10.48550/arXiv.1904.12191},
	abstract = {We consider the problem of learning an unknown function \$f\_\{{\textbackslash}star\}\$ on the \$d\$-dimensional sphere with respect to the square loss, given i.i.d. samples \${\textbackslash}\{(y\_i,\{{\textbackslash}boldsymbol x\}\_i){\textbackslash}\}\_\{i{\textbackslash}le n\}\$ where \$\{{\textbackslash}boldsymbol x\}\_i\$ is a feature vector uniformly distributed on the sphere and \$y\_i=f\_\{{\textbackslash}star\}(\{{\textbackslash}boldsymbol x\}\_i)+{\textbackslash}varepsilon\_i\$. We study two popular classes of models that can be regarded as linearizations of two-layers neural networks around a random initialization: the random features model of Rahimi-Recht (RF); the neural tangent kernel model of Jacot-Gabriel-Hongler (NT). Both these approaches can also be regarded as randomized approximations of kernel ridge regression (with respect to different kernels), and enjoy universal approximation properties when the number of neurons \$N\$ diverges, for a fixed dimension \$d\$. We consider two specific regimes: the approximation-limited regime, in which \$n={\textbackslash}infty\$ while \$d\$ and \$N\$ are large but finite; and the sample size-limited regime in which \$N={\textbackslash}infty\$ while \$d\$ and \$n\$ are large but finite. In the first regime we prove that if \$d{\textasciicircum}\{{\textbackslash}ell + {\textbackslash}delta\} {\textbackslash}le N{\textbackslash}le d{\textasciicircum}\{{\textbackslash}ell+1-{\textbackslash}delta\}\$ for small \${\textbackslash}delta {\textgreater} 0\$, then {\textbackslash}RF{\textbackslash}, effectively fits a degree-\${\textbackslash}ell\$ polynomial in the raw features, and {\textbackslash}NT{\textbackslash}, fits a degree-\$({\textbackslash}ell+1)\$ polynomial. In the second regime, both RF and NT reduce to kernel methods with rotationally invariant kernels. We prove that, if the number of samples is \$d{\textasciicircum}\{{\textbackslash}ell + {\textbackslash}delta\} {\textbackslash}le n {\textbackslash}le d{\textasciicircum}\{{\textbackslash}ell +1-{\textbackslash}delta\}\$, then kernel methods can fit at most a a degree-\${\textbackslash}ell\$ polynomial in the raw features. This lower bound is achieved by kernel ridge regression. Optimal prediction error is achieved for vanishing ridge regularization.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = feb,
	year = {2020},
	note = {arXiv:1904.12191},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Statistics Theory},
}

@misc{karoui_spectrum_2010,
	title = {The spectrum of kernel random matrices},
	url = {http://arxiv.org/abs/1001.0492},
	doi = {10.48550/arXiv.1001.0492},
	abstract = {We place ourselves in the setting of high-dimensional statistical inference where the number of variables \$p\$ in a dataset of interest is of the same order of magnitude as the number of observations \$n\$. We consider the spectrum of certain kernel random matrices, in particular \$n{\textbackslash}times n\$ matrices whose \$(i,j)\$th entry is \$f(X\_i'X\_j/p)\$ or \$f({\textbackslash}Vert X\_i-X\_j{\textbackslash}Vert{\textasciicircum}2/p)\$ where \$p\$ is the dimension of the data, and \$X\_i\$ are independent data vectors. Here \$f\$ is assumed to be a locally smooth function. The study is motivated by questions arising in statistics and computer science where these matrices are used to perform, among other things, nonlinear versions of principal component analysis. Surprisingly, we show that in high-dimensions, and for the models we analyze, the problem becomes essentially linear--which is at odds with heuristics sometimes used to justify the usage of these methods. The analysis also highlights certain peculiarities of models widely studied in random matrix theory and raises some questions about their relevance as tools to model high-dimensional data encountered in practice.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Karoui, Noureddine El},
	month = jan,
	year = {2010},
	note = {arXiv:1001.0492},
	keywords = {Mathematics - Statistics Theory, Statistics - Statistics Theory},
}

@misc{hu_sharp_2022,
	title = {Sharp {Asymptotics} of {Kernel} {Ridge} {Regression} {Beyond} the {Linear} {Regime}},
	url = {http://arxiv.org/abs/2205.06798},
	doi = {10.48550/arXiv.2205.06798},
	abstract = {The generalization performance of kernel ridge regression (KRR) exhibits a multi-phased pattern that crucially depends on the scaling relationship between the sample size \$n\$ and the underlying dimension \$d\$. This phenomenon is due to the fact that KRR sequentially learns functions of increasing complexity as the sample size increases; when \$d{\textasciicircum}\{k-1\}{\textbackslash}ll n{\textbackslash}ll d{\textasciicircum}\{k\}\$, only polynomials with degree less than \$k\$ are learned. In this paper, we present sharp asymptotic characterization of the performance of KRR at the critical transition regions with \$n {\textbackslash}asymp d{\textasciicircum}k\$, for \$k{\textbackslash}in{\textbackslash}mathbb\{Z\}{\textasciicircum}\{+\}\$. Our asymptotic characterization provides a precise picture of the whole learning process and clarifies the impact of various parameters (including the choice of the kernel function) on the generalization performance. In particular, we show that the learning curves of KRR can have a delicate "double descent" behavior due to specific bias-variance trade-offs at different polynomial scaling regimes.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Hu, Hong and Lu, Yue M.},
	month = may,
	year = {2022},
	note = {arXiv:2205.06798},
	keywords = {Computer Science - Machine Learning},
}

@article{mairal_kernel_nodate,
	title = {Kernel {Methods} in {Machine} {Learning}},
	language = {en},
	author = {Mairal, Julien and Vert, Jean-Philippe},
}

@misc{lu_equivalence_2023,
	title = {An {Equivalence} {Principle} for the {Spectrum} of {Random} {Inner}-{Product} {Kernel} {Matrices} with {Polynomial} {Scalings}},
	url = {http://arxiv.org/abs/2205.06308},
	doi = {10.48550/arXiv.2205.06308},
	abstract = {We investigate random matrices whose entries are obtained by applying a nonlinear kernel function to pairwise inner products between \$n\$ independent data vectors, drawn uniformly from the unit sphere in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$. This study is motivated by applications in machine learning and statistics, where these kernel random matrices and their spectral properties play significant roles. We establish the weak limit of the empirical spectral distribution of these matrices in a polynomial scaling regime, where \$d, n {\textbackslash}to {\textbackslash}infty\$ such that \$n / d{\textasciicircum}{\textbackslash}ell {\textbackslash}to {\textbackslash}kappa\$, for some fixed \${\textbackslash}ell {\textbackslash}in {\textbackslash}mathbb\{N\}\$ and \${\textbackslash}kappa {\textbackslash}in (0, {\textbackslash}infty)\$. Our findings generalize an earlier result by Cheng and Singer, who examined the same model in the linear scaling regime (with \${\textbackslash}ell = 1\$). Our work reveals an equivalence principle: the spectrum of the random kernel matrix is asymptotically equivalent to that of a simpler matrix model, constructed as a linear combination of a (shifted) Wishart matrix and an independent matrix sampled from the Gaussian orthogonal ensemble. The aspect ratio of the Wishart matrix and the coefficients of the linear combination are determined by \${\textbackslash}ell\$ and the expansion of the kernel function in the orthogonal Hermite polynomial basis. Consequently, the limiting spectrum of the random kernel matrix can be characterized as the free additive convolution between a Marchenko-Pastur law and a semicircle law. We also extend our results to cases with data vectors sampled from isotropic Gaussian distributions instead of spherical distributions.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Lu, Yue M. and Yau, Horng-Tzer},
	month = may,
	year = {2023},
	note = {arXiv:2205.06308},
	keywords = {Mathematics - Probability, Statistics - Machine Learning},
}

@misc{xiao_precise_2023,
	title = {Precise {Learning} {Curves} and {Higher}-{Order} {Scaling} {Limits} for {Dot} {Product} {Kernel} {Regression}},
	url = {http://arxiv.org/abs/2205.14846},
	doi = {10.48550/arXiv.2205.14846},
	abstract = {As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics (\$m{\textbackslash}to{\textbackslash}infty\$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension (\$m{\textbackslash}propto d\$). There is a wide gulf between these two regimes, including all higher-order scaling relations \$m{\textbackslash}propto d{\textasciicircum}r\$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the mean of the test error, bias, and variance, for data drawn uniformly from the sphere with isotropic random labels in the \$r\$th-order asymptotic scaling regime \$m{\textbackslash}to{\textbackslash}infty\$ with \$m/d{\textasciicircum}r\$ held constant. We observe a peak in the learning curve whenever \$m {\textbackslash}approx d{\textasciicircum}r/r!\$ for any integer \$r\$, leading to multiple sample-wise descent and nontrivial behavior at multiple scales.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Xiao, Lechao and Hu, Hong and Misiakiewicz, Theodor and Lu, Yue M. and Pennington, Jeffrey},
	month = jun,
	year = {2023},
	note = {arXiv:2205.14846},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hu_asymptotics_2024,
	title = {Asymptotics of {Random} {Feature} {Regression} {Beyond} the {Linear} {Scaling} {Regime}},
	url = {http://arxiv.org/abs/2403.08160},
	doi = {10.48550/arXiv.2403.08160},
	abstract = {Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters p? How should we choose p relative to the sample size n to achieve optimal test error? In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the d-dimensional sphere and compute sharp asymptotics for the RFRR test error in the high-dimensional polynomial scaling, where p, n, d → ∞ while p/dκ1 and n/dκ2 stay constant, for all κ1, κ2 ∈ R{\textgreater}0. These asymptotics precisely characterize the impact of the number of random features and regularization parameter on the test performance. In particular, RFRR exhibits an intuitive trade-off between approximation and generalization power. For n = o(p), the sample size n is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking p = ∞). On the other hand, if p = o(n), the number of random features p is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking n = ∞). Finally, a double descent appears at n = p, a phenomenon that was previously only characterized in the linear scaling κ1 = κ2 = 1. This completes the picture initiated in [GMMM21, MMM22, MM22, XHM+22].},
	language = {en},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Hu, Hong and Lu, Yue M. and Misiakiewicz, Theodor},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08160 [stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@inproceedings{pham_memorization_2024,
	title = {Memorization to {Generalization}: {The} {Emergence} of {Diffusion} {Models} from {Associative} {Memory}},
	shorttitle = {Memorization to {Generalization}},
	url = {https://openreview.net/forum?id=zVMMaVy2BY},
	abstract = {Hopfield networks are associative memory systems, designed for storing and retrieving specific patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the model's memorization capacity reaches its critical memory load - spurious states, or unintended stable points, emerge at the end of the retrieval dynamics. These particular states often appear as mixtures of the stored patterns, leading to incorrect recall. In this work, we propose that these spurious states are not necessarily a negative feature of retrieval dynamics, but rather that they serve as the onset of generalization. We employ diffusion models, commonly used in generative modelling, to demonstrate that their generalization stems from a phase transition which occurs as the number of training samples is increased. In the low data regime the model exhibits a strong memorization phase, where the network creates a distinct basin of attraction for each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime a different phase appears where an increase in the training set size fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, still have a distinct basin of attraction around them. From the perspective of Hopfield description these spurious states correspond to mixtures of "fundamental memories" which facilitate generalization through the superposition of underlying features, resulting in the creation of novel samples. Our findings provide a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of Hopfield networks, which illuminate the previously underappreciated view of diffusion models as Hopfield networks above the critical memory load.},
	language = {en},
	urldate = {2024-11-21},
	author = {Pham, Bao and Raya, Gabriel and Negri, Matteo and Zaki, Mohammed J. and Ambrogioni, Luca and Krotov, Dmitry},
	month = nov,
	year = {2024},
}

@misc{ross_geometric_2024,
	title = {A {Geometric} {Framework} for {Understanding} {Memorization} in {Generative} {Models}},
	url = {http://arxiv.org/abs/2411.00113},
	abstract = {As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose the manifold memorization hypothesis (MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of \$(i)\$ the ground truth data manifold and \$(ii)\$ the manifold learned by the model. This framework provides a formal standard for "how memorized" a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process.},
	urldate = {2024-11-21},
	publisher = {arXiv},
	author = {Ross, Brendan Leigh and Kamkari, Hamidreza and Wu, Tongzi and Hosseinzadeh, Rasa and Liu, Zhaoyan and Stein, George and Cresswell, Jesse C. and Loaiza-Ganem, Gabriel},
	month = oct,
	year = {2024},
	note = {arXiv:2411.00113},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kamkari_geometric_2024,
	title = {A {Geometric} {View} of {Data} {Complexity}: {Efficient} {Local} {Intrinsic} {Dimension} {Estimation} with {Diffusion} {Models}},
	shorttitle = {A {Geometric} {View} of {Data} {Complexity}},
	url = {http://arxiv.org/abs/2406.03537},
	abstract = {High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum -- i.e. the dimension of the submanifold it belongs to -- is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models: diffusion models (DMs). In this work, we show that the Fokker-Planck equation associated with a DM can provide an LID estimator which addresses the aforementioned deficiencies. Our estimator, called FLIPD, is easy to implement and compatible with all popular DMs. Applying FLIPD to synthetic LID estimation benchmarks, we find that DMs implemented as fully-connected networks are highly effective LID estimators that outperform existing baselines. We also apply FLIPD to natural images where the true LID is unknown. Despite being sensitive to the choice of network architecture, FLIPD estimates remain a useful measure of relative complexity; compared to competing estimators, FLIPD exhibits a consistently higher correlation with image PNG compression rate and better aligns with qualitative assessments of complexity. Notably, FLIPD is orders of magnitude faster than other LID estimators, and the first to be tractable at the scale of Stable Diffusion.},
	urldate = {2024-11-21},
	publisher = {arXiv},
	author = {Kamkari, Hamidreza and Ross, Brendan Leigh and Hosseinzadeh, Rasa and Cresswell, Jesse C. and Loaiza-Ganem, Gabriel},
	month = oct,
	year = {2024},
	note = {arXiv:2406.03537},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{stanczuk_diffusion_nodate,
	title = {Diffusion {Models} {Encode} the {Intrinsic} {Dimension} of {Data} {Manifolds}},
	abstract = {In this work, we provide a mathematical proof that diffusion models encode data manifolds by approximating their normal bundles. Based on this observation we propose a novel method for extracting the intrinsic dimension of the data manifold from a trained diffusion model. Our insights are based on the fact that a diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, at low noise levels, the diffusion model provides us with an approximation of the manifold’s normal bundle, allowing for an estimation of the manifold’s intrinsic dimension. To the best of our knowledge our method is the first estimator of intrinsic dimension based on diffusion models and it outperforms well established estimators in controlled experiments on both Euclidean and image data. The code is available at https: //github.com/GBATZOLIS/ID-diff.},
	language = {en},
	author = {Stanczuk, Jan Pawel and Batzolis, Georgios and Deveney, Teo and Schönlieb, Carola-Bibiane},
}

@book{guckenheimer_nonlinear_1983,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Nonlinear {Oscillations}, {Dynamical} {Systems}, and {Bifurcations} of {Vector} {Fields}},
	volume = {42},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-7020-1 978-1-4612-1140-2},
	url = {http://link.springer.com/10.1007/978-1-4612-1140-2},
	urldate = {2024-11-18},
	publisher = {Springer},
	author = {Guckenheimer, John and Holmes, Philip},
	year = {1983},
	doi = {10.1007/978-1-4612-1140-2},
	keywords = {Bifurcations, Chaos (Math.), Dynamische Systeme, Fields, Nichtlineare Schwingung, Oscillations, Seltsamer Attraktor, Vector, Verzweigung (Math.), differential equation, linear optimization, ordinary differential equation},
}

@misc{wang_diffusion_2024,
	title = {Diffusion {Models} {Learn} {Low}-{Dimensional} {Distributions} via {Subspace} {Clustering}},
	url = {http://arxiv.org/abs/2409.02426},
	abstract = {Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Wang, Peng and Zhang, Huijie and Zhang, Zekai and Chen, Siyi and Ma, Yi and Qu, Qing},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02426},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ross_geometric_2024-1,
	title = {A {Geometric} {Framework} for {Understanding} {Memorization} in {Generative} {Models}},
	url = {http://arxiv.org/abs/2411.00113},
	abstract = {As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose the manifold memorization hypothesis (MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of \$(i)\$ the ground truth data manifold and \$(ii)\$ the manifold learned by the model. This framework provides a formal standard for "how memorized" a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Ross, Brendan Leigh and Kamkari, Hamidreza and Wu, Tongzi and Hosseinzadeh, Rasa and Liu, Zhaoyan and Stein, George and Cresswell, Jesse C. and Loaiza-Ganem, Gabriel},
	month = oct,
	year = {2024},
	note = {arXiv:2411.00113},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ventura_manifolds_2024,
	title = {Manifolds, {Random} {Matrices} and {Spectral} {Gaps}: {The} geometric phases of generative diffusion},
	shorttitle = {Manifolds, {Random} {Matrices} and {Spectral} {Gaps}},
	url = {http://arxiv.org/abs/2410.05898},
	doi = {10.48550/arXiv.2410.05898},
	abstract = {In this paper, we investigate the latent geometry of generative diffusion models under the manifold hypothesis. To this purpose, we analyze the spectrum of eigenvalues (and singular values) of the Jacobian of the score function, whose discontinuities (gaps) reveal the presence and dimensionality of distinct sub-manifolds. Using a statistical physics approach, we derive the spectral distributions and formulas for the spectral gaps under several distributional assumptions and we compare these theoretical predictions with the spectra estimated from trained networks. Our analysis reveals the existence of three distinct qualitative phases during the generative process: a trivial phase; a manifold coverage phase where the diffusion process fits the distribution internal to the manifold; a consolidation phase where the score becomes orthogonal to the manifold and all particles are projected on the support of the data. This `division of labor' between different timescales provides an elegant explanation on why generative diffusion models are not affected by the manifold overfitting phenomenon that plagues likelihood-based models, since the internal distribution and the manifold geometry are produced at different time points during generation.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Ventura, Enrico and Achilli, Beatrice and Silvestri, Gianluigi and Lucibello, Carlo and Ambrogioni, Luca},
	month = oct,
	year = {2024},
	note = {arXiv:2410.05898},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{achilli_losing_2024,
	title = {Losing dimensions: {Geometric} memorization in generative diffusion},
	shorttitle = {Losing dimensions},
	url = {http://arxiv.org/abs/2410.08727},
	abstract = {Generative diffusion processes are state-of-the-art machine learning models deeply connected with fundamental concepts in statistical physics. Depending on the dataset size and the capacity of the network, their behavior is known to transition from an associative memory regime to a generalization phase in a phenomenon that has been described as a glassy phase transition. Here, using statistical physics techniques, we extend the theory of memorization in generative diffusion to manifold-supported data. Our theoretical and experimental findings indicate that different tangent subspaces are lost due to memorization effects at different critical times and dataset sizes, which depend on the local variance of the data along their directions. Perhaps counterintuitively, we find that, under some conditions, subspaces of higher variance are lost first due to memorization effects. This leads to a selective loss of dimensionality where some prominent features of the data are memorized without a full collapse on any individual training point. We validate our theory with a comprehensive set of experiments on networks trained both in image datasets and on linear manifolds, which result in a remarkable qualitative agreement with the theoretical predictions.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Achilli, Beatrice and Ventura, Enrico and Silvestri, Gianluigi and Pham, Bao and Raya, Gabriel and Krotov, Dmitry and Lucibello, Carlo and Ambrogioni, Luca},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08727},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hoover_memory_2024,
	title = {Memory in {Plain} {Sight}: {Surveying} the {Uncanny} {Resemblances} of {Associative} {Memories} and {Diffusion} {Models}},
	shorttitle = {Memory in {Plain} {Sight}},
	url = {http://arxiv.org/abs/2309.16750},
	abstract = {The generative process of Diffusion Models (DMs) has recently set state-of-the-art on many AI generation benchmarks. Though the generative process is traditionally understood as an "iterative denoiser", there is no universally accepted language to describe it. We introduce a novel perspective to describe DMs using the mathematical language of memory retrieval from the field of energy-based Associative Memories (AMs), making efforts to keep our presentation approachable to newcomers to both of these fields. Unifying these two fields provides insight that DMs can be seen as a particular kind of AM where Lyapunov stability guarantees are bypassed by intelligently engineering the dynamics (i.e., the noise and step size schedules) of the denoising process. Finally, we present a growing body of evidence that records DMs exhibiting empirical behavior we would expect from AMs, and conclude by discussing research opportunities that are revealed by understanding DMs as a form of energy-based memory.},
	urldate = {2024-11-17},
	publisher = {arXiv},
	author = {Hoover, Benjamin and Strobelt, Hendrik and Krotov, Dmitry and Hoffman, Judy and Kira, Zsolt and Chau, Duen Horng},
	month = may,
	year = {2024},
	note = {arXiv:2309.16750},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Dynamical Systems},
}

@book{strogatz_nonlinear_2015,
	address = {Boulder, CO},
	edition = {Second edition},
	title = {Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering},
	isbn = {978-0-8133-4910-7},
	shorttitle = {Nonlinear dynamics and chaos},
	language = {en},
	publisher = {Westview Press, a member of the Perseus Books Group},
	author = {Strogatz, Steven H.},
	year = {2015},
	note = {OCLC: ocn842877119},
	keywords = {Chaotic behavior in systems, Dynamics, Nonlinear theories},
}

@misc{radhakrishnan_overparameterized_2020,
	title = {Overparameterized {Neural} {Networks} {Implement} {Associative} {Memory}},
	url = {http://arxiv.org/abs/1909.12362},
	abstract = {Identifying computational mechanisms for memorization and retrieval of data is a long-standing problem at the intersection of machine learning and neuroscience. Our main finding is that standard overparameterized deep neural networks trained using standard optimization methods implement such a mechanism for real-valued data. Empirically, we show that: (1) overparameterized autoencoders store training samples as attractors, and thus, iterating the learned map leads to sample recovery; (2) the same mechanism allows for encoding sequences of examples, and serves as an even more efficient mechanism for memory than autoencoding. Theoretically, we prove that when trained on a single example, autoencoders store the example as an attractor. Lastly, by treating a sequence encoder as a composition of maps, we prove that sequence encoding provides a more efficient mechanism for memory than autoencoding.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
	month = sep,
	year = {2020},
	note = {arXiv:1909.12362},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cui_high-dimensional_2024,
	title = {High-dimensional asymptotics of denoising autoencoders$^{\textrm{*}}$},
	volume = {2024},
	issn = {1742-5468},
	url = {https://iopscience.iop.org/article/10.1088/1742-5468/ad65e1},
	doi = {10.1088/1742-5468/ad65e1},
	abstract = {We address the problem of denoising data from a Gaussian mixture using a twolayer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results accurately capture the learning curves on a range of real data sets.},
	language = {en},
	number = {10},
	urldate = {2024-11-12},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Cui, Hugo and Zdeborová, Lenka},
	month = oct,
	year = {2024},
	pages = {104018},
}

@misc{hu_asymptotics_2024-1,
	title = {Asymptotics of {Random} {Feature} {Regression} {Beyond} the {Linear} {Scaling} {Regime}},
	url = {http://arxiv.org/abs/2403.08160},
	abstract = {Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters \$p\$? How should we choose \$p\$ relative to the sample size \$n\$ to achieve optimal test error? In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the \$d\$-dimensional sphere and compute sharp asymptotics for the RFRR test error in the high-dimensional polynomial scaling, where \$p,n,d {\textbackslash}to {\textbackslash}infty\$ while \$p/ d{\textasciicircum}\{{\textbackslash}kappa\_1\}\$ and \$n / d{\textasciicircum}\{{\textbackslash}kappa\_2\}\$ stay constant, for all \${\textbackslash}kappa\_1 , {\textbackslash}kappa\_2 {\textbackslash}in {\textbackslash}mathbb\{R\}\_\{{\textgreater}0\}\$. These asymptotics precisely characterize the impact of the number of random features and regularization parameter on the test performance. In particular, RFRR exhibits an intuitive trade-off between approximation and generalization power. For \$n = o(p)\$, the sample size \$n\$ is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking \$p = {\textbackslash}infty\$). On the other hand, if \$p = o(n)\$, the number of random features \$p\$ is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking \$n = {\textbackslash}infty\$). Finally, a double descent appears at \$n= p\$, a phenomenon that was previously only characterized in the linear scaling \${\textbackslash}kappa\_1 = {\textbackslash}kappa\_2 = 1\$.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Hu, Hong and Lu, Yue M. and Misiakiewicz, Theodor},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08160},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@misc{dandi_random_2024,
	title = {A {Random} {Matrix} {Theory} {Perspective} on the {Spectrum} of {Learned} {Features} and {Asymptotic} {Generalization} {Capabilities}},
	url = {http://arxiv.org/abs/2410.18938},
	abstract = {A key property of neural networks is their capacity of adapting to data during training. Yet, our current mathematical understanding of feature learning and its relationship to generalization remain limited. In this work, we provide a random matrix analysis of how fully-connected two-layer neural networks adapt to the target function after a single, but aggressive, gradient descent step. We rigorously establish the equivalence between the updated features and an isotropic spiked random feature model, in the limit of large batch size. For the latter model, we derive a deterministic equivalent description of the feature empirical covariance matrix in terms of certain low-dimensional operators. This allows us to sharply characterize the impact of training in the asymptotic feature spectrum, and in particular, provides a theoretical grounding for how the tails of the feature spectrum modify with training. The deterministic equivalent further yields the exact asymptotic generalization error, shedding light on the mechanisms behind its improvement in the presence of feature learning. Our result goes beyond standard random matrix ensembles, and therefore we believe it is of independent technical interest. Different from previous work, our result holds in the challenging maximal learning rate regime, is fully rigorous and allows for finitely supported second layer initialization, which turns out to be crucial for studying the functional expressivity of the learned features. This provides a sharp description of the impact of feature learning in the generalization of two-layer neural networks, beyond the random features and lazy training regimes.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Dandi, Yatin and Pesce, Luca and Cui, Hugo and Krzakala, Florent and Lu, Yue M. and Loureiro, Bruno},
	month = oct,
	year = {2024},
	note = {arXiv:2410.18938},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@misc{misiakiewicz_six_2023,
	title = {Six {Lectures} on {Linearized} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2308.13431},
	abstract = {In these six lectures, we examine what can be learnt about the behavior of multi-layer neural networks from the analysis of linear models. We first recall the correspondence between neural networks and linear models via the so-called lazy regime. We then review four models for linearized neural networks: linear regression with concentrated features, kernel ridge regression, random feature model and neural tangent model. Finally, we highlight the limitations of the linear theory and discuss how other approaches can overcome them.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Misiakiewicz, Theodor and Montanari, Andrea},
	month = aug,
	year = {2023},
	note = {arXiv:2308.13431},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@article{mingo_free_nodate,
	title = {Free {Probability} and {Random} {Matrices}},
	language = {en},
	author = {Mingo, J A and Speicher, R},
}

@book{potters_first_2020,
	address = {Cambridge},
	title = {A {First} {Course} in {Random} {Matrix} {Theory}: for {Physicists}, {Engineers} and {Data} {Scientists}},
	isbn = {978-1-108-48808-2},
	shorttitle = {A {First} {Course} in {Random} {Matrix} {Theory}},
	url = {https://www.cambridge.org/core/books/first-course-in-random-matrix-theory/2292A554A9BB9E2A4697C35BCE920304},
	abstract = {The real world is perceived and broken down as data, models and algorithms in the eyes of physicists and engineers. Data is noisy by nature and classical statistical tools have so far been successful in dealing with relatively smaller levels of randomness. The recent emergence of Big Data and the required computing power to analyse them have rendered classical tools outdated and insufficient. Tools such as random matrix theory and the study of large sample covariance matrices can efficiently process these big data sets and help make sense of modern, deep learning algorithms. Presenting an introductory calculus course for random matrices, the book focusses on modern concepts in matrix theory, generalising the standard concept of probabilistic independence to non-commuting random variables. Concretely worked out examples and applications to financial engineering and portfolio construction make this unique book an essential tool for physicists, engineers, data analysts, and economists.},
	urldate = {2024-11-04},
	publisher = {Cambridge University Press},
	author = {Potters, Marc and Bouchaud, Jean-Philippe},
	year = {2020},
	doi = {10.1017/9781108768900},
}

@article{tao_topics_nodate,
	title = {Topics in random matrix theory},
	language = {en},
	author = {Tao, Terence},
}

@misc{loureiro_learning_2021,
	title = {Learning {Gaussian} {Mixtures} with {Generalised} {Linear} {Models}: {Precise} {Asymptotics} in {High}-dimensions},
	shorttitle = {Learning {Gaussian} {Mixtures} with {Generalised} {Linear} {Models}},
	url = {http://arxiv.org/abs/2106.03791},
	doi = {10.48550/arXiv.2106.03791},
	abstract = {Generalised linear models for multi-class classification problems are one of the fundamental building blocks of modern machine learning tasks. In this manuscript, we characterise the learning of a mixture of \$K\$ Gaussians with generic means and covariances via empirical risk minimisation (ERM) with any convex loss and regularisation. In particular, we prove exact asymptotics characterising the ERM estimator in high-dimensions, extending several previous results about Gaussian mixture classification in the literature. We exemplify our result in two tasks of interest in statistical learning: a) classification for a mixture with sparse means, where we study the efficiency of \${\textbackslash}ell\_1\$ penalty with respect to \${\textbackslash}ell\_2\$; b) max-margin multi-class classification, where we characterise the phase transition on the existence of the multi-class logistic maximum likelihood estimator for \$K{\textgreater}2\$. Finally, we discuss how our theory can be applied beyond the scope of synthetic data, showing that in different cases Gaussian mixtures capture closely the learning curve of classification tasks in real data sets.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Loureiro, Bruno and Sicuro, Gabriele and Gerbelot, Cédric and Pacco, Alessandro and Krzakala, Florent and Zdeborová, Lenka},
	month = dec,
	year = {2021},
	note = {arXiv:2106.03791},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
}

@misc{refinetti_classifying_2021,
	title = {Classifying high-dimensional {Gaussian} mixtures: {Where} kernel methods fail and neural networks succeed},
	shorttitle = {Classifying high-dimensional {Gaussian} mixtures},
	url = {http://arxiv.org/abs/2102.11742},
	doi = {10.48550/arXiv.2102.11742},
	abstract = {A recent series of theoretical works showed that the dynamics of neural networks with a certain initialisation are well-captured by kernel methods. Concurrent empirical work demonstrated that kernel methods can come close to the performance of neural networks on some image classification tasks. These results raise the question of whether neural networks only learn successfully if kernels also learn successfully, despite neural networks being more expressive. Here, we show theoretically that two-layer neural networks (2LNN) with only a few hidden neurons can beat the performance of kernel learning on a simple Gaussian mixture classification task. We study the high-dimensional limit where the number of samples is linearly proportional to the input dimension, and show that while small 2LNN achieve near-optimal performance on this task, lazy training approaches such as random features and kernel methods do not. Our analysis is based on the derivation of a closed set of equations that track the learning dynamics of the 2LNN and thus allow to extract the asymptotic performance of the network as a function of signal-to-noise ratio and other hyperparameters. We finally illustrate how over-parametrising the neural network leads to faster convergence, but does not improve its final performance.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Refinetti, Maria and Goldt, Sebastian and Krzakala, Florent and Zdeborová, Lenka},
	month = jun,
	year = {2021},
	note = {arXiv:2102.11742},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@misc{cui_asymptotics_2024,
	title = {Asymptotics of feature learning in two-layer networks after one gradient-step},
	url = {http://arxiv.org/abs/2402.04980},
	abstract = {In this manuscript, we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging the insight from (Ba et al., 2022), we model the trained network by a spiked Random Features (sRF) model. Further building on recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error of the sRF in the high-dimensional limit where the number of samples, the width, and the input dimension grow at a proportional rate. The resulting characterization for sRFs also captures closely the learning curves of the original network model. This enables us to understand how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Cui, Hugo and Pesce, Luca and Dandi, Yatin and Krzakala, Florent and Lu, Yue M. and Zdeborová, Lenka and Loureiro, Bruno},
	month = jun,
	year = {2024},
	note = {arXiv:2402.04980},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
}

@misc{far_spectra_2006,
	title = {Spectra of large block matrices},
	url = {http://arxiv.org/abs/cs/0610045},
	abstract = {In a frequency selective slow-fading channel in a MIMO system, the channel matrix is of the form of a block matrix. This paper proposes a method to calculate the limit of the eigenvalue distribution of block matrices if the size of the blocks tends to infinity. While it considers random matrices, it takes an operator-valued free probability approach to achieve this goal. Using this method, one derives a system of equations, which can be solved numerically to compute the desired eigenvalue distribution. The paper initially tackles the problem for square block matrices, then extends the solution to rectangular block matrices. Finally, it deals with Wishart type block matrices. For two special cases, the results of our approach are compared with results from simulations. The first scenario investigates the limit eigenvalue distribution of block Toeplitz matrices. The second scenario deals with the distribution of Wishart type block matrices for a frequency selective slow-fading channel in a MIMO system for two different cases of \$n\_R=n\_T\$ and \$n\_R=2n\_T\$. Using this method, one may calculate the capacity and the Signal-to-Interference-and-Noise Ratio in large MIMO systems.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Far, Reza Rashidi and Oraby, Tamer and Bryc, Wlodzimierz and Speicher, Roland},
	month = oct,
	year = {2006},
	note = {arXiv:cs/0610045},
	keywords = {Computer Science - Information Theory, Mathematics - Information Theory, Mathematics - Operator Algebras},
}

@book{couillet_random_2022,
	edition = {1},
	title = {Random {Matrix} {Methods} for {Machine} {Learning}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-00-912849-0 978-1-00-912323-5},
	url = {https://www.cambridge.org/core/product/identifier/9781009128490/type/book},
	abstract = {This book presents a unified theory of random matrices for applications in machine learning, offering a large-dimensional data vision that exploits concentration and universality phenomena. This enables a precise understanding, and possible improvements, of the core mechanisms at play in real-world machine learning algorithms. The book opens with a thorough introduction to the theoretical basics of random matrices, which serves as a support to a wide scope of applications ranging from SVMs, through semi-supervised learning, unsupervised spectral clustering, and graph methods, to neural networks and deep learning. For each application, the authors discuss small- versus large-dimensional intuitions of the problem, followed by a systematic random matrix analysis of the resulting performance and possible improvements. All concepts, applications, and variations are illustrated numerically on synthetic as well as real-world data, with MATLAB and Python code provided on the accompanying website.},
	language = {en},
	urldate = {2024-10-19},
	publisher = {Cambridge University Press},
	author = {Couillet, Romain and Liao, Zhenyu},
	month = jul,
	year = {2022},
	doi = {10.1017/9781009128490},
}

@misc{pesce_are_2023,
	title = {Are {Gaussian} data all you need? {Extents} and limits of universality in high-dimensional generalized linear estimation},
	shorttitle = {Are {Gaussian} data all you need?},
	url = {http://arxiv.org/abs/2302.08923},
	abstract = {In this manuscript we consider the problem of generalized linear estimation on Gaussian mixture data with labels given by a single-index model. Our first result is a sharp asymptotic expression for the test and training errors in the high-dimensional regime. Motivated by the recent stream of results on the Gaussian universality of the test and training errors in generalized linear estimation, we ask ourselves the question: "when is a single Gaussian enough to characterize the error?". Our formula allow us to give sharp answers to this question, both in the positive and negative directions. More precisely, we show that the sufficient conditions for Gaussian universality (or lack of thereof) crucially depend on the alignment between the target weights and the means and covariances of the mixture clusters, which we precisely quantify. In the particular case of least-squares interpolation, we prove a strong universality property of the training error, and show it follows a simple, closed-form expression. Finally, we apply our results to real datasets, clarifying some recent discussion in the literature about Gaussian universality of the errors in this context.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Pesce, Luca and Krzakala, Florent and Loureiro, Bruno and Stephan, Ludovic},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08923},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@misc{hu_universality_2022,
	title = {Universality {Laws} for {High}-{Dimensional} {Learning} with {Random} {Features}},
	url = {http://arxiv.org/abs/2009.07669},
	abstract = {We prove a universality theorem for learning with random features. Our result shows that, in terms of training and generalization errors, a random feature model with a nonlinear activation function is asymptotically equivalent to a surrogate linear Gaussian model with a matching covariance matrix. This settles a so-called Gaussian equivalence conjecture based on which several recent papers develop their results. Our method for proving the universality theorem builds on the classical Lindeberg approach. Major ingredients of the proof include a leave-one-out analysis for the optimization problem associated with the training process and a central limit theorem, obtained via Stein's method, for weakly correlated random variables.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Hu, Hong and Lu, Yue M.},
	month = oct,
	year = {2022},
	note = {arXiv:2009.07669},
	keywords = {Computer Science - Information Theory, Mathematics - Information Theory},
}

@misc{seddik_random_2020,
	title = {Random {Matrix} {Theory} {Proves} that {Deep} {Learning} {Representations} of {GAN}-data {Behave} as {Gaussian} {Mixtures}},
	url = {http://arxiv.org/abs/2001.08370},
	abstract = {This paper shows that deep learning (DL) representations of data produced by generative adversarial nets (GANs) are random vectors which fall within the class of so-called {\textbackslash}textit\{concentrated\} random vectors. Further exploiting the fact that Gram matrices, of the type \$G = X{\textasciicircum}T X\$ with \$X=[x\_1,{\textbackslash}ldots,x\_n]{\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{p{\textbackslash}times n\}\$ and \$x\_i\$ independent concentrated random vectors from a mixture model, behave asymptotically (as \$n,p{\textbackslash}to {\textbackslash}infty\$) as if the \$x\_i\$ were drawn from a Gaussian mixture, suggests that DL representations of GAN-data can be fully described by their first two statistical moments for a wide range of standard classifiers. Our theoretical findings are validated by generating images with the BigGAN model and across different popular deep representation networks.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Seddik, Mohamed El Amine and Louart, Cosme and Tamaazousti, Mohamed and Couillet, Romain},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08370},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{louart_random_2017,
	title = {A {Random} {Matrix} {Approach} to {Neural} {Networks}},
	url = {http://arxiv.org/abs/1702.05419},
	abstract = {This article studies the Gram random matrix model \$G={\textbackslash}frac1T{\textbackslash}Sigma{\textasciicircum}\{{\textbackslash}rm T\}{\textbackslash}Sigma\$, \${\textbackslash}Sigma={\textbackslash}sigma(WX)\$, classically found in the analysis of random feature maps and random neural networks, where \$X=[x\_1,{\textbackslash}ldots,x\_T]{\textbackslash}in\{{\textbackslash}mathbb R\}{\textasciicircum}\{p{\textbackslash}times T\}\$ is a (data) matrix of bounded norm, \$W{\textbackslash}in\{{\textbackslash}mathbb R\}{\textasciicircum}\{n{\textbackslash}times p\}\$ is a matrix of independent zero-mean unit variance entries, and \${\textbackslash}sigma:\{{\textbackslash}mathbb R\}{\textbackslash}to\{{\textbackslash}mathbb R\}\$ is a Lipschitz continuous (activation) function --- \${\textbackslash}sigma(WX)\$ being understood entry-wise. By means of a key concentration of measure lemma arising from non-asymptotic random matrix arguments, we prove that, as \$n,p,T\$ grow large at the same rate, the resolvent \$Q=(G+{\textbackslash}gamma I\_T){\textasciicircum}\{-1\}\$, for \${\textbackslash}gamma{\textgreater}0\$, has a similar behavior as that met in sample covariance matrix models, involving notably the moment \${\textbackslash}Phi={\textbackslash}frac\{T\}n\{{\textbackslash}mathbb E\}[G]\$, which provides in passing a deterministic equivalent for the empirical spectral measure of \$G\$. Application-wise, this result enables the estimation of the asymptotic performance of single-layer random neural networks. This in turn provides practical insights into the underlying mechanisms into play in random neural networks, entailing several unexpected consequences, as well as a fast practical means to tune the network hyperparameters.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Louart, Cosme and Liao, Zhenyu and Couillet, Romain},
	month = jun,
	year = {2017},
	note = {arXiv:1702.05419},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability},
}

@misc{dandi_universality_2023,
	title = {Universality laws for {Gaussian} mixtures in generalized linear models},
	url = {http://arxiv.org/abs/2302.08933},
	abstract = {Let (xi, yi)i=1,...,n denote independent samples from a general mixture distribution c∈C ρcPcx, and consider the hypothesis class of generalized linear models yˆ = F (Θ x). In this work, we investigate the asymptotic joint statistics of the family of generalized linear estimators (Θ1, . . . , ΘM ) obtained either from (a) minimizing an empirical risk Rˆn(Θ; X, y) or (b) sampling from the associated Gibbs measure exp(−βnRˆn(Θ; X, y)). Our main contribution is to characterize under which conditions the asymptotic joint statistics of this family depends (on a weak sense) only on the means and covariances of the class conditional features distribution Pcx. In particular, this allow us to prove the universality of different quantities of interest, such as the training and generalization errors, redeeming a recent line of work in high-dimensional statistics working under the Gaussian mixture hypothesis. Finally, we discuss the applications of our results to different machine learning tasks of interest, such as ensembling and uncertainty quantiﬁcation.},
	language = {en},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Dandi, Yatin and Stephan, Ludovic and Krzakala, Florent and Loureiro, Bruno and Zdeborová, Lenka},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08933 [math]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@misc{mai_breakdown_2024,
	title = {The {Breakdown} of {Gaussian} {Universality} in {Classification} of {High}-dimensional {Mixtures}},
	url = {http://arxiv.org/abs/2410.05609},
	abstract = {The assumption of Gaussian or Gaussian mixture data has been extensively exploited in a long series of precise performance analyses of machine learning (ML) methods, on large datasets having comparably numerous samples and features. To relax this restrictive assumption, subsequent efforts have been devoted to establish "Gaussian equivalent principles" by studying scenarios of Gaussian universality where the asymptotic performance of ML methods on non-Gaussian data remains unchanged when replaced with Gaussian data having the same mean and covariance. Beyond the realm of Gaussian universality, there are few exact results on how the data distribution affects the learning performance. In this article, we provide a precise high-dimensional characterization of empirical risk minimization, for classification under a general mixture data setting of linear factor models that extends Gaussian mixtures. The Gaussian universality is shown to break down under this setting, in the sense that the asymptotic learning performance depends on the data distribution beyond the class means and covariances. To clarify the limitations of Gaussian universality in classification of mixture data and to understand the impact of its breakdown, we specify conditions for Gaussian universality and discuss their implications for the choice of loss function.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Mai, Xiaoyi and Liao, Zhenyu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.05609},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@misc{grenioux_stochastic_2024,
	title = {Stochastic {Localization} via {Iterative} {Posterior} {Sampling}},
	url = {https://arxiv.org/abs/2402.10758v2},
	abstract = {Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, \${\textbackslash}textit\{Stochastic Localization via Iterative Posterior Sampling\}\$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics.},
	language = {en},
	urldate = {2024-10-09},
	journal = {arXiv.org},
	author = {Grenioux, Louis and Noble, Maxence and Gabrié, Marylou and Durmus, Alain Oliviero},
	month = feb,
	year = {2024},
}

@misc{huang_reverse_2023,
	title = {Reverse {Diffusion} {Monte} {Carlo}},
	url = {https://arxiv.org/abs/2307.02037v3},
	abstract = {We propose a Monte Carlo sampler from the reverse diffusion process. Unlike the practice of diffusion models, where the intermediary updates -- the score functions -- are learned with a neural network, we transform the score matching problem into a mean estimation one. By estimating the means of the regularized posterior distributions, we derive a novel Monte Carlo sampling algorithm called reverse diffusion Monte Carlo (rdMC), which is distinct from the Markov chain Monte Carlo (MCMC) methods. We determine the sample size from the error tolerance and the properties of the posterior distribution to yield an algorithm that can approximately sample the target distribution with any desired accuracy. Additionally, we demonstrate and prove under suitable conditions that sampling with rdMC can be significantly faster than that with MCMC. For multi-modal target distributions such as those in Gaussian mixture models, rdMC greatly improves over the Langevin-style MCMC sampling methods both theoretically and in practice. The proposed rdMC method offers a new perspective and solution beyond classical MCMC algorithms for the challenging complex distributions.},
	language = {en},
	urldate = {2024-10-09},
	journal = {arXiv.org},
	author = {Huang, Xunpeng and Dong, Hanze and Hao, Yifan and Ma, Yi-An and Zhang, Tong},
	month = jul,
	year = {2023},
}

@book{couillet_random_2022-1,
	edition = {1},
	title = {Random {Matrix} {Methods} for {Machine} {Learning}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-00-912849-0 978-1-00-912323-5},
	url = {https://www.cambridge.org/core/product/identifier/9781009128490/type/book},
	abstract = {This book presents a unified theory of random matrices for applications in machine learning, offering a large-dimensional data vision that exploits concentration and universality phenomena. This enables a precise understanding, and possible improvements, of the core mechanisms at play in real-world machine learning algorithms. The book opens with a thorough introduction to the theoretical basics of random matrices, which serves as a support to a wide scope of applications ranging from SVMs, through semi-supervised learning, unsupervised spectral clustering, and graph methods, to neural networks and deep learning. For each application, the authors discuss small- versus large-dimensional intuitions of the problem, followed by a systematic random matrix analysis of the resulting performance and possible improvements. All concepts, applications, and variations are illustrated numerically on synthetic as well as real-world data, with MATLAB and Python code provided on the accompanying website.},
	language = {en},
	urldate = {2024-10-07},
	publisher = {Cambridge University Press},
	author = {Couillet, Romain and Liao, Zhenyu},
	month = jul,
	year = {2022},
	doi = {10.1017/9781009128490},
}

@misc{adlam_random_2021,
	title = {A {Random} {Matrix} {Perspective} on {Mixtures} of {Nonlinearities} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1912.00827},
	abstract = {One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired signiﬁcant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to inﬁnity. We analyze the performance of random feature regression with features F = f (W X + B) for a random weight matrix W and random bias vector B, obtaining exact formulae for the asymptotic training and test errors for data generated by a linear teacher model. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we ﬁnd that a mixture of nonlinearities can improve both the training and test errors over the best single nonlinearity, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Adlam, Ben and Levinson, Jake and Pennington, Jeffrey},
	month = nov,
	year = {2021},
	note = {arXiv:1912.00827 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{adlam_neural_2020,
	title = {The {Neural} {Tangent} {Kernel} in {High} {Dimensions}: {Triple} {Descent} and a {Multi}-{Scale} {Theory} of {Generalization}},
	shorttitle = {The {Neural} {Tangent} {Kernel} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/2008.06786},
	abstract = {Modern deep learning models employ considerably more parameters than required to ﬁt the training data. Whereas conventional statistical wisdom suggests such models should drastically overﬁt, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a double descent curve, in which increasing a model’s capacity causes its test error to ﬁrst decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has non-monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Adlam, Ben and Pennington, Jeffrey},
	month = aug,
	year = {2020},
	note = {arXiv:2008.06786 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{pennington_nonlinear_2017,
	title = {Nonlinear random matrix theory for deep learning},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html},
	urldate = {2024-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Pennington, Jeffrey and Worah, Pratik},
	year = {2017},
}

@article{barbier_optimal_2019,
	title = {Optimal {Errors} and {Phase} {Transitions} in {High}-{Dimensional} {Generalized} {Linear} {Models}},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1708.03395},
	doi = {10.1073/pnas.1802705116},
	abstract = {Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing. In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmark models in neural networks. We evaluate the mutual information (or "free entropy") from which we deduce the Bayes-optimal estimation and generalization errors. Our analysis applies to the high-dimensional limit where both the number of samples and the dimension are large and their ratio is fixed. Non-rigorous predictions for the optimal errors existed for special cases of GLMs, e.g. for the perceptron, in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions. We believe that this random version of GLMs can serve as a challenging benchmark for multi-purpose algorithms. This paper is divided in two parts that can be read independently: The first part (main part) presents the model and main results, discusses some applications and sketches the main ideas of the proof. The second part (supplementary informations) is much more detailed and provides more examples as well as all the proofs.},
	number = {12},
	urldate = {2024-10-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, Léo and Zdeborová, Lenka},
	month = mar,
	year = {2019},
	note = {arXiv:1708.03395 [cond-mat, physics:math-ph]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematical Physics},
	pages = {5451--5460},
}

@misc{wang_evaluating_2024-1,
	title = {Evaluating the design space of diffusion-based generative models},
	url = {http://arxiv.org/abs/2406.12839},
	doi = {10.48550/arXiv.2406.12839},
	abstract = {Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in [Karras et al. 2022]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in [Song et al. 2020] is more preferable, but when it is less trained, the design in [Karras et al. 2022] becomes more preferable.},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Wang, Yuqing and He, Ye and Tao, Molei},
	month = sep,
	year = {2024},
	note = {arXiv:2406.12839 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{noauthor_random_nodate,
	title = {Random {Matrix} {Theory} and {Wireless} {Communications}},
	url = {https://ieeexplore.ieee.org/document/8187216},
	abstract = {Random matrix theory has found many applications in physics, statistics and engineering since its inception. Although early developments were motivated by practical experimental problems, random matrices are now used in fields as diverse as Riemann hypothesis, stochastic differential equations, condensed matter physics, statistical physics, chaotic systems, numerical linear algebra, neural networks, multivariate statistics, information theory, signal processing and small-world networks. Random Matrix Theory and Wireless Communications is the first tutorial on random matrices which provides an overview of the theory and brings together in one source the most significant results recently obtained. Furthermore, the application of random matrix theory to the fundamental limits of wireless communication channels is described in depth. The authors have created a uniquely comprehensive work that provides the reader with a full understanding of the foundations of random matrix theory and demonstrates the trends of their applications, particularly in wireless communications. Random Matrix Theory and Wireless Communications is a valuable resource for all students and researchers working on the cutting edge of wireless communications.},
	language = {en-US},
	urldate = {2024-09-28},
}

@book{noauthor_antoines_nodate,
	title = {Antoine's thesis},
}

@misc{shah_learning_2023,
	title = {Learning {Mixtures} of {Gaussians} {Using} the {DDPM} {Objective}},
	url = {http://arxiv.org/abs/2307.01178},
	doi = {10.48550/arXiv.2307.01178},
	abstract = {Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed. In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in \$d\$ dimensions with \$1/{\textbackslash}text\{poly\}(d)\$-separated centers. 2) We show gradient descent with a warm start learns mixtures of \$K\$ spherical Gaussians with \${\textbackslash}Omega({\textbackslash}sqrt\{{\textbackslash}log({\textbackslash}min(K,d))\})\$-separated centers. A key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Shah, Kulin and Chen, Sitan and Klivans, Adam},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01178 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ruelle_mathematical_1987,
	title = {A mathematical reformulation of {Derrida}'s {REM} and {GREM}},
	volume = {108},
	issn = {1432-0916},
	url = {https://doi.org/10.1007/BF01210613},
	doi = {10.1007/BF01210613},
	abstract = {The large system limit of the Random Energy Model (REM) and generalized Random Energy Model (GREM) of Derrida is investigated, and found to be universal. This permits systematic calculations of relevance in particular to Parisi's solution of the Sherrington-Kirkpatrick spin-glass model.},
	language = {en},
	number = {2},
	urldate = {2024-09-17},
	journal = {Communications in Mathematical Physics},
	author = {Ruelle, David},
	month = jun,
	year = {1987},
	keywords = {Complex System, Neural Network, Nonlinear Dynamics, Quantum Computing, Statistical Physic},
	pages = {225--239},
}

@misc{scarvelis_closed-form_2023,
	title = {Closed-{Form} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2310.12395},
	abstract = {Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via scorematching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighborbased estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Scarvelis, Christopher and Borde, Haitz Sáez de Ocáriz and Solomon, Justin},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12395 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{han_neural_2024,
	title = {Neural {Network}-{Based} {Score} {Estimation} in {Diffusion} {Models}: {Optimization} and {Generalization}},
	shorttitle = {Neural {Network}-{Based} {Score} {Estimation} in {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2401.15604},
	abstract = {Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved ﬁdelity, ﬂexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradientbased algorithms can learn the score function with a provable accuracy. As a ﬁrst step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing existing techniques from being applied directly. In this paper, we show that with proper designs, the evolution of neural networks during training can be accurately modeled by a series of kernel regression tasks. Furthermore, by applying an early-stopping rule for gradient descent and leveraging recent developments in neural tangent kernels, we establish the ﬁrst generalization error (sample complexity) bounds for learning the score function with neural networks, despite the presence of noise in the observations. Our analysis is grounded in a novel parametric form of the neural network and an innovative connection between score matching and regression analysis, facilitating the application of advanced statistical and optimization techniques.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Han, Yinbin and Razaviyayn, Meisam and Xu, Renyuan},
	month = mar,
	year = {2024},
	note = {arXiv:2401.15604 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	language = {en},
	urldate = {2024-09-07},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@book{amit_modeling_1989,
	address = {Cambridge},
	title = {Modeling {Brain} {Function}: {The} {World} of {Attractor} {Neural} {Networks}},
	isbn = {978-0-521-42124-9},
	shorttitle = {Modeling {Brain} {Function}},
	url = {https://www.cambridge.org/core/books/modeling-brain-function/2EA95FDABF616D187220A6B9596091B7},
	abstract = {One of the most exciting and potentially rewarding areas of scientific research is the study of the principles and mechanisms underlying brain function. It is also of great promise to future generations of computers. A growing group of researchers, adapting knowledge and techniques from a wide range of scientific disciplines, have made substantial progress understanding memory, the learning process, and self organization by studying the properties of models of neural networks - idealized systems containing very large numbers of connected neurons, whose interactions give rise to the special qualities of the brain. This book introduces and explains the techniques brought from physics to the study of neural networks and the insights they have stimulated. It is written at a level accessible to the wide range of researchers working on these problems - statistical physicists, biologists, computer scientists, computer technologists and cognitive psychologists. The author presents a coherent and clear nonmechanical presentation of all the basic ideas and results. More technical aspects are restricted, wherever possible, to special sections and appendices in each chapter. The book is suitable as a text for graduate courses in physics, electrical engineering, computer science and biology.},
	urldate = {2024-09-06},
	publisher = {Cambridge University Press},
	author = {Amit, Daniel J.},
	year = {1989},
	doi = {10.1017/CBO9780511623257},
}

@article{krotov_new_2023,
	title = {A new frontier for {Hopfield} networks},
	volume = {5},
	copyright = {2023 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-023-00595-y},
	doi = {10.1038/s42254-023-00595-y},
	abstract = {Over the past few years there has been a resurgence of interest in Hopfield networks of associative memory. Dmitry Krotov discusses recent theoretical advances and their broader impact in the context of energy-based neural architectures.},
	language = {en},
	number = {7},
	urldate = {2024-09-05},
	journal = {Nature Reviews Physics},
	author = {Krotov, Dmitry},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Statistical physics},
	pages = {366--367},
}

@inproceedings{okawa_compositional_2023,
	title = {Compositional {Abilities} {Emerge} {Multiplicatively}: {Exploring} {Diffusion} {Models} on a {Synthetic} {Task}},
	shorttitle = {Compositional {Abilities} {Emerge} {Multiplicatively}},
	url = {https://openreview.net/forum?id=frVo9MzRuU},
	abstract = {Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhibits a sudden "emergence" due to multiplicative reliance on the performance of constituent tasks, partially explaining emergent phenomena seen in generative models; and (iii) composing concepts with lower frequency in the training data to generate out-of-distribution samples requires considerably more optimization steps compared to generating in-distribution samples. Overall, our study lays a foundation for understanding emergent capabilities and compositionality in generative models from a data-centric perspective.},
	language = {en},
	urldate = {2024-09-05},
	author = {Okawa, Maya and Lubana, Ekdeep Singh and Dick, Robert P. and Tanaka, Hidenori},
	month = nov,
	year = {2023},
}

@book{panchenko_sherrington-kirkpatrick_2013,
	address = {New York, NY},
	series = {Springer {Monographs} in {Mathematics}},
	title = {The {Sherrington}-{Kirkpatrick} {Model}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-1-4614-6288-0 978-1-4614-6289-7},
	url = {https://link.springer.com/10.1007/978-1-4614-6289-7},
	language = {en},
	urldate = {2024-09-03},
	publisher = {Springer},
	author = {Panchenko, Dmitry},
	year = {2013},
	doi = {10.1007/978-1-4614-6289-7},
	keywords = {Aizenman-Sims-Starr scheme, Aldous-Hoover representation, Dovbysh-Sudakov representation, Gaussian processes, Ghirlanda-Guerra identities, Guerra replica symmetry breaking, Parisi ansatz, Parisi formula, Poisson processes, Poisson-Dirichlet processes, Ruelle probability cascades, Sherrington-Kirkpatrick model, Talagrand positivity principle, exchangeability, p-spin models, replica symmetry breaking, spin glass models, ultrametricity},
}

@misc{negri_storage_2023,
	title = {Storage and {Learning} phase transitions in the {Random}-{Features} {Hopfield} {Model}},
	url = {http://arxiv.org/abs/2303.16880},
	abstract = {The Hopfield model is a paradigmatic model of neural networks that has been analyzed for many decades in the statistical physics, neuroscience, and machine learning communities. Inspired by the manifold hypothesis in machine learning, we propose and investigate a generalization of the standard setting that we name Random-Features Hopfield Model. Here \$P\$ binary patterns of length \$N\$ are generated by applying to Gaussian vectors sampled in a latent space of dimension \$D\$ a random projection followed by a non-linearity. Using the replica method from statistical physics, we derive the phase diagram of the model in the limit \$P,N,D{\textbackslash}to{\textbackslash}infty\$ with fixed ratios \${\textbackslash}alpha=P/N\$ and \${\textbackslash}alpha\_D=D/N\$. Besides the usual retrieval phase, where the patterns can be dynamically recovered from some initial corruption, we uncover a new phase where the features characterizing the projection can be recovered instead. We call this phenomena the learning phase transition, as the features are not explicitly given to the model but rather are inferred from the patterns in an unsupervised fashion.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Negri, Matteo and Lauditi, Clarissa and Perugini, Gabriele and Lucibello, Carlo and Malatesta, Enrico},
	month = apr,
	year = {2023},
	note = {arXiv:2303.16880 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
}

@misc{kalaj_random_2024,
	title = {Random {Features} {Hopfield} {Networks} generalize retrieval to previously unseen examples},
	url = {http://arxiv.org/abs/2407.05658},
	abstract = {It has been recently shown that a learning transition happens when a Hopfield Network stores examples generated as superpositions of random features, where new attractors corresponding to such features appear in the model. In this work we reveal that the network also develops attractors corresponding to previously unseen examples generated with the same set of features. We explain this surprising behaviour in terms of spurious states of the learned features: we argue that, increasing the number of stored examples beyond the learning transition, the model also learns to mix the features to represent both stored and previously unseen examples. We support this claim with the computation of the phase diagram of the model.},
	language = {en},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Kalaj, Silvio and Lauditi, Clarissa and Perugini, Gabriele and Lucibello, Carlo and Malatesta, Enrico M. and Negri, Matteo},
	month = jul,
	year = {2024},
	note = {arXiv:2407.05658 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
}

@misc{biroli_kernel_2024,
	title = {Kernel {Density} {Estimators} in {Large} {Dimensions}},
	url = {http://arxiv.org/abs/2408.05807},
	abstract = {This paper studies Kernel density estimation for a high-dimensional distribution \${\textbackslash}rho(x)\$. Traditional approaches have focused on the limit of large number of data points \$n\$ and fixed dimension \$d\$. We analyze instead the regime where both the number \$n\$ of data points \$y\_i\$ and their dimensionality \$d\$ grow with a fixed ratio \${\textbackslash}alpha=({\textbackslash}log n)/d\$. Our study reveals three distinct statistical regimes for the kernel-based estimate of the density \${\textbackslash}hat {\textbackslash}rho\_h{\textasciicircum}\{{\textbackslash}mathcal \{D\}\}(x)={\textbackslash}frac\{1\}\{n h{\textasciicircum}d\}{\textbackslash}sum\_\{i=1\}{\textasciicircum}n K{\textbackslash}left({\textbackslash}frac\{x-y\_i\}\{h\}{\textbackslash}right)\$, depending on the bandwidth \$h\$: a classical regime for large bandwidth where the Central Limit Theorem (CLT) holds, which is akin to the one found in traditional approaches. Below a certain value of the bandwidth, \$h\_\{CLT\}({\textbackslash}alpha)\$, we find that the CLT breaks down. The statistics of \${\textbackslash}hat {\textbackslash}rho\_h{\textasciicircum}\{{\textbackslash}mathcal \{D\}\}(x)\$ for a fixed \$x\$ drawn from \${\textbackslash}rho(x)\$ is given by a heavy-tailed distribution (an alpha-stable distribution). In particular below a value \$h\_G({\textbackslash}alpha)\$, we find that \${\textbackslash}hat {\textbackslash}rho\_h{\textasciicircum}\{{\textbackslash}mathcal \{D\}\}(x)\$ is governed by extreme value statistics: only a few points in the database matter and give the dominant contribution to the density estimator. We provide a detailed analysis for high-dimensional multivariate Gaussian data. We show that the optimal bandwidth threshold based on Kullback-Leibler divergence lies in the new statistical regime identified in this paper. Our findings reveal limitations of classical approaches, show the relevance of these new statistical regimes, and offer new insights for Kernel density estimation in high-dimensional settings.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Biroli, Giulio and Mézard, Marc},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05807 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{li_good_2024,
	title = {A {Good} {Score} {Does} not {Lead} to {A} {Good} {Generative} {Model}},
	url = {http://arxiv.org/abs/2401.04856},
	abstract = {Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Li, Sixu and Chen, Shi and Li, Qin},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04856 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_sampling_2023,
	title = {Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
	shorttitle = {Sampling is as easy as learning the score},
	url = {http://arxiv.org/abs/2209.11215},
	doi = {10.48550/arXiv.2209.11215},
	abstract = {We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL\${\textbackslash}cdot\$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an \$L{\textasciicircum}2\$-accurate score estimate (rather than \$L{\textasciicircum}{\textbackslash}infty\$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does not reduce the complexity of SGMs.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru R.},
	month = apr,
	year = {2023},
	note = {arXiv:2209.11215 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@misc{block_generative_2022,
	title = {Generative {Modeling} with {Denoising} {Auto}-{Encoders} and {Langevin} {Sampling}},
	url = {http://arxiv.org/abs/2002.00107},
	abstract = {We study convergence of a generative modeling method that ﬁrst estimates the score function of the distribution using Denoising Auto-Encoders (DAE) or Denoising Score Matching (DSM) and then employs Langevin diffusion for sampling. We show that both DAE and DSM provide estimates of the score of the Gaussian smoothed population density, allowing us to apply the machinery of Empirical Processes. We overcome the challenge of relying only on L2 bounds on the score estimation error and provide ﬁnite-sample bounds in the Wasserstein distance between the law of the population distribution and the law of this sampling scheme. We then apply our results to the homotopy method of [SE19] and provide theoretical justiﬁcation for its empirical success.},
	language = {en},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Block, Adam and Mroueh, Youssef and Rakhlin, Alexander},
	month = oct,
	year = {2022},
	note = {arXiv:2002.00107 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{block_generative_2022-1,
	title = {Generative {Modeling} with {Denoising} {Auto}-{Encoders} and {Langevin} {Sampling}},
	url = {http://arxiv.org/abs/2002.00107},
	abstract = {We study convergence of a generative modeling method that ﬁrst estimates the score function of the distribution using Denoising Auto-Encoders (DAE) or Denoising Score Matching (DSM) and then employs Langevin diffusion for sampling. We show that both DAE and DSM provide estimates of the score of the Gaussian smoothed population density, allowing us to apply the machinery of Empirical Processes. We overcome the challenge of relying only on L2 bounds on the score estimation error and provide ﬁnite-sample bounds in the Wasserstein distance between the law of the population distribution and the law of this sampling scheme. We then apply our results to the homotopy method of [SE19] and provide theoretical justiﬁcation for its empirical success.},
	language = {en},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Block, Adam and Mroueh, Youssef and Rakhlin, Alexander},
	month = oct,
	year = {2022},
	note = {arXiv:2002.00107 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{yang_convergence_2023,
	title = {Convergence of the {Inexact} {Langevin} {Algorithm} and {Score}-based {Generative} {Models} in {KL} {Divergence}},
	url = {http://arxiv.org/abs/2211.01512},
	doi = {10.48550/arXiv.2211.01512},
	abstract = {We study the Inexact Langevin Dynamics (ILD), Inexact Langevin Algorithm (ILA), and Score-based Generative Modeling (SGM) when utilizing estimated score functions for sampling. Our focus lies in establishing stable biased convergence guarantees in terms of the Kullback-Leibler (KL) divergence. To achieve these guarantees, we impose two key assumptions: 1) the target distribution satisfies the log-Sobolev inequality (LSI), and 2) the score estimator exhibits a bounded Moment Generating Function (MGF) error. Notably, the MGF error assumption we adopt is more lenient compared to the \$L{\textasciicircum}{\textbackslash}infty\$ error assumption used in existing literature. However, it is stronger than the \$L{\textasciicircum}2\$ error assumption utilized in recent works, which often leads to unstable bounds. We explore the question of how to obtain a provably accurate score estimator that satisfies the MGF error assumption. Specifically, we demonstrate that a simple estimator based on kernel density estimation fulfills the MGF error assumption for sub-Gaussian target distribution, at the population level.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Yang, Kaylee Yingxi and Wibisono, Andre},
	month = jun,
	year = {2023},
	note = {arXiv:2211.01512 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@misc{conforti_score_2023,
	title = {Score diffusion models without early stopping: finite {Fisher} information is all you need},
	shorttitle = {Score diffusion models without early stopping},
	url = {http://arxiv.org/abs/2308.12240},
	doi = {10.48550/arXiv.2308.12240},
	abstract = {Diffusion models are a new class of generative models that revolve around the estimation of the score function associated with a stochastic differential equation. Subsequent to its acquisition, the approximated score function is then harnessed to simulate the corresponding time-reversal process, ultimately enabling the generation of approximate data samples. Despite their evident practical significance these models carry, a notable challenge persists in the form of a lack of comprehensive quantitative results, especially in scenarios involving non-regular scores and estimators. In almost all reported bounds in Kullback Leibler (KL) divergence, it is assumed that either the score function or its approximation is Lipschitz uniformly in time. However, this condition is very restrictive in practice or appears to be difficult to establish. To circumvent this issue, previous works mainly focused on establishing convergence bounds in KL for an early stopped version of the diffusion model and a smoothed version of the data distribution, or assuming that the data distribution is supported on a compact manifold. These explorations have lead to interesting bounds in either Wasserstein or Fortet-Mourier metrics. However, the question remains about the relevance of such early-stopping procedure or compactness conditions. In particular, if there exist a natural and mild condition ensuring explicit and sharp convergence bounds in KL. In this article, we tackle the aforementioned limitations by focusing on score diffusion models with fixed step size stemming from the Ornstein-Ulhenbeck semigroup and its kinetic counterpart. Our study provides a rigorous analysis, yielding simple, improved and sharp convergence bounds in KL applicable to any data distribution with finite Fisher information with respect to the standard Gaussian distribution.},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Conforti, Giovanni and Durmus, Alain and Silveri, Marta Gentiloni},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12240 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{gu_memorization_2023,
	title = {On {Memorization} in {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2310.02664},
	abstract = {Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models. Code is available at https://github.com/sail-sg/DiffMemorize.},
	language = {en},
	urldate = {2024-08-17},
	publisher = {arXiv},
	author = {Gu, Xiangming and Du, Chao and Pang, Tianyu and Li, Chongxuan and Lin, Min and Wang, Ye},
	month = oct,
	year = {2023},
	note = {arXiv:2310.02664 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{benton_denoising_2024,
	title = {From {Denoising} {Diffusions} to {Denoising} {Markov} {Models}},
	url = {http://arxiv.org/abs/2211.03595},
	doi = {10.48550/arXiv.2211.03595},
	abstract = {Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.},
	urldate = {2024-08-15},
	publisher = {arXiv},
	author = {Benton, Joe and Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
	month = feb,
	year = {2024},
	note = {arXiv:2211.03595 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{klebaner_introduction_2012,
	address = {London},
	edition = {Third edition},
	title = {Introduction to stochastic calculus with applications},
	isbn = {978-1-84816-831-2 978-1-84816-832-9},
	language = {en},
	publisher = {ICP, Imperial College Press},
	author = {Klebaner, Fima C.},
	year = {2012},
}

@book{karatzas_brownian_1998,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Brownian {Motion} and {Stochastic} {Calculus}},
	volume = {113},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-97655-6 978-1-4612-0949-2},
	url = {http://link.springer.com/10.1007/978-1-4612-0949-2},
	language = {en},
	urldate = {2024-08-15},
	publisher = {Springer},
	author = {Karatzas, Ioannis and Shreve, Steven E.},
	year = {1998},
	doi = {10.1007/978-1-4612-0949-2},
	keywords = {Brownian motion, Markov process, Martingal, Martingale, Semimartingal, Semimartingale, YellowSale2006, adopted-textbook, differential equation, integration, local time, measure, probability, stochastic calculus, stochastic differential equation, stochastic process, stochastic processes},
}

@book{karatzas_brownian_1998-1,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Brownian {Motion} and {Stochastic} {Calculus}},
	volume = {113},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-97655-6 978-1-4612-0949-2},
	url = {http://link.springer.com/10.1007/978-1-4612-0949-2},
	language = {en},
	urldate = {2024-08-15},
	publisher = {Springer New York},
	author = {Karatzas, Ioannis and Shreve, Steven E.},
	year = {1998},
	doi = {10.1007/978-1-4612-0949-2},
}

@book{steele_stochastic_2001,
	address = {New York, NY},
	title = {Stochastic {Calculus} and {Financial} {Applications}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-2862-7 978-1-4684-9305-4},
	url = {http://link.springer.com/10.1007/978-1-4684-9305-4},
	urldate = {2024-08-15},
	publisher = {Springer},
	author = {Steele, J. Michael},
	year = {2001},
	doi = {10.1007/978-1-4684-9305-4},
	keywords = {Stochastic Differential Equations, Stochastic Processes, Stochastic calculus, Uniform integrability, Variance, calculus, quantitative finance, statistics},
}

@book{le_gall_brownian_2016,
	address = {Cham},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Brownian {Motion}, {Martingales}, and {Stochastic} {Calculus}},
	volume = {274},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-31088-6 978-3-319-31089-3},
	url = {http://link.springer.com/10.1007/978-3-319-31089-3},
	language = {en},
	urldate = {2024-08-15},
	publisher = {Springer International Publishing},
	author = {Le Gall, Jean-François},
	year = {2016},
	doi = {10.1007/978-3-319-31089-3},
	keywords = {Brownian motion, Itô's formula, Markov process, harmonic function, martingale, martingale representation, quantitative finance, stochastic calculus, stochastic differential equation, stochastic integral},
}

@techreport{van_handel_probability_2014,
	address = {Fort Belvoir, VA},
	title = {Probability in {High} {Dimension}:},
	shorttitle = {Probability in {High} {Dimension}},
	url = {http://www.dtic.mil/docs/citations/ADA623999},
	language = {en},
	urldate = {2024-08-15},
	institution = {Defense Technical Information Center},
	author = {Van Handel, Ramon},
	month = jun,
	year = {2014},
	doi = {10.21236/ADA623999},
}

@book{bakry_analysis_2014,
	address = {Cham},
	series = {Grundlehren der mathematischen {Wissenschaften}},
	title = {Analysis and {Geometry} of {Markov} {Diffusion} {Operators}},
	volume = {348},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-319-00226-2 978-3-319-00227-9},
	url = {https://link.springer.com/10.1007/978-3-319-00227-9},
	language = {en},
	urldate = {2024-08-15},
	publisher = {Springer International Publishing},
	author = {Bakry, Dominique and Gentil, Ivan and Ledoux, Michel},
	year = {2014},
	doi = {10.1007/978-3-319-00227-9},
	keywords = {39B62, 39B72, 47D07, 53C21, Markov operators, curvature dimension condition, diffusion operators, functional inequalities, partial differential equations},
}

@book{kolokoltsov_markov_2011,
	title = {Markov {Processes}, {Semigroups} and {Generators}},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-3-11-025011-4},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110250114/html?lang=en},
	abstract = {Markov processes represent a universal model for a large variety of real life random evolutions. The wide flow of new ideas, tools, methods and applications constantly pours into the ever-growing stream of research on Markov processes that rapidly spreads over new fields of natural and social sciences, creating new streamlined logical paths to its turbulent boundary. Even if a given process is not Markov, it can be often inserted into a larger Markov one (Markovianization procedure) by including the key historic parameters into the state space. This monograph gives a concise, but systematic and self-contained, exposition of the essentials of Markov processes, together with recent achievements, working from the "physical picture" - a formal pre-generator, and stressing the interplay between probabilistic (stochastic differential equations) and analytic (semigroups) tools. The book will be useful to students and researchers. Part I can be used for a one-semester course on Brownian motion, Lévy and Markov processes, or on probabilistic methods for PDE. Part II mainly contains the author's research on Markov processes. From the contents: Tools from Probability and Analysis Brownian motion Markov processes and martingales SDE, ψ DE and martingale problems Processes in Euclidean spaces Processes in domains with a boundary Heat kernels for stable-like processes Continuous-time random walks and fractional dynamics Complex chains and Feynman integral},
	language = {en},
	urldate = {2024-08-15},
	publisher = {De Gruyter},
	author = {Kolokoltsov, Vassili N.},
	month = mar,
	year = {2011},
	doi = {10.1515/9783110250114},
	keywords = {Generators, Markov Processes, Semigroups},
}

@article{ledoux_geometry_2000,
	title = {The geometry of {Markov} diffusion generators},
	volume = {9},
	issn = {0240-2963},
	url = {https://afst.centre-mersenne.org/item/AFST_2000_6_9_2_305_0/},
	doi = {10.5802/afst.962},
	language = {en},
	number = {2},
	urldate = {2024-08-15},
	journal = {Annales de la faculté des sciences de Toulouse Mathématiques},
	author = {Ledoux, Michel},
	year = {2000},
	pages = {305--366},
}

@book{han_information-spectrum_2003,
	address = {Berlin, Heidelberg},
	series = {Stochastic {Modelling} and {Applied} {Probability}},
	title = {Information-{Spectrum} {Methods} in {Information} {Theory}},
	volume = {50},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-07812-5 978-3-662-12066-8},
	url = {http://link.springer.com/10.1007/978-3-662-12066-8},
	urldate = {2024-08-14},
	publisher = {Springer},
	author = {Han, Te Sun},
	editor = {Rozovskii, B. and Yor, M.},
	year = {2003},
	doi = {10.1007/978-3-662-12066-8},
	keywords = {Code, Information, Shannon, channel coding, coding, coding theory, communication, complexity, decoder, encoder, entropy, information and communication, circuits, information theory, information-spectrum, linear optimization, source coding},
}

@misc{weed_sharp_2017,
	title = {Sharp asymptotic and finite-sample rates of convergence of empirical measures in {Wasserstein} distance},
	url = {http://arxiv.org/abs/1707.00087},
	abstract = {The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from n independent samples from µ approaches µ in the Wasserstein distance of any order. We prove sharp asymptotic and ﬁnite-sample results for this rate of convergence for general measures on general compact metric spaces. Our ﬁnite-sample results show the existence of multi-scale behavior, where measures can exhibit radically diﬀerent rates of convergence as n grows.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Weed, Jonathan and Bach, Francis},
	month = jun,
	year = {2017},
	note = {arXiv:1707.00087 [math, stat]},
	keywords = {60B10, 62E17, Mathematics - Probability, Mathematics - Statistics Theory},
}

@misc{chewi_statistical_2024,
	title = {Statistical optimal transport},
	url = {http://arxiv.org/abs/2407.18163},
	abstract = {We present an introduction to the field of statistical optimal transport, based on lectures given at {\textbackslash}'Ecole d'{\textbackslash}'Et{\textbackslash}'e de Probabilit{\textbackslash}'es de Saint-Flour XLIX.},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Chewi, Sinho and Niles-Weed, Jonathan and Rigollet, Philippe},
	month = jul,
	year = {2024},
	note = {arXiv:2407.18163 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@inproceedings{cole_score-based_2023,
	title = {Score-based generative models break the curse of dimensionality in learning a family of sub-{Gaussian} distributions},
	url = {https://openreview.net/forum?id=wG12xUSqrI},
	abstract = {While score-based generative models (SGMs) have achieved remarkable successes in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a measure of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep network approximation rate for the true score function associated to the forward process, which is interesting in its own right.},
	language = {en},
	urldate = {2024-08-13},
	author = {Cole, Frank and Lu, Yulong},
	month = oct,
	year = {2023},
}

@misc{wibisono_optimal_2024,
	title = {Optimal score estimation via empirical {Bayes} smoothing},
	url = {http://arxiv.org/abs/2402.07747},
	abstract = {We study the problem of estimating the score function of an unknown probability distribution \${\textbackslash}rho{\textasciicircum}*\$ from \$n\$ independent and identically distributed observations in \$d\$ dimensions. Assuming that \${\textbackslash}rho{\textasciicircum}*\$ is subgaussian and has a Lipschitz-continuous score function \$s{\textasciicircum}*\$, we establish the optimal rate of \${\textbackslash}tilde {\textbackslash}Theta(n{\textasciicircum}\{-{\textbackslash}frac\{2\}\{d+4\}\})\$ for this estimation problem under the loss function \${\textbackslash}{\textbar}{\textbackslash}hat s - s{\textasciicircum}*{\textbackslash}{\textbar}{\textasciicircum}2\_\{L{\textasciicircum}2({\textbackslash}rho{\textasciicircum}*)\}\$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension \$d\$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss extensions to estimating \${\textbackslash}beta\$-H{\textbackslash}"older continuous scores with \${\textbackslash}beta {\textbackslash}leq 1\$, as well as the implication of our theory on the sample complexity of score-based generative models.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Wibisono, Andre and Wu, Yihong and Yang, Kaylee Yingxi},
	month = jun,
	year = {2024},
	note = {arXiv:2402.07747 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{block_rate_2023,
	title = {Rate of convergence of the smoothed empirical {Wasserstein} distance},
	url = {http://arxiv.org/abs/2205.02128},
	abstract = {Consider an empirical measure Pn induced by n iid samples from a d-dimensional K-subgaussian distribution P and let γ = N (0, σ2Id) be the isotropic Gaussian measure. We study the speed of convergence of the smoothed Wasserstein distance W2(Pn ∗ γ, P ∗ γ) = n−α+o(1) with ∗ being the convolution of measures.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Block, Adam and Jia, Zeyu and Polyanskiy, Yury and Rakhlin, Alexander},
	month = jul,
	year = {2023},
	note = {arXiv:2205.02128 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory},
}

@misc{goldfeld_convergence_2020,
	title = {Convergence of {Smoothed} {Empirical} {Measures} with {Applications} to {Entropy} {Estimation}},
	url = {http://arxiv.org/abs/1905.13576},
	abstract = {This paper studies convergence of empirical measures smoothed by a Gaussian kernel. Specifically, consider approximating \$P{\textbackslash}ast{\textbackslash}mathcal\{N\}\_{\textbackslash}sigma\$, for \${\textbackslash}mathcal\{N\}\_{\textbackslash}sigma{\textbackslash}triangleq{\textbackslash}mathcal\{N\}(0,{\textbackslash}sigma{\textasciicircum}2 {\textbackslash}mathrm\{I\}\_d)\$, by \${\textbackslash}hat\{P\}\_n{\textbackslash}ast{\textbackslash}mathcal\{N\}\_{\textbackslash}sigma\$, where \${\textbackslash}hat\{P\}\_n\$ is the empirical measure, under different statistical distances. The convergence is examined in terms of the Wasserstein distance, total variation (TV), Kullback-Leibler (KL) divergence, and \${\textbackslash}chi{\textasciicircum}2\$-divergence. We show that the approximation error under the TV distance and 1-Wasserstein distance (\${\textbackslash}mathsf\{W\}\_1\$) converges at rate \$e{\textasciicircum}\{O(d)\}n{\textasciicircum}\{-{\textbackslash}frac\{1\}\{2\}\}\$ in remarkable contrast to a typical \$n{\textasciicircum}\{-{\textbackslash}frac\{1\}\{d\}\}\$ rate for unsmoothed \${\textbackslash}mathsf\{W\}\_1\$ (and \$d{\textbackslash}ge 3\$). For the KL divergence, squared 2-Wasserstein distance (\${\textbackslash}mathsf\{W\}\_2{\textasciicircum}2\$), and \${\textbackslash}chi{\textasciicircum}2\$-divergence, the convergence rate is \$e{\textasciicircum}\{O(d)\}n{\textasciicircum}\{-1\}\$, but only if \$P\$ achieves finite input-output \${\textbackslash}chi{\textasciicircum}2\$ mutual information across the additive white Gaussian noise channel. If the latter condition is not met, the rate changes to \${\textbackslash}omega(n{\textasciicircum}\{-1\})\$ for the KL divergence and \${\textbackslash}mathsf\{W\}\_2{\textasciicircum}2\$, while the \${\textbackslash}chi{\textasciicircum}2\$-divergence becomes infinite - a curious dichotomy. As a main application we consider estimating the differential entropy \$h(P{\textbackslash}ast{\textbackslash}mathcal\{N\}\_{\textbackslash}sigma)\$ in the high-dimensional regime. The distribution \$P\$ is unknown but \$n\$ i.i.d samples from it are available. We first show that any good estimator of \$h(P{\textbackslash}ast{\textbackslash}mathcal\{N\}\_{\textbackslash}sigma)\$ must have sample complexity that is exponential in \$d\$. Using the empirical approximation results we then show that the absolute-error risk of the plug-in estimator converges at the parametric rate \$e{\textasciicircum}\{O(d)\}n{\textasciicircum}\{-{\textbackslash}frac\{1\}\{2\}\}\$, thus establishing the minimax rate-optimality of the plug-in. Numerical results that demonstrate a significant empirical superiority of the plug-in approach to general-purpose differential entropy estimators are provided.},
	language = {en},
	urldate = {2024-08-13},
	publisher = {arXiv},
	author = {Goldfeld, Ziv and Greenewald, Kristjan and Polyanskiy, Yury and Weed, Jonathan},
	month = may,
	year = {2020},
	note = {arXiv:1905.13576 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
}

@book{noauthor_controlled_2006,
	address = {New York},
	series = {Stochastic {Modelling} and {Applied} {Probability}},
	title = {Controlled {Markov} {Processes} and {Viscosity} {Solutions}},
	volume = {25},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-26045-7},
	url = {http://link.springer.com/10.1007/0-387-31071-1},
	language = {en},
	urldate = {2024-08-07},
	publisher = {Springer-Verlag},
	year = {2006},
	doi = {10.1007/0-387-31071-1},
	keywords = {Markov Chain, Markov process, Optimal control, control, control theory, optimization, programming, quantitative finance},
}

@article{nolen_partial_nodate,
	title = {Partial {Diﬀerential} {Equations} and {Diﬀusion} {Processes}},
	language = {en},
	author = {Nolen, James},
}

@article{bach_learning_nodate,
	title = {Learning {Theory} from {First} {Principles}},
	language = {en},
	author = {Bach, Francis},
}

@misc{chen_sampling_2023,
	title = {Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
	shorttitle = {Sampling is as easy as learning the score},
	url = {http://arxiv.org/abs/2209.11215},
	doi = {10.48550/arXiv.2209.11215},
	abstract = {We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL\${\textbackslash}cdot\$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an \$L{\textasciicircum}2\$-accurate score estimate (rather than \$L{\textasciicircum}{\textbackslash}infty\$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does not reduce the complexity of SGMs.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru R.},
	month = apr,
	year = {2023},
	note = {arXiv:2209.11215 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@misc{blessing_beyond_2024,
	title = {Beyond {ELBOs}: {A} {Large}-{Scale} {Evaluation} of {Variational} {Methods} for {Sampling}},
	shorttitle = {Beyond {ELBOs}},
	url = {http://arxiv.org/abs/2406.07423},
	abstract = {Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions. However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks, complicating the assessment of progress and hindering the decision-making of practitioners. In response to these challenges, our work introduces a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria. Moreover, we study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose. Our findings provide insights into strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments. The code is publicly available here.},
	language = {en},
	urldate = {2024-08-03},
	publisher = {arXiv},
	author = {Blessing, Denis and Jia, Xiaogang and Esslinger, Johannes and Vargas, Francisco and Neumann, Gerhard},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07423 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{song_maximum_2021,
	title = {Maximum {Likelihood} {Training} of {Score}-{Based} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2101.09258},
	abstract = {Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing ﬂows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a speciﬁc weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32 ˆ 32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.},
	language = {en},
	urldate = {2024-07-27},
	publisher = {arXiv},
	author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
	month = oct,
	year = {2021},
	note = {arXiv:2101.09258 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{wasserman_all_2004,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {All of {Statistics}: {A} {Concise} {Course} in {Statistical} {Inference}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-2322-6 978-0-387-21736-9},
	shorttitle = {All of {Statistics}},
	url = {http://link.springer.com/10.1007/978-0-387-21736-9},
	language = {en},
	urldate = {2024-07-26},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2004},
	doi = {10.1007/978-0-387-21736-9},
	keywords = {Bootstrapping, Mathematica, ROOT, Random variable, STATISTICA, classification, data mining, machine learning, mathematical statistics},
}

@inproceedings{yang_generalization_2022,
	title = {Generalization and {Memorization}: {The} {Bias} {Potential} {Model}},
	shorttitle = {Generalization and {Memorization}},
	url = {https://proceedings.mlr.press/v145/yang22a.html},
	abstract = {Models for learning probability distributions such as generative models and density estimators be- have quite differently from models for learning functions. One example is found in the memo- rization phenomenon, namely the ultimate convergence to the empirical distribution, that occurs in generative adversarial networks (GANs). For this reason, the issue of generalization is more subtle than that for supervised learning. For the bias potential model, we show that dimension- independent generalization accuracy is achievable if early stopping is adopted, despite that in the long term, the model either memorizes the samples or diverges.},
	language = {en},
	urldate = {2024-07-26},
	booktitle = {Proceedings of the 2nd {Mathematical} and {Scientific} {Machine} {Learning} {Conference}},
	publisher = {PMLR},
	author = {Yang, Hongkang and E, Weinan},
	month = apr,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1013--1043},
}

@misc{yi_generalization_2023,
	title = {On the {Generalization} of {Diffusion} {Model}},
	url = {http://arxiv.org/abs/2305.14712},
	doi = {10.48550/arXiv.2305.14712},
	abstract = {The diffusion probabilistic generative models are widely used to generate high-quality data. Though they can synthetic data that does not exist in the training set, the rationale behind such generalization is still unexplored. In this paper, we formally define the generalization of the generative model, which is measured by the mutual information between the generated data and the training set. The definition originates from the intuition that the model which generates data with less correlation to the training set exhibits better generalization ability. Meanwhile, we show that for the empirical optimal diffusion model, the data generated by a deterministic sampler are all highly related to the training set, thus poor generalization. This result contradicts the observation of the trained diffusion model's (approximating empirical optima) extrapolation ability (generating unseen data). To understand this contradiction, we empirically verify the difference between the sufficiently trained diffusion model and the empirical optima. We found, though obtained through sufficient training, there still exists a slight difference between them, which is critical to making the diffusion model generalizable. Moreover, we propose another training objective whose empirical optimal solution has no potential generalization problem. We empirically show that the proposed training objective returns a similar model to the original one, which further verifies the generalization ability of the trained diffusion model.},
	urldate = {2024-07-26},
	publisher = {arXiv},
	author = {Yi, Mingyang and Sun, Jiacheng and Li, Zhenguo},
	month = may,
	year = {2023},
	note = {arXiv:2305.14712 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@book{devroye_combinatorial_2001,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Combinatorial {Methods} in {Density} {Estimation}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-6527-6 978-1-4613-0125-7},
	url = {http://link.springer.com/10.1007/978-1-4613-0125-7},
	urldate = {2024-07-26},
	publisher = {Springer},
	author = {Devroye, Luc and Lugosi, Gábor},
	year = {2001},
	doi = {10.1007/978-1-4613-0125-7},
	keywords = {Density Estimation, Likelihood, Maxima, Probability theory, Variance},
}

@book{noauthor_notitle_nodate,
}

@misc{de_bortoli_convergence_2023,
	title = {Convergence of denoising diffusion models under the manifold hypothesis},
	url = {http://arxiv.org/abs/2208.05314},
	abstract = {Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference measure, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.},
	language = {en},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {De Bortoli, Valentin},
	month = may,
	year = {2023},
	note = {arXiv:2208.05314 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@article{lucibello_exponential_2024,
	title = {The {Exponential} {Capacity} of {Dense} {Associative} {Memories}},
	volume = {132},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/2304.14964},
	doi = {10.1103/PhysRevLett.132.077301},
	abstract = {Recent generalizations of the Hopfield model of associative memories are able to store a number \$P\$ of random patterns that grows exponentially with the number \$N\$ of neurons, \$P={\textbackslash}exp({\textbackslash}alpha N)\$. Besides the huge storage capacity, another interesting feature of these networks is their connection to the attention mechanism which is part of the Transformer architectures widely applied in deep learning. In this work, we study a generic family of pattern ensembles using a statistical mechanics analysis which gives exact asymptotic thresholds for the retrieval of a typical pattern, \${\textbackslash}alpha\_1\$, and lower bounds for the maximum of the load \${\textbackslash}alpha\$ for which all patterns can be retrieved, \${\textbackslash}alpha\_c\$, as well as sizes of attraction basins. We discuss in detail the cases of Gaussian and spherical patterns, and show that they display rich and qualitatively different phase diagrams.},
	language = {en},
	number = {7},
	urldate = {2024-07-22},
	journal = {Physical Review Letters},
	author = {Lucibello, Carlo and Mézard, Marc},
	month = feb,
	year = {2024},
	note = {arXiv:2304.14964 [cond-mat]},
	keywords = {Computer Science - Information Theory, Condensed Matter - Disordered Systems and Neural Networks},
	pages = {077301},
}

@misc{sclocchi_phase_2024,
	title = {A {Phase} {Transition} in {Diffusion} {Models} {Reveals} the {Hierarchical} {Nature} of {Data}},
	url = {http://arxiv.org/abs/2402.16991},
	abstract = {Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time t is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the generated sample may still be composed of low-level elements of the initial image. We validate these theoretical insights through numerical experiments on class-unconditional ImageNet diffusion models. Our analysis characterises the relationship between time and scale in diffusion models and puts forward generative models as powerful tools to model combinatorial data properties.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Sclocchi, Antonio and Favero, Alessandro and Wyart, Matthieu},
	month = mar,
	year = {2024},
	note = {arXiv:2402.16991 [cond-mat, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
}

@misc{pidstrigach_score-based_2022,
	title = {Score-{Based} {Generative} {Models} {Detect} {Manifolds}},
	url = {http://arxiv.org/abs/2206.01018},
	doi = {10.48550/arXiv.2206.01018},
	abstract = {Score-based generative models (SGMs) need to approximate the scores \${\textbackslash}nabla {\textbackslash}log p\_t\$ of the intermediate distributions as well as the final distribution \$p\_T\$ of the forward process. The theoretical underpinnings of the effects of these approximations are still lacking. We find precise conditions under which SGMs are able to produce samples from an underlying (low-dimensional) data manifold \${\textbackslash}mathcal\{M\}\$. This assures us that SGMs are able to generate the "right kind of samples". For example, taking \${\textbackslash}mathcal\{M\}\$ to be the subset of images of faces, we find conditions under which the SGM robustly produces an image of a face, even though the relative frequencies of these images might not accurately represent the true data generating distribution. Moreover, this analysis is a first step towards understanding the generalization properties of SGMs: Taking \${\textbackslash}mathcal\{M\}\$ to be the set of all training samples, our results provide a precise description of when the SGM memorizes its training data.},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Pidstrigach, Jakiw},
	month = oct,
	year = {2022},
	note = {arXiv:2206.01018 [cs, math, stat]},
	keywords = {68T99, Computer Science - Machine Learning, I.2.0, Mathematics - Numerical Analysis, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{goldt_gaussian_2021,
	title = {The {Gaussian} equivalence of generative models for learning with shallow neural networks},
	url = {http://arxiv.org/abs/2006.14709},
	abstract = {Understanding the impact of data structure on the computational tractability of learning is a key challenge for the theory of neural networks. Many theoretical works do not explicitly model training data, or assume that inputs are drawn component-wise independently from some simple probability distribution. Here, we go beyond this simple paradigm by studying the performance of neural networks trained on data drawn from pre-trained generative models. This is possible due to a Gaussian equivalence stating that the key metrics of interest, such as the training and test errors, can be fully captured by an appropriately chosen Gaussian model. We provide three strands of rigorous, analytical and numerical evidence corroborating this equivalence. First, we establish rigorous conditions for the Gaussian equivalence to hold in the case of single-layer generative models, as well as deterministic rates for convergence in distribution. Second, we leverage this equivalence to derive a closed set of equations describing the generalisation performance of two widely studied machine learning problems: two-layer neural networks trained using one-pass stochastic gradient descent, and full-batch pre-learned features or kernel methods. Finally, we perform experiments demonstrating how our theory applies to deep, pre-trained generative models. These results open a viable path to the theoretical study of machine learning models with realistic data.},
	language = {en},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and Mézard, Marc and Zdeborová, Lenka},
	month = may,
	year = {2021},
	note = {arXiv:2006.14709 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@article{goldt_modelling_2020,
	title = {Modelling the influence of data structure on learning in neural networks: the hidden manifold model},
	volume = {10},
	issn = {2160-3308},
	shorttitle = {Modelling the influence of data structure on learning in neural networks},
	url = {http://arxiv.org/abs/1909.11500},
	doi = {10.1103/PhysRevX.10.041044},
	abstract = {Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterised by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data, or assumes that elements of each data sample are drawn independently from some factorised probability distribution. These approaches are thus by construction blind to the correlation structure of real-world data sets and their impact on learning in neural networks. Here, we introduce a generative model for structured data sets that we call the hidden manifold model (HMM). The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a "Gaussian Equivalence Property" (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This permits us to analyse in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.},
	language = {en},
	number = {4},
	urldate = {2024-07-15},
	journal = {Physical Review X},
	author = {Goldt, Sebastian and Mézard, Marc and Krzakala, Florent and Zdeborová, Lenka},
	month = dec,
	year = {2020},
	note = {arXiv:1909.11500 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
	pages = {041044},
}

@misc{han_neural_2024,
	title = {Neural {Network}-{Based} {Score} {Estimation} in {Diffusion} {Models}: {Optimization} and {Generalization}},
	shorttitle = {Neural {Network}-{Based} {Score} {Estimation} in {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2401.15604},
	abstract = {Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved ﬁdelity, ﬂexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradientbased algorithms can learn the score function with a provable accuracy. As a ﬁrst step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing existing techniques from being applied directly. In this paper, we show that with proper designs, the evolution of neural networks during training can be accurately modeled by a series of kernel regression tasks. Furthermore, by applying an early-stopping rule for gradient descent and leveraging recent developments in neural tangent kernels, we establish the ﬁrst generalization error (sample complexity) bounds for learning the score function with neural networks, despite the presence of noise in the observations. Our analysis is grounded in a novel parametric form of the neural network and an innovative connection between score matching and regression analysis, facilitating the application of advanced statistical and optimization techniques.},
	language = {en},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Han, Yinbin and Razaviyayn, Meisam and Xu, Renyuan},
	month = mar,
	year = {2024},
	note = {arXiv:2401.15604 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_overview_2024,
	title = {An {Overview} of {Diffusion} {Models}: {Applications}, {Guided} {Generation}, {Statistical} {Rates} and {Optimization}},
	shorttitle = {An {Overview} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2404.07771},
	abstract = {Diffusion models, a powerful and universal generative AI technology, have achieved tremendous success in computer vision, audio, reinforcement learning, and computational biology. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of diffusion models is very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models, understanding their sample generation under various controls. Next, we overview the existing theories of diffusion models, covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional diffusion models and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of diffusion models.},
	language = {en},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Chen, Minshuo and Mei, Song and Fan, Jianqing and Wang, Mengdi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07771 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{li_generalization_2024,
	title = {On the {Generalization} {Properties} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2311.01797},
	abstract = {Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error (O(n−2/5 + m−4/5)) on both the sample size n and the model capacity m, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. This precisely elucidates the adverse effect of “modes shift” in ground truths on the model generalization. Moreover, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. Our findings contribute to the rigorous understanding of diffusion models’ generalization properties and provide insights that may guide practical applications.},
	language = {en},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Li, Puheng and Li, Zhong and Zhang, Huishuai and Bian, Jiang},
	month = jan,
	year = {2024},
	note = {arXiv:2311.01797 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_probability_2023,
	title = {The probability flow {ODE} is provably fast},
	url = {http://arxiv.org/abs/2305.11798},
	abstract = {We provide the ﬁrst polynomial-time convergence guarantees for the probability ﬂow ODE implementation (together with a corrector step) of score-based generative modeling. Our analysis is carried out in the wake of recent results obtaining such guarantees for the SDE-based implementation (i.e., denoising diﬀusion probabilistic modeling or DDPM), but requires the development of novel techniques for studying deterministic dynamics without contractivity. Through the use of a specially chosen corrector step based on the underdamped Langevin diﬀusion, we obtain better dimension dependence than prior works on DDPM (O(√d) vs. O(d), assuming smoothness of the data distribution), highlighting potential advantages of the ODE framework.},
	language = {en},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Chen, Sitan and Chewi, Sinho and Lee, Holden and Li, Yuanzhi and Lu, Jianfeng and Salim, Adil},
	month = may,
	year = {2023},
	note = {arXiv:2305.11798 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{de_bortoli_convergence_2023-1,
	title = {Convergence of denoising diffusion models under the manifold hypothesis},
	url = {http://arxiv.org/abs/2208.05314},
	abstract = {Denoising diffusion models are a recent class of generative models exhibiting state-of-the-art performance in image and audio synthesis. Such models approximate the time-reversal of a forward noising process from a target distribution to a reference measure, which is usually Gaussian. Despite their strong empirical results, the theoretical analysis of such models remains limited. In particular, all current approaches crucially assume that the target density admits a density w.r.t. the Lebesgue measure. This does not cover settings where the target distribution is supported on a lower-dimensional manifold or is given by some empirical distribution. In this paper, we bridge this gap by providing the first convergence results for diffusion models in this setting. In particular, we provide quantitative bounds on the Wasserstein distance of order one between the target data distribution and the generative distribution of the diffusion model.},
	language = {en},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {De Bortoli, Valentin},
	month = may,
	year = {2023},
	note = {arXiv:2208.05314 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{song_maximum_2021-1,
	title = {Maximum {Likelihood} {Training} of {Score}-{Based} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2101.09258},
	doi = {10.48550/arXiv.2101.09258},
	abstract = {Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32x32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
	month = oct,
	year = {2021},
	note = {arXiv:2101.09258 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{raya_spontaneous_2023,
	title = {Spontaneous {Symmetry} {Breaking} in {Generative} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2305.19693},
	doi = {10.48550/arXiv.2305.19693},
	abstract = {Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, while also increasing sample diversity (e.g., racial composition of generated CelebA images). Our work offers a new way to understand the generative dynamics of diffusion models that has the potential to bring about higher performance and less biased fast-samplers.},
	urldate = {2024-07-05},
	publisher = {arXiv},
	author = {Raya, Gabriel and Ambrogioni, Luca},
	month = oct,
	year = {2023},
	note = {arXiv:2305.19693 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{li_generalization_2024-1,
	title = {On the {Generalization} {Properties} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2311.01797},
	abstract = {Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error (O(n−2/5 + m−4/5)) on both the sample size n and the model capacity m, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. This precisely elucidates the adverse effect of “modes shift” in ground truths on the model generalization. Moreover, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. Our findings contribute to the rigorous understanding of diffusion models’ generalization properties and provide insights that may guide practical applications.},
	language = {en},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Li, Puheng and Li, Zhong and Zhang, Huishuai and Bian, Jiang},
	month = jan,
	year = {2024},
	note = {arXiv:2311.01797 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pidstrigach_score-based_2022-1,
	title = {Score-{Based} {Generative} {Models} {Detect} {Manifolds}},
	url = {http://arxiv.org/abs/2206.01018},
	abstract = {Score-based generative models (SGMs) need to approximate the scores ∇ log pt of the intermediate distributions as well as the ﬁnal distribution pT of the forward process. The theoretical underpinnings of the effects of these approximations are still lacking. We ﬁnd precise conditions under which SGMs are able to produce samples from an underlying (low-dimensional) data manifold M. This assures us that SGMs are able to generate the “right kind of samples”. For example, taking M to be the subset of images of faces, we ﬁnd conditions under which the SGM robustly produces an image of a face, even though the relative frequencies of these images might not accurately represent the true data generating distribution. Moreover, this analysis is a ﬁrst step towards understanding the generalization properties of SGMs: Taking M to be the set of all training samples, our results provide a precise description of when the SGM memorizes its training data.},
	language = {en},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Pidstrigach, Jakiw},
	month = oct,
	year = {2022},
	note = {arXiv:2206.01018 [cs, math, stat]},
	keywords = {68T99, Computer Science - Machine Learning, I.2.0, Mathematics - Numerical Analysis, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{koehler_sampling_2023,
	title = {Sampling {Multimodal} {Distributions} with the {Vanilla} {Score}: {Benefits} of {Data}-{Based} {Initialization}},
	shorttitle = {Sampling {Multimodal} {Distributions} with the {Vanilla} {Score}},
	url = {http://arxiv.org/abs/2310.01762},
	doi = {10.48550/arXiv.2310.01762},
	abstract = {There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on score functions -- derivatives of the log-likelihood of a distribution. In seminal works, Hyv{\textbackslash}"arinen proposed vanilla score matching as a way to learn distributions from data by computing an estimate of the score function of the underlying ground truth, and established connections between this method and established techniques like Contrastive Divergence and Pseudolikelihood estimation. It is by now well-known that vanilla score matching has significant difficulties learning multimodal distributions. Although there are various ways to overcome this difficulty, the following question has remained unanswered -- is there a natural way to sample multimodal distributions using just the vanilla score? Inspired by a long line of related experimental works, we prove that the Langevin diffusion with early stopping, initialized at the empirical distribution, and run on a score function estimated from data successfully generates natural multimodal distributions (mixtures of log-concave distributions).},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Koehler, Frederic and Vuong, Thuy-Duong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01762 [cs, math, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@misc{grohs_deep_2021,
	title = {Deep neural network approximation for high-dimensional parabolic {Hamilton}-{Jacobi}-{Bellman} equations},
	url = {http://arxiv.org/abs/2103.05744},
	abstract = {The approximation of solutions to second order Hamilton–Jacobi–Bellman (HJB) equations by deep neural networks is investigated. It is shown that for HJB equations that arise in the context of the optimal control of certain Markov processes the solution can be approximated by deep neural networks without incurring the curse of dimension. The dynamics is assumed to depend aﬃnely on the controls and the cost depends quadratically on the controls. The admissible controls take values in a bounded set.},
	language = {en},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Grohs, Philipp and Herrmann, Lukas},
	month = mar,
	year = {2021},
	note = {arXiv:2103.05744 [cs, math, stat]},
	keywords = {65C99, 65M99, 60H30, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{nusken_solving_2021,
	title = {Solving high-dimensional {Hamilton}–{Jacobi}–{Bellman} {PDEs} using neural networks: perspectives from the theory of controlled diffusions and measures on path space},
	volume = {2},
	issn = {2662-2971},
	shorttitle = {Solving high-dimensional {Hamilton}–{Jacobi}–{Bellman} {PDEs} using neural networks},
	url = {https://doi.org/10.1007/s42985-021-00102-x},
	doi = {10.1007/s42985-021-00102-x},
	abstract = {Optimal control of diffusion processes is intimately connected to the problem of solving certain Hamilton–Jacobi–Bellman equations. Building on recent machine learning inspired approaches towards high-dimensional PDEs, we investigate the potential of iterative diffusion optimisation techniques, in particular considering applications in importance sampling and rare event simulation, and focusing on problems without diffusion control, with linearly controlled drift and running costs that depend quadratically on the control. More generally, our methods apply to nonlinear parabolic PDEs with a certain shift invariance. The choice of an appropriate loss function being a central element in the algorithmic design, we develop a principled framework based on divergences between path measures, encompassing various existing methods. Motivated by connections to forward-backward SDEs, we propose and study the novel log-variance divergence, showing favourable properties of corresponding Monte Carlo estimators. The promise of the developed approach is exemplified by a range of high-dimensional and metastable numerical examples.},
	language = {en},
	number = {4},
	urldate = {2024-07-02},
	journal = {Partial Differential Equations and Applications},
	author = {Nüsken, Nikolas and Richter, Lorenz},
	month = jun,
	year = {2021},
	keywords = {Deep learning, Divergences between probability measures, Forward-backward SDEs, Hamilton–Jacobi–Bellman PDEs, Optimal control of diffusions, Rare event simulation},
	pages = {48},
}

@misc{cole_score-based_2024,
	title = {Score-based generative models break the curse of dimensionality in learning a family of sub-{Gaussian} probability distributions},
	url = {http://arxiv.org/abs/2402.08082},
	abstract = {While score-based generative models (SGMs) have achieved remarkable successes in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated to the forward process, which is interesting in its own right.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Cole, Frank and Lu, Yulong},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08082 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_score_2023,
	title = {Score {Approximation}, {Estimation} and {Distribution} {Recovery} of {Diffusion} {Models} on {Low}-{Dimensional} {Data}},
	url = {http://arxiv.org/abs/2302.07194},
	doi = {10.48550/arXiv.2302.07194},
	abstract = {Diffusion models achieve state-of-the-art performance in various generation tasks. However, their theoretical foundations fall far behind. This paper studies score approximation, estimation, and distribution recovery of diffusion models, when data are supported on an unknown low-dimensional linear subspace. Our result provides sample complexity bounds for distribution estimation using diffusion models. We show that with a properly chosen neural network architecture, the score function can be both accurately approximated and efficiently estimated. Furthermore, the generated distribution based on the estimated score function captures the data geometric structures and converges to a close vicinity of the data distribution. The convergence rate depends on the subspace dimension, indicating that diffusion models can circumvent the curse of data ambient dimensionality.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Chen, Minshuo and Huang, Kaixuan and Zhao, Tuo and Wang, Mengdi},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07194 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oko_diffusion_2023,
	title = {Diffusion {Models} are {Minimax} {Optimal} {Distribution} {Estimators}},
	url = {http://arxiv.org/abs/2303.01861},
	doi = {10.48550/arXiv.2303.01861},
	abstract = {While efficient distribution learning is no doubt behind the groundbreaking success of diffusion modeling, its theoretical guarantees are quite limited. In this paper, we provide the first rigorous analysis on approximation and generalization abilities of diffusion modeling for well-known function spaces. The highlight of this paper is that when the true density function belongs to the Besov space and the empirical score matching loss is properly minimized, the generated data distribution achieves the nearly minimax optimal estimation rates in the total variation distance and in the Wasserstein distance of order one. Furthermore, we extend our theory to demonstrate how diffusion models adapt to low-dimensional data distributions. We expect these results advance theoretical understandings of diffusion modeling and its ability to generate verisimilar outputs.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji},
	month = mar,
	year = {2023},
	note = {arXiv:2303.01861 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_generalization_nodate,
	title = {Generalization in diffusion models arises from geometry-adaptive harmonic representations},
	url = {https://arxiv.org/html/2310.02557v3},
	urldate = {2024-06-27},
}

@misc{kadkhodaie_generalization_2024,
	title = {Generalization in diffusion models arises from geometry-adaptive harmonic representations},
	url = {http://arxiv.org/abs/2310.02557},
	abstract = {Deep neural networks (DNNs) trained for image denoising are able to generate highquality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the “true” continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough. In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.},
	language = {en},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Kadkhodaie, Zahra and Guth, Florentin and Simoncelli, Eero P. and Mallat, Stéphane},
	month = apr,
	year = {2024},
	note = {arXiv:2310.02557 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cui_analysis_2024,
	title = {Analysis of learning a flow-based generative model from limited sample complexity},
	url = {http://arxiv.org/abs/2310.03575},
	doi = {10.48550/arXiv.2310.03575},
	abstract = {We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a shallow denoising auto-encoder trained on a finite number \$n\$ of samples from the target distribution. Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density. In particular, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as \${\textbackslash}Theta\_n({\textbackslash}frac\{1\}\{n\})\$. Finally, this rate is shown to be in fact Bayes-optimal.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Cui, Hugo and Krzakala, Florent and Vanden-Eijnden, Eric and Zdeborová, Lenka},
	month = jun,
	year = {2024},
	note = {arXiv:2310.03575 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kappen_path_2005,
	title = {Path integrals and symmetry breaking for optimal control theory},
	volume = {2005},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/physics/0505066},
	doi = {10.1088/1742-5468/2005/11/P11011},
	abstract = {This paper considers linear-quadratic control of a non-linear dynamical system subject to arbitrary cost. I show that for this class of stochastic control problems the non-linear Hamilton-Jacobi-Bellman equation can be transformed into a linear equation. The transformation is similar to the transformation used to relate the classical Hamilton-Jacobi equation to the Schr{\textbackslash}"odinger equation. As a result of the linearity, the usual backward computation can be replaced by a forward diffusion process, that can be computed by stochastic integration or by the evaluation of a path integral. It is shown, how in the deterministic limit the PMP formalism is recovered. The significance of the path integral approach is that it forms the basis for a number of efficient computational methods, such as MC sampling, the Laplace approximation and the variational approximation. We show the effectiveness of the first two methods in number of examples. Examples are given that show the qualitative difference between stochastic and deterministic control and the occurrence of symmetry breaking as a function of the noise.},
	number = {11},
	urldate = {2024-06-24},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Kappen, H. J.},
	month = nov,
	year = {2005},
	note = {arXiv:physics/0505066},
	keywords = {Physics - Computational Physics, Physics - General Physics},
	pages = {P11011--P11011},
}

@misc{ambrogioni_search_2023,
	title = {In search of dispersed memories: {Generative} diffusion models are associative memory networks},
	shorttitle = {In search of dispersed memories},
	url = {http://arxiv.org/abs/2309.17290},
	doi = {10.48550/arXiv.2309.17290},
	abstract = {Uncovering the mechanisms behind long-term memory is one of the most fascinating open problems in neuroscience and artificial intelligence. Artificial associative memory networks have been used to formalize important aspects of biological memory. Generative diffusion models are a type of generative machine learning techniques that have shown great performance in many tasks. Like associative memory systems, these networks define a dynamical system that converges to a set of target states. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is (asymptotically) identical to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure of a deep neural network. Leveraging this connection, we formulate a generalized framework for understanding the formation of long-term memory, where creative generation and memory recall can be seen as parts of a unified continuum.},
	urldate = {2024-06-24},
	publisher = {arXiv},
	author = {Ambrogioni, Luca},
	month = nov,
	year = {2023},
	note = {arXiv:2309.17290 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{garrigos_handbook_2024,
	title = {Handbook of {Convergence} {Theorems} for ({Stochastic}) {Gradient} {Methods}},
	url = {http://arxiv.org/abs/2301.11235},
	abstract = {This is a handbook of simple proofs of the convergence of gradient and stochastic gradient descent type methods. We consider functions that are Lipschitz, smooth, convex, strongly convex, and/or Polyak-Lojasiewicz functions. Our focus is on “good proofs” that are also simple. Each section can be consulted separately. We start with proofs of gradient descent, then on stochastic variants, including minibatching and momentum. Then move on to nonsmooth problems with the subgradient method, the proximal gradient descent and their stochastic variants. Our focus is on global convergence rates and complexity rates. Some slightly less common proofs found here include that of SGD (Stochastic gradient descent) with a proximal step in 11, with momentum in Section 7, and with mini-batching in Section 6.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Garrigos, Guillaume and Gower, Robert M.},
	month = mar,
	year = {2024},
	note = {arXiv:2301.11235 [math]},
	keywords = {65K05, 68T99, G.1.6, Mathematics - Optimization and Control},
}

@misc{noauthor_robust_nodate,
	title = {robust testing in high-dimensional sparse models - {Google} {Search}},
	url = {https://www.google.com/search?q=robust+testing+in+high-dimensional+sparse+models&sca_esv=8870e14de2d74e2a&sca_upv=1&sxsrf=ADLYWIIQp4mcNDdsRxsh9kaBDr2x_ngACg%3A1716814755946&ei=o4NUZounObiB9u8P-vazyAc&oq=robus&gs_lp=Egxnd3Mtd2l6LXNlcnAiBXJvYnVzKgIIADIEECMYJzIKEAAYgAQYQxiKBTIKEAAYgAQYQxiKBTIKEAAYgAQYQxiKBTIFEAAYgAQyERAuGIAEGLEDGIMBGMcBGK8BMgUQLhiABDIFEC4YgAQyBRAAGIAEMgUQABiABEjEEVAAWOkIcAB4AJABAJgBnwGgAYMFqgEDMC41uAEDyAEA-AEBmAIFoAKbBcICChAjGIAEGCcYigXCAgsQABiABBiRAhiKBcICCxAuGIAEGJECGIoFwgIOEAAYgAQYsQMYgwEYigXCAg4QLhiABBixAxiDARiKBcICEBAuGIAEGNEDGEMYxwEYigXCAgsQABiABBixAxiDAcICCxAuGIAEGLEDGIMBwgIKEC4YgAQYJxiKBcICChAuGIAEGEMYigXCAhAQABiABBixAxhDGIMBGIoFwgIKEAAYgAQYFBiHAsICCxAuGIAEGMcBGK8BmAMAkgcDMC41oAftmwE&sclient=gws-wiz-serp},
	urldate = {2024-05-27},
}

@misc{raya_spontaneous_2023-1,
	title = {Spontaneous {Symmetry} {Breaking} in {Generative} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2305.19693},
	doi = {10.48550/arXiv.2305.19693},
	abstract = {Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, while also increasing sample diversity (e.g., racial composition of generated CelebA images). Our work offers a new way to understand the generative dynamics of diffusion models that has the potential to bring about higher performance and less biased fast-samplers.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Raya, Gabriel and Ambrogioni, Luca},
	month = oct,
	year = {2023},
	note = {arXiv:2305.19693 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ambrogioni_statistical_2024,
	title = {The statistical thermodynamics of generative diffusion models: {Phase} transitions, symmetry breaking and critical instability},
	shorttitle = {The statistical thermodynamics of generative diffusion models},
	url = {http://arxiv.org/abs/2310.17467},
	abstract = {Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, variational inference and stochastic calculus, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We show that these phase-transitions are always in a mean-field universality class, as they are the result of a self-consistency condition in the generative dynamics. We argue that the critical instability that arises from the phase transitions lies at the heart of their generative capabilities, which are characterized by a set of mean field critical exponents. Furthermore, using the statistical physics of disordered systems, we show that memorization can be understood as a form of critical condensation corresponding to a disordered phase transition. Finally, we show that the dynamic equation of the generative process can be interpreted as a stochastic adiabatic transformation that minimizes the free energy while keeping the system in thermal equilibrium.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Ambrogioni, Luca},
	month = mar,
	year = {2024},
	note = {arXiv:2310.17467 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ambrogioni_statistical_2024-1,
	title = {The statistical thermodynamics of generative diffusion models: {Phase} transitions, symmetry breaking and critical instability},
	shorttitle = {The statistical thermodynamics of generative diffusion models},
	url = {http://arxiv.org/abs/2310.17467},
	doi = {10.48550/arXiv.2310.17467},
	abstract = {Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, variational inference and stochastic calculus, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We show that these phase-transitions are always in a mean-field universality class, as they are the result of a self-consistency condition in the generative dynamics. We argue that the critical instability that arises from the phase transitions lies at the heart of their generative capabilities, which are characterized by a set of mean field critical exponents. Furthermore, using the statistical physics of disordered systems, we show that memorization can be understood as a form of critical condensation corresponding to a disordered phase transition. Finally, we show that the dynamic equation of the generative process can be interpreted as a stochastic adiabatic transformation that minimizes the free energy while keeping the system in thermal equilibrium.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Ambrogioni, Luca},
	month = mar,
	year = {2024},
	note = {arXiv:2310.17467 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_likelihood_2021,
	title = {Likelihood {Training} of {Schrödinger} {Bridge} using {Forward}-{Backward} {SDEs} {Theory}},
	url = {https://openreview.net/forum?id=nioAdKCEdXB},
	abstract = {Schrödinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory – a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE.},
	language = {en},
	urldate = {2024-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chen, Tianrong and Liu, Guan-Horng and Theodorou, Evangelos},
	month = oct,
	year = {2021},
}

@inproceedings{song_generative_2019,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples 
comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2024-05-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Song, Yang and Ermon, Stefano},
	year = {2019},
}

@inproceedings{vargas_denoising_2022,
	title = {Denoising {Diffusion} {Samplers}},
	url = {https://openreview.net/forum?id=8pvnfTAbu1f},
	abstract = {Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal. While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schr{\textbackslash}"odinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks.},
	language = {en},
	urldate = {2024-05-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Vargas, Francisco and Grathwohl, Will Sussman and Doucet, Arnaud},
	month = sep,
	year = {2022},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2024-05-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
}

@article{berner_optimal_2023,
	title = {An optimal control perspective on diffusion-based generative modeling},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=oYIjw37pTP},
	abstract = {We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton--Jacobi--Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback--Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approaches on multiple numerical examples.},
	language = {en},
	urldate = {2024-05-22},
	journal = {Transactions on Machine Learning Research},
	author = {Berner, Julius and Richter, Lorenz and Ullrich, Karen},
	month = oct,
	year = {2023},
}

@article{barra_about_2014,
	title = {About a solvable mean field model of a {Gaussian} spin glass},
	volume = {47},
	copyright = {http://iopscience.iop.org/info/page/text-and-data-mining},
	issn = {1751-8113, 1751-8121},
	url = {https://iopscience.iop.org/article/10.1088/1751-8113/47/15/155002},
	doi = {10.1088/1751-8113/47/15/155002},
	abstract = {In a series of papers, we have studied a modified Hopfield model of a neural network, with learned words characterized by a Gaussian distribution. The model can be represented as a bipartite spin glass, with one party described by dichotomic Ising spins, and the other party by continuous spin variables, with an a priori Gaussian distribution. By application of standard interpolation methods, we have found it useful to compare the neural network model (bipartite) from one side, with two spin glass models, each monopartite, from the other side. Of these, the first is the usual Sherrington–Kirkpatrick model, the second is a spin glass model, with continuous spins and inbuilt highly nonlinear smooth cut-off interactions. This model is an invaluable laboratory for testing all techniques which have been useful in the study of spin glasses. The purpose of this paper is to give a synthetic description of the most peculiar aspects, by stressing the necessary novelties in the treatment. In particular, it will be shown that the control of the infinite volume limit, according to the well-known Guerra–Toninelli strategy, requires in addition one to consider the involvement of the cut-off interaction in the interpolation procedure. Moreover, the control of the ergodic region, the annealed case, cannot be directly achieved through the standard application of the Borel–Cantelli lemma, but requires previous modification of the interaction. This remark could find useful application in other cases. The replica symmetric expression for the free energy can be easily reached through a suitable version of the doubly stochastic interpolation technique. However, this model shares the unique property that the fully broken replica symmetry ansatz can be explicitly calculated. A very simple sum rule connects the general expression of the fully broken free energy trial function with the replica symmetric one. The definite sign of the error term shows that the replica solution is optimal. Then, we follow a deep strategy developed by Talagrand, to conclude that the replica symmetric ansatz not only gives a bound for the free energy, which cannot be improved by symmetry breaking, but gives also the true value for the free energy. For the sake of completeness, we study also the fluctuations of the (centered and rescaled) overlaps, by following a method similar to that exploited for the Sherrington–Kirkpatrick model, in the replica symmetric region.},
	number = {15},
	urldate = {2024-05-22},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Barra, Adriano and Genovese, Giuseppe and Guerra, Francesco and Tantari, Daniele},
	month = apr,
	year = {2014},
	pages = {155002},
}

@article{kosterlitz_spherical_1976,
	title = {Spherical {Model} of a {Spin}-{Glass}},
	volume = {36},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.36.1217},
	doi = {10.1103/PhysRevLett.36.1217},
	abstract = {A spherical model of a spin-glass is solved in the limit of infinite-ranged interactions with a Gaussian probability distribution. We use the known properties of a large random matrix, and show that the results are identical to those obtained by the n→0 trick. We believe that the solution is exact.},
	number = {20},
	urldate = {2024-05-22},
	journal = {Physical Review Letters},
	author = {Kosterlitz, J. M. and Thouless, D. J. and Jones, Raymund C.},
	month = may,
	year = {1976},
	note = {Publisher: American Physical Society},
	pages = {1217--1220},
}

@article{kosterlitz_spherical_1977,
	title = {Spherical model of a spin glass},
	volume = {86-88},
	issn = {0378-4363},
	url = {https://www.sciencedirect.com/science/article/pii/0378436377907161},
	doi = {10.1016/0378-4363(77)90716-1},
	abstract = {A spherical model of a spin glass is solved in the limit of infinite-ranged interactions with a gaussian probability distribution. It is shown that for suitable values of the mean and variance of the distribution, the model can display either spin glass or ferromagnetic ordering; identical results obtain using the n → 0 method. The distribution of internal fields is shown to be gaussian.},
	urldate = {2024-05-22},
	journal = {Physica B+C},
	author = {Kosterlitz, J. M. and Thouless, D. J. and Jones, Raymund C.},
	month = jan,
	year = {1977},
	pages = {859--860},
}

@article{arous_aging_2001,
	title = {Aging of spherical spin glasses},
	volume = {120},
	issn = {1432-2064},
	url = {https://doi.org/10.1007/PL00008774},
	doi = {10.1007/PL00008774},
	abstract = {Sompolinski and Zippelius (1981) propose the study of dynamical systems whose invariant measures are the Gibbs measures for (hard to analyze) statistical physics models of interest. In the course of doing so, physicists often report of an “aging” phenomenon. For example, aging is expected to happen for the Sherrington-Kirkpatrick model, a disordered mean-field model with a very complex phase transition in equilibrium at low temperature. We shall study the Langevin dynamics for a simplified spherical version of this model. The induced rotational symmetry of the spherical model reduces the dynamics in question to an N-dimensional coupled system of Ornstein-Uhlenbeck processes whose random drift parameters are the eigenvalues of certain random matrices. We obtain the limiting dynamics for N approaching infinity and by analyzing its long time behavior, explain what is aging (mathematically speaking), what causes this phenomenon, and what is its relationship with the phase transition of the corresponding equilibrium invariant measures.},
	language = {en},
	number = {1},
	urldate = {2024-05-22},
	journal = {Probability Theory and Related Fields},
	author = {Arous, G. Ben and Dembo, A. and Guionnet, A.},
	month = may,
	year = {2001},
	keywords = {Key words or phrases: Interacting random processes – Disordered systems – Large deviations – Statistical mechanics – Langevin dynamics – Eigenvalues, Random matrices, Mathematics Subject Classification (2000): 60H10, 82B44, 60F10, 60K35, 82C44, 82C31, 82C22, 15A18, 15A52},
	pages = {1--67},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{hendrycks_gaussian_2023,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	doi = {10.48550/arXiv.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jun,
	year = {2023},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_notitle_nodate-1,
}

@article{raginsky_stochastic_nodate,
	title = {Stochastic {Calculus} in {Machine} {Learning}: {Optimization}, {Sampling}, {Simulation}},
	language = {en},
	journal = {Diﬀusion Processes},
	author = {Raginsky, Maxim},
}

@misc{chen_probabilistic_2024,
	title = {Probabilistic {Forecasting} with {Stochastic} {Interpolants} and {F}{\textbackslash}"ollmer {Processes}},
	url = {http://arxiv.org/abs/2403.13724},
	doi = {10.48550/arXiv.2403.13724},
	abstract = {We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by square loss regression over the time-series data. We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a F{\textbackslash}"ollmer process. We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets.},
	urldate = {2024-05-18},
	publisher = {arXiv},
	author = {Chen, Yifan and Goldstein, Mark and Hua, Mengjian and Albergo, Michael S. and Boffi, Nicholas M. and Vanden-Eijnden, Eric},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13724 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{follmer_time_1986,
	address = {Berlin, Heidelberg},
	title = {Time reversal on {Wiener} space},
	isbn = {978-3-540-39703-8},
	doi = {10.1007/BFb0080212},
	language = {en},
	booktitle = {Stochastic {Processes} — {Mathematics} and {Physics}},
	publisher = {Springer},
	author = {Föllmer, H.},
	editor = {Albeverio, Sergio A. and Blanchard, Philippe and Streit, Ludwig},
	year = {1986},
	keywords = {Duality Equation, Stochastic Differential Equation, Time Reversal, Wiener Process, Wiener Space},
	pages = {119--129},
}

@article{bismut_conjugate_1973,
	title = {Conjugate convex functions in optimal stochastic control},
	volume = {44},
	issn = {0022-247X},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X73900668},
	doi = {10.1016/0022-247X(73)90066-8},
	number = {2},
	urldate = {2024-05-18},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Bismut, Jean-Michel},
	month = nov,
	year = {1973},
	pages = {384--404},
}

@misc{raissi_forward-backward_2018,
	title = {Forward-{Backward} {Stochastic} {Neural} {Networks}: {Deep} {Learning} of {High}-dimensional {Partial} {Differential} {Equations}},
	shorttitle = {Forward-{Backward} {Stochastic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1804.07010},
	abstract = {Classical numerical methods for solving partial diﬀerential equations suﬀer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial diﬀerential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to beneﬁt from the merits of automatic diﬀerentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial diﬀerential equations and forwardbackward stochastic diﬀerential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the eﬀectiveness of our approach for a couple of benchmark problems spanning a number of scientiﬁc domains including Black-Scholes-Barenblatt and HamiltonJacobi-Bellman equations, both in 100-dimensions.},
	language = {en},
	urldate = {2024-05-18},
	publisher = {arXiv},
	author = {Raissi, Maziar},
	month = apr,
	year = {2018},
	note = {arXiv:1804.07010 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Analysis of PDEs, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@book{chewi_log-concave_nodate,
	title = {Log-{Concave} sampling},
	author = {Chewi, Sinho},
}

@inproceedings{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	urldate = {2024-05-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	year = {2020},
	pages = {7537--7547},
}

@misc{noauthor_jaxcitationbib_nodate,
	title = {jax/{CITATION}.bib at main · google/jax},
	url = {https://github.com/google/jax/blob/main/CITATION.bib},
	abstract = {Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more - google/jax},
	language = {en},
	urldate = {2024-05-12},
	journal = {GitHub},
}

@article{neal_slice_2003,
	title = {Slice sampling},
	volume = {31},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-3/Slice-sampling/10.1214/aos/1056562461.full},
	doi = {10.1214/aos/1056562461},
	abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
	number = {3},
	urldate = {2024-05-12},
	journal = {The Annals of Statistics},
	author = {Neal, Radford M.},
	month = jun,
	year = {2003},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {65C05, 65C60, Adaptive methods, Gibbs sampling, Markov chain Monte Carlo, Metropolis algorithm, auxiliary variables, dynamical methods, overrelaxation},
	pages = {705--767},
}

@article{del_moral_sequential_2006,
	title = {Sequential {Monte} {Carlo} samplers},
	volume = {68},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2006.00553.x},
	doi = {10.1111/j.1467-9868.2006.00553.x},
	abstract = {Summary. We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
	language = {en},
	number = {3},
	urldate = {2024-05-12},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2006.00553.x},
	keywords = {Importance sampling, Markov chain Monte Carlo methods, Ratio of normalizing constants, Resampling, Sequential Monte Carlo methods, Simulated annealing},
	pages = {411--436},
}

@article{neal_annealed_2001,
	title = {Annealed importance sampling},
	volume = {11},
	issn = {1573-1375},
	url = {https://doi.org/10.1023/A:1008923215028},
	doi = {10.1023/A:1008923215028},
	abstract = {Simulated annealing—moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions—has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.},
	language = {en},
	number = {2},
	urldate = {2024-05-12},
	journal = {Statistics and Computing},
	author = {Neal, Radford M.},
	month = apr,
	year = {2001},
	keywords = {estimation of normalizing constants, free energy computation, sequential importance sampling, tempered transitions},
	pages = {125--139},
}

@book{noauthor_transformation_nodate,
	title = {Transformation of {Measure} on {Wiener} {Space}},
	url = {https://link.springer.com/book/10.1007/978-3-662-13225-8},
	language = {en},
	urldate = {2024-05-06},
}

@misc{saremi_chain_2023,
	title = {Chain of {Log}-{Concave} {Markov} {Chains}},
	url = {http://arxiv.org/abs/2305.19473},
	abstract = {We introduce a theoretical framework for sampling from unnormalized densities based on a smoothing scheme that uses an isotropic Gaussian kernel with a single fixed noise scale. We prove one can decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. Our construction is unique in that it keeps track of a history of samples, making it non-Markovian as a whole, but it is lightweight algorithmically as the history only shows up in the form of a running empirical mean of samples. Our sampling algorithm generalizes walk-jump sampling (Saremi \& Hyvärinen, 2019). The “walk” phase becomes a (non-Markovian) chain of (log-concave) Markov chains. The “jump” from the accumulated measurements is obtained by empirical Bayes. We study our sampling algorithm quantitatively using the 2-Wasserstein metric and compare it with various Langevin MCMC algorithms. We also report a remarkable capacity of our algorithm to “tunnel” between modes of a distribution.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Saremi, Saeed and Park, Ji Won and Bach, Francis},
	month = sep,
	year = {2023},
	note = {arXiv:2305.19473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{biroli_dynamical_2024,
	title = {Dynamical {Regimes} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2402.18491},
	doi = {10.48550/arXiv.2402.18491},
	abstract = {Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a 'speciation' transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a 'collapse' transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an 'excess entropy' in the data. The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models. Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Biroli, Giulio and Bonnaire, Tony and de Bortoli, Valentin and Mézard, Marc},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18491 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics},
}

@misc{biroli_dynamical_2024-1,
	title = {Dynamical {Regimes} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2402.18491},
	abstract = {Using statistical physics methods, we study generative diffusion models in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative diffusion process. The generative dynamics, starting from pure noise, encounters first a ’speciation’ transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a ’collapse’ transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an ’excess entropy’ in the data. The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for diffusion models. Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Biroli, Giulio and Bonnaire, Tony and de Bortoli, Valentin and Mézard, Marc},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18491 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics},
}

@incollection{noauthor_notitle_nodate-2,
	url = {https://link.springer.com/book/10.1007/978-1-4614-6289-7},
	language = {en},
	urldate = {2024-05-03},
	booktitle = {The {Sherrington}-{Kirkpatrick} {Model}},
}

@book{noauthor_stochastic_nodate,
	title = {Stochastic {Integration} and {Differential} {Equations}},
	url = {https://link.springer.com/book/10.1007/978-3-662-10061-5},
	language = {en},
	urldate = {2024-05-03},
}

@book{noauthor_sherrington-kirkpatrick_nodate,
	title = {The {Sherrington}-{Kirkpatrick} {Model}},
	url = {https://link.springer.com/book/10.1007/978-1-4614-6289-7},
	language = {en},
	urldate = {2024-05-03},
}

@misc{alaoui_sampling_2023,
	title = {Sampling from {Mean}-{Field} {Gibbs} {Measures} via {Diffusion} {Processes}},
	url = {http://arxiv.org/abs/2310.08912},
	doi = {10.48550/arXiv.2310.08912},
	abstract = {We consider Ising mixed \$p\$-spin glasses at high-temperature and without external field, and study the problem of sampling from the Gibbs distribution \${\textbackslash}mu\$ in polynomial time. We develop a new sampling algorithm with complexity of the same order as evaluating the gradient of the Hamiltonian and, in particular, at most linear in the input size. We prove that, at sufficiently high-temperature, it produces samples from a distribution \${\textbackslash}mu{\textasciicircum}\{alg\}\$ which is close in normalized Wasserstein distance to \${\textbackslash}mu\$. Namely, there exists a coupling of \${\textbackslash}mu\$ and \${\textbackslash}mu{\textasciicircum}\{alg\}\$ such that if \$(\{{\textbackslash}boldsymbol x\},\{{\textbackslash}boldsymbol x\}{\textasciicircum}\{alg\}){\textbackslash}in{\textbackslash}\{-1,+1{\textbackslash}\}{\textasciicircum}n{\textbackslash}times {\textbackslash}\{-1,+1{\textbackslash}\}{\textasciicircum}n\$ is a pair drawn from this coupling, then \$n{\textasciicircum}\{-1\}\{{\textbackslash}mathbb E\}{\textbackslash}\{{\textbackslash}{\textbar}\{{\textbackslash}boldsymbol x\}-\{{\textbackslash}boldsymbol x\}{\textasciicircum}\{alg\}{\textbackslash}{\textbar}\_2{\textasciicircum}2{\textbackslash}\}=o\_n(1)\$. For the case of the Sherrington-Kirkpatrick model, our algorithm succeeds in the full replica-symmetric phase. We complement this result with a negative one for sampling algorithms satisfying a certain `stability' property, which is verified by many standard techniques. No stable algorithm can approximately sample at temperatures below the onset of shattering, even under the normalized Wasserstein metric. Further, no algorithm can sample at temperatures below the onset of replica symmetry breaking. Our sampling method implements a discretized version of a diffusion process that has become recently popular in machine learning under the name of `denoising diffusion.' We derive the same process from the general construction of stochastic localization. Implementing the diffusion process requires to efficiently approximate the mean of the tilted measure. To this end, we use an approximate message passing algorithm that, as we prove, achieves sufficiently accurate mean estimation.},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08912 [math]},
	keywords = {Mathematics - Probability},
}

@misc{alaoui_sampling_2023-1,
	title = {Sampling from {Mean}-{Field} {Gibbs} {Measures} via {Diffusion} {Processes}},
	url = {http://arxiv.org/abs/2310.08912},
	abstract = {We consider Ising mixed \$p\$-spin glasses at high-temperature and without external field, and study the problem of sampling from the Gibbs distribution \${\textbackslash}mu\$ in polynomial time. We develop a new sampling algorithm with complexity of the same order as evaluating the gradient of the Hamiltonian and, in particular, at most linear in the input size. We prove that, at sufficiently high-temperature, it produces samples from a distribution \${\textbackslash}mu{\textasciicircum}\{alg\}\$ which is close in normalized Wasserstein distance to \${\textbackslash}mu\$. Namely, there exists a coupling of \${\textbackslash}mu\$ and \${\textbackslash}mu{\textasciicircum}\{alg\}\$ such that if \$(\{{\textbackslash}boldsymbol x\},\{{\textbackslash}boldsymbol x\}{\textasciicircum}\{alg\}){\textbackslash}in{\textbackslash}\{-1,+1{\textbackslash}\}{\textasciicircum}n{\textbackslash}times {\textbackslash}\{-1,+1{\textbackslash}\}{\textasciicircum}n\$ is a pair drawn from this coupling, then \$n{\textasciicircum}\{-1\}\{{\textbackslash}mathbb E\}{\textbackslash}\{{\textbackslash}{\textbar}\{{\textbackslash}boldsymbol x\}-\{{\textbackslash}boldsymbol x\}{\textasciicircum}\{alg\}{\textbackslash}{\textbar}\_2{\textasciicircum}2{\textbackslash}\}=o\_n(1)\$. For the case of the Sherrington-Kirkpatrick model, our algorithm succeeds in the full replica-symmetric phase. We complement this result with a negative one for sampling algorithms satisfying a certain `stability' property, which is verified by many standard techniques. No stable algorithm can approximately sample at temperatures below the onset of shattering, even under the normalized Wasserstein metric. Further, no algorithm can sample at temperatures below the onset of replica symmetry breaking. Our sampling method implements a discretized version of a diffusion process that has become recently popular in machine learning under the name of `denoising diffusion.' We derive the same process from the general construction of stochastic localization. Implementing the diffusion process requires to efficiently approximate the mean of the tilted measure. To this end, we use an approximate message passing algorithm that, as we prove, achieves sufficiently accurate mean estimation.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08912 [math]},
	keywords = {Mathematics - Probability},
}

@misc{richter_improved_2023,
	title = {Improved sampling via learned diffusions},
	url = {http://arxiv.org/abs/2307.01198},
	abstract = {Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr{\textbackslash}"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered approaches.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Richter, Lorenz and Berner, Julius and Liu, Guan-Horng},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01198 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{zhang_path_2022,
	title = {Path {Integral} {Sampler}: a stochastic control approach for sampling},
	shorttitle = {Path {Integral} {Sampler}},
	url = {http://arxiv.org/abs/2111.15141},
	abstract = {We present Path Integral Sampler{\textasciitilde}(PIS), a novel algorithm to draw samples from unnormalized probability density functions. The PIS is built on the Schr{\textbackslash}"odinger bridge problem which aims to recover the most likely evolution of a diffusion process given its initial distribution and terminal distribution. The PIS draws samples from the initial distribution and then propagates the samples through the Schr{\textbackslash}"odinger bridge to reach the terminal distribution. Applying the Girsanov theorem, with a simple prior diffusion, we formulate the PIS as a stochastic optimal control problem whose running cost is the control energy and terminal cost is chosen according to the target distribution. By modeling the control as a neural network, we establish a sampling algorithm that can be trained end-to-end. We provide theoretical justification of the sampling quality of PIS in terms of Wasserstein distance when sub-optimal control is used. Moreover, the path integrals theory is used to compute importance weights of the samples to compensate for the bias induced by the sub-optimality of the controller and time-discretization. We experimentally demonstrate the advantages of PIS compared with other start-of-the-art sampling methods on a variety of tasks.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Zhang, Qinsheng and Chen, Yongxin},
	month = mar,
	year = {2022},
	note = {arXiv:2111.15141 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{dockhorn_score-based_2022,
	title = {Score-{Based} {Generative} {Modeling} with {Critically}-{Damped} {Langevin} {Diffusion}},
	url = {http://arxiv.org/abs/2112.07068},
	doi = {10.48550/arXiv.2112.07068},
	abstract = {Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered "velocities" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.},
	urldate = {2024-03-04},
	publisher = {arXiv},
	author = {Dockhorn, Tim and Vahdat, Arash and Kreis, Karsten},
	month = mar,
	year = {2022},
	note = {arXiv:2112.07068 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{du_reduce_2023,
	title = {Reduce, {Reuse}, {Recycle}: {Compositional} {Generation} with {Energy}-{Based} {Diffusion} {Models} and {MCMC}},
	shorttitle = {Reduce, {Reuse}, {Recycle}},
	url = {http://arxiv.org/abs/2302.11552},
	abstract = {Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers. Intriguingly we find these samplers lead to notable improvements in compositional generation across a wide set of problems such as classifier-guided ImageNet modeling and compositional text-to-image generation.},
	urldate = {2024-02-29},
	publisher = {arXiv},
	author = {Du, Yilun and Durkan, Conor and Strudel, Robin and Tenenbaum, Joshua B. and Dieleman, Sander and Fergus, Rob and Sohl-Dickstein, Jascha and Doucet, Arnaud and Grathwohl, Will},
	month = nov,
	year = {2023},
	note = {arXiv:2302.11552 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_probability_2023,
	title = {The probability flow {ODE} is provably fast},
	url = {http://arxiv.org/abs/2305.11798},
	doi = {10.48550/arXiv.2305.11798},
	abstract = {We provide the first polynomial-time convergence guarantees for the probability flow ODE implementation (together with a corrector step) of score-based generative modeling. Our analysis is carried out in the wake of recent results obtaining such guarantees for the SDE-based implementation (i.e., denoising diffusion probabilistic modeling or DDPM), but requires the development of novel techniques for studying deterministic dynamics without contractivity. Through the use of a specially chosen corrector step based on the underdamped Langevin diffusion, we obtain better dimension dependence than prior works on DDPM (\$O({\textbackslash}sqrt\{d\})\$ vs. \$O(d)\$, assuming smoothness of the data distribution), highlighting potential advantages of the ODE framework.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Chen, Sitan and Chewi, Sinho and Lee, Holden and Li, Yuanzhi and Lu, Jianfeng and Salim, Adil},
	month = may,
	year = {2023},
	note = {arXiv:2305.11798 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{liu_flow_2022,
	title = {Flow {Straight} and {Fast}: {Learning} to {Generate} and {Transfer} {Data} with {Rectified} {Flow}},
	shorttitle = {Flow {Straight} and {Fast}},
	url = {http://arxiv.org/abs/2209.03003},
	doi = {10.48550/arXiv.2209.03003},
	abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions {\textbackslash}pi\_0 and {\textbackslash}pi\_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from {\textbackslash}pi\_0 and {\textbackslash}pi\_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of {\textbackslash}pi\_0 and {\textbackslash}pi\_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
	month = sep,
	year = {2022},
	note = {arXiv:2209.03003 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lee_convergence_2022,
	title = {Convergence of score-based generative modeling for general data distributions},
	url = {http://arxiv.org/abs/2209.12381},
	doi = {10.48550/arXiv.2209.12381},
	abstract = {Score-based generative modeling (SGM) has grown to be a hugely successful method for learning to generate samples from complex data distributions such as that of images and audio. It is based on evolving an SDE that transforms white noise into a sample from the learned distribution, using estimates of the score function, or gradient log-pdf. Previous convergence analyses for these methods have suffered either from strong assumptions on the data distribution or exponential dependencies, and hence fail to give efficient guarantees for the multimodal and non-smooth distributions that arise in practice and for which good empirical performance is observed. We consider a popular kind of SGM -- denoising diffusion models -- and give polynomial convergence guarantees for general data distributions, with no assumptions related to functional inequalities or smoothness. Assuming \$L{\textasciicircum}2\$-accurate score estimates, we obtain Wasserstein distance guarantees for any distribution of bounded support or sufficiently decaying tails, as well as TV guarantees for distributions with further smoothness assumptions.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Lee, Holden and Lu, Jianfeng and Tan, Yixin},
	month = oct,
	year = {2022},
	note = {arXiv:2209.12381 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{lee_convergence_2023,
	title = {Convergence for score-based generative modeling with polynomial complexity},
	url = {http://arxiv.org/abs/2206.06227},
	abstract = {Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density \$p\$ given a score estimate (an estimate of \${\textbackslash}nabla {\textbackslash}ln p\$) that is accurate in \$L{\textasciicircum}2(p)\$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using annealing to obtain a warm start at each step. Moreover, we show that a predictor-corrector algorithm gives better convergence than using either portion alone.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Lee, Holden and Lu, Jianfeng and Tan, Yixin},
	month = may,
	year = {2023},
	note = {arXiv:2206.06227 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{benton_error_2024,
	title = {Error {Bounds} for {Flow} {Matching} {Methods}},
	url = {http://arxiv.org/abs/2305.16860},
	abstract = {Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDE). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODE) rather than SDE. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the \$L{\textasciicircum}2\$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an \$L{\textasciicircum}2\$ bound on the approximation error and a certain regularity condition on the data distributions.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Benton, Joe and Deligiannidis, George and Doucet, Arnaud},
	month = feb,
	year = {2024},
	note = {arXiv:2305.16860 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lipman_flow_2023,
	title = {Flow {Matching} for {Generative} {Modeling}},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	month = feb,
	year = {2023},
	note = {arXiv:2210.02747 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{albergo_building_2023,
	title = {Building {Normalizing} {Flows} with {Stochastic} {Interpolants}},
	url = {http://arxiv.org/abs/2209.15571},
	abstract = {A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet \$32{\textbackslash}times32\$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to \$128{\textbackslash}times128\$.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Albergo, Michael S. and Vanden-Eijnden, Eric},
	month = mar,
	year = {2023},
	note = {arXiv:2209.15571 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_score-based_2023,
	title = {Score-based {Generative} {Modeling} {Through} {Backward} {Stochastic} {Differential} {Equations}: {Inversion} and {Generation}},
	shorttitle = {Score-based {Generative} {Modeling} {Through} {Backward} {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2304.13224},
	doi = {10.48550/arXiv.2304.13224},
	abstract = {The proposed BSDE-based diffusion model represents a novel approach to diffusion modeling, which extends the application of stochastic differential equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion models, our model can determine the initial conditions necessary to reach a desired terminal distribution by adapting an existing score function. We demonstrate the theoretical guarantees of the model, the benefits of using Lipschitz networks for score matching, and its potential applications in various areas such as diffusion inversion, conditional diffusion, and uncertainty quantification. Our work represents a contribution to the field of score-based generative learning and offers a promising direction for solving real-world problems.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Wang, Zihao},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13224 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@book{macris_lecture_nodate,
	title = {Lecture notes},
	author = {Macris, Nicolas},
}

@misc{noauthor_sop_nodate,
	title = {{SOP}: {Gody}},
}

@book{macris_statistical_nodate,
	title = {Statistical physics for communication and signal processing},
	author = {Macris, Nicolas and Rudiger},
}

@article{bunne_optimal_nodate,
	title = {Optimal {Transport} in {Learning}, {Control}, and {Dynamical} {Systems}},
	language = {en},
	author = {Bunne, Charlotte},
}

@misc{song_score-based_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = feb,
	year = {2021},
	note = {arXiv:2011.13456 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{lesieur_constrained_2017,
	title = {Constrained {Low}-rank {Matrix} {Estimation}: {Phase} {Transitions}, {Approximate} {Message} {Passing} and {Applications}},
	volume = {2017},
	issn = {1742-5468},
	shorttitle = {Constrained {Low}-rank {Matrix} {Estimation}},
	url = {http://arxiv.org/abs/1701.00858},
	doi = {10.1088/1742-5468/aa7284},
	abstract = {This article is an extended version of previous work of the authors [40, 41] on low-rank matrix estimation in the presence of constraints on the factors into which the matrix is factorized. Low-rank matrix factorization is one of the basic methods used in data analysis for unsupervised learning of relevant features and other types of dimensionality reduction. We present a framework to study the constrained low-rank matrix estimation for a general prior on the factors, and a general output channel through which the matrix is observed. We draw a paralel with the study of vector-spin glass models - presenting a unifying way to study a number of problems considered previously in separate statistical physics works. We present a number of applications for the problem in data analysis. We derive in detail a general form of the low-rank approximate message passing (Low- RAMP) algorithm, that is known in statistical physics as the TAP equations. We thus unify the derivation of the TAP equations for models as different as the Sherrington-Kirkpatrick model, the restricted Boltzmann machine, the Hopfield model or vector (xy, Heisenberg and other) spin glasses. The state evolution of the Low-RAMP algorithm is also derived, and is equivalent to the replica symmetric solution for the large class of vector-spin glass models. In the section devoted to result we study in detail phase diagrams and phase transitions for the Bayes-optimal inference in low-rank matrix estimation. We present a typology of phase transitions and their relation to performance of algorithms such as the Low-RAMP or commonly used spectral methods.},
	number = {7},
	urldate = {2024-01-14},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Lesieur, Thibault and Krzakala, Florent and Zdeborová, Lenka},
	month = jul,
	year = {2017},
	note = {arXiv:1701.00858 [cond-mat, stat]},
	keywords = {Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Mathematics - Statistics Theory},
	pages = {073403},
}

@misc{zdeborova_statistical_2015,
	title = {Statistical physics of inference: {Thresholds} and algorithms},
	shorttitle = {Statistical physics of inference},
	url = {https://arxiv.org/abs/1511.02476v5},
	abstract = {Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.},
	language = {en},
	urldate = {2024-01-12},
	journal = {arXiv.org},
	author = {Zdeborová, Lenka and Krzakala, Florent},
	month = nov,
	year = {2015},
	doi = {10.1080/00018732.2016.1211393},
}

@misc{bandeira_notes_2018,
	title = {Notes on computational-to-statistical gaps: predictions using statistical physics},
	shorttitle = {Notes on computational-to-statistical gaps},
	url = {http://arxiv.org/abs/1803.11132},
	doi = {10.48550/arXiv.1803.11132},
	abstract = {In these notes we describe heuristics to predict computational-to-statistical gaps in certain statistical problems. These are regimes in which the underlying statistical problem is information-theoretically possible although no efficient algorithm exists, rendering the problem essentially unsolvable for large instances. The methods we describe here are based on mature, albeit non-rigorous, tools from statistical physics. These notes are based on a lecture series given by the authors at the Courant Institute of Mathematical Sciences in New York City, on May 16th, 2017.},
	urldate = {2024-01-10},
	publisher = {arXiv},
	author = {Bandeira, Afonso S. and Perry, Amelia and Wein, Alexander S.},
	month = apr,
	year = {2018},
	note = {arXiv:1803.11132 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_localization_2022,
	title = {Localization {Schemes}: {A} {Framework} for {Proving} {Mixing} {Bounds} for {Markov} {Chains}},
	shorttitle = {Localization {Schemes}},
	url = {http://arxiv.org/abs/2203.04163},
	doi = {10.48550/arXiv.2203.04163},
	abstract = {Two recent and seemingly-unrelated techniques for proving mixing bounds for Markov chains are: (i) the framework of Spectral Independence, introduced by Anari, Liu and Oveis Gharan, and its numerous extensions, which have given rise to several breakthroughs in the analysis of mixing times of discrete Markov chains and (ii) the Stochastic Localization technique which has proven useful in establishing mixing and expansion bounds for both log-concave measures and for measures on the discrete hypercube. In this paper, we introduce a framework which connects ideas from both techniques. Our framework unifies, simplifies and extends those two techniques. In its center is the concept of a localization scheme which, to every probability measure, assigns a martingale of probability measures which localize in space as time evolves. As it turns out, to every such scheme corresponds a Markov chain, and many chains of interest appear naturally in this framework. This viewpoint provides tools for deriving mixing bounds for the dynamics through the analysis of the corresponding localization process. Generalizations of concepts of Spectral Independence and Entropic Independence naturally arise from our definitions, and in particular we recover the main theorems in the spectral and entropic independence frameworks via simple martingale arguments (completely bypassing the need to use the theory of high-dimensional expanders). We demonstrate the strength of our proposed machinery by giving short and (arguably) simpler proofs to many mixing bounds in the recent literature, including giving the first \$O(n {\textbackslash}log n)\$ bound for the mixing time of Glauber dynamics on the hardcore-model (of arbitrary degree) in the tree-uniqueness regime.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Chen, Yuansi and Eldan, Ronen},
	month = jun,
	year = {2022},
	note = {arXiv:2203.04163 [math-ph, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematical Physics, Mathematics - Probability, Statistics - Computation},
}

@misc{montanari_estimation_2019,
	title = {Estimation of {Low}-{Rank} {Matrices} via {Approximate} {Message} {Passing}},
	url = {http://arxiv.org/abs/1711.01682},
	doi = {10.48550/arXiv.1711.01682},
	abstract = {Consider the problem of estimating a low-rank matrix when its entries are perturbed by Gaussian noise. If the empirical distribution of the entries of the spikes is known, optimal estimators that exploit this knowledge can substantially outperform simple spectral approaches. Recent work characterizes the asymptotic accuracy of Bayes-optimal estimators in the high-dimensional limit. In this paper we present a practical algorithm that can achieve Bayes-optimal accuracy above the spectral threshold. A bold conjecture from statistical physics posits that no polynomial-time algorithm achieves optimal error below the same threshold (unless the best estimator is trivial). Our approach uses Approximate Message Passing (AMP) in conjunction with a spectral initialization. AMP algorithms have proved successful in a variety of statistical estimation tasks, and are amenable to exact asymptotic analysis via state evolution. Unfortunately, state evolution is uninformative when the algorithm is initialized near an unstable fixed point, as often happens in low-rank matrix estimation. We develop a new analysis of AMP that allows for spectral initializations. Our main theorem is general and applies beyond matrix estimation. However, we use it to derive detailed predictions for the problem of estimating a rank-one matrix in noise. Special cases of this problem are closely related---via universality arguments---to the network community detection problem for two asymmetric communities. For general rank-one models, we show that AMP can be used to construct confidence intervals and control false discovery rate. We provide illustrations of the general methodology by considering the cases of sparse low-rank matrices and of block-constant low-rank matrices with symmetric blocks (we refer to the latter as to the `Gaussian Block Model').},
	urldate = {2023-12-27},
	publisher = {arXiv},
	author = {Montanari, Andrea and Venkataramanan, Ramji},
	month = aug,
	year = {2019},
	note = {arXiv:1711.01682 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{montanari_posterior_2023,
	title = {Posterior {Sampling} from the {Spiked} {Models} via {Diffusion} {Processes}},
	url = {http://arxiv.org/abs/2304.11449},
	doi = {10.48550/arXiv.2304.11449},
	abstract = {Sampling from the posterior is a key technical problem in Bayesian statistics. Rigorous guarantees are difficult to obtain for Markov Chain Monte Carlo algorithms of common use. In this paper, we study an alternative class of algorithms based on diffusion processes. The diffusion is constructed in such a way that, at its final time, it approximates the target posterior distribution. The stochastic differential equation that defines this process is discretized (using a Euler scheme) to provide an efficient sampling algorithm. Our construction of the diffusion is based on the notion of observation process and the related idea of stochastic localization. Namely, the diffusion process describes a sample that is conditioned on increasing information. An overlapping family of processes was derived in the machine learning literature via time-reversal. We apply this method to posterior sampling in the high-dimensional symmetric spiked model. We observe a rank-one matrix \$\{{\textbackslash}boldsymbol {\textbackslash}theta\}\{{\textbackslash}boldsymbol {\textbackslash}theta\}{\textasciicircum}\{{\textbackslash}sf T\}\$ corrupted by Gaussian noise, and want to sample \$\{{\textbackslash}boldsymbol {\textbackslash}theta\}\$ from the posterior. Our sampling algorithm makes use of an oracle that computes the posterior expectation of \$\{{\textbackslash}boldsymbol {\textbackslash}theta\}\$ given the data and the additional observation process. We provide an efficient implementation of this oracle using approximate message passing. We thus develop the first sampling algorithm for this problem with approximation guarantees.},
	urldate = {2023-12-24},
	publisher = {arXiv},
	author = {Montanari, Andrea and Wu, Yuchen},
	month = apr,
	year = {2023},
	note = {arXiv:2304.11449 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@misc{albergo_stochastic_2023,
	title = {Stochastic {Interpolants}: {A} {Unifying} {Framework} for {Flows} and {Diffusions}},
	shorttitle = {Stochastic {Interpolants}},
	url = {http://arxiv.org/abs/2303.08797},
	doi = {10.48550/arXiv.2303.08797},
	abstract = {A class of generative models that unifies flow-based and diffusion-based methods is introduced. These models extend the framework proposed in Albergo \& Vanden-Eijnden (2023), enabling the use of a broad class of continuous-time stochastic processes called `stochastic interpolants' to bridge any two arbitrary probability density functions exactly in finite time. These interpolants are built by combining data from the two prescribed densities with an additional latent variable that shapes the bridge in a flexible way. The time-dependent probability density function of the stochastic interpolant is shown to satisfy a first-order transport equation as well as a family of forward and backward Fokker-Planck equations with tunable diffusion coefficient. Upon consideration of the time evolution of an individual sample, this viewpoint immediately leads to both deterministic and stochastic generative models based on probability flow equations or stochastic differential equations with an adjustable level of noise. The drift coefficients entering these models are time-dependent velocity fields characterized as the unique minimizers of simple quadratic objective functions, one of which is a new objective for the score of the interpolant density. We show that minimization of these quadratic objectives leads to control of the likelihood for generative models built upon stochastic dynamics, while likelihood control for deterministic dynamics is more stringent. We also discuss connections with other methods such as score-based diffusion models, stochastic localization processes, probabilistic denoising techniques, and rectifying flows. In addition, we demonstrate that stochastic interpolants recover the Schr{\textbackslash}"odinger bridge between the two target densities when explicitly optimizing over the interpolant. Finally, algorithmic aspects are discussed and the approach is illustrated on numerical examples.},
	urldate = {2023-12-21},
	publisher = {arXiv},
	author = {Albergo, Michael S. and Boffi, Nicholas M. and Vanden-Eijnden, Eric},
	month = nov,
	year = {2023},
	note = {arXiv:2303.08797 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Probability},
}

@misc{vyas_beyond_2023,
	title = {Beyond {Implicit} {Bias}: {The} {Insignificance} of {SGD} {Noise} in {Online} {Learning}},
	shorttitle = {Beyond {Implicit} {Bias}},
	url = {http://arxiv.org/abs/2306.08590},
	doi = {10.48550/arXiv.2306.08590},
	abstract = {The success of SGD in deep learning has been ascribed by prior works to the implicit bias induced by high learning rate or small batch size ("SGD noise"). While prior works that focused on offline learning (i.e., multiple-epoch training), we study the impact of SGD noise on online (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that large learning rate and small batch size do not confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating larger or more cost-effective gradient steps. Our work suggests that SGD in the online regime can be construed as taking noisy steps along the "golden path" of the noiseless gradient flow algorithm. We provide evidence to support this hypothesis by conducting experiments that reduce SGD noise during training and by measuring the pointwise functional distance between models trained with varying SGD noise levels, but at equivalent loss values. Our findings challenge the prevailing understanding of SGD and offer novel insights into its role in online learning.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Kaplun, Gal and Kakade, Sham and Barak, Boaz},
	month = jun,
	year = {2023},
	note = {arXiv:2306.08590 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arous_online_2021,
	title = {Online stochastic gradient descent on non-convex losses from high-dimensional inference},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/20-1288.html},
	abstract = {Stochastic gradient descent (SGD) is a popular algorithm for optimization problems arising in high-dimensional inference tasks. Here one produces an estimator of an unknown parameter from independent samples of data by iteratively optimizing a loss function. This loss function is random and often non-convex. We study the performance of the simplest version of SGD, namely online SGD, from a random start in the setting where the parameter space is high-dimensional. We develop nearly sharp thresholds for the number of samples needed for consistent estimation as one varies the dimension. Our thresholds depend only on an intrinsic property of the population loss which we call the information exponent. In particular, our results do not assume uniform control on the loss itself, such as convexity or uniform derivative bounds. The thresholds we obtain are polynomial in the dimension and the precise exponent depends explicitly on the information exponent. As a consequence of our results, we find that except for the simplest tasks, almost all of the data is used simply in the initial search phase to obtain non-trivial correlation with the ground truth. Upon attaining non-trivial correlation, the descent is rapid and exhibits law of large numbers type behavior. We illustrate our approach by applying it to a wide set of inference tasks such as phase retrieval, and parameter estimation for generalized linear models, online PCA, and spiked tensor models, as well as to supervised learning for single-layer networks with general activation functions.},
	number = {106},
	urldate = {2023-11-27},
	journal = {Journal of Machine Learning Research},
	author = {Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
	year = {2021},
	pages = {1--51},
}

@misc{jentzen_mathematical_2023,
	title = {Mathematical {Introduction} to {Deep} {Learning}: {Methods}, {Implementations}, and {Theory}},
	shorttitle = {Mathematical {Introduction} to {Deep} {Learning}},
	url = {http://arxiv.org/abs/2310.20360},
	doi = {10.48550/arXiv.2310.20360},
	abstract = {This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-\{{\textbackslash}L\}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Jentzen, Arnulf and Kuckuck, Benno and von Wurstemberger, Philippe},
	month = oct,
	year = {2023},
	note = {arXiv:2310.20360 [cs, math, stat]},
	keywords = {68T07, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{belkin_fit_2021,
	title = {Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation},
	shorttitle = {Fit without fear},
	url = {http://arxiv.org/abs/2105.14368},
	doi = {10.48550/arXiv.2105.14368},
	abstract = {In the past decade the mathematical theory of machine learning has lagged far behind the triumphs of deep neural networks on practical challenges. However, the gap between theory and practice is gradually starting to close. In this paper I will attempt to assemble some pieces of the remarkable and still incomplete mathematical mosaic emerging from the efforts to understand the foundations of deep learning. The two key themes will be interpolation, and its sibling, over-parameterization. Interpolation corresponds to fitting data, even noisy data, exactly. Over-parameterization enables interpolation and provides flexibility to select a right interpolating model. As we will see, just as a physical prism separates colors mixed within a ray of light, the figurative prism of interpolation helps to disentangle generalization and optimization properties within the complex picture of modern Machine Learning. This article is written with belief and hope that clearer understanding of these issues brings us a step closer toward a general theory of deep learning and machine learning.},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Belkin, Mikhail},
	month = may,
	year = {2021},
	note = {arXiv:2105.14368 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{hastie_surprises_2020,
	title = {Surprises in {High}-{Dimensional} {Ridgeless} {Least} {Squares} {Interpolation}},
	url = {http://arxiv.org/abs/1903.08560},
	doi = {10.48550/arXiv.1903.08560},
	abstract = {Interpolators -- estimators that achieve zero training error -- have attracted growing attention in machine learning, mainly because state-of-the art neural networks appear to be models of this type. In this paper, we study minimum \${\textbackslash}ell\_2\$ norm ("ridgeless") interpolation in high-dimensional least squares regression. We consider two different models for the feature distribution: a linear model, where the feature vectors \$x\_i {\textbackslash}in \{{\textbackslash}mathbb R\}{\textasciicircum}p\$ are obtained by applying a linear transform to a vector of i.i.d. entries, \$x\_i = {\textbackslash}Sigma{\textasciicircum}\{1/2\} z\_i\$ (with \$z\_i {\textbackslash}in \{{\textbackslash}mathbb R\}{\textasciicircum}p\$); and a nonlinear model, where the feature vectors are obtained by passing the input through a random one-layer neural network, \$x\_i = {\textbackslash}varphi(W z\_i)\$ (with \$z\_i {\textbackslash}in \{{\textbackslash}mathbb R\}{\textasciicircum}d\$, \$W {\textbackslash}in \{{\textbackslash}mathbb R\}{\textasciicircum}\{p {\textbackslash}times d\}\$ a matrix of i.i.d. entries, and \${\textbackslash}varphi\$ an activation function acting componentwise on \$W z\_i\$). We recover -- in a precise quantitative way -- several phenomena that have been observed in large-scale neural networks and kernel machines, including the "double descent" behavior of the prediction risk, and the potential benefits of overparametrization.},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
	month = dec,
	year = {2020},
	note = {arXiv:1903.08560 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{belkin_two_2020,
	title = {Two models of double descent for weak features},
	volume = {2},
	issn = {2577-0187},
	url = {http://arxiv.org/abs/1903.07571},
	doi = {10.1137/20M1336072},
	abstract = {The "double descent" risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features \$p\$ is close to the sample size \$n\$, but also that the risk decreases towards its minimum as \$p\$ increases beyond \$n\$. This behavior is contrasted with that of "prescient" models that select features in an a priori optimal order.},
	number = {4},
	urldate = {2023-11-02},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
	month = jan,
	year = {2020},
	note = {arXiv:1903.07571 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1167--1180},
}

@article{belkin_two_2020-1,
	title = {Two models of double descent for weak features},
	volume = {2},
	issn = {2577-0187},
	url = {http://arxiv.org/abs/1903.07571},
	doi = {10.1137/20M1336072},
	abstract = {The "double descent" risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features \$p\$ is close to the sample size \$n\$, but also that the risk decreases towards its minimum as \$p\$ increases beyond \$n\$. This behavior is contrasted with that of "prescient" models that select features in an a priori optimal order.},
	number = {4},
	urldate = {2023-11-01},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
	month = jan,
	year = {2020},
	note = {arXiv:1903.07571 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1167--1180},
}

@misc{liu_double_2022,
	title = {On the {Double} {Descent} of {Random} {Features} {Models} {Trained} with {SGD}},
	url = {http://arxiv.org/abs/2110.06910},
	doi = {10.48550/arXiv.2110.06910},
	abstract = {We study generalization properties of random features (RF) regression in high dimensions optimized by stochastic gradient descent (SGD) in under-/over-parameterized regime. In this work, we derive precise non-asymptotic error bounds of RF regression under both constant and polynomial-decay step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well for interpolation learning, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimum-norm interpolator, as a theoretical justification of using SGD in practice.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Liu, Fanghui and Suykens, Johan A. K. and Cevher, Volkan},
	month = oct,
	year = {2022},
	note = {arXiv:2110.06910 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wainwright_sharp_2009,
	title = {Sharp {Thresholds} for {High}-{Dimensional} and {Noisy} {Sparsity} {Recovery} {Using} \${\textbackslash}ell \_\{1\}\$-{Constrained} {Quadratic} {Programming} ({Lasso})},
	volume = {55},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/4839045/},
	doi = {10.1109/TIT.2009.2016018},
	language = {en},
	number = {5},
	urldate = {2023-10-09},
	journal = {IEEE Transactions on Information Theory},
	author = {Wainwright, Martin J.},
	month = may,
	year = {2009},
	pages = {2183--2202},
}

@misc{gamarnik_high-dimensional_2017,
	title = {High-{Dimensional} {Regression} with {Binary} {Coefficients}. {Estimating} {Squared} {Error} and a {Phase} {Transition}},
	url = {https://arxiv.org/abs/1701.04455v3},
	abstract = {We consider a sparse linear regression model Y=X{\textbackslash}beta{\textasciicircum}\{*\}+W where X has a Gaussian entries, W is the noise vector with mean zero Gaussian entries, and {\textbackslash}beta{\textasciicircum}\{*\} is a binary vector with support size (sparsity) k. Using a novel conditional second moment method we obtain a tight up to a multiplicative constant approximation of the optimal squared error {\textbackslash}min\_\{{\textbackslash}beta\}{\textbackslash}{\textbar}Y-X{\textbackslash}beta{\textbackslash}{\textbar}\_\{2\}, where the minimization is over all k-sparse binary vectors {\textbackslash}beta. The approximation reveals interesting structural properties of the underlying regression problem. In particular, a) We establish that n{\textasciicircum}*=2k{\textbackslash}log p/{\textbackslash}log (2k/{\textbackslash}sigma{\textasciicircum}\{2\}+1) is a phase transition point with the following "all-or-nothing" property. When n exceeds n{\textasciicircum}\{*\}, (2k){\textasciicircum}\{-1\}{\textbackslash}{\textbar}{\textbackslash}beta\_\{2\}-{\textbackslash}beta{\textasciicircum}*{\textbackslash}{\textbar}\_0{\textbackslash}approx 0, and when n is below n{\textasciicircum}\{*\}, (2k){\textasciicircum}\{-1\}{\textbackslash}{\textbar}{\textbackslash}beta\_\{2\}-{\textbackslash}beta{\textasciicircum}*{\textbackslash}{\textbar}\_0{\textbackslash}approx 1, where {\textbackslash}beta\_2 is the optimal solution achieving the smallest squared error. With this we prove that n{\textasciicircum}\{*\} is the asymptotic threshold for recovering {\textbackslash}beta{\textasciicircum}* information theoretically. b) We compute the squared error for an intermediate problem {\textbackslash}min\_\{{\textbackslash}beta\}{\textbackslash}{\textbar}Y-X{\textbackslash}beta{\textbackslash}{\textbar}\_\{2\} where minimization is restricted to vectors {\textbackslash}beta with {\textbackslash}{\textbar}{\textbackslash}beta-{\textbackslash}beta{\textasciicircum}\{*\}{\textbackslash}{\textbar}\_0=2k {\textbackslash}zeta, for {\textbackslash}zeta{\textbackslash}in [0,1]. We show that a lower bound part {\textbackslash}Gamma({\textbackslash}zeta) of the estimate, which corresponds to the estimate based on the first moment method, undergoes a phase transition at three different thresholds, namely n\_\{{\textbackslash}text\{inf,1\}\}={\textbackslash}sigma{\textasciicircum}2{\textbackslash}log p, which is information theoretic bound for recovering {\textbackslash}beta{\textasciicircum}* when k=1 and {\textbackslash}sigma is large, then at n{\textasciicircum}\{*\} and finally at n\_\{{\textbackslash}text\{LASSO/CS\}\}. c) We establish a certain Overlap Gap Property (OGP) on the space of all binary vectors {\textbackslash}beta when n{\textbackslash}le ck{\textbackslash}log p for sufficiently small constant c. We conjecture that OGP is the source of algorithmic hardness of solving the minimization problem {\textbackslash}min\_\{{\textbackslash}beta\}{\textbackslash}{\textbar}Y-X{\textbackslash}beta{\textbackslash}{\textbar}\_\{2\} in the regime n{\textless}n\_\{{\textbackslash}text\{LASSO/CS\}\}.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Gamarnik, David and Zadik, Ilias},
	month = jan,
	year = {2017},
}

@misc{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	doi = {10.48550/arXiv.1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2023-09-30},
	publisher = {arXiv},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv:1912.02292 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{eldan_lecture_nodate,
	title = {Lecture notes - {From} stochastic calculus to geometric inequalities},
	language = {en},
	author = {Eldan, Ronen},
}

@article{eldan_analysis_nodate,
	title = {Analysis of high-dimensional distributions using pathwise methods},
	abstract = {The goal of this note is to present an emerging method in the analysis of high dimensional distributions, which exhibits applications to several mathematical fields, such as functional analysis, convex and discrete geometry, combinatorics and mathematical physics. The method is based on pathwise analysis: One constructs a stochastic process, driven by Brownian motion, associated with the high-dimensional distribution in hand. Quantities of interest related to the distribution, such as covariance, entropy and spectral gap are then expressed via corresponding properties of the stochastic process, such as quadratic variation, making the former tractable through the analysis the latter. We focus on one particular manifestation of this approach, the Stochastic Localization process. We review several results which can be obtained using Stochastic Localization and outline the main steps towards their proofs. By doing so, we try to demonstrate some of the ideas and advantages of the pathwise approach. We focus on two types of results relevant to high dimensional distributions: The first one has to do with dimension-free concentration bounds, manifested by functional inequalities which have no explicit dependence on the dimension. Our main focus in this respect will be on the Kannan-Lova´sz-Simonovits conjecture, concerning the isoperimetry of high-dimensional log-concave measures. Additionally, we discuss concentration inequalities for Ising models and expansion bounds for complexanalytic sets. The second type of results concern the decomposition of a high-dimensional measure into mixtures of measures attaining a simple structure, with applications to meanfield approximations.},
	language = {en},
	author = {Eldan, Ronen},
}

@misc{alaoui_information-theoretic_2021,
	title = {An {Information}-{Theoretic} {View} of {Stochastic} {Localization}},
	url = {http://arxiv.org/abs/2109.00709},
	doi = {10.48550/arXiv.2109.00709},
	abstract = {Given a probability measure \${\textbackslash}mu\$ over \$\{{\textbackslash}mathbb R\}{\textasciicircum}n\$, it is often useful to approximate it by the convex combination of a small number of probability measures, such that each component is close to a product measure. Recently, Ronen Eldan used a stochastic localization argument to prove a general decomposition result of this type. In Eldan's theorem, the `number of components' is characterized by the entropy of the mixture, and `closeness to product' is characterized by the covariance matrix of each component. We present an elementary proof of Eldan's theorem which makes use of an information theory (or estimation theory) interpretation. The proof is analogous to the one of an earlier decomposition result known as the `pinning lemma.'},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Alaoui, Ahmed El and Montanari, Andrea},
	month = sep,
	year = {2021},
	note = {arXiv:2109.00709 [cs, math]},
	keywords = {Computer Science - Information Theory, Mathematics - Probability},
}

@misc{alaoui_sampling_2022,
	title = {Sampling from the {Sherrington}-{Kirkpatrick} {Gibbs} measure via algorithmic stochastic localization},
	url = {http://arxiv.org/abs/2203.05093},
	doi = {10.48550/arXiv.2203.05093},
	abstract = {We consider the Sherrington-Kirkpatrick model of spin glasses at high-temperature and no external field, and study the problem of sampling from the Gibbs distribution \${\textbackslash}mu\$ in polynomial time. We prove that, for any inverse temperature \${\textbackslash}beta{\textless}1/2\$, there exists an algorithm with complexity \$O(n{\textasciicircum}2)\$ that samples from a distribution \${\textbackslash}mu{\textasciicircum}\{alg\}\$ which is close in normalized Wasserstein distance to \${\textbackslash}mu\$. Namely, there exists a coupling of \${\textbackslash}mu\$ and \${\textbackslash}mu{\textasciicircum}\{alg\}\$ such that if \$(x,x{\textasciicircum}\{alg\}){\textbackslash}in{\textbackslash}\{-1,+1{\textbackslash}\}{\textasciicircum}n{\textbackslash}times {\textbackslash}\{-1,+1{\textbackslash}\}{\textasciicircum}n\$ is a pair drawn from this coupling, then \$n{\textasciicircum}\{-1\}{\textbackslash}mathbb E{\textbackslash}\{{\textbar}{\textbar}x-x{\textasciicircum}\{alg\}{\textbar}{\textbar}\_2{\textasciicircum}2{\textbackslash}\}=o\_n(1)\$. The best previous results, by Bauerschmidt and Bodineau and by Eldan, Koehler, and Zeitouni, implied efficient algorithms to approximately sample (under a stronger metric) for \${\textbackslash}beta{\textless}1/4\$. We complement this result with a negative one, by introducing a suitable "stability" property for sampling algorithms, which is verified by many standard techniques. We prove that no stable algorithm can approximately sample for \${\textbackslash}beta{\textgreater}1\$, even under the normalized Wasserstein metric. Our sampling method is based on an algorithmic implementation of stochastic localization, which progressively tilts the measure \${\textbackslash}mu\$ towards a single configuration, together with an approximate message passing algorithm that is used to approximate the mean of the tilted measure.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05093 [cond-mat]},
	keywords = {Computer Science - Data Structures and Algorithms, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Probability},
}

@misc{montanari_sampling_2023,
	title = {Sampling, {Diffusions}, and {Stochastic} {Localization}},
	url = {http://arxiv.org/abs/2305.10690},
	doi = {10.48550/arXiv.2305.10690},
	abstract = {Diffusions are a successful technique to sample from high-dimensional distributions can be either explicitly given or learnt from a collection of samples. They implement a diffusion process whose endpoint is a sample from the target distribution and whose drift is typically represented as a neural network. Stochastic localization is a successful technique to prove mixing of Markov Chains and other functional inequalities in high dimension. An algorithmic version of stochastic localization was introduced in [EAMS2022], to obtain an algorithm that samples from certain statistical mechanics models. This notes have three objectives: (i) Generalize the construction [EAMS2022] to other stochastic localization processes; (ii) Clarify the connection between diffusions and stochastic localization. In particular we show that standard denoising diffusions are stochastic localizations but other examples that are naturally suggested by the proposed viewpoint; (iii) Describe some insights that follow from this viewpoint.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Montanari, Andrea},
	month = may,
	year = {2023},
	note = {arXiv:2305.10690 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{montanari_sampling_2023-1,
	title = {Sampling, {Diffusions}, and {Stochastic} {Localization}},
	url = {http://arxiv.org/abs/2305.10690},
	doi = {10.48550/arXiv.2305.10690},
	abstract = {Diffusions are a successful technique to sample from high-dimensional distributions can be either explicitly given or learnt from a collection of samples. They implement a diffusion process whose endpoint is a sample from the target distribution and whose drift is typically represented as a neural network. Stochastic localization is a successful technique to prove mixing of Markov Chains and other functional inequalities in high dimension. An algorithmic version of stochastic localization was introduced in [EAMS2022], to obtain an algorithm that samples from certain statistical mechanics models. This notes have three objectives: (i) Generalize the construction [EAMS2022] to other stochastic localization processes; (ii) Clarify the connection between diffusions and stochastic localization. In particular we show that standard denoising diffusions are stochastic localizations but other examples that are naturally suggested by the proposed viewpoint; (iii) Describe some insights that follow from this viewpoint.},
	urldate = {2023-09-20},
	publisher = {arXiv},
	author = {Montanari, Andrea},
	month = may,
	year = {2023},
	note = {arXiv:2305.10690 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_220507488_nodate,
	title = {[2205.07488] {Robust} {Testing} in {High}-{Dimensional} {Sparse} {Models}},
	url = {https://arxiv.org/abs/2205.07488},
	urldate = {2023-08-05},
}

@book{karatzas_brownian_1998,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Brownian {Motion} and {Stochastic} {Calculus}},
	volume = {113},
	isbn = {978-0-387-97655-6 978-1-4612-0949-2},
	language = {en},
	urldate = {2023-07-23},
	publisher = {Springer},
	author = {Karatzas, Ioannis and Shreve, Steven E.},
	year = {1998},
	keywords = {Brownian motion, Markov process, Martingal, Martingale, Semimartingal, Semimartingale, YellowSale2006, adopted-textbook, differential equation, integration, local time, measure, probability, stochastic calculus, stochastic differential equation, stochastic process, stochastic processes},
}

@inproceedings{shamir_stochastic_2013,
	title = {Stochastic {Gradient} {Descent} for {Non}-smooth {Optimization}: {Convergence} {Results} and {Optimal} {Averaging} {Schemes}},
	shorttitle = {Stochastic {Gradient} {Descent} for {Non}-smooth {Optimization}},
	abstract = {Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines.  In this paper, we investigate the performance of SGD {\textbackslash}emphwithout such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the {\textbackslash}emphlast SGD iterate scales as O({\textbackslash}log(T)/{\textbackslash}sqrtT) for non-smooth convex objective functions, and O({\textbackslash}log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in {\textbackslash}citetRakhShaSri12arxiv is not as simple to implement). Finally, we provide some experimental illustrations.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shamir, Ohad and Zhang, Tong},
	month = feb,
	year = {2013},
	pages = {71--79},
}

@misc{yehudai_power_2022,
	title = {On the {Power} and {Limitations} of {Random} {Features} for {Understanding} {Neural} {Networks}},
	doi = {10.48550/arXiv.1904.00687},
	abstract = {Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features. In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we first review these techniques, providing a simple and self-contained analysis for one-hidden-layer networks. We then argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we rigorously show that random features cannot be used to learn even a single ReLU neuron with standard Gaussian inputs, unless the network size (or magnitude of the weights) is exponentially large. Since a single neuron is learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks.},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Yehudai, Gilad and Shamir, Ohad},
	month = feb,
	year = {2022},
	note = {arXiv:1904.00687},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{grohs_proof_2018,
	title = {A proof that artificial neural networks overcome the curse of dimensionality in the numerical approximation of {Black}-{Scholes} partial differential equations},
	doi = {10.48550/arXiv.1809.02362},
	abstract = {Artificial neural networks (ANNs) have very successfully been used in numerical simulations for a series of computational problems ranging from image classification/image recognition, speech recognition, time series analysis, game intelligence, and computational advertising to numerical approximations of partial differential equations (PDEs). Such numerical simulations suggest that ANNs have the capacity to very efficiently approximate high-dimensional functions and, especially, such numerical simulations indicate that ANNs seem to admit the fundamental power to overcome the curse of dimensionality when approximating the high-dimensional functions appearing in the above named computational problems. There are also a series of rigorous mathematical approximation results for ANNs in the scientific literature. Some of these mathematical results prove convergence without convergence rates and some of these mathematical results even rigorously establish convergence rates but there are only a few special cases where mathematical results can rigorously explain the empirical success of ANNs when approximating high-dimensional functions. The key contribution of this article is to disclose that ANNs can efficiently approximate high-dimensional functions in the case of numerical approximations of Black-Scholes PDEs. More precisely, this work reveals that the number of required parameters of an ANN to approximate the solution of the Black-Scholes PDE grows at most polynomially in both the reciprocal of the prescribed approximation accuracy \${\textbackslash}varepsilon {\textgreater} 0\$ and the PDE dimension \$d {\textbackslash}in {\textbackslash}mathbb\{N\}\$ and we thereby prove, for the first time, that ANNs do indeed overcome the curse of dimensionality in the numerical approximation of Black-Scholes PDEs.},
	urldate = {2022-12-19},
	publisher = {arXiv},
	author = {Grohs, Philipp and Hornung, Fabian and Jentzen, Arnulf and von Wurstemberger, Philippe},
	month = sep,
	year = {2018},
	note = {arXiv:1809.02362},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability, Quantitative Finance - Mathematical Finance},
}

@article{beck_overview_2023,
	title = {An overview on deep learning-based approximation methods for partial differential equations},
	volume = {28},
	issn = {1531-3492, 1553-524X},
	doi = {10.3934/dcdsb.2022238},
	abstract = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations (PDEs). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional PDEs. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional PDEs. In this article we offer an introduction to this field of research by revisiting selected mathematical results related to deep learning approximation methods for PDEs and reviewing the main ideas of their proofs. We also provide a short overview of the recent literature in this area of research.},
	number = {6},
	urldate = {2023-02-22},
	journal = {Discrete and Continuous Dynamical Systems - B},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno},
	year = {2023},
	keywords = {65M99 (Primary), 35-02, 65-02, 68T07 (Secondary), Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	pages = {3697--3746},
}

@article{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1718942115},
	abstract = {Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the "curse of dimensionality". This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.},
	number = {34},
	urldate = {2022-09-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	month = aug,
	year = {2018},
	note = {arXiv:1707.02568},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Mathematics - Probability},
	pages = {8505--8510},
}

@article{e_multilevel_2021,
	title = {Multilevel {Picard} iterations for solving smooth semilinear parabolic heat equations},
	volume = {2},
	issn = {2662-2963, 2662-2971},
	doi = {10.1007/s42985-021-00089-5},
	abstract = {We introduce a new family of numerical algorithms for approximating solutions of general high-dimensional semilinear parabolic partial differential equations at single space-time points. The algorithm is obtained through a delicate combination of the Feynman-Kac and the Bismut-Elworthy-Li formulas, and an approximate decomposition of the Picard fixed-point iteration with multilevel accuracy. The algorithm has been tested on a variety of semilinear partial differential equations that arise in physics and finance, with very satisfactory results. Analytical tools needed for the analysis of such algorithms, including a semilinear Feynman-Kac formula, a new class of semi-norms and their recursive inequalities, are also introduced. They allow us to prove for semilinear heat equations with gradient-independent nonlinearity that the computational complexity of the proposed algorithm is bounded by \$O(d{\textbackslash},{\textbackslash}varepsilon{\textasciicircum}\{-(4+{\textbackslash}delta)\})\$ for any \${\textbackslash}delta {\textbackslash}in (0,{\textbackslash}infty)\$ under suitable assumptions, where \$d{\textbackslash}in {\textbackslash}mathbb\{N\}\$ is the dimensionality of the problem and \${\textbackslash}varepsilon{\textbackslash}in(0,{\textbackslash}infty)\$ is the prescribed accuracy.},
	number = {6},
	urldate = {2023-03-06},
	journal = {Partial Differential Equations and Applications},
	author = {E, Weinan and Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas},
	month = dec,
	year = {2021},
	keywords = {Mathematics - Numerical Analysis},
	pages = {80},
}

@misc{hutzenthaler_multilevel_2020,
	title = {Multilevel {Picard} approximations for high-dimensional semilinear second-order {PDEs} with {Lipschitz} nonlinearities},
	doi = {10.48550/arXiv.2009.02484},
	abstract = {The recently introduced full-history recursive multilevel Picard (MLP) approximation methods have turned out to be quite successful in the numerical approximation of solutions of high-dimensional nonlinear PDEs. In particular, there are mathematical convergence results in the literature which prove that MLP approximation methods do overcome the curse of dimensionality in the numerical approximation of nonlinear second-order PDEs in the sense that the number of computational operations of the proposed MLP approximation method grows at most polynomially in both the reciprocal \$1/{\textbackslash}epsilon\$ of the prescribed approximation accuracy \${\textbackslash}epsilon{\textgreater}0\$ and the PDE dimension \$d{\textbackslash}in {\textbackslash}mathbb\{N\}={\textbackslash}\{1,2,3, {\textbackslash}ldots{\textbackslash}\}\$. However, in each of the convergence results for MLP approximation methods in the literature it is assumed that the coefficient functions in front of the second-order differential operator are affine linear. In particular, until today there is no result in the scientific literature which proves that any semilinear second-order PDE with a general time horizon and a non affine linear coefficient function in front of the second-order differential operator can be approximated without the curse of dimensionality. It is the key contribution of this article to overcome this obstacle and to propose and analyze a new type of MLP approximation method for semilinear second-order PDEs with possibly nonlinear coefficient functions in front of the second-order differential operators. In particular, the main result of this article proves that this new MLP approximation method does indeed overcome the curse of dimensionality in the numerical approximation of semilinear second-order PDEs.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Nguyen, Tuan Anh},
	month = oct,
	year = {2020},
	note = {arXiv:2009.02484},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Probability},
}

@article{pardoux_adapted_1990,
	title = {Adapted solution of a backward stochastic differential equation},
	volume = {14},
	issn = {0167-6911},
	doi = {10.1016/0167-6911(90)90082-6},
	abstract = {Let Wt; t ϵ [0, 1] be a standard k-dimensional Weiner process defined on a probability space (Ω, F, P), and let Ft denote its natural filtration. Given a F1 measurable d-dimensional random vector X, we look for an adapted pair of processes \{x(t), y(t); t ϵ [0, 1]\} with values in Rd and Rd×k respectively, which solves an equation of the form: x(t) + ∫t1f(s, x(s), y(s)) ds + ∫t1 [g(s, x(s)) + y(s)] dWs = X. A linearized version of that equation appears in stochastic control theory as the equation satisfied by the adjoint process. We also generalize our results to the following equation: x(t) + ∫t1f(s, x(s), y(s)) ds + ∫t1 g(s, x(s)) + y(s)) dWs = X under rather restrictive assumptions on g.},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Systems \& Control Letters},
	author = {Pardoux, E. and Peng, S. G.},
	month = jan,
	year = {1990},
	keywords = {Backward stochastic differential equation, adapted process},
	pages = {55--61},
}

@article{bismut_introductory_1978,
	title = {An {Introductory} {Approach} to {Duality} in {Optimal} {Stochastic} {Control}},
	volume = {20},
	issn = {0036-1445},
	doi = {10.1137/1020004},
	abstract = {We use a Fenchel duality scheme for solving control-constrained linear-quadratic optimal control problems.  We derive the dual of the optimal control problem explicitly, where the control constraints are embedded in the dual objective functional, which turns out to be continuously differentiable.  We specifically prove that strong duality and saddle point properties hold.  We carry out numerical experiments with the discretized primal and dual formulations of the problem, for which we implement powerful existing finite-dimensional optimization techniques and associated software.  We illustrate that by solving the dual of the optimal control problem, instead of the primal one, significant computational savings can be achieved.  Other numerical advantages are also discussed.},
	number = {1},
	urldate = {2023-07-19},
	journal = {SIAM Review},
	author = {Bismut, Jean-Michel},
	month = jan,
	year = {1978},
	pages = {62--78},
}

@misc{gonon_random_2021,
	title = {Random feature neural networks learn {Black}-{Scholes} type {PDEs} without curse of dimensionality},
	doi = {10.48550/arXiv.2106.08900},
	abstract = {This article investigates the use of random feature neural networks for learning Kolmogorov partial (integro-)differential equations associated to Black-Scholes and more general exponential L{\textbackslash}'evy models. Random feature neural networks are single-hidden-layer feedforward neural networks in which only the output weights are trainable. This makes training particularly simple, but (a priori) reduces expressivity. Interestingly, this is not the case for Black-Scholes type PDEs, as we show here. We derive bounds for the prediction error of random neural networks for learning sufficiently non-degenerate Black-Scholes type models. A full error analysis is provided and it is shown that the derived bounds do not suffer from the curse of dimensionality. We also investigate an application of these results to basket options and validate the bounds numerically. These results prove that neural networks are able to {\textbackslash}textit\{learn\} solutions to Black-Scholes type PDEs without the curse of dimensionality. In addition, this provides an example of a relevant learning problem in which random feature neural networks are provably efficient.},
	urldate = {2023-01-21},
	publisher = {arXiv},
	author = {Gonon, Lukas},
	month = jun,
	year = {2021},
	note = {arXiv:2106.08900},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Quantitative Finance - Mathematical Finance, Statistics - Machine Learning},
}

@article{e_algorithms_2022,
	title = {Algorithms for {Solving} {High} {Dimensional} {PDEs}: {From} {Nonlinear} {Monte} {Carlo} to {Machine} {Learning}},
	volume = {35},
	issn = {0951-7715, 1361-6544},
	shorttitle = {Algorithms for {Solving} {High} {Dimensional} {PDEs}},
	doi = {10.1088/1361-6544/ac337f},
	abstract = {In recent years, tremendous progress has been made on numerical algorithms for solving partial differential equations (PDEs) in a very high dimension, using ideas from either nonlinear (multilevel) Monte Carlo or deep learning. They are potentially free of the curse of dimensionality for many different applications and have been proven to be so in the case of some nonlinear Monte Carlo methods for nonlinear parabolic PDEs. In this paper, we review these numerical and theoretical advances. In addition to algorithms based on stochastic reformulations of the original problem, such as the multilevel Picard iteration and the Deep BSDE method, we also discuss algorithms based on the more traditional Ritz, Galerkin, and least square formulations. We hope to demonstrate to the reader that studying PDEs as well as control and variational problems in very high dimensions might very well be among the most promising new directions in mathematics and scientific computing in the near future.},
	number = {1},
	urldate = {2022-09-24},
	journal = {Nonlinearity},
	author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
	month = jan,
	year = {2022},
	keywords = {65C05, 65K10, 65M75, 90C06, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	pages = {278--310},
}

@book{klebaner_introduction_1998,
	edition = {2},
	title = {Introduction to {Stochastic} {Calculus} with {Applications}},
	publisher = {Imperial College Press, London},
	author = {Klebaner, Fima C},
	year = {1998},
}

@misc{noauthor_introductory_nodate,
	title = {An {Introductory} {Approach} to {Duality} in {Optimal} {Stochastic} {Control} {\textbar} {SIAM} {Review}},
	url = {https://epubs.siam.org/doi/10.1137/1020004},
	urldate = {2023-07-19},
}

@book{yong_stochastic_nodate,
	edition = {1},
	series = {Stochastic {Modelling} and {Applied} {Probability}},
	title = {Stochastic {Controls}: {Hamiltonian} {Systems} and {HJB} {Equations}},
	publisher = {Springer New York, NY},
	author = {Yong, Jiongmin and Yu Zhou, Xun},
}

@article{han_convergence_2020,
	title = {Convergence of the {Deep} {BSDE} {Method} for {Coupled} {FBSDEs}},
	volume = {5},
	issn = {2367-0126},
	url = {http://arxiv.org/abs/1811.01165},
	doi = {10.1186/s41546-020-00047-w},
	abstract = {The recently proposed numerical algorithm, deep BSDE method, has shown remarkable performance in solving high-dimensional forward-backward stochastic differential equations (FBSDEs) and parabolic partial differential equations (PDEs). This article lays a theoretical foundation for the deep BSDE method in the general case of coupled FBSDEs. In particular, a posteriori error estimation of the solution is provided and it is proved that the error converges to zero given the universal approximation capability of neural networks. Numerical results are presented to demonstrate the accuracy of the analyzed algorithm in solving high-dimensional coupled FBSDEs.},
	number = {1},
	urldate = {2023-07-11},
	journal = {Probability, Uncertainty and Quantitative Risk},
	author = {Han, Jiequn and Long, Jihao},
	month = dec,
	year = {2020},
	note = {arXiv:1811.01165 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability},
	pages = {5},
}

@misc{gonon_approximation_2021,
	title = {Approximation {Bounds} for {Random} {Neural} {Networks} and {Reservoir} {Systems}},
	url = {http://arxiv.org/abs/2002.05933},
	doi = {10.48550/arXiv.2002.05933},
	abstract = {This work studies approximation based on single-hidden-layer feedforward and recurrent neural networks with randomly generated internal weights. These methods, in which only the last layer of weights and a few hyperparameters are optimized, have been successfully applied in a wide range of static and dynamic learning problems. Despite the popularity of this approach in empirical tasks, important theoretical questions regarding the relation between the unknown function, the weight distribution, and the approximation rate have remained open. In this work it is proved that, as long as the unknown function, functional, or dynamical system is sufficiently regular, it is possible to draw the internal weights of the random (recurrent) neural network from a generic distribution (not depending on the unknown object) and quantify the error in terms of the number of neurons and the hyperparameters. In particular, this proves that echo state networks with randomly generated weights are capable of approximating a wide class of dynamical systems arbitrarily well and thus provides the first mathematical explanation for their empirically observed success at learning dynamical systems.},
	urldate = {2023-07-10},
	publisher = {arXiv},
	author = {Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
	month = feb,
	year = {2021},
	note = {arXiv:2002.05933 [cs, eess, math]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Probability},
}

@inproceedings{rahimi_random_2007,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	volume = {20},
	url = {https://papers.nips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
	urldate = {2023-07-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	year = {2007},
}

@misc{mei_generalization_2020,
	title = {The generalization error of random features regression: {Precise} asymptotics and double descent curve},
	shorttitle = {The generalization error of random features regression},
	url = {http://arxiv.org/abs/1908.05355},
	abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. Neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the test error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the test error is found above the interpolation threshold, often in the extreme overparametrization regime in which the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the \$d\$-dimensional sphere \${\textbackslash}mathbb S{\textasciicircum}\{d-1\}\$, from \$n\$ i.i.d. samples \$({\textbackslash}boldsymbol x\_i, y\_i){\textbackslash}in {\textbackslash}mathbb S{\textasciicircum}\{d-1\} {\textbackslash}times {\textbackslash}mathbb R\$, \$i{\textbackslash}le n\$. We perform ridge regression on \$N\$ random features of the form \${\textbackslash}sigma({\textbackslash}boldsymbol w\_a{\textasciicircum}\{{\textbackslash}mathsf T\} {\textbackslash}boldsymbol x)\$, \$a{\textbackslash}le N\$. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the test error, in the limit \$N,n,d{\textbackslash}to {\textbackslash}infty\$ with \$N/d\$ and \$n/d\$ fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures.},
	urldate = {2023-07-09},
	publisher = {arXiv},
	author = {Mei, Song and Montanari, Andrea},
	month = dec,
	year = {2020},
	note = {arXiv:1908.05355 [math, stat]},
	keywords = {62J99, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{noauthor_stochastic_nodate,
	title = {Stochastic {Analysis}-{Lecture} notes},
	author = {, Olivier},
}

@book{noauthor_distribution-free_nodate,
	title = {A distribution-free theory of nonparametric regression},
	author = {, Gyorfi},
}

@book{noauthor_backward_nodate,
	title = {Backward stochastic differential equations and quasilinear parabolic partial differential equations},
	author = {, E. Pardoux \& S. Peng},
}

@article{han_convergence_2020-1,
	title = {Convergence of the {Deep} {BSDE} {Method} for {Coupled} {FBSDEs}},
	volume = {5},
	issn = {2367-0126},
	url = {http://arxiv.org/abs/1811.01165},
	doi = {10.1186/s41546-020-00047-w},
	abstract = {The recently proposed numerical algorithm, deep BSDE method, has shown remarkable performance in solving high-dimensional forward-backward stochastic differential equations (FBSDEs) and parabolic partial differential equations (PDEs). This article lays a theoretical foundation for the deep BSDE method in the general case of coupled FBSDEs. In particular, a posteriori error estimation of the solution is provided and it is proved that the error converges to zero given the universal approximation capability of neural networks. Numerical results are presented to demonstrate the accuracy of the analyzed algorithm in solving high-dimensional coupled FBSDEs.},
	number = {1},
	urldate = {2023-06-15},
	journal = {Probability, Uncertainty and Quantitative Risk},
	author = {Han, Jiequn and Long, Jihao},
	month = dec,
	year = {2020},
	note = {arXiv:1811.01165 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability},
	pages = {5},
}

@article{barron_universal_1993,
	title = {Universal approximation bounds for superpositions of a sigmoidal function},
	volume = {39},
	issn = {0018-9448, 1557-9654},
	url = {https://ieeexplore.ieee.org/document/256500/},
	doi = {10.1109/18.256500},
	language = {en},
	number = {3},
	urldate = {2023-05-18},
	journal = {IEEE Transactions on Information Theory},
	author = {Barron, A.R.},
	month = may,
	year = {1993},
	pages = {930--945},
}

@article{berner_analysis_2020,
	title = {Analysis of the {Generalization} {Error}: {Empirical} {Risk} {Minimization} over {Deep} {Artificial} {Neural} {Networks} {Overcomes} the {Curse} of {Dimensionality} in the {Numerical} {Approximation} of {Black}-{Scholes} {Partial} {Differential} {Equations}},
	volume = {2},
	issn = {2577-0187},
	shorttitle = {Analysis of the {Generalization} {Error}},
	url = {http://arxiv.org/abs/1809.03062},
	doi = {10.1137/19M125649X},
	abstract = {The development of new classification and regression algorithms based on empirical risk minimization (ERM) over deep neural network hypothesis classes, coined deep learning, revolutionized the area of artificial intelligence, machine learning, and data analysis. In particular, these methods have been applied to the numerical solution of high-dimensional partial differential equations with great success. Recent simulations indicate that deep learning-based algorithms are capable of overcoming the curse of dimensionality for the numerical solution of Kolmogorov equations, which are widely used in models from engineering, finance, and the natural sciences. The present paper considers under which conditions ERM over a deep neural network hypothesis class approximates the solution of a \$d\$-dimensional Kolmogorov equation with affine drift and diffusion coefficients and typical initial values arising from problems in computational finance up to error \${\textbackslash}varepsilon\$. We establish that, with high probability over draws of training samples, such an approximation can be achieved with both the size of the hypothesis class and the number of training samples scaling only polynomially in \$d\$ and \${\textbackslash}varepsilon{\textasciicircum}\{-1\}\$. It can be concluded that ERM over deep neural network hypothesis classes overcomes the curse of dimensionality for the numerical solution of linear Kolmogorov equations with affine coefficients.},
	number = {3},
	urldate = {2023-05-04},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Berner, Julius and Grohs, Philipp and Jentzen, Arnulf},
	month = jan,
	year = {2020},
	note = {arXiv:1809.03062 [cs, math, stat]},
	keywords = {60H30, 65C30, 62M45, 68T05, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	pages = {631--657},
}

@article{beck_overview_2023-1,
	title = {An overview on deep learning-based approximation methods for partial differential equations},
	volume = {28},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1531-3492},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/dcdsb.2022238},
	doi = {10.3934/dcdsb.2022238},
	abstract = {It is one of the most challenging problems in applied mathematics to approximatively solve high-dimensional partial differential equations (PDEs). Recently, several deep learning-based approximation algorithms for attacking this problem have been proposed and tested numerically on a number of examples of high-dimensional PDEs. This has given rise to a lively field of research in which deep learning-based methods and related Monte Carlo methods are applied to the approximation of high-dimensional PDEs. In this article we offer an introduction to this field of research by revisiting selected mathematical results related to deep learning approximation methods for PDEs and reviewing the main ideas of their proofs. We also provide a short overview of the recent literature in this area of research.},
	language = {en},
	number = {6},
	urldate = {2023-04-30},
	journal = {Discrete and Continuous Dynamical Systems - B},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno},
	month = jun,
	year = {2023},
	note = {Publisher: Discrete and Continuous Dynamical Systems - B},
	pages = {3697--3746},
}

@misc{noauthor_overcoming_nodate,
	title = {Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations {\textbar} {Proceedings} of the {Royal} {Society} {A}: {Mathematical}, {Physical} and {Engineering} {Sciences}},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2019.0630},
	urldate = {2023-04-30},
}

@article{hutzenthaler_proof_2020,
	title = {A proof that rectified deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations},
	volume = {1},
	issn = {2662-2971},
	url = {https://doi.org/10.1007/s42985-019-0006-9},
	doi = {10.1007/s42985-019-0006-9},
	abstract = {Deep neural networks and other deep learning methods have very successfully been applied to the numerical approximation of high-dimensional nonlinear parabolic partial differential equations (PDEs), which are widely used in finance, engineering, and natural sciences. In particular, simulations indicate that algorithms based on deep learning overcome the curse of dimensionality in the numerical approximation of solutions of semilinear PDEs. For certain linear PDEs it has also been proved mathematically that deep neural networks overcome the curse of dimensionality in the numerical approximation of solutions of such linear PDEs. The key contribution of this article is to rigorously prove this for the first time for a class of nonlinear PDEs. More precisely, we prove in the case of semilinear heat equations with gradient-independent nonlinearities that the numbers of parameters of the employed deep neural networks grow at most polynomially in both the PDE dimension and the reciprocal of the prescribed approximation accuracy. Our proof relies on recently introduced full history recursive multilevel Picard approximations for semilinear PDEs.},
	language = {en},
	number = {2},
	urldate = {2023-04-30},
	journal = {SN Partial Differential Equations and Applications},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Nguyen, Tuan Anh},
	month = apr,
	year = {2020},
	keywords = {65C99, 68T05, Curse of dimensionality, Deep neural networks, High-dimensional PDEs, Information based complexity, Multilevel Picard approximations, Tractability of multivariate problems},
	pages = {10},
}

@article{hutzenthaler_proof_2020-1,
	title = {A proof that rectified deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations},
	volume = {1},
	issn = {2662-2971},
	url = {https://doi.org/10.1007/s42985-019-0006-9},
	doi = {10.1007/s42985-019-0006-9},
	abstract = {Deep neural networks and other deep learning methods have very successfully been applied to the numerical approximation of high-dimensional nonlinear parabolic partial differential equations (PDEs), which are widely used in finance, engineering, and natural sciences. In particular, simulations indicate that algorithms based on deep learning overcome the curse of dimensionality in the numerical approximation of solutions of semilinear PDEs. For certain linear PDEs it has also been proved mathematically that deep neural networks overcome the curse of dimensionality in the numerical approximation of solutions of such linear PDEs. The key contribution of this article is to rigorously prove this for the first time for a class of nonlinear PDEs. More precisely, we prove in the case of semilinear heat equations with gradient-independent nonlinearities that the numbers of parameters of the employed deep neural networks grow at most polynomially in both the PDE dimension and the reciprocal of the prescribed approximation accuracy. Our proof relies on recently introduced full history recursive multilevel Picard approximations for semilinear PDEs.},
	language = {en},
	number = {2},
	urldate = {2022-12-19},
	journal = {SN Partial Differential Equations and Applications},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Nguyen, Tuan Anh},
	month = apr,
	year = {2020},
	keywords = {65C99, 68T05, Curse of dimensionality, Deep neural networks, High-dimensional PDEs, Information based complexity, Multilevel Picard approximations, Tractability of multivariate problems},
	pages = {10},
}

@article{grohs_deep_2022,
	title = {Deep neural network approximations for {Monte} {Carlo} algorithms},
	volume = {3},
	issn = {2662-2963, 2662-2971},
	url = {http://arxiv.org/abs/1908.10828},
	doi = {10.1007/s42985-021-00100-z},
	abstract = {Recently, it has been proposed in the literature to employ deep neural networks (DNNs) together with stochastic gradient descent methods to approximate solutions of PDEs. There are also a few results in the literature which prove that DNNs can approximate solutions of certain PDEs without the curse of dimensionality in the sense that the number of real parameters used to describe the DNN grows at most polynomially both in the PDE dimension and the reciprocal of the prescribed approximation accuracy. One key argument in most of these results is, first, to use a Monte Carlo approximation scheme which can approximate the solution of the PDE under consideration at a fixed space-time point without the curse of dimensionality and, thereafter, to prove that DNNs are flexible enough to mimic the behaviour of the used approximation scheme. Having this in mind, one could aim for a general abstract result which shows under suitable assumptions that if a certain function can be approximated by any kind of (Monte Carlo) approximation scheme without the curse of dimensionality, then this function can also be approximated with DNNs without the curse of dimensionality. It is a key contribution of this article to make a first step towards this direction. In particular, the main result of this paper, essentially, shows that if a function can be approximated by means of some suitable discrete approximation scheme without the curse of dimensionality and if there exist DNNs which satisfy certain regularity properties and which approximate this discrete approximation scheme without the curse of dimensionality, then the function itself can also be approximated with DNNs without the curse of dimensionality. As an application of this result we establish that solutions of suitable Kolmogorov PDEs can be approximated with DNNs without the curse of dimensionality.},
	number = {4},
	urldate = {2023-04-29},
	journal = {Partial Differential Equations and Applications},
	author = {Grohs, Philipp and Jentzen, Arnulf and Salimova, Diyora},
	month = aug,
	year = {2022},
	note = {arXiv:1908.10828 [cs, math]},
	keywords = {65L70, 68T99, 65C05, 60H30, Computer Science - Machine Learning, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Mathematics - Probability},
	pages = {45},
}

@misc{grohs_lower_2021,
	title = {Lower bounds for artificial neural network approximations: {A} proof that shallow neural networks fail to overcome the curse of dimensionality},
	shorttitle = {Lower bounds for artificial neural network approximations},
	url = {http://arxiv.org/abs/2103.04488},
	doi = {10.48550/arXiv.2103.04488},
	abstract = {Artificial neural networks (ANNs) have become a very powerful tool in the approximation of high-dimensional functions. Especially, deep ANNs, consisting of a large number of hidden layers, have been very successfully used in a series of practical relevant computational problems involving high-dimensional input data ranging from classification tasks in supervised learning to optimal decision problems in reinforcement learning. There are also a number of mathematical results in the scientific literature which study the approximation capacities of ANNs in the context of high-dimensional target functions. In particular, there are a series of mathematical results in the scientific literature which show that sufficiently deep ANNs have the capacity to overcome the curse of dimensionality in the approximation of certain target function classes in the sense that the number of parameters of the approximating ANNs grows at most polynomially in the dimension \$d {\textbackslash}in {\textbackslash}mathbb\{N\}\$ of the target functions under considerations. In the proofs of several of such high-dimensional approximation results it is crucial that the involved ANNs are sufficiently deep and consist a sufficiently large number of hidden layers which grows in the dimension of the considered target functions. It is the topic of this work to look a bit more detailed to the deepness of the involved ANNs in the approximation of high-dimensional target functions. In particular, the main result of this work proves that there exists a concretely specified sequence of functions which can be approximated without the curse of dimensionality by sufficiently deep ANNs but which cannot be approximated without the curse of dimensionality if the involved ANNs are shallow or not deep enough.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Grohs, Philipp and Ibragimov, Shokhrukh and Jentzen, Arnulf and Koppensteiner, Sarah},
	month = mar,
	year = {2021},
	note = {arXiv:2103.04488 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@misc{hornung_space-time_2020,
	title = {Space-time deep neural network approximations for high-dimensional partial differential equations},
	url = {http://arxiv.org/abs/2006.02199},
	doi = {10.48550/arXiv.2006.02199},
	abstract = {It is one of the most challenging issues in applied mathematics to approximately solve high-dimensional partial differential equations (PDEs) and most of the numerical approximation methods for PDEs in the scientific literature suffer from the so-called curse of dimensionality in the sense that the number of computational operations employed in the corresponding approximation scheme to obtain an approximation precision \${\textbackslash}varepsilon{\textgreater}0\$ grows exponentially in the PDE dimension and/or the reciprocal of \${\textbackslash}varepsilon\$. Recently, certain deep learning based approximation methods for PDEs have been proposed and various numerical simulations for such methods suggest that deep neural network (DNN) approximations might have the capacity to indeed overcome the curse of dimensionality in the sense that the number of real parameters used to describe the approximating DNNs grows at most polynomially in both the PDE dimension \$d{\textbackslash}in{\textbackslash}mathbb\{N\}\$ and the reciprocal of the prescribed accuracy \${\textbackslash}varepsilon{\textgreater}0\$. There are now also a few rigorous results in the scientific literature which substantiate this conjecture by proving that DNNs overcome the curse of dimensionality in approximating solutions of PDEs. Each of these results establishes that DNNs overcome the curse of dimensionality in approximating suitable PDE solutions at a fixed time point \$T{\textgreater}0\$ and on a compact cube \$[a,b]{\textasciicircum}d\$ in space but none of these results provides an answer to the question whether the entire PDE solution on \$[0,T]{\textbackslash}times [a,b]{\textasciicircum}d\$ can be approximated by DNNs without the curse of dimensionality. It is precisely the subject of this article to overcome this issue. More specifically, the main result of this work in particular proves for every \$a{\textbackslash}in{\textbackslash}mathbb\{R\}\$, \$ b{\textbackslash}in (a,{\textbackslash}infty)\$ that solutions of certain Kolmogorov PDEs can be approximated by DNNs on the space-time region \$[0,T]{\textbackslash}times [a,b]{\textasciicircum}d\$ without the curse of dimensionality.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Hornung, Fabian and Jentzen, Arnulf and Salimova, Diyora},
	month = jun,
	year = {2020},
	note = {arXiv:2006.02199 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Mathematics - Probability},
}

@misc{hutzenthaler_strong_2021,
	title = {Strong \${L}{\textasciicircum}p\$-error analysis of nonlinear {Monte} {Carlo} approximations for high-dimensional semilinear partial differential equations},
	url = {http://arxiv.org/abs/2110.08297},
	doi = {10.48550/arXiv.2110.08297},
	abstract = {Full-history recursive multilevel Picard (MLP) approximation schemes have been shown to overcome the curse of dimensionality in the numerical approximation of high-dimensional semilinear partial differential equations (PDEs) with general time horizons and Lipschitz continuous nonlinearities. However, each of the error analyses for MLP approximation schemes in the existing literature studies the \$L{\textasciicircum}2\$-root-mean-square distance between the exact solution of the PDE under consideration and the considered MLP approximation and none of the error analyses in the existing literature provides an upper bound for the more general \$L{\textasciicircum}p\$-distance between the exact solution of the PDE under consideration and the considered MLP approximation. It is the key contribution of this article to extend the \$L{\textasciicircum}2\$-error analysis for MLP approximation schemes in the literature to a more general \$L{\textasciicircum}p\$-error analysis with \$p{\textbackslash}in (0,{\textbackslash}infty)\$. In particular, the main result of this article proves that the proposed MLP approximation scheme indeed overcomes the curse of dimensionality in the numerical approximation of high-dimensional semilinear PDEs with the approximation error measured in the \$L{\textasciicircum}p\$-sense with \$p {\textbackslash}in (0,{\textbackslash}infty)\$.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kuckuck, Benno and Padgett, Joshua Lee},
	month = oct,
	year = {2021},
	note = {arXiv:2110.08297 [cs, math]},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Probability},
}

@misc{hutzenthaler_speed_2022,
	title = {On the speed of convergence of {Picard} iterations of backward stochastic differential equations},
	url = {http://arxiv.org/abs/2107.01840},
	doi = {10.48550/arXiv.2107.01840},
	abstract = {It is a well-established fact in the scientific literature that Picard iterations of backward stochastic differential equations with globally Lipschitz continuous nonlinearity converge at least exponentially fast to the solution. In this paper we prove that this convergence is in fact at least square-root factorially fast. We show for one example that no higher convergence speed is possible in general. Moreover, if the nonlinearity is \$z\$-independent, then the convergence is even factorially fast. Thus we reveal a phase transition in the speed of convergence of Picard iterations of backward stochastic differential equations.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Hutzenthaler, Martin and Kruse, Thomas and Nguyen, Tuan Anh},
	month = apr,
	year = {2022},
	note = {arXiv:2107.01840 [math]},
	keywords = {65C99, 60H99, 60G99, Mathematics - Probability},
}

@article{e_multilevel_2019,
	title = {On multilevel {Picard} numerical approximations for high-dimensional nonlinear parabolic partial differential equations and high-dimensional nonlinear backward stochastic differential equations},
	volume = {79},
	issn = {0885-7474, 1573-7691},
	url = {http://arxiv.org/abs/1708.03223},
	doi = {10.1007/s10915-018-00903-0},
	abstract = {Parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) are key ingredients in a number of models in physics and financial engineering. In particular, parabolic PDEs and BSDEs are fundamental tools in the state-of-the-art pricing and hedging of financial derivatives. The PDEs and BSDEs appearing in such applications are often high-dimensional and nonlinear. Since explicit solutions of such PDEs and BSDEs are typically not available, it is a very active topic of research to solve such PDEs and BSDEs approximately. In the recent article [E, W., Hutzenthaler, M., Jentzen, A., and Kruse, T. Linear scaling algorithms for solving high-dimensional nonlinear parabolic differential equations. arXiv:1607.03295 (2017)] we proposed a family of approximation methods based on Picard approximations and multilevel Monte Carlo methods and showed under suitable regularity assumptions on the exact solution for semilinear heat equations that the computational complexity is bounded by \$O( d {\textbackslash}, {\textbackslash}epsilon{\textasciicircum}\{-(4+{\textbackslash}delta)\})\$ for any \${\textbackslash}delta{\textbackslash}in(0,{\textbackslash}infty)\$, where \$d\$ is the dimensionality of the problem and \${\textbackslash}epsilon{\textbackslash}in(0,{\textbackslash}infty)\$ is the prescribed accuracy. In this paper, we test the applicability of this algorithm on a variety of \$100\$-dimensional nonlinear PDEs that arise in physics and finance by means of numerical simulations presenting approximation accuracy against runtime. The simulation results for these 100-dimensional example PDEs are very satisfactory in terms of accuracy and speed. In addition, we also provide a review of other approximation methods for nonlinear PDEs and BSDEs from the literature.},
	number = {3},
	urldate = {2023-04-10},
	journal = {Journal of Scientific Computing},
	author = {E, Weinan and Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas},
	month = jun,
	year = {2019},
	note = {arXiv:1708.03223 [math]},
	keywords = {Mathematics - Numerical Analysis},
	pages = {1534--1571},
}

@article{pardoux_forward-backward_1999,
	title = {Forward-backward stochastic differential equations and quasilinear parabolic {PDEs}},
	volume = {114},
	issn = {1432-2064},
	url = {https://doi.org/10.1007/s004409970001},
	doi = {10.1007/s004409970001},
	abstract = {This paper studies, under some natural monotonicity conditions, the theory (existence and uniqueness, a priori estimate, continuous dependence on a parameter) of forward–backward stochastic differential equations and their connection with quasilinear parabolic partial differential equations. We use a purely probabilistic approach, and allow the forward equation to be degenerate.},
	language = {en},
	number = {2},
	urldate = {2023-04-04},
	journal = {Probability Theory and Related Fields},
	author = {Pardoux, Etienne and Tang, Shanjian},
	month = may,
	year = {1999},
	keywords = {Mathematics Subject Classification (1991): Primary 60H10, 60G44; Secondary 35K55},
	pages = {123--150},
}

@article{el_karoui_backward_1997,
	title = {Backward {Stochastic} {Differential} {Equations} in {Finance}},
	volume = {7},
	issn = {1467-9965},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9965.00022},
	doi = {10.1111/1467-9965.00022},
	abstract = {We are concerned with different properties of backward stochastic differential equations and their applications to finance. These equations, first introduced by Pardoux and Peng (1990), are useful for the theory of contingent claim valuation, especially cases with constraints and for the theory of recursive utilities, introduced by Duffie and Epstein (1992a, 1992b).},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Mathematical Finance},
	author = {El Karoui, N. and Peng, S. and Quenez, M. C.},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9965.00022},
	keywords = {Malliavin derivative, backward stochastic equation, constrained portfolio, hedging portfolios, incomplete market, mathematical finance, pricing, recursive utility, stochastic control, viscosity solution of PDE},
	pages = {1--71},
}

@article{beck_nonlinear_2021,
	title = {On nonlinear {Feynman}-{Kac} formulas for viscosity solutions of semilinear parabolic partial differential equations},
	volume = {21},
	issn = {0219-4937, 1793-6799},
	url = {http://arxiv.org/abs/2004.03389},
	doi = {10.1142/S0219493721500489},
	abstract = {The classical Feynman-Kac identity builds a bridge between stochastic analysis and partial differential equations (PDEs) by providing stochastic representations for classical solutions of linear Kolmogorov PDEs. This opens the door for the derivation of sampling based Monte Carlo approximation methods, which can be meshfree and thereby stand a chance to approximate solutions of PDEs without suffering from the curse of dimensionality. In this article we extend the classical Feynman-Kac formula to certain semilinear Kolmogorov PDEs. More specifically, we identify suitable solutions of stochastic fixed point equations (SFPEs), which arise when the classical Feynman-Kac identity is formally applied to semilinear Kolmorogov PDEs, as viscosity solutions of the corresponding PDEs. This justifies, in particular, employing full-history recursive multilevel Picard (MLP) approximation algorithms, which have recently been shown to overcome the curse of dimensionality in the numerical approximation of solutions of SFPEs, in the numerical approximation of semilinear Kolmogorov PDEs.},
	number = {08},
	urldate = {2023-03-23},
	journal = {Stochastics and Dynamics},
	author = {Beck, Christian and Hutzenthaler, Martin and Jentzen, Arnulf},
	month = dec,
	year = {2021},
	note = {arXiv:2004.03389 [math]},
	keywords = {Mathematics - Analysis of PDEs, Mathematics - Probability},
	pages = {2150048},
}

@article{beck_existence_2021,
	title = {On existence and uniqueness properties for solutions of stochastic fixed point equations},
	volume = {26},
	issn = {1553-524X},
	url = {http://arxiv.org/abs/1908.03382},
	doi = {10.3934/dcdsb.2020320},
	abstract = {The Feynman-Kac formula implies that every suitable classical solution of a semilinear Kolmogorov partial differential equation (PDE) is also a solution of a certain stochastic fixed point equation (SFPE). In this article we study such and related SFPEs. In particular, the main result of this work proves existence of unique solutions of certain SFPEs in a general setting. As an application of this main result we establish the existence of unique solutions of SFPEs associated with semilinear Kolmogorov PDEs with Lipschitz continuous nonlinearities even in the case where the associated semilinear Kolmogorov PDE does not possess a classical solution.},
	number = {9},
	urldate = {2023-03-23},
	journal = {Discrete \& Continuous Dynamical Systems - B},
	author = {Beck, Christian and Gonon, Lukas and Hutzenthaler, Martin and Jentzen, Arnulf},
	year = {2021},
	note = {arXiv:1908.03382 [math]},
	keywords = {60, Mathematics - Functional Analysis, Mathematics - Probability},
	pages = {4927},
}

@article{beck_solving_2021,
	title = {Solving the {Kolmogorov} {PDE} by means of deep learning},
	volume = {88},
	issn = {0885-7474, 1573-7691},
	url = {http://arxiv.org/abs/1806.00421},
	doi = {10.1007/s10915-021-01590-0},
	abstract = {Stochastic differential equations (SDEs) and the Kolmogorov partial differential equations (PDEs) associated to them have been widely used in models from engineering, finance, and the natural sciences. In particular, SDEs and Kolmogorov PDEs, respectively, are highly employed in models for the approximative pricing of financial derivatives. Kolmogorov PDEs and SDEs, respectively, can typically not be solved explicitly and it has been and still is an active topic of research to design and analyze numerical methods which are able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly all approximation methods for Kolmogorov PDEs in the literature suffer under the curse of dimensionality or only provide approximations of the solution of the PDE at a single fixed space-time point. In this paper we derive and propose a numerical approximation method which aims to overcome both of the above mentioned drawbacks and intends to deliver a numerical approximation of the Kolmogorov PDE on an entire region \$[a,b]{\textasciicircum}d\$ without suffering from the curse of dimensionality. Numerical results on examples including the heat equation, the Black-Scholes model, the stochastic Lorenz equation, and the Heston model suggest that the proposed approximation algorithm is quite effective in high dimensions in terms of both accuracy and speed.},
	number = {3},
	urldate = {2023-03-18},
	journal = {Journal of Scientific Computing},
	author = {Beck, Christian and Becker, Sebastian and Grohs, Philipp and Jaafari, Nor and Jentzen, Arnulf},
	month = sep,
	year = {2021},
	note = {arXiv:1806.00421 [cs, math, stat]},
	keywords = {65C99, 65M99, 60H30, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Probability, Statistics - Machine Learning},
	pages = {73},
}

@article{han_solving_2020,
	title = {Solving high-dimensional eigenvalue problems using deep neural networks: {A} diffusion {Monte} {Carlo} like approach},
	volume = {423},
	issn = {0021-9991},
	shorttitle = {Solving high-dimensional eigenvalue problems using deep neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120305660},
	doi = {10.1016/j.jcp.2020.109792},
	abstract = {We propose a new method to solve eigenvalue problems for linear and semilinear second order differential operators in high dimensions based on deep neural networks. The eigenvalue problem is reformulated as a fixed point problem of the semigroup flow induced by the operator, whose solution can be represented by Feynman-Kac formula in terms of forward-backward stochastic differential equations. The method shares a similar spirit with diffusion Monte Carlo but augments a direct approximation to the eigenfunction through neural-network ansatz. The criterion of fixed point provides a natural loss function to search for parameters via optimization. Our approach is able to provide accurate eigenvalue and eigenfunction approximations in several numerical examples, including Fokker-Planck operator and the linear and nonlinear Schrödinger operators in high dimensions.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Journal of Computational Physics},
	author = {Han, Jiequn and Lu, Jianfeng and Zhou, Mo},
	month = dec,
	year = {2020},
	keywords = {Deep neural networks, Diffusion Monte Carlo, Eigenvalue problem, Schrödinger equation},
	pages = {109792},
}

@misc{kutyniok_theoretical_2020,
	title = {A {Theoretical} {Analysis} of {Deep} {Neural} {Networks} and {Parametric} {PDEs}},
	url = {http://arxiv.org/abs/1904.00377},
	doi = {10.48550/arXiv.1904.00377},
	abstract = {We derive upper bounds on the complexity of ReLU neural networks approximating the solution maps of parametric partial differential equations. In particular, without any knowledge of its concrete shape, we use the inherent low-dimensionality of the solution manifold to obtain approximation rates which are significantly superior to those provided by classical neural network approximation results. Concretely, we use the existence of a small reduced basis to construct, for a large variety of parametric partial differential equations, neural networks that yield approximations of the parametric solution maps in such a way that the sizes of these networks essentially only depend on the size of the reduced basis.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Kutyniok, Gitta and Petersen, Philipp and Raslan, Mones and Schneider, Reinhold},
	month = may,
	year = {2020},
	note = {arXiv:1904.00377 [cs, math, stat]},
	keywords = {35A35, 35J99, 41A25, 41A46, 68T05, 65N30, Computer Science - Machine Learning, Mathematics - Functional Analysis, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{jentzen_proof_2021,
	title = {A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of {Kolmogorov} partial differential equations with constant diffusion and nonlinear drift coefficients},
	volume = {19},
	issn = {15396746, 19450796},
	url = {http://arxiv.org/abs/1809.07321},
	doi = {10.4310/CMS.2021.v19.n5.a1},
	abstract = {In recent years deep artificial neural networks (DNNs) have been successfully employed in numerical simulations for a multitude of computational problems including, for example, object and face recognition, natural language processing, fraud detection, computational advertisement, and numerical approximations of partial differential equations (PDEs). These numerical simulations indicate that DNNs seem to possess the fundamental flexibility to overcome the curse of dimensionality in the sense that the number of real parameters used to describe the DNN grows at most polynomially in both the reciprocal of the prescribed approximation accuracy \$ {\textbackslash}varepsilon {\textgreater} 0 \$ and the dimension \$ d {\textbackslash}in {\textbackslash}mathbb\{N\}\$ of the function which the DNN aims to approximate in such computational problems. There is also a large number of rigorous mathematical approximation results for artificial neural networks in the scientific literature but there are only a few special situations where results in the literature can rigorously justify the success of DNNs in high-dimensional function approximation. The key contribution of this paper is to reveal that DNNs do overcome the curse of dimensionality in the numerical approximation of Kolmogorov PDEs with constant diffusion and nonlinear drift coefficients. We prove that the number of parameters used to describe the employed DNN grows at most polynomially in both the PDE dimension \$ d {\textbackslash}in {\textbackslash}mathbb\{N\}\$ and the reciprocal of the prescribed approximation accuracy \$ {\textbackslash}varepsilon {\textgreater} 0 \$. A crucial ingredient in our proof is the fact that the artificial neural network used to approximate the solution of the PDE is indeed a deep artificial neural network with a large number of hidden layers.},
	number = {5},
	urldate = {2023-02-27},
	journal = {Communications in Mathematical Sciences},
	author = {Jentzen, Arnulf and Salimova, Diyora and Welti, Timo},
	year = {2021},
	note = {arXiv:1809.07321 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Mathematics - Probability},
	pages = {1167--1205},
}

@incollection{berner_modern_2022,
	title = {The {Modern} {Mathematics} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/2105.04026},
	abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
	urldate = {2023-02-22},
	author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
	month = dec,
	year = {2022},
	doi = {10.1017/9781009025096.002},
	note = {arXiv:2105.04026 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--111},
}

@misc{e_towards_2020,
	title = {Towards a {Mathematical} {Understanding} of {Neural} {Network}-{Based} {Machine} {Learning}: what we know and what we don't},
	shorttitle = {Towards a {Mathematical} {Understanding} of {Neural} {Network}-{Based} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2009.10713},
	doi = {10.48550/arXiv.2009.10713},
	abstract = {The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei},
	month = dec,
	year = {2020},
	note = {arXiv:2009.10713 [cs, math, stat]},
	keywords = {68T07 (primary), 26B40, 41A30, 35Q68, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{jentzen_convergence_2023,
	title = {Convergence analysis for gradient flows in the training of artificial neural networks with {ReLU} activation},
	volume = {517},
	issn = {0022247X},
	url = {http://arxiv.org/abs/2107.04479},
	doi = {10.1016/j.jmaa.2022.126601},
	abstract = {Gradient descent (GD) type optimization schemes are the standard methods to train artificial neural networks (ANNs) with rectified linear unit (ReLU) activation. Such schemes can be considered as discretizations of gradient flows (GFs) associated to the training of ANNs with ReLU activation and most of the key difficulties in the mathematical convergence analysis of GD type optimization schemes in the training of ANNs with ReLU activation seem to be already present in the dynamics of the corresponding GF differential equations. It is the key subject of this work to analyze such GF differential equations in the training of ANNs with ReLU activation and three layers (one input layer, one hidden layer, and one output layer). In particular, in this article we prove in the case where the target function is possibly multi-dimensional and continuous and in the case where the probability distribution of the input data is absolutely continuous with respect to the Lebesgue measure that the risk of every bounded GF trajectory converges to the risk of a critical point. In addition, in this article we show in the case of a 1-dimensional affine linear target function and in the case where the probability distribution of the input data coincides with the standard uniform distribution that the risk of every bounded GF trajectory converges to zero if the initial risk is sufficiently small. Finally, in the special situation where there is only one neuron on the hidden layer (1-dimensional hidden layer) we strengthen the above named result for affine linear target functions by proving that that the risk of every (not necessarily bounded) GF trajectory converges to zero if the initial risk is sufficiently small.},
	number = {2},
	urldate = {2023-02-14},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Jentzen, Arnulf and Riekert, Adrian},
	month = jan,
	year = {2023},
	note = {arXiv:2107.04479 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
	pages = {126601},
}

@article{yan_global_1994,
	title = {Global analysis of {Oja}'s flow for neural networks},
	volume = {5},
	issn = {1941-0093},
	doi = {10.1109/72.317720},
	abstract = {A detailed study of Oja's learning equation in neural networks is undertaken in this paper. Not only are such fundamental issues as existence, uniqueness, and representation of solutions completely resolved, but also the convergence issue is resolved. It is shown that the solution of Oja's equation is exponentially convergent to an equilibrium from any initial value. Moreover, the necessary and sufficient conditions are given on the initial value for the solution to converge to a dominant eigenspace of the associated autocorrelation matrix. As a by-product, this result confirms one of Oja's conjectures that the solution converges to the principal eigenspace from almost all initial values. Some other characteristics of the limiting solution are also revealed. These facilitate the determination of the limiting solution in advance using only the initial information. Two examples are analyzed demonstrating the explicit dependence of the limiting solution on the initial value. In another respect, it is found that Oja's equation is the gradient flow of generalized Rayleigh quotients on a Stiefel manifold.{\textless}{\textgreater}},
	number = {5},
	journal = {IEEE Transactions on Neural Networks},
	author = {Yan, Wei-Yong and Helmke, U. and Moore, J.B.},
	month = sep,
	year = {1994},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Artificial neural networks, Associative memory, Autocorrelation, Difference equations, Eigenvalues and eigenfunctions, Feature extraction, Neural networks, Neurofeedback, Sufficient conditions, Systems engineering and theory},
	pages = {674--683},
}

@misc{sirignano_mean_2019,
	title = {Mean {Field} {Analysis} of {Neural} {Networks}: {A} {Law} of {Large} {Numbers}},
	shorttitle = {Mean {Field} {Analysis} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1805.01053},
	doi = {10.48550/arXiv.1805.01053},
	abstract = {Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis, and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (A) large network sizes and (B) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called "propagation of chaos".},
	urldate = {2023-02-14},
	publisher = {arXiv},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = nov,
	year = {2019},
	note = {arXiv:1805.01053 [math]},
	keywords = {Mathematics - Probability},
}

@incollection{avila_continuous_2022,
	address = {Cham},
	title = {The {Continuous} {Formulation} of {Shallow} {Neural} {Networks} as {Wasserstein}-{Type} {Gradient} {Flows}},
	isbn = {978-3-031-05330-6 978-3-031-05331-3},
	url = {https://link.springer.com/10.1007/978-3-031-05331-3_3},
	abstract = {It has been recently observed that the training of a single hidden layer artiﬁcial neural network can be reinterpreted as a Wasserstein gradient ﬂow for the weights for the error functional. In the limit, as the number of parameters tends to inﬁnity, this gives rise to a family of parabolic equations. This survey aims to discuss this relation, focusing on the associated theoretical aspects appealing to the mathematical community and providing a list of interesting open problems.},
	language = {en},
	urldate = {2023-02-14},
	booktitle = {Analysis at {Large}},
	publisher = {Springer International Publishing},
	author = {Fernández-Real, Xavier and Figalli, Alessio},
	editor = {Avila, Artur and Rassias, Michael Th. and Sinai, Yakov},
	year = {2022},
	doi = {10.1007/978-3-031-05331-3_3},
	pages = {29--57},
}

@article{pfau_ab_2020,
	title = {Ab initio solution of the many-electron {Schrödinger} equation with deep neural networks},
	volume = {2},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.033429},
	doi = {10.1103/PhysRevResearch.2.033429},
	abstract = {Given access to accurate solutions of the many-electron Schrödinger equation, nearly all chemistry could be derived from first principles. Exact wave functions of interesting chemical systems are out of reach because they are NP-hard to compute in general, but approximations can be found using polynomially scaling algorithms. The key challenge for many of these algorithms is the choice of wave function approximation, or Ansatz, which must trade off between efficiency and accuracy. Neural networks have shown impressive power as accurate practical function approximators and promise as a compact wave-function Ansatz for spin systems, but problems in electronic structure require wave functions that obey Fermi-Dirac statistics. Here we introduce a novel deep learning architecture, the Fermionic neural network, as a powerful wave-function Ansatz for many-electron systems. The Fermionic neural network is able to achieve accuracy beyond other variational quantum Monte Carlo Ansatz on a variety of atoms and small molecules. Using no data other than atomic positions and charges, we predict the dissociation curves of the nitrogen molecule and hydrogen chain, two challenging strongly correlated systems, to significantly higher accuracy than the coupled cluster method, widely considered the most accurate scalable method for quantum chemistry at equilibrium geometry. This demonstrates that deep neural networks can improve the accuracy of variational quantum Monte Carlo to the point where it outperforms other ab initio quantum chemistry methods, opening the possibility of accurate direct optimization of wave functions for previously intractable many-electron systems.},
	number = {3},
	urldate = {2022-10-08},
	journal = {Physical Review Research},
	author = {Pfau, David and Spencer, James S. and Matthews, Alexander G. D. G. and Foulkes, W. M. C.},
	month = sep,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {033429},
}

@article{mei_mean_2018,
	title = {A {Mean} {Field} {View} of the {Landscape} of {Two}-{Layers} {Neural} {Networks}},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1804.06561},
	doi = {10.1073/pnas.1806579115},
	abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
	number = {33},
	urldate = {2023-01-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	month = aug,
	year = {2018},
	note = {arXiv:1804.06561 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{e_machine_2020,
	title = {Machine {Learning} from a {Continuous} {Viewpoint}},
	volume = {63},
	issn = {1674-7283, 1869-1862},
	url = {http://arxiv.org/abs/1912.12777},
	doi = {10.1007/s11425-020-1773-8},
	abstract = {We present a continuous formulation of machine learning, as a problem in the calculus of variations and differential-integral equations, in the spirit of classical numerical analysis. We demonstrate that conventional machine learning models and algorithms, such as the random feature model, the two-layer neural network model and the residual neural network model, can all be recovered (in a scaled form) as particular discretizations of different continuous formulations. We also present examples of new models, such as the flow-based random feature model, and new algorithms, such as the smoothed particle method and spectral method, that arise naturally from this continuous formulation. We discuss how the issues of generalization error and implicit regularization can be studied under this framework.},
	number = {11},
	urldate = {2023-01-26},
	journal = {Science China Mathematics},
	author = {E, Weinan and Ma, Chao and Wu, Lei},
	month = nov,
	year = {2020},
	note = {arXiv:1912.12777 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
	pages = {2233--2266},
}

@misc{e_towards_2020-1,
	title = {Towards a {Mathematical} {Understanding} of {Neural} {Network}-{Based} {Machine} {Learning}: what we know and what we don't},
	shorttitle = {Towards a {Mathematical} {Understanding} of {Neural} {Network}-{Based} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2009.10713},
	doi = {10.48550/arXiv.2009.10713},
	abstract = {The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.},
	urldate = {2023-01-26},
	publisher = {arXiv},
	author = {E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei},
	month = dec,
	year = {2020},
	note = {arXiv:2009.10713 [cs, math, stat]},
	keywords = {68T07 (primary), 26B40, 41A30, 35Q68, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@misc{gamarnik_stationary_2020,
	title = {Stationary {Points} of {Shallow} {Neural} {Networks} with {Quadratic} {Activation} {Function}},
	url = {http://arxiv.org/abs/1912.01599},
	doi = {10.48550/arXiv.1912.01599},
	abstract = {We consider the teacher-student setting of learning shallow neural networks with quadratic activations and planted weight matrix \$W{\textasciicircum}*{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{m{\textbackslash}times d\}\$, where \$m\$ is the width of the hidden layer and \$d{\textbackslash}le m\$ is the data dimension. We study the optimization landscape associated with the empirical and the population squared risk of the problem. Under the assumption the planted weights are full-rank we obtain the following results. First, we establish that the landscape of the empirical risk admits an "energy barrier" separating rank-deficient \$W\$ from \$W{\textasciicircum}*\$: if \$W\$ is rank deficient, then its risk is bounded away from zero by an amount we quantify. We then couple this result by showing that, assuming number \$N\$ of samples grows at least like a polynomial function of \$d\$, all full-rank approximate stationary points of the empirical risk are nearly global optimum. These two results allow us to prove that gradient descent, when initialized below the energy barrier, approximately minimizes the empirical risk and recovers the planted weights in polynomial-time. Next, we show that initializing below this barrier is in fact easily achieved when the weights are randomly generated under relatively weak assumptions. We show that provided the network is sufficiently overparametrized, initializing with an appropriate multiple of the identity suffices to obtain a risk below the energy barrier. At a technical level, the last result is a consequence of the semicircle law for the Wishart ensemble and could be of independent interest. Finally, we study the minimizers of the empirical risk and identify a simple necessary and sufficient geometric condition on the training data under which any minimizer has necessarily zero generalization error. We show that as soon as \$N{\textbackslash}ge N{\textasciicircum}*=d(d+1)/2\$, randomly generated data enjoys this geometric condition almost surely, while that ceases to be true if \$N{\textless}N{\textasciicircum}*\$.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Gamarnik, David and Kızıldağ, Eren C. and Zadik, Ilias},
	month = jul,
	year = {2020},
	note = {arXiv:1912.01599 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{mei_mean_2018-1,
	title = {A {Mean} {Field} {View} of the {Landscape} of {Two}-{Layers} {Neural} {Networks}},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1804.06561},
	doi = {10.1073/pnas.1806579115},
	abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
	number = {33},
	urldate = {2023-01-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	month = aug,
	year = {2018},
	note = {arXiv:1804.06561 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{e_machine_2020-1,
	title = {Machine {Learning} and {Computational} {Mathematics}},
	volume = {28},
	issn = {1815-2406, 1991-7120},
	url = {http://arxiv.org/abs/2009.14596},
	doi = {10.4208/cicp.OA-2020-0185},
	abstract = {Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of "black box" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, \{can\} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.},
	number = {5},
	urldate = {2023-01-24},
	journal = {Communications in Computational Physics},
	author = {E, Weinan},
	month = jun,
	year = {2020},
	note = {arXiv:2009.14596 [cs, math, stat]},
	keywords = {68T07, 46E15, 26B35, 26B40, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	pages = {1639--1670},
}

@article{yuan_theory_2019,
	title = {Theory of variational quantum simulation},
	volume = {3},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/1812.08767},
	doi = {10.22331/q-2019-10-07-191},
	abstract = {The variational method is a versatile tool for classical simulation of a variety of quantum systems. Great efforts have recently been devoted to its extension to quantum computing for efficiently solving static many-body problems and simulating real and imaginary time dynamics. In this work, we first review the conventional variational principles, including the Rayleigh-Ritz method for solving static problems, and the Dirac and Frenkel variational principle, the McLachlan's variational principle, and the time-dependent variational principle, for simulating real time dynamics. We focus on the simulation of dynamics and discuss the connections of the three variational principles. Previous works mainly focus on the unitary evolution of pure states. In this work, we introduce variational quantum simulation of mixed states under general stochastic evolution. We show how the results can be reduced to the pure state case with a correction term that takes accounts of global phase alignment. For variational simulation of imaginary time evolution, we also extend it to the mixed state scenario and discuss variational Gibbs state preparation. We further elaborate on the design of ansatz that is compatible with post-selection measurement and the implementation of the generalised variational algorithms with quantum circuits. Our work completes the theory of variational quantum simulation of general real and imaginary time evolution and it is applicable to near-term quantum hardware.},
	urldate = {2023-01-20},
	journal = {Quantum},
	author = {Yuan, Xiao and Endo, Suguru and Zhao, Qi and Li, Ying and Benjamin, Simon},
	month = oct,
	year = {2019},
	note = {arXiv:1812.08767 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {191},
}

@article{hofmann_role_2022,
	title = {Role of stochastic noise and generalization error in the time propagation of neural-network quantum states},
	volume = {12},
	issn = {2542-4653},
	url = {http://arxiv.org/abs/2105.01054},
	doi = {10.21468/SciPostPhys.12.5.165},
	abstract = {Neural-network quantum states (NQS) have been shown to be a suitable variational ansatz to simulate out-of-equilibrium dynamics in two-dimensional systems using time-dependent variational Monte Carlo (t-VMC). In particular, stable and accurate time propagation over long time scales has been observed in the square-lattice Heisenberg model using the Restricted Boltzmann machine architecture. However, achieving similar performance in other systems has proven to be more challenging. In this article, we focus on the two-leg Heisenberg ladder driven out of equilibrium by a pulsed excitation as a benchmark system. We demonstrate that unmitigated noise is strongly amplified by the nonlinear equations of motion for the network parameters, which causes numerical instabilities in the time evolution. As a consequence, the achievable accuracy of the simulated dynamics is a result of the interplay between network expressiveness and measures required to remedy these instabilities. We show that stability can be greatly improved by appropriate choice of regularization. This is particularly useful as tuning of the regularization typically imposes no additional computational cost. Inspired by machine learning practice, we propose a validation-set based diagnostic tool to help determining optimal regularization hyperparameters for t-VMC based propagation schemes. For our benchmark, we show that stable and accurate time propagation can be achieved in regimes of sufficiently regularized variational dynamics.},
	number = {5},
	urldate = {2023-01-20},
	journal = {SciPost Physics},
	author = {Hofmann, Damian and Fabiani, Giammarco and Mentink, Johan H. and Carleo, Giuseppe and Sentef, Michael A.},
	month = may,
	year = {2022},
	note = {arXiv:2105.01054 [cond-mat, physics:physics, physics:quant-ph]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics, Quantum Physics},
	pages = {165},
}

@article{levine_quantum_2019,
	title = {Quantum {Entanglement} in {Deep} {Learning} {Architectures}},
	volume = {122},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1803.09780},
	doi = {10.1103/PhysRevLett.122.065301},
	abstract = {Modern deep learning has enabled unprecedented achievements in various domains. Nonetheless, employment of machine learning for wave function representations is focused on more traditional architectures such as restricted Boltzmann machines (RBMs) and fully-connected neural networks. In this letter, we establish that contemporary deep learning architectures, in the form of deep convolutional and recurrent networks, can efficiently represent highly entangled quantum systems. By constructing Tensor Network equivalents of these architectures, we identify an inherent reuse of information in the network operation as a key trait which distinguishes them from standard Tensor Network based representations, and which enhances their entanglement capacity. Our results show that such architectures can support volume-law entanglement scaling, polynomially more efficiently than presently employed RBMs. Thus, beyond a quantification of the entanglement capacity of leading deep learning architectures, our analysis formally motivates a shift of trending neural-network based wave function representations closer to the state-of-the-art in machine learning.},
	number = {6},
	urldate = {2023-01-20},
	journal = {Physical Review Letters},
	author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon},
	month = feb,
	year = {2019},
	note = {arXiv:1803.09780 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	pages = {065301},
}

@article{sharir_neural_2022,
	title = {Neural tensor contractions and the expressive power of deep neural quantum states},
	volume = {106},
	issn = {2469-9950, 2469-9969},
	url = {http://arxiv.org/abs/2103.10293},
	doi = {10.1103/PhysRevB.106.205136},
	abstract = {We establish a direct connection between general tensor networks and deep feed-forward artificial neural networks. The core of our results is the construction of neural-network layers that efficiently perform tensor contractions, and that use commonly adopted non-linear activation functions. The resulting deep networks feature a number of edges that closely matches the contraction complexity of the tensor networks to be approximated. In the context of many-body quantum states, this result establishes that neural-network states have strictly the same or higher expressive power than practically usable variational tensor networks. As an example, we show that all matrix product states can be efficiently written as neural-network states with a number of edges polynomial in the bond dimension and depth logarithmic in the system size. The opposite instead does not hold true, and our results imply that there exist quantum states that are not efficiently expressible in terms of matrix product states or PEPS, but that are instead efficiently expressible with neural network states.},
	number = {20},
	urldate = {2023-01-20},
	journal = {Physical Review B},
	author = {Sharir, Or and Shashua, Amnon and Carleo, Giuseppe},
	month = nov,
	year = {2022},
	note = {arXiv:2103.10293 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantum Physics},
	pages = {205136},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {00219991},
	shorttitle = {{DGM}},
	url = {http://arxiv.org/abs/1708.07469},
	doi = {10.1016/j.jcp.2018.08.029},
	abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to \$200\$ dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a "Deep Galerkin Method (DGM)" since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
	urldate = {2023-01-20},
	journal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = dec,
	year = {2018},
	note = {arXiv:1708.07469 [math, q-fin, stat]},
	keywords = {Mathematics - Numerical Analysis, Quantitative Finance - Computational Finance, Quantitative Finance - Mathematical Finance, Statistics - Machine Learning},
	pages = {1339--1364},
}

@article{hutzenthaler_overcoming_2020,
	title = {Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations},
	volume = {476},
	issn = {1364-5021, 1471-2946},
	url = {http://arxiv.org/abs/1807.01212},
	doi = {10.1098/rspa.2019.0630},
	abstract = {For a long time it is well-known that high-dimensional linear parabolic partial differential equations (PDEs) can be approximated by Monte Carlo methods with a computational effort which grows polynomially both in the dimension and in the reciprocal of the prescribed accuracy. In other words, linear PDEs do not suffer from the curse of dimensionality. For general semilinear PDEs with Lipschitz coefficients, however, it remained an open question whether these suffer from the curse of dimensionality. In this paper we partially solve this open problem. More precisely, we prove in the case of semilinear heat equations with gradient-independent and globally Lipschitz continuous nonlinearities that the computational effort of a variant of the recently introduced multilevel Picard approximations grows polynomially both in the dimension and in the reciprocal of the required accuracy.},
	number = {2244},
	urldate = {2023-01-20},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Nguyen, Tuan Anh and von Wurstemberger, Philippe},
	month = dec,
	year = {2020},
	note = {arXiv:1807.01212 [cs, math]},
	keywords = {65M75, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Mathematics - Probability},
	pages = {20190630},
}

@misc{lloyd_30_2016,
	title = {30 {Travel} {Tips} {To} {Know} {Before} {Visiting} {Italy}},
	url = {https://handluggageonly.co.uk/2016/06/14/30-travel-tips-you-need-to-know-before-visiting-italy/},
	abstract = {Italy is one of the most beautiful countries to visit in Europe, nay, the world! My love for this country goes almost as deep as my love of chocolate-flavoured-anything, which I feel no remorse about},
	language = {en-US},
	urldate = {2023-01-16},
	journal = {Hand Luggage Only},
	author = {Lloyd},
	month = jun,
	year = {2016},
}

@article{park_expressive_2022,
	title = {Expressive power of complex-valued restricted {Boltzmann} machines for solving non-stoquastic {Hamiltonians}},
	volume = {106},
	issn = {2469-9950, 2469-9969},
	url = {http://arxiv.org/abs/2012.08889},
	doi = {10.1103/PhysRevB.106.134437},
	abstract = {Variational Monte Carlo with neural network quantum states has proven to be a promising avenue for evaluating the ground state energy of spin Hamiltonians. However, despite continuous efforts the performance of the method on frustrated Hamiltonians remains significantly worse than those on stoquastic Hamiltonians that are sign-free. We present a detailed and systematic study of restricted Boltzmann machine (RBM) based variational Monte Carlo for quantum spin chains, resolving how relevant stoquasticity is in this setting. We show that in most cases, when the Hamiltonian is phase connected with a stoquastic point, the complex RBM state can faithfully represent the ground state, and local quantities can be evaluated efficiently by sampling. On the other hand, we identify several new phases that are challenging for the RBM Ansatz, including non-topological robust non-stoquastic phases as well as stoquastic phases where sampling is nevertheless inefficient. Furthermore, we find that an accurate neural network representation of ground states in non-stoquastic phases is hindered not only by the sign structure but also by their amplitudes.},
	number = {13},
	urldate = {2023-01-09},
	journal = {Physical Review B},
	author = {Park, Chae-Yeun and Kastoryano, Michael J.},
	month = oct,
	year = {2022},
	note = {arXiv:2012.08889 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Statistical Mechanics, Quantum Physics},
	pages = {134437},
}

@article{park_geometry_2020,
	title = {Geometry of learning neural quantum states},
	volume = {2},
	issn = {2643-1564},
	url = {http://arxiv.org/abs/1910.11163},
	doi = {10.1103/PhysRevResearch.2.023232},
	abstract = {Combining insights from machine learning and quantum Monte Carlo, the stochastic reconfiguration method with neural network Ansatz states is a promising new direction for high-precision ground state estimation of quantum many-body problems. Even though this method works well in practice, little is known about the learning dynamics. In this paper, we bring to light several hidden details of the algorithm by analyzing the learning landscape. In particular, the spectrum of the quantum Fisher matrix of complex restricted Boltzmann machine states exhibits a universal initial dynamics, but the converged spectrum can dramatically change across a phase transition. In contrast to the spectral properties of the quantum Fisher matrix, the actual weights of the network at convergence do not reveal much information about the system or the dynamics. Furthermore, we identify a new measure of correlation in the state by analyzing entanglement in eigenvectors. We show that, generically, the learning landscape modes with least entanglement have largest eigenvalue, suggesting that correlations are encoded in large flat valleys of the learning landscape, favoring stable representations of the ground state.},
	number = {2},
	urldate = {2023-01-09},
	journal = {Physical Review Research},
	author = {Park, Chae-Yeun and Kastoryano, Michael J.},
	month = may,
	year = {2020},
	note = {arXiv:1910.11163 [cond-mat, physics:quant-ph, stat]},
	keywords = {Condensed Matter - Statistical Mechanics, Quantum Physics, Statistics - Machine Learning},
	pages = {023232},
}

@article{cai_approximating_2018,
	title = {Approximating quantum many-body wave functions using artificial neural networks},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.035116},
	doi = {10.1103/PhysRevB.97.035116},
	abstract = {In this paper, we demonstrate the expressibility of artificial neural networks (ANNs) in quantum many-body physics by showing that a feed-forward neural network with a small number of hidden layers can be trained to approximate with high precision the ground states of some notable quantum many-body systems. We consider the one-dimensional free bosons and fermions, spinless fermions on a square lattice away from half-filling, as well as frustrated quantum magnetism with a rapidly oscillating ground-state characteristic function. In the latter case, an ANN with a standard architecture fails, while that with a slightly modified one successfully learns the frustration-induced complex sign rule in the ground state and approximates the ground states with high precisions. As an example of practical use of our method, we also perform the variational method to explore the ground state of an antiferromagnetic J1−J2 Heisenberg model.},
	number = {3},
	urldate = {2022-12-22},
	journal = {Physical Review B},
	author = {Cai, Zi and Liu, Jinguo},
	month = jan,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {035116},
}

@article{berner_analysis_2020,
	title = {Analysis of the {Generalization} {Error}: {Empirical} {Risk} {Minimization} over {Deep} {Artificial} {Neural} {Networks} {Overcomes} the {Curse} of {Dimensionality} in the {Numerical} {Approximation} of {Black}-{Scholes} {Partial} {Differential} {Equations}},
	volume = {2},
	issn = {2577-0187},
	shorttitle = {Analysis of the {Generalization} {Error}},
	url = {http://arxiv.org/abs/1809.03062},
	doi = {10.1137/19M125649X},
	abstract = {The development of new classification and regression algorithms based on empirical risk minimization (ERM) over deep neural network hypothesis classes, coined deep learning, revolutionized the area of artificial intelligence, machine learning, and data analysis. In particular, these methods have been applied to the numerical solution of high-dimensional partial differential equations with great success. Recent simulations indicate that deep learning-based algorithms are capable of overcoming the curse of dimensionality for the numerical solution of Kolmogorov equations, which are widely used in models from engineering, finance, and the natural sciences. The present paper considers under which conditions ERM over a deep neural network hypothesis class approximates the solution of a \$d\$-dimensional Kolmogorov equation with affine drift and diffusion coefficients and typical initial values arising from problems in computational finance up to error \${\textbackslash}varepsilon\$. We establish that, with high probability over draws of training samples, such an approximation can be achieved with both the size of the hypothesis class and the number of training samples scaling only polynomially in \$d\$ and \${\textbackslash}varepsilon{\textasciicircum}\{-1\}\$. It can be concluded that ERM over deep neural network hypothesis classes overcomes the curse of dimensionality for the numerical solution of linear Kolmogorov equations with affine coefficients.},
	number = {3},
	urldate = {2022-12-19},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Berner, Julius and Grohs, Philipp and Jentzen, Arnulf},
	month = jan,
	year = {2020},
	note = {arXiv:1809.03062 [cs, math, stat]},
	keywords = {60H30, 65C30, 62M45, 68T05, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	pages = {631--657},
}

@article{hutzenthaler_overcoming_2020-1,
	title = {Overcoming the curse of dimensionality in the numerical approximation of semilinear parabolic partial differential equations},
	volume = {476},
	issn = {1364-5021, 1471-2946},
	url = {http://arxiv.org/abs/1807.01212},
	doi = {10.1098/rspa.2019.0630},
	abstract = {For a long time it is well-known that high-dimensional linear parabolic partial differential equations (PDEs) can be approximated by Monte Carlo methods with a computational effort which grows polynomially both in the dimension and in the reciprocal of the prescribed accuracy. In other words, linear PDEs do not suffer from the curse of dimensionality. For general semilinear PDEs with Lipschitz coefficients, however, it remained an open question whether these suffer from the curse of dimensionality. In this paper we partially solve this open problem. More precisely, we prove in the case of semilinear heat equations with gradient-independent and globally Lipschitz continuous nonlinearities that the computational effort of a variant of the recently introduced multilevel Picard approximations grows polynomially both in the dimension and in the reciprocal of the required accuracy.},
	number = {2244},
	urldate = {2022-12-19},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Hutzenthaler, Martin and Jentzen, Arnulf and Kruse, Thomas and Nguyen, Tuan Anh and von Wurstemberger, Philippe},
	month = dec,
	year = {2020},
	note = {arXiv:1807.01212 [cs, math]},
	keywords = {65M75, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Mathematics - Probability},
	pages = {20190630},
}

@article{cuzzocrea_variational_2020,
	title = {Variational {Principles} in {Quantum} {Monte} {Carlo}: {The} {Troubled} {Story} of {Variance} {Minimization}},
	volume = {16},
	issn = {1549-9618},
	shorttitle = {Variational {Principles} in {Quantum} {Monte} {Carlo}},
	url = {https://doi.org/10.1021/acs.jctc.0c00147},
	doi = {10.1021/acs.jctc.0c00147},
	abstract = {We investigate the use of different variational principles in quantum Monte Carlo, namely, energy and variance minimization, prompted by the interest in the robust and accurate estimation of electronic excited states. For two prototypical, challenging molecules, we readily reach the accuracy of the best available reference excitation energies using energy minimization in a state-specific or state-average fashion for states of different or equal symmetry, respectively. On the other hand, in variance minimization, where the use of suitable functionals is expected to target specific states regardless of the symmetry, we encounter severe problems for a variety of wave functions: as the variance converges, the energy drifts away from that of the selected state. This unexpected behavior is sometimes observed even when the target is the ground state and generally prevents the robust estimation of total and excitation energies. We analyze this problem using a very simple wave function and infer that the optimization finds little or no barrier to escape from a local minimum or local plateau, eventually converging to a lower-variance state instead of the target state. For the increasingly complex systems becoming in reach of quantum Monte Carlo simulations, variance minimization with current functionals appears to be an impractical route.},
	number = {7},
	urldate = {2022-12-05},
	journal = {Journal of Chemical Theory and Computation},
	author = {Cuzzocrea, Alice and Scemama, Anthony and Briels, Wim J. and Moroni, Saverio and Filippi, Claudia},
	month = jul,
	year = {2020},
	note = {Publisher: American Chemical Society},
	pages = {4203--4212},
}

@misc{von_glehn_self-attention_2022,
	title = {A {Self}-{Attention} {Ansatz} for {Ab}-initio {Quantum} {Chemistry}},
	url = {http://arxiv.org/abs/2211.13672},
	abstract = {We present a novel neural network architecture using self-attention, the Wavefunction Transformer (Psiformer), which can be used as an approximation (or Ansatz) for solving the many-electron Schr{\textbackslash}"odinger equation, the fundamental equation for quantum chemistry and material science. This equation can be solved from first principles, requiring no external training data. In recent years, deep neural networks like the FermiNet and PauliNet have been used to significantly improve the accuracy of these first-principle calculations, but they lack an attention-like mechanism for gating interactions between electrons. Here we show that the Psiformer can be used as a drop-in replacement for these other neural networks, often dramatically improving the accuracy of the calculations. On larger molecules especially, the ground state energy can be improved by dozens of kcal/mol, a qualitative leap over previous methods. This demonstrates that self-attention networks can learn complex quantum mechanical correlations between electrons, and are a promising route to reaching unprecedented accuracy in chemical calculations on larger systems.},
	urldate = {2022-12-02},
	publisher = {arXiv},
	author = {von Glehn, Ingrid and Spencer, James S. and Pfau, David},
	month = nov,
	year = {2022},
	note = {arXiv:2211.13672 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Computational Physics},
}

@article{choo_symmetries_2018,
	title = {Symmetries and {Many}-{Body} {Excitations} with {Neural}-{Network} {Quantum} {States}},
	volume = {121},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.167204},
	doi = {10.1103/PhysRevLett.121.167204},
	abstract = {Artificial neural networks have been recently introduced as a general ansatz to represent many-body wave functions. In conjunction with variational Monte Carlo calculations, this ansatz has been applied to find Hamiltonian ground states and their energies. Here, we provide extensions of this method to study excited states, a central task in several many-body quantum calculations. First, we give a prescription that allows us to target eigenstates of a (nonlocal) symmetry of the Hamiltonian. Second, we give an algorithm to compute low-lying excited states without symmetries. We demonstrate our approach with both restricted Boltzmann machines and feed-forward neural networks. Results are shown for the one-dimensional spin-1/2 Heisenberg model, and for the one-dimensional Bose-Hubbard model. When comparing to exact results, we obtain good agreement for a large range of excited-states energies. Interestingly, we find that deep networks typically outperform shallow architectures for high-energy states.},
	number = {16},
	urldate = {2022-10-19},
	journal = {Physical Review Letters},
	author = {Choo, Kenny and Carleo, Giuseppe and Regnault, Nicolas and Neupert, Titus},
	month = oct,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {167204},
}

@article{choo_symmetries_2018-1,
	title = {Symmetries and many-body excited states with neural-network quantum states},
	volume = {121},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1807.03325},
	doi = {10.1103/PhysRevLett.121.167204},
	abstract = {Artificial neural networks have been recently introduced as a general ansatz to compactly represent many- body wave functions. In conjunction with Variational Monte Carlo, this ansatz has been applied to find Hamil- tonian ground states and their energies. Here we provide extensions of this method to study properties of ex- cited states, a central task in several many-body quantum calculations. First, we give a prescription that allows to target eigenstates of a (nonlocal) symmetry of the Hamiltonian. Second, we give an algorithm that allows to compute low-lying excited states without symmetries. We demonstrate our approach with both Restricted Boltzmann machines states and feedforward neural networks as variational wave-functions. Results are shown for the one-dimensional spin-1/2 Heisenberg model, and for the one-dimensional Bose-Hubbard model. When comparing to available exact results, we obtain good agreement for a large range of excited-states energies. Interestingly, we also find that deep networks typically outperform shallow architectures for high-energy states.},
	number = {16},
	urldate = {2022-10-18},
	journal = {Physical Review Letters},
	author = {Choo, Kenny and Carleo, Giuseppe and Regnault, Nicolas and Neupert, Titus},
	month = oct,
	year = {2018},
	note = {arXiv:1807.03325 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
	pages = {167204},
}

@article{kato_eigenfunctions_1957,
	title = {On the eigenfunctions of many-particle systems in quantum mechanics},
	volume = {10},
	issn = {00103640, 10970312},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.3160100201},
	doi = {10.1002/cpa.3160100201},
	abstract = {The continuity properties or singularities of the eigenfunctions of a many-particle system in non-relativlstic quantum mechanics are investigated. It is shovm that all eigenfunctions of such a many-partiole system are continuous throughout the configuration space, and that the eigenfunctions have partial derivatives of first order (except at the Coulomb-type singular points of the potential) which are bounded functions The nature of the singularities of the eigenfunctions are described in greater detail.},
	language = {en},
	number = {2},
	urldate = {2022-10-18},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Kato, Tosio},
	year = {1957},
	pages = {151--177},
}

@article{lowdin_quantum_1955,
	title = {Quantum {Theory} of {Many}-{Particle} {Systems}. {II}. {Study} of the {Ordinary} {Hartree}-{Fock} {Approximation}},
	volume = {97},
	url = {https://link.aps.org/doi/10.1103/PhysRev.97.1490},
	doi = {10.1103/PhysRev.97.1490},
	abstract = {A system of N antisymmetric particles, moving under the influence of a fixed potential and their mutual many-particle interactions, is investigated in the ordinary Hartree-Fock scheme, having the total wave function approximated by a single Slater determinant. It is shown that all the density matrices of various orders, the wave function, and the entire physical situation depends only on a fundamental invariant ρ(x1, x2), which is identical with the first-order density matrix. The Hartree-Fock equations are expressed in terms of this quantity.},
	number = {6},
	urldate = {2022-10-18},
	journal = {Physical Review},
	author = {Löwdin, Per-Olov},
	month = mar,
	year = {1955},
	note = {Publisher: American Physical Society},
	pages = {1490--1508},
}

@article{kato_eigenfunctions_1957-1,
	title = {On the eigenfunctions of many-particle systems in quantum mechanics},
	volume = {10},
	issn = {1097-0312},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160100201},
	doi = {10.1002/cpa.3160100201},
	language = {en},
	number = {2},
	urldate = {2022-10-18},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Kato, Tosio},
	year = {1957},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160100201},
	pages = {151--177},
}

@misc{dawid_modern_2022,
	title = {Modern applications of machine learning in quantum sciences},
	url = {http://arxiv.org/abs/2204.04198},
	doi = {10.48550/arXiv.2204.04198},
	abstract = {In these Lecture Notes, we provide a comprehensive introduction to the most recent advances in the application of machine learning methods in quantum sciences. We cover the use of deep learning and kernel methods in supervised, unsupervised, and reinforcement learning algorithms for phase classification, representation of many-body quantum states, quantum feedback control, and quantum circuits optimization. Moreover, we introduce and discuss more specialized topics such as differentiable programming, generative models, statistical approach to machine learning, and quantum machine learning.},
	urldate = {2022-10-17},
	publisher = {arXiv},
	author = {Dawid, Anna and Arnold, Julian and Requena, Borja and Gresch, Alexander and Płodzień, Marcin and Donatella, Kaelan and Nicoli, Kim A. and Stornati, Paolo and Koch, Rouven and Büttner, Miriam and Okuła, Robert and Muñoz-Gil, Gorka and Vargas-Hernández, Rodrigo A. and Cervera-Lierta, Alba and Carrasquilla, Juan and Dunjko, Vedran and Gabrié, Marylou and Huembeli, Patrick and van Nieuwenburg, Evert and Vicentini, Filippo and Wang, Lei and Wetzel, Sebastian J. and Carleo, Giuseppe and Greplová, Eliška and Krems, Roman and Marquardt, Florian and Tomza, Michał and Lewenstein, Maciej and Dauphin, Alexandre},
	month = jun,
	year = {2022},
	note = {arXiv:2204.04198 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Mesoscale and Nanoscale Physics, Quantum Physics},
}

@article{carleo_machine_2019,
	title = {Machine learning and the physical sciences},
	volume = {91},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.91.045002},
	doi = {10.1103/RevModPhys.91.045002},
	abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.},
	number = {4},
	urldate = {2022-10-17},
	journal = {Reviews of Modern Physics},
	author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborová, Lenka},
	month = dec,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {045002},
}

@article{carleo_solving_2017,
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	url = {https://www.science.org/doi/10.1126/science.aag2302},
	doi = {10.1126/science.aag2302},
	number = {6325},
	urldate = {2022-10-17},
	journal = {Science},
	author = {Carleo, Giuseppe and Troyer, Matthias},
	month = feb,
	year = {2017},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {602--606},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hermann_ab-initio_2022,
	title = {Ab-initio quantum chemistry with neural-network wavefunctions},
	url = {http://arxiv.org/abs/2208.12590},
	doi = {10.48550/arXiv.2208.12590},
	abstract = {Machine learning and specifically deep-learning methods have outperformed human capabilities in many pattern recognition and data processing problems, in game playing, and now also play an increasingly important role in scientific discovery. A key application of machine learning in the molecular sciences is to learn potential energy surfaces or force fields from ab-initio solutions of the electronic Schr{\textbackslash}"odinger equation using datasets obtained with density functional theory, coupled cluster, or other quantum chemistry methods. Here we review a recent and complementary approach: using machine learning to aid the direct solution of quantum chemistry problems from first principles. Specifically, we focus on quantum Monte Carlo (QMC) methods that use neural network ansatz functions in order to solve the electronic Schr{\textbackslash}"odinger equation, both in first and second quantization, computing ground and excited states, and generalizing over multiple nuclear configurations. Compared to existing quantum chemistry methods, these new deep QMC methods have the potential to generate highly accurate solutions of the Schr{\textbackslash}"odinger equation at relatively modest computational cost.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Hermann, Jan and Spencer, James and Choo, Kenny and Mezzacapo, Antonio and Foulkes, W. M. C. and Pfau, David and Carleo, Giuseppe and Noé, Frank},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12590 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{cuzzocrea_variational_2020-1,
	title = {Variational {Principles} in {Quantum} {Monte} {Carlo}: {The} {Troubled} {Story} of {Variance} {Minimization}},
	volume = {16},
	issn = {1549-9618},
	shorttitle = {Variational {Principles} in {Quantum} {Monte} {Carlo}},
	url = {https://doi.org/10.1021/acs.jctc.0c00147},
	doi = {10.1021/acs.jctc.0c00147},
	abstract = {We investigate the use of different variational principles in quantum Monte Carlo, namely, energy and variance minimization, prompted by the interest in the robust and accurate estimation of electronic excited states. For two prototypical, challenging molecules, we readily reach the accuracy of the best available reference excitation energies using energy minimization in a state-specific or state-average fashion for states of different or equal symmetry, respectively. On the other hand, in variance minimization, where the use of suitable functionals is expected to target specific states regardless of the symmetry, we encounter severe problems for a variety of wave functions: as the variance converges, the energy drifts away from that of the selected state. This unexpected behavior is sometimes observed even when the target is the ground state and generally prevents the robust estimation of total and excitation energies. We analyze this problem using a very simple wave function and infer that the optimization finds little or no barrier to escape from a local minimum or local plateau, eventually converging to a lower-variance state instead of the target state. For the increasingly complex systems becoming in reach of quantum Monte Carlo simulations, variance minimization with current functionals appears to be an impractical route.},
	number = {7},
	urldate = {2022-10-16},
	journal = {Journal of Chemical Theory and Computation},
	author = {Cuzzocrea, Alice and Scemama, Anthony and Briels, Wim J. and Moroni, Saverio and Filippi, Claudia},
	month = jul,
	year = {2020},
	note = {Publisher: American Chemical Society},
	pages = {4203--4212},
}

@misc{entwistle_electronic_2022,
	title = {Electronic excited states in deep variational {Monte} {Carlo}},
	url = {http://arxiv.org/abs/2203.09472},
	doi = {10.48550/arXiv.2203.09472},
	abstract = {Obtaining accurate ground and low-lying excited states of electronic systems is crucial in a multitude of important applications. One ab initio method for solving the Schr{\textbackslash}"odinger equation that scales favorably for large systems is variational quantum Monte Carlo (QMC). The recently introduced deep QMC approach uses ansatzes represented by deep neural networks and generates nearly exact ground-state solutions for molecules containing up to a few dozen electrons, with the potential to scale to much larger systems where other highly accurate methods are not feasible. In this paper, we extend one such ansatz (PauliNet) to compute electronic excited states. We demonstrate our method on various small atoms and molecules and consistently achieve high accuracy for low-lying states. To highlight the method's potential, we compute the first excited state of the much larger benzene molecule, as well as the conical intersection of ethylene, with PauliNet matching results of more expensive high-level methods.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Entwistle, Mike and Schätzle, Zeno and Erdman, Paolo A. and Hermann, Jan and Noé, Frank},
	month = aug,
	year = {2022},
	note = {arXiv:2203.09472 [physics, stat]},
	keywords = {Physics - Chemical Physics, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{pathak_excited_2021,
	title = {Excited states in variational {Monte} {Carlo} using a penalty method},
	volume = {154},
	issn = {0021-9606, 1089-7690},
	url = {http://arxiv.org/abs/2009.13556},
	doi = {10.1063/5.0030949},
	abstract = {The authors present a technique using variational Monte Carlo to solve for excited states of electronic systems. The technique is based on enforcing orthogonality to lower energy states, which results in a simple variational principle for the excited states. Energy optimization is then used to solve for the excited states. This technique is applied to the well-characterized benzene molecule, in which \${\textbackslash}sim\$10,000 parameters are optimized for the first 12 excited states. Agreement within approximately 0.2 eV is obtained with higher scaling coupled cluster methods; small disagreements with experiment are likely due to vibrational effects.},
	number = {3},
	urldate = {2022-10-16},
	journal = {The Journal of Chemical Physics},
	author = {Pathak, Shivesh and Busemeyer, Brian and Rodrigues, João N. B. and Wagner, Lucas K.},
	month = jan,
	year = {2021},
	note = {arXiv:2009.13556 [cond-mat, physics:physics]},
	keywords = {Condensed Matter - Materials Science, Physics - Chemical Physics},
	pages = {034101},
}

@misc{hermann_ab-initio_2022-1,
	title = {Ab-initio quantum chemistry with neural-network wavefunctions},
	url = {http://arxiv.org/abs/2208.12590},
	doi = {10.48550/arXiv.2208.12590},
	abstract = {Machine learning and specifically deep-learning methods have outperformed human capabilities in many pattern recognition and data processing problems, in game playing, and now also play an increasingly important role in scientific discovery. A key application of machine learning in the molecular sciences is to learn potential energy surfaces or force fields from ab-initio solutions of the electronic Schr{\textbackslash}"odinger equation using datasets obtained with density functional theory, coupled cluster, or other quantum chemistry methods. Here we review a recent and complementary approach: using machine learning to aid the direct solution of quantum chemistry problems from first principles. Specifically, we focus on quantum Monte Carlo (QMC) methods that use neural network ansatz functions in order to solve the electronic Schr{\textbackslash}"odinger equation, both in first and second quantization, computing ground and excited states, and generalizing over multiple nuclear configurations. Compared to existing quantum chemistry methods, these new deep QMC methods have the potential to generate highly accurate solutions of the Schr{\textbackslash}"odinger equation at relatively modest computational cost.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Hermann, Jan and Spencer, James and Choo, Kenny and Mezzacapo, Antonio and Foulkes, W. M. C. and Pfau, David and Carleo, Giuseppe and Noé, Frank},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12590 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Computational Physics, Statistics - Machine Learning},
}

@misc{noauthor_deepqmc_2022,
	title = {{DeepQMC}},
	copyright = {MIT},
	url = {https://github.com/deepqmc/deepqmc},
	abstract = {Deep learning quantum Monte Carlo for electrons in real space},
	urldate = {2022-10-12},
	publisher = {deepqmc},
	month = oct,
	year = {2022},
	note = {original-date: 2019-12-06T14:50:59Z},
	keywords = {deep-learning, electrons, quantum-chemistry, quantum-monte-carlo},
}

@article{troyer_computational_2005,
	title = {Computational complexity and fundamental limitations to fermionic quantum {Monte} {Carlo} simulations},
	volume = {94},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/cond-mat/0408370},
	doi = {10.1103/PhysRevLett.94.170201},
	abstract = {Quantum Monte Carlo simulations, while being efficient for bosons, suffer from the "negative sign problem'' when applied to fermions - causing an exponential increase of the computing time with the number of particles. A polynomial time solution to the sign problem is highly desired since it would provide an unbiased and numerically exact method to simulate correlated quantum systems. Here we show, that such a solution is almost certainly unattainable by proving that the sign problem is NP-hard, implying that a generic solution of the sign problem would also solve all problems in the complexity class NP (nondeterministic polynomial) in polynomial time.},
	number = {17},
	urldate = {2022-10-12},
	journal = {Physical Review Letters},
	author = {Troyer, Matthias and Wiese, Uwe-Jens},
	month = may,
	year = {2005},
	note = {arXiv:cond-mat/0408370},
	keywords = {Computer Science - Computational Complexity, Condensed Matter - Statistical Mechanics, Condensed Matter - Strongly Correlated Electrons, High Energy Physics - Lattice, Physics - Computational Physics},
	pages = {170201},
}

@article{foulkes_quantum_2001,
	title = {Quantum {Monte} {Carlo} simulations of solids},
	volume = {73},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.73.33},
	doi = {10.1103/RevModPhys.73.33},
	abstract = {This article describes the variational and fixed-node diffusion quantum Monte Carlo methods and how they may be used to calculate the properties of many-electron systems. These stochastic wave-function-based approaches provide a very direct treatment of quantum many-body effects and serve as benchmarks against which other techniques may be compared. They complement the less demanding density-functional approach by providing more accurate results and a deeper understanding of the physics of electronic correlation in real materials. The algorithms are intrinsically parallel, and currently available high-performance computers allow applications to systems containing a thousand or more electrons. With these tools one can study complicated problems such as the properties of surfaces and defects, while including electron correlation effects with high precision. The authors provide a pedagogical overview of the techniques and describe a selection of applications to ground and excited states of solids and clusters.},
	number = {1},
	urldate = {2022-10-10},
	journal = {Reviews of Modern Physics},
	author = {Foulkes, W. M. C. and Mitas, L. and Needs, R. J. and Rajagopal, G.},
	month = jan,
	year = {2001},
	note = {Publisher: American Physical Society},
	pages = {33--83},
}

@article{foulkes_quantum_2001-1,
	title = {Quantum {Monte} {Carlo} simulations of solids},
	volume = {73},
	issn = {0034-6861, 1539-0756},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.73.33},
	doi = {10.1103/RevModPhys.73.33},
	language = {en},
	number = {1},
	urldate = {2022-10-10},
	journal = {Reviews of Modern Physics},
	author = {Foulkes, W. M. C. and Mitas, L. and Needs, R. J. and Rajagopal, G.},
	month = jan,
	year = {2001},
	pages = {33--83},
}

@misc{hutter_representing_2020,
	title = {On {Representing} ({Anti}){Symmetric} {Functions}},
	url = {http://arxiv.org/abs/2007.15298},
	doi = {10.48550/arXiv.2007.15298},
	abstract = {Permutation-invariant, -equivariant, and -covariant functions and anti-symmetric functions are important in quantum physics, computer vision, and other disciplines. Applications often require most or all of the following properties: (a) a large class of such functions can be approximated, e.g. all continuous function, (b) only the (anti)symmetric functions can be represented, (c) a fast algorithm for computing the approximation, (d) the representation itself is continuous or differentiable, (e) the architecture is suitable for learning the function from data. (Anti)symmetric neural networks have recently been developed and applied with great success. A few theoretical approximation results have been proven, but many questions are still open, especially for particles in more than one dimension and the anti-symmetric case, which this work focusses on. More concretely, we derive natural polynomial approximations in the symmetric case, and approximations based on a single generalized Slater determinant in the anti-symmetric case. Unlike some previous super-exponential and discontinuous approximations, these seem a more promising basis for future tighter bounds. We provide a complete and explicit universality proof of the Equivariant MultiLayer Perceptron, which implies universality of symmetric MLPs and the FermiNet.},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Hutter, Marcus},
	month = jul,
	year = {2020},
	note = {arXiv:2007.15298 [quant-ph]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantum Physics},
}

@misc{noauthor_ferminet_2022,
	title = {{FermiNet}: {Fermionic} {Neural} {Networks}},
	copyright = {Apache-2.0},
	shorttitle = {{FermiNet}},
	url = {https://github.com/deepmind/ferminet},
	abstract = {An implementation of the Fermionic Neural Network for ab-initio electronic structure calculations},
	urldate = {2022-10-08},
	publisher = {DeepMind},
	month = oct,
	year = {2022},
	note = {original-date: 2020-10-06T12:21:06Z},
}

@misc{noauthor_ferminet_nodate,
	title = {{FermiNet}: {Quantum} {Physics} and {Chemistry} from {First} {Principles}},
	shorttitle = {{FermiNet}},
	url = {https://www.deepmind.com/blog/ferminet-quantum-physics-and-chemistry-from-first-principles},
	abstract = {In an article recently published in Physical Review Research, we show how deep learning can help solve the fundamental equations of quantum mechanics for real-world systems. Not only is this an important fundamental scientific question, but it also could lead to practical uses in the future, allowing researchers to prototype new materials and chemical syntheses in silico before trying to make them in the lab. Today we are also releasing the code from this study so that the computational physics and chemistry communities can build on our work and apply it to a wide range of problems. We’ve developed a new neural network architecture, the Fermionic Neural Network or FermiNet, which is well-suited to modeling the quantum state of large collections of electrons, the fundamental building blocks of chemical bonds. The FermiNet was the first demonstration of deep learning for computing the energy of atoms and molecules from first principles that was accurate enough to be useful, and it remains the most accurate neural network method to date. We hope the tools and ideas developed in our AI research at DeepMind can help solve fundamental problems in the natural sciences, and the FermiNet joins our work on protein folding, glassy dynamics, lattice quantum chromodynamics and many other projects in bringing that vision to life.},
	language = {en},
	urldate = {2022-10-08},
}

@misc{spencer_better_2020,
	title = {Better, {Faster} {Fermionic} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2011.07125},
	doi = {10.48550/arXiv.2011.07125},
	abstract = {The Fermionic Neural Network (FermiNet) is a recently-developed neural network architecture that can be used as a wavefunction Ansatz for many-electron systems, and has already demonstrated high accuracy on small systems. Here we present several improvements to the FermiNet that allow us to set new records for speed and accuracy on challenging systems. We find that increasing the size of the network is sufficient to reach chemical accuracy on atoms as large as argon. Through a combination of implementing FermiNet in JAX and simplifying several parts of the network, we are able to reduce the number of GPU hours needed to train the FermiNet on large systems by an order of magnitude. This enables us to run the FermiNet on the challenging transition of bicyclobutane to butadiene and compare against the PauliNet on the automerization of cyclobutadiene, and we achieve results near the state of the art for both.},
	urldate = {2022-10-08},
	publisher = {arXiv},
	author = {Spencer, James S. and Pfau, David and Botev, Aleksandar and Foulkes, W. M. C.},
	month = nov,
	year = {2020},
	note = {arXiv:2011.07125 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Computational Physics},
}

@phdthesis{lin_neural_2022,
	title = {Neural {Network} {Antisymmetries} for {Quantum} {Many}-{Body} {Simulation}},
	url = {https://escholarship.org/uc/item/6vt8541f},
	abstract = {This work is concerned with the accurate numerical simulation of the many-electron problem, which involves the modeling of the electron wavefunction, from which all of the properties of a chemical or condensed matter system can, in principle, be computed. This problem poses a number of challenges, including the effective parametrization of the wavefunction space. The combination of neural networks and quantum Monte Carlo methods has arisen as a promising path forward for highly accurate electronic structure calculations. Previous proposals have combined equivariant neural network layers with a final antisymmetric layer in order to satisfy the antisymmetry requirements of the electronic wavefunction. However, to date it is unclear if one can represent arbitrary antisymmetric functions of physical interest, and it is difficult to precisely measure the expressiveness of the antisymmetric layer.In the first chapter of this dissertation, we begin by introducing the electronic structure problem and the variational nature of finding the lowest energy wavefunction, or ground state. We describe Metropolis Monte Carlo sampling techniques, as well as a simplifying reduction to the number of degrees of freedom. We conclude the first chapter with a brief discussion of some optimization techniques used to address the variational ground state problem once a trial parametrization has been established.In the next chapter, we then introduce the form of some modern neural-network based trial wavefunctions. We attempt to investigate the expressiveness of the antisymmetric layers by proposing explicitly antisymmetrized universal neural network layers. This approach has a computational cost which increases factorially with respect to the system size, but we are nonetheless able to apply it to small systems to better understand how the structure of the antisymmetric layer affects its performance. We first introduce a generic antisymmetric (GA) neural network layer, which we use to replace the entire antisymmetric layer of the highly accurate ansatz known as the FermiNet. We also consider a factorized antisymmetric (FA) layer which more directly generalizes the FermiNet by replacing the products of determinants with products of antisymmetrized neural networks.We next investigate the numerical performance of these explicitly antisymmetrized ansatzes. We demonstrate that the FermiNet-GA architecture can yield effectively the exact ground state energy for small atoms and molecules. We find, interestingly, that the resulting FermiNet-FA architecture does not outperform the FermiNet. This strongly suggests that the sum of products of antisymmetries is a key limiting aspect of the FermiNet architecture. To explore this further, we also investigate a slight modification of the FermiNet, called the full determinant mode, which replaces each product of determinants with a single combined determinant. We find that the full single-determinant FermiNet closes a large part of the gap between the standard single-determinant FermiNet and FermiNet-GA on small atomic and molecular problems. Surprisingly, on the nitrogen molecule at a dissociating bond length of 4.0 Bohr, the full single-determinant FermiNet can significantly outperform the largest standard FermiNet calculation with 64 determinants, yielding an energy within 0.4 kcal/mol of the best available computational benchmark.In the final chapter, we introduce the VMCNet repository, which was used to implement the numerical experiments previously described. VMCNet is intended to be a flexible, general purpose VMC framework which interfaces natively with the JAX library for rapid prototyping, performance benefits due to just-in-time XLA compilation, and easy dispatch to multiple GPU systems. We describe both the Python API, intended for more complex use-cases that require customization of finer details of the VMC loop, and the command-line interface, which provides a more streamlined and encapsulated way to run variational Monte Carlo experiments, including those described in this dissertation.},
	language = {en},
	urldate = {2022-10-08},
	school = {UC Berkeley},
	author = {Lin, Jeffmin},
	year = {2022},
}

@misc{ju_interpreting_2022,
	title = {Interpreting convolutional neural networks' low dimensional approximation to quantum spin systems},
	url = {http://arxiv.org/abs/2210.00692},
	doi = {10.48550/arXiv.2210.00692},
	abstract = {Convolutional neural networks (CNNs) have been employed along with Variational Monte Carlo methods for finding the ground state of quantum many-body spin systems with great success. In order to do so, however, a CNN with only linearly many variational parameters has to circumvent the ``curse of dimensionality'' and successfully approximate a wavefunction on an exponentially large Hilbert space. In our work, we provide a theoretical and experimental analysis of how the CNN optimizes learning for spin systems, and investigate the CNN's low dimensional approximation. We first quantify the role played by physical symmetries of the underlying spin system during training. We incorporate our insights into a new training algorithm and demonstrate its improved efficiency, accuracy and robustness. We then further investigate the CNN's ability to approximate wavefunctions by looking at the entanglement spectrum captured by the size of the convolutional filter. Our insights reveal the CNN to be an ansatz fundamentally centered around the occurrence statistics of \$K\$-motifs of the input strings. We use this motivation to provide the shallow CNN ansatz with a unifying theoretical interpretation in terms of other well-known statistical and physical ansatzes such as the maximum entropy (MaxEnt) and entangled plaquette correlator product states (EP-CPS). Using regression analysis, we find further relationships between the CNN's approximations of the different motifs' expectation values. Our results allow us to gain a comprehensive, improved understanding of how CNNs successfully approximate quantum spin Hamiltonians and to use that understanding to improve CNN performance.},
	urldate = {2022-10-08},
	publisher = {arXiv},
	author = {Ju, Yilong and Alam, Shah Saad and Minoff, Jonathan and Anselmi, Fabio and Pu, Han and Patel, Ankit},
	month = oct,
	year = {2022},
	note = {arXiv:2210.00692 [cond-mat, physics:physics, physics:quant-ph]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Strongly Correlated Electrons, Physics - Computational Physics, Quantum Physics},
}

@article{zaidi_information_2020,
	title = {On the {Information} {Bottleneck} {Problems}: {Models}, {Connections}, {Applications} and {Information} {Theoretic} {Views}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	shorttitle = {On the {Information} {Bottleneck} {Problems}},
	url = {https://www.mdpi.com/1099-4300/22/2/151},
	doi = {10.3390/e22020151},
	abstract = {This tutorial paper focuses on the variants of the bottleneck problem taking an information theoretic perspective and discusses practical methods to solve it, as well as its connection to coding and learning aspects. The intimate connections of this setting to remote source-coding under logarithmic loss distortion measure, information combining, common reconstruction, the Wyner–Ahlswede–Korner problem, the efficiency of investment information, as well as, generalization, variational inference, representation learning, autoencoders, and others are highlighted. We discuss its extension to the distributed information bottleneck problem with emphasis on the Gaussian model and highlight the basic connections to the uplink Cloud Radio Access Networks (CRAN) with oblivious processing. For this model, the optimal trade-offs between relevance (i.e., information) and complexity (i.e., rates) in the discrete and vector Gaussian frameworks is determined. In the concluding outlook, some interesting problems are mentioned such as the characterization of the optimal inputs (“features”) distributions under power limitations maximizing the “relevance” for the Gaussian information bottleneck, under “complexity” constraints.},
	language = {en},
	number = {2},
	urldate = {2022-10-06},
	journal = {Entropy},
	author = {Zaidi, Abdellatif and Estella-Aguerri, Iñaki and Shamai (Shitz), Shlomo},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {information bottleneck, logarithmic loss, rate distortion theory, representation learning},
	pages = {151},
}

@misc{dawid_modern_2022-1,
	title = {Modern applications of machine learning in quantum sciences},
	url = {http://arxiv.org/abs/2204.04198},
	doi = {10.48550/arXiv.2204.04198},
	abstract = {In these Lecture Notes, we provide a comprehensive introduction to the most recent advances in the application of machine learning methods in quantum sciences. We cover the use of deep learning and kernel methods in supervised, unsupervised, and reinforcement learning algorithms for phase classification, representation of many-body quantum states, quantum feedback control, and quantum circuits optimization. Moreover, we introduce and discuss more specialized topics such as differentiable programming, generative models, statistical approach to machine learning, and quantum machine learning.},
	urldate = {2022-09-29},
	publisher = {arXiv},
	author = {Dawid, Anna and Arnold, Julian and Requena, Borja and Gresch, Alexander and Płodzień, Marcin and Donatella, Kaelan and Nicoli, Kim A. and Stornati, Paolo and Koch, Rouven and Büttner, Miriam and Okuła, Robert and Muñoz-Gil, Gorka and Vargas-Hernández, Rodrigo A. and Cervera-Lierta, Alba and Carrasquilla, Juan and Dunjko, Vedran and Gabrié, Marylou and Huembeli, Patrick and van Nieuwenburg, Evert and Vicentini, Filippo and Wang, Lei and Wetzel, Sebastian J. and Carleo, Giuseppe and Greplová, Eliška and Krems, Roman and Marquardt, Florian and Tomza, Michał and Lewenstein, Maciej and Dauphin, Alexandre},
	month = jun,
	year = {2022},
	note = {arXiv:2204.04198 [cond-mat, physics:quant-ph]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Mesoscale and Nanoscale Physics, Quantum Physics},
}

@article{hermann_deep_2020,
	title = {Deep neural network solution of the electronic {Schr}{\textbackslash}"odinger equation},
	volume = {12},
	issn = {1755-4330, 1755-4349},
	url = {http://arxiv.org/abs/1909.08423},
	doi = {10.1038/s41557-020-0544-y},
	abstract = {[New and updated results were published in Nature Chemistry, doi:10.1038/s41557-020-0544-y.] The electronic Schr{\textbackslash}"odinger equation describes fundamental properties of molecules and materials, but can only be solved analytically for the hydrogen atom. The numerically exact full configuration-interaction method is exponentially expensive in the number of electrons. Quantum Monte Carlo is a possible way out: it scales well to large molecules, can be parallelized, and its accuracy has, as yet, only been limited by the flexibility of the used wave function ansatz. Here we propose PauliNet, a deep-learning wave function ansatz that achieves nearly exact solutions of the electronic Schr{\textbackslash}"odinger equation. PauliNet has a multireference Hartree-Fock solution built in as a baseline, incorporates the physics of valid wave functions, and is trained using variational quantum Monte Carlo (VMC). PauliNet outperforms comparable state-of-the-art VMC ansatzes for atoms, diatomic molecules and a strongly-correlated hydrogen chain by a margin and is yet computationally efficient. We anticipate that thanks to the favourable scaling with system size, this method may become a new leading method for highly accurate electronic-strucutre calculations on medium-sized molecular systems.},
	number = {10},
	urldate = {2022-09-28},
	journal = {Nature Chemistry},
	author = {Hermann, Jan and Schätzle, Zeno and Noé, Frank},
	month = oct,
	year = {2020},
	note = {arXiv:1909.08423 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Computational Physics, Statistics - Machine Learning},
	pages = {891--897},
}

@article{vicentini_netket_2022,
	title = {{NetKet} 3: {Machine} {Learning} {Toolbox} for {Many}-{Body} {Quantum} {Systems}},
	shorttitle = {{NetKet} 3},
	url = {http://arxiv.org/abs/2112.10526},
	doi = {10.21468/SciPostPhysCodeb.7},
	abstract = {We introduce version 3 of NetKet, the machine learning toolbox for many-body quantum physics. NetKet is built around neural-network quantum states and provides efficient algorithms for their evaluation and optimization. This new version is built on top of JAX, a differentiable programming and accelerated linear algebra framework for the Python programming language. The most significant new feature is the possibility to define arbitrary neural network ans{\textbackslash}"atze in pure Python code using the concise notation of machine-learning frameworks, which allows for just-in-time compilation as well as the implicit generation of gradients thanks to automatic differentiation. NetKet 3 also comes with support for GPU and TPU accelerators, advanced support for discrete symmetry groups, chunking to scale up to thousands of degrees of freedom, drivers for quantum dynamics applications, and improved modularity, allowing users to use only parts of the toolbox as a foundation for their own code.},
	urldate = {2022-09-28},
	journal = {SciPost Physics Codebases},
	author = {Vicentini, Filippo and Hofmann, Damian and Szabó, Attila and Wu, Dian and Roth, Christopher and Giuliani, Clemens and Pescia, Gabriel and Nys, Jannes and Vargas-Calderon, Vladimir and Astrakhantsev, Nikita and Carleo, Giuseppe},
	month = aug,
	year = {2022},
	note = {arXiv:2112.10526 [physics, physics:quant-ph]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Physics - Computational Physics, Quantum Physics},
	pages = {7},
}

@misc{huang_provably_2022,
	title = {Provably efficient machine learning for quantum many-body problems},
	url = {http://arxiv.org/abs/2106.12627},
	doi = {10.48550/arXiv.2106.12627},
	abstract = {Classical machine learning (ML) provides a potentially powerful approach to solving challenging quantum many-body problems in physics and chemistry. However, the advantages of ML over more traditional methods have not been firmly established. In this work, we prove that classical ML algorithms can efficiently predict ground state properties of gapped Hamiltonians in finite spatial dimensions, after learning from data obtained by measuring other Hamiltonians in the same quantum phase of matter. In contrast, under widely accepted complexity theory assumptions, classical algorithms that do not learn from data cannot achieve the same guarantee. We also prove that classical ML algorithms can efficiently classify a wide range of quantum phases of matter. Our arguments are based on the concept of a classical shadow, a succinct classical description of a many-body quantum state that can be constructed in feasible quantum experiments and be used to predict many properties of the state. Extensive numerical experiments corroborate our theoretical results in a variety of scenarios, including Rydberg atom systems, 2D random Heisenberg models, symmetry-protected topological phases, and topologically ordered phases.},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Huang, Hsin-Yuan and Kueng, Richard and Torlai, Giacomo and Albert, Victor V. and Preskill, John},
	month = feb,
	year = {2022},
	note = {arXiv:2106.12627 [quant-ph]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Quantum Physics},
}

@article{barbier_adaptive_2019,
	title = {The adaptive interpolation method for proving replica formulas. {Applications} to the {Curie}–{Weiss} and {Wigner} spike models},
	volume = {52},
	issn = {1751-8121},
	url = {https://doi.org/10.1088/1751-8121/ab2735},
	doi = {10.1088/1751-8121/ab2735},
	abstract = {In this contribution we give a pedagogic introduction to the newly introduced adaptive interpolation method to prove in a simple and unified way replica formulas for Bayesian optimal inference problems. Many aspects of this method can already be explained at the level of the simple Curie–Weiss spin system. This provides a new method of solution for this model which does not appear to be known. We then generalize this analysis to a paradigmatic inference problem, namely rank-one matrix estimation, also refered to as the Wigner spike model in statistics. We give many pointers to the recent literature where the method has been succesfully applied.},
	language = {en},
	number = {29},
	urldate = {2022-09-21},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Barbier, Jean and Macris, Nicolas},
	month = jun,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {294002},
}

@misc{lelarge_fundamental_2017,
	title = {Fundamental limits of symmetric low-rank matrix estimation},
	url = {http://arxiv.org/abs/1611.03888},
	doi = {10.48550/arXiv.1611.03888},
	abstract = {We consider the high-dimensional inference problem where the signal is a low-rank symmetric matrix which is corrupted by an additive Gaussian noise. Given a probabilistic model for the low-rank matrix, we compute the limit in the large dimension setting for the mutual information between the signal and the observations, as well as the matrix minimum mean square error, while the rank of the signal remains constant. We also show that our model extends beyond the particular case of additive Gaussian noise and we prove an universality result connecting the community detection problem to our Gaussian framework. We unify and generalize a number of recent works on PCA, sparse PCA, submatrix localization or community detection by computing the information-theoretic limits for these problems in the high noise regime. In addition, we show that the posterior distribution of the signal given the observations is characterized by a parameter of the same dimension as the square of the rank of the signal (i.e. scalar in the case of rank one). Finally, we connect our work with the hard but detectable conjecture in statistical physics.},
	urldate = {2022-09-21},
	publisher = {arXiv},
	author = {Lelarge, Marc and Miolane, Léo},
	month = mar,
	year = {2017},
	note = {arXiv:1611.03888 [math]},
	keywords = {Mathematics - Probability},
}

@inproceedings{alaoui_estimation_2018,
	title = {Estimation in the spiked {Wigner} model: {A} short proof of the replica formula},
	shorttitle = {Estimation in the spiked {Wigner} model},
	url = {http://arxiv.org/abs/1801.01593},
	doi = {10.1109/ISIT.2018.8437810},
	abstract = {We consider the problem of estimating a rank-one perturbation of a Wigner matrix in a setting of low signal-to-noise ratio. This serves as a simple model for principal component analysis in high dimensions. The mutual information per variable between the spike and the observed matrix, or equivalently, the normalized Kullback-Leibler divergence between the planted and null models are known to converge to the so-called \{{\textbackslash}em replica-symmetric\} formula, the properties of which determine the fundamental limits of estimation in this model. We provide in this note a short and transparent proof of this formula, based on simple executions of Gaussian interpolations and standard concentration-of-measure arguments. The {\textbackslash}emph\{Franz-Parisi potential\}, that is, the free entropy at a fixed overlap, plays an important role in our proof. Furthermore, our proof can be generalized straightforwardly to spiked tensor models of even order.},
	urldate = {2022-09-21},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Alaoui, Ahmed El and Krzakala, Florent},
	month = jun,
	year = {2018},
	note = {arXiv:1801.01593 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory},
	pages = {1874--1878},
}

@misc{barbier_mutual_2016,
	title = {Mutual information for symmetric rank-one matrix estimation: {A} proof of the replica formula},
	shorttitle = {Mutual information for symmetric rank-one matrix estimation},
	url = {http://arxiv.org/abs/1606.04142},
	doi = {10.48550/arXiv.1606.04142},
	abstract = {Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.},
	urldate = {2022-09-21},
	publisher = {arXiv},
	author = {Barbier, Jean and Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborova, Lenka},
	month = jun,
	year = {2016},
	note = {arXiv:1606.04142 [cond-mat, physics:math-ph]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Mathematical Physics},
}

@inproceedings{krzakala_mutual_2016,
	title = {Mutual {Information} in {Rank}-{One} {Matrix} {Estimation}},
	url = {http://arxiv.org/abs/1603.08447},
	doi = {10.1109/ITW.2016.7606798},
	abstract = {We consider the estimation of a n-dimensional vector x from the knowledge of noisy and possibility non-linear element-wise measurements of xxT , a very generic problem that contains, e.g. stochastic 2-block model, submatrix localization or the spike perturbation of random matrices. We use an interpolation method proposed by Guerra and later refined by Korada and Macris. We prove that the Bethe mutual information (related to the Bethe free energy and conjectured to be exact by Lesieur et al. on the basis of the non-rigorous cavity method) always yields an upper bound to the exact mutual information. We also provide a lower bound using a similar technique. For concreteness, we illustrate our findings on the sparse PCA problem, and observe that (a) our bounds match for a large region of parameters and (b) that it exists a phase transition in a region where the spectum remains uninformative. While we present only the case of rank-one symmetric matrix estimation, our proof technique is readily extendable to low-rank symmetric matrix or low-rank symmetric tensor estimation},
	urldate = {2022-09-21},
	booktitle = {2016 {IEEE} {Information} {Theory} {Workshop} ({ITW})},
	author = {Krzakala, Florent and Xu, Jiaming and Zdeborová, Lenka},
	month = sep,
	year = {2016},
	note = {arXiv:1603.08447 [cond-mat, stat]},
	keywords = {Computer Science - Information Theory, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Statistics Theory},
	pages = {71--75},
}

@misc{pourkamali_mismatched_2021,
	title = {Mismatched {Estimation} of rank-one symmetric matrices under {Gaussian} noise},
	url = {http://arxiv.org/abs/2107.08927},
	doi = {10.48550/arXiv.2107.08927},
	abstract = {We consider the estimation of an n-dimensional vector s from the noisy element-wise measurements of \${\textbackslash}mathbf\{s\}{\textbackslash}mathbf\{s\}{\textasciicircum}T\$, a generic problem that arises in statistics and machine learning. We study a mismatched Bayesian inference setting, where some of the parameters are not known to the statistician. We derive the full exact analytic expression of the asymptotic mean squared error (MSE) in the large system size limit for the particular case of Gaussian priors and additive noise. From our formulas, we see that estimation is still possible in the mismatched case; and also that the minimum MSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our technique relies on the asymptotics of the spherical integrals and can be applied as long as the statistician chooses a rotationally invariant prior.},
	urldate = {2022-09-14},
	publisher = {arXiv},
	author = {Pourkamali, Farzad and Macris, Nicolas},
	month = sep,
	year = {2021},
	note = {arXiv:2107.08927 [cs, math]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Information Theory},
}

@article{han_solving_2019,
	title = {Solving many-electron {Schrödinger} equation using deep neural networks},
	volume = {399},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999119306345},
	doi = {10.1016/j.jcp.2019.108929},
	abstract = {We introduce a new family of trial wave-functions based on deep neural networks to solve the many-electron Schro¨dinger equation. The Pauli exclusion principle is dealt with explicitly to ensure that the trial wave-functions are physical. The optimal trial wave-function is obtained through variational Monte Carlo and the computational cost scales quadratically with the number of electrons. The algorithm does not make use of any prior knowledge such as atomic orbitals. Yet it is able to represent accurately the groundstates of the tested systems, including He, H2, Be, B, LiH, and a chain of 10 hydrogen atoms. This opens up new possibilities for solving large-scale many-electron Schro¨dinger equation.},
	language = {en},
	urldate = {2022-09-11},
	journal = {Journal of Computational Physics},
	author = {Han, Jiequn and Zhang, Linfeng and E, Weinan},
	month = dec,
	year = {2019},
	pages = {108929},
}

@article{karniadakis_physics-informed_2021,
	title = {Physics-informed machine learning},
	volume = {3},
	issn = {2522-5820},
	url = {http://www.nature.com/articles/s42254-021-00314-5},
	doi = {10.1038/s42254-021-00314-5},
	abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-d imensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-t ime domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-b ased regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-d imensional problems.},
	language = {en},
	number = {6},
	urldate = {2022-09-11},
	journal = {Nature Reviews Physics},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = jun,
	year = {2021},
	pages = {422--440},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2022-09-11},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@misc{kovachki_universal_2021,
	title = {On universal approximation and error bounds for {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2107.07562},
	doi = {10.48550/arXiv.2107.07562},
	abstract = {Fourier neural operators (FNOs) have recently been proposed as an effective framework for learning operators that map between infinite-dimensional spaces. We prove that FNOs are universal, in the sense that they can approximate any continuous operator to desired accuracy. Moreover, we suggest a mechanism by which FNOs can approximate operators associated with PDEs efficiently. Explicit error bounds are derived to show that the size of the FNO, approximating operators associated with a Darcy type elliptic PDE and with the incompressible Navier-Stokes equations of fluid dynamics, only increases sub (log)-linearly in terms of the reciprocal of the error. Thus, FNOs are shown to efficiently approximate operators arising in a large class of PDEs.},
	urldate = {2022-09-11},
	publisher = {arXiv},
	author = {Kovachki, Nikola and Lanthaler, Samuel and Mishra, Siddhartha},
	month = jul,
	year = {2021},
	note = {arXiv:2107.07562 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@misc{lanthaler_error_2022,
	title = {Error estimates for {DeepOnets}: {A} deep learning framework in infinite dimensions},
	shorttitle = {Error estimates for {DeepOnets}},
	url = {http://arxiv.org/abs/2102.09618},
	doi = {10.48550/arXiv.2102.09618},
	abstract = {DeepONets have recently been proposed as a framework for learning nonlinear operators mapping between infinite dimensional Banach spaces. We analyze DeepONets and prove estimates on the resulting approximation and generalization errors. In particular, we extend the universal approximation property of DeepONets to include measurable mappings in non-compact spaces. By a decomposition of the error into encoding, approximation and reconstruction errors, we prove both lower and upper bounds on the total error, relating it to the spectral decay properties of the covariance operators, associated with the underlying measures. We derive almost optimal error bounds with very general affine reconstructors and with random sensor locations as well as bounds on the generalization error, using covering number arguments. We illustrate our general framework with four prototypical examples of nonlinear operators, namely those arising in a nonlinear forced ODE, an elliptic PDE with variable coefficients and nonlinear parabolic and hyperbolic PDEs. While the approximation of arbitrary Lipschitz operators by DeepONets to accuracy \${\textbackslash}epsilon\$ is argued to suffer from a "curse of dimensionality" (requiring a neural networks of exponential size in \$1/{\textbackslash}epsilon\$), in contrast, for all the above concrete examples of interest, we rigorously prove that DeepONets can break this curse of dimensionality (achieving accuracy \${\textbackslash}epsilon\$ with neural networks of size that can grow algebraically in \$1/{\textbackslash}epsilon\$). Thus, we demonstrate the efficient approximation of a potentially large class of operators with this machine learning framework.},
	urldate = {2022-09-11},
	publisher = {arXiv},
	author = {Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George Em},
	month = jan,
	year = {2022},
	note = {arXiv:2102.09618 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}

@misc{ananthaswamy_latest_2021,
	title = {Latest {Neural} {Nets} {Solve} {World}’s {Hardest} {Equations} {Faster} {Than} {Ever} {Before}},
	url = {https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/},
	abstract = {Two new approaches allow deep neural networks to solve entire families of partial differential equations, making it easier to model complicated systems and to do so orders of magnitude faster.},
	language = {en},
	urldate = {2022-09-11},
	journal = {Quanta Magazine},
	author = {Ananthaswamy, Anil},
	month = apr,
	year = {2021},
}

@inproceedings{bassily_local_2015,
	title = {Local, {Private}, {Efficient} {Protocols} for {Succinct} {Histograms}},
	url = {http://arxiv.org/abs/1504.04686},
	doi = {10.1145/2746539.2746632},
	abstract = {We give efficient protocols and matching accuracy lower bounds for frequency estimation in the local model for differential privacy. In this model, individual users randomize their data themselves, sending differentially private reports to an untrusted server that aggregates them. We study protocols that produce a succinct histogram representation of the data. A succinct histogram is a list of the most frequent items in the data (often called "heavy hitters") along with estimates of their frequencies; the frequency of all other items is implicitly estimated as 0. If there are \$n\$ users whose items come from a universe of size \$d\$, our protocols run in time polynomial in \$n\$ and \${\textbackslash}log(d)\$. With high probability, they estimate the accuracy of every item up to error \$O{\textbackslash}left({\textbackslash}sqrt\{{\textbackslash}log(d)/({\textbackslash}epsilon{\textasciicircum}2n)\}{\textbackslash}right)\$ where \${\textbackslash}epsilon\$ is the privacy parameter. Moreover, we show that this much error is necessary, regardless of computational efficiency, and even for the simple setting where only one item appears with significant frequency in the data set. Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for this task either ran in time \${\textbackslash}Omega(d)\$ or had much worse error (about \${\textbackslash}sqrt[6]\{{\textbackslash}log(d)/({\textbackslash}epsilon{\textasciicircum}2n)\}\$), and the only known lower bound on error was \${\textbackslash}Omega(1/{\textbackslash}sqrt\{n\})\$. We also adapt a result of McGregor et al (2010) to the local setting. In a model with public coins, we show that each user need only send 1 bit to the server. For all known local protocols (including ours), the transformation preserves computational efficiency.},
	urldate = {2022-08-16},
	booktitle = {Proceedings of the forty-seventh annual {ACM} symposium on {Theory} of {Computing}},
	author = {Bassily, Raef and Smith, Adam},
	month = jun,
	year = {2015},
	note = {arXiv:1504.04686 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, F.2.0},
	pages = {127--135},
}

@misc{mei_generalization_2020,
	title = {The generalization error of random features regression: {Precise} asymptotics and double descent curve},
	shorttitle = {The generalization error of random features regression},
	url = {http://arxiv.org/abs/1908.05355},
	doi = {10.48550/arXiv.1908.05355},
	abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. Neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the test error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the test error is found above the interpolation threshold, often in the extreme overparametrization regime in which the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the \$d\$-dimensional sphere \${\textbackslash}mathbb S{\textasciicircum}\{d-1\}\$, from \$n\$ i.i.d. samples \$({\textbackslash}boldsymbol x\_i, y\_i){\textbackslash}in {\textbackslash}mathbb S{\textasciicircum}\{d-1\} {\textbackslash}times {\textbackslash}mathbb R\$, \$i{\textbackslash}le n\$. We perform ridge regression on \$N\$ random features of the form \${\textbackslash}sigma({\textbackslash}boldsymbol w\_a{\textasciicircum}\{{\textbackslash}mathsf T\} {\textbackslash}boldsymbol x)\$, \$a{\textbackslash}le N\$. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the test error, in the limit \$N,n,d{\textbackslash}to {\textbackslash}infty\$ with \$N/d\$ and \$n/d\$ fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Mei, Song and Montanari, Andrea},
	month = dec,
	year = {2020},
	note = {arXiv:1908.05355 [math, stat]},
	keywords = {62J99, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{bodin_model_2021,
	title = {Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model},
	shorttitle = {Model, sample, and epoch-wise descents},
	url = {http://arxiv.org/abs/2110.11805},
	doi = {10.48550/arXiv.2110.11805},
	abstract = {Recent evidence has shown the existence of a so-called double-descent and even triple-descent behavior for the generalization error of deep-learning models. This important phenomenon commonly appears in implemented neural network architectures, and also seems to emerge in epoch-wise curves during the training process. A recent line of research has highlighted that random matrix tools can be used to obtain precise analytical asymptotics of the generalization (and training) errors of the random feature model. In this contribution, we analyze the whole temporal behavior of the generalization and training errors under gradient flow for the random feature model. We show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. This allows us to observe how the double and triple descents develop over time, if and when early stopping is an option, and also observe time-wise descent structures. Our techniques are based on Cauchy complex integral representations of the errors together with recent random matrix methods based on linear pencils.},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Bodin, Antoine and Macris, Nicolas},
	month = oct,
	year = {2021},
	note = {arXiv:2110.11805 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{ingster_nonparametric_2003,
	title = {Nonparametric {Goodness}-of-{Fit} {Testing} {Under} {Gaussian} {Models}},
	volume = {169},
	isbn = {978-0-387-95531-5},
	publisher = {Springer New York},
	author = {Ingster, Yu I. and Suslina, Irina A.},
	year = {2003},
	doi = {10.1007/978-0-387-21580-8},
}

@article{chen_general_2016,
	title = {A general decision theory for {Huber}'s ϵ-contamination model},
	volume = {10},
	url = {https://doi.org/10.1214/16-EJS1216},
	doi = {10.1214/16-EJS1216},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Chen, Mengjie and Gao, Chao and Ren, Zhao},
	year = {2016},
	mrnumber = {3579675},
	pages = {3752--3774},
}

@article{ingster_detection_2010,
	title = {Detection boundary in sparse regression},
	volume = {4},
	url = {https://doi.org/10.1214/10-EJS589},
	doi = {10.1214/10-EJS589},
	journal = {Electronic Journal of Statistics},
	author = {Ingster, Yuri I. and Tsybakov, Alexandre B. and Verzelen, Nicolas},
	year = {2010},
	mrnumber = {2747131},
	pages = {1476--1526},
}

@misc{wu_statistical_2018,
	title = {Statistical {Problems} with {Planted} {Structures}: {Information}-{Theoretical} and {Computational} {Limits}},
	author = {Wu, Yihong and Xu, Jiaming},
	year = {2018},
	note = {\_eprint: arXiv: 1806.00118},
}

@article{carpentier_minimax_2019,
	title = {Minimax {Rate} of {Testing} in {Sparse} {Linear} {Regression}},
	volume = {80},
	issn = {0005-1179},
	url = {https://doi.org/10.1134/S0005117919100047},
	number = {10},
	journal = {Autom. Remote Control},
	author = {Carpentier, A. and Collier, O. and Comminges, L. and Tsybakov, A. B. and Wang, Yu.},
	month = oct,
	year = {2019},
	note = {Place: USA
Publisher: Plenum Press},
	pages = {1817--1834},
}

@article{berthet_optimal_2013,
	title = {Optimal detection of sparse principal components in high dimension},
	volume = {41},
	issn = {0090-5364},
	url = {https://doi.org/10.1214/13-AOS1127},
	doi = {10.1214/13-AOS1127},
	number = {4},
	journal = {The Annals of Statistics},
	author = {Berthet, Quentin and Rigollet, Philippe},
	year = {2013},
	mrnumber = {3127849},
	pages = {1780--1815},
}

@incollection{tukey_survey_1960,
	title = {A survey of sampling from contaminated distributions},
	booktitle = {Contributions to probability and statistics},
	publisher = {Stanford Univ. Press, Stanford, Calif.},
	author = {Tukey, John W.},
	year = {1960},
	mrnumber = {0120720},
	pages = {448--485},
}

@article{baraud_non-asymptotic_2002,
	title = {Non-asymptotic minimax rates of testing in signal detection},
	volume = {8},
	issn = {1350-7265},
	number = {5},
	journal = {Bernoulli. Official Journal of the Bernoulli Society for Mathematical Statistics and Probability},
	author = {Baraud, Yannick},
	year = {2002},
	mrnumber = {1935648},
	pages = {577--606},
}

@article{verzelen_minimax_2012,
	title = {Minimax risks for sparse regressions: ultra-high dimensional phenomenons},
	volume = {6},
	url = {https://doi.org/10.1214/12-EJS666},
	doi = {10.1214/12-EJS666},
	journal = {Electronic Journal of Statistics},
	author = {Verzelen, Nicolas},
	year = {2012},
	mrnumber = {2879672},
	pages = {38--90},
}

@inproceedings{liu_high_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {High {Dimensional} {Robust} {Sparse} {Regression}},
	volume = {108},
	url = {https://proceedings.mlr.press/v108/liu20b.html},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Liu, Liu and Shen, Yanyao and Li, Tianyang and Caramanis, Constantine},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	month = aug,
	year = {2020},
	pages = {411--421},
}

@inproceedings{reeves_all-or-nothing_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {All}-or-{Nothing} {Phenomenon} in {Sparse} {Linear} {Regression}},
	volume = {99},
	url = {https://proceedings.mlr.press/v99/reeves19a.html},
	booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Reeves, Galen and Xu, Jiaming and Zadik, Ilias},
	editor = {Beygelzimer, Alina and Hsu, Daniel},
	month = jun,
	year = {2019},
	pages = {2652--2663},
}

@article{cai_statistical_2020,
	title = {Statistical and computational limits for sparse matrix detection},
	volume = {48},
	issn = {0090-5364},
	url = {https://doi.org/10.1214/19-AOS1860},
	doi = {10.1214/19-AOS1860},
	number = {3},
	journal = {The Annals of Statistics},
	author = {Cai, T. Tony and Wu, Yihong},
	year = {2020},
	mrnumber = {4124336},
	pages = {1593--1614},
}

@inproceedings{diakonikolas_outlier-robust_2019,
	title = {Outlier-{Robust} {High}-{Dimensional} {Sparse} {Estimation} via {Iterative} {Filtering}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/99607461cdb9c26e2bd5f31b12dcf27a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Diakonikolas, Ilias and Kane, Daniel and Karmalkar, Sushrut and Price, Eric and Stewart, Alistair},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{balakrishnan_computationally_2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Computationally {Efficient} {Robust} {Sparse} {Estimation} in {High} {Dimensions}},
	volume = {65},
	booktitle = {Proceedings of the 2017 {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Balakrishnan, Sivaraman and Du, Simon S. and Li, Jerry and Singh, Aarti},
	editor = {Kale, Satyen and Shamir, Ohad},
	month = jul,
	year = {2017},
	pages = {169--212},
}

@incollection{lai_agnostic_2016,
	title = {Agnostic estimation of mean and covariance},
	booktitle = {57th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}—{FOCS} 2016},
	publisher = {IEEE Computer Soc., Los Alamitos, CA},
	author = {Lai, Kevin A. and Rao, Anup B. and Vempala, Santosh},
	year = {2016},
	mrnumber = {3631029},
	pages = {665--674},
}

@incollection{diakonikolas_robust_2016,
	title = {Robust estimators in high dimensions without the computational intractability},
	url = {https://doi.org/10.1109/FOCS.2016.85},
	booktitle = {57th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}—{FOCS} 2016},
	publisher = {IEEE Computer Soc., Los Alamitos, CA},
	author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M. and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
	year = {2016},
	mrnumber = {3631028},
	doi = {10.1109/FOCS.2016.85},
	pages = {655--664},
}

@phdthesis{li_principled_2018,
	type = {{PhD} {Thesis}},
	title = {Principled {Approaches} to {Robust} {Machine} {Learning} and {Beyond}},
	school = {MIT},
	author = {Li, Jerry},
	year = {2018},
}

@article{huber_robust_1964,
	title = {Robust {Estimation} of a {Location} {Parameter}},
	volume = {35},
	url = {https://doi.org/10.1214/aoms/1177703732},
	doi = {10.1214/aoms/1177703732},
	number = {1},
	journal = {The Annals of Mathematical Statistics},
	author = {Huber, Peter J.},
	year = {1964},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {73 -- 101},
}

@book{canonne_survey_2020,
	series = {Graduate {Surveys}},
	title = {A {Survey} on {Distribution} {Testing}: {Your} {Data} is {Big}. {But} is it {Blue}?},
	url = {http://www.theoryofcomputing.org/library.html},
	number = {9},
	publisher = {Theory of Computing Library},
	author = {Canonne, Clément L.},
	year = {2020},
	doi = {10.4086/toc.gs.2020.009},
}

@misc{diakonikolas_recent_2019,
	title = {Recent {Advances} in {Algorithmic} {High}-{Dimensional} {Robust} {Statistics}},
	author = {Diakonikolas, Ilias and Kane, Daniel M.},
	year = {2019},
	note = {\_eprint: arXiv: 1911.05911},
}

@misc{hajek_computational_2015,
	title = {Computational {Lower} {Bounds} for {Community} {Detection} on {Random} {Graphs}},
	author = {Hajek, Bruce and Wu, Yihong and Xu, Jiaming},
	year = {2015},
	note = {\_eprint: 1406.6625},
}

@misc{wu_lecture_2020,
	title = {Lecture notes for {ECE598YW}: {Information}-theoretic methods for high-dimensional statistics},
	author = {Wu, Yihong},
	year = {2020},
}

@article{collier_minimax_2017,
	title = {Minimax estimation of linear and quadratic functionals on sparsity classes},
	volume = {45},
	issn = {0090-5364},
	url = {https://doi.org/10.1214/15-AOS1432},
	doi = {10.1214/15-AOS1432},
	number = {3},
	journal = {The Annals of Statistics},
	author = {Collier, Olivier and Comminges, Laëtitia and Tsybakov, Alexandre B.},
	year = {2017},
	mrnumber = {3662444},
	pages = {923--958},
}

@inproceedings{brennan_reducibility_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Reducibility and {Statistical}-{Computational} {Gaps} from {Secret} {Leakage}},
	volume = {125},
	url = {https://proceedings.mlr.press/v125/brennan20a.html},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Brennan, Matthew and Bresler, Guy},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	month = jul,
	year = {2020},
	pages = {648--847},
}

@inproceedings{diakonikolas_sample_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {The {Sample} {Complexity} of {Robust} {Covariance} {Testing}},
	volume = {134},
	url = {https://proceedings.mlr.press/v134/diakonikolas21a.html},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Diakonikolas, Ilias and Kane, Daniel M.},
	editor = {Belkin, Mikhail and Kpotufe, Samory},
	month = aug,
	year = {2021},
	pages = {1511--1521},
}

@misc{li_robust_2017,
	title = {Robust {Sparse} {Estimation} {Tasks} in {High} {Dimensions}},
	author = {Li, Jerry},
	year = {2017},
	note = {\_eprint: arXiv: 1702.05860},
}

@article{cai_optimal_2015,
	title = {Optimal estimation and rank detection for sparse spiked covariance matrices},
	volume = {161},
	issn = {0178-8051},
	url = {https://doi.org/10.1007/s00440-014-0562-z},
	doi = {10.1007/s00440-014-0562-z},
	number = {3-4},
	journal = {Probability Theory and Related Fields},
	author = {Cai, Tony and Ma, Zongming and Wu, Yihong},
	year = {2015},
	mrnumber = {3334281},
	pages = {781--815},
}

@incollection{diakonikolas_statistical_2017,
	title = {Statistical query lower bounds for robust estimation of high-dimensional {Gaussians} and {Gaussian} mixtures (extended abstract)},
	url = {https://doi.org/10.1109/FOCS.2017.16},
	booktitle = {58th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}—{FOCS} 2017},
	publisher = {IEEE Computer Soc., Los Alamitos, CA},
	author = {Diakonikolas, Ilias and Kane, Daniel M. and Stewart, Alistair},
	year = {2017},
	mrnumber = {3734219},
	doi = {10.1109/FOCS.2017.16},
	pages = {73--84},
}

@inproceedings{levy_learning_2021,
	title = {Learning with {User}-{Level} {Privacy}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/67e235e7f2fa8800d8375409b566e6b6-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Levy, Daniel and Sun, Ziteng and Amin, Kareem and Kale, Satyen and Kulesza, Alex and Mohri, Mehryar and Suresh, Ananda Theertha},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {12466--12479},
}

@article{kairouz_composition_2017,
	title = {The composition theorem for differential privacy},
	volume = {63},
	issn = {0018-9448},
	url = {https://doi.org/10.1109/TIT.2017.2685505},
	doi = {10.1109/TIT.2017.2685505},
	number = {6},
	journal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
	author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
	year = {2017},
	mrnumber = {3677761},
	pages = {4037--4049},
}

@inproceedings{feldman_generalization_2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Generalization for {Adaptively}-chosen {Estimators} via {Stable} {Median}},
	volume = {65},
	url = {https://proceedings.mlr.press/v65/feldman17a.html},
	booktitle = {Proceedings of the 2017 {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Feldman, Vitaly and Steinke, Thomas},
	editor = {Kale, Satyen and Shamir, Ohad},
	month = jul,
	year = {2017},
	pages = {728--757},
}

@incollection{hubert_chan_private_2010,
	series = {Lecture {Notes} in {Comput}. {Sci}.},
	title = {Private and continual release of statistics},
	volume = {6199},
	url = {https://doi.org/10.1007/978-3-642-14162-1_34},
	booktitle = {Automata, languages and programming. {Part} {II}},
	publisher = {Springer, Berlin},
	author = {Hubert Chan, T.-H. and Shi, Elaine and Song, Dawn},
	year = {2010},
	mrnumber = {2734664},
	doi = {10.1007/978-3-642-14162-1_34},
	pages = {405--417},
}

@inproceedings{dwork_differential_2010,
	title = {Differential privacy under continual observation},
	booktitle = {{STOC}'10—{Proceedings} of the 2010 {ACM} {International} {Symposium} on {Theory} of {Computing}},
	publisher = {ACM, New York},
	author = {Dwork, Cynthia and Naor, Moni and Pitassi, Toniann and Rothblum, Guy N.},
	year = {2010},
	mrnumber = {2743321},
	pages = {715--724},
}

@article{roberts_weak_1997,
	title = {Weak convergence and optimal scaling of random walk {Metropolis} algorithms},
	volume = {7},
	number = {1},
	journal = {Ann. Appl. Probab.},
	author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
	year = {1997},
	note = {Publisher: The Institute of Mathematical Statistics},
	pages = {110--120},
}

@inproceedings{gentry_trapdoors_2008,
	title = {Trapdoors for {Hard} {Lattices} and {New} {Cryptographic} {Constructions}},
	isbn = {978-1-60558-047-0},
	booktitle = {Proc. 40th {Annu}. {ACM} {Symp}. {Theory} {Comput}.},
	author = {Gentry, C. and Peikert, C. and Vaikuntanathan, V.},
	year = {2008},
	pages = {197--206},
}

@article{vatedka_secure_2015,
	title = {Secure {Compute}-and-{Forward} in a {Bidirectional} {Relay}},
	volume = {61},
	number = {5},
	journal = {IEEE Trans. Inf. Theory},
	author = {Vatedka, S. and Kashyap, N. and Thangaraj, A.},
	year = {2015},
	pages = {2531--2556},
}

@inproceedings{aggarwal_solving_2015,
	title = {Solving the {Shortest} {Vector} {Problem} in 2{\textasciicircum}n {Time} {Using} {Discrete} {Gaussian} {Sampling}: {Extended} {Abstract}},
	isbn = {978-1-4503-3536-2},
	booktitle = {Proc. {STOC}},
	author = {Aggarwal, D. and Dadush, D. and Regev, O. and Stephens-Davidowitz, N.},
	year = {2015},
	pages = {733--742},
}

@phdthesis{anaswara_sampling_2020,
	title = {Sampling {From} {Multidimensional} {Distributions} {Supported} {On} {A} {Lattice}},
	school = {Indian Institute of Science},
	author = {Anaswara, S.},
	year = {2020},
}

@book{conway_sphere_1999,
	edition = {3},
	title = {Sphere {Packings}, {Lattices} and {Groups}},
	publisher = {Springer-Verlag},
	author = {Conway, J. H. and Sloane, N. J. A.},
	year = {1999},
}

@article{tierney_markov_1994,
	title = {Markov {Chains} for {Exploring} {Posterior} {Distributions}},
	volume = {22},
	number = {4},
	journal = {Ann. Statist.},
	author = {Tierney, L.},
	year = {1994},
	note = {Publisher: The Institute of Mathematical Statistics},
	pages = {1701--1728},
}

@article{wang_geometric_2018,
	title = {On the {Geometric} {Ergodicity} of {Metropolis}-{Hastings} {Algorithms} for {Lattice} {Gaussian} {Sampling}},
	volume = {64},
	number = {2},
	journal = {IEEE Trans. Inf. Theory},
	author = {Wang, Z. and Ling, C.},
	year = {2018},
	pages = {738--751},
}

@article{mengersen_rates_1996,
	title = {Rates of convergence of the {Hastings} and {Metropolis} algorithms},
	volume = {24},
	number = {1},
	journal = {Ann. Statist.},
	author = {Mengersen, K. L. and Tweedie, R. L.},
	year = {1996},
	note = {Publisher: The Institute of Mathematical Statistics},
	pages = {101--121},
}

@article{livingstone_geometric_2019,
	title = {On the geometric ergodicity of {Hamiltonian} {Monte} {Carlo}},
	volume = {25},
	number = {4A},
	journal = {Bernoulli},
	author = {Livingstone, S. and Betancourt, M. and Byrne, S. and Girolami, M.},
	year = {2019},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	pages = {3109--3138},
}

@article{wang_learnable_2019,
	title = {Learnable {Markov} {Chain} {Monte} {Carlo} {Sampling} {Methods} for {Lattice} {Gaussian} {Distribution}},
	volume = {7},
	journal = {IEEE Access},
	author = {Wang, Z. and Lyu, S. and Liu, L.},
	year = {2019},
	pages = {87494--87503},
}

@article{dwivedi_log-concave_2019,
	title = {Log-concave sampling: {Metropolis}-{Hastings} algorithms are fast},
	volume = {20},
	number = {183},
	journal = {J. Mach. Learn. Res},
	author = {Dwivedi, R. and Chen, Y. and Wainwright, M. J. and Yu, B.},
	year = {2019},
	pages = {1--42},
}

@book{meyn_markov_2009,
	edition = {2nd},
	title = {Markov {Chains} and {Stochastic} {Stability}},
	isbn = {0-521-73182-8},
	publisher = {Cambridge University Press},
	author = {Meyn, S. and Tweedie, R. L.},
	year = {2009},
}

@book{brooks_handbook_2011,
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {1-4200-7941-7 978-1-4200-7941-8},
	publisher = {Chapman and Hall/CRC},
	editor = {Brooks, S. and Gelman, A. and Jones, G. and Meng, X.-L.},
	year = {2011},
}

@book{tsybakov_introduction_2008,
	title = {Introduction to {Nonparametric} {Estimation}},
	isbn = {0-387-79051-9},
	publisher = {Springer},
	author = {Tsybakov, A. B.},
	year = {2008},
}

@article{roberts_general_2004,
	title = {General state space {Markov} chains and {MCMC} algorithms},
	volume = {1},
	journal = {Probab. Surveys},
	author = {Roberts, G. O. and Rosenthal, J. S.},
	year = {2004},
	note = {Publisher: The Institute of Mathematical Statistics and the Bernoulli Society},
	pages = {20--71},
}

@article{jarner_geometric_2000,
	title = {Geometric ergodicity of {Metropolis} algorithms},
	volume = {85},
	issn = {0304-4149},
	number = {2},
	journal = {Stoch. Process. Their Appl},
	author = {Jarner, S. F. and Hansen, E.},
	year = {2000},
	keywords = {Geometric ergodicity, Metropolis algorithm, Monte carls, Super-exponential densities},
	pages = {341--361},
}

@article{follath_gaussian_2014,
	title = {Gaussian {Sampling} in {Lattice} {Based} {Cryptography}},
	volume = {60},
	journal = {Tatra Mt. Math. Publ.},
	author = {Folláth, J.},
	year = {2014},
	pages = {1--23},
}

@inproceedings{klein_finding_2000,
	title = {Finding the {Closest} {Lattice} {Vector} {When} {It}'s {Unusually} {Close}},
	isbn = {0-89871-453-2},
	booktitle = {Proc. {ACM}-{SIAM} {Symp}. {Discrete} {Algorithms}},
	author = {Klein, P.},
	year = {2000},
	pages = {937--941},
}

@article{roberts_exponential_1996,
	title = {Exponential convergence of {Langevin} distributions and their discrete approximations},
	volume = {2},
	number = {4},
	journal = {Bernoulli},
	author = {Roberts, G. O. and Tweedie, R. L.},
	year = {1996},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	pages = {341--363},
}

@article{ehm_convolution_2004,
	title = {Convolution {Roots} of {Radial} {Positive} {Definite} {Functions} with {Compact} {Support}},
	volume = {356},
	issn = {00029947},
	number = {11},
	journal = {Trans. Am. Math. Soc},
	author = {Ehm, W. and Gneiting, T. and Richards, D.},
	year = {2004},
	note = {Publisher: American Mathematical Society},
	pages = {4655--4685},
}

@article{liu_decoding_2011,
	title = {Decoding by {Sampling}: {A} {Randomized} {Lattice} {Algorithm} for {Bounded} {Distance} {Decoding}},
	volume = {57},
	number = {9},
	journal = {IEEE Trans. Inf. Theory},
	author = {Liu, S. and Ling, C. and Stehle, D.},
	year = {2011},
	pages = {5933--5945},
}

@article{george_mcmc_2021,
	title = {An {MCMC} {Method} to {Sample} from {Lattice} {Distributions}},
	url = {https://arxiv.org/pdf/2101.06453.pdf},
	author = {George, A. J. and Kashyap, N.},
	year = {2021},
}

@article{ling_achieving_2014,
	title = {Achieving {AWGN} {Channel} {Capacity} {With} {Lattice} {Gaussian} {Coding}},
	volume = {60},
	number = {10},
	journal = {IEEE Trans. Inf. Theory},
	author = {Ling, C. and Belfiore, J.},
	year = {2014},
	pages = {5918--5929},
}

@inproceedings{micciancio_worst-case_2004,
	title = {Worst-case to average-case reductions based on {Gaussian} measures},
	doi = {10.1109/FOCS.2004.72},
	booktitle = {45th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Micciancio, D. and Regev, O.},
	year = {2004},
	pages = {372--381},
}

@article{wang_decoding_2013,
	title = {Decoding by {Sampling} — {Part} {II}: {Derandomization} and {Soft}-{Output} {Decoding}},
	volume = {61},
	doi = {10.1109/TCOMM.2013.101813.130500},
	number = {11},
	journal = {IEEE Transactions on Communications},
	author = {Wang, Z. and Liu, S. and Ling, C.},
	year = {2013},
	pages = {4630--4639},
}

@article{forney_multidimensional_1989,
	title = {Multidimensional constellations. {II}. {Voronoi} constellations},
	volume = {7},
	doi = {10.1109/49.29616},
	number = {6},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Forney, G. D.},
	year = {1989},
	pages = {941--958},
}

@article{kschischang_optimal_1993,
	title = {Optimal nonuniform signaling for {Gaussian} channels},
	volume = {39},
	doi = {10.1109/18.256499},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Kschischang, F. R. and Pasupathy, S.},
	year = {1993},
	pages = {913--929},
}

@article{ling_semantically_2014,
	title = {Semantically {Secure} {Lattice} {Codes} for the {Gaussian} {Wiretap} {Channel}},
	volume = {60},
	doi = {10.1109/TIT.2014.2343226},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Ling, C. and Luzzi, L. and Belfiore, J. and Stehlé, D.},
	year = {2014},
	pages = {6399--6416},
}

@article{bou-rabee_coupling_2020,
	title = {Coupling and convergence for hamiltonian monte carlo},
	volume = {30},
	issn = {10505164},
	doi = {10.1214/19-AAP1528},
	abstract = {Based on a new coupling approach, we prove that the transition step of the Hamiltonian Monte Carlo algorithm is contractive w.r.t. a carefully designed Kantorovich (L1 Wasserstein) distance. The lower bound for the contraction rate is explicit. Global convexity of the potential is not required, and thus multimodal target distributions are included. Explicit quantitative bounds for the number of steps required to approximate the stationary distribution up to a given error ϵ are a direct consequence of contractivity. These bounds show that HMC can overcome diffusive behavior if the duration of the Hamiltonian dynamics is adjusted appropriately.},
	number = {3},
	journal = {Annals of Applied Probability},
	author = {BOU-RABEE, NAWAF and EBERLE, ANDREAS and ZIMMER, RAPHAEL},
	year = {2020},
}

@article{wang_lattice_2019,
	title = {Lattice {Gaussian} {Sampling} by {Markov} {Chain} {Monte} {Carlo}: {Bounded} {Distance} {Decoding} and {Trapdoor} {Sampling}},
	volume = {65},
	doi = {10.1109/TIT.2019.2901497},
	number = {6},
	journal = {IEEE Trans. Inf. Theory},
	author = {Wang, Zheng and Ling, Cong},
	year = {2019},
	pages = {3630--3645},
}

@book{lemmens_nonlinear_2012,
	title = {Nonlinear {Perron}–{Frobenius} {Theory}},
	publisher = {Cambridge University Press},
	author = {Lemmens, B. and Nussbaum, R.},
	year = {2012},
	doi = {10.1017/CBO9781139026079},
}

@article{noauthor_nonparametrie_nodate,
	title = {Nonparametrie {Goodness}-of-{Fit} {Testing} {Under} {Gaussian} {Models}},
}

@inproceedings{peikert_efficient_2010,
	title = {An {Efficient} and {Parallel} {Gaussian} {Sampler} for {Lattices}},
	booktitle = {Advances in {Cryptology}},
	publisher = {Springer Berlin Heidelberg},
	author = {Peikert, C.},
	editor = {Rabin, Tal},
	year = {2010},
	pages = {80--97},
}

@phdthesis{prest_gaussian_2015,
	title = {Gaussian sampling in lattice-based cryptography},
	school = {Ecole normale supérieure - ENS PARIS},
	author = {Prest, T.},
	year = {2015},
}

@article{roberts_complexity_2014,
	title = {Complexity {Bounds} for {MCMC} via {Diffusion} {Limits}},
	url = {https://arxiv.org/pdf/1411.0712.pdf},
	author = {Roberts, G. O. and Rosenthal, J. S.},
	year = {2014},
}

@article{chatterjee_properties_2010,
	title = {Properties of {Uniform} {Doubly} {Stochastic} {Matrices}},
	url = {https://arxiv.org/pdf/1010.6136.pdf},
	author = {Chatterjee, S. and Diaconis, P. and Sly, A.},
	year = {2010},
}

@phdthesis{smith_analyses_2015,
	title = {Some analyses of {Markov} chains by the coupling method},
	school = {Stanford University},
	author = {Smith, A.},
	year = {2015},
}

@article{knight_sinkhornknopp_2008,
	title = {The {Sinkhorn}–{Knopp} {Algorithm}: {Convergence} and {Applications}},
	volume = {30},
	doi = {10.1137/060659624},
	number = {1},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Knight, Philip A.},
	year = {2008},
	pages = {261--275},
}

@article{cappellini_random_2009,
	title = {Random bistochastic matrices},
	volume = {42},
	number = {36},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Cappellini, V. and Sommers, H. and Bruzda, W. and Życzkowski, K.},
	year = {2009},
	note = {Publisher: IOP Publishing},
}

@article{chen_fast_2018,
	title = {Fast {MCMC} {Sampling} {Algorithms} on {Polytopes}},
	volume = {19},
	number = {55},
	journal = {Journal of Machine Learning Research},
	author = {Chen, Y. and Dwivedi, R. and Wainwright, M. J. and Yu, B.},
	year = {2018},
	pages = {1--86},
}

@article{sinkhorn_concerning_1967,
	title = {Concerning nonnegative matrices and doubly stochastic matrices},
	volume = {21},
	number = {2},
	journal = {Pacific Journal of Mathematics},
	author = {Sinkhorn, R. and Knopp, P.},
	year = {1967},
	note = {Publisher: Pacific Journal of Mathematics, A Non-profit Corporation},
	pages = {343 -- 348},
}

@article{idel_review_2016,
	title = {A review of matrix scaling and {Sinkhorn}'s normal form for matrices and positive maps},
	url = {https://arxiv.org/pdf/1609.06349.pdf},
	author = {Idel, M.},
	year = {2016},
}

@article{krishnapur_iteration_nodate,
	title = {{ITERATION} {TO} {GENERATE} {DOUBLY} {STOCHASTIC} {MATRICES}},
	journal = {preprint},
	author = {Krishnapur, M. and Kashyap, N.},
}

@article{sinkhorn_problems_1969,
	title = {Problems {Involving} {Diagonal} {Products} in {Nonnegative} {Matrices}},
	volume = {136},
	journal = {Transactions of the American Mathematical Society},
	author = {Sinkhorn, Richard and Knopp, Paul},
	year = {1969},
	note = {Publisher: American Mathematical Society},
	pages = {67--75},
}
