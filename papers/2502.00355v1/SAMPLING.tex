\section{Main Ideas}
In this section, we present our methods in detail. We introduce two algorithms for sampling from probability distributions--both based on the stochastic interpolants framework and the FBSDE formulation for solving partial differential equations. First, we review the definition of a linear interpolant (similar to the one-sided linear stochastic interpolant in \cite{albergo_stochastic_2023}).

\begin{definition}[Linear interpolants] For some $T>0$, let $g,r:[0,T]\rightarrow \R_+$ be such that $\frac{g}{r}$ is a non-deceasing function. Let $x^*\sim\nu$ and $z\sim\cN{0,I_d}$ be independent random variables. A \textit{Linear interpolant} is a collection of random variables $\{x_t\}_{t\in[0,T]}$ given by 
\begin{equation}\label{eqn:half_interpolant}
x_t = g(t)x^*+r(t)z,\quad 0\le t\le T. 
\end{equation}   
We call $x_t$ a \textit{half interpolant} if we include the boundary condition $g(0)=0$. Further, we call $x_t$ a \textit{full interpolant} if $g$ and $r$ satisfies the boundary conditions $g(0) = r(T)=0, g(T)=1$. We don't enforce any condition on $r(0)$.
\end{definition}
Figure~\ref{fig:linearInterpolants} in Appendix~\ref{appndx:interpolants} shows some examples of linear interpolants. Observe that, since $g/r$ is a non-decreasing function, $x_t$ becomes more informative about $x^*$ as time progresses. For a full interpolant, $x_T$ is fully informative about $x^*$ while for a half interpolant, $x_T$ is still a noisy version of $x^*$. We will demonstrate that we can utilize either of these to develop sampling methods. When using half interpolants for sampling, the distribution $\nu$ will be set implicitly by the constraint that we want $x_T\sim\pi$ whereas, for full interpolants, we take $\nu$ equal to the target density $\pi$. First, we describe a sampling method using half interpolants and later extend it to the case of full interpolants.
\subsection{Sampling using half  interpolants}
Consider a half interpolant $x_t$ for $t\in[0,T]$. Note that it is not possible to obtain $x_t$ using (\ref{eqn:half_interpolant}),  since it requires knowledge about $\nu$, and samples $x^*$ from $\nu$. Instead, if we can implement a diffusion process $\{S_t\}_{t\in[0,T]}$ such that the distribution of $S_t$ is same as $x_t$ for all $t\in[0,T]$ and also enforce the condition that $S_T\sim\pi$, then we have a method that drives $S_0\sim\cN{0,r^2(0)I_d}$ to $S_T\sim\pi$. Constructing such processes is our aim here.  Let $\rho(t,\cdot)$ denote the probability density of $x_t$
\iffalse
Explicitly, $\rho$ is given by
\begin{align*}
    \rho(t,x) &= \frac{1}{(2\pi r^2(t))^{d/2}}\int_{\R^d}\nu(x^*)e^{-\frac{\norm{x-g(t)x^*}^2}{2r^2(t)}}dx^*.\\
\end{align*}
\fi
and $s(t,x) := \nabla\log{\rho(t,x)}$ denote the score funciton of the density $\rho$. The score function $s$ holds significant importance in generative modeling through diffusion processes. Notably, the score function is a pivotal component in our formulation as well. First, we present Lemma~\ref{lemma:pde_density} that characterizes the density function $\rho$ as a solution to certain PDEs \cite{albergo_stochastic_2023} (see Lemma~\ref{lemma_app:pde_density} in Appendix~\ref{sec:Proofs} for a proof).
\begin{lemma}\label{lemma:pde_density}
The probability density function of $x_t$ defined in (\ref{eqn:half_interpolant}) satisfies a PDE given by
\begin{equation}\label{eqn:first_order_pde}
    \partial_t\rho+\nabla\cdot(b\rho) = 0,\quad \rho(0,\cdot) = \cN{0,r^2(0)I_d},
    %\quad \rho(T,\cdot) = \pi(\cdot),
\end{equation}
where $b(t,x) = \dot g(t)\expectCond{x^*}{x_t=x}-r(t)\dot r(t)s(t,x)$. Equivalently, $\rho$ satisfies the following Fokker-Planck equation:
\begin{equation}\label{eqn:FPE}
    \partial_t\rho-\frac{\eps^2(t)}{2}\Delta\rho+\nabla\cdot\left(\left(b(t,x)+\frac{\eps^2(t)}{2}s(t,x)\right)\rho\right) = 0,%\quad \rho(0,\cdot) = \cN{0,r^2(0)I_d}.
\end{equation}
with initial condition $\rho(0,\cdot) = \cN{0,r^2(0)I_d}$.
\end{lemma}

Equation~\ref{eqn:first_order_pde} is the continuity equation for the density of particles in a velocity field $b$. This implies that a process $S_t$ with probability density $\rho(t,\cdot)$ can be obtained by solving an ODE with a random initial condition, given by
\begin{equation}\label{eqn:ODEforSampling}
dS_t = b(t,S_t)dt,\quad S_0\sim\cN{0,r^2(0)I_d}.    
\end{equation}
Similarly, equation~\ref{eqn:FPE} governs the evolution of the density of particles under a drift and diffusion component. That is, we can realize process $S_t$ by simulating the following SDE:
\begin{equation}\label{eqn:SDEforSampling}
   dS_t = \left(b(t,S_t)+\frac{\eps^2(t)}{2}s(t,S_t)\right)dt+\eps(t)dW_t,%\quad S_0\sim\cN{0,r^2(0)I_d},
\end{equation}
with $S_0\sim\cN{0,r^2(0)I_d}$, where $\{W_t\}_{t\in[0,T]}$ is a standard Brownian motion. Hence, it suffices to have access to functions $b$ and $s$ to implement a diffusion process $S_t$ that has the same marginal distribution as $x_t$. Moreover, Lemma~\ref{lemma:b_and_s} shows that having either $\expectCond{x^*}{x_t=x}$ or $s$ is sufficient to derive functions $b$ and $s$ (see Lemma~\ref{lemma_app:b_and_s} in Appendix~\ref{sec:Proofs} for a proof).
\begin{lemma}\label{lemma:b_and_s}
    Let $x_t$ be a linear or half interpolant. Let $s(t,x) = \nabla \rho(t,x)$ and $b(t,x) = \dot g(t)\expectCond{x^*}{x_t=x}-r(t)\dot r(t)s(t,x)$. Then, we have the following expressions for $b$ and $s$:
% \begin{equation}\label{eqn:b_and_s}
% \begin{split}
%   b(t,x) &= \begin{cases}
%       \frac{\dot r(t)}{r(t)}x+\left(\dot g(t)-\frac{g(t)\dot r(t)}{r(t)}\right)\expectCond{x^*}{x_t=x},\\
%       \frac{\dot g(t)}{g(t)}x+\left(r^2(t)\frac{\dot g(t)}{g(t)}-\dot r(t)r(t)\right)s(t,x)
%       \end{cases}\\
% s(t,x) &= \frac{g(t)\expectCond{x^*}{x_t=x}-x}{r^2(t)}.  
% \end{split}
% \end{equation}
\begin{align}\label{eqn:b_and_s}
\begin{split}
  b(t,x) &= \begin{cases}
      \frac{\dot r(t)}{r(t)}x+\left(\dot g(t)-\frac{g(t)\dot r(t)}{r(t)}\right)\expectCond{x^*}{x_t=x}\\
      \frac{\dot g(t)}{g(t)}x+\left(r^2(t)\frac{\dot g(t)}{g(t)}-\dot r(t)r(t)\right)s(t,x)
      \end{cases}, \\
s(t,x) &= \frac{g(t)\expectCond{x^*}{x_t=x}-x}{r^2(t)}.
\end{split}  
\end{align}
\end{lemma}

Relying on the score $s$ to obtain $b$ presents a challenge due to the condition $g(0)=0$, causing the first term in the second expression for $b$ to diverge as $t$ approaches 0, resulting in numerical instability. Since there are no boundary conditions on $r$ for a half interpolant, a better strategy is to obtain the function $\expectCond{x^*}{x_t=x}$ and use the first and third expressions to estimate $b$ and $s$ respectively. Towards this, consider the expression for $\rho$:
\begin{align*}
    \rho(t,x) &= \frac{1}{(2\pi r^2(t))^{d/2}}\int_{\R^d}\nu(dx^*)e^{-\frac{\norm{x-g(t)x^*}^2}{2r^2(t)}}\\
    &= \psi(t,x)\int_{\R^d}\nu(dx^*)e^{\frac{g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2},
\end{align*}
where $\psi(t,x)=\frac{1}{(2\pi r^2(t))^{d/2}}e^{-\frac{\norm{x}^2}{2r^2(t)}}$. For some $\beta:[0,T]\rightarrow\R_+$, we define the function
\begin{align}\label{eqn:velocity}
\begin{split}
    u(t,x) &= \log\frac{\rho(t,\beta(t)x)}{\psi(t,\beta(t)x)}\\
    &= \log\int_{\R^d}\nu(dx^*)e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}.
\end{split}
\end{align}
Taking the gradient of $u$, we see that $\expectCond{x^*}{x_t=x} = \frac{r^2(t)}{\beta(t)g(t)}\nabla u(t,\frac{x}{\beta(t)})$.
Lemma~\ref{lemma:velociyt_HJB_PDE} characterizes $u$ as the solution to an HJB PDE (refer to Lemma~\ref{lemma_app_app:velociyt_HJB_PDE} in Appendix~\ref{sec:Proofs} for a proof).
\begin{lemma}\label{lemma:velociyt_HJB_PDE}
    Let $u:[0,T]\times\R^d:\rightarrow\R$ be the function given in (\ref{eqn:velocity}). Then $u$ satisfies the following Hamilton-Jacobi-Bellman equation
\begin{equation}\label{eqn:velocity_pde}
    \partial_t u + \frac{\sigma^2}{2}\Delta u + \frac{\sigma^2}{2}\norm{\nabla u}^2 + \mu^T\nabla u = 0,
\end{equation}
where $\mu(t,x) = -\partial_t\log\left(\frac{\beta(t)g(t)}{r^2(t)}\right)x$ and $\sigma^2(t) = 2\frac{r^2(t)}{\beta^2(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)\geq 0$. 
\end{lemma}
Note that (\ref{eqn:velocity_pde}) is a backward Kolmogorov PDE and can be solved given a terminal condition $u(T,\cdot)$. To constrain that $X_T\sim\pi$, we want $\rho(T,\cdot)=\pi(\cdot)$, which together with (\ref{eqn:velocity}) yields
\begin{equation}\label{eqn:terminal_cond_vel_pde}
 u(T, x)=\varphi(x) \equiv \log\frac{\pi(\beta(T)x)}{\psi(T,\beta(T)x)}.   
\end{equation}
Note that this condition also implicitly determines $\nu$. Here, for the sake of simplicity in presentation, we assumed that $\pi$ is a normalized density. However, it is important to note that it can be replaced by an unnormalized density $\hat{\pi}$. This would merely shift the solution $u$ by a constant, which would not impact the sampling algorithm, as only $\nabla u$ is required in the sampling phase. The function $\beta$ is a design parameter, which we can choose such that the coefficients of the PDE are well defined for $t\in[0,T]$. Note that simply choosing $\beta(t) = 1$ can cause $\mu$ and $\sigma$ to diverge as $t$ goes to $0$. One possible choice for $\beta$ is $\frac{r}{g}$, which makes the coefficients well defined for all $t\in[0,T]$. 

We emphasize that our aim is to obtain the function $\expectCond{x^*}{x_t=x} = \frac{r^2(t)}{\beta(t)g(t)}\nabla u(t,\frac{x}{\beta(t)})$ along the sample paths $S_t$ given by our sampling equation (\ref{eqn:SDEforSampling}). A prominent strategy in optimal-control literature to obtain $\nabla u$ is to formulate an optimization problem whose solution is $\nabla u$ (see discussion in Appendix~\ref{sec:oc_based_approach}). %(see Remark~\ref{remark:oc_approach}).
However, for reasons explained below, we pursue an alternative approach to solve PDE (\ref{eqn:velocity_pde}). Given that (\ref{eqn:velocity_pde}) is a non-linear parabolic PDE, as previously established, we can derive its solutions by solving the corresponding FBSDE. An FBSDE corresponding to PDE (\ref{eqn:velocity_pde}) can be formulated as:
\begin{align}\label{eqn:fbsde_half_interpolant}
\begin{split}   
    dX_t &= \left(\mu(t,X_t)+\sigma(t) Z_t\right)dt+\sigma(t)dW_t,\\
    %\\
    dY_t &= \frac{1}{2}\norm{Z_t}^2dt + Z_t^TdW_t,
\end{split}
\end{align}
with $X_0=\xi$ an appropriately chosen initial condition and $Y_T=\varphi(X_T)$. Lemma~\ref{lemma:pde_fbsde} relates the processes in (\ref{eqn:fbsde_half_interpolant}) to the solution of the PDE (\ref{eqn:velocity_pde}) under the terminal condition (\ref{eqn:terminal_cond_vel_pde}) (refer to Lemma~\ref{lemma_app:pde_fbsde} in Appendix~\ref{sec:Proofs} for a proof).
\begin{lemma}\label{lemma:pde_fbsde}
    Let $u:[0,T]\times\R^d:\rightarrow\R$ be the solution to PDE (\ref{eqn:velocity_pde}). Then the processes $Y_t$ and $Z_t$ in (\ref{eqn:fbsde_half_interpolant}) are given by $Y_t=u(t,X_t)$ and $Z_t=\sigma(t)\nabla u(t,X_t)$.
\end{lemma}
\subsubsection{Solving the FBSDE}\label{sec:solving_fbsde}
We take a machine learning-based approach to solve the FBSDE (\ref{eqn:fbsde_half_interpolant}). In particular, we approximate the solution $u$ to the PDE (\ref{eqn:velocity_pde}) with a neural network $u^\theta$ paramterized by $\theta$. Subsequently, we obtain an approximation to $\nabla u$ by taking the gradient of $u^\theta$. These approximations enable us to approximate the processes $Y_t$ and $Z_t$. Thereafter, we can form a loss function based on the fact that $(X_t,Y_t,Z_t)$ satisfies the SDEs given in (\ref{eqn:fbsde_half_interpolant}). A subtle point in solving the above FBSDE (\ref{eqn:fbsde_half_interpolant}) is that the distribution of the process $X_t$ differs from that of the sampling process $S_t$ at each time instant $t$. Therefore, we might not obtain the function $\nabla u$ on the desired domain. We overcome this difficulty by noting that FBSDE (\ref{eqn:fbsde_half_interpolant}) is a local equation in the sense that it should be satisfied at each time instant $t$. In particular, we maintain a separate process $X_t$ in order to ensure that we learn $u$ and $\nabla u$ on the appropriate domain, and use a penalty term in the loss (Equ. \eqref{lossFBSDElocal}) which enforces constraints arising from FBSDE locally in time. This is a major advantage of FBSDE over optimal control based approaches for solving HJB PDEs.  Specifically, for a $\tau$ chosen uniformly at randomly from $[0,T]$ we set, $X=X_\tau$, 
$Y=u^\theta(\tau,X_\tau)$, $Z=\sigma(\tau)\nabla u^\theta(\tau,X_\tau)$, where $X_\tau$ is the solution to the following ODE (for $t=\tau)$:
\begin{equation}\label{eqn:fbsde_init_process}
    \frac{dX_t}{dt} = \frac{\dot r(t)}{r(t)}X_t+\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)\frac{r^2(t)}{\beta(t)}\nabla u^\theta\left(t,\frac{X_t}{\beta(t)}\right),
    %\frac{dX_t}{dt} = \frac{\dot r(t)}{r(t)}X_t+\left(\dot g(t)-\frac{g(t)\dot r(t)}{r(t)}\right)\frac{r^2(t)}{\beta(t)g(t)}\nabla u^\theta\left(t,\frac{X_t}{\beta(t)}\right),%\quad X_0\sim\cN{0,r^2(0)I_d}.
\end{equation}
with $X_0\sim\cN{0,r^2(0)I_d}$ and we 
form the following $\delta$-step discretization of the FBSDE as:
\begin{align}\label{eqn:fbsde_discretized_half_int}
\begin{split}
        \hat{X}_\delta &= X + \left(\mu(\tau,X)+\sigma(\tau)Z\right)\delta+\sigma(\tau)\sqrt{\delta}w,\\
       \hat{Y}_\delta &= Y + \frac{1}{2}\norm{Z}^2\delta+\sqrt{\delta}Z^Tw,
\end{split}
\end{align}
% \end{wrapfigure}    
where $w\sim\cN{0,I_d}$. 
The ODE in (\ref{eqn:fbsde_init_process}) coincides with the ODE in (\ref{eqn:ODEforSampling}) when $\nabla u^\theta$ equals $\nabla u$. Therefore, at optimum, we ensure that $\nabla u$ is learned over an appropriate domain. With an appropriately choosen $\lambda>0$, we use the equations (\ref{eqn:fbsde_discretized_half_int}), the quantity $Y_\delta = u^\theta(\tau+\delta,\hat{X}_\delta)$, and the boundary conditions of the FBSDE (\ref{eqn:fbsde_half_interpolant}), to form the loss function as follows:
\begin{equation}\label{lossFBSDElocal}
\cL(\theta) = \mathbb{E}{ \norm{\nabla u^\theta(T,X_{T})-\nabla\varphi(X_T)}^2}+\lambda\mathbb{E}{\left(\hat{Y}_{\delta}-Y_{\delta}\right)^2}. %\norm{u^\theta(T,X_{T})-\varphi(X_T)}^2 +   
\end{equation}
The expectations above are over the process $X_\tau$ (given $\tau$), the random time $\tau$, and $w$. Note that since we are more interested in the quantity $\nabla u$ than $u$, we formed the loss function in terms of the gradient of the terminal condition.  We find the approximate solution to FBSDE (\ref{eqn:fbsde_half_interpolant}) and in turn to the PDE (\ref{eqn:velocity_pde}) by minimizing $\cL(\theta)$ over $\theta$. In practice, we discretize the ODE (\ref{eqn:fbsde_init_process}) using the Euler-Maruyama scheme. We observe that the discretization error in this step does not directly contribute to the training error, as it only affects the precise path of $X_t$. We present the steps to compute loss in Procedure~\ref{proc:loss_half_int} in Appendix \ref{apndx:half_interpolant}.

\subsubsection{Sampling}
Once we have an estimate of $\nabla u$, we can derive the functions $b$ and $s$ by utilizing equations (\ref{eqn:b_and_s}). Substituting $\expectCond{x^*}{x_t=x}=\frac{r^2(t)}{\beta(t)g(t)}\nabla u(t,\frac{x}{\beta(t)})$, we get
\begin{align*}
    b(t,x) &= \frac{\dot r(t)}{r(t)}x+\frac{r^2(t)}{\beta(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)\nabla u\left(t,\frac{x}{\beta(t)}\right),\\
    s(t,x) &= \frac{1}{\beta(t)}\nabla u\left(t,\frac{x}{\beta(t)}\right) -\frac{x}{r^2(t)}.
\end{align*}
Subsequently, we can use either ODE (\ref{eqn:ODEforSampling}) or SDE (\ref{eqn:SDEforSampling}) for sampling. We outline the steps for sampling in Procedure~\ref{proc:sample_half_int} in Appendix \ref{apndx:half_interpolant}.

\iffalse
\begin{remark}\label{remark:oc_approach}
We remark that $\nabla u$ could a priori be obtained as a solution to an optimization problem. In particular, let $u$ be the solution to PDE (\ref{eqn:velocity_pde}) and for an $m\in C^1([0,T]\times\R^d,\R^d)$ let the process $X_t$ be generated by the SDE $dX_t = \left(\sigma^2 m(t,X_t)+\mu(t,X_t)\right)dt+\sigma(t) dW_t$. Then, we have 
\begin{equation}\label{eqn:oc_optimization}
    \nabla u = \argmin{m} \expectCond{\frac{1}{2}\int_0^T\sigma^2(t)\norm{m(s,X_s)}^2ds-\varphi(X_T)}{X_0}.
\end{equation}
Typically, $m$ is parameterized by a neural network, and its parameters are learned by solving the minimization problem in (\ref{eqn:oc_optimization}). However, observe that the domain over which the function $\nabla u$ is learned depends on the distribution of the diffusion process $X_t$ for the optimal $m$. In our case, this presents issues since the diffusion process employed for sampling (\ref{eqn:SDEforSampling}) is different from $X_t$. Hence, if solved using the optimal control approach, $\nabla u$ would be learned over a domain that differs from what is required.
\end{remark}
\fi

\subsection{Sampling using full interpolants}\label{sec:full_interpolant}
%In the preceding section, we explored the process of sampling using a half interpolant, which entails solving an HJB PDE. 
In this section, we introduce a method to sample from probability distributions using a full interpolant. Merely employing the previous method is destined to fail, as the coefficients of the PDE tend towards infinity if we impose the condition $r(T)=0$ required by the full interpolant.  Instead, we propose a two-step approach that involves solving two HJB PDEs; one for $t\in[0,T']$ and the other for $t\in[T',T]$ with $T'\in(0,T)$. The idea is to obtain $\expectCond{x^*}{x_t=x}$ for $t\in[0,T']$ and the score $s$ for $t\in[T',T]$. This is due to the observation that it is possible to form well-behaved PDEs for a quantity related to $\expectCond{x^*}{x_t=x}$ for $t\in[0,T']$ and similarly, for a quantity related to $s$ for $t\in[T',T]$. Since $b$ can be derived from either $\expectCond{x^*}{x_t=x}$ or $s$, we acquire the functions essential for sampling for all $t$ within the whole interval $[0,T]$.

First, let us consider $t$ in the interval $[T',T]$. For some function $\alpha:[T',T]\rightarrow R_+$, let $v(t,x) = \log\rho(t,\alpha(t)x)+d\log g(t)$. Taking the gradient of $v$, we observe that $v$ is related to the score by $s(t,x )=\frac{1}{\alpha(t)}\nabla v(t,\frac{x}{\alpha(t)})$. Lemma~\ref{lemma:score_pde} shows that $v$ satisfies an HJB PDE (see Lemma~\ref{lemma_app:score_pde} in Appendix~\ref{sec:Proofs} for a proof).
\begin{lemma}\label{lemma:score_pde}
    Let $v(t,x) = \log\rho(t,\alpha(t)x)+d\log g(t)$. Then $v$ satisfies the following Hamilton-Jacobi-Bellman equation
\begin{equation}\label{eqn:score_pde}
    \partial_t v+\frac{\bar{\sigma}^2}{2}\Delta v+\frac{\bar{\sigma}^2}{2}\norm{\nabla v}^2+\bar{\mu}^T\nabla v=0,
\end{equation}
where $\bar{\mu}(t,x) = \partial_t\log\left({\frac{g(t)}{\alpha(t)}}\right)x$ and $\bar{\sigma}^2 = 2\frac{r^2(t)}{\alpha^2(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)$.
\end{lemma}

For sampling, we need $x_T$ distributed as $\pi$. This provides us with the terminal condition
\begin{equation}\label{eqn:score_term_condition}
 v(T,x) = \varphi(x)\equiv \log\pi(\alpha(T)x)+d\log g(T).   
\end{equation}
 Similar to $\beta$ in the case of half interpolants, we can select $\alpha$ such that the PDE (\ref{eqn:score_pde}) is well defined for all $t$ in $[T',T]$. Here, simply choosing $\alpha = 1$ also suffices.

 For $t$ in $[0,T']$, it is natural to employ the function $u$ used in the half interpolant-based sampler. Therefore, we have to solve the PDE (\ref{eqn:velocity_pde}), this time however, subjected to a terminal condition dictated by the solution to PDE (\ref{eqn:score_pde}). The terminal condition is given by $\varphi'(x)= \log\frac{\rho(T',\beta(T')x)}{\psi(T',\beta(T')x)} =  v\left(T',\frac{\beta(T')x}{\alpha(T')}\right)-\log\psi(T',\beta(T')x)-d\log g(T').$ For completeness, we restate the PDE below:
\begin{equation}\label{eqn:pde_velocity_interpolant}
    \partial_t u + \frac{\sigma^2}{2}\Delta u + \frac{\sigma^2}{2}\norm{\nabla u}^2 + \mu^T\nabla u = 0,\quad u(T',x) = \varphi'(x).
\end{equation}
\iffalse
\begin{equation}
    \varphi'(x)= v\left(T',\frac{\beta(T')x}{\alpha(T')}\right)-\log\psi(T',\beta(T')x)-d\log g(T').
\end{equation}
\fi
To estimate $u$ and $v$, we need to solve PDEs (\ref{eqn:pde_velocity_interpolant}) and (\ref{eqn:score_pde}) subjected to the terminal conditions $\varphi'$ and $\varphi$ respectively. We again rely on solving the associated FBSDEs for solving the PDEs. The details of FBSDEs and the loss function for solving them can be found in Appendix~\ref{apndx:full_interpolant}.
%, although here we could have opted for the optimal control-based approach without any difficulty.

\section{Numerical Results}
We evaluated our methods on several distributions from the literature, that are considered challenging to sample from. More details of our implementation can be found in Section~\ref{apndx:implemnetation_details}. Below, we provide some distributions for which we present our results. Let $\Psi(x;\mu,\Sigma)$ denote the probability density of the $d$-dimensional Gaussian distribution with mean $\mu$ and covariance $\Sigma$. 
\iffalse
\begin{table}
  \caption{Distributions}
  \label{tab:dists}
  \centering
  \begin{tabular}{m{0.3\textwidth}m{0.4\textwidth}m{0.2\textwidth}}
    \toprule
     Distribution & PDF & Parameters\\
     \midrule
    Gaussian Mixture Model     & $\pi(x) = \frac{1}{M}\sum_{i=1}^{M} \Psi(x;\mu_i,\Sigma_i)$  & $d=2$    \\
             Funnel     &    $\pi(x) = \Psi(x_1;0,3)\Psi(x_2^d;0,e^{x_1}I_{d-1})$     & $d=10$ \\
   Double Well     & $\pi(x) = \sum_{i=1}^{M} \Psi(x;\mu_i,\Sigma_i)$  & $d=2$    \\
    \bottomrule
  \end{tabular}
\end{table}
\fi
% \begin{wraptable}[10]{R}{0.6\textwidth}
\begin{figure*}[ht]
  \centering
  \includegraphics[scale = 0.38]{images/evals_logZ.png}
  \caption{Estimates of $\log Z$ as a function of training steps along with $95\%$ confidence intervals.}
  \label{fig: results}
\end{figure*}

\paragraph{Gaussian mixture model (GMM):} The density of a Gaussian mixture model is given by $\pi(x) = \frac{1}{M}\sum_{i=1}^{M} \Psi(x;\mu_i,\Sigma_i)$. In our experiments, we use $M=9, \{\mu_i\}_{i=1}^9 = \{-5,0,5\}^2$ and $\Sigma_i = 0.3I_2$ for all $i$ (here $d=2$).
\paragraph{Neal's Funnel distribution:} Probability density of the funnel distribution is given by $\pi(x) = \Psi(x_1;0,s^2)\Psi(x_2, x_3,\dots, x_d;0,e^{x_1}I_{d-1})$. The values of the parameters are $d=10, s=3$. Reference \cite{neal_slice_2003} introduces this distribution and discusses the behavior of MCMC implementations. 
\paragraph{Double Well (DW):} The probability density of double well is given by $\pi(x) = \frac{1}{Z}\exp\left(-\sum_{i=1}^w(x_i^2-\delta)^2-\frac{1}{2}\sum_{i=w+1}^d x_i^2\right)$.

\paragraph{Soft Spherical Spin Glass model :} We consider a spin glass model whose probability density is given by $\pi(x) \propto \exp\{\frac{\beta}{\sqrt{2d}}x^TAx-\frac{\beta^2}{4d}\norm{x}^4-\frac{1}{2}\norm{x}^2\}$, where $A\in\R^{d\times d}$ with i.i.d. Gaussian entries.

We use our sampler for estimating different quantities associated to the above distributions. We estimate the \textit{log normalization constant} ($\log Z$) of the distribution using the method explained in Appendix~\ref{apndx:est_logZ}. Other quantities we consider are mean $L_1$-norm, and mean squared $L_2$-norm, whose estimates are obtained from samples using empirical average. We stress that our estimates are computed without using the importance sampling technique. Hence they reflect the `true' quality of the samples. In practice, we observe that the full interpolant-based sampling method gives better results compared to the half interpolant-based sampler. We mention that the full interpolant-based sampling method that we presented can in fact work with half interpolant functions as well. Figure~\ref{fig: results} shows the estimates of log normalization constant as a function of the training steps, given by the full interpolant-based sampling method. The interpolants used are visualized in Figure~\ref{fig:linearInterpolants}.

Additional numerical results can be found in Appendix~\ref{apndx:additionalExps}. In Section~\ref{sec:gaussian_spin_glass}, we use our sampler to confirm a predicted phase transition in the soft spherical spin glass model. In Section~\ref{sec:comparison_gbs} we compare our method against another diffusion based sampler called Generalized Bridge Sampler \cite{richter_improved_2023,blessing_beyond_2024}. In Table~\ref{tab:ablation} we show how different parameters affect the performance of our sampler.