\onecolumn
\aistatstitle{Supplementary Materials}
\section{Algorithms for Half Interpolant Sampler}\label{apndx:half_interpolant}
We compile the steps required to form the loss for half interpolant sampler into an algorithm. First, we define a sub-routine~\ref{proc:loss_fbsde_half_int} that generates the component in the loss due to FBSDE equations. This subroutine will also be used in the full interpolant sampler. 
\begin{procedure}
\DontPrintSemicolon
	\KwIn{$t,X,m,\delta,f,h$}
	\KwOut{L}
    $w\sim\cN{0,I_d}$\;
    $Z \gets h(t)\nabla m(t,X)$\;
    $\hat{Y} \gets m(t,X)+\frac{1}{2}\norm{Z}^2\delta+\sqrt{\delta}{Z}^Tw$\; 
    $X \gets X + \left(f(t,X)+h(t)Z\right)\delta+h(t)\sqrt{\delta}w$\;
    $Y \gets m(t+\delta,X)$\;
    $L \gets \left(\hat{Y}-Y\right)^2$\;
\Return{L}
\caption{LossFBSDE($t,X,m,\delta,f,h$)}\label{proc:loss_fbsde_half_int}
\end{procedure} 
Next, we present the procedure~\ref{proc:loss_half_int} to generate the overall loss for the half interpolant sampler. Here, we assume that the batch size equals 1. In practice, we use multiple realizations of $X$ and $\{t_i\}_{i=1}^{N-1}$ to form a single batch. This loss is subsequently minimized to find the optimal $\theta$.
\begin{procedure}
\DontPrintSemicolon
	\KwIn{$\theta,T,N,\delta,\lambda$}
	\KwOut{$\cL$}
$t_0 \gets 0, t_N \gets T$\;
$(t_1\le t_2\le\cdots\le t_{N-1}) \gets U([0,T])^{N-1}$\; %\Comment*[r]{Sorted}
$X\sim\cN{0,r^2(0)I_d}$\;
$\cL \gets 0$\;
\For{$i = 0,\cdots,N-1$}{
    $\cL \gets \cL + \lambda*\text{LossFBSDE}(t_i,X,u^\theta,\delta,\mu,\sigma)$\;
    $X \gets X + \left(\frac{\dot r(t_i)}{r(t_i)}X+\left(\dot g(t_i)-\frac{g(t_i)\dot r(t_i)}{r(t_i)}\right)\frac{r^2(t_i)}{\beta(t_i)g(t_i)}\nabla u^\theta\left(t_i,\frac{X}{\beta(t_i)}\right)\right)(t_{i+1}-t_i)$\;
}
$\cL\gets \cL + \norm{\nabla u^\theta(T,X)-\nabla\varphi(X)}^2$\;
\Return{$\cL$}	\caption{LossHalfInterpolant()}\label{proc:loss_half_int}
\end{procedure}
Once we have found the optimal $\theta$, we can sample from the target distribution by simulating the SDE given in (\ref{eqn:SDEforSampling}). Procedure~\ref{proc:sample_half_int} describes the steps involved in sampling.
\begin{procedure}
\DontPrintSemicolon
	\KwIn{$\theta^*,T,N',\eps$}
	\KwOut{$S$}
$\Delta \gets \frac{T}{N'}$\;
$S\sim\cN{0,r^2(0)I_d}$\;
\For{$i = 0,\cdots,N'-1$}{
    $t = i*\Delta$\;
    $w\sim\cN{0,I_d}$\;
    $b \gets \frac{\dot r(t)}{r(t)}S+\frac{r^2(t)}{\beta(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)\nabla u^{\theta^*}\left(t,\frac{S}{\beta(t)}\right)$\;
    $s \gets \frac{1}{\beta(t)}\nabla u^{\theta^*}\left(t,\frac{S}{\beta(t)}\right) -\frac{S}{r^2(t)}$\;
    $S \gets S + \left(b+\frac{\eps^2(t)}{2}s\right)\Delta+\sqrt{\Delta}\eps(t)w$\;
}
\Return{$S$}	\caption{SampleHalfInterpolant()}\label{proc:sample_half_int}
\end{procedure} 
% \end{wrapfigure}


\section{Details of the Full Interpolant-based Sampler}\label{apndx:full_interpolant}
In Section~\ref{sec:full_interpolant}, we have seen that in a full interpolant-based sampler, we have to solve two HJB PDEs given by (\ref{eqn:pde_velocity_interpolant}) and (\ref{eqn:score_pde}) under respective terminal conditions. Similar to the half interpolant-based sampler, we solve the PDEs by solving associated FBSDEs. An FBSDE corresponding to PDE (\ref{eqn:pde_velocity_interpolant}) can be written as
% and that of FBSDE (\ref{eqn:fbsde_score_interpolant}) as
\begin{equation}\label{eqn:fbsde_velocity_interpolant}
\begin{split}
    dX_t &= \left(\mu(t,X_t)+\sigma(t) Z_t\right)dt+\sigma(t)dW_t,\quad X_0 = \xi\\
    dY_t &= \frac{1}{2}\norm{Z_t}^2dt + Z_t^TdW_t,\quad Y_{T'} = \varphi'(X_{T'}),
\end{split}
\end{equation}
while for PDE (\ref{eqn:score_pde}), an FBSDE is
\begin{equation}\label{eqn:fbsde_score_interpolant}
\begin{split}   
    dX_t &= \left(\bar{\mu}(t,X_t)+\bar{\sigma}(t) Z_t\right)dt+\bar{\sigma}(t)dW_t,\quad X_{T'} = \xi'\\
    dY_t &= \frac{1}{2}\norm{Z_t}^2dt + Z_t^TdW_t,\quad Y_{T} = \varphi(X_T).
\end{split}
\end{equation}

\subsection{Solving the FBSDEs}
As in the case of half interpolants, we take a machine learning-based approach to solve FBSDEs (\ref{eqn:fbsde_score_interpolant}, \ref{eqn:fbsde_velocity_interpolant}). We approximate the solution to the PDEs (\ref{eqn:pde_velocity_interpolant}, \ref{eqn:score_pde}) $u,v$ with neural networks $u^\theta$ and $v^{\theta'}$ paramterized by $\theta$ and $\theta'$ respectively. Subsequently, we obtain approximations to processes $(Y_t, Z_t)$, which along with the FBSDEs (\ref{eqn:fbsde_score_interpolant},\ref{eqn:fbsde_velocity_interpolant}) help us form a loss function. Specifically, for $\tau,\tau'$ chosen independently and uniformly at randomly from $[0,T']$ and $[T',T]$ respectively,  we form the $\delta$-step discretization of the FBSDE (\ref{eqn:fbsde_velocity_interpolant}) as 
%and FBSDE (\ref{eqn:fbsde_score_interpolant}) as:
% \begin{minipage}[t]{0.5\textwidth}
\begin{align*}
       X &= X_\tau,\quad Z = \sigma(\tau)\nabla u^\theta(\tau,X),\\
       \hat{Y}_\delta &= u^\theta(\tau,X)+\frac{1}{2}\norm{Z}^2\delta+\sqrt{\delta}Z^Tw,\\
    \hat{X}_\delta &= X + \left(\mu(\tau,X)+\sigma(\tau)Z\right)\delta+\sigma(\tau)\sqrt{\delta}w\\
    Y_\delta &= u^\theta(\tau+\delta,\hat{X}_\delta),
\end{align*}
% \end{minipage}
and that of FBSDE (\ref{eqn:fbsde_score_interpolant}) as
% \begin{minipage}[t]{0.5\textwidth}
\begin{align*}
       X' &= X_{\tau'}\quad Z' = \bar{\sigma}(\tau')\nabla v^{\theta'}(\tau',X')\\
       \hat{Y}'_{\delta} &= v^{\theta'}(\tau',X')+\frac{1}{2}\norm{Z'}^2\delta+\sqrt{\delta}Z'^Tw'\\
        \hat{X}'_{\delta} &= X'+ \left(\bar{\mu}(\tau',X')+\bar{\sigma}(\tau')Z'\right)\delta+\bar{\sigma}(\tau')\sqrt{\delta}w'\\
    Y'_{\delta} &= v^{\theta'}(\tau'+\delta,\hat{X}'_{\delta})\\
\end{align*}
% \end{minipage}
where  $w,w'\overset{\text{i.i.d.}}{\sim}\cN{0,I_d}$ and $X_t$ is the solution to the following ODE:
\begin{align*}
    X_0 &\sim\cN{0,r^2(0)I_d}\\
    \frac{dX_t}{dt} &= \begin{cases}
        \frac{\dot r(t)}{r(t)}X_t+\left(\dot g(t)-\frac{g(t)\dot r(t)}{r(t)}\right)\frac{r^2(t)}{\beta(t)g(t)}\nabla u^\theta\left(t,\frac{X_t}{\beta(t)}\right),\quad 0\le t\le T'\\
        \frac{\dot g(t)}{g(t)}X_t+\frac{r(t)}{\alpha(t)}\left(\frac{\dot g(t)}{g(t)}r(t)-\dot r(t)\right)\nabla v^{\theta'}(t,\frac{X_t}{\alpha(t)}),\quad T'<t\le T.
    \end{cases}
\end{align*}
We then form a loss function as follows:
\begin{multline}
\cL(\theta,\theta') = \expect{ \norm{\nabla u^\theta(T',X_{T'})-\nabla\varphi'(X_{T'})}^2}+\lambda\expect{\left(\hat{Y}_{\delta}-Y_{\delta}\right)^2}\\
+\expect{ \norm{\nabla v^{\theta'}(T,X_{T})-\nabla\varphi(X_{T})}^2}+\lambda\expect{\left(\hat{Y}'_{\delta}-Y'_{\delta}\right)^2}.%\norm{u^\theta(T',X_{T'})-\varphi(X_{T'})}^2 + \norm{v^{\theta'}(T,X_{T})-\varphi'(X_{T})}^2 +   
\end{multline}
We find a solution to FBSDEs (\ref{eqn:fbsde_score_interpolant},\ref{eqn:fbsde_velocity_interpolant}) and in turn to the PDEs (\ref{eqn:pde_velocity_interpolant}, \ref{eqn:score_pde}) by minimizing $\cL(\theta,\theta')$ over $\theta, \theta'$. We compile the steps involved in computing the loss in Procedure~\ref{proc:loss_int}.
\begin{procedure}
\DontPrintSemicolon
	\KwIn{$\theta,T,T',N,\delta,\lambda$}
	\KwOut{$\cL$}
 $X\sim\cN{0,r^2(0)I_d}$\;
$\cL \gets 0$\;
$N' = \left\lceil \frac{NT'}{T}\right\rceil$\;
$t_0 \gets 0, t_{N'} \gets T'$\;
$(t_1\le t_2\le\cdots\le t_{N'-1}) \gets U([0,T'])^{N'-1}$\;
\For{$i = 0,\cdots,N'-1$}{
    $\cL \gets \cL + \lambda*\text{LossFBSDE}(t_i,X,u^\theta,\delta,\mu,\sigma)$\;
    $X \gets X + \left(\frac{\dot r(t_i)}{r(t_i)}X+\left(\dot g(t_i)-\frac{g(t_i)\dot r(t_i)}{r(t_i)}\right)\frac{r^2(t_i)}{\beta(t_i)g(t_i)}\nabla u^\theta\left(t_i,\frac{X}{\beta(t_i)}\right)\right)(t_{i+1}-t_i)$\;
}
$\cL\gets \cL + \norm{\nabla u^\theta(T',X)-\nabla\varphi'(X)}^2$\;
$t_0 \gets T', t_{N-N'} \gets T$\;
$(t_1\le t_2\le\cdots\le t_{N-N'-1}) \gets U([T',T])^{N-N'-1}$\;
\For{$i = 0,\cdots,N-N'-1$}{
    $\cL \gets \cL + \lambda*\text{LossFBSDE}(t_i,X,v^{\theta'},\delta,\bar{\mu},\bar{\sigma})$\;
    $X \gets X + \left(\frac{\dot g(t_i)}{g(t_i)}X+\frac{r(t_i)}{\alpha(t_i)}\left(\frac{\dot g(t_i)}{g(t_i)}r(t_i)-\dot r(t_i)\right)\nabla v^{\theta'}(t_i,\frac{X}{\alpha(t_i)})\right)(t_{i+1}-t_i)$\;
}
$\cL\gets \cL + \norm{\nabla v^{\theta'}(T,X)-\nabla\varphi(X)}^2$\;
\Return{$\cL$}
\caption{LossFullInterpolant()}\label{proc:loss_int}
\end{procedure}

\subsubsection{Sampling}
Once we have an estimate for $\nabla u, \nabla v$, we can estimate the functions $b$ and $s$ by using equations (\ref{eqn:b_and_s}). We have
\begin{equation*}
    s(t,x) = \begin{cases}
        \frac{1}{\beta(t)}\nabla u(t,\frac{x}{\beta(t)})-\frac{x}{r^2(t)},\quad &0\le t<T'\\
        \frac{1}{\alpha(t)}\nabla v(t,\frac{x}{\alpha(t)}),\quad &T'\le t\le T',
    \end{cases}
\end{equation*}
\begin{equation*}
    b(t,x) = \begin{cases}
        \frac{\dot r(t)}{r(t)}x+\frac{r^2(t)}{\beta(t)g(t)}\left(\dot g(t)-\frac{g(t)\dot r(t)}{r(t)}\right)\nabla u(t,\frac{x}{\beta(t)}),\quad &0\le t<T'\\
        \frac{\dot g(t)}{g(t)}x+\frac{r(t)}{\alpha(t)}\left(r(t)\frac{\dot g(t)}{g(t)}-\dot r(t)\right)\nabla v(t,\frac{x}{\alpha(t)}),\quad &T'\le t\le T'.
    \end{cases}
\end{equation*} 
Once we have estimates of $b$ and $s$, we can sample from $\pi$ by simulating ODE (\ref{eqn:ODEforSampling}) or SDE (\ref{eqn:SDEforSampling}). In the Procedure~\ref{proc:smaple_int}, we present the steps involved in the sampling phase.

\begin{procedure}
\DontPrintSemicolon
	\KwIn{$\theta,\theta',T,T',N',\eps$}
	\KwOut{$S$}
$\Delta \gets \frac{T}{N}$\;
$S\sim\cN{0,r^2(0)I_d}$\;
\For{$i = 0,\cdots,N-1$}{
    
    $t = i*\Delta$\;
    $w\sim\cN{0,I_d}$\;
    \eIf{$t\le T'$}{
    $b \gets \frac{\dot r(t)}{r(t)}S+\frac{r^2(t)}{\beta(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)\nabla u^\theta\left(t,\frac{S}{\beta(t)}\right)$\;
    $s \gets \frac{1}{\beta(t)}\nabla u^\theta\left(t,\frac{S}{\beta(t)}\right) -\frac{S}{r^2(t)}$\;
    }
    {
    $b \gets \frac{\dot g(t)}{g(t)}S+\frac{r(t)}{\alpha(t)}\left(\frac{\dot g(t)}{g(t)}r(t)-\dot r(t)\right)\nabla v^{\theta'}(t,\frac{S}{\alpha(t)})$\;
    $s \gets \frac{1}{\alpha(t)}\nabla v^{\theta'}(t,\frac{S}{\alpha(t)})$\;
    }
    $S \gets S + \left(b+\frac{\eps^2(t)}{2}s\right)\Delta+\sqrt{\Delta}\eps(t)w$
}
	\caption{SampleFullInterpolant()}\label{proc:smaple_int}
\end{procedure}

\section{Optimal Control-based Approach}\label{sec:oc_based_approach}
We remark that $\nabla u$ could a priori be obtained as a solution to an optimization problem (see Lemma~\ref{lemma_app:oc_optimization}). In particular, let $u$ be the solution to PDE (\ref{eqn:velocity_pde}) and for an $m\in C^1([0,T]\times\R^d,\R^d)$ let the process $X_t$ be generated by the SDE $dX_t = \left(\sigma^2 m(t,X_t)+\mu(t,X_t)\right)dt+\sigma(t) dW_t$. Then, we have 
\begin{equation}\label{eqn:oc_optimization}
    \nabla u = \argmin{m} \expectCond{\frac{1}{2}\int_0^T\sigma^2(t)\norm{m(s,X_s)}^2ds-\varphi(X_T)}{X_0}.
\end{equation}
Typically, $m$ is parameterized by a neural network, and its parameters are learned by solving the minimization problem in (\ref{eqn:oc_optimization}). However, observe that the domain over which the function $\nabla u$ is learned depends on the distribution of the diffusion process $X_t$ for the optimal $m$. In our case, this presents issues since the diffusion process employed for sampling (\ref{eqn:SDEforSampling}) is different from $X_t$. Hence, if solved using the optimal control approach, $\nabla u$ would be learned over a domain that differs from what is required.

\begin{lemma}\label{lemma_app:oc_optimization}
    Let $u$ be the solution to PDE (\ref{eqn:velocity_pde}) and let the process $X_t$ be generated by the SDE
    \begin{equation}\label{eqn_app:oc_diffusion}
         dX_t = \left(\sigma^2 m(t,X_t)+\mu(t,X_t)\right)dt+\sigma(t) dW_t.
    \end{equation}
    Then, we have 
\begin{align*}
    u(0,X_0) &= - \min_m \expectCond{\frac{1}{2}\int_0^T\sigma^2(t)\norm{m(s,X_s)}^2ds-\varphi(X_T)}{X_0},\\
    \nabla u &= \argmin{m} \expectCond{\frac{1}{2}\int_0^T\sigma^2(t)\norm{m(s,X_s)}^2ds-\varphi(X_T)}{X_0}.
\end{align*}
\end{lemma}
\begin{proof}
This result is a special example of the general theory of stochastic optimal control, but we present a short proof for the convenience of the reader.
    Applying Ito's lemma to $u(t,X_t)$ gives
\begin{align*}
    du(t,X_t) &= \partial_t u(t,X_t)dt+\nabla u(t,X_t)^TdX_t+\frac{\sigma^2}{2}\Delta u(t,X_t)dt,\\
    & = \left(\partial_t u(t,X_t)+\sigma^2 m^T\nabla u(t,X_t)+\mu^T\nabla u(t,X_t)+\frac{\sigma^2}{2}\Delta u(t,X_t)\right)dt+\sigma\nabla u^TdW_t,\\
    &= \left(\sigma^2 m^T\nabla u(t,X_t)-\frac{\sigma^2}{2}\norm{\nabla u(t,X_t)}^2\right)dt+\sigma\nabla u^TdW_t,\\
    &= -\frac{\sigma^2}{2}\left(\norm{\nabla u(t,X_t)-m}^2 -\norm{m}^2\right)dt+\sigma\nabla u^TdW_t.
\end{align*}
Integrating from $t$ to $T$ and taking expectation conditioned on $X_t$ gives
\begin{align*}
    u(t,X_t) &= \expectCond{\frac{\sigma^2}{2}\int_t^T\norm{\nabla u(s,X_s)-m(s,X_s)}^2ds}{X_t}\\
    &\qquad\qquad\qquad\qquad+ \expectCond{u(T,X_T) - \frac{\sigma^2}{2}\int_t^T\norm{m(s,X_s)}^2ds}{X_t},\\
    &\ge \expectCond{u(T,X_T) - \frac{\sigma^2}{2}\int_t^T\norm{m(s,X_s)}^2ds}{X_t}.\\
\end{align*}
and the equality is attained when $m(t,X_t)=\nabla u(t,X_t)$ a.s.
Thus, we get the following variational formulation:
\begin{align*}
    u(t,X_t) &= \max_m \expectCond{\varphi(X_T) - \frac{\sigma^2}{2}\int_t^T\norm{m(s,X_s)}^2ds}{X_t},\\
    &= - \min_m \expectCond{\frac{\sigma^2}{2}\int_t^T\norm{m(s,X_s)}^2ds-\varphi(X_T)}{X_t},\\
    \nabla u(t,X_t) &= \argmin{m} \expectCond{\frac{\sigma^2}{2}\int_t^T\norm{m(s,X_s)}^2ds-\varphi(X_T)}{X_t}.
\end{align*}
\end{proof}

\section{Interpolants}\label{appndx:interpolants}
Figure \ref{fig:linearInterpolants} shows some examples of linear interpolants that we used in our experiments. The first row has half interpolants and the second row contains full interpolants. We observed that keeping $\frac{\dot r(t)}{r(t)}$ close to $0$ for small values of $t$ is better for training. This is because, at the beginning of training, $\frac{\dot r(t)}{r(t)}X_t$ is the drift in (\ref{eqn:fbsde_init_process}). Having this negative will decrease the initial exploration of the space, while having it positive might cause stability issues. 
\begin{figure}
  \centering
  \includegraphics[scale = 0.6]{images/interpolants2.png}
  \caption{Examples of linear interpolants. Top row: half interpolants. Bottom row: full interpolants.}
  \label{fig:linearInterpolants}
\end{figure}

\section{Proofs of Lemmas}\label{sec:Proofs}
\begin{lemma}\label{lemma_app:pde_density}
The probability density function of $x_t$ defined in (\ref{eqn:half_interpolant}) satisfies a PDE given by
\begin{equation}\label{eqn_app:first_order_pde}
    \partial_t\rho+\nabla\cdot(b\rho) = 0,\quad \rho(0,\cdot) \equiv \cN{0,r^2(0)I_d},\quad \rho(T,\cdot) = \pi(\cdot),
\end{equation}
where $b(t,x) =  \dot g(t)\expectCond{x^*}{x_t=x}-r(t)\dot r(t)s(t,x),$. Equivalently, $\rho$ satisfies the following Fokker-Planck equation:
\begin{equation}\label{eqn_app:FPE}
    \partial_t\rho-\frac{\eps^2(t)}{2}\Delta\rho+\nabla\cdot\left(\left(b+\frac{\eps^2(t)}{2}s(t,x)\right)\rho\right) = 0,\quad \rho(0,\cdot) \equiv \cN{0,r^2(0)I_d}.
\end{equation}
\end{lemma}
\begin{proof}
    Let $\hat{\rho}(t,\cdot)$ be the characteristic funciton of $\rho(t,\cdot)$. That is,
    \begin{align*}
        \hat{\rho}(t,k) &= \int_{\R^d}\rho(t,x)e^{i\langle k,x\rangle}dx,\\
        &= \expect{e^{i\langle k,x_t\rangle}}.
    \end{align*}
    Taking derivative of $\hat{\rho}$ w. r. t. $t$, we get
    \begin{align*}
        \partial_t\hat{\rho} &= \left\langle ik,\expect{\dot x_t e^{i\langle k,x_t\rangle}}\right\rangle,\\
        &= \left\langle ik,\expect{\expectCond{\dot x_t}{x_t} e^{i\langle k,x_t\rangle}}\right\rangle.
    \end{align*}
    Taking the Fourier transform of above equation, we get
    \begin{align*}
        \partial_t\rho = \nabla\cdot(\expectCond{\dot x_t}{x_t=x}\rho).
    \end{align*}
    It remains to show that $b(t,x)=\expectCond{\dot x_t}{x_t}$. We have
    \begin{align*}
        \expectCond{\dot x_t}{x_t} &= \dot g(t)\expectCond{x^*}{x_t=x} + \dot r(t)\expectCond{z}{x_t=x},\\
        & = \dot g(t)\expectCond{x^*}{x_t=x}-r(t)\dot r(t)s(t,x),
    \end{align*}
    where we used the result that $\expectCond{z}{x_t=x} = -r(t)s(t,x)$.
    Fokker-Planck equation (\ref{eqn_app:FPE}) can be readily obtained by noting that $\nabla\cdot(s\rho) = \Delta\rho$.
\end{proof}


\begin{lemma}\label{lemma_app:b_and_s}
    Let $x_t$ be a linear stochastic interpolant. Let $s(t,x) = \nabla \rho(t,x)$ and $b(t,x) = \dot g(t)\expectCond{x^*}{x_t=x}-r(t)\dot r(t)s(t,x)$. Then, we have
\begin{equation}\label{eqn_app:b_and_s}
\begin{split}
  b(t,x) &= \begin{cases}
      \frac{\dot r(t)}{r(t)}x+\left(\dot g(t)-\frac{g(t)\dot r(t)}{r(t)}\right)\expectCond{x^*}{x_t=x},\\
      \frac{\dot g(t)}{g(t)}x+\left(r^2(t)\frac{\dot g(t)}{g(t)}-\dot r(t)r(t)\right)s(t,x),
      \end{cases}\\
s(t,x) &= \frac{g(t)\expectCond{x^*}{x_t=x}-x}{r^2(t)}.  
\end{split}
\end{equation}
\end{lemma}
\begin{proof}
    We have $x_t=g(t)x^*+r(t)z$. Taking expectation conditioned on $x_t=x$, we get
    \begin{equation*}
        x = g(t)\expectCond{x^*}{x_t=x}+r(t)\expectCond{z}{x_t=x}.
    \end{equation*}
    A direct computation (also known as the Tweedie's formula) yields that $\expectCond{z}{x_t=x} = -r(t)s(t,x)$. Hence we can derive $s(t,x)$ from $\expectCond{x^*}{x_t=x}$ and vice-versa. This proves the claims in the Lemma.
    
\end{proof}


\begin{lemma}\label{lemma_app_app:velociyt_HJB_PDE}
    Let $u:[0,T]\times\R^d:\rightarrow\R$ be the function given in (\ref{eqn:velocity}). Then $u$ satisfies the following Hamilton-Jacobi-Bellman equation
\begin{equation}\label{eqn_app:velocity_pde}
    \partial_t u + \frac{\sigma^2}{2}\Delta u + \frac{\sigma^2}{2}\norm{\nabla u}^2+\mu^T\nabla u = 0,
\end{equation}
where $\sigma^2(t) = 2\frac{r^2(t)}{\beta^2(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)$ and $\mu(t)= - \partial_t\log\left(\frac{\beta(t)g(t)}{r^2(t)}\right)x$.
\end{lemma}
\begin{proof}
    From the definition of $u$ in (\ref{eqn:velocity}), we can directly compute $\partial_tu, \nabla u$ and $\Delta u$, which gives
    \begin{align*}
        \partial_t u &= \frac{\int_{\R^d}\nu(dx^*)\left(\partial_t\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\partial_t\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2\right)e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}{\int_{\R^d}\nu(dx^*)e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}\\
        \nabla u &= \frac{\int_{\R^d}\nu(dx^*)\left(\frac{\beta(t)g(t)}{r^2(t)}\right)x^*e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}{\int_{\R^d}\nu(dx^*)e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}\\
        \nabla^2 u &= \frac{\int_{\R^d}\nu(dx^*)\left(\frac{\beta(t)g(t)}{r^2(t)}\right)^2x^*{x^*}^Te^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}{\int_{\R^d}\nu(dx^*)e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}-\nabla u\nabla u^T\\
        \Delta u &= \frac{\int_{\R^d}\nu(dx^*)\left(\frac{\beta(t)g(t)}{r^2(t)}\right)^2\norm{x^*}^2e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}{\int_{\R^d}\nu(dx^*)e^{\frac{\beta(t)g(t)}{r^2(t)}<x,x^*>-\frac{g^2(t)}{2r^2(t)}\norm{x^*}^2}}-\norm{\nabla u}^2.
    \end{align*}
This shows that $u$ satisfies the PDE (\ref{eqn_app:velocity_pde}).
\end{proof}

\begin{lemma}\label{lemma_app:pde_fbsde}
    Let $u:[0,T]\times\R^d:\rightarrow\R$ be the solution to PDE (\ref{eqn:velocity_pde}). Then the processes $Y_t$ and $Z_t$ in (\ref{eqn:fbsde_half_interpolant}) is given by $Y_t=u(t,X_t)$ and $Z_t=\sigma(t)\nabla u(t,X_t)$.
\end{lemma}
\begin{proof}
    We can rewrite the PDE (\ref{eqn:velocity_pde}) as 
    \begin{equation*}
        \partial_t u + \frac{\sigma^2}{2}\Delta u - \frac{\sigma^2}{2}\norm{\nabla u}^2 + (\mu+\sigma\nabla u)^T\nabla u = 0.
    \end{equation*}
    Thus, appealing to the connection between PDEs and FBSDEs established in the Preliminaries, we identify that $Y_t = u(t,X_t)$ and $Z_t=\sigma(t)\nabla u(t,X_t)$.
\end{proof}

\begin{lemma}\label{lemma_app:score_pde}
    Let $v(t,x) = \log\rho(t,\alpha(t)x)+d\log g(t)$. Then $v$ satisfies the following Hamilton-Jacobi-Bellman equation
\begin{equation}\label{eqn_app:score_pde}
    \partial_t v+\frac{\bar{\sigma}^2}{2}\Delta v+\frac{\bar{\sigma}^2}{2}\norm{\nabla v}^2+\partial_t\log{\frac{g(t)}{\alpha(t)}}x^T\nabla v=0,
\end{equation}
where $\bar{\sigma}^2 = 2\frac{r^2(t)}{\alpha^2(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)$.
\end{lemma}
\begin{proof}
\begin{align*}
    \partial_t v(t,x) &= (\partial_t\log\rho)(t,\alpha(t)x)+\dot\alpha x^T(\nabla\log\rho)(t,\alpha(t)x)+d\frac{\dot g(t)}{g(t)},\\
    &= -(b^T\nabla \log\rho)(t,\alpha x)-(\nabla\cdot b)(t,\alpha x)+\dot\alpha x^T(\nabla\log\rho)(t,\alpha(t)x)+d\frac{\dot g(t)}{g(t)},\\
    &= -\left(\frac{\dot g}{g}x+\frac{\bar{\sigma}^2}{2}\nabla v\right)^T\nabla v - \frac{\bar{\sigma}^2}{2}\Delta v + \frac{\dot \alpha}{\alpha}x^T\nabla v.
\end{align*}
where $\bar{\sigma}^2 = 2\frac{r^2(t)}{\alpha^2(t)}\left(\frac{\dot g(t)}{g(t)}-\frac{\dot r(t)}{r(t)}\right)$. Simplifying, we get
\begin{equation}
    \partial_t v+\frac{\bar{\sigma}^2}{2}\Delta v+\left(\left(\frac{\dot g}{g}-\frac{\dot \alpha}{\alpha}\right)x+\frac{\bar{\sigma}^2}{2}\nabla v\right)^T\nabla v=0.
\end{equation}
\end{proof}

\section{Implementation Details}\label{apndx:implemnetation_details}
We provide the details of our implementation here. Our implementation is based on the JAX\footnote{https://jax.readthedocs.io/en/latest/index.html} framework. We rely on the automatic differentiation feature in JAX to compute the gradients of functions required in our algorithms. Table~\ref{tab:hyperparams} gives the hyperparameters of the sampler that we used in our simulations. 
 All our simulations are performed with $T=1$. We note that choosing a small $T$ would require the neural network to represent functions with a large Lipshitz constant, thus increasing the complexity of the neural network. We next present the details of the neural networks used to represent the solution to the PDEs (\ref{eqn:velocity_pde}),(\ref{eqn:score_pde}) and (\ref{eqn:pde_velocity_interpolant}). 

\subsection{Neural networks}
In score-based generative modeling, the score function is parameterized directly by a neural network. Our formulation of solving PDEs using FBSDE requires us to have a representation for log-likelihood. Consequently, we utilize energy-based models and obtain the score function by taking the gradient of log-likelihood. However, since we consider unnormalized densities, we use only the score function to obtain the loss. Similar to the approach taken in \cite{zhang_path_2022,vargas_denoising_2022,berner_optimal_2023}, we integrate the terminal condition of PDEs into the architecture of the neural network. In particular, we use a neural network of the following form:
\begin{equation*}
     \Phi^\theta(t,x) = \Phi_1^{\theta_1}(t,x)+\Phi_2^{\theta_2}(t)\phi(x),
\end{equation*}
where $\theta = \{\theta_1,\theta_2\}$ and $\phi$ is the terminal condition of the PDE under consideration. We initialize the networks $\Phi_1^{\theta_1}$ with $0$ and $\Phi_2^{\theta_2}$ with $1$ so that $u^\theta(T,\cdot)=\phi(\cdot)$ in the beginning. Both networks use Fourier Embedding \cite{tancik_fourier_2020} to encode time. The architecture of the neural networks $\Phi_1^{\theta_1}$ and $\Phi_2^{\theta_2}$ can be described as follows:
$$
\Phi_1^{\theta_1}(t,x) = L_1\circ\varrho\circ L\circ\varrho\circ L\circ\varrho\bigg(L(x)+L\circ\varrho\circ L\circ\text{Fourier}(t)\bigg), 
$$
$$
\Phi_2^{\theta_2}(t) = L_1\circ\varrho\circ L\circ\varrho\circ L\circ\varrho\circ L\circ\text{Fourier}(t),
$$
where $\text{Fourier}(\cdot)$ is the Fourier embedding function with $128$ outputs, $L$ is a linear dense layer with $64$ outputs, $L_1$ is a linear dense layer with single output and $\varrho$ is the GELU \cite{hendrycks_gaussian_2023} activation function. We mention that this neural network architecture is similar to the one used in \cite{berner_optimal_2023}. However, the above neural network implements a scalar function that approximates the solution to a PDE, whereas the neural network in  \cite{berner_optimal_2023} approximates the gradient of the solution to the PDE.

For a full interpolant-based sampler, as explained earlier, we need to solve two PDEs. This requires two neural networks--one for $t\in[0,T']$ and another for $t\in[T',T]$. The networks we use are as follows:
$$
u^\theta(t,x) = \Phi_1^{\theta_1}(t,x)+\Phi_2^{\theta_2}(t)(-\psi(T',x)),
$$
$$
v^{\theta'}(t,x) = \Phi_1^{\theta_1'}(t,x)+\Phi_2^{\theta_2'}(t)\varphi(x),
$$
where $\psi$ is the function that appear in (\ref{eqn:velocity}) and $\varphi$ is given in (\ref{eqn:score_term_condition}.
% To make a fair comparison, in our experiments, we use the same network for half interpolant-based sampler as well.

\subsection{Training}
We use the Adam optimizer \cite{kingma_adam_2017} to optimize the weights of the neural networks. We use a piecewise linear scheduling for the learning rate with its value becoming 1/5th every 2000 training steps. We also do exponential moving averaging of the neural network weights. We detached the process \ref{eqn:fbsde_init_process} from the computational graph while computing the gradients. Table~\ref{tab:hyperparams} shows the values of the training-related hyperparameters used in our experiments.

\begin{table}
  \caption{Hyperparameters}
  \label{tab:hyperparams}
  \centering
  \begin{tabular}{ll}
    \toprule
    \multicolumn{2}{c}{\textbf{Sampler}} \\
    $T$ & 1.0\\
    $T'$ & 0.5\\
    $\beta(t)$ & $\frac{r(t)}{g(t)}$\\
    $\alpha(t)$ & 1.0\\
    $\delta$ & $5*10^{-6}$\\
    $\lambda$ & $2000/\delta$\\
    \midrule
    \multicolumn{2}{c}{\textbf{Training}}\\
    Number of steps & 10000\\
    Batch Size & 128\\
    Initial Learning rate     & $5*10^{-3}$\\
    Max Grad Norm   & 1.0\\
    SDE Steps & 60\\
    \midrule
    \multicolumn{2}{c}{\textbf{Sampling}}\\
    SDE steps & 1000\\
    Samples & 10000\\
    $\eps$ & 1.0\\
    \bottomrule
  \end{tabular}
\end{table}
Figure~\ref{fig:trainphase} shows the training loss as a function of training steps. The plot on the left is for GMM with $d=2$, and the plot on the right is for MoG ($d=100$).
\begin{figure}[!htb]
    \centering
        % \centering
        \includegraphics[scale=0.5]{images/trainPhase.png}
        \caption{Training phase plot at $d=2$ and $d=100$}
        \label{fig:trainphase}
\end{figure}

\subsection{Sampling}
We sample using procedures given in \ref{proc:sample_half_int} and \ref{proc:smaple_int}. The number of discretization steps we use is $1000$. We use a constant diffusion coefficient while sampling. See Table~\ref{tab:hyperparams}. 

\section{Estimating the normalization constant}\label{apndx:est_logZ}
One of the remarkable features of diffusion-based sampling approaches is that they allow for estimating the normalization constant of the given unnormalized density. We describe here the method for estimating the normalization constant for the half interpolant-based sampler. The log normalization constant can be estimated similarly for a full interpolant-based sampler. Let $u$ be the function given in (\ref{eqn:velocity}). We know $u$ is the solution to PDE (\ref{eqn:velocity_pde}) under the terminal condition \ref{eqn:terminal_cond_vel_pde}. Let $X_t$ be the solution to an SDE given by
\begin{equation*}
    dX_t = \left(\sigma^2(t)\nabla u(t,X_t)+ \mu(t,X_t)\right)dt + \sigma(t)dW_t,\quad X_0=0.
\end{equation*}
Applying It\^o's lemma to $u(t,X_t)$, we have
\begin{align*}
    du(t,X_t) &= \partial_t u(t,X_t)dt + \nabla u(t,X_t)^TdX_t + \frac{1}{2}\sigma^2(t)\Delta u(t,X_t)dt\\
    &=  \left(\partial_t u+\frac{1}{2}\sigma^2(t)\Delta u+\left(\sigma^2(t)\nabla u +\mu(t,X_t)\right)^T\nabla u\right)dt + \sigma(t)\nabla u^TdW_t\\
    &=  \frac{1}{2}\sigma^2(t)\norm{\nabla u(t,X_t)}^2dt + \sigma(t)\nabla u^TdW_t\\
\end{align*}
Integrating from $0$ to $T$, we get
\begin{equation}\label{eqn:logZ_est}
    u(0,0) = u(T,X_T)-\frac{1}{2}\int_0^T\sigma^2(t)\norm{\nabla u(t,X_t)}^2dt-\int_0^T\sigma(t)\nabla u^TdW_t
\end{equation}
Equation (\ref{eqn:logZ_est}) gives a viable strategy to estimate the normalization constant. Suppose we want to sample from $\pi = \frac{\hat{\pi}}{Z}$, given only $\hat{\pi}$. From the definition of $u$, we have $u(T,X_T) = \log\frac{\pi(\beta(T)X_T)}{\psi(T,\beta(T)X_T)} = \log\frac{\hat{\pi}(\beta(T)X_T)}{\psi(T,\beta(T)X_T)} - \log Z $. Moreover, we have $u(0,0) = 0$. Hence, we get
\begin{equation*}
0 = -\frac{1}{2}\int_0^T\sigma^2(t)\norm{\nabla u(t,X_t)}^2dt-\int_0^T\sigma(t)\nabla u^TdW_t + \log\frac{\hat{\pi}(\beta(T)X_T)}{\psi(T,\beta(T)X_T)} - \log Z,
\end{equation*}
which after taking expectation gives
\begin{multline}
    \log Z = \mathbb{E}\bigg[-\frac{1}{2}\int_0^T\sigma^2(t)\norm{\nabla u(t,X_t)}^2dt-\int_0^T\sigma(t)\nabla u^TdW_t\\ +\log\hat{\pi}(\beta(T)X_T)-\log(\psi(T,\beta(T)X_T))\bigg]
\end{multline}

\section{Additional Experiments and Results}\label{apndx:additionalExps}
\begin{figure}
  \centering
  \includegraphics[scale = 0.55]{images/dist_kde_plots.png}
  \caption{KDE plots of Gaussian mixture, and Rings distributions.}
  \label{fig:kde_plots_tech}
\end{figure}
Figure~\ref{fig:kde_plots_tech} shows the kernel density estimation (KDE) plots of samples obtained by our methods for 2-dimensional Gaussian mixture distribution and Rings distribution. These plots illustrate that our method can sample from distributions with multiple modes. 
%Table~\ref{tab:estimates_tech} shows the estimates of different quantities from these samples.
Figure~\ref{fig:estimates_functionals} shows the estimates of $\mathbb{E}|X|_1$ (Mean Absolute) and $\mathbb{E} \norm{X}^2$ (Mean Squared) computed from the samples obtained using full interpolant based sampler with different interpolants. Refer to Table~\ref{tab:estimates_tech} for the estimates at the end of the training for the interpolant $g(t)=\text{sin}(\pi t/2), r(t)=\text{cos}(\pi t/2)$. These estimates are without correction using important weights and hence indicate the true quality of the samples.
\begin{figure}
  \centering
  \includegraphics[scale = 0.35]{images/evals.png}
  \caption{Estimates of $\mathbb{E}|X|_1$ and $\mathbb{E}\norm{X}^2$ as a function of training steps.}
  \label{fig:estimates_functionals}
\end{figure}

\begin{table}
  \caption{Estimates (True values in the paranthesis)}
  \label{tab:estimates_tech}
  \centering
  \begin{tabular}{llll}
    \toprule
     \multirow{2}{*}{Quantity} & \multicolumn{3}{c}{Distributions} \\
     \cmidrule(r){2-4}
                               &    GMM          & DW$(d=10,w=3,\delta=2)$   & DW$(d=20,w=5,\delta=3)$  \\
     \midrule
             $\mathbb{E}{|X|_1}$     &    6.04$\pm$0.18 (7.19) & 9.75$\pm$0.04 (9.54)  &22.59$\pm$0.87 (20.42)    \\
              $\mathbb{E}{\norm{X}^2}$     &  28.36$\pm$1.05 (35.26) & 13.32$\pm$0.08 (12.51)  &36.11$\pm$2.88 (29.54)  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Sampling from a statistical physics inspired high-dimensional distribution}\label{sec:gaussian_spin_glass}
Now, we consider a statistical physics \textit{spin glass} model \cite{arous_aging_2001, barra_about_2014}. The precise definition is slightly different in these two works and we follow the incarnation of \cite{barra_about_2014}. This model was introduced as a ``soft spin'' version of the celebrated {\it spherical} Sherrington-Kirkpatrick model \cite{kosterlitz_spherical_1976}. The free energy can be computed exactly and here serves as a benchmark to assess the experiments. 

The probability density of this soft spin spherical spin glass is given by
\begin{equation*}
    \pi(x) = \frac{1}{Z}\exp\left\{\frac{\beta}{\sqrt{2d}}\sum_{i,j=1}^dA_{ij}x_ix_j-\frac{\beta^2}{4d}\left(\sum_{i=1}^dx_i^2\right)^2-\frac{1}{2}\sum_{i=1}^dx_i^2\right\},
\end{equation*}
where $A_{ij}\overset{\text{i.i.d.}}{\sim}\cN{0,I_d}$. The asymptotic rigorous prediction for the free energy \cite{barra_about_2014} is given by
$$
\lim_{d\rightarrow\infty}\frac{1}{d}\mathbb{E}\log Z = \begin{cases}
    0 &\text{for}\;\beta < 1,\\
    -\frac{1}{2}\log\beta + \frac{\beta\bar{q}}{2}+\frac{\beta^2\bar{q}^2}{d},\quad \bar{q} = \frac{\beta-1}{\beta^2} &\text{for}\;\beta \ge 1
\end{cases}
$$
where the limit is attained almost surely, in other words it is equal to the limit of the expectation with respect to $A_{ij}\overset{\text{i.i.d.}}{\sim}\cN{0,I_d}$).
The theory predicts that there is a phase transition at $\beta=1$.

We use our full interpolant-based sampler to estimate $\log Z$ for different values of $\beta$. In Figure~\ref{fig:logZ_gsg} we compare the obtained estimate of $\frac{1}{d}\log Z$ with the theoretical prediction. The estimates obtained through our sampler are consistent with the phase transition. The experimental points correspond to one instance of teh coupling matrix $A_{ij}$ indicating that already for $d=100$ the free energy is well concentrated around its average. 

\begin{figure}
  \centering
  \includegraphics[scale = 0.7]{images/logZ.png}
  \caption{Estimate of $\frac{1}{d}\log Z$ for Gaussian Spin Glass model with $d=100$.}
  \label{fig:logZ_gsg}
\end{figure}

\subsection{Training and Sampling Times}
\begin{table}
  \caption{Training and sampling times}
  \label{tab:times}
  \centering
  \begin{tabular}{LLL}
    \toprule
     Distribution & Training time \par per step (mS) & Sampling time per \par discretization step (mS) \\
     \midrule
    GMM & 97.7 & 9.3\\
    Funnel & 99.7 & 10.3\\
    Double well ($d=10$) & 102.7 & 9.8\\
    Double well ($d=20$) & 105.0 & 10.8\\
    Spin Glass ($d=100$) & 98.1 & 10.4\\
    \bottomrule
  \end{tabular}
\end{table}
In this section, we discuss the typical training and sampling time for our sampler. We ran our experiments on an NVIDIA GeForce GTX 1070 (8GB) GPU. In comparison to the conventional MCMC algorithms, a disadvantage of our methods is that we have learnable parameters. However, the training time required can be amortized in time if we are interested in drawing a large number of samples from a specific target distribution. Table~\ref{tab:times} shows the training time per gradient descent step required for different distributions. We take a total of 10,000 gradient descent steps during training. Hence, the total time required for training is around 17 minutes for each of the distributions. Table~\ref{tab:times} also shows the time required during the sampling phase per discretization step of the SDE. The number of Euler-Maruyama discretization steps we take is 1000, which amounts to a total of 10 seconds for drawing 10,000 samples. 

\subsection{Comparison to Langevin Diffusion for Sampling}
\begin{figure}
  \centering
  \includegraphics[scale = 0.55]{images/langevin.png}
  \caption{KDE plots of GMM samples generated by Langevin diffusion-based algorithm and full interpolant-based algorithm for different number of discretization steps.}
  \label{fig:langevin}
\end{figure}

\begin{table}
  \caption{Comparison of estimates using Langevin and Full interpolant sampler.}
  \label{tab:langevin_estimates}
  \centering
  \begin{tabular}{cccccc}
    \toprule
     Distribution & Steps & \multicolumn{2}{c}{$\mathbb{E}|X|_1$} & \multicolumn{2}{c}{$\mathbb{E}\norm{X}^2$} \\
     \cmidrule(r){3-6}
     & & LMC & FIS & LMC & FIS\\
     \midrule
    \multirow{4}{*}{GMM} & 10 & 1.1 & 5.5 & 1.5 & 25.2\\
    & 100 & 1.1 & 5.9 & 1.7 & 28.0\\
    & 1000 & 1.6 & 6.0 & 4.4 & 28.0\\
    & 10000 & 4.7 & 6.0 & 21.3 & 28.4\\
    \midrule
    \multirow{4}{*}{DW($d=10$)} & 10 & 8.1 & 9.7 & 9.9 & 13.3\\
    & 100 & 9.0 & 9.72 & 11.42 & 13.2\\
    & 1000 & 9.7 & 9.5 & 12.5 & 13.3\\
    & 10000 & 9.7 & 9.5 & 12.5 & 13.3\\
    \midrule
    \multirow{4}{*}{DW($d=20$)} & 10 & 16.4 & 22.0 & 20.6 & 22.15\\
    & 100 & 19.5 & 22.2 & 27.3 & 34.6\\
    & 1000 & 20.5 & 22.2 & 29.6 & 34.6\\
    & 10000 & 20.4 & 22.1 & 29.4 & 34.5\\
    \bottomrule
  \end{tabular}
\end{table}

To illustrate the benefits of our method, we compare it to the vanilla Langevin diffusion-based sampling algorithm. A Langevin diffusion is given by the solution to the SDE:
\begin{equation*}
    dS_t = \eta\nabla\log\pi(S_t)dt + \sqrt{2\eta} dW_t.
\end{equation*}
 It is well known that the distribution of $S_t$ as $t$ goes to infinity is $\pi$. A sampling scheme based on Langevin diffusion is given by its discretization:
 \begin{equation*}
     S_{n+1} = S_n + \nabla\log\pi(S_t)\delta + \sqrt{2\delta}w_{n+1},
 \end{equation*}
 where $w_i\overset{i.i.d.}{\sim}\cN{0,I_d}$. In Figure~\ref{fig:langevin} we compare the KDE plots of samples generated by a Langevin diffusion-based algorithm ($\delta=0.1$) and a full interpolant-based algorithm as a function of the number of discretization steps for a Gaussian mixture model. In this experiment, we made sure that the time required per discretization step is the same for both algorithms. We observe that the Langevin diffusion requires a large number of steps to explore all the modes of the distribution, while our algorithm explores all the modes even with merely 10 discretization steps. A comparison of estimates of $\mathbb{E}|X|_i$ and $\mathbb{E}\|X\|^2$ of Gaussian mixture and double well distributions obtained through Langevin Monte Carlo (LMC) and Full interpolant sampler (FIS) is given in Table~\ref{tab:langevin_estimates}.
\subsection{Comparison with Generalized Bridge Sampler (GBS)}\label{sec:comparison_gbs}
\begin{figure}
  \centering
  \includegraphics[scale = 0.4]{images/evals_logZ_HD.png}
  \caption{Estimates of $\frac{1}{d}\log Z$ as a function of training steps given by FIS and GBS for different targets and dimensions. }
  \label{fig:gbs_comp_plot}
\end{figure}

In Figure~\ref{fig:gbs_comp_plot} we compare the performance of FIS (Full Interpolant Sampler with trigonometric interpolant) and GBS (General Bridge Sampler) \cite{richter_improved_2023, blessing_beyond_2024} as a function of training steps for two different target distributions and for dimensions $d=10,100,200$. Here, MoG refers to a Mixture of isotropic Gaussian distribution with 10 components with their mean selected uniformly at random from $[-10,10]^d$ and MoS is a Mixture of Student t-distribution (degree of freedom = 2) with 10 components with their mean selected uniformly at random from $[-10,10]^d$. We chose to compare our algorithm with GBS because of their similar computational complexity. The implementation of GBS was taken from \cite{blessing_beyond_2024}.

% \begin{table}[!htb]
%   \footnotesize
%   \label{tab:gbs_com_wass}
%   \centering
%   \begin{tabular}{cccccccc}
%     \toprule
%      \multirow{3}{*}{Metric} & \multirow{3}{*}{Algorithm} &  \multicolumn{6}{c}{Distributions}\\
%      \cmidrule(r){3-8}
%      & & \multicolumn{3}{c}{MoG} & \multicolumn{3}{c}{MoS} \\
%      \cmidrule(r){3-8}
%      & & $d=10$ & $d=100$ & $d=200$ & $d=10$ & $d=100$ & $d=200$ \\
%      \midrule
%     \multirow{2}{*}{$\log Z$ (True value = 0)} & FIS &  $-2.43 \pm 0.02$ & $-47.24 \pm 26.1$ & $-183.56 \pm 90.24$ & $-2.68 \pm 0.04$ & $-56.75 \pm 9.5$ & $-203.77 \pm 23.65$\\
%     & GBS &  $-0.02 \pm 0.03$ & $-8.96 \pm 1.25$ & $-23.91 \pm 1.26$ &  $-2.22 \pm 0.63$ & $-143.3 \pm 3.11$ & $-342.7 \pm 12.24$\\
%     \midrule
%     \multirow{2}{*}{2-Wasserstein} & FIS &  $243.5 \pm 5$ & $5088 \pm 688$ & $10035 \pm 1136$ & $285.9 \pm 110$ & $3156 \pm 1253$ & $9208 \pm 1653$\\
%     & GBS &  $6.21 \pm 0.04$ & $147.8 \pm 0.83$ & $323.4 \pm 2.3$ &  $317.1 \pm 8.8$ & $14932 \pm 76$ & $34573 \pm 3650$\\
%     \bottomrule
%   \end{tabular}
%   \caption{Estimates of $\log Z$ and 2-Wasserstein distance obtained using FIS and GBS for different targets and dimensions.}
% \end{table}

\begin{table}[!htb]
  % \caption{Comparison of FIS and GBS algorithms across different distributions and dimensions.}
  \caption{Estimates of $\log Z$ and 2-Wasserstein distance obtained using FIS and GBS for different targets and dimensions.}
  \label{tab:gbs_com_was}
  \centering
  \begin{tabular}{cccccc}
    \toprule
     Distribution & Dimension & \multicolumn{2}{c}{$\log Z$ (True value = 0)} & \multicolumn{2}{c}{2-Wasserstein} \\
     \cmidrule(lr){3-4} \cmidrule(lr){5-6}
     & & FIS & GBS & FIS & GBS \\
     \midrule
    \multirow{3}{*}{MoG} & $d=10$ & $-2.43 \pm 0.02$ & $-0.02 \pm 0.03$ & $243.5 \pm 5$ & $6.21 \pm 0.04$ \\
     & $d=100$ & $-47.24 \pm 26.1$ & $-8.96 \pm 1.25$ & $5088 \pm 688$ & $147.8 \pm 0.83$ \\
     & $d=200$ & $-183.56 \pm 90.24$ & $-23.91 \pm 1.26$ & $10035 \pm 1136$ & $323.4 \pm 2.3$ \\
     \midrule
    \multirow{3}{*}{MoS} & $d=10$ & $-2.68 \pm 0.04$ & $-2.22 \pm 0.63$ & $285.9 \pm 110$ & $317.1 \pm 8.8$ \\
     & $d=100$ & $-56.75 \pm 9.5$ & $-143.3 \pm 3.11$ & $3156 \pm 1253$ & $14932 \pm 76$ \\
     & $d=200$ & $-203.77 \pm 23.65$ & $-342.7 \pm 12.24$ & $9208 \pm 1653$ & $34573 \pm 3650$ \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:gbs_com_was} compares the performance of FIS and GBS at the end of training (10000 iterations). The experimental setting is the same as that of Figure~\ref{fig:gbs_comp_plot}. The metrics we use for comparison are 
 and 2-Wasserstein distance (Entropy regularized optimal transport cost) \cite{blessing_beyond_2024}. Based on the observations in Figure~\ref{fig:gbs_comp_plot} and Table~\ref{tab:gbs_com_was}, it is not possible to definitively conclude that one method is better than the other, as their performance may depend on the specific distribution being considered.
 \subsection{Comparison with other Diffusion-based Samplers}
 Existing diffusion-based samplers that could be considered closest to our work are PIS \cite{zhang_path_2022}, DIS \cite{berner_optimal_2023}, and DDS \cite{vargas_denoising_2022}. These methods facilitates the use of important sampling techniques to correct the samples while estimating funcitonals. The estimate of $\log Z$ of GMM without importance sampling is mentioned in \cite{zhang_path_2022}, and it has a bias of -0.44. In this setting, our method has a lower bias given by $-0.26$. The $\log Z$ estimate of the Funnel distribution reported in \cite{zhang_path_2022} is much better than ours. However, as noted in \cite{berner_optimal_2023} and \cite{vargas_denoising_2022}, the Funnel distribution used in \cite{zhang_path_2022} has a favourable variance compared to the standard Funnel distribution that we used.

 Training of neural networks in \cite{zhang_path_2022}, DIS \cite{berner_optimal_2023}, and DDS \cite{vargas_denoising_2022} involves computing gradient of a Neural SDE, which can turn quite expensive computationally. However, our methods do not take gradient with respect to the process (\ref{eqn:fbsde_init_process}).  

 \subsection{Effect of different parameters}\label{sec:ablation}
 Table~\ref{tab:ablation} shows the influence of different hyperparameters on the estimates. The target distribution used is GMM ($d=2$).
 \begin{table}[!htb]
\caption{Influence of different hyperparameters on the estimates obtained. Here, $c_1 = 5*10^{-6}$, $c_2=4*10^{8}$.}
    \centering
    \begin{tabular}{ccccc}
        \toprule
         Hyperparameter & Value & \multicolumn{3}{c}{Estimates}\\
         \cmidrule(r){3-5}
         & & $\log Z$ & $\mathbb{E}|X|_1$ & $\mathbb{E}\norm{X}^2$\\
         \midrule
        \multirow{4}{*}{$\delta$} & $0.1c_1$ & $-0.66 \pm 0.10$ & $4.69 \pm 0.39$ & $20.44 \pm 2.26$\\
        & $1.0c_1$ & $-0.25 \pm 0.02$ & $6.05 \pm 0.24$ & $28.42 \pm 1.37$\\
        & $10c_1$ & $-0.67 \pm 0.19$ & $4.27 \pm 0.71$ & $18.41 \pm 3.93$ \\
        \midrule
        \multirow{4}{*}{$\lambda$} & $0.1c_2$ & $-0.30 \pm 0.02$ & $5.85 \pm 0.17$ & $27.25 \pm 0.97$\\
        & $1c_2$ & $-0.26 \pm 0.01$ & $6.02 \pm 0.21$ & $28.28 \pm 1.18$\\
        & $10c_2$ & $-0.74 \pm 0.16$ & $4.1 \pm 0.55$ & $17.4 \pm 3.05$ \\
        \midrule
        \multirow{4}{*}{SDE steps} & 30 & $-0.31 \pm 0.04$ & $5.86 \pm 0.31$ & $27.45 \pm 1.77$\\
        & 60 & $-0.27 \pm 0.02$ & $6.05 \pm 0.22$ & $28.47 \pm 1.22$\\
        & 100 & $-0.27 \pm 0.03$ & $6.07 \pm 0.20$ & $28.55 \pm 1.17$ \\
        \bottomrule
      \end{tabular}
    \label{tab:ablation}
\end{table}
 
 
