\section{Related Work}
\label{sec:background}

\textbf{Iterative Driving Scene Reconstruction}. Iterative driving scene reconstruction methods perform test-time per-scene fitting by incrementally updating the scene representation until reaching a predefined step count or convergence criteria. This makes them infeasible for real-time applications compared to feed-forward methods. Iterative scene reconstruction methods can broadly be classified into NeRF-based and 3D Gaussian-based approaches. 

% Neural Field-based
An early NeRF-based method, **Mildenhall et al., "Neural Scene Representation & Rendering"**____ decomposes a scene into static and dynamic objects, representing their relation via a hierarchical directed graph. Since then, a number of methods have used scene graphs or decompositions into static, dynamic, and flow fields to model scenes ____.
These methods often require LiDAR data and are slow to render, with training times exceeding 30 minutes in some cases. **Bemana et al., "MARS: Masked Recursive Autoencoders for Scalable Scene Understanding"**____ similarly uses a foreground-background decomposition, while **Li et al., "READ: Real-time End-to-end Avatars and Dynamics"**____ and **Kupyn et al., "StreetSurfing: Photorealistic Image Synthesis with Disentangled Representations"**____ focus only on static scenes.

% Gaussian-based
Gaussian-based parameterizations have gained popularity due to their ability to perform real-time rendering. Many of them model backgrounds and objects separately, using bounding boxes to identify the objects. Several Gaussian-based approaches also use scene graphs ____.
To alleviate artifacts per-scene reconstruction pipelines have been combined with diffusion-based priors ____ or by enforcing symmetry ____.
Most per-scene optimization methods rely on posed input images, though a few methods also jointly learn poses ____.
Despite being real-time renderable, offline reconstruction remains time-intensive and often LiDAR-dependent, limiting real-time applicability. \\

% putting it here to fix it in the right place
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{imgs/sshELF_method.pdf}
    \caption{\textbf{Overview of sshELF}. Given a few input images, sshELF first encodes them into latent features using a pre-trained DinoV2 (Sec.\ref{image_encoder}). As part of the \textit{backbone}, the latent features, together with a pre-trained depth head, are used to initialize the virtual views, which are refined using hierarchical ELF blocks consisting of cross- and self-attention layers (Sec. \ ref{backbone}).
    Reference and virtual views are then fed into the \textit{translator} part to predict 3D Gaussian splats (Sec. \ref{translator}). Not shown here is the rasterization part used for creating novel views (Sec. \ ref{rendering_nvs}).}
    \label{fig:sshELF_model}
\end{figure*}

\noindent \textbf{Few-View Reconstruction}. Iterative per-scene optimization is time-consuming and constrained in its generalizability, prompting the development of feedforward methods. 

% Neural Field-based
Early NeRF-based methods retrieve image features via view projection and aggregate the resulting features ____.
Some methods apply additional constraints using diffusion priors ____ or time-consuming iterative diffusion-based refinement ____.
Many of the mentioned papers focus on small-scale scenes or single objects. A method focusing on single-shot prediction in driving scenarios is **Dewan et al., "DistillNeRF: Distilling Single-Shot Priors from Per-Scene Optimization"**____, which distills single-shot priors from the per-scene optimization method EmerNeRF.
Closest to our work are **Zhou et al., "Neo360: NeRF-based 360-Degree Video Reconstruction"**____, which is limited to inward-facing views, and **Yen-Chia Liang et al., "6Img-to-3D: Real-Time 3D Scene Understanding from Multi-View Images"**____, which uses a slow triplane-based representation.

% Gaussian-based
Gaussian-based methods explicitly model scenes and enable a simpler single-shot parameterization compared to neural rendering approaches. Recent works focus on single objects or small-scale scenes ____,
but require input views with significant overlap. **Liu et al., "Flash3D: Real-Time 3D Scene Reconstruction"**____ uses depth prediction but fails to inpaint unseen regions. Methods like **Wang et al., "pixelSplat: Real-Time Novel View Synthesis via Pixel-Level Cross-Attention"**____, **Chen et al., "latentSplat: Latent Diffusion-based Novel View Synthesis"**____, and **Kuang et al., "MVSplat: Multi-View Scene Understanding for Real-Time 3D Reconstruction"**____ leverage cross-attention for image pairs, excelling in close-range novel view synthesis but struggling with large camera displacements. Concurrent work **Zhang et al., "DrivingForward: Driving Scenes Reconstruction from NuScenes Dataset"**____ reconstructs driving scenes from nuScenes but is limited to small viewpoint changes between consecutive timeframes.


Many few-shot 3D reconstructions and novel view synthesis methods overload the 3D Gaussian predictor to inpaint occluded parts of the scene by predicting several Gaussians per ray. This causes blurriness in the unseen parts of the scene, which are far away from the input views. Unlike existing methods, our work utilizes intermediate representations to generate unobserved views and thus obtain a more complete scene reconstruction.