%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% own imports
\usepackage{float}
\usepackage{adjustbox}      % adjust input figures
\usepackage{bm}             % math font 
\usepackage{multirow}       % multiple rows / colums in a table
\usepackage[symbol]{footmisc}   % footnote symbols
\usepackage{tabularx}
\definecolor{tabfirst}{rgb}{1, 0.7, 0.7} % red
\definecolor{tabsecond}{rgb}{1, 0.85, 0.7} % orange
\definecolor{tabthird}{rgb}{1, 1, 0.7} % yellow

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\def\transp{\mathsf{T}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{sshELF: Single-Shot Hierarchical Extrapolation of Latent Features}

\begin{document}

\twocolumn[
\icmltitle{sshELF: Single-Shot Hierarchical Extrapolation of Latent Features \\ for 3D Reconstruction from Sparse-Views}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Eyvaz Najafli}{equal,con,tue}
\icmlauthor{Marius Kästingschäfer}{equal,con,fre}
\icmlauthor{Sebastian Bernhard}{con}
\icmlauthor{Thomas Brox}{fre}
\icmlauthor{Andreas Geiger}{tue}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\author{%
  \textbf{Eyvaz Najafli}$^{1,3,*}$
  \quad
  \textbf{Marius Kästingschäfer}$^{1,2,*}$ 
  \quad
  \textbf{Sebastian Bernhard}$^{1}$\\
  \quad
  \textbf{Thomas Brox}$^{2}$
  \quad
  \textbf{Andreas Geiger}$^{3}$\\
  $^1$Continental
  \quad
  $^2$University of Freiburg
  \quad
  $^3$University of Tübingen\\
  \texttt{eyvaz.najafli@student.uni-tuebingen.de}\\
  \texttt{marius.kaestingschaefer@continental.com}
}

\icmlaffiliation{con}{Continental}
\icmlaffiliation{tue}{University of Tübingen}
\icmlaffiliation{fre}{University of Freiburg}

\icmlcorrespondingauthor{Eyvaz Najafli}{eyvaz.najafli@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Marius Kästingschäfer}{marius.kaestingschaefer@continental.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Computer Vision, Few-View-to-3D, Autonomous Driving}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Reconstructing unbounded outdoor scenes from sparse outward-facing views poses significant challenges due to minimal view overlap. 
Previous methods often lack cross-scene understanding and their primitive-centric formulations overload local features to compensate for missing global context, resulting in blurriness in unseen parts of the scene. 
We propose sshELF, a fast, single-shot pipeline for sparse-view 3D scene reconstruction via hierarchal extrapolation of latent features. 
Our key insights is that disentangling information extrapolation from primitive decoding allows efficient transfer of structural patterns across training scenes.
Our method: (1) learns cross-scene priors to generate intermediate virtual views to extrapolate to unobserved regions, (2) offers a two-stage network design separating virtual view generation from 3D primitive decoding for efficient training and modular model design, and (3) integrates a pre-trained foundation model for joint inference of latent features and texture, improving scene understanding and generalization. 
sshELF can reconstruct 360$^{\circ}$ scenes from six sparse input views and achieves competitive results on synthetic and real-world datasets. 
We find that sshELF faithfully reconstructs occluded regions, supports real-time rendering, and provides rich latent features for downstream applications. The code will be released.
\end{abstract}

\section{Introduction}
\label{sec:intro}

% \def\thefootnote{*}\footnotetext{Equal contribution.}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/teaser2.png}
    \caption{\textbf{Overview}. Given a number of input images, sshELF first reconstructs several virtual views and only then predicts the 3D Gaussian primitives of the scene from which novel views are rendered. The colors of the latent information correspond to different object classes, such as purple for buildings and green for vegetation. 
    \vspace{-0.4cm}}
    \label{fig:teaser}
\end{figure}

We consider the problem of reconstructing unbounded outdoor scenes from sparse outward-facing cameras with very little overlap between adjacent views. Solving this problem poses two fundamental challenges: (1) resolving distant object occlusions (areas hidden behind terrain or other vehicles) and ego-occlusions (regions obscured by the sensor platform itself, for example below the vehicle), and (2) overcoming limited multi-view correspondence cues due to minimal overlap. In practical autonomous driving applications, this dual challenge becomes even more demanding since dense bird's-eye views or occupancy maps are required in real-time.

% We consider the problem of reconstructing unbounded outdoor scenes from a few outward-facing cameras, where minimal overlap between adjacent views necessitates the extrapolation of occluded regions.

% disadvantages of existing related work: slow during inference (NeRF), small viewpoint changes, require per-scene optimization, what else?
% (Nerf, per-scene GS, GS)

While traditional neural radiance fields (NeRFs) and 3D Gaussian Splatting have advanced novel view synthesis, their reliance on per-scene optimization and dense view coverage limits applicability in real-world scenarios with sparse, non-overlapping inputs \cite{mildenhall2020nerf, kerbl3Dgaussians}.  Existing per-scene optimization methods are often tested on inward-facing datasets with high view overlap \cite{realestate10K, reizenstein2021commonobjects3dlargescale, deitke2022objaverseuniverseannotated3d} or where novel views are close to the input views, simplifying cross-view triangulation. In contrast, vehicle-mounted cameras are usually outward-facing with minimal camera overlap \cite{behley_semantickitti_2019, caesar_nuscenes_2020, Sun_2020_CVPR} and operate in large, unbounded outdoor environments. Recent feedforward approaches aim to generalize across scenes but struggle with large viewpoint changes \cite{MVSplat, yu2020pixelnerf}, some lack support for multi-view aggregation necessary for 360-degree surround-view synthesis, \cite{szymanowicz2024flash3d}, or are not real-time renderable \cite{gieruc20246imgto3d}.

Although Vision transformers trained on large datasets for metric depth prediction can provide useful priors \cite{bhat2023zoedepth, yin2023metric3d, marigold_ke2023repurposing}, the resulting depth maps, when combined with pixel information, are inadequate for generating complete 3D representations \cite{surround_monodepth2022, yang2024scaleawaresurroundmonodepthtransformers, SEED4D}. A key limitation is their inability to account for unobserved regions, including areas occluded by terrain or other objects and those obscured by the sensor platform itself, such as the ground beneath the vehicle. As a result, subsequent reconstructions exhibit incomplete geometry in those regions. Furthermore, when reconstructing geometry from multiple depth maps simultaneously, multi-view scale inconsistency at the border regions leads to artifacts. Thus resulting in low-quality 3D reconstructions and inadequate novel views \cite{szymanowicz2024flash3d, SEED4D}. \\
\vspace{-0.4cm}

\begin{figure} %[ht!]
    \centering
    \includegraphics[width=1\linewidth]{imgs/virtual_views.jpg}
    \caption{\textbf{Reference, Virtual and Novel Views}. An example showing input views in green, a set of virtual views in red, and potential novel views in blue. Virtual view generation is key to enhancing representational capacity and extrapolating to unobserved scene areas.
    \vspace{-0.4cm}}
    \label{fig:virtual_views}
\end{figure}

\noindent \textbf{Problem Statment}. \label{subsec:problem_statement} Our work tackles the problem of fast single-shot sparse-view 3D reconstruction for outdoor traffic scenes. We introduce an efficient, performant \underline{s}ingle-\underline{s}hot pipeline for sparse-view 3D reconstruction via \underline{h}ierarchical \underline{e}xtrapolation of \underline{l}atent \underline{f}eatures called \textbf{sshELF}. The paper is based on three key insights. 

First, existing models are constrained in their ability to infer unseen regions and views far from the input images since their representational capacity is limited \cite{MVSplat, pixelsplat_Charatan_2024_CVPR}. By restricting the process only to the information present in the input images, without intermediate steps or representations, the models struggle to generalize to unobserved parts of the scene. Unlike previous methods, sshELF generates several intermediate virtual views that help to reconstruct unseen regions. This way, our method is not restricted to the information in the given input images. In figure \ref{fig:virtual_views} we visualize example input, virtual and novel views.

Second, our network is decomposed into a \textit{backbone} that generates virtual views and a \textit{translator} that decodes the reference and virtual views into explicit Gaussian primitives from which novel views can be rendered in real-time. The separation enables the backbone to increase the information content while allowing the translator to lift higher-quality Gaussian primitives. As a byproduct, the decomposition facilitates isolated training of the two stages, leading to a significant reduction in computational requirements. Unlike wide-baseline volumetric representation \cite{LaRa}, our virtual views do not require dense sampling and have a lower memory overhead. Compared to end-to-end training, the decomposed approach further increases both the number and resolution of virtual views the backbone can generate. This thereby increases the amount of information one can pass to the translator. The partitioning into the backbone and translator further makes the design process more flexible since the sub-networks can be trained independently.

% foundation model aspect
Third, large pre-trained foundation models such as DinoV2 \cite{oquab2023dinov2} and near-metric depth estimation methods \cite{depth_anything_v2} are underutilized when employed solely as input to the model \cite{szymanowicz2024flash3d}. Unlike previous works, we incorporate pre-trained feature extractor and depth estimation models directly into our architecture. Incorporating pre-trained models as fundamental building blocks enables our backbone to output both texture and latent information for reference and virtual views. Our results outperform previous methods, suggesting that the rich training signal plays a crucial role. Properly leveraging the intermediate latent features of a pre-trained foundation model subsequently enables the optimal use of depth prediction models relying on multi-stage latent features. Moreover, obtaining latent information alongside Gaussian primitives opens up potential downstream applications such as semantic scene understanding or 3D detection. See Figure \ref{fig:teaser} for an example latent clustering.

Overall, sshELF is a fast two-stage single-shot unbounded 3D scene reconstruction pipeline, particularly well suited for outward-facing cameras with little view overlap. The contributions of this paper can be summarized as follows:
%\begin{enumerate}
\textbf{(1) Reconstruct Occluded Regions}. Our method reconstructs occluded regions faithfully where other methods fail, as shown by competitive results on both synthetic and real-world data.
\textbf{(2) High Speed and Visual Quality}. Our method can perform fast end-to-end 360$^{\circ}$ scene reconstruction and novel view synthesis from six unbounded surround vehicle views in 0.18s. sshELF can render far-away viewpoints with high quality as measured on synthetic and real-world data.
\textbf{(3) Optimal Utilization of Latent Information}. Since our method jointly predicts latent and texture information, as well as depth, we can leverage latent features to obtain insights into spatial semantics, occupancy, and geometry.
%\end{enumerate}

\noindent We perform an in-depth analysis to justify our architectural choices and compare our final model with multiple state-of-the-art approaches. We will make our code available.

\section{Related Work}
\label{sec:background}

\textbf{Iterative Driving Scene Reconstruction}. Iterative driving scene reconstruction methods perform test-time per-scene fitting by incrementally updating the scene representation until reaching a predefined step count or convergence criteria. This makes them infeasible for real-time applications compared to feed-forward methods. Iterative scene reconstruction methods can broadly be classified into NeRF-based and 3D Gaussian-based approaches. 

% Neural Field-based
An early NeRF-based method, Neural Scene Graph (NSG)~\cite{NSG_2021_CVPR} decomposes a scene into static and dynamic objects, representing their relation via a hierarchical directed graph. Since then, a number of methods have used scene graphs or decompositions into static, dynamic, and flow fields to model scenes \cite{ml_nsg_2024_CVPR, NeuRAD_Tonderski_2024_CVPR, turki2023suds, yang2023emernerf}. These methods often require LiDAR data and are slow to render, with training times exceeding 30 minutes in some cases. MARS~\cite{wu2023mars} similarly uses a foreground-background decomposition, while READ~\cite{li2022read} and StreetSurf~\cite{guo2023streetsurf} focus only on static scenes.

% Gaussian-based
Gaussian-based parameterizations have gained popularity due to their ability to perform real-time rendering. Many of them model backgrounds and objects separately, using bounding boxes to identify the objects. Several Gaussian-based approaches also use scene graphs~\cite{Hugs_Zhou_2024_CVPR, yan2024street, chen2024omnire, zhou2024drivinggaussian}. To alleviate artifacts per-scene reconstruction pipelines have been combined with diffusion-based priors~\cite{VEGS, Yu2024SGDSV} or by enforcing symmetry~\cite{khan2024autosplatconstrainedgaussiansplatting}. Most per-scene optimization methods rely on posed input images, though a few methods also jointly learn poses~\cite{chen2023periodic, li2024_VDG}. Despite being real-time renderable, offline reconstruction remains time-intensive and often LiDAR-dependent, limiting real-time applicability. \\

% putting it here to fix it in the right place
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{imgs/sshELF_method.pdf}
    \caption{\textbf{Overview of sshELF}. Given a few input images, sshELF first encodes them into latent features using a pre-trained DinoV2 (Sec.\ref{image_encoder}). As part of the \textit{backbone}, the latent features, together with a pre-trained depth head, are used to initialize the virtual views, which are refined using hierarchical ELF blocks consisting of cross- and self-attention layers (Sec. \ref{backbone}). Reference and virtual views are then fed into the \textit{translator} part to predict 3D Gaussian splats (Sec. \ref{translator}). Not shown here is the rasterization part used for creating novel views (Sec. \ref{rendering_nvs}).}
    \label{fig:sshELF_model}
\end{figure*}

\noindent \textbf{Few-View Reconstruction}. Iterative per-scene optimization is time-consuming and constrained in its generalizability, prompting the development of feedforward methods. 

% Neural Field-based
Early NeRF-based methods retrieve image features via view projection and aggregate the resulting features~\cite{yu2020pixelnerf, wang2021ibrnet, mvsnerf}. Some methods apply additional constraints using diffusion priors~\cite{ReconFusion_Wu_2024_CVPR} or time-consuming iterative diffusion-based refinement~\cite{szymanowicz2023viewset_diffusion, RenderDiffusion_Anciukevicius_2023_CVPR}. Many of the mentioned papers focus on small-scale scenes or single objects. A method focusing on single-shot prediction in driving scenarios is DistillNeRF~\cite{wang2024distillnerf}, which distills single-shot priors from the per-scene optimization method EmerNeRF. Closest to our work are Neo360~\cite{irshad2023neo360}, which is limited to inward-facing views, and 6Img-to-3D~\cite{gieruc20246imgto3d}, which uses a slow triplane-based representation.

% Gaussian-based
Gaussian-based methods explicitly model scenes and enable a simpler single-shot parameterization compared to neural rendering approaches. Recent works focus on single objects or small-scale scenes~\cite{xu2024grm, TriplaneGaussian_Zou_2024_CVPR, yang2024gaussianobject, szymanowicz24splatter}, but require input views with significant overlap. Flash3D~\cite{szymanowicz2024flash3d} uses depth prediction but fails to inpaint unseen regions. Methods like pixelSplat~\cite{pixelsplat_Charatan_2024_CVPR}, latentSplat~\cite{wewer2024latentsplat}, and MVSplat~\cite{MVSplat} leverage cross-attention for image pairs, excelling in close-range novel view synthesis but struggling with large camera displacements. Concurrent work DrivingForward~\cite{tian2024drivingforwardfeedforward3dgaussian} reconstructs driving scenes from nuScenes but is limited to small viewpoint changes between consecutive timeframes.


Many few-shot 3D reconstructions and novel view synthesis methods overload the 3D Gaussian predictor to inpaint occluded parts of the scene by predicting several Gaussians per ray. This causes blurriness in the unseen parts of the scene, which are far away from the input views. Unlike existing methods, our work utilizes intermediate representations to generate unobserved views and thus obtain a more complete scene reconstruction.

\section{Method}
\label{sec:methods}

% naming convention
We distinguish between reference, virtual, and novel views. Reference views describe the captured viewpoints fed as input into the architecture, and novel views describe the novel synthesized viewpoints. While previous work focuses on reference and novel views, we additionally introduce virtual views that define intermediate viewpoints between them, facilitating the inpainting of unobserved regions in the final reconstruction. 

Given $n_{\text{ref}}$ reference views containing RGB images $\bm{I}_{\text{ref}} \in \mathbb{R}^{3\times H\times W}$, their associated camera extrinsics $\bm{P}_{\text{ref}} = \left[\bm{R}_{\text{ref}} | \bm{T}_{\text{ref}} \right] \in \mathbb{R}^{3\times4}$ and intrinsics $\bm{K}_{\text{ref}} \in \mathbb{R}^{3\times3}$, sshELF generates consistent 3D geometry and synthesize $n_{\text{nvs}}$ novel surround views $\bm{I}_{\text{nvs}} \in \mathbb{R}^{3\times H\times W}$. The method is visualized in Figure~\ref{fig:sshELF_model}. The number of reference, virtual, and novel views can be flexibly varied within our pipeline. The image encoder, backbone, translator, and rendering process are described in the following sections.

\subsection{Image Encoder}\label{image_encoder}

Given images $\bm{I}_{\text{ref}}$, sshELF applies a pre-trained self-supervised vision transformer (ViT) \cite{dosovitskiy2020vit} to obtain patch-wise latent features $\bm{L}_{\text{ref}}=\{\boldsymbol{l}^{i}_{\text{ref}} \mid i=1\text{,...,} n_{\text{ref}} \} \in{\mathbb{R}^{n_{\text{ref}} \times (d_E + 3) \times (H / 14) \times (W / 14)}}$, and a class token [CLS] $\{\boldsymbol{g}^{i}_{\text{ref}}\}^{n}_{i=1}\in{\mathbb{R}^{d_E}}$ containing aggregated global feature information.  The model retrieves latent embeddings from the last $n$ ViT blocks at various depths, each of which is a $d_E$ dimensional vector. We use DINOv2~\cite{oquab2023dinov2} since it is semantically rich and retains geometric information well. To optimally preserve texture information, the normalized and resized RGB tensor is concatenated with each of the $n$ layer's patch embeddings, increasing the channel dimension of $\boldsymbol{l}^{i}_{\text{ref}}$ to $d_E + 3$.

\subsection{Backbone: Generating Virtual Views}\label{backbone}
% Feature Extrapolation

Given the intrinsics $\bm{K}_{\text{ref}}$ and extrinsic $\bm{P}_{\text{ref}}$ of the reference views, along with ground-truth image information and intrinsics $\bm{K}_{\text{vrt}}$ and extrinsic $\bm{P}_{\text{vrt}}$ of $n_{\text{vrt}}$ virtual views, the task of the backbone is to construct latent and texture information for the virtual views. During cross-scene training we randomly sample the virtual views with probabilities proportional to the degree of overlap with the reference views to maximize the information content of occluded regions. 

To initialize $n_{\text{vrt}}$ virtual views, texture information from the reference views is projected into 3D and back-projected onto the virtual views using point cloud rendering~\cite{ravi2020pytorch3d}. To obtain the required depth maps $\bm{D}_i$, the latents $\bm{L}_{\text{ref}}$ without texture information obtained from the reference views are fed into a fine-tuned dense depth prediction transformer~\cite{DPT_Ranftl_2021_ICCV}. The resulting depth maps $\bm{D_i}$ can then be used to project pixel information from the reference views into 3D space by
\vspace{-0.0cm}
%
\begin{equation} \label{equation:reference_feature_lift}
    \mathbf{M_i} = \begin{bmatrix}
    \mathbf{R}_{\text{ref}} & \mathbf{T}_{\text{ref}} \\
    \mathbf{0}^\transp & 1
  \end{bmatrix}^{-1}
  \begin{bmatrix}
    \mathbf{K}_{\text{ref}} & \mathbf{0} \\
    \mathbf{0}^\transp & 1
  \end{bmatrix}^{-1} \text{ } \mathbf{\hat{m}_{i}}\text{,}
\end{equation}
%
where $\mathbf{\hat{m}_{i}} = \begin{bmatrix}\mathbf{m_{i}} & 1 & 1/\mathbf{D}_{i} \end{bmatrix}^\transp$ representing the pixel coordinate in image space and $M_i$ representing the 3D point in world coordinates. In the next step, the feature information from the reference views is projected into 3D space to initialize the latent and texture information of the virtual views $\tilde{\bm{L}}_{\text{vrt}}=\{\boldsymbol{l}^{i}_{\text{vrt}} \mid i=1\text{,...,}n \}$. The initialized virtual views are now more informative than randomly initialized ones since they contain approximate texture and latent information, but they still suffer from occlusions and other artifacts. To refine the virtual views, we hierarchically \underline{e}xtrapolate the \underline{l}atent \underline{f}eatures via \textbf{ELF} blocks, each denoted as $\mathcal{\bm{E}(\cdot)}$. Ground Truth reference view information, virtual view initializations, and the output from the previous blocks are fed as input to the corresponding ELF block. In each ELF block, a succession of $cc$ cross-attention denoted as \textbf{CA} and $cs$ self-attention denotes as \textbf{SA} operations are applied to transfer information between reference and virtual views, followed by an \textbf{MLP} layer:
\vspace{-0.2cm}
% 
\begin{align}
    \begin{split}\label{equation:local_elf_iterations_1}
    \boldsymbol{\hat{l}}^{i}_{\text{vrt}} \text{  } &= \boldsymbol{\hat{l}}^{i-1}_{\text{vrt}} + \boldsymbol{\tilde{l}}^{i}_{\text{vrt}}  
    \end{split}\\
    \begin{split}\label{equation:local_elf_iterations_2}
    C(\boldsymbol{\hat{l}}^{i}_{\text{ref}},  \boldsymbol{\hat{l}}^{i}_{\text{vrt}})_{j} + &= \textbf{CA}(C(\boldsymbol{\hat{l}}^{i}_{\text{ref}}, \boldsymbol{\hat{l}}^{i}_{\text{vrt}})_{j}) \text{ }\text{ } \forall j \in\{1, \ldots, c c\}
    \end{split}\\
    \begin{split}\label{equation:local_elf_iterations_3}
    C(\boldsymbol{\hat{l}}^{i}_{\text{ref}}, \boldsymbol{\hat{l}}^{i}_{\text{vrt}})_{k} + &=\textbf{SA}(C(\boldsymbol{\hat{l}}^{i}_{\text{ref}}, \boldsymbol{\hat{l}}^{i}_{\text{vrt}})_{k}) \text{ }\text{ }  \forall k \in\{1, \ldots, c s\}
    \end{split}\\
    \begin{split}\label{equation:local_elf_iterations_4}
    C(\boldsymbol{\hat{l}}^{i}_{\text{ref}}, \boldsymbol{\hat{l}}^{i}_{\text{vrt}}) +&= \textbf{MLP}(C(\boldsymbol{\hat{l}}^{i}_{\text{ref}}, \boldsymbol{\hat{l}}^{i}_{\text{vrt}}))\text{,}
    \end{split}
\end{align}
%
where $C(\cdot\text{, }\cdot)$ concatenates two feature maps along the view dimension. The ELF block performs an inpainting-like refinement, addressing occlusions and enhancing texture. By predicting features for both virtual and reference views, we introduce a cycle-consistency constraint, ensuring that the ELF block preserves the reference features as close to the ground truth as possible while reconstructing virtual views. Following~\cite{wewer2024latentsplat}, we use epipolar geometry to inform and constrain the cross-attention computation described in equation \eqref{equation:local_elf_iterations_2}. A sequence of CNN and MLP layers is applied on the output of equation \ref{equation:local_elf_iterations_4} to infer the \texttt{[CLS]} token from the latent features.

In the final stage of the backbone, the resulting multi-stage latent codes for the virtual views are passed through a pre-trained depth head to obtain near-metric depth maps $D_{vrt}$. To achieve a complete scene reconstruction, we concatenate latent, texture, and depth predictions for the virtual views with the ground truth information from the reference views and feed them all as input to the translator. 

\subsection{Translator: Lifting to 3D}\label{translator}

The translator transforms the features resulting from the backbone into 3D Gaussian primitives. The input consists of aggregated latent, color, and approximate depth information of both reference and virtual views.
\vspace{-0.1cm}

\begin{equation} \label{definition:backbone_output}
    C(\boldsymbol{b}_{\text{ref}}\text{, }\boldsymbol{b}_{\text{vrt}})  \in {\mathbb{R}^{(n_{\text{ref}} + k_{\text{vrt}}) \times (d_E + 3 + 1) \times (h) \times (w)}}
\end{equation}

where $h \times w$ is the spatial dimension of the latent embeddings. We utilize a UNet-like architecture \cite{unet_2015, song2021scorebased} to map from each view's backbone features to 3D Gaussian splats. The translator architecture $\mathcal{T}(\cdot)$ consists of two parts: an encoder and a decoder. The output of equation \ref{definition:backbone_output} is fed into the encoder, which is passed through a single layer of \textbf{ELF} block $ \mathcal{\bm{E}}$ to enforce multi-view consistency over reference and virtual views on the lowest layers in the UNet. The decoder then predicts splats per view whereby intermediate skip connections between the decoder and the encoder help to preserve fine-grained details. The resulting Gaussian splats are projected to ego vehicle coordinates, concatenated across spatial and views dimensions. The scene is now parameterized with the 3D Gaussian primitives
\vspace{-0.1cm}

\begin{equation} \label{equation:gaussian_predict}
    \{\bm{\mu}_i, \bm{\Sigma}_{i}, \alpha_{i}, \mathbf{s}_{i}\}^{(n_{\text{ref}} + n_{\text{vrt}}) \times N}_{i=1} 
\end{equation}
%
where $\bm{\mu}_{i}\in\mathbb{R}^{3}$ is the per pixel location of the Gaussians, $\bm{\Sigma}_{i}\in\mathbb{R}^{3\times 3}$ is the covariance, $\alpha_{i} \in [0, 1)$ the opacity and $\mathbf{s}_{i}\in\mathbb{R}^{(l+1)^2}$ represents the coefficients for the spherical harmonics of degree $l$ and $N$ stands for the number of Gaussians generated per view. To ensure that the covariance matrix remains positive semi-definite, we predict a diagonal scaling matrix $\mathbf{S}$ and orthonormal rotation matrix $\mathbf{R}$ such that $\mathbf{\Sigma}=\mathbf{R} \mathbf{S} \mathbf{S}^\transp \mathbf{R}^\transp$, parameterized by the axis-scales $\mathbf{s} \in\mathbb{R}^{3}$ and $\mathbf{q} \in\mathbb{R}^{4}$ defining a normalized quaternion \cite{kerbl3Dgaussians}. To alleviate local minima during the learning process of 3D primitives, we apply a probabilistic depth map prediction similar to pixelSplat \cite{pixelsplat_Charatan_2024_CVPR}.

\subsection{Rendering Novel Views}\label{rendering_nvs}
% Splatting

Given extrinsics $\bm{P}_{\text{nvs}}$ and intrinsics $\bm{K}_{nvs}$, we render novel views $\bm{\hat{I}}_{\text{nvs}}$ using gaussian rasterization. To compute the unnormalized density of the $i^{th}$ 3D Gaussian the following function is applied 
\vspace{-0.0cm}
%
\begin{equation} \label{equation:gaussian_unnormalized_density}
    G_{i}(\mathbf{x}) = \text{exp}\Bigl( -\frac{1}{2}(\mathbf{x} - \bm{\mu}_i)^\transp \mathbf{\Sigma}^{-1}_{i} (\mathbf{x} - \bm{\mu}_i) \Bigl).
\end{equation}

The color of the Gaussians $\bm{c}\in\mathbb{R}^3$ when viewed from direction $\bm{d}\in\mathbb{R}^3$ is computed by summing the spherical harmonics basis $\bm{c}(\bm{d}) = \sum_{i=1}^{n} s_{i} \mathcal{B}_i (\bm{d})$, here $ \mathcal{B}_i$ is the $i^{\mathrm{th}}$ spherical harmonics basis function. Finally, the pixel intensity $\bm{c}$ is computed from the $(n_{\text{ref}} + n_{\text{vrt}}) \times N$ ordered Gaussians using alpha compositing in the following way
\vspace{-0.1cm}

\begin{equation}
\boldsymbol{c}=\sum_{i=1}^n \boldsymbol{c}_i \alpha_i \prod_{j=1}^{i-1}\left(1-\alpha_j\right)
\end{equation}

The Gaussians can be rendered in real-time following \cite{kerbl3Dgaussians}. The full architecture remains end-to-end differentiable; however, we train the backbone and the translator separately to be able to increase the number and resolution of inferred virtual views. 

\subsection{Training Objectives}

During cross-scene training, the model is compelled to learn transferable structural priors, enabling generalization across different scenes. We assume the virtual views to be available for supervision, which can be sampled from the existing data in practice. This allows us to obtain the ground truth virtual view features by feeding them through the pre-trained ViT blocks to get $\{\boldsymbol{l}^{i}_{\text{vrt}}\}_{i=1}^n$ for the training of the backbone. The translator can be trained separately with ground truth reference and virtual views. \\

\noindent \textbf{Backbone Objectives}. The backbone reconstructs virtual views consisting of latent features and texture information from reference views. An MSE loss between reconstructed features and ground truth features is applied at every stage of the backbone:
\vspace{-0.0cm}
%
\begin{equation}
    \mathcal{L}_{bb} = 
    \lambda_1 \mathcal{L}_{\text{MSE}}\left(\boldsymbol{\hat{L}}_{\text{vrt}}, \boldsymbol{L}_{\text{vrt}} \right) + 
    \lambda_2 \mathcal{L}_{\text{MSE}}\left(\boldsymbol{\hat{L}}_{\textbf{ref}}, \boldsymbol{L}_{\text{ref}} \right) 
\end{equation}
%
where $\lambda_1$, $\lambda_2 > 0$. The first part of the loss enforces the backbone to reconstruct intermediate and final latent features and low-resolution texture information of the virtual views. As an additional constraint, the second part of the loss enforces processed reference features to be similar to the ground truth reference features. This can be viewed as a cycle-consistency constraint whereby the information flowing from reference views to virtual views and then back should be maintained with minimal change. \\

\noindent  \textbf{Translator Objectives}. The translator predicts the Gaussian primitives from which the novel views are rasterized. The translator is trained with:
\vspace{-0.0cm}
%
\begin{equation}
    \mathcal{L}_{tr} = 
     \lambda_3 \mathcal{L}_{\text{MSE}}\left(\boldsymbol{ \hat{I}}_{\text{nvs}}, \boldsymbol{I}_{\text{nvs}} \right) + 
    \lambda_4 \mathcal{L}_{\text{MAE}}\left(\boldsymbol{\hat{D}}_{\text{ref}}, \boldsymbol{D}_{\text{nvs}} \right) 
\end{equation}
%
where $\lambda_3>0$, $\lambda_4 \geq 0$ and $\boldsymbol{\hat{D}}_{\text{ref}}$ is the $Z$-buffer (also known as depth buffer) retrieved from the renderer, which provides a depth approximation \cite{kerbl3Dgaussians}. This way, the error enforces texture and geometric correspondence where ground-truth depth maps are available, in the case of nuScenes $\lambda_4$ is set to zero. When point cloud information is available, we also experimented with adding a Chamfer distance loss.

\section{Experiments}
\label{sec:results}

A number of experiments are conducted to assess the capabilities of our method. We use synthetic and real-world driving data (Section \ref{subsec:datasets}) to test the performance of our method and consistency of our results across datasets. Our experimental setting highlights implementation details and applied metrics (Section \ref{subsec:experiment_settings}). Our quantitative and qualitative results (Section \ref{subsec:results}) show the visual quality and the high inference speed of our method. Our analysis and ablations (Section \ref{subsec:ablation}) demonstrate the necessity of the different model components.

\subsection{Datasets}\label{subsec:datasets}

\textbf{SEED4D}. We make use of the publically available Synthetic Ego--Exo Dynamic 4D (SEED4D) dataset \cite{SEED4D}. The dataset consists of synthetic ego- and exocentric views. The static version of the dataset that we use for training contains a total of 212k images from 2k driving scenes. Per scene six outward-facing ego-vehicle images and 100 spherical images for supervision exist. The ego-centric camera setup resembles the relative camera placement within the nuScene dataset, whereby the overlap between adjacent outward-looking views is minimal. We use the ego-vehicle images as reference views and sample virtual and novel views from the exocentric views. We follow the default split and use Town 1, 3 to 7, and 10 for training and reserving 100 scenes from Town 2 for testing. This configuration provides 1900 locations for cross-scene training. \newline

\noindent \textbf{NuScenes}. We additionally test on the well-established nuScenes dataset \cite{caesar_nuscenes_2020}. It comprises six ego vehicle views with only 10\% view overlap \cite{tian2024drivingforwardfeedforward3dgaussian} from 1000 driving scenes of 20 seconds. Each sequence comprises around 240 frames per camera, of which 40 are keyframes. We only use the keyframes during our experiments and follow the default split of 700 scenes for training and 150 for testing.
Since NuScenes does not contain exocentric views, we construct a multi-view evaluation setup by aggregating egocentric views captured across temporal sequences. In our framework, we utilize views with a temporal difference (TD) of zero (i.e., simultaneous captures from all vehicle-mounted cameras) as reference views. Novel view synthesis targets are then defined at TD=2, 3, and 4 timesteps after reference timestep, equivalent to 1s, 1.5s, and 2s temporal offsets respectively. Virtual views are place between reference and novel views.

\subsection{Experimental Setup}\label{subsec:experiment_settings}

\textbf{Baselines}. We compare our method against a number of methods from the original SEED4D paper and additional recent few-image novel-view-synthesis baselines. PixelNeRF~\cite{yu2020pixelnerf} uses projected image feature for conditioning a neural radiance field. SplatterImage~\cite{szymanowicz24splatter} predicts pixel-aligned 3D Gaussian primitives using a U-net.  MVSplat~\cite{MVSplat} utilizes cross-attention, a cost volume, and a pre-trained depth model. 6Img-to-3D~\cite{gieruc20246imgto3d} uses self- and cross-attention for parameterizing a triplane together with image feature projection. PixelSplat~\cite{pixelsplat_Charatan_2024_CVPR} utilizes epipolar cross-attention and performs a probabilistic prediction of pixel-aligned Gaussians. For nuScenes we focus exclusively on the most recent real-time capable methods.   \\

\noindent \textbf{Implementation Details}. We implement sshELF using PyTorch \cite{PyTorch_NeurIPS2019}, a memory-efficient attention mechanism \cite{xFormers2022} and the renderer implementation from the original 3DGS paper \cite{kerbl3Dgaussians}. Each hierarchical ELF block consists of two (cc) epipolar-cross attention parts and one (cs) self-attention block. The total number of ELF blocks is four in the backbone and one in the translator.

During cross-scene training, we set the number of reference views to 6, the number of virtual views to 6, and the number of novel views to 2. 
For the SEED4D dataset, the input views have a resolution of $896 \times 896$, the resolution of the virtual views in the backbone is $64 \times 64$, the DINO features are obtained with an input resolution of $896 \times 896$, and novel views have a resolution of $256 \times 256$.
When working with the nuScenes dataset, most values remain the same except for the rendering resolution of sshELF, which is increased to $896 \times 896$. The same resolutions are used for SplatterImage, pixelSplat, and MVSplat.
 
The translator is trained on ground truth reference and virtual views until convergence. Once the backbone is converged, the translator is fine-tuned with the estimated virtual views until convergence. We leverage the knowledge learned over the synthetic dataset and continue training on the nuScene dataset from the best model checkpoints for 100K steps. During the training of the backbone $\lambda_1 = 1000.0$, $\lambda_2 = 0.1$ and for the translator training $\lambda_3 = 100.0$ and $\lambda_4 = 0.001$. 
We use an Adam optimizer \cite{Kingma_Adam} and an intial learning rate of $1 \times 10^{-5}$, cosine annealing.
The backbone is trained using an A40 GPU with 48 GB, and the translator using a V100 GPU with 32 GB.\\

\noindent \textbf{Evaluation Metrics}. Performance is measured using the peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) \cite{ssim}, and learned perceptual image patch similarity (LPIPS) \cite{zhang2018unreasonable}. We additionally compute the depth root mean square error (D-RMSE) where ground truth metric depth is available or the Chamfer distance when LiDAR data is accessible.

\subsection{Results}\label{subsec:results}

\textbf{SEED4D}. As shown in Table~\ref{tab:results_seed4d}, sshELF outperforms all previous methods in terms of PSNR, and ranks second in SSIM and D-RMSE. Other methods suffer from incomplete geometry, particularly in hidden or sensor-blocked regions. sshELF achieves a runtime of 0.182 seconds, demonstrating competitive performance. Notably, sshELF's end-to-end performance is more than 15x faster than the second-best model, 6Img-to-3D.
%
\begin{table}[!ht]
    \centering
    %\vspace{-0.2cm}
    
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l c c c c c}
        \toprule
          Methods & \textbf{PSNR} & \textbf{SSIM} & \textbf{LPIPS} & \textbf{D-RMSE} & \textbf{Time} \\
         \midrule
          ZoeDepth & 5.47  & 0.25 &   0.56 &  11.73 & -- \\
          Metric3D  & 6.31  & 0.30 &  0.55 &  10.05 & -- \\
          NeRFacto   & 10.94  &  0.30 &  0.79 & -- & -- \\
          K-Planes  & 11.36  &  0.46 &   0.63 & -- & -- \\
          SplatFacto  & 11.61  &  0.49 &   0.66 &  -- & $\approx$  480s \\
          MVSplat  & 13.86 &  0.46 &  0.66 & 16.79 & \cellcolor{tabfirst}0.42ms \\
          PixelNeRF  & 14.50 & 0.55 & 0.65 & 19.24 & 1.86s\\
          SplatterImg.   & 17.79  & 0.58 &  0.57 & 11.05 & \cellcolor{tabthird}32ms  \\ 
          pixelSplat & \cellcolor{tabthird}18.03 & \cellcolor{tabthird}0.60 & \cellcolor{tabfirst}0.44 & \cellcolor{tabthird}7.26 &  \cellcolor{tabsecond}1.1ms \\ 
          6Img-to-3D  & \cellcolor{tabsecond}18.68 & \cellcolor{tabfirst}0.73 & \cellcolor{tabsecond}0.45 & \cellcolor{tabfirst}6.23 & 2.85s\\     
          \bottomrule
          sshELF (Ours) & \cellcolor{tabfirst}18.93 &  \cellcolor{tabsecond}0.65 & \cellcolor{tabthird}0.50  & \cellcolor{tabsecond}6.61 & 182ms\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{SEED4D Results}. Runtime comparison of scene-to-novel view inference, presented in both seconds and milliseconds to account for the large variations in execution time across different methods.}
    \label{tab:results_seed4d}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{imgs/seed4d_qualitiative.jpg}
    \caption{\textbf{Qualitative Novel View Synthesis Comparison on SEED4D Test Set}. Comparison of large-baseline novel view synthesis under sparse observation conditions. Six ego-centric input frames (top row) with limited overlap serve as reference views. We evaluate each method's ability to reconstruct exo-centric with a large offset to the input views.}
    \label{fig:seed4d_qualitiative}
\end{figure}

\noindent \textbf{NuScenes}. Quantitative multi-timestep results in Table~\ref{tab:results_nuscenes} demonstrate our method's superiority across visual and geometric metrics. The virtual view sampling in sshELF enables strategic allocation of scene representation capacity: by prioritizing reconstruction fidelity for distant viewpoints critical for wide-baseline tasks, our method inherently trades off minor quality reductions in near-field regions. In contrast methods like MVSplat~\cite{MVSplat} and PixelSplat~\cite{pixelsplat_Charatan_2024_CVPR}, which reprojecting input pixels onto local planes—a design that limits scalability to far-view synthesis. This approach fails to model occlusions or parallax effects at larger distances as seen in Figure \ref{fig:seed4d_qualitiative} for MVSplat. Compared to the baselines, our method more accurately represents color information and reconstructs occluded regions with greater fidelity.

\begin{table}[!ht] %[!ht]
    %\small
    \setlength{\tabcolsep}{2pt}
    %\vspace{-0.2cm}
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{l l cccc }
        \toprule
          & Methods & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{Chamfer}$\downarrow$ \\
         \midrule
       \multirow{3}{*}{\rotatebox{90}{\textbf{TD2}}} \rule{3pt}{0em} & MVSplat  & \cellcolor{tabthird}16.624 & \cellcolor{tabthird}0.436 & \cellcolor{tabsecond}0.553 & \cellcolor{tabsecond}646.11  \\
        & pixelSplat  & \cellcolor{tabsecond}18.031 & \cellcolor{tabsecond}0.459 & \cellcolor{tabfirst}0.495 & \cellcolor{tabthird}1.191M  \\
        \cmidrule{2-6} 
        &  sshELF (Ours) & \cellcolor{tabfirst}19.133 & \cellcolor{tabfirst}0.645 &  \cellcolor{tabthird}0.634 &  \cellcolor{tabfirst}51.67  \\
        \bottomrule
        \multirow{3}{*}{\rotatebox{90}{\textbf{TD3}}} & MVSplat  & \cellcolor{tabthird}16.551 & \cellcolor{tabsecond}0.448 & \cellcolor{tabsecond}0.575 & \cellcolor{tabsecond}2861.61 \\
        & pixelSplat  & \cellcolor{tabsecond}17.332 & \cellcolor{tabthird}0.426 & \cellcolor{tabfirst}0.532 &  \cellcolor{tabthird}0.144M  \\
        \cmidrule{2-6} 
        &  sshELF (Ours) & \cellcolor{tabfirst}18.174 & \cellcolor{tabfirst}0.635 & \cellcolor{tabthird}0.650 & \cellcolor{tabfirst}124.80 \\
        \bottomrule
        \multirow{3}{*}{\rotatebox{90}{\textbf{TD4}}} & MVSplat  & \cellcolor{tabthird}12.610 & \cellcolor{tabthird}0.380 & \cellcolor{tabthird}0.714 & \cellcolor{tabsecond}202.00 \\
        & pixelSplat  & \cellcolor{tabsecond}17.306 & \cellcolor{tabsecond}0.438 & \cellcolor{tabfirst}0.539 & \cellcolor{tabthird}0.163M \\
        \cmidrule{2-6} 
        &  sshELF (Ours) & \cellcolor{tabfirst}17.594 & \cellcolor{tabfirst}0.628 & \cellcolor{tabsecond}0.653 & \cellcolor{tabfirst}171.81 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{nuScenes Results}. Results are shown for temporal differences (TD) of 2, 3, and 4 in terms of PSNR, SSIM, LPIPS and Chamfer distance.}
     \label{tab:results_nuscenes}
\end{table}
\vspace{0.2cm}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/nuScenes_qualitative.jpg}
    \caption{\textbf{Qualitative Novel View Synthesis Comparison on nuScenes Test Set}. Visualization of multi-view synthesis results using six reference views captured at t=0. We compare novel views reconstructed at temporal difference of TD=2, 3, and 4 (1s, 1.5s, and 2s, respectively).}
    \label{fig:nuScenes_qualitative}
\end{figure}



%\underline{Question 1}  Which ELF block architecture is best suited for reconstructing virtual views? \\
%\underline{Question 2}: How impactful is the size of the reconstructed views for the overall model performance? \\
%\underline{Question 3}: Does adding a Chamfer distance loss to the model loss improve the results? \\

\subsection{Ablations and Analysis}\label{subsec:ablation}
%
The following questions are investigated:

\underline{Question 1}  Which ELF block architecture is best suited for reconstructing virtual views? \\
\underline{Question 2}: How impactful is the size of the reconstructed views for the overall model performance? \\
\underline{Question 3}: Does adding a Chamfer distance loss to the model loss improve the results? \\
%\begin{itemize}
%    \item \underline{Question 1}: Which ELF block architecture is best suited for reconstructing virtual %views?
%    \item \underline{Question 2}: How impactful is the size of the reconstructed views for the overall %model performance? 
%    \item \underline{Question 3}: Does adding a Chamfer distance loss to the loss formulation improve %the results?
%\end{itemize}

\noindent \textbf{Backbone Design (Q1)}. We experiment with varying the number of cross-attention and self-attention blocks within each ELF block. Performance is evaluated using PSNR, SSIM, and LPIPS metrics on reconstructed virtual views. Each resulting backbone is trained for 60K steps. The results obtained on the SEED4D dataset are summarized in Table \ref{tab:results_backbone_performance_elf}.
%
\begin{table}[!ht]
    \centering
    % w.r.t local ELF design (trained for 60K iterations)
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l c c c c c}
        \toprule
          \textbf{ELF} & \textbf{PSNR} $\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ \\
          \midrule
          1 cc with 1 cs  & 16.3002  &  \cellcolor{tabthird}0.4049 &   0.6032  \\
          2 cc with 1 cs  & \cellcolor{tabfirst}16.5557  &  \cellcolor{tabfirst}0.4184 &  \cellcolor{tabthird}0.5990 \\
          3 cc with 1 cs  & \cellcolor{tabsecond}16.4709  &  0.4022 &   0.6044 \\
          3 cc with 3 cs  & 16.2464  &  \cellcolor{tabsecond}0.4053 &   \cellcolor{tabfirst}0.5980 \\
          4 cc with 1 cs  & \cellcolor{tabthird}16.3215  &  0.4017 &   \cellcolor{tabsecond}0.5982 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Backbone Performance}. cc indicates the number of cross-attention blocks and cs the number of self-attention blocks.}
    \label{tab:results_backbone_performance_elf}
\end{table}

\noindent \textbf{Translator Design (Q2)}. We investigate the impact of input view resolution by varying the size of the views inputted into the translator. Additionally, we explore a different gradient propagation styles, as proposed in Depth Normalization Regularized Gaussians (DNG) \cite{li2024dngaussianoptimizingsparseview3d}. Table \ref{tab:translator_design}, presents the training results for each configuration after 60K steps on the SEED4D dataset, highlighting the influence of these design choices.
%
\begin{table} %[!ht]
    \centering
    %\vspace{-0.2cm}
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l c c c c}
        \toprule
          Resolution & \textbf{PSNR} $\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ \\
         \midrule
         64 w/o DNG  & 17.8219  &  \cellcolor{tabthird}0.5970 &  \cellcolor{tabthird}0.6089 \\
         64 with DNG  & \cellcolor{tabthird}17.9062  &  0.5947 &   0.6106 \\
         128 w/o DNG  & \cellcolor{tabsecond}17.9950  &  \cellcolor{tabsecond}0.6116 &  \cellcolor{tabsecond}0.6041 \\
         128 with DNG  & 15.2253  &  0.5437 &   0.6787 \\
         256 w/o DNG  &\cellcolor{tabfirst}18.1216  &  \cellcolor{tabfirst}0.6137 &  \cellcolor{tabfirst}0.6017 \\
         256 with DNG & 14.8780  &  0.5318 &   0.6802 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Translator ablation}. Our analysis reveals that DNG underperforms, while reconstruction quality improves with higher image resolution.}
    \label{tab:translator_design}
\end{table}


\noindent \textbf{Chamfer Distance Loss (Q3)}. Since the nuScenes dataset includes LiDAR data, we compute the Chamfer distance and experiment with enforcing a Chamfer distance loss during training. While this loss improves alignment with the ground truth geometry, it results in a slight degradation of visual metrics. Detailed results are provided in Table \ref{tab:results_chamfer_distance}.
\vspace{0.3cm}

\begin{table}[!ht]
    \setlength{\tabcolsep}{2pt}
    \vspace{-0.2cm}
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{l l cccc }
        \toprule
          & Methods & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{Chamfer}$\downarrow$ \\
         \midrule
       \multirow{2}{*}{\rotatebox{90}{\textbf{TD2}}} \rule{3pt}{0em} 
        & Ours w C & \cellcolor{tabsecond}18.980 & \cellcolor{tabsecond}0.645 & \cellcolor{tabsecond}0.641 & \cellcolor{tabfirst}10.89  \\
        & Ours w/o C &\cellcolor{tabfirst}19.133 & \cellcolor{tabfirst}0.645 & \cellcolor{tabfirst}0.634 &  \cellcolor{tabsecond}51.67  \\
        \bottomrule
        \multirow{2}{*}{\rotatebox{90}{\textbf{TD3}}}  & Ours w C & \cellcolor{tabsecond}18.133 & \cellcolor{tabsecond}0.636 & \cellcolor{tabsecond}0.654 & \cellcolor{tabfirst}13.30 \\
        & Ours w/o C & \cellcolor{tabfirst}18.174 & \cellcolor{tabfirst}0.635 & \cellcolor{tabfirst}0.650 & \cellcolor{tabsecond}124.80 \\
        \bottomrule
        \multirow{2}{*}{\rotatebox{90}{\textbf{TD4}}}  & Ours w C & \cellcolor{tabsecond}17.557 & \cellcolor{tabsecond}0.628 & \cellcolor{tabsecond}0.658 & \cellcolor{tabfirst}13.51 \\
        & Ours w/o C & \cellcolor{tabfirst}17.594 & \cellcolor{tabfirst}0.628 & \cellcolor{tabfirst}0.653 & \cellcolor{tabsecond}171.81 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Chamfer Distance Loss}. Results are shown for temporal differences (TD) of 2, 3, and 4 in terms of PSNR, SSIM, LPIPS, and Chamfer distance.}
    \label{tab:results_chamfer_distance}
\end{table}

\section{Conclusion}
\label{sec:dicussion}

% summary + conclusion
This paper introduces sshELF, a fast 3D Gaussian-based framework for reconstructing unbounded driving scenes from sparse, outward-facing views. Our method overcomes the critical challenge of reconstructing unobserved regions, such as distant object occlusions and ego-occlusions, that existing approaches fail to resolve. Our key innovation lies in decoupling information extrapolation from primitive decoding, enabling cross-scene transfer of structural patterns while maintaining a modular, real-time capable pipeline.

% results + limitation
Experiments on challenging synthetic and real-world datasets demonstrate that sshELF achieves competitive novel view synthesis results, even for heavily occluded regions. A current limitation is sensitivity to dynamic objects when aggregating multi-timestep data, which can introduce transient artifacts.
% future work
Future work will focus on (1) temporal filtering to mask dynamic objects during training, and (2) exploring the downstream performance of the obtained latent features. \\

\noindent \textbf{Impact Statement}. This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\noindent \paragraph{Acknowledgements}. The research leading to these results is partially funded by the German Federal Ministry for Economic Affairs and Climate Action within the project “NXT GEN AI METHODS".



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
