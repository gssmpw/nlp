\section{Related Work}
\label{sec:background}

\textbf{Iterative Driving Scene Reconstruction}. Iterative driving scene reconstruction methods perform test-time per-scene fitting by incrementally updating the scene representation until reaching a predefined step count or convergence criteria. This makes them infeasible for real-time applications compared to feed-forward methods. Iterative scene reconstruction methods can broadly be classified into NeRF-based and 3D Gaussian-based approaches. 

% Neural Field-based
An early NeRF-based method, Neural Scene Graph (NSG)~\cite{NSG_2021_CVPR} decomposes a scene into static and dynamic objects, representing their relation via a hierarchical directed graph. Since then, a number of methods have used scene graphs or decompositions into static, dynamic, and flow fields to model scenes \cite{ml_nsg_2024_CVPR, NeuRAD_Tonderski_2024_CVPR, turki2023suds, yang2023emernerf}. These methods often require LiDAR data and are slow to render, with training times exceeding 30 minutes in some cases. MARS~\cite{wu2023mars} similarly uses a foreground-background decomposition, while READ~\cite{li2022read} and StreetSurf~\cite{guo2023streetsurf} focus only on static scenes.

% Gaussian-based
Gaussian-based parameterizations have gained popularity due to their ability to perform real-time rendering. Many of them model backgrounds and objects separately, using bounding boxes to identify the objects. Several Gaussian-based approaches also use scene graphs~\cite{Hugs_Zhou_2024_CVPR, yan2024street, chen2024omnire, zhou2024drivinggaussian}. To alleviate artifacts per-scene reconstruction pipelines have been combined with diffusion-based priors~\cite{VEGS, Yu2024SGDSV} or by enforcing symmetry~\cite{khan2024autosplatconstrainedgaussiansplatting}. Most per-scene optimization methods rely on posed input images, though a few methods also jointly learn poses~\cite{chen2023periodic, li2024_VDG}. Despite being real-time renderable, offline reconstruction remains time-intensive and often LiDAR-dependent, limiting real-time applicability. \\

% putting it here to fix it in the right place
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{imgs/sshELF_method.pdf}
    \caption{\textbf{Overview of sshELF}. Given a few input images, sshELF first encodes them into latent features using a pre-trained DinoV2 (Sec.\ref{image_encoder}). As part of the \textit{backbone}, the latent features, together with a pre-trained depth head, are used to initialize the virtual views, which are refined using hierarchical ELF blocks consisting of cross- and self-attention layers (Sec. \ref{backbone}). Reference and virtual views are then fed into the \textit{translator} part to predict 3D Gaussian splats (Sec. \ref{translator}). Not shown here is the rasterization part used for creating novel views (Sec. \ref{rendering_nvs}).}
    \label{fig:sshELF_model}
\end{figure*}

\noindent \textbf{Few-View Reconstruction}. Iterative per-scene optimization is time-consuming and constrained in its generalizability, prompting the development of feedforward methods. 

% Neural Field-based
Early NeRF-based methods retrieve image features via view projection and aggregate the resulting features~\cite{yu2020pixelnerf, wang2021ibrnet, mvsnerf}. Some methods apply additional constraints using diffusion priors~\cite{ReconFusion_Wu_2024_CVPR} or time-consuming iterative diffusion-based refinement~\cite{szymanowicz2023viewset_diffusion, RenderDiffusion_Anciukevicius_2023_CVPR}. Many of the mentioned papers focus on small-scale scenes or single objects. A method focusing on single-shot prediction in driving scenarios is DistillNeRF~\cite{wang2024distillnerf}, which distills single-shot priors from the per-scene optimization method EmerNeRF. Closest to our work are Neo360~\cite{irshad2023neo360}, which is limited to inward-facing views, and 6Img-to-3D~\cite{gieruc20246imgto3d}, which uses a slow triplane-based representation.

% Gaussian-based
Gaussian-based methods explicitly model scenes and enable a simpler single-shot parameterization compared to neural rendering approaches. Recent works focus on single objects or small-scale scenes~\cite{xu2024grm, TriplaneGaussian_Zou_2024_CVPR, yang2024gaussianobject, szymanowicz24splatter}, but require input views with significant overlap. Flash3D~\cite{szymanowicz2024flash3d} uses depth prediction but fails to inpaint unseen regions. Methods like pixelSplat~\cite{pixelsplat_Charatan_2024_CVPR}, latentSplat~\cite{wewer2024latentsplat}, and MVSplat~\cite{MVSplat} leverage cross-attention for image pairs, excelling in close-range novel view synthesis but struggling with large camera displacements. Concurrent work DrivingForward~\cite{tian2024drivingforwardfeedforward3dgaussian} reconstructs driving scenes from nuScenes but is limited to small viewpoint changes between consecutive timeframes.


Many few-shot 3D reconstructions and novel view synthesis methods overload the 3D Gaussian predictor to inpaint occluded parts of the scene by predicting several Gaussians per ray. This causes blurriness in the unseen parts of the scene, which are far away from the input views. Unlike existing methods, our work utilizes intermediate representations to generate unobserved views and thus obtain a more complete scene reconstruction.