@article{bradley_rank_1952,
	title = {Rank analysis of incomplete block designs: {I}. {The} method of paired comparisons},
	volume = {39},
	shorttitle = {Rank analysis of incomplete block designs},
	url = {https://www.jstor.org/stable/2334029},
	number = {3/4},
	urldate = {2025-02-14},
	journal = {Biometrika},
	author = {Bradley, Ralph Allan and Terry, Milton E.},
	year = {1952},
	note = {Publisher: JSTOR},
	pages = {324--345},
}

@book{elo1978rating,
series = {Batsford chess books},
publisher = {Batsford},
isbn = {0713418605},
year = {1978},
title = {The rating of chessplayers, past and present},
language = {eng},
address = {London},
author = {Elo, Arpad E.},
keywords = {Chess players -- Rating of},
lccn = {^^^78057214^},
}

@inproceedings{gao_position_2024,
	title = {Position {Paper} {On} {Diagnostic} {Uncertainty} {Estimation} from {Large} {Language} {Models}: {Next}-{Word} {Probability} {Is} {Not} {Pre}-test {Probability}},
	shorttitle = {Position {Paper} {On} {Diagnostic} {Uncertainty} {Estimation} from {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=vMSnp12jRx},
	abstract = {Large language models (LLMs) are being explored for diagnostic decision support, yet their ability to estimate pre-test probabilities, vital for clinical decision-making, remains limited. This study evaluates two LLMs, Mistral-7B and Llama3-70B, using structured electronic health record data on three diagnosis tasks. We examined three current methods of extracting LLM probability estimations and revealed their limitations. We aim to highlight the need for improved techniques in LLM confidence estimation.},
	language = {en},
	urldate = {2025-01-04},
	author = {Gao, Yanjun and Myers, Skatje and Chen, Shan and Dligach, Dmitriy and Miller, Timothy A. and Bitterman, Danielle and Chen, Guanhua and Mayampurath, Anoop and Churpek, Matthew and Afshar, Majid},
	month = nov,
	year = {2024},
	file = {Full Text PDF:/Users/tmill/Zotero/storage/6ATC7Y2Z/Gao et al. - 2024 - Position Paper On Diagnostic Uncertainty Estimation from Large Language Models Next-Word Probabilit.pdf:application/pdf},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	doi = {10.48550/arXiv.2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 23+17 pages; refs added, typos fixed},
	file = {Preprint PDF:/Users/tmill/Zotero/storage/VCJCKC2J/Kadavath et al. - 2022 - Language Models (Mostly) Know What They Know.pdf:application/pdf;Snapshot:/Users/tmill/Zotero/storage/KL2TN2JD/2207.html:text/html},
}

@article{savage_large_2024,
	title = {Large language model uncertainty proxies: discrimination and calibration for medical diagnosis and treatment},
	issn = {1527-974X},
	shorttitle = {Large language model uncertainty proxies},
	url = {https://doi.org/10.1093/jamia/ocae254},
	doi = {10.1093/jamia/ocae254},
	abstract = {The inability of large language models (LLMs) to communicate uncertainty is a significant barrier to their use in medicine. Before LLMs can be integrated into patient care, the field must assess methods to estimate uncertainty in ways that are useful to physician-users.Evaluate the ability for uncertainty proxies to quantify LLM confidence when performing diagnosis and treatment selection tasks by assessing the properties of discrimination and calibration.We examined confidence elicitation (CE), token-level probability (TLP), and sample consistency (SC) proxies across GPT3.5, GPT4, Llama2, and Llama3. Uncertainty proxies were evaluated against 3 datasets of open-ended patient scenarios.SC discrimination outperformed TLP and CE methods. SC by sentence embedding achieved the highest discriminative performance (ROC AUC 0.68-0.79), yet with poor calibration. SC by GPT annotation achieved the second-best discrimination (ROC AUC 0.66-0.74) with accurate calibration. Verbalized confidence (CE) was found to consistently overestimate model confidence.SC is the most effective method for estimating LLM uncertainty of the proxies evaluated. SC by sentence embedding can effectively estimate uncertainty if the user has a set of reference cases with which to re-calibrate their results, while SC by GPT annotation is the more effective method if the user does not have reference cases and requires accurate raw calibration. Our results confirm LLMs are consistently over-confident when verbalizing their confidence (CE).},
	urldate = {2024-12-12},
	journal = {Journal of the American Medical Informatics Association},
	author = {Savage, Thomas and Wang, John and Gallo, Robert and Boukil, Abdessalem and Patel, Vishwesh and Safavi-Naini, Seyed Amir Ahmad and Soroush, Ali and Chen, Jonathan H},
	month = oct,
	year = {2024},
	pages = {ocae254},
	file = {Snapshot:/Users/tmill/Zotero/storage/U9KG39DC/7819854.html:text/html},
}

