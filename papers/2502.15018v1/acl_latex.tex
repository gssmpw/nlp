% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{multirow}
\usepackage{todonotes}
\usepackage{subcaption}
% \usepackage{authblk}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcommand{\cola}{\textit{CoLA}}
\newcommand{\clinifact}{\textit{CliniFact}} 

\title{Using tournaments to calculate AUROC for zero-shot classification with LLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Wonjin Yoon \\
%   Boston Children's Hospital \\
%   Harvard Medical School
%   \\\And
%   Ian Bulovic \\
%   Boston Children's Hospital \\
%    \\\And
%   Timothy A. Miller \\
%   Boston Children's Hospital \\
%   Harvard Medical School
%   }

\author{
 \textbf{Wonjin Yoon\textsuperscript{1,2}},
 \textbf{Ian Bulovic\textsuperscript{1}},
 \textbf{Timothy A. Miller\textsuperscript{1,2}} \\
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
\textsuperscript{1}Boston Children's Hospital \\
\textsuperscript{2}Harvard Medical School \\
 \small{
   \{firstname.lastname@childrens.harvard.edu\}
 }
}

\date{\today}
%}
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
% }

\begin{document}
\maketitle
\begin{abstract}
Large language models perform surprisingly well on many zero-shot classification tasks, but are difficult to fairly compare to supervised classifiers due to the lack of a modifiable decision boundary.
%
In this work, we propose and evaluate a method that converts binary classification tasks into pairwise comparison tasks, obtaining relative rankings from LLMs.
%
Repeated pairwise comparisons can be used to score instances using the Elo rating system (used in chess and other competitions), inducing a confidence ordering over instances in a dataset.
%
We evaluate scheduling algorithms for their ability to minimize comparisons, and show that our proposed algorithm leads to improved classification performance, while also providing more information than traditional zero-shot classification.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
In zero-shot classification with large language models (LLMs), a classification task is presented to the LLM in a prompt, and a discrete classification decision is extracted from the generated response.
%
These generated responses can be used to compute traditional scoring metrics of accuracy, precision, recall, and F1 score, to compare against other classification metrics.
%

Supervised methods, in contrast, typically output a probability distribution over possible outputs.
%
In the binary classification setting, real-world deployments of classifiers benefit from these explicit scores, as it allows downstream users to adjust decision thresholds to adapt classifiers to different use cases, with the same classifier potentially using different thresholds for different tasks.
%
In a \emph{screening} setting, recall is maximized at the expense of precision, because there may be such a high cost to missing cases that we can accept more false positives (e.g., doing research on out-of-hospital mortality, we want to capture as many potential cases as possible).
%
In a \emph{case-finding} setting, we may prefer to maximize precision at some expense to recall, for cases where we want our predicted cases to be a pure representation of that phenomenon (e.g., in a clinical trial recruitment setting, we want to err on the side of precision when it comes to satisfying inclusion and exclusion criteria).
%

Some commonly used instruments to evaluate a classifier at different decision points are the receiver-operator curve (ROC) and the precision-recall curve (PRC).
%
Measuring the area under the ROC and PRC curves (AUROC and AUPRC, respectively) can also be used as a single metric to summarize the amount of signal captured by a given classifier across all thresholds.


Zero-shot classification with LLMs does not have a straightforward way to perform the equivalent actions.
%
Unlike supervised methods, LLMs do not have the concept of a decision threshold, so they can only be evaluated at one point, and whether this point represents a bias towards high recall or high precision is not easy to measure.
%
For classification tasks where the class of interest has low prevalence, small changes in the number of positive predictions can have outsized effects on the F1 score, so we might prefer to use something like AUROC, especially if we are comparing between classification paradigms (supervised vs. zero-shot LLMs).
%and want an even footing.

%Previous work... (something to do with confidences?)
%\todo{Not a lot of room. did we need a previous work paragraph here?}

In this paper, we describe a method for estimating AUROC or AUPRC for a zero-shot LLM by turning the classification task into a ranking task between pairs of instances.
%
We use the Elo rating system~\cite{elo1978rating}, used for chess and online videogames, to assign scores to instances in a dataset, updating them after ``wins'' or ``losses,'' and use the ordering this rating induces to compute AUROC.
%
Since this system requires more LLM calls than straightforward classification, we also investigate various tournament scheduling strategies and empirically measure which schedules minimize the number of competitions to reach competitive performance.
%
On two diverse datasets, \cola~and \clinifact, we find that our proposed method leads to stronger classification performance in terms of F1 scores.
%
Random scheduling performs surprisingly well, though the ``Swiss'' system used in chess and a novel graph-based method are also strong in most settings.

%(What datasets did we use, what metrics did we use).
%
%We find that ...(which strategy worked the best)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background and Related Work}
Our method is inspired by ratings systems like Elo, developed in chess~\cite{elo1978rating}, but applied to many other settings, including online video games.
%
This and other related systems (c.f., the Bradley-Terry model~\cite{bradley_rank_1952}) are implicitly grounded in probability distributions around player ratings, where differences in distributions can be used to calculate probability of one competitor winning against another.
%
These methods also consist of update methods where, after a competition between player $A$ and player $B$, respective Elo ratings can be updated based on the difference between scores.
%
For the Elo system, the updated rating for player A ($\hat{e}_A$) with Elo rating $e_A$ against player $B$ with Elo rating $e_B$ is:
%For the Elo system, the update equation for player A with Elo rating $e_A$ against player $B$ with Elo rating $e_B$ is:
%k_factor * (1 - p.expected_win_prob(opp_elo))
\begin{equation}
\label{eqn:update}
\hat{e}_A = e_A + K * ([w_A] - P(e_A, e_B)
\end{equation}
where $[w_A]$ is $1$ if player $A$ wins or $0$ if not, and $K$ is a constant that modulates the rate of update.
%
We use $K=32$, as conventionally used in chess for players with few games played and lower ratings.
%
$P$ is a function representing the expected win probability of player $A$ against player $B$, defined as:
%return 1.0 / (1.0 + (10 ** ((opp_elo - self.elo) / 400)))


\begin{equation}
P(e_A, e_B) = {1 \over 1 + 10^{(e_B-e_A) \over 400}}
\end{equation}

Our method is motivated by the goal of generating ROC curves, and allowing practitioners to select appropriate cut-offs for downstream classification applications.
%
However, it shares a similarity with a recent thread of LLM-based research on the topic of estimating confidence of LLM answers.
%
If certainty could be estimated reliably, directly from LLM outputs, then ROC curves could be generated directly from those outputs.
%
However, recent work in this area shows mixed results, with some work showing difficulty finding correlations with certainty in zero-shot classification tasks~\cite{gao_position_2024}.
%
Other work does find some signal, but with white-box methods showing much greater signal than black-box methods~\cite{savage_large_2024}.
%
Work studying calibration claims that LLM uncertainties are calibrated for true/false question answering~\cite{kadavath_language_2022}, but that calibration only holds for the largest model they evaluate.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}

\subsection{Zero-shot Prompting}
To establish baseline performance, we use standard zero-shot prompting to get classification results.
%
The prompt templates for our two datasets can be found in Appendix~\ref{app:prompts}.

Explicitly trading off precision and recall via zero-shot classification off generation alone is impossible, but we explored the possibility of ``prompt steering,'' that is, prompting to encourage the model to err on the side of precision or recall (see Appendix~\ref{app:prompts} for specific prompt steering variations).
%

\subsection{Tournament-based method}
Our proposed method is to induce an overall ranking of instances by converting our zero-shot prompts into pairwise comparison prompts, iteratively comparing pairs of instances from our dataset, and scoring each instance based on its ``wins'' and ``losses'' in the tournament.
%
These scores can be converted to probabilities, and the probabilities used as thresholds for decision-making, allowing for the computation of receiver-operator characteristic (ROC) or precision-recall (PR) curves.

\subsubsection{Tournament setup}
We randomly initialize every instances in our dataset $D$ to have an Elo rating distributed around 1000.
%
In each round $R$, we obtain a ranking of instances by Elo rating, and produce a set of pairs of matchups for that round via a scheduling algorithm.
%
The instance pair in each matchup is inserted into a pairwise comparison prompt template (see Appendix~\ref{app:prompts}), and an answer is extracted using regular expressions.\footnote{We use the Inspect Eval framework\cite{UK_AI_Safety_Institute_Inspect_AI_Framework_2024} from the UK AI Safety Institute.}
%
At the end of each round, Elo ratings for each instance are updated according to Equation~\ref{eqn:update}.
%


\subsubsection{Scheduling strategies}
Since each instance is processed by the LLM one time in each round, a single round performs approximately as much computation as zero-shot classification.
%
However, after a single round the Elo scores and probabilities derived from them are unlikely to be very well sorted, so they likely require multiple rounds.
%
Therefore, we explore a variety of scheduling algorithms, with the goal of finding the algorithm that empirically minimizes the number of rounds required to obtain reliable AUROC values.
%
We evaluate the following scheduling strategies:

\textbf{Random}: In each round, we sample two instance indices at a time, without replacement, and match up those two indices in that round. There is no guarantee that two instances won't matchup repeatedly across rounds.

\textbf{Graph}: We model the set of instances as a graph with adjacency matrix $A$, initialized as the zero matrix. At the end of each round, we set $A_{i,j}=1$ for instance indices $i$ and $j$ that were matched up in that round. To schedule each round, we use the \emph{networkx} library~\cite{SciPyProceedings_11} to compute the minimum distance between all pairs of instances, and pair up instances that are furthest form each other first, again removing each instance from the pool for that round once it has been matched up. Instances that are not connected at all (all instances in the first round) are assigned distance equal to the size of the dataset. The motivation for this approach is the intuition that instances with long distance between them in the graph have the most uncertainty about their relative rankings and should be paired up. Compared to the \emph{Random} method, this will also reduce the number of repeated matches.

\textbf{Swiss}: This method, with empirically strong performance~\cite{sziklai_efficacy_2022}, ranks the instances by Elo rating, and splits the ordered list of instances into groups of size eight, and creates instance pairings within groups. In the variant we use, called ``The Dutch system,'' within each group player $i<4$ plays player $7-i$. A motivation for this approach is to allow players of similar rank to play each other. This might benefit in our case, where we might suspect local ordering to be more important for optimizing AUROC.
%\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Evaluation %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
We use two English language datasets to evaluate our proposed methods.
%
The Corpus of Linguistic Acceptability (\cola)~\cite{warstadt2018neural} is a dataset consisting of sentences from linguistics publications, labeled with whether they were considered \emph{acceptable} or not.
%
We use version 1.1 of \cola, and evaluate on the in-domain development set.

Our second data source is \clinifact~\cite{zhang_dataset_2025}, a dataset of scientific claims from clinical trial protocols labeled for whether they are supported or not by corresponding publications.

We use Llama-3.2-3B-Instruct and GPT-4o-mini\footnote{Specifically, we use gpt-4o-mini-2024-07-18.} as representative models, ranging from smaller, more open models, to larger, more closed models.
%
Our code to run these models uses the Inspect eval framework~\cite{UK_AI_Safety_Institute_Inspect_AI_Framework_2024}, which runs the Llama models using Huggingface transformers~\cite{wolf_transformers_2020}.
%
For metrics, we report precision, recall, F1,  accuracy, and AUROC.
%


%\subsection{Baselines}
%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|p{2.0cm}|c||c|c|c|c||c|c|c|c|}
        \hline
        \multirow{2}{=}{Model} & \multirow{2}{*}{Prompt style} & \multicolumn{4}{c||}{\cola} & \multicolumn{4}{c|}{\clinifact} \\
        \cline{3-10}
        & & Precision & Recall & F1 & Accuracy & Precision & Recall & F1 & Accuracy \\
        \hline
        \multirow{3}{=}{\raggedright Llama3.2-3b-Instruct} & Plain & 0.868 & 0.547 & 0.671 & 0.634 & 0.338 &    0.976 &    0.502 & 0.491 \\
        & Precision &0.759 & 0.458 & 0.571 & 0.533 & 0.312 & 0.928 & 0.467 & 0.443 \\
        & Recall & 0.754 & 0.660 & 0.704 & 0.619 & 0.324 & 0.976 & 0.487 & 0.459 \\
        \hline
        \multirow{3}{=}{GPT-4o-mini} & Plain & 0.914 & 0.877 & 0.895 & 0.858 & 0.660 & 0.747 & 0.701 & 0.832 \\
        & Precision & 0.910 & 0.890 & 0.900 & 0.863 & 0.670 & 0.735 & 0.701 & 0.835 \\
        & Recall & 0.869 & 0.929 & 0.898 & 0.854 & 0.660 & 0.747 & 0.701 & 0.832 \\
        \hline
    \end{tabular}
    }
    \caption{Performance of two LLMs at our two tasks, when run in binary classification mode. Precision and recall prompts emphasize the greater cost of false positives or false negatives, respectively. Precision, recall, and F1 are defined in terms of the positive class, \emph{Acceptable} or \emph{True}, for our two datasets.}
    \label{tab:ml_results}
\end{table*}
%%%%%%%%%%%%%% End Table 1 %%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% Figure 1 %%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama_cola.pdf}
        \caption{Llama-3.2-3B-Instruct performance on \cola}
    \end{subfigure}
    \hspace{5pt}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt4omini_cola.pdf}
        \caption{GPT-4o-mini performance on \cola}
        
    \end{subfigure}
    }
    \newline
    \\
    \resizebox{\textwidth}{!}{
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/llama_clinifact.pdf}
        \caption{Llama-3.2-3B-Instruct performance on \clinifact}
    \end{subfigure}
    \hspace{5pt}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gpt4omini_clinifact.pdf}
        \caption{GPT-4o-mini performance on \clinifact}
    \end{subfigure}
    }
    \caption{Figures showing AUROC improvements across rounds for each dataset and model.}
    \label{fig:tiled_plots}
\end{figure*}
%%%%%%%%%%%%%% End Figure 1 %%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%

\begin{table}[h!]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|p{2.0cm}||c|c|c||c|c|c|}
        \hline
        \multirow{2}{=}{Model} & \multicolumn{3}{c||}{\cola} & \multicolumn{3}{c|}{\clinifact} \\
        \cline{2-7}
        & Acc & F1 & AUC  & Acc & F1 & AUC \\
        \hline
        Llama3.2-3b-Instruct &   0.801 & 0.866 & 0.831 & 0.823 & 0.667 & 0.852 \\
        \hline
        GPT-4o-mini  & 0.873 & 0.910 & 0.928 & 0.873 & 0.753 & 0.883 \\
        \hline
    \end{tabular}
    }
    \caption{Optimal performance of two LLMs at our two tasks, when run in tournament mode. Precision, recall, and F1 are defined in terms of the positive class, \emph{Acceptable} or \emph{True}, for our two datasets. The \emph{Graph} scheduler was used for the GPT-4o-mini \cola{} results, while the \emph{Random} scheduler was used for the Llama \cola{} results. For \clinifact, \emph{Random} and \emph{Swiss} scheduler were used for Llama and GPT, respectively.}
    \label{tab:tourney_results}
\end{table}
%%%%%%%%%%%%%% End Table 2 %%%%%%%%%%%%%%%%%%%%%

\subsection{Results}
Table~\ref{tab:ml_results} shows the result of the zero-shot classifiers on our two datasets.
%
GPT-4o-mini significantly outperforms Llama-3.2-3b at both tasks, as might be expected.
%
F1 scores are respectable for \cola, but show imbalanced precision and recall.
%
For Clinifact, Llama-3.2-3b struggles significantly, but GPT-4o-mini performs respectably, with higher overall F1 and better balance between precision and recall.
%
On all configurations, prompt-based steering towards higher \emph{precision} is completely ineffective, while steering towards higher \emph{recall} does succeed for \cola.

Figure~\ref{fig:tiled_plots} shows AUROC values across tournament rounds for all four model/task combinations, with different schedulers represented as color lines.
%
The Random scheduler is surprisingly strong, obtaining the highest overall AUROC for the Llama/CoLA plot(Figure~\ref{fig:tiled_plots}a).
%
Table~\ref{tab:tourney_results} shows the F1 scores that are obtained by finding the optimal cutoff point on the ROC curve for the optimal AUROC round for each tournament experiment.
%
While the added benefit of AUROC scores and thresholds mean it would be sufficient to match the performance in the zero-shot setting, in fact we find that the tournament-style method leads to greatly improved F1 scores.
%
For \cola, Llama improves from 0.671 to 0.831, and GPT-4o-mini improves from 0.895 to 0.928.
%
For \clinifact, F1 scores with Llama improve from 0.502 to 0.667, and with GPT-4o-mini they improve from 0.832 to 0.753.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion and Conclusion}
One major benefit of this approach relative to other methods for estimating uncertainty is that it can be computed from generations only.
%
While closed models like ChatGPT make log probabilities available in their API, there is no guarantee that such functionality will always exist.
%
Our method only relies on the core functionality of LLMs, the ability to generate text.

A drawback of our approach is its requirement of multiple runs.
%
Our results show asymptotic behavior after 10-20 rounds, but the smaller dataset does asymptote quickly, raising the possibility that the optimal number of rounds is a function of dataset size.
%
Future work should explore this relationship to judge the feasibility of applying this method to larger datasets.

The code implementing our methods and generating these results will be made publicly available upon publication.

% \begin{itemize}
% %    \item This method works even without any access to logits or logprobs or activations (i.e. arbitrary black box models)
%     \item Discussion point: Multiple runs as a positive for the same reason of self-consistency -- different outcomes each time.
%     \item Future work: automatic rewriting of prompts from single instance to pairs
%     \item It's easy to add new instances by giving them an uninformative Elo rating and having them play some games against existing data
%     \item Time complexity and dataset lengths
%     \item Future work: Other scoring systems other than Elo (e.g., the one used by chatbot arena)
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
Research reported in this
publication was supported by the National Institute Of Mental Health and National Library of Medicine of the National Institutes of Health
under Award Numbers R01MH126977, R01LM012973, and R01LM012976. The content is solely the responsibility of the authors and does not
necessarily represent the official views of the National Institutes of Health.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Limitations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Limitations}
This work evaluates on two datasets, chosen to have different properties, but still limiting the generalizeability of the work.
%
We also evaluate only two models, which, though representative, cannot represent all models.
%
While our method applies in black box scenarios that other methods do not (like looking at first token probability), we did not include a direct comparison of performance to other methods.
%
The method we describe also requires more inference time than simple classification.
%
While we test scheduling algorithms to improve this performance, it will always require the equivalent of multiple inference passes per instance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Prompt Templates}
\label{app:prompts}
\subsection{Single instance prompts}
For the \cola~dataset, our baseline (single instance classification) prompt was: 
\begin{quote}
Please read the following sentence and decide whether it is "acceptable" in a linguistic sense (i.e., grammatical). Don't explain your reasoning, just answer "Yes" (acceptable) or "No" (unacceptable) on a new line. \{Input sentence\}
    \end{quote}
    

For the \clinifact~dataset, our baseline (single instance classification) prompt was: 
\begin{quote}
Instruction: Given a scientific claim and an abstract, determine if the abstract reports positive results (TRUE) or not (FALSE) about the claim. The task is to classify the pair claim abstract as follows: TRUE: if the abstract provides positive support for the claim. FALSE: if the abstract provides negative or inconclusive support for the claim or if the abstract provides contextual or background information without directly reporting results about the claim.\{Input sentence\}    
\end{quote}
%\emph{Instruction: Given a scientific claim and an abstract, determine if the abstract reports positive results (TRUE) or not (FALSE) about the claim. The task is to classify the pair claim abstract as follows: TRUE: if the abstract provides positive support for the claim. FALSE: if the abstract provides negative or inconclusive support for the claim or if the abstract provides contextual or background information without directly reporting results about the claim.\{Input sentence\}} % This prompt stemed from the prompt provided with the original dataset paper by \citealp{zhang2025dataset}, but modified to only produce TRUE or FALSE.

\subsection{Precision and recall-enhanced prompts}
We attempted to guide the model towards higher precision or recall with variations of the prompt stressing the consequences of false positives or negatives. These prompts are the same as the baseline prompt, with the following text added.

For the version favoring precision: 
\begin{quote}
The consequences for wrongly guessing \{Positive class\} are worse than the consequences for wrongly guessing \{Negative class\}.
\end{quote}


For the version favoring recall: 
\begin{quote}
The consequences for wrongly guessing \{Negative class\} are worse than the consequences for wrongly guessing \{Positive class\}.
\end{quote}

\subsection{Pairwise comparison prompts}
For the \cola\ dataset, the pairwise prompt was: 

\begin{quote}
You are an expert linguist deciding whether sentences are grammatically acceptable or not. Your task is to take in a pair of sentences and decide which is more acceptable. The output format should be \{"choice": <Sentence>, "reasoning": <your reasoning>\}, where <Sentence> should be the more acceptable or less unacceptable sentence, either "Sentence 1" or "Sentence 2". Here are the two sentences.

Sentence 1: \{text1\}

Sentence 2: \{text2\}
\end{quote}

For the \clinifact\ dataset, the pairwise prompt was:

\begin{quote}
    You are a biomedical researcher evaluating whether given scientific claims and their corresponding abstracts report positive results (TRUE) or not (FALSE). If an abstract presents inconclusive findings or does not provide information relevant to the claim, the answer is FALSE. Your task is to compare two claim-abstract pairs and determine which one is more likely to be classified as TRUE. If both pairs should be answered "TRUE", choose the one with higher confidence. The output format should be \{"choice": <Pair>, "reasoning": <your reasoning>\}, where <Pair> should be the claim abstract pair more likely to have the answer TRUE, either "Pair 1" or "Pair 2". Here are the two questions.
    
    Pair 1: \{text1\}
    
    Pair 2: \{text2\}
\end{quote}
\end{document}
