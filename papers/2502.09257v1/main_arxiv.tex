\documentclass[11pt]{article}




\usepackage{graphicx} % Required for inserting images
% \usepackage{fullpage}
\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage[extdef=true]{delimset}
\usepackage[colorlinks,allcolors=blue]{hyperref}
\usepackage{xcolor}
\usepackage[round]{natbib}
\usepackage{nicefrac}
\usepackage{dsfont}
\usepackage{bbding}


\hypersetup{
           breaklinks=true,   % splits links across lines
                              % etc.
}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}

\usepackage{tkdefs_arxiv}
% \usepackage[vvarbb]{newtxmath}
\usepackage[margin=1in]{geometry}
\usepackage[capitalise]{cleveref}

\newcommand\nnfootnote[2]{%
  \begin{NoHyper}
  \renewcommand\thefootnote{#1}\footnotetext{#2}%
  % \addtocounter{footnote}{1}%
  \end{NoHyper}
}

\usepackage[title]{appendix}
\usepackage{ifthen}

\newcommand{\eqdef}{\triangleq}
\renewcommand{\ind}[1]{\mathds{1}\{#1\}}
\newcommand{\bbE}{\boldsymbol{\mathbb{E}}}
\renewcommand{\E}{\bbE}
% \newcommand{\R}{\mathbb{R}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\liad}[1]{\textcolor{blue}{\bf \{LE: #1\}}}
\newcommand{\alon}[1]{\textcolor{red}{\bf \{AC: #1\}}}
\newcommand{\yishay}[1]{\textcolor{purple}{\bf \{YM: #1\}}}
\newcommand{\tk}[1]{\textcolor{magenta}{\bf \{TK: #1\}}}


% notation
\newcommand{\indh}[2]{J_{#1,#2}}
% \newcommand{\yprob}[3]{W_{#1}(#2,#3)}
\newcommand{\yprob}[3]{Q_{#2,#3}(#1)}
% \newcommand{\yprobg}[3]{W^\gamma_{#1}(#2,#3)}
\newcommand{\yprobg}[3]{Q^\gamma_{#2,#3}(#1)}
\newcommand{\loo}{\textsf{LOO}}
\newcommand{\oracle}{\textsf{ERM}}

% notation combinatorial
\newcommand{\truelab}{Y}
\newcommand{\predlist}{L}
\newcommand{\pred}{a}
\newcommand{\rew}{r}
\newcommand{\lab}{y}
\newcommand{\piout}{\pi_{\mathrm{out}}}
\newcommand{\hout}{h_{\mathrm{out}}}

\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\iwd}{\mathcal{D}'}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\obj}{F}
\newcommand{\objrand}{f}
\newcommand{\regret}{\mathcal{R}}


% \DeclareMathOperator*{\argmax}{arg\,max}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{cor}{Corollary}

\title{Bandit Multiclass List Classification}

\author{
Liad Erez$^{*}$
% \thanks{Blavatnik School of Computer Science, Tel Aviv University; \texttt{liaderez@mail.tau.ac.il}.}
\and
Tomer Koren$^{*,\dag}$
% \thanks{Blavatnik School of Computer Science, Tel Aviv University; \texttt{urisherman@mail.tau.ac.il}.}
}

\begin{document}

\maketitle

\nnfootnote{*}{ Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel.}
\nnfootnote{\textdagger}{ Google Research Tel Aviv, Israel.}

\begin{abstract}
    We study the problem of multiclass list classification with (semi-)bandit feedback, where input examples are mapped into subsets of size $m$ of a collection of $K$ possible labels, and the feedback consists of the predicted labels which lie in the set of true labels of the given example. Our main result is for the $(\eps,\delta)$-PAC variant of the problem for which we design an algorithm that returns an $\eps$-optimal hypothesis with high probability using a sample complexity of $O \big( (\mathrm{poly}(K/m) + sm / \eps^2) \log (|\calH|/\delta) \big)$ where $\calH$ is the underlying (finite) hypothesis class and $s$ is an upper bound on the number of true labels for a given example. This bound improves upon known bounds for combinatorial semi-bandits whenever $s \ll K$. Moreover, in the regime where $s = O(1)$ the leading terms in our bound match the corresponding full-information rates, implying that bandit feedback essentially comes at no cost. Our PAC learning algorithm is also computationally efficient given access to an ERM oracle for $\calH$. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of $\widetilde O(|\calH| + \sqrt{smT \log |\calH|})$. Our results generalize and extend those of \cite{erez2024real,erez2024fast} who consider the simpler single-label setting corresponding to $s=m=1$, and in fact hold for the more general contextual combinatorial semi-bandit problem with $s$-sparse rewards.
\end{abstract}

% \begin{keywords}%
%   list multiclass classification, combinatorial semi-bandits, PAC, regret%
% \end{keywords}

\section{Introduction}

Multiclass list classification is a learning problem where the goal is to map input examples to lists (i.e., subsets) of a collection of $K$ possible labels, and is a natural generalization of the fundamental problem of \emph{multiclass classification} where traditionally only a single label is predicted for each example. 
% In multiclass list classification, examples arrive sequentially, and upon predicting a list of labels of size $m \leq K$, the learner observes feedback and gains a zero-one reward of whether or not the predicted list contains the correct label. 
Existing work on multiclass list classification focuses on the full-information single-label setting, where a given example has a single correct label which is revealed to the learner after each prediction. The primary focus on this literature is on characterizing properties of the underlying hypothesis class $\calH$ under which various notions of learnability can be achieved; e.g., PAC learnability \citep{charikar2023characterization}, uniform convergence \citep{hanneke2024list} and online learnability \citep{moran2023list}.

In this work, we introduce the problem of \emph{bandit multiclass list classification}, where each example may have up to $s \leq K$ correct labels,%
\footnote{This is often referred to as \emph{multilabel} multiclass classification, to contrast with the single-label setting where $s=1$.} and upon predicting a list of labels of size $m \leq K$, the learner observes partial, a.k.a.\ ``bandit''%
\footnote{We actually consider \emph{semi-bandit} feedback, as will be discussed later on.}  feedback consisting only of 
% the zero-one reward \tk{consider not using "reward" just yet} of the labels in the predicted set; that is, 
the predicted labels which belong to the collection of $s$ correct labels for the given example. As a natural application, consider a recommendation system visited sequentially by users, where upon each visit the system is tasked with presenting a user with a list of $m$ recommended products available for purchase, after which the system observes the purchased products as feedback. This is naturally viewed as a bandit scenario in the sense that the system only observes the user's behavior with respect to the recommended products and not others. That is, the only way to obtain feedback on a given product is by actively recommending it to a user.

% Existing work on multiclass list classification focuses on the full-information single-label setting, where a given example has a single correct label which is revealed to the learner after each prediction. These works mostly aim to characterize properties of the underlying hypothesis class $\calH$ under which various notions of learnability can be achieved; namely PAC learnability \citep{charikar2023characterization}, uniform convergence \citep{hanneke2024list} and online learnability \citep{moran2023list}. 
% Notably, both of these works consider the \emph{single-label} classification setting in which every example has a single correct label (namely $s=1$). 
In the context of traditional multiclass classification (that is, when $m=s=1$), extensive research has studied the bandit variant of the problem, starting with the work of \cite{kakade2008efficient} who consider linear classification. Follow-up works study various questions of learnability of general hypothesis classes \citep{daniely2011multiclass,daniely2013price,raman2023multiclass, filmus2024bandit} and other recent works \cite{erez2024real,erez2024fast} study optimal rates for both the regret minimization and PAC objectives with a focus on finite hypothesis classes.. 

It is thus natural to study the problem of list multiclass classification with bandit feedback which, to our knowledge, our work is the first to tackle.

For agnostic bandit multiclass classification (that is, $m=1$) with finite hypothesis classes $\calH$, the recent work of \cite{erez2024real} characterized the optimal \emph{regret}; that is, the cumulative reward of the learner compared to that of the best hypothesis in $\calH$, and showed that it is of the form $\widetilde \Theta (\min \{|\calH| + \sqrt{sT}, \sqrt{KT \log |\calH|}\})$, implying that for relatively small hypothesis classes the classical $\sqrt{KT}$ rate of EXP4 \citep{auer2002nonstochastic} can be improved if $s \ll K$. In a subsequent work, \cite{erez2024fast} study the PAC variant of the problem where the goal is to learn a near-optimal hypothesis with respect to some unknown data distribution, and establish a sample complexity bound of $O((K^9 + 1/\eps^2) \log(|\calH|/\delta))$ for $(\eps,\delta)$-PAC learning in the single-label setting using a computationally efficient algorithm given an ERM oracle to $\calH$. The key takeaway from both of these works is that in the single-label setting, bandit feedback essentially comes at no additional cost compared to full feedback in the asymptotic regimes where $T \to \infty$ and $\eps \to 0$.

We are thus motivated to investigate the following question: \emph{what is the price of bandit feedback in PAC and online multiclass list classification?} In this work, we provide an answer to this question by designing PAC learning and regret minimization algorithms for bandit multiclass list classification with finite hypothesis classes, and prove that they attain sample complexity and regret bounds whose dominant terms match the corresponding full-information rates in the case where $s = O(1)$. 
% More concretely, in the $(\eps,\delta)$-PAC setting we design an algorithm which outputs an $\eps$-optimal hypothesis with probability at least $1-\delta$ using a total sample complexity of
% \begin{align*}
%     O \brk*{\brk*{\poly \brk*{\frac{K}{m}} + \frac{sm}{\eps^2}}\log \frac{|\calH|}{\delta}},
% \end{align*}
% and in the online setting we design a regret minimization algorithm which guarantees a regret bound of 
% \begin{align*}
%     \widetilde O \brk*{|\calH| + \sqrt{smT \log |\calH|}},
% \end{align*}

Our results in fact apply in further generality to the problem of \emph{contextual combinatorial semi-bandits} (CCSB) with $s$-sparse rewards. 
% \tk{maybe move the final sentence to a new paragraph and discuss what's the difference from prior work on combinatorial bandits?}
Prior work on combinatorial semi-bandits focuses for the most part on the regret minimization objective, with the rate of $O(\sqrt{mKT})$ obtained by \cite{audibert2014regret} for the vanilla non-contextual variant, and as remarked in the same paper, is optimal for non-sparse losses. The work of \cite{neu2015first} provides first-order regret bounds of the form $O(m \sqrt{K L^\star_T})$ where $L^\star_T$ is the cumulative loss of the best arm in hindsight, and while it may be a significant improvement in cases where the cumulative loss of the best arm is sublinear in $T$, the bound still contains a polynomial dependence on $K$.  To our knowledge, no prior works on either the vanilla or the contextual variants prove regret bounds which scale with the sparsity $s$ of the losses rather than directly with $K$.

To illustrate the challenge in answering this question, let us consider the naive approach for PAC learning in which lists of size $m$ are sampled uniformly at random, and bandit feedback is used to estimate the rewards of hypotheses in $\calH$ via importance sampling. Since the variance of such reward estimators scales polynomially with $K$, this approach ultimately results in a sample complexity of $\approx  K m / \eps^2$. This bound is far from optimal since, as we will show, the leading term in the optimal sample complexity bound in fact scales with $s$ instead of $K$. 
One could also consider reducing the list problem (where $s=1$ and $m$ is general) to the $s=m=1$ setting studied in previous work, by treating each list of size $m$ as a single independent label. This approach fails, however, as the sparsity property is lost in the sense that the sum of rewards of individual labels is now extremely large due to the blowup in the number of labels.

\subsection{Summary of Contributions}

We summarize our results for bandit multiclass list classification over a finite hypothesis class $\calH$:
% 
\begin{itemize}[leftmargin=3ex]
    \item Our main result involves the PAC setting (see \cref{sec:pac}), where we design an algorithm that with probability at least $1-\delta$ outputs a hypothesis $\hout \in \calH$ that is $\eps$-optimal with respect to the population reward, using a sample complexity of at most
    \begin{align*}
        O \brk*{\brk*{\poly \brk*{\frac{K}{m}} + \frac{sm}{\eps^2}}\log \frac{|\calH|}{\delta}}.
    \end{align*}
    Moreover, our algorithm is computationally efficient given an ERM oracle for $\calH$. We also present the high-level construction of a sample complexity lower bound of $\Omega(sm / \eps^2)$ which holds even for a single context, showing that the dominant term in our bound is in fact optimal. 
    % Further details can be found in \cref{sec:pac}.

    \item We also consider the online regret minimization setting, where we design an algorithm achieving an expected regret bound of
    \begin{align*}
        \widetilde O \brk*{|\calH| + \sqrt{smT \log |\calH|}},
    \end{align*}
    which holds even when the input and reward sequence is adversarial. Our algorithm is an instantiation of Follow-the-Regularized-Leader (FTRL) and uses a mixture between negative entropy and log-barrier regularizations, with importance-weighted loss estimators specialized for semi-bandit feedback. See \cref{sec:regret} for more details.

    \item 
    We further show that the above results hold 
    % In fact, our results hold 
    in a broad setting of \emph{contextual combinatorial semi-bandits} (CCSB) with $s$-sparse rewards over a finite policy class.
\end{itemize}
% 
Our starting point for addressing the PAC setting in CCSB is the recent work of \cite{erez2024fast} who study the single-label ($s=m=1$) classification setting. Following the general scheme they proposed, our algorithm operates in two stages by first computing a low-variance exploration distribution over policies in $\Pi$ and then using this distribution to uniformly estimate the policies' expected rewards. Generalizing the single-label classification setting to CCSB, however, comes with some nontrivial technical challenges. 
First, \cite{erez2024fast} use the single-label assumption in order to collect a labeled dataset by uniformly sampling labels. In the general setting, however, this approach cannot work effectively, since the full reward function cannot be inferred unless the predicted set contains all $s$ correct labels, which is a very unlikely event. We circumvent the issue by introducing additional importance sampling estimation with an appropriate modification of the convex potential in use, at the price of a slightly worse dependence on $K$ in the lower order term due to additional noise.
As a result of this added complexity, the stochastic optimization problem solved for obtaining the desired low-variance exploration distribution crucially requires a modification to allow for general (bounded) rewards, unlike the one-hot rewards in the single-label case.
% of the stochastic optimization problem defined in \cite{erez2024fast} for the single-label setting, in the sense that the more general problem has to correspond to semi-bandit reward estimation.
% 
% First, the convex log-barrier potential used in \cite{erez2024fast} in order to compute the exploration distribution has to be modified such that its gradient would now correspond to the the variance of the type of reward estimators used in the combinatorial setting. As we will show, a possible approach is to apply weighting of individual action-dependent potentials by the observed reward function.

 
\subsection{Additional Related Work}


% \paragraph{Multiclass list classification.} In the offline PAC setting, \cite{charikar2023characterization} characterize PAC learnability of a hypothesis class $\calH$ using a generalization of the DS-dimension. A subsequent work of \cite{moran2023list} consider the online setting and characterize learnability using a generalization of the Littlestone dimension 

\paragraph{List multiclass Classification.} The theoretical framework of multiclass list classification was originally introduced by \cite{brukhim2022characterization}. \cite{charikar2023characterization} provide a characterization of PAC learnability for multiclass list classification by generalizing the DS-dimension \citep{daniely2014optimal}, \cite{moran2023list} consider the regret minimization setting and characterize learnability by a generalization of the Littlestone dimension, and \cite{hanneke2024list} study the notions of uniform convergence and sample compression in the context of multiclass list classification. A regression-variant of list learning has also been studied in \cite{pabbaraju2024characterization} who provide a characterization of learnability for this problem. Notably, all of these works on multiclass list classification focus on the single-label setting.

\paragraph{Bandit multiclass classification.} The setting of bandit multiclass classification was originally introduced by \cite{kakade2008efficient}, with \cite{daniely2011multiclass} showing that learnability of deterministic learners in the realizable setting is characterized by the bandit Littlestone dimension. \cite{daniely2013price} generalize those results by showing that the bandit Littlestone dimension characterizes online learnability whenever the label set is finite, and \cite{raman2023multiclass} generalize this result to infinite label sets. Several previous works \citep{auer1999structural,daniely2011multiclass,long2017new} study the price of bandit feedback in the realizable setting, with the recent work of \cite{filmus2024bandit} showing that this price is bounded by a factor of $O(K)$ over the mistake bound in the full-information setting for randomized learners.

\paragraph{Combinatorial semi-bandits.} As previously remarked, bandit multiclass list classification is a special case of contextual combinatorial semi-bandits, a problem whose non-contextual variant was introduced by \cite{gyorgy2007line} in the framework of online shortest paths. This problem has been extensively studied in the bandit literature, mostly in the context of regret minimization \citep[etc.]{audibert2014regret,wen2015efficient,kveton2015tight,neu2015first, wei2018more, ito2021hybrid}. A regret bound of $O(\sqrt{mKT})$ was shown by \cite{audibert2014regret} for adversarial losses, and was proven by \cite{lattimore2018toprank} to be optimal in the multiple-play setting in which the available combinatorial actions are all subsets of size $m$ of the action set (which is the setting considered in this paper). The contextual combinatorial semi-bandit (CCSB) problem is considerably less explored, with some existing works \citep{qin2014contextual,wen2015efficient,takemura2021near,zierahn2023nonstochastic} focusing on the case where rewards are noisy linear functions of the inputs, and others \citep{kale2010non,krishnamurthy2016contextual} which consider a setting similar to ours with finite unstructured policy classes, and establish regret bounds of the form $O(\sqrt{mKT \log |\Pi|})$. To our knowledge, all previous results on combinatorial semi-bandits exhibit a polynomial dependence on $K$ in the leading term, thus leaving open the question of adaptivity to reward sparsity.

\paragraph{Combinatorial (full-)bandits.} Another well-studied variant of CCSB is known as \emph{combinatorial bandits} (also referred to as \emph{bandit combinatorial optimization}, \cite{mcmahan2004online,awerbuch2004adaptive}), where the feedback is limited only to the realized reward, that is, the sum of rewards of predicted actions. For the non-sparse version of the problem, regret bounds of $O(m^{3/2} \sqrt{KT})$ have been established in several previous works \citep{dani2007price,abernethy2008competing,bubeck2012towards,cesa2012combinatorial,hazan2016volumetric} and have subsequently been shown to be optimal \citep{cohen2017tight,ito2019improved}. In the context of multiclass classification, while perhaps not as natural as semi-bandit feedback, full-bandit feedback is well-motivated in some applications, for example, in cases where a recommendation system is interested in protecting the users' privacy by only observing the amount of products purchased rather than the products themselves. While our results apply in the semi-bandit model, examining the full-bandit variant raises some very interesting questions and generalizing our results to this setting seems highly non-trivial; see \cref{sec:discussion} for further discussion.

\section{Problem Setup}
\label{sec:setup}

\subsection{Bandit Multiclass List Classification}

We study a learning scenario where a learner has to map objects (examples) from a domain $\calX$ to subsets (lists) of size $m$ of a collection of $K \geq m$ possible labels, denoted by $\calY \eqdef [K]$. We denote by $\calL$ the set of all subsets of $\calY$ of size $m$, corresponding to all possible predictions. In the semi-bandit setting, the learner iteratively interacts with an environment according to the following, for $t=1,2,\ldots:$
% 
\begin{enumerate}[leftmargin=!,itemsep=0pt,parsep=0pt]
    \item[(i)] The environment generates a pair $(x_t, \truelab_t)$ where $x_t \in \calX$ and $\truelab_t \subseteq \calY$, the learner receives $x_t$;
    \item[(ii)] The learner predicts a list $\predlist_t \in \calL$;
    \item[(iii)] The learner observes \emph{semi-bandit feedback} consisting of $\predlist_t \cap \truelab_t$, namely the set of predicted labels belonging to the true label set.
\end{enumerate}
% 
We assume that there is a known bound $s \leq K$ on the cardinality of all true label sets $\truelab_t$\footnote{Note that the single-label classification setting corresponds to $s=1$.}. In the settings we consider, the learner's performance is measured with respect to an underlying hypothesis class $\calH \subseteq \brk[c]*{\calX \to \calL}$; focusing in this work on the case where $\calH$ is finite.


\paragraph{Agnostic PAC setting.} 
% 
In the $(\eps,\delta)$-agnostic PAC setting, the pairs $(x_t,\truelab_t)$ are generated in an i.i.d manner by some unknown distribution $\calD$. The learner's goal is to compute, using as few samples as possible, a hypothesis $\hat h \in \calH$ such that with probability at least $1 - \delta$ (over all randomness during the interaction with the environment),
\begin{align*}
    \rew_\calD(h^\star) - \rew_\calD(\hat h) \leq \eps,
\end{align*}
where $\rew_\calD(h) \eqdef \E_{(x,\truelab) \sim \calD} \big[\rew(h(x);\truelab) \big] = \E_{(x,\truelab) \sim \calD} \big[\abs{h(x) \cap \truelab} \big]$ is the population reward of $h$, and $h^\star \eqdef \argmax_{h \in \calH} \rew_\calD(h)$ is the best hypothesis in the class $\calH$ w.r.t. $\calD$. The learner's performance in this setting is measured in terms of \emph{sample complexity}, that is, the number of samples $(x_t,\truelab_t)$ generated by the environment during the interaction as a function of $(\eps,\delta)$ after which the learner outputs a hypothesis $\hat h$ satisfying the guarantee above.

\paragraph{Regret minimization setting.} 
% 
In the online regret setting, the pairs $(x_t,\truelab_t)$ are generated by an oblivious adversary.\footnote{We consider an oblivious adversary throughout, though most of our results extend to an adaptive adversary.} 
The interaction lasts for $T$ rounds, where in each round $t$, after predicting $\predlist_t \in \calL$, the learner gains a reward $\rew_t(\predlist_t; \truelab_t) \eqdef \abs{\predlist_t \cap \truelab_t} \in [0,s]$, and the objective is to ultimately minimize \emph{regret} with respect to $\calH$, defined as
\begin{align*}
    \regret_T \eqdef \sup_{h^\star \in \calH} \sum_{t=1}^T \rew_t(h^\star(x_t); \truelab_t) - \sum_{t=1}^T \rew_t(\predlist_t; \truelab_t).
\end{align*}

% \paragraph{Additional notation.} Given an example-label pair $(x,y) \in \calX \times \calY$ we define the binary vector 
% \begin{align*}
%     r_{x,y}(h) = \mathbf{1} \brk[c]*{y \in h(x)}.
% \end{align*}
% Given $P \in \Delta_N$, the probability of predicting a list containing $y$ when sampling a hypothesis from $P$ is
% \begin{align*}
%     W_{x,y}(P) = \sum_{h \in \calH} P(h) \mathbf{1} \brk[c]*{y \in h(x)} = P \cdot r_{x,y}.
% \end{align*}

\subsection{Contextual Combinatorial Semi-Bandits (CCSB)}
\label{sec:setup-ccsb}

The semi-bandit multiclass list classification problem is in fact a special case of \emph{contextual combinatorial semi-bandits} (abbreviated as CCSB for conciseness), with the condition that the true label sets are of bounded size corresponding to a bound on the $L_1$-norm of the reward vectors. 

Formally, a CCSB instance is specified by a context space $\calX$, a set of actions (or arms) $\calY$ of size $K$ with an induced combinatorial action set $\calA \eqdef \brk[c]*{\pred \in \{ 0,1 \}^K \mid \norm{\pred}_1 = m}$,\footnote{In some formulations, the available actions come from a fixed, arbitrary subset of $\calA$; here we focus on the setting where all subsets of $\calY$ of size $m$ are valid.} and an underlying policy class $\Pi \subseteq \brk[c]*{\calX \to \calA}$. The learner interacts with the environment according to the following protocol for $t=1,2,\ldots$:
% 
\begin{enumerate}[leftmargin=!,itemsep=0pt,parsep=0pt]
    \item[(i)] The environment generates a context-reward vector pair $(x_t, \rew_t)$ where $x_t \in \calX$ and $\rew_t \in [0,1]^K$, the learner receives $x_t$;
    \item[(ii)] The learner predicts $\pred_t \in \calA$;
    \item[(iii)] The learner gains reward $\rew_t \cdot \pred_t$ (namely the sum of rewards of predicted actions) and observes \emph{semi-bandit feedback} consisting of the rewards of the predicted actions $\brk[c]*{\rew_t(\lab) \mid \lab \in \pred_t}$.
\end{enumerate}
% 
With this definition, it is easily seen that semi-bandit multiclass list classification is a special case of CCSB characterized by a specific structure of the reward vectors, namely they are constrained to be binary vectors. In the $s$-sparse version of the generalized problem, it is assumed that $\norm{\rew_t}_1 \leq s$ for all $t$ for some known bound $s$ (note that it is always the case that $s \leq K$). We remark that since $\rew_t(\cdot) \in [0,1]$, this sparsity condition also implies that $\norm{\rew_t}_2^2 \leq s$. 
% The notions of $(\eps,\delta)$-PAC and regret objectives are generalized in a straightforward manner to CCSB. \tk{avoid talking about generalization as much as possible} 
In the $(\eps,\delta)$-PAC setting,\footnote{This is often referred to as ``best-arm identification'' in the context of multi-armed bandit problems.} the data is generated via $(x_t,r_t) \overset{\mathrm{iid}}{\sim} \calD$ and the goal is to use as few rounds as possible in order to compute, with probability at least $1-\delta$, a policy $\piout \in \Pi$ satisfying
\begin{align*}
    \rew_\calD(\pi^\star) - \rew_\calD(\piout) \leq \eps,
\end{align*}
where $\rew_\calD(\pi) = \E_{(x,\rew) \sim \calD} \brk[s]*{\rew^\top \pi(x)}$ and $\pi^\star = \argmax_{\pi \in \Pi} \rew_\calD(\pi)$, 
and regret is defined as
\begin{align*}
    \regret_T \eqdef \sup_{\pi^\star \in \Pi} \sum_{t=1}^T \rew_t^\top \pi^\star(x_t) - \sum_{t=1}^T \rew_t^\top \pred_t.
\end{align*}
% 
Since our results for semi-bandit list classification are valid in this more general CCSB setting, we henceforth state them in terms of the latter.

% \paragraph{Additional notation.} Given a finite set $\calS$ we denote $\Delta_\calS \eqdef \{ (p_1, \ldots, p_{|S|}) \in \R^{|\calS|}_{\geq 0} \mid \sum_{i=1}^{|\calS|} p_i = 1 \}$. \liad{Move next to display in main setting} 

\paragraph{ERM oracle.} To discuss the computational efficiency of our PAC learning algorithm, we assume access to an empirical risk minimization (ERM) oracle for $\Pi$. This oracle, denoted by $\oracle_\Pi$, gets as input a collection of pairs $S = \brk[c]*{(x_1,\hat \rew_1),\ldots,(x_n,\hat \rew_n)} \subseteq \calX \times \R^K$ and returns
\begin{align*}
    \oracle_\Pi(S) \in \argmax_{\pi \in \Pi} \sum_{i=1}^n \sum_{\lab \in \pi(x_i)} \hat \rew_i(\lab).
\end{align*}
Up to the fact that the reward estimations can be negative, this is a natural generalization of the optimization oracle used in previous works on contextual bandits \citep{dudik2011efficient,agarwal2014taming}. When we refer to the computational complexity of our algorithm, we assume that each call to $\oracle_\Pi$ takes constant time.



% \newpage

\begin{algorithm}[ht]
    \caption{PAC-COMBAND}
    \label{alg:pac-comband}
    \begin{algorithmic}
        \STATE{Parameters: $N_1, N_2, \gamma \in (0,\frac12]$.}
        % \STATE{Input: Dataset $S = \brk[c]*{(x_1,Y_1),\ldots, (x_n,Y_n)}$ of $n$ i.i.d samples from $\calD$.}
        \STATE{\textbf{Phase 1:}
        Run \cref{alg:fw-main} with $N_1$ samples to approximately solve the optimization problem defined in \cref{eq:phase-1-objective} up to an additive error of $\mu = s \gamma^2 m^2 / 2K^2$.}
        \STATE{Whenever \cref{alg:fw-main} requests a sample, receive $x \in \calX$, predict a uniformly random $\pred \in \calA$, receive feedback $\brk[c]*{\rew(\lab) \mid \lab \in \pred}$ and provide \cref{alg:fw-main} with $\objrand(\cdot;x,\rew,\pred)$ as defined in \cref{eqn:obj-rand}.}
        % \FOR{$i=1,\ldots,N_1$}
        %     \STATE{\textcolor{gray}{Environment generates $(x_i,\rew_i) \sim \calD$, algorithm receives $x_i$.}}
        %     \STATE{Pick $\pred_i \in \calA$ uniformly at random.}
        %     \STATE{Predict $\pred_i$ and receive feedback $\brk[c]*{\rew_i(\lab) \mid \lab \in \pred_i}$.}
        %     % \STATE{Construct gradient estimate $g_i(\cdot) : \Delta_\Pi \to \R^{|\Pi|}$ by \tk{maybe defer this to the text, only say here which functions FW operates on}
        %     % \begin{align}
        %     % \label{eqn:grad-est}
        %     %     \brk*{g_i(p)}_\pi = - (1-\gamma) \frac{K}{m} \sum_{\lab \in \pred_i} \frac{\ind{\lab \in \pi(x_i)} \rew_i(\lab)}{\yprobg{p}{x_i}{y}}, \quad \forall \pi \in \Pi.
        %     % \end{align}
        %     % } 
        %     \STATE{Feed the following function $f(\cdot,x_i,r_i,a_i) : \Delta_\Pi \to \R$ \cref{alg:fw-main}: \liad{Maybe now it'd be better to construct a dataset and feed it to FW?}
        %     \begin{align*}
        %         f \brk*{p; x_i,\rew_i,\pred_i} 
        %         \eqdef
        %         \frac{K}{m} \sum_{\lab \in \pred_i} \rew_i(y) \log \brk*{\yprobg{p}{x_i}{\lab}}.
        %     \end{align*}
        %     }
        % \ENDFOR
        \STATE{Let $\hat{p} \in \Delta_\Pi$ be the output of \cref{alg:fw-main}.
        % to approximately solve the stochastic optimization problem defined in \cref{eq:phase-1-objective} up to an additive error of $\mu = s \gamma^2 m^2 / 2K^2$. Let $\hat{p} \in \Delta_\Pi$ be its output.
        }
        \STATE{\textbf{Phase 2:}}
        \FOR{$i = 1,\ldots,N_2$} 
            \STATE{\textcolor{gray}{Environment generates $(x_i,\rew_i) \sim \calD$, algorithm receives $x_i$.}}
            % \STATE{Receive $x_i$.}
            \STATE{With prob.~$\gamma$ pick $\pred_i \in \calA$ uniformly at random; otherwise sample $\pi_i \sim \hat p$ and set $\pred_i = \pi_i(x_i)$.}
            \STATE{Predict $\pred_i$ and receive feedback $\brk[c]*{\rew_i(\lab) \mid \lab \in \pred_i}$.}
        \ENDFOR
        \STATE{Return:
        \[
            \piout = 
            % \oracle_\calH \brk*{\brk[c]*{\brk*{x_i, \hat L_i, \alpha_i}}_{i=1}^{M}}, 
            % \argmax_{h \in \calH} \brk[c]*{\sum_{i=1}^{N_2} \sum_{y \in L_i \cap Y_i} \frac{\indh{x_i}{y}(h)}{\yprobg{\hat p}{x_i}{y}}}
            \oracle_\Pi \brk*{\brk[c]*{(x_i,\hat \rew_i)}_{i=1}^{N_2}}, \quad \text{where} \quad \hat \rew_i(\lab) = \frac{\ind{\lab \in \pred_i} \rew_i(\lab)}{\yprobg{\hat p}{x_i}{\lab}} \quad \forall \lab \in \calY.
        \]
        % where $\hat \rew_i(\lab) = \frac{\ind{\lab \in \pred_i} \rew_i(\lab)}{\yprobg{\hat p}{x_i}{\lab}}$ for $\lab \in \calY$. \tk{put this in the display above with some spacing?}
        }
    \end{algorithmic}
\end{algorithm}


\section{Main Result: Agnostic PAC Setting}
\label{sec:pac}
% 
In this section we design and analyze a PAC learning algorithm for CCSB with $s$-sparse rewards. Our algorithm is displayed in \cref{alg:pac-comband}, and our main result is detailed in the following theorem:

\begin{theorem}
    \label{thm:pac-main}
    If we set $\gamma = \frac12$, $N_1 = \Theta \brk[big]{\brk*{K/m}^{10} \log (|\Pi|/\delta))}$, $N_2 = \Theta \brk*{\brk*{K/m \eps + s m / \eps^2}\log(|\Pi| / \delta) }$ and use stochastic Frank-Wolfe (\cref{alg:fw-main}) in \cref{alg:pac-comband} 
    % with \liad{Bake batch \& step sizes into FW} step sizes $\eta_t = 1/t$ and batch sizes $b_t$ defined as
    % \begin{align*}
    %     b_t = 
    %     \begin{cases}
    %         (\gamma m / K)^2 t^2, & t=2^k-1, \quad k=1,2,\ldots \\
    %         t, & otherwise,
    %     \end{cases}
    % \end{align*}    
    for $T = \Theta \brk[big]{(K/m)^5 \sqrt{\log (|\Pi|/\delta)}}$ iterations, then with probability at least $1-\delta$ \cref{alg:pac-comband} outputs $\piout \in \calH$ with $r_\calD(\pi^\star) - r_\calD(\piout) \leq \eps$ using a sample complexity of 
    \begin{align*}
        N_1 + N_2 = O \brk*{\brk*{\brk*{\frac{K}{m}}^{10} + \frac{s m}{\eps^2}} \log \frac{|\Pi|}{\delta}}.
    \end{align*}
    Furthermore, \cref{alg:pac-comband} makes a total of $O(\sqrt{N_1}) = O \brk[big]{(K/m)^5 \sqrt{\log (|\Pi|/\delta)}}$ calls to $\oracle_\Pi$.
\end{theorem}
Given a context-action pair $(x,y) \in \calX \times \calY$ and $p \in \Delta_\Pi \eqdef \brk[c]*{p \in \R^{|\Pi|}_+ \mid \sum_{i=1}^{|\Pi|}p_i = 1}$ we denote 
\[
\yprob{p}{x}{y} \eqdef \sum_{\pi \in \Pi} p(\pi) \ind{y \in \pi(x)},
\]
i.e., the probability that $y$ belongs to $\pi(x)$ when sampling $\pi \sim p$, and given $\gamma \in (0,1)$ we define 
\[
\yprobg{p}{x}{y} \eqdef (1-\gamma) \yprob{p}{x}{y} + \gamma m / K,
\]
that is, the distribution induced by mixing $p$ with a uniform distribution over $\calA$.

At a high level, our algorithm initially uses a stochastic optimization procedure to find an approximate minimizer of the following stochastic convex optimization problem: 
\begin{align}
\label{eq:phase-1-objective}
    &\mathrm{minimize} \quad \obj(p) \eqdef \E_{z \sim \iwd} \brk[s]*{\objrand(p;z)}, \quad p \in \Delta_\Pi, \\
    % &\text{where} \quad \objrand(p;x,\rew) \eqdef - \sum_{\lab \in \calY} r(\lab) \log \brk*{\yprobg{p}{x}{\lab}} \nonumber
    \label{eqn:obj-rand}
    &\text{where} \quad f \brk*{p; z=(x,\rew,\pred)} 
                \eqdef
                \frac{K}{m} \sum_{\lab \in \pred} \rew(y) \log \brk*{\yprobg{p}{x}{\lab}}.
\end{align}
Here $\iwd$ is the product distribution over $\calZ \eqdef \calX \times [0,1]^K \times \calA$ defined as $\iwd \eqdef \calD \times \mathrm{Unif}(\calA)$, that is, a pair $(x,r) \in \calX \times [0,1]^K$ is sampled from $\calD$ and $\pred \in \calA$ is sampled \emph{independently} uniformly at random. The random objective $\objrand(\cdot;z)$ is defined according to \cref{eqn:obj-rand}, and we note that even though the full reward function $\rew$ is not observed, $\objrand(\cdot;z)$ depends only on the reward of the actions in $\pred \sim \calA$ and can thus be fully accessed. The $K/m$ factor in \cref{eqn:obj-rand} is necessary due to the random sampling of $\pred \sim \calA$ used for importance-weighted estimation of the reward function, and it is straightforward to see that the expected objective has the following form:
\begin{align}
    \label{eqn:obj-expected}
    \obj(p) = \E_{(x,\rew) \sim \calD} \brk[s]*{\sum_{\lab \in \calY} \rew(\lab) \log \brk*{\yprobg{p}{x}{\lab}}}.
\end{align}

This form of $\obj(\cdot)$ is of crucial importance as its gradient at a point $p$ is proportional to the variance of an appropriate unbiased reward estimator for the policies in $\Pi$:
\begin{align*}
    % \left| \brk*{\nabla \obj(p)}_\pi \right|\lesssim 
    % \E_{(x,r) \sim \calD} \brk[s]*{\sum_{\lab \in \calY} \frac{r(\lab)\ind{ \lab \in \pi(x)}}{\yprobg{p}{x}{\lab}}} \quad \forall \pi \in \Pi.
    \mathrm{Var} \brk[s]*{R_p(\pi_j)} \lesssim m \cdot \left| \frac{\partial F}{\partial p_j}(p) \right| \quad \forall \pi_j \in \Pi.
\end{align*}
As we will show, using only $\poly(K/m)$ samples we are able to compute $\hat p \in \Delta_\Pi$ for which $\nabla \obj(\hat p)$ is bounded in $L_\infty$-norm by $2s$. This fact generalizes the key insight from the previous works of \cite{dudik2011efficient,agarwal2014taming,erez2024fast} in the sense that the reward estimators induced by $\hat p$ have variance bounded by $C \cdot sm$, where $C$ is an absolute constant. Crucially, this variance bound doesn't depend directly on the number of actions $K$, implying that we can use $\hat p$ to estimate the expected rewards of the policies in $\Pi$ with only $\approx sm / \eps^2$ samples by Bernstein's variance-sensitive concentration inequality.





\subsection{Analysis}
\label{sec:pac-analysis}

Here we detail the main steps in the analysis of \cref{alg:pac-comband}, and in particular, outline the main challenges compared to the single-label setting where $s=m=1$.

\paragraph{Initial exploration.}
% 
% The main fundamental difference between our approach and that of \cite{erez2024fast} is that, 
While in the single label classification setting it is possible to collect a dataset containing $\mathrm{poly}(K)$ i.i.d samples from $\calD$ by simply predicting labels uniformly, this approach does not work in the multilabel classification setting (i.e. $s > 1$) and neither in CCSB with $s$-sparse rewards. The reason is that in these settings a uniform prediction of a list of actions will simply not yield a full observation of the reward vector, even after $\mathrm{poly}(K)$ such predictions. Thus, instead of collecting a dataset, we use semi-bandit feedback directly in order to estimate the convex objective of interest using importance sampling, as detailed in \cref{alg:pac-comband}. These random objectives are unbiased with respect to $\obj(\cdot)$ defined in \cref{eqn:obj-expected}, while importance weighting causes their smoothness parameter to increase by a factor of $K/m$. This ultimately results in  
% the sample complexity of the stochastic Frank-Wolfe algorithm \tk{we did not discuss this yet at this point} to increase by a $\mathrm{poly}(K/m)$ factor, which is the reason for 
a slightly worse dependence on $K$ in the lower order term of our bound.

% \paragraph{Stochastic Frank-Wolfe.} 
\paragraph{Optimizing for low-variance exploration.}

In order to approximately solve the stochastic convex optimization problem defined in \cref{eq:phase-1-objective}, we employ a stochastic Frank-Wolfe (FW) algorithm with SPIDER gradient estimates \citep{yurtsever19b}, or more specifically, an $L_1/L_\infty$-specialized variant analyzed by \cite{erez2024fast}, formally described in \cref{sec:fw-appendix}. For our setup, we make use of \cref{alg:fw-main} over samples generated by the distribution $\iwd$ instead of $\calD$, where crucially the gradients of the random objectives defined in \cref{eqn:obj-rand} can be calculated exactly via
\begin{align}
\label{eqn:grad-obj-rand}
    \frac{\partial \objrand(p;x,\rew,\pred)}{\partial p_j} = - (1-\gamma) \frac{K}{m} \sum_{\lab \in \pred} \frac{\ind{\lab \in \pi_j(x)} \rew(\lab)}{\yprobg{p}{x}{y}}, \quad \forall j \in [|\Pi|].
\end{align}
The FW algorithm uses these gradients with a linear optimization oracle $\loo_\Pi$, defined by
\begin{align*}
    \loo_\Pi(v) \eqdef \argmax_{p \in \Delta_\Pi} v \cdot p, \quad \forall v \in \R^{|\Pi|}.
\end{align*}
Importantly, each call to $\loo_\Pi$ can be implemented by a call to $\oracle_\Pi$ (see \cref{sec:fw-appendix} for the proof). The generic analysis of the stochastic FW algorithm by \cite{erez2024fast} applied to \cref{eq:phase-1-objective}, allows us to conclude that with an appropriate choice of parameters, \cref{alg:fw-main} outputs a distribution $\hat p \in \Delta_\Pi$ with $F(\hat p) - F(p_\star) \leq \mu$ with high probability using a sample complexity of $O \brk*{\frac{s^2 K^6}{\gamma^4 m^6 \mu^2} \log \frac{|\Pi|}{\delta}}$. This follows from Lipschitz and smoothness properties (with respect to the $L_1$/$L_\infty$ norms) of the functions defined in \cref{eqn:obj-rand}, namely they are $G$-Lipschitz with $G = s K^2 / (\gamma m^2)$ and $\beta$-smooth with $\beta = s K^3 / (\gamma^2 m^3)$. For more details, see \cref{sec:fw-appendix}.

\paragraph{Low-variance reward estimation.} In the second phase of \cref{alg:pac-comband}, we use the low-variance exploration distribution $\hat p \in \Delta_\Pi$ computed in the first phase in order to estimate the expected reward of all policies in $\Pi$ using importance-weighted estimators, defined as 
\begin{align*}
    R_i(\pi) \eqdef \sum_{\lab \in \pi(x)} \frac{\rew_i(\lab) \ind{\lab \in \pred_i}}{\yprobg{\hat p}{x_i}{\lab}}, \quad i \in [N_2].
\end{align*}
The guarantee on $\hat p$ provides us with the ability to bound the variance of these estimators by $O(sm)$ (with no explicit dependence on $K$) which is why, using Bernstein's inequality, $O( K / (m \eps) + sm / \eps^2)$ samples in the second phase are sufficient for the average estimated rewards to constitute $\eps$-approximations of the true rewards uniformly over all policies in $\Pi$, thus implying the PAC guarantee for the policy which maximizes the average estimated reward.

% The following result is a consequence of the generic analysis of \cref{alg:fw-main} by \cite{erez2024fast} (Theorem 3), specialized here for the gradient estimators defined in \cref{eqn:grad-obj-rand}. A proof can be found in \cref{sec:fw-appendix}.


\begin{proof}[Proof of \cref{thm:pac-main} (sketch).]
We now give an overview of the proof of \cref{thm:pac-main}. We rely on the following key lemma which shows that a sufficiently approximate optimum $\hat p$ of the optimization problem defined in \cref{eq:phase-1-objective} is a point at which the gradient is bounded in $L_\infty$ norm by $s$. The proof of this lemma can be found in \cref{sec:pac-proofs}.
\end{proof}


\begin{lemma}
\label{lem:log-self-concordance}
Suppose $\gamma \leq \frac12$ and let $\hat p \in \Delta_{\Pi}$ be an approximate minimizer of the optimization problem defined in \cref{eq:phase-1-objective} up to an additive error of $\mu$.
Then,
% 
\begin{align*}
    \norm{\nabla \obj(\hat p)}_\infty \leq s + \sqrt{\frac{2 s \mu K^2}{\gamma^2 m^2}}.
\end{align*}
In particular, setting $\mu = s \gamma^2 m^2 / 2 K^2$ gives $\norm{\nabla \obj(\hat p)}_\infty \leq 2s$.
\end{lemma}
% 
Applying \cref{lem:log-self-concordance} with the guarantee for the stochastic FW algorithm, we deduce that phase 1 of \cref{alg:pac-comband} yields a distribution $\hat p \in \Delta_\Pi$ satisfying
\begin{align}
\label{eqn:variance-bound}
    \E_{(x,\rew) \sim \calD} \brk[s]*{\sum_{\lab \in \pi(x)} \frac{\rew(\lab) }{\yprobg{\hat p}{x}{\lab}}} \leq 4s \quad \forall \pi \in \Pi.
\end{align}
In phase 2, \cref{alg:pac-comband} uses samples from $\hat p$ to estimate the expected rewards of policies in $\Pi$ with the following estimators: 
\begin{align*}
    R_i(\pi) \eqdef \sum_{\lab \in \pi(x)} \frac{\rew_i(\lab) \ind{\lab \in \pred_i}}{\yprobg{\hat p}{x_i}{\lab}}, \quad i \in [N_2].
\end{align*}
It is straightforward to see that this is an unbiased estimator for $\rew_\calD(\pi)$, and moreover, using the Cauchy-Schwarz inequality and \cref{eqn:variance-bound}, its variance can be upper bounded by
\begin{align*}
    \mathrm{Var} \brk[s]*{R_i(\pi)} \leq m \E \brk[s]*{\sum_{\lab \in \pi(x_i)} \frac{\rew_i(\lab)}{\yprobg{\hat p}{x_i}{\lab}}} \leq 4sm.
\end{align*}
Using Bernstein's inequality and the fact that the random variables $R_i(\pi)$ are bounded by $K / (\gamma m)$ we deduce that the following sample complexity suffices for $(\eps,\delta)$-PAC:
\begin{align*}
    N_1 + N_2 = \Theta \brk*{\brk*{\brk*{\frac{K}{m}}^{10} +  \frac{K}{m \eps} + \frac{sm}{\eps^2}} \log \frac{|\Pi|}{\delta}}, 
\end{align*}
and second term can be dropped since the sum of the other two terms dominates the bound.




\subsection{Lower Bound}
\label{sec:lower-bound}


We now present a lower bound by which the dependence on $m$ and $s$ in \cref{thm:pac-main} is tight even in the non-contextual setting. 

\begin{theorem}
\label{thm:lower-bound}
    For any combinatorial semi-bandit algorithm $\mathsf{Alg}$ over $K$ actions and combinatorial actions of size $m \leq K/2$, there exists an $s$-sparse instance for which in order for $\mathsf{Alg}$ to output $\hat \pred \in \calA$ which is $\eps$-optimal for $\eps \ll 1/K$ with constant probability, $\mathsf{Alg}$ requires a sample complexity of at least $\widetilde \Omega \brk*{sm/\eps^2}$.
\end{theorem}
We provide a sketch of the proof of \cref{thm:lower-bound} below, and emphasize that it can be made formal using standard information-theoretic arguments. Additionally, this result can be easily extended to the classification setting by introducing correlation between the arms such that any realized reward will have $s$ arms with a reward of $1$ and the others with reward zero.

\begin{proof}[Proof of \cref{thm:lower-bound} (sketch).] Consider an instance specified by $m$ Bernoulli arms with expected reward $\frac{s}{K} + \frac{\eps}{m}$ each while the other $K-m$ arms have expected reward $\frac{s}{K} - \frac{\eps}{K-m}$. Denote the set of $m$ arms with highest expected reward by $\mathcal{S}$. Now, note that in order to find an $\frac{\eps}{2}$-optimal subset, $\mathsf{Alg}$ must identify at least half of the arms in $\mathcal{S}$, and thus is required to estimate all of the arms' rewards up to an accuracy of $\frac{\eps}{2m}$. Since each arm's reward distribution has variance of $\approx \frac{s}{K}$, the number of samples required to make such an estimation of the reward for each arm $\lab$ is
    \begin{align*}
        \frac{\operatorname{Var}(r_\lab)}{(\eps / m)^2} \approx \frac{s}{K} \cdot \frac{m^2}{\eps^2}.
    \end{align*}
    Since such an estimation needs to be performed for all $K$ arms and each time step gives $\mathsf{Alg}$ samples for $m$ arms via semi-bandit feedback, the total sample complexity is of order at least
    \begin{align*}
        \frac{K}{m} \cdot \frac{s}{K} \cdot \frac{m^2}{\eps^2} = \frac{s m}{\eps^2}.
    \end{align*}
\end{proof}


\section{Online Regret Minimization Setting}
\label{sec:regret}

In this section we present a regret minimization algorithm for CCSB with rewards that satisfy $\norm{\rew_t}_2^2 \leq s$ which is a slightly relaxed version of the assumption $\norm{\rew_t}_1 \leq s$. Instead of rewards in $[0,1]$, it will be convenient for us to instead consider losses, so we introduce the following negative loss vectors:
\begin{align*}
    \ell_t(\lab) = - \rew_t(\lab) \quad \forall t \in [T], \lab \in \calY.
\end{align*}
It is obvious that the losses satisfy $\norm{\ell_t}_2^2 \leq s$ and that the transformation from rewards to losses does not affect the regret. A natural approach for regret minimization in CCSB would be to use the EXP4 algorithm \citep{auer2002nonstochastic} over the policy class $\Pi$, which amounts to Follow-the-regularized-leader (FTRL) using \emph{negative entropy} regularization, defined as
\begin{align}
\label{eqn:entropy}
    H(p) \eqdef \sum_{i=1}^{|\Pi|} p_i \log p_i, \quad p \in \Delta_\Pi.
\end{align}
However, as been observed by \cite{erez2024real}, since the loss vectors are negative, this approach alone would not suffice to achieve a regret bound that is adaptive to the sparsity level $s$, and would instead yield a regret bound of the form $O(\sqrt{mKT \log |\Pi|})$. In order to adapt to sparsity, they introduce an additional \emph{log-barrier} regularization, defined as
\begin{align}
\label{eqn:log-bar}
    \Phi(p) \eqdef \sum_{i=1}^{|\Pi|} \log p_i, \quad p \in \Delta_\Pi.
\end{align}
We adopt this approach and generalize it to the combinatorial setup with \cref{alg:exp4-comb}.


\begin{algorithm}[ht]
\caption{EXP4-COMB-SPARSE}
\label{alg:exp4-comb}
    \begin{algorithmic}
        \STATE Parameters: $m, K, T, s$, finite policy class $\Pi \subseteq \brk[c]*{\calX \to \calA}$, step sizes $\eta > 0, 0 < \nu \leq 1$.
        \STATE Initialize $p_1 \in \Delta_\Pi$ as the uniform distribution over $\Pi$.
        \FOR{$t=1,2,\ldots,T$}
            \STATE Obtain $x_t \in \calX$.
            \STATE Sample $\pi_t \sim p_t$  and let $\pred_t = \pi_t(x_t) \in \calA$.
            \STATE Observe $\brk[c]*{\ell_t(\lab) \mid \lab \in \pred_t}$ and construct importance-weighted loss estimators for policies in $\Pi$ via
            \begin{align*}
                \hat c_t(i) = \sum_{\lab \in \pi_i(x_t)} \frac{\ell_t(\lab) \mathbf{1} \brk[c]*{\lab \in \pred_t}}{Q_t(\lab)} \quad \forall i \in [|\Pi|],
            \end{align*}
            where $Q_t(\lab) = \sum_{i=1}^{|\Pi|} p_t(i) \ind{\lab \in \pi_i(x_t)}$ is the probability that $\lab$ belongs to $\pred_t$ when sampling a policy using $p_t$.
            \STATE Update $p_t$ via
            \begin{align*}
                p_{t+1} = \argmin_{p \in \Delta_\Pi} \brk[c]*{p \cdot \sum_{\tau=1}^t \hat c_\tau + \frac{1}{\eta} H(p) + \frac{1}{\nu} \Phi(p)},
            \end{align*}
            where $H(\cdot)$ and $\Phi(\cdot)$ are defined in \cref{eqn:entropy} and \cref{eqn:log-bar} respectively.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}



We prove the following result for \cref{alg:exp4-comb}:
% 
\begin{theorem}
\label{thm:exp4-comb-regret}
    Assume that the loss vectors $\ell_t$ satisfy $\norm{\ell_t}_2^2 \leq s$ for all $t \in [T]$. Then \cref{alg:exp4-comb} with $\nu = \frac{1}{16}$ and $\eta = \sqrt{\log (|\Pi|)/(msT)}$ attains the following expected regret bound w.r.t.~a finite policy class $\Pi$:
    \begin{align*}
        \E \brk[s]*{\regret_T} &\leq O \brk*{|\Pi| \log (|\Pi|T) + \sqrt{smT \log |\Pi|}}.
    \end{align*}
\end{theorem}
% 
We remark that the linear dependence on $|\Pi|$ is essentially tight if we require the leading term to depend on $s$ rather than $K$, as shown in \cite{erez2024real} for the single-label setting. 

\begin{proof}[Proof of \cref{thm:exp4-comb-regret} (sketch).] We now give a high level overview of the proof of \cref{thm:exp4-comb-regret}; the full proof can be found in \cref{sec:regret-proofs}. We rely on the following expected regret bound which stems from a generic FTRL analysis: 
\begin{align*}
    \E[\regret_T] \leq 1 + \frac{|\Pi| \log (|\Pi|T)}{\nu} + \frac{\log |\Pi|}{\eta} + \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i)\hat c_t(i)^2},
\end{align*}
We focus on the last term in the bound which we refer to as the \emph{stability} term. In the combinatorial setting, the stability term incurs an extra $m$ factor in comparison to the non-combinatorial setting. Indeed, Using the Cauchy-Schwarz inequality, for all $t \in [T]$ and $i \in [|\Pi|]$ we have
\begin{align*}
    \E_t \brk[s]*{\hat c_t(i)^2} \leq m \E_t \brk[s]*{\sum_{\lab \in \pi_i(x_t)} \brk*{\frac{\ell_t(\lab) \mathbf{1} \brk[c]*{\lab \in \pred_t}}{Q_t(\lab)}}^2} 
    =
    m \sum_{\lab \in \pi_i(x_t)} \frac{\ell_t(\lab)^2}{Q_t(\lab)}.
\end{align*}
Taking expectations, the stability term is bounded by
\begin{align*}
    \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i) \E_t \brk[s]*{ \hat c_t(i)^2 }}
    \leq 
    \eta m \E \brk[s]*{\sum_{t=1}^T \norm{\ell_t}_2^2}
    \leq \eta s m T,
\end{align*}
and plugging in the specified values for $\eta$ and $\nu$ gives the desired bound.
\end{proof}

% The proof of \cref{thm:exp4-comb-regret} follows similar lines to the analysis of \cite{erez2024real} in terms of the stability properties obtained by using a log-barrier regularization. In the combinatorial setting, however, the stability term arising from the FTRL regret bound incurs an extra $m$ factor in comparison to the non-combinatorial setting. The proof of \cref{thm:exp4-comb-regret} relies on the following result which stems from a generic analysis of FTRL, and whose proof can be found in \cref{sec:regret-proofs}.





\section{Discussion \& Open Problems}
\label{sec:discussion}

We conclude the paper with a brief discussion of several open questions and interesting directions for further research.

\paragraph{Full-bandit feedback.} Our results for PAC learning in the semi-bandit setting raise a natural question regarding the \emph{full-bandit} feedback model, which is characterized by the fact that the observation upon predicting $\pred_t \in \calA$ consists of a single number $\rew_t^\top \pred_t$. Specifically, it is unclear given our results whether adapting to reward sparsity and obtaining improved sample complexity bounds of the form $\poly(s,m) / \eps^2$ is possible in the full-bandit model.
% \emph{What is the sample complexity of PAC learning in contextual combinatorial bandits with $s$-sparse rewards?} This setting is characterized by the fact that the observation upon predicting $\pred_t \in \calA$ consists of a single number $\rew_t^\top \pred_t$. 
While we do not fully know how to extend our techniques to the full-bandit setting, we present some interesting ideas which may provide initial directions towards answering this question. We focus on the question of how we should modify the log-barrier potential given in \cref{eq:phase-1-objective} to accommodate full-bandit feedback, that is, such that its gradient would correspond to the variance of an appropriate full-bandit reward estimator. Indeed, importance-weighted reward estimators for combinatorial bandits are widely used in the literature~\citep[see, e.g.,][]{audibert2014regret}, and it is straightforward to construct such an unbiased estimator $R(\pi)$ and show that its variance can be bounded as
\begin{align*}
    \mathrm{Var} \brk[s]*{R(\pi)}
    \leq s^2 \cdot \pi(x)^\top C^{-1} \pi(x),
\end{align*}
where $C$ is the covariance matrix associated with sampling a policy from $p$.
Interestingly, the latter quantity can be shown to be proportional to the gradient of the following log-determinant potential:
\begin{align*}
    \objrand(p;x) \eqdef - \log \det \brk*{C(p; x)} = - \log \det \brk*{\sum_{k=1}^{|\Pi|} p(k) \pi_k(x) \pi_k(x)^\top}
    ~,
\end{align*}
which would seem like a natural generalization of the log-barrier potential used in \cref{eq:phase-1-objective}. This variance bound, however, contains no dependence on the reward function $r$, and in turn the existing analysis would result in a sample complexity of $\approx\! K / \eps^2$, which we would like to avoid. 
% We emphasize that given an appropriate potential for which the gradient at an approximate minimizer is upper bounded by a quantity independent of $K$ (as with \cref{lem:log-self-concordance}), we can employ a similar technique to that used in the first phase of \cref{alg:pac-comband} with full-bandit gradient estimators in order to efficiently find an exploration distribution which would allow for uniform reward estimation using only $\approx  \mathrm{poly}(s,m) / \eps^2$ samples, thus resulting in an improved sample complexity bound for the full-bandit setting.

\paragraph{Improving the dependence on $\boldsymbol{K}$.} A natural question which is also left open in \cite{erez2024fast} for the single-label setting concerns improving the dependence on $K$ in the additive term of the sample complexity bound. We strongly believe that this term could be significantly improved, potentially reaching $K / \eps$ which would result in a minimax optimal sample complexity bound. The high polynomial degree in the current bound stems from the use of stochastic Frank-Wolfe as a means for obtaining the desired exploration distribution, and the convergence guarantees of this algorithm depend polynomially on the objective's very large ($\approx\! K^3$) smoothness parameter. Intuitively, minimizing the objective $\obj(\cdot)$ itself is not our primary goal, but rather we are interested in finding a point at which the gradient of $\obj(\cdot)$ is small. This naturally leads us to look for an optimization procedure which would minimize the norm of the gradient of $F(\cdot)$ directly, which can be done in various unconstrained optimization problems~\citep[see, e.g.,][]{foster2019complexity}, using a sample complexity which only depends logarithmically on the objective's smoothness. The constrained nature of our problem of interest, though, poses a significant challenge to adopting this approach directly. Nevertheless, we conjecture that leveraging specific properties of the objective function (such as self-concordance) may enable extending these methods to constrained settings. Such results would not only improve the sample complexity bounds for bandit multiclass classification but could also be of independent interest to the convex optimization research community. 

\paragraph{Extension to infinite classes.} Another natural research direction is to extend our result given in \cref{thm:pac-main} to hypothesis classes which could be infinite, but exhibit certain properties which allow for replacing the dependence on $\log |\calH|$ with some combinatorial dimension. For the single-label setting, \cite{erez2024fast} show that a finite Natarajan dimension is indeed such a property. We thus expect that for list classification, an appropriate generalization of the Natarajan dimension would give such results, in a similar manner to how the Littlestone dimension is generalized by \cite{moran2023list} to list classification in the online setting.

% Acknowledgments---Will not appear in anonymized version
\section*{Acknowledgements}
This project has received funding from the Israel Science Foundation (ISF, grant number 3174/23), and from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant agreement No.\ 101078075). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.
% This project has also received funding from 
% the Israel Science Foundation (ISF, grant number 3174/23).


\bibliography{bibliography}
\bibliographystyle{abbrvnat}

\newpage
\appendix

% \crefalias{section}{appendix} % uncomment if you are using cleveref

\section[Proofs for Section 3]{Proofs for \cref{sec:pac}}
\label{sec:pac-proofs}

To prove \cref{lem:log-self-concordance}, we will need the following auxiliary result:



\begin{lemma}
    \label{lem:bounded-grad}
    Suppose $\gamma \leq \frac12$ and let $p^\star = \argmin_{P \in \Delta_\Pi} \obj(P)$. 
    % Then for any $P \in \Delta_N$ it holds that
    % \begin{align*}
    %     \E \brk[s]*{\sum_{y \in Y}\frac{\yprobg{P}{x}{y}}{\yprobg{p^\star}{x}{y}}} \leq s.
    % \end{align*}
    % Then, for all $\pi \in \Pi$:
    % \begin{align*}
    %     \E_{(x,r) \sim \calD} \brk[s]*{\sum_{\lab \in \calY} \frac{r(\lab)\ind{ \lab \in \pi(x)}}{\yprobg{p^\star}{x}{\lab}}} \leq 2s.
    % \end{align*}
    Then it holds that
    $
        \norm{\nabla \obj(p^\star)}_\infty \leq s.
    $
\end{lemma}

\begin{proof}
    First note that the gradient of $\obj(\cdot)$ is given by
    \begin{align*}
        \brk*{\nabla \obj(p)}_\pi
        &=
        \E \brk[s]*{- \sum_{\lab \in \calY} \frac{(1-\gamma) \rew(\lab) \ind{ \lab \in \pi(x)}}{\yprobg{p}{x}{\lab}}}.
    \end{align*}
    Thus, using first-order convex optimality conditions, the following holds that for any $p \in \Delta_\Pi$,
    \begin{align*}
        \nabla \obj(p^\star) \cdot \brk*{p - p^\star} \geq 0,
    \end{align*}
    which with the explicit form of the gradient becomes
    \[
         \mathbb{E}\brk[s]*{\sum_{\lab \in \calY} \frac{(1-\gamma) \rew(\lab) \sum_{\pi \in \Pi} \brk*{p(\pi) - p^\star(\pi)} \ind{\lab \in \pi(x)}}{\yprobg{p^\star}{x}{\lab}}} 
        \le 
        0,
    \]
    or equivalently, after dividing by $1-\gamma$,
    \[
    \E \brk[s]*{\sum_{y \in \calY} \frac{\rew(\lab) \brk*{\yprobg{p}{x}{\lab} - \yprobg{p^\star}{x}{\lab}}}{\yprobg{p^\star}{x}{\lab}}} \leq 0,
    \]
    which rearranges to 
    \begin{align*}
        \E \brk[s]*{\sum_{y \in \calY}\frac{\rew(\lab) \yprobg{p}{x}{\lab}}{\yprobg{p^\star}{x}{\lab}}} \leq \E \brk[s]*{\sum_{y \in \calY} \rew(y)} = \E \brk[s]*{\norm{r}_1} \leq s.
    \end{align*}   
    Letting $p \in \Delta_\Pi$ be the distribution concentrated at some $\pi \in \Pi$, we get
    \[
        \mathbb{E}\brk[s]*{(1-\gamma) \sum_{y \in \calY}\frac{\rew(\lab) \ind{\lab \in \pi(x)}}{\yprobg{p^\star}{x}{\lab}}} \le s,                
    \]
    and the claim follows since the left-hand side equals $\brk*{- \nabla \obj(p^\star)}_\pi$ and is nonnegative.    
\end{proof}


\begin{proof}[Proof of \cref{lem:log-self-concordance}]
    Using \cref{lem:bounded-grad} and the triangle inequality, it suffices to show that 
    \begin{align*}
        \norm{\nabla \obj(\hat p) - \nabla \obj(p^\star)}_\infty \leq \sqrt{\frac{2 s \mu K^2}{\gamma^2 m^2}},
    \end{align*}
    and by the assumption on $\hat p$ this clearly holds if
    \begin{align*}
        \norm{\nabla \obj(\hat p) - \nabla \obj(p^\star)}^2_\infty \leq \frac{2 s K^2}{\gamma^2 m^2} \brk*{\obj(\hat p) - \obj(p^\star)}.
    \end{align*}
    To prove this inequality, we start with the left-hand side and use the explicit form of $\nabla \obj(\cdot)$:
    \begin{align}
    \label{eqn:sq-self-concordance}
        \norm{\nabla \obj(\hat p) - \nabla \obj(p^\star)}^2_\infty
        &=
        \max_{\pi \in \Pi} (1-\gamma)^2\brk*{\E \brk[s]*{\sum_{\lab \in \calY} \frac{\rew(\lab) \ind{\lab \in \pi(x)}}{\yprobg{\hat p}{x}{\lab}}} - \E \brk[s]*{\sum_{\lab \in \calY} \frac{\rew(\lab) \ind{\lab \in \pi(x)}}{\yprobg{p^\star}{x}{\lab}}}}^2 \nonumber\\
        &\leq
        \E \brk*{\sum_{\lab \in \calY} \frac{\rew(\lab) }{\yprobg{\hat p}{x}{\lab}} - \sum_{\lab \in \calY} \frac{\rew(\lab) }{\yprobg{p^\star}{x}{\lab}}}^2 \nonumber\\
        &=
        \E \brk[s]*{\norm{\rew}^2_1 \brk*{\sum_{\lab \in \calY} \frac{\rew(\lab)}{\norm{r}_1} \brk*{\frac{1}{\yprobg{\hat p}{x}{\lab}} - \frac{1}{\yprobg{p^\star}{x}{\lab}}}}^2} \nonumber\\
        &\leq
        \E \brk[s]*{\norm{\rew}_1 \sum_{\lab \in \calY} \rew(\lab) \brk*{\frac{1}{\yprobg{\hat p}{x}{\lab}} - \frac{1}{\yprobg{p^\star}{x}{\lab}}}^2} \nonumber\\
        &\leq
        s \cdot \E \brk[s]*{\sum_{\lab \in \calY} \rew(\lab) \brk*{\frac{1}{\yprobg{\hat p}{x}{\lab}} - \frac{1}{\yprobg{p^\star}{x}{\lab}}}^2},
    \end{align}
    where we used Jensen's inequality twice and the fact that $\norm{\rew}_1 \leq s$. Now, for any $(x,\lab) \in \calX \times \calY$ it holds that
    \begin{align*}
        \brk*{\frac{1}{\yprobg{\hat p}{x}{\lab}} - \frac{1}{\yprobg{p^\star}{x}{\lab}}}^2
        &=
        \frac{\brk*{\yprobg{\hat p}{x}{\lab} - \yprobg{p^\star}{x}{\lab}}^2}{\brk*{\yprobg{\hat p}{x}{\lab}}^2 \brk*{\yprobg{p^\star}{x}{\lab}}^2} \\
        &\leq \frac{K^2}{\gamma^2 m^2} \min \brk[c]*{\brk*{1 - \frac{\yprobg{\hat p}{x}{\lab}}{\yprobg{p^\star}{x}{\lab}}}^2, \brk*{1 - \frac{\yprobg{p^\star}{x}{\lab}}{\yprobg{\hat p}{x}{\lab}}}}, 
    \end{align*}
    where we have used the fact that $\yprobg{\cdot}{x}{y} \geq \gamma m / K$. Using the fact that $\frac12 \min \brk[c]*{(1-z)^2, (1-1/z)^2} \leq - \log z + z - 1$ with $z =\yprobg{\hat p}{x}{\lab} /  \yprobg{p^\star}{x}{\lab}$ (see e.g. \cite{erez2024fast}, Lemma 4) we obtain, after combining the above with \cref{eqn:sq-self-concordance} and taking expectations:
    \begin{align*}
        \norm{\nabla \obj(\hat p) - \nabla \obj(p^\star)}^2_\infty
        &\leq
        \frac{2 s K^2}{\gamma^2 m^2} \E \brk[s]*{\sum_{\lab \in \calY} \rew(\lab) \brk[c]*{- \log \frac{\yprobg{\hat p}{x}{\lab}}{\yprobg{p^\star}{x}{\lab}} + \frac{\yprobg{\hat p}{x}{\lab} - \yprobg{p^\star}{x}{\lab}}{\yprobg{p^\star}{x}{\lab}}}} \\
        &=
        \frac{2 s K^2}{\gamma^2 m^2} \E_{z \sim \iwd} \brk[s]*{\objrand(\hat p, z) - \objrand(p^\star,z) - \nabla \objrand(p^\star,z) \cdot (\hat p - p^\star)} \\
        &=
        \frac{2 s K^2}{\gamma^2 m^2} \brk*{\obj(\hat p) - \obj(p^\star) - \nabla \obj(p^\star) \cdot (\hat p - p^\star)} \\
        &\leq
        \frac{2 s K^2}{\gamma^2 m^2} \brk*{\obj(\hat p) - \obj(p^\star)},
    \end{align*}
    where in the second inequality we used first-order optimality conditions.
\end{proof}

\begin{proof}[Proof of \cref{thm:pac-main}]
    By \cref{lem:log-self-concordance} and \cref{thm:fw-conv}, phase 1 of \cref{alg:pac-comband} results in a distribution $\hat p \in \Delta_\Pi$ satisfying
    \begin{align*}
        (1-\gamma) \E_{(x,\rew) \sim \calD} \brk[s]*{\sum_{\lab \in \calY} \frac{\rew(\lab) \ind{\lab \in \pi(x)}}{\yprobg{\hat p}{x}{\lab}}} \leq 2s \quad \forall \pi \in \Pi,
    \end{align*}
    and since $\gamma = 1/2$ we get
    \begin{align*}
        \E_{(x,\rew) \sim \calD} \brk[s]*{\sum_{\lab \in \pi(x)} \frac{\rew(\lab) }{\yprobg{\hat p}{x}{\lab}}} \leq 4s \quad \forall \pi \in \Pi.
    \end{align*}
    Fix $\pi \in \Pi$ and for $i \in [N_2]$ define the induced importance-weighted reward estimator by $R_i(\pi) \eqdef \sum_{\lab \in \pi(x)} \frac{\rew_i(\lab) \ind{\lab \in \pred_i}}{\yprobg{\hat p}{x_i}{\lab}}$. This is an unbiased estimator of $\rew_\calD(\pi)$, since for a given $\lab \in \calY$ the probability that $y \in \pred_i$ equals $\yprobg{\hat p}{x}{\lab}$, and thus
    \begin{align*}
        \E \brk[s]*{R_i(\pi)} = \E \brk[s]*{\sum_{\lab \in \pi(x)} \rew(y)} = \E \brk[s]*{\rew \cdot \pi(x)} = \rew_\calD(\pi).
    \end{align*}
    Moreover, $R_1(\pi),\ldots,R_{N_2}(\pi)$ are i.i.d and exhibit the following variance bound:
    \begin{align*}
        \mathrm{Var} \brk[s]*{R_i(\pi)}
        &\leq
        \E \brk[s]*{R_i(\pi)^2} \\
        &=
        \E \brk[s]*{\brk*{\sum_{\lab \in \pi(x_i)} \frac{\rew_i(\lab) \ind{y \in \pred_i}}{\yprobg{\hat p}{x_i}{\lab}}}^2} \\
        &=
        m^2 \E \brk[s]*{\brk*{\frac{1}{m}\sum_{\lab \in \pi(x_i)} \frac{\rew_i(\lab) \ind{\lab \in \pred_i}}{\yprobg{\hat p}{x_i}{\lab}}}^2} \\
        &\leq
        m \E \brk[s]*{\sum_{\lab \in \pi(x_i)} \brk*{\frac{\rew_i(\lab) \ind{\lab \in \pred_i}}{\yprobg{\hat p}{x_i}{\lab}}}^2} \\
        &=
        m \E \brk[s]*{\sum_{\lab \in \pi(x_i)} \frac{\rew_i(\lab)}{\yprobg{\hat p}{x_i}{\lab}}} \leq 4sm,
    \end{align*}
    where in the first inequality we use Jensen's inequality. With the variance bound, we use Bernstein's inequality (e.g. \cite{lattimore2020bandit}, page 86) and the fact that the random variables $R_i(\pi)$ are bounded by $K / (\gamma m)$ to deduce that the following sample complexity suffices to have $\rew_\calD(\pi^\star) - \rew_\calD(\piout) \leq \eps$ with probability at least $1-\delta$:
    \begin{align*}
        N_1 + N_2 = \Theta \brk*{\brk*{\brk*{\frac{K}{m}}^{10} +  \frac{K}{m \eps} + \frac{sm}{\eps^2}} \log \frac{|\Pi|}{\delta}}, 
    \end{align*}
    and the proof is concluded once we note that the second term can be dropped via Young's inequality which shows that the sum of the other two terms dominates the bound.
\end{proof}

\subsection{Stochastic Frank-Wolfe}
\label{sec:fw-appendix}

We now present the details of the stochastic Frank-Wolfe optimization algorithm used to obtain an approximate minimizer of the objective defined in \cref{eq:phase-1-objective}. The algorithm operates under the following stochastic optimization model:
\begin{align*}
    &\mathrm{minimize} \quad \obj(w) \eqdef \E_z \brk[s]*{\objrand(w;z)}, \quad p \in \calW,
\end{align*}
where $\calW \subseteq \R^d$ is a convex domain. We assume that the algorithm has access to functions $\brk[c]*{f(\cdot,z_i)}_{i=1}^n$ where $z_i$ are sampled i.i.d. from an underlying distribution over a sample space $\calZ$, and the functions satisfy the following Lipschitz and smoothness conditions:
\begin{itemize}
    \item For all $w \in \calW$ and $z \in \calZ$ it holds that $\norm{\nabla \objrand(w;z)} \leq G$,
    \item For all $u,w \in \calW$ and $z \in \calZ$ it holds that $\objrand(w;z) \leq \objrand(u;z) + \nabla \objrand(u;z)^\top (w-u) + \frac{\beta}{2} \norm{w-u}^2_1$.
\end{itemize}

\begin{algorithm}
    \caption{Stochastic Frank-Wolfe with SPIDER gradient estimates}
    \label{alg:fw-main}
    \begin{algorithmic}
        \STATE{Parameters: Step sizes $\brk[c]*{\eta_t}_{t}$, batch sizes $\brk[c]*{b_t}_t$.}
        \STATE{Initialize $p_1 \in \Delta_\Pi$ and initial gradient estimate $v_1 = 0$.}
        \FOR{$t = 1,2,\ldots$}
            % \STATE{Set \tk{move this setting to theorem}}
            % \[
            %     b_t = \begin{cases}
            %     (G/\beta D)^2 \, t^2 & \text{if $t \in \set{\tau_k}_{k \geq 1}$};
            %     \\
            %     t & \text{otherwise.}
            %     \end{cases}
            % \]
            \STATE{Obtain $b_t$ functions $\objrand(\cdot;z_1),\ldots, \objrand(\cdot;z_{b_t})$;}
            \IF{$t = 2^k - 1$ for some $k=1,2,\ldots$}
            \STATE{Set}
            \[
                v_t = \frac{1}{b_t} \sum_{i=1}^{b_t} \nabla \objrand(p_t;z_i) ;
            \]
            \ELSE
            \STATE{Set}
            \[
                v_t = v_{t-1} + \frac{1}{b_t} \sum_{i=1}^{b_t} \br{ \nabla \objrand(p_t;z_i) - \nabla \objrand(p_{t-1};z_i) };
            \]
            \ENDIF
            \STATE{Compute $q_t = \loo_\Pi(v_t)$;}
            \STATE{Update $p_{t+1} = (1-\eta_t)p_t + \eta_t q_t$;}
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

Applying the generic result of this version of stochastic FW given in Theorem 3 of \cite{erez2024fast} to the stochastic objective defined in \cref{eqn:obj-rand}, we obtain the following result stating the rate of convergence and oracle complexity of \cref{alg:fw-main}.

\begin{theorem}
\label{thm:fw-conv}
    Let $p^\star = \argmin_{p \in \Delta_{\Pi}} \obj(p)$. Then \cref{alg:fw-main} with step sizes $\eta_t = 1/t$ and batch sizes $b_t$ defined by
    \begin{align*}
        b_t = 
        \begin{cases}
            (\gamma m / K)^2 t^2, & t=2^k-1, \quad k=1,2,\ldots \\
            t, & otherwise,
        \end{cases}
    \end{align*}
    outputs $\hat p \in \Delta_\Pi$ with $\obj(\hat p) - \obj(p^\star)  \leq \mu$ with probability at least $1-\delta$ using a sample complexity of 
    \begin{align*}
        O \brk*{\frac{s^2 K^6}{\gamma^4 m^6 \mu^2} \log \frac{|\Pi|}{\delta}}.
    \end{align*}
    Furthermore, \cref{alg:fw-main} makes at most $O \brk*{(s K^3) / (\gamma^2 m^3 \mu) \sqrt{\log |\Pi| / \delta}}$ calls to $\loo$.
\end{theorem}

\begin{proof}
    Examining the conditions of Theorem 3 in \cite{erez2024fast}, it suffices to prove the following properties of the functions $\objrand(\cdot;z)$ as defined in \cref{eqn:obj-rand}, where $z = (x,r,a) \in \calZ$:
    \begin{itemize}
        \item[(a)] Unbiased: \[
        \E_z \brk[s]*{\objrand(p;z)} = \nabla \obj(p);
        \]
        \item[(b)] Lipschitzness:
        \[
        \norm{\objrand(p;z)}_\infty \leq \frac{s K^2}{\gamma m^2};
        \]
        \item[(c)] Smoothness:
        \[
        \norm{\nabla \objrand(p;z) - \nabla \objrand(q;z)}_\infty \leq \frac{sK^3}{\gamma^2 m^3}\norm{p-q}_1.
        \]
    \end{itemize}
    To prove (a), observe that for all $\pi \in \Pi$:
    \begin{align*}
        \E \brk[s]*{\brk*{\nabla \objrand(p;z)}_\pi} 
        &= 
        \E_{(x,\rew) \sim \calD} \E_{\pred \sim \calA} \brk[s]*{- (1-\gamma) \frac{K}{m} \sum_{\lab \in \pred} \frac{\ind{\lab \in \pi(x)} \rew(\lab)}{\yprobg{p}{x}{y}}} \\
        &=
        \E_{(x,\rew) \sim \calD} \E_{\pred \sim \calA} \brk[s]*{- (1-\gamma) \frac{K}{m} \sum_{\lab \in \calY} \frac{\ind{\lab \in \pi(x)} \ind{\lab \in \pred} \rew(\lab)}{\yprobg{p}{x}{y}}} \\
        &=
        \E_{(x,\rew) \sim \calD} \brk[s]*{- (1-\gamma) \frac{K}{m} \sum_{\lab \in \calY} \frac{\ind{\lab \in \pi(x)} \Pr[\lab \in \pred_i] \rew(\lab)}{\yprobg{p}{x}{y}}} \\
        &=
        \E_{(x,\rew) \sim \calD} \brk[s]*{- (1-\gamma) \sum_{\lab \in \calY} \frac{\ind{\lab \in \pi(x)} \rew(\lab)}{\yprobg{p}{x}{y}}} \\
        &=
        \nabla \obj(p),
    \end{align*}
    where we have used the fact that the probability that a given action $\lab$ belongs to a random subset $\pred \sim \calA$ is $m / K$. To prove (b), we have
    \begin{align*}
        \norm{\nabla \objrand(p;z)}_\infty 
        &\leq 
        (1-\gamma) \frac{K}{m} \sum_{\lab \in \pred} \frac{\rew(\lab)}{\yprobg{p}{x}{\lab}} \\
        &\leq
        \frac{K^2}{\gamma m^2} \sum_{\lab \in \calY} \rew(\lab) \\
        &\leq \frac{s K^2}{\gamma m^2}.
    \end{align*}
    Finally, for (c), we have for all $\pi \in \Pi$:
    \begin{align*}
        \left| \brk*{\nabla \objrand(p;z)}_\pi - \brk*{\nabla \objrand(q;z)}_\pi \right| 
        &\leq
        (1-\gamma) \frac{K}{m} \sum_{\lab \in \pred} \rew(\lab) \left| \frac{1}{\yprobg{p}{x}{\lab}} - \frac{1}{\yprobg{q}{x}{\lab}} \right| \\
        &=
        (1-\gamma) \frac{K}{m} \sum_{\lab \in \pred} \rew(\lab) \left| \frac{\yprobg{p}{x}{\lab} - \yprobg{p}{x}{\lab}}{\yprobg{p}{x}{\lab}\yprobg{q}{x}{\lab}} \right| \\
        &\leq
        \frac{sK^3}{\gamma^2 m^3} \norm{p-q}_1.
    \end{align*}
\end{proof}

The following lemma asserts that each call to the linear optimization oracle in \cref{alg:fw-main} can in fact be implemented by a call to the ERM oracle $\oracle_\Pi$.

\begin{lemma}
\label{lem:fw-loo-erm}
    In \cref{alg:fw-main} with the random objectives defined in \cref{eqn:obj-rand}, each call to $\loo_\Pi$ can be implemented via a single call to $\oracle_\Pi$.
\end{lemma}

\begin{proof}
    For $t = 2^k - 1$, we have
    \begin{align*}
        \loo_\Pi(v_t) &= \argmax_{p \in \Delta_\Pi} \brk[c]*{\frac{1}{b_t} \sum_{i=1}^{b_t} \nabla \objrand(p_t;z_i)^\top p} \\
        &=
        \argmax_{\pi \in \Pi} \brk[c]*{\frac{1}{b_t} \sum_{i=1}^{b_t} \brk*{\nabla \objrand(p_t;z_i)}_\pi} \\
        &=
        \argmax_{\pi \in \Pi} \brk[c]*{\sum_{i=1}^{b_t} \sum_{\lab \in \pi(x_i)} \hat \rew_i(\lab)},
    \end{align*}
    where $\hat \rew_i(\lab) = - \frac{1}{b_t} (1-\gamma) \frac{K}{m}\frac{\ind{\lab \in \pred_i} \rew_i(\lab)}{\yprobg{p_t}{x_i}{\lab}}$ by \cref{eqn:grad-obj-rand}. For $2^k \leq t < 2^{k+1} - 1$, assuming inductively that $(v_{t-1})_\pi = \sum_{i} \sum_{\lab \in \pi(x_i)} \tilde \rew_i(\lab)$, then
    \begin{align*}
        \loo_\Pi(v_t) 
        = 
        \argmax_{\pi \in \Pi} \brk[c]*{\sum_i \sum_{\lab \in \pi(x_i)} \brk*{\tilde \rew_i(\lab) + \hat \rew_i(\lab)}},
    \end{align*}
    where $\hat \rew_i(\lab) = - \frac{1}{b_t} (1-\gamma) \frac{K}{m}\ind{\lab \in \pred_i} \rew_i(\lab) \brk*{\frac{1}{\yprobg{p_t}{x_i}{\lab}} - \frac{1}{\yprobg{p_{t-1}}{x_i}{\lab}}}$.
\end{proof}


\section[Proofs for Section 4]{Proofs for \cref{sec:regret}}
\label{sec:regret-proofs}

\cref{thm:exp4-comb-regret} primarily stems from the following the following result which is a consequence of a generic analysis of FTRL (see e.g. \cite{hazan2016introduction}, \cite{orabona2019modern} (Lemma 7.14)):

\begin{lemma}
\label{lem:exp4-generic}
    For all $p^\star \in \Delta_\Pi$, the following regret bound holds for \cref{alg:exp4-comb}:
    \begin{align*}
        \sum_{t=1}^T \hat c_t \cdot \brk*{p_t - p^\star} \leq R(p^\star) - R(p_1) + \frac{\eta}{2} \sum_{t=1}^T \sum_{i=1}^{|\Pi|} \tilde p_t(i) \hat c_t(i)^2,
    \end{align*}
    where $R(p) \eqdef \frac{1}{\eta} H(p) + \frac{1}{\nu} \Phi(p)$, and $\tilde p_t \in [p_t, p_{t+1}]$ is some point which lies on the line segment connecting $p_t$ and $p_{t+1}$.
\end{lemma}

In order to relate $\tilde p_t$ given in the bound to $p_t$, we use the following result which is where we make use of the properties of the log-barrier regularization $\Phi(\cdot)$.

\begin{lemma}
\label{lem:log-bar-stability}
    Assuming that $\nu \leq \frac{1}{16}$, it holds that $p_{t+1}(i) \leq \frac{1}{8 \nu} p_t(i)$ for all $i \in [|\Pi|]$.
\end{lemma}

\cite{erez2024real} prove this claim for the single-label setting, for completeness we include the proof for the multilabel setting. The proof requires the following preliminary definition:

\paragraph{Local and dual norms.} Given a strictly convex twice-differentiable function $F : \calW \to \R$ where $\calW \subseteq \R^d$ is a convex domain, we define the \emph{local norm} of a vector $g \in \calW$ about a point $z \in \calW$ with respect to $F$ by 
\begin{align*}
    \norm{g}_{F,z} \eqdef \sqrt{g^\top \nabla^2 F(z) g},
\end{align*}
and its \emph{dual norm} by
\begin{align*}
    \norm{g}^*_{F,z} \eqdef \sqrt{g^\top \nabla^2 F(z)^{-1} g}.
\end{align*}
% The proof also requires the following technical result from convex optimization:

% \begin{lemma}
%     \label{lem:ellipsoid}
%     Let $F : \calW \to \R$ be a strictly convex function over a convex domain $\calW \subseteq \R^d$ and let $\calE \subseteq \R^d$ be an ellipsoid centered around $w_c \in \calW$. Assume that $F(w) \geq F(w_c)$ for all $w \in \calW \cap \sigma(\calE)$ where $\sigma(\calE)$ is the boundary of $\calE$ and let $w^\star = \argmin_{w \in \calW} F(w)$. Then $w^\star \in \calE$.
% \end{lemma}

% \begin{proof}
%     \liad{TODO}
% \end{proof}

\begin{proof}[Proof of \cref{lem:log-bar-stability}]
    Fix $t \in [T]$ and define $F_t : \Delta_\Pi \to \R$ by
    \begin{align*}
        F_t(p) \eqdef \sum_{\tau=1}^{t-1} \hat c_\tau \cdot p + R(p),
    \end{align*}
    where $R(p)\eqdef H_\eta(p) + \Phi_\nu(p) \eqdef \frac{1}{\eta} H(p) + \frac{1}{\nu} \Phi(p) $. That is, $F_t$ is the function minimized by $p_t$ at round $t$ of \cref{alg:exp4-comb}. Now, by the form of the Hessian of $\Phi(\cdot)$, for all $p,p',q \in \Delta_\Pi$ it holds that
    \begin{align*}
        \norm{p - p'}^2_{\Phi_\nu,q} = \frac{1}{\nu} \sum_{i=1}^{|\Pi|} \frac{\brk*{p(i) - p'(i)}^2}{q(i)^2},
    \end{align*}
    and thus it suffices to prove that $\norm{p_{t+1} - p_t}^2_{\Phi_\nu,p_t} \leq \frac{c^2}{\nu}$ where $c \eqdef \frac{1}{8\nu} - 1 \geq 1$, since in this case for all $i \in [|\Pi|]$ we will have
    \begin{align*}
        \brk*{p_{t+1}(i) - p_t(i)}^2 \leq \brk*{\frac{1}{8 \nu} - 1}^2 p_t(i)^2,
    \end{align*}
    implying the result. 
    % Using \cref{lem:ellipsoid} with $\calW = \Delta_\Pi$, $\calE = \brk[c]*{p \in \Delta_\Pi \mid \norm{p-p_t}_{\Phi,p_t} \leq \frac{c^2}{\nu}}$ and $F = F_{t+1}$ (whose minimizer is $p_{t+1}$), it suffices to prove that for all $q \in \Delta_\Pi$ satisfying $\norm{q - p_t}^2_{\Phi,p_t} = \frac{c^2}{\nu}$ it holds that $F_{t+1}(q) \geq F_{t+1}(p_t)$.
    Note that a sufficient condition for the above is that for all $q \in \Delta_\Pi$ for which $\norm{q-p_t}_{\Phi_\nu,p_t} = \frac{c^2}{\nu}$ it holds that $F_{t+1}(q) \geq F_{t+1}(p_t)$. Indeed, in this case, if we define the convex set $\calE \eqdef \brk[c]*{p \in \Delta_\Pi \mid \norm{p-p_t}_{\Phi_\nu,p_t} \leq \frac{c^2}{\nu}}$ (which in particular contains $p_t$), since $F_{t+1}$ is strictly convex, the condition that $F_{t+1}(q) \geq F_{t+1}(p_t)$ for all $q$ on the relative boundary of $\calE$ implies that its minimizer, $p_{t+1}$, must belong to $\calE$.
    With that in mind, fix $q \in \Delta_\Pi$ with $\norm{q - p_t}^2_{\Phi_\nu,p_t} = \frac{c^2}{\nu}$. Using a second-order Taylor approximation of $F_{t+1}$ around $p_t$, we have
    \begin{align*}
        F_{t+1}(q) \geq F_{t+1}(p_t) + \nabla F_{t+1}(p_t)^\top \brk*{q-p_t} + \frac12 \norm{q-p_t}^2_{R,p},
    \end{align*}
    where $p$ is a point on the line segment connecting $p_t$ and $q$. Using the definition of $F_{t+1}$ and the fact that $\nabla^2 R(\cdot) \succeq \nabla^2 \Phi_\nu(\cdot)$ since $H_\eta$ is convex, we get
    \begin{align*}
        F_{t+1}(q) \geq F_{t+1}(p_t) + \nabla F_t(p_t)^\top \brk*{q - p_t} + \hat c_t^\top \brk*{q-p_t} + \frac12 \norm{q-p_t}^2_{\Phi_\nu,p},
    \end{align*}
    and using first-order convex optimality conditions for $p_t$ as a minimizer of $F_t$, we obtain
    \begin{align*}
        F_{t+1}(q) \geq F_{t+1}(p_t) + \hat c_t^\top \brk*{q-p_t} + \frac12 \norm{q-p_t}^2_{\Phi_\nu,p}.
    \end{align*}
    Starting with the local norm term, we note that since $\norm{q-p_t}^2_{\Phi_\nu,p_t} \leq \frac{c^2}{\nu}$ it holds that $q(i) \leq \frac{1}{8\nu} p_t(i)$ for all $i \in [|\Pi|]$, and since $p$ lies on the segment connecting $q$ and $p_t$, the same holds for $p$ instead of $q$. Therefore,
    \begin{align*}
        \norm{q-p_t}^2_{\Phi_\nu,p}
        &=
        \frac{1}{\nu} \sum_{i=1}^{|\Pi|} \frac{\brk*{q(i)-p_t(i)}^2}{p(i)^2} \\
        &\geq
        64 \nu \sum_{i=1}^{|\Pi|} \frac{\brk*{q(i)-p_t(i)}^2}{p_t(i)^2} \\
        &=
        64 \nu^2 \norm{q-p_t}^2_{\Phi_\nu,p_t} \\
        &=
        64 \nu c^2.
    \end{align*}
    Thus, to conclude the proof, we need to show that $\hat c_t^\top (q-p_t) \geq - 32 \nu c^2$. Indeed, since the losses are non-positive, we have
    \begin{align*}
        \hat c_t^\top (q-p_t)
        &= \frac{\ell_t(\pred_t)}{Q_t(\pred_t)} \sum_{i=1}^{|\Pi|} \brk*{q(i)-p_t(i)} \ind{\pi_i(x_t)=\pred_t} \\
        &\geq
        \frac{\ell_t(\pred_t)}{Q_t(\pred_t)} \sum_{i=1}^{|\Pi|} q(i) \ind{\pi_i(x_t)=\pred_t}.
    \end{align*}
    Using the fact that $q(i) \leq \frac{1}{8\nu}p_t(i)$ we further lower bound this term as
    \begin{align*}
        \hat c_t^\top (q-p_t)
        &\geq
        \frac{1}{8\nu} \frac{\ell_t(\pred_t)}{Q_t(\pred_t)} \sum_{i=1}^{|\Pi|} p_t(i) \ind{\pi_i(x_t)=\pred_t} \\
        &=
        \frac{1}{8\nu} \ell_t(\pred_t) \\
        &\geq - \frac{1}{8\nu},
    \end{align*}
    where we used the fact that $\ell_t(\cdot) \in [-1,0]$. The proof is concluded once we note that $32 \nu c^2 \geq \frac{1}{8\nu}$ if and only if $c^2 \geq 256 \nu^2$, which clearly holds since $256 \nu^2 \leq 1 \leq c^2$.
\end{proof}
With \cref{lem:exp4-generic} and \cref{lem:log-bar-stability} in hand, we can prove the following result:

\begin{theorem}
\label{thm:exp4-prelim}
    \cref{alg:exp4-comb} with $\nu \leq \frac{1}{16}$ and $\eta > 0$ attains the following expected regret bound:
    \begin{align*}
        \E[\regret_T] \leq 1 + \frac{|\Pi| \log (|\Pi|T)}{\nu} + \frac{\log |\Pi|}{\eta} + \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i)\hat c_t(i)^2},
    \end{align*}
\end{theorem}

\begin{proof}[Proof of \cref{thm:exp4-prelim}]
    Fix $p^\star \in \Delta_\Pi$ and for $\gamma = \frac{1}{|\Pi|T}$ let $p^\star_\gamma(i) \eqdef (1-|\Pi|\gamma)p^\star(i) + \gamma$ for all $i \in [|\Pi|]$. In addition, let $c_t \in [-1,0]^{|\Pi|}$ defined by $c_t(i) = \ell_t \brk*{\pi_i(x_t)}$. We have,
    \begin{align*}
        \sum_{t=1}^T c_t \cdot \brk*{p_t - p^\star}
        &=
        \sum_{t=1}^T c_t \cdot \brk*{p_t - p^\star_\gamma} + \sum_{t=1}^T c_t \cdot \brk*{p^\star_\gamma - p^\star} \\
        &=
        \sum_{t=1}^T c_t \cdot \brk*{p_t - p^\star_\gamma} + \sum_{t=1}^T \sum_{i=1}^{|\Pi|} c_t(i) \brk*{\gamma - |\Pi|\gamma p^\star(i)} \\
        &\leq
        \sum_{t=1}^T c_t \cdot \brk*{p_t - p^\star_\gamma} + \gamma |\Pi|T \\
        &=
        \sum_{t=1}^T c_t \cdot \brk*{p_t - p^\star_\gamma} + 1
    \end{align*}
    where we have used the fact that $c_t(i) \in [-1,0]$. Taking expectations and using \cref{lem:exp4-generic} and \cref{lem:log-bar-stability} together with the fact that $\E_t[\hat c_t] = c_t$, we obtain
    \begin{align*}
        \E[\regret_T]
        &\leq
        1 + \E \brk[s]*{\sum_{t=1}^T \E_t[\hat c_t] \cdot \brk*{p_t - p^\star_\gamma}} \\
        &=
        1 + \E \brk[s]*{\sum_{t=1}^T \hat c_t \cdot \brk*{p_t - p^\star_\gamma}} \\
        &\leq 
        R(p^\star_\gamma) - R(p_1) + \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i) \hat c_t(i)^2 } \\
        &\leq
        \frac{1}{\nu}\Phi(p^\star_\gamma) - \frac{1}{\eta}H(p_1) + \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i) \hat c_t(i)^2 } \\
        &\leq
        1 + \frac{|\Pi| \log (|\Pi|T)}{\nu} + \frac{\log |\Pi|}{\eta} + \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i)\hat c_t(i)^2},
    \end{align*}
    as claimed.
\end{proof}

\begin{proof}[Proof of \cref{thm:exp4-comb-regret}]
    Using \cref{thm:exp4-prelim} and considering the stability term, for all $t \in [T]$ and $i \in [|\Pi|]$ it holds that
    \begin{align*}\allowdisplaybreaks
        \E_t \brk[s]*{\hat c_t(i)^2} 
        &=
        \E_t \brk[s]*{\brk*{\sum_{\lab \in \pi_i(x_t)} \frac{\ell_t(\lab) \mathbf{1} \brk[c]*{\lab \in \pred_t}}{Q_t(\lab)}}^2} \\
        &=
        m^2 \E_t \brk[s]*{\brk*{\frac{1}{m} \sum_{\lab \in \pi_i(x_t)} \frac{\ell_t(\lab) \mathbf{1} \brk[c]*{\lab \in \pred_t}}{Q_t(\lab)}}^2}  \\
        &\leq
        m \E_t \brk[s]*{\sum_{\lab \in \pi_i(x_t)} \brk*{\frac{\ell_t(\lab) \mathbf{1} \brk[c]*{\lab \in \pred_t}}{Q_t(\lab)}}^2} \\
        &=
        m \sum_{\lab \in \pi_i(x_t)} \frac{\ell_t(\lab)^2}{Q_t(\lab)},
    \end{align*}
    where the third line uses Jensen's inequality, and the last equality uses the fact that $\E_t[\ind{\lab \in \pred_t}] = Q_t(\lab)$. Thus, the stability term is bounded by
    \begin{align*}
        \eta \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|}  p_t(i) \E_t \brk[s]*{ \hat c_t(i)^2 }} &\leq
        \eta m \E \brk[s]*{\sum_{t=1}^T \sum_{i=1}^{|\Pi|} p_t(i) \sum_{\lab \in \pi_i(x_t)} \frac{\ell_t(\lab)^2}{Q_t(\lab)}} \\
        &=
        \eta m \E \brk[s]*{\sum_{t=1}^T \sum_{\lab=1}^K \underbrace{\sum_{i=1}^{|\Pi|} p_t(i) \ind{\lab \in \pi_i(x_t)}}_{Q_t(\lab)} \frac{\ell_t(\lab)^2}{Q_t(\lab)}} \\
        &=
        \eta m \E \brk[s]*{\sum_{t=1}^T \norm{\ell_t}_2^2}
        \leq \eta s m T,
    \end{align*}
    and plugging in the specified values for $\eta$ and $\nu$ gives the desired bound.
\end{proof}

% \section{Full-Bandit Feedback}
% \label{sec:full-bandit}

% Fix a distribution $p \in \Delta_\Pi$ and let $p^\gamma$ denote the mixture of $p$ with a uniform distribution over $\calA$ with mixing factor $\gamma$, that is, with probability $1-\gamma$ we predict using $p$ and with probability $\gamma$ we sample $a \sim \calA$ uniformly. For each policy $\pi \in \Pi$ we define the importance-weighted reward estimator induced by $p^\gamma$ as
% \begin{align*}
%     R_i(\pi) \eqdef \pi(x_i)^\top C_i^{-1} \pred_i \pred_i^\top \rew_i, \quad \forall \pi \in \Pi,
% \end{align*}
% where $\pred_i \sim p^\gamma$ and $C_i \eqdef \E_{\pred_i \sim p} \brk[s]*{\pred_i \pred_i^\top} = (1-\gamma)\sum_\pi p(\pi) \pi(x_i) \pi(x_i)^\top + \gamma \binom{K}{m}^{-1} \sum_{\pred \in \calA} \pred \pred^\top$ is the covariance matrix associated with the sample. It is straightforward to see that this is an unbiased estimator of $\rew_\calD(\pi)$, so we attempt to bound its variance by a quantity which would correspond to the gradient of some convex potential. Examining the variance of this estimators, we have
% \begin{align*}
%     \mathrm{Var} \brk[s]*{R_i(\pi)}
%     \leq 
%     \E \brk[s]*{R_i(\pi)^2}
%     =
%     \E_{\pred_i \sim p} \brk[s]*{\rew_i^\top \pred_i \pred_i^\top C_i^{-1} \pi(x_i) \pi(x_i)^\top C_i^{-1} \pred_i \pred_i^\top \rew_i}.
% \end{align*}
% If we now upper bound the realized reward by the sparsity $s$, we can further bound the variance by
% \begin{align*}
%     \mathrm{Var} \brk[s]*{R_i(\pi)}
%     \leq s^2 \cdot \pi(x_i)^\top C_i^{-1} \E_{\pred_i \sim p} \brk[s]*{\pred_i \pred_i^\top} C_i^{-1} \pi(x_i) = s^2 \cdot \pi(x_i)^\top C_i^{-1} \pi(x_i),
% \end{align*}
% with the latter quantity being proportional to the gradient of the following log-determinant potential (see e.g. \cite{boyd2004convex}),
% \begin{align*}
%     \objrand(p;x_i) \eqdef - \log \det \brk*{C(p; x_i)} = - \log \det \brk*{\sum_{k=1}^{|\Pi|} p(k) \pi_k(x_i) \pi_k(x_i)^\top},
% \end{align*}
% which would seem like a natural generalization of the log-barrier potential used in \cref{eq:phase-1-objective}. When bounding the variance, however, we used a uniform upper bound of $s$ on the realized reward, which ultimately removed the entire dependence on the reward function $r_i$, and if we attempt to follow a similar approach to the proof of \cref{lem:bounded-grad} we will obtain a direct $K$-dependence for the gradient. If we try to maintain the dependence on $r_i$ while bounding the variance, it is unclear how the resulting expression can be related to the gradient of some (perhaps more complex) convex potential. 
% % We emphasize that given an appropriate potential for which the gradient at an approximate minimizer is upper bounded by a quantity independent of $K$ (as with \cref{lem:log-self-concordance}), we can employ a similar technique to that used in the first phase of \cref{alg:pac-comband} with full-bandit gradient estimators in order to efficiently find an exploration distribution which would allow for uniform reward estimation using only $\approx  \mathrm{poly}(s,m) / \eps^2$ samples, thus resulting in a similar sample complexity bound for the full-bandit setting.
% We emphasize that given an appropriate potential for which the gradient at an approximate minimizer is upper bounded by a quantity independent of $K$ (as with \cref{lem:log-self-concordance}), we can employ a similar technique to that used in the first phase of \cref{alg:pac-comband} with full-bandit gradient estimators in order to efficiently find an exploration distribution which would allow for uniform reward estimation using only $\approx  \mathrm{poly}(s,m) / \eps^2$ samples, thus resulting in an improved sample complexity bound for the full-bandit setting.

\end{document}
