@article{piet2023mark,
  title={Mark My Words: Analyzing and Evaluating Language Model Watermarks},
  author={Piet, Julien and Sitawarin, Chawin and Fang, Vivian and Mu, Norman and Wagner, David},
  journal={arXiv preprint arXiv:2312.00273},
  year={2023}
}

@article{Hoeffding1963,
  author = {Hoeffding, W.},
  year = {1963},
  title = {Probability Inequalities for Sums of Bounded Random Variables},
  journal = {Journal of the American Statistical Association},
  volume = {58},
  pages = {13--30},
  doi = {10.1080/01621459.1963.10500830},
  url = {https://doi.org/10.1080/01621459.1963.10500830}
}

@inproceedings{zhao2024provable,
title={Provable Robust Watermarking for {AI}-Generated Text},
author={Xuandong Zhao and Prabhanjan Vijendra Ananth and Lei Li and Yu-Xiang Wang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=SsmT8aO45L}
}

@InProceedings{kirchenbauer23a,
  title = 	 {A Watermark for Large Language Models},
  author =       {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {17061--17084},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/kirchenbauer23a.html},
  abstract = 	 {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.}
}
@article{kirchenbauer2023reliability,
  title={On the Reliability of Watermarks for Large Language Models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Shu, Manli and Saifullah, Khalid and Kong, Kezhi and Fernando, Kasun and Saha, Aniruddha and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2306.04634},
  year={2023}
}
@article{liu2024tuning,
  title={Tuning Language Models by Proxy},
  author={Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2401.08565},
  year={2024}
}


@article{kuditipudi2024robust,
title={Robust Distortion-free Watermarks for Language Models},
author={Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=FpaCL1MO2C},
note={}
}

@inproceedings{
wu2024dip,
title={A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models},
author={Yihan Wu and Zhengmian Hu and Junfeng Guo and Hongyang Zhang and Heng Huang},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=c8qWiNiqRY}
}

@inproceedings{hu2024unbiased,
title={Unbiased Watermark for Large Language Models},
author={Zhengmian Hu and Lichang Chen and Xidong Wu and Yihan Wu and Hongyang Zhang and Heng Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uWVC5FVidc}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{zhang2024tinyllama,
  title={TinyLlama: An Open-Source Small Language Model},
  author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024}
}
@article{jawahar2020automatic,
  title={Automatic detection of machine generated text: A critical survey},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2011.01314},
  year={2020}
}
@inproceedings{topkara2005natural,
  title={Natural language watermarking},
  author={Topkara, Mercan and Taskiran, Cuneyt M and Delp III, Edward J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VII},
  volume={5681},
  pages={441--452},
  year={2005},
  organization={SPIE}
}
@inproceedings{topkara2006hiding,
  title={The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions},
  author={Topkara, Umut and Topkara, Mercan and Atallah, Mikhail J},
  booktitle={Proceedings of the 8th workshop on Multimedia and security},
  pages={164--174},
  year={2006}
}
@inproceedings{chiang2004natural,
  title={Natural language watermarking using semantic substitution for chinese text},
  author={Chiang, Yuei-Lin and Chang, Lu-Ping and Hsieh, Wen-Tai and Chen, Wen-Chih},
  booktitle={Digital Watermarking: Second International Workshop, IWDW 2003, Seoul, Korea, October 20-22, 2003. Revised Papers 2},
  pages={129--140},
  year={2004},
  organization={Springer}
}
@inproceedings{venugopal2011watermarking,
  title={Watermarking the outputs of structured prediction with an application in statistical machine translation.},
  author={Venugopal, Ashish and Uszkoreit, Jakob and Talbot, David and Och, Franz Josef and Ganitkevitch, Juri},
  booktitle={Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  pages={1363--1372},
  year={2011}
}
@inproceedings{yang2022tracing,
  title={Tracing text provenance via context-aware lexical substitution},
  author={Yang,Xi and Zhang, Jie and Chen, Kejiang and Zhang, Weiming and Ma, Zehua and Wang, Feng and Yu, Nenghai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={11613--11621},
  year={2022}
}
@article{rizzo2019fine,
  title={Fine-grain watermarking for intellectual property protection},
  author={Rizzo, Stefano Giovanni and Bertini, Flavio and Montesi, Danilo},
  journal={EURASIP Journal on Information Security},
  volume={2019},
  pages={1--20},
  year={2019},
  publisher={Springer}
}
@inproceedings{topkara2006words,
  title={Words are not enough: sentence level natural language watermarking},
  author={Topkara, Mercan and Topkara, Umut and Atallah, Mikhail J},
  booktitle={Proceedings of the 4th ACM international workshop on Contents protection and security},
  pages={37--46},
  year={2006}
}
@inproceedings{topkara2006natural,
  title={Natural language watermarking: Challenges in building a practical system},
  author={Topkara, Mercan and Riccardi, Giuseppe and Hakkani-T{\"u}r, Dilek and Atallah, Mikhail J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VIII},
  volume={6072},
  pages={106--117},
  year={2006},
  organization={SPIE}
}
@inproceedings{helfrich2012dual,
  title={Dual canonicalization: An answer to the homograph attack},
  author={Helfrich, James N and Neff, Rick},
  booktitle={2012 eCrime Researchers Summit},
  pages={1--10},
  year={2012},
  organization={IEEE}
}
@article{gabrilovich2002homograph,
  title={The homograph attack},
  author={Gabrilovich, Evgeniy and Gontmakher, Alex},
  journal={Communications of the ACM},
  volume={45},
  number={2},
  pages={128},
  year={2002},
  publisher={ACM New York, NY, USA}
}
@inproceedings{pajola2021fall,
  title={Fall of Giants: How popular text-based MLaaS fall against a simple evasion attack},
  author={Pajola, Luca and Conti, Mauro},
  booktitle={2021 IEEE European Symposium on Security and Privacy (EuroS\&P)},
  pages={198--211},
  year={2021},
  organization={IEEE}
}
@inproceedings{boucher2022bad,
  title={Bad characters: Imperceptible nlp attacks},
  author={Boucher, Nicholas and Shumailov, Ilia and Anderson, Ross and Papernot, Nicolas},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={1987--2004},
  year={2022},
  organization={IEEE}
}
@misc{goodside2023adversarial,
  author = {Goodside, Riley},
  title = {There are adversarial attacks for that proposal as well --- in particular, generating with emojis after words and then removing them before submitting defeats it},
  howpublished = {Twitter},
  month = jan,
  year = {2023},
  note = {URL: \url{https://twitter.com/goodside/status/1610682909647671306}},
}



@INPROCEEDINGS{Fridrich2000fragile,
  author={Fridrich, J. and Goljan, M. and Baldoza, A.C.},
  booktitle={Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)}, 
  title={New fragile authentication watermark for images}, 
  year={2000},
  volume={1},
  number={},
  pages={446-449 vol.1},
  keywords={Authentication;Watermarking;Security;Cryptography;Pixel;Intelligent systems;Protection;Communication channels;Cryptographic protocols;Public key},
  doi={10.1109/ICIP.2000.900991}}


@ARTICLE{Shehab2018Fragile,
  author={Shehab, Abdulaziz and Elhoseny, Mohamed and Muhammad, Khan and Sangaiah, Arun Kumar and Yang, Po and Huang, Haojun and Hou, Guolin},
  journal={IEEE Access}, 
  title={Secure and Robust Fragile Watermarking Scheme for Medical Images}, 
  year={2018},
  volume={6},
  number={},
  pages={10269-10278},
  keywords={Watermarking;Matrix decomposition;Authentication;Transforms;Biomedical imaging;Symmetric matrices;Medical image security;tamper localization;singular value decomposition;fragile watermarking;arnold transformation;image security;authentication},
  doi={10.1109/ACCESS.2018.2799240}}

@article{Lin2011integrity,
author = {Lin, Pei-Yu and Lee, Jung-San and Chang, Chin-Chen},
title = {Protecting the content integrity of digital imagery with fidelity preservation},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/2000486.2000489},
doi = {10.1145/2000486.2000489},
abstract = {Fragile watermarking is applied to protect the content integrity of digital images. The main concerns related to watermarking include retaining the quality of the watermarked image and retaining the ability to detect whether any manipulation has occurred. Because recent watermarking techniques seriously distort the quality of the protected image after embedding the authentication code into the image content, attention has been drawn to how to satisfy both the need for image fidelity and detection ability. To account for the influence from both essentials, a novel algorithm is proposed in this article. The new scheme utilizes a weighted-sum function to embed (n + 1) authentication bits into a block with 2n pixels by modifying only one original pixel with (±1). With fewer authentication codes, the new process can protect the content of the image. The experimental results demonstrate that the approach can guarantee the fidelity of the watermarked image while retaining tamper-proof functionality.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = {Sep},
articleno = {15},
numpages = {20},
keywords = {weighted-sum function, key-lock-pair system, integrity protection, fragile watermarking, Authentication}
}

@article{krishna2023paraphrasing,
  title={Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense},
  author={Krishna, Kalpesh and Song, Yixiao and Karpinska, Marzena and Wieting, John and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2303.13408},
  year={2023}
}
@article{ueoka2021frustratingly,
  title={Frustratingly easy edit-based linguistic steganography with a masked language model},
  author={Ueoka, Honai and Murawaki, Yugo and Kurohashi, Sadao},
  journal={arXiv preprint arXiv:2104.09833},
  year={2021}
}
@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{tan2020detecting,
  title={Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News},
  author={Tan, Reuben and Plummer, Bryan and Saenko, Kate},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2081--2106},
  year={2020}
}
@inproceedings{gambini2022pushing,
  title={On pushing DeepFake Tweet Detection capabilities to the limits},
  author={Gambini, Margherita and Fagni, Tiziano and Falchi, Fabrizio and Tesconi, Maurizio},
  booktitle={Proceedings of the 14th ACM Web Science Conference 2022},
  pages={154--163},
  year={2022}
}
@article{tay2022unifying,
  title={Unifying language learning paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}
@article{wolff2020attacking,
  title={Attacking neural text detectors},
  author={Wolff, Max and Wolff, Stuart},
  journal={arXiv preprint arXiv:2002.11768},
  year={2020}
}
@misc{tian2023gptzero,
  author = {Tian, E.},
  title = {GPTZero update v1},
  howpublished = {Substack post},
  month = jan,
  year = {2023},
  note = {URL: \url{https://gptzero.substack.com/p/gptzeroupdate-v1}},
}
@article{chen2024knowledge,
  title={Knowledge Distillation for Closed-Source Language Models},
  author={Chen, Hongzhan and Quan, Xiaojun and Chen, Hehong and Yan, Ming and Zhang, Ji},
  journal={arXiv preprint arXiv:2401.07013},
  year={2024}
}
@inproceedings{merity2017pointer,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Byj72udxe}
}

@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@misc{openai2022chatgpt,
  title={ChatGPT: Optimizing Language Models for Dialogue},
  author={OpenAI},
  year={2022},
  howpublished={\url{https://openai.com/blog/chatgpt/}},
}
@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv},
  volume={abs/2303.08774},
  year={2023},
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@inproceedings{bergman2022guiding,
  title={Guiding the release of safer E2E conversational AI through value sensitive design},
  author={Bergman, A Stevie and Abercrombie, Gavin and Spruit, Shannon and Hovy, Dirk and Dinan, Emily and Boureau, Y-Lan and Rieser, Verena and others},
  booktitle={Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  year={2022},
  organization={Association for Computational Linguistics}
}
@article{mirsky2023threat,
  title={The threat of offensive ai to organizations},
  author={Mirsky, Yisroel and Demontis, Ambra and Kotak, Jaidip and Shankar, Ram and Gelei, Deng and Yang, Liu and Zhang, Xiangyu and Pintor, Maura and Lee, Wenke and Elovici, Yuval and others},
  journal={Computers \& Security},
  volume={124},
  pages={103006},
  year={2023},
  publisher={Elsevier}
}
@article{liang2023gpt,
  title={GPT detectors are biased against non-native English writers},
  author={Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu, Eric and Zou, James},
  journal={arXiv preprint arXiv:2304.02819},
  year={2023}
}
@inproceedings{gehrmann2019gltr,
  title={GLTR: Statistical Detection and Visualization of Generated Text},
  author={Gehrmann, Sebastian and Strobelt, Hendrik and Rush, Alexander M},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={111--116},
  year={2019}
}

@inproceedings{lee2024wrote,
    title = "Who Wrote this Code? Watermarking for Code Generation",
    author = "Lee, Taehyun  and
      Hong, Seokhee  and
      Ahn, Jaewoo  and
      Hong, Ilgee  and
      Lee, Hwaran  and
      Yun, Sangdoo  and
      Shin, Jamin  and
      Kim, Gunhee",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.268",
    pages = "4890--4911",
    abstract = "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task{'}s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.",
}

@inproceedings{Mitchelldetectgpt,
author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
title = {DetectGPT: zero-shot machine-generated text detection using probability curvature},
year = {2023},
publisher = {JMLR.org},
abstract = {The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for Detect-GPT. See ericmitchell.ai/detectgpt for code, data, and other project information.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1038},
numpages = {13},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}
@inproceedings{hovy2016enemy,
  title={The enemy in your own camp: How well can we detect statistically-generated fake reviews--an adversarial study},
  author={Hovy, Dirk},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={351--356},
  year={2016}
}
@misc{openai2023aiclassifier,
  title={New AI Classifier for Indicating AI-Written Text},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/}},
}
@inproceedings{zhao_protecting_2023,
	location = {Honolulu, Hawaii, {USA}},
	title = {Protecting language generation models via invisible watermarking},
	volume = {202},
	series = {{ICML}'23},
	abstract = {Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable {API} access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property ({IP}) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose {GINSEW}, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that {GINSEW} can effectively identify instances of {IP} infringement with minimal impact on the generation quality of protected {APIs}. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision ({mAP}) in detecting suspects compared to previous methods against watermark removal attacks.},
	pages = {42187--42199},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{JMLR}.org},
	author = {Zhao, Xuandong and Wang, Yu-Xiang and Li, Lei},
	date = {2023-07-23},
}

@article{aaronson2022my,
  title={My AI safety lecture for UT Effective Altruism},
  author={Aaronson, Scott},
  journal={Shtetl-Optimized: The blog of Scott Aaronson. Retrieved on September},
  volume={11},
  pages={2023},
  year={2022}
}

@InProceedings{christ2023undetectable,
title =  {Undetectable Watermarks for Language Models},  author =       {Christ, Miranda and Gunn, Sam and Zamir, Or},  booktitle =  {Proceedings of Thirty Seventh Conference on Learning Theory},  pages =  {1125--1139},  year =  {2024},  editor =  {Agrawal, Shipra and Roth, Aaron},  volume =  {247},  series =  {Proceedings of Machine Learning Research},  month =  {30 Jun--03 Jul},  publisher =    {PMLR},  pdf =  {https://proceedings.mlr.press/v247/christ24a/christ24a.pdf},  url =  {https://proceedings.mlr.press/v247/christ24a.html},  abstract =  {Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.  Prior works have suggested methods of embedding watermarks in model outputs, by *noticeably* altering the output distribution. We ask: Is it possible to introduce a watermark without incurring *any detectable* change to the output distribution? To this end, we introduce a cryptographically-inspired notion of undetectable watermarks for language models.  That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.}}

@misc{aaronson2023watermarking,
  title={Simons Institute Talk on Watermarking of Large Language Models},
  author={Aaronson, Scott},
  year={2023},
  howpublished={\url{https://simons.berkeley.edu/talks/scott-aaronson-ut-austin-openai-2023-08-17}},
}
@article{huang2023towards,
  title={Towards optimal statistical watermarking},
  author={Huang, Baihe and Zhu, Banghua and Zhu, Hanlin and Lee, Jason D and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2312.07930},
  year={2023}
}
@article{pang2024attacking,
  title={Attacking LLM Watermarks by Exploiting Their Strengths},
  author={Pang, Qi and Hu, Shengyuan and Zheng, Wenting and Smith, Virginia},
  journal={arXiv preprint arXiv:2402.16187},
  year={2024}
}
@article{wang2024stumbling,
  title={Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks},
  author={Wang, Yichen and Feng, Shangbin and Hou, Abe Bohan and Pu, Xiao and Shen, Chao and Liu, Xiaoming and Tsvetkov, Yulia and He, Tianxing},
  journal={arXiv preprint arXiv:2402.11638},
  year={2024}
}
@misc{carlini2024stealing,
      title={Stealing Part of a Production Language Model}, 
      author={Nicholas Carlini and Daniel Paleka and Krishnamurthy Dj Dvijotham and Thomas Steinke and Jonathan Hayase and A. Feder Cooper and Katherine Lee and Matthew Jagielski and Milad Nasr and Arthur Conmy and Eric Wallace and David Rolnick and Florian Tramèr},
      year={2024},
      eprint={2403.06634},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@misc{randor2023anthropictokenizer,
author = {Javier Randor},
title = {Anthropic Tokenizer},
year = {2023},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/javirandor/anthropic-tokenizer}},
}
@misc{Goodside2023,
author = {Goodside, R.},
title = {There are adversarial attacks for that proposal as well --- in particular, generating with emojis after words and then removing them before submitting defeats it.},
month = {January},
year = {2023},
howpublished = {Twitter},
url = {https://twitter.com/goodside/status/1610682909647671306}
}
@misc{zhu2024duwak,
      title={Duwak: Dual Watermarks in Large Language Models}, 
      author={Chaoyi Zhu and Jeroen Galjaard and Pin-Yu Chen and Lydia Y. Chen},
      year={2024},
      eprint={2403.13000},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lu2024entropybased,
      title={An Entropy-based Text Watermarking Detection Method}, 
      author={Yijian Lu and Aiwei Liu and Dianzhi Yu and Jingjing Li and Irwin King},
      year={2024},
      eprint={2403.13485},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023watermarking,
      title={Watermarking LLMs with Weight Quantization}, 
      author={Linyang Li and Botian Jiang and Pengyu Wang and Ke Ren and Hang Yan and Xipeng Qiu},
      year={2023},
      eprint={2310.11237},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{jovanovic2024watermark,
  title={Watermark Stealing in Large Language Models},
  author={Jovanovi{\'c}, Nikola and Staab, Robin and Vechev, Martin},
  journal={arXiv preprint arXiv:2402.19361},
  year={2024}
}
@article{bridle1989training,
  title={Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters},
  author={Bridle, John},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{li2024statistical,
  title={A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules},
  author={Li, Xiang and Ruan, Feng and Wang, Huiyuan and Long, Qi and Su, Weijie J},
  journal={arXiv preprint arXiv:2404.01245},
  year={2024}
}

@inproceedings{zhangwatermarks2024,
  title={Watermarks in the Sand: Impossibility of Strong Watermarking for Language Models},
  author={Zhang, Hanlin and Edelman, Benjamin L and Francati, Danilo and Venturi, Daniele and Ateniese, Giuseppe and Barak, Boaz},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@article{kendall1948rank,
  title={Rank correlation methods.},
  author={Kendall, Maurice George},
  year={1948},
  publisher={Griffin}
}
@article{spearman1904proof,
  title={THE PROOF AND MEASUREMENT OF ASSOCIATION BETWEEN TWO THINGS.},
  author={SPEARMAN, C},
  journal={The American Journal of Psychology},
  volume={15},
  number={1},
  year={1904}
}
@article{kendall1938new,
  title={A new measure of rank correlation},
  author={Kendall, Maurice G},
  journal={Biometrika},
  volume={30},
  number={1/2},
  pages={81--93},
  year={1938},
  publisher={JSTOR}
}
@article{somers1962new,
  title={A New Asymmetric Measure of Association for Ordinal Variables},
  author={Somers, Robert H},
  journal={American Sociological Review},
  volume={27},
  number={6},
  pages={799},
  year={1962},
  publisher={SAGE Publications}
}
@inproceedings{venugopal_watermarking_2011,
	location = {{USA}},
	title = {Watermarking the outputs of structured prediction with an application in statistical machine translation},
	isbn = {978-1-937284-11-4},
	series = {{EMNLP} '11},
	abstract = {We propose a general method to watermark and probabilistically identify the structured outputs of machine learning algorithms. Our method is robust to local editing operations and provides well defined trade-offs between the ability to identify algorithm outputs and the quality of the watermarked output. Unlike previous work in the field, our approach does not rely on controlling the inputs to the algorithm and provides probabilistic guarantees on the ability to identify collections of results from one's own algorithm. We present an application in statistical machine translation, where machine translated output is watermarked at minimal loss in translation quality and detected with high recall.},
	pages = {1363--1372},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Venugopal, Ashish and Uszkoreit, Jakob and Talbot, David and Och, Franz J. and Ganitkevitch, Juri},
	urldate = {2024-08-06},
	date = {2011-07-27},
}
@article{liu_multilingual_2020,
	title = {Multilingual Denoising Pre-training for Neural Machine Translation},
	volume = {8},
	url = {https://aclanthology.org/2020.tacl-1.47},
	doi = {10.1162/tacl_a_00343},
	abstract = {This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation ({MT}) tasks. We present {mBART}—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the {BART} objective (Lewis et al., 2019). {mBART} is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding {mBART} initialization produces performance gains in all but the highest-resource settings, including up to 12 {BLEU} points for low resource {MT} and over 5 {BLEU} points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1},
	pages = {726--742},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
	editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
	urldate = {2024-08-06},
	date = {2020},
	note = {Place: Cambridge, {MA}
Publisher: {MIT} Press},
}
@inproceedings{hermann_teaching_2015,
	location = {Cambridge, {MA}, {USA}},
	title = {Teaching machines to read and comprehend},
	series = {{NIPS}'15},
	abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
	pages = {1693--1701},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
	publisher = {{MIT} Press},
	author = {Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	urldate = {2024-08-06},
	date = {2015-12-07},
}
@article{kamaruddin_review_2018,
	title = {A Review of Text Watermarking: Theory, Methods, and Applications},
	volume = {6},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8268096},
	doi = {10.1109/ACCESS.2018.2796585},
	shorttitle = {A Review of Text Watermarking},
	abstract = {During the recent years, the issue of preserving the integrity of digital text has become a focus of interest in the transmission of online content on the Internet. Watermarking has a useful tool in the protection of digital text content as it solves the problem of tampering, duplicating, unauthorized access, and security breaches. The rapid development currently observable in information transfer and access is the consequences of the widespread usage of the Internet. When it comes to the different types of digital data, text constitutes the most complex and challenging type to which the method of text watermarking can be applied. Text watermarking constitutes a highly complex task, most of all, since only limited research has been done in this field. In order to ensure the successful evaluation, analysis, and implementation, a comprehensive research needs to be performed. This paper studies the theory, methods, and applications of text watermarking, which includes the discussion on the definition, embedding and extracting processes, requirements, approaches, and language applications of the established text watermarking methods. This paper reviews in detail the new classification of text watermarking, which is through embedding process and its related issues of attacks and language applicability. Open research challenges and future directions are also investigated, with a focus on its information integrity, information availability, originality preservation, information confidentiality, protection of sensitive information, document transformation, cryptography application, and language flexibility.},
	pages = {8011--8028},
	journal= {{IEEE} Access},
	author = {Kamaruddin,Nurul Shamimi and Kamsin, Amirrudin and Por, Lip Yee and Rahman, Hameedur},
	year = {2018},
	note = {Conference Name: {IEEE} Access},
	keywords = {Cryptography, Data mining, Encoding, Information protection, information security, Internet, Media, text analysis, text watermarking, watermarking, Watermarking},
}
@inproceedings{yoo_advancing_2024,
	location = {Mexico City, Mexico},
	title = {Advancing Beyond Identification: Multi-bit Watermark for Large Language Models},
	url = {https://aclanthology.org/2024.naacl-long.224},
	doi = {10.18653/v1/2024.naacl-long.224},
	shorttitle = {Advancing Beyond Identification},
	abstract = {We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages ({\textbackslash}geq 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time.},
	eventtitle = {{NAACL}-{HLT} 2024},
	pages = {4031--4055},
	booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yoo, {KiYoon} and Ahn, Wonhyuk and Kwak, Nojun},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	year = {2024},
}
@inproceedings{pan_risk_2023,
	location = {Singapore},
	title = {On the Risk of Misinformation Pollution with Large Language Models},
	url = {https://aclanthology.org/2023.findings-emnlp.97},
	doi = {10.18653/v1/2023.findings-emnlp.97},
	abstract = {We investigate the potential misuse of modern Large Language Models ({LLMs}) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering ({ODQA}) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which {LLMs} can be utilized to produce misinformation. Our study reveals that {LLMs} can act as effective misinformation generators, leading to a significant degradation (up to 87\%) in the performance of {ODQA} systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by {LLM}-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing {LLMs} as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.},
	eventtitle = {Findings 2023},
	pages = {1389--1403},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, willian},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	date = {2023-12},
    year={2023}
}

@InProceedings{pmlr-v235-chakraborty24a,
  title = 	 {Position: On the Possibilities of {AI}-Generated Text Detection},
  author =       {Chakraborty, Souradip and Bedi, Amrit and Zhu, Sicheng and An, Bang and Manocha, Dinesh and Huang, Furong},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {6093--6115},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/chakraborty24a/chakraborty24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/chakraborty24a.html},
  abstract = 	 {Our study addresses the challenge of distinguishing human-written text from Large Language Model (LLM) outputs. We provide evidence that this differentiation is consistently feasible, except when human and machine text distributions are indistinguishable across their entire support. Employing information theory, we show that while detecting machine-generated text becomes harder as it nears human quality, it remains possible with adequate text data. We introduce guidelines on the required text data quantity, either through sample size or sequence length, for reliable AI text detection, through derivations of sample complexity bounds. This research paves the way for advanced detection methods. Our comprehensive empirical tests, conducted across various datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) and with several state-of-the-art text generators (GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, Llama-2-70B-Chat-HF), assess the viability of enhanced detection methods against detectors like RoBERTa-Large/Base-Detector and GPTZero, with increasing sample sizes and sequence lengths. Our findings align with OpenAI’s empirical data related to sequence length, marking the first theoretical substantiation for these observations.}
}
@inproceedings{jin2019pubmedqa,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2567--2577},
  year={2019}
}
@misc{gao2024surveyfragilemodelwatermarking,
      title={A Survey of Fragile Model Watermarking}, 
      author={Zhenzhe Gao and Yu Cheng and Zhaoxia Yin},
      year={2024},
      eprint={2406.04809},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.04809}, 
}
@article{Imagefragilewatermarking,
author = {Wang, Yilong and Li, Zhenyu and Gong, Daofu and Lu, Haoyu and Liu, Fenlin},
title = {Image fragile watermarking algorithm based on deneighbourhood mapping},
journal = {IET Image Processing},
volume = {16},
number = {10},
pages = {2652-2664},
doi = {https://doi.org/10.1049/ipr2.12515},
url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12515},
eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12515},
abstract = {Abstract To address the security risk caused by fixed offset mapping and the limited recoverability of random mapping used in image watermarking, a self-embedding fragile image watermarking algorithm based on deneighbourhood mapping are proposed. First, the image is divided into several 2 × 2 blocks, and authentication watermark and recovery watermark are generated based on the average value of the image blocks. Then, the denighbourhood mapping is implemented as, for each image block, its mapping block is randomly selected outside its neighbourhood. Finally, the authentication watermark and the recovery watermark are embedded into the image block itself and its mapping block. Theoretical analysis indicates that in the case of continuous area tampering, the proposed watermarking algorithm can achieve a better recovery rate than that of the method based on the random mapping. The experimental results verify the rationality and effectiveness of the theoretical analysis. Moreover, compared with the existing embedding algorithms based on random mapping, chaos mapping, and Arnold mapping, in the case of continuous area tampering, the proposed algorithm also achieves a higher average recovery rate.},
year = {2022}
}
@article{pang2024freelunchllmwatermarking,
      title={No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices}, 
      author={Qi, Pang and Shengyuan, Hu and Wenting, Zheng and Virginia, Smith},
      journal={arXiv preprint arXiv:2402.16187},
      year={2024}  
}
@article{kim2024llmsonlineemergingthreat,
      title={When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs}, 
      author={Hanna Kim and Minkyoo Song and Seung Ho Na and Seungwon Shin and Kimin Lee},
      year={2024},
      journal={arXiv preprint arXiv:2410.14569},
}
@article{gloaguen2024discoveringcluesspoofedlm,
      title={Discovering Clues of Spoofed LM Watermarks}, 
      author={Thibaud Gloaguen and Nikola Jovanović and Robin Staab and Martin Vechev},
      year={2024},
      journal={arXiv preprint arXiv:2410.02693}, 
}
@misc{singh2023newevaluationmetricscapture,
      title={New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking}, 
      author={Karanpartap Singh and James Zou},
      year={2023},
      eprint={2312.02382},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.02382}, 
}
@article{Wu2023ASO,
  title={A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions},
  author={Junchao Wu and Shu Yang and Runzhe Zhan and Yulin Yuan and Derek F. Wong and Lidia S. Chao},
  journal={arXiv preprint arXiv:2310.14724},
  year={2023},
}
@article{Kuditipudi2023RobustDW,
  title={Robust Distortion-free Watermarks for Language Models},
  author={Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang},
  journal={Trans. Mach. Learn. Res.},
  year={2023},
  volume={2024},
  url={https://api.semanticscholar.org/CorpusID:260315804}
}
@InProceedings{pmlr-v247-christ24a,
  title = 	 {Undetectable Watermarks for Language Models},
  author =       {Christ, Miranda and Gunn, Sam and Zamir, Or},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {1125--1139},
  year = 	 {2024},
  editor = 	 {Agrawal, Shipra and Roth, Aaron},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/christ24a/christ24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/christ24a.html},
  abstract = 	 {Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text.  Prior works have suggested methods of embedding watermarks in model outputs, by *noticeably* altering the output distribution. We ask: Is it possible to introduce a watermark without incurring *any detectable* change to the output distribution? To this end, we introduce a cryptographically-inspired notion of undetectable watermarks for language models.  That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.}
}