@inproceedings{hu2024unbiased,
title={Unbiased Watermark for Large Language Models},
author={Zhengmian Hu and Lichang Chen and Xidong Wu and Yihan Wu and Hongyang Zhang and Heng Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uWVC5FVidc}
}

@InProceedings{kirchenbauer23a,
  title = 	 {A Watermark for Large Language Models},
  author =       {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {17061--17084},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/kirchenbauer23a.html},
  abstract = 	 {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.}
}

@inproceedings{lee2024wrote,
    title = "Who Wrote this Code? Watermarking for Code Generation",
    author = "Lee, Taehyun  and
      Hong, Seokhee  and
      Ahn, Jaewoo  and
      Hong, Ilgee  and
      Lee, Hwaran  and
      Yun, Sangdoo  and
      Shin, Jamin  and
      Kim, Gunhee",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.268",
    pages = "4890--4911",
    abstract = "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task{'}s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.",
}

@misc{singh2023newevaluationmetricscapture,
      title={New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking}, 
      author={Karanpartap Singh and James Zou},
      year={2023},
      eprint={2312.02382},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.02382}, 
}

@inproceedings{zhao2024provable,
title={Provable Robust Watermarking for {AI}-Generated Text},
author={Xuandong Zhao and Prabhanjan Vijendra Ananth and Lei Li and Yu-Xiang Wang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=SsmT8aO45L}
}

