\section{Experiments}\label{section:experiments}
This section applies the proposed CSG approaches to four contextual 2SP settings. We relegate details regarding the computing setup and software used to Appendix \ref{appendix:experimental_tooling}. 

\subsection{Newsvendor}

\textbf{Problem Setting:} The classic newsvendor problem is used to illustrate the proposed approach. Each day, a newsvendor purchases $y$ newspapers at unit-cost $c$. The vendor is budget-constrained, so they can only purchase at most $u$ papers. After purchasing, the vendor sells as many papers as possible for a unit price of $q$. When the day is done, the vendor can return the papers at a salvage price $r < c$. Suppose the vendor observes contextual information $\bs{x}$ before purchasing the paper. For example, $\bs{x}$ could represent the day of the week, the weather, and sales from previous days. The demand for the papers $\omega$ is unknown at the time of purchase. Thus, the newsvendor wishes to solve
    \begin{equation} \label{eq:newvendor}
    \min_{y} cy + \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}} \left[ Q(y, \omega)\right] \quad  \textit{s.t.} \quad  y \in [0, u],  \\
    \end{equation}
where $
        Q(y, \omega) = \min_{z \geq 0, w \geq 0} \ - q z - r w  \quad   \textit{s.t.} \quad z \leq \omega, \  z + w \leq y$. The decision variables $z$ and $w$ represent the quantities of papers sold and salvaged, respectively. We consider the following problem parameter setting $(c, q, r, u) = (1.0, 1.05, 0.1, 60.0)$. 

\vspace{\littlespace}
\noindent \textbf{Experimental Setup:}  A synthetic environment is used for evaluation purposes. $\bs{x}$ is set to be distributed according to a two-dimensional normal distribution. A randomly initialized neural network $\bs{f}_{\text{Random}} : \mathcal{X} \rightarrow \mathbb{R}_+^{200}$ is used to map $\bs{x}$ to an empirical demand distribution $\mathbb{P}_{\omega | \bs{x}}$ supported on $200$ points. The resulting joint distribution has an expectation of $15.1$ and a standard deviation of $0.25$ units.

The decision maker has a dataset of $n = 500$ samples of $\bs{x}$ and $\omega$. The \ref{eq:DCSG}, \ref{eq:static_PCSG}, and Dynamic-PCSG approaches are trained on this sample. In figures and tables, we label these approaches as $\textit{MMD}$, $\textit{Static}$ and $\textit{Dynamic}$. All approaches let $\bs{f}_{\phi}$ be parameterized as a fully connected feedforward neural network with ReLu activation, mapping $\bs{x}$ to $\mathbb{R}_{+}^K$. We select the architecture by considering the \ref{eq:DCSG} problem. A random search over a single training sample determines hyperparameters such as the number of hidden layers, number of units, and optimizer parameters. It optimizes $\mathcal{\hat{L}}_{\text{MMD}}$ on a fixed hold out of $20\%$ of the training samples. The loss network $E_{\psi}$'s architecture is selected similarly. First, $\bs{\hat{f}}_{\text{MMD}}$, is used to generate the dataset $\{\zeta_{1...K}^{(i)}, \omega^{(i)}, \ell^{(i)}_{\text{opt}} \}_{i=1}^n$, then a similar hold out procedure is used to select the architecture. The loss network $E_{\psi}$ is selected to be the same in both the static and dynamic approaches. We take this approach to selecting the network architectures in all the experiments considered in this work.

After training,  \ref{eq:DCSG}, \ref{eq:static_PCSG} and Dynamic-PCSG are evaluated by considering $n_{\text{val}} = 100$ out-of-sample contextual observations $\{ \bs{x}^{(i)}_{\text{val}} \}_{i=1}^{n_{\text{val}}}$, and associated distributions {\small $\{\mathbb{P}_{\omega| \bs{x}^{(i)}_{\text{val}} } \}_{i=1}^{n_{\text{val}}}$}. Given a task-mapping $\bs{f}$ along with an observation $\bs{x}^{(i)}$, ($\zeta$-SAA) is solved using the predicted surrogate scenarios $\zeta_{1...K}^{(i)} = \bs{f}(\bs{x}^{(i)})$. A resulting first-stage solution is then evaluated using the 2SP objective with the expected recourse calculated using $\mathbb{P}_{\omega | \bs{x}^{(i)}}$. Additionally, we also calculated the 2SP loss where the (\ref{eq:opt_search}) objective is replaced with the 2SP objective defined by $\mathbb{P}_{\omega | \bs{x}^{(i)}}$ and observed no difference between the two ways to evaluate solutions produced by $\bs{f}$. Let $v_{\text{MMD}}^{(i)}$, $v_{\text{Static}}^{(i)}$, and $v_{\text{Dynamic}}^{(i)}$ denote the objectives for the $i$th validation observation for \ref{eq:DCSG}, \ref{eq:static_PCSG} and Dynamic-PCSG respectively. The optimal newsvendor solution is also computed using $\mathbb{P}_{\omega | \bs{x}^{(i)}_{\text{val}}}$ with optimal objective value denoted by $v_{\text{2SP}}^{(i)}$.

We consider two additional benchmarks. The first considers the expected value solution, determined by considering the newsvendor problem with a single scenario given by $\mathbb{E}[\omega | \bs{x}]$. In practice, the conditional mean is not known, so this is a benchmark that cannot be implemented. Considering the expected value solution allows us to ascertain whether the proposed methodologies can unlock the value of the stochastic solution \citep{birge1982value}. The second benchmark is a quantile regression approach. It is well known that the optimal solution to the newsvendor is
$$
y^* = 
\begin{cases} 
0 & \text{if } \frac{q-c}{q-r} < F_{\mathbb{P}}(0), \\
u & \text{if } \frac{q-c}{q-r} > F_{\mathbb{P}}(u), \\
F_{\mathbb{P}}^{-1}\left( \frac{q-c}{q-r} \right) & \text{otherwise}. 
\end{cases}\quad , 
$$
where  $F_{\mathbb{P}}$ is the cumulative density according to $\mathbb{P}$ and $F^{-1}_{\mathbb{P}}(\alpha)$ is the $\alpha$ quantile of $F$. This suggests a quantile estimation approach to obtain a linear model \hbox{$F_{\mathbb{P}_{\omega|\bs{x}}}^{-1}\left( \frac{q-c}{q-r} \right) = \bs{\beta}^{\intercal} \bs{x}$}, from the training sample. The quantile estimation approach is well-studied in newsvendor problems. \citet{liu2022newsvendor} point out that quantile estimation is equivalent to determining the optimal linear (in $\bs{x}$) purchasing policy that minimizes the opportunity cost over the sample. The quantile regression (QR) is performed using the $\textsf{stats-models}$ package \citep{seabold2010statsmodels} that implements QR as in \citep{koenker2001quantile}. Let $v_{\mathbb{E}[\omega | \bs{x}]}^{(i)}$ and $v_{\text{QR}}^{(i)}$ denote the task losses for the solutions obtained via the expected value and QR approaches respectively for the $i$th validation sample. 


In this experiment $K$ varies in $\{1,2,5\}$, along with the MMD regularization parameter $\lambda \in \{10^{-1}, 10^{0}, 10^{1}\}$ to determine how the proposed approaches compare against the benchmarks. The experiment consists of $n_{\text{trials}} = 20$ trials. In each trial, a training set of $n$ joint observations and the evaluation set of $n_{\text{eval}}$ contexts and conditional distributions are sampled. Each approach is trained and evaluated. For each trial and validation instance, the optimality gap for each method is calculated as
$$
\textsf{Gap}_{m} = (v_{m} - v_{\text{2SP}})/|v_{\text{2SP}}|, \quad m \in \{\mathbb{E}[\omega | \bs{x}], \text{MMD}, \text{Static}, \text{Dynamic}, \text{QR} \}.
$$

\vspace{\littlespace}
\noindent \textbf{Results: } Figure \ref{fig:newsvendor} shows the cumulative distribution function (CDF) of the optimality gaps for each method by $K$ and $\lambda$ evaluated over the validation instances. If $(x,y)$ lies on the curve, the method achieves an optimality gap no more than $10^{x}$ on $100y\%$ of problems on the validation sets over all the trials. As expected, the quantile regression approach performs the best as it explicitly uses the samples to predict the closed-form solution to the contextual newsvendor problem. The MMD approach captures more of the value of the stochastic solution as $K$ is increased. However, the MMD approach has the downside that when $K = 1$, it reduces to conditional mean estimation via least-squares and, as such, is unlikely to offer any value over the expected value solution on average (since the expected value solution is the associated Bayes optimal predictor). 


\begin{figure}
    \FIGURE
    {\includegraphics[width=\textwidth]{Performance_Newsboy.eps}}
    {Out of sample optimality gaps for the contextual newsvendor problem.\label{fig:newsvendor}}
    {}
\end{figure}

The task-based approaches yield poorly performing scenario mappings when \hbox{$\lambda = 0.1$}. In this setting, the dynamic approach slightly improves on the static approach. Furthermore, the MMD approach outperforms when the task-based approaches lack regularization. When $K \in \{1, 2\}$ and $\lambda = 1.0$, the task-based approaches out-perform MMD significantly. Of the CSG approaches, the best-performing approaches when $K \in \{1, 2\}$ are the task-based approaches with $\lambda = 1.0$, and when $K = 5$, the best-performing approaches are the task-based approaches with $\lambda = 10$. However, when $K = 5$, the MMD approach is competitive with the task-based approaches. Furthermore, when $K > 1$, and $\lambda \geq 1.0$, the CSG approaches consistently exhibit value over the expected value solution. Lastly, task-based CSG outperforms the expected value solution when $K = 1$, $\lambda = 1.0$.


Overall, task-based approaches offer more value than the MMD approach for smaller values of $ K $. However, for larger $K$, the MMD approach can obtain higher-quality solutions. The importance of distributional regularization is also readily apparent, allowing PCSG methods to outperform DCSG significantly for small values of $K \leq 2$. Learning the surrogate scenario mapping for ($\zeta$-SAA) in a purely task-driven manner does not yield mappings that produce high-quality solutions. Although the QR approach exhibits the best performance, CSG methods still display desirable performance. Furthermore, CSG can be applied independently of the problem structure. The remaining experiments consider problem settings that are not endowed with a prescribed problem-driven approach to contextual optimization. 




\subsection{Capacity Planning (CEP1)} 
% write out formulation
\textbf{Problem Setting:} Next, we test CSG via a contextual version of the CEP1 problem discussed in the introduction. In this application, the production manager wishes to decide the total hours of added capacity $\bs{y}_{\text{cap}} \in \mathbb{R}^{n_{\text{machines}}}_+$ and hours of operation $\bs{y}_{\text{op}} \in \mathbb{R}^{n_{\text{machines}}}_+$ to meet uncertain demand for the $m$ parts $\omega \in \mathbb{R}_+^m$. The detailed formulation of the CEP1 problem is relegated to Appendix \ref{appendix:cep1_formulation}. 

\vspace{\littlespace}
\noindent \textbf{Experimental Setup:}
Similar to the newsvendor, we consider a randomly initialized neural network to generate the conditional distribution $\mathbb{P}_{\omega | \bs{x}}$ for context $\bs{x}$. The procedure used to define the underlying $\mathbb{P}_{\omega | \bs{x}}$ is described in Appendix \ref{appendix:cep1_data_generation}. The experimental setup is nearly identical to that of the newsvendor. Table \ref{tab:CEP1_parameters} contains the CEP1 experiment parameters. 



\begin{table}[h]
    \TABLE
    {Summary of CEP1 Experimental Parameters \label{tab:CEP1_parameters}}
    {\begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Number of samples ($n$) & 500 \\
        Number of out-of-sample instances ($n_{\text{eval}}$) & 200 \\
        Number of surrogate scenarios ($K$) & $\{1, 2, 5\}$ \\
        MMD regularization ($\lambda$, scaled by $\mathcal{\hat{L}}_{\text{MMD}}$) & $\mathcal{\hat{L}}_{\text{MMD}} * \tilde{\lambda}$ where $\tilde{\lambda} \in \{10^{-3+i}\}_{i=1}^4$ \\
        Number of sampling repetitions ($n_{\text{trials}}$) & 20 \\
        Number of loss-net updates in dynamic PCSG ($T$) & 4 \\
        \bottomrule
        \end{tabular}}{}
    \end{table}

The resulting 2SP is a two-stage linear program; thus, there are no guarantees for uniqueness. Furthermore, the task-net $\bs{f}_{\phi}$ is parameterized as a fully connected feedforward neural network with ReLu activation, mapping $\bs{x}$ to $\mathbb{R}_{+}^{m \times K}$.  

\vspace{\littlespace}
\noindent \textbf{Results:} The CDF of the optimality gap for each method, by $K$ and $\lambda$ is relegated to Appendix \ref{appendix:cep1_performance} for space considerations. Table \ref{tab:CEP1_gaps} shows the median optimality gaps over all the validation instances and trials. We observe in this setting that the value of considering stochastic solutions is not large since the expected value solution attains a median optimality gap of $1.69\%$. Unlike the newsvendor, the MMD approach does not yield solutions that are closer to optimal as $K$ increases, yielding some evidence of an overfitting effect for large $K$. Although the MMD approach still underperforms the expected value solutions when $K = 1$. In settings where $\lambda \geq 0.1$, the dynamic approach consistently improves on the static approach, highlighting the performance gains obtained by iteratively refining the loss-net via the dynamic approach. When $(K, \tilde{\lambda}) = (1, 0.01)$, the PSCR methods attain smaller optimality gaps than the other methods and can unlock the value of stochastic solution with a single scenario. When $K \geq 2$, the PCSG methods perform similarly to MMD. 



    
\begin{table}[h]
    \centering
        \begin{minipage}{0.49\textwidth}
        \centering
        \TABLE
        {(CEP1) Median optimality gaps on validation instances for various methods \label{tab:CEP1_gaps}}
        {
        \begin{tabular}{cccccc}
        \toprule
         & $K=1$ & $K=2$ & $K=5$ \\
        \midrule
        $\mathbb{E}[\omega | \mathbf{x}]$ & 1.69\% & 1.69\% & 1.69\% \\
        MMD & 2.36\% & 0.43\% & 0.76\% \\
        \midrule
        \multicolumn{1}{c}{$\tilde{\lambda}$} & \multicolumn{3}{c}{\textbf{Static}} \\
        \midrule
        0.01 & 1.50\% & 5.04\% & 6.31\% \\
        0.1  & 2.07\% & 1.23\% & 1.20\% \\
        1    & 2.31\% & 0.44\% & 0.65\% \\
        10   & 2.36\% & 0.39\% & 0.65\% \\
        \midrule
        \multicolumn{1}{c}{$\tilde{\lambda}$} &  \multicolumn{3}{c}{\textbf{Dynamic}} \\
        \midrule
        0.01 & 1.18\% & 0.57\% & 0.78\% \\
        0.1  & 1.89\% & 0.70\% & 0.99\% \\
        1    & 2.23\% & 0.44\% & 0.61\% \\
        10   & 2.31\% & 0.39\% & 0.49\% \\
        \bottomrule
        \end{tabular}}{}
    \end{minipage}
     \hfill
        \begin{minipage}{0.49\textwidth}
        %\captionsetup{width=.75\textwidth}
        \centering
        \TABLE
        {(CEP1) Fraction of \\ instances with the lowest \\ out-of-sample 2SP cost. \label{tab:CEP1_competition}}
        {
        \begin{tabular}{cccccc}
        \toprule
         & $K=1$ & $K=2$ & $K=5$ \\
        \midrule
        $\mathbb{E}[\omega | \mathbf{x}]$ & 11.6\% & 0.15\% & 0.10\% \\
        MMD & 0.0\% & 15.1\% & 11.7\% \\
        \midrule
        \multicolumn{1}{c}{$\tilde{\lambda}$} & \multicolumn{3}{c}{\textbf{Static}} \\
        \midrule
        0.01 & 29.8\% & 5.40\%  & 2.10\%  \\
        0.1  & 0.18\%  & 4.85\%  & 9.30\%  \\
        1    & 0.00\%  & 10.0\% & 13.7\% \\
        10   & 0.05\%  & 12.6\% & 14.4\% \\
        \midrule
        \multicolumn{1}{c}{$\tilde{\lambda}$} &  \multicolumn{3}{c}{\textbf{Dynamic}} \\
        \midrule
        0.01 & 55.7\% & 18.9\% & 15.5\% \\
        0.1  & 2.10\%  & 9.15\%  & 10.9\% \\
        1    & 0.40\%  & 10.1\% & 9.03\%  \\
        10   & 0.27\%  & 13.8\% & 13.3\% \\ 
        \bottomrule
        \end{tabular}}{}
    \end{minipage}
    \hfill
\end{table}


Table \ref{tab:CEP1_competition} compares the performance of the different methods across $K$ by showing the percentage of instances in which the particular method obtains the lowest out-of-sample 2SP cost (the columns sum to $100\%$). When $K=1$, the MMD (regression) approach never achieves the lowest 2SP objective. Knowing the true conditional expectation yields the best solution in only 11.6\% of instances. In contrast, the static and dynamic methods outperform when $\tilde{\lambda} = 0.01$, achieving the lowest cost in $29.8\%$ and $55.7\%$ of instances, respectively. When $K = 2$ $(5)$, the MMD approach becomes more competitive, obtaining the lowest 2SP cost in $15.1\%$ ($11.7\%$) instances. Still, when $K \in \{2, 5\}$, the dynamic approach with $\tilde{\lambda}$ obtains the best objective most often. Like the contextual newsvendor, task-based approaches offer more value than the MMD approach for smaller values of $K$. Although the MMD approach can obtain higher-quality solutions for larger $K$, with appropriate regularization, task-driven approaches tend to obtain the best results more often.


\subsection{Portfolio Optimization} 

\textbf{Problem Setting:} Next, we test CSG in a contextual portfolio optimization setting. In this application the trader wishes to select a portfolio $\bs{y} \in \mathbb{R}_+^{n_{\text{assets}}}$ to balance risk and expectation of the portfolio return $\omega^{\intercal} \bs{y}$ where $\omega \in \mathbb{R}^{n_{\text{assets}}}$ denotes the asset-returns. For execution reasons, the portfolio must consist of some subset $K_{\text{assets}} < n_{\text{assets}}$. The trader wishes to use contextual information to make better decisions quickly. In this setting, the trader takes $\bs{x} \in \mathbb{R}^{(n_{\text{assets}} + n_{\text{encoding}}) \times W}$ to be a panel of $W$ past return observations for each of the $n_{\text{assets}}$ assets, along with a representation of the 5-minute period within the 78 period trading session. The Conditional Value-at-Risk (CVaR) is the risk measure of choice. The reader is referred to \citet{krokhmal2002portfolio} for more details regarding portfolio selection based on CVaR. The detailed formulation of the CVaR problem is relegated to Appendix \ref{appendix:cvar_formulation}. 



\vspace{\littlespace}
\noindent \textbf{Experimental Setup:} We consider a data-driven approach to set up the contextual environment. The trader assumes that the returns satisfy the following relationship: $
\omega = \Phi_{\boldsymbol{\mu}}(\boldsymbol{x}) + \Phi_{\sigma} (\boldsymbol{x}) \odot \boldsymbol{\epsilon}$, 
where $\Phi_{\boldsymbol{\mu}} : \mathcal{X} \rightarrow \mathbb{R}^{n_{\text{assets}}}$ and $\Phi_{\sigma} : \mathcal{X} \rightarrow \mathbb{R}_+^{n_{\text{assets}}}$ are models given to the trader by a statistical modeler to estimate the conditional mean $\mathbb{E}[\omega |\bs{x}]$, and conditional standard deviation $\sqrt{\text{Var}[\omega | \bs{x}]}$ along with $\boldsymbol{\epsilon}$ denoting the errors which obey a uniform distribution supported on $n^* = 1559$ points $\{\bs{\epsilon}^{(i)}\}_{i=1}^{n^*}$, derived from a dataset of returns. The procedure defining  $\mathbb{P}_{\omega |\bs{x}}$ is fully described in Appendix \ref{appendix:cvar_data_generation}.

For a given contextual observation, the trader faces an instance of CVaR optimization defined on $n^*$ scenarios. We aim to explore whether the CSG frameworks proposed in this work can enable the trader to directly generate a set of $K$ scenarios from the context, which can then be solved more quickly than solving the problem on all $n^*$ scenarios. We consider a training set and a validation set to evaluate our framework. The trained task-networks are evaluated on the validation set using $\mathbb{P}_{\omega | \bs{x}}$ defined by $\Phi_{\bs{\mu}}$, $\Phi_{\sigma}$ and $\{\bs{\epsilon}^{(i)}\}_{i=1}^{n^*}$. This approach is sensible since our goal is to help the trader solve their CVaR optimization with $\mathbb{P}_{\omega | \bs{x}}$ defined as described above. \hbox{Table \ref{tab:CVaR_parameters}} highlights the experiment parameters.


\begin{table}[h]
    \TABLE
    {Summary of CVaR Experimental Parameters \label{tab:CVaR_parameters}}
    {\begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Number of samples ($n$) & $\lfloor{0.8n^* +\frac{1}{2}}\rfloor = 1247$\\
        Number of out-of-sample instances ($n_{\text{eval}}$) & $n^* - \lfloor{0.8n^* +\frac{1}{2}}\rfloor = 312$ \\
        Number of surrogate scenarios ($K$) & $\{1, 3, 5, 10, 20, 40\}$ \\
        MMD regularization ($\lambda$) & $\{80*2^{i}\}_{i=0}^7$ \\
        %Number of loss-net updates in dynamic PCSG ($T$) & 4 \\
        \bottomrule
        \end{tabular}}{}
    \end{table}


The resulting 2SP is mixed-binary in the first stage and is linear in the second stage; thus, there are no uniqueness guarantees. Furthermore, The task-net $\bs{f}_{\phi}$ is parameterized by an LSTM network that outputs a \hbox{$n_{\text{assets}} \times K$} matrix.



\vspace{\littlespace}
\noindent \textbf{Results:} The CDF of the optimality gap for each method, by $K$ and $\lambda$ is relegated to Appendix \ref{appendix:cvar_performance} for space considerations. Table \ref{tab:cvar_gaps} shows the median optimality gaps over the validation instances. Unlike CEP1, the value of stochastic solution is typically large, with the expected value solution exhibiting a median gap of $241\%$. Like the other experiments, the MMD approach produces solutions closer to optimal as $K$ increases while underperforming the expected value solutions when $K=1$.

\begin{table}[h]
    \centering 
        \TABLE
        {(CVaR) Median optimality gaps on validation instances for various methods. \label{tab:cvar_gaps}}
        {
        \begin{tabular}{ccccccc}
        \toprule
         & $K=1$ & $K=3$ & $K=5$ & $K=10$ & $K=20$ & $K=40$ \\
        \midrule
        $\mathbb{E}[\omega | \mathbf{x}]$ & 241\%      & 241\%      & 241\%      & 241\%      & 241\%      & 241\%      \\
        MMD & 483\%      & 166\%      & 122\%      & 55.1\%     & 18.7\%     & 10.6\%     \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{6}{c}{\textbf{Static}} \\
        \midrule
        80    & 1.03e7\%   & 486\%      & 96.9\%     & 79.0\%     & 38.2\%     & 118\%      \\
        160   & 7.41e3\%   & 120\%      & 100\%      & 84.9\%     & 92.2\%     & 25.7\%     \\
        320   & 2.60e7\%   & 106\%      & 101\%      & 45.7\%     & 28.4\%     & 11.9\%     \\
        640   & 424\%      & 98.4\%     & 96.9\%     & 93.8\%     & 29.7\%     & 10.9\%     \\
        1280  & 456\%      & 115\%      & 77.1\%     & 88.5\%     & 15.8\%     & 8.67\%     \\
        2560  & 474\%      & 182\%      & 70.8\%     & 32.5\%     & 27.6\%     & 12.4\%     \\
        5120  & 486\%      & 152\%      & 94.1\%     & 45.0\%     & 19.5\%     & 9.27\%     \\
        10240 & 485\%      & 164\%      & 76.2\%     & 33.6\%     & 18.0\%     & 17.5\%     \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{6}{c}{\textbf{Dynamic}} \\
        \midrule
        80    & 1.02e3\%   & 296\%      & 225\%      & 94.8\%     & 212\%      & 62.8\%     \\
        160   & 1.00e3\%   & 310\%      & 135\%      & 93.5\%     & 221\%      & 54.1\%     \\
        320   & 365\%      & 172\%      & 98.6\%     & 60.8\%     & 82.1\%     & 20.2\%     \\
        640   & 736\%      & 154\%      & 94.2\%     & 28.6\%     & 38.8\%     & 11.6\%     \\
        1280  & 433\%      & 108\%      & 91.2\%     & 51.8\%     & 24.2\%     & 18.6\%     \\
        2560  & 401\%      & 89.5\%     & 65.3\%     & 39.6\%     & 39.8\%     & 7.55\%     \\
        5120  & 439\%      & 160\%      & 66.2\%     & 28.1\%     & 14.4\%     & 10.3\%     \\
        10240 & 458\%      & 148\%      & 75.5\%     & 29.9\%     & 29.7\%     & 5.07\%     \\
        \bottomrule
        \end{tabular}}{}
    \end{table}
    \begin{table}[h]
         \centering 

        \TABLE
        {(CVaR) Fraction of instances with the lowest out-of-sample 2SP cost. \label{tab:cvar_competition}}
        {
        \begin{tabular}{ccccccc}
        \toprule
         & $K=1$ & $K=3$ & $K=5$ & $K=10$ & $K=20$ & $K=40$ \\
        \midrule
        $\mathbb{E}[\omega | \mathbf{x}]$ & 74.4\%   & 0.00\%    & 0.00\%    & 0.00\%    & 0.00\%    & 0.00\%    \\
        MMD & 0.32\%   & 0.32\%    & 0.00\%    & 0.00\%    & 30.8\%    & 0.32\%    \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{6}{c}{\textbf{Static}} \\
        \midrule
        80    & 0.00\%   & 0.00\%    & 12.8\%    & 6.09\%    & 0.00\%    & 0.00\%    \\
        160   & 0.00\%   & 8.65\%    & 8.65\%    & 4.81\%    & 0.00\%    & 0.00\%    \\
        320   & 0.00\%   & 13.1\%    & 4.81\%    & 0.00\%    & 0.00\%    & 4.81\%    \\
        640   & 2.24\%   & 14.7\%    & 26.0\%    & 0.00\%    & 10.3\%    & 29.5\%    \\
        1280  & 4.17\%   & 1.28\%    & 0.00\%    & 0.64\%    & 13.8\%    & 1.28\%    \\
        2560  & 0.64\%   & 0.00\%    & 2.56\%    & 0.32\%    & 6.73\%    & 0.00\%    \\
        5120  & 0.32\%   & 0.64\%    & 0.00\%    & 1.92\%    & 0.00\%    & 0.00\%    \\
        10240 & 0.32\%   & 0.96\%    & 0.00\%    & 20.2\%    & 0.32\%    & 0.00\%    \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{6}{c}{\textbf{Dynamic}} \\
        \midrule
        80    & 0.00\%   & 0.00\%    & 0.00\%    & 0.00\%    & 0.00\%    & 0.00\%    \\
        160   & 0.00\%   & 0.00\%    & 0.00\%    & 0.00\%    & 0.00\%    & 0.00\%    \\
        320   & 10.9\%   & 0.00\%    & 3.85\%    & 0.32\%    & 0.00\%    & 5.13\%    \\
        640   & 0.00\%   & 6.73\%    & 12.5\%    & 19.9\%    & 6.73\%    & 3.21\%    \\
        1280  & 2.88\%   & 6.41\%    & 1.28\%    & 0.00\%    & 0.00\%    & 0.00\%    \\
        2560  & 2.56\%   & 44.2\%    & 13.5\%    & 0.00\%    & 0.00\%    & 3.53\%    \\
        5120  & 1.28\%   & 0.00\%    & 13.1\%    & 16.7\%    & 31.4\%    & 0.64\%    \\
        10240 & 0.00\%   & 2.88\%    & 0.96\%    & 29.2\%    & 0.00\%    & 51.6\%    \\
        \bottomrule
        \end{tabular}}{}
\end{table}


When $K = 1$, the CSG approaches do not outperform the expected value solution, although the static and dynamic methods tend to outperform the MMD approach for sufficiently large $\lambda$. The static and dynamic approaches also attain smaller optimality gaps than the MMD approach for $K \geq 3$ and sufficiently large $\lambda$. For example, when $K = 3$ and $\lambda \geq 160$, the static approach outperforms the MMD approach. Lastly, for any fixed $K$, the dynamic approach attains a smaller optimality gap than the static approach for some suitable value of $\lambda$. For instance, when $K = 10$, the dynamic approach attains a median gap of $28.1\%$ when $\lambda = 5120$, whereas the static approach attains the static approach's best gap is $32.5\%$ when $\lambda = 2560$. When $K = 40$, the static and dynamic approaches have median gaps less than $10.0\%$, whereas the MMD approach does not. Notably, the dynamic approach with $(K, \lambda)= (40, 10240)$ can achieve a median gap of $5.07\%$.


Table \ref{tab:cvar_competition} compares the performance of the different methods across $K$ by showing the percentage of instances in which the particular method obtains the lowest out-of-sample 2SP cost (the columns sum to $100\%$). When $K=1$, the expected value solution dominates; however, when \hbox{$K > 1$}, the expected value solution never obtains the lowest out-of-sample cost. Furthermore, the problem-driven approaches always out-perform when \hbox{$K > 1$}, since for every $K$, there is some $\lambda$ such that the problem-driven approach wins most often. Unlike previous experiments, the task-based approaches offer more value than the MMD approach across all values of tested $K$. 

\subsection{Multidimensional Newsvendor with Substitution} 
% write out formulation, mip, non-linear

\noindent \textbf{Problem Setting:} Next, we test CSG in a contextual setting where the decision maker is tasked with solving a multidimensional newsvendor problem. We consider the version of the problem introduced by \citet{narum2024problem} and provide a brief overview. The Multidimensional Newsvendor with Substitution (MNV) is a production planning problem with a structure similar to (\ref{eq:newvendor}), where production decisions regarding $m$ products are made before uncertain demand $\omega \in \mathbb{R}^m$ is realized. The primary distinction is that MNV considers multiple products and allows products to be substituted with each other. The original application by \citet{vaagen2011modelling} considers fashion apparel where retailers offer substitutable products. The detailed formulation of the MNV problem is relegated to Appendix \ref{appendix:MNV_formulation}. 



\vspace{\littlespace}
\noindent \textbf{Experimental Setup:}  Similar to newsvendor and CEP1, we consider randomly initialized neural networks to generate the conditional distribution $\mathbb{P}_{\omega | \bs{x}}$ for context $\bs{x}$. The procedure used to define the underlying $\mathbb{P}_{\omega | \bs{x}}$ is described in Appendix \ref{appendix:MNV_data}. The process used to define $\mathbb{P}_{\omega | \bs{x}}$ aims to capture the salient aspects of the product-demand distribution as pointed out by \citet{vaagen2011modelling}. Table \ref{tab:MNV_parameters} highlights the experiment parameters. 
\begin{table}[h]
    \TABLE
    {Summary of MNV Experimental Parameters  \label{tab:MNV_parameters}}
    {
    \begin{tabular}{ll}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Number of samples ($n$) & $300$\\
    Number of out-of-sample instances ($n_{\text{eval}}$) & $100$ \\
    Number of surrogate scenarios ($K$) & $\{1, 3, 5, 10\}$ \\
    MMD regularization ($\lambda$) & $\{5*4^{i}\}_{i=0}^6$ \\
    \bottomrule
    \end{tabular}}{}
\end{table}

The resulting MNV problem is a mixed-binary program (in both stages); thus, there are no uniqueness guarantees. Furthermore, the task-net $\bs{f}_{\phi}$ is parameterized as a fully connected feedforward neural network with ReLu activation, mapping $\bs{x}$ to $\mathbb{R}_{+}^{m \times K}$.

\vspace{\littlespace}
\noindent \textbf{Results:} The CDF of the optimality gap for each method, by $K$ and $\lambda$ is relegated to Appendix \ref{appendix:MNV_performance} for space considerations. Table \ref{tab:mnv_gaps} shows the median optimality gaps over the validation instances. In the MNV setting, the value of stochastic solution is small, with the expected value solution exhibiting a median gap of $2.70\%$. The MMD approach produces solutions closer to optimal as $K$ is increased. However, the MMD approach does not outperform for $K > 1$ and outperforms the expected value solution when $K = 10$.

When $K = 1$, the CSG approaches underperform the expected value solution, although the static approach outperforms the MMD approach for sufficiently large $\lambda \geq 80$. When $K = 3$, the static and dynamic approaches underperform the MMD approach and perform similarly when $\lambda = 20480$. In the case, $K = 5$, the static approach ($\lambda \geq 320$) attains smaller median gaps than the expected value and MMD approach, whereas the dynamic approach does not. In particular, the static method when $(K, \lambda) \in \{(5, 5120), (5, 20480))$ attains smaller optimality gaps more frequently than the expected value and MMD approaches (see Appendix \ref{appendix:MNV_performance}). Lastly, when $K = 10$, the static and dynamic approaches outperform the expected value approach, but only the static approach obtains similar performance on par with that of the MMD approach (when $\lambda \in \{320, 5120\}$).

\begin{table}[h]
    \begin{minipage}{0.49\linewidth}
        \TABLE
        {(MNV) Median optimality gaps on validation instances by method. \label{tab:mnv_gaps}}
        {
        \begin{tabular}{ccccc}
        \toprule
         & $K=1$ & $K=3$ & $K=5$ & $K=10$ \\
        \midrule
        $\mathbb{E}[\omega | \mathbf{x}]$ & 2.70\%    & 2.70\%    & 2.70\%    & 2.70\%    \\
        MMD & 3.81\%    & 2.84\%    & 2.82\%    & 1.66\%    \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{4}{c}{\textbf{Static}} \\
        \midrule
        5     & 102\%     & 37.5\%    & 18.4\%    & 13.4\%    \\
        20    & 4.61\%    & 9.78\%    & 5.40\%    & 2.60\%    \\
        80    & 2.80\%    & 4.86\%    & 3.44\%    & 1.93\%    \\
        320   & 3.32\%    & 3.17\%    & 2.61\%    & 1.75\%    \\
        1280  & 3.67\%    & 3.15\%    & 2.65\%    & 1.81\%    \\
        5120  & 3.73\%    & 2.89\%    & 2.07\%    & 1.73\%    \\
        20480 & 3.72\%    & 2.80\%    & 2.19\%    & 1.90\%    \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{4}{c}{\textbf{Dynamic}} \\
        \midrule
        5     & 74.7\%    & 55.9\%    & 29.8\%    & 37.8\%    \\
        20    & 8.69\%    & 39.3\%    & 23.2\%    & 7.65\%    \\
        80    & 3.26\%    & 10.9\%    & 5.73\%    & 2.73\%    \\
        320   & 3.70\%    & 3.44\%    & 3.12\%    & 2.10\%    \\
        1280  & 4.18\%    & 3.76\%    & 2.79\%    & 2.16\%    \\
        5120  & 4.26\%    & 3.37\%    & 2.89\%    & 2.17\%    \\
        20480 & 4.26\%    & 2.83\%    & 2.85\%    & 2.01\%    \\
        \bottomrule
        \end{tabular}}{}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \TABLE
        {(MNV) Fraction of instances with the lowest out-of-sample 2SP cost. \label{tab:mnv_competition}}
        {
        \begin{tabular}{ccccc}
        \toprule
         & $K=1$ & $K=3$ & $K=5$ & $K=10$ \\
        \midrule
        $\mathbb{E}[\omega | \mathbf{x}]$ & 20.8\%   & 13.8\%   & 9.33\%   & 10.0\%    \\
        MMD & 2.17\%   & 11.7\%   & 7.00\%   & 9.50\%    \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{4}{c}{\textbf{Static}} \\
        \midrule
        5      & 2.17\%   & 3.67\%   & 0.67\%   & 1.50\%    \\
        20     & 12.5\%   & 0.50\%   & 8.00\%   & 9.50\%    \\
        80     & 11.5\%   & 5.50\%   & 6.50\%   & 7.67\%    \\
        320    & 2.00\%   & 7.83\%   & 6.17\%   & 9.50\%    \\
        1280   & 3.33\%   & 8.17\%   & 9.17\%   & 7.00\%    \\
        5120   & 2.00\%   & 6.83\%   & 10.0\%   & 5.50\%    \\
        20480  & 2.17\%   & 10.7\%   & 11.8\%   & 7.33\%    \\
        \midrule
        \multicolumn{1}{c}{$\lambda$} & \multicolumn{4}{c}{\textbf{Dynamic}} \\
        \midrule
        5      & 0.00\%   & 0.00\%   & 0.00\%   & 0.83\%    \\
        20     & 13.0\%   & 0.00\%   & 0.17\%   & 2.00\%    \\
        80     & 14.3\%   & 0.17\%   & 2.83\%   & 5.83\%    \\
        320    & 4.00\%   & 7.50\%   & 8.17\%   & 7.17\%    \\
        1280   & 4.50\%   & 7.50\%   & 7.33\%   & 5.67\%    \\
        5120   & 3.00\%   & 7.33\%   & 7.17\%   & 5.17\%    \\
        20480  & 2.50\%   & 8.83\%   & 5.17\%   & 5.83\%    \\
        \bottomrule
        \end{tabular}}{}
    \end{minipage}
\end{table}

Table \ref{tab:mnv_competition} compares the performance of the different methods across $K$ by showing the percentage of instances in which the particular method obtains the lowest out-of-sample 2SP cost (the columns sum to $100\%$). When $K=1$, the expected value solution dominates; however, the static and dynamic approaches with $\lambda \in \{20, 80\}$ are competitive. 
When $K > 1$, the expected value solution's relative performance deteriorates by roughly a factor of 2. Furthermore, in the cases $K > 1$, the wins are split more evenly among the MMD, static, and dynamic approaches, where the static and dynamic approaches are heavily regularized. 



Surprisingly, the dynamic approach fails to outperform the static approach (as one would expect that iteratively refining $E_{\psi}$ would yield better results). In the MNV example, $E_{\psi}$ was able to construct an accurate approximation on the dataset generated by the learned MMD task-net: $\zeta_{1 \hdots K}^{(i)} = \bs{\hat{f}}_{\text{MMD}}(\bs{x}^{(i)})$, $\ell_{\text{opt}}^{(i)} = \ell_{\text{opt}}(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)})$. However, the architecture for $E_{\psi}$, selected by random search with $20\%$ hold out, did not generalize to the dataset generated in subsequent iterations of the dynamic approach. When one faces this situation, a sensible approach is to consider task-nets $\bs{f}$ that produce $\tilde{\zeta}_{1 \hdots K}^{(i)} = \bs{f}(\bs{x}^{(i)})$ that are close to $\zeta_{1 \hdots K}^{(i)}$ with respect to MMD distance. Setting $\lambda$ to a large value aims to achieve this, and indeed, we observe in Tables \ref{tab:mnv_gaps} and \ref{tab:mnv_competition} that only heavily regularized PCSG approaches achieve out-performance relative to the MMD approach. 


\subsection{Timing Analysis}

This section explores the time trade-offs presented by the proposed methodologies. A time investment exists to train the task-net and loss-net used in the CSG approaches. However, once trained, the task-net can quickly generate solutions to 2SP by computing a forward pass through the task-net and solving ($\zeta$-SAA) on the resulting $K$ surrogate scenarios. The following constitutes the relevant computation times:
\begin{itemize} % Adjust indentation and spacing between items
    \item \textbf{MMD Training:} Time taken to train the task-net via DCSG.
    \item \textbf{Surrogate Solution Calculation:} Time taken to pass through the task network and compute an optimal solution to the ($\zeta$-SAA) problem for all $n_{\text{eval}}$ contextual problems in the evaluation set.
    \item \textbf{2SP Solve:} Time taken to solve all $n_{\text{eval}}$ contextual 2SPs in the evaluation set to optimality.
    \item \textbf{MMD Loss Evaluation:} Time taken to calculate the loss $\ell_{\text{opt}}$ over the training sample $S$.
    \item \textbf{Loss-Net Training:} Time taken to train the loss network.
    \item \textbf{Static Training:} Time taken to train the task network via Static-PCSG.
    \item \textbf{Dynamic Training:} Time taken to train the task network via Dynamic-PCSG.
\end{itemize}



The computation times for the processes above are shown in Tables \ref{tab:CVaR_times} and \ref{tab:MNV_times} for both the CVaR and MNV experiments, respectively, and are located in Appendix \ref{appendix:timing}. Tables \ref{tab:CVaR_times} and \ref{tab:MNV_times} indicate that CSG's trade-off between training time and quick surrogate solution evaluations can be worth it. For example, when $K = 40$, CSG can solve 312 CVaR instances in approximately $33$ seconds using the dynamic approach with $\lambda = 10240$. The resulting solutions have a median optimality gap of $5.07\%$. Similar statements also hold for MNV. Although the CSG methods require additional time to train $\bs{f}$, this time is incurred offline and is justified by repeatedly solving 2SPs in different contexts. 








