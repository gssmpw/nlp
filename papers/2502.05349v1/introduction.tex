\section{Introduction}\label{Introduction}

Two-stage stochastic programming is a widely adopted decision modeling approach that enables decision-makers to account for uncertainty that unfolds over two stages. The structure is as follows: decisions are made in the first stage before some uncertainty is realized, after which the decision maker has recourse and responds to the realized uncertainty (second stage). There is a cost for the first-stage decision, and for each specific realization of uncertainty, there is a cost linked to the recourse actions. Consequently, a common objective is to minimize the combined cost of the first-stage decision and the expected cost of recourse actions. \citet{Ntaimo2024} provides a recent survey of various applications of two-stage stochastic programming in logistics, portfolio management, and manufacturing, among others.


In the two-stage setting, the decision maker selects a first-stage decision $\boldsymbol{y}$ by solving a two-stage stochastic program (2SP)
\begin{equation}
    \min_{\boldsymbol{y}} h(\boldsymbol{y}) + \mathbb{Q}(\boldsymbol{y}) \quad \textit{s.t.} \quad \boldsymbol{y} \in \mathcal{Y}, \quad \boldsymbol{y} \in \mathbb{R}^{s_1},  \label{eq:Stage I} \tag{2SP}
\end{equation}
where $h:  \text{dom}(h) (\subseteq \mathbb{R}^{s_1}) \rightarrow \text{codom}(h) (\subseteq \mathbb{R})$ models the cost of the first-stage decision, with $\text{dom}(h)$ and $\text{codom}(h)$ denoting the domain and codomain of $h$ respectively. Additionally, $\mathcal{Y}$ is the feasible set for first-stage decisions, $\omega \in \Omega \subseteq \mathbb{R}^p$ represents the uncertainty distributed according to the probability measure $\mathbb{P}$, and $\mathbb{Q}(\boldsymbol{y}) = \mathbb{E}_{\mathbb{P}}[Q(\boldsymbol{y}, \omega)]$ is the expected recourse cost with $Q(\boldsymbol{y}, \omega)$ denoting the recourse cost of the first-stage decision $\boldsymbol{y}$ when uncertainty $\omega$ is realized. After the first-stage decision is made and uncertainty is realized, the decision maker has recourse $\boldsymbol{z}$ and determines the best recourse action by minimizing the cost
\begin{equation}
Q(\boldsymbol{y}, \omega) = \min_{\boldsymbol{z}} q(\boldsymbol{z}, \omega) \quad  \textit{s.t.} \quad \boldsymbol{z} \in \mathcal{Z}(\boldsymbol{y}, \omega), \quad \boldsymbol{z} \in \mathbb{R}^{s_2}, \label{eq:stage2} \tag{Stage II}
\end{equation} 
where $q: \text{dom}(q) (\subseteq \mathbb{R}^{s_2} \times \mathbb{R}^{p}) \rightarrow \text{codom}(q) (\subseteq \mathbb{R})$ models the recourse cost for uncertainty $\omega \in \Omega \subseteq \mathbb{R}^p$, the feasible set of recourse actions given the first-stage decision $\boldsymbol{y}$ and uncertainty $\omega$ is denoted by $\mathcal{Z}(\boldsymbol{y}, \omega)$. The following assumption is often employed for 2SPs, and we adopt it also.
\begin{assumption}\label{a1}
    It is assumed that (\ref{eq:stage2}) can be solved efficiently.
\end{assumption}


Solving (\ref{eq:Stage I}) involves evaluating a multidimensional integral that is usually not analytically tractable or amenable to numerical integration due to high dimensionality. Thus, the distribution is often represented by a finite set of scenarios $\hat{\Omega} = \{\omega^{(j)}\}_{j=1}^M \subseteq \Omega$. Sample average approximation (SAA) is a common approach for generating scenarios, which proceeds by sampling $\hat{\Omega}$ from $\mathbb{P}$ \citep{kleywegt2002sample}. Using $\hat{\Omega}$, the following problem is then solved in place of (\ref{eq:Stage I})
\begin{equation}
\min_{\boldsymbol{y} \in \mathcal{Y} \cap \mathbb{R}^{s_1}} \quad   h(\boldsymbol{y}) + \frac{1}{|\hat{\Omega}|}\sum_{j = 1}^{|\hat{\Omega}|}  \min_{\boldsymbol{z} \in \mathcal{Z}(\boldsymbol{y}, \omega^{(j)})} q(\boldsymbol{z}, \omega^{(j)}) .
\label{eq:2SP-SAA} \tag{2SP-SAA}
\end{equation}
Although SAA is asymptotically optimal, solving (\ref{eq:2SP-SAA}) on a small sample of scenarios can lead to suboptimal first-stage decisions and large sample sets are required to obtain high-quality solutions. One possible approach to solve (\ref{eq:2SP-SAA}) is to solve the \textit{extensive form}; a deterministic program formed by introducing scenario-specific copies of the second-stage variables. The size of the extensive form grows linearly with $M$, necessitating algorithms tailored to specific problem classes. Unfortunately, it is widely accepted that optimally solving (\ref{eq:2SP-SAA}) is intractable due to complicating problem components such as non-linear objectives and integrality requirements \citep{patel2022neur2sp, wu2022learning}. Hence, the distribution is often represented by a small set of scenarios consisting of a positive integer $K$ number of scenarios, referred to as surrogate scenarios $\zeta_{1...K} \in \Omega^K$, with the following implicit assumption.
\begin{assumption}\label{a2}
    (\ref{eq:2SP-SAA}) can be efficiently solved on $K$ scenarios for sufficiently small values of $K$.
\end{assumption}
Ideally, $\zeta_{1...K}$ are selected to ensure that the solution to (\ref{eq:2SP-SAA}) on the reduced set is nearly optimal with respect to the (\ref{eq:Stage I}) objective under $\mathbb{P}$. This process is known as \textit{scenario generation}. Typically, scenario generation is done by generating a discretized representation of $\mathbb{P}$ or by sampling a large number of scenarios $\hat{\Omega}$ from $\mathbb{P}$ and performing \textit{scenario reduction} to reduce its size from $M$ to $K$ with $K << M$. Scenario generation encapsulates either case. 

In practice, decision-makers are often tasked with making decisions repeatedly in different settings. Often, decision-makers find themselves in a contextual setting where they are faced with (i) a decision-making problem endowed with uncertainty $\omega$ that impacts the problem's important elements and (ii) the opportunity to exploit side-information that is correlated with $\omega$ at decision-making time. This work focuses on decision-making via 2SPs in a contextual setting. The side information, also referred to as contextual information $\boldsymbol{x} \in \mathcal{X} \subseteq \mathbb{R}^d$, follows a joint distribution with $\omega$ denoted by $\mathbb{P}_{(\boldsymbol{x}, \omega)}$. The marginal distribution of $\boldsymbol{x}$ and the conditional distribution of $\omega$ given $\boldsymbol{x}$ are denoted by $\mathbb{P}_{\boldsymbol{x}}$ and $\mathbb{P}_{\omega|\boldsymbol{x}}$ respectively. A realization of the context $\hat{\boldsymbol{x}}$ is observed before the first-stage decision, and as such, the decision maker wishes to solve (\ref{eq:Stage I}) with expected costs evaluated over $\mathbb{P}_{\omega|\boldsymbol{x} = \hat{\boldsymbol{x}}}$. We consider the following example to motivate the contextual setting. 

  \noindent \textbf{A Motivating Example:}  \citet{higle_sen_1996} introduce CEP1, a two-stage machine capacity expansion problem for a flexible manufacturing facility. The decision maker must plan the expansion of production capacity for $m$ parts on $n_{\text{machines}}$ machines. In its baseline state, machine $j \in [n_{\text{machines}}]  \coloneq [1,2,\hdots, n_{\text{machines}}]$ is available for $h_j$ hours per week, with additional hours available at a cost of $c_j$ per hour. Machines have usage limits and require maintenance based on usage. Part type $i \in [m]$ can be produced on machine $j$ at a rate of $a_{ij}$ with cost $g_{ij}$ per hour. The demand $\omega_{i}$ for product type $i$ is uncertain. First, the decision maker determines machine capacities (first stage) and then creates a production plan to minimize costs after demand $\omega_i$ is realized (second stage). Unmet demand is subcontracted at a premium. In the original problem, capacity decisions are made weekly, but if product demand fluctuates more frequently, adjusting capacity more often (e.g., minutely) may be valuable. This situation could arise if the product demands come from a manufacturing line with variable product demand. Here, context $\bs{x}$ includes data from the assembly line, such as past demands. Capacity decisions must be made quickly while incorporating the uncertain demand, its relationship to the known context, and the available recourse.
    
Thus, it is desirable to obtain an approach for efficiently solving 2SPs, repeatedly in different contexts $\bs{x}$, such that the resulting solution performs well according to the 2SP objective, evaluated according to $\mathbb{P}_{\omega | \bs{x}}$. However, decision-makers typically do not have access to the conditional distribution and instead only have access to historical data, motivating the following assumption.
\begin{assumption}\label{a3}
 The decision-maker does not have access to {\small $\mathbb{P}_{\omega|\boldsymbol{x} = \hat{\boldsymbol{x}}}$} and only has access to an independent and identically distributed (iid) sample of $n$ observations from {\small $\mathbb{P}_{(\boldsymbol{x}, \omega)}$}, denoted by {\small $S = \{(\boldsymbol{x}^{(i)}, \omega^{(i)})\}_{i=1}^{n} \overset{iid}{\sim} \mathbb{P}_{(\boldsymbol{x}, \omega)}$}. 
\end{assumption}

It is sensible for the decision maker to use $S$ to estimate $\mathbb{P}_{\omega|\boldsymbol{x}}$, denoted by $p_{\theta}(\omega | \boldsymbol{x})$ and parameterized by $\theta$. Figure \ref{fig:fig1} outlines a traditional approach to contextual 2SP based on the abovementioned considerations. First, data is used to estimate $p_{\theta}(\omega | \boldsymbol{x})$. A realization of the context $\hat{\boldsymbol{x}}$ is observed. However, solving (\ref{eq:Stage I}) with the estimated conditional distribution is not analytically tractable, thus, SAA is employed and a set of samples is generated via $p_{\theta}(\omega | \boldsymbol{x} = \hat{\boldsymbol{x}})$. The resulting instance of \hbox{(\ref{eq:2SP-SAA})} is often unwieldy, and the scenarios are reduced to the set of surrogate scenarios $\zeta_{1...K}$ using a scenario reduction approach. The resulting instance of (\ref{eq:2SP-SAA}) is finally solved on the reduced set of scenarios, yielding a first-stage solution. The combination of sampling from the estimated conditional distribution and the subsequent scenario reduction constitutes the standard approach one would take to generate $K$ scenarios in a contextual setting. \hbox{Figure \ref{fig:fig1}} visually outlines this approach.


\begin{figure}
    \FIGURE
%    {\includegraphics{figure-filename.pdf}} %Callout External Image
%	Include LaTeX figures
{\includegraphics[width=\textwidth]{drawing-5-informs.eps}}
{Optimization with Scenario Generation \label{fig:fig1}}
{A standard contextual 2SP approach: (i) estimates the conditional distribution from the data, (ii) generates scenarios via sampling from the estimated distribution, (iii) reduces generated scenarios, and (iv) optimizes based on the reduced set of scenarios }
\end{figure}

A downside of the approach in Figure \ref{fig:fig1} occurs when the decision maker is faced with a context realization $\hat{\boldsymbol{x}}$ since the scenarios must be sampled from $p_{\theta}(\omega| \boldsymbol{x})$ and subsequently reduced. Generally, scenario reduction algorithms have computational cost that scales at least linearly in the number of sampled scenarios. The justification of this claim is contained in the overview of scenario generation methods.

In time-sensitive applications where solutions to (\ref{eq:Stage I}) are desired under different contexts, a mapping $\boldsymbol{f}: \boldsymbol{x} \mapsto \zeta_{1...K}$, that is cheaply evaluated is desirable so that one obtains first-stage decisions in a short amount of time. This process of mapping context to surrogate scenarios and ultimately to a decision is referred to as \textit{Contextual Scenario Generation (CSG)} and forms the base of the methodology proposed in this work. The mapping $\boldsymbol{f}$ is called the task-mapping. Ultimately, the success of CSG rests on the ability to select $\boldsymbol{f}$, such that solving (\ref{eq:2SP-SAA}) on the resulting scenarios yields high-quality decisions. The ultimate goal of this work is to develop a general methodology for constructing a high-quality $\boldsymbol{f}$ for general 2SPs, given that we have access to historical data. The proposed CSG approach is shown visually in Figure \ref{fig:fig2}. 

\begin{figure}
    \FIGURE
%    {\includegraphics{figure-filename.pdf}} %Callout External Image
%	Include LaTeX figures
{\includegraphics[width=0.9\textwidth]{drawing-6-informs.eps}}
{Contextual Scenario Generation and Optimization \label{fig:fig2}}
{Contextual Scenario Generation and Optimization: (i) aims to learn a mapping $\bs{f}$ from data such that $\bs{f}: \bs{x} \mapsto \zeta_{1...K}$, (ii) the predicted scenarios  $\bs{f}$ are close in distributional distance to $\mathbb{P}_{\omega | \bs{x}}$, and (iii) solving \ref{eq:Stage I} on $\bs{f}$ yields high-quality solutions according to the \ref{eq:Stage I} objective evaluated using $\mathbb{P}_{\omega|\bs{x}}$ }
\end{figure}

\subsection*{Contributions}
The main contributions of this work are as follows:

\begin{itemize}
    \setlength\itemsep{-0.0em}
    \item This work presents the first framework for solving two-stage stochastic programs in a contextual setting by learning a mapping to a set of surrogate scenarios from the context available to the decision maker. We propose a distributional approach that aims to produce surrogate scenarios that mimic the true conditional distribution, followed by a problem-driven approach that considers the cost of the first-stage solution obtained via solving \ref{eq:2SP-SAA} on the surrogate scenarios.
     \item The proposed methodology makes minimal assumptions about the structure of the 2SP and loosely only requires Assumptions \ref{a1}-\ref{a3}.
 
    \item Computational experiments are performed using four application problems to demonstrate the proposed methodology's ability to produce high-quality solutions in various application domains with differing problem structures. We illustrate the proposed approach using the newsvendor problem, CEP1, portfolio optimization, and a multidimensional newsvendor problem with customer-directed substitutions. 
\end{itemize}



\subsection*{Structure of the paper}
First, Section \ref{section:Literature} provides background on scenario generation in stochastic programming, learning-based approaches and their use in stochastic programming, and an overview of solving difficult optimization problems via surrogate optimization. \hbox{Section \ref{section:methodology}} introduces the proposed distributional and problem driven approaches. Section \ref{section:experiments} presents the experimental setup and results of the computational experiments. Lastly, Section \ref{section:conclusion} summarizes the results, contributions, and concludes. 

\section{Related Works}\label{section:Literature}


\subsection{Contextual Optimization for Stochastic Programming}
% Learning-based approaches for stochastic programming have experienced success in recent years. Specifically, neural network approaches have been leveraged to solve challenging stochastic programs in both one-off and contextual settings. 

%% Standard contextual approaches conditional-density-estimation-then-optimize

This section highlights works focused on contextual stochastic optimization that are most related to the proposed methodology. We refer the reader to \citet{sadana2024survey} for a more general survey.  Furthermore, we leverage the categorization of contextual stochastic optimization introduced by \citet{estes2023smart}. The first general approach, referred to as \textit{conditional-density-estimation-then-optimize} estimates $\mathbb{P}_{\omega|\bs{x}}$ then solves (\ref{eq:Stage I}) with the estimated conditional distribution. For example, in the case of 2SPs, \citet{ban2019dynamic} use residuals from the trained regression models to estimate conditional distributions, whereas \citet{bertsimas2020predictive} reweighs samples to approximate $\mathbb{P}_{\omega|\bs{x}}$. However, as pointed out in the introduction, this typically yields a difficult problem, motivating the generation a manageable number of scenarios. 

The second approach referred to as \textit{direct-solution-prediction}, aims to directly estimate a mapping from the context $\bs{x}$ to decisions $\bs{y}$ such that the decisions are of high quality, yielding a policy optimization problem. For example, in the case of 2SPs, \citet{yilmaz2023deep} formulate a multiagent actor critique approach to solving contextual two-stage knapsack problems where the agents predict solutions to the first and second stages, respectively. Although the solution-prediction approach is effective in tailored settings, it is difficult to deal with general integrality constraints and other complicating problem features \citep{patel2022neur2sp,zharmagambetov2023landscape}. 

The \textit{predict-then-optimize} approach generates a point prediction of uncertainty from the context and solves (\ref{eq:Stage I}) using a corresponding singleton distribution. Naive versions use standard loss functions (e.g., least-squares), while \textit{smart predict-then-optimize} selects the predictor based on decision performance. For linear programs, \citet{elmachtoub2021smart} minimize decision regret with a surrogate loss function. \citet{estes2023smart} apply similar methods to linear 2SPs. Other approaches update models using gradients of downstream performance \citep{agrawal2019differentiable} but struggle with uninformative gradients \citep{grigas2021integrated, zharmagambetov2023landscape}. To address this, \citet{zharmagambetov2023landscape} propose modelling a surrogate loss function via neural networks. However, early iterations often lead to predictors whose predictions lie outside the region where the model accurately captures the loss, resulting in what we call \textit{loss error maximization}. Moreover, point predictions implicitly assume perfect knowledge, leading to decisions poorly hedged against uncertainty \citep{king2012modeling}.

The proposed CSG approach aims to address these concerns in the general setting of a large class of 2SPs, circumventing computational concerns by generating a small subset of scenarios and subsequently solving (\ref{eq:2SP-SAA}). This avoids feasibility issues commonly encountered in the direct-solution-prediction approach. Furthermore, the approach inherits all the problem-class generality of \citet{zharmagambetov2023landscape}'s approach while addressing the issue of loss error maximization. Lastly, CSG produces solutions that hedge against uncertainty by considering the surrogate scenarios as opposed to a single prediction.


\subsection{Scenario Generation}

Scenario generation techniques can be categorized into distributional and problem-based approaches. The distributional approach generates surrogate scenarios to mimic some features of the underlying distribution. This aim is typically achieved by minimizing a distributional distance \citep{dupavcova2003scenario} or by matching moments \citep{hoyland2003heuristic} between the scenarios and the underlying distribution. In contrast, problem-driven scenario generation methods account for problem-specific structures so that the resulting scenarios yield high-quality solutions when evaluated using the underlying distribution. Some problem driven approaches include: clustering scenarios based on objective values of candidate solutions \citep{bertsimas2023optimization}, identifying irrelevant scenarios based on the objective \citep{fairbrother2022tail}, and approximating the recourse function of a pool of candidate solutions \citep{narum2024problem}. In our setting, the scenario generation methods mentioned assume access to the conditional distribution or a sufficiently accurate scenario representation with enough samples $M$. One can estimate the conditional distribution as in the conditional-density-estimation-then-optimize approach; however, the generic problem-driven approaches depend at least linearly on $M$. In contrast, besides optimizing implementation error, the proposed CSG approach generates surrogate scenarios via a neural network forward pass, eliminating dependence on $M$ at decision time.




\subsection{Learning-based Approaches for Stochastic Programs}

Machine learning has been applied to solve stochastic programs by predicting solutions or costs. In particular, approaches that leverage machine learning to predict the recourse cost are most relevant to this work. For example, \citet{patel2022neur2sp}, \citet{lee2023value} and \citet{bae2023deep} predict expected recourse in generic 2SPs, leveraging these predictions in the solution of 2SPs. However, these methods do not directly address scenario generation. Although, some works apply machine learning to scenario generation. \citet{bengio2020learning} use regression to predict a single representative scenario for 2SP such that the scenario minimizes implementation error, but their method relies on heuristically generated datasets and doesn't handle multiple scenarios. \citet{wu2022learning} employ semi-supervised learning and conditional variational auto-encoders to generate scenario embeddings that align with the optimal expected cost of 2SP based on a subset of solved instances. Unlike their approach, which doesn't explicitly minimize implementation error, this work does not require the ability to solve large-scale 2SPs and instead assumes the ability to solve 2SPs on up to $K$ scenarios.






