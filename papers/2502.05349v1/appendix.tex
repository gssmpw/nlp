



\section{Comparing Conditional Distributions}\label{appendix:comparing}
In general, MMD distances differ from Wasserstein distances in several key regards. Computing Wasserstein metrics between two samples of size $n$ requires $O(\log(n)n^3)$ operations, whereas MMD requires $n^2$ operations \citep{pele2009fast}. Additionally, Wasserstein metrics have dimension-dependent sample complexity and require more samples in higher dimensional settings to bound the gap between the sampled distance and the true distance \citep{genevay2019sample}. Wasserstein also suffers from biased sample gradients \citep{bellemare2017cramer} unlike MMD. However, MMD induces a geometry on the space of probability measures that does not respect the distance metric on the underlying space. \citet{feydy2019interpolating} use Sinkhorn divergence: an entropic regularization of the Wasserstein distance with bias removed. They show that the Sinkhorn divergence interpolates between Wasserstein and MMD of the Laplacian RKHS based on the amount of regularization. The interpolation property also applies to complexities, with Sinkhorn achieving $O(n^2)$ and $O(1/\sqrt{n})$ computational and sample complexity, respectively \citep{genevay2019sample}. However, computing Sinkhorn divergence takes approximately 20-50 times as long as computing MMD \citep{feydy2019interpolating}. 

\citet{ren2016conditional} and \citet{park2020measure} consider conditional versions of the MMD metric that only require joint sampling access,
however, \citet{huang2022evaluating} points out that optimizing these metrics via batched gradients requires matrix inversions that scale with the batch size. Some authors consider $d^2_{\mathcal{F}_\text{MMD}}(\mathbb{P}_{\boldsymbol{x}, \omega}, \mathbb{P}_{\boldsymbol{x}, \eta})$ to infer a relationship between $\mathbb{P}_{\omega|\boldsymbol{x}}$ and $\mathbb{P}_{\eta|\boldsymbol{x}}$. For instance, \citet{huang2022evaluating} (Theorem 6) claim, under suitable assumptions (measurability, boundedness in expectation and characteristic kernels) that $d^2_{\mathcal{F}_\text{MMD}}(\mathbb{P}_{\boldsymbol{x}, \omega}, \mathbb{P}_{\boldsymbol{x}, \eta}) = 0 \iff \mathbb{P}_{\omega|\boldsymbol{x}} = \mathbb{P}_{\eta|\boldsymbol{x}}$ almost everywhere according to $\mathbb{P}_{\boldsymbol{x}}$. In the case of the energy distance, \citet{hagemann2023posterior} bounds $\mathbb{E}_{\boldsymbol{x} \sim \mathbb{P}_{\boldsymbol{x}}} \left[ d^2_{\mathcal{F}_\text{MMD}}\left( \mathbb{P}_{\omega |\boldsymbol{x}}, \mathbb{P}_{\eta |\boldsymbol{x}} \right) \right]$ by $d^2_{\mathcal{F}_\text{MMD}}(\mathbb{P}_{\boldsymbol{x}, \omega}, \mathbb{P}_{\boldsymbol{x}, \eta})^{\frac{1}{4(1+p+d)}}$ under regularity and compactness assumptions. \citet{chemseddine2023diagonal} show the opposite relationship holds for the 1-Wasserstein metric and consequently introduce a Wasserstein distance of order $q$, conditioned on $\boldsymbol{x}$, denoted by $d_{q, \boldsymbol{x}}$ that satisfies $\mathbb{E}_{\boldsymbol{x} \sim \mathbb{P}_{\boldsymbol{x}}} \left[ d_q^q \left( \mathbb{P}_{\omega |\boldsymbol{x}}, \mathbb{P}_{\eta |\boldsymbol{x}} \right) \right] = d_{q, \boldsymbol{x}}^q(\mathbb{P}_{\boldsymbol{x}, \omega}, \mathbb{P}_{\boldsymbol{x}, \eta})$, where $d_q^q(.,.)$ denotes the $q$-Wasserstein distance raised to the power $q$. In practice, \citet{chemseddine2023diagonal} implement their conditional distance by penalizing transport cost associated with $\boldsymbol{x}$ and use \citet{feydy2019interpolating}'s debiased Sinkhorn approach to compute the distance.

\section{Proof of Proposition \ref{prop:representation}}\label{appendx:prop2}
In this section, we provide proof of Proposition \ref{prop:representation}. The proof largely relies on Proposition 1 due to \citet{tabaghi2024universal}. First, we introduce some notation, followed by Proposition 1 by \citet{tabaghi2024universal} and the proof of Proposition \ref{prop:representation}. 

\subsection*{Notation} A multiset is a pair $(U, \nu)$ where $U$ denotes a set of objects and $\nu$ maps $U$ to the non-negative integers, where $\nu(u)$ represents the cardinality of object $u \in U$. Multisets are denoted by double brackets $\{\{ \}\}$. For example, the multiset $\{\{3, 3, 4\}\}$ has three elements with $\nu(3) = 2$ and $\nu(4) = 1$. For any domain $\Omega$, a multiset $U$ such that $U \subseteq \Omega$ means that the elements of $U$ are in $\Omega$. The cardinality of $U$, denoted by $|U|$ is the number of elements in $U$, counting repetitions, e.g. $|\{\{3, 3, 4\}\}| = 3$. For some $N$ in the positive integers $\mathbb{N}$ and domain $\Omega$, the following sets are introduced
$$
\mathbb{U}_{\Omega, N} = \{ \text{multiset} \ U \subseteq \Omega: |U| = N \}\ 
\text{and}\ 
\mathbb{U}_{\Omega, [N]} = \{ \text{multiset} \  U \subseteq \Omega: |U| \in [N] \}.
$$
\subsection*{\citet{tabaghi2024universal}'s Proposition 1} \citet{tabaghi2024universal} prove the following proposition regarding functions of multisets, which we repeat here for the reader's convenience. 

\begin{proposition}[Proposition 1 from (Tabaghi and Wang 2024)]\label{cor:two_multisets}
    \ \\ A (continuous) multiset function $f:\mathbb{U}_{\Omega,[N_1]} \times \mathbb{U}_{\Omega,[N_2]} \rightarrow \text{codom}(f)$, where $\Omega$ is a compact subset of $\ \mathbb{R}^{p}$, is (continuously) sum-decomposable via $\mathbb{R}^{{N+p \choose p}-1} \times \mathbb{R}^{{N+p \choose p}-1}$, that is, 
    \[
         f(U, U^{\prime}) = \tilde{\rho} \left( \sum_{x \in U} \tau(x), \sum_{x^{\prime} \in U^{\prime}} \tau(x^{\prime}) \right) \quad \forall U \in \mathbb{U}_{\Omega,[N_1]}, U^{\prime} \in \mathbb{U}_{\Omega,[N_2]} \
    \]
    where continuous $\tau: \mathbb{R}^{p} \rightarrow \mathbb{R}^{{N+p \choose p}-1}, N = \max \{N_1, N_2 \}$ and (continuous) $\tilde{\rho}: \mathbb{R}^{{N+p \choose p}-1} \times \mathbb{R}^{{N+p \choose p}-1} \rightarrow \text{codom}(\tilde{\rho})$, and $\text{codom}(f) \subset \text{codom}(\tilde{\rho})$.
\end{proposition}

\subsection*{Proof of Proposition \ref{prop:representation}}
% For the reader's convenience, we repeat Proposition \ref{prop:representation}, followed by its proof.
% \representation*
\begin{proof}
First, fix a value of $K \in \mathbb{N}$. We can identify $\ell_{\text{opt}}$ as a multiset function. Since the value $\ell_{\text{opt}}(\zeta_{1...K^{\prime}}, \omega)$ does not depend on the order of the $K^{\prime} \in \mathbb{N}$ scenarios $\zeta_{1...K^{\prime}}$, we can define the multisets $U_{\zeta_{1...K^{\prime}}} = \{\{ \zeta_k \}\}_{k = 1}^{K^{\prime}} \in \mathbb{U}_{\Omega, K^{\prime}}$ and $U_{\omega} = \{\{ \omega \}\} \in \mathbb{U}_{\Omega, 1}$ and define $\ell_{\text{opt}}^{\prime} : \mathbb{U}_{\Omega, [K^{\prime}]} \times \mathbb{U}_{\Omega, 1} \rightarrow \text{codom}(\ell_{\text{opt}})$ such that
$$
\ell_{\text{opt}}^{\prime} \left( U_{\zeta_{1...K^{\prime}}}, U_{\omega} \right) = \ell_{\text{opt}}(\zeta_{1...K^{\prime}}, \omega) \quad \forall \zeta_{1...K^{\prime}} \in \Omega^{K^{\prime}}, \omega \in \Omega, \ \forall K^{\prime} \in [K]. 
$$
The value of $\ell_{\text{opt}}^{\prime} \left( U_{\zeta_{1...K^{\prime}}}, U_{\omega} \right)$ is calculated by solving ($\zeta$-SAA) on the multiset of scenarios $U_{\zeta_{1...K^{\prime}}}$ and optimistically evaluating the resulting solutions on the singleton-multiset $U_{\omega}$. Applying Proposition 1 from \citep{tabaghi2024universal}, to $\ell_{\text{opt}}^{\prime}$ implies 
\begin{equation} \label{eqn:useful_equation}
    \ell_{\text{opt}}^{\prime} \left( U, U^{\prime} \right) = \tilde{\rho} \left( \sum_{u \in U} \tau(u), \sum_{u^{\prime} \in U^{\prime}} \tau(u^{\prime}) \right) \quad \forall U \in \mathbb{U}_{\Omega,[K]}, U^{\prime} \in \mathbb{U}_{\Omega,[1]}, 
\end{equation}
where $N = \max \{K, 1 \} = K$, continuous $\tau: \mathbb{R}^{p} \rightarrow \mathbb{R}^{{K+p \choose p}-1}$, (continuous) $\tilde{\rho}: \mathbb{R}^{{K+p \choose p}-1} \times \mathbb{R}^{{K+p \choose p}-1} \rightarrow \text{codom}(\tilde{\rho})$, and $\text{codom}(\ell_{\text{opt}}^{\prime}) \subset \text{codom}(\tilde{\rho})$.
Since (\ref{eqn:useful_equation}) holds for all $U \in \mathbb{U}_{\Omega,[K]}$,  it also holds for all $U \in \mathbb{U}_{\Omega,K}$, implying 
\begin{align*}
\ell_{\text{opt}}(\zeta_{1...K}, \omega)  &= \tilde{\rho} \left( \sum_{\zeta \in U_{\zeta_{1...K}}} \tau(\zeta), \sum_{\omega \in U_{\omega}} \tau(\omega) \right) \\
&= \tilde{\rho} \left( \sum_{k = 1}^K \tau(\zeta_k),  \tau(\omega) \right) \quad \forall \zeta_{1...K} \in \Omega^{K}, \omega \in \Omega.
\end{align*}
Lastly, letting $\rho : \mathbb{R}^{{K+p \choose p}-1} \times \mathbb{R}^{{K+p \choose p}-1} \rightarrow \text{codom}(\tilde{\rho})$ be defined such that
$$
\rho(\hat{\zeta}, \hat{\omega}) = \tilde{\rho}(K \hat{\zeta}, \hat{\omega}) \quad \forall \hat{\zeta} \in \mathbb{R}^{{K+p \choose p}-1},  \hat{\omega} \in \mathbb{R}^{{K+p \choose p}-1},
$$
yields the desired result. 
\end{proof}
\section{Experimental Information}

\subsection{Experimental Tools}\label{appendix:experimental_tooling}
The mathematical programming models are implemented in Python 3.7 and solved using Gurobi 11.0 on \textsf{Google Colab} equipped with an Intel(R) Xeon(R) CPU 2.20GHz, 51 gigabytes of random access memory (RAM) and a Tesla T4 GPU. All neural networks are trained using \textsf{skorch} \citep{skorch}. 

\subsection{CEP1 Experimental Supplement}\label{appendix:cep1_formulation_data}
\subsubsection{CEP1 Formulation} \label{appendix:cep1_formulation}
% Recall in CEP1, there are $m$ parts being produced on $n_{\text{machines}}$. Machine $j \in [n_{\text{machines}}]$ is available for $h_j$ hours per week, with the ability to purchase additional hours at an hourly cost of $c_j$. 
 Based on the CEP1 problem description, the decision-maker wishes to solve
    \begin{align}\label{eq:cep1}
    \min_{\bs{y}}\quad   \bs{c}^{\intercal} \bs{y}_{\text{cap}} +  \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}}& \left[ Q(\bs{y}_{\text{op}} , \omega)\right]  \tag{CEP1}  \\ \textit{s.t.}  \quad  -y_{\text{cap}, j} + y_{\text{op},j} & \leq h_j  & \forall j \in [n_{\text{machines}}]  \notag \\
    \quad  \bs{t}^{\intercal} \bs{y}_{\text{op}} & \leq T \notag \\
    \quad  \bs{0} \leq \bs{y}_{\text{cap}},\ & \bs{0} \leq \bs{y}_{\text{op}} \leq \bs{u}, \notag
    \end{align}
where $\bs{y}_{\text{cap}}$ and $\bs{y}_{\text{op}}$ denote the hours of added capacity and total operation of the machines, respectively. $\bs{c} \in \mathbb{R}_+^{n_{\text{machines}}}$ denotes the capacity cost vector, $\bs{h} \in \mathbb{R}_+^{n_{\text{machines}}}$ denotes the baseline capacities of the machines, $\bs{t} \in \mathbb{R}_+^{n_{\text{machines}}}$ is the maintenance requirements incurred per hour for all the machines, $T$ is the total maintenance limit, and $\bs{u} \in \mathbb{R}_+^{n_{\text{machines}}}$ denotes the upper bounds on utilization for each machine. The recourse cost is given by
    \begin{align}\label{eq:cep1stage2}
    Q(\bs{y}_{\text{op}}, \omega) = \min_{\bs{z}, \bs{s}} \quad   \sum_{i=1}^m \sum_{j=1}^{n_{\text{machines}}} g_{ij} z_{ij} &+ \sum_{i=1}^m p_i s_i  \tag{CEP1-Stage II} \\ \textit{s.t.}  \quad  \sum_{j=1}^{n_{\text{machines}}} a_{ij} z_{ij} +s_i & \geq \omega_i  & \forall i \in [m]  \notag \\
    \quad  \sum_{i=1}^m z_{ij} & \leq y_{\text{op}, j} & \forall j \in [n_{\text{machines}}] \notag \\
    \quad  z_{ij} & \geq 0 & \forall i \in [m], \ j \in [n_{\text{machines}}]  \notag \\
    \quad s_i & \geq 0 & \forall i \in [m], \notag
    \end{align} 
 where $z_{ij}$ and $s_i$ denote the hours producing part $i$ on machine $j$ and the production shortfall of part $i$ respectively. The uncertain demand for part $i$ is $\omega_i$. Machine $j$ produces part $i$ at a rate of $a_{ij} \geq 0$ with an hourly cost of $g_{ij} \geq 0$. Production shortfalls incur a penalty of $p_i \geq 0$. We use the same deterministic problem data as defined by \citep{higle_sen_1996} with $n_{\text{machines}} = 4$ and $m = 3$. The static data below is used to define the CEP1 instance considered in the manuscript. 
\[
n = 4, \quad m = 3, \quad T = 100
\]
\[
\bs{c} = (2.5, 3.75, 5.0, 3.0), \quad \bs{t} = (0.08, 0.04, 0.03, 0.01)
\]
\[
\bs{h} = (500, 500, 500, 500), \quad \bs{u} = (2000, 2000, 3000, 3000)
\]
\[
\bs{p} = (400, 400, 400)
\]
\[
[a_{ij}] = \begin{bmatrix}
0.6 & 0.6 & 0.9 & 0.8 \\
0.1 & 0.9 & 0.6 & 0.8 \\
2.6 & 3.4 & 3.4 & 0.9
\end{bmatrix}, \quad 
[q_{ij}] = \begin{bmatrix}
1.5 & 2.3 & 2.0 & 3.6 \\
0.05 & 0.2 & 0.5 & 0.8
\end{bmatrix}
\]
\subsubsection{CEP1 Stochastic Modelling} \label{appendix:cep1_data_generation}
\citet{higle_sen_1996} originally consider $\omega_i$, $ i \in \{1,2,3\}$ as an iid discrete random variable with $\omega_i \in \{0, 600, 1200, 1800, 2400, 3000\}$ being equally likely. We construct our contextual setting similarly. The contextual information $\bs{x}$ is distributed according to a four-dimensional normal distribution $N(\bs{0}, \bs{I})$. A neural network $\bs{f}_{\text{Random}} : \mathcal{X} \rightarrow \mathbb{R}^{6 \times m}$ is randomly initialized. For part $i$, the $i$th column of $\bs{f}_{\text{Random}}(\bs{x})$ corresponds to the $6$ points defining the support of $\omega_i$. Since each demand $\omega_i$ is supported on 6 points, there are 216 possibilities defining the demand distribution conditional on the context. Scaling is applied to the outputs from $\bs{f}_{\text{Random}}$ such that the demands expectation and standard deviation are $1500$ and $1024.7$, respectively. Any negative demand values are clipped to 0. This process defines $\mathbb{P}_{\omega | \bs{x}}$. 


\subsubsection{CEP1 Optimality Gap CDFs}\label{appendix:cep1_performance}
\begin{figure}[h]
    \FIGURE
    {\includegraphics[width=0.97\textwidth]{Performance_CEP1.eps}}
    {Out of sample optimality gaps for the contextual CEP1 problem.\label{fig:cep1_gaps}}
    {}
\end{figure}

\newpage 
\subsection{CVaR Experimental Supplement}
\subsubsection{CVaR Formulation} \label{appendix:cvar_formulation}
We consider a trader who wishes to balance the trade-off between risk and expected return. CVaR is used as the risk measure, and we employ the formulations introduced by \citet{krokhmal2002portfolio} to model the trader's problem. The trader wishes to solve
 \begin{align}\label{eq:CVaR}
 \min_{\bs{y}, \gamma, \bs{t}}\quad   - \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}}[\omega]^{\intercal} \bs{y}  
        &+  \lambda_{\text{risk}} \left( \gamma + \frac{1}{1-\alpha} \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}} \left[ Q(\bs{y} , \gamma,  \omega)\right] \right) \tag{CVaR} \\
 \textit{s.t.} \quad  &\bs{1}^{\intercal} \bs{y} = 1, \notag \\
        &y_i \leq t_i \quad \forall i \in [n_{\text{assets}}],  \notag \\
        &\bs{1}^{\intercal} \bs{t} \leq K_{\text{assets}}, \notag  \\
        &\bs{y} \in \mathbb{R}_+^{n_{\text{assets}}},\ \gamma \in \mathbb{R}, \ \bs{t} \in \{0,1\}^{n_{\text{assets}}}, \notag
 \end{align} 
 where $\bs{y}$, $\gamma$, and $\bs{w}$ represent the selected portfolio, a placeholder variable that at optimality equals the portfolio's value at risk (VaR) according to $\mathbb{P}_{\omega | \bs{x}}$, and the binary indicator variables that model $w_i = 0 \implies y_i = 0, \ \forall i \in [n_{\text{assets}}]$, respectively. The variables $\bs{y}$, $\gamma$, and $\bs{w}$ constitute the first-stage decisions. The relative risk aversion and VaR confidence level are denoted by $\lambda_{\text{risk}} > 0$ and $\alpha \in (0,1)$, respectively. The $n_{\text{assets}}$ random returns for each asset are represented by $\omega$. Lastly, the recourse cost is given by
 \begin{align}\label{eq:CVaRstage2} \tag{CVaR}
 Q(\bs{y}, \gamma,  \omega) = \min_{z} \quad  z \notag  \\ \textit{s.t.}  \quad  z & \geq -\omega^{\intercal} \bs{y} - \gamma  \notag \\
 \quad  z & \geq 0, \notag
 \end{align}
where $z$ is a variable that models the portfolio loss exceeding the value at risk. We consider the following instance parameters $\lambda_{\text{risk}} = 10$, $n_{\text{assets}} = 10$, $K_{\text{assets}} = 5$, and $\alpha = 0.9$.


\subsubsection{CVaR Stochastic Modelling} \label{appendix:cvar_data_generation}

A data-driven approach is considered to set up the contextual environment. The decision-maker limits the selection to tickers in the S\&P 500 as of December 29, 2023, and chooses the \(n_{\text{assets}}\) tickers with the smallest traded volumes as their asset universe. They desire the ability to be able to obtain portfolios that are of high quality according to (\ref{eq:CVaR}) every 5 minutes for hedging purposes. The context window $W$ is set to $78$ periods (a full trading session). Furthermore, as discussed in the text, the trader assumes that the returns satisfy the following relationship 
\begin{equation}\label{eq:appendix_returns_law}
\omega = \Phi_{\boldsymbol{\mu}}(\boldsymbol{x}) + \Phi_{\sigma} (\boldsymbol{x}) \odot \boldsymbol{\epsilon}
\end{equation}
where $\Phi_{\boldsymbol{\mu}} : \mathcal{X} \rightarrow \mathbb{R}^{n_{\text{assets}}}$ and $\Phi_{\sigma} : \mathcal{X} \rightarrow \mathbb{R}_+^{n_{\text{assets}}}$ are models given to the trader by a statistical modeler to estimate the conditional mean $\mathbb{E}[\omega |\bs{x}]$, and conditional standard deviation $\sqrt{\text{Var}[\omega | \bs{x}]}$ along with $\boldsymbol{\epsilon}$ denoting the random errors. There is a large dataset $S^* = \{\bs{x}^{(i)}, \omega^{(i)} \}_{i=1}^{n^*}$ of size $n^*$ used to produce $\Phi_{\boldsymbol{\mu}}$ and $\Phi_{\sigma} (\boldsymbol{x})$. Similar to \citet{deng2022predictive} and \citet{ban2019dynamic}, the residuals obtained from the trained models define the conditional distribution. That is, $\bs{\epsilon}$ is assumed to be uniformly distributed over $\{\bs{\epsilon}^{(i)}\}_{i=1}^{n^*}$ where 
\begin{equation}\label{eq:residuals}
\bs{\epsilon}^{(i)} = \left(\omega^{(i)} - \Phi_{\boldsymbol{\mu}}(\boldsymbol{x}^{(i)}) \right) \odot \begin{pmatrix}
    1/\Phi_{\sigma, 1} (\boldsymbol{x}^{(i)}) \\
    1/\Phi_{\sigma, 2} (\boldsymbol{x}^{(i)}) \\
    \vdots \\
    1/\Phi_{\sigma, n_{\text{assets}}} (\boldsymbol{x}^{(i)})
    \end{pmatrix}, \quad \forall i \in [n^*].
\end{equation}
Thus, the trader aims to solve (\ref{eq:CVaR}) with $\mathbb{P}_{\omega|\bs{x}}$ defined by (\ref{eq:appendix_returns_law}) and \hbox{$\bs{\epsilon} \sim \text{uniform} \{\bs{\epsilon}^{(i)}\}_{i=1}^{n^*}$}. 

A dataset of prices consisting of $21$ trading periods is gathered for each ticker via the IEX endpoint available via the Tiingo API, resulting in $1638$ returns $\bs{r}^{(j)} \in \mathbb{R}^{n_{\text{assets}}}, \ j = 1,\hdots, 1638$. The sample $S^*$ used to train $\Phi_{\bs{\mu}}$ and $\Phi_{\sigma}$ is formed as follows
\[
\bs{x}^{(i)} = \begin{pmatrix}
 \bs{r}^{(i)} & \dots & \bs{r}^{(i+W)} \\
    & (\textsf{PE}^{(1)}_{i \hdots i+W})^{\intercal} & \\
    & \vdots & \\
    & (\textsf{PE}^{(n_{\text{encoding}})}_{i \hdots i+W})^{\intercal}&
 \end{pmatrix}\  \text{and} \ \omega^{(i)} = \bs{r}^{(i+W+1)}, \ i = 1,\hdots,  n^* \coloneq 1559,
\]
where $\textsf{PE}^{(j)}_{i \hdots i+W} \in \mathbb{R}^{W}$ is the $j$th sinusoidal positional encoding of the period associated with indices $i,\hdots, W+i$ (the period of the $i$th datum is given by $i$ modulo $78$) \citep{vaswani2017attention}. We design $\Phi_{\boldsymbol{\mu}}$ and $\Phi_{\boldsymbol{\sigma}}$ such that the resulting distribution can generate the dataset $S^*$ as a sample. Specifically,  $\Phi_{\boldsymbol{\mu}}$ and $\Phi_{\boldsymbol{\sigma}}$ are parameterized by long short-term memory (LSTM) neural networks. The parameters of these networks are selected via random search, using a $20\%$ holdout for validation.

The networks $\Phi_{\boldsymbol{\mu}}$ and $\Phi_{\sigma}$ are fit in a two-step training procedure. First we train $\Phi_{\boldsymbol{\mu}}$ by optimizing \hbox{$\frac{1}{n}\sum_{i=1}^n|| \hat{\boldsymbol{r}}_i - \Phi_{\boldsymbol{\mu}}(\boldsymbol{x}_i)||_1$}. Then the vectors of residuals $\hat{\boldsymbol{\eta}}_i = |\hat{\boldsymbol{r}}_i - \hat{\Phi}_{\boldsymbol{\mu}}(\boldsymbol{x}_i)|$ are formed and $\Phi_{\sigma}$ is obtained by minimizing \hbox{$\frac{1}{n}\sum_{i=1}^n|| \hat{\boldsymbol{\eta}}_i - \Phi_{\sigma}(\boldsymbol{x}_i)||_1$}. Lastly equation (\ref{eq:residuals}) yields $\{\bs{\epsilon}^{(i)}\}_{i=1}^{n^*}$.


% \subsubsection{CVaR Results} \label{appendix:CVaR_results} 

\subsubsection{CVaR Optimality Gap CDFs}\label{appendix:cvar_performance}

    \begin{figure}[H]
        \FIGURE
        {\rotatebox{90}{\includegraphics[width=0.56\paperheight]{CVaR1.eps}}}
        {Out of sample optimality gaps for the contextual CVaR problem \label{fig:cvar_gaps1}} {Figure continued on next page}
    \end{figure}

    \begin{figure}[H]
            \FIGURE
            {\rotatebox{90}{\includegraphics[width=0.56\paperheight]{CVaR2.eps}}}
            {Out of sample optimality gaps for the contextual CVaR problem \label{fig:cvar_gaps2}} {}
        \end{figure}



\subsection{MNV Experimental Supplement}\label{appendix:MNV_data_formulation}

\subsubsection{MNV Formulation}\label{appendix:MNV_formulation}
This section presents the MNV problem as considered by \citet{narum2024problem}. There are $m$ products available for production. The decision maker wishes to select a production plan to produce at most $P$ items while respecting the overall production capacity $C$. They wish to solve
        \begin{align}\label{eq:mnv_firststage} 
        \max_{\bs{y}, \bs{t}} \quad \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}}\left[ Q(y, \omega) \right] - \sum_{i=1}^m c_i y_i  \tag{MNV} \\
        \textit{s.t.} \quad \sum_{i=1}^m y_i &\leq C  \notag \\
        \quad \sum_{i=1}^m t_i &\leq P  \notag \\
        \quad y_i &\leq M t_i & \forall i \in [m]  \notag \\
        \quad \bs{y} \in \mathbb{R}^m_+, & \ \bs{t} \in \{0, 1\}^m, \notag
        \end{align}
    where \(y_i\) represents the amount of product $i$ produced, \(t_i\) represents the binary decision indicating that product $i$ is eligible for production, and \(c_i\) is the product $i$ production cost. 

    The recourse profit is the total of the salvage values and the sales values, which come from both direct sales and customer-driven substitutions. The recourse profit is given by
        \begin{align}\label{eq:mnv_secondstage}
        Q(y, \omega) = \max_{\bs{s}, \bs{z}, \bs{\bar{z}}, \bs{w}, \bs{\hat{z}}} \quad \sum_{i=1}^m \left( v_i s_i + v_i \bar{z}_i + g_i w_i \right)  \tag{MNV-Stage II} \\
        \textit{s.t.} \quad s_i + \sum_{j \in [m] :j \neq i} z_{ji} &\leq \omega_i & \forall i \in [m]  \notag \\
        \quad z_{ij} &\leq \alpha_{ij} \left( \omega_j - s_j \right) & \forall i \in [m], j \in [m], i \neq j  \notag \\
        \quad \bar{z}_i &= \sum_{j \in [m]:j \neq i} z_{ij} & \forall i \in [m]  \notag \\
        \quad M \left( \hat{z}_j - 1 \right) &\leq s_j - y_j & \forall j \in [m]  \notag \\
        \quad z_{ij} &\leq M \hat{z}_j & \forall i \in [m], j \in [m]  \notag \\
        \quad w_i &= y_i - \left( s_i + \bar{z}_i \right) & \forall i \in [m]  \notag \\
        \quad \hat{z}_{ij} &\in \{0, 1\} &\ \forall i \in [m], j \in [m],  i \neq j \notag \\
        \quad \bs{s}, \bs{z}, \bs{\bar{z}}, \bs{w} &\in \mathbb{R}^m_+, \notag
        \end{align} 
where \(s_i\) represents the sales of product $i$, \(z_{ij}\), is the substitution sale amount of item $i$ to satisfy demand for item $j$, \(\bar{z}_i\) and the total amount of item $i$ substituted, \(w_i\) represents unsold inventory that is salvaged, and $\hat{z}_j$ is a binary variable indicating whether to start substitution sales satisfying demand for item $j$. 

 In the objective, parameters $v_i$ and $g_i$ denote the sales price and salve value of item $i$. The substitution rate $\alpha_{ij}$ average probability that item $j$ can be replaced by item $i$. $\alpha_{ij}$ is not necessarily symmetric, e.g., pink t-shirts can be substituted with white t-shirts more often than white shirts can with pink. The big-$M$s are set to be the capacity $C$. Lastly, $\omega \in \mathbb{R}^m_+$ denotes the uncertain demand for the $m$ products. We consider a setting with $m = 6$ potential products. The static data below is used to define the MNV instance considered in the manuscript. 
\[
\boldsymbol{v} = \left( 
46.111, \ 
44.691, \ 
46.448, \ 
48.406, \ 
44.476, \ 
44.476 
\right), 
\]
\[
\boldsymbol{c} = \left( 
18.980, \ 
25.618, \ 
24.163, \ 
26.217, \ 
17.974, \ 
26.418 
\right),
\]
\[
\boldsymbol{g} = \left( 
9.830, \ 
5.094, \ 
5.067, \ 
5.293, \ 
5.723, \ 
7.292 
\right)
\]
\[
[\gamma_{ij}] = \begin{bmatrix}
- & 0.087 & 0.184 & 0.042 & 0.088 & 0.110 \\
0.137 & - & 0.060 & 0.154 & 0.178 & 0.014 \\
0.182 & 0.051 & - & 0.285 & 0.290 & 0.243 \\
0.091 & 0.029 & 0.205 & - & 0.037 & 0.149 \\
0.010 & 0.273 & 0.078 & 0.199 & - & 0.156 \\
0.164 & 0.055 & 0.291 & 0.233 & 0.282 & -
\end{bmatrix},
\]
along with the constants $C = 70$ and  $P = 3$.

\subsubsection{MNV Stochastic Modelling}\label{appendix:MNV_data}

In the case of fashion retail, \citet{vaagen2011modelling} point out that demand is multi-modal with strong dependence. In light of this \citet{narum2024problem} model demand with a multivariate mixture distribution with binary stochastic variables to determine the regime for the product and two Normal distributions for the specific demand that large or small mean depending on the regime. We reflect these considerations in the contextual setup for the problem, which is as follows. 

The context $\bs{x} \in \mathbb{R}^p$ is assumed to be Normally distributed $N(\bs{0}, \bs{\Sigma})$ where $p=10$ and $\Sigma_{ij} = \rho^{|i - j|}$ with $\rho = 0.8$. We randomly initialize a matrix $\bs{W} \in \mathbb{R}^{m \times p}$ where the elements of $\bs{W}$ come from a standard Normal distribution. The random demand state $Z \in \{-1,1\}^{m}$ is given by $\text{sign}\left[\bs{W} \bs{x} + \bs{\epsilon}\right]$, where $\bs{\epsilon} \in \mathbb{R}^m$ represents the idiosyncratic demand uncertainty for each product. $\bs{\epsilon}$ is assumed to be multi-variate Normal $N(\bs{0}, \sigma \bs{I})$, where $\sigma$ is set such that the signal-to-noise ratio is $0.5$. Based on product $i$'s demand state $Z_i$, the demand $\omega_i$ is distributed according to $N(\mu_{Z_i}, \sigma^2_{Z_i})$. We set $\mu_{-1} = 5$, $\mu_{+1} = 15$, $\sigma_{-1} = 5/3$, and $\sigma_{+1} = 5/2$. The sampled demand is replaced with $0$ if it is negative. To define $\mathbb{P}_{\omega | \bs{x}}$ we sample $80$ observations via the aforementioned process. 


\newpage
\subsubsection{MNV Optimality Gap CDFs}\label{appendix:MNV_performance}
\ \ 
\begin{figure}[H]
\FIGURE
    {\includegraphics[width=0.99\textwidth]{Performance_MNV.eps}}
    {Out of sample optimality gaps for the contextual MNV problem \label{fig:MNV_gaps}}
    {}
\end{figure}



\subsection{Timing analysis }\label{appendix:timing}



Tables \ref{tab:CVaR_times} and \ref{tab:MNV_times} show the aforementioned metrics by $K$. Results for the newsvendor and CEP1 instances are not shown since directly solving the 2SPs is not as computationally challenging. We focus on the CVaR and MNV problems since the CVaR problem considers 1559 support points to define $\mathbb{P}_{\omega | \bs{x}}$ and MNV is complicated by mixed-binary variables in both stages. We do not show the results by $\lambda$ as it did not significantly impact timing. Furthermore, we do not show the time to compute solutions via the task nets obtained from static and dynamic approaches since they did not vary relative to the MMD task net (as their architecture is the same). 


\begin{table}[h]
    \TABLE 
    {Computation times (in seconds) for contextual CVaR  \label{tab:CVaR_times} }
    {
    \begin{tabular}{cccccccc}
    \hline
    $K$ & \begin{tabular}[c]{@{}c@{}}MMD \\ Training\end{tabular} & \begin{tabular}[c]{@{}c@{}}Surrogate Solution \\ Calculation (MMD)\end{tabular} & \begin{tabular}[c]{@{}c@{}} 2SP Solve \\ $n_{\text{eval}} = 312$ \end{tabular} & \begin{tabular}[c]{@{}c@{}}MMD Loss\\ Evaluation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Loss-Net \\ Training\end{tabular} & \begin{tabular}[c]{@{}c@{}}Static  \\  Training\end{tabular} & \begin{tabular}[c]{@{}c@{}}Dynamic \\ Training\end{tabular} \\ \hline
    1  & 167.6 & 1.2  & 1422.1 & 13.2  & 31.5 & 222.1 & 766.1  \\
3  & 172.4 & 3.0  & 1465.8 & 21.9  & 31.3 & 219.5 & 802.1  \\
5  & 166.7 & 3.0  & 1339.4 & 28.1  & 30.6 & 215.9 & 825.0  \\
10 & 168.8 & 5.6  & 1340.1 & 46.3  & 31.7 & 218.8 & 935.4  \\
20 & 167.1 & 11.1 & 1331.7 & 87.4  & 30.5 & 214.7 & 1123.6 \\
40 & 168.2 & 33.1 & 1392.0 & 273.8 & 31.5 & 217.7 & 1950.2 \\ \hline
    \end{tabular}%
    }{The median over $\lambda$ is displayed if the relevant method is dependent on $\lambda$ }
    \end{table}

    
\begin{table}[h]
    \TABLE 
    {Median computation times (in seconds) for contextual MNV
    \label{tab:MNV_times}}
    {
    \begin{tabular}{cccccccc}
    \hline
    $K$ & \begin{tabular}[c]{@{}c@{}}MMD \\ Training\end{tabular} & \begin{tabular}[c]{@{}c@{}}Surrogate Solution\\  Calculation (MMD)\end{tabular} & \begin{tabular}[c]{@{}c@{}} 2SP Solve \\ $n_{\text{eval}} = 300$ \end{tabular} & \begin{tabular}[c]{@{}c@{}}MMD Loss \\ Evaluation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Loss-Net \\ Training\end{tabular} & \begin{tabular}[c]{@{}c@{}}Static  \\  Training\end{tabular} & \begin{tabular}[c]{@{}c@{}}Dynamic \\ Training\end{tabular} \\ \hline
    1   & 22.6                                                    & 0.7                                                                             & 233.0     & 11.3                                                           & 20.5                                                         & 29.5                                                                & 203.6                                                       \\
    3   & 23.6                                                    & 2.3                                                                             & 245.3     & 25.6                                                           & 21.3                                                         & 30.4                                                                & 256.6                                                       \\
    5   & 21.8                                                    & 4.3                                                                             & 236.1     & 40.1                                                           & 18.8                                                         & 27.5                                                                & 308.4                                                       \\
    10  & 20.9                                                    & 8.4                                                                             & 234.1     & 84.9                                                           & 19.1                                                         & 27.9                                                                & 491.6                                                       \\ \hline
    \end{tabular}%
    }{}
    \end{table}

