\section{Related Works}
\label{section:Literature}


\subsection{Contextual Optimization for Stochastic Programming}
% Learning-based approaches for stochastic programming have experienced success in recent years. Specifically, neural network approaches have been leveraged to solve challenging stochastic programs in both one-off and contextual settings. 

%% Standard contextual approaches conditional-density-estimation-then-optimize

This section highlights works focused on contextual stochastic optimization that are most related to the proposed methodology. We refer the reader to **Robust Optimization: Theoretically Efficient, Computationally Feasible Algorithms**, 2006 for a more general survey.  Furthermore, we leverage the categorization of contextual stochastic optimization introduced by **A Dynamic Model for Demand Response and Network Constrained Supply Function Equilibrium**, 2011. The first general approach, referred to as \textit{conditional-density-estimation-then-optimize} estimates $\mathbb{P}_{\omega|\bs{x}}$ then solves (\ref{eq:Stage I}) with the estimated conditional distribution. For example, in the case of 2SPs, **Scenario Tree Optimization for Stochastic Energy Planning Under Uncertainty**, 2014 use residuals from the trained regression models to estimate conditional distributions, whereas **Conditional Density Estimation for Two-Stage Robust Problems**, 2017 reweighs samples to approximate $\mathbb{P}_{\omega|\bs{x}}$. However, as pointed out in the introduction, this typically yields a difficult problem, motivating the generation a manageable number of scenarios. 

The second approach referred to as \textit{direct-solution-prediction}, aims to directly estimate a mapping from the context $\bs{x}$ to decisions $\bs{y}$ such that the decisions are of high quality, yielding a policy optimization problem. For example, in the case of 2SPs, **Multiagent Actor Critique for Stochastic Programs**, 2020 formulate a multiagent actor critique approach to solving contextual two-stage knapsack problems where the agents predict solutions to the first and second stages, respectively. Although the solution-prediction approach is effective in tailored settings, it is difficult to deal with general integrality constraints and other complicating problem features ____. 

The \textit{predict-then-optimize} approach generates a point prediction of uncertainty from the context and solves (\ref{eq:Stage I}) using a corresponding singleton distribution. Naive versions use standard loss functions (e.g., least-squares), while \textit{smart predict-then-optimize} selects the predictor based on decision performance. For linear programs, **Robust Optimization for Linear Programs**, 2009 minimize decision regret with a surrogate loss function. **Learning to Optimize: A Neural Network Approach for Stochastic Programming**, 2018 apply similar methods to linear 2SPs. Other approaches update models using gradients of downstream performance ____ but struggle with uninformative gradients ____. To address this, **Neural Surrogate Losses for Stochastic Programs**, 2020 propose modelling a surrogate loss function via neural networks. However, early iterations often lead to predictors whose predictions lie outside the region where the model accurately captures the loss, resulting in what we call \textit{loss error maximization}. Moreover, point predictions implicitly assume perfect knowledge, leading to decisions poorly hedged against uncertainty ____.

The proposed CSG approach aims to address these concerns in the general setting of a large class of 2SPs, circumventing computational concerns by generating a small subset of scenarios and subsequently solving (\ref{eq:2SP-SAA}). This avoids feasibility issues commonly encountered in the direct-solution-prediction approach. Furthermore, the approach inherits all the problem-class generality of **A Stochastic Dual Dynamic Programming Method for Multi-stage Risk-Averse Optimization**, 2014's approach while addressing the issue of loss error maximization. Lastly, CSG produces solutions that hedge against uncertainty by considering the surrogate scenarios as opposed to a single prediction.


\subsection{Scenario Generation}

Scenario generation techniques can be categorized into distributional and problem-based approaches. The distributional approach generates surrogate scenarios to mimic some features of the underlying distribution. This aim is typically achieved by minimizing a distributional distance **Conditional Density Estimation for Two-Stage Robust Problems**, 2017 or by matching moments ____ between the scenarios and the underlying distribution. In contrast, problem-driven scenario generation methods account for problem-specific structures so that the resulting scenarios yield high-quality solutions when evaluated using the underlying distribution. Some problem driven approaches include: clustering scenarios based on objective values of candidate solutions ____, identifying irrelevant scenarios based on the objective ____, and approximating the recourse function of a pool of candidate solutions ____. In our setting, the scenario generation methods mentioned assume access to the conditional distribution or a sufficiently accurate scenario representation with enough samples $M$. One can estimate the conditional distribution as in the conditional-density-estimation-then-optimize approach; however, the generic problem-driven approaches depend at least linearly on $M$. In contrast, besides optimizing implementation error, the proposed CSG approach generates surrogate scenarios via a neural network forward pass, eliminating dependence on $M$ at decision time.




\subsection{Learning-based Approaches for Stochastic Programs}

Machine learning has been applied to solve stochastic programs by predicting solutions or costs. In particular, approaches that leverage machine learning to predict the recourse cost are most relevant to this work. For example, **Robust Optimization: Theoretically Efficient, Computationally Feasible Algorithms**, 2006, **Conditional Density Estimation for Two-Stage Robust Problems**, 2017 and **Learning to Optimize: A Neural Network Approach for Stochastic Programming**, 2018 predict expected recourse in generic 2SPs, leveraging these predictions in the solution of 2SPs. However, these methods do not directly address scenario generation. Although, some works apply machine learning to scenario generation. **Conditional Variational Autoencoders for Stochastic Programs**, 2020 use regression to predict a single representative scenario for 2SP such that the scenario minimizes implementation error, but their method relies on heuristically generated datasets and doesn't handle multiple scenarios. **Semi-supervised Learning with Conditional Variational Autoencoders for Stochastic Programming**, 2019 employ semi-supervised learning and conditional variational auto-encoders to generate scenario embeddings that align with the optimal expected cost of 2SP based on a subset of solved instances. Unlike their approach, which doesn't explicitly minimize implementation error, this work does not require the ability to solve large-scale 2SPs and instead assumes the ability to solve 2SPs on up to $K$ scenarios.