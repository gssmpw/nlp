 \section{Conclusion}\label{section:conclusion}

 This work introduces CSG, a framework for solving 2SPs in contextual settings. In time-sensitive applications, here-and-now decisions are required that consider the distribution of uncertainty conditioned on the available context. It is desirable to obtain solutions to contextual 2SPs efficiently. In this aim, CSG proposes learning a mapping from context to a set of $K$ scenarios that can then be used to solve the deterministic equivalent of the 2SP defined on the $K$ scenarios. CSG leverages contextual information to learn task-mappings that produce high-quality decisions while reducing the computational burden of solving large-scale stochastic programs repeatedly.

 Motivated by distributional approaches to scenario generation, we propose a distributional approach that leverages the MMD distance to create task mappings that remain close to the true conditional distribution across contexts. This method only requires joint samples, typically available in historical data, and does not need direct access to the true conditional distributions. Furthermore, we propose a problem-driven approach based on a bi-level scenario generation problem that addresses non-unique solutions and sparse gradients. Non-unique solutions are addressed by introducing an optimistic loss function that is easily computable for a large class of 2SPs. Furthermore, we mitigate the issue of sparse gradients by using a neural network to approximate the proposed problem-driven loss. We subsequently use the trained network along with MMD regularization to guide the gradient-based search for the task network. Lastly, we propose a dynamic approach that extends the static approach by iteratively refining the loss-net and task-net in an alternating fashion. We observe that the dynamic approach can improve the performance of the static approach if the loss-net can sufficiently approximate the problem-driven loss. 


A diverse array of contextual problem settings is employed to demonstrate the framework's effectiveness and illustrate its flexibility. We show that the proposed problem-driven methods rely critically on MMD regularization to produce high-quality task nets. Furthermore, we observe that the proposed methodology can unlock the value of stochastic solutions and can compute high-quality solutions in the considered problem settings. Additionally, the offline cost of training the models required for CSG is justified by the time savings in computing solutions to contextual 2SPs. 

Future work could explore the application and impacts of different distributional  distance measures. Furthermore, understanding finite sample learning bounds and the generalization/stability properties of the proposed methodologies is of interest. For instance, the MMD distance is rooted in kernel methods that have rich generalization theory \citep{JMLR:v13:gretton12a}. Lastly, problem-specific extensions and structured architectures could reveal new insights in the field of contextual optimization. 


