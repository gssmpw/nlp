\section{Proposed Methodology}\label{section:methodology}
This section introduces the relevant tools from scenario generation in Section \ref{subsection:dsg}, followed by the proposed distributional contextual scenario generation approach in Section \ref{subsection:DCSG}. Analogously, Section \ref{subsection:prelim_PSG} introduces a bi-level approach to scenario generation and discusses its properties, followed by its contextual extension to our setting in Section \ref{subsection:PCSG} and the associated solution approach in Section \ref{subsection:solution_approach}.

\subsection{Preliminaries: Distributional Scenario Generation}\label{subsection:dsg}

\citet{romisch2003stability} argue that integral probability metrics (IPMs) are a sensible choice of distances for 2SPs since IPMs such as the Fortet-Mourier and Wasserstein distances lead to stability bounds for 2SPs and form the basis of several existing scenario generation approaches. Given a class of real-valued bounded measurable functions associated with the sample space $\Omega$ along with two distributions $\mathbb{P}_{\omega}$ and $\mathbb{P}_{\eta}$ on $\Omega$, IPMs take the form
\begin{equation*}\label{eq:ipm}
d_{\mathcal{G}}(\mathbb{P}_{\omega}, \mathbb{P}_{\eta}) = \sup_{g \in \mathcal{G}} \left|  \mathbb{E}_{\omega \sim \mathbb{P}_{\omega}} [g(\omega)] - \mathbb{E}_{\eta \sim \mathbb{P}_{\eta}} [g(\eta)] \right|.
\end{equation*}

Different choices of $\mathcal{G}$ yield different distances. For example, when $\mathcal{G}$ is the set of 1-Lipshitz functions, $d_{\mathcal{G}}(\cdot, \cdot)$ corresponds to the 1-Wasserstein distance. Another metric of interest, maximum mean discrepancy (MMD), corresponds to the class $\mathcal{G}_{\text{MMD}} = \{g \in \mathcal{H}: \| g\|_{\infty} \leq 1 \} $ where $\mathcal{H}$ denotes a Reproducing Kernel Hilbert Space (RKHS) with associated positive semi-definite kernel $k: \Omega \times \Omega \rightarrow \mathbb{R}$. The kernel mean embedding of $\mathbb{P}_{\omega}$ in $\mathcal{G}_{\text{MMD}}$ is given by 
$\mu_{\mathbb{P}_{\omega}} \coloneq \mathbb{E}_{\omega \sim \mathbb{P}_{\omega}}[k(\cdot,\omega)]$, and is guaranteed to be an element of $\mathcal{G}_{\text{MMD}}$ if $\mathbb{E}_{\omega \sim \mathbb{P}_{\omega}}[\sqrt{k(\omega, \omega)}] < \infty$\footnote{This technical assumption is not overly restrictive since it holds for continuous kernels on
compact domains or continuous bounded kernels.} \citep{JMLR:v13:gretton12a}. \citet{JMLR:v13:gretton12a} showed that the squared MMD is given by
\begin{align*}
   d^2_{\mathcal{G}_{\text{MMD}}}(\mathbb{P}_{\omega}, \mathbb{P}_{\eta})& = \norm{\mu_{\mathbb{P}_{\omega}} - \mu_{\mathbb{Q}_{\omega}}}^2_{\mathcal{G}_{\text{MMD}}} \\ & =  \mathbb{E}_{(\omega, \omega') \sim \mathbb{P}_{\omega}}[k(\omega, \omega')] + \mathbb{E}_{(\eta, \eta') \sim \mathbb{P}_{\eta}}[k(\eta, \eta')] - 2\mathbb{E}_{\omega \sim \mathbb{P}_{\omega}, \eta \sim \mathbb{P}_{\eta}}[k(\omega, \eta)],
\end{align*}
where $\norm{.}_{\mathcal{G}_{\text{MMD}}}$ denotes the norm in the RKHS $\mathcal{G}_{\text{MMD}}$. A kernel $k$ is characteristic if $\mu : \mathbb{P} \mapsto \mu_{\mathbb{P}}$ is injective. Characteristic kernels guarantee that $d^2_{\mathcal{G}_{\text{MMD}}}(\mathbb{P}_{\omega}, \mathbb{P}_{\eta}) = 0 \iff \mathbb{P}_{\omega} = \mathbb{P}_{\eta}$. For more details regarding RKHS and embeddings of probability distributions, the reader is referred to \citep{JMLR:v13:gretton12a, muandet2017kernel}. 


For a given context $\bs{x}$, the distributional scenario generation (DSG) problem solves
\begin{equation}\label{eq:DSG}
\min_{\zeta_1,\hdots, \zeta_K} d_{\mathcal{G}}(\mathbb{P}_{\zeta_{1 \hdots K}}, \mathbb{P}_{\omega | \bs{x}}), \tag{DSG}
\end{equation}
where $\mathbb{P}_{\zeta_{1 \hdots K}} = \frac{1}{K} \sum_{k=1}^K \delta_{\zeta_k}$ is the empirical measure associated with the scenarios $\zeta_{1 \hdots K}$ and $\delta_{\zeta}$ is the Dirac delta function centered at $\zeta$. However, as pointed out in the introduction, in practice one only has historical data $S$ that is used to form an estimate $p_{\theta}(\omega | \bs{x})$ of $\mathbb{P}_{\omega | \bs{x}}$. In response, a sufficiently large number of samples are sampled from $p_{\theta}(\omega | \bs{x})$ and (\ref{eq:DSG}) is solved with $\mathbb{P}_{\omega | \bs{x}}$ replaced with the empirical measure supported on the samples. The next section presents the proposed contextual extension of (\ref{eq:DSG}).



\subsection{Distributional Contextual Scenario Generation}\label{subsection:DCSG}


Given a contextual realization $\hat{\bs{x}}$, the decision maker wishes to use $\mathbb{P}_{\omega | \bs{x} = \hat{\bs{x}}}$ to evaluate the expectation in (\ref{eq:Stage I}). We propose selecting $\bs{f}: \bs{x} \mapsto \zeta_{1...K}$ from a vector-valued function class $\mathcal{F}$ such that the distributional distance between the empirical distribution supported on $\bs{f}(\bs{x})$ and $\mathbb{P}_{\omega|\bs{x}}$ is minimized in expectation over $\mathbb{P}_{\bs{x}}$. We refer to $\mathcal{F}$ as the hypothesis set for the task-mapping. This approach is referred to as distributional contextual scenario generation (DCSG)
\begin{equation}\label{eq:DCSG}
\min_{\bs{f} \in \mathcal{F}} \mathcal{L}_{\text{dist}}(\bs{f}) := \mathbb{E}_{\bs{x} \sim \mathbb{P}_{\bs{x}}} \left[ d \left( \mathbb{P}_{\bs{f}(\bs{x})}, \mathbb{P}_{\omega |\bs{x}} \right) \right], \tag{DCSG}
\end{equation}
where $\mathbb{P}_{\bs{f}(\bs{x})} = \frac{1}{K} \sum_{k=1}^K \delta_{f_k(\bs{x})}$ is the empirical measure associated with the scenarios $\bs{f}(\bs{x})$ and $d(\cdot,\cdot )$ is a measure of distance between the distributions. It is not clear what class of distances should be used in the contextual setting. Several authors have considered this question in the context of generative modeling using both Wasserstein and MMD distances. A more thorough comparison of the different approaches to comparing conditional distributions is provided in \hbox{Appendix \ref{appendix:comparing}}. \citet{huang2022evaluating}'s work is similar to ours as they consider the same objective but in the setting of generative models. In this work, we set $d = d^2_{\mathcal{G}_{\text{MMD}}}$ due to the desirable properties it affords $\mathcal{L}_{\text{dist}}$ in the proposed contextual setting, which we discuss next. 

First, \citet{huang2022evaluating} provide the following theorem, showing that $\mathcal{L}_{\text{dist}}(\bs{f})$ is a metric. 
\begin{theorem}[Theorem 4, from \citep{huang2022evaluating}]\label{prop:metric}
 If (i) $k(\cdot,\cdot )$ is characteristic and measurable, (ii) $\mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}}[k(\omega, \omega)] < \infty$ and (iii) $\mathbb{E}_{\omega \sim \mathbb{P}_{\bs{f}(\bs{x})}}[k(\omega, \omega)] < \infty$ for all $\bs{x} \in \mathcal{X}$, then $\mathcal{L}_{\text{dist}}(\bs{f}) \geq 0$ and $\mathcal{L}_{\text{dist}}(\bs{f}) = 0 \iff\mathbb{P}_{\bs{f}(\bs{x})} = \mathbb{P}_{\omega|\bs{x}}$ almost everywhere according to $\mathbb{P}_{\bs{x}}$. 
\end{theorem}


Next, we note that $\mathcal{L}_{\text{dist}}(\bs{f}) $ can be optimized over $\bs{f}$ using only joint samples. $\mathcal{L}_{\text{dist}}(\bs{f}) $ can be written as
\begin{equation*}
  \begin{aligned}
  \mathcal{L}_{\text{dist}}(\bs{f}) =\ & \mathbb{E}_{\bs{x} \sim \mathbb{P}_{\bs{x}}} {\Bigg [} \mathbb{E}_{(\omega, \omega') \sim \mathbb{P}_{\omega|\bs{x}}}{\Big[}k(\omega, \omega'){\Big]} + \frac{1}{K^2} \sum_{i=1}^K \sum_{i' = 1}^K k{\big (} f_i(\bs{x}), f_{i'}(\bs{x}) {\big )}  \\ 
   \quad & -\frac{2}{K} \sum_{i=1}^K \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}}{\Big[} k {\big (} \omega, f_i(\bs{x}) {\big )}{\Big]} 
  {\Bigg]}  \\
  =\ & C + \frac{1}{K^2} \sum_{i=1}^K \sum_{i' = 1}^K \mathbb{E}_{\bs{x} \sim \mathbb{P}_{\bs{x}}} {\Big[} k {\big (} f_i(\bs{x}), f_{i'}(\bs{x}) {\big )} {\Big]} \\
  \quad & -\frac{2}{K} \sum_{i=1}^K \mathbb{E}_{\bs{x} \sim \mathbb{P}_{\bs{x}}} \mathbb{E}_{\omega \sim \mathbb{P}_{\omega|\bs{x}}} {\Big[} k {\big (} \omega, f_i(\bs{x}) {\big )}{\Big]} \\
   = \ & C + \mathbb{E}_{(\bs{x}, \omega) \sim \mathbb{P}_{\bs{x},\omega}} {\Bigg [} \underbrace{\frac{1}{K^2} \sum_{i=1}^K \sum_{i' = 1}^K k{\big (} f_i(\bs{x}), f_{i'}(\bs{x}) {\big )} - \frac{2}{K} \sum_{i=1}^K  k {\big (} \omega, f_i(\bs{x}) {\big )}}_{\coloneq \ell_{\text{MMD}}(\bs{f}(\bs{x}), \omega)} {\Bigg ],}
  \end{aligned}
  \end{equation*}  
where $C = \mathbb{E}_{\bs{x} \sim \mathbb{P}_{\bs{x}}} \mathbb{E}_{(\omega, \omega') \sim \mathbb{P}_{\omega|\bs{x}}}{\big[}k(\omega, \omega'){\big]}$ is a constant with respect to $\bs{f}$. \hbox{\citet{huang2022evaluating}} point out that evaluating $C$, requires $\omega$ and $\omega'$ to be drawn in a conditionally independent matter for each $\bs{x}$, which is not equivalent to globally sampling $(\bs{x}, \omega, \omega')$, since the latter is not necessarily conditionally independent. Let $\mathcal{L}_{\text{MMD}}(\bs{f}) \coloneq \mathbb{E}_{(\bs{x}, \omega)} [\ell_{\text{MMD}}(\bs{f}(\bs{x}), \omega)]$ denote the part of $\mathcal{L}_{\text{dist}}(\bs{f})$ that depends on $\bs{f}$. 

Since $C$ does not depend on $\bs{f}$, optimizing $\mathcal{L}_{\text{MMD}}(\cdot)$ is equivalent to optimizing $\mathcal{L}_{\text{dist}}(\cdot)$ over $\bs{f}$. Let $\hat{\mathcal{L}}_{\text{MMD}}(\bs{f}) \coloneq \frac{1}{n} \sum_{i=1}^n \ell_{\text{MMD}}(\bs{f}(\bs{x}^{(i)}), \omega^{(i)})$ denote the sample estimate. In practice, one selects $\bs{f}$ via empirical loss minimization
$
\min_{\bs{f} \in \mathcal{F}} \ \hat{\mathcal{L}}_{\text{MMD}}(\bs{f})
$.
Although $\hat{\mathcal{L}}_{\text{MMD}}(\cdot)$ is non-convex in general and optimization cannot be performed analytically. In the naive implementation, the computational cost to evaluate the batch gradient over $S$ is $K^2 + nK$ kernel evaluations, making optimization computationally tractable for small $K$. 



In this work, we consider kernel $k_E(\omega, \omega') = \frac{1}{2} \left(\norm{\omega}_2 + \norm{\omega'}_2 - \norm{\omega - \omega'} \right)$ corresponding to the energy-distance \citep{sejdinovic2013equivalence}. \citet{sejourne2023unbalanced} point out that optimization over MMD distances induced by parameterized kernels (e.g., the Gaussian kernel) are sensitive to their parameters, making the parameter-free $k_E$ an attractive choice. Furthermore, one can observe that $d^2_{\text{MMD}}$ with kernel $k_E$ is equivalent to selecting the negative Euclidean kernel $\tilde{k}_E(\omega, \omega') = -\norm{\omega - \omega'}_2$. In this case, $d^2_{\text{MMD}}$ respects the underlying geometry via scale equivariance \citep{szekely2012uniqueness}. That is, for $d^2_{\mathcal{G}_{\text{MMD}}}(\mathbb{P}_{\omega}, \mathbb{P}_{\eta})$, scaling the sample spaces of $\mathbb{P}_{\omega}$ and $\mathbb{P}_{\eta}$ by $c \in \mathbb{R}$, scales $d^2_{\mathcal{G}_{\text{MMD}}}(\mathbb{P}_{\omega}, \mathbb{P}_{\eta})$ by $|c|$. Furthermore, it is interesting to note that, in the case of the energy kernel $\tilde{k}_E$, along with setting $K=1$, optimizing $\mathcal{L}_{\text{MMD}}(\bs{f})$ reduces to least-squares. 

The energy distance satisfies the assumptions in Theorem \ref{prop:metric}. Assumptions (ii) and (iii) in \hbox{Theorem \ref{prop:metric}}, reduce to the bounded moment conditions present in Proposition 1 by \citet{szekely2012uniqueness}. Proposition 1 by \citet{szekely2012uniqueness} implies that the kernel $\tilde{k}_E$ is characteristic. Therefore, under the bounded moment conditions by \citet{szekely2012uniqueness}, assumption (i) in Theorem \ref{prop:metric} is automatically satisfied, implying Theorem \ref{prop:metric} holds.


\subsection{Preliminaries: Problem-driven Scenario Generation}\label{subsection:prelim_PSG}
This section discusses problem-driven scenario generation. We highlight two important features that guide our proposed contextual approach. Firstly, we consider computational issues regarding problem-driven scenario generation, followed by concerns caused by non-unique solutions to (\ref{eq:2SP-SAA}) defined on the surrogate scenarios. 

\subsubsection*{Bi-level Problem-driven Scenario Generation}
One can attempt to naively employ a bi-level approach to scenario generation based on selecting the scenarios so that the solution of (\ref{eq:2SP-SAA}), based on said scenarios, minimizes the (\ref{eq:Stage I}) objective. For a given context $\bs{x}$, this results in the following bi-level problem
\begin{align}
\min_{\zeta_1, ... ,\zeta_K} \quad & L_{\text{task}}(\zeta_{1 \hdots K}) \coloneq  h( \bs{y}(\zeta_{1...K}))+ \mathbb{E}_{\omega \sim \mathbb{P}_{\omega| \bs{x}}}\left[q \left(\bs{z}(\bs{y}(\zeta_{1...K}), \omega), \omega \right) \right] \quad \tag{BI-SAA} \label{eq:bisaa} \\ \textit{s.t.}  \quad & \bs{z}(\bs{y}(\zeta_{1...K}), \omega) \in \underset{z \in \mathcal{Z}(\bs{y}(\zeta_{1...K}), \omega)}{\text{argmin}} q(\bs{z}, \omega) \quad \forall\  \omega \in \Omega &   \tag{SP} \\
\quad & \bs{y}(\zeta_{1...K}) \in \underset{\bs{y}}{\text{proj}}\  \underset{\bs{y}, \bs{z}_1, ... \bs{z}_K}{\text{argmin}}\  h( \bs{y}) + \frac{1}{K}\sum_{i = 1}^K q(\bs{z}_i, \zeta_i) &  \tag{$\zeta$-SAA}\\
\quad & \hspace{11em}\bs{y} \in \mathcal{Y}, \  \bs{z}_i \in \mathcal{Z}(\bs{y}, \zeta_i) \ \forall i\  \in \{1,\hdots ,K\}, \notag
\end{align}  
where $\text{proj}_{\bs{y}}$ selects the $\bs{y}$ component of the \hbox{($\zeta$-SAA)} optimal solution set. Some notable aspects of (\ref{eq:bisaa}) exist. First, there is no need for feasibility constraints in the upper-level problem since $\bs{y}(\zeta_{1...K})$ is feasible by construction. The lower level problem \hbox{($\zeta$-SAA)} with solution $\bs{y}(\zeta_{1...K})$ is the surrogate SAA problem defined on $K$ scenarios. (\ref{eq:bisaa}) implicitly assumes \hbox{($\zeta$-SAA)} admits a unique solution for any $\zeta_{1\hdots K} \in \Omega^K$. Similar to (\ref{eq:DSG}), one can sample a sufficiently large number of scenarios from $p_{\theta}(\omega | \bs{x})$ and attempt to solve (\ref{eq:bisaa}) with $\mathbb{P}_{\omega | \bs{x}}$ replaced with the empirical measure supported on the samples.

\subsubsection*{Challenges with Gradients} \label{section:gradient_problems}
Consider the case where $\mathbb{P}_{\omega|\bs{x}}$ is replaced with $M$ scenarios. A heuristic approach to solve (\ref{eq:bisaa}) is to use gradients of the upper-level objective with respect to the upper-level decision variables $\zeta_{1...K}$. The scenarios can be initialized and updated via gradient descent,
$
\zeta \leftarrow \zeta - \eta \ \partial L_{\text{task}} / \partial \zeta
$, where the chain rule yields
$$\frac{\partial{L_{\text{task}}}}{\partial{\zeta}} = \frac{\partial{L_{\text{task}}}}{\partial{\bs{y}}} \frac{\partial{\bs{y}}}{\partial{\zeta}} + \frac{\partial{L_{\text{task}}}}{\partial{\bs{z}}} \frac{\partial{\bs{z}}}{{\partial{\bs{y}}}} \frac{\partial{\bs{y}}}{\partial{\zeta}},$$
where $\bs{z} \in \mathbb{R}^{M s_2}$ refers to the $M$ second-stage solutions stacked in a vector. The gradient of the upper-level cost relies on computing the gradients of the surrogate problem solution map with respect to the surrogate scenarios $\frac{\partial{\bs{y}}}{\partial{\zeta}}$ and the gradients of the subproblem solution map with respect to the first stage solution $\frac{\partial{\bs{z}}}{{\partial{\bs{y}}}}$. These gradients can be computed via various methods depending on the problem structure. For example, in the case of convexity, \citet{agrawal2019differentiable}'s \textsf{cvxpy layers} can compute the desired gradients. Unfortunately, the gradients are generally sparse and uninformative (even in non-combinatorial problems) \citep{grigas2021integrated,zharmagambetov2023landscape}. This forms the basis of the proposed problem-driven approach. We mitigate sparse gradients by using neural architectures to smooth out the loss of a particular set of surrogate scenarios and subsequently back-propagate smoothed problem-driven gradients to the task-mapping $\bs{f}$. 


\subsubsection*{Non-uniqueness of Optimal Solutions}\label{section:nonunique_optimal_solutions}

As written, (\ref{eq:bisaa}) is not well-defined if \hbox{($\zeta$-SAA)} has multiple optimal solutions. A couple of different approaches can address the issue of multiple optimal solutions.  In the case that \hbox{($\zeta$-SAA)} is a convex program with a compact feasible set, one can add a small quadratic regularization term $\epsilon \left(\norm{\bs{y}}_2^2 + \sum_{i=1}^K \norm{\bs{z_i}}_2^2 \right)$ to the \hbox{($\zeta$-SAA)} objective to induce strict convexity and hence uniqueness. The unique solution obtained by solving the regularized problem is guaranteed to have objective value within $\epsilon D^2$ of the optimal objective, where $D$ is the diameter of the compact feasible set \hbox{\citep{wilder_melding}}. If \hbox{($\zeta$-SAA)} has a more complex problem structure (e.g., mixed integer constraints), then the regularization trick described above may not be sufficient for uniqueness. We resort to the standard conventions in the bi-level optimization literature \citep{sinha2017review}. It is unclear which solution to \hbox{($\zeta$-SAA)} is implemented in the upper-level objective. In the classic optimistic (pessimistic) setting, nature selects the optimal solution to \hbox{($\zeta$-SAA)} that minimizes (maximizes) the upper-level objective. 

We let $\mathcal{Y}^*(\zeta_{1\hdots K})$ denote the set of $\bs{y}$ that form a part of an optimal solution to \hbox{($\zeta$-SAA)}, given $\zeta_{1\hdots K}$ i.e. $\mathcal{Y}^*(\zeta_{1\hdots K}) \coloneq $
\begin{align}
  \Bigg{\{}\bs{y} \in \mathcal{Y} : h( \bs{y}) + \frac{1}{K}\sum_{i = 1}^K  q(\bs{z}_i, \zeta_i) \leq v^*(\zeta_{1\hdots K}), \nonumber 
 \bs{z}_i \in \mathcal{Z}(\bs{y}, \zeta_i), \ i \in \{1,\hdots ,K\} \Bigg{\}} , \nonumber
\end{align}
where $v^*(\zeta_{1 \hdots K})$ is the optimal objective value of \hbox{($\zeta$-SAA)}.
The optimistic version of (\ref{eq:bisaa}) can be written as
\begin{equation}\label{eq:optbisaa}
 \min_{\zeta_1, ... ,\zeta_K}\  \min_{\bs{y} \in \mathcal{Y}^*(\zeta_{1\hdots K})} \quad   h( \bs{y})+ \mathbb{E}_{\omega \sim \mathbb{P}_{\omega| \bs{x}}}\left[Q \left(\bs{y}(\zeta_{1...K}), \omega \right) \right].  \tag{Opt-BI}
\end{equation}

The optimistic setting assumes that any of the best possible solutions among the optimal solution set $\mathcal{Y}^*(\zeta_{1\hdots K})$ according to $L_{\text{task}}(\zeta_{1 \hdots K})$ are selected. One could also consider the pessimistic version; given by replacing $\min_{\bs{y} \in \mathcal{Y}^*(\zeta_{1\hdots K})} $ with $\max_{\bs{y} \in \mathcal{Y}^*(\zeta_{1\hdots K})}$ in (\ref{eq:optbisaa}).
 


\subsection{Problem-driven Contextual Scenario Generation}\label{subsection:PCSG}

We aim to select $\bs{f} \in \mathcal{F}$ such that, given $\bs{x}$, the predicted surrogate scenarios $\bs{f}(\bs{x})$ produce the highest quality set of optimal \hbox{($\zeta$-SAA)} solutions $\mathcal{Y}^*(\bs{f}(\bs{x}))$. We measure the quality of the optimal solution set $\mathcal{Y}^*(\bs{f}(\bs{x}))$ by the best possible two-stage performance among solutions in $\mathcal{Y}^*(\bs{f}(\bs{x}))$. The goal of the problem-driven approach is to select $\bs{f}$ such that $\bs{f}$ is in (\ref{eq:optbisaa})'s solution set $\textit{(a.s)}$ with respect to $\mathbb{P}_{\bs{x}}$. This corresponds to 
\begin{equation*}
  \begin{aligned}
\bs{f}(\bs{x}) \in \underset{\zeta_1, ... ,\zeta_K}{\text{argmin}} \ \min_{\bs{y} \in \mathcal{Y}^*(\zeta_{1\hdots K})} \quad  & h( \bs{y}) + \mathbb{E}_{\omega \sim \mathbb{P}_{\omega | \bs{x}}} \left[ Q \left(\bs{y}, \omega \right) \right] 
\end{aligned}
\end{equation*}
holding (\textit{a.s.}) with respect to $\mathbb{P}_{\bs{x}}$. Theorem 14.60 due to \citet{rockafellar2009variational}, implies this is equivalent to $\bs{f}$ being an optimal solution to the following problem
\begin{equation} \label{eq:pCSG-opt1}
\min_{\bs{f} \in \mathcal{F}} \quad  \mathbb{E}_{\bs{x} \sim \mathbb{P}_{\bs{x}}} \left[ \min_{\bs{y} \in \mathcal{Y}^*(\bs{f}(\bs{x}))} h(\bs{y}) + \mathbb{E}_{\omega \sim \mathbb{P}_{\omega | \bs{x}}} \left[ Q \left(\bs{y}, \omega \right) \right] \right]. \tag{Opt-PCSG}
\end{equation} 
The following inequality holds
\begin{align} 
\min_{\bs{y} \in \mathcal{Y}^*(\bs{f}(\bs{x}))} h(\bs{y}) + \mathbb{E}_{\omega \sim \mathbb{P}_{\omega | \bs{x}}} \left[ Q \left(\bs{y}, \omega \right) \right]
= \ & \min_{\bs{y} \in \mathcal{Y}^*(\bs{f}(\bs{x}))} \mathbb{E}_{\omega \sim \mathbb{P}_{\omega | \bs{x}}} \left[ h(\bs{y}) + Q \left(\bs{y}, \omega \right) \right] \nonumber \\ \nonumber
\geq \ &  \mathbb{E}_{\omega \sim \mathbb{P}_{\omega | \bs{x}}} \left[ \min_{\bs{y} \in \mathcal{Y}^*(\bs{f}(\bs{x}))} h(\bs{y}) + Q \left(\bs{y}, \omega \right) \right],
\end{align}
where the inequality follows by relaxing nonanticipativity among the optimal solution set $\mathcal{Y}^*(\bs{f}(\bs{x}))$. Thus, the following optimistic relaxation of (\ref{eq:pCSG-opt1}) follows
\begin{equation}\label{eq:pCSG-opt2}
    \min_{\bs{f} \in \mathcal{F}} \quad   \mathbb{E}_{(\bs{x}, \omega) \sim \mathbb{P}_{\bs{x}, \omega}} \left[
    \min_{\bs{y} \in \mathcal{Y}^*(\bs{f}(\bs{x}))} h(\bs{y}) + Q \left(\bs{y}, \omega \right) \right]. \tag{Opt-PCSG'}
\end{equation} 
The advantage of (\ref{eq:pCSG-opt2}) over (\ref{eq:pCSG-opt1}) is that is does not require sampling access to $\mathbb{P}_{\omega|\bs{x}}$. Consequently, (\ref{eq:pCSG-opt2}) suggests the following loss function
$$\ell_{\text{opt}}(\zeta_{1...K}, \omega) \coloneq \min_{\bs{y} \in \mathcal{Y}^*(\zeta_{1 \hdots K})} h(\bs{y}) + Q \left(\bs{y}, \omega \right).$$ 
Evaluating $\ell_{\text{opt}}(\zeta_{1...K}, \omega)$ requires solving ($\zeta$-SAA), obtaining the optimal value $v^*(\zeta_{1 \hdots K})$, then given $\omega$, finding the solution among $\mathcal{Y}^*(\bs{f}(\bs{x}))$ that minimizes $h(\bs{y}) + Q \left(\bs{y}, \omega \right)$. The ability to efficiently evaluate $\ell_{\text{opt}}$ relies on i) the ability to efficiently solve \hbox{($\zeta$-SAA)} on $K$ scenarios (Assumption \ref{a2}), and ii) the ability to efficiently minimize the 2SP objective defined by a single scenario $\omega$ over the optimal solution set $\mathcal{Y}^*(\bs{f}(\bs{x}))$. Suppose \hbox{($\zeta$-SAA)} obtains optimal value $v^*(\zeta_{1 \hdots K})$, then evaluating $\ell_{\text{opt}}(\zeta_{1...K}, \omega)$ is equivalent to
  \begin{align}\label{eq:opt_search}
  \ell_{\text{opt}}(\zeta_{1 \hdots K}, \omega ) = \min_{\bs{y}, \bs{z}, \bs{z}_1, ... \bs{z}_K} \quad & h( \bs{y})+q(\bs{z}, \omega) \quad  \tag{Opt-Search} \\ \textit{s.t.}  
 \quad & h( \bs{y}) + \frac{1}{K}\sum_{i = 1}^K  q(\bs{z}_i, \zeta_i) \leq v^*(\zeta_{1\hdots K}) & \label{eq:optimality} \\
 \quad & \bs{y} \in \mathcal{Y},\ \bs{z} \in \mathcal{Z}(\bs{y}, \omega),\  \bs{z}_i \in \mathcal{Z}(\bs{y}, \zeta_i), \ \forall i\  \in \{1,\hdots ,K\}.\nonumber
  \end{align} 
(\ref{eq:opt_search}) has the same constraints as \hbox{($\zeta$-SAA)}, along with constraint (\ref{eq:optimality}) that ensures the optimality of $(\bs{y}, \bs{z}_1, ... \bs{z}_K)$ with respect to \hbox{($\zeta$-SAA)}. An additional decision variable $\bs{z}$ is introduced to model the recourse in response to scenario $\omega$. In addition to linear programs and convex programs with multiple solutions, the optimistic approach is generally amenable to mixed-integer programs (MIP) with convex relaxations since, in this case, the resulting instance of (\ref{eq:opt_search}) is a convex MIP with one additional constraint (\ref{eq:optimality}) and decision variable $\bs{z}$.

In the case where ($\zeta$-SAA) exhibits unique solutions, $\ell_{\text{opt}}$ reduces to the simplified loss function $h(\bs{y}(\zeta_{1...K})) + Q(\bs{y}(\zeta_{1...K}), \omega)$ whose evaluation only requires solving \hbox{($\zeta$-SAA)} (Assumption \ref{a2}) and a single subproblem (SP) (Assumption \ref{a1}) to obtain $\bs{y}(\zeta_{1..K})$ and $\bs{z}(\bs{y}(\zeta_{1...K}), \omega)$, respectively. Furthermore, if solving (\ref{eq:opt_search}) proves too computationally burdensome, then one can easily compute $h(\bs{y}(\zeta_{1...K})) + Q(\bs{y}(\zeta_{1...K}), \omega)$ as a heuristic loss (although learning may not be well-defined in this case). This work uses $\ell_{\text{opt}}$ for all problems under consideration since they lack uniqueness guarantees and are amenable to direct computation by solving (\ref{eq:opt_search}). Like the optimistic case, one can consider the pessimistic case. Following the same line of reasoning suggests \hbox{$\max_{\bs{y} \in \mathcal{Y}^*(\zeta_{1 \hdots K})} h(\bs{y}) + Q \left(\bs{y}, \omega \right)$} as a loss function. However, evaluating this loss is more difficult than evaluating $\ell_{\text{opt}}$. For instance, in the case of a two-stage linear program, evaluating the pessimistic loss is equivalent to maximizing a convex function. Due to this added complexity, this work considers the optimistic case.  

\subsection{Solution Methodology}\label{subsection:solution_approach}
Thus far, two loss functions have been introduced. In the case of  $\ell_{\text{MMD}}$, empirical risk minimization over a parametric class $\bs{f}_{\phi}, \ \phi \in \Phi$ via gradient-based methods is straightforward. We will typically take $\Phi$ as a class of neural networks. More details are provided in the experimental section. As discussed in Section \ref{section:gradient_problems}, $\ell_{\text{opt}}$ will in general exhibit sparse gradients with respect to the output of $\bs{f}_{\phi}$, making back-propagation of gradients ineffective. To address this, a neural network architecture (Loss-Net) is proposed to approximate the problem-based loss $\ell_{\text{opt}}$.


The network is a mapping $E_{\psi}$ from $\Omega^{K+1} \rightarrow \mathbb{R}$ with the aim that $E_{\psi}(\zeta_{1...K}, \omega) \approx \ell_{\text{opt}}(\zeta_{1...K}, \omega)$. The architecture is shown in Figure \ref{fig:lossnet}. The architecture's design is motivated by permutation invariant neural architectures \citep{zaheer2017deep,tabaghi2024universal}. In particular, \citet{patel2022neur2sp}'s Neur2SP architecture is notable since they also apply permutation invariant neural architectures to 2SPs. The network embeds each surrogate scenario $\zeta_k,\  k \in [K]$ into a latent space $\Omega' \subseteq \mathbb{R}^{p_{\text{latent}}}$ using an encoder $ \Psi_1 : \Omega \rightarrow \Omega'$. The embedded surrogate set is then represented as a single encoded scenario $\hat{\zeta}$ via mean aggregation i.e., $\hat{\zeta} = \frac{1}{K} \sum_{k=1}^K  \Psi_1(\zeta_{k})$. This ensures that the predictions from the network are invariant to the ordering of the input set $\zeta_{1...K}$. The input scenario is also embedded via $\Psi_1$: $\hat{\omega} =  \Psi_1(\omega)$. The embedding of the surrogate scenarios $\hat{\zeta}$ and embedded input scenario $\hat{\omega}$ are fed into a separate network $ \Psi_2:\Omega' \times \Omega' \rightarrow \mathbb{R}$ that outputs the approximation of $\ell_{\text{opt}}(\zeta_{1...K}, \omega)$ given by $$E_{\psi}(\zeta_{1...K}, \omega) = \Psi_2\left(\hat{\zeta},\  \hat{\omega}\right) = \Psi_2\left(\frac{1}{K} \sum_{k = 1}^K  \Psi_1(\zeta_k),\  \Psi_1(\omega) \right).$$ The networks, $\Psi_1$ and $\Psi_2$, are taken to be fully connected feedforward neural networks with Rectified Linear Unit (ReLu) activations of appropriate input and output dimensions, each with hyperparameters such as the numbers of hidden layers and activations. 


\begin{figure}[h]
  \FIGURE
   {\includegraphics[width=\textwidth]{scen-net.eps}}
    {Loss-Net Architecture
    \label{fig:lossnet}}
    {}
  \end{figure}

It would be ideal for $\ell_{\text{opt}}$ to be representable via a structure similar to $E_{\psi}$. Indeed, the following proposition shows that $\ell_{\text{opt}}$ can be represented as a composition of functions like $E_{\psi}$ when the scenario embedding space $\Omega'$ is of large dimensionality.

\begin{restatable}{proposition}{representation}\label{prop:representation}
% \begin{proposition}\label{prop:representation}
  For a fixed positive integer $K$, the (continuous) task-based loss $\ell_{\text{opt}} : \Omega^K \times \Omega \rightarrow \mathbb{R}$, where $\Omega \subseteq \mathbb{R}^p$ is compact, satisfies 
  \begin{equation*}\label{eqn:representer}
    \ell_{\text{opt}}(\zeta_{1...K}, \omega) = \rho\left(\frac{1}{K}\sum_{k=1}^K \tau(\zeta_k), \tau(\omega)\right) \quad \forall \zeta_{1...K} \in \Omega^K, \omega \in \Omega,
    \end{equation*}
  with  continuous $\tau : \mathbb{R}^p \rightarrow \mathbb{R}^{{K+p \choose p} - 1}$, (continuous) $\rho : \mathbb{R}^{{K+p \choose p} - 1} \times \mathbb{R}^{{K+p \choose p} - 1} \rightarrow \text{codom}(\rho)$, and $\text{codom}(\ell_{\text{opt}}) \subseteq \text{codom}(\rho)$
 %\end{proposition}
\end{restatable}
The proof of Proposition \ref{prop:representation} is a direct application of Proposition 1 by \citet{tabaghi2024universal}. The proof introduces additional notation and is provided in \hbox{Appendix \ref{appendx:prop2}}. Given the representation provided by Proposition \ref{prop:representation}, approximating $\ell_{\text{opt}}$ by $E_{\psi}$ amounts to replacing $\rho$ and $\tau$ with fully connected feedforward ReLu networks $\Psi_2$ and $\Psi_1$, respectively. Proposition \ref{prop:representation} produces a representation such that the continuity of $\ell_{\text{opt}}$ determines the continuity of $\rho$. At the same time, $\tau$ is continuous, independent of the continuity of $\ell_{\text{opt}}$. In any case, by using neural architectures, we leverage a continuous approximation to $\ell_{\text{opt}}$, irrespective of whether $\ell_{\text{opt}}$ is continuous by simply replacing $\rho$ with $\Psi_2$. Furthermore, the representation in Proposition \ref{prop:representation} has $\Omega' \subseteq \mathbb{R}^{{K+p \choose p} - 1}$, i.e. leverages a high-dimensional embedding of the scenarios. 


The loss network acts as replacement for $\ell_{\text{opt}}(\zeta_{1...K}, \omega)$. The basic idea underpinning the proposed approach is to replace $\ell_{\text{opt}}(\zeta_{1...K}, \omega)$ with $E_{\psi}(\zeta_{1...K}, \omega)$ and use it to learn $\bs{f}_{\phi}$. This idea is visually depicted in Figure \ref{fig:overall approach}. Given a trained $E_{\psi}$, one can use it to infer $\zeta_{1...K}$ via gradient-based minimization. The remainder of this section describes training procedures to learn $E_{\psi}$ and $\bs{f}_{\phi}$. 

\begin{figure}[h]
  \FIGURE
  {\includegraphics[width=\textwidth]{drawing-7.eps}}
  {Replacing $\ell_{\text{opt}}$ with $E_{\psi}$ \label{fig:overall approach}}
  {Replace the true downstream loss with the task-net approximated loss to avoid sparse gradients}
\end{figure}


\subsubsection*{Static Approach} 
We do not seek to construct $E_{\psi}$ such that it uniformly approximates $\ell_{\text{opt}}$ over $\Omega^{K+1}$. Instead, we wish to approximate $\ell_{\text{opt}}$ over a distribution $\mathbb{P}_{\zeta_1, \hdots, \zeta_K, \omega}$ that is reflective of the surrogate scenarios $\zeta_{1 \hdots K}$ and uncertainty that $E_{\psi}$ will encounter in practice. Once trained, $E_{\psi}$ is used to guide the learning for $\bs{f}_{\phi}$. We refer to this approach as the static approach.

A simple approach is to train $E_{\psi}$ over surrogate scenarios produced by \hbox{$\zeta_{1 \hdots K} = \bs{\hat{f}}_{\text{MMD}}(\bs{x})$}, where $\bs{\hat{f}}_{\text{MMD}}$ denotes the task-mapping obtained via minimization of $\hat{\mathcal{L}}_{\text{MMD}}$ over the iid sample $S$. Given $ \bs{\hat{f}}_{\text{MMD}}$, the sample for training $E_{\psi}$ is generated by forming the surrogate scenarios $\zeta_{1 \hdots K}^{(i)} = \bs{\hat{f}}_{\text{MMD}}(\bs{x}^{(i)})$, and finally evaluating the task-based loss $\ell_{\text{opt}}^{(i)} = \ell_{\text{opt}}(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)})$, yielding a dataset \hbox{$S' = \{(\bs{x}^{(i)}, \zeta_{1 \hdots K}^{(i)}, \omega^{(i)} ), \ell_{\text{opt}}^{(i)} \}_{i=1}^{n}$}. $E_{\psi}$ is trained by gradient-based methods to minimize the prediction error over $S'$
\begin{equation*}
\min_{\psi} \quad \frac{1}{n} \sum_{i = 1}^{n} \left(E_{\psi}(\zeta_{1...K}^{(i)}, \omega^{(i)})  - \ell_{\text{opt}}^{(i)} \right)^2,
\end{equation*}
where it is assumed that hyperparameters associated with the learning procedure, such as optimizer learning rates, batch size, and $l_2$ regularization on the weights, can be set such that the resulting loss-net achieves generalization. The resulting loss approximator, obtained from training over $S'$ is denoted by $\hat{E}$.


Although it is tempting to select the task-net to minimize the approximate task loss over a sample, we observed that optimizing the approximate task loss tended to yield surrogate scenario predictions $\bs{f}_{\phi}(\bs{x})$ that are far from the distribution of the samples used to train the loss network. I.e., directly minimizing the approximate task loss tends to yield a task-net whose predictions maximize the error of the approximate task loss (\textit{loss error maximization}). To mitigate this, we proposed regularizing the approximate task loss by the MMD loss to select $\bs{f}_{\phi}$
\begin{equation}\label{eq:static_PCSG}
 \min_{\phi} \quad \frac{1}{n} \sum_{i=1}^{n} \hat{E}(\bs{f}_{\phi}(\bs{x}^{(i)}), \omega^{(i)}) + \lambda \ell_{\text{MMD}}(\bs{f}_{\phi}(\bs{x}^{(i)}), \omega^{(i)}), \tag{Static-PCSG}
\end{equation}
where $\lambda \geq 0$ is a regularization penalty. The MMD regularization ensures that the surrogate scenario predictions do not stray too far from the input distribution used to train the loss net. Furthermore, the proposed problem is amenable to empirical risk minimization via the sample $S$, making training straightforward. Once $\hat{E}_{\psi}$ is trained, one can input any number of surrogate scenarios into $\Psi_1$, and a numerical result will still be produced. Although we do not explore it here, this can potentially reduce training time by using an $\hat{E}_{\psi}$ that is trained via cheaper evaluations of $\ell_{\text{opt}}$ using $K' < K$ surrogate scenarios. The static approach for selecting $\bs{f}_{\phi}$ is shown in Algorithm \ref{alg:static}. We denote the task network obtained by training on $S'$ by $\bs{\hat{f}}_{\text{PCSG}}$.

\begin{algorithm}[h] 
  \small
  \caption{Training and Using $E_{\psi}$ for Static PCSG}
  \begin{algorithmic}[1]
  \Require Loss function $\ell_{\text{opt}}$, regularization parameter $\lambda$, 
  Network architecture and training parameters (e.g. batch size ($B$), number of epochs ($E$), No. hidden units), Sample $S = \{(\bs{x}^{(i)}, \omega^{(i)})\}_{i=1}^n$, Trained MMD network $\bs{\hat{f}}_{\text{MMD}}$
    \State Initialize networks $E_{\psi}$ and $\bs{f}_{\phi}$ and hyperparameters: e.g. batch size ($B$)
  \For{each $(\bs{x}^{(i)}, \omega^{(i)}) \in S$}
      \State Evaluate $\zeta_{1 \hdots K}^{(i)} = \bs{\hat{f}}_{\text{MMD}}(\bs{x}^{(i)})$,  $\ell_{\text{opt}}^{(i)} = \ell_{\text{opt}}(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)})$

  \EndFor
  \State Form dataset $S' = \{(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)}, \ell_{\text{opt}}^{(i)})\}_{i=1}^{n}$
  \State Train $\hat{E}$ over $S'$: $\min_{\psi} \  \frac{1}{|S'|} \sum_{\zeta_{1 \hdots K}, \omega, \ell_{\text{opt}} \in S'} (E_{\psi}(\zeta_{1 \hdots K}, \omega) - \ell_{\text{opt}} )^2$, yielding $\hat{E}$
  \State Solve (\ref{eq:static_PCSG}) over $S$ using $\hat{E}$
  \State \textbf{Output:} Trained task-net $\bs{\hat{f}}_{\text{PCSG}}$ and loss-net $\hat{E}$ 
  \end{algorithmic}\label{alg:static}
  \end{algorithm}



\subsubsection*{Dynamic Approach}

After running Algorithm \ref{alg:static}, there is a trained task-net $\bs{\hat{f}}_{\text{PCSG}}$ and loss-net $\hat{E}$. By construction, $\hat{E}$ approximates $\ell_{\text{opt}}$ over the distribution of inputs $(\bs{\hat{f}}_{\text{MMD}}(\bs{x}), \omega)$, where $(\bs{x}, \omega) \sim \mathbb{P}_{\bs{x}, \omega}$. However, there is no guarantee that $\hat{E}$ approximates $\ell_{\text{opt}}$ over the analogous distribution induced by $\bs{\hat{f}}_{\text{PCSG}}$. Thus, additional samples for the loss-net are generated using $\bs{\hat{f}}_{\text{PCSG}}$, by forming the surrogate scenarios $\zeta_{1 \hdots K}^{(i)} = \bs{\hat{f}}_{\text{PCSG}}(\bs{x}^{(i)})$, and evaluating the task-based loss $\ell_{\text{opt}}^{(i)} = \ell_{\text{opt}}(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)})$. The resulting samples, in addition to the sample generated by $\bs{\hat{f}}_{\text{MMD}}$ then can be used to train $\hat{E}$ further. Once $\hat{E}$ is updated, resolving (\ref{eq:static_PCSG}) updates the task-mapping. The repetition of this process for $T$ iterations is referred to as the dynamic approach and is summarized in Algorithm \ref{algo:dynamic-training}.

\begin{algorithm}[h]
  \caption{Dynamic Training Algorithm for $E_{\psi}$ and $\bs{f}_{\phi}$}
  \label{algo:dynamic-training}
  \small 
  \begin{algorithmic}[1]
    \Require Same inputs as Algorithm \ref{alg:static}, iteration limit $T$
    \State Perform steps 1-5 in Algorithm \ref{alg:static}, yielding $S', \hat{E}$ and $\bs{\hat{f}}_{\text{PCSG}}$

    \For{$t= 1$ ... $T$}
      \For{each $(\bs{x}^{(i)}, \omega^{(i)}) \in S$}
      \State Evaluate $\zeta_{1 \hdots K}^{(i)} = \bs{\hat{f}}_{\text{PCSG}}(\bs{x}^{(i)})$,  $\ell_{\text{opt}}^{(i)} = \ell_{\text{opt}}(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)})$ 
      \State $S' \gets S' + \{(\zeta_{1 \hdots K}^{(i)}, \omega^{(i)}, \ell_{\text{opt}}^{(i)})\}_{i=1}^{n}$ \Comment{Replay buffer: keep past $T' < T$ iterations}
      \EndFor 
  \State Update $\hat{E}$ over $S'$: $\min_{\psi} \  \frac{1}{|S'|} \sum_{\zeta_{1 \hdots K}, \omega, \ell_{\text{opt}} \in S'} (E_{\psi}(\zeta_{1 \hdots K}, \omega) - \ell_{\text{opt}} )^2$
    \State Fix $\hat{E}$ and update $\bs{f}_{\phi}$ via (\ref{eq:static_PCSG}) over $S$ using $\hat{E}$
    \EndFor
    \State \textbf{Output:} Trained task-net $\bs{\hat{f}}_{\text{PCSG}}$ and loss-net $\hat{E}$ 
  \end{algorithmic}
\end{algorithm}


Algorithm \ref{algo:dynamic-training} is similar to the approach proposed by \citet{zharmagambetov2023landscape}. In the case of scenario generation, we observe that \citet{zharmagambetov2023landscape}'s approach fails without the use of MMD regularization. Thus, the implicit use of MMD to initialize the samples for $E_{\psi}$ and regularize the task-mapping $\bs{f}_{\phi}$ constitutes a critical difference between this work and theirs. Similar to \citet{zharmagambetov2023landscape}, the proposed methods do not maintain the entire history of examples for training $E_{\psi}$ at every iteration and instead, a \textit{replay buffer} \citep{lin1992self} is used, keeping, at most, the last $T = 3$ iterations of data. Furthermore, Algorithm \ref{algo:dynamic-training}, builds upon Algorithm \ref{alg:static}, which relies upon empirical minimization of $\mathcal{L}_{\text{MMD}}(\bs{f}_{\phi})$. Figure \ref{fig:overall training} visually displays the entire training process and the relationships between DCSG, Static PCSG, and Dynamic PCSG. 
\begin{figure}[h]
  \FIGURE
  { \includegraphics[width=\textwidth]{drawing-8_mod2.eps} }
  {Visual representation of the DCSG and PCSG training procedures \label{fig:overall training}}
  {}
\end{figure}

The experimental results will evaluate the performance of these three approaches relative to each other. In terms of training, minimizing $\hat{\mathcal{L}}_{\text{MMD}}(\bs{f}_{\phi})$ has the advantage that no optimization problems are required to evaluate the loss. However, the MMD approach does not consider the problem structure of (\ref{eq:2SP-SAA}). The static and dynamic approaches have the advantage of considering the downstream decision loss associated with a particular choice of $\bs{f}$. The static approach has the benefit over the dynamic approach that the training of $E_{\psi}$, and thus the repeated solution of optimization problems to evaluate $\ell_{\text{opt}}$ need only be performed once. While the dynamic approach offers an advantage over the static approach by aiming to construct better approximations of $\ell_{\text{opt}}$ near the chosen $\bs{f}$, it also has a drawback. Specifically, computations of $\ell_{\text{opt}}$ must be performed at each iteration to update the loss-net.
