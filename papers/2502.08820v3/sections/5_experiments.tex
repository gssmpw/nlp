\section{Experiments}
This section presents results highlighting CoALM's effectiveness in unifying conversational management and advanced API calling, outperforming specialized models across both TOD and LA benchmarks.

\input{tables/0_main_results-inform-success}


\subsection{Experimental Settings}

\noindent\textbf{Evaluation Benchmarks.} We evaluate our approach on three complementary benchmarks that assess different aspects of model performance: MultiWOZ 2.4 (TOD), API-Bank (LA), and BFCL V3 (LA). Specifically, MultiWOZ 2.4~\cite{ye-etal-2022-multiwoz} is a multi-domain TOD dataset covering scenarios such as hotel booking and transportation, where we measure Success Rate and Joint Goal Accuracy (JGA); in our zero-shot setting, we rely on the test set of 999 samples, using a slightly modified AutoTOD prompt~\cite{xu-etal-2024-rethinking}. API-Bank~\cite{li-etal-2023-apibank} focuses on evaluating tool-augmented LAs through 314 tool-use dialogues and 753 API calls, tested at two levels: L-1 (invoking a known API) and L-2 (retrieving and calling from multiple candidates). Lastly, BFCL V3\footnote{\url{https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html}}~\cite{patil2023gorilla} provides over 1{,}800 test cases spanning tasks like simple, multiple, and parallel function calls, evaluated by Abstract Syntax Tree (AST) accuracy and Executable Function Accuracy. See Appendix~\ref{app:benchmark-details} for further details. %  and implementation specifics.

%\noindent\textbf{Metrics.}
\vspace{3mm}

\noindent\textbf{Baselines.} In the LA tasks, we included strong baselines like Hammer \cite{Lin2024Hammer}, ToolAce \cite{Liu2024ToolACE}, Granite \cite{abdelaziz-etal-2024-granite} which represent state-of-the-art models in agentic tasks, including OpenAI models. For MultiWOZ evaluations, we recognize that many existing TOD models are trained with classification-based supervised fine-tuning, focusing primarily on DST. Such models do not support free-form dialogue generation, nor do they exhibit broader “chat” capabilities. In contrast, our approach aims to unify both conversational (LA) and agentic (TOD) tasks into a single, generative framework. On the other hand, there are some models evaluated in zero-shot settings but as per domain JGA, rather than overall JGA. That said, we used top popular zero-shot models FNCTOD \cite{li2024largelanguagemodelszeroshot} and NC-Latent-TOD \cite{king-flanigan-2024-unsupervised} as our TOD baselines in TOD. Please see Appendix \ref{app: model-overview} for more details of these baseline models. 



\input{tables/4_api_bank}

\input{tables/3_bfcl}

\input{tables/5_ablation}



\subsection{Results on MultiWOZ}
\vspace{3mm}

\noindent\textbf{LA models struggle with TOD.} Table~\ref{tab:tod-results} summarizes results on MultiWOZ 2.4. Baseline models optimized for function calling (ToolAce, Hammer, Granite, CodeAct) achieve low Success Rate and JGA. Although these agents can call APIs effectively, they fail to track user intents across multiple sessions or deliver correct final answers to the user, except ToolAce JGA reaches 34.4\% accuracy close with domain-specific TOD models like FNCTOD.  Instruction-tuned base LLMs like Llama 3.1 8B perform moderately better on MultiWOZ, reaching a 19.9\% Success rate and 26.3\% JGA. 

\vspace{3mm}

\noindent\textbf{CoALM surpasses and generalizes in TOD.} In contrast, our smallest CoALM 8B achieves 51.6\% Success Rate, more than doubling the Success performance compared to Llama 3.1 8B and surpassing other LAs. Moreover, our CoALM 70B model achieves top results on DST with achieving 43.8\% JGA, even outperforming GPT-4o and GPT-4o-mini. This shows CoALM’s ability with coherent multi-turn state-tracking, outperforming existing baselines and domain-specific models like FNCTOD.  Notably, CoALM's strong performance is achieved without any MultiWOZ samples in its CoALM-IT training dataset, demonstrating its robustness in out-of-distribution (OOD) generalization.

\subsection{Results on API-Bank and BFCL}

\noindent\textbf{CoALM adeptly orchestrates function calls.} Table~\ref{tab:apibank} shows API-Bank scores to test model's API calling capabilities where Rouge-L is the primary evaluation metric. TOD models in the bottom row yield suboptimal results in this task. On the other hand, CoALM 8B achieves a Rouge-L score of 92.8 at Level-1 and 81.9 at Level-2, surpassing both TOD-oriented models and tool-centric LAs by a significant margin. It also achieves top performance on nearly all metrics. Moreover, we scale CoALM 8B accuracy with CoALM 70B and CoALM 405B models achieving top best and second best scores. This suggests that CoALM’s balanced approach enables it not only to retrieve and call the correct API but also to generate precise responses grounded in the returned results, fulfilling complex user requests effectively. 

\vspace{3mm}

\noindent\textbf{CoALM outperforms specialized LAs and GPT-4o.} We next assess function calling accuracy on BFCL V3 (Table~\ref{tab:bfcl}). Models trained only for TOD or basic instruction-following underperform. While LAs like Hammer and ToolAce fare better, our smallest model CoALM 8B surpasses them (see Figure \ref{fig:error-analysis} for error analysis examples). Our larger scale models outperform GPT-4o, GPT-4o-mini and Llama-3.1-405B in overall accuracy. Remarkably, CoALM 405B achieves 100\% accuracy on the relevance detection task, highlighting its agentic reasoning capabilities through hallucination. CoALM 405B stands as the top-performing fully open-source model on BFCL V3 leaderboard.

\vspace{3mm}

\subsection{Domain Impact on Performance}
Table~\ref{tab:dataset-domain} highlights the performance impact of CoALM-IT's fine-tuning components. Removing LA datasets significantly reduces function calling performance, with API-Bank Rouge-L1 dropping 47.3\% and BFCL success falling 18.3\%.
Excluding the DST dataset leads to a notable decline in CoALM's JGA, dropping by 11.0\% relative to CoALM and even underperforming base Llama by 6.9\%. This underscores the essential role of fine-tuning on state tracking to capture user intents effectively.
Finally, removing the GPT-4-generated CRA dataset has negative impact on MultiWOZ 2.4's Success metric, which plummets by 11.7\%. Also, multi-turn function calling accuracy dropped in API-Bank, both in L1 and L2 metrics. This indicates that the CRA dataset is instrumental in developing coherent and contextually aware responses in multi-turn settings. However, JGA and BFCL's overall success see slight improvements, suggesting that certain specialized skills may benefit marginally in the absence of broader conversational reasoning.
These results confirm that each dataset is crucial for balanced task performance, enabling CoALM to generalize effectively across different tasks without overfitting to one domain.