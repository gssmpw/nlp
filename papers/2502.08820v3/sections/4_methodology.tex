\section{Methodology}
Our approach, illustrated in Figure \ref{fig:summary}, develops a unified agent skilled in goal-oriented multi-turn conversations and function calling. First, we build the CoALM-IT, a broad instruction-tuning (IT) dataset that spans multiple domains, tasks, and unique reasoning structures. Next, we do fine-tuning on the proposed CoALM-IT dataset to produce CoALM; a balanced conversational agent model series capable of complex reasoning, fluent dialogue, user intent fulfillment, and function calling. 


\subsection{Conversational Agent Dataset Generation}
\label{sec:dataset-generation}
To develop a conversational agent with diverse capabilities, we created a comprehensive dataset that combines samples across multiple skills essential for both multi-turn task-oriented conversations and tool utilization. Figure \ref{fig:summary} summarizes how the dataset is created and Table \ref{tab: data_training_mixture_stats} provides detailed statistics of CoALM-IT.

\vspace{3mm}

\noindent\textbf{TOD Datasets.} An accurate dialogue system needs to master three fundamental capabilities: providing accurate information to users, fulfilling user goals, and tracking dialogue states to understand user intents and goals throughout conversations \cite{paradise}. To equip our model with these skills, we utilized the SNIPS dataset \cite{coucke2018snips}, originally designed for language understanding but repurposed for single-turn dialogue state tracking (DST). We extracted its training split and converted it into the state tracking IT format by crafting a detailed instruction prompt, as illustrated in Figure \ref{tab:snips-dst}. This transformation resulted in a training set of 24,542 samples for effective DST.
%The main purpose of this domain corpus is to ensure that the model can accurately interpret user queries and derive corresponding dialogue states.

\vspace{3mm}

\noindent\textbf{Function Calling Datasets.} Tool calling capability is the ability to select appropriate APIs and access external knowledge, which is crucial in modern LAs. An effective agent must not only choose the correct API but also provide properly typed parameters (e.g., integers or strings) and manage complex scenarios involving sequential or parallel function calls. To develop these skills, we incorporated datasets from two state-of-the-art agent models: Hammer \cite{Lin2024Hammer} and ToolACE \cite{Liu2024ToolACE}. Hammer's training dataset incorporates random noise by replacing function and parameter names to prevent overfitting (see Figure \ref{fig:summary}), forcing the model to reason about API functionality through provided descriptions rather than memorizing specific identifiers. ToolACE provides multi-turn conversational scenarios in open-domain settings, where function calls may occur across multiple turns, but no database is provided. We post-process these datasets by incorporating the prompt instructions and adding conversation history if available. As reported in Table \ref{tab: data_training_mixture_stats}, the combined API calling corpus contains 216,319 samples. A function calling training sample for the Hammer dataset can be seen in Figure \ref{tab:hammer-api-2}.  % Can be shortened more

\vspace{3mm}

\noindent\textbf{Conversational ReAct-based API Calling (CRA) Dataset.} While state tracking enables the understanding of user intent and function calling provides external knowledge access, integrating these capabilities within multi-turn task-oriented conversations requires additional reasoning about when to make API calls and how to interpret their results. Our primary contribution is a completely new User and Agent conversation structure as \textbf{User-Thought1-Action-Observation-Thought2-Response}. Starting from multi-turn SGD dataset \cite{rastogi2020sgd}, we systematically transform each turn to include two distinct reasoning steps (Thought1 and Thought2) and potential API calls (Action and Observation), extending traditional ReAct format \cite{yao2023reactsynergizingreasoningacting-react} by incorporating GPT-4o for content generation (Figure \ref{fig:summary} top row). Our structure includes two main parts: (i) \textbf{User-Thought1-Action}, which focuses on understanding the user's intent with reasoning and invoking the right API, if necessary (Figure \ref{tab:sgd-sft-action} bottom). (ii) \textbf{Observation-Thought2-Response}, where the agent analyzes the returned observations and formulates an appropriate response to the user (Figure \ref{tab:sgd-sft-response} bottom). This transformation is achieved with a carefully designed prompt in Table \ref{tab: prompt}, which enforces strict “Role Definition”, “Task Information”, and “Output Format”. Since CRA is generated via GPT-4o \cite{Achiam2023GPT4TR}, it is also validated by human evaluators (Appendix \ref{human-validation}). Best of our knowledge, this is the first ReAct-based Conversational API dataset that incorporates multiple intermediate reasoning steps in multi-turn settings for TOD. This process yielded 82,236 samples, specifically tailored for task-oriented domains such as hotel bookings and restaurant reservations. 

\vspace{3mm}

We merge all three datasets into a single training set called CoALM-IT, please refer to Table \ref{tab: data_training_mixture_stats} for details. We fine-tune our CoALM models on this merged dataset in one pass. By interleaving samples from TOD, LA, and CRA, the model continuously practices different conversational skills without overfitting to any single domain or task type.

\subsection{Fine-tuning Towards Conversational Agents}
We followed a multitask fine-tuning approach to develop CoALM models' diverse capabilities across TOD, function calling, and multi-turn reasoning by training on CoALM-IT. Our training process is structured to target specific skills through different optimization objectives \textbf{completely in zero-shot settings}, as our CoALM-IT dataset does not contain any of the evaluation benchmark training sets.

\vspace{3mm}

\noindent\textbf{Multitask Fine-tuning.}  As described in Section \ref{sec:dataset-generation} and illustrated in Figure \ref{fig:summary}, our CoALM-IT dataset combines samples from three distinct domains, each designed to cultivate a specific skill: (i) TOD (Task-Oriented Dialogue) for strengthening dialogue state tracking, (ii) LA (Language Agent) for teaching the model when and how to invoke function calls, and (iii) ReAct for multi-turn conversation, multi-step reasoning and function calling.

For TOD, we augment SNIPS data with prompt instructions (Figure \ref{tab:snips-dst}), training the model to generate structured dialogue states in response to user queries. For function calling (LA), we optimize CoALM to select the correct APIs and produce accurate function calls with proper parameter types (Figure \ref{tab:hammer-api-2}), emphasizing reasoning over memorized patterns. We then address complex multi-turn conversations with API integration using our CRA dataset, formatted in the ReAct style. This stage uses two objectives: (1) action prediction (Figure \ref{tab:sgd-sft-action}), where the model learns to issue the appropriate function call given the conversation history, and (2) response generation (Figure \ref{tab:sgd-sft-response}), where it synthesizes coherent replies based on both API results and intermediate reasoning steps. Rather than merely producing answers, the model learns to reason, decide, and act in multiple stages before arriving at a final response. Notably, we trained our models on CoALM-IT by interleaving TOD, LA, and CRA samples, enabling the model to continuously practice diverse conversational skills while avoiding overfitting to any single domain or task type.

\vspace{3mm}

\noindent\textbf{Training Details.} We developed the CoALM model series by fine-tuning Llama 3.1 8B, Llama 3.3 70B, and Llama 3.1 405B \cite{Dubey2024TheL3-llama3} using a consistent Alpaca (Instruction-Input-Output) format. To balance efficiency and model quality, we applied LoRA \cite{hu2021lora} rank (r) = 16 and scaling factor ($\alpha$) = 32 to all linear layers, and trained in mixed-precision bfloat16 (bf16) on 8 NVIDIA H100 GPUs. Under these settings, CoALM 8B required approximately 8 hours of training, while CoALM 70B took about 60 hours. We used a global batch size of 8, trained for 3 epochs with a learning rate of $1e-4$, and employed a linear warm-up schedule with a 0.1 ratio. For CoALM 405B, we fine-tuned Llama 3.1 405B and using QLoRA \cite{dettmers2023qloraefficientfinetuningquantized} with the same rank and scaling factor using bitsandbytes \cite{bitsandbytes} with a quantization type of normalized float 4 (nf4). The precise  training configurations for CoALM 8B, CoALM 70B and CoALM 405 are included in the HuggingFace pages. Our training pipeline leveraged the Oumi framework\footnote{\url{https://github.com/oumi-ai/oumi}} to ensure reproducibility and streamlined management \cite{oumi2025}.


