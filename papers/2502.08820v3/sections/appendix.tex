\section{Problem Formulation}
\label{app: problem-formulation}

\subsection{End-to-End TOD Systems with LLMs} LLM-based end-to-end TOD systems generate contextually relevant responses based on dialogue history and task instructions. Let $F$ be a language model parameterized by $\theta$, which maps an input context given as prompt $T$ to an output system response $y_t$. At each dialogue turn $t$, the system receives three key components: task instructions $G$, dialogue history $H_t$ comprising of prior user-system interactions $\{(u_1, y_1), ..., (u_{t-1}, y_{t-1})\}$, and the current user input $u_t$. These elements are combined to form the complete prompt $T_t = (G, H_t, u_t)$. The model generates a response $y_t$ by modeling the conditional probability:
\begin{equation}
    P(y_t|T_t; \theta) = P(y_t|G, H_t, u_t; \theta),
\end{equation}
where $P(s_t|T_t; \theta)$ denotes the probability of generating the response $y_t$ given the prompt $T_t$ and the model parameters $\theta$. The dialogue progresses by updating the history after each turn $H_{t+1} = H_t + {[(u_t, s_t)]}$, maintaining the sequential nature of the interaction while preserving task orientation through $G$.

%Language models with tool-calling capabilities, known as Language Agents, facilitate structured interactions with external tools, APIs, or databases.
\subsection{Function Calling with Language Agents} A language model $F_\theta$ maps an input $x = (G, u, \Omega)$, where $G$ is the task prompt, $u$ is the user query, and $\Omega = \{f_1, \dots, f_n\}$ is the set of available functions with their arguments and descriptions to a structured function call $y$. The model generates target function call in a structured format, such as JSON or text schema. The generation probability is defined as:
\begin{equation}
P(y \mid x; \theta) = P(y \mid G, u, \Omega; \theta)
\end{equation}
This formulation enables the model to translate natural language inputs into precise and well-structured function calls, facilitating seamless integration with external systems.

\paragraph{ReAct Prompting.} ReAct \cite{yao2023reactsynergizingreasoningacting-react} integrate reasoning and action-taking to enable more effective decision-making. It facilitates intermediate reasoning by breaking down complex tasks into smaller, interpretable reasoning steps. Additionally, it enables interaction with external tools or APIs by producing structured actions that integrate effectively with external systems. As a result of an API execution, ReAct incorporates observations dynamically, adapting subsequent reasoning and actions based on the results of previous steps, thus improving the system's responsiveness and overall task performance.

\section{Details of the Evaluation Benchmarks}
\label{app:benchmark-details}

\paragraph{MultiWOZ 2.4.} MultiWOZ 2.4 \cite{ye-etal-2022-multiwoz} is a multi-domain TOD dataset designed to evaluate dialogue systems' ability to handle complex conversations across multiple domains such as hotel booking, restaurant reservations, and transportation. We employ two different metrics during our TOD evaluations MultiWOZ: \textit{Success Rate}, which assesses whether all user-requested information related to the entity is successfully provided and Joint Goal Accuracy (JGA) which measures the accuracy of predicted dialogue states, reflecting the system's ability to track user intents. During our zero-shot evaluations, we used its test set that contains 999 samples and incorporated AutoTOD prompt \cite{xu-etal-2024-rethinking} with slight modifications, thereby generating system responses analogous to those produced in a chat-based inference setting.

\paragraph{API-Bank.} API-Bank \cite{li-etal-2023-apibank} is designed to evaluate tool-augmented LAs, focusing on their ability to plan, retrieve, and invoke APIs effectively. It includes 314 tool-use dialogues and 753 API calls, with two evaluation levels: Level 1 (L-1), which tests the accuracy of invoking a known API based on a given query, and Level 2 (L-2), which assesses the retrieval and invocation of APIs from a candidate list, simulating real-world scenarios with multiple API options. By addressing these challenges, API-Bank advances the understanding and enhancement of tool-augmented reasoning in LLMs. During evaluations, we used the official evaluation code from the repository of previous works \cite{Lin2024Hammer}.

\paragraph{Berkeley Function Calling Leaderboard.} In addition to API-Bank, we also used BFCL V3\footnote{\url{https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html}} \cite{patil2023gorilla} which provides a diverse evaluation framework for assessing the models' ability to perform function calls across various objectives. It includes more than 1,800 test cases that span tasks such as simple functions, multiple functions, and parallel functions for Python and other environments such as REST APIs and JavaScript. Models are evaluated using two primary metrics: (i) Abstract Syntax Tree (AST) accuracy, which ensures syntactic correctness by verifying function structures, parameters, and types against predefined documentation and (ii) Executable Function Accuracy, which evaluates whether generated functions execute correctly and produce the expected outputs, emphasizing real-world applicability. In our experiments, we employed the official repository released by authors and followed the provided instructions to get model results.



\section{Baseline Model Overviews Used in Experiments}
\label{app: model-overview} 
In this section, we provide an overview of the models used in our experiments, including their brief descriptions, checkpoints, and the training re-production code references.

\subsection{Base Models}

\paragraph{Llama 3.1.} The Llama (Large Language Model Meta AI) \cite{Dubey2024TheL3-llama3} family is a set of open-source language models from Meta AI, ranging from 7 to 405 billion parameters. It trained on a large corpus of web content, academic texts, and books, they excel at reasoning, question-answering, and code generation. Their architecture supports efficient fine-tuning and deployment. In our experiments, we use Llama-3.1-8B-Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}, released in July 2024, which offers improved multilingual capabilities, longer context windows, and state-of-the-art performance in general knowledge, math, and tool usage

\paragraph{Mistral v03.} Mistral 7B \cite{jiang2023mistral} is one of the state-of-the-art, open-source LLMs produced by Mistral AI. It employs innovative mechanisms such as grouped-query and sliding window attention, which enable efficient processing of longer sequences and faster inference times. In our experiments, we use Mistral-7B-Instruct-v0.3\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}}, released on May 22, 2024, and available on Hugging Face.


\subsection{TOD Models}
 
\paragraph{LDST.} LDST (LLM-driven Dialogue State Tracking) \cite{feng2023towards} is an approach that overcomes the limitations of proprietary models in state tracking by leveraging a fine-tuned LLaMa 7B model. The approach combines a novel assembled domain-slot instruction tuning technique with parameter-efficient strategies, enabling resource-efficient performance that tries matches larger models. During our experiments and to fine-tune LDST we used the provided checkpoints and implementation details for LDST are available in their public repository \footnote{\url{https://github.com/WoodScene/LDST}}.


\paragraph{Fnc-TOD.} FNC-TOD (Function-Calling for Task-Oriented Dialogue) focuses on DST in LLMs through function calling mechanisms. The method conceptualizes domain schemas as functions and embeds function specifications within the system prompt. This approach achieved improved conversation state tracking in task-oriented dialogues using a fine-tuned Llama-2-13b-chat-hf model, trained on a focused dataset of 7,200 task-oriented dialogues spanning 36 domains. For our experiments, we utilized the authors' publicly released Zekunli/FncTOD-Llama-13b model available on Huggingface \footnote{\url{https://huggingface.co/Zekunli/FncTOD-Llama-13b}}.


\paragraph{NC-Latent-TOD.} This work introduces an unsupervised approach to TOD systems that operates solely with API schemas and unlabeled dialogues, eliminating the need for costly turn-level annotations. The system generates pseudo-labels for API calls and system actions while using a Hard-Expectation maximization approach with LLM predictions for iterative fine-tuning, enhanced by a noisy-channel reranking method \cite{king-flanigan-2024-unsupervised}. During our experiments, we used two different models nc-latent-tod-step-2-final\footnote{\url{https://huggingface.co/Brendan/nc-latent-tod-step-2-final}} and tod-zero-bqag3oyb-32000\footnote{\url{https://huggingface.co/Brendan/tod-zero-bqag3oyb-32000}} shared by the authors.


\subsection{Language Agents}

\paragraph{CodeAct-Agent.} CodeAct \cite{wang2024executable} is a framework that enables LLM agents to generate and execute Python code as actions to interact with environment, rather than being limited to JSON or structured text formats. By integrating a Python interpreter, it allows agents to dynamically adjust their actions based on execution results, leverage existing Python packages, and utilize programming constructs like loops and conditionals for complex operations. authors developed CodeActAgent by fine-tuning both Mistral 7B and Llama2 7B models on the CodeAct-Instruct dataset. For our experiments, we utilized the authors' officially released CodeActAgent-Mistral-7b-v0.1 model, available on Huggingface \footnote{\url{https://huggingface.co/xingyaoww/CodeActAgent-Mistral-7b-v0.1}}.

\paragraph{Granite-20B.} This work introduces Granite-20B, an open-source LLM, specifically designed for function calling capabilities. The model is trained using a multi-task approach on seven core function calling tasks: Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation. We used the offical model weights granite-20b-code-instruct-8k provided in Huggingface\footnote{\url{https://huggingface.co/ibm-granite/granite-20b-code-instruct-8k}}.

\paragraph{Hammer2.0-7B.} Hammer \cite{Lin2024Hammer} is a small scale model family up to 7B parameter models designed for on-device function calling and addresses generalization challenges in function calling through two key innovations: an irrelevance-augmented dataset that enhances models' ability to identify inappropriate functions, and a function masking technique that reduces naming-based misinterpretations by focusing on function descriptions. Built by fine-tuning the xLAM-function-calling dataset\footnote{\url{https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k}} with 7,500 additional instances for irrelevance detection, Hammer achieves state-of-the-art performance on BFCL Benchmark. For our experiments, we utilized the official Hammer 2.0 model weights available on Huggingface\footnote{\url{https://huggingface.co/MadeAgents/Hammer2.0-7b}}, along with training it from scratch for reproducibility using provided public repository and training scripts\footnote{\url{https://github.com/MadeAgents/Hammer}}.

\paragraph{ToolAce 8B.} This work introduces ToolACE \cite{Liu2024ToolACE}, an automated pipeline for generating high-quality function-calling training data. The system features a self-evolution synthesis process that curates a pool of 26,507 diverse APIs, coupled with a multi-agent dialogue generation system and a dual-layer verification process for ensuring data accuracy. Using data generated and fine-tuning on Llama-3.1-8B-Instruct, ToolACE achieve top results on the BFCL Leaderboard. We used the official Huggingface checkpoint\footnote{\url{https://huggingface.co/Team-ACE/ToolACE-8B}} and dataset\footnote{\url{https://huggingface.co/datasets/Team-ACE/ToolACE}}.

\section{Human Validation for Generated CRA Dataset}
\label{human-validation}
To analyze the quality of generated conversations, we implemented a systematic random sampling approach. From the generated dataset, we randomly selected 100 dialogue instances for validation. We conducted the evaluation against a predefined set of 51 available functions, covering transportation, booking, entertainment, and utility services. We scrutinized each function’s schema, including its parameters and expected usage, to ensure compliance. We asked a senior Computer Science student to evaluate these generated samples across four key dimensions:
\begin{itemize}
    \item \textbf{Undefined Function Call:} Validating API names and parameters against the predefined function list to identify undefined functions or invalid arguments.
    \item \textbf{Incorrect Argument Type:} Checking argument structures to ensure compliance with the function schemas.
    \item \textbf{Argument Hallucination:} Detecting unnecessary or irrelevant arguments misaligned with the conversation context.
    \item \textbf{Low-Quality Reasoning and Planning:} Identifying logical gaps in though steps or unnecessary API calls in ReAct structure.
\end{itemize}
We asked for a binary score (1 for no errors, 0 for detected issues) for each generated dialogue and provided mandatory feedback for any errors. Our evaluation of 100 dialogues showed a 9\% error rate, mostly in restaurant reservations where key details like the restaurant name or dining time were missing. These errors stemmed from Argument Hallucination and Low-Quality Reasoning. Results, including dialogue IDs, scores, and feedback, were systematically collected to identify areas for improvement.



% \section{Further Details About Experimental Setup and Evaluations}
% \label{app: eval-setup}
% TODO

% \subsection{MutliWOZ 2.4}
% TODO

% \subsection{Berkeley Function Calling Leaderboard (BFCL) V3}
% TODO

% \subsection{API-Bank}
% TODO

% \subsection{Qualitative Error Analysis}
% TODO

\begin{figure*}[t!]
\includegraphics[width=\textwidth]{figures/fig-error-api.pdf}
\caption{\textbf{Error Analysis of Function-Calling Results.} Illustrated performance comparison on function calling benchmarks API-Bank L1 (top) and BFCL V3 parallel function call (bottom). Results demonstrate CoALM's consistent performance compared to other baselines.}
\label{fig:error-analysis}
\end{figure*}
\clearpage

