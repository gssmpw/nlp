\section{Conclusion and Future Work}
In this work, we highlighted a critical gap between LA and TOD systems, where each excels in complementary capabilities - function calling and multi-turn conversation management, respectively. To solve this, we introduced CoALM, unified conversational agents that seamlessly integrates sophisticated API usage with natural multi-turn dialogue. 
Through fine-tuning on CoALM-IT with a hybrid fine-tuning strategy, CoALM achieves leading performance on both TOD and LA benchmarks, demonstrating that a single model can indeed master multi-turn conversations and tool use effectively. 

Future work can investigate using reinforcement learning (RL) to generate large-scale interaction trajectories supported with API calls could further enhance the self-evolution of conversational agents through purely RL-based optimization. 
Another direction is, improving multi-turn function calling and user interaction abilities of these models, which remains a difficult problem with generally low accuracy. 
We believe that our findings, methodologies, and published resources will foster future research to create more capable and versatile conversational systems.

%By addressing the question \textit{Can a Single Model Master Both Multi-turn Conversations and Tool Use?}, we tried to establish a new paradigm for developing unified conversational agents. 
%We believe that our findings, methodologies, and published resources will foster future research to create more capable and versatile conversational systems.