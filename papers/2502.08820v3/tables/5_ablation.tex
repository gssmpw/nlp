\begin{table*}[!t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l cc c cc c c}
\toprule
& \multicolumn{2}{c}{\textbf{TOD Task}} & & \multicolumn{4}{c}{\textbf{Function Calling Tasks}}  \\ \cmidrule{2-3} \cmidrule{5-8} 
& \multicolumn{2}{c}{\textbf{MultiWOZ 2.4}} & & \multicolumn{2}{c}{\textbf{API-Bank}}               & & \multicolumn{1}{c}{\textbf{BFCL-V3}} \\
  \cmidrule{2-3}
  \cmidrule{5-6}
  \cmidrule{8-8}
 Model & Success & DST &  & Rouge-L1 & Rouge-L2&  & Overall Success \\ \midrule
%\multicolumn{12}{c}{\textit{Open-source LLMs (LLaMA-3.1-based)}} \\
\textcolor{darkgreen}{}
%Llama 3.1 Base       & 8B & TODO  & TODO  & TODO & TODO  &  & TODO  & TODO &  & TODO  & TODO   \\
Llama 3.1 8B Instruct        & 19.9  & 26.3  &  & 72.7  & 75.2 &  &49.8    \\
$\;\;$ + CoALM-IT w/o LA     & 46.0 \small{($\textcolor{darkgreen}{26.1}\uparrow, \textcolor{red}{5.6}\downarrow$)}   \textcolor{red} & 28.5 \small{($\textcolor{darkgreen}{2.2}\uparrow, \textcolor{red}{1.9}\downarrow$)}  &  & 45.5 \small{($\textcolor{red}{27.2}\downarrow, \textcolor{red}{47.3}\downarrow$)} & 48.8 \small{($\textcolor{red}{26.4}\downarrow, \textcolor{red}{33.1}\downarrow$)} &  & 35.4 \small{($\textcolor{red}{14.4}\downarrow, \textcolor{red}{18.3}\downarrow$)}   \\
$\;\;$ + CoALM-IT w/o TOD   & 42.0 \small{($\textcolor{darkgreen}{22.1}\uparrow, \textcolor{red}{9.6}\downarrow$)}  & 19.4 \small{($\textcolor{red}{6.9}\downarrow, \textcolor{red}{11.0}\downarrow$)}  &  & 92.7 \small{($\textcolor{darkgreen}{20.0}\uparrow, \textcolor{red}{0.1}\downarrow$)}     & 78.9 \small{($\textcolor{darkgreen}{13.7}\uparrow, \textcolor{red}{3.0}\downarrow$)} &  & 54.1 \small{($\textcolor{darkgreen}{4.3}\uparrow, \textcolor{darkgreen}{0.4}\uparrow$)}   \\
$\;\;$ + CoALM-IT w/o CRA    & 50.0 \small{($\textcolor{darkgreen}{30.1}\uparrow, \textcolor{red}{1.6}\downarrow$)}  & \textbf{34.5} \small{($\textcolor{darkgreen}{8.2}\uparrow, \textcolor{darkgreen}{4.1}\uparrow$)}  &  & 91.3 \small{($\textcolor{darkgreen}{18.6}\uparrow, \textcolor{red}{1.5}\downarrow$)}  & 78.8 \small{($\textcolor{darkgreen}{3.6}\uparrow, \textcolor{red}{3.1}\downarrow$)} &  & \textbf{56.6} \small{($\textcolor{darkgreen}{10.6}\uparrow, \textcolor{darkgreen}{2.9}\uparrow$)}      \\ \midrule
CoALM 8B                         & \textbf{51.6} & 30.4  &  & \textbf{92.8}  & \textbf{81.9} &  & 53.7   \\ \bottomrule

\end{tabular}
}

\caption{\textbf{Dataset Domain Effects.}
Experimental results highlighting the impact of excluding specific domain datasets during CoALM fine-tuning. \textbf{w/o} indicates excluding the corresponding dataset during fine-tuning. Each row displays performance changes in parentheses with respect to base model (Llama) and final model (CoALM), i.e. ($\Delta$ Llama, $\Delta$ CoALM). Performance gains are highlighted in \textbf{\textcolor{darkgreen}{green}}, while drops are marked in \textbf{\textcolor{red}{red}}.
}
\vspace{-1mm}
\label{tab:dataset-domain}
\end{table*}
