\begin{table*}[!t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l r r r |c c |c  c c c c }
\toprule
\textbf{Model}    & \textbf{Base Model}  & \textbf{\# Samples}   & \textbf{\# domains}  &  \textbf{Multi-Turn}  &  \textbf{Multi-Step}  & \textbf{Zero/Few-shot}   & \textbf{CoT}   & \textbf{SC} & \textbf{ReAct}  & \textbf{Reflexion}      \\ \midrule
Agent-Tuning      & LLaMA2 7b-13b-70b     & 96,011               &  -                   &   \cmark              &  \xmark               &  \cmark                  &  \xmark        &  \xmark     &  \cmark         &  \xmark                \\
FireAct           & LLaMA2 7b-13b, GPT-3.5 & 1,000               &  -                   &   \xmark              &  \cmark               &  \xmark                  &  \cmark        &  \xmark     &  \cmark         &  \cmark                 \\
CodeAct           & Llama-2 7b, Mistral 7b & 78,385              &  -                   &   \cmark              &  \xmark               &  \xmark                  &  \xmark        &  \xmark     &  \cmark         &  \xmark                \\ 
Agent-Flan        & LLaMA2 7b-13b-70b     & 24,703               &  -                   &   \cmark              &  \xmark               &  \cmark                  &  \cmark        &  \xmark     &  \cmark         &  \xmark                \\ 
Gorilla           & LLaMA2 7b            & 15,218                &  3                   &   \xmark              &  \xmark               &  \cmark                  &  \xmark        &  \xmark     &  \xmark         &  \xmark                \\ 
ToolLLM           & LLaMA2 7b            & 126,486               &  49                  &   \xmark              &  \cmark               &  \cmark                  &  \xmark        &  \xmark     &  \cmark         &  \xmark                \\ 
ToolAlpaca        & Vicuna 7b-13b        & 3,938                 &  50                  &   \cmark              &  \xmark               &  \xmark                  &  \xmark        &  \xmark     &  \cmark         &  \xmark                \\ \midrule
ToolAce           & Llama 3.1-8B-Instruct & 11,300               &  390                 &   \cmark              &  \cmark               &  \xmark                  &  \xmark        &  \cmark     &  \xmark         &  \xmark                \\ 
xLAM              & DeepSeek-Coder-7B-instruct-v1.5 & 60,000     &  21                  &   \cmark              &  \xmark               &  \xmark                  &  \xmark        &  \xmark     &  \cmark         &  \xmark                \\
Hammer            & qwen2-1.5B-4B-7B     & 202,500               &  21                  &   \xmark              &  \cmark               &  \cmark                  &  \xmark        &  \xmark     &  \xmark         &  \xmark                \\ \bottomrule
\end{tabular}
 }
\caption{\textbf{Agent Fine-tuning Datasets.} Instruction datasets}
\label{tab: agernt-it}
\end{table*}

% evaluation
% Agent-Tuning: ALFWorld, WebShop, Mind2Web, AgentBench, SciWorld, HitpotQA, WebArena, MMLU, HumanEval, GSM8K, MT-Bench
% FireAct: HotpotQA, Bamboogle, StrategyQA, MMLU
% CodeAct: MINT, Miniwob++, SciWorld, MMLU, HymanEval, GSM8K, MTBench
% Agent-Flan: HotpotQA, SciWorld, WebArena, T-Eval
% Gorilla: Introduce Llama-based finetuned llm for writing API calls and APIBench consisting Huggingface, TorchHub, and TensorHub APIs. Eval only on APIBench
% ToolLLM: propose a framework for geenral tool use including data generation (self-instruct ToolBench), training (llama finetune) and evaluation (ToolEval and APIBench).
% ToolAlpaca: none    
% ToolAce: BFCL-v1-v2, MMLU, HumanEval, GSM8K, CommenSenseQA. Also define LLM function calls as Parallel and Dependent (sequential). Uses SC. In User-Tool-Assistant Format. It can be used if Thought step added. No thought
% xLAM: Webshop, ToolQuery, ToolBench, BFCL-v2      
% Hammer: BFCL-v2, API-Bank, Nexus Raven API Evaluation, Tool-Alpaca, Sea-Tools

% Ours: BFCL-v2, API-Bank, ToolBench, MultiWOZ.