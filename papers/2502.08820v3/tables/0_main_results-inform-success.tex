\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{l c c}
\toprule
\textbf{Method}                                   & \textbf{Success} & \textbf{JGA}    \\ \midrule
CoALM 8B (ours)                                    & 51.6             & 30.4            \\
CoALM 70B (ours)                                   & 69.4             & \textbf{43.8}   \\
CoALM 405B (ours)\textbf{$^*$}                                & 66.7             & 38.8             \\ \midrule
Hammer 2.0 7B                                     & 23.5             & 21.7            \\
ToolAce                                           & 18.0             & 34.4            \\ 
Granite-20B-Code                                  & 10.7             & 21.8            \\ 
CodeActAgent                                      & 9.5              & 20.2             \\ 
Llama 3.1 8B Instruct                             & 19.9             & 26.3             \\ 
Llama 3.3 70B Instruct                            & 67.6             & 40.8             \\ 
Mistral-7B-Instruct-v0.3                          & 31.2             & 27.0             \\ 
%Fnc-TOD 13B                                      & 2.6              & 2.7             \\
FNCTOD \cite{li2024largelanguagemodelszeroshot}   & 44.4             & 37.9             \\
NC-Latent-TOD \cite{king-flanigan-2024-unsupervised} & 68.3          & 39.7             \\
GPT 3.5 Turbo \cite{hudecek-dusek-2023-large}     & -                & 13.5             \\
%Llama-3.1 405b                                   & 79.4             & 43.1             \\ 
GPT4o-mini                                        & 69.9             & 38.4             \\
GPT4o                                             & \textbf{75.5}             & 36.9             \\ \bottomrule
\end{tabular}
}
\caption{\textbf{MultiWOZ 2.4 Benchmark Results.} Performance comparison across models on MultiWOZ 2.4 dialogue benchmark. Best scores are highlighted with \textbf{bold}. The asterisk (*) on CoALM 405B denotes the checkpoint from one completed epoch, as the model is still under training.}
\vspace{-5mm}
\label{tab:tod-results}
\end{table}

 %*ReActTOD 8B (SnipsState-SGD-Hammer-toolace)

 %Qwen2.5-7B-Instruct                          & 71.6            & 60.7             & 3.50          & TODO             \\ \midrule


% \begin{table}[!t]
% \centering
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{l c c c}
% \toprule
% \textbf{Method}                                  & \textbf{Inform} & \textbf{Success} & \textbf{JGA}    \\ \midrule
% CoALM 8B (ours)                                   & 64.8            & 51.6             & 30.4            \\
% CoALM 70B (ours)                                  & 80.6            & 69.4             & \textbf{43.8}   \\
% CoALM 405B (ours)                                 & 80.6             & 66.7            & 38.82   \\ \midrule
% Hammer 2.0 7B                                    & 54.1            & 23.5             & 21.7            \\
% ToolAce                                          & 42.0            & 18.0             & 34.4            \\ 
% Granite-20B-Code                                 & 22.0            & 10.7             & 21.8            \\ 
% %Fnc-TOD 13B                                     & 22.1            & 2.6              & 2.7             \\
% FNCTOD \cite{li2024largelanguagemodelszeroshot}  & -           & 44.4             & 37.9             \\
% Llama 3.1 8B Instruct                            & 49.8            & 19.9             & 26.3             \\ 
% Llama 3.3 70B Instruct                           & 78.7            & 67.6             & 40.8             \\ 
% Mistral-7B-Instruct-v0.3                         & 56.2            & 31.2             & 27.0             \\ 
% CodeActAgent                                     & 22.1            & 9.5              & 20.2             \\
% %Llama-3.1 405b                                  & 88.0            & 79.4              & 43.1             \\ 
% GPT4o-mini                                       & 81.1            & 69.9             & 38.4             \\
% GPT4o                                            & \textbf{85.3}   & \textbf{75.5}             & 36.9             \\ \bottomrule
% \end{tabular}
% }
% \caption{\textbf{MultiWOZ 2.4 Benchmark Results.} Performance comparison across models on MultiWOZ 2.4 dialogue benchmark. Our proposed CoALM model results are shown in the first row, followed by public baseline models in the second row, and final row presents reference scores from proprietary models. Best scores for open-source models are highlighted with \textbf{bold}.}
% \vspace{-5mm}
% \label{tab:tod-results}
% \end{table}