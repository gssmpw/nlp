\section{Baseline CED Framework}
This section is organized as follows: First, we introduce the online CED pipeline, which serves as the foundation for the subsequent parts. Second, to evaluate \emph{Hypothesis I}, we propose various neural and neurosymbolic baseline architectures. Third, we describe the construction and details of our CED dataset. Finally, we present experimental results to validate our hypothesis.

\subsection{Pipeline Overview}
We propose a two-module online processing system. As is shown in Fig.~\ref{fig:ced-pipeline}, the system takes sensor streams as inputs and applies a non-overlapping sliding window of size $W$ to process data streams into segments. Those segments $\{s_t\}_{t=1}^T$ are forwarded to the following two modules:

\begin{figure}[tb]
    \centering
        \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{0.cm}
    \includegraphics[width=0.95\columnwidth]{figs/ced_pipeline.png}
    \caption{Overview of the online CED pipeline.}
    \label{fig:ced-pipeline}
    \vspace{-1em}
\end{figure}

\textbf{Pretrained Feature Encoder.}
This component encodes each window segment of sensor data $s_t$ into a hign-dimensional embedding vector $v_t$, producing a time sequence of embedding vectors $\{v_t\}_{t=1}^T$ for all windows, which is then passed to the downstream module. The encoder is a pretrained model designed to extract latent features from raw sensor data. After pretraining, the encoder is frozen and shared across all complex event detector models. This ensures fair comparisons of those models by providing consistent sensor data embeddings for \emph{CE} pattern reasoning.

\textbf{Complex Event Detector.}  
This module takes the embedding vectors \(\{v_t\}_{t=1}^T\) as input to detect \emph{CE} patterns. Its output is an online \emph{CE} label sequence \(\{y_t\}_{t=1}^T\), where \(y_t\) is the \emph{CE} label at window \(t\). This component is responsible for reasoning about complex events and has both neural and neurosymbolic alternatives for baseline architecture comparison.

Let the pretrained feature encoder be denoted as \(m\), and the complex event detector as \(g\). The online CED task objective, as defined in Eq.~\ref{eq:1}, becomes:
\begin{equation}
    \min |\hat{y_t} - y_t|, \quad \textrm{where } \hat{y_t} = g\left(m\left(\mathbf{D}_t\right)\right), \quad 1 \leq t \leq T.
\end{equation}
Here, the pretrained feature encoder \(m\) remains consistent, while the complex event detector \(g\) is varied to facilitate comparisons among different baseline architectures.

\subsection{Baseline Architectures}
\subsubsection{Requirements}
We design various neural and neurosymbolic architectures to serve as the complex event detector in Fig.~\ref{fig:ced-pipeline}. They must meet the following requirements:

\textbf{Causal structure.}
Online CED requires the system to predict complex events at each time step $t$ using only information from previous observed timestamps (0 to $t$). Models must have a causal structure to ensure they do not access future information.

\textbf{Minimum receptive field.} 
For neural network architectures, the receptive field or context window of each model must be larger than the longest temporal patterns of complex events in the training data. This ensures the model can ``see'' the full pattern of complex events.

\subsubsection{Alternatives}\label{sec:alternatives}
We compare models from three architecture types: 

\textbf{End-to-end Neural Architecture.} 
Models that belong to this type directly take the high-dimensional sensor embedding vectors from the pretrained feature encoder. They will be trained end-to-end using the sensor embedding sequences and \emph{CE} labels. We choose a (1) \emph{Unidirectional LSTM} \cite{hochreiter1997long}, (2) a \textit{Causal TCN}\cite{bai2018tcn} that masks information from future timestamps in convolutional operations, (3) a \textit{Causal Transformer Encoder} with a triangular attention mask to restrict the model's self-attention to previous timestamps only, excluding information from future timestamps, and (4) a state-space model \emph{Mamba} \cite{gu2024mamba}. Please check Appendix~\ref{sec:baseline-models} for more model details.

\textbf{Two-stage Concept-based Neural Architecture.} The difference between this type and the previous one is that they contain a neural \emph{AE} classifier that maps each window of sensor embedding vector to a most probable \emph{AE} class. Then the sequence of concept \emph{AE}s will be passed to various neural backbone models to detect \emph{CE} patterns. The models of this type are denoted as \textbf{\emph{Neural AE + X}}, where \emph{\textbf{X}} stands for the backbone models. We consider \emph{Neural AE + LSTM}, \emph{Neural AE + TCN}, \emph{Neural AE + Transformer} and \emph{Neural AE + Mamba}. The backbone models are almost the same as those in the previous architecture, except that they take the one-hot embedding vectors, the \emph{AE} concept trace, as inputs.

\textbf{Neurosymbolic Architecture.} Unlike previous architectures, we design a neurosymbolic model that integrates prior knowledge of \emph{CE} rules, \emph{Neural AE + FSM}, \emph{Neural AE + FSM}. This model uses the same neural \emph{AE} classifier, outputting the most probable \emph{AE} label as a one-hot embedding. The resulting \emph{AE} concept trace is then processed by a user-defined symbolic reasonerâ€”a finite state machine (FSM) for each complex event rule. We also explored a probabilistic FSM in ProbLog~\cite{problog}, which leverages softmax embeddings from the \emph{AE} classifier for probabilistic reasoning over event sequences. However, this approach yielded only marginal improvements in preliminary evaluations and was abandoned for now due to its design complexity and reliance on expert intervention.




\subsubsection{Training Loss}
Due to the nature of the online CED task, as described in Section~\ref{sec:CED-task}, a data-driven method faces the challenge of class imbalance due to the temporal sparsity of \emph{CE} labeling. The ground-truth label sequence predominantly contains ``0"s, similar to the issue in some object detection tasks, where negative classes (background objects) vastly outnumber positive classes (foreground objects). To address this imbalance, we adopt Focal Loss (FL) \cite{DBLP:journals/corr/abs-1708-02002}, a modified cross-entropy loss that focuses on mistakes made on less frequent but more important classes. For simplicity, we describe FL using binary classification. Given a complex event dataset with $N$ training examples, where each example sequence $y_i$ has length $T$, the FL to optimize is:
\vspace{-1em}
\begin{equation}
\min_\theta L_{FL}\left(\theta\right)=-\sum_{i=1}^{N}\sum_{t=1}^{T}\alpha_{y_i(t)}\left(1-p_{y_i(t)}\right)^\gamma \log \left(p_{y_i(t)}\right),
\label{eq:fl}
\end{equation}
where $p_{y_i(t)}$ is the estimated probability of class $y$ at time $t$, $\gamma$ is the focusing parameter that reduces the contribution of frequent classes, and $\alpha_y$ is the class weight coefficient. After performing a grid search on hyperparameters, we set $\gamma = 2$, as recommended in prior work, $\alpha_0 = 0.005$ for the most frequent class ``0", and $\alpha_y = 0.25$ for other rare but critical \emph{CE} classes. 

% This configuration effectively balances the emphasis on rare but critical CE labels while mitigating the impact of class imbalance in the training data. (add in ablation )

% \textbf{End-to-end Neural Architecture.} This architecture belongs to \textit{Category I}, where neural models directly take the 128-dimensional sensor embedding vectors of the multimodal information for each window to detect complex events. We choose (1) a \textbf{\textit{Unidirectional LSTM}} \cite{hochreiter1997long} of 3 LSTM layers with a hidden dimension of 128; (2) a \textbf{\textit{Causal TCN}}\cite{bai2018tcn} that masks information from future timestamps in convolutional operations. It has one stack of residual blocks with a last dilation rate of 16 and a kernel size of 2. The number of filters for each level is 256. The receptive field is calculated as 
% $\textrm{\# stacks of blocks} \times \textrm{kernel size} \times \textrm{last dilation rate}= 1 \times 2 \times 16 = 32$, which is greater than the longest 2-min temporal pattern of our \textit{CE} dataset, corresponding to a receptive field greater than $ 2 \times 60 \div 5 = 24$ (\# seconds divided by the window size); and (3) A \textbf{\textit{Causal Transformer Encoder}} with a triangular attention mask to restrict the model's self-attention to previous timestamps only, excluding information from future timestamps. The causal transformer encoder uses 6 encoder layers with a hidden dimension 128, multi-head attention with 8 heads, and positional encoding \cite{DBLP:journals/corr/VaswaniSPUJGKP17}.

% \textbf{Two-stage Concept-based Neural Architecture.} This architecture falls into \textit{Category II}. It first maps the sequence of sensor embedding vectors to a sequence of \textit{AE}s with a fully-connected neural model and detects \textit{CE} using various neural backbone models, denoted as \textbf{\textit{Neural AE + X}}. Here the \textit\textbf{{X}} stands for the backbone models, which are \textbf{\textit{LSTM}}, \textbf{\textit{TCN}} and \textbf{\textit{Transformer}} with almost the same hyperparameters of previous models in {End-to-end Neural Architecture}, except that they take the 9-dimensional one-hot vectors as \textit{AE} concepts.

% \textbf{Neuro-symbolic Architecture.} This architecture also belongs to \textit{Category II}. The neuro-symbolic model we choose uses a user-defined symbolic FSM to detect a \textit{CE} from a sequence of \textit{AE}s.


% Basseline part use a table
% narce part use a plot




