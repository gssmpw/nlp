%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Complex Event Dataset Classes} \label{sec:ce_classes}
The 10 \emph{CE} classes of interest are defined here.

\begin{table*}[ht]
\centering
\caption{Complex Event Classes and Definitions}
{\small
\begin{tabular}{@{}p{3cm}p{0.8cm}p{8.2cm}p{4cm}@{}}
\toprule
\textbf{Complex Events}                 & \textbf{Labels} & \textbf{Definitions}                                                                                                                                                                                                                                       & \textbf{Category}                 \\ \midrule
Default ($e_0$)                               & 0               & When no complex events of interest take place.                                                                                                                                                                                                             & -                                 \\ \midrule
Workspace sanitary protocol violation ($e_1$)  & 1               & A person starts working (click or type) without washing hands for at least 20 seconds after they use the restroom. The hand washing should last for 20 seconds consecutively. Each time when a violation happens, trigger an alert immediately and reset the system. & Sequential + Temporal             \\ \midrule
Sanitary eating habit violation ($e_2$)        & 2               & Hands are not cleaned within 2 minutes before having a meal (including eat and drink). Cleaned hands are defined as washing for at least 20 seconds consecutively. Each meal session begins when the person eats or drinks and ends when related activities stop.          & Sequential + Temporal             \\ \midrule
Inadequate brushing time ($e_3$)               & 3               & Brushing teeth for less than 2 minutes. If brushing stops, wait for 10 seconds; otherwise, report a violation and reset the system.                                                                                                                        & Temporal (Relative + Duration)    \\ \midrule
Routine Sequence ($e_4$)                       & 4               & brush $\rightarrow u^* \rightarrow$ eat $\rightarrow u^*\rightarrow$ drink $|$ brush $\rightarrow u^* \rightarrow$ drink $\rightarrow u^*\rightarrow$ eat, \newline where $u = A \setminus \{\text{brush, eat, drink}\}$\dag.               & Sequential - Relaxed              \\ \midrule
Start working and then take a break ($e_5$)    & 5               & sit $\rightarrow u^* \rightarrow$ type/click $\rightarrow v^* \rightarrow$ walk, \newline where $u = A\setminus\{\text{sit, type, click, walk}\}$, and $v = A\setminus\{\text{type, click, walk}\}$\dag.                & Sequential - Relaxed              \\ \midrule
Sufficient Washing Reminder ($e_6$)            & 6               & The event is triggered when washing lasts for 30 seconds consecutively.                                                                                                                                                                                    & Temporal - Duration               \\ \midrule
Adequate brushing time ($e_7$)                 & 7               & The event is triggered when brushing lasts a total of 2 minutes. Timer pauses if brushing stops but resumes if brushing restarts. Once the 2-minute threshold is reached, the event is reported, and the timer resets.                                      & Temporal (Relative + Duration)    \\ \midrule
Post-Meal Rest ($e_8$)                         & 8               & After eating, wait for at least 3 minutes to work.                                                                                                                                                                                                         & Temporal - Relative               \\ \midrule
Active Typing Session ($e_9$)                  & 9               & The event occurs if at least 3 typing sessions (start typing, stop typing) happen within 60 seconds of the first session's start.                                                                                                                          & Repetition - Frequency            \\ \midrule
Focused Work Start ($e_{10}$)                     & 10              & The event is triggered by sitting after being seated, as long as no walking occurs during this time. The event is reported after exactly 5 clicks after sitting and before walking.                                                                          & Repetition - Contextual           \\ \bottomrule
\end{tabular}
}
\label{tab:complex_events}

\parbox{0.95\linewidth}{%
\raggedright % Left-align the notes
\footnotesize
\textbf{Notes:} \dag Here $A$ represents the set of all \emph{atomic events}.
}
\end{table*}


\section{Complex Event Simulator}\label{sec:simulator}
The complex event simulator is used to generate \emph{CE} dataset related to the \emph{CE} classes defined in Table~\ref{tab:complex_events}.

\subsection{Complex Event Simulator}
Due to the complexity of complex event patterns, each \textit{CE} has infinitely many combinations of \textit{AE}s over time. To generate a general distribution for complex events, we developed a stochastic \textit{CE} human activity simulator to synthesize multimodal time-series data for each \textit{CE} pattern. 

Fig.~\ref{fig:ce-simulator} illustrates the simulator used to generate stochastic \textit{CE} sequences. It consists of multiple \textit{Stages}, each containing a set of \textit{Activities} that occur with different probabilities. An \textit{Activity} is defined by a temporal sequence of \textit{Actions}. For example, the \textit{Activity} ``\textit{Use Restroom}" follows the sequence: \textit{`walk' $\rightarrow$ (`wash') $\rightarrow$ `sit' $\rightarrow$ `flush-toilet' $\rightarrow$ (`wash') $\rightarrow$ `walk'}, where parentheses indicate that an \textit{Action} occurs probabilistically. The duration of each \textit{Action} is randomly sampled within a user-defined threshold, allowing sequences to vary in length even for the same \textit{Activity}. Transitions between \textit{Stages} and \textit{Activities} are also stochastic.


\begin{figure}[h]
\centerline{\includegraphics[width=0.7\columnwidth]{figs/CE-Simulator.png}}
\caption{\textbf{Daily activity simulator.} Each \textit{Stage} has a set of $n$ \textit{Activities} that may happen according to a predefined distribution, where \textit{Activity} $i$ has a probability $p_i$ of taking place in that \textit{Stage}. Each \textit{Activity} is defined by a temporal combination of relevant \textit{AE}s. For example, in \textit{Daytime Stage} \textit{Activities} ``\textit{Walk-only}", ``\textit{Sit-only}", ``\textit{Restroom}," ``\textit{Work}", and ``\textit{Drink-only}" happen with probabilities $[0.27, 0.27, 0.02, 0.4, 0.04]$ respectively. Each \textit{Activity} is defined by the pattern displayed on the right side.}
\label{fig:ce-simulator}
\end{figure}

\subsection{Multimodal \emph{AE}s}
\begin{table}[h]
\setlength\tabcolsep{1.5pt}%
\centering
\caption{Definition of multimodal action classes}
\vskip 0.15in
\begin{tabular}{ l  r  r }\toprule
\textbf{Multimodal \textit{Action}} & \textbf{Audio class} & \textbf{IMU class} \\\midrule
\textbf{walk} & footsteps & walking \\
\textbf{sit} & no sound & sitting \\
\textbf{brush teeth} & brushing teeth & teeth \\
\textbf{click mouse} & mouse click & sitting \\
\textbf{drink} & drinking sipping & drinking \\
\textbf{eat} & eating & eating pasta \\
\textbf{type} & keyboard typing & typing \\
\textbf{flush toilet} & toilet flush & standing \\
\textbf{wash} & water-flowing & standing \\
\bottomrule
\end{tabular}
\label{tab:multimodal-action}
\vskip -0.1in
\end{table}


We synthesize multimodal sensor data for 9 \textit{Actions}, corresponding to the \textit{AE}s used to construct \textit{CE}s. Each \textit{Action} class is mapped to a pair of audio and IMU classes, as shown in the first column of Table.~\ref{tab:multimodal-action}. The 9 audio and 9 IMU classes are selected from the following two datasets:


\subsection{Audio dataset}
We utilize the ESC-70 dataset, a combination of the ESC-50 dataset \cite{esc50} and the Kitchen20 dataset \cite{kitchen20}. The ESC-50 dataset consists of 2000 5-second labeled environmental audio recordings, with 50 different classes of natural, human, and domestic sounds. The Kitchen20 dataset collects 20 labeled kitchen-related environmental sound clips. We also self-collected 40 additional 5-second silent sound clips for \textit{Actions} that do not have sound. We downsampled the original sampling rate of those recordings is 44.1 kHz to 16 kHz.

\subsection{IMU dataset} 
We use the WISDM dataset \cite{wisdm}, containing raw accelerometer and gyroscope sensor data collected from smartphones and smartwatches, at a sampling rate of 20 Hz. It was collected from 51 test subjects as they performed 18 activities for 3 minutes each. Based on the findings in a survey paper \cite{oluwalade2021human}, we only use smartwatch data for better accuracy. We segment the original data samples into non-overlapping 5-second clips.

Using those two datasets, we generate 500 multimodal data samples of 5 seconds per \textit{Action} class. To synthesize multimodal \textit{CE} sensor data, the 5-second sensor data clips for every \textit{Action} are concatenated according to the \textit{AE} patterns of the generated stochastic \textit{CE} sequences.





\section{FSM Examples}\label{sec:fsm}
Examples FSM Codes for $e_1$, $e_2$ and $e_3$ defined in Table~\ref{tab:complex_events}. Those implementations utilize an Extended Finite State Machine (eFSM) instead of a standard FSM to improve efficiency in counting tasks and simplify state logic. eFSMs enhance readability by incorporating variables and conditions for state transitions, making them easier to understand and manage. While any eFSM can still be represented as a standard FSM, using an eFSM allows for a more concise and structured approach to state management for easy interpretation.

\begin{breakablealgorithm}
\footnotesize
\caption{State Machine for Complex Event 1 Detection}\label{alg:fsm1}
\begin{algorithmic}[1]
\Require An activity input $x$ at time $t$
\Ensure A complex event label $y \in \{0, 1\}$; Returns 1 if the event of interest is detected at $t$, 0 otherwise
\State $y \gets 0$ 
    \State $state \gets 0$ \Comment{The initial state}
    \State $wash\_counter \gets 0$ \Comment{\parbox[t]{.35\linewidth}{\raggedleft Counter for continuous wash activities after restroom use}}
    
\Function{State\_Machine\_1}{$x$}

    \If{$state = 0$}
        \If{$x = flush\_toilet$}
            \State $state \gets 1$ \Comment{Transition to \textit{After restroom use} state}
            \State $wash\_counter \gets 0$ \Comment{Reset wash counter}
        \EndIf
    \ElsIf{$state = 1$}
        \If{$x = wash$}
            \State $wash\_counter \gets wash\_counter + 1$ \\\Comment{Increment wash counter}
            \If{$wash\_counter \geq 20$}
                \State $state \gets 0$ \Comment{\parbox[t]{.43\linewidth}{\raggedleft Reset to initial state after sufficient washing (20 seconds)}}
            \EndIf
        \ElsIf{$x = click\_mouse$ or $x = type$} \Comment{Working behavior}
            \If{$wash\_counter < 20$}
                \State $y \gets 1$ \Comment{Event detected}
            \EndIf
            \State $state \gets 0$ \Comment{Reset state}
        \Else
            \State $wash\_counter \gets 0$ \Comment{Reset wash counter for other activities}
        \EndIf
    \EndIf

    \State \Return $y$
\EndFunction
\end{algorithmic}
\end{breakablealgorithm}


\begin{breakablealgorithm}
\footnotesize
\caption{State Machine for Complex Event 2 Detection}\label{alg:fsm2}
\begin{algorithmic}[1]
\Require An activity input $x$ at time $t$
\Ensure A complex event label $y \in \{0, 2\}$; Returns 2 if the event of interest is detected at $t$, 0 otherwise
\State $y \gets 0$   
    \State $state \gets 0$ \Comment{The initial state}
    \State $wash\_count \gets 0$ \Comment{Counter for continuous wash activities}
    \State $time\_since\_wash \gets \infty$ \Comment{Time since last wash}
    \State $is\_meal \gets False$

    \If{$x = eat$  or  $x = drink$}
        \State $is\_meal \gets True$ \Comment{Check if the input is a meal activity}
    \EndIf
    \\
\Function{State\_Machine\_2}{$x$}
    \If{$state = 0$} 
        \State $wash\_count \gets 0$
        \If{$x = wash$}
            \State $state \gets 1$, $wash\_count \gets 1$
        \ElsIf{$is\_meal$}
            \State $state \gets 3$, $y \gets 2$ \Comment{Event detected}
        \EndIf
    \ElsIf{$state = 1$} \Comment{\textit{Washing hands} state, hands are not clean yet}
        \If{$x = wash$}
            \State $wash\_count \gets wash\_count + 1$
            \If{$wash\_count \geq 20$} \Comment{Washing for sufficient time (20 seconds)}
                \State $state \gets 2$ \Comment{Move to \textit{Clean hands} state}
                \State $time\_since\_wash \gets 0$
            \EndIf
        \ElsIf{$is\_meal$}
            \State $state \gets 3$,  $wash\_count \gets 0$, $y \gets 2$ \Comment{Event detected}
        \Else
            \State $state \gets 0$, $wash\_count \gets 0$
        \EndIf
    \ElsIf{$state = 2$} \Comment{\textit{Clean hands} state}
        \If{$is\_meal$}
            \State $state \gets 3$ \Comment{Stay in \textit{Clean hands} state during meal}
        \ElsIf{$x \in \{brush\_teeth, click\_mouse, flush\_toilet, type\}$}
            \State $state \gets 0$ \Comment{\parbox[t]{.4\linewidth}{\raggedleft Need to wash hands again after touching things}}
        \ElsIf{$x = wash$}
            \State $time\_since\_wash \gets 0$ \Comment{Reset timer, but stay in \textit{Clean hands} state}
        \Else
            \State $time\_since\_wash \gets time\_since\_wash + 1$
        \EndIf
        \If{$time\_since\_wash > 120$} 
            \State $state \gets 0$ \Comment{\parbox[t]{.4\linewidth}{\raggedleft More than 2 minutes passed since last wash, need to rewash hands}}
        \EndIf
    \ElsIf{$state = 3$} \Comment{\textit{Having meals} state, stop the timer}
        \If{$is\_meal$ or $x = sit$}
            \State \textbf{continue} \Comment{Stay in the \textit{Having meals} state}
        \ElsIf{$x \in \{brush\_teeth, click\_mouse, flush\_toilet, type\}$}
            \State $state \gets 0$ \Comment{Need to wash hands again after touching things}
        \ElsIf{$x = wash$}
            \State $time\_since\_wash \gets 0$
            \If{$wash\_count \geq 20$}
                \State $state \gets 2$ \Comment{Go back to \textit{Clean hands} state}
            \Else
                \State $state \gets 1$
            \EndIf
        \Else
            \If{$wash\_count \geq 20$} \Comment{Go back to \textit{Clean hands} state}
                \State $time\_since\_wash \gets time\_since\_wash + 1$
                \State $state \gets 2$
            \Else
                \State $state \gets 0$
            \EndIf
        \EndIf
    \EndIf

    \State \Return $y$
\EndFunction
\end{algorithmic}
\end{breakablealgorithm}


\begin{breakablealgorithm}
\footnotesize
\caption{State Machine for Complex Event 3 Detection}\label{alg:fsm3}
\begin{algorithmic}[1]
\Require An activity input $x$ at time $t$
\Ensure A complex event label $y \in \{0, 3\}$; Returns 3 if the event of interest is detected at $t$, 0 otherwise
\State $y \gets 0$ 
    \State $state \gets 0$ \Comment{The initial state}
    \State $brush\_counter \gets 0$ \Comment{Counter for total brushing time}
    \State $time\_since\_brush \gets 0$ \Comment{Time since last brush}\\

\Function{State\_Machine\_3}{$x$}
    
    
    \If{$state = 0$} \Comment{Initial state}
        \If{$x = brush\_teeth$}
            \State $state \gets 1$
            \State $brush\_counter \gets brush\_counter + 1$
        \EndIf
    \ElsIf{$state = 1$} \Comment{\textit{Brushing teeth} state}
        \If{$x = brush\_teeth$}
            \State $brush\_counter \gets brush\_counter + 1$
        \Else
            \State $state \gets 2$
            \State $time\_since\_brush \gets time\_since\_brush + 1$
        \EndIf
    \ElsIf{$state = 2$} \Comment{\textit{Wait for further brushing} state}
        \If{$x = brush\_teeth$}
            \State $state \gets 1$ \Comment{Return to \textit{Brushing teeth} state}
            \State $brush\_counter \gets brush\_counter + 1$
            \State $time\_since\_brush \gets 0$ \Comment{Reset counter for other actions}
        \Else
            \State $time\_since\_brush \gets time\_since\_brush + 1$
            \State $temp \gets brush\_counter$ \Comment{Record brushing time}
            
            \If{$time since brush > 2$}
                \State $state \gets 0$ \Comment{Return to initial state}
                \State $brush\_counter \gets 0$
                \State $time\_since\_brush \gets 0$
                
                \If{$temp < 120$} \Comment{Check if brushing was less than 2 minutes}
                    \State $y \gets 3$ \Comment{Event detected}
                \EndIf
            \EndIf
        \EndIf
    \EndIf
    \State \Return $y$

\EndFunction
\end{algorithmic}
\end{breakablealgorithm}




\section{Pretraining Feature Encodera and \emph{Neural AE} Classifier}\label{sec:pretrained-encoder}

The Feature Encoder and \textit{Neural AE} are trained at the same time. The architecture of the Feature Encoder combined with a \textit{Neural AE}  classifier is illustrated in Fig.~\ref{fig:fusion}. Given that the \emph{CE} dataset is a multimodal dataset containing both inertial and IMU sensor data, the Pretrained Feature Encoder is designed to generate a fused embedding that effectively integrates these two modalities.

\subsection{Early Fusion Model}
 Various approaches can be considered for multimodal, such as early fusion, late fusion, and hybrid fusion \cite{DBLP:journals/corr/BaltrusaitisAM17}. In our system, we employ early fusion, which involves integrating the features from both modalities immediately after extraction. Fig.~\ref{fig:fusion} provides an overview of the multimodal fusion module's structure. First, the model uses pre-trained audio and IMU modules, BEATs\cite{chen2022beats} and LIMU-Bert\cite{limubert}, respectively, to extract features from every 5-second audio and IMU clip. Then the audio and IMU embeddings undergo individual Gated Recurrent Unit (GRU) layers to obtain audio and IMU embeddings of the same dimension 128. Next, we concatenate them into an embedding of 256 and pass it through a Fusion Layer to create a joint representation of 128, which is trained alongside a downstream fully-connected NN for \textit{AE} classification. Notably, during the inference phase, the \textit{Neural AE} classifier is omitted, and only the output from the last hidden layer of the fusion layer is employed as the fusion embedding. 
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.4\columnwidth]{figs/fusion-model.png}
    \caption{\textbf{Overview of the multimodal fusion module.} In the figure, a rectangular block represents data or an embedding vector, while a rounded corner rectangular block represents a neural network. The dashed-lined block indicates that it is omitted during the inference phase.
    }
    \label{fig:fusion}
\end{figure}

\subsection{Training}
The BEATs model used for the audio module is already pre-trained, while the LIMU-Bert model for IMU module is pre-trained on large datasets. We freeze the parameters of both models and focus on training the other components of the multimodal fusion module. For training and testing the fusion module, we utilize the multimodal atomic action dataset introduced in Table.~\ref{tab:multimodal-action}. The training process minimize the cross-entropy loss:
\begin{equation}
    \min_\theta L_{m}= - \min_\theta \frac{1}{N}\left(\sum_{i=1}^N c_i \cdot \log \left(\hat{c_i}\right)\right), \textrm{where } \hat{c_i} = m_\theta(\mathbf{d_i})
\end{equation}
where $\mathbf{d}$ is the multimodal sensor data of 5-second window size, $\hat{c_i}$ is the predicted label of the \textit{AE} in that window, $c_i$ is the corresponding ground truth label, and $N$ represents the size of the multimodal atomic action dataset

\subsection{Evaluation of the \textit{Neural AE} classifier} We test the classifier using the multimodal \textit{AE} dataset, which is used in Neural \textit{AE} + X models. The classifier achieves 95\% accuracy on test set.

\section{Baseline Experiments}
\subsection{Model Details}\label{sec:baseline-models}
\textbf{End-to-end Neural Architectures:}
\begin{itemize}[nosep]
    \item \textbf{\textit{Unidirectional LSTM}}: 5 LSTM layers with a hidden dimension of 256 (\# parameters $\approx$ 2.5M).
    \item \textbf{\textit{Causal TCN}}: It masks information from future timestamps in convolutional operations. It has one stack of residual blocks with a last dilation rate of 32 and a kernel size of 3. The number of filters for each level is 256. The receptive field is calculated as 
$\textrm{\# stacks of blocks} \times \textrm{kernel size} \times \textrm{last dilation rate}= 1 \times 3 \times 32 = 96$, which corresponds to 8 minutes, greater than the longest 5-min temporal pattern of our \textit{CE} dataset, corresponding to a receptive field greater than $ 5 \times 60 \div 5 = 60$ (\# seconds divided by the window size). (\# parameters $\approx$ 4.6M)
\item \textbf{\textit{Causal Transformer Encoder}}: with a triangular attention mask to restrict the model's self-attention to previous timestamps only, excluding information from future timestamps. The causal transformer encoder uses 6 encoder layers with a hidden dimension 128, multi-head attention with 8 heads, and positional encoding \cite{DBLP:journals/corr/VaswaniSPUJGKP17}. (\# parameters $\approx$ 4.2M)

\item \textbf{\textit{Mamba}}: We use a Mamba model with 12 SSM blocks. We made this design choice because the original Mamba paper \cite{gu2024mamba} states that  two Mamba blocks are equivalent to one transformer layer. (\# parameters $\approx$ 1.4M)
\end{itemize}

\textbf{Two-stage Concept-based Neural Architecture.} \textbf{\textit{Neural AE + X}}s have almost the same architecture as the \emph{X}s in {End-to-end Neural Architecture}, except that they take the 9-dimensional one-hot vectors as \textit{AE} concepts.

\subsection{Training Details}\label{sec:baseline-training}
For training, we utilize one NVIDIA GeForce RTX 4090 GPU and four NVIDIA H100 GPUs. All the baseline neural models are trained using the AdamW optimizer with a learning rate of \(1 \times 10^{-3}\), a weight decay of 0.1, and a batch size of 256. \textbf{Focal Loss} is used to address class imbalance. Early stopping is applied based on the validation loss, with training halted if no improvement is observed after a predefined patience period. The maximum epochs of training is 5000. All experiments are repeated with 10 random seeds.

\subsection{Detailed Results}
Table~\ref{tab:baseline-results} presents the Macro $F1$ score, Positive $F1$ score, and $F1$ score for each class $e_i$. All neural models are trained on 10,000 \emph{CE} sensor data. We observe that all models perform poorly on $e_4$, possibly due to the similarity between drink and eat sensor embeddings.

\begin{table*}[h]
\caption{Baseline models' $F1$ scores with a 2-sigma confidence interval on 5-minute \emph{CE} test data.}%The precision and recall scores are calculated separately for each complex event class. We also calculate the macro-average precision and recall scores (denoted as Avg.) as an overall measure.
    \begin{center}
    \setlength\tabcolsep{1.5pt}%
    \vskip 0.15in
    {\scriptsize%\tiny
    \begin{tabular}{@{}lccccccccccccccc@{}}\toprule % Add @{} to remove the margin of first and last column rrrrrrrrrrrrrr
    & \textbf{All} & \textbf{Pos.} & $e_0$ & $e_1$ & $e_2$ & $e_3$ & $e_4$ & $e_5$ & $e_6$ & $e_7$ & $e_8$ & $e_9$ & $e_{10}$ \\ \midrule
    LSTM  & .89 $\pm$ .10 & .88 $\pm$ .11  & \textbf{1.0} $\pm$ .01 & .98 $\pm$ .06 & .85 $\pm$ .42 & .99 $\pm$ .03 & \textbf{.57} $\pm$ .12 & .98 $\pm$ .03 & .99 $\pm$ .01  & .93 $\pm$ .45 & .84 $\pm$ .15 & .83 $\pm$ .13 & .84 $\pm$ .36 \\
    TCN & .84 $\pm$ .03 & .82 $\pm$ .03 & .99 $\pm$ .0  & .92  $\pm$  .13& .83 $\pm$ .22 & .98 $\pm$ .04 & .55 $\pm$ .06& .89 $\pm$ .09 & .99 $\pm$ .01 & .97 $\pm$ .07  & \textbf{.86} $\pm$ .07 & .66 $\pm$ .14& .54 $\pm$ .10  \\
    Transformer & .78 $\pm$ .08 & .76 $\pm$ .09  & .99 $\pm$ .01 & .97 $\pm$ .05 & .74 $\pm$ .35 &.96 $\pm$ .05 & .47 $\pm$ .21 &.98 $\pm$ .03 & .77 $\pm$ .15  & .75 $\pm$ .14 & .80 $\pm$ .09& .60 $\pm$ .14 & .55 $\pm$ .33\\
    Mamba  & \textbf{.90} $\pm$ .08 & \textbf{.89} $\pm$ .09 & \textbf{1.0} $\pm$ .0 & \textbf{.99} $\pm$ .03 & .89 $\pm$ .20 & .98 $\pm$ .14 & .48 $\pm$ .09 & \textbf{.99} $\pm$ .01 & \textbf{1.0} $\pm$ .01  & .90 $\pm$ .59 & \textbf{.86} $\pm$ .12 & \textbf{.94} $\pm$ .11 & \textbf{.91} $\pm$ .20 \\\midrule
    Neural AE \\
    \phantom{A} + TCN   & .82 $\pm$ .01 & .80 $\pm$ .01  &.99 $\pm$ .0 &.90 $\pm$ .06  & .90 $\pm$ .02 &\textbf{1.0} $\pm$ .0 &.49 $\pm$ .04  &.95 $\pm$ .02 & 1.0 $\pm$ .0  & .99 $\pm$ .01 & .79 $\pm$ .02 & .57 $\pm$ .01 & .46 $\pm$ .06\\
    \phantom{A} + Transformer   & .76 $\pm$ .04 & .74 $\pm$ .05  &.99 $\pm$ .0 & .89 $\pm$ .03 & .89 $\pm$ .05 &.95 $\pm$ .21 & .48 $\pm$ .03 & .96 $\pm$ .06& .84 $\pm$ .19  & .82 $\pm$ .16 &.70 $\pm$ .07 & .43 $\pm$ .03 & .40 $\pm$ .08\\
    \phantom{A} + Mamba  & .83 $\pm$ .0 & .81 $\pm$ .0  & \textbf{1.0} $\pm$ .0& .91 $\pm$ .01 & \textbf{.92} $\pm$ .01 &\textbf{1.0} $\pm$ .0 & .48 $\pm$ .02 &.97 $\pm$ .01 & \textbf{1.0} $\pm$ .0  & \textbf{1.0} $\pm$ .0 & .76 $\pm$ .02& .60 $\pm$ .03 & .50 $\pm$ .01\\
    \phantom{A} + FSM  & .78 & .76  & .99 & .84 & .79 & \textbf{1.0} & .46 &.76 & \textbf{1.0}  & \textbf{1.0} &.75 &.52  &.50 \\
    % \phantom{A} + ProbLog & \\
    % % \midrule
    \bottomrule
    \end{tabular}
    }
    \label{tab:baseline-results}
    \end{center}
    \vskip -0.1in
\end{table*}


\section{LLM Synthesizer} \label{sec:llm_synthesizer}
\lstset{escapeinside={(*@}{@*)}}
\subsection{The Prompt Template}
\begin{lstlisting}[caption={LLM Prompt Template}]
You are a simulator that mimics daily human activities. You output sequences of activities as live streaming. At each window of 5-second, you generate a current activity label, which represents the activity that happens during this 5-second time window. Here's an example output of a live-streaming list of activities:

['walk', 'sit', 'sit', 'sit', 'sit', 'flush\_toilet', 'flush\_toilet', 'wash', 'wash', 'wash', 'wash', 'wash', 'wash', 'walk', 'walk', 'walk', 'walk']

I want you to write a simulator which synthesizes random activity sequences. Follow this protocol:
1. Design different semantic groups (e.g., hygiene, restroom, work...) that contain related activities. Each group should include all activities commonly associated with it in realistic scenarios. 
2. Design different range of time durations for both semantic groups and the activities in each group. 
3. Design the transition between semantic groups probabilistically governed by realistic probabilities. Some semantic group may have higher frequency while some may happen only once in some period of time. Also design a distribution of the initial group.
4. Design the sub-transitions within a semantic group using realistic probabilities, (e.g., one usually "sit" for some time before "flush_toilet"). Also, in reality, some activity may appear only once in the semantic group, use a dynamic weight adjustment so that the probability of ot becomes 0 once it has been selected during that group session.
5. Double check if the transition will give us activity patterns of interest. For instance, for events related to some semantic group, guarantee at least one group in the sequence (e.g., adjust probabilities dynamically to increase the probability of selecting this group after a certain amount of time has passed without it). 
6. Add a small portion of random noise or perturbation during the generation to increase the sequence variability. 

// User-defined input starts here
Now, we are interested in an event related to: 
    (*@\hl{[USER-PROVIDED EVENT DESCRIPTION]}@*)
    
// User-defined activity set
The activities you can use to synthesize the activity traces are: 
    (*@\hl{[USER-DEFINED ACTIVITY SET]}@*)
\end{lstlisting}

\subsection{Example Prompt \& Response}
Below is an example prompt used to guide the LLM in generating simulator code. In total, we use 10 LLM-generated simulators to synthesize pseudo \emph{AE} traces, ensuring data variability.

\begin{lstlisting}[caption={Example Prompt}]
You are a simulator that mimics daily human activities. You output sequences of activities as live streaming. At each window of 5-second, you generate a current activity label, which represents the activity that happens during this 5-second time window. Here's an example output of a live-streaming list of activities:

['walk', 'sit', 'sit', 'sit', 'sit', 'flush\_toilet', 'flush\_toilet', 'wash', 'wash', 'wash', 'wash', 'wash', 'wash', 'walk', 'walk', 'walk', 'walk']

I want you to write a simulator which synthesizes random activity sequences. Follow this protocol:
1. Design different semantic groups (e.g., hygiene, restroom, work...) that contain related activities. Each group should include all activities commonly associated with it in realistic scenarios. 
2. Design different range of time durations for both semantic groups and the activities in each group. 
3. Design the transition between semantic groups probabilistically governed by realistic probabilities. Some semantic group may have higher frequency while some may happen only once in some period of time. Also design a distribution of the initial group.
4. Design the sub-transitions within a semantic group using realistic probabilities, (e.g., one usually "sit" for some time before "flush_toilet"). Also, in reality, some activity may appear only once in the semantic group, use a dynamic weight adjustment so that the probability of ot becomes 0 once it has been selected during that group session.
5. Double check if the transition will give us activity patterns of interest. For instance, for events related to some semantic group, guarantee at least one group in the sequence (e.g., adjust probabilities dynamically to increase the probability of selecting this group after a certain amount of time has passed without it). 
6. Add a small portion of random noise or perturbation during the generation to increase the sequence variability. 

// User-defined input starts here
Now, we are interested in an event related to: workspace sanitary protocol. A person may or may not start working without washing hands for at least 20 seconds after they use the restroom. The person may wash hands differently, for example, they may forget to wash hands, or wash hands for different duration, or wash hands twice or more intermittently.
    
// User-defined activity set
The activities you can use to synthesize the activity traces are: 
["click_mouse", "flush_toilet", "sit", "type", "walk", "wash"]
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Example simulator code generated by LLM]
def generate_activity_sequence(duration=300):
        """
        Simulate activities based on semantic groups, durations, and transitions.

        Args:
            duration (int): Total simulation duration in seconds.

        Returns:
            list: List of activities in 5-second windows.
        """
        activities = []

        # Semantic groups and their activities
        semantic_groups = {
            "restroom": {
                "activities": ["sit", "flush_toilet", "wash"],
                "durations": {"sit": (5, 30), "flush_toilet": (5, 10), "wash": (5, 40)},
                "transitions": {"sit": ["flush_toilet"], "flush_toilet": ["wash"], "wash": []},
            },
            "hygiene": {
                "activities": ["wash"],
                "durations": {"wash": (10, 30)},
                "transitions": {"wash": []},
            },
            "work": {
                "activities": ["sit", "click_mouse", "type"],
                "durations": {"sit": (5, 20), "click_mouse": (5, 5), "type": (5, 5)},
                "transitions": {"sit": ["click_mouse", "type"], "click_mouse": ["type"], "type": ["click_mouse", "sit"]},
            },
            "movement": {
                "activities": ["walk"],
                "durations": {"walk": (5, 30)},
                "transitions": {"walk": []},
            },
        }

        # Transition probabilities between semantic groups
        group_transitions = {
            "restroom": [("hygiene", 0.6), ("work", 0.1), ("movement", 0.3)],
            "hygiene": [("work", 0.8), ("movement", 0.2)],
            "work": [("restroom", 0.2), ("work", 0.6), ("movement", 0.2)],
            "movement": [("work", 0.2), ("restroom", 0.7), ("hygiene", 0.1)],
        }

        group_durations = {"restroom": (20, 60), "hygiene": (10, 30), "work": (10, 180), "movement": (5, 50)}

        # Initial group probabilities
        initial_group_probabilities = [
            ("restroom", 0.1),
            ("hygiene", 0.3),
            ("work", 0.1),
            ("movement", 0.5),
        ]

        def select_activity(group, current_activity=None):
            """Select the next activity in a semantic group."""
            if current_activity:
                transitions = semantic_groups[group]["transitions"].get(current_activity, [])
                if transitions:
                    return random.choice(transitions)
            return random.choice(semantic_groups[group]["activities"])

        def weighted_choice(choices):
            """Select an option based on weighted probabilities."""
            total = sum(weight for _, weight in choices)
            rand = random.uniform(0, total)
            upto = 0
            for choice, weight in choices:
                if upto + weight >= rand:
                    return choice
                upto += weight

        current_time = 0
        current_group = weighted_choice(initial_group_probabilities)
        while current_time < duration:
            # Select a semantic group duration
            group_duration = random.randint(*group_durations[current_group])
            group_end_time = current_time + group_duration

            # Generate activities for the current semantic group
            current_activity = None
            while current_time < group_end_time and current_time < duration:
                activity = select_activity(current_group, current_activity)
                activity_duration = random.randint(*semantic_groups[current_group]["durations"][activity])
                activity_duration = min(activity_duration, group_end_time - current_time, duration - current_time)

                # Add activities for 5-second windows
                for _ in range(activity_duration // 5):
                    activities.append(activity)

                current_time += activity_duration
                current_activity = activity

            # Transition to the next semantic group
            current_group = weighted_choice(group_transitions[current_group])

        return activities
\end{lstlisting}

\section{\narce{} Experiments}
\subsection{Training Details}\label{sec:narce-training}
We utilize one NVIDIA GeForce RTX 4090 GPU and four NVIDIA H100 GPUs. Both the (Embedding Encoder +) NAR and Sensor Adapter are trained using the AdamW optimizer with a learning rate of \(1 \times 10^{-3}\), a weight decay of 0.1, and a batch size of 256. \textbf{Focal Loss} is used to address class imbalance. Early stopping is applied based on the validation loss, with training halted if no improvement is observed after a predefined patience period. The maximum epochs for training NAR is 5000, and for training Sensor Adapter is 10000. All experiments are repeated with 10 random seeds.


\subsection{Detailed Results - Wilcoxon Statistical Test}\label{sec:wilcoxon}  

\textbf{Comparison between narce\_4k and mamba\_4k.}  
We perform a Wilcoxon Signed-Rank Test to evaluate the null hypothesis $H_0$: narce\_4k is worse than mamba\_4k. The resulting p-values are 0.02, 0.3, and 0.03 for \emph{CE 5-min}, \emph{CE 15-min}, and \emph{CE 30-min}, respectively. Additionally, for the \emph{CE 15-min} dataset, we test the null hypothesis $H_0$: narce\_4k is better than mamba\_4k, obtaining a p-value of 0.69.   These results indicate that narce\_4k is significantly better than mamba\_4k on \emph{CE} 5-min and \emph{CE} 30-min. However, no significant difference is observed between narce\_4k and mamba\_4k on \emph{CE} 15-min.


\textbf{Comparison between narce\_4k and mamba\_10k.}  
We conduct two one-sided Wilcoxon Signed-Rank Tests to evaluate the null hypotheses: (1) $H_0$: narce\_4k is worse than mamba\_10k and (2) $H_0$: narce\_4k is better than mamba\_10k. However, none of the p-values for \emph{CE 5-min}, \emph{CE 15-min}, or \emph{CE 30-min} are significant enough to reject either hypothesis. Thus, we conclude that there is \textbf{no significant difference} between narce\_4k and mamba\_10k.



\textbf{Comparison between narce\_2k and mamba\_4k.}  
Similarly, we conduct two one-sided Wilcoxon Signed-Rank Tests to evaluate the null hypotheses: (1) $H_0$: narce\_2k is worse than mamba\_4k and (2) $H_0$: narce\_2k is better than mamba\_4k. However, none of the p-values for \emph{CE 5-min}, \emph{CE 15-min}, or \emph{CE 30-min} are significant enough to reject either hypothesis. Thus, we conclude that there is \textbf{no significant difference} between narce\_2k and mamba\_4k.






\section{Case Study - LLM for CED}\label{sec:llm_eval}
Many existing works show that LLMs have the ability to do high-level reasoning on sensor data\cite{penAI, LLMSense}. We also evaluated the ability of LLMs to perform online CED tasks. We simplify the setting to give LLMs ground truth sequences of atomic event labels and only investigate LLMs' ability for complex event reasoning. In this experiment,  each \emph{AE} label is expressed in words representing the activity happening in each 5-second window. For example, $[``walk", ``walk", ``sit"]$ means a person walks for 10 seconds and then sits down. We consider complex events $e_1, e_1$ and $e_2$ of 5 minutes, so each data sample has 60 activity labels. We provide the context and instruction of the CED task and definitions of complex events defined in \ref{tab:complex_events}, and LLMs are asked to predict the complex event labels for each 5-second window.\footnote{The prompts can be found here: \href {https://anonymous.4open.science/r/LLM-CED-Prompts-CD6C/}{/r/LLM-CED-Prompts-CD6C/}} We test on 2,000 examples. 

\subsection{Experiments.} We use three metrics to evaluate LLM's performance from different perspectives:
\begin{itemize}[leftmargin=10pt,nosep]
    \item \textit{Length Accuracy}: As an \textit{``n-to-n"} sequence prediction task, the complex event labels given by LLMs should have the same length as the input sequence. This metric evaluates the length match rate of complex event labels. 
    \item \textit{Conditional $F1$ Score}: This metric evaluates element-wise (time-wise) $F1$ score of the three complex event labels that require accurate prediction of both complex event types and timing, conditioned on the case when the LLM outputs a complex event sequence with the correct length $T$. We calculate the average $F1$ score of complex event label prediction from timestamp 1 to T.
    \item \textit{Coarse $F1$ Score}: This metric is a sample-wise coarse $F1$ score that evaluates complex event labeling at high-level. It does not require a precise match between the predicted and ground-truth complex event labels at every timestamp. It only requires the LLM to recognize a complex event type in the 5-minute sample correctly.
\end{itemize}
We test LLMs on both zero-shot and few-shot tasks. For few-shot experiments, we add three input-output example pairs in the prompt for the few-shot case. 

\begin{table}[h]
\caption{Evaluation results of LLMs.}%The precision and recall scores are calculated separately for each complex event class. We also calculate the macro-average precision and recall scores (denoted as Avg.) as an overall measure.
    \begin{center}
    \small
    \setlength\tabcolsep{1.5pt}%
    \begin{tabular}{@{}lccccccccccc@{}}\toprule % Add @{} to remove the margin of first and last column rrrrrrrrrrrrrr
    & \multirowcell{2}{\makecell[c]{$Length$\\$Acc.$}}  && \multicolumn{4}{c}{$Coarse$ $F1$}  && \multicolumn{4}{c}{$Conditional$ $F1$}\\
    \cmidrule{4-7} \cmidrule{9-12}
    & && $e_1$ & $e_2$ & $e_3$ & Avg.&&  $e_1$ & $e_2$ & $e_3$ & Avg.\\ \midrule
    \textit{Zero-shot} &\\
    Qwen2.5-7B & 0.04 && 0.51 & 0.67 & 0.67 & 0.62 && 0.0 & 0.18 & 0.05 & 0.07 \\
    Qwen2.5-14B & 0.12 && 0.60 & 0.66 & 0.57 & 0.61 && 0.14 & 0.15 & 0.04 & 0.11\\
    GPT-4o-mini  & 0.04 && 0.0 & 0.0 & 0.64 & 0.21 && 0.0 & 0.0 & 0.0& 0.0 \\
    GPT-4o & 0.12 && 0.87 & 0.78 & 0.80 & 0.82  &&  0.14  & 0.59  & 0.03  & 0.25 \\
    \midrule
    \textit{Few-shot} ($k = 3$) & \\
    Qwen2.5-7B & 0.04 && 0.49 & 0.68 & 0.71 & 0.63 && 0.0 & 0.19 & 0.04 & 0.08 \\
    Qwen2.5-14B & 0.14 && 0.62 & 0.66 & 0.60 & 0.63 && 0.13 & 0.13 & 0.03 & 0.10\\
    GPT-4o-mini & 0.03 && 0.0 & 0.0 & 0.58 & 0.19 && 0.0 & 0.0 & 0.0& 0.0 \\
    GPT-4o & 0.16 && 0.87 & 0.81 & 0.81 & 0.83 &&  0.13 & 0.63  & 0.14  & 0.30   \\
    % \midrule
    \bottomrule
    \end{tabular}
    \label{tab:results}
    \end{center}
\end{table}

\subsection{Results.} As shown in Table~\ref{tab:results}, we evaluated four SOTA LLM models for complex events 1, 2, and 3 individually and on average. All LLMs performed badly on the CED task. First, the low length accuracy may be caused by the hallucination of LLMs in long-chain reasoning. When we give an activity sequence of length 60 to LLMs and ask them to output results step by step, LLMs usually give complex event label sequence with shorter or longer lengths (mostly within 55 and 65), failing to keep track of the correct number of output labels. Second, since the $F1$ score cannot be calculated in a descent way when the predicted sequence is different from the ground-truth sequence, we calculate the conditioned $F1$ score only on LLM outputs with the correct length. GPT-4o performs slightly better than other models but is still far below satisfactory. This may be due to the poor long-chain reasoning and counting ability.  Third, we lose the constraints of capturing the exact time when a complex event occurs and use the coarse F1 score, which only requires LLMs to capture whether some complex event occurs within the 5-minute sequence. LLMs' performance improved greatly on this simpler task, implying they can somewhat reason the complex event pattern. However, they are inadequate for predicting the correct time. Lastly, adding few-shot examples does not help LLMs too much. This may be because (1) LLMs already suffer from generating ce sequence with the correct length, and (2) LLMs struggle to use those long and complex examples to self-check their understanding of complex events, let alone using the few-shot examples to enhance their reasoning.



\subsection{Key Take-aways}
Though LLMs have the most potential to perform well on online CED tasks, the current models still suffer from hallucinations and poor ability in long-chain reasoning. Also, as online CED usually requires timely inference at each time window, transformer-based LLMs will induce huge latency when we wait for blocks of inputs to be fed to LLM servers for processing. %at a low frequency. Suppose we request the LLM server with a higher frequency so that each block of inputs is smaller. In that case, we need to aid LLMs with an external memory smartly so that it can compress complex event information efficiently and effectively so that LLMs won’t miss the complex event that spans more extended temporal traces than the block size of inputs. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%