\subsection{Historical Context} 
% Notes:
% - [ ] one section about transformer/stats space models struggle to learn rules in complex events; mamba is better than other NNs but context length / online setting
% - [ ] and one more subsection about LLM-based language models also struggles.
% - [ ] use llm for pattern recognition - focus more on the pattern recognition (on images) in real-time i,
% - [ ] and LLM on synthetic data (AE concepts), few-shot, finetuning on OOD longer traces
% - [ ] discuss the real-time issue, wait for blocks to process also induce latency
% - [ ] temporal for IMU sequences, spatial for images
% should be 2/3 page + a figure
% NS and NNs models
% LLMs
% Needs more temporal and spatial reasoning ability
% Given all the requirement satisfied above, reasoning is still missing...
Most existing work on CPS-IoT applications has focused on short-time perception tasks, such as human activity recognition or object detection, which typically require only a few seconds of sensor data for inference. However, to achieve a human-like understanding of the world, a system must capture high-level contextual information over extended periods, which is an aspect often overlooked in current work. 

% Why it's important to detect CE in CPT-IoT. Existing methods fail. shrink dataset and intro part.
% Last, bring out current architectures that don't do well and are not FMs. LLM is the only foundation model bring an opportunity but are really bad.
% (1) state space model seems attractivtive , (2) task vacadility or data versadility to complex events and modalities, LLMs generate codes -> neurosymbolic generate code.
\begin{figure*}[t]
    \centering
\includegraphics[width=0.9\textwidth]{figs/ce_overview.png}
    \caption{(a) Sanitary protocol violation in smart home health monitoring system. (b) Detecting coordinated terrorist attacks at different locations across the city using the surveillance system. (c) In a real-time complex event detection (CED) task, only the raw sensor streams and ground-truth complex event labels are provided.}
    \label{fig:ce_overview}
\end{figure*}

Semantically meaningful observations are often complex. For example, for an outside observer to ascertain that an office building is empty (say, at the end of a work day), it needs the accumulated record of all entry and exit events since the building was last unlocked. Considering a camera with a 30 frame/second rate, an 8-hour work-day generates nearly one million frames, far more than the context window of modern LLMs. It is therefore important to know what needs to be remembered. Unfortunately, the answer depends on the task. Thus, it is useful to define the concept of a \textbf{Complex Event}. A complex event represents a high-level scenario with spatiotemporal rules and patterns that require aggregating and reasoning over numerous short-term activities, which we call \textbf{Atomic Events}. An atomic event is the smallest building block of a complex event pattern, (e.g., human activity recognition or object detection within a short window). Understanding and recognizing complex events (e.g., "the building has become empty") is important for any smart system toward real intelligence. For example, Fig.~\ref{fig:ce_overview}(a) defines a complex event where an intelligent assistant on mobile devices understands the sanitary protocol and alerts users of potential violations during the time of pandemic to help them keep healthy. Fig.~\ref{fig:ce_overview}(b) shows a scenario in a smart city, a surveillance system must infer potential security threats such as coordinated terrorist activities by analyzing data across distributed cameras. Detecting these complex events allows identifying critical high-level events, enabling more effective and timely responses in CPS-IoT applications.


\subsubsection{Challenges.}
Complex Events Detection (CED) is a challenging task with several key features:
\begin{itemize}%[leftmargin=10pt]
    \item \textit{Complex Patterns}: A system must identify key patterns relevant to the complex event while ignoring irrelevant activities, referred to as "don’t care" elements, or "X." For example, the sanitary protocol can be represented as "Use restroom → X → Wash hands → X → Eat," where "X" includes other irrelevant activities like "walking" or "sitting." Incorporating "X" broadens the range of possible satisfying sequences, and the temporal aspect further amplifies this probability space exponentially.%, making it significantly challenging to learn complex event rules using purely data-driven methods. %(use numbers like $S^n$)

    \item \textit{Long \& Varied Temporal Span}: Complex events have much longer and more varied time dependencies. Still consider the sanitary protocol example, violation occurs when the person skips "Wash hands" after "Use restroom" and before "Eat". However, the time gap between those atomic events can vary significantly, and the system should recognize complex events despite of this.
    
    \item \textit{Sensitivity to Latency}: Complex events require immediate attention. For instance, in a nursing monitor system, we shouldn't wait to analyze data until the end of the day. Instead, we need a real-time system that alerts nurses when safety protocols are violated.
\end{itemize}
To satisfy those requirements, a CPS-IoT system must reason long-term spatiotemporal complex patterns and react to complex events in real time.

\subsubsection{Dataset.}
To investigate the performance of different models on a real-time CED task, we designed a multimodal complex event dataset in a smart health monitoring setting. We define three complex event rules\label{ce_definitions}. (1) Go back to work without washing hands for at least 20 seconds consecutively after using the restroom; (2) Within 2 minutes before having a meal, wash hands for less than 20 seconds consecutively. If they touch other things after washing, they need to wash again; (3) Brushing teeth for less than 2 minutes. If brushing stops, wait 10 seconds until we stop the timing.

We create a stochastic simulator that mimics daily human behaviors and synthesize the corresponding sensor traces using existing IMU and Audio datasets, WIDSM\cite{wisdm} and ESC50\cite{esc50}. We create real-time complex event labels every 5 seconds for each sensor trace. Fig.~\ref{fig:ce_overview}(c) illustrates the task.

\subsubsection{ Exploring Neural \& Neurosymbolic Methods.}
There are two approaches to building a real-time Complex Event Detection (CED) framework. The first uses sequential neural network models like CNNs, Transformers, and RNNs to learn complex event rules in a data-driven manner. The second employs neurosymbolic architectures with hand-coded rules, such as DeepProbLog\cite{deepproblog} and NeurASP \cite{neurasp}, which utilize neural network outputs for probabilistic symbolic computation. The neurosymbolic method we design integrates neural networks for atomic activities with user-defined rules for complex events. We chose Finite State Machines (FSMs) to meet real-time CED requirements for their fast inference and low memory usage. Currently, our symbolic FSMs are rigid, and our next step is to implement a probabilistic FSM using ProbLog. Preliminary experiments will assess the potential of these methods for real-time CED tasks.

\textbf{Experiments.} We design models that analyze sensor embeddings or the most probable atomic activity label in non-overlapping 5-second windows to determine if a complex event occurs. We compare three model types: (1) \textit{End-to-End Models}: Includes Transformer, LSTM, TCN, and Mamba, trained on sensor latent embedding sequences and complex event labels; (2)\textit{Concept Bottlenecked Models}: Consists of AE + Transformer, AE + LSTM, AE + TCN, and AE + Mamba. These models use a pre-trained classifier (AE) to obtain atomic activity labels, which are then used to train the neural backbones; (3)\textit{Neurosymbolic Model}: The AE+FSM model employs the most probable atomic activity label as input for rule-based finite state machines (FSMs).

Both the Transformer and TCN models use causal masks, and the TCN has an 8-minute receptive field. All models are trained on 10,000, validated on 1,000, and tested on 1,000 examples of 5-minute complex events.
\begin{figure}[t]
    \centering
\includegraphics[width=0.95\columnwidth]{figs/ce_train_results.png}
    \caption{Average F1 scores of models on complex events with different temporal spans.}
    \label{fig:ce_train_results}
\end{figure}

\textbf{Results.} Fig.~\ref{fig:ce_train_results} presents our preliminary results for various models. We also tested these models on out-of-distribution (OOD) complex events lasting 3 minutes, 15 minutes, and 30 minutes, all adhering to the same rules but varying in temporal span. The Mamba and AE+FSM models outperformed others on average. The AE + FSM model incorporates correct complex event rules; its performance declines greatly with longer traces due to cumulative errors from imperfect atomic event inference. While the Mamba model showed the best generalization on the OOD test sets, we still noted a performance drop as the temporal span increased. Additionally, the data required to train these neural network baselines incurs significant data collection and labeling costs in the real world.

\subsubsection{Evaluation of LLMs.}
Many existing works show that LLMs have the ability to do high-level reasoning on sensor data\cite{penAI}\cite{LLMSense}. We also evaluated the ability of LLMs to perform real-time CED tasks. The task is similar to the previous one, but we simplify the setting to give LLMs ground truth sequences of atomic event labels and only investigate LLMs' ability for complex event reasoning. In this experiment,  each atomic event label is expressed in words representing the activity happening in each 5-second window. For example, $[``walk", ``walk", ``sit"]$ means a person walks for 10 seconds and then sits down. We consider complex events of 5 minutes, so each data sample has 60 activity labels. We provide the context and instruction of the CED task and definitions of complex events defined in \ref{ce_definitions}, and LLMs are asked to predict the complex event labels for each 5-second window.\footnote{The prompts can be found here: \href {https://anonymous.4open.science/r/LLM-CED-Prompts-CD6C/}{/r/LLM-CED-Prompts-CD6C/}} We test on 2,000 examples. 


\textbf{Experiments.} We use three metrics to evaluate LLM's performance from different perspectives:
\begin{itemize}%[leftmargin=10pt]
    \item \textit{Length Accuracy}: As an \textit{``n-to-n"} sequence prediction task, the complex event labels given by LLMs should have the same length as the input sequence. This metric evaluates the length match rate of complex event labels. 
    \item \textit{Conditional $F1$ Score}: This metric evaluates element-wise (time-wise) $F1$ score of the three complex event labels that require accurate prediction of both complex event types and timing, conditioned on the case when the LLM outputs a complex event sequence with the correct length $T$. We calculate the average $F1$ score of complex event label prediction from timestamp 1 to T.
    \item \textit{Coarse $F1$ Score}: This metric is a sample-wise coarse $F1$ score that evaluates complex event labeling at high-level. It does not require a precise match between the predicted and ground-truth complex event labels at every timestamp. It only requires the LLM to recognize a complex event type in the 5-minute sample correctly.
\end{itemize}
We test LLMs on both zero-shot and few-shot tasks. For few-shot experiments, we add three input-output example pairs in the prompt for the few-shot case. 

\begin{table}[t]
\caption{Evaluation results of LLMs.}%The precision and recall scores are calculated separately for each complex event class. We also calculate the macro-average precision and recall scores (denoted as Avg.) as an overall measure.
    \begin{center}
    \small
    \setlength\tabcolsep{1.5pt}%
    \begin{tabular}{@{}lccccccccccc@{}}\toprule % Add @{} to remove the margin of first and last column rrrrrrrrrrrrrr
    & \multirowcell{2}{\makecell[c]{$Length$\\$Acc.$}}  && \multicolumn{4}{c}{$Coarse$ $F1$}  && \multicolumn{4}{c}{$Conditional$ $F1$}\\
    \cmidrule{4-7} \cmidrule{9-12}
    & && $e_1$ & $e_2$ & $e_3$ & Avg.&&  $e_1$ & $e_2$ & $e_3$ & Avg.\\ \midrule
    \textit{Zero-shot} &\\
    Qwen2.5-7B & 0.04 && 0.51 & 0.67 & 0.67 & 0.62 && 0.0 & 0.18 & 0.05 & 0.07 \\
    Qwen2.5-14B & 0.12 && 0.60 & 0.66 & 0.57 & 0.61 && 0.14 & 0.15 & 0.04 & 0.11\\
    GPT-4o-mini  & 0.04 && 0.0 & 0.0 & 0.64 & 0.21 && 0.0 & 0.0 & 0.0& 0.0 \\
    GPT-4o & 0.12 && 0.87 & 0.78 & 0.80 & 0.82  &&  0.14  & 0.59  & 0.03  & 0.25 \\
    \midrule
    \textit{Few-shot} ($k = 3$) & \\
    Qwen2.5-7B & 0.04 && 0.49 & 0.68 & 0.71 & 0.63 && 0.0 & 0.19 & 0.04 & 0.08 \\
    Qwen2.5-14B & 0.14 && 0.62 & 0.66 & 0.60 & 0.63 && 0.13 & 0.13 & 0.03 & 0.10\\
    GPT-4o-mini & 0.03 && 0.0 & 0.0 & 0.58 & 0.19 && 0.0 & 0.0 & 0.0& 0.0 \\
    GPT-4o & 0.16 && 0.87 & 0.81 & 0.81 & 0.83 &&  0.13 & 0.63  & 0.14  & 0.30   \\
    % \midrule
    \bottomrule
    \end{tabular}
    \label{tab:results}
    \end{center}
\end{table}

\textbf{Results.} As shown in Table~\ref{tab:results}, we evaluated four SOTA LLM models for complex events 1, 2, and 3 individually and on average. All LLMs performed badly on the CED task. First, the low length accuracy may be caused by the hallucination of LLMs in long-chain reasoning. When we give an activity sequence of length 60 to LLMs and ask them to output results step by step, LLMs usually give complex event label sequence with shorter or longer lengths (mostly within 55 and 65), failing to keep track of the correct number of output labels. Second, since the $F1$ score cannot be calculated in a descent way when the predicted sequence is different from the ground-truth sequence, we calculate the conditioned $F1$ score only on LLM outputs with the correct length. GPT-4o performs slightly better than other models but is still far below satisfactory. This may be due to the poor long-chain reasoning and counting ability.  Third, we lose the constraints of capturing the exact time when a complex event occurs and use the coarse F1 score, which only requires LLMs to capture whether some complex event occurs within the 5-minute sequence. LLMs' performance improved greatly on this simpler task, implying they can somewhat reason the complex event pattern. However, they are inadequate for predicting the correct time. Lastly, adding few-shot examples does not help LLMs too much. This may be because (1) LLMs already suffer from generating ce sequence with the correct length, and (2) LLMs struggle to use those long and complex examples to self-check their understanding of complex events, let alone using the few-shot examples to enhance their reasoning.

\subsubsection{Key Take-aways}
FMs capable of reasoning long and complex temporal patterns are still lacking. Though LLMs have the most potential to perform well on CED tasks, the current models still suffer from hallucinations and poor ability in long-chain reasoning. Also, as CED usually requires timely inference at each time window, transformer-based LLMs will induce huge latency when we wait for blocks of inputs to be fed to LLM servers for processing. %at a low frequency. Suppose we request the LLM server with a higher frequency so that each block of inputs is smaller. In that case, we need to aid LLMs with an external memory smartly so that it can compress complex event information efficiently and effectively so that LLMs won’t miss the complex event that spans more extended temporal traces than the block size of inputs. 
In our preliminary experiments on different model architectures and methods, we also find that state-based methods such as the Mamba model and FSM engines are more suitable when dealing with complex event patterns that may span an unlimited time, as they can compress information into states efficiently. However, few FMs come in this direction, and the CED task remains challenging in many fields.
\iffalse
\subsection{Practical Experiences with TSFMs in CPS-IoT}
\begin{itemize}
    \item Overview of case studies and experiments involving TSFMs. Taskwise using HAR? Modalities such as IMU?
    \item Highlighting successful/challenging applications, including insights on architecture, latency, model capabilities, multimodality, multi-vintage, and scalability.
    \item What testbed is needed for the paper? Profiling FM on mobile/edge devices?
\end{itemize}

\subsection{Challenges and Limitations Encountered/Where are we right now}
\begin{itemize}
    \item Deployment consideration. system requirements. Adapt to different. Fine-tune while adapting to a particular resource profile (SWaP footprint)
    \item Task Agnosticism. Current models are predominantly focused on forecasting tasks, which limits their utility in CPS-IoT applications
    \item  Latency Constraints. Many CPS-IoT applications demand real-time or near-real-time response, such as in autonomous vehicles or industrial automation.
    \item Temporal and Spatial reasoning. Reasoning over time and across different locations may require the usage of graph and temporal models.
    \item Incorporating and Learning Physical Principles. Current FMs are primarily data-driven and struggle to incorporate explicit physical models from domain-specific knowledge (e.g., thermodynamics in HVAC systems, kinematics in robotics).

\end{itemize}

\fi