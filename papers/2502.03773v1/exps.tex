\section{Experiments}\label{sec:expts}

\paragraph{Datasets \& Models.} We use three standard fairness benchmarks for experimentation : Adult \cite{Adult}, Credit \cite{credit} and German Credit \cite{German}. Adult has 14 input features, Credit has 23 input features, and German has 20 input features. All the continuous features in the datasets are standardized. We train two kinds of models on these datasets, neural networks and random forests. Our neural networks are 2-layer fully connected ReLU activated networks with 16 hidden units in each layer, trained using Stochastic Gradient Descent in PyTorch~\cite{paszke2019pytorch} with a learning rate of 0.001 for 400 epochs. The weights and biases are converted to fixed-point representation with four decimal places for making them compatible with ZKP libraries which do not work with floating points, leading to a $\sim1\%$ test accuracy drop. Our random forests are trained using Scikit-Learn~\cite{pedregosa2011scikit} with 5-6 decision trees in each forest.

\paragraph{ZKP Configuration.} We code \name with different variants of LIME in the \textit{ezkl} library \cite{ezkl2024} (Version 18.1.1) which uses Halo2 \cite{halo2} as its underlying proof system in the Rust programming language, resulting in $\sim 3.7k$ lines of code. Our ZKP experiments are run on an Ubuntu server with 64 CPUs of x86\_64 architecture and 256 GB of memory, without any explicit parallelization. We use default configuration for \textit{ezkl}, except for 200k rows for all lookup arguments in \textit{ezkl} and \name. We use KZG \cite{kate2010constant} commitments for our scheme that are built into \textit{ezkl}.

\textbf{Research Questions \& Metrics.}~We ask the following questions for the different variants of LIME.

Q1) How faithful are the explanations generated by the LIME variant?\\
Q2) What is the time and memory overhead introduced by implementing the LIME variant in a ZKP library?

To answer Q1, we need a measure of fidelity of the explanation, we use `Prediction Similarity' defined as the similarity of predictions between the explanation classifier and the original model in a local region around the input. We first sample points from a local\footnote{Note that this local region is for evaluation and is different from the local neighborhood in LIME.} region around the input point, then classify these according to both the explanation classifier and the original model and report the fraction of matches between the two kinds of predictions as prediction similarity. In our experiments, the local region is created by sampling 1000 points from a Uniform distribution of half-edge length 0.2 or a Gaussian distribution centered at the input point with a standard deviation of 0.2.

%We then classify the local points according to both the explanation classifier and the original model and report the fraction of matches between the two kinds of predictions.

% given the original model $f$, an input point $x$, explanation classifier $e$ and a local region $A_x$ surrounding it, fidelity of the explanation classifier $e$ is given as $\operatorname{Pr}_{x^{\prime} \in{ }_\mu C_\pi}\left(f\left(x^{\prime}\right)=e(x^{\prime})\right)$


%To calculate this measure, we sample points 
%we use similarity between the predictions of the explanation classifier and the original model in a local region around the input as the measure. The local region\footnote{Note that this local region is for evaluation and is different from the local neighborhood in LIME.} is given by randomly sampling 1000 points from a gaussian distribution centered at the input point and standard deviation of 0.2. We  classify the local points according to both the explanation classifier and the original model and report the fraction of matches between the two kinds of predictions.\cy{different word than local region?}

To answer Q2, we will look at the proof generation time taken by the prover to generate the ZK proof, the verification time taken by the verifier to verify the proof and the proof length which measures the size of the generated proof.

\subsection{Standard LIME Variants}\label{subsec:expstandardlime}
In this section we compare the different variants of Standard LIME, given in Alg.~\ref{alg:limevarinclear} Sec.~\ref{sec:varlime}, w.r.t. the fidelity of their explanations and ZKP overhead.

\textit{Setup.} We use the \hyperlink{https://github.com/marcotcr/lime}{LIME} library for experimentation and run the different variants of LIME with number of neighboring samples $n=300$ and length of explanation $K=5$. Based on the sampling type, we either sample randomly from a hypercube with half-edge length as 0.2 or from a gaussian distribution centered around the input point with a standard deviation of 0.2. Based on the kernel type we either do not use a kernel or use the exponential kernel with a bandwidth parameter as $\sqrt{\#features}~* 0.75$ (default value in the LIME library). Rest of the parameters also keep the default values of the LIME library. Our results are averaged over 50 different input points sampled randomly from the test set.

%Given the four variants of LIME produced by changing the sampling distribution and kernel, we explore their fidelity and ZKP-overhead tradeoff. For the sampling step in LIME we sample 300 or 900 neighboring points for different configurations. For more details about hyperparameters, kindly refer to the Appendix `  Sec.\ref{app:sec:expdetails}.
Results for NNs with 300 neighboring samples and Gaussian sampling for fidelity evaluation are described below. Results for uniform sampling fidelity evaluation, fidelity evaluation with neighborhood $n=5000$ points and all results for RFs can be found in the Appendix Sec.~\ref{app:sec:expdetails}.

\textit{Fidelity Results.} As shown in Fig.~\ref{fig:fidelity_plots_all} left, we do not find a huge difference between the explanation fidelities of the different variants of LIME as the error bars significantly overlap. This could be due to the small size of the local neighborhoods where the kernel or sampling doesn't matter much. However, for the credit dataset, which has the highest number of input features, gaussian sampling works slightly better than uniform, which could be because of the worsening of uniform sampling with increasing dimension.

%due to the better quality of samples generated from gaussian sampling as the samples concentrate around the input while 

%observe that broadly all the four variants of LIME produce explanations which are approximately equally faithful. Looking closely,

%We observe that the type of sampling or the kernel does not affect the prediction similarity much in most cases as shown in Fig.\ref{fig:fidelity_plots_all}. This could be due to the small size of the local neighborhoods where these factors do not end up playing a big role. For the german dataset, we observe that uniform sampling matches the mean performance of gaussian sampling by increasing the number of neighboring samples, we show results for 900 neighborhood samples with uniform sampling compared to 300 for gaussian sampling.

%has a more significant effect on fidelity of explanations rather than the type of kernel. Gaussian sampling leads to more faithful explanations than uniform sampling, which requires more number of samples to obtain similar fidelity (atleast three times for our datasets \& models). This is shown in Fig.\ref{fig:simpvsorig}.

%We observe that the type of sampling has a more significant effect on fidelity of explanations rather than the type of kernel. Gaussian sampling leads to more faithful explanations than uniform sampling, which requires more number of samples to obtain similar fidelity (atleast three times for our datasets \& models). This is shown in Fig.\ref{fig:simpvsorig}.
\textit{ZKP Overhead Results.} Across the board, proof generation takes a maximum of $\sim1.5$ minutes, verification time takes a maximum of $\sim0.12$ seconds and proof size is a maximum of $\sim13$KB, as shown in Fig.~\ref{fig:pvtime_psize_brkdwn}. Note that while proof generation time is on the order of minutes, verification time is on the order of seconds -- this is due to the inherent design of ZKPs, requiring much lesser resources at the verifier's end (contrary to consistency-based explanation checks). We also observe that the dataset type does not have much influence on the ZKP overhead; this is due to same ZKP backend parameters needed across datasets.

Furthermore, we see that gaussian sampling leads to a larger ZKP overhead. This can be attributed to our implementation of gaussian sampling in the ZKP library, wherein we first create uniform samples and then transform them to gaussian samples using the inverse CDF method, leading to an additional step in the gaussian sampling ZKP circuit as compared to that of uniform sampling. Similarly, using the exponential kernel leads to a larger overhead over not using it due to additional steps related to verifying the kernel.

%observe that the proof generation time for gaussian sampling is much smaller than Uniform sampling -- this is mainly due to the fewer neighboring points needed for gaussian sampling as compared to uniform sampling. Since each neighboring point necessitates a proof of inference (Sec. \ref{sec:verifylime} point 3), fewer the neighboring points, smaller is the proof generation time. Additionally not using the exponential kernel reduces the proof generation time, since it removes the need of generating proof for the kernel which takes significant time due to lookup table checks corresponding to the exponential function (Sec. \ref{sec:verifylime} point 2). This is show in Fig.\ref{fig:prooftime_nn}. In the figure we see an abnormally high proof generation time for the U+E variant of Credit dataset. This is because proof generation time directly depends on the size of the lookup table of the exponential function, which can be much larger for uniform sampling than gaussian sampling since the samples are no longer concentrated around the mean leading to a larger domain. The proof sizes are very small (max. 20KB) and also follow a similar trend as shown in Fig.\ref{fig:prooflen_nn} \cy{EVAN : write expl for why is the len swapped for U+E and U+N}. 

%\el{new explanation below}

%We note that for the U+E variant of the Credit dataset, although the proof generation time is higher than the Adult or German datasets, the proof size is smaller. This happens is because of how \textit{ezkl} converts inferrence proofs to the underlying proof system Halo2. 

%To explain further for those more versed in ZKPs, the U+E variant of the Credit dataset doesn't fit in the same number of Halo2 rows as the German or Adult dataset because it has more input features than Adult or German. This means U+E Credit is forced to use two times the number of rows as German or Adult because the number of rows must be a power-of-two. Most of these rows aren't needed for the circuit, so the compilation inside of \textit{ezkl} to Halo2 uses those additional empty rows to remove extra columns, which leads to an overall lower proof size.

%We discovered this is because with the larger parameter sizes for U+E, \textit{ezkl} is able to compile the same relation in a more length-efficient manner (by reducing the number of columns used by Halo2 \el{idk if I should get into this detail...}).

%\cy{There are free rows in credit which are taken up by columns. credit circuit is bigger than others due to more input features.}

%The verification time also follows a similar trend, as shown in Fig. \ref{fig:verifytime_nn}. Note that the verification time is on the order of seconds (max. 0.4 sec) while proof generation time is on the order of minutes (max. 5.5 mins), making it a much lighter task on the customer end.

Overall, `gaussian sampling and no kernel' variant of LIME is likely the most amenable for a practical ZKP system as it produces faithful explanations with a small overhead.
 
%We introduce two different versions of LIME : ZKP-Amenable LIME which is a simplified version of LIME to reduce ZKP costs and RobustLIME to make LIME more robust for adversarial circumstances. Now we evaluate if both these versions of LIME produce explanations which are comparable to those from the original version.

%The metric we use for comparison is fidelity of the explanation in a local region around the input point as compared to the original classifier. To calculate this, we randomly sample 1000 points in a local region around the input point and classify them according to both the explanation classifier and the original model. We report the fraction of matches between the two kinds of predictions. We average our results over 100 different input points sampled from the test set.

%We observe that the fidelity of explanations returned by the two versions of LIME is comparable to those from the original version of LIME across datasets. Additionally, RobustLIME leads to a higher performance as compared to original LIME.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/nn_zkp_amen_allvariants_gaussiansamplingpts_300_uniformsamplingpts_900_0.2_usual_gaussian.pdf} % Replace with your image file
%     \caption{Fidelity of different variants of LIME.}
%     \label{fig:simpvsorig}
% \end{figure}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/nn_border_lime_npoints_300_0.2_gaussian.pdf} % Replace with your image file
%     \caption{Fidelity of Basic vs. Border LIME with 300 neighboring points.}
%     \label{fig:borderlimevsnormal_nn_300}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/nn_border_lime_npoints_900_0.2_gaussian.pdf} % Replace with your image file
%     \caption{Fidelity of Basic vs. Border LIME with 900 neighboring points.}
%     \label{fig:borderlimevsnormal_nn_900}
% \end{figure}

\begin{figure*}[hbt!]
    \centering
    \begin{minipage}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/2compnn___usual_zkp_amen_allvariants_0.2_g300_u300_usual_gaussian_fid_var_0.2.pdf}
        %\caption*{(a)}
        \label{fig:simpvsorig_nn}
    \end{minipage}\hfill
    \begin{minipage}{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/3compnn__secure_border_allvariants_gaussiansamplingpts_300_uniformsamplingpts_900_0.2_fid_var_0.2_gaussian.pdf}
        %\caption*{(b)}
        \label{fig:border_3comp_all_nn}
    \end{minipage}\hfill
    \begin{minipage}{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/nn_border_lime_npoints_300_0.2_fid_var_0.2_gaussian.pdf}
        %\caption*{(c)}
        \label{fig:borderlimevsnormal_nn_300}
    \end{minipage}\hfill
    \caption{Results for NNs. G/U: gaussian or uniform sampling, E/N: using or not using the exponential kernel. Left: Fidelity of different variants of Standard LIME, Mid: Fidelity of different variants of BorderLIME , Right: Fidelity of Standard vs. BorderLIME.}
    \label{fig:fidelity_plots_all}
\end{figure*}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/benchmarks/prove_time_nn_comparison.pdf} % Replace with your image file
%     \caption{Proof Generation time for different variants of LIME.}
%     \label{fig:prooftime_nn}
% \end{figure}

%\begin{figure*}[h!]
%    \centering
%    \includegraphics[width=\linewidth]{plots_exp/benchmarks/nn_comparison.pdf} % Replace with your image file
%    \caption{\el{This is what the whole line figure looks like. I think the individual ones are more readable if we can spare the space}}
%    \label{fig:nn}
%\end{figure*}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/benchmarks/verify_time_nn_comparison.pdf} % Replace with your image file
%     \caption{Verification time for different variants of LIME.}
%     \label{fig:verifytime_nn}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/benchmarks/proof_len_nn_comparison.pdf} % Replace with your image file
%     \caption{Proof Sizes for different variants of LIME.}
%     \label{fig:prooflen_nn}
% \end{figure}

\begin{figure*}[hbt!]
    \centering
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots_exp/benchmarks/prove_time_nn_comparison.pdf}
        %\caption*{(a)}
        \label{fig:prooftime_nn}
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots_exp/benchmarks/proof_len_nn_comparison.pdf}
        %\caption*{(b)}
        \label{fig:prooflen_nn}
    \end{minipage}\hfill
    \begin{minipage}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots_exp/benchmarks/verify_time_nn_comparison.pdf}
        %\caption*{(c)}
        \label{fig:verifytime_nn}
    \end{minipage}\hfill
    \caption{Results for NNs. G/U: gaussian or uniform sampling, E/N: using or not using the exponential kernel. Left: Proof Generation Time (in mins), Mid: Proof Size (in KBs), Right: Verification times (in secs) for different variants of Standard LIME. All configurations use the same number of Halo2 rows, $2^{18}$, and lookup tables of size 200k.}
    \label{fig:pvtime_psize_brkdwn}
\end{figure*}


\subsection{BorderLIME}\label{subsec:expborderlime}
In this section we compare the variants of BorderLIME (Alg.~ \ref{alg:robustLIME_highlevel}, Sec.~\ref{sec:varlime}) and BorderLIME vs. Standard LIME w.r.t. the fidelity of their explanations and ZKP overheads.

\textit{Setup.} We implement BorderLIME with all of the Standard LIME variants (Step 6 of Alg.~\ref{alg:robustLIME_highlevel}). For the purpose of experimentation, we fix the iteration threshold to $T=250$, number of directions to $m=5$. Then to \textit{approximate} the stability radius $\delta$, we incrementally go over the set \{0.01, 0.03, 0.05, 0.07, 0.1, 0.15\} and use the smallest value for which an opposite class point is found for all 50 randomly sampled input points. While this is a heuristic approach and does not guarantee the theoretically minimal stability radius, it provides a practical estimate of stability radius for efficient experimentation. Once a suitable value of stability radius is identified, we tighten the number of directions by reducing them while ensuring that at least one opposite-class point exists for each input. Our results are averaged over 50 input points. The exact parameter values used in our final setup can be found in App. Sec.~\ref{app:sec:expdetails}.


%While determining the precise stability radius is an important research direction, it is beyond the scope of this work. Instead, we adopt a pragmatic approach to ensure efficient experimentation while maintaining the core idea of stability radius estimation.

Results for NNs with 300 neighboring samples and Gaussian sampling for fidelity evaluation are described below. Results for uniform sampling fidelity evaluation, fidelity evaluation with neighborhood $n=5000$ points and all results for RFs can be found in the Appendix Sec.~\ref{app:sec:expdetails}.

\textit{Fidelity Results.} Comparing different variants of BorderLIME based on the LIME implementation, we observe that the difference in explanation fidelity between gaussian and uniform sampling becomes more pronounced compared to standard LIME as shown in Fig.~\ref{fig:fidelity_plots_all}, reinforcing the importance of gaussian sampling. This gap can sometimes be reduced by using more neighborhood points, i.e. a larger $n$, when uniformly sampling. As demonstrated in Fig.~\ref{fig:fidelity_plots_all} mid, with three times more points for uniform sampling, we can match the fidelity of gaussian explanations for Adult and German datasets. Comparing the G+N version of BorderLIME and standard LIME in Fig.~\ref{fig:fidelity_plots_all} right, we observe that explanations generated by BorderLIME are atleast as faithful as standard LIME and can sometimes be better hinting to its capability of generating more meaningful explanations.

%not true for RFs - The fidelity increases as the number of neighborhood points increase \ref{fig:borderlimevsnormal_nn_900} which can be the result of better LASSO training due to more opposite class points.


\textit{ZKP Overhead Results.} We observe that BorderLIME has a larger ZKP overhead than standard LIME as shown in Table~\ref{tab:borderlimevslimeallzkp}; this can be attributed to the additional steps needed in BorderLIME to find the border point with opposite label (Alg.~\ref{alg:findclosestpoint}) which also have to be proved and verified. Similar to the previous subsection, the overhead is similar across datasets and verification is orders of magnitude cheaper than proof generation.


\begin{table}[ht]
  \centering
  \smallskip
  \scalebox{0.92}{
  \begin{tabular}{l|c|c}

    ZKP Overhead Type & BorderLIME & LIME \\
    \toprule 
    
    Proof Generation Time (mins) & 4.85 $\pm$ $10^{-2}$ &  1.17 $\pm$ $10^{-2}$
    \\
     \hline 
    
    Verification Time (secs) &  0.30 $\pm$ $10^{-2}$ & 0.11 $\pm$ $10^{-2}$
    \\

    \hline


    Proof Size (KB) &  18.30 $\pm$ $0$ & 10.40 $\pm$ $0$ 
    \\
    
    \end{tabular}}
      \caption{\label{tab:borderlimevslimeallzkp} ZKP Overhead of BorderLIME and Standard LIME (both  G+N variant) for NNs. Overhead for BorderLIME is larger than that for LIME. Results are consistent across all datasets.}%300 neighboring points
\end{table}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/nn_border_lime_npoints_300_0.2_gaussian.pdf} % Replace with your image file
%     \caption{Fidelity of Basic vs. Border LIME with 300 neighboring points.}
%     \label{fig:borderlimevsnormal_nn_300}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/fidelity_plots/nn_border_lime_npoints_900_0.2_gaussian.pdf} % Replace with your image file
%     \caption{Fidelity of Basic vs. Border LIME with 900 neighboring points.}
%     \label{fig:borderlimevsnormal_nn_900}
% \end{figure}



% \begin{figure*}[hbt!]
%     \centering
%     \begin{minipage}{0.33\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots_exp/benchmarks/prove_time_secure_nn_two.pdf}
%         %\caption*{(a)}
%         \label{fig:zkp_borderlimevsnormal_prooftime}
%     \end{minipage}\hfill
%     \begin{minipage}{0.33\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots_exp/benchmarks/proof_len_secure_nn_two.pdf}
%         %\caption*{(b)}
%         \label{fig:zkp_borderlimevsnormal_prooflen}
%     \end{minipage}\hfill
%     \begin{minipage}{0.33\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{plots_exp/benchmarks/verify_time_secure_nn_two.pdf}
%         %\caption*{(c)}
%         \label{fig:zkp_borderlimevsnormal_verifytime}
%     \end{minipage}\hfill
%     \caption{Left: Proof Generation Time (in mins), Mid: Proof Size (in KBs) and Right: Verification times (in secs) for standard LIME vs. BorderLIME. Both use the Gaussian + No kernel variant.}
%     \label{fig:pvtime_psize_brkdwn_secure}
% \end{figure*}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/benchmarks/proof_len_secure_nn_two.pdf} % Replace with your image file
%     \caption{Proof Sizes for basic LIME vs. BorderLIME}
%     \label{fig:zkp_borderlimevsnormal_prooflen}
% \end{figure}


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/benchmarks/prove_time_secure_nn_two.pdf} % Replace with your image file
%     \caption{Proof Generation Times for basic LIME vs. BorderLIME}
%     \label{fig:zkp_borderlimevsnormal_prooftime}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{plots_exp/benchmarks/verify_time_secure_nn_two.pdf} % Replace with your image file
%     \caption{Verification Times for basic LIME vs. BorderLIME}
%     \label{fig:zkp_borderlimevsnormal_verifytime}
% \end{figure}