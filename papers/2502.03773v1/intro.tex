\section{Introduction}\label{sec:intro}
\begin{tcolorbox}[colback=blue!10, colframe=gray!50, boxrule=0.3mm, sharp corners]
``Bottom line: Post-hoc explanations
are highly problematic in an
adversarial context" \cite{bordt2022post}
\end{tcolorbox}


\begin{figure}[t]
    \centering \includegraphics[width=\linewidth]{flow-grey-border_png.png}
    \vspace{-25pt}
        \caption{Pictorial Representation of \name}
      \label{fig:pictorialexpproof}
\end{figure} 


Explanations have been seen as a way to enhance trust in machine learning (ML) models by virtue of making them transparent. Although starting out as a debugging tool, they are now also widely proposed to prove fairness and sensibility of ML-based predictions for societal applications, in research studies \citep{langer2021we, smuha2019eu, kastner2021relation, von2021transparency, leben2023explainable, karimi2020survey, wachter2017counterfactual, liao2021human} and regulations alike (Right to Explanation \cite{wikipedia_right_to_explanation}). However, as discussed in detail by \cite{bordt2022post}, many of these use-cases are adversarial in nature where the involved parties have misaligned interests and are incentivized to manipulate explanations to meet their ends. For instance, a bank which denies loan to an applicant based on an ML model's prediction has an incentive to return an \textit{incontestable} explanation to the applicant rather than reveal the true workings of the model since the explanation can be used by the applicant to prove discrimination in the court of law \cite{bordt2022post}. In fact, many previous studies show that adversarial manipulations of explanations are possible in realistic settings with systematic and computationally feasible attacks \cite{slack2020fooling, shahin2022washing, slack2021counterfactual, yadav2024influence}. As such, despite the demand, explanations fail to be operational as a trust-enhancing tool. % For example, a bank which uses an ML model to make loan predictions and provides explanations to justify its predictions as an incentive to provide incontestable explanations rather than reveal the true workings of the model. 




A major barrier to using explanations in adversarial contexts is that organizations keep their models confidential due to IP and legal reasons. However, confidentiality aids in manipulating explanations by allowing model swapping -- a model owner can use different models for generating predictions vs. explanations, swap the model for specific inputs, or change the model post-audits \cite{slack2020fooling, yadav2022xaudit, yan2022active}. This problem demands a technical solution which guarantees that a specific model is used for all inputs, for generating both the prediction and the explanation and prove this to the customer on the receiving end while keeping the model confidential.
% For example, under confidentiality the model developer can use different models for in vs. out of distribution points and generate innocuous explanations even though the model was biased to begin with \cite{slack2020fooling}. 

Another important barrier to using explanations in adversarial contexts is that many explanation algorithms are not deterministic and have many tunable parameters. An adversary can choose these parameters adversarially to make discriminatory predictions seem benign. Moreover, there is no guarantee that the model developer is following the explanation algorithm correctly to generate explanations. A plausible solution to counter this problem is consistency checks \cite{bhattacharjee2024auditing, dasgupta2022framework}. Apart from being a rather lopsided ask where the onus of proving correctness of explanations lies completely on the customer, these checks require collecting multiple explanation-prediction pairs for different queries and are therefore infeasible for individual customers in the real world. Compounding the issue, it has been shown that auditing local explanations with consistency checks is hard \cite{bhattacharjee2024auditing}.
Note that many of these issues persist even with a perfectly faithful algorithm for generating explanations.



We address the aforementioned challenges by proposing a system called \name. \name gives a protocol consisting of (1) cryptographic commitments which guarantee that the same model is used for all inputs and (2) Zero-Knowledge Proofs (ZKPs) which guarantee that the explanation was computed correctly using a predefined explanation algorithm, both while maintaining model confidentiality. See Fig.~\ref{fig:pictorialexpproof} for a pictorial representation of \name.

\name ensures uniformity of the model and explanation parameters through cryptographic commitments \cite{blum1983coin}. Commitments publicly bind the model owner to a fixed set of model weights and explanation parameters while keeping the model confidential. Commitments for ML models are a very popular and widely researched area in cryptography and hence we use standard procedures to do this ~\citep{kate2010constant}.

Furthermore, we wish to provide a way to the customer to verify that the explanation was indeed computed correctly using the said explanation algorithm, without revealing model weights. To do this, we employ a cryptographic primitive called Succinct Zero-Knowledge Proofs \cite{GMR, GMW}. ZKPs allow a prover (bank) to prove a statement (explanation) about its private data (model weights) to the verifier (customer) without leaking the private data. The prover outputs a cryptographic proof and the verifier on the other end verifies the proof in a computationally feasible way. In our case, if the proof passes the verifier's check, it means that the explanation was correctly computed using the explanation algorithm and commited weights without any manipulation.

How are explanations computed? The explanation algorithm we use in our paper is a popular one called LIME \cite{ribeiro2016should}, which returns a local explanation for the model decision boundary around an input point. We choose a local explanation rather than a global one since customers are often more interested in the behavior of the model around their input specifically. Additionally, LIME is model-agnostic, meaning that it can be used for any kind of model class (including non-linear ones).

Traditionally ZKPs are slow and are infamous for adding a huge computational overhead for proving even seemingly simple algorithmic steps. Moreover many local explanation algorithms such as LIME require solving an optimization problem and involve non-linear functions such as exponentials, which makes it infeasible to simply reimplement LIME as-is in a ZKP library. To remedy these issues, we experiment with different variants of LIME exploring the resulting tradeoffs between explanation-fidelity and ZKP-overhead. To make our ZKP system efficient, we also utilize the fact that verification can be easier than re-running the computation -- instead of \textit{solving} the optimization problem within ZKP, we verify the optimal solution using duality gap.


%Lastly, we observe that the LIME algorithm is not meant to be robust to adversaries by design, as it was to be primarily used for debugging by the model developer. Hence, we come up with a more robust version called robustLIME and also understand its performance-ZKP tradeoff.\cy{better way to put this?}


\textbf{Experiments.}~We evaluate \name on fully connected ReLU Neural Networks and Random Forests for three standard datasets on an Ubuntu Server with 64 CPUs of x86\_64 architecture and 256 GB of memory without any explicit parallelization. Our results show that \name is computationally feasible, with a maximum proof generation time of 1.5 minutes, verification time of 0.12 seconds and proof size of 13KB for NNs and standard LIME.