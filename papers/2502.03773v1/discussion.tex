\section{Discussion}\label{sec:discuss}
While \name guarantees model and parameter uniformity as well as correctness of explanations for a given model, it cannot prevent the kinds of manipulation where the model itself is corrupted -- the model can be trained to create innocuous explanations while giving biased predictions. Here usually a regularization term corresponding to the manipulated explanations is added to the loss function \cite{aivodji2019fairwashing, yadav2024influence}. Preventing such attacks requires a ZK proof of training; this is well-studied in the literature but is outside the scope of this work and we refer an interested reader to \cite{garg2023experimenting}.

Furthermore, to provide end-to-end trust guarantees for fully secure explanations, the explanations should be (1) faithful, stable and reliable, (2) robust to realistic adversarial attacks (such as the one mentioned above) and (3) should also be verifiable under confidentiality. This paper looks at the third condition by giving a protocol \name and implementing it for verifiable explanations under confidentiality, which has not been studied prior to our work. As such, we view \textit{our work as complementary and necessary} for end-to-end explanation trust guarantees.

%In attacks such as fairwashing \cite{anders2020fairwashing}, the adversary corrupts the training process and uses a tampered model outputted from the training process to give both predictions and explanations. Since the model $f$ has now itself changed and is being used to compute both predictions and explanations, \name cannot detect this manipulation. A ZK proof of learning/training might be successful in detecting manipulations of this kind.


%Adversarial manipulation can happen during the model training itself such that the model is trained to create innocuous explanations while giving biased predictions. Here usually a regularization term corresponding to the manipulated explanations is added to the loss function \cite{aivodji2019fairwashing, yadav2024influence}. Preventing such attacks requires ZK proof of training; however this is outside the scope of this paper and we refer an interested reader to \cite{garg2023experimenting, abbaszadeh2024zero}.