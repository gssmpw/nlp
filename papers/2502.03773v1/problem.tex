\section{The Problem of Explanation Manipulation \& Desiderata for Potential Solution}\label{sec:probsol}

\begin{tcolorbox}[colback=gray!10, colframe=gray!50, boxrule=0.5mm, sharp corners]
``Bottom line: Post-hoc explanations
are highly problematic in an
adversarial context" \cite{bordt2022post}
\end{tcolorbox}

Explanations have been seen as a way to increase trust in ML models by making models transparent \cite{} and consequently play a key role in regulations such as EU's GDPR and Biden's Executive Order \cite{}. In these regulations, explanations are seen as an to answer concerns regarding discrimination, recourse, opaqueness and correctness of predictions. Indeed, there are numerous studies that advocate for, build, and utilize explanations for such use cases \cite{yadav2022xaudit,}. exps are intended as a way to increase trust, add discussion about tampered model, you cantt do wht was done in hima's paper -- proof of learning might be a solution.

However, as argued by \cite{bordt2022post}, these use-cases are adversarial in nature, meaning involved parties have opposing interests and unfortunately  explanations (as deployed currently) are an ill fit for adversarial contexts. For instance, consider a bank which denies loan for an applicant based on an ML model's prediction and now has to return an explanation to the applicant justifying the model's prediction. Since the explanation can be used by the applicant in the court of law to prove discrimination, the bank is incentivized to return an \textit{incontestable} explanation rather than reveal the true working of the model.

%; rather it is  as long as this manipulation can remain under the veil. uses an ML model to make loan grant decisions

Adversarial manipulations to meet incentives of the above kind are made easier when the model is kept confidential (due to IP and legal reasons) and cannot be seen by customers on the receiving end of explanations. What if the model is swapped internally for different customers? For instance, \cite{slack2020fooling} show that under confidentiality the model developer can use different models for in vs. out of distribution points and generate innocuous explanations even though the model was biased to begin with. Similar is the case for auditors, who determine the correctness of explanations usually with API access to the hidden model. In the absence of perpetual testing, nothing stops the model developer from changing the model post-auditing even if the model and explanations were found to be unproblematic by the audit initially.

Another plausible avenue for manipulation of explanations is the explanation algorithm itself. In the absence of a test for explanation computation for each input, what is the guarantee that the explanation algorithm is not tampered with? For instance, the interpretable model in LIME could be learnt with non-uniform and adversarial samples for minority groups, such that the resulting explanations make discriminatory predictions look benign. Adversarial interpretable models can be learnt by solving an optimization problem while being good approximations to the original model as shown in \cite{shahin2022washing}.

Even worse, what stops the explanations from being arbitrary random justifications for malicious decisions? A versed reader might suggest consistency checks as a solution to this problem\cite{}. However, for consistency checks to work multiple pairs of explanations and predictions have to be collected over different queries, which is infeasible for individual customers. More importantly, here the onus of testing the correctness of explanations lies on the customers -- this makes for a rather irrational and lopsided ask, for multiple reasons including the fact that the customer is a layman in majority cases and even when not, may not have the resources to test.

Note that the above issues persist even with a perfectly faithful algorithm for generating explanations.

%which require a collection of many explanations and pred

To summarize, with the current technical tools and infrastructure, explanations can fool customers and auditors alike and lead to a false sense of security and trust in the society while benefiting adversaries. This deters their applicability in anything with legal consequences such as demonstrating fairness, auditing \cite{yadav2022xaudit} and so on. A lack of tools for verifiability and accountability in turn hinders the widespread adaptability of ML models in the society.


Motivated by these problems, in this paper, we work on a system, \name, to operationalize explanations in adversarial settings and make them functional for legally consequential tasks.

%\cite{yadav2022xaudit} discuss that even though explanations can reduce the query complexity of auditing models, correctness of explanations is a huge deterrant.

%Indeed a lot of work in the literature has shown that an adversary can manipulate explanations to achieve its ill-intentions while being discreet \cite{slack2020fooling, ifchhavi,aivodji2019fairwashing, anders2020fairwashing, shahin2022washing}. Specifically, when explanations serve to demonstrate fairness of a model, ironically they can be used by an adversary to hide the underlying unfairness, popularly termed `fairwashing' \citep{aivodji2019fairwashing, anders2020fairwashing, shahin2022washing}. When explanations are used as a tool to find the monetary value of training data, an adversary can use them to superficially increase the value of some training samples \cite{ifchhavi}. When explanations are used in auditing, they can be used to mislead auditing decisions \cite{yadav2022xaudit}. 

%Adversarial manipulations with explanations prevalently exploit the Rashomon Effect -- the fact that for any task, there can exist multiple accurate models which lie on a spectrum when it comes to properties such as fairness and privacy \cite{}. Due to this effect, an adversary with ill-intentions can use two different but indistinguishable models for giving predictions vs. explanations \cite{slack2020fooling, ifchhavi,aivodji2019fairwashing, anders2020fairwashing, shahin2022washing, noppel2024sok}. In the loan example earlier the bank would use two similarly accurate models with this technique, one for giving predictions such that they benefit the bank and another for giving incontestable explanations for these questionable predictions. Since the predictions are e

%A prevalent technique for adversarial manipulation with explanations is to use two different but indistinguishable models for giving predictions vs. explanations \cite{slack2020fooling, ifchhavi,aivodji2019fairwashing, anders2020fairwashing, shahin2022washing, noppel2024sok}. This is possible due to the existence of multiple accurate models which lie on a spectrum when it comes to properties such as fairness, privacy and so on -- called the Rashomon Effect in ML models \cite{}. 

%For instance, in the loan example earlier, the bank would keep two models with similar accuracy

%To achieve this goal, the bank trains another version of the unfair model such that the new model has similar test accuracy as the unfair model but the explanations coming from the new model project fairness, as desired by the bank.

%consider the following fairwashing setting : a bank which grants loans using an ML model. This bank is incentivized monetarily to use an unfair model for accepting/rejecting loan applications but still needs to project to an external entity such as a customer or an auditor that it is using a fair model, to avoid legal action against itself.



%accepts or rejects loan applications using the unfair model (due to some incentives) but the explanations come from the fair model. Since the two models are indistinguishable w.r.t. test accuracy for an external entity, the.. a bank holds two loan prediction models -- a fair and an unfair one -- which are indistinguishable in terms of test accuracy. .. a bank makes loan prediction using an unfair model but gives explanations using a fair model.  This has been a long-standing problem , detering the usage of explanations in .... leading to negative stuff if explanations are even the right ttool forr anything adversarial... In audititng the deterrant is the trustworthiness of thee eexplantions (faithfulness aside).. essentially explanations can give a false sense of security or trust.

\textbf{Formal Problem Setting.} Formally, a model owner confidentially holds a classification model $f$ which is not publicly released due to legal and IP reasons. A user supplies an input $x$ to the model owner, who responds with a prediction $f(x)$ and an explanation $\Ef$  where $\E$ is the possibly-randomized algorithm generating the explanation.%The explanation algorithm $\E$ may be randomized.

\textbf{Solution Desiderata.} To make explanations functional for accountability in adversarial settings, we wish to give the following four guarantees for the above setting :

\begin{enumerate}
    \item (Model Uniformity) the same model $f$ is used by the model owner for all inputs  : our solution is to use cryptographic commitments which force the model owner to commit to a model fixed apriori,
    \item (Explanation Correctness) the explanation algorithm $\E$ is run correctly for generating explanations without any adversarial manipulation for all inputs : our solution is to use Zero-Knowledge Proofs, wherein the model owner supplies a cryptographic proof of correctness to be verified by the customer in a very feasible way,
    \item (Model Consistency) the same model $f$ is used for inference and generating explanations : this is ensured by generating inference and explanations as a part of a since system and using model commitments,
    \item (Model Confidentiality) the model $f$ is kept confidential in the sense that any technique for guaranteeing (1)-(3) does not leak anything else about the hidden model $f$ than is already leaked by predictions $f(x)$ and explanations $\Ef$ without using the technique : this comes as a by-product of using ZKPs and we prove this later in Sec. theorem \ref{},
    \item (Technique Security?) the technique used for guaranteeing (1)-(3) is sound and complete: this comes as a by-product of using ZKPs and we prove this later in Sec. theorem \ref{}. (i want to say something like cnt be manipulated more than what can be done already with predictions + explanations -- but this might run against us)
\end{enumerate}