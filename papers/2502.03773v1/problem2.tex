\section{Problem Setting \& Desiderata for Solution}\label{sec:probsol}



%By virtue of aiming to make models transparent,
%\cy{shorten this and put more focus on the formal setting onwards}
%Explanations are intended as a way to improve trust in ML models by virtue of making them transparent. Consequently, other than benign debugging applications, they are also seen as an answer to concerns regarding discrimination, recourse and correctness of predictions \cite{}. However, many of these use-cases involve parties with misaligned incentives, which leads to failure of explanations as a trust-enhancing tool. For instance, consider a bank which denies loan to an applicant based on an ML model's prediction and now has to return an explanation to the applicant justifying the model's prediction. Since the explanation can be used by the applicant to prove discrimination in the court of law, the bank is incentivized to return an \textit{incontestable} explanation rather than reveal the true working of the model.

%Such adversarial manipulations are exacerbated by conditions in which models are deployed, an important one being confidentiality of models. For instance, a bank which trains a loan prediction model on sensitive data will keep its model confidential due to IP and legal reasons. But confidentiality now enables the bank to potentially swap the model at its choice of customers without being caught. \cite{slack2020fooling} show that under confidentiality the model developer can use different models for in vs. out of distribution points and generate innocuous explanations even though the model was biased to begin with. Similar is the case for auditors, who determine the correctness of explanations usually with API access to the hidden model. In the absence of perpetual testing, nothing stops the bank from changing the model post-auditing even if the model and explanations were found to be unproblematic by the audit initially.

%Additionally, explanation algorithms themselves are often not fully deterministic and involve hyperparameters which can be tampered with while seeming completely benign. For instance, if the bank uses LIME explanations, it could learn the interpretable model in LIME with non-uniform and adversarial samples for minority groups such that the resulting explanations make discriminatory predictions look safe \cite{}. It could also pick different values of bandwidth parameter for minority groups as discussed in Sec. \ref{sec:advlime}. Adversarial interpretable models can also be learnt by solving an optimization problem while being good approximations to the original model as shown in \cite{shahin2022washing}.

%A plausible solution to some of these issues could be consistency checks. However, doing such checks is infeasible for individual customers as these involve collecting multiple pairs of explanations and predictions over different queries to get correct answers with high probability \cite{}. More importantly, here the onus of testing the correctness of explanations \textit{completely} lies on the customers -- this makes for a rather irrational and lopsided ask for multiple reasons including the fact that the customer is a layman in many cases and even when not, may not have the resources to test.

%\textit{Note that the above issues persist even with a perfectly faithful algorithm for generating explanations.}

To recall, explanations fail as a trust-enhancing tool in adversarial use-cases and can lead to a false sense of security while benefiting adversaries. Motivated by these problems, we investigate if a technical solution can be designed to operationalize explanations in adversarial settings.


\textbf{Formal Problem Setting.} Formally, a model owner confidentially holds a classification model $f$ which is not publicly released due to legal and IP reasons. A customer supplies an input $x$ to the model owner, who responds with a prediction $f(x)$ and an explanation $\Ef$  where $\E$ is the possibly-randomized algorithm generating the explanation. %This explanation can be verified by the customer. The customer is also guaranteed that the same model is used for everyone.

%The explanation algorithm $\E$ may be randomized.

\textbf{Solution Desiderata.} A technical solution to operationalize explanations in adversarial use-cases should provide the following guarantees.

\begin{enumerate}
    \item (Model Uniformity) the same model $f$ is used by the model owner for all inputs  : our solution is to use cryptographic commitments which force the model owner to commit to a model prior to receiving inputs,
    \item (Explanation Correctness) the explanation algorithm $\E$ is run correctly for generating explanations for all inputs : our solution is to use Zero-Knowledge Proofs, wherein the model owner supplies a cryptographic proof of correctness to be verified by the customer in a computationally feasible manner,
    \item (Model Consistency) the same model $f$ is used for inference and generating explanations : this is ensured by generating inference and explanations as a part of the same system and by using model commitments,
    \item (Model Confidentiality) the model $f$ is kept confidential in the sense that any technique for guaranteeing (1)-(3) does not leak anything else about the hidden model $f$ than is already leaked by predictions $f(x)$ and explanations $\Ef$ without using the technique : this comes as a by-product of using ZKPs and commitments (See Sec. \ref{app:subsec:secproof} for the formal theorem and proof),
    \item (Technique Reliability) the technique used for guaranteeing (1)-(4) is sound and complete (as in Sec.\ref{sec:prelims}): this comes as a by-product of using ZKPs and commitments (See Sec. \ref{app:subsec:secproof} for the formal theorem and proof).
\end{enumerate}

Our solution \name which provides the above guarantees will be discussed in Sec. \ref{sec:verifylime}.