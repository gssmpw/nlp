\section{Variants of LIME}\label{sec:varlime}

%In this section, we propose different variants of LIME with the aim of identifying more ZKP-amenable designs by evaluating and comparing their overheads later on in Sec.\ref{sec:expts}.

Building zero-knowledge proofs of explanations requires the explanation algorithm to be implemented in a ZKP library\footnote{More precisely, arithmetic circuits for the explanation algorithm are implemented in the ZKP library.} which is known to introduce a significant computational overhead. Given this, a natural question that comes to mind is if there exist variants of LIME which provide similar quality of explanations but are more ZKP-amenable by design, meaning they introduce a smaller ZKP overhead?

\textbf{Standard LIME Variants.}~To create variants of standard LIME (Alg.\ref{alg:limeinclear}), we focus on the two steps which are carried out numerous times and hence create a computational bottleneck in the LIME algorithm -- sampling around input $x$ (Step 6 in Alg. \ref{alg:limeinclear}) and computing distance using exponential kernel (Step 7 in Alg. \ref{alg:limeinclear}). For sampling, we propose two options as found in the literature : gaussian (G) and uniform (U)  \cite{ribeiro2016should, garreau2020explaining, garreau2020looking}. For the kernel we propose to either use the exponential (E) kernel or no (N) kernel. These choices give rise to four variants of LIME, mentioned in Alg. \ref{alg:limevarinclear}. We address each variant by the intials in the brackets, for instance standard LIME with uniform sampling and no kernel is addressed as `LIME\_U+N'.

%propose some other variants of LIME and test if they can provide similar quality 

%\paragraph{Border LIME.} Traditionally LIME is created for the trusted setting where model developers are benign. However as mentioned earlier, the settings in which post-hoc explanations are meant to be used are adversarial in nature, which necessitates a version of LIME which is robust to adversarial manipulations. Next we briefly mention the threat model followed by the attack and defense for one such manipulation and additional concerns to robustify LIME.
%hold a non-malicious model and to set the parameters of LIME without adversarial incentives

%\textit{Threat Model.} The adversary in our case is the model developer. The adversary has access to the full training data and trains the model, but does not have access to the test inputs (which may come from a different distribution than the training distribution). Upon receiving a test input, it returns a prediction and an explanation. Parameters of the explanation algorithm are also set by the adversary. \cy{should refer to model developer as adversary or model developer?adversary should be mentioned at beginning or end?}

%The adversary here is the model developer itself who has access to the full training data and trains the model. Since explanations are provided by the adversary, it also gets to set the parameters of the explanation algorithm. However the adversary does not know the kind of inputs it will receive at test time. Having received an input from the user, the adversary returns a corresponding prediction and an explanation. %This adversary has its own incentives due to which it can create malicious explanations, for instance a bank not wanting to give loans to a minority group might give unfair loan application predictions and wishes to hide the unfairness with explanations.

%Now we will discuss some plausible adversarial manipulations and ways to prevent them with ZKPs and commitments.

%\textit{Attack1 and Defense.} The adversary can use different values for hyperparameters in LIME for different inputs to create seemingly innocuous explanations. For instance let us consider the bandwith parameter $\sigma$ of the similarity kernel in LIME, which controls the size of the neighborhood with large weights around the input point. A large value of the parameter means a large region around the input will be highly weighted and as a result the generated explanation tends to the global explanation with increasing sampling radius. This phenomena can be exploited by the adversary to hide discrimination \cy{shouldnt talk too much about fairness?} apparent in local explanations
%by outputting global explanations which may seem innocuous \cite{}. Our framework can rule out this kind of attack by (1) having the model developer cryptographically commit to the parameters of the explanation algorithm  apriori, which enforces that the same value of the parameters are used for all input points (this value can come from regulatory suggestions), or (2) having the customer (aka verifier) supply the value of the parameters to be used by the model developer.

%, but had it output the local explanation, the discrimination could have been apparent in the explanation 

%giving discriminatory predictions for a minority group in a part of the input space; it can hide its intent by setting a large value of the bandwidth parameter and 

%Additionally, the parameters of LIME can be set in an adversarial fashion. Consider the bandwidth parameter $\sigma$ of the similarity kernel which plays a key role in producing faithful explanations -- this is evidenced by many studies in the explanation literature which study its effect on the generated explanation and explore ways to set its value \cite{}. A small value of the bandwidth parameter results in a small neighborhood around the input point with high weights, thereby leading to unstable explanations (as all the sampled points are very similar to the input making it harder to learn a classifier). On the other hand, a large value of the parameter with a large neighborhood around the input implies that a global explanation is being learnt, rather than a local one, resulting in unfaithful explanations. While prior studies majorly look at the bandwidth parameter from the lens of faithfulness of generated explanations, we observe that an adversary can use this parameter to generate explanations that match its incentives. For instance, a model developer giving discriminatory predictions for a minority group in a part of the input space can hide its intent by setting a large value of the bandwidth parameter and outputting global explanation which may be seemingly innocuous, but had it output the local explanation, the discrimination could have been apparent in the explanation. To eliminate the potential for such attacks, we propose that either (1) the prover cryptographically commit to a value of the bandwidth parameter apriori, which enforces that the same value of the bandwidth parameter is used for all input points (this value can come from regulatory suggestions), or (2) the verifier supply a value of the bandwidth parameter to be used by the prover. Note that the same recommendations can apply to other parameters.

%Certain traditional considerations for LIME become more critical when viewed through the lens of adversarial manipulation. We describe these considerations below and give solutions to deal with them in our ZKP system.

\textbf{BorderLIME.} An important consideration for generating meaningful local explanations is that the sampled neighborhood should contain points from different classes \cite{laugel2018defining}. Any reasonable neighborhood for an input far off from the decision boundary will only contain samples from the same class, resulting in vacuous explanations.


To remedy the problem, \cite{laugel2017inverse, laugel2018defining} propose a \textit{radial} search algorithm, which finds the closest point to the input $x$ belonging to a different class, $x_{border}$, and then uses $x_{border}$ as the input to LIME (instead of original input $x$). Their algorithm incrementally grows (or shrinks) a search area radially from the input point and relies on random sampling within each `ring' (or sphere), looking for points with an opposite label. To cryptographically prove this algorithm, we would either have to reimplement the algorithm as-is or would have to give a probabilistic security guarantee (using a concentration inequality), both of which would require many classifier calls and thereby many proofs of inference, becoming inefficient in a ZKP system.


We transform their algorithm into a line search version, called BorderLIME, given in Alg. \ref{alg:robustLIME_highlevel} and \ref{alg:findclosestpoint}, using the notion of Stability Radius which is now fed as a parameter to the algorithm. The stability radius for an input $x$, $\delta_x$, is defined as the 
largest radius for which the model prediction remains unchanged within a ball of that radius around the input \( x \). The stability radius \( \delta \) is defined as the minimum stability radius across all inputs \( x \) sampled from the data distribution $D$. Formally,  
$
\delta = \inf_{x \sim \mathcal{D}} \delta_x, \quad \text{where} \quad \delta_x = \sup \{ r \geq 0 \mid f(x') = f(x), \forall x' \in \mathcal{B}(x, r) \}
$. Here \( \mathcal{B}(x, r) = \{ x' \mid \|x' - x\| \leq r \} \) denotes a ball of radius \( r \) centered at \( x \). Stability radius ensures that for any input from the data distribution, the model's prediction remains stable within at least a radius of \( \delta \).

Our algorithm samples $m$ directions and then starting from the original input $x$, takes $\delta$ steps until it finds a point with a different label along all these directions individually. The border point $x_{border}$ is that oppositely labeled point which is closest to the input $x$. Furthermore, unlike in the algorithm in \cite{laugel2017inverse}, our algorithm can exploit parallelization by searching along the different directions in parallel since these are independent.

%This algorithm uses the notion of Stability Radius $\delta$, such that the model prediction remains the same in a ball $\mathcal{B}(x, \delta)$ of radius $\delta$ around any input $x$ from the data distribution.  Formally, $\delta = \sup \{ r \geq 0 \mid f(x') = f(x), \forall x' \in \mathcal{B}(x, r) \}$ where \( \mathcal{B}(x, r) = \{ x' \mid \|x' - x\| \leq r \} \) denotes the ball of radius \( r \) centered at \( x \). largest radius for which the model prediction remains unchanged within a ball $\mathcal{B}(x, \delta)$ of radius $\delta$ around any input \( x \) from the data distribution. 

Determining the optimal value of the stability radius is an interesting research question, but it is not the focus of this work. We leave an in-depth exploration of this topic to future work while providing some high-level directions and suggestions next. Stability radius can (and perhaps should) be found \textit{offline} using techniques as proposed in \cite{jordan2019provable, yadav2024fairproofconfidentialcertifiable} or through an offline empirical evaluation on in-distribution points. A ZK proof for this radius can be generated one-time, in an offline manner and supplied by the model developer (for NNs see \cite{yadav2024fairproofconfidentialcertifiable}). It can also be  pre-committed to by the model developer (see Sec. \ref{sec:verifylime}).  

%If the stability radius is reasonable enough (meaning that the model is not too sensitive), the algorithm will only go through a few iterations. and finding a reasonable stability radius by starting with a value and reducing it iteratively; in a real-world setting if the test points are in-distribution, the stability radius found in such a way will work.


\begin{algorithm}[tbh]
\begin{algorithmic}[1]
 \caption{\textsc{BorderLIME}}
   \label{alg:robustLIME_highlevel}
    
    \STATE {\bfseries Input:} Input point $x$, Classifier $f$
    \STATE {\bfseries Parameters:} Number of points $n$ to be sampled around input point, Length of explanation $K$, Bandwidth parameter $\sigma$ for similarity kernel

    \STATE  {\bfseries Output:} Explanation $e$
    \STATE
    \STATE $x_{border}:=$\\\hspace{2em}\textsc{Find\_Closest\_Point\_With\_Opp\_Label}($x, f$) \hfill \textcolor{blue}{$\rhd$} See Alg. \ref{alg:findclosestpoint}
    \STATE $e :=$ \textsc{LIME}($x_{border}, f$) \textcolor{blue}{$\rhd$} Note that any variant of LIME can be used here
    \STATE Return Explanation $e$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tbh]
\begin{algorithmic}[1]
 \caption{\textsc{Find\_Closest\_Point\_With\_Opp\_Label}}
   \label{alg:findclosestpoint}
    
    \STATE {\bfseries Input:} Input point $x$, Classifier $f$
    \STATE {\bfseries Parameters:} Number of random directions $m$, Stability radius $\delta$, Iteration Threshold $T$

    \STATE  {\bfseries Output:} Opposite label point $x_{border}$
    \STATE
    \STATE $\left\{\vec{u}_0, \vec{u}_1 \cdots \vec{u}_{m-1}\right\}:=$ Sample $m$ random directions
    \STATE Initialize $\textsc{$dist_0$} \cdots \textsc{$dist_{m-1}$}$ as $\inf$
    \FOR{$\vec{u}_i \in \left\{\vec{u}_0, \vec{u}_1 \cdots \vec{u}_{m-1}\right\}$}
    \STATE $x_{border_i} := x$
    \STATE $iter := 0$
    \WHILE{$f(x_{border_i}) == f(x)$ and $iter \leq T$}
    \STATE $x_{border_i} := x_{border_i}+ \delta \vec{u}_i$
    \STATE $iter := iter + 1$
    \ENDWHILE
    \IF{$f(x_{border_i}) != f(x)$}
    \STATE \textsc{$dist_i$} $:= \ell_2{(x, x_{border_i})}$
    \ENDIF
    \ENDFOR
    \STATE $x_{border}:= x_{border_i}$ \textrm{such that} $i:= \arg \min dist_i$
    \STATE Return $x_{border}$
\end{algorithmic}
\end{algorithm}

%\textit{Discussion on other kinds of attacks.} The model developer can train the model such that the model is very unstable in particular regions of the input space corresponding to minority classes, yet the LIME explanation will not be able to catch this manipulation. To remedy this problem, we propose to obtain a stability radius for the input point such the model prediction does not change in a ball of stability radius around the input point and check that the radius is above a threshold. A ZK proof for this radius can be supplied by the model developer along with the LIME ZK proof such as shown in \cite{yadav2024fairproofconfidentialcertifiable}.

%Lastly, adversarial manipulation can happen during the model training itself such that the model is trained to create innocuous explanations while giving biased predictions. Here usually a regularization term corresponding to the manipulated explanation is added to the loss function \cite{aivodji2019fairwashing, yadav2024influence}. Preventing such attacks requires ZK proof of training; however this is outside the scope of this paper and we refer an interested reader to \cite{garg2023experimenting, abbaszadeh2024zero}.

%This kind of manipulation requires other kinds of ZKP solutions. 


%\cy{have a section on limitations - our thing does test time doesnt prevent training time manipulations}Other kinds of manipulation where a term corresponding to the explanation is added to the loss function \cite{aivodji2019fairwashing, yadav2024influence} can be addressed with a ZK proof of training; however this is outside the scope of this paper and we refer an interested reader to \cite{garg2023experimenting, abbaszadeh2024zero}.

%However, this technique only gets the closest point in one direction -- the model makes its prediction due to the decision boundaries in multiple directions \cy{I dont think this argument is correct -- one direction might be enough to make the prediction}. Our resulting algorithm is called robustLIME and is given in Alg. \ref{alg:robustLIME}.

%\cy{COMMENTS on DAN's version of the algorithm: instead of picking the minimum distance $x_{border}$ Line 13 of Alg. \ref{alg:findclosestpoint} and line 5 of \ref{alg:robustLIME_highlevel}, dan's algorithm uses all the points that we came across along all directions to fit the linear model. This is wrong for 2 reasons. Firstly, this will be a highly imbalanced dataset with only m points of the opposite class (incase we wish to set m to number of dimensions, maximum number of dimensions in our datasets is 24, which might be too expensive) -- hence it essentially doesn't solve the problem that we had in the first place. Secondly, I don't think fitting a linear model across all directions is correct -- I think the linear model should be based on the closest point, not on all the directions. Do you have a pictorial example where the decision would be affected by more direction that the closest point one??}