%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false,printfolios=true}
\renewcommand\footnotetextcopyrightpermission[1]{}
%%
\usepackage{algorithm}  % 提供algorithm环境
\usepackage{algpseudocode}  % 替代algorithmic,提供\State和\For等命令
\usepackage{amsmath}  % 用于数学符号
\usepackage{multirow}
\usepackage{multicol}
\usepackage{pifont} % 包含 \ding{55}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Arxiv]{Make sure to enter the correct
  conference title from your rights confirmation email}{2025}{arxiv}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Structure-prior Informed Diffusion Model for Graph Source Localization with Limited Data}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Hongyi Chen}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Shenzhen International Graduate School, Tsinghua University}
  \city{Shenzhen}
  \country{China}
}

\author{Jingtao Ding}
\affiliation{%
  \institution{Department of Electronic Engineering, Tsinghua University}
  \city{Beijing}
  \country{China}
}

\author{Xiaojun Liang}
\affiliation{%
  \institution{Peng Cheng Laboratory}
  \city{Shenzhen}
  \country{China}
}

\author{Yong Li}
\affiliation{%
  \institution{Department of Electronic Engineering, Tsinghua University}
  \city{Beijing}
  \country{China}
}

\author{Xiao-Ping Zhang}
\affiliation{%
  \institution{Shenzhen International Graduate School, Tsinghua University}
  \city{Shenzhen}
  \country{China}
}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The source localization problem in graph information propagation is crucial for managing various network disruptions, from misinformation spread to infrastructure failures. While recent deep generative approaches have shown promise in this domain, their effectiveness is limited by the scarcity of real-world propagation data. This paper introduces SIDSL (\textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion model for \textbf{S}ource \textbf{L}ocalization), a novel framework that addresses three key challenges in limited-data scenarios: unknown propagation patterns, complex topology-propagation relationships, and class imbalance between source and non-source nodes. SIDSL incorporates topology-aware priors through graph label propagation and employs a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP). Additionally, we propose a structure-prior biased denoising scheme that initializes from structure-based source estimations rather than random noise, effectively countering class imbalance issues. Experimental results across four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3\% improvements in F1 scores compared to state-of-the-art methods. Notably, when pretrained with simulation data of synthetic patterns, SIDSL maintains robust performance with only 10\% of training data, surpassing baselines by more than 18.8\%. These results highlight SIDSL's effectiveness in real-world applications where labeled data is scarce.

% Graph information propagation issues like the spread of misinformation, cyber threats, or infrastructure breakdowns are prevalent and have significant societal impacts. Identifying the source of such propagation by analyzing snapshots of affected networks is crucial for managing crises like disease outbreaks and enhancing network security. Traditional methods rely on metrics derived from graph topology and are limited to specific propagation models, while deep learning models face the challenge of data scarcity. We propose \textbf{SIDSL}~(\textbf{A}daptive \textbf{S}ource \textbf{L}ocalization \textbf{Diff}sion Model), a novel adaptive source localization diffusion model to achieve accurate and robust source localization across different graph topologies and propagation modes by fusing the principles of information propagation and restructuring the label propagation process within the conditioning module. Our approach can not only capture the characteristics of propagation patterns effectively but also adapt to real-world patterns quickly on synthetic propagation data when domain data is limited. Evaluations of various datasets demonstrate SIDSL's superior effectiveness, accuracy, and adaptability in real-world applications, showcasing its robust performance across different localization scenarios. The code can be found at https://anonymous.4open.science/r/SIDSL-4FE0.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10002951.10003227</concept_id>
%        <concept_desc>Information systems~Information systems applications</concept_desc>
%        <concept_significance>300</concept_significance>
%        </concept>
%    <concept>
%        <concept_id>10003752.10003753.10003757</concept_id>
%        <concept_desc>Theory of computation~Probabilistic computation</concept_desc>
%        <concept_significance>100</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[300]{Information systems~Information systems applications}
% \ccsdesc[100]{Theory of computation~Probabilistic computation}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Source Localization, Diffusion Model, Graph Source Localization, Information Diffusion}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle



\section{Introduction}
In today’s highly interconnected world, graph information propagation issues, such as misinformation spread, cyber threats, and infrastructure failures, have far-reaching consequences for society~\citep{ding2024artificialintelligencecomplexnetwork}. The ability to quickly identify the source of these disruptions is critical for mitigating their impact. By analyzing snapshots of affected networks, we can trace the origin of the spread, a process essential for managing crises like disease outbreaks~\citep{ru2023inferring}, enhancing network security~\citep{kephart1993measuring}, and preventing further damage in scenarios such as power grid failures~\citep{amin2007preventing}.

Early methods~\citep{lappas2010finding,shah2012rumor,prakash2012spotting,luo2013identifying,zhu2014information,zhu2014robust} for source localization in graphs rely on metrics or heuristics derived from the graph's topology, applicable only to specific propagation patterns like the Susceptible-Infected (SI) or Independent Cascade (IC) models. Notably, Wang et al.~\citep{wang2017multiple} overcome this limitation by introducing a label propagation algorithm based on the intuition of source centrality. Nevertheless, the approach has limitations in both expressiveness and scalability, as they cannot effectively encode graph topological information. Data-driven methods~\citep{dong2019multiple,wang2022invertible,hou2023sequential} overcome these limitations as they directly learn graph neural networks~(GNNs) to capture the propagation process exhibited in empirical data, but still neglect the indeterminacy of information propagation that corresponds to the uncertain nature of source localization~\cite{ling2022source}.
Recently, deep generative models including variational autoencoders~\citep{ling2022source}, normalization flows~\citep{xu2024pgsl} and diffusion models~\citep{huang2023two,yan2024diffusion} have been adopted for solving the source localization problem. These probabilistic generative methods can quantify the indeterminacy in source localization by learning the empirical data distribution and advance the state-of-the-art performance.

However, collecting real-world propagation data for deep generative methods is difficult and costly, posing significant requirements on source localization models that can adapt to real-world environments with limited data. This brings up three main following challenges.
\begin{itemize}
    \item \textbf{Firstly, real-world graphs typically exhibit unknown propagation patterns, which becomes far more challenging to characterize when data is limited.} In this regard, existing methods~\citep{dong2019multiple,wang2022invertible,ling2022source,yan2024diffusion} rely purely on data to gain an understanding of the propagation patterns, limiting their capability to generalize in unseen scenarios. 
% This challenge highlights the importance of leveraging pattern properties that remain stable across different propagation scenarios.
    \item \textbf{Secondly, complex interrelations between propagation patterns and graph topology are difficult to capture with limited data.}
Existing deep learning methods rely on a large amount of labeled data from the target network~(i.e., identified source nodes from historical propagation) to account for the impact of structural heterogeneity on propagation patterns. Due to this heavy dependence on labeled data, these models typically underperform when applied to networks with limited training samples, hindering their practical applicability.
    \item \textbf{Thirdly, the inherent class imbalance between source and non-source nodes becomes more harmful under data scarcity, compromising the accuracy and reliability of source identification.} In most propagation scenarios, source nodes naturally constitute a small minority of the total graph nodes~\cite{cheng2024gin}. Limited training data can further amplify this problem, leading to biased distribution models that tend to degenerate towards predicting all nodes as non-sources.
\end{itemize}




% Therefore, in this paper, we propose a novel method, namely \textbf{A}daptive \textbf{S}ource \textbf{L}ocalization \textbf{Diff}sion Model (\textbf{SIDSL}), a diffusion-based~\cite{ho2020denoising} framework that learns the source node distribution conditioned on both network topology and observed node states for source localization. 
Therefore, in this paper, we propose a novel generative diffusion framework, namely \textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion Model for \textbf{S}ource \textbf{L}ocalization (\textbf{SIDSL}), which leverages topology-aware priors to achieve robust source localization with limited propagation data collected from real environments. Taking both graph topology and observed node states as input, SIDSL employs a denoising diffusion model guided by structural priors to predict source node distributions.
To tackle the first challenge, 
We leverage graph structure-based source estimations, generated through graph label propagation, which identifies potential sources based on locally highest infection values. Integrated into the denoising network, these topology-aware priors provide stable guidance across different propagation patterns, enhancing generalization to unseen propagation scenarios.
% we leverage graph structure-based source estimations as priors, which capture topology-dependent source characteristics that remain stable across different propagation patterns. The estimations are obtained by applying graph label propagation, which identifies potential source nodes based on their locally highest aggregated infection values in the graph structure. These topology-aware priors provide stable guidance by being integrated as conditional input to the denoising network, allowing our diffusion framework to combine structural knowledge with data-driven pattern learning. This design enhances the model's generalization capability when encountering new propagation scenarios.
To address the second challenge, 
We design a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP) that combines label propagation for tracing infection pathways with GNN for efficient topology-aware feature extraction, enabling effective learning of topology-propagation relationships from limited data.
% we propose a novel conditional design of GNN-parameterized label propagation (GNN-LP). This module captures the interplay between topology and propagation by combining two complementary components: label propagation traces infection pathways to reveal structural dependencies, while GNN learns to extract topology-aware features efficiently from limited samples. Through this integration, our model transforms discrete infection states into representations that encode both propagation dynamics and structural patterns, enabling effective learning of their complex relationships even with limited data.
To tackle the third challenge, 
we develop a structure-prior biased denoising process that initializes denoising process from structure-based source estimations rather than random noise, creating a natural bias towards potential source nodes to prevent degeneration with limited, imbalanced data.
% we propose an innovative estimation-biased denoising scheme that starts from structure-based source estimations rather than random noise. This informed initialization creates a natural bias towards potential source nodes in the denoising trajectory, effectively preventing the model from collapsing into trivial non-source predictions even with limited and imbalanced training data.

%mention pretraining
The above designs work together in our structure-prior informed diffusion framework to capture stable structural patterns independent of specific propagation dynamics, which allows our model to learn pattern-invariant features using synthetic propagation data simulated from established models~\cite{keeling2005networks,kempe2003maximizing,kermack1927contribution}. By focusing on these invariant features rather than specific propagation patterns, our model can effectively transfer the learned knowledge to real-world scenarios through efficient few-shot or zero-shot learning, as shown in Figure \ref{fig:approach}.

Our contributions are summarized as follows:\\
(1)~We propose a structure-prior informed diffusion framework that effectively addresses the challenge of limited data in source localization by incorporating topology-aware priors, enabling robust generalization to unknown propagation patterns in real-world scenarios.\\
(2)~We introduce a series of innovative techniques, including a propagation-enhanced conditional denoiser with GNN-LP module and estimation-biased denoising, which work synergistically to handle structural heterogeneity and class imbalance issues. \\
(3) We evaluate SIDSL's performance across four real-world datasets and demonstrate its superior effectiveness in source identification tasks. Our method consistently outperforms state-of-the-art baselines by 7.5-13.3\% in F1 scores. Through effective pretraining with simulation data of synthetic patterns, SIDSL maintains robust performance with limited empirical data, surpassing baselines by over 19\% in few-shot (using only 10\% training samples) and 40\% in zero-shot settings, demonstrating strong generalization capability in real-world applications. Additional evaluations further validate SIDSL's effective synthetic-to-real transfer capability in improving sample efficiency and reducing training time.
% (3)~We evaluate the performance of SIDSL against state-of-the-art methods under various propagation patterns and real network datasets. Additionally, we assess the model's generalizability across different network topologies and propagation patterns, demonstrating its ability to overcome the identified challenges. SIDSL shows a 7.5\%-12.1\% improvement in real-world propagation datasets, highlighting its accuracy and adaptability.

\begin{figure}[t]
\centering
  \includegraphics[width=\linewidth]{fig/task.pdf} 
  % \vspace{-1em}
  \caption{The proposed SIDSL approach.}
    \label{fig:approach}
    \vspace{-2em}
\end{figure}

\section{Related Works}
\label{r_w}
\subsection{Source Localization}
As the inverse problem of information propagation on networks, source localization refers to inferring the initial propagation sources given the current diffused observation, such as the states of the specified sensors or a snapshot of the whole network status~\citep{shelke2019source}. It can be applied to tasks like rumor source identification and finding the origin of rolling blackouts in intelligent power grids~\citep{shelke2019source}. Early approaches are rule-based and rely on metrics or heuristics derived from the network’s
topology for source identification~\citep{shah2011rumors,zhu2014information,zhu2014robust}. For example, \citet{shah2011rumors} develop a rumor-centrality-based maximum likelihood estimator under the Susceptible-Infected (SI)~\citep{kermack1927contribution} propagation pattern. This kind of method fails to effectively encode topology information. Later, deep learning-based methods devised for capturing the topological effect exhibited in
empirical data have been proposed~\citep{lappas2010finding,luo2013identifying,wang2017multiple,dong2019multiple,wang2022invertible,hou2024new}. However, most of them fail to model the uncertainty of the location of sources, as the forward propagation process is stochastic. To overcome this, deep generative models have been adopted~\cite{ling2022source,yan2024diffusion,wang2024joint,xu2024pgsl,huang2023two}. SLVAE~\citep{ling2022source} utilizes the Variational Auto-Encoders~(VAEs) backbone and optimizes the posterior for better prediction. However, it is difficult to converge when the propagation pattern is complicated due to the nature of VAEs. DDMSL~\citep{yan2024diffusion} models the Susceptible-Infected-Recovered~(SIR) ~\citep{kermack1927contribution}infection process into the discrete Diffusion Model~(DM)~\citep{ho2020denoising}, and design a reversible residual block based on Graph Convolutional Networks~(GCNs)~\citep{kipf2016semi}. However, it requires additional intermediate propagation data and cannot be generalized to other propagation patterns. Our method demonstrates superior functionality and adaptability for real-world applications, requiring fewer input data while addressing existing limitations, thus offering greater practical value. {We provide a comparison of typical source localization methods in the Appendix~\ref{app:comp}.} 
% can be put back to main content

\subsection{Typical Propagation Models}
Information propagation estimation models information spread in networks and explains propagation sources, with applications in event prediction~\citep{zhao2021event}, adverse event detection~\citep{wang2018multi}, and disease spread prediction~\citep{Tang2023Enhancing}. Two main model categories exist: infection models and influence models. Infection models like Susceptible-Infected (SI) and Susceptible-Infected-Susceptible (SIS) manage transitions between susceptible and infected states in networks~\citep{kermack1927contribution,keeling2005networks}. In these models, infected nodes attempt to infect adjacent nodes with probability $\beta$ at each iteration, while in SIS, infected nodes may revert to susceptible with probability $\lambda$. The Susceptible-Infected-Recovered (SIR) model extends this by adding a recovered state.

Independent Cascade (IC) and Linear Threshold (LT)~\citep{kempe2003maximizing} are influence models that examine influence spread in social and infrastructure networks. In the IC model, nodes are either active or inactive, starting with initial active nodes. Newly activated nodes get one chance to activate inactive neighbors, with activation probability based on edge weight. The LT model activates inactive nodes when accumulated neighbor influence exceeds a threshold.
% Information propagation estimation involves approximating and reproducing the spread of information in a network and providing explanations based on propagation sources. This task has applications in event prediction~\citep{zhao2021event}, adverse event detection~\citep{wang2018multi}, and disease spread prediction~\citep{Tang2023Enhancing}. Models for this purpose fall into two main categories: infection models and influence models. Infection models, such as the Susceptible-Infected (SI) and Susceptible-Infected-Susceptible (SIS), manage transitions between susceptible and infected statuses in networks, offering different switching paths for these changes~\citep{kermack1927contribution,keeling2005networks}. Specifically, every infected node attempts to infect adjacent nodes with probability $\beta$ at each iteration. However, in the SIS model, infected nodes might revert to being susceptible with a certain probability $\lambda$. A more complex case is the Susceptible-Infected-Recovered (SIR) model, which additionally considers the recovered state.

% Independent Cascade (IC) and Linear Threshold (LT)~\citep{kempe2003maximizing} are two typical influence models examining how influence spreads in social networks or infrastructure networks. The IC model involves nodes that can either be active or inactive. The process begins with a set of initial active nodes. At each step, any newly activated node can activate its inactive neighbors with a single chance. The probability of activation is dependent on the weight of the edge between nodes. As for the LT model, each inactive node becomes active only if it receives enough influence (over a threshold) from its neighbors.

\section{Preliminaries}
\label{Pre}
\subsection{Problem Formulation}
Our research problem is formulated as follows. Given an undirected social network $\mathcal{G} = (V, E)$ where $V$ is the node set, $E$ is the edge set, and $Y = \{Y_1, \ldots, Y_{|V|}\}$ is an infection state of all nodes in $\mathcal{G}$, which describes that a subset of nodes in $\mathcal{G}$ have been infected. Each $Y_i \in \{1, 0\}$ denotes the infection state of node $v_i \in V$, where $Y_i = 1$ indicates that $v_i$ is infected and otherwise $Y_i = 0$ indicates it is uninfected.
We aim to find the original propagation source $\hat{X}$ from the propagated observation $Y$, so that the loss with the ground Truth source set $X^* \in \{1, 0\}^{|V|\times1}$ is minimized, i.e. $\hat{X}=argmin_X||X-X^*||_2^2$. To account for the uncertainty in source localization, we need to construct a probabilistic model $P(X|Y,\mathcal{G})$, which can be used to sample for the final prediction.


\subsection{Label Propagation based Source Identification}
\label{subsec:lpsi}
In realistic situations, the intractable propagation process does not have an explicit prior, and it is also challenging to value appropriate parameters for the pre-selected underlying propagation model. To address this, \citet{wang2017multiple} propose Label Propagation based Source Identification~(LPSI). Since LPSI investigates the same problem as our research, we use it as a baseline to compare performance. LPSI captures source centrality characteristics in the method design. The centrality of sources shows that nodes far from the source are less likely to be infected than those near it~\citep{shah2012rumor}, which can also be observed in the real-world data by our analysis in the Appendix~\ref{app:assumption}. Based on these ideas, they propose to perform label propagations on the observation state of the network. By setting $Y[Y=0]=-1$ and $~\mathcal{Z}^{t=0}\xleftarrow{}Y$, the iteration of label propagation and the convergence states are as follows:
\begin{equation}
    \mathcal{Z}^{t+1}_i=\alpha\sum_{j:j\in\mathcal{N}(I)}S_{ij}\mathcal{Z}_j^t+(1-\alpha)Y_i.
    \label{lpsi}
\end{equation}
$\mathcal{Z}$ finally converges to:$\mathcal{Z}^*=(1-\alpha)(I-\alpha S)^{-1}Y$,where $S=D^{-1/2}AD^{-1/2}$ is the normalized weight matrix of graph $\mathcal{G}$, $\alpha$ is the fraction of label information from neighbors, and $\mathcal{N}(i)$ stands for the neighbor set of the node $i$. After obtaining the converged label matrix $\mathcal{Z}^*$, one node can identified as a source when its final label is larger than its neighbors.
While node labels alone cannot fully capture complex structural information, this method still effectively identifies structural patterns related to source centrality~(Appendix~\ref{app:assumption}). In our work, we leverage the idea of LPSI to generate structural guidance for efficient source identification and details can be found in Section 4.



\section{SIDSL: the Proposed Method}
\subsection{Diffusion Model for Source Localization}
To capture the indeterminacy of the ill-posed localization problem, it is essential to build a probabilistic model that can also leverage the topological information in the graph structure. We consider using the generative diffusion model (DM) framework to tackle this requirement by modifying it as a source predictor, which classifies each node into two categories: source or non-source. The diffusion model aims to learn source distributions by gradually adding Gaussian noise in samples in the forward process and learning to reverse this process using denoising networks conditioned on observations and graph structure. 

The forward process of the diffusion model is to gradually corrupt the initial source labels~($X_0=X$) by adding Gaussian noise over $n$ timesteps, which can be formulated as:
\begin{equation}
    \begin{aligned}
        p(X_{1:n}|X_0,Y,\mathcal{G}) &= \prod_{t=1}^n p(X_t|X_{t-1},Y,\mathcal{G}),\\
        p(X_t|X_{t-1},Y,\mathcal{G}) &= \mathcal{N}(X_t; \sqrt{1-\beta_t}X_{t-1}, \beta_tI), 
    \end{aligned}
\end{equation}
where $\beta_t$ is the noise schedule, $X_0$ represents the initial source labels, $Y$ denotes the observations, $\mathcal{G}$ represents the graph structure, and $X_t$ denotes the noisy features at timestep $t$. At the end of forward process, $X_n$ becomes pure Gaussian noise. While the forward process only requires source labels, the reverse process leverages observation $Y$ and graph structure $\mathcal{G}$ as conditional inputs to guide the denoising.

In the reverse process, it gradually transforms pure Gaussian noise into the original source labels, generating new samples $\hat{X}\in[0,1]^{|V|\times1}$ representing the probability of each node being a source. The final source predictions are obtained by thresholding these probabilities. To enhance stability across different propagation patterns, we leverage graph structure-based source estimations as conditional priors. These priors $X_{est}=\mathcal{Z}^*$ are obtained through label propagation (Section \ref{subsec:lpsi}), which identifies potential sources by aggregating infection values through the graph's normalized weight matrix while capturing source centrality characteristics. The convergence result $\mathcal{Z}^*$ reflects each node's structural importance relative to the observed infection pattern.

The reverse process can be formulated as:
\begin{equation}
    \begin{aligned}
        q(X_{n-1:0}|X_n,Y,\mathcal{G}) &= \prod_{t=n}^1 q(X_{t-1}|X_t,Y,\mathcal{G}),\\
        q(X_{t-1}|X_t,Y,\mathcal{G}) &= \mathcal{N}(X_{t-1}; \mu_\theta(X_t,t,Y,\mathcal{G},X_{est}), \sigma_t^2I),
    \end{aligned}
\end{equation}
where $\mu_\theta$ is parameterized by a denoising network $f_\theta$ that predicts the mean of the Gaussian distribution and $\sigma_t$ is the predicted variance. The diffusion framework, whose iterative diffusion process enables fine-grained integration of prior knowledge at multiple scales through its progressive denoising steps, allows for elegant integration of prior knowledge. By integrating topology-aware priors $X_{est}$ as stable guidance across different propagation patterns, our diffusion framework combines structural knowledge with data-driven pattern learning, enhancing generalization to new propagation scenarios. 



\begin{figure}[t]
\centering
  \includegraphics[width=0.95\linewidth]{fig/framework.pdf} 
  \vspace{-1em}
  \caption{The framework of SIDSL.}
    \label{fig:framework}
    \vspace{-2em}
\end{figure}

\subsection{Structure-prior biased Denoising Process}
In most propagation scenarios, source nodes constitute a small minority of total graph nodes~\cite{cheng2024gin}. With limited training data, this severe class imbalance often leads models to degenerate towards predicting all nodes as non-sources, as the model overfits to the dominant non-source class. To address this, inspired by~\citet{han2022card}, we propose an structure-prior biased denoising scheme that set the prior of the denoising process to structure-based source estimations rather than random noise~(illustrated in Figure~\ref{fig:framework}). This design leverages two key insights: (1) structure-based estimations provide reliable initial source candidates by capturing inherent source structural properties like centrality, making them more informative than random noise, and (2) starting the denoising process from these estimations creates a natural bias in the trajectory space, encouraging the model to explore regions with higher likelihood of containing true sources. By incorporating these topology-aware priors into the initialization and denoising process, our approach effectively prevents model collapse while reducing the data requirements for learning accurate source patterns.

Specifically, we first modify the mean of the diffusion endpoint (or reverse starting point) as the graph structure-based source estimation $X_{est}=X_{est}(Y,\mathcal{G})$ instead of using standard Gaussian noise, i.e.:
\begin{equation}
    p(X_n|Y,\mathcal{G})=\mathcal{N}(X_{est}(Y,\mathcal{G}),I).
\end{equation}
According to the original notation in \citet{ho2020denoising}, the Markov transition should be modified as:
\begin{equation}
    p(X_t|X_{t-1},Y,\mathcal{G})=\mathcal{N}(\sqrt{1-\beta_t}X_{t-1}+(1-\sqrt{1-\beta_t})X_{est},\beta_t I),
\end{equation}
which derives the closed-form distribution with arbitrary $t$:
\begin{equation}
    p(X_t|X_0,Y,\mathcal{G}))=\mathcal{N}(\sqrt{\bar\alpha_t}X_{0}+(1-\sqrt{\bar\alpha_t})X_{est}(Y,\mathcal{G}),(1-\bar\alpha_t)I),
\end{equation}
where $\alpha_t:=1-\beta_t, \bar\alpha_t:=\prod_t\alpha_t$.

In the reverse denoising process, the reverse Markov denoiser $q(X_{t-1}|X_t,Y,\mathcal{G})$ recovers the original data. DM framework trains the parameterized denoiser to fit the ground truth forward process posterior:
\begin{equation}
\begin{aligned}
    p(X_{t-1}|X_t,X_0,Y,\mathcal{G})&=
    p(X_{t-1}|X_t,X_0,X_{est}(Y,\mathcal{G}))\\
    &=\mathcal{N}(\tilde{\mu}(X_t,X_0,X_{est}(Y,\mathcal{G})),\tilde{\beta_t}I),
\end{aligned}
\end{equation}
where
\begin{equation}
  \begin{aligned}
  \tilde{\mu}(X_t,X_0,X_{est}(Y,\mathcal{G})):=&\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}X_0+\frac{(1-\bar\alpha_{t-1})\sqrt{\alpha_t}}{1-\bar\alpha_t}X_t+ \\
  &(1+\frac{(\sqrt{\bar\alpha_t}-1)(\sqrt{\alpha_t}+\sqrt{\bar\alpha_{t-1}})}{1-\bar\alpha_t})X_{est}, \\
  \tilde\beta_t:=&\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t.
  \end{aligned}
\label{eq9}
\end{equation}
The denoising network $f_\theta$ is set to estimate the ground truth source $X$(i.e. $X_0$ in Equation~(\ref{eq9})), which we empirically find more effective. The denoise network outputs the estimated source vector $\tilde{X}_0:=f_\theta(X_t,X_{est},Y,\mathcal{G},t)$ to calculate the posterior for step-by-step denoising. The denoising network $f_\theta$ can be trained by the simple L2 loss function~\cite{ho2020denoising}:
\begin{equation}
\label{eq:train}
    L(\theta)=\mathrm{E}_{X_0\sim p(X_0|\cdot),t,\epsilon}||X_0-f_\theta(X_t,t,\cdot)||^2_2,
\end{equation}
where $\cdot$ represents the conditional inputs.
% The above-mentioned framework is illustrated in Figure~\ref{fig:framework}.





\begin{figure}[t]
\centering
  \includegraphics[width=0.6\linewidth]{fig/denoising.pdf} 
  \vspace{-1em}
  \caption{The architecture of the denoising network.}
    \label{fig:denoising}
    \vspace{-2em}
\end{figure}


\subsection{Propagtion-enhanced Conditional Denoiser}
\subsubsection{Denoising Network Architecture}

The architecture of our denoising network is shown in Figure~\ref{fig:denoising}. 

\textbf{Encoding the noisy input and soft labels.} The pre-estimated $X_{est}$ is forwarded through a multi-layer GNN to capture the hidden message with graph structural information. Subsequently, it is added to the noisy input $X_t$ and passed through a linear layer. The final input for the GNN encoder is $Z_{e}=\mathrm{Linear}(\mathrm{GNN}(X_{est})\oplus X_t)\oplus Emb(t),$ 
where for the denoising step t, we use the classical sinusoidal embedding~\citep{vaswani2017attention}. The $\oplus$ indicates element-wise sum.  
$Z_e$ is then passed through a GCN-based encoder and is smoothed through a softmax function $\sigma$ and layer normalization:
\begin{equation}
    Z_{d}=\mathrm{LN}(\sigma(\mathrm{GNN}(Z_e))).
\end{equation}
Softmax and layer normalization operations are then used to improve the network's representational capacity and convergence performance, resulting in better performance and faster training~\citep{huang2023normalization}.

\textbf{Conditioning.} Shown at the left part of the figure, a GCN-based module learns the encoding carrying the source prominence and centrality from the infection state input $Y$, which will be elaborated on in the next section.

\textbf{Decoder.} $Z_d$  and encoded condition $h_{out}$ are decoded through a GCN-based module, resulting in the estimation for the uncorrupted sample $X_0$ (i.e. $X$):
\begin{equation}
\tilde{X_0} = \mathrm{GNN}(Z_d, h_{out}).
\end{equation}
\subsubsection{GNN-parameterized Label Propagation(GNN-LP)}
Our conditioning module takes the observed infection states as input, which contain crucial coupling information between spreading dynamics and network topology. To effectively extract this information, we introduce the GNN-LP module. We first employ label propagation~\citep{wang2017multiple}, which first mark infected nodes as 1 and uninfected as -1 in the observation state $Y$, resulting in $Y^*$. 
The propagation follows the update rule: the label of a node in the next step is a combination of its original label and the sum of normalized labels from its neighbors. We can rewrite this iteration as:
\begin{equation}
    \mathcal{Z}^{t+1}_i=\hat\alpha Y_i^*+\sigma(\sum_{j:j\in\mathcal{N}(I)}\phi(\mathcal{Z}_j^t, S_{ij})),
\end{equation}
where we add non-linear transformations $h(\cdot)$ and $\sigma(\cdot)$ to enhance the expressiveness of the propagation process.
As propagation continues, each node's label value changes according to the graph structure. These changes reflect the node's importance (centrality) and spreading influence in the network. By observing these changes, we can understand the relationship between propagation patterns and network structure. We then enhance this structural diffusion with GNNs, which learn topology-specific message passing rules to capture how different local structures influence propagation patterns. We can notice that the structure of the above equation exactly matches the form of the general Graph Neural Network~(GNN)~\citep{gilmer2017neural}, and the parameterization can be achieved by using a residual block combined with a graph convolutional network(GCN,~\cite{kipf2016semi}):
\begin{equation}
    \begin{aligned}
        % Y[\text{uninfected}]=-1,\quad Y[\text{infected}]=1,\\
        g(h^{(l)})=\sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\cdot h^{(l)}\cdot w),\\
        h^{(0)}=Y^*U^T,\quad h^{(l+1)}=h^{(0)}+ g(h^{(l)}).
    \end{aligned}
\end{equation}
Among them, $U\in\mathbb{R}^{C\times1}$ is the linear transformation, $\sigma$ is the activation operator {PReLU}, $h^{(l)}$ stands for the output hidden state of the $l$-th layer of the GCN, $\tilde{A}=A+I$ is the adjacency matrix with self-loops, and $\tilde{D}$ is the degree matrix of $\tilde{A}$. The final layer's output $h^{(l_f)}$ is projected back to dimension 1 and multiplied by the graph's Laplacian matrix $L$, i.e. $h_{out}:=L\cdot h^{(l_f)}$, which highlights the prominent nodes with higher propagation propagated labels among their neighbors. 

Through this integration, the module transforms binary infection states $Y$ into continuous representations $h_{out}$ that simultaneously encode both the spreading process and network structure information, enabling effective learning of their complex interactions even with limited training samples.

\begin{table*}[t]
\small
\renewcommand{\arraystretch}{0.85} 
% \setlength{\tabcolsep}{3pt}
\centering
\caption{Performance evaluation without pretraining under the real-world propagation. Best and second best (F1, RE, PR) are highlighted with bold and underlines respectively.}
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\toprule
\multicolumn{1}{c|}{Datasets} & \multicolumn{3}{c|}{Digg} & \multicolumn{3}{c|}{Twitter} & \multicolumn{3}{c|}{Android} & \multicolumn{3}{c}{Christianity} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
Methods & F1 & RE & PR & F1 & RE & PR & F1 & RE & PR & F1 & RE & PR \\
\midrule
Netsleuth & 0.006 & 0.003 & 0.000 & 0.160 & 0.181 & 0.143 & 0.142 & 0.105 & 0.219 & 0.128 & 0.099 & 0.181 \\
LPSI & \underline{0.544} & 0.516 & \textbf{0.575} & 0.487 & 0.495 & 0.479 & 0.348 & 0.517 & 0.268 & 0.221 & 0.282 & 0.198 \\
GCNSI & 0.458 & 0.411 & 0.517 & 0.374 & 0.352 & 0.399 & 0.383 & 0.474 & 0.321 & 0.343 & 0.321 & 0.370 \\
TGASI & 0.472 & 0.406 & 0.564 & 0.362 & 0.327 & 0.405 & 0.388 & 0.462 & 0.335 & 0.377 & 0.339 & \underline{0.423} \\
SLVAE & 0.479 & 0.565 & 0.416 & 0.353 & 0.424 & 0.302 & \underline{0.467} & \underline{0.588} & 0.387 & \underline{0.458} & \underline{0.662} & 0.351 \\
DDMSL & 0.517 & \underline{0.592} & 0.459 & \underline{0.492} & \underline{0.504} & \underline{0.481} & 0.448 & 0.540 & \underline{0.432} & 0.417 & 0.481 & 0.368 \\
SIDSL(Ours) & \textbf{0.585} & \textbf{0.605} & \underline{0.566} & \textbf{0.546} & \textbf{0.516} & \textbf{0.580} & \textbf{0.522} & \textbf{0.702} & \textbf{0.439} & \textbf{0.519} & \textbf{0.747} & \textbf{0.436} \\
\midrule
$\Delta$ & +7.5\% & +2.2\% & -1.6\% & +11.0\% & +2.4\% & +20.6\% & +11.8\% & +48.3\% & +1.6\% & +13.3\% & +12.8\% & +3.1\% \\
\bottomrule
\end{tabular}
\label{tab:np_results}
\end{table*}

\begin{table*}[ht]
\small
\renewcommand{\arraystretch}{0.85} 
% \setlength{\tabcolsep}{5pt}
\centering
\caption{Few-shot learning performance evaluation under the real-world propagation with pretraining (P) and without pretraining (NP). Results show performance using simulation data(IC and LT) for pretraining, and using limited real-world data (10\%) for few-shot learning. The rule-based methods are omitted as they do not support pretraining. Best and second best are highlighted with bold and underline respectively.}
\begin{tabular}{c|c|ccc|ccc|ccc|ccc} 
\toprule 
\multicolumn{2}{c|}{Dataset} & \multicolumn{3}{c|}{Digg} & \multicolumn{3}{c|}{Twitter} & \multicolumn{3}{c|}{Android} & \multicolumn{3}{c}{Christianity} \\ 
\cmidrule(lr){1-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} 
Methods & Config & F1 & RE & PR & F1 & RE & PR & F1 & RE & PR & F1 & RE & PR \\ 
\midrule 
\multirow{2}{*}{GCNSI} & P & 0.382 & 0.398 & 0.367 & 0.291 & 0.251 & 0.346 & 0.194 & 0.145 & \underline{0.293} & 0.159 & 0.174 & \underline{0.146} \\ 
& NP & 0.162 & 0.201 & 0.136 & 0.003 & 0.015 & 0.002 & \underline{0.012} & 0.029 & 0.008 & \underline{0.176} & \underline{0.179} & 0.178 \\ 
\midrule 
\multirow{2}{*}{TGASI} & P & \underline{0.406} & 0.407 & \underline{0.405} & \underline{0.292} & 0.249 & \underline{0.353} & \underline{0.212} & \underline{0.184} & 0.251 & \underline{0.206} & 0.193 & \underline{0.221} \\ 
& NP & \underline{0.297} & \underline{0.303} & \underline{0.291} & 0.003 & 0.015 & 0.001 & 0.012 & \underline{0.062} & 0.006 & 0.036 & 0.038 & 0.035 \\ 
\midrule 
\multirow{2}{*}{SLVAE} & P & 0.315 & 0.441 & 0.246 & 0.195 & 0.325 & 0.139 & 0.036 & 0.132 & 0.021 & 0.171 & \underline{0.215} & 0.142 \\ 
& NP & 0.103 & 0.188 & 0.071 & 0.030 & 0.042 & 0.023 & 0.009 & 0.010 & 0.008 & 0.008 & 0.016 & 0.005 \\ 
\midrule 
\multirow{2}{*}{DDMSL} & P & 0.348 & \underline{0.504} & 0.266 & 0.250 & \underline{0.359} & 0.192 & 0.109 & 0.114 & 0.104 & 0.165 & 0.145 & 0.191 \\ 
& NP & 0.005 & 0.003 & 0.015 & \underline{0.101} & \underline{0.095} & \underline{0.108} & 0.010 & 0.015 & \underline{0.009} & 0.009 & 0.005 & 0.045 \\ 
\midrule 
\multirow{2}{*}{SIDSL(Ours)} & P & \textbf{0.571} & \textbf{0.623} & \textbf{0.527} & \textbf{0.518} & \textbf{0.503} & \textbf{0.547} & \textbf{0.547} & \textbf{0.567} & \textbf{0.529} & \textbf{0.483} & \textbf{1.000} & \textbf{0.319} \\ 
& NP & \textbf{0.384} & \textbf{0.409} & \textbf{0.362} & \textbf{0.120} & \textbf{0.118} & \textbf{0.143} & \textbf{0.018} & \textbf{0.253} & \textbf{0.015} & \textbf{0.432} & \textbf{0.757} & \textbf{0.309} \\ 
\midrule
\multirow{2}{*}{$\Delta$} & P & +41\% & +24\% & +30\% & +77\% & +40\% & +55\% & +158\% & +208\% & +81\% & +134\% & +365\% & +44\% \\
& NP & +29\% & +35\% & +24\% & +19\% & +24\% & +32\% & +50\% & +308\% & +67\% & +145\% & +323\% & +74\% \\
\bottomrule 
\end{tabular}
\label{tab:few_shot}
\vspace{-1em}
\end{table*}


\begin{algorithm}[t]
\caption{Pretraining on synthetic data and few-shot learning on real data.}
\label{alg:pretrain}
\begin{algorithmic}[1]
\Require Graph $\mathcal{G}=(V,E)$, synthetic data generators $\mathcal{M}s$ (e.g. SIS, IC), few-shot samples $\mathcal{D}_{real}$
\State // Phase 1: Learn from Diverse Propagation Patterns
\State Generate synthetic $(X^*, Y)$ pairs using $\mathcal{M}s$ on $\mathcal{G}$
\State Train diffusion model using Eq.\ref{eq:train} with $(X^*, Y, \mathcal{G})$

\State // Phase 2: Adapt to Real Scenarios
\For{each $(X^*, Y)$ in $\mathcal{D}_{real}$}
    \State Fine-tune model using Eq.\ref{eq:train} with $(X^*, Y, \mathcal{G})$
\EndFor

\State // Inference on Real New Cases
\State \textbf{Input}: Observation $Y_{new}$ on $\mathcal{G}$
\State Compute structure-based prior $X_{est}$ from $Y_{new}$ using Eq.\ref{lpsi}
\State Sample initial noise $X_n \sim \mathcal{N}(0,I)$
\State \Return $\hat{X} \leftarrow$ denoise($X_n$) with $(X_{est}, Y_{new}, \mathcal{G})$

\end{algorithmic}
\vspace{-0.4em}
\end{algorithm}

\subsection{Pretrain using Simulation data from Established Models}
\label{sec:method_pretrain}
The proposed structure-prior informed diffusion framework effectively extracts topology-aware features that remain largely stable across different propagation dynamics. These designs enable the framework to learn diverse knowledge from different propagation patterns while maintaining stable, pattern-invariant feature extraction capabilities centered on network structure. With this ability to capture generalizable source-topology relationships, we can effectively utilize synthetic data from established models (e.g. IC, LT, SIS) for pretraining, followed by efficient adaptation to real-world scenarios through few-shot learning, thus providing a practical solution for source localization in settings where real-world propagation training data is limited. We provide the illustration of the above process in Algorithm~\ref{alg:pretrain}, and in Section~\ref{sec:exp_pre}, we evaluate SIDSL's ability to bridge simulation and reality by comparing two approaches: one using pretrained models with few-shot learning, and another trained exclusively on real-world data.



\section{Experiments}
\subsection{Experiment Settings}
The basic settings of our experiments are shown below.

\textbf{Datasets.} Following~\citet{ling2022source} and ~\citet{huang2023two}, we use four real-world propagation data to evaluate SIDSL. The real-world datasets include \textit{Digg}, \textit{Twitter}, \textit{Android} and \textit{Christianity}, in which the real propagation cascades are available. For each cascade in all sets, we designate the infected nodes at the first 10\% of the propagation time as source nodes and take the network's infection status at 30\% of the propagation time as observation input. In the context of real-world applications, we often can only collect sufficient data for analysis after some time has elapsed since the occurrence of the event. Therefore, attempting to predict what initially happened in the process when we have observed enough propagation patterns at a certain degree of infection time is very much in line with the needs of real-world operations. Please refer to the Appendix~\ref{app:data} for specific details of the datasets.

\textbf{Implementation Details.} For each dataset, the ratio of training, validation, and testing portion is 6:1:1. For the diffusion framework of SIDSL, we use $T=500$ maximum diffusion timestep and linear schedule for noise scheduling. In the denoising network, we leverage a 2-layer graph convolutional network~(GCN) to forward the LPSI estimation $X_{est}$ condition. The GNN encoder and decoder comprise 3-layer GCNs with a hidden dimension of 128. The residual GNN of the conditioning module is a 2-layer GCN, with a hidden dimension of 8. The learning rate is searched from {0.01, 0.005, 0.001}, and the maximum number of training epochs is set to 500 for all datasets. We train our model using Adam optimizer and a learning rate scheduler with a linear decay. Our model is trained on a single NVIDIA GeForce RTX 2080 Ti. The code implementation can be found at https://anonymous.4open.science/r/ASLDiff-4FE0.

\textbf{Baselines.} Following previous works~\citep{ling2022source, yan2024diffusion}, we selected two representative heuristic methods, \textit{i.e.}, Netsleuth~\citep{prakash2012spotting} and LPSI~\citep{wang2017multiple}, and deep learning methods, \textit{i.e.}, GCNSI~\citep{dong2019multiple}, TGASI~\citep{hou2023sequential} and recent generative deep learning methods SLVAE~\citep{ling2022source}, DDMSL~\citep{yan2024diffusion}. These baselines are all state-of-the-art (SOTA) multi-source localization methods in their domains. Please refer to the Appendix~\ref{app:baseline} for specific information of baselines and our method.

\textbf{Metrics.} Following previous works~\citep{wang2022invertible}, we adopt three metrics: 1) F1-score~(F1): The harmonic mean of recall and precision, emphasizing the balance between precision and recall; 2) Recall~(RE): The proportion of positive cases~(source nodes) that are correctly identified, focusing on the model's ability to detect all relevant instances; 3) Precision~(PR): The proportion of actual positive cases among the samples judged as positive, highlighting the model's ability to avoid false positives. We do not use accuracy since in highly imbalanced datasets with rare positive cases, accuracy is a misleading metric as it can achieve deceptively high values by simply predicting the majority (negative) class, thus failing to capture the model's true discriminative capability for the minority class (source) of interest.



\subsection{Overall Performance on Real-world Datasets}

To evaluate the real-world performance of our proposed method against baselines, we conduct direct training and testing on four real-world datasets for all methods. 
The experimental results are presented in Table \ref{tab:np_results}. Our proposed SIDSL outperforms all baselines across nearly all metrics on all datasets. Specifically, SIDSL achieves F1 scores that exceed the second-best baseline by 7.5\%, 11.0\%, 11.8\%, and 13.3\% on Digg, Twitter, Android, Christianity, respectively, which demonstrates SIDSL's superior ability to predict source nodes despite their sparsity. 

Further, we have the following findings. \textbf{First}, compared to other generative methods like DDMSL and SLVAE which rely purely on data-driven approaches, SIDSL demonstrates superior performance, validating the effectiveness of incorporating structural prior information on top of purely data-driven distribution learning.
\textbf{Second}, compared to rule-based and pure learning-based methods, generative methods (SIDSL, DDMSL and SLVAE) show relatively strong performance, highlighting the advantages of effective source distribution modeling for source localization accuracy. LPSI and other deep learning-based methods perform comparably well, as they successfully capture structural characteristics of source nodes through either rules or learning. Netsleuth demonstrates the weakest performance, likely due to its design for SI propagation patterns~\cite{keeling2005networks,prakash2012spotting}, which limits its generalization to more complex patterns and larger network scales in real-world data.
\textbf{Finally}, comparing the performance across different datasets, the most substantial improvement of SIDSL is observed on the Christianity dataset~(13.3\% in F1), likely because its relatively smaller network scale (in terms of nodes and edges) makes topological features more easily capturable by all methods, thereby amplifying the benefits of structural prior knowledge injection in SIDSL. 
% \textbf{Finally}, under the 10\% low-sample training condition, all learning-based methods experience significant performance degradation. Nevertheless, our method maintains the best performance among learning-based approaches, with F1 scores surpassing other methods by 94.9\%, 18.8\%, 50\%, and 180.5\% across the four datasets. However, the performance gap compared to the training-free rule-based method LPSI indicates a limitation of learning-based methods in real-world scenarios. To address this limitation, we propose a synthetic data pretraining strategy in Section \ref{sec:method_pretrain}, with performance comparisons and evaluations presented in the following subsection.

\begin{table*}[ht]
\small
\renewcommand{\arraystretch}{0.9} 
\setlength{\tabcolsep}{5pt}
\centering
\caption{Zero-shot performance evaluation with pretraining under the real-world propagation. Results show performance without finetuning when using simulation data(IC and LT) for pretraining. The rule-based methods are omitted as they do not support pretraining. Best and second best are highlighted with bold and underline respectively.}
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\toprule
\multicolumn{1}{c}{Dataset} & \multicolumn{3}{|c}{Digg} & \multicolumn{3}{|c}{Twitter} & \multicolumn{3}{|c}{Android} & \multicolumn{3}{|c}{Christianity} \\
\cmidrule(lr){1-1} \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
Methods & F1 & RE & PR & F1 & RE & PR & F1 & RE & PR & F1 & RE & PR \\
\midrule
GCNSI & 0.195 & 0.273 & 0.152 & \underline{0.203} & 0.162 & \underline{0.271} & 0.103 & 0.079 & \underline{0.148} & 0.030 & 0.060 & 0.020 \\
TGASI & 0.189 & 0.281 & 0.142 & 0.199 & 0.172 & 0.236 & \underline{0.109} & \underline{0.107} & 0.110 & 0.059 & 0.067 & 0.052 \\
SLVAE & 0.278 & 0.236 & \underline{0.337} & 0.172 & 0.205 & 0.148 & 0.004 & 0.025 & 0.002 & 0.080 & \underline{0.140} & 0.056 \\
DDMSL & \underline{0.291} & \underline{0.339} & 0.264 & 0.203 & \underline{0.272} & 0.162 & 0.019 & 0.013 & 0.038 & \underline{0.133} & 0.127 & \underline{0.147} \\
SIDSL(Ours) & \textbf{0.539} & \textbf{0.629} & \textbf{0.472} & \textbf{0.509} & \textbf{0.569} & \textbf{0.460} & \textbf{0.506} & \textbf{0.914} & \textbf{0.355} & \textbf{0.465} & \textbf{1.000} & \textbf{0.305} \\
\midrule
$\Delta$ & +85\% & +86\% & +40\% & +151\% & +109\% & +70\% & +364\% & +754\% & +140\% & +250\% & +614\% & +108\% \\
\bottomrule
\end{tabular}
\label{tab:zero_shot}
\vspace{-1em}
\end{table*}






\subsection{Performance in Low Data Regime}
\label{sec:exp_pre}

To assess how deep learning methods leverage simulation data pretraining for few-shot and zero-shot learning in real-world source localization, we generate pretraining data using standard IC and LT propagation models (1:1 ratio) across four networks. These models effectively capture social network dynamics by representing real-world interaction randomness, cumulative influence, and topology-based spread patterns. For few-shot learning~(P), we fine-tune the pretrained models with limited real data (10\%). We also compare the performance of all the methods trained on equivalent data volumes but without pretraining~(NP). For zero-shot learning, all the methods are directly tested after being pretrained. 

\textbf{Few-shot analysis.} The result of few-shot learning is shown in Tabel~\ref{tab:few_shot}. Rule-based methods (Netsleuth and LPSI) are not shown here because they are training-free. SIDSL demonstrates superior performance compared to all baseline methods across all datasets, both with and without pretraining, achieving improvements of 24\%$\sim$365\% across all metrics. 
% Further, we have the following two findings. \textbf{First}, from the improvement percentages, SIDSL's performance advantage over the best baseline is generally larger with pretraining than without pretraining. In terms of F1 score which reflects overall accuracy, the improvement margin with pretraining increased by 12\%$\sim$108\% across most datasets, with Christianity being the only exception showing an 11\% decrease. This pattern suggests that pretraining enhances SIDSL's ability to effectively learn from limited data, demonstrating the model's superior capacity to leverage knowledge from simulated data to improve real-world performance. 
% \textbf{Second}, among baseline methods, generative approaches (SLVAE, DDMSL) show limited ability to leverage pretraining. While TGASI achieves consistent improvements with pretraining across datasets (e.g., F1 increases from 0.297 to 0.406 on Digg), SLVAE and DDMSL show less stable gains (e.g., SLVAE's F1 only increases from 0.009 to 0.036 on Android). This suggests their sensitivity to distribution shifts between simulation and real-world data. In contrast, our method demonstrates robust transfer learning capability with significant improvements from pretraining across all datasets, thanks to our effective incorporation of pattern-invariant structural prior information.
Further, we have the following two findings. \textbf{First}, SIDSL demonstrates enhanced performance advantages over baselines when leveraging pretraining, as evidenced by the improvement percentages across datasets. Among the improvements ($\Delta$) across different datasets and metrics, 8 out of 12 cases show larger gains with P compared to NP. This consistent pattern of larger improvements with pretraining demonstrates SIDSL's superior ability to effectively leverage information from simulated data to real-world scenarios in limited-data settings.
\textbf{Second}, the baseline methods exhibit varying capabilities in utilizing pretraining. While the non-generative method TGASI shows consistent improvements with pretraining across datasets, generative approaches like SLVAE and DDMSL demonstrate limited and unstable gains (e.g., SLVAE's F1 only increases from 0.009 to 0.036 while DDMSL's F1 only increases from 0.010 to 0.109 on Android). This contrast highlights the generative models' sensitivity to distribution shifts between simulated and real-world data, whereas our method achieves robust and significant improvements through effective incorporation of pattern-invariant structural prior information.

\textbf{Zero-shot analysis.} The results of zero-shot learning are shown in Table~\ref{tab:zero_shot}. SIDSL significantly outperforms all baseline methods across all datasets, achieving improvements of 85\%\~364\% in F1 scores. The performance gap is particularly notable on Android and Christianity datasets, where SIDSL achieves over 0.9 recall while maintaining reasonable precision. In contrast, baseline methods show limited zero-shot transfer capability with F1 scores mostly below 0.3. Compared to rule-based methods in Table~\ref{tab:np_results}, zero-shot SIDSL still outperforms them in most metrics. These findings demonstrates SIDSL's superior ability to leverage knowledge from simulation data without fine-tuning.


\begin{figure}[t]
\centering
  \includegraphics[width=0.48\linewidth]{fig/digg_pattern.pdf} 
  \includegraphics[width=0.48\linewidth]{fig/twitter_pattern.pdf} 
  \includegraphics[width=0.48\linewidth]{fig/android_pattern.pdf} 
  \includegraphics[width=0.48\linewidth]{fig/christ_pattern.pdf} 
  
  \caption{Few-shot performance of SIDSL with different pretraining dataset compositions~(IC+LT, IC, LT), followed by finetuning on real-world propagation data. "NP" denotes "non-pretrained".}
\label{fig:pattern}
\vspace{-2em}
\end{figure}





\subsection{Analysis of Pretraining on Simulation Data}
To investigate how pretraining on simulation data affects model's few-shot/zero-shot performance under different data volume, we evaluate SIDSL's F1 scores across pretraining dataset with different propagation model combinations (IC, LT, and IC+LT with ratio 1:1) and compare it with non-pretrained~(NP) SIDSL. We then test them under different ratios of the finetune dataset of real-world propagation data. The results of four datasets are shown in Figure~\ref{fig:pattern}.

\textbf{Finetune data volume analysis.}  Across all four datasets, models with pretraining consistently outperformed non-pretrained (NP) models in low-data scenarios. Under IC+LT combination, the pretrained models achieved optimal F1 scores on Digg and Twitter using just 5\% of training data, while NP models required 50\%. Similarly, on Android and Christianity datasets, pretrained models reached near-peak performance (within 0.033 and 0.018 F1) with only 20\% of the data, compared to NP models which needed 50\%. These findings suggest that aligning pretraining patterns with specific propagation characteristics benefits model performance.

\textbf{Pattern analysis.} On the Digg dataset, SIDSL pretrained with IC+LT and IC configurations outperform that using LT, indicating that IC patterns better align with Digg's propagation characteristics, as also shown in~\cite{zhang2024information}. For Android and Christianity Q\&A datasets, IC+LT and LT configurations performed better in few-shot scenarios. This is likely because user participation in these platforms' information cascades (e.g., comments) is driven by cumulative influence from multiple sources (LT) rather than single independent triggers (IC). 
On Twitter, IC+LT configuration performs the best, demonstrating that pattern diversity enhances pretraining effectiveness on this dataset.


% \begin{table}
% \footnotesize
% \setlength{\tabcolsep}{4pt}
% \centering
% \caption{Ablation study.}
% \begin{tabular}{llcccccc}
% \toprule
% Dataset & & \multicolumn{3}{c}{\text{Digg}} & \multicolumn{3}{c}{\text{Christianity}} \\
% \cmidrule(lr){1-8}
% \text{Data} & \text{Ablation} & \text{F1} & \text{RE} & \text{PR} & \text{F1} & \text{RE} & \text{PR} \\
% \cmidrule(lr){1-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8}
% \multirow{3}{*}{100\%} & \text{SIDSL} & \textbf{0.585} & \textbf{0.605} & \textbf{0.566} & \textbf{0.530} & \textbf{0.961} & 0.373 \\
% & \text{SIDSL w/o SBD} & 0.384 & 0.321 & 0.477 & 0.428 & 0.487 & \textbf{0.382} \\
% & \text{SIDSL w/o GNN-LP} & 0.543 & 0.549 & 0.538 & 0.509 & 0.901 & 0.355 \\
% \midrule
% \multirow{3}{*}{10\%} & \text{SIDSL} & \textbf{0.384} & \textbf{0.409} & \textbf{0.362} & \textbf{0.432} & \textbf{0.757} & \textbf{0.309} \\
% & \text{SIDSL w/o SBD} & 0.208 & 0.188 & 0.232 & 0.279 & 0.344 & 0.235 \\
% & \text{SIDSL w/o GNN-LP} & 0.320 & 0.342 & 0.301 & 0.341 & 0.501 & 0.258 \\
% \bottomrule
% \end{tabular}
% \label{tab:ablation}
% \end{table}
\begin{table}
\footnotesize
\setlength{\tabcolsep}{4pt}
\centering
\caption{Ablation study. The relative performance change of each ablation against SIDSL is reported.}
\vspace{-1.5em}
\begin{tabular}{llcccccc}
\toprule
Dataset & & \multicolumn{3}{c}{Digg} & \multicolumn{3}{c}{Christianity} \\
\cmidrule(lr){1-8}
Data & Ablation & F1 & RE & PR & F1 & RE & PR \\
\cmidrule(lr){1-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8}
\multirow{3}{*}{100\%} & SIDSL & 100\% & 100\% & 100\% & 100\% & 100\% & 100\% \\
& SIDSL w/o SBD & -34\% & -47\% & -16\% & -19\% & -49\% & +2\% \\
& SIDSL w/o GNN-LP & -7\% & -9\% & -5\% & -4\% & -6\% & -5\% \\
\midrule
\multirow{3}{*}{10\%} & SIDSL & 100\% & 100\% & 100\% & 100\% & 100\% & 100\% \\
& SIDSL w/o SBD & -46\% & -54\% & -36\% & -35\% & -55\% & -24\% \\
& SIDSL w/o GNN-LP & -17\% & -16\% & -17\% & -21\% & -34\% & -17\% \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\vspace{-1.5em}
\end{table}


\subsection{Ablation Study}
We conduct ablation studies to investigate the significance of key components in SIDSL. For the first ablation model, SIDSL w/o SBD (structure-prior biased diffusion), instead of computing estimation through Equation~\ref{lpsi}, we directly substitute it with the node having the highest closeness centrality in the infection subgraph. For the second ablation, SIDSL w/o GNN-LP, we replace the observation state encoding with a standalone GNN while maintaining an equivalent parameter count. 

Table~\ref{tab:ablation} presents the ablation results on Digg and Christianity datasets under both full (100\%) and limited (10\%) training data conditions. Removing either component leads to decreased overall performance in both 100\% and 10\% data conditions. There are two more findings:
\textbf{First}, removing SBD leads to more severe performance degradation compared to removing GNN-LP, with the impact most pronounced in recall metrics. The substantial decline in recall (TP/(TP+FN)) indicates a reduction in true positive predictions (as TP+FN remains constant for the same test set). Notably, the decline in recall exceeds that of precision (TP/(TP+FP)), which also implies a decrease in positive predictions~(TP+FP). This pattern suggests that removing SBD causes the model to predict fewer positive samples overall, demonstrating SBD's role in maintaining the model's predictive balance.
\textbf{Second}, the impact of removing GNN-LP is more pronounced under limited data conditions, with F1 scores dropping by -17\% to -21\% at 10\% data versus -4\% to -7\% at full data. This demonstrates GNN-LP's particular effectiveness in low-resource scenarios.

% This occurs because bias guidance is weakened in the diffusion model. Combined with class imbalance, this ablation causes the model to favor negative predictions, resulting in substantially reduced RE. 
% Additionally, under few-shot conditions (10\%), the performance of w/o GNN-LP also deteriorates, highlighting the adaptability of the original design in few-shot learning scenarios.


\begin{figure}[t]
\centering
  \includegraphics[width=0.99\linewidth]{fig/time_performance.pdf} 
  \vspace{-1em}
  \caption{Training and inference time~(per sample) comparison on Digg dataset. "NP" denotes the "non-pretrained" version. "P" denotes using the scheme of "pretraining+finetuning", and the overall training time is reported.}
    \label{fig:time}
    \vspace{-1.5em}
\end{figure}

% hou2024new，wang2024joint
\subsection{Time Cost Analysis}
We present a detailed comparison of the computational cost of our proposed model against baselines in the Digg dataset, which has one of the largest networks for better comparison. Results are shown in Figure \ref{fig:time}. While our model's original training duration is the second highest, our model requires less time to pretrain on the synthetic data and finetune than initially training on the real data~(-25\%), which makes the training time the second lowest. 

In inference time, our model is the second highest because of the iterative nature of the DDPM-based denoising process. We opt for DDPM as our foundation due to its classical design and proven effectiveness. It's worth highlighting that SIDSL's architecture is fully compatible with more computationally efficient diffusion variants, such as DDIM~\cite{song2020denoising}, which could substantially reduce the current computational overhead. This flexibility, combined with our model's transfer capabilities, makes SIDSL particularly resource-efficient in practical deployments. 

\begin{figure}[t]
\centering
  \includegraphics[width=0.99\linewidth]{fig/simulation_f1_iclt.pdf} 
  \vspace{-1em}
  \caption{Performance under IC and LT propagation patterns.}
    \label{fig:iclt_simulation}
\vspace{-2em}
\end{figure}


\subsection{Results on Synthetic Propagation Patterns}
It is a common practice in previous works~\cite{ling2022source,yan2024diffusion,wang2022invertible} to evaluate methods' performance on synthetic data with established propagation patterns. Following \cite{ling2022source,wang2022invertible}, we conducted experiments on two commonly evaluated networks: \textit{Net Science~(Net)} and \textit{Power Grid(Power)}. Using both IC and LT models, we simulated 100 steps until convergence to generate two sets of synthetic data. Strong baseline models were trained and tested on these datasets, with results shown in Figure~\ref{fig:iclt_simulation}. Our method outperforms the baselines, achieving F1 score improvements of 7.6\% and 4.7\% under the IC model, and 2.7\% and 1.7\% under the LT model on the two datasets respectively. These results demonstrate our method's ability to accurately identify sources with established propagation patterns.


\section{Conclusion}
In this paper, we presented SIDSL, a structure-prior informed diffusion framework for robust source localization in graphs with limited real-world propagation data. By incorporating topology-aware priors through graph label propagation, a GNN-parameterized label propagation module, and a structure-prior biased denoising process, our framework effectively addresses three key challenges: unknown propagation patterns, complex topology-propagation relationships, and class imbalance issues. Extensive experiments on four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3\% improvements in F1 scores over state-of-the-art methods. More importantly, through effective pretraining with synthetic data, SIDSL maintains robust performance with only 10\% training data, showing remarkable advantages in both few-shot and zero-shot learning scenarios. These results validate our framework's strong generalization capability and practical applicability in real-world source localization tasks where labeled data is scarce. Future directions include improving model efficiency through hierarchical decomposition approaches to handle larger-scale networks while maintaining localization accuracy.
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\appendix

% \begin{table*}[t]
% \centering
% \caption{Comparison of different source localization methods. \textbf{Ind.}: Indeterminacy. \textbf{Obs. input}: Observation input. \textbf{CN}: Cross-network inference on target network~(trained on another network). \textbf{PI}: Prior-knowledge informed}
% \label{tab:comp}
% \begin{tabular}{@{}lcccccc@{}}
% \toprule
% \textbf{Category} & \textbf{Method} &\textbf{ Ind.} & \textbf{Applicable patterns} & \textbf{Obs. input} & \textbf{CN} & \textbf{PI} \\ \midrule
% Rule-based & NetSleuth(\citep{prakash2012spotting}) & \ding{55} & SI & single snapshot & - & - \\
%  & OJC(\citep{zhu2017catch}) & \ding{55} & SI, SIR, IC & single snapshot & - & - \\
%  & LPSI(\citep{wang2017multiple}) & \ding{55} & SI, SIR, IC & single snapshot & - & - \\
% Data-driven & GCNSI(\citep{dong2019multiple}) & \ding{55} & SI, SIR, IC & single snapshot & \checkmark & \checkmark \\
%  & IVGD(\citep{wang2022invertible}) & \ding{55} & IC & single snapshot & \ding{55} & \ding{55} \\
%  & SLVAE(\citep{ling2022source}) & \checkmark & SI, SIR, real-world & single snapshot & \ding{55} & \ding{55} \\
%  & SLDiff(\citep{huang2023two}) & \ding{55} & real-world & multiple snapshot & \ding{55} & \ding{55} \\
%  & TGASI(\citep{hou2023sequential}) & \ding{55} & SI, SIR, IC & multiple snapshot & \ding{55} & \ding{55} \\
%  & DDMSL(\citep{yan2024diffusion}) & \checkmark & SI, SIR, real-world & single snapshot & \ding{55} & \checkmark \\
%  & PGSL(\citep{xu2024pgsl}) & \checkmark & SI, SIR, real-world & single snapshot & \ding{55} & \ding{55}  \\
%  & GINSD(\citep{cheng2024gin}) & \ding{55} & IC & single snapshot & \ding{55} & \ding{55} \\
% &SIDSL(Ours)  & \checkmark & SI(S/R), IC, real-world & single snapshot & \checkmark & \checkmark \\ \bottomrule
% \end{tabular}

% \label{table:comparison}
% \end{table*}

% \begin{table*}[t]

% \centering

% \caption{Comparison of different source localization methods. \textbf{Ind.}: Indeterminacy. \textbf{Obs. input}: Observation input. \textbf{CN}: Cross-network inference on target network~(trained on another network). \textbf{PI}: Prior-knowledge informed}

% \label{tab:comp}

% \begin{tabular}{@{}lcccccc@{}}

% \toprule

% \textbf{Category} & \textbf{Method} &\textbf{ Ind.} & \textbf{Applicable patterns} & \textbf{Obs. input} & \textbf{CN} & \textbf{PI} \\ \midrule

% Rule-based & NetSleuth(\citep{prakash2012spotting}) & \ding{55} & SI & single snapshot & - & - \\

%  & OJC(\citep{zhu2017catch}) & \ding{55} & SI, SIR & single snapshot & - & - \\

%  & LPSI(\citep{wang2017multiple}) & \ding{55} & any patterns & single snapshot & - & - \\

% Data-driven & GCNSI(\citep{dong2019multiple}) & \ding{55} & any patterns & single snapshot & \checkmark & \checkmark \\

%  & IVGD(\citep{wang2022invertible}) & \ding{55} & IC & single snapshot & \ding{55} & \ding{55} \\

%  & SLVAE(\citep{ling2022source}) & \checkmark & any patterns & single snapshot & \ding{55} & \ding{55} \\

%  & SLDiff(\citep{huang2023two}) & \ding{55} & any patterns & multiple snapshot & \ding{55} & \ding{55} \\

%  & TGASI(\citep{hou2023sequential}) & \ding{55} & any patterns & multiple snapshot & \ding{55} & \ding{55} \\

%  & DDMSL(\citep{yan2024diffusion}) & \checkmark & any patterns & single snapshot & \ding{55} & \checkmark \\

%  & PGSL(\citep{xu2024pgsl}) & \checkmark & any patterns & single snapshot & \ding{55} & \ding{55}  \\

%  & GINSD(\citep{cheng2024gin}) & \ding{55} & any patterns & single snapshot & \ding{55} & \ding{55} \\

% &SIDSL(Ours)  & \checkmark & any patterns & single snapshot & \checkmark & \checkmark \\ \bottomrule

% \end{tabular}

% \label{table:comparison}

% \end{table*}
\begin{table*}[t]
\centering
\caption{Comparison of different source localization methods. \textbf{Ind.}: Indeterminacy. \textbf{Obs. input}: Observation input. \textbf{PI}: Prior-knowledge informed}
\label{tab:comp}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Category} & \textbf{Method} &\textbf{ Ind.} & \textbf{Applicable patterns} & \textbf{Obs. input} & \textbf{PI} \\ \midrule
Rule-based & NetSleuth(\citep{prakash2012spotting}) & \ding{55} & SI & single snapshot & - \\
 & OJC(\citep{zhu2017catch}) & \ding{55} & SI, SIR & single snapshot & - \\
 & LPSI(\citep{wang2017multiple}) & \ding{55} & any patterns & single snapshot & - \\
Data-driven & GCNSI(\citep{dong2019multiple}) & \ding{55} & any patterns & single snapshot & \checkmark \\
 & IVGD(\citep{wang2022invertible}) & \ding{55} & IC & single snapshot & \ding{55} \\
 & SLVAE(\citep{ling2022source}) & \checkmark & any patterns & single snapshot & \ding{55} \\
 & SLDiff(\citep{huang2023two}) & \ding{55} & any patterns & multiple snapshot & \ding{55} \\
 & TGASI(\citep{hou2023sequential}) & \ding{55} & any patterns & multiple snapshot & \ding{55} \\
 & DDMSL(\citep{yan2024diffusion}) & \checkmark & any patterns & multiple snapshot & \checkmark \\
 & PGSL(\citep{xu2024pgsl}) & \checkmark & any patterns & single snapshot & \ding{55} \\
 & GINSD(\citep{cheng2024gin}) & \ding{55} & any patterns & single snapshot & \ding{55} \\
 & NFSL(\citep{hou2024new}) & \ding{55} & any patterns & single snapshot & \checkmark \\
&SIDSL(Ours)  & \checkmark & any patterns & single snapshot & \checkmark \\ \bottomrule
\end{tabular}
\label{table:comparison}
\end{table*}

\section{Comparison of multiple source localization methods}\label{app:comp}
In Table~\ref{table:comparison}, we compare the functionality, requirements, and application scenarios of mainstream source localization methods. “\textbf{Ind.}" refers to whether the method considers modeling the indeterminacy of source locations. "\textbf{Applicable patterns}" refers to the specific propagation pattern to which the method can be applied. "\textbf{Obs. input}" refers to the required input for the method to detect the sources. 
% "\textbf{CN}" refers to whether the data-driven method, after being trained on one network, can effectively generalize its inference capabilities to different networks. 
"\textbf{PI}" refers to whether the data-driven method is informed by prior knowledge.

From the demonstration, we can observe that our method is designed to be the most functional and capable of handling a broader range of real-world applications. Our method also requires less input data, which is more practical. The method proposed holds significant practical value and addresses the limitations of the existing methods.
Additionally, as another method based on the diffusion model, DDMSL and TGASI require the propagation process data during training and the acquisition or calculation of parameters for the infectious model before source localization. This limitation restricts the model's practical application value. Also, PGSL resembles SLVAE's framework and merely utilizes a flow-based model to replace the VAE in SLVAE, while our diffusion model exhibits stronger distribution modeling capabilities. GINSD considers incomplete user data scenarios and utilizes a positional embedding module to distinguish incomplete nodes in the source inference process, and as we do not consider such circumstances, GINSD reduces to a simple GAT-based baseline similar to GCNSI.

It should also be noted that two recent works~\citep{wang2024joint,ling2024source} focus on source localization in a cross-platform setting, which is orthogonal to our research problem and thus not discussed. 

\section{Analyzing source centrality in empirical data}
\label{app:assumption}
\begin{figure}[h]
    \centering
    % \hspace{-0.7cm}
    % \subfloat[]{\includegraphics[width=0.33\textwidth]{fig/cc_Digg_new_2.png}\label{fig:ccsub1}}
    % % \hfill
    % \subfloat[]{\includegraphics[width=0.33\textwidth]{fig/cc_Digg_new_3.png}\label{fig:ccsub2}}
    % % \hfill
    % \subfloat[]{\includegraphics[width=0.33\textwidth]{fig/cc.png}\label{fig:ccsub3}}
    % \hspace{-0.5cm}
    \includegraphics[width=\linewidth]{fig/centrality_analysis.pdf}
    \caption{(a)(b) Normalized closeness centrality of infected users and sources for all cascades in \textit{Digg}. The blue histogram shows the normalized closeness centrality distribution of infected nodes, while the red one shows that of source nodes. (a) is the density distribution of closeness centrality. The dashed line indicates the mean centrality for each node type. (b) is the frequency distribution of closeness centrality. The orange box highlights the part where centrality is above 0.8. (c) The closeness centrality(CC) probability density function of the predicted and ground truth source nodes on the \textit{Digg} dataset. The blue histogram shows the normalized closeness centrality distribution of the ground truth sources, while the red one shows that of the predicted ones.}
% \vspace{-0.5cm}
    
    \label{fig:cc}
\end{figure}

We believe that the source centrality assumption is not only common in most existing propagation patterns and real-world scenarios, as evidenced by the literature~\citep{ali2020revisit,dong2019multiple}, but also validated by the competitive performance on real-world datasets of baselines like LPSI and GCNSI, which are devised based on similar assumptions. We show the analytical results demonstrating the effectiveness of the assumption in the following.

In our analysis of the real-world dataset Digg, we evaluate the normalized(max-min) closeness centrality density and frequency of the source nodes in the subgraph consisting of infected nodes to partially reflect the centrality characteristic of the sources. The closeness centrality~(CC) specifically reflects the node topological distance to all other nodes in the subgraph, rather than its degree attribute. The result is shown in Figure~\ref{fig:cc}(a). From Figure~\ref{fig:cc}(a), the mean normalized closeness centrality of sources is higher than the average of all infected nodes, and source nodes cover over 63\% of the nodes with the centrality score exceeding 0.8, as shown in Figure~\ref{fig:cc}(b). The overall results demonstrate the crucial role of source nodes in the information diffusion process and their higher likelihood of being central to the network structure within the cascades. 

Our proposed method, rather than strictly adhering to the centrality in outputting prediction results, exhibits stronger expressive capabilities. Intuitively, on homogeneous networks—where the probability of propagation along network edges is the same and fixed—the assumption can be strictly applied to locate the source of propagation. Such a propagation pattern that strictly obeys the centrality assumption is an indispensable subset that can be covered by the propagation patterns our model can characterize. As the proposed model leverages a simulated label propagation conditional module based on the centrality assumption but employs a graph neural network to learn the influence of the network's heterogeneous topology from the data, other circumstances can also be modeled when learning from the data within our flexible data-driven framework. We have statistically analyzed the closeness centrality~(CC) probability density function of the source nodes predicted by our trained SIDSL model on the \textit{Digg} dataset and compared it with the ground truth centrality of the source nodes in Figure~\ref{fig:cc}(c). The mean and standard deviation for the CCs of the predicted sources are 0.7020 and 0.1444, and that for the CCs of the ground truth sources are 0.7044 and 0.1567, showing that there is no harmful bias in our method's prediction. This statistical result indicates that our model captures the source distribution observed in empirical data, not just theoretical derivations. Our method not only uses knowledge to guide inference to accelerate learning but also learns distribution patterns beyond the knowledge, from the data.

\begin{figure}[h]
    \centering
    % \hspace{-0.7cm}
    % \subfloat[]{\includegraphics[width=0.33\textwidth]{fig/cc_Digg_new_2.png}\label{fig:ccsub1}}
    % % \hfill
    % \subfloat[]{\includegraphics[width=0.33\textwidth]{fig/cc_Digg_new_3.png}\label{fig:ccsub2}}
    % % \hfill
    % \subfloat[]{\includegraphics[width=0.33\textwidth]{fig/cc.png}\label{fig:ccsub3}}
    % \hspace{-0.5cm}
    \includegraphics[width=\linewidth]{fig/sir.pdf}
    \caption{Additional experiments for simulated SIR scenarios on basic performance comparison.}
% \vspace{-0.5cm}
    
    \label{fig:sir}
\end{figure}
\section{Additional Results of Performance under Other Propagation Patterns}
\label{app:res}
We conduct additional experiments for simulated SIR scenarios on basic performance comparison. The results are shown in Figure~\ref{fig:sir}. It can be seen that our model can still achieve competitive results compared to these baselines, proving our method's {applicability}. The results also indicate that in terms of precision, ours achieved the highest score, more than 30\% higher than the second-best SLVAE. Although we have a lower recall rate, a decrease of 0.09 only indicates around 1 node is not recalled from the ground truth, as only around 10 nodes are chosen as the ground truth sources in each infection. However, an increase of 0.3 in precision represents around 6 nodes correctly identified without false positives. Therefore, precision should be considered the more critical metric in source localization problems than recall when the F1 scores are similar, and SIDSL demonstrates the strongest competitiveness among the four methods.

\section{Dataset Description}\label{app:data}

The detailed description of the adopted datasets is presented as follows.
\begin{table}[h]
\centering
\caption{Overview of networks in the datasets. "CC" denotes "clustering coefficient"}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{ccccc}
\hline
\textbf{Dataset} & Nodes & Edges  & Mean Degree & CC \\ \hline

\textbf{Digg}    & 14511 & 194405 & 13.39       & 0.1353                 \\ 
\textbf{Twitter}    & 12619 & 309621 & 24.52       & 0.2962                \\ 
\textbf{Android}    & 9958 & 42915 & 8.62       & 0.4121                \\ 
\textbf{Christianity}    & 2897 & 30044 & 20.74       & 0.6027                \\ 
\textbf{Jazz}    & 198   & 2742   & 13.84       & 0.6174                 \\
\textbf{Net}     & 1589  & 2742   & 1.72        & 0.6377                 \\
\textbf{Power}   & 4941  & 6594   & 1.33        & 0.0801                 \\
\hline
\end{tabular}%

\label{tab:dataset}
\end{table}


\subsection{Real-world Propagation Dataset: Digg}
The datasets selected, Digg and Twitter, represent real-world social networks where information propagation can be authentically traced, and are commonly used for evaluation in previous works~\cite{ling2022source,huang2023two}. Both include propagation cascades demonstrating the time stamps and the information diffusion trace among users of each post or message. A connection network of all users is also provided in each dataset. Both datasets are pertinent to our study because they exemplify real-world dynamics of information spread. 

Digg~\citep{rossi2015network} is real-world social network data showcasing voting records of stories that made it to Digg's front page in 2009, with each story's spread counted as one diffusion cascade. We randomly choose 100 stories to form our dataset. The nodes~(voters) involved in these stories form a subgraph of the original one, where the links represent the friendship of voters. The statistics of this friendship network are shown in Table~\ref{tab:dataset}. 
Drawing an analogy to the spread of a virus during a pandemic, it is often difficult to detect the virus at the very beginning, but after some time has passed—such as when the manifestation of symptoms—we can observe the infection status of the population. As a result, for each story cascade, we choose the top 10\% of nodes and 30\% of nodes as diffusion sources and observed influenced nodes based on their influenced time. 

% In section 5.3, we also perform simulations on Digg of the SIS and the IC model for few-shot experiments. In the pretrain dataset preparation, we hold the network's topology and randomly choose between 0.15\% and 1.5\% of nodes to be the spreading sources of each propagation. We then use the SIS and the IC model to simulate 100 steps or until convergence.

\subsection{Real-world Propagation Dataset: Twitter}
The Twitter~\citep{yang2021full} dataset is a collection of social network and public tweets written in English that were posted on the social media platform Twitter~(a.k.a X) from March 24th to April 25th, 2012. The network statistics are shown in Table~\ref{tab:dataset}. Each tweet can be counted as one propagation cascade. Same as \textit{Digg}, for each cascade, we choose the top 10\% of nodes and 30\% of nodes as diffusion sources and observed influenced nodes based on their influenced time. 

% In section 5.3, we also perform simulations on Twitter of the SIS and the IC model for few-shot experiments. In the pretrain dataset preparation, we hold the network's topology and randomly choose between 0.15\% and 1.5\% of nodes to be the spreading sources of each propagation. We then use the SIS and the IC model to simulate 100 steps or until convergence.
\subsection{Community Q\&A Dataset: Android and Christianity}
In Stack Exchange Q\&A communities, knowledge dissemination occurs through a series of social interactions. When users post questions, provide answers, leave comments, or cast votes, these activities form a complex social network structure. Each post is marked with specific tags (such as "google-pixel-2" in the Android community), and these posts with identical tags, arranged chronologically, create what we call "cascades." Our study focuses on two Stack Exchange communities with distinctly different themes: the Android community, which specializes in mobile device technical support, and the Christianity community, which centers on religious and theological discussions.

Same as \textit{Digg}, for each cascade, we choose the top 10\% of nodes and 30\% of nodes as diffusion sources and observed influenced nodes based on their influenced time. 

\subsection{Synthetic Dataset} We synthesize propagation data under SIR, IC, and LT models on these three real-world networks: \textit{jazz, network science} and \textit{power grid}. These networks differ in scale, sparsity, and clustering characteristics, which enables us to investigate the model's performance on different types of networks. The statistic overview is presented in Table~\ref{tab:dataset}. For the propagation models, the propagation properties of the SIS infection model are determined by the inherent characteristics of the disease, applying homogeneity for all nodes/edges, i.e. the infection and recovery rates in SIR are constant; for the IC and the LT influence model, the heterogeneous propagation probability of each edge is considered, which is set to be inversely proportional to the degree of the target node. This aligns with real-world propagation patterns, where nodes with more connections tend to be less receptive to information from each neighbor.


\begin{itemize}
    \item \textit{Jazz}~\citep{rossi2015network}. The provided dataset is a network of collaborations among Jazz musicians. Each node in the network represents a musician, and every edge connects two musicians who have performed together in a band. Rumors or infectious diseases are applicable to be propagated on such networks. We randomly choose 5\% of nodes to be the spreading sources of each propagation and use SIS, IC, or LT models to simulate 100 steps or simulate until convergence. 
    \item \textit{Network Science~(Net)}~\citep{rossi2015network}. This is a coauthorship network of scientists working on network theory. Nodes represent scientists and edges represent collaborations. Influential information can be propagated on such networks. We randomly choose 0.5\% of nodes to be the spreading sources of each propagation and use SIS, IC, or LT models to simulate 100 steps or simulate until convergence. 
    \item \textit{Power Grid~(Power)}~\citep{watts1998collective}. This is a topology network of the power grid network across the Western United States. In this network, each connection denotes a transmission line for electrical power. The nodes signify one of three components: a power generation unit, a transformer, or a distribution substation. Blackouts can be propagated on such a network. We randomly choose 0.5\% of nodes to be the spreading sources of each propagation and use SIS, IC, or LT models to simulate 100 steps or simulate until convergence. 
\end{itemize}


\section{Baselines} \label{app:baseline}
We compare the performance of SIDSL against three state-of-the-art baselines of source localization methods using propagation snapshot observations. To the best of our knowledge, these methods are the only ones that illustrate their superiority against other works on locating sources without knowing the underlying propagation pattern, which is the same as ours. The detailed information is presented as follows.
\begin{itemize}
    \item \textbf{NetSleuth}~\citep{prakash2012spotting} utilizes a minimum description length approach to filter nodes from multiple sources, yet it is exclusively designed to operate within the Susceptible-Infected (SI) model framework.
    
    \item \textbf{LPSI}~\citep{wang2017multiple} is a novel method for detecting multiple sources of information diffusion in networks without a predefined propagation model, leveraging the concept of source prominence and label propagation to identify probable sources based on local peaks in the propagation landscape. In our experiment, the parameter $\alpha$ in LPSI is determined by testing it among the values \{0.1, 0.3, 0.5, 0.7, 0.9\} for each evaluation dataset and then selecting the best one.
    
    \item \textbf{GCNSI}~\citep{dong2019multiple} introduces a deep learning approach for identifying multiple rumor sources in social networks without needing the underlying propagation model, using graph convolutional networks to enhance prediction precision through spectral domain convolution and multi-order neighbor information. The setting of this model follows the description in~\citep{dong2019multiple}.
    
    \item \textbf{SLVAE}~\citep{ling2022source} is a probabilistic framework designed to tackle the challenge of source localization in graph diffusion problems using a variational autoencoder approach to quantify uncertainty and leverage prior knowledge. We follow the original implementation in the paper, tune the learning rate from 0.001 to 0.05, and select the best one.

    \item \textbf{TGASI}~\citep{hou2023sequential} is a sequence-to-sequence framework for multiple rumor source detection that considers heterogeneous user behavior in time-varying scenarios. It uses a GNN-based encoder to generate multiple features and a GRU-based decoder with temporal attention to infer sources. TGASI is designed with transferability and uses a unique loss function. 

    \item \textbf{DDMSL}~\citep{yan2024diffusion} proposes a novel probabilistic model for source localization and diffusion path reconstruction in complex networks. By formulating information propagation as a discrete diffusion process, DDMSL employs a reversible residual network to construct a denoising-diffusion model in discrete space. This approach allows for both accurate source identification and comprehensive reconstruction of information diffusion pathways. 
\end{itemize}

\section{Experiments and Implementation Details}
For each dataset, the ratio of training, validation, and testing portion is 6:1:1. 
For the diffusion framework of SIDSL, we use $T=500$ maximum diffusion timestep and linear schedule for noise scheduling. In the denoising network, we leverage a 2-layer graph convolutional network~(GCN) to forward the LPSI estimation $X_{est}$. The GNN encoder and decoder comprise 3-layer GCNs with a hidden dimension of 128. The residual GNN of the conditioning module is a 2-layer GCN, with a hidden dimension of 8. The learning rate is searched from {0.01, 0.005, 0.001}, and the maximum number of training epochs is set to 500 for all datasets. In the few-shot learning experiments, the maximum pretrain/finetune epoch is set to 300.  We train our model using Adam optimizer and a learning rate scheduler with a linear decay. Our model is trained on a single NVIDIA GeForce RTX 2080 Ti. The code implementation can be found at https://anonymous.4open.science/r/ASLDiff-4FE0.

\section{Limitations}
Our proposed method also exhibits certain limitations. Our approach may, to some extent, depend on the accuracy of the advice provided by soft labels, despite our application of various sophisticated designs to enhance the model's adaptability. As a result, when confronted with more complex scenarios, our method might reveal limitations. On the other hand, the sampling speed of our multi-step diffusion model may be slower compared to some deep learning methods, which could become a bottleneck for applications requiring real-time localization. 
{While computational constraints currently limit our model's direct application to million-node networks, the core principles we developed can be integrated into hierarchical approaches. This hierarchical strategy would effectively reduce the network scale, allowing us to leverage our method's proven strength in accurate source localization for moderately-sized networks.}
We will continue to conduct in-depth research in these areas.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
