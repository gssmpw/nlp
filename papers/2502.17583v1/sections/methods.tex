% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\section{Prediction Framework}\label{sec:framework}

Our aim is to capitalize on existing Earth system model (ESM) simulations for prediction on multi-year-to-decadal timescales.
We do this using an analog forecasting framework.
% In analog forecasting, an initial state (from which we are predicting) is matched to a library of potential analogs.
% The future of the best matching analogs are then used as a prediction for the future of the initial state.
Here, the library of analogs is composed of model simulations from the CMIP6 suite, and the matching method uses a learned mask of weights that emphasizes important precursor regions for the specific prediction task (target region, lead time, and variable).
We match on, and predict, annual mean 2-meter temperature.
Some example masks for different regions for a lead time of five years are shown in Figure \ref{fig:global_masks}.

\begin{figure}[h!]
    \noindent\includegraphics[width=\textwidth]{figures/global_mask_schematic.png}
    \caption{Model learned masks for annual mean 2-meter temperature for five regions: the western United States, the Amazon, northern Europe, the great lakes of Africa, and southern India.
    Each region (in fact, each grid cell) has a unique mask highlighting global locations that are important markers for the evolution of that region, on that time scale.}
    \label{fig:global_masks}
\end{figure}

% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\subsection{Data}\label{sec:data}

Our dataset includes 2-meter temperature from $29$ ESMs from the CMIP6 suite of simulations, detailed in Table \ref{table:cmip6}.
All model runs are obtained from the Earth System Grid Federation \cite<ESGF;>{esgf}.
All members include the historical period from $1850$-$2014$, while a subset contain the Shared Socioeconomic Pathways (SSP) projections corresponding to SSP-3.7.0 (usually $2015$-$2100$, but some models do not reach $2100$).
In total, there are $285$ simulations that include both the historical period and SSP projections, with an additional $183$ simulations that include only the historical period.
Observations of surface temperature are from the Berkeley Earth Surface Temperature (BEST) dataset \cite{best}.

All data is resampled to annual averages, leaving over $100,000$ states in the analog library.
Data is regridded to the coarsest native resolution among the models, $2.77$ (latitude) by $2.81$ (longitude) degrees, which translates to $65$ latitudes and $128$ longitudes.
Higher resolutions ($1.5$ and $2$ degrees) were tested for five year predictions of the western United States, but did not improve performance.
Data is normalized by baselining on a $30$ year ($1961-1990$) mean for each member,  which allows even warm biased models to be useful members of the analog library, while maintaining the mean climate trend.

\begin{table}[h!]
    \caption{CMIP6 dataset: total number of members for each model, as well as number (in parenthesis) that include their associated SSP-3.7.0 projections.}\label{table:cmip6}
    \centering
    \begin{tabular}{l r | l r | l r}
        \hline
        model & members & model & members & model & members \\
        \hline
        ACCESS-CM2    & 10 (5)  & ACCESS-ESM1-5 & 40 (40) & AWI-CM-1-1  & 5 (5)   \\
        AWI-ESM-1-1   & 1  (0)  & BCC-CSM2      & 3  (1)  & BCC-ESM1    & 3  (0)  \\
        CAMS-CSM1     & 3  (2)  & CESM2         & 10 (3)  & CESM2-WACCM & 3  (3)  \\
        CMCC-CM2-HR4  & 1  (0)  & CMCC-CM2-SR5  & 11 (1)  & CMCC-ESM2   & 1  (1)  \\
        CNRM-CM6-1    & 24 (6)  & CanESM5       & 60 (45) & CanESM5-1   & 72 (20) \\
        CanESM5-CanOE & 3  (3)  & GISS-E2-1-G   & 47 (23) & GISS-E2-2-G & 11 (5)  \\
        MIROC6        & 50 (50) & MIROC-ES2H    & 3  (0)  & MIROC-ES2L  & 30 (10) \\
        MPI-ESM1-2-HR & 10 (10) & MPI-ESM1-2-LR & 30 (30) & MRI-ESM2-0  & 11 (5)  \\
        NorESM2-LM    & 3  (3)  & NorESM2-MM    & 2  (1)  & TaiESM      & 2  (1)  \\
        UKESM1-0-LL   & 18 (11) & UKESM1-1-LL   & 1  (1)  &             &         \\
        \hline
    \end{tabular}
\end{table}

To compare our predictions with established methods, we obtain two IESMs, described in Table \ref{table:iesms}.
The IESMs are bias corrected by adding a lead time dependent bias to each member.
The bias is the mean difference between observations and each ensemble member for the $15$ year period preceding and including the initialization year.
There are many methods for correcting model drift in IESMs \cite{meehl2022}.
While more complicated correction methods may improve the IESMs performance, we opt for this correction for two reasons.
First, it is a simple method of correcting the IESMs.
Second, for clarity we present our method without making similar corrections to the analog library.
To avoid unduly favoring the IESMs, we maintain the simple IESM correction.
IESMs are then processed using the same steps as the analog library.

\begin{table}
    \caption{
        Initialized Earth System Models (IESMs): Both models use full field initialization beginning in November of each year and are run out to ten years \cite{meehl2021}.
    }\label{table:iesms}
    \centering
    \begin{tabular}{l c r}
        \hline
        model & members & initialized components \\
        \hline
        CESM1-1-CAM5-CMIP5 & 40 & Ocean  \\
        CMCC-CM2-SR5       & 10 & All    \\
        \hline
    \end{tabular}
\end{table}

% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\subsection{Mask of Weights}\label{sec:mask}

In order to identify precursor regions on which to match an initial state with a library of analogs, we turn to machine learning.
The CMIP6 data is split into three sets: an analog library, a training library, and a validation library.
The analog library is split off first, so that it contains at least one member from every model (there are no repeated members between the three libraries).
For each model, these libraries contain at most five members for the analog library, three members for the training library, and two members for the validation library, depending on availability.
Each of these libraries is processed into an input set and a target set.
The target region is selected for the target data.
Both the input and target data are normalized by their mean and standard deviation, and shifted by the prediction lead time.

\begin{figure}[h!]
    \noindent\includegraphics[width=\textwidth]{figures/main_schematic.png}
    \caption{Schematic illustrating the machine learning component of the prediction framework.
    The learned mask identifies global precursors for predicting a specific variable for a given lead time (L in the figure) for a specific region (western United States in the figure).}
    \label{fig:framework}
\end{figure}

Figure \ref{fig:framework} illustrates the machine learning task we employ to learn these precursor regions.
Two random states (global maps, one from the analog library, one from the training library) are selected and their difference is multiplied by a mask of weights (initialized as all ones).
For training, the batch size used is $64$ and the activation function for the mask is relu.
The weighted mean-squared error (MSE) of these two states is then passed through a single linear node which predicts the corresponding future target region MSE.
The mask of weights is updated through backpropagation (learning rate of $0.001$) until a strong correspondence between weighted input MSE and future target region MSE is established.
We employ early stopping (patience of $50$ epochs and minimum change of $0.0005$) using the MSE computed between states selected from the analog library and validation library, which contains $2500$ samples.
The loss is the error between the predicted and true future target MSE. 

In addition to the model learned mask, we train a second mask using transfer learning \cite{pan2010, ghani2024}.
We update the model learned mask by retraining on models (training and validation library) and BEST observations (analog library, includes years up to $2008$).
The learning rate is reduced to $0.0001$, and a trainable dense layer with five nodes and elu activation are added to the architecture between the input MSE and linear node.
We manually stop the training after $150$ epochs, to avoid the mask forgetting too much of the model learned mask weights.


% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\subsection{Analog Selection}\label{sec:selection}

From this point on there is no machine learning, only the learned mask is retained for the remainder of the prediction framework.
The full CMIP6 data set is preprocessed using the same steps outlined in Section \ref{sec:data}, without splitting into multiple libraries.

\begin{figure}[h!]
    \noindent\includegraphics[width=\textwidth]{figures/matching_schematic.png}
    \caption{Schematic illustrating the method we use to select analogs.
    Rather than match only the initialization state to the analog library (blue dashed lines), we enforce that years prior to the initialization also match (yellow solid lines, an example three year match).
    We call this tethering and it helps avoid selecting by-chance matches that may have very different evolutions.}
    \label{fig:matching}
\end{figure}

Analog selection is usually done by matching the single initialization state to all of the individual potential analog states.
This means finding the lowest MSE between the initialization state and the library of potential analogs.
The match is often calculated either everywhere on the globe (a global mask) or in the region of interest (a regional mask).
We compare the performance of our method to these two baselines in Section \ref{sec:results}.
In contrast, we use our learned mask and we match over multiple years (which we call ``tethering'') to find the best analogs.

Figure \ref{fig:matching} is a schematic of our analog selection method.
In the case of tethering, the initialization state has a weight of one, while previous years are inverse weighted, e.g., $1/2$, $1/3$ in the example shown.
Throughout, we use a tether length of two years, though the length of the optimal tether will likely be region and lead time specific, and could be determined using a left out model.
We leave this for future work.
An illustrative example of the year of the selected analogs versus each prediction year is shown in Figure \ref{fig:year_comp} for the model learned mask and a regional mask.


% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\subsection{Evaluation Metrics}\label{sec:metrics}

In order to comprehensively evaluate the performance of our prediction framework, we use several metrics.
As a base metric, we calculate the MSE between the prediction and truth over the target region.
We also evaluate our method using the continuous ranked probability score \cite<CRPS;>{Gneiting2007}.
The CRPS measures how representative a predicted distribution is, given a true value, allowing us to quantify the performance of the analog distribution.
CRPS collapses to mean absolute error in the case of a deterministic prediction.
Lower values of CRPS indicate better performance.

Additional metrics shown in the Supplementary Material are the Earth Mover's Distance \cite<EMD, calculated using the pyemd\footnote{\url{https://pypi.org/project/pyemd/}} package,>{pele2008, pele2009} and a measure of class accuracy.
The EMD measures similarity between two distributions \cite{rubner1998}; in our case, the predicted and true spatial temperature distribution at each time.
The EMD is an optimal transport problem, where, in our case, predicted temperatures are shifted around the target region until the predicted map matches the true map.
Smaller values of EMD indicate the prediction was closer to the truth.
In addition to redistributing temperatures to match, some excess temperature (bias) may need to be added or discarded, and a penalty can be added for this case.
We use a penalty of one, meaning for every degree added or removed, one is added to the final EMD value.
The EMD allows us to quantify how well our analogs are matching the temperature patterns in the region of interest.

For the class accuracy, we coarsen the truth and the prediction into a number of bins (bin edges are spatially and temporally determined), then calculate what fraction of the predictions fall in the same bin as the truth.
In the Supplementary Material, we show results for four classes.
The class accuracy allows us to quantify how well the analogs capture temperature more broadly.

