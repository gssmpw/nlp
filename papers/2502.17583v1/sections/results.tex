% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\section{Results}\label{sec:results}

Using the MSE and CRPS metrics (Section \ref{sec:metrics}), we now compare the learned mask analogs to global and regional mask analogs, as well as IESMs.
Results for additional metrics (EMD, class accuracy) can be seen in Supplementary Material Figures \ref{fig:emd_lead}-\ref{fig:emdacc_region}.

Throughout, predictions are for a single annual average of the target region map, e.g., the 5 year prediction initialized in 2010 is for the year 2015 only.
We treat each prediction as continuous, e.g., the best matching analog for each year is combined into a single time series map prediction, as is the second best match for each year, and so on.
We refer to these time series map predictions as analog forecasts, which we use individually (for metric distributions) or to create an ensemble mean prediction.
Example predictions, averaged over the target region, are shown in Figure \ref{fig:wus_timeseries}.
Individual analog forecasts allow us to assess predictions that have similar variability to observations, while the analog ensemble mean prediction helps determine how well we are capturing average trends \cite{Kim2025}.
All summary metrics shown are the mean metric over the target region and time.
We first assess performance for lead times 1 to 10 years for a single region (the western United States).
We then assess performance for a single lead time (5 years) in five different regions (shown in Figure \ref{fig:global_masks}).

\begin{figure}[h!]
    \noindent\includegraphics[width=\textwidth]{figures/timeseries_wus5.png}
    \caption{Example region-averaged analog predictions and observations for the western United States for a lead time of 5 years.
    The 50 analog ensemble mean is shown (yellow-black), along with each of the 50 individual analog forecasts (thin, yellow), and the truth from the BEST observational dataset (white-black).
    }
    \label{fig:wus_timeseries}
\end{figure}

% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\subsection{Western United States 1-10 Year Predictions}\label{sec:wus}

\begin{figure}[h!]
    \centering
    \noindent\includegraphics[width=0.49\textwidth]{figures/wus_masks_lt1510.png}
    \noindent\includegraphics[width=0.49\textwidth]{figures/wus_tlmasks_lt1510.png}
    \caption{Weighted masks for the western United States region (the black bounded area), scaled by the maximum weight to highlight the pattern.
    The left panels show model learned masks for lead times of 1, 5, and 10 years.
    The right panels show transfer learned masks (see Section \ref{sec:mask}) for the same lead times.
    }
    \label{fig:wus_masks}
\end{figure}

The learned masks for western United States 2-meter temperature prediction are lead time dependent.
We show three examples of the model learned and transfer learned masks on the left and right sides of Figure \ref{fig:wus_masks}, respectively.
Focusing first on the model learned masks, we see that the region itself is usually an important precursor region.
Though varying in the specific pattern, the mid-latitudes are consistently important precursor regions, while the tropics are less important.
The Southern Ocean is not an important precursor region for any of the lead times, while the Mediterranean Sea is important for all lead times.
The Gulf of Mexico is important for lead times greater than 1 year.
North America, outside the western United States region, Asia, Europe, and northern Africa all have low importance.
Antarctica often has some importance, while the Arctic Ocean has negligable weights.

The strongest exception to these general trends occur for lead times of 4 and 8 years (not shown here, masks for all lead times can be seen in Figures \ref{fig:wus_all_learned_masks} and \ref{fig:wus_all_tl_masks} in the Supplementary Material).
For those lead times, North America, Asia, Europe, and northern Africa have more importance.
Antarctica has nearly zero weights, while the Arctic Ocean is more important than at the other lead times.

The transfer learned masks in Figure \ref{fig:wus_masks} show the masks after finetuning on a subset of the BEST dataset.
The transfer learned masks have primarily been refined, with important regions made even more important, and low importance regions reduced further, indicating that the observations prefer more sparsity.

\begin{figure}[h!]
    \centering
    \noindent\includegraphics[width=0.8\textwidth]{figures/wus_lead_full.png}
    \caption{Western United States prediction metrics for lead times 1 to 10 years, covering the period $1864$-$2023$.
    (a) CRPS for 50 analog forecast distributions.
    (b) MSE for the 50 analog ensemble mean predictions.
    (c) MSE for each distribution of 50 analog forecasts, as well as the median MSE from these distributions.
    }
    \label{fig:metrics_wus_full}
\end{figure}

Figure \ref{fig:metrics_wus_full} shows the CRPS and MSE (mean over the time period $1864$-$2023$) for analog forecasts of the western United States, with lead times 1 to 10 years.
We use 50 analog forecasts throughout.
MSE and CRPS versus the number of analogs used is shown in Supplementary Material Figures \ref{fig:mse_nana_full} and \ref{fig:crps_nana_full}.
For years before $1956$, the BEST dataset does not have data at every global location, thus the learned and global analogs are matched on only the available locations for those years.

The metrics in Figure \ref{fig:metrics_wus_full} are all for analog forecasts, but the analogs are selected by matching either everywhere (global), in just the western United States (regional), or using our model learned mask of weights.
At all lead times, the model learned mask produces a better distribution of analogs according to the CRPS metric, shown in panel (a).
Likewise, the learned mask produces an ensemble mean prediction that better tracks the true mean regional trend at all lead times, according the MSE of the ensemble mean prediction, shown in panel (b).
Finally, the analog forecast MSE distribution, shown in panel (c), using the model learned mask is lower than the other masks, indicating that the learned masks selects analogs that are better representations of the regional variability.

\begin{figure}[h!]
    \centering
    \noindent\includegraphics[width=0.8\textwidth]{figures/wus_lead_tl.png}
    \caption{Same as Figure \ref{fig:metrics_wus_full}, but comparing the model learned mask, the transfer learned mask, and the IESMs.
    The period covered is $2009$-$2018$, which allows observations up to $2008$ to be used for the transfer learning.
    }
    \label{fig:metrics_wus_tl}
\end{figure}

In Figure \ref{fig:metrics_wus_tl}, we compare the performance of our model learned mask, along with our transfer learned mask, to a set of $50$ IESM members.
The time period covered for these metrics is more limited for two reasons.
For the transfer learning, we need as many samples as we can get while still leaving some years for testing.
We use BEST observations up to $2008$.
For the IESMs, the latest common initialization year is $2017$, so $2018$ is the final year for which we have predictions for every lead time.
Thus, the period examined in Figure \ref{fig:metrics_wus_tl} is ten years, $2009$-$2018$.

For both CRPS and MSE, the IESMs outperform our learned masks for lead times 1 and 2 years.
For lead times longer than 2 years, our learned masks outperform the IESMs.
The transfer learned mask improves upon the model learned mask in several cases.
For both the CRPS and ensemble mean MSE (panels (a) and (b)), it is as good or better than the model learned mask for all lead times except 10.
For the analog forecast MSE distribution (panel (c)), the transfer learned mask improves on the model learned mask for lead times 1, 3, 4, 5, and 7.
Our focus here is not on transfer learning, thus we have not optimized the transfer learning process (number of observation samples, training epochs, updated architecture).
Based on these results, incorporating observations in the analog framework is a promising direction for future work.

% --------------------------------------------------------------------------
% --------------------------------------------------------------------------
\subsection{Regional 5 Year Predictions}\label{sec:regions}

We now explore the performance of our method in different regions.
To the western United States we add four additional regions: the Amazon, northern Europe, southern India, and the African Great Lakes.
All predictions in this section are for a lead time of 5 years, and we explore the model learned mask only, dropping the transfer learned mask so that results can be evaluated over a longer time period.

\begin{figure}[h!]
    \centering
    \noindent\includegraphics[width=0.75\textwidth]{figures/regions5_full.png}
    \caption{5 year prediction metrics for five different regions (see Figure \ref{fig:global_masks}) covering the period $1956$-$2023$.
    (a) CRPS for 50 analog forecast distributions.
    (b) MSE for the 50 analog ensemble mean predictions.
    (c) MSE for each distribution of 50 analog forecasts, as well as the median MSE from these distributions.
    }
    \label{fig:metrics_regions_full}
\end{figure}

We once again start by comparing to alternative analog methods in Figure \ref{fig:metrics_regions_full}, which shows the same metrics as the figures in Section \ref{sec:wus}.
For the CRPS and ensemble mean MSE (panels (a) and (b)), the learned mask outperforms the global and regional mask in all of the regions, with the global mask generally outperforming the regional.
For the analog forecast MSE distribution (panel (c)), the learned mask outperforms the global and regional mask in four out of the five regions.
The Amazon region is the exception, where the regional mask performs best.
This indicates that local conditions in the Amazon region are very important for predicting future variability of that region.
The learned mask (Figure \ref{fig:global_masks}) does place a lot of weight in the Amazon region, but highlights other regions as more important.
This may mean the learned masks could be improved with the addition of a loss term that more directly accounts for variability.

\begin{figure}[h!]
    \centering
    \noindent\includegraphics[width=0.75\textwidth]{figures/regions5_tl.png}
    \caption{Same as Figure \ref{fig:metrics_regions_full}, but for the period $1999$-$2018$.
    Bias-corrected IESMs (CESM and CMCC, see Table \ref{table:iesms}) can be directly compared for this time period.
    }
    \label{fig:metrics_regions_tl}
\end{figure}

Figure \ref{fig:metrics_regions_tl} includes the IESMs, and covers the twenty year time period $1999$-$2008$.
Once again, the learned mask is as good or better than the other analog methods for all regions and metrics.
The learned mask outperforms the IESMs in the western United States region.
The learned mask performs similarly to the IESMs in the Amazon, southern India, and the African Great Lakes, while the IESMs outperform the learned mask in northern Europe.
For the analog forecast MSE, the learned mask outperforms the IESMs in the western United States, while the IESMs outperform the learned masks in the Amazon, northern Europe and the African Great Lakes.

