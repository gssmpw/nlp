\section{Related Work}
\subsection{Feature-space-based Interpolation Methods}
Feature-space-based interpolation methods typically divide the point cloud upsampling network into three crucial steps: feature extraction, feature expansion, and coordinate prediction. As a pioneer work, PU-Net \cite{yu2018pu} first proposes this three-step framework to effectively learn the mapping from sparse point clouds to dense point clouds. Subsequently, with advancements in network designs, more sophisticated end-to-end networks have emerged \cite{yu2018ec, yifan2019patch, li2019pu, qian2020pugeo, zhao2021sspu, ye2021meta, Mao2022pu}. To better encode local point information from point neighborhoods, PU-GCN \cite{qian2021pu} incorporates graph convolutional networks to encode point cloud features. The transformer structure is first introduced by PU-Transformer \cite{qiu2022pu} for extracting fine-grained point cloud features. SPU-Net \cite{liu2022spu} introduces self-supervised learning by innovatively designing a coarse-to-fine structure for point cloud upsampling. Drawing inspiration from the Dis-PU \cite{li2021point} framework, SSPU-Net \cite{wang2023sspu} integrates frequency-aware attention mechanisms to extract edge and contour information from point clouds. However, these methods directly predict point cloud coordinates after the feature expansion step, which is challenging work and usually results in lower accuracy. Our proposed method captures local topological connections and variations between point cloud patches, enabling the generation of highly accurate upsampled point clouds.

\subsection{Point-cloud-based Interpolation Methods}
Point-cloud-based interpolation methods first insert rough point clouds directly between the input points. Then, they predict the correct location of rough points through the learned shape and structural features \cite{lv2021approximate, lv2022intrinsic, wei2023ipunet, rong2024repkpu, liu2024pu}. As a pioneering work in shape expression, NePs \cite{feng2022neural} is the first to introduce implicit neural fields for representing the global shapes of point clouds. SAPCU \cite{zhao2022self} and PUSS-AS \cite{zhao2023self} propose self-supervised approaches by regressing seed points to the object surface using learned signed distance functions. P2PNet is proposed by Grad-PU \cite{he2023grad} to refine seed points through learned point-to-point distance functions. To further remove holes, IFLDI \cite{li2023learning} guides seed point projection by learning unsigned distance fields and local distance indicators. More recently, SPU-PMD \cite{liu2024spu} designs a series of mesh deformers to improve coarse points generated from the mesh interpolator. However, these methods are easily making incorrect predictions for rough points. Our proposed method avoids generating rough points and can produce arbitrary-scale upsampled point clouds with just one-time training.