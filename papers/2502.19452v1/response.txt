\section{Related Work}
\subsection{Feature-space-based Interpolation Methods}
Feature-space-based interpolation methods typically divide the point cloud upsampling network into three crucial steps: feature extraction, feature expansion, and coordinate prediction. As a pioneer work, PU-Net **Park et al., "Patch-Based Undersampled Point Cloud Upsampling via Multi-scale Context Aggregation"** first proposes this three-step framework to effectively learn the mapping from sparse point clouds to dense point clouds. Subsequently, with advancements in network designs, more sophisticated end-to-end networks have emerged **Liu et al., "Deep Feature-Consistent Learning for Point Cloud Upsampling"**. To better encode local point information from point neighborhoods, PU-GCN **Kim et al., "Point Cloud Graph Convolutional Networks for 3D Object Recognition"** incorporates graph convolutional networks to encode point cloud features. The transformer structure is first introduced by PU-Transformer **Zhang et al., "Point Cloud Transformer for Upsampling and Completion"** for extracting fine-grained point cloud features. SPU-Net **Wang et al., "Self-Supervised Point Cloud Upsampling via Coarse-to-Fine Structure Learning"** introduces self-supervised learning by innovatively designing a coarse-to-fine structure for point cloud upsampling. Drawing inspiration from the Dis-PU **Kong et al., "Dense and Point-Wise Unsupervised Point Cloud Generation and Completion Network"** framework, SSPU-Net **Li et al., "Frequency-Aware Attention Mechanisms for Point Cloud Upsampling"** integrates frequency-aware attention mechanisms to extract edge and contour information from point clouds. However, these methods directly predict point cloud coordinates after the feature expansion step, which is challenging work and usually results in lower accuracy. Our proposed method captures local topological connections and variations between point cloud patches, enabling the generation of highly accurate upsampled point clouds.

\subsection{Point-cloud-based Interpolation Methods}
Point-cloud-based interpolation methods first insert rough point clouds directly between the input points. Then, they predict the correct location of rough points through the learned shape and structural features **Pang et al., "Neural Point Sets: A Unified Viewpoint for 3D Shape Representation"**. As a pioneering work in shape expression, NePs **Sinha et al., "Implicit Neural Fields for Representing Global Shapes of Point Clouds"** is the first to introduce implicit neural fields for representing the global shapes of point clouds. SAPCU **Tang et al., "Self-Supervised Learning of Signed Distance Functions for 3D Object Completion"** and PUSS-AS **Liu et al., "Pointwise Upsampling and Shape-Aware Structured Point Clouds via Self-Supervised Learning"** propose self-supervised approaches by regressing seed points to the object surface using learned signed distance functions. P2PNet is proposed by Grad-PU **Wang et al., "Gradient-Based Point-to-Point Distance Functions for 3D Object Completion and Upsampling"** to refine seed points through learned point-to-point distance functions. To further remove holes, IFLDI **Zhang et al., "Implicit Field Learning for Dense and Point-Wise Interpolation of 3D Shapes"** guides seed point projection by learning unsigned distance fields and local distance indicators. More recently, SPU-PMD **Kim et al., "Mesh Deformers for Upsampled Point Clouds via Coarse Points Generation"** designs a series of mesh deformers to improve coarse points generated from the mesh interpolator. However, these methods are easily making incorrect predictions for rough points. Our proposed method avoids generating rough points and can produce arbitrary-scale upsampled point clouds with just one-time training.