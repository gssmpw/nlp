\begin{algorithm}[t]

\caption{Background-Focal GDA}
\label{algorithm:full_information_gda}

\begin{algorithmic}[1]
\STATE \textbf{Input\,} 
Background policies $\mathcal{B}$, 
and
learning rates ($\eta_\pi$, $\eta_\beta$).
\STATE Simplex projector $\mathcal{P}$
\STATE Initialise the main policy parameters $\theta_0$ randomly
\STATE Initialise the distribution as uniform $\beta_0 = \mathcal{U}(\Sigma(\mathcal{B}))$ 

\FOR{$t=0, \dots, N-1$}

    %\STATE Compute induced MDP $\mu(\beta_t) = \langle S, A, P_{\beta_t}, \rho_{\beta_t}, \gamma \rangle$
    \STATE Compute $U(\pi_{\theta_t}, \sigma)$ for all $\sigma \in \Sigma(\mathcal{B})$
    \STATE Compute $U(\pi_{\theta_t}, \beta_t) = \sum_\sigma U(\pi_{\theta_t}, \sigma) \beta_t(\sigma)$
    \STATE Compute $R(\pi_{\theta_t}, \sigma) = U^*(\sigma) - U(\pi_{\theta_t}, \sigma)$ for all $\sigma \in \Sigma(\mathcal{B})$

    \STATE Obtain $L(\pi_{\theta_t}, \beta_t) = \sum_\sigma R(\pi_{\theta_t}, \sigma)\beta_t(\sigma)$

    \IF{solving maximin utility \eqref{eq:mbmarl.maximin}}
        \STATE Update distribution $\beta_{t+1} = \mathcal{P}(\beta_t -\eta_{\beta}\nabla_{\beta} U(\pi_{\theta_t}, \beta_t))$
    \ENDIF
    \IF{solving minimax regret \eqref{eq:mbmarl.minimax}}
        \STATE Update distribution $\beta_{t+1} = \mathcal{P}(\beta_t + \eta_{\beta}\nabla_{\beta} L(\pi_{\theta_t}, \beta_t))$
    \ENDIF
    \STATE Update policy parameters $\theta_{t+1} = \theta_t + \eta_{\theta}\nabla_{\theta} U(\pi_{\theta_t}, \beta_t)$
\ENDFOR

\RETURN $\theta^*, \beta^*$ uniformly sampled from $\{(\theta_1, \beta_1), \dots, (\theta_N, \beta_N)\}$
\end{algorithmic}
\end{algorithm}