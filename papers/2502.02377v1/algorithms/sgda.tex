\begin{algorithm}[h]

\caption{Background-Focal SGDA}
\label{algorithm:unknown_mdp_gda}

\begin{algorithmic}[1]
\STATE \textbf{Input\,} 
set of background policies $\mathcal{B}$, batch size $B$,
learning rates ($\eta_\pi$, $\eta_\beta$)

\STATE Simplex projector $\mathcal{P}$
\STATE Initialise randomly the main policy parameters $\theta_0$
\STATE Initialise the belief as the uniform distribution over possible scenarios $\beta_0 = \mathcal{U}(\Sigma(\mathcal{B}))$  

\FOR{$t=0, \dots, N-1$}

    %\STATE Compute induced MDP $\mu(\beta_t) = \langle S, A, P_{\beta_t}, \rho_{\beta_t}, \gamma \rangle$
    \STATE Sample $B$ scenarios $\sigma_1, \dots, \sigma_B \sim \beta_t$
    \STATE Estimate $\hat U(\pi_{\theta_t}, \sigma_i)$ by deploying $\pi_{\theta_t}$ on $\sigma_i$, for  $i = 1, \dots, B$
    \STATE Compute $\hat U(\pi_{\theta_t}, \beta_t) = \dfrac{1}{B} \sum_{i=1}^{B} \hat U(\pi_{\theta_t}, \sigma_i)$
    
    \STATE Compute $\hat R(\pi_{\theta_t}, \sigma_i) = U^*(\sigma_i) - \hat U(\pi_{\theta_t}, \sigma_i)$ for each  $i = 1, \dots, B$

    \STATE Compute $\hat L(\pi_{\theta_t}, \beta_t) = \dfrac{1}{B} \sum_{i=1}^{B} \hat R(\pi_{\theta_t}, \sigma_i)$
    \IF{solving maximin utility}
        \STATE Update belief $\beta_{t+1} = \mathcal{P}(\beta_t - \eta_{\beta}\nabla_\beta \hat U(\pi_{\theta_t}, \beta_t))$
    \ENDIF
    \IF{solving minimax regret}
    \STATE Update belief $\beta_{t+1} = \mathcal{P}(\beta_t + \eta_{\beta}\nabla_\beta \hat L(\pi_{\theta_t}, \beta_t))$
    \ENDIF
    
    \STATE Update policy parameters $\theta_{t+1} = \theta_t + \eta_{\theta}\nabla_\theta \hat U(\pi_{\theta_t}, \beta_t)$
\ENDFOR

\RETURN $\theta^*, \beta^*$ uniformly at random from $\{(\theta_1, \beta_1), \dots, (\theta_N, \beta_N)\}$
\end{algorithmic}
\end{algorithm}