% Unofficial University of Cambridge Poster Template
% https://github.com/andiac/gemini-cam
% a fork of https://github.com/anishathalye/gemini
% also refer to https://github.com/k4rtik/uchicago-poster

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[orientation=portrait, size=a0,scale=1.0]{beamerposter}
% \usepackage[width=36, height=24, scale=1.0]{beamerposter}
%\usepackage[scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{nott}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
\usepackage{adjustbox}
\input{macros}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.02\paperwidth}
\setlength{\colwidth}{0.46\paperwidth}
\setlength{\maxlogowidth}{0mm}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

\newcommand{\boxalign}[2][0.99\textwidth]{
 \par\noindent\tikzstyle{mybox} = [draw=black,inner sep=6pt]
 \begin{center}\begin{tikzpicture}
  \node [mybox] (box){%
   \begin{minipage}{#1}{\vspace{-5mm}#2}\end{minipage}
  };
 \end{tikzpicture}\end{center}
}

% ====================
% Title
% ====================

\title{A Minimax-Bayes Approach to Ad Hoc Teamwork}

\author{Victor Villin \inst{1} \and Thomas Kleine Buening \inst{2} \and Christos Dimitrakakis \inst{2 3}}

\institute[shortinst]{\inst{1} University of Neuchatel \samelineand \inst{2} The Alan Turing Institute  \samelineand 
\inst{3} University of Oslo 
}

% ====================
% Footer (optional)
% ====================

\footercontent{
  EWRL17 (2024)
  \hfill
  \href{mailto:victor.villin@unine.ch}{victor.villin@unine.ch}  } 
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
\logoleft{\centering
\includegraphics[height=5cm]{figures/unine_logo_blanc.png}
}

\logoright{\centering
\includegraphics[height=3.5cm]{figures/Alan_Turing_Institute_logo_white.png}
\hspace{1mm}
\includegraphics[height=3.5cm]{figures/uio-segl-negativ-150x150.png}
}
%\logoright{\centering\includegraphics[height=5cm]{figures/ICML_logo_white.png}}

% ====================
% Body
% ====================

\begin{document}

% Refer to https://github.com/k4rtik/uchicago-poster
% logo: https://www.cam.ac.uk/brand-resources/about-the-logo/logo-downloads
% \addtobeamertemplate{headline}{}
% {
%     \begin{tikzpicture}[remember picture,overlay]
%       \node [anchor=north west, inner sep=3cm] at ([xshift=-2.5cm,yshift=2.75cm]current page.north west)
%       {\includegraphics[height=4.5cm]{logos/unott-logo.eps}}; 
%     \end{tikzpicture}
% }

\begin{frame}[t]

\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}
  
    \begin{block}{Learning policies for AHT is challenging}
    \vspace{8mm}
    \begin{alertblock}{}
        \bfemph{Ad Hoc Teamwork (AHT)} occurs when multiple agents, initially \bfemph{unfamiliar} with each other, must \bfemph{collaborate} to achieve a \bfemph{common} goal.
    \end{alertblock}
    \begin{enumerate}
        \item Numerous and diverse \bfemph{scenarios} possible,
        \item Existing methods offer \bfemph{limited guarantees} in terms of \bfemph{worst-case} AHT performance,
        \item \bfemph{Robust AI-Human Cooperation} is becoming a concern,
        \item The distribution of \bfemph{training} partners is typically \bfemph{not} the distribution of partners \bfemph{after deployment}.
    \end{enumerate}

    \begin{alertblock}{}
    We consider using the \bfemph{worst} possible prior over scenarios to learn a robust policy, an idea adopted from the \bfemph{minimax-Bayes} concept.
    \end{alertblock}
    \vspace{-5mm}
    % \begin{alertblock}{}
    %     {We propose an \bfemph{environment design} framework for IRL to recover robust estimates of the true reward function with the \bfemph{least} number of samples.
    %     }
    % \end{alertblock}
  \end{block}


    \begin{block}{Evaluating AHT capabilities}
    \vspace{5mm}
    We are interested in learning a \bfemph{robustly cooperative} policy $\pi^f$ for some $m$-player Partially Observable Markov Game (POMG) $\mdp$.
    \vspace{-5mm}
     \heading{Scenarios.}
 A scenario $\scenario=(c, \bpi^b)$ is characterised by its actors:
    \begin{itemize}
        \item $c$ \bfemph{focal} players $\bpi^f = (\pi^f, \dots, \pi^f)$ (all equal to the learned policy).
        \item $m-c$ \bfemph{background} players $\bpi^b = (\pi^b_1, \dots, \pi^b_{m-c})$ (fixed),
        \item Each scenario can be seen \bfemph{as its own} $c$-player POMG $\mdp(\scenario)$.
        \item We construct scenario sets with a background population of policies $\bgpop$:
    \end{itemize}
    \vspace{-5mm}
    \begin{equation*}
        \scenarioset(\bgpop) \defeq \{ (c, \bpi^b) \mid 1 \leq c \leq m, \bpi^b \in \bgpop^{m-c}\},
\end{equation*}

    \heading{Objectives.} We want a policy that \bfemph{reliably maximises utility / minimises regret}, regardless of the scenario:
    \vspace{-6mm}
    \begin{itemize}
        \item $\scenario$-induced reward function $\rewardfunc_\scenario$; focal actions $\afocal = (a^f_1, \dots, a^f_c)$,
        \vspace{-2mm}
        \begin{equation*}
        \label{eq:EU}
        \text{\bfemph{Mean focal utility:}}\quad
        U(\pi, \scenario) \defeq 
        % \mathbb{E} [u(\pi, \scenario)] =
        \sum_{t=1}^T  \dfrac{1}{c} \sum_{i=1}^c \mathbb{E}^{\pi}_{\mdp(\scenario)}[\rho_\scenario(s_t, \afocal_t, i)], \hfill 
        \end{equation*}
    \item Maximal utility for scenario $\scenario$, $U^*(\scenario)$,
        \begin{equation*}
         \text{\bfemph{Regret:}}\quad
        R(\pi,\scenario) \defeq U^*(\scenario) - U(\pi, \scenario).
        \end{equation*}
    \end{itemize}
    \vspace{5mm}
    We \bfemph{assess} learning methods with the following two-phased protocol:
        \vspace{-5mm}
        \heading{1) Training}:
        \vspace{-5mm}
        \begin{itemize}
            \item A background population of training partners $\poptrain$ is \bfemph{provided},
            \item A test set of scenarios $\scenariotest$ is kept \bfemph{held-out},
            \item The learner is allowed to do anything for $N$ environment steps.
        \end{itemize}
        \vspace{-10mm}
        \heading{2) Testing}:
        \vspace{-5mm}
        \begin{itemize}
            \item \bfemph{No more} learning,
            \item The obtained policy is \bfemph{evaluated} on held-out test scenarios $\scenariotest$ with:
        \vspace{3mm}
        \begin{alertblock}{}
        \vspace{-16mm}
        \begin{align*}
        \label{eq:metrics}
        & \text{\bfemph{Average utility}} && \;\;\text{\bfemph{Worst-case utility}} && \;\;\text{\bfemph{Worst-case regret}}\\
        &  \perf(\pi, \scenarioset) = \dfrac{1}{|\scenarioset|}\sum_{\scenario \in \scenarioset} U(\pi, \scenario), \quad && \wcu(\pi, \scenarioset) =  \min_{\scenario \in \scenarioset} U(\pi, \scenario),\quad&& \wcr(\pi, \scenarioset) =  \max_{\scenario \in \scenarioset} R(\pi, \scenario).
    \end{align*}
    \vspace{-8mm}
    \end{alertblock}
        \end{itemize}


    %\heading{Assumptions.}
    %\vspace{-5mm}
    %\begin{enumerate}
    %    \item Absence of prior coordination,
    %    \item No control over teammates
    %    \item All agents share a common objective, with different preferences: %prosociality levels $\lambda_i$ and risk aversion $\delta_i$.
    %\end{enumerate} 
    \end{block}
    \begin{block}{Achieving Robust AHT}
    Two main ingredients:
    \vspace{-5mm}
    \begin{itemize}
        \item[\textbf{1)}] Diverse training partners representative of what is in nature.
        \item[\textbf{2)}] \textbf{An appropriate \bfemph{prior} $\beta$ over scenarios.}
    \end{itemize}
    \vspace{-8mm}
    \hspace{1cm} \Rightarrow We pick the \bfemph{minimax} prior w.r.t. utility/regret.
    %\heading{1) Constructing Training Scenarios.} To thoroughly assess the \bfemph{effects} of partner priors, we adopt the following  approach:
    %\vspace{-8mm}
    %\begin{itemize}
    %\item Background policies have different \bfemph{preferences}: prosociality levels $\lambda_i$ and risk aversion $\delta_i$, 
    %\item Policies are organized into \bfemph{sub-populations} $\bgpop = \bigcup_k \bgpop_k$ of varying sizes,
    %\item Sub-populations are \bfemph{separately trained} through Population Play (PP),
    %\item \bfemph{Distinct} habits, common practices, and established conventions will emerge within each group, mimicking \bfemph{various} cultures.
    %\end{itemize}
    \vspace{8mm}
    
    With \bfemph{expected utility} and \bfemph{Bayesian regret} over scenario distributions defined as:
    \begin{equation*}
         U(\pi, \beta) \defeq \sum_{\scenario} U(\pi, \scenario) \beta(\scenario), \quad\quad\quad L(\pi, \beta) \defeq \sum_\sigma R(\pi, \mdp) \beta(\sigma).
    \end{equation*}
    
    \begin{alertblock}{}
    We play one of the following minimax games:
    \begin{equation*}
     \max_{\pi} \min_{\beta \in \Delta(\scenarioset(\bgpop))} U(\pi, \beta), \;\;\text{(1)}\quad\quad\quad \min_{\pi} \max_{\beta \in \Delta(\scenarioset(\bgpop))}L(\pi, \beta). \;\;\text{(2)}
    \end{equation*}
    \vspace{-5mm}
    \end{alertblock}
    \begin{itemize}
        \item \bfemph{Compute} solutions ($\pi^*, \beta^*$) with (stochastic) \bfemph{gradient descent ascent},
        \item \bfemph{Convergence guarantees} for \bfemph{simple} parametrisations of policies,
        \item \bfemph{Approximate} $c$-player scenarios to \bfemph{single-agent} environments by replacing delayed $c-1$ copies with a delayed version of the focal policy $\pi^f_{t-d}$.
        \item \bfemph{Assumption}: No "impossible" scenario.
    \end{itemize}
    \vspace{-10mm}
    \end{block}
    \end{column}

    \separatorcolumn
    
    \begin{column}{\colwidth}

    \begin{figure}
    \centering
    \vspace{-5mm}
    \includegraphics[width=0.6\textwidth]{data/illustration_bf.png}
    \vspace{-3mm}
    \caption{Training framework proposed in this work.}
    \vspace{-6mm}
    \end{figure}

\begin{block}{Robustness Guarantees}
    Policies \bfemph{$\pi^*_U$} and \bfemph{$\pi^*_R$} solving~(1) and (2), respectively, have several \bfemph{properties}.
    
    \textbf{In-Distribution}: \bfemph{Optimal} worst-case utility/regret on the \bfemph{training set}.
    
    \textbf{Out-Of-Distribution}: If the true scenarios are all \bfemph{$\epsilon$-close} to one of the training scenarios:
    \begin{equation*}
         \wcu(\pi^*_U, \scenarioset^\text{true}) > \max_{\pi} \left( \wcu(\pi, \scenarioset^\text{train}) - \dfrac{\epsilon T^2 \rmax}{2} \right),
    \end{equation*}
    \begin{equation*}
        \wcr(\pi^*_R, \scenarioset^\text{true}) < \min_{\pi} \left( \wcr(\pi, \scenarioset^\text{train}) + \epsilon T^2 \rmax \right).
    \end{equation*}
    \vspace{-18mm}
    
\end{block}
\begin{block}{Experimental Results}
    \heading{\Large Repeated Prisoner's Dilemma. (3 rounds, Fully adaptive policies)}
    
    \textbf{Training partners $\poptrain$}: 9 ad-hoc policies (pure defect/cooperate, tit-for-tat...)
    
    \textbf{Test partners $\poptest$}: 512 sampled policies $\epsilon$-close to training partners ($\epsilon=0.5$). 

    \begin{table}[t]
    %\small
    \setlength{\tabcolsep}{20pt}
    \centering
    \caption{
    Scores on the repeated Prisoner's Dilemma. Note: lower regret is better.}
    \vspace{-8mm}
    \label{tab:prisoner.train}
    \begin{tabular}{l||ccc||ccc||} %\toprule
    & \multicolumn{3}{c||}{$\scenarioset(\poptrain)$} & \multicolumn{3}{c||}{$\scenarioset(\poptest)$}\\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
               & $\perf$ & $\wcu$ & $\wcr$ & $\perf$ & $\wcu$ & $\wcr$ \\ \midrule
        Minimax Utility (MU)  & $7.69$ & $\mathbf{3.00}$ & $9.00$ & $\mathbf{8.34}$ & $\mathbf{3.00}$ & $9.00$\\
        Maximin Regret (MR) & $8.23$ & $2.26$ & $\mathbf{3.79}$ & $7.96$ & $2.78$ & $\mathbf{4.35}$\\ \hdashline
        Population Best Response (PBR)  & $\mathbf{8.54}$ & $2.00$ & $4.97$ & $8.07$ & $2.48$ & $5.63$\\
        Fictitious-Play (FP) & $7.06$ & $0.14$ & $10.56$ & $6.33$ & $0.56$ & $9.96$\\
        Self-Play (SP)  & $7.25$ & $0.47$ & $10.19$ & $6.49$ & $0.86$ & $9.65$\\
        Random & $7.40$ & $1.50$ & $5.50$ & $7.47$ & $2.18$ & $5.20$\\
    % \bottomrule
    \end{tabular}
    \end{table}
    \input{data/prisoners/binary_trees_poster}

    \vspace{-5mm}

    \heading{\Large Collaborative Cooking. (Partial observability, LSTM policies)}
    
    \vspace{-1mm}
    \textbf{Training/Test partners}: c.f. Figure~1.

       \begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
            \centering
            \begin{subfigure}[b]{0.4\linewidth}
            \includegraphics[width=\linewidth]{data/cooking/layout_circuit.png}
            \vspace{0.5mm}
            \end{subfigure}
            \includegraphics[width=0.45\linewidth]{data/cooking/circuit/circuit_uniform_utility.png}
            \caption{Circuit layout.}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
            \centering
            \begin{subfigure}[b]{0.4\linewidth}
            \includegraphics[width=\linewidth]{data/cooking/layout_cramped.png}
            \vspace{0.5mm}
            \end{subfigure}
            \includegraphics[width=0.45\linewidth]{data/cooking/cramped/cramped_worst_case_utility.png}
            \caption{Circuit layout.}
    \end{subfigure}
    \vspace{-3mm}
    \caption{Average utility learning curve over the \bfemph{training} scenarios.}
    \end{figure}

    \vspace{-6mm}
    
    \begin{table}[t]
    %\footnotesize
    \setlength{\tabcolsep}{16pt}
    \centering
    \caption{Scores aggregated over the two kitchen layouts (3 runs).}
    \vspace{-4mm}
    \label{tab:cooking.train}
    \begin{tabular}{l||ccc||ccc||}%\toprule
    & \multicolumn{3}{c||}{$\scenarioset(\poptrain)$} & \multicolumn{3}{|c||}{$\scenarioset(\poptest)$} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
               & $\perf$ & $\wcu$ & $\wcr$ & $\perf$ & $\wcu$ & $\wcr$ \\ \midrule
        MU & $\mathbf{266.9 {\scriptstyle \pm 4.3}}$ & $\mathbf{225.3 {\scriptstyle\pm 11.5}}$ & $266.0 {\scriptstyle\pm 7.9}$ & $\mathbf{195.7 {\scriptstyle\pm 6.2}}$ & $\mathbf{66.0 {\scriptstyle\pm 6.8}}$ & $266.4 {\scriptstyle\pm 10.1}$ \\
        MR & $232.0 {\scriptstyle\pm 18.6}$ & $144.3 {\scriptstyle\pm 14.4}$ & $\mathbf{230.7 {\scriptstyle\pm 28.2}}$ & $172.2 {\scriptstyle\pm 15.4}$ & $65.1 {\scriptstyle\pm 16.0}$ & $\mathbf{248.2 {\scriptstyle\pm 28.4}}$ \\ \hdashline
        PBR & $209.7 {\scriptstyle\pm 23.9}$ & $96.8 {\scriptstyle\pm 13.4}$ & $357.6 {\scriptstyle\pm 16.1}$ & $151.4 {\scriptstyle\pm 19.5}$ & $33.6 {\scriptstyle\pm 5.9}$ & $327.1 {\scriptstyle\pm 14.0}$ \\
        FP & $129.9 {\scriptstyle\pm 13.9}$ & $0.2 {\scriptstyle\pm 0.1}$ & $483.5 {\scriptstyle\pm 16.1}$ & $152.2 {\scriptstyle\pm 16.8}$ & $16.7 {\scriptstyle\pm 11.7}$ & $369.7 {\scriptstyle\pm 19.7}$ \\
        SP & $124.8 {\scriptstyle\pm 26.4}$ & $15.7 {\scriptstyle\pm 10.5}$ & $460.8 {\scriptstyle\pm 21.7}$ & $117.4 {\scriptstyle\pm 12.5}$ & $6.7 {\scriptstyle\pm 3.5}$ & $367.5 {\scriptstyle\pm 11.6}$ \\
        Random & $42.8 {\scriptstyle\pm 0.0}$ & $0.0 {\scriptstyle\pm 0.0}$ & $505.4 {\scriptstyle\pm 0.0}$ & $32.2 {\scriptstyle\pm 0.0}$ & $0.0 {\scriptstyle\pm 0.0}$ & $445.0 {\scriptstyle\pm 0.0}$ \\ 
    %\bottomrule
    \end{tabular}
    \end{table}
    
\end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}


\end{document}
