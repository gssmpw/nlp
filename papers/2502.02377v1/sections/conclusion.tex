\section{Conclusion}

We explored how to compute robust adaptive policies for Ad Hoc Teamwork. Building on Minimax-Bayes RL, we introduced a method to identify minimax distributions over background populations, which consistently yielded more robust policies compared to training with a uniform distribution. Surprisingly, we also found that training on minimax distributions can significantly accelerate learning.
However, utilising regret as an objective to tune the training distribution proved computationally expensive when best-response utilities are not readily available. In some special instances, we also found that the minimax-Bayes training approach w.r.t.\ utility can prevent policies from learning certain game dynamics.  

Looking forward, we see great potential in extending our approach to a curriculum learning framework based on partner distributions, which could dramatically improve sample efficiency, asymptotic performance, and AHT robustness. 

% --- 

% We investigated how to obtain robust adaptive policies in an AHT setting. Leveraging previous work on Minimax-Bayes RL, we propose a method to find worst-case distributions over background populations, which led to consistently robust policies compared to simply training policies using a uniform distribution over the background population. On one hand, we remarked that exploiting regret is computationally demanding when best-response utilities are not readily available, and observed that worst-case distributions can prevent policies from learning certain aspects of the game. On the other hand, we have found the unexpected results that these distributional choices can significantly accelerate learning, on top of improved AHT robustness and performance. 

% Looking ahead, we see strong potential in extending our approach to a curriculum-learning framework based on partner distributions. This approach could dramatically enhance sample efficiency, asymptotic performance, and robustness.

% \input{sections/limitations}
%\input{sections/future}