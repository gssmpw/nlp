\section{Achieving Robust AHT}
\label{seq:robust_aht}



% Motivation : Learning against a uniform distribution only guarantees best performance on that exact distribution of scenarios.

To learn a policy able to cooperate with new partners, a straightforward idea is to reconstruct scenarios that would be encountered in nature. A roadblock to this approach however is that it requires two main ingredients: a) a diverse pool of partners, and b) a prior distribution over them. The prior, often neglected, is important as it captures our uncertainty about the true partners observed in nature.

In Section~\ref{subsec:constructing_training_scenarios}, we reflect on motivating previous work on diverse behaviour generation, before describing our own adopted approach. Section~\ref{subsec:minimax_bayes_aht} then introduces the Minimax-Bayes idea to AHT, by stating the connections of our setting to Minimax-Bayes Reinforcement Learning (MBRL).

%For now, we assume we have access to such a pretrained background population. The training of a background population is a key part of the whole learning process, it will be considered in later stages of this work.
\subsection{Constructing Training Scenarios}
\label{subsec:constructing_training_scenarios}

% learning against mixture of policies is not the same as learning against a distribution of policies.

%To illustrate with an example, in a setting where company coworkers have to realise a project, there might be workers that have a high preference for their own contribution (with a better chance to get promoted later), while there may be others that are inclined to delegate their work for things they are unsure about to the team. 
Before learning any robust policy, we need to construct a diverse set of scenarios. A background population that encompasses a wide range of behaviours is needed in order to reconstruct scenarios existing in nature. Previous work on AHT tackled the issue in various manners, such as using genetic algorithms \citep{muglich_generalized_beliefs_cooperative_2022}, rule-based policies generated with MAP Elites \citep{canaan_generating_adapting_diverse_2023}, SP policies \citep{strouse_collaboration_with_humans_2021}, explicit behavior diversification through regularisation \citep{lupu_trajectory_diversity_zero_2021}, or through evolved pseudo-rewards \citep{jaderberg_human_level_performance_2019}. Based on real-life examples and aiming to thoroughly assess the effects of partner priors, we adopt the following approach:
\begin{itemize}[leftmargin=12pt]
    \item We assume a class of reward functions for background policies:
    \begin{equation*}
        \rho_\text{social+risk} (s, \mathbf{a}, i) = \rho_\text{social}^+ (s, \mathbf{a}, i) - \delta_i \rho_\text{social}^- (s, \mathbf{a}, i),
    \end{equation*}
    with $\rewardfunc_\text{social}$ defined as
    \begin{equation*}
        \rewardfunc_\text{social}(s, \mathbf{a}, i) = \lambda_i \rho(s, \mathbf{a}, i) + (1-\lambda_i) \sum_{j=1}^m \rho(s, \mathbf{a}, j),
    \end{equation*}
    where $\rho^+$ and $\rho^-$ are the positive and negative parts of $\rho$, and $\lambda_i$ and $\delta_i$ denoting levels of prosociality \citep{peysakhovich_prosocial_learning_agents_2017} and risk-aversion, respectively. In other words, each background policy has their own preferences ($\lambda_k, \delta_k$).
    %Combining values of prosociality and risk-aversion allows for the consideration of behaviours with a wide range of preferences.
    \item Policies are organised into sub-populations $\bgpop = \bigcup_k \bgpop_k$ of varying sizes.
    \item Each sub-populations are separately trained using PP.
\end{itemize}
Given the diverse preferences and varying sizes of the sub-populations, distinct habits and established conventions are more likely to emerge from each group \cite{strouse_collaboration_with_humans_2021}. This choice for constructing scenarios ensures a diverse generation of scenarios, important to ablate the effects of various scenario priors on AHT robustness.
Note that this choice for constructing scenarios remains arbitrary and is not the main focus of our work.
%We could also introduce policies of different skill levels, i.e. fully trained or early stage policies, but decide to k


\input{figures/framework}

\subsection{Minimax-Bayes AHT}
\label{subsec:minimax_bayes_aht}

In the standard single-agent Bayesian RL setting, the learner selects a subjective belief $\beta$ over candidate Markov Decision Processes (MDPs) $\mathcal{M}$ for the unknown, true environment $\mdp^* \in \mathcal{M}$. The learner's objective is to maximise its expected expected utility with respect to the chosen prior $U(\pi, \beta) = \int_\mathcal{M} U(\pi, \mdp) \diff \beta(\mdp)$, i.e. finding the Bayes-optimal policy. In MBRL, \citet{buening_minimax_bayes_reinforcement_2023} proposed considering the worst possible prior for the agent, without knowledge of the policy that will be chosen. This approach can be interpreted as nature playing the minimising player against the policy learner in a simultaneous-move zero-sum normal-form game. Learning against a worst-case prior intuitively makes the policy more robust, as it prepares for the worst outcomes.

To transfer this idea to our setting, we remark that any finite background population $\bgpop$ provides a finite set of POMGs $\mathcal{M}_\bgpop = \{\mdp(\scenario) | \scenario \in \scenarioset(\bgpop)\}$. The principal difference here is the use of POMGs rather than MDPs. We extend the notion of expected utility with respect to a prior over scenarios, i.e. when $\beta \in \Delta(\scenarioset(\bgpop))$:
\begin{equation}
    U(\pi, \beta) \defeq \mathbb{E}_{\scenario \sim \beta}[U(\pi, \scenario)] = \sum_{\scenario} U(\pi, \scenario) \beta(\scenario).
\end{equation}
This allows us to formulate the following maximin game:\footnote{
If we have a subjective prior, we could learn the distribution within an $\epsilon$-ball around that prior \citep{li_bayes_optimal_robust_2024}. We however consider the full simplex for simplicity.
}
\begin{equation}
    \label{eq:mbmarl.maximin}
    \max_{\pi \in \policies} \min_{\beta \in \Delta(\scenarioset(\bgpop))} U(\pi, \beta).
\end{equation}
Similarly to \citet{buening_minimax_bayes_reinforcement_2023}, we are interested in knowing whether such a game has a solution (i.e., a value), assuming that nature and the agent play simultaneously without knowledge of each other's move. This is relevant in our setting because the policy learner does not know the true distribution of partners available in nature, while nature's distribution does not depend on the policy that will be picked. Fortunately, \eqref{eq:mbmarl.maximin} has a value when $\bgpop$ is finite.
\begin{corollary}[\citet{buening_minimax_bayes_reinforcement_2023}]
For an $m$-player POMG $\mdp$ in a finite state-action space, with a known reward function and a finite horizon, and a background population $\bgpop$, the maximin game \eqref{eq:mbmarl.maximin} has a value:
\begin{equation}
    \label{eq:maximin_value}
    \max_{\pi \in \policies} \min_{\beta \in \Delta(\scenarioset(\bgpop))} U(\pi, \beta) = \min_{\beta \in \Delta(\scenarioset(\bgpop))} \max_{\pi \in \policies} U(\pi, \beta).
\end{equation}
\end{corollary}
\begin{proof}
First, observe that for any stochastic policy $\pi \in \policies$, there exists a distribution over deterministic policies $\phi \in \Delta(\deterministicpolicies)$ such that $\pi(a_t|h_t) = \sum_{d \in \deterministicpolicies} d(a_t|h_t) \phi(d)$. Consequently, we can rewrite the utility as $U(\pi, \beta) = \sum_{d \in \deterministicpolicies} \sum_{\sigma \in \scenarioset(\bgpop)} U(d, \sigma) \phi(d) \beta(\sigma)$. This demonstrates that $U$ is bilinear in $\phi$ and $\beta$, which allows us to apply the minimax theorem, thus proving the result.
\end{proof}
Importantly, prior work that chooses arbitrarily a fixed prior is limited in terms of robustness guarantees: it only ensures maximal utility for their specific prior. In contrast, a policy $\pi^*_U$ solving the maximin utility problem \eqref{eq:mbmarl.maximin} has its utility lower-bounded on $\scenarioset(\bgpop)$:
\begin{equation}
    \label{eq:utility_lower_bound}
    \forall \beta \in \Delta(\scenarioset(\bgpop)), \quad U(\pi^*_U, \beta) \geq U(\pi^*_U, \beta^*_U),
\end{equation}
where $\beta^*_U$ is the worst-case prior for $\pi^*_U$.
Simply put, $\pi^*_U$ performs the worst when the prior is its worst-case $\beta^*_U$, but can only improve when the prior deviates from $\beta^*_U$. Additionally, it is also optimal on the worst-case prior:
\begin{equation}
\label{eq:best_on_worst_case_prior}
    \forall \pi\in\policies, \quad U(\pi^*_U, \beta^*_U) \geq U(\pi, \beta^*_U).
\end{equation}
Note that this differs fundamentally from merely finding the best response to a fixed worst-case prior $\arg\max_{\pi} U(\pi, \beta^*_U)$, which once again, only has a guaranteed optimal utility on $\beta^*_U$.
\begin{corollary}[\citet{buening_minimax_bayes_reinforcement_2023}]
    \label{corollary:min_dirac}
    For any policy $\pi\in\policies$ and background population $\bgpop \subset \policies$, we have 
    \begin{equation}
        \min_{\beta\in\Delta(\scenarioset(\bgpop))} U(\pi, \beta)= \wcu(\pi, \scenarioset(\bgpop)). 
    \end{equation}
\end{corollary}
\begin{proof}
    This follows directly from the results of \citet{buening_minimax_bayes_reinforcement_2023}, using utility in place of regret and recognising that Dirac distributions associated with scenarios in $\scenarioset(\bgpop)$ are always contained in $\Delta(\scenarioset(\bgpop))$.
\end{proof}
\begin{lemma}
    For any background population $\bgpop \subset \policies$ and $\pi^*_U$ the policy solving the maximin utility game~\eqref{eq:mbmarl.maximin}, we have
    \begin{equation}
    \label{eq:optimal_worst_case_utility}
    \wcu(\pi^*_U, \scenarioset(\bgpop)) = \max_{\pi\in\policies} \wcu(\pi, \scenarioset(\bgpop)).
    \end{equation}
\end{lemma}
\begin{proof}
    By Corollary~\ref{corollary:min_dirac}, we can write that $\max_{\pi}\min_{\beta} U(\pi, \beta)=\max_{\pi}\wcu(\pi, \scenarioset(\bgpop))$.
    However, we also have $\max_{\pi}\min_{\beta} U(\pi, \beta) = \min_{\beta} U(\pi^*_U, \beta) = \wcu(\pi^*_U,\scenarioset(\bgpop))$.
\end{proof}
Thus, a policy solving the maximin utility game~\eqref{eq:mbmarl.maximin} is guaranteed to have an optimal worst-case utility on its training set.


% https://www.epfl.ch/labs/lia/wp-content/uploads/2023/01/multi-mdp.pdf

% Results
% -> 