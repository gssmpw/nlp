\section*{Appendix}

\section{Algorithms}
\label{appendix:algorithms}
We provide an implementation for solving the maximin utility/minimax regret problems when the game is unknown, in Algorithm~\ref{algorithm:unknown_mdp_gda}.

\input{algorithms/sgda}

\section{Ommitted proofs}

\begin{proof}[Proof of Lemma~\ref{lemma:scenario_equivalence}]
Let $\scenario$ be a scenario in $S$. $\scenarioset$ is an $\epsilon$ net for $S$, then there must exist a scenario $\scenario'$ such that $d(\scenario, \scenario')<\epsilon$. 
This implies for the induced transition function that for any $\afocal, s$:
        \begin{align}
            \sum_{s'}\big|P_\scenario(s'| s, \afocal) - P_{\scenario'}(s'| s, \afocal) \big| & \leq \sum_{s'}\sum_{\abackground} P(s'| s, \afocal, \abackground) \big|\prod_i{\bpi}_i^b((\abackground_i \mid h_i)-\prod_i{\bpi'}_i^b(\abackground_i \mid h_i) \big| \\
            &\leq \sum_{\abackground} \sum_{s'} P(s'| s, \afocal, \abackground) \sum_i \big|{\bpi}_i^b(\abackground_i \mid h_i) - {\bpi'}_i^b(\abackground_i \mid h_i) \big|  \\
            &= \sum_{\abackground} \sum_i \big|{\bpi}_i^b((\abackground_i \mid h_i) - {\bpi'}_i^b(\abackground_i \mid h_i) \big|\sum_{s'} P(s'| s, \afocal, \abackground) \\
            &= \sum_{\abackground} \sum_i \big|{\bpi}_i^b((\abackground_i \mid h_i) - {\bpi'}_i^b(\abackground_i \mid h_i) \big| \\
            & < \epsilon.
            %&\leq  (m-c)\epsilon \sum_{s'}\sum_{\abackground} P(s'| s, \afocal, \abackground) = (m-c)\epsilon |\actions|^{(m-c)}
        \end{align}
Similarly with the induced reward function, we also have for any $\afocal, s,  i$: 
        \begin{align}
            \label{eq:reward_similarity}
            \big|\rewardfunc_\scenario(s, \afocal, i) - \rewardfunc_{\scenario'}(s, \afocal, i) \big| & \leq \sum_{\afocal} |\rewardfunc(s, \afocal, \abackground, i)| \sum_j \big| {\bpi}_j^b((\abackground_j \mid h_j)-{\bpi'}_j^b((\abackground_j \mid h_j) \big| \\ 
            & \leq \rmax \sum_{\abackground} \sum_j \big| {\bpi}_j^b((\abackground_j \mid h_j)-{\bpi'}_j^b((\abackground_j \mid h_j) \big| \\
            & < \epsilon\rmax,
        \end{align}
        where $\rmax$ denotes the maximal absolute step reward.

        We use the notation $U_t(\pi, \scenario, s)$ to denote the utility of a policy $\pi$ when $s_t=s$, with $T-t$ timesteps remaining until the episode terminates. We will prove by induction that for any $t \in [1\dots T]$ and any $s$:
        \begin{equation}
             |U_t(\pi, \scenario, s)-U_t(\pi, \scenario', s)| < \epsilon\rmax (T-t+1)(T-t)/2.
        \end{equation}
        At $t = T$, we have:
        \begin{align}
         & |U_T(\pi, \sigma, s) - U_T(\pi, \sigma', s)| \\
         &= |\sum_{\afocal} \pi(\afocal, h_T) \dfrac{1}{c}\sum_i^c\big[\rewardfunc_\scenario(s, \afocal, i) - \rewardfunc_{\scenario'}(s, \afocal, i)\big]| \\
         & < \epsilon\rmax.
        \end{align}
        For some $t \in [1\dots T]$, assume that the inductive hypothesis holds at $t+1$: $|U_{t+1}(\pi, \scenario, s)-U_{t+1}(\pi, \scenario', s)| \leq \epsilon\rmax(T-t)(T-t-1)$. We have:
        \begin{align}
         |U_t(\pi, \sigma, s) - U_t(\pi, \sigma', s)| & \leq \sum_{\afocal} \pi(\afocal, h_t)
         (\dfrac{1}{c}\sum_i^c \big| \rewardfunc_\sigma(s, \afocal, i) - \rewardfunc_{\sigma'}(s, \afocal, i)\big|) \\
         & + \sum_{s'\in \states} \big|P_\scenario(s'|\afocal, s) U_{t+1}(\pi, \scenario, s') - P_{\scenario'}(s'|\afocal, s) U_{t+1}(\pi, \scenario', s') \big| \\
         & < \sum_{\afocal} \pi(\afocal, h_t) \Big[\epsilon\rmax \\
         &+ \sum_{s'\in \states} | P_\scenario(s'|\afocal, s) - P_{\scenario'}(s'|\afocal, s) | |U_{t+1}(\pi, \scenario, s')| \\
         &+ \sum_{s'\in \states}P_{\scenario'}(s'|\afocal, s)|U_{t+1}(\pi, \scenario, s') - U_{t+1}(\pi, \scenario', s')| \Big] \\
         & < \sum_{\afocal} \pi(\afocal, h_t) \epsilon\rmax\Big[1 + (T-t) + \dfrac{1}{2}(T-t)(T-t-1) \Big] \\
         & = \dfrac{1}{2}\epsilon\rmax(T-t+1)(T-t).
        \end{align}
        This proves by induction that for any $t \in [1\dots T]$ and $s$, $|U_t(\pi, \scenario, s)-U_t(\pi, \scenario', s)| < \epsilon\rmax (T-t+1)(T-t)/2$.
        The stated result is recovered with $t=1$.
    \end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:scenario_equivalence_regret}]
    Let $\scenario$ be a scenario from $S$. $\scenarioset$ is an $\epsilon$-net for $S$, therefore there is a scenario $\scenario'\in\scenarioset$ such that $d(\scenario, \scenario') < \epsilon$.
    We begin by writing that for any policy $\pi\in\policies$, $U(\pi, \scenario) \leq |U(\pi, \scenario) - U(\pi, \scenario')| +  U(\pi, \scenario')$. Taking the maximum over policies on both sides, we get
    \begin{align}
        U^*(\scenario) &\leq \max_\pi |U(\pi, \scenario) - U(\pi, \scenario')| +  U(\pi, \scenario') \\
        & \leq \max_\pi |U(\pi, \scenario) - U(\pi, \scenario')| + U^*(\scenario')\\
        & \leq \dfrac{\epsilon T^2\rmax}{2} + U^*(\scenario'),
    \end{align}
    where the last inequality is obtained through Lemma~\ref{lemma:scenario_equivalence}.
    Hence moving $U^*(\scenario')$ to the left hand side, we get $U^*(\scenario)- U^*(\scenario') \leq \epsilon T^2\rmax/2 $. As the above logic also holds by swapping the roles of $\scenario$ and $\scenario'$, we can further note that $|U^*(\scenario)- U^*(\scenario')| \leq \epsilon T^2\rmax/2 $.
    Finally, by the definition of regret, it holds for any policy $\pi\in\policies$ that
    \begin{align}
        |R(\pi, \scenario)-R(\pi, \scenario')| &\leq |U^*(\scenario)- U^*(\scenario')| + |U(\pi, \scenario) - U(\pi, \scenario')| \\
        & < \epsilon T^2\rmax,
    \end{align}
    which concludes the proof.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:wcu.guarantees}]
     Let $\scenario_\text{wc}(\scenarioset)$ and $\scenario_\text{wc}(S)$ be the worst-case scenarios for $\pi^*_U$ on $\scenarioset$ and $S$, respectively: $\wcu(\pi^*_U, \scenarioset) = U(\pi^*_U, \scenario_\text{wc}(\scenarioset))$ and $\wcu(\pi^*_U, S) = U(\pi^*_U, \scenario_\text{wc}(S))$. Because $\pi^*_U$ is the solution of the maximin utility problem \eqref{eq:mbmarl.maximin} on $\scenarioset$, we can also write that $U(\pi^*_U, \scenario_\text{wc}(\scenarioset)) = \max_\pi \wcu(\pi, \scenarioset)$. Now, because $\scenarioset$ is an $\epsilon$-net for $S$, we have two cases: 
    \begin{enumerate}
        \item $d(\scenario_\text{wc}(\scenarioset), \scenario_\text{wc}(S)) < \epsilon$. Lemma~\ref{lemma:scenario_equivalence} can be applied, obtaining $U(\pi^*_U, \scenario_\text{wc}(S)) > U(\pi^*_U, \scenario_\text{wc}(\scenarioset)) - \epsilon T^2 \rmax/2$.
        
        \item $d(\scenario_\text{wc}(\scenarioset), \scenario_\text{wc}(S)) \geq \epsilon$, meaning that there is another scenario $\scenario_\epsilon \in \scenarioset$ such that  $d(\scenario_\epsilon, \scenario_\text{wc}(S)) < \epsilon$.
        Since we have $U(\pi^*_U, \scenario_\text{wc}(S)) > U(\pi^*_U,\scenario_\epsilon) - \epsilon T^2 \rmax/2$ through Lemma~\ref{lemma:scenario_equivalence}, and because of the definition of $\scenario_\text{wc}(\scenarioset)$ we have $U(\pi^*_U, \scenario_\epsilon)\geq U(\pi^*_U, \scenario_\text{wc}(\scenarioset))$, the desired result is obtained by transitivity:
        \begin{align}
            \wcu(\pi^*_U, S) &= U(\pi^*_U, \scenario_\text{wc}(S))\\
            &> U(\pi^*_U,\scenario_\epsilon) - \dfrac{\epsilon T^2\rmax}{2}\\
            & \geq U(\pi^*_U, \scenario_\text{wc}(\scenarioset)) - \dfrac{\epsilon T^2\rmax}{2}\\
            & = \max_\pi \wcu(\pi, \scenarioset) - \dfrac{\epsilon T^2\rmax}{2}.
        \end{align}
    \end{enumerate}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:wcr.guarantees}]
         Let $\scenario_\text{wc}(\scenarioset)$ and $\scenario_\text{wc}(S)$ be the worst-case scenarios for $\pi^*_R$ on $\scenarioset$ and $S$, respectively: $\wcr(\pi^*_R, \scenarioset) = R(\pi^*_R, \scenario_\text{wc}(\scenarioset))$ and $\wcr(\pi^*_R, S) = R(\pi^*_R, \scenario_\text{wc}(S))$. Because $\pi^*_R$ is the solution of the minimax regret problem \eqref{eq:mbmarl.minimax} on $\scenarioset$, we can also write that $R(\pi^*_R, \scenario_\text{wc}(\scenarioset)) = \min_\pi \wcr(\pi, \scenarioset)$. Now, because $\scenarioset$ is an $\epsilon$-net for $S$, we have two cases: 
    \begin{enumerate}
        \item $d(\scenario_\text{wc}(\scenarioset), \scenario_\text{wc}(S)) < \epsilon$. Lemma~\ref{lemma:scenario_equivalence_regret} can be applied, obtaining $R(\pi^*_R, \scenario_\text{wc}(S)) > R(\pi^*_R, \scenario_\text{wc}(\scenarioset)) + \epsilon T^2 \rmax$.
        
        \item $d(\scenario_\text{wc}(\scenarioset), \scenario_\text{wc}(S)) \geq \epsilon$, meaning that there is another scenario $\scenario_\epsilon \in \scenarioset$ such that  $d(\scenario_\epsilon, \scenario_\text{wc}(S)) < \epsilon$.
        Since we have $R(\pi^*_R, \scenario_\text{wc}(S)) > R(\pi^*_R,\scenario_\epsilon) + \epsilon T^2 \rmax$ through Lemma~\ref{lemma:scenario_equivalence_regret}, and because of the definition of $\scenario_\text{wc}(\scenarioset)$ we have $R(\pi^*_R, \scenario_\epsilon)\leq R(\pi^*_R, \scenario_\text{wc}(\scenarioset))$, the desired result is obtained by transitivity:
        \begin{align}
            \wcr(\pi^*_R, S) &= R(\pi^*_R, \scenario_\text{wc}(S))\\
            &< R(\pi^*_R,\scenario_\epsilon) + \epsilon T^2\rmax\\
            & \leq R(\pi^*_R, \scenario_\text{wc}(\scenarioset)) + \epsilon T^2\rmax\\
            & = \min_\pi \wcr(\pi, \scenarioset) + \epsilon T^2\rmax.
        \end{align}
    \end{enumerate}
\end{proof}


\section{Additional experimental results}

\paragraph{Repeated Prisoner's Dilemma}
In this section of the experiments, we utilised nine specific ad-hoc policies to create the set of training partners $\poptrain$. The set was constructed as follows:
\begin{itemize}
    \item A pure cooperative policy: always chooses to cooperate.
    \item A pure defecting policy: always chooses to defect.
    \item Two tit-for-tat policies: \begin{itemize}
        \item One that begins with cooperation and then mimics the opponent's last action in subsequent rounds.
        \item Another that starts with defection and also replicates the opponent's last action thereafter.
    \end{itemize}
    \item Two tat-for-tit policies: \begin{itemize}
        \item One that starts by cooperating and subsequently takes the opposite action of the opponent's last move.
        \item Another that begins by defecting and then does the same.
    \end{itemize}
    \item A cooperate-until-defected policy: cooperates until it encounters a defection from its opponent.
    \item A defect-until-cooperated policy: defects until it observes cooperation from its opponent.
    \item A fully random policy: selects actions randomly, without any strategy.
\end{itemize}
We provide a plot of the performance/robustness metrics of the learned policies in function of how novel the test scenarios are in Figure~\ref{fig:prisoners.epsilon_plot}. As test scenarios become more distinct from the training ones, PBR experiences the largest performance drop. In comparison, MR's worst-case regret shows only a slight increase, and MU maintains consistent worst-case utility. There reason for MU's policy constant worst-case utility/regret is that its worst-case scenario in both cases is the universalisation scenario (where it plays against itself) for all values of $\epsilon$. No matter how different are the test policies from the training policies, one policy's utility/regret on the universalisation scenario won't change (there is no background player in this scenario).
\input{sections/epsilon_plots}

\paragraph{Robust AHT on Collaborative Cooking}

We plotted the learning curves of distributions when trying to solve the maximin utility/minimax regret problems with Algorithm~\ref{algorithm:unknown_mdp_gda} on Collaborative Cooking, in Figure~\ref{fig:cooking.learning_curves_prior}. This displays the clear differences between distributions learned by MU and MR on both kitchen layouts. The low variance between runs also highlights the consistency of the stochastic GDA Algorithm~\ref{algorithm:unknown_mdp_gda}. Note that the plots are highly smoothed, as the scenario distribution fluctuate a lot more than the policy.
\input{data/cooking/distributions}
    

\section{Additional experimental details}

\paragraph{Robust AHT on Collaborative Cooking}
To facilitate the training of our policies in Collaborative Cooking, we used a shaping pseudo-reward of $1$ when tomatoes were placed in the cooking pot. For the background policies, we further altered the reward function to restrict delivery rewards to the player that delivered. Combining this new reward function with varying levels of prosociality and risk-aversion helped the background policies adopt diversified ways to solve the game.

The architecture for the agents consisted of a convolutional network with two layers, having 16 and 32 output channels, kernel shapes of 8 and 4, and strides of 8 and 1, respectively. The output of the convNet was concatenated with the previous action taken before being passed into a dense layer of size 256 and an LSTM with 256 units. Policy and baseline (for the critic) were produced by linear layers connected to the output of the LSTM. Notably, to increase stability, we made the critic output a separate value for each training scenario.

We chose PPO to train our policies, using the Adam optimiser with a learning rate of $2 \times 10^{-4}$, a discount factor of $0.99$, a GAE lambda of $0.95$, a KL coefficient of $1.0$ with a KL target of $0.01$, and a PPO clipping parameter of $0.3$. Gradients were clipped at 4.0. We did not employ entropy regularisation. PPO was set to run 2 epochs per batch, each containing $64000$ samples, with minibatches of $1000$ samples each. Finally, the unroll length for the LSTM was set at $20$.

For the prior, we used a learning rate of $0.4$. 

\paragraph{Implementation}
The code used for all of our experiments is available at \hyperlink{https://github.com/AkiEl9/minimax_bayes_aht}{\url{https://github.com/AkiEl9/minimax_bayes_aht}}.