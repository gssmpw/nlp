\section{Experiments}
\label{seq:experiments}
% What should we chose for baselines ?
% PSRO ? adversarial
% Fictitious Play -> OK
% Fictitious Co-Play -> OK (similar to our uniform baseline)
% Other-play ? -> potential
The aim of our experiments is to highlight the importance of partner distribution in the learning process. To achieve this, we evaluate our proposed strategies, Maximin Utility (MU) and Minimax Regret (MR), on two distinct problems. First, we consider the fully known and observable Iterated Prisoner's Dilemma to validate the theoretical results. Following this, we test our approaches on a deep-RL task, the Collaborative Cooking (Overcooked) game \citep{carroll_utility_learning_about_2019, strouse_collaboration_with_humans_2021, leibo_scalable_evaluation_multi_2021,agapiou_melting_pot_2_2023}.
We remind that we want to optimise policies with respect to the focal-per-capita return \eqref{eq:EU} rather than individual returns. Naturally, across all of our experiments, we reward focal policies with average focal step rewards. For each scenario $\scenario$, this is defined as $
    \rho^\text{train}_\sigma(s, \afocal, i) = \frac{1}{c} \sum_{j=1}^c \rho_\sigma(s, \afocal, j)
$.
Throughout our experiments, we benchmark against three distribution management strategies: 
\begin{itemize}[leftmargin=12pt]
    \item Population Best Response (PBR): uniform distribution over scenarios,
    \item Self-Play (SP): playing solely with copies of the focal policy,
    \item Fictitious Play (FP): sampling partners uniformly from the history of focal policies $\pi_0, \dots, \pi_t$.
\end{itemize}

%SP which fixes the prior as the Dirac distribution $\beta_\text{SP}(\scenario^\text{SP}) = 1$, FP which is similar to SP but has the versions of the copies sampled uniformly from the full history of policies $\pi_0, \dots, \pi_t$, and Population Best-Response (Population best response) which learns the best response to the training background population by maintaining a uniform prior $\beta_\text{Population best response} = \mathcal{U}(\scenarioset(\poptrain))$.

%Approaches are consistently evaluated on their training background population, as well as on separate test sets, in order to evaluate their AHT capabilities.

\subsection{Iterated Prisoner's Dilemma}
\label{subsec:prisoners}
In these experiments, all computations can be exact. This includes the gradient calculation for the prior, as well as for the agent's policies. We focus on the Iterated Prisoner's Dilemma, where two players play the matrix game (Table~\ref{tab:prisoners_matrix}) repeatedly for $\horizon=3$ rounds. 
\paragraph{Experimental Setup.}
In the Iterated Prisoner's Dilemma, players receive and observe rewards based on their chosen actions, specified by the payoff matrix. The game has one state, and the outcomes observed are enough to determine the joint actions, making it fully observable. We learn softmax, fully adaptive policies, where actions depend on the entire history of observations and actions.
Unlike other experiments, we do not use the approach described in Section~\ref{subsec:constructing_training_scenarios}. Instead, the learner interacts with a background population $\poptrain$ composed of nine ad-hoc policies, such as pure cooperation/defection, tit-for-tat, cooperate-until-defected, and fully random.
For AHT assessment, we adequately construct a test background population $\poptest$, ensuring that its scenarios are $\epsilon$-close to those in the training phase. Specifically, we uniformly sample $512$ stochastic policies that are $\epsilon$-close to the policies in the training background population, setting $\epsilon = 0.5$.

\begin{table}[t]
    \centering
    \caption{Payoff matrix of the Prisoner's Dilemma.}
    \label{tab:prisoners_matrix}
    \begin{tabular}{c|c:c}
         & Cooperate  & Defect \\
         \hline
        Cooperate & $(4, 4)$ & $(0, 5)$\\
        \hdashline
        Defect  & $(5, 0)$ & $(1, 1)$\\
    \end{tabular}
\end{table}
\input{data/prisoners/table_t3}


\paragraph{Results.} 
Table~\ref{tab:prisoner.train} summarises the performance on both training and test sets. As expected, on the training set, PBR performs the best under the uniform prior ($\perf$), MU has the highest worst-case utility ($\wcu$), and MR exhibits the lowest worst-case regret ($\wcr$). On the test set however, MU outperforms PBR in terms of average utility and maintains the highest worst-case utility, while MR continues to excel in minimising worst-case regret. These results indicate that best responses to populations does not ensure
the best robustness to new partners. SP and FP agents as well, overfit to their established conventions, leading to poor transferability across training and test policies.
Figure~\ref{fig:prisoners.binary_trees} visualises the learned policies under different distribution regimes, highlighting their disparity. For example, the population best response is a strategy close to "cooperate-until-defected", while MU's policy heavily favors defection. 
Crucially, this figure points to a potential improvement for future work: during optimisation, the worst-case distribution can force policies onto a narrow subset of scenarios, leaving others unexplored. Due to the MU regime, the policy is forced to face a pure defecting opponent, its worst-case scenario, entirely cutting its exposure to cooperative strategies. As seen in Figure~\ref{subfig:prisoners.mu}, the policy does not know what to do if the opponent chooses to collaborate. This suggests that it is possible to have a policy with equal worst-case utility but improved worst-case regret and overall performance.

\input{data/prisoners/binary_trees}


\subsection{Robust AHT on Collaborative Cooking}
\label{subsec:cooking}
For this section, we tackle the Collaborative Cooking game \citep{agapiou_melting_pot_2_2023}, where two players act as chefs in a gridworld kitchen, working together to deliver as many tomato soup dishes as possible within a set time. Each have to collect tomatoes, cook them, prepare dishes, and deliver the soup. Successful deliveries reward both players equally.\footnote{When training background policies specifically, we only assign a reward of 20 points to the player who delivers the dish, and 1 point to players who contribute by placing a tomato into the pot. This incentivises diverse behaviour generation.} Players must navigate the kitchen, interact with objects in the right order, and coordinate with each other. Each player has a local, partial RGB view of the environment. All of our policies in this section are using deep recurrent (LSTM) neural networks.

\input{data/cooking/environment_description}
\input{data/cooking/table}
\input{data/cooking/metrics}


\paragraph{Experimental Setup.} Two separate background populations, $\poptrain$ and $\poptest$, are generated according to Section~\ref{subsec:constructing_training_scenarios}. Both populations are trained with an identical setup, differing only in their seed. Each is partitioned into four sub-populations of sizes 2, 3, and 5, totaling 10 policies. Prosociality and risk-aversion for all background policies are sampled uniformly in $[-0.2, 1.2]$ and $[0.1,2]$ respectively.\footnote{Those intervals were chosen empirically to ensure diverse and cooperative policies.}
For a fair comparison and to focus on scenario distribution learning, we assume that $\poptrain$ is readily available to all approaches, which can be exploited for a maximum of $4 \times 10^7$ environment steps to learn a policy with PPO \citep{schulman_proximal_policy_optimization_2017}. We run each training procedure on three different seeds (solely $\poptrain$ and $\poptest$ remain consistent across runs). We evaluate the approaches on two different kitchen layouts: Circuit and Cramped \citep{agapiou_melting_pot_2_2023}, which are visualised in Figure~\ref{fig:layouts}. 
On top of our own test scenarios, we assess the learned policies on the Melting Pot benchmark scenarios, comparing them against the baselines reported in the original paper \citep{agapiou_melting_pot_2_2023}: an Actor-Critic Baseline (ACB), V-MPO \citep{song_vmpo_on_policy_2019}, and OPRE \citep{vezhnevets_options_responses_grounding_2020}.\footnote{Since their baseline policies are not publicly available, we were unable to evaluate them on our scenarios, which explains the missing values in Table~\ref{tab:cooking.train}.} These baselines were trained for $10^9$ steps without access to our background policies. In each scenario (including the Melting Pot scenarios), regret was computed by estimating best responses with PPO for $10^7$ steps. Lastly for MU and MR, we run a stochastic version of Algorithm~\ref{algorithm:full_information_gda}, and constrain the learned distribution to keep sampling a random scenario with probability $0.05$, effectively allowing us to maintain utility estimates across all scenarios. 


\paragraph{Results.} The results in Table~\ref{tab:cooking.train} clearly show that MU outperforms all other evaluated methods. Looking at the robustness metrics, the utility formulation has the best worst-case utility ($\wcu$) overall and the best worst-case regret ($\wcr$) on the Melting Pot scenarios. MR also performs better than any other benchmarked method overall, securing the lowest worst-case regrets on both the training set and our own test set. In terms of average performance ($\perf$), MU and MR are consistently the best and second-best, respectively, which is particularly notable on the training set where PBR was expected to perform the best.

One possible explanation for the globally lower performance of the regret approach compared to utility is that the best responses for training scenarios are too approximate. Figure~\ref{fig:cooking.learning_curves} suggests another hypothesis for why MU and MR considerably outperform leaning population best responses and other approaches: their scenario distributions during training have a similar effect to curriculum learning, introducing indirect exploration in behaviours compared to fixed distributions, and a smoother learning curve. In fact, they appear to speed up learning. In contrast, Figure~\ref{fig:cooking:learning_curves.circuit} shows that training against a uniform distribution of scenarios fails to improve in at least one scenario on the Circuit layout, with a worst-case utility close to 0. 

%Finally, Figure~\ref{fig:cooking.learning_curves_prior} highlights the consistency of the stochastic GDA algorithm, with little variation in learned distributions between runs. It also displays the clear differences between distributions learned by Maximin utility and Minimax regret on both kitchen layouts.

