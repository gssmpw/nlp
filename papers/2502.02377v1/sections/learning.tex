\input{algorithms/gda}
\section{Computing Solutions}
\label{seq:learning}

We now desire to calculate the solution pairs for both the maximin utility \eqref{eq:mbmarl.maximin} and minimax regret \eqref{eq:mbmarl.minimax} games. \citet{buening_minimax_bayes_reinforcement_2023} theoretically proved that GDA has convergence guarantees when the game is played between a policy learned with softmax parameterisation and nature learning its distribution over a finite set of MDPs. These results apply if all scenarios induce single-agent POMGs, as partial observability does not interfere with proving the required properties. However, when the focal policy is deployed in a scenario with $c>1$ copies, the game is no longer single-agent. To approximate the reduction of these multi-agent POMGs to single-agent POMGs during training, we propose using delayed versions $\pi_{t-d}$ of the focal policy $\pi_{t}$ for the $c-1$ remaining copies. This common practice smooths the behavior of the copies and favours proper convergence by treating the copies as fixed policies.

Algorithm~\ref{algorithm:full_information_gda}, a GDA algorithm adapted to our setting, can be employed to learn solutions for both the maximin utility and minimax regret problems. Furthermore, in case the POMG is not known, one can straightforwardly adapt the algorithm to a stochastic version, by resorting to sampling scenarios and performing policy rollouts to estimate utility, regret, and gradients. 


