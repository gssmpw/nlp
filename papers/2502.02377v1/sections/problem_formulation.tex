\section{Problem Formulation}

% TODO : use macros

\subsection{Preliminaries}

An $m$-player Partially Observable Markov Game (POMG) is given by a tuple $\mdp = \langle \states, \observations, \actions, \obsfunc, \transfunc, \rewardfunc , \horizon \rangle$ defined on finite sets of states $\states$,
observations $\observations$ and actions $\actions$. The observation function $\obsfunc: \states \times \{1, \dots, m\} \rightarrow \observations$ provides a state space view for each player. In each state, each player $i$ chooses an action $a_i \in \actions$. Following their joint action $\mathbf{a} = (a_1, \dots, a_m) \in \actions^m$, the state is updated according to the transition function $P: \states \times \actions^m \rightarrow \Delta(\states)$. After a transition, each player receives a reward defined by $\rewardfunc: \states \times \actions^m \times \{1, \dots, m\} \rightarrow \mathbb{R}$. The game ends after $\horizon$ transitions. Permuting player indices does not have any effect on $\mdp$. We denote $\rmax$ the maximum absolute step reward.
    
A policy $\pi: \observations \times \actions \times \observations \times \actions \times \dots \times \observations \rightarrow \Delta(\actions)$ is a probability distribution over a single agent's actions, conditioned on that agent's history of observations and actions. We denote $\policies$ the set of all policies and $\deterministicpolicies \subset \policies$ the set of deterministic policies.

\subsection{Scenarios}
% Why not notation: $\bpi^b = (\pi^b_1, \dots, \pi^b_{m-c}) \in \Pi^{m-c}$ for background policies and $\bpi^f = (\pi^f, \dots, \pi^f)$ copies of policy $\pi^f$. 
Let a \emph{scenario} $\scenario=(c, \bpi^b)$ be defined by its number of \emph{focal} players $c$, and its \emph{background} players $\bpi^b = (\pi^b_1, \dots, \pi^b_{m-c}) \in \policies^{m-c}$. We say we deploy a policy $\pi^f$ in scenario $\scenario$ if the $c$ focal players are equal to $\pi^f$. Hence, in addition to the $m-c$ many background policies $\bpi^b$, there are $c$ many focal policies $\bpi^f = (\pi^f, \dots, \pi^f)$. We denote $ \afocal \in \actions^c$ and $\abackground \in \actions^{m-c}$ the joint actions of the focal and background players, respectively.
A background population $\bgpop \subset \policies$ is a finite set of policies, to which we assign a set of scenarios:\footnote{The definition of background populations in \eqref{eq:background_population} is largely inspired from the work of \citet{leibo_scalable_evaluation_multi_2021} and \citet{agapiou_melting_pot_2_2023}. This is the most general formulation of the problem and will be used as is throughout this work. Depending on the game $\mdp$, a restricted set may make more sense, such as limiting to $c=1$.}
\begin{equation}
\label{eq:background_population}
\scenarioset(\bgpop) \defeq \{(c, \bpi^b) \mid 1 \leq c \leq m, \bpi^b \in \bgpop^{m-c}\}.
\end{equation}
A scenario $\scenario$ on $\mdp$ can be viewed as its own $c$-player POMG, through the marginalisation of the policies of its background players.\footnote{Each scenario can be seen as a decentralised partially observable Markov decision process \citep{oliehoek_decentralized_pomdps_2012} constrained by the fact that the $c$ players are copies.} We denote $\mdp(\scenario) = \langle \states, \observations, \actions, \obsfunc_\scenario, \transfunc_\scenario, \rewardfunc_\scenario, \gamma ,\horizon \rangle$ the POMG induced by scenario $\scenario$, where $\obsfunc_\scenario: \states \times \{1, \dots, c\} \rightarrow \observations $ is the corresponding observation function, $P_\scenario: \states \times \actions^c \rightarrow \Delta(\states)$ is the transition function given by  
\begin{equation*}
P_{\scenario}(s' \mid s, \afocal) =
\begin{cases}
P(s' \mid s, \afocal), & c = m \\
\sum_{\abackground} \Big(P(s' \mid s, \afocal, \abackground) 
\prod_{i} \bpi^b_i (\abackground_i \mid h_i)\Big), & c < m \\
\end{cases}
\end{equation*}
and $\rho_\scenario: \states \times \actions^c \times \{1, \dots, c\} \rightarrow \mathbb{R}$ is the induced reward function with:
\begin{equation*}
\rho_\scenario(s, \afocal, i) =
\begin{cases}
\rho(s, \afocal, i), & c = m\\
\sum_{\abackground} \Big( \rho(s, \afocal, \abackground, i)
\prod_{j} \bpi^b_{j}(\abackground_j |h_j)\Big), & c < m \\
\end{cases}
\end{equation*}
where $h_j$ is the history of observations and actions of the $j$-th policy and $\abackground_j$ its action in $\abackground$. We denote the scenario that only involves the focal policy, i.e. the universalisation scenario~ \citep{leibo_scalable_evaluation_multi_2021}, by $\scenario^\text{SP} = (m, \emptyset)$. This setup is closely related to N-agent AHT \citep{wang_n_agent_ad_2024}, in that the proportion of focal/background players can vary and is not known in advance.
%A test scenario $\scenario^\text{test}(c, \mathbf{b})$ is characterized by its background players, which are uniquely drawn from a background population $\mathcal{B}^\text{test}$.

\subsection{Evaluation}
The expected utility of a policy $\pi$ in scenario $\scenario$ is the mean return of the focal policies given by the expected \emph{focal-per-capita return} \citep{leibo_scalable_evaluation_multi_2021, agapiou_melting_pot_2_2023}:
\begin{equation}
    \label{eq:EU}
    %U_t(\pi, \scenario, s) \defeq 
    U(\pi, \scenario) \defeq 
    \sum_{t=1}^T  \dfrac{1}{c} \sum_{i=1}^c \mathbb{E}^{\pi}_{\mdp(\scenario)}\left[\rho_\scenario(s_t, \afocal_t, i) \right].
\end{equation}
% Do you ever need $u(\pi, \sigma)$ ?
%where $r_{t,i}=\rho_\scenario(s_t, \afocal_t, i)$ is the reward collected by the $i$-th player at timestep $t$.
% TODO : Justify this definition for utility, how is it different from single agent rewards.
%This definition for utility rates higher policies that maximize the mean utility of its copies in the scenario, which is slightly different from maximizing one-self's return. This entails that a good policy w.r.t \eqref{eq:EU} differentiates
$U^*(\scenario) \defeq \max_{\pi\in \Pi} U(\pi, \scenario)$ denotes the maximal utility achievable in scenario $\scenario$. This definition for utility represents the need for autonomous agents to always maximise the mean joint rewards of its copies, regardless of the scenario.
We can further define the notion of regret incurred by deploying some policy $\pi$ on scenario $\scenario$, as the gap between the maximal utility and the utility of $\pi$ on $\scenario$:
\begin{equation}
    \label{eq:regret}
    R(\pi,\scenario) \defeq U^*(\scenario) - U(\pi, \scenario).
\end{equation}
%The best response utility $U^*(\scenario)$ can be obtained through an oracle that knows the scenario.
To assess a learning method in terms of AHT, we use the evaluation protocol of \citet{leibo_scalable_evaluation_multi_2021}. This has two phases:
\begin{enumerate}[leftmargin=12pt]
    \item \textbf{Training phase}: A background population $\poptest$ is kept hidden. The policy learner has access to the game $\mdp$ with no restrictions, apart from accessing $\poptest$. For example, the learner is free to use a modified instance $\mdp'$ of $\mdp$, where the observation function, $\obsfunc$, may be adjusted to include information about other players, or where the reward function, $\rewardfunc$, could be altered to provide joint rewards instead of individual ones.
    \item \textbf{Testing phase}: The obtained policy is fixed and cannot be trained any further. We compute the performance of the policy on $\mdp$ by taking its average expected utility across a series of unseen test scenarios $\scenariotest \subset \scenarioset(\poptest)$:
  \begin{equation}
        \label{eq:metrics.performance}
        \perf(\pi, \scenarioset) \defeq \dfrac{1}{|\scenarioset|}\sum_{\scenario \in \scenarioset} U(\pi, \scenario),
    \end{equation}
    In addition, we consider two metrics related to robustness, namely worst-case utility and worst-case regret:
    %\footnote{Actually, the "universalization test" $\scenario(m, \emptyset)$) is included in the test scenarios in \cite{leibo_scalable_evaluation_multi_2021}, which evaluates the performance of the policy when it only plays with copies of itself. For simplicity, we assume we know nothing at all about $\Sigma^\text{test}$.}:
    %\begin{equation}
    %    \label{eq:metric.performance}
    %    \perf(\pi, \scenarioset) = \dfrac{1}{|%\scenarioset|}\sum_{\scenario \in \scenarioset} U(\pi, \scenario).
    %\end{equation}
    \begin{equation}
        \label{eq:metrics.utiliy}
        \wcu(\pi, \scenarioset) \defeq \min_{\scenario \in \scenarioset} U(\pi, \scenario), \quad\;\;  \wcr(\pi, \scenarioset) \defeq \max_{\scenario \in \scenarioset} R(\pi, \scenario).
    \end{equation}
    Maximising $\wcu$ is typically preferable when falling below a certain utility threshold must be avoided at all costs; for instance minimising casualties in a surgical context. Conversely, minimising $\wcr$ avoids decisions that lead to significantly worse utility than the optimal utility. 
\end{enumerate}
The final objective is to design a learning process outputting a policy that reliably maximises utility or minimises regret on possibly unseen scenarios. 

\subsection{General Assumptions}
\label{sec:assumptions}
To ensure our setting aligns with the AHT literature, we must adhere to three assumptions \citep{mirsky_survey_ad_hoc_2022}: a) the absence of prior coordination. The learner must be capable of cooperating with the team on-the-fly, without relying on previously established collaboration strategies. b) There is no control over teammates, the learner can control its own copies but not other agents in the configuration. c) All agents (focal and background) are assumed to have a partially shared objective. Their reward function may be slightly different, reflecting varying preferences. In this work, we choose to address this last point by assuming a class of possible reward functions for the background players. 
%\paragraph{Availability of pre-trained policies.} We assume that some pre-trained policies $\poptrain$ are available. We consider $\poptrain$ as an ingredient, that can be exploited by any learning method. This allows to restrict evaluation to the process of exploiting data available at train time. $\poptrain$ could be obtained trough any appropriate method. [Cite].
