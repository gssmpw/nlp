
\section{Introduction}

% collaborative
% ad hoc evaluation
% partial observability
% possibly adaptive policies

% Learn and infer policy types from observations
% train policies to explicitly interact with their teammates in order to learn about their behaviours.
% or Adapting to Current Teammates, at test time, even when teammates switch policy

Domain generalisation is often crucial in Reinforcement Learning (RL) and is typically assessed by placing an agent in novel environments \citep{cobbe_quantifying_generalization_reinforcement_2019}. Likewise, in Multi-Agent Reinforcement Learning (MARL), generalisation to new agents can be evaluated by pairing a trained policy with unseen actors \citep{barrett_empirical_evaluation_ad_2011, hu_other_play_zero_2020, leibo_scalable_evaluation_multi_2021, agapiou_melting_pot_2_2023}. While zero-shot domain adaptation is a valuable property \citep{higgins_darla_improving_zero_2017, schafer_task_generalisation_multi_2022}, it is equally important to ensure proper transfer to new behaviours in multi-agent settings, especially in situations where undesired interactions may arise \citep{gleave_adversarial_policies_attacking_2019}. More specifically, Ad Hoc Teamwork (AHT) occurs when an agent, initially unfamiliar with its teammates, must collaborate to achieve a common goal. In a world where autonomous agents are being progressively introduced in such tasks, cooperation with humans is becoming a major concern \citep{stone_ad_hoc_autonomous_2010, ji_ai_alignement_comprehensive_2023}.

Efforts in AHT have primarily focused on learning and inferring behavioural models or teammates types \citep{barrett_empirical_evaluation_ad_2011, albrecht_empirical_study_practical_2015, barrett_making_friends_fly_2017, chen_aateam_achieving_ad_2020, muglich_generalized_beliefs_cooperative_2022}, adapting to behaviour shifts \citep{manish_ad_hoc_teamwork_2019}, and enhancing generalisation by encouraging diversity in partners during training \citep{jaderberg_human_level_performance_2019,hu_other_play_zero_2020, charakorn_investigating_partner_diversification_2020,lupu_trajectory_diversity_zero_2021, strouse_collaboration_with_humans_2021}. However, these methods provide limited guarantees on the worst-case AHT performance.

% a warehouse robot must cooperate with both other robots and human employees
%Imagine a scenario where a surgeon collaborates with an automated agent. Each surgeon has their own approach, spanning from a novice with cautious yet less precise actions, to a more seasoned professional prioritizing safety. Possibly however, there exists a renowned expert who occasionally takes calculated risks resulting in miraculous outcomes. With such diverse strategies in play, the automated agent must properly accommodate to any chosen approach.

%The nature of AHT tasks can encompass highly varied \emph{scenarios}, posing inherent risks. For instance, in autonomous driving, agents must navigate alongside human drivers which have diverse driving habits and other autonomous cars. Similarly, in cases like automated surgical collaboration, agents must adapt to a spectrum of (human) surgeon approaches, ranging from cautious novices to experienced professionals. It is in fact possible to think of those scenarios as separate (sometimes approximate) single-agent environments \citep{eriksson_risk_sensitive_bayesian_2022}, effectively modeling the problem as a composition of simpler problems that must be solved by the same agent.
A multi-agent system can encompass numerous and diverse \emph{scenarios}, each characterised by its actors. For example, autonomous cars operate alongside human drivers and other autonomous vehicles. Similarly, in a surgical setting, a robot may need to cooperate with surgeons who have a wide range of different habits and expertise. In each of these scenarios, we can adopt the perspective that the \emph{focal} actors are controlled by an automated agent, whereas the other actors are viewed as fixed, forming the \emph{background} of the task \citep{leibo_scalable_evaluation_multi_2021, agapiou_melting_pot_2_2023}. These scenarios can be viewed as distinct environments, as each combination of background actors induces different transition dynamics and reward functions. A common practice involves constructing representative scenarios and training a policy on a uniform distribution over them \citep{strouse_collaboration_with_humans_2021,lupu_trajectory_diversity_zero_2021}. However, this only optimises performance for that specific distribution.

Recent studies in zero-shot domain transfer showed that selecting an appropriate prior over training environments is key to learning robust policies \citep{pinto_robust_adversarial_reinforcement_2017, dennis_emergent_complexity_zero_2020, garcin_how_level_sampling_2023, jiang_prioritized_level_replay_2021,buening_minimax_bayes_reinforcement_2023, li_bayes_optimal_robust_2024}. Intuitively, this insight should apply to the AHT setting as well, suggesting that choosing a specific prior over scenarios/partners may improve the robustness of learned policies. Assuming that no information is available about the teammates at test time (and their distribution), we consider the \emph{worst} possible prior over the training set of partners given our policy, an idea adopted from the minimax-Bayes concept \citep{berger_statistical_decision_theory_1985}.
%We show that under the access of a population of pre-trained policies, a finite set of approximate single-agent environments can be constructed

%Hence based on the Minimax-Bayes idea applied to Reinforcement Learning \citep{berger_statistical_decision_theory_1985, buening_minimax_bayes_reinforcement_2023}, our work we assume the worst-case prior over. We show that under the access of a population of pre-trained policies, a finite set of single-agent environments, which we call \emph{scenarios}, can be constructed.

% Concerns, why our method is needed ?

% Robust policies -> higher predictability

% In related works : This problem first formalised by \citep{stone_ad_hoc_autonomous_2010} has  

% for the car example, it can really consider any scenario, from full selfplay to only one AI with many humans.


% par about Mutli-agent RL, why do we need this

% introduce the problem of common practices and their assumptions -> no real consideration about robustness, but rather generalisation

% zero-shot generalization

% give concrete example for why robustness may be necessary in MARL (risk-averse surgeon, unexpected cooperators, risk minimization, regret minimization, adversarial attacks)

% par about how we can represent the problem as a set of simpler problems, where what is left is determine the right distribution.

%Any multi-agent problem can be more simply be represented as a set of scenarios, where at any given time, nature picks one of them. , where each scenario is defined by its known actors.


%More generally, such tasks can be depicted by a world with unknown agents acting independently. Many complex fields entail very diversified scenarios where ad-hoc teamwork is needed. For instance, automated cars have to both deal with diverse human drivers and other automated cars possibly from different manufacturers \citep{leibo_scalable_evaluation_multi_2021}. As such, in a world where the proportion of actors which are also automated is unknown, robust agents must also cooperate with "copies" of themselves. 


% contributions 
\paragraph{Contributions.} We make the following contributions: 
\begin{enumerate}[topsep=4pt, leftmargin=12pt]
    \item We adapt Minimax-Bayes Reinforcement Learning (MBRL) \citep{buening_minimax_bayes_reinforcement_2023} to the AHT setting, reasoning about uncertainty with respect to partners rather than environments (Section~\ref{seq:robust_aht}).
    \item We examine the advantages of using utility and regret for AHT robustness, and provide solutions to target either metric (Section~\ref{seq:utility_or_regret}).
    \item We study out-of-distribution robustness guarantees (Section~\ref{section:ood}).
    \item We propose a Gradient Descent-Ascent (GDA) \citep{lin_gradient_descent_ascent_2020} based algorithm, in conjunction with policy-gradient methods, and discuss its convergence for softmax policies (Section~\ref{seq:learning}).
    \item We conduct extensive experiments to evaluate our approach. We deploy learned policies on both seen and unseen scenarios for cooperative problems, including a partially observable cooking task from the Melting Pot suite \citep{leibo_scalable_evaluation_multi_2021, agapiou_melting_pot_2_2023}. We compare our approach against Self-Play (SP), Fictitious Play (FP) \citep{brown_iterative_solution_games_1951, heinrich_fictitious_self_play_2015} as well as learning a policy w.r.t.\ a fixed uniform distribution over scenarios \citep{lupu_trajectory_diversity_zero_2021}, which is related to fictitious co-play \citep{strouse_collaboration_with_humans_2021}, as both learn the best response to a population of policies (Section~\ref{seq:experiments}).
    \item Our results confirm the theory and empirically demonstrate that our approach leads to the most robust solutions for both simple and deep RL coordination tasks, even when teammates are adaptive. 
    %This highlights the importance of choosing an appropriate distribution over training partners/scenarios to find policies that better transfer to new teammates.
\end{enumerate}