\section{Related Work}

% MARL minimax q learning, minimax bayesian rl
% RL domain transfer
% mention everything related, as much as possible
% 

\paragraph{Ad Hoc Teamwork.}
In AHT, we are interested in developing agents capable of cooperating with other unfamiliar agents without any form of prior coordination \citep{rovatsos_towards_social_complexity_2002,stone_ad_hoc_autonomous_2010, barrett_empirical_evaluation_ad_2011, barrett_making_friends_fly_2017}. Popular approaches usually involve some form of Population Play (PP), where policies forming a population are learning by interacting with each other \citep{lupu_trajectory_diversity_zero_2021, muglich_equivariant_networks_zero_2022, leibo_scalable_evaluation_multi_2021, agapiou_melting_pot_2_2023}. 
% Describe methods using PP which do not play around distributions
Key strategies for ensuring generalisation to new partners include promoting policy diversity within the training population \citep{charakorn_investigating_partner_diversification_2020} and preventing overfitting to training partners \citep{lanctot_unified_game_theoretic_2017}. Both \citet{lupu_trajectory_diversity_zero_2021} and \citet{strouse_collaboration_with_humans_2021} previously showed that learning a best response to a more diverse population leads to improved generalisation. Additionally, \citet{jaderberg_human_level_performance_2019} showed the effectiveness of PP when diversity is encouraged through evolving pseudo-rewards. However, PP still struggles with producing policies that are robustly collaborative with new partners and sometimes exhibits overfitting \citep{carroll_utility_learning_about_2019,leibo_scalable_evaluation_multi_2021,agapiou_melting_pot_2_2023}.

To further improve AHT, several works suggest inferring the teammates' models/types, maintaining a belief about ad hoc partners based on previous interactions within an episode \citep{barrett_empirical_evaluation_ad_2011, albrecht_empirical_study_practical_2015}. This was shown to help substantially in the partially observable setting \citep{gu_online_ad_hoc_2021, ribeiro_making_friends_dark_2023, dodampegama_knowledge_based_reasoning_2023}. Efforts have also been made to improve the learning and generalisation of such models to new partners \citep{barrett_cooperating_unknown_teammates_2015, barrett_making_friends_fly_2017, muglich_generalized_beliefs_cooperative_2022}.

% As AHT typically involves partial observability, recent advances also focused on how to be robust in that setting 
% (gu) introduce an information-based regularizer to derive proxy representations of the learned variables from local observation
% (ribeiro) capable of inferring the underlying target task and teammate’s behaviour from the agent’s history of observations
% (dodampegama) Knowledge-based Reasoning

%In many collaborative settings, multiple joint-strategies that are optimal but incompatible exist. Mechanisms preventing policies from converging to one of many equivalent but incompatible policies are also a direction that has been taken to improve AHT \citep{hu_other_play_zero_2020, muglich_equivariant_networks_zero_2022, lanctot_unified_game_theoretic_2017}.

An alternative approach proposed by \citet{li_robust_multi_agent_2019} involves a robust formulation of deep deterministic policy gradients, assuming worst-case teammates. Unlike our setup, they train a joint policy that remains consistent throughout learning, and design their algorithm specifically for deep deterministic policy gradients, whereas our approach is compatible with any policy-gradient algorithm.



Even though the aforementioned methods attempt to improve cooperative robustness, they always assume specific distributions for the partners. For example, \citet{jaderberg_human_level_performance_2019} used a distribution favoring the matchmaking of policies of similar levels with the intuition that the reward signal is stronger in those cases. However, it does not provide any insights on its effects on AHT robustness. As a result, the actual impact of the training partner distribution on robustness is left under-explored. This holds significant potential, as it can be exploited in conjunction with previously studied mechanisms to substantially enhance AHT robustness.

%A good amount of work evaluate their policies with held-out partners that are

%\citet{muglich_equivariant_networks_zero_2022} previously showed that it is possible to constraint policies to be equivariant, meaning that they are prevented to arbitrarily converge to one of many equivalent but mutually incompatible policies.

% FCP

% Transfer to Humans
%(We do not evaluate our policies with humans. We however evaluate our policies on held-out scenarios composed of policies of identical architectures.)

\paragraph{Zero-shot Domain Transfer.}
AHT can be seen as a form of zero-shot domain transfer. Each possible team composition involving the focal agent can be considered a different environment. In the single-agent setting, \citet{jiang_prioritized_level_replay_2021} demonstrated that adapting the training environment distribution by prioritising environments with higher prediction loss (a measure of the policy's lack of knowledge) leads to improved sample efficiency and generalisation. Building on this idea, \citet{garcin_how_level_sampling_2023} prioritised environments where the mutual information between the learning policy's internal representation and the environment identity was lower, using information theory to achieve similar results. The idea of tampering with the environment distribution was also explored by \citet{pinto_robust_adversarial_reinforcement_2017}, who employed a maximin utility formulation to choose continuous adversarial environment perturbations throughout learning. Instead of utility, \citet{dennis_emergent_complexity_zero_2020} stressed the advantages of using regret by proposing a training environment sampling scheme avoiding entirely unsolvable and uninformative environments. Most relevant to this work,  \citet{buening_minimax_bayes_reinforcement_2023} conducted a study over worst-case priors (for both utility and regret) over training environments, and proved that worst-case distributions are well-suited for domain transfer. \citet{li_bayes_optimal_robust_2024} later reaffirmed those results, learning worst-case distributions within ambiguity sets of subjective priors. Finally, there exist works on domain transfer in the MARL setting \citep{schafer_task_generalisation_multi_2022}, but this differs from our focus on transferring to new partners. This related work is consistently in favor of caring about environment distributions for robustness, providing strong motivation to bring this concern to AHT. 
