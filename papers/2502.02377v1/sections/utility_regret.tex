\section{Utility or Regret?}
\label{seq:utility_or_regret}

Optimising for the worst-case utility \eqref{eq:mbmarl.maximin} might be problematic. Nature could resort to only picking scenarios where the focal players achieve the worst possible score. Then, the distribution trivially minimises utility for any chosen policy, preventing the latter to learn anything.
\citet{buening_minimax_bayes_reinforcement_2023} addresses this issue by instead considering the regret of a policy. The difference is that ‘impossible’ scenarios will always yield zero regret for any policy, thus becoming irrelevant for a regret-maximising nature. Letting $L(\pi, \beta) \defeq \sum_\sigma R(\pi, \mdp) \beta(\sigma)$ be the Bayesian regret with respect to a prior $\beta$, we now formulate the following minimax regret game:
\begin{equation}
    \label{eq:mbmarl.minimax}
    \min_{\pi \in \policies} \max_{\beta \in \Delta(\scenarioset(\bgpop))} L(\pi, \beta).
\end{equation}
One can also prove that this above game has a value. Moreover, a solution  ($\pi^*_R, \beta^*_R$) to \eqref{eq:mbmarl.minimax} exhibits properties analogous to those in equations~\eqref{eq:utility_lower_bound}, \eqref{eq:best_on_worst_case_prior} and \eqref{eq:optimal_worst_case_utility}, but in terms of regret. $\pi^*_R$ has its Bayesian regret upper-bounded by $L(\pi^*_R, \beta^*_R)$ on $\scenarioset(\bgpop)$. It is also optimal under the worst-case prior $\beta^*_R$ and achieves optimal worst-case regret $\wcr$ on $\scenarioset(\bgpop)$.

Should utility or regret be used as an objective? Exploiting regret ensures that scenarios on which you can improve the most are sampled more often. It also ensures that degenerate scenarios get discarded as their regret is always zero. However, it demands the calculation of best responses for each scenario, which becomes taxing as the number of scenarios or problem complexity grows.
    %\begin{equation*}
    %    |\Sigma(\mathcal{B})| = \sum_{c=1}^m \multiset{|\mathcal{B}|}{m-c} = \sum_{c=1}^m {|\mathcal{B}| + m - c \choose m-c}.
    %\end{equation*}
To reduce the computational burden, we can approximate those best responses, or subsample the set of scenarios.
An alternative way is to make use of the utility notion under some additional conditions.

\begin{definition}[Non-degenerative population]
        A background population of policies $\bgpop \subset \policies$ is non-degenerative if and only if for any scenario $\scenario\in \scenarioset(\bgpop)$, there exists two distinct policies $\pi_1$ and  $\pi_2 \in \policies, \pi_1 \neq \pi_2$ such that $U(\pi_1, \scenario) \neq U(\pi_2, \scenario)$.
    \end{definition}
\begin{lemma}
    If a background population $\bgpop\subset\policies$ is non-degenerative, then
    for any scenario $\scenario \in \scenarioset(\bgpop)$, there exists a policy $\pi \in \policies$ such that $R(\pi, \scenario) > 0$. 
    \begin{proof}
        $\bgpop$ is non-degenerative, for any scenario $\scenario \in \scenarioset(\bgpop)$ there must exist two policies $\pi_1$ and $\pi_2$ such that $U(\pi_1, \scenario) > U(\pi_2, \scenario)$. We have by definition $U^*(\scenario)\geq U(\pi_1, \scenario)$, hence $R(\pi_2, \scenario) > 0$.
    \end{proof}
\end{lemma}
Making the assumption that a background population is non-degenerative is in general realistic for cooperative tasks. This translates into only considering reasonable behaviors for the background population, or tasks where teammates cannot completely cancel out the actions of the focal players. Under the assumption of a non-degenerative background population, no distribution can deadlock the policy learner into stale scenarios. Hence, the utility-minimising opponent in Equation~\ref{eq:mbmarl.maximin} can no longer trivially minimise utility.
For the remainder of the paper, background populations are assumed to be non-degenerative. 

% No need to debate on who is needed more, we should explain in which condition utility can be used.

% argue that in some context, it is easy to compute the maximal utility for all scenarios.

%The two get different when the intervals of min-max utility between scenarios do not overlap fully.

%\begin{lemma}
%Under a fixed distribution $\beta$ over scenarios,  optimising a policy to minimise utility or maximise regret with respect to $\beta$ yields equivalent results: $\arg\max_{\pi \in \policies} U(\pi, \beta) = \arg\min_{\pi \in \policies} R(\pi, \beta)$.
%\end{lemma}

