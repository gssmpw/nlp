\paragraph{Citations}
\begin{itemize}
    \item Zero-shot coordination. Attempt to improve upon self-play by adding other policies to the mix and breaking symmetries \citep{hu_other_play_zero_2020}. \citep{muglich_equivariant_networks_zero_2022} better ways to break symmetries pluggable to NNs.
    \item Zero-shot generalisation / Ad-hoc teamwork / Zero-shot transfer to new co-player. Hanabi with humans \citep{hu_other_play_zero_2020}. \citep{leibo_scalable_evaluation_multi_2021, agapiou_melting_pot_2_2023} show that cross-play (XP) with VMPO/A3C with joint rewards is usually not enough to learn robust policies, serves as "default" empirical evaluation for zero-shot generalisation. \citep{barrett_empirical_evaluation_ad_2011} first empirical evaluation of zero-shot generalisation. 
    \item Beginnings of Ad hoc Teamwork, \citep{bowling_coordination_adapatation_impromptu_2005, rovatsos_towards_social_complexity_2002}. 2022 survey on ad-hoc teamwork, \citep{mirsky_survey_ad_hoc_2022} ("All agents are assumed to have a common objective, but some teammates might have additional, individual objectives, or even completely different rewards").
    \citep{albrecht_empirical_study_practical_2015} proposes way to adapt to policy types using pseudo-bayesian beliefs..
     \citep{muglich_generalized_beliefs_cooperative_2022} similar (must look deeper into the differences). \citep{stone_ad_hoc_autonomous_2010} formalizes the problem as a challenge, and pushes the community to work on ad-hoc teamwork generalisation, gives good examples.
    \item Zero-shot domain adaptation. \citep{higgins_darla_improving_zero_2017}. \citep{schafer_task_generalisation_multi_2022} task generalisation in MARL. \citep{garcin_how_level_sampling_2023} observes the necessity to have a precise distribution over "scenarios", here they describe it as tasks, they also compare with unsupervised environment design. 
    \item How to evaluate/quantify generalization in RL.\citep{cobbe_quantifying_generalization_reinforcement_2019}. Scenario at test time should be different from train time. -> achievable with other policies.
    \item Human-AI cooperation. 
    \item Minimax-Bayes \citep{berger_statistical_decision_theory_1985}
    \item Diverse policy generation for ad-hoc teamwork. \citep{canaan_generating_adapting_diverse_2023} uses rule based policies generated with MAP-elites. \citep{muglich_generalized_beliefs_cooperative_2022} uses GA with simple NN policies. \citep{lupu_trajectory_diversity_zero_2021} through regularisation. \citep{jaderberg_human_level_performance_2019} reward shaping
    \item plastic algorithms ? \citep{barrett_cooperating_unknown_teammates_2015, barrett_making_friends_fly_2017}
    \item model based RL for both env and policies (ad hoc generalisation) \citep{ribeiro_teamster_model_based_2023}
    \item overfitting to policies encountered during training. \citep{lanctot_unified_game_theoretic_2017}
    \item Collaborating with Humans without Human Data -> training policies independently to break symmetries
    \item best response to populations \citep{lupu_trajectory_diversity_zero_2021, strouse_collaboration_with_humans_2021}
    \item the challenge of the ZSC framework arises from the fact that
many collaborative settings admit multiple joint-strategies
that are optimal under self-play (SP) (Tesauro, 1994) yet
incompatible. Thus, if we naively train two independent
agents in SP, there is no guarantee that they will converge to
compatible policies.
\end{itemize}