\begin{figure}
    \centering
    \includegraphics[width=0.47\textwidth]{data/illustration_bf.png}
    \caption{Illustration of the framework used in this paper. Prior to training the focal policy $\pi$, background policies with different preferences ($\lambda_i, \delta_i$) learn by interacting within sub-populations of varying sizes. These sub-populations are then combined to form a background population, $\poptrain$, used as a common ‘training dataset’ for all algorithms.\\ 
    Our primary focus is on the training phase, where the focal policy $\pi$ is trained while the distribution $\beta$ over scenarios is tuned according to the proposed minimax game. These scenarios mix copies of $\pi$ with policies from $\poptrain$, where the self-play scenario $\scenario^\text{SP}$ has the policy interacting only with copies of itself. }
    \label{fig:main_illustration}
\end{figure}