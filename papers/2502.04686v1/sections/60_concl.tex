In this work, we presented Latent Space Policy Optimization (LSPO), an iterative framework that combines structured game-solving techniques with the expressive power of large language models to build strategic language agents in free-form social deduction games. By abstracting unconstrained language action space into a discrete latent strategy space, our approach enables efficient CFR in the latent space to overcome intrinsic bias and learn strong strategies. We then perform iterative fine-tuning via DPO to align the LLM's language generation with the evolving strategy and expand the latent strategy space to address the action coverage issue. Our extensive evaluation in the Werewolf game demonstrates that LSPO not only addresses intrinsic biases and action coverage issues inherent in prompt-based agents, but also achieves increasing performance with respect to iterations and outperforms four state-of-the-art baseline agents. Looking ahead, we envision LSPOâ€™s synergy of latent-space abstraction and preference-based language alignment can be extended to a variety of other complex decision-making tasks with free-form language actions.

% multi-agent and human-in-the-loop settings, paving the way for more robust and versatile language-grounded decision-making systems.