% In this work, we concentrate on a seven-player variant of the Werewolf game, featuring two Werewolves, one Seer, one Doctor, and three Villagers. We develop agents capable of engaging in a text-based version of the game, where all interactions are conducted through natural language without reliance on non-verbal cues such as tone or facial expressions.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \caption{Overview of the Latent Space Policy Optimization (LSPO) framework. Each iteration consists of three components. (1) Latent space construction: generate language actions with the LLM and cluster the vast language action into a finite latent strategy space. (2) Policy optimization in latent space: reformulate the original game as an abstracted game and apply game-theoretic methods to learn latent space policy. (3) Latent space expansion: fine-tune the LLM to align with the latent space policy and generate new strategies to expand the latent strategy space.}
    \label{fig:overview}
\end{figure*}

Werewolf is a popular social deduction game where players with hidden roles cooperate and compete with others in natural languages. The Werewolf side needs to conceal their identities and eliminate the other players, while the Village side needs to identify their teammates and vote out the Werewolves. Players are required to have both language proficiency for communication and strategic ability for decision-making to achieve strong performance in the Werewolf game. We consider a seven-player game with two Werewolves being the Werewolf side and one Seer, one Doctor, and three Villagers being the Village side.
% \gf{It's overlapped with the first sentence of the next paragraph.} 
Detailed descriptions of the game's rule, observation space, and reward function can be found in Appendix~\ref{app:game}. 
% More detailed descriptions of the game can be found in Appendix~\ref{app:game}. 
% \gf{It would be better to say what additional details are in the appendix based on the next section.}

\subsection{Game Environment}

We consider a text-based seven-player Werewolf game that proceeds through natural languages. We exclude other information like the speaking tone, facial expression, and body language~\cite{lai2022werewolf}. This pure text-based environment is a common setup in the literature~\cite{xu2023exploring,xu2023language,wu2024enhance,bailis2024werewolf}. 

\textbf{Roles and Objectives.} 
At the beginning of each game, the seven players are randomly partitioned into two sides. The Werewolf side has two Werewolf players who know each other's role and aim to eliminate the other players while avoiding being discovered. The Village side has one Seer who can check the role of one player each night, one Doctor who can protect one player each night, and three Villagers without any ability. The players in the Village side only know their own role and need to share information to identify the Werewolves and vote them out.


\textbf{Game Progression.} 
The game proceeds by alternating between night round and day round. In the night round, players can perform secret actions that are only observable by themselves. More specifically, the two Werewolves can choose a target player to eliminate, the Seer can choose a target player to investigate whether the player's role is Werewolf, and the Doctor can choose a target player to protect the player from being eliminated. The Doctor does not know the target player chosen by the Werewolves. If the Doctor chooses the same target player as the Werewolves, then no player is eliminated in this night round, otherwise, the Doctor fails to protect any player, and the target chosen by the Werewolves is eliminated.

\textbf{Observations and Actions.} 
The language observation of each agent is a list of natural languages that log the game history to the current step. This list include both private information that are only observable to the current player and public information that are shared by all players. The private information includes the role of the current player, the secret actions in the night round for the Werewolf, Seer, and Doctor, and the teammate for the Werewolf. The public information includes the ID of the current player, the eliminated player in each night and day round, the discussion, and the voting result in each day round. 
% \gf{One example observation input is illustrated in xxx.}

Player actions are also in the form of natural language and can be categorized into three types: secret actions, which are secret actions performed during the night, such as choosing a target player to eliminate, investigate, or protect; discussion actions, which are statements made during the day to influence other players' perceptions and decisions; and voting actions, which are choices made during the voting round to vote for on player or choose not to vote.


\subsection{Challenges for Language Agents}

Unlike board, card, or video games with a finite set of actions, Werewolf has a free-form language action space. The vast space of natural language actions poses two key challenges for language agents to achieve strong performance in the Werewolf game.
% \gf{intrinsic bias and unbounded action coverage. However, I think intrinsic bias issue is not caused by the vast space but from the training data? (Ignore it if I'm wrong...)}

\textbf{Intrinsic Bias in Action Generation.}
As observed in simple games like Rock-Paper-Scissor~\cite{xu2023language}, pure LLM-based agents tend to exhibit intrinsic bias in their action generation, which is inherited from the model's pre-training data. 
This issue is more pronounced in complex language games like Werewolf, where the opponents can exploit these predictable biases to counteract the agent's move. Therefore, mitigating intrinsic bias is essential for language agents to reduce exploitation and achieve strong performance.

\textbf{Coverage of Unbounded Action Space.}
Due to the immense combinatorial space induced by free-form text, it is impractical to map every possible utterance to an action in the language space. On the other hand, manually engineering or prompting an LLM to produce a limited set of actions may fail to capture the full strategic landscape. Even if an agent optimally masters the action distribution within a limited subset, it could be easily exploited by out-fo-distribution utterance. Consequently, inadequate coverage of the action space could result in suboptimal performance in free-form language games like Werewolf.

