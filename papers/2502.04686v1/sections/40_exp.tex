We conduct extensive experiments in the Werewolf game to evaluate the effectiveness of our LSPO framework. We use ``Llama-3-8B-Instruct'' as the base model in our experiments. We first visualize how the latent strategy space evolves to show that our agents progressively acquire more complex strategic behaviors. We then quantitatively evaluate the performance of our LSPO agent using prediction accuracy and win rate to show the improving performance over iterations.  We also compare the LSPO agent with four state-of-the-art agents, showing that our agents achieve the highest win rate as both the Werewolf side and the Village side. We further perform ablation studies to assess the effectiveness of specific designs in our framework. More implementation details can be found in Appendix~\ref{app:training}. 
% \yc{we didn't say which LLM in the whole paper? also the baseline agents. it is important to mention this for a fair comparison.}


\begin{table*}[t]
    \centering
    \input{tabs/iteration}
    \caption{The prediction accuracy and win rate of the LSPO agents in different iterations.}
    \label{tab:iteration}
    % \vspace{-2mm}
\end{table*}


\subsection{Latent Space Visualization}

To gain insight into how LSPO organizes free-form language actions into discrete latent strategies, we first visualize the latent strategy space constructed at different training iterations. Specifically, for each role in the Werewolf game, we gather the utterances generated by the LSPO agent in $100$ games, embed them with the sentence encoder, and apply dimensionality reduction for projection. The visualization of latent spaces for the Werewolf and the Seer in different iterations is shown in Figure~\ref{fig:latent_space}. Earlier iterations yield relatively indistinct clusters, reflecting a lack of strategic diversity. Over successive iterations, clearer and more refined clusters emerge, indicating that the LSPO agent evolves toward an increasingly structured latent space and learn to express different strategic intentions such as accusing specific roles, defending teammates, and bluffing.

\textbf{Werewolf's Latent Space.}
In the first iteration, the latent space of the Werewolf is dominated by three main clusters. The blue cluster corresponds to a simple strategy of concealing its role or pretending to be a villager, while the smaller orange cluster reflects strategies like pretending to be a Seer or a Doctor. There is even a green cluster corresponding to unintentionally revealing the true role of a Werewolf, which is obviously a flawed strategy. As training proceeds, we see more sophisticated patterns emerge. The flawed strategy of disclosing one's Werewolf role disappears, and the agent begins to incorporate deliberate bluffs and misdirections instead. For example, the red cluster features the agent pretending to be a Seer and providing fabricated investigative results to sow confusion, and the purple cluster centers on defending the teammate and redirecting suspicion onto other players, leveraging more nuanced language and reasoning to guide the conversation toward scapegoats. This refined partitioning demonstrates that the Werewolf agent progressively covers an increasing number of latent strategies.


\textbf{Seer's Latent Space.}
In the first iteration, the Seer’s latent space is relatively coarse, containing primarily two strategies including staying silent about its true role or revealing its role and sharing information. This shows a limited range of strategic diversity in the early stage. As training proceeds through the second and third iterations, the Seer’s latent space becomes more diverse. The emergent red cluster features direct accusations once the Seer identifies a Werewolf, while the green cluster corresponds to concealing the role yet subtly guiding discussions to protect verified teammates. Notably, by the final iteration, the model develops a voting coordination strategy in which the Seer explicitly asks all the Villagers to vote for a strongly suspected Werewolf to maximize the Villager's chance of success. This progression implies that the Seer agent increasingly learns to balance openness and secrecy, aligning its communication style with the evolving game context to better support the Village side.


\subsection{Iterative Performance Evaluation}

We then evaluate how the performance of our LSPO agent progresses with more iterations, demonstrating that our framework produces increasingly stronger strategic language agents over time. We focus on two key metrics including 
prediction accuracy and win rate.

\textbf{Prediction Accuracy.}
Accurate role identification is a critical aspect of Werewolf, as it underpins effective decision-making and voting. Therefore, we measure the agent’s ability to predict the roles of other players with an additional prediction phase before each voting phase in a Werewolf game. Specifically, we use the final-iteration LSPO agent as the fixed opponent and let LSPO agents at different iterations play against this opponent for $100$ games. For the Werewolf side, a higher prediction accuracy of crucial roles like Seer and Doctor allows them to eliminate these threats earlier. Conversely, for the Village side, a higher prediction accuracy of Werewolves improves their chance to vote out the Werewolf and win the game.

\textbf{Win Rate.}
While prediction accuracy serves as an intermediate metric to evaluate the agents' reasoning and decision-making ability, we also use the win rate as a direct measure of the performance of our agents. Similar to the evaluation of prediction accuracy, we use the final-iteration LSPO agent as the fixed opponent and let our agents at different iterations play $100$ games against the opponent. A higher win rate indicates a stronger performance in the game.

% \paragraph{Evaluation Result.}
As shown in Table~\ref{tab:iteration}, both prediction accuracy and win rate exhibit a clear growing trend as the iteration increases, indicating that our iterative LSPO framework steadily strengthens the agents’ reasoning and decision-making capabilities. From the Werewolf side, the identification rate for the Seer starts off relatively high but has only modest improvement. This is because the Seer often reveals its roles to share information, making it easier for the Werewolf side to identify. By contrast, the Werewolf's prediction accuracy of the Doctor shows more significant gains, reflecting the strategic importance of eliminating the Doctor who can save potential victims. On the Village side, identifying the Werewolf and the Seer benefits most from iterative learning, since confirming these central roles is crucial for coordinated voting and elimination of Werewolves. Overall, these results confirm that our framework consistently improves the strategic language abilities of the LSPO agent, enabling it to adapt and excel in complex social deduction scenarios with each additional iteration.


\begin{table*}[t]
    \centering
    \input{tabs/head2head}
    \caption{Comparison between our LSPO agent with state-of-the-art agents in the Werewolf game.}
    \label{tab:head2head}
\end{table*}


\subsection{Comparison with State-of-the-Art Agents}

We compare the performance of the LSPO agent in the Werewolf game with four state-of-the-art agents including Reason and Act (ReAct)~\cite{yao2022react}, Recursive Contemplation (ReCon)~\cite{wang2023avalon}, a Cicero-like agent~\cite{meta2022human}, and Strategic Language Agent (SLA)~\cite{xu2023language}. As some of these methods were not initially developed for Werewolf, we make minor modifications to ensure compatibility with our experimental setting while preserving each approach’s core design. 
% \yc{The details of Baseline can be found in Appendix \ref{app:baseline}.}\yc{due to time limit, just put prompt in appendix is fine.}

\textbf{ReAct.}
ReAct is a classic prompt-based method that synergizes reasoning and acting for agent tasks. We implement ReAct for the Werewolf game by providing the LLM with raw game observations to generate both intermediate reasoning and final actions within a single prompt.

\textbf{ReCon.}
ReCon is another prompt-based method designed for Avalon agents. The ReCon agent is prompted to first think from its own perspective and then think from its opponents' perspective to generate the final action. We make slight modifications in the prompt to apply ReCon in the Werewolf game.

\textbf{Cicero-Like.}
The Cicero agent is created for the game of Diplomacy with finite action space and consists of a strategic reasoning module and a dialogue module. We implement a Cicero-like agent for the Werewolf game by predefining an action space of $13$ primitive actions like ``claim to be the Seer'', ``do not reveal role'', etc. An RL policy is learned to select these actions in each state and generate action-conditioned languages in the game.

\textbf{SLA.}
SLA combines reinforcement learning and LLM to overcome intrinsic bias and build strategic language agents for the Werewolf game. We adopt the same implementation as described in the paper~\cite{xu2023language}.

We compare our final-iteration LSPO agent with the aforementioned four baselines through two head-to-head evaluation setups. In the first setup, our LSPO agent takes the Werewolf side and we let each of the five agents including our agent and four baseline agents take the Village side to play $100$ Werewolf games with our LSPO agent. This setup measures the Village side's win rate against the LSPO agent as the Werewolves. In the second setup, we reverse the roles and let the LSPO agent take the Village side and compare the win rate of five agents as the Werewolves averaged over $100$ games. As shown in the bold numbers in Table~\ref{tab:head2head}, our LSPO agent achieve the highest win rates both as the Werewolves and as the Villagers. 

The strong performance of our LSPO agent is largely attributable to its iterative interplay between latent space strategy learning and preference-based fine-tuning, which refines both language and decision-making over time. By contrast, ReAct and ReCon rely on prompt-based approaches without game-theoretic updates, leaving them susceptible to intrinsic biases from pretraining data and limiting their performance in complex decision-making tasks. The Cicero-like agent is constrained by a predefined action set, making it difficult to evolve more subtle and diverse strategies as the game progresses. SLA partially addresses the intrinsic bias issues by generating multiple candidate actions and using RL to select from them. However, it still relies on a prompt-based method that can suffer from limited coverage of potential strategies. In comparison, our LSPO method integrates CFR’s policy improvement and latent-space cluster refinement with preference-based LLM alignment, enabling it to explore, exploit, and continuously expand the range of viable strategic moves in social deduction games.

\begin{table}[t]
    \centering
    \input{tabs/ablation}
    \caption{Ablation on fine-tuning with latent space policy.}
    \label{tab:ablation}
    % \vspace{-4mm}
\end{table}

\subsection{Ablation Studies}

To show the effectiveness of our design, we compare the LSPO agent with an ablated version of itself. This ablated agent only performs latent space construction and policy optimization in latent space, without LLM fine-tuning and latent space expansion. To generate discussion action in gameplay, this agent first uses the latent space policy to sample a latent strategy, then the previously collected discussions corresponding to the latent strategy are used as few-shot examples to prompt the LLM for the discussion action. We compare this agent with the LSPO agent trained for one iteration and the result is shown in Table~\ref{tab:ablation}. The LSPO agent trained for one iteration achieves higher win rates than the ablated agent as both the Village side and the Werewolf side. This result indicates that fine-tuning the LLM to align with the latent space policy can help the LLM better generalize to new language actions beyond the collected samples and expand the latent strategy space.

