Developing intelligent agents that can reason rationally, make strategic decisions, and interact with humans has been a long-term goal in artificial intelligence (AI) research~\cite{wooldridge1995intelligent,russell2016artificial}. In recent years, large language model (LLM)-based agents have made significant strides towards this goal by exhibiting strong performance in open-ended conversation and multi-step decision-making~\cite{brown2020language,ouyang2022training}. Trained on massive text corpora, LLM-based agents have demonstrated remarkable versatility across various domains, ranging from web navigation~\cite{nakano2021webgpt,yao2022react} and code generation~\cite{chen2021evaluating,yang2024swe} to video game environment~\cite{wang2023voyager} and real-world scenarios~\cite{ahn2022can,brohan2023rt}. Beyond single-agent tasks, LLM-based agents have also shown potential in multi-agent interactions including collaborative teamwork~\cite{li2023camel}, adversarial gameplay~\cite{meta2022human}, and human-AI interation~\cite{park2023generative,liu2023llm}.

% Building agents that are capable of logical reasoning, strategic decision-making, and communication with humans has been a longstanding aspiration in the field of AI~\citep{wooldridge1995intelligent,goodwin1995formalizing}.
% With their extensive knowledge and emergent generalizability, large language models (LLMs) have shown great potential in constructing intelligent agents and have led to many recent advancements~\citep{ouyang2022training,wei2022emergent}.
% These LLM-based agents demonstrate proficiency in a wide range of tasks including web surfing~\citep{nakano2021webgpt,yao2022react,zheng2024gpt}, complex video games~\citep{wang2023voyager,ma2023large}, and real-world applications~\citep{ahn2022can,shah2023lm}.
% Moreover, in multi-agent scenarios, LLM-based agents exhibit the ability to produce human-like interactions~\citep{park2023generative,williams2023epidemic}, achieve zero-shot cooperation~\citep{li2023camel,chen2023agentverse}, and compete with adversarial players~\citep{meta2022human,wang2023avalon}.

Among these interactive domains, social deduction games such as Werewolf present unique challenges because they require both high-level strategic decision-making and free-form conversational abilities. Unlike classic games with predefined and limited actions such as board games~\cite{silver2016mastering,silver2018general}, card games~\cite{moravvcik2017deepstack,brown2018superhuman}, and video games~\cite{mnih2013playing,vinyals2019grandmaster}, Werewolf relies heavily on free-form conversation to achieve agreements and perform strategic deceptions. Players must communicate, bluff, and infer hidden roles through unrestricted, natural language interactions. This free-form language space expands the strategic possibilities and introduces additional complexity unmatched by more rigidly defined domains. As a result, Werewolf can serve as an ideal testbed for developing strategic agents with language-grounded decision-making capabilities.

However, designing an effective AI agent that can interact with human players in Werewolf or similar free-form language environments is still challenging. 
On the one hand, classic game-solving approaches like Counterfactual Regret Minimization (CFR) and reinforcement learning (RL) have proven successful in games like Go and Poker, thanks to their ability to handle finite action spaces. %\yw{I removed \emph{large}. Since LLM action space is much larger. but \emph{structured} also sounds wired. can we use some word similar to \emph{finite and discrete?} or just put the word \emph{finite}?}
% Unfortunately, the vastness of free-form language actions renders a direct application \yw{renders a direct application infeasile.... why not say it in two sentences?. sth like. however action space in werewolf is free-form game. Directly applying these methods can be computational infeasible. }of such methods computationally infeasible. 
% \gf{However, Werewolf has a free-form action space, making direct application of these methods computationally infeasible.}
However, Werewolf has a free-form action space, making direct application of these methods computationally infeasible.
Mapping every possible utterance to an action in the original text space becomes prohibitively large, leading to immense difficulty in strategy representation and equilibrium-finding. 
% On the other hand, purely large language model (LLM)–based approaches face their own limitations \yw{can we put more explanations? say sth like, one the other hands, many language-agents that are built directly on commercial LLM APIs also have limitations. These methods are often prompt-based without performing any training for the base LLM. This is because that for social deduction games, data are expensive. .... therefore, the success of these approaches fully rely on the general reasoning capabilities of LLMs to produce candidate actions, which can be limited in complex games like werewolf. (you need to mention the limitation of existing prompt-based approaches is that the action coverage issue. It is not only the bias, but also the coverage. your ICML24 paper (partially) address the bias issue but does not touch the issue of coverage. You address the bias issue by run CFR but also address the coverage issue by running iterative latent space learning.)}. Prompt engineering methods are often insufficient for complex social deduction games like Werewolf because the intrinsic bias inherited from pretraining data can undermine agents' strategic decision-making ~\cite{xu2023language}. Moreover, fine-tuning an LLM to excel in social deduction games usually requires specialized, high-quality data that demands expensive human labor. 
% \yw{I think you should raise a question here to motivate and highlight your method here. sth like : Can we have a method that both leverage the specialized reasoning capabilities for decision making and leverage the generalization capabilities of LLMs?}
% \gf{On the other hand, many language agents built directly on commercial LLM APIs has their own limitations. These methods typically rely on prompt engineering without training the base LLM, which means their success dependes entirely on the genreal reasoning capabilities of LLMs to generate candidate actions. However, prompt-based methods suffer not only from bias but also from limited action coverage, making them ineffective in social deduction games like Werewolf. While some approaches~\cite{xu2023language} partially address the bias issue in language agents, it remains an ongoing challenge, and the coverage issue is largely unaddressed, restricting the diversity and strategic depth of generated actions. This raises an important question: \textbf{can we develop a method that leverages LLMs’ specialized reasoning for decision-making while also enhancing their generalization capabilities to improve both bias and action coverage?}}
On the other hand, many language agents built directly on commercial LLM APIs have their own limitations. 
% \yw{I think it is better to first mention pure API/prompting-based methods. Those fine-tuning methods can be mentioned in the very end since it is too expensive. }
% Some methods~\cite{chen2023fireact,wu2024enhance} fine-tune the LLM to excel in a specific task but demand expensive human labor for high-quality fine-tuning data. 
These methods typically rely on prompt engineering without training the base LLM, which means their success depends entirely on the general reasoning capabilities of LLMs to generate actions. 
% Unfortunately, prompt-based methods suffer from intrinsic bias and limited coverage in their generated actions~\cite{xu2023language}, making them ineffective in social deduction games like Werewolf. 
Unfortunately, prompt-based methods suffer from intrinsic bias in their generated actions~\cite{xu2023language}, making them ineffective in social deduction games like Werewolf. Moreover, these methods face the issue of action space coverage because they fully rely on the LLM to generate actions. Since the LLM is a general model that is not fine-tuned, the possible strategy space can be largely under-explored. Some work~\cite{chen2023fireact,wu2024enhance} mitigate these issues by fine-tuning the LLM for a specific task, which requires expensive human labor for high-quality data.
% yw{I think it would be better if you can explain the coverage issue with more concise words. E.g., ``Prompt-based methods fully rely on the LLM to generate possible action candidates. Since LLM is a general model and never fine-tuned, the possible strategy space can be largely under-explored.''}
% \yw{you can just say ``Xu et al. partially tackles the bias issue by additionally training a small network to calibrate the LLM output distribution. However, it still relies on a fixed LLM API to produce action candidates, leaving the coverage issue unaddressed''. sth like this }
\citet{xu2023language} partially tackles the bias issue by additionally training a small network to calibrate the LLM output distribution. However, it still relies on a fixed LLM API to produce action candidates, leaving the coverage issue unaddressed.
% While some approaches~\cite{xu2023language} partially address the bias issue in language agents, the coverage issue is largely unaddressed, restricting the diversity and strategic depth of generated actions.
% Some methods~\cite{chen2023fireact,wu2024enhance} fine-tune the LLM to excel in a specific task but demand expensive human labor for high-quality fine-tuning data. 
This raises an important question: \emph{Can we have a method that leverages both the specialized reasoning capabilities for decision-making and the generalization capabilities of LLMs?}


% While LLM-based agents have shown promising results in many tasks, they typically require enormous amounts of high-quality training data \yw{This claim is a bit questionable. people may say that we only need 10K samples to fine tune an LLM. I think a better argument would be sth like "expensive human labor for high-quality fine-tuning data"? the reason we use CFR is because we do not need human labor. This is a great thing.} to master the intricate deception and inference skills crucial in Werewolf. Additionally, the biases in pre-trained data can negatively impact strategic decision-making \yw{We can cite our paper saying that even in the state-of-the-art LLMs, there exist bias due to balablala. The critical point is to say, (1) fine-tune is expensive; (2) directly running GPT model isn't sufficient. }, and prompt engineering alone may not capture the full spectrum of game-specific tactics.

% However, designing an effective AI agent that can interact with human players in Werewolf or similar free-form language environments is still challenging. 
% On the one hand, classical approaches like Counterfactual Regret Minimization (CFR) and reinforcement learning (RL) have proven successful in games like Go and Poker, thanks to their ability to handle large but structured action spaces. Unfortunately, the vastness of free-form language actions renders a direct application of such methods computationally infeasible. Mapping every possible utterance to an action in the original text space becomes prohibitively large, leading to immense difficulty in strategy representation and equilibrium-finding. On the other hand, a purely large language model (LLM)–based approach faces its own limitations. While LLM-based agents have shown promising results in many tasks, they typically require enormous amounts of high-quality training data \yw{This claim is a bit questionable. people may say that we only need 10K samples to fine tune an LLM. I think a better argument would be sth like "expensive human labor for high-quality fine-tuning data"? the reason we use CFR is because we do not need human labor. This is a great thing.} to master the intricate deception and inference skills crucial in Werewolf. Additionally, the biases in pre-trained data can negatively impact strategic decision-making \yw{We can cite our paper saying that even in the state-of-the-art LLMs, there exist bias due to balablala. The critical point is to say, (1) fine-tune is expensive; (2) directly running GPT model isn't sufficient. }, and prompt engineering alone may not capture the full spectrum of game-specific tactics.


% In this work, we propose an Iterative Latent Strategy Space CFR framework \yw{maybe a name?} tailored for free-form language games, taking Werewolf as the primary example. 
In this work, we propose an iterative Latent Space Policy Optimization (LSPO) framework to build strategic language agents for free-form language games, taking Werewolf as our testbed. 
% the primary example\yw{if you say \emph{primary example}, which means you have \emph{minor example}. Do you really have a minor example? if no then just say we consider werewolf as our testbed. }. 
% Our approach bridges the gap between purely rule-based game solvers and purely data-driven language models by introducing a discretized latent strategy space. \yw{why do you want to bridge the gap? and what is the gap???? if you want to use the wording \emph{bridge the gap}, then you need to clearly state the gap in the previous paragraph.}
% \gf{Our approach combines structured game-solving techniques with language models by introducing a discretized latent strategy space. Purely rule-based solvers struggle in language games with free-form text action spaces, while data-driven language models lack strategic depth, making it difficult to balance reasoning and natural language generation. To address this, }
% Purely rule-based solvers struggle in language games with free-form text action spaces, while data-driven language models lack strategic depth, making it difficult to balance reasoning and natural language generation.
Our approach combines structured game-solving techniques with language models by introducing a discrete latent strategy space.
More specifically, we first map the free-form utterances into a manageable, discrete representation, where we can effectively apply CFR to discover near-optimal strategies. Then, we map the latent strategies back to natural language to generate dialogues, which are used to fine-tune the large language model via Direct Preference Optimization (DPO)~\cite{rafailov2024direct} and expand the latent space.
% \yw{just say DPO is okay. DPO is an algorithm not really a mechanism.}
By iterating between these latent space CFR steps and LLM fine-tuning, our method yields an evolving agent that addresses both the intrinsic bias issue with game-solving techniques and the action coverage issue with latent space expansion, leading to strong performance in the Werewolf game.
% \yw{can we put a concrete number here?}
% \yw{I feel if you can put a tiny conclusion here, the texts can read better.}

We perform extensive experiments in the Werewolf game to demonstrate the effectiveness of our LSPO framework. We first analyze how the latent strategy space evolves between iterations to show that our agents learn increasingly complex and strategic behaviors. Then we quantitatively evaluate the prediction accuracy and win rate of our LSPO agent to show their improving performance with respect to iterations. Next, we compare our agents against state-of-the-art Werewolf agents and find that the LSPO agent achieves the highest win rate. We also conduct ablation studies to assess the effectiveness of our design in the LSPO framework.

