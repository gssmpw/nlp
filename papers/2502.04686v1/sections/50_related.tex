\textbf{Large Language Model-Based Agents.}

% \paragraph{Agents Powered by Large Language Models.}
Recent advancements in large language models (LLMs) have led to the development of agents capable of performing complex tasks across various domains, such as web interactions~\cite{nakano2021webgpt,yao2022webshop,deng2023mind2web}, code generation~\cite{chen2021evaluating,yang2024swe}, gaming environments~\cite{huang2022language,wang2023describe,wang2023voyager,ma2023large}, real-world robotics~\cite{ahn2022can,huang2022inner,vemprala2023chatgpt}, and multi-agent systems~\cite{park2023generative,li2023camel,chen2023agentverse}.
A common approach in these works is to exploit the reasoning capabilities and in-context learning of LLMs to improve decision-making processes.
Chain-of-Thought (CoT) prompting~\cite{wei2022chain} has been instrumental in enabling LLMs to perform step-by-step reasoning.
Building upon this, ReAct~\cite{yao2022react} synergizes reasoning and action to enhance performance across various tasks.
Subsequent research has incorporated self-reflection~\cite{shinn2023reflexion} and strategic reasoning~\cite{gandhi2023strategic} to further refine agent behaviors.
However, these methods can still suffer from the intrinsic biases and coverage issue of LLM-based agents, leading to suboptimal performance in complex games. A representative method that addresses these issues in the game of Diplomacy is Cicero~\cite{meta2022human}, which first uses a strategic module to produce action intent and then generates action-conditioned natural languages with a dialogue module. However, Diplomacy is a board game with finite action space and does not have the action coverage issue, making it not suitable for free-form language games with unbounded text action space.

Due to the high demand for both advanced communication skills and strategic reasoning, social deduction games like Werewolf and Avalon have been proposed as testbeds to build language agents with strategic ability. 
Earlier attempts to create agents for these games often rely on predefined protocols or limited communication capabilities~\cite{wang2018application}, restricting their effectiveness.
% DeepRole~\cite{serrino2019finding} utilizes CFR with deductive reasoning to play Avalon but does not consider natural language communication.
Recent works have explored using LLMs to enable natural language interactions in these games.
For instance, \citet{xu2023exploring} developed a prompt-based Werewolf agent that uses heuristic information retrieval and experience reflection.
Similarly, ReCon~\cite{wang2023avalon} introduced a prompt-based method for playing Avalon by considering both the agent's perspective and that of opponents.
However, these LLM-based agents may still be restricted by intrinsic bias and limited coverage of action space, affecting their decision-making quality.
Strategic Language Agent (SLA)~\cite{xu2023language} partially solves these issues by generating diverse action candidates and learning an RL policy to mitigate intrinsic bias. However, this method still relies on a fixed LLM to produce the action candidate, which can fail to address the coverage issue. Our approach mitigate the intrinsic bias by applying game-theoretic methods to optimize policy in a discrete latent space and tackles the coverage issue by iteratively expanding the latent space by aligning the LLM to the latent space policy, leading to strong performance in the Werewolf game.


% \gf{It seems your ICML24 paper is missing here. I'm not sure if there are others that should be discussed as well.}
% However, these LLM-based agents may still be limited by the biases inherent in language models, affecting their decision-making quality. \gf{Should we clarify the concept of “intrinsic bias” in the context of social deduction games?}

\textbf{Game-Theoretic Algorithms.}
Counterfactual Regret Minimization (CFR)~\cite{zinkevich2007regret} is a foundational algorithm for solving imperfect-information games, particularly those involving hidden information and strategic deception like poker~\cite{moravvcik2017deepstack,brown2018superhuman,brown2019superhuman}. The core principle of CFR is to iteratively reduce regret across players’ decision points in the game tree, converging toward strategies that approximate a Nash equilibrium. Subsequent refinements of CFR~\cite{lanctot2009monte,tammelin2014solving,brown2019deep} have expanded its scalability and adaptability to a broader range of scenarios. Of particular note is DeepRole~\cite{serrino2019finding}, which integrates deductive reasoning with CFR to play the social deduction game Avalon without communication. Our method combines CFR with language models by introducing a finite latent strategy space to enable it to solve free-form language games.


Reinforcement learning (RL) methods, on the other hand, have reached remarkable achievements in complex domains like Go~\cite{silver2016mastering,silver2018general} and video games~\cite{vinyals2019grandmaster,berner2019dota}, often surpassing expert human performance. A seminal technique in these successes is self-play and its variants\citep{heinrich2015fictitious,heinrich2016deep,hennes2020neural,xu2023fictitious}, where agents repeatedly train against older versions of themselves to refine their policies. Another prominent line of work is Policy-Space Response Oracles (PSRO)~\citep{lanctot2017unified,muller2019generalized}, an iterative procedure that produces best responses to a growing population of policies in a meta-game. Conceptually, our iterative framework is related to PSRO in that we both solve an abstracted game before enlarging it to approach the full original game. The difference is that PSRO treats newly learned policies as meta-actions to form a normal-form meta-game, whereas our approach clusters free-form language actions into a discrete latent action space to reformulate the original game as an extensive-form game with finite action space.
% Reinforcement learning has achieved remarkable success in games with imperfect information, such as Go~\cite{silver2016mastering,silver2018general}, poker~\cite{moravvcik2017deepstack,brown2019superhuman}, and complex video games~\cite{vinyals2019grandmaster,berner2019dota}.
% Techniques like self-play~\cite{heinrich2015fictitious,heinrich2016deep,hennes2020neural,xu2023fictitious} and population-based training~\cite{lanctot2017unified,muller2019generalized} have been pivotal in these achievements.
% Counterfactual Regret Minimization (CFR)~\cite{zinkevich2007regret,lanctot2009monte,tammelin2014solving,brown2019deep} is another powerful method that iteratively minimizes regret to approximate Nash equilibria in extensive-form games.
% While previous works have applied these methods to environments with well-defined action spaces, integrating them with LLMs to handle the unbounded 
% \gf{natural language actions} %action space of natural language 
% in games like Werewolf 
% \gf{remains a significant challenge.} %presents new challenges.
% \gf{To address this,} our approach leverages CFR in conjunction with LLMs to generate optimal action policies within the complex dynamics of the Werewolf game. \gf{I think this part is about the action coverage issue. It would be better to use a consistent terminology.}

