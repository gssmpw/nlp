% In this section, we introduce our proposed Latent Space Policy Optimization (LSPO) framework, designed to address the challenges of free-form language games. The framework combines game-theoretic optimization and large language model (LLM) fine-tuning, enabling the agent to iteratively improve both its strategic reasoning and linguistic expressiveness. Our method consists of four main components, as detailed below.

To tackle the intrinsic bias and the coverage issue,
% \gf{Is it appropriate to claim "address" here? It appears quite strong in comparison to "mitigate".} 
we propose an iterative Latent Space Policy Optimization (LSPO) framework. Our method combines game-theoretic optimization with LLM fine-tuning and operates on an expanding latent strategy space to iteratively improve the agent's decision-making ability and action coverage. As shown in Figure~\ref{fig:overview}, our framework has three components including latent space construction, policy optimization in latent space, and latent space expansion. More implementation details can be found in Appendix~\ref{app:method}.

\subsection{Latent Space Construction}

One of the key challenges in free-form language games like Werewolf is achieving broad coverage of the unbounded text space while maintaining a computationally tractable action representation for game-theoretic methods. To strike a balance between coverage and tractability, we propose to abstract the vast language action space into a finite set of latent strategies, which we then expand over iterations for better coverage. Specifically, our latent space construction in each iteration involves two steps including latent strategy generation and clustering.

% \paragraph{\yc{Step 1: }Latent Strategy Generation.}
\textbf{Latent Strategy Generation.}
In our setting, secret actions and voting actions are already discrete and therefore do not require further abstraction. We focus instead on the free-form discussion actions, which we aim to capture as latent strategies. We assume that each role in the game has the same set of latent strategies across all discussion rounds and collect the latent strategies for each role by letting the current LLM agent self-play as different roles for multiple trajectories.
% \gf{Does it mean one-to-one game play for the same role? Or play a complete game with the identical agent playing different players. I think it would be the latter case.} 
To further improve the coverage of latent strategies, we prompt the LLM to generate $N$ strategically distinct discussion candidates and randomly choose one to execute in the game. This process encourages diversity in the collected discussion actions and generate a set of latent strategies in natural language for each role. 
% \yc{In practice, N is xx. or mention this in experiment 4.1}

% \paragraph{\yc{Step 2: }Latent Strategy Clustering.}
\textbf{Latent Strategy Clustering.}
Although we generate a set of latent strategies for each role, they are still in the form of natural language. To transform them into a discrete latent strategy space, we embed each discussion action into a vector representation using an embedding model such as ``text-embedding-3-small'' that captures its semantic and contextual information. We then apply a simple $k$-means clustering algorithm to partition the embedded utterances into $k$ clusters, where each cluster represents a distinct latent strategy. Clustering reduces the infinite free-form text space to a finite set of abstract strategies, paving the way for subsequent game-theoretic optimization. By interpreting each cluster as a latent action, we can more efficiently search for and optimize strategic policies with minimal sacrifice of coverage of language space. 
% \gf{It would be better to visualize this two-step process with a concrete example. Besides, I'm curious how the number of clustering centers affects the performance, and how you plan to discuss on the "coverage" with this sampling+clustering process.}

% \subsection{Mapping Free-Form Language to Latent Strategy Space}

% A key challenge in free-form language games is the unbounded nature of the language space, which makes traditional game-solving methods computationally infeasible. To address this, we construct a \textit{discrete latent strategy space} by abstracting the free-form language space into a finite representation. 

% \paragraph{Data Generation.}
% To construct the latent strategy space, we first simulate multiple games using the LLM agent as a participant. During these games, the agent generates language utterances in response to various game states. Each state is represented as a combination of the agent’s knowledge, public information, and other players’ observed actions. This process yields a dataset of language utterances paired with their corresponding states, which serves as the basis for latent space construction.

% \paragraph{Latent Space Construction.} 
% Each collected utterance is converted into a high-dimensional vector representation using a pre-trained language model. These embeddings encode the semantic and contextual information of the utterances, enabling clustering based on their strategic similarity.
% We apply a simple K-means clustering algorithm to partition the embeddings into $k$ discrete clusters. Each cluster represents a latent strategy, effectively reducing the infinite free-form language space into a finite, discrete action space. The choice of $k$ is a hyperparameter that controls the granularity of the latent strategies. Clusters are interpreted as abstracted decision options, enabling downstream application of game-theoretic techniques.


\subsection{Policy Optimization in Latent Space}

Another challenge in free-form language games is to address the intrinsic bias in the agent's action distribution. After constructing a discrete latent strategy space, we can reformulate the original game with unbounded language space as an abstracted game with a finite latent strategy space. This reformulation allows us to apply standard game-solving techniques such as Counterfactual Regret Minimization (CFR) or reinforcement learning (RL) methods to learn near-optimal strategies that overcome the intrinsic bias. In our implementation, we employ CFR as the game solver.
% ,\yc{the latter sentence can be deleted} and other game-theoretic methods can also be used in our framework to solve the abstracted game. \gf{You always mention CFR AND RL before but you only employ CFR here. It's a bit weird to ignore the RL side.}

\textbf{Abstracted Game Formulation.} 
To represent the game in a compact, finite form, we replace the free-form discussion actions with the discrete latent strategies from latent space construction. Specifically, in the abstracted game, the secret action and voting action remain the same, and the discussion action is replaced by the latent strategy. The state in the abstracted game is a vector including information like the player's role, secret action, etc., and history of past latent strategies. The transition dynamics and payoff function remain unchanged in the abstracted game. This abstracted representation retains the key strategic elements of the original game while reducing the complexity of the action space, making large-scale game-solving computationally tractable.

\textbf{Optimal Policy Learning.} 
Once the game is represented in this discrete form, we apply CFR to learn a policy and solve the abstracted game. Classical CFR~\cite{zinkevich2007regret} iteratively improves policies by minimizing counterfactual regret $R$ for each information set.
For each iteration $t$, the regret for each action $a$ in the latent space is updated by:
% \yc{a is action or strategy?}
\begin{equation}
R_t(a) = R_{t-1}(a) + u(\sigma_t^a, \sigma_t^{-a}) - u(\sigma_t),
\end{equation}
where $u(\sigma_t^a, \sigma_t^{-a})$ is the utility of taking action $a$ under the current strategy profile $\sigma_t$, and $u(\sigma_t)$ is the utility under the full strategy profile.
% \yc{explain R,t}
We use neural networks to approximate regret value to scale CFR to more complex games and learn a policy for each different role in the Werewolf game. By repeatedly simulating self-play among agents employing Deep CFR in the abstracted game, each role’s policy converges to a near-optimal strategy profile. The resulting latent space policies address the intrinsic bias in action distribution and achieve strong strategic play in the abstracted game.


% \subsection{CFR in Latent Strategy Space}

% Once the discrete latent strategy space is constructed, we perform Counterfactual Regret Minimization (CFR) in this abstracted game environment. 

% \paragraph{Abstracted Game Representation.} 
% The original free-form language game is transformed into an abstracted game using the latent strategy space. Each cluster is treated as a discrete action, and transitions between states are governed by the latent strategies of all players. rewards are calculated based on the success or failure of the agent’s role-specific objectives.

% \paragraph{Optimal Strategy Learning.}

% CFR minimizes cumulative regret by iteratively updating the agent’s strategy profile. For each iteration $t$, the regret for each latent strategy $a$ is updated as follows:
% \begin{equation}
% R_t(a) = R_{t-1}(a) + u(\sigma_t^a, \sigma_t^{-a}) - u(\sigma_t),
% \end{equation}
% where $u(\sigma_t^a, \sigma_t^{-a})$ is the utility of taking action $a$ under the current strategy profile $\sigma_t$, and $u(\sigma_t)$ is the utility under the full strategy profile. Strategies are adjusted based on the cumulative regret to converge toward a Nash equilibrium in the abstracted game.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/latent_space.pdf}
    % \includegraphics[width=0.9\linewidth]{figs/latent_space_temp.pdf}
    \caption{Visualization of the latent space of Werewolf and Seer in different iterations.}
    \label{fig:latent_space}
\end{figure*}

\subsection{Latent Space Expansion}
To further improve the agent’s performance in free-form language games, the latent space must remain sufficiently expressive to cover novel strategies and resist exploitation by out-of-distribution actions. We achieve this by fine-tuning the LLM to align with the learned policy in the abstracted game and then re-generating and expanding the latent strategy space using the fine-tuned LLM. This iterative process progressively increases coverage of the action space, enabling stronger and more robust decision-making.

\textbf{Alignment to Latent Space Policy.}
We employ Direct Preference Optimization (DPO)~\cite{rafailov2024direct} to fine-tune the LLM so that its open-ended language outputs align with the near-optimal strategies derived from the abstracted game. To construct the preference dataset required by DPO, we leverage game trajectories generated during latent space construction. We record the language observation for the LLM agent at each discussion phase as the prompt, and use the $N$ discussion candidates as the response candidates. Each of the discussion candidates can be mapped to one of the latent strategies, and the preference label is determined by the regret value of the latent strategies. Intuitively, a discussion action with a lower regret value is preferred. With this preference dataset, we perform DPO to align the LLM toward the learned policy in the abstracted game for better performance in the original game.

\textbf{Update of Latent Space.}
Once the LLM is fine-tuned, it can produce a broader distribution of actions that reflect the refined policy. We exploit this enhanced generative capacity to expand the latent space in the next iteration. Specifically, we repeat the latent strategy generation and clustering procedures with the fine-tuned LLM to re-generate and expand the latent strategy space. This updated latent space offers increased coverage of potential strategies, enabling subsequent policy optimization to discover previously unexplored high-reward actions. Through iterative alignment and expansion, the agent continually refines its discussion strategies and achieves strong play in the free-form language game. 
% \gf{I would expect there are concrete examples to show how the strategy set covers more actions during the iteration.}


% \subsection{Mapping Back to Language Space}

% To deploy the learned strategies in the original game, we map the latent strategies back to the free-form language space. This process involves generating natural language utterances that align with the latent strategies.

% \paragraph{Language Generation via Prompting.} 
% For each latent strategy cluster, we identify representative utterances from the original dataset. These examples are used as prompts to guide the LLM in generating new utterances. The representative examples capture the strategic intent of the cluster, ensuring that the generated language aligns with the optimal latent strategies.

% \paragraph{Preference Learning with Regret Values.} 
% To further refine the LLM, we use the regret values computed during CFR to construct preference pairs for fine-tuning. Specifically, utterances generated from clusters with lower regret values are preferred over those with higher regret values. These preference pairs are used to fine-tune the LLM via Direct Preference Optimization (DPO), improving its ability to produce strategic language aligned with the learned strategies.

% \subsection{Iterative Refinement}

% After fine-tuning the LLM, the updated model is used to generate new language data, repeating the process outlined above. This iterative refinement allows the agent to continually improve both the latent strategy space and the language model's performance.