\subsection{Hyperparameters}

For latent space construction, we let the LLM agent play $1000$ games to collect all discussion actions generated by each role in these games. For diverse action generation, we prompt the LLM to generate $3$ action candidates and randomly select one to execute in the game. We pair the language observation with the $3$ action candidates to use for preference-based fine-tuning in the following components. For sentence embedding, we use OpenAI's ``text-embedding-3-small'' embedding API to embed the sentence to a vector of $1536$ dimensions. Then we apply standard $k$-means clustering to cluster the embedding and get the discrete latent strategy space. The number of clusters $k$ in the first iteration is $3$ for the Werewolf and $2$ for the Seer, Doctor, and Villagers. In each iteration, we add $1$ cluster to the existing clusters. That is, if the first iteration has $k$ clusters, then the $i$-th iteration has $k + i - 1$ clusters.

For policy optimization in latent space, we use a learning rate of $1\times10^{-3}$ to train a Deep CFR network. The buffer size of each role's model is $5\times10^5$, and each model is trained for $1500$ iterations with batch size $4096$ using the Adam optimizer. 

For latent space expansion, we apply DPO with $\beta=0.1$, learning rate $1\times10^{-6}$, and trained for $2$ epoch with batch size $64$.


\subsection{Counterfactual Regret Minimization}

Counterfactual Regret Minimization (CFR) (\cite{zinkevich2007regret}) is a self-play algorithm, and each player continuously updates their strategies according to regret matching to achieve a Nash equilibrium.

We use the following notation. $Z$ is the set of all the end states $z$. $h\sqsubset z$ means state $h$ is a prefix of state $z$, that is, $z$ can be achieved from $h$. $\pi_p^\sigma$ is the probability contribution of the player $p$, and $\pi^\sigma = \prod_p \pi_p^\sigma$. $\pi_{-p}^\sigma$ is the probability contribution of all players except player $p$. $u_p(z)$ is the utility function for the player $p$ in the state $z$.

Counterfactual value for a state $h$ and a player $p$ according to startegy $\sigma$ is defined as:
\begin{equation}
    v_{p}^{\sigma}(h) = \sum_{z\in Z, h \sqsubset z} \pi^\sigma_{-p}(h)\pi^\sigma(z|h)u_p(z).
\end{equation}
The regret for a action $a$ in state $h$ for player $p$ is defined as: $v_p^{\sigma|_{h\to a}}(h) - v_{p}^{\sigma}(h)$, where $\sigma|_{h\to a}$ is same to $\sigma$ except in state $h$ the player will choose action $a$. The regret matching is choosing the strategy according to sum of previous regret values defined as $R(h,a)$, then the new strategy $\sigma(h,a) = \frac{R(h,a)^+}{\sum_{a'} R(h,a')^+}$, $R(h,a)^+ = \max(0,R(h,a))$. If $\sum_{a'} R(h,a')^+ =0$, just set $\sigma$ to be uniform random.

Because the game tree is very big, it is impossible to traverse the entire tree,  our implementation is based on deep CFR (\cite{brown2019deep}). We use a neural network to fit observation to regret value. The amount of computation required to search for only one player is also unacceptable, so a restriction is added based on deep CFR. If the number of layers currently searched is too large, the previous strategy is directly used to sample the actions of all players until the end of the game and return the utility for each player in that state. The complete process can be seen as running some complete game trajectories, and then starting from each intermediate node, searching a few layers to do CFR.

\subsection{Baseline Implementation}
ReAct, ReCon, and SLA are implemented following the original paper. The Cicero-like agent predefines a set of high-level atomic actions and trains an RL policy with this fixed action space. 
The RL policy takes the embeddings of the information record and deduction result as input and selects the atomic action based on this input.
Then the natural language actions used in gameplay are generated by prompting the LLM to follow the selected atomic actions.
In our case, the atomic action set consists of 13 actions including ``idle'', ``target player\_0'', ``target player\_1'', ``target player\_2'', ``target player\_3'', ``target player\_4'', ``target player\_5'', ``target player\_6'', ``claim to be a Werewolf'', ``claim to be a Seer'', ``claim to be a Doctor'', ``claim to be a Villager'', and ``do not reveal role''. 
