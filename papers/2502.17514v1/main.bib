@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{team2024chameleon,
  title={Chameleon: Mixed-modal early-fusion foundation models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@misc{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. In arXiv [cs. CL]. arXiv},
  author={Team, Gemini and Reid, M and Savinov, N and Teplyashin, D and Dmitry, Lepikhin and Lillicrap, T and Alayrac, JB and Soricut, R and Lazaridou, A and Firat, O and others},
  year={2024}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{zhang2023towards,
  title={Towards best practices of activation patching in language models: Metrics and methods},
  author={Zhang, Fred and Nanda, Neel},
  journal={arXiv preprint arXiv:2309.16042},
  year={2023}
}

@misc{maniparambil2024visionlanguageencodersrepresent,
      title={Do Vision and Language Encoders Represent the World Similarly?}, 
      author={Mayug Maniparambil and Raiymbek Akshulakov and Yasser Abdelaziz Dahou Djilali and Sanath Narayan and Mohamed El Amine Seddik and Karttikeya Mangalam and Noel E. O'Connor},
      year={2024},
      eprint={2401.05224},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.05224}, 
}

@misc{gurnee2024languagemodelsrepresentspace,
      title={Language Models Represent Space and Time}, 
      author={Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2310.02207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.02207}, 
}

@misc{huang2021makesmultimodallearningbetter,
      title={What Makes Multi-modal Learning Better than Single (Provably)}, 
      author={Yu Huang and Chenzhuang Du and Zihui Xue and Xuanyao Chen and Hang Zhao and Longbo Huang},
      year={2021},
      eprint={2106.04538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.04538}, 
}


@misc{driess2023palmeembodiedmultimodallanguage,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.03378}, 
}

@misc{tong2024eyeswideshutexploring,
      title={Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs}, 
      author={Shengbang Tong and Zhuang Liu and Yuexiang Zhai and Yi Ma and Yann LeCun and Saining Xie},
      year={2024},
      eprint={2401.06209},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.06209}, 
}

@misc{stan2024lvlminterpretinterpretabilitytoollarge,
      title={LVLM-Interpret: An Interpretability Tool for Large Vision-Language Models}, 
      author={Gabriela Ben Melech Stan and Estelle Aflalo and Raanan Yehezkel Rohekar and Anahita Bhiwandiwalla and Shao-Yen Tseng and Matthew Lyle Olson and Yaniv Gurwicz and Chenfei Wu and Nan Duan and Vasudev Lal},
      year={2024},
      eprint={2404.03118},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.03118}, 
}

@misc{huh2024platonicrepresentationhypothesis,
      title={The Platonic Representation Hypothesis}, 
      author={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},
      year={2024},
      eprint={2405.07987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07987}, 
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@misc{gandelsman2024interpretingclipsimagerepresentation,
      title={Interpreting CLIP's Image Representation via Text-Based Decomposition}, 
      author={Yossi Gandelsman and Alexei A. Efros and Jacob Steinhardt},
      year={2024},
      eprint={2310.05916},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.05916}, 
}

@misc{palit2023visionlanguagemechanisticinterpretabilitycausal,
      title={Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP}, 
      author={Vedant Palit and Rohan Pandey and Aryaman Arora and Paul Pu Liang},
      year={2023},
      eprint={2308.14179},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.14179}, 
}
@misc{anthropic2024claude,
  author = {Anthropic},
  title = {Introducing the next generation of Claude},
  year = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}},
  note = {Available online}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{OpenAI2024gpt4o,
  author = {OpenAI},
  title = {Hello GPT-4o},
  year = {2024},
  howpublished = {\url{https://openai.com/index/hello-gpt-4o/}},
  note = {Available online}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{bordes2024introduction,
  title={An introduction to vision-language modeling},
  author={Bordes, Florian and Pang, Richard Yuanzhe and Ajay, Anurag and Li, Alexander C and Bardes, Adrien and Petryk, Suzanne and Ma{\~n}as, Oscar and Lin, Zhiqiu and Mahmoud, Anas and Jayaraman, Bargav and others},
  journal={arXiv preprint arXiv:2405.17247},
  year={2024}
}

@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@inproceedings{yuksekgonul2023and,
  title={When and why vision-language models behave like bags-of-words, and what to do about it?},
  author={Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{rai2024practical,
  title={A practical review of mechanistic interpretability for transformer-based language models},
  author={Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  journal={arXiv preprint arXiv:2407.02646},
  year={2024}
}

@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@inproceedings{dravid2023rosetta,
  title={Rosetta neurons: Mining the common units in a model zoo},
  author={Dravid, Amil and Gandelsman, Yossi and Efros, Alexei A and Shocher, Assaf},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1934--1943},
  year={2023}
}

@article{wang2022interpretability,
  title={Interpretability in the wild: a circuit for indirect object identification in gpt-2 small},
  author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@article{tigges2024llm,
  title={LLM Circuit Analyses Are Consistent Across Training and Scale},
  author={Tigges, Curt and Hanna, Michael and Yu, Qinan and Biderman, Stella},
  journal={arXiv preprint arXiv:2407.10827},
  year={2024}
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@misc{sharkey2022features,
  title={Taking features out of superposition with sparse autoencoders},
  author={Sharkey, Lee and Braun, Dan and Millidge, Beren},
  year={2022},
  note={[Interim research report]},
  month={Dec},
  day={13},
  url={https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition}
}

@misc{peigne2023features,
  title={Taking features out of superposition with sparse autoencoders more quickly with informed initialization},
  author={Peign{\'e}, Pierre},
  year={2023},
  month={Sep},
  day={24},
  url={https://www.lesswrong.com/posts/YJpMgi7HJuHwXTkjk/taking-features-out-of-superposition-with-sparse}
}

@article{yun2021transformer,
  title={Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  author={Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A and LeCun, Yann},
  journal={arXiv preprint arXiv:2103.15949},
  year={2021}
}

@article{bricken2023towards,
  title={Towards monosemanticity: Decomposing language models with dictionary learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others},
  journal={Transformer Circuits Thread},
  volume={2},
  year={2023}
}

@article{quantmeyer2024and,
  title={How and where does CLIP process negation?},
  author={Quantmeyer, Vincent and Mosteiro, Pablo and Gatt, Albert},
  journal={arXiv preprint arXiv:2407.10488},
  year={2024}
}

@inproceedings{palit2023towards,
  title={Towards vision-language mechanistic interpretability: A causal tracing tool for blip},
  author={Palit, Vedant and Pandey, Rohan and Arora, Aryaman and Liang, Paul Pu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2856--2861},
  year={2023}
}

@article{feng2024monitoring,
  title={Monitoring latent world states in language models with propositional probes},
  author={Feng, Jiahai and Russell, Stuart and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2406.19501},
  year={2024}
}

@article{stan2024lvlm,
  title={LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models},
  author={Stan, Gabriela Ben Melech and Rohekar, Raanan Yehezkel and Gurwicz, Yaniv and Olson, Matthew Lyle and Bhiwandiwalla, Anahita and Aflalo, Estelle and Wu, Chenfei and Duan, Nan and Tseng, Shao-Yen and Lal, Vasudev},
  journal={arXiv preprint arXiv:2404.03118},
  year={2024}
}

@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@inproceedings{zhu2024minigpt,
  title={Mini{GPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=1tZbq88f27}
}

@inproceedings{wu2024nextgpt,
  title={{NE}xT-{GPT}: Any-to-Any Multimodal {LLM}},
  author={Shengqiong Wu and Hao Fei and Leigang Qu and Wei Ji and Tat-Seng Chua},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=NZQkumsNlf}
}

@article{xie2024show,
  title={Show-o: One single transformer to unify multimodal understanding and generation},
  author={Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2408.12528},
  year={2024}
}

@article{wu2024janus,
  title={Janus: Decoupling visual encoding for unified multimodal understanding and generation},
  author={Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2410.13848},
  year={2024}
}

@article{wang2024emu3,
  title={Emu3: Next-token prediction is all you need},
  author={Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others},
  journal={arXiv preprint arXiv:2409.18869},
  year={2024}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv e-prints},
  pages={arXiv--2308},
  year={2023}
}

@article{panickssery2023steering,
  title={Steering llama 2 via contrastive activation addition},
  author={Panickssery, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  journal={arXiv preprint arXiv:2312.06681},
  year={2023}
}

@article{gao2020pile,
  title={The {P}ile: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{laurencon2023obelics,
      title={OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
      author={Hugo Laurençon and Lucile Saulnier and Léo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
      year={2023},
      eprint={2306.16527},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{chern2024anole,
  title={ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation},
  author={Chern, Ethan and Su, Jiadi and Ma, Yan and Liu, Pengfei},
  journal={arXiv preprint arXiv:2407.06135},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@misc{bloom2024saetrainingcodebase,
   title = {SAELens},
   author = {Joseph Bloom, Curt Tigges and David Chanin},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}

@article{imagenet15russakovsky,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = { {ImageNet Large Scale Visual Recognition Challenge} },
    Year = {2015},
    journal   = {International Journal of Computer Vision (IJCV)},
    doi = {10.1007/s11263-015-0816-y},
    volume={115},
    number={3},
    pages={211-252}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{ji2024align,
  title={Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback},
  author={Ji, Jiaming and Zhou, Jiayi and Lou, Hantao and Chen, Boyuan and Hong, Donghai and Wang, Xuyao and Chen, Wenqi and Wang, Kaile and Pan, Rui and Li, Jiahao and others},
  journal={arXiv preprint arXiv:2412.15838},
  year={2024}
}

@misc{fu2023mme,
  title={MME: a comprehensive evaluation benchmark for multimodal large language models. CoRR abs/2306.13394 (2023)},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  year={2023}
}

@article{chen2023alpagasus,
  title={Alpagasus: Training a better alpaca with fewer data},
  author={Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others},
  journal={arXiv preprint arXiv:2307.08701},
  year={2023}
}

@article{tu2024resofilter,
  title={ResoFilter: Rine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis},
  author={Tu, Zeao and Meng, Xiangdi and He, Yu and Yao, Zihan and Qi, Tianyu and Liu, Jun and Li, Ming},
  journal={arXiv preprint arXiv:2412.14809},
  year={2024}
}

@article{du2023mods,
  title={Mods: Model-oriented data selection for instruction tuning},
  author={Du, Qianlong and Zong, Chengqing and Zhang, Jiajun},
  journal={arXiv preprint arXiv:2311.15653},
  year={2023}
}

@article{li2023one,
  title={One shot learning as instruction data prospector for large language models},
  author={Li, Yunshui and Hui, Binyuan and Xia, Xiaobo and Yang, Jiaxi and Yang, Min and Zhang, Lei and Si, Shuzheng and Liu, Junhao and Liu, Tongliang and Huang, Fei and others},
  journal={arXiv preprint arXiv:2312.10302},
  year={2023}
}

@article{li2023quantity,
  title={From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning},
  author={Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
  journal={arXiv preprint arXiv:2308.12032},
  year={2023}
}

@article{zhou2023lima,
  title={LIMA: Less Is More for Alignment. CoRR abs/2305.11206 (2023)},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  volume={10},
  year={2023}
}

@article{yu2024rlaifv,
  title={RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness}, 
  author={Yu, Tianyu and Zhang, Haoye and Yao, Yuan and Dang, Yunkai and Chen, Da and Lu, Xiaoman and Cui, Ganqu and He, Taiwen and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
  journal={arXiv preprint arXiv:2405.17220},
  year={2024},
}

@article{wu2023next,
  title={Next-gpt: Any-to-any multimodal llm},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2309.05519},
  year={2023}
}

@article{zhang2018visual,
  title={Visual interpretability for deep learning: a survey},
  author={Zhang, Quan-shi and Zhu, Song-Chun},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={19},
  number={1},
  pages={27--39},
  year={2018},
  publisher={Springer}
}

@inproceedings{zhang2018interpretable,
  title={Interpretable convolutional neural networks},
  author={Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8827--8836},
  year={2018}
}