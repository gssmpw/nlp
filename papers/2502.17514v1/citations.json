[
  {
    "index": 0,
    "papers": [
      {
        "key": "OpenAI2024gpt4o",
        "author": "OpenAI",
        "title": "Hello GPT-4o"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "team2024gemini",
        "author": "Team, Gemini and Reid, M and Savinov, N and Teplyashin, D and Dmitry, Lepikhin and Lillicrap, T and Alayrac, JB and Soricut, R and Lazaridou, A and Firat, O and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. In arXiv [cs. CL]. arXiv"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "anthropic2024claude",
        "author": "Anthropic",
        "title": "Introducing the next generation of Claude"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wu2023next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "team2024chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wu2024janus",
        "author": "Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others",
        "title": "Janus: Decoupling visual encoding for unified multimodal understanding and generation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "Qwen-VL",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhu2024minigpt",
        "author": "Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny",
        "title": "Mini{GPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "rai2024practical",
        "author": "Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu",
        "title": "A practical review of mechanistic interpretability for transformer-based language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "olah2020zoom",
        "author": "Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan",
        "title": "Zoom in: An introduction to circuits"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "dravid2023rosetta",
        "author": "Dravid, Amil and Gandelsman, Yossi and Efros, Alexei A and Shocher, Assaf",
        "title": "Rosetta neurons: Mining the common units in a model zoo"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yun2021transformer",
        "author": "Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A and LeCun, Yann",
        "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors"
      },
      {
        "key": "bricken2023towards",
        "author": "Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others",
        "title": "Towards monosemanticity: Decomposing language models with dictionary learning"
      },
      {
        "key": "sharkey2022features",
        "author": "Sharkey, Lee and Braun, Dan and Millidge, Beren",
        "title": "Taking features out of superposition with sparse autoencoders"
      },
      {
        "key": "peigne2023features",
        "author": "Peign{\\'e}, Pierre",
        "title": "Taking features out of superposition with sparse autoencoders more quickly with informed initialization"
      },
      {
        "key": "elhage2022toy",
        "author": "Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others",
        "title": "Toy models of superposition"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cunningham2023sparse",
        "author": "Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee",
        "title": "Sparse autoencoders find highly interpretable features in language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "gao2024scaling",
        "author": "Gao, Leo and la Tour, Tom Dupr{\\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey",
        "title": "Scaling and evaluating sparse autoencoders"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhou2023lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "LIMA: Less Is More for Alignment. CoRR abs/2305.11206 (2023)"
      },
      {
        "key": "chen2023alpagasus",
        "author": "Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others",
        "title": "Alpagasus: Training a better alpaca with fewer data"
      },
      {
        "key": "du2023mods",
        "author": "Du, Qianlong and Zong, Chengqing and Zhang, Jiajun",
        "title": "Mods: Model-oriented data selection for instruction tuning"
      },
      {
        "key": "li2023one",
        "author": "Li, Yunshui and Hui, Binyuan and Xia, Xiaobo and Yang, Jiaxi and Yang, Min and Zhang, Lei and Si, Shuzheng and Liu, Junhao and Liu, Tongliang and Huang, Fei and others",
        "title": "One shot learning as instruction data prospector for large language models"
      },
      {
        "key": "li2023quantity",
        "author": "Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing",
        "title": "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning"
      },
      {
        "key": "tu2024resofilter",
        "author": "Tu, Zeao and Meng, Xiangdi and He, Yu and Yao, Zihan and Qi, Tianyu and Liu, Jun and Li, Ming",
        "title": "ResoFilter: Rine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhou2023lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "LIMA: Less Is More for Alignment. CoRR abs/2305.11206 (2023)"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "chen2023alpagasus",
        "author": "Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others",
        "title": "Alpagasus: Training a better alpaca with fewer data"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2023quantity",
        "author": "Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing",
        "title": "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhou2023lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "LIMA: Less Is More for Alignment. CoRR abs/2305.11206 (2023)"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "chen2023alpagasus",
        "author": "Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others",
        "title": "Alpagasus: Training a better alpaca with fewer data"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "li2023quantity",
        "author": "Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing",
        "title": "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning"
      }
    ]
  }
]