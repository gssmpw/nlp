\section{Related Work}
\label{sec:Related Work}

\paragraph{Multimodal Large Language Model}

Bommasani et al., "On the Opportunities and Risks of Foundation Models" ____ is a type of LLM integrated with multimodal modules that incorporate multimodal information to deal with multimodal tasks. Based on the method of integrating vision features into the model, most ____s can be categorized into three types:
% Some state-of-the-art ____s have already achieved significant capabilities in multimodal understanding and generation, such as GPT-4o ____ , Gemini-1.5 ____ , and Claude3-opus ____ .

 
\begin{itemize}
    \item \textit{CLIP-based ____s}: These models encode images with CLIP ____ and use MLP to project visual features. Examples include LLaVA ____ series and NExT-GPT ____.
    \item \textit{Early-Fusion ____s}: These models directly tokenize visual features for input. Examples include Chameleon ____ and Janus ____ series.
    \item \textit{Q-Former-based MLLMs}: These models use a structure similar to Q-Former ____ to extract visual representations, represented by Qwen-VL ____ and MiniGPT-4 ____.
\end{itemize}

Our study focuses on the CLIP-based and early-fusion ____s. Specifically, we select LLaVA-NeXT-7B and Chameleon-7B as the target models.

\paragraph{Mechanistic interpretability with Sparse Autoencoder}

Mechanistic interpretability seeks to uncover and explain the internal mechanisms that enable models to understand input data and generate responses ____ . Specifically, most current mechanistic interpretability methods focus on analyzing features, smaller units that contribute to performing explainable semantic tasks, within models ____ .
% In terms of vision features interpretability, ____ discovered the presence of 'Rosetta Neurons' across multiple models, which suggests that different neural networks share similar representations.

Sparse Autoencoder (Hinton et al., "A Sparsity-Boosting Autoencoder for Deep Learning") ____ aims to learn sparse and interpretable features from polysemantic model representations ____ . By introducing sparsity constraints, the activation values in the hidden layers of ____ are mostly zero, allowing ____ to encode polysemantic features in LLM to monosemantic ones. 
% ____ firstly utilized ____ to decipher interpretable features from small LLMs. ____ trained an ____ on GPT-4, proposing a recipe for training ____s on LLM and measures to evaluate ____ metrics. 

In this paper, we extended the scope of ____ to ____s, thereby building ____ . We further demonstrated ____'s capability and transferability on ____s, and built a data filter tool based on ____ to enhance multimodal alignment.

\paragraph{Data Filter in Alignment}

Data filtering ensures that only relevant high-quality data are used during the alignment of LLM or ____ s, thus reducing the quantity of data while achieving greater performance ____ . For example, LIMA ____ , ALPAGASUS ____ , and IFD ____ use human annotation, API annotation and train a new model for annotation to score data separately. Our method, ____ -based data filter, provides a self-guided and interpretable metric to evaluate the similarity of multimodal data, which indicates their qualities. The method is stable and efficient for models of different architectures.
% For example:

% \begin{itemize}
%     \item LIMA ____ emphasizes the importance of quality over quantity in data filtering, demonstrating that effective data filtering can unlock the latent capabilities of pretrained LLMs.
%     \item ALPAGASUS ____ integrates ChatGPT into the data filter pipeline, achieving superior performance while significantly reducing training time.
%     \item IFD ____ is a self-guided metric to evaluate the alignment difficulty of data samples. The IFD score measures the degree of assistance that instructions provide in generating aligned outputs.
% \end{itemize}