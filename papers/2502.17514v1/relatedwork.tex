\section{Related Work}
\label{sec:Related Work}

\paragraph{Multimodal Large Language Model}

\model{} is a type of LLM integrated with multimodal modules that incorporate multimodal information to deal with multimodal tasks. Based on the method of integrating vision features into the model, most \model{}s can be categorized into three types:
% Some state-of-the-art \model{}s have already achieved significant capabilities in multimodal understanding and generation, such as GPT-4o \cite{OpenAI2024gpt4o}, Gemini-1.5 \cite{team2024gemini}, and Claude3-opus \cite{anthropic2024claude}. 

 
\begin{itemize}
    \item \textit{CLIP-based \model{}s}: These models encode images with CLIP \cite{radford2021learning} and use MLP to project visual features. Examples include LLaVA \cite{liu2024visual} series and NExT-GPT \cite{wu2023next}.
    \item \textit{Early-Fusion \model{}s}: These models directly tokenize visual features for input. Examples include Chameleon \cite{team2024chameleon} and Janus \cite{wu2024janus} series.
    \item \textit{Q-Former-based MLLMs}: These models use a structure similar to Q-Former \cite{li2023blip} to extract visual representations, represented by Qwen-VL \cite{Qwen-VL} and MiniGPT-4 \cite{zhu2024minigpt}.
\end{itemize}

Our study focuses on the CLIP-based and early-fusion \model{}s. Specifically, we select LLaVA-NeXT-7B and Chameleon-7B as the target models.

\paragraph{Mechanistic interpretability with Sparse Autoencoder}

Mechanistic interpretability seeks to uncover and explain the internal mechanisms that enable models to understand input data and generate responses \cite{rai2024practical}. Specifically, most current mechanistic interpretability methods focus on analyzing features, smaller units that contribute to performing explainable semantic tasks, within models \cite{olah2020zoom}.
% In terms of vision features interpretability, \citet{dravid2023rosetta} discovered the presence of 'Rosetta Neurons' across multiple models, which suggests that different neural networks share similar representations.

Sparse Autoencoder (\old{}) aims to learn sparse and interpretable features from polysemantic model representations \cite{yun2021transformer,bricken2023towards,sharkey2022features,peigne2023features, elhage2022toy}. By introducing sparsity constraints, the activation values in the hidden layers of \old{} are mostly zero, allowing \old{} to encode polysemantic features in LLM to monosemantic ones. 
% \citet{cunningham2023sparse} firstly utilized \old{} to decipher interpretable features from small LLMs. \citet{gao2024scaling} trained an \old{} on GPT-4, proposing a recipe for training \old{}s on LLM and measures to evaluate \old{} metrics. 

In this paper, we extended the scope of \old{} to \model{}s, thereby building \ours{}. We further demonstrated \ours{}'s capability and transferability on \model{}s, and built a data filter tool based on \ours{} to enhance multimodal alignment.

\paragraph{Data Filter in Alignment}

Data filtering ensures that only relevant high-quality data are used during the alignment of LLM or \model{} s, thus reducing the quantity of data while achieving greater performance \cite{zhou2023lima,chen2023alpagasus,du2023mods,li2023one,li2023quantity,tu2024resofilter}. For example, LIMA \cite{zhou2023lima}, ALPAGASUS \cite{chen2023alpagasus}, and IFD \cite{li2023quantity} use human annotation, API annotation and train a new model for annotation to score data separately. Our method, \ours{}-based data filter, provides a self-guided and interpretable metric to evaluate the similarity of multimodal data, which indicates their qualities. The method is stable and efficient for models of different architectures.
% For example:

% \begin{itemize}
%     \item LIMA \cite{zhou2023lima} emphasizes the importance of quality over quantity in data filtering, demonstrating that effective data filtering can unlock the latent capabilities of pretrained LLMs.
%     \item ALPAGASUS \cite{chen2023alpagasus} integrates ChatGPT into the data filter pipeline, achieving superior performance while significantly reducing training time.
%     \item IFD \cite{li2023quantity} is a self-guided metric to evaluate the alignment difficulty of data samples. The IFD score measures the degree of assistance that instructions provide in generating aligned outputs.
% \end{itemize}



%self guide
%stable
%interp
%efficiency