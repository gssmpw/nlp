\section{Proof of Lemmas for Theorem~\ref{thm:general}}
\label{section:kac_rice}

\subsection{Derving the Kac-Rice equation on the parameter manifold: Proof of Lemma~\ref{prop:kac_rice_manifold}}
\label{sec:proof_of_kac_rice_on_manifold}
The goal of this section is to verify the generalization of the Kac-Rice integral to our setting and derive Proposition~\ref{prop:kac_rice_manifold} of Section~\ref{sec:pf_thm1_kr_integral}.
We continue suppressing the arguments in the definitions whenever it doesn't not cause confusion. For instance, we will often write $\bL$ for $\bL(\bbV;\bw)$ and $\cM$ for $\cM(\cuA,\cuB,\sPi).$


\subsubsection{Some properties of the parameter manifold and the gradient process}

We begin with the following lemma establishing basic regularity conditions of the manifold $\cM$.
\label{sec:properties_manifold}
\begin{lemma}[Regularity of the parameter manifold]
\label{lemma:manifold_dim}
Assume that $\ell,\rho$ and $(\cuA,\cuB)$ satisfy Assumptions~\ref{ass:loss},~\ref{ass:regularizer} and~\ref{ass:sets}, respectively. 
Then for $\sfsigma_\bR,\sfsigma_\bV,\sfsigma_\bL,\sfsigma_\bG \ge 0$,
$\cM(\cuA,\cuB,\sPi)$ is a differentiable manifold of dimension ${dk + (n-k)(k+k_0)}$, and in particular is bounded and orientable. 
\end{lemma}
\begin{proof}
Let 
\begin{align}
    \cV_0 :=\Big\{
   (\bTheta,\bbV) &: \hmu\in\cuA,\;
   \hnu\in\cuB,\; 
   \sfA_\bR\succ \bR(\hmu_{\bTheta,\bTheta_0}) \succ\sfsigma_\bR,\;\\
   &\quad 
\sfA_\bV \succ \E_{\hnu}[\bv\bv^\sT] \succ \sfsigma_{\bV}, \;
\E_{\hnu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;
\sigma_{\min}\left( \bJ_{(\bbV,\bTheta)} \bG\right) > n\,\sfsigma_{\bG}
    \Big\}.
\end{align}
We first prove that $\cV_0$ is a bounded open set. To do so, given that $\cuA$ and $\cuB$ are open under the weak convergence topology, it suffices to show that 
    $\sigma_{\min}(\bJ_{\bbV,\bTheta} \bG(\bbV,\bTheta)) > n \sfsigma_{\bG}$
forms an open set. This follows directly from the regularity properties given in the assumptions: since the second order partial derivatives of the form $\partial_{i,j}\ell$, $i\in[k+k_0],j\in[k]$ are, by Assumption~\ref{ass:loss}, continuous, 
and the regularizer $\rho_0$ is second order continuously differentiable by Assumption~\ref{ass:regularizer}, $\bG$ is continuously differentiable, and hence the constraint defined by 
$\sigma_{\min}(\bJ_{\bbV,\bTheta} \bG(\bbV,\bTheta)) > n \sfsigma_{\bG}$ for any $\sfsigma_\bG \ge 0$ indeed defines an open set. Checking the remaining constraints defining $\cV_0$ to define an open set is straightforward.
Now note that $\bzero$ is a regular value of $\bG(\bbV,\bTheta)$ when restricted to $\cV_0$ for any $\sfsigma_\bG \ge 0$, which implies $\cM = \{(\bTheta,\bbV):\; (\bTheta,\bbV)\in\cV_0, \; \bG(\bTheta,\bbV) = \bzero\}$ is a bounded regular level set, and hence is a bounded oriented differentiable manifold. The dimension can then be found to be $dk + (n-k)(k+k_0)$ by dimension counting.
\end{proof}

Next, we move on to studying the null space of the covariance of $\bzeta$ and showing that its degeneracy is constant in $(\bTheta,\bbV) \in\cM$. 
First, by straightforward computations, we obtain the following for the mean $\bmu(\bTheta,\bbV)$ and covariance $\bSigma(\bTheta,\bbV)$ of $\bzeta(\bTheta,\bbV)$:

\begin{equation}
 \bmu := [\brho_1^\sT,\dots,\brho_k^\sT,-\bv_1^\sT,\dots,-\bv_k^\sT, -\bv_{0,1}^\sT,\dots,-\bv_{0,k_0}^\sT]^\sT,
\quad 
\bSigma:= 
   \begin{bmatrix}
       \bL^\sT \bL \otimes \bI_{d}  & \bM & \bM_0\\
       \bM^\sT  & \bTheta^\sT \bTheta \otimes \bI_n & \bTheta^\sT\bTheta_0 \otimes \bI_n\\
       \bM_0^{\sT} & \bTheta_0^\sT\bTheta  \otimes \bI_n& \bTheta_0^{\sT}\bTheta_0 \otimes \bI_n
   \end{bmatrix} ,
\end{equation}
where
\begin{equation}
    \bM :=  \begin{bmatrix}
        \btheta_1\bell_1^\sT & \dots  & \btheta_k \bell_1^\sT\\
        \vdots  &   & \vdots \\
        \btheta_1\bell_k^\sT & \dots  & \btheta_k \bell_k^\sT\\
    \end{bmatrix}
    \in \R^{d k\times n k},\quad
    \bM_0 :=
    \begin{bmatrix}
        \btheta_{0,1}\bell_1^\sT & \dots  & \btheta_{0,k_0} \bell_1^\sT\\
        \vdots  &   & \vdots \\
        \btheta_{0,1}\bell_k^\sT & \dots  & \btheta_{0,k_0} \bell_k^\sT\\
        \end{bmatrix} \in \R^{d{k} \times n k_0 }.
\end{equation}
The following lemma characterizes the nullspace of $\bSigma$.
\begin{lemma}[Eigenvectors of the nullspace of $\bSigma$]
\label{lemma:eig_vecs_NS_Sigma}
Let
% \begin{equation}
% \ba_{i,j} := \ba_{i,j}(\bTheta,\bbV) := \left(
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{j -1}, \btheta_i^\sT, \underbrace{\bzero^\sT,\dots,\bzero^\sT}_{k-j}, 
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{i -1}, -\bell_j^{\sT}, \underbrace{\bzero^\sT,\dots,\bzero^\sT}_{k-i}, 
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k_0}
% \right), \quad i,j \in [k].
% \end{equation}
\begin{equation}
    \ba_{i,j}(\bTheta,\bbV):=
    \big(\be_{k,j}^\sT\otimes\btheta_i^\sT,
    -\be_{k,i}^\sT\otimes\bell_j(\bbV)^\sT,
    \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k_0}
    \big)^\sT,\qquad i,j\in[k],
\end{equation}
% \begin{equation}
% \ba_{0,i,j} \equiv \ba_{0,i,j}(\bbV) := \left(
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{j -1}, \btheta_{0,i}^{\sT}, \underbrace{\bzero^\sT,\dots,\bzero^\sT}_{k-j}, 
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k},
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{i -1}, -\bell_j^{\sT}, \underbrace{\bzero^\sT,\dots,\bzero^\sT}_{k_{0}-i}
% \right),\quad i\in[k_0], j\in[k].
% \end{equation}
\begin{equation}
    \ba_{0,i,j}(\bbV):= \big(\be_{k,j}^\sT\otimes\btheta_{0,i}^\sT,
    \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k},
    -\be_{k_0,i}^\sT\otimes\bell_j(\bbV)^\sT
    \big)^\sT,\qquad i\in[k_0],j\in[k],
\end{equation}
in which $(\be_{k,j})_{j\in[k]}$ and $(\be_{k_0,i})_{i\in[k_0]}$ are the canonical basis vectors for $\R^k$ and $\R^{k_0}$, respectively. 
Then for any $(\bTheta,\bbV) \in \cM(\cuA,\cuB,\sPi)$:
\begin{enumerate}
    \item We have
  \begin{equation}
 \Nul(\bSigma(\bTheta,\bbV)) =   \mathrm{span}\left(\left\{\ba_{i,j}(\bTheta,\bbV): i,j \in [k]\right\}\cup \left\{\ba_{0,i,j}(\bbV): i\in[k_0], j \in[k]  \right\} \right);
  \end{equation}

  \item the collection of basis vectors
$\left\{\ba_{i,j}(\bTheta,\bbV): i,j \in [k]\right\}\cup \left\{\ba_{0,i,j}(\bbV): i\in[k_0], j \in[k]  \right\}$ are linearly independent, and hence, 
\begin{equation}
\dim(\Nul(\bSigma(\bTheta,\bbV))) = k^2 + kk_0,\;\; \rank(\bSigma(\bTheta,\bbV)) 
=dk + (n-k)(k+k_0),
%\quad\textrm{for}\quad (\bTheta,\bbV)\in\cM(\cuA,\cuB,\sPi);
\end{equation}
\item the mean $\bmu(\bTheta,\bbV)$ of $\bzeta(\bTheta,\bbV)$ is orthogonal to $\Nul(\bSigma(\bTheta,\bbV))$.
\end{enumerate}
\end{lemma}


\begin{proof}
Fix $(\bTheta,\bbV)$ throughout and suppress these in the notation.
It is easy to check that $\bSigma \ba_{i,j} = \bzero$ and $\bSigma \ba_{0,i,j} = \bzero$ and hence the span of these vectors is contained in $\Nul(\bSigma)$.
We show the converse. Let $\bc \in\R^{dk}$, $\bd \in \R^{nk + nk_0}$ such that $\bSigma [\bc^\sT, \bd^\sT]^\sT= \bzero$. We then have
\begin{align*}
    \left(\bL^\sT\bL \otimes \bI_d\right) \bc + [\bM, \bM_0] \bd &= \bzero\\
    \left[] \bM, \bM_0\right]^\sT\bc + \left(\bR \otimes \bI_n\right) \bd &= \bzero.
\end{align*}
Let us define the sets
\begin{align*}
    \cS_1:= &\{
    \be_{k,j}\otimes\btheta_i:\; i,j\in[k]
    \}\cup \{\be_{k,j}\otimes \btheta_{0,i} 
    \; : i\in[k_0],j\in[k]\}\subset \R^{dk},\\
    \cS_2:=&\{
    (\be_{k,i}^\sT\otimes\bell_j^\sT,
    \bzero^\sT)^\sT:\; i,j\in[k]\} \cup 
    \{(\bzero^\sT,
    \be_{k_0,i}^\sT\otimes\bell_j^\sT, )^\sT :\;i\in[k_0],j\in[k]\}
    \subset\R^{nk+nk_0}.
\end{align*}
Note that 
% $(\bM,\bM_0) \bd  \in \vspan\left\{ \btheta_1, \dots,\btheta_k, \btheta_{0,1},\dots,\btheta_{0,k_0}\right\}$
%and 
%$\left( \bM, \bM_0\right)^\sT\bc \in \vspan\left\{\bell_1,\dots,\bell_k\right\}$,
$[\bM,\bM_0]\bd\in\vspan(\cS_1)$,
$[\bM,\bM_0]^\sT\bc\in \vspan(\cS_2)$, and $\bL^\sT\bL$ and $\bR$ are invertible. This implies that  
%$\bc \in \vspan\left\{ \btheta_1, \dots,\btheta_k, \btheta_{0,1},\dots,\btheta_{0,k_0}\right\}$
$\bc\in\vspan(\cS_1)$
and 
%$\bd \in \vspan\left\{\bell_1,\dots,\bell_k\right\}$.
$\bd\in\vspan(\cS_2)$
%
Hence, any $\ba \in \Nul(\bSigma)$ satisfies
\begin{equation}
    \ba \in
    \mathrm{span}\left(\left\{\ba_{i,j}: i,j \in [k]\right\}\cup \left\{\ba_{0,i,j}: i\in[k_0], j \in[k]  \right\} \cup
    \left\{ \tilde \ba_{i,j} : i,j \in [k] \right\}
    \cup
\left\{ \tilde \ba_{0,i,j} : i\in [k_0],j\in[k] \right\}
 \right)
\end{equation}
where 
% \begin{equation}
% \tilde\ba_{i,j} := \left(
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{j -1}, \btheta_i^\sT, \underbrace{\bzero^\sT,\dots,\bzero^\sT}_{k-j}, 
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k+k_0}
% \right)
% \end{equation}
\begin{equation*}
\tilde\ba_{i,j} := \big(
\be_{k,j}^\sT\otimes\btheta_i^\sT,
\underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k+k_0}
\big)^\sT\qquad\text{and}\qquad
\tilde\ba_{0,i,j} := \big(
\be_{k,j}^\sT\otimes\btheta_{0,i}^\sT,
\underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k+k_0}
\big)^\sT.
\end{equation*}
%and
% \begin{equation}
% \tilde\ba_{0,i,j} := \left(
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{j -1}, \btheta_{0,i}^\sT, \underbrace{\bzero^\sT,\dots,\bzero^\sT}_{k-j}, 
% \underbrace{\bzero^\sT, \dots, \bzero^\sT}_{k+k_0}
% \right).
% \end{equation}



We'll show that the collection $\{\bSigma \tilde \ba_{i,j}\} \cap \{\bSigma \tilde \ba_{0,i,j}\}$ is linearly independent.
This will then imply the desired inclusion
 $\Nul(\bSigma) \subseteq    \mathrm{span}\left(\left\{\ba_{i,j}: i,j \in [k]\right\}\cup \left\{\ba_{0,i,j}: i\in[k_0], j \in[k]  \right\} \right)$.
One can compute 
\begin{equation*}
    \bSigma \tilde \ba_{i,j} =
     \begin{bmatrix}(\bL^\sT\bL \otimes \bI_d)(\be_j\otimes \btheta_i)\\
   (\bR(\bTheta,\bTheta_0) \otimes \bI_n) (\be_j \otimes \bell_j)
   \end{bmatrix}
    ,
    \quad
    \bSigma \tilde \ba_{0,i,j} =
     \begin{bmatrix}(\bL^\sT\bL \otimes \bI_d)(\be_j\otimes \btheta_{i,0})\\
   (\bR(\bTheta,\bTheta_0) \otimes \bI_n) (\be_j \otimes \bell_j)
   \end{bmatrix}.
\end{equation*}
Once again, the linear independence of the columns of $\bL$ and $(\bTheta,\bTheta_0)$ now implies the desired linear independence.

Finally, the statement about the mean follows by using the constraint
$\bG(\bbV,\bTheta) = \bzero$ for $(\bTheta,\bbV) \in \cM(\cuA,\cuB,\sPi)$ and carrying out the computation.
\end{proof}


As a corollary, we have the following.
\begin{corollary}
\label{cor:proj}
Under Assumptions~\ref{ass:loss} and~\ref{ass:regularizer},
there exists 
$\bB_{\bSigma}:\R^m\rightarrow\R^{m_n \times (m_n-r_k)}$, 
whose columns
   %$\left\{\boldb_1(\bTheta,\bbV),\dots, \boldb_{m-k(k+k_0)}(\bTheta,\bbV)\right\}$ 
   are twice continuously differentiable,
   %in $(\bTheta,\bbV)$, 
   and are, for each $(\bTheta,\bbV)\in \cM(\cuA,\cuB,\sPi)$, an orthonormal basis of 
$\Col(\bSigma(\bTheta,\bbV))$.
\end{corollary}








\subsubsection{Concluding the proof of Proposition~\ref{prop:kac_rice_manifold}}

Let us cite the following lemma which will be useful in checking the non-degeneracy of the process required for the Kac-Rice formula to hold.
\begin{lemma}[Proposition 6.5,~\cite{azais2009level}]
\label{lemma:proc_grad_as_0}
    Let $\bh:\cU \to\R^m$ be a $C^2$ random process over $\cU \subseteq \R^m$. 
Let $\mathcal{K} \subseteq \cU$ be a compact set. 
Fix $\bu \in\R^m$.
Assume that for some $\delta >0$,
\begin{equation}
    \sup_{\bt \in \mathcal{K}} \sup_{ \bs  \in B_\delta^m(\bu) } p_{\bh(\bt)}(\bs)  < \infty.
\end{equation}
Then 
    \begin{equation}
        \P\left( \exists \bt \in \mathcal{K}:  \bh(\bt) = \bu,\; \det(\bJ_\bt \bh(\bt)) = 0\right) = 0.
    \end{equation}
\end{lemma}
%\begin{remark}
%    Clearly, we don't need uniformity in $\bTheta$ since we'll likely be restricting our attention to $\bTheta$ in a bounded subset of $\R^{d\times k}$, but for now I'll leave it as so. The modification is straightforward.
%\end{remark}

We are now ready to prove the proposiiton.
\begin{proof}[Proof of Proposition~\ref{lemma:kac_rice_manifold}]
Throughout, fix $\bw$ and use $\E$ and $\P$ for the conditional expectation and probability.
Consider a local chart $(\cU, \bpsi)$ of $\cM(\cuA,\cuB,\sPi)$.
%, where $\cU \subseteq \cM(\cuA,\cuB,\sPi)$ is open in the topology of the manifold and $\bpsi:\cU\rightarrow\R^{m-r_k}$. 
We will prove the claim when the parameter space is restricted to $\cU$. Since $\cM(\cuA,\cuB,\sPi)$ is an oriented bounded manifold, extension to $\cM(\cuA,\cuB,\sPi)$ can then be done via partitions of unity.
Namely, letting 
\begin{align}
    N_0 :=\left|\left\{
   (\bTheta,\bbV)\in \cU :
   \bzeta = \bzero,  
   \bH \succ n \sfsigma_\bH
    \right\}\right|,
\end{align}
%
%
%\begin{align}
%    N_0 :=\left|\left\{
%   (\bTheta,\bbV)\in \cU :
%   \hmu\in \cuA,\, 
%   \hmu\in \cuA,\, 
%   \bzeta = \bzero,  
%   \bL^\sT \bbV = \bzero, 
% \bR \succ\bzero,\;
% \bH \succ \bzero
%    \right\}\right|,
%\end{align}
we show that
\begin{align}
\E[N_0] &=\int_{(\bTheta,\bbV)\in \cU}  \E\left[\left| \det (\de\bz(\bTheta,\bbV))\right|
    \one_{\bH \succ n\sfsigma_\bH}
    \big| \bz(\bTheta,\bbV)= \bzero\right] p_{(\bTheta
    ,\bbV)}(\bzero)  \de_\cM V.
\end{align}

Define $\cO := \left\{ \bH  \succ n \sfsigma_\bH\right\}$, and for any $\bs\in\bpsi(\cU)$, define
\begin{equation}
\bg(\bs) := \bz(\bpsi^{-1}(\bs)) = \bB_{\bSigma}(\bpsi^{-1}(\bs))^\sT \bzeta(\bpsi^{-1}(\bs)),\quad
\bh(\bs) := (\bH/n )\circ\bpsi^{-1}(\bs),
\end{equation}
where $\bg: \bpsi(\cU) \mapsto \R^{m_0} $ and $\bh :\bpsi(\cU) \mapsto \R^M$ for
$m_0 = nk + nk_0 + dk - k^2- kk_0$ and $M = d^2k^2$,
and note that in this notation,
\begin{equation}
    N_0 = \left|\left\{\bs \in \bpsi(\cU) :  \bz(\bs) = \bzero,\; \bh(\bs) \in\cO\right\}\right|.
\end{equation}

We check the conditions of Theorem~\ref{thm:kac_rice}:
Clearly, $\bpsi(\cU)$ is an open subset of $\R^m$ by definition, and
condition \emph{(1.)} is by definition of the process. Condition \emph{(2.)} holds by Assumptions~\ref{ass:loss} and~\ref{ass:regularizer}. Meanwhile, condition \emph{(3.)} is guaranteed by definition of $\bB_{\bSigma}(\bt)$ and Lemma~\ref{lemma:eig_vecs_NS_Sigma}. 

We move on to condition \emph{(4.)}. % is the statement of Lemma~\ref{lemma:kr_condition4}. 
Let
\begin{equation}
    \cE_0(\sfsigma_\bH) :=\left\{ \exists (\bTheta,\bbV) \in \cM(\cuA,\cuB,\sPi)  : \bz(\bTheta,\bbV) =0,\; 
        \lambda_{\min}\left(\bH(\bTheta,\bbV) \right) =  n\sfsigma_\bH\right\}.
\end{equation}
We will show that
    \begin{equation}
    \P\left(\cE_0(\sfsigma_\bH)\right)= 0
    \end{equation}
by applying Lemma~\ref{lemma:proc_grad_as_0}.
Observe that for any $(\bTheta,\bbV)$ such that $\bz(\bTheta,\bbV) = \bzero$, we have by orthogonality of the mean of $\bzeta$ to the null space of $\bSigma$ that $\grad_\bTheta \hat R_n (\bTheta) = \bzero.$
Furthermore, at any such point, $\bH(\bTheta,\bbV)/n = \grad^2_\bTheta \hat R_n(\bTheta) = \bJ_{\bTheta} \tilde\bzeta$ where we defined the (non-Gaussian) process
 $\tilde \bzeta(\bTheta) := 
\grad_\bTheta \hat R_n(\bTheta)$.
Hence, we can immediately see that
$\cE_0 \subseteq  \tilde\cE_0$ for
\begin{align*}
\tilde\cE_0 :&=
\left\{
\exists \bTheta : \; 
\sfA_\bR \succeq \bR \succeq \sfsigma_\bR,\;
\tilde \bzeta(\bTheta) = \bzero,\;
\left|\det\left(\bJ_\bTheta \tilde \bzeta(\bTheta) - \sfsigma_\bH \bI\right)\right| = 0
\right\}\\
&=
\left\{
\exists \bTheta : \; 
\sfA_\bR \succeq \bR \succeq \sfsigma_\bR,\;
\tilde \bzeta(\bTheta) - \sfsigma_\bH \btheta = -\sfsigma_\bH \btheta,\;
\left|\det\left(\bJ_\bTheta (\tilde \bzeta(\bTheta) - \sfsigma_\bH \btheta) \right)\right| = 0
\right\};
\end{align*}
here, the vector $\btheta\in\R^{dk}$ denotes the concatenation of the columns of $\bTheta.$

We apply Lemma~\ref{lemma:proc_grad_as_0} to this event to show that it has probability $0$.
Notice that the density of the process $\bTheta\mapsto \tilde \bzeta(\bTheta) - \sfsigma_\bH \btheta$ at $\bu = -\sfsigma_\bH\btheta$ is the density of $\tilde\bzeta(\bTheta)$ at zero. In turn, under Assumption~\ref{ass:density}, the random variable $\tilde \bzeta(\bTheta)$ has a bounded density in a neighborhood of $0$ uniformly in $\bTheta$ (with constant depending on $\sfA_\bR$).
Meanwhile, Assumption~\ref{ass:loss} guarantees that $\bTheta \mapsto \tilde \bzeta(\bTheta)$ is $C^2$. So Lemma~\ref{lemma:proc_grad_as_0} applies to the compact set $\cK := \{\bTheta: \sfA_\bR \succeq \bR \succeq \sfsigma_\bR\}$, allowing us to deduce
$\P(\cE_0) \le \P(\tilde\cE_0) = 0$, verifying condition~\textit{(4.)} of Theorem~\ref{thm:kac_rice}.

Finally, condition \emph{(5.)} results from Lemma~\ref{lemma:proc_grad_as_0} noting that $\overline{\bpsi(\cU)}$ is compact. 
%Finally, we check condition \emph{(5.)}.

% We cannot apply Lemma~\ref{lemma:proc_grad_as_0} directly prove this condition is that the parameter set $\psi(\cU)$ may be unbounded. However, it is bound by a high probability. 
% Namely, for $C>0$, define the event $\cG_C := \left\{\norm{\bX}_\op \le C \sqrt{n}\right\}$.
% %and let $ C_0:= \norm{\bTheta_0}_2$.
% On $\cG_C$, for any $\bt = (\bTheta,\bbV)$ such that $\bz(\bt) = \bzero$, we have $\bzeta(\bt) = \bzero$ by Lemma~\ref{lemma:mean_cov}, implying that
% \begin{equation}
% \norm{\bbV}_\op = \norm{\bX(\bTheta,\bTheta_0)}_\op \le C \sfA_\bR \sqrt{n}.
%     %\norm{\bv_i}_2 = \norm{\bX \btheta_i}_2 \le C \sfA_\bR \sqrt{n} =: C_1(n),\quad
%     %\norm{\bu_{i_0}}_2 = \norm{\bX \btheta_{0,i_0}}_2 \le C C_0\sqrt{n} =:C_0(n).
% \end{equation}
% %
% So denoting
% \begin{equation}
%     \cB_C :=\{(\bTheta,\bbV)\in\cM : \|\bbV\|_\op \le C \sfA_\bR \sqrt{n}\},
% \end{equation}
% we have that \kas{perhaps mention that $\bTheta$ is by default bounded}
% \begin{align}
% \label{eq:prob_in_cond_5}
%   \P\left( \left\{\exists \bs \in\psi(\cU) : \bg(\bs) = \bzero,\;\det( \bJ_{\bs} \bg(\bs) ) = 0\right\} \cap \cG_C\right)
%   &=
%   \P\left( \left\{\exists \bs \in {\psi(\cU\cap \cB_C)} : \bg(\bs) = \bzero,\;\det( \bJ_\bs \bg(\bs) ) = 0\right\} \cap \cG_C\right)\\
%   &\le
%   \P\left( \left\{\exists \bs \in \overline{\psi(\cU\cap \cB_C)} : \bg(\bs) = \bzero,\;\det( \bJ_\bs \bg(\bs) ) = 0\right\} \right),
% \end{align}
% which puts us in a position to apply Lemma~\ref{lemma:proc_grad_as_0}: by Corollary~\ref{cor:proj} and Assumptions~\ref{ass:loss} and~\ref{ass:regularizer}, $\bs\mapsto \bz(\bs)$ is $C^2$. 
% Furthermore, since $\bz(\bs)$ is a non-degenerate Gaussian for all $\bs$, we have for its density $p_{\bz(\bs)}(\bu)$
% computed in Lemma~\ref{lemma:density},
% \begin{equation}
%     \sup_{\bs \in\psi(\cU\cap\cB_C)} \sup_{\bu \in B_2^k(\delta)} p_{\bz(\bs)}(\bu)
%     \le \sup_{\bs \in\psi(\cU\cap\cB_C)} p_{\bz(\bs)}(\E[\bz(\bs)]) = 
%     \sup_{\bs \in\psi(\cU\cap\cB_C)} \frac{\det^*(\bSigma(\psi^{-1}(\bs))^{-1/2})}{(2\pi)^{m_0/2}}< C_0(\sfs_\bR,\sfs_\bL,n,d)
% \end{equation}
% for some $C_0 \in(0,\infty)$ depending depending on $(\sfs_\bR,\sfs_\bL)$ and $n,d$. 
% So an application Lemma~\ref{lemma:proc_grad_as_0} allows conclude that the probability in Eq.~\eqref{eq:prob_in_cond_5} is equal to $0$.
% Therefore,
% \begin{align}
% \P\left( \left\{\exists \bs \in\psi(\cU) : \bz(\bs) = \bzero,\;\det( \bJ_\bs \bz(\bs) ) = 0\right\}\right)
% %&\le
% %  \P\left( \left\{\exists \bs \in\overline{\psi(\cU\cap \cB_C)} : \bz(\bs) = \bzero,\;\det( \bJ_\bs \bz(\bs) ) = 0\right\} \cap \cG_C\right) + \P(\cG_C^c)\nonumber\\
%   &\le  \P(\cG_C^c)
% \end{align}
% for all $C>0$. Taking $C\to\infty$ so that $\P(\cG_C^c) \to0$ shows that condition \emph{(5.)} of Theorem~\ref{thm:kac_rice} holds. 

So we conclude 
the integral formula
\begin{equation}
\label{eq:N0_kac_rice_app}
\E[N_0] =  \int_{\bs\in\bpsi(\cU)} \E\left[\left|\det \bJ_\bs\bz(\bs) \right| \one_{\bh(\bs) \in \cO} \big| \bg(\bs) = \bzero \right]  p_{\bz(\bs)}(\bzero)\de \bs.
\end{equation}
For $\bs\in\bpsi(\cU)$ we have
\begin{align*}
    \E\left[|\det \bJ_\bs \bz(\bs)| \one_{\bh(\bs)\in\cO} \big| \bg(\bs) = \bzero \right]=
    \left|\det\left( 
\de\bpsi^{-1}(\bs)\right)\right|
    \E\Big[&
    \left| \det \left( \de \bz(\bTheta,\bbV) \big|_{(\bTheta,\bbV) = \psi^{-1}(\bs)}\right)
    \right| \\
    &\one_{(\bH/n)\circ\bpsi^{-1}(\bs)\succ \bzero } 
\big| 
 \bz(\bpsi^{-1}(\bs)) = \bzero 
    \Big].
\end{align*}
Changing variables via $\bpsi(\bTheta,\bbV) = \bs$ and noting once again that $p_{\bz(\bs)}(\bzero) = p_{(\bTheta,\bbV)}(\bzero)$ defined in Lemma~\ref{lemma:density}, we have via Eq.~\eqref{eq:N0_kac_rice_app}


\begin{align*}
\E[N_0] &= \int_{\bs\in\bpsi(\cU)} 
    \E\left[
\left|\det \left( \de \bz(\bTheta,\bbV) \big|_{(\bTheta,\bbV) = \psi^{-1}(\bs)}\right)\right|
    \one_{\bh(\bs) \in \cO}
\big| 
 \bz(\psi^{-1}(\bs)) = \bzero 
    \right] p_{\bpsi^{-1}(\bs)}(\bzero) 
   \left|\det \left(\de\bpsi^{-1}(\bs)\right)\right|
    \de \bs\\
    &=\int_{(\bTheta,\bbV) \in \cU}  \E\left[\left| \det (\de\bz(\bTheta,\bbV))\right|
    \one_{\bH/n \succ \sfsigma_\bH}
    \big| \bz (\bTheta,\bbV) = \bzero\right] p_{(\bTheta,\bbV)}(\bzero)  \de_\cM V
\end{align*}
as desired.
\end{proof}

%\subsection{Reduction to the desired constraints}
%\label{sec:reduction_desired_constraints}
%\bns{This section needs rewriting.}
%%\begin{assumption}
%%    
%%For any $\bw \in \supp(\P_\bw)^n$, assume that one of these 
%%\begin{enumerate}
%%    \item The manifolds
%%\begin{equation}
%%   \left\{(\bV,\bU):
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero,
%%    \quad\det\left( \grad_{\bV,\bU} \left(\bL^\sT (\bV,\bU)\right)^\sT\grad_{\bV,\bU} \left(\bL^\sT (\bV,\bU)\right) \right) = 0\; 
%%   \right\}
%%\end{equation}
%%and
%%\begin{equation}
%%   \left\{(\bV,\bU):
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero,
%%    \quad
%%    \det\left(\bL^\sT\bL\right) = 0,\;  
%%   \right\}    
%%\end{equation}
%%each have dimension at most \bns{$n-d +k+k_0-1$}; or
%%\item For each $i\in[k]$ and $i_0\in[k_0]$ each of the manifolds
%%\begin{equation}
%%    \left\{\bv_i \in\R^n: \exists (\bV_{-i},\bU) \;\textrm{s.t.}\;
%%\det\left(\grad_{\bV,\bU} \left(\bL(\bV,\bU)^\sT (\bV,\bU)\right)\right) = 0,\; 
%%    \det\left(\bL(\bV,\bU)^\sT\bL(\bV,\bU)\right) = 0,\; 
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero
%%    \right\}
%%\end{equation}
%%and
%%\begin{equation}
%%    \left\{\bu_{i_0} \in\R^n: \exists (\bV,\bU_{-{i_0}}) \;\textrm{s.t.}\;
%%\det\left(\grad_{\bV,\bU} \left(\bL(\bV,\bU)^\sT (\bV,\bU)\right)\right) = 0,\; 
%%    \det\left(\bL(\bV,\bU)^\sT\bL(\bV,\bU)\right) = 0,\; 
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero
%%    \right\}
%%\end{equation}
%%has dimension at most $n-d - 1$.
%%\end{enumerate}
%%
%%\end{assumption}
%%
%\begin{assumption}
%\label{ass:curvature}
%For any $\bw \in \supp(\P_\bw)^n$, assume that
%%\begin{enumerate}
%%    \item The manifolds
%%\begin{equation}
%%   \left\{(\bV,\bU):
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero,
%%    \quad\det\left( \grad_{\bV,\bU} \left(\bL^\sT (\bV,\bU)\right)^\sT\grad_{\bV,\bU} \left(\bL^\sT (\bV,\bU)\right) \right) = 0\; 
%%   \right\}
%%\end{equation}
%%and
%%\begin{equation}
%%   \left\{(\bV,\bU):
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero,
%%    \quad
%%    \det\left(\bL^\sT\bL\right) = 0,\;  
%%   \right\}    
%%\end{equation}
%%each have dimension at most $(n-d)+k+k_0-1$; or
%%\item For each $i\in[k]$ and $i_0\in[k_0]$ each of the manifolds
%%\begin{equation}
%%    \left\{\bv_i \in\R^n: \exists (\bV_{-i},\bU) \;\textrm{s.t.}\;
%%    \det\left(\bL(\bV,\bU)^\{\sT\bL(\bV,\bU)\right) = 0,\; 
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero
%%    \right\}
%%\end{equation}
%%and
%%\begin{equation}
%%    \left\{\bu_{i_0} \in\R^n: \exists (\bV,\bU_{-{i_0}}) \;\textrm{s.t.}\;
%%\det\left(\grad_{\bV,\bU} \left(\bL(\bV,\bU)^\sT (\bV,\bU)\right)\right) = 0,\; 
%%    \det\left(\bL(\bV,\bU)^\sT\bL(\bV,\bU)\right) = 0,\; 
%%    (\bV,\bU)^\sT(\bV,\bU) \succ \bzero
%%    \right\}
%%\end{equation}
%%has dimension at most $n-d - 1$.
%%\end{enumerate}
%for each $i\in[k]$ and $i_0\in[k_0]$ each of the manifolds
%\am{ Also, would be clearer to define these as projections.} 
%\bns{I will rewrite this later. It doesn't seem clear to me how to ensure that $\eps_\bR>0 \Rightarrow \exists \eps_\bV >0$, but perhaps this can be absorbed in the definition of $\cB$.}
%\begin{equation}
%    \left\{\bv_i \in\R^n: \exists (\bV_{-i},\bU) \;\textrm{s.t.}\;
%    %(\bV,\bU)^\sT(\bV,\bU) \succ \bzero,\quad
%\det\left(\left|\bD_{\bV,\bU} \left(\bL(\bV,\bU)^\sT 
%(\bV,\bU)\right)\right|\right) = 0,\; \det(|\bV,\bU|) > 0
%    \right\},\quad
%\end{equation}
%\begin{equation}
%    \left\{\bv_i \in\R^n: \exists (\bV_{-i},\bU) \;\textrm{s.t.}\;
%    %(\bV,\bU)^\sT(\bV,\bU) \succ \bzero,\quad
%    \det\left(|\bL(\bV,\bU)|\right) =0,\; 
%    \det|(\bV,\bU)|  >\bzero
%    \right\},
%\end{equation}
%\begin{equation}
%    \left\{\bu_{i_0} \in\R^n: \exists (\bV,\bU_{-{i_0}}) \;\textrm{s.t.}\;
%    %(\bV,\bU)^\sT(\bV,\bU) \succ \bzero,\quad
%\det\left(\left|\bD_{\bV,\bU} \left(\bL(\bV,\bU)^\sT (\bV,\bU)\right)\right|\right) = 0,\; 
%\det\left(\left|(\bV,\bU)\right|\right) >0
%    \right\},
%\end{equation}
%and
%\begin{equation}
%    \quad
%    \left\{\bu_{i_0} \in\R^n: \exists (\bV,\bU_{-{i_0}}) \;\textrm{s.t.}\;
%%(\bV,\bU)^\sT(\bV,\bU) \succ \bzero,\quad
%    \det\left(\left|\bL(\bV,\bU)\right|\right) = 0,\; 
%\det(|(\bV,\bU) |) >0
%    \right\}
%\end{equation}
%each have dimension at most $n-d-1$.
%
%\end{assumption}
%
%%\begin{remark}
%%   Note that the first part of this assumption implies the second (but could be easier to check). This follows from the requirement that $(\bV,\bU)^\sT(\bV,\bU) \succ \bzero$.  \am{What is the first part and what the second part? Would be better to give names.}
%%\end{remark}
%%
%%
%\begin{lemma}
%\label{lemma:as_no_zeros_degenerate}
%    Under Assumption~\ref{ass:curvature}, we have for all $\bw \in \supp(\P_\bw)^n$,
%    \begin{equation}
%        \P\left(\forall(\bTheta,\bbV): \bzeta =\bzero, \bR\succ \bzero\; \Rightarrow
%        \det \bD\bL^\sT (\bbV) \neq 0,\; \bL^\sT\bL \succ\bzero,\;\bbV^\sT\bbV\succ\bzero
%        | \bw \right) = 1.
%    \end{equation}
%\end{lemma}
%\begin{proof}
%Since $n>d$, almost surely, $\bX$ will have full column rank so that
%\begin{equation}
%   \P\left(\bzeta = 0,\; \bR\succ\bzero \Rightarrow \bbV^\sT\bbV \succ \bzero \right)= 1.
%\end{equation}
%Now the result follows since under Assumption~\ref{ass:curvature},  the subspace
%$\{\bX\btheta_i:\btheta_i\in\reals^d\}=\{\bX\btheta_{0,i_0}:\btheta_{0,i_0}\in\reals^d\}$ 
%will almost surely be outside of the sets defined in the statement of the assumption.
%%So we show that
%%\begin{equation}
%%    \P\left(\exists (\bTheta,\bV,\bU) \;\textrm{s.t.}\; 
%%    \bzeta = 0, \bR\succ \bzero, (\bV,\bU)^\sT(\bV,\bU) \succ\bzero,\;
%%\det \grad \bL^\sT (\bV,\bU) \neq 0,\; \bL^\sT\bL \succ\bzero
%%    \right)= 0.
%%\end{equation}
%%Letting $\cS_i$ be the sets in Assumption~\ref{ass:curvature}, we have
%
%\end{proof}
%
%\begin{lemma}
%    Under Assumptions~\ref{ass:differntiability},\ref{ass:density},~\ref{ass:sets} and~\ref{ass:curvature}, we have for any open bounded $\cA\subseteq \R^{d\times k}$,
%    \begin{equation}
%        \E[Z_n(\cA,\cB)|\bw] = 
%        \int_{\bt \in \cM(\cA,\cB)}  \E\left[\left| \det (\de \bz(\bt) )\right|
%    \one_{\bH(\bt) \succ \bzero}
%    \big| \bzeta (\bt) = \bzero, \bw\right] p_\bt(\bzero)  \de S.
%    \end{equation}
%\end{lemma}
%
%\begin{proof}
%Once again, write $\E,\P$ for the conditional quantities given $\bw$.
%Recall the definition
%\begin{equation}
%    Z_n(\cA,\cB) = \left| \left\{
%   (\bTheta,\bbV) : \hmu_{\bTheta,\bTheta_0}\in\cA,\;\hnu_{\bbV}\in\cB, \;\bzeta = 0,  
%   \bL^\sT \bbV = \bzero, \;
% \bR(\hat \mu_{\bTheta,\bTheta_0}) \succ\eps_\bR,\;
%  \bH \succ \eps_\bH
%    \right\}\right|.
%\end{equation}
%By Lemma~\ref{lemma:as_no_zeros_degenerate},
%\begin{equation}
%    \E[Z_n(\cA) ] = \E[\tilde Z_n(\cA)]
%\end{equation}
%
%where
%\begin{equation}
%   \tilde Z_n(\cA) = \left| \left\{
%   (\bTheta,\bbV) : \bTheta\in\cA, \;\bzeta = 0,  
%   \bL^\sT \bbV = \bzero, 
% \bR \succ\eps_\bR,\;
%  \bH \succ \eps_\bH,\;
%|\bD_\bbV (\bL(\bbV)^\sT\bbV)|^2 \succ \bzero,\;
%\bL^\sT\bL \succ \bzero\;
%\bbV^\sT\bbV\succ \bzero
%    \right\}\right|.
%\end{equation}
%
%Since the mean of $\bzeta$ is in the columns space of its covariance by Lemma~\ref{lemma:mean_cov},
%the latter quantity is equal to $\E[N]$ defined in Lemma~\ref{lemma:kac_rice_manifold}.
%\end{proof}
%
%
%%\textcolor{blue}{
%%\begin{question}
%%I don't see how to get a non-asymptotic statement like we want in the main Theorem from this.
%%To illustrate the problem I'm facing, 
%%for simplicity, take $k = 1$ and $k_0 = 0$ and let $\bt \in\R^N$.
%% Suppose that after applying the formula above, we obtain something of the form
%% \begin{equation}
%%\E[Z_n] =    \int_{\bt \in\cM} e^{n\varphi(\bt)} h(\bt) \de V= 
%% \lim_{\eps \to 0}
%%        \frac1{2 \eps}
%%        \int_{\bt \in\R^N} e^{n\varphi(\bt)} h(\bt) \one_{\{|g(\bt)| \le \eps\}}  \norm{\grad g(\bt)}_2\de V .
%% \end{equation}
%% for some density $h(\bt)$, and let $\cM = \{g(\bt) = 0\}$  for some $g :\R^{N} \to \R$.
%%Then this implies (under a proper application of our asymptotics) for some $G$
%%\begin{equation}
%%    \frac1n \log(\E[Z_n]) \le \lim_{\eps\to 0} \left\{\frac1{n}\log\frac1{2\eps} + 
%%    \sup_{\nu : |\E_\nu{G(T)}| \le \eps} \left\{\E_\nu[\varphi(T)] - \KL(\nu \| \mu_h) \right\} + o(1)
%%    \right\}.
%%\end{equation}
%%\end{question}
%%}
%%\bns{Actually maybe i know how to do this by instead bounding the term on the LHS in the first display above by the integral + some term depending on $\eps$, that holds for all $\eps$ sufficiently small. Such a bound should exist by smoothness of the integrand }
%
%

\subsection{Integration over the manifold: Proof of Lemma~\ref{lemma:manifold_integral}}
\label{section:manifold_integration}
In this section, we upper bound the integral on the manifold appearing in Lemma~\ref{lemma:kac_rice_manifold} by an integral over a \emph{$\beta$-blowup} of the manifold $\cM$ as stated in Lemma~\ref{lemma:manifold_integral}.

%Recalling the error term $\Err_0(\beta,n,k)$ in the statement of Lemma~\ref{lemma:manifold_integral}, we will here obtain 
%the explicit quantity
%\begin{equation}
%    \Err_0(\beta,n,k) := 
%    \left(\frac1{1 - \beta\;r_k^2 C }\right)^{(m-r_k)/2}
%        \left(\frac{r_k^{5/2} C}{\beta(\sfsigma_{\bG} - \beta C r_k^2)}\right)^{r_k}.
%\end{equation}



%defined by
%\begin{equation}
%    \cM^\up{\beta} := \{\bu\in\R^{nk+nk_0 +dk}: \exists\;\bu_0\in\cM,\quad \norm{\bu-\bu_0}_2\le \beta\}.
%\end{equation}
%Recall the quantity 
%$\sfsigma_{\bG} := 1 \wedge \inf_{\bu \in\cM} \sigma_{\min}(\bJ_{\bu} \bg(\bu)^\sT).
%$
%The following lemma is the main result of this section. 

%\begin{lemma}[Manifold integral lemma]
%\label{lemma:manifold_integral} 
%Let $r_k := k(k+k_0)$, $m:= nk + nk_0 + dk$.
%Let $f: \R^{m}\rightarrow \R$ be a nonnegative continuous function. 
%There exists a constant $C = C(\sfA_{\bV},\sfA_{\bR})>0$ that depends only on $(\sfA_{\bV},\sfA_{\bR})$, such that for positive
%\begin{equation}
%   \beta \le \frac{C(\sfA_{\bV},\sfA_{\bR}) \sfsigma_{\bG}^3}{r_k^6} \wedge 1,
%\end{equation}
%we have
%\begin{equation}
%        \int_{(\bTheta,\bbV)\in \cM} f(\bTheta,\bbV) \de_\cM V
%        \le
%        E_f(\beta,n,k) 
%        \int_{(\bTheta,\bbV)\in \cM^{(\beta)}} f(\bTheta,\bbV)\de(\bTheta,\bbV),
%\end{equation}
%where the multiplicative error is given by
%\begin{equation}
%    E_f(\beta,n,k) := \left(\frac1{1 - \beta\;r_k^2 C }\right)^{(m-r_k)/2}
%        \left(\frac{r_k^{5/2} C}{\beta(\sfsigma_{\bG} - \beta C r_k^2)}\right)^{r_k}
%\exp\{\beta\;\norm{\log f}_{\Lip,\cM^{(1)}}\}.
%\end{equation}
%\end{lemma}

\subsubsection{Preliminaries}
Throughout this section, we'll use $m_n := nk +nk_0 + dk$ and $r_k := k(k+k_0)$.
To avoid working with tensors when differentiating, we will vectorize $\bG,\bbV$ and $\bTheta$. We will define the function $\bg :\R^{m_n}\to\R^{r_k}$ as the ``vectorization'' of $\bG :\R^{d\times k} \times \R^{n \times (k+k_0)} \to \R^{k\times(k+k_0)}.$
Using this notation, we'll define the quantities for $\tau >0$,
\begin{equation}
%\sfsigma_{\bG} := 1 \wedge \inf_{\bu \in\cM} \sigma_{\min}(\bJ_{\bu} \bg(\bu)^\sT),\quad
    \sfA_{\bG,1}^\up{\tau} := 1 \vee  
    \sup_{ \bu \in \cM^\up{\tau} }\|\bJ_\bu\bg(\bu)\|_\op,
\quad\quad
    \sfA_{\bG,2}^\up{\tau} := 1 \vee  
    \sup_{ \bu \in \cM^\up{\tau} }\|(\grad^2_\bu g_j(\bu))_{j\in[r_k]}\|_\op.    %\inf_{ \bu \in \cM }\sum_{j=1}^{r_k}\| \grad^2_\bu \bg_j(\bu)\|_\op
\end{equation}
Recall additionally
$\sfsigma_{\bG}$ satisfying $\sfsigma_\bG \le \inf_{\bu \in\cM} \sigma_{\min}(\bJ_{\bu} \bg(\bu)^\sT)$ of Section~\ref{sec:definitions}.
We assume without loss of generality that $\sfsigma_\bG \le 1$.
For $t>0$ define a normal tubular neighborhood $\cT(t)$ around the manifold $\cM$ by
\begin{equation}
\label{eq:tube_def}
    \cT(t):=
    \left\{(\bu,\bx)\in \cM\times \R^{r_k}: \norm{\bx}_2 < t \right\}.
\end{equation}
Define the function
$\bvarphi: \R^{m}\times \R^{r_k} \rightarrow \R^{m}$ by 
    \begin{equation}\label{eq:noraml-tube-chart}
        \bvarphi(\bu,\bx) := \bu + \bJ_{\bu}\bg(\bu)^\sT \bx.
    \end{equation}
This function will serve as our coordinate change. We will first show that this function is a local diffeomorphism and that the smallest singular value of its jacobian is lower bounded uniformly on $\cT(t)$ for the correct choice of $t$. 

\subsubsection{Showing that \texorpdfstring{$\varphi$}{phi} is a Local Diffeomorphism}
   %Let $\eps>0$ such that
   %    $\eps\le  
   %     \inf_{\bu\in\cM} \frac{\eps_D^2}{4\norm{\bD^2_{\bu}\bg(\bu)}_\op}.
   %     $
\begin{lemma}[Local diffeomorphism]\label{lem:local-diffeomorphis}

Assume 
$\tau \; \sfA_{\bG,2} \le (\sfsigma_\bG \wedge 1/2)$. 
Then, $\bvarphi$ restricted to the domain $\cT(\tau)$ is a local diffeomorphism 
and for $(\bu,\bx) \in\cT(\tau)$, we have the bound
\begin{equation}
    |\det(\de \bvarphi(\bu,\bx))| \ge (1 - \tau\;\sfA_{\bG,2})^{(m-r_k)/2} ( \sfsigma_{\bG} - \tau \sfA_{\bG,2})^{r_k}.
\end{equation}
\end{lemma}
\begin{proof} 
    Note that $\cT(\tau)$ is an open subset of the smooth manifold $\cM\times \R^{r_k}$, and consequently is a smooth manifold embedded in the ambient space of dimension $m+r_k$ with 
    $ \dim\left(\cT(\tau)\right) = \dim(\cM)+r_k = m.$
To lower bound the determinant, we will show that $\de\bvarphi(\bu,\bx) : T_{(\bu,\bx)}\cT(\tau)\rightarrow \R^m$ is a low-rank perturbation of a matrix that is approximately identity.

First, we'll compute the determinant of $\de \bvarphi$ for $(\bu,\bx)\in \cT(\tau)$. 
The (euclidean) Jacobian of $\bvarphi$ is given by
  \begin{equation}
      \bJ_{\bu,\bx}\bvarphi(\bu,\bx)= 
      \begin{bmatrix}
          \bI_{m} + 
          \bJ_\bu \circ \{ \bJ_{\bu}\bg(\bu)^\sT\bx\} \;\;,
          & 
          \bJ_\bu\bg(\bu)^\sT
      \end{bmatrix}
      \in \R^{m\times(m+r_k)}.
  \end{equation}
Letting $\bB(\bu)\in\R^{m\times(m-r_k)}$ be an orthonormal basis of $T_\bu\cM$,
%, and let $\bB^\perp_\bu \in\R^{m\times r_k}$ be a basis of the orthogonal complement. 
we define
      \begin{equation}\bB_0(\bu,\bx) := 
        \begin{bmatrix}
            \bB({\bu})& \bzero_{m\times r_k}\\
            \bzero_{r_k\times(m-r_k)}& \bI_{r_k} 
        \end{bmatrix}\in\R^{(m+r_k)\times(m)},
    \end{equation}
and note that since $T_{(\bu,\bx)} \cT(\tau)$ is isomorphic to $T_{(\bu)}\cM\oplus \R^{r_k}$
at $(\bu,\bx)\in \cT(\tau)$, 
we have that 
\begin{equation}
\label{eq:det_dphi_identification}
    \det(\de \bvarphi )  = \det(\bA),\quad \textrm{where}\quad
    \bA :=  \left[\bB
    +
    (\bJ_{\bu}\circ\{\bJ_{\bu} \bg(\bu)^\sT\bx\})\bB
    ,  \bJ_\bu \bg(\bu)^\sT \right]\in \R^{m\times m}.
\end{equation}
To simplify notation, from hereon, let us use the shorthand
    $\bH_0 := (\bJ_{\bu}\circ\{\bJ_{\bu} \bg(\bu)^\sT\bx\})$ and $\bF_0 :=\bJ_\bu \bg(\bu)^\sT.$
Now to lower bound the determinant above, noting that
\begin{equation}
\bA^\sT\bA = \begin{bmatrix}
\bB^\sT \left(\bI_{m} + \bH_0\right)^\sT(\bI_{m} +\bH_0) \bB &   \bB^\sT\bH_0^\sT \bF_0\\
\bF_0^\sT  \bH_0\bB &  \bF_0^\sT\bF_0
\end{bmatrix}
\end{equation}
(using $\bF^\sT\bB = 0$, $\bB^\sT\bB = \bI_{m}$),
we have
\begin{equation}
\label{eq:det_AA_lb}
    \det(\bA^\sT\bA) \ge \det\left(\bB^\sT \left(\bI_{m} + \bH_0\right)^\sT(\bI_{m} +\bH_0) \bB \right) 
\lambda_{\min}(\bA^\sT\bA)^{r_k}.
\end{equation}

The definition in Eq.~\eqref{eq:det_dphi_identification} readily gives a na\"ive bound on the smallest singular value of $\bA$ through
\begin{equation}
\label{eq:sigmamin_A_lb}
\sigma_{\min}(\bA) \ge \sigma_{\min}([\bB, \bF_0]) - \|\bH_0\|_\op 
\ge  \sfsigma_{\bG} -  \|\bH_0\|_\op,
\end{equation}
since $\sfsigma_\bG \le 1.$
Meanwhile, we have
\begin{equation}
\label{eq:det_BHB_lb}
    \det
\left(\bB^\sT \left(\bI_{m} + \bH_0\right)^\sT(\bI_{m} +\bH_0) \bB \right) 
\ge \lambda_{\min}(\bI_{m-r_k} + \bDelta_0)^{m-r_k}
\end{equation}
for some matrix $\bDelta_0 \in \R^{(m-r_k)\times (m-r_k)}$ satisfying
$\|\bDelta_0\|_\op \le (1 + 2\|\bH_0\|_\op)\|\bH_0\|_\op.$
Finally, to bound $\|\bH_0\|_\op$, we expand via its definition 
\begin{equation}
   \|\bH_0\|_\op  =
\Big\|\bJ_{\bu}\circ\Big\{\sum_{j=1}^{r_k} \grad_\bu g_j(\bu) x_j\Big\}\Big\|_\op
=
\Big\|\sum_{j=1}^{r_k} \grad^2_\bu g_j(\bu) x_j\Big\|_\op \le \|\bx\|_2 
\|(\grad^2_\bu g_j(\bu))_{j\in[r_k]}\|_\op  \le \sfA_{\bG,2}\;\tau.
\end{equation}
So for $\tau \; \sfA_{\bG,2} \le 1/2$, we have $\|\bDelta_{0}\|_\op \le 2 \tau \;\sfA_{\bG,2}.$
Combining this with Equations~\eqref{eq:det_AA_lb},~\eqref{eq:sigmamin_A_lb} and~\eqref{eq:det_BHB_lb} concludes the proof.


%for the matrix $\bA\in\R^{m \times m}$  defined by (suppressing the argument of $\bB(\bu)$),
%\begin{equation}
%\bA := \bA_0 + \bA_1 \quad 
%\textrm{where}\quad
%\bA_0 := \left[\bB ,  \bJ_\bu \bg(\bu)^\sT \right],\quad
%\bA_1 := \left[(\bJ_{\bu}\circ\{\bJ_{\bu} \bg(\bu)^\sT\bx\})\bB , \bzero \right].
%\end{equation}

\end{proof}


\subsubsection{
Showing that \texorpdfstring{$\varphi$}{phi} is a 
Global diffeomorphism}
%The next step is to find $\eps_\cM>0$ such that for any $\eps\in(0,\eps_\cM)$, $\bvarphi$ becomes a global diffeomorphism on $\cT(\eps)$.
%
%To establish such a bound, we need to know how the function $\bg$ behaves when we move further away from the manifold within the ambient space. Hence throughout, let us fix a constant $\Delta>0$, and let a $\Delta$-blowup of $\cM$ to be
%\begin{equation}
%    \cM^\Delta := \{\bu\in\R^{m}: \exists\bu_0 \in\cM,\quad \norm{\bu-\bu_0}\le \Delta\}.
%\end{equation}
%To avoid cluttering, let us define 
%
%
%\begin{equation}
%   \sfD^\Delta := \sup_{\bu\in\cM^\Delta} \norm{\bJ_\bu\bg(\bu)}_\op,  \quad 
%   \sfD := \sup_{\bu\in\cM} \norm{\bJ_\bu\bg(\bu)}_\op,
%\end{equation}
%\begin{equation}
%   \sfH^\Delta := \sup_{\bu\in\cM^\Delta} \norm{\bD^2_\bu\bg(\bu)}_\op,  \quad 
%   \sfH := \sup_{\bu\in\cM} \norm{\bD^2_\bu\bg(\bu)}_\op.
%\end{equation}
%The main result of this section is the following:

%\begin{lemma}[Global diffeomorphism] \label{lem:global-radius}
%    Let $\beta\in(0,\eps_\cM)$, where
%    \begin{equation}
%        \eps_\cM :=  \frac1{6\sfD^2}\left(
%        \left(\frac{\eps_D^3}{32(\sfD^\Delta)^2
%        \sfH^\Delta}\right) 
%        \wedge
%        (2\eps_D\Delta)\right).
%    \end{equation}
%    Then, $\bvarphi$ is a global diffeomorphism on the manifold $\cT(\eps)$.
%\end{lemma}

\begin{lemma}[Implicit function theorem: modified Theorem 2.9.10 \cite{hubbard2015vector}] 
\label{lem:implicit-function}
      Let $\boldf:\cU\rightarrow\R^p$ 
    for $\cU \subseteq \R^{m+p}$
      be a smooth function. 
      Fix $(\bx_0,\by_0)\in\cU$ such that 
     $\boldf(\bx_0,\by_0)=\bzero$, $\bJ_\bx \boldf(\bx_0,\by_0)\in\R^{p\times p}$ is invertible, and $\bJ_\by \boldf(\bx_0,\by_0)=\bzero$.
    Choose $\delta_0>0$ so that $\Ball^{p+m}_{\delta_0}((\bx_0,\by_0)) \subseteq \cU$, and
\begin{equation}
    %\sum_{j=1}^p\sup_{(\bx,\by) \in B_{\delta_0}(\bx_0,\by_0)} \|\grad^2 \boldf_j(\bx,\by)\|_\op^2 
    \sup_{(\bx,\by) \in \Ball^{p+m}_{\delta_0}((\bx_0,\by_0))}
\big\|\big(\grad^2 \boldf_{j}(\bx,\by)\big)_{j\in[p]}\big\|_\op 
    \le  \frac1{\delta_0}\sigma_{\min}\left( \bJ_{\bx} \boldf(\bx_0,\by_0)\right).
\end{equation}
      Then, for $r_0:= \delta_0 \sigma_{\min}(\bJ_\bx\boldf(\bx_0,\by_0))/2$, there exists a unique smooth mapping
      $\bpsi:\Ball_{r_0}^p(\by_0)\rightarrow \Ball_{\delta_0}^m(\bx_0)$
      satisfying $\bpsi(\by_0)=\bx_0$, and
      \begin{equation}
          \{(\bx,\by) \in \Ball_{\delta_0}^m(\bx_0)\times\Ball_{r_0}^p(\by_0) 
          :\boldf(\bx,\by) = \bzero \} = 
          \{(\bpsi(\by),\by): \by\in \Ball_{r_0}^p(\by_0)\}.
      \end{equation}
\end{lemma}
\begin{proof}
We invoke Theorem 2.9.10 of \cite{hubbard2015vector} which asserts the existence and uniqueness of such a $\bpsi$ under the conditions of the lemma, if the Jacobian satisfies:
\begin{equation}
\label{eq:jacobian_lipschitz_IFT}
     \|\bJ\boldf(\bx_1,\by_1) - \bJ\boldf(\bx_2,\by_2)\|_\op
     \le 
     \frac{1}{2 r_0 \|\bL^{-1}\|_\op^2} \|(\bx_1, \by_1) - (\bx_2,\by_2)\|_2,
\end{equation}
where 
\begin{equation}
   \bL :=   \begin{bmatrix}
        \bJ_{\bx}  \boldf(\bx_0,\by_0) & 
        \bJ_{\by}  \boldf(\bx_0,\by_0)\\
\bzero & \bI_m
   \end{bmatrix}.
\end{equation}
To de-clutter notation, define
\begin{equation}
    s_{\min}(\bx_0,\by_0) := \sigma_{\min} (\bJ_\bx\boldf(\bx_0,\by_0)^\sT),
\quad 
A_{\delta_0}(\bx_0,\by_0) :=  
\sup_{(\bx,\by) \in \Ball_{\delta_0}((\bx_0,\by_0))}
\big\|\big(\grad^2 \boldf_{j}(\bx,\by)\big)_{j\in[p]}\big\|_\op 
%\sum_{j=1}^p\sup_{(\bx,\by) \in B_{\delta_0}(\bx_0,\by_0)} \|\grad^2 \boldf_j(\bx,\by)\|_\op^2.
\end{equation}
So for any $\bz_1 := (\bx_1,\by_1), \bz_2 :=(\bx_2,\by_2)\in B_{\delta_0}(\bx_0,\by_0)$ we have
\begin{align*}
     \|\bJ\boldf(\bz_1) - \bJ\boldf(\bz_2)\|_\op
&=  
\Big\|\Big(\int_{0}^1 \grad^2 \boldf_{j}(\bz_1  + t(\bz_2 - \bz_1)) (\bz_2 - \bz_1) \de t\Big)_{j\in[p]}\Big\|_\op \\
&=
\Big\|\int_{0}^1 \Big(\grad^2 \boldf_{j}(\bz_1  + t(\bz_2 - \bz_1)) (\bz_2 - \bz_1) \Big)_{j\in[p]} \de t\Big\|_\op \\
&\le 
\sup_{t}\Big\|\Big(\grad^2 \boldf_{j}(\bz_1  + t(\bz_2 - \bz_1)) (\bz_2 - \bz_1) \Big)_{j\in[p]}\Big\|_\op \\
&\le 
%\sup_{\bz \in \cB_{\delta_0}(\bx_0,\by_0)}
%\big\|\big(\grad^2 \boldf_{j}(\bz)\big)_{j\in[p]}\big\|_\op 
A_{\delta_{0}}(\bx_0,\by_0)
\|(\bz_2 - \bz_1) \|_2.
%\\
%    &= \sum_{j=1}^{p} \|\grad \boldf_{j}(\bx_1,\by_1) - \grad \boldf_{j}(\bx_2,\by_2) \|_2^2\\
%    &\le
%  \frac12  A_{\delta_0}(\bx_0,\by_0)  \|(\bx_1,\by_1) - (\bx_2,\by_2)\|_2^2\\
%  &\le  \frac{s_{\min}(\bx_0,\by_0)^2}{\delta_0^2} 
%  \|(\bx_1,\by_1) - (\bx_2,\by_2)\|_2^2.
\end{align*}
So by assumptions of the lemma,
\begin{equation}
     \|\bJ\boldf(\bx_1,\by_1) - \bJ\boldf(\bx_2,\by_2)\|_\op
  \le  \frac{s_{\min}(\bx_0,\by_0)}{\delta_0} 
  \|(\bx_1,\by_1) - (\bx_2,\by_2)\|_2.
\end{equation}
After substituting the definition of $r_0$, and noting that $\bJ_\by \boldf(\bx_0,\by_0) =\bzero$ by assumption so that $\sigma_{\min}(\bL) = s_{\min}(\bx_0,\by_0),$
we conclude that
the inequality of Eq.~\eqref{eq:jacobian_lipschitz_IFT} holds, giving the existence and uniqueness of the $\bpsi$ defined in the statement.

So all the conditions are satisfied, and we can apply \cite{hubbard2015vector} to get the existence of a function $\bpsi:B_R(\by_0)\rightarrow B_{\delta}(\bx_0)$ such that $\boldf(\bpsi(\by),\by) = \bzero$ for all $\by\in B_R(\by_0)$. 
%This, alongside the uniqueness of $\bpsi$ implies the set inequality of the lemma.
    \end{proof}


\begin{lemma}[Tangentional-Decomposition]
\label{lem:tangentional-decomposition}
Fix a point $\bu_0 \in\cM \subseteq \R^{m_n}$. Let $\bU_0:= [\bB_0^\perp, \bB_0]\in\R^{m_n\times m_n}$, where $\bB_0\in\R^{m_n\times (m_n-r_k)},\bB_0^\perp \in\R^{m_n\times r_k}$ are basis matrices for the tangent space and normal space of $\cM$ at $\bu_0$ respectively.  For $\bu \in \R^{m_n}$, let $[\bu_N^\sT,\bu_T^\sT]^\sT := \bU_0^{-1}\bu$, and use the notation $\bJ_{\bu_N} \bg(\bu)
            := \bJ_{\bu_N}\bg(\bU_0
            [\bu_N^\sT,\bu_T^\sT]^\sT
            )\in \R^{r_k\times r_k}$ and $\bJ_{\bu_T} \bg(\bu) := \bJ_{\bu_T}\bg(\bU_0
            [\bu_N^\sT,\bu_T^\sT]^\sT
            )\in \R^{r_k\times (m_n- r_k)}$.
Then 
\begin{enumerate}
    \item we have
\begin{equation}
\label{eq:derivatives_along_tangent_and_normal}
\bJ_{\bu} \bg(\bu) = [\bJ_{\bu_N}\bg(\bu), \bJ_{\bu_T}\bg(\bu)]\bU_0^\sT
,\quad
        \bJ_{\bu_T}
        \bg(\bu_0)=\bzero,\quad
        \sigma_{\min}\left(\bJ_{\bu_N}\bg(\bu_0)\right)\ge \sfsigma_\bG.
\end{equation}
\item
For any $\tau\in(0,1)$,
\begin{equation}
\label{eq:variation_msv}
            \norm{\bu -\bu_0}_2\le \frac{\sfsigma_\bG}{2 \sfA_{\bG,2}^\up{\tau}}\wedge \tau
            \quad\Rightarrow\quad
            \sigma_{\min}\left(
            \bJ_{\bu_N}
            \bg(\bu)\right)
            \geq
        \frac{\sfsigma_{\bG}}{2}.
\end{equation}
\end{enumerate}

\end{lemma}

\begin{proof}
That the Jacobian of $\bg$ can be decomposed as in~\eqref{eq:derivatives_along_tangent_and_normal} follows from the chain rule. Then $\bJ_{\bu_\bT}\bg(\bu_0) =  \bJ_{\bu}\bg(\bu_0) \bB_{0}  = \bzero$ by definition of $\cM$, and $\sigma_{\min}(\bJ_{\bu_\bT}\bg(\bu_0)) =\sigma_{\min}( \bJ_{\bu}\bg(\bu_0)\bB_{0}) \ge \sfsigma_{\bG}$ follow immediately.

Now to prove Eq.~\eqref{eq:variation_msv}, an argument similar to the one in Lemma~\ref{lem:implicit-function} gives
\begin{align}
\nonumber
|\sigma_{\min}\left(\bJ_{\bu_N}  \bg(\bu) \right)-\sigma_{\min}\left(\bJ_{\bu_N}  \bg(\bu_0) \right)|
&\le 
\|\bJ_{\bu_N} \bg(\bu) - \bJ_{\bu_N} \bg(\bu_0)\|_\op \le \sfA_{\bG,2}^\up{\tau} \|\bu - \bu_0\|_2 
\le
\sfA_{\bG,2}^\up{\tau} \left( \frac{\sfsigma_{\bG}}{2 \sfA_{\bG,2}^\up{\tau}} \wedge \tau\right)\\
&\le  \frac{\sfsigma_{\bG}}{2}.
\end{align}
\end{proof}
\begin{corollary}[Implicit chart] \label{cor:implicit-chart}
Fix arbitrary $\bu_0 \in\cM \subseteq \R^m$. 
Use the notation introduced in in Lemma~\ref{lem:tangentional-decomposition}. For $\bu\in\R^m$, denote $[\bu_N^\sT,\bu_T^\sT]^\sT := \bU_0^{-1}\bu$.
Fix $\tau,\tau_N,\tau_T >0$ such that
\begin{equation}
\label{eq:tau_N_tau_T}
\tau_N  < \frac{\sfsigma_{\bG}}{4 \sfA_{\bG,2}^\up{\tau}} 
\wedge \frac{\tau}{2}, \quad\quad  \tau_T <  \frac{\sfsigma_{\bG} \tau_N }{2} \wedge \frac{\tau}{2}.
\end{equation}
Then there exists smooth functions 
$\bpsi_0: \Ball_{\tau_T}^{m-r_k}(\bu_{0,T}) \rightarrow\R^{r_k}$,
$\tilde\bpsi_0 :\Ball_{\tau_T}^{m-r_k}(\bu_{0,T})  \rightarrow \R^m$ such that
the following hold:
\begin{enumerate}
    \item we have
\begin{equation}
  \tbpsi_0(\bz) := \bU[\bpsi_0(\bz)^\sT,\bz^\sT]^\sT, \quad\quad
  \tbpsi_0(\bu_{0,T}) = \bu_0,\quad\quad
     \bJ_{\bz} \psi(\bz)  = -(\bJ_{\bu_N} \bg\circ\tbpsi_0(\bz))^{-1} \bJ_{\bu_T}\bg\circ\tbpsi_0(\bz),\quad\quad
\end{equation}
and
\begin{equation}
    \{\bu: (\bu_N,\bu_\bT) \in\Ball^{r_K}_{\tau_N}(\bu_{0,N}) \times\Ball^{m-r_k}_{\tau_T}(\bu_{0,T}) :\bg(\bu)=\bzero \} = 
    \{
    \tbpsi_0(\bz)
    : \bz \in\Ball_{\tau_T}^{m-r_k}(\bu_{0,T})\}.
\end{equation} 
\item we have
\begin{equation}
\label{eq:hess_psi_ub}
\bJ_{\bz} \bpsi_0(\bu_{0,T}) = \bzero,\quad\quad
\textrm{and}
    \quad\quad
    \sup_{\bz\in\Ball_{\tau_T}^{m-r_k}(\bu_{0,T})}
    \|\big(\grad^2 \bpsi_{0,j}(\bz)\big)_{j\in[r_k]}\|_\op \le  16 \frac{(\sfA_{\bG,1}^\up{\tau})^2 \sfA_{\bG,2}^\up{\tau}}{\sfsigma_{\bG}^3}.
\end{equation}
\end{enumerate}
\end{corollary}
\begin{proof}
The statement is largely a corollary of Lemma~\ref{lem:implicit-function} applied to $(\bu_N,\bu_T) \mapsto \bg(\bU [\bu_N^\sT,\bu_\bT^\sT]^\sT])$.

Indeed, the conditions of Lemma~\ref{lem:implicit-function} are verified by Lemma~\ref{lem:tangentional-decomposition}: we have 
$\bJ_{\bu_T}\bg(\bu_0)=\bzero$,
and by the choice of $\tau_N$,
\begin{equation}
\sup_{\bu\in\Ball_{\tau_N}^m(\bu_0)}  \|(\grad^2_{\bu_N,\bu_T} \bg_j(\bu))_{j\in[r_k]}\|_\op = 
\sup_{\bu\in\Ball_{\tau_N}^m(\bu_0)}  \|(\grad^2_{\bu} \bg_j(\bu))_{j\in[r_k]}\|_\op \le  \sfA_{\bG,2}^{\up{\tau}} \le  \frac{\sfsigma_{\bG}}{2 \tau_N}
\stackrel{(a)}{\le} \frac1{\tau_N} \sigma_{\min}(\bJ_{\bu_N}\bg(\bu_0)),
\end{equation}
where $(a)$ follows by the lower bound in Lemma~\ref{lem:tangentional-decomposition}.
Furthermore, $\bJ_{\bu_N}\bg(\bu_0)$ is non-singular since $\bu_0\in\cM$.

So Lemma~\ref{lem:implicit-function} holds and gives the statement of the lemma (after a change of variables), except for the display~\eqref{eq:hess_psi_ub}.
The equality in this display holds by
\begin{equation}
\bJ_{\bz} \bpsi_0(\bu_{0,T})  = -(\bJ_{\bu_N} \bg(\bu_0))^{-1} \bJ_{\bu_T}\bg(\bu_0) \stackrel{(a)}{=} \bzero
\end{equation}
where $(a)$ is by Lemma~\ref{lem:tangentional-decomposition}.
For the bound in Eq.~\eqref{eq:hess_psi_ub}, we differentiate 
$\bJ_{\bz} \bpsi_0(\bz)  = -(\bJ_{\bu_N} \bg\circ\tbpsi_0(\bz))^{-1} \bJ_{\bu_T}\bg\circ\tbpsi_0(\bz)$ element-wise. 
Namely, for $i \in[r_k], j\in[m-r_k]$, this can be written as
\begin{equation}
            (\grad_{\bu_N}g_i\circ \tbpsi_0(\bz))^\sT
            \left({\partial_{\bz_j}}
            \bpsi(\bz)\right) = 
            -\partial_{\bu_{T,j}}g_i\circ
            \tbpsi_0(\bz),
            \quad\quad\textrm{where}\quad\quad 
            \partial_{\bz_j}\bpsi(\bz) =   
            \left(\partial_{\bz_j}\psi_{l}(\bz) \right)_{l\in r_k} \in\R^{r_k}.
        \end{equation}
        Differentiating both sides now gives 
        %\kas{check transpose}
        %\bns{Check what you needed to check please}
    \begin{equation}    
       \bJ_\bz\left(\grad_{\bu_N}g_i\circ \tbpsi_0(\bz)\right)^\sT
       \left(\partial_{\bz_j}\bpsi(\bz)\right)
        +        
        \bJ_\bz \left(\partial_{\bz_j}
        \bpsi(\bz)\right)^\sT
        (\grad_{\bu_N}g_i\circ \tbpsi_0(\bz))
        =
       - \bJ_\bz \tbpsi_0(\bz)^\sT \grad_{\bu}
       \left( \partial_{\bu_{T,j}}g_i\circ
        \tbpsi_0(\bz)\right).
        \end{equation}
    By re-ordering this equation and letting $\bu = \tbpsi_0(\bz)$, we get
    \begin{align*}
    \bJ_\bz\left(\partial_{\bz_j} \bpsi(\bz)\right)^\sT (\grad_{\bu_N} g_i(\bu)) = -\bJ_\bz \tbpsi_0(\bz)^\sT\grad_\bu \left(\partial_{\bu_{T,j}}g_i(\bu)\right)
    - \bJ_\bz(\tbpsi_0(\bz))^\sT \bJ_\bu\left(\grad_{\bu_N}g_i(\bu)\right)^\sT(\partial_{\bz_j}\bpsi(\bz)).
    \end{align*}
    Next, by concatenating different values of $i$ and $j$,
    % \begin{align}
    %        \bJ_\bz\left(\partial_{\bz_j} \bpsi(\bz)\right)^\sT
    %        \left(\bJ_{\bu_N} \bg(\bu)\right)^\sT = 
    %        -\bJ_\bz \tbpsi_0(\bz)^\sT \bJ(\partial_{\bu_{T,j}} \bg(\bu))^\sT - \bJ_\bz\left(\tbpsi_0(\bz)\right)^\sT
    %        \left(\bJ_\bu(\grad_{\bu_N}g_i(\bu))^\sT
    %        \right)_{i\in[r_k]} 
    %        \left(\bI_{r_k} \otimes \partial_{\bz_j} \bpsi_0(\bz)\right),
    % \end{align}
    %and further by concatenating different values of $j$, 
    \begin{align*}
           &\left(\bJ_\bz\left(\partial_{\bz_j} \bpsi(\bz)\right)\right)_{j\in[m-r_k]}^\sT
           \left(\bI_{m-r_k}\otimes \bJ_{\bu_N} \bg(\bu)\right)^\sT = 
           -\bJ_\bz \tbpsi_0(\bz)^\sT
           \left(\bJ(\partial_{\bu_{T,j}} \bg(\bu))\right)_{j\in[m-r_k]}^\sT \\
           &\hspace{30mm}- \bJ_\bz\left(\tbpsi_0(\bz)\right)^\sT
           \left(\bJ_\bu(\grad_{\bu_N}g_i(\bu))
           \right)_{i\in[r_k]} ^\sT
           \left(\bI_{r_k} \otimes \partial_{\bz_j} \bpsi_0(\bz)^\sT\right)_{j\in[m-r_k]}^\sT.
    \end{align*}
     Each term in the equation above has a bounded operator norm. Namely, recall from Lemma \ref{lem:tangentional-decomposition} that since $\norm{\bu-\bu_0}\le \sqrt{\tau_T^2 +\tau_N^2} \le{\sfsigma_\bG}/{(2\sfA_{\bG,2}^\up{\tau})}\wedge\tau$, we have $\norm{\bJ_{\bu_N}\bg(\bu)^{-1}}_\op\le 2/{\sfsigma_\bG}$, and recall from Lemma \ref{lem:implicit-function} that
    \begin{equation}
          \norm{\bJ\bpsi_0(\bz)}_\op\le \norm{\bJ_{\bu_N}\bg(\bu)^{-1}}_\op\norm{\bJ_{\bu_T}\bg(\bu)}_\op\le \frac{2\sfA_{\bG,1}}{\sfsigma_{\bG,1}^\up{\tau}},\qquad
         \norm{\bJ \tbpsi_0(\bz)}_\op \le 1+ \norm{\bJ\bpsi_0(\bz)}_\op \le 1+  \frac{2\sfA_{\bG,1}^\up{\tau}}{\sfsigma_\bG}.
    \end{equation}
    Further, recall that by definition, on the set $\bu\in \Ball_\tau^m(\bu_0)$,
    \begin{equation}
        \norm{\left(\bJ(\partial_{\bu_{T,j}} \bg(\bu))\right)_{j\in[m-r_k]}}_\op = \norm{\left(\bJ_\bu(\grad_{\bu_N}g_i(\bu))
           \right)_{i\in[r_k]}}_\op 
           \le \sfA_{\bG,2}^\up{\tau}.
    \end{equation}
    Hence overall we can simplify the operator norm of the grand matrix of the second derivatives of $\bpsi$ to
    \begin{equation}
        \norm{\left(\bJ_\bz\left(\partial_{\bz_j} \bpsi_0(\bz)\right)\right)_{j\in[m-r_k]}}_\op = \norm{\big(\grad^2 \bpsi_{0,j}(\bz)\big)_{j\in[r_k]}}_\op 
           \le \frac2{\sfsigma_\bG}
           \frac{2\sfA_{\bG,1}^\up{\tau}\sfA_{\bG,2}^\up{\tau}}{\sfsigma_\bG}\left( 2+ \frac{2\sfA_{\bG,1}^\up{\tau}}{\sfsigma_\bG}\right)
           \le \frac{16(\sfA_{\bG,1}^\up{\tau})^2 \sfA_{\bG,2}^\up{\tau}}{\sfsigma_{\bG}^3}.
    \end{equation}

    
\end{proof}
\begin{lemma}[Global diffeomorphism]
\label{lemma:global_diff}
Fix $\tau >0$.
If $\beta$ satisfies
    \begin{equation}
        \beta\le 
        \frac{\sfsigma_\bG^3}{32(\sfA_{\bG,1}^{(\tau)})^2
        \sfA_{\bG,2}^{(\tau)}}
        \wedge
        \frac{\sfsigma_\bG}{4}\tau,
    \end{equation}
then the mapping $\bvarphi$ defined in Eq.~\eqref{eq:noraml-tube-chart} is a global diffeomorphism on the tube $\cT\left(\beta/(2\sfA_{\bG,1})\right)$. 
    %\kas{note $\sfA_{\bG,1}$ really needs to be only sup on the manifold cause I want this to get canceled later in the integral. Add the definition.}
\end{lemma}
\begin{proof} 
   Recall that by Lemma \ref{lem:local-diffeomorphis}, $\bvarphi$ is a local diffeomorphism on the tube $\cT\left(\beta/(2\sfA_{\bG,1})\right)$. To complete the proof, we need to show that it is also a bijection on this set.
    Let $(\bu_0,\bx_0),(\bu_1,\bx_1)\in \cT\left(\beta/(2\sfA_{\bG,1})\right)$, and assume there exists some $\bw\in\R^{m}$ such that
        \begin{align*}
       \bw &=
       \varphi(\bu_0, \bx_0)  =
       \varphi(\bu_1, \bx_1) \\
       &= 
       \bu_0+ \bJ_{\bu}\bg
       (\bu_0)^\sT\bx_0 =
       \bu_1+ \bJ_{\bu}\bg
       (\bu_1)^\sT\bx_1.
    \end{align*}
Then we have the bounds
    \begin{align}
        &\norm{\bu_1-\bu_0}_2 = \norm{\bJ_\bu \bg(\bu_0)^\sT\bx_0 - \bJ_\bu \bg(\bu_1)^\sT \bx_1}_2 \stackrel{(a)}\le       \sfA_{\bG,1}\left(\norm{\bx_0}_2+\norm{\bx_1}_2\right)\le \beta,
    \end{align}
since $\bu_0,\bu_1 \in\cM$ and by the choice of $\beta$, 
and similarly
\begin{align}
        &\norm{\bw-\bu_0}_2 = \|{\bJ_{\bu} \bg(\bu_0)^\sT\bx_0}\|_2
        \le \sfA_{\bG,1} \norm{\bx_0}_2\le \frac\beta2.
\end{align}
So $\bu_1\in\Ball_{\beta}^{m}(\bu_0)$, $\bw\in \Ball_\beta^{m}(\bu_0)$.
Now for any $\tau_N,\tau_T$ as in Eq.~\eqref{eq:tau_N_tau_T}, 
Corollary~\ref{cor:implicit-chart} furnishes the 
functions $\tbpsi_0$ and $\bpsi_0$, defined therein, that are smooth on the set $\Ball_{\tau_T}^{m-r_k}(\bu_{0,T})$.
Since $\sfA_{\bG,1}^\up{\tau},\sfA_{\bG,2}^\up{\tau} \ge1$ and $\sfsigma_{\bG}\le 1$ by definition, it is easy to check that for $\tau_T = \beta,$ there exists a choice of $\tau_N$  so that both satisfy Eq.~\eqref{eq:tau_N_tau_T}. So the functions $\bpsi_0, \tbpsi_0$ are hence defined and smooth on $\Ball_\beta^{m-r_k}(\bu_{0,T})$, $\tbpsi_0(\bu_{0,T}) = \bu_0$ and there exists $\bz_1 \in \Ball_{\beta}^{m-r_k}(\bu_{0,T}) $ such that $\tbpsi_0(\bz_1) = \bu_1$.
Defining the function
\begin{equation}
F_0(\bz) := \|{\tbpsi_0(\bz) - \bw}\|^2,
\end{equation}
we will show \textbf{(1.)} that both $\bz_1$ and $\bu_{0,T}$ are critical points of $\bF_0$, and \textbf{(2.)} that $\bF_0$ is strictly convex on $\Ball_{\beta}^{m-r_k}(\bu_{0,T})$. 
This will imply that $\bz_1 = \bu_{0,T}$, and hence that $\bu_1 = \tbpsi_0(\bz_1) = \tbpsi_0(\bu_{0,T}) = \bu_0$.

To verify \textbf{(1.)}, we use the identity for $\bJ_{\bz} \bpsi_0(\bz)$ derive in Corollary~\ref{cor:implicit-chart}. Namely,
    \begin{align*}
        \frac12\bJ_\bz F_0(\bz_1) =& 
        \bJ_\bz \tbpsi_0(\bz_1)^\sT(\tbpsi_0(\bz_1)-\bw)\\
        =&  \bJ_\bz(\bU(\bpsi_0(\bz_1),\bz_1))^\sT
        (\bJ_{\bu}
        \bg(\bu_1)^\sT\bx)\\
        =& [
            \bJ_\bz\bpsi_0(\bz_1)^\sT,\bI
        ]\bU^\sT
        (\bJ_{\bu}
        \bg(\bu_1)^\sT\bx)\\
        =&  [
            -(\bJ_{\bu_N}
            \bg(\bu_1)^{-1}
            \bJ_{\bu_T}
            \bg(\bu_1))^\sT,
            \bI]
        \bU^\sT\bU
        [
            \bJ_{\bu_N}
            \bg(\bu_1),
            \bJ_{\bu_T}
            \bg(\bu_1)
        ]^\sT\bx\\
        =& [
            -\bJ_{\bu_T}
            \bg{(\bu_1)^\sT
            (\bJ_{\bu_N}
            \bg(\bu_1)^{-1}})^\sT,
            \bI]
        \begin{bmatrix}
            \bJ_{\bu_N}
            \bg(\bu_1)^\sT\\ 
            \bJ_{\bu_T}
            \bg(\bu_1)^\sT
        \end{bmatrix}\bx=\bzero.
    \end{align*}
A similar calculation then shows that $\bJ_\bz\left(F_0{}\right) = \bzero$.

Now to prove \textbf{(2.)}, we compute the hessian $\grad_\bz^2F_0\in\R^{(m-r_k)\times(m-r_k)}$ as
     \begin{align} 
        \frac12\grad^2_\bz F_0(\bz) &=
         \bJ_\bz\tbpsi(\bz)^\sT\bJ_\bz\tbpsi(\bz)
         + \sum_{i=1}^{m}
        (\tbpsi_i(\bz)-\bw_i)\grad^2 \tbpsi_i(\bz)\\
        &\succeq \sigma_{\min}(\bJ_\bz\tbpsi(\bz))^2 - 
        \big\|(\grad^2 \tbpsi_i(\bz))_{i\in [m]}\big\|_\op\big\|\tbpsi(\bz)-\bw\big\|_2.
\nonumber
     \end{align}
     Both terms in the equation above can be further simplified using the definition of $\tbpsi$: namely, we have
$$\big\|(\grad^2\tbpsi_i(\bz))_{i\in [m]}\big\|_\op = \big\|(\grad^2\bpsi_j(\bz) )_{j\in[m-r_k]}\big\|_\op$$
and
     \begin{align}
             \bJ_\bz \tbpsi(\bz)^\sT\bJ_\bz \tbpsi(\bz) &= 
            \bI_{m-r_k}+
             \bJ_\bz\bpsi(\bz)^\sT\bJ_\bz\bpsi(\bz)^\sT
             \succeq 
             1-\norm{\bJ_\bz\bpsi(\bz)}_\op^2\\
             &\stackrel{(a)}\succeq 1 -
             \beta^2\hspace{-3mm} 
             \sup_{\bz\in \Ball_\beta^{m-r_k}(\bu_{0,T})}
             \norm{(\grad^2\bpsi_j(\bz) )_{j\in[m-r_k]}}_\op^2 ,
             \nonumber
     \end{align}
      where in $(a)$ we used $\bJ_\bz \bpsi(\bu_{0,T})=\bzero$ 
      and bounded the Jacobian at points near $\bu_{0,T}$ via a computation similar to that done in Lemma~\ref{lem:implicit-function}.
Finally, using Corollary \ref{cor:implicit-chart} to bound the second derivative and combining with the displays above gives
    \begin{equation}
        \frac12\grad^2_\bz F(\bz) \succeq 
        1 -\left( 8 \frac{(\sfA_{\bG,1}^\up{\tau})^2 \sfA_{\bG,2}^\up{\tau}}{\sfsigma_{\bG}^3}\beta\right)^2 -  8 \frac{(\sfA_{\bG,1}^\up{\tau})^2 \sfA_{\bG,2}^\up{\tau}}{\sfsigma_{\bG}^3}\beta \succ 0.
    \end{equation}
So $F$ is a strictly convex on the open convex set $\Ball_{\beta}^{m-r_k}(\bu_{0,T}).$

Finally, since $\bu_0 = \bu_1$, we have $\bJ_{\bu}\bg(\bu_0)^\sT \bx_0 =\bJ_{\bu}\bg(\bu_1)^\sT \bx_1.$ Then $\bx_0 = \bx_1$
follows from the fact that $\bJ_{\bu}\bg(\bu)$ is full rank on $\cM$.
\end{proof}


%\subsubsection{Local Diffeomorphism}
%\begin{lemma}[Projection onto the manifold]
%\label{lem:convex-projection}
%Fix arbitrary $\bu_0\in\cM\subset \R^m$ and $\tau\in(0,1)$ and work under 
%the definitions and assumptions of Corollary~\ref{cor:implicit-chart}.
%Let $\beta>0$ satisfy
%    \begin{equation}
%        \beta\le 
%        \frac{\sfsigma_\bG^3}{32(\sfA_{\bG,1}^{(\tau)})^2
%        \sfA_{\bG,2}^{(\tau)}}
%        \wedge
%        \frac{\sfsigma_\bG}{4}\tau.
%    \end{equation}
%For $\bw\in \cB^{m}_{\beta}(\bu_0)$, the function
%        $F_0(\bz):= \norm{\bpsi_0(\bz) -\bw}_2^2$
%    is strictly convex on the set $\cB^{m-r_k}_{\beta}(\bu_{0,T})$.
%\end{lemma}
%\begin{proof}
%For any $\tau_N,\tau_T$ as in Eq.~\eqref{eq:tau_N_tau_T}, 
%Corollary~\ref{cor:implicit-chart} furnishes the 
%functions $\tbpsi$ and $\bpsi$, defined therein, that are smooth on the set $\cB_{\tau_T}^{m-r_k}(\bu_{0,T})$.
%Since $\sfA_{\bG,1}^\up{\tau},\sfA_{\bG,2}^\up{\tau} \ge1$ and $\sfsigma_{\bG}\le 1$ by definition, it is easy to check that there exists a choice of $\tau_N,\tau_T$ satisfying Eq.~\eqref{eq:tau_N_tau_T}, so that $\beta \le \tau_T$. The function $\bpsi, \tbpsi$ are hence smooth on
%$\cB_\beta^{m-r_k}(\bu_{0,T})$.
%
%
%    This allows us to directly compute $\grad_\bz^2F_0\in\R^{(m-r_k)\times(m-r_k)}$ as
%     \begin{align} 
%        \frac12\grad^2_\bz F_0(\bz) =
%         \bJ_\bz\tbpsi(\bz)^\sT\bJ_\bz\tbpsi(\bz)
%         + \sum_{i=1}^{m}
%        (\tbpsi_i(\bz)-\bw_i)\grad^2 \tbpsi_i(\bz)
%        &\succeq \sigma_{\min}(\bJ_\bz\tbpsi(\bz))^2 - 
%        \big\|(\grad^2 \tbpsi_i(\bz))_{i\in [m]}\big\|_\op\big\|\tbpsi(\bz)-\bw\big\|_2.
%     \end{align}
%     Both terms in the equation above can be further simplified using the definition of $\tbpsi$: namely, we have
%$$\big\|(\grad^2\tbpsi_i(\bz))_{i\in [m]}\big\|_\op = \big\|(\grad^2\bpsi_j(\bz) )_{j\in[m-r_k]}\big\|_\op$$
%and
%     \begin{align}
%            & \bJ_\bz \tbpsi(\bz)^\sT\bJ_\bz \tbpsi(\bz) = 
%            \bI_{m-r_k}+
%             \bJ_\bz\bpsi(\bz)^\sT\bJ_\bz\bpsi(\bz)^\sT
%             \succeq 
%             1-\norm{\bJ_\bz\bpsi(\bz)}_\op^2
%             \stackrel{(a)}\succeq 1 -
%             \beta^2\hspace{-3mm} 
%             \sup_{\bz\in \cB_\beta^{m-r_k}(\bu_{0,T})}
%             \norm{(\grad^2\bpsi_j(\bz) )_{j\in[m-r_k]}}_\op^2 ,
%     \end{align}
%      where in (a) we used $\bJ_\bz \bpsi(\bu_{0,T})=\bzero$ 
%      and bounded the Jacobian at points near $\bu_{0,T}$ via a computation similar to that done in Lemma~\ref{lem:implicit-function}.
%Finally, using Corollary \ref{cor:implicit-chart} to bound the second derivative and combining with the displays above gives
%    \begin{equation}
%        \frac12\grad^2_\bz F(\bz) \succeq 
%        1 -\left( 8 \frac{(\sfA_{\bG,1}^\up{\tau})^2 \sfA_{\bG,2}^\up{\tau}}{\sfsigma_{\bG}^3}\beta\right)^2 -  8 \frac{(\sfA_{\bG,1}^\up{\tau})^2 \sfA_{\bG,2}^\up{\tau}}{\sfsigma_{\bG}^3}\beta \succ 0.
%    \end{equation}
%Hence, since the domain of $F$ is an open convex set, $F$ is a strictly convex.    
%\end{proof}
%
%\subsubsection{Global results from Local diffeomorphism}
%
%\begin{lemma}[Global bijection]
%    Consider the setting of lemma \ref{lem:convex-projection}.
%    Fix $\tau\in(0,1)$ and let $\beta>0$ satisfy the upper bound there.
%    The mapping $\bvarphi$ defined in Eq.\eqref{eq:noraml-tube-chart} is a global diffeomorphism on the tube $\cT\left(\beta/(2\sfA_{\bG,1})\right)$. 
%    %\kas{note $\sfA_{\bG,1}$ really needs to be only sup on the manifold cause I want this to get canceled later in the integral. Add the definition.}
%\end{lemma}
%\begin{proof}
%   Recall that by Lemma \ref{lem:local-diffeomorphis}, $\bvarphi$ is a local diffeomorphism on the tube $\cT\left(\beta/(2\sfA_{\bG,1})\right)$. To complete the proof, we need to show that it is also a bijection on this set.
%    Let $(\bu_0,\bx_0),(\bu_1,\bx_1)\in \cT\left(\beta/(2\sfA_{\bG,1})\right)$, and assume there exists some $\bw\in\R^{m}$ such that
%        \begin{align}
%       \bw &=
%       \varphi(\bu_0, \bx_0)  =
%       \varphi(\bu_1, \bx_1) \\
%       &= 
%       \bu_0+ \bJ_{\bu}\bg
%       (\bu_0)^\sT\bx_0 =
%       \bu_1+ \bJ_{\bu}\bg
%       (\bu_1)^\sT\bx_1.
%    \end{align}
%Then we have the bounds
%    \begin{align}
%        &\norm{\bu_1-\bu_0}_2 = \norm{\bJ_\bu \bg(\bu_0)^\sT\bx_0 - \bJ_\bu \bg(\bu_1)^\sT \bx_1}_2 \stackrel{(a)}\le       \sfA_{\bG,1}\left(\norm{\bx_0}_2+\norm{\bx_1}_2\right)\le \beta,
%    \end{align}
%since $\bu_0,\bu_1 \in\cM$ and by the choice of $\beta$, 
%and similarly
%\begin{align}
%        &\norm{\bw-\bu_0}_2 = \|{\bJ_{\bu} \bg(\bu_0)^\sT\bx_0}\|_2
%        \le \sfA_{\bG,1} \norm{\bx_0}_2\le \frac\beta2.
%\end{align}
%So $\bu_1\in\cB_{\beta}^{m}(\bu_0)$, $\bw\in \cB_\beta^{m}(\bu_0)$.
%Applying Corollary \ref{cor:implicit-chart} at $\bu_0$ for 
% 
%    \kas{we can apply implicit theorem since $\beta<\tau_N$ and $\beta\le \tau_T$ at the same time}     
%But by Lemma~\ref{lem:convex-projection}, the function
%$F_0(\bz) = \norm{\tbpsi(\bz) - \bw}^2$ is strictly convex on the set $\cB_\beta^{m-r_k}(\bu_{0,T})$
%
%    Further, for the given $\beta$ and $\bu_0$, by Corollary \ref{cor:implicit-chart}, there exists $\bz_1\in\cB^{m-r_k}_\beta(\bu_{0,T})$ such that $\bu_1 = \tbpsi(\bz_1)$. Hence around the point $\bu_0$ for the given radius $\beta$, Lemma \ref{lem:convex-projection} implies that the function 
%    $F_0(\bz) = \norm{\tbpsi(\bz) - \bw}^2$ is strictly convex on the set $\cB_\beta^{m-r_k}(\bu_{0,T})$.  We show that both $\bu_{0,T}$ and $\bz_1$ are critical points of this function, and therefore should be equal. Write
%    \begin{align}
%        \frac12\bJ_\bz F_0(\bz_1) =& 
%        \bJ_\bz \tbpsi(\bz_1)^\sT(\tbpsi(\bz_1)-\bw)\\
%        =&  \bJ_\bz(\bU(\bpsi(\bz_1),\bz_1))^\sT
%        (\bJ_{\bu}
%        \bg(\bu_1)^\sT\bx)\\
%        =& \begin{bmatrix}
%            \bJ_\bz\bpsi(\bz_1)^\sT,\bI
%        \end{bmatrix} \bU^\sT
%        (\bJ_{\bu}
%        \bg(\bu_1)^\sT\bx)\\
%        =&  \begin{bmatrix}
%            -(\bJ_{\bu_N}
%            \bg(\bu_1)^{-1}
%            \bJ_{\bu_T}
%            \bg(\bu_1))^\sT&
%            \bI
%        \end{bmatrix}\bU^\sT\bU
%        \begin{bmatrix}
%            \bJ_{\bu_N}
%            \bg(\bu_1)& 
%            \bJ_{\bu_T}
%            \bg(\bu_1)
%        \end{bmatrix}^\sT\bx\\
%        =& \begin{bmatrix}
%            -\bJ_{\bu_T}
%            \bg{(\bu_1)^\sT
%            (\bJ_{\bu_N}
%            \bg(\bu_1)^{-1}})^\sT&
%            \bI
%        \end{bmatrix}
%        \begin{bmatrix}
%            \bJ_{\bu_N}
%            \bg(\bu_1)^\sT\\ 
%            \bJ_{\bu_T}
%            \bg(\bu_1)^\sT
%        \end{bmatrix}\bx=\bzero,
%    \end{align}
%    Equivalently, $\bJ_\bz\left(\norm{\tbpsi(\bu_{0,T})-\bw}^2\right) = \bzero$.
%    This implies that $\bu_1=\bu_0$, and since $\bJ_\bu \bg(\bu_0)^\sT$ and $\bJ_\bu \bg(\bu_0)^\sT$ are both full rank we further get $\bx_0=\bx_1$. Hence, $\bvarphi$ is a bijection on the set $\cT\left(\frac{\beta}{2\sfA_{\bG,1}}\right)$.
%    
%\end{proof}
%
%
%\begin{lemma}\label{lemma:local-radius}
%    Consider the assumptions and setting of the Lemma \ref{lem:convex-projection}, and further assume that $\beta < \tau$.
%    Then, the mapping $\bvarphi$ defined in Eq.~\eqref{eq:noraml-tube-chart} is a bijection on the set 
%    \begin{equation}
%        \cN_\beta(\bu_0):=\left\{(\bu,\bx)\in \cT\bigg(\frac{\beta}{2\sfA_{\bG,1}^{(\tau)}}\bigg): \norm{\bu - 
%        \bu_0}_2 < \frac\beta2\right\}.
%    \end{equation}
%\end{lemma}
%\begin{proof}
%    Let $(\bu_1,\bx_1),
%    (\bu_2,\bx_2)\in \cN_\beta(\bu_0)$, and assume there exists some $\bw\in\R^{m}$
%
%    
%    Note for the given $\beta$ and $\bu_0$, by Corollary \ref{cor:implicit-chart}, there exists $\bz_1,\bz_2\in\cB^{m-r_k}_\beta(\bu_{0,T})$ such that $\bu_1 = \tbpsi(\bz_1)$ and $\bu_2 = \tbpsi(\bz_2)$. Hence,
%    \begin{align}
%       \norm{\bu_0-\bw}\le  \norm{\bu_1-\bw} + \norm{\bu_1-\bu_0} &\le
%        \norm{\bJ_{\bu}\bg(\bu_1)^\sT\bx_1} +\frac\beta2
%        \stackrel{(a)}\le 
%        \sfA_{\bG,1}^{(\tau)} \norm{\bx_1}_2 +\frac\beta2\le 
%        \beta,
%    \end{align}
%    where in (a) we used $\norm{\bu_1-\bu_0}_2\le\tau$. Hence for the given $\bu_0$, $\beta$, and $\bw$, Lemma \ref{lem:convex-projection} implies that the function $F_0(\bz) = \|{\tbpsi(\bz) - \bw}\|^2$ is strictly convex on the set $\cB^{m-r_k}_{\beta}(\bu_{0,T})$. We show that both $\bz_1$ and $\bz_2$ are critical points of this function, and therefore should be equal. Write
%    \begin{align}
%        \frac12\bJ_\bz F_0(\bz_1) =& 
%        \bJ_\bz \tbpsi(\bz_1)^\sT(\tbpsi(\bz_1)-\bw)\\
%        =&  \bJ_\bz(\bU(\bpsi(\bz_1),\bz_1))^\sT
%        (\bJ_{\bu}
%        \bg(\bu_1)^\sT\bx)\\
%        =& \begin{bmatrix}
%            \bJ_\bz\bpsi(\bz_1)^\sT,\bI
%        \end{bmatrix} \bU^\sT
%        (\bJ_{\bu}
%        \bg(\bu_1)^\sT\bx)\\
%        =&  \begin{bmatrix}
%            -(\bJ_{\bu_N}
%            \bg(\bu_1)^{-1}
%            \bJ_{\bu_T}
%            \bg(\bu_1))^\sT&
%            \bI
%        \end{bmatrix}\bU^\sT\bU
%        \begin{bmatrix}
%            \bJ_{\bu_N}
%            \bg(\bu_1)& 
%            \bJ_{\bu_T}
%            \bg(\bu_1)
%        \end{bmatrix}^\sT\bx\\
%        =& \begin{bmatrix}
%            -\bJ_{\bu_T}
%            \bg{(\bu_1)^\sT
%            (\bJ_{\bu_N}
%            \bg(\bu_1)^{-1}})^\sT&
%            \bI
%        \end{bmatrix}
%        \begin{bmatrix}
%            \bJ_{\bu_N}
%            \bg(\bu_1)^\sT\\ 
%            \bJ_{\bu_T}
%            \bg(\bu_1)^\sT
%        \end{bmatrix}\bx=\bzero,
%    \end{align}
%    Equivalently, $\bJ_\bz\left(\norm{\tbpsi(\bz_2)-\bw}^2\right) = \bzero$.
%    This implies that $\bu_1=\bu_2$, and hence $\bvarphi$ is a bijection on the set $\bN_\eps(\bu_0)$.
%    
%\end{proof}
%
%Now we have everything to show that $\bvarphi$ is a global diffeomorphism:
%
%
%\begin{proof}[Proof of lemma \ref{lem:global-radius}]
%    By lemma \ref{lem:local-diffeomorphis}, we know that for the given $\beta$, $\bvarphi$ is a local diffeomorphism for $\cT(\beta)$.
%    To complete the proof, we must show that $\bvarphi$ is a bijection on $\cT(\beta)$. By contradiction, assume that there exists 
%    $(\bu_1,\bx_1),(\bu_2,\bx_2)\in \cT(\beta)$ such that
%    \begin{equation}
%        \bu_1
%        +\bJ_{\bu}\bg(\bu_1)^\sT\bx =
%        \bu_2 + \bJ_{\bu}\bg(\bu_2)^\sT\bx.
%    \end{equation}
%    This implies that
%    \begin{equation}
%        \norm{\bu_1-
%        \bu_2} = \norm{\bJ_\bu\bg(\bu_1)^\sT\bx_1-\bJ_{\bu}\bg(\bu_2)^\sT\bx_2}\le \sup_{(\bu)\in\cM}
%        \norm{\bJ_{\bu}\bg(\bu)}_\op (\norm{\bx_1}+\norm{\bx_2})\le 2\sfD \eps_\cM .
%    \end{equation}
%    However, for the given $\eps_\cM$, we can use lemma \ref{lemma:local-radius}, this time around the point $\bu_1$, to conclude that $\bvarphi$ is a bijection on the set 
%    \begin{equation}
%        \{(\bu,\bx)\in \cT(2\eps_\cM): 
%        \norm{\bu_1-\bu}\le 2\sfD \eps_\cM\},
%    \end{equation}
%    which is a contradiction. This completes the proof.   
%\end{proof}
%

\subsubsection{Integration over the manifold }
\begin{lemma}[Manifold integral lemma]
\label{lem:intg-tube} 
Let $f:\cM\rightarrow \R$ be a differentiable nonnegative function. Assume
$\beta$ satisfies
    \begin{equation}
        \beta\le 
        \sup_{t > 0} \left\{
        \frac{\sfsigma_\bG^3}{64(\sfA_{\bG,1}^{(t)})^2
        \sfA_{\bG,2}^{(t)}}
        \wedge
        \frac{\sfsigma_\bG}{8}t\right\}.
    \end{equation}
Then
    \begin{equation}
        \int_{\bu\in \cM} f(\bu) \de_\cM V
        \le  
        \Err_{\textrm{blow-up}}(n,\beta) 
\exp\{\beta\;\norm{\log f}_{\Lip,\cM^{(\beta)}}\}
        \int_{\by\in \cM^{(\beta)}} f(\by)\de\by
    \end{equation}
    where
    \begin{equation}
       \Err_{\textrm{blow-up}}(n,\beta)  :=\left(\frac1{1 - \beta\;\sfA_{\bG,2}/\sfA_{\bG,1}}\right)^{(m_n-r_k)/2}
        \left(\frac{\sqrt{r_k} \sfA_{\bG,1}}{\beta(\sfsigma_{\bG} - \beta \sfA_{\bG,2}/\sfA_{\bG,1})}\right)^{r_k}
    \end{equation}
\end{lemma}
\begin{proof}
Fix $\beta$ to satisfy the condition of the lemma and let $\tbeta := \beta /\sfA_{\bG,1}.$
Recall the definition of Eq.~\eqref{eq:tube_def}. We can upper bound the integral of interest by an integral over $\cT(\tbeta)$ as
    \begin{align}
        \int_{\bu\in\cM} f(\bu)\de_\cM V& \stackrel{(a)}\le 
        \left(\frac{\sqrt{r_k}}{\tbeta}\right)^{r_k}
        \int_{\bu\in\cM} \int_{\bx\in\R^{{r_k}}} 
        f(\bu)\one_{\{\norm{\bx}_2\le \tbeta\}} \de_\cM V\de\bx
         = \left(\frac{\sqrt{r_k}}{\tbeta}\right)^{r_k} 
        \int_{(\bu,\bx)\in \cT(\tbeta)} f(\bu) \de_{\cM\times\R^{r_k}} V,
        \label{eq:manifold_int_lemma_bound_1}
    \end{align}
   where $\de_{\cM\times\R^{r_k}} V$ is the volume element of $\cM\times \R^{r_k},$
  and in $(a)$ we used $\vol(\Ball^{r_k}_{\tbeta}(\bzero))^{-1} \le \left(\sqrt{r_k}/{\tbeta}\right)^{r_k}$.

By Lemma~\ref{lemma:global_diff}, $\varphi$ is a global diffeomorphism on $\cT(\tbeta)$ for the choice of $\tbeta$. Then using the uniform lower bound on
$|\det(\de\bvarphi^{-1}(\bx))| = |\det  \left(\de\bvarphi(\bx)\right)^{-1}|$ of Lemma~\ref{lem:local-diffeomorphis}, we can bound the integral over $\cT(\tbeta)$ in the previous display as
  \begin{align}
  \nonumber
        \int_{(\bu,\bx)\in \cT(\tbeta)} f(\bu) \de_{\cM\times\R^{r_k}} V
        & \stackrel{(b)}{=} 
        \int_{\by\in\bvarphi(\cT(\tbeta))} f(\tbpi_\cM(\by)) \big| 
        \det\left( \de\bvarphi^{-1}(\by) \right)\big|\de\by\\
        &{\le}
        \left(\frac1{1 - \tbeta\;\sfA_{\bG,2}}\right)^{(m-r_k)/2}
        \left(\frac1{\sfsigma_{\bG} - \tbeta \sfA_{\bG,2}}\right)^{r_k}
        \int_{\by\in\bvarphi(\cT(\tbeta))} f(\tbpi_\cM(\by)) \de\by.
        \label{eq:manifold_int_lemma_bound_2}
  \end{align}
Now for any  $\by\in \bvarphi(\cT(\tbeta))$,
since $\bvarphi$ is bijective on this set, there exists a unique $(\bu,\bx) \in \cT(\tbeta)$ such that $\by = \bvarphi(\bu,\bx) = \bu + \bJ_{\bu}\bg(\bu)^\sT\bx$. From this we see that
    \begin{align}
        &\norm{\by - \bu}_2 = 
        \|{\bJ_{\bu}
        \bg(\bu)^\sT\bx}\|_2
        \le \|{\bJ_{\bu}
        \bg(\bu)}\|_\op\norm{\bx}\le
        \sfA_{\bG,1} \tbeta = \beta, \quad\quad\textrm{and similarly}\quad\quad
        \norm{\by - \tbpi_\cM(\by)} 
        \le \beta.
    \end{align}
So
\begin{equation}
 \bvarphi(\cT(\tbeta))\subseteq\cM^{(\beta)}\quad\quad\textrm{and}\quad\quad
   f(\tbpi_{\cM}(\by)) \le \exp \left\{\|\log f\|_{{\Lip},\cM^{(\beta)}}\beta\right\} f(\by).
\end{equation}
These along with the nonnegativity of $f$ now give the bound
\begin{equation}
        \int_{\by\in\bvarphi(\cT(\tbeta))} f(\tbpi_\cM(\by)) \de\by \le
        \exp \left\{\|\log f\|_{{\Lip},\cM^{(\beta)}}\beta\right\}
        \int_{\by\in \cM^{(\beta)}} f(\by)\de\by.
\end{equation}
Combining with the bounds in Eq.~\eqref{eq:manifold_int_lemma_bound_1} and
Eq.~\eqref{eq:manifold_int_lemma_bound_2} yields the lemma.
\end{proof}



\subsubsection{Proof of Lemma~\ref{lemma:manifold_integral}}

By computing derivatives of the component of $g_j$ for $j \in[r_k]$ and using the 
Lipschitz assumptions of Assumption~\ref{ass:loss}
on the partials of $\ell$ and the continuity assumptions of Assumption~\ref{ass:regularizer} on the partials of $\rho$, one can check that there exists a constant $C_0(\sfA_{\bV},\sfA_{\bR})>0$ 
depending only on $\sfA_{\bV},\sfA_{\bR}$, such that
\begin{equation}
    \sup_{\bu \in\cM^\up{1}}\max_{j \in [r_k]}\|\grad g_{j}(\bu)\|_2 
   \vee  
    \sup_{\bu \in\cM^\up{1}}\max_{j \in [r_k]}\|\grad^2 g_{j}(\bu)\|_\op
    \le  C_0(\sfA_{\bV},\sfA_{\bR})\; r_k.
\end{equation}
So by definition of $\sfA_{\bG,1}^\up{t},\sfA_{\bG,2}^\up{t}$, 
\begin{equation}
   1 \le \sfA_{\bG,1}\vee \sfA_{\bG,2} \le \sfA_{\bG,1}^{\up{1}}
 \vee \sfA_{\bG,2}^{\up{1}} \le  r_k^2 \; C_0(\sfA_\bV,\sfA_\bR).
\end{equation}
And so there exists some constant $C_1 = C_1(\sfA_{\bV},\sfA_{\bR})> 0$ such that if 
\begin{equation}
   \beta \le \frac{C_1(\sfA_{\bV},\sfA_{\bR}) \sfsigma_{\bG}^3}{r_k^6},
\end{equation}
then $\beta$ satisfies the upper bound of Lemma~\ref{lemma:manifold_integral}.
\qed

\subsection{Bounding the density of the gradient process: Proof of Lemma~\ref{lemma:density_bounds}}
\label{sec:density_bound}

We begin by computing the density $p_{\bTheta,\bbV}(\bzero)$ appearing in Lemma~\ref{lemma:kac_rice_manifold}.

\begin{lemma}[Density] 
\label{lemma:density}
Let $\bB_{\bSigma}$ be as in Corollary~\ref{cor:proj}.
For $(\bTheta,\bbV)\in \cM$, the density of $\bz(\bTheta,\bbV) = \bB_{\bSigma}(\bTheta,\bV))^\sT\bzeta(\bTheta,\bbV)$ at $\bzero$ is given by 
\begin{equation}
\label{eq:density}
  p_{\bTheta,\bbV}(\bzero) 
    :=
\frac{
    \exp\left\{-\frac{1}2\left(
    n^2\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bRho\right) + \Tr(\bbV \bR^{-1}\bbV) + n\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bL^\sT \bbV \bR^{-1}(\bTheta,\bTheta_0)^\sT\right)
%    \Tr\left(\bbV
%\bR^{-1}(\bTheta)
%    \bbV^\sT\right)
    \right)\right\}}
    {
\det^*(2\pi\bSigma(\bTheta,\bbV))^{1/2}
    }
\end{equation}
%\begin{equation}
%\label{eq:density}
%  p_{\bTheta,\bbV}(\bzero) 
%    :=
%%\frac{
%%\det^*(\bSigma(\bTheta,\bbV))^{-1/2}
%%\det\left( \bR(\bTheta) \right)^{-n/2}\det\left(\bL^\sT\bL\right)^{-d/2}
%%}{(2 \pi)^{(dk + (n-k)(k+k_0))/2}}
%\det^*(2\pi\bSigma(\bTheta,\bbV))^{-1/2}
%    \exp\left\{-\frac12\left(
%\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bRho\right) + \Tr(\bbV \bR^{-1}\bbV) + \Tr\left(\bRho (\bL^\sT\bL)^{-1}\bL^\sT \bbV \bR^{-1}(\bTheta,\bTheta_0)^\sT\right)
%%    \Tr\left(\bbV
%%\bR^{-1}(\bTheta)
%%    \bbV^\sT\right)
%    \right)\right\}
%\end{equation}
where $\det^*$ denotes the product of the non-zero eigenvalues.
\end{lemma}
\begin{proof}
In what follows, let us suppress the argument $(\bTheta,\bbV)$ throughout.
Recall Lemma~\ref{lemma:eig_vecs_NS_Sigma} giving the mean and covariance of $\bzeta$.
What we need to show is that the quantity multiplying the factor $-1/2$ in the exponent of~\eqref{eq:density} is equal to 
$\bmu^\sT\bB (\bB^\sT\bSigma\bB)^{-1}\bB^\sT \bmu =
\bmu^\sT\bSigma^\dagger \bmu$. This fact follows from straightforward  (albeit tedious) algebra after applying the stationary condition $\bG = \bzero$. 

Indeed, to see this, let $\ba = (\ba_1^\sT ,\ba_2^\sT)^\sT$ where $\ba_1 \in\R^{dk}$, $\ba_2 \in\R^{n(k+k_0)}$, such that
$\bSigma^\dagger \bmu = \ba.$ 
Since $\bmu$ is orthogonal to the null space of $\bSigma$, we must have $\bSigma \ba = \bmu$, and hence
\begin{align}
\label{eq:pinv_lin_eq}
    \left(\bL^\sT \bL \otimes \bI_d \right)\ba_1 &+ [\bM, \bM_0] \ba_2 
    =  \overline\brho\\
    [\bM,\bM_0]^\sT \ba_1 &+(\bR \otimes \bI_n) \ba_2   = -\overline \bv
\end{align}
where $\overline \bv \in \R^{n(k+k_0)}$ and $\overline \brho\in\R^{dk}$ denotes the concatenation of the columns of $\bbV$ and $n\bRho$, respectively. 
Solving the for $\ba_1$ in terms of $\ba_2$, and vice-versa for the second equation allows us to conclude that 
\begin{align}
\label{eq:muTa}
   \bmu^\sT\bSigma^\dagger \bmu = \bmu^\sT\ba &=  \overline\brho^\sT(\bL^\sT\bL \otimes \bI_d)^{-1}\overline\brho + \overline\bv^\sT(\bR\otimes \bI_n)^{-1}\overline \bv 
   \underbrace{-\overline \brho^\sT(\bL^\sT\bL \otimes \bI_d)^{-1} [\bM,\bM_0]\ba_2}_{=:\textrm{(I)}}\\
   &\quad+ \underbrace{\overline\bv^\sT(\bR\otimes \bI_n)^{-1}[\bM,\bM_0]^\sT \ba_1}_{=:\textrm{(II)}}.
   \nonumber
\end{align}
Now write
\begin{align*}
  \textrm{(I)}  &\stackrel{(a)}{=}-\overline \btheta^\sT\left(\bI_k \otimes \bRho (\bL^\sT\bL)^{-1}\bL\right)\ba_2\\
  &\stackrel{(b)}=
   \overline\bv^\sT \left(\bI_k \otimes  \bL(\bL^\sT \bL)^{-1}\bL^{\sT}\right)
\ba_2\\ 
&=  \overline\bv^\sT(\bR\otimes \bI_n)^{-1}
\left(\bR \otimes \bL(\bL^\sT \bL)^{-1}\bL^{\sT}\right)
\ba_2\\
&\stackrel{(c)}{=}   \overline\bv^\sT(\bR\otimes \bI_n)^{-1}[\bM,\bM_0]^\sT (\bL^\sT\bL \otimes \bI_d)^{-1}[\bM,\bM_0]\ba_2\\
&\stackrel{(d)}{=} -\textrm{(II)} + \overline\bv^\sT (\bR \otimes \bI_n)^{-1} [\bM,\bM_0]^\sT (\bL^\sT\bL \otimes \bI_d)^{-1} \overline\brho,
\end{align*}
where in $(a)$ we used $\overline\btheta\in\R^{d(k+k_0)}$ to denote the concatenation of the columns of $(\bTheta,\bTheta_0)$, in $(b)$ we used the constraint $\bG(\bbV,\bTheta) =\bzero$, in $(c)$ we used  the identity (easily verifiable directly from the definitions)
\begin{equation}
   [\bM,\bM_0]^\sT(\bL^\sT\bL \otimes \bI_d)^{-1}[\bM,\bM_0] = \bR\otimes \bL(\bL^\sT\bL)^{-1}\bL^\sT,
\end{equation}
and in $(d)$ we used Eq.~\eqref{eq:pinv_lin_eq} to write $\ba_1$ appearing in \textrm{(II)} in terms of $\ba_2.$
Combining with Eq.~\eqref{eq:muTa} we conclude that
\begin{equation}
\bmu^\sT \bSigma^\dagger \bmu  = n^2 \Tr\left(\bRho (\bL^\sT\bL)^{-1}\bRho\right) + \Tr(\bbV \bR^{-1}\bbV) + n\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bL^\sT \bbV \bR^{-1}(\bTheta,\bTheta_0)^\sT\right).
\end{equation}
\end{proof}

Next, the following lemma bound the pseudo determinant term appearing in the expression for the density above.
%This section gives a bound on the density term of Lemma~\ref{lemma:density}.
%First we deal with the covariance.
%Recall the definition of $\bSigma(\bTheta,\bbV)$ in Lemma~\ref{lemma:mean_cov}.
%Asymptotically, one expects that the psuedo-determinant term appearing in the density in Lemma~\ref{lemma:density} asymptotically satisfies $\det^*(\bSigma(\bTheta,\bbV)) \asymp \det(\bR(\bTheta))^{n} \det(\bL(\bbV)^\sT\bL(\bbV))^d$ since it's a rank $r_k$ perturbation away from a block diagonal matrix whose determinant is the latter quantity. The following lemma gives a bound to this effect.
\begin{lemma}[Bounding the determinant of the covariance]
\label{lemma:det_star_bound}
Let $r_k := k^2 + k_0 k$.
Under the assumptions of Section~\ref{sec:assumptions},
for any $(\bTheta,\bbV)  \in \cM(\cuA,\cuB)$, we have
   \begin{equation}
       \det^* \left(\bSigma(\bTheta,\bbV))\right)^{-1/2} \le \det(\bR(\bTheta))^{-n/2} \det(\bL(\bbV)^{\sT}\bL(\bbV))^{-(d-r_k)/2}
   \end{equation}
\end{lemma}
\begin{proof}
We'll suppress the index $(\bTheta,\bbV)\in\cM$ throughout the proof.
   Recall the definition of $\bSigma$.
   Let $\bM_1 := [\bM,\bM_0]\in\R^{dk \times (nk + nk_0)}$ where $\bM,\bM_0$ are the off diagonal blocks of $\bSigma$ defined in that lemma.
For any $\eps >0$, we have
\begin{align*}
   \det\left(\bSigma + \eps\bI\right)  &\ge \det\left(
   \begin{pmatrix}
       \bL^\sT \bL \otimes \bI_{d} + \eps \bI_{kd}  & \bM_1 \\
       \bM_1^\sT  & \bR \otimes \bI_n 
   \end{pmatrix}
   \right)\\
&=\det\left(\bR \otimes \bI_n\right) 
\det\left(
\left(\bL^\sT\bL + \eps \bI_{k}\right)\otimes \bI_d - \bM_1 \left(\bR^{-1}\otimes \bI_n\right) \bM_1^\sT
\right).
\end{align*}
With some algebra, one can show that
\begin{equation}
   \bM_1 \left(\bR^{-1}\otimes \bI_n\right) \bM_1^\sT = \bL^\sT\bL \otimes (\bTheta,\bTheta_0)\bR^{-1}(\bTheta,\bTheta_0)^\sT.
\end{equation}
Denoting the rank $r_k$ orthogonal projector $\bP_\bR :=
(\bTheta,\bTheta_0)\bR^{-1}(\bTheta,\bTheta_0)^\sT\in\R^{d\times d}$ and using $\bP_R^\perp$ for the complementary orthogonal projector, we can compute the second determinant term in the above display as
\begin{align*}
    \det\left(
\left(\bL^\sT\bL + \eps \bI_{k}\right)\otimes \bI_d - \bM_1\left(\bR\otimes \bI_n\right) \bM_1^\sT
\right) &=  \det\left( (\bL^\sT\bL + \eps \bI_{k})\otimes \bP_\bR^\perp +
(\bL^\sT\bL + \eps\bI_{k}) \otimes \bP_\bR - \bL^\sT\bL\otimes \bP_\bR
\right)\\
&=\det\left( (\bL^\sT\bL + \eps \bI_{k})\otimes \bP_\bR^\perp +
\eps\bI_{k} \otimes \bP_\bR
\right)\\
&= \det\left(\bL^\sT\bL+ \eps \bI_{k}\right)^{d-r_k} \eps^{r_k}.
\end{align*}
So we conclude that for any $\eps >0$,
\begin{equation}
   \det(\bSigma +\eps \bI) \ge \det(\bR)^n \det(\bL^\sT\bL + \eps\bI_k)^{d-r_k}  \eps^{r_k}.
\end{equation}
Using that the dimension of the nullspace of $\bSigma$ is $r_k$ by Lemma~\ref{lemma:eig_vecs_NS_Sigma}, we then have
\begin{align*}
    \det^*(\bSigma) &:= \lim_{\eps \to 0} \frac1{\eps^{r_k}} \det(\bSigma + \eps\bI)\\
    &\ge \det(\bR)^{n}  \det(\bL^\sT\bL )^{d-r_k}
\end{align*}
as claimed.
\end{proof}
\begin{proof}[Proof of Lemma~\ref{lemma:density_bounds}]
The proof is a direct corollary of Lemma~\ref{lemma:det_star_bound}.
Indeed, rewriting the expression for $p_{\bTheta,\bbV}(\bzero)$ from Lemma~\ref{lemma:density} in terms of $\hmu,\hnu$, and ignoring exponentially trivial factors for large enough $n$, we reach the statement of the lemma.
\end{proof}

Finally, for future reference, we record the following uniform bound on the density.
\begin{corollary}[Uniform bound on the density]
\label{cor:uniform_density_bound}
In the setting of Lemma~\ref{lemma:density_bounds}, we have the following uniform bound holding for all $(\bTheta,\bbV) \in\cM$:
\begin{equation}
    p_{\bTheta,\bbV}(\bzero) \le \frac{\sfsigma_{\bR}^{-nk/2} \sfsigma_{\bL}^{-(d-r_k)/2}}{ (2\pi)^{(dk + nk + nk_0 -r_k)/2} n^{dk/2}}
\end{equation}
\end{corollary}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of the determinant: Proof of Lemma~\ref{lemma:CE_bound}}
\label{sec:determinant_bound}
\subsubsection{Relating the determinant of the differential to the determinant of the Hessian: Proof of Lemma~\ref{lemma:lb_singular_value_Df}}
%The objective of analysis in this section is the term
%$\E[|\det( \de\bz(\bTheta,\bbV) )| \one_{\bH(\bTheta,\bbV)  \succ n\sfsigma_\bH} | \bzeta =\bzero,\bw ]$, for $(\bTheta,\bbV) \in\cM(\cuA,\cuB)$.
%We will relate the determinant of $\de z(\bTheta,\bbV))$ by the determinant of $\bH(\bTheta,\bbV)$ by a perturbation argument. For this reason, we require a lowerbound on the smallest singular value of $\de \bz(\bTheta,\bbV)$. The following lemma, whose proof is largely algebraic, gives this lower bound.

%\begin{lemma}[Lower bound on the singular values of $\bJ_{(\bTheta,\bbV)} \bzeta $]
%\label{lemma:lb_singular_value_Df}
%%Let $\grad^2 \rho(\bTheta) \in\R^{dk \times dk}$ be the Hessian of $\rho$ viewed as a map $\rho : \R^{dk} \to \R$.
%Under Assumption~\ref{ass:loss} and~\ref{ass:regularizer}, we have the bound
%   \begin{equation}
%\sigma_{\min}(\bJ_{(\bTheta,\bbV)} \bzeta) \ge \;\frac{\sigma_{\min}(\bH(\bTheta,\bbV))}{\Err_{{\sigma}}(\bX^\sT\bX; \sfK,\tilde\sfK)}
%   \end{equation}
%   where the multiplicative error is given by
%\begin{equation}
%\Err_\sigma(\bX^\sT\bX;\sfK,\tilde\sfK) :=
%C(\sfK,\tilde\sfK, \sfA_\bR)
%\left(
%\|(\bX^\sT\bX)^{-1}\|_\op^{3/2} +1
%\right)
%\left(\|\bX^\sT\bX\|_\op^{7/2}+ n^3\right)
%%    E(\bX^\sT\bX;\sfK,\tilde\sfK) := C(\|\bX^\sT\bX\|_\op^{5/2} \|(\bX^\sT\bX)^{-1}\|_\op^{3/2} (\sfK^2 + \tilde{\sfK}^2 + 1 ))
%\end{equation}
%for some universal constant $C>0$ depending only on $\sfK,\tilde\sfK$.
%\end{lemma}
We suppress  the dependence on $(\bTheta,\bbV)$ in what follows.
In analogy with the definition of $\bSec$ in Eq.~\eqref{eq:SecDef},
define $\tilde{\bSec}$ as follows
%
\begin{equation}
   \tilde\bSec := \begin{pmatrix}
\tilde\bSec_{i,j}(\bbV)
   \end{pmatrix}_{i,j \in[k]}
,\quad
    \tilde\bSec_{i,j}:= \Diag\left\{\frac{\partial^2}{\partial {v_i}\partial {u_j}}\ell(\bbV)\right\}.\label{eq:TildeSecDef}
\end{equation}
We'll also introduce the notation
\begin{equation}
\bS := n\grad^2\rho(\bTheta)\quad\quad
    \tbH := (\bI_k\otimes\bX)^\sT \tilde\bSec (\bI_k \otimes \bX),
    \quad\quad
    \widehat\bSigma := (\bI_k\otimes\bX)^\sT(\bI_k\otimes\bX).
\end{equation}
Without loss of generality, we'll assume $\bS$ is invertible throughout (otherwise we can perturb $\bS$ and take the perturbation parameter to $0$).
A direct computation yields
    \begin{equation}
        \bJ \bzeta = \begin{pmatrix}
            \bS& (\bI_k\otimes\bX)^\sT\bSec &
            (\bI_k\otimes \bX)^\sT\tilde\bSec\\
            \bI_k\otimes\bX& -\bI&\bzero\\
            \bzero &\bzero &-\bI
        \end{pmatrix}.
    \end{equation}
We'd like our bounds to be in terms of the matrix $\bH$ which is interpreted as the second derivative at the minimizer, instead of the obtuse quantity $(\bI\otimes \bX)^\sT\bK^2 (\bI\otimes \bX).$ For this reason, we introduce the approximate isometry
%\bns{I will normalize by $\sqrt{n}$ at the end. Perhaps the correct way to do this is to normalize only $\bX$ by $\sqrt{n}$ and not $\bI$, but it turns out the bound is the same order regardless using this approach because we're using a crude bound on the operator norm of the inverse later.}
%\begin{equation}
%    \bN_0 := \begin{pmatrix}
%        \bI & \bzero  &  \bzero\\
%        \bzero & \frac1{\sqrt{n}}(\bI \otimes \bX) & \bzero\\
%        \bzero & \bzero & \frac1{\sqrt{n}} (\bI \otimes \bX).
%    \end{pmatrix}
%\end{equation}
\begin{equation}
    \bN_0 := \begin{pmatrix}
        \bI_{dk} & \bzero  &  \bzero\\
        \bzero & (\bI_k \otimes \bX) & \bzero\\
        \bzero & \bzero & (\bI_k \otimes \bX)
    \end{pmatrix}
\end{equation}
which satisfies $\norm{\bN_0}_\op \le \norm{\bX}_\op$.
Then, to lower bound $\sigma_{\min}(\bJ \bzeta),$ we will use the inequality
\begin{equation}
\label{eq:lb_isometry}
    \sigma_{\min}(\bJ \bzeta)\ge \norm{\bX}_\op^{-1} \sigma_{\min}(\bJ \bzeta\; \bN_0),
\end{equation}
and then bound $\sigma_{\min}(\bD \bzeta \bN_0)$ by 
bounding the operator norm of the inverse of $\bN_1 := (\bD\bzeta \bN_0)^\sT(\bD \bzeta \bN_0)$ and bounding its operator norm block-wise. 
This matrix $\bN_1$ can be straightforwardly computed to be
%\begin{equation}
%   \bN_1 := (\bD \bzeta(\bt) \bN_0)^{\sT}(\bD \bzeta(\bt) \bN_0)
%   := \begin{pmatrix}
%       \widehat \bSigma &  -\frac1{\sqrt{n}}\widehat\bSigma & \bzero\\
%    -\frac1{\sqrt{n}} \widehat\bSigma & \frac1n\bH^2 + \frac1n \widehat\bSigma & \frac1n \bH \tilde\bH^\sT\\
%    \bzero & \frac1n \tilde \bH\bH & \frac1n \tilde\bH \tilde\bH^\sT +\frac1n \widehat \bSigma
%   \end{pmatrix}.
%\end{equation}
\begin{equation}
   \bN_1 
   = \begin{pmatrix}
       \bS^2 + \widehat \bSigma &  \bS \bH -\widehat\bSigma & \bS \tbH\\
    \bH \bS - \widehat\bSigma & \bH^2 +  \widehat\bSigma &  \bH \tbH\\
    \tbH^\sT\bS &  \tbH^\sT\bH &  \tbH^\sT \tbH +\widehat \bSigma
   \end{pmatrix}.
\end{equation}

\noindent So letting
\begin{equation}    
\bA_0 := \bS^2 + \widehat \bSigma,\quad
\bB_0  :=  \begin{pmatrix}
   \bS\bH-  \widehat\bSigma &  \bS\tbH
\end{pmatrix},
\quad
\bC_0 :=\begin{pmatrix}
   \bH^2 + \widehat\bSigma &  \bH \tbH\\
     \tbH^\sT\bH & \tbH^\sT \tbH + \widehat \bSigma
\end{pmatrix},\quad
\bD_0 := \bC_0 - \bB^\sT_0 \bA_0^{-1} \bB_0,
\end{equation}
%\begin{align}
%(\bD_0 - \bC_0 \bA_0^{-1} \bB_0)^{-1} = 
%\begin{pmatrix}
%    \frac1n\bH^2  & \frac1n \bH \tilde\bH^\sT\\
%    \frac1n \tilde \bH\bH & \frac1n \tilde\bH \tilde\bH^\sT +\frac1n \widehat \bSigma
%\end{pmatrix}^{-1}
%=:
%\begin{pmatrix}
%    \bA_1 & \bB_1\\
%    \bC_1 & \bD_1\\
%\end{pmatrix}^{-1}.
%\end{align}
we note that since,
\begin{equation}
    \bN_1^{-1} = \begin{pmatrix}
        \bA_0^{-1} + \bA^{-1} \bB_0 \bD_0^{-1} \bB_0^\sT \bA^{-1} & 
- \bA^{-1} \bB_0 \bD_0^{-1}\\
-\bD_0^{-1} \bB_0^\sT \bA^{-1} &  \bD_0^{-1}
    \end{pmatrix},
\end{equation}
we can bound  
\begin{equation}
\label{eq:M1_norm_bound}
    \norm{\bN_1^{-1}}_\op \le \norm{\bA_0^{-1}}_\op  + \left(1 + \norm{\bA_0^{-1}}_\op\norm{\bB_0}_\op\right)^2\norm{\bD_0^{-1}}_\op.
\end{equation}

Let us first compute $\bD_0$, then bound the norm of its inverse. We'll explicate the computation only for the upper-left and lower-right blocks; for the former we have
\begin{align*}
   &\bH^2 + \widehat\bSigma - (\bH \bS - \widehat\bSigma)(\bS^2 + \widehat\bSigma)^{-1}(\bS \bH - \widehat\bSigma)\\
    &= \bH\left(\bI - (\bI + \bS^{-1}\widehat\bSigma\bS^{-1})^{-1}\right)\bH
    + \widehat\bSigma^{1/2}\left(\bI - (\bI + \widehat\bSigma^{-1/2}\bS^2 \widehat\bSigma^{-1/2})^{-1} \right)\widehat\bSigma^{1/2} + \bH\bS(\bS^2 + \widehat\bSigma)^{-1}\widehat\bSigma\\
 &\quad+\widehat\bSigma(\bS^2 + \widehat\bSigma)^{-1}\bS\bH
    \\
    &= \bH(\bI + \bS^{-1}\widehat\bSigma\bS^{-1})^{-1}\bS^{-1}\widehat\bSigma\bS^{-1}\bH
    +\widehat\bSigma^{1/2}(\bI + \widehat\bSigma^{-1/2}\bS^2 \widehat\bSigma^{-1/2})^{-1}
    \widehat\bSigma^{-1/2}\bS^2+ \bH\bS(\bS^2 + \widehat\bSigma)^{-1}\widehat\bSigma\\
    &\quad+\widehat\bSigma(\bS^2+ \widehat\bSigma)^{-1}\bS\bH\\
    &=  \bH(\bS\widehat\bSigma^{-1}\bS + \bI)^{-1} \bH + \bS(\bI + \bS \widehat\bSigma^{-1}\bS)^{-1}\bS
    +  \bH(\bS\widehat\bSigma^{-1}\bS + \bI)^{-1} \bS + \bS(\bS\widehat\bSigma^{-1}\bS + \bI)^{-1} \bH\\
    &= (\bH+\bS)(\bS\widehat\bSigma^{-1}\bS + \bI)^{-1} (\bH +\bS).
\end{align*}
Meanwhile, for the lower-right block we compute
\begin{align*}
    \tbH^\sT\tbH + \widehat\bSigma - \tbH^\sT \bS(\bS^2 + \widehat\bSigma)^{-1} \bS\tbH 
    &= \tbH^\sT\left(\bS^{-1}\widehat\bSigma \bS^{-1} + \bI\right)^{-1} \tbH + \widehat\bSigma.
\end{align*}
So denoting
\begin{equation}
    \tilde\bS := \bS\widehat\bSigma^{-1}\bS + \bI,
\end{equation}
a similar computation gives the remaining blocks which allows us to write
\begin{align*}
   \bD_0
   &= \begin{pmatrix}
       (\bH+\bS)\tbS^{-1} (\bH +\bS) & (\bH+\bS)\tbS^{-1} \tbH\\
       \tbH^\sT\tbS^{-1}(\bH +\bS) & 
       \tbH^\sT\tbS^{-1}\tbH + \widehat\bSigma
   \end{pmatrix} =: 
   \begin{pmatrix}
      \bA_1 & \bB_1\\
      \bB_1^\sT &\bD_1
   \end{pmatrix}.
\end{align*}
%\begin{align}
%    \widehat\bH^\sT\tilde\bH  + \widehat\bSigma  - \widehat\bH^\sT\bS(\bS^2 + \widehat\bSigma)^{-1}\bS\tilde\bH &= \tilde\bH^\sT(\bS\widehat\bSigma^{-1}\bS + \bI)^{-1}\tilde\bH + \widehat\bSigma.
%\end{align}
Now to invert $\bD_0$, a straightforward computation gives
$(\bD_1 - \bB_1^\sT \bA_1^{-1} \bB_1)^{-1} =    \widehat\bSigma^{-1}$
and so 
\begin{align*}
   \bD_0^{-1}
&= 
\begin{pmatrix}
   (\bH +\bS)^{-1}\tbS (\bH +\bS)^{-1} + (\bH +\bS)^{-1}\tbH \widehat\bSigma^{-1}\tbH(\bH +\bS)^{-1}
   & -(\bH +\bS)^{-1}\tbH \widehat\bSigma^{-1}\\
   -\widehat\bSigma^{-1} \tbH^\sT(\bH +\bS)^{-1} & \widehat\bSigma^{-1}
\end{pmatrix}.
\end{align*}

So we have the bound 
\begin{align}
\label{eq:D0_inv_norm}
   \norm{\bD_0^{-1}}_\op \le \|(\bH+\bS)^{-2}\tbS\|_\op + \left( 1 + \|{\tbH}\|_\op\|{(\bH +\bS)^{-1}}\|_\op \right)^2 \|\widehat\bSigma^{-1}\|_\op.
\end{align}
So using the crude bound $\|\bB_0\|_\op \le \|\bS \bH\|_\op + \|\widehat\bSigma\|_\op + \|\bS\tbH\|_\op$, and the bound of Eq.~\eqref{eq:D0_inv_norm} into Eq.~\eqref{eq:M1_norm_bound} gives
\begin{align*}
    &\|\bN_1^{-1}\| \le \|(\bS^2 + \widehat\bSigma)^{-1}\|_\op\\
&+\left(1 + 
    \|(\bS^2 + \widehat\bSigma)^{-1}\|_\op\cdot(\|\bS \bH\|_\op + \|\widehat\bSigma\|_\op + \|\bS\tbH\|_\op)
    \right)^2\\
    &\hspace{50mm}\cdots\left(
    \|(\bH+\bS)^{-2}\tbS\|_\op + \left( 1 + \|{\tbH}\|_\op\|{(\bH +\bS)^{-1}}\|_\op \right)^2 \|\widehat\bSigma^{-1}\|_\op\right).
\end{align*}
Recalling the definition $\sfK = \sup_{\bv,\bu,w} \norm{\grad^2 \ell(\bv,\bu,w)}_{\op}$, and
$\tilde\sfK = \sup_{\bv,\bu,w} \norm{(\partial_{i}\partial_j \ell(\bv,\bu,w))_{i\in[k_0], j\in[k]}}_{\op}$ from Appendix~\ref{sec:RMT}, we can simplify the bounds as
\begin{align*}
    \left(1 + 
    \|(\bS^2 + \widehat\bSigma)^{-1}\|_\op\cdot(\|\bS \bH\|_\op + \|\widehat\bSigma\|_\op + \|\bS\tbH\|_\op)
    \right)^2
&\le 
    C_0 \left(1 + 
    \|\widehat\bSigma^{-1}\|_\op^2 \|\widehat\bSigma\|_\op^2\cdot(\|\bS\|_\op^2 (\sfK + \tilde\sfK)^2 + 1)
    \right)\\
&\le C_1(\sfK,\tilde\sfK)
    \|\widehat\bSigma^{-1}\|_\op^2 
\|\widehat\bSigma\|_\op^2
    \left( \|\bS\|_\op^2 + 1
    \right)
\end{align*}
and
\begin{align*}
    &\left(
    \|(\bH+\bS)^{-2}\tbS\|_\op + \left( 1 + \|{\tbH}\|_\op\|{(\bH +\bS)^{-1}}\|_\op \right)^2 \|\widehat\bSigma^{-1}\|_\op\right)
    \\
&\hspace{10mm}\le 2\|(\bH+\bS)^{-2}\|_\op
\left(\|\bS\|_\op^2 \|\widehat\bSigma^{-1}\|_\op + 1 + \tilde\sfK^2 \|\widehat\bSigma\|_\op^2\|\widehat\bSigma^{-1}\|_\op    +  
\sfK^2 \|\widehat\bSigma^{-1}\|_\op  \|\widehat\bSigma\|_\op^2 
+  \|\widehat\bSigma^{-1}\|_\op \|\bS\|_\op^2
\right)\\
&
\hspace{10mm}\le 4\|(\bH+\bS)^{-2}\|_\op\|\widehat\bSigma^{-1}\|_\op
\left(\|\bS\|_\op^2 +  \|\widehat\bSigma\|_\op + \tilde\sfK^2 \|\widehat\bSigma\|_\op^2    +  
\sfK^2   \|\widehat\bSigma\|_\op^2 
\right)\\
&\hspace{10mm}\le C_3(\sfK,\tilde \sfK) \|(\bH+\bS)^{-2}\|_\op\|\widehat\bSigma^{-1}\|_\op
\left( \|\bS\|_\op^2 + \|\widehat\bSigma\|_\op^2 +1
\right).
\end{align*}
Then using $\bS^2+ \widehat\bSigma \succeq \bS^2 + \sfK^{-1}\bH  \succeq 
 \sfK^{-1}\left(\bS^2 +\bH \right)$ for $\sfK \ge 1$,
 the bound on $\|\bN_1^{-1}\|_\op$
 becomes
\begin{align*}
\|\bN_1^{-1}\|_\op
&\le  \|(\bS^2 + \widehat\bSigma)^{-1}\|_\op +  
C_4(\sfK,\tilde\sfK)
\|(\bS + \bH)^{-2}\|_\op
\|\widehat \bSigma^{-1}\|_\op^3 \| \widehat\bSigma\|_\op^2 \left(\|\bS\|_\op^2 + \|\widehat\bSigma\|_\op^2 + 1\right)^2\\
&\le C_5(\sfK,\tilde\sfK)
\|(\bS + \bH)^{-2}\|_\op
\left(
\|\widehat\bSigma^{-1}\|_\op^3 +1
\right)
\left(\|\widehat\bSigma\|_\op^2+ \|\bS\|_\op^2  + 1\right)^3
\end{align*}

Using Eq.~\eqref{eq:lb_isometry} then gives the desired lower bound
\begin{align*}
\sigma_{\min}(\bJ\bzeta) &\ge \frac{
\sigma_{\min}(\bH + \bS) 
}
{
\|\widehat\bSigma\|_\op^{1/2}
C_5(\sfK,\tilde\sfK)^{1/2}
\left(
\|\widehat\bSigma^{-1}\|_\op^3 +1
\right)^{1/2}
\left(\|\widehat\bSigma\|_\op^2+ \|\bS\|_\op^2  + 1\right)^{3/2}
}\\
&\ge \frac{
\sigma_{\min}(\bH + \bS) 
}
{
C_6(\sfK,\tilde\sfK)
\left(
\|\widehat\bSigma^{-1}\|_\op^{3/2} +1
\right)
\left(\|\widehat\bSigma\|_\op^{7/2}+ \|\bS\|_\op^3  + 1\right)
}
\end{align*}
allowing us to deduce the lemma after using $\|\bS\|_\op \le n \sup_{\|\bTheta\|^2 \le \sfA_{\bR}} \|\grad^2\rho(\bTheta)\| \le C_7(\sfA_{\bR})$ by continuity in Assumption~\ref{ass:regularizer}.

\qed



%
%\begin{align}
%    &\|\bM_1^{-1}\| \le \|(\bS^2 + \widehat\bSigma)^{-1}\|_\op\\
%&+\left(1 + 
%    \|(\bS^2 + \widehat\bSigma)^{-1}\|_\op\cdot(\|\bS \bH\|_\op + \|\widehat\bSigma\|_\op + \|\bS\tbH\|_\op)
%    \right)^2
%    \left(
%    \|(\bH+\bS)^{-2}\tbS\|_\op + \left( 1 + \|{\tbH}\|_\op\|{(\bH +\bS)^{-1}}\|_\op \right)^2 \|\widehat\bSigma^{-1}\|_\op\right)\\
%    &\le \frac{C_0}{\sigma_{\min}(\bH + \bS)^2 \sigma_{\min}(\bS^2 + \widehat\bSigma)} \left( 1 + \|\bS\|_\op^2 \|\widehat\bSigma^{-1}\|_\op + \|\widehat\bSigma^{-1}\|_\op \|\bH + \bS\|_\op^2
%   + \|\tbH\|_\op^2\|\widehat\bSigma^{-1}\|_\op
%    \right)\cdot\\
%    &\quad\quad\left(\|\bS^2\|_\op +  \|\widehat\bSigma\|_\op + \|(\bS^2 +\widehat\bSigma)^{-1}\|_\op(\|\bS\|_\op^2 \sfK^2 \|\widehat\bSigma\|_\op^2 
%    +\|\bS\|_\op^2 \tilde\sfK^2 \|\widehat\bSigma\|_\op^2
%    +\|\widehat\bSigma\|_\op^2
%    )\right)\\
%    &\le 
%    \frac{C_1 
%    (1+ \|\bS\|_\op^2)}{\sigma_{\min}(\bH + \bS)^2 \sigma_{\min}(\bS^2 + \widehat\bSigma)} 
%    \left( 1 + \|\widehat\bSigma\|_\op^2 \|\widehat\bSigma^{-1}\|_\op (\sfK^2 + \tilde\sfK^2 + 1 )\right)^2.
%\end{align}
%Using Eq.~\eqref{eq:lb_isometry} then gives the desired lower bound
%\begin{align}
%\sigma_{\min}(\bD \bzeta) &\ge \frac{
%\sigma_{\min}(\bH + \bS) \sigma_{\min}(\bS^2 + \widehat\bSigma)^{1/2}
%}{(1 + \|\bS\|)_\op} \frac1{C_2\|\widehat\bSigma\|_\op^{1/2} \left(  \|\widehat\bSigma\|_\op^2 \|\widehat\bSigma^{-1}\|_\op (\sfK^2 + \tilde\sfK^2 + 1 )\right)}\\
%&\ge \sigma_{\min}(\bH +\bS)  \frac1{(1 + \|\bS\|_\op)E(\widehat\bSigma; \sfK,\tilde\sfK)}.
%\end{align}
%where
%\begin{equation}
%    E(\widehat\bSigma;\sfK,\tilde\sfK) := C_2(1 +  \|\widehat\bSigma\|_\op^2 \|\widehat\bSigma^{-1}\|_\op (\sfK^2 + \tilde\sfK^2 + 1 )).
%\end{equation}

This lower bound on the smallest singular of $\bJ\bzeta$ allows us to deduce the following relation between the determinants a priori mentioned on the conditioning event $\bzeta = \bzero$.

\begin{lemma}[Relating $\det(\de \bz)$ to $\det(\bH)$]
\label{lemma:dz_to_detH}
Let $\Err_{\sigma}(\bX)$ be the error term defined in Lemma~\ref{lemma:lb_singular_value_Df} and $r_k = k^2 + k_0k$. 
Under the assumptions of Section~\ref{sec:assumptions}, we have for any $(\bTheta,\bbV) \in\cM$, we have on the event $\{\bzeta(\bTheta,\bbV) = 0\}$,
\begin{equation}
|\det( \de\bz(\bTheta,\bbV) )| \le \ \frac{|\det\left(\bH(\bTheta,\bbV) \right)| }{\sigma_{\min}(\bH(\bTheta,\bbV))^{r_k}}  \Err_\sigma(\bX)^{r_k}.
\end{equation}
\end{lemma}
\begin{proof}
Let $r_k := k^2 + kk_0$. Let $m = nk + nk_0 + dk$ be the dimension of the full space.
We'll suppress the dependence on the indices $(\bTheta,\bbV)$ in what follows.
Let $\bB_{\bSigma},\bB_\bT \in \R^{m \times (m -r_k)}$ be the basis matrices for the column space of the covariance $\bSigma$, and tangent space of $\cM$, respectively. Recall that the co-dimensions of both of these spaces are $r_k$.
Let $\bB_{\bT^c},\bB_{\bSigma^c}$ be bases for the orthocomplements.

Recall that by Lemma~\ref{lemma:eig_vecs_NS_Sigma}, $\bzeta$ is identically equal to zero whenever $\bzeta$ is in the nullspace of $\bSigma$. Hence, $\bB_{\bSigma^c}^\sT \bJ\bzeta\bB_{\bT} = \bzero$ so that 
\begin{equation}
\det(\de\bz)  = \det\left(\bB_{\bSigma}^\sT \bJ \bzeta \bB_\bT\right)  =\frac{ \det\left( \bJ \bzeta\right)}
   {\det( \bB_{{\bSigma}^c}^\sT \bJ \bzeta \bB_{\bT^c})}.
\end{equation}
Using that
\begin{equation}
\label{eq:projected_grad_singular_val_lb}
    \sigma_{\min}(\bB_{{\bSigma}^c}^\sT \bJ \bzeta \bB_{\bT^c}) \ge 
   % \sigma_{\min}(\bJ \bzeta ) \lambda_{\min}(\bB_{\bSigma^c}^\sT\bB_{\bSigma^c})^{1/2} \lambda_{\min}(\bB_{\bT^c}^\sT \bB_{\bT^c})^{1/2} =
    \sigma_{\min}(\bJ \bzeta ),
\end{equation}
we conclude
\begin{align*}
   \left|\det\left(\bB_{\bSigma}^\sT \bJ \bzeta \bB_\bT\right) \right| \stackrel{(a)}{=}  
  \frac{\left|\det\left( \bH \right)\right|}{\prod_{i=1}^{r_k} \sigma_i(\bB_{\bSigma^c}^\sT \bJ \bzeta \bB_{\bT^c})}
  \stackrel{(b)}{\le} \frac{\left( \Err_\sigma(\bX)\right)^{r_k}}
  {\sigma_{\min}(\bH)^{r_k}} 
  \det\left( \bH \right)
\end{align*}
where $(a)$ follows from Eq.~\eqref{eq:det_projection} and the identities $|\det(\bJ\bzeta)| = |\det(\bH)|$ and $|\det(\bA)| = \prod_{i} |\lambda_i(\bA)| = \prod_i \sigma_i(\bA)$ for any square matrix $\bA,$ and $(b)$ follows from Eq.~\eqref{eq:projected_grad_singular_val_lb} and Lemma~\ref{lemma:lb_singular_value_Df}.
\end{proof}



\label{section:determinant_bound}

\subsubsection{Conditioning and concentration}
Using our random matrix theory results of Section~\ref{sec:RMT},
we bound the conditional expectation of the bound obtained from Lemma~\ref{lemma:dz_to_detH}. 
One difficulty we will face is bounding the smallest singular value of $\bX^\sT\bX$ on the event $\bzeta(\bTheta,\bbV) =\bzero. $ Notice on this event, we have $\bX^\sT \bbV + n\bRho =\bzero$ and $\bX[\bTheta,\bTheta_0] = \bbV.$ The next lemma gives bounds on the moments of the inverse singular value in terms of the extreme singular values of $\bbV,[\bTheta,\bTheta_0]$ and $\bL$.
%\am{Perhaps put an indicator of $\sigma_{\min}(\bX^\sT\bX)\ge c$ in the very beginning?}


Let us introduce the following quantities for this section
\begin{equation}
   \sfA_{\bL}  :=  1\vee
   \sup_{\substack{\|\bV\|_\op \le \sqrt{n}\sfA_\bV \\ 
  \|\bw\|_2 \le \sfA_{\bw}\sqrt{n}
   }} \frac1{\sqrt{n}}\|\bL(\bV,\bw)\|_\op,\quad
   \sfA_{\bRho} := 1 \vee
   \sup_{\|\bTheta\|_\op \le \sfA_\bR} \|\bRho(\bTheta)\|_\op.
\end{equation}
Note that by Assumptions~\ref{ass:loss} and~\ref{ass:regularizer}, we have $\sfA_\bL,\sfA_\bRho$ are bounded by some positive constant $C(\sfA_\bV,\sfA_\bw),C(\sfA_\bR)$, depending only on
$(\sfA_\bV,\sfA_\bw)$, $\sfA_\bR$ (and $\sOmega$), respectively.


\begin{lemma}[Lower bounding $\sigma_{\min}(\bX^\sT\bX)$ at the zeros of $\bzeta$] 
\label{lemma:lsv_sigma_conditional}
Under the assumption of Section~\ref{sec:assumptions},
for any $p>1$ and $\|\bw\|_2 \le \sfA_{\bw}\sqrt{n}$, if $n> d+k$,
\begin{equation}
\E\left[\sigma_{\min}(\bX^\sT\bX)^{-p} | \bzeta = \bzero, \bw\right] \le 
\frac{C^p}{d^{p}} \left( 
\frac{\sfA_{\bR}^{5} \sfA_{\bbV}^{4}}{\sfsigma_{\bbV}^{4} \sfsigma_{\bR}^{2}} \frac{\sfA_{\bL}^{4} }{\sfsigma_{\bL}^{4}}\left(\sfA_{\bRho}^{2} + 1\right)
\right)^p \left( \alpha_n - \frac{k}{d} - 1\right)^{-p} 
\end{equation}
for some universal constant $C>0$.
\end{lemma}
\begin{proof}
Note that on $\{\bzeta = \bzero\}$, we have $\bX[\bTheta,\bTheta_0] = \bbV$ and $\bL^\sT \bX = - \bRho^\sT$. Letting $\bP_{\bTheta}, \bP_\bL$ be the projections on the columns spaces of $[\bTheta,\bTheta_0],\bL$ respectively, we can write 
\begin{equation}
    \bX = \bP_\bL^\perp \bX \bP_\bTheta^\perp - \bL(\bL^\sT\bL)^{-1} \bRho^\sT \bP_{\bTheta}^\perp  + \bbV\bR^{-1} \bTheta^\sT.
\end{equation}
Letting $\bB_\bTheta\in\R^{d \times (k+k_0)}$ be a basis matrix for the columnspace of $(\bTheta,\bTheta_0)$ and $\bB_\bTheta^\perp \in\R^{d \times(d - k-k_0)}$ be a basis matrix for its complement, we define
\begin{equation}
    \tilde\bX^\sT\tilde\bX = \begin{pmatrix}
    \bG^\sT\bP_{\bL}^\perp\bG -  \bA^\sT\bA &  (\bG -\bA)^\sT \bC\\
    \bC^\sT(\bG - \bA) & \bC^\sT\bC
    \end{pmatrix} \in \R^{d\times d}
\end{equation}
where $\bG\in\R^{n\times(d - k - k_0)}$ is a matrix of i.i.d. Guassian entries and 
    $\bA := \bL(\bL^\sT\bL)^{-1} \bRho^\sT \bB_{\bTheta}^\perp,  \bC:= \bbV\bR^{-1} (\bTheta,\bTheta_0)^{\sT} \bB_{\bTheta}$.
Then by Gaussian conditioning, we immediately see that $\E[\sigma_{\min}(\bX^\sT\bX)^{-p} | \bzeta = 0] = \E[\bsigma_{\min}(\tilde\bX^\sT\tilde\bX)^{-p}].$
Now we can bound $\sigma_{\min}(\tilde\bX^\sT\tilde\bX)^{-p}$ using the block inversion formula. Namely,
\begin{align}
\nonumber
   &\E\left[\sigma_{\min}(\tilde\bX^\sT\tilde\bX)^{-p}\right] \\
   &\le \E\Big[ \Big(\|{(\bC^\sT\bC)^{-1}}\|_\op + 
   \left(1 + \|{(\bC^\sT\bC)^{-1}}\|_\op (\|\bG\bP_L^\perp\|_\op +\|\bA\|_\op)\|\bC\|_\op\right)^2
   \|((\bG - \bA)^\sT\bP_{\bL}^\perp\bP_{\bC}^\perp\bP_\bL^\perp (\bG - \bA))^{-1}\|_\op\Big)^p\Big]\\
   &\le 2^p \Big( 
   \underbrace{\E\Big[\left(1 + \|{(\bC^\sT\bC)^{-1}}\|_\op (\|\bG\bP_L^\perp\|_\op +\|\bA\|_\op)\|\bC\|_\op\right)^{4p}\Big]^{1/2}}_{\mathrm{(I)}}
   \underbrace{
   \E\Big[\|((\bG - \bA)^\sT\bP_{\bL}^\perp\bP_{\bC}^\perp\bP_\bL^\perp (\bG - \bA))^{-1}\|_\op^{2p}\Big]^{1/2}}_{\mathrm{(II)}}\nonumber \\
   &\hspace{20mm}+ \|{(\bC^\sT\bC)^{-1}}\|_\op^p \Big). \label{eq:lb_sigma_min_XX_decomp}
\end{align}
Let's bound each of the terms $\mathrm{(I)}$ and $\mathrm{(II)}$ in what follows.

\noindent\textbf{The term $\mathrm{(I)}$:}
By recognizing $\|\bG\bP_{\bL}^\perp\|_\op$ as the operator norm of a standard Gaussian matrix in $\R^{(n-k)\times(d- k-k_0)}$
and using the bounds
\begin{equation}
\|(\bC^\sT\bC)^{-1}\|_\op \le  \frac{\|(\bTheta,\bTheta_0)\|^4_\op}{\lambda_{\min}(\bbV^\sT\bbV)},
\quad \|\bC\|_\op \le \frac{\|\bbV\|_\op \|(\bTheta,\bTheta_0)\|_\op}{\lambda_{\min}(\bR)},\quad
\|\bA\|_\op \le  \frac{\|\bL\|_\op \|\bRho\|_\op}{\lambda_{\min}(\bL^\sT\bL)},
\end{equation}
we can bound
\begin{align*}
\mathrm{(I)}^2 &\le 
C_1^{p}
\left( 1 +
 \|{(\bC^\sT\bC)^{-1}}\|_\op^{4p}\|\bC\|_\op^{4p}
\left(
\|\bA\|_\op^{4p}
+  n^{2p}
\right)\right)\\
&\le C_1^p \left(1 +  \frac{\norm{\bR}_\op^{10p} \|\bbV\|_\op^{4p} }{\sigma_{\min}(\bbV)^{8p} \sigma_{\min}(\bR)^{4p} } \left( \frac{\|\bL\|_\op^{4p} \|\bRho\|_\op^{4p}}{\sigma_{\min}(\bL)^{8p}} + n^{2p} \right)\right)\\
&\stackrel{(a)}{\le} C_1^p \left( 1 + \frac{\sfA_{\bR}^{10p} \sfA_{\bbV}^{4p}}{\sfsigma_{\bbV}^{8p} \sfsigma_{\bR}^{4p}}\left( \frac{\sfA_{\bL}^{4p} \sfA_{\bRho}^{4p}}{\sfsigma_{\bL}^{8p}} + 1\right) \right)\\
&\le C_2^p  \frac{\sfA_{\bR}^{10p} \sfA_{\bbV}^{8p}}{\sfsigma_{\bbV}^{8p} \sfsigma_{\bR}^{4p}} \frac{\sfA_{\bL}^{8p} }{\sfsigma_{\bL}^{8p}}\left(\sfA_{\bRho}^{4p} + 1\right).
\end{align*}


\noindent\textbf{The term $\mathrm{(II)}$:}
For the second term, we similarly observe that
\begin{equation}
\E\left[\|((\bG - \bA)^\sT\bP_{\bL}^\perp\bP_{\bC}^\perp\bP_\bL^\perp (\bG - \bA))^{-1}\|_\op^{2p}\right] \le \E\left[\sigma_{\min}(\tilde\bG)^{-4p}\right]
\end{equation}
where $\tilde\bG \in\R^{(n - 2k-k_0) \times (d - k - k_0)}$ is a Gaussian matrix with i.i.d. uncentered entries. 
A standard result then gives
\begin{equation}
    \mathrm{(II)}^{2} \le  C_3^{p}\left( \sqrt{n - 2k - k_0} - \sqrt{d - k-k_0}\right)^{-4p} \le  C_3 \left(\frac{\sqrt{n - k} + \sqrt{d}}{ n - d- k}\right)^{4p} = 
    \frac{C_3^{p}}{d^{2p}} \left( \left(\alpha_n - \frac{k}{d}\right)^{1/2} -1 \right)^{-4p}
\end{equation}
for some universal $C_3>0$, whenever $n > d + k$.
%A standard lower bound on the smallest singular value of $\tilde \bG$ gives
%\begin{equation}
%    \P\left( \sigma_{\min}(\tilde \bG) \le \sqrt{n - 2k - k_0} - \sqrt{d - k-k_0} -t \right) \le e^{-t^2/2}
%\end{equation}
%for any $t>0$.

\noindent\textbf{Combining the bounds:}
Combining the above bounds on $\mathrm{(I)}$ and $\mathrm{(II)}$ with Eq.~\eqref{eq:lb_sigma_min_XX_decomp} we obtain
\begin{align*}
\E\left[\sigma_{\min}(\tilde\bX^\sT\tilde\bX)^{-p}\right]
&\le C_4^p \left( \frac1{d^p}\left( 
\frac{\sfS_{\bR}^{5} \sfS_{\bbV}^{4}}{\sfs_{\bbV}^{4} \sfs_{\bR}^{2}} \frac{\sfS_{\bL}^{4} }{\sfs_{\bL}^{4}}\left(\sfS_{\bRho}^{2} + 1\right)
\right)^p \left( \alpha_n - \frac{k}{d} -1 \right)^{-p}  
+  \frac{1}{n^p}  \left(\frac{\sfS_{\bR}^2}{\sfs_{\bbV}^2}\right)^{p}\right)\\
&\le
\frac{C_4^p}{d^{p}} \left( 
\frac{\sfS_{\bR}^{5} \sfS_{\bbV}^{4}}{\sfs_{\bbV}^{4} \sfs_{\bR}^{2}} \frac{\sfS_{\bL}^{4} }{\sfs_{\bL}^{4}}\left(\sfS_{\bRho}^{2} + 1\right)
\right)^p \left( \alpha_n - \frac{k}{d} - 1\right)^{-p} 
\end{align*}
as desired.


%\begin{align}
%   \E\left[\sigma_{\min}(\tilde\bX^\sT\tilde\bX)^{-p}\right] 
%   &\le 
%   C_3^p \left(
%   \frac{\|(\bTheta,\bTheta_0)\|^5_\op \|\bbV\|_\op^2}{\lambda_{\min}(\bbV^\sT\bbV) \lambda_{\min}(\bR)}
%   \right)^{2p}
%   \left(\left(
%    \frac{\|\bL\|_\op \|\bRho\|_\op}{\lambda_{\min}(\bL^\sT\bL)  }
%   \right)^{2p}+
%    n^{p}
%\right)
%\frac{1}{d^p} \left( \left(\alpha - \frac{k}{d}\right)^{1/2} -1 \right)^{-2p}\\
%%\left(\frac{\sqrt{\alpha}+ 1}{(\alpha - 1)\sqrt{d} - k/\sqrt{d}} + 1\right)^{p}\\
%&\le
%   C_3^p \left(
%   \frac{\|\bR\|^3_\op \|\bbV\|_\op^2 \|\bL\|_\op^2}{\lambda_{\min}(\bL^\sT\bL)\lambda_{\min}(\bbV^\sT\bbV) \lambda_{\min}(\bR)}
%   \right)^{2p}
%   \frac{\left(
%    \|\bRho\|_\op^{2p}+
%    n^{p}
%\right)}{d^p}
%\left( \left(\alpha - \frac{k}{d}\right)^{1/2} -1 \right)^{-2p}
%%\left(\frac{\sqrt{\alpha}+ 1}{(\alpha - 1)\sqrt{d} - k/\sqrt{d}} + 1\right)^{p}\\
%\end{align}
\end{proof}

Before proceeding to the analysis of the conditional expectation, let us give the following lemma regarding concentration of Lipschitz functions of $\bH_0$. The proof of this is standard.


\begin{lemma}[Concentration of Lipschitz functions of the Hessian]
\label{lemma:concentration_lipschitz_func}
Assume $f :\R \to\R$ is Lipschitz.  
Recall $\bH_0 = \left(\bX\otimes\bI_k\right)^\sT \bSec \left(\bX \otimes \bI_k\right)$, where 
$\bK$ was defined in Eq.~\eqref{eq:SecDef}, and let $\bS\in\R^{dk\times dk}$ be any deterministic symmetric matrix.
Then there exist absolute constants $c,C > 0$ such that, 
for any 
\begin{equation}
    t \ge \frac{\sfK\norm{f}_{\Lip}}{\alpha_n^{1/2}} \frac{k}{n^{1/2}},
\end{equation}
we have
\begin{equation}
    \P_\bX\left(\left|\frac1n\Tr(f((\bH_0 + \bS)/n)) - \frac1n\E_\bX\left[ \Tr(f((\bH_0 + \bS)/n))\right]\right| \ge t\right) 
\le
     C \exp \left\{ 
    -c\frac{ t n^{3/2} \alpha^{1/2} }{  k \sfK \norm{f}_{\Lip}}
    \right\}
%C_1 \exp \left\{ 
%    -c_1\frac{ t^2 n^2 \alpha_n }{\norm{f}_{\Lip}^2(\gamma^{1/2} + 1)^2 k^2 \sfK^2}
%    \right\}
%    +
%    C_2\exp\left\{  -c_2 n\gamma\right\}.
\end{equation}
The same bound holds for any matrix $\bSec= (\bSec_{ij})_{i,j\le k}$ with $\bSec_{ij}\in\reals^{n\times n}$
a diagonal matrix with diagonal entries bounded (in absolute value) by $\sfK$.
\end{lemma}

\begin{proof}
First, we bound the variation of the function
\begin{equation}
\label{eq:lip_func_of_X}
   g(\bX) := \Tr f\Big((\bI \otimes \bX)^\sT \bSec (\bI \otimes \bX)/n + \bS/n\Big).
\end{equation}
Let $\bM = (\bI \otimes \bX)^\sT \bSec (\bI \otimes \bX)/n + \bS/n$ and
$\bM' = (\bI \otimes \bX')^\sT \bSec (\bI \otimes \bX')/n + \bS/n$.
Use $\{\lambda_i\}_{i\in[dk]}$ and $\{ \lambda_i'\}_{i \in[dk]}$ to denote their eigenvalues respectively. By Hoffman-Wielandt, we have
    \begin{align*}
        \left| \Tr f(\bM) - \Tr f(\bM') \right|
        &= \left| \sum_{i=1}^{dk} f(\lambda_i) - \sum_{i=1}^{dk}  f(\lambda_i')\right|
         \le \norm{f}_{\Lip} \min_{\sigma } \sum_{i=1}^{dk} \left|\lambda_i - \lambda_{\sigma(i)}' \right| 
         \le \norm{f}_{\Lip} (dk)^{1/2} \norm{\bM - \bM'}_F.
    \end{align*}
We can bound the norm of the difference 
\begin{align*}
    \norm{\bM - \bM'}_F &
    \le\frac1n\norm{(\bI \otimes \bX) - (\bI \otimes \bX')}_F \norm{\bK}_\op \left(\norm{\bI \otimes \bX}_\op + \norm{\bI \otimes \bX'}_\op \right)\\
    &\le \frac{k^{1/2}}{n} \sfK \norm{\bX - \bX'}_F (\norm{\bX}_\op + \norm{\bX'}_\op).
\end{align*}
Now for any $\gamma > 4$, define
\begin{equation}
\cA_\gamma :=     \left\{ \norm{\bX}_\op \le n^{1/2}\gamma^{1/2} \right\}.
\end{equation}
The above bound on the variation of $g$ implies that on $\cA_\gamma$, we have
\begin{equation}
    |g(\bX) - g(\bX') | \le \frac{C_0\,\sfK\, \norm{f}_{\Lip}}{\alpha_n^{1/2}} k\gamma^{1/2}    \norm{\bX - \bX'}_F\, .
\end{equation}
%the function defined in
% Eq.~\eqref{eq:lip_func_of_X} has Lipschitz constant bounded by
%\begin{equation}
%    \norm{g}_{\Lip} = \frac{2 \gamma(n) k \sfK \norm{f}_{\Lip} } { \alpha^{1/2}}
%\end{equation}
We can apply Gaussian concentration on this event. For $t>0$, we have for some universal constant $C_2,c_2,C_1,c_1 >0$,
\begin{align*}
\P\left(\left| \frac1n \Tr f(\bH_0) - \frac1n \E\left[\Tr f(\bH_0)\right] \right| \ge t \right)&\le 
    \P\left(\left\{\left| \frac1n \Tr f(\bH_0) - \frac1n \E\left[\Tr f(\bH_0)\right] \right| \ge t \right\} \cap \cA_\gamma\right)  
    +
    \P\left( \cA_\gamma^c\right) \\
    &\le  C_1 \exp \left\{ 
    -c_1\frac{ t^2 n^2 \alpha }{ \gamma  k^2 \sfK^2 \norm{f}_{\Lip}^2}
    \right\}
    +
     C_2\exp\left\{  -c_2 n\gamma\right\}.
\end{align*}
Choosing $\gamma$ to satisfy
\begin{equation}
\gamma = 4\frac{t n^{1/2}\alpha_n^{1/2}}
{k \sfK \norm{f}_{\Lip}},
\end{equation}
for appropriate universal $c_3>0$ gives the desired bound, as long as 
\begin{equation}
    t \ge \frac{\sfK\norm{f}_{\Lip}}{\alpha_n^{1/2}} \frac{k}{n^{1/2}}.
\end{equation}
\end{proof}

%\begin{lemma}[Interlacing theorem]
%Let $\bH,\bH_k \in \R^{m \times m}$.
%    Suppose $\rank(\bH - \bH_k) \le r_k.$ Then for any monotonically non-decreasing function $f$, we have
%    \begin{equation}
%    \sum_{i = 1}^m f(\lambda_i(\bH_k))  \le  \sum_{i=r_k}^{m } f(\lambda_i(\bH)) +  r_k f( \lambda_{\max}(\bH_k)).
%    \end{equation}
%\end{lemma}
%\begin{proof}
%This is a direct consequence of Weyl's inequality and interlacing of the eigenvalues of $\bH$ and $\bH_k$. Namely, we have for $i \in \{1, \dots , m- r_k\}$
%   $\lambda_i(\bH_k) \le \lambda_{i + r_k}(\bH) $
%and for $i\in\{m - r_k  +1, m\}$,
%  $\lambda_i(\bH_k) \le \lambda_{\max}(\bH_k)$. \am{Here eigenvalues are increasing: Please double check
%  that this matches the notations section}
%\end{proof}


\subsubsection{Proof of Lemma~\ref{lemma:CE_bound}}
\label{sec:pf_lemma_CE_bound}
%\begin{lemma}[Bounding the conditional expectation of the determinant]
% \label{lemma:CE_bound}
%Fix $\tau_0,\tau_1 \in (0,1)$, and $\bw$ satisfying $\|\bw\|_2 \le \sfA_{\bw}\sqrt{n}$.
%Then under the assumptions of Section~\ref{sec:assumptions}, there exist constants $C,c>0$ depending only on $\sOmega$ and $C_0(\tau_0)$ depending only on $\sOmega$ and $\tau_0$ such that for all $n > C_0(\tau_0)$,
%\begin{enumerate}
%\item for any $(\bbV,\bTheta) \in\cM(\cuA,\cuB)$ satisfying $\mu_{\star}(\hnu,\hmu)((-\infty, \tau_0)) < \tau_0,$ we have
%\begin{align}
%&\E[|\det \big(\de \bz(\bt)\big)|\one_{\bH/n + \grad^2\rho \succ \sfsigma_\bH} \big|\bzeta(\bt) = 0 ,\bw ]
%%&
%%\quad{\le}
%%\frac{(1 + \sfS_{\grad^2\rho})^{r_k}\; \sfC_0^{3r_k}\; n^{dk+2r_k}}{(n\sfs_\bH)^{r_k}}
%%\Bigg(\exp\left\{
%%\E\left[\Tr\log^{(\eps)}\left(\frac{\bH+\bS}n\right) \right] + n C(\sfD,k)\eps_n
%%\right\}
%%\left(\frac{\sfC_1}{\eps}\right)^{12r_k}
%%+ \exp\left\{ -n^{3/2} \eps_n\right\}
%%\Bigg)\\
%%&
%\le
%n^{dk}
%\Bigg(\exp\left\{
%\E\left[\Tr\log^{(\tau_1)}\left(\frac{\bH}n +\grad^2\rho\right)\Big| \bw \right] +  \frac{C n^{1-1/4}}{\tau-1}
%\right\}
%+ \exp\left\{ -n^{5/4}\right\}
%\Bigg)
%\end{align}
%
%\item For $(\bbV,\bTheta)\in\cM$ satisfying 
%$\mu_{\star}(\hnu,\hmu)((-\infty, \tau_0)) \ge \tau_0,$
%\begin{align}
%\E[|\det \big(\de \bz(\bt)\big)|\one_{\bH/n + \grad^2\rho \succ \sfsigma_\bH} \big|\bzeta(\bt) = 0,\bw ]
%& \le C \exp \left\{ 
%    -c 
%    \tau_0^2
%    n^{3/2}
%    \right\}.
%\end{align}
%\end{enumerate}
%
%
%%\notate{put it in the old form.
%%\begin{equation}
%%    \omega_{\DC,1}(n,k,\eps) :=  ,
%%    \omega_{\DC,2}(n,k,\eps)
%%\end{equation}
%%\label{lemma:concentration_det}
%%Let $\bDelta_k$ be as in Lemma~\ref{lemma:conditioning}.
%%For any $\eps\in(0,1)$,  we have
%%\begin{align}
%%    \E_\bX\left[|\det\left(\bH + \bE_k\right)| \norm{\bX}_\op^3
%%    \one_{\left(\bH +\bE_k\right)/n\succ \eps_\bH}\right]
%%    \leq
%%    n^{dk}\left(\exp\left\{ \E[\Tr(\log^\up{\eps}(\bH/n))
%%    + n \omega_{\DC,1}(n,k,\eps)\right\} + \omega_{\DC,2}(n,k)\right)\, ,
%%\end{align}
%%where we recall that  $\log^\up{\eps}(t) := \log( \eps \vee t)$,
%% and
%%\begin{align}
%%\omega_{\DC,1}(n,k,\eps)&:= 
%%C(\sfD)\left(
%%     \frac{\log(n)k^2  }{n^{1/2}\eps}
%%     + \frac{r_k}{n}\log\left(\frac{1}{\eps}\right)\right),\\
%%     \omega_{\DC,2}(n, k) &:= C(\sfD) \exp\left\{-c(\sfD) n \log(n) k\right\}.
%%%
%%\end{align}
%%}
%
%\end{lemma}
Throughout the proof, we will use the notation
$\bS = \bS(\bTheta) := n\grad^2\rho(\bTheta).$ 
%We'll also write $E(\;\cdot\;) := E(\;\cdot\;; \sfK,\tilde\sfK)$ for the error term introduced in Lemma~\ref{lemma:lb_singular_value_Df}.

\noindent\textbf{Step 1: Conditioning as a perturbation.}
Let us first recall that the event $\{\bzeta = \bzero\}$, is equivalent
to $\bL^\sT\bX = - n\bRho^\sT$ and $\bX(\bTheta,\bTheta_0) = \bbV$. 
So letting $\bP_{\bTheta}, \bP_\bL$ be the projections onto the columns spaces of $(\bTheta,\bTheta_0),\bL$ respectively, we have on $\{\bzeta = \bzero\}$
\begin{equation}
    \bX = \bP_\bL^\perp \bX \bP_\bTheta^\perp - n \bL(\bL^\sT\bL)^{-1} \bRho^\sT \bP_{\bTheta}^\perp  + \bbV\bR^{-1} \bTheta^\sT.
\end{equation}
Hence for any function $g$,
\begin{equation}
    \E[g(\bX) | \bzeta = \bzero] = \E[g(\bX + \bDelta_{0,k})]
\end{equation}
for some matrix $\bDelta_{0,k} = \bDelta_{0,k}(\bTheta,\bbV)$ 
satisfying
%\begin{align}
%    &\rank(\bDelta_{0,k}) \le r_k,\\
%    &\norm{\bDelta_{0,k}}_\op \le \norm{\bX}_\op.
%\end{align}
\begin{align}
    &\rank(\bDelta_{0,k}) \le 4(k+k_0),\\
    &\norm{\bDelta_{0,k}}_\op \le C_0\sqrt{n}\max\left(\frac{\norm{\bX}_\op}{\sqrt{n}} ,
    \frac{\sfA_{\bbV}}{\sfsigma_{\bR}^{1/2}} + \frac{\sfA_{\bRho}}{ \sfsigma_{\bL}^{1/2}}\right)
\end{align}
for some $C_0 >0.$
%Let $\bP_1, \bP_2$ be the orthogonal projectors onto the column space of $\bL$, $\bTheta,\bTheta_0$ respectively. Then on the event $\bzeta =0$, 
%$\bP_1 \bX =0$ and $\bX\bP_2 = (\bV,\bU) \bR^{-1}(\bTheta,\bTheta_0)$ so on this event
%\begin{equation}
%    \bX = (\bP_1  + \bP_1^\perp)\bX (\bP_2 + \bP_2^\perp) =  
%    \bP_1^\perp (\bV,\bU) \bR^{-1}(\bTheta,\bTheta_0) + \bP_1^\perp \bX \bP_2^\perp.
%\end{equation}
Consequently, letting
\begin{equation}
\label{eq:Delta_1k_def}
   \Delta_{1,k}  :=  (\bI \otimes \bDelta_{0,k})^\sT \bSec (\bI\otimes \bX ) + (\bI \otimes \bX)^\sT \bSec (\bI \otimes \bDelta_{0,k}) + (\bI \otimes \bDelta_{0,k})^\sT \bSec (\bI\otimes \bDelta_{0,k}),
\end{equation}
%and
%\begin{equation}
%   \Delta_{2,k}  :=   \bDelta_{0,k}^\sT \bX  + \bX^\sT\bDelta_{0,k} + \bDelta_{0,k}^\sT \bDelta_{0,k},
%\end{equation}
we have
\begin{align}
&\E[|\det \big(\de \bz(\bt)\big)|\one_{\bH \succ n\eps_\bH} \big|\bzeta(\bt) = 0 ]\\
&\quad\stackrel{(a)}{\le}
\E\left[  \frac{|\det\left(\bH\right)| }{\sigma_{\min}(\bH)^{r_k}}  \Err_\sigma(\bX)^{r_k}
\one_{\{\bH\succ n\eps_\bH \}}
\bigg| \bzeta = \bzero\right]\\
&
\quad\stackrel{(b)}{\le}
\frac{1}{(n\eps_\bH)^{r_k}}
\E\left[  \big|\det\left(\bH + \bDelta_{1,k}\right)\big|   \Err_\sigma(\bX + \bDelta_{0,k})^{r_k} \one_{\{\bH + \bDelta_{1,k} \succ n\eps_\bH\}}\right]
\label{eq:concentration_decomp_0}
\end{align}
where 
$(a)$ follows by Lemma~\ref{lemma:dz_to_detH} and  $(b)$ follows by Eq.~\eqref{eq:conditioning_generic}.
Observe that $\rank(\bI\otimes \bDelta_{0,k}) \le 4r_k.$ 
Letting $r_k' := 12 r_k$, we have from the definition in~Eq.~\eqref{eq:Delta_1k_def}, 
that
\begin{align}
\rank(\bDelta_{1,k}) &\le r'_k\\
\|\bDelta_{1,k}\|_\op &\le  3\sfK \|\bX\|_\op (1 + \|\Delta_{0,k}\|_\op)^2 \le 
3\sfK \|\bX\|_\op \left(1 +
\norm{\bX}_\op+
    \frac{\sigma_{\max}(\bbV)}{\lambda_{\min}(\bR)^{1/2}} + \frac{\sigma_{\max}(\bRho)}{ \lambda_{\min}(\bL^\sT\bL)^{1/2}}
\right)^2.
\end{align}
Then by Cauchy's interlacing theorem, we have for any $i\ge dk-r'_k$,
\begin{equation}
\label{eq:interlacing_2}
    \lambda_{i+r'_k}(\bH+  \bDelta_{1,k})\leq \lambda_{i}(\bH ),\quad \lambda_i(\bH + \bDelta_{1,k})\leq \|\bH +  \bDelta_{1,k}\|_\op.
\end{equation}

\noindent \textbf{Step 2: Needed bounds on some moments.}
Define the quantities
\begin{equation}
   \Delta_3 := \E\left[\Err(\bX + \bDelta_{0,k})^{2r_k}\right]^{1/2},\quad
   \Delta_4(p) := 
 \E\left[ 
\left( \frac{\|\bH +  \bDelta_{1,k}\|_\op}{n}\right)^{p}\right] \quad\textrm{for}\quad p>1.
\end{equation}
These will reappear in several places in the proof, we let us preempt this by giving a bound on these quantities.
%
First we bound $\Delta_3$. Recalling the definition of the error term $\Err_\sigma(\bX)$ introduced in Lemma~\ref{lemma:lb_singular_value_Df}, we write
\begin{align}
       \Delta_3^2 &=  \E\left[E_\sigma(\bX)^{2r_k}|\bzeta = \bzero\right] 
= C_0(\sfK,\tilde\sfK, \sfA_\bR)^{2r_k} \E\left[ (\|\bX^\sT\bX\|_\op^{7r_k} + n^{7r_k}) \left( \sigma_{\min}(\bX^\sT\bX)^{-3r_k} + 1 \right)\right]
\\
    &\le C_1(\sfK,\tilde\sfK, \sfA_\bR)^{r_k} 
    \underbrace{\E\left[(\|\bX\|_\op^{28r_k} + \|\bDelta_{0,k}\|_\op^{28r_k} + n^{14 r_k} ) \right]^{1/2}}_{\mathrm{(I)}}
    \underbrace{
   \E\left[ (\sigma_{\min}(\bX^\sT\bX)^{-6r_k} + 1)| \bzeta = \bzero\right]^{1/2}}_{\mathrm{(II)}}.
\end{align}
The term $\mathrm{(I)}$ can be bounded directly as
\begin{align}
  \mathrm{(I)}
  &\le C_3^{r_k} 
   \left(n^{7 r_k} +   \left( \frac{\|\bbV\|_\op}{\sigma_{\min}(\bR)^{1/2}} + \frac{n\|\bRho\|_\op}{\sigma_{\min}(\bL)} \right)^{14r_k}
  \right)
  \le C_4^{r_k}n^{7r_k} \left(1 +  \left(\frac{\sfA_{\bbV}^2}{\sfsigma_{\bR}} + \frac{\sfA_{\bRho}^2}{\sfsigma_{\bL}^2}\right)\right)^{7r_k}.
\end{align}
For the term $\mathrm{(II)}$, we use~Lemma~\ref{lemma:lsv_sigma_conditional} to bound the smallest singular value of $\bX^\sT\bX$ on $\{\bzeta = 0\}$:
\begin{align}
    \mathrm{(II)} \le 
\frac{C_5^{r_k}}{d^{3r_k}} \left( 
\frac{\sfA_{\bR}^{5} \sfA_{\bbV}^{4}}{\sfsigma_{\bbV}^{4} \sfsigma_{\bR}^{2}} \frac{\sfA_{\bL}^{4} }{\sfsigma_{\bL}^{4}}\left(\sfA_{\bRho}^{2} + 1\right)
\right)^{3r_k} \left( \alpha - \frac{k}{d} - 1\right)^{-3r_k} + 1.
\end{align}
So we obtain
\begin{equation}
    \Delta_3 \le  \sfC_0^{3r_k}n^{4r_k} 
%
%    \left(1 +  \left(\frac{\sfA_{\bbV}^2}{\sfsigma_{\bR}} + \frac{\sfA_{\bRho}^2}{\sfsigma_{\bL}^2}\right)\right)^{5r_k}
%\left( 
%\frac{\sfA_{\bR}^{5} \sfA_{\bbV}^{4}}{\sfsigma_{\bbV}^{4} \sfsigma_{\bR}^{2}} \frac{\sfA_{\bL}^{4} }{\sfsigma_{\bL}^{4}}\left(\sfA_{\bRho}^{2} + 1\right)
%\right)^{3r_k} \left( \alpha - \frac{k}{d} - 1\right)^{-3r_k}
\end{equation}
where we defined
\begin{equation}
   \sfC_0 := 
    C_6(\sfK,\tilde\sfK,\sfA_\bR)\, \alpha
    \left(1 +  \left(\frac{\sfA_{\bbV}^2}{\sfsigma_{\bR}} + \frac{\sfA_{\bRho}^2}{\sfsigma_{\bL}^2}\right)\right)^{2}
\left( 
\frac{\sfA_{\bR}^{5} \sfA_{\bbV}^{4}}{\sfsigma_{\bbV}^{4} \sfsigma_{\bR}^{2}} \frac{\sfA_{\bL}^{4} }{\sfsigma_{\bL}^{4}}\left(\sfA_{\bRho}^{2} + 1\right)
\right) \left(\left( \alpha - \frac{k}{d} - 1\right)^{-1} + 1 \right)
\end{equation}

To bound $\Delta_4(p)$, we have for any $p>1$,
\begin{align}
 %\E\left[ 
%\left( \frac{\|\bH + \bS+ \bDelta_{1,k}\|_\op}{n}\right)^{p}\right] 
\Delta_4(p)
%&\le
%\frac{2^p}{n^p}
%\left( \E\left[ 
%\|\bH\|^p_\op | \bzeta = 0\right] + \|\bS \|_\op^p\right)\\
&\le 
\frac{C_7^p}{n^p}
\left( \E\left[ 
\sfK^p(\|\bX\|^{2p}_\op + \|\bDelta_{0,k}\|_\op^{2p})\right] + \|\bS \|_\op^p\right)\\
&\le\frac{C_8^p}{n^p}
\left( 
\sfK^p n^p + 
\sfK^p\left(\frac{\|\bbV\|_\op}{\sigma_{\min}(\bR)^{1/2}} + n\frac{\|\bRho\|_\op}{ \sigma_{\min}(\bL)}\right)^{2p}
  + \|\bS \|_\op^p\right)\\
&\le \sfC_1^p
%  &\le C_2^p \sfK^p
%\left( 
%1 + 
%\frac{\sfA_{\bbV}^2}{\sfsigma_{\bR}} + \frac{\sfA_{\bRho}^2}{ \sfsigma_{\bL}^2}
%  + \sfA_{\grad^2\rho}\right)^p.\\
\end{align}
where 
\begin{equation}
    \sfC_1 := C_{9} \sfK 
\left( 
1 + 
\frac{\sfA_{\bbV}^2}{\sfsigma_{\bR}} + \frac{\sfA_{\bRho}^2}{ \sfsigma_{\bL}^2}
  + \sfA_{\grad^2\rho}\right).
\end{equation}

\noindent \textbf{Step 3: Proof of item \textit{1.}}
We will use the constraint on the minimum singular value of the Hessian to constrain the  asymptotic spectral measure $\mu_{\star}(\hnu_{\bbV}, \hmu_{\sqrt{d}\bTheta})$. 
Namely, fixing $\tau_0>0$,
we will show that for $\bbV$ satisfying
\begin{equation}
\label{eq:constraint_on_V_asymp}
\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0)) \ge \tau_0,
\end{equation}
 the value of the expectation in~\eqref{eq:concentration_decomp_0} is small.
%More precisely, We will that, by Lipschitz concentration (Lemma~\ref{lemma:concentration_lipschitz_func}), the expectation in~\eqref{eq:concentration_decomp_0} is small --in an appropriate sense-- for any $\bbV$ such that 
%$\inf \supp(\mu_\star(\widehat\nu_{\bV,\bU,\bw})) < s_0$.
To this ends, define the event
\begin{equation}
    \Omega_{3}:=\left\{
    \big|\left\{ \lambda \in \spec\left(\bH/n \right) : \lambda \leq 0\right\} \big| < r'_k
    \right\},
\end{equation}
Then by Eq.~\eqref{eq:interlacing_2},  $\{\lambda_{\min}(\bH + \bDelta_{1,k})/n> 0\}\subseteq \Omega_{3}$. 
We'll bound the probability of $\Omega_3$ for $\bbV$ satisfying~\eqref{eq:constraint_on_V_asymp}. 
Define the Lipschitz test function $f_{\tau_0}:\R\to\R$ as
\begin{equation}f_{\tau_0}(\lambda) = 
    \begin{cases}
        1 & \text{ if } \lambda\leq -\tau_0,\\
        1- \frac{1}{\tau_0}(\lambda +\tau_0)& \text{ if }-\tau_0<\lambda\leq 0\\
        0& \text{ if } 0<\lambda.
    \end{cases}
\end{equation}
This function has Lipschitz modulus bounded by $\tau_0^{-1}$. Furthermore,  
if
$\bbV$ satisfies Eq.~\eqref{eq:constraint_on_V_asymp}, then
$\tau_0 <
    \E_{\mu_\star}\left[ f_{\tau_0}(\Lambda) \right].$
    %\E_{\mu_\star}\left[ f_{1}(\lambda) \right] 
On the other hand, on the event $\Omega_{3},$ we have
   $\Tr\,f_{\tau_0}(\bH /n)
   =   \sum_{i=1}^{n} f_{\tau_0}(\lambda_i(\bH /n))
   \le r_k'.$
%   \le 
%\frac1{n}  \sum_{i=1}^{n}  \one_{\{\lambda_i((\bH +\bS)/n) \le 0\}}
Hence, we can bound for any such $\bbV$
\begin{align}
    %\P\left( \left\{x_{\min}(\mu_{\widehat\nu_{\bV,\bU}}) < -\delta \right\} \cap \cA_n \right)
\P\left(\Omega_{3} \right)
   % &\le \P\left( \left\{\frac{1}{dk}\Tr\left(f_{\delta}(\bH/n)\right)\leq \frac{3  r_k}{dk}\right\} \cap
   % \left\{\E_{\mu_{\star}} 
   % \left[ f_{\delta}(\Lambda)\right] \geq \delta\right\}
   % \right)\\
    &\le  
    \P\left(\left|\frac1{dk} \Tr(f_{\tau_0}(\bH/n)) - \E_{\mu_{\star}}[f_{\tau_0}(\Lambda)] \right| > \tau_0- \frac{r_k'}{dk}\right)\\
    &\le
    \P\left(\left|\frac1{n} \Tr(f_{\tau_0}(\bH/n)) - 
    \frac1{n} \E\left[\Tr(f_{\tau_0}(\bH + /n))\right]
    \right| > \frac{dk}{n}\tau_0- \frac{r_k'}{n} - \omega_{n}(\tau_0)\right)\\
    &
\le
     C_{10} \exp \left\{ 
    -c\frac{ 
    \tau_0(k\tau_0- r_k'/d - \alpha_n\omega_{n}(\tau_0))
    n^{3/2} }{ \alpha_n^{1/2} k \sfK }
    \right\}
\end{align}
where $\omega_{n}(\tau_0) \to 0$ for any $\delta >0$ as $n\to\infty,$ by Proposition~\ref{prop:uniform_convergence_lipschitz_test_functions}.
So we conclude that
for any $\bbV$ satisfying~\eqref{eq:constraint_on_V_asymp}
\begin{align}
&\E\left[  \big|\det\left(\bH  + \bDelta_{1,k}\right)\big|   \Err_\sigma(\bX + \bDelta_{0,k})^{r_k} \one_{\{\bH  + \bDelta_{1,k} \succ 0\}}\right]
\le 
\E\left[  \big|\det\left(\bH +  \bDelta_{1,k}\right)\big|   \Err_\sigma(\bX + \bDelta_{0,k})^{r_k} \one_{\Omega_3}\right]\\
%&\hspace{20mm}\le 
%\E\left[\big|\det\left(\bH + \bS + \bDelta_{1,k}\right)\big|^4\right]^{1/4}\E\left[E(\bX^\sT\bX + \bDelta_{2,k})^{2r_k}\right]^{1/2} \P\left(\one_{\Omega_3}\right)^{1/4}\\
&\hspace{20mm}\le n^{dk + 4r_k}  
C_{11}
\sfC_0^{3r_k}
\;
\sfC_1^{dk} \exp \left\{ 
    -c_2\frac{ 
    \delta(k\delta- r_k'/d - \alpha_n\omega_{n}(\delta))
    n^{3/2} }{ \alpha_n^{1/2} k \sfK }
    \right\}\\
    &\hspace{20mm}\stackrel{(a)}{\le} \exp\left\{ - c_3 \delta^2 n^{3/2}\right\},
\end{align}
where in $(a)$ we took $n > C_{12}(\tau_0)$ for some $C_{12}$ so that so that
$\max\{\sfC_0,\sfC_1\} \le  e^{\sqrt{n}}$ and $\omega_{n}(\tau) <\tau/3.$


\noindent\textbf{Step 4: 
Proof of item \textit{2.}}
We now deal with the determinant term for $(\bTheta,\bbV)$ not satisfying~\eqref{eq:constraint_on_V_asymp}. 
Namely, we'll show concentration of the determinant.

First, note that since $t\mapsto \log t$ is monotonically increasing for $t>0$, when $\bH +  \bDelta_{1,k} \succ \bzero$, we have for any $\tau_1>0$,
  \begin{align}
\label{eq:log_det_to_log_eps_2}
      \log \left(\det((\bH + \bDelta_{1,k})/n)\right) =& \sum_{i=1}^{dk}\log 
      (\lambda_i(\bH+  \bDelta_{1,k} )/n)\\
      %=& \sum_{i=1}^{dk}\log^{(\eps_\bH)}
      %(\lambda_i(\bH+\bE_k))\\
      \leq& \sum_{i=1}^{dk -r'_k}\log \lambda_i(\bH/n) + r_k'\log\left( \frac{\|\bH +  \bDelta_{1,k}\|_\op}{n}\right)\\
      %\leq & \sum_{i=1}^{dk} \log^{(\eps)} \lambda_i(\bH/n) - 3r_k\log\eps + 3r_k\log(2\norm{\bH}_\op)\\
      \leq& \Tr\left(\log^{(\tau_1)}(\bH /n) \right) + 
r_k'\log\left( \frac{\|\bH+  \bDelta_{1,k}\|_\op}{n \tau_1}\right),
  \end{align}
where we defined $\log^{(\tau_1)}(t) := \log(t \vee \tau_1).$
Combining with the result of Step 1, we conclude that
\begin{align}
\label{eq:step_2_det_conc_result}
&\E[|\det \big(\de \bz\big)|\one_{\bH  \succ \bzero} \big|\bzeta = 0 ]\\
&
\quad{\le}
\frac{n^{dk}}{(n\sfsigma_\bH)^{r_k}}
\E\left[ 
\exp\left\{
\Tr\log^{(\tau_1)}\left(\frac{1}n\bH\right) \right\}  
\left( \frac{\|\bH + \bDelta_{1,k}\|_\op}{n \tau_1}\right)^{12 r_k}
\Err_0(\bX + \bDelta_{0,k})^{r_k} \one_{\{\bH +  \bDelta_{1,k} \succ \bzero\}}\right].
\end{align}
Noting that for any $\eps>0$, 
$t\mapsto\log^\up{\eps}(t)$ has Lipschitz modulus bounded by $\eps^{-1}$, we now apply Lipschitz concentration (Lemma~\ref{lemma:concentration_lipschitz_func}) to $\Tr\log^\up{\tau_1}(\bH /n)$.
To this end, for any $t_n>0$, define the event
%\begin{equation}
%   t >  \frac{\sfK}{\alpha_n^{1/2}} \frac{k}{\eps n^{1/2}},
%\end{equation}
define the event 
\begin{equation}
    \Omega_4 := \left\{
    \left|\frac1{n}\Tr\Big(\log^{(\tau_1)}
    \left(\bH/{n}\right)\Big) - 
    \frac1{n}\E_\bX\left[ \Tr\Big(\log^{(\tau_1)}
    \left(\bH/{n}\right)
    \Big)\right]\right| \le t_n
    \right\}.
\end{equation}
Then the expectation in Eq.~\eqref{eq:step_2_det_conc_result} is bounded as
\begin{align}
&\E\left[ 
\exp\left\{
\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right) \right\}  
\left( \frac{\|\bH + \bDelta_{1,k}\|_\op}{n \tau_1}\right)^{r_k'}
\Err_\sigma(\bX + \bDelta_{0,k})^{r_k} \one_{\{\bH +  \bDelta_{1,k} \succ \bzero\}}\right]\\
&\quad\le 
\exp\left\{
\E\left[\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right) \right] + nt_n
\right\}
\E\left[ 
\left( \frac{\|\bH + \bDelta_{1,k}\|_\op}{n \tau_1}\right)^{r_k'}
\Err_\sigma(\bX + \bDelta_{0,k})^{r_k} \one_{\{\bH +  \bDelta_{1,k} \succ \bzero\}}\right]\\
&\hspace{20mm}+
\E\left[ 
\left( \frac{\|\bH +  \bDelta_{1,k}\|_\op}{n}\right)^{dk}
\Err_\sigma(\bX + \bDelta_{0,k})^{r_k} \one_{\{\bH +  \bDelta_{1,k} \succ \bzero\}} \one_{\Omega_4^c}\right]\\
&\le 
\Delta_3\Bigg(\exp\left\{
\E\left[\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right) \right] + nt_n
\right\}
\left(\frac{1}{\tau_1}\right)^{12r_k}
\Delta_4(24 r_k)^{1/2}+
\P(\Omega_4^c)^{1/4}
\Delta_4(4dk)^{1/4}
\Bigg) ,
\label{eq:concentration_bound_decomp}
\end{align}
%
Choosing $t_n$ in the definition of $\Omega_4$ as 
\begin{equation}
\label{eq:choice_of_t_concentraion}    
%t_n :=  \frac{\sfK k^2\log(n)}{ n^{1/2} \eps \alpha_n^{1/2}}
%\quad 
%\bns{\textrm{change to}}
t_n :=  \frac{ c_1\sfK k}{ \tau_1 \alpha_n^{1/2}} n^{-1/4}
\end{equation}
for appropriate constant $c$, we conclude 
by Lemma~\ref{lemma:concentration_lipschitz_func}
we have
$\P(\Omega_4^c) \le  C_{13}(\sfK) \exp\left\{- n^{5/4}\right\}$.
%\begin{equation}
%    %\P(\Omega_4^c) \le  C_5(\sfK) \exp\left\{- n c_1(\sfK) \log(n) k\right\}.
%   %\bns{\textrm{Change to}} 
%    \P(\Omega_4^c) \le  C_5(\sfK) \exp\left\{- c_1(\sfK)n^{3/2} \eps_n\right\}.
%\end{equation}
Then after combining with Eq.~\eqref{eq:concentration_bound_decomp} along with the bounds on $\Delta_3$ and $\Delta_4$ derived previously we have
\begin{align}
&\E[|\det \big(\de \bz\big)|\one_{\bH  \succ \bzero} \big|\bzeta = \bzero ]\\
&
\quad{\le}
\frac{\; \sfC_0^{3r_k}\; n^{dk+4r_k}}{(n\sfsigma_\bH)^{r_k}}
\Bigg(\exp\left\{
\E\left[\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right) \right] + n\frac{c_1 \sfK k }{\tau_1 \alpha_n^{1/2}} n^{-1/4}
\right\}
\left(\frac{\sfC_1}{\tau_1}\right)^{12r_k}
+C_{14}\sfC_1^{dk} \exp\left\{- n^{5/4}  \right\}
\Bigg)\\
&\quad{\stackrel{(a)}{\le}}
{n^{dk}}
\Bigg(\exp\left\{
\E\left[\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right) \right] + n \frac{C_{15} n^{-1/4}}{\tau_1}
\right\}
+C_{16}\exp\left\{- n^{5/4}\right\}
\Bigg)
\end{align}
where in $(a)$ we took $n > C_{17}$ so that  $\sfC_0,\sfC_1 \le  e^{n^{1/2}}$ and
used that
$\sfsigma_{\bH}^{-1} =  e^{o(n)}$.
\qed








\subsection{Asymptotics of the Kac-Rice integral}
\label{sec:kr_asymptotics}
What remains now is to study the asymptotics of the integral to derive the upper bound of Theorem~\ref{thm:general}. 
Let us begin by proving Lemma~\ref{lemma:asymp_1} in the next section.

\subsubsection{Proof of Lemma~\ref{lemma:asymp_1}}
\label{sec:proof_prop_asymp_1}

Fix $\tau_0,\tau_1 \in(0,1)$ and $\beta$ as in the statement of the proposition.
Once again, we suppress the indices $(\bTheta,\bbV)$ in the arguments.

\noindent\textbf{Step 1: obtaining the hard constraint on the support.}
First, we show that
\begin{align}
\mathrm{(I)} &:= \limsup_{n\to\infty}\frac1n\log\left(
\E_\bw\left[
\int_{\cM}
\E[|\det \big(\de \bz\big)|\one_{\bH \succ n\sfsigma_\bH} \big|\bzeta = 0 ,\bw]
p_{\bTheta,\bbV}(\bzero)
\right)
\one_{\bw\in\cG}
\right]
\one_{\mu_{\star}(\hnu,\hmu)((-\infty,- \tau_0)) \ge \tau_0} \de_{\cM} V\\
&= -\infty.
\end{align}
Directly by item \textit{(2.)} of Lemma~\ref{lemma:CE_bound}, followed by the bound on $p_{\bTheta,\bbV}(\bzero)$ of Corollary~\ref{cor:uniform_density_bound}, we have for some $C_0,c_0$ depending only on $\sOmega$,
\begin{align}
\textrm
{(I)}&\le 
\limsup_{n\to\infty}\frac1n \log\left(
 C_0 e^{-c_0 \tau_0^2 n^{3/2}} 
 \E_\bw\left[
\int_{\cM} 
p_{\bTheta,\bbV}(\bzero)
\de_{\cM} V\;
\one_{\bw \in\cG}
\right]
\right)\\
&\le 
\limsup_{n\to\infty}\frac1n \log\left(
\frac{
 C_0 e^{-c_0 \tau_0^2 n^{3/2}}
\sfsigma_{\bR}^{-nk/2} \sfsigma_{\bL}^{-(d-r_k)/2}}{ (2\pi)^{(dk + nk + nk_0 -r_k)/2} n^{dk/2}}
 \E_\bw[\vol(\cM) \one_{\bw \in \cG}]
\right).
\end{align}
To estimate $\vol(\cM)$, we use Lemma~\ref{lemma:manifold_integral} with $f=1$ and the $\beta$ chosen. Letting $\Err_{\textrm{blow-up}}(\beta,n)$ be the multiplicative error defined therein, we have
\begin{equation}
    \vol(\cM) \le 
    \Err_{\textrm{blow-up}}(\beta,n)
    \vol(\cM^\up{\beta}) \le 
    \Err_{\textrm{blow-up}}(\beta,n)
     \vol\left(\Ball_{(k+k_0)\sfA_{\bbV}}^{n(k+k_0)}(\bzero)\right)
    \vol\left(\Ball_{(k+k_0)\sfA_{\bTheta}}^{dk}(\bzero)\right)
\end{equation}
where we used that $\cM \subseteq
\Ball_{(k+k_0)\sfA_{\bV}}^{n(k+k_0)}(\bzero) \times \Ball_{(k+k_0)\sfA_{\bTheta}}^{dk}(\bzero)$. Evaluating these terms, substituting into the upperbound on (I), then taking $n\to\infty$ shows the claim.  (Recall that $\sfsigma_\bR^{-1},\sfsigma_\bL^{-1},\sfA_{\bbV},\sfA_{\bR} = O(1)$).

\noindent\textbf{Step 2: bounding the asymptotically dominating term.}
Define
\begin{align}
F_{n,\tau_1}(\bbV,\bTheta,\bw)
&:=
 \frac{k}{2\alpha_n}\log(\alpha_n)+
\frac{k}{\alpha_n}\E\left[\frac1{dk}\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right)\Big| \bw \right]  - \frac{1}{2\alpha_n}\log \det\left(\frac{\bL^\sT\bL}{n}\right)\\
&\quad+ \frac{1}{2\alpha_n}\Tr(\bTheta^\sT\bTheta) 
   -\frac{n}{2}
\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bRho^\sT\right) + \frac1{2}\Tr\left(\frac1n\bbV^\sT\bL (\bL^\sT\bL)^{-1}\bL^\sT \bbV \bR^{-1}\right)\\
&+ \frac12 \Tr\left(\frac1n\bbV(\bI - \bR^{-1})\bbV^\sT\right)
-\frac1{2}\log\det(\bR).
\end{align}
We show that
\begin{align}
\mathrm{(II)} &:=
\limsup_{n\to\infty}\frac1n\log\left(
\E_\bw\left[
\int_{\cM}
\E[|\det \big(\de \bz\big)|\one_{\bH \succ n\sfsigma_\bH} \big|\bzeta = 0, \bw ]
\; \one_{\bw \in \cG}
\right]
\right)
\one_{\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0])< \tau_0} \de_{\cM} V \\
&\le \limsup_{n\to\infty}
   \frac1n\log\left( \E_\bw\left[\int_{\cM}\exp\left\{nF_{n,\tau_1}(\bbV,\bTheta)\right\} p_{1}(\bbV) p_{2}(\bTheta)
   \one_{\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0]) < \tau_0}
   \de_\cM V
   \; \one_{\bw\in\cG}
   \right]\right)
   \label{eq:bound_step_2_asymptotics_1}
\end{align}
In what follows, we use 
\begin{equation}
    K_{n,\tau_1}(\bbV,\bTheta,\bw) := 
\E\left[\frac1{dk}\Tr\log^{(\tau_1)}\left(\frac{\bH}n\right) \bigg| \bw\right],
\end{equation}
By item \textit{(1.)} of Lemma~\ref{lemma:CE_bound},
there exists $C_1$ depending only on $\sOmega$ such that
for any $(\hnu,\hmu)$ satisfying the 
support condition
$\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0])< \tau_0$,
\begin{align}
\E[|\det \big(\de \bz\big)|\one_{\bH  \succ n\sfsigma_\bH} \big|\bzeta = \bzero,\bw ]\le
n^{dk}
\Bigg(\exp\left\{
\frac{nk}{\alpha_n} K_{n,\tau_1}(\bbV,\bTheta,\bw) +  \frac{C_1 n^{1-1/4}}{\tau_1}
\right\}
+ \exp\left\{ -n^{5/4} \right\}
\Bigg)\
\end{align}
Note that we have the uniform-bound
\begin{align}
    \exp\left\{- n^{5/4}\right\} \le \exp\left\{\frac{nk}{\alpha_n} \log(\tau_1) + \frac{C_1 n^{1-1/4}}{\tau_1}\right\} &\le 
    \exp\left\{
\frac{nk}{\alpha_n} K_{n,\tau_1}(\bbV,\bTheta,\bw) +  \frac{C_1 n^{1-1/4}}{\tau_1}
\right\},
\end{align}
holding for $n$ large enough, uniformly over all $(\bTheta,\bbV,\bw)$.
Then since the term $n^{1-1/4}C_1/\tau_1$ is exponentially trivial, we conclude that
\begin{equation}
   \mathrm{(II)}  \le  \limsup_{n\to\infty} \frac1n \log \E_\bw\left[ \int_{\cM} 
   \exp\left\{
   \frac{nk}{\alpha} K_{n,\tau_1}(\bbV,\bTheta,\bw)
   \right\}
\one_{\{\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0])< \tau_0\}}
   p_{\bbV,\bTheta}(\bzero)\de_\cM V \; \one_{\bw\in\cG}\right].
\end{equation}
What remains to
conclude Eq.~\eqref{eq:bound_step_2_asymptotics_1} is to recall the bound on $p_{\bbV,\bTheta}(\bzero)$ in Lemma~\ref{lemma:density_bounds} and ignore the exponentially trivial factors of $\sfA_{\bL}^{r_k^2/2}$, and simplify to obtain a bound in terms of of $F_{n,\tau_1}$.

\noindent \textbf{Step 3: Estimating the integral over the manifold.}
We now rewrite the bound on $\mathrm{(II)}$ as an expectation over the blow-up of the manifold from Lemma~\ref{lemma:manifold_integral}. 
Choose a sequence $\beta_n = c_0 \sfsigma_{\bG,n}^3$ for sufficiently small constant $c_0>0$ so that $\beta_n$ satisfies the condition in Lemma~\ref{lemma:manifold_integral} for all $n$ sufficiently large.
We apply this lemma with this chosen value of $\beta_n$ to the function 
\begin{equation}
    f(\bTheta,\bbV) :=  e^{nF_{n,\tau_1}(\bTheta,\bbV)} p_1(\bbV)p_2(\bTheta).
\end{equation}
Via Wielandt-Hoffman, it's easy to verify that under Assumption~\ref{ass:loss} and~\ref{ass:regularizer}, guaranteeing the Lipschitzness and local Lipschitzness of the derivatives of the loss and the regularizer, respectively, that

\begin{equation}
 \|\log f\|_{\Lip,\cM^\up{1}} 
\le C_3(\tau_1, \sfA_{\bR},\sfA_{\bV},\sfA_{\bw},\sfsigma_{\bR},\sfsigma_{\bV},\alpha_n, r_k) \; n
\end{equation} 
for some $C_3$ that remains bounded for $\alpha_n$ in a compact subset of $(1,\infty)$, so that
\begin{equation}
 \lim_{n\to\infty}\frac1n \left(\beta_n\|\log f\|_{\Lip,\cM^\up{1}}  \right) = 0
\end{equation}
by the choice of $\beta_n$.
Similarly,
recalling $\Err_{\textrm{blow-up}}(\beta,n)$ the multiplicative error term defined in Lemma~\ref{lem:intg-tube}, we see that
\begin{align}
    \limsup_{n\to\infty} \log(
    \Err_{\textrm{blow-up}}(\beta_n,n)) 
    &=
   \limsup_{n\to\infty} \frac1n \log\left(\left(\frac1{1 - \beta_n\;r_k^2 C_4 }\right)^{(m-r_k)/2}
        \left(\frac{r_k^{5/2} C_4}{\beta_n(\sfsigma_{\bG,n} - \beta_n C_4 r_k^2)}\right)^{r_k}\right) =0,
\end{align}
by the choice of $\beta_n$ and Assumption~\ref{ass:params} that $\sfsigma_{\bG,n} = e^{-o(n)}$.
Combining with \textbf{Step 2} we conclude that
\begin{align}
   \mathrm{(II)} &\le \limsup_{n\to\infty}\frac1n\log
   \E\left[\E\left[\exp\left\{nF_{n,\tau_1}(\bbV,\bTheta)\right\}
   \one_{\{\mu_{\star}(\hnu,\hmu)((-\infty,-\tau_0]) < \tau_0\} \cap \cM^{(\beta_n)}} \Big| \bw
   \right] \one_{\bw\in\cG}\right],
%   &\limsup_{n\to\infty}\frac1n\log
%   \E\left[\E\left[\exp\left\{nF_{n,\tau_1}(\bbV,\bTheta)\right\}
%   \one_{\{\mu_{\star}(\hnu,\hmu)((-\infty, \tau_0]) < \tau_0\} \cap \cM^{(\beta_n)}} \Big| \bw
%   \right] \one_{\bw\in\cG}\right]
\end{align}
where the expectation is under $p_1(\bbV),p_2(\bTheta).$


%For instance, note that we have
%\begin{equation}
%    \|\log p_1\|_{\Lip,\cM^\up{1}} \le C_2 (k+k_0)^{1/2} (1 + \sfA_{\bV} ) 
%    \quad\quad\textrm{and}\quad\quad
%    \|\log p_2\|_{\Lip,\cM^\up{1}} \le C_3 k^{1/2} (1 + \sfA_{\bR}) 
%\end{equation}
%for universal constants $C_2,C_3 >0.$
%
%Similarly for $F_{n,\tau_1}(\bbV,\bTheta)$: we have
%\begin{equation}
%    \frac{nk}{dk \alpha_n}\| K_{n,\tau_1}(\bbV,\bTheta)\|_{\Lip,\cM^\up{1}} \le  \|\\|(dk)^{1/2}\|\|
%\end{equation}
%

\noindent\textbf{Step 4: Concluding.} Finally, we write the bound on \textrm{(II)} in terms of empirical measures $\hmu,\hnu$ of $\sqrt{d}[\bTheta,\bTheta_0]$, $[\bbV,\bw]$ respectively.
Set
\begin{equation}
K_{\tau_1}(\hnu,\hmu) := \int \log(\lambda \vee \tau_1) \mu_{\star}(\hnu,\hmu)(\de \lambda)
\end{equation}
then note that
by the uniform bounds of Proposition~\ref{prop:uniform_convergence_lipschitz_test_functions}, we have
\begin{align}
\exp\left\{\frac{nk}{\alpha_n} K_{n,\tau_1}(\bbV,\bTheta) 
\right\}
\le
\exp\left\{\frac{nk}{\alpha_n} K_{\tau_1}(\hnu,\hmu) +   \frac{nk}{\alpha_n} \omega_{\textrm{ST}}(n, \tau_1)
\right\}
\end{align}
for $\omega_{\textrm{ST}}(n, \tau_1) = o(1)$ uniformly over $\hnu,\hmu$ so that 
$n k\omega_{\textrm{ST}}(n, \tau_1)/\alpha_n$ is exponentially trivial.
Furthermore, it's easy to check that for any $\beta \in (0,1)$, 
we have
\begin{equation}
 \cM^\up{\beta_n} \subseteq \{
 (\bTheta,\bbV) : (\hmu,\hnu) \in \cuM^\up{\beta_n}  \}
 \subseteq \{
 (\bTheta,\bbV) : (\hmu,\hnu) \in \cuM^\up{\beta}  \}
\end{equation}
for $n$ sufficiently large,
since $\beta_n \to 0$.
So since the integrand is nonnegative, 
and combining with the bound on \textrm{(II)} from \textbf{Step 3}, and recalling the definition of $\phi_{\tau_1}$ from the statement of the proposition, we obtain
\begin{equation}
   \mathrm{(II)} \le \limsup_{n\to\infty}\frac1n\log
   \E_\bw\left[\E\left[\exp\left\{n\phi_{\tau_1}(\hnu,\hmu)\right\}
   \one_{\{\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0]) < \tau_0\} \cap \cuM^{(\beta)}}
   | \bw \right] \one_{\bw\in\cG}\right] 
   %+ o_\beta(1;\tau_1),
\end{equation}
Finally, combining this with the bound on $\textrm{(I)}$ from \textbf{Step 1}, and invoking Lemma~\ref{lemma:kac_rice_manifold} gives the result of the lemma.
\qed


\subsubsection{Large deviations and completing the proof of Theorem~\ref{thm:general}}
\label{sec:proof_thm1_large_deviations}

To obtain the asymptotic formula of Theorem~\ref{thm:general} and complete the proof, we study the limit obtained in Lemma~\ref{lemma:asymp_1} and obtain an upper bound via Varadhan's integral lemma. 


%\begin{lemma}
%\label{lem:varadhan}
%     Working under the assumptions of Section~\ref{sec:assumptions} and the notation of Lemma~\ref{lemma:asymp_1} and Theorem~\ref{thm:general},
%   % \begin{equation}
%   %     \cuV:= \overline{\cuM}\cap\{(\mu,\nu):\mu_{\star}(\mu,\nu)((-\infty,0)) =0,\;\; \nu_{w}=\P_w, \;\;\mu_{{\btheta_0}}=\mu_{0}\},
%   % \end{equation}
%   % and for $\delta>0$, let $\cG_\delta$ be any set satisfying
%   % \begin{equation}
%   % \cG_\delta \subseteq \cB_{\sfA_{w}\sqrt{n}}^n (\bzero) \cap \{\bw \in\R^n :  d_{\textrm{LU}}(\hnu_{\bw}, \P_w) < \delta\}.
%   % \end{equation}
%  we have 
%    \begin{equation}
%        \lim_{\delta\to 0}\limsup_{n\to \infty}\frac1n \log\left(
%        \E[Z_n(\cuA,\cuB,\sPi,\bTheta_0)
%         \one_{
%             \cG_\delta}]\right) \le
%        \sup_{(\nu,\mu)\in \cuS}
%        \big\{\phi_0(\nu,\mu) - \KL(\nu_{\bv|\bw}\| \cN(\bzero,\bI_{k+k_0})) 
%        -\frac1\alpha \KL(\mu_{\btheta|{\btheta_0}}\| \cN(\bzero,\bI_k)) \big\}      
%    \end{equation}
%\end{lemma}
%







%To apply Varadhan's Lemma, we must first establish a Large Deviation Principle (LDP) for the empirical measure $(\hmu,\hnu)$. To do so, we find good rate functions for $\hmu$, and $\hnu$  separately and then use the contraction principle to derive a good rate function for $(\hmu,\hnu)$.

%Let us first obtain the LDP for $\hmu$.
% Let $C_b(\R^{k_0}) $ denote the collection of bounded continuous
%    functions mapping $\R^{k_0}$ to $\R$.
%For any $\varrho \in C_b(\R^{k_0})$, 
%   define the logarithmic moment generating function as the limit
%    \begin{equation}
%        \Lambda_0(\varrho):=  \lim_{n\to\infty} \frac1n \log \E\left[
%       \exp\left\{ \sum_{i=1}^d\varrho(\sqrt{d} \btheta_{0,j}) \right\} \right]
%       = \lim_{n\to\infty}  \frac1n
%       d \langle \varrho,\hmu_{\sqrt{d} \bTheta_0}\rangle
%         = 
%         \frac1\alpha\langle \varrho,\mu_{0} \rangle.
%    \end{equation}
%    Noting that $\{\hmu_{\sqrt{d_n}\bTheta_0}\}_{n \in \N}$ is a deterministic sequence of points in the set $\cuP(\R^{k_0})$,
%     indexed by $n$,
%     that converge in the weak topology, the collection is exponentially tight, and therefore 
%by the G\"artner-Ellis theorem alongside 
%the weak convergence implied by Assumption \ref{ass:theta_0} of $\hat\mu_{\sqrt{d}\bTheta_0}$ to $\mu_0,$ 
%     $\hmu_{\sqrt{d}\bTheta_0}$ satisfies a full LDP with good rate function $\Lambda^\star_0$ defined as the Legendre transform of $\Lambda_0$:
%     \begin{equation}
%         \Lambda^\star_0(\tilde\mu_0):= \sup_{\varrho\in C_b(\R^{k_0})} 
%         \left\{ \langle\varrho,\tilde\mu_0\rangle - \langle \varrho,\mu_{0}\rangle\right\},
%     \end{equation}     
%     for $\tilde\mu_0 \in\cuP(\R^{k_0})$.
%Meanwhile, an immediate application of Sanov's Theorem establishes an LDP for $\hmu_{\sqrt{d}\hat\bTheta}$ when $\hat\bTheta$ with rows i.i.d. from $\normal(\bzero,\bI_k/d)$, with good rate function 
%\begin{equation}
%I_1(\tilde\mu_1) := \frac{1}{\alpha}\KL(\tilde\mu_1\|\cN(\bzero,\bI_k))
%\end{equation}
%for $\tilde\mu_1 \in\cuP({\R^{k}}).$
%
%Letting $F:\cuP(\R^{k}) \times \cuP(\R^{k_0}) \mapsto \cuP(\R^{k+k_0})$ defined by
%\begin{equation}
%    \tilde \mu := F(\tilde \mu_1,\tilde \mu_0) \quad\quad 
%    \tilde \mu_{\cdot | \btheta_0} =  \tilde \mu_1
%\end{equation}
%The contraction principle~\notate{Statement 4.2.7 of Dembo} then gives the good rate function for the joint $\mu$ as 
%\begin{equation}
%    I_m(\tilde \mu) := \inf_{(\tilde \mu_1,\tilde \mu_0): \tilde \mu_{\btheta} }
%\end{equation}
%
%
%
%
%and 
%    $ \KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times \P_w) $, respectively.
%    Also by Lemma 6.2.6 from \cite{dembo2009large}, the laws of the sequences $\{\hmu_{\cI_{\btheta_1}|\cI_{\btheta_0}}\}$ and $\{\hnu\}$ are exponentially tight. Therefore, by contraction principle, $(\hmu,\hnu)$ satisfies the LDP with the good rate function
%    \begin{equation}
%       \frac1\alpha\KL(\mu_{\cI_{\btheta_1}|\cI_{\btheta_0}}\|\cN(\bzero,\bI_k))
%        + \KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times \P_w)  + 
%        \Lambda^\star(\mu_{\cI_0}).
%    \end{equation}
%------






First, by Sanov's Theorem applied to $\hmu$, viewing the marginals 
$\{\hmu_{\sqrt{d_n}\bTheta_0}\}_{n}$ as a deterministic sequence of points in the set $\cuP(\R^{k_0})$  converging to $\mu_0$ (and hence exponentially tight), we have for any Borel measurable $\cU_1\subseteq\cuP(\R^{k+k_0})$,
\begin{equation}
 \lim_{n\to\infty} \frac1n\log \P_{\substack{\bTheta\sim \cN(\bI/d) \\ \bTheta_0\sim \hmu_{\sqrt{d}\bTheta_0}} }\left(\hmu\in\cU_1\right)   \le -\inf_{\substack{\mu\in\overline{\cU_1}\\ \mu_{(\btheta_0)} = \mu_0 }} 
 \frac1\alpha\KL(\mu_{\cdot|\btheta_0}\|\cN(\bzero,\bI_k)).
\end{equation}
Similarly, by Sanov's once again, for any measurable $\cU_2\subseteq\cuP(\R^{k+k_0+1}), $ we have
\begin{equation}
 \lim_{n\to\infty} \frac1n\log \P_{\substack{\bbV\sim \cN(\bI) \\ \bw\sim \P_\bw} }\left(\hnu\in\cU_2\right)   \le -\inf_{\substack{\nu\in\overline\cU_2}} 
 \KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times \P_w).
\end{equation}
The contraction principle then gives the LDP for the pair $(\hmu,\hnu):$ 
\begin{equation}
    \lim_{n\to\infty} \frac1n \log \P((\hmu,\hnu) \in   \cU) 
    \le -\inf_{\substack{(\mu,\nu) \in\cU\\ \mu_{(\btheta_0)} = \mu_0}} I(\mu,\nu)
\end{equation}
 for
    \begin{equation}
        I(\mu,\nu) :=      \frac1\alpha\KL(\mu_{\cdot|\btheta_0}\|\cN(\bzero,\bI_k))+ \KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times \P_w)) 
    \end{equation}
    and $\cU$ measurable subset of $\cuP(\R^{k+k_0}) \times \cuP(\R^{k+k_0+1})$.

  
Now we use this LDP above alongside Varadhan's integration lemma to bound the limit in Lemma~\ref{lemma:asymp_1}.
   % \kas{Mistake here: secon moments are not bounded on the blow up. chagne the def of blow up}
Observe first that one can directly show the exponent $\phi_{\tau_1}(\hmu,\hnu)$ of Lemma~\ref{lemma:asymp_1} is uniformly bounded for $(\hmu,\hnu) \in {\cuM^\up{\beta}}$. Indeed, in Section~\ref{sec:RMT} in the proof of Proposition~\ref{prop:uniform_convergence_lipschitz_test_functions}, we showed that $\mu_\star(\hnu,\hmu)$ is compactly supported, with support bounded uniformly in $(\hmu,\hnu)$, and hence its truncated logarithmic potential is finite. Furthermore, the bounds $\sfA_{\bR},\sfA_{\bbV},\sfsigma_{\bR},\sfsigma_{\bL},\sfsigma_{\bV}$ in the definition of $\cuM$ guarantee uniform bounds on the functionals of $\nu,\mu$ appearing in the definition of $\phi_{\tau_1}$, so we have sufficient exponential tightness to apply Lemma 4.3.6 of \cite{dembo2009large} in what follows:
Define for $\beta\ge 0, \tau_0\ge 0, \delta\ge 0$,
    \begin{equation}
        \cuS(\beta,\tau_0,\delta):= 
        \overline{\cuM^{(\beta)}} \cap 
        \{(\mu,\nu):\; \mu_\star(\mu,\nu)((-\infty,-\tau_0))\le \tau_0, \;\;
        \de_{\textrm{BL}}(\nu_{w},\P_w)\le\delta,
        \},
    \end{equation} 
    and
    \begin{equation}
        \cuS_0(\beta,\tau_0,\delta) := \cuS(\beta,\tau_0,\delta) \cap
       \{\mu_{(\btheta_0)}  = \mu_0\}.
    \end{equation}
    %Next, note that since $\cuS_{\beta,\delta_1,\gamma}$ is a closed set, 
    Then we have
    %foll4owing an application of Lemma~\ref{lemma:asymp_1} gives us :
    \begin{align} 
         \limsup_{n\to\infty}&\frac1n \log ( \E[ Z_n\one_{\bw \in \cG_\delta}])
        \le
        \limsup_{n\to\infty}\frac1n\log
      \E\left[\exp\left\{n\phi_{\tau_1}(\hnu,\hmu)\right\}
       \one_{\{(\hmu,\hnu)\in\cuS(\beta,\tau_0,\gamma)\}}
       \right]\\
%       \le&
%        \sup_{(\mu,\nu)\in \cuS_{\beta,\tau_1,\delta}}
%        \Big\{ \phi_{\tau_1}(\mu,\nu) -  \frac1\alpha\KL(\mu_{\cdot|\btheta_0}\|\cN(\bzero,\bI_k))
%         - \KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times \P_w)  - 
%        \Lambda^\star(\mu_{\cI_0}) \Big\}\\
%        \label{eq:varadhan+blowup}
        \le& \sup_{\substack{(\mu,\nu)\in \cuS_0(\beta,\tau_0,\delta)}}
        \big\{ \phi_{\tau_1}(\mu,\nu) - \frac1\alpha\KL(\mu_{\cI_{\btheta}|\cI_{\btheta_0}}\|\cN(\bzero,\bI_k))
         -\KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times \P_w)) 
        \big\},
        \label{eq:varadhan+blowup}
    \end{align}
    where in the first inequality we used Lemma~\ref{lemma:asymp_1}.
%where in the last inequality we used  that $\Lambda^\star(\nu) = \infty$ for any $\nu\neq \mu_{\btheta_0}$, and $\Lambda^\star(\mu_{\btheta_0}) = 0$.
Now what's left is to show that we can the parameters $\tau_0,\tau_1$ and $\beta$ to $0$, and in the second we used the referenced Varadhan's integral upper bound from~\cite{dembo2009large} and that of $\cuS_0(\beta,\tau_0,\delta)$ is closed. (To see that it is indeed closed, note that we have shown in Section~\ref{sec:RMT} that if $(\mu,\nu) \mapsto \mu_\star$ is continuous in the topology of weak convergence, meanwhile, for a weakly converging sequence of random variables $X_n \to X$, we have $\P(X \in \cU) \le \liminf_{n} \P(X_n \in\cU)$ for any open set $\cU$).

Now note that $\cuS_0(\beta,\tau_0,\delta)$ is a compact subset of $\cuP(\R^{k+k_0})\times \cuP(\R^{k+k_0+1})$. This can be verified through Prokhorov's Theorem: if we show $\cuM^\up{\beta}$ is tight, Prokhorov implies that $\overline{\cuM^\up{\beta}}$ is compact which gives the compactness of $\cuS_0(\beta,\tau_0,\delta);$ the closed subset of $\overline{\cuM^\up{\beta}}$. 

To prove $\cuM^\up{\beta}$ is tight, note that for any $(\mu,\nu)\in {\cuM^\up{\beta}}$, 
letting $\bar\btheta^\sT = [\btheta^\sT,\btheta_0^\sT],$
we have for some $C$ depending on $\beta$,
    $(\sfA_\bR + C(\beta))\bI \succ \E_{\mu}[\bar\btheta \bar\btheta^\sT]$. Hence $\|\E_{\mu}[\btheta]\|_2^2\le (k+k_0)(\sfA_{\bR} + C(\beta))$, so an application of Markov's yields
    $\P_{\mu}\left(\|\btheta\|_2 > t\right) \le (k+k_0)(C(\beta) + \sfA_\bR)/t^2 \to 0$ as $t\to 0$.
With a similar inequality applied with $\nu$ instead of $\mu$ then gives tightness of $\cuM^\up{\beta}$ for any $\beta\ge 0$.


Now 
let us first take $\beta\rightarrow 0$
in the bound of
    Eq.~\eqref{eq:varadhan+blowup}.
Take a sequence $\{\beta_i\}_{i\in\N}$ such that $\beta_i\to 0$, and by compactness, take  $\{(\mu_{\beta_i},\nu_{\beta_i})\in \cuS_0(\beta_i,\tau_0,\delta)\}$ so that 
    \begin{equation}
        \limsup_{\beta\to 0}\sup_{(\nu,\mu)\in \cuS_0(\beta,\tau_0,\delta)} \{\phi_{\tau_1}(\mu,\nu) - I(\mu,\nu)  \}=
        \limsup_{i\to \infty} \{\phi_{\tau_1}(\mu_{\beta_i},\nu_{\beta_i}) - I(\mu_{\beta_i},\nu_{\beta_i})\}.
    \end{equation}
Noting that $\{\cuS_0(\beta_i,\tau_0,\delta)\}_{i\in \N}$ is a decreasing sequence of closed sets, every converging subsequence of $\{(\mu_{\beta_i},\nu_{\beta_i})\}$ converges to a point in the set $\bigcap_{i\in \N}\cuS_0(\beta_i,\tau_0,\delta) = \cuS_0(0,\tau_0,\delta)$. Since $I(\mu,\nu)$ is lower semi-continuous and $\phi_{\tau_1}(\mu,\nu)$ 
is continuous on $\cuM^\up{\beta}$ for $\beta$ sufficiently small, we conclude that $\phi_{\tau_1} - I$ is upper semi-continuous so that
    \begin{align}
         \limsup_{\beta\to 0}\sup_{(\nu,\mu)\in \cuS_0(\beta,\tau_0,\delta)} \phi_{\tau_1}(\mu,\nu) - I(\mu,\nu) &\le 
         \sup_{(\nu,\mu)\in \cuS_0(0,\tau_0,\delta)}\phi_{\tau_1}(\mu,\nu) - I(\mu,\nu).
    \end{align}
A similar argument then shows the same for the limit $\tau_0\to 0$
after observing that the sequence of sets indexed by decreasing $\tau_0$ is indeed decreasing, and similarly for the limit $\delta \to 0$. Combining with Eq.~\eqref{eq:varadhan+blowup} and noting that 
$\cuV = \cuS_0(0, 0,0 )$ where $\cuV$ was defined in the statement of Theorem~\ref{thm:general}, we have
    \begin{align} 
    \label{eq:beta-tau1-gamma}
        \limsup_{\delta\to 0}\limsup_{n\to\infty} &\frac1n \log\left( \E[ Z_n(\cuA,\cuB, \sPi,\bTheta_0)] \one_{\cG_\delta}  \right) 
        \le \sup_{(\mu,\nu)\in \cuV }
        \left\{\phi_{\tau_1}(\nu,\mu) - I(\mu,\nu)\right\}.
    \end{align}

Now let us pass to the $\tau_1 \to 0$ limit.
    First, recalling the definition of $\phi_{\tau_1}$ from Lemma~\ref{lemma:asymp_1}, we note that we can write 
    \begin{equation}
       \phi_{\tau_1}(\mu,\nu) - I(\mu,\nu) = F(\mu,\nu) + \int \log(\lambda \vee \tau_1) \mu_{\star}(\mu,\nu)(\de \lambda)
    \end{equation}
    for some $F(\mu,\nu)$ that is uniformly upper bounded on $\cuM$: indeed, the definition of $\cM$ guarantees that all terms (other than the logarithmic potential) are finite, and $-I(\mu,\nu) \le 0$ by definition.

Choosing a sequences
    $\{(\mu_{i},\nu_{i})\in \cuV\}$ 
    and $\{\tau_{i}\}_{i\in\N}$ with $\tau_{i}\to 0$ satisfying
\begin{equation}
 \mathrm{(I)}:= \limsup_{\tau_1\to 0}\sup_{(\nu,\mu)\in \cuV} \{\phi_{\tau_1}(\mu,\nu) - I(\mu,\nu)  \}
        = \limsup_{i\to \infty} \left\{
        F(\mu_i,\nu_i) + \int \log(\lambda \vee \tau_i) \mu_{\star}(\mu_i,\nu_i)(\de \lambda)
        \right\},
\end{equation}
we have by compactness (after passing to a subsequence and relabeling) that
$(\mu_i,\nu_i)$ converge to $(\mu_0,\nu_0) \in \cuV$ so that $\mu_\star(\mu_i,\nu_i) \to \mu_\star(\mu_0,\nu_0)$ weakly.  
Since $\mu_\star$ is compactly supported for any $\mu,\nu$ so that the positive part of the logarithm is uniformly integrable, and $F(\mu,\nu)$ is uniformly upper bounded on $\cM$, we have by Fatou's Lemma that
\begin{equation}
   \mathrm{(I)}  \le F(\mu_0,\nu_0) + \int \log(\lambda) \mu_\star(\mu_0,\nu_0)(\de \lambda) \le \sup_{(\mu,\nu)\in\cuV }  \left\{\phi_0(\mu,\nu) - I(\mu,\nu)\right\}.
\end{equation}
Finally, by noting that
$\KL(\nu\|\cN(\bzero,\bI_{k+k_0})\times\P_w) = \KL(\nu_{\cdot| w}\|\cN(\bzero,\bI_{k+k_0})) $ for $(\mu,\nu) \in\cuV$, we see that $\phi_0(\mu,\nu) - I(\mu,\nu) = -\Phi_\gen(\mu,\nu)$ for $\Phi_\gen$ as in the statement of Theorem~\ref{thm:general}.




%and assume for contradiction that
%\begin{equation}
%    \mathrm{(I)} >  \sup_{(\mu,\nu) \in\cuV} \left\{\phi_{0}(\mu,\nu) -  I(\mu,\nu)\right\}.
%\end{equation}
%
%    
%    \begin{align}
% \mathrm{(I)}:= \limsup_{\tau_1\to 0}\sup_{(\nu,\mu)\in \cuV} \{\phi_{\tau_1}(\mu,\nu) - I(\mu,\nu)  \}
% &> 
%        \limsup_{i\to \infty} \left\{
%        F(\mu_i,\nu_i) + \int \log(\lambda \vee \tau_i) \mu_{\star}(\mu_i,\nu_i)(\de \lambda)
%        \right\}\\
%        \int\limsup_{i\to \infty} \left\{
%        F(\mu_i,\nu_i) + \log(\lambda \vee \tau_i) \mu_{\star}(\mu_i,\nu_i)(\de \lambda)
%        \right\}.
%    \end{align}
%
%\begin{align}
%    \mathrm{(I)}:=\limsup_{\tau_1\to 0} 
%    \sup_{\cuS} \int \log^{(\tau_1)}(\lambda )\mu_\star(\mu,\nu)(\de\lambda)
%    =& \limsup_{\tau_1\to 0} 
%    \int \big(0\wedge\log^{(\tau_1)}(\lambda ) + \log^{(1)}(\lambda)\big)
%    \mu_\star(\mu_{\tau_1},\nu_{\tau_1})(\de\lambda),
%\end{align}
%
% Let $\{(\mu_{\tau_{1,j}},\nu_{\tau_{1,j}})\}_{j\in\cuJ}$ be a converging subsequence of $\{(\mu_{\tau_{1,j}},\nu_{\tau_{1,j}})\}_{i\in\N}$. Since $\cuS$ is compact, the limit point of any converging subsequence of  $\{(\mu_{\tau_{1,j}},\nu_{\tau_{1,j}})\}$ is a point in $\cuS$. Further since for any $(\mu,\nu)\in\cuS$, $\mu_\star(\mu,\nu)((-\infty,0]=0)$,   for any $\lambda>0$, $\limsup_{\tau_1\to 0} 0\wedge\log^{(\tau_1)}(\lambda ) = 0\wedge\log(\lambda)$, and $\log^{(1)}(\cdot)$ is integrable $\mu_\star(\mu,\nu)$, we can use Fatou's lemma alongside the dominated convergence to get
%\begin{equation}\label{eq:tau0}
%       \mathrm{(I)}\le \sup_{(\mu,\nu)\in\cuS} \int \log(\lambda) \mu_\star(\mu,\nu)(\de\lambda).
%\end{equation} 
% Hence combining the Eq.~\eqref{eq:beta-tau1-gamma} and Eq.~\eqref{eq:tau0} gives the desired result.
 

 
     


