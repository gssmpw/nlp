\section{Proofs for applications}
\subsection{Analysis of multinomial regression: Proof of Proposition~\ref{prop:multinomial}}
   Consider the regularized multinomial regression problem with empirical risk
   \begin{equation}
       \hat R_{n,\lambda}(\bTheta) := \frac1n\sum_{i=1}^n \left\{ \log\left(1 + \sum_{j=1}^k e^{\bx_i^\sT\btheta_j}\right) - \by_i^\sT\bTheta^{\sT}\bx_i \right\}  + \frac{\lambda}{2} \|\bTheta\|_F^2
   \end{equation}
   for $\lambda \ge 0$. We will prove Proposition~\ref{prop:multinomial}
   for $\lambda =0$, but it will be convenient to study the above problem for $\lambda$ near $0$ for additional regularity.

For future reference, we  introduce the notation
%
\begin{align}
\ell_{i}(\bv):=
   \log\Big( \sum_{j=1}^k e^{v_j} + 1\Big)  
   -  \by_{i}^\sT \bv\, .\label{eq:ell_i}
\end{align}
%
We will use the following lemma that is 
an adaptation of Lemmas S6.1 and S6.4 of~\cite{tan2024multinomial}.
Although the parametrization in these lemmas is slightly different,
the proof required here can be derived from the proofs of those lemmas. For the convenience of the reader, we provide it here.
\begin{lemma}\label{lemma:LocalStrongMultinomial}
Under the assumptions of Proposition~\ref{prop:multinomial}, for any constant
$\rho>0$, there exists $c = c(\rho;\bR_{00},\alpha,\rho)>0$ such that
%
\begin{align}
&   \lim_{n\to\infty}\P\Big(\frac1n\sum_{i=1}^n \grad\ell_{i}(\bTheta^\sT\bx_i) 
    \grad\ell_{i}(\bTheta^\sT\bx_i)^\sT \succeq c\bI_k
    \;\;\forall \bTheta:\|\bTheta\|_F^2\le \rho\Big) = 1\,,
    \label{eq:outer_prod_singular_value_lb}\\
&\lim_{n\to\infty}\P\Big(\nabla^2 \hR_{n,0}(\bTheta)\succeq c\bI_{dk}\;\;\forall \bTheta:\|\bTheta\|_F^2\le \rho\Big) = 1\,.
\label{eq:hessian_singular_value_lb_multinomial}
\end{align}
\end{lemma}
\begin{proof}[Proof (Due to~\cite{tan2024multinomial}).]

Define the event
\begin{equation}
\Omega_{1,n} := \left\{ \sigma_{\max}(\bX) \le C_0 \sqrt{n},\quad \sigma_{\min}(\bX)\ge c_0\sqrt{n}\right\}
\end{equation}
for $C_0,c_0$ chosen so that $\Omega_{1,n}$ are high probability sets.

\noindent\textbf{Proof of Eq.~\eqref{eq:hessian_singular_value_lb_multinomial}.}
For a subset $\cI\subseteq [n]$, let $\bX_\cI = (\bx_i^\sT)_{i\in\cI} \in\R^{|\cI| \times d}$.
Now since $n/d_n \rightarrow \alpha$, we can choose a $\beta \in(\alpha^{-1},1)$ so that for some constant $c_0>0$ independent of $n$, we have $\lim_{n\to\infty}\P(\Omega_{3,n}) =1$ for the event
\begin{equation}
  \Omega_{1,n} := \left\{\sigma_{\min}(\bX_{\cI}) > c_0\sqrt{n} \;\; \textrm{for all}\;\; \cI \subseteq [n] \;\;\textrm{with}\;\; |\cI| \ge \beta n \right\}.
\end{equation}
(See for instance Lemma S6.3 of~\cite{tan2024multinomial}. The argument is a standard union bound using lower bound on the singular values of Gaussian matrices.)
For any $D  > 1$, $\|\bTheta\|_F^2 \le \rho$, we have on the event $\Omega_{1,n}$
\begin{equation}
   D \cdot|\{i\in[n] : \|\bTheta^\sT\bx_i\|_F^2 \ge D\}| \le  \sum_{i=1}^n \|\bTheta^\sT\bx_i\|_F^2 \le \|\bX\|_\op^2 \|\bTheta\|_F^2 \le C_0^2 \rho n =: \rho_0 n,
\end{equation}
so that, choosing $D =D_\beta := \rho_0 (1-\beta)^{-1}$,  we have for
\begin{equation}
      |\cI_\beta| \ge \beta n  \quad\textrm{for}\quad \cI_\beta := \{i \in[n] : \|\bTheta^\sT\bx_i\|_F^2 \le D_\beta \}\, .
\end{equation}
Furthermore, we have for each $i\in\cI_\beta$,
\begin{equation}
\label{eq:entry_wise_bound_p_hessian}
    \bp(\bTheta^\sT\bx_i) \in  [c_1,1-c_1]^k
\end{equation}
for some $c_1\in(0,1)$ independent of $n$.

Now we can compute for $\bv\in\R^k$ (note that, with the definition \eqref{eq:ell_i}, $\grad^2 \ell_i(\bv)$ is independent of $\by_i$, so we omit the subscript $i$):
\begin{equation}
    \grad^2 \ell(\bv) = \Diag(\bp(\bv)) - \bp(\bv)\bp(\bv)^\sT,
\end{equation}
Hence, for any $\bu$, letting $\bu^2 = \bu\odot \bu$,
\begin{equation}
  \bu^\sT \grad^2 \ell(\bv)\bu   = \bu^2 \odot \bp(\bv) - (\bu^\sT\bp(\bv))^2
  \stackrel{(a)}{\ge}
 \bu^2 \odot \bp(\bv) - (\bu^2 \odot \bp(\bv))\|\bp(\bv)\|_1  \stackrel{(b)}{=} 
 (\bu^2 \odot \bp(\bv)) (p_0(\bv)),
\end{equation}
where $(a)$ is an application of Cauchy Schwarz, and $(b)$ follows from the identity  $\sum_{j\in[k]}p_j(\bv) + p_0(\bv) = 1.$
This, along with Eq.~\eqref{eq:entry_wise_bound_p_hessian} then implies
that for $i\in \cI_\beta$, 
\begin{equation}
    \lambda_{\min}(\grad^2 \ell(\bTheta^\sT\bx_i))  > c_3
\end{equation}
for some $c_3$ independent of $n$.
Now noting that
\begin{equation}
\grad^2 \hat R_{n,0}(\bTheta) = \frac1n\sum_{i=1}^n \grad^2\ell(\bTheta^\sT\bx_i) \otimes (\bx_i\bx_i^\sT),
\end{equation}
we can bound 
on the high probability event $\Omega_{1,n}\cap\Omega_{2,n}$,
\begin{align}
    \lambda_{\min}(n\grad^2 \hat R_{n,0}(\bTheta)) 
    \ge
    \lambda_{\min}\left(\sum_{i\in\cI_\beta} \grad^2\ell(\bTheta^\sT\bx_i) \otimes (\bx_i\bx_i^\sT)\right)
    \ge 
    c_3 \lambda_{\min}\left(\sum_{i\in\cI_\beta} \bI_k \otimes (\bx_i\bx_i^\sT)\right)
   % &\ge c_3 \lambda_{\min}(\bX_{\cI_\star}^\sT\bX_{\cI_\star})\\
    \ge c_3 c_0^2 n,
\end{align}
showing the claim.


\noindent
\textbf{Proof of Eq.~\eqref{eq:outer_prod_singular_value_lb}.}
By the upper bound on $\|\bTheta_0\|_F$, we can find some $\gamma\in(0,1)$ constant in $n$ so that the event
\begin{align}
\Omega_{3,n} &:= \left\{\sum_{i=1}^n \one_{\{\by_i = \be_{j}\}} \ge \gamma n  \quad\textrm{for all}\quad j \in\{0,\dots,k\} \right\}
\end{align} 
holds with high probability.
Without loss of generality, for what follows, assume that $\gamma n$ is an integer.

We next construct $\gamma n$ subsets of $[n]$ so that, in each subset, there is exactly one index corresponding to a label of each class.  Namely, find disjoint index sets $\{\cI_l\}_{l\in [\gamma n]}$, $\cI_l\subseteq [n]$, so that for each $l\in [\gamma n]$, we have
\begin{equation}
|\cI_l| = k+1, \quad\quad\textrm{and,  for each}\; j\in\{0,\dots,k\},\;\sum_{i\in \cI_l} \one_{\by_i = \be_j} =1.
\end{equation}
%
We claim that,  on the event $\Omega_{1,n}$, for any $\bTheta$ with  $\|\bTheta\|_F^2\le \rho$,
there must exist a $\cL \subseteq [\gamma n]$ such that
\begin{equation}
\label{eq:cL_index_set_bound}
    |\cL| \ge \frac{\gamma n}{2} \quad\textrm{s.t. for all $l\in\cL$}.\quad \sum_{i \in\cI_l} \|\bTheta^\sT\bx_i\|_F^2 \le \frac{ 2\rho_0}{\gamma}\, .
\end{equation}
Indeed, this follows from
\begin{equation}
|\cL^c| \min_{l\in\cL^c} \sum_{i\in\cI_l} \|\bTheta^\sT\bx_i\|_F^2 \le 
\sum_{l\in[\gamma n]} \sum_{i\in\cI_k} \|\bTheta^\sT\bx_i\|_F^2  \le 
\|\bX\|_\op^2 \|\bTheta\|_F^2
\le C_0^2 \rho n.
\end{equation}
%\begin{equation}
%\sum_{l\in\cL} \sum_{i\in\cI_l}\|\bTheta^\sT\bx_i\|_F^2 \le  \sum_{l \in [\gamma n]}  \sum_{i\in\cI_l} \|\bTheta^\sT \bx_i\|_F^2 \le  \|\bX\|_\op^2 \|\bTheta\|_F^2 \le   C_0^2 \rho n =:  \rho_0 n.
%\end{equation}
%%Letting $\cI_\star = \cup_{l\in\cL} \cI_l$ be the union of these indices, we have for each $i\in\cI_\star$ 
%
Therefore, by definition of $\bp$, we have for each $i\in \cup_{l\in\cL} \cI_l$,
\begin{equation}
\label{eq:entry_wise_bounds_p}
    \bp(\bTheta^\sT\bx_i)\in [c_4,1-c_4]^k
\end{equation}
for some $c_4 \in (0,1)$  independent of $n$.

By straightforward computation, we have for each $l\in[\gamma n]$
\begin{equation}
    \sum_{i \in\cI_l} \grad \ell_i(\bTheta^\sT\bx_i) \grad \ell_i(\bTheta^\sT\bx_i)^\sT = \bS^\sT\bB^\sT(\bI_{k+1} - \bP_l)^\sT(\bI_{k+1} - \bP_l)\bB \bS
\end{equation}
where $\bS \in\R^{k\times k}$ is a permutation matrix,
\begin{equation}
    \bP_l = (p_j(\bTheta^\sT\bx_i))_{i\in [\cI_l], j\in\{0,\dots,k\}} \quad\textrm{and}\quad
    \bB = \begin{bmatrix} \bI_k \\
    \bzero^\sT
    \end{bmatrix} \in\R^{(k+1) \times k}.
\end{equation}
The bound in Eq.~\eqref{eq:entry_wise_bounds_p} then implies 
that for each $l\in\cL$, $\bP_l\in\R^{(k+1)\times (k+1)}$, which is a stochastic matrix, has entries that are in $[c_5,1-c_5]$ for some $c_5>0$. Hence, it is a stochastic matrix of an irreducible and aperiodic Markov chain, so that $(\bI_{k+1} - \bP_l)\bu = \bzero$ if and only if $\bu =a\one_{k+1}$ for $a\in\R$. 
Letting $\cP$ be the space of these stochastic matrices whose entries are in $[c_5,1-c_5]$, we have by compactness of this space that 
for some $\bP_\star \in \cP$, 
\begin{align}
    \min_{l\in\cL}\min_{\substack{\bv \in\R^{k}
    \\
    \|\bv\|_2 = 1
    }}\bv^\sT\bB^\sT(\bI_{k+1} - \bP_l)^\sT(\bI_{k+1} - \bP_l)\bB\bv
    &\ge  
    \min_{\substack{\bu\in\R^{k+1}\\ \|\bu\|_2 =1\\
    u_{k+1} = 0
    }}\bu^\sT(\bI_{k+1} -\bP_\star)^\sT(\bI_{k+1} -\bP_\star )\bu \ge c_5
\end{align}
for some $c_6>0$ independent of $n$ (but dependent on $c_1$).
So we can bound
\begin{align}
    \lambda_{\min}\left( \frac1n\sum_{i=1}^n \grad \ell_i(\bTheta^\sT\bx_i)
    \grad \ell_i(\bTheta^\sT\bx_i)^\sT
    \right) 
    \ge 
    \lambda_{\min}\left(\frac1n \sum_{l\in |\cL|} 
    \bB^\sT (\bI_{k+1} - \bP_l)^\sT (\bI_{k+1} - \bP_l) \bB
    \right)
    \ge \frac{|\cL|}n  c_6.
\end{align}
The bound on $|\cL|$ now gives the desired result of Eq.~\eqref{eq:outer_prod_singular_value_lb}.
\end{proof}


%\begin{proof}
%We'll use $\be_{p,j}$ in what follows to denote the $j$th canonical basis vector in $\R^{p}$.
%Define the set
%\begin{equation}
%A_y := \{ \bY \in\R^{n \times k}: \sum_{i=1}^n \one_{\{\by_i = \be_{k,j}\}} \ge \gamma n  \quad\textrm{for all}\quad j \in\{0,\dots,k\} \}.
%\end{equation}
%
%Fix any $\bQ \in\R^{(k+1)\times k}$  satisfying
%\begin{equation}
%    \bQ^\sT\bQ = \bI_k ,\quad\quad \bQ\bQ^\sT = \bI - \frac{1}{k+1}\one\one^\sT.
%\end{equation}
%For $i\in[n]$, define $\tilde\by_i\in\R^{k+1}$ by 
%\begin{equation}
%   \tilde \by_{i}  := \begin{cases}
%   \be_{k+1,j} & \by_i =\be_{k,j},\quad j\in[k]\\
%   \be_{k+1,k+1} & \by_i = \be_0,
%   \end{cases}
%\end{equation}
%i.e., $\tilde\by_i$ is a standard one-hot encoding.
%Define for $\bv \in\R^{k}$,
%\begin{equation}
%    l_i(\bv) :=  \log\left(\sum_{j=1}^{k+1} \exp\{\be_{j}^\sT\bQ \bv\}\right) - \tilde \by_i^\sT \bQ \bv.
%\end{equation}
%
%Lemma S6.1 of~\cite{tan2024multinomial} proves that for all $\by\in A_y$, and any $\bM$ satisfying $\|\bM \bQ^\sT\|_F^2 \le n c_0$, there exsits some constant $c$ depending on $\gamma$ and $c_0$ so that
%\begin{equation}
%   \frac1n \sum_{i=1}^n \bQ \grad l_i(\bM^\sT\be_i) \grad l_i(\bM^\sT\be_i)^\sT  \succeq
%    c(\gamma,c_0) \bI_k.
%\end{equation}
%\end{proof}





\begin{lemma}[Bounded empirical risk minimizer for multinomial regression]
\label{lemma:equivalence_multinomial}
Under the assumptions of Proposition~\ref{prop:multinomial}, 
%Let $\hat R_{n,\lambda}(\bTheta)$ be the risk for multinomial regression with regularization parameter $\lambda \ge 0$.
the following are equivalent:
\begin{enumerate}
 \item There exists $C>0$ independent of $n$  such that, for all $\lambda>0$ 
    \begin{equation}
     \lim_{n\to\infty} \P\left(\|\hat\bTheta_\lambda\|_F < C \right) = 1.
    \end{equation}
    \item There exists $C>0$ independent of $n$ and $\lambda$, such that, 
    \begin{equation}
        \lim_{\lambda \to 0+} \lim_{n\to\infty} \P\left(\|\hat\bTheta_\lambda\|_F < C \right) = 1.
    \end{equation}
\item There exists $C>0$ such that
    \begin{equation}
         \lim_{n\to\infty} \P\left( \hat\bTheta \;\mbox{\rm exists}\;,\|\hbTheta\|_F 
 < C\right) = 1.
    \end{equation}
\end{enumerate}
%
If any of the above holds, then, for any $\delta>0$, we have
 \begin{equation}
        \lim_{\lambda \to 0+} \lim_{n\to\infty} \P\left(\|\hbTheta-\hbTheta_\lambda\|_F < \delta \right) = 1.\label{eq:LambdaPerturbation}
    \end{equation}
On the other hand, if for all $C>0$, we have
    \begin{equation}
    \label{eq:diverging_in_lambda}
        \lim_{\lambda \to 0} \lim_{n\to\infty} \P\left(\|\hat\bTheta_\lambda\|_F > C\right) = 1,
    \end{equation}
    then for all $C>0,$
    \begin{equation}
    \label{eq:any_seq_minimzers_diverges}
         \lim_{n\to\infty} \P\left( \hat\bTheta \;\mbox{\rm exists}\;,\|\hbTheta\|_F 
 < C\right) = 0.
    \end{equation}
\end{lemma}
\begin{proof}
We will show $\textit{1}\Rightarrow \textit{2}\Rightarrow \textit{3} \Rightarrow \textit{1}$. 

\vspace{0.15cm}

\noindent $\textit{1}\Rightarrow \textit{2}$. This is obvious.

\vspace{0.15cm}

\noindent $\textit{3}\Rightarrow \textit{1}$. Define $r_n(\lambda) := \inf_{\bTheta}\hR_{n,\lambda}(\bTheta)$.
This is a non-decreasing non-negative concave function of $\lambda$. By the envelope theorem, for any
$0\le \lambda_1<\lambda_2$, we have 
%
\begin{align}
\frac{1}{2}\|\hbTheta_{\lambda_1}\|^2_F \ge \frac{r(\lambda_2)-r(\lambda_1)}{\lambda_2-\lambda_1} \ge \frac{1}{2}\|\hbTheta_{\lambda_2}\|^2_F\, ,
\end{align}
%
where, for $\lambda=0$, $\|\hbTheta_{0}\|_F$ is the norm of any minimizer when this exists.
It follows in particular that $\|\hbTheta_{\lambda}\|_F\le \|\hbTheta\|_F$ for any $\lambda>0$, and therefore the claim follows. 

\vspace{0.15cm}

\noindent $\textit{2}\Rightarrow \textit{3}$. Fix $C$ as in point \textit{2}, $\delta_0>0$ and we chose $\lambda_0>0$
such that
$\lim_{n\to\infty}\P(\|\hbTheta_{\lambda}\|_F<C)\ge 1-\delta_0$ for all $\lambda\in(0,\lambda_0)$. Let $c_0 = c_0(2C)>0$
be given as per Lemma \ref{lemma:LocalStrongMultinomial}. Hence, with probability $1-\delta_0-o_n(1)$,
%
\begin{align}
\|\bTheta\|_F\le 2C \;\;\Rightarrow\;\;
\hR_{n,0}(\bTheta)&\ge \hR_{n,0}(\hbTheta_{\lambda}) +\< \hR_{n,0}(\hbTheta_{\lambda}),\bTheta-\hbTheta_{\lambda}\>
+\frac{c_0}{2}\|\bTheta-\hbTheta_{\lambda}\|_F^2\\
&\ge \hR_{n,0}(\hbTheta_{\lambda}) -\lambda\< \hbTheta_{\lambda},\bTheta-\hbTheta_{\lambda}\>
+\frac{c_0}{2}\|\bTheta-\hbTheta_{\lambda}\|_F^2\, .
\end{align}
%
Recalling that $\|\hbTheta_{\lambda}\|_F<C$, this implies
%
\begin{align}
\frac{2\lambda C}{c_0}<C \;\;\Rightarrow \;\; \|\bTheta-\hbTheta_{\lambda}\|_F\le \frac{2\lambda}{c_0}\|\hbTheta_{\lambda}\|_F\le \frac{2\lambda C}{c_0}\, .
\end{align}
%
The first condition can be satisfied by eventually decreasing $\lambda_0$. We thus conclude that, for each 
$\delta_0>0$, there exists $\lambda_0>0$ such that 
%
\begin{align}
\lim_{n\to\infty}\P\Big(\|\hbTheta\|_F\le 2C, \|\hbTheta_{\lambda}-\hbTheta\|_F\le \frac{2\lambda C}{c_0}\Big)\ge 1-\delta_0\, .
\end{align}
%
The claim \textit{3} follows by dropping the second inequality
 $\|\hbTheta_{\lambda}-\hbTheta\|_F\le 2\lambda C/c_0$ and noting that $\delta_0$ is arbitrary.

Equation \eqref{eq:LambdaPerturbation} follows by dropping the 
inequality $\|\hbTheta\|_F\le 2C$ in the last display.
 
\vspace{0.15cm}

Finally Eq.~\eqref{eq:diverging_in_lambda} implies Eq.~\eqref{eq:any_seq_minimzers_diverges} ~ by
the monotonicity of $\|\hbTheta_{\lambda}\|_F$ in $\lambda$ proven above.
\end{proof}
%
%We will show $\textit{1}\Rightarrow \textit{2}\Rightarrow \textit{3} \Rightarrow \textit{1}$. 
%
%\am{Modified the proof of first implication}
%
%Let $\Omega_{n,1}(\lambda,C)$ be the event $\{\|\hat\bTheta_\lambda\|_F < C\}.$ 
%The stationarity conditions of $\bTheta \mapsto \hat R_{n,\lambda}(\bTheta) $ at $\hat\bTheta_\lambda$
%are given by $\grad \hat R_{n,0}(\hat\bTheta_\lambda) = - \lambda \hat\bTheta_\lambda,$
%so on $\Omega_{n,1}$,
%%$\|\grad \hat R_{n,0}(\hat\bTheta_\lambda)\|_2 \le \lambda C$,
%so we have for any $\eps >0$ and any  $C>0$,
%\begin{equation}
%\|\grad \hat R_{n,0}(\hat\bTheta_\lambda)\|_2 \le \lambda C\, .
%\end{equation}

%Define the event
%
%\begin{align}
%
%\Omega_{n,0}(C):= \Big\{\grad^2\hat R_{n,0}(\bTheta) \succeq c_0\bI,\;\;\;
%\forall\bTheta:\; \|\bTheta\|_F\le C \Big\}\, .
%
%\end{align}
%
%By an argument similar to Lemma S6.2 of~\cite{tan2024multinomial}, for
%any $C>0$ there exists some $c_0>0$ independent of $n$
%such that $\P(\Omega_{n,0})\to 1$ as $n\to\infty$.

%Letting $C$ be such that $\lim_{\lambda\to 0}\lim_{n\to\infty}\P(\Omega_{n,1}(\lambda,C))=1$. On  $\Omega_{n,0}%(2C)\cap\Omega_{n,1}(\lambda,C)$. So on the intersection of these events, for any $\|\tilde\bTheta\|_F\le 2C$
%\begin{equation} 
%   \hat R_{n,0}(\tilde\bTheta) \ge
%   \hat R_{n,0}(\hat\bTheta_\lambda)  + 
%   \langle \grad \hat R_{n,0}(\hat\bTheta_{\lambda}),(\tilde\bTheta - \hat\bTheta_\lambda) \rangle  + \frac{c_0}{4} %\|\tilde\bTheta - \hat\bTheta_\lambda\|_F\cdot\Big(\|\tilde\bTheta - \hat\bTheta_\lambda\|_F\wedge C\Big),
%\end{equation}
%so that minimizing over $\tilde\bTheta$ on both sides gives
%\begin{align*}
%\|\grad \hat R_{n,0}(\hat\bTheta_\lambda)\|_F\le \frac{c_0C}{2} \;\;\Rightarrow\;\;
%   \min_{\tilde\bTheta}\hat R_{n,0}(\tilde\bTheta) &\ge 
%  \hat R_{n,0}(\hat\bTheta_\lambda)   - \frac{1}{c_0}\|\grad \hat R_{n,0}(\hat\bTheta_\lambda)\|_F^2\\
%  &\ge \hat R_{n,0}(\hat\bTheta_\lambda)-\frac{C^2}{c_0}\lambda^2.
%\end{align*}
%
%Since $\hat R_{n,0}(\hat\bTheta_\lambda) >c $ for some $c$ on $\Omega_{n,1}(\lambda,C)$, \am{Why???} taking  $n\to\infty$ followed by $\lambda \to0$ gives the claim of $\textit{2},$
%proving $\textit{1}\Rightarrow\textit{2}$.
%
%
%To show the implication $\textit{2}\Rightarrow \textit{3},$ note that when $\hat\bTheta$ diverges, $\hat R_{n,0}%(\hat\bTheta) \to 0$ giving the contrapositive of the desired statement. \am{The opposite of the conclusion is not that 
%$\hbTheta$ `diverges'.}
%
%Finally, to prove $\textit{3}\Rightarrow \textit{1}$, extend the above definition by letting
%$\Omega_{n,1}(0,C):=\{\hbTheta \mbox{ exists } \|\hbTheta\|_F<C\}$. 
%On $\Omega_{n,0}(2C)\cap \Omega_{n,1}(0,C)$, we have
%\begin{equation}
%\hat R_{n,0}(\hat\bTheta_{\lambda})  \ge  \hat R_{n,0}(\hat\bTheta)
%+\frac{c_0}{4} \|\tilde\bTheta - \hat\bTheta_\lambda\|_F\cdot\Big(\|\tilde\bTheta - \hat\bTheta_\lambda\|_F\wedge C\Big)
%\, .
%\end{equation}
%On the other hand
%\begin{align}
%\hat R_{n,0}(\hat\bTheta_{\lambda})&\le \hat R_{n,0}(\hat\bTheta) +
%\frac{\lambda}{2}\Big(\|\hbTheta\|_F^2-\|\hbTheta_{\lambda}\|_F^2\Big)\\
%&\le \hat R_{n,0}(\hat\bTheta) + \frac{\lambda C^2}{2}\, .
%\end{align}
%
%Using together the last two displays, we get 
%\begin{equation}
%\frac{c_0}{4} \|\tilde\bTheta - \hat\bTheta_\lambda\|_F\cdot\Big(\|\tilde\bTheta - \hat\bTheta_\lambda\|_F\wedge C\Big)\le \frac{\lambda C^2}{2}
%\end{equation}
%
%Once again by Lemma S6.2 of~\cite{tan2024multinomial}, there exists a high probability event $\Omega_{n,4}$ so that $\grad^2 \hat R_{n,0}(\hat\bTheta) \succ c_1 \bI$ for $c_1$ independent of $n$ (and $\lambda)$ on $\Omega_{3,n}\cap\Omega_{4,n}$, and so on the intersection of these events along with the intersection of the high probability event $\Omega_{5,n}$ where $\bTheta \mapsto \hat R_{n,0}(\bTheta)$ is Lipschitz with constant $C_1 >0$ independent of $n$ (and $\lambda$), we have
%\begin{equation}
%    \|\hat\bTheta_{\lambda} - \hat\bTheta\|_F^2 \le \frac{2}{c_1} \left(\hat R_{n,0}(\hat\bTheta_{\lambda})  - \hat R_{n,0}(\hat\bTheta)\right) \le
%     \frac{2 C_1}{c_1}\|\hat\bTheta_\lambda - \hat\bTheta\|_F.
%\end{equation}
%So if $\hat\bTheta$ is bounded by a constant independent of $n$ and $\lambda$, then so is $\hat\bTheta_\lambda$, proving the final implication.

%Now, by inspection of the argument for the implication \textit{3.}$\Rightarrow$\textit{1.}, we see that if instead of assuming that $\Omega_{3,n}$ is a high probability event, we assume that $\P(\Omega_{3,n}) > 0$, then we conclude that there exists $C>0$ so that  $\lim_{\lambda \to 0}\lim_{n\to\infty} \P(\|\hat\bTheta_{\lambda}\|_F < C) > 0$. 
%Taking the contrapositive of this statement proves that Eq.~\eqref{eq:diverging_in_lambda} implies ~Eq.~\eqref{eq:any_seq_minimzers_diverges}.

%\end{proof}

\begin{lemma}
\label{lemma:multinomial_regularized}
Consider the setting of Proposition~\ref{prop:multinomial}.
For any $\lambda >0$, the equations 
\begin{align}
\label{eq:multinomial_FP_regularized}
   \alpha \E\left[(\bp(\bv) - \by)(\bp(\bv) - \by)^\sT\right]  &= \bS^{-1} (\bR/\bR_{00}) \bS^{-1}\\
  \E[(\bp(\bv) - \by) (\bv^\sT,\bg_0^\sT)]  &= \lambda (\bR_{11},\bR_{10})
\end{align}
have a unique solution $(\bR_{\opt}(\lambda),\bS_{\opt}(\lambda))$.

Furthermore, letting $\hat\bTheta_\lambda$ be the unique minimizer of $\hat R_{n,\lambda}$, and $(\mu^\opt(\lambda),\nu^\opt(\lambda))$ by defined in terms of $\bR^\opt(\lambda),\bS^\opt(\lambda)$ via Definition~\ref{def:opt_FP_conds},
then points $\textit{1.-3.}$ of Theorem~\ref{thm:global_min} hold.
\end{lemma}
\begin{proof}
Uniqueness of the solution follows from Theorem \ref{prop:simple_critical_point_variational_formula}
(point \textit{3.(a)} holds because $\lambda>0$).

In order to prove that the conclusions of Theorem~\ref{thm:global_min} holds, we
will apply that theorem to a modification of the problem under consideration.
Namely, we will perform a smoothing of the labels $\by_i$, and check that the assumption holds.

For $\eps>0$, let $\by_{i,\eps}\in\R^k$ be a smoothing of $\by_i$ so that it has entries in $[0,1]$, 
$\by_{i,\eps} := \boldf_{\eps}(\bTheta_0^{\sT}\bx_i, w_i)$ for some $\boldf_\eps$ that 
is a $C^2$, Lipschitz function of $\bTheta_0^{\sT}\bx_i$ for all fixed $\eps>0$, where $w_i$ is a uniform random variables; and satisfies 
$\P\left(\by_i \neq \by_{i,\eps}| \bTheta_0^\sT\bx_i\right) \le \eps$ (such a smoothing can be constructed, for instance, by writing $\by_i$ as a function of a uniformly distributed random variable and indicators, then smoothing the indicator appropriately).
Define the smoothed regularized MLE, for $\lambda>0$,
\begin{equation}
     \hat\bTheta_{n,\eps,\lambda} := 
     \argmin_{\bTheta\in\R^{d\times k}}  \hat R_{n,\eps,\lambda}(\bTheta),
    \quad
\hat R_{n,\eps,\lambda}(\bTheta) :=
     \frac1n \sum_{i=1}^n
    \ell_{i,\eps}(\bTheta^\sT\bx_i) + \frac{\lambda}{2} \|\bTheta\|_F^2,
\end{equation}
where
\begin{equation}
    \ell_{i,\eps}(\bTheta^\sT\bx_i):=
   \log\Big( \sum_{j=1}^k e^{\bx_i^\sT\btheta_j} + 1\Big)  
   -  \by_{i,\eps}^\sT \bTheta \bx_i
  \, .
\end{equation}
%
Note that $\ell_{i,\eps}(\bTheta^\sT\bx_i) = 
L(\bTheta^\sT\bx_i,\boldf_{\eps}(\bTheta_0^\sT\bx_i,w_i))$  depends on $\bTheta_0^\sT\bx_i,w_i$ through the labeling function $\boldf_{\eps}$. 
To avoid clutter, we use the notation $\ell_{i,\eps}(\bTheta^\sT\bx_i)$ instead of $\ell(\bTheta^\sT\bx_i,\bTheta_0^\sT\bx_i,w_i)$ which is used in other sections.


We check the conditions of Theorem~\ref{thm:global_min} for this setting.
Assumptions~\ref{ass:regime} and~\ref{ass:theta_0} are given.
It's easy to check that Assumptions~\ref{ass:loss},~\ref{ass:Data},~\ref{ass:convexity} and~\ref{ass:noise} hold.

We next show that the minimizer $\hat\bTheta_{n,\eps,\lambda}$ is, with high probability, in the set of critical points $\cE(\bTheta_0,\bw)$ defined in Theorem~\ref{thm:global_min} 
for which our theory applies.
For this, we will need to show that $\sigma_{\max}(\hat\bTheta_{\eps,\lambda}, \bTheta_0) \le C,$  for some $C$ independent of $n$,
and that the Hessian  along with the gradient outer product are 
lower bounded
at $\hat\bTheta_{\eps,\lambda}$ by some $c>0$ independent of $n$.
In what follows, let 
\begin{equation}
    \Omega_{1,n} := \{   C_0\sqrt{n} \ge \sigma_{\max}(\bX) \ge \sigma_{\min}(\bX)\ge \sqrt{n} c_0 \}
\end{equation}
for some $C_0,c_0 >0$ chosen so that $\Omega_{1,n}$ is a high probability event.

\paragraph{Upper bound on $\bR(\hmu_{\sqrt{d}[\hat\bTheta_{\eps,\lambda},\bTheta_0]})$.}
For any $\lambda >0$, it is easy to see $\|\hat\bTheta_{\eps,\lambda}\|_F \le C_0/\lambda$ for some $C_0$ independent of $n,\eps$, since the multinomial loss is lower bounded by zero.
This along with the assumption that $\bR_{00}=\lim_{n\to\infty}\bTheta_0^{\sT}\bTheta_0$
is finite  implies  $
\bR(\hmu_{\sqrt{d}[\hat\bTheta_{\eps,\lambda},\bTheta_0]})\prec C\bI$ for all fixed $\lambda >0$.

\paragraph{Lower bound on the Hessian $\grad^2_{\bTheta}\hat R_{n,\eps,\lambda}(\bTheta)$.} Clearly, since 
$\bTheta\mapsto \ell_{i,\eps}(\bTheta)$ is convex, we have for any $\lambda >0$, $\grad^2_{\bTheta}\hat R_{n,\eps,\lambda}(\bTheta) \succeq \lambda/2\bI.$ 

\paragraph{Lower bound on the gradient outer product $\E_{\hnu}[\nabla\ell\nabla\ell^{\sT}]$.}

Let
\begin{equation}
    \bA_i := 
    \grad\ell_{i}(\hat\bTheta_\lambda^\sT\bx_i)
    \grad\ell_{i}(\hat\bTheta_\lambda^\sT\bx_i)^\sT,
    \quad\quad
    \bA_{i,\eps} := 
    \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)
    \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)^\sT
\end{equation}

By Lemma \ref{lemma:LocalStrongMultinomial}, using the definition of $\ell_i$ therein,
 for any fixed $\lambda >0$ there exists $c_1(\lambda)$ independent of $n$
 such that, with high probability
\begin{equation}\label{eq:nablaell_1}
    \frac1n\sum_{i=1}^n \bA_i \succ c_1(\lambda) \bI\, .
\end{equation}
We'll show that the smallest singular value of $n^{-1}\sum_i \bA_i$ is sufficiently close to that of $n^{-1}\sum_{i}\bA_{i,\eps}$.

First, on $\Omega_{1,n}$, we have by strong convexity for $\lambda >0$, 
\begin{equation}
\label{eq:min_lipschitz_in_y}
    \|\hat\bTheta_{\eps,\lambda} - \hat\bTheta_{\lambda}\|_F \le \frac{C}{ \lambda \sqrt{n}} \|\bY_{\eps} - \bY\|_F
\end{equation}
where $\bY,\bY_{\eps} \in\R^{n\times k}$ are the matrices whose rows are the labels $\by_i^\sT,\by_{i,\eps}^\sT$, respectively, for $i\in[n]$.


Meanwhile $\|\grad\ell_{i}(\hat\bTheta_{\lambda}^\sT\bx_i)\|_2,\|\grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)\|_2   \le C$ for some $C > 0$ independent of $n$.
Further using the  Lipschitz continuity of the minimizer in the labels, we have, on $\Omega_{1,n}$,
\begin{align}
\label{eq:grad_diff_to_y_diff}
 \|\grad\ell_{i}(\hat\bTheta_{\lambda}^\sT\bx_i) - \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)\|_2  
 &\le \|\bp(\hat\bTheta_{\eps,\lambda}^\sT \bx_i) - \bp(\hat\bTheta_{\lambda}^\sT\bx_i)\|_2 + 
 \|\by_{i} - \by_{\eps,i}\|_2\\
 &\le C 
 \|(\hat\bTheta_{\eps,\lambda}- \hat\bTheta_{\lambda})^\sT\bx_i\|_2 +  
 \|\by_{i} - \by_{\eps,i}\|_2.
 %&\le \left(\frac{C}{\lambda} + 1\right) \|\by_{i} - \by_{\eps,i}\|_2\, .
\end{align}


Now note that we have for all $i,j\in[n]$,
\begin{align}
\nonumber
\Tr\left(\bA_i(\bA_j - \bA_{j,\eps}))\right)
&= 
\left(\grad\ell_{j}(\hat\bTheta_\lambda^\sT\bx_i)^\sT
\grad\ell_{i}(\hat\bTheta_\lambda^\sT\bx_i)
\right)^2 -
\left(\grad\ell_{i}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)^\sT
\grad\ell_{j}(\hat\bTheta_\lambda^\sT\bx_j)
\right)^2\\
\label{eq:outer_prod_to_grad_diff}
&\quad\le
C\|\grad\ell_{i}(\hat\bTheta_\lambda^\sT\bx_i) 
-\grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)
\|_2
\end{align}
where in the inequality we used Cauchy Schwarz and that $\|\grad \ell_{i,\eps}\|_2,\|\grad \ell_{i}\|_2$ are uniformly bounded by a constant $C>0$ independent of $n$.
A similar bound clearly holds for $\Tr(\bA_{i,\eps}(\bA_j - \bA_{j,\eps})).$
So we can bound
\begin{align}
\label{eq:grad_outer_product_diff}
    \left\|\frac1n\sum_{i=1}^n  
    \bA_i -
    \frac1n\sum_{i=1}^n  \bA_{i,\eps} \right\|_F^2
    &=
    \frac1{n^2} \sum_{i,j\in[n]} \Tr\left((\bA_i-\bA_{i,\eps})(\bA_j-\bA_{j,\eps})^\sT \right)\\
    &\le
    \frac1{n^2} \sum_{i,j\in[n]}\left\{ |\Tr\left(\bA_i(\bA_j - \bA_{j,\eps}))\right)| + |\Tr\left(\bA_{i,\eps}(\bA_j - \bA_{j,\eps})\right)|
    \right\}\\
    &\stackrel{(a)}\le 
    \frac{C}{n}  \sum_{i=1}^n
    \|\grad\ell_{i}(\hat\bTheta_\lambda^\sT\bx_i) 
-\grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)
\|_2\\
&\stackrel{(b)}{\le}
\frac{C}{n}\sum_{i=1}^n \| (\hat\bTheta_{\eps,\lambda} - \hat\bTheta_{\lambda})^\sT\bx_i \|_2 +\|\by_i - \by_{\eps,i}\|_2\\
&\stackrel{(c)}{\le} \frac{C}{\sqrt{n}} \left( \| \bX (\hat\bTheta_{\eps,\lambda} - \hat\bTheta_{\lambda})\|_F + \| \bY - \bY_{\eps}\|_F\right)\\
& \stackrel{(d)}{\le} \frac{C}{\sqrt{n}}\left(\frac{\|\bX\|_\op}{\sqrt{n}\lambda} + 1\right) \|\bY - \bY_{\eps}\|_F,
\end{align}
where in $(a)$ we used Eq.~\eqref{eq:outer_prod_to_grad_diff}, in $(b)$ we used
Eq.~\eqref{eq:grad_diff_to_y_diff}, in $(c)$ we used $\|\bv\|_1 \le \sqrt{n}\|\bv\|_2$ for $\bv\in\R^{n}$, and in $(d)$ we used Eq.~\eqref{eq:min_lipschitz_in_y}.
So on the high probability events $\Omega_{1,n}\cap\Omega_{2,n}$,
\begin{equation}\label{eq:nablaell_2}
    \frac1n\sum_{i=1}^n \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i) 
    \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)^\sT  \succeq
    \frac1n\sum_{i=1}^n \grad\ell_{i}(\hat\bTheta_{\lambda}^\sT\bx_i)
    \grad\ell_{i}(\hat\bTheta_{\lambda}^\sT\bx_i)^\sT -  \left(\frac{C}{\sqrt{n}}(\lambda^{-1} + 1) \| \bY - \bY_{\eps}\|_F\right)^{1/2} \bI_k.
\end{equation}
%
By construction of the smoothing, we have
    $\E[\|\by_i - \by_{i,\eps}\|_2^2 | \bTheta_0^\sT\bx_i] \le C \eps$
and $\|\by_i - \by_{i,\eps}\|_2^2 < C$ almost surely, for some $C >0$ independent of $n$. So by Hoeffding's inequality, for any $\delta> 0$ we can choose $\eps = \eps(\delta,\lambda)>0$
such that
\begin{equation}
    \lim_{n\to\infty }\P\left(\frac1{\sqrt{n}} \|\bY - \bY_\eps\|_F > \delta\right) =0\, .
\end{equation}
%
By choosing $\delta$ sufficiently small and using Eqs.~\eqref{eq:nablaell_1} 
\eqref{eq:nablaell_2}, we conclude that, with high probability,
\begin{equation}
    \frac1n\sum_{i=1}^n \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i) 
    \grad\ell_{i,\eps}(\hat\bTheta_{\eps,\lambda}^\sT\bx_i)^\sT
    \succeq \frac{c_1(\lambda)}{2}\bI\, .
%    \frac1n\sum_{i=1}^n \grad\ell_{i} 
%    \grad\ell_{i}^\sT -  \left(\frac{4 C_1^3 k^3}{\sqrt{n}} \| \bY - \bY_{\eps}\|_F\right)^{1/2} \bI_k.
\end{equation}
This shows that for all $\lambda,\eps >0$, with high probability  the minimizer $\hat\bTheta_{\eps,\lambda} \in \cE(\bTheta_0)$ for all $\eps >0$, i.e.,
%, when
%$\bw$ is in the high probability set
\begin{equation}
\lim_{n\to\infty}\P\big(\hat\bTheta_{\eps,\lambda} \in \cE(\bTheta_0) \big) = 1\,.
\end{equation}
Hence we have shown that the conditions of Theorem~\ref{thm:global_min} are satisfied.
This allows us to conclude the statement of the lemma when $\by$ is replaced with $\by_{\eps}$ in equations~\eqref{eq:multinomial_FP_regularized}, and $\hat\bTheta_{\lambda}$ is replaced by $\hat\bTheta_{\eps,\lambda}$ for $\eps >0$ sufficiently small.
Continuity of these equations in $\eps$ along with the consequence of strong convexity~\eqref{eq:min_lipschitz_in_y} allows us to then pass to the limit $\eps\to 0$ giving the desired claim.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:multinomial}]
\noindent{\emph{Claim \textit{1(a)}.}}
Since the system~\eqref{eq:FP_multinomial} has a (finite) solution $(\bR^\opt,\bS^\opt)$,
by Theorem~\ref{thm:simple_critical_point_variational_formula}, this solution is unique implying the claim. 

\noindent{\emph{Claim \textit{1(b)}.}}
Let $(\bR^\opt(\lambda),\bS^\opt(\lambda))$ the unique solution for $\lambda>0$,
which corresponds to the unique minimizer of $F(\bK,\bM)$ defined in 
Eq.~\eqref{eq:FKM_Def}, by Theorem \ref{prop:simple_critical_point_variational_formula}. Since $F(\,\cdot\, )$ depends continuously on $\lambda$
and has a unique minimizer for $\lambda=0$ (by the previous point), it follows that $(\bR^\opt(\lambda),\bS^\opt(\lambda)) \rightarrow (\bR^\opt,\bS^\opt)$ as $\lambda\to 0$.
In particular, there exists $C>0$ independent of $n$ and $\lambda$, such that,
for all $\lambda>0$ smalle enough
    \begin{equation}
         \lim_{n\to\infty} \P\left(\|\hat\bTheta_\lambda\|_F < C \right) = 1.\label{eq:LastBDD}
    \end{equation}
The equivalence of Lemma~\ref{lemma:equivalence_multinomial} then 
implies claim~\textit{(b)}.

\noindent{\emph{Claims \textit{1(c)}, \textit{1(d)}.}}
Since Eq.~\eqref{eq:LastBDD} implies Eq.~\eqref{eq:LambdaPerturbation}, 
and $\lim_{\lambda\to 0}(\bR^\opt(\lambda),\bS^\opt(\lambda)) = (\bR^\opt,\bS^\opt)$,
statements $(c)$ and $(d)$ follow from Lemma \ref{lemma:multinomial_regularized}.


\noindent{\emph{Claims \textit{2}.}}
If  the system~\eqref{eq:FP_multinomial} does not have a (finite) solution,
then we claim that  $\lim_{\lambda\to 0}\Tr(\bR^\opt(\lambda))=\infty$.
Indeed, if it by contradiction $\lim\inf_{\lambda\to 0}\Tr(\bR^\opt(\lambda))<\infty$,
then we can find a sequence $\bR^{\opt}(\lambda_i)$, $i\in \N$ 
with $\lambda_i\to 0$ and $\bR^{\opt}(\lambda_i)$ converging to a finite
limit  $\bR^{\opt}$ (recall that $\Tr(\bR)\le C$ is a compact subset of $\bR\succeq \bzero$), and this would be a solution of the system \eqref{eq:FP_multinomial}
with $\lambda=0$, leading to a contradiction.

Since  $\Tr(\bR^\opt(\lambda))$  is unbounded as $\lambda\to 0$, by Lemma~\ref{lemma:multinomial_regularized} there exists a sequence $\lambda_i\to 0$ 
such that for any $C>0$,
\begin{equation}
   \lim_{i\to \infty} \lim_{n\to\infty} \P(\|\hat\bTheta_{\lambda_i}\|_F > C)  = 1.
\end{equation}
Applying Lemma~\ref{lemma:equivalence_multinomial}, 
we thus conclude that Eq.~\eqref{eq:any_seq_minimzers_diverges}.
\end{proof}



%\begin{proof}[Proof of Proposition~\ref{prop:multinomial}]
%\textbf{Bounded MLE exists:}
%First assume that there exists some $C>0$ such that
%\begin{equation}
%    \lim_{n\to\infty}\P\left(\hat\bTheta \textrm{ exists},\|\hat\bTheta\| < C\right) = 1.
%\end{equation}
%Then by Lemma S6.2 of~\cite{tan2024multinomial}, w.h.p., the hessian at the MLE is lower bounded by a constant independent of $n$, implying that for any $\eps>0$,
%\begin{equation}
%    \lim_{\lambda \to 0} \limsup_{n}\P\left(\|\hat\bTheta_{n,\lambda} - \hat\bTheta_n\|>\eps \right)= 0.
%\end{equation}
%By continuity of the fixed point equations in $\lambda$, we conclude that there must exist some $\bR_\opt$ given as the $\lambda \to 0$ limit of $\bR_{\opt}(\lambda)$ that solves these equations at $\lambda = 0$.
%By Lemma~\notate{ref}, this solution must then be unique by strict convexity
%
%\textbf{No bounded MLE:}
%Conversely, assume that for any $C>0$,
%\begin{equation}
%    \lim_{n\to\infty}\P\left(\|\hat\bTheta\| < C\right) < 1.
%\end{equation}
%Then by Lemma~\notate{ref}, we have that for any $C  > 0$,
%\begin{equation}
%    \lim_{\lambda \to 0} \lim_{n\to\infty} \P\left( \|\hat\bTheta_{\lambda}\|_F < C \right) < 1.
%\end{equation}
%So for any $C >0,$ by Lemma~\ref{lemma:multinomial_regularized},
%\begin{align}
%   \one_{\{\|\bR_\opt(\lambda)\| < C/2 \}}&\le \lim_{n\to\infty} \P\left( \|\bR_\opt(\lambda)\| < C/2 ,\; \|\bR_\opt(\lambda) - \bR(\hat \bTheta_\lambda)\| < C/2\right)\\
%   &\le \lim_{n\to\infty}  \P\left( \|\bR(\hat\bTheta_\lambda)\| < C\right)\\
%   &\le \lim_{n\to\infty}  \P\left( \|\hat\bTheta_\lambda\| < C\right).
%\end{align}
%Taking $\lambda \to 0$ shows that
%$\lim_{\lambda \to 0} \bR_\opt(\lambda) = \infty$, implying that fixedpoint equations are not satisfied for any finite $\bR$ at $\lambda =0$.

%\end{proof}


%\subsubsection{AMP}
%Define the GAMP recursion
%\begin{align}
%    \hat\bTheta^{t+1} =& 
%    \alpha\bX^\sT\left(\bY - 
%    \bp(\Prox(\bU^{t} + \bY\bS_t; \bS_t))\right) \bS_{t+1}
%    + \hat\bTheta^t \bS_t^{-1}\bS_{t+1}\in\R^{d\times k},\\
%    \bU^{t+1} = & \bX \hat \bTheta^{t+1}
%    -\left( \bY - \bp(\Prox(\bU^{t} + \bY\bS_t;\bS_t))\right)\bS_{t+1}\in \R^{n\times k},
%\end{align}
%where the $\bp$ and $\Prox$ are applied row-wise, with the state evaluation iteration given by \kas{$St+1 -> St s_ts_t+1^{-1} =$}
%\begin{align}
%    \alpha \; 
%    \E[\left(\bp(\bv_t)- \by \right)
%    \left(\bp(\bv_t) - \by\right)^\sT ]   &=  \bS_t^{-1}(\bR_{t+1}/ \bR_{00})\bS_{t+1}^{-1},\\
%   \bR_{01,t} - \alpha\bS_{t+1}\E[   (\bp(\bv_t)- \by )\bg_0^\sT  ] &= \bR_{01,t+1}  ,\\
%    \bS_t(\bI_k - \E[(\bI_k + \bS_t \bJ \bp(\bv_k))^{-1}])^{-1}  &= \alpha\bS_{t+1},
%\end{align}
%for $\bv_t = \Prox(\bg_t + \bS_t\by)$, where 
%\begin{equation}
%(\bg_t^\sT,\bg_0^\sT)\sim \cN(\bzero, \bR^t),
%\qquad\bR^t = 
%    \begin{bmatrix}
%        \bR_{11,t}&\bR_{01,t}\\
%        \bR_{01,t}^\sT& \bR_{00}
%    \end{bmatrix}.
%\end{equation}