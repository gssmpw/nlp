%%%%%%Arxiv submission format:
\documentclass[11pt]{article}
\usepackage{ifthen}
\usepackage{tabulary}
\newboolean{arxiv}
\setboolean{arxiv}{true}



%%%%%%Colt submission format:
%\documentclass[anon,12pt]{colt2022} % Anonymized submission
%\documentclass[final,12pt]{colt2022} % Include author names
%\newboolean{arxiv}
%\setboolean{arxiv}{false}



\usepackage{ifthen}

%%%%Import packages according to submission
\ifthenelse{\boolean{arxiv}}{
\usepackage{arxiv_def}
}
{
\usepackage{times}
\usepackage{colt_def}
}


\usepackage[toc,page]{appendix}
\usepackage{xr}
\usepackage{mathtools}
\usepackage{yfonts}  
\usepackage{subcaption} % loads the caption package




%\usepackage{tgheros}
\DeclareSymbolFont{Greekletters}{OT1}{iwona}{m}{n}
\DeclareSymbolFont{greekletters}{OML}{iwona}{m}{it}
\DeclareMathSymbol{\salpha}{\mathord}{greekletters}{"0B}
\DeclareMathSymbol{\sbeta}{\mathord}{greekletters}{"0C}
\DeclareMathSymbol{\sgamma}{\mathord}{greekletters}{"0D}
\DeclareMathSymbol{\sOmega}{\mathord}{Greekletters}{"0A}
\DeclareMathSymbol{\smu}{\mathord}{greekletters}{"16}
\DeclareMathSymbol{\svarepsilon}{\mathord}{greekletters}{"22}
\DeclareMathSymbol{\svarrho}{\mathord}{greekletters}{"25}
\DeclareMathSymbol{\svarphi}{\mathord}{greekletters}{"27}

%\definecolor{cc}{RGB}{1,11,111}

\newcommand{\am}[1]{\textcolor{red}{[AM: #1]}}
\newcommand{\bns}[1]{\textcolor{orange}{[BS: #1]}}
\newcommand{\kas}[1]{\textcolor{brown}{[KA: #1]}}





\makeatletter
\newcommand{\vast}{\bBigg@{3}}
\newcommand{\Vast}{\bBigg@{4}}
\makeatother



\begin{document}

\title{Local minima of the empirical risk in high dimension:\\
General theorems and convex examples}


\author{Kiana Asgari\thanks{Department of Management Science and Engineering, Stanford University} \;\;
\and\;\;
Andrea Montanari\thanks{Department of Statistics and Department of Mathematics, Stanford University} 
	%
	\and 
	%
	Basil Saeed\thanks{Department of Electrical Engineering, Stanford University}
	%
}

\maketitle

\begin{abstract}
We consider a general model for high-dimensional empirical risk minimization whereby the data $\bx_i$ are $d$-dimensional isotropic Gaussian vectors, the model is parametrized by $\bTheta\in \reals^{d\times k}$, and the loss depends on the data via the projection $\bTheta^{\sT}\bx_i$. This setting covers as special cases 
classical statistics methods (e.g. multinomial regression and other generalized linear models), but also two-layer fully connected neural networks with $k$ hidden neurons.

We use the Kac-Rice formula from Gaussian process theory to derive a bound on the expected number of local minima of this empirical risk,
under the proportional asymptotics in which $n,d\to\infty$, with $n\asymp d$. Via Markov's inequality, this bound allows to determine the positions of these minimizers (with exponential deviation bounds) and hence derive sharp asymptotics on the estimation and prediction error.

In this paper, we apply our characterization to convex losses,
where high-dimensional asymptotics were not (in general) rigorously established for $k\ge 2$. We show that our approach is tight and
allows to prove previously conjectured results. In addition, we characterize the spectrum of the Hessian at the minimizer. A companion paper applies our general result to non-convex examples.
\end{abstract}


\tableofcontents

%\section{Facts}
%
%If $X$ is a random vector with a density $p$ in $\reals^d$ and $h$ is a continuous function in $\reals^d$,
%and $M=\{x:g(x)=0\}$ is a regular manifold ($g:\reals^d\to\reals^k$ with $\rank(Dg(x))=k$ on $M$)
%then 
%%
%\begin{align}
%    \int_M\, h(x)\cdot p(x) \, \de_M x = \E[ h(X)|g(X)=0]\, P_{g(X)}(0)\, .
%\end{align}




\section{Introduction}
\label{sec:Intro}

Empirical risk minimization (ERM) is by far the most popular parameter
estimation technique in high-dimensional statistics and 
machine learning. While empirical process theory provides a
fairly accurate picture of its behavior when the sample size is sufficiently large \cite{van1998asymptotic,vershynin2018high}, a wealth of new phenomena arise when the number of 
model parameters per sample becomes of order one \cite{montanari2018mean}.
Among the most interesting of such phenomena:
exact or weak recovery phase transitions \cite{DMM09,BayatiMontanariLASSO,lelarge2019fundamental,barbier2019optimal,mignacco2020role}; 
information-computation gaps \cite{celentano2022fundamental,schramm2022computational}; 
benign overfitting and double descent
\cite{hastie2022surprises}. 
%(Of course, the number of parameters per
%sample is not always a good measure of complexity, and a more accurate
%statement would involve, for instance, Radamacher complexity.)

The proportional regime is increasingly of interest because of the adoption of
ever-more expressive families of statistical models by practitioners.
For instance, scaling laws in AI development imply that optimal predictive performances are achieved when he number of model parameters scales roughly proportionally to the number of samples \cite{kaplan2020scaling,hoffmann2022training}.

The theoretical toolbox at our disposal to understand ERM in the
proportional regime is far less robust and general than textbook 
asymptotic statistics. Approximate message passing (AMP) algorithms 
admit a general high-dimensional characterization
\cite{bayati2011dynamics,BayatiMontanariLASSO,donoho2016high},
and gives access to certain local 
minimizers of the empirical risk, but not all of them, 
and require case-by-case technical analysis. Gaussian 
comparison inequalities provide a very elegant route \cite{ThrampoulidisOyHa15,thrampoulidis2018precise}, but 
give two-sided bounds only for convex problems, and only succeed 
when a simple comparison process exists that yields a sharp 
inequality. Leave-one out techniques \cite{el2018impact} can be powerful,  but 
also crucially rely on the condition that perturbing the ERM problem by leaving out one datapoint does not affect significantly the minimizer. This in turn is challenging to guarantee without convexity. 

\subsection{Setting}

The main objective of this paper is to develop a new approach 
for ERM analysis that is based on the celebrated Kac-Rice formula for the expected number of zeros of a Gaussian process
\cite{Kac1943,Rice1945}.
We consider a general empirical risk of the form
\begin{equation}
\label{eq:erm_obj_0}
    \widehat R_n(\bTheta) := \frac1n \sum_{i=1}^n L(\bTheta^\sT \bx_i, \by_i) + \frac1d\sum_{j=1}^d \sum_{i=1}^k\rho(\sqrt{d}\Theta_{i,j})
\end{equation}
where
    $\bx_i\in\R^d$, $\by_i\in\R^{q},$ 
    $\bTheta = (\Theta_{i,j})_{i\in[d],j\in[k]} \in \R^{d\times k}$, while the loss function $L : \R^{k+q} \to \R$, $(\bu,\by)\mapsto L(\bu,\by)$, and regularizer
$\rho :\R \to \R$ are differentiable and independent of $n,d$.

Sharp asymptotics for the ERM problem \eqref{eq:erm_obj_0}
are generally unknown, even when $L$ is convex, although conjectures can be derived
using statistical physics methods \cite{engel2001statistical,montanari2024friendly}. Even when
$L$, $\rho$ are convex, Gaussian comparison inequalities fail to provide acharacterization for 
$k\ge 2$.

We assume the covariate vectors $(\bx_i:i\le n)$ to be i.i.d.
with $\bx_i\sim\normal(0,\bI_d)$ and the response variable to be distributed according to a general multi-index model.
Namely
%
\begin{align}
\nonumber
\P(\by_i\in S|\bx_i) = \rP(S |\bTheta_0^{\sT}\bx_i)\, ,
\end{align}
%
where $\bTheta_0^{d\times k_0}$ is a fixed matrix of coefficients and  $\rP:\cB_{\reals^q}\times\reals^{k_0}\to [0,1]$ is a probability kernel. More concretely (and equivalently)
$\by_i = \bphi(\bTheta_0^{\sT}\bx_i,w_i)$ for some  measurable function $\bphi:\reals^{k_0}\times\reals\to\reals^q$ and
$w_i\sim \P_{w}$ independent of $\bx_i$. 

It might be useful to point out a couple of examples.


\paragraph{Multinomial regression.} We define labels via one-hot encoding, i.e. 
$\by_i\in \reals^{k+1}$ takes value in 
$\{\be_0=\bzero,\be_1,\dots,\be_k\}$, $q=k+1$. Further, we assume a well specified model whereby
$\P(\by_i=\be_j|\bx_i) = p_j(\bTheta_0^{\sT}\bx_i)$
and for $j\in\{1,\dots,k\}$, $\bv_0 \in\R^{k}$,
\begin{equation}
   p_j(\bv_0)  := \frac{\exp\{ v_{0,j}\} }{ 1  + \sum_{j'=1}^k \exp\{ v_{0,j'}\}},\quad\quad 
   p_{0}(v_0)  := \frac{1 }{ 1  + \sum_{j'=1}^k \exp\{ v_{0,j'}\}}. \label{eq:MultiNomialDef}
\end{equation}

The maximum likelihood estimator is obtained using cross-entropy loss
%
\begin{align}
\nonumber
L(\bv, \by) := -\sum_{j=0}^k y_j\log p_j(\bv)\, .
\end{align}
%
Despite its simplicity, high-dimensional asymptotics for this model are well understood only for the case of 
two classes, i.e. $k=1$
\cite{sur2019modern,montanari2019generalization,deng2022model}.
We will apply our general theory to this model in Section
\ref{sec:Multinomial}. 

\paragraph{Two-layer neural networks.}
Consider, to be definite, a binary classification problem
whereby $y_i\in\{0,1\}$, with $\P(y_i=1|\bx_i) = \varphi(\bTheta_0^{\sT}\bx_i)$. It makes sense to fit 
a two-layer  neural network,  with $k\ge k_0$ (for $\bTheta_{\cdot,i}$ the $i$-th column of $\bTheta$):
%
\begin{align}
\nonumber
f(\bx;\bTheta) = \sum_{i=1}^ka_i \sigma(\bTheta_{\cdot,i}^{\sT}\bx)\, .
\end{align}
%
For simplicity, we can think of the second layer weights $(a_i)$
as fixed. The ERM problem can be recast in the form \eqref{eq:erm_obj_0}
by setting:
%
\begin{align}
\nonumber
L(\bv, y) := - y f_0(\bv) + \log (1+e^{f_0(\bv)})\, ,\;\;\;\;\;
f_0(\bv):= \sum_{i=1}^k a_i\sigma(v_i)\, .
\end{align}
%
We will use our general theory to treat this example in a companion paper \cite{OursInPreparation}.


\subsection{Summary of results: Number of local minima and topological trivialization}


The main results of this paper are:
%
\begin{enumerate}
\item A general upper bound on the exponential growth rate
of the expected number of local minima of $ \hR_n(\bTheta)$ 
in any specified domain of the parameter space.
\item An analysis of this upper bound in the case of convex losses,
showing that it implies sharp asymptotics on the properties of the empirical risk minimizer. 
\item A demonstration of how these general results can be applied to yield concrete predictions on specific problems.
We specialize to exponential families, and even more explicitly to multinomial regression.
\end{enumerate}
%
Analyzing our general 
upper bound (point 1 above) 
in the case of non-convex losses is somewhat more challenging but does still provide sharp results
in several cases. We will present those results in a separate paper \cite{OursInPreparation}. 

Omitting several technical details, let
$\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}$ be the empirical distribution of the  
rows of the matrix $\sqrt{d}[\bTheta,\bTheta_0]\in\reals^{d\times (k+k_0)}$ and $\hnu_{\bX\bTheta,\by}$ be the empirical distribution of the  
rows of the matrix $[\bX\bTheta,\by]\in\reals^{k\times q}$. 
For any set of probability distributions $\cuA\subseteq \cuP(\R^{k+k_0}),\cuB\subseteq\cuP(\R^{k+q})$ (we denote by $\cuP(\Omega)$ the set
of probability measures on the Borel space $\Omega$), define
%
\begin{align}
\nonumber
\cZ_n(\cuA, \cuB):=\Big\{\mbox{ local minima of }  \hR_n(\bTheta)
\mbox{ s.t.  }  \hmu_{\sqrt{d}[\bTheta,\bTheta_0]}\in\cuA, \hnu_{\bX\bTheta,\by}\in\cuB \Big\}\, .
\end{align}
%
We consider the asymptotics $n,d\to\infty$ with $n/d\to\alpha\in(0,\infty)$, with $k,k_0$, $\bw$, $\rP$, $L$, $\rho$
fixed.
Our main result is an inequality of the form (below $|\cS|$ denotes the cardinality of set $\cS$):
%
\begin{align}
%
    \lim_{n,d\to\infty}\frac{1}{n}\log \E |\cZ_n(\cuA, \cuB)|
    \le -\inf_{\mu\in\cuA,\nu\in\cuB}\Phi(\mu,\nu)\, .\label{eq:Summary}
\end{align}
%
Via Markov inequality, such a bound allows to
localize the minimizers. To be concrete, for a test function 
$\psi:\reals^k\times\reals^{k_0}\to \reals$,
and any local minimizer $\hbTheta$
(with $\hbtheta_i$ , $\btheta_{0,i}$ the $i$-th rows of $\hbTheta$, $\bTheta_0$), we have
%
\begin{align}
\nonumber
\P\left\{\frac{1}{d}\sum_{i=1}^d\psi(\sqrt{d}\hbtheta_{i},\sqrt{d}\btheta_{0,i})
\in I\right\} \le \exp\left\{-n\inf_{\mu\in\cuA(I,\psi), \nu}
\Phi(\mu,\nu)+o(n)\right\}\, ,
\end{align}
%
where $\cuA(I,\psi): = \{\mu: \int \psi(\bt,\bt_0)\mu(\de\bt,\de\bt_0)\in I\}$.

We expect Eq.~\eqref{eq:Summary} hold with equality, although we do not attempt to prove. More crucially, we observe that in a number of
setting of interest, our bound allows to identify 
the precise limit of $\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}$ and $\hnu_{\bX\bTheta,\by}$. Namely,
in many cases of interest there exist $(\mu_\star,\nu_\star)$ such that:
%
\begin{equation}\label{eq:Trivialization}
\begin{split}
    &\Phi(\mu,\nu)\ge 0 \;\;\;\forall \mu,\nu\, ,\\
    &\Phi(\mu,\nu) = 0 \;\;\Leftrightarrow  (\mu,\nu) = (\mu_\star,\nu_\star)\, ,
    \end{split}
\end{equation}
%
which implies that $\mu_\star,\nu_\star$ is the unique limit:
%
\begin{align}
\nonumber
\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}\weakc \mu_\star\,,\;\;\;\;\;\;
\hnu_{\bX\bTheta,\by}\weakc\nu_\star\, .
\end{align}
We will refer to Eq.~\eqref{eq:Trivialization} as the \emph{rate trivialization property}. 
In particular, we will prove that rate trivialization
takes place for strictly convex ERM problems. 
Although strictly convex ERM problems have a unique minimizer,
rate trivialization is far from obvious because of the inequality 
in Eq.~\eqref{eq:Summary}. 
Hence, convex examples provide an important test of our general theory, 
and also an important domain of application.

In our companion paper we will characterize regimes in which 
\eqref{eq:Trivialization} holds for non-convex losses hence determining the asymptotics in those problems as well.

\subsection{Related work}

A substantial line of work studies the existence and properties
of local minima of the empirical risk for a variety of statistics 
and machine learning problems, see e.g. 
\cite{mei2018landscape,sun2018geometric,soltanolkotabi2018theoretical}. However,
concentration techniques used in these works typically require $n/d\to\infty$.

The Kac-Rice approach has a substantial history in statistical
physics where it has been used to characterize the landscape of 
mean field spin glasses 
\cite{bray1980metastable}. 
We refer in particular to the seminal works
of Fyodorov \cite{fyodorov2004complexity} and Auffinger, Ben Arous, Cern\`y \cite{auffinger2013random}, as well as to the recent papers
\cite{subag2017complexity,ros2023high} and references therein.
It has also a history in statistics, where it has been used for statistical analysis of Gaussian fields \cite{adler2009random}.

Within high-dimensional statistics, the Kac-Rice approach was first used
in \cite{arous2019landscape} to characterize 
topology of the likelihood function for Tensor PCA. 
In particular, these authors showed that the expected number of modes of the likelihood can grow exponential in the dimension, hence 
making optimization intractable.
The Kac-Rice approach was used in \cite{fan2021tap} to show that Bayes-optimal estimation 
in high dimension can be perforemd for $\integers_{2}$-synchronization via minimization of the so-called Thouless-Anderson-Palmer (TAP) free energy. These results  were substantially strengthened in \cite{celentano2023local} which proved local convexity of TAP free energy in a neighborhood of
the global minimum. 

None of the above papers studied the ERM problem that is the focus of the present paper.
The crucial challenge to apply the Kac-Rice formula to
the empirical risk of Eq.~\eqref{eq:erm_obj_0} lies in the fact that
the empirical risk function $\hR_n(\,\cdot\,)$ is not a Gaussian process
(even for Gaussian covariates $\bx_i$).
In contrast, 
\cite{arous2019landscape,fan2021tap,celentano2023local}
treat problems  for which $\hR_n$ is itself Gaussian.
In the recent review paper \cite{ros2023high}, Fyodorov and Ros 
mention non-Gaussian landscapes as an outstanding challenge even at the non-rigorous theoretical physics

While in principle Kac-Rice formula can be extended to non-Gaussian processes
(provided the gradient of $\hR_n$ has a density), this creates technical difficulties.  
In a notable paper, Maillard, Ben Arous, Biroli \cite{maillard2020landscape} 
followed this route to study (a special case of) the ERM problem
of Eq.~\eqref{eq:erm_obj_0}. They derive an upper bound
of the form \eqref{eq:Summary}, albeit with a different function 
$\Phi(\mu,\nu)$. However, it is unknown whether their bound has
the rate trivialization property \eqref{eq:Trivialization},
even when applied to convex ERM problems\footnote{Strictly speaking, \cite{maillard2020landscape} does not apply to any non-quadratic convex risk function, but replicating their proof in a more general settings leads us to this conclusion.}.

Our approach is quite different from earlier works.
We apply Kac-Rice formula to a process in $(n+d)k$ dimensions,
that we refer to as the `gradient process.'
The gradient process extends the gradient $\nabla \hR_n(\bTheta)$ and has two important additional properties: it is Gaussian;
zeros of the gradient process (with certain additional conditions) correspond to local minima of the empirical risk.

We finally note that, in the case of convex losses, an alternative proof technique is available,
which is based on approximate message passing (AMP) algorithms. This approach was initially developed to
analyze the Lasso \cite{BayatiMontanariLASSO} and subsequently refined and extended, see e.g.
\cite{donoho2016high,sur2019modern,loureiro2021learning,ccakmak2024convergence}.
While our main motivation is to move beyond convexity, our approach recovers and unifies,
with some distinctive advantages. 

\subsection*{Notations}
We denote by $\cuP(\Omega)$ the set of (Borel) probability measure on 
 $\Omega$ (which will always be Polish equipped with its Borel $\sigma$-field). 
Additionally, we denote by $\cuP_n(\Omega)\subset \cuP(\Omega)$ the set of empirical measures on $\Omega$ of $n$ atoms.
 Throughout we assume $\cuP(\Omega)$ is endowed with the topology induced by the Lipschitz bounded metric, defined by
\begin{equation}
\nonumber
\dBL(\mu,\nu) = \sup_{\vartheta\in \cF_{\textrm{LU}}} 
    \left|\int \vartheta(\omega) \,\mu(\de\omega) - \int \vartheta(\omega) \,\nu(\de\omega)\right|
    \, ,
\end{equation}
where $\cF_{\mathrm{LU}}$ is the class of Lipschitz continuous functions $\vartheta:\Omega\to \R$ with Lipschitz constant at most 1 and uniformly  bounded by 1.
We will also use $W_2(\mu,\nu)$ to denote the Wasserstein 2-distance.

etting $\Omega = \Omega_1\times\Omega_2$, $\ba_2 $ a generic point in $\Omega_2$, and $\mu\in \cuP(\Omega)$, we denote by $\mu_{\cdot|\ba_2}\in\cuP(\Omega_1)$ to be the regular conditional distribution of $\mu$ given $\ba_2$.  
We also denote by $\mu_{(\ba_2)}\in\cuP(\Omega_2)$  the restriction of $\mu$ to $\Omega_2$ (refer to  Section D.3 \cite{dembo2009large} for more about product spaces).

We let $\sS^k$ be the
set of $k\times k$ symmetric matrices with entries in $\R$, and 
$\sS^k_{\succeq 0},\sS^k_{\succ 0}$ the subsets of positive semidefinite, positive definite matrices, respectively.
For a matrix $\bZ \in\C^{k\times k}$, define
\begin{equation}
\nonumber
\Re(\bZ) =  \frac12 \left(\bZ + \bZ^*\right),\quad
    \Im(\bZ) := \frac1{2i } \left(\bZ - \bZ^*\right)\, ,
\end{equation}
and let 
\begin{equation}
\nonumber
\bbH_+^k := \left\{\bZ \in\C^{k\times k} : {\Im(\bZ)} \succ \bzero\right\},
\quad 
\bbH_-^k := \left\{\bZ \in\C^{k\times k} : \Im(\bZ) \prec \bzero\right\}.
\end{equation}
Note that $\Re(\bZ)$ and $\Im(\bZ)$ are self-adjoint for any $\bZ$.
Given a self-adjoint matrix $\bA\in\C^{n\times n}$, we denote by $\lambda_1(\bA)\ge,
\dots,\ge \lambda_n(\bA)\in\reals$ its  eigenvalues, and by
$\lambda_{\min}(\bA), \lambda_{\max}(\bA)$ the minimum and  maximum
eigenvalues, respectively. 
For $m\le n$, we use $\sigma_1(\bA)\ge \dots \ge\sigma_{m}(\bA)$ for the singular values of a matrix $\bA\in\C^{m\times n}$, and $\sigma_{\max}=\sigma_1,\sigma_{\min}=\sigma_m$ to denote the minimum and maximum of these.
We use $\grad_\ba f \in \R^m,\grad_\ba^2 f \in\R^{m\times m}$ to denote the Euclidean gradient and Hessian of $f:\R^m \to \R$ with respect to $\ba\in\R^m$, for some $m$. Furthermore, for function $f :\R^{m\times n} \to \R$, $\grad^2_\bA f(\bA)  \in\R^{mn\times mn}$, 
in the Hessian defined by identifying $\R^{m\times n}$ with $\R^{mn}$.
Similarly, for a function $\boldf :\R^{m} \to \R^{n},$  we denote by $\bJ_\ba \boldf \in\R^{n \times m}$ its Jacobian matrix with respect to $\ba\in\R^m$. For $\bF:\R^{n\times m} \to \R^{k\times p},$ 
we use $\bJ_{\bA} \bF \in\R^{kp \times nm}$ to denote the Jacobian with respect to $\bA \in\R^{n\times m}$ after vectorizing the input and output by concatenating the columns. We'll often omit the argument in the subscript when it is clear from context.

We use $|S|$ to denote the cardinality of set $S$.
We denote the Euclidean ball of radius $r$ and center $\ba$ in $\reals^d$ by $\Ball^d_r(\ba)$. We similarly use $\Ball^{n\times d}_r(\bA)$ to denote the Frobenius norm ball of matrices centered at $\bA\in\R^{n\times d}$.
%
For two distributions $\nu,\mu$, we use
$\KL(\nu\|\mu)$ to denote their KL-divergence.
%*****************************************************************
%*****************************************************************
%
\section{Main results I: General empirical risk minimization}
\subsection{Definitions}
\label{sec:definitions}

As  already stated, we assume $\by_i = \bphi(\bTheta_0^\sT\bx_i,w_i)$
for $w_i$ independent of $\bx_i\sim\normal(0,\bI_d)$.
In our general treatment, we will explicate the dependence of $\by_i$ on $\bTheta_0^\sT,\bx_i,w_i$ and write for each $i\in[n]$,
\begin{equation}
\nonumber
    \ell(\bTheta^\sT \bx_i, \bTheta_0{^\sT} \bx_i, w_i) = L(\bTheta^\sT\bx_i, \bphi(\bTheta_0^\sT\bx_i,w_i)).
\end{equation}
Hence, from a mathematical viewpoint, the empirical risk \eqref{eq:erm_obj_0} is equivalent 
to
\begin{equation}
\label{eq:erm_obj}
    \widehat R_n(\bTheta) = \frac1n \sum_{i=1}^n \ell(\bTheta^\sT \bx_i, \bTheta_0{^\sT} \bx_i, w_i) + \frac1d\sum_{j=1}^d \sum_{i=1}^k\rho(\sqrt{d}\Theta_{i,j})\, .
\end{equation}
(Of course, from the statistical viewpoint, estimation proceeds by minimizing \eqref{eq:erm_obj_0}, without knowledge of $\bTheta_0$.)

Here
    $\bx_i\in\R^d,
    \bTheta = (\Theta_{i,j})_{i\in[d],j\in[k]} \in \R^{d\times k}$, $\bTheta_0\in \R^{d \times k_0}$,
    $\bw:= (w_1,\dots,w_n) \in \R^n$, $\ell : \R^{k+k^*+1} \to \R$,
     $(\bu,\bv,w)\mapsto \ell(\bu,\bv,w)$, and
$\rho :\R \to \R$.
Recall that
$\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}$ denotes the empirical distribution of
rows of $\sqrt{d}\begin{bmatrix}\bTheta,\bTheta_0\end{bmatrix}\in \R^{d\times (k+k_0)}$. We further define 
%
\begin{align}
\bR(\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}) :=\begin{bmatrix}
\bTheta^{\sT}\bTheta & \bTheta^{\sT}\bTheta_0\\
\bTheta_0^{\sT}\bTheta & \bTheta_0^{\sT}\bTheta_0\\
\end{bmatrix}=\int \bt\bt^{\sT} \hmu_{\sqrt{d}[\bTheta,\bTheta_0]}(\de\bt)\, .
\end{align}
%
Given block matrix $\bR\in \sS_{k+k_0}$ we define the Schur complement of  the $k_0\times k_0$ block as:
%
\begin{align}
\nonumber
\bR = \begin{bmatrix}
\bR_{11} & \bR_{10}\\
\bR_{01} & \bR_{00}\\
\end{bmatrix}
\;\;\Rightarrow\;\; \bR/\bR_{00} = \bR_{11}-\bR_{10}\bR_{00}^{-1}
\bR_{01}\, .
\end{align}
%

We will consider local minima of the ERM problem satisfying a set of
constraints  specified by the parameters:
%
\begin{equation}
\nonumber
    \sPi := (\sfsigma_{\bH}(n), \sfsigma_{\bG}(n),
    \sfsigma_{\bL},\sfsigma_\bV,\sfsigma_\bR,\sfA_{\bR},\sfA_\bV)\, . %\quad\textrm{for}\quad
    %\sfA_{\bR},\sfA_\bV \ge 1, 
\end{equation}
Namely,  for $\cuA\subseteq \cuP(\R^{k+k_0}),\cuB\subseteq\cuP(\R^{k+k_0}\times \R)$, 
we define 
%
\begin{align}
\label{eq:number_of_zeros_main}
& Z_n(\cuA,\cuB,\bTheta_0,\bw,\sPi) :=  |\cZ_n(\cuA,\cuB,\bTheta_0,\bw,\sPi)|\, ,
\end{align}
where
%
\begin{align}
\label{eq:set_of_zeros_main}
\cZ_n&(\cuA,\cuB,\bTheta_0,\bw,\sPi):=\\
&\Big\{\bTheta\in \R^{d\times k}:\; 
\nabla \hR_n(\bTheta)=\bzero\,, \hmu_{\sqrt{d}[\bTheta,\bTheta_0]} 
\in \cuA,\;\hnu:=\hnu_{\bX\bTheta,\bX\bTheta_0,\bw} \in\cuB,\;
\nabla^2 \hR_n(\bTheta)\succeq \sfsigma_\bH,
\nonumber \\
& 
\sfA_{\bR} \succ \bR(\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}) \succ\sfsigma_\bR,
\sfA_\bV \succ \E_{\hnu}[\bv\bv^\sT] \succ \sfsigma_{\bV}, \;
\E_{\hnu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;
\sigma_{\min}\left( \bJ_{(\bV,\bV_0,\bTheta)} \bG\right) > n\,\sfsigma_{\bG}(n)
\Big\}\,,\nonumber
\end{align}
where 
%
\begin{enumerate}
%
\item $\hnu_{\bX\bTheta,\bX\bTheta_0,\bw}$ denotes the empirical distribution of rows of $[\bX\bTheta,\bX\bTheta_0,\bw]\in\R^{n\times(k+k_0+1)}$.
\item For $\bV^\sT = (\bv_j^\sT)_{j\in[k]} \in \R^{k\times d}, \bV_0^\sT = (\bv_{0,j}^\sT)_{j\in[k_0]} \in\R^{k_{0}\times d}$ and $\bw\in\R^n$, defining
\begin{equation}
\label{eq:def_bL_bRho}
  \bL(\bV,\bV_0;\bw)=\left(\frac{\partial}{\partial_{v_j}} \ell(\bv_i,\bv_{0,i}, w_i)\right)_{i\in[n],j\in[k]}\quad
  \textrm{and}
  \quad\quad
  \bRho(\bTheta) :=  \frac1{\sqrt{d}}\left(\rho'(\sqrt{d} \Theta_{i,j})\right)_{i\in[d],j\in[k]},
\end{equation}

\begin{equation}
\label{eq:def_G}
\bG(\bV,\bV_0,\bTheta;\bw) := \frac1n \bL(\bV,\bV_0;\bw)^\sT[\bV,\bV_0] + \bRho(\bTheta)^\sT[\bTheta,\bTheta_0] \in\R^{k\times(k+k_0)}\, ,
\end{equation}
and $\bJ_{(\bV,\bV_0,\bTheta)} \bG$ denotes the Jacobian matrix of this map.
\end{enumerate}

For $z\in\C$ with $\Im(z) >0$, $\bS\in\reals^{k\times k}$ a symmetric matrix
and $\nu\in\cuP^{\R^{k+k_0+1}}$,
%a probability distribution over the  
%$k\times k$ symmetric matrix $\bD$, 
we let  $\bF_z(\bS; \nu)\in\C^{k\times k}$
\begin{equation}
     \bF_z(\bS; \nu) := \Big( \E_\nu[(\bI + \grad^{2}\ell(\bv,\bv_0,w)\bS )^{-1}
     \grad^{2}\ell(\bv,\bv_0,w)
     ] - z \bI \Big)^{-1}\,.
\end{equation}
%
We let $\bS_\star(\nu;z)$ be the the unique solution to 
\begin{equation}
\label{eq:fp_eq}
\bF_z(\bS_\star;\nu )= \alpha\bS_\star,
\end{equation}
%
and define $\bS_0(\nu) = \lim_{\eps\to 0} \bS_\star(\nu;i\eps)$.
Existence and uniqueness of $\bS_*(\nu;z)$ and  $\bS_0(\nu)$ are proven in 
Appendix~\ref{sec:RMT}, which extends classical results on the Marchenko-Pastur law. 
Appendix~\ref{sec:RMT} also proves that the function
\begin{equation}
\nonumber
    z \mapsto \frac1k \Tr(\alpha\bS_\star(\nu;z))
\end{equation}
%
is the  Stieltjes transform on a unique measure on $\reals$ which we denote by
$\mu_{\MP}(\nu)\in\cuP(\R)$.

Observe that for any fixed $\bTheta$, the empirical spectral distribution  of the Hessian of the regularization term
(as an element of $\R^{dk \times dk}$) is given by
\begin{equation}
\nonumber
    \mu_{\rho}(\bTheta) :=  \frac1{dk}\sum_{j=1}^k \sum_{i=1}^d \delta_{\rho''(\sqrt{d}\Theta_{i,j})}
    = \frac1k \sum_{j=1}^k \rho''_{\#} \hmu_{j} ,\quad\quad\textrm{where}\quad\quad
    \hmu_j := \hmu_{\sqrt{d}\bTheta_{\cdot, j}}.
\end{equation}
We will often abuse notation and denote this by $\mu_{\rho}( \hmu_{\sqrt{d}\bTheta})$ since this is purely a function of the empirical distribution of $\bTheta.$
We let 
\begin{equation}
\nonumber
    \mu_\star(\nu,\mu) := \mu_{\MP}(\nu) \boxplus \mu_{\rho}(\mu) 
\end{equation}
where $\boxplus$ denotes the free additive convolution.

Finally, we recall the definition of the multivariate proximal operator. 
For $\bz \in\R^k, \bv_0 \in\R^{k_0}, \bS\in\R^{k\times k}, w\in\R,$  and $f : \R^{k+k_0+ 1} \to\R,$ let
\begin{equation}
\nonumber
    \Prox_{f(\cdot, \bv_0, w)}(\bz;\bS):=\arg\min_{\bx\in\R^k}\left( \frac12(\bx-\bz)^\bT\bS^{-1}(\bx-\bz) + f(\bx,\bv_0,w)\right)\in\R^k.
\end{equation}






\subsection{General assumptions}

\label{sec:assumptions}

We will make the following assumptions.
\begin{assumption}[Regime]
\label{ass:regime}
We assume the proportional asymptotics, i.e., $d := d_n$ such that
   \begin{equation}
       \alpha_n := \frac{n}{d_n} \to \alpha \in (1, \infty), \quad\quad\textrm{and} \quad\quad k,k_0 \le C
   \end{equation}
   for some universal constant $C$ independent of $n$.
Furthermore, for all $n,$ we have $n>d_n + k$.
\end{assumption}

\begin{assumption}[Loss]
\label{ass:loss}
\label{ass:density}
Assume that the following partial derivatives of $\ell:\R^{k+k_0+ 1} \to\R$ exist and are Lipschitz:
%
\begin{equation}
\nonumber
    \frac{\partial}{\partial u_l}\ell(\bu),\quad\quad
    \frac{\partial^2}{\partial {u_j} \partial{u_l}} \ell(\bu),\quad\quad
    \frac{\partial^3}{\partial u_i \partial u_j \partial u_l}\ell(\bu), \quad\quad\textrm{for}\quad\quad i,j\in[k+k_0], l\in[k], 
\end{equation}
%
Furthermore, for any $\bw\in\supp(\P_\bw)^n, \bTheta \in\R^{d\times k}$, $\bTheta_0 \in\R^{d \times k_0}$ and $l\in[k]$, the random variable
    \begin{equation}
\nonumber
        \frac{\partial}{\partial{u_l}}\ell(\bTheta^\sT\bx, \bTheta_0^\sT \bx, \bw),\quad 
        \bx\sim \cN(0,\bI_d)
    \end{equation}
    has a (bounded) density in a neighborhood of $0$, for all  
    $\bTheta, \bTheta_0$ such that $\sfsigma_\bR \prec \bR(\bTheta,\bTheta_0) \prec  \sfA_\bR$. 
\end{assumption}
\begin{assumption}[Regularizer]
\label{ass:regularizer}
The regularizer $\rho:\R \to \R$ 
is three times continuously differentiable. 
\end{assumption}

\begin{assumption}[Constraint sets]
\label{ass:sets}
The constraint sets $\cuA\subseteq \cuP(\R^{k+k_0}),\cuB\subseteq \cuP(\R^{k+k_0+1})$ are measurable, and 
\begin{equation}
\nonumber
    \{\bTheta :
    \in\R^{d\times k} : \hmu_{\sqrt{d}[\bTheta,\bTheta_0]} \in \cuA\},\quad\quad
    \{\bbV \in\R^{n\times (k+k_0)} : \hnu_{\bbV,\bw} \in\cuB\} 
\end{equation}
are open for any fixed $\bw \in \R^n$.
%open in the topology induced by the bounded Lipschitz metric.
\end{assumption}

\begin{assumption}[Data generation]\label{ass:Data}
The covariates/noise pairs $((\bx_i,w_i):i\le n)$ are i.i.d. with $(\bx_i,w_i)\sim\cN(\bzero,\bI)\otimes \P_w$. 
\end{assumption}

\begin{assumption}[Distribution of $\bTheta_0$]
\label{ass:theta_0}
There exists a constant $c$ independent of $n$ such that 
$\sigma_{\min}(\bTheta_0) \succ c \bI$.
   Furthermore, for some $\mu_0 \in\cuP(\R^{k_0})$, we have the convergence 
   \begin{equation}
\nonumber
       \hmu_{\sqrt{d}\bTheta_0} \stackrel{W_2}{\Rightarrow} \mu_0.
   \end{equation}
\end{assumption}


\begin{assumption}[The parameters $\sPi$]
\label{ass:params}
 The parameters $\sfsigma_{\bL},\sfsigma_\bV,\sfsigma_\bR,\sfA_{\bR},\sfA_\bV$ are fixed independent of $n,d$ in the proportional asymptotics. Parameters $\sfsigma_{\bH}(n)$, $\sfsigma_{\bG}(n)$ can depend on $n$, subject to
 $\sfsigma_{\bH}(n) = e^{-o(n)}$, $\sfsigma_{\bG}(n) = e^{-o(n)}$.
\end{assumption}

%\bns{Add the technical bound so that $\sfsigma \le 1$ and $\sfA \ge 1$} 



\subsection{General variational upper bound on the number of critical points}
%
Our main theorem gives a general formula that upper bounds the number of critical points of the risk~\eqref{eq:erm_obj}. 
To state it, let $\cuV := \cuV(\cuA,\cuB,\sPi,\P_\bw,\smu_{0})$ be defined by
\begin{align}
\nonumber
 \cuV(\cuA,\cuB,\sPi,\P_\bw,\smu_{0}) := \Big\{(\mu,\nu) \in \cuA\times \cuB &:\;
\sfA_{\bR} \succ \bR(\mu) \succ\sfsigma_\bR,\;
\sfA_\bV \succ \E_{\nu}[\bv\bv^\sT] \succ \sfsigma_{\bV},\;
\E_{\nu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;\\
&\quad\quad\nonumber
\E_\nu[\grad \ell(\bv,\bv_0,w)(\bv,\bv_0)^\sT + 
     \E_\mu[\rho'(\btheta) (\btheta, \btheta_0)^\sT] =   \bzero_{k\times (k+k_0)},\\
    &\quad\quad
\mu_{(\btheta_0)} = \smu_{0},\; \nu_{(w)} = \P_w,\;
\mu_{\star}(\mu,\nu)((-\infty,0)) = 0
\Big\}.\label{eq:GeneralSet}
\end{align}
%
Further, define $\Phi_\gen:\cuP(\R^{k}\times \R^{k_0})\times \cuP(\R^{k+k_*+1})\times\sS^{k+k_0}\to\reals $
via
\begin{align}
\Phi_\gen(\mu,\nu,\bR)&:=-\frac{k}{2\alpha}\log(\alpha)
-\frac{k}{\alpha}
\int \log(\lambda ) \mu_{\star}(\nu,\mu)(\de \lambda)
  + \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
  - \frac{1}{2\alpha} \Tr\left(\bR_{11}\right) 
 \nonumber \\
  &\quad+ \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
+\frac1{2}\log\det(\bR)
 - \frac12 \Tr\left((\bI_{k+k_0} - \bR^{-1})\E_\nu[[\bv^\sT,\bv_0^\sT]^\sT [\bv^\sT,\bv_0^\sT]]\right)  
\nonumber\\
   &\quad
+\frac1{2}
\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\mu[\grad \rho \grad \rho^\sT]\right)\label{eq:PhiGen} \\
  &\quad- \frac1{2}\Tr\left(\E_\nu\left[[\bv^\sT,\bv_0^\sT]^\sT\grad\ell^\sT\right] (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\nu[\grad\ell[\bv^\sT,\bv_0^\sT]^\sT] \bR^{-1}\right)\nonumber\\
%
    &\quad
   +\KL( \nu_{\cdot |w} \|\cN(0,\bI_{k+k_0}) ) + \frac1\alpha\KL(\mu_{\cdot | {\btheta_0}}\| \cN(\bzero, \bI_{k})).\nonumber
\end{align}


\begin{theorem}
\label{thm:general}
For $\delta>0$, define
\begin{equation}
\nonumber
    \cG_\delta := 
    \{\bw \in\Ball_{\sfA_{w}\sqrt{n}}^n (\bzero)  : \dBL(\hnu_{\bw}, \P_w) < \delta
    \}\, .
\end{equation}
Then under  Assumptions \ref{ass:regime} to \ref{ass:params} of Section~\ref{sec:assumptions}, we have
\begin{align}
\nonumber
 \lim_{\delta\to0}\lim_{n\to\infty}\frac{1}{n}\log\E_{\bX,\bw}\left[Z_n(\cuA,\cuB,\bTheta_*,\bw,\sPi) \one_{\cG_\delta}\right]
    \le- \inf_{(\mu,\nu) \in \cuV} \Phi_\gen(\mu,\nu, \bR(\mu)).
\end{align}
%
%
\end{theorem}




\section{Main results II: Convex empirical risk minimization}


\subsection{Assumptions for convex empirical risk minimization}

We specialize our treatment to the case of convex losses and ridge regularization, 
making the following additional assumptions.
%
\begin{assumption}[Convex loss and ridge regularizer]
\label{ass:convexity}    
We have
$\grad_{\bv}^2 \ell(\bv,\bv_0,w) \succeq\bzero$ for all
$(\bv,\bv_0,w)\in\reals^{k+k_0+1}$. 
%the function $\bv \mapsto \ell(\bv,\bv_0,w)$ is convex 
%for almost every $(\bv,\bv_0,w) \sim \nu$, and every $\nu\in \cuB$. 
Furthermore, for some $\lambda\ge 0$ fixed, we take 
$$\rho(t) = \frac{\lambda}{2}t^2.$$
\end{assumption}

\begin{assumption}[Distribution of $\bw$]
\label{ass:noise}
The noise variables $w_i \stackrel{i.i.d.}{\sim} \P_{w}$ 
are subgaussian with subgaussian constant bounded by a constant $C>0$ independent of $n$.
\end{assumption}


\subsection{Simplified variational upper bound for convex losses and ridge regularizer}
We specialize our result to the case of convex loss and ridge regularization.  
In doing so, our approach recovers and extends results obtained with alternative technique.
We provide a comparison in Remark \ref{rmk:ComparisonCVX}. Most importantly, within our treatment,
the asymptotics for convex losses is a special case of a more general result.

The next definition plays a crucial role in what follows.
%
\begin{definition}
\label{def:opt_FP_conds}
   We say that the pair $(\mu^\opt,\nu^\opt)\in\cuP(\R^{k+k_0}) \times \cuP(\R^{k+k_0+1})$ satisfies the \emph{critical point optimality condition} if the following holds. The pair $(\bR,\bS) \in \R^{(k+k_0)\times (k+k_0)} \times \R^{k\times k}$ satisfy the following set of equations
%\
\begin{align}
\label{eq:opt_fp_eqs}
    & \alpha \E_{\nu^\opt}[\grad\ell(\bv,\bv_0,W)\grad\ell(\bv,\bv_0,w)^\sT]
    =\bS^{-1} (\bR/\bR_{00}) \bS^{-1}
    \\
    &\E_{\nu^\opt}\left[ \grad \ell(\bv,\bv_0,w) (\bv,\bv_0)^\sT\right] + \lambda (\bR_{11},\bR_{10}) = \bzero_{k\times (k+k_0)},
\end{align}
where 
%
   \begin{align}
&\nu^\opt = \mathrm{Law}(
\Prox( \bg; \bS, \bg_0, w),\bg_0,w),\quad\quad\textrm{where}\quad\quad
(\bg^\sT,\bg_0^\sT)^\sT \sim \cN\left( \bzero_{k+k_0},\bR\right)\;\indep \;w\sim\P_W,
\\
&\mu^\opt_{(\btheta_0)} = \mu_{0},\quad  \mu^\opt_{\cdot|\btheta_0}= \cN(\bR_{10}({\bR_{00}})^{-1}\btheta_0,\bR/\bR_{00}), \quad\quad\textrm{for}\quad\quad\btheta_0\in\R^{k_0},\label{eq:muopt}
    %&\E_{\nu}\left[ \grad \ell(\bv,\bv_0,W) (\bv,\bv_0)^\sT\right] + \lambda (\bR_{11},\bR_{10}) = \bzero_{k\times (k+k_0)}.
\end{align}
and we introduced the notation $\Prox( \bg; \bS, \bg_0, w) = 
\Prox_{\ell(\,\cdot\,,\bg_0,w)}( \bg; \bS)$.
   %$\bR = \bR(\mu)$,
\end{definition}


\begin{theorem}[Rate function under convexity]
\label{thm:convexity}
Consider the setting of Theorem \ref{thm:general}.
Let Assumptions \ref{ass:regime} to \ref{ass:convexity} hold, and
define 
%
\begin{align}
\Phi_\cvx(\nu,\mu,\bS,\bR)
&:=
    -\lambda\Tr(\bS)  -
    \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bS) ]  +\frac1\alpha \log\det(\bS) + \frac{k}{2\alpha} \log(\alpha) +\frac{k}{\alpha}\\
  &\quad+ \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
- \frac{1}{2\alpha} \Tr\left(\bR_{11}\right)
% &\quad-\frac{\lambda^2}{2\alpha}
%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\bR_{11}\right) + \frac{\lambda^2}{2\alpha}\Tr\left( (\E[\grad\ell\grad\ell^\sT])^{-1} [\bR_{11},\bR_{00}] \bR^{-1}[\bR_{11},\bR_{00}]^\sT\right).
+  \KL(\nu_{\cdot|w} \| \cN(\bzero, \bR)) + \frac1\alpha \KL ( \mu_{\cdot| \btheta_0}\| \cN(\bzero,\bI_k)).\nonumber
\end{align}
Further, define the sets 
\begin{align}
\nonumber
\cuV(\bR, \cuB) := \Big\{\nu \in  \cuB &:
\sfA_\bV \succ \E_{\nu}[\bv\bv^\sT] \succ \sfsigma_{\bV},\;
\E_{\nu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\\
&\E_\nu[\grad \ell(\bv,\bv_0,w)(\bv,\bv_0)^\sT] + 
     \lambda (\bR_{11},\bR_{1,0}) =   \bzero,\;
 \nu_{(w)} = \P_w\; \Big\}
\end{align}
and
\begin{equation}
   \cuT(\cuA) := \{\mu \in\cuA : \mu_{(\btheta_0)} = \mu_{0}, \; 
   \sfA_{\bR} \succ \bR(\mu) \succ\sfsigma_\bR,\;
   \}.
\end{equation}
Then the following hold.
\begin{enumerate}
    \item Upper bound on the rate function:
\begin{align}
\nonumber
   &\limsup_{\delta\to 0 }\limsup_{n\to\infty}
   \frac1n\log (\E[Z_n(\cuA,\cuB, \sPi,\bw) \one_{\bw\in\cG_\delta}] )
   \le 
-\inf_{\mu\in \cuT(\cuA)}
   \inf_{\nu\in\cuV(\bR(\mu),\cuB)}\sup_{\bS\succ\bzero}
   \Phi_\cvx(\nu,\mu,\bS,\bR(\mu)).
\nonumber
\end{align}
\item For any $\mu\in\cuT(\cuA)$, $\nu \in\cuV(\bR(\mu),\cuB)$, 
\begin{equation}
\nonumber
    \sup_{\bS \succ\bzero} \Phi_\cvx(\mu,\nu, \bR(\mu),\bS) \ge 0
\end{equation}
with equality if and only if $(\mu,\nu)$ satisfy the critical point optimality condition 
of Definition~\ref{def:opt_FP_conds}.
\end{enumerate}
\end{theorem}


\begin{remark}[Comparison with related work]\label{rmk:ComparisonCVX}
As mentioned in the introduction, several earlier papers analyzed convex ERM in settings analogous to the one considered
here using AMP algorithms \cite{bayati2010combinatorial,donoho2016high,sur2019modern}. The idea is to construct an AMP algorithm minimize the empirical risk and establish sharp asymptotics for the minimzer in two steps: $(1)$~Characterize the high-dimensional asymptotics of AMP via state evolution (for any constant number of iterations); 
$(2)$~Prove that AMP converges to the minimizer in $O(1)$ iterations. 
For the case $k>1$, this approach was initiated in \cite{loureiro2021learning}, with important steps completed in \cite{ccakmak2024convergence}, albeit under the assumption of strong convexity. 

Even in the convex case, the Kac-Rice approach has some important advantages. Most notably, it can be used to characterizes \emph{all}
potential minimizers, 
via point 2 of Theorem \ref{thm:convexity}, as all solutions of the critical point optimality conditions of Definition \ref{def:opt_FP_conds}. 
Indeed, while Theorem \ref{thm:convexity} focuses on second-order strict minimizers, other minimizers can be accessed by a simple perturbation argument.
Further, this result emerges as a special case of a more general one, which is the main point of the paper.
\end{remark}
%
%
%*****************************************************************
%
\subsection{Rate trivialization under convexity}

\begin{theorem}
\label{thm:global_min}
With the definitions of Theorem~\ref{thm:convexity}, 
let Assumptions~\ref{ass:regime},~\ref{ass:loss},~\ref{ass:Data},~\ref{ass:theta_0},~\ref{ass:convexity},~\ref{ass:noise}
hold.

Assume there exists unique $\mu^\opt,\nu^\opt$ that satisfy 
 the critical point optimality condition of Definition~\ref{def:opt_FP_conds}, 
and 
there exist constants $C,c>0$ and (for each $n$) a critical point $\hat\bTheta_n\in\reals^{d\times k}$
such that, with high probability, 
%
\begin{align}
\hat\bTheta_n \in \cE(\bTheta_0) := \Big\{
\bTheta\in\R^{k\times d} &: 
\nabla^2 \hR_n(\bTheta)\succeq e^{-o(n)},
\,
 \bR(\hmu_{\sqrt{d}[\bTheta,\bTheta_0]})\prec C\bI,\; %\succ c,
%C \succ \E_{\hnu}[\bv\bv^\sT] \succ c, \;
\E_{\hnu}[\grad\ell \grad\ell^\sT] \succ c\bI
\Big\}\, ,\label{eq:SetUniqueness}
\end{align} 
%
where the expectation $\E_{\hnu}[\, \cdot\,]$ is with respect to $\hnu=\hnu_{\bX\hbTheta,\bX\bTheta_0,\bw}$.
(In particular, by convexity $\hat\bTheta_n$ is the unique empirical risk minimizer.)
Then the following hold:
\begin{enumerate}
    \item We have, in probability, 
    \begin{equation}
    \hmu_{\sqrt{d} [\hat\bTheta_n,\bTheta_0 ]} \stackrel{W_2}{\Rightarrow} \mu^\opt ,\quad\quad
    \hnu_{(\bX\hat\bTheta_n,\bX\bTheta_0,\bw)} \stackrel{W_2}{\Rightarrow} \nu^\opt\, .
    \end{equation}
    %
    \item The empirical spectral distribution of the Hessian at the minimizer converges weakly to $\mu_\star(\nu^\opt)$ in probability. Equivalently,  for all $z \in\bbH_+$, the following limit holds in probability:
    \begin{equation}
    \lim_{n\to\infty}\frac1{dk} (\bI_k \otimes \Tr) \left(\grad^2\hat R_n(\hat\bTheta_n) - z \bI_{dk} \right)^{-1} = 
      \alpha \, \bS_\star(\nu^\opt, z -\lambda )\, .
    \end{equation}
    \item Finally  a sufficient condition for  the solution to the critical point optimality condition  
    to be unique is stated in Theorem \ref{prop:simple_critical_point_variational_formula} below.
\end{enumerate}
\end{theorem}

\begin{remark}
   Notice that the set in Eq.~\eqref{eq:SetUniqueness} does not include any constraint on the
 Jacobian of $\bG$, which instead enters the set \eqref{eq:GeneralSet} of Theorem~\ref{thm:general}. 
 Indeed, a key step in the proof Theorem \ref{thm:global_min}  is to lower bound the minimum singular value of the Jacobian of $\bG$ in terms of the minimum singular values of $[\bTheta,\bTheta_0]$ and the Hessian of $\hR_n$.
 (See Lemma~\ref{lemma:jacobian_lb} in the appendix, or the statement of its result in Section~\ref{sec:pf_convex_results} )
\end{remark}

We finally show that the the solution of the critical point optimality condition of 
Definition~\ref{def:opt_FP_conds}  is unique for strictly convex ERM problems. 
A crucial observation is that, under Assumption~\ref{ass:convexity}
(with $\cuB = \cuP(\R^{k+k_0+1})$),  $\bR^\opt,\bS^\opt$ solve the fixed point equations in Definition~\ref{def:opt_FP_conds}, 
if they are a stationary point of the following min-max problem:
   \begin{equation}
   \label{eq:min_max_critical_pts}
       \min_{\bR \succeq \bzero} \max_{\bS\succeq\bzero} \left\{
       \E\left[\More_{\ell(\cdot, \bg_0,w)}(\bg;\bS)\right] - \frac1{2\alpha}\Tr(\bS^{-1}(\bR/\bR_{00})) 
       + \frac{\lambda}{2} \Tr(\bR_{11})\right\}
   \end{equation}
   where 
   \begin{align}
   \label{eq:moreau_def}
       \More_{\ell(\cdot, \bg_0,w)}(\bg;\bS)
       := \min_{\bx \in\R^{k}}\left\{ \frac12(\bx - \bg)^\sT\bS^{-1}(\bx - \bg) + \ell(\bx,\bg_0 ,w) \right\}
   \end{align}
   is the \emph{Moreau envelope}, and  the expectation is over $(\bg,\bg_0) \sim \cN(\bzero,\bR), w\sim \P_w$. (See the proof of
   Theorem \ref{prop:simple_critical_point_variational_formula} for a proof of this claim.)
   
 The next theorem shows that $\bR^{\opt}$ also corresponds to the minimizer of a convex program
 which is unique under strict convexity.
 %
\begin{theorem}
\label{prop:simple_critical_point_variational_formula}
\label{thm:simple_critical_point_variational_formula}
Under Assumption~\ref{ass:convexity}, any solution $\bR^\opt$ of critical point optimality condition of 
Definition~\ref{def:opt_FP_conds}   corresponds to a minimizer of the following
convex program. 
Let $(\Omega,\cF,\P)$ be a probability space with independent random variables 
$\bz_0\sim\normal(\bzero,\bI_{k_0})$,
$\bz_1\sim\normal(0,\bI_k)$ and
$w\sim\P_w$ defined on it, and additional independent randomness $Z\sim \Unif([0,1))$,
and $L^2 = L^2(\Omega\to\R^k,\cF,\P)$. Define
\begin{equation}
\nonumber
    %\cS(\bSigma) :=
    %\left\{(\bu, \bB)  \in L_2 \times  \sfS^k : 
    %\E[\bu\bu^\sT]  \preceq \bB,\;
    %\bB \bSigma^{-1} \bB \succeq \alpha \bI
    %\right\}\,.
    \cS(\bK) :=
    \left\{\bu  \in L_2 : 
    \E[\bu\bu^\sT]  \preceq \alpha^{-1}\bK^2
    %\bB \bSigma^{-1} \bB \succeq \alpha \bI
    \right\}\,.
\end{equation}
%
%\bns{I think the formula without $\bB$ that doesn't really seem convex is probably clearner to present. Thoughts?}
Further define the function $F:   \sfS^k_{\ge}\times \R^{k\times k_0}\to \R$ by
%
\begin{align}
%F(\bSigma,\bM):= \inf_{(\bu,\bB)\in\cS(\bSigma)} \E[\ell(\bu + \bSigma \bz_1 + \bM \bz_0, \bR_{00}^{1/2}\bz_0, W)]  + \lambda(\bSigma^2 + \bM\bM^\sT)\, .
F(\bK,\bM):= \inf_{\bu\in\cS(\bK)} \E[\ell(\bu + \bK \bz_1 + \bM \bz_0, \bR_{00}^{1/2}\bz_0, w)]  + \frac{\lambda}{2} \Tr(\bK^2 + \bM\bM^\sT) \, .\label{eq:FKM_Def}
\end{align}
%
Then 
\begin{enumerate}
\item $F$ is convex on $\sfS^k_{\succeq}\times \R^{k\times k_0}$.
\item The minimizers of $F$ are in one-to-one correspondence with the solutions $\bR^\opt$ of the
critical point optimality condition of  Definition~\ref{def:opt_FP_conds}, via 
%
\begin{equation}
\nonumber
 (\bR^{\opt}/\bR_{00}, \bR^\opt_{10} \bR_{00}^{-1/2}) =  ((\bK^\opt)^2,\bM^\opt)\, 
\end{equation}
%
where $\bK^\opt,\bM^\opt$ are the minimizers of $F(\bK,\bM).$
%
\item  $F$ is strictly convex if
either of the following conditions hold:
\begin{enumerate}
    \item $\lambda>0$. In this case the minimizer $(\bK,\bM)$ exists uniquely.
    \item $\lambda =0$ and $\ell(\, \cdot\, , \bv_0, w)$ is strictly convex for all $\bv_0,w$.
    In this case either the minimizer $(\bK,\bM)$ exists uniquely, or any minimizing sequence diverges.
\end{enumerate}
\end{enumerate}
\end{theorem}

%
%***********************************************************
%

\section{Application: Exponential families and multinomial regression}
We demonstrate how to apply the theory in the last section to regularizad maximum-likelihood
estimation (MLE) in exponential families. A large number of earlier works
studied the case of exponential families with a single parameter, a prominent example being logistic
regression \cite{sur2019modern,candes2020phase,montanari2019generalization,deng2022model,zhao2022asymptotic}. 


Here we obtain sharp high-dimensional asymptotics in the general case. 

\subsection{General exponential families}

Fix $m$ and $k = k_0$.
%and an exponential family with sufficient statistics $\bt$ and
%a reference measure $\nu_0$.
Given $\bt:\R^{m} \to \R^{k}$, and a reference measure $\nu_0$ on $\reals^m$, 
define the probability distribution on $\R^m$ 
%
\begin{align}
 \rP(\de\by|\bfeta)=  e^{\bfeta^\sT \bt(\by_i) - a(\bfeta)}
 \nu_0(\de\by)
 \, ,\;\;\;\;
 a(\bfeta) := \log\left\{\int e^{\bfeta^\sT \bt(\by_i)}
 \nu_0(\de\by)\right\}\, .\label{eq:ExpoDef1}
 \end{align}
 
We  assume to be given i.i.d. samples $(\bx_i,\by_i) \in \R^d\times \R^{m}$  for $i\le n$,
where $\bx_i\sim\normal(\bzero,\bI_d)$ and 
%
\begin{align}
    \P(\by_i\in S|\bx_i) = \rP(S|\bTheta_0^{\sT}\bx_i)\, .\label{eq:ExpoDef2}
\end{align}
%
We then consider the regularized  MLE defined as the minimizer of
%
\begin{equation}
     \hR_n(\bTheta) := \frac1n \sum_{i=1}^n \left\{ a(\bTheta^\sT \bx_i) -
      \<\bTheta^{\sT}\bx_i  ,\bt(\by_i)\>
      \right\} + \frac{\lambda}{2} \|\bTheta\|_F^2\, ,\label{eq:RiskExpo}
\end{equation}
%
for $\lambda \ge 0$.

In this case, the critical point optimality conditions of Definition \ref{def:opt_FP_conds}
reduce to the following set of equations for  $\bR\in\sfS^{2k}_{\succeq \bzero}$ 
and $\bS \in \sfS^k_{\succ \bzero}$:
   \begin{align}
    &\alpha \; \E[(\grad a(\bv) - \bt(\by)) (\grad a(\bv) -\bt(\by))^\sT]  = \bS^{-1}(\bR/ \bR_{00}) \bS^{-1}\, ,\label{eq:ExpoFP1}\\
    &\E\left[ (\grad a(\bv) - \bt(\by))(\bv^{\sT},\bg_0^\sT)\right] + \lambda (\bR_{11},\bR_{10}) = \bzero_{k\times (k+k)},\label{eq:ExpoFP2}
\end{align}
where, letting $\ell(\bv,\by):= a(\bv) - \<\bv, \bt(\by)\>$,
\begin{align}
\bv  = \Prox_{\ell(\,\cdot\,,\by)}( \bg; \bS),\quad\quad
\by \sim \rP(\by | \bfeta=\bg_0),\quad
[\bg^\sT,\bg_0^\sT]^\sT \sim \cN\left( \bzero_{k+k},\bR\right). \label{eq:ExpoFP3}
\end{align}

Here we state a simple general result under a strong convexity assumption.
In the next section we show that our general results can also be applied when strong 
convexity fails, by considering the case of multinomial regression.
%
\begin{proposition}\label{propo:Exponential}
Consider the exponential family of Eqs.~\eqref{eq:ExpoDef1}, \eqref{eq:ExpoDef2}, 
and assume that  $a_1\bI_m\preceq \nabla^2 a(\bfeta) \preceq a_2\bI_m$
for some constant $0<a_1\le a_2$, and that $n,d\to\infty$ with $n/d\to\alpha\in(1,\infty)$.
Further assume that $\bTheta^{\sT}_0\bTheta_0/d\to\bR_{00}\in\R^{k\times k}$ as $n,d\to\infty$.
Let $\hbTheta_n$ the the regularized  MLE  \eqref{eq:RiskExpo} (almost surely 
unique for all $n$ large enough).

Then \eqref{eq:ExpoFP1}, \eqref{eq:ExpoFP2} admit a unique solution $\bR^\opt, \bS^\opt$.
Further, if for
some $a_3 > 0$ independent of $n$; we have either
\begin{enumerate}
    \item 
    $\lambda_{\min}(\E_{\hnu}[\grad\ell \grad \ell^\sT])\ge a_3$ ; or
   \item  
   $\alpha > k$ and
   $\lambda_{\min}(\hbTheta_n^{\sT}\hbTheta_n/d)\ge a_3$,
\end{enumerate}
%$\lambda_{\min}(\hbTheta_n^{\sT}\hbTheta_n/d)\ge e^{-o(n)}$,
then we have, in probability, 
    \begin{equation}
    \hnu_{(\bX\hat\bTheta_n,\bY)} \stackrel{W_2}{\Rightarrow} \nu^\opt\, ,
    \end{equation}
    %
   where $\nu^\opt={\rm Law}(\bv,\by)$,
 and $\bv,\by$ are the random variables in Eq.~\eqref{eq:ExpoFP3}.
 
     If  in addition $\hmu_{\sqrt{d} \bTheta_0 } \stackrel{W_2}{\Rightarrow} \mu_0$,
    then the following holds, with   $\mu^\opt$  determined by $\bR^\opt$ as per Eq.~\eqref{eq:muopt},
     \begin{equation}
    \hmu_{\sqrt{d} [\hat\bTheta_n,\bTheta_0 ]} \stackrel{W_2}{\Rightarrow} \mu^\opt .
     \end{equation}
    %
\end{proposition}


\subsection{Revisiting multinomial regression}
\label{sec:Multinomial}

As a special case of exponential family, we consider multinomial regression 
with $k+1$ classes labeled $\{0,\dots,k\}$. 
The labels distribution 
and loss function have been already defined in Section \ref{sec:Intro}.

For $j\in\{1,\dots, k\}$,  $\be_j$ denotes the canonical basis vector in $\R^{k}$ 
and we let $\be_0 = \bzero_k$.  We encode class labels by letting 
$\by_i\in \{\be_0,\dots,\be_k\}$.
The regularized MLE minimizes the following risk function:
%
\begin{align}
\hR_n(\bTheta) = 
\frac1n \sum_{i=1}^n \bigg\{ -  \<\bTheta^\sT\bx_i,\by_i\>+
   \log\Big( 1+\sum_{j=1}^k e^{\<\be_j,\bTheta^{\sT}\bx_i\>}\Big)  
    \bigg\} +\frac{\lambda}{2}\|\bTheta\|_F^2\, .\label{eq:RiskMultinomial}
\end{align}
%
We also define the moment generating function $a:\reals^k\to\reals$ via
\begin{align}
    a(\bv) := \log\Big( 1+\sum_{j=1}^k e^{v_j}\Big)  \, .
\end{align}


For multinomial regression, Eqs.~\eqref{eq:ExpoFP1}, \eqref{eq:ExpoFP2}  take the even more explicit form
\begin{align}
\label{eq:FP_multinomial}
   \alpha \; \E[   (\bp(\bv)- \by )(\bp(\bv) - \by)^\sT ]   &=  \bS^{-1}(\bR/ \bR_{00})\bS^{-1},\\
   \E[   (\bp(\bv)- \by )(\bv^\sT,\bg_0^\sT)] &= \bzero\, ,\nonumber
   %\kas{=-\lambda \bR_{10}}
   %,\\
   %\E[   (\bp(\bv)- \by )\bv^\sT ] &= \bzero_{k\times k}
   %\kas{ \E[(I + \bS \bJ \bp(\bv))^{-1}] = (1-\frac1\alpha)\bI_{k\times k}+2\lambda \bS} ,
\end{align}
where $\bp(\bv) := \big( p_{j}(\bv) \big)_{j\in[k]}$ for $p_j$ defined in Eq.~\eqref{eq:MultiNomialDef}, and the random variables $\bv, \by$ 
have joint distribution defined by
\begin{align}
\bv  = \Prox_{a(\, \cdot\, )}( \bg + \bS \by; \bS),\quad
   \P\left(\by = \be_j\right)  = p_{j}(\bg_0),\; j\in\{0,\dots,k\},\quad
[\bg^\sT,\bg_0^\sT]^\sT \sim \cN\left( \bzero_{k+k_0},\bR\right)\, .\label{eq:FP_Multi3}
  \end{align}
  %


\begin{proposition}
\label{prop:multinomial}
Consider multinomial regression under the model of Eqs.~\eqref{eq:MultiNomialDef} with risk function \eqref{eq:RiskMultinomial} for $\lambda=0$. 
Assume that $n,d\to\infty$ with $n/d\to\alpha\in(0,\infty)$
and that $\bTheta^{\sT}_0\bTheta_0/d\to\bR_{00}\in\R^{k\times k}$ as $n,d\to\infty$, with 
$\bR_{00}\succ \bzero$ strictly.

For any $\alpha >1$ the following hold:
\begin{enumerate}
    \item 
If the system~\eqref{eq:FP_multinomial} has a solution $(\bR^\opt,\bS^\opt)$, then 
\begin{enumerate}
    \item  $(\bR^\opt,\bS^\opt)$  is
the unique solution of Eq.~\eqref{eq:FP_multinomial}.
\item We have,
for  $\hbTheta:= \argmin \hR_n(\bTheta)$ and some finite $C>0$, 
\begin{equation}
\label{eq:mle_exists_condition}
    \lim_{n\to\infty } \P\left( \hbTheta  \;\mbox{\rm exists},\; \|\hat\bTheta\|_F < C\right) = 1.
\end{equation}
\item Letting $\mu^\opt$ be  determined by $\mu_0,\bR^\opt$ as per Eq.~\eqref{eq:muopt},
and $\nu^\opt={\rm Law}(\bv,\by)$ with $\bv,\by$ defined by Eq.~\eqref{eq:FP_Multi3},
we have
    \begin{equation}
    \hmu_{\sqrt{d} [\hat\bTheta_n,\bTheta_0 ]} \stackrel{W_2}{\Rightarrow} \mu^\opt ,\quad\quad
    \hnu_{(\bX\hat\bTheta_n,\bX\bTheta_0,\bw)} \stackrel{W_2}{\Rightarrow} \nu^\opt\, .
    \end{equation}
    (For the first limit we assume $\hmu_{\sqrt{d} \bTheta_0 } \stackrel{W_2}{\Rightarrow} \mu_0$.)
    %
    \item The empirical spectral distribution of the Hessian at the minimizer 
    converges weakly to $\mu_\star(\nu^\opt)$ in probability. Equivalently, for all $z \in\bbH_+$,
    \begin{equation}
    \lim_{n\to\infty}\frac1{dk} (\bI_k \otimes \Tr) \left(\grad^2\hat R_n(\hat\bTheta_n) - z \bI_{dk} \right)^{-1} = 
      \alpha \, \bS_\star(\nu^\opt, z)
    \end{equation}
    in probability.
    \end{enumerate}
\item Conversely, if the system~\eqref{eq:FP_multinomial} does not have a solution, then, for all $C>0$,
\begin{equation}
\lim_{n\to\infty } \P\left( \hat\bTheta  \;\mbox{\rm exists},\; \|\hat\bTheta\|_F < C\right) = 0.
\end{equation}
\end{enumerate}
\end{proposition}

\begin{remark}
    A detailed study of high-dimensional asymptotics in multinomial regression was recently carried out in 
    \cite{tan2024multinomial}. However the techniques of \cite{tan2024multinomial} only allows to characterize
    the distribution of the MLE on null covariates. 
\end{remark}


\begin{figure}[t]
    \centering
     \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/multinomial/regularized/reg_misclassification_test_errors_error_k2_k02.pdf}
    \end{subfigure}
    % Subfigure (a)
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/multinomial/regularized/reg_test_errors_error_k2_k02.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/multinomial/regularized/reg_train_errors_error_k2_k02.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
       \centering
        \includegraphics[width=\textwidth]{figures/multinomial/regularized/reg_norms_error_k2_k02.pdf}
    \end{subfigure}

    
    \caption{Train/test error (log loss), estimation error, and classification error
    of ridge regularized multinomial regression, for $(k+1)=3$ symmetric classes,
    as a function of the regularization parameter $\lambda$ for several values of $\alpha$. 
    Empirical results are averaged over 100 independent trials,  with $d = 250$. 
    Continuous lines are theoretical predictions obtained by solving numerically the system \eqref{eq:FP_multinomial}.
   }
    \label{fig:regularized_error}
\end{figure} 

\begin{figure}[t]
    \centering
    
     \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/multinomial/error_vs_alpha/misclassification_errors_vs.pdf}
    \end{subfigure}
    % Subfigure (a)
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/multinomial/error_vs_alpha/test_errors_vs_alpha.pdf}
    \end{subfigure}
    % Subfigure (b)
    \begin{subfigure}[t]{0.45\textwidth}
       \centering
        \includegraphics[width=\textwidth]{figures/multinomial/error_vs_alpha/train_errors_vs_alpha_k2_k02.pdf}
    \end{subfigure}
    % Subfigure (c)
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/multinomial/error_vs_alpha/F_norm_vs_alpha.pdf}
    \end{subfigure}
    % Subfigure (d)
    
    \caption{Train/test error (log loss), estimation error, and classification error
    of multinomial regression, for $(k+1)=3$ symmetric classes, as a function of $\alpha$ for different values of $\bR_{00}$ specified in the text.
%    $
%    \bR_{00}^{(1)} 
%    =\bR_{00}^s(1,1/2), 
%    = \begin{bmatrix}
%        1&1/2\\1/2&1
%    \end{bmatrix},
      %\bR_{00}^{(2)}  = \bR_{00}^s(1, 0.9), 
     % = \begin{bmatrix}
     %   1&0.9\\0.9&1
     % \end{bmatrix},
      %\bR_{00}^{(3)} =  \bR_{00}^s(1,-1/2)$.
    %  \begin{bmatrix}
    %    1&-1/2\\-1/2&1
    %\end{bmatrix}
    Empirical results are averaged over 100 independent trials,  with $d = 250$. 
   }
    \label{fig:error_vs_alpha}
\end{figure} 


\begin{figure}[t]
    \centering
    % Subfigure (a)
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/multinomial/esd/esd3.pdf}
        \label{fig:subfig1}
    \end{subfigure}
    % Subfigure (b)
    \begin{subfigure}[t]{0.45\textwidth}
       \centering
        \includegraphics[width=\textwidth]{figures/multinomial/esd/esd5.pdf}
        \label{fig:subfig2}
    \end{subfigure}
    % Subfigure (c)
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/multinomial/esd/esd10.pdf}
  %      \label{fig:subfig3}
    \end{subfigure}
    % Subfigure (d)
    \begin{subfigure}[t]{0.45\textwidth}
   %    \centering
        \includegraphics[width=\textwidth]{figures/multinomial/esd/esd20.pdf}
        \label{fig:subfig4}
    \end{subfigure}
    
    \caption{Histograms of the empirical spectral distribution of the Hessian at the MLE
    for multinomial regression with three symmetric classes, in $d=250$ dimensions, aggregated over $100$ 
    independent realizations.
    From left to right, $\alpha = 3$, $\alpha = 5$, $\alpha=10$, and $\alpha = 20$. Blue lines represent the theoretical distribution derived from Proposition \ref{prop:multinomial}.}
    \label{fig:Spectrum}
\end{figure} 



\section{Numerical experiments}

In this section we compare the predictions of our theory to numerical simulations 
for ridge-regularized multinomial regression, presented in Section \ref{sec:Multinomial}.
We present the results of two types of experiments:
\begin{enumerate}
\item Experiments with synthetic data, distributed according to the model used in our analysis.
In this case, we observe very close agreement between experiments and asymptotic predictions already
when $d\gtrsim 250$. 
\item Experiments with the image-classification dataset Fashion-MNIST \cite{xiao2017fashion}. In this case, we construct the feature vectors $\bx_i$ by passing the images through a one-layer random neural network \cite{rahimi2008random}. Of course, the resulting vectors $\bx_i$ are non-Gaussian, but we nevertheless observe encouraging agreement with the predictions.
\end{enumerate}


\subsection{Synthetic data}

In Fig.~\ref{fig:regularized_error}, we consider the case of $(k+1)= 3$ classes which
are completely symmetrical (the optimal decision regions are congruent). A simple 
calculation reveals that this corresponds to the case $\bR_{00}= \bR_{00}^s(c)$
for some $c>0$,
where
\begin{align}
\bR_{00}^s(c) :=
\begin{bmatrix}
        c & c/2 \\ 
        c/2 & c
    \end{bmatrix}\,.
%\begin{bmatrix}
%        c & b\,c/2 \\ 
%        b\,c/2 & c
%    \end{bmatrix}\,.
\end{align}
In the simulations  of Fig.~\ref{fig:regularized_error}, we use $c=1$.
    We compare empirical results
    for estimation error, test error and train error (in log-loss). We observe that theory matches well with the numerical simulations even for moderate dimensions.

In Fig.~\ref{fig:error_vs_alpha}, we once again compare the same empirical and theoretical quantities, for different values of the ground truth parameters (encoded in $\bR_{00}$) as a function of $\alpha$. We consider 3 different values of $\bR_{00}$:
\begin{equation}
    \bR_{00}^{(1)} 
    := \begin{bmatrix}
        1&1/2\\1/2&1
    \end{bmatrix},
    \quad\quad
      \bR_{00}^{(2)}  
      := \begin{bmatrix}
        1&0.9\\0.9&1
      \end{bmatrix},\quad\quad
      \bR_{00}^{(3)} := 
      \begin{bmatrix}
        1&-1/2\\-1/2&1
    \end{bmatrix}.
\end{equation}


%log-loss test and train errors, classification test error, and estimation error
%on the geometry of the ground truth parameters (encoded in $\bR_{00}$), for different values of $\alpha$.



In Fig.~\ref{fig:Spectrum}, we consider the first setting above with $\bR_{00} = \bR_{00}^s(1)$, $\lambda =0$
and plot the empirical spectral distribution of the Hessian $\nabla^2\hR_n(\hbTheta)$
at the MLE $\hbTheta$. We compare this with the prediction of Proposition \ref{prop:multinomial}.
Again, the agreement is excellent. 

We observe that the spectrum structure changes significantly around $\alpha\gtrsim 5$,
developing two `bumps.' This is related to the structure of the population Hessian,
which is easy to derive:
%
\begin{align}
\nabla^2 R(\bTheta)\big|_{\bTheta=\bTheta_0} = \E[\bA(\bTheta_0^{\sT}\bx)]\otimes \bI_d
+\sum_{a,b=1}^k\E[\partial^2_{a,b} \bA(\bTheta_0^{\sT}\bx)]\otimes \bTheta_{\cdot,a}
\bTheta_{\cdot,b}^{\sT}\, ,\label{eq:HessianDecomposition}
%
\end{align}
%
where  $\bA:\R^k\to\R^{k\times k}$ is defined by
%
\begin{align}
 \bA(\bv) := \nabla_{\bv}^2\ell(\bv,\bv_{0},w)\big|_{\bv_0= \bv}\, .
\end{align}
%
Hence, neglecting the second term in Eq.~\eqref{eq:HessianDecomposition}
(which is low rank and contributes at most $k^2$ outlier eigenvalues), the eigenvalues should concentrate (for large $n$) around  the $k$ eigenvalues of $\E[\bH(\bTheta_0^{\sT}\bx)]$.
The actual structure of $\grad^2\hat R_n(\hat\bTheta)$ 
in Fig.~\ref{fig:Spectrum} is significantly different: eigenvalues do not concentrate, but 
we begin seeing a $k$-mode structure emerging for $\alpha\gtrsim 10$.


We summarize a few qualitative findings that emerge from these experiments and our theory:
\begin{enumerate}
\item In the noisy regime considered here,
unregularized multinomial regression is substantially suboptimal
even when the number of samples per dimension is quite large ($\alpha=10$). 
See Fig.~\ref{fig:regularized_error}.
\item Similarly, the structure of the Hessian and the empirical risk minimizer, is very different from classical theory, even when $\alpha = 20$. See Fig.~\ref{fig:Spectrum}.
\end{enumerate}

\subsection{Fashion-MNIST data}
\label{sec:Fashion-MNIST}


\begin{figure}[htbp]
    \centering

    % Column titles with smaller font
    {\tiny  % Adjust font size (can also use \tiny if needed)
    \makebox[0.32\textwidth]{ 250 features}  % First column title
    \makebox[0.32\textwidth]{ 350 features}  % Second column title
    \makebox[0.32\textwidth]{ 500 features}  % Third column title
    }

    \vspace{0.15cm}  % Space between titles and first row

    \begin{tabular}{@{}c@{} c@{} c@{}}   % Create a table-like structure for labels and images
        % First row: Classification
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/250/classification_tanh_250.pdf}
        \end{subfigure} &
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/350/classification_tanh_350.pdf}
        \end{subfigure} &
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/500/classification_tanh_500.pdf}
        \end{subfigure} \\

        \vspace{0.3cm}  

        % Second row: Test Errors
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/250/test_tanh_250.pdf}
        \end{subfigure} &
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/350/test_tanh_350.pdf}
        \end{subfigure} &
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/500/test_tanh_500.pdf}
        \end{subfigure} \\

        \vspace{0.3cm}  

        % Third row: Train Errors
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/250/train_tanh_250.pdf}
        \end{subfigure} &
                \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/350/train_tanh_350.pdf}
        \end{subfigure} &
        \begin{subfigure}[b]{0.335\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/500/train_tanh_500.pdf}
        \end{subfigure} \\

    \end{tabular}

    \caption{Performance of multinomial regression on the Fashion-MNIST dataset 
    for $(k+1)=3$ classes, as a function of $\alpha$. We construct feature vectors $\bx_i$ using a random one-layer neural network, as discussed in 
    Section \ref{sec:Fashion-MNIST}.}
    \label{fig:mnist_tanh}
\end{figure}



\begin{figure}[!ht]
    \centering
    \begin{tabular}{c@{}c@{}}  % Remove spacing between columns
        % Subfigure (a)
        \begin{subfigure}[t]{0.52\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/esd/esd_350_tanh_alpha10.0.pdf}
        \end{subfigure} &
        % Subfigure (b)
        \begin{subfigure}[t]{0.52\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/multinomial/tanh/esd/esd_350_tanh_alpha30.0.pdf}
        \end{subfigure} 
    \end{tabular}

    \caption{Histograms: Empirical spectral distribution of the Hessian at the MLE. Here we use use Fashion-MNIST data for $(k+1)=3$ classes, and construct feature vectors $\bx_i$ using a random one-layer neural network, as discussed in 
    Section \ref{sec:Fashion-MNIST}.}
    \label{fig:ESD_mnist}
\end{figure}











In Fig.~\ref{fig:mnist_tanh}, we report the result of our experiments on the Fashion MNIST
dataset \cite{xiao2017fashion}. This consists of overall $70000$ grayscale images of dimension $28\times 28$ belonging to $10$
classes (split into $60000$ images for training and $10000$ images for testing).
We select $k+1=3$ classes (pullovers, coats, and shirts), for a total of $N=18000$ training images. We standardize the entries of these images to get 
vectors $\{\bz_i\}_{i\le N}$, $\bz_i\in\reals^{d_0}$, $d_0=784$.
We then construct feature vectors $\bx_i$ by using a one-layer neural network with random weights. Namely
%
\begin{align}
\obx_i = \sigma(\bW\bz_i)\, ,
\end{align}
%
where $\bW= (W_{jl})_{j\le d,l\le d_0}$ is a matrix with i.i.d. entries $W_{jl}\sim\normal(0,1/d_0)$,
and $\sigma:\R\to\R$ is a (nonlinear) activation function which acts on vectors entrywise.
We then construct vectors $(\bx_i)_{i\le N}$ by `whitening' the $\{\obx_i\}_{i\le N}$. 
Namely, letting $\hbSigma:= N^{-1}\sum_{i\le N}\obx_i\obx_i^{\sT}$,
we define $\bx_i = \hbSigma^{-1/2}\obx_i$.
In Fig.~\ref{fig:mnist_tanh} we use $\sigma(x) = \tanh(x)$, but similar results are obtained with other activations.

For several values of $n$, we subsample $n$ out of the $N$ samples and fit multinomial regression and
average the observed test/train error and classification error to obtain the empirical data
in Fig.~\ref{fig:mnist_tanh}.

Since we do not know the ground truth, we fit multinomial regression to the whole 
$N$ samples $\{(y_i,\bx_i)\}_{i\le N}$ thus constructed, and assume that the resulting estimate coincides with
$\bTheta_0$. We then extract $\bR_{00} = \bTheta_0^{\sT}\bTheta_0$, which is the only unknown quantity
to evaluate the predictions of Proposition \ref{prop:multinomial}.


In Fig.~\ref{fig:ESD_mnist} we plot the empirical spectral distribution of the Hessian
at the MLE, and compare it with the theoretical prediction of Proposition \ref{prop:multinomial}.

These experiments suggest the following conclusions:
%
\begin{itemize}
    \item Figures \ref{fig:mnist_tanh} and  \ref{fig:ESD_mnist} shows reasonable quantitative agreement between theoretical predictions and experiments with real data. Given  the Gaussian covariates of Proposition \ref{prop:multinomial}, such agreement is surprising. While recent universality results \cite{hu2022universality,montanari2022universality,pesce2023gaussian} points in this direction, there is still much unexplained in this agreement.
    \item We constructed isotropic feature vectors $\bx_i$ through  the `whitening'
     step $\bx_i = \hbSigma^{-1/2}\obx_i$. We expect it to be  possible to generalize 
     our results to non-isotropic feature vectors, but leave it for future work.
\end{itemize}










%
%*****************************************************************
%*****************************************************************
%
\section{General empirical risk minimization: Proof of Theorem~\ref{thm:general}}
%\subsection{The Kac-Rice integral and its limit: Proof of Theorem~\ref{thm:general}}
\label{sec:pf_thm1}
\subsection{Applying the Kac-Rice integral formula}
\label{sec:pf_thm1_kr_integral}
As discussed in the introduction, the proof of Theorem~\ref{thm:general} relies on the Kac-Rice equation for counting the number of zeros of a Gaussian process. 
We recall the generic Kac-Rice formula in the following theorem, which is an adaptation of Theorem 6.2 from \cite{azais2009level}. The only modification is the introduction of the process $\bh$ and the associated assumptions and event $\{\bh \in\cO\}$. 
Its proof is essentially the same as the one in  \cite{azais2009level}.
\begin{theorem}[Modification of Theorem 6.2, \cite{azais2009level}]
\label{thm:kac_rice}
Let $\cT$ be an open subset of $\R^m$, and $\cO$ be an open subset of $\R^M$.
Let $\bz: \cT \to \R^m$ 
and $\bh:\cT  \to \R^M$ be random fields.
Assume that 
\begin{enumerate}
    \item  $\bz,\bh$ are jointly Gaussian.
    \item Almost surely the function $\bt\mapsto \bz(\bt)$ is of class $C^1$.
    \item For each $\bt\in \cT$, $\bz(\bt)$ has a nondegenerate distribution (i.e., positive definite covariance).
    \item We have 
    $\P(\exists \bt \in \cT, \bz(\bt) = \bzero, \bh(\bt) \in \partial\cO) = 0$.
    \item We have $\P(\exists \bt \in \cT, \bz(\bt) = \bzero, \det(\bJ_\bt \bz(\bt)) = \bzero) = 0$.
\end{enumerate}
Then for every Borel set $\cB$ contained in $\cU$, denoting 
$N_0(\cB) := \left|\left\{ \bt : \bz(\bt) =  \bzero, \; \bh(\bt) \in \cO \right\}\right|,$
we have
\begin{equation}
\E[N_0(\cB)]  = \int_{\cB} \E\left[|\det \bJ_\bt \bz(\bt)| \one_{\bh(\bt) \in \cO} \big| \bz(t) = \bzero\right] p_{\bz(t)}(\bzero) \de \bt,
\end{equation}
where $p_{\bz(\bt)}$ denotes the density of $\bz(\bt)$.
\end{theorem}
Our goal is to apply this formula to a Gaussian process whose zeros correspond to the critical points of the ERM problem of~\eqref{eq:erm_obj}.
Before we introduce this process, let us fix some notation to streamline the exposition:
For $j\in[k]$, let $\bell_j(\bV,\bV_0;\bw) \in\R^n$
and $\rho_j(\bTheta) \in\R^{d}$ 
be the columns of the matrices $\bL(\bV,\bV_0;\bw)$ and $\bRho(\bTheta)$
of~\eqref{eq:def_bL_bRho}, respectively.
Recall that we use $\bV \in\R^{n\times k},\bV_0\in\R^{n\times k_0}$ for the matrices whose columns are $\bv_j, \bv_{0,l}$, respectively, for $j\in[k],l\in[k_0]$.
For convenience, in what follows we'll often use the notation $\bbV := [\bV,\bV_0]$, and suppress the dependence on the arguments in the notation whenever it does not cause confusion.
Furthermore, it'll often be convenient to work with the empirical distributions $\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}$ of $\sqrt{d}[\bTheta,\bTheta_0]$ and $\hnu_{[\bbV,\bw]}$ of $[\bbV,\bw]$.
We will use the less cumbersome notation $\hmu,\hnu$ for these throughout.

Let $m_n := dk +nk +nk_0$. For fixed $\bw\in\R^n$, define the \emph{gradient process}
$\bzeta(\,\cdot\,;\bw) : \R^{m_n} \to \R^{m_n},$ 
\begin{equation}
\nonumber
    \bzeta(\bTheta,\bV,\bV_0;\bw) :=
    %\boldf(\btheta_1,\dots,\btheta_k, \bv_1,\dots,\bv_k , \bu_1,\dots,\bu_{k_0}):=
    \begin{bmatrix}
     \bX^\sT\bell_1(\bV,\bV_0,\bw) + n \brho_1(\bTheta)\\
     \vdots\\
     \bX^\sT \bell_k(\bV,\bV_0,\bw) + n \brho_k(\bTheta)\\
     \bX\btheta_1 - \bv_1\\
     \vdots\\
     \bX\btheta_k - \bv_k\\
     \bX\btheta_{0,1} - \bv_{0,1}\\
     \vdots\\
     \bX\btheta_{0,k_0} - \bv_{0,k_0}
    \end{bmatrix}.\label{eq:GradProcess}
\end{equation}
%
From the definition of $\bzeta$, it's easy to note that for any $\bw$,
\begin{equation}
    \{(\bTheta,\bbV) : \bzeta(\bTheta,\bbV; \bw) = \bzero \} = \{(\bTheta,\bbV): \grad_\bTheta \hat R_n(\bTheta) = \bzero, \bbV = \bX [\bTheta,\bTheta_0]\}.
\end{equation}
Let
%
\begin{equation}
\label{eq:bH_def}
\bH(\bTheta,\bbV;\bw) := \bH_0(\bbV; \bw) + n \grad^2 \rho(\bTheta), \quad\quad
    \bH_0(\bbV;\bw) := \left(\bI_k \otimes \bX\right)^\sT \bSec(\bbV;\bw)\left(\bI_k \otimes \bX\right)\, ,
\end{equation}
\begin{equation}
   \bSec(\bbV;\bw) := \begin{pmatrix}
\bSec_{i,j}(\bbV;\bw)
   \end{pmatrix}_{i,j \in[k]}
,\quad
    \bSec_{i,j}(\bbV;\bw):= \Diag\left\{(\partial_{i,j}\ell(\bbV;\bw))\right\}\in\R^{n\times n}, \quad i,j\in[k]\, .
    \label{eq:SecDef}
\end{equation}
Recall the notation used here for the hessian of the regularizer
 $\grad^2\rho(\bTheta) = \Diag\{\rho''(\sqrt{d}\Theta_{i,j})\}_{i,j \in [d]\times [k]} \in\R^{dk\times dk}$.
% 
Observe the relation between these quantities and the Hessian of the empirical risk at the points $\bzeta = \bzero$. We have
%
\begin{align}
\label{eq:inclusion_1_f=0}
\{\bzeta =\bzero \} \;\; \subseteq \;\;
\{\bbV = \bX(\bTheta,\bTheta_0) \} \;\;\subseteq\;\;
\left\{\nabla^2\hR_n(\bTheta) = \frac1n \bH(\bTheta,\bbV;\bw) \right\}\,.
\end{align}
%

In order to study the expected size of $Z_n$ defined in Eq.~\eqref{eq:number_of_zeros_main}, we
would like to apply
Theorem~\ref{thm:kac_rice} to $\bzeta$ with the constraint $\bH_0/n + \grad^2\rho \succ \sfsigma_\bH$ on the Hessian, along with the additional constraints of $\cZ_n$ defined in~\eqref{eq:set_of_zeros_main} on the index set.
However, the process $\bzeta$ is \emph{degenerate}: as we show in Appendix~\ref{section:kac_rice}, its covariance has rank $m_n -r_k$ for $r_k := k(k+k_0)$
while Theorem~\ref{thm:kac_rice} requires the dimension of the index set to be the same as the rank of the covariance of the process. On the other hand, by the KKT conditions, the points $(\bTheta,\bbV)$ corresponding to critical points of $\hat R_n$ belong 
to the $m_n -r_k$ dimensional manifold 
$\cM_0 := \{\bG(\bTheta,\bbV) = 0\}$
where $\bG$ was defined in~\eqref{eq:def_G}.
Furthermore, with some algebra (Lemma~\ref{lemma:eig_vecs_NS_Sigma}), one can show that the mean $\bmu(\bTheta,\bbV)$ of the process $\bzeta(\bTheta,\bbV)$ is orthogonal to the nullspace of the covariance 
 at any point $(\bTheta,\bbV)$ in this manifold, so that restricting to this manifold gives a process of dimension $m_n - r_k$ to which we apply Theorem~\ref{thm:kac_rice} to.
%, the index set of the gradient process
%becomes $(m_n - r_k)$-dimensional.
The following lemma provides the necessary extension of
this theorem to our setting.
%
\begin{lemma}[Kac-Rice on the manifold]
\label{prop:kac_rice_manifold}
\label{lemma:kac_rice_manifold}
Fix $\bw \in\R^n$ (suppressed in the notation).
For $\cuA,\cuB$ as in Assumption~\ref{ass:sets}, define the \emph{parameter manifold}
\begin{align}
\label{eq:param_manifold_def}
\cM(\cuA,\cuB, \sPi) := \Big\{
   (\bTheta,\bbV) &: \hmu\in\cuA,\;
   \hnu\in\cuB,\;
   \bG(\bbV,\bTheta) =  \bzero,\;
   \sfA_\bR\succ \bR(\hmu_{\sqrt{d}[\bTheta,\bTheta_0]}) \succ\sfsigma_\bR,\;\\
   &\quad 
\sfA_\bV \succ \E_{\hnu}[\bv\bv^\sT] \succ \sfsigma_{\bV}, \;
\E_{\hnu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;
\sigma_{\min}\left( \bJ_{(\bbV,\bTheta)} \bG^\sT\right) > n\,\sfsigma_{\bG}
    \Big\}.
\end{align}
Let $\bB_{\bSigma}(\bTheta,\bbV)$ be a basis matrix for the column space of the covariance of $\bzeta$ at $(\bTheta,\bbV)$ (defined in Corollary~\ref{cor:proj}), and 
$\bz(\bTheta,\bbV) := \bB_{\bSigma}(\bTheta,\bbV)^\sT \bzeta(\bTheta,\bbV).$
Let $p_{\bTheta,\bbV}(\bzero)$ be the density of $\bz(\bTheta,\bbV)$, and finally, let
\begin{align}
\nonumber
Z_{0,n}(\cuA,\cuB,\sPi) := \Big|\Big\{
   (\bTheta,\bbV) &: \hmu\in\cuA,\;
   \hnu\in\cuB,\;
   \bzeta = \bzero,\;
   \bG(\bbV,\bTheta) =  \bzero,\;
   \frac1n \bH \succ \sfsigma_\bH,\; 
   \sfA_\bR\succ \bR(\hmu_{\bTheta,\bTheta_0}) \succ\sfsigma_\bR,\;\\
   &\quad 
\sfA_\bV \succ \E_{\hnu}[\bv\bv^\sT] \succ \sfsigma_{\bV}, \;
\E_{\hnu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;
\sigma_{\min}\left( \bJ_{(\bbV,\bTheta)} \bG^\sT\right) > n\,\sfsigma_{\bD}
    \Big\}\Big|.
\end{align}
Then under Assumptions~\ref{ass:loss},~\ref{ass:regularizer},~\ref{ass:sets}, we have 
\begin{align}
\label{eq:kr_eq_manifold}
\E[Z_{0,n}(\cuA,\cuB,\sPi)|\bw] 
    &=\int_{(\bTheta,\bbV) \in \cM(\cuA,\cuB,\sPi)}  \E\left[\left| \det (\de \bz(\bTheta,\bbV) )\right|
    \one_{\bH \succ n\sfsigma_\bH}
    \big| \bz  = \bzero, \bw\right] p_{\bTheta,\bbV}(\bzero)  \de_\cM V
\end{align}
where the latter is an integral over the manifold $\cM$ 
(with the volume element denoted by $\de_{\cM}V$), and $\de \bz(\bTheta,\bbV) : T_{(\bTheta,\bbV)}\cM \mapsto \R^{m_n - r_k}$ is the differential which we identify with a $\R^{(m_n - r_k)\times (m_n - r_k)}$ matrix.
\end{lemma}
The proof of this lemma is deferred to Section~\ref{sec:proof_of_kac_rice_on_manifold}.
Directly from the definitions above, we have
\begin{equation}
\label{eq:inclusion_2_f=0}
    \{\grad \widehat R_n(\bTheta) = \bzero \} \;\; \subseteq\;\; \{\bzeta =\bzero,\; \bG(\bbV,\bTheta) = \bzero\}.
\end{equation}
This inclusion along with the one of 
Eq.~\eqref{eq:inclusion_1_f=0}
implies then that for $\cuA,\cuB$
in their respective domains, we have 
\begin{equation}
      Z_n(\cuA,\cuB, \sPi)
      \le Z_{0,n}(\cuA,\cuB, \sPi)
\end{equation}
for $Z_n$ as in Eq.~\eqref{eq:number_of_zeros_main}.
So to derive the bound of Theorem~\ref{thm:general}, we will study the asymptotics (up to first order in the exponent) of the integrand in Eq.~\eqref{eq:kr_eq_manifold}.

\subsection{Integration over the manifold}
To control the integral in Eq.~\eqref{eq:kr_eq_manifold} over the parameter manifold, we'll upper bound the integral by a volume integral over the \emph{$\beta$-blow up} of $\cM$ defined by
\begin{equation}
    \cM^\up{\beta}(\cuA,\cuB,\sPi) := \{\bu\in\R^{m_n}: \exists\;\bu_0\in\cM(\cuA,\cuB,\sPi),\quad \norm{\bu-\bu_0}_2\le \beta\}
\end{equation}
for some $\beta >0$. Since we are interested in the asymptotics of the integral in Eq.~\eqref{eq:kr_eq_manifold},  we would like to choose $\beta$
independent of $n$ but small enough so that the 
volume integral is a good approximation of the manifold integral.

The needed regularity of  $\cM(\cuA,\cuB,\sPi)$ is guaranteed by 
Assumption~\ref{ass:params}, which states that  minimum singular value of the Jacobian of the constraint $\bG$ defining $\cM$ is lower bounded by a constant $\sfsigma_\bG$ independent of $n$. The estimate of manifold integrals by volume integrals is formalized  by
the following lemma.
%
\begin{lemma}[Manifold integral lemma]
\label{lemma:manifold_integral} 
Let $r_k := k(k+k_0)$, $m_n:= nk + nk_0 + dk$.
Let $f: \cM^\up{1} \subseteq \R^{m_n}\rightarrow \R$ be a nonnegative continuous function. 
There exists a constant $C = C(\sfA_{\bV},\sfA_{\bR})>0$ that depends only on $(\sfA_{\bV},\sfA_{\bR})$, such that for positive
\begin{equation}
   \beta_n  \le \frac{C(\sfA_{\bV},\sfA_{\bR}) \sfsigma_{\bG}(n)^3}{r_k^6} %\wedge 1,
\end{equation}
we have
\begin{equation}
        \int_{(\bTheta,\bbV)\in \cM} f(\bTheta,\bbV) \de_\cM V
        \le
        \Err_{\sblowup}(n)
         \,
        e^{\beta_n\;\norm{\log f}_{\Lip,\cM^{(1)}}}
        \int_{(\bTheta,\bbV)\in \cM^{(\beta_n)}} f(\bTheta,\bbV)\de(\bTheta,\bbV),
\end{equation}
where the multiplicative error $\Err_{\sblowup}(n)$ is given explicitly in Lemma~\ref{lem:intg-tube} and satisfies
\begin{equation}
    \lim_{n\to\infty}  \frac1n \log 
    \Err_{\sblowup}(n)
 = 0.
\end{equation}
%\begin{equation}
    %E(\beta,n,k) := \left(\frac1{1 - \beta\;r_k^2 C }\right)^{(m-r_k)/2}
        %\left(\frac{r_k^{5/2} C}{\beta(\sfsigma_{\bG} - \beta C r_k^2)}\right)^{r_k}
%\end{equation}
\end{lemma}
The proof of this lemma is deferred to Section~\ref{section:manifold_integration} of the appendix.

The determinant term appearing in Eq.~\eqref{eq:kr_eq_manifold}
is that of the differential $\de \bz(\bTheta,\bbV)$ defined on the tangent space of $\cM$. 
We relate this to the Euclidean Jacobian of $\bzeta(\bTheta,\bbV)$ which will be defined on $\cM^\up{1} \subseteq R^{m_n}$.
Let $\bB_{\bT(\bTheta,\bbV)}$ and $\bB_{\bSigma(\bTheta,\bbV)}$ be a basis for the tangent space of $\cM$ and the column space of $\bSigma(\bTheta,\bbV)$ at $(\bTheta,\bbV)$, respectively. Further let
$\bB_{\bT^c(\bTheta,\bbV)}$ and $\bB_{\bSigma^c(\bTheta,\bbV)}$ be basis matrices for the complement of these spaces.
Suppressing $(\bTheta,\bbV)$ in the arguments, we have in this notation $\det(\de\bz)  = \det\left(\bB_{\bSigma}^\sT \bJ \bzeta \bB_\bT\right)$.
Since the codimension of  the tangent space of $\cM$, and of the column space of $\bSigma$ are $r_k=O(1)$, we expect that $\log |\det(\de \bz(\bTheta,\bbV))| = \log \det|\bJ \bzeta(\bTheta,\bbV)|+O(1)$ for large $n$. In turn, we can directly compute
    \begin{equation}
        \bJ \bzeta = \begin{bmatrix}
            n\grad^2 \rho& (\bI_k\otimes\bX)^\sT\bSec &
            (\bI_k\otimes \bX)^\sT\tilde\bSec\\
            \bI_k\otimes\bX& -\bI&\bzero\\
            \bzero &\bzero &-\bI
        \end{bmatrix}
\end{equation}
to see that $|\det(\bJ \bzeta)| =  \det\left(\bH \right)$,
where $\bH$ was defined in~\eqref{eq:bH_def}.
Characterizing the asymptotic spectral density of $\bH$ will allows us to determine the asymptotics $\E[\det(\bH)]$ using Gaussian concentration. 
Of course, we actually need to modify such calculation to correctly account for the 
conditioning on $\bz=\bzero$.

We will formalize this analysis in the below, while deferring most technical details to the appendix.


\paragraph{Relating $\det(\de \bz)$ to $\det(\bH)$.}
As noted previously (and stated in Lemma~\ref{lemma:eig_vecs_NS_Sigma} of the appendix), the projection of $\bzeta$ onto the nullspace of $\bSigma$ vanishes,
so that
$\bB_{\bSigma^c}^\sT \bJ\bzeta\bB_{\bT} = \bzero$ and hence
\begin{equation}
\label{eq:det_projection}
\det(\de\bz)  = \det\left(\bB_{\bSigma}^\sT \bJ \bzeta \bB_\bT\right)  =\frac{ \det\left( \bJ \bzeta\right)}
   {\det( \bB_{{\bSigma}^c}^\sT \bJ \bzeta \bB_{\bT^c})}\, .
\end{equation}
Since the dimensions of the tangent space and the nullspace of $\bSigma$ are $r_k$, we then have
\begin{equation}
\label{eq:det_to_min_singular_value}
\big|\det( \bB_{{\bSigma}^c}^\sT \bJ \bzeta \bB_{\bT^c})\big|
\ge
   {\sigma_{\min}(\bB_{{\bSigma}^c}^\sT \bJ \bzeta \bB_{\bT^c})^{r_k}}
   \ge 
   {\sigma_{\min}(\bJ \bzeta)^{r_k}}.
\end{equation}
hence, we can upper bound the determinant of $\de \bz$ given a lower bound on the minimum singular value of $\bJ\bzeta$. The following lemma furnishes the latter in terms of the minimum singular value of $\bH$.

\begin{lemma}[Lower bound on the singular values of $\bJ_{(\bTheta,\bbV)} \boldf $]
\label{lemma:lb_singular_value_Df}
Under Assumption~\ref{ass:loss} and~\ref{ass:regularizer}, we have the bound
   \begin{equation}
\sigma_{\min}(\bJ_{(\bTheta,\bbV)} \boldf) \ge \;\frac{\sigma_{\min}(\bH)}{\Err_\sigma(\bX)}
   \end{equation}
   where the multiplicative error is given by
\begin{equation}
\Err_\sigma(\bX) :=
C(\sfA_\bR)
\left(
\|(\bX^\sT\bX)^{-1}\|_\op^{3/2} +1
\right)
\left(\|\bX^\sT\bX\|_\op^{7/2}+ n^3\right)
%    E(\bX^\sT\bX;\sfK,\tilde\sfK) := C(\|\bX^\sT\bX\|_\op^{5/2} \|(\bX^\sT\bX)^{-1}\|_\op^{3/2} (\sfK^2 + \tilde{\sfK}^2 + 1 ))
\end{equation}
for some constant $C>0$ depending only on $\sfA_{\bR}$.
%and $\sOmega$.
\end{lemma}

When combined with equations~\eqref{eq:det_projection} and~\eqref{eq:det_to_min_singular_value}, this lemma then gives the bound 
\begin{equation}
\label{eq:simplfied_det_bound}
\E\left[\left| \det (\de \bz(\bTheta,\bbV) )\right|
    \one_{\bH\succ n\sfsigma_\bH}
    \big| \bz  = \bzero\right] 
    \le 
\frac{1}{(n \sfsigma_{\bH})^{r_k}}
\E\left[\left| \det (
\bH)\right|
    \one_{\bH \succ n\sfsigma_\bH} 
    \Err_\sigma(\bX)^{r_k}
    \big| \bz = \bzero\right].
\end{equation}

Now letting
\begin{equation}
\label{eq:integrand_of_interest}
  f(\bTheta,\bbV)  :=
\frac{1}{(n \sfsigma_{\bH})^{r_k}}
\E\left[\left| \det (
\bH)\right|
    \one_{\bH \succ n\sfsigma_\bH} 
    \Err_\sigma(\bX)^{r_k}
    \big| \bz = \bzero\right]
%\E\left[\left| \det (\de \bz(\bTheta,\bbV) )\right|
%    \one_{\bH \succ n\sfsigma_\bH}
%    \big| \bz  = \bzero, \bw\right]
    p_{\bTheta,\bbV}(\bzero),
\end{equation}
the function $f$ bounds the integrand in Eq.~\eqref{eq:kr_eq_manifold}. 
We'll apply Lemma~\ref{lemma:manifold_integral} to obtain an integral over the blow-up $\cM^\up{1}$. First, we'll 
an upper estimate for the density term $p_{\bTheta,\bbV}(\bzero)$ in the next section.

   % which will allow us to upper bound $\tilde f$ with some function $f$ defined and Lipschitz on the blow-up $\cM^\up{1}$, such that $f \ge \tilde f$  on $\cM$.
   % 
   % and study the asymptotics 
   % (up to first order in the exponent) of this bound. This is done in the next section.
%In the next section, we begin this task by giving an upper estimate for the density term $p_{\bTheta,\bbV}(\bzero)$.

%The integrand of interest here is of course
%%we must now study the asymptotics (up to first order in the exponent) of the function
%\begin{equation}
%\label{eq:integrand_of_interest}
%  \tilde f(\bTheta,\bbV)  :=
%\E\left[\left| \det (\de \bz(\bTheta,\bbV) )\right|
%    \one_{\bH \succ n\sfsigma_\bH}
%    \big| \bz  = \bzero, \bw\right] p_{\bTheta,\bbV}(\bzero)
%\end{equation}
%appearing in Eq.~\eqref{eq:kr_eq_manifold}, which is defined on $\cM$.
%%and bound the Lipschitz constant of the logarithm of this function.
%To use this lemma to obtain an upper bound on the integral of $\tilde f$, we'll first upper bounding the determinant term
%$\E\left[\left| \det (\de \bz(\bTheta,\bbV) )\right|
%    \one_{\bH \succ n\sfsigma_\bH}
%    \big| \bz  = \bzero, \bw\right]$ by a term defined on $\cM^\up{1}\subseteq \R^{m_n}$. 



\subsection{Computing and bounding the density term}
For any  
$(\bTheta,\bbV)$, the term $p_{\bTheta,\bbV}(\bzero)$ corresponds to the density function of a Gaussian  random variable at $\bzero$.
With some algebra, this can be shown to be  (cf. Lemma~\ref{lemma:density})
\begin{equation}\label{eq:DensityAtZero}
  p_{\bTheta,\bbV}(\bzero) 
    :=
\frac{
    \exp\left\{-\frac{1}2\left(n^2
\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bRho\right) + \Tr(\bbV \bR^{-1}\bbV) + n\Tr\left(\bRho (\bL^\sT\bL)^{-1}\bL^\sT \bbV \bR^{-1}(\bTheta,\bTheta_0)^\sT\right)
%    \Tr\left(\bbV
%\bR^{-1}(\bTheta)
%    \bbV^\sT\right)
    \right)\right\}}
    {
\det^*(2\pi\bSigma(\bTheta,\bbV))^{1/2}
}
    \, ,
\end{equation}
with $\det^*$ denoting the product of the non-zero eigenvalues, and the covariance $\bSigma(\bTheta,\bbV)$ is given by
\begin{equation}
\bSigma:= 
   \begin{bmatrix}
       \bL^\sT \bL \otimes \bI_{d}  & \bM & \bM_0\\
       \bM^\sT  & \bTheta^\sT \bTheta \otimes \bI_n & \bTheta^\sT\bTheta_0 \otimes \bI_n\\
       \bM_0^{\sT} & \bTheta_0^\sT\bTheta  \otimes \bI_n& \bTheta_0^{\sT}\bTheta_0 \otimes \bI_n
   \end{bmatrix} ,
\end{equation}
where

\begin{equation}
    \bM :=  \begin{bmatrix}
        \btheta_1\bell_1^\sT & \dots  & \btheta_k \bell_1^\sT\\
        \vdots  &   & \vdots \\
        \btheta_1\bell_k^\sT & \dots  & \btheta_k \bell_k^\sT\\
    \end{bmatrix}
    \in \R^{d k\times n k},\quad
    \bM_0 :=
    \begin{bmatrix}
        \btheta_{0,1}\bell_1^\sT & \dots  & \btheta_{0,k_0} \bell_1^\sT\\
        \vdots  &   & \vdots \\
        \btheta_{0,1}\bell_k^\sT & \dots  & \btheta_{0,k_0} \bell_k^\sT\\
        \end{bmatrix} \in \R^{d{k} \times n k_0 }.
\end{equation}

From the low-rank structure of $\bM$ and $\bM_0$, one would expect that 
$\det^*(\bSigma(\bTheta,\bbV))$ is approximately given by $\det(\bL^\sT\bL \otimes \bI_d)\cdot\det(\bR(\bTheta) \otimes \bI_n)$ for large $n$ and fixed $k,k_0$. 
If we use this heuristics in Eq.~\eqref{eq:DensityAtZero} rewrite
various quantities 
in terms of the empirical measures $\hmu$ and $\hnu$, we reach the conclusion
of the following lemma. (We refer to Appendix~\ref{sec:density_bound} for its proof.)
%This in turn allows us to upper bound the density term by the following expression.
 \begin{lemma}[Bounding the density]
\label{lemma:density_bounds}
Define the Gaussian densities
\begin{align}
    p_{1}(\bTheta) := \frac{d^{dk/2}}{(2\pi)^{dk/2}} \exp\left\{-\frac{d}{2} \Tr\left(\bTheta^\sT\bTheta\right)\right\},\quad\quad
    p_{2}(\bbV) := 
    \frac1{(2\pi)^{n(k+k_0)/2}}
\exp\left\{-\frac12
    \Tr\left(\bbV^\sT
    \bbV\right)
    \right\}.
\end{align}
Under Assumptions \ref{ass:regime} to \ref{ass:params} of Section~\ref{sec:assumptions}, there exist constants $n_0, C >0$ depending only on 
%$\sOmega$ and 
$k,k_0$, such that for all $(\bTheta,\bbV) \in\cM(\cuA,\cuB)$ and $n> n_0,$
\begin{equation}
   p_{\bTheta,\bbV}(\bzero)\le C(k,k_0) \,
   (\alpha_n^{1/2} d)^{-dk}
   e^{n h_0(\hmu,\hnu;\alpha_n)}  \;
    p_1(\bbV) p_2(\bTheta)
\end{equation}
where $\alpha_n:=n/d$ and 
%
\begin{align}
\nonumber
h_0(\mu,\nu;\alpha) :=& - \frac{1}{2\alpha} \log \det(\E_\hnu[\grad\ell\grad\ell^\sT]) 
+\frac{1}{2 \alpha}\Tr(\bR_{11}(\hmu))
-
\frac{1}2 \Tr(\E_\hmu[\grad \rho\grad\rho^\sT] \E_\hnu[\grad\ell\grad\ell^\sT]^{-1})
- \frac{1}2 \log\det (\bR(\hmu))
\\
&+\frac{1}2 \Tr\big(\E_\hnu[\bbv\grad\ell^\sT] \E_\hnu[\grad\ell\grad\ell^\sT]^{-1} 
\E_\nu[\grad\ell\bbv^\sT]
\bR(\hmu)^{-1}
\big)  + \frac{1}{2}\Tr\big((\bI_k - \bR(\hmu)^{-1}) \E_\hnu[\bbv\bbv^\sT]\big).
\end{align}
 \end{lemma} 

In the above lemma, we deliberately wrote the bound in terms of product of a density over $(\bTheta,\bbV)$  (the term $p_1(\bbV) p_2(\bTheta)$), 
and a term that depends on $(\bTheta,\bbV)$ only through their empirical distributions.
 Recalling $f$ defined in Eq.~\eqref{eq:integrand_of_interest}, when the bound on the density is combined with Lemma~\ref{lemma:manifold_integral}, and 
assuming we can show that $\|\log f\|_{\Lip}\le C n$ for some constant $C>0$ independent of $n$,
then we have
\begin{align}
\label{eq:expectation_over_bV_bTheta}
    \lim_{n\to\infty} \frac1n &\log \E[Z_{0,n}| \bw] \\
    &\le
    \lim_{n\to\infty} \frac1n\log
    \E_{\substack{\bTheta\sim p_1\\ \bbV\sim p_2}}\Big[
\E\left[\left| \det (\bH )\right|
\Err_\sigma(\bX)^{r_k}
    \one_{\bH \succ n\sfsigma_\bH}
    \big| \bz  = \bzero, \bw\right] (\alpha_n^{1/2} d)^{-dk} e^{n h_0(\hmu,\hnu;\alpha_n)} \, \one_{(\bTheta,\bbV) \in \cM^\up{\beta}}
    \Big]\, .\nonumber
\end{align}
%
The details of bounding the Lipschitz constant are left to Appendix~\ref{sec:kr_asymptotics}.
We will continue our analysis by estimating the right-hand side of Eq.~\eqref{eq:expectation_over_bV_bTheta}.
In particular, in the next section we will consider 
the conditional expectation involving the determinant.




\subsection{Analysis of the determinant term}


\paragraph{Asymptotic spectral density of $\bH$.}
Recall now the definition of $\mu_\star(\nu,\mu)$ introduced in Section~\ref{sec:definitions}.
The following proposition affirms that $\mu_\star$ is the limit of the empirical spectral distribution of $\bH$, uniformly over $\bbV,\bTheta.$ We recall that $\bH_0=\bH_0(\bbV;\bw)$ is the Hessian of the loss part of
the risk, cf. Eq.~\eqref{eq:bH_def}. We also use the notation $\bH_0=\bH_0(\hnu_n)$, since this depends only on
the empirical distribution of the rows of $[\bbV,\bw]$.
%
\begin{proposition}
\label{prop:uniform_convergence_lipschitz_test_functions}
Under Assumptions of Section~\ref{sec:assumptions}, we have for any Lipschitz function $g:\R\to\R$,
  \begin{equation}
  \lim_{\substack{n\to\infty\\n/d \to \alpha}}
  \sup_{\substack{\bw\in\R^n\\(\bbV,\bTheta)\in\cM(\cuA,\cuB)}}\left|\frac1{dk}\E\left[\Tr \,g\left(\frac1n\bH(\bTheta,\bbV;\bw)\right)\right]
      - \int g(\lambda) \mu_\star(\hnu,\hmu)(\de \lambda)
      \right| = 0.
  \end{equation}
%
  Moreover, if $\hnu \Rightarrow \nu$ weakly in probability for some $\nu\in\cuP(\R^{k+k_0+1})$,
  we have for any fixed $z\in\bbH_+$ the convergence in probability
\begin{equation}
\label{eq:ST_convergence_in_P_seq_measures}
     \frac1{dk} (\bI_k \otimes \Tr) \left(\bH_0(\hnu_n) - z\bI_{dk}\right)^{-1} \to \alpha \bS_\star(\nu,z)\, ,
\end{equation}
where for any $z\in \bbH_+$,  $\bS_{\star}(\nu,z)$ is defined as the unique solution of  Eq.~\eqref{eq:fp_eq}.
\end{proposition}
%
The proof of this proposition is deferred to Appendix~\ref{sec:RMT},
which also proves uniqueness of  $\bS_\star(\nu,z)$ in Appendix \ref{app:sec:UniquenessSstar}. There, we analyze the empirical Steiltjes transform of $\bH_0$  
via a leave-one-out approach that is similar to the one used in deriving the asymptotic density of Wishart matrices $\bX^\sT\bX$~\cite{BaiSilverstein}. Of course the difference here is that the Stieltjes transform is an element of $\C^{k\times k}$ (often called the operator-valued Stieltjes transform in the free probability literature~\cite{speicher2019non}). As a result, the analysis requires additional  care compared to the scalar case to deal with the additional complications arising from non-commutativity. 
Finally, the empirical spectral distribution of $\bH$ frollows from the one of $\bH_0$
via a free probability argument.
See Appendix~\ref{sec:RMT} for details.

\paragraph{Conditioning on $\bz=\bzero$ and concentration of the determinant.}
We outline the main steps in bounding the conditional expectation of the determinant appearing in the right-hand side of Eq.~\eqref{eq:simplfied_det_bound}.
We leave most technical details to Appendix~\ref{sec:determinant_bound}.

First, note that
since the mean of $\bzeta$ is in the column space of $\bSigma(\bTheta,\bbV)$ for any $(\bTheta,\bbV)$, conditioning on $\bz = \bzero$ is equivalent to conditioning 
on $\bzeta = \bzero$. 
The latter meanwhile is  to conditioning on
$\{\bL^\sT\bX = - n\bRho^\sT,\; \bX[\bTheta,\bTheta_0] = \bbV\}$. 
So letting $\bP_{\bTheta}, \bP_\bL$ be the projections onto the columns spaces of $[\bTheta,\bTheta_0],\bL$ respectively, we have on $\{\bzeta = 0\}$
\begin{equation}
    \bX = \bP_\bL^\perp \bX \bP_\bTheta^\perp - \bL(\bL^\sT\bL)^{-1} \bRho^\sT \bP_{\bTheta}^\perp  + \bbV\bR^{-1} [\bTheta,\bTheta_0]^\sT.
\end{equation}
%

Hence for any measurable function $g$,
\begin{equation}
\label{eq:conditioning_generic}
    \E[g(\bX) | \bzeta = 0] = \E[g(\bX + \bDelta_{0,k})]
\end{equation}
for some matrix $\bDelta_{0,k} = \bDelta_{0,k}(\bTheta,\bbV)$ 
satisfying
%\begin{align}
%    &\rank(\bDelta_{0,k}) \le r_k,\\
%    &\norm{\bDelta_{0,k}}_\op \le \norm{\bX}_\op.
%\end{align}
\begin{align}
    &\rank(\bDelta_{0,k}) \le 4(k+k_0),\\
    &\norm{\bDelta_{0,k}}_\op \le C\sqrt{n} \max\left(\frac{\norm{\bX}_\op}{\sqrt{n}} ,
    \frac{\sfA_{\bV}}{\sfsigma_{\bR}^{1/2}} + \frac{\sfA_{\bR}}{ \sfsigma_{\bL}^{1/2}}\right)
\end{align}
for some constant $C>0$ independent of $n$.
%only on $\sOmega$.

The identity \eqref{eq:conditioning_generic} allows us to bound the expectation in
Eq.~\eqref{eq:expectation_over_bV_bTheta}. Namely,
recalling
that $\bH = (\bI_k \otimes \bX)^\sT\bSec(\bI_k \otimes \bX) + n \grad^2\rho$, we have
\begin{align}
&
\E\left[\left| \det (\bH )\right|
\Err_\sigma(\bX)^{r_k}
    \one_{\bH \succ n\sfsigma_\bH}
    \big| \bz  = \bzero, \bw\right]
%\E[|\det \big(\de \bz\big)|\one_{\bH \succ n\sfsigma_\bH} \big| \bzeta = 0 ]
{\le}
%\frac{1}{(n\sfsigma_\bH)^{r_k}}
\E\left[  \big|\det\left(\bH + \bDelta_{1,k}\right)\big|    \one_{\{\bH + \bDelta_{1,k} \succ n\sfsigma_\bH\}}
\Err_{\sigma}(\bX + \bDelta_{0,k})^{r_k}
\right]
\label{eq:concentration_decomp_0_main}
\end{align}
for some $\bDelta_{1,k}$ of rank at most some $r'_k = O(r_k)$. 
The effect of the error term $\Err_\sigma$ will be bounded in Appendix~\ref{sec:determinant_bound} where we show that it contributes at most a multiplicative factor that is polynomial in $n$, and is thereby exponentially trivial.
Meanwhile, the interlacing theorem implies that, for any $i\ge dk-r'_k$,
\begin{equation}
\label{eq:interlacing}
    \lambda_{i+r'_k}(\bH+  \bDelta_{1,k})\leq \lambda_{i}(\bH),\quad \lambda_i(\bH + \bDelta_{1,k})\leq \|\bH + \bDelta_{1,k}\|_\op.
\end{equation}
As a consequence, the deteminant involving $\bH$ in Eq.~\eqref{eq:concentration_decomp_0_main} can be estimated as follows.
For any $\tau_1 >0$,
  \begin{align}
\label{eq:log_det_to_log_eps}
      \det((\bH  + \bDelta_{1,k})/n)=& \exp\left\{\sum_{i=1}^{dk}\log 
      (\lambda_i(\bH+  \bDelta_{1,k} )/n)\right\}\\
      %=& \sum_{i=1}^{dk}\log^{(\eps_\bH)}
      %(\lambda_i(\bH+\bE_k))\\
      \leq& \exp\left\{\sum_{i=1}^{dk -r'_k}\log \lambda_i(\bH /n) +
      r_k'\log\left( \frac{\|\bH + \bDelta_{1,k}\|_\op}{n}\right)\right\}\\
      %\leq & \sum_{i=1}^{dk} \log^{(\eps)} \lambda_i(\bH/n) - 3r_k\log\eps + 3r_k\log(2\norm{\bH}_\op)\\
      \leq& \exp\left\{\Tr\left(\log^{(\tau_1)}(\bH /n) \right)\right\} \cdot
\left( \frac{\|\bH+ \bDelta_{1,k}\|_\op}{n \tau_1}\right)^{r'_k},
  \end{align}
  where $\log^\up{\tau_1}(t) := \log(\tau_1 \vee t).$
As we show in Section~\ref{sec:pf_lemma_CE_bound} the appendix, 
the term $\|\bH+ \bDelta_{1,k}\|_\op$ is at most polynomial in $n$ uniformly over $(\bTheta,\bbV) \in \cM$ with high probability.
Therefore, for the indicator involving the minimal singular value of $\bH$
in Eq.~\eqref{eq:concentration_decomp_0_main}, we note that 
$$\{\bH + \bDelta_{1,k} \succ n \sfsigma_\bH\}\subseteq
    \left\{
    \big|\left\{ \lambda \in \spec\left(\bH /n \right) : \lambda \leq 0\right\} \big| < r'_k
    \right\}= \{ \hmu_{\bH}(-\infty,0) < r'_k/n\}$$
    where $\hmu_{\bH}$ is the empirical spectral measure of $\bH/n$.
    
With the proper formalization of the above, along with a concentration argument showing that Lipschitz functions of $\bH$ concentrate super-exponentially, we reach the following lemma which summarizes the results of analyzing the determinant.
We'll use the bound in Eq.~\eqref{eq:simplfied_det_bound} to state this more concisely.
%\bns{Add blurb about concentration and perturbation explaining the proof of the following lemma, and how we get the hard constraint on the ASD}
%\bns{Remark about passing to hard constraint.}

\begin{lemma}[Bounding the conditional expectation of the determinant]
 \label{lemma:CE_bound}
Fix $\tau_0,\tau_1 \in (0,1)$, and $\bw$ satisfying $\|\bw\|_2 \le \sfA_{\bw}\sqrt{n}$.
Then under Assumptions \ref{ass:regime} to \ref{ass:params} of Section~\ref{sec:assumptions}, there exist constants $C,c>0$,
%depending only on $\sOmega$
and $C_0(\tau_0)$ depending on $\tau_0$, both independent of $n$, such that for all $n > C_0(\tau_0)$, 
\begin{enumerate}
\item For any $(\bbV,\bTheta) \in\cM(\cuA,\cuB)$ satisfying $\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0)) < \tau_0,$ we have
\begin{align}
&\E[|\det \big(\de \bz\big)|\one_{\bH\succ n\sfsigma_\bH} \big|\bzeta=\bzero,\bw]
\le
n^{dk}
\Bigg(\exp\left\{
\E\left[\Tr\log^{(\tau_1)}\left(\bH/n\right)\Big| \bw\right] +  \frac{C n^{1-1/4}}{\tau_1}
\right\}
+ \exp\left\{ -n^{5/4}\right\}
\Bigg)
\end{align}

\item For $(\bbV,\bTheta)\in\cM$ satisfying 
$\mu_{\star}(\hnu,\hmu)((-\infty, -\tau_0)) \ge \tau_0,$
\begin{align}
\E[|\det \big(\de \bz\big)|\one_{\bH \succ n\sfsigma_\bH} \big|\bzeta = 0 ,\bw]
& \le C \exp \left\{ 
    -c 
    \tau_0^2
    n^{3/2}
    \right\}.
\end{align}
\end{enumerate}
\end{lemma}



\subsection{Asymptotics of the integral}
Finally, the analysis of the previous three sections can be applied to upper bound the asymptotics of the integral of Eq.~\eqref{eq:kr_eq_manifold}.
By recalling that $Z_n \le Z_{0,n}$, and combining the bound of Eq.~\eqref{eq:expectation_over_bV_bTheta} along with Lemma~\ref{lemma:CE_bound} to bound the conditional expectation, we arrive at the following lemma.

\begin{lemma}[Upper bound on the Kac-Rice integral]
\label{prop:asymp_1}
\label{lemma:asymp_1}
Fix $\tau_0,\tau_1\in (0,1)$. Let $\cG\subseteq \Ball_{\sfA_{\bw}\sqrt{n}}^n(\bzero)$ be any measurable subset.
Define 
\begin{equation}
\nonumber
\phi_{\tau_1}(\nu,\mu; \alpha)
:=
 \frac{k}{2\alpha}\log(\alpha)+
\frac{k}{\alpha}
\int \log^{(\tau_1)}(\lambda) \mu_{\star}(\nu,\mu)(\de \lambda) + h_0(\mu,\nu; \alpha),\quad\quad\textrm{and}
\end{equation}
%
\begin{equation}
\nonumber
    \cuM^{(\beta)}(\cuA,\cuB,\sPi) := \left\{ (\mu,\nu) :  \exists\;  (\mu_0,\nu_0) \in \cuM(\cuA,\cuB,\sPi) \; \textrm{s.t.} \;  W_2(\nu,\nu_0) < \beta, W_2(\mu,\mu_0) < \beta \right\},\quad\quad\textrm{where}
\end{equation}
\begin{align}
\nonumber
\cuM(\cuA,\cuB,\sPi) := \Big\{(\mu,\nu) \in \cuA\times \cuB &:\;
\sfA_{\bR} \succ \bR(\mu) \succ\sfsigma_\bR,\;
\sfA_\bV \succ \E_{\nu}[\bv\bv^\sT] \succ \sfsigma_{\bV},\;
\E_{\nu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;\\
&\quad\quad\quad
\E_\nu[\grad \ell(\bv,\bv_0,w)(\bv,\bv_0)^\sT]+ 
     \E_\mu[\rho'(\btheta) (\btheta, \btheta_0)^\sT] =   \bzero_{k\times (k+k_0)}
\Big\}.
\nonumber
\end{align}
%\begin{align}
%\phi_{\tau_1}(\nu,\mu)
%&:=
% \frac{k}{2\alpha}\log(\alpha)+
%\frac{k}{\alpha}
%\int \log^{(\tau_1)}(\lambda) \mu_{\star}(\nu,\mu)(\de \lambda)
%  - \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
%+ \frac{1}{2\alpha} \Tr\left(\bR_{11}(\mu)\right) 
%-\frac1{2}\log\det(\bR(\mu))
%\\
%   &\quad-\frac1{2\alpha}
%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\mu[\grad \rho \grad \rho^\sT]\right) + \frac1{2\alpha}\Tr\left(\E\left[\bbv\grad\ell^\sT\right] (\E[\grad\ell\grad\ell^\sT])^{-1}\E[\grad\ell\bbv^\sT] \bR(\mu)^{-1}\right) \\
%&\quad+ \frac12 \Tr\left((\bI_k - \bR(\mu)^{-1})\E[\bbv\bbv^\sT]\right).
%\end{align}
Under Assumptions~\ref{ass:regime},\ref{ass:loss},\ref{ass:regularizer} and~\ref{ass:sets}, there exists a constant $C(\sfA_{\bR},\sfA_{\bV})>0$ depending only on $(\sfA_{\bR},\sfA_{\bV})$, such that
for any $\beta\in(0, 1)$,
we have
\begin{align}
\limsup_{n\to\infty}\frac1n\log\E[Z_n(\cuA,\cuB) \one_{\bw\in\cG}]
&\le
   \limsup_{n\to\infty}\frac1n\log
   \E_\bw\E_{\substack{\bTheta\sim p_1\\ \bbV\sim p_2}}\left[e^{n\phi_{\tau_1}(\hnu,\hmu)}
   \one_{\{\mu_{\star}(\hmu,\hnu)((-\infty, -\tau_0]  ) < \tau_0\} \cap \cuM^{(\beta)}}
   \right] \one_{\bw \in\cG }.
\end{align}
\end{lemma}

The technical details of the proof are left to Appendix~\ref{sec:proof_prop_asymp_1}.

At this point, 
the statement of Theorem~\ref{thm:general} follows from this lemma as a consequence of Sanov's theorem and Varadhan's lemma, from which the $\KL$-divergence terms in the formula \eqref{eq:PhiGen} for  $\Phi_{\gen}$ 
appear as the rate function for large deviations of the empirical measures  $\hmu$ and $\hnu$.
The final details of this are left to Appendix~\ref{sec:proof_thm1_large_deviations}.







\section{Convex empirical risk minimization: Proofs of Theorems \ref{thm:convexity}, \ref{thm:global_min}}
\label{sec:pf_convex_results}

\subsection{Proof of Theorem~\ref{thm:convexity}}

\subsubsection{Simplified variational upper bound under convexity: Proof of point 
\textit{1}}
The upper bound  of Theorem~\ref{thm:convexity} is a special case of Theorem~\ref{thm:general} under the additional assumption of convexity.
In order to derive $\phi_\cvx$ from $\phi_\gen$ of Theorem~\ref{thm:general} 
under Assumption~\ref{ass:convexity}, we begin with the following identity
easily verifiable from the definition of the $\KL$-divergence: denoting $\bbv^\sT := [\bv^\sT,\bv_0^\sT],$  we have
\begin{align*}
   -\frac12 \log \det(\bR)  + \frac12 \Tr((\bI_{k+k_0} - \bR^{-1}) \E_{\nu}[\bbv\bbv^\sT]) -
   \KL(\nu_{\cdot|w} \| \cN(\bzero, \bI_{k+k_0}))
= - \KL(\nu_{\cdot|w} \| \cN(\bzero, \bR)).
\end{align*}
%
Meanwhile, for $\rho(t) = \lambda t^2/2$, so that $\grad_{\btheta}\rho(\btheta) = \lambda \btheta,$  one obtains after using the constraint $\E[\grad\ell \bbv^\sT + \grad \rho [\btheta^\sT,\btheta_0^\sT]] = \bzero$  that
\begin{align*}
   &-\frac1{2\alpha}
\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\mu[\grad \rho \grad \rho^\sT]\right) + \frac1{2\alpha}\Tr\left(\E_{\nu}\left[\bbv\grad\ell^\sT\right] (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\nu[\grad\ell\bbv^\sT] \bR(\mu)^{-1}\right)\\
&=
   -\frac{\lambda^2}{2\alpha}
\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\bR_{11}(\mu)\right) + \frac{\lambda^2}{2\alpha}\Tr\left((\E_\nu[\grad\ell\grad\ell^\sT])^{-1}
 [\bR_{11}(\mu),\bR_{10}(\mu)] \bR(\mu)^{-1} [\bR_{11}(\mu),\bR_{10}(\mu)]^\sT
\right)\\
&= 0\, ,
\end{align*} where the last equality follows by noting that $[\bR_{11},\bR_{10}] \bR^{-1} [\bR_{11},\bR_{10}]^\sT = \bR_{11}$.
Comparing $\Phi_\cvx$ with $\Phi_\gen$, we see that what remains now is to obtain the following bound on the logarithmic potential.

\begin{lemma}[Variational principle for the log potential]
\label{lemma:variational_log_pot}
Let
\begin{equation}
    \mu_{\star,\lambda}(\nu) :=
    \mu_{\MP}(\nu) \boxplus  \delta_{\lambda}.
\end{equation}
   Under Assumption~\ref{ass:convexity} , for any $\nu\in\cuB$, and $\lambda \ge0$,
   we have
    \begin{equation}
    \label{eq:variational_log_pot}
        k\int\log(\zeta ) \mu_{\star,\lambda}(\nu) (\de\zeta)
\le \inf_{\bS\succ\bzero} K_{-\lambda}(\bS;\nu),
    \end{equation}  
    where
\begin{equation}
    K_z(\bQ;\nu):= -\alpha z \Tr(\bQ) + \alpha \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bQ) ]  - \log\det(\bQ) - k (\log(\alpha) + 1).
\end{equation}
\end{lemma}
Since $\rho''(t) = \lambda$ under Assumption~\ref{ass:convexity}, note that for any $(\nu,\mu) \in\cuP(\R^{k+k_0+1})\times \cuP(\R^{k+k_0})$, $\mu_\star(\mu,\nu) = \mu_{\star,\lambda}(\nu).$
This finally shows $\phi_\gen(\nu,\mu,\bR) \le \sup_{\bS\succ\bzero}\phi_\cvx(\nu,\mu,\bR,\bS)$ as claimed, giving the claim of point~\textit{1} of the theorem.

\begin{remark}
Note that for $z\in\bbH^+$, by directly differentiating $K_z$ of Lemma~\ref{lemma:variational_log_pot} with respect to $\bQ$ we can easily see that $\bS_\star$ defined by~Eq.~\eqref{eq:fp_eq} is a critical point of $K_z(\bQ;\nu)$.
In the proof of Lemma~\ref{lemma:variational_log_pot} which is deferred to 
Section~\ref{sec:log_pot_proof} of the appendix,
 we show that under the convexity assumption of Assumption~\ref{ass:convexity} this critical point is the  minimizer of $K_z(\bQ).$
\end{remark}

\subsubsection{The critical point optimality condition: Proof of point~\texorpdfstring{$\textit{2}$}{2}}
Let $\bC = \bC(\bR) := \bR/\bR_{00}$ for ease of notation.
Let us decompose $\Phi_\cvx$ as $\Phi_\cvx = \Phi_{\cvx,1} + \Phi_{\cvx,2}$ for
\begin{align}
\Phi_{\cvx,1}(\mu)
&:=\frac{k}{2\alpha}
- \frac{1}{2\alpha} \Tr\left(\bR_{11}(\mu)\right)
+ \frac1{2\alpha} \log \det(\bC(\bR(\mu)))
 + \frac1\alpha \KL ( \mu_{\btheta| \btheta_0}\| \cN(\bzero,\bI_k)).\\
    \Phi_{\cvx,2}(\nu,\bR,\bS) &:=
    -\lambda\Tr(\bS)  +
    \frac{1}{2\alpha} \log\det\left(\E_\nu\left[ \grad \ell\grad\ell^\sT\right]\right)
+ 
\frac1{\alpha}\log\det\bS 
- \E_{\nu}\left[\log \det \left(\bI_k + \grad^2 \ell^{1/2} \bS \grad^2 \ell^{1/2}\right)\right]\nonumber\\
&\quad+\frac{k}{2\alpha} 
+ \frac{k}{2\alpha} \log(\alpha)
-\frac1{2\alpha} \log\det\left(
\bC(\bR)
\right)   + \KL\left(\nu_{\bv,\bv_0 | w}\| \cN(\bzero, \bR)\right).
\end{align}
%
We'll treat each of $\Phi_{\cvx,1}$ and $\Phi_{\cvx,2}$ separately, and show that they are both strictly positive, unless $(\mu,\nu) = (\mu^\opt,\nu^\opt)$, which case both are identically equal to $0$.

\noindent\textbf{Lower bounding $\Phi_{\cvx,1}$:}
For  fixed $\bR$, consider  the following minimization problem
\begin{align}
    \inf_{\mu : \bR(\mu) = \bR}  \frac1{\alpha}\KL(\mu_{\cdot| \btheta_0} \| \cN(\bzero,\bI_k)) \, ,
\end{align}
%
where it is understood that the ``outer" expectation in the conditional divergence is taken with respect to measure $\mu_{(\btheta_0)} = \mu_0$. By maximum entropy property of Gaussian measures, we obtain that the minimizer is such that $\mu_{\cdot|\btheta_0}=
\normal(\bA\btheta_0,\bB)$ for certain matrices $\bA$, $\bB$. Enforcing the 
constraint  $\bR(\mu) = \bR$ we obtain that the minimizer is in fact
$\mu^\opt$ 
of Definition~\ref{def:opt_FP_conds}.
We can then directly compute for the fixed $\bR$,
\begin{align}
     \frac1{\alpha}\KL( \mu^\opt_{\cdot|{\btheta_0}} \| \cN(\bzero,\bI_k))  
     &= 
     \frac1{2\alpha}\E_{\btheta_0\sim\mu_0}\left[ -\log\det(\bC(\bR)) - k 
     + \Tr(\bR_{11} - \bR_{10} \bR_{00}^{-1} \bR_{10})+
      \btheta_0^\sT \bR_{00}^{-1}\bR_{01}\bR_{10}\bR_{00}^{-1} \btheta_0\right]
      \nonumber\\
      &=- \frac1{2\alpha}\log\det(\bC(\bR)) - \frac{k}{2\alpha} + \frac{1}{2\alpha}\Tr(\bR_{11}).
\end{align}
Consequently, for any $\mu$ as in the statement of the theorem, we have
\begin{align}
    \Phi_{\cvx,1}(\mu) &\ge \inf_{\mu \in\cuP(\R^{k+k_0})} \Phi_{\cvx,1}(\mu) = 
    \Phi_{\cvx,1}(\mu^\opt)\\
    &=
    \inf_{\bR\succeq \bzero} \left\{
\frac{k}{2\alpha}
- \frac{1}{2\alpha} \Tr\left(\bR_{11}(\mu)\right)
+ \frac1{2\alpha} \log \det(\bC(\bR(\mu))) 
- \frac1\alpha \KL(\mu^\opt \| \cN(\bzero,\bI_k))
    \right\} = 0,
\end{align}
with equality if and only if $\mu = \mu^\opt$.

\noindent \textbf{Lower bounding $\Phi_{\cvx,2}$:}
To lower bound $\Phi_{\cvx,2}$, we'll rewrite the divergence term as a divergence involving the distribution $\nu^\opt$ of the proximal operator in the Definition~\ref{def:opt_FP_conds}.
To derive the density of $\nu^\opt$, it is useful to observe that for $f(\bv,\bv_0, w)$ convex in $\bv$ for fixed $\bv_0,w$,
 the map $\bz \mapsto \Prox_{f(\cdot, \bv_0, w)}(\bz; \bS)$  is invertible  for any $\bS \succeq \bzero_{k\times k}$, with inverse given by
\begin{equation}
\Prox_{f(\cdot,\bv_0,w)}^{-1}(\bv; \bS) = \bS\grad f(\bv,\bv_0,w)+\bv,
\end{equation}
which can be derived from the first order conditions
\begin{align}
    %\rho_{\bQ,\bv_0,w}(\bv)=&\min_{\bx\in\R^k}\left( \frac12 (\bx-\bv)^\bT\bQ^{-1}(\bx-\bv)+\rho(\bx,\bv_0,w)\right)\in\R\\
     \bS\grad f(\Prox_{f(\cdot;\bv_0,w)}(\bz;\bS),\bv_0,w)=\bz- \Prox_{f(\cdot,\bv_0,w)}(\bz; \bS)
\end{align}
where $\grad f \in\R^{k}$  is the gradient of $f$ with respect to the first $k$ variables.

In what follows, we use $(\bv,\bv_0,w)$, $\bv\in\R^{k},\bv_0\in\R^{k_0},w\in\R$ to denote random variables whose distribution is $\nu^\opt.$
To that end, let $\bg,\bg_0$ be jointly Gaussian as in Definition~\ref{def:opt_FP_conds}, and $w\sim\P_w$. 
For any $\bS,\bR \succ\bzero$,
denoting
$p^\opt_{\bS,\bR}(\bv| \bv_0, w)$ the conditional density of 
$\Prox(\bg; \bS, \bg_0, w) = \Prox_{\ell(\,\cdot\,;\bg_0,w)}(\bg; \bS)$ 
given $w$,$\bv_0=\bg_0$,
we find that
\begin{align}
    p^\opt_{\bS,\bR}(\bv|\bv_0, w) =& \exp\left\{ -\frac12 (\bS\grad\ell(\bv,\bv_0, w )+\bv-\bmu(\bv_0,\bR))^\sT\bC(\bR)^{-1}(\bS\grad\ell(\bv,\bv_0, w )+\bv-\bmu(\bv_0,\bR))\right\}\nonumber\\
    &\quad\quad(2\pi)^{-k/2}\det(\bC(\bR))^{-1/2}
     \det\left(\bI_k+\grad^2\ell(\bv,\bv_0,w)^{1/2}\bS \grad^2\ell(\bv,\bv_0,w)^{1/2}\right),
\end{align}
where $\bmu := \bmu(\bv_0, \bR) := \bR_{10}\bR_{00}^{-1}\bv_0$.
Using this identity, the KL
divergence of the conditional measure $\nu_{\bv|\bv_0,w}$ with respect to $\cN(\bmu,\bC)$ can be written as
\begin{align*}
    \KL\left(\nu_{\cdot|\bv_0,w}\|  \cN(\bmu,\bC)\right) 
    &=\KL\left(\nu_{\cdot|\bv_0,w}\Big\|  p^\opt_{\bS,\bR}(\cdot | \bv_0, w) \right) 
    +
     \E_\nu\left[\log\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right]\\
     &\quad\quad-\frac12\E_\nu\left[\grad \ell^\sT \bS \bC(\bR)^{-1} \bS\grad \ell\right]
     -\E_\nu\left[
    \grad \ell^\sT \bS \bC(\bR)^{-1} \left(\bv - \bmu\right)
     \right].
\end{align*}
Recall that $\nu\in\cuV(\bR)$
implies that $\E[\grad \ell \cdot (\bv^\sT,\bv_0^\sT)] + \lambda (\bR_{00},\bR_{01}) = \bzero,$ whence
\begin{align*}
\E_\nu\left[
    \grad \ell^\sT \bS \bC(\bR)^{-1} \left(\bv - \bmu\right)
     \right]  &=  \Tr\left( \bS \bC(\bR)^{-1} \E_{\nu}[\bv \grad\ell^\sT - \bR_{10}\bR_{00}^{-1}\bv_0\grad\ell^\sT]\right)  \\
&=
-\lambda\Tr\left( \bS 
(\bR_{00} - \bR_{10}\bR_{00}^{-1}\bR_{01})^{-1}
(\bR_{00} - \bR_{10}\bR_{00}^{-1}\bR_{01})\right) \\
&= -\lambda \Tr(\bS).
\end{align*}
This, along with the chain rule for the KL-divergence and the expansion of the conditional KL above gives
\begin{align}
\nonumber
  \KL\left(\nu_{\bv, \bv_0|w}\|  \cN(\bzero,\bR)\right) &= 
\KL\left(\nu_{\bv|\bv_0,w}\|  p_{\bS,\bR}^\opt(\cdot | \bv_0, w) \right) 
    +
     \E_\nu\left[\log\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right]\\
  &\quad\quad
  -\frac12\E_\nu\left[\grad \ell^\sT \bS \bC(\bR)^{-1} \bS\grad \ell\right]
  +\KL\left(\nu_{\bv_0|w}\|  \cN(\bzero,\bR_{00})\right) + \lambda \Tr(\bS).
\end{align}
By substituting this equality for the KL term into $\Phi_{\cvx,2}$ and carrying out the appropriate cancellations, 
this shows that for any $\mu,\nu$ as in the statement, 
\begin{align}
   \sup_{\bS\succ\bzero}\Phi_{\cvx,2}(\nu,\bR(\mu),\bS) 
     &=
    \sup_{\bS\succ\bzero} \bigg\{\frac1{2\alpha} \log\det \left(
    \E_\nu[\grad \ell \grad \ell^\sT ] \bS^2 \bC(\bR(\mu))^{-1}
    \right)-\frac12\E_\nu[\grad\ell^\sT \bS \bC(\bR(\mu))^{-1}\bS \grad\ell]\nonumber\\
    &\hspace{15mm}+\frac k{2\alpha}\log(\alpha e) +\KL(\nu_{\bv|\bv_0,w}\|p^\opt_{\bS,\bR(\mu)})+\KL(\nu_{\bv_0|W}\|\cN(\bzero,\bR(\mu)_{00}))\bigg\}\\
&\stackrel{(a)}{\ge}
    \sup_{\bS\succ\bzero} \left\{M(\bS;\nu, \bR(\mu)) \right\}\, ,\nonumber
\end{align}
where
\begin{equation}
    M(\bS;\nu, \bR) = \frac1{2\alpha} \log\det \left(
    \E_{\nu}[\grad \ell \grad \ell^\sT ] \bS^2 \bC(\bR)^{-1}
    \right)-\frac12\E_{\nu}[\grad\ell^\sT \bS \bC(\bR)^{-1}\bS \grad\ell]+\frac k{2\alpha}\log(\alpha e).
\end{equation}
The inequality in $(a)$ follows from non-negativity of the KL-divergence and holds with equality if and only if $\nu_{\bv,\bv_0| w} = \nu_{\bv,\bv_0|w}^\opt$, the measure induced by the density $p^\opt_{\bS,\bR}$ defined above.
Since $\E_\nu[\grad\ell\grad\ell^\sT]\succ \bzero,  \bR(\mu) \succ\bzero$ for such measures, one can check that $M(\bS;\nu,\bR)$ is strictly concave in $\bS$ and is uniquely maximized at 
\begin{equation}
    \bS= \bS^\opt(\nu,\bR) =\frac1{\sqrt\alpha}\bC(\bR)^{1/2}\left(\bC(\bR)^{-1/2}\E_{\nu}[\grad\bell\grad\bell^\sT]^{-1} \bC(\bR)^{-1/2}\right)^{1/2}\bC(\bR)^{1/2},
\end{equation}
with $M(\bS^\opt(\nu,\bR);\nu, \bR) = 0$.
\newline

\noindent\textbf{Concluding:}
Using the lower bounds above, for any $\mu,\nu$ as in the statement, we have by design
\begin{align}
   \sup_{\bS\succ\bzero}  
   \Phi_\cvx(\mu,\nu,\bR(\mu),\bS) &=
    \Phi_{\cvx,1}(\mu)  
    +\sup_{\bS\succ\bzero}\Phi_{\cvx,2}(\nu,\bR(\mu),\bS)\\
    &= \inf_{\mu_0 : \bR(\mu_0) = \bR(\mu)}
  \left\{
    \Phi_{\cvx,1}(\mu_0) + \sup_{\bS\succ\bzero} \Phi_{\cvx,2}(\nu^\opt,\bR(\mu), \bS) 
    \right\}\nonumber\\
    &= \inf_{\mu_0 : \bR(\mu_0) = \bR(\mu)}
    \Phi_{\cvx,1}(\mu_0) + \sup_{\bS\succ\bzero} \Phi_{\cvx,2}(\nu^\opt,\bR(\mu), \bS) 
    \nonumber\\
    &\stackrel{(a)}{\ge} 0 + \sup_{\bS\succ\bzero} \Phi_{\cvx,2}(\nu^\opt,\bR(\mu), \bS)
    \nonumber\\
    &\stackrel{(b)}{\ge} 0 + 
\sup_{\bS\succ\bzero}  M(S; \nu,\bR(\mu)) = 0.\nonumber
\end{align}
where in $(a)$ we used that for any $\bR \succ\bzero$, the Gaussian measure $\mu^\opt = \mu^\opt(\bR)$ chosen previously satisfies $\Phi_{\cvx,1}(\mu^\opt) = 0$.
By the previous steps, $(a)$ and $(b)$ hold with equality if and only $(\mu,\nu)$ are as given in Definition~\ref{def:opt_FP_conds}.

%Combining with Eq.~\eqref{eq:phi_1_LB} shows that $\sup_{\bS}\Phi \ge0$, with equality if and only if the inequality in $(a)$ and the inequality in Eq.~\eqref{eq:phi_1_LB} hold with equality, i.e., if and only if $\nu$ and $\mu$ satisfy the equations in Definition~\ref{def:opt_FP_conds}.

\subsection{Proof of Theorem~\ref{thm:global_min}}
%\paragraph{Convergence of the empirical distributions: proof of item~\textit{1.}}
To prove point \textit{1}, we'll look at critical points $\hat\bTheta_n$ of the 
empirical risk $\hat R_n$ such that $\hmu_{\sqrt{d}[\hbTheta_n,\bTheta_0]}$
and $\hnu_{[\bX\hbTheta_n,\bX\bTheta_0,\bw]}$ belong to the complement  of the sets
%
\begin{equation}
    \cuA_\eps:=  \{\mu : W_2(\mu,\mu^\opt) \le\eps \},\quad
    \quad\quad
    \cuB_\eps := 
     \{\nu : W_2(\nu,\nu^\opt) \le\eps \},
\end{equation}
for fixed $\eps >0$. We will then apply Theorem~\ref{thm:convexity} to deduce that the probability that there exist such critical points vanishes under the high-dimensional asymptotics.

Let $\Omega_0 := \{\widehat\bTheta_n \in \cE(\bTheta_0)\}$, and $\Omega_1 := \{ n C_0(\alpha) \succ \bX^\sT\bX\succ nc_0(\alpha)\}$.
We cite two results from Appendix~\ref{sec:simplifying_constraint_set} allowing to simplify the set in Eq.~\eqref{eq:set_of_zeros_main} to the set $\cE$ defined in Theorem~\ref{thm:convexity}:
First, 
under the conditions of the theorem, Lemma~\ref{lemma:jacobian_lb} gives the deterministic bound
\begin{equation}
    \sigma_{\min}\left( \bJ_{(\bbV,\bTheta)} \bG^\sT\right) \ge 
     \frac{\sigma_{\min}(\grad^2 \hat R_n(\bTheta))
     \sigma_{\min}([\bTheta,\bTheta_0])
     }{(1 + \|\bX\|_\op )}.
\end{equation}
%
%
%\begin{equation}
%    \sigma_{\min}\left( \bJ_{(\bbV,\bTheta)} \bG^\sT\right) \ge 
%    C \frac{\sigma_{\min}(\grad^2 \hat R_n(\bTheta))}{(1 + \lambda \|\grad^2\hat R_n(\bTheta)\|_\op) } \frac{\sigma_{\min}([\bTheta,\bTheta_0])}{ \|\bX\|_\op \vee 1}.
%\end{equation}
Since Assumption~\ref{ass:loss} guarantees that $\|\grad^2\hat R_n(\bTheta)\|_\op =O(1)$ on $\Omega_1$, we have $\sigma_{\min}(\bJ_{(\bbV,\bTheta)} \bG) = e^{-o(n)}$ on this event.
Further, under the same conditions, Lemma~\ref{lemma:min_sv_Theta} of Appendix shows that for any $C,c$, there exists $\delta>0$ sufficient small so that
\begin{equation}
\Omega_2 := \left\{ \textrm{for all}\; \bTheta 
\;\textrm{with}\;
\|\bTheta\|_F \le C\;
\textrm{and}\;
\grad \hat R_n(\bTheta)  = \bzero,
\;\textrm{if}\;
\sigma_{\min}(\bL) \ge  c\; 
\;\textrm{then}\;
\sigma_{\min}([\bTheta,\bTheta_0]) \ge \delta,\;  
\right\}
\end{equation}
is a high probability event. 
Hence, on $\Omega_0 \cap\Omega_1\cap\Omega_2$, we have $\hat\bTheta_n \in \cZ_n$ of Eq.~\eqref{eq:set_of_zeros_main} for some choice of $\sPi$ satisfying Assumption~\ref{ass:params}.

Then, by Theorem~\ref{thm:convexity} 
there exists some $c_0(\eps) >0$ such that
for any $\mu \in\cuT(\cuA^c_\eps),\nu \in\cuV(\bR(\mu),\cuB^c_\eps)$, $\sup_{\bS\succ\bzero}\Phi_\cvx(\mu,\nu,\bR(\mu)) > c(\eps)$ uniformly. 
So
using the shorthand 
$\hmu:=\hat\mu_{\sqrt{d}[\hat\bTheta_n,\bTheta]}$ and 
$\hnu := \hat\nu_{[\bX\hat\bTheta_n,\bX\bTheta]}$,
 we can bound for any $\delta>0$,
\begin{align*}
    \P\left( \left\{ W_2(\hmu, \mu^\opt) > \eps\right\}
\cup
\left\{ W_2(\hnu, \nu^\opt) > \eps\right\}
    \right)
    &\le  \P\left(\{(\hmu,\hnu) \in\cuA_\eps^c \times \cuB_\eps^c\} \cap \{\bw \in\cG_\delta\}\cap \Omega_0\cap\Omega_1\right)\\
&\quad\quad + \P(\Omega_0^c)
+ \P(\Omega_1^c)+ \P(\cG_\delta^c)\\
&\le \P\left(\one_{\hat\bTheta_n \in \cZ_n(\cuA_\eps^c,\cuB_\eps^c, \sPi)} \one_{\bw \in\cG_\delta}\right)
+ \P(\Omega_0^c)
+ \P(\Omega_1^c)
+ \P(\cG_\delta^c).
\end{align*}
Taking $n\to\infty$ and noting that
\begin{equation}
    \lim_{n\to\infty }
( \P(\Omega_0^c)
+ \P(\Omega_1^c)
+ \P(\cG_\delta^c)) = 0
\end{equation}
by the assumption on $\hat\bTheta_n$ and Assumption~\ref{ass:noise},
we conclude by Theorem~\ref{thm:convexity} that for all $\eps>0$
\begin{equation}
    \lim_{n\to\infty}\P\left( \left\{W_2(\hmu, \mu^\opt) > \eps\right\}
\cup
\left\{ W_2(\hnu, \nu^\opt) > \eps\right\}
    \right)
    \le 
\lim_{n\to\infty}
\E[\cZ_{n}(\cuA_\eps^c, \cuB_\eps^c, \sPi)\one_{\{\bw\in\cG_\delta\}} ] \le \lim_{n\to\infty} e^{- n c(\eps)} = 0
\end{equation}
giving the statement of \textit{1} of the theorem.

Claim \textit{2} now follows from the convergence in point~\textit{1} and Proposition~\ref{prop:uniform_convergence_lipschitz_test_functions}. This concludes the proof of the theorem.

\subsection*{Acknowledgments}
This work was supported by the NSF through award DMS-2031883, the Simons Foundation through
Award 814639 for the Collaboration on the Theoretical Foundations of Deep Learning, 
and the ONR grant N00014-18-1-2729.

%\paragraph{Convergence of the distribution of the hessian: proof of item~\textit{2.}}



%by Lemma~\ref{lemma:rate_matrix_ST} along with an argument similar to that of Lemma~\ref{lemma:asymp_ST}, we can deduce that for any $\hnu \Rightarrow \nu$ in probability,
%\begin{equation}
%     \frac1{dk} (\bI_k \otimes \Tr) \left(\bH(\hnu_n) - z\bI_{dk}\right)^{-1} \to \alpha \bS^\opt(\nu,z)
%\end{equation}
%in probability. The claim now follows by from \textit{1.} after recalling the definition of $\bH$.


%\subsection{Proof of Proposition~\ref{prop:simple_critical_point_variational_formula}}
%\bns{Do we add this here?}




%
%********************************************************
%
%\section{Notation}
%\begin{table}[htbp]\caption{Table of common notational shorthands used}
%\centering % to have the caption near the table
%\begin{tabular}{r c p{10cm} }
%$\bV \in\R^{n\times k},\bV_0 \in\R^{n\times k_0},\bbV\in\R^{n\times(k+k_0)}$ &$\:=$& indices of gradient process, $\bbV = (\bV,\bV_0)$\\
%$\bRho \in\R^{d\times k}$ &$:=$ & Jacobian of $\rho(\bTheta)$\\
%$\bSec\:= \bSec(\bbV,\bw)\in\R^{dk\times dk}$ &$\:=$& matrix with blocks $\bK_{i,j}:=\Diag(\partial_{i,j}\ell(\bbV)),i,j\in[k]$\\
%$\bH\equiv \bH(\bbV,\bw)$& $:=$&$\left(\bI_k \otimes \bX\right)^\sT \bSec(\bbV) (\bI_k\otimes\bX)$\\
%$\bG(\bbV,\bTheta) $ &$:=$ & the KKT constraint function $\bL^\sT\bbV + \bRho(\bTheta)^\sT\bTheta $\\
%$\hnu,\hmu$ & $:=$ & empirical measures of $(\bV,\bU,\bw), \sqrt{d}(\bTheta,\bTheta_0)$, respectively.\\
%$\cuP(\cK),\cK\subseteq\R^{m}$ &$\equiv$& set of probability measures with support in $\cK$.\\
%$\alpha_n$ &$:=$& $n/d$\\
%$r_k$ &$:=$& $(k^2 + kk_0).$\\
%$k_+(d)$&$:=$& $k \vee \log(d)$\\
%$\sfD$ & $:=$ & $\sup_{\bv,\bu,w} \norm{\grad^2\ell(\bv,\bu,w)}_\op$\\
%$\sfS_{\bbV}$ & $:=$ & $\frac1{\sqrt{n}}\sup_{\cB} \norm{(\bV,\bU)}_\op$\\
%$\sfA_{\bbV}$ & $:=$ & $\frac1{\sqrt{n}}\sup_{\cB} \norm{(\bV,\bU)}_F$\\
%$\sfS_{\bL}$ & $:=$ & $\frac1{\sqrt{n}}\sup_{\cB} \norm{\bL(\bV,\bU,\bw)}_\op$\\
%$\sfs_{\bL}$ & $:=$ & $\frac1{\sqrt{n}} \inf\sigma_{\min}(\bL(\bV,\bU,\bw))$\\
%$\sfS_{\bRho}$ & $:=$ & $\frac1{n} \sup_{\cA}\norm{\bRho}_\op$\\
%$\sfS_{\grad^2\rho}$ & $:=$ & $\frac1{n}\sup \norm{\grad^2\rho(\bTheta)}_\op$\\
%%$\sfR_n$ & $:=$ & $\sup_{\bTheta} \|\bR(\bTheta,\bTheta_0)\|_\op$\\
%%$\sfV_n$ & $:=$ & $\sup_{\bV,\bU} \|(\bV,\bU)\|_\op/\sqrt{n}$\\
%%$\sfL_n$ & $:=$ & $\sup_{\bV,\bU} \|\bL(\bV,\bU)\|_\op/\sqrt{n}$\\
%$\bF_z(\bS; \nu)$ &$:=$&$\left( \E_\nu[(\bI + \grad^2\ell\bS )^{-1}\grad^2\ell] - z \bI \right)^{-1}.$\\
%%\bF(\bQ ;\nu )$ &$:=$& $\alpha^{-1} \bQ^{-1} - \E_\nu[(\bI_{k} + \grad^2\ell \bQ)^{-1} \grad^2 \ell]\in\C^{k\times k}$.\\
%$\mu_\cI$ for $\mu\in\cuP(\R^{m}),\cI\subseteq [m]$ &$\equiv$&  the marginal of $\mu$ on coordinates indexed by $\cI$.\\
%$\pi_\cI(\cS),\cS \subseteq\R^m, \cI \subseteq [m]$ &$\equiv$&
%the projection of $\cS$ in the coordinates indexed by $\cI$.\\
%$\bS_n(z)\equiv \bS_n(z;\bbV,\bw)$& $:=$&$(\bI_k \otimes \Tr)(\bH - z n \bI_{dk})^{-1}.$\\
%$\cM \equiv \cM(\cA,\cB)$,$\cA\subseteq \cuP(\R^{k+k_0)},\cB\subseteq\cuP(\R^{k+k_0+1})$&$\equiv$& Parameter manifold defined in Eq.~\eqref{eq:param_manifold_def}.\\
%$\cM_0 \equiv \cM_0(\cA,\cB)$,$\cA\subseteq \cuP(\R^{k+k_0)},\cB\subseteq\cuP(\R^{k+k_0+1})$&$\equiv$& (without the $\bG = 0$ constraint)
%\end{tabular}
%\label{tab:notation}
%\end{table}
\newpage
\appendix

\input{Appendix_A_RMT}
\input{Appendix_B_THM1}
\input{Appendix_C_CONVEX}
\input{Appendix_D_EXAMPLES}


%\input{Appendix_E_EXPERIMENTS}

%\input{KR_integral_analysis}
%\input{Large_deviations}
%\input{Convex_case_analysis}



\bibliographystyle{amsalpha}
\bibliography{all-bibliography}

\end{document}  