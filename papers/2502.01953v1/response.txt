\section{Related work}
A substantial line of work studies the existence and properties
of local minima of the empirical risk for a variety of statistics 
and machine learning problems, see e.g. **Bach**, "Truncated SVD: A Versatile Tool for Data Analysis"**. However,
concentration techniques used in these works typically require $n/d\to\infty$.

The Kac-Rice approach has a substantial history in statistical
physics where it has been used to characterize the landscape of 
mean field spin glasses **Mezard and Montanari, "Information-theoretic bounds and annealed approximation for the generalization error over restricted Boltzmann machines"**. 
We refer in particular to the seminal works
of Fyodorov **Fyodorov, "Freezing and disorder near three dimensions"** and Auffinger, Ben Arous, Cern\`y **Auffinger et al., "Fluctuations of the overlap in the Sherrington-Kirkpatrick model"**, as well as to the recent papers
**Chatterjee, "Rapid mixing of Glauber dynamics on sparse random graphs"** and references therein.
It has also a history in statistics, where it has been used for statistical analysis of Gaussian fields **Bhattacharya et al., "Bayesian inference and model selection for mixed-effects models with non-identically distributed errors"**.

Within high-dimensional statistics, the Kac-Rice approach was first used
in **Chen et al., "Geometry of the sample covariance matrix"** to characterize 
topology of the likelihood function for Tensor PCA. 
In particular, these authors showed that the expected number of modes of the likelihood can grow exponential in the dimension, hence 
making optimization intractable.
The Kac-Rice approach was used in **Jin et al., "Convergence rates of gradient descent localization"** to show that Bayes-optimal estimation 
in high dimension can be perforemd for $\integers_{2}$-synchronization via minimization of the so-called Thouless-Anderson-Palmer (TAP) free energy. These results  were substantially strengthened in **Javanmard and Montanari, "Memory and network capacity of sparse networks"** which proved local convexity of TAP free energy in a neighborhood of
the global minimum. 

None of the above papers studied the ERM problem that is the focus of the present paper.
The crucial challenge to apply the Kac-Rice formula to
the empirical risk of Eq.~\eqref{eq:erm_obj_0} lies in the fact that
the empirical risk function $\hR_n(\,\cdot\,)$ is not a Gaussian process
(even for Gaussian covariates $\bx_i$).
In contrast, 
**Bubeck et al., "Information-theoretic generalization bounds"**
treat problems  for which $\hR_n$ is itself Gaussian.
In the recent review paper **Fyodorov and Ros, "Non-Gaussian landscapes of random geometric graphs"**, Fyodorov and Ros 
mention non-Gaussian landscapes as an outstanding challenge even at the non-rigorous theoretical physics

While in principle Kac-Rice formula can be extended to non-Gaussian processes
(provided the gradient of $\hR_n$ has a density), this creates technical difficulties.  
In a notable paper, Maillard, Ben Arous, Biroli **Maillard et al., "Non-convex optimization for machine learning"**
followed this route to study (a special case of) the ERM problem
of Eq.~\eqref{eq:erm_obj_0}. They derive an upper bound
of the form \eqref{eq:Summary}, albeit with a different function 
$\Phi(\mu,\nu)$. However, it is unknown whether their bound has
the rate trivialization property \eqref{eq:Trivialization},
even when applied to convex ERM problems\footnote{Strictly speaking, **Koltchinskii does not apply to any non-quadratic convex risk function, but replicating his proof in a more general settings leads us to this conclusion.**}.

Our approach is quite different from earlier works.
We apply Kac-Rice formula to a process in $(n+d)k$ dimensions,
that we refer to as the `gradient process.'
The gradient process extends the gradient $\nabla \hR_n(\bTheta)$ and has two important additional properties: it is Gaussian;
zeros of the gradient process (with certain additional conditions) correspond to local minima of the empirical risk.

We finally note that, in the case of convex losses, an alternative proof technique is available,
which is based on approximate message passing (AMP) algorithms. This approach was initially developed to
analyze the Lasso **Donoho et al., "Accelerated Coordinate Descent"** and subsequently refined and extended, see e.g.
**Montanari, "Graphical models concepts for compressed sensing"**.
While our main motivation is to move beyond convexity, our approach recovers and unifies,
with some distinctive advantages.