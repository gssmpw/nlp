\section{Technical lemmas for the proof of Theorems~\ref{thm:convexity},~\ref{thm:global_min} and~\ref{thm:simple_critical_point_variational_formula}}

%Since $\rho''(t) = \lambda$ under this assumption, note that for any $(\nu,\mu) \in\cuP(\R^{k+k_0+1})\times \cuP(\R^{k+k_0})$, 
%\begin{equation}
%    \mu_{\star}(\mu,\nu) =
%    \mu_{\MP}(\mu) \boxplus  \delta_{\lambda}.
%\end{equation}
%So we use the notation $\mu_{\star,\lambda}(\nu)$ in this setting. 

%Note that under the setting of Assumption~\ref{ass:convexity}, we have for all $\lambda\ge0$,  and $\nu\in\cuP^(\R^{k+k_0+1})$,
%\begin{equation}
%\supp\left(\mu_{\star,\lambda}(\nu)\right) \subseteq [0,\infty).
%\end{equation}

%Note that under Assumption~\ref{ass:convexity}, for any $\nu\in\cuP(\R^{k+k_0+1}),$  we have
%    \begin{equation}
%        \inf \supp(\mu_\star(\nu)) = -\inf_{\bS \succ \bzero } \frac1k \Tr\left(\frac1\alpha \bS^{-1} - \E_\nu[(\bI_{k} + \grad^2\ell \bS)^{-1} \grad^2 \ell]\right) \ge 0
%    \end{equation}


\subsection{The logarithmic potential: Proof of Lemma~\ref{lemma:variational_log_pot}}
\label{sec:log_pot_proof}
Recall the definition of $K_z$ in Lemma~\ref{lemma:variational_log_pot}. We extend it below to complex $z$:
for any $\nu\in\cuP(\R^{k+k_0+1})$, $z \in \C \setminus \supp(\mu_{\star,0}(\nu))$ 
with $\Im(z) \ge 0$, $\bQ\in\bbH_+^k$,
\begin{equation}
    K_z(\bQ;\nu):= -\alpha z \Tr(\bQ) + \alpha \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bQ) ]  - \log\det(\bQ) - k (\log(\alpha) + 1)
\end{equation}
where $\log$ denotes the complex logarithm (with a branch on the negative real axis). 

\begin{lemma}
\label{lemma:log_pot_z}
Under Assumptions \ref{ass:regime} to \ref{ass:params} of Section~\ref{sec:assumptions} along with
the additional Assumption~\ref{ass:convexity}, we have
    \begin{equation}
    \label{eq:log_pot}
        k\int\log(\zeta - z) \de \mu_{\star,0}(\zeta)
=  K_z(\bS_\star(z;\nu);\nu),
    \end{equation}  
where 
$\bS_\star(z;\nu)$ is the unique solution of \eqref{eq:fp_eq} 
was defined in Eq.~\eqref{eq:def_S_star} for $z\in\bbH_+$
(see also Eq.~\eqref{eq:def_S_star}), and is extended by analytic continuation to $x\in\R \setminus \supp(\mu_{\star,0}(\nu))$.

%Furthermore, 
%letting 
%\begin{equation}
%    \bS_\star(x;\nu) := \lim_{\eps\to0} \bS_\star(x +  i\eps;\nu), \quad\quad\textrm{for}\quad\quad x\in\R,
%\end{equation}

Consequently, for any $\lambda \ge 0$, and $\nu$ with $\inf\supp(\mu_{\star,0}(\nu))\ge -\lambda$,
\begin{equation}
\label{eq:log_pot_0}
k\int \log(\zeta )\mu_{\star,\lambda}(\nu)(\de\zeta) \le  \limsup_{\delta \to 0+}K_{-(\lambda+\delta)}(\bS_{\star}(-(\lambda+\delta);\nu);\nu).
\end{equation}
%with equality if
%$\inf\supp(\mu_{\star,\lambda}(\nu))> -\lambda$.
\end{lemma}
\begin{proof}
Fix $z\in\bbH_+$.
By taking derivatives, one can easily see that $\bQ = \bS_\star(z;\nu)$ is a critical point of $K_z(\bQ;\nu)$,
whence
\begin{equation}
   \frac{\partial}{\partial z} K_z(\bS_\star(z;\nu);\nu) =  -\alpha \Tr(\bS_\star(z;\nu)) = - k  \int \frac1{\zeta - z} \mu_{\star,0}(\nu)(\de\zeta) =  
k \frac{\partial}{\partial z}\int \log(\zeta )\mu_{\star,0}(\nu)(\de\zeta).
\end{equation}
Equation \eqref{eq:log_pot} now follows by showing
%
\begin{align}
    \lim_{\Re(z)\to-\infty}\left| K_z(\bS_\star(z;\nu);\nu)-k\int \log(\zeta )\mu_{\star,\lambda}(\nu)(\de\zeta)
    \right| = 0\, .
\end{align}
%
Analytic continuation then gives the equality for $z$ on the real line outside of the support.

We next prove Eq.~\eqref{eq:log_pot_0}.


Since $\mu_{\star,\lambda}(\nu)$ is compactly supported
by Corollary~\ref{cor:S_star_min_singular_value_bound},
 we always have 
\begin{equation}
\int_1^{\infty} \log(\zeta )\mu_{\star,\lambda}(\nu)(\de\zeta)  <\infty\, . 
\end{equation}
%
Further, if 
\begin{equation}
\int_{0}^{1} \log(\zeta-z )\mu_{\star,\lambda}(\nu)(\de\zeta)  = -\infty,
\end{equation}
then Eq.~\eqref{eq:log_pot_0} holds trivially.
Therefore, it's sufficient to consider the case where $|\log(\zeta)|$ is integrable with respect to $\mu_{\star,\lambda}$ for any $\lambda\ge 0$. In this case, Eq.~\eqref{eq:log_pot_0} follows directly by domination:
\begin{align}
 \int \log(\zeta )\mu_{\star,\lambda}(\nu)(\de\zeta) 
= 
\lim_{\delta \to 0+}\int \log(\zeta  + \delta)\mu_{\star,\lambda}(\nu)(\de\zeta) 
=
\lim_{\delta \to 0+}
K_{-(\lambda+\delta)}(\bS_{\star}(-(\lambda+\delta);\nu);\nu).
\end{align}
\end{proof}


\begin{lemma}[Local strict convexity of $K$]
\label{lemma:strict_convexity_K}
Fix  $x\in\reals_{\ge 0}$.
Assume that 
\begin{equation}
\label{eq:ass_nondegenerate_as}
    \P_\nu(\grad^2\ell(\bv,\bu,w) = \bzero ) \neq 1.
\end{equation}
Under the 
Assumptions \ref{ass:regime} to \ref{ass:params} 
 of Section~\ref{sec:assumptions} along with Assumption~\ref{ass:convexity}, if $\bS\succ\bzero$ satisfies
\begin{equation}
\label{eq:derivative_K_0}
    \alpha^{-1} \bS^{-1} -\E_\nu[(\bI + \grad^2\ell \bS)^{-1}\grad^2\ell] = x\bI,
\end{equation}
then $\bS \mapsto K_{-x}(\bS; \nu)$ is
strictly convex at $\bS$.
\end{lemma}

\begin{proof}
%We will prove this using the approach of
%Lemma~\ref{lemma:uniqueness_ST} for proving uniqueness of the Stieltjes transform for $\Im(z) > 0$.
%For ease of notation, denote $\bD \equiv  \grad^2\ell$.
%Fix $\bS_1,\bS_2 \succ\bzero$ any two solutions of Eq.~\eqref{eq:derivative_K_0}. In a manner similar to Section~\notate{ref}, we define the operator
%\begin{equation}
%    \bT(\bDelta) := \alpha_n^2 \bS_1 \E[(\bI + \bD \bS_1)^{-1}\bD\bDelta 
%(\bI + \bD \bS_2)^{-1} \bD
%    ]\bS_2.
%\end{equation}
%By the approach of  Lemma~\ref{lemma:uniqueness_ST} and the bounds of Lemma~\ref{lemma:op_norm_bound_power_T}, we note that, to prove uniquness, it's sufficient to show that
%\begin{equation}
%\label{eq:op_norm_bound_for_unique_critical_point_K0}
%    \alpha_n\norm{\E[\bB^{-1/2} \bS_i(\bI + \bD \bS_i)^{-1} \bD \bB (\bI + \bD \bS_i)^{-1} \bD \bS_i \bB^{-1/2}]}_\op < 1
%\end{equation}
%for $i\in\{1,2\}$, and any choice of $\bB \succ\bzero$ of bounded condition number.
%Indeed, this will show that the Neumann series of $\alpha_n\bT$ is convergent implying the invertibility of $(\bI - \alpha_n\bT)^{-1}$, allowing us to carry out an argument analogous to Lemma~\ref{lemma:uniqueness_ST}.
%%
%We will pursue the bound in~\eqref{eq:op_norm_bound_for_unique_critical_point_K0} and leave the remaining details to the reader. Namely, we will prove this bound for $\bB = \bS_i$.
%
%%Suppress the subscript $i$ for $\bS_i$ in what follows, and define for $t>0$,
%%    \begin{equation}
%%        \bF(t)  := \frac1{t\alpha} \bS^{-1}   -\E_\nu[(\bI  + \grad^2 \ell t \bS)^{-1}\grad^2\ell].
%%    \end{equation}
%% 
%%We show next that the expectation in~\eqref{eq:op_norm_bound_for_unique_critical_point_K0} is implied by the relation $\bF'(1) \prec 0$: Indeed, the derivative $\bF'(t)$ can be computed directly as
%%\begin{align}
%%    \bF'(t) &=  
%%   \E[(\bI + t \bD \bS)^{-1}\bD \bS(\bI + t\bD\bS)^{-1}\bD] 
%%   - \frac1{t^{2} \alpha} \bS^{-1}
%%    \\
%%&= 
%%\frac1{t^2}
%%   \bS^{-1/2}\left(t^2\E[\bS^{1/2}(\bI + t \bD \bS)^{-1}\bD \bS(\bI + t\bD\bS)^{-1}\bD\bS^{1/2}] 
%%   - \frac1{\alpha} \right)\bS^{-1/2},
%%   \end{align}
%%from which the claim readily follows.  
%%What remains is to show that $\bF'(1) \prec \bzero$ indeed holds.
%%
%%
%%By symmetry and positive semi-definiteness of $\bD$ and $\bS$, we can rewrite the previous display as
%%\begin{align}
%%\label{eq:F_prime_1_simple_form}
%%\bF'(1) = 
%%   \bS^{-1/2}\left(
%%   \E[(\bS^{1/2}\bD^{1/2}(\bI +  \bD^{1/2} \bS \bD^{1/2})^{-1}\bD^{1/2} \bS^{1/2})^2]
%%   - \frac1{\alpha} \right)\bS^{-1/2}.
%%\end{align}
%%Letting $\bA = \bD^{1/2}\bS^{1/2}$ we note that 
%%\begin{align}
%%    \bI - \bA^\sT(\bI + \bA\bA^\sT)\bA = (\bI + \bA^\sT\bA)^{-1} \succ \bzero,
%%\end{align}
%%so that $\bI \succ \bA^\sT(\bI + \bA\bA^\sT)\bA$, allowing us to conclude 
%%\begin{equation}
%%\bA^\sT(\bI + \bA\bA^\sT)\bA \succeq
%%(\bA^\sT(\bI + \bA\bA^\sT)\bA)^2,
%%\end{equation}
%%with the relation holding \emph{strictly} whenever $\bA$ is invertible. Since $\P_\nu(\bA\, \textrm{invertible})\neq 0$,
%%and noting $\E[(\bA^\sT(\bI + \bA\bA^\sT)\bA)^2]$ corresponds to the expectation term in Eq.~\eqref{eq:F_prime_1_simple_form}, we conclude
%%\begin{align}
%%    \bF'(1)  &\prec
%%   \bS^{-1/2}\left(
%%   \E[(\bS^{1/2}\bD^{1/2}(\bI +  \bD^{1/2} \bS \bD^{1/2})^{-1}\bD^{1/2} \bS^{1/2})]
%%   - \frac1{\alpha} \right)\bS^{-1/2}
%%=\bzero
%%\end{align}
%%by assumption on $\bS$ being a solution to the fixed-point equation. This conclude the proof of the lemma.
%%\end{proof}
%With this choice of $\bB$, we have by symmetry and positive semi-definiteness of $\bD$ and $\bS$, 
%\begin{align}
%\bS^{1/2}(\bI + \bD \bS)^{-1} \bD \bS (\bI + \bD \bS)^{-1} \bD \bS^{1/2}
%   = \left(
%   (\bS^{1/2}\bD^{1/2}(\bI +  \bD^{1/2} \bS \bD^{1/2})^{-1}\bD^{1/2} \bS^{1/2})^2
%   \right)
%\end{align}
%Letting $\bA = \bD^{1/2}\bS^{1/2}$ we note that 
%\begin{align}
%    \bI - \bA^\sT(\bI + \bA\bA^\sT)\bA = (\bI + \bA^\sT\bA)^{-1} \succ \bzero,
%\end{align}
%so that $\bI \succ \bA^\sT(\bI + \bA\bA^\sT)\bA$, allowing us to conclude 
%\begin{equation}
%\bA^\sT(\bI + \bA\bA^\sT)\bA \succeq
%(\bA^\sT(\bI + \bA\bA^\sT)\bA)^2,
%\end{equation}
%with the relation holding \emph{strictly} whenever $\bA$ is invertible. Since $\P_\nu(\bA\, \textrm{invertible})\neq 0$,
%taking expectation proves Eq.~\eqref{eq:op_norm_bound_for_unique_critical_point_K0}, implying $\bS_1 = \bS_2$.
%
%\bns{from here}
For ease of notation, denote $\bD :=  \grad^2\ell$ and suppress its arguments throughout.
%We now show that the function is strictly convex locally around $\bS_0$.
 For any $\bZ\succ\bzero$, let $\bH_\bS(\bZ)$ denote the second derivative tensor of $K_{-x}$ at $\bS$, applied to $\bZ$.
Now let $\bS_0$ be a point satisfying the critical point equation~\eqref{eq:derivative_K_0}.
To save on notation,  we denote
\begin{equation}
   \bM := \bM(\bS_0;\bD):=  \bS_0^{1/2}\bD^{1/2}(\bI+\bD^{1/2} \bS_0\bD^{1/2})^{-1} \bD^{1/2}\bS_0^{1/2},\quad\quad\textrm{and}\quad\quad
   \bA := \bS_0^{-1/2}\bZ\bS_{0}^{-1/2}.
\end{equation}
By Assumption~\ref{ass:convexity}, we have $\bD \succeq \bzero$ almost surely, and hence $\bM$ satisfies  $\bzero \preceq \bM \prec\bI$.
Therefore we can lower bound
\begin{align}
\label{eq:second_derivative_K_0}
  \frac1\alpha\Tr( \bZ \bH_{\bS_0}( \bZ)) &= \frac1\alpha\Tr(\bZ \bS_0^{-1} \bZ \bS_0^{-1}) - 
  \E[
\Tr(\bS_0^{-1/2}\bZ \bS_0^{-1/2}\bM(\bS_0;\bD) \bS_0^{-1/2}\bZ\bS_0^{-1/2}\bM(\bS_0;\bD) )]\\
&= \frac1\alpha \Tr(\bA^2) - \E[\Tr(\bA\bM\bA\bM)]\\
&\stackrel{(a)}{\ge} \frac1\alpha \Tr(\bA^2) - \E[\Tr(\bA\bM\bA)]\\
  %\Tr(\bZ \bD^{1/2}(\bI+\bD^{1/2} \bS\bD^{1/2})^{-1} \bD^{1/2} \bZ \bD^{1/2}(\bI  +\bD^{1/2}\bS\bD^{1/2})^{-1} \bD^{1/2})].
 &\stackrel{(b)}{\ge} x  \Tr(\bZ\bS_0^{-1} \bZ) \ge 0,
\end{align}
for all $x\ge0$,
where $(a)$ follows from $\bM\prec\bI$, and in $(b)$ we used that
%where $(a)$ follows from $\bM\prec\bI$, holding by definition of $\bM$.
for $\bS_0$ satisfying Eq.~\eqref{eq:derivative_K_0}, we have 
$\E[\bM(\bS_0;\bD)] = \alpha^{-1}\bI   - x\bS_0$, giving 
%$\Tr( \bZ \bH(\bS_0) \bZ) \ge 0$ for any $x$ nonnegative,
convexity at $\bS_0$. To show strict convexity, we'll show that the inequality $(a)$ cannot hold with equality under the assumptions of the lemma.

Let $\{\lambda_i\}$ be the eigenvalues of $\bA^{1/2}\bM\bA^{1/2}$, and $\{\tilde\lambda_i\}$ be the eigenvalues of $\bA$. 
Write
\begin{align}
 \Tr(\bA \bM\bA)- \Tr(\bA \bM \bA \bM)
 %&=
%\Tr(\bA^{1/2}\bM\bA^{1/2}\bA)- \Tr(\bA^{1/2}\bM\bA^{1/2} \bA^{1/2}\bM\bA^{1/2})\\
 &= \langle \bA^{1/2}\bM \bA^{1/2}, \bA\rangle -\| \bA^{1/2}\bM \bA^{1/2}\|_F^2 
=  \sum_{i=1}^k \lambda_i \tilde\lambda_i -\sum_{i=1}^k \lambda_i^2.
\end{align}
Since $\bM \prec \bI$, we have $\lambda_i < \tilde\lambda_i$ for all $i$. So if $\bM \neq \bzero$, we have $\lambda_i >0$ for some $i$ so that
    $\sum_{i} \lambda_i \tilde\lambda_i > \sum_i \lambda_i^2.$
So
\begin{equation}
  \Tr(\bA \bM \bA \bM) < \Tr(\bA \bM\bA)
\end{equation}
for all $\bM \neq \bzero$. Since the assumption
in Eq.~\eqref{eq:ass_nondegenerate_as} 
implies that $\P(\bM = \bzero) \neq 1$,  we conclude that $(a)$ holds strictly as desired.
%, where the second relation holds strictly when $\bD\succ\bzero.$ So by the assumption that $\bD\succ\bzero$ on a set of a measure non-zero, we conclude
%\begin{align}
%  \E[
%\Tr(\bS_0^{-1/2}\bZ \bS_0^{-1/2}\bM(\bS_0;\bD) \bS_0^{-1/2}\bZ\bS_0^{-1/2}\bM(\bS_0;\bD) )] &<
%  \E[
%\Tr(\bS_0^{-1/2}\bZ \bS_0^{-1/2}\bM(\bS_0;\bD) \bS_0^{-1/2}\bZ\bS_0^{-1/2})]\\
%&\stackrel{(a)}{=} 
%\frac1\alpha \Tr(\bS_0^{-1}\bZ \bS_0^{-1} \bZ)
%- x\Tr(\bZ\bS_0^{-1}\bZ)
%,
%\end{align}
%where in $(a)$ we used that $\E[\bM(\bS_0;\bD)] = \alpha^{-1}\bI   - x\bS$ by Eq.~\eqref{eq:derivative_K_0}.
%%\begin{align}
%  \Tr(\bZ \bD^{1/2}(\bI+\bD^{1/2} \bS_0\bD^{1/2})^{-1} \bD^{1/2} \bZ \bD^{1/2}(\bI  +\bD^{1/2}\bS_0\bD^{1/2})^{-1} \bD)
%  &= 
%  \Tr(\bS_0^{-1/2}\bZ \bS_0^{-1/2}\bM \bS^{-1/2}\bZ\bS^{-1/2}\bM )\\
%  &\ge 
%  \Tr(\bS^{-1/2}\bZ \bS^{-1/2}\bM \bS^{-1/2}\bZ\bS^{-1/2}),
%\end{align} 
%with strict inequality whenever $\bD\succ\bzero$ is invertible. 
%By the assumption that $\bD \succ\bzero$ with non-zero probability, and noting that $\E[\bM(\bS;\bD)] =(\alpha^{-1}-x)\bI$ whenever $\bS$ satisfies Eq.~\eqref{eq:derivative_K_0}, we conclude that
%Now to conclude, we use this upper bound in~\eqref{eq:second_derivative_K_0}, we can now lower bound
%\begin{align}
%  \frac1\alpha\Tr( \bZ \bH(\bS_0) \bZ) > 
%  x \Tr(\bZ\bS_0^{-1}\bZ) \ge 0,
%\end{align}
%for all $x\ge 0$,
%giving the desired strict convexity.
\end{proof}

%\begin{lemma}[Variational principle for the log potential]
%\label{lemma:variational_log_pot}
%   Under Assumption~\ref{ass:convexity} , for any $\nu\in\cuB$, and $\lambda \ge0$,
%   we have
%    \begin{equation}
%    \label{eq:log_pot}
%        k\int\log(\zeta ) \mu_{\star,\zeta}(\nu) (\de\zeta)
%\le \inf_{\bS\succ\bzero} K_{-\lambda}(\bS;\nu).
%    \end{equation}  
%\end{lemma}
We move on to the proof of Lemma~\ref{lemma:variational_log_pot}
\begin{proof}[Proof of Lemma~\ref{lemma:variational_log_pot}]
Without loss of generality, assume that $\log(\zeta)$ is absolutely integrable under $\mu_{\star,\lambda}$
for all $\lambda\ge 0$. Indeed, for $\lambda>0$ this holds because $\mu_{\star,\lambda}$ is compactly supported inside
$[\lambda,\infty)$.  For $\lambda=0$, the positive part of $\log(\zeta)$ is integrable because
$\mu_{\star,0}$ is compactly supported, and  lack of absolute integrability implies that the integral diverges to $-\infty$.

Under Assumption~\ref{ass:convexity}, we have $\supp(\mu_{\star,0}(\nu))\subseteq[0,\infty),$  so for any $\lambda \ge0,$ Eq.~\eqref{eq:log_pot_0} of Lemma~\ref{lemma:log_pot_z} yields
\begin{equation}
k\int \log(\zeta )\mu_{\star,\lambda}(\nu)(\de\zeta) \le  \limsup_{\delta \to 0}K_{-(\lambda+\delta)}(\bS_{\star}(-(\lambda+\delta);\nu);\nu).
\end{equation}

Now observe that the absolute integrability assumption implies that $\P_\nu(\grad^2\ell(\bv,\bu,w) = \bzero ) \neq 1.$ 
Indeed, otherwise, we must have $\mu_{\star,0}(\nu) = \delta_0$
(for instance, this can be seen by noting for any $z \in\C - \supp(\mu_{\star,0}(\nu))$ with $\Im(z) >0$,
$\bS(z) = \alpha^{-1}z^{-1}\bI$, implying a degenerate measure).
So Lemma~\ref{lemma:strict_convexity_K} holds with $x = \delta + \lambda$.
In particular, this gives that at the point $\bS_\star(-(\delta+\lambda);\nu)$ which satisfies Eq.~\eqref{eq:derivative_K_0}, the continuous function $\bS \mapsto K_{-(\delta+\lambda)}(\bS;\nu)$ is strictly convex, implying that $\bS_\star(-(\delta+\lambda);\nu)$ is the unique global minimum. 
Combining this with the above display gives
\begin{align}
k\int \log(\zeta )\mu_{\star,\lambda}(\nu)(\de\zeta) 
&\le \limsup_{\delta\to 0+}  \inf_{\bS\succ\bzero} K_{-(\lambda+\delta)}(\bS;\nu)
\le 
  \inf_{\bS\succ\bzero}
  \limsup_{\delta\to 0+} 
  K_{-(\lambda+\delta)}(\bS;\nu) = 
  \inf_{\bS\succ\bzero}
  K_{-\lambda}(\bS;\nu),
\end{align}
as desired.

\end{proof}





%\begin{lemma}
%Assume that  \bns{These are the assumptions on $\nu$ that we need. Is convexity enough or do we need $\inf\supp\mu_\star(\nu) > 0.$}
%\begin{equation}
%\left|\int \log(\lambda) \mu_\star(\nu)\de \lambda \right| < \infty,\quad
%\bS_0(\nu) := \lim_{\eps \to 0 }\bS_\star(i \eps; \nu) \succ \bzero,\quad\textrm{and}\quad
%    \P_\nu(\grad^2\ell(\bv,\bu,w) \succ \bzero ) \neq 0.
%\end{equation}
%Then under Assumption~\ref{ass:convexity}, we have
%
%%for any $\nu$ with $\inf\supp(\mu_\star(\nu)) > 0$,
%    \begin{equation}
%    \label{eq:log_pot}
%        k\int\log(\lambda ) \mu_\star(\nu) (\de\lambda)
%= \inf_{\bS\succ\bzero} K_0(\bS;\nu).
%    \end{equation}  
%\end{lemma}
%\begin{proof}
%Recall that for any $\eps>0$, 
%by Lemma~\ref{lemma:log_pot_z},
%\begin{equation}
%\label{eq:recalling_log_pot_z}
%    k\int\log(\lambda - i\eps) \mu_\star(\nu)(\de\lambda)= - i\alpha_n \eps \Tr(\bS_\star(i\eps;\nu)) + K_0(\bS_\star(i\eps;\nu);\nu)
%\end{equation}
%%Since we assume the existence of the log potential of $\mu_\star$, we have by domination
%%\begin{equation}
%%    \lim_{\eps\to 0}k\int\log(\lambda - i\eps) \mu_\star(\nu)(\de\lambda) = 
%%    k\int\log(\lambda ) \mu_\star(\nu)(\de\lambda).
%%\end{equation} 
%%Letting
%%\begin{equation}
%%\bS_0(\nu) := \lim_{\eps \to 0}\bS_{\star}(i\eps;\nu),
%%\end{equation}
%Taking $\eps\to 0$ in Eq.~\eqref{eq:recalling_log_pot_z}, we see that it's sufficient for 
%\begin{equation}
%    \inf_{\bS\succ\bzero} K_0(\bS;\nu) = K_0(\bS_0(\nu);\nu),
%    %,\quad\quad\textrm{and}\quad\quad
% %\bS_0(\nu)  \succ\bzero.
%\end{equation}
%to hold, but
%this follows from the previous lemma.
%\end{proof}

%\begin{lemma}
%Under Assumption~\ref{ass:convexity}, for any $\nu\in\cuP(\R^{k+k_0+1}),$  we have
%    \begin{equation}
%        \inf \supp(\mu_\star(\nu)) = -\inf_{\bS \succ \bzero } \frac1k \Tr\left(\frac1\alpha \bS^{-1} - \E_\nu[(\bI_{k} + \grad^2\ell \bS)^{-1} \grad^2 \ell]\right) \ge 0
%    \end{equation}
%\end{lemma}


%For $\nu$ a measure on $\R^{k+k_0 + 1}$, $\bS\succ\bzero,$
%recall the definition
%\begin{equation}
%    K_{-\lambda}(\bS;\nu):=
%    \lambda \alpha \Tr(\bS)  + 
%    \alpha \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bS) ]  - \log\det(\bS) - k (\log(\alpha_n) + 1).
%\end{equation}

%\subsection{Variational formula under convexity: Proof of Theorem~\ref{thm:convexity}}


%\begin{align}
%\phi(\nu,\mu,\bS,\bR)
%&:=
% \frac{k}{2\alpha}\log(\alpha)+
%    \lambda \alpha \Tr(\bS)  + 
%    \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bS) ]  -\frac1\alpha \log\det(\bS) - \frac{k}{\alpha} \log(\alpha_n) -\frac{k}{\alpha}\\
%  &\quad- \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
%+ \frac{1}{2\alpha} \Tr\left(\bR_{11}\right) 
%-\frac1{2}\log\det(\bR)
%+ \frac12 \Tr\left((\bI_k - \bR^{-1})\E[\bbv\bbv^\sT]\right)
%\\
%   &\quad-\frac1{2\alpha}
%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\mu[\grad \rho \grad \rho^\sT]\right) + \frac1{2\alpha}\Tr\left(\E\left[\bbv\grad\ell^\sT\right] (\E[\grad\ell\grad\ell^\sT])^{-1}\E[\grad\ell\bbv^\sT] \bR^{-1}\right).
%\end{align}
%
%
%%\begin{theorem}
%%   Consider the setting of Theorem~\ref{thm:general}, and define
%%\begin{align}
%%\phi(\nu,\mu,\bS,\bR)
%%&:=
%% \frac{k}{2\alpha}\log(\alpha)+
%%    \lambda\Tr(\bS)  + 
%%    \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bS) ]  -\frac1\alpha \log\det(\bS) - \frac{k}{\alpha} \log(\alpha_n) -\frac{k}{\alpha}\\
%%  &\quad- \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
%%+ \frac{1}{2\alpha} \Tr\left(\bR_{11}\right) 
%%-\frac1{2}\log\det(\bR)
%%+ \frac12 \Tr\left((\bI_k - \bR^{-1})\E[\bbv\bbv^\sT]\right).
%%% &\quad-\frac{\lambda^2}{2\alpha}
%%%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\bR_{11}\right) + \frac{\lambda^2}{2\alpha}\Tr\left( (\E[\grad\ell\grad\ell^\sT])^{-1} [\bR_{11},\bR_{00}] \bR^{-1}[\bR_{11},\bR_{00}]^\sT\right).
%%\end{align}
%%Then we have
%%\begin{align}
%%   &\limsup_{\delta\to 0 }\limsup_{n\to\infty}\E[Z_n(\cuA,\cuB, \sPi,\bw) \one_{\bw\in\cG_\delta}] \\
%%   &\le 
%%   \sup_{(\mu,\nu)\in\cuM \cap \cuS_0}\inf_{\bS\succ\bzero}\left\{
%%   \phi(\nu,\mu,\bS,\bR(\mu))
%%   -  \KL(\nu_{\cI_{v}|\cI_{w}} \| \cN(\bzero, \bI_{k+k_0})) - \frac1\alpha \left( \mu_{\cI_{\btheta}| \cI_{\btheta_0}}\| \cN(\bzero,\bI_k) \right)
%%   \right\}.
%%\end{align}
%%
%%\end{theorem}
%
%
%\begin{theorem}
%\label{thm:convexity}
%   Consider the setting of Theorem~\ref{thm:general} and Assumption~\ref{ass:convexity}. 
%  For $\bR \succ\bzero,$  define the set
%\begin{align}
%\nonumber
%\cuV(\bR) := \Big\{\nu \in  \cuB &:\;
%\sfA_{\bR} \succ \bR \succ\sfsigma_\bR,\;
%\sfA_\bV \succ \E_{\nu}[\bv\bv^\sT] \succ \sfsigma_{\bV},\;
%\E_{\nu}[\grad\ell \grad\ell^\sT] \succ \sfsigma_{\bL},\;\\
%&\quad\quad\quad\nonumber
%\E_\nu[\grad \ell(\bv,\bv_0,w)(\bv,\bv_0)^\sT] + 
%     \lambda (\bR_{11},\bR_{1,0}) =   \bzero_{k\times (k+k_0)},\;
% \nu_{\cI_{w}} = \P_w,\; \Big\}
%\end{align}
%and
%\begin{equation}
%   \cuT := \{\mu \in\cuA : \mu_{\cI_1} = \mu_{\btheta_0} \}.
%\end{equation}
%Let
%\begin{align}
%\Phi(\nu,\mu,\bS,\bR)
%&:=
%    \lambda\Tr(\bS)  + 
%    \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bS) ]  -\frac1\alpha \log\det(\bS) - \frac{k}{2\alpha} \log(\alpha) -\frac{k}{\alpha}\\
%  &\quad- \frac{1}{2\alpha}\log \det\left( \E_{\nu}[\grad \ell\grad\ell^\sT]\right)
%+ \frac{1}{2\alpha} \Tr\left(\bR_{11}\right)
%% &\quad-\frac{\lambda^2}{2\alpha}
%%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\bR_{11}\right) + \frac{\lambda^2}{2\alpha}\Tr\left( (\E[\grad\ell\grad\ell^\sT])^{-1} [\bR_{11},\bR_{00}] \bR^{-1}[\bR_{11},\bR_{00}]^\sT\right).
%-  \KL(\nu_{\cI_{v}|\cI_{w}} \| \cN(\bzero, \bR)) - \frac1\alpha \KL ( \mu_{\cI_{\btheta}| \cI_{\btheta_0}}\| \cN(\bzero,\bI_k)).
%\end{align}
%Then we have
%\begin{align}
%   &\limsup_{\delta\to 0 }\limsup_{n\to\infty}\E[Z_n(\cuA,\cuB, \sPi,\bw) \one_{\bw\in\cG_\delta}] 
%   \le 
%\sup_{\mu\in \cuT}
%   \sup_{\nu\in\cuV(\bR(\mu))}\inf_{\bS\succ\bzero}
%   \Phi(\nu,\mu,\bS,\bR(\mu)).
%\end{align}
%\end{theorem}

%\bns{This was moved to main paper. Delete}
%We'll show that under Assumption~\ref{ass:convexity}, the formula for $\phi$ of Theorem~\ref{thm:general} can be simplified to the one in the statement.
%The following identity can be easily verified from the definition of $\KL$:
%\begin{align}
%   -\frac12 \log \det(\bR)  + \frac12 \Tr((\bI_k - \bR)^{-1} \E[\bbv\bbv^\sT]) -
%   \KL(\nu_{\cI_{v}\| \cI_{w}} \| \cN(\bzero, \bI_{k+k_0}))
%= - \KL(\nu_{\cI_{v}\| \cI_{w}} \| \cN(\bzero, \bR)).
%\end{align}
%Meanwhile, for $\rho(t) = \lambda t^2/2$, so that $\grad_{\btheta}\rho(\btheta) = \lambda \btheta,$  one obtains after using the constraint $\E[\grad\ell \bbv^\sT + \grad \rho [\btheta,\btheta_0]^\sT] = \bzero$  that
%\begin{align}
%   &-\frac1{2\alpha}
%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\E_\mu[\grad \rho \grad \rho^\sT]\right) + \frac1{2\alpha}\Tr\left(\E\left[\bbv\grad\ell^\sT\right] (\E[\grad\ell\grad\ell^\sT])^{-1}\E[\grad\ell\bbv^\sT] \bR(\mu)^{-1}\right)\\
%&\hspace{20mm}=
%   -\frac{\lambda^2}{2\alpha}
%\Tr\left( (\E_\nu[\grad\ell\grad\ell^\sT])^{-1}\bR_{11}(\mu)\right) + \frac{\lambda^2}{2\alpha}\Tr\left((\E[\grad\ell\grad\ell^\sT])^{-1}
% [\bR_{11}(\mu),\bR_{10}(\mu)] \bR(\mu)^{-1} [\bR_{11}(\mu),\bR_{10}(\mu)]^\sT
%\right)\\
%&\hspace{20mm}= 0
%\end{align} where the last equality follows from $[\bR_{11},\bR_{10}] \bR^{-1} [\bR_{11},\bR_{10}]^\sT - \bR_{11}$.
%Finally, using the upper bound on the log potential in Lemma~\ref{lemma:variational_log_pot}, one concludes the theorem as a corollary of Theorem~\ref{thm:general}.
%\qed

%\subsection{Variational formula under convexity}
%For $\bTheta \in\R^{d\times k}$, $\bTheta_0\in\R^{d\times k}$, 
%$\hmu_{\bTheta,\bTheta_0}$ to be the empirical distribution of
%rows of $[\bTheta,\bTheta_0]\in \R^{d\times (k+k_0)}$. We further define  
%%
%\begin{align}
%\bR(\hmu_{\bTheta,\bTheta_*}) :=\left(\begin{matrix}
%\bTheta^{\sT}\bTheta & \bTheta^{\sT}\bTheta_0\\
%\bTheta_0^{\sT}\bTheta & \bTheta_0^{\sT}\bTheta_0\\
%\end{matrix}\right)=\int \bt\bt^{\sT} \hmu_{\bTheta,\bTheta_0}(\de\bt)\, .
%\end{align}
%%
%Given block matrix $\bR\in \sS_{k+k_0}$ we define
%%
%\begin{align}
%\bR = \left(\begin{matrix}
%\bR_{11} & \bR_{10}\\
%\bR_{01} & \bR_{00}\\
%\end{matrix}\right)
%\;\;\Rightarrow\;\; \bR/\bR_{00} = \bR_{11}-\bR_{10}\bR_{00}^{-1}
%\bR_{01}\, .
%\end{align}
%%
%Let $\cA\subseteq \cuP(\R^{k+k_0}),\cB \subseteq \cuP(\R^{k+k_0+1})$ and $\eps_\bH,\eps_{\bL},\eps_{\bR}> 0$.
%Define
%%
%\begin{align}
%Z_n(\cA,\cB,\bTheta_0,\eps_\bH,\eps_\bR) := \left|\Big\{\bTheta\in \R^{d\times k}:\; 
%\nabla_{\bTheta} \hR_n(\bTheta)=\bzero,\,
%\nabla^2_{\bTheta} \hR_n(\bTheta)\succeq \eps_\bH,\,
%\;\; \hmu_{\bTheta,\bTheta_0} 
%\in \cA,\,
%\hat\nu_{\bX\bTheta,\bX\bTheta_0} \in\cB,\;
%\bR(\hmu_{\bTheta,\bTheta_0}) \succ\eps_{\bR}
%\Big\}\right|\,.
%\end{align}
%\bns{We will need assumptions guaranteeing a quantity lower bound on $\bL^\sT\bL\succ \eps_\bL$, $\tilde\bV^\sT\tilde\bV\succ\eps_\bV $ from $\bR \succ \eps_\bR$.}
%%
%Further, we denote by 
%\begin{equation}
%\cuL(\bTheta_0) := \left\{
%\mu \in\cuP(\R^{k+k_0})  :  \mu_{\btheta_0} = \widehat\mu_{\bTheta_0},  \bR(\mu) \succ\eps_{\bR}
%\right\},
%\end{equation}
%and
%\begin{align}
%    \cV(\P_w) &:= \Big\{
%    \nu \in\cuP(\R^{k+k_0 + 1}) : \E_\nu[\grad \ell \bv^\sT] = \bzero_{k\times k}, \;
%     \E_\nu[\grad \ell \bu^\sT] = \bzero_{k\times k_0}, 
%     \;
%       \nu_W =  \P_{\bw},\;\\
%&\quad\quad\quad\textcolor{red}{\inf_{\bS \succ\bzero} G(\bS;\nu) < \eps_{\bH}},\;
%\textcolor{red}{\E_\nu[\grad \ell \grad\ell^\sT]\succ\eps_{\bL}},\,
%\textcolor{red}{\E_\nu[(\bv,\bu)(\bv,\bu)^\sT] \succ \eps_{\bV} }
%%    \inf_{\bA \succ \bzero_{k\times k}} 
%    %\frac1k \Tr\left(\bG(\bA; \nu)\right) \le 0
%    \Big\}\, .
%\end{align}
%%
%Further define $\Phi_n:\cuP(\R^{k+k_*})\times\cuP(\R^{k+k_*+1})\times\sS^{k+k_0}\times \sS^k \to \R$
%via
%\begin{align}
%    \Phi_n(\mu,\nu,\bR,\bS)&:=  
%\frac{k}{2 \alpha_n} \log(\alpha_n) -
% \E_{\nu}[\log\det(\bI + \grad^2 \ell(\bv,\bu, w)\bS) ]  +\frac1{\alpha_n} \log\det(\bS) + \frac{k}{\alpha_n}\\
%&\quad+ \frac1{2\alpha_n} \log\det(\E_{\nu_{\tilde\bv,w}}[\grad\ell\grad\ell^\sT]) 
%-\frac{1}{2\alpha_n} \Tr\left(\bR_{11} \right)
%+\KL(\nu_{\tilde\bv | \bw}, \cN( 0 ,\bR))
%+\frac{1}{\alpha_n}\KL(\mu_{\btheta|\btheta_0}, \cN( 0 , \bI_k ))
%\\
% &= \frac{1}{2\alpha_n} \log\det\left(\E_\nu\left[ \grad \ell\grad\ell^\sT\right]\right)
%+ 
%\frac1{\alpha_n}\log\det\bS 
%- \E_{\nu}\left[\log \det \left(\bI_k + \grad^2 \ell^{1/2} \bS \grad^2 \ell^{1/2}\right)\right]
%\nonumber\\
%&\quad+\frac{k}{2\alpha_n} 
%+ \frac{k}{2\alpha_n} \log(\alpha_n)
%-\frac1{2\alpha_n} \log\det\left(
%\bR/\bR_{00}
%\right)  \\
%&\quad + \KL\left(\nu_{\bv,\bu | W}\| \cN(\bzero, \bR)\right)
%+\frac{1}{\alpha_n}\KL\big(\mu_{\btheta|\btheta_0}\| \normal(0,\bR/\bR_{00})\big)
%-\frac1{2\alpha_n} \Tr\left(\E_{\mu_{\btheta|\btheta_0}}[\btheta\btheta^\sT(\bR/\bR_{00})^{-1}]\right)
%.\nonumber
%\end{align}
%Let $G: \sS^k\times \cuP(\R^{k+k_0+1})\mapsto \R $ be defined by
%\begin{equation}
%    G(\bS ;\nu ) := \frac1k \Tr\left(\frac1\alpha_n \bS^{-1} - \E_\nu[(\bI_{k} + \grad^2\ell \bS)^{-1} \grad^2 \ell]\right).
%\end{equation}
%
%%\textcolor{blue}{
%%\begin{align}
%%     \frac{d}{2}\Tr\left( \E_{\mu_{\theta| \theta_0}} \left[\bR_{11}\right]\right) 
%%    -\KL(\mu_{\btheta|\btheta_0}\| \cN(0,\bI)) &= 
%%      H(\mu_{\btheta| \btheta_0}) -  \frac{d}{2}\log(2\pi)\\
%%     &=
%%      H(\mu_{\btheta| \btheta_0})  
%%    + \E_{\mu_{\btheta|\btheta_0}}\left[\log\left( \frac{\det(\bR/ \bR_{00})^{-d/2}}{(2\pi)^{d/2}}\exp\left\{-\frac{1}{2} \btheta^\sT(\bR/\bR_{00})^{-1}\btheta
%%    \right\}\right)\right]\\
%%&\quad+\frac{d}{2}\log\det\left(\bR/\bR_{00}\right) + \frac12 \Tr\left(\E_{\mu_{\btheta|\btheta_0}}\left[\btheta\btheta^\sT\right](\bR/\bR_{00})^{-1}\right).
%%\end{align}
%%}
%
%
%\begin{theorem}[Formula under convexity]
%\label{thm:convexity}
%Under Assumption~\notate{ref, and assumption on $\cA,\cB$ from LDP},
% we have  
%\begin{align}
% \frac{1}{n}\log\E_{\bX}\left[Z(\cA,\cB,\bTheta_0,\eps_\bH,\eps_\bR)\right]
%    =-\inf_{ \mu\in\cA\cap\cuL(\bTheta_0)} 
%    \inf_{\nu \in \cB\cap\cV(\P_w)}\sup_{\bS\succ\bzero}\Phi_n(\mu,\nu, \bR(\mu), \bS) 
%   + \textcolor{red}{\omega(k,n,d,\eps_\bH,\eps_\bL,\eps_{\bV},\eps_{\bR})}
%\end{align}
%%
%where
%\begin{equation}
%\textcolor{red}{\omega(k,n,d,\eps_\bH,\eps_\bL,\eps_{\bV},\eps_{\bR})} = \dots.
%\end{equation}
%\end{theorem}
%%\begin{remark}
%%Clearly, the constraints on $\nu$ \textcolor{red}{in red}
%%\begin{equation}
%% \inf_{\bS \succ\bzero} G(\bS;\nu) < 0 ,\quad \E_{\nu}[\grad \ell\grad\ell^\sT] \succ\bzero
%%\end{equation}
%%and  on $\mu$
%%\begin{equation}
%%    \bR(\mu)\succ\bzero
%%\end{equation}
%%can be removed and still obtain a valid upper bound. Specifically, removing the support constraint can be done before applying the large deviation result, if one first realizes the logarithmic potential as a variational principle at that stage.
%%\end{remark}
%
%


%\subsection{Solving the asymptotic formula}
%
%\begin{remark}
%    Note the conditions on the determinant and trace in 1. above imply that the 
%    symmetric matrix 
%\begin{equation}
%    \E[\grad \ell \grad \ell^\sT]^{1/2}\bS \bSigma^{-1} \bS \E[\grad \ell \grad \ell^\sT]^{1/2}
%\end{equation}
%has eigenvalues all equal to $1/\alpha$ and hence 
%\begin{equation}
%    \E[\grad \ell \grad \ell^\sT]^{1/2}\bS \bSigma^{-1} \bS \E[\grad \ell \grad \ell^\sT]^{1/2} = \frac1\alpha \bI_k.
%\end{equation}
%
%\end{remark}
%
%\bns{The following proof has been moved. Erase it}
%\begin{proof}[Proof of Theorem~\ref{thm:global_min}]
%Let $\bSigma = \bSigma(\bR) := \bR/\bR_{00}$ for ease of notation.
%Define
%\begin{align}
%\Phi_1(\mu)
%&:=\frac{k}{2\alpha}
%- \frac{1}{2\alpha} \Tr\left(\bR_{11}(\mu)\right)
%+ \frac1{2\alpha} \log \det(\bSigma(\bR(\mu)))
% + \frac1\alpha \KL ( \mu_{\btheta| \btheta_0}\| \cN(\bzero,\bI_k)).\\
%    \Phi_2(\nu,\bR,\bS) &:=
%    -\lambda\Tr(\bS)  +
%    \frac{1}{2\alpha} \log\det\left(\E_\nu\left[ \grad \ell\grad\ell^\sT\right]\right)
%+ 
%\frac1{\alpha}\log\det\bS 
%- \E_{\nu}\left[\log \det \left(\bI_k + \grad^2 \ell^{1/2} \bS \grad^2 \ell^{1/2}\right)\right]\\
%&\quad+\frac{k}{2\alpha} 
%+ \frac{k}{2\alpha} \log(\alpha)
%-\frac1{2\alpha} \log\det\left(
%\bSigma(\bR)
%\right)   + \KL\left(\nu_{\bv,\bv_0 | W}\| \cN(\bzero, \bR)\right).
%\end{align}
%We'll lower bound each of these functions separately in what follows.
%
%\noindent\textbf{Lower bounding $\Phi_1$:}
%For any fixed $\bR,$  it's easy to see that the minimizing measure of 
%\begin{align}
%    \inf_{\mu : \bR(\mu) = \bR}  \frac1{\alpha}\KL(\mu_{\btheta| \btheta_0} \| \cN(\bzero,\bI_k)) 
%\end{align}
%will be Gaussian.
%By Gaussian conditioning, one then sees that for $\btheta_0 \in\R^{k_0}$,
%$\mu'_{{\btheta}|{\btheta_0}}(\btheta_0)$ 
%of Definition~\ref{def:opt_FP_conds}
%is the unique Gaussian measure joint measure $\mu'$ satisfies the second moment constraint $\bR = \bR(\mu')$.
%We can then directly compute for the fixed $\bR$,
%\begin{align}
%     \frac1{\alpha}\KL( \mu' \| \cN(\bzero,\bI_k))  
%     &= 
%     \frac1{2\alpha}\E_{\btheta_0\sim\mu_{\btheta_0}}\left[ -\log\det(\bSigma(\bR)) - k 
%     + \Tr(\bR_{11} - \bR_{10} \bR_{00}^{-1} \bR_{10})+
%      \btheta_0^\sT \bR_{00}^{-1}\bR_{01}\bR_{10}\bR_{00}^{-1} \btheta_0\right]\\
%      &=- \frac1{2\alpha}\log\det(\bSigma(\bR)) - \frac{k}{2\alpha} + \frac{1}{2\alpha}\Tr(\bR_{11}).
%\end{align}
%Consequently, for any $\mu$ as in the statement of the theorem, we have
%\begin{equation}
%    \Phi_1(\mu) \ge \inf_{\mu \in\cuP(\R^{k+k_0})} \Phi_1(\mu) = \inf_{\bR\succeq \bzero} \left\{
%\frac{k}{2\alpha}
%- \frac{1}{2\alpha} \Tr\left(\bR_{11}(\mu)\right)
%+ \frac1{2\alpha} \log \det(\bSigma(\bR(\mu))) 
%- \frac1\alpha \KL(\mu' \| \cN(\bzero,\bI_k))
%    \right\} = 0,
%\end{equation}
%with equality if and only if $\mu = \mu'$.
%
%\noindent \textbf{Lower bounding $\Phi_2$:}
%To lower bound $\Phi_2$, we'll rewrite the divergence term as a divergence involving the distribution $\nu^\star$ of the proximal operator in the Definition~\ref{def:opt_FP_conds}.
%In what follows, we use $(\bv,\bv_0,W)$, $\bv\in\R^{k},\bv_0\in\R^{k_0},W\in\R$ to denote random variables whose distribution is $\nu^\star.$
%To that end, let $\bg,\bg_0$ be jointly Gaussian as in Definition~\ref{def:opt_FP_conds}, and $W\sim\P_w$. 
%For any $\bS,\bR \succ\bzero$,
%denoting
%$p_{\bS,\bR}(\bv| \bv_0, W)$ the conditional density of $\Prox(\bg; \bS, \bg_0, W)$ given $W,\bv_0$,
%we find that
%\begin{align}
%    p^\star_{\bS,\bR}(\bv|\bv_0, W) =& \exp\left\{ -\frac12 (\bS\grad\ell(\bv,\bv_0, W )+\bv-\bmu(\bv_0,\bR))^\sT\bSigma(\bR)^{-1}(\bS\grad\ell(\bv,\bv_0, W )+\bv-\bmu(\bv_0,\bR))\right\}\\
%    &\quad\quad(2\pi)^{-k/2}\det(\bSigma(\bR))^{-1/2}
%     \det\left(\bI_k+\grad^2\ell(\bv,\bv_0,W)^{1/2}\bS \grad^2\ell(\bv,\bv_0,W)^{1/2}\right),
%\end{align}
%where $\bmu := \bmu(\bv_0, \bR) := \bR_{10}\bR_{00}^{-1}\bv_0$.
%So the divergence of the conditional measure $\nu_{\bv|\bv_0,W}$ from $\cN(\bmu,\bSigma)$ can be written as
%\begin{align*}
%    \KL\left(\nu_{\bv|\bv_0,W}\|  \cN(\bmu,\bSigma)\right) 
%    &=\KL\left(\nu_{\bv|\bv_0,W}\|  p^\star_{\bS,\bR}(\cdot | \bv_0, W) \right) 
%    +
%     \E_\nu\left[\log\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right]
%     -\frac12\E_\nu\left[\grad \ell^\sT \bS \bSigma(\bR)^{-1} \bS\grad \ell\right]\\
%     &-\E_\nu\left[
%    \grad \ell^\sT \bS \bSigma(\bR)^{-1} \left(\bv - \bmu\right)
%     \right].
%\end{align*}
%Recall that $\nu\in\cuV(\bR)$
%implies that $\E[\grad \ell \cdot (\bv^\sT,\bv_0^\sT)] + \lambda (\bR_{00},\bR_{01}) = \bzero,$ whence
%\begin{align}
%\E_\nu\left[
%    \grad \ell^\sT \bS \bSigma(\bR)^{-1} \left(\bv - \bmu\right)
%     \right]  &=  \Tr\left( \bS \bSigma(\bR)^{-1} \E_{\nu}[\bv \grad\ell^\sT - \bR_{10}\bR_{00}^{-1}\bv_0\grad\ell^\sT]\right)  \\
%&=
%-\lambda\Tr\left( \bS 
%(\bR_{00} - \bR_{10}\bR_{00}^{-1}\bR_{01})^{-1}
%(\bR_{00} - \bR_{10}\bR_{00}^{-1}\bR_{01})\right) \\
%&= -\lambda \Tr(\bS).
%\end{align}
%This, along with the chain rule for the KL-divergence and the expansion of the conditional KL above gives
%\begin{align}
%\nonumber
%  \KL\left(\nu_{\bv, \bv_0|W}\|  \cN(\bzero,\bR)\right) &= 
%\KL\left(\nu_{\bv|\bv_0,W}\|  p^\star(\cdot | \bv_0, W) \right) 
%    +
%     \E_\nu\left[\log\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right]
%     -\frac12\E_\nu\left[\grad \ell^\sT \bS \bSigma(\bR)^{-1} \bS\grad \ell\right]\\
%  &\quad\quad+\KL\left(\nu_{\bv_0|W}\|  \cN(\bzero,\bR_{00})\right) + \lambda \Tr(\bS).
%\end{align}
%By substituting this equality for the KL term into $\Phi_2$ and carrying out the appropriate cancellations, 
%this shows that for any $\mu,\nu$ as in the statement, 
%\begin{align}
%   \sup_{\bS\succ\bzero}\Phi_2(\nu,\bR(\mu),\bS) 
%     &=
%    \sup_{\bS\succ\bzero} \bigg\{\frac1{2\alpha} \log\det \left(
%    \E_\nu[\grad \ell \grad \ell^\sT ] \bS^2 \bSigma(\bR(\mu))^{-1}
%    \right)-\frac12\E_\nu[\grad\ell^\sT \bS \bSigma(\bR(\mu))^{-1}\bS \grad\ell]\\
%    &\hspace{15mm}+\frac k{2\alpha}\log(\alpha e) +\KL(\nu_{\bv|\bv_0,W}\|p^\star_{\bS,\bR(\mu)})+\KL(\nu_{\bv_0|W}\|\cN(\bzero,\bR(\mu)_{00}))\bigg\}\\
%&\stackrel{(a)}{\ge}
%    \sup_{\bS\succ\bzero} \left\{M(\bS;\nu, \bR(\mu)) \right\}
%\end{align}
%where
%\begin{equation}
%    M(\bS;\nu, \bR) = \frac1{2\alpha} \log\det \left(
%    \E_{\nu}[\grad \ell \grad \ell^\sT ] \bS^2 \bSigma(\bR)^{-1}
%    \right)-\frac12\E_{\nu}[\grad\ell^\sT \bS \bSigma(\bR)^{-1}\bS \grad\ell]+\frac k{2\alpha}\log(\alpha e).
%\end{equation}
%The inequality in $(a)$ follows from non-negativity of the KL-divergence and holds with equality if and only if $\nu_{\bv,\bv_0| W} = \nu_{\bv,\bv_0|W}^\star$, the measure induced by the density $p^\star_{\bS,\bR}$ defined above.
%Since $\E_\nu[\grad\ell\grad\ell^\sT]\succ \bzero,  \bR(\mu) \succ\bzero$ for such measures, one can check that $M(\bS;\nu,\bR)$ is strictly concave in $\bS$ and is uniquely maximized at 
%\begin{equation}
%    \bS= \bS^\star(\nu,\bR) =\frac1{\sqrt\alpha}\bSigma(\bR)^{1/2}\left(\bSigma(\bR)^{-1/2}\E_{\nu}[\grad\bell\grad\bell^\sT]^{-1} \bSigma(\bR)^{-1/2}\right)^{1/2}\bSigma(\bR)^{1/2},
%\end{equation}
%with $M(\bS^\star(\nu,\bR);\nu, \bR) = 0$.
%\newline
%
%\noindent\textbf{Concluding:}
%Using the lower bounds above, for any $\mu,\nu$ as in the statement, we have by design
%\begin{align}
%   \sup_{\bS\succ\bzero}  
%   \Phi(\mu,\nu,\bR(\mu),\bS) &=
%    \Phi_1(\mu)  
%    +\sup_{\bS\succ\bzero}\Phi_2(\nu,\bR(\mu),\bS)
%    = \inf_{\mu_0 : \bR(\mu_0) = \bR(\mu)}
%  \left\{
%    \Phi_{1}(\mu_0) + \sup_{\bS\succ\bzero} \Phi_2(\nu',\bR(\mu), \bS) 
%    \right\}\\
%    &= \inf_{\mu_0 : \bR(\mu_0) = \bR(\mu)}
%    \Phi_{1}(\mu_0) + \sup_{\bS\succ\bzero} \Phi_2(\nu',\bR(\mu), \bS) 
%    \stackrel{(a)}{\ge} 0 + \sup_{\bS\succ\bzero} \Phi_2(\nu',\bR(\mu), \bS)\\
%    &\stackrel{(b)}{\ge} 0 + 
%\sup_{\bS\succ\bzero}  M(S; \nu,\bR(\mu)) = 0.
%\end{align}
%where in $(a)$ we used that for any $\bR \succ\bzero$, the Gaussian measure $\mu' = \mu'(\bR)$ chosen previously satisfies $\Phi_1(\mu') = 0$.
%By the previous steps, $(a)$ and $(b)$ hold with equality if and only $(\mu,\nu)$ are as given in Definition~\ref{def:opt_FP_conds}.
%
%%Combining with Eq.~\eqref{eq:phi_1_LB} shows that $\sup_{\bS}\Phi \ge0$, with equality if and only if the inequality in $(a)$ and the inequality in Eq.~\eqref{eq:phi_1_LB} hold with equality, i.e., if and only if $\nu$ and $\mu$ satisfy the equations in Definition~\ref{def:opt_FP_conds}.
%
%\textbf{Proof of \textit{1.} and \textit{2.}:}
%To prove \textit{1.}. let $\Omega_0 := \{\widehat\bTheta_n \in \cE(\bTheta_0)\}$, and $\Omega_1 := \{ n C_0(\alpha) \succ \bX^\sT\bX\succ nc_0(\alpha)\}$. 
%Since Assumption~\ref{ass:loss} guarantees that $\|\grad^2\hat R_n(\bTheta)\|_\op =O(1)$ on $\Omega_1$, by Lemma~\ref{lemma:jacobian_lb}, $\sigma_{\min}(\bJ_{(\bbV,\bTheta)} \bG) = e^{-o(n)}$ on this event, so that on $\Omega_0 \cap\Omega_1$, the point $\hat\bTheta_n \in \cZ_n$ of Eq.~\eqref{eq:set_of_zeros_main} for some choice of $\sPi$ satisfying Assumption~\ref{ass:params}.
%
%For $\eps >0$, consider the set 
%\begin{equation}
%    \cuA_\eps:=  \{\mu : d_{\BL}(\mu,\mu_\opt) \le\eps \},\quad
%    \quad\quad
%    \cuB_\eps := 
%     \{\nu : d_{\BL}(\nu,\nu_\opt) \le\eps \}.
%\end{equation}
%Then
%there exists some $c_0(\eps) >0$ such that
%for any $\mu \in\cuT(\cuA^c_\eps),\nu \in\cuV(\bR(\mu),\cuB^c_\eps)$, $\sup_{\bS\succ\bzero}\Phi(\mu,\nu,\bR(\mu)) > c(\eps)$ uniformly. 
%So
%using the shorthand 
%$\hmu:=\hat\mu_{\sqrt{d}[\hat\bTheta_n,\bTheta]}$ and 
%$\hnu := \hat\nu_{[\bX\hat\bTheta_n,\bX\bTheta]}$,
% we can bound for any $\delta>0$,
%\begin{align}
%    \P\left( \left\{ d_{\LU}(\hmu, \mu_\opt) > \eps\right\}
%\cup
%\left\{ d_{\LU}(\hnu, \nu_\opt) > \eps\right\}
%    \right)
%    &\le  \P\left(\{(\hmu,\hnu) \in\cuA_\eps^c \times \cuB_\eps^c\} \cap \{\bw \in\cG_\delta\}\cap \Omega_0\cap\Omega_1\right) + \P(\Omega_0^c)
%+ \P(\Omega_1^c)
%+ \P(\cG_\delta^c)\\
%&\le \P\left(\one_{\hat\bTheta_n \in \cZ_n(\cuA_\eps^c,\cuB_\eps^c, \sPi)} \one_{\bw \in\cG_\delta}\right)
%+ \P(\Omega_0^c)
%+ \P(\Omega_1^c)
%+ \P(\cG_\delta^c).
%\end{align}
%Taking $n\to\infty$ and noting that
%\begin{equation}
%    \lim_{n\to\infty }
%( \P(\Omega_0^c)
%+ \P(\Omega_1^c)
%+ \P(\cG_\delta^c)) = 0
%\end{equation}
%by the assumption on $\hat\bTheta_n$ and Assumption~\ref{ass:noise},
%we conclude by Theorem~\ref{thm:convexity} that for all $\eps>0$
%\begin{equation}
%    \lim_{n\to\infty}\P\left( \left\{ d_{\LU}(\hmu, \mu_\opt) > \eps\right\}
%\cup
%\left\{ d_{\LU}(\hnu, \nu_\opt) > \eps\right\}
%    \right)
%    \le 
%\lim_{n\to\infty}
%\E[\cZ_{n}(\cuA_\eps^c, \cuB_\eps^c, \sPi)\one_{\{\bw\in\cG_\delta\}} ] \le \lim_{n\to\infty} e^{- n c(\eps)} = 0
%\end{equation}
%giving the statement of \textit{1.} of the Theorem. 
%Finally, for \textit{2.}, by Lemma~\ref{lemma:rate_matrix_ST} along with an argument similar to that of Lemma~\ref{lemma:asymp_ST}, we can deduce that for any $\hnu \Rightarrow \nu$ in probability,
%\begin{equation}
%     \frac1{dk} (\bI_k \otimes \Tr) \left(\bH(\hnu_n) - z\bI_{dk}\right)^{-1} \to \alpha \bS_\star(\nu,z)
%\end{equation}
%in probability. The claim now follows by from \textit{1.} after recalling the definition of $\bH$.
%
%
%
%\end{proof}
%
%
%
%
%\newpage
%
%\subsection{Solving the asymptotic formula}
%To solve the formula, let us define the multivariate proximal operator in what follows. For $\bz \in\R^k, \bu \in\R^{k_\star}, \bS\in\R^{k\times k}, w\in\R,$ let
%\begin{equation}
%    \Prox(\bz;\bS, \bu, w)=\arg\min_{\bx\in\R^k}\left( \frac12(\bx-\bz)^\bT\bS^{-1}(\bx-\bz) + \ell(\bx,\bu,w)\right)\in\R^k.
%\end{equation}
%Observe that for $\ell$ convex in $\bv$ at fixed $\bu,w$,  the map $\bz \mapsto \Prox(\bz; \bS, \bu, w)$  is invertible  for any $\bS \succeq \bzero_{k\times k}$, with inverse given by
%
%\begin{equation}
%\Prox^{-1}(\bv; \bS, \bu, w) = \bS\grad\ell(\bv,\bu,w)+\bv,
%\end{equation}
%which can be derived from the first order conditions
%\begin{align}
%    %\rho_{\bQ,\bu,w}(\bv)=&\min_{\bx\in\R^k}\left( \frac12 (\bx-\bv)^\bT\bQ^{-1}(\bx-\bv)+\rho(\bx,\bu,w)\right)\in\R\\
%     \bS\grad\ell(\Prox(\bz;\bS, \bu, w),\bu,w)=\bz- \Prox_{\bu,w}(\bz; \bS, \bu, w)
%\end{align}
%where $\grad \ell \in\R^{k}$  is the gradient of $\ell$ with respect to the first $k$ variables.
%The following defines the optimality conditions for a given $(\mu,\nu,\bR,\bS)$.
%
%\begin{definition}
%\label{def:opt_FP_conds}
%   We say that the pair $(\mu',\nu')\in \cuL(\bTheta_0)\times \cV(\P_w)$ satisfy the \emph{fixed point conditions} if the following holds:
%   With the definitions $\bR' = \bR(\mu')$, $(\bg^\sT,\bg_0^\sT)^\sT \sim \cN\left( \bzero_{k+k_0},\bR'\right),\;W\sim\P_w,$
%%\begin{align}
%%\Tr\left(\bS (
%%\bR_{11} - \bR_{12}\bR^{-1}_{22}\bR_{21})^{-1} \bS 
%%\E_{\nu}[\grad \rho \grad \rho^\sT]\right) &= \Tr\left(\frac1\alpha \bI_k\right)\\
%%\det\left( \bS (
%%\bR_{11} - \bR_{12}\bR^{-1}_{22}\bR_{21})^{-1} \bS 
%%\E_{\nu}[\grad \rho \grad \rho^\sT]\right) &= \det\left(\frac1\alpha \bI_k\right),
%%\end{align}
%and
%\begin{equation}
%    \bS'\equiv \bS'(\nu', \mu') =\frac1{\sqrt\alpha_n}(\bR'/\bR'_{00})^{1/2}\left((\bR'/\bR'_{00})^{-1/2}\E_{\nu'}[\grad\bell\grad\bell^\sT]^{-1} (\bR'/\bR'_{00})^{-1/2}\right)^{1/2}(\bR'/\bR'_{00})^{1/2},
%\end{equation}
%the following fixed point equations are satisfied
%   \begin{align}
%&\nu' \equiv \nu'_{\bv,\bu,W} = \mathrm{Law}(
%\Prox( \bg; \bS', \bg_0, W),\bg_0,W),\\
%&\mu_{\btheta_0}' = \mathrm{Law}(\hat\mu_{\bTheta_0}),\quad  \mu'_{\btheta|\btheta_0} = \cN(\bzero,\bR'/\bR'_{00}),\\
%    &\E_{\nu'}\left[ \grad \ell(\bv,\bu,W) (\bv,\bu)^\sT\right] = \bzero_{k\times (k+k_0)}.
%\end{align}
%\end{definition}
%\begin{remark}
%    Note the conditions on the determinant and trace in 1. above imply that the 
%    symmetric matrix 
%\begin{equation}
%    \E[\grad \ell \grad \ell^\sT]^{1/2}\bS \bSigma^{-1} \bS \E[\grad \ell \grad \ell^\sT]^{1/2}
%\end{equation}
%has eigenvalues all equal to $1/\alpha$ and hence 
%\begin{equation}
%    \E[\grad \ell \grad \ell^\sT]^{1/2}\bS \bSigma^{-1} \bS \E[\grad \ell \grad \ell^\sT]^{1/2} = \frac1\alpha \bI_k.
%\end{equation}
%
%\end{remark}
%
%\begin{theorem}
%Fix $\cA,\cB$ as in Theorem~\notate{ref}. For any $\mu\in\cuL(\bTheta_0)\cap \cA, \nu \in\cV(\P_w) \cap \cB$,
%\begin{equation}
%    \sup_{\bS\succ\bzero}\Phi(\mu,\nu,\bR(\mu),\bS) \ge 0,
%\end{equation}
%with equality if and only if 
% $(\mu,\nu)$ satisfy the fixed point conditions of Definition~\ref{def:opt_FP_conds}.
%%For any $\bR \succeq \bzero_{(k+k^\star)\times (k+k^\star)}$, $\nu \in\cQ(\P_w)$, we have
%\end{theorem}
%\begin{proof}
%Let $\bSigma \equiv \bSigma(\bR) = \bR/\bR_{00}$ for ease of notation.
%%Let $(\mu',\nu',\bS')$ satisfy the conditions in Definition~\ref{def:opt_FP_conds}.
%%We'll show 
%%\begin{equation}
%%    \Phi(\mu,\nu,\bR(\mu),\bS) \ge \Phi(\mu',\nu',\bR(\mu'),\bS) = 0
%%\end{equation}
%%with equali
%Define
%\begin{align}
%\Phi_1(\mu) &:= \frac{1}{\alpha_n}\KL\big(\mu_{\btheta|\btheta_0}\| \normal(\bzero,\bR/\bR_{00})\big)
%-\frac1{2\alpha_n} \Tr\left(\E_{\mu_{\btheta|\btheta_0}}[\btheta\btheta^\sT(\bR/\bR_{00})^{-1}]\right).\\
%    \Phi_2(\nu,\bR,\bS) &:=
%    \frac{1}{2\alpha_n} \log\det\left(\E_\nu\left[ \grad \ell\grad\ell^\sT\right]\right)
%+ 
%\frac1{\alpha_n}\log\det\bS 
%- \E_{\nu}\left[\log \det \left(\bI_k + \grad^2 \ell^{1/2} \bS \grad^2 \ell^{1/2}\right)\right]\\
%&\quad+\frac{k}{2\alpha_n} 
%+ \frac{k}{2\alpha_n} \log(\alpha_n)
%-\frac1{2\alpha_n} \log\det\left(
%\bSigma(\bR)
%\right)   + \KL\left(\nu_{\bv,\bu | W}\| \cN(\bzero, \bR)\right).
%\end{align}
%Note that $\Phi(\mu,\nu,\bR(\mu),\bS) =  \Phi_1(\mu)+
%\Phi_2(\nu,\bR(\mu),\bS).$
%
%First observe that for any $\mu \in\cA \cap\cuL(\bTheta_0)$,
%\begin{equation}
%\label{eq:phi_1_LB}
%\Phi_1(\mu) \ge \inf_{\mu\in\cuP(\R^{k+k_0})} \Phi_1(\mu) = 0
%\end{equation}
%with the minimizer achieved uniquely at $\mu =\mu'$ of Definition~\ref{def:opt_FP_conds}.
%
%To deal with $\Phi_2$, we'll rewrite the divergence term as a divergence involving the distribution of the proximal operator in the definition~\ref{def:opt_FP_conds}.
%To that end, let $\bg,\bg_0$ be jointly Gaussian as in definition~\ref{def:opt_FP_conds}, and $W\sim\P_w$. Denoting
%$p(\bv|\bg_\star,W)\equiv p_{\bS,\bR}(\bv| \bg_\star, W)$ the conditional density of $\Prox(\bg; \bS, \bg_\star, W)$ given $W,\bg_\star$,
%we find that
%\begin{align}
%    \pi_{\bS,\bR}(\bv|\bu, W) =& \exp\left\{ -\frac12 (\bS\grad\ell(\bv,\bu, W )+\bv-\bmu(\bu,\bR))^\sT\bSigma(\bR)^{-1}(\bS\grad\ell(\bv,\bu, W )+\bv-\bmu(\bu,\bR))\right\}\\
%    &\quad\quad(2\pi)^{-k/2}\det(\bSigma(\bR))^{-1/2}
%     \det\left(\bI_k+\grad^2\ell(\bv,\bu,W)^{1/2}\bS \grad^2\ell(\bv,\bu,W)^{1/2}\right),
%\end{align}
%where $\bmu\equiv \bmu(\bu, \bR)=\bR_{12}\bR_{22}^{-1}\bu$.
%So the divergence of the conditional measure $\nu_{\bv|\bu,W}$ from $\cN(\bmu,\bSigma)$ can be written as
%\begin{align*}
%    \KL\left(\nu_{\bv|\bu,W}\|  \cN(\bmu,\bSigma)\right) 
%    &=\KL\left(\nu_{\bv|\bu,W}\|  p(\cdot | \bu, W) \right) 
%    +
%     \E_\nu\left[\log\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right]
%     -\frac12\E_\nu\left[\grad \ell^\sT \bS \bSigma(\bR)^{-1} \bS\grad \ell\right]\\
%     &-\E_\nu\left[
%    \grad \ell^\sT \bS \bSigma(\bR)^{-1} \left(\bv - \bmu\right)
%     \right].
%\end{align*}
%Noting that $\nu\in\cV(\P_w)$ implies that
%$\E_\nu\left[
%    \grad \ell^\sT \bS \bSigma(\bR)^{-1} \left(\bv - \bmu\right)
%     \right] =0$, we conclude by an application of the chain rule for the KL-divergence that
%\begin{align}
%\nonumber
%  \KL\left(\nu_{\bv, \bu|W}\|  \cN(\bzero,\bR)\right) &= 
%\KL\left(\nu_{\bv|\bu,W}\|  p(\cdot | \bu, W) \right) 
%    +
%     \E_\nu\left[\log\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right]
%     -\frac12\E_\nu\left[\grad \ell^\sT \bS \bSigma(\bR)^{-1} \bS\grad \ell\right]\\
%  &\quad\quad+\KL\left(\nu_{\bu|W}\|  \cN(\bzero,\bR_{00})\right).
%\end{align}
%This shows that for any $\mu,\nu$ as in the statement,
%\begin{align}
%   \sup_{\bS\succ\bzero}\Phi_2(\nu,\bR(\mu),\bS) 
%     &=
%    \sup_{\bS\succ\bzero} \bigg\{\frac1{2\alpha} \log\det \left(
%    \E_\nu[\grad \ell \grad \ell^\sT ] \bS^2 \bSigma(\bR(\mu))^{-1}
%    \right)-\frac12\E_\nu[\grad\ell^\sT \bS \bSigma(\bR(\mu))^{-1}\bS \grad\ell]\\
%    &\quad+\frac k{2\alpha}\log(\alpha e) +\KL(\nu_{\bv|\bu,W}\|p_{\bS,\bR(\mu)})+\KL(\nu_{\bu|W}\|\cN(\bzero,\bR_{00}(\mu)))\bigg\}\\
%&\stackrel{(a)}{\ge}
%    \sup_{\bS\succ\bzero} \left\{\frac1{2\alpha} \log\det \left(
%    \E_\nu[\grad \ell \grad \ell^\sT ] \bS^2 \bSigma(\bR(\mu))^{-1}
%    \right)-\frac12\E_\nu[\grad\ell^\sT \bS \bSigma(\bR(\mu))^{-1}\bS \grad\ell]+\frac k{2\alpha}\log(\alpha e) \right\}\\
%    &=0
%\end{align}
%with the maximum achieved uniquely at $\bS = \bS'(\nu,\mu)$ by strict concavity of the term above in $\bS$. Combining with Eq.~\eqref{eq:phi_1_LB} shows that $\sup_{\bS}\Phi \ge0$, with equality if and only if the inequality in $(a)$ and the inequality in Eq.~\eqref{eq:phi_1_LB} hold with equality, i.e., if and only if $\nu$ and $\mu$ satisfy the equations in Definition~\ref{def:opt_FP_conds}.
%
%
%\end{proof}
%
%
%
%\begin{proof}
%Let $\bg,\bg_\star$  be jointly Gaussian with mean $\bzero$ and covariance $\bR$, and let
%$\pi_{\bS,\bR}(\bv| \bg_\star, W)$ be the conditional density of $\Prox(\bg; \bS, \bg_\star, W)$ given $W,\bg_\star$.
%Then we can write
%\begin{align}
%    \pi_{\bS,\bR}(\bv|\bu, W) =& \exp\left\{ -\frac12 (\bS\grad\ell(\bv,\bu, W )+\bv-\bmu(\bu,\bR))^\sT\bSigma(\bR)^{-1}(\bS\grad\ell(\bv,\bu, W )+\bv-\bmu(\bu,\bR))\right\}\\
%    &(2\pi)^{-k/2}\det(\bSigma(\bR))^{-1/2}
%     \det\left(\bI_k+\grad^2\ell(\bv,\bu,W)^{1/2}\bS \grad^2\ell(\bv,\bu,W)^{1/2}\right),
%\end{align}
%where $\bmu(\bu, \bR)=\bR_{12}\bR_{22}^{-1}\bu$ and $\bSigma(\bR)=\bR_{11}-\bR_{12}\bR_{22}^{-1}\bR_{21}$.
%Noting that
%\begin{align}
%  \KL\left(\nu_{\bv, \bu|W}\|  \cN(\bzero,\bR)\right) &= 
%  \KL\left(\nu_{\bv|\bu,W}\|  \cN(\bmu,\bSigma)\right) + 
%  \KL\left(\nu_{\bu|W}\|  \cN(\bzero,\bR_{2,2})\right),
%\end{align}
%and by using the density above,\bns{Is the $+$ below a $-$?}
%\begin{align}
%    \KL\left(\nu_{\bv|\bu,W}\|  \cN(\bm(\bR),\bSigma(\bR))\right) 
%    &=\KL\left(\nu_{\bv|\bu,W}\|  \pi_{\bS,\bR}(\cdot | \bu, W) \right) 
%    +
%     \E_\nu\left[\log\left(\det\left(\bI_k+\grad^2\ell^{1/2}\bS \grad^2\ell^{1/2}\right)\right)\right]\\
%     &-\frac12\E_\nu\left[\grad \ell^\sT \bS \bSigma(\bR)^{-1} \bS\grad \ell\right]
%     {\color{red}+}\E_\nu\left[
%    \grad \ell^\sT \bS \bSigma(\bR)^{-1} \left(\bv - \bmu(\bu;\bR)\right)
%     \right].
%\end{align}
%Recalling the condition $\E_\nu[\grad\ell\bv^\sT] = \bzero, \E_\nu[\grad\ell \bu^\sT] = \bzero,$ and the definition of $\bmu$, 
%we conclude 
%\begin{align}
%\E_\nu\left[
%    \grad \ell^\sT \bS \bSigma(\bR)^{-1} \left(\bv - \bmu(\bu;\bR)\right)
%     \right]
%     &=
%\Tr\left(\E_\nu\left[
%     \bS \bSigma(\bR)^{-1} \left(\bv - \bmu(\bu;\bR)\grad \ell^\sT\right)
%     \right]\right) = 0.
%\end{align}
%
%Combining, we can rewrite $\Phi$ (recalling its definition) as
%\begin{align}
%\Phi(\bR,\nu,\bS) &:= \frac{1}{2\alpha} \log\det\left(\E_\nu\left[ \grad \ell\grad\ell^\sT\right]\right) \\
%&\quad+ 
%\frac1{\alpha}\log\det\bS 
%- \E\left[\log \det \left(\bI_k + \grad^2 \ell^{1/2} \bS \grad^2 \ell^{1/2}\right)\right]
%\\
%&\quad+\frac{k}{2\alpha} 
%+ \frac{k}{2\alpha} \log(\alpha)
%-\frac1{2\alpha} \log\det\left(
%\bSigma(\bR)
%\right)  + \KL\left(\nu_{\bv,\bu | W}\| \cN(\bzero, \bR)\right)\\
%     &=\frac1{2\alpha} \log\det \left(
%    \E_\nu[\grad \ell \grad \ell^\sT ] \bS^2 \bSigma(\bR)^{-1}
%    \right)-\frac12\E[\grad\ell^\sT \bS \bSigma(\bR)^{-1}\bS \grad\ell]\\
%    &\quad+\frac k{2\alpha}\log(\alpha e) +\KL(\nu_{\bv|\bu,W}\|\pi_{\bv|\bu,W})+\KL(\nu_{\bu|W}\|\cN(\bzero,\bR_{22})).
%\end{align}
%The claim now follows.
%\end{proof}





































%\section{Large deviations}
%\bns{There are many issues here that are not addressed in what is below: 
%\begin{enumerate}
%    \item We need to take into account the conditioning on $\bw$. So at some point one must bound 
%        $d(\widehat\nu_\bw,\P_w)$ for appropriate $d$. 
%    \item We need to deal with the integral constraint on the manifold: the problematic constraint is of the the form $\bL(\bV)^\sT\bV = 0$. This is a manifold of zero measure under the Gaussian distribution on $\bV$. I think this will require more work: namely, I believe we cannot get around quantifying the error term in the approximation of this integral to one where the constraint is replaced with $\norm{bL(\bV)^\sT\bV}\le \eps$,
%    and doing a sensitivity analysis on the constraint $|\E[\ell(V)V]|\le \eps$ that shows up in the asumptotic formula.
%    \item In the non-convex case, we have to deal with functions of the form 
%    \begin{equation}
%        \nu \mapsto F(\E_\nu[G(\bX,\bS(\nu)])),
%    \end{equation}
%    for $\bS$  that isn't a ``nice" function of $\nu$ (see Lemma~\ref{lemma:log_pot_z}).
%%    \item Even in the convex case, it seems to me that we require that the constraint on $\nu$
%%    \begin{equation}
%%        \inf\supp(\mu_\star(\nu)) >0
%%    \end{equation}
%%to appear in the asymptotic formula to get the tight bound. Though, we may be able to deal with this by 
%\end{enumerate}
%}
%
%
%
%%\section{Question about the left edge of support of $\mu_\star$}
%%  @Andrea: Is it clear to you what the correct formulation of the following is for the non convex case:
%%  Let 
%%  \begin{equation}
%%  \bG(\bS;\nu) :=  \alpha^{-1} \bS -\E_{\nu}[(\bI + \bD\bS)^{-1}\bD ],
%%  \end{equation}
%%  where the expectation is over $\bD\sim\nu$ for $\bD$ symmetric.
%%  If $\bD$ is positive semi-definite almost surely, then 
%%  \begin{equation}
%%      -\inf_{\bS\succ\bzero} \frac1k \Tr(\bG(\bS;\nu)) = \inf \supp(\mu_\star(\nu))
%%  \end{equation}
%%  where $\mu_\star(\nu)$ is the measure whose Stieltjes transform is characterized by $\bG(\bS_\star(z);\nu) = -z\bI$. In the general case where $\bD$ is not assume PSD, we need some constraint on the set in the $\inf$ constraint. It's not clear to me what the correct one is in the general $k$ case. As a reminder for the $k=1$, I copied over your old notes below. 
%%
%%\paragraph{Limit of the log determinant}
%%
%%Will assume $\nu$ to have bounded support with $x_{\min} = \min(x:x\in\supp(\nu))$,
%%$x_{\max} = \max(x:x\in\supp(\nu))$.
%%
%%Let $u_M=\infty$ if $x_{\min}\ge 0$ and $u_M:=-1/x_{\min}$ otherwise. For $z\in\bbC$ and $u\in (0,u_{M})$,
%%define (note that the logarithm arguments are positive in this domain):
%%%
%%\begin{align}
%%K(u;z) = -\delta zu +\delta\int \log(1+ux)\, \nu(\de x)-\log u-\log\delta-1\, ,
%%\end{align}
%%%
%%For $u\in\bbH:=\{w\in\bbC:\Im(w)>0\}$
%%the upper half plane, we define $K(u;z)$ to be the unique analytic function with 
%%whose value for  $u\in (0,u_{0})$ is given above. We denote by $\bbU:= \bbH\cap (0,u_0)$,
%%
%%Let $\bW\in\reals^{n\times d}$ be a matrix with i.i.d. entries $W_{ij}\sim\normal(0,1)$,
%%and $D$ a diagonal matrix whose entries empirical distribution converges to $\nu$
%%(with max/min entry converging to $x_{\max}$, $x_{\min}$). For $\Re(z)\le \lambda_{\min}(\bH)$, we define 
%%%
%%\begin{align}
%%\kappa_n(z) := \frac{1}{d} \log\det (\bH-z\id_d)\, ,\;\;\; \bH:= \frac{1}{n}\bW^{\sT}\bD\bW\, .
%%\end{align}
%%%
%%we consider $n,d\to\infty$ with $n/d\to\delta\in(1,\infty)$.
%%We also define the Stieltjis transform
%%%
%%\begin{align}
%%s_n(z) := \frac{1}{d} \Tr\big\{ (\bH-z\id_d)^{-1}\big\}\, ,\, .
%%\end{align}
%%%
%%
%%We also define
%%%
%%\begin{align}
%%G(u) := \frac{1}{\delta u} -\int \frac{x}{1+xu}\nu(\de x)\, .
%%\end{align}
%%
%%\begin{lemma}\label{lemma:Min_G}
%%The function $G$ has a unique local (hence global) minimum $u_{0}$ in $(0,u_M)$, and
%%\begin{align}
%%    \lim_{n,d\to\infty}\lambda_{\min}(\bH) = -G(u_0)\, .\label{eq:LimLambda_min}
%%\end{align}
%%\end{lemma}
%%%
%%\begin{proof}
%%To prove existence of the minimum we proceed sligthly differently depending whether 
%%$x_{\min}\ge 0$ or $x_{\min}<0$:
%%\begin{itemize}
%%\item If $x_{\min}\ge 0$, then $G(u) = 1/(\delta u)+O(1)$ as $u\to 0$ and
%%$G(u) = -(1-\delta^{-1})u^{-1}+O(u^{-2})$ as $u\to\infty$. Hence there is necessarily a local minimum
%%in $(0,\infty)$.
%%\item If   $x_{\min}\ge 0$, then again $G(u) = 1/(\delta u)+O(1)$ as $u\to 0$ and 
%%%
%%\begin{align}
%%    u_M^2G'(u_M) = -\frac{1}{\delta} +\int_{x_{\min}}^{\infty}\left(\frac{x}{x-x_{\min}}\right)^2\nu(\de x) >0\,. 
%%\end{align}
%%\end{itemize}
%%%
%%To prove uniqueness, note that
%%%
%%\begin{align}
%%    u^2G'(u) = -\frac{1}{\delta} +\int\left(\frac{xu}{1+xu}\right)^2\nu(\de x)\, ,
%%\end{align}
%%%
%%and this  is strictly increasing on $(0,u_M)$.
%%
%%The limit in Eq.~\eqref{eq:LimLambda_min} follows from \am{Find reference}
%%\end{proof}
%%%
%%\begin{lemma}\label{lemma:ustar}
%%Let $u_0$ be defined as in the statement of Lemma \ref{lemma:Min_G}.
%%For $z\in (-\infty,-G(u_0)]$ there is a unique solution $u_*=u_*(z)$ to 
%%%
%%\begin{align}
%%G(u) = -z\, ,\;\;\;\; u\in (0,u_0]\, ,
%%\end{align}
%%%
%%Further:
%%%
%%\begin{enumerate}
%%\item $u_*(z)$ is analytic, with $u_*(z) = -1/(\delta z)+o(1/z)$ as $z\to -\infty$.
%%\item  For $z\in (-\infty,-G(u_0))$  $u_*$ is the unique solution to 
%%%
%%\begin{align}
%%G(u) = -z\, ,\;\;\;\; u\in (0,u_M), G'(u)<0\, .
%%\end{align}
%%%
%%\item We have
%%%
%%\begin{align}
%%\lim_{n,d\to\infty}s_n(z) = \delta \, u_*(z)\, .
%%\end{align}
%%\end{enumerate}
%%\end{lemma}
%%%
%
%
%
%
%\newpage
%
%%The constraint becomes:
%%\begin{align}
%%    %\inf_{\{\bS : \inf_{\bD \in\supp_{\nu} \lambda_{\min}((\bI + \bD\bS)^{-1}\bD) \ge 0 }\}} \bQ(\bS;\nu) 
%%    \sup_{\xi>0} \inf_{\bS \succ\bzero}\sup_{\bD \in\supp(\nu)} \left\{
%%    \bQ(\bS;\nu)  - \xi\lambda_{\min}((\bI +\bD \bS)^{-1} \bD)
%%    \right\} < -\delta
%%\end{align}
%
%
%
%%\begin{lemma}
%%Consider the standard probability simplex $\Delta^m$ \emph{as a subset of } $\R^{m}$. For any $m>1$, there exists a cover for $\Delta^m$ with balls $\cB_2^m(m^{-1/2})$ of size at most \bns{add}.
%%\end{lemma}
%%\begin{proof}
%%We will bound the covering number by the packing number which we will crudely bound via a volume argument.
%%Define 
%%\begin{align}
%%   \Delta_2^m(\eps):= \{\bx\in\R^{m+1} : \bx^\sT\one = 1, \inf_{\by \in\Delta^m}\norm{\bx - \by}_2 \le \eps \}.
%%   %\{\bx\in\R^{m+1} : \bx^\sT\one = 1, \inf_{\by \in\Delta^m}\norm{\bx - \by}_1 \le \eps\sqrt{m} \} =: \Delta_1^m(\eps \sqrt{m}).
%%\end{align}
%%Note that 
%%\begin{align}
%%   \area\left(\Delta_2^m(\eps)\right)  &=
%%   \area\left(\{\bx \in\R^{m+1} : \bx = \by + \bdelta + \eps \one,\, \by\in\Delta^m,\,
%%   \norm{\bdelta}_2 \le \eps,\, \bdelta^\sT\one = 0 
%%   \}\right)\\
%%   &\le 
%%   \area\left(\{\bx \in\R^{m+1} : \bx^\sT\one = 1 + (m+1)\eps,\, \bx \succeq \bzero
%%   \}\right),
%%\end{align}
%%the latter inequality follows by inclusion.
%%Letting $\tilde\Delta_2^m(\eps),\tilde\Delta^m\subseteq\R^m$ be an embedding of $\Delta_2^m(\eps),\Delta^m$ in $\R^m$ (fix position and orientation for both arbitrarily), we have
%%\begin{equation}
%%\vol(\tilde \Delta^m + \cB_2^m(\eps)) =\vol(\tilde \Delta_2^m(\eps))  
%%   =\area(\Delta^m_2(\eps)) \le \area(\Delta_1^m(\eps \sqrt{m}).
%%\end{equation}
%%
%%Now observe that the 
%%
%%\begin{equation}
%%    \area(\Delta_1^m(\eps\sqrt{m}) = \area\left(
%%    \{\bx \in\R^{m+1}: \bx\succ\bzero, \bx^\sT \one = \eps \sqrt{m}\}
%%    \right).
%%\end{equation}
%%
%%\newpage
%%For $\eps >0$,
%%let $N(\eps)$ be the covering number of the simplex $\Delta^m$ with balls of radius $\cB_2^m(2\eps)$. 
%%Recall that a standard packing argument shows that
%%\begin{equation}
%%\label{eq:packing_bound}
%%    N(\eps) \le \frac{\vol(\Delta^m + \cB_2^m(\eps))}{\vol(\cB_2^m(\eps))}.
%%\end{equation}
%%
%%For $a >0$, let 
%%$\Delta^m(a)$
%%%\begin{equation}
%%%\Delta^m(a) := \Delta^m + \cB_1^m(a)
%%%\end{equation}
%%be a scaling of the standard simplex where the edges are scaled by $a$; more explicitly, it is given by
%%\begin{equation}
%%   \mathrm{convhull}\left(\{a \, \be_i : i \in[m]\}\right) \equiv \{\bx \in\R^{m+1}: \bx\succ\bzero, \bx^\sT \one = a\}.
%%\end{equation}
%%where $\be_i \in\R^{m+1}$ are the canonical basis elements. (Note that $\Delta^m(1) = \Delta^m).$
%%One can show~\bns{reference or show calculation} that 
%%\begin{equation}
%%\label{eq:vol_simplex_scaled}
%%    \vol(\Delta^m(a)) = \frac{a^{m} \sqrt{m+1}}{m!}.
%%\end{equation}
%%
%%To compute the numerator in~\eqref{eq:packing_bound}, we note that
%%\begin{equation}
%%    |\Delta^m + \cB^m_2(\eps) |\le  |\Delta^m(\eps \sqrt{m})|.
%%\end{equation}
%%Indeed, for any $\bv$ in the set on left, we have $\bv = \tilde \bv + \bu$
%%for $\bv \in \Delta^m, \bu \in\cB^m_2(\eps)$ and hence
%%\begin{equation}
%%    \bv^\sT \one = \norm{\bv}_1 \le \norm{ \tilde \bv}_1 + \sqrt{m} \norm{\bu}_2  \le 1 +  \sqrt{m} \eps.
%%\end{equation}
%%and use Eq.~\eqref{eq:vol_simplex_scaled} to compute the desired quantity.
%%To see Eq.~\eqref{eq:vol_simplex_scaled}, consider 
%%\end{proof}
%%
%
%
%\subsection{Quantitative Varadhan Lemma- Sanov Approach}
%
%
%
%The goal of this section is to drive a non-asymptotic variant of Varadhan's lemma over the space of the probability measures. Namely, let $\Sigma\subseteq\R^k$ be a Polish space and let $X_1,\dots,X_n$ be a sequanece of $\Sigma$-valued random variables, identically distributed according the the measure $\mu\in \cM_1(\Sigma)$. The empirical distribution of $\bX:=(X_1,\dots,X_n)$ is given by
%\begin{equation}
%    \widehat\nu_\bX:= \frac1n\sum_{i=1}^n \delta_{X_i}.
%\end{equation}
%To set up the framework, let us equip $\cM_1(\Sigma)$ with the Lipschitz Bounded distance $d_{BL}$. Note that the topology induced on $\cM_1(\Sigma)$ corresponds to the weak convergence of probability measures. 
%
%Throughout this section, let $\Delta^m$ be the standard $m$-simplex given by
%\begin{equation}
%    \Delta^m:=\{(x_0,\dots,x_m): \sum_{i=0}^m x_i=1,x_i\geq 0\}.
%\end{equation}
%Further, let $C_b(\Sigma)$ denote the collection of bounded continuous functions $\phi:\Sigma\rightarrow \R$, equipped with the supremum norm.
%
%As before, let $N^d(A,\eps)$ be the $\eps$-covering number of the set $A$ equipped with the distance function $d$.
%\begin{lemma}[Covering number of the simplex]
%    Let $\eps>0$. Then,
%    \begin{equation}
%        \log N^{\norm{.}_2}(\Delta^m,\eps)\leq \frac1\eps\log m.
%    \end{equation}
%\end{lemma}
%\begin{proof}
%    
%\end{proof}
%
%
%
%Next lemma drives a non-asymptotic rate function for the random variable $\widehat\nu_\bX$.
%\begin{lemma} [Non-asymptotic Sanov on convex sets]
%    For any $n>0$ and any compact convex set $B\subseteq\cM_1(\Sigma)$, we have the following statement:
%    \begin{equation}
%        \frac1n\log\P_\mu\left[ \widehat \nu_\bX\in B\right] \leq -\inf_{\nu\in B} \KL(\nu\|\mu).
%    \end{equation}
%\end{lemma}
%
%\begin{proof} 
%Observe that for any $\phi\in C_b(\Sigma)$, 
%\begin{align}
%    \P_\mu\left[ \widehat \nu_\bX\in B\right]\leq & 
%    \E\left[ \exp\left\{ n\langle\phi,\widehat\nu_\bX \rangle
%    -n\inf_{\nu\in B} \langle \phi,\nu\rangle \right\} 
%    \one_{B}(\widehat\nu_\bX)\right]\\
%    \leq& \exp\left\{ -n\inf_{\nu\in B}\langle
%    \phi,\nu\rangle \right\}
%    \E\left[\exp\left\{
%    n\langle \phi, \frac1n\sum_{i=1}^n \delta_{X_i} \rangle\right\}\right]\\
%    =& \exp\left\{ -n\inf_{\nu\in B} \left[ \langle \phi,\nu\rangle - \log\int_\Sigma e^\phi\de\mu \right] \right\}\,
%\end{align}
%where the last equality is the result of the independence of the variables $\delta_{X_i}$.
%
%The next step is to optimize the parameter $\phi$. Note that $C_b(\Sigma)$ is a convex vector space, and
%$\log\int_\Sigma e^\phi\de\mu $ is a convex function of $\phi$, and hence
%\begin{equation}
%    \langle \phi,\nu\rangle - \log\int_\Sigma e^\phi\de\mu
%\end{equation}
%is concave and upper semi-continuous in the argument $\phi\in C_b(\Sigma)$ and linear and lower semi-continuous in $\nu\in B$. \kas{ I think it's continuous in both?}The conclusion follows by using the Sion's min-max theorem:
%\begin{align}
%    \frac1n\log\P_\mu\left[ \widehat\nu_\bX\in B\right]&
%    \leq -\sup_{\phi\in C_b(\Sigma) }\inf_{\nu\in B} \left[\langle \phi,\nu\rangle - \log\int_\Sigma e^\phi\de\mu\right]\\
%    =& - \inf_{\nu\in B} \sup_{\phi\in C_b(\Sigma) }\left[ \langle \phi,\nu\rangle - \log\int_\Sigma e^\phi\de\mu\right]\\
%    =& -\inf_{\nu\in B} \KL(\nu\|\mu),
%\end{align}
%    where the last inequality is the result of \ref{dembo: 6.2.13}[dembo: 6.2.13].
%
%
%
%\end{proof}
%
%
%
%
%
%
%
%
%\begin{lemma}[Quantitative Varadhan's Lemma]
%    Let $k\equiv k(n)>0$, $\Sigma\subseteq \R^k$ be a compact polish space with diameter $R$, and $F:\cM_1(\Sigma)\rightarrow \R$ be a Borel-measurable function. Define
%    \begin{equation}
%        \Phi(n,\mu):= \frac1n\log\E_\mu\exp\left\{
%        nF(\widehat\nu_\bX)\right\},
%    \end{equation}
%
%    and assume that the following conditions hold: \kas{todo: the tail condition for the non-compact case}
%    \begin{enumerate}
%        \item $F$ is Lipschitz with the Lipschitz constant $L$.
%    \end{enumerate}
%\end{lemma}
%Then,
%\begin{equation}
%    \Phi(n,\mu)\leq \sup_{\nu\in\cM_1(\Sigma)}\left\{
%    F(\nu) - \KL(\nu\|\mu)\right\} + ?.
%\end{equation}
%
%
%\begin{proof}
%
%
%    Let $m>0$ and $\eps>0$. We will chose exact values of these parameters later. By lemma \ref{} for some constant $C$,
%    \begin{equation}
%        N^{\norm{.}_2}\left(\Sigma, R\left(\frac 1{Cm}\right)^{1/k}
%        \right)\leq m.
%    \end{equation}
%    Let us define, for any $\nu\in\cM_1(\Sigma)$, its discretized version $\nu^{(m)}\in\Delta^m$ via
%    \begin{equation}
%        \nu^{(m)}_i\equiv \nu^{(m)}(a_i) := \nu(A_i),\text{ for } i=0,\dots,m
%    \end{equation}
%    where $A_1,\dots,A_m$ are Borel-measurable sets that form an $R\left(1/{Cm}\right)^{1/k}$-partition of $\Sigma$, and $a_i\in A_i$ are arbitrary fixed points.
%    \kas{Needs details about the partitioning: measurable, radius, number of them}.
%        
%    
%    We start the proof by covering the simplex $\Delta^m$; namely,
%
%    \kas{Andrea, you can read the following 6 equations plus the definitions we used in lemma 44. Note that in this section we used $\cM_1$ to represent the probabilty space, and it is not equal to the manifold $\cM$.}
%    \kas{The following four equations are for the case where we are on the manifold $\cM=\{\bV\in \Sigma^{ n}: \E_{\widehat\nu_\bV}[G]=0\}$}
%    \begin{align}
%    &\frac1n\int_{\bV\in\cM} \exp\{n F(\widehat\nu_\bV)\} p_1(\bV) \de V \\  
%    &\leq \frac1n\log(N(\Delta^m,\eps)) + 
%    \sup_{\nu\in \Delta^m} \frac1n\log 
%    \int_{\bV\in\cM} \exp\{n F(\widehat\nu_\bV)\} p_1(\bV) \one_{\{\|\widehat\nu_\bV^{(m)} - \nu\|\leq \eps\}}\de_\cM V \\
%    &  \leq \frac1n\log(N(\Delta^m,\eps)) + 
%    \sup_{\nu\in \Delta^m} \left\{
%    \sup_{\bar\nu\in \cP(\Sigma),\|\bar\nu^{(m)}-\nu\|\leq \eps, \E_{\bar\nu}(\bG)=0} F(\bar \nu) + 
%    \frac1n\log 
%    \int_{\bV\in\cM} p_1(\bV) \one_{\{\|\widehat\nu^{(m)}_\bV - \nu\|\leq \eps\}}\de_\cM V\right\}\\
%     &  \leq \frac1n\log(N(\Delta^m,\eps)) + 
%    \sup_{\nu\in \Delta^m} \left\{
%    \sup_{\bar\nu\in \cP(\Sigma),\|\bar\nu^{(m)}-\nu\|\leq \eps, \E_{\bar\nu}(\bG)=0} F(\bar \nu) + 
%    \frac1n\log 
%    \int_{\bV} p_1(\bV) \one_{\{\|\widehat\nu^{(m)}_\bV - \nu\|\leq \eps\}} \one_{\{ \E_{\widehat\nu_\bV}[\bG]\leq \eps_2 
%    \}} \de \bV +\omega_{\cM}(\eps_2) \right\}\\
%     &  \leq \frac1n\log(N(\Delta^m,\eps)) + 
%    \sup_{\nu\in \Delta^m} \left\{
%    \sup_{\bar\nu\in \cP(\Sigma),\|\bar\nu^{(m)}-\nu\|\leq \eps, \E_{\bar\nu}(\bG)=0} F(\bar \nu) + 
%    \frac1n\log 
%    \P\left[\{\norm{\hat\nu^{(m)}_\bV - \nu}\leq \eps\} \cap \{\E_{\widehat\nu_\bV} [\bG]\leq \eps_2\}\right]
%    +\omega_{\cM}(\eps_2) \right\}\\
%     &\leq \frac1n\log(N(\Delta^m,\eps)) + 
%    \sup_{\nu\in \Delta^m} \left\{
%    \sup_{\bar\nu\in \cP(\Sigma),\|\bar\nu^{(m)}-\nu\|\leq \eps, \E_{\bar\nu}(\bG)=0} F(\bar \nu)  
%    -\inf_{\bar\nu:\norm{\bar\nu^{(m)} - \nu}\leq \eps  ,\E_{\bar\nu} [\bG]\leq \eps_2} \KL(\bar\nu\| \cN(\bzero, \bR(\bTheta)))   
%    +\omega_{\cM}(\eps_2) \right\}
%    \end{align}
%
%\kas{The rest is the equivalence of the last equations, but over the ambient space, ignoring the manifold.}
%
%
%
%    
%    \begin{align}
%        \Phi(n,\mu) =& \frac1n\log\E\left[\exp\left\{
%        nF(\widehat\nu_\bX)\right \}\right]\\
%        \leq &
%        \frac1n\log\left( N^{\norm{.}_2}(\Delta^m,\eps)\right)
%        + \sup_{\nu\in\Delta^m} \frac1n\log\E_\mu\left[
%        \exp\{nF(\widehat\nu_\bX)\}
%        \one(\norm{{\widehat\nu_\bX}^{(m)}-\nu}_2\leq \eps)
%        \right]\\
%        \leq& \frac1n\log\left( N^{\norm{.}_2}(\Delta^m,\eps)\right) + 
%        \sup_{\nu\in\Delta^m} \left\{ \sup_{\substack{\bar \nu\in \cM_1(\Sigma):\\
%        \norm{\bar\nu^{(m)}-\nu}_2\leq \eps}} F(\bar \nu) 
%        +\frac1n\log\P\left[
%        \norm{\widehat\nu_\bX^{(m)}-\nu}_2\leq \eps\right]
%        \right\}.
%    \end{align}
%For the ease of notation let us define, for any $\nu\in \Delta^m$,
%\begin{equation}
%    \Omega(\nu,\eps;m):= \left\{\bar\nu\in\cM_1(\Sigma): \norm{\bar\nu^{(m)}-\nu}_2\leq \eps \right\}.
%\end{equation}
%Note that $\Omega(\nu,\eps;m)$ is a convex compact subset of $\cM_1(\Sigma)$. To see this, consider any two probability measures $\nu_1,\nu_2\in\Omega(\nu,\eps;m)$ and $i\in[m]$:
%\begin{align}
%    (\alpha \nu_1^{(m)} + (1-\alpha)\nu_2^{(m)})(a_i) = &
%    \alpha \nu_1(A_i) + (1-\alpha)\nu_2(A_i)\\
%    =& \alpha \nu_1^{(m)}(a_i) + (1-\alpha)\nu_2^{(m)}(a_i),
%\end{align}
%and hence,
%\begin{align}
%    \norm{(\alpha \nu_1^{(m)} + (1-\alpha)\nu_2^{(m)})-\nu}_2 \leq& \alpha\norm{\nu_1^{(m)}-\nu}_2+ (1-\alpha)\norm{\nu_2^{(m)}-\nu}_2\leq \eps,
%\end{align}
%which proves the convexity of $\Omega(\nu,\eps;m)$. \kas{Is the compactness obvious?} 
%
%Further, note that
%\begin{align}
%    d_{BL}(\nu_1^{(m)},\nu_1) \leq& 
%    \sup_{f\in \cF_{LU}}\left\{ \sum_{i=0}^m 
%    \sup_{a\in A_i}|f(a)-f(a_i)|\nu_1(A_i)
%    \right\}\\
%    \leq & R\left(\frac1{Cm}\right)^{1/k},
%\end{align}
%and
%\begin{align}
%    d_{BL}(\nu_1^{(m)},\nu)\leq&
%    \sup_{f\in \cF_{LU}}\left\{ \sum_{i=0}^m 
%    f(a_i)(\nu^{(m)}(a_i) - \nu(a_i))\right\}\\
%    \leq& \left(\sum_{i=1}^m f(a_i)^2\right)^{1/2} \norm{\nu_1^{m} - \nu}_2\\
%    \leq& \sqrt{m}\eps. 
%\end{align}
%Hence overall we get
%\begin{align}
%    d_{BL}(\nu_1,\nu_2)\leq& 
%    d_{BL}(\nu_1,\nu^{(m)}_1)+ d_{BL}(\nu^{(m)}_1,\nu)
%    + d_{BL}(\nu_2,\nu^{(m)}_2)+ d_{BL}(\nu^{(m)}_2,\nu)\\
%    \leq & C_1\left(\sqrt{m}\eps + R\left(\frac1{Cm}\right)^{1/k}\right)
%\end{align}
%for some constant $C_1$.
%
%This allows us to apply lemma \ref{}[Sanov] alongside the lipcshitzness assumption of the function $F$ to get
%\begin{align}
%        \Phi(n,\mu)
%        \leq& \frac1n\log\left( N^{\norm{.}_2}(\Delta^m,\eps)\right)+ 
%        \sup_{\nu\in\Delta^m} \left\{ \sup_{\bar\nu\in \Omega(\nu,\eps;m)} F(\bar \nu) 
%        - \inf_{\bar\nu\in\Omega(\nu,\eps;m)}\KL(\bar\nu\|\mu)
%        \right\}\\
%        \leq&  
%        \frac1n\log\left( N^{\norm{.}_2}(\Delta^m,\eps)\right) + \sup_{\nu\in\Delta^m}
%        \left\{ \sup_{\bar\nu\in \Omega(\nu,\eps;m)} F(\bar\nu) - \KL(\bar\nu\|\mu) + 
%        LC_1\left(\sqrt{m}\eps + R\left(\frac1{Cm}\right)^{1/k}\right) \right\}\\
%        \leq& \frac1n\log\left( N^{\norm{.}_2}(\Delta^m,\eps)\right) +
%        LC_1\left(\sqrt{m}\eps + R\left(\frac1{Cm}\right)^{1/k}\right)+
%        \sup_{\nu\in\cM_1(\Sigma)}\left\{
%        F(\nu)-\KL(\nu\|\mu) 
%        \right\}.
%\end{align}
%Next, let 
%\begin{equation}
%    \eps = , \quad m = .
%\end{equation}
%Consequently, we can use lemma \ref{}[simplex] to get
%\begin{equation}
%    \Phi(n,\mu) \leq \sup_{\nu\in\cM_1(\Sigma)}
%    \left\{ F(\nu) + \KL(\nu\|\mu)
%    \right\} + 
%\end{equation}
%\end{proof}
%
%
%\subsection{Indicator function}
%\kas{tackeling $x_{\min}(\mu^*)$}
%In this section, we 
%
%
%
%
%\newpage
%\subsection{Quantitative Varadhan Lemma- Cramer approach}\kas{Specialized to our choice of function $F$. TODO: the minmax needs lower semi cont condition to be checked. instead of $\R^{k_1}$, use generic $\bSigma$.}
%Throughout this section, $\cM_1(\Sigma)$ denotes the space of the (Borel) probability measures on the (Polish) space $\Sigma$. The following theorem presents a variant of Varadhan's lemma applicable in non-asymptotic settings, providing explicit quantitative error bounds.
%\begin{theorem}
%     Let $k_1\equiv k_1(n)$, $k_2\equiv k_2(n)$, and let $G:\R^{k_1}\rightarrow \R^{k_2}$ and $F:\R^{k_2}\rightarrow \R$ be two continuous and Borel-measurable fucntions. \kas{only Borel or cont. which one?}Define
%    \begin{equation}
%        \Phi(n,\mu) := \frac1n\log \E_\mu \exp\left\{ n F(\widehat S_n)\right\},
%    \end{equation}
%    where $\mu\in\cM_1(\R^{k_1})$, $(X_1,\dots,X_n)\sim \mu^{\otimes n}$ and $\widehat S_n = \frac1n\sum_{i=1}^n G(X_i)$. 
%      
%
%
%    
%    Assume that the following conditions hold:
%    \begin{enumerate}
%        %\item  $\Psi_*:=\sup_{\nu\in\cM_1(\R^{k_1})} \Psi(\nu;\mu)$.
%        
%        %\item   $\cH_*:=\sup_{\bs\in\R^{k_2}}\cH(\bs)$ is achieved at a point $\bs_*\in\R^{k_2}$.\kas{Can we get rid of the unique assumption?}
%        %and there exists a neighborhood of $\bs_*$ such as $\cB(\bs_*, r_*)$ where $\bz$ is the unique maximizer of the function $\cH$ at $\cB(\bs_*,r_*)$.%
%        
%        % \item The Hessian of the function $F$ has is Lipschitz in a neighborhood of $\bs_*$, i.e. there exists $L_n$ such that for any $\bs_1,\bs_2\in\cB(\bs_*,r_*)$:
%        % \begin{equation}
%        %      \| \nabla^2 F(\bs_1) - \nabla^2 F(\bs_2)\| \leq L_n 
%        %     \|\bs_1-\bs_2\|_2.
%        % \end{equation}
%        
%        \item \kas{Generalizing the Lipschitz assumption:} $F$ is Pseudo-Lipschitz; i.e. there exists constants $L\geq0$ and $p\geq 0$ such that for any $\bz_1,\bz_2\in\R^{k_2}$:
%        \begin{equation}
%            |F(\bz_1)-F(\bz_2)|\leq L(1+\|\bz_1\|^p+\|\bz_2\|^p)\norm{\bz_1-\bz_2}.
%        \end{equation}
%        
%        \item For some constant $\alpha>0$, there exists $L_0,L_1$ and $K_0,K_1$ such that for all $\bz\in\R^{k_2}$:
%        \begin{align}
%            &F(\bz)\leq L_0+L_1 \|\bz\|^\alpha,\\
%            &\P_\mu\left[\norm{\widehat S_n}\geq t\right]\leq \exp\left\{K_0 n-K_1 n t^\alpha\right\},
%        \end{align}
%        and $K_1>L_1$.
%    \end{enumerate}
%
%
%    
%    Then,\kas{add explicit terms}
%    \begin{equation}
%         \Phi(n,\mu)\leq \sup_{\nu\in\cM_1(\Sigma)} 
%         \left\{F\left(\int_{\Sigma} G(\bx)\de\nu(\bx)\right) - \KL(\nu\|\mu) \right\}
%        + O\left(\frac{k_2}{n}\log\left(\frac{n}{k_2}\right)\right).  
%    \end{equation}   
%\kas{This is not the correct rate when the assumptions' parameters are dependent on $k_2$; For the correct rate in this case we should use the last equation in this section for the explicit dependency.}
%\kas{Postponed to the time when we have an exact formula for F and G to avoid cluttering.}
%\end{theorem}
%
%
%For the ease of notation from now on, Let $\Lambda$ be the log-Laplace transform of the random variable  $G(X)$ where $X \sim\mu$, and let $\Lambda^\star$ be the corresponding Legendre-Frenchel transform, i.e.
%     \begin{align}
%         &\Lambda(\blambda) = \log \E_\mu \exp\{\langle\blambda,G(X) \rangle\},\\
%         & \Lambda^\star(\bz) = \sup_{\blambda\in\R^{k_2}}
%         \{\langle \blambda, \bz\rangle - \Lambda(\blambda)\}.
%     \end{align}
%   
%    Further, define
%    \begin{align}
%        &\Psi(\nu;\mu):=F\left(\int_{\R^{k_1}} G(\bx)\de\nu(\bx)\right) -
%        \KL(\nu\|\mu),\\
%        &\cH(\bz):= F(\bz)-\Lambda^*(\bz).
%    \end{align}
%Before proceeding to the proof, let us break down the function $\Phi$ into two parts:
%    \begin{align}
%        \Phi_1(n;\mu)&:=\frac1n \log\E_\mu \exp\{nF(\widehat S_n)\}\one_{B(\bzero,R)}(\widehat S_n),\\
%        \Phi_2(n;\mu)&:=\frac1n \log\E_\mu \exp\{nF(\widehat S_n)\}\one_{B(\bzero,R)^c}(\widehat S_n) .\\      
%    \end{align}
%where $R$ is a positive parameter and its exact value will be chosen later.
%
%From now on, let $N_d(B,\epsilon)$ be the minimum number of $\epsilon$-balls covering the set $B\subseteq \R^d$.
%\begin{lemma}\kas{add refrence}
%    Let $\epsilon<1$, $\bx\in\R^d$ and $r>0$. Then,
%    \begin{equation}
%       \log N_d(B(\bx,r),\epsilon)\leq C d\log\left(\frac r{\epsilon}\right)
%    \end{equation}
%    for some absolute constant $C$.
%\end{lemma}
%
%
%The first step of the proof is to find a non-asymptotic rate function for the random variable $\widehat S_n$.
%\begin{lemma}[Cramer's Upper Bound on open sets] For any $n>0$ and any open measurable set $B\in \R^{k_2}$, we have the following statement:
%\begin{equation}
%    \frac1n\log \P_\mu\left[ \widehat S_n\in B\right] \leq
%    -\inf_{\bz\in B} \Lambda^\star(\bz).
%\end{equation}   
%\end{lemma}
%\begin{proof}
%    Observe that for any $\blambda\in\R^{k_2}$,
%    \begin{align}
%        \P_{\mu}\left[\widehat S_n\in B \right]  \leq&
%        \E\left[\exp\left\{ n\langle \blambda, \widehat S_n\rangle  -n\inf_{\bz\in B} 
%        \langle \blambda, \bz\rangle \right\} \
%        \one_{B}(\widehat S_n)\right]\\
%        \leq & \exp\left\{ -n\inf_{\bz\in B} 
%        \langle \blambda, \bz \rangle\right\}
%        \E \exp\left\{ n\langle \blambda,\widehat S_n \rangle\right\}\\
%        =& \exp\left\{-n\inf_{\bz\in B}[\langle  \blambda, \bs
%        \rangle - \Lambda(\blambda)]\right\}.
%    \end{align}
%    Note that the last equality is the result of the independence of the variables $G(X_i)$.
%    
%    The next step is to optimize the parameter $\blambda$. Note that the Laplace transform $\Lambda(.)$ is a convex function \kas{Dembo 2.2.31}, and hence 
%    $$\langle \blambda, \bz\rangle - \Lambda(\blambda)$$
%    is concave in the argument $\blambda$ and linear in $\bs$. The conclusion follows by using the min-max theorem: \kas{I think we need some finiteness over $\Lambda$ here?}
%    \begin{align}
%        \frac1n\log\P_\mu\left[\widehat S_n\in B\right]\leq&
%        -\sup_{\blambda\in\R^{k_2}}\inf_{ \bz\in\cB }[\langle  \blambda, \bz
%        \rangle - \Lambda(\blambda)]\\
%        =&-\inf_{ \bz\in B}\sup_{\blambda\in\R^{k_2}}[\langle  \blambda, \bz
%        \rangle - \Lambda(\blambda)]\\
%        =&- \inf_{\bz\in B} \Lambda^\star( \bz).
%    \end{align}  
%\end{proof}
%
%
%
%\begin{lemma}
%    Assume the condition \ref{}1 of Theorem \ref{}3 holds and let $\epsilon>0$. Then,
%    \begin{equation}
%        \Phi_1(n;\mu)\leq
%        \frac{CK_2}{n}\log\left(\frac{R}{\epsilon}\right) + \cH_* +L\epsilon(1+2(R+\epsilon)^p).
%    \end{equation}
%\end{lemma}
%\begin{proof}
%We start the proof by covering the set $B(\bzero, R)$ with $\epsilon$-balls. Hence,
%    \begin{align}
%        \Phi_1(n;\mu)=&\frac1n \log\E_\mu \exp\{nF(\widehat S_n)\}\one_{B(\bzero,R)}(\widehat S_n)\\
%        \leq &\frac1n
%        \log\left(N_{k_2} \left(B(\bzero,R),\epsilon\right)\right) + 
%        \sup_{\bz\in B(\bzero,R)} \frac1n\log\E\left[ 
%        \exp\{nF(\widehat S_n)\}
%        \one_{B(\bz,\epsilon)}(\widehat S_n)\right] \\
%        \leq & \frac{Ck_2}{n}\log\left(\frac{R}{\epsilon}\right) + 
%        \sup_{\bz\in B(\bzero,R)}
%        \left\{\sup_{\bar\bz\in B(\bz,\epsilon)}F(\bar\bz)+
%        \frac1n \log\P_\mu\left[\widehat S_n\in B(\bz,\epsilon)\right]\right\}\\        
%        \leq & \frac{Ck_2}{n}\log\left(\frac{R}{\epsilon}\right) + 
%        \sup_{\bz\in B(\bzero,R)}
%        \left\{\sup_{\bar\bz\in B(\bz,\epsilon)}F(\bar\bz)
%        -\inf_{\bar\bz \in B(\bz,\epsilon)}\Lambda^\star(\bar\bz)\right\},
%    \end{align}
%
%where the last inequality is the result of the lemma \ref{}36.
%
%\kas{Is the following assumption required for what follows(existence of a converging sequence)?
%$$B(\bzero,R+\epsilon)\subseteq D_{\Lambda^\star}:=\{\bz: \Lambda^\star(\bz)<\infty\}.$$}
%
%Now note that for any $\delta>0$, there exists $\bz_\delta\in B(\bz,\epsilon)$ and $\bar\bz_\delta\in B(\bz,\epsilon)$ such that
%\begin{equation}
%    \Lambda^\star(\bar\bz_\delta)\leq \inf_{\bar\bz\in B(\bz,\epsilon)}\Lambda^\star(\bar\bz) + \delta,\qquad 
%    F(\bz_\delta) \geq  \sup_{\bar\bz\in B(\bz,\epsilon)} F(\bar\bz) -\delta.
%\end{equation}
%
%    
%Further, assumption 1 of Thorem 1 implies that
%\begin{align}
%    F(\bz_\delta)\leq &F(\bar\bz_\delta) + L(1+\|\bz_\delta\|^p+\|\bar\bz_\delta\|^p)\|\bar\bz_\delta-\bz_\delta\|\\
%    \leq & F(\bar\bz_\delta)+ L\epsilon(1+2(R+\epsilon)^p).
%\end{align}
%
%
%By incorporating these facts,
%\begin{align}
%    \Phi_1(n;\mu)\leq& \frac{CK_2}{n}\log\left(\frac{R}{\epsilon}\right) + 
%    \sup_{\bz\in B(\bzero,R)} \left\{
%    \lim_{\delta\rightarrow 0}
%     F(\bz_\delta) -\Lambda^\star(\bar \bz_\delta) + 2\delta
%    \right\}\\
%    \leq& \frac{CK_2}{n}\log\left(\frac{R}{\epsilon}\right) + 
%    \sup_{\bz\in B(\bzero,R)} \left\{
%    \sup_{\bar\bz\in B(\bz,\epsilon)}
%     F(\bar\bz) -\Lambda^\star(\bar \bz) + L\epsilon(1+2(R+\epsilon)^p)
%    \right\}\\
%     \leq& \frac{CK_2}{n}\log\left(\frac{R}{\epsilon}\right)+
%     L\epsilon(1+2(R+\epsilon)^p) + 
%    \sup_{\bz\in B(\bzero,R+\epsilon)} \left\{
%     F(\bz) -\Lambda^\star( \bz) 
%    \right\}\\
%    \leq &\frac{CK_2}{n}\log\left(\frac{R}{\epsilon}\right)  +L\epsilon(1+2(R+\epsilon)^p)+ \cH_*,
%\end{align}
%which completes the proof.
%\end{proof}
%
%\begin{lemma}
%    Assume condition 2\ref{} of Theorem 3 \ref{} holds. Then,
%    \begin{equation}
%    \Phi_2(n,\mu)  \le L_0  + K_0  - (K_1 - L_1)R^\alpha + \frac1n \log\left(\frac{K_1}{K_1 - L_1}\right).
%    \end{equation}
%\end{lemma}
%\begin{proof}
%
%In what follows, define
%\begin{equation}
%    Y_n := \norm{\widehat S_n }.
%\end{equation}
%Using assumption 2,
%    \begin{align}
%        \Phi_2(n,\mu) =&  \frac1n \log\E_\mu \left[ \exp
%        \{nF(\widehat S_n)\}\one_{B(\bzero,R)^c}(\widehat S_n)\right] \\       
%        \leq&\frac1n \log\E_\mu\left[
%        \exp\left\{n(L_0+L_1\norm{\widehat S_n}^\alpha )\right\}
%        \one_{B(\bzero,R)^c}(\widehat S_n)\right]\\
%        =&L_0 + \frac1n \log\E_\mu\left[
%        \exp\left\{nL_1 Y_n^\alpha \right\}
%        \one_{Y_n \geq R}\right]\\
%        =& L_0 + \frac1n \log 
%        \int_{0}^{\infty}
%        \P\mu\left[\exp\left\{nL_1 Y_n^\alpha \right\}
%        \one_{Y_n > R} \ge t\right]\de t.
%\end{align}
%
%Now denoting $\varphi(R) := \exp\{n L_1 R^\alpha\},$ we have
%\begin{equation}
%    \P\mu\left[
%    \exp\left\{nL_1 Y_n^\alpha \right\} \one_{Y_n \geq R} \ge t
%    \right]  = 
%    \begin{cases}
%    \P_\mu\left[
%    Y_n > R
%    \right]  & t< \varphi(R)\\ 
%    \P_\mu\left[
%    Y_n >  \varphi^{-1}(t)
%    \right]  & t\ge   \varphi(R)
%    \end{cases},
%\end{equation}
%so that, applying the tail-bound of Assumption 2 \ref{}, and using that $K_1 > L_1$
%we have
%\begin{align}
%\int_{0}^{\infty}
%        \P\left(\exp\left\{nL_1 Y_n^\alpha \right\}
%        \one_{Y_n > R} \ge t\right)\de t
%        &= 
%        \varphi(R) \P(Y_n > R) + \int_{\varphi(R)}^\infty \P(Y_n \ge \varphi^{-1}(t)) \de t\\
%        &\le \varphi(R) e^{K_0 n - K_1 n R^\alpha}
%        + e^{K_0 n} \int_{\varphi(R)}^\infty \left(\frac{1}{t}\right)^{K_1/L_1} \de t\\
%        &= e^{n( K_0 -(K_1- L_1) r^\alpha)}  + 
%        \frac{L_1}{K_1- L_1}e^{K_0 n} e^{ - n r^\alpha(K_1- L_1)}.
%\end{align}
%Combining with the equation ~\notate{ref} gives the bound 
%\begin{equation}
%    \Phi_3(n,\mu)  \le L_0  + K_0  - (K_1 - L_1)R^\alpha + \frac1n \log\left(\frac{K_1}{K_1 - L_1}\right).
%\end{equation}
%
%\end{proof}
%
%
%
%\begin{lemma}
%    Consider the setting of the theorem 3. Then,
%    \begin{equation}
%        \cH_*=\Psi_*.
%    \end{equation}
%\end{lemma}
%\begin{remark}
%    This equality can be justified for any fixed dimensions \( k_2 \) and \( k_1 \) by applying Varadhan's lemma twice: once using Cramr's theorem for the random variable \(\widehat{S}_n\), and once using Sanov's theorem for the empirical measure \(\frac{1}{n} \sum_{i=1}^n \delta_{X_i}\). For completeness, however, we provide an independent proof of this statement.
%\end{remark}
%
%
%
%\begin{proof}\kas{Is there any assumptions needed on $D_\Lambda$?}
%Note that following the definition of $\Psi$,
%\begin{align}
%    \sup_{\nu\in\cM_1(\R^{k_1})}\Psi(\nu;\mu) = & 
%    \sup_{\nu\in\cM_1(\R^{k_1})} F\left(\int_{\R^{k_1}} G(\bx)\de \nu(\bx)\right) -\KL(\nu\|\mu)\\
%    =&\sup_{\bz\in\R^{k_2}}\sup_{\substack{\nu \in \cM_1(\R^{k_1}),\\ \int_{\R^{k_1} }G(\bx)\de\nu(\bx)=\bz}} F(\bz) - \KL(\nu\|\mu)\\
%    =&\sup_{\bz\in\R^{k_2}}\sup_{\nu\in\cM_1(\R^{k_1})} \inf_{\blambda\in\R^{k_2}}
%    F(\bz) +\langle\lambda,\int_{\R^{k_1}} G(\bx)\de\nu(\bx) -\bz\rangle
%    -\KL(\nu\|\mu),
%\end{align}
%where we used the Lagrange multiplier theorem in the last equality. 
%
%Next, note that $\cM_1(\R^{k_1})$ is a convex set and $\KL(.\|\mu)$ is a convex function in the space of all probability measures $\cM_1(\R^{k_1})$.  Hence, Von Neumann minimax Theorem is applicable in our setting:
%
%\begin{align}
%    \sup_{\nu\in\cM_1(\R^{k_1})}\Psi(\nu;\mu) = &\sup_{\bz\in\R^{k_2}} \inf_{\blambda\in\R^{k_2}}  \sup_{\nu\in\cM_1{\R^{k_1}}}  
%    F(\bz) +\langle\lambda,\int_{\R^{k_1}} G(\bx)\de\nu(\bx) -\bz\rangle
%    -\KL(\nu\|\mu)\\
%    =&\sup_{\bz\in\R^{k_2}} \inf{\blambda\in\R^{k_2}} F(\bz)-\langle \blambda,\bz\rangle 
%    +\sup_{\nu\in\cM_1(\R^{k_1})} \int_{\R^{k_1}} \left(
%    \langle \blambda,G(\bx)\rangle
%    -\frac{\de\nu(\bx)}{\de\mu(x)}\right)\de\nu(\bx).
%\end{align}
%By optimizing over $\nu$, we get the desired result:
%\begin{align}
%    \Psi_* = & \sup_{\bz\in\R^{k_2}} \inf_{\blambda\in\R^{k_2}} F(\bz)-\langle \blambda,\bz\rangle   + \bLambda(\blambda)=\cH_*. 
%\end{align}
%
%
%    
%\end{proof}
%
%
%
%
%Now, we can move on to the proof of Theorem 3.
%\begin{proof}[Proof of Theorem 3]
%Let
%\begin{equation}
%    R=\left(\frac{L_0+K_0 - \cH_*}{K_1-L_1}\right)^{1/\alpha}.
%\end{equation}
%Hence, lemma \ref{} implies that
%\begin{align}
%    \Phi_2(n;\mu)& \leq L_0  + K_0  - (K_1 - L_1)R^\alpha + \frac1n \log\left(\frac{K_1}{K_1 - L_1}\right)\\
%    &=   \cH_* +\frac1n\log\left(\frac{K_1}{K_1-L_1}\right).
%\end{align}
%Further, let 
%\begin{equation}
%    \epsilon =\min\{ \frac{k_2}{n},R\}.
%\end{equation}
%Using the result of lemma \ref{ },
%
%\begin{align}
%    \Phi_1 \leq& \cH_* +
%    \frac{Ck_2}{n}\log\left(\frac{R}{\epsilon}\right) 
%    +L\epsilon(1+2(R+\epsilon)^p)\\
%    =& \cH_* + \frac{k_2}{n}\left[C\log\left(\frac{n}{k_2}\right) + \frac{C}{\alpha}
%    \log\left(\frac{L_0+K_0 - \cH_*}{K_1-L_1}\right) +
%    L+2L\left(\left(\frac{L_0+K_0 - \cH_* }{K_1-L_1}\right)^{1/\alpha}+\frac{k_2}{n}\right)^p\right]
%\end{align}
%Hence, incorporating the result of Lemma \ref{} gives us the final result:
%
%\begin{align}
%    \Phi(n;\mu) \leq&
%    \max\{\Phi_1(n;\mu),\Phi_2(n;\mu)\} + \frac{\log 3}{n}\\
%     \leq & \Psi_* + \max\bigg\{
%     \frac{k_2}{n}\left[C\log\left(\frac{n}{k_2}\right) + \frac{C}{\alpha}
%    \log\left(\frac{L_0+K_0 - \cH_*}{K_1-L_1}\right) +
%    L+2L\left(\left(\frac{L_0+K_0 - \cH_* }{K_1-L_1}\right)^{1/\alpha}+\frac{k_2}{n}\right)^p\right]\\
%    &\qquad\qquad ,\frac1n  \log\frac{K_1}{K_1-L_1}
%    \bigg\}\\
%    \leq& \Psi_* + O\left(\frac{k_2}{n}\log\left(\frac{n}{k_2}\right)\right).
%\end{align}
%\end{proof}
%
%
%
%
%
%
%
%
%
%\newpage
%\subsection{Adding the Stieltjse Transform constraint}
%Throughout this section, let $\widehat\nu_\bX = \frac1n \sum_{i=1}^n \delta(\bX_i)$ be the empirical distribution of columns of the matrix $\bX$. Recall that $S^\star(\nu)\in\sS^{k\times k}$ satisfies the equation 
%\begin{equation}
%   \bG(S;\nu):= \frac1\alpha S^{-1} - \E_\nu[(\bI_k+\nabla^2\bell S)^{-1}\nabla^2\bell]=0.
%\end{equation}
%We are interested in bounding
%\begin{equation}
%    \frac1n\log\E_{\mu} \left[ \exp\left\{nF\left(
%    \E_{\widehat\nu_X}\left[ H(V,S^\star(\widehat\nu_X) \right]
%    \right) \right\} \right],
%\end{equation}
%where $X = (X_1,\dots,X_n)\sim \mu^{\otimes n}$ and $V|X\sim \widehat \nu_X$.
%\paragraph{Step 1: Covering $\sS^{k\times k}$}.
%Let $R>0$. Let us first consider the term
%\begin{equation}
%    \Phi_1:=\frac1n\E_{\mu} \left[ \exp\left\{nF\left(
%    \E_{\widehat\nu_X}\left[ H(V,S^\star(\widehat\nu_\bX) \right]
%    \right) \right\} \one_{B(\bzero,R)}S^\star(\widehat\nu_X) \right].
%\end{equation}
%Consider an $\eps$ cover of $B(\bzero,R)\subset \sS^{k\times k}$. Using the log-sum inequality, we get
%\begin{align}
%    \Phi_1& \leq \frac{k^2}{n} \log\left(\frac{R}{\eps}\right)
%    +\sup_{\bS\in B(\bzero,R)} \frac1n\E_{\mu} \left[ \exp\left\{nF\left(
%    \E_{\widehat\nu_X}\left[ H(V,S^\star(\widehat\nu_X) \right]
%    \right) \right\} \one_{B(\bS,\eps)}(S^\star(\widehat\nu_X)) \right]\\
%    &\leq \frac{k^2}{n} \log\left(\frac{R}{\eps}\right)
%    +\sup_{\bS\in B(\bzero,R)} \frac1n\E_{\mu} \left[ \exp\left\{nF\left(
%    \E_{\widehat\nu_X}\left[ H(V,\bS) \right]
%    \right) \right\} \one_{B(\bS,\eps)}(S^\star(\widehat\nu_X)) \right] + \omega_F(\eps)\\
%    &\leq \frac{k^2}{n} \log\left(\frac{R}{\eps}\right)
%    +\sup_{\bS\in B(\bzero,R)} \frac1n\E_{\mu} \left[ \exp\left\{nF\left(
%    \E_{\widehat\nu_X}\left[ H(V,\bS) \right]
%    \right) \right\} 
%    \one(\bG(\bS,\widehat\nu_X)\leq \eps_2) \right] + \omega_F(\eps)
%\end{align}
%\paragraph{Step 2: Indicator function}
%Next, we need to show that for any $\bS\in\sS^{k\times k}$,
%\begin{equation}
%    \frac1n\E_{\mu} \left[ \exp\left\{nF\left(
%    \E_{\widehat\nu_X}\left[ H(V,S) \right]
%    \right) \right\} \one(\bG(S,\widehat\nu_X)\leq \eps_2) \right]
%    \leq \sup_{\substack{\nu,\\ \bG(\bS,\nu)\leq \eps_2}} F(\E_\nu[H(V,\bS)]) - \KL(\nu\|\mu) + \omega_\bG(\eps_2),
%\end{equation}
%where $\bG$ is a linear function of $\nu$.
%\paragraph{Step 3: } 
%Lastly, we need to show 
%\begin{equation}
%    \sup_{\bS} \sup_{\nu:\bG(\bS,\nu)\leq \eps_2}
%    F(\E_\nu[H(V,\bS)]) - \KL(\nu\|\mu) + \omega_F(\eps) + \omega_\bG(\eps_2) \leq \sup_{\nu} F(\E_{\nu}[H(V,S^\star(\nu))]) - \KL(\nu\|\mu) + \omega(\eps) 
%\end{equation}
%
%
%
%
%
%
%
%
%
%
%
%
%\newpage
%\subsection{Quantitative Varadhan Lemma on the Manifold}
%
%we are interested in applying the Large deviation results from the last section to an object of the form
%\begin{equation}
%    \int_{\bt\in \cM} \exp\{nF(\widehat\nu_\bt)\} P(\bt) \de V,
%\end{equation}
%where $P(\bt_i)\sim\cN(\bzero, \bR(\bTheta)$ is the Gaussian density, as defined in \ref{}.
%
%To be able to apply Lemme \ref{}[Varadhan], we write the integration over the manifold $\cM$ as integration over the whole space using the Dirac delta function:
%
%\begin{lemma}
%    \begin{equation}
%        \int_{\bt \in \cM}\exp\{nF(\widehat\nu_\bt)\} P(\bt) \de V =  
%        \lim_{\eps\rightarrow 0 }\int_{\R^{nk+nk_0}} \exp\{nF(\widehat\nu_\bt)\}P(\bt)
%        \one
%        \left(\bL(\bt)^\sT(\bt)\leq \eps\right)  \de \bt
%    \end{equation}
%\end{lemma}
%
%
%\begin{lemma}
%    let $L:\R^{k_1}\rightarrow\R^{r_k}$ and $F:\R^{k_1}\rightarrow\R^{k_2}$ be two borel-measurable functions and let $X$ be  a random variable with distribution $\nu\in\cM_1(\R^{k_1})$. Then,
%    \begin{equation}
%        \E[F(X)|L(X)=0] = \frac{\E[F(X)\cdot\delta(L(X))]}
%        {\E[\delta(L(X))]},
%    \end{equation}
%    where $\delta$ is the Dirac Delta function.
%\end{lemma}
%\begin{lemma}
%    Let $L:\R^{k_1}\rightarrow\R^{r_k}$. Then,
%    \begin{equation}
%        \E\left[
%        \delta(\sum_{i=1}^n L(X_i)) \leq
%        \right]
%    \end{equation}
%\end{lemma}
%\begin{proof}
%    First, observe that by definition,
%    \begin{align}
%        \E[\delta(\sum_{i=1}^n L(X_i))] 
%        =& \lim_{\eps\rightarrow 0 }
%        \frac1{2\eps}\P\left(|\sum_{i=1}^n L(X_i)|\leq \eps\right)\\
%        \leq & \lim \frac{1}{2\eps} \exp\{-n\inf_{\bz\in B(\bzero,n\eps)}\Lambda^\star_L(\bz)\}.
%    \end{align}
%    
%\end{proof}
%
%
%\begin{lemma}
%    Let $k_1\equiv k_1(n)$, $k_2\equiv k_2(n)$, and let $G:R^{k_1}\rightarrow \R^{k_2}$. Let $\cM\subset\R^{nk_1}$ be a smooth manifold defined by
%    \begin{equation}
%        \cM\equiv\cM(n):=\{\bx=(\bx_1^\sT,\dots,\bx_n^\sT)\in\R^{nk_1}: \sum_{i=1}^{n}L(\bx_i)=0\}
%    \end{equation}
%    , where $L:\R^{k_1}\rightarrow\R^{r_k}$ is smooth and differentiable. Further
%    Define
%    \begin{equation}
%        \Phi(n,\mu):= \frac1n\log
%        \int_{\bx\in\cM}
%        \exp\left\{
%        n F(\widehat S_n(\bx))\de \mu^{\otimes n }(\bx)
%        \right\}\de V.
%    \end{equation}
%    Then, under the assumptions of the Theorem \ref{},
%    \begin{equation}
%        \Phi(n,\mu)\leq \sup_{\substack{\nu\in \cM_1(\R^{k_1}),\\ \E_\nu(L(X))=0}}
%    \end{equation}
%    
%\end{lemma}
%
%\begin{proof}
%    
%\end{proof}


\subsection{Simplifying the constraint set in Theorem~\ref{thm:global_min}}
\label{sec:simplifying_constraint_set}
We state and prove the two lemmas referenced in the proof of Theorem~\ref{thm:global_min} that allow us to simplify the set of critical points on which the rate function bound is applicable. 
\begin{lemma}[Lower bounding the smallest singular value of the Jacobian]
\label{lemma:jacobian_lb}
Assume $\rho(t) = \lambda \; t^2/2$ for $\lambda \ge0$.
For any critical point $\bTheta$ of $\widehat R_n(\bTheta)$, 
we have under Assumption~\ref{ass:loss},
\begin{equation}
    \sigma_{\min}\left( \bJ_{(\bTheta,\bbV)} \bG^\sT\right) \ge 
     \frac{\sigma_{\min}(\grad^2 \hat R_n(\bTheta))
     \sigma_{\min}([\bTheta,\bTheta_0])
     }{(1 + \|\bX\|_\op )}.
\end{equation}
\end{lemma}

\begin{proof}
Recall that $\bJ_{(\bTheta,\bbV)}\bG\in\reals^{k(k+k_0)\times (dk + n (k+k_0))}$ denotes the Euclidean Jacobian of the function $\bg : \R^{dk + n (k+k_0)} \to \R^{k(k+k_0)}$ obtained from vectorizing $\bG$ and its arguments.
Namely,
\begin{equation}
    \bg(\bTheta,\bbV) = (g_{i,j}(\bTheta,\bbV)_{i\in[k],j\in[k+k_0]},\quad
    g_{i,j}(\bTheta,\bbV) = \begin{cases}
       \frac1n\bell_i^\sT\bv_j  + \lambda \btheta_i^\sT\btheta_j & j \le k\\
       \frac1n\bell_i^\sT\bv_{0,j}  + \lambda \btheta_i^\sT\btheta_{0,j - k} & 
       j  > k
    \end{cases},
    i \in[k].
\end{equation}
%So the first $k$ columns of $\bJ \bG^\sT$ are given by
%\begin{equation}
%\frac1n
%     \begin{bmatrix}
%         \bSec (\bI_k \otimes \bv_1) \\
%         \vdots\\
%         \bSec (\bI_k \otimes \bv_k) \\
%    \end{bmatrix}  + 
%        (\bI_k \otimes \bell)
% \quad \textrm{where} \quad 
%\bar\bell = \begin{bmatrix}
%    \bell_1\\
%    \vdots\\
%    \bell_k
%\end{bmatrix} \in\R^{nk},
%\end{equation}
%and the last $k$ columns are given by
%with the definitions of Eq.~\eqref{eq:SecDef}
%\begin{equation}
%    \bJ\bG^\sT = \begin{bmatrix}
%    \frac1n
%     \begin{bmatrix}
%         \bSec (\bI_k \otimes \bv_1) \\
%         \vdots\\
%         \bSec (\bI_k \otimes \bv_k) \\
%    \end{bmatrix} 
%    & 
%    \frac1n
%   \begin{bmatrix}
%         \tilde\bSec (\bI_k \otimes \bv_1) \\
%         \vdots\\
%         \tilde\bSec (\bI_k \otimes \bv_k) \\
%   \end{bmatrix} 
%   & 
%   \lambda\begin{bmatrix}
%          \bI_k\otimes \btheta_1\\
%         \vdots\\
%         \bI_k \otimes \btheta_k \\
%   \end{bmatrix}\\
%   %%%%%%%%%%%%%%
%   \frac1n
%     \begin{bmatrix}
%         \bSec (\bI_k \otimes \bv_{0,1}) \\
%         \vdots\\
%         \bSec (\bI_k \otimes \bv_{0,k_0}) \\
%    \end{bmatrix}
%    & 
%    \frac1n
%   \begin{bmatrix}
%         \tilde\bSec (\bI_k \otimes \bv_{0,1}) \\
%         \vdots\\
%         \tilde\bSec (\bI_k \otimes \bv_{0,k_0}) \\
%   \end{bmatrix} 
%   & 
%   \lambda\begin{bmatrix}
%          \bI_k\otimes \btheta_{0,1}\\
%         \vdots\\
%         \bI_k \otimes \btheta_{0,k_0} \\
%   \end{bmatrix}\\
%    \end{bmatrix}
%    +
%    \begin{bmatrix}
%        (\bI_k \otimes \bar\bell) & 
%        \bzero_{nk^2 \times k_0} &  \lambda (\bI_k \otimes \bar\btheta)\\
%        \bzero_{nkk_0 \times k} & 
%        (\bI_{k_0} \otimes \bar\bell)
%         & \bzero_{dk}
%    \end{bmatrix}
%\end{equation}
Recalling the definitions in Eq.~\eqref{eq:SecDef} and Eq.~\eqref{eq:TildeSecDef} of $\bSec$ and $\tilde\bSec$, with sufficient diligence, the desired Jacobian can be computed to be 
\begin{align}
    \bJ\bG^\sT &= \begin{bmatrix}
    \frac1n
     \begin{bmatrix}
         \bSec (\bI_k \otimes \bv_1),& \dots ,&
         \bSec (\bI_k \otimes \bv_k) 
    \end{bmatrix} 
&
   \frac1n
     \begin{bmatrix}
         \bSec (\bI_k \otimes \bv_{0,1}), &
         \dots,&
         \bSec (\bI_k \otimes \bv_{0,k_0}) \\
    \end{bmatrix}\\
    \frac1n
   \begin{bmatrix}
         \tilde\bSec (\bI_k \otimes \bv_1), &
         \dots,&
         \tilde\bSec (\bI_k \otimes \bv_k) \\
   \end{bmatrix} 
   &
    \frac1n
   \begin{bmatrix}
         \tilde\bSec (\bI_k \otimes \bv_{0,1}), &
         \dots,&
         \tilde\bSec (\bI_k \otimes \bv_{0,k_0}) 
   \end{bmatrix} \\
   \lambda\begin{bmatrix}
          \bI_k\otimes \btheta_1, &
         \dots,&
         \bI_k \otimes \btheta_k 
   \end{bmatrix}
   &
   \lambda\begin{bmatrix}
          \bI_k\otimes \btheta_{0,1},&
         \dots,&
         \bI_k \otimes \btheta_{0,k_0} 
   \end{bmatrix} 
    \end{bmatrix}\\
    &\hspace{80mm}+
     \begin{bmatrix}
       \frac1n(\bI_{k + k_0} \otimes \bL) \\
       [\lambda(\bI_k \otimes \bTheta), \bzero_{dk \times kk_0}]
    \end{bmatrix} \in\R^{(nk + nk_0 + dk) \times k(k+k_0)}.
\end{align}
Define
\begin{equation}
    \bB := \begin{bmatrix}
       (\bI_k \otimes \bX^\sT)  &
        \bzero_{dk\times n k_0} &
        \bI_{dk}
    \end{bmatrix} \in\R^{dk \times (nk + nk_0 + dk)}.
\end{equation}
Recalling the definition of $\bH_0$ in Eq.~\eqref{eq:bH_def} and
noting that at any critical point of $\hat R_n(\bTheta)$, we have
\begin{equation}
  \bX^\sT\bK[\bv_i,\bv_{0,j}] = \bH_0 [\btheta_i , \btheta_{0,j}]
  \quad
  \textrm{and}
  \quad
  \bX^\sT\bell_{i}  = - \lambda \btheta_i \quad \textrm{for}\quad i\in[k],j\in[k_0],
\end{equation}
we can compute at a critical point
\begin{equation}
    \bB \bJ\bG^\sT = 
       \left(\frac1n \bH_0 + \lambda\bI_{dk}\right)\bA,
       \quad\textrm{where}
       \quad
       \bA := \left[(\bI_k \otimes \btheta_1),\dots,(\bI_k\otimes\btheta_k),(\bI_k \otimes \btheta_{0,1},\dots,(\bI_k\otimes\btheta_{0,k_0})\right].
\end{equation}
By direct computation, one finds that for some permutation matrix $\bP \in \R^{dk \times dk}$, we have
\begin{equation}
    \bA^\sT\bA = \bP (\bI_k \otimes [\bTheta,\bTheta_0]^\sT[\bTheta,\bTheta_0]) \bP^\sT,
\end{equation}
so that $\sigma_{\min}(\bA) =  \sigma_{\min}([\bTheta,\bTheta_0]),$  and hence
\begin{equation}
    \|\bB\|_\op \sigma_{\min}(\bJ \bG^\sT) \ge 
    \sigma_{\min}(\grad^2 \hat R_n(\bTheta)) \sigma_{\min}([\bTheta,\bTheta_0]).
\end{equation}
Using $\|\bB\|_\op \le \|\bX\|_\op + 1$ gives the claim.
\end{proof}

%\begin{lemma}[Lower bounding the smallest singular value of the Jacobian]
%\label{lemma:jacobian_lb}
%Assume $\rho(t) = \lambda \; t^2/2$ for $\lambda \ge0$.
%For any critical point $\bTheta$ of $\widehat R_n(\bTheta)$, 
%we have under Assumptions~\ref{ass:loss} on $\ell$
%\begin{equation}
%    \sigma_{\min}\left( \bJ_{(\bbV,\bTheta)} \bG^\sT\right) \ge 
%    C \frac{\sigma_{\min}(\grad^2 \hat R_n(\bTheta))}{(1 + \lambda \|\grad^2\hat R_n(\bTheta)\|_\op) } \frac{\sigma_{\min}([\bTheta,\bTheta_0])}{ \|\bX\|_\op \vee 1},
%\end{equation}
%for some universal constant $C>0$.
%\end{lemma}
%
%\begin{proof}
%\bns{fix this}
%%\subsubsection{Proof of Lemma~\ref{lemma:jacobian_lb}}
%\label{sec:proof_of_lemma_jacobian_lb}
%%For the case of $\lambda = 0$, 
%%consider the block matrix
%%\begin{equation}
%%    \bB_0 := \begin{bmatrix}
%%       \bI_k \otimes  \bX^\sT & \bzero & \bzero \\ 
%%        \bzero & \bzero_{kd \times kn} &\bzero\\
%%        \bzero & \bzero & \bzero_{kd \times kd}
%%    \end{bmatrix}.
%%\end{equation}
%%By explicitly computing $\bJ_{(\bbV,\bTheta)} \bG$, and with some algebra, one can see that
%%the singular values of 
%% $\bB_0 \bJ_{(\bbV,\bTheta)} \bG^\sT$ are equal to those of $\grad^2\hat R_n(\bTheta) (\bI_k \otimes  [\bTheta,\bTheta_0])).$ This gives the bound
%% \begin{equation}
%%     \sigma_{\min}(\bR) \sigma_{\min}(\bH)  \le \|\bB_0\|_\op \sigma_{\min}(\bJ_{(\bbV,\bTheta)} \bG).
%% \end{equation}
%%Noting that $\|\bB_0\|_\op \le \|\bX\|_\op $  gives the result.
%%Now for $\lambda >0$, let
%Let
%\begin{equation}
%    \bB_1 := \begin{bmatrix}
%       \bI_k \otimes \bX^\sT & \bzero & \bzero \\ 
%        \bzero & \bzero_{kd\times kn} &\bzero\\
%        \bzero & \bzero &\bI_d
%    \end{bmatrix}
%\end{equation}
%and denote the hessian of the loss by $\bH_0$, namely, $\bH_0 = \grad^2\hat R_n(\bTheta)- \lambda \bI$.
%By explicit computation, for $(\bTheta,\bbV)$ satisfying $\bG(\bTheta,\bbV) = \bzero$, the singular values of 
%$\bB_1 \bJ_{(\Theta,\bbV)} \bG^\sT$ can be shown to be equal to those of 
%\begin{equation}
%\bA:=
%\bA_0
%\begin{bmatrix}
%   \bI_{k}\otimes [\bTheta,\bTheta_0] & \bzero \\
%   \bzero & \bI_k\otimes [\bTheta,\bTheta_0]
%\end{bmatrix}
%\begin{bmatrix}
%   \bI_{k + k_0} \\
%   \bP
%\end{bmatrix},
%\quad\quad\textrm{where}\quad\quad
%\bA_0:=
%\begin{bmatrix}
%   \bH_0 & -\lambda\bI_{dk} \\
%   \lambda \bI_{dk} & \lambda \bI_{dk}
%\end{bmatrix}
%\end{equation}
%where $\bP \in\R^{(k+k_0)\times (k+k_0)}$ is some matrix obtained from a  permutation matrix by setting $k_0$ of the ones to $0$.
%Then
%\begin{equation}
%   \bA_0^\sT\bA_0  =   
%\begin{bmatrix}
%   \bH_0^2 +  \lambda^2 \bI& -\lambda \bH_0 + \lambda^2\bI_{dk} \\
%   -\lambda\bH_0 + \lambda^2 \bI_{dk} & 2 \lambda^2 \bI_{dk}.
%\end{bmatrix}
%\end{equation}
%So by the block inverse formula
%\begin{align}
%   \sigma_{\min}(\bA_0)^2 &\ge  \left( \|(\bH_0^2 + \lambda^2\bI)^{-1} \|_\op +\big(1 + \|(\bH_0^2 + \lambda^2\bI)^{-1} \|_\op \|-\lambda \bH_0 + \lambda^2 \bI\|_\op \big)^2  \|2(\bH_0 + \lambda\bI)^{-2}\|_\op  \right)^{-1}\\
%   &\ge  
%   \frac{\sigma_{\min}(\bH_0^2 + \lambda^2 \bI)}{1 + 2 \big(1 + \lambda \|\bH_0 - \lambda \bI\|_\op \big)^2}
%\ge   
%   \frac{\sigma_{\min}( \grad^2\hat R_n(\bTheta))^2}{2 + 4 \big(1 + \lambda \|\grad^2\hat R_n(\bTheta))\|_\op \big)^2}.
%\end{align}
%So \begin{equation}
%    \|\bB_1\|_\op\sigma_{\min}(\bJ_{(\bbV,\bTheta)} \bG^\sT) \ge 
%    \sigma_{\min}(\bB_1\bJ_{(\bbV,\bTheta)} \bG^\sT) \ge  \sigma_{\min}(\bA_0)  \sigma_{\min}([\bTheta,\bTheta_0]) \sigma_{\min}(\bI + \bP^\sT\bP).
%\end{equation}
%Now by definition of $\bP$, the matrix $\bP^\sT\bP$ is a diagonal matrix with only $1$'s and $0$'s along its main diagonal.  Meanwhile, $\|\bB_1\|_\op \le \|\bX\|_\op \vee 1$. Combining these gives the bound of the lemma.
%\end{proof}
%
\begin{lemma}\label{lemma:VolumeBound}
Assume $\sigma_{\min}(\bTheta_0) \succ r\bI$ for some $r>0$.
Define
   \begin{equation}
       \cS_{\delta,R} := \{ \bTheta \in\R^{d\times k} : \|\bTheta\|_F \le R,\quad \sigma_{\min}([\bTheta,\bTheta_0]) \le \delta \}
   \end{equation}
   for $\delta < r/2$.
Then 
%
\begin{equation}
    \vol(\cS_{\delta,R}) \le
     k (C(R,r))^{dk} (\sqrt{k} \delta)^{d - k - k_0+1}.
\end{equation}
for constant $C(R,r)>0$ depending only on $r$ and $R.$
\end{lemma}
\begin{proof}
If $\bTheta \in\cS_{\delta,R}$, then there exists some $(\bbeta^\sT,\bbeta_0^\sT)^\sT\in\R^{k+k_0}$ with norm 1 such that $\bTheta\bbeta+\bTheta_0\bbeta_0 = \bu$ for some $\bu$ with $\|\bu\|_2 \le \delta.$
Let $j = \argmax_{i\in[k]} |\beta_i|$, where $\beta_i$ is the $i$-th coordinate of $\bbeta$, and denote by $\bP^\perp_{j}$ the projection onto the orthocomplement of 
\begin{equation}
    \textrm{span}\left(\{\btheta_{0,i}\}_{i\in[k_0]} \cap \{\btheta_{i}\}_{i\in[k], i\neq j} \right).
\end{equation}
Since
$\bu = \sum_{i=1}^k \beta_i \btheta_i + \sum_{i=1}^{k_0} \beta_{0,i} \btheta_{0,i},$
we have $\delta \ge |\beta_j|\|\bP_{j}^\perp \btheta_j\|_2$.
Now note that $\sigma_{\min}(\bTheta_0) > 2\delta$
and $\|\bTheta\|_F \le R$ implies that there must exist some constant $c_0(R,c)$ depending only on $R$ and $r$ and such that $\|\bbeta\|_2 \ge c_0(R,c).$ 
Indeed, we have
\begin{equation}
    r \|\bbeta_0\|_2 \le \|\bTheta_0 \bbeta_0\| = \|\bu - \bTheta\bbeta\| \le \delta + R \|\bbeta\|_2.
\end{equation}
Using that $\|\bbeta\|_2^2 + \|\bbeta_0\|_2^2 = 1$ and $\delta < r/2$, this then gives
\begin{equation}
   \|\bbeta\|_2^2 \ge \frac12\frac{r^2}{r^2 + 2 R^2}.
\end{equation}
Hence, for $j$ being the index of maximum mass as above, we have $|\beta_j| \ge c_0 k^{-1/2}$. This allows us to conclude that
$\cS_{\delta,R} \subseteq \bigcup_{j=1}^k  \cV_{j}(\delta, R)$
where
\begin{equation}
    \cV_{j}(\delta, R) := \left\{\|\bTheta\|_F \le R ,\quad   \|\bP_{-j} \btheta_j\|_2 \le \frac{\sqrt{k}}{c_0} \delta\right\}.
\end{equation}
Meanwhile, for any $j \in[k]$,
\begin{equation}
    \vol( \cV_j(\delta,R)) \le  (C R)^{dk} \left(\frac{\sqrt{k} \delta}{c_0}\right)^{d - k - k_0+1}.
\end{equation}
Bounding the volume of the union by the sum of the volumes gives the claim.

%$\delta \ge \|\bP_{-j} \bTheta \bbeta\|_2  = |\beta_j| \|\bP_{-j}\btheta_j\|_2,$ 
%where
%$j := \argmax_{i \in[k+k_0]} |\beta_i|$ and $\bP_{-j}$ is the projection onto the span of $\{\btheta_i\}_{i\neq j}$. 
%Since $|\beta_j| \le k^{-1/2},$ this implies that
%    $\cS_{\delta,R} \subseteq \bigcup_{j=1}^k  \cV_{j}(\delta, R)$
%where
%\begin{equation}
%    \cV_{j}(\delta, R) := \{\|\bTheta\|_F \le R ,\quad   \|\bP_{-j} \btheta_j\|_2 \le \sqrt{k} \delta\}.
%\end{equation}
\end{proof}


\begin{lemma}
\label{lemma:min_sv_Theta}
Let $\bL(\bV,\bV_0,\bw)\in\R^{n\times k}$ be as defined in~\eqref{eq:def_bL_bRho}.
Assume $\sigma_{\min}(\bTheta_0) \succ c_0\bI$ for some $c_0 >0$.
Then under Assumption~\ref{ass:loss} on the loss, with the ridge regularizer $\rho(t) = \lambda t^2/2$,
%\begin{equation}
%    \bQ(\bTheta) := \frac1n \sum_{i=1}^n (\grad \ell \grad \ell^\sT)(\bTheta\bx_i,\bTheta_{0}\bx_i, \bw_i).
%\end{equation}
for any fixed $C,c>0$ and $\lambda \ge 0$, there exists $\delta >0$ sufficiently small such that
\begin{equation}
\lim_{n\to\infty}\P\left( \exists \bTheta :\sigma_{\min}([\bTheta,\bTheta_0]) < \delta,\;  \sigma_{\min}(\bL(\bX\bTheta,\bX\bTheta_0,\bw)) \ge  n\, c,\; \|\bTheta\|_F \le C,\;
\grad \hat R_n(\bTheta)  = \bzero
\right)   = 0.
\end{equation}
\end{lemma}
\begin{proof}
Let
\begin{equation}
    \bF(\bTheta) := \frac1{\sqrt{n}} \bX^\sT \bL(\bX\bTheta,\bX\bTheta_0,\bw) + \sqrt{n}\lambda \bTheta\, ,
\end{equation}
be the scaled gradient of the empirical risk.
Conditional on $\bX[\bTheta,\bTheta_0] = \bbV$ and $\bw,$ the random variable $\bP_{[\bTheta,\bTheta_0]}^\perp\bF(\bTheta)$ is  distributed as $\bU_{\bTheta}\bZ_0$ where  $\bU_{\bTheta}\in\reals^{n\times (d-k-k_0)}$ is a basis of the orthogonal complement of $[\bTheta,\bTheta_0]$
and
\begin{equation}
    \bZ_0 \sim \cN(\bzero, \bI_{d- k - k_0} \otimes \bL^\sT\bL/n)\, .
\end{equation}
Therefore we can bound for any fixed $\bTheta$,
\begin{align}
\label{eq:small_ball_prob}
\P\left( \|\bF(\bTheta)\|_F \le \eps \sqrt{nk} , \bL^\sT\bL \succ c n  \right)
&\le\E\left[\P\left( \|\bF(\bTheta)\|_F \le \eps \sqrt{nk} , \bL^\sT\bL \succ c n  \Big| \bX[\bTheta,\bTheta_0]=\bbV, \bw \right)\right]\\
&\le (C_0 \eps)^{dk - k - k_0}\, ,\nonumber
\end{align}
for some $C_0>0$ depending only on $c$.
For fixed $\delta,\eps >0$, consider now the sets
\begin{equation}
    \cA_0(\delta) := \{\bTheta : \sigma_{\min}(\bTheta,\bTheta_0) \le \delta\},\quad\quad
    \cA(\delta,\eps) := \{\bTheta \in\cA_0(\delta) : \|\bF(\bTheta)\|_F \le \eps\}.
\end{equation}
Since Assumption~\ref{ass:loss} guarantees that for any $\bTheta_1,\bTheta_2$,
\begin{equation}
    \|\bF(\bTheta_1) - \bF(\bTheta_2)\|_F^2 \le C_1 \,k \frac1n \|\bX\|^4_\op  \|\bTheta_1 -\bTheta_2\|_F^2,
\end{equation}
We have on the high probability event $\Omega_0 := \{\|\bX\|_\op \le 3 \sqrt{n}\},$
for any $\tilde\bTheta \in \cA_0(\delta)$ with $\bF(\tilde\bTheta) = \bzero$, 
\begin{equation}
\|\bTheta - \tilde \bTheta\|_F \le \eps/2 \;\;\Rightarrow\;\;
    \|\bF(\bTheta)\|_F  \le C_2 \sqrt{k n} \eps,\quad
\sigma_{\min}([\bTheta,\bTheta_0]) \le \delta + \eps\, .
\end{equation}
Namely, letting $\Ball_{\eps/2}^{d\times k}(\bzero)$ be the Euclidean ball in $\R^{d\times k}$ of radius $\eps/2$, this 
shows that
\begin{equation}
\cA_0(\delta) + \Ball_{\eps/2}^{d\times k}(\bzero)  \subseteq   \cA(\delta + \eps , C_2 \sqrt{k n} \eps).
\end{equation}
And since $\cA(\delta + \eps , C_2 \sqrt{k n} \eps) \subseteq \cA_0(\delta +\eps)$, standard bounds on the covering number $\cN_{\eps}( \cA_0(\delta)\cap\Ball_{C}^{d\times k}(\bzero))$ of $\cA_0(\delta)\cap\Ball_{C}^{d\times k}(\bzero)$ with Euclidean balls of radius $\eps$ give
\begin{equation}
   \cN_\eps(
   \cA_0(\delta)\cap\Ball_C^{d\times k}(\bzero))\;
   \vol(\Ball_{\eps/2}^{d\times k}(\bzero))  \le  \vol\big((\cA_0(\delta)\cap\Ball_C^{d\times k}(\bzero)) + \Ball_{\eps/2}^{d\times k}(\bzero)\big) 
   \le   \vol(\cA_0(\delta +\eps)\cap\Ball_C^{d\times k}(\bzero)),
\end{equation}
where $``+"$ denotes the Minkowski sum of sets.
Therefore by~\eqref{eq:small_ball_prob}, 
%
\begin{align*}
   &\P\left(\exists \bTheta \in \cA_0(\delta) : \bF(\bTheta) = \bzero, \bL^\sT\bL \succ n c,\; \|\bTheta\|_F \le C\right)   \\
  &\le 
  \P\left(
  \exists \bTheta \in \cN_\eps(\cA_0(\delta)) : \|\bF(\bTheta)\|_F \le 2 C_2 \sqrt{kn} \eps, \;
  \bL^\sT \bL \succ c_0 n,\; 
  \|\bTheta\|_F \le C
  \right)\\
  &\le \cN_\eps(\cA_0(\delta) \cap\Ball_C^{d\times k}(\bzero)) (2 C_2 C_0 \eps)^{dk - k - k_0}\\
  &\le (C_3 \eps)^{dk - k- k_0} \left(\frac{1}{C_4 \eps}\right)^{dk}   \vol(\cA_0(\delta +\eps) \cap \Ball_{C}^{d\times k}(\bzero))\\
  &\stackrel{(a)}{\le} (C_3 \eps)^{dk - k- k_0} \left(\frac{1}{C_4 \eps}\right)^{dk}  k (C_5)^{dk}  (\sqrt {k} (\delta + \eps) )^{d -k -k_0 -1}\\
  &\le k C_6^{dk} \frac{(\sqrt{k}(\delta+\eps))^{d-k-k_0-1}}{\eps^{k+k_0}}\, ,
\end{align*}
where in step $(a)$ we used Lemma \ref{lemma:VolumeBound}. 
Now choose $\delta,\eps$ sufficiently small so that the latter quantity converges to $0$ as $n\to\infty.$



\end{proof}


%

\subsection{Proof of Theorem~\ref{thm:simple_critical_point_variational_formula}}
To see that $F$ is convex, define $F:   L^2\times\sfS^k_{\ge}\times \R^{k\times k_0}\to \R$  via
\begin{align}
\cuF(\bu,\bK,\bM):= \E[\ell(\bu + \bK \bz_1 + \bM \bz_0, \bR_{00}^{1/2}\bz_0, w)]  + \lambda(\bK^2 + \bM\bM^\sT)\, .
\end{align}
Since $\cuF$ is jointly convex, the convexity of $F$ will follow if we conclude that the set
\begin{equation}
    \cA := \{(\bK,\bu) \in \sfS^k \times L_2 : \E[\bu\bu^\sT]\preceq \alpha^{-1}\bK^2\}
\end{equation}
is jointly convex. But this is clear once we write
\begin{equation}
\label{eq:constraint_set_A}
    \cA = \bigcap_{\substack{V\in L^2\\\bv \in \R^k}} \cA_0(V,\bv),
    \quad\quad 
    \cA_0(V,\bv) := \{(\bK,\bu) : \bv^\sT\bK \bv  -2\E[V\, \bv^\sT\bu] + \E[V^2] \ge 0\},
\end{equation}
since $\cA_0(V,\bv)$ is convex. This shows the claim in~\textit{1.} of the theorem.

To prove the claim in~\textit{2.}, 
let $\bg = \bK\bz_1 + \bM \bz_0, \bg_0  = \bR_{00}^{1/2} \bz_0$,
and write
\begin{align}
    F(\bK,\bM)
&=
\inf_{\bu \in \cS(\bK)} 
\left\{
\E\left[\ell(\bu+ \bg,\bg_0,w) 
\right]
+
\frac{\lambda}{2}\bR_{11}
\right\}\\
&=
\inf_{\bu \in L^2} 
\sup_{\bQ\succ \bzero} 
\left\{
\E\left[\ell(\bu+ \bg,\bg_0,w) 
\right]
+
\frac12\Tr\left(
(\E[
\bu\bu^\sT]
- \alpha^{-1} \bK^2
)\bQ\right)+
\frac{\lambda}{2}\bR_{11}
\right\}\\
&= 
\sup_{\bQ\succ \bzero} 
\inf_{\bu \in L^2} 
\left\{
\E\left[\ell(\bu+ \bg,\bg_0,w) 
\right]
+
\frac12\Tr\left(
(\E[
\bu\bu^\sT]
- \alpha^{-1} \bK^2
)\bQ\right)+
\frac{\lambda}{2}\bR_{11}
\right\}\\
&= 
\sup_{\bS\succ \bzero} 
\inf_{\bx \in L^2} 
\left\{
\E\left[\ell(\bx,\bg_0,w) 
+\frac12 (\bg - \bx)^\sT\bS^{-1}(\bg-\bx)
\right]
-\frac1{2\alpha} \Tr\left(\bS^{-1}\bK^2\right) +
\frac{\lambda}{2}\bR_{11}
\right\}\\
&=\sup_{\bS\succ \bzero}  G(\bK,\bM,\bS).
\end{align}
where $G(\bK,\bM, \bS)$ is the objective in Eq.~\eqref{eq:min_max_critical_pts} after the reparametrization 
\begin{equation}
(\bR/\bR_{00},\bR_{10}\bR_{00}^{-1/2}) = (\bK^2,\bM), 
\end{equation}
i.e.,
\begin{equation}
    G(\bK,\bM,\bS) :=
       \E\left[\More_{\ell(\cdot, \bg_0,w)}(\bg;\bS)\right] - \frac1{2\alpha}\Tr(\bS^{-1}\bK^2) 
       + \frac{\lambda}{2}\Tr(\bK^2 + \bM\bM^\sT)
\end{equation}
%
with the Moreau envelope defined in Eq.~\eqref{eq:moreau_def}.
%We'll first show that 
%\begin{equation}
%\label{eq:F_and_G_relation}
%    \sup_{\bS\succ\bzero} G(\bK,\bM,\bS) =  F(\bK,\bM).
%\end{equation}
%By optimizing over $\bx\in L^2$, we can write
%%where
%%\begin{equation}
%%    \cA := \{\bu \in L^2 : \alpha\E[\bu\bu^\sT]\preceq \bSigma\}.
%%\end{equation}
%This proves Eq.~\eqref{eq:F_and_G_relation}.
%, for $\bSigma\in\sfS^k$ define
%\begin{equation}
%    \cA(\bSigma) :=
%    \left\{(\bu, \bB)  \in L_2 \times  \sfS^k : 
%    \E[\bu\bu^\sT]  \preceq \bB,\;
%    \bB \preceq \alpha^{-1} \bSigma
%    \right\}\,
%\end{equation}
 %note that we can write
%\begin{equation}
    %F(\bSigma,\bM) = \inf_{(\bu,\bB)\in\bS(\bSigma)}
   %\E[\ell(\bu + \bSigma \bz_1 + \bM \bz_0, \bR_{00}^{1/2}\bz_0, w)]  + \lambda(\bSigma^2 + \bM\bM^\sT).
%\end{equation}
Now, by straightforward differentiation of $G(\bK,\bM,\bS)$ with respect to each of $\bK,\bM,\bS$, one can show that the critical points of $G(\bK,\bM,\bS)$ are given by $(\bK,\bM, \bS) = (\bR^\opt/\bR_{00},\bR_{10}^\opt \bR_{00}^{-1},\bS^\opt)$ by checking that the stationarity conditions corresponds to Eq.~\eqref{eq:opt_fp_eqs}.
Furthermore, by definition, $\bS^\opt(\bR)$, the solution of~\eqref{eq:opt_fp_eqs} is unique for each $\bR$ (as the limit of the Stieltjes Transform $\bS_\star$ or as the minimizer of a strongly convex program as seen in the proof of Theorem~\ref{thm:global_min}).
Then by differentiation of $\bG$ with respect to $\bS$, one can show that $G(\bK,\bM,\bS)$ is locally concave at $\bS = \bS^\opt$ for fixed $\bR.$ This shows that indeed $\bS^\opt(\bR)$ is the maximizer of $G(\bK,\bM,\bS).$
%To prove the characterization in Eq.~\eqref{eq:min_max_critical_pts}, what remains is to show that the critical points $\bR^\opt$ of $\sup_{\bS \succ\bzero}G(\bR,\bS)$ are local minima. 
Combined with the convexity of $F(\bK,\bM)$ proves the claim.

Finally, the claim in~\textit{3.} follows from strict convexity of  
$(\bK,\bM,\bu) \mapsto \E\left[\ell(\bu+ \bg,\bg_0,w)
\right]
+
\frac{\lambda}{2}\bR_{11}$ 
under condition~\textit{(a)} and~\textit{(b)}, and the convexity of the constraint set $\cA$ of Eq.~\eqref{eq:constraint_set_A}.



