%Entries
@inproceedings{grounded,
  title={Grounded situation recognition},
  author={Pratt, Sarah and Yatskar, Mark and Weihs, Luca and Farhadi, Ali and Kembhavi, Aniruddha},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part IV 16},
  pages={314--332},
  year={2020},
  organization={Springer}
}
@inproceedings{situation,
  title={Situation recognition: Visual semantic role labeling for image understanding},
  author={Yatskar, Mark and Zettlemoyer, Luke and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5534--5542},
  year={2016}
}
@inproceedings{more,
  title={MORE: A Multimodal Object-Entity Relation Extraction Dataset with a Benchmark Evaluation},
  author={He, Liang and Wang, Hongke and Cao, Yongchang and Wu, Zhen and Zhang, Jianbing and Dai, Xinyu},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4564--4573},
  year={2023}
}
@article{llava1,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}
@inproceedings{llava2,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@InProceedings{clip,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
}
@ARTICLE{edit,
  author={Yujian, Li and Bo, Liu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Normalized Levenshtein Distance Metric}, 
  year={2007},
  volume={29},
  number={6},
  pages={1091-1095},
  keywords={Signal processing algorithms;Sequences;Cost function;Handwriting recognition;Information retrieval;Biomedical signal processing;Computational biology;Error correction;Pattern recognition;Image recognition;Sequence comparison;Levenshtein distance;normalized edit distance;metric;AESA.},
  doi={10.1109/TPAMI.2007.1078}}

@article{chatgpt,
  title={ChatGPT: Optimizing Language Models for Dialogue},
  author={OpenAI},
  journal={OpenAI Blog},
  year={2022},
  url={https://openai.com/blog/chatgpt/}
}
@article{claude,
  title={Introducing Claude},
  author={Anthropic},
  journal={Anthropic Blog},
  year={2022},
  url={https://www.anthropic.com/index/introducing-claude}
}

@article{music1,
  title={Issues surrounding the preservation of digital music documents},
  author={Lee, Brent},
  journal={Archivaria},
  pages={193--204},
  year={2000}
}
@article{music2,
  title={Using psychological principles of memory storage and preference to improve music recommendation systems},
  author={Chmiel, Anthony and Schubert, Emery},
  journal={Leonardo Music Journal},
  volume={28},
  pages={77--81},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{music3,
  title={Using psychological principles of memory storage and preference to improve music recommendation systems},
  author={Chmiel, Anthony and Schubert, Emery},
  journal={Leonardo Music Journal},
  volume={28},
  pages={77--81},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{lam2024efficient,
  title={Efficient neural music generation},
  author={Lam, Max WY and Tian, Qiao and Li, Tang and Yin, Zongyu and Feng, Siyuan and Tu, Ming and Ji, Yuliang and Xia, Rui and Ma, Mingbo and Song, Xuchen and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{yu-etal-2023-musicagent,
    title = "{M}usic{A}gent: An {AI} Agent for Music Understanding and Generation with Large Language Models",
    author = "Yu, Dingyao  and
      Song, Kaitao  and
      Lu, Peiling  and
      He, Tianyu  and
      Tan, Xu  and
      Ye, Wei  and
      Zhang, Shikun  and
      Bian, Jiang",
    editor = "Feng, Yansong  and
      Lefever, Els",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-demo.21",
    doi = "10.18653/v1/2023.emnlp-demo.21",
    pages = "246--255",
    abstract = "AI-empowered music processing is a diverse feld that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classifcation). For developers and amateurs, it is very diffcult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience. The code is available on GitHub along with a brief instructional video.",
}
@article{agostinelli2023musiclm,
  title={Musiclm: Generating music from text},
  author={Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023}
}
@inproceedings{liu2024music,
  title={Music understanding llama: Advancing text-to-music generation with question answering and captioning},
  author={Liu, Shansong and Hussain, Atin Sakkeer and Sun, Chenshuo and Shan, Ying},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={286--290},
  year={2024},
  organization={IEEE}
}
@inproceedings{tian2024n,
  title={N-gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding},
  author={Tian, Jinhao and Li, Zuchao and Li, Jiajia and Wang, Ping},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={14},
  pages={15364--15372},
  year={2024}
}
@article{li2023jen,
  title={Jen-1: Text-guided universal music generation with omnidirectional diffusion models},
  author={Li, Peike and Chen, Boyu and Yao, Yao and Wang, Yikai and Wang, Allen and Wang, Alex},
  journal={arXiv preprint arXiv:2308.04729},
  year={2023}
}
@article{huangmulan,
  title={MULAN: A JOINT EMBEDDING OF MUSIC AUDIO AND NATURAL},
  author={Huang, Qingqing and Jansen, Aren and Lee, Joonseok and Ganti, Ravi and Li, Judith Yue and Ellis, Daniel PW}
}
@article{li2024llava,
  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{sun2022ringmo,
  title={RingMo: A remote sensing foundation model with masked image modeling},
  author={Sun, Xian and Wang, Peijin and Lu, Wanxuan and Zhu, Zicong and Lu, Xiaonan and He, Qibin and Li, Junxi and Rong, Xuee and Yang, Zhujun and Chang, Hao and others},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2022},
  publisher={IEEE}
}
@article{copet2024simple,
  title={Simple and controllable music generation},
  author={Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{huang2024audiogpt,
  title={Audiogpt: Understanding and generating speech, music, sound, and talking head},
  author={Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={21},
  pages={23802--23804},
  year={2024}
}
@article{yuan2024chatmusician,
  title={ChatMusician: Understanding and Generating Music Intrinsically with LLM},
  author={Yuan, Ruibin and Lin, Hanfeng and Wang, Yi and Tian, Zeyue and Wu, Shangda and Shen, Tianhao and Zhang, Ge and Wu, Yuhang and Liu, Cong and Zhou, Ziya and others},
  journal={arXiv preprint arXiv:2402.16153},
  year={2024}
}
@article{lu2023musecoco,
  title={Musecoco: Generating symbolic music from text},
  author={Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
  journal={arXiv preprint arXiv:2306.00110},
  year={2023}
}
@misc{Nottingham,
  title={ABC version of the Nottingham Music Database},
  author={ James Allwright },
  year={2003},
  howpublished={https://abc.sourceforge.net/NMD/index.html},
}
@inproceedings{geerlings2020interacting,
  title={Interacting with GPT-2 to generate controlled and believable musical sequences in ABC notation},
  author={Geerlings, Carina and Merono-Penuela, Albert},
  booktitle={Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA)},
  pages={49--53},
  year={2020}
}
@inproceedings{ryu2024mid,
  title={MID-FiLD: MIDI Dataset for Fine-Level Dynamics},
  author={Ryu, Jesung and Rhyu, Seungyeon and Yoon, Hong-Gyu and Kim, Eunchong and Yang, Ju Young and Kim, Taehyun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={1},
  pages={222--230},
  year={2024}
}
@article{sturm2013gtzan,
  title={The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use},
  author={Sturm, Bob L},
  journal={arXiv preprint arXiv:1306.1461},
  year={2013}
}
@inproceedings{ccano2017moodylyrics,
  title={Moodylyrics: A sentiment annotated lyrics dataset},
  author={{\c{C}}ano, Erion and Morisio, Maurizio},
  booktitle={Proceedings of the 2017 international conference on intelligent systems, metaheuristics \& swarm intelligence},
  pages={118--124},
  year={2017}
}
@article{gpt4,
  title={{GPT}-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{visualGLM,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{qwen,
  title={Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@article{BLIP,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{InternLM,
      title={InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model}, 
      author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Xilin Wei and Songyang Zhang and Haodong Duan and Maosong Cao and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      year={2024},
      eprint={2401.16420},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{HendrycksBBZMSS21,
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  biburl = {https://www.bibsonomy.org/bibtex/2ab4bd8032379bce3c1f402859c138571/dblp},
  booktitle = {ICLR},
  ee = {https://openreview.net/forum?id=d7KBjmI3GmQ},
  interhash = {408537ae76bc10eb44bc08d7673c5922},
  intrahash = {ab4bd8032379bce3c1f402859c138571},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2024-04-10T01:22:14.000+0200},
  title = {Measuring Massive Multitask Language Understanding.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2021.html#HendrycksBBZMSS21},
  year = 2021
}
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}
@article{wang2023pandalm,
  title={Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization},
  author={Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others},
  journal={arXiv preprint arXiv:2306.05087},
  year={2023}
}
@article{valmeekam2024planbench,
  title={Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change},
  author={Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{guha2024legalbench,
  title={Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models},
  author={Guha, Neel and Nyarko, Julian and Ho, Daniel and R{\'e}, Christopher and Chilton, Adam and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{sun2024scieval,
  title={Scieval: A multi-level large language model evaluation benchmark for scientific research},
  author={Sun, Liangtai and Han, Yang and Zhao, Zihan and Ma, Da and Shen, Zhennan and Chen, Baocai and Chen, Lu and Yu, Kai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19053--19061},
  year={2024}
}
@inproceedings{downie2014ten,
  title={Ten years of MIREX: reflections, challenges and opportunities},
  author={Downie, J Stephen and Hu, Xiao and Lee, Jin Ha and Choi, Kahyun and Cunningham, Sally Jo and Hao, Yun},
  booktitle={ISMIR 2014},
  pages={657--662},
  year={2014},
  organization={ISMIR}
}
@article{kong2020panns,
  title={Panns: Large-scale pretrained audio neural networks for audio pattern recognition},
  author={Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={2880--2894},
  year={2020},
  publisher={IEEE}
}
@inproceedings{zhao2021musicoder,
  title={Musicoder: A universal music-acoustic encoder based on transformer},
  author={Zhao, Yilun and Guo, Jia},
  booktitle={MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22--24, 2021, Proceedings, Part I 27},
  pages={417--429},
  year={2021},
  organization={Springer}
}
@article{melechovsky2023mustango,
  title={Mustango: Toward controllable text-to-music generation},
  author={Melechovsky, Jan and Guo, Zixun and Ghosal, Deepanway and Majumder, Navonil and Herremans, Dorien and Poria, Soujanya},
  journal={arXiv preprint arXiv:2311.08355},
  year={2023}
}
@inproceedings{dey-etal-2024-socialite,
    title = "{SOCIALITE}-{LLAMA}: An Instruction-Tuned Model for Social Scientific Tasks",
    author = "Dey, Gourab  and
      V Ganesan, Adithya  and
      Lal, Yash Kumar  and
      Shah, Manal  and
      Sinha, Shreyashee  and
      Matero, Matthew  and
      Giorgi, Salvatore  and
      Kulkarni, Vivek  and
      Schwartz, H.",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-short.40",
    pages = "454--468",
    abstract = "Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama {---} an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, suggesting instruction tuning can lead to generalized social understanding. All resources including our code, model and dataset can be found through [bit.ly/socialitellama](https://bit.ly/socialitellama/).",
}
@misc{gpt4-eval,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{baez-saggion-2023-lsllama,
    title = "{LSL}lama: Fine-Tuned {LL}a{MA} for Lexical Simplification",
    author = "Baez, Anthony  and
      Saggion, Horacio",
    editor = "{\v{S}}tajner, Sanja  and
      Saggio, Horacio  and
      Shardlow, Matthew  and
      Alva-Manchego, Fernando",
    booktitle = "Proceedings of the Second Workshop on Text Simplification, Accessibility and Readability",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.tsar-1.10",
    pages = "102--108",
    abstract = "Generative Large Language Models (LLMs), such as GPT-3, have become increasingly effective and versatile in natural language processing (NLP) tasks. One such task is Lexical Simplification, where state-of-the-art methods involve complex, multi-step processes which can use both deep learning and non-deep learning processes. LLaMA, an LLM with full research access, holds unique potential for the adaption of the entire LS pipeline. This paper details the process of fine-tuning LLaMA to create LSLlama, which performs comparably to previous LS baseline models LSBert and UniHD.",
}
@inproceedings{wan2021fl,
  title={FL-MSRE: A few-shot learning based approach to multimodal social relation extraction},
  author={Wan, Hai and Zhang, Manrong and Du, Jianfeng and Huang, Ziling and Yang, Yufei and Pan, Jeff Z},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={15},
  pages={13916--13923},
  year={2021}
}
@inproceedings{glue,
  title={Superglue: Learning feature matching with graph neural networks},
  author={Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4938--4947},
  year={2020}
}
@inproceedings{yuan2023joint,
  title={Joint multimodal entity-relation extraction based on edge-enhanced graph alignment network and word-pair relation tagging},
  author={Yuan, Li and Cai, Yi and Wang, Jin and Li, Qing},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11051--11059},
  year={2023}
}
@inproceedings{tjh1,
  title={N-gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding},
  author={Tian, Jinhao and Li, Zuchao and Li, Jiajia and Wang, Ping},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={14},
  pages={15364--15372},
  year={2024}
}
@inproceedings{tjh2,
  title={Fine-grained position helps memorizing more, a novel music compound transformer model with feature interaction fusion},
  author={Li, Zuchao and Gong, Ruhan and Chen, Yineng and Su, Kehua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={4},
  pages={5203--5212},
  year={2023}
}
@misc{deepseekvl,
      title={DeepSeek-VL: Towards Real-World Vision-Language Understanding}, 
      author={Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Yaofeng Sun and Chengqi Deng and Hanwei Xu and Zhenda Xie and Chong Ruan},
      year={2024},
      eprint={2403.05525},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{cogagent,
      title={CogAgent: A Visual Language Model for GUI Agents}, 
      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2312.08914},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models}, 
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{minicpm,
      title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone}, 
      author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and Chen, Qianyu and Zhou, Huarong and Zou, Zhensheng and Zhang, Haoye and Hu, Shengding and Zheng, Zhi and Zhou, Jie and Cai, Jie and Han, Xu and Zeng, Guoyang and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
      journal={arXiv preprint 2408.01800},
      year={2024},
}

@misc{instruct,
      title={LAVIS: A Library for Language-Vision Intelligence}, 
      author={Dongxu Li and Junnan Li and Hung Le and Guangsen Wang and Silvio Savarese and Steven C. H. Hoi},
      year={2022},
      eprint={2209.09019},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models}, 
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{deepseek1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@inproceedings{ZIQI-Eval,
    title = "The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models",
    author = "Li, Jiajia  and
      Yang, Lu  and
      Tang, Mingni  and
      Chenchong, Chenchong  and
      Li, Zuchao  and
      Wang, Ping  and
      Zhao, Hai",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.194/",
    doi = "10.18653/v1/2024.findings-acl.194",
    pages = "3246--3257",
    abstract = "Benchmark plays a pivotal role in assessing the advancements of large language models (LLMs). While numerous benchmarks have been proposed to evaluate LLMs' capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities. To address this gap, we present ZIQI-Eval, a comprehensive and large-scale music benchmark specifically designed to evaluate the music-related capabilities of LLMs.ZIQI-Eval encompasses a wide range of questions, covering 10 major categories and 56 subcategories, resulting in over 14,000 meticulously curated data entries. By leveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to evaluate and analyze LLMs' performance in the domain of music.Results indicate that all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant room for improvement in their musical capabilities.With ZIQI-Eval, we aim to provide a standardized and robust evaluation framework that facilitates a comprehensive assessment of LLMs' music-related abilities. The dataset is available at GitHub and HuggingFace."
}