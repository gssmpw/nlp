\section{PathFinder}
\label{sec:methods}
The multi-agent multi-modal framework proposed in this study includes four agents: 1) Triage Agent ; 2) Navigation Agent ; 3) Description Agent ; and 4) Diagnosis Agent. The details of training data and model architectures are described below. Figure \ref{fig:nav-pipeline-simple} demonstrates how the four agents interact with each other towards the final goal which is diagnosing a WSI.

\subsection{Triage Agent}
The Triage Agent is an image-only transformer-based model tasked with separating class 1 (nevus/mild atypia and moderate atypia/dysplasia) from the rest in the M-Path dataset (Refer to section \ref{data-mpath} for M-Path class definitions). We describe the data preparation, model architecture, and training details below.

\noindent\textbf{Data Generation.} Each whole slide image (WSI) is divided into non-overlapping $512 \times 512$ patches at $10\times$ magnification. Background patches (saturation less than 15) are discarded. If fewer than 150 patches remain, we randomly select additional patches from the WSI, apply the saturation filter again, and include the ones that pass. These additional patches may overlap with existing patches but ensure that each WSI contains sufficient information. All patches are then rearranged based on their spatial coordinates. The patches are embedded using the Quilt-Net image encoder~\cite{ikezogwo2023quilt}, resulting in a feature vector of shape $(N, 768)$ per WSI, where $N$ is the total number of patches for the WSI.

\noindent
\textbf{Model Architecture.} The Triage Agent includes several sequential stages (See Fig \ref{fig:triage} in the Appendix): The feature vector is initially projected from $(N, 768)$ to $(N, dim)$ using a linear layer to align with the model’s embedding dimension $dim$. For compatibility with 2D processing, the vector is reshaped into a square grid through padding to dimensions $H \times H$, where $H$ is the smallest integer satisfying $H \times H \geq N$, with padding achieved by repeating the first $M = H^2 - N$ features. The padded vector is then processed through a transformer block, followed by positional encoding via the Pyramid Position Encoding Generator (PPEG)~\cite{shao2021transmil}, and an additional transformer block, where each transformer block contains a single self-attention layer. Subsequently, multi-scale convolutional layers and a squeeze-and-excitation (SE) block~\cite{hu2018senet} refine the vector, capturing spatial patterns across scales and emphasizing key features. The output is then flattened and transformed back to the embedding dimension. A learnable class token is appended to capture global context, and the modified vector is passed through another transformer block, positional encoding, and a final transformer block. Finally, the class token is pooled and passed through an MLP head to produce the model’s output.

\noindent
\textbf{Training Details.} We used binary cross-entropy loss for the classification task. The embedding dimension $dim$ is set to 512. Training hyperparameters are as follows: a batch size of 1, learning rate of $2 \times 10^{-4}$, weight decay of $1 \times 10^{-5}$, and gradient accumulation over 32 steps. Training is conducted for up to 100 epochs, with early stopping after 30 epochs without improvement to prevent overfitting. Our approach achieved higher F1-score and accuracy compared to other methods that are directly comparable for this task (See Table \ref{tab:triageexperiments} in the Appendix).


\subsection{Navigation Agent}
\label{nav-agent}
The Navigation Agent is designed to mimic a pathologist’s methodical approach to identifying regions of interest (ROIs) in whole slide images (WSIs). Unlike traditional systems that scan the entire WSI in a single, mechanistic sweep, our Navigation Agent adopts a more human-like, iterative process collaborating with the Description Agent. It begins by pinpointing an initial ROI, much as a pathologist would focus on one area at a time. This selected ROI is then relayed to the Description Agent, which provides a natural-language description of the area. Figure \ref{fig:nav-pipeline} illustrates the workflow of the Navigation Agent in the left panel.

In our initial attempt, we designed the Navigation Agent using a multi-modal architecture inspired by LLaVA \cite{liu2023improved}, integrating an image encoder and a large language model (LLM). The image encoder extracted features from a low-resolution version of the WSI, and the LLM processed these features along with previous text descriptions to predict the next ROI. Specifically, the WSI was divided into a grid of patches, and the LLM would output the grid coordinates of the most relevant patch based on both visual and textual inputs. However, this approach faced significant challenges due to the limited size of our training dataset. The model tended to overfit, frequently selecting central patches regardless of the input (see Appendix \ref{supp:llava-nav} for details). This limitation prompted the exploration of more data-efficient methods that could better generalize from limited samples.

To overcome these challenges, we restructured the Navigation Agent to directly generate an importance map over the WSI, conditioned on textual descriptions from previous observations. This approach removes the dependency on the LLM for spatial selection and leverages a feedback mechanism between the image and text modalities. Let $I^{(t)}$ be the input WSI at iteration $t$, with previously selected patches masked out to avoid re-sampling and $D^{(1:t)} = \{D^{(1)}, D^{(2)}, \dots, D^{(t)}\}$ be the set of textual descriptions up to iteration $t$. At each iteration $t$, the Navigation Agent processes the masked WSI $I^{(t)}$ to predict an importance map $M^{(t)}$, indicating the likelihood of each region being the next ROI. The importance map is conditioned on the aggregated textual information from previous descriptions. We define the importance map generation as $M^{(t)} = f_{\text{Nav}} \left( I^{(t)}, E^{(t-1)} \right)$ where, $f_{\text{Nav}}$ is the Navigation Agent's function (implemented as a lightweight U-Net \cite{ronneberger2015u}) that has four layers in both encoder and decoder and is conditined with text embeddings of descriptions, as well as the masked version of the WSI that masks the earlier predicted ROIs, and $E^{(t-1)}$ is the aggregated text embedding up to iteration $t - 1$. $E^{(t-1)}$ is computed by encoding each description $D^{(k)}$ using a pre-trained Text-to-Text-Transfer-Transformer (T5) text encoder \cite{raffel2020exploring} and averaging the embeddings:

\begin{equation}
    E^{(t-1)} = \frac{1}{t-1} \sum_{k=1}^{t-1} \text{T5}_{\text{text}}(D^{(k)})
\end{equation}

At the first iteration ($t = 1$), since there are no prior descriptions, the importance map is generated solely from the unmasked WSI $M^{(1)} = f_{\text{Nav}} \left( I^{(1)} \right)$.From the importance map $M^{(t)}$, we then statistically sample the next patch to analyze. The probability $p_{(i,j)}^{(t)}$ of selecting a location $(i, j)$ is proportional to its importance score:

\begin{equation}
    p_{(i,j)}^{(t)} = \frac{M_{(i,j)}^{(t)}}{\sum\limits_{(i', j')} M_{(i', j')}^{(t)}}
\end{equation}

We then sample the patch coordinates $(i^{*}, j^{*})$ based on this probability distribution:$(i^{*}, j^{*}) \sim p_{(i,j)}^{(t)}$. The selected high-resolution patch corresponding to $(i^{*}, j^{*})$ is sent to the Description Agent, which generates a new textual description $D^{(t)}$. The new description $D^{(t)}$ is encoded and incorporated into the aggregated text embedding $E^{(t)}$:

\begin{equation}
    E^{(t)} = \frac{1}{t} \sum_{k=1}^{t} \text{T5}_{\text{text}}(D^{(k)})
\end{equation}

This updated embedding $E^{(t)}$ is then used to condition the Navigation Agent in the next iteration, enabling the model to refine its importance map $M^{(t+1)}$ based on both the visual information from $I^{(t+1)}$ and the accumulated textual insights. Therefore, we refer to it as the Text-conditioned Visual Navigator.


\input{figures/ablate_fig}

\noindent
\textbf{Training details.} To train the Navigation Agent, we constructed a dataset from M-Path \cite{onega2018accuracy} consisting of WSIs and sequences of textual descriptions for the most important patches. Each training sample includes: The WSI and the corresponding masked versions, the set of descriptions $D^{(1:t)}$ for each iteration generated by Quilt-LLAVA \cite{seyfioglu2024quilt} and the ground truth importance maps derived from pathologist annotations. We minimized the binary cross-entropy loss between the predicted importance maps $M^{(t)}$ and the ground truth maps $\hat{M}^{(t)}$. Finally, to prevent overfitting, we paraphrased each description multiple times while preserving the semantic meaning, augmenting the textual data available for training.



\subsection{Description Agent}

We utilize Quilt-LLaVA \cite{seyfioglu2024quilt}, a multi-modal large language model capable of describing individual histopathology patches, as our Description Agent. While the original Quilt-LLaVA generates highly detailed findings, in this work, we instruction-tuned the model to produce more concise summaries, optimizing for computational efficiency. Using captions from the Quilt-1M dataset \cite{ikezogwo2024quilt}, we prompted GPT-4 to generate a list of findings as concise as possible. This process yielded 102,000 instruction-tuning samples. Figure \ref{fig:descriptionprompt} in the Appendix presents our prompt and a sample data entry generated for tuning the Description Agent. The Quilt-LLaVA 7B model was instruction-tuned for one epoch to obtain the Description Agent.

\subsection{Diagnosis Agent}
\label{trajectories}
The Diagnosis Agent is a language-only model that analyzes all the gathered natural text descriptions produced by the Description Agent over all the patches identified by the Navigation Agent, to analyze natural text descriptions of histopathological findings and classify them into three categories (classes 2, 3, and 4).

\noindent
\textbf{Data Generation.} To train the Diagnosis Agent, we generated diagnostic trajectories—sequences of patch descriptions that simulated how a pathologist examined a whole slide image (WSI). Using our Navigation Agent, we proceeded as follows.

We first obtained a heatmap for a sub-sampled WSI ($512 \times 512$ pixels) using a text-conditioned U-Net model, which highlighted regions of diagnostic significance. The WSI was divided into a $16 \times 16$ grid, creating 256 patches of $32 \times 32$ pixels each. Each patch received an importance score based on the mean intensity of the heatmap over the patch, indicating its diagnostic relevance. These scores were normalized across all patches.

To generate a single trajectory, we iterated the following steps ten times, yielding ten patches per case. At each iteration, a patch was selected using weighted probabilistic sampling based on the normalized importance scores, introducing variability and ensuring different patches were chosen across iterations. The selected patch was then cropped from the high-resolution $10\times$ WSI, and a description was generated by the Description Agent. Selected patches were masked on the WSI to prevent reselection, and all descriptions generated thus far were combined into a single text input for the next iteration. The generation of a single trajectory is presented in the right panel of Figure \ref{fig:nav-pipeline}.

For each WSI in the training and validation sets, we generated five ($n=5$) different trajectories, each containing ten patch descriptions, to capture various examination patterns. For the test set, we extracted additional trajectories ($n=20$) to assess the effect of trajectory number on diagnosis results. To introduce further variability, we used a LLaMA 3.1 Instruct model~\cite{dubey2024llama} at each iteration to rephrase the text descriptions. This approach effectively simulated the variability among pathologists, who might examine a single case using different patterns while seeking diagnostically relevant regions.





\noindent
\textbf{Training Details.} The Diagnosis Agent consists of a large language model (LLM) with a classification head on top. The classification head maps the LLM’s output (vocabulary size) to the number of classes, producing the final classification probabilities using a single linear layer.

We expand the training set to enhance diversity and robustness by resampling to create $20{,}000$ cases, resulting in $100{,}000$ trajectories for training. Each trajectory consists of a randomly selected number of descriptions (between five and ten), and we shuffle the sequence of descriptions within each trajectory to prevent over-fitting to any specific order. Each trajectory is formatted as a prompt to the LLM:

\textit{“The image descriptions below are extracted from different patches from the same whole slide image (WSI); please tell me which class the image belongs to: {descriptions}”},
\noindent
where \textit{{descriptions}} is the list of selected descriptions.

We fine-tune the LLM using LoRA (Low-Rank Adaptation)~\cite{hu2021loralowrankadaptationlarge} with the scaling factor $\alpha=8$, dropout rate $0.1$, and rank parameter $r=8$ in the LoRA layers. The model is trained using cross-entropy loss, with a learning rate of $5 \times 10^{-5}$, weight decay $0.001$, and batch size $16$. Due to resource constraints and the limited size of the dataset, we selected GPT-2~\cite{radford2019language} as the base pre-trained LLM.




