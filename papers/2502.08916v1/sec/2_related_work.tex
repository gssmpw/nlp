\section{Related Work}
\textbf{Multi-modal Histopathology Models.} 
There have been a series of studies in histopathology that leverage WSI-level and pacth-level images to train unimodal classifiers based on multiple instance modeling leveraging pretrained feature extractors \cite{shao2021transmil, scatnet, hatnet}. More recently unimodal foundational models trained on varying self-supervised objectives have achieved significant improvements on performance downstream \cite{xu2024whole, virchow, ikezogwo2022multi, uni}. With the introduction of large-scale multi-modal datasets in histopathology, we have seen significant advancements, with the emergence of large language models and vision-language models for histopathology. For instance, studies like Quilt-1M \cite{ikezogwo2023quilt} and PathGen-1.6M \cite{sun2024pathgen} curate large histopathology image-text paired dataset and train CLIP-based models to learn joint vision-language representations, significantly enhancing clinical histology downstream tasks on patch-level. On the WSI-level, PathAlign \cite{ahmed2024pathalign} aligns diagnostic texts from pathology reports with corresponding WSIs, facilitating applications such as automatic report generation and case/patient-level visual question answering, moving towards a more clinically integrated and holistic diagnostic process. While many other studies like Quilt-LLaVA \cite{seyfioglu2024quilt}, SlideChat \cite{chen2024slidechat}, and PathChat \cite{pathchat} train histopathology Multi-modal Large Language models (MLLM) and improve on diagnostic reasoning tasks, none of these models effectively automatically navigate the giga-pixel scale WSIs towards a diagnosis.

\noindent\textbf{The role of Navigation in Histopathology Diagnosis.}
Computational pathology studies have tried to capture and analyze the navigation patterns of pathologists when reviewing digital slide images \cite{roa2010experimental, mercan2018characterizing, molin2015slide, ghezloo2022analysis} specifically characterizing mouse patterns, zooming in/out, and panning the field of view (FOV) to piece out morphological clues towards a diagnosis. Often, these studies juxtapose the navigation patterns of junior and senior-level pathologists. NaviPath \cite{gu2023augmenting}, presents a human-AI collaborative navigation system designed to seamlessly integrate into pathologists' workflows, considering the specific domain knowledge and navigation strategies required for effective examination of pathology scans. 

\noindent\textbf{Multi-agent Systems.} The concept of multi-agent systems has gained traction in AI research, particularly for tasks requiring dynamic behavior and contextual understanding. Recent research has demonstrated the potential of large foundation models in creating interactive agent-based AI systems including interactions between robots, environments, and humans in the field of robotics \cite{durante2024interactive, han2024llm,wu2023autogen}. These systems can perform complex tasks by leveraging the strengths of individual agents utilizing collaboration and coordination. The potential of multi-agent systems in handling real-world scenarios has been demonstrated in recent studies including but not limited to role-playing \cite{li2303camel}, reasoning \cite{du2023improving}, gaming \cite{huang2024far} and software engineering \cite{he2024llm}. In the medical domain some studies have explored role-playing providers (clinicians) treating patients and accumulating proficiency with increasing interactions \cite{doctorsimul, medagentsbench}. These studies are centered around multiple providers; however, in medical image analysis, multi-agent systems can simulate the collaborative nature of different sub-tasks within the human-expert decision-making processes, including region navigation, understanding and holistic diagnosis.
