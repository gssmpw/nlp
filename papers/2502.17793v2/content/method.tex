\section{\textsc{SYNTHIA}: Novel Concept Design with Affordance Composition}
Our ultimate goal is to utilize T2I models in designing novel concepts that are both visually novel and functionally coherent. Specifically, we take the desired affordances as text inputs, the T2I models should generate an image that depicts the novel concept design.
%\kw{I would suggest first specify what is the input/output of the novel concept design. Also, would be good to also specify what is the goal of training}
To achieve this, we (1) construct a training recipe that explicitly embeds hierarchical relations on visual concepts, parts, and corresponding affordances, and (2) fine-tune T2I models with curriculum-based optimization.
% The following sections detail each stage.

\subsection{Affordance Composition Curriculum}
\label{sec:data_gen}
The primary challenge in the novel concept generation of existing T2I models is the lack of structured functional grounding. These models often struggle to design visually novel yet functionally coherent concepts while maintaining intended functionalities. For example, when combining affordances like \texttt{Brew} and \texttt{Cut}, they may prioritize aesthetics over functionality, omitting parts or objects relevant to \texttt{Brew} (Fig~\ref{fig:distant}). To address this, we construct a structured training recipe in two key steps: (1) building a hierarchical concept ontology, and (2) designing an affordance sampling strategy for curriculum-based training. This improves the model's composition ability by learning the connection between concepts and affordances.
%\kw{Would be good to elaborate how these steps can resolve the limitation.}

% \vspace{-0.05in}
\paragraph{Hierarchical Concept Ontology.}
To provide a structured basis for novel concept synthesis with functional coherence, we define a hierarchical concept ontology that decomposes visual concepts into constituent parts and their affordances, capturing concept-affordance associations (Fig~\ref{fig:ontology}). This ontology allows T2I models to retrieve relevant parts based on affordances, enabling generation to be well-grounded on the functionality of concepts rather than superficial visual feature combinations. Formally, we define the ontology as a four-level hierarchy $\mathcal{O} = (\mathcal{S}, \mathcal{C}, \mathcal{P}, \mathcal{A})$. Superordinate $\mathcal{S}$ denotes the highest-level categories, such as \texttt{furniture}, followed by Concept $\mathcal{C}$, which is the set of visual concepts. Each $c \in \mathcal{C}$ belongs to a superordinate category $s \in \mathcal{S}$, e.g., $\mathcal{S}_\texttt{table} = \texttt{furniture}$, and decomposes into its parts $\mathcal{P}$ that serve specific functions in an object design, e.g., $\mathcal{P}_{\texttt{table}} = \{\texttt{leg}, \texttt{drawer}\}$. The Affordance $\mathcal{A}$ describes functionalities of concepts and parts. Both a concept $c \in C$ and its part $p \in P$ are linked to affordances set $\mathcal{A}_c = \{a_1, \cdots, a_n\} \in \mathcal{A}$, e.g, $\mathcal{A}_{\texttt{table}} = \{\texttt{write}, \texttt{organize}\}$, and $\mathcal{A}_p = \{p_1, \cdots, p_n\}$, e.g., $\mathcal{A}_{\texttt{leg}} = \{\texttt{support}\}$. Our ontology spans 30 superordinates, 590 concepts, 1172 parts, and 686 affordances, explicitly providing a structured representation of how affordance is associated with fine-grained parts for functionally grounded novel concept synthesis.
% , rather than relying on implicit or stylistic cues alone.
% \vspace{-0.1in}

\paragraph{Affordance Sampling.}
\label{sec:affor-sampling}
Given our ontology, we can utilize it to create fine-tuning data to improve the functional coherence of the generated novel concept by T2I models. A naive approach to obtaining training data would be to exhaustively pair all possible affordances. 
%\kw{related to my previous comments, it's not clear what is the task and what is the training data you're constructing.}
However, this would yield 235K affordance pairs, which is computationally expensive. Moreover, random combination risks generating redundant concepts (e.g., \texttt{Heat} and \texttt{Cook} examples in Fig~\ref{fig:close}) or functionally incoherent objects. To achieve sufficiently different affordance pairs that enable novel concept synthesis while still being functionally integrable, we introduce a distance-based affordance sampling strategy that selects meaningful, disparate affordance pairs based on ontology-derived distances.

We define a concept distance $D_{C}(c_i, c_j)$ between two concepts $c_i, c_j \in \mathcal{C}$ by incorporating functional relatedness at the affordance level and semantic similarity at the concept level. We compute functional relatedness using Jaccard similarity $J(X, Y) = \frac{|X \cap Y|}{|X \cup Y|}$ between their affordance sets while quantifying the semantic similarity $\text{Sim}$ via measuring embedding similarity using the BERT~\citep{devlin2019bertpretrainingdeepbidirectional} model as follows:
% . The concept distance $D_{\mathcal{C}}(c_i, c_j)$ quantifies the overlap of affordances associated with different concepts, approximating the similarity between two concepts as follows: 
\begin{align}
    \label{eq:dist_concept}
     D_{\mathcal{C}} (c_i, c_j) &= \alpha*\{J(\mathcal{A}_{c_i}, \mathcal{A}_{c_j}) + J(\mathcal{A}_{P_{c_i}}, \mathcal{A}_{P_{c_j}})\}, \notag \\
                        % &\quad+ \beta* J(\mathcal{A}_{P_{c_i}}, \mathcal{A}_{P_{c_j}}) \notag \\
                        &\quad+ \beta*\text{Sim}(\text{BERT}(c_i), \text{BERT}(c_j)),  
\end{align}
where $\alpha, \beta$ are adjustable hyperparameters that balance between functional relatedness based on affordances, and semantic relevance of concepts, respectively.
Since we prioritize affordance-level similarity over concept-level similarity during training, we set $\alpha=0.7$ and $\beta=0.3$.
% \kw{How to set these $\alpha$, $\beta$}
Two semantically similar concepts sharing more affordances have closer distances, such as \texttt{sofa} and \texttt{chair}, while those that have different affordances and semantic differences, such as \texttt{car} and \texttt{vacuum cleaner} have more distant distances.

% \heng{Notations need to be further cleaned up. you are using A to refer to two different things. you don't need to use p and q, instead you can just use i and j to index a pair}

We further obtain affordance distance $D_{\mathcal{A}} (a_i, a_j)$ between two affordances $a_i, a_j \in \mathcal{A}$ by averaging pairwise concept distances $D_{\mathcal{C}}(\cdot, \cdot)$ between associated concepts:
\begin{sizeddisplay}{\normalsize}
\begin{align}
     \label{eq:dist_affordance}
     D_{\mathcal{A}} (a_i, a_j) &= \frac{1}{|C_{a_i}| \cdot |C_{a_j}|} \sum_{c_p \in C_{a_i}} \sum_{c_q \in C_{a_j}} D_{C}(c_p, c_q),
\end{align}
\end{sizeddisplay}
where $C_{a_i}$ and $C_{a_j}$ are the sets of concepts associated with affordances $a_i$ and $a_j$, respectively. The resulting $D_{\mathcal{A}}(\cdot, \cdot)$ is distributed from 0.1 to 1.0.
% detailed in Appendix~\ref{fig:afford_dist}. 

Based on our distance metric, close affordance pairs associated with similar concepts, e.g., $\{\texttt{sit}, \texttt{rest}\}$ from $\{\texttt{sofa}, \texttt{chair}\}$, support learning basic affordance-concept associations, which can be easily merged into existing concepts. In contrast, distant affordance pairs derived from sufficiently distant concepts, e.g., $\{\texttt{drive}, \texttt{vacuum}\}$ from $\{\texttt{car}, \texttt{vacuum cleaner}\}$, enforce greater functional coherence by requiring meaningful part-affordance integration, which is more complex than a trivial combination. 
% \vspace{-0.05in}
\paragraph{Curriculum Construction.}
In novel concept generation, existing T2I models struggle with (1) concept-affordance associations and (2) the composition of functionally coherent affordances into a single concept. To address these challenges with limited data, we propose a three-stage curriculum that progressively increases affordance pair distances. In the earliest stage, we utilize close affordance pairs to reinforce fundamental knowledge of the concept-affordance associations. The second stage employs the affordance pairs from the mid-range distances to encourage the model to learn the fine-grained compositional structures while maintaining prior knowledge. In the last stage, we only introduce distant affordance pairs to challenge the model to synthesize novel, functionally coherent concepts by applying the previously learned basics on the fine-grained parts and affordance relations.

We sample 600 affordance pairs uniformly across the full distance spectrum and categorize them into three groups. For training images used as pseudo novel concepts, we generate 10 images per pair using DALL-E~\cite{ramesh2021zeroshottexttoimagegeneration} with GPT-4~\citep{openai2024gpt4ocard} generated captions that describe different novel designs integrating the specified affordances. We then filter images using CLIP similarity scores and manually select the top three. This curriculum-based training enables T2I models to learn basic concept-affordance associations while fusing affordances into coherent and functionally meaningful designs. Thus, the T2I models can successfully produce novel concepts that are visually distinctive and functionally coherent. 
% To effectively address this with a limited amount of data, we build a curriculum with the increasing difficulty of affordance pairs in terms of their distance. The close affordance pairs allow T2I models to learn basic knowledge of visual concepts and their corresponding affordances while distant affordance pairs force them to coherently merge parts relevant to affordances. For a training curriculum, we uniformly sample 600 affordance pairs among 235K possible candidates covering the full distance spectrum and divide them into three stages (200 affordance pairs per stage). For each pair, we generate three image-text samples, resulting in 1800 samples, where we create image captions that describe the given affordances but differ from existing concepts using GPT-4o~\citep{openai2024gpt4ocard}. We then generate corresponding images aligned with the image caption using DALL-E~\cite{}. To maintain high-quality data, we apply CLIP~\cite{radford2021learningtransferablevisualmodels}-based and manual filtering. Further details are in Appendix~\ref{}. \heng{the last sentence is difficult to understand. rewrite to make it clear}

% stage1: association between concept / affordance
% stage2: composition
% stage3: distant affordance

\subsection{Contrastive Fine-tuning with Curriculum Learning}
\label{sec:ft}
The goal of fine-tuning T2I models is to enable them to fuse multiple affordances into a single, functionally coherent concept while ensuring visual novelty. With our curriculum, we propose a curriculum learning strategy to fine-tune the diffusion-based T2I models. From a data-driven perspective, training with affordance pairs and DALL-E-generated pseudo-novel concepts helps the model design novel concepts given specified affordances.

To further enhance visual novelty, we incorporate contrastive learning objectives, ensuring that generated images not only reflect desired affordances but also differ from existing concepts associated with them. Specifically, we define two sets of constraints derived from our ontology to guide the model: (1) \textit{Positive Constraints} specify the target affordances that must be included in the novel concepts, shaping their functional structure; (2) \textit{Negative Constraints} consist of all existing concepts from our ontology that already have the target affordances in the positive constraints. These act as references to avoid. By adhering to these constraints, the model generates concepts that successfully integrate the specified affordances while maintaining a high degree of novelty. 
% This fosters the design of innovative and functionally grounded visual concepts.
% Suppose we have sampled a set of image-text pairs that contain novel concepts based on our affordance sampling strategy, 
% \heng{not sure what you mean by data generation pipeline. how do you generate ground truth?} 
% we propose a novel curriculum learning-based framework to train a diffusion model-based T2I model. Our goal is to generate novel, semantically accurate, and highly novel
%\heng{do you mean highly novel instead of highly diverse?}
% images that satisfy the desired affordances. In addition, we incorporate a contrastive learning objective such that the generated image should contain the desired parts/affordances as well as be distant from any existing concepts in our ontology.
%\heng{does the strategy work? did you do ablation test by turning it off?}

% \paragraph{Training Framework.}
% The goal of fine-tuning T2I models is to enforce the model to learn how to fuse multiple affordances into a single functional concept, which is also different from the existing concept. To achieve this goal, we introduce two sets of constraints, contrastively guiding T2I models to generate novel outputs as follows:
% \begin{itemize}
%     \item \textbf{Positive Constraints} specify the desired affordances that must be included in the novel concept. These affordances define the functional structure of the output.
%     \item \textbf{Negative Constraints} consist of all existing concepts from the ontology that already have the desired affordances in the positive constraints. They act as references for what the generated images should avoid.
% \end{itemize}



% \paragraph{Curriculum Learning.}
% We incorporate a curriculum learning strategy that gradually increases the complexity of the learning process, which enables the diffusion model to effectively fuse multiple affordances while maintaining novelty with a few data. 
%First, we collect the images for each existing concept from the ontology, and then take these images and their corresponding affordances as training data to fine-tune the diffusion-based T2I model. This step improves the compositional ability of the model by learning the fine-grained parts-level affordances of each existing concept. Then we continue to train the framework with the novel images obtained from our data generation pipeline in Sec~\ref{sec:data_gen}. This gradual progress is controlled along two dimensions of difficulty:
% \begin{itemize}
%     \item \textbf{Number of concepts.} We incrementally increase the number of concepts included in the negative constraints, where the model learns to generate novel concepts and maintain dissimilarity with an expanding existing conceptual space. 
%     \item \textbf{Affordance distance.} In addition to expanding the negative constraints, we also increase the affordance distance for positive constraints, which requires the model to synthesize images with affordances that are increasingly distinct and complex. 
% \end{itemize}
% We utilize the rich structural information from the hierarchical concept ontology that we have constructed as it includes a comprehensive network of concepts, their parts, and affordances. This allows us to compute the affordance distances based on their relationships in the ontology and sample the existing concepts for negative constraints. We increase the affordance distance for positive constraints, which requires the model to synthesize images with affordances that are increasingly distinct and complex. In this way, the curriculum learning approach helps the model handle complex synthesis tasks while maintaining novelty in the generated images.
% \vspace{-0.05in}
\paragraph{Training Objectives.}
The training objective of fine-tuning is formulated using a triplet loss, which can balance two components to achieve the desired outcomes. The first component aims to minimize the similarity loss between the generated image and the pseudo-novel image created during curriculum construction, ensuring visual novelty. To reduce the overfitting problem, we also sample multiple pseudo-novel images that describe different concepts. Given the set of affordances $\mathcal{A}_{pos} = \{a_1, \cdots, a_n\}$ in the positive constraints, together with a sampled image $I_i^{+}$ from the pseudo-novel images from DALL-E, the positive loss is defined as follows:
\begin{equation}
    \mathcal{L}_{pos}(\theta_{t}) = \| I_{i}^{+} - \hat{I}_{i} \|^2_2 + \mathbb{E}_{ \epsilon, t} \left[ \|\epsilon - \epsilon_\theta(t)\|^2_2 \right],
\end{equation}
where $\theta_{t}$ is T2I model parameters, $\hat{I}_{i}$ denotes the generated image, $\epsilon$ is Gaussian random noise. We employ noise prediction loss, where the model takes the latent embedding of $I_{i}^{+}$ as input and predicts the noise as $\epsilon_\theta(t)$, preventing catastrophic forgetting of learned training distribution.
% These images are clustered, and their average is taken as the representative ground truth. The loss is then computed as the distance between the generated image and this averaged ground-truth cluster.

The second component of the triplet loss maximizes the similarity loss between the generated image and a randomly sampled existing concept image $I_{i}^{-}$ that contains partial affordances from the positive constraints as follows:
\begin{equation}
    \mathcal{L}_{neg}(\theta_{t}) = \| I_{i}^{-} - \hat{I}_{i} \|^2_2
\end{equation}
In this way, the model learns to avoid generating existing concept images and increase its novelty.
% For each concept in the negative constraints, we sample a set of representative images for each existing concept. 
% Then, our objective is to maximize the average distance between the generated image and the existing concept image. 

Our overall triplet loss is defined as follows:
\begin{equation}
    \mathcal{L}(\theta_{t}) = \mathcal{L}_{pos}(\theta_{t}) - \gamma * \mathcal{L}_{neg}(\theta_{t}),
\end{equation}
where $\gamma$ is an adjustable hyperparameter.
By balancing two losses, our framework ensures that the generated images align with the desired affordances while remaining distinct from existing concepts.

\subsection{Novel Concept Generation during Inference}
\label{sec:inference}
After fine-tuning the diffusion-based T2I models, our approach requires only the desired affordances as positive constraints during inference time, eliminating the need for manually collecting existing concepts as negative constraints. This efficiency gain stems from incorporating both positive and negative constraints--derived from our hierarchical concept ontology--into the training objective. By embedding these constraints during training, the model learns concept-affordance associations and improves its ability to compose parts associated with desired affordances into a novel design. Therefore, the model can produce novel, structured designs without redundant textual prompting.




