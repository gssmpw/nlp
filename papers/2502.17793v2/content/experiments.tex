\section{Experiments}
\label{sec:experiment}
% \input{table/table1}

% \heng{need a comprehensive table to show stats about ontology and data annotated}
% \heng{show examples on your method output vs. baselines. what are the remaining challenges? do some error analysis}

\input{table/table2}

% \input{figure/our_rel_eval}

\subsection{Experimental Setup}

\paragraph{Datasets.}
To train T2I models with our approach, we construct a dataset from two types of resources (more details in Appendix \ref{app:dataset}): (1) \textit{Existing Concept Images}: For each existing concept in our ontology, we collect 60 images from external platforms including Google Images and iStock. To ensure that images are object-centric and aligned with the concept, we filter out low-quality images using CLIP model~\citep{radford2021learningtransferablevisualmodels}. (2) \textit{Generated Novel Concept Images}: With our affordance sampling, we uniformly sample 600 affordance pairs among 235K possible pairs for fine-tuning. For the test dataset, we select 500 affordance pairs among the ones not used for fine-tuning.
% \vspace{-0.05in}
\paragraph{Foundation Models}
We adopt Kandinsky3.0 \citep{vladimir-etal-2024-kandinsky} as the T2I backbone model; it generates images based on a given text prompt, with an optional negative text prompt to refine outputs. During fine-tuning, we incorporate the desired affordances as positive inputs, while using the existing concepts from the ontology as negative constraints. During the inference, we provide only the text prompts with desired affordances, ``\textit{a new design that has functions of \{desired affordances\}.}''. Training details are provided in Appendix \ref{app:hyperparam}

\input{figure/rel_eval}
% \vspace{-0.05in}
\paragraph{Baselines Methods}
We compare our proposed method against three baseline methods, which are Stable Diffusion~\cite{esser2024scalingrectifiedflowtransformers}, Kandinsky~\cite{arkhipkin2023kandinsky}, and ConceptLab~\cite{richardson2024conceptlab}. While Stable Diffusion and Kandinsky are general T2I models, ConceptLab optimizes generation over diffusion before creative concept design. For a fair comparison, we fine-tune ConceptLab using the same training data as our method. In contrast, our framework directly fine-tuned the diffusion model, integrating the hierarchical visual ontology to enforce the design of a single, coherent concept that achieves multiple affordances. Details on the baselines can be found in Appendix~\ref{app:baseline}.

%\kw{Do these three baselines also trained on the data you collect? One concern of this comparison is that the training data are generated from DALLE, fine-tuning a model on that would like doing distillation on DALLE generation. It is fine as we can sell this paper as proposing this training pipeline, but we need to make it clear the condition of comparisons. Btw, I wonder can we also compare with DALLE? }

\subsection{Evaluation Metrics}
\label{sec:eval}
\paragraph{Automatic Evaluation.}
To automatically evaluate the performance of our proposed method, we design four novel metrics to assess the quality of the generated data:
% \vspace{-0.05in}
\begin{itemize}
    \item \textbf{Faithfulness:} This metric evaluates how well the generated object aligns with instructions, focusing on its intended affordances and whether the image effectively conveys the object's purpose.
    \vspace{-0.05in}
    \item \textbf{Novelty} assesses the originality and creativity of the generated design, emphasizing uniqueness and unconventional concepts that surprise or intrigue users.
    % \vspace{-0.02in}
    \item \textbf{Practicality} evaluates the real-world applicability of the design. It examines usability, alignment with human preferences, and feasibility for production.
    % \vspace{-0.02in}
    \item \textbf{Coherence} evaluates whether the generated image is object-centric, depicting a single clear and functional object without unintended elements. It examines whether multiple affordances are fused into a unified concept rather than shown as separate objects.
\end{itemize}

For all four metrics, we use absolute scores ranging from 1 to 5, with higher scores indicating better quality. However, since the scores for these metrics may be influenced by subjective interpretation, we also include a relative evaluation. Specifically, we present each generated image with its corresponding DALL-E generated image, and ask the automatic evaluator to compare and determine which is superior or if they are equally strong. This relative comparison ensures a more fair evaluation and reduces potential biases.
We use GPT-4o \citep{openai2024gpt4ocard} as the evaluation model to assess the metrics. The detailed evaluation prompts used are provided in Appendix \ref{appendix:eval_prompt}.
% \vspace{-0.05in}
\paragraph{Human Evaluation.}
To assess the quality of the generated concepts beyond automated evaluations, we conduct a human evaluation with 36 non-design expert annotators. Recruited from the university across diverse majors, they are provided with a detailed rubric using the same metrics and a 1-5 scale as the automated evaluations. We randomly sample 10 affordance pairs for four models, with each sample independently evaluated. This allows direct comparisons between human and automated scores, capturing nuanced aspects of evaluation quality. Details of the evaluation process are documented in Appendix~\ref{app:humanevaluation} to ensure transparency.


\input{figure/learning_curve}
\input{figure/figure3}


\subsection{Results and Analysis}
\paragraph{Automatic Evaluation}
In Table \ref{tbl:abs_results}, we compare \textsc{Synthia} against three existing T2I models using our evaluation metrics. For a fair comparison, we randomly sample 500 test pairs that are not used for training.
From the results in Table \ref{tbl:abs_results}, we observe that the Stable Diffusion model always maintains a high practicality but lower novelty. This aligns with our observation (Fig~\ref{fig:examples}) that it tends to generate existing concepts rather than novel concepts. In addition, when it cannot generate an existing concept that satisfies all affordances in the text prompt, the Stable Diffusion model will generate multiple objects in an image without any fusion. This is also reflected in the low coherence scores for both concept and affordance levels. For the other baseline methods, Kandinsky-3 and ConceptLab show an increase in terms of novelty and coherence. However, they suffer from a reduction in practicality. Compared to all the models, our method \textsc{Synthia} achieves the best faithfulness, novelty, and coherence scores while maintaining high performance in practicality. These results reflect that finetuning with the curriculum strategy can successfully follow the text instruction, fuse various affordances, and generate novel concepts.
% \vspace{-0.05in}
\paragraph{Human Evaluation}
To assess the consistency of human evaluations, we computed inter-annotator agreement (IAA) between two independent raters, where ratings were considered in agreement if their absolute difference was $\leq 1$. IAA across all images was 67.5\% with Cohen's Kappa of 22.3\%, where Novelty achieved the highest agreement at 70.9\%, followed by Faithfulness (68.5\%), Practicality (66.4\%), and Coherence (64.1\%). Additionally, the agreement between aggregated human ratings and automatic evaluations reached 91.25\%, indicating that human annotators achieve a reasonable level of consistency while automatic evaluations closely align with human judgments. The human evaluation results consistently demonstrate that our model generates functionally coherent and visually novel concepts with outperforming scores on faithfulness, novelty, practicality, and coherence (Table~\ref{tbl:abs_results}). More results are included in Figure \ref{fig:human_eval_results}.

\subsection{Ablation Studies}
\paragraph{The Size of Fine-tuning Training Data}
In our experiment, We fine-tune the diffusion model using 600 affordance pairs as the training data. To investigate the impact of the training data size, we compare performance across different scales: training with 200, 400, 600, and 800 affordance pairs and using automatic evaluation. As shown in Figure \ref{fig:num_of_data}, we find that the performance improves with larger datasets, and reaches the optimal point at 600. Across all four training sizes, our model consistently outperforms baseline methods.
% This may be because a higher number of training data tends to cause overfitting and catastrophic forgetting problems. \heng{the previous sentence is weak. You should put hypothetical explanation in a paper. dig deeper and find the precise reasons} 
% Overall, with all four sizes of the training data, our model significantly outperforms the baseline methods.
% \vspace{-0.05in}
\paragraph{Number of Positive Affordances}
To evaluate the impact of the number of positive affordances in the input prompt, we also conduct experiments using 3 and 4 positive affordances and compare the performance of all methods. As shown in Table \ref{results_automatic_triple} and \ref{results_automatic_four}, while a slight performance drop occurs across all models as the number of affordances increases, our method consistently maintains high novelty and coherence, showing the generalization ability of \textsc{Synthia} to integrate multiple affordances.
% \vspace{-0.05in}
\paragraph{Effectiveness of Affordance Sampling}
To examine the impact of affordance pair distance on novelty, we select $100$ pairs with the lowest and $100$ pairs with the highest distance scores from the test set. The automatic novelty scores for each group, shown in Table \ref{results_automatic_novelty_sampling}, demonstrate that all three three baseline methods achieve relatively low novelty scores for close affordance pairs, which indicates a tendency to generate existing concepts rather than novel designs. In contrast, our method always exhibits high novelty across various distances and outperforms the baseline models.
% \vspace{-0.05in}
\paragraph{Effectiveness of Curriculum Learning}
% \heng{can you show learnign curve? curriculum learning is supposed to speed up learning. also the motivation of curriculum learning is still not clear. you mentioned data scarcity. wny organize learning materials in certain way can solve data scacity problem?}
In our framework, we incorporate a curriculum learning (CL) strategy by gradually increasing the difficulty of the training during the fine-tuning process. To examine the importance of this component, we also compare the performance with and without curriculum learning (Fig~\ref{fig:learning_curve}). Specifically, we train the diffusion model by randomly shuffling the training data and computing the absolute automatic evaluation results. As shown in Table \ref{results_automatic_no_curriculum}, without curriculum learning, we observe a performance drop and results demonstrate the effectiveness of the CL.

% # of training pairs
% 
