
\section{Introduction}


% existing works can be categorized into 3 parts
% [1] AI-Assisted Inspiration
% [2] Compositionality of T2I models
% [3] Novel Concept Generation 
% Limitation:
% [3] => Previous studies primarily focus on aesthetic or stylistic innovation, often disregarding the nuanced functional requirements that make designs practically viable.
% + Existing generative AI models lack the structured understanding of the relationships between concepts, parts, and affordances required to generate such multi-functional designs. => give generated examples obtained from Xiaomeng => propose ontology as a data-driven approach that can explicitly encode the structured relationship into the generative AI models


Imagine a coffee machine with wheels that brews a morning coffee and delivers it to your bed every morning. This example illustrates a novel concept that is atypical and dissimilar to everyday concepts we regularly encounter in our lives. Novel concept synthesis requires an effective fusion of disparate concepts (e.g., \texttt{coffee machine}, \texttt{trolley}), akin to how humans blend ideas across cognitive domains to generate creative innovations~\cite{fauconnier2002wwt, Han2018THECD}. \looseness=-1
% \kw{I would move the following sentences "T2I models ... visualized concepts." to related work. It's not relevant to the discussion of "novel concept" here.}
% Text-to-Image (T2I) models such as DALL-E~\citep{dalle} and Stable Diffusion~\citep{stabled} have ignited a new era of creative possibilities, transforming how artists and designers project their ideas into tangible, visualized concepts \citep{wang2024diffusion, boutin2023diffusion, vinker2023concept, richardson2024conceptlab, rahman2024visual, kwon2024concept, wang2024divide}. 

\input{figure/figure1_all}

Existing studies on conceptual design using T2I models have enabled rapid ideation of novel visual concepts \citep{cai2023designaid, ma2023conceptual, wang2024inspired, lin2025inkspire} by identifying user challenges such as interpreting abstract concepts in language to help visualize a novel design concept~\citep{lin2025inkspire}, or using large language models (LLMs) to bootstrap initial ideation in texts \citep{cai2023designaid, zhu2023generative}. However, they often naively feed LLM-generated textual prompts into T2I models, relying on simple key phrases or semantic variations of concept description~\citep{cai2023designaid, wang2024inspired}. While existing works show that T2I models can generate images that seem to correctly reflect complex human-formulated textual descriptions (e.g., \textit{``beautiful rendering of neon lights in futuristic cyberpunk city''}), they do not focus on whether a model can synthesize a novel concept when given a set of affordances (e.g., \texttt{brew}, \texttt{deliver}) as input, while ensuring these affordances are preserved.

% \violet{the transition does not feel smooth to me. I think we'll benefit from giving an example of the input and output of this T2I for novel concept design, and where the inputs come from. E.g., did humans/users specify the combination, or LLMs figure them out? My feeling is your approach differ from existing ones on how to source this input. but if not, more clarity need to be provided to better distinguish your work and prior work. Also, a teaser figure (a motivation figure that you put in the first page. A simplified version of your figure 1 with comparisons to prior works would work well) to show this difference will probably help.}

An important aspect lacking in existing approaches to concept synthesis is their focus on pixel-based control, overlooking the structural and functional roles embedded in design. Many real-world concepts are naturally ``decomposable'' into parts, where each part signals a specific functionality. To address this, we propose \textsc{Synthia}, a framework for Concept \textbf{Synth}es\textbf{i}s with \textbf{A}ffordance composition that generates functionally coherent and visually novel concepts given a set of desired affordances. Unlike prior works relying on complex descriptive text to generate stylistic variations or aesthetic features~\cite{richardson2024conceptlab, vinker2023concept}, \textsc{Synthia} leverages affordances--defined as ``the functionality offered by an object or its parts''--as a structural guide for novel concept synthesis. By aligning textual descriptions with affordances as control signals, our models implicitly learn to ``decompose and reassemble'' functional parts, ensuring that, for instance, a hybrid of a \texttt{coffee machine} and a \texttt{trolley} not only appears novel but also retains its brewing and mobility functions, achieving \textit{functional coherence}. 


\input{figure/concept_fig}
To facilitate structured affordance composition, we construct a hierarchical concept ontology that decomposes visual concepts (e.g., \texttt{Furniture}-\texttt{Sofa}) into their constituent parts (e.g., \texttt{leg}, \texttt{cushion}) and associated affordances (e.g., \texttt{support}, \texttt{rest}). It provides a structured representation of concept-affordance associations, serving as the foundation for generating functionally meaningful designs. Inspired by the theory of combinational creativity in humans~\cite{Han2018THECD}, which suggests novel concepts emerge from disparate ideas, we propose an affordance sampling mechanism that strategically selects affordances associated with sufficiently different concepts using our novel similarity-based metric (\S\ref{sec:data_gen}). This ensures that generated designs integrate novel functionalities, avoiding trivial combinations, whereas random sampling yields similar affordances (e.g., \texttt{cook}, \texttt{heat}) that result in redundant outputs (Fig~\ref{fig:close}). \looseness=-1
% An important aspect lacking in existing approaches to concept synthesis is that they tend to focus on pixel-based control, overlooking the structural and functional roles embedded within a design. In our view, many real-world concepts are naturally ``decomposable'' into parts, where each part signals a specific functionality. By aligning textual descriptions with these affordances, our approach leverages instructions based on affordances as an additioanl control signal to generate new concepts. This strategy encourages the model to implicitly learn to ``decompose and reassemble'' functional parts, ensuring that, e.g., a hybrid of a \texttt{coffee machine} and a \texttt{trolley}, not only looks novel but also retains the brewing and mobility functions of its predecessors, or the \textit{functional coherence}. 
% \heng{show this example in one of the figures and point to it}
% \violet{is it true that prior work will have input prompt of "coffee machine and trolley", and yours will use something like "coffee machine for brewing and trolley for transportation"? This part is still pretty unclear to me.} \jeongh{this is just an example we made up in this paper; our model will use receive as input a set of affordances only: (brew, deliver)}

% \heng{another important angle is missing in motivation - it's more about visual-text alignment. existing work takes control on pixel level. but you noticed that many concepts are decomposable into parts and some of these parts indicate affordance. We can call these parts with affordance as building blocks/functional modules. So we can send instructions based on functional modules as control to generate new concepts. We train the model so it implicitly learn 'decompose and reassemble' by associating affordance to parts. It will be good to visualize attention etc. to confirm the method does learn functional module level association. So the results can be more explanable.}
% \kw{Agree. We need to make the logic flow and motivation clear in the 2nd paragraph. I would suggest following the example and discussion in the first paragraph, you can first define functional coherence and argue why it is important. Then like Heng said --argue existing work focusing on pixel-level control miss this important aspect. I would move all other irrelevant discussion like T2I are popular, and what other related works address to the related work section.  }


% In this work, we propose \textsc{Synthia}, a framework for Concept \textbf{Synth}es\textbf{i}s with \textbf{A}ffordance composition that generates functionally coherent and visually novel concepts given a set of desired affordances. Unlike prior works that rely on complex, descriptive text to generate stylistic variations or aesthetic features~\cite{richardson2024conceptlab, vinker2023concept}, \textsc{Synthia} leverages affordances--defined as ``the functionality offered by an object or its parts''--as a structural guide for novel concept synthesis. We construct a hierarchical concept ontology, which decomposes visual concepts (e.g., \texttt{Furniture}-\texttt{Sofa}) into constituent parts (e.g., \texttt{leg}, \texttt{cushion}) and their associated affordances (e.g., \texttt{support}, \texttt{rest}). This ontology provides a structured representation of concept-affordance association, serving as building blocks for generating functionally meaningful designs.
% \kw{I feel the first part of this sentence repeats line 134, I would see how to merge these sentences to make this paragraph more concise.}


% \zhenhailong{this part is more like a novel task setting instead of a key part for our method/framework?} \jeongh{The affordance sampling strategy is a key part of our proposed method upon which we build our curriculum learning.}

% \kw{I would probably start this paragraph by motivating the challenge of affordance sampling. Why we need affordance sampling, why it is important in novel concept design.}
% Inspired by the theory of combinational creativity in humans~\cite{Han2018THECD}, which suggests that novel concepts emerge from disparate ideas, we propose an affordance sampling mechanism that strategically selects affordances associated with sufficiently different concepts using our novel similarity-based metric (\S\ref{sec:data_gen}). This ensures that generated designs integrate novel functionalities, avoiding trivial combinations, whereas random sampling often yields similar affordances (e.g., \texttt{cook}, \texttt{heat}) that result in redundant outputs (Fig~\ref{fig:close}). 

% \violet{again, what's your actual input to the T2I model? It's confusing to me when you sometimes mention objects and sometimes mention affordance.}
We also introduce a new curriculum learning scheme that fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. Our curriculum gradually increases the affordance distance, allowing models to first learn basic concept-affordance associations from close affordance pairs before tackling complex affordance compositions that integrate multiple affordances into a single, coherent form. To further ensure novelty, we employ contrastive objectives to push learned representations away from existing concepts in our ontology. This addresses a critical limitation of existing T2I models, which struggle to generate coherent multi-functional concepts (Fig.~\ref{fig:distant}). Without structured affordance composition, models tend to default to familiar objects--e.g., when prompted with \texttt{drive} and \texttt{vacuum} affordances, Stable Diffusion models simply generate a car with missing \texttt{vacuum} functions (Fig.~\ref{fig:distant}), rather than blending both affordances into an integrated design. Importantly, unlike existing AI-driven design frameworks that rely on detailed LLM-generated descriptions, \textsc{Synthia} enables direct affordance-based prompting, e.g., ``\textit{a new design that has functions of \{desired affordances\}.}''. Our model implicitly learns concept-affordance associations, producing novel, structured designs without redundant textual prompting. \looseness=-2
% \heng{this method seems to require these existing concepts are representative. How do we make sure they are representative?} To effectively and contrastively fine-tune T2I models with limited data, we employ curriculum learning, which gradually increases synthesis complexity by varying affordance distances as shown in Fig. \ref{fig:main_figure}.
% \kw{I feel this paragraph is a bit lengthy and readers may lose focus. I would directly start with "We also propose a curriculum learning scheme to fine-tune .. This approach effective resolves ...}

To evaluate our framework, we uniformly sample 500 unseen affordance pairs from our ontology and assess generated concepts using automatic and human evaluation. We design evaluation metrics (\S\ref{sec:eval}) that measure faithfulness, novelty, practicality, and coherence.  
% \heng{In your training set and test set, do you have any overlapped affordance pairs? i just want to make sure this is taken care of}
% \kw{I would add a bit more details here like what aspect we evaluate, the size of data and the evaluation protocol etc. Otherwise, it read a bit vague.} 
Experiments show that \textsc{Synthia} significantly outperforms baselines, creating designs that are visually novel and functionally coherent, with consistently higher scores across all metrics. Our contributions are as follows:
\vspace{-0.05in}
\begin{itemize}
    \item We introduce a hierarchical concept ontology that encodes concept-affordance associations, serving as crucial building blocks for novel concept synthesis with functional coherence.
    \vspace{-0.05in}
    \item We propose an affordance sampling strategy that guides disparate affordance selection, avoiding redundant functionalities while ensuring coherent concept synthesis.
    \vspace{-0.05in}
    \item We develop a curriculum-based optimization for affordance composition that fine-tunes T2I models, enabling T2I models to fuse multiple affordances into a single coherent concept.
    % \item We propose metrics to qualitatively evaluate AI-driven designs, focusing on the quality and practicality of the generated novel concepts.
    
\end{itemize}