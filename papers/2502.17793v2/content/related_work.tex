\section{Related Work}
% \kw{Are there any related work considering affordance in generation? I feel the discussion of T2I models is a bit shallow and general. If there are other related work related to concept generation with T2I models or AI for design, we should discuss here.}
\paragraph{\textbf{Text-to-Image Models.}}
The advancement of text-to-image (T2I) models has enabled high-quality image synthesis from textual descriptions \citep{sohn2023styledrop, xue2024raphael, shi2024instantbooth, chen2024pixart}. Especially, the invention of diffusion-based models, such as DALL-E \citep{dalle} and Stable Diffusion \citep{stabled}, significantly increases the performance of the T2I generation by utilizing a transformer-based architecture, where the image embeddings and text encodings are aligned in the shared representation space. For instance, \citet{bao2024separateandenhancecompositionalfinetuningtext2image} propose a compositional fine-tuning method for T2I Diffusion Models that focuses on two novel objectives and performs fine-tuning on critical parameters. However, these models still struggle to understand practical functionalities and integrate multiple components into coherent novel concepts. This highlights the need for a new framework to enhance the compositional reasoning ability of T2I models, which our work aims to address.
% These models are designed mainly from two types of networks, Generative Adversarial Networks (GANs) \citep{xu2017attnganfinegrainedtextimage, kang2023gigagan} and Diffusion Models \citep{dalle, stabled, bao2024separateandenhancecompositionalfinetuningtext2image}.

% \paragraph{GAN-based T2I Framework.} Early T2I frameworks mostly utilize GAN-based architectures. For example, StackGAN \citep{zhang2017stackgantextphotorealisticimage} generates photo-realistic images conditioned on text descriptions by decomposing the hard problems into a sketch refinement process to capture more fine-grained details. \citet{hu2022textimagegenerationsemanticspatial} further improves this process by proposing the Semantic-Spatial Aware GAN, which addresses the inconsistency issue between words in the sentences and the generated images. However, these models have limitations when generating complex and highly imaginative images due to the lack of alignment between text and image modalities. 

% \paragraph{Diffusion Model-based T2I Framework.} The invention of diffusion-based models, such as DALL-E \citep{dalle} and Stable Diffusion \citep{stabled} significantly increases the performance of the T2I generation. These models utilize a transformer-based architecture, where the image embeddings and text encodings are aligned in the shared representation space. For instance, \citet{bao2024separateandenhancecompositionalfinetuningtext2image} propose a compositional finetuning for T2I Diffusion Models that focuses on two novel objectives and performs finetuning on critical parameters. However, these T2I diffusion models suffer from the challenge of understanding practical functionalities and integrate multiple components into coherent novel concepts. These challenges highlight the need for a new framework that can enhance the compositional reasoning ability of T2I models, which our work aims to offer.
\paragraph{\textbf{Novel Concept Generation.}}
The great power of T2I models provides a potential boost to content creation \citep{ko2023large, rangwani2024crafting, sankar2024novelideagenerationtool, rahman2024visual, tang2024lgm}. Novel concept generation aims to produce visual outputs that extend the existing concepts by specifying the requirements as input to the T2I models. For instance, Concept Weaver \citep{kwon2024concept} first generates a template image based on a text prompt, then refines it using a concept fusion strategy. ConceptLab \citep{richardson2024conceptlab} utilizes Diffusion Prior models and formulates the generation process as an optimization process over the output space of the diffusion prior. Yet, they focus on concept-level generation and ignore the relationships between concepts and their parts. By prioritizing aesthetics, they limit real-world practicality. Our work bridges this gap by designing an affordance-driven framework for novel concept synthesis, ensuring the fusion of desired functions to output novel but practical concepts.

% \subsection{Curriculum Learning}
% Curriculum learning is inspired by human learning process, where the complexity of training data or tasks is formulated in a progressively complex way to optimize the model training \citep{bengio2009curriculum}. It is widely applied in various domains such as natural language processing and computer vision \citep{wang2019dynamic, campos2021curriculum}. Vision Language Models (VLMs) which integrate visual and textual modalities, show great potential and challenges for curriculum learning \citep{zhang2021curriculum, saha2024exploringcurriculumlearningvisionlanguage}. For example, TONICS \citep{srinivasan2022curriculumlearningdataefficientvisionlanguage} uses a smaller set of paired data with a curriculum learning algorithm to learn fine-grained vision-language alignments. Kangaroo \citep{liu2024kangaroopowerfulvideolanguagemodel} is a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos understanding. In this paper, we will utilize curriculum learning to finetune the diffusion model by gradually adding difficulty of the training data in terms of two dimensions: (1) The number of concepts/affordances in the input constraints; (2) Distance of the affordances in the concept ontology. In this way, we can improve the compositionality of diffusion models and generate more coherent novel concepts.


