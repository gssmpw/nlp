\documentclass[journal]{IEEEtran}
%\documentclass[sigconf]{acmart}
\usepackage{url}
\usepackage[noadjust]{cite}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{makecell}
\usepackage{ragged2e}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{nomencl}
\makenomenclature
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{balance}
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\usepackage{pifont}
\usepackage{tabulary}
\usepackage{comment}
\usepackage{lipsum}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{array}
\usepackage{array, tabularx, multirow, booktabs, diagbox}
\usepackage{adjustbox}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\usepackage[table,xcdraw]{xcolor} % For row coloring
\usepackage{colortbl} % For alternating row colors
\usepackage{amssymb} % For check and cross marks
\renewcommand\qedsymbol{$\blacksquare$}




\newcommand{\tick}{\ding{52}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{subcaption}
\usepackage[font={small}]{caption}
%\usepackage{subfig}
\usepackage{epstopdf}
\usepackage{pdfpages}
\usepackage{algorithmicx}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
% \usepackage[noend]{algpseudocode}
\usepackage{pifont}
\usepackage{bm}
\usepackage{authblk}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\pagenumbering{gobble}
\let\labelindent\relax
\usepackage[inline]{enumitem}
% \usepackage{float}
\usepackage{multirow}
\usepackage{commath}
\UseRawInputEncoding
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Vision-Language Models for Edge Networks: A Comprehensive Survey}


\author{Ahmed~Sharshar,~Latif~U.~Khan,~\IEEEmembership{Member,~IEEE},~Waseem~Ullah,~\IEEEmembership{Member,~IEEE},~Mohsen~Guizani,~\IEEEmembership{Fellow,~IEEE}






% \author{ABC
% %[latif.khan@mbzuai.ac.ae, mguizani@ieee.org]%, Bassem.Ouni@tii.ae]
% %\\$\ddagger$~Wireless Research Institute, ZTE Corporation, Beijing 100029, China and the State Key Laboratory of Mobile Network and Mobile Multimedia Technology, Shenzhen 518055, China.\\
        
% %,~Walid~Saad,~\IEEEmembership{Fellow,~IEEE},~Dusit~Niyato,~\IEEEmembership{Fellow,~IEEE},~Zhu~Han,~\IEEEmembership{Fellow,~IEEE},~and~Choong~Seon~Hong,~\IEEEmembership{Senior~Member,~IEEE}
    
\IEEEcompsocitemizethanks{
 \IEEEcompsocthanksitem Corresponding author: Ahmed Sharshar {ahmed.sharshar@mbzuai.ac.ae}
\IEEEcompsocthanksitem A. Sharshar is affiliated with the Computer Vision Department, while L. U. Khan, W. Ullah, and M. Guizani are with the Machine Learning Department, all at Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE.

}}

\markboth{}{}%
	


\maketitle


% }


% \thanks{
% 	}
% }

 %The paper headers
% \markboth{IEEE Communications Magazine}{}%

% Traditional machine learning is based on the migration of data from massively distributed IoT devices to a centralized cloud for training and thus, poses serious security and privacy concerns.

% \IEEEcompsoctitleabstractindextext{%
% \justify
\begin{abstract} 
Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings.
\end{abstract}

\begin{IEEEkeywords}
Vision language models, edge computing, efficient fine-tuning, transformers, large language models.
\end{IEEEkeywords}


\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/main_figure.pdf}
    \caption{An Overview of The Role of VLMs in IoT Applications.}
    \label{fig:Main Overview}
\end{figure*}



The integration of vision and language understanding in artificial intelligence has given rise to VLMs, which combine visual inputs with natural language processing to perform tasks such as image captioning, visual question answering, and visual content generation \cite{Lu2019, Li2020, Kim2021,khan2024edge}. These models have demonstrated promising capabilities in various domains, from social media content moderation to assisting autonomous vehicle navigation, enabling machines to interact with their environment more intuitively and human-likely. Although VLMs offer many benefits, it is challenging to extend VLMs at the network edge. Extending VLMs to edge devices remains very challenging due to resource limitations of edge devices (e.g., smartphone and wearable). Edge devices, characterized by their limited processing power, memory, and energy consumption, require VLMs that are accurate but also lightweight and efficient \cite{Li2019, Wu2020}. The challenges posed by these constraints necessitate innovative approaches to model design and optimization to ensure that VLMs can be effectively deployed on edge platforms \cite{lin2023pushing}.


Recent studies have aimed to compress VLMs and use edge deployment with pruning, quantization, and Knowledge Distillation methods \cite{efficientvlm2024}. Pruning consists of removing redundant or insignificant parameters from the model, reducing the model's size and computational overhead while maintaining similar performance \cite{Han2016}. Quantization reduces the precision of model weights and activations, which can greatly impact memory usage and inference speed \cite{Jacob2018}. In knowledge distillation, knowledge from a large, cumbersome model (teacher) is distilled into a smaller model (student) \cite{Hinton2015}. Moreover, purpose-built hardware accelerators (e.g., Google’s Edge TPU) and edge-native architectures have also played a crucial role in enhancing the accessibility in deploying VLMs on edge-constrained hardware \cite{Sze2017, Reddi2020}. This survey highlights these advancements and presents a comprehensive overview of lightweight visual language models (VLMs) for edge applications, discussing the trade-offs involved in striking the balance between model efficiency and performance.


\subsection{Motivation}
The demand for real-time processing of visual tasks, for example, autonomous driving, smart surveillance, and augmented reality, is one of the primary reasons to deploy VLMs on edge devices \cite{Chen2019, Wang2020, Tan2021}. ITS, one of the crucial applications, includes object detection, traffic sign recognition, and pedestrian detection. Offloading this processing to the cloud incurs latency, impeding time-critical use cases. Likewise, edge processing in smart surveillance: Processing video features on edge devices (e.g., IP cameras) protects the privacy of target information by reducing private data transmission over networks \cite{Lane2015}. Augmented reality applications also use low-latency processing to interact seamlessly. These applications are made possible by lightweight VLMs that guarantee high-performance resource usage at the edge \cite{Jouppi2017}. Calculations performed closer to the data reduce latency and add reliability by reducing reliance on stable connections.


The use of VLMs on edge devices faces challenges. Current VLMs are unusually large and do not fit into the memory/storage of most edge devices. For instance, the GPT-3 model with 175 billion parameters demands about 350 GB for inference memory alone \cite{Brown2020}, while CLIP, a common VLM, has 63 million parameters \cite{Radford2021}, which is not appropriate for edge devices and other limited resources regions. These models require considerable hardware resources and energy, which is typically a limitation for low backup energy devices. Edge devices, including smartphones and Internet of Things (IoT) sensors, are typically equipped with a backup energy capacity of 1,000 mAh to 5,000 mAh \cite{Zanella2020}, making it challenging to run power-hungry computations locally with these models. 


In addition, when such models perform inference, they may quickly consume the energy supplies of edge devices, restricting their operational time and efficiency \cite{Zhang2020}. Moreover, the computational complexity of these models often requires hardware accelerators (GPUs, TPUs, etc.), which may not be realistic for many edge computing applications due to cost and power availability \cite{Chen2021}. Figure \ref{fig:Main Overview} shows an overview of using VLM in the IOT framework.


We address these issues with a balance of model complexities over resource allocations. Compromises in performance and accuracy are often required to balance model complexity against resource efficiency in addressing these issues. One specific type of low-resource VLMs is model compression methods, including pruning, quantization, and knowledge distillation, which can significantly decrease VLMs' size and computational cost. However, this may result in a momentary drop in accuracy, which ought to be judiciously managed to make the model effective for the application \cite{Han2016, Jacob2018, Hinton2015}. 


The second key issue comes from the devices' heterogeneity in computing power and local energy reserves. This variability adds complexity to the deployment process for VLMs, as the models must be tuned to the capabilities of each device. Some works, such as dynamic inference, have attempted this by scaling the network with respect to the computation available at inference, balancing resource use and accuracy \cite{Tan2019, Yu2019}. Model scaling, for instance, makes several models of different complexities so they can be deployed on edge devices with differing capabilities. Dynamic inference techniques can account for this by adjusting the computation at inference time and balancing speed and accuracy based on real-time resource availability.


Further research is, however, required to create solutions that are domain-agnostic and can be adapted to the different constraints of different edge environments. To facilitate edge-native VLM adoption, it is crucial to have these models work with reasonable efficiency over significant device variance with minor device specialization.

Furthermore, optimizing VLMs for edge devices includes research on novel model architectures with lower computational requirements by design. One such adaptation, for example, is using transformer-based models that offer more efficient variants for edge deployment \cite{Vaswani2017, Chen2020}. These adaptations generally include simplifying the attention mechanisms or reducing the layers in the model to reduce computational overhead. In addition, there is a trend of using attention approaches and lightweight CNNs to achieve trade-offs between effectiveness and resources \cite{Hu2018, Howard2019}. To boost performance on mobile devices, MobileNetV3 is proposed with architectural innovations: depthwise separable convolutions that compress the number of parameters and the computations required \cite{Howard2019}. These architectural advances play an important role in expanding the potential of what is possible with VLMs on edge devices, allowing more capable models to run within the constraints of simpler hardware.

Lightweight VLMs can have a variety of application domains that are growing rapidly. For instance, through VLMs, medical image analysis and diagnostics can be performed directly on portable devices, enabling immediate feedback and decision support \cite{Esteva2017, Topol2019}. This capability is invaluable in remote or resource-poor environments where access to advanced medical care is restricted. Through visual recognition and language understanding capabilities, VLMs allow for next-generation inventory management and customer interaction in retail \cite{Ren2015, Liu2016}. For example, VLMs can examine smart shop assistants that identify and explain products in detail to customers seamlessly and grammatically. These models can have implications for multiple domains, showcasing the potential versatility of lightweight VLMs. VLMs cover a tremendous spectrum of applications, from driving efficiency in industries where real-time visual inspections can be automated through VLMs to enabling everyday experiences for those with disabilities by describing the contents of the camera stream captured in the real world.




\begin{table*}[ht]
\centering
\caption{Comparison of Various Studies on Vision-Language Models (VLMs).}
\renewcommand{\arraystretch}{1.2} % Adjust the value to increase/decrease the row height
\rowcolors{1}{gray!15}{white} % Alternating row colors
\begin{tabular}{l c c c c p{4.5cm}} % No borders here, removed the "|"
\hline
\textbf{Reference} & \textbf{Security and Privacy} & \textbf{Efficient Fine Tuning} & \textbf{On Edge Inference} & \textbf{Applications} & \textbf{Remark} \\ \hline
Du \emph{et al.} \cite{surveyvisionlanguagepretrainedmodels} & \xmark & \cmark & \xmark & \cmark & This work surveys vision-language pre-trained models, focusing on their architectures, training methods, and applications. \\  \hline

Li \emph{et al.} \cite{visionlanguageintelligencetasksrepresentation} & \xmark & \cmark & \xmark & \xmark & The study explores vision-language intelligence, emphasizing tasks, representation learning, and the development of large models. \\ \hline

Xing \emph{et al.} \cite{SurveyofEfficientFine-TuningMethods} & \cmark & \cmark & \xmark & \xmark & The paper provides an overview of efficient fine-tuning methods for vision-language models, with a focus on Prompt and Adapter techniques. \\ \hline

Ghosh \emph{et al.} \cite{ghosh2024exploringfrontiervisionlanguagemodels} & \xmark & \xmark & \xmark & \cmark & This article reviews current methodologies and future directions of vision-language models, with emphasis on their development and applications. \\ \hline

Zhang \emph{et al.} \cite{zhang2024visionlanguagemodelsvisiontasks} & \xmark & \cmark & \xmark & \cmark & This study highlights vision-language models for vision tasks, emphasizing their theoretical foundations and practical applications. \\ \hline

Cui \emph{et al.} \cite{ASurveyonMultimodalforAutonomousDrivin} & \xmark & \cmark & \xmark & \cmark & This review discusses multimodal large language models for autonomous driving, with attention to their applications and efficiency. \\ \hline

Yin \emph{et al.} \cite{surveymultimodallargelanguage} & \cmark & \xmark & \xmark & \cmark & This comprehensive study offers an in-depth overview of multimodal large language models. \\ \hline

Jin \emph{et al.} \cite{efficientmultimodallargelanguage} & \xmark & \cmark & \xmark & \cmark & This research examines efficient multimodal large language models, focusing on their design and applications. \\ \hline

Our Survey & \cmark & \cmark & \cmark & \cmark & N/A\\
\hline
\end{tabular}%
\label{tab:literaturecomparison}
\end{table*}






\subsection{Market Statistics and Research Trends}
VLMs have rapidly emerged as a new market in recent years, fueled by the rising demand for intelligent systems that can understand and reason with both visual and textual information. The global AI market was valued at USD 58.3 billion in 2021, and it is expected to reach USD 309.6 billion by 2026, with a CAGR of 39.7\% \cite{MarketsandMarkets2021}. This segment of the VLM market is projected to grow at the most rapid rate. The overall VLM market is anticipated to grow significantly over the projection period. The global market size of VLMs is estimated to reach around \$2.5 billion in 2024, increasing from \$1.8 billion in 2023 and \$1.2 billion in 2022 \cite{huggingface2024}. Why is everyone talking about it? Because VLMs find application in diverse sectors, including healthcare, automotive, and consumer electronics. For example, one of the factors driving the growth of the VLMs market is the adoption of VLMs to develop ADAS and autonomous driving solutions in the automotive industry. At the same time, with the increase in smart devices and the IoT, the need for lightweight VLMs that perform well on edge devices has become increasingly urgent.



Research trends in VLMs indicate a strong focus on enhancing model efficiency and accuracy while reducing computational overhead. Recent studies have explored various techniques for model compression, including pruning, quantization, and knowledge distillation \cite{Han2016, Hinton2015, Jacob2018}. Additionally, there is growing interest in developing new architectures that leverage the strengths of both CNNs and transformer models \cite{Vaswani2017, Howard2019}. These hybrid models aim to balance the computational efficiency of CNNs with the powerful representation capabilities of transformers. Another emerging trend is using multi-task learning frameworks, where a single VLM is trained to perform multiple related tasks, improving overall efficiency and reducing the need for task-specific models. Notably, the number of research papers published on VLMs and AI on edge devices has increased significantly, reflecting the growing academic interest in this field.



The applications of VLMs are expanding rapidly, with significant investments being made in sectors such as healthcare, retail, and security. In healthcare, VLMs are utilized for tasks such as medical image analysis, disease diagnosis, and telemedicine, providing real-time assistance to healthcare professionals \cite{Esteva2017, Topol2019}. The retail sector is leveraging VLMs for enhanced customer experiences through smart shopping assistants and personalized marketing \cite{Ren2015, Liu2016}. Security applications include automated surveillance systems that can analyze and interpret visual data to detect anomalies and potential threats. These applications demonstrate the versatility and impact of VLMs across various industries, driving further research and development in this field. AI development on edge devices has become a critical area of focus due to the need for real-time processing, reduced latency, and improved privacy. Edge AI involves deploying AI models directly on devices such as smartphones, cameras, and IoT sensors, enabling local data processing without relying on cloud infrastructure \cite{Lane2015}. This shift towards edge computing is driven by the limitations of cloud-based AI, including latency issues, bandwidth constraints, and data privacy concerns. Research in edge AI is focused on optimizing model architectures and developing specialized hardware accelerators to support efficient inference on resource-constrained devices \cite{Sze2017, Reddi2020}. Companies like NVIDIA, Intel, and Google invest heavily in edge AI solutions, indicating a robust market growth trajectory. According to Allied Market Research, the global edge AI hardware market is expected to reach USD 3.89 billion by 2025, growing at a CAGR of 20.6\% from 2018 to 2025 \cite{AlliedMarketResearch2020}.



\subsection{Existing Surveys and Tutorials}

Few surveys and tutorials have reviewed VLMs, their efficiency, and applications \cite{surveyvisionlanguagepretrainedmodels, visionlanguageintelligencetasksrepresentation, SurveyofEfficientFine-TuningMethods, ghosh2024exploringfrontiervisionlanguagemodels, zhang2024visionlanguagemodelsvisiontasks, ASurveyonMultimodalforAutonomousDrivin, surveymultimodallargelanguage, efficientmultimodallargelanguage}. Table \ref{tab:literaturecomparison} summarizes some of this work Scopes and how it is different than ours.
 The authors in\cite{surveyvisionlanguagepretrainedmodels} focused on vision-language pre-trained models, discussing the evolution of these models, different architectures used, and methods for integrating vision and language modalities. Another work \cite{visionlanguageintelligencetasksrepresentation} explored vision-language intelligence, emphasizing tasks, representation learning, and the development of large models. They provided insights into the performance improvements and future research directions in this area. Xing et al. \cite{SurveyofEfficientFine-TuningMethods} surveyed efficient fine-tuning methods for vision-language models, focusing on Prompt and Adapter techniques. They discussed various strategies to enhance fine-tuning efficiency and addressed challenges related to efficient fine-tuning. Ghosh et al. \cite{ghosh2024exploringfrontiervisionlanguagemodels} provided a comprehensive overview of the current methodologies and future directions of vision-language models, highlighting the strengths and limitations of existing approaches and suggesting areas for further exploration. Zhang et al. \cite{zhang2024visionlanguagemodelsvisiontasks} surveyed vision-language models for vision tasks, discussing the theoretical foundations, practical applications, and identifying challenges and opportunities in applying these models in fields like medical imaging and industrial automation. Another survey \cite{ASurveyonMultimodalforAutonomousDrivin} focused on multimodal large language models for autonomous driving, discussing the integration of different modalities, methodologies to enhance model performance, and specific applications in autonomous driving scenarios. Yin et al. \cite{surveymultimodallargelanguage} surveyed multimodal large language models with a focus on efficient design and diverse applications, covering architectures, strategies to enhance efficiency, and applications in fields like biomedical analysis and document understanding. Lastly, Jin et al. \cite{efficientmultimodallargelanguage} provided a survey on efficient multimodal large language models, discussing methods to reduce computational costs, improve efficiency, and applications in areas like high-resolution image understanding and medical question-answering, highlighting future research directions and challenges in the field.
 

Different from existing works \cite{surveyvisionlanguagepretrainedmodels, visionlanguageintelligencetasksrepresentation, SurveyofEfficientFine-TuningMethods, ghosh2024exploringfrontiervisionlanguagemodels, zhang2024visionlanguagemodelsvisiontasks, ASurveyonMultimodalforAutonomousDrivin, surveymultimodallargelanguage, efficientmultimodallargelanguage}, we present a comprehensive overview of VLMs, including key design aspects and high-level architecture. We also provide deployment challenges on edge devices. Furthermore, several open research challenges are discussed, along with promising solution approaches.


\subsection{Our Survey}
This survey aims to examine the techniques, architectures, and applications that define the rapidly evolving area of VLMs for edge networks. By addressing the challenges and showcasing the solutions, this paper contributes to the ongoing efforts to make sophisticated VLMs accessible and practical for edge computing environments. The continued innovation in this field promises to unlock new capabilities and applications, bringing the power of AI-driven vision and language understanding to a broader range of devices and use cases. Our survey aims to answer the following questions:
\begin{itemize}
    \item How do we efficiently enable VLM at the network edge? 
    \item What are the existing schemes and their limitations that will help deploy VLM at the network edge?
    \item How does one enable secure and privacy-ware VLM?
    \item What are the challenges and their possible solutions in allowing VLMs to at the network edge?
    \item What are the different application domains for VLMs, and what opportunities are available?
\end{itemize} 

Our contributions are summarized as follows:
\begin{itemize}
    \item We present the key concepts, main design aspects, and high-level architecture for Vision-Language Models.
    \item A comprehensive cycle for extending the VLMs from the cloud to the edge is provided, considering efficient training and fine-tuning methods, edge deployment challenges, and privacy and security issues. We consider issues related to designing efficient VLMs, deploying them on edge devices, addressing privacy and security concerns, and enhancing their performance on low-resource devices.
    \item Several open challenges are presented, including the difficulties of deploying VLMs on edge devices and fine-tuning them with limited resources. Moreover, we discussed about promising solution approaches.
\end{itemize}


\section{Fundamentals of Vision Language Models}


\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{Images/Single_Stream.pdf}
        \caption*{Single Stream}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{Images/Dual_Stream.pdf}
        \caption*{Dual Stream}
    \end{minipage}
    \caption{The architecture of Vision-Language Pre-trained (VLP) models typically includes three key components: Visual Embedding (VE), Textual Embedding (TE), and Modality Fusion (MF). Fig. (a) illustrates a dual-stream model, while Fig. (b) depicts a single-stream model. In dual-stream models, modality fusion can be optional and generally occurs through interactions (often via cross-attention) between the separate language and image encoders. Conversely, in single-stream models, modality fusion is inherently integrated within a unified encoder, which is typically a multi-layer transformer \cite{visionlanguageintelligencetasksrepresentation}.}
    \label{Single vs dual}
\end{figure*}

VLMs are designed to process and integrate visual and textual information simultaneously. These models leverage the combined power of computer vision and natural language processing to perform various multimodal tasks such as image captioning, visual question answering (VQA), and image-text retrieval. This section provides a detailed theoretical understanding of how VLMs work, including their mathematical representation and model architectures.

\subsection{Key Concepts}
They learn to align visual and textual modalities in a shared representation space, enabling cross-modal understanding and interaction. This process is a series of steps per modality (text and image) of tokenization, embedding, and encoding. In doing so, VLMs are able to model rich semantic interactions both within a single modality and cross-modality as one unified feature that connects image, text, and sound representations, improving downstream tasks like captioning, retrieval, and question answering.

\textbf{Text Representation}

Assuming a text input sequence \( T = [t_1, t_2, \ldots, t_N] \) where each \( t_i \) is the \( i \)-th token, the representation for \( T \) can be obtained through tokenization and embedding. Steps in the Flow: Each token \( t_i \) is mapped to a high-dimensional word embedding \( \mathbf{e}_i \) that captures the semantic features of the word. This mapping is provided by a pretrained embedding function (often the output of transformer-based models like BERT, or other pretrained language models \cite{devlin2018bert,liu2021pretrained}):
\begin{equation}
\mathbf{e}_i = \text{Embedding}(t_i).
\end{equation}

In order to account for this sequence information, we simply add positional embeddings \( \mathbf{p}_i \) to each of the token embeddings. These positional embeddings give a sense of word position, which is essential for maintaining the structure of the text in tasks that rely on understanding word relationships \cite{vaswani2017attention}:
\begin{equation}
\mathbf{h}_i = \mathbf{e}_i + \mathbf{p}_i.
\end{equation}

This word + positional embedding \( \mathbf{h}_i \) is then forwarded through a number of transformer layers to allow the model to assemble a rich contextual representation of the text input. These contextualized embeddings are used to derive a single representation that joins together with the visual features.

\textbf{Image Representation}

For the image input \( I \), the representation appropriately consists of obtaining informative visual features that inform the image’s semantic and spatial content. This approach is most often implemented with either convolutional neural networks (CNNs) or Vision Transformers (ViTs), depending on the model's architecture. The image \( I \) is generally decomposed into a grid of patches, where each patch \( I_j \) represents a local image region. The next step consists of encoding each patch into a feature vector \( \mathbf{v}_j \) representing its visual content \cite{ren2015faster,dosovitskiy2020image}:

\begin{equation}
    \mathbf{v}_j = \text{VisionEncoder}(I_j).
\end{equation}



The feature vectors \( \mathbf{v}_j \) are obtained from the last convolutional layers for CNN-based models. In contrast, for ViTs, each image patch is linearly embedded and further processed using self-attention layers. This yields a set of visual embeddings containing both low-level and high-level features. The embeddings are mapped with text embeddings in a multimodal representation space to enable cross-modal tasks.



%\subsection{Vision-Language Interaction Mechanisms}

\subsection{Mechanisms for Vision-Language Interaction}

At the center of VLMs is the integration of textual and visual embeddings. There are two main architectures to achieve this — fusion and dual encoders. Fig. \ref{Single vs dual} illustrates the key dissimilarity between the two Architectures.



\textbf{Single-Stream Architecture (Fusion Encoders):}

In contrast, single-stream models do early fusion by interleaving visual and textual encodings into a single sequence fed through a common encoder — often, a transformer \cite{li2019visualbert,su2019vlbert}. This architecture relies on the assumption that a single transformer encoder can adequately model the interactions among the modalities. This means that the language and image tokens are tokenized and embedded and then combined into one sequence — the model processes them together, having the ability to learn visual and textual attributes at the same time. This approach encodes the two modalities using this common representation, which can help efficiently model the intricate relationships and interactions between them. In the single-stream framework, text embeddings \( \mathbf{h}_i \) and image embeddings \( \mathbf{v}_j \) are concatenated and processed through a transformer \cite{li2019visualbert,su2019vlbert}:

\begin{equation}
    \mathbf{z}_k = \text{Transformer}([\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_N; \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_M]).
\end{equation}




A single model that can perform all tasks is usually a huge benefit because the implementation is much simpler and more efficient. They reduce memory and potential inference times by using one encoder instead of two, simplifying the architecture. Moreover, such a unified approach becomes a powerful tool for tasks demanding rich interaction between text and image, like image captioning and VQA. This has been evidenced by models like ViLT \cite{kim2021vilt}, which utilize a vision-and-language transformer without convolutional or region-based supervision and still perform strongly. 


However, the single-stream approach has its problems, too, such as the increasing computational burden as longer sequences have to be concatenated and processed, which can be computationally intensive. In addition, the model has to learn from both modalities simultaneously, resulting in potentially non-ideal performance.


\textbf{Dual-Stream Architecture (i.e., Dual Encoders):}
On the other hand, dual-stream models adopt independent encoders for both visual and textual data, encode each modality separately, and then join their representation either through cross-attention mechanisms or other approaches. This architecture is especially useful when each input modality has limited overlapping features and can be processed differently. These independent processing streams are then merged in a higher-level step (usually through a cross-modal attention mechanism) that allows the model to learn how the modalities interact with each other after being processed and encoded independently. Treating individual modality streams with flexible structures provides full flexibility and may lead to more robust performance, as the model can capture and preserve the unique characteristics of each modality before combining them. Text and image embeddings are processed independently and later merged in the dual-stream architecture \cite{radford2021learning,lu2019vilbert,tan2019lxmert}:
\begin{align}
    \mathbf{h}'_i &= \text{TextEncoder}(\mathbf{h}_i), \\
    \mathbf{v}'_j &= \text{ImageEncoder}(\mathbf{v}_j), \\
    \mathbf{z}_k &= \text{CrossAttention}([\mathbf{h}'_1, \ldots, \mathbf{h}'_N], [\mathbf{v}'_1, \ldots, \mathbf{v}'_M]).
\end{align}

The most notable are dual-stream models, such as ViLBERT \cite{lu2019vilbert} and LXMERT \cite{tan2019lxmert}, which use separate transformers for image and text. This is especially useful for tasks in which the relationships between the modalities are complex and need to be modeled in detail, such as VQA and image-text retrieval. Because each stream can process and encode its own domain separately, dual-stream models may outperform single-stream models on tasks requiring deep, specialized processing of images and text. 

However, this method can be computationally complex in terms of having more than one set of encoders and an additional step for the integration (often requiring some kind of sophisticated attention mechanism to align the modalities effectively).


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Images/APoLLo_framework.pdf}
    \caption{The APoLLo framework provides a unified approach to multi-modal adapter and prompt learning for Vision-Language Pretraining (VLP) models. It incorporates both image (yellow) and text (red) adapters, which are connected via cross-modal attention mechanisms to enhance alignment between the two modalities. Each modality processes augmented inputs: text generated by large language models (LLM) and images synthesized by text-conditioned diffusion models. This cross-modal interaction improves the coherence and performance of multi-modal tasks \cite{chowdhury2023apollo}.}

    \label{fig:apollo}
\end{figure*}



\subsection{Efficient Fine-Tuning Methods for Vision-Language Models}

Proper fine-tuning mechanisms are critical when adapting large-scale VLMs to downstream tasks with limited computational budgets. Due to their effectiveness in alleviating resource burden related to retraining and full fine-tuning of large models, these techniques have become increasingly popular. This section describes a few diverse lines of research on efficient fine-tuning that emerged in recent years, centering on the topics of prompt-based methods and adapter-based methods.

\subsubsection{Fine-tuning with Prompts}
Their methodology for practicing a specific task with few parameter updates is to shape the input in such a way as to activate the pre-trained model’s capacity, known as prompt-based fine-tuning methods.

\textbf{a. Prompt Tuning:} Creating prompts to prompt the model to produce task-appropriate outputs. Prompts can be hard (discrete text) or soft (continuous vectors). Hard prompts refer to fixed text templates that you include in your input; soft prompts are the continuous embedding of learned vectors injected into your input sequence. CoOp (Context Optimization) and CoCoOp (Conditional Context Optimization) apply learnable soft prompts to enhance the adaptability of the model across varied image recognition tasks \cite{zhou2022learning, zhou2022conditional}.


\textbf{b. Prefix Tuning:} Prefix tuning introduces continuous task-specific vectors (prefixes) to the input of each transformer layer. These prefixes act as virtual tokens, guiding the model's attention mechanism. Lester et al. demonstrated that prefix tuning could achieve competitive performance with minimal additional parameters by adding prefixes to the transformer layers without modifying the original model weights \cite{lester2021power}.

\textbf{c. P-Tuning:} P-tuning extends prompt tuning by using a trainable prefix of virtual tokens that guide the model to focus on task-relevant information. This method is particularly effective in few-shot learning scenarios, where it significantly improves the model's performance with limited data \cite{liu2021ptuning}.

\textbf{d. Prompt Tuning for Vision-Language Models:} Techniques like DenseCLIP and ProDA have been developed to extend prompt tuning specifically for vision-language tasks. These methods use prompt-based learning to align visual and textual features more effectively, achieving performance comparable to full fine-tuning \cite{rao2022denseclip, wang2021proda}.

\subsubsection{Adapter-Based Fine-Tuning}
Adapter-based methods introduce lightweight, task-specific modules into the pre-trained model, allowing efficient adaptation without full model fine-tuning.

\textbf{a. Adapter Modules:} Adapters are small feed-forward networks inserted between the layers of the pre-trained model. They enable task-specific learning by adjusting only the adapter parameters while keeping the original model weights frozen. Houlsby et al. demonstrated that adapter modules could achieve performance comparable to full fine-tuning with significantly fewer trainable parameters \cite{houlsby2019parameter}.

\textbf{b. LoRA (Low-Rank Adaptation):} LoRA reduces the number of trainable parameters by decomposing the weight updates into low-rank matrices. This method allows efficient adaptation of large models with a minimal computational footprint. Hu et al. showed that LoRA could achieve substantial parameter efficiency while maintaining high performance on various downstream tasks \cite{hu2021lora}.

\textbf{c. Parallel Adapter Networks:} Parallel adapters introduce additional parallel pathways in the transformer architecture, allowing for efficient multi-task learning. Pfeiffer et al. proposed AdapterFusion, which combines multiple adapter modules trained on different tasks, enabling the model to leverage shared knowledge across tasks \cite{pfeiffer2021adapterfusion}.

\textbf{d. Task-Specific Adapters:} Techniques like VL-Adapter and Clip-Adapter have been developed to provide efficient task-specific fine-tuning for vision-language tasks. These adapters are designed to handle the unique requirements of multimodal data, improving performance while minimizing computational costs \cite{sung2021vl, gao2021clip}.

\textbf{e. Hybrid Methods:} Some recent approaches combine prompt-based and adapter-based methods to leverage the advantages of both. APoLLo (Adaptive Prompt Learning) integrates prompts and adapters to achieve efficient and robust fine-tuning for vision-language models \cite{chowdhury2023apollo}. Fig.\ref{fig:apollo} explains APoLLo framework for fine-tuning VLMs.


\subsection{Existing VLM Models}


\begin{table*}[ht]
\centering
\caption{A Comparison Between Some Lightweight Vision-Language Models.}
\renewcommand{\arraystretch}{1.2} % Adjust the 
\begin{tabular}{l c c p{2cm} p{5cm}} % No borders here, removed the "|"
\hline
\textbf{Model} & \textbf{Year} & \textbf{Fusion Scheme} & \textbf{Parameters} & \textbf{Applications} \\ \hline

MobileVLM \cite{mobilevlmv2} & 2024 & Single stream & 1.1B & Mobile applications, Real-time image captioning \\  \hline

LightCLIP \cite{lightclip2024} & 2024 & Dual stream & 2.45M & Image classification, Zero-shot learning \\  \hline

MoE-TinyMed \cite{moetinymed2024} & 2024 & Single stream & Not specified & Medical imaging, Diagnostic assistance \\  \hline

EfficientVLM \cite{efficientvlm2024} & 2024 & Single stream & 92M & Visual question answering, Image retrieval \\  \hline

PaLI-3 \cite{pali32024} & 2024 & Single stream & Not specified & Image captioning, Object detection \\  \hline

RegionGPT \cite{regiongpt2024} & 2024 & Dual stream & Not specified & Regional image analysis, Multimodal translation \\  \hline

Ins-DetCLIP \cite{insdetclip2024} & 2024 & Single stream & Not specified & Object detection, Scene understanding \\  \hline

Unified-IO \cite{unifiedio2024} & 2024 & Single stream & 1B & Integrated multimodal tasks, Visual question answering \\  \hline

LightVLP \cite{lightvlp2024} & 2024 & Dual stream & Not specified & Cross-modal retrieval, Visual grounding \\  \hline

Xmodel-VLM \cite{xmodelvlm2024} & 2024 & Single stream & 1.1B & Text-image alignment, Visual question answering \\  \hline

EM-VLM4AD \cite{emvlm4ad2024} & 2024 & Single stream & 223M (T5-Base) / 750M (T5-Large) & Autonomous driving, Traffic behavior prediction \\  \hline

CLIP-Adapter \cite{gao2021clip} & 2023 & Dual stream & Not specified & Image-text retrieval, Few-shot learning \\  \hline

Lightweight Unsupervised Federated Learning \cite{lightfed2023} & 2023 & Dual stream & Not specified & Distributed learning, Privacy-preserving training \\  \hline

ALLaVA \cite{allava2024} & 2022 & Single stream & Not specified & Vision-language instruction tuning, Data synthesis \\  \hline

CLIP \cite{clip2022} & 2022 & Dual stream & 400M & Zero-shot learning, Image-text matching \\  \hline

VisualBERT \cite{visualbert2022} & 2022 & Single stream & 110M & Visual question answering, Image captioning \\  \hline

MiniVLM \cite{minivlm2022} & 2022 & Single stream & 45.7M & Lightweight image-text processing, Visual question answering \\  \hline

\end{tabular}%
\label{tab:comparison}
\end{table*}




Vision-language models have advanced significantly in recent years, offering capabilities that span across various domains such as image classification, autonomous driving, UI understanding, and more. Table \ref{tab:comparison} shows a comparision between some of the lightweight VLMs, where here we discuss some of the available VLMs, especially those lightweight:

\textbf{ViTamin} is a vision-language model for scalable applications emphasizing image classification and open-vocabulary detection. It uses a Vision Transformer (ViT) base and the CLIP framework, achieving improved zero-shot performance on ImageNet while remaining small. ViTamin processes large datasets, making it suitable for visual recognition tasks and automatic visual description \cite{Chen_2024_CVPR}.

\textbf{LINGO-2}, developed by Wayve, extends vision-language-action models for autonomous driving. It combines visual input, natural language, and action sequences to generate driving behaviors and textual commentaries, increasing explainability. Using a multimodal encoder-decoder architecture, the lightweight 5-billion-parameter model achieves real-world and simulation-capable performance \cite{wayve2024lingo}.

\textbf{InstructBLIP} advances vision-language modeling through instruction tuning, transforming datasets into instruction-following formats. Built on BLIP-2, it surpasses prior state-of-the-art in tasks like question-answering and image captioning, using a Query Transformer for improved adaptability and performance \cite{instructblip2024}.

\textbf{RAVEN} integrates a base VLM with retrieval-augmented frameworks for general-purpose vision-language tasks, excelling in VQA and captioning. Its CLIP-based encoder and transformer decoder enable fine-tuning without retrieval-specific parameters, supporting diverse multimodal applications \cite{raven2024}.

\textbf{ScreenAI} focuses on understanding UIs and infographics through a multimodal encoder-decoder framework. Extending PaLI and incorporating pix2struct's patching strategy, it excels in UI navigation, question-answering, and summarization, leveraging annotated screenshots and infographics \cite{screenai2024}.

\textbf{ALLaVA} uses synthetic data from GPT-4V, employing a captioning-then-QA pipeline with a pre-trained vision encoder and small language model. Fine-tuning on synthesized datasets improves comprehension and reduces hallucinations, achieving strong performance with fewer parameters \cite{allava2024}.

\textbf{Xmodel-VLM}, a lightweight vision-language model for consumer devices, pairs a CLIP ViT-L/14 visual encoder with Xmodel-LM 1.1B, achieving low computational cost and competitive performance on benchmarks \cite{xmodelvlm2024}.

\textbf{MobileVLM V2}, optimized for mobile devices, incorporates a Lightweight Downsample Projector (LDPv2) to reduce visual tokens and speed up inference. Its MobileLLaMA architecture excels in fast, reliable multimodal processing \cite{mobilevlmv2}. Figure \ref{fig:mobilevlm} illustrates the basic model architecture of mobileVLM.

\textbf{LightVLP} adopts the Gated Interactive Masked AutoEncoder architecture for lightweight pre-training. Its multimodal encoder aligns visual and textual inputs efficiently, enabling high-quality outputs with fewer parameters \cite{lightvlp2024}.

\textbf{EM-VLM4AD}, designed for VQA in autonomous driving, combines multi-view image embedding with a gated pooling attention mechanism and a scaled-down T5 language model. It achieves strong performance in perception and planning tasks \cite{emvlm4ad2024}.

The lightweight VLMs discussed show efficiency and specialization but face challenges in robustness, adaptability, and generalization. Future work should focus on adaptive learning mechanisms, enhanced transfer learning, and dynamic fusion strategies to improve performance in diverse domains and ensure transparency and interpretability for critical applications like healthcare and autonomous driving.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/MobileLLama.pdf}
    \caption{The MobileVLM architecture. Inputs include visual data \(X_v \) and textual queries \(X_q\), processed by a vision encoder and tokenizer, respectively, producing hidden states \(H_v\) and \(H_q\). These are fed into MobileLLaMA, which outputs a response \(Y_a\). The Lightweight Downsample Projector (LDP) efficiently processes the visual input using depthwise and pointwise convolutions \cite{mobilevlmv2}.}
    \label{fig:mobilevlm}
\end{figure*}



\section{VLMs for Edge Networks}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/EdgeVLM.pdf}
    \caption{The challenge of adapting large vision-language models to edge devices across different visual modalities. In this example, a resource-constrained cleaning robot equipped with RGB and depth cameras is used. The robot generates RGB-depth image pairs without scene labels. Using the pre-trained image encoder from CLIP as the teacher, the EdgeVL framework transfers knowledge to a smaller student encoder. This process requires no labels or human intervention, enabling the student model to directly process RGB or depth images for open-vocabulary scene classification on the device. EdgeVL distills the knowledge from the pre-trained visual encoder to the student model. In stage 2, it first fake-quantizes the pretrained student model, then uses contrastive learning to refine the student model \cite{edgevl2024}.
}
    \label{fig:edgevl}
\end{figure*}


Edge devices form an important layer in the IoT architecture and are stationed at the periphery of a network. This allows for real-time insights as they process, store, and compute data locally, transferring less data to potential server farms for processing. The main reasons to deploy edge devices are to deal with lower latency (reduced response time, less significant temporal variability), less usage of data bandwidth, and improved data privacy (sensitive data handling locally) \cite{khan2024resourceICC}.

There are conventional edge devices and intelligent edge devices. Examples of regular edge devices are routers and switches that control the flow of data between the networks with low computation power \cite{alansi2021intelligence}. On the contrary, intelligent edge devices (e.g., IoT gateways and smart cameras) have richer processing abilities to accomplish machine learning inference or data analytics tasks \cite{alizadeh2016authentication}. Mobile devices employ hardware such as System-on-Chip (SoC), Graphics Processing Units (GPUs), and specific processors, making it easier to run complex algorithms on less power \cite{karthikeyan2019applications}.

Below are the key features of edge devices:

\begin{itemize}
\item \textbf{On-Premises Processing}: Local data can be processed at the edge to facilitate rapid data analysis, computation, and feedback without relying on external cloud systems \cite{shiyun2021primer}.
\item \textbf{Autonomy and Low Latency}: These devices provide autonomous decision-making abilities, which are highly necessary for use cases such as self-operated vehicles and manufacturing \cite{shah2020sdn}.
\item \textbf{Higher Security and Privacy}: Data processed at the edge limits exposure of sensitive information, resulting in a higher level of data security and privacy \cite{alansi2021intelligence}.
\item \textbf{Versatile}: Edge devices can be used for a diverse range of applications, including smart cities, industry monitoring, healthcare, and consumer electronic applications \cite{xiao2019edge}.
\end{itemize}

Edge devices have specific technical characteristics depending on the application. SoCs are very much used in the IoT gateways for effective data processing between balanced computational time and energy efficiency. On the other hand, for heavy computing tasks, such as real-time image processing in smart cameras, GPUs or special processors like Application-Specific Integrated Circuits (ASICs) may be used \cite{karthikeyan2019applications}.




\subsection{Existing Low Complexity VLMs}
IoT is a network of devices that connect to the internet to collect, transmit, and analyze data. These may include sensors, smart appliances, wearables, and industrial devices. With the combination of IoT systems with advanced technologies such as Large Language Models (LLMs) and VLMs, sophisticated applications have been achieved, enabling automation, decision-making, and user interaction. In contrast, IoT systems generally consist of three essential layers: perception layer, network layer, and application layer \cite{shiyun2021primer, shah2020sdn}. Sensors and actuators responsible for data collection and control actions are also part of the perception layer. 

\textbf{Data Network Layer:} Securing communications between devices and centralized systems, the network layer makes sure that the data can be sent from one device to another or to a centralized system and is often based on protocols such as Wi-Fi, Bluetooth, or LPWAN. Here, the application layer operates over wired or wireless mediums to process and analyze the data to provide useful insights and services to end users \cite{shiyun2021primer, shah2020sdn}.

VLMs are models that combine visual understanding with human-like text generation or understanding to enhance the human-machine IoT interaction experience. The integration of LLMs and VLMs has also resulted in Generative IoT (GIoT) systems, which support the automation of complex tasks, enrich user interactions, and enable real-time decisions \cite{efficient2024prompting, llmind2023}.

\par 
Many papers and models have been developed to run VLM on edge devices.  \textbf{EdgeVL} is a novel framework designed to adapt large VLMs for edge devices by leveraging dual-modality knowledge distillation and quantization-aware contrastive learning. Fig.\ref{fig:edgevl} illustrates how the model focuses on efficiently aligning features from RGB and non-RGB images without manual annotation, making it versatile for various visual modalities. EdgeVL achieves up to a 15.4\% improvement in accuracy and a 93-fold reduction in model size, which is crucial for deployment on resource-limited devices. The model's design allows for a streamlined adaptation process, where a student encoder is trained to mimic a large, pre-trained teacher model like CLIP, ensuring high-quality feature extraction despite the reduced model size \cite{edgevl2024}.\par

\begin{itemize}
\item \textbf{Moondream2}: Moondream2 is an open-source, lightweight vision-language model (VLM) optimized for mobile and edge devices. With 1.8 billion parameters, it requires under 5GB of memory, making it deployable on low-cost, single-board computers such as Raspberry Pi. Its architecture is designed for efficiency, enabling real-time image recognition and understanding capabilities. This model is suitable for applications such as security and behavioral analysis, showcasing its utility in low-resource environments \cite{moondream2024}.

\item \textbf{VILA (Visual Language) model}: VILA focuses on pre-training techniques optimized for efficient edge deployment. The model employs interleaving data and instruction fine-tuning to maintain high performance while reducing computational demands. It is adaptable to various hardware, including devices like Jetson Orin. VILA also emphasizes multi-modal pre-training, enhancing in-context learning and multi-image reasoning capabilities \cite{vila2024}.

\item \textbf{MobileVLM V2}: Building upon the MobileLLaMA series, MobileVLM V2 emphasizes lightweight design for edge deployment. It introduces a novel Lightweight Downsample Projector (LDPv2) that improves vision-language feature alignment with minimal parameters. This approach involves pointwise and depthwise convolutions, along with a pooling layer to compress image tokens. MobileVLM V2 achieves significant reductions in model size and computational requirements, making it ideal for real-time applications on resource-constrained devices \cite{mobilevlmv2}.

\item \textbf{EDGE-LLM}: EDGE-LLM is a framework designed to adapt large language models for efficient deployment on edge devices. It addresses computational and memory overhead challenges through techniques like efficient tuning and memory management. This model supports continuous and privacy-preserving adaptation and inference, offering a robust solution for deploying VLMs in sensitive and resource-limited environments \cite{edgeLLM2024}.

\item \textbf{Vision Transformer Models}: Recent advancements in vision transformers have been adapted for mobile and edge devices to maintain high accuracy with minimized model size. Techniques such as token pruning, quantization, and the introduction of convolutions in transformers (e.g., CvT and TinyViT) have been explored. These models cater to tasks like object detection and instance segmentation, highlighting the versatility of vision transformers in edge applications \cite{transformer2024}.
\end{itemize}


\subsection{Deployment of VLMs on Edge Devices}

In order to deploy the VLM model on edge devices, several important steps are required to achieve efficient deployment. These steps (as shown in Fig.~\ref{fig:design}) include:

\subsubsection{Data Selection and Pre-processing}

Pre-processing data effectively is crucial for optimizing performance, particularly when dealing with heterogeneous data distributed across various edge devices. The process begins with \textbf{Data Collection}, where diverse data types such as images, text, and other relevant formats are gathered from multiple sources. The collected data must be segmented to ensure efficient processing across different environments, categorizing the data into Edge-Appropriate Data and Cloud-Appropriate Data. Edge-appropriate data generally consists of smaller, less complex datasets that can be processed in real-time on edge devices using lightweight models, while cloud-appropriate data involves more complex or sensitive information that necessitates extensive computational resources available in cloud environments \cite{Shi2021DataSelection,Souza2024BICFL}.\par

The next phase is \textbf{Feature Extraction and Selection}. During this step, relevant features are extracted from the raw data, enabling the child model on the edge device to process it efficiently. Feature selection determines which features should be processed locally and which should be sent to the cloud for further analysis, often using heuristics or lightweight models to assess data importance or complexity \cite{Sha2024EdgeFL,Hong2024Auction}. 

To optimize further, \textbf{Data Compression techniques} minimize the bandwidth required for data transmission between edge and cloud. Standard methods include quantization, dimensionality reduction, and image compression. Local pre-processing on edge devices also helps reduce the volume of data transmitted, enhancing system efficiency \cite{Souza2024BICFL,Sha2024EdgeFL}. Advanced methodologies such as Asynchronous Aggregation and Cluster Pairing introduce an intermediate layer of edge servers between clients and the cloud, aggregating local models asynchronously to reduce communication overhead and speed up convergence. This method is effective in managing system heterogeneity \cite{Sha2024EdgeFL}. Another approach is using Bioinspired Computing (BIC) algorithms, like Particle Swarm Optimization (PSO) and Genetic Algorithms, which address challenges in federated learning (FL), such as communication costs and system heterogeneity. These algorithms optimize resource allocation and data partitioning, ensuring relevant and manageable local processing \cite{Souza2024BICFL}. Synchronous-Asynchronous Hybrid Update Strategy combines synchronous and asynchronous updates to mitigate staleness effects caused by Non-IID data, integrating local updates with global synchronization to enhance model accuracy and reduce idle times \cite{Sha2024EdgeFL}. These pre-processing strategies are essential for enhancing the efficiency and performance of federated learning models in distributed, resource-constrained environments.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{Images/desing_cycle.pdf}
    \caption{Design Process of Distributed Edge VLMs.}

    \label{fig:design}
\end{figure}

\subsubsection{Model Choice on Edge and Cloud}

Performing better when one tries to deploy models on edge or cloud must involve a thoughtful choice as a tradeoff between performance and resources. Deciding where to process information is influenced by the computational capacity of edge devices, task complexity, and the need for real-time processing, among others.

\textbf{Edge-Appropriate Models} are lightweight models meant to work under low computational power constraints. These models should also be computationally less expensive and, therefore, capable of performing inference in real-time on more straightforward tasks. Examples include MobileNet and SqueezeNet, which have fewer parameters and optimized architecture for low-power environments \cite{Howard2017MobileNets,Sandler2018MobileNetV2}. 

\textbf{Cloud-Appropriate Models}, on the other hand, refer to deeper and resource-heavy architectures such as ResNet-50, BERT, as well as other large transformer-based architectures that require a considerable amount of computing resources to maintain but offer improved accuracy and broader analysis capabilities when processed from cloud environments \cite{He2016ResNet,Devlin2019BERT}.

Cloud-native models can also be very complex and resource-intensive. Larger models designed for advanced tasks need high computational power and significant memory resources. These models include ResNet-50, BERT, or even larger transformer architectures, which require more resources than are suitable for edge execution but can achieve better accuracy and more profound analysis when executed in cloud environments \cite{He2016ResNet,Devlin2019BERT}.


\textbf{Compression Techniques}: Various model compression techniques enable deploying more complex models on resource-constrained edge devices by reducing model size and computational requirements without significantly compromising performance. Fig.~\ref{fig:compression} summarizes these techniques. These techniques include quantization, model compression, and knowledge distillation, among others. In Quantization, the precision of weights and activations is reduced from 32-bit floating-point to lower-bit representations (e.g., 16-bit or 8-bit integers), drastically reducing model size and computational overhead \cite{Jacob2018Quantization}. In Pruning, the process involves removing less significant neurons, weights, or layers to reduce size and complexity, achieving substantial reductions in model size and inference time \cite{han2016deepcompression}. on the other hand, Knowledge Distillation transfers knowledge from a larger, complex model (teacher) to a smaller model (student), enabling comparable performance with fewer parameters, useful for edge deployment \cite{Hinton2015Distillation}.\par

On the other hand, other advanced techniques have emerged to further enhance model compression and efficiency. Neural Architecture Search (NAS) automates the design of efficient model architectures by searching a predefined space, optimizing for edge deployment \cite{Elsken2019NAS}. Layer-wise Adaptive Rate Scaling (LARS) adjusts learning rates of different layers during training, combining with other compression methods to fine-tune performance \cite{You2017LARS}. Federated Dropout uses different subsets of a model's parameters during training in a federated setting, reducing communication costs and yielding a smaller, more efficient model for edge deployment \cite{Caldas2018FederatedDropout}. These techniques contribute to effective model deployment across edge and cloud environments, ensuring that models are well-suited to their respective operational constraints while maintaining high performance. \par

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{Images/compression.pdf}
    \caption{An Overview of Compression Schemes for VLMs.}
    \label{fig:compression}
\end{figure}

\subsubsection{VLMs Implementation Across Distribution Locations}

Federated Learning (FL) is an innovative machine learning method that enables the training of models across decentralized devices or servers without requiring raw data exchange. This paradigm resolves critical issues about data privacy, security, and regulatory compliance, as it allows individual data sources (i.e., mobile devices or edge servers) to work together to train a shared global model without the need to share their data, keeping it stored locally. The FL method features multiple rounds of aggregating model updates from all devices involved in the training process, and the new global model is synthesized based on the received updates. It minimizes the possibility of private data breaches and reduces communication costs associated with centralized data processing. Federated Learning demonstrates the most promise in scenarios involving distributed, privacy-sensitive, and heterogeneous data, contributing to its applications in healthcare, finance, and IoT environments \cite{Kairouz2019AdvancesFL,Yang2019FederatedLearning,McMahan2017CommunicationEfficientFL}. State-of-the-art scalable and robust FL systems, coupled with advanced techniques such as model compression (to lower the model size), federated averaging (for communication efficiency), and differential privacy (to enhance efficiency in resource-constrained environments), have further improved FL systems \cite{McMahan2017CommunicationEfficientFL,Caldas2018FederatedDropout}. FL is still maturing, and it will be one of the building blocks of secure and efficient machine learning in the era of ubiquitous computing. 
\par

\paragraph{Model Partitioning}

The setup of FL starts with the \textbf{model partitioning}, which determines the architecture of the parent model hosted on the cloud (more computation-heavy) and the child model hosted on the edge side (computationally lightweight). The Parent-Child Model Setup is important: The child model is local, light on resources, and only has to perform simple tasks; the Parent model is more complex and is responsible for pushing updates. A child model capable of processing independently for online inference with minimal on-device memory is achieved using the hierarchical architecture. On the other hand, the cloud model deals with model updates and computationally intensive tasks \cite{Kairouz2019AdvancesFL,Yang2019FederatedLearning}.

Then, the next stage is delegating the jobs. As processes (real-time predictions, local data processing, etc.) are done in the edge model with low latency (or even with no latency), it stands to reason that the situation can be similar to Interaction with Cloud. On the other hand, the cloud model executes resource-intensive tasks, including model aggregation, complex analytics, and batch updates. This division of tasks permits leveraging the available edge and cloud resources efficiently \cite{McMahan2017CommunicationEfficientFL,Sattler2019RobustFL}. \par



\paragraph{Data Distribution Policy}

Once models are separated, {a data distribution policy} is implemented. In contrast, local data handling is seen as relevant to processes on the edge, which generate updates or predictions locally and communicate less frequently with the cloud.


\begin{enumerate}

  \item \textbf{Selective Uploading}: These methods determine which part of the processed data are essential to upload to the cloud, with only significant information related to the update will be sent or data needed deeper analysis \cite{Smith2017FederatedMultiTask,Yurochkin2019BayesianNonparametricFL}. Keeping only the most relevant data to upload reduces the amount of data transferred over the network and increases the system's efficiency.



  \item \textbf{Model Update Mechanism}: It maintains the synchronization of the edge model with the compatibility and component updates of the cloud model. Local training: The child model can be trained using edge devices where the input data is collected locally, leaving the limited computational capacity behind.



  \item \textbf{Federated Averaging Algorithm}: The updates from the different edge devices are averaged on the cloud to update the global model to achieve better hypotheses. Federated Averaging enables the international model to gain insights from non-independent data sources without revealing them to others, which minimizes the probability of data leakages \cite{McMahan2017CommunicationEfficientFL,Caldas2018FederatedDropout}.



  \item \textbf{Model Synchronization}: This ensures that the child model stays frozen with global model updates. Periodic sync lets the cloud return model parameters to the edge device so the child model can benefit from collective learning happening on devices. In this way, the model remains accurate and efficient across the FL system while simultaneously synchronizing and performing local training \cite{Sattler2019RobustFL,Konečný2016FederatedOptimization}.

\end{enumerate}

\subsubsection{Post Processing and Evaluation}

Post-processing and evaluation are essential stages in the FL system life-cycle, ensuring that both local and global models achieve optimal performance across heterogeneous environments. The first step in post-processing is \textbf{Local Evaluation}, which continues to measure the child model performance on the edge device using local data. It is important to notice performance degradation, which signals a model to be updated or updated from the cloud. New approaches utilize continual learning and on-the-fly performance tracking to adjust the model dynamically to the fluctuating edge contexts \cite{Yao2022EdgeFL}. As indicated by \cite{Wang2021EdgeContinualLearning}, lightweight performance estimation and anomaly detection algorithms are deployed locally to monitor changes in the model behavior, triggering invocation of deployment requests to the cloud.

The Feedback Loop is an integral part of local evaluation as it helps understand how the edge model performs and what kind of data or scenarios the edge model finds difficult. This feedback allows the global model to be refined by showing where more training or tweaks are needed. Many researchers use reinforcement learning and meta-learning to let the edge model self-improve in the long run. At the same time, this data also enhances the global model \cite{Li2021FederatedMetaLearning}. 

Global Model Evaluation: It merges data and updates from multiple edge devices in the cloud to comprehensively evaluate the global model. This aggregation enables the cloud to evaluate global model performance in heterogeneous environments and user space. Federated evaluation frameworks have been developed using privacy-preserving methods to aggregate performance metrics without revealing individual user data \cite{Xu2022FederatedEval}. These frameworks leverage secure aggregation and differential privacy to ensure correct and secure global model evaluation.

Model Tuning on the Aggregation: A global model is fine-tuned on the server side based on the aggregated updates from edge devices. This tuning process captures edge-level nuances in data dynamics and contributes to increasing the generalizability of the global model. Sophisticated approaches, such as federated hyperparameter tuning and federated Bayesian optimization, are employed to ensure the global model remains adapted to different conditions and devices \cite{Zhang2021FederatedHyperparameter}.

\subsubsection{Deployment and Continuous Learning}

Deployment and continuous learning are essential components of FL systems, ensuring that models are effectively updated, scaled, and adapted to changing environments. However, recent progress has led to various state-of-the-art (SOTA) methods that improve these processes to enable more robust and scalable FL.

\textbf{Model Update Deployment} — Frequent model parameters are released from the cloud to edge devices as model updates to ensure Ongoing Learning of the Edge devices. This integration ensures that edge devices are always using the latest model updates. Emerging approaches focus on transferring lightweight model updates instead of complete model files and applying differential synchronization mechanisms to limit the pass-through data between cloud and edge, mitigating latency and bandwidth consumption \cite{Liu2024ContinuousFL}. Models with sparse updates, such as federated dropout, only require the update of critical model parameters, leading to higher efficiency in using limited network resources, as proposed by the work in \cite{Zhang2024SparseUpdates}.


Edge Device Management ensures efficient deployment of updates across multiple edge devices. Modern approaches leverage orchestration frameworks that automate deployment, ensuring timely updates for all devices. These frameworks often employ decentralized deployment strategies, where updates are propagated in a peer-to-peer fashion, reducing load on central servers and improving scalability \cite{Wang2024EdgeDeployment}. Adaptive deployment techniques manage heterogeneous edge environments, tailoring updates to each device's capabilities \cite{Chen2024AdaptiveFL}.


A \textbf{Scalable Architecture} is essential for handling an increasing number of edge devices. SOTA methods focus on creating architectures that can scale horizontally by adding more edge nodes and vertically by enhancing cloud resources. Cloud-native technologies like Kubernetes and containerization have been widely adopted to manage the deployment of FL models across large clusters of edge devices \cite{Xu2023KubeFL}. These technologies allow for dynamic scaling, ensuring that the system can handle the aggregation of updates from a growing number of devices without bottlenecks.

Adaptation to New Data is another critical aspect of continuous learning. As data environments evolve, FL systems must adapt to new data types and patterns to maintain model accuracy. Techniques such as continual learning and federated meta-learning enable models to adapt without forgetting previously learned information \cite{Yao2024ContinualFL}. Moreover, integrating reinforcement learning into FL systems allows models to adjust learning strategies dynamically based on environmental changes, ensuring sustained performance even in non-stationary environments \cite{Zhu2024RLFL}.

Existing Vision-Language Models face substantial limitations when deployed on edge devices due to high computational and memory requirements for complex visual and language processing. These models are often designed for cloud environments with abundant resources, making them challenging to run efficiently on resource-constrained devices such as IoT gateways and mobile robots. Edge devices typically lack the hardware for large-scale data processing, leading to issues with latency, limited real-time processing capabilities, and restricted power efficiency. Additionally, current models struggle to adapt across different visual data modalities (e.g., RGB and depth images) and frequently rely on centralized cloud-based data aggregation, raising concerns about data privacy and network dependency.

To overcome these limitations, it is essential to develop Vision-Language Models optimized for edge deployment. These models should incorporate lightweight architectures that balance computational efficiency with high performance. Techniques such as quantization, pruning, and knowledge distillation effectively reduce model size and computational demand without compromising accuracy. Advanced methodologies like Neural Architecture Search and Federated Learning enhance adaptability in distributed, resource-limited environments while maintaining data privacy \cite{khan2023joint}. Future models must also support flexible, real-time processing across various visual modalities and incorporate continual learning mechanisms to adapt dynamically to new data patterns and evolving tasks in diverse edge applications.


\begin{figure*}[t]
    \centering
   \includegraphics[width=0.8\textwidth]{Images/VLM.pdf}
    \caption{Illustration of representative tasks from three categories of Vision-Language (VL) problems, image-text tasks, vision tasks as VL, and video-text tasks \cite{visionlanguagepretrainingbasicsrecent}.}
    \label{fig:VL Overview}
\end{figure*}

\section{Recent Advances}

Applications of lightweight VLMs are expanding rapidly across various industries, driven by the need for efficient, on-device multi-modal processing. There are many tasks where VLMs can help, including Text-Image Retrieval, image captioning, Question and answer Classification, object detection, and segmentation, as shown in Figure \ref{fig:VL Overview}. These models are being deployed in fields such as autonomous systems, healthcare, surveillance, environmental monitoring, and many other applications. Table \ref{tab:vlm_applications} summarizes the applications we covered in this survey.

\begin{table*}[ht!]
\centering
\renewcommand{\arraystretch}{1.5} % Increases the row height
\caption{Summary of Vision-Language Model Applications in Different Domains}
\label{tab:vlm_applications}
\begin{tabularx}{\textwidth}{p{3cm} X} % Removed borders here
\hline
\textbf{Application Domain} & \textbf{Application and Description} \\ \hline

\multirow{5}{3cm}{Medical Domain} 
& Bilingual Medical Mixture LLM: BiMediX enables multilingual medical interactions, improving task efficiency in both English and Arabic \cite{li2024bimediX}. \\  
& Medical Visual Question Answering: ViLMedic supports visual question answering and radiology report generation on edge devices for real-time medical data interaction \cite{delbrouck2022vilmedic}. \\  
& Computer-Aided Diagnosis: MedBLIP provides zero-shot classification for Alzheimer’s disease using 3D images and electronic health records \cite{chen2023medblip}. \\  
& Ultrasound-Guided Diagnosis: COVID-LWNet uses ultrasound images to classify lung conditions, performing efficiently on portable devices \cite{karar2021covid}. \\  
& Medical Report Generation: VLMs are integrated with large language models to automate radiology report generation \cite{yang2023customizing}. \\ \hline

\multirow{4}{3cm}{Environmental Monitoring \\ and Aerial Imaging} 
& Aerial Imaging for Environmental Change Detection: A bi-modal transformer-based VLM helps track and assess environmental changes using aerial imagery \cite{bazi2022bimodal}. \\  
& Climate and Land-Use Analysis: SATIN classifies satellite imagery for tasks like deforestation monitoring, optimized for resource-constrained devices \cite{roberts2023satin}. \\  
& UAV Navigation for Environmental Surveys: AerialVLN enables UAVs to navigate complex environments for wildlife tracking and environmental assessments \cite{liu2023aerialvln}. \\  
& Remote Sensing Change Detection: ChangeCLIP improves surface change detection by integrating multimodal data from bitemporal satellite images \cite{DONG202453}. \\ \hline

\multirow{5}{*}{Autonomous Systems} 
& Autonomous Driving and Transportation Systems: VLMs assist autonomous vehicles by enhancing traffic scene understanding and decision-making \cite{zhou2023vlm_autonomous}. \\  
& Language Prompts in Autonomous Driving: NuPrompt integrates vision-language prompts to improve vehicle response to natural language commands in complex driving scenarios \cite{wu2023languageprompt_autonomous}. \\  
& Real-Time Object Detection in Traffic: A VLM system improves object detection under diverse weather conditions in urban intersections \cite{gao2022citytraffic_autonomous}. \\  
& UAV-Based Autonomous Navigation: AerialVLN supports autonomous navigation for UAVs using visual and language inputs for environmental monitoring \cite{liu2023aerialvln}. \\  
& Disaster Response in Autonomous Systems: Crisscross Vision Transformers process aerial imagery for real-time disaster response decision-making \cite{deng2023crisscross}. \\ \hline

\multirow{4}{*}{Surveillance} 
& Urban Dynamics and Policy Compliance: A vision-language model tracks changes in urban activity and policy compliance using dashcam data \cite{chowdhury2021tracking}. \\  
& Threat Detection : VLMs analyze aerial and ground-level imagery to detect potential threats in public areas \cite{wen2023visionlanguage_remote}. \\  
& Real-Time Activity Monitoring: monitors crowded public spaces, identifying anomalies and suspicious behaviors \cite{du2023success_vqa}. \\  
& Smart City Surveillance: VLMs process multimodal data for real-time even monitoring in urban environments \cite{li2023vision}. \\ \hline

\end{tabularx}
\end{table*}




\subsection{VLM in healthcare}

VLMs are increasingly important in various medical applications. Their simultaneous processing of visual and textual data allows for more accurate diagnoses and effective medical workflows. Below are some applications of VLMs in the medical domain, particularly focusing on lightweight models designed to work on edge devices.

\textbf{Bilingual Medical Mixture LLM}  
\cite{li2024bimediX} propose BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. BiMediX supports various medical tasks, including multi-turn dialogues, multiple-choice questions, and open-ended queries. We developed a semi-automated English-to-Arabic translation pipeline and a comprehensive evaluation benchmark for Arabic medical LLMs. We also present BiMed1.3M, a bilingual dataset of 1.3 million medical interactions, which powers the model's instruction tuning. BiMediX outperforms state-of-the-art models Med42 and Meditron, offering 8-times faster inference, and exceeds Jais-30B on both Arabic and bilingual medical benchmarks.

\textbf{Medical Visual Question Answering (VQA)}  
Integrating VLMs in medical visual question answering tasks can help doctors make faster and more informed decisions. \cite{delbrouck2022vilmedic} developed ViLMedic, a multimodal framework that supports a variety of medical tasks such as visual question answering and radiology report generation. This framework includes multiple pretrained models designed for efficient deployment on edge devices, enabling real-time interaction with medical data.

\textbf{Computer-Aided Diagnosis (CAD)}  
Another key application is computer-aided diagnosis (CAD). \cite{chen2023medblip} proposed MedBLIP, a lightweight VLM designed to analyze 3D medical images and electronic health records for Alzheimer’s diagnosis. The model uses pre-trained image encoders and large language models to provide accurate zero-shot classification for Alzheimer’s disease. This approach demonstrates the viability of VLMs in providing real-time CAD support on resource-constrained devices.

\textbf{Ultrasound-Guided Diagnosis for COVID-19}  
Portable ultrasound devices are critical in diagnosing diseases like COVID-19. \cite{karar2021covid} proposed COVID-LWNet, a lightweight deep learning model designed to classify lung conditions using ultrasound images. The model showed excellent performance on edge devices, making it suitable for field deployment where real-time diagnostic support is essential.

\textbf{Medical Report Generation}  
VLMs have also been used to generate medical reports based on visual data automatically. \cite{yang2023customizing} introduced a system that integrates VLMs with large language models to generate detailed medical reports. This system has shown great potential in automating radiology report generation and improving efficiency and accuracy in clinical settings.


\subsection{VLMs in Environmental Monitoring and Aerial Imaging}

VLMs are increasingly being employed in environmental monitoring, including tasks such as aerial imaging, environmental change detection, and disaster assessment. These models are particularly valuable when deployed on edge devices for real-time monitoring in remote or resource-constrained areas. Below are some of the key applications of VLMs in this domain, with a focus on lightweight models designed for edge deployment.

\textbf{Aerial Imaging for Environmental Change Detection}  
One of the critical applications of VLMs in temporal change detection involves monitoring geographic landscapes to support environmental analysis and urban development planning. To address limitations in capturing dynamic shifts, this study introduces an annotated dataset of video frame pairs to trace evolving geographical features over time. Building on techniques like Low-Rank Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning, models such as Video-LLaVA and LLaVA-NeXT-Video are fine-tuned to achieve high accuracy in tracking and describing land-use transformations. GeoLLaVA \cite{geollava} models demonstrate notable performance gains, achieving a BERT score of 0.864 and a ROUGE-1 score of 0.576, underscoring their enhanced capabilities for precise, temporal environmental monitoring.


\textbf{Vision-Language Models for Climate and Land-Use Analysis}  
\cite{roberts2023satin} introduced SATIN, a multi-task metadataset designed for classifying satellite and aerial imagery using VLMs. The model is optimized for environmental applications like land-use planning and deforestation monitoring. It shows high transfer performance in zero-shot classification tasks, making it effective for rapid deployment in environmental surveys on resource-constrained devices.


\textbf{Vision-Language Navigation for UAVs in Environmental Surveys}  
VLMs are also used in UAV-based environmental navigation. \cite{liu2023aerialvln} introduced AerialVLN, a vision-language navigation model designed for UAVs. This model enables UAVs to navigate complex environments while performing environmental surveys or wildlife tracking. AerialVLN's lightweight architecture makes it ideal for deployment on UAVs, enabling real-time decision-making during aerial environmental surveys.


\textbf{Remote Sensing Change Detection (RSCD)}  
ChangeCLIP is a novel framework for remote sensing change detection (RSCD) that leverages the vision-language model CLIP, which aims to improve the detection of surface changes from bitemporal images \cite{DONG202453}. While traditional methods primarily focus on visual representation, ChangeCLIP integrates multimodal data by reconstructing CLIP to extract bitemporal features and proposing a differential features compensation module to capture detailed semantic changes. Additionally, it introduces a vision-language-driven decoder that enhances image semantics by combining image-text encoding with visual features during decoding. ChangeCLIP achieves state-of-the-art results on five RSCD benchmark datasets: LEVIR-CD (85.20\%), LEVIR-CD+ (75.63\%), WHUCD (90.15\%), CDD (95.87\%), and SYSU-CD (71.41\%).



\subsection{Autonomous Applications of VLMs}

VLMs have significant applications in autonomous systems, particularly in autonomous driving. These models enhance autonomous systems by combining visual and linguistic data for a deeper understanding of the environment. Below are key applications:

\textbf{VLMs in Autonomous Driving and Intelligent Transportation Systems}  
\cite{zhou2023vlm_autonomous} studied VLMs in autonomous driving, showing how integrating visual inputs with natural language processing enhances decision-making for driving safety and efficiency. The paper highlights advances and challenges in object detection, traffic scene understanding, and decision-making.

\textbf{Language Prompts in Autonomous Driving}  
\cite{wu2023languageprompt_autonomous} introduced NuPrompt, a vision-language model using prompts to enhance understanding of natural language commands in driving scenes. With a novel benchmark dataset, NuPrompt demonstrated applications in tracking objects and informed decision-making.

\textbf{Real-Time Object Detection in Urban Traffic}  
\cite{gao2022citytraffic_autonomous} proposed a 2-stage VLM system for traffic object detection and classification under varying weather and traffic conditions. It enhances object recognition in urban intersections, improving safety for vehicles and pedestrians.

\textbf{Autonomous Navigation Using Vision-Language Models}  
\cite{liu2023aerialvln} developed AerialVLN, a VLM for UAV-based navigation. It combines visual and language inputs for tasks like environmental monitoring and search-and-rescue missions, enabling real-time decision-making in complex environments.

\textbf{Crisscross Vision Transformers for Autonomous Disaster Response}  
\cite{deng2023crisscross} introduced a Crisscross Vision Transformer for disaster-prone areas. By processing aerial imagery and textual descriptions, the model supports real-time decision-making in dynamic environments like floods and landslides.


\subsection{Surveillance Applications of VLMs}

VLMs have emerged as transformative tools in surveillance due to their ability to integrate visual and textual information, which enhances real-time monitoring, anomaly detection, and decision-making. These models provide contextual insights into scenes and human behavior, making them essential in various surveillance applications.

In urban surveillance, lightweight VLMs facilitate real-time anomaly detection, facial recognition, and scene analysis, all critical for smart city environments where low latency and immediate response are required~\cite{techhq2024}. In healthcare, VLMs extend to medical imaging diagnostics by analyzing X-rays, MRIs, and related reports to deliver on-device diagnostics, which is vital in remote or field hospital settings with limited cloud connectivity~\cite{cai2024self}. Furthermore, VLMs enhance smart home interactions by processing visual and natural language commands. For example, smart cameras equipped with VLMs can describe scenes, enhancing the user experience with IoT devices~\cite{vik2024moondream}. VLMs also contribute to environmental monitoring and precision agriculture, analyzing satellite and drone imagery to optimize water usage, detect pests, and monitor environmental changes in real-time~\cite{cai2024self}. Retail and e-commerce industries also benefit from VLMs, as they support visual search, recommendations, and inventory management, enabling customers to search products with images on mobile devices~\cite{vik2024moondream}.

\textbf{Tracking Urban Dynamics and Policy Compliance}  
\cite{chowdhury2021tracking} proposed a VLM-based sensing model to monitor urban activity across New York City using dashcam data, generating text-based descriptions to track changes in urban patterns and compliance with social distancing. This approach reduced storage requirements and addressed privacy concerns, making it suitable for large-scale urban surveillance.

\textbf{Surveillance and Threat Detection in Public Spaces}  
\cite{wen2023visionlanguage_remote} examined VLMs in remote sensing for threat detection, focusing on public areas. By combining image captioning and object detection, the model analyzes aerial and ground-level imagery to identify potential threats, such as abandoned objects or suspicious behaviors. The integration of visual data with language descriptions enhances interpretability in security contexts.

\textbf{Monitoring Real-Time Activity in Crowded Areas}  
In 2023, \cite{du2023success_vqa} introduced a VLM for detecting predefined behaviors in crowded environments. This model analyzes visual and textual inputs to flag successful or suspicious activities, making it valuable for crowd management and anomaly detection by providing contextual insights to human operators.

\textbf{Vision-Language Models for Smart City Surveillance}  
Smart city surveillance requires processing multimodal data for efficient decision-making. \cite{li2023vision} developed a model to predict human activity in urban settings by integrating visual and language inputs. Connected to city-wide sensors, this model supports automatic detection of unusual events like unauthorized vehicle entry or loitering, enhancing urban safety.

\section{Open Challenges}
This section presents novel challenges in advancing VLMs for edge applications. Recent surveys highlighted several critical issues. Table \ref{tab:vlm_challenges} shows the challenges we introduce and the difference between these and previous discussions. The alignment gap between visual and textual modalities reduces effectiveness in tasks like image captioning and question answering~\cite{surveyvisionlanguagepretrainedmodels, visionlanguageintelligencetasksrepresentation}. VLMs depend on large-scale datasets, which are costly to develop and limit accessibility for smaller institutions~\cite{ghosh2024exploringfrontiervisionlanguagemodels}. High computational demands hinder deployment on resource-constrained devices~\cite{efficientmultimodallargelanguage, SurveyofEfficientFine-TuningMethods}, and VLMs often fail to generalize across domains, underperforming on novel tasks~\cite{zhang2024visionlanguagemodelsvisiontasks}. Additionally, parameter inefficiencies in dual-stream architectures increase memory usage, making them less suitable for real-time or edge-based applications~\cite{surveyvisionlanguagepretrainedmodels}. These challenges underscore the need for innovative approaches distinct from current solutions in the literature.

The surveyed papers address various challenges associated with deploying LLMs on edge devices. Cai et al. \cite{cai2024selfadapting} discuss high computational demands and reliance on large-scale datasets, limiting VLM deployment on resource-constrained devices, along with an alignment gap between visual and textual modalities. Bhardwaj et al. \cite{surveyintegrationoptimization} emphasize optimization to balance computational demands, energy efficiency, and model scalability for edge devices. Lu et al. \cite{vliai20survey} address generalizing VLMs for edge AI, including AI integration into wearables, hardware miniaturization, and user-friendly interfaces. Qu et al. \cite{mobileedgeintelligence} propose mobile edge intelligence to bridge cloud and on-device AI, highlighting privacy concerns, latency issues, and resource limitations of edge devices. Lin et al. \cite{pushing6Gedge} focus on deploying VLMs in 6G edge networks, identifying challenges like response times, bandwidth costs, and privacy risks. Yuan et al. \cite{generativeinferenceoverview} discuss energy efficiency and computational constraints for mobile devices running VLM inference tasks, proposing approaches to enhance energy efficiency. Qu et al. \cite{mobileedgeintelligencemodern} examine limitations of on-device LLMs due to edge devices' constrained capacity, advocating mobile edge intelligence to reduce latency and privacy issues. Chen et al. \cite{edgegeneralintelligence} explore LLM integration into edge intelligence, focusing on adaptive applications and throughput challenges for small models on edge devices. Lee et al. \cite{visiontransformermobileedge} discuss adapting Vision Transformers for mobile and edge devices, emphasizing computational efficiency and implementation challenges on resource-limited devices.





\begin{table*}[ht!]
\centering
\renewcommand{\arraystretch}{1.5} % Increases the row height
\caption{Challenges Discussed in Surveys on Vision-Language Models for Edge Devices}
\label{tab:vlm_challenges}
\begin{tabularx}{\textwidth}{p{5cm} X} % Removed borders here
\hline
\textbf{Reference} & \textbf{Challenges Discussed} \\ \hline

Cai et al. (2024) \cite{cai2024selfadapting} 
& High Computational Demands, Large Dataset Dependency, Visual-Text Alignment Issues. \\ \hline

Bhardwaj et al. \cite{surveyintegrationoptimization} 
& Edge Deployment Challenges, High Computation, Energy Efficiency, Scalability. \\ \hline

Lu et al. (2024) \cite{vliai20survey} 
& Generalization, AI in Wearables, Miniaturizing Hardware, User Interface Design. \\ \hline

Qu et al. (2024) \cite{mobileedgeintelligence} 
& Cloud-Edge Gap, Privacy And Latency Issues, Limited Edge Device Resources. \\ \hline

Lin et al. (2023) \cite{pushing6Gedge} 
& Long Cloud Response Times, High Bandwidth Costs, Privacy, 6G Edge Potential. \\ \hline

Yuan et al. \cite{generativeinferenceoverview} 
& Limited Battery, Computing Power, Energy Efficiency For Mobile Devices. \\ \hline

Qu et al. (2024) \cite{mobileedgeintelligencemodern} 
& Limited On-Device Capacity, Cloud Privacy And Latency, Mobile-Edge Intelligence. \\ \hline

Chen et al. \cite{edgegeneralintelligence} 
& Edge Model Adaptability, Performance Evaluation, Throughput For Small Models. \\ \hline

Lee et al. (2024) \cite{visiontransformermobileedge} 
& Compact Vision Transformer Design, Performance-Efficiency Balance, Edge Deployment. \\ \hline

Ours 
& Compressed Lightweight VLMs, VLMs Optimization, Distributed Implementation, Context-Aware VLMs, Cross-Modality Learning And Adaptation For Multi-Sensor Applications, Security, Privacy, Communication Model For Edge VLMs. \\ \hline

\end{tabularx}
\end{table*}


%Recent advancements in Vision-Language Models (VLMs) have centered on making these models more efficient and resource-conserving, making them suitable for edge devices with limited computational resources. This section outlines both technical trends and applications in the field.


%\textbf{Model Compression Techniques}
\subsection{Compressed Light Weight VLMs for Edge Networks}
{\em How does one propose novel compressed light-weight VLMs for edge networks with a reasonable performance?} Generally, it is challenging to extend VLMs to the network edge due to high computing resource requirements for training complex VLM models with large number of parameters. To do resolve this challenge, one way is to use compression schemes. Several model compression techniques are being explored to run large VLMs on edge devices. \textit{Knowledge distillation} and \textit{quantization} are two prominent approaches to reduce model size and minimize computation time without sacrificing too much performance. Knowledge distillation transfers knowledge from a larger model to a smaller one, allowing the lightweight version to retain the essential functionalities of the original. For instance, the EdgeVL framework utilizes dual-modality knowledge distillation to support both RGB and non-RGB images, reducing the model size by up to 93 times and improving accuracy by 15\% on edge devices~\cite{cai2024self,vik2024moondream}. Similarly, quantization-aware training helps adapt models for lower precision, conserving memory and power while maintaining accuracy. Although these existing schemes performs better, we might need novel compression schemes for specific applications (e.g., VLMs for medical imaging at the network edge and remote sensing assisted by drones) on VLMs at the network edge. Therefore, there is a need for more novel schemes for various applications. For instance, MiniVLM utilizes knowledge distillation to significantly reduce the model size for edge deployment while maintaining over 90\% of the original performance, making it suitable for cross-modal retrieval tasks \cite{minivlm2022}. Another approach, DIME-FM, distills large VLMs like CLIP into smaller, more efficient models using unpaired image and text data. This method maintains transferability and robustness, making it suitable for resource-constrained edge applications such as real-time image and text matching tasks \cite{dmt2023}.


\subsection{Visual-Language Models Optimization for Edge Networks}
{\em How do we optimize VLMs models architecture for edge devices in terms of performance and complexity?} Another technical trend involves optimizing the \textit{visual encoder} and \textit{language model components}. Researchers are focusing on balancing these two components to achieve efficiency in resource-constrained environments. For example, the \textit{Imp} project explores the use of smaller LLMs like Phi-2 and optimized visual encoders like \textit{SigLIP}, which perform better than traditional CLIP-based encoders~\cite{cai2024self,github2024edgevl}. This results in a better generalization when deployed on edge devices. These advancements significantly reduce the required computational power, making models more suitable for mobile and embedded systems. Many recent applications of lightweight VLMs are tailored for specific edge use cases, such as real-time analysis for drones, robots, and surveillance systems. For instance, the \textit{Moondream2} model is designed to be highly efficient and able to process complex vision-language tasks like interpreting security footage and performing remote inspections~\cite{vik2024moondream, techhq2024}. These models run on as little as $5$GB of memory, making them ideal for fully remote use cases where continuous connectivity cannot be guaranteed. A significant technical trend is the adaptation of VLMs to work with various modalities, such as depth and thermal cameras, alongside traditional RGB images. This cross-modality adaptation is crucial for applications in autonomous systems, where VLMs must process diverse visual inputs. EdgeVL, for instance, employs \textit{cross-modality learning}, enabling efficient operation across different input types while preserving performance, which is essential for robots and autonomous systems operating in dynamic environments~\cite{cai2024self,simple2024}.

\subsection{Distributed Implementation of Edge VLMs}
{\em How do we enable distributed implementation of VLMs for various applications?} Federated learning and edge computing are emerging as critical enablers for distributed lightweight VLMs. \textit{Federated learning} allows models to be fine-tuned directly on edge devices without transferring sensitive data to the cloud, preserving user privacy and reducing latency~\cite{cai2024self}. This is particularly important in healthcare and security applications with high data sensitivity. In parallel, edge computing enables these models to process data closer to the source, improving response times and making real-time decision-making more feasible~\cite{vik2024moondream}. Besides the applications we mentioned earlier, there are some trends in which VLMs are gaining significant traction due to their ability to perform complex tasks on resource-constrained edge devices. In autonomous systems such as drones, robots, and autonomous vehicles, VLMs are utilized for real-time object detection, scene understanding, and navigation, allowing these systems to operate without reliance on cloud computing. For example, drones equipped with VLMs can monitor wildlife, inspect infrastructure, and assess environmental conditions, which has crucial implications for disaster relief and agriculture~\cite{cai2024self}.


%The current advancements in lightweight Vision-Language Models (VLMs) open up a wide range of opportunities for future work, both in terms of technical contributions and application development. Below are several ideas for projects that can further explore and develop the capabilities of lightweight VLMs across different domains.

%\textbf{Optimizing Lightweight VLM Architectures for Edge Devices}: One technical direction for future work could be optimizing lightweight VLM architectures for edge devices by further exploring model pruning and quantization techniques. This would allow researchers to create even smaller models that maintain performance. A promising area of exploration is improving quantization-aware training to minimize performance loss when handling multiple modalities such as RGB, depth, and infrared inputs. Additionally, enhancing multi-modal knowledge distillation methods could transfer knowledge from large, multi-modal models to smaller edge-ready versions, allowing for more efficient processing without sacrificing functionality.
\subsection{Context-Aware VLMs for Edge Networks}
{\em How do we propose VLMs for edge networks with context-awareness?} Extending VLMs to the network edge by using the edge data for further training (i.e., context-aware) of pre-trained models is necessary for many applications to adapt to specific scenarios. Meanwhile, the models deployed at the network edge must be lightweight during extension. Recent advancements in context-aware VLMs have emphasized the development of lightweight, task-specific architectures suited for edge networks where computational resources are limited. These models, such as EM-VLM4AD, MiniDrive, and LiteViLA, incorporate several techniques to efficiently manage multimodal data by reducing model complexity without significantly compromising performance \cite{gopalkrishnan2024emvlm4ad, cheng2024litevila, curto2023uavsceneunderstanding}. One of the core techniques used in lightweight VLMs involves leveraging efficient image embedding mechanisms, such as ViT-based patch projections and gated pooling attention, allowing multi-view image data to be processed with minimal latency. EM-VLM4AD, for example, flattens image patches and performs gated pooling to facilitate processes requiring fewer resources to produce a single representation by compressing and summarizing multiple views before fusing a language model (e.g., T5-base) for question-answering based tasks in autonomous driving applications \cite{gopalkrishnan2024emvlm4ad}. It reduces inference time and achieves superior accuracy in several tasks, especially in path planning and traffic behavior analysis. For example, LiteViLA employs a Mixture of Adapters (MoA) approach, dynamically activating lightweight adapters specialized for individual subtasks, including object detection and scene understanding, resulting in efficient resource allocation and allowing for a diverse array of tasks to be performed \cite{cheng2024litevila}. LiteViLA supports different edge tasks in autonomous systems, and such modularity provides robustness for every operational condition. Moreover, models originally proposed for drones \cite{curto2023uavsceneunderstanding} also combine lightweight portions, such as YOLOv7 detectors, with VLM architectures to generate real-time object detection and scene description. These models focus on low latency by implementing simple structures of encoder-decoder and use quantized or pruned LLMs (GPT-3, TinyLLaVA) for the language, which makes them ideal for use in applications where power and performance are the most significant constraints.




\subsection{Cross-Modality Learning and Adaptation for Multi-Sensor Applications}
{\em How does one enable VLMs to effectively integrate and adapt to diverse sensor modalities, such as thermal, depth, and hyperspectral data in many edge applications?} Cross-modality learning and adaptation have emerged as one of the most recent and important technologies in promoting the advancement of VLMs to work across various sensor types, including thermal, depth, and hyperspectral modalities. It uses data observed from multiple domains to boost the ability of VLMs to perceive a scene, which is a fundamental requirement for tasks related to autonomous systems, robot control, and environmental monitoring. For example, the models ViPT \cite{zhu2024crossmodality}, UC2 \cite{cheng2024visualprompt}, and CMT \cite{tian2023cmt} employed cross-modal fusion methods combining thermal and RGB data into a shared feature space that enables coherent interpretation in all respective modalities. ViPT is designed to build on pre-trained RGB-based models that can serve as multi-modal backbones for new tasks like tracking, where RGB and depth data can be fused through a transformer-based encoder that incorporates modality-complementary prompters \cite{cheng2024visualprompt}. Likewise, depth estimation models commonly rely on RGB and thermal data fusion using dedicated networks, such as a 3D cross-modal transformation module, which aligns data from separate modalities to increase depth prediction accuracy in dim lighting scenarios \cite{tian2023cmt}. These models have recently been explored to generate confidence maps and align the modalities originating from the various sensors to select the most accurate one, thus enhancing outputs from the complex multi-sensor setups. These cross-modal approaches have proven successful in various fields outside of robotics. Analogous fusion techniques in environmental monitoring and agriculture would allow models to deal with multimodal data, for example, from drones (hyperspectral imagery) or IoT sensors instead of drone imagery more tightly connected to the ecosystem monitoring and precision agriculture use cases \cite{zhu2024crossmodality, curto2023uavsceneunderstanding}.

\subsection{Security}
{\em How do we enable edge VLMs while ensuring security?} The deployment of lightweight VLMs in both cloud and edge environments introduces significant security challenges, as these models are susceptible to a range of attacks. Due to their large-scale usage, VLMs are exposed to risks like model inversion attacks, which allow adversaries to reconstruct training data from model outputs, compromising privacy. This vulnerability arises from their reliance on shared representations across modalities, making them targets for privacy-related attacks \cite{usynin2022beyond, zhou2023boosting}. Recent research has focused on enhancing robustness against adversarial attacks. Adversarial training incorporates noise into predictions, reducing the effectiveness of adversarial examples. Robust gradient masking methods limit adversaries' ability to exploit gradients \cite{zhang2022towards, zhou2023boosting}. Ensemble-based defenses, such as randomized input transformations and multi-layer protection, provide a layered security approach. Additionally, using adversarial examples to "boost" defenses prevents unauthorized data reconstruction from VLM outputs \cite{usynin2022beyond, zhang2022towards}.\par

When VLMs are deployed in distributed settings, ensuring secure data transmission between edge devices and cloud servers is critical. Without proper encryption, attackers can intercept or manipulate transmitted data. Secure communication protocols like AES-256 encryption and lightweight frameworks ensure robust protection without performance loss \cite{wen2021defending}. Dynamic key exchange protocols provide real-time key generation, improving resistance to attacks. Secure Multiparty Computation (SMC) techniques allow joint computations without revealing inputs \cite{bonawitz2017practical}. Edge VLMs are prone to hardware attacks, including tampering and malware. Trusted Execution Environments (TEEs) like ARM TrustZone mitigate these risks by securely running critical components, even in untrusted environments \cite{zhou2023boosting}. Blockchain-based solutions enhance protection against unauthorized firmware updates, maintaining edge device integrity \cite{rathore2023formal}.\par

Poisoning attacks involve inserting malicious data into VLM training sets, degrading performance. Federated learning environments are especially vulnerable. Byzantine-resilient federated systems detect and eliminate poisoned data without affecting performance \cite{lu2023setlevel}. Anomaly detection methods effectively filter out malicious updates. Securing aggregation of model updates in federated learning is vital to prevent data inference or performance degradation. Differential privacy techniques obscure individual updates while maintaining accuracy. Homomorphic encryption protects data during model aggregation \cite{park2022privacy}. Blockchain-based systems ensure transparency and prevent tampering \cite{bonawitz2017practical}. Trust frameworks and blockchain-based governance models maintain transparency in access control and secure deployment of VLMs. Ethical governance frameworks ensure compliance with security standards \cite{rathore2023formal}.
\par




\subsection{Privacy} 
{\em How do we propose privacy-aware VLMs?} The rapid adoption of VLMs in cloud-based systems has heightened concerns about user privacy. VLMs process multimodal data—such as personal images and text—often transmitted to remote servers, raising challenges in ensuring data confidentiality. Safeguarding sensitive information against unauthorized access is critical. Recent advancements focus on privacy-preserving techniques that enable secure inference and data handling without compromising performance. Federated learning improves privacy by keeping data localized while sharing only model updates, but privacy concerns remain due to possible inference of sensitive information from these updates. \cite{park2022federated} proposed a homomorphic encryption-based framework, allowing model updates to be computed without exposing raw data. Earlier, \cite{fang2021privacy} introduced the PFMLP framework using partially homomorphic encryption to protect data during federated learning with minimal accuracy loss. In 2023, \cite{shen2023privacy} introduced a privacy-preserving inference framework combining homomorphic encryption and random privacy masks, preventing access to raw input data with low computational overhead. Similarly, \cite{hussien2023secure} integrated secure multiparty computation (SMC) with homomorphic encryption to enhance federated learning systems.\par

Another privacy issue in cloud-based VLMs is the risk of membership inference attacks, where VLMs trained on private data collections lead to privacy concerns \cite{park2022homomorphic}. \cite{park2022homomorphic} introduced a federated learning framework encrypting model updates with homomorphic encryption to prevent sensitive data leakage. \cite{ma2021privacy} proposed a multi-key encryption design protecting against membership inference attacks by ensuring updates are encrypted and inaccessible to a single participant. Additionally, cloud-based VLMs need to address data ownership and control. Once uploaded, users lose control over their data. \cite{du2022signcryption} developed a federated learning framework for IoT systems, decentralizing client data and safeguarding against collusion attacks. Privacy laws like GDPR impose further constraints. \cite{sébert2023combining} proposed a framework combining differential privacy with homomorphic encryption for GDPR compliance, maintaining data utility without sacrificing model performance.\par

\subsection{Communication Model for Edge VLMs}  
{\em How does one enable communication resources efficient edge VLMs?} Training requires significant communication resources, especially for distributed VLMs at the network edge. Offloading model processing to the cloud introduces communication overhead. \cite{naveen2022memory} proposed an optimized distributed CNN framework to reduce memory footprint and communication overhead in edge-cloud setups. \cite{shaowang2021declarative} introduced a declarative framework optimizing data flows between edge devices and the cloud. Energy consumption is another major challenge. In 2023, \cite{fu2023energy} introduced an energy-efficient framework for NLP on edge devices leveraging heterogeneous memory architectures to reduce energy consumption while maintaining high performance. \cite{hu2021pipeline} developed EdgePipe, a distributed framework using pipeline parallelism to improve energy efficiency during inference, achieving speedups without sacrificing accuracy. 

Real-time inference is critical for edge applications. \cite{xu2023devit} proposed DeViT, a framework decomposing large vision transformers into smaller models for collaborative inference on edge devices, reducing latency and communication overhead. \cite{hu2021pipeline} also showed pipeline parallelism could speed up inference on heterogeneous edge devices, achieving high throughput with negligible accuracy drop. Generalizing VLMs across heterogeneous devices presents another challenge. \cite{dutta2023search} proposed DCA-NAS, enabling neural architecture search for diverse hardware configurations, allowing fine-tuned model designs for varying constraints. \cite{hu2021pipeline} demonstrated pipeline parallelism's adaptability to heterogeneous hardware, improving performance and flexibility.

\section{Conclusion and Future Directions}
\subsection{Conclusion}
In summary, this survey provides a comprehensive bottom line of recent advances, challenges, and opportunities for applying VLMs on edge devices. Vision-language models are strong, merging visual and language understanding to perform complex tasks, such as captioning images, visual question-answering, etc. This can be used for variance applications, such as smart surveillance, answering, video analysis, etc.

However, these models' widespread deployment and usage on edge devices are significantly limited due to the constraints of edge devices' processing capability, storage, and power. In order to make VLMs lightweight and efficient with low-performance degradation, these limitations can be approached through advanced optimization algorithms like pruning, quantization, knowledge distillation, and efficient hardware utilization. Next, we provide a thorough taxonomy with respect to model training and fine-tuning strategies, considerations for runtime deployment of VLMs to low-resource (edge) environments, and privacy and security. These unique capabilities make it possible to deploy VLMs on edge for several applications, such as real-time autonomous systems decision-making, privacy-preserving intelligent surveillance, and medical diagnostics in local regions. Nevertheless, open research problems remain to be solved, especially in developing interoperability solutions for massive edge deployment. We hope that future research can further develop the practical use of VLMs, which would lead to these models being a usable and efficient background for use in resource-constrained environments.

\subsection{Future Directions}
We anticipate that generalizing VLMs to the network edge will play an essential role in many real-time applications. Edge-based distributed VLMs can use less computing and communication resources and are thus more suitable for a broad range of applications. However, multiple challenges still exist to be solved despite all the advantages. Additional development is needed to create efficient learning schemes for specific applications and adaptive learning that adjusts learning depending on the capacity of edge resources available at the requested time. Furthermore, examining approaches to privacy-preserving and secure federated learning will be important to tackling data security issues in distributed settings. Another exciting research avenue is efficient, high-performance, lightweight architectures for real-time deployment.

In addition to the design of learning algorithms, an effective communication mode for edge-based distributed VLMs should be proposed. This necessitates extensive analytical and simulation around designing such a model and efficient hardware implementation. This requires designing hardware accelerators specializing in more efficient communication and lower latency. Also, including energy-efficient units will allow edge devices to handle distributed VLMs without breaching the power ceilings. In general, the edge-based VLMs constitute a promising direction for future work.





% \bibliographystyle{plain}

\bibliographystyle{IEEEtran}
\bibliography{Database}

\begin{IEEEbiography}
[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Ahmed-Sharshar.jpeg}}]{Ahmed Sharshar} is currently pursuing a PhD in Computer Vision at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) in Abu Dhabi, UAE. He previously received his Master of Science degree in Computer Vision from MBZUAI. He obtained his Bachelor of Engineering degree in Computer Engineering from the Egypt-Japan University of Science and Technology (E-JUST), Egypt.
His research primarily focuses on developing lightweight models and expanding their applications across various domains, such as natural language processing, computer vision, and human-computer interaction. Specifically, he aims to make these models more efficient and accessible, ensuring broader usability and practical deployment.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{latif.jpg}}]{Latif U. Khan} received his Ph.D. degree in Computer Engineering at Kyung Hee University (KHU), South Korea. Prior to that, He received his MS (Electrical Engineering) degree with distinction from University of Engineering and Technology, Peshawar, Pakistan in 2017. He is the recipient of KHU Best thesis award. He is author of two books: (a) Network Slicing for $5$G and Beyond and (b) Federated Learning for Wireless Networks. He has reviewed over 200 times for the top ISI- Indexed journals and conferences. He has authored many most popular articles in the leading journals (i.e., IEEE Communications Surveys and Tutorials) and magazines (IEEE Communication Magazine, IEEE Network, and IEEE Wireless Communications Magazine). His research interests include analytical techniques of optimization and game theory to edge computing, end-to-end network slicing, wireless federated learning, and digital twins. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Images/Waseem_Ullah.jpg}}]{Waseem Ullah} (Student Member, IEEE) received his M.S. degree in Computer Science from Islamia College Peshawar, Pakistan, in 2019. He completed his Ph.D. program at Sejong University, Seoul, South Korea, with the Intelligent Media Laboratory (IM Lab). Currently, he is serving as a Postdoctoral Fellow at the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE. His research interests include computer vision techniques, anomaly detection, bioinformatics, pattern recognition, deep learning, image processing, video analysis, and medical image analysis. He has published several articles in reputed peer-reviewed international journals, such as Future Generation Computer Systems, Multimedia Tools and Applications, IEEE Transactions on Human-Machine Systems, Knowledge-Based Systems, Computational Intelligence and Neuroscience, Applied Sciences, and MDPI Sensors. Furthermore, he is involved in reviewing several articles for publication in peer-reviewed journals. 
\end{IEEEbiography}



\begin{IEEEbiography}[{\includegraphics[width=1.5in,height=1.25in,clip,keepaspectratio]{mohsen-guizani.jpg}}]{Mohsen Guizani} (S’85, M'89, SM'99, F’09) received his B.S. (with distinction) and M.S. degrees in electrical engineering, and M.S. and Ph.D. degrees in computer engineering from Syracuse University, New York, in 1984, 1986, 1987, and 1990, respectively. He is currently a Professor with the Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE. Previously, he served in different academic and administrative positions at the University of Idaho, Western Michigan University, the University of West Florida, the University of Missouri-Kansas City, the University of Colorado-Boulder, and Syracuse University. His research interests include wireless communications and mobile computing, computer networks, mobile cloud computing, security, and smart grid. He was the Editor-in-Chief of IEEE Network. He serves on the Editorial Boards of several international technical journals, and is the Founder and Editor-in-Chief of the Wireless Communications and Mobile Computing journal (Wiley). He is the author of nine books and more than 500 publications in refereed journals and conferences. He has guest edited a number of Special Issues in IEEE journals and magazines. He has also served as a TPC member, Chair, and General Chair of a number of international conferences. Throughout his career, he received three teaching awards and four research awards. He also received the 2017 IEEE Communications Society WTC Recognition Award as well as the 2018 AdHoc Technical Committee Recognition Award for his contribution to outstanding research in wireless communications and ad hoc sensor networks. He was the Chair of the IEEE Communications Society Wireless Technical Committee and the Chair of the TAOS Technical Committee. He served as a IEEE Computer Society Distinguished Speaker and is currently an IEEE ComSoc Distinguished Lecturer. He is a Senior Member of ACM.

\end{IEEEbiography}






\end{document}


