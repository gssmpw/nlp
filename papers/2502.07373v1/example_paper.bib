
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
%Entries

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{li2024gslb,
  title={GSLB: the graph structure learning benchmark},
  author={Li, Zhixun and Sun, Xin and Luo, Yifan and Zhu, Yanqiao and Chen, Dingshuo and Luo, Yingtao and Zhou, Xiangxin and Liu, Qiang and Wu, Shu and Wang, Liang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{tyson2022automation,
  title={Automation, AI \& work},
  author={Tyson, Laura D and Zysman, John},
  journal={Daedalus},
  volume={151},
  number={2},
  pages={256--271},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{sparck1972tfidf,
  title={A statistical interpretation of term specificity and its application in retrieval},
  author={Sparck Jones, Karen},
  journal={Journal of documentation},
  volume={28},
  number={1},
  pages={11--21},
  year={1972},
  publisher={MCB UP Ltd}
}

@inproceedings{ramos2003usingtf,
  title={Using tf-idf to determine word relevance in document queries},
  author={Ramos, Juan and others},
  booktitle={Proceedings of the first instructional conference on machine learning},
  volume={242},
  number={1},
  pages={29--48},
  year={2003},
  organization={Citeseer}
}

@article{hong2024datainterpreter,
  title={Data interpreter: An llm agent for data science},
  author={Hong, Sirui and Lin, Yizhang and Liu, Bang and Liu, Bangbang and Wu, Binhao and Zhang, Ceyao and Wei, Chenxing and Li, Danyang and Chen, Jiaqi and Zhang, Jiayi and others},
  journal={arXiv preprint arXiv:2402.18679},
  year={2024}
}

@inproceedings{song2023llmplanner,
  title={Llm-planner: Few-shot grounded planning for embodied agents with large language models},
  author={Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M and Chao, Wei-Lun and Su, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2998--3009},
  year={2023}
}

@article{clune2019ai,
  title={AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence},
  author={Clune, Jeff},
  journal={arXiv preprint arXiv:1905.10985},
  year={2019}
}



@article{li2024autokaggle,
  title={AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions},
  author={Li, Ziming and Zang, Qianbo and Ma, David and Guo, Jiawei and Zheng, Tuney and Liu, Minghao and Niu, Xinyao and Wang, Yue and Yang, Jian and Liu, Jiaheng and others},
  journal={arXiv preprint arXiv:2410.20424},
  year={2024}
}

@article{zhu2024autotqa,
  title={AutoTQA: Towards Autonomous Tabular Question Answering through Multi-Agent Large Language Models},
  author={Zhu, Jun-Peng and Cai, Peng and Xu, Kai and Li, Li and Sun, Yishen and Zhou, Shuai and Su, Haihuang and Tang, Liu and Liu, Qi},
  journal={Proceedings of the VLDB Endowment},
  volume={17},
  number={12},
  pages={3920--3933},
  year={2024},
  publisher={VLDB Endowment}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@INPROCEEDINGS{Tang:12KDDCross,
    AUTHOR = "Jie Tang and Sen Wu and Jimeng Sun and Hang Su",
    TITLE = "Cross-domain Collaboration Recommendation",
    BOOKTITLE = "KDD'2012",
    YEAR = {2012},
}

@inproceedings{sankar2020dysat,
  title={Dysat: Deep neural representation learning on dynamic graphs via self-attention networks},
  author={Sankar, Aravind and Wu, Yanhong and Gou, Liang and Zhang, Wei and Yang, Hao},
  booktitle={Proceedings of the 13th International Conference on Web Search and Data Mining},
  pages={519--527},
  year={2020}
}

@article{wu2022handling,
  title={Handling Distribution Shifts on Graphs: An Invariance Perspective},
  author={Wu, Qitian and Zhang, Hengrui and Yan, Junchi and Wipf, David},
  journal={International Conference on Learning Representations},
  year={2022}
}


@article{tuyls2006evolutionary,
  title={An evolutionary dynamical analysis of multi-agent learning in iterated games},
  author={Tuyls, Karl and Hoen, Pieter Jan T and Vanschoenwinkel, Bram},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={12},
  pages={115--153},
  year={2006},
  publisher={Springer}
}

@article{shoham2007if,
  title={If multi-agent learning is the answer, what is the question?},
  author={Shoham, Yoav and Powers, Rob and Grenager, Trond},
  journal={Artificial intelligence},
  volume={171},
  number={7},
  pages={365--377},
  year={2007},
  publisher={Elsevier}
}

@article{li2016multi,
  title={A multi-agent genetic algorithm for community detection in complex networks},
  author={Li, Zhangtao and Liu, Jing},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={449},
  pages={336--347},
  year={2016},
  publisher={Elsevier}
}

@article{shi2024red,
  title={Red teaming language model detectors with language models},
  author={Shi, Zhouxing and Wang, Yihan and Yin, Fan and Chen, Xiangning and Chang, Kai-Wei and Hsieh, Cho-Jui},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={174--189},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{romera2024mathematical,
  title={Mathematical discoveries from program search with large language models},
  author={Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M Pawan and Dupont, Emilien and Ruiz, Francisco JR and Ellenberg, Jordan S and Wang, Pengming and Fawzi, Omar and others},
  journal={Nature},
  volume={625},
  number={7995},
  pages={468--475},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{tao2023program,
  title={Program synthesis with generative pre-trained transformers and grammar-guided genetic programming grammar},
  author={Tao, Ning and Ventresque, Anthony and Saber, Takfarinas},
  booktitle={2023 IEEE Latin American Conference on Computational Intelligence (LA-CCI)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}


@inproceedings{liu2020mapper,
  title={Mapper: Multi-agent path planning with evolutionary reinforcement learning in mixed dynamic environments},
  author={Liu, Zuxin and Chen, Baiming and Zhou, Hongyi and Koushik, Guru and Hebert, Martial and Zhao, Ding},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={11748--11754},
  year={2020},
  organization={IEEE}
}


@article{xu2022gps,
  title={GPS: Genetic prompt search for efficient few-shot learning},
  author={Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
  journal={arXiv preprint arXiv:2210.17041},
  year={2022}
}

@inproceedings{cetnarowicz1996application,
  title={The application of evolution process in multi-agent world to the prediction system},
  author={Cetnarowicz, Krzysztof and Kisiel-Dorohinicki, Marek and Nawarecki, Edward},
  booktitle={Proceedings of the Second International Conference on Multi-Agent Systems, ICMAS},
  volume={96},
  pages={26--32},
  year={1996}
}

@inproceedings{wu2022discovering,
  author    = {Yingxin Wu and
               Xiang Wang and
               An Zhang and
               Xiangnan He and
               Tat{-}Seng Chua},
  title     = {Discovering Invariant Rationales for Graph Neural Networks},
  booktitle = {The Tenth International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2022}
}

@article{zhu2021shift,
  title={Shift-robust gnns: Overcoming the limitations of localized graph training data},
  author={Zhu, Qi and Ponomareva, Natalia and Han, Jiawei and Perozzi, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{gagnon2022woods,
  title={WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks},
  author={Gagnon-Audet, Jean-Christophe and Ahuja, Kartik and Darvishi-Bayazi, Mohammad-Javad and Dumas, Guillaume and Rish, Irina},
  journal={arXiv preprint arXiv:2203.09978},
  year={2022}
}

@inproceedings{du2021adarnn,
  title={Adarnn: Adaptive learning and forecasting of time series},
  author={Du, Yuntao and Wang, Jindong and Feng, Wenjie and Pan, Sinno and Qin, Tao and Xu, Renjun and Wang, Chongjun},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={402--411},
  year={2021}
}

@inproceedings{kim2021reversible,
  title={Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift},
  author={Kim, Taesung and Kim, Jinhee and Tae, Yunwon and Park, Cheonbok and Choi, Jang-Ho and Choo, Jaegul},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{venkateswaran2021environment,
  title={Environment agnostic invariant risk minimization for classification of sequential datasets},
  author={Venkateswaran, Praveen and Muthusamy, Vinod and Isahagian, Vatche and Venkatasubramanian, Nalini},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1615--1624},
  year={2021}
}

@article{lu2021diversify,
  title={DIVERSIFY to Generalize: Learning Generalized Representations for Time Series Classification},
  author={Lu, Wang and Wang, Jindong and Chen, Yiqiang and Sun, Xinwei},
  journal={arXiv preprint},
  year={2021}
}

@article{skarding2021foundations,
  title={Foundations and Modeling of Dynamic Networks Using Dynamic Graph Neural Networks: A Survey},
  author={Skarding, Joakim and Gabrys, Bogdan and Musial, Katarzyna},
  journal={IEEE Access},
  pages={79143--79168},
  year={2021}
}

@article{zhu2022learnable,
  title={Learnable Encoder-Decoder Architecture for Dynamic Graph: A Survey},
  author={Zhu, Yuecai and Lyu, Fuyuan and Hu, Chengming and Chen, Xi and Liu, Xue},
  journal={arXiv preprint arXiv:2203.10480},
  year={2022}
}


@article{zhang2024cut,
 Author = {Guibin Zhang and Yanwei Yue and Zhixun Li and Sukwon Yun and Guancheng Wan and Kun Wang and Dawei Cheng and Jeffrey Xu Yu and Tianlong Chen},
Title = {Cut the Crap: An Economical Communication Pipeline for LLM-based Multi-Agent Systems},
Year = {2024},
  journal={arXiv preprint arXiv:2410.02506},
}


@inproceedings{wang2021inductive,
  author    = {Yanbang Wang and
               Yen{-}Yu Chang and
               Yunyu Liu and
               Jure Leskovec and
               Pan Li},
  title     = {Inductive Representation Learning in Temporal Networks via Causal
               Anonymous Walks},
  booktitle = {9th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2021}
}

@article{cong2021dynamic,
  title={Dynamic Graph Representation Learning via Graph Transformer Networks},
  author={Cong, Weilin and Wu, Yanhong and Tian, Yuandong and Gu, Mengting and Xia, Yinglong and Mahdavi, Mehrdad and Chen, Chun-cheng Jason},
  journal={arXiv preprint arXiv:2111.10447},
  year={2021}
}

@inproceedings{yang2021discrete,
  title={Discrete-time Temporal Network Embedding via Implicit Hierarchical Learning in Hyperbolic Space},
  author={Yang, Menglin and Zhou, Min and Kalander, Marcus and Huang, Zengfeng and King, Irwin},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1975--1985},
  year={2021}
}

@inproceedings{sun2021hyperbolic,
  title={Hyperbolic variational graph neural network for modeling dynamic graphs},
  author={Sun, Li and Zhang, Zhongbao and Zhang, Jiawei and Wang, Feiyang and Peng, Hao and Su, Sen and Yu, Philip S},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={5},
  pages={4375--4383},
  year={2021}
}

@inproceedings{xu2020inductive,
  author    = {Da Xu and
               Chuanwei Ruan and
               Evren K{\"{o}}rpeoglu and
               Sushant Kumar and
               Kannan Achan},
  title     = {Inductive representation learning on temporal graphs},
  booktitle = {8th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2020}
}

@article{wang2021tcl,
  title={Tcl: Transformer-based dynamic graph modelling via contrastive learning},
  author={Wang, Lu and Chang, Xiaofu and Li, Shuang and Chu, Yunfei and Li, Hui and Zhang, Wei and He, Xiaofeng and Song, Le and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2105.07944},
  year={2021}
}

@article{rossi2020temporal,
  title={Temporal graph networks for deep learning on dynamic graphs},
  author={Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael},
  journal={arXiv preprint arXiv:2006.10637},
  year={2020}
}

@article{hajiramezanali2019variational,
  title={Variational graph recurrent neural networks},
  author={Hajiramezanali, Ehsan and Hasanzadeh, Arman and Narayanan, Krishna and Duffield, Nick and Zhou, Mingyuan and Qian, Xiaoning},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{pareja2020evolvegcn,
  title={Evolvegcn: Evolving graph convolutional networks for dynamic graphs},
  author={Pareja, Aldo and Domeniconi, Giacomo and Chen, Jie and Ma, Tengfei and Suzumura, Toyotaro and Kanezashi, Hiroki and Kaler, Tim and Schardl, Tao and Leiserson, Charles},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5363--5370},
  year={2020}
}

@inproceedings{seo2018structured,
  title={Structured sequence modeling with graph convolutional recurrent networks},
  author={Seo, Youngjoo and Defferrard, Micha{\"e}l and Vandergheynst, Pierre and Bresson, Xavier},
  booktitle={International Conference on Neural Information Processing},
  pages={362--373},
  year={2018},
  organization={Springer}
}

@article{kipf2016variational,
  title={Variational graph auto-encoders},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1611.07308},
  year={2016}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint},
  year={2019}
}

@inproceedings{sagawa2019distributionally,
  title={Distributionally Robust Neural Networks},
  author={Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B and Liang, Percy},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{krueger2021out,
  title={Out-of-distribution generalization via risk extrapolation (rex)},
  author={Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Le Priol, Remi and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5815--5826},
  year={2021}
}

@article{cadene2019rubi,
  title={Rubi: Reducing unimodal biases for visual question answering},
  author={Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  journal={Advances in neural information processing systems},
  year={2019}
}

@article{aggarwal2014evolutionary,
  title={Evolutionary network analysis: A survey},
  author={Aggarwal, Charu and Subbian, Karthik},
  journal={ACM Computing Surveys (CSUR)},
  volume={47},
  number={1},
  pages={1--36},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@inproceedings{qiu2020temporal,
  title={Temporal network embedding with high-order nonlinear information},
  author={Qiu, Zhenyu and Hu, Wenbin and Wu, Jia and Liu, Weiwei and Du, Bo and Jia, Xiaohua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5436--5443},
  year={2020}
}

@inproceedings{huang2020motif,
  title={Motif-Preserving Temporal Network Embedding.},
  author={Huang, Hong and Fang, Zixuan and Wang, Xiao and Miao, Youshan and Jin, Hai},
  booktitle={IJCAI},
  pages={1237--1243},
  year={2020}
}

@inproceedings{zhou2018dynamic,
  title={Dynamic network embedding by modeling triadic closure process},
  author={Zhou, Lekui and Yang, Yang and Ren, Xiang and Wu, Fei and Zhuang, Yueting},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{trivedi2019dyrep,
  title={Dyrep: Learning representations over dynamic graphs},
  author={Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet and Zha, Hongyuan},
  booktitle={International conference on learning representations},
  year={2019}
}

@article{ding2021closer,
  title={A Closer Look at Distribution Shifts and Out-of-Distribution Generalization on Graphs},
  author={Ding, Mucong and Kong, Kezhi and Chen, Jiuhai and Kirchenbauer, John and Goldblum, Micah and Wipf, David and Huang, Furong and Goldstein, Tom},
  year={2021}
}

@article{kovanen2011temporal,
  title={Temporal motifs in time-dependent networks},
  author={Kovanen, Lauri and Karsai, M{\'a}rton and Kaski, Kimmo and Kert{\'e}sz, J{\'a}nos and Saram{\"a}ki, Jari},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2011},
  number={11},
  pages={P11005},
  year={2011},
  publisher={IOP Publishing}
}

@article{benson2016higher,
  title={Higher-order organization of complex networks},
  author={Benson, Austin R and Gleich, David F and Leskovec, Jure},
  journal={Science},
  volume={353},
  number={6295},
  pages={163--166},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{paranjape2017motifs,
  title={Motifs in temporal networks},
  author={Paranjape, Ashwin and Benson, Austin R and Leskovec, Jure},
  booktitle={Proceedings of the tenth ACM international conference on web search and data mining},
  pages={601--610},
  year={2017}
}

@article{zitnik2019evolution,
  title={Evolution of resilience in protein interactomes across the tree of life},
  author={Zitnik, Marinka and Sosi{\v{c}}, Rok and Feldman, Marcus W and Leskovec, Jure},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={10},
  pages={4426--4433},
  year={2019},
  publisher={National Acad Sciences}
}

@book{coleman1994foundations,
  title={Foundations of social theory},
  author={Coleman, James S},
  year={1994},
  publisher={Harvard university press}
}

@article{huang2015triadic,
  title={Triadic closure pattern analysis and prediction in social networks},
  author={Huang, Hong and Tang, Jie and Liu, Lu and Luo, JarDer and Fu, Xiaoming},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={27},
  number={12},
  pages={3374--3389},
  year={2015},
  publisher={IEEE}
}

@article{kovanen2013temporal,
  title={Temporal motifs reveal homophily, gender-specific patterns, and group talk in call sequences},
  author={Kovanen, Lauri and Kaski, Kimmo and Kert{\'e}sz, J{\'a}nos and Saram{\"a}ki, Jari},
  journal={Proceedings of the National Academy of Sciences},
  volume={110},
  number={45},
  pages={18070--18075},
  year={2013},
  publisher={National Acad Sciences}
}

@book{glymour2016causal,
  title={Causal inference in statistics: A primer},
  author={Glymour, Madelyn and Pearl, Judea and Jewell, Nicholas P},
  year={2016},
  publisher={John Wiley \& Sons}
}

@article{pearl2000models,
  title={Models, reasoning and inference},
  author={Pearl, Judea and others},
  journal={Cambridge, UK: CambridgeUniversityPress},
  volume={19},
  pages={2},
  year={2000}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  year={2017}
}

@book{tian2006characterization,
  title={A characterization of interventional distributions in semi-Markovian causal models},
  author={Tian, Jin and Kang, Changsung and Pearl, Judea},
  year={2006},
  publisher={eScholarship, University of California}
}

@article{brown1992survivorship,
  title={Survivorship bias in performance studies},
  author={Brown, Stephen J and Goetzmann, William and Ibbotson, Roger G and Ross, Stephen A},
  journal={The Review of Financial Studies},
  volume={5},
  number={4},
  pages={553--580},
  year={1992},
  publisher={Oxford University Press}
}

@article{berk1983introduction,
  title={An introduction to sample selection bias in sociological data},
  author={Berk, Richard A},
  journal={American sociological review},
  pages={386--398},
  year={1983}
}

@book{simmel1950sociology,
  title={The sociology of georg simmel},
  author={Simmel, Georg},
  volume={92892},
  year={1950},
  publisher={Simon and Schuster}
}

@article{shen2021towards,
  title={Towards out-of-distribution generalization: A survey},
  author={Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  journal={arXiv preprint arXiv:2108.13624},
  year={2021}
}

@article{nascimento2021dynamic,
  title={Dynamic graph in a symbolic data framework: An account of the causal relation using COVID-19 reports and some reflections on the financial world},
  author={Nascimento, Diego C and Pimentel, Bruno A and Souza, Renata MCR and Costa, Lilia and Gon{\c{c}}alves, Sandro and Louzada, Francisco},
  journal={Chaos, Solitons \& Fractals},
  volume={153},
  pages={111440},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{zhang2021dyngraphtrans,
  title={DynGraphTrans: Dynamic Graph Embedding via Modified Universal Transformer Networks for Financial Transaction Data},
  author={Zhang, Shilei and Suzumura, Toyotaro and Zhang, Li},
  booktitle={2021 IEEE International Conference on Smart Data Services (SMDS)},
  pages={184--191},
  year={2021},
  organization={IEEE}
}

@inproceedings{berger2006framework,
  title={A framework for analysis of dynamic social networks},
  author={Berger-Wolf, Tanya Y and Saia, Jared},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={523--528},
  year={2006}
}

@inproceedings{greene2010tracking,
  title={Tracking the evolution of communities in dynamic social networks},
  author={Greene, Derek and Doyle, Donal and Cunningham, Padraig},
  booktitle={2010 international conference on advances in social networks analysis and mining},
  pages={176--183},
  year={2010},
  organization={IEEE}
}

@article{peng2021dynamic,
  title={Dynamic graph convolutional network for long-term traffic flow prediction with reinforcement learning},
  author={Peng, Hao and Du, Bowen and Liu, Mingsheng and Liu, Mingzhe and Ji, Shumei and Wang, Senzhang and Zhang, Xu and He, Lifang},
  journal={Information Sciences},
  volume={578},
  pages={401--416},
  year={2021},
  publisher={Elsevier}
}

@article{peng2020spatial,
  title={Spatial temporal incidence dynamic graph neural networks for traffic flow forecasting},
  author={Peng, Hao and Wang, Hongfei and Du, Bowen and Bhuiyan, Md Zakirul Alam and Ma, Hongyuan and Liu, Jianwei and Wang, Lihong and Yang, Zeyu and Du, Linfeng and Wang, Senzhang and others},
  journal={Information Sciences},
  volume={521},
  pages={277--290},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{wang2022causal,
  title={Causal Representation Learning for Out-of-Distribution Recommendation},
  author={Wang, Wenjie and Lin, Xinyu and Feng, Fuli and He, Xiangnan and Lin, Min and Chua, Tat-Seng},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={3562--3571},
  year={2022}
}

@article{jin2021community,
  title={Community detection and co-author recommendation in co-author networks},
  author={Jin, Tian and Wu, Qiong and Ou, Xuan and Yu, Jianjun},
  journal={International Journal of Machine Learning and Cybernetics},
  volume={12},
  number={2},
  pages={597--609},
  year={2021},
  publisher={Springer}
}

@inproceedings{ahuja2020empirical,
  author    = {Kartik Ahuja and
               Jun Wang and
               Amit Dhurandhar and
               Karthikeyan Shanmugam and
               Kush R. Varshney},
  title     = {Empirical or Invariant Risk Minimization? {A} Sample Complexity Perspective},
  booktitle = {9th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2021}
}

@article{huang2020graph,
  title={Graph meta learning via local subgraphs},
  author={Huang, Kexin and Zitnik, Marinka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5862--5874},
  year={2020}
}

@article{li2022out,
  title={Out-Of-Distribution Generalization on Graphs: A Survey},
  author={Li, Haoyang and Wang, Xin and Zhang, Ziwei and Zhu, Wenwu},
  journal={arXiv preprint},
  year={2022}
}

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations},
  year      = {2015},
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  year={2019}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}
@inproceedings{chang2020invariant,
  title={Invariant rationalization},
  author={Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
  booktitle={International Conference on Machine Learning},
  pages={1448--1458},
  year={2020},
  organization={PMLR}
}

@inproceedings{ahuja2020invariant,
  title={Invariant risk minimization games},
  author={Ahuja, Kartik and Shanmugam, Karthikeyan and Varshney, Kush and Dhurandhar, Amit},
  booktitle={International Conference on Machine Learning},
  pages={145--155},
  year={2020},
  organization={PMLR}
}

@inproceedings{rosenfeld2020risks,
  author    = {Elan Rosenfeld and
               Pradeep Kumar Ravikumar and
               Andrej Risteski},
  title     = {The Risks of Invariant Risk Minimization},
  booktitle = {9th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2021}
}


@inproceedings{mitrovic2020representation,
  author    = {Jovana Mitrovic and
               Brian McWilliams and
               Jacob C. Walker and
               Lars Holger Buesing and
               Charles Blundell},
  title     = {Representation Learning via Invariant Causal Mechanisms},
  booktitle = {9th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2021}
}

@article{barrat2004architecture,
  title={The architecture of complex weighted networks},
  author={Barrat, Alain and Barthelemy, Marc and Pastor-Satorras, Romualdo and Vespignani, Alessandro},
  journal={Proceedings of the national academy of sciences},
  volume={101},
  number={11},
  pages={3747--3752},
  year={2004},
  publisher={National Acad Sciences}
}

@inproceedings{Cho2014LearningPR,
  title={Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
  author={Kyunghyun Cho and Bart van Merrienboer and Çaglar G{\"u}lçehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  booktitle={EMNLP},
  year={2014}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{qin2022graph,
  title={Graph Neural Architecture Search Under Distribution Shifts},
  author={Qin, Yijian and Wang, Xin and Zhang, Ziwei and Xie, Pengtao and Zhu, Wenwu},
  booktitle={International Conference on Machine Learning},
  pages={18083--18095},
  year={2022}
}

@article{li2022ood,
  title={Ood-gnn: Out-of-distribution generalized graph neural network},
  author={Li, Haoyang and Wang, Xin and Zhang, Ziwei and Zhu, Wenwu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zhang2022learning,
  author    = {Zeyang Zhang and
               Ziwei Zhang and
               Xin Wang and
               Wenwu Zhu},
  title     = {Learning to Solve Travelling Salesman Problem with Hardness-Adaptive
               Curriculum},
  booktitle = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence},
  pages     = {9136--9144},
  publisher = {{AAAI} Press},
  year      = {2022},
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@article{hsieh2018learning,
  title={Learning to decompose and disentangle representations for video prediction},
  author={Hsieh, Jun-Ting and Liu, Bingbin and Huang, De-An and Fei-Fei, Li F and Niebles, Juan Carlos},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{ma2018disentangled,
  title={Disentangled person image generation},
  author={Ma, Liqian and Sun, Qianru and Georgoulis, Stamatios and Van Gool, Luc and Schiele, Bernt and Fritz, Mario},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={99--108},
  year={2018}
}

@inproceedings{ma2019disentangled,
  title={Disentangled graph convolutional networks},
  author={Ma, Jianxin and Cui, Peng and Kuang, Kun and Wang, Xin and Zhu, Wenwu},
  booktitle={International conference on machine learning},
  pages={4212--4221},
  year={2019},
  organization={PMLR}
}

@article{yang2020factorizable,
  title={Factorizable graph convolutional networks},
  author={Yang, Yiding and Feng, Zunlei and Song, Mingli and Wang, Xinchao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20286--20296},
  year={2020}
}

@article{wang2022disentangled,
  title={Disentangled Representation Learning for Recommendation},
  author={Wang, Xin and Chen, Hong and Zhou, Yuwei and Ma, Jianxin and Zhu, Wenwu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@article{li2021disentangled,
  title={Disentangled contrastive learning on graphs},
  author={Li, Haoyang and Wang, Xin and Zhang, Ziwei and Yuan, Zehuan and Li, Hang and Zhu, Wenwu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21872--21884},
  year={2021}
}

@article{li2022disentangled,
  title={Disentangled Graph Contrastive Learning With Independence Promotion},
  author={Li, Haoyang and Zhang, Ziwei and Wang, Xin and Zhu, Wenwu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@article{chen2021curriculum,
  title={Curriculum Disentangled Recommendation with Noisy Multi-feedback},
  author={Chen, Hong and Chen, Yudong and Wang, Xin and Xie, Ruobing and Wang, Rui and Xia, Feng and Zhu, Wenwu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26924--26936},
  year={2021}
}

@inproceedings{wang2021multimodal,
  title={Multimodal disentangled representation for recommendation},
  author={Wang, Xin and Chen, Hong and Zhu, Wenwu},
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2021}
}

@inproceedings{ma2020disentangled,
  title={Disentangled self-supervision in sequential recommenders},
  author={Ma, Jianxin and Zhou, Chang and Yang, Hongxia and Cui, Peng and Wang, Xin and Zhu, Wenwu},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={483--491},
  year={2020}
}

@article{ma2019learning,
  title={Learning disentangled representations for recommendation},
  author={Ma, Jianxin and Zhou, Chang and Cui, Peng and Yang, Hongxia and Zhu, Wenwu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{wang2020disenhan,
  title={Disenhan: Disentangled heterogeneous graph attention network for recommendation},
  author={Wang, Yifan and Tang, Suyao and Lei, Yuntong and Song, Weiping and Wang, Sheng and Zhang, Ming},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={1605--1614},
  year={2020}
}

@article{chen2016infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{denton2017unsupervised,
  title={Unsupervised learning of disentangled representations from video},
  author={Denton, Emily L and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{tran2017disentangled,
  title={Disentangled representation learning gan for pose-invariant face recognition},
  author={Tran, Luan and Yin, Xi and Liu, Xiaoming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1415--1424},
  year={2017}
}

@inproceedings{liu2020independence,
  title={Independence promoted graph disentangled networks},
  author={Liu, Yanbei and Wang, Xiao and Wu, Shu and Xiao, Zhitao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={4916--4923},
  year={2020}
}

@inproceedings{zhang2021disentangled,
  title={Disentangled dynamic graph deep generation},
  author={Zhang, Wenbin and Zhang, Liming and Pfoser, Dieter and Zhao, Liang},
  booktitle={Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)},
  pages={738--746},
  year={2021},
  organization={SIAM}
}

@inproceedings{du2022disentangled,
  author    = {Yuanqi Du and
               Xiaojie Guo and
               Hengning Cao and
               Yanfang Ye and
               Liang Zhao},
  title     = {Disentangled Spatiotemporal Graph Generative Models},
  booktitle = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence},
  pages     = {6541--6549},
  publisher = {{AAAI} Press},
  year      = {2022}
}

@article{chen2022invariance,
  title={Invariance Principle Meets Out-of-Distribution Generalization on Graphs},
  author={Chen, Yongqiang and Zhang, Yonggang and Yang, Han and Ma, Kaili and Xie, Binghui and Liu, Tongliang and Han, Bo and Cheng, James},
  journal={arXiv preprint},
  year={2022}
}

@article{fan2021generalizing,
  title={Generalizing Graph Neural Networks on Out-Of-Distribution Graphs},
  author={Fan, Shaohua and Wang, Xiao and Shi, Chuan and Cui, Peng and Wang, Bai},
  journal={arXiv preprint arXiv:2111.10657},
  year={2021}
}

@inproceedings{chang2020continuous,
  title={Continuous-time dynamic graph learning via neural interaction processes},
  author={Chang, Xiaofu and Liu, Xuqin and Wen, Jianfeng and Li, Shuang and Fang, Yanming and Song, Le and Qi, Yuan},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={145--154},
  year={2020}
}

@inproceedings{huang2021coupled,
  title={Coupled Graph ODE for Learning Interacting System Dynamics.},
  author={Huang, Zijie and Sun, Yizhou and Wang, Wei},
  booktitle={KDD},
  pages={705--715},
  year={2021}
}

@article{li2021intention,
  title={Intention-aware sequential recommendation with structured intent transition},
  author={Li, Haoyang and Wang, Xin and Zhang, Ziwei and Ma, Jianxin and Cui, Peng and Zhu, Wenwu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  publisher={IEEE}
}

@inproceedings{cai2021structural,
  title={Structural temporal graph neural networks for anomaly detection in dynamic graphs},
  author={Cai, Lei and Chen, Zhengzhang and Luo, Chen and Gui, Jiaping and Ni, Jingchao and Li, Ding and Chen, Haifeng},
  booktitle={Proceedings of the 30th ACM international conference on Information \& Knowledge Management},
  pages={3747--3756},
  year={2021}
}

@inproceedings{deng2020dynamic,
  title={Dynamic knowledge graph based multi-event forecasting},
  author={Deng, Songgaojun and Rangwala, Huzefa and Ning, Yue},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1585--1595},
  year={2020}
}

@inproceedings{yao2021interpretable,
  title={Interpretable clustering on dynamic graphs with recurrent graph neural networks},
  author={Yao, Yuhang and Joe-Wong, Carlee},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={5},
  pages={4608--4616},
  year={2021}
}

@inproceedings{you2019hierarchical,
  title={Hierarchical temporal convolutional networks for dynamic recommender systems},
  author={You, Jiaxuan and Wang, Yichen and Pal, Aditya and Eksombatchai, Pong and Rosenburg, Chuck and Leskovec, Jure},
  booktitle={The world wide web conference},
  pages={2236--2246},
  year={2019}
}

@inproceedings{wang2021tedic,
  title={TEDIC: Neural modeling of behavioral patterns in dynamic social interaction networks},
  author={Wang, Yanbang and Li, Pan and Bai, Chongyang and Leskovec, Jure},
  booktitle={Proceedings of the Web Conference 2021},
  pages={693--705},
  year={2021}
}

@inproceedings{wu2020temp,
  author    = {Jiapeng Wu and
               Meng Cao and
               Jackie Chi Kit Cheung and
               William L. Hamilton},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {5730--5746},
  publisher = {Association for Computational Linguistics},
  year      = {2020}
}

@inproceedings{li2019fates,
  title={Fates of Microscopic Social Ecosystems: Keep Alive or Dead?},
  author={Li, Haoyang and Cui, Peng and Zang, Chengxi and Zhang, Tianyang and Zhu, Wenwu and Lin, Yishi},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={668--676},
  year={2019}
}

@inproceedings{yao2022wildtime,
  title={Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time},
  author={Huaxiu Yao and Caroline Choi and Yoonho Lee and Pang Wei Koh and Chelsea Finn},
  booktitle={Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
}

@inproceedings{yao2022improving,
  title={Improving Out-of-Distribution Robustness via Selective Augmentation},
  author={Yao, Huaxiu and Wang, Yu and Li, Sai and Zhang, Linjun and Liang, Weixin and Zou, James and Finn, Chelsea},
  booktitle={Proceeding of the Thirty-ninth International Conference on Machine Learning},
  year={2022}
}

@inproceedings{li2022gil,
  title={Learning Invariant Graph Representations for Out-of-Distribution Generalization},
  author={Li, Haoyang and Zhang, Ziwei and Wang, Xin and Zhu, Wenwu},
  booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{zhang2022dynamic,
  title={Dynamic graph neural networks under spatio-temporal distribution shift},
  author={Zhang, Zeyang and Wang, Xin and Zhang, Ziwei and Li, Haoyang and Qin, Zhou and Zhu, Wenwu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{hu2020open,
  title={Open graph benchmark: Datasets for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22118--22133},
  year={2020}
}

@INPROCEEDINGS{Tang:08KDD,
    AUTHOR = "Jie Tang and Jing Zhang and Limin Yao and Juanzi Li and Li Zhang and Zhong Su",
    TITLE = "ArnetMiner: Extraction and Mining of Academic Social Networks",
    pages = "990-998",
    YEAR = {2008},
    BOOKTITLE = "KDD'08",
}

@inproceedings{sinha2015overview,
  title={An overview of microsoft academic service (mas) and applications},
  author={Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-june Paul and Wang, Kuansan},
  booktitle={Proceedings of the 24th international conference on world wide web},
  pages={243--246},
  year={2015},
  organization={ACM}
}

@article{wang2020microsoft,
  title={Microsoft academic graph: When experts are not enough},
  author={Wang, Kuansan and Shen, Zhihong and Huang, Chiyuan and Wu, Chieh-Han and Dong, Yuxiao and Kanakia, Anshul},
  journal={Quantitative Science Studies},
  volume={1},
  number={1},
  pages={396--413},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{kipf2016semi,
  author    = {Thomas N. Kipf and
               Max Welling},
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {5th International Conference on Learning Representations},
  publisher = {OpenReview.net},
  year      = {2017}
}

@inproceedings{velivckovicgraph,
  title={Graph Attention Networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations}
}

@ARTICLE{9847099,
  author={Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Domain Generalization: A Survey}, 
  year={2023},
  number={4},
  pages={4396-4415},
  doi={10.1109/TPAMI.2022.3195549}}

@ARTICLE{9476906,
  author={Christiansen, Rune and Pfister, Niklas and Jakobsen, Martin Emil and Gnecco, Nicola and Peters, Jonas},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Causal Framework for Distribution Generalization}, 
  year={2022},
  volume={44},
  number={10},
  pages={6614-6630},
  doi={10.1109/TPAMI.2021.3094760}}

@ARTICLE{9801711,
  author={Peng, Xi and Qiao, Fengchun and Zhao, Long},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Out-of-Domain Generalization From a Single Source: An Uncertainty Quantification Approach}, 
  year={2022},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/TPAMI.2022.3184598}}

@ARTICLE{9792207,
  author={Ao, Sheng and Guo, Yulan and Hu, Qingyong and Yang, Bo and Markham, Andrew and Chen, Zengping},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={You Only Train Once: Learning General and Distinctive 3D Local Descriptors}, 
  year={2023},
  volume={45},
  number={3},
  pages={3949-3967},
  doi={10.1109/TPAMI.2022.3180341}}

@ARTICLE{9556560,
  author={Yao, Zhiyu and Wang, Yunbo and Wang, Jianmin and Yu, Philip S. and Long, Mingsheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={VideoDG: Generalizing Temporal Relations in Videos to Novel Domains}, 
  year={2022},
  volume={44},
  number={11},
  pages={7989-8004},
  doi={10.1109/TPAMI.2021.3116945}}

@ARTICLE{9730006,
  author={Tian, Chris Xing and Li, Haoliang and Xie, Xiaofei and Liu, Yang and Wang, Shiqi},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Neuron Coverage-Guided Domain Generalization}, 
  year={2023},
  volume={45},
  number={1},
  pages={1302-1311},
  doi={10.1109/TPAMI.2022.3157441}}

@ARTICLE{9445225,
  author={Azizzadenesheli, Kamyar},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Importance Weight Estimation and Generalization in Domain Adaptation Under Label Shift}, 
  year={2022},
  volume={44},
  number={10},
  pages={6578-6584},
  doi={10.1109/TPAMI.2021.3086060}}

@inproceedings{bevilacqua2021size,
  title={Size-invariant graph representations for graph classification extrapolations},
  author={Bevilacqua, Beatrice and Zhou, Yangze and Ribeiro, Bruno},
  booktitle={International Conference on Machine Learning},
  pages={837--851},
  year={2021}
}

@inproceedings{han2022g,
  title={G-mixup: Graph data augmentation for graph classification},
  author={Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Hu, Xia},
  booktitle={International Conference on Machine Learning},
  pages={8230--8248},
  year={2022}
}

@ARTICLE{9780235,
  author={Xu, Xinyi and Deng, Cheng and Xie, Yaochen and Ji, Shuiwang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Group Contrastive Self-Supervised Learning on Graphs}, 
  year={2023},
  volume={45},
  number={3},
  pages={3169-3180},
  doi={10.1109/TPAMI.2022.3177295}}


@ARTICLE{9875989,
  author={Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Explainability in Graph Neural Networks: A Taxonomic Survey}, 
  year={2022},
  volume={},
  number={},
  pages={1-19},
  doi={10.1109/TPAMI.2022.3204236}}

@ARTICLE{9764632,
  author={Xie, Yaochen and Xu, Zhao and Zhang, Jingtun and Wang, Zhengyang and Ji, Shuiwang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Self-Supervised Learning of Graph Neural Networks: A Unified Review}, 
  year={2023},
  volume={45},
  number={2},
  pages={2412-2429},
  doi={10.1109/TPAMI.2022.3170559}}

@ARTICLE{9770382,
  author={Liu, Yixin and Jin, Ming and Pan, Shirui and Zhou, Chuan and Zheng, Yu and Xia, Feng and Yu, Philip},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Graph Self-Supervised Learning: A Survey}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TKDE.2022.3172903}}

@ARTICLE{9842366,
  author={Guan, Shanyan and Xu, Jingwei and He, Michelle Zhang and Wang, Yunbo and Ni, Bingbing and Yang, Xiaokang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Out-of-Domain Human Mesh Reconstruction via Dynamic Bilevel Online Adaptation}, 
  year={2023},
  volume={45},
  number={4},
  pages={5070-5086},
  doi={10.1109/TPAMI.2022.3194167}}

@ARTICLE{9154576,
  author={Zhang, Ziyuan and Tran, Luan and Liu, Feng and Liu, Xiaoming},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={On Learning Disentangled Representations for Gait Recognition}, 
  year={2022},
  volume={44},
  number={1},
  pages={345-360},
  doi={10.1109/TPAMI.2020.2998790}}

@ARTICLE{9716806,
  author={Wang, Yingqian and Wang, Longguang and Wu, Gaochang and Yang, Jungang and An, Wei and Yu, Jingyi and Guo, Yulan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Disentangling Light Fields for Super-Resolution and Disparity Estimation}, 
  year={2023},
  volume={45},
  number={1},
  pages={425-443},
  doi={10.1109/TPAMI.2022.3152488}}

@ARTICLE{9495181,
  author={Bai, Yan and Liu, Jun and Lou, Yihang and Wang, Ce and Duan, Ling-Yu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Disentangled Feature Learning Network and a Comprehensive Benchmark for Vehicle Re-Identification}, 
  year={2022},
  volume={44},
  number={10},
  pages={6854-6871},
  doi={10.1109/TPAMI.2021.3099253}}

@ARTICLE{9241434,
  author={Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs}, 
  year={2022},
  volume={44},
  number={4},
  pages={2004-2018},
  doi={10.1109/TPAMI.2020.3034267}}

@ARTICLE{9200697,
  author={Simonelli, Andrea and Bulò, Samuel Rota and Porzi, Lorenzo and Antequera, Manuel López and Kontschieder, Peter},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Disentangling Monocular 3D Object Detection: From Single to Multi-Class Recognition}, 
  year={2022},
  volume={44},
  number={3},
  pages={1219-1231},
  doi={10.1109/TPAMI.2020.3025077}}

@ARTICLE{9585547,
  author={Eom, Chanho and Lee, Wonkyung and Lee, Geon and Ham, Bumsub},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Disentangled Representations for Short-Term and Long-Term Person Re-Identification}, 
  year={2022},
  volume={44},
  number={12},
  pages={8975-8991},
  doi={10.1109/TPAMI.2021.3122444}}

@inproceedings{liu2021heterogeneous,
  title={Heterogeneous risk minimization},
  author={Liu, Jiashuo and Hu, Zheyuan and Cui, Peng and Li, Bo and Shen, Zheyan},
  booktitle={International Conference on Machine Learning},
  pages={6804--6814},
  year={2021},
  organization={PMLR}
}

@article{10.1145/3604427,
author = {Li, Haoyang and Zhang, Ziwei and Wang, Xin and Zhu, Wenwu},
title = {Invariant Node Representation Learning under Distribution Shifts with Multiple Latent Environments},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3604427},
doi = {10.1145/3604427},
abstract = {Node representation learning methods, such as graph neural networks, show promising results when testing and training graph data come from the same distribution. However, the existing approaches fail to generalize under distribution shifts when the nodes reside in multiple latent environments. How to learn invariant node representations to handle distribution shifts with multiple latent environments remains unexplored. In this paper, we propose a novel Invariant Node representation Learning (INL) approach capable of generating invariant node representations based on the invariant patterns under distribution shifts with multiple latent environments by leveraging the invariance principle. Specifically, we define invariant and variant patterns as ego-subgraphs of each node, and identify the invariant ego-subgraphs through jointly accounting for node features and graph structures. In order to infer the latent environments of nodes, we propose a contrastive modularity-based graph clustering method based on the variant patterns. We further propose an invariant learning module to learn node representations that can generalize to distribution shifts. We theoretically show that our proposed method can achieve guaranteed performance under distribution shifts. Extensive experiments on both synthetic and real-world node classification benchmarks demonstrate that our method greatly outperforms state-of-the-art baselines under distribution shifts.},
note = {Just Accepted},
journal = {ACM Transactions on Information Systems (TOIS)},
month = {jun},
keywords = {Node Representation Learning, Graph Neural Networks, Distribution Shift}
}


@article{cai2023user,
  title={User cold-start recommendation via inductive heterogeneous graph neural network},
  author={Cai, Desheng and Qian, Shengsheng and Fang, Quan and Hu, Jun and Xu, Changsheng},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={41},
  number={3},
  pages={1--27},
  year={2023},
  publisher={ACM New York, NY}
}

@article{chen2020neural,
  title={Neural feature-aware recommendation with signed hypergraph convolutional network},
  author={Chen, Xu and Xiong, Kun and Zhang, Yongfeng and Xia, Long and Yin, Dawei and Huang, Jimmy Xiangji},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={39},
  number={1},
  pages={1--22},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{huang2023position,
  title={Position-enhanced and time-aware graph convolutional network for sequential recommendations},
  author={Huang, Liwei and Ma, Yutao and Liu, Yanbo and Danny Du, Bohong and Wang, Shuliang and Li, Deyi},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={41},
  number={1},
  pages={1--32},
  year={2023},
  publisher={ACM New York, NY}
}

@article{ma2023kr,
  title={Kr-gcn: Knowledge-aware reasoning with graph convolution network for explainable recommendation},
  author={Ma, Ting and Huang, Longtao and Lu, Qianqian and Hu, Songlin},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={41},
  number={1},
  pages={1--27},
  year={2023},
  publisher={ACM New York, NY}
}

@article{yang2021hgat,
  title={HGAT: Heterogeneous graph attention networks for semi-supervised short text classification},
  author={Yang, Tianchi and Hu, Linmei and Shi, Chuan and Ji, Houye and Li, Xiaoli and Nie, Liqiang},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={39},
  number={3},
  pages={1--29},
  year={2021},
  publisher={ACM New York, NY}
}

@article{zhang2022efraudcom,
  title={efraudcom: An e-commerce fraud detection system via competitive graph neural networks},
  author={Zhang, Ge and Li, Zhao and Huang, Jiaming and Wu, Jia and Zhou, Chuan and Yang, Jian and Gao, Jianliang},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={40},
  number={3},
  pages={1--29},
  year={2022},
  publisher={ACM New York, NY}
}

@article{zitnik2018modeling,
  title={Modeling polypharmacy side effects with graph convolutional networks},
  author={Zitnik, Marinka and Agrawal, Monica and Leskovec, Jure},
  journal={Bioinformatics},
  volume={34},
  number={13},
  pages={i457--i466},
  year={2018},
  publisher={Oxford University Press}
}

@article{li2022graph,
  title={Graph representation learning in biomedicine and healthcare},
  author={Li, Michelle M and Huang, Kexin and Zitnik, Marinka},
  journal={Nature Biomedical Engineering},
  volume={6},
  number={12},
  pages={1353--1369},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{li2023preference,
  title={Preference-aware Graph Attention Networks for Cross-Domain Recommendations with Collaborative Knowledge Graph},
  author={Li, Yakun and Hou, Lei and Li, Juanzi},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={41},
  number={3},
  pages={1--26},
  year={2023},
  publisher={ACM New York, NY}
}

@article{xie2021graph,
  title={Graph neural collaborative topic model for citation recommendation},
  author={Xie, Qianqian and Zhu, Yutao and Huang, Jimin and Du, Pan and Nie, Jian-Yun},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={40},
  number={3},
  pages={1--30},
  year={2021},
  publisher={ACM New York, NY}
}

@article{wang2021combining,
  title={Combining graph convolutional neural networks and label propagation},
  author={Wang, Hongwei and Leskovec, Jure},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={40},
  number={4},
  pages={1--27},
  year={2021},
  publisher={ACM New York, NY}
}


@inproceedings{bi2023predicting,
  title={Predicting the silent majority on graphs: Knowledge transferable graph neural network},
  author={Bi, Wendong and Xu, Bingbing and Sun, Xiaoqian and Xu, Li and Shen, Huawei and Cheng, Xueqi},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={274--285},
  year={2023}
}

@inproceedings{wu2020dynamic,
  title={Dynamic graph convolutional networks for entity linking},
  author={Wu, Junshuang and Zhang, Richong and Mao, Yongyi and Guo, Hongyu and Soflaei, Masoumeh and Huai, Jinpeng},
  booktitle={Proceedings of The ACM Web Conference 2020},
  pages={1149--1159},
  year={2020}
}

@inproceedings{taheri2019learning,
  title={Learning to represent the evolution of dynamic graphs with recurrent models},
  author={Taheri, Aynaz and Gimpel, Kevin and Berger-Wolf, Tanya},
  booktitle={Proceedings of the ACM Web Conference 2019},
  pages={301--307},
  year={2019}
}

@inproceedings{bai2023hgwavenet,
  title={HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction},
  author={Bai, Qijie and Nie, Changli and Zhang, Haiwei and Zhao, Dongming and Yuan, Xiaojie},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={523--532},
  year={2023}
}

@inproceedings{liu2022confidence,
  title={Confidence may cheat: Self-training on graph neural networks under distribution shift},
  author={Liu, Hongrui and Hu, Binbin and Wang, Xiao and Shi, Chuan and Zhang, Zhiqiang and Zhou, Jun},
  booktitle={Proceedings of the ACM Web Conference 2022},
  pages={1248--1258},
  year={2022}
}

@inproceedings{tang2023dynamic,
  title={Dynamic Graph Evolution Learning for Recommendation},
  author={Tang, Haoran and Wu, Shiqing and Xu, Guandong and Li, Qing},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1589--1598},
  year={2023}
}

@inproceedings{fu2021sdg,
  title={Sdg: A simplified and dynamic graph neural network},
  author={Fu, Dongqi and He, Jingrui},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2273--2277},
  year={2021}
}

@inproceedings{wang2022disenctr,
  title={DisenCTR: Dynamic graph-based disentangled representation for click-through rate prediction},
  author={Wang, Yifan and Qin, Yifang and Sun, Fang and Zhang, Bo and Hou, Xuyang and Hu, Ke and Cheng, Jia and Lei, Jun and Zhang, Ming},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2314--2318},
  year={2022}
}

@inproceedings{zhao2023time,
  title={Time-interval Aware Share Recommendation via Bi-directional Continuous Time Dynamic Graphs},
  author={Zhao, Ziwei and Zhu, Xi and Xu, Tong and Lizhiyu, Aakas and Yu, Yu and Li, Xueying and Yin, Zikai and Chen, Enhong},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={822--831},
  year={2023}
}

@inproceedings{yang2023generic,
  title={A Generic Learning Framework for Sequential Recommendation with Distribution Shifts},
  author={Yang, Zhengyi and He, Xiangnan and Zhang, Jizhi and Wu, Jiancan and Xin, Xin and Chen, Jiawei and Wang, Xiang},
  booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2023}
}

@inproceedings{gao2023alleviating,
  title={Alleviating structural distribution shift in graph anomaly detection},
  author={Gao, Yuan and Wang, Xiang and He, Xiangnan and Liu, Zhenguang and Feng, Huamin and Zhang, Yongdong},
  booktitle={Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
  pages={357--365},
  year={2023}
}

@inproceedings{liu2023good,
  title={Good-d: On unsupervised graph out-of-distribution detection},
  author={Liu, Yixin and Ding, Kaize and Liu, Huan and Pan, Shirui},
  booktitle={Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
  pages={339--347},
  year={2023}
}

@inproceedings{yang2023interpretable,
  title={Interpretable Research Interest Shift Detection with Temporal Heterogeneous Graphs},
  author={Yang, Qiang and Ma, Changsheng and Zhang, Qiannan and Gao, Xin and Zhang, Chuxu and Zhang, Xiangliang},
  booktitle={Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
  pages={321--329},
  year={2023}
}

@inproceedings{wang2023tutorial,
  title={A Tutorial on Domain Generalization},
  author={Wang, Jindong and Li, Haoliang and Pan, Sinno and Xie, Xing},
  booktitle={Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
  pages={1236--1239},
  year={2023}
}

@inproceedings{chen2022learning,
  title={Learning to generalize in heterogeneous federated networks},
  author={Chen, Cen and Ye, Tiandi and Wang, Li and Gao, Ming},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={159--168},
  year={2022}
}

@inproceedings{wang2022adagcl,
  title={AdaGCL: Adaptive Subgraph Contrastive Learning to Generalize Large-scale Graph Training},
  author={Wang, Yili and Zhou, Kaixiong and Miao, Rui and Liu, Ninghao and Wang, Xin},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2046--2055},
  year={2022}
}

@inproceedings{wang2022imbalanced,
  title={Imbalanced graph classification via graph-of-graph neural networks},
  author={Wang, Yu and Zhao, Yuying and Shah, Neil and Derr, Tyler},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2067--2076},
  year={2022}
}

@inproceedings{wang2019heterogeneous,
  title={Heterogeneous graph attention network},
  author={Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Ye, Yanfang and Cui, Peng and Yu, Philip S},
  booktitle={The world wide web conference},
  pages={2022--2032},
  year={2019}
}

@inproceedings{wang2017community,
  title={Community preserving network embedding},
  author={Wang, Xiao and Cui, Peng and Wang, Jing and Pei, Jian and Zhu, Wenwu and Yang, Shiqiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@article{wu2020comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={1},
  pages={4--24},
  year={2020},
  publisher={IEEE}
}

@article{xu2018powerful,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}

@inproceedings{zhang2023graph,
  title={Graph meets llms: Towards large graph models},
  author={Zhang, Ziwei and Li, Haoyang and Zhang, Zeyang and Qin, Yijian and Wang, Xin and Zhu, Wenwu},
  booktitle={NeurIPS 2023 Workshop: New Frontiers in Graph Learning},
  year={2023}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@misc{debate2-thu,
   author = {Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
   title = {Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
   pages = {arXiv:2305.19118},
   month = {May 01, 2023},
   note = {Work in progress},
   abstract = {Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate},
   keywords = {Computer Science - Computation and Language},
   year = {2023},
   type = {Electronic Article}
}

@misc{PHPrompting,
   author = {Zheng, Chuanyang and Liu, Zhengying and Xie, Enze and Li, Zhenguo and Li, Yu},
   title = {Progressive-Hint Prompting Improves Reasoning in Large Language Models},
   pages = {arXiv:2304.09797},
   month = {April 01, 2023},
   note = {Tech Report},
   abstract = {The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%), AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).},
   keywords = {Computer Science - Computation and Language
Computer Science -
Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@inproceedings{blender,
    title = "{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
    author = "Jiang, Dongfu  and
      Ren, Xiang  and
      Lin, Bill Yuchen",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "14165--14178",
    abstract = "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
}

@book{bondy1976graph,
  title={Graph theory with applications},
  author={Bondy, John Adrian and Murty, Uppaluri Siva Ramachandra and others},
  volume={290},
  year={1976},
  publisher={Macmillan London}
}

@inproceedings{
wang2023selfconsistency,
title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@inproceedings{spielman2008graph,
  title={Graph sparsification by effective resistances},
  author={Spielman, Daniel A and Srivastava, Nikhil},
  booktitle={Proceedings of the fortieth annual ACM symposium on Theory of computing},
  pages={563--568},
  year={2008}
}

@inproceedings{chen2021unified,
  title={A unified lottery ticket hypothesis for graph neural networks},
  author={Chen, Tianlong and Sui, Yongduo and Chen, Xuxi and Zhang, Aston and Wang, Zhangyang},
  booktitle={International conference on machine learning},
  pages={1695--1706},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2024graph,
  title={Graph lottery ticket automated},
  author={Zhang, Guibin and Wang, Kun and Huang, Wei and Yue, Yanwei and Wang, Yang and Zimmermann, Roger and Zhou, Aojun and Cheng, Dawei and Zeng, Jin and Liang, Yuxuan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{achille2018critical,
  title={Critical learning periods in deep networks},
  author={Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{entezari2020all,
  title={All you need is low (rank) defending against adversarial attacks on graphs},
  author={Entezari, Negin and Al-Sayouri, Saba A and Darvishzadeh, Amirali and Papalexakis, Evangelos E},
  booktitle={Proceedings of the 13th international conference on web search and data mining},
  pages={169--177},
  year={2020}
}

@inproceedings{ennadir2024simple,
  title={A Simple and Yet Fairly Effective Defense for Graph Neural Networks},
  author={Ennadir, Sofiane and Abbahaddou, Yassine and Lutzeyer, Johannes F and Vazirgiannis, Michalis and Bostr{\"o}m, Henrik},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={21063--21071},
  year={2024}
}

@inproceedings{alchihabi2023efficient,
  title={Efficient Low-Rank GNN Defense Against Structural Attacks},
  author={Alchihabi, Abdullah and En, Qing and Guo, Yuhong},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}

@article{you2019drawing,
  title={Drawing early-bird tickets: Towards more efficient training of deep networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G and Wang, Zhangyang and Lin, Yingyan},
  journal={arXiv preprint arXiv:1909.11957},
  year={2019}
}

@article{zhang2024two,
  title={Two heads are better than one: Boosting graph sparse training via semantic and topological awareness},
  author={Zhang, Guibin and Yue, Yanwei and Wang, Kun and Fang, Junfeng and Sui, Yongduo and Wang, Kai and Liang, Yuxuan and Cheng, Dawei and Pan, Shirui and Chen, Tianlong},
  journal={arXiv preprint arXiv:2402.01242},
  year={2024}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{li2023api,
  title={Api-bank: A comprehensive benchmark for tool-augmented llms},
  author={Li, Minghao and Zhao, Yingxiu and Yu, Bowen and Song, Feifan and Li, Hangyu and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2304.08244},
  year={2023}
}

@article{wang2023brave,
  title={Brave the wind and the waves: Discovering robust and generalizable graph lottery tickets},
  author={Wang, Kun and Liang, Yuxuan and Li, Xinglin and Li, Guohao and Ghanem, Bernard and Zimmermann, Roger and Yi, Huahui and Zhang, Yudong and Wang, Yang and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{chen2023demystifying,
  title={Demystifying graph sparsification algorithms in graph properties preservation},
  author={Chen, Yuhan and Ye, Haojie and Vedula, Sanketh and Bronstein, Alex and Dreslinski, Ronald and Mudge, Trevor and Talati, Nishil},
  journal={Proceedings of the VLDB Endowment},
  volume={17},
  number={3},
  pages={427--440},
  year={2023},
  publisher={VLDB Endowment}
}

@ARTICLE{augmented-lm-survey,
       author = {{Mialon}, Gr{\'e}goire and {Dess{\`\i}}, Roberto and {Lomeli}, Maria and {Nalmpantis}, Christoforos and {Pasunuru}, Ram and {Raileanu}, Roberta and {Rozi{\`e}re}, Baptiste and {Schick}, Timo and {Dwivedi-Yu}, Jane and {Celikyilmaz}, Asli and {Grave}, Edouard and {LeCun}, Yann and {Scialom}, Thomas},
        title = "{Augmented Language Models: a Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = feb,
          eid = {arXiv:2302.07842},
        pages = {arXiv:2302.07842},
archivePrefix = {arXiv},
       eprint = {2302.07842},
 primaryClass = {cs.CL},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{audio-gpt,
   author = {Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and Ren, Yi and Zhao, Zhou and Watanabe, Shinji},
   title = {AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head},
   pages = {arXiv:2304.12995},
   month = {April 01, 2023},
   abstract = {Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence
Computer Science - Sound
Electrical
Engineering and Systems Science - Audio and Speech Processing},
   year = {2023},
   type = {Electronic Article}
}

@misc{visual-gpt,
   author = {Chen, Jun and Guo, Han and Yi, Kai and Li, Boyang and Elhoseiny, Mohamed},
   title = {VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning},
   pages = {arXiv:2102.10407},
   month = {February 01, 2021},
   abstract = {The ability to quickly learn from a small quantity oftraining data widens the range of machine learning applications. In this paper, we propose a data-efficient image captioning model, VisualGPT, which leverages the linguistic knowledge from a large pretrained language model(LM). A crucial challenge is to balance between the use of visual information in the image and prior linguistic knowledge acquired from pretraining. We designed a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the pretrained LM as the language decoder ona small amount of in-domain training data. The proposed self-resurrecting activation unit produces sparse activations but has reduced susceptibility to zero gradients. We train the proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions training data. Under these conditions, we outperform the best baseline model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. To the best of our knowledge, this is the first work that improves data efficiency of image captioning by utilizing LM pretrained on unimodal data. Our code is available at: https://github.com/Vision-CAIR/VisualGPT.},
   keywords = {Computer Science - Computer Vision and Pattern Recognition
Computer
Science - Artificial Intelligence
Computer Science - Computation and
Language
Computer Science - Multimedia},
   year = {2021},
   type = {Electronic Article}
}

@inproceedings{zitzler2004indicator,
  title={Indicator-based selection in multiobjective search},
  author={Zitzler, Eckart and K{\"u}nzli, Simon},
  booktitle={International conference on parallel problem solving from nature},
  pages={832--842},
  year={2004},
  organization={Springer}
}

@article{lee2025evolving-deepmind,
  title={Evolving Deeper LLM Thinking},
  author={Lee, Kuang-Huei and Fischer, Ian and Wu, Yueh-Hua and Marwood, Dave and Baluja, Shumeet and Schuurmans, Dale and Chen, Xinyun},
  journal={arXiv preprint arXiv:2501.09891},
  year={2025}
}

@article{xue2024genagent,
  title={GenAgent: Build Collaborative AI Systems with Automated Workflow Generation--Case Studies on ComfyUI},
  author={Xue, Xiangyuan and Lu, Zeyu and Huang, Di and Ouyang, Wanli and Bai, Lei},
  journal={arXiv preprint arXiv:2409.01392},
  year={2024}
}

@article{white2023nas1000,
  title={Neural architecture search: Insights from 1000 papers},
  author={White, Colin and Safari, Mahmoud and Sukthanker, Rhea and Ru, Binxin and Elsken, Thomas and Zela, Arber and Dey, Debadeepta and Hutter, Frank},
  journal={arXiv preprint arXiv:2301.08727},
  year={2023}
}

@article{chollet2019arc,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@article{guo2023evoprompt,
  title={Connecting large language models with evolutionary algorithms yields powerful prompt optimizers},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu},
  journal={arXiv preprint arXiv:2309.08532},
  year={2023}
}

@article{hu2024routerbench,
  title={ROUTERBENCH: A Benchmark for Multi-LLM Routing System},
  author={Hu, Qitian Jason and Bieker, Jacob and Li, Xiuyu and Jiang, Nan and Keigwin, Benjamin and Ranganath, Gaurav and Keutzer, Kurt and Upadhyay, Shriyash Kaustubh},
  journal={arXiv preprint arXiv:2403.12031},
  year={2024}
}

@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@article{chen2023autoagents,
  title={Autoagents: A framework for automatic agent generation},
  author={Chen, Guangyao and Dong, Siwei and Shu, Yu and Zhang, Ge and Sesay, Jaward and Karlsson, B{\"o}rje F and Fu, Jie and Shi, Yemin},
  journal={arXiv preprint arXiv:2309.17288},
  year={2023}
}

@article{feng2024graphrouter,
  title={Graphrouter: A graph-based router for llm selections},
  author={Feng, Tao and Shen, Yanzhen and You, Jiaxuan},
  journal={arXiv preprint arXiv:2410.03834},
  year={2024}
}

@ARTICLE{neural-sequence,
       author = {{Dabagia}, Max and {Papadimitriou}, Christos H. and {Vempala}, Santosh S.},
        title = "{Computation with Sequences in the Brain}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
         year = 2023,
        month = jun,
          eid = {arXiv:2306.03812},
        pages = {arXiv:2306.03812},
archivePrefix = {arXiv},
       eprint = {2306.03812},
 primaryClass = {cs.NE},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{khattab2023dspy,
  title={Dspy: Compiling declarative language model calls into self-improving pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T and Moazam, Hanna and others},
  journal={arXiv preprint arXiv:2310.03714},
  year={2023}
}

@article{zelikman2023self,
  title={Self-taught optimizer (stop): Recursively self-improving code generation},
  author={Zelikman, Eric and Lorch, Eliana and Mackey, Lester and Kalai, Adam Tauman},
  journal={arXiv preprint arXiv:2310.02304},
  year={2023}
}

@article{liu2024evaluating,
  title={Evaluating Language Models for Efficient Code Generation},
  author={Liu, Jiawei and Xie, Songrun and Wang, Junhao and Wei, Yuxiang and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2408.06450},
  year={2024}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@misc{chatdev,
   author = {Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
   title = {Communicative Agents for Software Development},
   pages = {arXiv:2307.07924},
   month = {July 01, 2023},
   note = {25 pages, 9 figures, 2 tables},
   abstract = {Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development.},
   keywords = {Computer Science - Software Engineering
Computer Science -
Computation and Language
Computer Science - Multiagent Systems},
   year = {2023},
   type = {Electronic Article}
}

@article{fernando2023promptbreeder,
  title={Promptbreeder: Self-referential self-improvement via prompt evolution},
  author={Fernando, Chrisantha and Banarse, Dylan and Michalewski, Henryk and Osindero, Simon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2309.16797},
  year={2023}
}

@article{hu2024evomac,
  title={Self-Evolving Multi-Agent Collaboration Networks for Software Development},
  author={Hu, Yue and Cai, Yuzhu and Du, Yaxin and Zhu, Xinyu and Liu, Xiangrui and Yu, Zijie and Hou, Yuchen and Tang, Shuo and Chen, Siheng},
  journal={arXiv preprint arXiv:2410.16946},
  year={2024}
}

@article{zhang2024gdesigner,
  title={G-designer: Architecting multi-agent communication topologies via graph neural networks},
  author={Zhang, Guibin and Yue, Yanwei and Sun, Xiangguo and Wan, Guancheng and Yu, Miao and Fang, Junfeng and Wang, Kun and Cheng, Dawei},
  journal={arXiv preprint arXiv:2410.11782},
  year={2024}
}

@misc{debate3-multi-models,
   author = {Xiong, Kai and Ding, Xiao and Cao, Yixin and Liu, Ting and Qin, Bing},
   title = {Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate},
   pages = {arXiv:2305.11595},
   month = {May 01, 2023},
   abstract = {Large Language Models (LLMs) have demonstrated human-like intelligence and are widely used in various applications. However, LLMs still exhibit various kinds of inconsistency problems. Existing works mainly focus on the inconsistency issues within a single LLM, while we investigate the inter-consistency among multiple LLMs, which is critical for collaborating to solve a complex task. To examine whether LLMs can collaborate to ultimately achieve a consensus for the shared goal and whether LLMs easily change their viewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct a three-stage debate aligned with real-world scenarios: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance. Moreover, we observe that stronger LLMs tend to dominate the debates by adhering to their perspectives, while weaker ones are more likely to change viewpoints. Additionally, we highlight the importance of a competent judge, such as GPT-4, to draw more proper conclusions. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}


@article{chen2024compundLLM,
  title={Are more llm calls all you need? towards scaling laws of compound inference systems},
  author={Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2403.02419},
  year={2024}
}

@article{piatti2024cooperate,
  title={Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents},
  author={Piatti, Giorgio and Jin, Zhijing and Kleiman-Weiner, Max and Sch{\"o}lkopf, Bernhard and Sachan, Mrinmaya and Mihalcea, Rada},
  journal={arXiv preprint arXiv:2404.16698},
  year={2024}
}

@article{zhang2006introduction,
  title={Introduction to graph theory},
  author={Zhang, Ping and Chartrand, Gary},
  journal={Tata McGraw-Hill},
  volume={2},
  pages={2--1},
  year={2006}
}

@article{ma2023laser,
  title={Laser: Llm agent with state-space exploration for web navigation},
  author={Ma, Kaixin and Zhang, Hongming and Wang, Hongwei and Pan, Xiaoman and Yu, Wenhao and Yu, Dong},
  journal={arXiv preprint arXiv:2309.08172},
  year={2023}
}


@article{bouzenia2024repairagent,
  title={Repairagent: An autonomous, llm-based agent for program repair},
  author={Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
  journal={arXiv preprint arXiv:2403.17134},
  year={2024}
}



@article{ishibashi2024selforganize-mother,
  title={Self-organized agents: A llm multi-agent framework toward ultra large-scale code generation and optimization},
  author={Ishibashi, Yoichi and Nishimura, Yoshimasa},
  journal={arXiv preprint arXiv:2404.02183},
  year={2024}
}

@article{mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{hendrycks2021ethics,
  title={Aligning AI With Shared Human Values},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@misc{generative-agents-simulacra,
   author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Ringel Morris, Meredith and Liang, Percy and Bernstein, Michael S.},
   title = {Generative Agents: Interactive Simulacra of Human Behavior},
   pages = {arXiv:2304.03442},
   month = {April 01, 2023},
   abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
   keywords = {Computer Science - Human-Computer Interaction
Computer Science -
Artificial Intelligence
Computer Science - Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@article{zhang2024classroom,
  title={Simulating classroom education with llm-empowered agents},
  author={Zhang, Zheyuan and Zhang-Li, Daniel and Yu, Jifan and Gong, Linlu and Zhou, Jinchang and Liu, Zhiyuan and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2406.19226},
  year={2024}
}

@article{zhao2023competeai,
  title={Competeai: Understanding the competition behaviors in large language model-based agents},
  author={Zhao, Qinlin and Wang, Jindong and Zhang, Yixuan and Jin, Yiqiao and Zhu, Kaijie and Chen, Hao and Xie, Xing},
  journal={arXiv preprint arXiv:2310.17512},
  year={2023}
}

@misc{multi-persona,
   author = {Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
   title = {Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
   pages = {arXiv:2307.05300},
   month = {July 01, 2023},
   note = {work in progress},
   abstract = {Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
   keywords = {Computer Science - Artificial Intelligence
Computer Science -
Computation and Language},
   year = {2023},
   type = {Electronic Article}
}

@misc{bargaining-feedback,
   author = {Fu, Yao and Peng, Hao and Khot, Tushar and Lapata, Mirella},
   title = {Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback},
   pages = {arXiv:2305.10142},
   month = {May 01, 2023},
   note = {Preprint. Code at https://github.com/FranxYao/GPT-Bargaining},
   abstract = {We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game's rules or cannot incorporate AI feedback for further improvement. (2) Models' abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.},
   keywords = {Computer Science - Computation and Language},
   year = {2023},
   type = {Electronic Article}
}

@misc{sot,
   author = {Ning, Xuefei and Lin, Zinan and Zhou, Zixuan and Yang, Huazhong and Wang, Yu},
   title = {Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding},
   pages = {arXiv:2307.15337},
   month = {July 01, 2023},
   abstract = {This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}

@article{hua2023war,
  title={War and peace (waragent): Large language model-based multi-agent simulation of world wars},
  author={Hua, Wenyue and Fan, Lizhou and Li, Lingyao and Mei, Kai and Ji, Jianchao and Ge, Yingqiang and Hemphill, Libby and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2311.17227},
  year={2023}
}

@article{chen2023gamegpt,
  title={Gamegpt: Multi-agent collaborative framework for game development},
  author={Chen, Dake and Wang, Hanbin and Huo, Yunhao and Li, Yuzhao and Zhang, Haoyang},
  journal={arXiv preprint arXiv:2310.08067},
  year={2023}
}


@article{cohen2023lm,
  title={Lm vs lm: Detecting factual errors via cross examination},
  author={Cohen, Roi and Hamri, May and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2305.13281},
  year={2023}
}

@misc{self-correction,
   author = {Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang},
   title = {Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies},
   pages = {arXiv:2308.03188},
   month = {August 01, 2023},
   note = {Work in Progress},
   abstract = {Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}


@article{qian2024scaling,
  title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
  author={Qian, Chen and Xie, Zihao and Wang, Yifei and Liu, Wei and Dang, Yufan and Du, Zhuoyun and Chen, Weize and Yang, Cheng and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2406.07155},
  year={2024}
}

@inproceedings{zhuge2024gptswarm,
  title={GPTSwarm: Language Agents as Optimizable Graphs},
  author={Zhuge, Mingchen and Wang, Wenyi and Kirsch, Louis and Faccio, Francesco and Khizbullin, Dmitrii and Schmidhuber, J{\"u}rgen},
  booktitle={Forty-first International Conference on Machine Learning},
year={2024}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{coalitional-game,
  author={Saad, Walid and Han, Zhu and Debbah, Merouane and Hjorungnes, Are and Basar, Tamer},
  journal={IEEE Signal Processing Magazine}, 
  title={Coalitional game theory for communication networks}, 
  year={2009},
  volume={26},
  number={5},
  pages={77-97},
}

@inproceedings{held-yang-2023-shapley,
    title = "Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers",
    author = "Held, William  and
      Yang, Diyi",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.177",
    pages = "2416--2427",
    abstract = "Multilingual transformer-based models demonstrate remarkable zero and few-shot transfer across languages by learning and reusing language-agnostic features. However, as a fixed-size model acquires more languages, its performance across all languages degrades. Those who attribute this interference phenomenon to limited model capacity address the problem by adding additional parameters, despite evidence that transformer-based models are overparameterized. In this work, we show that it is possible to reduce interference by instead identifying and pruning language-specific attention heads. First, we use Shapley Values, a credit allocation metric from coalitional game theory, to identify attention heads that introduce interference. Then, we show that pruning such heads from a fixed model improves performance for a target language on both sentence classification and structural prediction. Finally, we provide insights on language-agnostic and language-specific attention heads using attention visualization.",
}

@INPROCEEDINGS{imp-score,
  author={Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I. and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={NISP: Pruning Networks Using Neuron Importance Score Propagation}, 
  year={2018},
  volume={},
  number={},
  pages={9194-9203}}

@misc{human-eval,
   author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Petroski Such, Felipe and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Hebgen Guss, William and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
   title = {Evaluating Large Language Models Trained on Code},
   pages = {arXiv:2107.03374},
   month = {July 01, 2021},
   keywords = {Computer Science - Machine Learning},
   year = {2021},
   type = {Electronic Article}
}

@ARTICLE{minecraft-agent,
       author = {{Zhu}, Xizhou and {Chen}, Yuntao and {Tian}, Hao and {Tao}, Chenxin and {Su}, Weijie and {Yang}, Chenyu and {Huang}, Gao and {Li}, Bin and {Lu}, Lewei and {Wang}, Xiaogang and {Qiao}, Yu and {Zhang}, Zhaoxiang and {Dai}, Jifeng},
        title = "{Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
         year = 2023,
        month = may,
          eid = {arXiv:2305.17144},
        pages = {arXiv:2305.17144},
archivePrefix = {arXiv},
       eprint = {2305.17144},
 primaryClass = {cs.AI},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{meta-gpt,
   author = {Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin},
   title = {MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},
   pages = {arXiv:2308.00352},
   month = {August 01, 2023},
   abstract = {Recently, remarkable progress has been made in automated task-solving through the use of multi-agent driven by large language models (LLMs). However, existing LLM-based multi-agent works primarily focus on solving simple dialogue tasks, and complex tasks are rarely studied, mainly due to the LLM hallucination problem. This type of hallucination becomes cascading when naively chaining multiple intelligent agents, resulting in a failure to effectively address complex problems. Therefore, we introduce MetaGPT, an innovative framework that incorporates efficient human workflows as a meta programming approach into LLM-based multi-agent collaboration. Specifically, MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to enhance structured coordination. Subsequently, it mandates modular outputs, empowering agents with domain expertise comparable to human professionals, to validate outputs and minimize compounded errors. In this way, MetaGPT leverages the assembly line paradigm to assign diverse roles to various agents, thereby establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments on collaborative software engineering benchmarks demonstrate that MetaGPT generates more coherent and correct solutions compared to existing chat-based multi-agent systems. This highlights the potential of integrating human domain knowledge into multi-agent systems, thereby creating new opportunities to tackle complex real-world challenges. The GitHub repository of this project is publicly available on:https://github.com/geekan/MetaGPT.},
   keywords = {Computer Science - Artificial Intelligence
Computer Science -
Multiagent Systems},
   year = {2023},
   type = {Electronic Article}
}

@article{zheng2023take-a-step-back,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}

@misc{self-debug,
   author = {Olausson, Theo X. and Priya Inala, Jeevana and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando},
   title = {Demystifying GPT Self-Repair for Code Generation},
   pages = {arXiv:2306.09896},
   month = {June 01, 2023},
   abstract = {Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -- in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness of self-repair is only seen in GPT-4. We also observe that self-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback on the programs generated by GPT-3.5 and using expert human programmers to give feedback on the programs generated by GPT-4, we unlock significant performance gains.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence
Computer Science - Programming Languages
Computer Science - Software Engineering},
   year = {2023},
   type = {Electronic Article}
}

@article{xu2023expertprompting,
  title={Expertprompting: Instructing large language models to be distinguished experts},
  author={Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
  journal={arXiv preprint arXiv:2305.14688},
  year={2023}
}

@inproceedings{
chen2023codet,
title={CodeT:  Code Generation with Generated Tests},
author={Bei Chen and Fengji Zhang and Anh Nguyen and Daoguang Zan and Zeqi Lin and Jian-Guang Lou and Weizhu Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ktrw68Cmu9c}
}

@ARTICLE{self-evaluation-decode,
       author = {{Xie}, Yuxi and {Kawaguchi}, Kenji and {Zhao}, Yiran and {Zhao}, Xu and {Kan}, Min-Yen and {He}, Junxian and {Xie}, Qizhe},
        title = "{Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2023,
        month = apr,
          eid = {arXiv:2305.00633},
        pages = {arXiv:2305.00633},
archivePrefix = {arXiv},
       eprint = {2305.00633},
 primaryClass = {cs.CL},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{llm-overconfident,
       author = {{Xiong}, Miao and {Hu}, Zhiyuan and {Lu}, Xinyang and {Li}, Yifei and {Fu}, Jie and {He}, Junxian and {Hooi}, Bryan},
        title = "{Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = jun,
          eid = {arXiv:2306.13063},
        pages = {arXiv:2306.13063},
archivePrefix = {arXiv},
       eprint = {2306.13063},
 primaryClass = {cs.CL},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{llm-as-a-judge,
   author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric. P and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
   title = {Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
   pages = {arXiv:2306.05685},
   month = {June 01, 2023},
   abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA/Vicuna. We will publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}

@misc{sliding-window,
   author = {Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and Bendersky, Michael},
   title = {Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting},
   pages = {arXiv:2306.17563},
   month = {June 01, 2023},
   note = {12 pages, 3 figures},
   abstract = {Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, by over 5% at NDCG@1. On TREC-DL2019, PRP is only inferior to the GPT-4 solution on the NDCG@5 and NDCG@10 metrics, while outperforming other existing solutions, such as InstructGPT which has 175B parameters, by over 10% for nearly all ranking metrics. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity. We also discuss other benefits of PRP, such as supporting both generation and scoring LLM APIs, as well as being insensitive to input ordering.},
   keywords = {Computer Science - Information Retrieval
Computer Science -
Computation and Language
Computer Science - Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@inproceedings{SHAP,
author = {Lundberg, Scott M. and Lee, Su-In},
title = {A Unified Approach to Interpreting Model Predictions},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4768–4777},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@Article{shapley-origin,
  author={Stan Lipovetsky and Michael Conklin},
  title={{Analysis of regression in game theory approach}},
  journal={Applied Stochastic Models in Business and Industry},
  year=2001,
  volume={17},
  number={4},
  pages={319-330},
  month={October},
  keywords={},
  abstract={Working with multiple regression analysis a researcher usually wants to know a comparative importance of predictors in the model. However, the analysis can be made difficult because of multicollinearity among regressors, which produces biased coefficients and negative inputs to multiple determination from presum ably useful regressors. To solve this problem we apply a tool from the co‐operative games theory, the Shapley Value imputation. We demonstrate the theoretical and practical advantages of the Shapley Value and show that it provides consistent results in the presence of multicollinearity. Copyright © 2001 John Wiley \& Sons, Ltd.},
  url={https://ideas.repec.org/a/wly/apsmbi/v17y2001i4p319-330.html}
}

@misc{got,
   author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
   title = {Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
   pages = {arXiv:2308.09687},
   month = {August 01, 2023},
   abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-ofThought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
   keywords = {Computer Science - Computation and Language
Computer Science -
Artificial Intelligence
Computer Science - Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@misc{agent-bench,
   author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
   title = {AgentBench: Evaluating LLMs as Agents},
   pages = {arXiv:2308.03688},
   month = {August 01, 2023},
   note = {38 pages},
   abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench},
   keywords = {Computer Science - Artificial Intelligence
Computer Science -
Computation and Language
Computer Science - Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@misc{cot,
   author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
   title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
   journal = {arXiv:2201.11903},
   month = {January 01, 2022},
   year = {2022}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv:1408.5882},
  year={2014}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@inproceedings{listmle,
author = {Xia, Fen and Liu, Tie-Yan and Wang, Jue and Zhang, Wensheng and Li, Hang},
title = {Listwise Approach to Learning to Rank: Theory and Algorithm},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the ground-truth list. Existing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and good performances by them have been observed. Unfortunately, the underlying theory was not sufficiently studied so far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through investigations on the properties of the loss functions, including consistency, soundness, continuity, differentiability, convexity, and efficiency. A sufficient condition on consistency for ranking is given, which seems to be the first such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet. The use of the likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function offers better properties, and also leads to better experimental results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1192–1199},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@misc{autogpt,
  author = {Toran Bruce Richards and et al.},
  title = {Auto-GPT: An Autonomous GPT-4 Experiment},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Significant-Gravitas/Auto-GPT}}
}

@misc{autogen,
   author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
   title = {AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},
   journal = {arXiv:2308.08155},
   month = {August 01, 2023},
   year = {2023}
}

@inproceedings{
yao2023react,
title={ReAct: Synergizing Reasoning and Acting in Language Models},
author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@ARTICLE{voyager,
       author = {{Wang}, Guanzhi and {Xie}, Yuqi and {Jiang}, Yunfan and {Mandlekar}, Ajay and {Xiao}, Chaowei and {Zhu}, Yuke and {Fan}, Linxi and {Anandkumar}, Anima},
        title = "{Voyager: An Open-Ended Embodied Agent with Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2023,
        month = may,
          eid = {arXiv:2305.16291},
        pages = {arXiv:2305.16291},
archivePrefix = {arXiv},
       eprint = {2305.16291}
}

@misc{jin2023surrealdriver,
      title={SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model}, 
      author={Ye Jin and Xiaoxi Shen and Huiling Peng and Xiaoan Liu and Jingli Qin and Jiayang Li and Jintao Xie and Peizhong Gao and Guyue Zhou and Jiangtao Gong},
      year={2023},
      eprint={2309.13193},
      archivePrefix={arXiv}
}

@article{human-team-optimization,
author = {Lykourentzou, Ioanna and Vinella, Federica Lucia and Ahmed, Faez and Papastathis, Costas and Papangelis, Konstantinos and Khan, Vassilis-Javed and Masthoff, Judith},
title = {Self-Organization in Online Collaborative Work Settings},
year = {2022},
issue_date = {July-September 2022},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {1},
number = {1},
abstract = {As the volume and complexity of distributed online work increases, collaboration among people who have never worked together in the past is becoming increasingly necessary. Recent research has proposed algorithms to maximize the performance of online collaborations by grouping workers in a top-down fashion and according to a set of predefined decision criteria. This approach often means that workers have little say in the collaboration formation process. Depriving users of control over whom they will work with can stifle creativity and initiative-taking, increase psychological discomfort, and, overall, result in less-than-optimal collaboration results—especially when the task concerned is open-ended, creative, and complex. In this work, we propose an alternative model, called Self-Organizing Pairs (SOPs), which relies on the crowd of online workers themselves to organize into effective work dyads. Supported but not guided by an algorithm, SOPs are a new human-centered computational structure, which enables participants to control, correct, and guide the output of their collaboration as a collective. Experimental results, comparing SOPs to two benchmarks that do not allow user agency, and on an iterative task of fictional story writing, reveal that participants in the SOPs condition produce creative outcomes of higher quality, and report higher satisfaction with their collaboration. Finally, we find that similarly to machine learning-based self-organization, human SOPs exhibit emergent collective properties, including the presence of an objective function and the tendency to form more distinct clusters of compatible collaborators.},
journal = {Collective Intelligence},
month = {sep},
numpages = {35},
keywords = {self-organization, Online collaborative work, macrotask, distributed work, complex work}
}

@misc{babyagi,
  author = {Yohei Nakajima},
  title = {BabyAGI},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yoheinakajima/babyagi}}
}

@misc{agentgpt,
  author = {Reworkd},
  title = {AgentGPT},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/reworkd/AgentGPT}}
}

@article{zhang2023astools,
  title={Exploring collaboration mechanisms for llm agents: A social psychology view},
  author={Zhang, Jintian and Xu, Xin and Deng, Shumin},
  journal={arXiv preprint arXiv:2310.02124},
  year={2023}
}

@article{pesce2023learning,
  title={Learning multi-agent coordination through connectivity-driven communication},
  author={Pesce, Emanuele and Montana, Giovanni},
  journal={Machine Learning},
  volume={112},
  number={2},
  pages={483--514},
  year={2023},
  publisher={Springer}
}

@inproceedings{liu2022temporal,
  title={Temporal dynamic weighted graph convolution for multi-agent reinforcement learning},
  author={Liu, Yuntao and Dou, Yong and Li, Yuan and Xu, Xinhai and Liu, Donghong},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={44},
  number={44},
  year={2022}
}

@article{hu2024magraph,
  title={Learning multi-agent communication from graph modeling perspective},
  author={Hu, Shengchao and Shen, Li and Zhang, Ya and Tao, Dacheng},
  journal={arXiv preprint arXiv:2405.08550},
  year={2024}
}

@misc{bolaa,
   author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Xue, Le and Heinecke, Shelby and Murthy, Rithesh and Feng, Yihao and Chen, Zeyuan and Niebles, Juan Carlos and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
   title = {BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents},
   pages = {arXiv:2308.05960},
   month = {August 01, 2023},
   note = {Preprint},
   abstract = {The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \url{https://github.com/salesforce/BOLAA}.},
   keywords = {Computer Science - Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}

@misc{cumulative-cr,
   author = {Zhang, Yifan and Yang, Jingqin and Yuan, Yang and Chi-Chih Yao, Andrew},
   title = {Cumulative Reasoning with Large Language Models},
   pages = {arXiv:2308.04371},
   month = {August 01, 2023},
   abstract = {While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, CR streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3%, and achieves the astonishing accuracy of 98.04% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94%, which signifies a substantial enhancement of 20% over the previous state-of-the-art method (code is available at https://github.com/iiis-ai/cumulative-reasoning).},
   keywords = {Computer Science - Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}

@misc{tptu,
   author = {Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Zeng, Xingyu and Zhao, Rui},
   title = {TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents},
   pages = {arXiv:2308.03427},
   month = {August 01, 2023},
   abstract = {With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.},
   keywords = {Computer Science - Artificial Intelligence},
   year = {2023},
   type = {Electronic Article}
}

@article{lu2023chameleon,
  title={Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.09842},
  year={2023}
}

@misc{deep-network,
   author = {Sordoni, Alessandro and Yuan, Xingdi and Côté, Marc-Alexandre and Pereira, Matheus and Trischler, Adam and Xiao, Ziang and Hosseini, Arian and Niedtner, Friederike and Le Roux, Nicolas},
   title = {Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference},
   pages = {arXiv:2306.12509},
   month = {June 01, 2023},
   keywords = {Computer Science - Computation and Language
Computer Science -
Machine Learning},
   year = {2023},
   type = {Electronic Article}
}

@misc{chen2023agentverse,
      title={AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents}, 
      author={Weize Chen and Yusheng Su and Jingwei Zuo and Cheng Yang and Chenfei Yuan and Chen Qian and Chi-Min Chan and Yujia Qin and Yaxi Lu and Ruobing Xie and Zhiyuan Liu and Maosong Sun and Jie Zhou},
      year={2023},
      eprint={2308.10848},
      archivePrefix={arXiv}
}

@article{rasal2024llm,
  title={Llm harmony: Multi-agent communication for problem solving},
  author={Rasal, Sumedh},
  journal={arXiv preprint arXiv:2401.01312},
  year={2024}
}

@misc{madaan2023selfrefine,
    title={Self-Refine: Iterative Refinement with Self-Feedback}, 
    author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Sean Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and Amir Yazdanbakhsh and Peter Clark},
    year={2023},
    eprint={2303.17651},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{aggarwal2023adaptive-consistency,
      title={Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs}, 
      author={Pranjal Aggarwal and Aman Madaan and Yiming Yang and Mausam},
      year={2023},
      eprint={2305.11860},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tot,
   author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
   title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
   journal = {arXiv:2305.10601},
   month = {May 01, 2023},
   year = {2023}
}

@ARTICLE{embodied,
       author = {{Zhang}, Hongxin and {Du}, Weihua and {Shan}, Jiaming and {Zhou}, Qinhong and {Du}, Yilun and {Tenenbaum}, Joshua B. and {Shu}, Tianmin and {Gan}, Chuang},
        title = "{Building Cooperative Embodied Agents Modularly with Large Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
         year = 2023,
        month = jul,
          eid = {arXiv:2307.02485},
        pages = {arXiv:2307.02485},
archivePrefix = {arXiv},
       eprint = {2307.02485},
 primaryClass = {cs.AI},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{chateval,
       author = {{Chan}, Chi-Min and {Chen}, Weize and {Su}, Yusheng and {Yu}, Jianxuan and {Xue}, Wei and {Zhang}, Shanghang and {Fu}, Jie and {Liu}, Zhiyuan},
        title = "{ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2023,
        month = aug,
       eprint = {2308.07201}
}

@ARTICLE{llm-dba,
       author = {{Zhou}, Xuanhe and {Li}, Guoliang and {Liu}, Zhiyuan},
        title = "{LLM As DBA}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Databases, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2023,
        month = aug,
          eid = {arXiv:2308.05481},
        pages = {arXiv:2308.05481},
archivePrefix = {arXiv},
       eprint = {2308.05481},
 primaryClass = {cs.DB},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{gpt-iot,
       author = {{Nascimento}, Nathalia and {Alencar}, Paulo and {Cowan}, Donald},
        title = "{GPT-in-the-Loop: Adaptive Decision-Making for Multiagent Systems}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Multiagent Systems, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Software Engineering},
         year = 2023,
        month = aug,
          eid = {arXiv:2308.10435},
        pages = {arXiv:2308.10435},
archivePrefix = {arXiv},
       eprint = {2308.10435},
 primaryClass = {cs.MA},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{post-2018-sacrebleu,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}

@article{Ren2020CodeBLEUAM,
  title={CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  author={Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and M. Zhou and Ambrosio Blanco and Shuai Ma},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.10297},
}

@inproceedings{trueskill,
 author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {TrueSkill\texttrademark : A Bayesian Skill Rating System},
 url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf},
 volume = {19},
 year = {2006}
}

@ARTICLE{self-collab-codegen,
       author = {{Dong}, Yihong and {Jiang}, Xue and {Jin}, Zhi and {Li}, Ge},
        title = "{Self-collaboration Code Generation via ChatGPT}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Software Engineering},
         year = 2023,
        month = apr,
          eid = {arXiv:2304.07590},
        pages = {arXiv:2304.07590},
archivePrefix = {arXiv},
       eprint = {2304.07590},
 primaryClass = {cs.SE},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{human-collaboration,
  title={Advancing the Science of Collaborative Problem Solving},
  author={Arthur C. Graesser and Stephen M. Fiore and Samuel Greiff and Jessica Andrews-Todd and Peter W. Foltz and Friedrich W. Hesse},
  journal={Psychological Science in the Public Interest},
  year={2018},
  volume={19},
  pages={59 - 92},
}

@ARTICLE{multi-agent,
  author={Dorri, Ali and Kanhere, Salil S. and Jurdak, Raja},
  journal={IEEE Access}, 
  title={Multi-Agent Systems: A Survey}, 
  year={2018},
  volume={6},
  number={},
  pages={28573-28593}}

@misc{chatllm-network,
   author = {Hao, Rui and Hu, Linmei and Qi, Weijian and Wu, Qingliu and Zhang, Yirui and Nie, Liqiang},
   title = {ChatLLM Network: More brains, More intelligence},
   pages = {arXiv:2304.12998},
   month = {April 01, 2023},
   abstract = {Dialogue-based language models mark a huge milestone in the field of artificial intelligence, by their impressive ability to interact with users, as well as a series of challenging tasks prompted by customized instructions. However, the prevalent large-scale dialogue-based language models like ChatGPT still have room for improvement, such as unstable responses to questions and the inability to think cooperatively like humans. Considering the ability of dialogue-based language models in conversation and their inherent randomness in thinking, we propose ChatLLM network that allows multiple dialogue-based language models to interact, provide feedback, and think together. We design the network of ChatLLMs based on ChatGPT. Specifically, individual instances of ChatGPT may possess distinct perspectives towards the same problem, and by consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM network system can conduct decision-making more objectively and comprehensively. In addition, a language-based feedback mechanism comparable to backpropagation is devised to update the ChatGPTs within the network. Experiments on two datasets demonstrate that our network attains significant improvements in problem-solving, leading to observable progress amongst each member.},
   keywords = {Computer Science - Artificial Intelligence
Computer Science -
Computation and Language},
   year = {2023},
   type = {Electronic Article}
}

@INPROCEEDINGS{human-team-building,
  author={Liu, Qing and Luo, Tie and Tang, Ruiming and Bressan, Stéphane},
  booktitle={2015 IEEE International Conference on Communications (ICC)}, 
  title={An efficient and truthful pricing mechanism for team formation in crowdsourcing markets}, 
  year={2015},
  volume={},
  number={},
  pages={567-572}}

@article{zhang2023exploring,
  title={Exploring collaboration mechanisms for llm agents: A social psychology view},
  author={Zhang, Jintian and Xu, Xin and Deng, Shumin},
  journal={arXiv preprint arXiv:2310.02124},
  year={2023}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib"). 

-----Surveys-----
@article{arXiv2023_Survey-LLM,
  author       = {Wayne Xin Zhao and
                  Kun Zhou and
                  Junyi Li and
                  Tianyi Tang and
                  Xiaolei Wang and
                  Yupeng Hou and
                  Yingqian Min and
                  Beichen Zhang and
                  Junjie Zhang and
                  Zican Dong and
                  Yifan Du and
                  Chen Yang and
                  Yushuo Chen and
                  Zhipeng Chen and
                  Jinhao Jiang and
                  Ruiyang Ren and
                  Yifan Li and
                  Xinyu Tang and
                  Zikang Liu and
                  Peiyu Liu and
                  Jian{-}Yun Nie and
                  Ji{-}Rong Wen},
  title        = {A Survey of Large Language Models},
  journal      = {arXiv preprint},
  volume       = {abs/2303.18223},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.18223},
  doi          = {10.48550/arXiv.2303.18223}
}

@article{arXiv2023_Survey-MLLM,
  author       = {Shukang Yin and
                  Chaoyou Fu and
                  Sirui Zhao and
                  Ke Li and
                  Xing Sun and
                  Tong Xu and
                  Enhong Chen},
  title        = {A Survey on Multimodal Large Language Models},
  journal      = {arXiv preprint},
  volume       = {abs/2306.13549},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.13549},
  doi          = {10.48550/arXiv.2306.13549}
}

@article{arXiv2023_Survey-LLM-KGCR,
  author       = {Yuqi Zhu and
                  Xiaohan Wang and
                  Jing Chen and
                  Shuofei Qiao and
                  Yixin Ou and
                  Yunzhi Yao and
                  Shumin Deng and
                  Huajun Chen and
                  Ningyu Zhang},
  title        = {LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities},
  journal      = {CoRR},
  volume       = {abs/2305.13168},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.13168},
  doi          = {10.48550/arXiv.2305.13168}
}

@article{arXiv2024_Survey-LLM-Psychology-Applications,
  author       = {Luoma Ke and 
                  Song Tong and 
                  Peng Chen and 
                  Kaiping Peng},
  title        ={Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review}, 
  journal      = {CoRR},
  volume       = {abs/2401.01519},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.01519}
}

@article{FCS2024_Survey-Agent,
  author       = {Lei Wang and
                  Chen Ma and
                  Xueyang Feng and
                  Zeyu Zhang and
                  Hao Yang and
                  Jingsen Zhang and
                  Zhiyuan Chen and
                  Jiakai Tang and
                  Xu Chen and
                  Yankai Lin and
                  Wayne Xin Zhao and
                  Zhewei Wei and
                  Ji{-}Rong Wen},
  title        = {A Survey on Large Language Model based Autonomous Agents},
  journal      = {Front. Comput. Sci.},
  volume       = {18},
  year         = {2024}
}
arXiv2023_Survey-Agent

@article{arXiv2023_Survey-Agent_2,
  author       = {Zhiheng Xi and
                  Wenxiang Chen and
                  Xin Guo and
                  Wei He and
                  Yiwen Ding and
                  Boyang Hong and
                  Ming Zhang and
                  Junzhe Wang and
                  Senjie Jin and
                  Enyu Zhou and
                  Rui Zheng and
                  Xiaoran Fan and
                  Xiao Wang and
                  Limao Xiong and
                  Yuhao Zhou and
                  Weiran Wang and
                  Changhao Jiang and
                  Yicheng Zou and
                  Xiangyang Liu and
                  Zhangyue Yin and
                  Shihan Dou and
                  Rongxiang Weng and
                  Wensen Cheng and
                  Qi Zhang and
                  Wenjuan Qin and
                  Yongyan Zheng and
                  Xipeng Qiu and
                  Xuanjing Huan and
                  Tao Gui},
  title        = {The Rise and Potential of Large Language Model Based Agents: {A} Survey},
  journal      = {arxiv preprint},
  volume       = {abs/2309.07864},
  year         = {2023}
}

@article{arXiv2023_Survey-Agent_3,
  author       = {Chen Gao and 
                  Xiaochong Lan and 
                  Nian Li and 
                  Yuan Yuan and 
                  Jingtao Ding and 
                  Zhilun Zhou and 
                  Fengli Xu and 
                  Yong Li},
  title        = {Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives},
  journal      = {CoRR},
  volume       = {abs/2312.11970},
  year         = {2023}
}

@article{arXiv2024_Survey-Agent_4,
  author       = {Yuheng Cheng and 
                  Ceyao Zhang and 
                  Zhengwen Zhang and 
                  Xiangrui Meng and 
                  Sirui Hong and 
                  Wenhao Li and 
                  Zihao Wang and 
                  Zekai Wang and 
                  Feng Yin and 
                  Junhua Zhao and 
                  Xiuqiang He},
  title        = {Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects},
  journal      = {CoRR},
  volume       = {abs/2401.03428},
  year         = {2024}
}

@article{arXiv2024_Survey-Agents-CompExp,
  author       = {Qun Ma and 
                  Xiao Xue and 
                  Deyu Zhou and 
                  Xiangning Yu and 
                  Donghua Liu and 
                  Xuwen Zhang and 
                  Zihan Zhao and 
                  Yifan Shen and 
                  Peilin Ji and 
                  Juanjuan Li and 
                  Gang Wang and 
                  Wanpeng Ma},
  title        = {Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective},
  journal      = {CoRR},
  volume       = {abs/2402.00262},
  year         = {2024}
}

@article{arXiv2024_Survey-MultiAgent,
  author       = {Taicheng Guo and 
                  Xiuying Chen and 
                  Yaqi Wang and 
                  Ruidi Chang and 
                  Shichao Pei and 
                  Nitesh V. Chawla and 
                  Olaf Wiest and 
                  Xiangliang Zhang},
  title        = {Large Language Model based Multi-Agents: A Survey of Progress and Challenges},
  journal      = {CoRR},
  volume       = {abs/2402.01680},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.01680}
}

@article{arXiv2024_Survey-MultiAgent_2,
  author       = {Pouya Pezeshkpour and 
                  Eser Kandogan and 
                  Nikita Bhutani and 
                  Sajjadur Rahman and 
                  Tom Mitchell and 
                  Estevam Hruschka},
  title        = {Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions},
  journal      = {CoRR},
  volume       = {abs/2402.01108},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.01108}
}

@article{arXiv2024_Survey-MultiAgent-System,
  author       = {Hung Du and 
                  Srikanth Thudumu and 
                  Rajesh Vasa and 
                  Kon Mouzakis},
  title        = {A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions},
  journal      = {CoRR},
  volume       = {abs/2402.01968},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.01968}
}

@article{arXiv2024_Survey-MultiAgent-System_2,
  author       = {Shanshan Han and 
                  Qifan Zhang and 
                  Yuhang Yao and 
                  Weizhao Jin and 
                  Zhaozhuo Xu and 
                  Chaoyang He},
  title        = {LLM Multi-Agent Systems: Challenges and Open Problems},
  journal      = {CoRR},
  volume       = {abs/2402.03578},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.03578}
}

@article{J2024_Survey-AI-SocialScience,
  author       = {Ruoxi Xu and 
                  Yingfei Sun and 
                  Mengjie Ren and 
                  Shiguang Guo and 
                  Ruotong Pan and 
                  Hongyu Lin and 
                  Le Sun and 
                  Xianpei Han},
  title        = {AI for social science and social science of AI: A Survey},
  journal      = {Information Processing \& Management},
  volume       = {61},
  number       = {3},
  pages        = {103665},
  year         = {2024},
  issn         = {0306-4573},
  url          = {https://www.sciencedirect.com/science/article/pii/S0306457324000256},
  doi          = {https://doi.org/10.1016/j.ipm.2024.103665}
}
arXiv2024_Survey-AI-SocialScience

@article{arXiv2023_Survey-MultiAgentCooperation,
  author       = {Yali Du and
                  Joel Z. Leibo and
                  Usman Islam and
                  Richard Willis and
                  Peter Sunehag},
  title        = {A Review of Cooperation in Multi-agent Learning},
  journal      = {CoRR},
  volume       = {abs/2312.05162},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.05162},
  doi          = {10.48550/ARXIV.2312.05162}
}

@article{arXiv2024_Survey-AgentAI_MMInteraction,
  author       = {Zane Durante and 
                  Qiuyuan Huang and 
                  Naoki Wake and 
                  Ran Gong and 
                  Jae Sung Park and 
                  Bidipta Sarkar and 
                  Rohan Taori and 
                  Yusuke Noda and 
                  Demetri Terzopoulos and 
                  Yejin Choi and 
                  Katsushi Ikeuchi and 
                  Hoi Vo and 
                  Li Fei-Fei and 
                  Jianfeng Gao},
  title        = {Agent AI: Surveying the Horizons of Multimodal Interaction},
  journal      = {CoRR},
  volume       = {abs/2401.03568},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.03568}
}

@article{arXiv2024_Survey-CooperativeAgent-RL,
  author       = {Jiechuan Jiang and 
                  Kefan Su and 
                  Zongqing Lu},
  title        = {Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey},
  journal      = {CoRR},
  volume       = {abs/2401.04934},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.04934}
}

@inproceedings{ICLR2024_Sycophancy-LLM,
  author       = {Mrinank Sharma and
                  Meg Tong and
                  Tomasz Korbak and
                  David Duvenaud and
                  Amanda Askell and
                  Samuel R. Bowman and
                  Newton Cheng and
                  Esin Durmus and
                  Zac Hatfield{-}Dodds and
                  Scott R. Johnston and
                  Shauna Kravec and
                  Timothy Maxwell and
                  Sam McCandlish and
                  Kamal Ndousse and
                  Oliver Rausch and
                  Nicholas Schiefer and
                  Da Yan and
                  Miranda Zhang and
                  Ethan Perez},
  title        = {Towards Understanding Sycophancy in Language Models},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=tvhaxkMKAn}
}
arXiv2023_Sycophancy-LLM

@article{J2008_Survey-MultiAgent-Reinforce,
  author       = {Lucian Busoniu and
                  Robert Babuska and
                  Bart De Schutter},
  title        = {A Comprehensive Survey of Multiagent Reinforcement Learning},
  journal      = {{IEEE} Trans. Syst. Man Cybern. Part {C}},
  volume       = {38},
  number       = {2},
  pages        = {156--172},
  year         = {2008},
  url          = {https://doi.org/10.1109/TSMCC.2007.913919},
  doi          = {10.1109/TSMCC.2007.913919}
}

@article{arXiv2023_Survey-MultiAgent-Reinforce,
  author       = {Dom Huh and 
                  Prasant Mohapatra},
  title        = {Multi-agent Reinforcement Learning: A Comprehensive Survey}, 
  journal      = {arxiv preprint},
  volume       = {abs/2312.10256},
  year         = {2023},
  url          = {https://arxiv.org/abs/2312.10256}
}

@article{arXiv2023_Survey-Hallucination_LFM,
  author       = {Vipula Rawte and
                  Amit P. Sheth and
                  Amitava Das},
  title        = {A Survey of Hallucination in Large Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2309.05922},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.05922},
  doi          = {10.48550/arXiv.2309.05922}
}

@article{J2023_Survey-Hallucination_NLG,
  author       = {Ziwei Ji and
                  Nayeon Lee and
                  Rita Frieske and
                  Tiezheng Yu and
                  Dan Su and
                  Yan Xu and
                  Etsuko Ishii and
                  Yejin Bang and
                  Andrea Madotto and
                  Pascale Fung},
  title        = {Survey of Hallucination in Natural Language Generation},
  journal      = {{ACM} Comput. Surv.},
  volume       = {55},
  number       = {12},
  pages        = {248:1--248:38},
  year         = {2023},
  url          = {https://doi.org/10.1145/3571730},
  doi          = {10.1145/3571730}
}

@article{arXiv2023_Survey-Hallucination_LLM,
  author       = {Yue Zhang and
                  Yafu Li and
                  Leyang Cui and
                  Deng Cai and
                  Lemao Liu and
                  Tingchen Fu and
                  Xinting Huang and
                  Enbo Zhao and
                  Yu Zhang and
                  Yulong Chen and
                  Longyue Wang and
                  Anh Tuan Luu and
                  Wei Bi and
                  Freda Shi and
                  Shuming Shi},
  title        = {Siren's Song in the {AI} Ocean: {A} Survey on Hallucination in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.01219},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.01219},
  doi          = {10.48550/ARXIV.2309.01219}
}

@article{arXiv2024_Survey-Hallucination,
  author       = {Junliang Luo and 
                  Tianyu Li and 
                  Di Wu and 
                  Michael Jenkin and 
                  Steve Liu and 
                  Gregory Dudek},
  title        = {Hallucination Detection and Hallucination Mitigation: An Investigation},
  journal      = {CoRR},
  volume       = {abs/2401.08358},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.08358}
}

@article{arXiv2024_Analysis-Hallucination,
  author       = {Ziwei Xu and 
                  Sanjay Jain and 
                  Mohan Kankanhalli},
  title        = {Hallucination is Inevitable: An Innate Limitation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2401.11817},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.11817}
}


-----Multi-Agent-----
@inproceedings{1995_MultiAgent-System,
  author       = {Gerhard Wei{\ss}},
  title        = {Adaptation and Learning in Multi-Agent Systems: Some Remarks and a Bibliography},
  booktitle    = {Adaption and Learning in Multi-Agent Systems},
  series       = {Lecture Notes in Computer Science},
  volume       = {1042},
  pages        = {1--21},
  publisher    = {Springer},
  year         = {1995},
  url          = {https://doi.org/10.1007/3-540-60923-7\_16},
  doi          = {10.1007/3-540-60923-7\_16}
}

@article{J2000_MultiAgent-System,
  author       = {Peter Stone and
                  Manuela M. Veloso},
  title        = {Multiagent Systems: {A} Survey from a Machine Learning Perspective},
  journal      = {Auton. Robots},
  volume       = {8},
  number       = {3},
  pages        = {345--383},
  publisher    = {Springer},
  year         = {2000},
  url          = {https://doi.org/10.1023/A:1008942012299},
  doi          = {10.1023/A:1008942012299}
}

@book{Book2006_MultiAgent-System,
  author       = {Jos\'{e} M. Vidal},
  title        = {Fundamentals of Multiagent Systems: Using NetLogo Models},
  publisher    = {Unpublished},
  year         = {2006},
  url          = {http://www.multiagent.com/fmas},
  note         = {\url{http://www.multiagent.com}},
  cluster      = {10994802165800486817}
}

@book{Book2009_MultiAgent-System,
  author       = {Michael J. Wooldridge},
  title        = {An Introduction to MultiAgent Systems, Second Edition},
  publisher    = {Wiley},
  url          = {https://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/imas/IMAS2e.html},
  year         = {2009},
  isbn         = {978-0-470-51946-2}
}

@article{arXiv2024_Formal-LLM,
  author       = {Zelong Li and 
                  Wenyue Hua and 
                  Hao Wang and 
                  He Zhu and 
                  Yongfeng Zhang},
  title        = {Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents},
  journal      = {CoRR},
  volume       = {abs/2402.00798},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.00798}
}

@article{arXiv2023_Agents,
  author       = {Wangchunshu Zhou and
                  Yuchen Eleanor Jiang and
                  Long Li and
                  Jialong Wu and
                  Tiannan Wang and
                  Shi Qiu and
                  Jintian Zhang and
                  Jing Chen and
                  Ruipu Wu and
                  Shuai Wang and
                  Shiding Zhu and
                  Jiyu Chen and
                  Wentao Zhang and
                  Ningyu Zhang and
                  Huajun Chen and
                  Peng Cui and
                  Mrinmaya Sachan},
  title        = {Agents: An Open-source Framework for Autonomous Language Agents},
  journal      = {CoRR},
  volume       = {abs/2309.07870},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.07870},
  doi          = {10.48550/arXiv.2309.07870}
}

@article{arXiv2023_OpenAgents,
  author       = {Tianbao Xie and
                  Fan Zhou and
                  Zhoujun Cheng and
                  Peng Shi and
                  Luoxuan Weng and
                  Yitao Liu and
                  Toh Jing Hua and
                  Junning Zhao and
                  Qian Liu and
                  Che Liu and
                  Leo Z. Liu and
                  Yiheng Xu and
                  Hongjin Su and
                  Dongchan Shin and
                  Caiming Xiong and
                  Tao Yu},
  title        = {OpenAgents: An Open Platform for Language Agents in the Wild},
  journal      = {CoRR},
  volume       = {abs/2310.10634},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.10634},
  doi          = {10.48550/ARXIV.2310.10634}
}

@article{arXiv2023_AutoAgents,
  author       = {Guangyao Chen and
                  Siwei Dong and
                  Yu Shu and
                  Ge Zhang and
                  Jaward Sesay and
                  B{\"{o}}rje F. Karlsson and
                  Jie Fu and
                  Yemin Shi},
  title        = {AutoAgents: {A} Framework for Automatic Agent Generation},
  journal      = {CoRR},
  volume       = {abs/2309.17288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.17288},
  doi          = {10.48550/ARXIV.2309.17288}
}

@article{arXiv2023_CGMI,
  author       = {Jinxin Shi and
                  Jiabao Zhao and
                  Yilei Wang and
                  Xingjiao Wu and
                  Jiawen Li and
                  Liang He},
  title        = {{CGMI:} Configurable General Multi-Agent Interaction Framework},
  journal      = {CoRR},
  volume       = {abs/2308.12503},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.12503},
  doi          = {10.48550/ARXIV.2308.12503}
}

@article{arXiv2023_Believability-Agents,
  author       = {Yang Xiao and 
                  Yi Cheng and 
                  Jinlan Fu and 
                  Jiashuo Wang and 
                  Wenjie Li and 
                  Pengfei Liu},
  title        = {How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation},
  journal      = {CoRR},
  volume       = {abs/2312.17115},
  year         = {2023},
  url          = {https://arxiv.org/abs/2312.17115}
}

@article{arXiv2023_MAgIC,
  author       = {Lin Xu and
                  Zhiyuan Hu and
                  Daquan Zhou and
                  Hongyu Ren and
                  Zhen Dong and
                  Kurt Keutzer and
                  See{-}Kiong Ng and
                  Jiashi Feng},
  title        = {MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration},
  journal      = {CoRR},
  volume       = {abs/2311.08562},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.08562},
  doi          = {10.48550/ARXIV.2311.08562}
}

@article{arXiv2024_AgentBoard,
  author       = {Chang Ma and 
                  Junlei Zhang and 
                  Zhihao Zhu and 
                  Cheng Yang and 
                  Yujiu Yang and 
                  Yaohui Jin and 
                  Zhenzhong Lan and 
                  Lingpeng Kong and 
                  Junxian He},
  title        = {AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents},
  journal      = {CoRR},
  volume       = {abs/2401.13178},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.13178}
}

@article{arXiv2024_Evaluate-Agents,
  author       = {Lize Alberts and 
                  Geoff Keeling and 
                  Amanda McCroskery},
  title        = {What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents},
  journal      = {CoRR},
  volume       = {abs/2401.09082},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.09082}
}

---Multi-Agent Collaboration---
@article{arXiv2023_InterAct,
  author       = {Po{-}Lin Chen and
                  Cheng{-}Shang Chang},
  title        = {InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent},
  journal      = {CoRR},
  volume       = {abs/2308.01552},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.01552},
  doi          = {10.48550/ARXIV.2308.01552}
}

@article{arXiv2023_Multi-Agent-Collaboration_Intelligent,
  author       = {Yashar Talebirad and
                  Amirhossein Nadiri},
  title        = {Multi-Agent Collaboration: Harnessing the Power of Intelligent {LLM} Agents},
  journal      = {CoRR},
  volume       = {abs/2306.03314},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.03314},
  doi          = {10.48550/ARXIV.2306.03314}
}

@inproceedings{UIST2023_Agent-Simulate-Interaction,
  author       = {Joon Sung Park and
                  Joseph C. O'Brien and
                  Carrie Jun Cai and
                  Meredith Ringel Morris and
                  Percy Liang and
                  Michael S. Bernstein},
  title        = {Generative Agents: Interactive Simulacra of Human Behavior},
  booktitle    = {{UIST}},
  pages        = {2:1--2:22},
  publisher    = {{ACM}},
  year         = {2023}
}


@inproceedings{AAAI2024_CooperativeAgents_ProAgent,
  author       = {Ceyao Zhang and
                  Kaijie Yang and
                  Siyi Hu and
                  Zihao Wang and
                  Guanghe Li and
                  Yihang Sun and
                  Cheng Zhang and
                  Zhaowei Zhang and
                  Anji Liu and
                  Song{-}Chun Zhu and
                  Xiaojun Chang and
                  Junge Zhang and
                  Feng Yin and
                  Yitao Liang and
                  Yaodong Yang},
  title        = {ProAgent: Building Proactive Cooperative Agents with Large Language
                  Models},
  booktitle    = {{AAAI}},
  pages        = {17591--17599},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i16.29710},
  doi          = {10.1609/AAAI.V38I16.29710}
}

@article{sun2023corex,
  title={Corex: Pushing the boundaries of complex reasoning through multi-model collaboration},
  author={Sun, Qiushi and Yin, Zhangyue and Li, Xiang and Wu, Zhiyong and Qiu, Xipeng and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2310.00280},
  year={2023}
}

@article{chen2024comm,
  title={CoMM: Collaborative multi-agent, multi-reasoning-path prompting for complex problem solving},
  author={Chen, Pei and Han, Boran and Zhang, Shuai},
  journal={arXiv preprint arXiv:2404.17729},
  year={2024}
}

@inproceedings{ICLR2024_MultiAgent_AgentVerse,
  author       = {Weize Chen and
                  Yusheng Su and
                  Jingwei Zuo and
                  Cheng Yang and
                  Chenfei Yuan and
                  Chen Qian and
                  Chi{-}Min Chan and
                  Yujia Qin and
                  Yaxi Lu and
                  Ruobing Xie and
                  Zhiyuan Liu and
                  Maosong Sun and
                  Jie Zhou},
  title        = {AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=EHg5GDnyq1}
}

@article{arXiv2023_Dynamic-LLM-Agent,
  author       = {Zijun Liu and
                  Yanzhe Zhang and
                  Peng Li and
                  Yang Liu and
                  Diyi Yang},
  title        = {Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization},
  journal      = {CoRR},
  volume       = {abs/2310.02170},
  year         = {2023}
}

@inproceedings{EMNLP2023-Demo_CollaborativeLLMs,
  author       = {Kai Lv and
                  Shuo Zhang and
                  Tianle Gu and
                  Shuhao Xing and
                  Jiawei Hong and
                  Keyu Chen and
                  Xiaoran Liu and
                  Yuqing Yang and
                  Honglin Guo and
                  Tengxiao Liu and
                  Yu Sun and
                  Qipeng Guo and
                  Hang Yan and
                  Xipeng Qiu},
  title        = {CoLLiE: Collaborative Training of Large Language Models in an Efficient Way},
  booktitle    = {{EMNLP} (Demos)},
  pages        = {527--542},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-demo.48}
}

@article{J2024_MechAgents-MultiAgentCollaborations,
  author       = {Bo Ni and
                  Markus J. Buehler},
  title        = {MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge},
  journal      = {Extreme Mechanics Letters},
  volume     = {67},
  pages      = {102131},
  year       = {2024},
  issn       = {2352-4316},
  url        = {https://www.sciencedirect.com/science/article/pii/S2352431624000117},
  doi        = {https://doi.org/10.1016/j.eml.2024.102131}
}
arXiv2023_MechAgents-MultiAgentCollaborations

@article{arXiv2023_MultiAgent-Coordination-Eval,
  author       = {Saaket Agashe and
                  Yue Fan and
                  Xin Eric Wang},
  title        = {Evaluating Multi-Agent Coordination Abilities in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2310.03903},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.03903},
  doi          = {10.48550/ARXIV.2310.03903}
}

@inproceedings{ICLR2024_Agent-Interactive-Eval,
  author       = {Xuhui Zhou and
                  Hao Zhu and
                  Leena Mathur and
                  Ruohong Zhang and
                  Haofei Yu and
                  Zhengyang Qi and
                  Louis{-}Philippe Morency and
                  Yonatan Bisk and
                  Daniel Fried and
                  Graham Neubig and
                  Maarten Sap},
  title        = {{SOTOPIA:} Interactive Evaluation for Social Intelligence in Language Agents},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=mM7VurbA4r}
}
arXiv2023_Agent-Interactive-Eval

@inproceedings{AAMAS2024_AgentInteraction-Quantifying,
  author       = {Yuxin Chen and
                  Chen Tang and
                  Ran Tian and
                  Chenran Li and
                  Jinning Li and
                  Masayoshi Tomizuka and
                  Wei Zhan},
  title        = {Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization},
  booktitle    = {{AAMAS}},
  pages        = {2201--2203},
  publisher    = {{ACM}},
  year         = {2024},
  url          = {https://dl.acm.org/doi/10.5555/3635637.3663107},
  doi          = {10.5555/3635637.3663107}
}
arXiv2023_AgentInteraction-Quantifying

@article{arXiv2023_LLM-Deliberation,
  author       = {Sahar Abdelnabi and
                  Amr Gomaa and
                  Sarath Sivaprasad and
                  Lea Sch{\"{o}}nherr and
                  Mario Fritz},
  title        = {LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games},
  journal      = {CoRR},
  volume       = {abs/2309.17234},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.17234},
  doi          = {10.48550/ARXIV.2309.17234}
}

@article{arXiv2023_MultiAgent-Cooperation,
  author       = {Rafael Pina and
                  Varuna De Silva and
                  Corentin Artaud},
  title        = {Discovering Causality for Efficient Cooperation in Multi-Agent Environments},
  journal      = {CoRR},
  volume       = {abs/2306.11846},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.11846},
  doi          = {10.48550/ARXIV.2306.11846}
}

@article{arXiv2023_MultiAgent-Algorithms,
  author       = {Lin Yang and
                  Xuchuang Wang and
                  Mohammad Hajiesmaili and
                  Lijun Zhang and
                  John C. S. Lui and
                  Don Towsley},
  title        = {Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs},
  journal      = {CoRR},
  volume       = {abs/2308.04314},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.04314},
  doi          = {10.48550/ARXIV.2308.04314}
}


-----Debate and Reflection-----
@book{Book1971_Rhetoric,
  author       = {Perelman, Chaim},
  title        = {The new rhetoric},
  publisher    = {Springer},
  year         = {1971},
  url          = {https://link.springer.com/chapter/10.1007/978-94-010-1713-8_8}
}

@book{Book2005_Society-Dissent,
  author       = {Cass R Sunstein},
  title        = {Why societies need dissent},
  publisher    = {Harvard University Press},
  year         = {2005},
  url          = {https://doi.org/10.4159/9780674267657}
}

@article{J2009_DecisionMaking,
  author       = {Leila Amgoud and
                  Henri Prade},
  title        = {Using arguments for making and explaining decisions},
  journal      = {Artif. Intell.},
  volume       = {173},
  number       = {3-4},
  pages        = {413--436},
  publisher    = {Elsevier},
  year         = {2009},
  url          = {https://doi.org/10.1016/j.artint.2008.11.006},
  doi          = {10.1016/j.artint.2008.11.006} 
}

@article{arXiv2023_MultiAgent-Debate,
  author       = {Yilun Du and
                  Shuang Li and
                  Antonio Torralba and
                  Joshua B. Tenenbaum and
                  Igor Mordatch},
  title        = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  journal      = {CoRR},
  volume       = {abs/2305.14325},
  year         = {2023}
}

@article{yan2024depending,
  title={Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games},
  author={Yan, Yikuan and Zhang, Yaolun and Huang, Keman},
  journal={arXiv preprint arXiv:2403.17674},
  year={2024}
}

@article{saad2024archon,
  title={Archon: An architecture search framework for inference-time techniques},
  author={Saad-Falcon, Jon and Lafuente, Adrian Gamarra and Natarajan, Shlok and Maru, Nahum and Todorov, Hristo and Guha, Etash and Buchanan, E Kelly and Chen, Mayee and Guha, Neel and R{\'e}, Christopher and others},
  journal={arXiv preprint arXiv:2409.15254},
  year={2024}
}

@article{zhang2022automaticcot,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}


@inproceedings{holt2024l2mac,
  title={L2MAC: Large Language Model Automatic Computer for Extensive Code Generation},
  author={Holt, Samuel and Luyten, Max Ruiz and van der Schaar, Mihaela},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{zhou2023large,
  title={Large language model as a policy teacher for training reinforcement learning agents},
  author={Zhou, Zihao and Hu, Bin and Zhao, Chenyang and Zhang, Pu and Liu, Bin},
  journal={arXiv preprint arXiv:2311.13373},
  year={2023}
}

@article{arXiv2023_MultiAgent-Debate_2,
  author       = {Tian Liang and
                  Zhiwei He and
                  Wenxiang Jiao and
                  Xing Wang and
                  Yan Wang and
                  Rui Wang and
                  Yujiu Yang and
                  Zhaopeng Tu and
                  Shuming Shi},
  title        = {Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
  journal      = {CoRR},
  volume       = {abs/2305.19118},
  year         = {2023}
}

@inproceedings{ICLR2024_Multiagent-Debate-Embeddings,
  author       = {Chau Pham and
                  Boyi Liu and
                  Yingxiang Yang and
                  Zhengyu Chen and
                  Tianyi Liu and
                  Jianbo Yuan and
                  Bryan A. Plummer and
                  Zhaoran Wang and
                  Hongxia Yang},
  title        = {Let Models Speak Ciphers: Multiagent Debate through Embeddings},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=sehRvaIPQQ}
}
arXiv2023_Multiagent-Debate-Embeddings

@inproceedings{ICLR2024_Multiagent-Debate-Eval,
  author       = {Chi{-}Min Chan and
                  Weize Chen and
                  Yusheng Su and
                  Jianxuan Yu and
                  Wei Xue and
                  Shanghang Zhang and
                  Jie Fu and
                  Zhiyuan Liu},
  title        = {ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=FQepisCUWu}
}
arXiv2023_Multiagent-Debate-Eval

@article{J1985_Reflection,
  author       = {R. J. Bogumil},
  title        = {The reflective practitioner: How professionals think in action},
  journal      = {Proc. {IEEE}},
  volume       = {73},
  number       = {4},
  pages        = {845--846},
  year         = {1985},
  url          = {https://doi.org/10.1109/PROC.1985.13210},
  doi          = {10.1109/PROC.1985.13210}
}

@book{Book2010_Reflection,
  author       = {Bolton, Gillie},
  title        = {Reflective practice: Writing and professional development},
  publisher    = {Sage publications},
  year         = {2010},
  url          = {https://uk.sagepub.com/en-gb/eur/reflective-practice/book252252}
}

@article{zelikman2023parsel,
  title={Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions},
  author={Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah and Haber, Nick},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={31466--31523},
  year={2023}
}

@article{huang2024anpl,
  title={ANPL: towards natural programming with interactive decomposition},
  author={Huang, Di and Nan, Ziyuan and Hu, Xing and Jin, Pengwei and Peng, Shaohui and Wen, Yuanbo and Zhang, Rui and Du, Zidong and Guo, Qi and Pu, Yewen and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% reflection-1 3 prompt
@article{reflexion,
  author       = {Noah Shinn and
                  Beck Labash and
                  Ashwin Gopinath},
  title        = {Reflexion: an autonomous agent with dynamic memory and self-reflection},
  journal      = {arXiv preprint},
  volume       = {abs/2303.11366},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.11366},
  doi          = {10.48550/arXiv.2303.11366}
}

% reflection-2 reinforcement
arXiv2023_Self-Refine
@inproceedings{NeurIPS2023_Self-Refine,
  author       = {Aman Madaan and
                  Niket Tandon and
                  Prakhar Gupta and
                  Skyler Hallinan and
                  Luyu Gao and
                  Sarah Wiegreffe and
                  Uri Alon and
                  Nouha Dziri and
                  Shrimai Prabhumoye and
                  Yiming Yang and
                  Shashank Gupta and
                  Bodhisattwa Prasad Majumder and
                  Katherine Hermann and
                  Sean Welleck and
                  Amir Yazdanbakhsh and
                  Peter Clark},
  title        = {Self-Refine: Iterative Refinement with Self-Feedback},
  booktitle    = {NeurIPS},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html}
}

@article{J2003_Reflection-ContinuingEducation,
  author       = {Mezirow, Jack},
  title        = {How critical reflection triggers transformative learning},
  journal      = {Adult and Continuing Education: Teaching, learning and research},
  volume       = {4},
  pages        = {199},
  publisher    = {Taylor \& Francis},
  year         = {2003},
  url          = {https://www.colorado.edu/plc/sites/default/files/attached-files/how_critical_reflection_triggers_transfo.pdf}
}

@article{arXiv2024_Self-Contrast,
  author       = {Wenqi Zhang and 
                  Yongliang Shen and 
                  Linjuan Wu and 
                  Qiuying Peng and 
                  Jun Wang and 
                  Yueting Zhuang and 
                  Weiming Lu},
  title        = {Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives}, 
  journal      = {CoRR},
  volume       = {abs/2401.02009},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.02009}
}


-----Interactive NLP-----
@article{arXiv2023_InteractiveNLP,
  author       = {Zekun Wang and
                  Ge Zhang and
                  Kexin Yang and
                  Ning Shi and
                  Wangchunshu Zhou and
                  Shaochun Hao and
                  Guangzheng Xiong and
                  Yizhi Li and
                  Mong Yuan Sim and
                  Xiuying Chen and
                  Qingqing Zhu and
                  Zhenzhu Yang and
                  Adam Nik and
                  Qi Liu and
                  Chenghua Lin and
                  Shi Wang and
                  Ruibo Liu and
                  Wenhu Chen and
                  Ke Xu and
                  Dayiheng Liu and
                  Yike Guo and
                  Jie Fu},
  title        = {Interactive Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/2305.13246},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.13246},
  doi          = {10.48550/arXiv.2305.13246}
}

@inproceedings{J2023_InteractiveNLP,
  author       = {Guanhua Zhang and
                  Matteo Bortoletto and
                  Zhiming Hu and
                  Lei Shi and
                  Mihai B{\^{a}}ce and
                  Andreas Bulling},
  title        = {Exploring Natural Language Processing Methods for Interactive Behaviour Modelling},
  booktitle    = {{INTERACT} {(3)}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14144},
  pages        = {3--26},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-42286-7\_1},
  doi          = {10.1007/978-3-031-42286-7\_1}
}


-----Human-AI Simulation-----
@article{NMI2023_Human-Like-AI,
  author       = {Edgar A. Du{\'{e}}{\~{n}}ez{-}Guzm{\'{a}}n and
                  Suzanne Sadedin and
                  Jane X. Wang and
                  Kevin R. McKee and
                  Joel Z. Leibo},
  title        = {A social path to human-like artificial intelligence},
  journal      = {Nat. Mac. Intell.},
  volume       = {5},
  number       = {11},
  pages        = {1181--1188},
  year         = {2023},
  url          = {https://doi.org/10.1038/s42256-023-00754-x},
  doi          = {10.1038/S42256-023-00754-X}
}

@inproceedings{ICLR2024_LLM-Simulate-Society,
  author       = {Ruibo Liu and
                  Ruixin Yang and
                  Chenyan Jia and
                  Ge Zhang and
                  Denny Zhou and
                  Andrew M. Dai and
                  Diyi Yang and
                  Soroush Vosoughi},
  title        = {Training Socially Aligned Language Models in Simulated Human Society},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=NddKiWtdUm}
}
arXiv2023_LLM-Simulate-Society

@article{arXiv2023_LLMAgents-Simulate-Society_S3,
  author       = {Chen Gao and
                  Xiaochong Lan and
                  Zhihong Lu and
                  Jinzhu Mao and
                  Jinghua Piao and
                  Huandong Wang and
                  Depeng Jin and
                  Yong Li},
  title        = {S\({}^{\mbox{3}}\): Social-network Simulation System with Large Language Model-Empowered Agents},
  journal      = {CoRR},
  volume       = {abs/2307.14984},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.14984},
  doi          = {10.48550/ARXIV.2307.14984}
}

@inproceedings{UIST2022_SocialSimulacra,
  author       = {Joon Sung Park and
                  Lindsay Popowski and
                  Carrie J. Cai and
                  Meredith Ringel Morris and
                  Percy Liang and
                  Michael S. Bernstein},
  title        = {Social Simulacra: Creating Populated Prototypes for Social Computing Systems},
  booktitle    = {{UIST}},
  pages        = {74:1--74:18},
  publisher    = {{ACM}},
  year         = {2022},
  url          = {https://doi.org/10.1145/3526113.3545616},
  doi          = {10.1145/3526113.3545616}
}

@article{arXiv2024_AgentAlignment-SocialNorms,
  author       = {Shimin Li and 
                  Tianxiang Sun and 
                  Xipeng Qiu},
  title        = {Agent Alignment in Evolving Social Norms},
  journal      = {CoRR},
  volume       = {abs/2401.04620},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.04620}
}

@inproceedings{NAACL2022-Findings_Align-GLMs,
  author       = {Ruibo Liu and
                  Ge Zhang and
                  Xinyu Feng and
                  Soroush Vosoughi},
  title        = {Aligning Generative Language Models with Human Values},
  booktitle    = {{NAACL-HLT} (Findings)},
  pages        = {241--252},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-naacl.18},
  doi          = {10.18653/V1/2022.FINDINGS-NAACL.18}
}

@inproceedings{NeurIPS2022_Re-Align,
  author       = {Ruibo Liu and
                  Chenyan Jia and
                  Ge Zhang and
                  Ziyu Zhuang and
                  Tony X. Liu and
                  Soroush Vosoughi},
  title        = {Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/01c4593d60a020fed5607944330106b1-Abstract-Conference.html}
}

@article{arXiv2023_Align-Chatbot,
  author       = {Chunpu Xu and 
                  Steffi Chern and 
                  Ethan Chern and 
                  Ge Zhang and 
                  Zekun Wang and 
                  Ruibo Liu and 
                  Jing Li and 
                  Jie Fu and 
                  Pengfei Liu},
  title        = {Align on the Fly: Adapting Chatbot Behavior to Established Norms},
  journal      = {CoRR},
  volume       = {abs/2312.15907},
  year         = {2023},
  url          = {https://arxiv.org/abs/2312.15907}
}

@article{arXiv2023_Human-AI_Collaboration,
  author       = {Andrew Fuchs and
                  Andrea Passarella and
                  Marco Conti},
  title        = {Optimizing delegation between human and {AI} collaborative agents},
  journal      = {CoRR},
  volume       = {abs/2309.14718},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.14718},
  doi          = {10.48550/ARXIV.2309.14718}
}

@inproceedings{ICLR2024_Human-Agent-Collaboration,
  author       = {Yiming Gao and
                  Feiyu Liu and
                  Liang Wang and
                  Zhenjie Lian and
                  Dehua Zheng and
                  Weixuan Wang and
                  Wenjin Yang and
                  Siqin Li and
                  Xianliang Wang and
                  Wenhui Chen and
                  Jing Dai and
                  Qiang Fu and
                  Wei Yang and
                  Lanxiao Huang and
                  Wei Liu},
  title        = {Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=BqEvdOS1Hs}
}

@article{arXiv2024_Human-Agent-Collaboration,
  author       = {Xueyang Feng and 
                  Zhi-Yuan Chen and 
                  Yujia Qin and 
                  Yankai Lin and 
                  Xu Chen and 
                  Zhiyuan Liu and 
                  Ji-Rong Wen},
  title        = {Large Language Model-based Human-Agent Collaboration for Complex Task Solving},
  journal      = {CoRR},
  volume       = {abs/2402.12914},
  year         = {2024},
  url          = {https://arxiv.org/abs/2402.12914}
}

@article{TEVC2023_MultiAgent-Collaboration-SocialRoles,
  author       = {Yaqing Hou and 
                  Mingyang Sun and 
                  Yifeng Zeng and 
                  Yew-Soon Ong and 
                  Yaochu Jin and 
                  Hongwei Ge and 
                  Qiang Zhang},
  journal      = {IEEE Transactions on Evolutionary Computation}, 
  title        = {A Multi-agent Cooperative Learning System with Evolution of Social Roles}, 
  volume       = {},
  number       = {},
  pages        = {1-1},
  year         = {2023},
  url          = {https://ieeexplore.ieee.org/document/10104101},
  doi          = {10.1109/TEVC.2023.3268076}
}

@inproceedings{EMNLP2023-Findings_LEGO,
  author       = {Zhitao He and
                  Pengfei Cao and
                  Yubo Chen and
                  Kang Liu and
                  Ruopeng Li and
                  Mengshu Sun and
                  Jun Zhao},
  title        = {{LEGO:} {A} Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {9142--9163},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.findings-emnlp.613}
}

@article{Nature2023_Role-Play-LLM,
  author       = {Murray Shanahan and
                  Kyle McDonell and
                  Laria Reynolds},
  title        = {Role play with large language models},
  journal      = {Nat.},
  volume       = {623},
  number       = {7987},
  pages        = {493--498},
  year         = {2023},
  url          = {https://doi.org/10.1038/s41586-023-06647-8},
  doi          = {10.1038/S41586-023-06647-8}
}

@article{arXiv2023_PlayGames-LLM,
  author       = {Elif Akata and
                  Lion Schulz and
                  Julian Coda{-}Forno and
                  Seong Joon Oh and
                  Matthias Bethge and
                  Eric Schulz},
  title        = {Playing repeated games with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2305.16867},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.16867},
  doi          = {10.48550/arXiv.2305.16867}
}

@article{arXiv2023_Agent-Simulate-OpinionDynamics,
  author       = {Yun{-}Shiuan Chuang and
                  Agam Goyal and
                  Nikunj Harlalka and
                  Siddharth Suresh and
                  Robert Hawkins and
                  Sijia Yang and
                  Dhavan Shah and
                  Junjie Hu and
                  Timothy T. Rogers},
  title        = {Simulating Opinion Dynamics with Networks of LLM-based Agents},
  journal      = {CoRR},
  volume       = {abs/2311.09618},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.09618},
  doi          = {10.48550/ARXIV.2311.09618}
}

@article{arXiv2023_Agent-Simulate-OpinionDynamics_Survey,
  author       = {Yun{-}Shiuan Chuang and
                  Timothy T. Rogers},
  title        = {Computational Agent-based Models in Opinion Dynamics: {A} Survey on Social Simulations and Empirical Studies},
  journal      = {CoRR},
  volume       = {abs/2306.03446},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.03446},
  doi          = {10.48550/ARXIV.2306.03446}
}

@article{arXiv2024_AI-Human-Creative,
  author       = {Haonan Wang and 
                  James Zou and 
                  Michael Mozer and 
                  Anirudh Goyal and 
                  Alex Lamb and 
                  Linjun Zhang and 
                  Weijie J Su and 
                  Zhun Deng and 
                  Michael Qizhe Xie and 
                  Hannah Brown and 
                  Kenji Kawaguchi},
  title        = {Can AI Be as Creative as Humans?},
  journal      = {CoRR},
  volume       = {abs/2401.01623},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.01623}
}

@article{arXiv2023_LLM-Simulator,
  author       = {Chuyi Kong and
                  Yaxin Fan and
                  Xiang Wan and
                  Feng Jiang and
                  Benyou Wang},
  title        = {Large Language Model as a User Simulator},
  journal      = {CoRR},
  volume       = {abs/2308.11534},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.11534},
  doi          = {10.48550/ARXIV.2308.11534}
}

@article{arXiv2023_MetaAgents,
  author       = {Yuan Li and
                  Yixuan Zhang and
                  Lichao Sun},
  title        = {MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents},
  journal      = {CoRR},
  volume       = {abs/2310.06500},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.06500},
  doi          = {10.48550/ARXIV.2310.06500}
}

@misc{meyer2021entspann,
  title={Entspann dich, Deutschland! TK-Stressstudie 2021},
  author={Meyer, B and Zill, A and Dilba, D and Voermans, S},
  year={2021},
  publisher={Hamburg: Techniker Krankenkasse}
}

@article{wang2024tokeneconomy,
  title={Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies},
  author={Wang, Junlin and Jain, Siddhartha and Zhang, Dejiao and Ray, Baishakhi and Kumar, Varun and Athiwaratkun, Ben},
  journal={arXiv preprint arXiv:2406.06461},
  year={2024}
}

@article{PNAS2024_TuringTest_Chatbots-Humans,
  author       = {Qiaozhu Mei and
                  Yutong Xie and
                  Walter Yuan and
                  Matthew O. Jackson},
  title        = {A Turing test of whether AI chatbots are behaviorally similar to humans},
  journal      = {Proceedings of the National Academy of Sciences},
  volume       = {121},
  number       = {9},
  pages        = {e2313925121},
  year         = {2024},
  url          = {https://doi.org/10.1073/pnas.2313925121},
  doi          = {10.1073/pnas.2313925121}
}
arXiv2023_TuringTest_Chatbots-Humans

@article{arXiv2024_BehavioralSimulation,
  author       = {Cheng Wang and 
                  Chuwen Wang and 
                  Yu Zhao and 
                  Shirong Zeng and 
                  Wang Zhang and 
                  Ronghui Ning},
  title        = {Behavioral Simulation: Exploring A Possible Next Paradigm for Science},
  journal      = {CoRR},
  volume       = {abs/2401.09851},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.09851}
}

@article{arXiv2023_Agent-BehaviorExplanation,
  author       = {Xijia Zhang and
                  Yue Guo and
                  Simon Stepputtis and
                  Katia P. Sycara and
                  Joseph Campbell},
  title        = {Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation},
  journal      = {CoRR},
  volume       = {abs/2311.18062},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.18062},
  doi          = {10.48550/ARXIV.2311.18062}
}

@article{arXiv2023_Agent-BehaviorExplaining,
  author       = {Xijia Zhang and
                  Yue Guo and
                  Simon Stepputtis and
                  Katia P. Sycara and
                  Joseph Campbell},
  title        = {Explaining Agent Behavior with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.10346},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.10346},
  doi          = {10.48550/ARXIV.2309.10346}
}

@article{arXiv2023_Agents-High-Level-Behavior,
  author       = {Maxwell Crouse and
                  Ibrahim Abdelaziz and
                  Kinjal Basu and
                  Soham Dan and
                  Sadhana Kumaravel and
                  Achille Fokoue and
                  Pavan Kapanipathi and
                  Luis A. Lastras},
  title        = {Formally Specifying the High-Level Behavior of LLM-Based Agents},
  journal      = {CoRR},
  volume       = {abs/2310.08535},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.08535},
  doi          = {10.48550/ARXIV.2310.08535}
}

@article{arXiv2024_Agents-Simulate-Trust,
  author       = {Chengxing Xie and
                  Canyu Chen and
                  Feiran Jia and
                  Ziyu Ye and
                  Kai Shu and
                  Adel Bibi and
                  Ziniu Hu and
                  Philip H. S. Torr and
                  Bernard Ghanem and
                  Guohao Li},
  title        = {Can Large Language Model Agents Simulate Human Trust Behaviors?},
  journal      = {CoRR},
  volume       = {abs/2402.04559},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.04559},
  doi          = {10.48550/ARXIV.2402.04559}
}

@article{arXiv2024_Reward-Socially-RL-Agent,
  author       = {Zhaoyue Wang},
  title        = {Towards Socially and Morally Aware RL agent: Reward Design With LLM},
  journal      = {CoRR},
  volume       = {abs/2401.12459},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.12459}
}

@inproceedings{ICLR2024_LLM-Simulate-CognitiveModel,
  author       = {Marcel Binz and
                  Eric Schulz},
  title        = {Turning large language models into cognitive models},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=eiC4BKypf1}
}
arXiv2023_LLM-Simulate-CognitiveModel

@article{arXiv2023_Agent-Cognitive,
  author       = {Theodore R. Sumers and
                  Shunyu Yao and
                  Karthik Narasimhan and
                  Thomas L. Griffiths},
  title        = {Cognitive Architectures for Language Agents},
  journal      = {CoRR},
  volume       = {abs/2309.02427},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.02427},
  doi          = {10.48550/ARXIV.2309.02427}
}
TMLR

@article{J2024_Interactions-Cognitive-LLMs,
  author       = {Youzhi Qu and 
                  Penghui Du and 
                  Wenxin Che and 
                  Chen Wei and 
                  Chi Zhang and 
                  Wanli Ouyang and 
                  Yatao Bian and 
                  Feiyang Xu and 
                  Bin Hu and 
                  Kai Du and 
                  Haiyan Wu and 
                  Jia Liu and 
                  Quanying Liu},
  title        = {Promoting interactions between cognitive science and large language models},
  journal      = {The Innovation},
  pages        = {100579},
  issn         = {2666-6758},
  year         = {2024},
  url          = {https://www.sciencedirect.com/science/article/pii/S2666675824000171},
  doi          = {https://doi.org/10.1016/j.xinn.2024.100579}
}

@article{arXiv2024_Multi-Agent_Conversation_CognitiveBias,
  author       = {Yu He Ke and 
                  Rui Yang and 
                  Sui An Lie and 
                  Taylor Xin Yi Lim and 
                  Hairil Rizal Abdullah and 
                  Daniel Shu Wei Ting and 
                  Nan Liu},
  title        = {Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias},
  journal      = {CoRR},
  volume       = {abs/2401.14589},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.14589}
}

@article{arXiv2023_AI-Agent,
  author       = {Sungwoo Lee and
                  Younghyun Oh and
                  Hyunhoe An and
                  Hyebhin Yoon and
                  Karl J. Friston and
                  Seok Jun Hong and
                  Choong{-}Wan Woo},
  title        = {Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents},
  journal      = {CoRR},
  volume       = {abs/2309.05999},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.05999},
  doi          = {10.48550/ARXIV.2309.05999}
}

@article{arXiv2024_Agent-Preference,
  author       = {Zihong He and 
                  Changwang Zhang},
  title        = {AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2401.02870},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.02870}
}

@article{PANS2022_Agent-Cooperation-Competition,
  author       = {Euel Elliott and 
                  L. Douglas Kiel},
  title        = {Exploring cooperation and competition using agent-based modeling},
  journal      = {Proceedings of the National Academy of Sciences},
  volume       = {99},
  number       = {suppl\_3},
  pages        = {7193-7194},
  year         = {2002},
  url          = {https://www.pnas.org/doi/abs/10.1073/pnas.102079099},
  doi          = {10.1073/pnas.102079099}
}

@article{arXiv2023_LyfeAgents,
  author       = {Zhao Kaiya and
                  Michelangelo Naim and
                  Jovana Kondic and
                  Manuel Cortes and
                  Jiaxin Ge and
                  Shuying Luo and
                  Guangyu Robert Yang and
                  Andrew Ahn},
  title        = {Lyfe Agents: Generative agents for low-cost real-time social interactions},
  journal      = {CoRR},
  volume       = {abs/2310.02172},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.02172},
  doi          = {10.48550/ARXIV.2310.02172}
}

@inproceedings{ICLR2023_Reasoning-Simulation,
  author       = {Ruibo Liu and
                  Jason Wei and
                  Shixiang Shane Gu and
                  Te{-}Yen Wu and
                  Soroush Vosoughi and
                  Claire Cui and
                  Denny Zhou and
                  Andrew M. Dai},
  title        = {Mind's Eye: Grounded Language Model Reasoning through Simulation},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=4rXMRuoJlai}
}

@article{arXiv2024_Human-AI-Interaction_Gesture,
  author       = {Philipp Wicke},
  title        = {Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction},
  journal      = {CoRR},
  volume       = {abs/2401.17858},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.17858}
}

@article{arXiv2023_Human-like-Agents,
  author       = {Thuy{-}Ngoc Nguyen and
                  Chase McDonald and
                  Cleotilde Gonzalez},
  title        = {Credit Assignment: Challenges and Opportunities in Developing Human-like {AI} Agents},
  journal      = {CoRR},
  volume       = {abs/2307.08171},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.08171},
  doi          = {10.48550/ARXIV.2307.08171}
}

@article{arXiv2024_Human-Centered-LM,
  author       = {Nikita Soni and 
                  Niranjan Balasubramanian and 
                  H. Andrew Schwartz and 
                  Dirk Hovy},
  title        = {Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?},
  journal      = {CoRR},
  volume       = {abs/2401.12492},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.12492}
}

-----Theory-----
@book{Book1973_Theory_NLP,
  author       = {Alfred V. Aho and
                  Jeffrey D. Ullman},
  title        = {The theory of parsing, translation, and compiling. 2: Compiling},
  publisher    = {Prentice-Hall},
  year         = {1973},
  url          = {https://www.worldcat.org/oclc/310805948},
  isbn         = {0139145648}
}

@incollection{2018_Theory,
  author       = {Mezirow, Jack},
  title        = {Transformative learning theory},
  booktitle    = {Contemporary theories of learning},
  pages        = {114--128},
  year         = {2018},
  publisher    = {Routledge},
  url          = {https://www.wichita.edu/services/mrc/OIR/Pedagogy/Theories/transformative.php}
}

---Byzantine Consensus Theory---
@inproceedings{USENIX1999_Theory-FaultTolerance,
  author       = {Miguel Castro and
                  Barbara Liskov},
  title        = {Practical Byzantine Fault Tolerance},
  booktitle    = {{OSDI}},
  pages        = {173--186},
  publisher    = {{USENIX} Association},
  year         = {1999},
  url          = {https://dl.acm.org/citation.cfm?id=296824}
}

---Collaborative Learning---
@incollection{Book2013_InfoProcess-Collaboration,
  author       = {Noreen M Webb},
  title        = {Information processing approaches to collaborative learning},
  booktitle    = {The international handbook of collaborative learning},
  publisher    = {Routledge},
  pages        = {19--40},
  year         = {2013},
  url          = {https://psycnet.apa.org/record/2013-13644-001}
}

---Personality----
@book{Book1999_Personality,
  author       = {Friedman, Howard S and 
                  Schustack, Miriam W},
  title        = {Personality: Classic theories and modern research},
  publisher    = {Allyn and Bacon Boston, MA},
  year         = {1999},
  url          = {https://books.google.com/books/about/Personality.html?id=ziTvDAAAQBAJ}
}

@article{J2008_Overconfidence,
  author       = {Moore, Don A and Healy, Paul J},
  title        = {The trouble with overconfidence.},
  journal      = {Psychological review},
  volume       = {115},
  number       = {2},
  pages        = {502},
  publisher    = {American Psychological Association},
  year         = {2008},
  url          = {https://healy.econ.ohio-state.edu/papers/Moore_Healy-TroubleWithOverconfidence_WP.pdf}
}

@book{Book1985_MedievalPoliticalTheology,
  author       = {Ernst H. Kantorowicz},
  title        = {The King's Two Bodies: A Study in Medieval Political Theology},
  publisher    = {Princeton University Press},
  ISBN         = {9780691169231},
  urldate      = {2023-06-21},
  year         = {1985},
  url          = {http://www.jstor.org/stable/j.ctvcszz1c}
}


-----Social Psychology View-----
@article{J2009_SocialPsychology,
  author       = {Johnson, David W and Johnson, Roger T},
  title        = {An educational psychology success story: Social interdependence theory and cooperative learning},
  journal      = {Educational researcher},
  volume       = {38},
  number       = {5},
  pages        = {365--379},
  publisher    = {Sage Publications},
  year         = {2009},
  url          = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=72585feb1200d53a81d4fb3e64862d69317b72c3}
}

@article{1982_SocialPsychology,
  author       = {Tajfel, Henri},
  title        = {Social psychology of intergroup relations},
  journal      = {Annual review of psychology},
  volume       = {33},
  number       = {1},
  publisher    = {Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA},
  pages        = {1--39},
  year         = {1982},
  url          = {https://www.annualreviews.org/doi/abs/10.1146/annurev.ps.33.020182.000245?journalCode=psych}
}

@incollection{2004_SocialPsychology,
  author       = {Tajfel, Henri and 
                 Turner, John C},
  title        = {The social identity theory of intergroup behavior},
  booktitle    = {Political psychology},
  pages        = {276--293},
  year         = {2004},
  publisher    = {Psychology Press},
  url          = {https://psycnet.apa.org/record/2004-13697-016}
}

---Society of Mind---
@book{Book1988_SoM,
  author       = {Minsky, Marvin},
  title        = {Society of mind},
  year         = {1988},
  publisher    = {Simon and Schuster},
  url          = {https://www.simonandschuster.com/books/Society-Of-Mind/Marvin-Minsky/9780671657130}
}

@article{yuan2024evoagent,
  title={EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms},
  author={Yuan, Siyu and Song, Kaitao and Chen, Jiangjie and Tan, Xu and Li, Dongsheng and Yang, Deqing},
  journal={arXiv preprint arXiv:2406.14228},
  year={2024}
}

@article{hu2024adas,
  title={Automated design of agentic systems},
  author={Hu, Shengran and Lu, Cong and Clune, Jeff},
  journal={arXiv preprint arXiv:2408.08435},
  year={2024}
}

@article{shang2024agentsquare,
  title={AgentSquare: Automatic LLM Agent Search in Modular Design Space},
  author={Shang, Yu and Li, Yu and Zhao, Keyu and Ma, Likai and Liu, Jiahe and Xu, Fengli and Li, Yong},
  journal={arXiv preprint arXiv:2410.06153},
  year={2024}
}

@article{J2003_SoM,
  author       = {Push Singh},
  title        = {Examining the Society of Mind},
  journal      = {Comput. Artif. Intell.},
  volume       = {22},
  number       = {6},
  pages        = {521--543},
  year         = {2003},
  url          = {http://www.cai.sk/ojs/index.php/cai/article/view/467}
}

@article{shen2024hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shao2023characterllm,
  title={Character-llm: A trainable agent for role-playing},
  author={Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2310.10158},
  year={2023}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2023graphtoolformer,
  title={Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt},
  author={Zhang, Jiawei},
  journal={arXiv preprint arXiv:2304.11116},
  year={2023}
}


@article{shen2023taskbench,
  title={Taskbench: Benchmarking large language models for task automation},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2311.18760},
  year={2023}
}


@article{wang2023describe,
  title={Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents},
  author={Wang, Zihao and Cai, Shaofei and Chen, Guanzhou and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  journal={arXiv preprint arXiv:2302.01560},
  year={2023}
}


@article{wang2023plan,
  title={Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models},
  author={Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2305.04091},
  year={2023}
}


@inproceedings{deng2023plug,
  title={Plug-and-play policy planner for large language model powered dialogue agents},
  author={Deng, Yang and Zhang, Wenxuan and Lam, Wai and Ng, See-Kiong and Chua, Tat-Seng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{zhu2024knowagent,
  title={Knowagent: Knowledge-augmented planning for llm-based agents},
  author={Zhu, Yuqi and Qiao, Shuofei and Ou, Yixin and Deng, Shumin and Zhang, Ningyu and Lyu, Shiwei and Shen, Yue and Liang, Lei and Gu, Jinjie and Chen, Huajun},
  journal={arXiv preprint arXiv:2403.03101},
  year={2024}
}


@inproceedings{zhong2024memorybank,
  title={Memorybank: Enhancing large language models with long-term memory},
  author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={19724--19731},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{he2023lego,
  title={LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation},
  author={He, Zhitao and Cao, Pengfei and Chen, Yubo and Liu, Kang and Li, Ruopeng and Sun, Mengshu and Zhao, Jun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={9142--9163},
  year={2023}
}


@article{packer2023memgpt,
  title={Memgpt: Towards llms as operating systems},
  author={Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G and Stoica, Ion and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2310.08560},
  year={2023}
}

@inproceedings{hatalis2023memorymatter,
  title={Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents},
  author={Hatalis, Kostas and Christou, Despina and Myers, Joshua and Jones, Steven and Lambert, Keith and Amos-Binks, Adam and Dannenhauer, Zohreh and Dannenhauer, Dustin},
  booktitle={Proceedings of the AAAI Symposium Series},
  volume={2},
  number={1},
  pages={277--280},
  year={2023}
}



@article{zhang2024aflow,
  title={Aflow: Automating agentic workflow generation},
  author={Zhang, Jiayi and Xiang, Jinyu and Yu, Zhaoyang and Teng, Fengwei and Chen, Xionghui and Chen, Jiaqi and Zhuge, Mingchen and Cheng, Xin and Hong, Sirui and Wang, Jinlin and others},
  journal={arXiv preprint arXiv:2410.10762},
  year={2024}
}

@inproceedings{NeurIPS2023camel,
  author       = {Guohao Li and
                  Hasan Hammoud and
                  Hani Itani and
                  Dmitrii Khizbullin and
                  Bernard Ghanem},
  title        = {{CAMEL:} Communicative Agents for "Mind" Exploration of Large Language Model Society},
  booktitle    = {NeurIPS},
  year         = {2023}
}
arXiv2023_Agent-SoM

@article{arXiv2023_SoM-NL,
  author       = {Mingchen Zhuge and
                  Haozhe Liu and
                  Francesco Faccio and
                  Dylan R. Ashley and
                  R{\'{o}}bert Csord{\'{a}}s and
                  Anand Gopalakrishnan and
                  Abdullah Hamdi and
                  Hasan Abed Al Kader Hammoud and
                  Vincent Herrmann and
                  Kazuki Irie and
                  Louis Kirsch and
                  Bing Li and
                  Guohao Li and
                  Shuming Liu and
                  Jinjie Mai and
                  Piotr Piekos and
                  Aditya Ramesh and
                  Imanol Schlag and
                  Weimin Shi and
                  Aleksandar Stanic and
                  Wenyi Wang and
                  Yuhui Wang and
                  Mengmeng Xu and
                  Deng{-}Ping Fan and
                  Bernard Ghanem and
                  J{\"{u}}rgen Schmidhuber},
  title        = {Mindstorms in Natural Language-Based Societies of Mind},
  journal      = {CoRR},
  volume       = {abs/2305.17066},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.17066},
  doi          = {10.48550/arXiv.2305.17066}
}
DBLP:journals/corr/abs-2305-17066


---Theory Of Mind---
@article{J2002_TheoryOfMind,
  author       = {Siegal, Michael and 
                  Varley, Rosemary},
  title        = {Neural systems involved in 'theory of mind'},
  journal      = {Nature Reviews Neuroscience},
  volume       = {3},
  number       = {6},
  pages        = {463--471},
  publisher    = {Nature Publishing Group UK London},
  year         = {2002},
  url          = {https://www.nature.com/articles/nrn844},
  doi          = {10.1038/nrn844}
}

@article{J2004_TheoryOfMind,
  author       = {Leslie, Alan M and 
                  Friedman, Ori and 
                  German, Tim P},
  title        = {Core mechanisms in ‘theory of mind’},
  journal      = {Trends in cognitive sciences},
  volume       = {8},
  number       = {12},
  pages        = {528--533},
  issn         = {1364-6613},
  publisher    = {Elsevier},
  year         = {2004},
  url          = {https://www.sciencedirect.com/science/article/pii/S1364661304002608},
  doi          = {https://doi.org/10.1016/j.tics.2004.10.001}
}

@inproceedings{EMNLP2022_TheoryOfMind,
  author       = {Maarten Sap and
                  Ronan Le Bras and
                  Daniel Fried and
                  Yejin Choi},
  title        = {Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs},
  booktitle    = {{EMNLP}},
  pages        = {3762--3780},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.emnlp-main.248},
  doi          = {10.18653/V1/2022.EMNLP-MAIN.248}
}

@inproceedings{EMNLP2023_TheoryOfMind-Agents,
  author       = {Huao Li and
                  Yu Quan Chong and
                  Simon Stepputtis and
                  Joseph Campbell and
                  Dana T. Hughes and
                  Charles Lewis and
                  Katia P. Sycara},
  title        = {Theory of Mind for Multi-Agent Collaboration via Large Language Models},
  booktitle    = {{EMNLP}},
  pages        = {180--192},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.13}
}

@inproceedings{EACL2024_TheoryOfMind,
  author       = {Natalie Shapira and
                  Mosh Levy and
                  Seyed Hossein Alavi and
                  Xuhui Zhou and
                  Yejin Choi and
                  Yoav Goldberg and
                  Maarten Sap and
                  Vered Shwartz},
  title        = {Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models},
  booktitle    = {{EACL} {(1)}},
  pages        = {2257--2273},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.eacl-long.138}
}
arXiv2023_TheoryOfMind

@article{arXiv2023_TheoryOfMind-Agents,
  author       = {Pei Zhou and
                  Aman Madaan and
                  Srividya Pranavi Potharaju and
                  Aditya Gupta and
                  Kevin R. McKee and
                  Ari Holtzman and
                  Jay Pujara and
                  Xiang Ren and
                  Swaroop Mishra and
                  Aida Nematzadeh and
                  Shyam Upadhyay and
                  Manaal Faruqui},
  title        = {How FaR Are Large Language Models From Agents with Theory-of-Mind?},
  journal      = {CoRR},
  volume       = {abs/2310.03051},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.03051},
  doi          = {10.48550/ARXIV.2310.03051}
}

@article{arXiv2023_TheoryofMind-Game,
  author       = {Jiaxian Guo and
                  Bo Yang and
                  Paul Yoo and
                  Bill Yuchen Lin and
                  Yusuke Iwasawa and
                  Yutaka Matsuo},
  title        = {Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware {GPT-4}},
  journal      = {CoRR},
  volume       = {abs/2309.17277},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.17277},
  doi          = {10.48550/ARXIV.2309.17277}
}

---Group---
@article{Science2010_HumanDynamics,
  author       = {Anita Williams Woolley and 
                  Christopher F. Chabris and 
                  Alex Pentland and 
                  Nada Hashmi and 
                  Thomas W. Malone},
  title        = {Evidence for a Collective Intelligence Factor in the Performance of Human Groups},
  journal      = {Science},
  volume       = {330},
  number       = {6004},
  pages        = {686-688},
  year         = {2010},
  doi          = {10.1126/science.1193147},
  url          = {https://www.science.org/doi/abs/10.1126/science.1193147}
}

@misc{1968_GroupDynamics,
  author       = {Dorwin Cartwright and 
                  Alvin Zander},
  title        = {Group dynamics},
  publisher    = {Harper+ Row},
  year         = {1968},
  url          = {https://psycnet.apa.org/record/1968-12031-000}
}

@article{J2018_GroupDynamics,
  author       = {Wilfred R Bion},
  title        = {Group dynamics: A re-view},
  journal      = {New directions in psychoanalysis},
  pages        = {440--477},
  publisher    = {Routledge},
  year         = {2018},
  url          = {https://www.taylorfrancis.com/chapters/edit/10.4324/9780429477546-19/group-dynamics-re-view-bion}
}

@book{Book2014_GroupDynamics,
  author       = {Donelson R Forsyth},
  title        = {Group dynamics},
  publisher    = {Wadsworth Cengage Learning},
  year         = {2014},
  url          = {https://scholarship.richmond.edu/bookshelf/5/}
}

@book{Book2018_GroupDynamics,
  author       = {Donelson R Forsyth},
  title        = {Group dynamics},
  publisher    = {Cengage Learning},
  year         = {2018},
  url          = {https://books.google.com.sg/books?id=vg9EDwAAQBAJ&newbks=0&source=newbks_fb&redir_esc=y}
}

@article{J1987_GroupDynamics,
  author       = {Clayton P Alderfer},
  title        = {An intergroup perspective on group dynamics},
  journal      = {Handbook of organizational behavior},
  volume       = {190},
  pages        = {222},
  year         = {1987},
  url          = {https://apps.dtic.mil/sti/citations/ADA135582}
}

@inproceedings{yin2023exchange,
  title={Exchange-of-thought: Enhancing large language model capabilities through cross-model communication},
  author={Yin, Zhangyue and Sun, Qiushi and Chang, Cheng and Guo, Qipeng and Dai, Junqi and Huang, Xuan-Jing and Qiu, Xipeng},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={15135--15153},
  year={2023}
}

@inproceedings{chen2024benchmarking,
  title={Benchmarking large language models in retrieval-augmented generation},
  author={Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17754--17762},
  year={2024}
}

@article{zhuang2023toolchain,
  title={Toolchain*: Efficient action space navigation in large language models with a* search},
  author={Zhuang, Yuchen and Chen, Xiang and Yu, Tong and Mitra, Saayan and Bursztyn, Victor and Rossi, Ryan A and Sarkhel, Somdeb and Zhang, Chao},
  journal={arXiv preprint arXiv:2310.13227},
  year={2023}
}

@article{shen2024smallllms,
  title={Small llms are weak tool learners: A multi-llm agent},
  author={Shen, Weizhou and Li, Chenliang and Chen, Hongzhan and Yan, Ming and Quan, Xiaojun and Chen, Hehong and Zhang, Ji and Huang, Fei},
  journal={arXiv preprint arXiv:2401.07324},
  year={2024}
}

@article{J1998_SmallGroupDynamics-Discussions,
  author       = {David Wyatt Seal and 
                  Laura M Bogart and 
                  Anke A Ehrhardt},
  title        = {Small group dynamics: The utility of focus group discussions as a research method},
  journal      = {Group Dynamics: Theory, Research, and Practice},
  volume       = {2},
  number       = {4},
  pages        = {253},
  publisher    = {Educational Publishing Foundation},
  year         = {1998},
  url          = {https://doi.org/10.1037/1089-2699.2.4.253}
}

@book{Book1972_Groupthink,
  author       = {Janis, Irving L},
  title        = {Victims of Groupthink: A psychological study of foreign-policy decisions and fiascoes.},
  publisher    = {Houghton Mifflin},
  year         = {1972},
  url          = {https://psycnet.apa.org/record/1975-29417-000}
}

@article{J2015_Group,
  author       = {Iyengar, Shanto and Westwood, Sean J},
  title        = {Fear and loathing across party lines: New evidence on group polarization},
  journal      = {American journal of political science},
  volume       = {59},
  number       = {3},
  pages        = {690--707},
  publisher    = {Wiley Online Library},
  year         = {2015},
  url          = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=f8248c39c3daff874fb0f6f5abc667ebcdfee024}
}

@article{J2003_Intergroup-Intragroup_Culture,
  author       = {Masaki Yuki},
  title        = {Intergroup Comparison versus Intragroup Relationships: A Cross-Cultural Examination of Social Identity Theory in North American and East Asian Cultural Contexts},
  journal      = {Social Psychology Quarterly},
  issn         = {01902725},
  number       = {2},
  pages        = {166--183},
  volume       = {66},
  publisher    = {[Sage Publications, Inc., American Sociological Association]},
  urldate      = {2024-01-16},
  year         = {2003},
  url          = {http://www.jstor.org/stable/1519846}
}

@article{J1995_IntragroupConflict,
  author       = {Karen A Jehn},
  title        = {A multimethod examination of the benefits and detriments of intragroup conflict},
  journal      = {Administrative science quarterly},
  publisher    = {JSTOR},
  pages        = {256--282},
  year         = {1995},
  url          = {https://www.jstor.org/stable/2393638},
  doi          = {10.2307/2393638}
}

@article{J2003_SmartGroups,
  author       = {Brigid Barron},
  title        = {When Smart Groups Fail},
  journal      = {Journal of the Learning Sciences},
  volume       = {12},
  number       = {3},
  pages        = {307--359},
  publisher    = {Routledge},
  year         = {2003},
  url          = {https://doi.org/10.1207/S15327809JLS1203_1},
  doi          = {10.1207/S15327809JLS1203\_1}
}

---Intelligence---
@book{Book2005_CrowdWisdom,
  author       = {Surowiecki, James},
  title        = {The Wisdom of Crowds},
  isbn         = {0385721706},
  publisher    = {Anchor},
  year         = {2005},
  url          = {https://books.google.com.sg/books/about/The_Wisdom_of_Crowds.html?id=hHUsHOHqVzEC&redir_esc=y}
}

@article{J1996_Intelligence,
  author       = {Neisser, Ulric and 
                  Boodoo, Gwyneth and 
                  Bouchard Jr, Thomas J and 
                  Boykin, A Wade and 
                  Brody, Nathan and 
                  Ceci, Stephen J and 
                  Halpern, Diane F and 
                  Loehlin, John C and 
                  Perloff, Robert and 
                  Sternberg, Robert J and 
                  others},
  title        = {Intelligence: knowns and unknowns.},
  journal      = {American psychologist},
  volume       = {51},
  number       = {2},
  pages        = {77},
  publisher    = {American Psychological Association},
  year         = {1996},
  url          = {https://psycnet.apa.org/record/1996-02655-001}
}

@article{1961_Intelligence,
  author       = {Spearman, Charles},
  title        = {"General Intelligence" Objectively Determined and Measured.},
  publisher    = {Appleton-Century-Crofts},
  year         = {1961},
  url          = {https://psycnet.apa.org/record/1926-00296-001}
}

---Conformity---
@article{J2004_Conformity,
  author       = {Robert B. Cialdini
                  and Noah J. Goldstein},
  title        = {Social Influence: Compliance and Conformity},
  journal      = {Annual Review of Psychology},
  volume       = {55},
  number       = {1},
  pages        = {591-621},
  note         = {PMID: 14744228},
  year         = {2004},
  url          = {https://doi.org/10.1146/annurev.psych.55.090902.142015},
  doi          = {10.1146/annurev.psych.55.090902.142015}
}

@article{J1969_Conformity,
  author       = {Vernon L. Allen and John M. Levine},
  title        = {Consensus and conformity},
  journal      = {Journal of Experimental Social Psychology},
  volume       = {5},
  number       = {4},
  pages        = {389-399},
  issn         = {0022-1031},
  year         = {1969},
  url          = {https://www.sciencedirect.com/science/article/pii/0022103169900328},
  doi          = {https://doi.org/10.1016/0022-1031(69)90032-8}
}

@book{Book2011_Negotiation,
  author       = {Fisher, Roger and 
                  Ury, William L and 
                  Patton, Bruce},
  title        = {Getting to yes: Negotiating agreement without giving in},
  publisher    = {Penguin},
  year         = {2011},
  url          = {https://www.pon.harvard.edu/shop/getting-to-yes-negotiating-agreement-without-giving-in/}
}

@article{J2015_Conformity,
  author       = {Julie C Coultas
                  and Edwin JC van Leeuwen},
  title        = {Conformity: Definitions, types, and evolutionary grounding},
  journal      = {Evolutionary perspectives on social psychology},
  publisher    = {Springer},
  pages        = {189--202},
  year         = {2015},
  url          = {https://link.springer.com/chapter/10.1007/978-3-319-12697-5\_15}
}

---Consensus---
@article{J1967_Consensus-Sociological,
  author       = {Scheff, Thomas J},
  title        = {Toward a sociological model of consensus},
  journal      = {American Sociological Review},
  pages        = {32--46},
  publisher    = {JSTOR},
  year         = {1967},
  url          = {https://doi.org/10.2307/2091716},
  doi          = {10.2307/2091716}
}

@article{J1974_ConsensusReaching,
  author       = {Morris H. Degroot},
  title        = {Reaching a Consensus},
  journal      = {Journal of the American Statistical Association},
  volume       = {69},
  number       = {345},
  pages        = {118-121},
  publisher    = {Taylor & Francis},
  year         = {1974},
  url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10480137},
  doi          = {10.1080/01621459.1974.10480137}
}

@article{J2018_Emergence-Consensus,
  author       = {Baronchelli, Andrea},
  title        = {The emergence of consensus: a primer},
  journal      = {Royal Society open science},
  volume       = {5},
  number       = {2},
  pages        = {172189},
  publisher    = {The Royal Society Publishing},
  year         = {2018},
  url          = {http://doi.org/10.1098/rsos.172189},
  doi          = {10.1098/rsos.172189}
}

---Social---
@article{arXiv2023_Agent-SocialChoiceTheory,
  author       = {Marc Lanctot and
                  Kate Larson and
                  Yoram Bachrach and
                  Luke Marris and
                  Zun Li and
                  Avishkar Bhoopchand and
                  Thomas W. Anthony and
                  Brian Tanner and
                  Anna Koop},
  title        = {Evaluating Agents using Social Choice Theory},
  journal      = {CoRR},
  volume       = {abs/2312.03121},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.03121},
  doi          = {10.48550/ARXIV.2312.03121}
}

@article{J2000_Agent-SocialScience,
  author       = {Nigel Gilbert and 
                  Pietro Terna},
  title        = {How to build and use agent-based models in social science},
  journal      = {Mind \& Society},
  volume       = {1},
  pages        = {57--72},
  publisher    = {Springer},
  year         = {2000},
  url          = {https://link.springer.com/article/10.1007/BF02512229}
}

@book{Book2012_Agent-SocialScience,
  author       = {Joshua M Epstein},
  title        = {Generative social science: Studies in agent-based computational modeling},
  publisher    = {Princeton University Press},
  year         = {2012},
  url          = {https://press.princeton.edu/books/ebook/9781400842872/generative-social-science}
}

@book{Book2023_Agent-SocialDynamics-Culture,
  author       = {Paul Smaldino},
  title        = {Modeling social behavior: Mathematical and agent-based models of social dynamics and cultural evolution},
  publisher    = {Princeton University Press},
  year         = {2023},
  url          = {https://press.princeton.edu/books/paperback/9780691224145/modeling-social-behavior}
}

@article{J2017_SocialInfluence_OpinionDynamics,
  author       = {Andreas Flache and
                  Michael M{\"{a}}s and
                  Thomas Feliciani and
                  Edmund Chattoe{-}Brown and
                  Guillaume Deffuant and
                  Sylvie Huet and
                  Jan Lorenz},
  title        = {Models of Social Influence: Towards the Next Frontiers},
  journal      = {J. Artif. Soc. Soc. Simul.},
  volume       = {20},
  number       = {4},
  year         = {2017},
  url          = {https://doi.org/10.18564/jasss.3521},
  doi          = {10.18564/JASSS.3521}
}

@article{J2021_SocietalDynamics_OpinionDynamics,
  author       = {Jan Lorenz and 
                  Martin Neumann 
                  and Tobias Schr{\"o}der},
  title        = {Individual attitude change and societal dynamics: Computational experiments with psychological theories.},
  journal      = {Psychological Review},
  volume       = {128},
  number       = {4},
  pages        = {623},
  publisher    = {American Psychological Association},
  year         = {2021},
  url          = {https://doi.org/10.1037/rev0000291}
}

---Psychology---
@article{PNAS2023_CognitivePsychology_LLM,
  author       = {Marcel Binz and 
                  Eric Schulz},
  title        = {Using cognitive psychology to understand GPT-3},
  journal      = {Proceedings of the National Academy of Sciences},
  volume       = {120},
  number       = {6},
  pages        = {e2218523120},
  year         = {2023},
  url          = {https://www.pnas.org/doi/abs/10.1073/pnas.2218523120},
  doi          = {10.1073/pnas.2218523120}
}

@article{arXiv2023_MachinePsychology,
  author       = {Thilo Hagendorff},
  title        = {Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods},
  journal      = {CoRR},
  volume       = {abs/2303.13988},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.13988},
  doi          = {10.48550/ARXIV.2303.13988}
}

@article{J2023_LLM-Psychology,
  author       = {Dorottya Demszky and 
                  Diyi Yang and 
                  David S. Yeager and 
                  Christopher J. Bryan and 
                  Margarett Clapper and 
                  Susannah Chandhok and  
                  Johannes C. Eichstaedt and  
                  Cameron Hecht and  
                  Jeremy Jamieson and  
                  Meghann Johnson and  
                  Michaela Jones and 
                  Danielle Krettek-Cobb and 
                  Leslie Lai and 
                  Nirel JonesMitchell and  
                  Desmond C. Ong and  
                  Carol S. Dweck and  
                  James J. Gross and 
                  James W. Pennebaker},
    title      = {Using large language models in psychology},
    journal    = {Nature Reviews Psychology},
    year       = {2023},
    month      = {Nov},
    day        = {01},
    volume     = {2},
    number     = {11},
    pages      = {688-701},
    issn       = {2731-0574},
    url        = {https://doi.org/10.1038/s44159-023-00241-5},
    doi        = {10.1038/s44159-023-00241-5}
}

@inproceedings{NAACL2024-Findings_PsychometricLLM,
  author       = {Tatsuki Kuribayashi and
                  Yohei Oseki and
                  Timothy Baldwin},
  title        = {Psychometric Predictive Power of Large Language Models},
  booktitle    = {{NAACL} (Findings)},
  pages        = {},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2311.07484}
}
arXiv2023_PsychometricLLM

@article{arXiv2024_Multi-Agent_PsySafe,
  author       = {Zaibin Zhang and 
                  Yongting Zhang and 
                  Lijun Li and 
                  Hongzhi Gao and 
                  Lijun Wang and 
                  Huchuan Lu and 
                  Feng Zhao and 
                  Yu Qiao and 
                  Jing Shao},
  title        = {PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety},
  journal      = {CoRR},
  volume       = {abs/2401.11880},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.11880}
}

---Democracy---
@book{Book2006_DemocracyModel,
  author       = {David Held},
  title        = {Models of democracy},
  publisher    = {Polity},
  year         = {2006},
  url          = {https://www.sup.org/books/title/?id=10597}
}

@book{Book2006_Deliberative-Democracy,
  author       = {Diana C Mutz},
  title        = {Hearing the other side: Deliberative versus participatory democracy},
  publisher    = {Cambridge University Press},
  year         = {2006},
  url          = {https://www.cambridge.org/core/books/hearing-the-other-side/7CB061238546313D287668FF8EFE2EF7}
}

---other related---
@article{arXiv2023_SocialPsychology-Vehicles,
  author       = {Xiao Li and
                  Kaiwen Liu and
                  H. Eric Tseng and
                  Anouck Girard and
                  Ilya V. Kolmanovsky},
  title        = {Interaction-Aware Decision-Making for Autonomous Vehicles in Forced Merging Scenario Leveraging Social Psychology Factors},
  journal      = {CoRR},
  volume       = {abs/2309.14497},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.14497},
  doi          = {10.48550/ARXIV.2309.14497}
}

@book{Book2019_CriticalThinking,
  author       = {Paul, Richard and Elder, Linda},
  title        = {The miniature guide to critical thinking concepts and tools},
  publisher    = {Rowman \& Littlefield}, 
  year         = {2019},
  url          = {https://www.criticalthinking.org/files/Concepts_Tools.pdf}
}

@book{Book1994_Framework,
  author       = {Popper, Karl Raimund},
  title        = {The myth of the framework: In defence of science and rationality},
  publisher    = {Psychology Press},
  year         = {1994},
  url          = {http://www.math.chalmers.se/~ulfp/Review/framework.pdf}
}

@article{J2012_Circulations,
  author       = {Munro, Iain},
  title        = {The management of circulations: Biopolitical variations after Foucault},
  journal      = {International Journal of Management Reviews},
  volume       = {14},
  number       = {3},
  pages        = {345--362},
  publisher    = {Wiley Online Library},
  year         = {2012}, 
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-2370.2011.00320.x}
}


-----Datasets-----

@inproceedings{NeurIPS2021_Dataset-MATH,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {NeurIPS Datasets and Benchmarks},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html}
}

@article{arXiv2022_Dataset-ChessMoveValidity,
  author       = {Aarohi Srivastava and
                  Abhinav Rastogi and
                  Abhishek Rao and
                  Abu Awal Md Shoeb and
                  Abubakar Abid and
                  Adam Fisch and
                  Adam R. Brown and
                  Adam Santoro and
                  Aditya Gupta and
                  Adri{\`{a}} Garriga{-}Alonso and
                  Agnieszka Kluska and
                  Aitor Lewkowycz and
                  Akshat Agarwal and
                  Alethea Power and
                  Alex Ray and
                  Alex Warstadt and
                  Alexander W. Kocurek and
                  Ali Safaya and
                  Ali Tazarv and
                  Alice Xiang and
                  Alicia Parrish and
                  Allen Nie and
                  Aman Hussain and
                  Amanda Askell and
                  Amanda Dsouza and
                  Ameet Rahane and
                  Anantharaman S. Iyer and
                  Anders Andreassen and
                  Andrea Santilli and
                  Andreas Stuhlm{\"{u}}ller and
                  Andrew M. Dai and
                  Andrew La and
                  Andrew K. Lampinen and
                  Andy Zou and
                  Angela Jiang and
                  Angelica Chen and
                  Anh Vuong and
                  Animesh Gupta and
                  Anna Gottardi and
                  Antonio Norelli and
                  Anu Venkatesh and
                  Arash Gholamidavoodi and
                  Arfa Tabassum and
                  Arul Menezes and
                  Arun Kirubarajan and
                  Asher Mullokandov and
                  Ashish Sabharwal and
                  Austin Herrick and
                  Avia Efrat and
                  Aykut Erdem and
                  Ayla Karakas and
                  et al.},
  title        = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  journal      = {arXiv preprint},
  volume       = {abs/2206.04615},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2206.04615},
  doi          = {10.48550/arXiv.2206.04615}
}

@article{arXiv2023_Dataset-BOLAA,
  author       = {Zhiwei Liu and
                  Weiran Yao and
                  Jianguo Zhang and
                  Le Xue and
                  Shelby Heinecke and
                  Rithesh Murthy and
                  Yihao Feng and
                  Zeyuan Chen and
                  Juan Carlos Niebles and
                  Devansh Arpit and
                  Ran Xu and
                  Phil Mui and
                  Huan Wang and
                  Caiming Xiong and
                  Silvio Savarese},
  title        = {{BOLAA:} Benchmarking and Orchestrating LLM-augmented Autonomous Agents},
  journal      = {CoRR},
  volume       = {abs/2308.05960},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.05960},
  doi          = {10.48550/arXiv.2308.05960}
}


-----Experiments-----
@misc{PGN,
  author       = {fsmosca},
  title        = {pgn-standard},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  url          = {https://github.com/fsmosca/PGN-Standard}
}

@misc{ChatGPT-OpenAI,
  title        = {ChatGPT: Optimizing Language Models for Dialogue},
  note         = {\url{https://openai.com/blog/chatgpt/}},
  author       = {OpenAI},
  year         = {2022}
}

@inproceedings{NeurIPS2022_InstructGPT,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html}
}

@article{arXiv2023_LLaMA,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.13971},
  doi          = {10.48550/ARXIV.2302.13971}
}

@article{arXiv2023_Qwen,
  author       = {Jinze Bai and
                  Shuai Bai and
                  Yunfei Chu and
                  Zeyu Cui and
                  Kai Dang and
                  Xiaodong Deng and
                  Yang Fan and
                  Wenbin Ge and
                  Yu Han and
                  Fei Huang and
                  Binyuan Hui and
                  Luo Ji and
                  Mei Li and
                  Junyang Lin and
                  Runji Lin and
                  Dayiheng Liu and
                  Gao Liu and
                  Chengqiang Lu and
                  Keming Lu and
                  Jianxin Ma and
                  Rui Men and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Chuanqi Tan and
                  Sinan Tan and
                  Jianhong Tu and
                  Peng Wang and
                  Shijie Wang and
                  Wei Wang and
                  Shengguang Wu and
                  Benfeng Xu and
                  Jin Xu and
                  An Yang and
                  Hao Yang and
                  Jian Yang and
                  Shusheng Yang and
                  Yang Yao and
                  Bowen Yu and
                  Hongyi Yuan and
                  Zheng Yuan and
                  Jianwei Zhang and
                  Xingxuan Zhang and
                  Yichang Zhang and
                  Zhenru Zhang and
                  Chang Zhou and
                  Jingren Zhou and
                  Xiaohuan Zhou and
                  Tianhang Zhu},
  title        = {Qwen Technical Report},
  journal      = {CoRR},
  volume       = {abs/2309.16609},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.16609},
  doi          = {10.48550/ARXIV.2309.16609}
}

@article{arXiv2023_Mistral,
  author       = {Albert Q. Jiang and
                  Alexandre Sablayrolles and
                  Arthur Mensch and
                  Chris Bamford and
                  Devendra Singh Chaplot and
                  Diego de Las Casas and
                  Florian Bressand and
                  Gianna Lengyel and
                  Guillaume Lample and
                  Lucile Saulnier and
                  L{\'{e}}lio Renard Lavaud and
                  Marie{-}Anne Lachaux and
                  Pierre Stock and
                  Teven Le Scao and
                  Thibaut Lavril and
                  Thomas Wang and
                  Timoth{\'{e}}e Lacroix and
                  William El Sayed},
  title        = {Mistral 7B},
  journal      = {CoRR},
  volume       = {abs/2310.06825},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.06825},
  doi          = {10.48550/ARXIV.2310.06825}
}

@article{arXiv2024_Mixtral,
  author       = {Albert Q. Jiang and 
                  Alexandre Sablayrolles and 
                  Antoine Roux and 
                  Arthur Mensch and 
                  Blanche Savary and 
                  Chris Bamford and 
                  Devendra Singh Chaplot and 
                  Diego de las Casas and 
                  Emma Bou Hanna and 
                  Florian Bressand and 
                  Gianna Lengyel and 
                  Guillaume Bour and 
                  Guillaume Lample and 
                  Lélio Renard Lavaud and 
                  Lucile Saulnier and 
                  Marie-Anne Lachaux and 
                  Pierre Stock and 
                  Sandeep Subramanian and 
                  Sophia Yang and 
                  Szymon Antoniak and 
                  Teven Le Scao and 
                  Théophile Gervet and 
                  Thibaut Lavril and 
                  Thomas Wang and 
                  Timothée Lacroix and 
                  William El Sayed},
  title        = {Mixtral of Experts},
  journal      = {CoRR},
  volume       = {abs/2401.04088},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.04088}
}

@article{J2023_Prompt-Survey,
  author    = {Pengfei Liu and
               Weizhe Yuan and
               Jinlan Fu and
               Zhengbao Jiang and
               Hiroaki Hayashi and
               Graham Neubig},
  title     = {Pre-train, Prompt, and Predict: {A} Systematic Survey of Prompting Methods in Natural Language Processing},
  volume    = {55},
  number    = {9},
  journal   = {ACM Comput. Surv.},
  numpages  = {35},
  publisher = {Association for Computing Machinery},
  issn      = {0360-0300},
  year      = {2023},
  url       = {https://doi.org/10.1145/3560815},
  doi       = {10.1145/3560815}
}

@inproceedings{WWW2022_KnowPrompt,
  author       = {Xiang Chen and
                  Ningyu Zhang and
                  Xin Xie and
                  Shumin Deng and
                  Yunzhi Yao and
                  Chuanqi Tan and
                  Fei Huang and
                  Luo Si and
                  Huajun Chen},
  title        = {KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction},
  booktitle    = {{WWW}},
  pages        = {2778--2788},
  publisher    = {{ACM}},
  year         = {2022},
  url          = {https://doi.org/10.1145/3485447.3511998},
  doi          = {10.1145/3485447.3511998}
}

@inproceedings{ACL2022-Short_P-Tuning,
  author       = {Xiao Liu and
                  Kaixuan Ji and
                  Yicheng Fu and
                  Weng Tam and
                  Zhengxiao Du and
                  Zhilin Yang and
                  Jie Tang},
  title        = {P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},
  booktitle    = {{ACL} {(2)}},
  pages        = {61--68},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.acl-short.8},
  doi          = {10.18653/v1/2022.acl-short.8}
}

@article{arXiv2024_MoreAgents,
  author       = {Junyou Li and 
                  Qin Zhang and 
                  Yangbin Yu and 
                  Qiang Fu and 
                  Deheng Ye},
  title        = {More Agents Is All You Need},
  journal      = {CoRR},
  volume       = {abs/2402.05120},
  year         = {2024}
}

@article{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}

@inproceedings{shirzad2023exphormer,
  title={Exphormer: Sparse transformers for graphs},
  author={Shirzad, Hamed and Velingker, Ameya and Venkatachalam, Balaji and Sutherland, Danica J and Sinop, Ali Kemal},
  booktitle={International Conference on Machine Learning},
  pages={31613--31632},
  year={2023},
  organization={PMLR}
}


@inproceedings{tan2023virtual,
  title={Virtual node tuning for few-shot node classification},
  author={Tan, Zhen and Guo, Ruocheng and Ding, Kaize and Liu, Huan},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2177--2188},
  year={2023}
}

@inproceedings{zhao2024causality,
  title={Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks},
  author={Zhao, Kesen and Zhang, Liang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{chen2024internet,
  title={Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence},
  author={Chen, Weize and You, Ziming and Li, Ran and Guan, Yitong and Qian, Chen and Zhao, Chenyang and Yang, Cheng and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2407.07061},
  year={2024}
}

@article{rosenbluth2024distinguished,
  title={Distinguished In Uniform: Self Attention Vs. Virtual Nodes},
  author={Rosenbluth, Eran and T{\"o}nshoff, Jan and Ritzert, Martin and Kisin, Berke and Grohe, Martin},
  journal={arXiv preprint arXiv:2405.11951},
  year={2024}
}

@inproceedings{ICLR2023_Self-Consistency,
  author       = {Xuezhi Wang and
                  Jason Wei and
                  Dale Schuurmans and
                  Quoc V. Le and
                  Ed H. Chi and
                  Sharan Narang and
                  Aakanksha Chowdhery and
                  Denny Zhou},
  title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=1PL1NIMMrw}
}

@inproceedings{ACL2023-Findings_Eval-LLM-Behavior,
  author       = {Ethan Perez and
                  Sam Ringer and
                  Kamile Lukosiute and
                  Karina Nguyen and
                  Edwin Chen and
                  Scott Heiner and
                  Craig Pettit and
                  Catherine Olsson and
                  Sandipan Kundu and
                  Saurav Kadavath and
                  Andy Jones and
                  Anna Chen and
                  Benjamin Mann and
                  Brian Israel and
                  Bryan Seethor and
                  Cameron McKinnon and
                  Christopher Olah and
                  Da Yan and
                  Daniela Amodei and
                  Dario Amodei and
                  Dawn Drain and
                  Dustin Li and
                  Eli Tran{-}Johnson and
                  Guro Khundadze and
                  Jackson Kernion and
                  James Landis and
                  Jamie Kerr and
                  Jared Mueller and
                  Jeeyoon Hyun and
                  Joshua Landau and
                  Kamal Ndousse and
                  Landon Goldberg and
                  Liane Lovitt and
                  Martin Lucas and
                  Michael Sellitto and
                  Miranda Zhang and
                  Neerav Kingsland and
                  Nelson Elhage and
                  Nicholas Joseph and
                  Noem{\'{\i}} Mercado and
                  Nova DasSarma and
                  Oliver Rausch and
                  Robin Larson and
                  Sam McCandlish and
                  Scott Johnston and
                  Shauna Kravec and
                  Sheer El Showk and
                  Tamera Lanham and
                  Timothy Telleen{-}Lawton and
                  Tom Brown and
                  Tom Henighan and
                  Tristan Hume and
                  Yuntao Bai and
                  Zac Hatfield{-}Dodds and
                  Jack Clark and
                  Samuel R. Bowman and
                  Amanda Askell and
                  Roger Grosse and
                  Danny Hernandez and
                  Deep Ganguli and
                  Evan Hubinger and
                  Nicholas Schiefer and
                  Jared Kaplan},
  title        = {Discovering Language Model Behaviors with Model-Written Evaluations},
  booktitle    = {{ACL} (Findings)},
  pages        = {13387--13434},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-acl.847},
  doi          = {10.18653/V1/2023.FINDINGS-ACL.847}
}

@article{arXiv2023_Analyze-LLM-Behavior,
  author       = {Lingjiao Chen and
                  Matei Zaharia and
                  James Zou},
  title        = {How is ChatGPT's behavior changing over time?},
  journal      = {CoRR},
  volume       = {abs/2307.09009},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09009},
  doi          = {10.48550/ARXIV.2307.09009}
}

@article{arXiv2024_AntEval,
  author       = {Yuanzhi Liang and 
                  Linchao Zhu and 
                  Yi Yang},
  title        = {AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions},
  journal      = {CoRR},
  volume       = {abs/2401.06509},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.06509}
}

@inproceedings{ICLR2024_LLM-Bias-MCS,
  author       = {Chujie Zheng and
                  Hao Zhou and
                  Fandong Meng and
                  Jie Zhou and
                  Minlie Huang},
  title        = {Large Language Models Are Not Robust Multiple Choice Selectors},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=shr9PXz7T0}
}

-----Majority Vote-----
@article{Science2022_AlphaCode,
  author       = {Yujia Li and
                  David H. Choi and
                  Junyoung Chung and
                  Nate Kushman and
                  Julian Schrittwieser and
                  R{\'{e}}mi Leblond and
                  Tom Eccles and
                  James Keeling and
                  Felix Gimeno and
                  Agustin Dal Lago and
                  Thomas Hubert and
                  Peter Choy and
                  Cyprien de Masson d'Autume and
                  Igor Babuschkin and
                  Xinyun Chen and
                  Po{-}Sen Huang and
                  Johannes Welbl and
                  Sven Gowal and
                  Alexey Cherepanov and
                  James Molloy and
                  Daniel J. Mankowitz and
                  Esme Sutherland Robson and
                  Pushmeet Kohli and
                  Nando de Freitas and
                  Koray Kavukcuoglu and
                  Oriol Vinyals},
  title        = {Competition-level code generation with AlphaCode},
  journal      = {Science},
  volume       = {378},
  number       = {6624},
  pages        = {1092-1097},
  year         = {2022},
  url          = {https://www.science.org/doi/abs/10.1126/science.abq1158},
  doi          = {10.1126/science.abq1158}
}
arXiv2022_AlphaCode

@inproceedings{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={International Conference on Machine Learning},
  pages={22137--22176},
  year={2023},
  organization={PMLR}
}


@inproceedings{
shridhar2021alfworld,
title={{\{}ALFW{\}}orld: Aligning Text and Embodied Environments for Interactive Learning},
author={Mohit Shridhar and Xingdi Yuan and Marc-Alexandre Cote and Yonatan Bisk and Adam Trischler and Matthew Hausknecht},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0IOX0YcCdTn}
}

@article{austin2021mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{gsm8k,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {arXiv prepring},
  volume       = {abs/2110.14168},
  year         = {2021}
}

@article{roy2016solving,
  title={Solving general arithmetic word problems},
  author={Roy, Subhro and Roth, Dan},
  journal={arXiv preprint arXiv:1608.01413},
  year={2016}
}

@inproceedings{fu2022complexity,
  title={Complexity-based prompting for multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{patel2021nlp,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  journal={arXiv preprint arXiv:2103.07191},
  year={2021}
}

@article{arXiv2023_ReConcile,
  author      = {Justin Chih-Yao Chen and 
                 Swarnadeep Saha and 
                 Mohit Bansal},
  title       = {ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs}, 
  journal     = {arxiv preprint},
  volume      = {2309.07864}, 
  eprint      = {2309.13007},
  year        = {2023},
  url         = {https://arxiv.org/abs/2309.13007}
}

@article{arXiv2023_Multi-Agent-Consensus,
  author       = {Huaben Chen and
                  Wenkang Ji and
                  Lufeng Xu and
                  Shiyu Zhao},
  title        = {Multi-Agent Consensus Seeking via Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2310.20151},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.20151},
  doi          = {10.48550/ARXIV.2310.20151}
}




-----Applications-----
@article{arXiv2023_LLMs-Simulate_SocialMedia,
  author       = {Petter T{\"{o}}rnberg and
                  Diliara Valeeva and
                  Justus Uitermark and
                  Christopher Bail},
  title        = {Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms},
  journal      = {CoRR},
  volume       = {abs/2310.05984},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.05984},
  doi          = {10.48550/ARXIV.2310.05984}
}



----Others----
@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    publisher = {American Psychological Association},
    address = {Washington, DC},
    year    = {1983}
}

@article{J1981_Alternation,
  author       = {Ashok K. Chandra and
                  Dexter Kozen and
                  Larry J. Stockmeyer},
  title        = {Alternation},
  journal      = {J. {ACM}},
  volume       = {28},
  number       = {1},
  pages        = {114--133},
  year         = {1981},
  url          = {https://doi.org/10.1145/322234.322243},
  doi          = {10.1145/322234.322243}
}

@inproceedings{ICML2007_L1Regular,
  author       = {Galen Andrew and
                  Jianfeng Gao},
  title        = {Scalable training of L\({}^{\mbox{1}}\)-regularized log-linear models},
  booktitle    = {{ICML}},
  series       = {{ACM} International Conference Proceeding Series},
  volume       = {227},
  pages        = {33--40},
  publisher    = {{ACM}},
  year         = {2007},
  url          = {https://doi.org/10.1145/1273496.1273501},
  doi          = {10.1145/1273496.1273501}
}

@book{Book1997_Algorithms-DataStruct,
  author       = {Dan Gusfield},
  title        = {Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology},
  publisher    = {Cambridge University Press},
  year         = {1997},
  url          = {https://doi.org/10.1017/cbo9780511574931},
  doi          = {10.1017/CBO9780511574931},
  isbn         = {0-521-58519-8}
}

@article{arXiv2015_YaraParser,
  author       = {Mohammad Sadegh Rasooli and
                  Joel R. Tetreault},
  title        = {Yara Parser: {A} Fast and Accurate Dependency Parser},
  journal      = {CoRR},
  volume       = {abs/1503.06733},
  year         = {2015},
  url          = {http://arxiv.org/abs/1503.06733}
}

@article{JMLR2005_LearningStruct,
  author       = {Rie Kubota Ando and
                  Tong Zhang},
  title        = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
  journal      = {J. Mach. Learn. Res.},
  volume       = {6},
  pages        = {1817--1853},
  year         = {2005},
  url          = {http://jmlr.org/papers/v6/ando05a.html}
}

@article{J1965_FourierSeries-Comp,
  author       = {Cooley, James W. and 
                  Tukey, John W.},
  title        = {An algorithm for the machine calculation of complex {F}ourier series},
  journal      = {Mathematics of Computation},
  volume       = {19},
  number       = {90},
  pages        = {297--301},
  year         = {1965},
  url          = {https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@article{arXiv2023_Agent-DUMA,
  author       = {Xiaoyu Tian and
                  Liangyu Chen and
                  Na Liu and
                  Yaxuan Liu and
                  Wei Zou and
                  Kaijiang Chen and
                  Ming Cui},
  title        = {{DUMA:} a Dual-Mind Conversational Agent with Fast and Slow Thinking},
  journal      = {CoRR},
  volume       = {abs/2310.18075},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.18075},
  doi          = {10.48550/ARXIV.2310.18075}
}

@article{arXiv2023_AgentTuning,
  author       = {Aohan Zeng and
                  Mingdao Liu and
                  Rui Lu and
                  Bowen Wang and
                  Xiao Liu and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {AgentTuning: Enabling Generalized Agent Abilities for LLMs},
  journal      = {CoRR},
  volume       = {abs/2310.12823},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.12823},
  doi          = {10.48550/ARXIV.2310.12823}
}

@article{arXiv2023_FireAct,
  author       = {Baian Chen and
                  Chang Shu and
                  Ehsan Shareghi and
                  Nigel Collier and
                  Karthik Narasimhan and
                  Shunyu Yao},
  title        = {FireAct: Toward Language Agent Fine-tuning},
  journal      = {CoRR},
  volume       = {abs/2310.05915},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.05915},
  doi          = {10.48550/ARXIV.2310.05915}
}

@article{agashe2023evaluating,
  title={Evaluating multi-agent coordination abilities in large language models},
  author={Agashe, Saaket and Fan, Yue and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2310.03903},
  year={2023}
}



@inproceedings{NAACL2024_Agent-Self-Collaboration,
  author       = {Zhenhailong Wang and 
                  Shaoguang Mao and 
                  Wenshan Wu and 
                  Tao Ge and 
                  Furu Wei and 
                  Heng Ji},
  title        = {Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
  booktitle    = {{NAACL}},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}
arXiv2024_Agent-Self-Collaboration

@article{arXiv2023_Benchmarking-Agents,
  author       = {Qian Huang and
                  Jian Vora and
                  Percy Liang and
                  Jure Leskovec},
  title        = {Benchmarking Large Language Models As {AI} Research Agents},
  journal      = {CoRR},
  volume       = {abs/2310.03302},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.03302},
  doi          = {10.48550/ARXIV.2310.03302}
}

@article{arXiv2023_Agents-SampleEfficiency,
  author       = {Zhihan Liu and
                  Hao Hu and
                  Shenao Zhang and
                  Hongyi Guo and
                  Shuqi Ke and
                  Boyi Liu and
                  Zhaoran Wang},
  title        = {Reason for Future, Act for Now: {A} Principled Framework for Autonomous {LLM} Agents with Provable Sample Efficiency},
  journal      = {CoRR},
  volume       = {abs/2309.17382},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.17382},
  doi          = {10.48550/ARXIV.2309.17382}
}

@article{arXiv2023_UnifiedAgent-FM,
  author       = {Norman Di Palo and
                  Arunkumar Byravan and
                  Leonard Hasenclever and
                  Markus Wulfmeier and
                  Nicolas Heess and
                  Martin A. Riedmiller},
  title        = {Towards {A} Unified Agent with Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2307.09668},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09668},
  doi          = {10.48550/ARXIV.2307.09668}
}

@article{arXiv2023_Universal-Agent,
  author       = {Anees Aslam},
  title        = {Universal Language Modelling agent},
  journal      = {CoRR},
  volume       = {abs/2306.06521},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.06521},
  doi          = {10.48550/ARXIV.2306.06521}
}

@article{arXiv2023_Hinder-Agents,
  author       = {Sukai Huang and
                  Nir Lipovetzky and
                  Trevor Cohn},
  title        = {A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents},
  journal      = {CoRR},
  volume       = {abs/2305.16621},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.16621},
  doi          = {10.48550/ARXIV.2305.16621}
}

@inproceedings{CoLLAs2023_Autotelic-Agents,
  author       = {C{\'{e}}dric Colas and
                  Laetitia Teodorescu and
                  Pierre{-}Yves Oudeyer and
                  Xingdi Yuan and
                  Marc{-}Alexandre C{\^{o}}t{\'{e}}},
  title        = {Augmenting Autotelic Agents with Large Language Models},
  booktitle    = {CoLLAs},
  series       = {Proceedings of Machine Learning Research},
  volume       = {232},
  pages        = {205--226},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v232/colas23a.html}
}
arXiv2023_Autotelic-Agents

@inproceedings{EMNLP2023_ConceptualStructure_LLM,
  author       = {Siddharth Suresh and
                  Kushin Mukherjee and
                  Xizheng Yu and
                  Wei{-}Chun Huang and
                  Lisa Padua and
                  Timothy T. Rogers},
  title        = {Conceptual structure coheres in human cognition but not in large language models},
  booktitle    = {{EMNLP}},
  pages        = {722--738},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.47}
}

@article{Games2021_SocialStrategies-CooperativeBehaviour,
  author       = {Robin Watson and
                  Thomas J. H. Morgan and
                  Rachel L. Kendal and
                  Julie Van de Vyver and
                  Jeremy Kendal},
  title        = {Social Learning Strategies and Cooperative Behaviour: Evidence of Payoff Bias, but Not Prestige or Conformity, in a Social Dilemma Game},
  journal      = {Games},
  volume       = {12},
  number       = {4},
  pages        = {89},
  year         = {2021},
  url          = {https://doi.org/10.3390/g12040089},
  doi          = {10.3390/G12040089}
}

@inproceedings{ACL2024_AUTOACT-Self-Planning,
  author       = {Shuofei Qiao and 
                  Ningyu Zhang and 
                  Runnan Fang and 
                  Yujie Luo and 
                  Wangchunshu Zhou and 
                  Yuchen Eleanor Jiang and 
                  Chengfei Lv and 
                  Huajun Chen},
  title        = {AUTOACT: Automatic Agent Learning from Scratch via Self-Planning},
  booktitle    = {{ACL}},
  pages        = {},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.05268}
}
arXiv2024_AUTOACT-Self-Planning

@article{arXiv2024_Self-Rewarding-LMs,
  author       = {Weizhe Yuan and 
                  Richard Yuanzhe Pang and 
                  Kyunghyun Cho and 
                  Sainbayar Sukhbaatar and 
                  Jing Xu and 
                  Jason Weston},
  title        = {Self-Rewarding Language Models},
  journal      = {CoRR},
  volume       = {abs/2401.10020},
  year         = {2024},
  url          = {https://arxiv.org/abs/2401.10020}
}


@article{serrano2003topology,
  title={Topology of the world trade web},
  author={Serrano, Ma Angeles and Bogun{\'a}, Mari{\'a}n},
  journal={Physical Review E},
  volume={68},
  number={1},
  pages={015101},
  year={2003},
  publisher={APS}
}

@article{fagiolo2010evolution,
  title={The evolution of the world trade web: a weighted-network analysis},
  author={Fagiolo, Giorgio and Reyes, Javier and Schiavo, Stefano},
  journal={Journal of Evolutionary Economics},
  volume={20},
  pages={479--514},
  year={2010},
  publisher={Springer}
}

@article{garlaschelli2007interplay,
  title={Interplay between topology and dynamics in the World Trade Web},
  author={Garlaschelli, Diego and Di Matteo, Ticiana and Aste, Tomaso and Caldarelli, Guido and Loffredo, Maria I},
  journal={The European Physical Journal B},
  volume={57},
  pages={159--164},
  year={2007},
  publisher={Springer}
}

@inproceedings{pbft,
author = {Castro, Miguel and Liskov, Barbara},
title = {Practical Byzantine Fault Tolerance},
year = {1999},
isbn = {1880446391},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the Third Symposium on Operating Systems Design and Implementation},
pages = {173–186},
numpages = {14},
location = {New Orleans, Louisiana, USA},
series = {OSDI '99}
}


@article{farahani2013review,
  title={A review of urban transportation network design problems},
  author={Farahani, Reza Zanjirani and Miandoabchi, Elnaz and Szeto, Wai Yuen and Rashidi, Hannaneh},
  journal={European journal of operational research},
  volume={229},
  number={2},
  pages={281--302},
  year={2013},
  publisher={Elsevier}
}


@article{bell1997transportation,
  title={Transportation network analysis},
  author={Bell, Michael GH and Iida, Yasunori and others},
  year={1997},
  publisher={Wiley Online Library}
}

@inproceedings{fan2019graph,
  title={Graph neural networks for social recommendation},
  author={Fan, Wenqi and Ma, Yao and Li, Qing and He, Yuan and Zhao, Eric and Tang, Jiliang and Yin, Dawei},
  booktitle={The world wide web conference},
  pages={417--426},
  year={2019}
}

@article{belkin2006manifold,
  title={Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.},
  author={Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
  journal={Journal of machine learning research},
  volume={7},
  number={11},
  year={2006}
}

@inproceedings{zhou2005learning,
  title={Learning from labeled and unlabeled data on a directed graph},
  author={Zhou, Dengyong and Huang, Jiayuan and Sch{\"o}lkopf, Bernhard},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={1036--1043},
  year={2005}
}

@inproceedings{wang2019kgat,
  title={Kgat: Knowledge graph attention network for recommendation},
  author={Wang, Xiang and He, Xiangnan and Cao, Yixin and Liu, Meng and Chua, Tat-Seng},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={950--958},
  year={2019}
}

@article{nabti2017querying,
  title={Querying massive graph data: A compress and search approach},
  author={Nabti, Chemseddine and Seba, Hamida},
  journal={Future Generation Computer Systems},
  volume={74},
  pages={63--75},
  year={2017},
  publisher={Elsevier}
}

@article{ribeiro2021survey,
  title={A survey on subgraph counting: concepts, algorithms, and applications to network motifs and graphlets},
  author={Ribeiro, Pedro and Paredes, Pedro and Silva, Miguel EP and Aparicio, David and Silva, Fernando},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{bouhenni2021survey,
  title={A survey on distributed graph pattern matching in massive graphs},
  author={Bouhenni, Sarra and Yahiaoui, Said and Nouali-Taboudjemat, Nadia and Kheddouci, Hamamache},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{fulber2020network,
  title={Network service topology: Formalization, taxonomy and the custom specification model},
  author={Fulber-Garcia, Vinicius and Duarte Jr, Elias P and Huff, Alexandre and dos Santos, Carlos RP},
  journal={Computer Networks},
  volume={178},
  pages={107337},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{arnold2020cloud,
  title={Cloud provider connectivity in the flat internet},
  author={Arnold, Todd and He, Jia and Jiang, Weifan and Calder, Matt and Cunha, Italo and Giotsas, Vasileios and Katz-Bassett, Ethan},
  booktitle={Proceedings of the ACM Internet Measurement Conference},
  pages={230--246},
  year={2020}
}


@article{mialon2023gaiabenchmark,
  title={Gaia: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journal={arXiv preprint arXiv:2311.12983},
  year={2023}
}


@article{fu2020topology,
  title={Topology optimization against cascading failures on wireless sensor networks using a memetic algorithm},
  author={Fu, Xiuwen and Pace, Pasquale and Aloi, Gianluca and Yang, Lin and Fortino, Giancarlo},
  journal={Computer Networks},
  volume={177},
  pages={107327},
  year={2020},
  publisher={Elsevier}
}

@article{zhu2024llms,
  title={Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities},
  author={Zhu, Yuqi and Wang, Xiaohan and Chen, Jing and Qiao, Shuofei and Ou, Yixin and Yao, Yunzhi and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={World Wide Web},
  volume={27},
  number={5},
  pages={58},
  year={2024},
  publisher={Springer}
}

@article{zhu2022multi,
  title={Multi-modal knowledge graph construction and application: A survey},
  author={Zhu, Xiangru and Li, Zhixu and Wang, Xiaodan and Jiang, Xueyao and Sun, Penglei and Wang, Xuwu and Xiao, Yanghua and Yuan, Nicholas Jing},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={36},
  number={2},
  pages={715--735},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zhu2021network,
  title={Network planning with deep reinforcement learning},
  author={Zhu, Hang and Gupta, Varun and Ahuja, Satyajeet Singh and Tian, Yuandong and Zhang, Ying and Jin, Xin},
  booktitle={Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
  pages={258--271},
  year={2021}
}




@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}


@article{sun2023pearl,
  title={Pearl: Prompting large language models to plan and execute actions over long documents},
  author={Sun, Simeng and Liu, Yang and Wang, Shuohang and Zhu, Chenguang and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2305.14564},
  year={2023}
}

@inproceedings{ruan2023tptu,
  title={Tptu: Task planning and tool usage of large language model-based ai agents},
  author={Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Mao, Hangyu and Li, Ziyue and Zeng, Xingyu and Zhao, Rui and others},
  booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},
  year={2023}
}

@article{sun2023self,
  title={Self-supervised hypergraph representation learning for sociological analysis},
  author={Sun, Xiangguo and Cheng, Hong and Liu, Bo and Li, Jia and Chen, Hongyang and Xu, Guandong and Yin, Hongzhi},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={11},
  pages={11860--11871},
  year={2023},
  publisher={IEEE}
}

@article{li2023survey,
  title={A survey of graph meets large language model: Progress and future directions},
  author={Li, Yuhan and Li, Zhixun and Wang, Peisong and Li, Jia and Sun, Xiangguo and Cheng, Hong and Yu, Jeffrey Xu},
  journal={arXiv preprint arXiv:2311.12399},
  year={2023}
}

@inproceedings{sun2023all,
  title={All in One: Multi-Task Prompting for Graph Neural Networks},
  author={Sun, Xiangguo and Cheng, Hong and Li, Jia and Liu, Bo and Guan, Jihong},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2120--2131},
  year={2023}
}

@article{shu2024llm,
  title={When llm meets hypergraph: A sociological analysis on personality via online social networks},
  author={Shu, Zhiyao and Sun, Xiangguo and Cheng, Hong},
  journal={arXiv preprint arXiv:2407.03568},
  year={2024}
}