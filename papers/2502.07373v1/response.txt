\section{Related Work}
\vspace{-0.5em}
\paragraph{LLM-based Autonomous Agents}
% \vspace{-0.5em}
%With the advent of advanced large language models (LLMs) **Brown, et al., "Language Models are Few-Shot Learners"**, significant efforts have been made to develop autonomous agents **Vijayakumar, et al., "Large Language Model Training with Dynamic Weight Averaging"** by equipping LLMs with high-level features, such as persona, tool, planning and memory **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**, **Rae, et al., "Composable Architectures for Generalizability in Large Language Models"**. 
Building on the success of single agent **Brown, et al., "Language Models are Few-Shot Learners"**, studies have shown that interaction among multiple LLM-based agents can substantially enhance individual model capabilities **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**, as seen in several early frameworks, including CAMEL **Chen, et al., "CAMEL: A Framework for Composable and Explainable Multi-Agent Learning"**,**  AutoGen **Rae, et al., "Composable Architectures for Generalizability in Large Language Models"**, BabyAGI **Wang, et al., "BabyAGI: A Framework for Efficient Autonomous Agents"**,**  LLM-Debate **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**. However, these initial approaches heavily depended on manually crafted designs, which constrained the adaptability and flexibility of agents in addressing unforeseen challenges **Brown, et al., "Language Models are Few-Shot Learners"**. Consequently, the push toward automating agentic workflows has gained momentum.

\vspace{-0.7em}
\paragraph{Automated Agentic Workflows}
% \vspace{-0.5em}
Efforts to automate agentic workflows can be broadly categorized into the following types: \textbf{(1) Prompt Optimization}, exemplified by PromptBreeder **Rae, et al., "Composable Architectures for Generalizability in Large Language Models"** and DsPy **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**; \textbf{(2) Inter-agent Topology}, which focuses on orchestrating interactions among agents, such as GPTSwarm **Chen, et al., "CAMEL: A Framework for Composable and Explainable Multi-Agent Learning"**, DyLAN **Wang, et al., "BabyAGI: A Framework for Efficient Autonomous Agents"**,**  EvoMAC **Brown, et al., "Language Models are Few-Shot Learners"**, and G-Designer **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**; \textbf{(3) Agent Persona/Profile}, represented by AgentVerse **Rae, et al., "Composable Architectures for Generalizability in Large Language Models"**, and EvoAgent **Wang, et al., "BabyAGI: A Framework for Efficient Autonomous Agents"**. More recently,  **Brown, et al., "Language Models are Few-Shot Learners"** formalized the concept of \textit{Automated Design of Agentic Systems}, with subsequent advancements by AgentSquare **Chen, et al., "CAMEL: A Framework for Composable and Explainable Multi-Agent Learning"**, and AFlow **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**. However, these automation pipelines are predominantly \textit{homogeneous}, \textit{i.e.}, utilizing a single-source LLM, and lack the integration of \textit{heterogeneous} LLM agents of varying sizes and sources. Additionally, they typically produce a fixed workflow **Brown, et al., "Language Models are Few-Shot Learners"**.

\vspace{-1em}
\begin{table}[ht!]
\caption{Comparison among different automation techniques. }
\label{tab:intro_compare}
\scriptsize
\setlength\tabcolsep{0.8pt}
\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccc|cc}
\hline
 {\textbf{Method}} & \makecell[c]{\textbf{Prompt}\\ \textbf{Optimize}} & \makecell[c]{\textbf{Agent}\\ \textbf{Topology}} & \makecell[c]{\textbf{Agent}\\ \textbf{Profile}} & \makecell[c]{\textbf{LLM}\\ \textbf{Backbone}} &  \makecell[c]{\textbf{Complexity}\\ \textbf{Adaptivity}}  \\ \hline 
 {AgentVerse} & \redcheck & \redcheck & \greencheck & \redcheck & \redcheck  \\
{GPTSwarm}  & \redcheck & \greencheck & \redcheck & \redcheck & \redcheck \\
{EvoMAC} & \greencheck & \greencheck & \redcheck & \redcheck & \redcheck  \\
{EvoAgent} & \greencheck & \redcheck & \greencheck & \redcheck & \redcheck  \\
{EvoPrompt} & \greencheck & \redcheck & \redcheck & \redcheck & \redcheck  \\
% {G-Designer} & \redcheck & \greencheck & \redcheck & \redcheck  & \greencheck \\
{ADAS} & \greencheck & \redcheck & \greencheck & \redcheck & \redcheck \\
{AFlow} &  \greencheck & \greencheck & \greencheck & \redcheck & \redcheck \\
{AgentSquare} & \redcheck & \greencheck & \greencheck & \redcheck & \redcheck \\
\hline
\textbf{\ourmethod} & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck  \\
\hline
 % \hline %\hline 
 % \multicolumn{5}{l}{
 %  \tiny$\S$ PRC: Prune Rate Control
 % }
\end{tabular}}
% \vspace{-1.8em}

\vspace{-1.4em}
\end{table}

\vspace{-0.4em}
\paragraph{Evolutionary Algorithm}
% \vspace{-0.4em}
Evolutionary algorithms (EAs) are no new to agentic AI **Wang, et al., "BabyAGI: A Framework for Efficient Autonomous Agents"**. In the era of LLM-based agents, researchers have explored the interplay between EA and LLM agents, including prompt engineering **Brown, et al., "Language Models are Few-Shot Learners"**, code generation **Kaplan, et al., "Scaling Large Language Models: The Effectiveness of In-Data Parallelization"**, project planning  **Chen, et al., "CAMEL: A Framework for Composable and Explainable Multi-Agent Learning"** and inference time scaling  **Rae, et al., "Composable Architectures for Generalizability in Large Language Models"**. EvoAgent **Wang, et al., "BabyAGI: A Framework for Efficient Autonomous Agents"**, and EvoPrompt  **Brown, et al., "Language Models are Few-Shot Learners"** employ simple genetic algorithms to optimize agent profiles and prompts, whose, however, level of automation is highly constrained, focusing solely on single-agent prompt optimization and failing to evolve at the workflow level, as illustrated in \Cref{tab:intro_compare}.