\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figure/example}
    \caption{(a) Existing medical image segmentation methods: Performance evaluation on unseen modalities is challenging, often requiring retraining and hyperparameter optimization for each unseen modality dataset, which incurs substantial costs. (b) Schematic diagram of the proposed TTMG framework for unseen modality generalizable medical image segmentation: When an arbitrary unseen modality instance is input, TTMG projects the misaligned, unseen modality features into a well-aligned, seen modality feature distribution using modality-specific style bases, guided by the probability of the instance belonging to each modality \textbf{without additional training}.}
    \label{fig:unseen_modality_generalization}
\end{figure}

Medical image segmentation has been considered a core technology for the early detection of cancer and abnormal tissues that significantly impact human life \cite{coates2015tailoring}. Therefore, medical experts have widely employed traditional automatic segmentation algorithms \cite{otsu1979threshold, kass1988snakes, tizhoosh2005image, haralick1987image}. However, these algorithms are vulnerable to various artifacts and complex morphological structures in medical images, making them hard to generalize to new patients’ cases \cite{riccio2018new}. Consequently, these problems have weakened the reliability of computer-based diagnostic analysis and hindered the development of digital healthcare, which is the leading future industry field \cite{park2019treatment, gaube2021ai}.

Due to advancements in deep learning \cite{krizhevsky2012imagenet, he2016deep, dosovitskiy2021an, wang2022pvt, han2022vision}, recent medical image segmentation models have focused on specific types of medical images, including polyp \cite{fan2020pranet, zhao2021automatic, hu2023ppnet, liu2024cafe}, skin cancer \cite{sarker2018slsdeep, he2023co, din2024lscs}, breast tumor \cite{qi2023mdf, he2024multi}, or lung infection segmentation \cite{fan2020inf, liu2024automated}. These segmentation tasks are often tied to medical imaging modalities, including colonoscopy, dermoscopy, ultrasound, radiology, etc \cite{ma2024segment, guo2019deep}. However, this approach typically necessitates training separate models for each segmentation task, posing significant practical challenges and incurring high costs related to dataset curation, model retraining, and hyperparameter tuning \cite{zhang2022understanding, chen2024versatile}. While some studies \cite{zhao2023m, Rahman_2023_WACV, rahman2024g, nam2024modality} have aimed to improve generalization to unseen clinical settings, no research has explicitly focused on enhancing generalization to entirely unseen modalities—such as \textit{training on colonoscopy datasets and evaluating ultrasound datasets}—using a \textbf{single model}.

Generalizing across various segmentation tasks is particularly challenging due to the inherent heterogeneity of medical image characteristics, where different modalities exhibit diverse feature distributions, noise patterns, and resolution levels \cite{darzi2024review}. To tackle these challenges, we leverage foundational concepts from domain generalization techniques \cite{choi2021robustnet, huang2023style, zhou2023instance, zhou2024test}, which aim to improve generalization ability on arbitrary unseen domains. However, conventional domain generalization methods often struggle to capture complex anatomical structures and retain modality-invariant features in medical images, as they are primarily designed for visual domains with more uniform characteristics.

To address these challenging issues, we propose a novel technique called \textit{Modality-Aware Style Projection (MASP, Figure \ref{fig:TTMG}.(b))}, leveraging prototype learning by representing the style bases of each accessible modality during training as learnable parameters. Our approach begins by identifying the modality to which the input instance belongs and then projecting the instance into the most similar style space, as illustrated in Figure \ref{fig:unseen_modality_generalization}. While MASP effectively projects instances into the most similar style distribution among seen modalities, it does not mitigate feature covariance, which can lead to overfitting to specific modalities and ultimately hinder the model's ability to generalize effectively to unseen modalities \cite{li2021simple, zhang2024domain}. To address this limitation, we introduced \textit{Modality-Sensitive Instance Whitening (MSIW, Algorithm \ref{alg_MSIW})}, which identifies and removes modality-specific information while preserving modality-invariant features. MSIW  computes the covariance matrix variance between the original and the modality style-projected feature maps, whitening only the covariance matrix elements with high variance. The resulting \textbf{Test-Time Modality Generalization (TTMG)}, which integrates the MASP and MSIW, achieved the highest generalization performance for unseen modality generalizable medical image segmentation \textbf{without additional training}. The contributions of this paper can be summarized as follows:

To address these challenging issues, we propose a novel technique called \textit{Modality-Aware Style Projection (MASP, Figure \ref{fig:TTMG}.(b))}, leveraging prototype learning by representing the style bases of each accessible modality during training as learnable parameters. Our approach begins by identifying the modality to which the input instance belongs and then projecting the instance into the most similar style space, as illustrated in Figure \ref{fig:unseen_modality_generalization}. While MASP effectively projects instances into the most similar style distribution among seen modalities, it does not mitigate feature covariance, which can lead to overfitting to specific modalities and ultimately hinder the model's ability to generalize effectively to unseen modalities \cite{li2021simple, zhang2024domain}. To address this limitation, we introduced \textit{Modality-Sensitive Instance Whitening (MSIW, Algorithm \ref{alg_MSIW})}, which identifies and removes modality-specific information while preserving modality-invariant features. MSIW  computes the covariance matrix variance between the original and the modality style-projected feature maps, whitening only the covariance matrix elements with high variance. The resulting \textbf{Test-Time Modality Generalization (TTMG)}, which integrates the MASP and MSIW, achieved the highest generalization performance for unseen modality generalizable medical image segmentation \textbf{without additional training}. The contributions of this paper can be summarized as follows:

\begin{itemize}
    \item We propose a TTMG framework, mainly comprising MASP and MSIW, for unseen modality generalizable medical image segmentation without additional training. 
    
    \item MASP integrates modality classification modules to project the feature map of a test instance into the most similar style space among available modalities. Meanwhile, MSIW removes only modality-sensitive information while preserving modality-invariant features.
    
    \item We have achieved considerably higher performance than the existing DGSS technique by conducting experiments on a total of eleven datasets, including four modalities that are used as core diagnostic modalities in medical image segmentation.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/TTMG.png}
    \caption{(a) The overall architecture of the proposed framework, called \textbf{Test-Time Modality Generalization (TTMG)}, which mainly comprises \textit{MASP (Figure \ref{fig:TTMG}.(b))} and \textit{MSIW (Algorithm \ref{alg_MSIW})}. (b) MASP stage ($M = 3$ in this figure). (c) Notation description used in this paper.}
    \label{fig:TTMG}
\end{figure*}