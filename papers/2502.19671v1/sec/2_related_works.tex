\section{Related Works}

The advancements in deep learning have significantly influenced automatic medical image segmentation \cite{ronneberger2015u, chen2021transunet, zhao2023m, nam2024modality}. However, these models are inherently constrained by the unique challenges posed by the distinct characteristics of each modality, such as variations in artifacts and feature distributions. Consequently, these approaches require expensive retraining for each new modality, limiting their scalability and hindering their practical application. Although some studies \cite{zhou2022generalizable, bastico2023simple, gu2024train} have explored modality generalization within a specific segmentation task, such as brain tumor segmentation, generalization across different tasks remains largely unexplored. To address this unexplored task, we focus on domain generalization (DG) techniques, which aim to enhance generalization performance on arbitrary unseen domains. Recent DG approaches primarily focus on three strategies: 1) Instance Normalization (IN) \cite{ulyanov2017improved, luo2018differentiable, pan2018two}, 2) Instance Whitening (IW) \cite{huang2018decorrelated, pan2019switchable}, and 3) Style Projection \cite{zhou2023instance, huang2023style, ahn2024style}. IN and IW work by standardizing feature maps and decorrelating the covariance of feature maps to reduce the domain-specific variations and improve the model's robustness to unseen domains, respectively. Additionally, SAN-SAW \cite{peng2022semantic} and RobustNet \cite{choi2021robustnet} adopted the strategies of IN and IW, respectively, by selectively applying normalization and whitening: SAN-SAW performs IN only on semantically similar regions, while RobustNet identifies domain-specific information to apply partial whitening rather than whitening the whole feature map. Additionally, BlindNet \cite{ahn2024style}  mitigates style influence through covariance alignment in the encoder with semantic consistency contrastive learning in the decoder. Recently, SPCNet \cite{huang2023style}, IAIW \cite{zhou2023instance}, and TTDG \cite{ahn2024style} have learned style bases from the seen domain and projected unseen domain data onto these learned style bases, facilitating domain generalization at test time. Inspired by these approaches, we developed the TTMG framework for unseen modality generalizable medical image segmentation, integrating domain-specific style recognition algorithm (RobustNet) and prototype learning-based style projection (SPCNet). To the best of our knowledge, prior research has not addressed generalization across diverse modalities beyond specific clinical settings. Our framework bridges this gap by enabling robust performance on unseen modalities without costly retraining, improving scalability, and supporting practical implementation in real-world medical applications.