\section{Method}

In this section, we outline the problem setup (Section \ref{ss31_problem_setup}) for unseen modality generalizable medical image segmentation. Subsequently, we introduce a novel framework, called \textbf{Test-time Modality Generalization (TTMG)}, which integrates two main components: \textit{Modality-Aware Style Projection (MASP, Section \ref{ss32_modality_aware_style_projection})} and \textit{Modality-Sensitive Instance Whitening (MSIW, Section \ref{ss33_modality_sensitive_instance_whitening})}. Our novel framework is designed to improve generalization ability on arbitrary unseen modality segmentation tasks \textbf{without additional training} by projecting arbitrary unseen modality instances to seen modality style distribution. We summarized the technical novelty and social impact of TTMG in the Appendix.

\subsection{Problem Setup}
\label{ss31_problem_setup}
We assume the available of $M$ distinct source modality datasets, denoted as $\mathcal{S} = \{ S_{1}, S_{2}, \dots, S_{M} \}$. The $m$-th source modality dataset $S_{m}$ consists of image-label pairs $(x^{i}_{m}, y^{i}_{m})$, where $x^{i}_{m} \in \mathbb{R}^{H \times W \times C}$ is the image and $y^{i}_{m} \in \mathbb{R}^{H \times W \times K}$ is the corresponding pixel-wise label with $K$ classes in $m$-th source modality. Thus, $S_{m} = \{ (x^{i}_{m}, y^{i}_{m}) \}_{i = 1}^{N_{m}}$, where $N_{m}$ represents the number of samples in the $m$-th modality dataset. And, $(H, W, C)$ denotes the height, width, and number of channels of the input image, respectively. This paper aims to train a binary medical image segmentation model $\varphi_{\text{seg}}$ using a multi-source modality scheme ($M > 1$, $K = 1$) that can generalize to arbitrary unseen modality datasets, which are inaccessible during the training phase.

\subsection{Modality-Aware Style Projection}
\label{ss32_modality_aware_style_projection}

\noindent \textit{Motivation:} We observe that each modality dataset exhibits a distinct feature distribution (Figure \ref{fig:FeatureProjection}.(a)), significantly contributing to performance degradation in deep learning models \cite{gao2022towards, bayram2023domain}. This result is due to learned features often overfitting to the specific characteristics of the training modality, limiting their ability to transfer and generalize effectively to unseen modalities. However, if we can estimate the most likely modality of a test instance and effectively leverage this information, the model's generalization ability to unseen modalities can be significantly improved. To fully leverage this modality information, we propose the \textit{Modality-Aware Style Projection (MASP)}, which is composed of four key steps: \textit{1) Modality Classification Module}, \textit{2) Style Mining and IN}, \textit{3) Style Bases Recalibration}, and \textit{4) Feature Projection}. For clarity, we assume all operations are performed in the $l$-th encoder stage. The overall architecture of the TTMG and MASP stage is illustrated on Figure \ref{fig:TTMG}.

\noindent \textbf{Modality Classification Module.} Let $F \in \mathbb{R}^{H_{l} \times W_{l} \times C_{l}}$ be the feature map at $l$-th encoder stage where $(H_{l}, W_{l}, C_{l})$ denotes the height, width, and number of channels of the input feature map $F$, respectively. The feature map $F$ is then passed through $\varphi_{\text{mc}} (\cdot; \textbf{W})$, a module comprising a Global Average Pooling (GAP) layer followed by a fully connected layer, parameterized by $\textbf{W} \in \mathbb{R}^{C_{l} \times M}$. Finally, we can obtain the probability $p_{m}$ that the input feature map belongs to the $m$-th modality for $m = 1, 2, \dots, M$ by applying the Softmax operation as follows:
\begin{equation}
    p_{m} = \frac{\textbf{exp}(o_{m})}{\sum_{m^{'} = 1}^{M} \textbf{exp} (o_{m^{'}}) }
\end{equation}
\noindent where $o_{m}$ is an $m$-th node output of $\varphi_{\text{mc}} (F; \textbf{W}) \in \mathbb{R}^{M}$. Note that $P = \{ p_{1}, p_{2}, \dots, p_{M} \}$ satisfies the both conditions $\sum_{m = 1}^{M} p_{m} = 1$ and $p_{m} \in [0, 1]$ for all $m = 1, 2, \dots, M$.

\noindent \textbf{Style Mining and IN.} Previous DG methods \cite{choi2021robustnet, lee2022wildnet, huang2023style, zhou2023instance, zhou2024test} have shown that deep layers in segmentation networks capture content information from input images, while shallow layers focus on style features. Moreover, these approaches model the style distribution space using the channel-wise mean and standard deviation of feature maps. Therefore, we obtain the style vector of the feature map as follows:
\begin{equation}
    \begin{cases}
        \mu &= \frac{1}{H_{l}W_{l}} \sum_{h = 1}^{H_{l}} \sum_{w = 1}^{W_{l}} (F)_{h, w, :} \\
        \sigma &= \sqrt{\frac{1}{H_{l}W_{l}} \sum_{h = 1}^{H_{l}} \sum_{w = 1}^{W_{l}} (F - \mu)^{2}_{h, w, :}}
    \end{cases}
\end{equation}
\noindent where $\mu \in \mathbb{R}^{C_{l}}$ and $\sigma \in \mathbb{R}^{C_{l}}$ represent the channel-wise mean and standard deviation of feature map $F$, respectively. Since we assume that the characteristics of different modalities are reflected in their distinct feature distributions, we apply IN \cite{ulyanov2017improved} to remove modality-specific style information by standardizing the features to a normalized distribution (i.e., zero mean and unit variance) as $F_{n} = \frac{F - \mu}{\sigma + \epsilon} \in \mathbb{R}^{H_{l} \times W_{l} \times C_{l}}$ where $\epsilon$ denotes a small value to prevent zero division ($\epsilon = 10^{-6}$).

\noindent \textbf{Style Bases Recalibration.} Now, we recalibrate the style bases $(\mu, \sigma)$ of the input feature map $F$ using a style bases bank that represents the styles of each accessible modality during the training phase. To implement this style recalibration  process, we defined the modality-specific style representation bases bank $\beta_{m} = \{ (\mu^{k}_{m}, \sigma^{k}_{m}) \}_{k = 1}^{K_{m}}$, where $K_{m}$ denotes the number of style bases defining the $m$-th modality style space, inspired by prototype learning-based DG approaches \cite{huang2023style, zhou2023instance, zhou2024test}. Then, we compute the cosine similarity $s^{k}_{m}$ between the input feature map style $(\mu, \sigma)$ and $k$-th style $(\mu^{k}_{m}, \sigma^{k}_{m})$ in $m$-th modality-specific style bases bank $\beta_{m}$ for $k = 1, 2, \dots, K_{m}$ and $m = 1, 2, \dots, M$ as follows:
\begin{equation}
    s^{k}_{m} = \frac{\mu \cdot \mu^{k}_{m}}{|| \mu || \cdot || \mu^{k}_{m} ||} + \frac{\sigma \cdot \sigma^{k}_{m}}{|| \sigma || \cdot || \sigma^{k}_{m} ||}
\end{equation}
\noindent Subsequently, we apply the Softmax operation to cosine similarity values $s^{k}_{m}$ to obtain weight value $\textbf{w}_{m} = \{ w^{k}_{m} \}_{k = 1}^{K_{m}}$ satisfying $\sum_{k = 1}^{K_{m}} w^{k}_{m} = 1$ and $w^{k}_{m} \in [0, 1]$ for $k = 1, 2, \dots, K_{m}$. This process ensures that the cosine similarity scores $s^{k}_{m}$ are normalized into a probabilistic distribution, allowing for an adaptive weighting of different styles from the style bank based on their relevance to the input feature map. Finally, using this weight value, we can obtain recalibrated modality-specific style bases $(\mu_{m}, \sigma_{m})$ for each $m = 1, 2, \dots, M$ as follows:
\begin{equation}
        \mu_{m} = \sum_{k = 1}^{K_{m}} w^{k}_{m} \cdot \mu^{k}_{m}, \sigma_{m} = \sum_{k = 1}^{K_{m}} w^{k}_{m} \cdot \sigma^{k}_{m}
\end{equation}

\noindent \textbf{Feature Projection.} Finally, we project the normalized features $F_{n}$ onto the style distribution of each modality by using the corresponding modality-specific style bases $(\mu_{m}, \sigma_{m})$ as $\mathbf{F}_{m} = \{ F_{m} \}_{m = 1}^{M}$ where $F_{m} = F_{n} \cdot \sigma_{m} + \mu_{m} \in \mathbb{R}^{H_{l} \times W_{l} \times C_{l}}$ for each $m = 1, 2, \dots, M$. Therefore, a $F_{m}$ represents a feature map with modality-specific style. Subsequently, the modality classification output $P$ is used to map the feature map to the most similar modality style space as follows:
\begin{equation}
    F_{\text{MASP}} = \sum_{m = 1}^{M} p_{m} \cdot F_{m} \in \mathbb{R}^{H_{l} \times W_{l} \times C_{l}}
\end{equation}
\noindent By utilizing the modality probability $P = \{ p_{m} \}_{m = 1}^{M}$, we can more precisely capture the most representative feature distribution of each modality. Additionally, this probabilistic information allows the model to adjust its feature extraction process based on the likelihood of each modality, enabling a more accurate projection of the distinct characteristics of the seen modalities. Subsequently, the output feature map $F_{\text{MASP}}$ forwards to the next encoder block.

\begin{algorithm}[t]
\caption{Modality-Sensitive Instance Whitening}
\label{alg_MSIW}
\textbf{Input}: Original feature map $F$, Modality-aware Style-projected feature map $F_{\text{MASP}}$, and Modality-specific style-projected feature maps $F_{m}$ for $m = 1, 2, \dots, M$\\
\textbf{Output}: MSIW loss $\mathcal{L}_{\text{MSIW}}$
\begin{algorithmic}[1] %[1] enables line numbers
\STATE Initialize $\mathcal{L}_{\text{MSIW}} = 0$
\STATE Compute covariance matrix $\Lambda_{0}$ from $F$
\STATE Compute covariance matrix $\Lambda_{M + 1}$ from $F_{\text{MASP}}$
\FOR{$F_{m}$ for $m = 1, 2, \dots, M$}
    \STATE Compute covariance matrix $\Lambda_{m}$ from $F_{m}$
\ENDFOR
\STATE Compute the variance matrix $\mathbf{V}$ of $\Lambda_{m}$ for $m = 0, 1, \dots, M, M + 1$
\STATE Apply $k$-means clustering on the strict upper triangular element $\mathbf{V}_{\text{U}}$ of the variance matrix $\mathbf{V}$
\STATE Separate each cluster into two groups $\mathbf{M}_{\text{low}}$ and $\mathbf{M}_{\text{high}}$
\FOR{$\Lambda_{m}$ for $m = 0, 1, \dots, M, M + 1$}
    \STATE $\mathcal{L}_{\text{MSIW}} = \mathcal{L}_{\text{MSIW}} + || \Lambda_{m} \odot \mathbf{M}_{\text{high}} ||_{1}$
\ENDFOR
\STATE $\mathcal{L}_{\text{MSIW}} = \frac{1}{M + 2} \mathcal{L}_{\text{MSIW}} $
\RETURN $\mathcal{L}_{\text{MSIW}}$
\end{algorithmic}
\end{algorithm}

\subsection{Modality-Sensitive Instance Whitening}
\label{ss33_modality_sensitive_instance_whitening}

\noindent \textit{Motivation:} Although $F_{\text{MASP}}$ is aligned with the most similar modality style distribution via the MASP stage, high covariance between feature channels still limits generalization \cite{li2021simple, zhang2024domain}. While IW effectively addresses this issue, completely whitening covariance degrades performance on seen modalities \cite{choi2021robustnet, peng2022semantic}. To address this issue, we propose \textit{Modality-Sensitive Instance Whitening (MSIW)}, which separates modality-sensitive and modality-invariant information and then selectively whitening only the covariance of modality-sensitive information.

To separate the modality-sensitive information, we use a total $M + 2$ feature maps including original feature $F$, modality-specific style-projected feature $F_{m}$ for $m = 1, 2, \dots, M$, and modality-aware style-projected feature $F_{\text{MASP}}$. Inspired by previous works \cite{choi2021robustnet, jung2023local}, we define modality-sensitive information as the channels that exhibit high variance of the covariance matrix $\Lambda_{m}$ obtained from original, modality-specific style-projected, and modality-aware style-projected feature map. Subsequently, the variance matrix of covariance matrices is calculated as follows:
\begin{equation}
    \mathbf{V} = \frac{1}{M + 2} \sum_{m = 0}^{M + 1} \Lambda_{m}
\end{equation}
\noindent where $\Lambda_{0}$, and $\Lambda_{M + 1}$ denote the covariance matrices of the original and modality-aware style projected feature maps, respectively. And, $\Lambda_{m}$ denotes the covariance matrix of modality-specific style projected feature map for $m = 1, 2, \dots, M$. Since the variance matrix $\mathbf{V}$ is a symmetric matrix, we only consider the strict upper triangular element $\mathbf{V}_{\text{U}} = \{\mathbf{V}_{i, j} | i < j \text{ for } i = 1, \dots, C_{l} \text{ and } j = 1, \dots, C_{l}\}$ to eliminate modality-sensitive information. Then, $\mathbf{V}_{\text{U}}$ is divided into $k$ clusters using $k$-means clustering \cite{lloyd1982least}, and each cluster is separated into two groups, called $\mathbf{M}_{\text{low}} = \{ v_{1}, \dots, v_{s} \} $ and $\mathbf{M}_{\text{high}}  = \{ v_{s + 1}, \dots, v_{k} \}$ with low and high variance, respectively. Therefore, we assume that $\mathbf{M}_{\text{low}}$ and $\mathbf{M}_{\text{high}}$ have modality-invariant and modality-sensitive information, respectively. Finally, we define MSIW loss as $\mathcal{L}_{\text{MSIW}} = \mathbb{E} \left[ || \Lambda_{m} \odot \mathbf{M}_{\text{high}} ||_{1} \right]$. Algorithm \ref{alg_MSIW} describes the detailed training algorithm for MSIW.

\subsection{Network Training}
During training, we define $\mathcal{L}^{l}_{\text{con}}$ to preserve the contents information of style-projected features by maximizing the cosine similarities between normalized and modality-specific style-projected features as follows:
\begin{equation}
    \mathcal{L}_{\text{con}} = -\frac{1}{M} \sum_{m = 1}^{M} \log\left( \frac{\textbf{exp} (z_{nm})}{\sum_{m^{'} = 1}^{M} \textbf{exp} (z_{nm^{'}})} \right)
\end{equation}
\noindent where $z_{nm} = \frac{F_{n}}{||F_{n}||_{2}} \cdot \frac{F_{m}}{||F_{m}||_{2}}$. Additionally, we compute $\mathcal{L}^{l}_{\text{cls}}$ to classify seen modality during training as follows:
\begin{equation}
    \mathcal{L}_{\text{cls}} = \sum_{m = 1}^{M} y^{\text{cls}}_{m} \log(p_{m})
\end{equation}
\noindent where $y^{\text{cls}}$ is the label for the available modality to train modality classification network $\varphi_{\text{mc}} (\cdot; \textbf{W})$. Finally, we combine four different loss terms for the end-to-end training as follows:
\begin{equation}
    \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{seg}} + \frac{1}{L} \sum_{l \in L} \left( \mathcal{L}^{l}_{\text{cls}} + \mathcal{L}^{l}_{\text{con}} + \mathcal{L}^{l}_{\text{MSIW}} \right)
\end{equation}
\noindent where $L$ denotes the set of layers to which MASP and MSIW are applied. And, the loss function for segmentation was defined as $\mathcal{L}_{\text{seg}} = \mathcal{L}^{w}_{IoU} + \mathcal{L}^{w}_{bce}$, where $\mathcal{L}^{w}_{IoU}$ and $\mathcal{L}^{w}_{bce}$ are the weighted IoU and binary cross entropy (BCE) loss functions, respectively. This loss function is identically defined in previous studies \cite{fan2020pranet, zhao2023m, nam2024modality}. For each training phase, the parameter of segmentation network $\varphi_{\text{seg}}$, classification network $\varphi_{\text{mc}}$, and style bases $\beta_{m}$ for $m = 1, 2, \dots, M$ are updated in end-to-end manner. After the training phase, all parameters are not updated during the inference phase.

\begin{table*} [t]
    \centering
    \small
    \setlength\tabcolsep{2.5pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.80} % Tighter
    \begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc|cc} 
    \hline
    \multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{4}{c|}{Training Modalities (C, U, D)} & \multicolumn{4}{c|}{Training Modalities (C, U, R)} & \multicolumn{4}{c|}{Training Modalities (C, D, R)} & \multicolumn{4}{c}{Training Modalities (U, D, R)} \\\cline{2-17}
     & \multicolumn{2}{c|}{Seen (C, U, D)} & \multicolumn{2}{c|}{Unseen (R)} & \multicolumn{2}{c|}{Seen (C, U, R)} & \multicolumn{2}{c|}{Unseen (D)} & \multicolumn{2}{c|}{Seen (C, D, R)} & \multicolumn{2}{c|}{Unseen (U)} & \multicolumn{2}{c|}{Seen (U, D, R)} & \multicolumn{2}{c}{Unseen (C)} \\\cline{2-17}
     & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU \\
     \hline
     baseline \cite{chen2018encoder} & 80.98 & 73.65 & 14.30 & 9.12 & \textcolor{blue}{\textbf{\textit{76.20}}} & \textcolor{blue}{\textbf{\textit{68.67}}} & 42.43 & 36.24 & 77.95 & 70.62 & 13.55 & 8.16 & 76.32 & 67.99 & 19.36 & 12.78 \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IN \cite{ulyanov2017improved}}} & \textcolor{red}{\textbf{\underline{82.24}}} & \textcolor{red}{\textbf{\underline{75.00}}} & 13.92 & 8.79 & 74.23 & 66.48 & 48.15 & \textcolor{blue}{\textbf{\textit{39.85}}} & \textcolor{red}{\textbf{\underline{78.17}}} & \textcolor{blue}{\textbf{\textit{70.84}}} & 26.82 & 17.94 & 74.49 & 65.32 & 14.22 & 9.06 \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.26)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.35)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.38)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.33)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-1.97)}}}          & \textcolor{red}{\scriptsize{\textbf{(-2.19)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.72)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.61)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.22)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.22)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+13.27)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+9.78)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-1.83)}}}          & \textcolor{red}{\scriptsize{\textbf{(-2.67)}}}
      & \textcolor{red}{\scriptsize{\textbf{(-5.14)}}}          & \textcolor{red}{\scriptsize{\textbf{(-3.72)}}} \\ 
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IW \cite{huang2018decorrelated}}} & 72.93 & 64.74 & 9.85 & 6.21 & 60.36 & 52.12 & \textcolor{blue}{\textbf{\textit{50.11}}} & 38.79 & 65.98 & 58.45 & 34.65 & 25.30 & 68.64 & 61.26 & 15.01 & 9.35 \\
      & \textcolor{red}{\scriptsize{\textbf{(-8.05)}}}          & \textcolor{red}{\scriptsize{\textbf{(-8.91)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-4.45)}}}          & \textcolor{red}{\scriptsize{\textbf{(-2.91)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-15.84)}}}         & \textcolor{red}{\scriptsize{\textbf{(-16.55)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+7.68)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.55)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-11.97)}}}         & \textcolor{red}{\scriptsize{\textbf{(-12.17)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+21.10)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+17.14)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-7.68)}}}          & \textcolor{red}{\scriptsize{\textbf{(-6.73)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-4.35)}}}          & \textcolor{red}{\scriptsize{\textbf{(-3.43)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IBN \cite{pan2018two}}} & 80.98 & 73.89 & \textcolor{blue}{\textbf{\textit{16.43}}} & \textcolor{blue}{\textbf{\textit{10.93}}} & \textcolor{blue}{\textbf{\textit{76.20}}} & 68.64 & 43.50 & 36.59 & 78.00 & 70.62 & 30.74 & 20.97 & \textcolor{blue}{\textbf{\textit{77.87}}} & \textcolor{blue}{\textbf{\textit{69.54}}} & 20.51 & 13.73 \\
      & \scriptsize{\textbf{(+0.00)}}                           & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.24)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.13)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.81)}}} 
      & \scriptsize{\textbf{(+0.00)}}                           & \textcolor{red}{\scriptsize{\textbf{(-0.03)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.07)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.35)}}}
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.05)}}}  & \scriptsize{\textbf{(+0.00)}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+17.19)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+12.81)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.55)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.55)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.15)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.95)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{RobustNet \cite{choi2021robustnet}}} & 79.84 & 73.35 & 7.17 & 4.62 & 70.20 & 62.40 & 46.89 & 38.44 & 70.78 & 63.59 & \textcolor{red}{\textbf{\underline{43.63}}} & \textcolor{red}{\textbf{\underline{33.81}}} & 72.87 & 64.72 & 17.59 & 11.74 \\
      & \textcolor{red}{\scriptsize{\textbf{(-1.14)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.30)}}}
      & \textcolor{red}{\scriptsize{\textbf{(-7.13)}}}          & \textcolor{red}{\scriptsize{\textbf{(-4.50)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-6.00)}}}          & \textcolor{red}{\scriptsize{\textbf{(-6.27)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.46)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.20)}}}
      & \textcolor{red}{\scriptsize{\textbf{(-7.17)}}}          & \textcolor{red}{\scriptsize{\textbf{(-7.03)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+30.08)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+25.65)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-3.45)}}}          & \textcolor{red}{\scriptsize{\textbf{(-3.27)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-1.77)}}}          & \textcolor{red}{\scriptsize{\textbf{(-1.04)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{SAN-SAW \cite{peng2022semantic}}} & 80.29 & 73.12 & 14.24 & 8.58 & 75.14 & 67.85 & 40.04 & 30.35 & 77.11 & 70.04 & 32.67 & 23.48 & 77.63 & 69.52 & 20.91 & 13.89 \\
      & \textcolor{red}{\scriptsize{\textbf{(-0.69)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.53)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.06)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.54)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-1.06)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.82)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-2.39)}}}          & \textcolor{red}{\scriptsize{\textbf{(-5.89)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.84)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.58)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+19.12)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+15.32)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.31)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.53)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.55)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.11)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{SPCNet \cite{huang2023style}}} & \textcolor{blue}{\textbf{\textit{81.23}}} & \textcolor{blue}{\textbf{\textit{74.13}}} & 12.86 & 8.36 & 74.95 & 67.47 & 47.70 & 39.60 & \textcolor{blue}{\textbf{\textit{78.11}}} & \textcolor{red}{\textbf{\underline{70.86}}} & 20.12 & 12.83 & 77.63 & 69.49 & 19.87 & 13.47 \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.25)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.48)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-1.44)}}}         & \textcolor{red}{\scriptsize{\textbf{(-0.76)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-1.25)}}}         & \textcolor{red}{\scriptsize{\textbf{(-1.20)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.27)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.36)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.16)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.24)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+6.57)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.67)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.31)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.50)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.51)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.69)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{BlindNet \cite{ahn2024style}}} & 79.87 & 72.75 & 4.54 & 3.04 & 72.69 & 65.16 & 27.63 & 22.65 & 73.07 & 65.56 & 33.06 & 25.55 & 76.49 & 68.36 & \textcolor{blue}{\textbf{\textit{22.73}}} & \textcolor{blue}{\textbf{\textit{17.58}}} \\
      & \textcolor{red}{\scriptsize{\textbf{(-1.11)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.90)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-9.76)}}}          & \textcolor{red}{\scriptsize{\textbf{(-6.08)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-3.51)}}}          & \textcolor{red}{\scriptsize{\textbf{(-3.51)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-14.80)}}}         & \textcolor{red}{\scriptsize{\textbf{(-13.59)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-4.88)}}}          & \textcolor{red}{\scriptsize{\textbf{(-5.06)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+19.51)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+17.39)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.17)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.37)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.37)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.80)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{TTMG (Ours)}}} & 79.93 & 72.38 & \textcolor{red}{\textbf{\underline{17.55}}} & \textcolor{red}{\textbf{\underline{11.63}}} & \textcolor{red}{\textbf{\underline{78.79}}} & \textcolor{red}{\textbf{\underline{71.10}}} & \textcolor{red}{\textbf{\underline{52.55}}} & \textcolor{red}{\textbf{\underline{42.40}}} & \textcolor{red}{\textbf{\underline{78.17}}} & \textcolor{blue}{\textbf{\textit{70.84}}} & \textcolor{blue}{\textbf{\textit{38.47}}} & \textcolor{blue}{\textbf{\textit{27.93}}} & \textcolor{red}{\textbf{\underline{78.59}}} & \textcolor{red}{\textbf{\underline{70.61}}} & \textcolor{red}{\textbf{\underline{25.05}}} & \textcolor{red}{\textbf{\underline{17.61}}} \\
      & \textcolor{red}{\scriptsize{\textbf{(-1.05)}}}          & \textcolor{red}{\scriptsize{\textbf{(-1.27)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.25)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.51)}}}
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.59)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.43)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.12)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+6.16)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.22)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.22)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+24.92)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+19.77)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.27)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.62)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.69)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.83)}}} \\ \cline{2-17}
      & \textbf{-2.31} & \textbf{-2.62} & \textbf{+1.12} & \textbf{+0.70} 
      & \textbf{+2.59} & \textbf{+2.43} & \textbf{+2.44} & \textbf{+2.55} 
      & \textbf{+0.00} & \textbf{-0.02} & \textbf{-5.16} & \textbf{-5.88} 
      & \textbf{+0.72} & \textbf{+1.07} & \textbf{+2.32} & \textbf{+0.03} \\
     \hline
    \end{tabular}
    \caption{Segmentation results on three modality training schemes (C, U, D), (C, U, R), (C, D, R), and (U, D, R).}
    \label{tab:comparison_three_modality}
\end{table*}