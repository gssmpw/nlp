\section{Experiment Results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/QualitativeResults.png}
    \caption{Qualitative comparison of other methods and TTMG on Radiology and Ultrasound modalities.  (a) Input images with ground truth. (b) baseline (DeepLabV3+ with ResNet50, \cite{chen2018encoder}). (c) IN \cite{ulyanov2017improved}. (d) IW \cite{huang2018decorrelated}. (e) IBN \cite{pan2018two}. (f) RobustNet \cite{choi2021robustnet}. (g) SAN-SAW \cite{peng2022semantic}. (h) SPCNet \cite{huang2023style}. (i) BlindNet \cite{ahn2024style}. (j) \textbf{TTMG (Ours)}. In this figure, \textcolor{green}{\textbf{Green}} and \textcolor{red}{\textbf{Red}} lines denote the boundaries of the ground truth and prediction, respectively.}
    \label{fig:QualitativeResults}
\end{figure*}

\subsection{Experiment Settings}
\label{s41_experiment_settings}

In this paper, we use eleven datasets spanning four modalities (colonoscopy (C), ultrasound (U), dermoscopy (D), and radiology (R)), which have been used for training and evaluating various deep learning-based medical image segmentation models \cite{zhou2018unet++, chen2021transunet,  cao2022swin, wang2022uctransnet, zhao2023m, jiang2023vig, wang2024cfatransunet, nam2024modality}. For convenience, we denote the seen and unseen modalities as the test datasets, which are the same and different modalities as source training modality datasets, respectively. Due to the page limit, we present the detailed dataset description in the Appendix \ref{appendix_dataset_descriptions}. Since this paper consider the case of multi-source modality ($M > 1$), we conducted two experiments for $M = 3$ and $M = 2$, which are listed in Tables \ref{tab:comparison_three_modality} and \ref{tab:comparison_two_modality}, respectively. To evaluate the performance of each method, we selected two metrics, the Dice Score Coefficient (DSC) and mean Intersection over Union (mIoU), which are widely used in medical image segmentation. Additionally, the quantitative results with more various metrics \cite{margolin2014evaluate, fan2017structure, fan2018enhanced} and metrics descriptions are also available in the Appendix. 

We compared the proposed \textbf{TTMG} with seven representative domain generalization method, including IN \cite{ulyanov2017improved}, IW \cite{huang2018decorrelated}, IBN \cite{pan2018two}, RobustNet \cite{choi2021robustnet}, SAN-SAW \cite{peng2022semantic}, SPCNet \cite{huang2023style}, and BlindNet \cite{ahn2024style}. For all tables, \textcolor{red}{\textbf{\underline{Red}}} and \textcolor{blue}{\textbf{\textit{Blue}}} are the first and second-best performance results, respectively. And, $( \cdot )$ denotes the performance gap between the baseline and each method. Additionally, the last row indicates the performance gap between \textbf{TTMG} and other best methods.

\subsection{Implementation Details}
\label{s42_implementation_details}

\noindent\textbf{Training Settings.} We implemented TTMG on a single NVIDIA RTX 3090 Ti in Pytorch 1.8 \cite{NEURIPS2019_9015}. Following the other DGSS methods, we choose DeepLabV3+ \cite{chen2018encoder} with ResNet50 \cite{he2016deep} as baseline. Additionally, we also provide the experiment results with other backbones (MobileNetV2 \cite{sandler2018mobilenetv2} and ShuffleNetV2 \cite{ma2018shufflenet}) and TransUNet \cite{chen2021transunet}, which is the most representative Transformer-based model, in the Appendix (Table \ref{tab:backbone_type}) due to the page limit. Additionally, we employ an Adam optimizer \cite{KingmaB14} with an initial learning rate of $10^{-4}$ and reduced the parameters of each method to $10^{-6}$ using a cosine annealing learning rate scheduler \cite{loshchilov2016sgdr}. According to the most representative works for medical image segmentation, the required epochs for each modalities are 50, 100, 100, and 200 epochs. Since we only consider multi-modality ($M > 1$) training scheme in this paper, we choose the largest epoch when different epochs are used. For example, we trained the model using colonoscopy and radiology modalities, and then we optimized the model for 200 epochs since the radiology dataset requires more epochs. During training, we used horizontal/vertical flipping, with a probability of 50\%, and rotation between $-5^{\circ}$ and $5^{\circ}$, which is widely used in medical image segmentation \cite{fan2020pranet, zhao2021automatic, zhao2023m, nam2024modality, nam2025transgunet}. At this stage, because images in each dataset have different resolutions, all images were resized to $352 \times 352$.

\noindent\textbf{Hyperparameters of TTMG.} Key hyperparameters for TTMG on all datasets were set to $K_{m} = 8$ in MASP, and $k = 3$ and $s = 1$ in MSIW following RobustNet \cite{choi2021robustnet}. Additionally, $\beta_{m}$ is randomly initialized to start training. We apply MASP and MSIW in two shallow layers, that is, $L = \{ 1, 2 \}$ (Stage 1 and Stage 2). In the Appendix, we provide the experiment results on various hyperparameter settings. \vspace{-0.15cm}

\begin{table*} [t]
    \centering
    \small
    \setlength\tabcolsep{5.5pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.80} % Tighter
    \begin{tabular}{c|cc|cc|cc|cc|cc|cc} 
    \hline
    \multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{4}{c|}{Training Modalities (C, D)} & \multicolumn{4}{c|}{Training Modalities (C, R)} & \multicolumn{4}{c}{Training Modalities (C, U)} \\\cline{2-13}
     & \multicolumn{2}{c|}{Seen (C, D)} & \multicolumn{2}{c|}{Unseen (U, R)} & \multicolumn{2}{c|}{Seen (C, R)} & \multicolumn{2}{c|}{Unseen (D, U)} & \multicolumn{2}{c|}{Seen (C, U)} & \multicolumn{2}{c}{Unseen (D, R)} \\\cline{2-13}
     & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU \\
    \hline
    baseline \cite{chen2018encoder} & 81.62 & \textcolor{blue}{\textbf{\textit{75.69}}} & 14.81 & 9.00 & 73.51 & 66.57 & 4.82 & 3.40 & 80.62 & \textcolor{blue}{\textbf{\textit{73.45}}} & 31.78 & 25.68 \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IN \cite{ulyanov2017improved}}} & 81.19 & 74.55 & 21.86 & 14.59 & \textcolor{blue}{\textbf{\textit{74.82}}} & \textcolor{blue}{\textbf{\textit{67.44}}} & 2.23 & 1.49 & 80.04 & 72.57 & 34.65 & 27.97 \\
      & \textcolor{red}{\scriptsize{\textbf{(-0.43)}}}         & \textcolor{red}{\scriptsize{\textbf{(-1.14)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+7.05)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.59)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.31)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.87)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-2.59)}}}         & \textcolor{red}{\scriptsize{\textbf{(-1.91)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.58)}}}         & \textcolor{red}{\scriptsize{\textbf{(-0.88)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.87)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.29)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IW \cite{huang2018decorrelated}}} & 70.82 & 62.99 & 18.41 & 12.37 & 53.71 & 46.37 & 4.52 & 2.67 & 66.70 & 57.98 & 32.87 & 25.58 \\
      & \textcolor{red}{\scriptsize{\textbf{(-10.80)}}}        & \textcolor{red}{\scriptsize{\textbf{(-12.70)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.60)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.37)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-19.80)}}}        & \textcolor{red}{\scriptsize{\textbf{(-20.20)}}} 
      & \textcolor{red}{\scriptsize{(\textbf{-0.30)}}}         & \textcolor{red}{\scriptsize{\textbf{(-0.73)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-13.92)}}}        & \textcolor{red}{\scriptsize{\textbf{(-15.47)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.09)}}} & \textcolor{red}{\scriptsize{\textbf{(-0.10)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IBN \cite{pan2018two}}} & \textcolor{red}{\textbf{\underline{82.30}}} & \textcolor{red}{\textbf{\underline{75.72}}} & 21.93 & 15.15 & 73.92 & 66.60 & 2.50 & 1.83 & 80.62 & 73.31 & \textcolor{blue}{\textbf{\textit{37.59}}} & \textcolor{red}{\textbf{\underline{31.41}}} \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.68)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.03)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+7.12)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+6.15)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.41)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.03)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-2.32)}}}         & \textcolor{red}{\scriptsize{\textbf{(-1.57)}}} 
      & \scriptsize{\textbf{(+0.00)}}                          & \textcolor{red}{\scriptsize{\textbf{(-0.14)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.81)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.73)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{RobustNet \cite{choi2021robustnet}}} & 79.69 & 72.54 & \textcolor{blue}{\textbf{\textit{25.49}}} & \textcolor{blue}{\textbf{\textit{19.29}}} & 67.18 & 60.01 & 7.13 & 4.63 & 75.83 & 68.01 & 31.92 & 25.96 \\
      & \textcolor{red}{\scriptsize{\textbf{(-1.93)}}}          & \textcolor{red}{\scriptsize{\textbf{(-3.15)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.68)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.29)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-6.33)}}}          & \textcolor{red}{\scriptsize{\textbf{(-6.56)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.31)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.23)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-4.79)}}}          & \textcolor{red}{\scriptsize{\textbf{(-5.44)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.14)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.28)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{SAN-SAW \cite{peng2022semantic}}} & 75.25 & 68.14 & 17.76 & 12.65 & 68.22 & 61.31 & 4.95 & 3.49 & 70.38 & 62.50 & 28.59 & 22.18 \\
      & \textcolor{red}{\scriptsize{\textbf{(-6.37)}}}         & \textcolor{red}{\scriptsize{\textbf{(-7.55)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.95)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.65)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-5.20)}}}         & \textcolor{red}{\scriptsize{\textbf{(-5.26)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.13)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.09)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-10.24)}}}        & \textcolor{red}{\scriptsize{\textbf{(-10.95)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-3.19)}}}         & \textcolor{red}{\scriptsize{\textbf{(-3.50)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{SPCNet \cite{huang2023style}}} & \textcolor{blue}{\textbf{\textit{81.95}}} & 75.23 & 13.97 & 8.73 & 72.61 & 65.65 & \textcolor{blue}{\textbf{\textit{8.42}}} & \textcolor{blue}{\textbf{\textit{5.71}}} & \textcolor{blue}{\textbf{\textit{80.63}}} & 73.32 & 35.56 & 28.88 \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.33)}}} & \textcolor{red}{\scriptsize{\textbf{(-0.46)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.84)}}}         & \textcolor{red}{\scriptsize{\textbf{(-0.27)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.90)}}}         & \textcolor{red}{\scriptsize{\textbf{(-0.92)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.60)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.31)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.01)}}} & \textcolor{red}{\scriptsize{\textbf{(-0.13)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.78)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.20)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{BlindNet \cite{ahn2024style}}} & 80.58 & 73.76 & 24.51 & 18.32 & 73.02 & 65.75 & 7.06 & 4.81 & 78.07 & 70.57 & 28.96 & 23.27 \\
      & \textcolor{red}{\scriptsize{\textbf{(-1.04)}}}         & \textcolor{red}{\scriptsize{\textbf{(-1.93)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+9.70)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+9.32)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.49)}}}         & \textcolor{red}{\scriptsize{\textbf{(-0.82)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.24)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.41)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-2.55)}}}         & \textcolor{red}{\scriptsize{\textbf{(-2.88)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-2.82)}}}         & \textcolor{red}{\scriptsize{\textbf{(-2.41)}}} \\
     \hline
     \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{TTMG (Ours)}}} & 77.85 & 70.53 & \textcolor{red}{\textbf{\underline{25.75}}} & \textcolor{red}{\textbf{\underline{20.10}}} & \textcolor{red}{\textbf{\underline{76.12}}} & \textcolor{red}{\textbf{\underline{68.77}}} & \textcolor{red}{\textbf{\underline{19.47}}} & \textcolor{red}{\textbf{\underline{13.68}}} & \textcolor{red}{\textbf{\underline{81.19}}} & \textcolor{red}{\textbf{\underline{73.66}}} & \textcolor{red}{\textbf{\underline{39.22}}} & \textcolor{blue}{\textbf{\textit{31.15}}} \\
      & \textcolor{red}{\scriptsize{\textbf{(-3.77)}}}          & \textcolor{red}{\scriptsize{\textbf{(-5.16)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.94)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+11.10)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.61)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.20)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+14.65)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.28)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.57)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.21)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+7.44)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.47)}}} \\\cline{2-13}
     & \textbf{-4.45} & \textbf{-5.19} & \textbf{+0.26}  & \textbf{+0.81} 
     & \textbf{+1.30} & \textbf{+1.33} & \textbf{+11.05} & \textbf{+7.97} 
     & \textbf{+0.56} & \textbf{+0.21} & \textbf{+1.63} & \textbf{-0.26} \\
     \hline
     \hline
    \multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{4}{c|}{Training Modalities (D, U)} & \multicolumn{4}{c|}{Training Modalities (D, R)} & \multicolumn{4}{c}{Training Modalities (R, U)} \\\cline{2-13}
     & \multicolumn{2}{c|}{Seen (D, U)} & \multicolumn{2}{c|}{Unseen (C, R)} & \multicolumn{2}{c|}{Seen (D, R)} & \multicolumn{2}{c|}{Unseen (C, U)} & \multicolumn{2}{c|}{Seen (R, U)} & \multicolumn{2}{c}{Unseen (C, D)} \\\cline{2-13}
     & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU \\
    \hline
    baseline \cite{chen2018encoder} & 84.60 & 76.56 & 16.97 & 10.70 & 75.42 & 67.39 & 18.76 & 12.25 & 70.77 & 61.59 & 3.92 & 2.33 \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IN \cite{ulyanov2017improved}}} & 85.70 & 77.85 & 21.95 & 14.77 & 57.06 & 49.18 & 20.66 & 13.26 & 70.49 & 60.96 & 19.01 & 15.83 \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.10)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.29)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.98)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.07)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-18.36)}}}         & \textcolor{red}{\scriptsize{\textbf{(-18.21)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.90)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.01)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.28)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.63)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+15.09)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+13.50)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IW \cite{huang2018decorrelated}}} & 81.15 & 72.05 & 14.09 & 8.84 & 62.49 & 56.17 & 22.83 & 15.33 & 56.05 & 48.01 & 17.64 & 13.43 \\
      & \textcolor{red}{\scriptsize{\textbf{(-3.45)}}}          & \textcolor{red}{\scriptsize{\textbf{(-4.51)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-2.88)}}}          & \textcolor{red}{\scriptsize{\textbf{(-1.86)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-12.93)}}}         &  \textcolor{red}{\scriptsize{\textbf{(-11.22)}}} 
      &  \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.07)}}} &  \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.08)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-14.72)}}}         & \textcolor{red}{\scriptsize{\textbf{(-13.58)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+13.72)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+11.10)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{IBN \cite{pan2018two}}} & \textcolor{red}{\textbf{\underline{86.38}}} & \textcolor{red}{\textbf{\underline{78.42}}} & \textcolor{blue}{\textbf{\textit{22.41}}} & 14.39 & \textcolor{blue}{\textbf{\textit{75.84}}} & \textcolor{blue}{\textbf{\textit{67.81}}} & 22.83 & 15.87 & \textcolor{blue}{\textbf{\textit{71.77}}} & \textcolor{blue}{\textbf{\textit{62.38}}} & 19.76 & 15.81 \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.78)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.86)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.44)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.69)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.42)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.42)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.07)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.62)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.00)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.79)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+15.84)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+13.48)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{RobustNet \cite{choi2021robustnet}}} & 84.21 & 75.95 & 18.86 & 12.87 & 69.28 & 61.64 & 24.19 & 18.15 & 63.79 & 55.14 & 16.47 & 13.42 \\
      & \textcolor{red}{\scriptsize{\textbf{(-0.39)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.61)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.89)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.17)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-6.14)}}}          & \textcolor{red}{\scriptsize{\textbf{(-5.75)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.43)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.90)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-6.98)}}}          & \textcolor{red}{\scriptsize{\textbf{(-6.45)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+12.55)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+11.09)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{SAN-SAW \cite{peng2022semantic}}} & 81.65 & 72.24 & 19.36 & 12.06 & 67.62 & 60.41 & 23.20 & 15.60 & 66.64 & 57.97 & 5.33 & 3.83 \\
      & \textcolor{red}{\scriptsize{\textbf{(-2.95)}}}         & \textcolor{red}{\scriptsize{\textbf{(-4.32)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.39)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.36)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-7.80)}}}         & \textcolor{red}{\scriptsize{\textbf{(-6.98)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.44)}}} &  \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.35)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-4.13)}}}         & \textcolor{red}{\scriptsize{\textbf{(-3.62)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.41)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.50)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{SPCNet \cite{huang2023style}}} & 85.99 & 78.23 & 21.67 & 14.71 & 75.75 & 67.59 & 20.34 & 13.66 & 70.72 & 61.27 & 16.69 & 13.15 \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.39)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.67)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.70)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.01)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.33)}}}  &  \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.20)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.58)}}}  &  \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.41)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-0.05)}}}          & \textcolor{red}{\scriptsize{\textbf{(-0.32)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+12.77)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.82)}}} \\
    \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{BlindNet \cite{ahn2024style}}} & 84.86 & 77.02 & 21.30 & \textcolor{blue}{\textbf{\textit{15.28}}} & 71.85 & 63.93 & \textcolor{blue}{\textbf{\textit{25.96}}} & \textcolor{blue}{\textbf{\textit{18.73}}} & 66.07 & 57.35 & \textcolor{blue}{\textbf{\textit{20.12}}}  & \textcolor{blue}{\textbf{\textit{16.85}}} \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.26)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+0.46)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.33)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+4.58)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-3.57)}}}          &  \textcolor{red}{\scriptsize{\textbf{(-3.46)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+7.20)}}}  &  \textcolor{ForestGreen}{\scriptsize{\textbf{(+6.48)}}} 
      & \textcolor{red}{\scriptsize{\textbf{(-4.70)}}}          & \textcolor{red}{\scriptsize{\textbf{(-4.24)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+16.20)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+14.52)}}} \\
     \hline
     \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{TTMG (Ours)}}} & \textcolor{blue}{\textbf{\textit{86.26}}} & \textcolor{blue}{\textbf{\textit{78.39}}} & \textcolor{red}{\textbf{\underline{25.67}}} & \textcolor{red}{\textbf{\underline{17.90}}} & \textcolor{red}{\textbf{\underline{81.29}}} & \textcolor{red}{\textbf{\underline{73.34}}} & \textcolor{red}{\textbf{\underline{27.30}}} & \textcolor{red}{\textbf{\underline{18.98}}} &  \textcolor{red}{\textbf{\underline{73.62}}} &  \textcolor{red}{\textbf{\underline{65.22}}} &  \textcolor{red}{\textbf{\underline{20.96}}} &  \textcolor{red}{\textbf{\underline{16.98}}} \\
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.66)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+1.83)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+8.70)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+7.20)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.87)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+5.95)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+8.54)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+6.73)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.85)}}}  & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.63)}}} 
      & \textcolor{ForestGreen}{\scriptsize{\textbf{(+17.04)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+14.65)}}} \\\cline{2-13}
     & \textbf{-0.12} & \textbf{-0.03} & \textbf{+3.26} & \textbf{+2.62} 
     & \textbf{+5.45} & \textbf{+5.53} & \textbf{+1.34} & \textbf{+0.25} 
     & \textbf{+1.85} & \textbf{+2.84} & \textbf{+0.84} & \textbf{+0.13} \\
     \hline
    \end{tabular}
    \caption{Segmentation results on two modality training schemes (C, D), (C, R), (C, U), (D, U), (D, R), and (R, U).}
    \label{tab:comparison_two_modality}
\end{table*}

\subsection{Result Analysis}
\noindent \textbf{Quantitative Results.} We used the same model as that employed for the ’Seen’ modalities in Table \ref{tab:comparison_three_modality} and Table \ref{tab:comparison_two_modality} to evaluate the domain generalization performance for ’Unseen’ modalities for each table. We first trained each model on three modalities ((C, U, D), (C, U, R), (C, D, R), (U, D, R)) and then evaluated the performance on seen and unseen modality datasets. As shown in Table \ref{tab:comparison_three_modality}, TTMG has significantly improved the unseen modality generalization performance compared to the baseline. When compared to IN, IW, and IBN, TTMG exhibited DSC improvement of 7.63\%, 6.00\%, and 5.61\%, respectively, on average. Additionally, compared to SPCNet, which used the style projection approach based on prototype learning, TTMG demonstrated DSC improvement of 0.86\%, and 8.27\% on seen and unseen modality datasets on average. Furthermore, we trained each model on the more limited training scenario where the number of seen modalities was reduced to two ((C, D), (C, R), (C, U), (D, U), (D, R), (R, U)) and then evaluated them. As shown in Table \ref{tab:comparison_two_modality}, TTMG still outperforms various modalities settings even limited modality scenarios. Most notably, only TTMG and RobustNet improved generalization performance on the unseen modality dataset compared to the baseline in all scenarios. However, RobustNet shows severe performance degradation in all cases on the seen modality dataset. As shown in the Appendix (Table \ref{tab:efficiency_analysis}), TTMG performs without substantial computational cost and parameters. Additionally, TTMG still achieves higher performance in both seen and unseen modalities compared to existing DG methods even on different backbones (MobileNetV2 \cite{sandler2018mobilenetv2} and ShuffleNetV2 \cite{ma2018shufflenet}) and TransUNet \cite{chen2021transunet} in the Appendix (Table \ref{tab:backbone_type}). These results highlight that employing a prototype learning-based style-projection (MASP) with MSIW is versatile and crucial for efficiently generalizing unseen modalities in medical image segmentation.

\noindent \textbf{Qualitative Results.} Figure \ref{fig:QualitativeResults} illustrates qualitative results across different methods for radiology and ultrasound modalities. The baseline provides adequately seen modality datasets while producing highly noisy predictions on unseen modalities.  Although IN, IW, and IBN standardize feature map distributions between seen and unseen modalities, they still produce noisy predictions and demonstrate degraded performance on seen modalities. RobustNet, which removes domain-sensitive information through visual transformation, does not address modality-sensitive information, resulting in suboptimal performance on unseen modalities. Although SPCNet is similar to our approach (prototype learning-based style projection), semantic clustering in the decoder struggles to capture complex anatomical structures in medical images. In contrast, TTMG achieves robust predictions across all modalities, effectively handling noise, varying lesion sizes, and complex anatomical structures by leveraging the dual capabilities of MASP and MSIW. Due to the page limit, we also provide more qualitative results on the other various training scenarios in Appendix.

\noindent \textbf{Feature Visualization.} We employed T-SNE \cite{van2008visualizing} visualization in the (R, U) to (C, D) training scenario to illustrate how data achieves unseen modality generalization. Figure \ref{fig:FeatureProjection} shows the feature distributions across various modalities, where MASP effectively projects the feature representations around the style bases of seen modalities. This clustering effect compensates for the characteristic differences between modalities, facilitating more robust generalization to unseen modalities. Additionally, we observed that the baseline model exhibits high feature covariance, whereas TTMG selectively maintains lower feature covariance as shown in Figure \ref{fig:FeatureCovariance}. This effect aligns well with the objective of MSIW, effectively enhancing generalization.

\subsection{Ablation Study}

We want to clarify that the same settings that mentioned in Section \ref{s42_implementation_details} were applied across all ablation studies.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/FeatureProjection.png}
    \caption{T-SNE visualization \cite{van2008visualizing} of features for different modalities (a) before and (b) after test-time style projection from Stage1.}
    \label{fig:FeatureProjection}
\end{figure}

\begin{table}[t]
    \centering
    \footnotesize
    \setlength\tabcolsep{5.0pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.85} % Tighter
    \begin{tabular}{c|c|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{3}{*}{Setting}} & \multicolumn{1}{c|}{\multirow{3}{*}{Configuration}} & \multicolumn{4}{c}{Training Modalities (R, U)} \\ \cline{3-6}
     & &  \multicolumn{2}{c|}{Seen (R, U)} & \multicolumn{2}{c}{Unseen (C, D)} \\ \cline{3-6}
     & &  DSC & mIoU & DSC & mIoU \\
     \hline
     S0 & baseline & 70.77 & 61.59 & 3.92 & 2.33 \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{S1}} &  \multicolumn{1}{c|}{\multirow{2}{*}{+ MASP \scriptsize{(w/o $\mathcal{L}_{\text{con}}$)}}} & 73.17 & 64.53 & 16.31 & 12.38 \\
      & & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.40)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.94)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+12.39)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.05)}}} \\
      \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{S2}} &  \multicolumn{1}{c|}{\multirow{2}{*}{+ MASP \scriptsize{(w $\mathcal{L}_{\text{con}}$)}}} & \textcolor{red}{\textbf{\underline{73.87}}} & \textcolor{red}{\textbf{\underline{65.23}}} & 16.64 & 12.90 \\
      & & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.10)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.64)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+12.72)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+10.57)}}} \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{S3}} &  \multicolumn{1}{c|}{\multirow{2}{*}{+ MSIW}} & 73.50 & 64.87 & \textcolor{blue}{\textbf{\textit{18.94}}} & \textcolor{blue}{\textbf{\textit{15.18}}} \\
      & & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.73)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.28)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+15.02)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+12.85)}}} \\
     \hline
     \multicolumn{1}{c|}{\multirow{3}{*}{S4}} & \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{TTMG \scriptsize{(Ours)}}}} & \textcolor{blue}{\textbf{\textit{73.62}}} &  \textcolor{blue}{\textbf{\textit{65.22}}} &  \textcolor{red}{\textbf{\underline{20.96}}} &  \textcolor{red}{\textbf{\underline{16.98}}} \\
      & & \textcolor{ForestGreen}{\scriptsize{\textbf{(+2.85)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+3.63)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+17.04)}}} & \textcolor{ForestGreen}{\scriptsize{\textbf{(+14.66)}}} \\ \cline{3-6}
      & & \textbf{-0.25} & \textbf{-0.01} & \textbf{+2.02} & \textbf{+1.80} \\
     \hline
    \end{tabular}
    \caption{Ablation study of TTMG components (MASP and MSIW) on seen (R, U) and unseen (C, D) modalities. } 
    \label{tab:effectiveness_masp_msiw}
\end{table}

\noindent \textbf{Effectiveness of MASP and MSIW.} In this section, we conducted ablation studies on the (R, U) to (C, D) training scenario to demonstrate the effectiveness of MASP and MSIW, which are core components of the TTMG framework. As listed in Table \ref{tab:effectiveness_masp_msiw}, our approach (S4) exhibited the best performance on unseen modalities. The most notable result is that the application of MASP regardless of $\mathcal{L}_{\text{con}}$ provides the significant DSC improvement of 12.39\% and 12.72\% compared to baseline (S0). Compared to S1 and S2, applying $\mathcal{L}_{\text{con}}$ achieves performance improvement in both seen and unseen modalities because the content of the feature can be maintained after the projection. Additionally, applying MSIW alone (S3) resulted in a significant DSC and mIoU improvement of 15.02\% and 12.85\%, respectively, for unseen modalities, underscoring MSIW's role in reducing modality-sensitive information. Consequently, TTMG (S2 + S3), which employs the dual utilization of MASP and MSIW with $\mathcal{L}_{\text{con}}$, performs significantly better generalization ability on unseen modality datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/Correlation_Matrix.png}
    \caption{Visualization of covariance matrix extracted from baseline and TTMG.}
    \label{fig:FeatureCovariance}
\end{figure}

\begin{table}[t]
    \centering
    \footnotesize
    \setlength\tabcolsep{7.0pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.85} % Tighter
    \begin{tabular}{c|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{3}{*}{Number of Style Bases $K_{m}$}} & \multicolumn{4}{c}{Training Modalities (R, U)} \\ \cline{2-5}
     & \multicolumn{2}{c|}{Seen (R, U)} & \multicolumn{2}{c}{Unseen (C, D)} \\ \cline{2-5}
     & DSC & mIoU & DSC & mIoU \\
     \hline
     $K_{m} = 4$ & 73.21 & 64.46 & \textcolor{blue}{\textbf{\textit{18.93}}} & \textcolor{blue}{\textbf{\textit{15.17}}} \\
     \textbf{$K_{m} = 8$ \scriptsize{(Ours)}} & \textcolor{blue}{\textbf{\textit{73.62}}} & \textcolor{blue}{\textbf{\textit{65.22}}} & \textcolor{red}{\textbf{\underline{20.96}}} & \textcolor{red}{\textbf{\underline{16.98}}} \\
     $K_{m} = 16$ & \textcolor{red}{\textbf{\underline{75.02}}} & \textcolor{red}{\textbf{\underline{66.34}}} & 18.04 & 14.27 \\
     $K_{m} = 32$ & 73.13 & 64.57 & 17.65 & 12.76 \\
     $K_{m} = 64$ & 72.21 & 63.43 & 14.77 & 70.72 \\
     \hline
    \end{tabular}
    \caption{Effect of the number of style bases \( K_{m} \) on TTMG’s generalization performance across seen (R, U) and unseen (C, D) modalities.}
    \label{tab:number_of_style_bases}
\end{table}

\noindent \textbf{Number of Style Bases.} In this section, we investigate the impact of the number of style bases $K_{m}$ on the generalization ability of TTMG in the (R, U) to (C, D) training scenario. As shown in Table \ref{tab:number_of_style_bases}, TTMG achieves the highest generalization performance on unseen modality datasets with $K_{m} = 8$, while also maintaining strong performance on seen modality datasets. With a lower number of style bases, such as $K_{m} = 4$, the limited bases fail to capture the complex style distribution of medical image datasets, resulting in lower performance. Conversely, as $K_{m}$ increases to 16, 32, and 64, performance on seen modalities improves; however, generalization on unseen modalities decreases, likely due to overfitting to the seen modality style bases. Based on these results, we set $K_{m} = 8$ for all experiments. \vspace{-0.25cm}