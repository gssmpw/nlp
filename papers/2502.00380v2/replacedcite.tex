\section{Related Work}
\label{related-work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluating Clustering Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Assessing the performance of clustering algorithms is inherently challenging due to the unsupervised nature of the task. Unlike classification problems, clustering generally lacks ground truth labels in real-world applications. As a result, evaluation relies on metrics that either estimate the quality of clusters based on their internal structure or compare clustering results against external references when available.

Metrics such as the Silhouette Score ____, Calinski-Harabasz Index ____, and Davies-Bouldin Index ____ are commonly used to evaluate clustering without reference to external labels. The Silhouette Score defined as $\frac{a-b}{\text{max}\left(a,b\right)}$, for example, measures how well-separated clusters are by comparing the average intra-cluster distance $a$ to the nearest inter-cluster distance $b$. This metric ranges from \text{-}1 indicating incorrect clustering to 1 indicating dense, well-separated clusters, with scores near 0 suggesting overlapping or indistinct clusters.

In the absence of real-world labeled data, artificial clustering problems can be generated from classification datasets or simulated data. By controlling the underlying structure and labels of synthetic datasets, researchers can benchmark algorithms under varying conditions, such as noise levels, cluster overlap, or dimensionality. This allows for the use of external validation metrics, like the Rand Index (RI), which measures the agreement between pairs of samples, treating cluster assignments as equivalent regardless of permutation, and the Adjusted Rand Index (ARI) which adjusts the RI to account for chance. The ARI is bounded between \text{-}0.5 for discordant clustering to 1.0 for perfect agreement clustering, having a value close to 0 for random assignment of clusters ____.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clustering Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Traditional clustering methods are divided into hierarchical and partitional strategies. Hierarchical clustering builds a hierarchy of clusters, which can be visualized through dendrograms. These methods are further divided into agglomerative (bottom-up) and divisive (top-down) approaches. Agglomerative algorithms begin with each instance as an individual cluster, merging them iteratively based on a measure of dissimilarity, called linkage, such as single-linkage, complete-linkage, and Ward-linkage ____. In contrast, divisive methods like DIANA ____ start with a single cluster containing all instances, splitting them iteratively.

Partitional clustering methods optimize a criterion function, often requiring the number of clusters as input. These methods include distance-based algorithms (e.g., K-Means ____, and Affinity Propagation ____), density-based algorithms (e.g., DBSCAN ____, and HDBSCAN ____), and grid-based approaches like CLIQUE ____. For a comprehensive review on clustering algorithms and their taxonomy, the reader is refereed to ____.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{High-Dimensional Clustering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Clustering high-dimensional data presents additional challenges, including the curse of dimensionality and the sparsity of meaningful distances. To address these issues, specialized techniques have been proposed:

\textbf{Subspace and Projected Clustering:} Algorithms like CLIQUE ____, and PROCLUS ____ identifies clusters within specific subspaces of the feature space, while methods like DOC ____ and K-Means Projective Clustering ____, further assign data points to unique clusters in lower-dimensional subspaces. Despite their utility, these methods often struggle with overlapping clusters and require careful parameter tuning.

\textbf{Ensemble Clustering:} Ensemble (or consensus) clustering aggregates multiple clustering solutions derived from various algorithms or projections to enhance robustness and accuracy ____. By combining diverse partitions, ensemble methods mitigate the challenges posed by high-dimensional data and leverage complementary information.

\textbf{Projection-Based Methods:} Projection-based clustering approaches reduce the dimensionality of data while retaining meaningful cluster structures. Methods such as PCA ____ and t-SNE ____ are frequently used for dimensionality reduction before clustering. Recent techniques include Latent Low-Rank Representation (LatLRR) ____, Robust LatLRR (RLLRR) ____, and Iterative Reweighted Frobenius norm Regularized Latent Low-Rank (IRFLLRR) ____ which adaptively project data into subspaces optimized for clustering. While projection-based methods enhance interpretability and scalability, they may require careful parameter selection and are sometimes limited by the quality of the projection.

While some algorithms can be clearly label as applying one of those techniques, many clustering methods combine more than one technique to tackle the challenges of clustering high-dimensional data such as Spectral Clustering by Subspace Randomization and Graph Fusion (SC-SRGF) ____ or Hierarchical High-Dimensional Unsupervised Active Learning Method (HiDUALM) ____ which combines techniques from Subspace Clustering and Ensemble clustering. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%