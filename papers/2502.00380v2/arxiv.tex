\documentclass{article}

\usepackage[dvipsnames]{xcolor}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
% \usepackage{subfig}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{longtable}
% \usepackage{tabularx}
\usepackage{xtab}
\usepackage{array}
\usepackage{listings}
% \usepackage{subcaption}

\usepackage{ulem}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} 
 
% \usepackage[nohyperref]{icml2025}
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\input{macros}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{stmaryrd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OUR packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{color}
\usepackage{multirow}
\usepackage{nccmath, amssymb}
\usepackage{bbold}
\usepackage{thmtools,thm-restate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{nicefrac}
\newcommand\boldred[1]{\textcolor{BrickRed}{\textbf{#1}}}
% \usepackage{lscape} 

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% also comment out the lines below, I have only included them so we can see each others comments
% \setlength{\marginparwidth}{2cm}
% \setlength{\marginparsep}{0cm}
% \usepackage[textsize=tiny]{todonotes}
% \usepackage{ulem}

\newcommand{\karim}[2][]{\todo[color=magenta!20,#1]{{\bf KL:} #2}}
\newcommand{\kat}[2][]{\todo[color=green!20,#1]{{\bf Kat:} #2}}
\newcommand{\KL}[1]{\textcolor{magenta}{#1}}
\newcommand{\Kat}[1]{\textcolor{purple}{#1}}
\newcommand{\bruno}[2][]{\todo[color=blue!40,#1]{{\bf BB:} #2}}
\newcommand{\BB}[1]{\textcolor{blue}{#1}}

% \paperwidth=\dimexpr \paperwidth + 6cm\relax
% \oddsidemargin=\dimexpr\oddsidemargin + 6cm\relax
% \evensidemargin=\dimexpr\evensidemargin + 6cm\relax
% \marginparwidth=\dimexpr \marginparwidth + 6cm\relax



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{\ourmethod: A Scalable and Interpretable Clustering Framework for High-Dimensional Data}
\title{\ourmethod: A Scalable and Interpretable Clustering Framework for High-Dimensional Data}

\date{01/02/2025}


\author{Bruno Belucci\footnote{CEREMADE, Université Paris Dauphine-PSL, Paris, France}, Karim Lounici\footnote{CMAP, École Polytechnique, Palaiseau, France} and Katia Meziani\footnote{CEREMADE, Université Paris Dauphine-PSL, Paris, France}}












\begin{document}


%\twocolumn[


\maketitle


\vskip 0.3in
%]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Clustering high-dimensional data poses significant challenges due to the curse of dimensionality, scalability issues, and the presence of noisy and irrelevant features.
We propose Consensus Hierarchical Random Feature (\ourmethod), a novel clustering method designed to address these challenges effectively. \ourmethod leverages random feature selection to mitigate noise and dimensionality effects, repeatedly applies K-Means clustering in reduced feature spaces, and combines results through a unanimous consensus criterion. This iterative approach constructs a cluster assignment matrix, where each row records the cluster assignments of a sample across repetitions, enabling the identification of stable clusters by comparing identical rows. Clusters are organized hierarchically, enabling the interpretation of the hierarchy to gain insights into the dataset. \ourmethod is computationally efficient with a running time comparable to K-Means, scalable to massive datasets, and exhibits robust performance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and OPTICS. Experimental results on synthetic and real-world datasets confirm the method's ability to reveal meaningful patterns while maintaining scalability, making it a powerful tool for high-dimensional data analysis.



\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




Clustering is a cornerstone of unsupervised learning that involves partitioning data into groups or clusters based on similarity. It is a vital tool in various domains, including computer vision \cite{chao2021}, bioinformatics \cite{rezaul2020clustering}, and natural language processing \cite{li2021research}, where identifying patterns in data is essential. With the rise of modern applications, datasets have become increasingly high-dimensional, often containing a large number of samples \((n)\) and features \((p)\). This growth poses significant challenges in clustering high-dimensional data. 

High-dimensional datasets suffer from the well-known "curse of dimensionality." As the dimensionality \(p\) increases, the relevant information often lies in a low-dimensional subspace, with the remaining dimensions contributing predominantly to noise. Consequently, data points tend to become equidistant in high-dimensional space, rendering traditional distance-based clustering algorithms, such as K-Means, less effective \cite{beyer1999nearest}. Specifically, the Euclidean distance metric loses its discriminative power, resulting in poor clustering performance. Another critical challenge is scalability: traditional clustering methods, originally designed for low-dimensional or small datasets, often struggle with high computational and memory demands when applied to high-dimensional data settings \cite{steinbach2004challenges,assent2012clustering,zimek2012survey,mahdi2021scalable}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{related-work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluating Clustering Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Assessing the performance of clustering algorithms is inherently challenging due to the unsupervised nature of the task. Unlike classification problems, clustering generally lacks ground truth labels in real-world applications. As a result, evaluation relies on metrics that either estimate the quality of clusters based on their internal structure or compare clustering results against external references when available.

Metrics such as the Silhouette Score \cite{silhouette1987}, Calinski-Harabasz Index \cite{calinsky1974}, and Davies-Bouldin Index \cite{davies1979} are commonly used to evaluate clustering without reference to external labels. The Silhouette Score defined as $\frac{a-b}{\text{max}\left(a,b\right)}$, for example, measures how well-separated clusters are by comparing the average intra-cluster distance $a$ to the nearest inter-cluster distance $b$. This metric ranges from \text{-}1 indicating incorrect clustering to 1 indicating dense, well-separated clusters, with scores near 0 suggesting overlapping or indistinct clusters.

In the absence of real-world labeled data, artificial clustering problems can be generated from classification datasets or simulated data. By controlling the underlying structure and labels of synthetic datasets, researchers can benchmark algorithms under varying conditions, such as noise levels, cluster overlap, or dimensionality. This allows for the use of external validation metrics, like the Rand Index (RI), which measures the agreement between pairs of samples, treating cluster assignments as equivalent regardless of permutation, and the Adjusted Rand Index (ARI) which adjusts the RI to account for chance. The ARI is bounded between \text{-}0.5 for discordant clustering to 1.0 for perfect agreement clustering, having a value close to 0 for random assignment of clusters \cite{ari1985}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clustering Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Traditional clustering methods are divided into hierarchical and partitional strategies. Hierarchical clustering builds a hierarchy of clusters, which can be visualized through dendrograms. These methods are further divided into agglomerative (bottom-up) and divisive (top-down) approaches. Agglomerative algorithms begin with each instance as an individual cluster, merging them iteratively based on a measure of dissimilarity, called linkage, such as single-linkage, complete-linkage, and Ward-linkage \cite{agglomerative1984}. In contrast, divisive methods like DIANA \cite{diana1990} start with a single cluster containing all instances, splitting them iteratively.

Partitional clustering methods optimize a criterion function, often requiring the number of clusters as input. These methods include distance-based algorithms (e.g., K-Means \cite{kmeans_complexity}, and Affinity Propagation \cite{affinitypropagation2007}), density-based algorithms (e.g., DBSCAN \cite{dbscan1996}, and HDBSCAN \cite{hdbscan2013}), and grid-based approaches like CLIQUE \cite{clique1999}. For a comprehensive review on clustering algorithms and their taxonomy, the reader is refereed to \cite{dataclustering1999, surveyclustering2005, reviewclustering2017, comprehensivesurvey2022}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{High-Dimensional Clustering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Clustering high-dimensional data presents additional challenges, including the curse of dimensionality and the sparsity of meaningful distances. To address these issues, specialized techniques have been proposed:

\textbf{Subspace and Projected Clustering:} Algorithms like CLIQUE \cite{clique1999}, and PROCLUS \cite{proclus2000} identifies clusters within specific subspaces of the feature space, while methods like DOC \cite{DOC2002} and K-Means Projective Clustering \cite{kmeansprojective2004}, further assign data points to unique clusters in lower-dimensional subspaces. Despite their utility, these methods often struggle with overlapping clusters and require careful parameter tuning.

\textbf{Ensemble Clustering:} Ensemble (or consensus) clustering aggregates multiple clustering solutions derived from various algorithms or projections to enhance robustness and accuracy \cite{clusterensembles2003}. By combining diverse partitions, ensemble methods mitigate the challenges posed by high-dimensional data and leverage complementary information.

\textbf{Projection-Based Methods:} Projection-based clustering approaches reduce the dimensionality of data while retaining meaningful cluster structures. Methods such as PCA \cite{pca1901} and t-SNE \cite{tsne2008} are frequently used for dimensionality reduction before clustering. Recent techniques include Latent Low-Rank Representation (LatLRR) \cite{llr2013}, Robust LatLRR (RLLRR) \cite{RLLRR2014}, and Iterative Reweighted Frobenius norm Regularized Latent Low-Rank (IRFLLRR) \cite{irfllrr2023} which adaptively project data into subspaces optimized for clustering. While projection-based methods enhance interpretability and scalability, they may require careful parameter selection and are sometimes limited by the quality of the projection.

While some algorithms can be clearly label as applying one of those techniques, many clustering methods combine more than one technique to tackle the challenges of clustering high-dimensional data such as Spectral Clustering by Subspace Randomization and Graph Fusion (SC-SRGF) \cite{sc-srgf2020} or Hierarchical High-Dimensional Unsupervised Active Learning Method (HiDUALM) \cite{hidualm2024} which combines techniques from Subspace Clustering and Ensemble clustering. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We propose a novel clustering method, named Consensus Hierarchical Random Feature Clustering (\ourmethod) which integrates principles from hierarchical clustering, projection-based clustering, and ensemble clustering. Our method is designed to address the limitations of existing techniques in high-dimensional settings, with a focus on scalability, robustness to noise, and interpretability.

The key steps of our method are as follows:

\begin{enumerate}
    \item \textbf{Feature sub-sampling:} Randomly sample subsets of features and apply any clustering method\footnote{In our experiments, we focus on $K$-means as it is really fast and not memory intensive. Applying $K$-means to all the randomly sampled subsets proved to be among the best methods in our benchmark.} to the resulting low-dimensional representations.
    \vspace{-0.2cm}
   
    \item \textbf{Cluster Assignment Matrix:} Repeat this process multiple times to construct a clustering matrix recording the cluster to which each sample has been assigned across all the repetitions. 
        \vspace{-0.2cm}

    \item \textbf{Unanimous Consensus:} Identify clusters by grouping samples consistently clustered together across all repetitions. Select a medoid for each of these clusters.
        \vspace{-0.2cm}

    \item \textbf{Iterative Agglomeration:} Repeat the previous steps on the set of medoids of the previously created clusters to merge them iteratively until no further merging is possible.
\end{enumerate}

This approach addresses the challenges of high-dimensional clustering as follows:
\begin{itemize}
    \item \textbf{Curse of Dimensionality:} By leveraging random feature selection and repeated consensus, our method significantly mitigates the effects of high dimensionality while preserving meaningful information.
   % \vspace{-0.2cm}
    \item \textbf{Scalability:} The random sampling and iterative design enable efficient processing of high-dimensional and massive datasets as we essentially perform a low-dimensional K-Means at each repetition.
   % \vspace{-0.2cm}
    \item \textbf{Interpretability:} Our novel hierarchical unanimous consensus approach systematically organizes data into nested clusters, enabling an in-depth exploration of multi-level clustering structures and uncovering relationships across different levels of granularity.
   % \vspace{-0.2cm}
   
    \item \textbf{Efficient Consensus Scheme.} Our approach differs from traditional ensemble clustering, which typically involves solving a challenging optimization problem to derive a consensus from multiple distinct clustering results. In contrast, our hierarchical method eliminates the need for a post-clustering optimization step. Instead, it aggregates the nodes of the constructed (incomplete) hierarchical clustering tree, a computationally inexpensive operation.
\end{itemize}

Our numerical experiments demonstrate that our method efficiently scales to extremely high-dimensional settings where SOTA techniques such as HDBSCAN \cite{hdbscan2013}, OPTICS \cite{optics1999}, and SC-SRGF \cite{sc-srgf2020} fail to compute.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\ourmethod Algorithm}
\label{proposed-algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Detailed Method}
\label{sub-sec:method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



Below, we provide a detailed description of our method's implementation and its hyperparameters. A pseudocode implementation can be found in \cref{alg:CoHiRF} and a visual representation of the iterative step of our method is described in \cref{fig:rec-clustering}.

\begin{algorithm}[t]
\caption{\ourmethod Algorithm}
\label{alg:CoHiRF}
\begin{algorithmic}

\STATE \textbf{Input:} Data $\bbX$, number of sampled features $q$, number of repetitions $R$, number of clusters $C$
\STATE \textbf{Output:} Final partition of samples $\{\mathcal{C}^*_{c}\}_c$

\STATE \textbf{Initialize:} $X \gets \bbX$, $n_{\text{prev}} \gets 0$, $n_{\text{curr}} \gets n$
\WHILE{$n_{\text{curr}} \neq n_{\text{prev}}$}
    \STATE $\text P \gets \emptyset$ \COMMENT{Matrix to store K-Means labels across repetitions}
    \FOR{$r = 1$ \textbf{to} $R$}
        \STATE Randomly sample $q$ features from $X$
        \STATE Perform K-Means on the sampled features, obtaining labels $\text p_r \in \{1,\ldots,C\}^{n_{\text{curr}}}$
        \STATE Append labels $\text p_r$ to $\text P$
    \ENDFOR
    \STATE Assign a unique code to each row of $\text P$ 
    \STATE Update $n_{\text{prev}} \gets n_{\text{curr}}$ 
    \STATE Update $n_{\text{curr}} \gets \text{number of unique codes in } \text P$ 
    \STATE Assign codes to each sample in $X$ and their parent samples to form partition $\mathcal{C}_j$ 

\STATE \textbf{Medoid Computation:}
    \FOR{$k = 1$ \textbf{to} $n_{\text{curr}}$}
        \STATE Compute the medoid $\bx_{k}$ of the $k$-th cluster: 
        \STATE Append the medoid to $X$ 
        \STATE Assign all other samples in the cluster as children of the medoid
    \ENDFOR
\ENDWHILE

\STATE \textbf{Return:} $\{\mathcal{C}^*_{c}\}_c$ \COMMENT{Final clusters after stabilization}

\end{algorithmic}
\end{algorithm}


\textbf{Input.} Consider the data set $\dset = \left\lbrace \bx_1,\ldots,\bx_{\samplesize} \right\rbrace$. We introduce the data matrix

$$
\bbX = 
\left[ 
\begin{array}{c}
    \bx_1^{\top}\\
     \vdots\\
     \bx_{\samplesize}^{\top}
\end{array}
\right]\in \R^{\samplesize \times \dimsize}.
$$




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Hyperparameters.}
We denote by $\llbracket n_1, n_2 \rrbracket=\{ n_1, n_1 + 1, \dots, n_2 \}$ the set of all integers.  Our methods depends on the following hyperparameters: 
\begin{itemize}
    \item The number of randomly sampled features : $q\in\llbracket2,\overline{q}\rrbracket$  with
    $\overline{q}=\min\left(30,\dimsize-1\right)$
    \vspace{-0.2cm}
    \item Number of repetitions: $R\in \llbracket  2, 10 \rrbracket$
    \vspace{-0.2cm}
    \item Number of clusters used in the internal clustering step:  $C\in  \llbracket  2, 10\rrbracket$. While we use K-Means by default, any other clustering algorithm  could be employed. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{[Initialization (Step $e=0$)]} We start from the whole data set and initialize
    $$
    \begin{cases}
    \samplesize^{(0)} = \samplesize\,&\\
      \bx_i^{(0)} = \bx_i,&\forall i\in\llbracket 1, \samplesize^{(0)} \rrbracket\\
        e=1
    \end{cases}
    $$

\textbf{[Step $e$]} (\cref{fig:rec-clustering}) From Step $(e-1)$, we recover a data set 
    $$\mathcal D_{n^{(e-1)}} = \left\lbrace \bx_i, i\in\llbracket1,n^{(e-1)}\rrbracket\right\rbrace$$
    \begin{enumerate}
        \item[$(i.)$] We build the sample matrix from the data set $\mathcal D_{n^{(e-1)}}$           
        $$
        \bbX^{(e)} = 
        \left[ 
        \begin{array}{c}
            (\bx_1^{(e-1)})^{\top}\\
             \vdots\\
             (\bx_{\samplesize^{(e-1)}}^{(e-1)})^{\top}
        \end{array}
        \right]\in \R^{\samplesize^{(e-1)} \times \dimsize}.
        $$
        
    
        \item[$(ii.)$] We first construct $R$ random submatrices by repeatedly sampling a subset of  $q$ features and extracting the corresponding submatrix from  $\bbX^{(e)}$:
           $$
            \bbX_{q,1}^{(e)} =  
            \left[ 
            \begin{array}{c}
                (\bx_1^{(e-1)})^\top\\
                 \vdots\\
                 (\bx_{\samplesize^{(e-1)}}^{(e-1)})^\top
            \end{array}
            \right]\in \R^{\samplesize^{(e-1)} \times q}
            $$
       This process is repeated $R$ times, yielding the random data matrices  $\bbX_{q,1}^{(e)}, \ldots, \bbX_{q,R}^{(e)} \in \R^{\samplesize^{(e-1)} \times q}$.    
            
     
        \item[$(iii)$] For each $r \in \llbracket 1, R \rrbracket$, we apply the standard K-Means algorithm to $\bbX_{q,r}^{(e)}$ with $\clusternum$ clusters. The results of the clustering procedure across all $R$ repetitions are collected in the matrix  
        $$
        \text P^{(e)} = \Big[\text p^{(e)}_{i,r}\Big]_{i,r} \in \left\{1, \ldots, C \right\}^{n^{(e-1)} \times R},
        $$ 
        where $p^{(e)}_{i,r}$ represents the cluster assigned to the $i$-th observation during the $r$-th repetition.  Since the cluster labels are not inherently identifiable, we adopt the following convention: cluster $c = 1$ is assigned to the cluster containing the first sample $\bx_1^{(0)}$. The cluster $c = 2$ corresponds to the next sample $\bx_i^{(0)}$ with the smallest index $i$ that is not clustered together with $\bx_1^{(0)}$ during the first repetition ($r = 1$), and so on for the following clusters $c \in \llbracket 2, C \rrbracket$.
 
        \item[$(iv.)$] We denote by $\samplesize^{(e)}$ the number of distinct rows in $\text P^{(e)}$. We partition the samples $\mathcal{D}_{n^{(e-1)}}$ of size $\samplesize^{(e-1)}$ into $\samplesize^{(e)}$ disjoint sets, denoted $\mathcal{C}_{k}^{(e)}$, $k\in\llbracket 1, \samplesize^{(e)} \rrbracket$, which consist of observations sharing identical row vectors $\text p^{(e)}_{i, \cdot} \in \{1,\ldots,C\}^{R}$. Concretely, we group together all the samples $\bx_i^{(e-1)}$ that have been clustered together throughout all the $R$ repetitions. Note that:

        \begin{equation*}
        \begin{split}
            & \samplesize^{(e)} \leq \min \left( \samplesize^{(e-1)}, C^{R} \right), \quad \text{and} \\
            & \sum_{k=1}^{\samplesize^{(e)}} |\mathcal{C}_{k}^{(e)}| = \samplesize^{(e-1)}.
        \end{split}
        \end{equation*}



  
        \item[$(v.)$] For each $\llbracket 1, \samplesize^{(e)} \rrbracket$, we select the \textbf{medoid} denoted $\bx_{k}^{(e)}\in\R^{\dimsize}$, for each cluster $\mathcal{C}_{k}^{(e)}$. Let

        \begin{multline*}
        \bx_{k}^{(e)} =\\ \argmin_{\bx_i^{(e-1)} \in \mathcal{C}_{k}^{(e)}} \left\lbrace \sum_{\bx_j^{(e-1)} \in \mathcal{C}_{k}^{(e)}} \left\lvert \langle \bx_i^{(e-1)}, \bx_j^{(e-1)} \rangle \right\rvert \right\rbrace.
        \end{multline*}

        This expression selects the sample $\bx_i^{(e-1)}$ within the cluster $\mathcal{C}_{k}^{(e)}$ that minimizes the sum of the absolute inner products with all other points in the same cluster, which defines the medoid of $\mathcal{C}_{k}^{(e)}$.
    \end{enumerate}


 

 
  
\textbf{[Repeat until]} $\samplesize^{(e^*)} = \samplesize^{(e^*-1)}$
    
    
\textbf{[Final step $e^*$]}
    At the final step $e^*$, we collect the sets $\mathcal{C}_{k}^{(e^*)}$ for $k = \llbracket1, \samplesize^{(e^*)}\rrbracket$ corresponding to a cluster at step $e^*$. The final number of clusters is given by $C^* = \samplesize^{(e^*)}$. We then reconstruct the final $C^*$ clusters by tracing the evolution of the sets through all the previous steps. Specifically, for each final cluster $c \in \llbracket 1, C^* \rrbracket$, we have:
    
    \hfuzz 10pt  % ignore following overfull
    \begin{multline*}
    \mathcal{C}_c^* =\\ \bigcup_{k_{e^*} \in \mathcal{C}_c^{(e^*)}} \left( \bigcup_{k_{e^*-1} \in \mathcal{C}_{k_{e^*}}^{(e^*-1)}} \left( \cdots \left( \bigcup_{k_1 \in \mathcal{C}_{k_2}^{(1)}} \mathcal{C}_{k_1}^{(1)} \right) \right) \right).
    \end{multline*}
    \hfuzz 0.1pt

    This means that each final cluster $\mathcal{C}_{c}^{*}$ is formed by the union of clusters across all steps $e = 1, 2, \dots, e^*$. This process allows us to reconstruct the final clusters by following the evolution of each observation across all steps of the algorithm. Each final cluster $\mathcal{C}_{c}^{*}$ contains all the observations that were grouped together throughout the iterations, with each observation $\bx_i$ being assigned to the final cluster $c$  in which it converged after the algorithm completed.
%\end{itemize}

%   \vspace{-0.5cm} 
\begin{figure}[http!]
   % \centering
    \includegraphics[height=7cm, width=\linewidth, keepaspectratio]{images/Step.pdf}
    \caption{Illustration of one iteration of the \ourmethod algorithm with $q=2$, $C=2$ and $R=4$. At the beginning of step $e$, we start with $n^{(e-1)}$ samples (the medoids inherited from the previous iteration). For each repetition $r\in [R]$, subsample $q$ features at random and cluster the $n^{(e-1)}$ samples using K-Means with $C=2$, then Identify clusters by grouping together the samples consistently clustered together across all repetitions. Obtain that way $n^{(e)}$ newly formed clusters and finally choose a medoid for each of the formed clusters.}
    \label{fig:rec-clustering}
    \vskip -0.2cm
\end{figure}
 




    


This novel clustering procedure is best described as an agglomerative hierarchical iterative consensus-based method, where at each step, the algorithm merges clusters based on their consensus across multiple random repetitions  




The process converges when no further grouping is possible, meaning each "surviving" sample becomes its own unique representative. However, at the final step, the method reconstructs the final clusters by tracing the evolution of each observation through all previous steps. This merging process reveals a hierarchical structure, where clusters formed at earlier stages are fused to form the final clusters. See \cref{fig:hierarchy-concept} for a concept illustration.


This flexibility allows the algorithm to adapt to varying data structures, making it particularly useful for exploratory analysis. Furthermore, the hierarchical nature of the procedure facilitates interpretability, providing insights into how the clusters evolved over iterations and allowing for an intuitive understanding of the relationships between data points across different levels of granularity. See Appendix \ref{app:hierarchy} for an experience illustrating this aspect on the \texttt{iris} dataset. 





\ourmethod does not require knowledge of the true number of clusters $C^*$. Our method stops when no further grouping can be made, meaning that the number of clusters $C^*$ is effectively identified during the process itself, removing the necessity of defining it in advance. Notably, there is no direct correspondence between parameter $C$ in our method and $C^* $; we may use different values of $C$, either smaller or larger than $C^* $, in each iteration. When the clustering problem has a sufficiently high signal-to-noise ratio, \ourmethod accurately recovers the true number of clusters $C^*$.



\begin{figure}[http!]
    \vskip -0.2in
    \begin{center}
    \includegraphics[width=6.7cm, height=8cm]{images/DessinALgo1.pdf}
     %\includegraphics[height=8cm, width=\linewidth, keepaspectratio]{images/Algo.pdf}
    \caption{Concept representation for the hierarchical clustering structure built by \ourmethod. The hierarchy is built iteratively by our method without requiring solving any additional expensive optimization problem. The final step of \ourmethod is thus immediate.}
    \label{fig:hierarchy-concept}
    \end{center}
    \vskip -0.2in
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Complexity Analysis}
\label{sub-sec:complexity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%
Our algorithm's running time is dominated by the number of times we run K-Means and the calculation of similarities to determine the closest sample to every other sample in the cluster. We use Lloyd's algorithm for K-Means to cluster $\samplesize$ samples of dimension $\dimsize$ into $\clusternum$ clusters with a maximum of $I$ iterations. This has a running time on the order of $O\left(\samplesize\,\dimsize\,\clusternum\,I\right)$ \cite{kmeans_complexity}. Since we repeat K-Means $R$ times, the total running time for this step is $O\left(\samplesize\,\dimsize\,\clusternum\,I\,R\right)$. As for calculating the similarities, we assume that after $R$ repetitions of K-Means, the consensus step of \ourmethod has found $K$ clusters, with the samples uniformly distributed across them. Therefore, for each cluster, the number of points will be approximately $\samplesize/K$.
To calculate the $K$ medoids, we have a theoretical running time of order $O\left(\left(\frac{\samplesize}{K}\right)^{2}\,\dimsize\,K\right)=O\left(\frac{\samplesize^{2}}{K}\,\dimsize\right)$. Thus, the cost of the first iteration of our algorithm is $O\left(\samplesize\,\dimsize\,\clusternum\,I\,R\right)+O\left(\frac{\samplesize^{2}}{K}\,\dimsize\right)$. 

Note that this is the theoretical running time of only one iteration, which starts with $\samplesize$ samples. The number of samples for the following iterations reduces drastically after the first iteration. Typically, in our experience, we observed that after just one iteration, the number of (medoid) samples $\samplesize^{(1)} = K$ is on the order of tens, and at most a few hundred, which greatly speeds up our algorithm. This reduction allows us to compute the exact medoids more efficiently in the following iterations, resulting in a much faster overall computation time. If one does not want to use exact medoids to further reduce the theoretical time complexity, it is entirely possible to replace exact medoids with approximate medoids or centroids, which does not significantly affect the statistical performance, as demonstrated in the experiments in Section \ref{sec:exp}. \\
As for the space complexity, it is dominated by the storage of the data, $O\left(\samplesize\dimsize\right)$, and the storage of the similarity matrix, $O\left(\left(\frac{\samplesize}{K}\right)^{2}\right)$ during the first step.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Every experiment was performed in a machine with an Intel(R) Xeon(R) CPU E5-2630 v4 and a maximum of 126~Gb of memory. Our code for running experiments can be found in \cref{app:code}.

\textbf{Compared methods.} We compared the performance of the following models: \ourmethod (ours), K-Means \cite{kmeans_complexity}, Affinity Propagation \cite{affinitypropagation2007}, CLIQUE \cite{clique1999}, Spectral Clustering \cite{spectralclustering2000}, Mean Shift \cite{meanshift2002}, IRFLLRR \cite{irfllrr2023}, K-Means Projective Clustering (K-Means Proj.) \cite{kmeansprojective2004}, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) \cite{dbscan1996}, Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) \cite{hdbscan2013}, PROCLUS \cite{proclus2000}, SC-SRGF \cite{sc-srgf2020}, and four types of Agglomerative Clustering models with different linkage methods: Single Linkage, Complete Linkage, Average Linkage, and Ward’s Method \cite{agglomerative1984}. Those methods were selected based on their performance and popularity in the literature, and the availability of their source code. All the algorithms were implemented in Python.
Specifically, we translated IRFLLRR, SC-SRGF, and K-Means Proj. from Matlab to Python and updated the syntax of PROCLUS from Python 2 to Python 3. Note that the tables and figures presented below include only the models that demonstrated competitive performance in our experiments.


\textbf{Hyperparameters optimization.} We evaluated each algorithm using its optimized hyperparameters to achieve the best possible performance. By default, we used 100 trials of the Tree-structured Parzen Estimator (TPE) for hyperparameter optimization \cite{tpe2023}, unless a different optimization method was explicitly recommended in the original paper or in the algorithm’s official package implementation. The hyperparameter search spaces for each algorithm are detailed in \cref{app:search-space}. The objective of the hyperparameter optimization routine was to maximize the Adjusted Rand Index \cite{ari1985} for the real-world datasets.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scalability}
\label{sub-sec:scalability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[http!]
    \centering
    \includegraphics[width=\columnwidth]{images/time_n-features_n-samples14427_better.pdf}
    \caption{Running time of the tested algorithms for 14427 samples. Because of memory constraints HDBSCAN could not run with more than 4163 features.}
    \label{fig:time-features}
    % \vskip -0.2in
\end{figure}

\begin{figure}[http!]
    \centering
    \includegraphics[width=\columnwidth]{images/time_n-samples_n-features14427_better.pdf}
    \caption{Running time of the tested algorithms for 14427 features. Because of memory constraints and/or time constraints HDBSCAN could not run with more than 4163 features and OPTICS, Affinity Propagation and SC-SRGF with more than 14427 features.}
    \label{fig:time-samples}
    \vskip -0.2in
\end{figure}

We evaluated the scalability of the tested models with respect to both the number of features and the number of samples. To this end, we generated datasets with the number of samples (\samplesize) and number of features (\dimsize) varying in a logarithmic grid of six values: [100, 347, 1202, 4163, 14427, 50000]. The cluster centers are drawn uniformly at random from the set of vertices of a $\dimsize$-dimensional hypercube, with edge length $\Delta=100$ (i.e.,the inter-cluster distance), creating $C^*=5$ clusters. Samples within each cluster are randomly drawn from a Normal distribution with an identity covariance matrix (thus controlling the intra-cluster distance), centered at the corresponding cluster center. 

Additionally we have included two variants of \ourmethod, denoted \ourmethod-1000 and \ourmethod-Sampled. The first variant replaces the exact calculation of the medoid in \cref{sub-sec:method} step $e$ $(v.)$ by an approximated version which sub-samples the population of each cluster by a maximum of 1000 examples. This variant mitigates the quadratic running time presented in \cref{sub-sec:complexity}. The second variation samples a subset of 1024 samples in \cref{sub-sec:method} after step $e$ $(i.)$, and proceeds as usual with the rest of the algorithm by considering the unsampled samples as medoids in step $e$ $(v.)$. This technique allow us to scale to high-dimensional data settings, where the number of samples is high, decreasing the running time of \ourmethod and enabling out-of-core computation by loading partial data dynamically.

It is important to note that every model we tested in this configuration could perfectly partition the generated data, obtaining a perfect ARI of $1.0$. Therefore, we are not interested in the ARI performance, but rather in determining the scalability limits of each model.


The results can be found in \cref{fig:time-features} and \cref{fig:time-samples}. We observe that \ourmethod is significantly faster than the other algorithms, in general being 10x faster for smaller numbers of samples $\samplesize$ and features $\dimsize$, and up to 1000x faster for larger $\samplesize,\dimsize$, having a runtime comparable to $K$-Means with optimized hyperparameters. Besides, \ourmethod can run for every configuration of generated datasets, including the ones with large $\dimsize$ and $\samplesize$, which is not the case for other algorithms like HDBSCAN, SC-SRGF, and Affinity Propagation.


While CoHiRF is designed to be scalable, it has a quadratic dependence on the number of samples when computing the exact medoid \cref{sub-sec:complexity}. However, this limitation can be addressed through subsampling strategies that approximate the medoid without a full pairwise comparison, as shown in \cref{sub-sec:real-data}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment on Synthetic Data}
\label{sub-sec:synthetic-data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[http!]
    \centering
    \includegraphics[width=\columnwidth]{images/gaussian_n-samples1000_n-features10000_with_time.pdf}
    \caption{Running time and ARI for tested algorithm for different inter-cluster distances in an experiment with 1000 samples and 10000 features.}
    \label{fig:ari-time-gaussian}
    % \vskip -0.2in
\end{figure}

\begin{figure}[http!]
    \centering
    \includegraphics[width=\columnwidth]{images/hypercube_n-samples100000_n-features10000_pct-random0.0_with_time_calinski.pdf}
    \caption{Running time and ARI for tested algorithm for different inter-cluster distances in an experiment with $10^5$ samples and $10^4$ features.}
    \label{fig:ari-time-hypercube-huge}
    % \vskip -0.2in
\end{figure}

To access the limit of each model in clustering data with small inter-cluster distances, we generated datasets with 1000 samples (\samplesize) and 10000 features (\dimsize) where 5 centers were chosen randomly in a \dimsize-dimensional space separated by distances varying between 70 and 200. Then we equally sampled from 5 multivariate Normal distribution with mean equal to the center value and identity covariance matrix. The result can be seen in \cref{fig:ari-time-gaussian}, where we observe that \ourmethod performs well for distances up to 100, but with time comparable to K-Means and DBSCAN. Note, however, that under this settings a simpler algorithm as K-Means seems to be better suited. 

This is not anymore the case if we change the settings of the experiment to consider higher dimensional data. We present in \cref{fig:ari-time-hypercube-huge} the results of an experiment using the hypercube setting presented in \cref{sub-sec:scalability} with $10^5$ samples and $10^4$ features. Note that under this conditions, only \ourmethod and K-Means can run under a reasonable amount of time with our memory constraints. \ourmethod outperform K-Means, with the sampled variant of \ourmethod being capable to run in similar time as K-Means.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment on Real Data}
\label{sub-sec:real-data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We have also tested the clustering algorithms on real-world datasets. For a fair comparison, we have used classification datasets with known labels. All datasets are available on the OpenML platform \cite{openml2020}, and a summary of their characteristics is provided in \cref{tab:datasets}, including the number of samples $\samplesize$, total features $\dimsize$, categorical features $\categoricalnum$, and the number of classes which corresponds to the number of clusters $\clusternum^*$. All selected datasets have been recently considered benchmarks for classification tasks \cite{whenneuralnets2023}, and we have tried to include datasets which we judged can be well adapted from a classification to a clustering task, notably the dataset \texttt{Iris}, which is widely known in the domain, the dataset \texttt{Nursery} which is derived from a hierarchical decision model and, the dataset \texttt{Ecoli} which comes from the biology domain and present protein location sites. The other datasets were selected because of their relatively large number of samples and/or number of features. All datasets were preprocessed in the same manner before being given as input to the algorithms: we have one-hot encoded categorical features and we have standardize continuous features.

\input{table_datasets}

To showcase the flexibility and potential of \ourmethod, we introduce another variant of our model, \ourmethod-RBF which modifies the medoid selection criterion in \cref{sub-sec:method}, step $e$ $(v.)$, instead of using cosine distance, \ourmethod-RBF selects the medoid that maximizes similarity with all other samples based on a radial basis function (RBF) kernel. Finally for comparison with our base model, we have added the model \ourmethod-Full which does not perform sampling of the features

We report the performance results of a subset of the algorithms tested on real datasets in \cref{tab:res-real-data} and we have included all the algorithms and their parameters in \cref{app:real-data-time}. Our results show that \ourmethod is highly competitive compared to other methods, achieving the highest ARI on four datasets (\texttt{Garbner-2001}, \texttt{Nursery}, \texttt{Segment}, and \texttt{Shuttle}) and ranking second on three (\texttt{Alizadeh-2000-v2}, \texttt{Ecoli}, and \texttt{Satimage}). Additionally, \ourmethod consistently outperforms K-Means, the base clustering algorithm used within our approach, except for the dataset \texttt{Coil-20} While SC-SRGF also demonstrated strong performance on real-world data, it faced computational limitations. In contrast, \ourmethod proved to be a practical and efficient alternative, capable of running on a modest setup while being up to 100× faster.

Furthermore, \ourmethod is a powerful tool for interpretation. Since \ourmethod is fast, multiple dendrograms can be displayed for various runs with different hyperparameter configurations, providing domain experts with a convenient tool to apply their expertise and identify the most relevant dendrogram. See Figure \ref{fig:iris} in Appendix for an example of a dendrogram constructed by \ourmethod on the \texttt{Iris} dataset.

Finally, another advantage of our \ourmethod its computational efficiency, which enables systematic hyperparameter optimization. Our experiments, as illustrated in \cref{app:hyperparameter}, show that only a subset of the tested configurations yield strong performance, reinforcing the importance of automated hyperparameter tuning. As a best practice, we recommend optimizing $q$, $R$, and $C$ to achieve the best clustering results.

\input{table_real_data}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The proposed method, \ourmethod, leverages random feature projections, repeated K-Means clustering, and a unanimous consensus criterion to efficiently address the challenges of clustering high-dimensional data. By systematically reducing dimensionality and a novel consensus cluster assignment scheme, \ourmethod achieves robust and scalable clustering performance. Notably, \ourmethod does not require prior knowledge of the number of clusters $C^*$, as the hierarchical consensus naturally determines the cluster structure. Additionally, its iterative nature is a key advantage for interpretability, providing insights into complex data by organizing it in multi-level clustering structures. 


The second key advantage of \ourmethod is its scalabilty, and our experiments with approximated medoids suggest that the trade-off between accuracy and computational efficiency can be adjusted dynamically, making \ourmethod adaptable to different computational constraints. By combining \ourmethod with a sampling technique in a mini-batch fashion, we effectively tackle the high-dimensionality problem in both the number of samples and the number of features and we enable out-of-core computation by loading partial data dynamically.

Our experiments on both real and synthetic datasets demonstrate that \ourmethod is a computationally efficient alternative to state-of-the-art clustering methods, particularly in high-dimensional settings where other popular methods fail to compute. It scales effectively to extremely high-dimensional datasets while maintaining strong statistical accuracy and interpretability, making it valuable for real-world applications. 


We proposed to use K-Means as the base clustering algorithm in \ourmethod, primarily due to its low memory footprint and fast execution time. While this choice proved to be useful and allowed us to obtain state-of-the art results, greatly surpassing the performance of the base algorithm, our method is flexible and not restricted to K-Means. Any clustering method can be substituted in its place. 

Looking ahead, we aim to extend \ourmethod by integrating alternative clustering techniques within our consensus framework to leverage their respective strengths while maintaining scalability for more challenging clustering tasks.





\bibliography{Clustering}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



% \begin{algorithm}[http!]
% \caption{\ourmethod Algorithm}
% \label{alg:CoHiRF}
% \begin{algorithmic}

% \STATE \textbf{Input:} Data $\bbX$, number of sampled features $q$, number of repetitions $R$, number of clusters $C$
% \STATE \textbf{Output:} Final partition of samples $\{\mathcal{C}^*_{c}\}_c$

% \STATE \textbf{Initialize:} $X \gets \bbX$, $n_{\text{prev}} \gets 0$, $n_{\text{curr}} \gets n$
% \WHILE{$n_{\text{curr}} \neq n_{\text{prev}}$}
%     \STATE $\text P \gets \emptyset$ \COMMENT{Matrix to store K-Means labels across repetitions}
%     \FOR{$r = 1$ \textbf{to} $R$}
%         \STATE Randomly sample $q$ features from $X$
%         \STATE Perform K-Means on the sampled features, obtaining labels $\text p_r \in \{1,\ldots,C\}^{n_{\text{curr}}}$
%         \STATE Append labels $\text p_r$ to $\text P$
%     \ENDFOR
%     \STATE Assign a unique code to each row of $\text P$ 
%     \STATE Update $n_{\text{prev}} \gets n_{\text{curr}}$ 
%     \STATE Update $n_{\text{curr}} \gets \text{number of unique codes in } \text P$ 
%     \STATE Assign codes to each sample in $X$ and their parent samples to form partition $\mathcal{C}_j$ 

% \STATE \textbf{Medoid Computation:}
%     \FOR{$k = 1$ \textbf{to} $n_{\text{curr}}$}
%         \STATE Compute the medoid $\bx_{k}$ of the $k$-th cluster: 
%         \STATE Append the medoid to $X$ 
%         \STATE Assign all other samples in the cluster as children of the medoid
%     \ENDFOR
% \ENDWHILE

% \STATE \textbf{Return:} $\{\mathcal{C}^*_{c}\}_c$ \COMMENT{Final clusters after stabilization}

% \end{algorithmic}
% \end{algorithm}

\section{Algorithms Search Space}
\label{app:search-space}

In \cref{tab:search-space} we present the search space of each clustering algorithm.

\input{table_search_space}


\section{Hierarchical clustering experiment}
\label{app:hierarchy}

Our algorithm is an agglomerative clustering method as illustrated in  \cref{fig:hierarchy-concept}. We explored this aspect with an experiment on the \texttt{iris}
data where we represent the hierarchy built by \ourmethod-RBF, the result can be seen in \cref{fig:iris}.

\begin{figure*}[http!]
    %\vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\linewidth,height=16cm, keepaspectratio]{images/Iris_Color1.pdf}}
    \caption{Hierarchical clustering structure obtained by \ourmethod-RBF. We present the number of examples of each iris species inside each cluster. From left to right, we can observe the clusters that are merged at each iteration of our algorithm. Note that contrary from most hierarchical clustering algorithm, \ourmethod can merge more than one cluster at each step, which contributes to its speed.}
    \label{fig:iris}
    \end{center}
    \vskip -0.2in
\end{figure*}



\newpage
\section{Hyperparameter tuning}
\label{app:hyperparameter}

Even though we do not need to specify the number of clusters, our algorithm has three main hyperparameters: the number of repetitions $R$, the number of sampled components $q$ and the number of clusters $C$ used in the K-Means algorithm. Find the right balance between those parameters can be challenging, fortunately, as our algorithm is fast, we can afford to tune those hyperparameters. Empirically, we have defined that a good search space is $R\in[2,10]$, $q\in[2,30]$, and $k\in[2,10]$.

To illustrate the importance of hyperparameter tuning we present in \cref{fig:parallel} the result of the search for optimal hyperparameters in one experience. We generated datasets with $\samplesize=10^3$ samples, creating $C^*=5$ clusters. The cluster centers are drawn uniformly at random from the set of vertices of a $10^4$-dimensional hypercube, with edge length $\Delta=50$ (i.e.,the inter-cluster distance). Samples within each cluster are randomly drawn from a Normal distribution with an identity covariance matrix (thus controlling the intra-cluster distance), centered at the corresponding cluster center. 

Note that from 100 different runs, only 4 have find a good combination of the hyperparameters, that in this case perfectly partition the data and find the correct number of clusters. In this case, the right balance was to use a small number of repetitions and K-Means clusters with a large number of sampled components.

\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{images/parallel-plot_50.0class-sep_1000.0n-samples_10000.0n-features.pdf}}
    \caption{Parallel plot showing the runs from the hypercube experiment, with $n=1000$, $p=10000$, and $\text{Class Separation}=50$. The runs in green have perfectly partition the synthetic data.}
    \label{fig:parallel}
    \end{center}
    \vskip -0.2in
\end{figure}

\newpage
\section{Code}
\label{app:code}

The code is available on https://github.com/BrunoBelucci/cohirf

\newpage
\section{Experiment on Real Data}
\label{app:real-data-time}
In \cref{tab:res-real-data-time} we present the same results as in \cref{tab:res-real-data}, but with the running time of each algorithm.

\input{table_real_data_with_time_and_parameters}

\end{document}


