@article{affinitypropagation2007,
  title = {Clustering by {{Passing Messages Between Data Points}}},
  author = {Frey, Brendan J. and Dueck, Delbert},
  year = {2007},
  month = feb,
  journal = {Science},
  volume = {315},
  number = {5814},
  pages = {972--976},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1136800},
  urldate = {2025-01-10},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such ``exemplars'' can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called ``affinity propagation,'' which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
  file = {/home/bbelucci/Zotero/storage/DXZE55UG/Frey and Dueck - 2007 - Clustering by Passing Messages Between Data Points.pdf}
}

@inproceedings{aggarwal2001surprising,
  title = {On the {{Surprising Behavior}} of {{Distance Metrics}} in {{High Dimensional Space}}},
  booktitle = {Database {{Theory}} --- {{ICDT}} 2001},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  editor = {{Van den Bussche}, Jan and Vianu, Victor},
  year = {2001},
  pages = {420--434},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44503-X_27},
  abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1norm) is consistently more preferable than the Euclidean distance metric L(2norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  isbn = {978-3-540-44503-6},
  langid = {english},
  keywords = {Confusion Matrice,Distance Metrics,High Dimensional Space,Manhattan Distance,Query Point},
  file = {/home/bbelucci/Zotero/storage/TXC3GJCU/Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in High Dimensional Space.pdf}
}

@article{agglomerative1984,
  title = {Efficient Algorithms for Agglomerative Hierarchical Clustering Methods},
  author = {Day, William H. E. and Edelsbrunner, Herbert},
  year = {1984},
  month = dec,
  journal = {Journal of Classification},
  volume = {1},
  number = {1},
  pages = {7--24},
  issn = {1432-1343},
  doi = {10.1007/BF01890115},
  urldate = {2025-01-26},
  abstract = {Whenevern objects are characterized by a matrix of pairwise dissimilarities, they may be clustered by any of a number of sequential, agglomerative, hierarchical, nonoverlapping (SAHN) clustering methods. These SAHN clustering methods are defined by a paradigmatic algorithm that usually requires 0(n3) time, in the worst case, to cluster the objects. An improved algorithm (Anderberg 1973), while still requiring 0(n3) worst-case time, can reasonably be expected to exhibit 0(n2) expected behavior. By contrast, we describe a SAHN clustering algorithm that requires 0(n2 logn) time in the worst case. When SAHN clustering methods exhibit reasonable space distortion properties, further improvements are possible. We adapt a SAHN clustering algorithm, based on the efficient construction of nearest neighbor chains, to obtain a reasonably general SAHN clustering algorithm that requires in the worst case 0(n2) time and space.},
  langid = {english},
  keywords = {Algorithm complexity,Algorithm design,Centroid clustering method,Geometric model,SAHN clustering method},
  file = {/home/bbelucci/Zotero/storage/STKHW29M/Day and Edelsbrunner - 1984 - Efficient algorithms for agglomerative hierarchical clustering methods.pdf}
}

@article{al-sharoaMultiViewRobustTensorBased2022,
  title = {Multi-{{View Robust Tensor-Based Subspace Clustering}}},
  author = {{Al-Sharoa}, Esraa M. and {Al-Wardat}, Mohammad A.},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {134292--134306},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3232285},
  urldate = {2024-11-20},
  abstract = {In this era of technology advancement, huge amount of data is collected from different disciplines. This data needs to be stored, processed and analyzed to understand its nature. Networks or graphs arise to model real-world systems in the different fields. Early work in network theory adopted simple graphs to model systems where the system's entities and interactions among them are modeled as nodes and static, single-type edges, respectively. However, this representation is considered limited when the system's entities interact through different sources. Multi-view networks have recently attracted attention due to its ability to consider the different interactions between entities explicitly. An important tool to understand the structure of multi-view networks is community detection. Community detection or clustering reveals the significant communities in the network which provides dimensionality reduction and a better understanding of the network. In this paper, a new robust clustering algorithm is proposed to detect the community structure in multi-view networks. In particular, the proposed approach constructs a 3-mode tensor from the normalized adjacency matrices that represent the different views. The constructed tensor is decomposed into a self-representation and error components where the extracted self-representation tensor is used to detect the community structure of the multi-view network. Moreover, a common subspace is computed among all views where the contribution of each view to the common subspace is optimized. The proposed method is applied to several real-world data sets and the results show that the proposed method achieves the best performance compared to other state-of-the-art algorithms.},
  keywords = {Clustering algorithms,low-rank representation,Manifolds,Matrix decomposition,Multi-view networks,optimization,Periodic structures,spectral clustering,Static VAr compensators,Symmetric matrices,tensor decomposition,Tensors},
  file = {/home/bbelucci/Zotero/storage/9GYUZF5J/Al-Sharoa and Al-Wardat - 2022 - Multi-View Robust Tensor-Based Subspace Clustering.pdf;/home/bbelucci/Zotero/storage/VCZJ8H7Z/9999185.html}
}

@article{ari1985,
  title = {Comparing Partitions},
  author = {Hubert, Lawrence and Arabie, Phipps},
  year = {1985},
  month = dec,
  journal = {Journal of Classification},
  volume = {2},
  number = {1},
  pages = {193--218},
  issn = {1432-1343},
  doi = {10.1007/BF01908075},
  urldate = {2025-01-26},
  abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {\textpm}1.},
  langid = {english},
  keywords = {Consensus indices,Measures of agreement,Measures of association},
  file = {/home/bbelucci/Zotero/storage/U32QM64G/Hubert and Arabie - 1985 - Comparing partitions.pdf}
}

@inproceedings{arthurHowSlowKmeans2006,
  title = {How Slow Is the K-Means Method?},
  booktitle = {Proceedings of the Twenty-Second Annual Symposium on {{Computational}} Geometry},
  author = {Arthur, David and Vassilvitskii, Sergei},
  year = {2006},
  month = jun,
  series = {{{SCG}} '06},
  pages = {144--153},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1137856.1137880},
  urldate = {2025-01-23},
  abstract = {The k-means method is an old but popular clustering algorithm known for its observed speed and its simplicity. Until recently, however, no meaningful theoretical bounds were known on its running time. In this paper, we demonstrate that the worst-case running time of k-means is superpolynomial by improving the best known lower bound from {\textohm}(n) iterations to 2{\textohm}({\textsurd}n).},
  isbn = {978-1-59593-340-9},
  file = {/home/bbelucci/Zotero/storage/MJQ3V2BA/Arthur and Vassilvitskii - 2006 - How slow is the k-means method.pdf}
}

@article{assent2012clustering,
  title = {Clustering High Dimensional Data},
  author = {Assent, Ira},
  year = {2012},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {2},
  number = {4},
  pages = {340--350},
  issn = {1942-4795},
  doi = {10.1002/widm.1062},
  urldate = {2025-01-14},
  abstract = {High-dimensional data, i.e., data described by a large number of attributes, pose specific challenges to clustering. The so-called `curse of dimensionality', coined originally to describe the general increase in complexity of various computational problems as dimensionality increases, is known to render traditional clustering algorithms ineffective. The curse of dimensionality, among other effects, means that with increasing number of dimensions, a loss of meaningful differentiation between similar and dissimilar objects is observed. As high-dimensional objects appear almost alike, new approaches for clustering are required. Consequently, recent research has focused on developing techniques and clustering algorithms specifically for high-dimensional data. Still, open research issues remain. Clustering is a data mining task devoted to the automatic grouping of data based on mutual similarity. Each cluster groups objects that are similar to one another, whereas dissimilar objects are assigned to different clusters, possibly separating out noise. In this manner, clusters describe the data structure in an unsupervised manner, i.e., without the need for class labels. A number of clustering paradigms exist that provide different cluster models and different algorithmic approaches for cluster detection. Common to all approaches is the fact that they require some underlying assessment of similarity between data objects. In this article, we provide an overview of the effects of high-dimensional spaces, and their implications for different clustering paradigms. We review models and algorithms that address clustering in high dimensions, with pointers to the literature, and sketch open research issues. We conclude with a summary of the state of the art. {\copyright} 2012 Wiley Periodicals, Inc. This article is categorized under: Technologies {$>$} Structure Discovery and Clustering},
  copyright = {Copyright {\copyright} 2012 John Wiley \& Sons, Inc.},
  langid = {english},
  file = {/home/bbelucci/Zotero/storage/XB352DW6/Assent - 2012 - Clustering high dimensional data.pdf;/home/bbelucci/Zotero/storage/F7NQUFPW/widm.html}
}

@inproceedings{beyer1999nearest,
  title = {When {{Is}} ``{{Nearest Neighbor}}'' {{Meaningful}}?},
  booktitle = {Database {{Theory}} --- {{ICDT}}'99},
  author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  editor = {Beeri, Catriel and Buneman, Peter},
  year = {1999},
  pages = {217--235},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-49257-7_15},
  abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions.},
  isbn = {978-3-540-49257-3},
  langid = {english},
  file = {/home/bbelucci/Zotero/storage/W4UDA8TS/Beyer et al. - 1999 - When Is “Nearest Neighbor” Meaningful.pdf}
}

@incollection{bookdataminingchap102012,
  title = {10 - {{Cluster Analysis}}: {{Basic Concepts}} and {{Methods}}},
  shorttitle = {10 - {{Cluster Analysis}}},
  booktitle = {Data {{Mining}} ({{Third Edition}})},
  author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  editor = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  year = {2012},
  month = jan,
  series = {The {{Morgan Kaufmann Series}} in {{Data Management Systems}}},
  pages = {443--495},
  publisher = {Morgan Kaufmann},
  address = {Boston},
  doi = {10.1016/B978-0-12-381479-1.00010-1},
  urldate = {2025-01-26},
  abstract = {This chapter presents the basic concepts and methods of cluster analysis. The requirements of clustering methods for massive amounts of data and various applications are studied. Several basic clustering techniques are discussed organized into the following categories: partitioning methods, hierarchical methods, density-based methods, and grid-based methods). Evaluation process for clustering methods is also discussed. A cluster is a collection of data objects that are similar to one another within the same cluster and are dissimilar to the objects in other clusters. The process of grouping a set of physical or abstract objects into classes of similar objects is called clustering. Clustering is the process of grouping a set of data objects into multiple groups or clusters so that objects within a cluster have high similarity, but are very dissimilar to objects in other clusters. Dissimilarities and similarities are assessed based on the attribute values describing the objects and often involve distance measures. Clustering as a data mining tool has its roots in many application areas such as biology, security, business intelligence, and Web search. Cluster analysis has extensive applications, including business intelligence, image pattern recognition, Web search, biology, and security. Cluster analysis can be used as a standalone data mining tool to gain insight into the data distribution, or as a preprocessing step for other data mining algorithms operating on the detected clusters.},
  isbn = {978-0-12-381479-1},
  file = {/home/bbelucci/Zotero/storage/BHPNY89Q/Han et al. - 2012 - 10 - Cluster Analysis Basic Concepts and Methods.pdf}
}

@incollection{bookdataminingchap112012,
  title = {11 - {{Advanced Cluster Analysis}}},
  booktitle = {Data {{Mining}} ({{Third Edition}})},
  author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  editor = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  year = {2012},
  month = jan,
  series = {The {{Morgan Kaufmann Series}} in {{Data Management Systems}}},
  pages = {497--541},
  publisher = {Morgan Kaufmann},
  address = {Boston},
  doi = {10.1016/B978-0-12-381479-1.00011-3},
  urldate = {2025-01-27},
  abstract = {This chapter discusses the advanced topics of cluster analysis. In conventional cluster analysis, an object is assigned to one cluster exclusively. However, in some applications, there is a need to assign an object to one or more clusters in a fuzzy or probabilistic way. Fuzzy clustering and probabilistic model-based clustering allow an object to belong to one or more clusters. A partition matrix records the membership degree of objects belonging to clusters. There are two major categories of clustering methods for high-dimensional data: subspace clustering methods and dimensionality reduction methods. Subspace clustering methods search for clusters in subspaces of the original space. Dimensionality reduction methods create a new space of lower dimensionality and search for clusters there. Probabilistic model-based clustering has a general framework and is a method for deriving clusters where each object is assigned a probability of belonging to a cluster. Probabilistic model-based clustering is widely used in many data mining applications such as text mining. Clustering high-dimensional data is used when the dimensionality is high and conventional distance measures are dominated by noise. Fundamental methods for cluster analysis on high-dimensional data are introduced. Graph and network data are increasingly popular in applications such as online social networks, the World Wide Web, and digital libraries. The key issues in clustering graph and network data, including similarity measurement and clustering methods are studied. In some applications various constraints may exist. These constraints may rise from background knowledge or spatial distribution of the objects. The process of how to conduct cluster analysis with different kinds of constraints is discussed.},
  isbn = {978-0-12-381479-1}
}

@misc{boutsidisRandomProjections$k$means2010,
  title = {Random {{Projections}} for \$k\$-Means {{Clustering}}},
  author = {Boutsidis, Christos and Zouzias, Anastasios and Drineas, Petros},
  year = {2010},
  month = nov,
  number = {arXiv:1011.4632},
  eprint = {1011.4632},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1011.4632},
  urldate = {2024-11-18},
  abstract = {This paper discusses the topic of dimensionality reduction for \$k\$-means clustering. We prove that any set of \$n\$ points in \$d\$ dimensions (rows in a matrix \$A {\textbackslash}in {\textbackslash}RR{\textasciicircum}\{n {\textbackslash}times d\}\$) can be projected into \$t = {\textbackslash}Omega(k / {\textbackslash}eps{\textasciicircum}2)\$ dimensions, for any \${\textbackslash}eps {\textbackslash}in (0,1/3)\$, in \$O(n d {\textbackslash}lceil {\textbackslash}eps{\textasciicircum}\{-2\} k/ {\textbackslash}log(d) {\textbackslash}rceil )\$ time, such that with constant probability the optimal \$k\$-partition of the point set is preserved within a factor of \$2+{\textbackslash}eps\$. The projection is done by post-multiplying \$A\$ with a \$d {\textbackslash}times t\$ random matrix \$R\$ having entries \$+1/{\textbackslash}sqrt\{t\}\$ or \$-1/{\textbackslash}sqrt\{t\}\$ with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Data Structures and Algorithms},
  file = {/home/bbelucci/Zotero/storage/3X48NPRW/Boutsidis et al. - 2010 - Random Projections for $k$-means Clustering.pdf;/home/bbelucci/Zotero/storage/2EJQ7M6M/1011.html}
}

@article{calinsky1974,
  title = {A Dendrite Method for Cluster Analysis},
  author = {Cali{\'n}ski, T. and Harabasz, J},
  year = {1974},
  month = jan,
  journal = {Communications in Statistics},
  volume = {3},
  number = {1},
  pages = {1--27},
  publisher = {Taylor \& Francis},
  issn = {0090-3272},
  doi = {10.1080/03610927408827101},
  urldate = {2025-01-27},
  abstract = {A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the "best number" of clusters is suggested. It is a"variance ratio criterion" giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965).},
  keywords = {approximate grouping procedure,cluster analysis,minimum variance (WGSS) criterion for optimal grouping,numerical taxonomy,shortest dendrite = minimum spanning tree,variance ratio criterion for best number of groups}
}

@article{campelloHierarchicalDensityEstimates2015,
  title = {Hierarchical {{Density Estimates}} for {{Data Clustering}}, {{Visualization}}, and {{Outlier Detection}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Zimek, Arthur and Sander, J{\"o}rg},
  year = {2015},
  month = jul,
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {10},
  number = {1},
  pages = {5:1--5:51},
  issn = {1556-4681},
  doi = {10.1145/2733381},
  urldate = {2025-01-10},
  abstract = {An integrated framework for density-based cluster analysis, outlier detection, and data visualization is introduced in this article. The main module consists of an algorithm to compute hierarchical estimates of the level sets of a density, following Hartigan's classic model of density-contour clusters and trees. Such an algorithm generalizes and improves existing density-based clustering techniques with respect to different aspects. It provides as a result a complete clustering hierarchy composed of all possible density-based clusters following the nonparametric model adopted, for an infinite range of density thresholds. The resulting hierarchy can be easily processed so as to provide multiple ways for data visualization and exploration. It can also be further postprocessed so that: (i) a normalized score of ``outlierness'' can be assigned to each data object, which unifies both the global and local perspectives of outliers into a single definition; and (ii) a ``flat'' (i.e., nonhierarchical) clustering solution composed of clusters extracted from local cuts through the cluster tree (possibly corresponding to different density thresholds) can be obtained, either in an unsupervised or in a semisupervised way. In the unsupervised scenario, the algorithm corresponding to this postprocessing module provides a global, optimal solution to the formal problem of maximizing the overall stability of the extracted clusters. If partially labeled objects or instance-level constraints are provided by the user, the algorithm can solve the problem by considering both constraints violations/satisfactions and cluster stability criteria. An asymptotic complexity analysis, both in terms of running time and memory space, is described. Experiments are reported that involve a variety of synthetic and real datasets, including comparisons with state-of-the-art, density-based clustering and (global and local) outlier detection methods.},
  file = {/home/bbelucci/Zotero/storage/5GFQGSY3/Campello et al. - 2015 - Hierarchical Density Estimates for Data Clustering, Visualization, and Outlier Detection.pdf}
}

@article{chao2021,
  title = {A Survey on Multiview Clustering},
  author = {Chao, Guoqing and Sun, Shiliang and Bi, Jinbo},
  year = {2021},
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {2},
  number = {2},
  pages = {146--168},
  doi = {10.1109/TAI.2021.3065894},
  keywords = {Canonical correlation analysis (CCA),clustering,Clustering algorithms,Computer science,data mining,k-means,machine learning,Machine learning,Mixture models,multiview learning,nonnegative matrix factorization (NMF),Semisupervised learning,spectral clustering,subspace clustering,survey,Taxonomy,Web pages}
}

@article{clique1999,
  title = {Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications},
  author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
  year = {1998},
  month = jun,
  journal = {SIGMOD Rec.},
  volume = {27},
  number = {2},
  pages = {94--105},
  issn = {0163-5808},
  doi = {10.1145/276305.276314},
  urldate = {2024-11-18},
  abstract = {Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.},
  file = {/home/bbelucci/Zotero/storage/3TA4XYSU/Agrawal et al. - 1998 - Automatic subspace clustering of high dimensional data for data mining applications.pdf}
}

@article{clusterensembles2003,
  title = {Cluster Ensembles --- a Knowledge Reuse Framework for Combining Multiple Partitions},
  author = {Strehl, Alexander and Ghosh, Joydeep},
  year = {2003},
  month = mar,
  journal = {J. Mach. Learn. Res.},
  volume = {3},
  number = {null},
  pages = {583--617},
  issn = {1532-4435},
  doi = {10.1162/153244303321897735},
  urldate = {2025-01-27},
  abstract = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.},
  file = {/home/bbelucci/Zotero/storage/JFR7ZYSW/Strehl and Ghosh - 2003 - Cluster ensembles --- a knowledge reuse framework for combining multiple partitions.pdf}
}

@article{comprehensivesurvey2022,
  title = {A Comprehensive Survey of Clustering Algorithms: {{State-of-the-art}} Machine Learning Applications, Taxonomy, Challenges, and Future Research Prospects},
  shorttitle = {A Comprehensive Survey of Clustering Algorithms},
  author = {Ezugwu, Absalom E. and Ikotun, Abiodun M. and Oyelade, Olaide O. and Abualigah, Laith and Agushaka, Jeffery O. and Eke, Christopher I. and Akinyelu, Andronicus A.},
  year = {2022},
  month = apr,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {110},
  pages = {104743},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.104743},
  urldate = {2025-01-26},
  abstract = {Clustering is an essential tool in data mining research and applications. It is the subject of active research in many fields of study, such as computer science, data science, statistics, pattern recognition, artificial intelligence, and machine learning. Several clustering techniques have been proposed and implemented, and most of them successfully find excellent quality or optimal clustering results in the domains mentioned earlier. However, there has been a gradual shift in the choice of clustering methods among domain experts and practitioners alike, which is precipitated by the fact that most traditional clustering algorithms still depend on the number of clusters provided a priori. These conventional clustering algorithms cannot effectively handle real-world data clustering analysis problems where the number of clusters in data objects cannot be easily identified. Also, they cannot effectively manage problems where the optimal number of clusters for a high-dimensional dataset cannot be easily determined. Therefore, there is a need for improved, flexible, and efficient clustering techniques. Recently, a variety of efficient clustering algorithms have been proposed in the literature, and these algorithms produced good results when evaluated on real-world clustering problems. This study presents an up-to-date systematic and comprehensive review of traditional and state-of-the-art clustering techniques for different domains. This survey considers clustering from a more practical perspective. It shows the outstanding role of clustering in various disciplines, such as education, marketing, medicine, biology, and bioinformatics. It also discusses the application of clustering to different fields attracting intensive efforts among the scientific community, such as big data, artificial intelligence, and robotics. This survey paper will be beneficial for both practitioners and researchers. It will serve as a good reference point for researchers and practitioners to design improved and efficient state-of-the-art clustering algorithms.},
  keywords = {Automatic clustering,Clustering,Clustering algorithms partitioning,Data mining,Hierarchical clustering,K-Means,Optimization algorithms Machine learning,Supervised learning,Unsupervised learning},
  file = {/home/bbelucci/Zotero/storage/4XMG3LJ6/Ezugwu et al. - 2022 - A comprehensive survey of clustering algorithms State-of-the-art machine learning applications, tax.pdf;/home/bbelucci/Zotero/storage/KFA5BYSC/S095219762200046X.html}
}

@article{dataclustering1999,
  title = {Data Clustering: A Review},
  shorttitle = {Data Clustering},
  author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
  year = {1999},
  month = sep,
  journal = {ACM Comput. Surv.},
  volume = {31},
  number = {3},
  pages = {264--323},
  issn = {0360-0300},
  doi = {10.1145/331499.331504},
  urldate = {2025-01-26},
  abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
  file = {/home/bbelucci/Zotero/storage/UQI2IKFD/Jain et al. - 1999 - Data clustering a review.pdf}
}

@article{davies1979,
  title = {A {{Cluster Separation Measure}}},
  author = {Davies, David L. and Bouldin, Donald W.},
  year = {1979},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-1},
  number = {2},
  pages = {224--227},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1979.4766909},
  urldate = {2025-01-27},
  abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
  keywords = {Algorithm design and analysis,Cluster,Clustering algorithms,Data analysis,data partitions,Density measurement,Dispersion,Humans,Missiles,multidimensional data analysis,Multidimensional systems,parametric clustering,Partitioning algorithms,partitions,Performance analysis,similarity measure},
  file = {/home/bbelucci/Zotero/storage/QFEILV3Y/Davies and Bouldin - 1979 - A Cluster Separation Measure.pdf;/home/bbelucci/Zotero/storage/4F8K886I/4766909.html}
}

@inproceedings{dbscan1996,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  month = aug,
  series = {{{KDD}}'96},
  pages = {226--231},
  publisher = {AAAI Press},
  address = {Portland, Oregon},
  urldate = {2025-01-10},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
  file = {/home/bbelucci/Zotero/storage/HYKJVERP/Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf}
}

@incollection{diana1990,
  title = {Divisive {{Analysis}} ({{Program DIANA}})},
  booktitle = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  author = {Kaufman, Leonard and Rousseeuw, Peter J.},
  year = {1990},
  month = mar,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  pages = {253--279},
  publisher = {Wiley},
  doi = {10.1002/9780470316801},
  urldate = {2025-01-28},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-0-471-87876-6 978-0-470-31680-1},
  langid = {english}
}




@inproceedings{DOC2002,
  title = {A {{Monte Carlo}} Algorithm for Fast Projective Clustering},
  booktitle = {Proceedings of the 2002 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Procopiuc, Cecilia M. and Jones, Michael and Agarwal, Pankaj K. and Murali, T. M.},
  year = {2002},
  month = jun,
  series = {{{SIGMOD}} '02},
  pages = {418--427},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/564691.564739},
  urldate = {2024-11-18},
  abstract = {We propose a mathematical formulation for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This allows us to develop a Monte Carlo algorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We implemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that our method is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detecting rotated human faces in cluttered images.},
  isbn = {978-1-58113-497-1},
  file = {/home/bbelucci/Zotero/storage/CL5AJ7DB/Procopiuc et al. - 2002 - A Monte Carlo algorithm for fast projective clustering.pdf}
}

@article{golalipourClusteringClusteringEnsemble2021,
  title = {From Clustering to Clustering Ensemble Selection: {{A}} Review},
  shorttitle = {From Clustering to Clustering Ensemble Selection},
  author = {Golalipour, Keyvan and Akbari, Ebrahim and Hamidi, Seyed Saeed and Lee, Malrey and Enayatifar, Rasul},
  year = {2021},
  month = sep,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {104},
  pages = {104388},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2021.104388},
  urldate = {2025-01-27},
  abstract = {Clustering, as an unsupervised learning, is aimed at discovering the natural groupings of a set of patterns, points, or objects. In clustering algorithms, a significant problem is the absence of a deterministic approach based on which users can decide which clustering method best matches a given set of input data. This is due to using certain criteria for optimization. Clustering ensemble as a knowledge reuse offers a solution to solve the challenges inherent in clustering. It seeks to explore results of high stability and robustness by composing computed solutions achieved by base clustering algorithms without getting access to the features. Combining base clusterings together degrades the quality of the final solution when low-quality ensemble members are used. Several researchers in this field have suggested the concept of clustering ensemble selection for the aim of selecting a subset of base clustering based on quality and diversity. While clustering ensemble makes a combination of all ensemble members, clustering ensemble selection chooses a subset of ensemble members and forms a smaller cluster ensemble that performs better than the clustering ensemble. This survey includes the historical development of data clustering that makes an overview on basic clustering techniques, discusses clustering ensemble algorithms including ensemble generation mechanisms and consensus function, and point out clustering ensemble selection techniques with considering quality and diversity.},
  keywords = {Cluster analysis,Clustering ensemble,Clustering ensemble selection,Consensus clustering,Data clustering},
  file = {/home/bbelucci/Zotero/storage/9BPBW3L3/Golalipour et al. - 2021 - From clustering to clustering ensemble selection A review.pdf;/home/bbelucci/Zotero/storage/CBAU3NH6/S0952197621002360.html}
}

@inproceedings{hdbscan2013,
  title = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  year = {2013},
  pages = {160--172},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37456-2_14},
  abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
  isbn = {978-3-642-37456-2},
  langid = {english},
  keywords = {Cluster Tree,Core Object,Density Threshold,Hierarchical Cluster Method,Minimum Span Tree},
  file = {/home/bbelucci/Zotero/storage/VHLJ6UBQ/Campello et al. - 2013 - Density-Based Clustering Based on Hierarchical Den.pdf}
}

@article{hidualm2024,
  title = {A {{Novel Hierarchical High-Dimensional Unsupervised Active Learning Method}}},
  author = {Haghzad Klidbary, Sajad and Javadian, Mohammad},
  year = {2024},
  month = jul,
  journal = {International Journal of Computational Intelligence Systems},
  volume = {17},
  number = {1},
  pages = {193},
  issn = {1875-6883},
  doi = {10.1007/s44196-024-00601-w},
  urldate = {2024-11-07},
  abstract = {This paper processes a novel hierarchical high-dimensional clustering algorithm based on the Active Learning Method (ALM), which is a fuzzy-learning algorithm. The hierarchical part of the algorithm is composed of two phases: divisible and agglomerative. The divisible phase, a zooming-in-process, searches for sub-clusters in already-found clusters hierarchically. At each level of the hierarchy, the clusters are found by an ensemble clustering method based on the density of data. This part of the algorithm blurs each data point as multiple one-dimensional fuzzy membership functions called ink-drop patterns; then, it accumulates the ink-drop patterns of all data points on every dimension separately. Next, it performs one-dimensional density partitioning to produce an ensemble of clustering solutions; after that, combining the results is done based on a novel consensus method with the aid of prime numbers. An agglomerative phase is a bottom-up approach that merges clusters based on a novel distance metric, named \$\$\{K\}{\textasciicircum}\{2\}\$\$-nearest neighbor. The algorithm is named as the Hierarchical High-Dimensional Unsupervised Active Learning Method (HiDUALM) and is explained in more detail throughout this paper. Although the classical clustering methods are not suitable for high-dimensional data clustering, the proposed method solves the problems related to speed and memory using ensemble learning, while due to its hierarchy and the use of different distance criteria, different levels of the cluster provide the clause. Experiments on synthetic and real-world datasets are presented to show the effectiveness of the proposed-clustering algorithm.},
  langid = {english},
  keywords = {Artificial Intelligence,Ensemble clustering,Hierarchical clustering,High-dimensional clustering,Machine learning (ML),Noise elimination,Unsupervised active learning method (ALM)},
  file = {/home/bbelucci/Zotero/storage/UTIMDKWE/Haghzad Klidbary and Javadian - 2024 - A Novel Hierarchical High-Dimensional Unsupervised Active Learning Method.pdf}
}

@article{irfllrr2023,
  title = {{{LatLRR}} for Subspace Clustering via Reweighted {{Frobenius}} Norm Minimization},
  author = {Liu, Zhuo and Hu, Dong and Wang, Zhi and Gou, Jianping and Jia, Tao},
  year = {2023},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {224},
  pages = {119977},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.119977},
  urldate = {2024-11-19},
  abstract = {Subspace clustering has attracted much attention on account of its excellent performance in various tasks, such as computer vision and pattern recognition. Since the high-dimensional data may comprise redundant information and noises, low-dimensional subspace structure is difficult to be recovered exactly. Existing methods adopt the non-convex Schatten-p norm to approximate the rank function closer. However, these methods are still unable to handle each singular value flexibly, which may yield unsatisfactory solutions. To address this deficiency, in this paper, we propose a iterative reweighted Frobenius norm regularized latent low rank representation (IRFLLRR) model, which can restrain the small and large rank components simultaneously with rational weights. Specifically, the reweighting strategy can preserve the desirable structure information, while removing the sparse noise and redundant information. Hence the constructed coefficient matrix can capture the global structure of data more adequately. To solve the proposed model, an efficient algorithm is developed via the alternating direction method of multipliers (ADMM), which ensures each subproblem can be optimized in closed-form. In addition, we provide detailed mathematical proofs to guarantee that the sequence generated by our algorithm convergences to a Karush--Kuhn--Tucker (KKT) point. Extensive experimental results reveal the superiority of the proposed method over several state-of-the-art methods. MATLAB code is available at https://github.com/wangzhi-swu/IRFLLRR.},
  keywords = {ADMM,Iterative reweighted Frobenius norm,Latent low rank representation,Subspace clustering},
  file = {/home/bbelucci/Zotero/storage/2D9YXBTT/Liu et al. - 2023 - LatLRR for subspace clustering via reweighted Frobenius norm minimization.pdf;/home/bbelucci/Zotero/storage/HS3P8LGT/S0957417423004797.html}
}

@book{kaufmanFindingGroupsData1990,
  title = {Finding Groups in Data: An Introduction to Cluster Analysis},
  shorttitle = {Finding Groups in Data},
  author = {Kaufman, Leonard and Rousseeuw, Peter J.},
  year = {1990},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {Wiley},
  address = {New York},
  isbn = {978-0-471-87876-6},
  lccn = {QA278 .K38 1990},
  keywords = {Cluster analysis}
}

@article{kmeans_complexity,
  title = {Algorithm {{AS}} 136: {{A K-Means Clustering Algorithm}}},
  shorttitle = {Algorithm {{AS}} 136},
  author = {Hartigan, J. A. and Wong, M. A.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {28},
  number = {1},
  eprint = {2346830},
  eprinttype = {jstor},
  pages = {100--108},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9254},
  doi = {10.2307/2346830},
  urldate = {2025-01-23},
  file = {/home/bbelucci/Zotero/storage/F9Y3IKS7/Hartigan and Wong - 1979 - Algorithm AS 136 A K-Means Clustering Algorithm.pdf}
}

@inproceedings{kmeansprojective2004,
  title = {K-Means Projective Clustering},
  booktitle = {Proceedings of the Twenty-Third {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Agarwal, Pankaj K. and Mustafa, Nabil H.},
  year = {2004},
  month = jun,
  series = {{{PODS}} '04},
  pages = {155--165},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1055558.1055581},
  urldate = {2024-11-18},
  abstract = {In many applications it is desirable to cluster high dimensional data along various subspaces, which we refer to as projective clustering. We propose a new objective function for projective clustering, taking into account the inherent trade-off between the dimension of a subspace and the induced clustering error. We then present an extension of the k-means clustering algorithm for projective clustering in arbitrary subspaces, and also propose techniques to avoid local minima. Unlike previous algorithms, ours can choose the dimension of each cluster independently and automatically. Furthermore, experimental results show that our algorithm is significantly more accurate than the previous approaches.},
  isbn = {978-1-58113-858-0},
  file = {/home/bbelucci/Zotero/storage/9DPBEH9H/Agarwal and Mustafa - 2004 - k-means projective clustering.pdf}
}

@article{li2021research,
  title = {Research on the Natural Language Recognition Method Based on Cluster Analysis Using Neural Network},
  author = {Li, Guang and Liu, Fangfang and Sharma, Ashutosh and Khalaf, Osamah Ibrahim and Alotaibi, Youseef and Alsufyani, Abdulmajeed and Alghamdi, Saleh},
  year = {2021},
  journal = {Mathematical Problems in Engineering},
  volume = {2021},
  number = {1},
  pages = {9982305},
  publisher = {Wiley Online Library}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2025-01-10},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {/home/bbelucci/Zotero/storage/RUANM8ME/Lloyd - 1982 - Least squares quantization in PCM.pdf;/home/bbelucci/Zotero/storage/67SE4ALZ/1056489.html}
}

@article{llr2013,
  title = {Robust {{Recovery}} of {{Subspace Structures}} by {{Low-Rank Representation}}},
  author = {Liu, Guangcan and Lin, Zhouchen and Yan, Shuicheng and Sun, Ju and Yu, Yong and Ma, Yi},
  year = {2013},
  month = jan,
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {35},
  number = {1},
  pages = {171--184},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2012.88},
  urldate = {2025-01-18},
  abstract = {In this paper, we address the subspace clustering problem. Given a set of data samples (vectors) approximately drawn from a union of multiple subspaces, our goal is to cluster the samples into their respective subspaces and remove possible outliers as well. To this end, we propose a novel objective function named Low-Rank Representation (LRR), which seeks the lowest rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary. It is shown that the convex program associated with LRR solves the subspace clustering problem in the following sense: When the data is clean, we prove that LRR exactly recovers the true subspace structures; when the data are contaminated by outliers, we prove that under certain conditions LRR can exactly recover the row space of the original data and detect the outlier as well; for data corrupted by arbitrary sparse errors, LRR can also approximately recover the row space with theoretical guarantees. Since the subspace membership is provably determined by the row space, these further imply that LRR can perform robust subspace clustering and error correction in an efficient and effective way.},
  file = {/home/bbelucci/Zotero/storage/RNFYYDB4/Liu et al. - 2013 - Robust Recovery of Subspace Structures by Low-Rank Representation.pdf}
}

@article{mahdi2021scalable,
  title = {Scalable Clustering Algorithms for Big Data: {{A}} Review},
  author = {Mahdi, Mahmoud A and Hosny, Khalid M and Elhenawy, Ibrahim},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {80015--80027},
  publisher = {IEEE}
}

@article{meanshift2002,
  title = {Mean Shift: A Robust Approach toward Feature Space Analysis},
  shorttitle = {Mean Shift},
  author = {Comaniciu, D. and Meer, P.},
  year = {2002},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {5},
  pages = {603--619},
  issn = {1939-3539},
  doi = {10.1109/34.1000236},
  urldate = {2025-01-10},
  abstract = {A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.},
  keywords = {Convergence,Density functional theory,Image analysis,Image color analysis,Image resolution,Image segmentation,Kernel,Pattern recognition,Robustness,Smoothing methods},
  file = {/home/bbelucci/Zotero/storage/6I2SGATV/Comaniciu and Meer - 2002 - Mean shift a robust approach toward feature space.pdf;/home/bbelucci/Zotero/storage/CJK9WJN9/1000236.html}
}

@inproceedings{monathScalableHierarchicalAgglomerative2021,
  title = {Scalable {{Hierarchical Agglomerative Clustering}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Monath, Nicholas and Dubey, Kumar Avinava and Guruganesh, Guru and Zaheer, Manzil and Ahmed, Amr and McCallum, Andrew and Mergen, Gokhan and Najork, Marc and Terzihan, Mert and Tjanaka, Bryon and Wang, Yuan and Wu, Yuchen},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1245--1255},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3447548.3467404},
  urldate = {2025-01-19},
  abstract = {The applicability of agglomerative clustering, for inferring both hierarchical and flat clustering, is limited by its scalability. Existing scalable hierarchical clustering methods sacrifice quality for speed and often lead to over-merging of clusters. In this paper, we present a scalable, agglomerative method for hierarchical clustering that does not sacrifice quality and scales to billions of data points. We perform a detailed theoretical analysis, showing that under mild separability conditions our algorithm can not only recover the optimal flat partition but also provide a two-approximation to non-parametric DP-Means objective. This introduces a novel application of hierarchical clustering as an approximation algorithm for the non-parametric clustering objective. We additionally relate our algorithm to the classic hierarchical agglomerative clustering method. We perform extensive empirical experiments in both hierarchical and flat clustering settings and show that our proposed approach achieves state-of-the-art results on publicly available clustering benchmarks. Finally, we demonstrate our method's scalability by applying it to a dataset of 30 billion queries. Human evaluation of the discovered clusters show that our method finds better quality of clusters than the current state-of-the-art.},
  isbn = {978-1-4503-8332-5},
  file = {/home/bbelucci/Zotero/storage/3J84RIHA/Monath et al. - 2021 - Scalable Hierarchical Agglomerative Clustering.pdf}
}

@misc{mullnerModernHierarchicalAgglomerative2011,
  title = {Modern Hierarchical, Agglomerative Clustering Algorithms},
  author = {M{\"u}llner, Daniel},
  year = {2011},
  month = sep,
  number = {arXiv:1109.2378},
  eprint = {1109.2378},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1109.2378},
  urldate = {2025-01-10},
  abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Statistics - Machine Learning},
  file = {/home/bbelucci/Zotero/storage/7PUCJQR4/Müllner - 2011 - Modern hierarchical, agglomerative clustering algo.pdf;/home/bbelucci/Zotero/storage/7K8UB4E5/1109.html}
}

@article{openml2020,
  title = {{{OpenML-python}}: An Extensible Python {{API}} for {{OpenML}}},
  author = {Feurer, Matthias and {van Rijn}, Jan N. and Kadra, Arlind and Gijsbers, Pieter and Mallik, Neeratyoy and Ravi, Sahithya and Mueller, Andreas and Vanschoren, Joaquin and Hutter, Frank},
  year = {2020},
  journal = {arXiv},
  volume = {1911.02490}
}

@article{optics1999,
  title = {{{OPTICS}}: Ordering Points to Identify the Clustering Structure},
  shorttitle = {{{OPTICS}}},
  author = {Ankerst, Mihael and Breunig, Markus M. and Kriegel, Hans-Peter and Sander, J{\"o}rg},
  year = {1999},
  month = jun,
  journal = {SIGMOD Rec.},
  volume = {28},
  number = {2},
  pages = {49--60},
  issn = {0163-5808},
  doi = {10.1145/304181.304187},
  urldate = {2025-01-10},
  abstract = {Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.},
  file = {/home/bbelucci/Zotero/storage/BKX5YRSN/Ankerst et al. - 1999 - OPTICS ordering points to identify the clustering.pdf}
}

@article{parsonsSubspaceClusteringHigh2004,
  title = {Subspace Clustering for High Dimensional Data: A Review},
  shorttitle = {Subspace Clustering for High Dimensional Data},
  author = {Parsons, Lance and Haque, Ehtesham and Liu, Huan},
  year = {2004},
  month = jun,
  journal = {SIGKDD Explor. Newsl.},
  volume = {6},
  number = {1},
  pages = {90--105},
  issn = {1931-0145},
  doi = {10.1145/1007730.1007731},
  urldate = {2025-01-27},
  abstract = {Subspace clustering is an extension of traditional clustering that seeks to find clusters in different subspaces within a dataset. Often in high dimensional data, many dimensions are irrelevant and can mask existing clusters in noisy data. Feature selection removes irrelevant and redundant dimensions by analyzing the entire dataset. Subspace clustering algorithms localize the search for relevant dimensions allowing them to find clusters that exist in multiple, possibly overlapping subspaces. There are two major branches of subspace clustering based on their search strategy. Top-down algorithms find an initial clustering in the full set of dimensions and evaluate the subspaces of each cluster, iteratively improving the results. Bottom-up approaches find dense regions in low dimensional spaces and combine them to form clusters. This paper presents a survey of the various subspace clustering algorithms along with a hierarchy organizing the algorithms by their defining characteristics. We then compare the two main approaches to subspace clustering using empirical scalability and accuracy tests and discuss some potential applications where subspace clustering could be particularly useful.},
  file = {/home/bbelucci/Zotero/storage/34KF57PH/Parsons et al. - 2004 - Subspace clustering for high dimensional data a review.pdf}
}

@article{pca1901,
  title = {{{LIII}}. {{On}} Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, Karl},
  year = {1901},
  month = nov,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  publisher = {Taylor \& Francis},
  issn = {1941-5982},
  doi = {10.1080/14786440109462720},
  urldate = {2025-01-27},
  file = {/home/bbelucci/Zotero/storage/SMB6YDK2/Pearson - 1901 - LIII. On lines and planes of closest fit to systems of points in space.pdf}
}

@inproceedings{proclus2000,
  title = {Fast Algorithms for Projected Clustering},
  booktitle = {Proceedings of the 1999 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Aggarwal, Charu C. and Wolf, Joel L. and Yu, Philip S. and Procopiuc, Cecilia and Park, Jong Soo},
  year = {1999},
  month = jun,
  series = {{{SIGMOD}} '99},
  pages = {61--72},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/304182.304188},
  urldate = {2024-12-15},
  abstract = {The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data.},
  isbn = {978-1-58113-084-3},
  file = {/home/bbelucci/Zotero/storage/IC85FZAL/Aggarwal et al. - 1999 - Fast algorithms for projected clustering.pdf}
}

@article{projectionbasedclustering2021,
  title = {Using {{Projection-Based Clustering}} to {{Find Distance-}} and {{Density-Based Clusters}} in {{High-Dimensional Data}}},
  author = {Thrun, Michael C. and Ultsch, Alfred},
  year = {2021},
  month = jul,
  journal = {Journal of Classification},
  volume = {38},
  number = {2},
  pages = {280--312},
  issn = {1432-1343},
  doi = {10.1007/s00357-020-09373-2},
  urldate = {2025-01-27},
  abstract = {For high-dimensional datasets in which clusters are formed by both distance and density structures (DDS), many clustering algorithms fail to identify these clusters correctly. This is demonstrated for 32 clustering algorithms using a suite of datasets which deliberately pose complex DDS challenges for clustering. In order to improve the structure finding and clustering in high-dimensional DDS datasets, projection-based clustering (PBC) is introduced. The coexistence of projection and clustering allows to explore DDS through a topographic map. This enables to estimate, first, if any cluster tendency exists and, second, the estimation of the number of clusters. A comparison showed that PBC is always able to find the correct cluster structure, while the performance of the best of the 32 clustering algorithms varies depending on the dataset.},
  langid = {english},
  keywords = {Cluster analysis,Data visualization,Dimensionality reduction},
  file = {/home/bbelucci/Zotero/storage/EAC5CE72/Thrun and Ultsch - 2021 - Using Projection-Based Clustering to Find Distance- and Density-Based Clusters in High-Dimensional D.pdf}
}

@article{reviewclustering2017,
  title = {A Review of Clustering Techniques and Developments},
  author = {Saxena, Amit and Prasad, Mukesh and Gupta, Akshansh and Bharill, Neha and Patel, Om Prakash and Tiwari, Aruna and Er, Meng Joo and Ding, Weiping and Lin, Chin-Teng},
  year = {2017},
  month = dec,
  journal = {Neurocomputing},
  volume = {267},
  pages = {664--681},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.06.053},
  urldate = {2025-01-26},
  abstract = {This paper presents a comprehensive study on clustering: exiting methods and developments made at various times. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some similarity inherent among them. There are different methods for clustering the objects such as hierarchical, partitional, grid, density based and model based. The approaches used in these methods are discussed with their respective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are the central components of clustering, are also presented in the paper. The applications of clustering in some fields like image segmentation, object and character recognition and data mining are highlighted.},
  keywords = {Clustering,Data mining,Pattern recognition,Similarity measures,Unsupervised learning},
  file = {/home/bbelucci/Zotero/storage/IFSW8WGD/Saxena et al. - 2017 - A review of clustering techniques and developments.pdf;/home/bbelucci/Zotero/storage/3AMY66XC/S0925231217311815.html}
}

@article{rezaul2020clustering,
  title = {Deep Learning-Based Clustering Approaches for Bioinformatics},
  author = {Karim, Rezaul and Beyan, Oya and Zappa, Achille and Costa, Ivan and {Rebholz-Schuhman}, Dietrich and Cochez, Michael and Decker, Stefan},
  year = {2020},
  month = feb,
  journal = {Briefings in bioinformatics},
  volume = {22},
  doi = {10.1093/bib/bbz170}
}

@article{RLLRR2014,
  title = {Robust Latent Low Rank Representation for Subspace Clustering},
  author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao and Gao, Junbin},
  year = {2014},
  month = dec,
  journal = {Neurocomputing},
  volume = {145},
  pages = {369--373},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2014.05.022},
  urldate = {2025-01-27},
  abstract = {Subspace clustering has found wide applications in machine learning, data mining, and computer vision. Latent Low Rank Representation (LatLRR) is one of the state-of-the-art methods for subspace clustering. However, its effectiveness is undermined by a recent discovery that the solution to the noiseless LatLRR model is non-unique. To remedy this issue, we propose choosing the sparest solution in the solution set. When there is noise, we further propose preprocessing the data with robust PCA. Experiments on both synthetic and real data demonstrate the advantage of our robust LatLRR over state-of-the-art methods.},
  keywords = {Latent low rank representation,Subspace clustering},
  file = {/home/bbelucci/Zotero/storage/ZMD4BGIQ/S0925231214006365.html}
}

@article{sc-srgf2020,
  title = {Spectral {{Clustering}} by {{Subspace Randomization}} and {{Graph Fusion}} for {{High-Dimensional Data}}},
  author = {Cai, Xiaosha and Huang, Dong and Wang, Chang-Dong and Kwoh, Chee-Keong},
  year = {2020},
  month = apr,
  journal = {Advances in Knowledge Discovery and Data Mining},
  volume = {12084},
  pages = {330},
  doi = {10.1007/978-3-030-47426-3_26},
  urldate = {2024-11-08},
  abstract = {Subspace clustering has been gaining increasing attention in recent years due to its promising ability in dealing with high-dimensional data. However, most of the existing subspace clustering methods tend to only exploit the subspace information to ...},
  langid = {english},
  file = {/home/bbelucci/Zotero/storage/GAFSY227/Cai et al. - 2020 - Spectral Clustering by Subspace Randomization and Graph Fusion for High-Dimensional Data.pdf}
}

@article{silhouette1987,
  title = {Silhouettes: {{A}} Graphical Aid to the Interpretation and Validation of Cluster Analysis},
  shorttitle = {Silhouettes},
  author = {Rousseeuw, Peter J.},
  year = {1987},
  month = nov,
  journal = {Journal of Computational and Applied Mathematics},
  volume = {20},
  pages = {53--65},
  issn = {0377-0427},
  doi = {10.1016/0377-0427(87)90125-7},
  urldate = {2025-01-26},
  abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an `appropriate' number of clusters.},
  keywords = {classification,cluster analysis,clustering validity,Graphical display},
  file = {/home/bbelucci/Zotero/storage/ESMIIYH6/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation and validation of cluster analysis.pdf;/home/bbelucci/Zotero/storage/9ITD6JXB/0377042787901257.html}
}

@article{spectralclustering2000,
  title = {Normalized Cuts and Image Segmentation},
  author = {Shi, Jianbo and Malik, J.},
  year = {2000},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {8},
  pages = {888--905},
  issn = {1939-3539},
  doi = {10.1109/34.868688},
  urldate = {2025-01-10},
  abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
  keywords = {Bayesian methods,Brightness,Clustering algorithms,Coherence,Data mining,Eigenvalues and eigenfunctions,Filling,Image segmentation,Partitioning algorithms,Tree data structures},
  file = {/home/bbelucci/Zotero/storage/67LNCMZ9/Shi and Malik - 2000 - Normalized cuts and image segmentation.pdf;/home/bbelucci/Zotero/storage/KAYF6PP4/Shi and Malik - 2000 - Normalized cuts and image segmentation.pdf;/home/bbelucci/Zotero/storage/L6F7FZM4/868688.html}
}

@incollection{steinbach2004challenges,
  title = {The {{Challenges}} of {{Clustering High Dimensional Data}}},
  booktitle = {New {{Directions}} in {{Statistical Physics}}: {{Econophysics}}, {{Bioinformatics}}, and {{Pattern Recognition}}},
  author = {Steinbach, Michael and Ert{\"o}z, Levent and Kumar, Vipin},
  editor = {Wille, Luc T.},
  year = {2004},
  pages = {273--309},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-08968-2_16},
  urldate = {2025-01-14},
  abstract = {Cluster analysis divides data into groups (clusters) for the purposes of summarization or improved understanding. For example, cluster analysis has been used to group related documents for browsing, to find genes and proteins that have similar functionality, or as a means of data compression. While clustering has a long history and a large number of clustering techniques have been developed in statistics, pattern recognition, data mining, and other fields, significant challenges still remain. In this chapter we provide a short introduction to cluster analysis, and then focus on the challenge of clustering high dimensional data. We present a brief overview of several recent techniques, including a more detailed description of recent work of our own which uses a concept-based clustering approach.},
  isbn = {978-3-662-08968-2},
  langid = {english},
  keywords = {Concept Space,Document Cluster,Frequent Itemset,Grid Cell,High Dimensional Data},
  file = {/home/bbelucci/Zotero/storage/TU3QCNK7/Steinbach et al. - 2004 - The Challenges of Clustering High Dimensional Data.pdf}
}

@article{surveyclustering2005,
  title = {Survey of Clustering Algorithms},
  author = {Xu, Rui and Wunsch, D.},
  year = {2005},
  month = may,
  journal = {IEEE Transactions on Neural Networks},
  volume = {16},
  number = {3},
  pages = {645--678},
  issn = {1941-0093},
  doi = {10.1109/TNN.2005.845141},
  urldate = {2025-01-26},
  abstract = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
  keywords = {Adaptive resonance theory (ART),Application software,Bioinformatics,cluster validation,clustering,clustering algorithm,Clustering algorithms,Computer science,Data analysis,Humans,Machine learning,Machine learning algorithms,neural networks,proximity,self-organizing feature map (SOFM),Statistics,Traveling salesman problems},
  file = {/home/bbelucci/Zotero/storage/56G9S4ZK/Xu and Wunsch - 2005 - Survey of clustering algorithms.pdf;/home/bbelucci/Zotero/storage/CR57KWMR/1427769.html}
}

@misc{tpe2023,
  title = {Tree-{{Structured Parzen Estimator}}: {{Understanding Its Algorithm Components}} and {{Their Roles}} for {{Better Empirical Performance}}},
  shorttitle = {Tree-{{Structured Parzen Estimator}}},
  author = {Watanabe, Shuhei},
  year = {2023},
  month = may,
  number = {arXiv:2304.11127},
  eprint = {2304.11127},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11127},
  urldate = {2025-01-27},
  abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/bbelucci/Zotero/storage/XY5U77PF/Watanabe - 2023 - Tree-Structured Parzen Estimator Understanding Its Algorithm Components and Their Roles for Better.pdf;/home/bbelucci/Zotero/storage/SVWFFTAR/2304.html}
}

@article{tsne2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  urldate = {2025-01-27},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {/home/bbelucci/Zotero/storage/X3XT8M2Z/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

@misc{whenneuralnets2023,
  title = {When {{Do Neural Nets Outperform Boosted Trees}} on {{Tabular Data}}?},
  author = {McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and C, Vishak Prasad and Feuer, Benjamin and Hegde, Chinmay and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  year = {2023},
  month = oct,
  number = {arXiv:2305.02997},
  eprint = {2305.02997},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.02997},
  urldate = {2024-01-16},
  abstract = {Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. A remarkable exception is the recently-proposed prior-data fitted network, TabPFN: although it is effectively limited to training sets of size 3000, we find that it outperforms all other algorithms on average, even when randomly sampling 3000 training datapoints. Next, we analyze dozens of metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/bbelucci/Zotero/storage/GINLGNQB/McElfresh et al. - 2023 - When Do Neural Nets Outperform Boosted Trees on Ta.pdf;/home/bbelucci/Zotero/storage/NX5EYM35/2305.html}
}

@article{zhang2020deep,
  title = {Deep Learning-Based Clustering Approaches for Bioinformatics},
  author = {Zhang, Zhen and Li, Qiang and Zhu, Zhen and Zeng, Liying and {al.}, et},
  year = {2021},
  journal = {Briefings in Bioinformatics},
  volume = {22},
  number = {1},
  pages = {393--403},
  publisher = {Oxford University Press}
}

@article{ZHANG2022108428,
  title = {Weighted Clustering Ensemble: {{A}} Review},
  shorttitle = {Weighted Clustering Ensemble},
  author = {Zhang, Mimi},
  year = {2022},
  month = apr,
  journal = {Pattern Recognition},
  volume = {124},
  pages = {108428},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2021.108428},
  urldate = {2025-01-18},
  abstract = {Clustering ensemble, or consensus clustering, has emerged as a powerful tool for improving both the robustness and the stability of results from individual clustering methods. Weighted clustering ensemble arises naturally from clustering ensemble. One of the arguments for weighted clustering ensemble is that elements (clusterings or clusters) in a clustering ensemble are of different quality, or that objects or features are of varying significance. However, it is not possible to directly apply the weighting mechanisms from classification (supervised) domain to clustering (unsupervised) domain, also because clustering is inherently an ill-posed problem. This paper provides an overview of weighted clustering ensemble by discussing different types of weights, major approaches to determining weight values, and applications of weighted clustering ensemble to complex data. The unifying framework presented in this paper will help clustering practitioners select the most appropriate weighting mechanisms for their own problems.},
  keywords = {Ensemble selection,Fuzzy clustering,Labeling correspondence,Multi-view data,Temporal data},
  file = {/home/bbelucci/Zotero/storage/MFQMTSJX/Zhang - 2022 - Weighted clustering ensemble A review.pdf;/home/bbelucci/Zotero/storage/BZ4LEHDR/S003132032100604X.html}
}

@article{zimek2012survey,
  title = {A Survey on Unsupervised Outlier Detection in High-Dimensional Numerical Data},
  author = {Zimek, Arthur and Schubert, Erich and Kriegel, Hans-Peter},
  year = {2012},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {5},
  number = {5},
  pages = {363--387},
  publisher = {Wiley Online Library}
}
