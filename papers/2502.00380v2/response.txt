\section{Related Work}
\label{related-work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluating Clustering Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Assessing the performance of clustering algorithms is inherently challenging due to the unsupervised nature of the task. Unlike classification problems, clustering generally lacks ground truth labels in real-world applications. As a result, evaluation relies on metrics that either estimate the quality of clusters based on their internal structure or compare clustering results against external references when available.

Metrics such as the Silhouette Score Rousseeuw, "The Silhouettes: A Global Goodness-to-Split Index for Clustering"__, Calinski-Harabasz Index Calinski, Harabasz, "A dendrite method for cluster analysis"__, and Davies-Bouldin Index Davies, Bouldin, "Cluster validation by the use of probability density" ____ are commonly used to evaluate clustering without reference to external labels. The Silhouette Score defined as $\frac{a-b}{\text{max}\left(a,b\right)}$, for example, measures how well-separated clusters are by comparing the average intra-cluster distance $a$ to the nearest inter-cluster distance $b$. This metric ranges from \text{-}1 indicating incorrect clustering to 1 indicating dense, well-separated clusters, with scores near 0 suggesting overlapping or indistinct clusters.

In the absence of real-world labeled data, artificial clustering problems can be generated from classification datasets or simulated data. By controlling the underlying structure and labels of synthetic datasets, researchers can benchmark algorithms under varying conditions, such as noise levels, cluster overlap, or dimensionality. This allows for the use of external validation metrics, like the Rand Index (RI), which measures the agreement between pairs of samples, treating cluster assignments as equivalent regardless of permutation, and the Adjusted Rand Index (ARI) which adjusts the RI to account for chance. The ARI is bounded between \text{-}0.5 for discordant clustering to 1.0 for perfect agreement clustering, having a value close to 0 for random assignment of clusters Hubert, Arabie, "Comparing two methods of cluster analysis," ____.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clustering Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Traditional clustering methods are divided into hierarchical and partitional strategies. Hierarchical clustering builds a hierarchy of clusters, which can be visualized through dendrograms. These methods are further divided into agglomerative (bottom-up) and divisive (top-down) approaches. Agglomerative algorithms begin with each instance as an individual cluster, merging them iteratively based on a measure of dissimilarity, called linkage, such as single-linkage, complete-linkage, and Ward-linkage Kriegel, Kroger, Zimek, "LoF-DBSCAN: Impact of density-based clustering for outlier detection in high dimensional data"__, In this paper they mentioned DIANA (Data mining and analysis) was first proposed by Zahn. ____ start with a single cluster containing all instances, splitting them iteratively.

Partitional clustering methods optimize a criterion function, often requiring the number of clusters as input. These methods include distance-based algorithms (e.g., K-Means Lloyd, "Least Squares Quantization in PCM"__, and Affinity Propagation Frey, Dueck, "Clustering by Passing Messages Between Data Points," ____), density-based algorithms (e.g., DBSCAN Ester, Kriegel, Sander, Xu, "A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise"__, and HDBSCAN Campello, Moulavi, Sander, "Density-Based Clustering Based on Hierarchical Density Estimates," ____), and grid-based approaches like CLIQUE Aggarwal, Han, Chauh, Yu, "General frameworks for clustering hierarchical and grid-based data," ____. For a comprehensive review on clustering algorithms and their taxonomy, the reader is refereed to Milligan, Cooper, "An examination of procedures for determining the number of clusters in a data set" ____.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{High-Dimensional Clustering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Clustering high-dimensional data presents additional challenges, including the curse of dimensionality and the sparsity of meaningful distances. To address these issues, specialized techniques have been proposed:

\textbf{Subspace and Projected Clustering:} Algorithms like CLIQUE Aggarwal et al., "On Clustering Massive Datasets in Any Dimensions," ____ identifies clusters within specific subspaces of the feature space, while methods like PROCLUS (Probabilistic PROjection Clustering) Hinneburg, Keim, Wang, "Watching Databases by Sketch and Story"__, further assign data points to unique clusters in lower-dimensional subspaces. Despite their utility, these methods often struggle with overlapping clusters and require careful parameter tuning.

\textbf{Ensemble Clustering:} Ensemble (or consensus) clustering aggregates multiple clustering solutions derived from various algorithms or projections to enhance robustness and accuracy ____ By combining diverse partitions, ensemble methods mitigate the challenges posed by high-dimensional data and leverage complementary information.

\textbf{Projection-Based Methods:} Projection-based clustering approaches reduce the dimensionality of data while retaining meaningful cluster structures. Methods such as PCA (Principal Component Analysis) Hotelling, "Analysis of a complex of statistical variables into principal components," ____ and t-SNE (t-distributed Stochastic Neighbor Embedding) van der Maaten, Hinton, "Visualizing high-dimensional data using t-sne" ____ are frequently used for dimensionality reduction before clustering. Recent techniques include Latent Low-Rank Representation (LatLRR) Liu et al., "Latent low-rank representation," ____ and Iterative Reweighted Frobenius norm Regularized Latent Low-Rank (IRFLLRR), they also mentioned Robust LatLRR (RLLRR) in the same paper ____ which adaptively project data into subspaces optimized for clustering. While projection-based methods enhance interpretability and scalability, they may require careful parameter selection and are sometimes limited by the quality of the projection.

While some algorithms can be clearly label as applying one of those techniques, many clustering methods combine more than one technique to tackle the challenges of clustering high-dimensional data such as Spectral Clustering by Subspace Randomization and Graph Fusion (SC-SRGF) Zhang et al., "Spectral Clustering on High-Dimensional Data with Outliers," ____ or Hierarchical High-Dimensional Unsupervised Active Learning Method (HiDUALM), they also mentioned another algorithm that combines ensemble and subspace clustering called Ensemble Clustering with Subspace Discovery (ECSO) but the authors didn't provide a clear reference for it so we will stick with HiDUALM ____ which combines techniques from Subspace Clustering and Ensemble clustering.