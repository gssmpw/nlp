@inproceedings{DOC2002,
  title = {A {{Monte Carlo}} Algorithm for Fast Projective Clustering},
  booktitle = {Proceedings of the 2002 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Procopiuc, Cecilia M. and Jones, Michael and Agarwal, Pankaj K. and Murali, T. M.},
  year = {2002},
  month = jun,
  series = {{{SIGMOD}} '02},
  pages = {418--427},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/564691.564739},
  urldate = {2024-11-18},
  abstract = {We propose a mathematical formulation for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This allows us to develop a Monte Carlo algorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We implemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that our method is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detecting rotated human faces in cluttered images.},
  isbn = {978-1-58113-497-1},
  file = {/home/bbelucci/Zotero/storage/CL5AJ7DB/Procopiuc et al. - 2002 - A Monte Carlo algorithm for fast projective clustering.pdf}
}

@article{RLLRR2014,
  title = {Robust Latent Low Rank Representation for Subspace Clustering},
  author = {Zhang, Hongyang and Lin, Zhouchen and Zhang, Chao and Gao, Junbin},
  year = {2014},
  month = dec,
  journal = {Neurocomputing},
  volume = {145},
  pages = {369--373},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2014.05.022},
  urldate = {2025-01-27},
  abstract = {Subspace clustering has found wide applications in machine learning, data mining, and computer vision. Latent Low Rank Representation (LatLRR) is one of the state-of-the-art methods for subspace clustering. However, its effectiveness is undermined by a recent discovery that the solution to the noiseless LatLRR model is non-unique. To remedy this issue, we propose choosing the sparest solution in the solution set. When there is noise, we further propose preprocessing the data with robust PCA. Experiments on both synthetic and real data demonstrate the advantage of our robust LatLRR over state-of-the-art methods.},
  keywords = {Latent low rank representation,Subspace clustering},
  file = {/home/bbelucci/Zotero/storage/ZMD4BGIQ/S0925231214006365.html}
}

@article{affinitypropagation2007,
  title = {Clustering by {{Passing Messages Between Data Points}}},
  author = {Frey, Brendan J. and Dueck, Delbert},
  year = {2007},
  month = feb,
  journal = {Science},
  volume = {315},
  number = {5814},
  pages = {972--976},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1136800},
  urldate = {2025-01-10},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such ``exemplars'' can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called ``affinity propagation,'' which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
  file = {/home/bbelucci/Zotero/storage/DXZE55UG/Frey and Dueck - 2007 - Clustering by Passing Messages Between Data Points.pdf}
}

@article{agglomerative1984,
  title = {Efficient Algorithms for Agglomerative Hierarchical Clustering Methods},
  author = {Day, William H. E. and Edelsbrunner, Herbert},
  year = {1984},
  month = dec,
  journal = {Journal of Classification},
  volume = {1},
  number = {1},
  pages = {7--24},
  issn = {1432-1343},
  doi = {10.1007/BF01890115},
  urldate = {2025-01-26},
  abstract = {Whenevern objects are characterized by a matrix of pairwise dissimilarities, they may be clustered by any of a number of sequential, agglomerative, hierarchical, nonoverlapping (SAHN) clustering methods. These SAHN clustering methods are defined by a paradigmatic algorithm that usually requires 0(n3) time, in the worst case, to cluster the objects. An improved algorithm (Anderberg 1973), while still requiring 0(n3) worst-case time, can reasonably be expected to exhibit 0(n2) expected behavior. By contrast, we describe a SAHN clustering algorithm that requires 0(n2 logn) time in the worst case. When SAHN clustering methods exhibit reasonable space distortion properties, further improvements are possible. We adapt a SAHN clustering algorithm, based on the efficient construction of nearest neighbor chains, to obtain a reasonably general SAHN clustering algorithm that requires in the worst case 0(n2) time and space.},
  langid = {english},
  keywords = {Algorithm complexity,Algorithm design,Centroid clustering method,Geometric model,SAHN clustering method},
  file = {/home/bbelucci/Zotero/storage/STKHW29M/Day and Edelsbrunner - 1984 - Efficient algorithms for agglomerative hierarchical clustering methods.pdf}
}

@article{ari1985,
  title = {Comparing Partitions},
  author = {Hubert, Lawrence and Arabie, Phipps},
  year = {1985},
  month = dec,
  journal = {Journal of Classification},
  volume = {2},
  number = {1},
  pages = {193--218},
  issn = {1432-1343},
  doi = {10.1007/BF01908075},
  urldate = {2025-01-26},
  abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {\textpm}1.},
  langid = {english},
  keywords = {Consensus indices,Measures of agreement,Measures of association},
  file = {/home/bbelucci/Zotero/storage/U32QM64G/Hubert and Arabie - 1985 - Comparing partitions.pdf}
}

@article{calinsky1974,
  title = {A Dendrite Method for Cluster Analysis},
  author = {Cali{\'n}ski, T. and Harabasz, J},
  year = {1974},
  month = jan,
  journal = {Communications in Statistics},
  volume = {3},
  number = {1},
  pages = {1--27},
  publisher = {Taylor \& Francis},
  issn = {0090-3272},
  doi = {10.1080/03610927408827101},
  urldate = {2025-01-27},
  abstract = {A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the "best number" of clusters is suggested. It is a"variance ratio criterion" giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965).},
  keywords = {approximate grouping procedure,cluster analysis,minimum variance (WGSS) criterion for optimal grouping,numerical taxonomy,shortest dendrite = minimum spanning tree,variance ratio criterion for best number of groups}
}

@article{clique1999,
  title = {Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications},
  author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
  year = {1998},
  month = jun,
  journal = {SIGMOD Rec.},
  volume = {27},
  number = {2},
  pages = {94--105},
  issn = {0163-5808},
  doi = {10.1145/276305.276314},
  urldate = {2024-11-18},
  abstract = {Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.},
  file = {/home/bbelucci/Zotero/storage/3TA4XYSU/Agrawal et al. - 1998 - Automatic subspace clustering of high dimensional data for data mining applications.pdf}
}

@article{clusterensembles2003,
  title = {Cluster Ensembles --- a Knowledge Reuse Framework for Combining Multiple Partitions},
  author = {Strehl, Alexander and Ghosh, Joydeep},
  year = {2003},
  month = mar,
  journal = {J. Mach. Learn. Res.},
  volume = {3},
  number = {null},
  pages = {583--617},
  issn = {1532-4435},
  doi = {10.1162/153244303321897735},
  urldate = {2025-01-27},
  abstract = {This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.},
  file = {/home/bbelucci/Zotero/storage/JFR7ZYSW/Strehl and Ghosh - 2003 - Cluster ensembles --- a knowledge reuse framework for combining multiple partitions.pdf}
}

@article{comprehensivesurvey2022,
  title = {A Comprehensive Survey of Clustering Algorithms: {{State-of-the-art}} Machine Learning Applications, Taxonomy, Challenges, and Future Research Prospects},
  shorttitle = {A Comprehensive Survey of Clustering Algorithms},
  author = {Ezugwu, Absalom E. and Ikotun, Abiodun M. and Oyelade, Olaide O. and Abualigah, Laith and Agushaka, Jeffery O. and Eke, Christopher I. and Akinyelu, Andronicus A.},
  year = {2022},
  month = apr,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {110},
  pages = {104743},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.104743},
  urldate = {2025-01-26},
  abstract = {Clustering is an essential tool in data mining research and applications. It is the subject of active research in many fields of study, such as computer science, data science, statistics, pattern recognition, artificial intelligence, and machine learning. Several clustering techniques have been proposed and implemented, and most of them successfully find excellent quality or optimal clustering results in the domains mentioned earlier. However, there has been a gradual shift in the choice of clustering methods among domain experts and practitioners alike, which is precipitated by the fact that most traditional clustering algorithms still depend on the number of clusters provided a priori. These conventional clustering algorithms cannot effectively handle real-world data clustering analysis problems where the number of clusters in data objects cannot be easily identified. Also, they cannot effectively manage problems where the optimal number of clusters for a high-dimensional dataset cannot be easily determined. Therefore, there is a need for improved, flexible, and efficient clustering techniques. Recently, a variety of efficient clustering algorithms have been proposed in the literature, and these algorithms produced good results when evaluated on real-world clustering problems. This study presents an up-to-date systematic and comprehensive review of traditional and state-of-the-art clustering techniques for different domains. This survey considers clustering from a more practical perspective. It shows the outstanding role of clustering in various disciplines, such as education, marketing, medicine, biology, and bioinformatics. It also discusses the application of clustering to different fields attracting intensive efforts among the scientific community, such as big data, artificial intelligence, and robotics. This survey paper will be beneficial for both practitioners and researchers. It will serve as a good reference point for researchers and practitioners to design improved and efficient state-of-the-art clustering algorithms.},
  keywords = {Automatic clustering,Clustering,Clustering algorithms partitioning,Data mining,Hierarchical clustering,K-Means,Optimization algorithms Machine learning,Supervised learning,Unsupervised learning},
  file = {/home/bbelucci/Zotero/storage/4XMG3LJ6/Ezugwu et al. - 2022 - A comprehensive survey of clustering algorithms State-of-the-art machine learning applications, tax.pdf;/home/bbelucci/Zotero/storage/KFA5BYSC/S095219762200046X.html}
}

@article{dataclustering1999,
  title = {Data Clustering: A Review},
  shorttitle = {Data Clustering},
  author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
  year = {1999},
  month = sep,
  journal = {ACM Comput. Surv.},
  volume = {31},
  number = {3},
  pages = {264--323},
  issn = {0360-0300},
  doi = {10.1145/331499.331504},
  urldate = {2025-01-26},
  abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
  file = {/home/bbelucci/Zotero/storage/UQI2IKFD/Jain et al. - 1999 - Data clustering a review.pdf}
}

@article{davies1979,
  title = {A {{Cluster Separation Measure}}},
  author = {Davies, David L. and Bouldin, Donald W.},
  year = {1979},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-1},
  number = {2},
  pages = {224--227},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1979.4766909},
  urldate = {2025-01-27},
  abstract = {A measure is presented which indicates the similarity of clusters which are assumed to have a data density which is a decreasing function of distance from a vector characteristic of the cluster. The measure can be used to infer the appropriateness of data partitions and can therefore be used to compare relative appropriateness of various divisions of the data. The measure does not depend on either the number of clusters analyzed nor the method of partitioning of the data and can be used to guide a cluster seeking algorithm.},
  keywords = {Algorithm design and analysis,Cluster,Clustering algorithms,Data analysis,data partitions,Density measurement,Dispersion,Humans,Missiles,multidimensional data analysis,Multidimensional systems,parametric clustering,Partitioning algorithms,partitions,Performance analysis,similarity measure},
  file = {/home/bbelucci/Zotero/storage/QFEILV3Y/Davies and Bouldin - 1979 - A Cluster Separation Measure.pdf;/home/bbelucci/Zotero/storage/4F8K886I/4766909.html}
}

@inproceedings{dbscan1996,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  month = aug,
  series = {{{KDD}}'96},
  pages = {226--231},
  publisher = {AAAI Press},
  address = {Portland, Oregon},
  urldate = {2025-01-10},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
  file = {/home/bbelucci/Zotero/storage/HYKJVERP/Ester et al. - 1996 - A density-based algorithm for discovering clusters.pdf}
}

@incollection{diana1990,
  title = {Divisive {{Analysis}} ({{Program DIANA}})},
  booktitle = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  author = {Kaufman, Leonard and Rousseeuw, Peter J.},
  year = {1990},
  month = mar,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  pages = {253--279},
  publisher = {Wiley},
  doi = {10.1002/9780470316801},
  urldate = {2025-01-28},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-0-471-87876-6 978-0-470-31680-1},
  langid = {english}
}

@inproceedings{hdbscan2013,
  title = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  year = {2013},
  pages = {160--172},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37456-2_14},
  abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
  isbn = {978-3-642-37456-2},
  langid = {english},
  keywords = {Cluster Tree,Core Object,Density Threshold,Hierarchical Cluster Method,Minimum Span Tree},
  file = {/home/bbelucci/Zotero/storage/VHLJ6UBQ/Campello et al. - 2013 - Density-Based Clustering Based on Hierarchical Den.pdf}
}

@article{hidualm2024,
  title = {A {{Novel Hierarchical High-Dimensional Unsupervised Active Learning Method}}},
  author = {Haghzad Klidbary, Sajad and Javadian, Mohammad},
  year = {2024},
  month = jul,
  journal = {International Journal of Computational Intelligence Systems},
  volume = {17},
  number = {1},
  pages = {193},
  issn = {1875-6883},
  doi = {10.1007/s44196-024-00601-w},
  urldate = {2024-11-07},
  abstract = {This paper processes a novel hierarchical high-dimensional clustering algorithm based on the Active Learning Method (ALM), which is a fuzzy-learning algorithm. The hierarchical part of the algorithm is composed of two phases: divisible and agglomerative. The divisible phase, a zooming-in-process, searches for sub-clusters in already-found clusters hierarchically. At each level of the hierarchy, the clusters are found by an ensemble clustering method based on the density of data. This part of the algorithm blurs each data point as multiple one-dimensional fuzzy membership functions called ink-drop patterns; then, it accumulates the ink-drop patterns of all data points on every dimension separately. Next, it performs one-dimensional density partitioning to produce an ensemble of clustering solutions; after that, combining the results is done based on a novel consensus method with the aid of prime numbers. An agglomerative phase is a bottom-up approach that merges clusters based on a novel distance metric, named \$\$\{K\}{\textasciicircum}\{2\}\$\$-nearest neighbor. The algorithm is named as the Hierarchical High-Dimensional Unsupervised Active Learning Method (HiDUALM) and is explained in more detail throughout this paper. Although the classical clustering methods are not suitable for high-dimensional data clustering, the proposed method solves the problems related to speed and memory using ensemble learning, while due to its hierarchy and the use of different distance criteria, different levels of the cluster provide the clause. Experiments on synthetic and real-world datasets are presented to show the effectiveness of the proposed-clustering algorithm.},
  langid = {english},
  keywords = {Artificial Intelligence,Ensemble clustering,Hierarchical clustering,High-dimensional clustering,Machine learning (ML),Noise elimination,Unsupervised active learning method (ALM)},
  file = {/home/bbelucci/Zotero/storage/UTIMDKWE/Haghzad Klidbary and Javadian - 2024 - A Novel Hierarchical High-Dimensional Unsupervised Active Learning Method.pdf}
}

@article{irfllrr2023,
  title = {{{LatLRR}} for Subspace Clustering via Reweighted {{Frobenius}} Norm Minimization},
  author = {Liu, Zhuo and Hu, Dong and Wang, Zhi and Gou, Jianping and Jia, Tao},
  year = {2023},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {224},
  pages = {119977},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.119977},
  urldate = {2024-11-19},
  abstract = {Subspace clustering has attracted much attention on account of its excellent performance in various tasks, such as computer vision and pattern recognition. Since the high-dimensional data may comprise redundant information and noises, low-dimensional subspace structure is difficult to be recovered exactly. Existing methods adopt the non-convex Schatten-p norm to approximate the rank function closer. However, these methods are still unable to handle each singular value flexibly, which may yield unsatisfactory solutions. To address this deficiency, in this paper, we propose a iterative reweighted Frobenius norm regularized latent low rank representation (IRFLLRR) model, which can restrain the small and large rank components simultaneously with rational weights. Specifically, the reweighting strategy can preserve the desirable structure information, while removing the sparse noise and redundant information. Hence the constructed coefficient matrix can capture the global structure of data more adequately. To solve the proposed model, an efficient algorithm is developed via the alternating direction method of multipliers (ADMM), which ensures each subproblem can be optimized in closed-form. In addition, we provide detailed mathematical proofs to guarantee that the sequence generated by our algorithm convergences to a Karush--Kuhn--Tucker (KKT) point. Extensive experimental results reveal the superiority of the proposed method over several state-of-the-art methods. MATLAB code is available at https://github.com/wangzhi-swu/IRFLLRR.},
  keywords = {ADMM,Iterative reweighted Frobenius norm,Latent low rank representation,Subspace clustering},
  file = {/home/bbelucci/Zotero/storage/2D9YXBTT/Liu et al. - 2023 - LatLRR for subspace clustering via reweighted Frobenius norm minimization.pdf;/home/bbelucci/Zotero/storage/HS3P8LGT/S0957417423004797.html}
}

@article{kmeans_complexity,
  title = {Algorithm {{AS}} 136: {{A K-Means Clustering Algorithm}}},
  shorttitle = {Algorithm {{AS}} 136},
  author = {Hartigan, J. A. and Wong, M. A.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {28},
  number = {1},
  eprint = {2346830},
  eprinttype = {jstor},
  pages = {100--108},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9254},
  doi = {10.2307/2346830},
  urldate = {2025-01-23},
  file = {/home/bbelucci/Zotero/storage/F9Y3IKS7/Hartigan and Wong - 1979 - Algorithm AS 136 A K-Means Clustering Algorithm.pdf}
}

@inproceedings{kmeansprojective2004,
  title = {K-Means Projective Clustering},
  booktitle = {Proceedings of the Twenty-Third {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Agarwal, Pankaj K. and Mustafa, Nabil H.},
  year = {2004},
  month = jun,
  series = {{{PODS}} '04},
  pages = {155--165},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1055558.1055581},
  urldate = {2024-11-18},
  abstract = {In many applications it is desirable to cluster high dimensional data along various subspaces, which we refer to as projective clustering. We propose a new objective function for projective clustering, taking into account the inherent trade-off between the dimension of a subspace and the induced clustering error. We then present an extension of the k-means clustering algorithm for projective clustering in arbitrary subspaces, and also propose techniques to avoid local minima. Unlike previous algorithms, ours can choose the dimension of each cluster independently and automatically. Furthermore, experimental results show that our algorithm is significantly more accurate than the previous approaches.},
  isbn = {978-1-58113-858-0},
  file = {/home/bbelucci/Zotero/storage/9DPBEH9H/Agarwal and Mustafa - 2004 - k-means projective clustering.pdf}
}

@article{llr2013,
  title = {Robust {{Recovery}} of {{Subspace Structures}} by {{Low-Rank Representation}}},
  author = {Liu, Guangcan and Lin, Zhouchen and Yan, Shuicheng and Sun, Ju and Yu, Yong and Ma, Yi},
  year = {2013},
  month = jan,
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {35},
  number = {1},
  pages = {171--184},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2012.88},
  urldate = {2025-01-18},
  abstract = {In this paper, we address the subspace clustering problem. Given a set of data samples (vectors) approximately drawn from a union of multiple subspaces, our goal is to cluster the samples into their respective subspaces and remove possible outliers as well. To this end, we propose a novel objective function named Low-Rank Representation (LRR), which seeks the lowest rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary. It is shown that the convex program associated with LRR solves the subspace clustering problem in the following sense: When the data is clean, we prove that LRR exactly recovers the true subspace structures; when the data are contaminated by outliers, we prove that under certain conditions LRR can exactly recover the row space of the original data and detect the outlier as well; for data corrupted by arbitrary sparse errors, LRR can also approximately recover the row space with theoretical guarantees. Since the subspace membership is provably determined by the row space, these further imply that LRR can perform robust subspace clustering and error correction in an efficient and effective way.},
  file = {/home/bbelucci/Zotero/storage/RNFYYDB4/Liu et al. - 2013 - Robust Recovery of Subspace Structures by Low-Rank Representation.pdf}
}

@article{pca1901,
  title = {{{LIII}}. {{On}} Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, Karl},
  year = {1901},
  month = nov,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  publisher = {Taylor \& Francis},
  issn = {1941-5982},
  doi = {10.1080/14786440109462720},
  urldate = {2025-01-27},
  file = {/home/bbelucci/Zotero/storage/SMB6YDK2/Pearson - 1901 - LIII. On lines and planes of closest fit to systems of points in space.pdf}
}

@inproceedings{proclus2000,
  title = {Fast Algorithms for Projected Clustering},
  booktitle = {Proceedings of the 1999 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Aggarwal, Charu C. and Wolf, Joel L. and Yu, Philip S. and Procopiuc, Cecilia and Park, Jong Soo},
  year = {1999},
  month = jun,
  series = {{{SIGMOD}} '99},
  pages = {61--72},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/304182.304188},
  urldate = {2024-12-15},
  abstract = {The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data.},
  isbn = {978-1-58113-084-3},
  file = {/home/bbelucci/Zotero/storage/IC85FZAL/Aggarwal et al. - 1999 - Fast algorithms for projected clustering.pdf}
}

@article{reviewclustering2017,
  title = {A Review of Clustering Techniques and Developments},
  author = {Saxena, Amit and Prasad, Mukesh and Gupta, Akshansh and Bharill, Neha and Patel, Om Prakash and Tiwari, Aruna and Er, Meng Joo and Ding, Weiping and Lin, Chin-Teng},
  year = {2017},
  month = dec,
  journal = {Neurocomputing},
  volume = {267},
  pages = {664--681},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.06.053},
  urldate = {2025-01-26},
  abstract = {This paper presents a comprehensive study on clustering: exiting methods and developments made at various times. Clustering is defined as an unsupervised learning where the objects are grouped on the basis of some similarity inherent among them. There are different methods for clustering the objects such as hierarchical, partitional, grid, density based and model based. The approaches used in these methods are discussed with their respective states of art and applicability. The measures of similarity as well as the evaluation criteria, which are the central components of clustering, are also presented in the paper. The applications of clustering in some fields like image segmentation, object and character recognition and data mining are highlighted.},
  keywords = {Clustering,Data mining,Pattern recognition,Similarity measures,Unsupervised learning},
  file = {/home/bbelucci/Zotero/storage/IFSW8WGD/Saxena et al. - 2017 - A review of clustering techniques and developments.pdf;/home/bbelucci/Zotero/storage/3AMY66XC/S0925231217311815.html}
}

@article{sc-srgf2020,
  title = {Spectral {{Clustering}} by {{Subspace Randomization}} and {{Graph Fusion}} for {{High-Dimensional Data}}},
  author = {Cai, Xiaosha and Huang, Dong and Wang, Chang-Dong and Kwoh, Chee-Keong},
  year = {2020},
  month = apr,
  journal = {Advances in Knowledge Discovery and Data Mining},
  volume = {12084},
  pages = {330},
  doi = {10.1007/978-3-030-47426-3_26},
  urldate = {2024-11-08},
  abstract = {Subspace clustering has been gaining increasing attention in recent years due to its promising ability in dealing with high-dimensional data. However, most of the existing subspace clustering methods tend to only exploit the subspace information to ...},
  langid = {english},
  file = {/home/bbelucci/Zotero/storage/GAFSY227/Cai et al. - 2020 - Spectral Clustering by Subspace Randomization and Graph Fusion for High-Dimensional Data.pdf}
}

@article{silhouette1987,
  title = {Silhouettes: {{A}} Graphical Aid to the Interpretation and Validation of Cluster Analysis},
  shorttitle = {Silhouettes},
  author = {Rousseeuw, Peter J.},
  year = {1987},
  month = nov,
  journal = {Journal of Computational and Applied Mathematics},
  volume = {20},
  pages = {53--65},
  issn = {0377-0427},
  doi = {10.1016/0377-0427(87)90125-7},
  urldate = {2025-01-26},
  abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an `appropriate' number of clusters.},
  keywords = {classification,cluster analysis,clustering validity,Graphical display},
  file = {/home/bbelucci/Zotero/storage/ESMIIYH6/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation and validation of cluster analysis.pdf;/home/bbelucci/Zotero/storage/9ITD6JXB/0377042787901257.html}
}

@article{surveyclustering2005,
  title = {Survey of Clustering Algorithms},
  author = {Xu, Rui and Wunsch, D.},
  year = {2005},
  month = may,
  journal = {IEEE Transactions on Neural Networks},
  volume = {16},
  number = {3},
  pages = {645--678},
  issn = {1941-0093},
  doi = {10.1109/TNN.2005.845141},
  urldate = {2025-01-26},
  abstract = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
  keywords = {Adaptive resonance theory (ART),Application software,Bioinformatics,cluster validation,clustering,clustering algorithm,Clustering algorithms,Computer science,Data analysis,Humans,Machine learning,Machine learning algorithms,neural networks,proximity,self-organizing feature map (SOFM),Statistics,Traveling salesman problems},
  file = {/home/bbelucci/Zotero/storage/56G9S4ZK/Xu and Wunsch - 2005 - Survey of clustering algorithms.pdf;/home/bbelucci/Zotero/storage/CR57KWMR/1427769.html}
}

@article{tsne2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  urldate = {2025-01-27},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {/home/bbelucci/Zotero/storage/X3XT8M2Z/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

