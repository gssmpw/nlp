\section{Preliminaries}
In this section, we introduce Q-learning, the key algorithm that inspires {\ours} to extract Q-values from the exploration trees. 
Q-learning~\citep{qlearning} is a traditional model-free reinforcement learning algorithm, where a value function $Q(s,a)$ is trained to represent the expected future rewards by taking action $a$ given state $s$. The optimal Q-function can be written as,
\begin{align}
\begin{aligned}
    \label{equation:q-function}
    Q^{\star}(s,a) = \max_{\pi} \mathbb{E}[&r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots \mid \\ & s_t = s, a_t = a, \pi ],
\end{aligned}
\end{align}
where $\pi$ is the policy, $\gamma$ is the discount factor, and $r_t$ is the received reward at step $t$. Given the definition of optimal Q-fucntion in Equation~\ref{equation:q-function}, the Bellman Optimality Equation~\citep{bellman2015applied} of Q-function can be written as,
\begin{equation}
\begin{aligned}
    Q^{\star}(s_t, a_t) = r_t + \gamma  \max_{a\in\mathcal{A}} Q^{\star}(s_{t+1}, a).
\end{aligned}
\end{equation}
In Q-learning, the value model $Q(s_t,a_t)$ is updated iteratively by,
\begin{align}
    Q(s_t, a_t) \leftarrow &(1-\alpha)Q(s_t, a_t) \\
     & + \alpha ( r_t + \gamma \max_{a\in\mathcal{A}} Q(s_{t+1}, a)),
 \end{align}
where $\alpha$ is the learning rate and $\mathcal{A}$ is the action space. Combining immediate rewards from the current action and future potential rewards from subsequent actions, Q-value can be interpreted as the expected long-term value of taking a specific action in a given state.


 In complex interactive tasks, the agent needs to account not only for immediate rewards but also for the potential long-term effects of its current decisions. This is where the Q-value becomes essential. However, directly adapting RL algorithms such as Q-learning to language agents can be sample-inefficient~\citep{NEURIPS2018_inefficentQ}. This is because the action space in language agent tasks is typically a vast vocabulary, which may lead to an explosion of potential action sequences to be explored. To address this challenge, our approach successfully adapts Q-value extraction to language agent tasks by introducing an exploration tree, which we will introduce in the next section.




