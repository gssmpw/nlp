\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{Discussion}
% \textbf{Choices of Selected Compared Baselines.}
% We do not include larger-scale base models to train the Q function due to the limit of sources in the academia.
% Furthermore, to ensure fair comparison, we do not compare the very latest work like~\citet{wang2024q*}, ~\citet{zhai2024enhancing} and ~\citet{putta2024agentq} in our main table due to the lack of release of their code and slight difference with our experimental setups. Besides, the recent work \citet{zhang2024rest-mcts} combines self-reflection with reward modeling to do self-reinforced training so the comparison might not be so fair, and it takes too long to run and lack of our compute resources.

\textbf{Why supervised train the offline QNet using LLM as the backbone instead of directly using deep Q-learning?}
Directly applying deep Q-learning~\citep{qlearning} to language agents face critical challenges. First, the action space (all possible text outputs) is unbounded and orders of magnitude larger than typical RL environments (e.g., Atari's 18 discrete actions). Standard exploration strategies like $\epsilon$-greedy fail because random text sampling rarely yields meaningful rewards or trajectories. Second, language tasks often involve sparse rewards, destabilizing Q-learning's reliance on frequent reward signals. Pure online Q-learning would suffer from high gradient variance and require infeasible exploration budgets.

Initializing the value function model from a well-pretrained large foundation model can encode rich linguistic and reasoning priors, as well as world commonsense knowledge~\citep{bansal2024videophy, song-etal-2024-eto, xu2022universal, math-shepherd}. So we initialize our QNet with the LLM trained in the agent environment to embrace both knowledge during pretraining and agent specific capabilities, thus boosting the adaption to the long-term value modeling.


\subsection{Experimental details}\label{appendix:exp-details}

\subsubsection{Datasets}
We follow the setup of ETO~\citep{song-etal-2024-eto} to use the three agent tasks for our experiments.

(a) WebShop is an online shopping environment. The available action types for agents include \textit{search[keywords]} and \textit{click[value]}. The agent is instructed to complete the task with ReAct\citep{yao2023react}-style response. The instruction is specified in Figure~\ref{fig:webshop_inst}. 

(b) ALFWorld \citep{shridhar2021alfworld} consists of interactive TextWorld environments paralleling the embodied worlds. In this setup, agents must explore and complete complex household tasks. The ALFWorld dataset includes both seen and unseen evaluation sets. The seen set tests in-distribution generalization, while the unseen set evaluates out-of-distribution generalization, featuring entirely new task instances for the agents to solve.

(c) SciWorld \citep{wang2022scienceworld} is a text-based virtual platform designed around conducting basic scientific experiments across ten task categories, such as thermodynamics and electrical circuits. Agents engage in embodied, interactive environments to grasp scientific concepts through practical tasks. Each task in ScienceWorld includes optional subgoals, with the final reward calculated based on the achievement of these subgoals.

We have summarize the statistics of SFT datasets for behavior cloning on all the environments in the main body. Note that the default reward from the environment is zero for the intermediate step before terminal. For self-generation and tree construction, we set the maximum step as 5 in WebShop and 18 in ALFWorld and SciWorld. For inference, we set the maximum step as 5 in WebShop and 40 in ALFWorld and SciWorld. The instruction templates are displayed in Figure~\ref{fig:webshop_inst}, ~\ref{fig:sciworld_inst} and ~\ref{fig:alfworld_inst}.


\subsubsection{QNet}\label{appendix:qnet}

\textbf{Model Architecture.}
Our QNet is designed by sharing the backbone of the Large Language Model (LLM) and appending a value head to predict Q-values. Specifically, we utilize a pre-trained LLM, denoted as $\text{LLM}_\theta$, which serves as the foundational model for encoding input sequences. The value head is a Multi-Layer Perceptron (MLP) that takes the hidden states from the LLM and outputs scalar Q-value predictions.

Formally, given an input sequence of tokens $\mathbf{x} = (x_1, x_2, \dots, x_n)$, the LLM produces hidden states $\mathbf{h} = (h_1, h_2, \dots, h_n)$:

\begin{equation}
\mathbf{h} = \text{LLM}_\theta(\mathbf{x}),
\end{equation}

where $h_t \in \mathbb{R}^d$ represents the hidden state at time step $t$, and $d$ is the hidden size of the LLM.

The value head $\text{MLP}_\phi$ processes each hidden state $h_t$ to predict the corresponding Q-value $\hat{q}_t$:

\begin{equation}
\hat{q}_t = \text{MLP}_\phi(h_t),
\end{equation}

where $\hat{q}_t \in \mathbb{R}$ is the predicted Q-value at time step $t$, and $\phi$ denotes the parameters of the MLP.

The MLP consists of multiple layers with ReLU activations, culminating in a linear layer that outputs a scalar Q-value. This design allows the model to capture complex patterns in the hidden representations and map them to accurate Q-value estimates.

\textbf{Training Objective.}
Given an explored trajectory $\mathbf{x} = (x_1, x_2, \dots, x_n)$ with an associated target Q-value $q$, we train the QNet by minimizing the Mean Squared Error (MSE) loss between the predicted Q-values $\hat{q}_t$ and the target Q-value $q$ at each time step:

\begin{equation}
\mathcal{L}(\theta, \phi) = \frac{1}{n} \sum_{t=1}^{n} \left( \hat{q}_t - q \right)^2.
\end{equation}

By minimizing this loss, we encourage the QNet to produce consistent Q-value estimations across the sequence that align with the target Q-value $q$. This training objective emphasizes accurate Q-value predictions at each token, reinforcing the model's ability to assess the long-term value of actions throughout the trajectory.

\textbf{Implementation Details.}
In practice, we implement the value head as an MLP with two hidden layers of size 1024 and ReLU activation functions.


The entire model, including the LLM and the value head, operates in bfloat16 precision to optimize memory usage without sacrificing performance. The LLM backbone remains frozen or fine-tuned depending on the specific experimental setup, allowing us to leverage pre-trained language representations while focusing on learning accurate Q-value predictions through the value head. By integrating the value head with the LLM, our QNet effectively combines language understanding with reinforcement learning principles, enabling the agent to make informed decisions based on both linguistic context and estimated future rewards.

\subsection{Algorithms}
\input{algos/stage2}
\input{algos/construct_q_from_tree}
\subsubsection{Pseudocode of exploration tree construction and Q-value distillation}\label{appdix:pseudocode_of_stage2}

In this section, we provide the pseudocode of constructing an exploration tree in stage 2 in Algorithm~\ref{algo:stage2} and and how we distill the Q-value from an exploration tree in Algorithm~\ref{algo:estimate_q}.


\input{algos/Q_guided_exploration}

\subsubsection{Q-guided generation}\label{appendix:q-explore}
In this section, we present the pseudocode of Q-guided generation in Algorithm~\ref{algo:q-explore}, which is a critical component of our framework.


\label{method:perturb}\textbf{Perturbation augmented generation}. In WebShop, due to the limited diversity of sampled actions, we introduce augmenting action diversity with perturbation during this stage, which is realized by prompting GPT-3.5-Turbo to paraphrase the task description. This utilization of perturbation enables us to inject more variability into the prompts that guide action selection, substantially enriching the range and relevance of possible actions. Such enhanced prompts help prepare the model to handle more diverse and unforeseen situations effectively. We augmented the action diversity in all inference-based algorithms when evaluating in WebShop for fair comparison. Noted that it costs too much on ALFWorld and SciWorld, so we only conduct perturbation on the WebShop.

We introduce our implementation details and examples as follows. We use GPT-3.5-Turbo to perturb the task descriptions using the prompt \textit{``Paraphrase the text: \{task description\}''}. We show an illustrative example on a WebShop task in Figure~\ref{fig:perturb}.

\subsection{Hyper-parameters}
\input{tables/hyper-params}
We summarize the hyper-parameters used across both all stages of {\ours} in this section.
The hyper-parameters leveraged in behavior cloning and self-training is in Table~\ref{tab:hyper-params}. Training QNet shares all the same hyperparameters, except that the number of training epochs is set to 2.

\input{figs/webshop_inst}
\input{figs/perturbation}


