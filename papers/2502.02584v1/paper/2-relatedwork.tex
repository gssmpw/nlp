\section{Related Work}
\subsection{Large Language Model Agent}
Large language models have shown impressive performance in complex interactive tasks, such as web navigation~\citep{yao2022webshop}, scientific reasoning~\citep{wang2022scienceworld}, and action planning in embodied environments~\citep{shridhar2021alfworld}.  ReAct ~\citep{yao2023react} developed a prompting method to shape language models as agents that can reason and act. While several works ~\citep{shen2024hugginggpt,song2023restgpt} improve agent performance with closed-source LLM controllers, the open-source LLM agents still offer unique advantages like accessibility and customization. FireAct ~\citep{chen2023fireact} and LUMOS ~\citep{yin2024lumos} leverage high-quality data generated by experts and employ teacher-forcing to improve the performance of open-source agents. In line with this, our {\ours} is also based on open-source LLMs.


\subsection{Self-Improvement for LLM}
The self improvement of LLM can be a good way to improve LLM without heavy human annotation, which can be divided into two parts.
(1) Training models on self-generated data is a promising approach. A large number of works~\citep{dou2024re-rest,wang2022learning,yuan2023RFT,singh2023rest-em, gulcehre2023rest, math-shepherd} follow the paradigm of self-training, which filters positive self-generated data and performs model training on those filtered data. Some other works ~\citep{song-etal-2024-eto,setlur2024-8fold} utilize both positive and negative data to construct preference pairs and update the policy using direct preference optimization~\citep{rafailov2024direct}. 
(2) Another approach is to scale up the computation of inference to improve the outputs of LLMs. The methods include guiding the inference based on scalar-based reward models~\citep{wang2024q*, xu2022universal, zhai2024enhancing} and modifying the output conditioning on the language feedback (critique provided by the LLM itself or another critique LLM)~\citep{zhou2024language, wu2024vdebugger, shinn2023reflexion}. 
In our paper, we focus on the self-improvement at inference time using our proposed process reward models.

\subsection{Process Reward Modeling for LLM}
Existing works have explored various strategies and reasoning policies for process reward modeling.
~\cite{mathprocessoutcome} and~\cite{verifystepbystep} utilize human-annotated step-level correctness to train a reward model. Math-Shepherd~\citep{math-shepherd} infers per-step rewards through random rollouts. TS-LLM~\citep{tsllm} employs an MCTS-based policy and infers per-step rewards using the TD-$\lambda$~\citep{sutton1988learning} method. 
ReST-MCTS*~\citep{zhang2024rest-mcts} uses Monte Carlo tree search (MCTS) with re-inforced self-training to enhance the diversity and performance on general reasoning tasks like maths, science, and code.
Most recently, \citet{wang2024q*} and \citet{zhai2024enhancing} also use step-level guidance for agent inference through training a step-level value model. \citet{putta2024agentq} applies a hybrid process reward modeling for web navigation tasks by combining Monte Carlo Tree Search (MCTS) rewards with scores generated by large language models to form process rewards. 
Our approach focuses on solving complex agent tasks by providing effective per-step guidance for LLM agent inference. 
Our method differs from \citet{putta2024agentq} because we do not rely on a strong proprietary LLM to provide rewards. Compared with \citet{wang2024q*} and \citet{zhai2024enhancing}, we shift our focus on more complex agent tasks with larger search space and deeper search depth like ALFWorld and SciWorld. Compared with ~\citet{zhang2024rest-mcts}, our framework is much simpler with less stages of training and more straightforward to make process reward modeling works better compared with strong training-based baselines.
