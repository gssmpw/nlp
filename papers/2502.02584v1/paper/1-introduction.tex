\section{Introduction}
\label{sec:introduction}
\input{figs/pipeline}


Supervised fine-tuning (SFT) is commonly employed to make base LLMs perform effective reasoning and planning in complex agent tasks by imitating expert trajectories~\citep{chen2023fireact,yin2024lumos}. 
However, the substantial human annotations required to collect training data present a significant bottleneck, limiting both performance and scalability. 
This challenge is particularly pronounced in agent tasks~\citep{yao2022webshop, shridhar2021alfworld, wang2022scienceworld}, where data scarcity is a critical issue due to the inherent complexity and diversity of real-world interactions. 
To overcome this challenge, self-improvement techniques have shown to be a promising area of research, enabling LLMs to learn from self-generated data without extensive human intervention~\citep{wang2022learning, singh2023rest-em, vstar, zhang2024rest-mcts}, and most recently, enable LLMs to improve their outputs by scaling up their test-time computation in a more intelligent way~\cite{wang2024q*, shinn2023reflexion, snell2024scaling}.
Inspired by this, we focus on improving the inference-time search for language agents, which is a crucial technique for the success of more generalizable agents in real-world environments. 
% 

An essential component of inference-time scaling methods is the reward model~\citep{snell2024scaling}, which evaluates the quality of self-explored data. Many existing works derive a single outcome reward based on ground-truth~\citep{wang2024q*, shinn2023reflexion}. Although this approach is straightforward, it falls short in handling complex agent tasks, since an outcome-reward model cannot accurately score each step within a long trajectory in intricate scenarios. In addition, a trajectory achieving a high final outcome reward does not necessarily indicate that every action taken was optimal; the agent may have completed the task successfully, but some actions could have been inefficient or suboptimal~\citep{mathprocessoutcome}.

Therefore, a good process reward model is necessary to learn from the environmental feedback and provide stepwise evaluations of agent actions. This model enables the agent to fully understand and learn from the intermediate stages of complex tasks, ultimately improving performance and generalization. The key challenge lies in developing an effective process reward model for self-improvement without relying on extensive human annotations for the stepwise reward. There has been a thread of work that has focused on process reward modeling~\citep{mathprocessoutcome, verifystepbystep, math-shepherd, autoprm-chen-etal-2024}. However, these methods rely on either costly human annotation or computationally heavy random rollouts, rendering them inefficient for the self-improvement of language model agents.

To reduce the annotation reliance on process rewards, we propose {\ours} to perform effective process reward modeling to guide agent inference. Specifically, we explicitly formalize the self-generated exploratory trajectories as exploration trees and update the process rewards of all the tree nodes based on the tree structures. To better capture the future utility at each step of a multi-turn reasoning process, we employ the Bellman equation \citep{bellman2015applied} to learn a Q-based process reward. Unlike simple outcome-based rewards, this Q-value captures how immediate decisions contribute to longer-term payoffs, enabling finer-grained control over the reasoning trajectory. Moreover, the Bellman update rule iteratively refines the estimated Q-values by propagating future rewards back to earlier states, reducing reliance on sparse or delayed feedback signals. This allows us to efficiently gather supervision for state-action pairs without requiring explicit annotations of full trajectories. With these Q values in hand, we can then train a function approximator (QNet) \citep{qlearning} to predict the expected return of any partial solution, ultimately providing a strong inductive bias to guide open-language agents. By prioritizing actions with higher estimated Q values, the agent steers its own reasoning in a more targeted manner, facilitating efficient stepwise planning within expansive search spaces.


While recent attempts like KIMI-k1.5~\citep{team2025kimi} and Deepseek-R1~\citep{guo2025deepseek} report failures in process reward modeling, we argue that such modeling is indispensable for agent tasks. Back-and-forth agent behaviors inherently create stepwise inefficiencies (e.g., repetitive environment queries or cyclic reasoning), which the sparse outcome rewards cannot diagnose. Our Q-value estimation directly addresses this by propagating future utility backward through trajectories, dynamically pruning actions with low reward while preserving critical decision points. This enables agents to disentangle productive reasoning from wasteful loops, even with limited supervision. To summarize, our contribution can be divided into three folds:

    \textbf{1)} \textbf{Process Reward Modeling with Q-Value Estimation}: We introduce {\ours}, a novel strategy that leverages estimated Q-values to generate intermediate annotations for language agents, providing stepwise guidance for model inference. We visualize the overall framework of {\ours} in Figure~\ref{fig:pipeline}.    
    
     \textbf{2)} \textbf{Q-Guided Generation Strategy}: We propose a Q-guided generation technique that significantly enhances agent performance via process-based guidance during inference, ensuring effective decision making at each step.    
     
    \textbf{3)} \textbf{Superior Performance with Limited Supervision}: {\ours} shows strong performance on a set of diverse agent environments, including WebShop, ALFWorld, and SciWorld. {\ours} can give effective inference-time guidance even when nearly half of the annotated data is reduced. These experimental results highlight the efficiency and robustness of {\ours} in scenarios with limited supervision.
    
