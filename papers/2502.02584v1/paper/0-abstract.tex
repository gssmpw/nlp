\begin{abstract}
Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance.
To address this, we propose \textbf{{\ours}} (\textbf{Q}-guided \textbf{L}anguage \textbf{A}gent \textbf{S}tepwise \textbf{S}earch), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents.
By introducing a exploration tree and performing process reward modeling, {\ours} provides effective intermediate guidance for each step. 
With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks.
Notably, even with almost half the annotated data, {\ours} retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically show that {\ours} can lead to more effective decision making through qualitative analysis.
\footnote{We will release our code and data in \url{https://github.com/Rafa-zy/QLASS}}
%  
\end{abstract}