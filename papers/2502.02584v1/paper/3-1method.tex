
\section{\ours~Pipeline Details}
\input{algos/pipeline}
In this section, we will follow the order of {\ours} pipeline and introduce each critical component step by step. The overall pipeline is shown in Figure~\ref{fig:pipeline} and Algorithm~\ref{algo:pipeline}.

First, we will describe the initial stage of behavior cloning. Then, we will explain how the exploration tree is constructed during the second \textbf{\textit{self-generation}} stage and how we use it to extract Q-values as the supervision to train Q-network (\textbf{\textit{QNet Training}}). Finally, we will detail the \textit{\textbf{Q-guided generation}} how the QNet is employed to guide the agent's test-time inference in a stepwise manner.

\subsection{Behavioral Cloning}
 Behavior cloning provides a strong initial foundation for language agents by supervised fine-tuning on expert trajectories. Formally, the first stage of {\ours} is to supervised fine-tune our language agent, denoted as the policy $\pi$, on a set of annotated samples $\mathcal{D}_{expert}$. We use ReAct~\citep{yao2023react}-style data for supervised fine-tuning, which additionally generates Chain-of-Thought (CoT)~\citep{cot} reasoning paths before executing each action. We will use $a$ to denote the complete ReAct-style response generated by $\pi$ for simplicity.

Formally, given a dataset $\mathcal{D}_{expert} = \{(u_i, a^i_t,o^i_t)_{t=1}^T\}_{i=1}^{N}$, where $u_i$ represents the task description, $T$ is the trajectory length, $N$ is the number of trajectories in the expert dataset, $o^i_t$ is the environment observation after taking action $a^i_t$ at step $t$, we optimize the policy $\pi$ by minimizing the negative log-likelihood loss:
\begin{equation}
\label{equation:BCloss}
\mathcal{L}(\theta) = - \sum_i \sum_t \log \pi_\theta(a^i_t \mid u_i,a^i_{<t},o^i_{<t}),
\end{equation}
where $\theta$ denotes the parameters of the policy model $\pi_\theta$, which outputs the probability of action $a$ given task description $u$ and historical interactions $h_t=\{a_{<t},o_{<t}\}$.


\subsection{Constructing an Exploration Tree}
The supervised fine-tuned agents can explore the environment and collect a large amount of trajectories. However, due to the extremely large search space of language agents, directly sampling trajectories without any guidance may lead to low efficiency. To address this issue, we propose to construct an exploration tree during \textbf{\textit{self-generation}}.

\input{figs/tree}

\subsubsection{Tree Structure}
For a trajectory, we take the task description as the root node, and each node below the root node is composed of the state, action, and related information for each step. For all trajectories of a task, they can be seen as different branches that originate from the same root node.

Specifically, a Tree Node $N$ in an exploration tree $\mathcal{T}$ is defined as a set of the following attributes:

\textbf{State} ($s_t$): Represents the accumulated historical context from the initiation of the process up to the current time step $t$, encapsulating all preceding reasoning paths and actions. Formally, the state at time $t$ is given by
\begin{equation}
\label{equation:state_definition}
    s_t = \{u, a_1, o_1, \ldots, a_{t-1}, o_{t-1} \},
\end{equation}
    including the task description $u$ and history at step $t$.
    
    
\textbf{Action} ($a_t$): denotes the specific operation performed at the current node, which affects the subsequent state. The action is selected by the policy language agent $\pi$ and is conditioned on the current state and reasoning path.
    
    
\textbf{Reward} ($r_t$): the immediate feedback received from environment after performing action $a_t$. In most language agent tasks, the immediate rewards from environments are set to zero or very sparse. For example, WebShop~\citep{yao2022webshop} only provides a final reward from 0 to 1 at the end of trajectories. 


\textbf{Children} ($\mathcal{C}$): is represented by a list containing nodes explored at the next step.

\textbf{Q-value} ($q$): represents the expected total future reward achievable starting from the current state $s_t$, taking action $a_t$. The Q-values are updated once an exploration tree is constructed. We will introduce how we extract Q-values in the following section.

\subsubsection{Tree Construction}
With each step in a trajectory formalized as a TreeNode, the entire trajectory is a branch within an exploration tree. To explicitly construct an exploration tree that captures potential generations from the root node (i.e., the initial task), exploring new trajectories can be viewed as expanding new branches from the existing TreeNodes. 
 For any non-leaf tree node, effective generation can be achieved by:
 \textbf{1)} directly exploring and adding new child nodes that differ from the existing ones. \textbf{2)} For each branch that reaches a leaf node, we assess its quality based on the final reward provided by the environment. If the branch yields a zero reward, we stop generation on that branchâ€™s nodes, thereby reducing ineffective generation.
We introduce our strategy for tree construction in detail as follows.

\textbf{Tree Pruning.} In practice, we have found that the average depths of tree searching for agent tasks are large. Building an exploration tree and expanding every potential tree nodes may lead to heavy cost to the trajectory generation. To address this, we propose several strategies to reduce the computational burden during tree construction.
We employ pre-pruning techniques to lower the generation costs when constructing an exploration tree for each task. First, we limit the expansion of tree nodes to the early stages of a trajectory (e.g., the first three to five steps, depending on the environment's complexity, with details provided in Appendix~\ref{appendix:exp-details}). 

Next, when a branch leads to a zero-outcome reward at its leaf node, we propagate a \texttt{Stop expansion} signal from the leaf node back to the earliest unexpanded intermediate node on that branch. This helps prioritize the generation of optimal trajectories given a limited generation budget. This construction process is illustrated in Figure~\ref{fig:tree}.

With a set of exploration trees, we aim to efficiently gather effective step-wise signals for training an effective process reward model. Since most language agent tasks only return an outcome reward at the end of the trajectory, which is stored at the leaf nodes of the exploration tree, we need to develop methods to leverage these outcome rewards to generate effective intermediate signals. 

\textbf{Extracting Q-values.} After constructing an exploration tree, with the outcome rewards stored in leaf node rewards, we estimate the Q-values for each intermediate node leveraging
\begin{equation}
\label{equation:update_q}
    Q(s_t, a_t) = r_t + \gamma \max_{a_{t+1} \sim \mathcal{C}_t} [ Q(s_{t+1}, a_{t+1}) ],
\end{equation}
where $\gamma$ is the discount factor, $s_{t+1}$ is the new state after action $a_t$,$\ \mathcal{C}_t$ is the children set containing nodes explored at the next step, and the expectation is over actions $a_{t+1}$ drawn from the policy $\pi$.
We provide the pseudocode of tree construction and Q-value estimation on the exploration trees in Appendix~\ref{appdix:pseudocode_of_stage2}.
    
\subsection{QNet Training}

Inspired by the value function representing the expected long-term value in Q-learning~\citep{qlearning}, we extract Q-values for each node on the exploration trees using Equation~\ref{equation:update_q}. For each node \(N=(s,a,q,\dots)\) in the collected exploration trees, we can then construct a supervised dataset \(D_Q=\{(s,a,q)\}\) to train a Q-network (QNet), initialized from a supervised-fine-tuned LLM. Directly applying online Q-learning in language settings is often impractical, due to the unbounded action space of natural language and the sparse nature of rewards. Instead, by labeling each node with a corresponding Q-value offline and then training QNet in a purely supervised manner, we bypass the instability and excessive exploration costs that typical reinforcement learning loops would incur in high-dimensional language environments. The model architecture of QNet is introduced in Appendix~\ref{appendix:qnet}.


\textbf{Training Objective.} Given each exploration tree $\mathcal{T}$ with $n$ nodes: $\mathcal{T} = (N_1, N_2, \dots, N_n)$, we train the QNet $ \mathcal{Q}_\phi$ by minimizing the Mean Squared Error (MSE) loss between the predicted Q-values $\hat{q}_t$ and the target Q-value $q$ calculated previously at each time step,
\begin{equation}
\mathcal{L}(\phi) = \frac{1}{n} \sum_{t=1}^{n} \left( \hat{q}_t - q_t \right)^2.
\end{equation}
By minimizing this loss, we encourage the QNet to produce consistent Q-value estimations across the sequence that align with the target Q-value $q$. This training objective emphasizes accurate Q-value predictions at each token, reinforcing the model's ability to assess the long-term value of actions throughout the trajectory.

\subsection{Q-Guided Generation}
The effectiveness of a good process reward model can be represented by whether it can lead to better agent self-improvement. Therefore, we conduct Q-guided generation for self-improvement to evaluate the effectiveness of {\ours}. Q-guided generation enables agents to generate each step under the guidance of QNet. At each step, agents sample several actions and the one with the highest Q-value is executed by the agent. We provide a more detailed algorithm of Q-guided generation in Appendix~\ref{appendix:q-explore}.

In this section, we introduce {\ours}, a strategy that leverages Q-value estimation for process reward modeling, providing step-wise guidance for language agents. Additionally, we propose a Q-guided generation strategy that enhances decision-making of the language agent by using Q-values to drive more effective generation during inference. 
