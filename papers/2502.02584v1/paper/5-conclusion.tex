\section{Conclusion}

In this paper, we introduce {\ours}, a novel approach that enhances open-source language agents at inference time by integrating Q-value-based process guidance. By modeling the Q-value at each intermediate step during planning, our method offers step-wise feedback that surpasses the limitations of outcome-based reward models, particularly in complex, long-horizon tasks. 
Through extensive experiments, we have demonstrated that {\ours} significantly improves the language agent to search more intelligently. Moreover, our method demonstrates strong performance even in scenarios with limited annotated data used for behavior cloning. This work paves the way for more efficient and scalable self-improvement techniques in language models, enabling them to tackle complex tasks with reduced reliance on human annotations.

% \section*{Impact Statement}
% Our work aims to improve the inference-time search for open language agents by learning Q-based process reward which reduces the reliance on extensive human annotations. By leveraging self-generated data and more efficient stepwise evaluations, we hope to lower the barrier to developing advanced language agents for tasks where data is scarce. This has the potential to broaden access to high-quality AI-driven solutions, enabling researchers, even those with limited resources—to deploy models that can handle complex, multi-turn tasks without much human annotation efforts.

% From an ethical and societal standpoint, more capable and autonomous language agents raise considerations regarding misinformation, biases, and potential misuse. In particular, any method that enhances an agent’s ability to explore and generate content at scale should be paired with careful oversight and robust filtering mechanisms. While our approach focuses on improving performance through better trajectory evaluation rather than content manipulation, we acknowledge that improved capabilities can be leveraged for harmful or deceptive purposes if not properly governed.







