\section{Experiment}
\input{tables/dataset_stats}
\input{tables/self_improvement}

In this section, we aim to evaluate the effectiveness of {\ours} for solving complex agent tasks in the following aspects:
1) whether {\ours} can aid better decision making on different complex agent tasks;
2) whether the Q-value in {\ours} is an effective process reward to facilitate self-improvement;
3) whether {\ours} can retain strong performance with reduced annotated data.

\subsection{Setup}
\textbf{Datasets.}
We assess the ability of {\ours} on WebShop~\citep{yao2022webshop}, ALFWorld~\citep{shridhar2021alfworld} and SciWorld~\cite{wang2022scienceworld}. These environments only provide a single outcome reward at the end of each trajectory. The statistics of three agent datasets are displayed in Table~\ref{table:dataset_stats}.
The evaluation metric is the reward averaged on the test sets. During the sampling process, environments will give a termination signal when certain actions like ``Click[Buy Now]'' in Webshop are taken or the set maximum steps are reached. Details can be found in Appendix~\ref{appendix:exp-details}.


\textbf{Training Setup.}
In our work, we mainly use Llama-2-7B-Chat as base policy model and QNet backbone. 
We train our models mainly using 4 or 8 A6000 GPUs. The experiments on Webshop, including the training of SFT model, QNet, self-generation and Q-guided exploration, takes one or two days and the experiments on ALFWorld and SciWorld takes four or five days. The detailed hyper-parameters for training and model architectures can be found in Appendix~\ref{appendix:exp-details}.

\textbf{Baselines}.
1) \textbf{SFT}~\citep{chen2023fireact} is the base agent after supervised fine-tuning on the expert data.
2) \textbf{RFT} (Rejection sampling Fine-Tuning)~\citep{yuan2023RFT} is a self-improvement baseline which is trained on the merged data consisting of successful trajectories sampled and expert data. 
3) \textbf{ETO}~\citep{song-etal-2024-eto} is a self-improvement baseline which updates policy via constructing trajectory-level preference pairs and conducting DPO.
4) \textbf{PPO} (Proximal Policy Optimization)~\citep{schulman2017proximal}: a reinforcement learning baseline which directly trains the base agents to optimize the final rewards.
5) \textbf{Best-of-N} samples N trajectories for each task and selects the one with the highest oracle outcome reward. 

N is set to 6 in Table~\ref{tab:self_improvement} and Table~\ref{tab:fewer_annotation}.
All the inference-time baselines in the tables are under the same search budget for fair comparison.
6) \textbf{Closed-source agents}: GPT-3.5-Turbo and GPT-4 with ReAct prompting~\citep{yao2023react}, and methods depending on the emergent properties of self-reflection and planning from large proprietary models, and we use \textbf{Reflexion}~\citep{shinn2023reflexion} as the baseline (use GPT-4o as the base model).

\subsection{Evaluation Results}
In this section, we compare the performance of our {\ours} with all the baselines on WebShop, SciWorld, and ALFWorld. We evaluate all algorithms using one-shot evaluation. The decoding temperatures are set to 0.7 for {\ours} and Best-of-N and 0 for other baselines.

\noindent \textbf{Overall Baseline Comparison.} Results are summarized in Table~\ref{tab:self_improvement}. From Table~\ref{tab:self_improvement}, we can observe that {\ours} consistently achieves the highest scores among all the open-sourced baselines, including both training-based methods and inference-based methods. {\ours} also demonstrates comparable performance with the best proprietary baselines. Specifically, GPT-4 is the state-of-the-art model, but {\ours} still outperforms it on all three benchmarks by 17.9\% on average, especially on SciWorld and ALFWorld. Also, {\ours} outperforms ETO and PPO consistently by over 5\% on average, which are two strong baselines based on multiple stages of training, including supervised fintuning on expert trajectories, training reward models and doing DPO or PPO on the explored trajectories. We achieve better performance while avoiding the heavy cost (including the hyperparameter tuning on DPO / PPO).


\noindent \textbf{Inference-time Search Efficiency.}
\input{figs/inference_comparison}
We compare {\ours} and Best-of-N under different search budgets and visualize the results in Figure~\ref{fig:infer_curve}.
We find that increasing the number of completion tokens will improve the performance of all inference methods. We can observe that {\ours} is consistently better than Best-of-N under almost all the search budgets.  
Another notable observation is that compared with Best-of-N (68.4) under 400K tokens, {\ours} (70.3) with only about half of search budgets under 240k tokens, outperforms the highest score of Best-of-N (68.4).
Also, as the completion tokens approach 360K, Best-of-N begins to flatten, while the score of {\ours} still gets improved by a relatively larger margin from 360K tokens to 400K tokens. This indicates that our approach is a more effective way to scale up the compute for inference-time self-improvement.
\footnotetext[1]{Part of the results results are adopted from \cite{song-etal-2024-eto} and \cite{zhou2024language}.}


\input{figs/self_train_comparison}
\noindent \textbf{Self-training Performance.}
Since process reward modeling is an important module in our framework, we ablate on how different choices of process reward can affect the performance. We mainly experiment with three approaches of constructing process rewards for each intermediate nodes on the exploration trees:
\texttt{Q-value} (ours) is to estimate Q-value for each state-action pair (i.e. each tree node except for root node) using Equation~\ref{equation:update_q}; \texttt{Avg reward}~\citep{math-shepherd} computes the averaged the final rewards; \texttt{Reward}~\citep{yuan2024free} directly treats the final outcome reward and backpropagates it as the process reward for each intermediate step.
In addition to the self-improvement at inference time, we also evaluate the effectiveness of {\ours} for selecting high-quality data for self-training. We train the base agent on the SFT dataset in addition to the Q-guided generated data. Results are visualized in Figure~\ref{fig:self_train+prm}. We observe that {\ours} achieves the highest among all the self-training baselines, compared with RFT which leverages oracle outcome rewards to filter high-quality trajectories and baselines guided by other process reward models such as \texttt{Reward} and \texttt{Avg Reward}.


\noindent \textbf{Ablation on Process Reward Modeling.}
We train three different process reward models guiding trajectory generation for self-training. Self-training results are in Figure~\ref{fig:self_train+prm}. From Figure~\ref{fig:self_train+prm}, we can observe that \texttt{Q-value} utilized by our {\ours} yields the best performance, while the one using \texttt{Avg reward} is slightly better than the one directly using \texttt{Reward}, indicating the effectiveness of using \texttt{Q-value} to model process reward.



\begin{figure*}[htbp]
    \centering
\includegraphics[width=0.9\linewidth,trim={0 0 0 55},clip]{figs/raw/case_alf3.pdf}
    \vspace{-35pt}
    \caption{One example on the ALFWorld, the right is {\ours} and the left is the SFT baseline.}
    \vspace{-8pt}
    \label{fig:case_alfworld}
\end{figure*}

\subsection{Fewer Annotations}
\input{tables/fewer_annotation}
\input{tables/arch_ablation}
In many real-world applications, collecting large amounts of expert-annotated data is both time-consuming and costly. To evaluate the effectiveness of our approach under such constraints, we designed this setup with fewer annotations to test how quickly the agents adapt to new environments in this section.
We extract 1000 trajectories as a subset from the original 1938 trajectories. Under this setup, all baselines can only conduct behavior cloning with access to the SFT dataset of 1K trajectories. After that, baselines like RFT, ETO and {\ours} which involve generation can explore on 1938 tasks.
The performance comparison is listed in Table~\ref{tab:fewer_annotation}. We can observe that {\ours} outperforms other methods trained on both the full WebShop training set and WebShop-1000 subset. This highlights the robustness of our method, especially its potential in scenarios with scarce expert data. While other methods like RFT and SFT show a significant drop in performance, {\ours} remains effective, demonstrating the advantage of Q-guided generation for data selection even in annotation-limited environments.


\subsection{Case Study}

We pick out an example from ALFWorld in Figure~\ref{fig:case_alfworld} to showcase the difference between baselines and our models.
The SFT agent correctly picks up the lettuce, cools it using the fridge, and places it on the countertop in the beginning. However, it continues performing redundant actions afterward, such as repeatedly opening and closing the fridge. The environment responds with ''Nothing happened`` until the agent exhausts its step limit, failing to recognize the task is already complete.
By contrast, the \ours{} uses a stepwise reward mechanism. Once it has cooled the lettuce and placed it on a countertop, it gains no further reward from reopening the fridge or replacing the lettuce. Consequently, the {\ours} avoids futile actions and terminates as soon as the goal is satisfied, successfully completing the task in fewer steps. We can observe that Q-value gradually grows with the number of steps, but suddenly converges to an extremely high or low value at Action 5, indicating it is a key step that differentiates success and failure, where {\ours} assigns ''cool lettuce with fridge 1`` with very high Q-value, only gives 0.10 to ''close fridge 1`` that leads to nonsense behavior. 

\subsection{Ablations Across Different Base Policy Models}
To validate the robustness of our method across different model architectures, we also conduct experiments on a large base model: Llama-2-13B. As shown in Table~\ref{tab:sciworld_reward}, {\ours} still outperforms the baseline reported in \citet{song-etal-2024-eto} on the SciWorld benchmark.