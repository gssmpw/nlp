\begin{algorithm}[tb]
\caption{General {\ours} Pipeline}
\label{algo:pipeline}
\begin{algorithmic}
\STATE \textbf{Input:} Expert dataset $\mathcal{D}_{expert} = \{(u_i, a^i_t,o^i_t)_{t=1}^T\}_{i=1}^{N}$, policy $\pi_\theta$, QNet $\mathcal{Q}_{\phi}$
   
   \STATE \textbf{Stage 1: Behavior Cloning}
   \STATE Train $\pi_\theta$ on $\mathcal{D}_{expert}$ minimizing loss~\ref{equation:BCloss} 
   
   \STATE \textbf{Stage 2: Construct Reasoning Trees}
   \FOR{$i = 1$ \textbf{to} $N$ } 
   
     \STATE Construct a reasoning tree with Algorithm~\ref{algo:stage2}
     \STATE Update Q-values recursively with Equation~\ref{equation:update_q}
   \ENDFOR
   
   \STATE Collect Q-values from $\{T_i\}_{i=1}^N$ as dataset $\mathcal{D}_Q$
   
   \STATE \textbf{Stage 3: QNet Training}
   \STATE Train QNet $\mathcal{Q}_{\phi}$ on dataset $\mathcal{D}_Q$
   
   \STATE \textbf{Step 4: Q-guided Generation}
   \STATE Use QNet $\mathcal{Q}_{\phi}$ to score state-actions at each step
\end{algorithmic}
\end{algorithm}
