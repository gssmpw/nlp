\begin{algorithm}[tb]
\caption{Q-guided Generation}
\label{algo:q-explore}
\begin{algorithmic}
\STATE \textbf{Input}: A LLM agent $\pi_\theta$, a given task description $u$, an action set $\mathcal{A}_{t}$ containing $M$ candidates at step $t$, a trained QNet $\mathcal{Q}_\phi$, sampled trajectory number $N$, max trajectory length $L$

   \STATE traj\_candidates = [ ]
   
   \FOR{$i = 1$ \textbf{to} $N$}
      \STATE Initialize state $s_i \gets [u]$
      
      \FOR{$t = 1$ \textbf{to} $L$}
         \STATE Collect a set of action candidates $\mathcal{A}_t \gets \text{Sample } a \sim \pi_\theta( a \mid s_i)$ for $M$ times
         \STATE $a_t \gets \text{argmax}_{a \sim \mathcal{A}_t} \mathcal{Q}_\phi(s_i, a)$ 
         \hfill \textcolor{gray}{$\vartriangleright$Select the best action with max Q-value}
         \STATE Take action $a_t$, and receive new observation $o_t$ from environment
         \STATE $s_i \gets s_i + [a_t, o_t]$ \hfill \textcolor{gray}{$\vartriangleright$Update state with executed action and new observation}
         
         \IF{$s_i$ is the final state}
            \STATE \textbf{break} \hfill \textcolor{gray}{$\vartriangleright$Exit loop if stop condition is met}
         \ENDIF
      \ENDFOR
      
      \STATE traj\_candidates.append($s_i$)
   \ENDFOR
   
   \STATE Select the best trajectory $s$ with best final reward $s.\text{reward}$ from traj\_candidates
\end{algorithmic}
\end{algorithm}
