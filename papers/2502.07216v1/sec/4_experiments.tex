

\section{Experiment}



\begin{table}[]
\centering

\caption{Ablation studies on component effectiveness.} 
	% \scriptsize
 % \footnotesize
	\vspace{-8pt}
    % \resizebox{1.0
    %     \linewidth}{!}{
% \begin{tabular}{l|l|cccc}
\begin{tabularx}{0.47\textwidth}{l|XXXX}
\hline
              & AP$_{Tot.}$ & AP$_{S}$ & AP$_{M}$ & AP$_{L}$ \\ \hline
Swin Block      & 0.577        & 0.309    & 0.594    & 0.620    \\
\ +MS Train     & 0.590        & 0.344    & 0.599    & 0.628    \\
\ +MS Inference & 0.606        & 0.367    & 0.612    & 0.649    \\ \hline
Local Block     & 0.594        & 0.319    & 0.583    & 0.644    \\ 
\ +Global Block & 0.654        & 0.238    & 0.611    & 0.744    \\
\ +MS Train     & 0.765        & 0.386    & 0.763    & 0.817    \\
\ +MS Inference & 0.773        & 0.443    & 0.775    & 0.813    \\
\ +C-NMS        & 0.780        & 0.508    & 0.781    & 0.823    \\ \hline
\end{tabularx}
% \end{tabular}
% }
\label{tab:ablation}
\end{table}





\begin{table*}[]
\centering
% \vspace{-10pt}
\caption{\textbf{Comparison with the state-of-the-art methods on DOTA-v1.0}. The short names for categories are defined as (abbreviation-full name): PL-Plane, BD-Baseball diamond, BR-Bridge, GTF-Ground field track, SV-Small vehicle, LV-Large vehicle, SH-Ship, TC-Tennis court, BC-Basketball court, ST-Storage tank, SBF-Soccer-ball field, RA-Roundabout, HA-Harbor, SP-Swimming pool, and HC-Helicopter. BB means Backbone. Conv. means our SparseNet. Trans. means our SparseFormer.} 
\vspace{-8pt}
% \footnotesize
% \setlength\tabcolsep{3.pt}
% \small
	% \scriptsize
	% \renewcommand\arraystretch{1.2}
	% \setlength\tabcolsep{1.6pt}
		\resizebox{1.0
			\linewidth}{!}{
% \begin{tabularx}{1\textwidth}{l|l|ccccccccccccccc|cc}
% \begin{tabularx}{1.1\textwidth}{l|l|XXXXXXXXXXXXXXXX|c}
\begin{tabular}{l|l|ccccccccccccccc|cc}
\hline
Method                                  & BB      & PL & BD    & BRe & GTF   & SV    & LV    & SH  & TC    & BC    & ST    & SBF   & RA    & HA & SP    & HC    & mAP            & GFLOPs          \\ \hline
FR-O     & R-101    & 79.42 & 77.13 & 17.70  & 64.05 & 35.30 & 38.02 & 37.16 & 89.41 & 69.64 & 59.28 & 50.30 & 52.91 & 47.89  & 47.40 & 46.30 & 54.13          & 282.88          \\
ICN            & R-101    & 81.36 & 74.30 & 47.70  & 70.32 & 64.89 & 67.82 & 69.98 & 90.76 & 79.06 & 78.20 & 53.64 & 62.90 & 67.02  & 64.17 & 50.23 & 68.16          & -               \\
RoI-Trans. & R-101    & 88.64 & 78.52 & 43.44  & 75.92 & 68.81 & 73.68 & 83.59 & 90.74 & 77.27 & 81.46 & 58.39 & 53.54 & 62.83  & 58.93 & 47.67 & 69.56          & 296.74          \\
CADNet              & R-101    & 87.80 & 82.40 & 49.40  & 73.50 & 71.10 & 63.50 & 76.60 & 90.90 & 79.20 & 73.30 & 48.40 & 60.90 & 62.00  & 67.00 & 62.20 & 69.90          & -               \\
DRN               & H-104 & 88.91 & 80.22 & 43.52  & 63.35 & 73.48 & 70.69 & 84.94 & 90.14 & 83.85 & 84.11 & 50.12 & 58.41 & 67.62  & 68.60 & 52.50 & 70.70          & -               \\
CenterMap       & R-50     & 88.88 & 81.24 & 53.15  & 60.65 & 78.62 & 66.55 & 78.10 & 88.83 & 77.80 & 83.61 & 49.36 & 66.19 & 72.10  & 72.36 & 58.70 & 71.74          & -               \\
SCRDet          & R-101    & 89.98 & 80.65 & 52.09  & 68.36 & 68.36 & 60.32 & 72.41 & 90.85 & 87.94 & 86.86 & 65.02 & 66.68 & 66.25  & 68.24 & 65.21 & 72.61          & -               \\
R$^3$Det           & R-152    & 89.49 & 81.17 & 50.53  & 66.10 & 70.92 & 78.66 & 78.21 & 90.81 & 85.26 & 84.23 & 61.81 & 63.77 & 68.16  & 69.83 & 67.17 & 73.74          & 480.33          \\
S$^2$A-Net          & R-50     & 89.11 & 82.84 & 48.37  & 71.11 & 78.11 & 78.39 & 87.25 & 90.83 & 84.90 & 85.64 & 60.36 & 62.60 & 65.26  & 69.13 & 57.94 & 74.12          & 193.11          \\
CFA            & R-101     & 89.26 & 81.72 & 51.81  & 67.17 & 79.99 & 78.25 & 84.46 & 90.77 & 83.40 & 85.54 & 54.86 & 67.75 & 73.04  & 70.24 & 64.96 & 75.05          & 265.96          \\
CSL            & R-152     & 90.25 & 85.53 & 54.64  & 75.31 & 70.44 & 73.51 & 77.62 & 90.84 & 86.15 & 86.69 & 69.60 & 68.04 & 73.83  & 71.10 & 68.93 & 76.17          & 383.13          \\
ReDet               & ReR-50        & 88.79 & 82.64 & 53.97  & 74.00 & 78.13 & 84.06 & 88.04 & 90.89 & 87.78 & 85.75 & 61.76 & 60.39 & 75.96  & 68.07 & 63.59 & 76.25          & -               \\
\rowcolor{LightGreen} 
O-Rep.           & Swin-T        & 89.11 & 82.32 & 56.71  & 74.95 & 80.70 & 83.73 & 87.67 & 90.81 & 87.11 & 85.85 & 63.60 & 68.60 & 75.95  & 73.54 & 63.76 & 77.63          & 221.32          \\ \hline
Ours                              & Conv.     & 89.00 & 82.42 & 55.04  & 74.19 & 79.62 & 81.54 & 87.77 & 90.90 & 87.08 & 85.83 & 64.18 & 64.13 & 74.65  & 71.21 & 58.74 & 76.42          & \textbf{167.24}          \\
\rowcolor{LightBlue} 
Ours                              & Trans.    & 89.45 & 85.81 & 55.18  & 77.65 & 78.51 & 83.45 & 87.81 & 90.90 & 86.88 & 86.26 & 63.59 & 67.30 & 75.94  & 73.65 & 66.69 & \textbf{77.94} & 174.31 \\ \hline
\end{tabular}
}
% \vspace{-5pt}
\label{tab:sota_dota}
\end{table*}


\subsection{Effectiveness Evaluation}
\vspace{2mm}\noindent\textbf{Datasets.} Our evaluation is on two public benchmarks with HRW shots, PANDA~\cite{wang2020panda} and DOTA-v1.0~\cite{xia2018dota}. 
PANDA is the first human-centric gigapixel-level dataset.
It contains 18 scenes with over 15,974.6k bounding boxes annotated. 
Specifically, there are 13 scenes for training and 5 scenes for testing.
DOTA is a large-scale dataset to evaluate the oriented object detection in aerial images with sizes up to 4000$\times$4000.
It contains 2,806 images and 188,282 instances with oriented bounding box annotations, covered by 15 object classes.

\vspace{2mm}\noindent\textbf{Evaluation metrics.}  
We report the FLOPs and standard COCO metrics including AP$_{total}$, AP$_S$~($<96\times96$), AP$_{M}$~($96\times96-288\times288$) and AP$_{L}$~($>288\times288$).
For quantitative efficiency evaluation, we use the average FLOPs of each detector to process a 1280 $\times$ 800 window in the datasets.
%It is worth noting that the metric we are most concerned with in this work is FLOPs, which are used to quantitatively analyze the efficiency of different detectors when processing the same batch of images with the size of 1280 $\times$ 800.
% All the FLOPs in this work are computed 
Further, we calculate FLOPs in foreground and background, respectively, to show the efficiency of our SparseFormer to reduce the computation on backgrounds.
% The foreground is the region of all the objects which to be detected.

\vspace{2mm}\noindent\textbf{Implementation details.}
%
We implement the detectors using MMDetection~\cite{mmdetection}. To ensure a fair comparison, we evaluate these two detectors across four different backbones, including Swin, DEG, and our own proprietary design, all configured with identical numbers of hyperparameters~(e.g., depths, embedding dimension, number of multi-heads). All models are trained from scratch for 36 epochs, in line with the observations in \cite{he2019rethinking}.
% This also ensures a fair comparison with Swin and DEG, affirming that the performance improvement stems from our novel design rather than from better pre-trained weights.


\begin{figure}[!]
 \centering
	% \begin{center}
		\includegraphics[width=1.0\linewidth]{pic/sotav9.pdf}
	% \end{center}
\vspace{-7pt}
	\caption{\textbf{FLOPs vs. AP.} Our methods reduce FLOPs up to 50\% and improve detection accuracy in HRW shots.}
%        \vspace{-20pt}
	\label{fig:sota}	
\end{figure}


\vspace{2mm}\noindent\textbf{Results on PANDA.}  
% For a fair comparison, the hyperparameter ~(e.g., depths, embedding dimension, number of multi-head) of our model except the keeping ratios are all same as the tiny version of Swin Transformer.
%
We compare our model with a different keeping ratio $k$ to current state-of-the-art methods on the first gigapixel-level dataset PANDA, which not only has the challenge of wide FoV, but also super high resolution. The results are presented in \cref{tab:sota}. 
We first produce two baselines, one based on ATSS framework~\cite{zhang2020atss} with dynamic head block~\cite{dai2021dynamic} which achieves GFLOPs of 114.80, the other based on DINO~\cite{zhang2022dino} and GFLOPs of 132.84. Then, the backbone is modified to SparseFormer for further experiments. Note that the keeping ratio means the ratio of keeping tokens based on the previous stage, so the ratio of each stage based on the full number of tokens is $[k, k^2, k^3, k^4] $. We can observe our method achieves more than 5\% increase in AP over SotAs, with only 75.71 GFLOPs (43\% reduction from Swin-T, 63\% reduction from PAN~\cite{fan2022speed}). The most notable thing is that the reduced FLOPs are mainly from the background region, which is why we can significantly reduce the amount of computation but maintain high performance. An additional note is that GigaDet and PAN is to accelerate the detector by optimizing the process. Unlike these approaches, our work does not prescribe a specific pipeline. Instead, we propose a model-agnostic strategy that could be seamlessly integrated into existing pipelines.


\vspace{2mm}\noindent\textbf{Results on  DOTA.} 
We choose aerial images that also have HRW shots to verify the generalization.
The compared methods include: Faster RCNN-O~\cite{ren2015faster}, ICN~\cite{azimi2018towards}, RoI-Transformer~\cite{ding2018learning}, CADNet~\cite{zhang2019cad}, DRN~\cite{pan2020dynamic}, CenterMap~\cite{wang2020learning}, SCRDet~\cite{yang2022scrdet++}, R$^3$Det~\cite{yang2021r3det},  S$^2$A-Net~\cite{han2021align}, CFA~\cite{Guo_2021CVPR_CFA}, CSL~\cite{yang2020arbitrary}, ReDet~\cite{han2021redet} , Or-RepPoints~\cite{li2022oriented}. RoI-Transformer is used as the baseline detector for comparison and we set $k=0.7$. In \cref{tab:sota_dota}, SparseFormer improves mAP from 69.56\% to 77.94\% and reduces 296.74 GFLOPs to 174.31 GFLOPs.
% Compared to the recent methods, such as ReDet\cite{han2021redet} and CFA~\cite{guo2021beyond}, we can observe SparseFormer also has comparable or higher AP and much lower FLOPs. In addition, when combined with ResNet-50, our model (SparseNet) also achieves the best accuracy in ConvNet-based models and reduces GFLOPs significantly.
Compared to SotA Transformer-based method Or-RepPoints, we achieve 0.3\% AP improvement and reduce 21\% GFLOPs. Compared to the method with similar computation S$^2$A-Net, we surpass its AP by 3.82\%. 
% For the ConvNet version, we \textbf{reduce the GFLOPs up to 56\%} while increasing accuracy [\textcolor{cyan}{54}].
This indicates a significant enhancement in terms of accuracy and efficiency. 
The DOTA~\cite{xia2018dota} dataset presents a formidable challenge, yet our approach achieves the precision of the current SotAs with significantly fewer FLOPs. This not only validates the design intention behind SparseFormer to reduce computational demands but also demonstrates its generalizability across various tasks and domains.
\subsection{Ablation Study}
\vspace{2mm}\noindent\textbf{Component effectiveness.} We investigate the effectiveness of global block, C-NMS, multi-scale training~(MS Train) and inference~(MS Inference). Evaluation is conducted on the PANDA with $k=0.7$. As shown in \cref{tab:ablation}, all components can significantly improve performance with a little extra cost which also shows that our strategies are useful for object detection in HRW shots. 


\begin{figure*}[!]
	\begin{center}
		\includegraphics[width=1\linewidth]{pic/viz.pdf}
	\end{center}
	\vspace{-7pt}
    \caption{\textbf{Visualization of the window scores.} We illustrate the first-stage window scores of SparseFormer, with the left two columns of images from PANDA~\cite{wang2020panda} and the right two columns from DOTA~\cite{xia2018dota}. Highlighted points indicate areas requiring extraction of fine-grained features.}
	\label{fig:viz}	
 \vspace{-5pt}
\end{figure*}


\begin{table}
\centering
% \vspace{-10pt}
\caption{Ablation studies on sparse ratio. Based on the following results, we set the ratio to 0.7 to maintain the best balance between speed and accuracy in other experiments.} 
\vspace{-10pt}
% \footnotesize
\setlength\tabcolsep{2.2pt}
% \begin{tabularx}{0.48\textwidth}{l|c|*{3}{X}|*{4}{X}}
\begin{tabular}{l|c|*{3}{c}|*{4}{c}}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Ratio} & \multicolumn{3}{c|}{GFLOPs} & \multicolumn{4}{c}{AP}         \\ \cline{3-9} 
                        &                        & Fore   & Back     & All     & Total & Small & Medium & Large \\ \hline
                        & 0.1                & 5.91   & 31.24    & 37.15   & 75.1 & 31.6 & 71.4  & 85.4 \\
                        & 0.3                & 5.90   & 34.14    & 40.04   & 75.9 & 34.5 & 72.7  & 85.3 \\
DyHead                  & 0.5                & 6.16   & 40.59    & 46.75   & 76.5 & 34.7 & 73.5  & 85.5 \\
                        & 0.7                & 6.19   & 60.87    & 67.06   & 77.1 & 36.4 & 74.0  & 86.3 \\
                        & 1.0                & 6.21   & 117.9   & 124.1  & 78.2 & 44.2 & 75.4  & 86.3 \\ \hline
                        & 0.1                & 6.74   & 44.77    & 51.51   & 75.6 & 33.5 & 75.1  & 82.4 \\
                        & 0.3                & 6.79   & 48.86    & 54.65   & 76.1 & 41.1 & 76.2  & 81.3 \\
DINO                    & 0.5                & 6.84   & 52.53    & 59.37   & 76.7 & 42.5 & 77.2  & 80.9 \\
                        & 0.7                & 6.90   & 68.81    & 75.71   & 77.3 & 44.3 & 77.5  & 81.3 \\
                        & 1.0                & 7.06   & 134.2   & 141.3  & 78.0 & 53.0 & 77.8  & 81.9 \\ \hline
\end{tabular}
\label{tab:sparse_ratio}
\end{table}






\vspace{2mm}\noindent\textbf{Keeping ratio.} 
% Discarding the worthless grids is an important part of our strategy. So we conduct experiments to study the effect of the keeping ratio of the grids on the final performance.
% \cref{tab:sparse_ratio} shows the results and the keeping ratio $k$ is formulated as $[k, k^2, k^3, k^4]$ for each stage.
% As the features become more and more sparse, we can see a significant drop in FLOPs but an insignificant drop in accuracy. 
%
Our strategy involves discarding the grids that are deemed unimportant. We study the impact of the grid-keeping ratio on the final performance. \cref{tab:sparse_ratio} presents our findings, where the keeping ratio $k$ is represented as $[k, k^2, k^3, k^4]$ for each stage. As the features become sparser, we observe a significant reduction in FLOPs, but the decrease in accuracy is insignificant.

\vspace{2mm}\noindent\textbf{Effect on ScoreNet.}
\label{sec:study_scorenet}
%
We study the effect of different post-processing of residual values which feed into ScoreNet~(introduced in~\cref{sec:local}). $z$ and $\hat{z}$ denote the original features and aggregated features, respectively, which are the same in~\cref{eq:variance}. 
As can be seen from the last three lines, several variants based on residuals have a performance within the error range, so we consider using less computation and do not perform any redundant processing on them.
Compared with $Z$, which directly uses all the features in the window, the average feature $\hat{z}$ can achieve better results. We think this is because the ScoreNet is a simple MLP, it cannot take advantage of complex features $z$ well, while the $\hat{z}$ is easier to be classified based on color~(blue for the sky and green for glasses) and other patterns.




\begin{table}[]
\centering
\caption{Ablation studies on the selection strategy. The results indicate that using the difference is more effective than directly inputting \(z\), and using the p-norm brings no gains. Therefore, we choose \(z - \hat{z}\) in other experiments.}
\vspace{-10pt}
\label{tab:score}
\begin{tabularx}{0.5\textwidth}{X|XXXX}
\hline
                & AP$_{\text{Total}}$ & AP$_{\text{S}}$ & AP$_{\text{M}}$ & AP$_{\text{L}}$ \\ \hline
$\hat{z}$       & 0.748        & 0.452    & 0.757    & 0.786    \\
$z$             & 0.735        & 0.283    & 0.737    & 0.805    \\
$z-\hat{z}$     & 0.773        & 0.443    & 0.775    & 0.813    \\
$|z-\hat{z}|$   & 0.773        & 0.474    & 0.770    & 0.818    \\
$(z-\hat{z})^2$ & 0.771        & 0.468    & 0.784    & 0.811    \\ \hline
\end{tabularx}
\end{table}




\subsection{Comparison on Edge Device}
The HRW shots are usually captured by edge devices like UAVs. UAV detectors typically cannot run on large computing devices, but instead run on low-power edge devices. Because it is usually difficult to quantify FLOPs on edge devices, we use NVIDIA AGX Orin (top power 60W) to evaluate the average inference time of each detector on the gigapixel-level images from PANDA and the results are shown in~\cref{tab:fps}.
% It is easy to observe our method could reduce inference time compared to previous methods. 
Notably, our method can largely reduce inference time compared to previous methods. Our method is 3$\times$ faster than PAN and has 5.8\% increase of AP.
%The reason for comparing inference time is because it exists some modules may be difficult to quantify with FLOPs, especially on edge devices. 
%For example, the detection head of a two-stage detector is dynamic which means it is hard to calculate real FLOPs but could provide a trend.
We can see because of the complex head structure, the inference speed of dynamic-head~\cite{dai2021dynamic} is not ideal. 
On the opposite, DINO~\cite{zhang2022dino} shows promising FPS than the previous work and the improvement of the speed is clearer.
Compared to the competitive approach DEG~\cite{song2021dynamic},  our method could achieve much better performance with faster speed.


\subsection{Model-agnostic Study}
It is noteworthy that our strategy is model-agnostic, enabling seamless integration with either ConvNet or Transformer architectures. This flexibility leads to the creation of SparseNet and SparseFormer. Building upon the previously mentioned SparseFormer, we have innovated by substituting every self-attention module with convolution layers. As illustrated in \cref{tab:sota} and \cref{tab:sota_dota}, SparseNet demonstrates performance that is not only comparable but also competitive with the renowned ResNet~\cite{he2016deep}. Especially noteworthy is that SparseNet reduces the GFLOPs up to 56\% while increasing accuracy compared to CSL~\cite{yang2020arbitrary} and it achieves the lowest GFLOPs on the DOTA dataset~\cite{xia2018dota}, underscoring its efficiency and effectiveness in complex computational tasks.

\subsection{Visualization of Sparse Windows}

In order to better understand how window sparsification works, we visualize the selected windows from each stage in \cref{fig:viz}. The red patches represent regions with higher scores, while the blue patches indicate lower scores. SparseFormer will perform fine-grained feature extraction on the regions with higher scores. This illustration highlights the advantages of computation reduction on background areas and low-entropy foregrounds. Additionally, the results validate the effectiveness of our SparseFormer approach. 
The PANDA~\cite{wang2020panda} and DOTA~\cite{xia2018dota} datasets focus on different target objects, they share the common characteristic of containing large-scale background areas, making the sparsification approach particularly relevant. We believe that this methodology will not only benefit object detection in HRW shots but also various other vision tasks.

\begin{table}[t]
\centering
% \vspace{-10pt} 
\caption{Comparison of the inference time on edge-device.}
\vspace{-10pt}
	% \scriptsize
 % \footnotesize
	% \renewcommand\arraystretch{1.2}
	\setlength\tabcolsep{2.2pt}
		% \resizebox{1.0
		% 	\linewidth}{!}{
   
% \begin{tabularx}{0.48\textwidth}{l|XXXXXX}
\begin{tabular}{l|cccccc}
\hline
Method                                  & AP$_{Total}$ & AP$_{S}$ & AP$_{M}$ & AP$_{L}$ & Latency(s) & FPS   \\ \hline
FasterRCNN~\cite{ren2015faster}         & -            & 0.190    & 0.552    & 0.774    & 140        & 0.007 \\
CascadeRCNN~\cite{cai2018cascade}        & -            & 0.227    & 0.579    & 0.765    & 200        & 0.005 \\
PAN~\cite{fan2022speed}                 & 0.715        & 0.256    & 0.719    & 0.768    & 43         & 0.023 \\ \hline
DyHead~\cite{dai2021dynamic}      & 0.592        & 0.165    & 0.537    & 0.694    & 63         & 0.015 \\
DyHead+DEG~\cite{song2021dynamic} & 0.575        & 0.154    & 0.508    & 0.695    & 58         & 0.017 \\
\rowcolor{LightGreen} 
DyHead+Ours                       & 0.771        & 0.364    & 0.740    & 0.863    & 52         & 0.019 \\ \hline
DINO~\cite{zhang2022dino}              & 0.606        & 0.367    & 0.612    & 0.649    & 19         & 0.052 \\
DINO+DEG~\cite{song2021dynamic}        & 0.582        & 0.339    & 0.578    & 0.624    & 15         & 0.066 \\
\rowcolor{LightBlue} 
DINO+Ours                               & 0.773        & 0.443    & 0.775    & 0.813    & 14         & 0.071 \\ \hline
\end{tabular}

% }

\label{tab:fps}
\end{table}
