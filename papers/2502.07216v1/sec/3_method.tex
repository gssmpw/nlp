\section{Proposed Method}


We address the unique challenges of HRW detection by proposing the Sparse Vision Transformer. This model efficiently extracts valuable features from sparse information, while enlarging the receptive field to handle huge scale changes. To tackle the problem of incomplete large objects on intersecting sliced areas, we modify vanilla NMS. Additionally, we introduce our HRW-based augmentation for both training and inference to enhance the detection accuracy for both large and small objects. 
The pipeline is shown in Figure \ref{fig:pipeline}.


\subsection{Overview of SparseFormer}


An ideal vision model should be able to extract meaningful information from sparse data using limited calculations, just like our human eyes tend to focus on valuable areas over unimportant background information. To achieve this, we design a novel Sparse Vision Transformer called SparseFormer. It dynamically selects key regions and enables dynamic receptive fields to cover objects with various scales. The overall framework of SparseFormer is illustrated in Figure \ref{fig:overall}. 
%
Inspired by Swin Transformer~\cite{liu2021swin}, we split the input image into non-overlapping patches to generate tokens. SparseFormer consists of four stages that work together to produce an adaptive representation. Each stage begins with a patch merging layer that concatenates the features of each group of $2\times2$ neighboring patches. The concatenated features are then projected to half of their dimension using a linear layer.

Each stage of SparseFormer is centered around attention blocks that are designed to capture long-range and short-range interactions at different scales. To achieve this, we take both the advantages of the vanilla self-attention Transformer block and the Swin Transformer block. In this way, we develop two distinct types of sparse-style blocks. One is used to capture long-range interactions at a coarse grain, while the other focuses on short-range interactions at a finer scale. To facilitate this approach, we introduce the concept of \emph{Window} which divides each feature map into equally spaced windows. Operations within each window are considered ``local'', while operations that encompass all windows are ``global." %This definition of ``Window" is critical to understanding the functioning of the SparseFormer model.


% \vspace{2mm}\noindent\textbf{Global and local Attention blocks.}
We outline the global and local attention blocks in more detail. 
We construct the global block using the standard multi-head self-attention (MSA)~\cite{vaswani2017attention} and MLP module with aggregated features, or only convolution layers, as detailed in \cref{sec:global}. We construct the local block by adding a sparsification step and an inverse sparsification step before and after the Swin Transformer~\cite{liu2021swin} block, as described in \cref{sec:local}. Unlike previous work \cite{yang2021focalglobal, wang2021pnp}, we do not build separate branches for global and local attention. Instead, the local attention is positioned after the global one to obtain more details, rather than different features. When a stage has multiple blocks, the ordering of global attention blocks (G) and local attention blocks (L) follows a pattern of `GGLL'.

\begin{figure}[!]
\centering
	% \begin{center}
		\includegraphics[width=1.0\linewidth]{pic/pipelinev2.pdf}
	% \end{center}
	\vspace{-20pt}
	% \caption{\textbf{The pipeline of one inference period.} Intuitively show the relationship and order of our proposed modules.}
        \caption{\textbf{Pipeline of SparseFormer in one forward inference.} First, we perform multi-scale slicing on a gigapixel image. Then, we apply patch partitioning to each slice, and group neighboring patches into windows. Global Attention utilizes aggregated features to quickly obtain coarse-grained information. Local Attention selects important windows to extract fine-grained information.}
    \vspace{-12pt}
	\label{fig:pipeline}	
\end{figure}

\begin{figure*}[!]
\centering
	% \begin{center}
		\includegraphics[width=1\linewidth]{pic/overview3.pdf}
	% \end{center}
         \vspace{-18pt}
	\caption{\textbf{Network Architecture of SparseFormer.} 
 The \textcolor{red}{red box} represents the interaction range of attention.\includegraphics[width=0.25cm]{pic/train.png} means tokens are updated by self-attention and \includegraphics[width=0.25cm]{pic/retain.png} means they remain unchanged.  We partition the image into tokens and group them into windows. Global attention extracts coarse-grained features from all windows based on aggregated tokens and merges them with the original features. Local attention selects only the windows with complex details for fine-grained feature extraction through our ScoreNet, while the rest retain their original features to save computational resources.
}
         \vspace{-6pt}
	\label{fig:overall}	
\end{figure*}



\subsection{Global Attention on Aggregated Features}
\label{sec:global}

\noindent\textbf{Feature aggregation.} 
\label{para:feature_agg}
Global attention aims to capture coarse-grained features based on long-range interaction. As such, we generate low-resolution information by sparsifying the feature in each window.
As shown in Figure \ref{fig:overall}, we begin each stage with the global attention block.
%
The primary function of this block is to aggregate the features of each window. 
%
To achieve this, we take the input feature map $z$ and divide it into windows of size $M$, ensuring they do not overlap. The left-top location of each window is given by ($x$, $y$), and each token within the window has a relative location ($\Delta x$, $\Delta y$).
%
We then calculate the aggregated features using the following formula:
\begin{equation}
\begin{split}
     & \bar{z}_{x',y'}  = 
     \frac{{\textstyle \sum_{\Delta x,\Delta y}^{}}\alpha _{\Delta x,\Delta y} \cdot  z_{x+\Delta x,y+\Delta y}}
     { 
     {\textstyle \sum_{\Delta x,\Delta y}^{}}\alpha _{\Delta x,\Delta y}}, \\
\end{split}
\label{eq:aggregate}
\end{equation}
Here, $x' =x / M$ and $y' =y / M$. $\alpha _{\Delta x,\Delta y}$ is the weight of each token. In this paper, we assign equal weights to all tokens by setting $\alpha _{\Delta x,\Delta y}=1$.
%
After aggregating the features using the above formula, we obtain the aggregated feature $\bar{z}$ which can be further used for attention.


\vspace{2mm}\noindent\textbf{Window-level global attention.}
Feature aggregation is a technique that reduces the number of tokens by a multiple of $M^2$, which is equivalent to a $M\times$ downsampling of resolution. This reduction in tokens allows us to use global attention interaction without expensive computation. 
With the aggregated features, consecutive global blocks are computed as follows: 
\begin{equation}
\begin{split}
  & \bar{z}^{l} = \text{Layer}(\bar{z}^{l-1}; \Theta),
 \end{split}
 \label{eq:global_att}
\end{equation}
where $\bar{z}^{l}$ refers to the output features of the $l$-th global block.

\vspace{2mm}\noindent\textbf{Inverse aggregated features.} 
The aggregated features contain abstract information that facilitates global content-dependent interactions among different image regions. However, their resolution differs from the input feature map. Consequently, we convert the window-level features back to the token-level using an inverse function of equation \cref{eq:aggregate}, as follows:
\begin{equation}
\begin{split}
     & z_{x+\Delta x,y+\Delta y}  = 
     \alpha _{\Delta x,\Delta y} \cdot  \bar{z}_{x', y'}. \\
     % & x = M\cdot x', y = M\cdot y',
\end{split}
\label{eq:antiaggregate}
\end{equation}
Here, $x = M\cdot x'$ and $y = M\cdot y'$, where ($x'$, $y'$) and ($x$, $y$) represent the location on the input and output feature maps, respectively. Additionally, ($\Delta x$, $\Delta y$) represents the relative location with respect to ($x$, $y$). We consider ($x$, $y$) as the top-left of each window on the output feature map, where the windows are partitioned in the same manner as in the feature aggregation process.


This step extracts the output feature map from the successive global block~(\cref{eq:global_att}). Then, we invert it using \cref{eq:antiaggregate} and denote the resulting feature map by $z^{\text{global}}$. It is worth noting that the final global feature $z^{\text{global}}$ has the same resolution as the input feature map $z$. %This concludes the global attention operation. 
Even though aggregated features have a lower resolution, the global attention operation can provide more non-local information with little extra computation.

\subsection{Local Attention on Sparse Windows}
\label{sec:local}

\noindent\textbf{Variance-based scoring.}
Note that coarse-grained features for each window can achieve high efficiency. However, we still need fine-grained features that can extract object details to accurately detect objects. As such, we drop certain windows based on their low information content to reduce computation. Our goal is to identify windows that require further local attention because their window-level feature cannot represent their inner token-level features. 

We start with an initial feature map $z$ of dimensions $H \times W \times C$ before applying global and local attention. We then get the aggregated feature $\bar{z}$ from $z$ using \cref{eq:aggregate} and apply the inverse sparsification function via \cref{eq:antiaggregate}, which produces an intermediate feature map $\hat{z}$ with the same resolution as $z$. Next, we calculate the residual $r$ between $\hat{z}$ and $z$ and concatenate the features of each window to obtain the tokens of $M\times M\times C$ dimensions that are $\frac{H}{M}\times \frac{W}{M}$ in size. 
We construct a ScoreNet using MLP to generate the scores based on each residual: 
\begin{equation}
\text{ScoreNet}(z, \hat{z}) = \text{SoftMax}(\text{MLP}(z - \hat{z})),
\label{eq:variance}
\end{equation}
where the MLP projects ($M\times M\times C$)-dimensional features for each window to 1 dimension, and the SoftMax operation calculates the score for each window. A higher score indicates greater variance, meaning high-variance windows
require fine-grained attention. In other words, we discard windows with lower scores during local attention.
Once we have ranked the windows, we can selectively choose a part of them to generate finer-level features. Before doing so, we update the feature map $z$ with global feature $z^{\text{global}}$ using: 
\begin{equation}
z \gets  z + z^{\text{global}}.
\label{eq:update_gloabl}
\end{equation}


\vspace{2mm}\noindent\textbf{Windows sparsification.}
We start by analyzing the global attention and variance-based scoring to obtain the initial feature $z$ and scores for each window. Next, we partition $z$ into windows of size $\frac{H}{M}\times \frac{W}{M}$, in the same way as the ScoreNet. We represent these windows as a matrix $Z\in \mathbb{R}^{N\times D}$, where $N$ is the total number of windows, i.e., $N=\frac{H}{M}\times \frac{W}{M}$ and $D=M\times M\times C$.
%
To determine which windows to keep, we define a hyperparameter $k$ to represent the keeping ratio. We maintain a binary decision mask vector $A\in \{0, 1\}^N$ to indicate whether to drop or keep each window based on $k$ and scores. The value of $k$ would depend on the specific task at hand, and can be adjusted as required.
%
The sparse matrix $S\in \mathbb{R}^{K\times N}$ collects the one-hot encoding of vector $A$, where $K$ is the number of keeping windows, i.e., $K=k\cdot N$. Using this sparse matrix, we compute the features of the sparse windows as follows:
\begin{equation}
\begin{split}
    &  \hat{Z}_{s} = S \times Z,
\end{split}
\end{equation}
%
The output feature $\hat{Z}_{s} \in \mathbb{R}^{K\times D}$ is then used as input for the local attention.

\vspace{2mm}\noindent\textbf{Shifted window-based attention.}
We utilize the Shift Window-based Attention module which was first introduced in Swin Transformer~\cite{liu2021swin}. The consecutive local blocks can be represented as:
\begin{equation}
\begin{split}
    & z^{l} = \text{W-Layer}(z^{l-1}; \Theta), \\
    & z^{l+1} = \text{SW-Layer}(z^{l}; \Theta),
\end{split}
\end{equation}
where $z^l$ denotes the output features of the local block $l$. The layer can be either a self-attention or convolution module.
%
To fuse the output and input features of local attention, we use:
\begin{equation}
\hat{Z} \gets A^T\times \hat{Z}_s + (\mathbf{1} -A^T) \times Z.
\end{equation}
Here, $\hat{Z}_s$ is updated by local attention, while $\hat{Z}$ is the output of each stage of SparseFormer. Finally, we revert $\hat{Z}$ back into the original dimensional space of $H\times W\times C$ to obtain the final feature map, denoted as $z$.  The window-based attention, based on variance-based scoring, can extract more local information in a lightweight form, thus improving the detection performance of small objects while saving computation for the background.

\setlength{\textfloatsep}{5pt}
\begin{algorithm}[t]
\caption {Cross-slice NMS (C-NMS)}
\label{alg:nmbs}
\SetKwInOut{KwInput}{Variables}
\SetKwInOut{KwOutput}{Functions}
\KwInput{
$\mathcal{B}_{1}$, $\mathcal{B}_{2}$ are candidate box sets from two slices, $\tau$ is the C-NMS threshold\;
}
\KwOutput{
$\mathop{\mathrm{NMS}}(\cdot)$ is the conventional NMS\; \qquad \qquad \quad $\mathop{\mathrm{AREA}}(\cdot)$ calculates the area of a box\;
}
$\mathcal{B}_{1}^{\prime} \gets \mathop{\mathrm{NMS}}(\mathcal{B}_{1}); \mathcal{B}_{2}^{\prime} \gets \mathop{\mathrm{NMS}}(\mathcal{B}_{2})$\;
$\mathcal{B} \gets \mathcal{B}_{1}^{\prime} \cup \mathcal{B}_{2}^{\prime}; \mathcal{B}^{\prime} \gets \emptyset$\;
\While {$\mathcal{B} \ne \emptyset$}{
    $m \gets \mathop{\mathrm{argmax}}_{i} \mathop{\mathrm{AREA}}(b_{i})$, s.t. $b_{i} \in \mathcal{B}$\;
    $ \mathcal{B}^{\prime} \gets \mathcal{B}^{\prime} \cup \left\{b_m \right\}; \mathcal{B} \gets \mathcal{B} - \left\{b_m \right\}$\;
    \For {$b_i \in \mathcal{B}$}{
        \If {$IoU(b_{i}, b_{m}) \ge \tau $}{
            $\mathcal{B} \gets \mathcal{B} - \left\{b_i\right\}$\; 
        }
    }
}
\textbf{return} $\mathcal{B}^{\prime}$\;
\end{algorithm}


\begin{table*}[]
\centering
\caption{\textbf{Comparison with the SotAs on PANDA.} ``F'' and ``B'' denote foreground and background, respectively (A=F+B). ``*'' denotes re-implementation in ~\cite{fan2022speed}. GFLOPs for the two-stage detector exclude detection heads due to dynamic cost.}
\vspace{-6pt}
% \small
% \footnotesize
% \setlength\tabcolsep{4pt}
		\resizebox{1.0
			\linewidth}{!}{
\begin{tabular}{l|c|ccc|cccc}
\hline
Method                                  & Backbone       & GFLOPs-F & GFLOPs-B & GFLOPs-A & AP$_{Total}$ & AP$_{S}$ & AP$_{M}$ & AP$_{L}$ \\ \hline
FasterRCNN~\cite{ren2015faster}         & ResNet-101     & 14.15    & 268.98   & 283.14   & -            & 0.190    & 0.552    & 0.744    \\
FasterRCNN*~\cite{fan2022speed}         & ResNet-50      & 10.35    & 196.71   & 207.07   & 0.705        & 0.203    & 0.712    & 0.760    \\
RetinaNet~\cite{lin2017focal}           & ResNet-101     & 15.77    & 299.62   & 315.39   & -            & 0.221    & 0.561    & 0.740    \\
CascadeRCNN~\cite{cai2018cascade}       & ResNet-101     & 15.54    & 295.24   & 310.78   & -            & 0.227    & 0.579    & 0.765    \\
ClusDet~\cite{yang2019clustered}        & ResNet-50      & 10.35    & 196.71   & 207.07   & 0.718        & 0.219    & 0.696    & 0.782    \\
DMNet~\cite{li2020density}              & ResNet-50      & 10.35    & 196.71   & 207.07   & 0.540        & 0.119    & 0.371    & 0.714    \\
GigaDet~\cite{chen2022towards}          & CSP-DarkNet-53 & 4.61     & 87.59    & 92.20    & 0.684        & 0.210    & 0.599    & 0.762    \\
PAN~\cite{fan2022speed}                 & ResNet-50      & 10.35    & 196.71   & 207.07   & 0.715        & 0.256    & 0.719    & 0.768    \\ \hline
Dynamic-Head~\cite{dai2021dynamic}      & Swin-T         & 5.74     & 109.1    & 114.8    & 0.592        & 0.165    & 0.537    & 0.694    \\
Dynamic-Head+DEG~\cite{song2021dynamic} & PVT-DEG        & 6.12     & 60.11    & 66.23    & 0.575        & 0.154    & 0.508    & 0.695    \\
\rowcolor{LightGreen} 
Dynamic-Head+Ours                       & SparseFormer   & 6.29     & 58.35    & 64.64    &  0.771        &  0.364    &  0.740    & 0.863    \\ \hline
DINO~\cite{zhang2022dino}               & Swin-T         & 6.64     & 126.19  & 132.84   & 0.606        & 0.367    & 0.612    & 0.649    \\
DINO+DEG~\cite{song2021dynamic}         & PVT-DEG        & 6.77     & 78.57    & 85.34    & 0.582        & 0.339    & 0.578    & 0.624    \\
\rowcolor{LightBlue} 
DINO+Ours                               & SparseFormer   & 6.90     & 68.81    & 75.71    & 0.780        & 0.508    & 0.781    & 0.823    \\ \hline
DINO~\cite{zhang2022dino}               & ResNet-50      & 6.21     & 118.02   & 124.24   & 0.542        & 0.289    & 0.530    & 0.592    \\
DINO+Ours                               & SparseNet      & 6.53     & 100.97   & 107.50   & 0.746        & 0.381    & 0.754    & 0.797    \\ \hline
\end{tabular}
% \vspace{-14pt}
}
\label{tab:sota}
\end{table*}

\vspace{2mm}\noindent\textbf{End-to-end Optimization.}
It is challenging to optimize the ScoreNet because we only use the output to sort the windows, and the gradient cannot be back-propagated. To overcome this issue, we implement the Gumbel-Softmax trick~\cite{maddison2016concrete} to relax the sampling process, making it differentiable. This trick provides a bridge for gradient back-propagation between soft values and binarized values through re-parameterization.
Hence, we re-write \cref{eq:update_gloabl} as:
\begin{equation}
     z \gets  z + (1-s)\times z^{\text{global}}, 
\label{eq:re_write_update_gloabl}
\end{equation}
Here, $s$ represents the output of the SoftMax function, which indicates the scores of windows.


\begin{figure}[!]
\centering
	% \begin{center}
		\includegraphics[width=1.0\linewidth]{pic/difference_v3.pdf}
	% \end{center}
        \vspace{-12pt}
	\caption{\textbf{Featured detection example on large objects with slicing aid.} Detector yields two boxes based on overlapped slices. NMS, relying on the detection scores, will wrongly select the blue box for the kid.}
    % \vspace{-10pt}
	\label{fig:challenge}	
\end{figure}

\subsection{Cross-slice Non-Maximum Suppression}


In HRW shot processing, the slicing strategy generates box candidates for each slice, which must then be merged into a mutually non-conflicting box set. However, using Non-Maximum Suppression (NMS) to select the highest-scoring boxes may lead to incomplete boxes when objects are on the edge regions of multiple slices (For a more detailed explanation and visual representation, refer to \cref{fig:challenge}). To address this, we propose a Cross-slice Non-Maximum Suppression (C-NMS) strategy, as shown in \cref{alg:nmbs}, that prioritizes boxes with the maximum area across multiple slices, rather than just the highest scores. The C-NMS algorithm consists of two stages: a local suppression stage and a cross-slice suppression stage.

\subsection{Multi-scale Training and Inference}



Due to memory limitations, it is not possible to train and test super high-resolution datasets at their original size. Therefore, we use a slicing strategy in both the training and testing phases. 
To make better use of the multi-scale information, we use high-resolution images and divide them into slices of varying sizes using the slicing strategy. All slices are scaled to the same size, enabling effective training and inference for the object detector. We divide the image into grids of 16$\times$16, 8$\times$8, 4$\times$4, and 2$\times$2 grids, respectively, and remove the slices with no objects. This approach allows us to analyze and understand the complex features of these images, ultimately improving the overall accuracy and effectiveness of the detector.

During the inference phase, we use slicing windows of two sizes: the original one and one quarter of both height and width.
Instead of simply combining the two windows, we set different receptive fields for the two types of windows with a threshold $T_a$. Based on the first window, we remove the prediction boxes larger than $T_a$. We only keep the boxes larger than $T_a$ for the second window. This follows the idea of scale-specific design~\cite{singh2018analysis, singh2018sniper}, where we should arrange each window to cover the appropriate scale to improve performance. With this technique, we can quickly and accurately process high-resolution images.
