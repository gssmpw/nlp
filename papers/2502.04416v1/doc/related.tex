\section{Related Work}

In contrast to pretraining MoE models from scratch, recent research has investigated the feasibility of constructing MoE architectures by repurposing existing dense LLMs. 
Current methodologies for deriving MoE models from dense checkpoints generally follow two paradigms: (1) partitioning parameters of FFNs while preserving the original model’s total parameter count~\cite{zuo2022moebert,moefication}, or (2) expanding the model’s overall capacity while retaining activation dimensions comparable to standard dense models~\cite{komatsuzaki2022sparse,wu2024parameter}. 
This work prioritizes the former approach.
Notably, MoEBERT~\cite{zuo2022moebert} introduces an importance-driven strategy to transform FFNs into expert modules by strategically redistributing top-scoring neurons across specialized components. Concurrently, MoEfication~\cite{moefication} leverages the discovery of sparse activation patterns in ReLU-based FFNs within T5 architectures, enabling the decomposition of these layers into distinct expert groups governed by a learned routing mechanism.
Based on continual training, the LLaMA-2 7B model is modified as a LLaMA-MoE-3.5B MoE model, where the parameters of the original FFNs are partitioned into multiple experts~\cite{llama-moe}. After training with 200B tokens, the LLaMA-MoE-3.5B model significantly outperforms dense models that contain similar activation parameters. Furthermore, based on a two-stage post-training strategy, an MoE model is constructed from the LLaMA3 8B model, where both attention and MLP are partitioned into MoE blocks~\cite{llama-moe-v2}]. 

Extensive experiments have shown the effectiveness of constructing an MoE model from a dense model, and many techniques can be utilized to guarantee performance recovery. However, such performance recovery is extremely resource-consuming, which is unfavorable for efficient deployment in industrial applications. Therefore, more lightweight methods are required, such that performance recovery can be done within hours and even training-free.


Note that model compression such as pruning and quantization is another important technique for efficient LLM inference~\cite{lin2024awq,pei2023quantization,zou2024bie,lin2024qserve}. Pruning is among the most widely utilized approaches to detect and remove redundant or less significant parameters from models, thereby resulting in a sparser weight matrix and faster inference. ShortGPT \cite{shortgpt} has put forward a simple layer-removal approach. This approach is based on block influence, which is determined by the similarity between a layer's input and output. SliceGPT \cite{slicegpt} substitutes each weight matrix with a smaller dense matrix, thereby decreasing the embedding dimension of the network. By taking into account contextual sparsity in real time, Deja Vu~\cite{dejavu} and FuseGPT~\cite{fusegpt} have been proposed to accelerate LLM inference. In contrast to the post-training methods, Learn-To-be-Efficient is designed to train efficiency-aware LLMs so that they learn to activate fewer neurons and achieve a more favorable trade-off between sparsity and performance~\cite{learn}. 