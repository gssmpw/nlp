\section{Experiments}
\input{table/ppl}
\iffalse
Here, we first describe the experimental setting for CMoE implementation.
We then present the main comparison results from perplexity to zero-shot task results, including the model without fine-tuning and with fine-tuning on modest data. Finally, we conduct ablation studies for CMoE.

\subsection{Experimental Setting}
\fi

CMoE is implemented based on Hugging Face Transformers~\cite{wolf2019huggingface} together with Pytorch~\cite{paszke2019pytorch}.
The experiments are conducted on one NVIDIA H800 PCIe 80GB graphics card with CUDA Driver 12.6.
We randomly select samples from WikiText-2 training dataset~\cite{merity2016pointer} as calibration and fine-tuning data.
We use only 8 examples with 2,048 sequence length as calibration for calculating the hidden state $\mathbf{H}$ in \eqref{eq:h_matrix}.
We set $K_a = 10$ for the activation status record, which sounds counter-intuitive but works best in practice.
For lightweight fine-tuning, we run $1$ epoch using the Adam optimizer~\cite{kingma2014adam} with $\beta_1$ = 0.9 and $\beta_2$ = 0.95.
We also employ LoRA~\cite{hu2021lora} with a rank of 8 and $lora\_alpha = 32$.
We fine-tune all the models including the baselines with $2,048$ samples.
We set different initial learning rates for the score scale $\mathbf{u}$ and other parameters, i.e. $0.001$ and $5.95e^{-5}$, respectively.
We set the bias update speed $\gamma$ to $0.001$.


\subsection{Main Results}

We compare CMoE with the up-to-date baseline \textbf{LLaMA-MoE}~\cite{llama-moe}, in which the neurons are randomly split, and continual pre-training is carried out with additional router networks. The experiments cover both training-free and lightweight fine-tuning versions. We design these experiments to demonstrate the remarkable post-training performance of CMoE. we demonstrate the results on two different datasets, i.e. WikiText-2~\cite{merity2016pointer} and C4~\cite{raffel2020exploring}. The baseline models are chosen as LLaMA-2-7B and LLaMa-3-8B. 


% \textbf{LLaMA-MoE-v2}: (MLP part) Initialize router networks based on hidden features input and do importance-based neuron partition by measuring the loss change of neuron pruned.


\minisection{Language Modeling Performance.}
We evaluate the perplexity of the constructed Mixture-of-Experts (MoE) models in Table~\ref{tab:1}. We use abbreviations to denote the composition of experts. For instance, `S2A2E16' represents 2 shared experts, 2 activated routed experts, and a total of 16 experts and the total activation ratio is $(2+2)/16 = 25\%$. 
Our findings indicate that in the training-free scenario, the ppl of our baseline LLaMA-MoE on both LLaMA-2-7B and LLaMA-3-8B are NaN across different datasets. In contrast, the perplexity of the proposed CMoE can be effectively controlled. Moreover, after fine-tuning, we observe that the perplexity can be reduced to as low as 12.73 when the activation ratio is 25\%, which corresponds to a sparsity of 75\%. Additionally, the CMoE model outperforms all state-of-the-art (SOTA) structured pruning methods~\cite{dejavu, slicegpt, fusegpt}. 

% \input{table/zero-shot}

\minisection{Downstream Tasks Performance.}
We also evaluate the performance of the constructed MoE models on various downstream tasks. 
We evaluate on the following benchmarks: 
32-shot BoolQ~\cite{clark2019boolq}, 0-shot PIQA~\cite{bisk2020piqa}, 0-shot SciQ~\cite{welbl2017crowdsourcing}, 5-shot Winogrande~\cite{sakaguchi2021winogrande}, 25-shot ARC-Challenge~\cite{clark2018think}, and 10-shot HellaSwag~\cite{zellers2019hellaswag}.
The results are presented in \cref{table:zero_shot}, where we denote fine-tuning or not with `FT'.
In all activation ratio configurations, we find that the proposed CMoE outperforms the LLaMA-MoE in accuracy across a diverse set of downstream tasks, on both training-free and fine-tuning scenarios. 


To illustrate the performance of the models, we select the SciQ dataset for zero-shot testing and the BoolQ dataset for few-shot testing as representative examples. When the activation ratio is set at a fixed value of 25\% (corresponding to a sparsity rate of 75\%), after fine-tuning, the LLaMA-MoE model can only achieve an accuracy equivalent to 21.2\% of that of the relevant dense model on the SciQ dataset and 45.31\% on the BoolQ dataset. Given that the perplexity (ppl) of the LLaMA-MoE model is Not a Number (NaN) in the training-free scenario, we do not report its accuracy in this case.
In contrast, even without fine-tuning (i.e., in the training-free scenario), the proposed CMoE model demonstrates superior performance, attaining an accuracy equivalent to 56.3\% of that of the dense model on the SciQ dataset and 54\% on the BoolQ dataset. After fine-tuning, the performance of the CMoE model is further enhanced, with the accuracy reaching 76.59\% on the SciQ dataset and 74.3\% on the BoolQ dataset (relative to dense model).


\subsection{Ablation Studies}
We conduct ablation studies with LLaMA-2-7B and data randomly selected from WikiText-2 training datasets.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.86\linewidth]{./pdfplot/runtime/runtime}\vspace{-2mm} 
    \caption{Trade-off between Model Performance and Construction Time with Increasing Training Data.
    }
    \label{fig:ab_1}
\end{figure}

\input{table/zero-shot}

\begin{figure*}[t!]
    \centering
    \subfigure[]{\includegraphics[height=2.75cm]{./pdfplot/shared/shared}\label{fig:ab_2}} 
    \subfigure[]{\includegraphics[height=2.75cm]{./pdfplot/activate/activate}\label{fig:ab_3}} 
    \subfigure[]{\includegraphics[height=2.95cm]{./pdfplot/balance/balance}\label{fig:ab_4}} 
    \caption{
        Ablation studies:(a) Impact of shared expert ratio on model performance;
        (b) Activation Rate vs.~Model Performance;
        (c) Effect of Load Balancing.
    }
    \label{fig:ablation_all} 
\end{figure*}


\minisection{Impact of Training Data Size.}
To quantify the relationship between training data volume, model performance, and computational efficiency, we systematically vary the number of training samples from 0 (initial state) to 4,096 while measuring perplexity (PPL) and construction time. 
\Cref{fig:ab_1} illustrates the results on the setting of ``S2A2E16'', revealing critical trends in performance-cost trade-offs.

Increasing training data from 0 to 4,096 samples reduces perplexity by 80.4\% (62.30 $\to$ 12.21) but non-linearly increases construction time from 298s to 4,502s. 
While performance improves sharply initially (64 samples cut PPL by 63\%), gains diminish beyond 1,024 samples (13.47 PPL), with marginal improvements (12.21 PPL at 4,096 samples) requiring disproportionately higher runtime (+133\% from 2,048 to 4,096 samples). 
The results demonstrate that CMoE only needs a modest of data to achieve low perplexity, while further performance increase is hard and possibly demands more diverse data.



%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=0.86\linewidth]{./pdfplot/shared/shared}\vspace{-2mm} 
%    \caption{Impact of shared expert ratio on model performance
%    }
%    \label{fig:ab_2}
%\end{figure}

\minisection{Shared Experts Ratio.}
We carry out an extensive analysis of the influence of the shared expert ratio on perplexity. In our experimental setup, we have a total of 32 experts, among which 8 are activated. We systematically adjust the proportion of shared experts within these 8 activated experts. As depicted in Table~\Cref{fig:ab_2}, the results clearly demonstrate a distinct trend: as the ratio of shared experts rises from 0.125 to 0.875, the perplexity continuously decreases. There is a remarkable improvement in the perplexity value, dropping from 14.48 to 11.93. This finding indicates that shared experts generally contribute positively to the performance of the model. However, it is also evident that the marginal returns tend to diminish when the ratio of shared experts reaches a relatively high level.

%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=0.86\linewidth]{./pdfplot/activate/activate}\vspace{-2mm} 
%    \caption{Activation Rate vs. Model Performance.
%    }
%    \label{fig:ab_3}
%\end{figure}

\minisection{Activation Rate.}
We conduct an external analysis to investigate how the total activation rate (comprising both shared and activated routed experts) impacts model performance across different domains, in comparison with dense baselines. We perform evaluations on the CMoE model featuring a total of 16 experts. Specifically, we vary the ratio of shared experts and activated routed experts at a fixed proportion of 1:1 on the WikiText-2 and C4 datasets.
As illustrated in Table~\Cref{fig:ab_3}, a monotonic decrease in PPL can be found as the activation rate increases, gradually approaching the performance level of the dense model. For both datasets, an activation rate of 75\% enables the model to achieve nearly comparable performance to the dense model. On the WikiText-2 dataset, the PPL values are 5.79 for the model under consideration and 5.27 for the dense model; on the C4 dataset, the corresponding values are 11.19 and 7.27. These results demonstrate that CMoE architectures can attain a performance quality comparable to that of dense models, even when characterized by relatively high sparsity.


%\begin{figure}[tb!]
%    \centering
%    \includegraphics[width=0.86\linewidth]{./pdfplot/balance/balance}\vspace{-2mm} 
%    \caption{Effect of Load Balancing.
%    }
%    \label{fig:ab_4}
%\end{figure}




\minisection{Load Balancing.}
CMoE can inherently achieve effective load-balancing among routed experts, with the exception of some special blocks. For example, the last block of LLaMA-2-7B exhibits extremely unbalanced expert counts.
However, this issue can be effectively resolved by the load-balancing mechanism we introduced in \Cref{sec:diff}. 
As presented in \Cref{fig:ab_4}, prior to the implementation of the load-balancing strategy, substantial disparities in the workloads assigned to different experts are clearly observable. Specifically, Expert 3 and Expert 8 are burdened with disproportionately high loads, handling 11,163 and 11,225 instances respectively. In stark contrast, other experts process a significantly lower number of instances. For example, Expert 12 and Expert 13 handle only 100 and 102 instances respectively.
Upon the application of our proposed load - balancing mechanism, a more equitable and uniform distribution of the computational workload across all experts is successfully achieved. After the adjustment, the number of instances processed by each expert falls within the range of 1,443 to 3,584.

% CMoE can originally achieve good load-balancing on routed experts except for some special blocks, such as the last block of LLaMA-2-7B, which shows extremely unbalanced on experts counts.
% However, it can be well-solved by the load balancing mechanism we introduced in \Cref{sec:diff}.
% As in \Cref{fig:ab_4}, prior to load balancing, we observe significant load disparities, with experts 3 and 8 handling disproportionately high loads (11,163 and 11,225 instances respectively), while others process substantially fewer cases (e.g., experts 12 and 13 with only 100 and 102 instances).
% After applying our load balancing mechanism, we achieve a more uniform distribution of workload across experts, with counts ranging from 1,443 to 3,584 instances.

