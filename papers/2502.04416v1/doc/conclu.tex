\section{Conclusion}
We present CMoE, a framework that efficiently carves sparse Mixture-of-Experts (MoE) architectures from dense LLMs through parameter reorganization and lightweight adaptation. 
By leveraging activation sparsity patterns in FFN layers, CMoE groups neurons into shared and routed experts via a novel balanced linear assignment formulation. 
The router is analytically initialized from activation statistics and refined via differentiable scaling and load balancing.
Experiments show that CMoE can construct effective Mixture-of-Experts (MoE) models, achieving comparable perplexity to dense models and outperforming baseline models. For instance, on the SciQ dataset, CMoE can reach 56.3\% even without fine-tuning and it can further improve the accuracy to 76.59\% after fine-tuning, a huge outperforming compared with 21.2\% (the accuracy of the baseline LLaMA-MoE).
Extensive experiments have demonstrated that CMoE offers a practical approach for deploying large language models (LLMs) in resource-constrained environments.
