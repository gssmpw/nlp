\section{Introduction}

Large language models (LLMs) have demonstrated exceptional proficiency in managing complex tasks and exhibiting emergent capabilities across diverse domains and applications,
particularly when scaled to billions of parameters~\cite{zhang2022opt, touvron2023llama, liu2024visual, liu2024deepseek}. 
While LLMs have attained remarkable success, their expanding computational demands and model sizes have intensified challenges related to practical deployment,
especially in environments with limited hardware resources or stringent latency requirements.
To mitigate these challenges, mixture-of-experts (MoE) architectures~\cite{lepikhin2020gshard, du2022glam,fedus2022switch,dai2024deepseekmoe} have emerged as a promising paradigm. 
Unlike dense LLMs, where all parameters are activated for every input token, MoE models replace monolithic feed-forward networks (FFNs) with sparsely activated experts: specialized sub-networks that process inputs conditionally via dynamic routing. 
This design decouples model capacity from computational cost—activating only a subset of experts per token while preserving the model’s expressive power.

Recently, researchers have found that there is high activation sparsity in the hidden neurons of FFNs of dense LLMs,
which motivates them to develop sparsity-aware acceleration techniques to reduce computational overhead while maintaining model performance~\cite{dejavu,moefication}.
Building upon this insight, a growing body of research has focused on transforming dense LLMs into MoE architectures through strategic reorganization of FFN parameters but not training MoE from scratch~\cite{llama-moe,llama-moe-v2,zheng2024learn}. 
The prevailing methodology replaces conventional FFN layers with MoE layers,
where neurons are partitioned into multiple expert sub-networks while maintaining the original parameter count. 
During inference, a routing mechanism selectively activates only a subset of experts per input token, thereby achieving dynamic sparsity without compromising model capacity.
However, due to the high sparsity target always required in MoE models,
these works often need massive computing resources and billions of training data for continual pre-training on the constructed MoE models.

To address the aforementioned limitations, we propose carved MoE, named CMoE,
a framework that efficiently carves sparse MoE architectures from dense LLMs through parameter reorganization and training-free structural adaptation. 
Unlike prior approaches that rebuild MoE models from scratch via resource-intensive pre-training,
CMoE strategically ``carves'' experts from the dense model’s existing feed-forward network (FFN) neurons while preserving their inherent knowledge. 
This carving process is both computationally efficient—requiring only minutes of lightweight processing—and sophisticated,
as it leverages systematic neuron grouping and analytical router construction to retain performance with minimal fine-tuning.
The core innovation lies in CMoE’s ability to restructure dense FFNs into MoE layers without re-training the base model. 
First, we identify neurons that universally encode common knowledge (a.k.a.~shared experts) and those that exhibit specialized, input-dependent activation patterns (a.k.a. routed experts).
Shared experts are systematically retained by selecting neurons with the highest activation rates, ensuring they capture broadly applicable features.
By formulating routed expert grouping as a balanced linear assignment problem, solved via the Jonker-Volgenant algorithm,
CMoE clusters neurons into experts while maintaining parameter balance and activation coherence. 
Second, we derive the routing mechanism directly from the dense model’s activation statistics, bypassing the need for end-to-end router training.
This involves constructing a differentiable routing function initialized using representative neurons from each expert cluster,
enabling immediate usability while preserving optimization potential.

In summary, the key contributions of this paper are:
\begin{itemize}
    \item CMoE: A framework that efficiently carves MoE from dense LLMs by reorganizing FFN neurons into shared/routed experts, eliminating costly pre-training.
    \item Shared and routed experts: ensuring parameter balance and activation coherence for CMoE.
    \item Training-Free Routing: using representative neurons and lightweight adaptation for router initialization, which enables rapid performance recovery.

\end{itemize}

Extensive experiments show that, with activation ratio $25\%$, CMoE can maintain a reasonable perplexity even without fine-tuning, and achieve $76.59\%$ accuracies of dense models on some downstream benchmarks with lightweight fine-tuning on 2,048 samples.

% Training MoEs from scratch often leads to a significant overall budget, compared with this, starting from the dense model can provide flexible structure design choices for MoE~\cite{llama-moe, llama-moe-v2, wei2024skywork}. 


