\section{Background}

This study primarily focuses on the LLaMA family~\cite{touvron2023llama, dubey2024llama}, which uses SwiGLU~\cite{shazeer2020glu} as the activation function. However, our analysis and findings can be adapted to most of the FFN structures of existing LLMs, including the ReLU-based FFNs~\cite{nair2010rectified}.

An FFN exists in the tail of each transformer block, which gets the input embedding $\mathbf{x}\in \mathbb{R}^{d}$ and then contributes to the output together with the residual connection, i.e. $\mathbf{x} + F(\mathbf{x})$.
Typically, an FFN is a two-layer fully connected network, i.e. the up projection and down projection layer, with an activation function between them.
For LLaMA, the SwiGLU composes another gate projection layer.
Given the up projection weight $\mathbf{W}_{up}\in \mathbb{R}^{d \times d_h}$, the gate projection weight $\mathbf{W}_{gate} \in \mathbb{R}^{d \times d_h}$ and the down projection weight $\mathbf{W}_{down} \in \mathbb{R}^{d_h \times d}$, the process of an FFN is given by:
\begin{equation}\label{eq:ori_ffn}
\begin{aligned}
    & F(\mathbf{x}) = \mathbf{h}\mathbf{W}_{down}, \\
    & \mathbf{h}    \!=\! \text{SwiGLU}(\mathbf{x}\mathbf{W}_{up}) \!=\! \text{Swish} (\mathbf{x}\mathbf{W}_{gate}) \odot (\mathbf{x}\mathbf{W}_{up}),
    % F(\mathbf{x}) &= \mathbf{h}\mathbf{W}_{down},
\end{aligned}
\end{equation}
where $\text{Swish}(x) = x \cdot\sigma(x)$ is element-wise and $\sigma(\cdot)$ is the sigmoid function.

The basic MoE architecture is composed of a set of $N$ independent FFNs as experts, $\{E_1, E_2, . . . , E_N \}$, and a router network $G$~\cite{lepikhin2020gshard}.
% \footnote{***briefly mention what is an expert here, a FFN or a NN in general?}
The output of an MoE-version FFN is then obtained by
% \begin{figure}
\begin{equation}\label{eq:router_basic}
\begin{aligned}
    \!\!\!\!F_{MoE}(\mathbf{x}) \!&=\! \sum^{N}_{i=1} g_i E_i(\textbf{x}), \\
    g_i \!&=\! \left\{ \begin{array}{rl}
        s_i, \!&s_i \in \text{TopK}(\{s_i | 1\leq j\leq N\}, K),  \\ 
        0, \!& \text{otherwise},
    \end{array}\right. \\
    \textbf{s} \!&=\! \left[s_1,s_2,\cdots, s_N\right] = G(\textbf{x}),
\end{aligned}
\end{equation}
% \end{figure}
where $g_i$ is the score for the $i$-th expert, $\textbf{s} \in \mathbb{R}^{N}$ is the token-to-expert affinity, i.e. the output of $G$, and $\text{TopK}(\cdot, K )$ denotes the set comprising $K$ highest scores among the affinity scores calculated for $\mathbf{x}$ on all experts.

% when we re-examize the isoloted output, we will find that the activation results determine the output contribution.
