\begin{table*}[t!]
    \centering
    \vspace{-2mm}
    \caption{Comparison of perplexity results. Randomly select samples from WikiText-2 training dataset for calibration and training.}
    % \vskip -0.15in
    \label{tab:1}
    % \renewcommand{\arraystretch}{1.18}
    \begin{adjustbox}{max width=0.85\textwidth}
    \begin{tabular}{cc|cc|cc|cc|cc}
        \toprule
        \multirow{3}{*}{Method} & \multirow{3}{*}{Type}   &\multicolumn{4}{c|}{LLaMA-2-7B} & \multicolumn{4}{c}{LLaMA-3-8B} \\

        ~ & ~  &\multicolumn{2}{c|}{Training-free} & \multicolumn{2}{c|}{Fine-tuning} &\multicolumn{2}{c|}{ Training-free} & \multicolumn{2}{c}{Fine-tuning} \\
        
         ~ & ~ & WikiText-2 & C4 & WikiText-2 & C4 & WikiText-2 & C4 & WikiText-2 & C4\\
         \midrule
         \midrule
         Dense & - & 5.27 & 7.27 & - & - & 6.14 & 9.44 & - & - \\
         LLaMA-MoE & A2E8 & nan & nan & 468.00 & 2660.68 & nan & nan & 988.20 & 7521.83 \\
         LLaMA-MoE & A4E16 & nan & nan & 540.62 & 2690.68 & nan & nan & 1094.24 & 7758.91 \\
         CMoE & S1A1E8 & \bf 60.86 & \bf 135.61 & 12.76 & \bf  32.12 & 162.74 & 324.71 & 21.16 & \bf  64.03 \\
         CMoE & S1A3E16 & 89.19 & 180.65 & 13.84 & 33.56 & 262.85 & 465.09 & 22.97 & 72.34 \\
         CMoE & S2A2E16 & 62.30 & 136.12 & \bf  12.73 & 32.37 & \bf 143.38 & \bf 284.19 & \bf  21.01 & 65.57 \\
        \bottomrule
    \end{tabular}
     % \vspace{-0.6cm}
    % \vskip -0.1in
    \end{adjustbox}%\vspace{2mm}
\end{table*}