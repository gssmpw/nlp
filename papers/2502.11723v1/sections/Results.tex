\section{Experimental Results}
\label{results} 

We conducted an extensive benchmarking of different decoding strategies across 5 text generation tasks. The batch size is set to 1 in all experiments, as our selected text generation tasks from LM-Evaluation-Harness do not allow batch size modification for these cases. For each decoding method, we experimented multiple hyperparameters, repeating each configuration 5 times to capture variations in the LLM's inference behavior. Through preliminary testing, we found that 5 repetitions were sufficient to capture variations. During each run, we tracked both text generation quality and energy consumption. After completing each job, we filtered the energy usage data to focus solely on the active GPU processing periods. This approach allowed us to accurately quantify the GPU energy consumption and ensured that our findings are both reliable and reflective of real-world GPU usage during text generation tasks. 

In the following sections, we examine the trade-offs between text generation quality and GPU energy consumption for each decoding strategy and analyze how hyperparameter tuning impacts energy usage.

\subsection{Text Generation Quality vs Energy Efficiency}
\label{experimental:TG-EF}

The text generation quality and energy consumption for various decoding strategies using the Qwen2.5 7B-Instruct model are presented in Table \ref{tab:performance_energy_comparison}. The results were obtained using the best hyperparameter configuration that delivered the highest quality for each decoding method. The corresponding energy consumption for this optimal hyperparameter was averaged across all 5 runs and reported in watt-hours. Additionally, we evaluated the trade-off between quality and energy across different methods using an Efficiency Ratio (ER = Quality/Energy). This metric measures the generative quality achieved per unit of energy consumed, making it valuable for identifying methods that offer high quality while minimizing energy usage. 

\begin{table*}[h]
\footnotesize
\centering
\caption{Text Generation Quality and Average Energy Consumption across Decoding Methods in their best Hyperparameter Setting using Qwen2.5 7B-Insruct with batch size=1. The associated hyperparameters are listed in Table \ref{tab:optimal hyperparameter}. The table uses color coding to highlight key metrics: blue indicates high generative quality, red signifies high energy consumption, and green represents a high efficiency ratio.}
\label{tab:performance_energy_comparison}
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row height
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\begin{tabular}{!{\vrule width 1.2pt}c|c!{\vrule width 1.2pt}c|c|c|c|c|c!{\vrule width 1.2pt}c|c|c|c|c|c!{\vrule width 1.2pt}}
\noalign{\hrule height 1.2pt}
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{6}{c!{\vrule width 1.2pt}}{\textbf{Deterministic Methods}} & \multicolumn{6}{c!{\vrule width 1.2pt}}{\textbf{Stochastic Methods}} \\
\cline{3-14}
& & \textbf{Greedy} & \textbf{Beam} & \textbf{DBS} & \textbf{DoLa} & \textbf{CS} & \textbf{AD} & $\tau$ & \textbf{Top-p} & \textbf{Top-k} & \textbf{Eps} & \textbf{Typical} & \textbf{Min-p} \\
\noalign{\hrule height 1.2pt}
\multirow{3}{*}{De→En} & \textit{BLEU} & 41.09 &\cellcolor{blue!35} 42.71 &\cellcolor{blue!35} 42.65 & 22.97 & 41.03 &\cellcolor{blue!25} 41.76 & 41.02 & 41.26 & 41.11 & 31.53 & 35.68 & 38.39 \\
& \textit{Wh} & 9.54 & 11.45 & \cellcolor{red!50} 14.58 & \cellcolor{red!70} 22.72 & \cellcolor{red!50} 13.57 & 8.33 & 9.68 & 9.68 & 9.64 & 10.28 & 10.36 & 9.76 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{ER} &\cellcolor{green!40} 4.30 & 3.73 & 2.92 & 1.01 & 3.02 &\cellcolor{green!70} 5.01 & 4.23 &\cellcolor{green!40} 4.26 &\cellcolor{green!40} 4.26 & 3.06 & 3.44 & 3.93 \\
\hline
\multirow{3}{*}{En→De} & \textit{BLEU} & 26.66 &\cellcolor{blue!35} 29.88 &\cellcolor{blue!35} 29.33 & 19.11 & 26.94 & 26.47 & 26.73 &\cellcolor{blue!25} 26.96 & 26.85 & 18.12 & 24.93 & 24.97 \\
& \textit{Wh} & 13.23 & \cellcolor{red!50} 18.37 & \cellcolor{red!50} 17.74 & \cellcolor{red!70} 22.12 & 15.67 & 12.31 & 13.42 & 13.49 & 13.33 & 14.04 & 13.76 & 13.49 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{ER} &\cellcolor{green!40} 2.01 & 1.62 & 1.65 & 0.86 & 1.71 &\cellcolor{green!70} 2.15 &\cellcolor{green!40} 1.99 &\cellcolor{green!40} 1.99 &\cellcolor{green!40} 2.01 & 1.29 & 1.81 & 1.85 \\
\hline
\multirow{3}{*}{GSM8K} & \textit{Exact-M} & 0.82 &\cellcolor{blue!35} 0.90 & 0.87 & 0.79 & 0.85 & 0.80 &\cellcolor{blue!25} 0.88 &\cellcolor{blue!25} 0.88 & 0.86 & 0.83 & 0.85 & 0.87 \\
& \textit{Wh} & 13.51 & 20.32 & \cellcolor{red!70} 30.75 &\cellcolor{red!50} 18.11 &\cellcolor{red!50} 21.53 & 10.63 & 13.27 & 13.58 & 13.81 & 13.79 & 13.82 & 13.48 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{ER} & 0.06 & 0.044 & 0.028 & 0.043 & 0.039 &\cellcolor{green!70} 0.075 &\cellcolor{green!50} 0.066 &\cellcolor{green!50} 0.064 & 0.062 & 0.06 & 0.061 &\cellcolor{green!50} 0.064 \\
\hline
\multirow{3}{*}{Code2Text-Py} & \textit{S-BLEU} & 0.86 & 0.84 & 0.83 &\cellcolor{blue!25} 0.92 &\cellcolor{blue!35} 1.16 & 0.88 & 0.90 & 0.90 &\cellcolor{blue!25} 0.92 & 0.90 & 0.90 &\cellcolor{blue!25} 0.93 \\
& \textit{Wh} & 18.77 & 19.91 &\cellcolor{red!50} 23.85 &\cellcolor{red!50} 27.05 & \cellcolor{red!70}36.78 & 15.06 & 18.98 & 18.82 & 18.67 & 18.91 & 18.92 & 18.82 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{ER} & 0.045 & 0.042 & 0.034 & 0.034 & 0.031 &\cellcolor{green!70} 0.058 & 0.047 & 0.047 &\cellcolor{green!50} 0.049 & 0.047 & 0.047 &\cellcolor{green!50} 0.049 \\
\hline
\multirow{3}{*}{Code2Text-Js} & \textit{S-BLEU} & 0.73 & 0.74 & 0.69 & 0.72 &\cellcolor{blue!35} 1.18 & 0.76 &\cellcolor{blue!25} 0.78 &\cellcolor{blue!25} 0.80 &\cellcolor{blue!25} 0.78 & 0.75 & 0.75 & 0.75 \\
& \textit{Wh} & 18.97 & 20.21 & \cellcolor{red!50}22.43 &\cellcolor{red!50} 26.57 &\cellcolor{red!70} 36.77 & 15.63 & 19.06 & 19.15 & 18.94 & 18.97 & 18.79 & 19.04 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{ER} & 0.038 & 0.036 & 0.03 & 0.027 & 0.032 &\cellcolor{green!70} 0.048 &\cellcolor{green!50} 0.04 &\cellcolor{green!50} 0.041 &\cellcolor{green!50} 0.041 & 0.039 & 0.039 & 0.039 \\
\noalign{\hrule height 1.2pt}
\end{tabular}
}
\vskip -0.1in
\end{table*}

In both translation tasks, Beam Search and DBS achieve the highest BLEU scores—up to about 42.7 for De→En and 29.9 for En→De—but also draw considerable power, pulling down their overall efficiency ratio. In contrast, AD delivers BLEU scores near—or on par with—those top‐performing beam‐based methods while consuming far less power, thereby attaining the best efficiency ratios. Greedy decoding, along with Temperature, Top‐k, and Top‐p sampling methods, occupies a middle ground: they offer moderately high BLEU at moderate energy costs, resulting in fairly respectable efficiency.

Similarly, in GSM8K, Beam search achieves the highest exact-match accuracy (0.90) but requires 20.32 Wh of energy, resulting in a modest efficiency ratio. In contrast, AD maintains solid accuracy (0.80) at just 10.63 Wh, yielding the highest efficiency ratio among all methods. Several stochastic methods also strike a better balance than beam-based approaches, generating high-quality texts at more moderate energy levels. 

For both Python and JavaScript code summarization tasks, CS achieves the highest smoothed bleu scores, exceeding 1.0 in both cases, but also consumes the highest energy usage, putting it at a disadvantage efficiency‐wise. In both cases, CS, DoLa and DBS have the highest amount of energy consumption compared to other methods. In contrast, AD delivers near‐competitive smoothed bleu scores at a fraction of the energy, leading to the highest efficiency ratios among the tested strategies. Stochastic sampling techniques fall into a middle zone, similar to translation and math problem-solving tasks, offering moderate generation quality without the extreme energy consumption of beam-based or contrastive methods in deterministic approaches. 

To better demonstrate how the choice of decoding strategy affects generative quality and energy consumption, we employed the Relative Standard Deviation expressed as \(\text{RSD} (\%) = \frac{\sigma}{\mu} \times 100\) mathematically. RSD quantifies the variability of the data relative to its mean value. We utilize this statistical metric to provide a normalized measure of variability in text generation quality and energy consumption using various decoding methods across different tasks. Intuitively, a lower RSD indicates greater stability and consistency, while a higher value reflects greater variability. We depicted the RSD of quality and energy consumption for each task in Figure \ref{rsd-dataset} (based on the results from Table \ref{tab:performance_energy_comparison}). It is clear that the selection of decoding strategies significantly affects the variability of energy consumption. Although in some cases, such as GSM8k, we do not observe much variation in output quality when using different decoding methods, we do see significant energy variation in this case. This highlights the importance of being more energy-conscious when choosing a decoding strategy for our task. Choosing an inappropriate strategy may not greatly reduce output quality, but it can lead to a significant increase in energy consumption.   

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{figures/tradeoff_rsd.pdf}}
\caption{Relative Standard Deviation of Text Generation Quality and GPU Energy Usage for each task across all decoding strategies using Qwen2.5-7B-Instruct.}
\label{rsd-dataset}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table*}[h]
\footnotesize
\centering
\caption{Relative Standard Deviation (RSD) and Sharpe Ratio (SR) for each Decoding Strategy. RSD is calculated for text generation quality and average energy consumption across different hyperparameter values. SR takes into account the quality and energy consumption of all hyperparameter runs within a decoding method. The table uses color coding to highlight key metrics: blue indicates high RSD in quality, red signifies high RSD in energy consumption, and green represents a high SR.}
\label{tab:rsd_energy_performance_hyper}
\vskip 0.15in
\resizebox{1\textwidth}{!}{ % Scale the table to 90% of its original width
\setlength{\tabcolsep}{3pt} % Moderate column spacing
\renewcommand{\arraystretch}{1.3} % Adjust row height
\begin{tabular}{!{\vrule width 1.2pt}>{\centering\arraybackslash}p{2cm}|c!{\vrule width 1.2pt}c|c|c|c|c|c!{\vrule width 1.2pt}c|c|c|c|c|c!{\vrule width 1.2pt}}
\noalign{\hrule height 1.2pt}  
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{6}{c!{\vrule width 1.2pt}}{\textbf{Deterministic Methods}} & \multicolumn{6}{c!{\vrule width 1.2pt}}{\textbf{Stochastic Methods}} \\
\cline{3-14}
& & \textbf{Beam} & \textbf{DBS} & \textbf{DoLa} & \textbf{CS(\scalebox{0.7}{k=5})} & \textbf{CS(\scalebox{0.7}{k=25})} & \textbf{AD} & \textbf{$\tau$} & \textbf{Top-p} & \textbf{Top-k} & \textbf{Eps} & \textbf{Typical} & \textbf{Min-p} \\
\noalign{\hrule height 1.2pt}
\multirow{3}{*}{De→En} & \textit{BL (RSD\%)} & 0.31 & 0.46 &\cellcolor{blue!60} 25.96 &\cellcolor{blue!60} 68.62 &\cellcolor{blue!60} 94.99 & 0.49 &\cellcolor{blue!60} 51.21 & 9.22 & 12.30 & 2.99 & 5.32 & 6.83 \\
& \textit{Wh (RSD\%)} & 13.32 & 11.39 &\cellcolor{red!50} 20.08 &\cellcolor{red!60} 32.96 &\cellcolor{red!60} 53.45 & 2.25 &\cellcolor{red!60} 35.55 & 2.73 & 2.99 & 1.53 & 2.63 & 2.09 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{SR} & \(7e^{-3}\) & \(8e^{-3}\) & \(8e^{-4}\) & \(e^{-3}\) & \(3e^{-4}\) &\cellcolor{green!70} \(6e^{-2}\) & \(e^{-3}\) &\cellcolor{green!70} \(3.6e^{-2}\) & \(2.8e^{-2}\) &\cellcolor{green!70} \(5.1e^{-2}\) & \(3.3e^{-2}\) &\cellcolor{green!70} \(4.5e^{-2}\) \\
\noalign{\hrule height 1.2pt}
\multirow{3}{*}{En→De} & \textit{BL (RSD\%)} & 1.30 & 2.16 &\cellcolor{blue!60} 28.86 &\cellcolor{blue!60} 79.02 &\cellcolor{blue!60} 104.95 & 5.41 &\cellcolor{blue!60} 51.50 & 13.69 & 19.64 & 2.18 & 4.22 & 9.11 \\
& \textit{Wh (RSD\%)} & 11.09 & 7.02 &\cellcolor{red!50} 16.47 &\cellcolor{red!60} 18.22 &\cellcolor{red!60} 44.12 & 4.37 &\cellcolor{red!50} 22.77 & 1.45 & 1.88 & 0.92 & 0.59 & 1.98 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{SR} & \(4e^{-3}\) & \(6e^{-3}\) & \(9e^{-4}\) & \(e^{-3}\) & \(2e^{-4}\) & \(1.3e^{-2}\) & \(e^{-3}\) &\cellcolor{green!70} \(3.1e^{-2}\) & \(1.9e^{-2}\) &\cellcolor{green!70} \(3.5e^{-2}\) &\cellcolor{green!70} \(7.3e^{-2}\) & \(2.2e^{-2}\) \\
\noalign{\hrule height 1.2pt}
\multirow{3}{*}{GSM8K} & \textit{EM (RSD\%)} & 1.85 & 1.42 & 3.26 &\cellcolor{blue!60} 64.70 &\cellcolor{blue!60} 92.36 &\cellcolor{blue!60} 24.21 &\cellcolor{blue!60} 41.12 & 3.07 & 3.05 & 1.43 & 2.11 & 2.02 \\
& \textit{Wh (RSD\%)} &\cellcolor{red!60} 30.50 &\cellcolor{red!60} 20.06 & 0.31 &\cellcolor{red!50} 17.24 & \cellcolor{red!50}19.80 & 8.61 &\cellcolor{red!40} 16.39 & 0.75 & 0.86 & 1.27 & 2.56 & 1.19 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{SR} & \(3e^{-5}\) & \(4.5e^{-5}\) & \cellcolor{green!70}\(2.3e^{-3}\) & \(3.2e^{-5}\) & \(8e^{-6}\) & \(2.04e^{-4}\) & \(7.7e^{-5}\) & \cellcolor{green!70}\(1.9e^{-3}\) &\cellcolor{green!70} \(1.7e^{-3}\) & \(1.1e^{-3}\) & \(6.3e^{-4}\) &\cellcolor{green!70} \(1.3e^{-3}\) \\
\noalign{\hrule height 1.2pt}
\multirow{3}{*}{Code2Text-Py} & \textit{SBL (RSD\%)} & 4.67 & 6.99 & 0.48 & 6.77 &\cellcolor{blue!60} 12.35 & 1.44 &\cellcolor{blue!60} 11.03 & 3.18 & 3.30 & 3.16 & 2.66 & 4.88 \\
& \textit{Wh (RSD\%)} & \cellcolor{red!60}12.13 &\cellcolor{red!60} 10.07 & 0.35 & 0.76 & 1.42 & 1.69 & 0.57 & 0.44 & 0.33 & 0.53 & 0.32 & 0.27 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{SR} & \(8e^{-5}\) & \(8.4e^{-5}\) & \(1.7e^{-3}\) & \(1.2e^{-3}\) & \(5.2e^{-4}\) & \(9.1e^{-4}\) & \(1.9e^{-3}\) & \(2.5e^{-3}\) &\cellcolor{green!70} \(3.3e^{-3}\) & \(2.2e^{-3}\) &\cellcolor{green!70} \(3e^{-3}\) &\cellcolor{green!70} \(3.6e^{-3}\) \\
\noalign{\hrule height 1.2pt}
\multirow{3}{*}{Code2Text-Js} & \textit{SBL (RSD\%)} & 3.63 & 3.27 & 2.12 & 1.26 &\cellcolor{blue!60} 21.19 & 1.08 &\cellcolor{blue!60} 7.20 & 4.57 & 3.44 & 2.39 & 2.70 & 1.53 \\
& \textit{Wh (RSD\%)} & \cellcolor{red!60}13.67 &\cellcolor{red!60} 10.59 & 0.17 & 0.37 & 1.64 & 1.33 & 0.38 & 0.49 & 0.57 & 0.14 & 0.56 & 0.20 \\
\cline{2-14} % Horizontal line beneath Wh
& \textit{SR} & \(6e^{-5}\) & \(6.7e^{-5}\) &\cellcolor{green!70} \(2.6e^{-3}\) & \(2e^{-3}\) & \(4e^{-4}\) & \(9.7e^{-4}\) & \(2.3e^{-3}\) & \(1.9e^{-3}\) & \(1.7e^{-3}\) &\cellcolor{green!70} \(3.9e^{-3}\) & \(1.7e^{-3}\) &\cellcolor{green!70} \(3.1e^{-3}\) \\
\noalign{\hrule height 1.2pt}
\end{tabular}  
} % End resizebox 
\vskip -0.1in
\end{table*}

\subsubsection{Key Findings:} 
Across all tasks, AD consistently delivers outstanding energy efficiency while demonstrating near-competitive text generation quality. In contrast, widely adopted beam-based deterministic methods offer strong generative quality but also consume disproportionately high amounts of energy, calling into question their default and unnecessary use for every application. Top‐p, Top‐k, and temperature‐based sampling methods exhibit similar power consumption across tasks, resulting in very similar efficiency ratios. Within an acceptable margin, all stochastic methods—when tuned to their optimal hyperparameters—consume essentially the same amount of energy across all tasks. This consistent pattern suggests that, despite their different approaches to sampling, they impose comparable computational overhead when generating output, making them viable alternatives for each other in scenarios where a stable trade‐off between energy efficiency and performance is desired across the selected tasks. Contrastive Search stands out in certain tasks, yet its significant energy draw negatively affect its overall energy efficiency. Except for GSM8k, DoLa consistently ranks among the worst in terms of efficiency ratio. Although this newly introduced method aims to reduce hallucinations and improve truthfulness in LLM outputs, it does not offer high generation quality in our chosen tasks and remains one of the highest energy‐consuming approaches. Finally, the overall energy consumption pattern in each decoding strategy seems to remain consistent across benchmarks. This creates opportunity to optimize decoding approaches in light of both quality requirements and energy considerations.

\subsection{Hyperparameter Sensitivity Analysis}

To investigate the impact of hyperparameter choices on both text generation quality and energy consumption, it is crucial to determine their sensitivity across different configurations. To quantify this, we calculate the RSDs of both quality and energy to measure their variability and consistency across all hyperparameters for each decoding strategy. In this way, we can capture the degree of sensitivity of each method to hyperparameter changes, allowing for an interpretable comparison between decoding methods in each task. In this scenario, a lower RSD suggests robustness, where hyperparameter changes have minimal effect on the observed metrics. 

We also employed the Sharpe Ratio \cite{sharpe}, a classic statistical metric from finance, to incorporate the recorded energy variations observed in each run for a given hyperparameter within our analysis. Mathematically expressed as \(\frac{\mu - r_{f}}{\sigma}\), Sharpe Ratio measeures how well the average return of an investment compensates the volitality (risks) of returns. This reward-to-variability metric can be adapted in our analysis by treating the average of text generation qualities as the mean return of investments and the variability (standard deviation) of energy consumptions across all hyperparameter runs as the risks for each decoding strategy. This approach provides us a single metric to identify decoding strategies that are less sensitive to hyperparameter changes while simultaneously rewarding strong generative quality and penalizing energy fluctuations. A higher SR ratio signifies that the in all of our experiment runs for a specific decoding method, it consistently delivers good quality while minimizing energy consumption variations. Since we are ranking all decoding strategies within each task, we consider the risk‐free rate to be zero. Table \ref{tab:rsd_energy_performance_hyper} presents the RSD of text generation quality and average energy consumption across all hyperparameter values for each decoding strategy, along with the SR between quality mean and energy consumption variations across all hyperparameter runs.        

Interestingly, beam-based methods exhibit relatively low RSD in BLEU scores, indicating stable generation quality across hyperparameter changes in both translation tasks. However, in both tasks, we observe a similar pattern where CS and DoLa methods show the highest variations in quality and energy, highlighting their strong sensitivity to hyperparameter tuning. Stochastic methods like Typical and Min‐p, and Top-p sampling generally remain more stable in both quality and energy, resulting in competitive Sharpe Ratios. In contrast, we find that Temperature sampling has significantly higher RSD values for both quality and energy compared to other stochastic techniques. 

In GSM8K, we observe a similar pattern with CS and Temperature sampling methods exhibiting the highest variations in both quality and energy across their hyperparameters. Interestingly, AD shows one of the highest RSD values for quality, while its energy consumption remains relatively stable. In contrast, beam-based methods display the opposite trend, with minimal variations in quality but the highest fluctuations in energy across their hyperparameters. Another notable observation is that DoLa maintains extremely low energy variation, resulting in the highest SR ratio among all methods. Similar to translation tasks, most stochastic methods achieve a good balance between performance and energy stability, leading to high SR ratios.     

Across both Python and JavaScript code summarization tasks, we observe a similar pattern where CS and Temperature sampling exhibit the highest variation in generation quality across hyperparameters. In contrast, DoLa remains significantly more stable, leading to the highest SR ratio in Python summarization. Methods like Beam and DBS experience considerable energy fluctuations while maintaining moderate variability in generation quality. Similar to previous tasks, many stochastic approaches demonstrate relatively low RSD in energy consumption and moderate RSD in quality, resulting in high SR in code summarization tasks.

\subsubsection{Key Findings}

Across all tasks, we observe that, except for Temperature sampling, the other stochastic decoding methods achieve some of the highest SR ratios, maintaining good quality with minimal energy fluctuations across different hyperparameter choices. This suggests that these methods are less sensitive to hyperparameter tuning and are more energy-conscious relative to the quality they produce. The reason Temperature sampling exhibits significant variation in both energy consumption and quality is the inclusion of high-temperature values in our choices. Higher temperatures \(\tau > 1\) increase randomness in token selection, reducing output fluency and coherence. This quality loss is further illustrated in Figure \ref{appendix:fig-temperature}. Across all selected tasks, the model struggled to enhance diversity without sacrificing accuracy, leading to a noticeable drop in quality and increased energy consumption, particularly in translation and math problem-solving tasks. This is particularly interesting because, while Temperature sampling was among the methods with the highest efficiency ratio in Table \ref{tab:performance_energy_comparison}, the RSD results in Table \ref{tab:rsd_energy_performance_hyper} reveals that it has the greatest variation in both quality and energy across different hyperparameters. This underscores the importance of hyperparameter optimization when using this method. 

In some cases, such as beam-based methods in the math problem-solving task, we observe significant energy fluctuations despite minimal variation in quality. Conversely, in code summarization tasks, Temperature sampling exhibits the opposite trend, with substantial quality variation but stable energy usage. This proves that variations in quality and energy across hyperparameters do not always correlate. 

Another notable finding is that, while in Section \ref{experimental:TG-EF} we identified AD as the most energy-efficient method in its best hyperparameter configuration for each task, We observe that only in the German-English translation task does it rank among the best methods with the highest SR ratio. In other cases, it exhibits variations in both generation quality and energy consumption across its selected hyperparameters, implying that AD performs optimally only when properly tuned in some text generation tasks. Additionally, for beam-based methods, we observe considerable energy fluctuations when increasing the beam width in code summarization and math problem-solving tasks (Figure \ref{appendix:fig-beam}). However, in translation tasks, energy consumption remains relatively stable, and the quality does not vary significantly. Both of these findings highlight the importance of task-specific hyperparameter tuning for certain decoding strategies. 

Contrastive Search stands out as the most sensitive to hyperparameter changes among all methods, with \(k=25\) exhibiting significantly greater energy fluctuations than \( k\) across different penalty \(\alpha\) values. Larger \( k\) values can lead to substantial shifts in both performance and energy consumption. Additionally, the impact of the \(\alpha\) parameter varies considerably depending on the task, contributing to its high RSD values in both quality (up to 104.95\%) and energy consumption (up to 53.45\%). 

Lastly, we observe a unique pattern in the DoLA method. While Table \ref{tab:performance_energy_comparison} shows that it is among the methods with the highest energy consumption in its best hyperparameter setting, it also achieves some of the highest SR ratios in math problem-solving and translation tasks in Table \ref{tab:rsd_energy_performance_hyper}. This indicates that, although DoLA may consume significant energy, its energy usage remains stable across different hyperparameters while still delivering acceptable (though not outstanding) generation quality in certain NLP tasks.



