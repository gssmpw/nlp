\section{Limitations \& Future Work}
\label{limitation}

In this study, we confined our experiments to a relatively small-scale LLM, Qwen2.5 7B-Instruct, leaving the investigation of larger models and architectures for future work. While we carefully selected hyperparameters for our experiments, our approach was not exhaustive, leaving a broader range of parameter settings yet to be explored. Additionally, because LM-Evaluation-Harness framework did not offer benchmarking open-ended text generation tasks at the time of conducting this research, we focused on a subset of directed tasks that best suited our objectives. Recent decoding strategies research efforts significantly revolves around open-ended language generation, such as story creation and text continuation; thus, exploring how various decoding techniques influence energy consumption in such scenarios would be a valuable direction to pursue. We also did not combine different decoding strategies, suggesting another avenue for future research. Finally, our analysis focused on GPU energy consumption; however, future work could extend this scope to encompass other factors such as CPU power usage, memory and temperature, providing a more comprehensive understanding of how decoding strategies influence overall resource consumption.
