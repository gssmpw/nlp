\section{Conclusion}
\label{conclusion}

In this study, we investigated the impact of decoding strategies and hyperparameter choices on both generative quality and GPU energy consumption during LLM inference across various text generation tasks. Through a comprehensive evaluation, we found that, in all tasks, the choice of decoding strategy significantly affects GPU energy consumption during inference, even in cases where it has minimal impact on output quality. Additionally, we observed that different decoding methods present distinct trade-offs between text quality and energy efficiency. While deterministic methods generally achieve top-tier quality in most generation tasks, they also tend to consume more energy and exhibit higher variability across their hyperparameters. In contrast, stochastic approaches appear to be more energy-efficient, often providing a better quality-to-energy ratio and demonstrating lower sensitivity to hyperparameter changes relative to their output quality. Overall, no single decoding strategy outperforms all others across every metric. Instead, the optimal choice depends on the specific task and whether the priority is maximizing generation quality, minimizing energy consumption, or achieving a stable balance between both. Our findings highlight the importance of carefully selecting decoding strategies, taking into account not only output quality but also computational and energy costs.