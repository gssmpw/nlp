\section{Background}
\label{background}

\subsection{Decoding Strategies}

LLMs generate text sequentially in an autoregressive manner. At each step, the model computes a probability distribution over the vocabulary. Decoding strategies determine how the model converts these probabilities into generated text. The choice of decoding method significantly influences the quality, diversity, and coherence of the output. These strategies are generally categorized into two types: deterministic and stochastic.

\subsection{Deterministic Decoding Strategies}
\label{deterministic decoding}

\begin{itemize}\itemsep0em 
    \item \textbf{Greedy:} The most basic decoding strategy where the model select the token with the highest probability at each generation step \cite{two,consistency}. Since tokens with the highest probability are always chosen while the rest are completely ignored, this technique may leads to repetition and degeneration, especially in tasks where diversity in the output text is an important factor.
    
    \item \textbf{Beam Search:} Instead of greedily selecting the tokens, \cite{beam} proposed keeping track of multiple candidate sequences (a beam), expanding them at each generation step, and selecting the top ones based on their overall scores after multiple steps. The number of these candidate sequences is controlled by the beam width, which serves as a hyperparameter.
    
    \item \textbf{Diverse Beam Search (DBS):} \cite{dbs} introduced an extension of the original Beam Search method by dividing the candidate sequences into multiple subgroups while enforcing diversity within each group. The key hyperparameters for this approach are the beam width and the number of groups.
    \item \textbf{Contrastive Search (CS):} \cite{six} introduced a contrastive framework to penalize repetitive token selections while maintaining coherence. After selecting the top-k candidates from the model, they are assessed based on the likelihood assigned by the LLM and a degeneration penalty that evaluates how close the token is relative to the previous context. This method have two hyperparameters: k, which controls candidate selection based on top-k, and \( \alpha\), which controls degeneration.
    \item \textbf{DoLa:} \cite{seven} designed a decoding technique to improve factual accuracy in LLMs. Unlike other decoding methods that rely solely on the final layer’s logits, DoLa compares the output logits of earlier layers with those of the final layer. The specific layers to be contrasted with the final one can be set as a hyperparameter.
    \item \textbf{Assisted Decoding (AD):} A novel strategy, also known as speculative decoding, was introduced by \cite{speculative} to accelerate LLM inference. This technique uses an assistant model to generate multiple candidate tokens in advance, which are then verified by a larger target model. In this way, the number of sequential computations during inference is significantly reduced, speeding up the decoding process. Since we used only the n-gram-based speculative decoding in the greedy setting and not the model-based approach, we categorize it as deterministic. The hyperparameter in this case is the number of tokens to be output as candidate tokens.
\end{itemize}

\subsection{Stochastic Decoding Strategies}
\label{stochastic decoding}

\begin{itemize}\itemsep0em 
    \item \textbf{Temperature (\( \tau\)):} This sampling method simply modifies the logits before applying the Softmax \cite{bridle} function in the decoding process. When the temperature is low, the method becomes more deterministic by increasing the likelihood of selecting high-probability tokens. In contrast, selecting higher temperature values introduce randomness in generation by increasing the likelihood of choosing lower-probability tokens.
    
    \item \textbf{Top-k:} \cite{topk} introduced a method which sampling is performed from the \( k\) most probable candidates in the vocabulary at each token generation step. A smaller \( k\) makes the output more deterministic, whereas a larger k allows tokens with lower probabilities to be considered in the selection process.
    
    \item \textbf{Top-p (Nucleus):} Rather than selecting a fixed number of tokens and sampling from them, \cite{topp} proposed selecting tokens based on their probability distribution. They suggested choosing the minimal set of tokens whose cumulative probability goes beyond a probability \( p\)
    
    \item \textbf{Epsilon Sampling:} \cite{epsilon} introduced another truncation sampling approach by selecting only the tokens whose conditional likelihoods are above an entropy-dependent threshold \( \epsilon\). This ensures that the highly unlikely tokens are excluded. 
    \item \textbf{Typical Sampling:} To generate text that resembles most to human writing, \cite{typical} proposed selecting tokens based on information entropy. In the sampling approach, we select tokens based on how close a token’s probability is to the expected information content at each step. Thus, tokens that are too predictable or too unlikely are discarded, with the selection process controlled by a typicality threshold.
    \item \textbf{Min-p Sampling:} \cite{eight} proposed a dynamic token selection strategy to overcome the limitations of other truncation-based sampling methods. Their approach adjusts the filtering threshold at each step based on the probability of the most likely token. The key hyperparameter is a base probability, which is multiplied by the likelihood of the most probable token to dynamically adjust the threshold for the next generation step. 
\end{itemize} 
