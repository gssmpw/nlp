\section{Experimental Setting}
\label{experiments} 

\subsection{Compute Resources} 

All the experiments were conducted on a compute node equipped with two NVIDIA A100-PCIE-40GB GPUs and a Dual 64-c ore AMD EPYC 7763 Processor @ 2.45GHz. The system is managed by SLURM \cite{slurm}, an open-source workload manager that facilitates resource allocation, job scheduling, and queue management. In each job (experiment), we requested only one GPU. Before running the main job, we start monitoring and collecting the GPU resource utilization data over the job’s run time. This monitoring is stopped immediately after the main job is finished. The maximum power draw for both GPUs is set to 250 watts.

All the experiments were run on an isolated compute node to minimize the variations in the LLM inference measurements. When running the same experiment (a specific decoding method with fixed parameters with same input data) multiple times alongside other users’ jobs in a compute node, we sometimes observed significant variations in the total inference time and consumed energy. These fluctuations in LLM inference workloads are due to the shared nature of the system (the potential disruptions of other jobs) and distinct compute characteristics of LLMs \cite{eleven}. Consequently, we submitted all our jobs on isolated node to mitigate the significant variations.  

\subsection{Monitoring Tools}

We use the \textit{nvidia-smi} \cite{smi} utility to monitor and manage GPUs in our experiments. This command-line utility provides detailed information regarding GPU performance metrics including GPU utilization, temperature, memory usage, memory clock frequency, power consumption, and etc. We use this real-time monitoring utility to record GPU resource usage including utilization and power consumption at 1000 ms intervals for each experiment. 

To quantify the total energy consumption, we rely on the \( E = \sum_{i=1}^{n} P_i \cdot \Delta t_i\) formula that relates energy to power (\( P_i\)) and time period (\(\Delta t_i\)). Since we recorded power measurements at 1-second intervals (\(\Delta t_i = 1\)), assuming constant power over each second, the energy consumption for each second simplifies to the recorded power (in watts). As a result, the total energy consumption in joules is equivalent to the summation of the recorded power values over the entire inference duration. GPU energy consumption, especially in LLM workloads, results in significantly large values when measured in joules, as joules are a relatively small unit for quantifying energy. To facilitate easier comparison between decoding methods, we converted the energy consumption from joules (J) to watt-hours (Wh) for reporting in this paper(\( 1\mathrm{Wh} = 3.6 \times 10^3 \, \mathrm{J}\)).

\subsection{Benchmarks}

For reproducibility, we used \textit{LM-evaluation-harness} \cite{eval-harness} to benchmark and evaluate the chosen LLMs across different decoding methods. This is an open-source framework designed for evaluating and testing the performance of the LLMs across a wide range of tasks and benchmarks. We evaluated our selected decoding strategies across three text generation tasks: \textit{Translation}, \textit{Code Summarization}, and \textit{Math}. For all three tasks, we utilized only a subset of the full datasets. Using the complete dataset would unnecessarily increase inference time and energy consumption during experimentation, without bringing any meaningful insights to the study. We emphasize that we reported only the available performance metrics for the generated texts as provided by the LM-evaluation-harness framework across our selected tasks.   

\subsubsection{Translation}

We selected WMT16 \cite{wmt2016} dataset to evaluate the translation performance. WMT is a widely recognized competition for machine translation tasks, providing high-quality datasets across multiple languages. In our experiments, we only focused on German-English and English-German and only considered the first 500 input prompts of the whole dataset in both cases. Also, the number of few-shots was set to 0 for all translation jobs. 

We report the \textit{BLEU} \cite{bleu} in our translation experiments. This widely-used evaluation metric measures the n-gram overlap between the generated output and the ground truth, with higher scores indicating the better generative performance. 

\subsubsection{Code Summarization}

The CodeXGLUE \cite{codexglue} benchmark suit is a collection of datasets designed to evaluate the AI models on various code-related tasks. Specifically, we used Code Summarization \cite{codesearchnet} task to generate concise and coherent descriptions of source code snippets. This task evaluates the LLM's ability to understand the semantics of code and translate it into meaningful summaries. We only summarized the first 100 code snippets of Python and Java Script programming languages in to natural language descriptions while the number of few-shots was set to 0 for all experiments. 

We use the \textit{Smoothed BLEU} \cite{smoothed} evaluation metric, a modified version of the standard BLEU, to alleviate the issue of non-overlapping high-order n-grams in the code summarization task \cite{codexglue}. 

\subsubsection{Math} 

We utilized the GSM8K dataset \cite{gsm8k} to evaluate the reasoning and mathematical problem-solving capabilities of LLMs under different decoding strategies. Although the popular GSM8K consists of 8K grade-school-level math problems, we only focused on the first 100 problems out of the whole benchmark. In this specific task, we used 5-shot prompts in our experiments. 

For this benchmark, the \textit{Exact Match} accuracy is reported as the performance of the generated text compared to the ground truth. As explained in \cite{lessons}, the exact match checks if each token in the a language model's output aligns with the greedy decoding generation of the input at each step. If all tokens align perfectly, the LLM's generated text is considered an exact match. Thus, a higher value indicates better text generation performance.


\subsection{LLM} 

In our experiments, we used Qwen 2.5 \cite{qwen2.5}, a recent series of decoder-only LLMs. Specifically, we experimented different decoding strategies with 7B-instruction-tuned model. We selected this model size due to its robust performance across a wide range of text generation tasks and to accelerate the decoding process in our experiments. 
%By utilizing two model sizes, we aim to analyze how scaling parameters can affect decoding behavior, runtime, and energy consumption in large-scale inference settings.      

\subsection{Strategies \& Hyperparameters}  

We employed the decoding strategies introduced in Sections \ref{deterministic decoding} \& \ref{stochastic decoding} in our study. All adopted strategies have been implemented in the Hugging Face \cite{transformers} library. We also tuned the hyperparameters of each strategy To evaluate their impact on text quality and energy consumption across our selected tasks. The range of hyperparameter values for each decoding technique was selected based on recommendations from standard practices and recent studies \cite{one,two}. However, in some cases, we included unconventional hyperparameter values to investigate their impact, particularly on energy consumption. For example, in the Top-K decoding strategy, we conducted experiments with \( k = 1\). While \( k = 1\) is essentially equivalent to the greedy decoding strategy, this allowed us to examine whether differences in implementation between greedy decoding and Top-K decoding result in variations in performance or energy consumption. Table \ref{table-hyper} lists the utilized decoding strategies and their associated hyperparameters. Our goal is not to identify the optimal decoding hyperparameters for the entire benchmark suite across different models but rather to examine the relationship between performance and energy consumption in comparable settings for each decoding strategy. By focusing on a limited subset of datasets, we aim to determine which decoding strategy is more energy-efficient during LLM inference.

 



