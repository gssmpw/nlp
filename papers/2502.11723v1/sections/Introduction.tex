\section{Introduction}
\label{intro}

In the past few years, we have witnessed a significant increase in the applications of Large Language Models (LLMs). Powerful text generation LLMs such as OpenAI's ChatGPT \cite{chatgpt}, Google’s Gemini \cite{gemini}, and Anthropic’s Claude Sonnet \cite{sonnet} have been completely intertwined in our daily lives. The ability of these models to generate high-quality coherent response text is significantly influenced by the choice of decoding strategies during inference. Decoding strategies transform the probability distributions generated by LLMs into fluent and relevant output text \cite{one,two,three}. They determine how tokens are selected at each step of text generation. This transformation termed as decoding is as crucial as the model itself in achieving the best quality of the generated text. However, the selection of the appropriate decoding strategy and tuning their specific parameters remain insufficiently explored \cite{one}. In most cases, the evaluations of the newly proposed, state-of-the-art LLMs are only conducted using the default decoding method, instead of studying the effects of adopting different strategies and varying their parameters to achieve the best benchmarking results.     

On the other hand, the popularity and increasing applications of LLMs across various domains has caused the AI community to continuously scale the model sizes up to hundreds of billion parameters \cite{palm}. This growth has led to a significant surge in computational demands causing a considerable energy consumption associated with the hardware these models are being deployed, especially the GPUs \cite{nine,ten}. GPUs have become indispensable to modern high-performance computing (HPC) infrastructure due to their efficient parallel processing \cite{sixteen}. This enhanced capability has made them an excellent accelerator for training and deploying LLMs. However, their significant energy consumption has raised serious alarms recently due to their increasing scale and the global push towards sustainable AI \cite{nine,twelve}. Although several research have studied the energy consumption and environmental footprints regarding training LLMs \cite{thirteen,fourteen,fifteen,seventeen}, the energy consumption concerns associated with LLM inference have been studied much less. This contrasts with the fact that LLM inference has become the main contributor of AI energy footprints recently \cite{eleven}. 

Prior research has primarily focused on the introduction of new text generation decoding strategies in the past few years \cite{four,five,six,seven,eight}. Each one of these techniques claims to be a superior method based on text quality metrics on specific text generation tasks.  While the effects of various decoding strategies on text quality, diversity, and coherence have been studied recently \cite{one,two}, their implications for computational efficiency and energy consumption remain unexplored. Decoding strategies differ in their computational complexity and, consequently, their energy demands. Decoding strategies differ in their computational complexity and, consequently, their energy demands. For instance, beam search, which explores multiple hypotheses simultaneously, is more computationally demanding compared to simpler methods like greedy search. Understanding how these decoding techniques affect energy consumption is necessary for optimizing the LLM inference in resource-constrained devices and achieving an appropriate balance between model performance and sustainability goals. 

The main contribution of this research is to analyze the impact of decoding strategies on GPU energy consumption, including:

\begin{itemize}\itemsep0em 
\item \textit{Measuring and comparing GPU energy consumption across various decoding strategies}
\item \textit{Analyzing the trade-offs between energy efficiency and generated text quality for different decoding techniques}
\item \textit{Investigating the relationship between hyperparameter configurations and GPU energy consumption across decoding methods}
\item \textit{Developing an evaluation framework combining energy monitoring, quality metrics and statistical analysis across multiple NLP tasks}
\item \textit{Introducing new efficiency metrics that combine generation quality and energy consumption to enable systematic comparison of decoding strategies}
\item \textit{Quantifying the stability and robustness of different decoding methods through hyperparameter sensitivity analysis}
\end{itemize}

By providing these contribution, our research aims to contribute to the current literature of compute performance and energy utilization characteristics of LLM inference. Additionally, we hope it encourages further analysis, benchmarking, and broader sharing of systematic performance insights across a wider range of large models, particularly under varying hardware configurations. This can be especially relevant for researchers and users with limited computational resources. 