\section{Related Work}
\subsection{Brain Graph Analysis}

Brain graphs, reflecting the connections in human neural system, are constructed from various brain health data, such as functional magnetic resonance imaging (fMRI), positron emission tomography (PET), and electroencephalography (EEG)____. Recently, graph learning-based brain graph analysis has attracted increased attention, dominating a range of tasks (e.g., brain disease detection and treatment recommendation)____. NeuroGraph____ collects various brain connectome datasets for benchmarking graph learning models in brain graph analytical tasks (e.g., gender identification). BrainPrint____ develops a network estimation module and a graph analysis module to embed EEG features. 
BrainGNN____ contains special ROI-aware graph convolutional layers to capture the functional information of brain networks for fMRI analysis. %STAGIN____ uses a spatiotemporal attention mechanism in graph neural networks to learn the dynamic brain graph representation, capturing the temporal changes in brain graphs. 
BrainGB____ summarizes the pipelines of brain graph construction. BRAINNETTF____ utilizes a Transformer-based model to analyze brain graphs, while ignoring the structural encoding of ROIs and failing to preserve the small-world architecture of brain graphs. MSE-GCN____ applies multiple parallel graph convolutional network layers to encode brain structural and functional connectivities to detect early Alzheimerâ€™s disease (AD). GroupBNA____ constructs group-specific brain networks via a group-adaptive brain network augmentation strategy.


\subsection{Graph Transformers}

Graph transformers attempting to generalize Transformer models to graph data have shown significant performance in graph representation tasks____. SAN____ leverages the full Laplacian spectrum as the learned PE of input nodes, emphasizing the global structural information of the graph. Graphomer____ introduces three SE methods to the Transformer architecture, including centrality encoding, spatial encoding and edge encoding, for graph representation learning. SAT____ proposes a structure-aware self-attention mechanism to extract subgraph representations of nodes. SGFormer____ presents a single-layer attention model utilizing linear complexity to capture global dependencies among nodes. Geoformer____ considers atomic environments as the PE of nodes in molecular graphs. EXPHORMER____ proposes a sparse attention mechanism based on virtual global nodes and expander graphs for large graph representation learning. GRIT____ proposes a learned PE based on relative random walk probabilities and a flexible self-attention mechanism aiming to update both node and node-pair representations.