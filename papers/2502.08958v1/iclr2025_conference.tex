
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath,amsthm}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{lipsum} % 
\usepackage{booktabs} % 
\usepackage{soul}
\usepackage{wrapfig} 
%\usepackage{subfigure}
%\bibliographystyle{unsrt} 
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\title{Biologically Plausible Brain Graph Transformer}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy


\author{
Ciyuan Peng$^{1}$, Yuelong Huang$^{2}$, Qichao Dong$^{3}$, Shuo Yu$^{2}$, Feng Xia$^{4}$\thanks{Corresponding authors: \texttt{f.xia@ieee.org, jinyaochu@westlake.edu.cn}}, Chengqi Zhang$^{5}$, Yaochu Jin$^{6}$\footnotemark[1] \\
$^{1}$Federation University Australia, $^{2}$Dalian University of Technology, $^{3}$Zhejiang Gongshang University, \\
$^{4}$RMIT University, $^{5}$Hong Kong Polytechnic University, $^{6}$Westlake University 
%\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

%Biological plausibility is the foundation for brain graph 

%State-of-the-art brain graph analysis methods generally lack biological plausibility, primarily because they fail to fully encode the small-world architecture of brain graphs (accompanied by the presence of hubs and functional modules). 


State-of-the-art brain graph analysis methods fail to fully encode the small-world architecture of brain graphs (accompanied by the presence of hubs and functional modules), and therefore lack biological plausibility to some extent. This limitation hinders their ability to accurately represent the brain's structural and functional properties, thereby restricting the effectiveness of machine learning models in tasks such as brain disorder detection. In this work, we propose a novel Biologically Plausible Brain Graph Transformer (BioBGT) that encodes the small-world architecture inherent in brain graphs. Specifically, we present a network entanglement-based node importance encoding technique that captures the structural importance of nodes in \textit{global information propagation} during brain graph communication, highlighting the biological properties of the brain structure. Furthermore, we introduce a functional module-aware self-attention to preserve the \textit{functional segregation and integration} characteristics of brain graphs in the learned representations. Experimental results on three benchmark datasets demonstrate that BioBGT outperforms state-of-the-art models, enhancing biologically plausible brain graph representations for various brain graph analytical tasks\footnote{Our code is available at \url{https://github.com/pcyyyy/BioBGT}.}.
\end{abstract}

\section{Introduction}

\begin{wrapfigure}{r}{0.5\textwidth} 
    \centering
    \vspace{-1.5\baselineskip} 
    \includegraphics[width=0.5\textwidth]{./fig/figure1.pdf}  
    \caption{Small-world architecture of brain graphs.}
    \label{figure1}
    \vspace{-\baselineskip}  
\end{wrapfigure}
Brain graphs, also known as brain networks, are a primary form to present the complex interactions among regional activities, functional correlations, and structural connections within the brain~\citep{seguin2023brain,wu2024network,zhu2024spatio}. Brain graphs are constructed based on information extracted from brain data, such as functional magnetic resonance imaging (fMRI), with regions of interest (ROIs) as nodes and the correlations among ROIs as edges. One of the most important characteristics of brain graphs is their \textit{small-world architecture}, with scientific evidence supporting the presence of \textit{hubs} and \textit{functional modules} in brain graphs~\citep{liao2017small,swanson2024neural}. First, it is demonstrated that nodes in brain graphs exhibit a high degree of difference in their importance, with certain nodes having more central roles in information propagation~\citep{lynn2019physics,betzel2024hierarchical}. These nodes are perceived as hubs, as shown in Figure~\ref{figure1}~(a) (the visualization is based on findings by~\cite{seguin2023brain}), which are usually highly connected so as to support efficient communication within the brain. Second, human brain consists of various functional modules (e.g., visual cortex), where ROIs within the same module exhibit high functional coherence, termed functional integration, while ROIs from different modules show lower functional coherence, termed functional segregation~\citep{rubinov2010complex,seguin2022network}. Therefore, brain graphs are characterized by community structure, reflecting functional modules.
Figure~\ref{figure1}~(b) visualizes the functional connectivity of a sample brain from ADHD-200\footnote{\url{https://fcon_1000.projects.nitrc.org/indi/adhd200/}} dataset. The functional module labels are empirically provided based on~\cite{dosenbach2010prediction}. ROIs in the same module have strong connections (high temporal correlations), while those from different modules show weaker connections.


%Moreover, functional modules in BGs reflect the biological properties of functional integration, which combine specialized information from distributed brain regions, and functional segregation, which allows specialized information processing within densely interconnected groups of brain regions~\citep{rubinov2010complex,sporns2013network}. 

%Graph representation learning has been widely applied to brain graph analysis~\citep{kan2022brain,liu2023braintgl,ding2023lggnet,peng2024adaptive}, dominating a range of tasks (e.g., brain disease detection and treatment recommendation). Graph neural network (GNN) and its variants utilize information aggregation methods (e.g., message-passing mechanism) to refine brain graph representations and capture the topological structure of BGs~\citep{li2021braingnn,kan2022fbnetgen}. 

%Due to the tremendous success of Transformers in graph representation, 


With the significant ability of graph transformers in capturing interactions between nodes~\citep{ma2023single,shehzad2024graph,yi2024nar}, Transformer-based brain graph learning methods have gained prominence~\citep{kan2022brain,bannadabhavi2023community}. Despite these advancements, there is still a lack of tailored design for brain graphs that consider the small-world architecture as an essential feature. Consequently, the learned representations of current methods may lack sufficient biological plausibility for brain graphs. This limitation can be understood from two perspectives. First, most current studies consider connections within the brain as pairwise correlations between nodes, and typically treat all nodes equally. For example, Brain Network Transformer~\citep{kan2022brain} assumes that all nodes in a brain graph have the same degree and each node is connected with all the other nodes. However, nodes can have significantly different roles in terms of propagating information within the brain graph~\citep{lynn2019physics}. Second, current methods often encode the correlations between nodes simply based on node-level similarities, ignoring the existence of functional modules within a brain graph. Unfortunately, the existing labeling of functional modules is largely empirical and lacks precision~\citep{kan2022brain}. Therefore, this limitation is particularly evident in brain datasets where functional module labels are unavailable or inaccurate. This impedes preserving the functional segregation and integration characteristics of the brain.
%\cite{akiki2019determining} %功能标签100种
%For example, [3] provides more than 100 different functional module organizations based on hierarchical clustering. 
% Most current methods do not provide explicit supervision signals to guide model to obtain biological properties of brains. 

To this end, this paper proposes a brain graph representation learning technique that departs from existing methods. %We aim to enhance the biological plausibility of the learned representations, with a focus on encoding the small-world architecture of brain graphs. 
We aim to improve the alignment of the learned representations with biological properties, particularly by encoding small-world features commonly observed in brain graphs. 
%We propose a Biologically Plausible Brain Graph Transformer (BioBGT), which emphasizes the biological plausibility of brain graph representations from two perspectives: \textit{node importance encoding} and \textit{functional module encoding}. 
%Node importance within a graph can be interpreted in various aspects. 
We propose a Biologically Plausible Brain Graph Transformer (BioBGT), which aligns brain graph representations with biological properties through two main components: \textit{node importance encoding} and \textit{functional module encoding}. \textbf{(1)} For brain graphs as communication networks, node importance is reflected by how crucial a node is in propagating information across the network~\citep{seguin2023brain}. Thus, we propose a node importance encoding technique based on \textit{network entanglement}. Given the topology of a brain graph, the global information diffusion process is modeled through quantum entanglement, wherein the importance of a node is measured by the changes in the density matrix-based spectral entropy before and after perturbing the local connections surrounding the node. The encoding of node importance is thereafter embedded into node representations, reflecting the small-world architecture in terms of the presence of hubs. \textbf{(2)} We then present a functional module-aware self-attention to preserve the functional segregation and integration characteristics of brain graphs in the learned representations. Particularly, we design a community contrastive strategy-based functional module extractor to refine nodes' similarities at the functional modular level, instead of merely calculating node correlations at the node level. Therefore, we can obtain \textit{functional module-aware node representations} for the self-attention mechanism.

\textbf{Contributions.}~%This paper highlights that brain graph representations obtained from learning models should be biologically plausible. %This paper emphasizes the significance of biologically plausible brain graph representations for brain analysis. align closely with biological properties
This paper highlights that brain graph representations obtained from learning models should align closely with the biological properties of the brain. Under this perspective, \textbf{i)}~we propose a new Biologically Plausible Brain Graph Transformer entitled BioBGT that encodes the small-world architecture of brain graphs to enhance the biological plausibility of the learned representations; \textbf{ii)}~we present a network entanglement-based node importance encoding technique, capturing node importance in the information propagation across brain graphs; \textbf{iii)}~we introduce a functional module-aware self-attention, yielding functional module-aware node representations with the functional segregation and integration characteristics of brain graphs preserved; \textbf{iv)}~experimental results show the effectiveness of our model design and the superiority of our model performance, especially in brain disease detection tasks. 


\section{Preliminaries}

\subsection{Problem Definition}

A brain graph presents the connectivity between ROIs, characterized by the small-world architecture. A brain graph with $n$ nodes (ROIs) is denoted as $G=(V,E,\mathbf{X})$, where $V$ stands for the node set, $E$ is the edge set, and $\mathbf{X}\in \mathbb{R}^{n\times d}$ represents the feature matrix with the $i$-th row vector $\mathbf{x}_i\in \mathbb{R}^{d}$ indicating the feature of node $i$. Here, $d$ is the hidden feature dimension. Hubs and functional modules are two crucial indicators of the small-world brain graph~\citep{rubinov2010complex}. This paper suggests that the biological plausibility of brain graph representations can be enhanced by capturing these features to an extent. For a given brain graph, our model aims to learn representations that incorporate biologically relevant features and achieve accurate brain graph analysis.


\subsection{Graph Transformers}
A Transformer architecture is composed of multiple Transformer layers, each of which contains a self-attention module followed by a feed-forward network (FFN)~\citep{vaswani2017attention}. In the self-attention module, the input feature matrix $\mathbf{X}\in \mathbb{R}^{n\times d}$ is first projected to query matrix $\mathbf{Q}$, key matrix $\mathbf{K}$, and value matrix $\mathbf{V}$ by the corresponding projection matrices$\mathbf{W}_Q\in\mathbb{R}^{d\times d_\mathcal{K}}$, $\mathbf{W}_K\in \mathbb{R}^{d\times d_\mathcal{K}}$, and $\mathbf{W}_V\in \mathbb{R}^{d\times d_\mathcal{K}}$:
\begin{equation}
	\centering
	\label{eq1}
	\mathbf{Q}=\mathbf{X}\mathbf{W}_Q, \quad \mathbf{K}=\mathbf{X}\mathbf{W}_K,\quad\mathbf{V}=\mathbf{X}\mathbf{W}_V.
\end{equation}
Then, the self-attention is calculated as:
\begin{equation}
	\centering
	\label{eq2}
	\mathbf{A}=\frac{\mathbf{Q}\mathbf{K}^{\mathsf{T}}}{ \sqrt{d_\mathcal{K}}}, \quad Attn(\mathbf{X})=softmax(\mathbf{A})\mathbf{V}.
\end{equation}
Here, $\mathbf{A}$ indicates the attention matrix representing the similarity between queries and keys, $d_\mathcal{K}$ is the dimension of $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$. Extending Equation~(\ref{eq2}) to the multi-head attention is common and straightforward. Afterwards, the output of the self-attention module is fed to a FFN module:
\begin{equation}
	\centering
	\label{eq3}
	\mathbf{ \tilde{X}}=\mathbf{X}+Attn(\mathbf{X}),\quad
	\mathbf{\hat{X}}= \mathbf{W}_2 ReLU(\mathbf{W}_1\mathbf{ \tilde{X}}).
\end{equation}
Here, $ReLU(\cdot)$ stands for the activation function. $\mathbf{W}_2$ and $\mathbf{W}_1$ are the projection matrices.

%When applying classic Transformers to graph data, these methods utilize the self-attention mechanism to encode the correlation (similarity) between nodes but ignore the graph structure. Therefore, 
Graph transformers are proposed for applying Transformers to graph data, which introduces the structural information of graphs as structural encoding (SE) or positional encoding (PE), such as Laplacian PE, spatial encoding, and edge encoding~\citep{DBLP:conf/iclr/DwivediL0BB22,geisler2023transformers,abs_2403_01232,Xing24}. However, these methods exhibit limitations when applied to brain graphs because they do not adapt to the specific small-world characteristic, including the presence of hubs in information propagation and functional modules. %As a consequence, their learned brain graph representations are insufficiently biologically plausible.

\section{Biologically Plausible Brain Graph Transformer}

 
In this section, we present BioBGT in detail. We describe how to enhance the biological plausibility of brain graph representations by focusing on two key aspects: node importance encoding and functional module encoding. First, we design a network entanglement-based node importance encoding method in the input layer, denoted as $\Phi(\cdot)$. Then, to encode functional modules, we present a functional module-aware self-attention, denoted as $\text{FM-}Attn(\cdot)$. Therefore, for each node, we rewrite the left part of Equation~(\ref{eq3}) as: 
\begin{equation}
	\label{eq4}
	\centering
	\tilde{\mathbf{x}}_i=\Phi(\mathbf{x}_i)+\text{FM-}Attn(i).
\end{equation}
Figure~\ref{framework} shows the overall framework of our model. We will introduce the functions of $\Phi(\cdot)$ and $\text{FM-}Attn(\cdot)$ in Section~\ref{structure} and Section~\ref{function}, respectively.

\begin{figure}[!htbp]
	\centering
 \vspace{-1.5\baselineskip} % Reduce space above the figure
	\includegraphics[height=8.2cm]{./fig/framework.pdf}
	\caption{Overall framework of BioBGT.}
 \label{framework}
 \vspace{-\baselineskip}
\end{figure}

\subsection{Network Entanglement-based Node Importance Encoding}
\label{structure}
We measure node importance in information propagation based on network entanglement, importing quantum entanglement into brain graphs. Quantum entanglement is a phenomenon in quantum mechanics, describing the correlations between particles~\citep{yu2023quantum}. Mathematically, quantum entanglement is often represented by a density matrix of quantum entangled states, which captures the entangled relationships between particles in the entire entangled system~\citep{weedbrook2012gaussian}.
When combined with network information theory, concepts from quantum entanglement can provide a powerful lens for analyzing the \textit{global topology} and \textit{information diffusion} of graphs~\citep{huang2024identifying}. Inspired by this, we treat the brain graph as an entangled system, where nodes and their connections reflect interdependent states. The density matrix is used to quantify structural information. This approach enables us to capture the intricate entangled relations between nodes, offering insight into both the global topological features and the information diffusion process within brain graphs. 

%This density matrix captures the entangled relations between nodes, revealing the intricate topological features and information diffusion process of brain graphs.



\begin{proposition}[Density matrix as structural information]\label{proposition 1}
	The structural information of a brain graph $G$, including the connection strength between nodes and the degree distribution of nodes, is encoded by its density matrix, which stands as a normalized information diffusion propagator and formulated as $\rho_G= \frac{e^{-\gamma \mathbf{L}}}{Z}$. Here, $e^{-\gamma \mathbf{L}}$ is the information diffusion propagator, $\gamma$ denotes the positive parameter, $\mathbf{L}$ is the Laplacian matrix of $G$, and $Z=\text{Tr} (e^{-\gamma \mathbf{L}})$ represents the partition function of $G$.
\end{proposition}

Appendix~\ref{proof1} gives the complete proof. In quantum information theory, von Neumann entropy is used to measure the uncertainty or randomness of quantum systems~\citep{huang2024identifying}. It quantifies the degree of entanglement present in quantum systems. When it comes to the complex graph scenario, density matrix-based spectral entropy is considered as the counterpart of von Neumann entropy, capturing the global topology and information diffusion process of graphs~\citep{de2016spectral}. It is formulated as: 
\begin{equation}
	\label{eq5}
	\centering
	\mathcal{S}(G)=-Tr(\rho_G\log_2\rho_G),
\end{equation}
where $\mathcal{S}(G)$ is the density matrix-based spectral entropy of $G$, and $Tr(\cdot)$ indicates the trace operation computing the trace of the product of the density matrix $\rho_G$ and its natural logarithm. The perturbation of a single node on the whole graph can be quantified by the change of density matrix-based spectral entropy, defined as node entanglement (NE)~\citep{huang2024identifying}. We define node importance degree based on the NE value.

\begin{definition}[Node importance degree]\label{definition 1}
The node importance degree of node $i$ is defined as its NE value, formulated as $\mathcal{NE}(i) = \|\mathcal{S}(G_i)-\mathcal{S}(G)\|$, where $G_i$ denotes the $i$-control graph obtained after the perturbation of node $i$. A node with a higher NE value is considered more important and possesses hub attributes, exhibiting a greater disparity between the density matrix-based spectral entropy of the original graph and that of the perturbed graph.
\end{definition}
Notably, NE measures the node importance in terms of the influence of one node on the global topology and information diffusion throughout the graph. Compared to other methods, such as degree centrality (DC), betweenness centrality (BC), closeness centrality (CC), and eigenvector centrality (EC), which emphasize the local structure or local message passing, NE is more reliable for node importance measuring (especially in communication networks like brain graphs). Appendix~\ref{nim} gives a detailed discussion. The next theorem shows the quantification analysis of entanglement.
%For example, the spectral entropy of a hub-control graph could change greatly compared to the original graph, therefore, a hub has a high NE value. The next theorem shows the quantification analysis of NE.

\begin{theorem}[Quantification analysis of entanglement]\label{theorem 1} 
Assume that the number of connected components in the $i$-control graph is the same as the original graph, denoted as $\alpha_i = \alpha$. The NE value of node $i$ is approximated as
\begin{equation}
	\label{eq6}
	\centering
	\mathcal{NE}(i)\approx \left\|      \frac{2m\gamma n^2}{\ln 2 (n-\alpha)^2} \frac{\Delta Z}{ZZ_i} + \log_2(\frac{Z_i}{Z}) \right\|,
\end{equation}
where, $n$ and $m$ are the numbers of nodes and edges, respectively. $Z_i$ stands as the partition function for $G_i$, and $\Delta Z=Z_i-Z$.
\end{theorem}
The complete proof is given in Appendix~\ref{proof2}. Then, we design our node importance encoding $\Phi(\cdot)$ by assigning each node the learnable embedding vector of its node importance degree in the input layer. For node $i$, its node representation in input layer is updated to $\mathbf{x'}_i$:
\begin{equation}
	\label{eq7}
	\centering
	\mathbf{x’}_i=\Phi(\mathbf{x}_i) = \mathbf{x}_i + \mathbf{x}_{\mathcal{NE}(i)}
\end{equation}
$\mathbf{x}_{\mathcal{NE}(i)}$ is the learnable embedding vector specified by $\mathcal{NE}(i)$. %By using the node importance encoding in the input, our model can capture both the global topology and information diffusion in the attention mechanism.

\subsection{Functional Module-Aware Self-Attention}\label{function}

In this section, we first propose a community contrastive strategy-based functional module extractor, which can capture the functional segregation and integration characteristics of the brain. Then, the obtained functional module-aware node representations from the extractor are learned by an updated self-attention mechanism, which can calculate node similarity at the functional module level.

\subsubsection{Community Contrastive Strategy-based Functional Module Extractor}

Given a brain graph $G$, the representation of node $i$ after node importance encoding is $\mathbf{x'}_i$, we then can obtain its updated representation after our functional module extractor $\psi$, indicated as $\mathbf{h}_i:=\psi (i,\mathcal{M}_i)$, where $\mathcal{M}_i$ stands for the functional module node $i$ belongs to. 

In $\psi$, we first utilize an unsupervised community detection method, Louvain algorithm~\citep{blondel2008fast}, to highlight the functional modules. This approach particularly addresses the challenge posed by the absence of functional module labels, which is a limitation encountered in many empirically labeled datasets. Then, we apply graph augmentation to generate two graph views of $G$ by modifying its structural information and enhancing functional modules. Particularly, we apply an edge dropping strategy~\citep{DBLP:conf/iclr/RongHXH20,DBLP:conf/ijcai/ChenZLZ0Z23} to achieve graph augmentation. The main idea of the edge dropping strategy is dropping less important edges while preserving the functional module structure. Details of edge dropping strategy are given in Appendix~\ref{communal}. After graph augmentation, we can obtain two augmented graph views $G^1$ and $G^2$.  

Then, $G^1$ and $G^2$ are fed into a graph neural network-based view encoder $\text{GNN}(\cdot)$ to obtain the representations of two graph views, denoted as $\mathbf{H}^1\sim\text{GNN}(G^1)$ and $\mathbf{H}^2\sim \text{GNN}(G^2)$. To enhance \textit{inter-module differences} and \textit{intra-module similarities}, we design a contrastive objective strategy by setting nodes from the same function module as positive samples, while those from different function modules as negative samples. We adopt the InfoNCE~\citep{oord2018representation} as the contrastive loss function:
\begin{equation}
	\label{eq8}
	\mathcal{L} = - \frac{1}{n}\sum_{i=1}^{n}\log \frac{exp{(Sim(\mathbf{h}_{i}^1,\mathbf{h}_{i}^{pos}))}}{\sum_{j=1}^{n^\text{Neg}} exp{(Sim(\mathbf{h}_{i}^1,\mathbf{h}_{j}^1))}+\sum_{j=1}^{n^\text{Neg}} exp{(Sim(\mathbf{h}_{i}^1,\mathbf{h}_{j}^2))}}.
\end{equation}
Here, $Sim(\cdot)$ is the score function measuring the similarity between two nodes. For an anchor node $i$ in $G^1$, its representation is $\mathbf{h}_i^1$, we consider the nodes within functional module $\mathcal{M}_i$ from both graphs $G^1$ and $G^2$ as the positive samples, denoted as $\mathbf{h}_{i}^{pos}$, otherwise they are considered as negative samples. $n^\text{Neg}$ indicates the number of negative samples in a graph view. Consequently, the updated functional module-aware representation of node $i$ can be obtained, denoted as $\mathbf{h}_i$.

%\citep{hassani2020contrastive}. 

\subsubsection{Updated Self-Attention Mechanism}

After obtaining the functional module-aware node representations, we design an updated self-attention mechanism. %preserving functional segregation and integration characteristics of brain graphs. 
Inspired by ~\cite{mialon2021graphit}, we design the self-attention mechanism as a kernel smoother to capture the similarity between each pair of nodes. Particularly, we define trainable exponential kernels on functional module-aware node representations. The updated self-attention is formulated as:
\begin{equation}
	\label{eq9}
	\centering
	\text{FM-}Attn (i)=\sum_{j\in V}\frac{exp\bigg(\langle\mathbf{W}_Q\mathbf{h}_i, \mathbf{W}_K\mathbf{h}_j\rangle/\sqrt{d_{\mathcal{K}}}\bigg)}{\sum_{u\in V}exp\bigg(\langle\mathbf{W}_Q\mathbf{h}_i, \mathbf{W}_K\mathbf{h}_u\rangle/\sqrt{d_{\mathcal{K}}}\bigg)}f(\mathbf{h}_j).
\end{equation}
Here, $exp\bigg(\langle\mathbf{W}_Q\mathbf{h}_a, \mathbf{W}_K\mathbf{h}_b\rangle/\sqrt{d_{\mathcal{K}}}\bigg)$ is a non-negative kernel, where $\langle\cdot,\cdot\rangle$ indicates the dot product. $f(\cdot)$ is a linear value function.

This updated self-attention mechanism can capture node similarity from the functional module level, without destroying the coherence of functional module-aware node representations. Representations of nodes in the same functional module are closer, while those from different modules keep farther. Therefore, the obtained node representations are more biologically plausible, preserving functional segregation and integration characteristics. The next theorem guarantees that our self-attention function can controllably preserve functional modules.

\begin{theorem}[Controllability analysis of functional module-aware self-attention]\label{theorem 2}
%Lipschitz mapping $f$ with the Lipschitz constant $C$. 
Assume the functional module extractor $\psi$ is bounded by a constant $C_{\psi}$. For any two nodes $a$ and $b$, the distance between their representations after the functional module-aware self-attention is bounded by:
\begin{equation}
\label{eq10}
\centering
\| \text{FM-}Attn(a) - \text{FM-}Attn(b)\| \leq C_\mathcal{M} \| \mathbf{h}_a - \mathbf{h}_b \|.
\end{equation}
$\mathbf{h}_a:=\psi (a,\mathcal{M}_a)$ and $\mathbf{h}_b:=\psi (b,\mathcal{M}_b)$ are representations of nodes $a$ and $b$ after the functional module extractor, respectively. $C_\mathcal{M}$ is a constant.
\end{theorem}
This theorem demonstrates that node representations will maintain their relative distances after undergoing the functional module-aware self-attention mechanism. For example, after the self-attention, two nodes within the same functional module will remain close to each other, while two nodes from different functional modules will remain distant from each other. This is crucial for ensuring that the self-attention mechanism preserves functional modules while capturing the similarity between nodes. The proof of this theorem is provided in Appendix~\ref{proof3}.

\section{Experiments}

\subsection{Experimental Setup}\label{setup}
\textbf{Datasets.}~We conduct experiments on fMRI data collected from three benchmark datasets. (1) Autism Brain Imaging Data Exchange (ABIDE)~\footnote{\url{https://fcon_1000.projects.nitrc.org/indi/abide/}} dataset. This dataset contains resting-state fMRI data of $1,009$ anonymous subjects (age range: 5-64 years old) including $516$ Autism spectrum disorder patients and $493$ normal controls. The ROIs of brain graphs in ABIDE are defined by Craddock 200 atlas~\citep{craddock2012whole}. (2) Alzheimer’s Disease Neuroimaging Initiative (ADNI)~\footnote{\url{https://adni.loni.usc.edu/}} dataset. The collected dataset comprises a total of $407$ subjects, including $190$ normal controls, $170$ mild cognitive impairment (MCI) patients, and $47$ Alzheimer’s disease patients, carefully matched for both age and sex ratio. The ROI definition in ADNI dataset is based on AAL atlas~\citep{tzourio2002automated}. (3) Attention Deficit Hyperactivity Disorder (ADHD-200)~\footnote{\url{https://fcon_1000.projects.nitrc.org/indi/adhd200/}} dataset. This dataset contains $459$ subjects from 7 to 21 years old. $230$ subjects are typically developing individuals and $229$ subjects are ADHD patients. The ROI definition in ADHD-200 dataset is also based on Craddock 200 atlas. The number of ROIs in ABIDE, ADNI, and ADHD-200 datasets are 200, 90, and 190, respectively. Notably, brain graphs are constructed by computing the Pearson correlation coefficient (PCC)~\citep{cohen2009pearson} between ROIs based on the collected fMRI data. In particular, thresholds are set to keep edges with higher PCC values (weights) and drop those with lower PCC values (see Table~\ref{para} in Appdendix~\ref{imple}).



\textbf{Evaluation Metrics.}\label{evaluation}
We evaluate our model on the graph classification task. For ABIDE and ADHD-200 datasets, our model aims to detect whether the subject is a patient or a normal control. Therefore, the classification tasks in these two datasets are binary classification problems. For ADNI dataset, there are three groups, including normal controls, mild cognitive impairment patients, and Alzheimer’s disease patients. Thus, disease detection in ADNI dataset is a multiple classification problem. We use five metrics to evaluate the model performance: (1) Test accuracy (ACC) indicates the ratio of brain graphs that are correctly classified out of all samples; (2) F1 score is the harmonic mean of precision and recall; (3) Area under the receiver operating characteristic curve (AUC) shows the trade-off between true positive rate and false positive rate; (4) Sensitivity (Sen.) refers to true positive rate; (5) Specificity (Spe.) gives the true negative rate. For the multiclass classification task on the ADNI dataset, we use macro averaging for the F1 score, sensitivity, and specificity.
All results are the average values of $10$ random runs on test sets with the standard deviation.


\textbf{Baseline Methods.}~We compare our model with state-of-the-art methods: (1) typical machine learning (ML) methods, including SVM (with a linear kernel) and Random Forest; (2) graph transformer models, including SAN~\citep{kreuzer2021rethinking}, Graph Transformer (Graph Trans.)~\citep{dwivedi2020generalization}, Graphormer~\citep{ying2021transformers}, BRAINNETTF~\citep{kan2022brain}, SAT~\citep{chen2022structure}, Polynormer~\citep{abs_2403_01232}, Gradformer~\citep{DBLP:conf/ijcai/0008YZMP024}, and GTSP~\citep{liu2024exploring}; (3) graph neural networks for brain graph analysis, including GAT~\citep{DBLP:conf/iclr/VelickovicCCRLB18}, BrainGNN~\citep{li2021braingnn}, BrainGB~\citep{cui2022braingb}, MCST-GCN~\citep{zhu2024spatio}, and GroupBNA~\citep{peng2024adaptive}. For the SAT model, we consider its two variants as baselines: SAT without positional encoding (SAT-PE) and SAT with positional encoding (SAT+PE). 

%SIGformer\cite{chen2024sigformer}
%Polynormer\cite{abs_2403_01232}

\textbf{Implementation Details.}~Our model is implemented using PyTorch Geometric v2.0.4 and PyTorch v1.9.1. Model training is performed on an NVIDIA A6000 GPU with 48GB of memory. Our model is trained using the AdamW optimizer~\citep{DBLP:conf/iclr/LoshchilovH19}, and the cross-entropy loss is used for classification tasks. Each dataset is randomly split, with 80\% used for training, 10\% for validation, and 10\% for testing. Full implementation is given in Appendix~\ref{imple}.
%We train our model for 200 epochs, with a learning rate of 1e-3. A weight decay of 1e-5 is applied for regularization. 


\subsection{Results}

The experimental results (ACC and AUC) on the three datasets are summarized in Table~\ref{result}. The results for F1, Sen., and Spe. are provided in Appendix~\ref{b2}. As experimental results show, the overall performance of BioBGT is superior to that of other baselines on all three datasets. For example, in the ADHD-200 dataset, we can see that the performance of BioBGT is distinguished, with the best accuracy and F1 score. Notably, our F1 score is around 4.21\% higher than the second-best baseline. In the ABIDE dataset, BioBGT achieves a 5.76\% improvement in accuracy over the second-best baseline. The experimental results demonstrate that our model excels in various brain disorder detection tasks.

%The experimental results demonstrate that our model excels in brain graph representation with their biological plausibility preserved. This indicates the small-world architecture of brain graphs is a crucial feature of their representations, reflecting on various node importance degrees (the presence of hubs) and functional modules. %A more detailed analysis of experimental results is given in Appendix~\ref{b2}.

\begin{table}[!ht]
	\centering
 %\vspace{-\baselineskip} 
	%\caption{The average F1 score (mean ± margin of error), accuracy (mean ± margin of error).}
	\caption{Results (mean ± margin of error) on three datasets (\%).}
 %\vspace{-\baselineskip}
	\label{result}
	\renewcommand\arraystretch{1}
 \setlength{\tabcolsep}{3pt} % Adjust column separation
 \small
 \resizebox{\textwidth}{!}{
	\begin{tabular}{cccc|cc|cc}
		\toprule
        \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{ADHD-200} &\multicolumn{2}{c|}{ABIDE}&\multicolumn{2}{c}{ADNI} \\\cmidrule{3-8}
    &  & ACC & AUC & ACC &AUC&ACC & AUC \\\midrule
    \multirow{2}{2cm}{ML Methods}
    &SVM& 53.56$\pm$2.73 &54.66$\pm$3.40&49.01$\pm$1.70& 49.05$\pm$1.94&32.29$\pm$2.63&49.88$\pm$3.10\\
    &Random Forest& 58.96$\pm$2.77&59.49$\pm$2.38&51.14$\pm$3.08&51.41$\pm$3.23&\underline{49.03$\pm$1.27}&58.18$\pm$2.31\\\midrule
        
		\multirow{9}{2cm}{Graph Transformer Models}
		&SAN&51.09$\pm$2.00&51.22$\pm$2.21&49.80$\pm$1.97&50.20$\pm$2.34&34.44$\pm$4.61&49.23$\pm$2.67 \\
		&Graph Trans.& 50.76$\pm$2.07&51.49$\pm$1.15& 50.20$\pm$0.50&48.20$\pm$0.16&40.28$\pm$4.17&52.31$\pm$2.04\\
        &Graphormer&61.60$\pm$0.90&58.64 $\pm$1.50&58.40$\pm$0.68&57.61$\pm$0.72&35.64$\pm$2.17&48.19$\pm$12.69\\
		&SAT-PE& 60.00$\pm$2.73&59.68$\pm$2.60& 60.60$\pm$3.11&59.14$\pm$4.56&39.96$\pm$1.51&48.17$\pm$6.57\\
		&SAT+PE&64.44$\pm$3.45&64.21$\pm$3.40&58.76$\pm$4.88 &69.29$\pm$5.48& 41.51$\pm$4.01&42.13$\pm$5.74\\   &BRAINNETTF&\underline{70.80$\pm$2.70}&\textbf{79.36$\pm$3.43}&\underline{68.24$\pm$2.24}&\textbf{78.38$\pm$3.43}& 47.39$\pm$3.11&55.72$\pm$7.13\\
        &Polynormer&64.78$\pm$2.34&63.61$\pm$2.43&57.03$\pm$0.96&56.42$\pm$1.56&41.85$\pm$2.12&54.34$\pm$4.37\\
        &Gradformer&68.94$\pm$3.18 &67.83$\pm$4.66&61.56$\pm$4.13&61.75$\pm$4.29&46.54$\pm$2.72&53.88$\pm$2.37 \\  
        &GTSP & 61.70$\pm$3.81&61.41$\pm$2.90&61.37$\pm$3.59&60.43$\pm$3.47&47.27$\pm$3.81&53.59$\pm$3.26 \\
        \midrule
        
		\multirow{5}{2cm}{Graph Neural Networks}
        &GAT&55.38$\pm$3.18&54.97$\pm$3.28& 53.51$\pm$2.54&53.41$\pm$2.48&34.99$\pm$7.43&51.73$\pm$6.66\\
		&BrainGNN& 55.76$\pm$1.20&58.00$\pm$0.49& 51.34$\pm$1.17&54.27$\pm$0.66&43.33$\pm$4.08&50.21$\pm$2.97\\       &BrainGB&68.20$\pm$7.81&\underline{74.64$\pm$10.10}&65.12$\pm$3.90&70.32$\pm$3.66& 44.34$\pm$3.90&62.24$\pm$4.68\\
		
        &MCST-GCN&59.06$\pm$2.69&59.05$\pm$3.89&54.22$\pm$2.40&55.18$\pm$2.35&48.44$\pm$3.12&\underline{62.25$\pm$2.93}\\      &GroupBNA&69.87$\pm$3.02&71.16$\pm$4.53&63.14$\pm$2.65&71.30$\pm$3.81&46.72$\pm$1.33&50.85$\pm$8.10 \\\midrule
		\multirow{1}{2cm}{Our Model} &\textbf{BioBGT}& \textbf{71.06$\pm$0.08}&71.64$\pm$1.14&\textbf{74.00$\pm$2.01}&\underline{73.33$\pm$2.37}& \textbf{52.08$\pm$2.08}&\textbf{62.33$\pm$5.98}\\
		\bottomrule	
	\end{tabular}}
\end{table}

\subsection{Ablation Studies}
We conduct a series of ablation studies on three datasets to validate the effectiveness of each component in BioBGT. To verify how the network entanglement-based node importance encoding benefits the model performance, we conduct an ablation experiment by removing the node importance encoding, denoted as “-NE”. In addition, to show the effectiveness of our functional module-aware self-attention, we remove the community contrastive strategy-based functional module extractor and replace our $\text{FM-}Attn(\cdot)$ with a normal self-attention $Attn(\cdot)$ (see Equation~(\ref{eq2})), denoted as ``-$\text{FM-}Attn$''. Figure~\ref{figure7} compares the performance of BioBGT with the altered models on three datasets. BioBGT achieves superior performance compared to the model without node importance encoding (-NE), indicating that our node importance encoding method is crucial for the model performance. Furthermore, BioBGT shows better performance than the altered model -$\text{FM-}Attn$. This indicates that it is essential to encode node similarities from the functional module level and preserve functional segregation and integration characteristics of brain graphs.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{fig/adhd_ab.pdf}
        \caption{ADHD-200}
        \label{fig:sub1}
    \end{subfigure}
    \hspace{0.02\textwidth} %
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{fig/abide_ab.pdf}
        \caption{ABIDE}
        \label{fig:sub2}
    \end{subfigure}
    \hspace{0.02\textwidth} % 
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{fig/adni_ab.pdf}
        \caption{ADNI}
        \label{fig:sub3}
    \end{subfigure}
    \caption{Model performance of BioBGT and its altered models.}
    \label{figure7}
\end{figure}

\subsection{Comparative Analysis of Node Importance Measurement}

To validate the effectiveness of NE in measuring node importance, we perform a comparative analysis between BioBGT and its variants, which replace NE-based node importance encoding with (1) Laplacian matrix-based positional encoding, denoted as ``+PE''; (2)degree centrality encoding, denoted as ``+DC''; (3) Laplacian matrix and degree centrality encoding, denoted as ``+PE+DC''; (4) betweenness centrality encoding, denoted as ``+BC''; (5) closeness centrality encoding, denoted as ``+CC''; (6) eigenvector centrality, denoted as ``+EC''. The results are summarized in Table~\ref{abresult1} and Table~\ref{abresult2}~(see Appendix~\ref{b2}). The overall performance of BioBGT is significantly better than other variants. This indicates that our node importance encoding method is crucial for the model performance, suggesting NE is more reliable for node importance measuring. %as well as reflecting structural characteristics and information diffusion within brain graphs.

\begin{table}[!ht]
	\centering
  %\vspace{-\baselineskip} 
	\caption{The results (F1, ACC, AUC) for BioBGT and its variants on three datasets (\%).}
   % \vspace{-\baselineskip} 
	\label{abresult1}
	\renewcommand\arraystretch{1}
 \setlength{\tabcolsep}{2pt} % Adjust column separation
 \scriptsize
		\begin{tabular}{cccccccccc}\toprule
			\multirow{2}{1cm}{}& \multicolumn{3}{c}{ABIDE} &\multicolumn{3}{c}{ADNI} &\multicolumn{3}{c}{ADHD-200}\\ \cline{2-10}
			& F1& ACC&AUC&F1& ACC&AUC& F1& ACC&AUC\\\midrule
			%-NE& 62.56$\pm$7.05& 64.20$\pm$2.28&63.89$\pm$1.56& 29.38$\pm$6.93& 49.43$\pm$3.11&54.85$\pm$2.30& 58.80$\pm$4.98& 64.40$\pm$2.02&64.67$\pm$1.87\\%只要对比
			+PE& 54.00$\pm$2.97& 60.60$\pm$2.25&60.91$\pm$2.05& 30.09$\pm$3.36& 49.43$\pm$2.42&52.14$\pm$2.41& 69.21$\pm$7.14& 67.56$\pm$3.24&67.39$\pm$2.80\\%对比+PE
			+DC& 59.73$\pm$4.23& 61.20$\pm$1.88&61.28$\pm$1.80& 27.27$\pm$1.57& 50.57$\pm$1.64&55.45$\pm$3.59& 74.22$\pm$1.35& 69.78$\pm$2.33&70.18$\pm$2.28\\ %对比+中心
			+PE+DC& 56.64$\pm$2.40& 63.00$\pm$1.63&63.32$\pm$1.55& 27.77$\pm$1.08& 50.94$\pm$1.25&55.08$\pm$3.94& 73.60$\pm$4.28& 70.67$\pm$4.00&70.79$\pm$4.10\\ %对比+pe+中心
         +BC&52.77$\pm$1.30&70.00$\pm$6.12&65.62$\pm$7.29& 26.70$\pm$4.26&48.11$\pm$3.20&51.38$\pm$7.88& 72.20$\pm$3.61& 71.12$\pm$0.88&70.09$\pm$1,05\\ %betweeness
          +CC& 62.43$\pm$1.53&73.75$\pm$6.50&70.84$\pm$7.65& 25.84$\pm$5.50& 48.11$\pm$3.61&50.68$\pm$11.24& 73.78$\pm$5.11& \textbf{72.69$\pm$0.88}&69.38$\pm$0.94\\ %closeness
           +EC& 53.13$\pm$1.62& 71.25$\pm$8.20&66.67$\pm$9.88& 27.75$\pm$9.12& 47.64$\pm$3.08&54.39$\pm$8.41& 73.09$\pm$4.98& 71.11$\pm$1.11&68.99$\pm$0.99\\ \midrule%eigen
           
			\textbf{BioBGT}&\textbf{68.41$\pm$2.19}& \textbf{74.00$\pm$2.01}&\textbf{73.33$\pm$2.37}&\textbf{32.29$\pm$2.31}& \textbf{52.08$\pm$2.08}&\textbf{61.33$\pm$5.98}&\textbf{74.63$\pm$1.18}& 71.06$\pm$0.08&\textbf{71.64$\pm$1.14}\\\bottomrule
	\end{tabular}
% \vspace{-\baselineskip} 
\end{table}

\subsection{Biological Plausibility Analysis}

\begin{wrapfigure}{r}{0.4\textwidth} 
    \centering
    \vspace{-1\baselineskip} 
    \includegraphics[width=0.35\textwidth]{./fig/ABIDE_50_NE_NEff.jpg}  
    \caption{The NE and NEff values of 50 randomly selected nodes from a sample in the ABIDE dataset.}
    \label{Neff1}
    \vspace{-\baselineskip}  
\end{wrapfigure}
We assess the biological plausibility of our node importance encodings and functional module-aware node representations by proving their consistency with existing neuroscience knowledge and providing reasonable explainability.

%\begin{figure}[!htbp]
%	\centering
    %\vspace{-1.5\baselineskip} % Reduce space above the figure
	%\includegraphics[height=4.5cm]{./fig/bio.png}
	%\caption{Biological plausibility analysis.}
  %  \label{figure6}
 %   \vspace{-1.5\baselineskip} % Further reduce space below the caption if needed
%\end{figure}


\textbf{Biological Plausibility in Node Importance Encoding.}~
%This paper defines the node importance degree of each node as its NE value. 
Node efficiency (NEff) is one of the key metrics for complex graphs, which is used to measure the efficiency of information propagation between a given node and all other nodes in a graph~\citep{latora2001efficient}. It can effectively quantify the communication strength of nodes, capturing their ability to facilitate information propagation across the graph. A high NEff value suggests that the node plays a crucial role as a hub for information propagation, enabling close and effective communication with the rest of the graph. Notably, NEff has been widely utilized in the field of neuroscience to identify critical regions in functional brain graphs~\citep{bullmore2009complex}.


Therefore, we compare the NE values of each node with their corresponding NEff values to evaluate the validity of NE as a measure of node importance. Figure~\ref{Neff1} presents the NE and NEff values of 50 randomly selected nodes from a sample randomly chosen in the ABIDE dataset. In addition, visualizations of NE and NEff values for all nodes in a randomly selected graph across three datasets are provided in Appendix~\ref{appendix_ne}. As shown, the changing trend of the NE curve aligns closely with that of the NEff curve, demonstrating that nodes with high NE values also exhibit high NEff values, and vice versa. This consistency suggests that NE aligns with biological plausibility as a measure of node importance in information propagation. To further validate the effectiveness of NE in measuring node importance, we also compare each node's NE value to its average functional connectivity (FC) strength with all of the other nodes (see Appendix~\ref{FCappendix}).



\textbf{Biological Plausibility in Functional Module-Aware Self-Attention.}~
Figure~\ref{figure6} displays the heatmaps of the average self-attention scores from the ADHD-200 test set, output by Graphormer (a), SAT (b), and our functional module-aware self-attention mechanism (c). Based on the empirical labels of brain regions and functional modules~\citep{dosenbach2007distinct,dosenbach2010prediction}, the ROIs are classified into 6 functional modules, including visual cortex (Vis), motion control (MC), cognition control (CC), auditory cortex (Aud), language processing (LP), and executive control (EC). The detailed functional module division is provided in Appendix~\ref{division}. As illustrated in Figure~\ref{figure6}, compared to the heatmaps produced by Graphormer and SAT, the heatmap of our model clearly shows that the learned attention scores of our model align better with the division of functional modules. For example, nodes within the visual cortex exhibit higher attention similarity. This observation verifies that our functional module-aware self-attention preserves the characteristics of functional segregation and integration within brain graph representations better. To further analyze the relationships between the attention patterns learned by our model and the biological patterns of target diseases, we visualize the self-attention scores of samples from each class in the three datasets (see Appendix~\ref{appendix_attention}). 


\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/graphormer-attention.png}
        \caption{Graphormer}
        \label{fig:sub1}
    \end{subfigure}
    \hspace{0.02\textwidth} %
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/SAT-attention.png}
        \caption{SAT}
        \label{fig:sub2}
    \end{subfigure}
    \hspace{0.02\textwidth} % 
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/attention_with_label.png}
        \caption{BioBGT}
        \label{fig:sub3}
    \end{subfigure}
    \caption{The heatmaps of the average self-attention scores. Compared to other methods, heatmap (c) shows that learned attention scores of BioBGT align better with the division of functional modules.}
    \vspace{-1\baselineskip} 
    \label{figure6}
\end{figure}

%Note that since there exists no available functional module labels for the atlas of the ABIDE dataset, we cannot visualize the correlations between attention scores and functional modules.
%\vspace{-1.5\baselineskip} 


\section{Related Work}

\subsection{Brain Graph Analysis}

Brain graphs, reflecting the connections in human neural system, are constructed from various brain health data, such as functional magnetic resonance imaging (fMRI), positron emission tomography (PET), and electroencephalography (EEG)~\citep{bullmore2009complex,bessadok2022graph}. Recently, graph learning-based brain graph analysis has attracted increased attention, dominating a range of tasks (e.g., brain disease detection and treatment recommendation)~\citep{10388338,kan2022brain,liu2023braintgl,ding2023lggnet}. NeuroGraph~\citep{said2024neurograph} collects various brain connectome datasets for benchmarking graph learning models in brain graph analytical tasks (e.g., gender identification). BrainPrint~\citep{wang2020brainprint} develops a network estimation module and a graph analysis module to embed EEG features. 
BrainGNN~\citep{li2021braingnn} contains special ROI-aware graph convolutional layers to capture the functional information of brain networks for fMRI analysis. %STAGIN~\citep{kim2021learning} uses a spatiotemporal attention mechanism in graph neural networks to learn the dynamic brain graph representation, capturing the temporal changes in brain graphs. 
BrainGB~\citep{cui2022braingb} summarizes the pipelines of brain graph construction. BRAINNETTF~\citep{kan2022brain} utilizes a Transformer-based model to analyze brain graphs, while ignoring the structural encoding of ROIs and failing to preserve the small-world architecture of brain graphs. MSE-GCN~\citep{lei2023multi} applies multiple parallel graph convolutional network layers to encode brain structural and functional connectivities to detect early Alzheimer’s disease (AD). GroupBNA~\citep{peng2024adaptive} constructs group-specific brain networks via a group-adaptive brain network augmentation strategy.


\subsection{Graph Transformers}

Graph transformers attempting to generalize Transformer models to graph data have shown significant performance in graph representation tasks~\citep{kong2023goat,luo2024transformers,luo2024fairgt}. SAN~\citep{kreuzer2021rethinking} leverages the full Laplacian spectrum as the learned PE of input nodes, emphasizing the global structural information of the graph. Graphomer~\citep{ying2021transformers} introduces three SE methods to the Transformer architecture, including centrality encoding, spatial encoding and edge encoding, for graph representation learning. SAT~\citep{chen2022structure} proposes a structure-aware self-attention mechanism to extract subgraph representations of nodes. SGFormer~\citep{wu2024simplifying} presents a single-layer attention model utilizing linear complexity to capture global dependencies among nodes. Geoformer~\citep{wang2024geometric} considers atomic environments as the PE of nodes in molecular graphs. EXPHORMER~\citep{shirzad2023exphormer} proposes a sparse attention mechanism based on virtual global nodes and expander graphs for large graph representation learning. GRIT~\citep{ma2023graph} proposes a learned PE based on relative random walk probabilities and a flexible self-attention mechanism aiming to update both node and node-pair representations.

\section{Conclusion}\label{discussions}

This paper presents the Biologically Plausible Brain Graph Transformer (BioBGT) model with a network entanglement-based node importance encoding technique and an updated functional module-aware self-attention mechanism. Extensive experiments on three benchmark datasets demonstrate our BioBGT outperforms state-of-the-art baselines, as well as enhances the biological plausibility of brain graph representations. Significantly, BioBGT offers valuable insights into enhancing the efficacy of brain graph analytical tasks, notably in the realm of improving disease detection. Our work could potentially advance digital health. Importantly, this work contributes to the intersection of neuroscience and artificial intelligence by proposing a brain graph representation learning technique that enhances biological plausibility.

While these findings are encouraging, some limitations remain. Firstly, according to current neuroscience knowledge, the biological properties of the brain are highly complex and remain uncertain, with many underlying mechanisms still requiring further research. Therefore, it is unlikely that a fully biologically plausible brain graph can be constructed. Instead, we can strive to build brain graphs that enhance biological plausibility by incorporating insights from existing knowledge, such as the brain's small-world architecture as one of its key features. Then, the computation complexity of network entanglement and the quadratic complexity of our functional module-aware self-attention module restrict the applicability of BioBGT. Although we have managed to keep the number of parameters comparable to other models, computational efficiency still needs future improvement. Therefore, it is worth exploring how to trade off the biological plausibility of brain graph representations and model computation complexity.


%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.
\newpage

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\newpage
\appendix
\section{Proofs}\label{proofs}

\subsection{Proof of Proposition 1}\label{proof1}

\textbf{Proposition~\ref{proposition 1}}~\textbf{(Density matrix as structural information.)}~The structural information of a brain graph $G$, including the connection strength between nodes and the degree distribution of nodes, is encoded by its density matrix, which stands as a normalized information diffusion propagator and formulated as $\rho_G= \frac{e^{-\gamma \mathbf{L}}}{Z}$. Here, $e^{-\gamma \mathbf{L}}$ is the information diffusion propagator, $\gamma$ denotes the positive parameter, $\mathbf{L}$ is the Laplacian matrix of $G$, and $Z=\text{Tr} (e^{-\gamma \mathbf{L}})$ represents the partition function of $G$.
\begin{proof}
According to the theory of diffusion processes, an information diffusion process in a graph can be represented in exponential form as $e^{-\gamma \mathbf{L}}$~\citep{gasteiger2019diffusion,li2024generalized}. Therefore, the density matrix can be regarded as a diffusion matrix along with structural information. In particular, the Laplacian matrix, $\mathbf{L}= \mathcal{D} -\mathcal{ A}$, indicates the difference between the degree matrix $\mathcal{D}$ and the adjacency matrix $\mathcal{ A}$. The degree matrix $\mathcal{D}$ is a diagonal matrix with its diagonal element $d_{ii}$ representing the degree of nodes $i$. The elements $a_{ij}$ of the adjacency matrix $\mathcal{ A}$ represent the connection strengths between nodes $i$ and $j$. Therefore, the density matrix $\rho_G$ captures the connection strengths and degree distribution information among nodes in the graph $G$, encoding the global structural information of the graph $G$.
\end{proof}

\subsection{Proof of Theorem 1}\label{proof2}
\textbf{Theorem~\ref{theorem 1}}~\textbf{(Quantification analysis of entanglement.)}
Assume that the number of connected components in the $i$-control graph is the same as the original graph, denoted as $\alpha_i = \alpha$. The NE value of node $i$ is approximated as
\begin{equation*}
	\label{eq6}
	\centering
	\mathcal{NE}(i)\approx \left\|      \frac{2m\gamma n^2}{\ln 2 (n-\alpha)^2} \frac{\Delta Z}{ZZ_i} + {\log_2(\frac{Z_i}{Z})} \right\|,
\end{equation*}
where, $n$ and $m$ are the numbers of nodes and edges, respectively. $Z_i$ stands as the partition function for $G_i$, and $\Delta Z=Z_i-Z$.
\begin{proof}
According to Equation~(\ref{eq5}) and spectral decomposition theory, the density matrix-based spectral entropy of graph $G$ is 
\begin{equation*}
	\label{sp}
	\centering
	\begin{split}
	\mathcal{S}(G) &= \mathcal{S}(\rho_G)\\
	&=-Tr(\rho_G\log_2\rho_G)\\
	&=-\sum_{j=1}^{n}\lambda_j(\rho_G)\log_2\lambda_j(\rho_G),\\
		\end{split}
\end{equation*}
where $\lambda_j(\rho_G) = \frac{e^{-\gamma \lambda_j(\mathbf{L})}}{Z}$. Therefore, we have
\begin{equation*}
		\label{sp2}
	\centering
	\begin{split}
		\mathcal{S}(G) &=\frac{\gamma}{\ln2}\sum_{j=1}^{n}\lambda_j(\mathbf{L})\lambda_j(\rho_G)+\log_2Z \\
		&=\frac{\gamma}{\ln2}\sum_{j=1}^{n}\langle\lambda_j(\mathbf{L})\lambda_j(\rho_G)\rangle+\log_2Z\\
		 &\approx \frac{\gamma}{\ln2}\sum_{j=1}^{n}\langle\lambda_j(\mathbf{L})\rangle\langle\lambda_j(\rho_G)\rangle+\log_2Z.
	\end{split}
\end{equation*}
The approximation comes from mean-field approximation. 
Assume the number of $\lambda_j$ satisfying $\lambda_j(\mathbf{L})=0$ is the same as the number of connected components $\alpha$. Refer to ~\cite{huang2024identifying}, we can get
\begin{equation*}
	\label{sp3}
	\centering
	\begin{split}
	\langle\lambda_j(\mathbf{L})\rangle&=\frac{1}{n}\sum_{j=1}^{n}\lambda_j(\mathbf{L})\\
	&=\frac{1}{n-\alpha}\sum_{j=\alpha+1}^{n}\lambda_j(\mathbf{L})\\
	&=\frac{2m}{n-\alpha}.
		\end{split}
\end{equation*}
Likewise,
\begin{equation*}
	\label{sp4}
	\centering
	\begin{split}
		\langle\lambda_j(\rho_G)\rangle&=\frac{1}{n}\sum_{j=1}^{n}\frac{e^{-\gamma \lambda_j(\mathbf{L})}}{Z}\\
		&=\frac{1}{n-\alpha}\sum_{j=\alpha+1}^{n}\frac{e^{-\gamma \lambda_j(\mathbf{L})}}{Z}\\
		&=\frac{1}{n-\alpha}(1-\frac{\alpha}{Z}).
	\end{split}
\end{equation*}
Therefore, $\mathcal{S}(G)$ can be approximated as
\begin{equation*}
	\label{sp5}
	\centering
	\begin{split}
		\mathcal{S}(G)&\approx \frac{\gamma}{\ln2}\frac{2mn}{n-\alpha}\cdot \frac{n}{n-\alpha}(1-\frac{\alpha}{Z})+\log_2Z\\
		&=\frac{2m\gamma n^2}{\ln 2(n-\alpha)^2}(1-\frac{\alpha}{Z})+\log_2Z.
	\end{split}
\end{equation*}
Then, we can quantify the perturbation of node $i$ on graph $G$ by capturing the changes of density matrix-based spectral entropy from $\mathcal{S}(G)$ to $\mathcal{S}(G_i)$, obtaining $\mathcal{NE}(i)$
\begin{equation*}
	\label{sp6}
	\centering
	\begin{split}
		\mathcal{NE}(i)&=\|\mathcal{S}(G_i)-\mathcal{S}(G)\|\\
		&\approx\left\|\left(\frac{2m\gamma n^2}{\ln 2(n-\alpha_i)^2}(1-\frac{\alpha_i}{Z_i})+\log_2Z_i\right)-\left(\frac{2m\gamma n^2}{\ln 2(n-\alpha)^2}(1-\frac{\alpha}{Z})+\log_2Z\right)\right\|.
	\end{split}
\end{equation*}
Suppose $\alpha_i=\alpha$, we can get
\begin{equation*}
	\label{sp7}
	\centering
	\begin{split}
		\mathcal{NE}(i)&\approx\left\|\left(\frac{2m\gamma n^2}{\ln 2(n-\alpha)^2}(1-\frac{\alpha}{Z_i})+\log_2Z_i\right)-\left(\frac{2m\gamma n^2}{\ln 2(n-\alpha)^2}(1-\frac{\alpha}{Z})+\log_2Z\right)\right\|\\
		&=\left\|      \frac{2m\gamma n^2}{\ln 2 (n-\alpha)^2} (\frac{1}{Z}-\frac{1}{Z_i}) + \log_2(\frac{Z_i}{Z}) \right\|\\
		&=\left\|  \frac{2m\gamma n^2}{\ln 2 (n-\alpha)^2} \frac{\Delta Z}{ZZ_i} + \log_2(\frac{Z_i}{Z}) \right\|
	\end{split}
\end{equation*}
Here, $\Delta Z=Z_i-Z$. 
\end{proof}

\subsection{Proof of Theorem 2}\label{proof3}
\textbf{Theorem~\ref{theorem 2}}~\textbf{(Controllability analysis of functional module-aware self-attention.)}
Assume the functional module extractor $\psi$ is bounded by a constant $C_{\psi}$. For any two nodes $a$ and $b$, the distance between their representations after the functional module-aware self-attention is bounded by:
\begin{equation*}
	\label{fa}
	\centering
	\| \text{FM-}Attn(a) - \text{FM-}Attn(b)\| \leq C_\mathcal{M} \| \mathbf{h}_a - \mathbf{h}_b \|.
\end{equation*}
$\mathbf{h}_a:=\psi (a,\mathcal{M}_a)$ and $\mathbf{h}_b:=\psi (b,\mathcal{M}_b)$ are representations of nodes $a$ and $b$ after functional module extractor, respectively. $C_\mathcal{M}$ is a constant.

%Lipschitz mapping $f$ with the Lipschitz constant $C$. functional module extractor $\psi$ is bounded by a constant $C_{\psi}$. For any pair of nodes $a$ and $b$, the distance between their representations after the functional module-aware self-attention is bounded by

\begin{proof}
 The similarity representations of nodes $a$ and $b$ can be denoted as
\begin{equation*}
		\label{fa1}
		\centering
	\begin{split}
	\mathbf{z}_a&= (\left\langle \mathbf{W}_Q \mathbf{h}_a, \mathbf{W}_K \mathbf{h}_i \right\rangle)_{i \in V} \in \mathbb{R}^n,\\
	\mathbf{z}_b&= (\left\langle \mathbf{W}_Q \mathbf{h}_b, \mathbf{W}_K \mathbf{h}_i \right\rangle)_{i \in V} \in \mathbb{R}^n.
	\end{split}
\end{equation*}
Then, the $softmax(\mathbf{z})$ with the $k$-th coefficient is
\begin{equation*}
\label{12}
\centering
softmax(\mathbf{z})_k = \frac{\exp(\mathbf{z}_k/ \sqrt{d_\mathcal{K}})}{\sum_{j=1}^{n} \exp(\mathbf{z}_j / \sqrt{d_\mathcal{K}})}.
\end{equation*}
Afterwards, we can get
\begin{equation*}
	\label{key1}
	\centering
	\begin{split}
	\left\| \text{FM-}\text{Attn}(a) - \text{FM-}\text{Attn}(b) \right\| &= \left\| \sum_{i \in V} softmax(\mathbf{z}_a)_i f(\mathbf{h}_i) - \sum_{i \in V} softmax(\mathbf{z}_b)_i f(\mathbf{h}_i) \right\|\\
	&=\left\| \sum_{i\in V} \left(  softmax(\mathbf{z}_a)_i - softmax(\mathbf{z}_b)_i  \right ) f(\mathbf{h}_i)\right\| \\
	&\leq \left\| softmax(\mathbf{z}_a)-softmax(\mathbf{z}_b)  \right\|\sqrt{ \sum_{i \in V} \| f(\mathbf{h}_i)\|^2   } \\
	&\leq \frac{1}{\sqrt{d_\mathcal{K}}}\|   \mathbf{z}_a -\mathbf{z}_b  \| \sqrt{n} C.
\end{split}
\end{equation*}
The first inequality is based on a simple matrix norm inequality, and the second inequality is based on the fact that $softmax$ function is $\frac{1}{\sqrt{d_\mathcal{K}}}$-Lipschitz~\citep{gao2017properties}. $C$ is the Lipschitz constant. Then, we can infer that
\begin{equation*}
	\label{key11}
	\centering
	\begin{split}
	\| \mathbf{z}_a -\mathbf{z}_b  \|^2&=\sum_{i \in V}\Bigl(\left\langle \mathbf{W}_Q \mathbf{h}_a, \mathbf{W}_K \mathbf{h}_i \right\rangle- \left\langle \mathbf{W}_Q \mathbf{h}_b, \mathbf{W}_K \mathbf{h}_i \right\rangle\Bigr)^2\\
	&=\sum_{i \in V}\Bigl(    \left\langle \mathbf{W}_Q (\mathbf{h}_a-\mathbf{h}_b), \mathbf{W}_K \mathbf{h}_i \right\rangle     \Bigr)^2\\
	&\leq \sum_{i \in V} \Bigl(   \|   \mathbf{W}_Q(\mathbf{h}_a-\mathbf{h}_b)\|^2    \| \mathbf{W}_K  \mathbf{h}_i \|^2      \Bigr)\\
	&\leq \|\mathbf{W}_Q\|^2_{\infty}\|\mathbf{h}_a-\mathbf{h}_b\|^2 C_{\psi}^2 \|\mathbf{W}_K\|^2_{\infty}\\
	&=C_{\psi}^2 \|\mathbf{W}_Q\|^2_{\infty}  \|\mathbf{W}_K\|^2_{\infty} \|\mathbf{h}_a-\mathbf{h}_b\|^2.
	\end{split}
\end{equation*}
The first inequality comes from the Cauchy-Schwarz inequality. The second inequality uses the definition of spectral norm, and constant $C_{\psi}$ is the bound of the functional module extractor $\psi$. Therefore, we can infer that
\begin{equation*}
	\label{key2}
	\centering
	\begin{split}
		\left\| \text{FM-}\text{Attn}(a) - \text{FM-}\text{Attn}(b) \right\|&\leq \frac{1}{\sqrt{d_\mathcal{K}}}\|   \mathbf{z}_a -\mathbf{z}_b  \| \sqrt{n} C.\\
		&\leq \sqrt{\frac{n}{d_\mathcal{K}}} C C_{\psi}\|\mathbf{W}_Q\|_{\infty}  \|\mathbf{W}_K\|_{\infty} \|\mathbf{h}_a-\mathbf{h}_b\|.
	\end{split}
\end{equation*}
A constant $C_\mathcal{M}$ can be defined as
\begin{equation*}
	\label{key222}
	\centering
	\begin{split}
	C_\mathcal{M}=\sqrt{\frac{n}{d_\mathcal{K}}} C C_{\psi}\|\mathbf{W}_Q\|_{\infty}  \|\mathbf{W}_K\|_{\infty}.
	\end{split}
\end{equation*}
Consequently, we can obtain the inequality in the Theorem~\ref{theorem 2}.

\begin{equation*}
	\label{xx}
	\centering
	\| \text{FM-}Attn(a) - \text{FM-}Attn(b)\| \leq C_\mathcal{M} \| \mathbf{h}_a - \mathbf{h}_b \|.
\end{equation*}

\end{proof}

\section{Reliability of NE for Node Importance Measuring}\label{nim}

Given a node $i$, its importance degree in the information propagation across the graph is defined as its NE value, which is obtained by measuring the disparity between the density matrix-based spectral entropy of the original graph and that of the $i$-control graph. 
The density matrix-based spectral entropy captures both the global topology and information diffusion process of the graph~\citep{huang2024identifying}. Therefore, NE can fully reflect the significance of a node on the information propagation across the graph. For example, if the density matrix-based spectral entropy of the $i$-control graph changes greatly compared to the original graph, it indicates that induced perturbation of node $i$ can lead to significant changes in the global topology and information diffusion pattern of the graph. This means node $i$ plays an important role in the graph in terms of information propagation. Particularly, brain graphs are communication networks, in which information propagation is a crucial aspect~\citep{seguin2023brain}. Hence, it is essential to measure node importance based on NE reflecting on the changes in the global topology and information diffusion of brain graphs.

\begin{figure}[!htbp]
	\centering
	\includegraphics[height=4cm]{./fig/dcne.png}
	\caption{An example of the fragility of DC in measuring node importance in brain graphs. Highlighted edges represent paths of information diffusion.}
    \label{figure3}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[height=3cm]{./fig/vis.pdf}
	\caption{Visualization of the DC (a), BC (b), CC (c), EC (d), and NE (e) values of nodes in a brain graph from the ABIDE dataset. The colors of nodes are proportional to the normalized values of the DC and NE.}
    \label{figure4}
\end{figure}

Due to the capability of NE to reflect the global topology and information diffusion, it is more reliable for node importance measuring in brain graphs compared to other methods, such as centralities~\citep{das2018study}. Four representative centralities, including degree centrality (DC), closeness centrality (CC), betweenness centrality (BC), and eigenvector centrality (EC), emphasize the local structure or local message passing, having limitations to fully capture both global topology and information diffusion process. Therefore, they are not reliable for measuring node importance, especially for brain graphs. For example, Figure~\ref{figure3} shows the fragility of DC in measuring node importance in brain graphs. Node $a$ plays a more important role in information propagation than node $b$, while its DC value (=4) is lower than that of node $b$ (=6). We also visualize the DC, BC, CC, EC, and NE values of nodes in a real brain graph from the ABIDE dataset~\footnote{\url{https://fcon_1000.projects.nitrc.org/indi/abide/}} (see Figure~\ref{figure4}). As shown in Figure~\ref{figure4}, NE can better distinguish node importance in the entire brain graph. In contrast, other methods may mistakenly identify marginal nodes with many direct connections but less significance in information propagation as important nodes. For example, especially in Figure~\ref{figure4} (c) and (d), many nodes exhibit disproportionately high CC and EC values, highlighting their inability to effectively differentiate between truly important and less significant nodes in information propagation.


\section{Edge Dropping Strategy}\label{communal}

Following ~\cite{DBLP:conf/ijcai/ChenZLZ0Z23}, we apply the edge dropping strategy to achieve graph augmentation. The main idea of our edge dropping strategy is dropping less important edges while preserving the functional module structure. Particularly, our edge dropping strategy is based on inter-modular edges first rationale. That is inter-modular edges are less important than intra-module edges. We define a scoring function $IM(\cdot)$ to calculate the importance of each edge, it must meet the following condition:
\begin{equation}
	\label{ed1}
	\centering
	IM(e_{intra})>IM(e_{inter}).
\end{equation}
Here, $e_{intra}$ and $e_{inter}$ are intra-module and inter-modular edges, respectively. If nodes $i$ and $j$ are from the same functional module, the scoring function of their edge $e_{ij}$ is:
\begin{equation}
	\label{ed2}
	\centering
	IM(e_{ij})= \omega_{e_{ij}}+max(\omega),
\end{equation}
where $\omega_{e_{ij}}$ is the weight of edge $e_{ij}$, and $max(\omega)$ indicates the highest edge weight in the graph. On the other hand, if nodes $i$ and $j$ are from different functional modules,  the scoring function of their edge $e_{ij}$ is:
\begin{equation}
	\label{ed2}
	\centering
	IM(e_{ij})= \omega_{e_{ij}}-max(\omega).
\end{equation}
Consequently, the scoring function of inter-module edges will keep lower than that of intra-module edges. Our edge dropping strategy will first consider dropping inter-module edges with lower importance scores. Therefore, the functional module structure can be preserved in two augmented graphs.

\section{Experimental Details and Additional Results}

\subsection{Implementation Details}\label{imple} 
The detailed hyperparameter settings for training BioBGT on three datasets are summarized in Table~\ref{seting}.
 \begin{table}[!ht]
	\centering
	%\caption{The average F1 score (mean ± margin of error), accuracy (mean ± margin of error).}
	\caption{Hyperparameters for training BioBGT on three different datasets.}
	\label{seting}
	\renewcommand\arraystretch{0.8}
	\begin{tabular}{c|ccc}
		\toprule
		Hyperparameter& ABIDE& ADNI&ADHD-200\\\midrule
        % \#Parameter&465k&455k&454k\\
        \#Layers&3&3&3\\
		\#Attention heads&8&8&8\\
        Threshold of edge weight&0.3&0&0\\
        Hidden dimensions&128&128&128\\
        FFN hidden dimensions&256&256&256\\
        Dropout&0.5&0.1&0.1\\
        Readout method&mean&mean&mean\\
        Learning rate&3e-4&3e-4&3e-4\\
        Batch size&128&128&128\\
        \#Epochs&200&200&200\\
        Weight decay&1e-4&1e-4&1e-4\\
        Warm-up Steps&10&10&10\\
		\bottomrule	
	\end{tabular}
\end{table}

 \begin{table}[!ht]
	\centering
	%\caption{The average F1 score (mean ± margin of error), accuracy (mean ± margin of error).}
	\caption{The number of parameters for different models on three datasets.}
	\label{para}
	\renewcommand\arraystretch{0.8}
	\begin{tabular}{c|ccc}
		\toprule
		Method& ABIDE& ADNI&ADHD-200\\\midrule
       Polynormer&9.78M&9.80M&9.83M\\
       Random Forest&21K&18K&9K\\
       Gradformer& 455K&446K&468K\\
       GTSP &427K &424K&430K \\
       BioBGT&$465$K & $455$K & $454$K\\
		\bottomrule	
	\end{tabular}
\end{table}

\textbf{Number of parameters and computation time.}
The number of parameters for BioBGT are $465$K, $455$K, and $454$K on ABIDE, ADNI, and ADHD-200, respectively. The running time for training BioBGT on ABIDE, ADNI, and ADHD-200 is 650 s/epoch, 265 s/epoch, and 251 s/epoch, respectively. Table~\ref{para} presents the number of parameters for different models on three datasets.

%BNT 4.0M on ABIDE 



\subsection{Additional Experimental Results}\label{b2}

Table~\ref{resultADHD-200}, Table~\ref{resultABIDE}, and Table~\ref{resultADNI} give the additional results (F1, Sen., Spe.) for BioBGT compared to state-of-the-art methods on ADHD-200, ABIDE, and ADNI datasets, respectively. %BioBGT shows the highest accuracy having 4.69\% improvement compared to the second-best model. 
The overall experimental result reveals that BioBGT outperforms other methods, suggesting its superiority in various brain graph analysis tasks. Please note that due to the randomness of experimental results, the reproduced results of some baselines may be a little different from those in the original papers. Table~\ref{abresult2} summarizes the results of sensitivity and specificity for BioBGT and its variants on three datasets. 

\begin{table}[!ht]
	\centering
 %\vspace{-\baselineskip} 
	%\caption{The average F1 score (mean ± margin of error), accuracy (mean ± margin of error).}
	\caption{Additional results on ADHD-200 (\%).}
 %\vspace{-\baselineskip}
	\label{resultADHD-200}
	\renewcommand\arraystretch{1}
 \setlength{\tabcolsep}{3pt} % Adjust column separation
 \small
	\begin{tabular}{ccccc}
		\toprule
		\multicolumn{2}{c}{Method}& F1 &Sen.&Spe.\\\midrule

         \multirow{2}{2cm}{ML Methods}
    &SVM&58.91$\pm$5.47&68.75$\pm$16.11&40.58$\pm$18.58\\
    &Random Forest&57.07$\pm$4.13&55.01$\pm$9.30&63.95$\pm$8.04\\\midrule
    
		\multirow{9}{2cm}{Graph Transformer Models}
		&SAN&48.19$\pm$9.76 &50.43$\pm$19.32&51.74$\pm$20.16\\
		&Graph Trans.&55.58$\pm$4.18&62.39$\pm$9.43&39.13$\pm$10.74\\
        &Graphormer&69.25$\pm$3.05&83.34$\pm$2.90&33.96$\pm$6.10\\
		&SAT-PE&65.37$\pm$1.61 &73.91$\pm$3.73&45.45$\pm$13.20\\
		&SAT+PE&68.30$\pm$3.83 &\underline{75.65$\pm$7.40}&52.73$\pm$10.67\\
        &BRAINNETTF&70.42$\pm$3.06&66.69$\pm$4.93&75.75$\pm$4.63\\
        &Polynormer&58.62$\pm$12.93&51.57$\pm$17.68&69.63$\pm$11.59\\
        & Gradformer&69.01$\pm$5.39&71.66$\pm$9.37&63.99$\pm$12.97\\
        &GTSP &64.59$\pm$7.29 &75.60$\pm$10.47 &47.22$\pm$14.84\\
        \midrule
        
		\multirow{5}{2cm}{Graph Neural Networks}
        &GAT&57.85$\pm$5.87&64.54$\pm$13.97&45.10$\pm$18.34\\
		&BrainGNN&55.89$\pm$1.21&56.09$\pm$2.13&55.43$\pm$3.11\\
		&BrainGB&63.73$\pm$11.93 &58.26$\pm$15.82&\underline{78.20$\pm$8.19}\\
		
        &MCST-GCN&54.76$\pm$1.20&30.42$\pm$2.17&68.05$\pm$1.79\\
        &GroupBNA&\underline{70.61$\pm$2.35} &74.05$\pm$4.60&72.93$\pm$6.49\\\midrule
		\multirow{1}{2cm}{Our Model}&\textbf{BioBGT}&\textbf{74.63$\pm$1.18}&\textbf{89.39$\pm$5.66}&\textbf{84.07$\pm$2.19}\\
		\bottomrule	
	\end{tabular}
\end{table}


 \begin{table}[!ht]
	\centering
	\caption{Additional results on ABIDE (\%).}
	\renewcommand\arraystretch{1}
    \setlength{\tabcolsep}{3pt} % Adjust column separation
    \small
	\begin{tabular}{ccccccc}
		\toprule
		\multicolumn{2}{c}{Method}& F1&Sen.&Spe.\\\midrule

         \multirow{2}{2cm}{ML Methods}
    &SVM&52.02$\pm$2.16&54.41$\pm$5.44&43.69$\pm$1.90 \\
    &Random Forest&51.46$\pm$3.87&51.23$\pm$7.82&51.58$\pm$5.75\\\midrule
    
		\multirow{9}{2cm}{Graph Transformer Models}
		&SAN&47.52$\pm$3.74&44.95$\pm$6.53&54.85$\pm$7.48\\
		&Graph Trans.&57.98$\pm$0.52&67.38$\pm$0.78&32.32$\pm$0.45\\
        &Graphormer&64.17$\pm$2.89 &\textbf{70.64$\pm$8.74}&46.49$\pm$12.51\\
		&SAT-PE&67.21$\pm$3.84 &64.76$\pm$8.62&53.52$\pm$15.34\\
		&SAT+PE&\underline{68.33$\pm$2.88} &69.29$\pm$5.48&43.24$\pm$18.03\\  &BRAINNETTF&68.20$\pm$2.31&69.39$\pm$5.22&66.95$\pm$5.28\\
        &Polynormer&44.20$\pm$11.13&38.16$\pm$17.04&\underline{74.68$\pm$15.46}\\
        & Gradformer&62.12$\pm$3.37&59.16$\pm$4.77&64.35$\pm$10.99\\
        &GTSP&65.36$\pm$6.22&69.86$\pm$11.57&50.99$\pm$12.59\\
        \midrule
        
		\multirow{5}{1.8cm}{Graph Neural Networks}
        &GAT&56.12$\pm$6.25&57.73$\pm$12.96&48.13$\pm$12.44\\
		&BrainGNN&50.09$\pm$1.55&54.85$\pm$4.12&47.96$\pm$2.92\\	&BrainGB&66.95$\pm$5.08&67.01$\pm$10.00&60.07$\pm$8.53\\
		
         &MCST-GCN&52.63$\pm$6.33&48.80$\pm$6.88&45.85$\pm$10.19\\
          &GroupBNA&61.64$\pm$1.20 &64.72$\pm$5.43&65.28$\pm$6.27\\\midrule
		\multirow{1}{2cm}{Our Model}&\textbf{BioBGT}&\textbf{68.41$\pm$2.19} &\underline{70.00$\pm$6.25}&\textbf{76.67$\pm$9.77}\\
		\bottomrule	
	\end{tabular}
 \label{resultABIDE}
\end{table}

 \begin{table*}[!ht]
	\centering
	\caption{Additional results on ADNI (\%).}
	\label{resultADNI}
	\renewcommand\arraystretch{1}
 \setlength{\tabcolsep}{3pt} % Adjust column separation
 \small
	\begin{tabular}{ccccc}
		\toprule
		\multicolumn{2}{c}{Method}& F1&Sen.&Spe.\\\midrule

         \multirow{2}{2cm}{ML Methods}
    &SVM&26.69$\pm$4.56&27.33$\pm$5.12&74.91$\pm$0.63 \\
    &Random Forest&29.18$\pm$2.13&32.33$\pm$1.11&79.03$\pm$0.34\\\midrule
    
		\multirow{9}{2cm}{Graph Transformer Models}
		&SAN&14.73$\pm$3.01&23.24$\pm$2.65&74.30$\pm$1.41\\
		&Graph Trans.&20.96$\pm$1.51 &26.65$\pm$2.24&76.14$\pm$1.22\\		&Graphormer&21.63$\pm$5.85&25.72$\pm$4.97&75.78$\pm$3.47\\
		&SAT-PE&24.08$\pm$4.18&30.67$\pm$4.19&76.16$\pm$0.29\\
		&SAT+PE&19.66$\pm$5.13&26.72$\pm$3.46&76.16$\pm$2.15\\
        &BRAINNETTF&35.64$\pm$3.97&35.25$\pm$3.29&\underline{80.03$\pm$4.43}\\  
        &Polynormer&16.75$\pm$3.05&25.72$\pm$0.92&75.47$\pm$0.60\\
        &Gradformer&23.80$\pm$2.61&29.90$\pm$1.92&78.18$\pm$1.24\\
        &GTSP&25.48$\pm$4.14&30.39$\pm$1.91&78.26$\pm$1.00\\
        \midrule
        
		\multirow{5}{2cm}{Graph Neural Networks}
        &GAT&25.70$\pm$3.54&30.92$\pm$4.01&77.72$\pm$1.40\\
		&BrainGNN&23.23$\pm$4.67&29.52$\pm$3.31&77.47$\pm$1.80\\
		&BrainGB&32.22$\pm$7.96&34.04$\pm$6.48&78.78$\pm$1.54\\
		  
        &MCST-GCN&\textbf{37.44$\pm$3.12}&\textbf{38.06$\pm$2.99}&70.52$\pm$2.71\\       
        &GroupBNA&\underline{35.85$\pm$1.38} &34.98$\pm$9.72&78.25$\pm$6.78\\\midrule
       \multirow{1}{2cm}{Our Model}&\textbf{BioBGT}&32.29$\pm$2.31&\underline{35.65$\pm$1.43}&\textbf{80.39$\pm$0.82}\\
		\bottomrule	
	\end{tabular}
\end{table*}

\begin{table*}[!ht]
	\centering
	\caption{The results (Sen. and Spe.) for BioBGT and its variants on three datasets (\%).}
	\label{abresult2}
	\renewcommand\arraystretch{1}
 \setlength{\tabcolsep}{2.5pt} % Adjust column separation
 \small
	\setlength{\tabcolsep}{2mm}{
		\begin{tabular}{ccccccc}\toprule
			\multirow{2}{1cm}{}& \multicolumn{2}{c}{ABIDE} &\multicolumn{2}{c}{ ADNI} &\multicolumn{2}{c}{ADHD-200}\\ \cline{2-7}
			& Sen.& Spe.& Sen.& Spe& Sen.& Spe\\\midrule
			%-NE& 58.06$\pm$12.08& 68.72$\pm$11.90& 33.53$\pm$4.81& 79.58$\pm$1.73& 50.91$\pm$8.77& 78.42$\pm$5.22\\
			+PE& 45.49$\pm$4.36&76.33$\pm$7.05& 34.50$\pm$2.36& 79.44$\pm$1.15&74.78$\pm$18.66& 60.00$\pm$12.80\\
			+DC& 57.26$\pm$9.11& 65.31$\pm$9.32& 32.68$\pm$1.05& 79.69$\pm$0.73& 88.42$\pm$9.08& 51.94$\pm$12.25\\
			+PE+DC& 47.45$\pm$2.84&79.18$\pm$2.28& 32.99$\pm$0.88&79.82$\pm$0.73& 82.53$\pm$12.77& 59.05$\pm$10.57\\
            +BC& 43.75$\pm$13.98&87.50$\pm$4.17 &31.53$\pm$2.66& 78.71$\pm$1.36& 77.12$\pm$13.78&74.75$\pm$7.46\\ %betweeness
          +CC& 56.25$\pm$11.92&85.41$\pm$3.61&31.50$\pm$3.20& 78.84$\pm$1.80& 77.17$\pm$15.56&71.59$\pm$9.74\\ %closeness
           +EC& 43.75$\pm$8.30&\textbf{89.58$\pm$3.61}&33.99$\pm$7.13& 78.99$\pm$1.27& 81.52$\pm$18.57&70.46$\pm$8.37\\ \midrule
			\textbf{BioBGT}& \textbf{70.00$\pm$6.25}&76.67$\pm$9.77& \textbf{35.65$\pm$1.43}&\textbf{80.39$\pm$0.82}& \textbf{89.39$\pm$5.66}& \textbf{84.07$\pm$2.19}\\\bottomrule
	\end{tabular}}
\end{table*}

\subsection{Additional NE and NEff Curves for Three Datasets}\label{appendix_ne}

We provide visualizations of NE and NEff values for all nodes in brain graphs across three datasets. The graphs from the ABIDE, ADHD-200, and ADNI datasets contain 200, 190, and 90 nodes, respectively. Figure~\ref{fig:ABIDE_NE_NEffi} provides the curves for all 200 nodes from a randomly selected sample in the ABIDE dataset. Figure~\ref{fig:ADHD_NE_NEff} shows the curves for all 190 nodes from a randomly selected sample in the ADHD-200 dataset. Figure~\ref{fig:ADNI_NE_NEffi} shows the curves for all 90 nodes from a randomly selected sample in the ADNI dataset.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/ABIDE_NE_NEffi.png}
    \caption{The NE and NEff values of 200 nodes from a randomly selected sample in the ABIDE dataset.}
    \label{fig:ABIDE_NE_NEffi}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/ADHD_NE_NEff.png}
    \caption{The NE and NEff values of 190 nodes from a randomly selected sample in the ADHD-200 dataset.}
    \label{fig:ADHD_NE_NEff}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{fig/ADNI_NE_NEffi.png}
    \caption{The NE and NEff values of 90 nodes from a randomly selected sample in the ADNI dataset.}
    \label{fig:ADNI_NE_NEffi}
\end{figure}




\subsection{NE and FC Strength Curves for Three Datasets}\label{FCappendix}

%Pearson correlation coefficient (PCC) value between each node pair indicates the degree of their influence on each other~\citep{hou2022distance,xi2023identifying}. PCC has been widely used to approximate brain functional connectivity strength in neuroscience~\citep{li2021braingnn}. Existing studies have shown that PCC-based brain graph structures exhibit greater biological reliability than those measured by other metrics~\citep{liang2012effects}. They claim that PCC is a suitable choice for measuring the global topological properties of functional brain graphs. A node's average PCC value can indicate its communication strength with other nodes, which can be interpreted as an aspect of its importance in information propagation.

%Therefore, we compare the NE value of each node to its average PCC value with all of the other nodes. Figure~\ref{bio1} shows the NE and average PCC values of $50$ randomly selected nodes from a randomly selected sample in the ABIDE dataset. {\color{blue}In addition, visualizations of NE and average PCC values for all nodes in a randomly selected graph across three datasets are provided in Appdendix~\ref{appendix_nepcc}.} We can see that the changing trend of the NE curve is almost consistent with that of the PCC curve, proving that nodes with large NE values also have large PCC values, and vice versa. This consistency suggests that NE could be a biologically plausible measure of node importance.


To further validate the effectiveness of NE in measuring node importance, we compare each node's NE value to its average functional connectivity (FC) strength with all of the other nodes. FC strength is widely regarded as a suitable metric for capturing the global topological properties of functional brain graphs~\citep{liang2012effects}. Notably, FC strength is commonly quantified using the Pearson correlation coefficient (PCC) between nodes, a standard measure in brain graph analysis~\citep{li2021braingnn}. A node's average FC strength (average PCC value) reflects its communication strength with other nodes, providing an additional perspective on its importance in information propagation. Figures~\ref{fig:ABIDE_AllNode_NE&PCC}, \ref{fig:ADHD_AllNode_NE&PCC}, and \ref{fig:ADNI_AllNode_NE&PCC} illustrate the NE and FC strength curves for all nodes in randomly selected samples from the ABIDE, ADHD-200, and ADNI datasets, respectively. The results reveal an obvious alignment between the trends of the NE and FC strength curves, indicating that nodes with high NE values also exhibit high FC strength, and vice versa. This consistency further supports the validity of NE as a measure for quantifying node importance in information propagation.


%\begin{figure}
 %   \centering
  %  \includegraphics[width=0.5\linewidth]{iclr2025/fig/bio1.png}
 %   \caption{\color{blue}The NE and FC values of 90 nodes from a randomly selected sample in the ADNI dataset.}
 %   \label{fig:bio1}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/ABIDE_AllNode_NE_PCC.png}
    \caption{The NE value and FC strength of 200 nodes from a randomly selected sample in the ABIDE dataset.}
    \label{fig:ABIDE_AllNode_NE&PCC}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/ADHD_AllNode_NE_PCC.png}
    \caption{The NE value and FC strength of 190 nodes from a randomly selected sample in the ADHD-200 dataset.}
    \label{fig:ADHD_AllNode_NE&PCC}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{fig/ADNI_AllNode_NE_PCC.png}
    \caption{The NE value and FC strength of 90 nodes from a randomly selected sample in the ADNI dataset.}
    \label{fig:ADNI_AllNode_NE&PCC}
\end{figure}


\subsection{Functional Module Division}\label{division}

Table~\ref{tab:fm} gives the empirical labels of ROIs and functional modules. The functional modules used in this study are based on empirical labels. These labels represent the best effort to categorize regions based on known functional associations, but they are inherently limited due to the complex biological properties of the brain graph. Empirical functional modules often encompass ROIs from diverse brain regions, resulting in heterogeneity within each module. For example, in the auditory cortex, both temporal lobe regions (e.g., `temporal 103') and thalamic regions (e.g., `thalamus 57') are included due to their involvement in auditory processing~\citep{rauschecker2009maps,jones2012thalamus}. This diversity may reduce the uniformity of high self-attention scores within the module. In addition, because of limited label availability, there are no available labels for the atlas of the ADNI and ABIDE datasets. Therefore, we can only provide labels for the ADHD-200 dataset.

\begin{table}[]
    \centering
        \caption{The labels of ROIs and functional modules.}
    \begin{tabular}{c|c} % Increase the width as necessary
        \toprule
        Functional Module & ROI \\
        \midrule
        \multirow{7}{1cm}{\centering Vis} & `occipital 146', `occipital 141', `occipital 136', `occipital 137', \\
        & `occipital 92', `occipital 149', `occipital 148', `occipital 145', \\
        & `occipital 147', `occipital 142', `occipital 139', `occipital 135', \\
        & `occipital 133', `occipital 129', `occipital 126', `occipital 118', \\
        & `occipital 119', `occipital 106', `post occipital 160', `post occipital 158', \\
        & `post occipital 159', `post occipital 157', `post occipital 156', \\
        & `post occipital 153', `post occipital 154', `post occipital 152' \\ \midrule

       \multirow{7}{1cm}{\centering MC} & `inf cerebellum 155', `inf cerebellum 150', `inf cerebellum 151', \\
        & `inf cerebellum 140', `inf cerebellum 131', `inf cerebellum 122', \\
        & `inf cerebellum 121', `inf cerebellum 110', `lat cerebellum 128', \\
        & `lat cerebellum 113', `lat cerebellum 109', `lat cerebellum 98', \\
        & `med cerebellum 143', `med cerebellum 144', `med cerebellum 138', \\
        & `med cerebellum 130', `med cerebellum 127', `med cerebellum 120', \\
        & `post parietal 99', `SMA 43', `basal ganglia 71', `basal ganglia 38', \\
        & `basal ganglia 39', `basal ganglia 30' \\\midrule
        
        \multirow{6}{1cm}{\centering CC} 
        & `post cingulate 115', `post cingulate 111', `post cingulate 108', \\
        &`post cingulate 93', `post cingulate 90', `post cingulate 73', \\
        &`vlPFC 15', `vmPFC 13', `vmPFC 11', \\
        &`vmPFC 7', `vmPFC 6', `vmPFC 1', \\
        & `IPL 107', `IPL 104', `IPL 101', \\
        &`IPL 96', `IPL 88', `IPS 116', `IPS 114' \\ \midrule
        
        \multirow{4}{1cm}{\centering Aud}
        &`sup temporal 100', `temporal 103', `temporal 95',\\
       & `temporal 78', `thalamus 57', `thalamus 58', \\
        & `thalamus 47', `inf temporal 91', `inf temporal 72', \\
        &`inf temporal 63', `temporal 123' \\ \midrule
         \multirow{3}{3cm}{\centering LP}
        &`angular gyrus 102', `mid insula 61', `mid insula 59', \\
        &`mid insula 44', `aPFC 5', `angular gyrus 124', \\
        &`angular gyrus 117', `aPFC 2', `aPFC 3'  \\ \midrule

        \multirow{3}{1cm}{\centering EC}
        &`dACC 27', `dFC 36', `dFC 34', `dFC 29', \\
        &`dlPFC 24', `dlPFC 22', `vPFC 23', `vent aPFC 10', \\
        &`vent aPFC 9',`vlPFC 12',`dlPFC 16',`dFC 3'  \\
        \bottomrule
    \end{tabular}
    \label{tab:fm}
\end{table}


\subsection{Analysis of Attention Patterns Output by BioBGT}\label{appendix_attention}

\textbf{Comparsion between NCs and ADHD patients.}~Studies in neuroscience have shown that functional connectivity (FC) in ADHD patients can be either enhanced or weakened to varying degrees, disrupting functional modularity~\citep{wang2020disrupted}. For instance, Sripada et al. demonstrated that the FC strength within the cognition control (CC) module is reduced in individuals with ADHD~\citep{sripada2014lag}. Figure~\ref{fig:ADHDheat} presents the heatmaps of the average self-attention scores for normal controls (NCs) and ADHD patients from the ADHD-200 test set. It is evident that the functional modules in the heatmap of ADHD patients are less distinct, while the NC group's heatmap shows clearer functional modules, such as Vis, CC, and MC modules. Specifically, the CC module in the NC group exhibits higher internal strength and stronger modularity compared to the CC module in the ADHD group, which aligns with Sripada et al.'s findings in neuroscience.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ADHD-NC.png}
        \caption{NC}
        \label{fig:sub1}
    \end{subfigure}
    \hspace{0.02\textwidth} %
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ADHD.png}
        \caption{ADHD}
        \label{fig:sub2}
    \end{subfigure}
    \caption{The heatmaps of the average self-attention scores of normal controls (a) and ADHD patients (b) in the ADHD-200 test set.}
    \vspace{-1\baselineskip} 
    \label{fig:ADHDheat}
\end{figure}


\textbf{Comparsion between NCs and ASD patients.}~Studies have shown that the small-world properties of brain graphs in autism spectrum disorder (ASD) patients are generally lower than those of typically developing individuals~\citep{itahashi2014altered}. Particularly, the hub properties of nodes are weakened in brain graphs of ASD patients. Figure~\ref{fig:ABIDEheat} shows the heatmaps of the average self-attention scores for NCs and ASD patients from the ABIDE test set. It can be observed that the attention heatmap of NCs contains nodes with strong hub properties, indicating high correlations with most other nodes. In contrast, the attention heatmap of ASD patients shows weaker hub properties, which aligns with the finding that the small-world properties of functional connectivity in ASD patients are diminished.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ABIDE0.png}
        \caption{NC}
        \label{fig:sub1}
    \end{subfigure}
    \hspace{0.02\textwidth} %
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ABIDE-NC.png}
        \caption{ASD}
        \label{fig:sub2}
    \end{subfigure}
    \caption{The heatmaps of the average self-attention scores of normal controls (a) and ASD patients (b) in the ABIDE test set.}
    \vspace{-1\baselineskip} 
    \label{fig:ABIDEheat}
\end{figure}

\textbf{Comparsion between NCs, MCI patients, and AD patients.}
Different stages of Alzheimer’s disease exhibit distinct functional connectivity patterns~\citep{sanz2010loss}. Specifically, as the disease progresses, the modular characteristics of functional brain graphs decline~\citep{dai2014disrupted,zhao2012disrupted}. The brain graphs of NCs display more distinct modular structures, while the functional connectivity patterns of mild cognitive impairment (MCI) patients begin to show signs of disorganization. For patients with Alzheimer’s disease (AD), the partitioning of functional modules almost disappears, and modularity is significantly diminished. Figure~\ref{fig:ADNI-heat} presents the heatmaps of the average self-attention scores for NCs (a), MCI patients (b), and AD patients (c) in the ADNI test set. It is evident that the brain graphs of NCs exhibit more distinct functional modules compared to those of MCI and AD patients.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ADNI-NC.png}
        \caption{NC}
        \label{fig:sub1}
    \end{subfigure}
    \hspace{0.02\textwidth} %
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ADNI-MCI.png}
        \caption{MCI}
        \label{fig:sub2}
    \end{subfigure}
    \hspace{0.02\textwidth} % 
    \begin{subfigure}{0.25\textwidth}
        \includegraphics[width=\linewidth]{fig/ADNI-AD.png}
        \caption{AD}
        \label{fig:sub3}
    \end{subfigure}
    \caption{The heatmaps of the average self-attention scores normal controls (a),  MCI patients (b), and AD patients (c) in the ADNI test set. }
    \vspace{-1\baselineskip} 
    \label{fig:ADNI-heat}
\end{figure}


\subsection{Model Generalizability Analysis}
We suggest that BioBGT not only performs well on brain graphs but may also generalize effectively to other networks with similar structural characteristics, such as the presence of hubs and modules. To prove this, we apply BioBGT to other types of networks, such as citation networks. We train our model on the Citeseer~\citep{giles1998citeseer} and Cora~\citep{mccallum2000automating} datasets for node classification tasks. Table~\ref{ab3} highlights the superiority of BioBGT on both Citeseer and Cora datasets. Therefore, our model shows generalizability in extending to other networks that contain hubs and modules.

\begin{table*}[]
	\centering
 %\vspace{-1\baselineskip} 
	\caption{Model performance on Citeseer and Cora (\%).}
% \vspace{-1\baselineskip}
	\label{ab3}
	\renewcommand\arraystretch{0.8}
 \setlength{\tabcolsep}{3pt} % Adjust column separation	
 \small
		\begin{tabular}{ccccccc}\toprule
			\multirow{2}{1cm}{}& \multicolumn{3}{c}{Citeseer} &\multicolumn{3}{c}{ Cora} \\ \cline{2-7}
			& F1& ACC&AUC& F1& ACC&AUC\\\midrule
			GAT& 45.45$\pm$5.17& \textbf{70.01$\pm$0.68}&81.06$\pm$1.91& 81.02$\pm$0.46& 81.74$\pm$0.53&\textbf{97.38$\pm$0.03}\\
			%SAN& 7.69$\pm$0.26& 29.13$\pm$0.74&49.05$\pm$0.83& 7.64$\pm$0.45& 28.84$\pm$0.62&48.99$\pm$1.03\\
			%Graph Trans.& 7.19$\pm$0.26& 28.86$\pm$0.15&51.26$\pm$0.02& 5.97$\pm$0.30& 15.85$\pm$0.13&48.70$\pm$0.21\\
			SAT+PE& 64.04$\pm$1.25& 68.80$\pm$0.52&87.69$\pm$1.99& 81.62$\pm$0.56& 82.84$\pm$0.87&94.63$\pm$0.34\\\midrule
			\textbf{BioBGT}&\textbf{64.80$\pm$0.34}& 69.02$\pm$0.70&\textbf{88.04$\pm$1.51}& \textbf{82.03$\pm$1.33}& \textbf{83.14$\pm$0.80}&94.58$\pm$0.24\\\bottomrule
	\end{tabular}
% \vspace{-1.5\baselineskip} 
\end{table*}

\end{document}
