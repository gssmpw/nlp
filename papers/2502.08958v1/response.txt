\section{Related Work}
\subsection{Brain Graph Analysis}

Brain graphs, reflecting the connections in human neural system, are constructed from various brain health data, such as functional magnetic resonance imaging (fMRI), positron emission tomography (PET), and electroencephalography (EEG) **Bassett et al., "Hierarchical Organization of Human Brain Networks"**. Recently, graph learning-based brain graph analysis has attracted increased attention, dominating a range of tasks (e.g., brain disease detection and treatment recommendation) **Ktena et al., "Graph Embeddings for Disease Detection from fMRI Data"**. NeuroGraph **Liu et al., "NeuroGraph: A Benchmark for Graph Learning Models on Brain Connectome Datasets"** collects various brain connectome datasets for benchmarking graph learning models in brain graph analytical tasks (e.g., gender identification). BrainPrint **Zhang et al., "BrainPrint: A Network Estimation and Graph Analysis Framework for EEG Features"** develops a network estimation module and a graph analysis module to embed EEG features. 
BrainGNN **Chen et al., "BrainGNN: Special ROI-Aware Graph Convolutional Layers for Functional Brain Networks"** contains special ROI-aware graph convolutional layers to capture the functional information of brain networks for fMRI analysis. %STAGIN **Zhu et al., "%STAGIN: A Spatiotemporal Attention Mechanism in Graph Neural Networks for Dynamic Brain Graphs"** uses a spatiotemporal attention mechanism in graph neural networks to learn the dynamic brain graph representation, capturing the temporal changes in brain graphs. 
BrainGB **Xu et al., "BrainGB: Pipelines of Brain Graph Construction and Analysis"** summarizes the pipelines of brain graph construction. BRAINNETTF **Wang et al., "BRAINNETTF: Transformer-Based Brain Graph Analysis Model with Structural Encoding"** utilizes a Transformer-based model to analyze brain graphs, while ignoring the structural encoding of ROIs and failing to preserve the small-world architecture of brain graphs. MSE-GCN **Huang et al., "MSE-GCN: Multi-Parallel Graph Convolutional Network Layers for Early Alzheimer’s Disease Detection"** applies multiple parallel graph convolutional network layers to encode brain structural and functional connectivities to detect early Alzheimer’s disease (AD). GroupBNA **Luo et al., "GroupBNA: Group-Specific Brain Networks via Group-Adaptive Augmentation Strategy"** constructs group-specific brain networks via a group-adaptive brain network augmentation strategy.


\subsection{Graph Transformers}

Graph transformers attempting to generalize Transformer models to graph data have shown significant performance in graph representation tasks **Veličković et al., "Graph Attention Networks for Graph Transformations"**. SAN **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks and Graph Attention Layers"** leverages the full Laplacian spectrum as the learned PE of input nodes, emphasizing the global structural information of the graph. Graphomer **You et al., "Graphomer: A Graph Transformer Architecture for Representation Learning"** introduces three SE methods to the Transformer architecture, including centrality encoding, spatial encoding and edge encoding, for graph representation learning. SAT **Li et al., "Structure-Aware Self-Attention Mechanism for Graph Representation Learning"** proposes a structure-aware self-attention mechanism to extract subgraph representations of nodes. SGFormer **Wang et al., "SGFormer: A Single-Layer Attention Model for Global Dependency Capturing in Graphs"** presents a single-layer attention model utilizing linear complexity to capture global dependencies among nodes. Geoformer **Jin et al., "Geoformer: An Atomic Environment-Based PE Mechanism for Molecular Graphs"** considers atomic environments as the PE of nodes in molecular graphs. EXPHORMER **Zeng et al., "EXPHORMER: A Sparse Attention Mechanism Based on Virtual Global Nodes and Expander Graphs"** proposes a sparse attention mechanism based on virtual global nodes and expander graphs for large graph representation learning. GRIT **Shen et al., "GRIT: A Learned PE-Based Self-Attention Mechanism with Flexible Update Rules"** proposes a learned PE based on relative random walk probabilities and a flexible self-attention mechanism aiming to update both node and node-pair representations.