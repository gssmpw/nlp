\section{Related Work}
\subsection{Brain Graph Analysis}

Brain graphs, reflecting the connections in human neural system, are constructed from various brain health data, such as functional magnetic resonance imaging (fMRI), positron emission tomography (PET), and electroencephalography (EEG)~\citep{bullmore2009complex,bessadok2022graph}. Recently, graph learning-based brain graph analysis has attracted increased attention, dominating a range of tasks (e.g., brain disease detection and treatment recommendation)~\citep{10388338,kan2022brain,liu2023braintgl,ding2023lggnet}. NeuroGraph~\citep{said2024neurograph} collects various brain connectome datasets for benchmarking graph learning models in brain graph analytical tasks (e.g., gender identification). BrainPrint~\citep{wang2020brainprint} develops a network estimation module and a graph analysis module to embed EEG features. 
BrainGNN~\citep{li2021braingnn} contains special ROI-aware graph convolutional layers to capture the functional information of brain networks for fMRI analysis. %STAGIN~\citep{kim2021learning} uses a spatiotemporal attention mechanism in graph neural networks to learn the dynamic brain graph representation, capturing the temporal changes in brain graphs. 
BrainGB~\citep{cui2022braingb} summarizes the pipelines of brain graph construction. BRAINNETTF~\citep{kan2022brain} utilizes a Transformer-based model to analyze brain graphs, while ignoring the structural encoding of ROIs and failing to preserve the small-world architecture of brain graphs. MSE-GCN~\citep{lei2023multi} applies multiple parallel graph convolutional network layers to encode brain structural and functional connectivities to detect early Alzheimerâ€™s disease (AD). GroupBNA~\citep{peng2024adaptive} constructs group-specific brain networks via a group-adaptive brain network augmentation strategy.


\subsection{Graph Transformers}

Graph transformers attempting to generalize Transformer models to graph data have shown significant performance in graph representation tasks~\citep{kong2023goat,luo2024transformers,luo2024fairgt}. SAN~\citep{kreuzer2021rethinking} leverages the full Laplacian spectrum as the learned PE of input nodes, emphasizing the global structural information of the graph. Graphomer~\citep{ying2021transformers} introduces three SE methods to the Transformer architecture, including centrality encoding, spatial encoding and edge encoding, for graph representation learning. SAT~\citep{chen2022structure} proposes a structure-aware self-attention mechanism to extract subgraph representations of nodes. SGFormer~\citep{wu2024simplifying} presents a single-layer attention model utilizing linear complexity to capture global dependencies among nodes. Geoformer~\citep{wang2024geometric} considers atomic environments as the PE of nodes in molecular graphs. EXPHORMER~\citep{shirzad2023exphormer} proposes a sparse attention mechanism based on virtual global nodes and expander graphs for large graph representation learning. GRIT~\citep{ma2023graph} proposes a learned PE based on relative random walk probabilities and a flexible self-attention mechanism aiming to update both node and node-pair representations.