% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[accsupp]{axessibility}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{2762} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Emergence of Human Visual Perception Characteristics in Large Vision Foundation Models: Similarities and Differences}
% \title{Perceptual Comparisons Between Large Vision Foundation Models and the Human Visual System: A Multidimensional Evaluation}

%\title{From Near-Threshold to Supra-Threshold: A Multidimensional Perceptual Analysis of Human Vision and Large Vision Foundation Models}

% \title{Do computer vision foundation models learn the low-level characteristic of the human visual system?}
\title{\vspace{-20pt}Do computer vision foundation models learn \\ the low-level characteristics of the human visual system?}


%%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{Yancheng Cai\\
% University of Cambridge\\
% {\tt\small yc613@cam.ac.uk}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Fei Yin\\
% University of Cambridge\\
% {\tt\small fy277@cam.ac.uk}
% \and Dounia Hammou\\
% University of Cambridge\\
% {\tt\small dh706@cam.ac.uk}
% \and Rafal Mantiuk\\
% University of Cambridge\\
% {\tt\small mantiuk@gmail.com}
% }

% \author{Yancheng Cai, University of Cambridge, {\tt\small yc613@cam.ac.uk}\\
% Fei Yin, University of Cambridge, {\tt\small fy277@cam.ac.uk}\\
% Dounia Hammou, University of Cambridge, {\tt\small dh706@cam.ac.uk}\\
% Rafal Mantiuk, University of Cambridge, {\tt\small mantiuk@gmail.com}\\
% }
\author{Yancheng Cai, Fei Yin, Dounia Hammou, Rafal Mantiuk; University of Cambridge, UK\\{{\tt\small yc613@cam.ac.uk}, {\tt\small fy277@cam.ac.uk}, {\tt\small dh706@cam.ac.uk}, {\tt\small mantiuk@gmail.com}}
}

\input{macros}
\begin{document}
\maketitle
\begin{abstract}
Computer vision foundation models, such as DINO or OpenCLIP, are trained in a self-supervised manner on large image datasets. Analogously, substantial evidence suggests that the human visual system (HVS) is influenced by the statistical distribution of colors and patterns in the natural world, characteristics also present in the training data of foundation models. The question we address in this paper is whether foundation models trained on natural images mimic some of the low-level characteristics of the human visual system, such as contrast detection, contrast masking, and contrast constancy. Specifically, we designed a protocol comprising nine test types to evaluate the image encoders of 45 foundation and generative models. Our results indicate that some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some of the characteristics of human vision, but other models show little resemblance. Foundation models tend to show smaller sensitivity to low contrast and rather irregular responses to contrast across frequencies. The foundation models show the best agreement with human data in terms of contrast masking. Our findings suggest that human vision and computer vision may take both similar and different paths when learning to interpret images of the real world. Overall, while differences remain, foundation models trained on vision tasks start to align with low-level human vision, with DINOv2 showing the closest resemblance. Our code is available on \url{https://github.com/caiyancheng/VFM_HVS_CVPR2025}.

\end{abstract}

\section{Introduction}
\label{sec:intro}
Computer vision foundation models, such as DINO~\cite{caron2021emerging} or OpenCLIP~\cite{ilharco_gabriel_2021_5143773, Radford2021LearningTV}, show exceptional ability to generalize to different tasks and are becoming cornerstones of many computer vision methods. They owe their exceptional performance to self-supervised training on very large image datasets. The human visual system also owes much of its capability to being able to perceive the world, over many years from infancy to childhood \cite{Braddick_Atkinson_2011}. A question arises: if the neural network and the visual system are trained by being exposed to a large number of images of the world, will they share their low-level vision characteristics? If they do, we will know that those low-level characteristics arise naturally and likely reflect the statistics of real-world scenes. If they do not, it means that human low-level vision characteristics are specific to the optical/biological limitations of human vision rather than natural image statistics. Our analysis is meant to shed some light on how the vision, either biological or computational, may develop from observing samples of the world, taking either the same or different routes to accomplish their respective tasks. 

\begin{figure}[t]
  \vspace{-10pt}
  \centering
      \includegraphics[width=\linewidth]{images/first_figure_8.pdf}
  \caption{To determine whether image encoders of foundation models exhibit a similar low-level characteristic as human vision, we test them on psychophysical stimuli for which human data is available. We want to test the alignment of contrast encoding between human and computational vision models.}
  \label{fig:dream}
\end{figure}

In particular, we are interested in the characteristics that are well understood and measured in human vision science using psychophysical methods: contrast detection \cite{barten1999contrast}, contrast masking \cite{Legge_Foley_1980} and contrast constancy \cite{georgeson1975contrast}. Contrast detection and contrast masking quantify the ability of the visual system to detect small contrast patterns, either on uniform backgrounds (contrast detection) or on backgrounds with patterns (contrast masking). Contrast detection and masking capture the ``bottlenecks'' of the visual system --- the characteristic that may prevent us from detecting patterns that are too dark or too small. Similarly, cameras used for computer vision are limited by the MTF of the lens, sensor resolution, photon and sensor noise, and we can expect that computer vision methods may need to deal with similar limitations. 

Contrast constancy is the term used in vision science to describe the invariance of the visual system to spatial frequency \cite{georgeson1975contrast} and partially luminance \cite{Kulikowski_1976,Peli_1995}. Georgeson and Sullivan \cite{georgeson1975contrast} showed that the perceived magnitude of the contrast that is well above the detection threshold (supra-threshold) appears to us the same regardless of spatial frequency. This is a very important characteristic as it allows us to see contrast (and therefore objects) the same regardless of the viewing distance; otherwise, the frequencies would change with the viewing distance and hence the contrast appearance. A partial constancy (invariance) is also observed across luminance \cite{Kulikowski_1976,Peli_1995}, though there is a significant deviation from constancy at lower luminance levels, once the visual system needs to rely on the rod vision. The invariance is also an important feature of many computer vision methods. For example in SIFT features \cite{Lowe_2004} have been designed to be invariant to the changes in contrast, brightness, scale, and rotation. In our experiments, we used the supra-threshold contrast matching test to assess whether the models exhibit the characteristic of contrast constancy.

Numerous works on adversarial attacks demonstrated that the classification performance of deep learning models can be greatly degraded by visually inconsequential changes \cite{Goodfellow_Shlens_Szegedy_2014}. At the same time, human vision does not suffer from such adversarial vulnerability \cite{wichmann2023deep}. This is one of the most salient arguments put forward to state that deep architectures are different from human vision. Here, we propose a different methodology to study this question. We consider the deep neural network to be a black box and compare its responses to well-understood and measured characteristics of the human visual system. In particular, we want to check whether the foundation vision models share the same ``bottlenecks'' and invariance properties as the visual system. To achieve this, we test foundation models on basic vision stimuli, such as Gabor patches and band-limited noise, and compare the response of those models with the psychophysical data collected from human observers. 

In summary, our contributions are as follows:
\begin{itemize}
\item We developed a protocol to evaluate the similarity between machine vision models and the human visual system. This protocol includes contrast detection, contrast masking, and contrast constancy, subdivided into nine distinct test types that collectively capture the low-level fundamental characteristics of human vision.
\item We tested the image encoders of 45 foundation and generative models. The results reveal similarities between certain foundation models (\eg, DINOv2 and OpenCLIP) and human vision, particularly in the contrast masking test. However, differences persist across other tests.
\end{itemize}


\section{Related Work}
\label{sec:relat}
Since the advent of deep learning, machine vision models based on foundation models~\cite{ dosovitskiy2020image, oquab2023dinov2, kirillov2023segment} and DNNs~\cite{tian2023multi, lu2023efficient, zhang2025bridgenet, lv2025drkd}  have successfully handled numerous advanced visual tasks. However, researchers have observed that machine vision operates differently from human vision. \cite{geirhos2018imagenet} revealed that standard CNNs trained on ImageNet are strongly biased toward texture recognition rather than shape, which contrasts with human visual patterns. \cite{wichmann2023deep} provided further evidence that deep neural networks (DNNs) differ significantly from the HVS, demonstrating poor robustness in object classification under 3D viewpoint changes and image distortions, and showing vulnerability to adversarial examples, which are rarely problematic for humans. \cite{bowers2023deep} pointed out that DNNs performing well in benchmark tests share little overlap with biological vision mechanisms and fail to account for many findings in psychological studies of human vision. This highlights a clear distinction between machine and human vision, leading to the rise of interest in domain adaptation~\cite{cai2023rethinking} and making networks robust to adversarial attacks~\cite{yin2023generalizable}.

But there is also evidence that the gap between neural network-based machine vision models and human vision is gradually narrowing. \cite{tuli2021convolutional} compared vision transformers (ViT)~\cite{dosovitskiy2020image} and CNNs, finding that ViT not only achieves superior task accuracy but also exhibits weaker inductive biases, with error patterns more consistent with human errors. \cite{geirhos2021partial} discovered that the long-standing robustness gap between humans and CNNs in handling distortions is shrinking. \cite{ghildyal2024foundation, croce2024adversarially} also demonstrated that foundation models like DINO~\cite{caron2021emerging} and CLIP~\cite{radford2021learning} can generate more accurate and robust metrics for low-level perceptual similarity.

Most of the aforementioned studies focus on high-level task performance (\eg, accuracy, consistency, ...), which may not reveal whether computation models suffer from the same bottlenecks and rely on the same invariances as human vision. To that end, \cite{li2022contrast, akbarinia2023contrast} have attempted to reveal CSF characteristics within pretrained architectures by training a head with a contrast discrimination classifier. The problem with this approach is that it introduces a bias by relying on a classifier trained to compare contrast. Such studies also make an incorrect assumption that CSF explains both near-threshold and super-threshold vision, while contrast constancy results (see \secref{sub_SCM}) show that this is not the case.  In contrast, we examine networks' low-level characteristics without additional task-specific training, considering both near-threshold and supra-threshold vision. 




\section{Testing framework}
\label{sec:imple}
We first explain the tested models, testing methods, result visualization, and the strategy we used to summarize and quantify our results. 

\subsection{Tested models and testing methodology}

The objective is to evaluate the responses of machine vision foundation models to stimuli commonly used in human vision research~\cite{ashraf2024castlecsf, yancheng2024elaTCSF} and to compare these responses with psychophysical human data. We tested a representative set of 45 models, encompassing the most influential large vision foundation models, including the variants of DINO~\cite{caron2021emerging}, DINOv2~\cite{oquab2023dinov2,darcet2023vitneedreg}, OpenCLIP~\cite{ilharco_gabriel_2021_5143773, Radford2021LearningTV}, SAM~\cite{kirillov2023segment}, SAM-2~\cite{ravi2024sam}, and MAE~\cite{he2022masked}, as well as the encoder used for the latent space of generative model Stable Diffusion (SD-VAE, \cite{rombach2022high}). Additionally, we report the responses of ColorVideoVDP~\cite{mantiuk2024colorvideovdp}, which is an image and video quality metric that explicitly models low-level human vision and acts as a reference for a low-level human vision model. All models and their variants are listed in \figref{All_score_result}.

To test these models, we need to compare pairs of images and assess the ``perceived'' difference between them. We adopt a methodology inspired by 2-alternative-forced-choice (2AFC) psychophysical experiments. For example, for a pattern detection task, a pair of images could be a Gabor patch (test) and a uniform field (reference), as shown on the left of \figref{pipeline}. 
As such patterns are calibrated in physical light units in vision science, we generate these patterns as luminance maps, scaled in physical units of \cdms. These luminance maps are then mapped from the linear space to the sRGB color space using a display model (display peak luminance of 400\cdms) and fed into the image encoder of the foundation model for feature extraction. Note that the sRGB space is almost universally used to represent training datasets and is expected input for the tested encoders. To ensure that models can operate on small contrast values, we modified them to accept floating-point values (instead of 8-bit integers) as input. This was necessary as the quantization artifacts in 8-bit images are often larger than the detection thresholds of human vision.

To investigate whether the distances in the feature space reflect the perceptual detection thresholds and invariances, we experimented with a series of distance measures, including $L_1$ and $L_2$. We found the cosine similarity expressed as a relative angle ($S_\ind{ac}$) yielded results most consistent with the psychophysical data. $S_\ind{ac}$ is defined as:
\begin{equation}
S_\ind{ac} = \frac{1}{\pi}\arccos \left( \frac{F_\ind{T} \cdot F_\ind{R}}{\norm{F_\ind{T}}\norm{F_\ind{R}}}\right)
    \label{eq:sa},
\end{equation}
where $F_\ind{T}$ and $F_\ind{R}$ are the test and reference feature vectors (feature maps reshaped into one dimension), and $\cdot$ denotes the dot product. $S_\ind{ac} = 0$ indicates two input images are equivalent, while $S_\ind{ac}=1$ indicates large differences.

To compare the encoder responses with psychophysical data, we need to be able to map the image sampling frequency into the spatial frequency on the retina. For that, we select the effective resolution of 60 pixels-per-degree, which is typical for modern monitors. We note, however, that the choice of this parameter is arbitrary and the model similarity scores can be shifted by a small multiplier along the spatial frequency axis. The luminance maps are generated at the resolution of 224$\times$224 pixels, corresponding to the size of 3.7$\times$3.7 visual degrees. It roughly aligned with the extend of the human foveal vision, and the stimuli span across multiple receptive fields of human vision and multiple patches/tokens of a foundation model. 

For ColorVideoVDP, it can work directly on linear physical units, so conversion to sRGB was unnecessary. We directly use its quality score instead of $S_\ind{ac}$.  Note that the primary focus of this study is on foundation models, with the ColorVideoVDP metric used solely as a baseline for comparison purposes.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/Pipeline_9.pdf}
  \caption{Pipeline for computing $S_\ind{ac}$. Generate test and reference images in the linear luminance space (\csdm), transform them to the sRGB color space using a display model, and input each into the image encoder. Reshape the output features into one-dimensional vectors $F_\ind{T}$ and $F_\ind{R}$, then compute $S_\ind{ac}$.}
  \label{fig:pipeline}
\end{figure}


\begin{figure}[t]
  \centering
      \includegraphics[width=\linewidth]{images/Gabor_SpF_Contrast_sup.pdf}
  \caption{Gabors with different spatial frequencies (x-axis) and contrast (y-axis) used as the test images in the contrast detection tests (\secref{contrast-detection}). ``cpd" denotes cycles per degree. Note that the high-frequency patterns can be rendered with aliasing artifacts on the screen or in print --- those were not present in our tests.}
  \label{fig:gabor_spf_contrast}
\end{figure}

\subsection{Model alignment score}
\label{sec:model-alignment-score}

Most of our results will be represented as contour plots of model responses, providing qualitative interpretation. Here, we explain our measure of model alignment, which provides quantitative scores. 

As an example, we take the leftmost contour plot in row (a) of \figref{contour-main}. Each point on the contour plot corresponds to $S_\ind{ac}$ between the test image as shown in \figref{gabor_spf_contrast} and a uniform field of the same mean luminance. The dashed line represents human contrast detection data, predicted with castleCSF \cite{ashraf2024castlecsf}. A well-aligned model should show one of the contour lines that follows the dashed castleCSF line. We cannot directly use the $S_\ind{ac}$ values along the dashed line as the measure of alignment because some models result in $S_\ind{ac}=0$ for most points near the detection threshold (detect no difference). Therefore, instead, we rely on the measure of change in $S_\ind{ac}$ in the neighborhood of the dashed line. 

For a well-aligned model, the perceived differences in the neighborhood of the detection threshold (dashed line) should increase as the contrast increases (the sensitivity decreases), Furthermore, the values should be similar along the dashed line. The measure these two properties, we sample the $S_\ind{ac}$ values for the points that are shifted in contrast (vertical direction) from the dashed line by a multiplier $m$, where $0.5{\leq}m{\leq}2$ (note the logarithmic scale in the contour plot in \figref{contour-main}). We collect such data for multiple frequencies (or other dimensions) along the dashed line and calculate the Spearman rank order correlation between the multipliers $m$ and the $S_\ind{ac}$ values. If the properties mentioned above are preserved, the correlation coefficient value $r_s$ should be close to 1. 

The above strategy is used for all contrast detection and contrast masking experiments. For the contrast matching experiment, we use the root mean squared error (RMSE) between the model and human matching data, expressed as the logarithm of contrast. 

\section{Experiments}
\label{sec:exper}
\begin{figure*}[t]
  \centering
      \includegraphics[width=\textwidth]{images/Test_Examples.pdf}
  \caption{Examples of test images for contrast detection and contrast masking. The detailed explanation of the stimuli can be found in the \supplementary{}. ``Ach." denotes achromatic. }
  \label{fig:test_examples}
  \label{fig:stimuli}
\end{figure*}

\begin{table*}[t]
\begin{center}
\caption{
Key parameters for all our tests. Note that ``Radius" does not apply when the pattern is not a Gabor.
}
\label{tab:params}
\setlength{\tabcolsep}{0.7mm}
\footnotesize
{\scalebox{1.05}{
\begin{tabular}{c|c|c|c|c}
\toprule
Test & Spatial Frequency (cpd) & Luminance (\csdm{}) & Radius (degree) & Contrast \\ \hline\hline
Spatial Frequency - Gabor   Achromatic & 0.5 - 32 & 100 & 1 & 0.001 - 1 \\ \hline
Spatial Frequency - Noise   Achromatic & 0.5 - 32 & 100 & - & 0.001 - 1 \\ \hline
Spatial Frequency - Gabor RG & 0.5 - 32 & 100 & 1 & 0.001 - 0.12 \\ \hline
Spatial Frequency - Gabor YV & 0.5 - 32 & 100 & 1 & 0.001 - 0.8 \\ \hline
Luminance - Gabor Achromatic & 2 & 0.1 - 200 & 1 & 0.001 - 1 \\ \hline
Area - Gabor Achromatic & 8 & 100 & 0.1 - 1 & 0.001 - 1 \\ \hline
Phase-Coherent Masking & 2 (mask) / 2 (test) & 32 & 0.5 (test) & 0.005 - 0.5 (mask) / 0.01 - 0.5 (test) \\ \hline
Phase-Incoherent Masking & 0 - 12 (mask) / 1.2 (test) & 37 & 0.8 (test) & 0.005 - 0.5 (mask) / 0.01 - 0.5 (test) \\ \hline
Contrast Matching & 5 (reference) / 0.25 - 25 (test) & 10 & - & 0.005 - 0.629 (reference) \\ \bottomrule
\end{tabular}
}}
\end{center}
\end{table*}

\begin{figure*}[t]
  \centering
  \vspace{0pt}
      \includegraphics[width=1\textwidth]{images/contour_plot_8.pdf}
  \caption{
  Selected representative experimental results. Each row represents a test, and each column corresponds to a model, selected as the best-performing in their original tasks. (a)-(f): contour plots of contrast detection $S_\ind{ac}$, with the ground truth castleCSF~\cite{ashraf2024castlecsf}. (g),(h): contour plots of contrast masking $S_\ind{ac}$, with the ground truth from~\cite{foley1994human} and~\cite{gegenfurtner1992contrast}, respectively. Different colored solid lines representing different $S_{\mathrm{ac}}$ values, where purple indicates the minimum difference ($S_{\mathrm{ac}}\to0$). (i): results from the contrast matching experiment, where different colors represent different $C_r$ values. The dashed lines are human results~\cite{georgeson1975contrast}, while the solid lines are model-predicted results from Equation~\ref{eq:cm_cos}. Results for all models are in the supplementary material.}
  \label{fig:contour-main}
  \label{fig:Contour_1}
\end{figure*}

\begin{figure}[t]
  \centering
      \includegraphics[width=\linewidth]{images/All_Score_Result_4.pdf}
  \caption{The quantified similarity error between all 45 models and HVS under 9 different tests. For the Contrast Detection and Contrast Masking tasks, Spearman Correlation was used as the metric, with higher values (closer to 1) indicating greater similarity to human vision. For the Supra-threshold Contrast Matching task, RMSE was used as the metric, with lower values (closer to 0) indicating better similarity.}
  \label{fig:All_score_result}
\end{figure}


\subsection{Contrast detection and CSF}
\label{sec:sub_CD}
\label{sec:contrast-detection}

We begin by testing the foundation models' ability to detect low-contrast (near-threshold) patterns (\eg, Gabor patches, band-limited noise) and compare their performance with the human data. As the reference human data, we rely on the caslteCSF \cite{ashraf2024castlecsf}, which is the recent contrast sensitivity function, modeling contrast detection of both achromatic and chromatic patterns. 


\paragraph{Spatial Frequency}

Contrast sensitivity of the human eye is typically associated with the variation across the spatial frequency. The visual system exhibits a band-pass characteristic, with a peak sensitivity between 2 and 4 cycles per degree (cpd), depending on the luminance and other parameters of the stimulus. The lower sensitivity of the visual system at lower frequencies is associated with the mechanism of lateral inhibition \cite{barten1999contrast}, which helps to reduce the influence of (low-frequency) illumination on the perceived images. Such invariance to illumination is a desirable property if the goal is to recognize objects regardless of illumination conditions. The drop in sensitivity at high frequencies is associated with the limitations of eye optics (achromatic contrast) and cone density (chromatic contrast). Here we want to test whether the computer-vision foundation models pick up similar traits when trained on natural images. 

To test encoder responses across frequencies, we generated a 2D array of image pairs, in which the reference image had uniform luminance, and the test image contained a Gabor patch, as shown in \figref{gabor_spf_contrast} (refer to \supplementary{} for the visualization of other stimuli). We generate such Gabors for achromatic (see \figref{stimuli}-a) and chromatic modulation (see \figref{stimuli}-c,d). We also tested band-limited noise (see \figref{stimuli}-b). The test patterns had fixed size but varying spatial frequency (x-axis) and contrast (y-axis). Because most of the contrast detection data are plotted as the function of sensitivity, we follow this convention and plot the sensitivity, which is the inverse of the contrast. This corresponds to the reversal of the axis on the logarithmic plots, which we use in our analysis. The other parameters of the stimuli are listed in Table~\ref{tab:params}. Although we tested multiple variants of each foundation model, here we show only the variant with the highest complexity and best performance on its original high-level tasks. The contour plots for other variants can be found in the \supplementary{}.  

The contour plots of foundation model responses are shown together with a contrast sensitivity function (castleCSF \cite{ashraf2024castlecsf}) in rows (a)--(d) of \figref{contour-main}. If the foundation models had the same contrast detection characteristics as the human eye, we would expect the smallest difference (the lowest $S_\ind{ac}$) contour line to follow the black dashed curve of castleCSF. This is not the case for any of the tested foundation models. Some models, and in particular DINOv2 and SD-VAE, show overall band-pass characteristics, in particular for noise and achromatic Gabors. SD-VAE has a drop of sensitivity at lower frequencies much larger than that of the visual system. The responses to chromatic patterns (rows (c) and (d)) tend to be less regular than to achromatic patterns and lack the shift toward lower frequencies, which is observed in the human data. Both OpenCLIP and SAM-2 show a very inconsistent response across the spatial frequencies. 

From the data, we can conclude that foundation models do not follow the sensitivity pattern of the visual system, but some may show a band-pass characteristic that is associated with the CSF. Many models show lower sensitivity at lower frequencies, which may indicate that those models obtained some invariance to (low-frequency) illumination through training. 

\paragraph{Luminance}

The sensitivity of the human eye increases with the luminance. In dim light, human contrast sensitivity increases proportionally to the square root of retinal illuminance, following the DeVries-Rose law. Conversely, in bright light, sensitivity follows Weber’s law, remaining independent of illuminance~\cite{rovamo1995neural}. In this experiment, we want to test whether the computer vision models lose sensitivity at lower luminance levels. Such a loss could be justified by camera noise, which increases in terms of contrast as the light intensity decreases~\cite{Aguerrebere_2013}. 

To produce contour plots, we generated a 2D array of image pairs in a similar manner as for spatial frequency variations in the section above, but instead of varying spatial frequency, we varied luminance, as shown in \figref{stimuli}-e. As a reminder, we did not pass the absolute luminance values directly to each model but instead converted them to display-encoded (gamma-corrected) sRGB space --- the colour space used for training datasets (see \figref{pipeline}). Such an encoding partially compensates for perceptual non-uniformity of luminance. 

The results, shown in row (e) of \figref{contour-main}, indicate a systematic drop in sensitivity with luminance for all tested models. The drop in sensitivity of those models is faster than that observed in the human data, in particular for OpenCLIP.

We can conclude that the models trained on sufficiently large datasets mimic the sensitivity of the visual system to luminance but with a faster drop in sensitivity. This, however, may be the result of using sRGB representation for training datasets, which were presumably mostly well-exposed and contained relatively little information in darker image regions. 


\paragraph{Area}
Stimulus area (size) significantly affects sensitivity, as larger stimuli activate more retinal cells. Sensitivity increases with area up to the saturation point, denoted as the critical area~\cite{Rovamo_1993}. The response to stimuli of different sizes tests the model's ability to pool information across the visual field. 

In this experiment, we vary the size of the Gaussian envelope limiting stimulus size to observe its effect on model responses. We follow the same procedure as in the two previous sections and generate pairs with a uniform field and a Gabor patch (see \figref{stimuli}-f). The parameters of the Gabor patch are listed in Table~\ref{tab:params}. 

The results, shown in row (f) of \figref{contour-main}, indicate that most models show summation across the area (the $S_\ind{ac}$ values increase with the area) and the rate of the increase varies across the models; DINO and SAM-2 have a smaller increase, SD-VAR has a higher increase of $S_\ind{ac}$ than the human data, and only DINOv2 roughly matches human performance. OpenCLIP shows no consistent patterns. We can conclude that many models show spatial pooling characteristic that shares the trend observed in human data, though the actual slope of the increase is typically different. 



\subsection{Contrast masking}
\label{sec:sub_CM}
Contrast masking explains the decreased visibility of a signal (test) due to the presence of a supra-threshold background (mask). The masking function defines the relationship between the threshold test contrast required for signal detection and the mask contrast. Put simply, a pattern is more difficult to detect in the presence of another pattern of similar spatial frequency and orientation. A typical masking characteristic of the visual system is shown in rows (g) and (h) of \figref{contour-main}. It consists of a relatively shallow segment at low mask contrast (near the detection threshold), with the slope increasing for high mask contrast. The shape of the curve is influenced by the specific properties of the mask and test signals~\cite{daly1992visible, watson1997model}. We will consider the case in which the masker is a sinusoidal grating of the same frequency as the test pattern (phase-coherent masking, row (g)) and when the masker is noise (phase-incoherent masking, row (h)) \cite{daly1992visible, gegenfurtner1992contrast, foley1994human}, as shown in~\figref{stimuli}-g,h.

First, we consider the fundamental form of \emph{phase-coherent masking} in which the masker image (reference in \figref{pipeline}) is a sinusoidal grating and the test image is the masker plus a Gabor patch of the same frequency and phase as the masker (see \figref{test_examples}g). Contrast masking data, shown as the dashed black curve in row (g) of \figref{contour-main}, shows the smallest contrast of the test Gabor that is detectable in the presence of the masker of a given contrast. As the contrast of the masker increases, the smallest detectable contrast of the test also needs to increase. However, such an increase starts only for a masker that has the contrast sufficiently high to be detected. If the contrast of the masker is near the detection threshold, we can observe a dipper effect --- the contrast detection is facilitated by a masker~\cite{foley1994human, watson1987efficiency, wilson1979four}. Such an effect can be only observed in phase coherent masking. 

As an example of \emph{phase-incoherent masking}, we will consider a masker with band-limited noise and a test with a Gabor patch --- see \figref{test_examples}h. The human detection thresholds for such masking patterns are similar to those for phase-coherent masking, except that the dipper effect disappears \cite{van1988effects}. 

The differences predicted by DINOv2 and OpenCLIP are surprisingly well-aligned with the human contrast masking data --- their responses roughly match the slopes of the human data. The alignment is stronger for the phase-incoherent masking. This is particularly notable for OpenCLIP, which did not show any consistent trends for contrast detection. Other models do not show strong alignment with the human data. 

Overall, computational models are better aligned with human data for contrast masking than for contrast detection. One possible explanation is that the signals that induce contrast masking are plentiful in natural images, but contrast detection stimuli, which involve barely noticeable patterns on uniform backgrounds, are rare. Therefore, computational models are more likely to pick up the characteristic that is well represented in the training datasets. 

\subsection{Supra-threshold contrast matching}
\label{sec:sub_SCM}

While contrast detection and contrast masking explain the just detectable (near-threshold) contrast, most of the vision tasks, such as detection or recognition, involve well-visible (supra-threshold) contrast. Supra-threshold human vision has been studied in contrast-matching experiments in which the magnitude of one contrast is visually matched to the magnitude of another contrast of a different frequency \cite{georgeson1975contrast} or luminance \cite{Kulikowski_1976,Peli_1995}. One of the most significant findings of those studies is \emph{contrast constancy} \cite{georgeson1975contrast} --- the ability of the visual system to match physical contrast across frequencies and luminance levels. The results of the seminal study of Georgeson and Sullivan \cite{georgeson1975contrast} on matching contrast across frequencies are shown as dashed lines in row (i) of \figref{contour-main}. At small contrast, the dashed lines show a band-pass shape that follows the contrast sensitivity function. However, as the contrast is increased, the lines become flat showing little influence of frequency on contrast perception. This is an important property that lets us see objects to have the same appearance regardless of the viewing distance. Such a scale invariance is also important for neural networks that are tasked to detect or recognize objects regardless of their size. 

We followed the experimental setup from~\cite{georgeson1975contrast}, where the reference was a 5\,cpd, 10\csdm{} sinusoidal grating, presented at eight distinct contrast levels $c_\ind{r}$. The test stimulus had the same luminance but a different spatial frequency $\rho_\ind{t}$. In~\cite{georgeson1975contrast}, observers adjusted the test stimulus contrast $c_\ind{t}$ until its apparent contrast matched that of the reference (contrast matching). In our experiments, we match contrast encodings of sinusoidal gratings: the ($S_\ind{ac}()$, \eqref{sa}), between a feature vector of a sinusoidal grating, $F(\rho, c)$ of frequency $\rho$ and contrast $c$, and a uniform field, $U=F(\rho_\ind{t}, 0)$. We find the test contrast $c_\ind{t}$ that minimizes the expression:
% \begin{equation}
%     \argmin_{c_\ind{t}} \left(\frac{S_\ind{ac}(F(\rho_\ind{r}, c_\ind{r}), U)}{S_\ind{ac}(F(\rho_\ind{r}, 1), U)} - \frac{S_\ind{ac}(F(\rho_\ind{t}, c_\ind{t}), U)}{S_\ind{ac}(F(\rho_\ind{t}, 1), U)}
%   \right)^2\,,
%   \label{eq:cm_cos}
% \end{equation}
\begin{equation}
    \argmin_{c_\ind{t}} \left(S_\ind{ac}(F(\rho_\ind{r}, c_\ind{r}), U) - S_\ind{ac}(F(\rho_\ind{t}, c_\ind{t}), U)
  \right)^2\,,
  \label{eq:cm_cos}
\end{equation}
where the reference frequency $\rho_\ind{r}=5$\,cpd. The denominators in the expression are used to normalize contrast across frequencies. We experimented with other contrast encodings, including a direct comparison of feature vectors, but the formula above resulted in the best contrast constancy properties across the models. 

The matching contrast predictions for foundation models are visualized as continuous lines in row (i) of \figref{Contour_1}. The plots show that only DINOv2 and OpenCLIP roughly follow the dashed contrast constancy lines. Both models show less attenuation (more constancy) at the highest spatial frequencies, which could be advantageous when the model needs to work with small-scale features. Both models show attenuation of low frequencies (below 1\,cpd), suggesting worse contrast constancy in that frequency range. Other models, including DINO and SAM-2, suffer from large instability across frequencies, or very heavy attenuation of low frequencies in the case of SD-VAE. To conclude, we can observe only partial contrast constancy for selected models. 

\subsection{Model alignment scores}

As the analysis of all 45 variants of the models is infeasible in the scope of this paper, we prepared quantitative results according to the method explained in \secref{model-alignment-score} and summarized them in \figref{All_score_result}. Those let us make three observations:

%The primary focus of this analysis is the visualization in~\figref{contour-main}, which reveals far more detailed insights than the quantitative results alone. However, due to space constraints, including all visualizations in the main text is infeasible. Thus, we present the quantitative results (Model Alignment Scores, defined in Section 3.2) in~\figref{All_score_result}.

First, ColorVideoVDP, which is a visual metric that models human low-level vision, is better aligned with the human data in almost all contrast detection tasks and in terms of contrast constancy, as expected. However, certain variants of OpenCLIP and DINOv2 can match or surpass the ColorVideoVDP alignment in terms of contrast masking. 
%although certain OpenCLIP and DINOv2 variants achieve higher alignment scores on phase-coherent masking tasks, foundation models generally do not surpass the alignment scores of the perception-based image quality metric, ColorVideoVDP, across most dimensions.
Second, the alignment scores of different foundation model variants (\eg, OpenCLIP) show significant variations and alignment scores appear unrelated to the complexity of the variants or their performance on higher-level tasks. %This suggests potential intrinsic randomness within these models, indicating an unreliable capture of low-level human visual characteristics.
Finally, DINOv2 variants, which have been trained to solve vision tasks, show the greatest alignment with the human data among all foundation models. %This could be an indicator that the training used in those models resulted in similar contrast encoding as that found in human vision.  

%This may indicate that future computer vision foundation models will be even more aligned with the low-level contrast encoding of the human visual system. 

%, indicating a current trend toward closer alignment with the human visual system.


% \subsection{Further Validation: Classification Masking}
% \begin{figure}[t]
%   \centering
%       \includegraphics[width=\linewidth]{images/Application_Noise_Masking_2.png}
%   \caption{Image Classification Noise Masking. The upper half presents image examples under varying mask noise contrast levels. The lower half shows accuracy curves across 20 generated mask contrast levels for four DINOv2 architectures.}
%   \label{fig:Application_Noise_Masking}
% \end{figure}

% \RM{This section needs to be updated or removed, as we discussed.}

% The tests above focus on perceptual experiments. To further validate the impact of HVS-model similarity on computer vision tasks, we evaluated noise masking in the image classification task. Specifically, we sampled 5 images from each of the 1000 classes in ImageNet’s validation dataset, adding 20 different noise masking contrasts to each image (using the noise mask in~\figref{test_examples} h), generating a total of 100k images. We then tested four full DINOv2 classification architectures. The results (\figref{Application_Noise_Masking}) clearly show stable classification accuracy at low masking contrasts, followed by a sharp decline at high masking contrasts, consistent with the fundamental characteristics of HVS. 



\section{Conclusions}

If we believe that the goal of both biological and computational low-level vision is to efficiently encode visual information, we can expect that computational models trained on large natural image datasets will share similarities with human vision. In this work, we find that selected computational models, e.g., variants of DINOv2 and OpenCLIP, show surprisingly high alignment with supra-threshold human contrast masking and contrast matching data, but little alignment with the near-threshold contrast detection. This means that computation models do not have the same ``bottlenecks'' as human vision, but, through training, they attain invariance and efficient contrast coding that resembles that of the visual system. We hope that our testing protocol with basic psychophysical stimuli will provide a useful tool for examining future computational models of vision.

%It is important to note that our study is not meant to indicate whether the foundation models should or should not follow the characteristics of the visual system. Instead, this work is meant to provide another tool to analyze large learning-based models, which are otherwise difficult to interpret. 

%Early computer vision models focused on modeling the human visual system (HVS), but with the rise of deep learning, the focus shifted to task accuracy for efficient industrial applications. While some researchers consider advanced DNN-based models and large vision foundation models to be effective HVS simulations, most perception scientists still find significant differences between these models and the HVS. This divergence poses challenges for further collaboration between machine and human vision researchers.



%Thus, we propose a standardized protocol to assess the similarities and differences between machine vision models and the HVS. This protocol includes nine distinct test types covering fundamental characteristics of human vision. We further evaluated 45 state-of-the-art machine vision models and observed that they exhibit similarity to the HVS in the area and luminance dimensions for near-threshold contrast detection. However, in the spatial frequency dimension, particularly in color perception, the models display notable divergence from the HVS. In supra-threshold contrast masking tasks, machine vision models also show similarities to the HVS. We conducted additional analyses to investigate the underlying causes of these similarities and differences, aiming to provide guidance for future research. Notably, our analysis requires further validation.

%\label{sec:exper}
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{LVM_sythestic_test}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
