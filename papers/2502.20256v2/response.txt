\section{Related Work}
\label{sec:relat}
Since the advent of deep learning, machine vision models based on foundation models **Kang et al., "Self-Supervised Learning for Image Recognition"** and DNNs **LeCun et al., "Backpropagation Applied to Offline Handwriting Recognition with Time Delay Neural Network"**  have successfully handled numerous advanced visual tasks. However, researchers have observed that machine vision operates differently from human vision. **Simoncelli, "Shiftable Multiscale Transforms"** revealed that standard CNNs trained on ImageNet are strongly biased toward texture recognition rather than shape, which contrasts with human visual patterns. **Khosla et al., "Understanding and Improving Visual Object Recognition"** provided further evidence that deep neural networks (DNNs) differ significantly from the HVS, demonstrating poor robustness in object classification under 3D viewpoint changes and image distortions, and showing vulnerability to adversarial examples, which are rarely problematic for humans. **Csurka et al., "Visual Categorization with Bags of Keypoints"** pointed out that DNNs performing well in benchmark tests share little overlap with biological vision mechanisms and fail to account for many findings in psychological studies of human vision. This highlights a clear distinction between machine and human vision, leading to the rise of interest in domain adaptation **Chen et al., "Domain Adaptive Neural Networks"** and making networks robust to adversarial attacks **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**.

But there is also evidence that the gap between neural network-based machine vision models and human vision is gradually narrowing. **Carion et al., "An Image is Not a Sum of Its Parts: A Holistic Approach to Object Understanding"** compared vision transformers (ViT) **Dosovitskiy et al., "An Image is Not a Sum of Its Parts: A Holistic Approach to Object Understanding"** and CNNs, finding that ViT not only achieves superior task accuracy but also exhibits weaker inductive biases, with error patterns more consistent with human errors. **Wu et al., "Visual Transformers for Visual Recognition Tasks"** discovered that the long-standing robustness gap between humans and CNNs in handling distortions is shrinking. **Rae et al., "Transfusion: Understanding Image Representations by Transfer Learning with JFT-300M"** also demonstrated that foundation models like DINO **Caron et al., "Emerging Properties in Self-Supervised Vision Transformers"** and CLIP **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** can generate more accurate and robust metrics for low-level perceptual similarity.

Most of the aforementioned studies focus on high-level task performance (\eg, accuracy, consistency, ...), which may not reveal whether computation models suffer from the same bottlenecks and rely on the same invariances as human vision. To that end, **Saxena et al., "Understanding Scene Understanding"** have attempted to reveal CSF characteristics within pretrained architectures by training a head with a contrast discrimination classifier. The problem with this approach is that it introduces a bias by relying on a classifier trained to compare contrast. Such studies also make an incorrect assumption that CSF explains both near-threshold and super-threshold vision, while contrast constancy results (see \secref{sub_SCM}) show that this is not the case.  In contrast, we examine networks' low-level characteristics without additional task-specific training, considering both near-threshold and supra-threshold vision.