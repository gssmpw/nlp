\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{2762} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}
\input{macros}

\begin{document}

\newcommand{\rone}{{\color{red}{Ykt8}}}
\newcommand{\rtwo}{{\color{ForestGreen}{KF6N}}}
\newcommand{\rthree}{{\color{blue}{6W3d}}}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\LaTeX\ Guidelines for Author Response}  % **** Enter the paper title here

% \maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\noindent
We are very grateful for the constructive feedback provided by the reviewers. We agree with the majority of the comments and will introduce the changes to improve the exposition and clarity of the work. We hope our work, demonstrating alignment between foundation models and low-level human vision, will spark interest and research into similarities and differences between human and machine vision. 

\noindent[\textbf{\rone}] \textbf{Linear colors vs. sRGB in ColorVideoVDP} ColorVideoVDP natively operates on linear color values, and if supplied with images in the sRGB space, it will convert them to a linear space (CIE XYZ). Therefore, conversion to sRGB was unnecessary. 

\noindent[\textbf{\rone}] 
\textbf{ColorVideoVDP as a baseline} 
Please note that we rely on psychophysical data or models (castleCSF) rather than ColorVideoVDP when evaluating the alignment of the foundation models.  ColorVideoVDP serves only as a baseline that we selected as it explicitly models low-level human vision characteristic. We did not include other visual metric (\eg, HDR-VDP) because they do not model chromatic vision and would further complicate the analysis. 

%includes a built-in display model that converts from linear luminance to the CIE 1931 XYZ and subsequently to the DKL color space (refer to Figure 2 and Section 3.1 of the ColorVideoVDP paper). Therefore, introducing an additional display model is unnecessary (Figure 2 in our main text). The DKL space is specifically used for the Contrast Sensitivity and Masking stage (via castleCSF). Since foundation models are trained on gamma-encoded sRGB images, test images remain in the sRGB space for consistency. Furthermore, the linear RGB space derived from gamma-decoded sRGB is linearly transformable to the XYZ space, ensuring no fundamental inconsistency.

%\noindent[\textbf{\rone}] \textbf{Other HVS metrics} ColorVideoVDP [SIGGRAPH 2024], integrating key parameters including color, luminance, spatial frequency, temporal frequency, and area, represents the most advanced HVS-based quality assessment model. As the successor to HDR-VDP-2, HDR-VDP-3, and FovVideoVDP, it incorporates the advanced CSF - castleCSF. The lack of a comparable parameter set in other models makes them unsuitable for tests in this work.

\noindent[\textbf{\rone}] \textbf{Variation in the alignment scores (Figure~6)} There is a variation in the alignment scores (Spearman $r_s$, RMSE) across different variants of each foundation model because each variant represents a distinct model with a unique architecture and potentially trained on different datasets. We could not discuss all 45 variants in the paper, but their complete results are shown in the supplementary materials (\verb+webpage/index.html+). The results (contour plots) are consistent with the summary presented in Figure~6.

%The significant variations in Spearman coefficients (Figure 6) stem from variations in the experimental results (Figure 5 and supplementary materials: \verb+webpage/index.html+) and are discussed in Section 4.4. Such variability is expected, as foundation models lack inherent HVS perceptual modeling, and training different sub-models yields diverse outcomes. This highlights a broader issue: the redundancy in foundation model parameter spaces, where increased complexity does not necessarily enhance alignment with HVS. While foundation models achieve high task accuracy via complex architectures and extensive parameter optimization, it may represent "task overfitting". In contrast, HVS models like ColorVideoVDP effectively capture human perception using few parameters (less than 100). We caution against interpreting Figure 6 scores in isolation, as experimental results in Figure 5 and the supplementary materials provide more detailed and accurate insights.

% \noindent[\textbf{\rone}][\textbf{\rtwo}] \textbf{Future Directions and Applications} This study establishes a critical link between HVS and machine vision (foundation model), offering new pathways for interdisciplinary research. Current machine vision models, while excelling in task accuracy, lack the interpretability, stability, and standardization often demanded in practical applications — areas where HVS insights are invaluable. Conversely, HVS research relies on costly and small-scale psychophysical experiments, limiting its scalability.

% To address these gaps, our findings advocate a bold paradigm shift: transitioning from black-box statistical models to interpretable mechanistic models inspired by HVS. This vision has two concrete implications: (1) For machine vision researchers, incorporating HVS mechanisms into vision foundation models can significantly enhance interpretability, robustness, and computational efficiency; (2) For HVS researchers, machine vision models can serve as cost-effective proxies for psychophysical experiments, accelerating data acquisition while preserving reliability. This synergy not only bridges two disciplines but also lays the groundwork for next-generation vision systems that are both efficient and biologically informed.

\noindent[\textbf{\rtwo}] \textbf{Clarifying Methodology}
The methodology is inspired by 2-alternative-forced-choice (2AFC) psychophysical experiments, in which a pair of test and reference stimuli is presented to an observer to measure their response. Similarly, as shown in Figure 2, we input a pair of images (test and reference) into identical image encoders and compute \( S_{\ind{ac}} \) in the feature space to quantify the response of the model (to the contrast between the pair of images). 

\noindent[\textbf{\rtwo}] \textbf{Psychophysical data}
We selected classic measurement results from seminal studies on contrast masking [Foley'94; Gegenfurtner'92] and contrast constancy [Georgeson \& Sullivan'75]. Later studies replicated those results. Contrast detection experiments rely on castleCSF as it distills data from 18 representative studies. 

%. The test images are fed into the foundation models to compute \( S_{\mathrm{ac}} \) (Figure 2 in main text). Psychophysical human data are derived from perceptual experiments involving human participants. This is a well-established research area, with typical experimental methods including the method-of-adjustment (MOA) and the 2-alternative-forced-choice (2AFC) procedure, often controlled using the QUEST method [Watson and Pelli 1983]. CSF models (\eg, elaTCSF [SIGGRAPH Asia 2024] and castleCSF [JOV 2024]) are developed based on human experimental data and HVS knowledge. By integrating CSF into the HVS-based VDP model, the ColorVideoVDP [SIGGRAPH 2024] used in our experiments was created, which also requires the test images as input.

% \noindent[\textbf{\rtwo}][\textbf{\rthree}] \textbf{Insights and analysis}
% We cannot give definite answer of why some models are more aligned (with the HVS) than the others. Instead, we provide our conjecture in the last paragraph of the corresponding section, e.g., lack of under-exposed images in the training could make the models less sensitive at low luminance levels. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/combined_plot_3.pdf}
  \caption{The performance of DINO/DINOv2/OpenCLIP on their classification tasks (from their GitHub repo) shows a potential correlation with the alignment scores in our masking/matching tests.}
  \label{fig:rebuttal}
  \vspace{-10pt}
\end{figure}

\noindent[\textbf{\rtwo}][\textbf{\rthree}] \textbf{Reasons for Model-HVS alignment} We cannot be certain what causes good or bad alignment with the visual system, but we provide our conjectures in the last paragraph of the corresponding section, \eg, explaining poor alignment at low luminance (line 354), or the reason for the better alignment with contrast masking data (line 431). We agree with the reviewers that finding such causes would be very insightful, but our approach can only indicate correlations, it cannot be used to find causality. 

%We also explored the correlation between other factors and HVS alignment scores. The results indicate a potential correlation between the models' performance on their original tasks and the alignment scores (Fig.~\ref{fig:rebuttal}). However, the correlation between model parameters and GFlops with the alignment scores is much weaker (around 0.3, not shown).


%We cannot give definite answer of why some models are more aligned (with the HVS) than others, as a thorough analysis exceeds the scope of this conference paper. Instead, we provide our conjecture in the last paragraph of the corresponding section, \eg, lack of under-exposed training images could make the models less sensitive at low luminance levels. We further propose the following conjectures:
%(1) Insufficient training data leads to model-HVS misalignment. For example, insufficient bit depth (\eg, 8 bit) in images fails to produce adequately low contrast levels, resulting in models exhibiting lower sensitivity than humans in near-threshold contrast detection tasks. The lack of sufficient high spatial frequency training data leads to inconsistent results for frequencies above 16 cpd.
%(2) Models suitable for different tasks exhibit varying alignment. In the Contrast Masking test, the image generator SD-VAE misaligns with humans, while task-flexible models like OpenCLIP and DINOv2 demonstrate stronger alignment.
%(3) Model complexity affects alignment, but no clear trend emerges. As shown in main text Figure 6 and Supplementary Table 1, for DINOv2, most best results are found in the more complex architectures, ViT-L and ViT-g, with none for ViT-S. Similarly, in OpenCLIP, ViT-L/14 achieves the best alignment in 4 out of 9 tests. Conversely, MAE exhibits the opposite pattern, with ViT-B outperforming larger architectures.

\noindent[\textbf{\rone}]\noindent[\textbf{\rtwo}][\textbf{\rthree}] \textbf{Practical implications} 
Our paper answers the question of whether foundation models exhibit low-level characteristics of the HVS, but it does not address the question of whether they should. Intrigued by the reviewers' comments, we checked whether our alignment scores (Fig. 6 in the paper) can indicate how well a model can perform on computer vision tasks. In Fig.~\ref{fig:rebuttal}, we show scatter plots of the alignment scores and different performance indicators for DINO, DINOv2, and OpenCLIP (data were not available for other models). The correlations (absolute value 0.55–0.8) suggest that good alignment with the contrast masking/matching characteristic can improve model's performance. Such results were consistent for the alignment of contrast masking and contrast matching, less so for detection (as expected). We did not find a strong correlation between alignment scores and the parameters of the model architecture (model size, number of parameters) or computational GFlops. We hope that future work can provide stronger evidence for the benefits of model-HVS alignment and spark interest in using low-level human vision models to introduce invariances or constraints into the training of the foundation models (via architectural changes, loss functions, or data augmentation).

%This study explores the connection between the HVS and machine vision (foundation models), aiming to open new pathways for interdisciplinary research. We hope our work will inspire interest in integrating well-studied low-level human visual mechanisms into foundation models. 

%The strength of foundation models lies in their training on large-scale datasets. However, even such datasets may fail to encompass all extreme conditions (\eg, extremely bright, dark, or high-frequency scenarios). Insights from low-level HVS models can introduce invariances or constraints into the training of foundation models, reducing reliance on even larger datasets. This integration can be achieved through architectural modifications, loss functions, or data augmentation strategies. Furthermore, HVS models are highly parameter-efficient, suggesting that HVS-inspired foundation models could achieve superior robustness and generalization with significantly fewer parameters. 
% However, if task-specific accuracy is the criterion, a successful foundation model does not need to replicate human vision, as computer vision can achieve greater efficiency or bypass the limitations of human vision.

% (1) HVS can inform foundation model design, as replicating human visual patterns may enhance image encoding efficiency or achieve scale and luminance invariance. This study finds that some advanced models (\eg, DINOv2) exhibit superior scale invariance (contrast constancy) compared to others. Additionally, HVS models require few parameters, and human vision demonstrates strong robustness and generalization, making this a promising direction for future research. (2) Nevertheless, a successful computer vision foundation model does not need to replicate human vision, as computer vision can be more efficient or circumvent the limitations of human vision. Moreover, foundation models tend to be more specialized, utilizing representations better suited to specific tasks.

% A different take (yet to be nicely phrased):

% * We hope our paper can spark interest in incorporating some well-research mechanisms of low-level human vision into foundation models. 

% * The strength of the foundation models comes from using very large datasets. Yet, even those datasets may not cover all edge cases.

% * Instead of using even larger datasets, we could use insights from the models of low-level human vision to introduce invariances or constraints into the training of the foundation models. This could be done via architectural changes, loss functions, or data augmentation. 

% * some models (e.g., DINOv2) have better scale invariance (contrast constancy) than the others 

% * a successful computer vision model does not need to replicate human vision

% ** computer vision can be more efficient or avoid bottlenecks of human vision

% ** foundation models tends to be more specialized and use the representations are that more suitable for a given task

% * but if it does, it means that replicating patterns of human vision can bring some inherent benefits, such as higher efficiency (of encoding) or invariance to scale or luminance. 


% \noindent[\textbf{\rtwo}][\textbf{\rthree}] \textbf{Model-HVS Alignment Reason Analysis} We begin by analyzing three aspects: training data, the model's original task, and model complexity, followed by analysis of specific model series. Note that these conclusions require further validation in future research. 

% (1) Training data significantly affects model alignment. Human sensitivity can reach 1000 (capable of detecting a contrast of 0.001), whereas 8-bit images cannot have such small contrast. This discrepancy results in most foundation models exhibiting lower sensitivity than humans in near-threshold contrast detection tasks. Furthermore, due to the lack of large-area high spatial frequency content in training datasets, many foundation models produce noisy results above 16 cpd. The 8-bit depth further limits dynamic range, reducing model sensitivity in low-luminance conditions ($< 1$ \csdm). (2) 

\noindent[\textbf{\rtwo}][\textbf{\rthree}] \textbf{Figure 5} (a)–(h) are contour plots of $S_{\mathrm{ac}}$ (Eq.~1), with different colored solid lines representing different $S_{\mathrm{ac}}$ values, where purple indicates the minimum difference ($S_{\mathrm{ac}}\to0$). We will improve the legend and caption for better clarity.

\noindent[\textbf{\rthree}] \textbf{Scope of study} We will adjust the title and writing.

\noindent[\textbf{\rthree}] \textbf{Experimental setup, image size} The image resolution of $224{\times}224$ corresponds to the crop size used to train DINO and DINOv2 (sec. 6.6 in [29]). This corresponds to a visual angle of approximately $3.7^\circ{\times}3.7^\circ$, roughly aligned with the extend of the foveal vision. The stimuli were exactly as those used in psychophysical experiments. Such stimuli span across multiple receptive fields of human vision and multiple patches/tokens of a foundation model. 

%As stated in line 215, all test images were set to a resolution of 224×224, which is one of the standard resolutions used during the pre-training of many foundation models. This corresponds to a visual angle of approximately 3.7×3.7 degrees, roughly aligned with the foveal region where human visual sensitivity peaks. Thus, this resolution is reasonable for both HVS models and foundation models. Furthermore, this test requires the models to simultaneously account for both global and local features, as it involves consideration of area and spatial frequency.


%%%%%%%%% REFERENCES
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

\end{document}
