\section{Related Work}
\label{sec:relat}
Since the advent of deep learning, machine vision models based on foundation models~\cite{ dosovitskiy2020image, oquab2023dinov2, kirillov2023segment} and DNNs~\cite{tian2023multi, lu2023efficient, zhang2025bridgenet, lv2025drkd}  have successfully handled numerous advanced visual tasks. However, researchers have observed that machine vision operates differently from human vision. \cite{geirhos2018imagenet} revealed that standard CNNs trained on ImageNet are strongly biased toward texture recognition rather than shape, which contrasts with human visual patterns. \cite{wichmann2023deep} provided further evidence that deep neural networks (DNNs) differ significantly from the HVS, demonstrating poor robustness in object classification under 3D viewpoint changes and image distortions, and showing vulnerability to adversarial examples, which are rarely problematic for humans. \cite{bowers2023deep} pointed out that DNNs performing well in benchmark tests share little overlap with biological vision mechanisms and fail to account for many findings in psychological studies of human vision. This highlights a clear distinction between machine and human vision, leading to the rise of interest in domain adaptation~\cite{cai2023rethinking} and making networks robust to adversarial attacks~\cite{yin2023generalizable}.

But there is also evidence that the gap between neural network-based machine vision models and human vision is gradually narrowing. \cite{tuli2021convolutional} compared vision transformers (ViT)~\cite{dosovitskiy2020image} and CNNs, finding that ViT not only achieves superior task accuracy but also exhibits weaker inductive biases, with error patterns more consistent with human errors. \cite{geirhos2021partial} discovered that the long-standing robustness gap between humans and CNNs in handling distortions is shrinking. \cite{ghildyal2024foundation, croce2024adversarially} also demonstrated that foundation models like DINO~\cite{caron2021emerging} and CLIP~\cite{radford2021learning} can generate more accurate and robust metrics for low-level perceptual similarity.

Most of the aforementioned studies focus on high-level task performance (\eg, accuracy, consistency, ...), which may not reveal whether computation models suffer from the same bottlenecks and rely on the same invariances as human vision. To that end, \cite{li2022contrast, akbarinia2023contrast} have attempted to reveal CSF characteristics within pretrained architectures by training a head with a contrast discrimination classifier. The problem with this approach is that it introduces a bias by relying on a classifier trained to compare contrast. Such studies also make an incorrect assumption that CSF explains both near-threshold and super-threshold vision, while contrast constancy results (see \secref{sub_SCM}) show that this is not the case.  In contrast, we examine networks' low-level characteristics without additional task-specific training, considering both near-threshold and supra-threshold vision.