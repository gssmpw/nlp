%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\calN}{\mathcal{N}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bC}{\mathbb{C}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\norm}[1]{\left\| #1\right\|}
\allowdisplaybreaks[4]

\newcommand{\zc}[1]{\textcolor{red}{[ZC: #1]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{multirow}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Subgraph GNNs for Graphs with Bounded Cycles}

\begin{document}

\twocolumn[
\icmltitle{On the Expressive Power of Subgraph Graph Neural Networks for Graphs with Bounded Cycles}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ziang Chen}{xxx}
\icmlauthor{Qiao Zhang}{yyy}
\icmlauthor{Runzhong Wang}{zzz}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA, United States}
\icmlaffiliation{yyy}{Sierra Canyon School, Chatsworth, CA, United States}
\icmlaffiliation{zzz}{Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, MA, United States}

\icmlcorrespondingauthor{Ziang Chen}{ziang@mit.edu}
\icmlcorrespondingauthor{Runzhong Wang}{runzhong@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Graph neural networks (GNNs) have been widely used in graph-related contexts. It is known that the separation power of GNNs is equivalent to that of the Weisfeiler-Lehman (WL) test; hence, GNNs are imperfect at identifying all non-isomorphic graphs, which severely limits their expressive power. This work investigates $k$-hop subgraph GNNs that aggregate information from neighbors with distances up to $k$ and incorporate the subgraph structure. We prove that under appropriate assumptions, the $k$-hop subgraph GNNs can approximate any permutation-invariant/equivariant continuous function over graphs without cycles of length greater than $2k+1$ within any error tolerance. We also provide an extension to $k$-hop GNNs without incorporating the subgraph structure. Our numerical experiments on established benchmarks and novel architectures validate our theory on the relationship between the information aggregation distance and the cycle size.
\end{abstract}


\section{Introduction}

Graph-based machine learning models, known as graph neural networks (GNNs) \citep{scarselli2008graph,wu2020comprehensive,zhou2020graph,kipf2016semi,velivckovic2017graph}, have emerged as powerful tools for interpreting and making predictions on data that can be represented as networks of interconnected points. These models excel at uncovering the underlying structure in graph data, leading to breakthroughs in numerous sectors, including but not limited to physics \citep{shlomi2020graph}, chemistry \citep{Reiser2022GraphNN,Fung2021BenchmarkingGN,Coley2018AGN}, bioinformatics \citep{zhang2021graph}, finance \citep{wang2021review}, electronic engineering \citep{liao2021review, he2021overview, lee2022graph}, and operations research \citep{gasse2019exact}.


From a theoretical standpoint, GNNs are utilized to learn or approximate functions on graph-structured data. It is crucial to analyze and understand the expressiveness of GNNs, that is, to determine the class of functions on graphs that these networks can effectively approximate. This analysis offers valuable insights that guide the design of more powerful and efficient GNN architectures.

A fundamental concept in the domain of GNNs is the message-passing mechanism \citep{Gilmer2017NeuralMP}, which progressively refines node representations by aggregating information from neighboring nodes. To formalize this process, consider a graph $ G $ composed of a vertex set $ V = \{v_1,v_2,\dots,v_n\}$ and an edge set $ E $, with each vertex $ v_i \in V $ initially endowed with a feature vector $h_i^{(0)}$. The message-passing scheme iteratively updates these vectors. At each iteration or layer, the update for a given vertex integrates input from its immediate neighbors through a combination of two steps: a local transformation and an aggregation. The local transformation applies a learnable function to each neighbor's feature, while the aggregation step combines these transformed features using a permutation-invariant operation such as summing, averaging, or taking the maximum value. This aggregated information is then merged with the current feature of the vertex to produce an updated representation.
Mathematically, in the $ l $-th iteration/layer, the updated feature $ h_i^{(l)} $ for vertex $ v_i $ can be expressed as:
\begin{equation}\label{eq:MP}
    \begin{split}
        h_i^{(l)} = f^{(l)} \bigg( h_i^{(l-1)}, \mathrm{AGGR}\Big(\Big\{\Big\{g^{(l)}(h_j^{(l-1)}) \quad\quad\quad\ \  & \\
        : v_j \text{ adjacent to } v_i\Big\}\Big\}\Big) \bigg), &
    \end{split}
\end{equation}
where $ f^{(l)} $ and $ g^{(l)} $ are learnable functions at layer $ l $, and $ h_j^{(l-1)} $ represents the feature of vertex $ v_j $ from the previous layer. The notation $ \{\{\cdot\}\} $ indicates a multiset that can accommodate duplicate elements, ensuring that all contributions from neighboring vertices are considered, even if some vertices share the same feature values.


Despite their empirical successes, message-passing graph neural networks (MP-GNNs) have limitations in terms of separation power or expressive power. Notably, they can fail to differentiate between certain non-isomorphic graphs. For instance, as illustrated in Figure~\ref{fig:not_iso_WL}, consider two distinct graphs where vertices of the same color start with identical features. Even though these graphs are structurally different, MP-GNNs cannot distinguish between them because vertices of matching colors will end up with the same feature representations after any number of message-passing rounds. This outcome persists irrespective of the specific choices for functions $ f^{(l)}, g^{(l)} $, or the aggregation method employed. The reason is that each vertex gathers indistinguishable aggregated information from its neighbors, leading MP-GNNs to perceive these non-isomorphic structures as identical.

    \begin{figure}[htb!]
    \centering
		\begin{tikzpicture}[
			constraintb/.style={circle, draw = blue, scale = 0.8},
			variablex/.style={circle, draw = red, scale = 0.8},
			]
			
			%Nodes
			\draw (-0.7,1.5) node[constraintb] (v1) {$v_2$};
			\draw (1.5,0.5) node[constraintb] (v2) {$v_4$};
			\draw (1.5,-0.5) node[variablex] (v3) {$v_5$};
			\draw (-0.7,-1.5) node[variablex] (v4) {$v_7$};
			\draw (0.7,1.5) node[variablex] (w1) {$v_3$};
			\draw (-1.5,0.5) node[variablex] (w2) {$v_1$};
			\draw (-1.5,-0.5) node[constraintb] (w3) {$v_8$};
			\draw (0.7,-1.5) node[constraintb] (w4) {$v_6$};
			
			%Lines
			\draw[-] (v1.east) -- (w1.west);
            \draw[-] (w1.south east) -- (v2.north);
            \draw[-] (v2.west) -- (w2.east);
            \draw[-] (w2.north) -- (v1.south west);

            \draw[-] (v3.south) -- (w4.north east);
            \draw[-] (w4.west) -- (v4.east);
            \draw[-] (v4.north west) -- (w3.south);
            \draw[-] (w3.east) -- (v3.west);
			
			%Nodes
			\draw (3.8,1.5) node[constraintb] (2v1) {$v_2$};
			\draw (6,0.5) node[constraintb] (2v2) {$v_4$};
			\draw (6,-0.5) node[variablex] (2v3) {$v_5$};
			\draw (3.8,-1.5) node[variablex] (2v4) {$v_7$};
			\draw (5.2,1.5) node[variablex] (2w1) {$v_3$};
			\draw (3,0.5) node[variablex] (2w2) {$v_1$};
			\draw (3,-0.5) node[constraintb] (2w3) {$v_8$};
			\draw (5.2,-1.5) node[constraintb] (2w4) {$v_6$};
			
			%Lines
			\draw[-] (2v1.east) -- (2w1.west);
            \draw[-] (2w1.south east) -- (2v2.north);
            \draw[-] (2v2.south) -- (2v3.north);
            \draw[-] (2w2.north) -- (2v1.south west);

            \draw[-] (2v3.south) -- (2w4.north east);
            \draw[-] (2w4.west) -- (2v4.east);
            \draw[-] (2v4.north west) -- (2w3.south);
            \draw[-] (2w3.north) -- (2w2.south);
			
		\end{tikzpicture}
    \caption{Two non-isomorphic graphs that cannot be distinguished by MP-GNNs or the WL test.}
	\label{fig:not_iso_WL}
    \end{figure}

    The separation and expressive power of MP-GNNs are closely tied to the Weisfeiler-Lehman (WL) test \citep{weisfeiler1968reduction}, a classic algorithm designed to tackle the graph isomorphism problem. At its core, the WL test operates as a color refinement procedure: initially, each vertex $ v_i $ is assigned a color $ C^{(0)}(v_i) $ based on its initial features. The algorithm then proceeds iteratively by updating the colors according to the following rule:
\begin{equation}\label{eq:WL}
    \begin{split}
        C^{(l)}(v_i) = \mathrm{HASH}\bigg( C^{(l-1)}(v_i), \Big\{\Big\{C^{(l-1)}(v_j) :\quad\quad & \\
        v_j \text{ adjacent to } v_i\Big\}\Big\}\bigg), &
    \end{split}
\end{equation}
which mirrors the structure of the update mechanism in \eqref{eq:MP}. Assuming the hash function is collision-free, two vertices will share the same color at iteration $ l $ if and only if they have identical colors and multisets of neighbors' colors at iteration $ l-1 $. The WL test concludes when the color partition stabilizes, typically within no more than $ n $ iterations, where $ n $ is the number of vertices. It deems two graphs isomorphic if their final color multisets match.

It has been demonstrated that MP-GNNs possess separation power equivalent to that of the Weisfeiler-Lehman (WL) test \citep{xu2018powerful}. This means that two graphs are identified as non-isomorphic by the WL test if and only if they produce distinct outputs in some MP-GNN. Furthermore, it has been proven in \citet{azizian2020expressive, geerts2022expressiveness} that GNNs can universally approximate any continuous functions whose separation capabilities are bounded above by the associated WL test. However, no polynomial-time algorithms are known to perfectly solve the graph isomorphism problem, implying that the WL test cannot distinguish certain pairs of non-isomorphic graphs, such as those illustrated in Figure~\ref{fig:not_iso_WL}. As a result, MP-GNNs are unable to represent or approximate all permutation-invariant or permutation-equivariant functions.

In response to these limitations, researchers have proposed alternative GNN architectures designed to enhance separation capabilities. A prominent approach in the literature involves the use of higher-order GNNs \citep{morris2019weisfeiler, maron2019provably, geerts2020expressive, geerts2020walk, azizian2020expressive, zhao2022practical, geerts2022expressiveness, morris2020weisfeiler}, which correspond to higher-order WL tests \citep{cai1992optimal}. Essentially, a $ k $-th order GNN assigns features to each $ k $-tuple of vertices and updates these features based on information from neighboring tuples. This mechanism allows for a more nuanced representation of graph structures, thereby improving the model's ability to distinguish between non-isomorphic graphs.

In this work, we explore an alternative technique that has gained prominence in recent literature \citep{zhang2021nested,zhao2021stars,bevilacqua2021equivariant,frasca2022understanding} to enhance the expressive power of MP-GNNs. This approach involves incorporating subgraph structures, moving beyond the reliance on vertex features from immediate neighboring nodes alone. Architectures that adopt this strategy are referred to as subgraph GNNs. By integrating subgraph information, these models can capture more complex and nuanced patterns within graph data, thereby improving their ability to distinguish between different graph structures \citep{feng2022powerful,huangboosting}. We further perform numerical validation with established benchmarks~\citep{gomez2018automatic} and GNN architectures~\citep{ying2021transformers} in graph learning, whereby a strong correlation between theoretical and numerical results is witnessed.


\paragraph{Our contributions} The main contributions of this paper are summarized as follows.
\begin{itemize}
    \item We rigorously characterize the separation power of subgraph GNNs by demonstrating that they can perfectly distinguish a large family of graphs with bounded cycle lengths. Specifically, under appropriate assumptions, we prove that GNNs leveraging subgraph structures within a distance $ k $ from the current vertex can distinguish all non-isomorphic graphs that do not contain cycles of length greater than $ 2k+1 $. Based on this result, we show that such $k$-hop subgraph GNNs can approximate any permutation-invariant/equivariant continuous functions on graphs without cycles of length greater than $ 2k+1 $.
    \item We extend the result to $k$-hop GNN that aggregates vertex features within distance $k$ but does not leverage the subgraph structure. Such GNN architecture is provably expressive on graphs with no cycles of length greater than $ 2k-1 $.
    \item We empirically validate our theoretical findings on the relationship between the information aggregation distance $k$ and the cycle size $(\sim 2k)$, providing valuable insights on selecting practical information aggregation distance.
\end{itemize}


    \paragraph{Organization}
    The rest of this paper will be organized as follows. We define subgraph GNNs and the associated WL test and introduce the motivation in \Cref{sec:subgraph_GNN}. Our main theory for the expressive power of $k$-hop subgraph GNNs is presented in \Cref{sec:main_result} with an extension to $k$-hop GNNs. \Cref{sec:numerics} presents the numerical experiments and the whole paper is concluded in \Cref{sec:conclude}.


\section{Subgraph graph neural networks and the Weisfeiler-Lehman test}
\label{sec:subgraph_GNN}

This section describes the motivation and sets up the basics for subgraph GNNs.

\subsection{Motivation}
Note that MP-GNNs \eqref{eq:MP} have a fundamental limitation that they fail to distinguish some non-isomorphic graphs, such as those in Figure~\ref{fig:not_iso_WL}. One idea to enhance the expressive power is to incorporate more information from neighboring vertices and subgraphs:
\begin{itemize}
    \item The aggregation in \eqref{eq:MP} uses $\calN(v_i)$, the set of neighbors of $v_i$. To incorporate additional information, one can define $d(u,v)$ as the shortest-path distance between $u$ and $v$ in the graph $G$ and
    \begin{equation*}
        \calN_k(v_i) := \left\{v \in G : d(v, v_i) \leq k \right\}, \quad k \geq 1.
    \end{equation*}
    
    \item Beyond the features of vertices in $\calN_k(v_i)$, one can also capture edge information, i.e., whether two vertices are connected. This means that the topology of $G|_{\calN_k(v_i)}$, the subgraph of $G$ restricted to $\calN_k(v_i)$ (known as the $k$-hop subgraph rooted at $v_i$), can be used to update the feature of $v_i$.
\end{itemize}
Let $(G, h^{(l-1)})_{v_i, k}$ denote the subgraph $G|_{\calN_k(v_i)}$ rooted at $v_i$, with each vertex having a feature from $h^{(l-1)}$. Accordingly, the vertex feature update rule is given by
\begin{equation}\label{eq:khop_GNN}
    h_i^{(l)} = f^{(l)}\left(h_i^{(l-1)}, g^{(l)}\left((G, h^{(l-1)})_{v_i, k}\right)\right).
\end{equation}
The functions $f^{(l)}$ and $g^{(l)}$ are learnable, with $g^{(l)}$ taking constant value on isomorphic rooted graphs. This scheme, termed the \emph{$k$-hop subgraph GNN}, has various applications and adaptations in the existing literature \cite{zhang2021nested, zhao2021stars, bevilacqua2021equivariant, frasca2022understanding}. Notably, the learnable function $g^{(l)}$ is often parameterized as another GNN applied to the smaller subgraph $(G, h^{l-1})_{v_i, k}$.

Consider the two non-isomorphic graphs in Figure~\ref{fig:not_iso_WL} indistinguishable by MP-GNNs. It can be seen that the $2$-hop subgraph GNN can successfully distinguish them. Specifically, the $2$-hop subgraphs rooted at $v_1$ are shown in Figure~\ref{fig:2hop} and are clearly non-isomorphic, indicating that $v_1$ in the two graphs in Figure~\ref{fig:not_iso_WL} will have different feature after one layer of the $2$-hop subgraph GNN, as long as $g^{(1)}$ can distinguish these two non-isomorphic subgraphs.
\begin{figure}[htb!]
    \centering
		\begin{tikzpicture}[
			constraintb/.style={circle, draw = blue, scale = 0.8},
			variablex/.style={circle, draw = red, scale = 0.8},
			]
			
			%Nodes
			\draw (-1.2,0) node[constraintb] (v1) {$v_2$};
			\draw (1.2,0) node[constraintb] (v2) {$v_4$};
			\draw (0,-1.2) node[variablex] (w1) {$v_3$};
			\draw (0,1.2) node[variablex] (w2) {$v_1$};
			
			%Lines
			\draw[-] (v1.south) -- (w1.west);
            \draw[-] (w1.east) -- (v2.south);
            \draw[-] (v2.north) -- (w2.east);
            \draw[-] (w2.west) -- (v1.north);
			
			%Nodes
			\draw (5,0) node[constraintb] (2v1) {$v_2$};
			\draw (2,-1.2) node[variablex] (2v4) {$v_7$};
			\draw (6,-1.2) node[variablex] (2w1) {$v_3$};
			\draw (4,1.2) node[variablex] (2w2) {$v_1$};
			\draw (3,0) node[constraintb] (2w3) {$v_8$};
			
			%Lines
			\draw[-] (2v1.south east) -- (2w1.north west);
            \draw[-] (2w2.south east) -- (2v1.north west);
            \draw[-] (2v4.north east) -- (2w3.south west);
            \draw[-] (2w3.north east) -- (2w2.south west);
			
		\end{tikzpicture}
    \caption{$2$-hop subgraphs rooted at $v_1$ for graphs in Figure~\ref{fig:not_iso_WL}}
	\label{fig:2hop}
    \end{figure}

    Another observation is that the $2$-hop subgraph GNN fails if we increase the cycle sizes in Figure~\ref{fig:not_iso_WL}—for instance, by changing one graph to have two $6$-cycles and the other a single $12$-cycle. In general, the $k$-hop subgraph GNN fails to distinguish between a graph with two $(2k+2)$-cycles and one with a single $(4k+4)$-cycle, though it succeeds when the cycle sizes are smaller. This suggests that larger cycles limit the separation power of the $k$-hop subgraph GNN.

    Such observation and motivation align with empirical findings in the literature. The ZINC dataset \cite{dwivedi2023benchmarking} consists of molecular graphs with no large cycles, and variants of subgraph GNNs have shown notable improvement over message-passing GNNs on this dataset \cite{zhang2023rethinking,zhang2021nested,zhao2021stars,bevilacqua2021equivariant,frasca2022understanding}.

At the end of this subsection, we comment that the above observation on $k$-hop subgraph GNNs requires that $g^{(l)}$ has a relatively strong separation power on the subgraph $(G, h^{(l-1)})_{v_i, k}$, which is achievable if the size of $(G, h^{(l-1)})_{v_i, k}$ is small or $G$ is sparse. However, if $G$ is relatively dense, then the size of $(G, h^{(l-1)})_{v_i, k}$ might be comparable with $G$ and the subproblem of finding expressive $g^{(l)}$ might be difficult.

\subsection{$k$-hop subgraph GNNs}

We rigorously define the $k$-hop subgraph GNNs in this subsection, for which we define the graph space first.

\begin{definition}[Space of graphs with vertex features]
    We use $\calG_{n,m}$ to denote the space of all undirected unweighted graphs of $n$ vertices with each vertex equipped with a feature in $\bR^{m}$. The space $\calG_{n,m}$ is equipped with the product topology of discrete topology (of graphs without vertex features) and Euclidean topology (of vertex features).
\end{definition}

We use $(G,H)$ to denote an element in $\calG_{n,m}$ where $G=(E,V)$ is an undirected unweighted graph, and $H=(h_1,h_2,\dots,h_n)$ is the collection of all vertex features.
Given $(G,H)\in\calG_{n,m}$ and $k\geq 1$, the $k$-hop subgraph GNN is defined as follows. 
\begin{itemize}
    \item The embedding layer maps each vertex feature $h_i\in\bR^m$ as an embedding vector
    \begin{equation*}
        h_i^{(0)} = f^{(0)}(h_i),
    \end{equation*}
    where $f^{(0)}$ is learnable.
    \item For $l=1,2,\dots,L$, the information aggregation layer computes $h_i^{(l)}$ according to \eqref{eq:khop_GNN} for $i=1,2,\dots,n$.
    \item There are two types of outputs. The graph-level output computes a real number for the whole graph, namely
    \begin{equation*}
        y = r\left(\mathrm{AGGR}\left(\left\{\left\{h_i^{(L)}:i\in\{1,2,\dots,n\}\right\}\right\}\right)\right),
    \end{equation*}
    where $r$ is learnable.
    The vertex-level output assigns a real number for each vertex:
    \begin{equation*}
        y_i = r(h_i^{(L)}), \quad i=1,2,\dots,n.
    \end{equation*}
\end{itemize}
In general, the intermediate vertex features $h_i^{(l)}$ can be defined in any topological space, while one usually uses Euclidean spaces in practice. Throughout this paper, we always consider continuous $f^{(l)}$, $g^{(l)}$, and $r$, guaranteeing that all $k$-hop subgraph GNNs are continuous.

\begin{definition}[Spaces of $k$-hop subgraph GNNs]
    We use $\calF_k$ to denote the collection of all $k$-hop subgraph GNNs with graph-level output, and use $\calF_{k,v}$ to denote the collection of all $k$-hop subgraph GNNs with vertex-level output.
\end{definition}

It is clear that a $k$-hop subgraph GNN with graph-level output is permutation-invariant, and a $k$-hop subgraph GNN with vertex-level output is permutation-equivariant, with respect to the following definition.

\begin{definition}[Permutation-invariant and permutation-equivariant functions]
    We say that a function $\Phi:\calG_{n,m}\to\bR$ is permutation-invariant if
    \begin{equation*}
        \Phi(\sigma\ast(G,H)) = \Phi(G,H),\quad\forall~\sigma\in S_n,
    \end{equation*}
    where $S_n$ denotes the permutation group on $\{1,2,\dots,n\}$ and $\sigma\ast(G,H)$ is the graph obtained by relabeling vertices in $(G,H)$ according to the permutation $\sigma$,
    and that a function $\Phi:\calG_{n,m}\to\bR^n$ is permutation-equivariant if 
    \begin{equation*}
        \Phi(\sigma\ast(G,H)) = \sigma(\Phi(G,H)),\quad\forall~\sigma\in S_n.
    \end{equation*}
\end{definition}

\subsection{$k$-hop subgraph WL test and the equivalent separation power}
\label{sec:equivalent_separation}

It is known that the MP-GNNs and the classic WL test have equivalent separation power \cite{xu2018powerful}. The same equivalence extends naturally for other variants of GNNs and WL test. In particular, we consider the $k$-hop subgraph GNN and the associated WL test stated in Algorithm~\ref{alg:WL}.

\begin{algorithm}[htb!]
\caption{$k$-hop Subgraph Weisfeiler-Lehman test}\label{alg:WL}
\begin{algorithmic}
\REQUIRE A graph $(G,H)\in\calG_{n,m}$ and iteration limit.
\STATE Initialize the vertex color
\begin{equation*}
    C_i^{(0)}=\mathrm{HASH}(h_i),\quad i=1,2,\dots,n
\end{equation*}
\WHILE{$l=1$, $2$, \dots, $L$}
    \STATE Refine the color
    \begin{equation}\label{eq:color-refine}
        C_i^{(l)} = \mathrm{HASH}\left(C_i^{(l-1)},(G,C^{(l-1)}_{v_i,k})\right),
    \end{equation}
    for $i=1,2,\dots,n$.
\ENDWHILE
\STATE \textbf{Output:} Color multiset $\{\{C_i^{(L)}:i\in\{1,2,\dots,n\}\}\}$. 
\end{algorithmic}
\end{algorithm}

We define two equivalence relationships below that characterize the separation power of the $k$-hop subgraph WL test.

\begin{definition}
    For $(G,H),(\hat{G},\hat{H})\in\calG_{n,m}$, denote $\{\{C_i^{(L)}:i\in\{1,2,\dots,n\}\}\}$ and $\{\{\hat{C}_i^{(L)}:i\in\{1,2,\dots,n\}\}\}$ as their final color multisets output by the $k$-hop subgraph WL test.
    \begin{itemize}
        \item[(i)] We say $(G,H)\ensuremath{\stackrel{k}{\sim}}(\hat{G},\hat{H})$ if $\{\{C_i^{(L)}:i\in\{1,2,\dots,n\}\}\}=\{\{\hat{C}_i^{(L)}:i\in\{1,2,\dots,n\}\}\}$ for any $L>0$ and any hash function.
        \item[(ii)] We say $(G,H)\ensuremath{\stackrel{k,v}{\sim}}(\hat{G},\hat{H})$ if $C_i^{(L)}=\hat{C}_i^{(L)},\ i=1,2,\dots,n$, for any $L>0$ and any hash function.
    \end{itemize}
\end{definition}

We remark that two multisets are identical if for any element, its multiplicities in two multisets are the same. Throughout this paper, we would say that two graphs $(G,H)$ and $(\hat{G},\hat{H})$ are indistinguishable by $k$-hop subgraph WL test if $(G,H)\ensuremath{\stackrel{k}{\sim}}(\hat{G},\hat{H})$.

The following theorem states the equivalence between the separation power of $k$-hop subgraph GNNs and the $k$-hop subgraph WL test.

\begin{theorem}\label{thm:equiv_GNN_WL}
    For any $(G,H),(\hat{G},\hat{H})\in\calG_{n,m}$ and any $k>0$, the following are equivalent:
    \begin{itemize}
        \item[(i)] $(G,H)\ensuremath{\stackrel{k}{\sim}}(\hat{G},\hat{H})$.
        \item[(ii)] $F(G,H)=F(\hat{G},\hat{H})$ for any $F\in\calF_k$.
        \item[(iii)] For any $F_v\in \calF_{k,v}$, there exists $\sigma\in S_n$ such that $F_v(G,H) = \sigma(F_v(\hat{G},\hat{H}))$. 
    \end{itemize}
    Moreover, $(G,H)\ensuremath{\stackrel{k,v}{\sim}}(\hat{G},\hat{H})$ if and only if $F_v(G,H)=F_v(\hat G,\hat H)$ for any $F_v\in\calF_{k,v}$.
\end{theorem}

The proof of \Cref{thm:equiv_GNN_WL} follows similar lines as in the proof of Theorem 4.2 in \cite{chen2022representing}, which is straightforward and is hence omitted. Similar equivalence also holds for two vertices in a single graph, as stated in the corollary below.

\begin{corollary}\label{cor:equiv_GNN_WL}
    For any $(G,H)\in\calG_{n,m}$ and any $k>0$. Let $\{\{C_i^{(L)}:i\in\{1,2,\dots,n\}\}\}$ be the color multiset output by the $k$-hop subgraph WL test. For any $i,i'\in\{1,2,\dots,n\}$, the following are equivalent:
    \begin{itemize}
        \item[(i)] $C_i^{(L)} = C_{i'}^{(L)}$ for any $L>0$ and any hash function.
        \item[(ii)] $F_v(G,H)_i = F_v(G,H)_{i'}$ for any $F_v\in\calF_{k,v}$. 
    \end{itemize}
\end{corollary}

\begin{proof}
    Apply \Cref{thm:equiv_GNN_WL} to $(G,H)$ and $\sigma\ast(G,H)$ where $\sigma$ is the permutation that switches $i,i'$ and keep all other indices unchanged.
\end{proof}



\section{Expressive power of $k$-hop subgraph GNNs}
\label{sec:main_result}

This section presents our main results on the expressive power of $k$-hop subgraph GNNs. We consider $k=1$ in \Cref{sec:1hop_subgraphGNN} and $k\geq 2$ in \Cref{sec:khop_subgraphGNN}, and discuss an extension in \Cref{sec:extension}.

\subsection{$1$-hop subgraph GNNs}
\label{sec:1hop_subgraphGNN}

Our main theorem on $1$-hop subgraph GNNs is that they can approximate any permutation-invariant/equivariant continuous functions on graphs without cycles of length greater than $3$.

\begin{theorem}\label{thm:GNN-1hop}
    Let $\bP$ be a Borel probability measure on $\calG_{n,m}$. Suppose that for $\bP$-almost surely $(G,H)$, the graph $G$ is connected and has no cycles of length greater than $3$. Then, the following hold.
    \begin{itemize}
        \item[(i)] For any $\epsilon,\delta>0$ and any permutation-invariant continuous function $\Phi:\calG_{n,m}\to\bR$, there exists $F\in\calF_1$ such that
        \begin{equation*}
            \bP\left[|F(G,H)-\Phi(G,H)|>\delta\right] <\epsilon.
        \end{equation*}
        \item[(ii)] For any $\epsilon,\delta>0$ and any permutation-equivariant continuous function $\Phi_v:\calG_{n,m}\to\bR^n$, there exists $F_v\in\calF_{1,v}$ such that
        \begin{equation*}
            \bP\left[\|F_v(G,H)-\Phi_v(G,H)\|>\delta\right] <\epsilon.
        \end{equation*}
    \end{itemize}
\end{theorem}

Throughout this paper, we always denote $\|\cdot\|$ as the standard $\ell_2$-norm on $\bR^n$.
We describe the main idea here and the detailed proof of \Cref{thm:GNN-1hop} is deferred to \Cref{sec:pf_1hop_subgraphGNN}. 
The classic Stone-Weierstrass theorem states that under mild conditions, a function class can universally approximate any continuous function if and only if it separates points, i.e., for any two different inputs, at least one function in that class has different outputs. Therefore, based on Stone-Weierstrass-type theorems, it suffices to show that $1$-hop subgraph GNNs have strong enough separation power to distinguish all non-isomorphic connected graphs with no cycles of length greater than $3$. Noticing the equivalence results in \Cref{sec:equivalent_separation}, one only needs to explore the separation power of $1$-hop subgraph WL test.

\begin{theorem}\label{thm:1hop-WL}
    Consider $(G,H),(\hat{G},\hat{H})\in\calG_{n,m}$. Suppose that $G$ and $\hat{G}$ are both connected and have no cycles of length greater than $3$. If $(G,H)\ensuremath{\stackrel{1}{\sim}}(\hat{G},\hat{H})$, then $(G,H)$ and $(\hat{G},\hat{H})$ must be isomorphic.
\end{theorem}

In the acyclic graph setting, it is proved in \citet{bamberger2022topological} that two trees indistinguishable by the classic WL test~\eqref{eq:WL} must be isomorphic. \Cref{thm:1hop-WL} can be viewed as a generalization of this result from~\citep{bamberger2022topological}.

The key idea in the proof of \Cref{thm:1hop-WL} is inductively constructing the isomorphism. We consider stabilized colors output by $1$-hop WL test without hash collision and start from two vertices of the same color, one from each graph. The $1$-hop subgraphs rooted at these two vertices are isomorphic, guaranteed by the same color. Then we inductively extend the subgraphs by adding neighbors of two vertices in the current subgraphs of the same color, which maintains the isomorphism, until they reach the whole graphs.

\subsection{$k$-hop subgraph GNNs with $k\geq 2$}
\label{sec:khop_subgraphGNN}


This subsection concerns the expressive power of $k$-hop subgraph GNNs for $k\geq 2$ and the main theory is an extension of \Cref{thm:GNN-1hop}, in the sense that $k$-hop subgraph GNNs ($k\geq 2$) can approximate any permutation-invariant/equivariant continuous functions on graphs without cycles of length greater than $2k+1$, but an additional assumption is required.

\begin{definition}
    A graph $(G,H)\in\calG_{n,m}$ is said to be $k$-separable if the following condition holds when the $k$-hop subgraph WL test terminates with stabilized colors and without hash collisions: For any three vertices $u, v_1, v_2$ with $d(u, v_1) = d(u, v_2) = k$ and $v_1 \neq v_2$, the colors of $v_1$ and $v_2$ are different.
\end{definition}

\begin{theorem}\label{thm:GNN-khop}
    Let $\bP$ be a Borel probability measure on $\calG_{n,m}$. Suppose that $\bP$-almost surely, $(G,H)$ is $k$-separable and $G$ is connected with no cycles of length greater than $2k+1$. Then, the following hold.
    \begin{itemize}
        \item[(i)] For any $\epsilon,\delta>0$ and any permutation-invariant continuous function $\Phi:\calG_{n,m}\to\bR$, there exists $F\in\calF_k$ such that
        \begin{equation*}
            \bP\left[|F(G,H)-\Phi(G,H)|>\delta\right] <\epsilon.
        \end{equation*}
        \item[(ii)] For any $\epsilon,\delta>0$ and any permutation-equivariant continuous function $\Phi_v:\calG_{n,m}\to\bR^n$, there exists $F_v\in\calF_{k,v}$ such that
        \begin{equation*}
            \bP\left[\|F_v(G,H)-\Phi_v(G,H)\|>\delta\right] <\epsilon.
        \end{equation*}
    \end{itemize}
\end{theorem}

The proof of \Cref{thm:GNN-khop} follows a similar framework as \Cref{thm:GNN-1hop} and is deferred to \Cref{sec:pf_khop_subgraphGNN}, where the key step is the following theorem that is an analog of \Cref{thm:1hop-WL}.

\begin{theorem}\label{thm:khop-WL}
    Consider $k\geq 2$ and $(G,H),(\hat{G},\hat{H})\in\calG_{n,m}$ that are both $k$-separable. Suppose that $G$ and $\hat{G}$ are both connected and have no cycles of length greater than $2k+1$. If $(G,H)\ensuremath{\stackrel{k}{\sim}}(\hat{G},\hat{H})$, then $(G,H)$ and $(\hat{G},\hat{H})$ must be isomorphic.
\end{theorem}

We remark that even restricted to $k$-separable graphs, the $k$-hop subgraph WL test \eqref{eq:color-refine} still has strictly stronger separation power compared to the classic WL test \eqref{eq:WL}. To illustrate this, we present two non-isomorphic $k$-separable graphs that can be distinguished by the $k$-hop subgraph WL test, but are, however, treated the same by the classic WL test. Let $k=3$, and consider the two graphs in Figure~\ref{fig2:counterex-WL} with initial vertex features as labeled by colors.
\begin{figure}[htb!]
    \centering
\begin{tikzpicture}[
roundnode/.style={circle, draw = blue, scale = 1},
roundnode1/.style={circle, draw = red, scale = 1},
roundnode2/.style={circle, draw = green, scale = 1},
roundnode3/.style={circle, draw = black, scale = 1},
roundnode4/.style={circle, draw = yellow, scale = 1},
roundnode5/.style={circle, draw = purple, scale = 1},
roundnode6/.style={circle, draw = gray, scale = 1},
]
%Nodes
\draw (-0.4,0) node[roundnode]        (v1)          { };
\draw (0.4,0) node[roundnode]        (v2)        { };
\draw (-1.2,0.6) node[roundnode1]        (v3)        { };
\draw (1.2,-0.6) node[roundnode1]        (v4)          { };
\draw (-1.2,-0.6) node[roundnode2]        (v5)        { };
\draw (1.2,0.6) node[roundnode2]        (v6)        { };
\draw (-1.2,-1.5) node[roundnode3]        (v7)          { };
\draw (1.2,1.5) node[roundnode3]        (v8)        { };
\draw (-0.4,-1.5) node[roundnode4]        (v9)        { };
\draw (0.4,1.5) node[roundnode4]        (v10)          { };
\draw (0.4,-1.5) node[roundnode5]        (v11)        { };
\draw (-0.4,1.5) node[roundnode5]        (v12)        { };
\draw (1.2,-1.5) node[roundnode6]        (v13)        { };
\draw (-1.2,1.5) node[roundnode6]        (v14)        { };

\draw (4,0.6) node[roundnode]        (v15)          { };
\draw (4,-0.6) node[roundnode]        (v16)        { };
\draw (2.8,0.6) node[roundnode1]        (v17)        { };
\draw (5.2,-0.6) node[roundnode1]        (v18)          { };
\draw (2.8,-0.6) node[roundnode2]        (v19)        { };
\draw (5.2,0.6) node[roundnode2]        (v20)        { };
\draw (2.8,-1.5) node[roundnode3]        (v21)          { };
\draw (5.2,1.5) node[roundnode3]        (v22)        { };
\draw (3.6,-1.5) node[roundnode4]        (v23)        { };
\draw (4.4,1.5) node[roundnode4]        (v24)          { };
\draw (4.4,-1.5) node[roundnode5]        (v25)        { };
\draw (3.6,1.5) node[roundnode5]        (v26)        { };
\draw (5.2,-1.5) node[roundnode6]        (v27)        { };
\draw (2.8,1.5) node[roundnode6]        (v28)        { };

%Lines
\draw[-] (v1.0) -- (v2.180);
\draw[-] (v1.143.1) -- (v3.323.1);
\draw[-] (v2.323.1) -- (v4.143.1);
\draw[-] (v1.216.9) -- (v5.36.9);
\draw[-] (v2.36.9) -- (v6.216.9);
\draw[-] (v3.270) -- (v5.90);
\draw[-] (v4.90) -- (v6.270);
\draw[-] (v5.270) -- (v7.90);
\draw[-] (v6.90) -- (v8.270);
\draw[-] (v7.0) -- (v9.180);
\draw[-] (v8.180) -- (v10.0);
\draw[-] (v9.0) -- (v11.180);
\draw[-] (v10.180) -- (v12.0);
\draw[-] (v11.0) -- (v13.180);
\draw[-] (v12.180) -- (v14.0);

\draw[-] (v15.270) -- (v16.90);
\draw[-] (v15.180) -- (v17.0);
\draw[-] (v16.0) -- (v18.180);
\draw[-] (v15.0) -- (v20.180);
\draw[-] (v16.180) -- (v19.0);
\draw[-] (v17.270) -- (v19.90);
\draw[-] (v18.90) -- (v20.270);
\draw[-] (v19.270) -- (v21.90);
\draw[-] (v20.90) -- (v22.270);
\draw[-] (v21.0) -- (v23.180);
\draw[-] (v22.180) -- (v24.0);
\draw[-] (v23.0) -- (v25.180);
\draw[-] (v24.180) -- (v26.0);
\draw[-] (v25.0) -- (v27.180);
\draw[-] (v26.180) -- (v28.0);
\end{tikzpicture}
    \caption{Two non-isomorphic $3$-separable graphs indistinguishable by the classic WL test, but distinguishable by the $3$-hop subgraph WL test.}
    \label{fig2:counterex-WL}
\end{figure}
 Notice that neither graph has a cycle with more than $2k+1=7$ vertices. Furthermore, for any vertex $u$ in either graph, any distinct vertices $v_1$ and $v_2$ with distance exactly $3$ from $u$ are of different colors. Thus, our results imply that these two graphs can be distinguished by the $3$-hop subgraph WL test. However, we can see that the coloring on both graphs immediately stabilizes when the classic WL test is applied, so the classic WL test cannot distinguish between the graphs. Moreover, this example is non-trivial in the sense that any $3$-hop subgraph in either graph is not the entire graph.

 At the end of this subsection, let us mention some related works analyzing the separation power of the subgraph GNNs. \citet{feng2022powerful} show that the separation power of subgraph GNNs is partially stronger than the third-order WL test. More related to our work, it is proved in \citet{huangboosting} that subgraph GNNs can ``count" cycles of length up to $4$ and some variant can ``count" cycles of length up to $6$. It is worth noting that the subgraph topology is integrated in a specific way in \citet{feng2022powerful,huangboosting}, while we always assume that $g^{(l)}$ has strong enough expressive/separation power on the $k$-hop subgraphs, without fixing the structure of $g^{(l)}$.

\subsection{An extension to $k$-hop GNNs}
\label{sec:extension}

This subsection extends our theory to $k$-hop GNNs, for which the vertex feature is updated by aggregating information in $\calN_k(v_i)$ without incorporating the subgraph structure
\begin{align}
    h_i^{(l)} = f^{(l)} \bigg( h_i^{(l-1)}, \mathrm{AGGR}\Big(\Big\{\Big\{g^{(l)}\big(h_j^{(l-1)},d(v_i,&v_j)\big) \notag \\
    : v_j\in \calN_k(v_i)\Big\}\Big\}\Big) \bigg) &, \label{eq:agg_func}
\end{align}
where $f^{(l)}$ and $g^{(l)}$ are learnable continuous functions.
The initial embedding layer and the final output layer of $k$-hop GNNs are the same as $k$-hop subgraph GNNs. 

\begin{definition}[Spaces of $k$-hop GNNs]
    We use $\calF_k'$ to denote the collection of all $k$-hop GNNs with graph-level output, and use $\calF_{k,v}'$ to denote the collection of all $k$-hop GNNs with vertex-level output.
\end{definition}

It is clear that the implementation of $k$-hop GNNs is cheaper than $k$-hop subgraph GNNs as some topology information about the subgraph is dropped. The trade-off is that $k$-hop GNNs have a bit weaker expressive/separation power. To rigorously introduce our next theorem on the the expressive power of $k$-hop GNNs, we require the associated $k$-hop WL test implements the color refinement as follows: 
\begin{equation*}
    \begin{split}
        C^{(l)}(v_i) = \mathrm{HASH}\bigg( C^{(l-1)}(v_i),\qquad\qquad\qquad\qquad\qquad & \\
        \Big\{\Big\{\big(C^{(l-1)}(v_j), d(v_i,v_j) \big): v_j\in \calN_k(v_i)\Big\}\Big\}\bigg).&
    \end{split}
\end{equation*}


\begin{definition}
    A graph $(G,H)\in\calG_{n,m}$ is said to be $k$-strongly separable if the following condition holds when the $k$-hop WL test terminates with stabilized colors and without hash collisions: For any two vertices $v_1, v_2$ with $d(v_1, v_2) \leq 2k$, the colors of $v_1$ and $v_2$ are different.
\end{definition}

Our main result in this subsection is that $k$-hop GNNs can universally approximate any permutation-invariant/equivariant continuous functions on $k$-strongly separable graphs with no cycles of length greater than $2k-1$.

\begin{theorem}\label{thm2:GNN-khop}
    Let $k\geq 2$ and let $\bP$ be a Borel probability measure on $\calG_{n,m}$. Suppose that $\bP$-almost surely, $(G,H)$ is $k$-strongly separable and $G$ is connected with no cycles of length greater than $2k-1$. Then, the following hold.
    \begin{itemize}
        \item[(i)] For any $\epsilon,\delta>0$ and any permutation-invariant continuous function $\Phi:\calG_{n,m}\to\bR$, there exists $F\in\calF_k'$ such that
        \begin{equation*}
            \bP\left[|F(G,H)-\Phi(G,H)|>\delta\right] <\epsilon.
        \end{equation*}
        \item[(ii)] For any $\epsilon,\delta>0$ and any permutation-equivariant continuous function $\Phi_v:\calG_{n,m}\to\bR^n$, there exists $F_v\in\calF_{k,v}'$ such that
        \begin{equation*}
            \bP\left[\|F_v(G,H)-\Phi_v(G,H)\|>\delta\right] <\epsilon.
        \end{equation*}
    \end{itemize}
\end{theorem}

Similarly, the proof of \Cref{thm2:GNN-khop} is built on that the $k$-hop WL test can distinguish all non-isomorphic $k$-strongly separable graphs.

\begin{theorem}\label{thm2:khop-WL}
    Let $k\geq 2$ and consider $k$-strongly separable graphs $(G,H),(\hat{G},\hat{H})\in\calG_{n,m}$. Suppose that $G$ and $\hat{G}$ are both connected and have no cycles of length greater than $2k-1$. If $(G,H)$ and $(\hat{G},\hat{H})$ are indistinguishable by the $k$-hop WL test, then they must be isomorphic.
\end{theorem}

We defer the detailed proof to \Cref{sec:pf_khop_GNN}. It is worth noting that when $k=1$, \Cref{thm2:khop-WL} holds even without the $1$-strong separability assumption. This is because that $1$-hop WL test coincides with the classic WL test and it is proved in \citet{bamberger2022topological} that the classic WL test can distinguish any non-isomorphic trees. In this sense, \Cref{thm2:khop-WL} can be regarded as an extension of \citet{bamberger2022topological}. Another related work is \citet{feng2022powerful} that proves that the separation power of the $k$-hop WL test is upper bounded by that of the third-order WL test. 

We also comment that Theorem~\ref{thm2:khop-WL} does not hold true if the $k$-strong separability condition is removed. Consider the two graphs in Figure~\ref{fig3:counterex-WL}, in which all vertices have the same initial feature.
\begin{figure}[htb!]
    \centering
\begin{tikzpicture}[
roundnode/.style={circle, draw = blue, scale = 1},
]
%Nodes
\draw (-2.82,1.2472) node[roundnode]        (v1)          { };
\draw (-3.54,0) node[roundnode]        (v2)        { };
\draw (-2.82,-1.2472) node[roundnode]        (v3)        { };
\draw (-1.38,-1.2472) node[roundnode]        (v4)          { };
\draw (-0.66,0) node[roundnode]        (v5)        { };
\draw (-1.38,1.2472) node[roundnode]        (v6)        { };
\draw (1.38,1.2472) node[roundnode]        (v7)          { };
\draw (0.66,0) node[roundnode]        (v8)        { };
\draw (1.38,-1.2472) node[roundnode]        (v9)        { };
\draw (2.82,-1.2472) node[roundnode]        (v10)          { };
\draw (3.54,0) node[roundnode]        (v11)        { };
\draw (2.82,1.2472) node[roundnode]        (v12)        { };

%Lines
\draw[-] (v1.240) -- (v2.60);
\draw[-] (v2.300) -- (v3.120);
\draw[-] (v3.0) -- (v4.180);
\draw[-] (v4.60) -- (v5.240);
\draw[-] (v5.120) -- (v6.300);
\draw[-] (v6.180) -- (v1.0);
\draw[-] (v1.300) -- (v4.120);
\draw[-] (v2.0) -- (v5.180);
\draw[-] (v3.60) -- (v6.240);

\draw[-] (v7.330) -- (v11.150);
\draw[-] (v8.30) -- (v12.210);
\draw[-] (v9.90) -- (v7.270);
\draw[-] (v10.150) -- (v8.330);
\draw[-] (v11.210) -- (v9.30);
\draw[-] (v12.270) -- (v10.90);
\draw[-] (v7.300) -- (v10.120);
\draw[-] (v8.0) -- (v11.180);
\draw[-] (v9.60) -- (v12.240);
\end{tikzpicture}
    \caption{The $k$-strong separability assumption is necessary in \Cref{thm2:GNN-khop}}
    \label{fig3:counterex-WL}
\end{figure}
Each vertex has three neighbors of distance $1$, two neighbors of distance $2$, and no neighbors of higher distance, so all vertices would have the same color in any $k$-hop WL test for any positive integer $k$. Thus, these two non-isomorphic graphs cannot be distinguished by $k$-hop WL test for any $k\geq 1$, which does not contradicts \Cref{thm2:khop-WL} as these two graphs are clearly not $k$-strongly separable. This example also illustrates that it is impossible to prove the conclusion of \Cref{thm2:khop-WL} without any condition.



\section{Numerical Experiments}
\label{sec:numerics}

\subsection{Experimental Setting and Dataset}

Based on our theoretical discoveries, we implement a family of models, namely $k$-hop Graphormers, following the Graphormer backbone and their official implementation~\citep{ying2021transformers}.
The numerical experiment aims to validate our theoretical findings on an established graph learning benchmark using a state-of-the-art neural network architecture.
% especially validating our theoretical finding between the longest cycle of the graph and the $\sim2k$ bound of expressiveness guarantee. 
All experiments are performed on a server with Intel 6230R CPU, single 2080Ti GPU, and 512GB RAM.

\begin{figure}[tb!]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.9\linewidth]{zinc-full-cycle.pdf}
    \vspace{-20pt}
    \caption{Statistics on the longest cycle length across all testing molecules in the ZINC dataset indicate that most molecules have a longest cycle of 6, aligning with the chemical intuition that 6-membered rings are particularly stable. The peaks observed at 9 and 10 also support the prevalence of common fused ring systems.
    % Given these findings, and considering our theoretical bounds on GNN expressiveness for cycle sizes no more than $2k-1$ or $2k+1$, we anticipate a notable performance boost around $k=3$ for our $k$-hop Graphormers.
    }
    \vspace{-10pt}
    \label{fig:zinc-full-cycle}
\end{figure}

We test our model on the ZINC graph-learning dataset, a subset of the ZINC database with 250,000 molecules developed by \citet{gomez2018automatic}. 
% The original paper develops several properties such as synthetic accessibility score~\citep{ertl2009estimation}, and qualitative estimate of drug-likeness~\citep{bickerton2012quantifying}, but 
On this dataset, the aim of the machine learning community mainly focuses on predicting the water-octanol partition coefficient (logP, \citet{wildman1999prediction}) by neural networks, whereby the ground truth labels are computed with cheminformatics tools. As shown in \Cref{fig:zinc-full-cycle}, most graphs in the ZINC testing dataset have a longest cycle of 6. Given these findings, and considering our theoretical bounds on GNN expressiveness for cycle sizes no more than $2k-1$ or $2k+1$, we anticipate a notable performance boost around $k=3$ for our $k$-hop Graphormers.

In line with peer methods, we report the mean absolute error (MAE) between GNN predictions and ground-truth labels.

% Following peer methods, we test on ZINC-Full with 250,000 molecules and ZINC-Subset with 12,000 molecules. 

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.9\linewidth]{zinc-full-mae.pdf}
    \vspace{-20pt}
    \caption{Mean absolute error (MAE) on ZINC testing dataset w.r.t.\ different values of $k$ for our $k$-hop Graphormer. Aligned with our theoretical results and dataset statistics in Fig.~\ref{fig:zinc-full-cycle}, we see a significant performance boost from $k=1$ to $3$, and the performance gets almost saturated for larger $k$s. All results are obtained with 3 random seeds.}
    \vspace{-10pt}
    \label{fig:zinc-full-mae}
\end{figure}


\subsection{$k$-hop Graphormer Implementation Details}
Our implementation of $k$-hop Graphormer follows the architecture proposed by \citet{ying2021transformers}. Graphormer replaces edge-wise message-passing with attention layers, whereby the connectivity information is incorporated into attention weights, which are initialized with pairwise shortest path distance. We modify its attention mechanism to fit Graphormers with our $k$-hop theory, by masking out all attention paths that are beyond the $k$-hop neighbors of the aggregated node. This implementation is in line with the aggregation function in \eqref{eq:agg_func} and $d(v_i,v_j)$ is the shortest path distance. Graphormer could be viewed as a special case of $k$-hop Graphormer by setting $k = \infty$. 
We reimplement and test all Graphormer models with the following default hyperparameters: Graphormer-Slim backbone, Adam with $2\times10^{-4}$ starting learning rate and ending $10^{-9}$, 12 Transformer layers, 80 hidden dimensions, 8 attention heads, 60,000 warm-up steps, and 400,000 total steps. 

% We also notice another improvement of Graphormer named Graphormer-GD by replacing the shortest path distance with a so-called resistance distance~\citep{zhang2023rethinking}. We follow their model design of a Gaussian encoder for connectivity information. Graphormer-GD is a $k$-hop Graphormer by replacing spatial distance with shortest path distance and $k\rightarrow\infty$.

% \begin{table*}[t]
% \caption{Mean Absolute Error (MAE) on ZINC test set.}
%   \vspace{2px}
%   \label{tab:zinc}
%   \small
%   % \fontsize{8}{9}\selectfont
%   % \scriptsize
%   % \setlength\tabcolsep{1pt}
%   \centering
%   % \resizebox{0.98\textwidth}{!}
%   { \renewcommand{\arraystretch}{1.0}
%     \begin{tabular}{clccc}
%     \toprule
%     \multirow{2}{*}{Method} & \multirow{2}{*}{Model} & \multirow{2}{*}{Params} & \multicolumn{2}{c}{Test MAE}\\
%     & & & ZINC-Subset & ZINC-Full \\ \midrule
    
%     \multirow{7}{*}{MPNNs}
%     & GIN~\citep{xu2019powerful} & 509,549 & 0.526$\pm$0.051 &  0.088$\pm$0.002 \\
%     & GraphSAGE~\citep{hamilton2017inductive} & 505,341 & 0.398$\pm$0.002 & 0.126$\pm$0.003 \\
%     & GAT~\citep{velivckovic2018graph} & 531,345 & 0.384$\pm$0.007 & 0.111$\pm$0.002 \\
%     & GCN~\citep{kipf2017semisupervised} & 505,079 & 0.367$\pm$0.011 & 0.113$\pm$0.002 \\

%     & MoNet~\citep{monti2017geometric} & 504,013 & 0.292$\pm$0.006 & 0.090$\pm$0.002 \\
%     & \textls[-25]{GatedGCN-PE\citep{bresson2017residual}} & 505,011 & 0.214$\pm$0.006 & - \\
%     & MPNN(sum)~\citep{gilmer2017neural} &  480,805 & 0.145$\pm$0.007 & - \\
%     & PNA~\citep{corso2020principal} & 387,155 & 0.142$\pm$0.010 & - \\ 
%     \midrule
%     % \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Higher-order\\GNNs\end{tabular}} & RingGNN~\citep{chen2019equivalence} & 178.03 & 527,283 &  0.353$\pm$0.019 & - \\
%     % & 3WLGNN~\citep{maron2019provably} & 179.35 & 507,603 &  0.303$\pm$0.068 & - \\
%     % \midrule
%     % \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Substructure-\\based GNNs\end{tabular}}
%     % & GSN~\citep{bouritsas2022improving} & - & $\sim$500k & 0.101$\pm$0.010 & - \\
%     % & CIN-Small~\citep{bodnar2021cellular} & - & $\sim$100k & 0.094$\pm$0.004 & 0.044$\pm$0.003 \\ \midrule
    
%     % \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Subgraph\\GNNs\end{tabular}}
%     % & NGNN~\citep{zhang2021nested} & - & $\sim$500k & 0.111$\pm$0.003 & 0.029$\pm$0.001 \\
%     % & DSS-GNN~\citep{bevilacqua2022equivariant} & - & 445,709 & 0.097$\pm$0.006 & - \\
%     % & GNN-AK~\citep{zhao2022stars} & - & $\sim$500k & 0.105$\pm$0.010 & - \\
%     % % & GNN-AK-CTX~\citep{zhao2022stars} & 0.093$\pm$0.002 & - \\
%     % & GNN-AK+~\citep{zhao2022stars} & - & $\sim$500k & 0.091$\pm$0.011 & - \\
%     % & SUN~\citep{frasca2022understanding} & 15.04 & 526,489 & 0.083$\pm$0.003 & - \\ \midrule
    
%     \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Graph\\Transformers\end{tabular}}
%     & GT~\citep{dwivedi2021generalization} & 588,929 & 0.226$\pm$0.014 & - \\
%     & SAN~\citep{kreuzer2021rethinking}  & 508,577 & 0.139$\pm$0.006 & - \\
%     & Graphormer~\citep{ying2021transformers}  & 489,321 & 0.122$\pm$0.006 & 0.052$\pm$0.005 \\ 
%     & Graphormer-GD \citep{zhang2023rethinking} & 502,793 & 0.081$\pm$0.009 & 0.025$\pm$0.004 \\
%     \midrule
%     & Graphormer-GD (our reimplementation)  & 502,793 &  &  0.0581\\
%     & $k$-hop Graphormer ($k=4$, ours)  & 502,793 &  &  0.0535$\pm$0.002\\
%      \bottomrule
%     \end{tabular}
%     }
%     \vspace{-8pt}
% \end{table*}


\subsection{Results and Discussions}
In experiments, we evaluate our $k$-hop Graphormers with $k=1,2,3,4,5,7,10$, and infinity i.e.\ feature could be aggregated from all nodes by attention.
As shown in \Cref{fig:zinc-full-mae}, $1$-hop Graphormer performs relatively inferior, and the performance improves significantly with larger $k$ and reaches a consistently good performance after $k>3$, validating our theoretical discovery that a $k$-hop Graphormer has the expressiveness to learn graphs with longest cycles of $\sim2k$. The performance becomes nearly saturated between $k=3$ and $10$, which is within expectation as the marginal improvements in covered graphs are less significant if read together with Figure~\ref{fig:zinc-full-cycle}. We also see a slight performance drop in the special case of $k=\infty$, indicating that too many message paths may introduce more noise than information to the model performance.
A smaller $k$ also brings potential efficiency improvements as it has fewer message-passing paths than an infinite $k$. It is not obvious for the ZINC dataset but will become valuable for larger-sized graphs.

\begin{table}[t]
\caption{Mean Absolute Error (MAE) on ZINC test set.}
  \vspace{2px}
  \label{tab:zinc}
  \small
  % \fontsize{8}{9}\selectfont
  % \scriptsize
  % \setlength\tabcolsep{1pt}
  \centering
  \resizebox{\linewidth}{!}
  { \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{clc}
    \toprule
    Aggregation & Model & Test MAE\\\midrule
    \multirow{4}{*}{1-hop} & GIN~\citep{xu2019powerful} & 0.088$\pm$0.002 \\
    & GraphSAGE~\citep{hamilton2017inductive} & 0.126$\pm$0.003 \\
    & GAT~\citep{velivckovic2018graph} & 0.111$\pm$0.002 \\
    % & GCN~\citep{kipf2017semisupervised} & 0.113$\pm$0.002 \\
    & \textbf{$k$-hop Graphormer} ($k=1$)  & 0.459$\pm$0.001\\
    \midrule
    \multirow{2}{*}{$k$-hop} & \textbf{$k$-hop Graphormer} ($k=4$)  & 0.054$\pm$0.002\\
     & \textbf{$k$-hop Graphormer} ($k=10$)  & 0.053$\pm$0.001\\
    \midrule
    \multirow{2}{*}{full-graph} & Graphormer~\citep{ying2021transformers}  & 0.058$\pm$0.005 \\ 
     & \textbf{$k$-hop Graphormer} ($k=\infty$)  & 0.058$\pm$0.005\\
     \bottomrule
    \vspace{-20pt}
    \end{tabular}
    }
\end{table}

When compared with other 1-hop aggregation methods (traditional message-passing GNNs), as shown in Table~\ref{tab:zinc}, our 1-hop Graphormer has a similar MAE in scale but is inferior to 1-hop GNNs. With an appropriate $k$ that reflects the distribution of cycle lengths in data, $k$-hop Graphormers reach a significant improvement. Our model degenerates to the original Graphormer when $k\rightarrow \infty$.
It is also worth noting that this benchmark is nearly saturated, for example, \citet{zhang2023rethinking} improves performance with a thorough search of the hyperparameter space but their official implementation does not include the hyperparameters to reproduce the reported results. Considering all of these, we believe it is beyond the scope of this paper to develop new state-of-the-art. Therefore, our main focus of experiments is to validate our theoretical results, i.e., the correlation between $k$ values and $k$-hop Graphormers' performance.

% It is also worth noting that Graphormer-GD~\citep{zhang2023rethinking} reports a significant improvement over the Graphormer baseline with 0.025$\pm$0.004 MAE. However, our reimplementation of it with our hyperparameters yields a similar performance compared to the original Graphormer, indicating that the empirical improvement might mainly come from a more thorough search of the hyperparameter space. The official code from \citet{zhang2023rethinking} does not include the hyperparameters to reproduce the reported results. Considering the accuracy improvement on the ZINC dataset is nearly saturated, it is beyond the scope of this paper to develop state-of-the-art models on a saturated benchmark. Therefore, our main focus of experiments is the validation of our theoretical results, i.e.\ the correlation between $k$ values and $k$-hop Graphormers' performance.

% Therefore, we use the same hyperparameters from \citet{ying2021transformers} and mainly focus on the validation of our theoretical results, i.e.\ the correlation between $k$ values and $k$-hop Graphormers' performance.




\section{Conclusion}
\label{sec:conclude}

This paper rigorously evaluates the efficiency of GNNs that leverage subgraph structures, particularly on graphs with bounded cycles, which represent many real-world datasets. In particular, we prove that $k$-hop subgraph GNNs can reliably predict properties of graphs without cycles of length greater than $2k+1$, which is unconditionally if $k=1$ and requires an additional assumption for $k\geq 2$. The theory is extended to $k$-hop GNNs without considering the subgraph structure for graphs with no cycles of length greater than $2k-1$. The correlation between $k$-hop GNNs and $\sim2k$ cycle size is further validated by numerical experiments.

Let us also comment on the limitations of the current work. Firstly, it is unclear whether the $k$-separability in \Cref{thm:GNN-khop} can be removed or not. Secondly, though examples in Figure~\ref{fig3:counterex-WL} illustrate that \Cref{thm2:GNN-khop} cannot hold unconditionally, it remains unknown whether the $k$-strong separability can be weakened or what the weakest assumption is. Those directions deserve future research.


% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
Z. Chen and Q. Zhang thank the MIT PRIMES-USA program for making this paper possible.

% \textbf{Do not} include acknowledgements in the initial version of the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and usually should) include acknowledgements.  Such acknowledgements should be placed at the end of the section, in an unnumbered section that does not count towards the paper page limit. Typically, this will include thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in an unnumbered section at the end of the paper (co-located with Acknowledgements -- the two may appear in either order, but both must be before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice:

% ``This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review.

% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs for Section~\ref{sec:1hop_subgraphGNN}}
\label{sec:pf_1hop_subgraphGNN}

The proof of \Cref{thm:GNN-1hop} is based on \Cref{thm:1hop-WL} and its corollary.

\begin{corollary}\label{cor:1hop-WL}
    Consider any $(G,H)\in\calG_{n,m}$ where $G$ is connected and has no cycles of length greater than $3$. Let $\{\{C_i^{(L)}:i\in\{1,2,\dots,n\}\}\}$ be the color multiset output by the $1$-hop subgraph WL test. For any $i,i'\in \{1,2,\dots,n\}$, if $C_i^{(L)} = C_{i'}^{(L)}$ holds for any $L>0$ and any hash function, then we have for any permutation-equivariant function $\Phi:\calG_{n,m}\to\bR^n$ that $\Phi(G,H)_i = \Phi(G,H)_{i'}$.
\end{corollary}

We will postpone the proofs of \Cref{thm:1hop-WL} and \Cref{cor:1hop-WL} and first prove Theorem~\ref{thm:GNN-1hop} (i) using \Cref{thm:1hop-WL} and the Stone-Weierstrass theorem.

\begin{proof}[Proof of Theorem~\ref{thm:GNN-1hop} (i)]
    There exists a compact and permutation-invariant subset $X\subseteq \calG_{n,m}$ such that $\bP[X]>1-\epsilon$ and that for any $(G,H)\in X$, $G$ is connected and has no cycles of length greater than 3. Due to Theorem~\ref{thm:1hop-WL} and the permutation-invariant property of $\Phi$, $\Phi|_X:X\to\bR$ induces a continuous map on the quotient space $\widetilde{\Phi|_X}: X/\ensuremath{\stackrel{1}{\sim}} \to \bR$
    By the same reason, for $F\in\calF_{1}$, $F|_X:X\to\bR$ also induces a continuous map $\widetilde{F|_X}: X/\ensuremath{\stackrel{1}{\sim}} \to \bR$.
    Consider any $(G,H),(\hat{G},\hat{H})\in X$ that represent different elements in $ X/\ensuremath{\stackrel{1}{\sim}}$, \Cref{thm:equiv_GNN_WL} guarantees that there exists $F\in\calF_1$ such that $F(G,H)\neq F(\hat{G},\hat{H})$, suggesting that $\{\widetilde{F|_X}:F\in\calF_1\}$ separates points on $X/\ensuremath{\stackrel{1}{\sim}}$. Therefore, by the Stone-Weierstrass theorem, one can conclude that there exists $F\in \calF_1$ such that
    \begin{equation*}
        \left\|\widetilde{F|_X}- \widetilde{\Phi|_X}\right\|_{L^\infty(X/\ensuremath{\stackrel{1}{\sim}})} <\delta,
    \end{equation*}
    which implies that
    \begin{equation*}
        |F(G,H)-\Phi(G,H)|<\delta,\quad\forall~(G,H)\in X.
    \end{equation*}
    Thus, it holds that
    \begin{equation*}
        \bP\left[|F(G,H)-\Phi(G,H)|>\delta\right]\leq \bP[\calG_{n,m}\backslash X] <\epsilon,
    \end{equation*}
    which completes the proof.
\end{proof}

The proof of \Cref{thm:GNN-1hop} (ii) requires a generalized Stone-Weierstrass theorem for equivariant functions. 

\begin{theorem}[Generalized Stone-Weierstrass theorem, Theorem 22 in \cite{azizian2020expressive}]\label{thm:equivariant_stone_weierstrass}
    Let $X$ be a compact topological space and let $\mathbf{G}$ be a finite group that acts continuously on $X$ and $\bR^n$. Define the collection of all equivariant continuous functions from $X$ to $\bR^n$ as follows:
    \begin{equation*}
        \calC_e(X,\bR^n) = \{F\in \calC(X,\bR^n) : F(g\ast x) = g\ast F(x),\ \forall~x\in X,g\in\mathbf{G}\}.
    \end{equation*}
    Consider any $\calF\subset \calC_e(X,\bR^n)$ and any $\Phi\in \calC_e(X,\bR^n)$. Suppose the following conditions hold:
    \begin{itemize}
        \item[(i)] $\calF$ is a subalgebra of $\calC(X,\bR^n)$ and $\mathbf{1}\in \calF$.
        \item[(ii)] For any $x,x'\in X$, if $f(x) = f(x')$ holds for any $f\in\calC(X,\bR)$ with $f\mathbf{1}\in \calF$, then for any $F\in \calF$, there exists $g\in \mathbf{G}$ such that $F(x) = g\ast F(x')$.
        \item[(iii)] For any $x,x'\in X$, if $F(x) = F(x')$ holds for any $F\in \calF$, then $\Phi(x) = \Phi(x')$.
        \item[(iv)] For any $x\in X$, it holds that $\Phi(x)_i = \Phi(x)_{i'},\ \forall~(i,i')\in I(x)$, where $$I(x) = \left\{(i,i')\in \{1,2,\dots,n\}^2: F(x)_i = F(x)_{i'},\ \forall~F\in\calF\right\}.$$
    \end{itemize}
    Then for any $\epsilon>0$, there exists $F\in\calF$ such that
    \begin{equation*}
        \sup_{x\in X}\| \Phi(x) - F(x) \|<\epsilon.
    \end{equation*}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:GNN-1hop} (ii)]
    There exists a compact and permutation-invariant subset $X\subseteq \calG_{n,m}$ such that $\bP[X]>1-\epsilon$ and that for any $(G,H)\in X$, $G$ is connected and has no cycles of length greater than 3. The rest is to
    apply \Cref{thm:equivariant_stone_weierstrass} on $X$ and $\calF = \calF_{1,v}$, for which one needs to verify the four conditions in \Cref{thm:equivariant_stone_weierstrass}.
    \begin{itemize}
        \item \emph{Verification of Condition (i).} By its construction, $\calF_{1,v}$ is a subalgebra of $\calC(X,\bR)$. In addition, $\mathbf{1}\in\calF_{1,v}$ if the output function $r$ always takes the constant value $1$. 
        \item \emph{Verification of Condition (ii).} Notice that $\calF_1\mathbf 1\subset \calF_{1,v}$. If $F(G,H) = F(\hat{G},\hat{H}),\ \forall~F\in\calF_1$, then \Cref{thm:equiv_GNN_WL} implies that for any $F_v\in\calF_{1,v}$, one has $F_v(G,H) = \sigma(F_v(\hat{G},\hat{H}))$ for some permutation $\sigma\in S_n$.
        \item \emph{Verification of Condition (iii).} Suppose that $F_v(G,H) = F_v(\hat{G},\hat{H}),\ \forall~F_v\in\calF_{1,v}$. By \Cref{thm:equiv_GNN_WL}, it holds that $(G,H)\ensuremath{\stackrel{1,v}{\sim}}(\hat{G},\hat{H})$. By \Cref{thm:1hop-WL}, we know that $(G,H)$ and $(\hat{G},\hat{H})$ are isomorphic, i.e., $(G,H) = \sigma\ast (\hat{G},\hat{H})$ for some $\sigma\in S_n$, which leads to
        \begin{equation}\label{eq1:Phiv}
            \Phi_v (G,H) = \Phi_v(\sigma\ast (\hat{G},\hat{H})) = \sigma(\Phi_v (\hat{G},\hat{H})).
        \end{equation}
        Moreover, it follows from $\sigma\ast (\hat{G},\hat{H}) = (G,H)\ensuremath{\stackrel{1,v}{\sim}}(\hat{G},\hat{H})$ and \Cref{cor:1hop-WL} that
        \begin{equation}\label{eq2:Phiv}
            \Phi_v(\hat{G},\hat{H})_i = \Phi_v(\hat{G},\hat{H})_{\sigma(i)},\quad\forall~i\in\{1,2,\dots,n\}.
        \end{equation}
        Then one can conclude $\Phi_v(G,H) = \Phi_v(\hat
        G,\hat H)$ by combining \eqref{eq1:Phiv} and \eqref{eq2:Phiv}.
        \item \emph{Verification of Condition (iv).} Condition (iv) is a direct corollary of \Cref{cor:equiv_GNN_WL} and \Cref{cor:1hop-WL}.
    \end{itemize}
\end{proof}

Finally, we present the proof of \Cref{thm:1hop-WL} and \Cref{cor:1hop-WL}.

\begin{proof}[Proof of Theorem~\ref{thm:1hop-WL}]
    Let $\mathcal{A}=(A_1,A_2,\ldots,A_s)$ be an $s$-tuple of subgraphs of a graph $A$, and let $\mathcal{B}=(B_1,B_2,\ldots,B_s)$ be an $s$-tuple of subgraphs of a graph $B$. Let $V(\mathcal{A})$ be the union of the vertices in $A_1,A_2,\ldots,A_s$, and let $V(\mathcal{B})$ be the union of the vertices in $B_1,B_2,\ldots,B_s$. We say that $\mathcal{A}$ and $\mathcal{B}$ are isomorphic if there exists a bijective map of vertices of $V(\mathcal{A})$ to vertices of $V(\mathcal{B})$ such that for any $i \in \{1,2,\ldots,s\}$,
    \begin{itemize}
        \item all vertices of $A_i$ are mapped to vertices of $B_i$ with the same label/feature and vice versa
        \item all edges of $A_i$ are mapped to edges of $B_i$ and vice versa.
    \end{itemize}

    Consider $(G,H)\ensuremath{\stackrel{1}{\sim}}(\hat{G},\hat{H})$, i.e., $(G,H)$ and $(\hat{G},\hat{H})$ cannot be distinguished by the $1$-hop subgraph WL test. When there are no hash collisions and the colors stabilize, the multisets of final colors of vertices in $G$ and $\hat{G}$ are the same, and any $v_1 \in G$ and $v_2 \in \hat{G}$ with the same color must have isomorphic $1$-hop subgraphs.
    
    We abbreviate an induced subgraph of $(G,H)$ or $(\hat{G},\hat{H})$ as its set of vertices. For any set $S$ of vertices, let $\mathcal{N}(S)$ be the set of all vertices in $S$ or neighboring some vertex of $S$. We prove the following statement by induction: for any $t \in \{1,2,\ldots,|G|\}$, there exist connected isomorphic subsets $S_1 \subseteq V(G)$ and $S_2 \subseteq V(\hat{G})$ of size $t$, where $V(G)$ and $V(\hat{G})$ are vertex sets of $G$ and $\hat{G}$ respectively, such that $(S_1,\mathcal{N}(S_1))$ and $(S_2,\mathcal{N}(S_2))$ are isomorphic. For the base case, choose any two vertices in $G$ and $\hat{G}$ with the same color. For the inductive step, suppose that $S_1$ and $S_2$ are sets of size $t<|V(G)|$, and we want to find two sets $S_1'$ and $S_2'$ with size $t+1$ that satisfy the inductive statement. Let $v_1$ be a vertex not in $S_1$ adjacent to a vertex in $S_1$, and let $v_2$ be the image of $v_1$ under the isomorphism $f: (S_1,\mathcal{N}(S_1))\to (S_2,\mathcal{N}(S_2))$, i.e., $v_2=f(v_1)$. Let $\mathcal{N}(v_1)$ and $\mathcal{N}(v_2)$ be the sets of vertices with distance at most $1$ from $v_1$ and $v_2$, respectively. Then $\calN(v_1)$ and $\calN(v_2)$ are isomorphic since $v_1$ and $v_2$ are of the same color. We aim to show that $f$ can be extended to an isomorphism from $(S_1 \cup \{v_1\},\mathcal{N}(S_1) \cup \mathcal{N}(v_1))$ to $(S_2 \cup \{v_2\},\mathcal{N}(S_2) \cup \mathcal{N}(v_2))$.
    
    Consider $T_1=\mathcal{N}(v_1) \setminus \mathcal{N}(S_1)$ and $T_2=\mathcal{N}(v_2) \setminus \mathcal{N}(S_2)$. We claim that any vertex $u_1$ of $T_1$ cannot be connected to a vertex of $\mathcal{N}(S_1)$ other than $v_1$. If $u_1$ is connected to some vertex $u_2 \ne v_1$ in $\mathcal{N}(S_1)$, then both $v_1$ and $u_2$ must have some neighbor in $S_1$: call these $u_3$ and $u_4$. If $u_3=u_4$, then we have the cycle $u_1 \to v_1 \to u_3 \to u_2 \to u_1$. If $u_3 \ne u_4$, then there must be a path through edges of $S_1$ from $u_3$ to $u_4$, so we create a cycle containing $u_1 \to v_1 \to u_3 \to \cdots \to u_4 \to u_2 \to u_1$. Both of these cycles have a length greater than $3$, which is a contradiction. Thus, $u_1$ is not connected to any vertex of $\mathcal{N}(S_1)$ other than $v_1$. Similarly, any vertex of $T_2$ is not connected to a vertex of $\mathcal{N}(S_2)$ other than $v_2$.

    Another observation is that in the induced subgraph of $T_1$ (or $T_2$), the degree of each vertex is at most $1$. In particular, if $u_1\in T_1$ is connected to $u_2,u_3\in T_1$ with $u_2\neq u_3$, then there is a cycle $u_2\to u_1 \to u_3\to v_1\to u_2$ of length $4$, which is a contradiction.  

    Notice that $f: (S_1,\mathcal{N}(S_1))\to (S_2,\mathcal{N}(S_2))$ is an isomorphism and that $\calN(S_1)\cap \calN(v_1)$ is isomorphic to $\calN(S_2)\cap \calN(v_2)$. It can be seen that the multisets of vertex colors in $T_1$ and $T_2$ are the same. Additionally, edges connecting vertices in $T_1$ can be paired with edges connecting vertices in $T_2$, so that the paired edges have the same multiset of end vertex features. Since no such edges share a common end vertex, guaranteed by the above observation, one can extend $f$ to an isomorphism from $(S_1 \cup \{v_1\},\mathcal{N}(S_1) \cup \mathcal{N}(v_1))$ to $(S_2 \cup \{v_2\},\mathcal{N}(S_2) \cup \mathcal{N}(v_2))$. Thus, we have proven the inductive step and the proof is completed. 
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:1hop-WL}]
    By the proof of Theorem~\ref{thm:1hop-WL}, there exists a permutation $\sigma\in S_n$ such that $\sigma(i)=i'$ and $\sigma\ast (G,H) = (G,H)$. Then the result holds immediately.
\end{proof}


\section{Proofs for \Cref{sec:khop_subgraphGNN}}
\label{sec:pf_khop_subgraphGNN}

\begin{proof}[Proof of \Cref{thm:GNN-khop}]
    Based on \Cref{thm:khop-WL}, the proof of \Cref{thm:GNN-khop} follows the same lines as the proof of \Cref{thm:GNN-1hop}.
\end{proof}

Next, we present the proof of \Cref{thm:khop-WL}.
Let $S$ be a subset of vertices of a graph. Define $\mathcal{N}_k(S)$ as the set of all vertices with distance at most $k$ from any vertex in $S$ and $\mathcal{N}_k(v)$ as the set of all vertices with distance at most $k$ from $v$. If $S$ is nonempty, define $d(v,S)$ as the minimum distance from $v$ to any vertex in $S$.

To prove our \Cref{thm:khop-WL}, we need a lemma, which rules out the existence of undetected edges when we do our induction.

\begin{lemma}\label{lem:cycle_2k+1}
    Let $k\geq 2$ and let $S$ be a connected subset of vertices of a connected graph $G$ with no cycles of length greater than $2k+1$. Let $u_1$ be a vertex not in $S$ adjacent to a vertex in $S$. Then, no vertex in $T=\mathcal{N}_k(u_1) \setminus \mathcal{N}_k(S)$ can be connected to a vertex in $\mathcal{N}_k(S) \setminus \mathcal{N}_k(u_1)$.
\end{lemma}

\begin{proof}
    Assume for the sake of contradiction that there exists a vertex $u_{k+1} \in T$ connected to $v_k \in \mathcal{N}_k(S) \setminus \mathcal{N}_k(u_1)$. Notice that $d(v_k,S) \le k$ because $v_k \in \mathcal{N}_k(S)$. If $d(v_k,S)<k$, then $d(u_{k+1},S) \le k$, which contradicts $u_{k+1} \notin \mathcal{N}_k(S)$. Thus, $d(v_k,S)=k$.

    Therefore, there must exist vertices $u_2,u_3,\ldots,u_k$ and $v_0,v_1,\ldots,v_{k-1}$ such that $u_i$ and $u_{i+1}$ are connected for $i \in \{1,2,\ldots,k\}$, $v_i$ and $v_{i+1}$ are connected for $i \in \{0,1,\ldots,k-1\}$, and $v_0 \in S$.

    We claim that $u_1,u_2,\ldots,u_{k+1},v_0,\ldots,v_k$ are pairwise distinct. For any two connected vertices $a$ and $b$, notice that $|d(a,S)-d(b,S)| \le 1$ because any path of length $s$ from $a$ to a vertex of $S$ can be extended to a path of length $s+1$ from $b$ to a vertex of $S$ and vice versa. Since $d(u_1,S)=1$, $d(u_{k+1},S)=k+1$, $d(v_0,S)=0$, and $d(v_k,S)=k$, we must have $d(u_i,S)=i$ and $d(v_i,S)=i$ for all valid $i$. Thus, the only possible pairs of vertices of $u_1,u_2,\ldots,u_{k+1},v_0,\ldots,v_k$ that can be equal are $(u_i,v_i)$ for $i \in \{1,2,\ldots,k\}$. Assume for the sake of contradiction that $u_i=v_i$ for some $i$. Then, there exists a path $u_1 \to u_2 \to \cdots \to u_i \to v_{i+1} \to \cdots \to v_k$ of length $k-1$ from $u_1$ to $v_k$, contradicting the fact that $v_k \notin \mathcal{N}_k(u_1)$. Thus, the vertices $u_1,u_2,\ldots,u_{k+1},v_0,\ldots,v_k$ are pairwise distinct.

    Since $S$ is connected, there exists a path with edges in $S$ from $v_0$ to a vertex in $S$ adjacent to $u_1$. We can combine this path with $u_1 \to u_2 \to \cdots \to u_{k+1} \to v_k \to v_{k-1} \to \cdots \to v_0$ to create a cycle containing vertices $u_1,u_2,\ldots,u_{k+1},v_0,\ldots,v_k$. This cycle contains at least $2k+2$ vertices, a contradiction.
\end{proof}


\begin{proof}[Proof of \Cref{thm:khop-WL}]
    We work with the same notation and setting as in the proof of \Cref{thm:1hop-WL}.
    Consider $(G,H)\ensuremath{\stackrel{k}{\sim}}(\hat{G},\hat{H})$, i.e., $(G,H)$ and $(\hat{G},\hat{H})$ cannot be distinguished by the $k$-hop subgraph WL test. When there are no hash collisions and the colors stabilize, the multisets of final colors of vertices in $G$ and $\hat{G}$ are the same, and any $v_1 \in G$ and $v_2 \in \hat{G}$ with the same color must have isomorphic $k$-hop subgraphs rooted at them.

    We prove the following statement by induction: for any $t \in \{1,2,\ldots,|G|\}$, there exist connected isomorphic subsets $S_1 \subseteq V(G)$ and $S_2 \subseteq V(\hat{G})$ of size $t$ such that $(S_1,\calN_k(S_1))$ and $(S_2,\calN_k(S_2))$ are isomorphic. For the base case, choose any two vertices in $G$ and $\hat{G}$ with the same color. For the inductive step, suppose that $S_1$ and $S_2$ are valid sets of size $t<|V(G)|$, and we want to find two sets $S_1'$ and $S_2'$ with size $t+1$ satisfying the inductive statement. Let $v_1$ be a vertex not in $S_1$ adjacent to a vertex in $S_1$, and let $v_2$ be the image of $v_1$ under an isomorphism $f$ from $(S_1,\calN_k(S_1))$ to $(S_2,\calN_k(S_2))$. Then $\calN_k(v_1)$ and $\calN_k(v_2)$ are isomorphic since $v_1$ and $v_2$ are of the same color, and $f$ takes $\calN_k(S_1) \cap \calN_k(v_1)$ to $\calN_k(S_2) \cap \calN_k(v_2)$.

    The $k$-separability assumption guarantees that vertices in $\calN_k(v_i)\setminus\calN_k(S_i)$ have distinct colors for $i=1,2$. Thus, there is a unique way to extend $f$ as a map from $\calN_k(S_1)\cup \calN_k(v_1)$ to $\calN_k(S_2)\cup \calN_k(v_2)$, which keeps that a vertex has the same color as its image. We then verify that this extension is still an isomorphism. Consider any $u_1\in \calN_k(v_1)\setminus \calN_k(S_1)$ and any $w_1\in \calN_k(v_1)$ with $u_1\neq w_1$. Denote $u_2 = f(u_1)$ and $w_2 = f(w_1)$. Then $d(u_1,v_1) = d(u_2,v_2)=k$. We claim that $u_1$ and $w_1$ are connected if and only if $u_2$ and $w_2$ are connected.
    \begin{itemize}
        \item Case 1: $d(w_1,v_1)=d(w_2,v_2)=k$. By the $k$-separability assumption and the isomorphism between $\calN_k(v_1)$ and $\calN_k(v_2)$, we immediately have that $u_1$ and $w_1$ are connected if and only if $u_2$ and $w_2$ are connected.
        \item Case 2: $d(w_1,v_1)=d(w_2,v_2)=k-1$. Note that the multisets of vertex colors of the direct neighbors of $w_1$ and $w_2$ are the same, which combined with the $k$-separability assumption that $u_1$ and $w_1$ are connected if and only if $u_2$ and $w_2$ are connected.
        \item Case 3: $d(w_1,v_1)=d(w_2,v_2)\leq k-2$. Then $u_i$ and $w_i$ are not connected for $i=1,2$.
    \end{itemize}
    
    
    By \Cref{lem:cycle_2k+1} and the above arguments, we conclude that $f$ is an isomorphism from $(S_1 \cup \{v_1\},\mathcal{N}_k(S_1) \cup \mathcal{N}_k(v_1))$ to $(S_2 \cup \{v_2\},\mathcal{N}_k(S_2) \cup \mathcal{N}_k(v_2))$. This completes the inductive step.
    % Notice that any vertex in $N_1 \setminus \overline{S_1}$ has distance $k$ from $v_1$, so we know which vertices of $N_1$ it is connected to, since we can distinguish the vertices of $N_1 \setminus \overline{S_1}$ by label. We also know by the above lemma that no vertex in $N_1 \setminus \overline{S_1}$ is connected to any vertex in $\overline{S_1} \setminus N_1$, so we know whether any two vertices of $\overline{S_1} \cup N_1$ are connected. These edges must be the same edges as in $\overline{S_2} \cup N_2$, so $\overline{S_1} \cup N_1$ and $\overline{S_2} \cup N_2$ are isomorphic and the inductive step is complete.
\end{proof}

\section{Proofs for \Cref{sec:extension}}
\label{sec:pf_khop_GNN}

\begin{proof}[Proof of \Cref{thm2:GNN-khop}]
    Based on the \Cref{thm2:khop-WL}, one can prove \Cref{thm2:GNN-khop} following the same lines in the proof of \Cref{thm:GNN-1hop}.
\end{proof}

\begin{proof}[Proof of \Cref{thm2:khop-WL}]
    % We claim that using the $k$-hop WL test and the condition that any two vertices such that the distance between them is at most $2k$ are of different labels, we can obtain all the information we would otherwise obtain using the $(k-1)$-hop subgraph WL test and the same condition. 
    Let us consider the final stabilized colors on $(G,H)$ and $(\hat{G},\hat{H})$ generated by $k$-hop WL test ($k\geq 2$) without hash collisions. 
    For any vertex $v$, notice that any pair of vertices in $\mathcal{N}_k(v)$ have a distance at most $2k$ from each other, so they are of different colors. Suppose $u_1$ and $u_2$ are vertices in $\mathcal{N}_{k-1}(v)$. If the color of $u_1$ implies it has a neighbor with the same color as $u_2$, then this neighbor must be $u_2$, as the only neighbors of $u_1$ are in $\mathcal{N}_k(v)$ and all vertices in $\mathcal{N}_k(v)$ are of different colors. Otherwise, $u_1$ and $u_2$ cannot be connected by edges. Thus, for any $u_1$ and $u_2$ in $\mathcal{N}_{k-1}(v)$, we can uniquely determine whether there is an edge between $u_1$ and $u_2$ by their colors. This implies that two vertices of the same color must have isomorphic $(k-1)$-hop subgraphs, i.e., implementing $(k-1)$-hop subgraph WL test does not lead to a strict color refinement. Then the result is a direct corollary of \Cref{thm:1hop-WL} and \Cref{thm:khop-WL}.
\end{proof}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
