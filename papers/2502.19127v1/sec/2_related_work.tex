\section{Related Works}
\textbf{Factual hallucination mitigation.} Several studies \citep{wang2019revisiting, gardent2017creating} have explored mitigating hallucinations by improving the quality of pre-training data. However, processing vast datasets is time-consuming \citep{zhang2023siren} and is not applicable for models that have completed training. Other approaches \citep{dola, zhang2023alleviating, li2023inference, lee2022factuality} focus on inference-time enhancement, yet these strategies have limited generalization due to their reliance on domain-specific data \citep{self-eval-skt}, along with more difficulty generating fluent or diverse texts \citep{ji2023survey}. Furthermore, methods \citep{nakano2021webgpt, gou2024critic} that utilize retrieval-augmented (RAG) techniques introduce significant system complexity \citep{facttune} and depend heavily on the quality of external knowledge bases \citep{zhang2023siren}. Additionally, post-training LLM through SFT \citep{elaraby2023halo,alignmentforhonesty} and Reinforcement Learning \citep{ouyang2022training,kang2024unfamiliar} exhibits a promising reduction in factual error rates. Recently, \citet{facttune, flame, self-eval-skt} use preference learning on self-generated responses. They mainly focus on open-ended questions and rate responses by first adopting external models to split responses into atomic facts, then verifying each fact via RAG \citep{facttune, flame} or a model fine-tuned on millions of related data \citep{self-eval-skt}. This leads to significant complexity, especially when responses contain hundreds of atomic facts. In contrast, SMA targets precise QA with standard answers, simplifying verification, where no additional training or external databases are required. Moreover, the effects of these methods fail to generalize to other tasks related to factuality and lead to trade-offs in different abilities, while SMA achieves unanimous improvements on them.

\textbf{Precise factual QA tasks} include discriminative, multiple-choice, and generative forms, where the former two \citep{thorne2018fever, MMLU, liu2022token, mishra2024fine} only have a limited answer space that allows models to guess the correct answer by chance, therefore unable to accurately judge whether the corresponding knowledge is possessed. Generative datasets designed with adversarial intents \citep{lin2022truthfulqa, cheng2023evaluating} can effectively provoke hallucinations but tend to focus on specific scenarios, limiting their capacities to reflect performance on more general questions. While large simple generative QA datasets \citep{yang2015wikiqa, joshi2017triviaqa, yang2018hotpotqa, kwiatkowski2019natural} exist, they are mostly built years ago with no domain annotations. In contrast, our annotated dataset offers a comprehensive and up-to-date assessment.
