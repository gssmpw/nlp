\section{Experiments}
In this section, we present the training results using SMA. Comparison with the other three baselines validates our effectiveness, and more ablation studies are conducted to investigate how our detailed settings influence the training outcomes.

\subsection{Settings}
We use Qwen2-7B-Instruct \citep{qwen2} and Baichuan1-Chat as experimental base models. To have a comparable training computation with baselines, we randomly sample a small split from the FactualBench training set, containing 24k samples, which we denote as \textit{(small)}. Since the verification can be costly and time-consuming frequently visiting GPT4 through API, we adopt weaker models, Qwen and Baichuan, as verifiers respectively to accelerate the process. These models still have acceptable judgment accuracy since standard answers are also provided. For each question, we sample $n=8$ responses from the model and reserve up to $m=8$ preference pairs for the tuning set. We set top-k$=$50, top-p$=$0.9, temperature$=$1.4 for Qwen2, and temperature$=$1.2 for Baichuan1. Training details are provided in Appendix \ref{sec:training details}.

For baselines, we select FLAME \citep{flame}, FactTune-FS \citep{facttune}, and Self-Eval-SKT \citep{self-eval-skt}, all of which aim to enhance factuality. These methods involve training on open-ended questions and additional attention on instruct-following queries \citep{kopf2023openassistant} in \citet{flame} or adversarial questions \citep{lin2022truthfulqa} in \citet{self-eval-skt}. We reproduce their training procedures on Qwen2 and Baichuan1 adhering to the settings in their original papers.

We use FactualBench to evaluate factuality on precise and simple QA, with more benchmarks assessing factuality across different tasks: TruthfulQA \citep{lin2022truthfulqa} and HalluQA \citep{cheng2023evaluating} for generative tasks and factuality to adversarial questions, CMMLU \citep{li2023cmmlu} for multiple-choice task, and HaluEval \citep{li2023halueval} for discriminative task. Additionally, we adopt AlignBench \citep{liu2023alignbench} containing 8 sub-tasks for comprehensive advanced abilities and AlpacaEval \citep{alpaca_eval} for helpfulness to reflect the broader impact of training beyond factuality. We report the average score (out of 10) for AlignBench, the win rate (\%) against the base model for AlpacaEval, and the accuracy (\%) for the rest. Since Self-Eval-SKT uses partial data from TruthfulQA, we report the accuracy on the rest of the data for this method. We calculate \textit{Avg.} averaging performance on those benchmarks, where the AlignBench score is multiplied by 10 to align with other metrics, and AlpacaEval is excluded due to its relative metric. More details about the benchmarks are provided in Appendix \ref{cha:benchmark detail}.

\subsection{Main Results}
\begin{table*}[ht]
  \centering
   \resizebox{0.9\textwidth}{!}{%
    \begin{tabular}{l|ccccc|ccccccccc|c|c}
    \toprule[1.5pt]
     Model & \rotatebox{80}{\textbf{FactualBench} (gen.)} & \rotatebox{80}{\textbf{TruthfulQA} (gen.)} & \rotatebox{80}{\textbf{HalluQA} (gen.)} & \rotatebox{80}{\textbf{CMMLU} (mc.)} & \rotatebox{80}{\textbf{HaluEval} (disc.)} & \rotatebox{80}{\textbf{AlignBench}} & \rotatebox{80}{--Prof. Knowledge} & \rotatebox{80}{--Mathematics} & \rotatebox{80}{--Fundamental Lang.} & \rotatebox{80}{--Logical Reasoning} & \rotatebox{80}{--Understanding} & \rotatebox{80}{--Writing} & \rotatebox{80}{--Role Play} & \rotatebox{80}{--Open-ended} & \rotatebox{80}{AlpacaEval (helpful)} & \rotatebox{80}{\textbf{$\Delta$Avg.}} \\ 
     \midrule
     \multicolumn{17}{c}{\sc Qwen2-7B-Instruct} \\ 
    \midrule
     Base  & 56.27 & 52.75 & 46.44 & 80.85 & 52.30 & 6.69 & 6.62 & 6.65 & 6.51 & 5.07 & 6.76 & 7.15 & 7.59 & 7.46 & 50.00 & -  \\ 
     \midrule
     FLAME  & \textcolor{red}{55.20} & \textcolor{red}{50.43} & 50.00 & \textcolor{red}{80.12} & \textcolor{red}{51.66} & 6.80 & \textcolor{red}{6.59} & \textcolor{red}{6.22} & 6.60 & \bf 5.83 & 6.78 & 7.31 & \bf 7.85 & 7.72 & \bf 68.32 & \textcolor{red}{-0.02}  \\
     FactTune-FS  & \textcolor{red}{56.24} & 54.47 & 50.44 & \textcolor{red}{80.12} & \textcolor{red}{50.81} & \textcolor{red}{6.49} & \textcolor{red}{6.35} & \textcolor{red}{6.37} & \textcolor{red}{6.32} & 5.14 & \textcolor{red}{6.31} & \textcolor{red}{6.77} & \textcolor{red}{7.49} & \textcolor{red}{7.45} & \textcolor{red}{48.51} & +0.24  \\
     Self-Eval-SKT & \textcolor{red}{53.32} & \bf 57.99 & \bf 50.67 & \textcolor{red}{80.25} & \textcolor{red}{49.43} & \textcolor{red}{6.44} & \textcolor{red}{6.40} & 6.67 & \textcolor{red}{6.27} & 5.08 & \textcolor{red}{6.17} & \textcolor{red}{6.84} & \textcolor{red}{7.09} & \textcolor{red}{7.10} & 50.87 & +0.09 \\ 
     \midrule
     SMA (small) & \bf 58.81 & 54.47 & 49.78 & \bf 82.15 & \bf 54.00 & \bf 6.96 & \bf 6.63 & \bf 6.94 & \bf 6.94 & 5.56 & \bf 6.93 & \bf 7.43 & 7.84 & \bf 7.92 & 58.26 & \bf +2.22  \\ 
     \midrule
    \multicolumn{17}{c}{\sc Baichuan1-Chat} \\ 
    \midrule
     Base  & 48.24 & 30.23 & 32.00 & 48.85 & 50.35 & 5.03 & 5.34 & 2.71 & 5.57 & 3.20 & 5.86 & 6.32 & 6.33 & 6.63 & 50.00 & -  \\ 
     \midrule
     FLAME  & 51.16 & \textcolor{red}{29.62} & 32.00 & 49.33 & \bf 51.28 & 5.21 & 5.80 & 2.85 & 5.65 & \bf 3.43 & \bf 6.05 & \textcolor{red}{6.21} & 6.38 & \bf 7.00 & \bf 56.46 & +0.92  \\
     FactTune-FS  & 50.43 & 31.95 & \textcolor{red}{30.89} & 48.94 & 50.93 & \textcolor{red}{4.29} & \textcolor{red}{4.56} & \textcolor{red}{2.17} & \textcolor{red}{4.12} & \textcolor{red}{2.51} & \textcolor{red}{4.98} & \textcolor{red}{5.45} & \textcolor{red}{5.76} & \textcolor{red}{6.37}  & 52.24 & \textcolor{red}{-0.66}  \\
     Self-Eval-SKT & 48.41 & \bf 36.11 & 33.33 & 49.24 & \textcolor{red}{50.29} & \textcolor{red}{4.83} & 5.37 & 2.76 & \textcolor{red}{5.09} & 3.39 & \textcolor{red}{5.57} & \textcolor{red}{5.75} & \textcolor{red}{5.84} & \textcolor{red}{6.11}  & 54.84 & +0.95 \\ 
     \midrule
     SMA (small) & \bf 57.37 & 33.78 & \bf 38.44 & \bf 50.13 & 50.63 & \bf 5.30 & \bf 5.92 & \bf 3.02 & \bf 5.66 & 3.37 & 5.97 & \bf 6.53 & \bf 6.55 & 6.79 & 54.84 & \bf +3.90  \\ 
     \midrule
     SMA (full) & \underline{58.29} & 35.86 & \underline{38.89} & \underline{50.92} & \underline{52.05} & \underline{5.38} & \underline{6.25} & \underline{3.03} & \underline{5.76} & \underline{3.55} & \underline{6.12} & 6.52 & 6.36 & 6.79 & \underline{63.99} & \underline{+4.97}  \\ 
     \bottomrule[1.5pt]
    \end{tabular}
    }
  \caption{\label{tab:main result}
    Performance on benchmarks reflecting factuality, helpfulness, and comprehensive abilities. We mark the decreased results in \textcolor{red}{red}, and the best results in \textbf{bold} (better results of SMA(full) in \underline{underline}). Sub-tasks of AlignBench are listed in abbreviation. Domain-level accuracy on FactualBench is shown in Appendix \ref{cha:detailed main experiment result}
  }
\end{table*}

We present model performance on benchmarks after training in Table \ref{tab:main result}, where we also include training results using SMA on full FactualBench training set \textit{(full)} to further utilize our dataset, which achieves better results. SMA is the only one to achieve unanimous improvement across all benchmarks, including all sub-tasks in AlignBench. In contrast, the other three baselines show decreased performance not only on factuality-related tasks but also on advanced skills and helpfulness, highlighting the difficulty in generalization of these methods and the effectiveness of SMA. Specifically, SMA achieves 2.22 and 3.90 improvement in $Avg.$ on Qwen2 and Baichuan1 respectively, $4 \times$ and $9 \times$ to the best baselines. Moreover, SMA achieves the best results on almost all benchmarks, except for TruthfulQA, HalluQA, and AlpacaEval, where Self-Eval-SKT and FLAME benefit more from in-domain training. Notably, changes on FactualBench reveal that SMA stimulates partial potential in model memory, while baselines show limited improvement and even declines, which indicates that training on imprecise open-ended questions with average precision metrics offers limited gains in the model's utilization of specific factual knowledge. 

\subsection{Ablation Studies}
\begin{figure*}
  \includegraphics[width=0.24\textwidth]{latex/img/distance_k_10_BCdposft_qwen72_fb.pdf} \hfill
  \includegraphics[width=0.24\textwidth]{latex/img/distance_k_10_BCdposft_qwen72_truthful.pdf} \hfill
  \includegraphics[width=0.24\textwidth]{latex/img/distance_k_10_BCdposft_qwen72_hallu.pdf} \hfill
  \includegraphics[width=0.24\textwidth]{latex/img/distance_k_10_BCdposft_qwen72_cmmlu.pdf}
  \caption {Changes of Baichuan1 alignment with Qwen2-72B-Instruct on four benchmarks after training.}
  \label{fig:alignment}
\end{figure*}

More ablation studies are conducted to further validate the effectiveness of our settings. Detailed results are shown in Appendix \ref{cha:detailed ablation experiment result}.

\textbf{Ablation on data sources.} Our method adopts self-generated responses to align models, denoted as \textit{self}. In addition, we validate more data sources. The standard answers and wrong answers from the dataset generated by GPT4 are denoted as \textit{dataset}. Model responses given the reference descriptions are denoted as \textit{w/ desc.}, which are generally correct since standard answers are contained in descriptions. We also train Qwen on responses generated by \textit{Baichuan}. For SFT, a single label is selected per question. Training results are shown in Table \ref{tab:data source ablation}.

Training on self-generated data yields better results for both DPO and SFT. While SFT on ground truth data (\textit{dataset} and \textit{w/ desc.}) improves performance on FactualBench, it leads to sharp declines on other tasks, which can be attributed to learning on responses with extremely different styles, short and concise, from the model itself. For DPO, training on \textit{dataset} or other model's responses can still achieve competitive results. However, it is crucial to have chosen and rejected in the same distribution to prevent reward hacking \citep{shekhar2024see}.

\begin{table}[!t]
  \centering
   \resizebox{\linewidth}{!}{%
  \begin{tabular}{c|cc|cccc}
    \toprule[1.5pt]
    Loss & Chosen & Rejected & FactualBench & Alignbench & AlpacaEval & $\Delta$Avg. \\
    \midrule
    \multicolumn{7}{c}{\sc Qwen2-7B-Instruct} \\ 
    \midrule
    SFT & self & - & 55.43 & 6.63 & 44.22 & -0.66 \\
    SFT & Baichuan & - & 49.97 & 4.98 & 15.03 & -13.61 \\
    SFT & dataset & - & 50.38 & 3.56 & 7.20 & -23.22 \\
    \midrule
    DPO & self & self & \bf 58.81 & \bf 6.96 & \bf 58.26 & \bf +2.22 \\
    DPO & Baichuan & Baichuan & 58.17 & 6.71 & 39.19 & +0.45 \\
    DPO & dataset & dataset & 55.75 & 6.50 & 36.06 & -0.65 \\
    \midrule
    \multicolumn{7}{c}{\sc Baichuan1-Chat} \\ 
    \midrule
    SFT & self & - & 51.33 & 5.04 & 37.58 & +1.29 \\
    SFT & w/ desc. & - & 55.63 & 4.47 & 36.96 & -5.69 \\
    SFT & dataset & - & 55.86 & 3.73 & 26.65 & -10.18 \\
    \midrule
    DPO & self & self & \bf 58.29 & \bf 5.38 & \bf 63.99 & \bf +4.97 \\
    DPO & w/ desc. & self & 18.17 & 4.07 & 32.80 & -13.67 \\
    DPO & dataset & self & 5.40 & 3.28 & 19.07 & -21.56 \\
    DPO & dataset & dataset & 49.08 & 4.82 & 39.07 & -1.40 \\
    \bottomrule[1.5pt]
  \end{tabular}
  }
  \caption{Results after training on different data sources.}
  \label{tab:data source ablation}
\end{table}

\textbf{Ablation on loss functions.} We choose DPO for its fine-grained bi-directional signals, and SFT training is conducted for effectiveness comparison. Beyond SFT on a single label per question (\textit{single label}), we also explore SFT the model on all correct answers (\textit{all labels}). Moreover, existing researches suggest that fusing DPO with SFT loss can help mitigate overoptimization on rejected labels \citep{he2024complex, liu2024provably}, which we denote as \textit{SFT+DPO}. Furthermore, additional SFT training before DPO on the tuning set is supposed to reduce distribution shift issues and thus help training \citep{xu2024dpo}, which we denote as \emph{SFT then DPO}. All training is conducted on self-generated data. 

The comparison between DPO and SFT shows that preference data will lead to greater improvement, even for \textit{DPO (small)} with less tuning data than \textit{SFT (single label)}, confirming that unidirectional signal is indeed insufficient for our task. Additionally, the difference between SFT on \textit{single label} and \textit{all labels} demonstrates that more labels for the same question in SFT will not enhance training effectiveness. Moreover, neither \textit{SFT then DPO} nor \textit{SFT+DPO} outperforms DPO. Since the data are sampled from the model itself, there is little distribution shift and a low likelihood of having reward hacking solely on rejected labels during training, emphasizing the stability of our method.

\begin{table}[!t]
  \centering
   \resizebox{\linewidth}{!}{%
  \begin{tabular}{c|cccc}
    \toprule[1.5pt]
    Loss & FactualBench & AlignBench & AlpacaEval & $\Delta$Avg. \\
    \midrule
    \multicolumn{5}{c}{\sc Baichuan1-Chat} \\ 
    \midrule
    SFT (single label) & 51.33 & 5.04 & 37.58 & +1.29 \\
    SFT (all labels) & 52.37 & 5.03 & 31.06 & +0.32 \\
    \midrule
    DPO (small) & 57.37 & 5.30 & 54.84 & +3.90 \\
    DPO (full) & \bf 58.29 & \bf 5.38 & \bf 63.99 & \bf +4.97 \\
    \midrule
    SFT then DPO & 54.74 & 5.07 & 54.53 & +4.03 \\
    SFT + DPO & 57.16 & 5.13 & 63.91 & +4.09 \\
    \bottomrule[1.5pt]
  \end{tabular}
  }
  \caption{Results after training on different losses.}
  \label{tab:loss ablation}
\end{table}

Furthermore, we argue that models obtain better representation ability after DPO. \citet{huh2024platonic} has found that the representation alignment degree, measured by mutual nearest-neighbor metric\footnote{We introduce its definition and calculation in Appendix \ref{cha:metric}}, increases with performance. We calculate Baichuan1 alignment with Qwen2-72B-Instruct \citep{qwen2}, which serves as a strong representation function, on several benchmarks and present the results in Figure \ref{fig:alignment}. The DPO model achieves higher accuracy and deeper alignment with Qwen2-72B than both the base and SFT models, indicating a better representation ability is achieved.

\textbf{Ablation on tuning data sizes.} A noticeable performance gap exists between the model trained on \textit{small} split and the one trained on \textit{full} split, motivating an exploration of the training efficacy of different tuning data sizes. We present the overall improvement of Baichuan1, measured by \textit{$\Delta$Avg.}, after DPO on different volumes of training questions in Figure \ref{fig:scalinglaw}. The improvement continues to increase (in logarithmic rate) as the size of preference pairs expands, stressing the benefit of a larger dataset, while early training with our method already improves the overall performance effectively.

\begin{figure}[!t]
  \includegraphics[width=\columnwidth]{latex/img/scalinglaw.pdf}
  \caption{Improvement increases at a logarithmic rate with the training data size of DPO expanding.}
  \label{fig:scalinglaw}
\end{figure}
