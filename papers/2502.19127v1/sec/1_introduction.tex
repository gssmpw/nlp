\section{Introduction}
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/img/radar.pdf}
  \vspace{-3ex}
  \caption{Previous methods on factual hallucination mitigation exhibit poor generalizability across different factuality tasks and suffer from degradations on comprehensive abilities and helpfulness, while our self-memory alignment (SMA) improves model performance on all seven benchmarks, with a significant advantage on \textit{Avg.}}
  \label{fig:radar}
  \vspace{-2ex}
\end{figure}

\textbf{Factual hallucinations} occur when LLMs generate inaccurate or entirely fabricated contents in response to queries \citep{zhang2023siren, huang2023survey}, which can undermine user trust in models and cause significant harm, especially when LLMs are deployed in high-stake applications \citep{ji2023survey, ahmad2023creating, kang2023deficiency}. Furthermore, identifying hallucinations is challenging, as the fabricated contents are often presented plausibly and convincingly, making it difficult for both models and users to recognize inaccuracies \citep{kaddour2023challenges, zhang2023siren}, emphasizing the essentiality of mitigating hallucinations. 

% \zyc{Among various approaches to mitigate factuality hallucination, ranging from pre-training~\cite{gardent2017creating,wang2019revisiting} to inference~\cite{dola,gou2024critic}, post-training~\cite{flame, facttune, self-eval-skt} has become popular due to its independence from large-scale data manipulation and additional computational costs during decoding. Current methods typically augment factuality with data from generative open-ended QA tasks, with correctness assessed by metrics like FActScore \citep{factscore}. However, as illustrated in Figure~\ref{fig:radar}, improvements in factuality on benchmarks such as TruthfulQA~\cite{lin2022truthfulqa} and HalluQA~\cite{cheng2023evaluating} appear to be superficial, with poor generalization to other factuality-related tasks and a decline in overall performance. These challenges are due to the entity ambiguity after fact decomposition~\citep{chiang2024merging, wanner2024dndscore} and the lack of reference answers in open-ended QA. Moreover, the adopted metrics introduce biases by balancing length and accuracy~\citep{wei2024long}. These issues are not alleviated when training on complex instructions~\cite{flame} or advanced capabilities~\cite{lin2024mitigating}.}

Among various approaches to mitigate factual hallucinations, from pre-training \citep{gardent2017creating,wang2019revisiting} to inference time \citep{nakano2021webgpt,dola}, post-training \citep{flame, facttune} has become popular for not requiring large-scale data manipulation or additional computational complexity during runtime. Recent methods typically enhance factuality by training on open-ended questions, e.g., ``Tell me a bio of \texttt{an entity}'', which are broad and imprecise. They leave additional space for models to provide answers with different contents, subsequently assessed using average factual precision metrics like FActScore \citep{factscore}. However, as shown in Figure~\ref{fig:radar}, these methods lead to declines in other factuality-related tasks and trade-offs in overall performance. The poor generalization can be attributed to the biased signals from these metrics mixing accuracy with length \citep{wei2024long}. Moreover, wrong judgments on the correctness of atomic facts, which are caused by the entity ambiguity after response decomposition \citep{chiang2024merging, wanner2024dndscore} and the lack of standard answers in open-ended QA, degrade training effects. The trade-offs are not alleviated with additional training on advanced abilities \citep{zhao2023survey} related to factuality under adversarial queries \citep{self-eval-skt} or complex instructions following \citep{flame}, as they are not necessary for other tasks and can lead to forgetting of acquired abilities \citep{ouyang2022training, lin2024mitigating}.

% \zyc{To tackle these challenges, we propose \textbf{self-memory alignment} to reduce factual hallucination while improving generalization and preserving other capabilities. This approach enhances the LLMâ€™s ability to leverage its \textit{memory}, i.e., the knowledge acquired during pre-training, which is a crucial factor for both factuality improvement~\cite{wang2023survey} and general capabilities~\cite{zhao2023survey}. Concretely, we perform post-training on short, fact-seeking queries with standard answers about a large scope of entities, which are simple and precise, decoupling factuality from other advanced abilities and excluding other attributes from the correctness. These features help avoid the mentioned issues of methods which take open-ended QA with long responses multiple factual claims. Rather than directly constructing supervised QA pairs as external knowledge injection, which can disturb the original capabilities of the LLM~\cite{gudibande2023false, zhang2023siren} and sometimes inadvertently promote hallucination~\cite{flame, gekhman2024does, huang2023survey}, we assume that most factual knowledge has been learned by the LLM in its pre-training phase which can be correctly predicted via sampling and better stimulated in inference. With the factual queries, we sample diverse answers given by the LLM, and group those with correct and incorrect answers into pairs for training using Direct Preference Optimization (DPO)~\cite{rafailov2023direct}.}

To address the above problems, we propose \textbf{self-memory alignment (SMA)} to reduce factual hallucinations and improve generalization across other capabilities. The key idea is to enhance LLM's precise utilization of its existing \textit{memory}, i.e., the internalized knowledge derived from pre-training, which is a crucial factor for improving factuality \citep{wang2023survey} and a fundamental ability for LLM generation and understanding \cite{zhao2023survey}. Concretely, we align the model on precise fact-seeking QA with standard answers, which is a typical task for factual hallucinations and knowledge utilization evaluation \citep{roberts2020much, ji2023survey, zhao2023survey}. The task is short-form and simple, excluding other attributes from the correctness and decoupling factuality from advanced abilities. These features help avoid the mentioned issues of methods that use imprecise open-ended QA. Rather than directly building the tuning set from external sources, which can teach LLM undesired behavior clones instead of true abilities \citep{gudibande2023false, zhang2023siren} and inadvertently promote hallucinations \citep{huang2023survey, flame, gekhman2024does}, we sample answers from the LLM itself and pairing correct and incorrect responses for Direct Preference Optimization (DPO) \citep{rafailov2023direct} training. The pairwise optimization is adopted for more granular bi-directional controls and better generalization \citep{zhang2023siren, chu2025sft} than Supervised Fine-tuning (SFT), which only provides uni-directional signals.

% \zyc{However, a dataset with precise factual QA across diverse domains is lacking for training. Existing datasets~\cite{yang2015wikiqa, joshi2017triviaqa, yang2018hotpotqa, kwiatkowski2019natural} are usually outdated and short in fine-grained annotations, like entity domains and attribute divisions, which cannot provide accurate information and support factuality enhancement with simple factual queries. To this end, we build \textbf{FactualBench}, a large-scale dataset containing 181k Chinese data from 21 domains. The construction is based on encyclopedia database, which is commonly used in pre-training corpus~\cite{liu2024datasets, ando2024wikisqe} and can be taken as existing knowledge for LLMs, and adopts multiple filtering strategies to guarantee data quality. Evaluating on a test split of FactualBench, we find that while FactualBench is challenging for modern LLMs, high-temperature sampling significantly boosts accuracy, providing a foundation for mitigating hallucination through improved memory utilization.}

However, a large dataset with precise factual QA across diverse domains is lacking for training. Existing datasets \citep{yang2015wikiqa, joshi2017triviaqa, yang2018hotpotqa, kwiatkowski2019natural} are usually outdated and fall short in fine-grained domain annotations, limiting their accuracy and diversity. To this end, we build \textbf{FactualBench}, a large-scale dataset with 181k Chinese QA data spanning 21 domains. We extract knowledge from the Internet encyclopedia, a widely used pre-training corpus \citep{liu2024datasets, ando2024wikisqe}, which can be taken as existing knowledge for LLMs. Additionally, it has also been verified that incorporating pre-training data in fine-tuning helps reduce the forgetting of acquired abilities \citep{ouyang2022training}. Multiple filtering strategies are adopted to guarantee the data quality. Evaluations on the test split of FactualBench reveal that the task is challenging for LLMs. Moreover, we find that models can answer more questions correctly when afforded greater diversity, such as increasing generation temperature, providing a foundation for mitigating hallucinations through improved memory utilization.

Extensive experiments on Qwen and Baichuan demonstrate that only SMA achieves generalized improvement on FactualBench and all six other benchmarks concerning diverse factuality assessment, as well as helpfulness and other comprehensive skills. SMA obtains 4$\times$ and 9$\times$ increases on average, significantly surpassing the existing methods \citep{factscore, flame, self-eval-skt}. More ablation studies are performed to further investigate how different settings influence the training outcomes. Our work suggests that LLMs can experience overall enhancement solely trained on precise and simple QA, and benefit from a re-boost of knowledge from the pre-training corpus.  
