\section{Method}
\begin{figure*}[t]
  \includegraphics[width=\linewidth]{latex/img/main.pdf}
  \caption{The framework of our work. \textbf{Left}: We first extract factual knowledge from the Internet encyclopedia and construct a large and comprehensive dataset, FactualBench. Several filtering strategies are adopted for higher quality. \textbf{Right}: Next, we align LLM on self-generated response pairs on FactualBench. We elicit diverse responses to the same question, verify each correctness compared to the standard answer, and sample preference pairs for training.}
  \label{fig:main}
\end{figure*}

To mitigate factual hallucinations and prevent trade-offs in other abilities beyond factuality, we propose SMA to augment the model's utilization of its existing knowledge. For training and evaluation, we build FactualBench consisting of precise and simple QA data without malicious or misleading adversarial intents. In this section, we will introduce the dataset and the alignment method in detail.

\subsection{FactualBench}
The Internet encyclopedia is selected as the source of our dataset since it contains various factual information across domains \citep{wang2023survey, bai2024coig}, which is also a commonly used corpus in LLM pre-training \citep{liu2024datasets, ando2024wikisqe}. Specifically, we use Baidu baike\footnote{\url{https://baike.baidu.com/}}, a prominent encyclopedia in the Chinese community. We design a model-based pipeline to generate a large volume of data costlessly and quickly, adopting GPT4\footnote{We use the version of gpt-4-0125-preview in this paper.} \citep{achiam2023gpt} and Baichuan model\footnote{\url{https://www.baichuan-ai.com/}} for their strong instruction following capabilities. 

During pre-construction, we observe four typical types of low-quality data. 1) Long-tailed questions with obscure and useless related knowledge. 2) Questions with multiple correct answers. This is primarily due to some imprecise terms in questions that invite subjective judgments or the existence of more valid answers beyond encyclopedia knowledge. 3) Questions with incorrect standard answers. The model may extract knowledge falsely, which becomes frequent when paragraphs are extremely long or difficult to understand. Some questions fall into this category because they are time-sensitive, but the knowledge in the encyclopedia is outdated. 4) Questions that are not self-contained. Questions containing vague pronouns or ambiguous nouns with multiple interpretations, e.g., abbreviations and names without clear contexts, will confuse answerers unless the word has the same meaning in the vast majority of cases. To guarantee the data quality, we apply several filtering strategies and few-shot prompts in the construction of the dataset.

\textbf{Construction and Composition.} As illustrated in Figure \ref{fig:main} (left), FactualBench is constructed into five steps. \textit{1) Entry filtering}. We initially sample millions of entries from publicly available encyclopedias, ensuring broad coverage over subjects and domains. For each entry, we retain its object, view count, and brief description. To avoid generating questions on long-tailed knowledge, we set a view count threshold of 0.5M, and 89,658 entries remain after this filtering. \textit{2) Description filtering}. The performance of the model tends to decrease as the context length increases \citep{liu2024lost, sun2023survey, li2024long}. Excessively lengthy descriptions can provide superfluous information and lead to low-quality responses. Conversely, overly brief descriptions lack sufficient factual information. To balance this, we filter out descriptions shorter than 100 characters and truncate those exceeding 800 characters. 64,315 entries remain after this process. \textit{3) Question generation}. We instruct GPT4 to generate up to three precise questions per truncated description. For each question $Q_i$, GPT4 is also required to provide one standard answer $X_i^0$ and three wrong answers $\{X_i^j\}_{j=1}^3$ for further evaluation and training uses. To ensure adherence to our instructions, we add two examples for few-shot prompting. A total of 192,927 QA samples are generated in this process. \textit{4) Question classification}. A domain classifier based on Baichuan, fine-tuned on massive high-quality data, is employed to categorize all generated questions into different domains $D_i$. We maintain domains containing more than 500 questions and uniformly categorize the rest as \emph{others}. \textit{5) Question filtering}. We query GPT4 once again to filter out low-quality questions. Each question is assessed independently without the corresponding description, and GPT4 is instructed to identify whether the question falls into one of the low-quality types through step-by-step reasoning. 

\begin{CJK*}{UTF8}{gbsn}
\begin{table}[!b]
  \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{cc}
    \toprule[1.5pt]
    \multirow{2}{*}{Question $Q_i$} & 第一台微波量子放大器是在哪一年制成的？\\
    & In which year was the first microwave quantum amplifier made? \\ 
    \midrule
    \multirow{2}{*}{Standard Answer $X_i^0$} & 第一台微波量子放大器是在1954年制成的。\\
    & The first microwave quantum amplifier was made in 1954. \\
    \midrule
    \multirow{2}{*}{Wrong Answer $X_i^1$} & 第一台微波量子放大器是在1958年制成的。\\
    & The first microwave quantum amplifier was made in 1958. \\
    \multirow{2}{*}{Wrong Answer $X_i^2$} & 第一台微波量子放大器是在1960年制成的。\\
    & The first microwave quantum amplifier was made in 1960. \\
    \multirow{2}{*}{Wrong Answer $X_i^3$} & 第一台微波量子放大器是在1962年制成的。\\
    & The first microwave quantum amplifier was made in 1962. \\ \midrule
    \multirow{2}{*}{Domain $D_i$} & 高新科技 \\
    & high technology \\
    \bottomrule[1.5pt]
    \end{tabular}%
    }
  \caption{Each sample in FactualBench contains a question $Q_i$, a standard answer $X_i^0$, 3 wrong answers $\{X_i^j\}$ and a domain $D_i$ it belongs to. The English translation is for reference. Appendix \ref{cha:more examples} presents more examples.}
  \label{tab:example}
\end{table}
\end{CJK*}

Finally, 181,176 questions are reserved, where assessments of 1,000 samples indicate that an approximately 86\% high-quality rate is acquired. To evaluate the LLMs' ability to utilize knowledge, we randomly select a subset of questions for the test set. We do selection taking each entry (entries containing \textit{others} domain questions are excluded) as a unit to maintain that all questions in the test set are separate from the training set, and restrict each domain to a similar number of questions. 3,462 questions are selected, and the remaining 177,714 samples form the training set. We rephrase and refine low-quality questions in the test set after selection to ensure its high quality. We present the construction prompts in Appendix \ref{cha:generation prompts}, a sample in Table \ref{tab:example}, and the domain distribution in Table \ref{tab:construction}.

\begin{CJK*}{UTF8}{gbsn}
\begin{table}[!htb]
  \centering
   \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccc}
    \toprule[1.5pt]
    Domain    & 中文名   & Test & Training & Total \\
    \midrule
    film\&entertainment     & 影视娱乐  &   201        &   54,489       &  54,690     \\
    eduaction\&training           & 教育培养  &   161        &   3,703       &  3,864     \\
    physics, chemistry, mathematics\&biology   & 数理化生  &    201 &   9,189    &  9,390     \\
    history\&traditional culture    & 历史国学  &    202       &   18,108       &  18,310     \\
    biography           & 人物百科  &    201       &   11,844       &  12,045     \\
    politics\&law            & 政治法律  &   175        &   6,368       &  6,453     \\
    economics\&management         & 经济管理  &    160       &   4,543       &  4,703     \\
    computer science           & 计算机科学 &  201         &  6,253        &  6,454     \\
    medical                   & 医学    &     167      &   7,073       &   7,240    \\
    sociology\&humanity        & 社会人文  &    199       &    8,503      &  8,702     \\
    agriculture, forestry, fisheries\&allied industries & 农林牧渔  & 153 & 3,728 &  3,881  \\
    astronomy\&geography          & 天文地理  &    160       &   3,896       &  4,056     \\
    sports\&tourism            & 运动旅游  &    157       &   4,869       &  5,026     \\
    digital\&automotive         & 数码汽车  &    176       &   3,887       &  4,063     \\
    industrial engineering       & 工业工程  &   172        &   3,283       &  3,455     \\
    military\&war                 & 军武战争  &    151       &   2,569       &  2,720     \\
    slang\&memes                    & 网词网梗  &    151       &   529       &  680     \\
    work\&life                      & 工作生活  &    174       &   5,853       &  6,027     \\
    high technology                  & 高新科技  &    150       &   310       &  460     \\
    religion\&culture           & 信仰文化  &     150      &   510       &  660     \\
    others                   & 其他    &    -       &   18,207       &  18,207     \\ 
    \midrule
    total                     & -     &   3,462        &   177,714       &   181,176   \\
    \bottomrule[1.5pt]
    \end{tabular}%
    }
  \caption{Domain distribution of FactualBench.}
  \label{tab:construction}
\end{table}
\end{CJK*}

\textbf{Evaluation.} Following previous works \citep{liu2023alignbench, zheng2023judging}, a model-based approach is employed to expedite the evaluation. Note that rule-based automatic metrics such as ROUGE \citep{lin2004rouge} and BLEU \citep{papineni2002bleu} have been shown to exhibit significant biases in evaluation \citep{lou2024large}, we assess the correctness of the answer at a semantic-level. The verifier is supposed to focus solely on the content directly addressing the question and ignore the extraneous information. A response is considered correct only when it indeed answers the question (rather than ``I don't know'') and matches the standard answer. This is reasonable since the model is expected to have been trained on the relevant, frequently viewed data and should possess the necessary knowledge, and the portion of evasive answers only counts for approximately 1\%, which affects the evaluation result lightly. To improve judgment accuracy, we provide several examples and instruct the verifier to offer analyses before making the final decision. GPT4 is chosen as the verifier, which achieves a 96\% consistency with humans, validating the effectiveness. We present the evaluation prompt in Appendix \ref{cha:evaluation prompts}.

14 popular LLMs are evaluated on FactualBench: Baichuan series \citep{yang2023baichuan}, Qwen series \citep{qwen, qwen2}, Llama-3 series \citep{llama3modelcard}, Yi \citep{ai2024yi}, Command-R series \citep{CommandR, CommandR+}, DeepSeek \citep{deepseekv2}, and GPT4, where we prioritize the chat/instruct versions. We list the brief results in Table \ref{tab:eval result}. The accuracy (Acc.) on our test set ranges from 39.11\% to 75.62\%, indicating that LLMs still have deficiencies in the basic factual QA task. Detailed domain-level accuracy and additional analyses of the results can be found in Appendix \ref{cha:detailed result}. 

\begin{table}[t]
  \centering
    \resizebox{\columnwidth}{!}{%
    
    \begin{tabular}{cc}
    \toprule[1.5pt]
     Model & Acc. \\ \midrule
     \bf Baichuan1 & 48.24 \\  
     \bf Baichuan2 & 55.37 \\ 
     \bf Qwen1.5-7B & 48.87 \\ 
     \bf Qwen2-7B & 56.27 \\  
     Llama-3-8B & 39.11 \\ 
     \bottomrule[1.5pt]
   \end{tabular}%
   
   \begin{tabular}{|cc|}
    \toprule[1.5pt]
     Model & Acc. \\ \midrule
     \bf Baichuan3 & 67.50 \\
     \bf Yi-34B & 67.30 \\
     Command-R 35B & 54.30 \\
     Llama-3-70B & 49.65 \\
     \bf Qwen2-72B & 73.71 \\
     \bottomrule[1.5pt]
   \end{tabular}%
   
   \begin{tabular}{cc}
    \toprule[1.5pt]
     Model & Acc. \\ \midrule
     \bf Baichuan4 & 75.07 \\
     Command-R+ 104B &  60.17 \\
     \bf DeepSeek-v2 & \multirow{2}{*}{75.62} \\
     \bf -0628 MoE-236B & \\
     GPT4 & 65.71 \\
     \bottomrule[1.5pt]
   \end{tabular}%
   
    }
  \caption{Performance on FactualBench rated by GPT4. Models in bold are proficient in Chinese.}
  \label{tab:eval result}
\end{table}

\subsection{Self-Memory Alignment}
For cases where the model initially provides incorrect responses, we observe that it can generate correct answers when given greater output diversity. Taking Baichuan1 as an example, we increase the response variability by increasing the generation temperature and sampling the model's responses eight times (\textit{high temp. BO8}), contrasting with the standard inference setting (\textit{low temp. BO1}). Given the extensive answer space in the generative task, it is statistically improbable for a model to randomly guess the correct answer, so we consider the model to possess relevant knowledge if at least one of the generated responses is correct. As illustrated in Figure \ref{fig:BO1andBO8}, comparison between \textit{BO8} and \textit{BO1} reveals that a substantial portion of the model's capabilities remains underutilized, indicating an untapped potential in the memory. This also verifies the feasibility of building pairs on self-generated responses. Some specific cases are provided in Appendix \ref{cha:BO8 cases}.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/img/BO1andBO8.pdf}
  \caption{A comparison between Baichuan1 accuracy in \textit{low temp. BO1} and \textit{high temp. BO8}. Significant gaps in all domains demonstrate the potential of the model. Each domain is represented by its first five letters.}
  \label{fig:BO1andBO8}
\end{figure}

To stimulate the potential and enhance the knowledge utilization of models, we propose self-memory alignment (SMA) that aligns models on self-generated responses to precise and simple QA through preference learning. As shown in Figure \ref{fig:main} (right), the alignment includes three phases. \textit{1) Diversity Sampling.} For each question $Q_i$ in FactualBench training set ${\mathcal{D}^{\text{train}}}$, we sample $n$ responses from the model $\pi$ in higher diversity by increasing generation configurations such as temperature, top-p, and top-k. \textit{2) Reference-Based Verification.} The collected candidate responses are then provided to a verifier model, together with the standard answer $X_i^0$ from FactualBench. The verifier evaluates the responses after carefully analyzing, which acts as a judge function $\mathcal{J}$ to output $1$ or $0$ indicating correctness or not. Each evaluation result is formatted in a consistent manner to facilitate subsequent classification. \textit{3) Pairs Construction.} We classify all responses according to their correctness, discarding those with uncertain evaluation results (due to verifier failing in instruction following or low-quality of questions), and construct a set as follows: 
\begin{equation}\textstyle
    \{(\text{Prompt } Q_i, \text{Chosen } A_i^c, \text{Rejected } A_i^r)\},
\end{equation}
which is under the following constraint conditions:
\begin{align}\textstyle
    (Q_i, X_i^0) \sim {\mathcal{D}^{\text{train}}}; A_i^c, A_i^r \sim \pi (\cdot | Q_i);\\
    \mathcal{J}(Q_i, A_i^c, X_i^0) = 1;  \mathcal{J}(Q_i, A_i^r, X_i^0) = 0.
\end{align}
However, different questions can contribute significantly varying numbers of preference pairs ($=$ correct count $\times$ incorrect count). To balance this disparity, we randomly down-sample up to $m$ pairs for each question, which compose the tuning set.

In this way, we can quickly generate a tuning set $\mathcal{D}^{\text{tuning}}$ containing massive data without human intervention. Then fine-tune the model on the tuning data through preference learning, DPO \citep{rafailov2023direct}, whose loss is defined as follows:
\begin{equation}\textstyle
    -E \left[ \log \sigma ( \beta \log \frac{\pi_{\theta} (A_i^c | Q_i)} {\pi_{\text{ref}} (A_i^c | Q_i)} - \beta \log \frac {\pi_{\theta} (A_i^r | Q_i)} {\pi_{\text{ref}} (A_i^r | Q_i)}) \right],
\end{equation}
where $(Q_i, A_i^c, A_i^r) \sim \mathcal{D}^{\text{tuning}}$, $\pi_{\theta}$ is the optimal model initialized in model $\pi$ before optimization, while $\pi_{\text{ref}}$ is the frozen $\pi$. $\sigma$ denotes the sigmoid function, and $\beta$ is a hyperparameter.
