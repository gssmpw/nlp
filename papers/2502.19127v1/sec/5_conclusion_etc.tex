\section{Conclusion}
We propose self-memory alignment (SMA) to mitigate factual hallucinations and achieve generalized improvement in model performance. We select precise and simple factual QA as our training task and align models on self-generated preference data to enhance the model's ability to utilize its memory. A large-scale, multi-domain dataset FactualBench is constructed from the Internet encyclopedia for training and evaluation. Extensive experiments show that SMA significantly improves model performance across diverse tasks concerning factuality, helpfulness, and comprehensive advanced skills, which suggests that simply training on precise factual QA related to pre-training knowledge has the potential for the overall improvement of the model.

\FloatBarrier

\section*{Limitations}
Although extensive experiments and ablation studies across diverse benchmarks validate the effectiveness of our method, certain limitations require further improvement.

\textbf{Validations on additional models.} We primarily conduct training and evaluations on Qwen2-Instruct and Baichuan1-Chat models. However, it remains unclear whether our method can achieve similar effects on models with different properties, such as pre-trained models without any alignment training, models that are proficient in languages other than Chinese, and models with larger parameter sizes and stronger base abilities. Further validation across these diverse models would help assess the broader applicability of our method.

\textbf{Alignment with more algorithms.} The improvement curve observed in Figure \ref{fig:scalinglaw} exhibits an approximate logarithmic growth with diminishing marginal returns, and the model gains half of the improvement during the early training period. This suggests a potential in our training dataset for yielding further enhancement with thorough exploitation, such as adopting algorithms that are closer to online learning, including Proximal Policy Optimization \citep{schulman2017proximal} and iterative DPO algorithms \citep{iterativedpo, iterdpo2}.

\textbf{Hallucination mitigation in broader contexts.} Factual Hallucinations occur not only in closed-book tasks, as discussed in this paper, but also in open-book tasks. These include text reading comprehension and text summarization tasks, which require the model's utilization of knowledge within the provided context instead of the model's existing knowledge. We propose to investigate more different tasks to verify whether the improvement on the model derived from our method can have a broader generalization.

\section*{Ethics Statement}
All experiments and analyses in this study are conducted for research purposes, aiming to enhance the factuality, robustness, and trustworthiness of LLMs and mitigate factual hallucinations. We collect data from the Internet following their license and only for research use.

The data source we use to build FactualBench is a publicly available Internet encyclopedia, which may contain information related to specific individuals, places, or sensitive physiological or medical content. Yet all the information is well-known, and we extract it without the intention to violate privacy or safety policies. Despite our efforts to ensure higher quality, the dataset could still contain inaccuracies or outdated information, which means that it should not be considered a golden knowledge base in any case and should only be adopted for research purposes.

The other six benchmarks in this study are well-established, and we use them to assess the capabilities of different models and methods in line with their original purpose. 
