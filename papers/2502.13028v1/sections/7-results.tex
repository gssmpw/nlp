\section{Results and Discussion}
We now discuss our experimental results and findings. 

\begin{table*}[htbp]
\centering
\caption{Percentage win-rate for Faithfulness to Writing History evaluated by GPT-4o. Each cell (`X-Y') shows `X' as the method win-rate and `Y' as the Average Author win-rate, with ties as the remainder. Best win-rates for each source is underlined.}
\label{tab:faith-auth-history}
\small
\begin{tabular}{p{2cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.75cm}}
\toprule
\textbf{Source} & \textbf{Oracle} & \textbf{RAG} & \textbf{Delta} & \textbf{Writing Sheet} & \textbf{Writing Summary} & \textbf{Writing Sheet nP} & \textbf{Writing Summary nP} \\
\midrule
AO3 & 52-32 & 31-54 & 49-41 & \underline{74}-20 & \underline{80}-10 & 68-21 & 72-19 \\
Reddit & 74-21 & 35-56 & 58-26 & \underline{86}-7 & \underline{89}-7 & 74-19 & 72-19 \\
Storium & 50-40 & 42-45 & 45-42 & 68-15 & \underline{75}-8 & \underline{70}-20 & 68-20 \\
N.Magazine & 71-14 & 57-21 & 50-36 & \underline{79}-7 & 71-21 & 71-21 & 71-0 \\
New Yorker & 53-40 & 27-67 & 53-40 & \underline{67}-20 & \underline{80}-20 & 67-27 & \underline{80}-13 \\
Overall & 60-29 & 38-49 & 51-37 & \underline{75}-14 & \underline{79}-13 & 70-22 & 73-14 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Percentage win-rate for Similarity with Author Story evaluated by GPT-4o and Human (last column). Each cell (`X-Y') shows `X' as the method win-rate and `Y' as the Average Author win-rate, with ties as the remainder. Best win-rates for each source is underlined.}
\label{tab:sim-auth-story}
\small
\begin{tabular}{p{2cm} p{1cm} p{1cm} p{1cm} p{1.25cm} p{1.5cm} p{1.25cm} p{1.75cm} p{1.25cm}}
\toprule
\textbf{Source} & \textbf{Oracle} & \textbf{RAG} & \textbf{Delta} & \textbf{Writing Sheet} & \textbf{Writing Summary} & \textbf{Writing Sheet nP} & \textbf{Writing Summary nP} & \textbf{Human Eval} \\
\midrule
AO3 & 71-18 & 30-54 & 45-32 & 40-41 & \underline{60}-30 & 46-38 & 45-40 & 37-33 \\
Reddit & 91-5 & 37-46 & 49-35 & \underline{54}-35 & 49-42 & 47-40 & 42-44 & 52-37 \\
Storium & 60-30 & 45-40 & 35-35 & 52-32 & 50-35 & \underline{57}-14 & 14-29 & 30-30 \\
N.Magazine & 64-21 & 43-36 & 36-43 & 50-21 & \underline{57}-21 & 43-43 & 50-29 & 48-30 \\
New Yorker & 67-13 & 27-47 & 40-40 & 27-60 & 27-47 & 40-60 & \underline{47}-40 & 70-15 \\
Overall & 71-17 & 36-45 & 41-37 & 45-38 & \underline{49}-35 & \underline{47}-39 & 40-36 & 47-29 \\
\bottomrule
\end{tabular}
\end{table*}




\subsection{Automatic Evaluation}
Our automatic evaluation results show that, first, Writing Sheet and Summary consistently outperform other methods. Second, persona descriptions improve faithfulness to writing history but improve similarity to the ground truth only for select sources like Reddit. Third, personalization is more effective for sources like Reddit than AO3 and Storium, and for narrative categories like Creativity, and Language Use, compared to Plot.


\subsubsection{Faithfulness to Writing History}


Table~\ref{tab:faith-auth-history} shows results for faithfulness to writing history using GPT-4o as \(\text{LLM\textsubscript{story}}\) for our personalization methods. We observe the following:

\paragraph{Writing Sheet and Summary achieve best faithfulness to writing history:}  

Writing Sheet and Summary achieve the highest win-rates across all sources, outperforming Average Author by 64\%. The strong performance of the Writing Summary follows from its reliance on the Author Writing Summary as both a reference for evaluation and a generation constraint. Writing Sheet, despite not being explicitly conditioned on the Author Writing Summary, generalizes well, likely due to its format being the same as the Author Writing Summary. Oracle outperforms Delta and RAG, benefiting from direct conditioning on ground-truth story rules, which enhances adherence to the author's writing history. However, Oracle underperforms the Writing Sheet and Summary, likely due to its lack of explicit conditioning on the Author Writing Summary. Another possible cause is that the author's writing style possibly changes over time, which is not fully captured by the ground-truth story rules. See Appendix~\ref{app:faith-auth-history} for an example.


\paragraph{Reddit shows the highest win-rates among sources:} 
Reddit exhibits the highest win-rates across all methods, suggesting its stories are more conducive to personalization. This trend likely stems from Reddit's crowd-sourced writing prompts, where authors create prompts for others. Therefore, there are a range of diverse topics, ranging from power, survival, and mystery to humor and the supernatural. The broad thematic variety may have enabled personalization methods to capture distinct stylistic elements more effectively, resulting in higher win-rates (See the paragraph Themes in Appendix~\ref{app:dataset}).

\paragraph{Persona descriptions improve faithfulness to writing history:}
We see that adding persona descriptions in the system prompt for Writing Sheet and Summary improves win-rates by around 5\% over their nP variants. This result likely follows from personas enhancing the LLM’s role-playing capabilities, allowing for a more faithful representation of the author’s writing style \citep{jiang2024evaluating, wang-etal-2024-rolellm, wallace2024instruction}.


\subsubsection{Similarity to Author Story}

Table~\ref{tab:sim-auth-story} shows results for similarity to the author story using GPT-4o as \(\text{LLM\textsubscript{story}}\) for our personalization methods. We observe the following:

\paragraph{Writing Sheet and Summary outperform baselines of RAG and Delta:} 
Oracle achieves the highest win-rates across all methods as its story rules come from the ground-truth author story. Reddit shows the highest Oracle win-rate (91\%), followed by 65\% on average for other methods, indicating easier personalization likely due to the broad thematic variety in the writing prompts. Writing Sheet and Summary outperform RAG and Delta by 11\% and 6\%, highlighting the benefit of summarizing the author's writing history \citep{richardson2023integrating}.


\paragraph{Persona descriptions help in sources with broad thematic variety in writing prompts:} 
Writing Summary with persona descriptions outperforms its nP variant by 9\% across all sources. However, Writing Sheet shows no significant difference from its nP variant except for Reddit, where it surpasses it by 7\%. This result suggests that persona descriptions enhance personalization in sources that are easier to personalize, such as Reddit, but have limited impact elsewhere \citep{zheng-etal-2024-helpful}. The broad thematic variety in Reddit's writing prompts creates a greater divergence between author styles and the Average Story, leading to more informative Author Writing Sheets and persona descriptions that further aid personalization. See Appendix~\ref{sec:app-persona-sim-story} for an example.


\paragraph{Creativity and Language Use are the easiest categories to personalize:}


\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\linewidth]{figures/cat_wise/Reddit.pdf}
\caption{Win-rate of personalization methods vs. Average Author for Reddit across four narrative categories. D: Delta, S: Writing Summary, WS: Writing Sheet.}\label{fig:reddit_win_rates}
\end{figure}

Table~\ref{tab:category_comparison_column} (in the Appendix) shows that Delta, Writing Summary, and Writing Sheet outperform Average Author in Creativity (23\% higher) and Language Use (15\% higher) \citep{huot2024agents}, but perform worse in Plot and slightly worse in Development \citep{tian-etal-2024-large-language, xu2024echoes}. Creativity and Language Use are less dependent on the writing prompt, making them easier to transfer across prompts, whereas Plot and Development are more closely tied to specific prompts, making generalization difficult, especially without thematic overlap.  

To better understand personalization performance in Reddit, the easiest source to personalize, Figure~\ref{fig:reddit_win_rates} shows that Writing Sheet is the best-performing method across all categories, with a 35\% advantage over Delta and 20\% over Writing Summary in Creativity. Writing Sheet explicitly summarizes differences between the ground-truth author story and the Average Story, providing more informative guidance for personalization. This advantage is particularly evident in Creativity, which is less constrained by the writing prompt. See Appendix~\ref{app:cat-wise-results} for plots of other data sources.

\subsubsection{Traditional Metrics}  
Table~\ref{tab:traditional-metrics} (in the Appendix) shows results with the traditional metrics. 

\paragraph{Lexical Overlap and Diversity Metrics yield similar values:}
We observe that Lexical Overlap and Diversity metrics yield similar values, as all methods use the same generation model, leading to overlapping lexical distributions and limiting these metrics' ability to capture nuanced stylistic differences \citep{zheng2023judging, xie-etal-2023-next}. 

\paragraph{Writing Sheet outperforms other methods, especially for Style metrics:} We observe that Writing Sheet nP consistently performs best across all metrics, particularly in Style metrics for both Author History and Author Story, as measured by LUAR \citep{rivera-soto-etal-2021-learning}. This improvement likely results from the Writing Sheet explicitly summarizing an author's stylistic deviations from an Average Author, enhancing personalization.


\subsection{Human Evaluation}

Table~\ref{tab:sim-auth-story} shows win-rates from human evaluation (Section~\ref{sec:human-eval-story-gen}) under the Human Eval column, showing that personalization methods—Delta, Writing Sheet, and Summary—outperform the Average Author method by \emph{18\% points} absolute win-rate in terms of similarity to ground-truth writings. See Appendix~\ref{app:human-eval-story-gen} for details on experiment design, and qualitative examples. 

\paragraph{Low inter-annotator agreement due to task subjectivity:}
Fleiss’ Kappa for inter-annotator agreement ranges from 0.2 to 0.4, indicating low but above-random agreement, likely due to the challenges of annotating long-form subjective texts \citep{subbiah-etal-2024-storysumm}. Specifically, different annotators prioritize different narrative categories when determining which story in a pair better reflects the author’s style, leading to disagreement. 

\paragraph{Sources like Reddit and New Yorker have better win-rates than others:} 
Reddit and New Yorker achieve the highest win-rates, with Reddit benefiting from a broad thematic variety in writing prompts and New Yorker from advanced narrative devices like subtext, which humans assess more reliably than LLMs \citep{subbiah-etal-2024-reading}. In contrast, AO3 and Storium exhibit lower win-rates due to LLMs' familiarity with fanfiction tropes (AO3)\footnote{\url{https://archiveofourown.org/admin_posts/25888}} and the lack of strong stylistic differentiation in Storium beyond its open-ended storytelling structure \citep{xu2024echoes, tian-etal-2024-large-language}. These findings suggest that personalization is most effective when authors engage with diverse topics or possess distinct stylistic traits (such as complex narrative structures, creative elements, and advanced language use). These characteristics, reflected in their past writing history, provide clear signals for us to mimic their writing style.

\paragraph{Narrative categories like Creativity, and Language use are more conducive to personalization:} 
Annotators favor personalized stories for Creativity, and Language Use, noting their stronger use of deeper symbolism, thematic richness, layered narratives, and expressive language. However, Average Author was preferred when personalized methods incorrectly introduce elements not present in the author's ground-truth story, suggesting deviations from the author's usual style. 