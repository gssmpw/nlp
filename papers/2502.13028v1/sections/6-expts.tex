\section{Experiments}
In this section, we describe the implementation details and evaluation methods. 

\subsection{Implementation Details}
We prompt GPT-4o in a chain-of-thought manner \citep{wei2022chain, shashidhar-etal-2024-unsupervised} with a temperature of 0.0 and a maximum token limit of 4096 to generate story rules for the Delta and Oracle methods, as well as for generating the Author Writing Sheet, Author Writing Summary, and their respective persona descriptions (\(\text{LLM\textsubscript{persona}}\)) and personalized story rules (\(\text{LLM\textsubscript{rule}}\)). For story generation (\(\text{LLM\textsubscript{story}}\)) across all methods, including \emph{Average Author}, we evaluate three models: GPT-4o \citep{bubeck2023sparks}, Llama 3.1 8B, and Llama 3.1 70B \citep{dubey2024llama}, using a temperature of 0.7 and top\_p of 0.95 \citep{wang-etal-2024-rolellm} (See Appendix~\ref{app:llama-results} for results using Llama models for story generation). Additionally, we implement an ablation variant of our proposed method for Writing Sheet and Writing Summary that excludes the persona description from the system prompt called \emph{Writing Sheet nP} and \emph{Writing Summary nP} respectively.  


\subsection{Automatic Evaluation}

We evaluate the performance of personalized story generation using two automated evaluation methods: 
First, we use GPT-4o-as-a-judge which is shown to be a scalable way to approximate human judgments for complex open-ended tasks \citep{zheng2023judging} and calculate the win-rate of our personalization methods against the non-personalized method, \emph{Average Author} on two aspects: 
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Faithfulness to Writing History}: We evaluate how well the generated story aligns with the author history by measuring win-rates using Claims from the \hyperlink{sec:writing-summary}{Author Writing Summary} as the ground-truth reference for each narrative category \citep{wang-etal-2024-rolellm, wang2023automated, yunusov-etal-2024-mirrorstories}.
    \item \textbf{Similarity to Author Story}: We assess how closely the generated story matches the author's ground-truth story by measuring win-rates using the ground-truth author story as the reference for each narrative category \citep{lyu2024href, shashidhar-etal-2024-unsupervised}.
\end{itemize}
For both aspects, we prompt GPT-4o in a chain-of-thought manner \citep{wei2022chain} to assign scores (1â€“5) to stories. We randomly shuffle the order of compared pairs, one generated by a personalization method and the other by the \emph{Average Author} method, to avoid biases. Scores are broken down into four narrative categories: Plot, Creativity, Development, and Language Use \citep{saha-etal-2024-branch}. The story with the higher score in a category is declared the winner, and the overall winner is based on the highest total score across all categories. Additionally, we evaluate personalization using traditional metrics \citep{xie-etal-2023-next} including lexical overlap, story diversity, and stylistic similarity (see Appendix~\ref{app:trad-merics} for more details).



\subsection{Human Evaluation}
\label{sec:human-eval-story-gen}

We conduct a human evaluation on a subset of our data to further assess subtext and identify any interesting insights that the LLM judges might have missed \citep{chakrabarty2024art, subbiah-etal-2024-reading}, by comparing stories generated by personalization methods (\emph{Delta}, \emph{Writing Sheet}, \emph{Writing Summary}) against the \emph{Average Author} method for similarity to the ground-truth author story \citep{lyu2024href}. We ask annotators to justify their choices after making the comparison. Three annotators, recruited via Upwork (\$17/hour), evaluated 45 author stories in total (15 common to establish agreement and 10 exclusive to each annotator to increase coverage of annotation \citep{song-etal-2024-veriscore}). Each author story had three pairs, each containing a story from one personalization method, resulting in 135 annotated story pairs.
