\section{Related Work}

\subsection{Personalization and Role-Playing}
Recent works have introduced benchmark datasets for personalizing LLM outputs in tasks like email, abstract, and news writing, focusing on shorter outputs (e.g., 300 tokens for product reviews \citep{kumar2024longlamp} and 850 for news writing \citep{shashidhar-etal-2024-unsupervised}). These methods infer user traits from history for task-specific personalization \citep{sun-etal-2024-revealing, sun-etal-2025-persona, pal2024beyond, li2023teach, salemi2025reasoning}. In contrast, we tackle the more subjective problem of long-form story writing, with author stories averaging 1500 tokens. Unlike prior role-playing approaches that use predefined personas (e.g., Tony Stark, Confucius) \citep{wang-etal-2024-rolellm, sadeq-etal-2024-mitigating, tu2023characterchat, xu2023expertprompting}, we propose a novel method to infer story-writing personas from an author’s history to guide role-playing.


\subsection{Story Understanding and Generation}  
Prior work on persona-aware story generation \citep{yunusov-etal-2024-mirrorstories, bae-kim-2024-collective, zhang-etal-2022-persona, chandu-etal-2019-way} defines personas using discrete attributes like personality traits, demographics, or hobbies. Similarly, \citep{zhu-etal-2023-storytrans} explore story style transfer across pre-defined domains (e.g., fairy tales, martial arts, Shakespearean plays). In contrast, we mimic an individual author's writing style based on their history. Our approach differs by (1) inferring long-form author personas—descriptions of an author’s style from their past works, rather than relying on demographics, and (2) handling long-form story generation, averaging 1500 tokens per output, exceeding typical story lengths in prior research.