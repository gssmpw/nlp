\section{Author Writing Sheet}
\label{sec:author-writing-sheet}

Motivated by the connection between writing education and personalization \citep{li2023teach}, we propose a novel method to infer implicit story-writing characteristics of authors based on their history in the profiling set $P$. We group these characteristics into four narrative categories: Plot, Creativity, Development, and Language Use, inspired by narrative theory \citep{pavis1998dictionary, card1999characters, noble1994conflict, huot2024agents}, to assess fine-grained, multi-dimensional story-writing attributes:
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Plot}: Story structure, conflict introduction, prompt engagement, and resolution.
    \item \textbf{Creativity}: Genre blending, unconventional prompt reinterpretation, and unique elements.
    \item \textbf{Development}: Character depth, emotional arcs, and immersive settings.
    \item \textbf{Language Use}: Diction, style, rhetorical devices, pacing, and dialogue.
\end{itemize}


Inspired by Common Core (CC) Standards in English Language and Arts \citep{national2010common}, each characteristic is represented as a Claim-Evidence pair, where the Claim summarizes the author's writing style, and Evidence consists of excerpts from the author-written story to justify the Claim. These characteristics are organized into an \emph{Author Writing Sheet} \citep{gurung-lapata-2024-chiron}.

Stage 1 in Figure~\ref{fig:method} (and Algorithm~\ref{alg:author_writing_sheet} in the Appendix) illustrates our method for generating the Author Writing Sheet by iteratively processing each story in an author's profiling set. For a given writing prompt ($wp_t$) and corresponding author-written story ($s_{at}$), we use an LLM (\(\text{LLM\textsubscript{avg}}\)) to generate an \emph{average story} ($s_{bt}$). Since LLMs are pre-trained on large-scale text data from the web, this average story simulates a typical author's response based on source-specific stylistic patterns 
(see \hyperlink{sec:avg-author}{Average Author}). We prompt an LLM (\(\text{LLM\textsubscript{sheet}}\)) to compare the average and author-written stories and generate Claim-Evidence pairs for each narrative category to form an intermediate Author Writing Sheet $A_t'$, which captures how the author's writing characteristics deviate from those of a typical author for that source \citep{shashidhar-etal-2024-unsupervised, krishna-etal-2020-reformulating}.


To construct the final Author Writing Sheet $A_{|P|}$, we iteratively merge each intermediate Author Writing Sheet $A_t'$ with the aggregated sheet from prior stories $A_{t-1}$, maintaining a moving update similar to \citep{chang2023booookscore}. During merging, we prompt an LLM (\(\text{LLM\textsubscript{combine}}\)) to integrate $A_t'$ into $A_{t-1}$ by grouping equivalent Claims within each narrative category, selecting the best Evidence for grouped Claims, and retaining ungrouped Claims with their respective Evidence. To improve clarity, the updated sheet includes a timestamp identifier for each Evidence corresponding to the story it belongs to and also limits each narrative category to 10 Claim-Evidence pairs, prioritizing grouped Claims. For \(\text{LLM\textsubscript{avg}}\), \(\text{LLM\textsubscript{sheet}}\), and \(\text{LLM\textsubscript{combine}}\) we use GPT-4o prompted in a chain-of-thought manner with a temperature of 0.0 and a maximum output of 4096 tokens \citep{wei2022chain, shashidhar-etal-2024-unsupervised}.

Our method processes each story separately, making the update process efficient when new submissions are added \citep{yeh2024ghostwriter, yuan2022wordcraft}. This method avoids the need to reprocess all previous stories, reducing computational cost and mitigating the limitations of long-context inputs in LLMs \citep{zhou2023don, magar-schwartz-2022-data, li2024long}. This approach draws parallels to \textit{Knowledge Tracing} in education research, where a model continuously updates its understanding of a student's progress without re-evaluating past interactions \citep{liu-etal-2022-open, scarlatos2024exploring}. See Appendix \ref{app:cost-analysis} for cost analysis.



\paragraph{Validating Author Writing Sheets:}

We conduct a human evaluation with three Upwork annotators (\$17/hour). Annotators assess the Claim-Evidence pairs based on two criteria: (1) whether the Claims can be reasonably inferred from the story (Yes/No) and (2) whether the Evidence supports the Claims (Yes/Partially/No). The study includes 42 authors spanning all five sources, each represented by one sampled story resulting in 188 annotated claims. Of these, 12 stories are annotated by all annotators to measure inter-rater agreement, while each annotator evaluates 10 additional exclusive stories. Results show unanimous agreement on the `Yes' label for Claim inference and moderate agreement on Evidence support (Krippendorffâ€™s Alpha$=.57$), with 93\% of Evidence fully supporting Claims and 7\% providing partial support. These findings confirm the reliability and quality of the Author Writing Sheets and support their use in personalized story generation (see Appendix~\ref{app:human-author-sheets} for more details).
