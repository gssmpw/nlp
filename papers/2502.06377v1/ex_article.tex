% SIAM Article Template
\documentclass[onefignum,onetabnum]{siamart171218}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric Positive Definite Matrices with Applications to Covariance Matrices},
  pdfauthor={A. Paterson, J. Pestana,  V. Dolean}
}
\fi
\usepackage{comment}
% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.
\usepackage{subcaption}   
\usepackage{multirow}
\usepackage{setspace} 
%% Use \myexternaldocument on Overleaf
\myexternaldocument{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>
\newcommand{\sig}{\mathcal{H}} %{\boldsymbol{\Sigma}}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\sigg}{\boldsymbol{\Sigma}}
\newcommand{\rv}{\mathbf{X}}
\newcommand{\rvy}{\mathbf{Y}}
\newcommand{\RV}{\rv=[X_{1}, X_{2}, \ldots, X_n]'}
\let\b\mathbf
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Q}{\mathcal{A}}
\newcommand{\Perm}{\mathcal{P}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\tsig}{\tilde{\mathcal{H}}} %{\tilde{\boldsymbol{\Sigma}}} 
\newcommand{\tsigg}{\tilde{\boldsymbol{\Sigma}}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Qdim}{\mathcal{A} \in \mathbb{R}^{(m+n) \times (m+n)}}
\usepackage{dsfont}
\usepackage{todonotes}
\def\vic#1{\todo[inline, color=blue!40]{#1}}

\usepackage{algorithmic} % <- Preamble
\usepackage{cleveref}
\Crefname{ALC@unique}{Line}{Lines} % <- Preamble

\begin{document}
\maketitle
\begin{abstract}
Obtaining the inverse of a large symmetric positive definite matrix $\mathcal{A}\in\mathbb{R}^{p\times p}$ is a continual challenge across many mathematical disciplines. The computational complexity associated with direct methods can be prohibitively expensive, making it infeasible to compute the inverse. In this paper, we present a novel iterative algorithm (IBMI), which is designed to approximate the inverse of a large, dense, symmetric positive definite matrix. The matrix is first partitioned into blocks, and an iterative process using block matrix inversion is repeated until the matrix approximation reaches a satisfactory level of accuracy. We demonstrate that the two-block, non-overlapping approach converges for any positive definite matrix, while numerical results provide strong evidence that the multi-block, overlapping approach also converges for such matrices.


%Computing (parts of) a covariance matrix from its inverse, the precision matrix, proves to be a continual challenge in multivariate statistics. The computational complexity associated with direct methods can be prohibitively expensive, making it infeasible to compute the inverse, even for smaller dimensions. In this paper, a novel iterative algorithm is presented, which is designed to approximate the covariance matrix from the precision matrix efficiently. An iterative process of partitioning the precision matrix and using block matrix inversion is repeated until the covariance matrix approximation reaches a satisfactory level of accuracy. %This approach reduces the overall computational complexity, providing a solution for handling larger covariance matrices. The two-block method is shown to converge for any Hermitian positive definite matrix, while numerical evidence suggests that multi-block, overlapping approaches also converge for such matrices. The method is validated on a number of test examples from statistics and other applications.
\end{abstract}

% REQUIRED
\begin{keywords}
  symmetric positive definite matrix, block matrix inversion, covariance matrix 
\end{keywords}
\begin{AMS}
65F05  15A09
\end{AMS}

\section{Introduction} \label{sec:lit review}
%Obtaining the inverse, or selected elements of the inverse, of a large symmetric positive definite matrix $\mathcal{A}\in\mathbb{R}^{n\times n}$ arises in a number of fields, including computational physics, machine learning and Bayesian statistics. In particular, a number of important computations in statistics require the inverse of positive definite matrices, or certain sub-matrices of these, e.g., within Gaussian process regression. For large-scale problems this task remains expensive in terms of both computation and memory, and although there are methods available for computing selected elements of the inverse, sub-matrices of the inverse, or the inverse itself, the task remains challenging. 
Finding the inverse of a large, symmetric positive definite matrix is crucial in various fields such as Bayesian inference \cite{Rue_2023}, computational physics \cite{FIND.ALG}, and medical imaging \cite{medical_imaging}. The difficulty in obtaining the inverse of a symmetric positive definite matrix $\Q \in \mathbb{R}^{p \times p}$, where $\sig=\Q^{-1}$, lies in the computational expense of doing so. Direct inversion techniques, such as those based on Gaussian elimination, can require $\mathcal{O}(p^3)$ flops and have a $\mathcal{O}(p^2)$ storage cost \cite[\textsection3.11]{Duff_chpt3}, making it infeasible to calculate the direct inverse for larger matrices. 
%Direct methods of inverting $\sig=\Q^{-1}$ involving Gaussian elimination can cost upwards of $\mathcal{O}(p^3)$ for dense matrices. The Cholesky decomposition is a popular factorisation technique used to decompose a matrix $\b{A}$ into the product of lower triangular matrices $\b{A}= \b{L} \b{L}^{\top}$. However, the computational costs associated with this is equivalent to calculating the direct inverse $\b{A}^{-1}$, with a computational complexity of up to $\mathcal{O}(n^{3})$. Additionally, the memory storage required to store $\b{L}$ can become problematic when dealing with larger problems.
%
One well-known method to invert a (dense) symmetric positive definite matrix is to use the Cholesky factorisation to decompose a matrix $\Q$ into the product of lower triangular matrices $\Q= \b{L} \b{L}^{\top}$. Then, $\Q^{-1}$ is obtained by first solving the $p$ linear systems $\b{L} \b{z}_i = \b{e}_i$, where $\b{e}_i$ is the $i$th unit vector, and then solving $\b{L}^\top\b{h}_i = \b{z}_i$, where $\b{h}_i$ is the $i$th column of $\sig = \Q^{-1}$. %by first finding $\b{L}^{-1}$, the inverse  $\sig = \Q^{-1}$ is found by $\sig = \left(\b{L} \b{L}^{\top} \right)^{-1} = \b{L}^{-\top}\b{L}^{-1}$. 
These three steps to obtain $\sig$ can be combined into one sweep as described in \cite{SPD_inversion}. %by adopting parallel computing techniques such as load balancing. 
Alternatively, $p$ linear systems could be solved using a method such as the preconditioned conjugate gradient (PCG) method, which can solve large symmetric positive definite linear systems of the form $\Q\b{x} = \b{b}$. For dense matrices that can be represented using a hierarchical low-rank format, with invertible diagonal blocks, it is also to approximate the entire inverse (see, e.g., \cite[\textsection  2.8]{bebendorf}).
\\ \\ 
There exist numerous methods to obtain the full (or partial) inverse of large \textit{sparse} symmetric positive definite matrices. In 1973, Takahashi et al.\ derived a method for sparse matrix inversion \cite{TAKAHASHI}, that was  further analysed by Erisman and Tinney \cite{Erisman.Taka}. The starting point for the method is the observation that, given a  symmetric, non-singular matrix $\Q \in \mathbb{R}^{p \times p}$ and its $\b{LDL}^{\top}$ factorisation $\Q = \b{LDL}^{\top}$, the inverse satisfies:
%Starting with symmetric, non-singular matrix $\b{A} \in \mathbb{R}^{n \times n}$, the inverse is obtained by performing an $\b{LDL}^{\top}$ factorisation and then solving for  $\b{A}^{-1}$ \cite{Erisman.Taka}: 
\begin{align} \label{eq:taka1}
    \sig &= \b{D}^{-1}\b{L}^{-1} + \left(\b{I}-  \b{L}^{\top} \right) \sig.
\end{align}
The key observation is that $\b{I}- \b{L}^{\top}$ involves only the upper triangular part of $\Q$. Thus, if we wish to compute elements $\sig_{ij}$, $i\le j$ in the upper triangular part of $\sig$ (which, since $\Q$ is symmetric, also computes elements $\sig_{ji}$ in the lower triangular part), we can work with triangular matrices only. 
%he above equation is useful as the inverse can be found without having to calculate $\b{L}^{-1}$ first as \eqref{eq:taka1} can be rewritten as:
This leads to the recursive formula: 
\begin{equation} \label{eq:taka2}
    h_{ij} = d_{ij}^{-1} - \sum_{k>i}^{n} l_{ki}h_{kj} \ \text{ for} \ i\leq j,
\end{equation}
for elements of $\sig=\Q^{-1}$.
%\vic{we don't see immediately the formula is recursive since the definition of $z_i = L^Th_i$ is hidden in the text above}
By ordering $\Q$ so that the desired elements of $\sig$ will occur in its lower-right corner, we can compute these desired elements with fewer computations. 
%When $i\neq j$, the first term $d_{i,j}$ is discarded and \eqref{eq:taka2}, can be used to calculate selected elements in $\b{A}^{-1}$ by recursively calculating each $z_{ij}$ for each $i,j = n , \ldots, 1$. 
The computational cost also depends on the sparsity of $\b{L}$, since we may find that  many $l_{ik}=0$. We note that Rue and Martino \cite{RUE20073177} generalise the Takahashi recurrences to enable them to compute the marginal variances for Gaussian Markov random fields (GMRFs) with additional constraints. \\ \\ 
Other algorithms based on Gaussian elimination  for finding a partial inverse of a sparse symmetric matrix include %We mention an algorithm by  Liu, et al., \cite{Liu2014SUPERFAST} that computes arbitrary elements of the inverse of hierarchically semiseparable (HSS) matrices, which tend to arise naturally from discretized PDEs. The algorithm uses the randomised, structured multifrontal block $\mathbf{LDL}^{\top}$ factorisation, which breaks down larger problems into more manageable frontal matrices which can be processed independently, improving scalability. 
the Selinv \cite{SelInv} and the FIND algorithm \cite{FIND.ALG}, which were developed to solve the non-equilibrium Green's function to calculate electron densities. The Selinv method exploits the block structure of each supernode in a left-looking supernodal $\b{L}\b{D}\b{L}^{\top}$ factorisation to compute selected elements of $\sig$. 
%Selinv starts by factorising a sparse symmetric matrix using a super nodal left looking $\b{L}\b{D}\b{L}^{\top}$ factorisation. The block structure of each supernode can then be exploited to find the selected elements $\b{A}_{ij}$ for $\b{L}_{ij} \neq 0$. 
The FIND algorithm can be used to compute diagonal elements of the inverse of a symmetric matrix, using a factorisation based on a bottom-up \textbf{LU} factorisation after appropriate reordering by nested dissection. 
%Similarly, the FIND algorithm, which also relies on a computational efficient factorisation, can be used to calculate the inverse of the diagonal of the matrix. Nested dissection is used to reorder the matrix, followed by \textbf{LU} factorisation in a bottom up manner, therefore reducing the number of fill-ins. 
\\ \\ 
Obtaining the covariance matrix from its inverse, also known as the precision matrix, is a well-known challenge within multivariate statistics. The covariance matrix is a dense symmetric positive definite matrix $\sig \in \mathbb{R}^{p\times p}$, unlike its inverse, the precision matrix, $\Q \in \mathbb{R}^{p\times p}$ which is often sparse. If only the diagonal of $\sig$ is required, Hutchinson's stochastic estimator \cite{Hutchinson} can be applied: 
%needed to be obtained, Hutchinson's stochastic estimator \cite{Hutchinson} could be used: 
\begin{equation} \label{Hutch}
    \text{diag}(\sig) \approx  \left[ \sum_{k=1}^{K} \z_{k} \odot \mathcal{H} \ \z_{k}  \right] \oslash  \left[ \sum_{k=1}^{K} \z_{k} \odot \z_{k} \right],
\end{equation}
where elements of the random vectors $\z_{k}$, for  $k\in\{1,\ldots,K\}$, take the value $1$ or $-1$ with equal probability. Here, $\odot$ represents element-wise multiplication (the Hadamard product) of the vectors and $\oslash$ represents their element-wise division. 
\\ \\ %Furthermore, in multivariate statistics, using the precision matrix denoted $\b{Q}$, to obtain selected elements of its inverse, the covariance matrix $\boldsymbol{\Sigma}$, has become increasingly sought after in recent years. 
The full covariance matrix $\sig = \Q^{-1}  \in \mathbb{R}^{p\times p}$ can be approximated using a Monte Carlo method that first computes $N_s$ samples $\z_k \sim \mathcal{N}(0,\Q^{-1})$, $k = 1,\dotsc, N_s$, using, e.g., the approaches in \cite{ChowandSaad, pap_yuille_2010, papandyuille}. These samples are then used to form the Monte Carlo estimator mentioned in \cite{papandyuille}, which has the standard Monte Carlo convergence rate of $\mathcal{O}(N_s^{\frac{1}{2}})$:
\begin{equation} \label{MC Estimators}
    \b{\hat{\mathcal{H}}_{MC}} = \frac{1}{N_{s}}\sum_{j=1}^{N_{s}} \b{z}^{j}\b{z}^{j\top}  =\frac{1}{N_{s}}\b{ZZ}^\top, %\qquad  \sigma^{2}_{MC, i}=\frac{1}{N_{s}} \sum_{j=1}^{N_{s}} \big( z_{i}^{(j)} \big)^{2}, 
    \qquad 
    \b{Z} =[\z_1,\z_2, \ldots, \z_{N_s}].
\end{equation}
%However, since this is a Monte-Carlo method, its convergence rate is $\mathcal{O}(N_s)^{\frac{1}{2}}.$ 
In 2018, Sidén et al. \cite{Finn} developed three Rao-Blackwellized Monte Carlo (RBMC) estimators for approximating elements of $\sig$ that improve on \eqref{MC Estimators} by combining it with the Law of Total Variance. %The notation $i^{c}$ refers to the complement of $i$. Note that because $z_i$ is a single element, $Q_{i,i}$ is also a single element of the precision matrix. 
One of these, the Block RBMC estimator,  approximates a principal sub-matrix of  $\sig$. The block estimator requires two sets $\I$, and $\I^c$  that partition the row/column indices of   $\ \in \mathbb{R}^{p\times p}$, i.e., $ \I \cup \I^c = \{1, \ldots, p\}$, $\I\cap \I_c = \emptyset$. The matrix $\hat{\sig}_{\I}$ is then defined to be the principal sub-matrix of the approximate inverse, corresponding to the elements in the rows and columns indexed in the set $\I$. The block RBMC estimator is then defined as: 
\begin{align}
\hat{\sig}_{\I} &\approx \Q_{\I}^{-1}+\frac{1}{N_s} \Q_{\I}^{-1} \Q_{\I, \I^{c}} \b{Z}_{\I^{c}} \left(\b{Z}_{\I^c}\right)^{\top} \! \!  \left(\Q_{\I,\I^c}\right)^{\top} \Q_{\I}^{-1}. \label{eq:block RBMC} \vspace{-0.20cm}
\end{align}
As for the simple Monte Carlo estimator \eqref{MC Estimators}, here $N_s$ is the number of Gaussian samples $\z_k \sim \mathcal{N}(0,\Q^{-1})$, while $\b{Z}_{\I^{c}}$ represents the sub-matrix of $\b{Z}$ in \eqref{MC Estimators} formed from the rows indexed by $\I^{c}$. 
When $|\I|=1$, the Block RBMC estimator becomes the simple RBMC estimator described in \cite{Finn}, which can compute one marginal variance. The authors also describe an iterative interface method based on the Block RBMC estimator in \cite{Finn}, that can more accurately approximate the diagonal of $\sig$ than  Hutchison's estimator in \eqref{Hutch} but at a higher computational cost. The iterative interface method is designed to compute selected elements of the covariance matrix, but it cannot approximate all elements of $\sig$ simultaneously \cite[\textsection 3.2.2]{Finn}. 
%\vic{We should probably clearly state somewhere: the main novelty of of the paper is...perhaps mention the downsides of the previous method and summarise the main results}
\\ \\
Zhumekenov et al., \cite{Rue_2023} presented an alternative method of selected inversion for spatio-temporal Gaussian Markov random fields (GMRFs) which includes recovering the marginal variances starting from the precision matrix. Their method is a hybrid approach, taking inspiration from the RBMC estimators from Sidén et al. \cite{Finn}, and Krylov subspace methods, which are becoming increasingly popular for solving large linear systems in multivariate statistics. 

%\vic{Before the summary of the paper I would underline the main contributions of the paper and improvements with respect to the state of the art (what are the main gains, what do we want to achieve and what are the main applications we target), but perhaps it is planned to do it later? I feel that if it is included in the summary the reader will miss the main point}
%
\subsection{Main Contributions}
The existing literature provides numerous methods for computing selected elements of the inverse of a symmetric positive definite matrix. However, there is still a notable gap of approaches which can accurately and efficiently approximate a \textbf{full} inverse, as current methods are not able to accurately approximate all the off-diagonal elements.  
% for the inverse of a symmetric positive definite matrix. 
In this paper, we introduce the following contributions, which aim to reduce this gap.
\begin{itemize}
    \item \textbf{Novel iterative block matrix inversion algorithm (IBMI).} We advance the current literature by proposing a novel block matrix inversion algorithm, designed to efficiently approximate the \emph{whole} inverse of a dense symmetric positive definite matrix. Using the Block RBMC estimator as a starting point, we establish a link between \cref{eq:block RBMC} and block matrix inversion. A breakdown of how the algorithm iteratively updates the approximated inverse through block matrix inversion will be provided. Notably, our algorithm achieves an accurate approximation of the inverse not only for the principal sub-matrices, but also for the off-diagonal elements, addressing a significant limitation with current methods. 
    %which can be used to accurately approximate the inverse of any dense symmetric positive definite matrix, including the off-diagonal elements which are a known struggle to approximate.
    %The algorithm provides an efficiently and accurate way of approximating the off-diagonal elements, something which the existing literature struggles to accomplish.
    \item \textbf{Analysed convergence, cost, and error bound.} When $\Q$ is partitioned into two non-intersecting sets, the algorithm is guaranteed to converge for any symmetric positive definite matrix $\Q$. This has been shown both theoretically and numerically, and a bound is derived for the error after each iteration. When the algorithm is generalised to the multi-block overlapping case, numerical results show that the algorithm can also converge. Additionally, 
    %for specific scenarios, 
    we show that \cref{alg:IBMI} can outperform direct methods such as MATLAB's inversion function (\texttt{inv}). This advantage is further explored in the breakdown of the cost of the algorithm, where we show when the algorithm converges in one iteration it can outperform direct methods in terms of complexity.

    \item \textbf{Applications.} The algorithm is applicable to any symmetric positive definite matrix. However, we choose to focus on covariance matrices  when performing numerical experiments. This was motivated by the 
    abundance of applications that require the inverse of a covariance matrix, known as the precision matrix, in multivariate statistics and data science e.g., Gaussian process regression \cite[\textsection 2]{GPR_application}. A lot of the literature reviewed in \cref{sec:lit review} focussed on the (partial) inversion of sparse symmetric positive definite matrices. The IBMI algorithm is a novel method which can obtain the inverse of both sparse and \emph{dense} symmetric positive definite matrices.%, addressing a gap which is not covered in current methods.  %is applicable to any symmetric positive definite matrix $\Q$. 
\end{itemize}
%
The paper is structured as follows; \cref{sec:background} details the novel iterative block matrix inversion algorithm. The convergence of the IBMI algorithm is proven in \cref{sec:conv} and the computational cost is discussed in \cref{sec:cost}. Numerical results in \cref{sec:graphs} will confirm theoretical findings and illustrate the performance of the IBMI algorithm on cases not covered by the theory. Finally, a discussion will conclude the paper in \cref{sec:conclusions}.

%the link between the block RBMC estimator \eqref{eq:block RBMC} and block matrix inversion to create 

%As we will show in \cref{sec:background}, the block RBMC estimator \eqref{eq:block RBMC} can also be understood in terms of approximate block matrix inversion of the matrix $\b{Q}$. By exploiting this fact, we propose a novel block matrix inversion algorithm in \cref{sec:alg}, which improves upon the initial approximation \eqref{eq:block RBMC} and is applicable to any symmetric positive definite matrix $\Q$. Mathematical analysis is provided for the iterative algorithm to show we can gain an approximation of the \emph{whole} matrix $\sig$. We then prove that, when the rows of $\Q$ are partitioned into two non-intersecting sets, the algorithm is guaranteed to converge for any symmetric positive definite matrix $\Q$. 

 


\section{An Iterative Algorithm for Matrix Inversion} \label{sec:background}
%\vic{I would move technical aspects, like notations, definitions and properties from the introduction to the section they are used. I don't have a strong opinion about this but I think it would improve readability}
%\vic{I would also announce the reasoning leading to our algorithm, at the beginning of the section and perhaps adding a first subsection entitled "link between the MC estimator and Schur complement", explain briefly why is not a good choice in order to motivate further improvements}
%
The motivation for, and development of, the iterative block matrix inversion algorithm (IBMI) will be detailed in this section. We first start by making the link between the Block RBMC estimator in \cref{eq:block RBMC} and block matrix inversion. Details of the IBMI algorithm will then be given, first for the simplest partitioning -- the two-block, non-overlapping case -- and then for the multi-block overlapping case. %Then the full IBMI algorithm will be shown, along with a final remark on the algorithm's initial guess. 

\subsection{Link between the RBMC Estimator and Block Matrix Inversion}
It will now be demonstrated that the Block RBMC estimator in \cref{eq:block RBMC} can also be obtained by approximate block matrix inversion. To do so, we first recall the two index sets, $\I$ and $\I^c$,  from  \cref{eq:block RBMC} that partition the row/column indices of $\Q \in \mathbb{R}^{p\times p}$, and that satisfy $\I \cup \I^c = \{1, \ldots, p\}$, $\I\cap \I_c = \emptyset$. Then, we permute the matrix $\Q\in\mathbb{R}^{p\times p}$ so that the rows and columns corresponding to indices in $\I$ appear first, and then partition this permuted matrix $\Perm\Q\Perm^\top$ as: 

\begin{equation} \Perm\Q \Perm^\top= {\begin{bmatrix} 
 \Q_{\I} & \Q_{\I,\I^c} \\ 
 \Q_{\I^c,\I} & \Q_{\I^c}
 \end{bmatrix}} , \qquad \text{where} \quad \I \cup \I_c = \{1,\dotsc,p\}.
 %\begin{bmatrix} \label{Q_partitioning}
 %\Q_{\I,\I} & \Q_{\I,\I^c} \\ 
 %\Q_{\I^c,\I} & \Q_{\I^c,\I^c}
 %\end{bmatrix},
\end{equation}
The matrix $\Q_{\I}$ has rows and columns indexed by $\I$, $\Q_{\I_c}$ has rows and columns indexed by $\I_c$, $\Q_{\I,\I_c}$ has rows indexed by $\I$ and columns by $\I_c$ and $\Q_{\I_c,\I}=\Q_{\I,\I_c}^\top$. Then, the well known block matrix inversion formula (see, e.g., \cite[pg.19]{SchurComplementTxbk}) gives: 
\vspace{-0.15cm}
\begin{equation}
\label{BMIQ}
\begin{aligned}
\Perm \!\Q^{-1} \! \Perm^\top& = \! 
\begin{bmatrix}     
    \Q_{\I}^{-1} \! + \! \Q_{\I}^{-1}\Q_{\I,\I^c} \sig_{\I^c} \left(\Q_{\I,\I^c}\right)^{\top} \! \Q_{\I}^{-1} & 
    -\Q_{\I}^{-1}\Q_{\I,\I^c} \sig_{\I^c} \\ 
    - \sig_{\I^c}\! \left(\Q_{\I,\I^c}\right)^{\top} \!\Q_{\I}^{-1} & 
    \sig_{\I^c}
    \end{bmatrix} \\
     & = \! 
    \begin{bmatrix}
        \sig_{\I} & \! \! \! \! \sig_{\I,\I^c} \\ 
         \sig_{\I^c,\I} & \! \! \! \!\sig_{\I^c}
    \end{bmatrix} \\& = \! \Perm\sig\Perm^\top,
\end{aligned}
\end{equation}
where $\sig_{\I^c} =  (\Q_{\I^c} - (\Q_{\I,\I^c})^{\top} \Q_{\I}^{-1} \Q_{\I,\I^c})^{-1}$ is the inverse of the Schur complement. Inverse permutations can then be applied to recover $\sig=\Q^{-1}$. A link can now be made with the Block RBMC estimator, as the top left principal sub-matrix in \cref{BMIQ} looks almost equal to the Block RBMC estimator \cref{eq:block RBMC}, which can be rewritten as:
\begin{align*}
    \hat{\sig}_{\I} \approx  \Q_{\I}^{-1} + \Q_{\I}^{-1}\Q_{\I,\I^c} \tsig_{\I^c}\Q_{\I^c,\I} \Q_{\I}^{-1} \approx \sig_{\I}, 
    \qquad 
     \tsig_{\I^c}= \frac{1}{N_s}&\b{Z}_{\I^c} 
     (\b{Z}_{\I^c})^{\top}.
\end{align*}
Thus, by approximating the inverse of the Schur complement $\tsig_{\I^c}$, an approximation of the top left principal sub-matrix $\tsig_{\I}$, can be obtained. Crucially, approximations to the off-diagonal sub-matrices of the first matrix in  \eqref{BMIQ} can also be obtained without additional computations (because  $\Q_{\I}^{-1} \Q_{\I,\I_c} \tsig_{\I^c}$ is required to compute $\tsig_{\I}$) and an approximation of the complete matrix $\tsig$ can be obtained. %, without having to calculate the expensive Schur complement inverse. 
%Each block of 
The resulting approximated matrix $\tsig$ is:
%can therefore be calculated as: 
\begin{align} \label{eq:IBMI}
    \Perm\tsig\Perm^\top = 
    \begin{bmatrix}
        \Q_{\I}^{-1}+ \Q_{\I}^{-1} \Q_{\I,\I^c} \tsig_{\I^c} \Q_{\I^c,\I}\Q_{\I} ^{-1} & 
         -\Q_{\I}^{-1} \  \Q_{\I,\I^c}\tsig_{\I^c}\\ 
         - \tsig_{\I^c}\Q_{\I^c,\I}\Q_{\I}^{-1} &  \tsig_{\I^c}
    \end{bmatrix}.
\end{align}
The Monte Carlo estimator in \eqref{MC Estimators} could be used for the Schur complement approximation $ \tsig_{\I^c}$, but this is certainly not the only choice. Other possible choices for the initial guess will be discussed at the end of \cref{sec:alg}. \\ \\ 
%
\textbf{The Two-Block Non-Overlapping Case}\label{sec:two-block}
\\ %
Numerical evidence suggests the approximation in \cref{eq:IBMI} may not be very accurate, as $|\tsig_{ij} - \sig_{ij}|$ $i,j = 1,\dotsc p$, may be large when $|i-j|$ is large, i.e., elements in the off-diagonal blocks may be poorly approximated. To measure this initial approximation, symmetric positive definite matrices were generated using the RBF covariance kernel (given in \cref{tab:kernels}, discussed in \cref{sec:graphs}) and the error of the first approximation was recorded using the error estimate in \cref{appx:A}. The smallest matrix, of dimension $p=2^6$, had an error of 0.856886. As the dimension of the matrix increased, the error increased linearly, and the largest matrix, of dimension $p=2^{14}$, had an error of 20.9872. This trend was consistent with other matrices tested.\\ \\%with  qualitatively similar properties. \\ \\ 
%
This initial approximation can be improved by iteratively updating the matrix, as we describe in this section. The key idea involves choosing different sets of indices for $\I$, and applying the block matrix inversion formula in \eqref{eq:IBMI} using elements of the most recently computed $\tsig$ to approximate $\tsig_{\I^c}$. 
\\ \\ 
%\vic{I think the phrase "in our experience" is a bit too vague, perhaps say "numerical/empirical evidence shows that" and also give an estimate of how large this could be. I think the notion of accuracy needs to be explained, what we expect, even if it is properly defined later etc...}
%The above block matrix inversion equation \eqref{eq:IBMI} can be used to produce an accurate approximation of $\sig$, by partitioning the matrix into blocks using indices $\I=\{1,2, \ldots, n\}$, and using the most recently computed matrix to approximate $\tsig_{\I^c}$. 
% 
For simplicity, the two-block non-overlapping case for a matrix $\Q \in \mathbb{R}^{p\times p}$ will be discussed here. In this case, two non-intersecting sets, $\I_1$ and $\I_2$, are introduced, where $ \I_1 \cup \I_2 = \{1,2,\ldots, p\},\  \I_1 \cap \I_2 = \emptyset$. At each iteration, denoted $r = 1,2,\dotsc$,  we cycle through these two sets, with the current set indicated by $k \in \{1,2\}$. The notation $\tsig^{(r,k)}$ is used to keep count of the iteration and set, $r$ and $k$, when updating the approximated matrix $\tsig$. Additionally, permutation matrices are denoted by $\Perm_k \in \mathbb{R}^{p\times p}$, where $\Perm_k$ permutes the rows of a matrix so that those indexed by elements of $\I_k$ appear before those indexed by elements of $\I_k^{c}$.
%Then, the permutation matrix is introduced $\Perm_{1} \in \mathbb{R}^{p\times p}$, which permutes the rows of a matrix so that those indexed by elements of $\I_1$ appear before those indexed by elements of $\I_1^{c}$. 
%\vic{Just a suggestion: the two paragraphs below a slightly too descriptive, can we introduce a definition in which we explain synthetically the algorithm and the add a remark to underline the special cases. I know it's not easy to explain it, but I think it would improve readability}
\\ \\ %
\textbf{Iteration 1, Set 1}
%We start the $r=1$ iteration by using set $k=1$ to define $\I$, 
We first set the iteration index $r=1$. Then, the set index $k=1$ is used to determine $\I$ in \eqref{eq:IBMI}, i.e., we let $\I=\I_1$ and $\I^c=\I_1^c$. An initial guess is made for the inverse of the Schur complement, $ \tsig^{(0,1)}_{\I_1^c}$, and is substituted into \eqref{eq:IBMI} to give the first approximation: 
%where the notation $\tsig^{(r,k)}$ and $\tsig^{(r,k)}_{\I_1^c}$ is used here to denote both the iteration, $r$, and the index set, $k$, that defines $\I$. Substituting $ \tsig^{(0,1)}_{\I_1^c}$ into \eqref{eq:IBMI} gives the first approximation: 
\begin{equation} \label{eq:iter1 set1}
\begin{aligned}
    \Perm_1\tsig^{(1, 1) } _{\I_1}\Perm_1^{\top} &= \begin{bmatrix}\Q^{-1}_{\I_1} \! +\! \Q^{-1}_{\I_1}  \Q_{\I_1,\I_1^c}  \boxed{{}{\tsig}^{(0,1)}_{\I_1^c}} \  \Q_{\I_1^c,\I_1} \Q_{\I_1}^{-1} &
- \Q_{\I_1}^{-1} \Q_{\I_1,\I_1^c}  \boxed{{} \tsig^{(0,1)}_{\I_1^c}} \ \\
- \ \boxed{{}\tsig^{(0,1)}_{\I_1^c}} \ \Q_{\I_1^c,\I_1} \Q_{\I_1}^{-1} &  \boxed{{}\tsig^{(0,1)}_{\I_1^c}} 
\end{bmatrix}\\
&=\begin{bmatrix}
\tsig_{\I_1}^{(1,1)} & \tsig_{\I_1, \I_1^{c}}^{(1,1)} \\
\tsig_{\I_1^{c},\I_1}^{(1,1)} & \tsig_{ \I_1^{c}}^{(0,1)} 
\end{bmatrix}
. 
\end{aligned}
\vspace{0.15cm} 
\end{equation}
%
Note that having just two, non-overlapping sets leads to the special case where $\I_1^{c}=\I_2$, and vice versa.  Hence, $\Q_{\I_1} \equiv \Q_{\I_2^c}$ and $\Q_{\I_2} \equiv \Q_{\I_1^c}$. Therefore $\Perm_1\tsig^{(1, 1) }_{\I_1}\Perm_1^{\top}$, can be re-written as: 
\begin{equation*}
\Perm_1\tsig^{(1, 1)}_{\I_1}\Perm_1^{\top} = 
    \begin{bmatrix}
\tsig_{\I_1}^{(1,1)} & \tsig_{\I_1, \I_2}^{(1,1)} \\
\tsig_{\I_2,\I_1}^{(1,1)} & \tsig_{\I_2}^{(0,1)} 
\end{bmatrix}.
\end{equation*}
%
%Moreover, $\tsig_{\I^{c}}^{(1,1)} = \tsig_{\I^{c}}^{(0,1)}$. 
%
% I think this is correct but not needed until the next section $\tsig^{(1,2)}_{\I_2^c} \equiv \tsig_{\I_1}^{(1,1)}$
% 
\textbf{Iteration 1, Set 2} \\ %
Now, set $k=2$, so that $\I=\I_2$ and $\I^c=\I_2^c=\I_1$ in \eqref{eq:IBMI}. %, which permutes a matrix so that the rows indexed by elements of $\I_2$ appear before those indexed by elements of $\I_2^{c} = \I_1$. 
Then, an updated approximation of the matrix $\sig$ is obtained from \cref{eq:IBMI} and the permutation matrix $\Perm_2 \in \mathbb{R}^{p\times p}$. However, instead of using the initial guess, $\tsig_{\I_2}^{(0,1)}$, as an approximation of the inverse of the Schur complement, as in the previous approximation, we set $\tsig_{\I^c_2} = \tsig_{\I_1}^{(1,1)}$ in \cref{eq:IBMI}. %Therefore $ \tsig_{\I_2^c}^{(1,2)} = \tsig_{\I_1}^{(1,1)}$. %when selecting the inverse of the Schur complement. 
The updated matrix approximation using $\I_2$ is then 
\begin{equation} 
\label{eq:2block2}
\begin{aligned}
\Perm_2\tsig^{(1,2)}_{\I_2}\Perm_2^\top &= \begin{bmatrix}\Q^{-1}_{\I_2} \! +\! \Q^{-1}_{\I_2}  \Q_{\I_2,\I_2^c}  \boxed{{}{\tsig}^{(1,1)}_{\I_1}} \  \Q_{\I_2^c,\I_2} \Q_{\I_2}^{-1} &
- \Q_{\I_2}^{-1} \Q_{\I_2,\I_2^c}  \boxed{{} \tsig^{(1,1)}_{\I_1}} \ \\
- \ \boxed{{}\tsig^{(1,1)}_{\I_1}} \ \Q_{\I_2^c,\I_2} \Q_{\I_2}^{-1} &  \boxed{{}\tsig^{(1,1)}_{\I_1}} 
\end{bmatrix}\\
&=\begin{bmatrix}
\tsig_{\I_2}^{(1,2)} & \tsig_{\I_2, \I_1}^{(1,2)} \\
\tsig_{\I_1,\I_2}^{(1,2)} & \tsig_{ \I_1}^{(1,1)} 
\end{bmatrix}. 
\end{aligned}
\vspace{-0.1cm}  
\end{equation}
This completes one full iteration, as both sets have been used to update the approximate inverse $\tsig$. 
\begin{comment}
\begin{equation*}
\vspace{-0.1cm}
    {\tsig}^{(r=1)} = \tsig^{(1)} = 
    \begin{bmatrix}
       \tsig_{\I_{1}}^{(1)} & \cdot \\ 
       \cdot & \tsig_{\I_{2}}^{(1)} \\ 
    \end{bmatrix}.  
\end{equation*}
\end{comment}
%Additionally, the off-diagonal blocks can be calculated at this stage using \eqref{eq:IBMI}. 
%
This iterative process then continues by incrementing $r$ and iterating through the index sets $k = 1,2$ as described above. In each case, the matrix $\tsig_{\I^c}$ in \eqref{eq:IBMI} is obtained from the most recently computed approximation of $\tsig$. 
%This process of obtaining a better matrix approximation for $\tsig$ by using the previous approximation can be repeated until an accurate approximated matrix is produced. The portion of the previous approximated matrix which is kept is dictated by the current set $\I_k$. 
For example, at the next step after \eqref{eq:2block2}, with $r=2$ and $k=1$,
%if the next approximation was to be approximated following from \eqref{eq:2block2}, then 
the principal sub-matrix $\tsig_{\I_2}^{(1,2)}$ would be retained when calculating $\tsig^{(2,1)}$, as we would set $\tsig_{\I_1^c}^{(2,1)} = \tsig_{\I_2}^{(1,2)}$.\\\\
%\vic{Perhaps say briefly from the very begining why the overlap is important (added)} 
%
In general, 
\begin{equation} 
\label{eq:2blockk1}
\begin{aligned}
    \Perm_1\tsig^{(r, 1)} _{\I_1}\Perm_1^{\top} &= \begin{bmatrix}\Q^{-1}_{\I_1} \! +\! \Q^{-1}_{\I_1}  \Q_{\I_1,\I_1^c}  \tsig^{(r-1,1)}_{\I_1^c} \  \Q_{\I_1^c,\I_1} \Q_{\I_1}^{-1} &
- \Q_{\I_1}^{-1} \Q_{\I_1,\I_1^c}   \tsig^{(r-1,1)}_{\I_1^c} \ \\
- \tsig^{(r-1,1)}_{\I_1^c} \ \Q_{\I_1^c,\I_1} \Q_{\I_1}^{-1} &  \tsig^{(r-1,1)}_{\I_1^c} 
\end{bmatrix}\\
&=\begin{bmatrix}
\tsig_{\I_1}^{(r,1)} & \tsig_{\I_1, \I_1^{c}}^{(r,1)} \\
\tsig_{\I_1^{c},\I_1}^{(r,1)} & \tsig_{ \I_1^{c}}^{(r-1,1)} 
\end{bmatrix}
\end{aligned}
\vspace{0.15cm} 
\end{equation}
and 
\begin{equation} 
\label{eq:2blockk2}
\begin{aligned}
\Perm_2\tsig^{(r,2)}_{\I_2}\Perm_2^\top &= \begin{bmatrix}\Q^{-1}_{\I_2} \! +\! \Q^{-1}_{\I_2}  \Q_{\I_2,\I_2^c}  {\tsig}^{(r,1)}_{\I_1} \  \Q_{\I_2^c,\I_2} \Q_{\I_2}^{-1} &
- \Q_{\I_2}^{-1} \Q_{\I_2,\I_2^c}  \tsig^{(r,1)}_{\I_1} \ \\
- \tsig^{(r,1)}_{\I_1} \ \Q_{\I_2^c,\I_2} \Q_{\I_2}^{-1} &  \tsig^{(r,1)}_{\I_1} 
\end{bmatrix}\\
&=\begin{bmatrix}
\tsig_{\I_2}^{(r,2)} & \tsig_{\I_2, \I_1}^{(r,2)} \\
\tsig_{\I_1,\I_2}^{(r,2)} & \tsig_{ \I_1}^{(r,1)} 
\end{bmatrix}. 
\end{aligned}
\vspace{-0.1cm}  
\end{equation}
Before presenting the full novel iterative block matrix inversion algorithm, we first generalise the two-block, non-overlapping case to the multi-block overlapping case. Introducing multiple blocks is essential when handling large matrices, while overlap significantly improves the convergence rate by facilitating faster transfer of information between the blocks. % when using block matrix inversion. 
The full IBMI algorithm will then be presented before we remark on the choice of initial guess. 
\\ \\ %  
\textbf{The Multi-Block, Overlapping Case} \\ 
For larger matrices, $\Q_{\I_1}$ and $\Q_{\I_2}$ are too large to efficiently invert in \eqref{eq:IBMI}. The two-block case can be generalised to multiple blocks by partitioning the diagonal using multiple sets $\I_k$ for $k=1,\ldots, K$. 
%multiple, smaller blocks. When partitioning the precision matrix, multiple different sets can be used to partition the diagonal, denoted $\I_k$ for $k=1,\ldots, K$. 
\begin{equation*}
 \Q= \begin{bmatrix}
        \Q_{\I_{1}} & \cdot  & \cdot& \cdot \\ 
         \cdot &\Q_{\I_2} & \cdot& \cdot \\ 
         \cdot & \cdot & \ddots& \vdots \\ 
         \cdot & \cdot & \hdots &\Q_{\I_{K}}
    \end{bmatrix}. 
\end{equation*}
When using multiple sets, the blocks $\Q_{\I_k}$ are smaller and can be inverted much faster. At every iteration we cycle through $k=1,\dotsc,K$. For each value of $k$ we set $\I = \I_k$, and $\I^c = \{1,\dotsc, p\}\setminus \I_k$ in the approximate block matrix inversion formula \cref{eq:IBMI}, always using the most recently computed approximation to define $\tsig_{\I^c}$.
%a set $\I_k$ will be chosen to represent $\I$ and all the other sets will be combined to form  $\I_{k}^c$. 
For example, if $K=4$ non-overlapping sets are used, we partition  $\Q$ %be partitioned into four non-overlapping diagonal blocks 
as in \cref{eq: Multi 4 sets}. When $k=1$, we let $\I = \I_1$ and $\I^c = \I_2 \ \cup \  \I_3 \  \cup  \ \I_4$. A visual representation of this partitioning into the $2 \times 2$ structure, which is needed in \eqref{eq:IBMI} for block matrix inversion, is given below, with dots representing off-diagonal block matrices. 
\begin{align}
\Q= \ 
\begin{array}{|c|c|c|c|} \hline \label{eq: Multi 4 sets}
        \Q_{\I_{1}} & \cdot & \cdot & \cdot \\ \hline
\cdot & \Q_{\I_{2}} & \cdot & \cdot \\ \hline
\cdot & \cdot & \Q_{\I_{3}} & \cdot \\ \hline
\cdot & \cdot & \cdot & \Q_{\I_{4}} \\ \hline 
\end{array}\ ,  \ \
\Perm_1\Q_{\I_1}\Perm_1^{\top}\!=\! \ 
\begin{array}{|c|ccc|}\hline
 \Q_{\I_{1}}& \phantom{\sig_{\I^c}^{(N)}} &   \cdot             &   \phantom{\sig_{\I^c}^{(N)}} \\ \hline
 \phantom{\sig_{\I^c}^{(N)}} &  & &\phantom{\sig_{\I^c}^{(N)}}  \\
   \cdot  & \phantom{\sig_{\I^c}^{(N)}} & \Q_{\I^c_1} &  \\
 &  &  \phantom{\sig_{\I^c}^{(N)}} &  \\ \hline
\end{array} \ . 
\end{align}
Next, we set $k=2$ and let  $\I=\I_2$ and $\I^c = \I_{1}  \ \cup \ \I_3 \  \cup \ \I_4$. We continue in this manner until all $K=4$ sets are used for $\I$ in \eqref{eq:IBMI} to complete the first iteration. 
\\\\
Overlap between the blocks is also introduced to speed up the convergence of the IBMI algorithm. The four-block partitioning with overlap is shown in \cref{eq: overlap_figure}. Here, $\I=\I_1$ and $\I = \{ 1 , \ldots, p \}  \backslash \I_1$. The set $\I$ captures the elements in $\I_2$ that are not included in $\I_1$, (i.e., the elements of $\I_2$ that are not in the overlap) as well all elements in the remaining sets, namely $\I_3$ and $\I_4$. 
As for the non-overlapping case, a visual representation of the resulting partitioning into the $2 \times 2$ structure for \eqref{eq:IBMI} is given in \cref{eq: overlap with BMI}. A similar process is then repeated for the other three sets, $\I_2$, $\I_3$ and $\I_4$, to complete one iteration. 

\begin{align} 
\Q&= \ 
\begin{array}{|ccc ccc ccc|} \hline \label{eq: overlap_figure}
    \cdot & \cdot & \multicolumn{1}{c|}{\cdot} & \cdot & \cdot & \cdot & \cdot &\cdot &\cdot   \\ 
\cdot & \! \Q_{\I_{1}}  & \multicolumn{1}{c|}{\cdot} & \cdot & \cdot & \cdot & \cdot &\cdot &\cdot  \\ 
\cline{3-5}
\cdot & \multicolumn{1}{c|}{\cdot} & \multicolumn{1}{c|}{\cdot} & \cdot  & \multicolumn{1}{c|}{\cdot} & \cdot & \cdot &\cdot &\cdot  \\ 
\cline{1-3}
\cdot & \multicolumn{1}{c|}{\cdot}&\cdot & \Q_{\I_{2}}  &\multicolumn{1}{c|}{\cdot} &   \cdot & & \cdot &\cdot   \\ 
\cline{5-7}
\cdot & \multicolumn{1}{c|}{\cdot} & \cdot & \multicolumn{1}{c|}{\cdot}  & \multicolumn{1}{c|}{\cdot}& \cdot& \multicolumn{1}{c|}{\cdot} &\cdot &\cdot  \\ 
\cline{3-5}
\cdot & \cdot & \cdot &  \multicolumn{1}{c|}{\cdot} & \cdot & \Q_{\I_3}& \multicolumn{1}{c|}{\cdot}  &\cdot &\cdot  \\ 
\cline{7-9}
\cdot & \cdot & \cdot &  \multicolumn{1}{c|}{\cdot} & \cdot & \multicolumn{1}{c|}{\cdot}& \multicolumn{1}{c|}{\cdot}  &\cdot &\cdot  \\ 
\cline{5-7}
\cdot & \cdot & \cdot &  \cdot & \cdot & \multicolumn{1}{c|}{\cdot} &\cdot  & \Q_{\I_4}  &\cdot  \\ 
\cdot & \cdot & \cdot &  \cdot & \cdot& \multicolumn{1}{c|}{\cdot}& \cdot &\cdot &\multicolumn{1}{c|}{\phantom{a}\cdot \phantom{a}} \\ 
\hline
\end{array}\ ,  \\
\! \! \! \! \!\Perm_1\Q_{\I_1}\Perm_1^{\top} &=
\begin{array}{|ccc ccc ccc|} \hline \label{eq: overlap with BMI}
    \cdot & \cdot & \multicolumn{1}{c|}{\cdot} & \cdot & \cdot & \cdot & \cdot &\cdot &\cdot   \\ 
\cdot & \Q_{\I_{1}} & \multicolumn{1}{c|}{\cdot} & \cdot & \cdot & \cdot & \cdot &\cdot &\cdot  \\ 
\cdot & \cdot & \multicolumn{1}{c|}{\cdot} & \cdot  & \cdot & \cdot & \cdot &\cdot &\cdot  \\ \hline
\cdot & \cdot & \multicolumn{1}{c|}{\cdot}  & \cdot  &\cdot &   \cdot & \cdot& \cdot &\cdot   \\ 
\cdot & \cdot & \multicolumn{1}{c|}{\cdot}  & {\cdot}  & \cdot& \cdot& \cdot &\cdot &\cdot  \\ 
\cdot & \cdot & \multicolumn{1}{c|}{\cdot}  &  \cdot & \cdot & \Q_{\I_1^c} & \cdot &\cdot &\cdot  \\ 
\cdot & \cdot & \multicolumn{1}{c|}{\cdot} &  {\cdot} & \cdot & \cdot& \cdot  &\cdot &\cdot  \\ 
\cdot & \cdot & \multicolumn{1}{c|}{\cdot} &  \cdot & \cdot & \cdot &\cdot  & \cdot  &\cdot  \\ 
\phantom{a}\cdot \phantom{a}& \cdot & \multicolumn{1}{c|}{\phantom{a}\cdot \phantom{a}} &  \phantom{a}\cdot \phantom{a} & \cdot& \cdot & \cdot &\cdot  &\cdot   \\ 
\hline
\end{array} \ . 
\end{align}
\\ \\ %
%\vic{I think the link between this section and the following is missing. Perhaps announce what we are doing or not doing next with this generalisation of the overlapping algorithm. Is the next part just an abstract presentation of what we have shown above?}
%
In this section, we presented the IBMI algorithm for the particular case of two non-overlapping blocks. We then described the generalisation to the case of multiple, overlapping blocks. 
In the next section, we give the full IBMI algorithm for this general case, and discuss the choice of initial guess.
%Now that we have seen how the IBMI algorithm can be expanded to use a multi-block partitioning with overlap, the full IBMI algorithm can be presented. This version of the algorithm will be generalised for $K$ overlapping blocks. 

\subsection{Iterative Block Matrix Inversion (IBMI) Algorithm} \label{sec:alg}
The full iterative block matrix inversion algorithm is given in \cref{alg:IBMI}, which can be applied for $\I_k$ sets for $k=1, \ldots, K$. The algorithm will produce a final matrix $\tsig_{\text{final}}$ from the matrix $\Q$ and will also return the number of iterations $r$ taken to reach the desired tolerance level set by the user. The error estimate used in the stopping condition in  \cref{alg:IBMI} is further detailed in \cref{appx:A}, but alternative stopping conditions could be implemented.
%\vic{Introduce the acronym early enough so that you can use it later in the text (I added it above as well) }
%This tolerance level is the desired error when the approximated matrix is taken away from the exact solution
%\\ \\ 
%The notation $\left[\tsig_{\I_{k-1}}^{(r,k)} \right]_{\I_{k}^c}$ describes how the previously approximated matrix is partitioned with respect to the current set $\I_k$. The previous matrix which is needed is given by $\tsig_{\I_{k-1}}^{(r,k)}$, inside the square brackets. This matrix is partitioned with the respect to the current sets, $\I_k$ and $\I_k^c$ and only the diagonal block matrix involving $\I_{k}^c$ is retained.  Therefore, the subscript $\big[ \cdot \big]_{\I_{k}^c}$ refers to the block matrix dictated by the current set $\I_k$, which needs to be retained from the previously approximated matrix. Additionally, when the number of iterations increases from $r$ to $r+1$, the previous approximation needed for $\tsig_{\I_k^c}$ is held within the previous iteration $r$. 
\begin{algorithm}
\caption{Iterative Block Matrix Inversion (IBMI) Algorithm} \label{alg:IBMI}\begin{algorithmic}
%\begin{algorithmic}[1]
%\caption{Pseudocode Iterative Block Matrix Inversion Algorithm}  
\STATE{Inputs: $\Q$, \texttt{tol, $\I_{k}$ for $k=1, \ldots , K$}, initial approximation $\tsig_{\I_1^c}^{(0,1)}$ of $\sig_{\I_1^{c}}$ in \cref{eq:IBMI}.}
\STATE{\textbf{While} \texttt{error } $<$ \texttt{tol}}
\FOR{$k=1:K$}
\STATE{Determine $\I_k^c$.}
\IF {$k=1$} 
  \STATE{Get $\tsig_{\I_k^{c}}^{(r,k)}$ from $\tsig^{(r-1,K)}$.}
\ELSE
  \STATE{Get $\tsig_{\I_k^{c}}^{(r,k)}$ from $\tsig^{(r,k-1)}$.}
\ENDIF
% \STATE{Get $\tsig_{\I_k^{c}}^{(r,k)}$ 
% from the previously approximated matrix, i.e., from  $\tsig^{(r,k-1)}_{\I_k^c}$ if $k>$.} % dictated $\qquad \qquad $$\qquad \qquad $ by the current set $\I_k^c$.}
% \STATE{Either $\tsig^{(r,k-1)}_{\I_k^c}$ for $k>1$ or $\tsig^{(r-1,K)}_{\I_k^c}$ for k=1.}
\STATE{Use $\tsig_{\I_k^c}^{(r,k)}$ and $\Q$ in the block matrix inversion equation \eqref{eq:IBMI}}.
%\State {$\tsig_{\I_k, \I_k^c} = -\Q_{\I_k}^{-1} \Q_{\I_k,\I_k^c} \tsig_{\I_k^c}.$}
%\State{ $ \tsig_{\I_k^c, \I_k} = ( \tsig_{\I_k,\I_k^c})^\top. $}
%\State{ $\tsig_{\I_k} = \Q^{-1}_{\I} +  \Q^{-1}_{\I_k}  \Q_{\I_k,\I_k^c} \tsig_{\I_k^c} \Q_{\I_k^c,\I_k} \Q_{\I_k}^{-1}. $ }   
\STATE{Obtain updated approximation $\displaystyle \tsig^{(r,k)} = \begin{bmatrix}
        \tsig_{\I_k}^{(r,k)} & \tsig_{\I_k, \I_k^c}^{(r,k)} \\ 
        \tsig_{\I_k^c, \I_k}^{(r,k)} & \tsig_{ \I_k^c}^{(r,k)} \end{bmatrix}$.}
\ENDFOR
\STATE{Compute error estimate.}
\STATE{\textbf{Return:} $\tsig_{\text{final}} = \tilde{\sig}^{(r,K)}$ and number of iterations $r$.}
\end{algorithmic}
\end{algorithm}
\\\\
%
We end this section by remarking on the choice of the initial guess for \cref{alg:IBMI}. 
In our experiments, we take $\tsig_{\I_1^c}^{(0,1)}$ to be the identity matrix of the appropriate dimension. This initial guess still produces an accurate approximation $\tsig$ of $\sig=\Q^{-1}$ and we find that \cref{alg:IBMI} converges within a small number of iterations for our test matrices (see \cref{sec:graphs}). However, any symmetric positive definite approximation of $\tsig_{\I_1^c}$ can be used as an initial guess. For example, the Monte Carlo estimators in \cref{MC Estimators} or the diagonal matrix obtained from Hutchinson's estimator \cref{Hutch} could be used in statistical applications. 
%\newpage
\section{Convergence of the IBMI algorithm}
\label{sec:conv}
%The convergence of the algorithm will now be discussed. If a SPD matrix $\Q$ is partitioned into $K$ equal sized sets, let $\mid \!\I\!\mid \ = m$ and $\mid\!\I^c\!\mid = n$. Then $\Q \in \mathbb{R}^{(m+n)\times (m+n)}$. The convergence of a two block non-overlapping matrix $\sig$, will be found. %This is then extended to the $3 \times 3$ block cases, and a comment is made for generalising this to higher dimensions. 
\begin{comment}
\begin{definition}
    Given two sets $\I_k \in \mathbb{R}^n$ and $\I_k^c \in \mathbb{R}^m$, the precision matrix $\Q \in \mathbb{R}^{(m+n) \times (m+n)}$ and covariance matrix $\sig \in \mathbb{R}^{(m+n) \times (m+n)}$ can be expressed in the following notation for any $k \in \mathbb{R}$: 
\begin{equation} \label{equ:k sets}
    \Q= \begin{bmatrix}
            \Q_{\I_{k}}         & \Q_{\I_k,\I_k^c} \\ 
            \Q_{\I_k^c,\I_k}    & \Q_{\I_{k}^c}
    \end{bmatrix}, \qquad 
       \sig= \begin{bmatrix}
            \sig_{\I_{k}}      & \sig_{\I_k,\I_k^c} \\ 
            \sig_{\I_k^c,\I_k} & \sig_{\I_{k}^c}
        \end{bmatrix}. 
\end{equation}
\end{definition}

Regardless of partitioning and overlap, these two forms above will be referred to when using the block matrix inversion equations shown in \eqref{BMI}. 
\end{comment}
%\subsection{Two-block non-overlapping convergence} \label{sec: two-block-convergence}
In this section, the convergence of \cref{alg:IBMI} will be examined for the particular case of two non-overlapping blocks (cf.\ \cref{sec:two-block}). In this case, the diagonal blocks of the symmetric positive definite matrix $\Q$ are defined by the non-intersecting sets $\I_1$ and $\I_2$. %which satisfy  $\I_1 \ \cup\  \I_2 = \{1,\dotsc,p\}$, $\I_1\cap \I_2 = \emptyset$. 
Recall that in this case $\I_1^{c} = \I_2$ and $\I_2^{c} = \I_1$. %Our goal will be to bound the norm of  $\sig_{\I_2}-\tsig_{\I_2}^{(r,2)}$, i.e., the error, after $r$ iterations, of the approximation to $\sig_{\I_2}$, the diagonal block of $\sig$ formed from the rows and columns indexed in $\I_2$. 
%\vic{Shouldn't we move the next explanations into the proof of the theorem, it seems to me they would fit better there rather than before the theorem}
The first step will be to show that the error at the $r$th iteration is related to the error in the initial guess.%, as the following lemma shows.approximation $\tsig_{\I_2}^{(r,2)}$ satisfies a recurrence relation.
%\vic{Can we put this preliminary result \eqref{lemma3.3} into a lemma? (think the wording still has to be fixed but I made it a lemma) }
\begin{lemma} \label{equ:lemma}
Let $\Q \in \mathbb{R}^{p \times p } $ be a symmetric positive definite matrix with inverse $\sig$, and let $\I_1$ and $\I_2$ be index sets such that $\I_1 \cup \I_2 = \{ 1,2, \ldots, p\}$, $\I_1 \cap \I_2 = \emptyset$. Let $\sig_{\I_2}$ be the sub-matrix formed from the rows and columns of $\sig$ indexed by $\I_2$, and let $\tsig_{\I_2}^{(r,2)}$ be the approximation of this matrix after $r$ complete iterations of \cref{alg:IBMI}. Then the error $\tsig^{\left(r,2\right)}_{\I_{2}} - {\sig}_{\I_{2}}$ at iteration $r$ satisfies, 
%can be defined after $r$ iterations: 
\\ 
   \begin{equation}
       \label{lemma3.3}
   \tsig^{\left(r,2\right)}_{\I_{2}} - {\sig}_{\I_{2}}  \!=\left( \Q_{\I_2}^{-1} \Q_{\I_2,\I^c_2} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2}\right)^{(r)} \left[\tsig_{\I_{2}}^{(0,2)} -  \sig_{\I_2}\right] \left( \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \right)^{(r)}.
   \end{equation}
\end{lemma}
\begin{proof}
To begin, we see from \cref{eq:2blockk1} that the upper diagonal block of the approximation,  $\tsig_{\I_1}^{(r,1)}$, at iteration $r$ is: 
%\vic{Can we refer to a precise equation instead of the whole subsection? (think I've done this)}
\begin{align*} %\label{equ:2blockconv}
     \tsig_{\I_{1}}^{(r,1)} = \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \tsig_{\I_2}^{(r-1,2)} \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}.
\end{align*}
This approximation is then used to update $\tsig_{\I_2} ^{(r,2)}$ using \cref{eq:2blockk2} to give 
%For the same iteration, the matrix approximation can be updated using the second set $\I_2$. The previously updated matrix in \eqref{equ:2blockconv} can be substituted into this update as $\tsig_{\I_2^c}^{(r,2)} \equiv \tsig_{\I_1}^{(r,1)}$: 
\begin{equation}
\label{eq:approx_schur_recurrence}
\begin{aligned}
    \tsig_{\I_{2}}^{(r,2)} &= \Q_{\I_2}^{-1} + \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \tsig_{\I_1}^{(r,1)} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \\ 
      &= \Q_{\I_2}^{-1} + \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[ \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \tsig_{\I_{2}}^{(r-1,2)}  \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}. \\ 
      %&= f\left(\tsig_{\I_2}^{(r-1,2)} \right),
\end{aligned}
\end{equation}
% where 
% \[
% f(\mathcal{X}) = \Q_{\I_2}^{-1} + \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[ \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \mathcal{X}  \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} .
% \]
The exact Schur complement satisfies the same recurrence, since in this case \eqref{eq:IBMI} reduces to \eqref{BMIQ}. Hence, 
%when the exact inverse of the Schur complement is used, an expression can be made for the exact solution $\sig_{\I_2}$: 
\begin{equation}
\label{eq:exact_schur_recurrence}
\begin{aligned}
    \sig_{\I_{2}}  &= \Q_{\I_2}^{-1} + \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \sig_{\I_1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \\     
    &= \Q_{\I_2}^{-1} + \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[ \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \sig_{\I_{2}}  \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}, \\ 
     % &= f\left(\sig_{\I_2} \right),
\end{aligned}
\end{equation}
 where $ \sig_{\I_2} = \left( \Q_{\I^c} - \Q_{\I^c,\I} \Q_{\I}^{-1} \Q_{\I,\I^c}\right)^{-1}$.
It then follows from \cref{eq:approx_schur_recurrence} and \cref{eq:exact_schur_recurrence} that  
%As this iteration is complete, the error between the exact solution and the approximation can be found for iteration $r$: 
\begin{align*}
    \tsig^{\left(r,2\right)}_{\I_{2}} - {\sig}_{\I_{2}}  %& \!= \! f \left( \tsig_{\I_2}^{(r-1,2)} \right) - f \left(\sig_{\I_2} \right) \\
    %& \! \! = \Q_{\I_2}^{-1} \! + \! \Q_{\I_2}^{-1} \Q_{\I_2,\I_1}\! \! \left[ \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \tsig_{\I_{2}}^{(r-1,2)}  \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}- \\ 
    %& \qquad \Q_{\I_2}^{-1} -\Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[ \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \sig_{\I_{2}} \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \\
    & = \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \left[\tsig_{\I_{2}}^{(r-1)} - \sig_{\I_2}\right]  \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}  \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}.
    %&   = \left( \Q_{\I_2}^{-1} \Q_{\I_2,\I^c_2} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2}\right)^{(r)} \left[\tsig_{\I_{2}}^{(0)} -  \sig_{\I_2}\right] \left( \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \right)^{(r)}. 
\end{align*}
By induction, on the iteration $r$, we
obtain the result.
\begin{comment}
we prove \cref{equ:lemma}:% which relates the error at iteration $r$ to the error in the initial guess: 
\begin{align} \label{lemma3.3}
     \tsig^{\left(r,2\right)}_{\I_{2}} - {\sig}_{\I_{2}}  & \!=\left( \Q_{\I_2}^{-1} \Q_{\I_2,\I^c_2} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2}\right)^{(r)} \left[\tsig_{\I_{2}}^{(0,2)} -  \sig_{\I_2}\right] \left( \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \right)^{(r)}. 
\end{align}
\end{comment}
\end{proof}
\begin{comment}
\begin{proof}
 For notation simplicity, let $\Q_* = \Q_{\I_2}^{-1} \Q_{\I_2,\I_1}  \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} $. 
 \\ Then, when $r=1$,  
\begin{align*}
    \tsig^{\left(1\right)}_{\I_{2}} - {\sig}_{\I_{2}}  & \!= \! f \left(\tsig_{\I_2}^{(1)} \right) - f\Big(\sig_{\I_2}\Big) \\
     & =  \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[ \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \tsig_{\I_{2}}^{(0)}  \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}  \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \ - \\ 
    & \qquad \qquad \qquad  -\Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[\Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \sig_{\I_{2}} \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right] \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \\ 
&   = \Q_* \left[\tsig_{\I_{2}}^{(0)} -  \sig_{\I_2}\right] \Q_*^{\top}. 
\end{align*}
Let $r \geq 1$ and assume the following holds for $r=R$ iterations: 
\begin{align*}
    \tsig^{\left(R\right)}_{\I_{2}} - {\sig}_{\I_{2}}  & \!= \! f \left(\tsig_{\I_2}^{(R)} \right) - f\Big(\sig_{\I_2}\Big) \\ 
    &   = \left(\Q_*\right)^{(R)} \left[\tsig_{\I_{2}}^{(0)} -  \sig_{\I_2}\right] \left(\Q_{*}^{\top}\right)^{(R)}. 
\end{align*}
Using the above induction hypothesis, the above statement holds for $r=R+1$ iteration: 
    \begin{align*}
    \tsig^{\left(R+1\right)}_{\I_{2}} -  {\sig}_{\I_{2}} &=  f \left( \tsig_{\I_2}^{(R+1)} \right) - f\Big(\sig_{\I_2}\Big) \\
    & =\Q_{*}\left( f \left(\tsig_{\I_2} ^{(R)} \right) - f\Big(\sig_{\I_2}\Big)\right) \Q_{*}^{\top} \\
     & = \Q_{*}\left( \left(\Q_*\right)^{(R)} \left[\tsig_{\I_{2}}^{(0)} -  \sig_{\I_2}\right] \left(\Q_{*}^{\top}\right)^{(R)} \right) \Q_{*}^{\top} \\ 
     & = \left(\Q_*\right)^{(R+1)} \left[\tsig_{\I_{2}}^{(0)} -  \sig_{\I_2}\right] \left(\Q_{*}^{\top}\right)^{(R+1)}  . 
     \end{align*}
\end{proof}
\end{comment}
\cref{equ:lemma} can now be used to bound the error in the iterative block matrix inversion algorithm (\cref{alg:IBMI}), as we now show. \\ % 
\begin{theorem}\label{lemma}
Let $\Q \in \mathbb{R}^{p\times p}$ be a symmetric positive definite matrix with inverse $\sig$, and let $\I_1$ and $\I_2$ be index sets such that $\I_1 \cup \I_2 = \{1,2,\dotsc,p\}, \ \I_1 \cap \I_2 = \emptyset$. Let $\sig_{\I_2}$ be the sub-matrix formed from the rows and columns of $\sig$ indexed by $\I_2$, and let $\tsig_{\I_2}^{(r,2)}$ be the approximation of this matrix after $r$ complete iterations of \cref{alg:IBMI} with sets $\I_1$ and $\I_2$.  
Then, the error in $\tsig_{\I_2}^{(r,2)}$ can be bounded by, 
%partitioned into two non-overlapping block given by the sets $\mid \! \I_1 \! \mid= {n}$, and $ \mid \! \I_2 \!\mid = {m}$. Then the error after $R$ iterations of the algorithm described in \eqref{alg:IBMI} is: 
\begin{equation}
\label{eq:bound}
\left\|{\tsig}_{\I_{2}}^{(r,2)} - {\sig}_{\I_{2}} \right\|_2 \leq \left\| \left( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2}\right)\right\|_{2}^{2r} \left\| \tsig^{(0,2)}_{\I_2} -  \sig_{\I_2} \right\|_{2}. 
\end{equation}
Moreover, the iterative method will converge for any symmetric positive definite initial guess $\tsig^{(0,2)}_{\I_2}$. 
\end{theorem}
\begin{proof}
Our goal will be to bound the norm of  $\sig_{\I_2}-\tsig_{\I_2}^{(r,2)}$, i.e., the error, after $r$ iterations, of the approximation to $\sig_{\I_2}$.%, the diagonal block of $\sig$ formed from the rows and columns indexed in $\I_2$. 
 Taking 2-norms of \eqref{lemma3.3} shows that 
\begin{align*}
    & \! \! \! \left\|{\tsig}^{(r,2)}_{\I_{2}} \!\! \! - {\sig}_{\I_{2}} \right\|_2 \! \! = \!  \left\| \left( \Q_{\I_2}^{-1}\! \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \right)^{\!(r)} \! \! \left[ \tsig_{\I_2}^{(0,2)}\! \!\! \!-\! \sig_{\I_2} \! \right] \! \! \left( \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \! \right)^{\!(r)}\right\|_2 \\
%& \qquad  \leq \left\|( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1, \I_2})^{R} \right\|_{2} \left\|\tsig_{\I_2}^{(0,2)}- \sig_{\I_2}\right\|_{2} 
%\left\|\left( \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}\right)^{R} \right\|_{2}  \\ 
&  \qquad =  \left\|\left( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \right)^{r} \right\|_{2}^{2}\left\| \tsig_{\I_2}^{(0,2)}- \sig_{\I_2}\right\|_{2} \\ 
& \qquad \leq  \left\|\left( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \right) \right\|_{2}^{2r} \left\|\tsig_{\I_2}^{(0,2)}- \sig_{\I_2}\right\|_{2}. 
\end{align*}
This proves the first part. \\ \\
%
The second part  follows from Theorem 7.7.7 in \cite[pg.497]{Horn_Johnson_2012} %and Theorem 10.1.1 in \cite[pg.511]{geneMatrixComps}. 
which shows that, whenever $\Q$ is symmetric positive definite, $\rho( \Q_{\I_1,\I_2}\Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1}) < 1$ where $\rho(\cdot)$ is the spectral radius. It then follows, by similarity, that 
%By using a similarity transform, we find 
that $\rho( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1}\Q_{\I_1,\I_2})<1$.
%by using Corollary 1.3.4 in \cite[pg.58]{Horn_Johnson_2012}. % if there's a proof of this that would be great to reference. 
Finally, since $A^k \to 0$ as $k \to \infty$ if and only if $\rho(A)< 1$ for any square matrix $A$, we see that 
% Section 10.1.2 and Theorem 10.1.1 states that the convergence of an iterative method is dependent on the eigenvalues, specifically the spectral radius of the matrix $\b{G}$ where $\b{G}=\b{M}^{-1}\b{N}$ for the decomposition $\Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} = \b{M}- \b{N} \in \mathbb{R}^{n\times n}$ . Readers are recommended the full proof in \cite[pgs.336, 511]{geneMatrixComps} which proves $\left(\b{M}^{-1}\b{N}\right)^{k} \Rightarrow 0$ iff $\rho\left(\b{M}^{-1}\b{N}\right)<1$. Using this result, it can be seen that, 
$\left\|\left( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \right) \right\|_{2}^{2r} \rightarrow 0$ as $r \to \infty$. Therefore, the two block non-overlapping case in \cref{alg:IBMI} will  converge for any symmetric positive definite matrix. 
\end{proof}

\begin{remark}
Although the current convergence analysis is limited to the two-block non-overlapping case, numerical experiments have suggested that the algorithm converges for any symmetric positive definite matrix when $K>2$ overlapping blocks are used. More evidence of this is detailed in \cref{sec:graphs}. 
%Our numerical experiments suggest that the method converges for any symmetric positive definite matrix when $K>2$ blocks and/or overlapping blocks are used. However, we have not been able to prove this. 
\end{remark}
%\vic{This sounds a bit pessimistic and would rather flip the phrase to end with a positive note: "although the current convergence theory is limited to a two-block decomposition, numerical experiments show convergence for a decomposition into more overlapping blocks"}


\section{Computational cost of the IBMI algorithm} \label{sec:cost}
%The cost of Algorithm \eqref{alg:IBMI} depends not only on the size of $\Q$ but also its sparsity pattern. To give an idea of the cost of the method, two extreme cases are considered: a dense matrix and a tri-diagonal matrix. 
%\vic{I think the purpose of this section is to underline that despite the fact that we don't have theoretical results for the above we can estimate and show a better computational cost with respect to the state of the art. In the paragraph below the word "cost" is used to much, rephrase by using synonyms}
%\vic{Perhaps say that this is done in the multiblock case thus covering the situation where the convergence analysis is not avaialble (added this)}
The computational cost of one iteration of \cref{alg:IBMI} will now be discussed for the multi-block non-overlapping case. The case of overlapping blocks can be treated in a similar way. This analysis provides insight into the efficiency of \cref{alg:IBMI}, even when a precise convergence analysis is unavailable for this partitioning. The most expensive operations of the algorithm are inverting the principal sub-matrices $\Q_{\I_k}$, $k = 1,\dotsc, K$, and performing matrix-matrix multiplications. Although the exact cost of these operations will depend on the properties of $\Q$, and the sets $\I_k$, the following analysis provides a sense of the cost per iteration. 
%\vic{I don't understand the last phrase: in what sense the cost will depend on the properties of A? JP: I think Ann meant the cost of computing the inverse and the mat-mat products.}
% 
Assume for simplicity that $\Q\in\mathbb{R}^{Km\times Km}$ is a dense, symmetric positive definite matrix. Additionally, assume that the sets $\I_k$ are chosen so that $|\I_k| = m$, $k = 1,\dots, K$, with $\cup_{i=1}^{K} \I_{k} = \{1,\dots, Km\}$ and $\I_j\cap \I_k = \emptyset$, $j\ne k$. \\ \\
%$\Q$ is partitioned into $K$ equal sized sets, then let $\mid \I \mid =m$ and $ \mid \I^c \mid =n$,  where $n=(K-1)m$, then $\Qdim$. 
The flop\footnote{{Here, flop stands for floating point operations per second.}} counts for matrix-matrix and matrix-vector products are calculated according to \cite[pg.18]{geneMatrixComps}. For matrices $A \! \in \mathbb{R}^{q\times s}$, $B\in \mathbb{R}^{s\times t}$ and $C\in\mathbb{R}^{q\times t}$, and a vector $\b{v} \in \mathbb{R}^s$, the cost of computing $AB+C$ is $\mathcal{O}(2qst)$ flops, and the cost of computing $A\b{v}$ is $\mathcal{O}(2qs)$ flops.\\ \\
%which are $2mnp$ and $2n$ flops respectively. 
We start by calculating the cost of each sub-matrix of the block matrix inversion equations from \cref{eq:IBMI}: \vspace{-0.25cm}
\begin{align*}
    \tsig_{\I,\I^c} &= -\Q_{\I}^{-1} \Q_{\I,\I^c} \tsig_{\I^c}, \\ 
    \tsig_{\I^c,\I} &= -\tsig_{\I^c} \Q_{\I^c,\I} \Q_{\I}^{-1}, \\ 
    \tsig_{\I} \quad & = \Q_{\I}^{-1} + \Q_{\I}^{-1} \Q_{\I,\I^c} \tsig_{\I^c} \Q_{\I^c,\I} \Q_{\I}^{-1}. 
\end{align*}
Starting with the off-diagonal blocks, the most costly operation is computing $\Q_{\I}^{-1}$, which involves $\mathcal{O}(m^3/3)$ flops to obtain a Cholesky factorisation and $\mathcal{O}(2m^3)$ flops to solve the linear systems required to find the inverse. Next, the matrix-matrix product $\Q_{\I}^{-1}\Q_{\I,\I^c}$ requires $\mathcal{O}\left(2\left(K-1\right)m^3\right)$ flops. We note that this product, or its transpose, appears four times in the above block matrix inversion equations. Once $\Q_{\I}^{-1}\Q_{\I,\I^c}$ is determined, the matrix-matrix product $\Q_{\I}^{-1}\Q_{\I,\I^c}\tsig_{\I^c}$ requires $\mathcal{O}\left(2(K-1)^2m^3\right)$ flops. Therefore, the cost of obtaining the off-diagonal blocks is: 
\begin{align*}
     \text{Cost}( \tsig_{\I,\I^c}) & = \mathcal{O}\left(\frac{m^3}{3}\right) + \mathcal{O}\left(2m^3\right) + \mathcal{O}\left(2(K-1)m^3\right)+ \mathcal{O}\left(2(K-1)^2m^3\right) \\ 
    & = \mathcal{O}\left(\left(\frac{7}{3} + 2K(K-1)\right)m^3\right). 
\end{align*}
% \begin{align*}
%      \text{Cost}( \tsig_{\I,\I^c}) & = \mathcal{O}(m^3)+ \mathcal{O}(2m^2n)+ \mathcal{O}(2mn^2) \\ 
%     & = \mathcal{O}(m^3+2m^2n+2n^2m). 
% \end{align*}
Since $\sig$ is symmetric, only one off-diagonal block needs to be explicitly computed. 
%Only one off-diagonal block has to be calculated as the resulting matrix is symmetric, and therefore the transpose of the resulting off diagonal matrix is used. 
\\ \\
Once the off-diagonal block has been found, the upper diagonal block requires one matrix-matrix product  $(\Q_{\I}^{-1}\Q_{\I,\I^c}\tsig_{\I^c})(\Q_{\I}^{-1}\Q_{\I,\I^c})^\top$, and one matrix-matrix addition. %The off-diagonal block $\tsig_{\I,\I^c}$ is contained within $\tsig_{\I}$ and is multiplied by $\Q_{\I}^{-1}\Q_{\I,\I^c}$. 
The total number of flops required to calculate $\tsig_{\I}$ is $\mathcal{O}\left(2(K-1)m^3\right)$ flops.
\begin{comment}
\begin{align*}
\tsig_{\I} &= \Q_{\I}^{-1} +\Q_{\I}^{-1} \Q_{\I,\I^c} \tsig_{\I^c} \Q_{\I^c,\I} \Q_{\I}^{-1}. \\ 
     \text{Cost}( \tsig_{\I}) & = *adding* \mathcal{O}(m^3)+ \mathcal{O}(2m^2n)+ \mathcal{O}(2mn^2) \\ 
    & = \mathcal{O}(m^3+2m^2n+2n^2m+2N_sn). 
\end{align*}
\end{comment}
%Finally, the cost of the Monte Carlo estimator, which is used to approximate the first Schur complement, $\tsig_{\I_1}$ is calculated. The $N_s$ vectors $\Z$ which are used in $ \hat{\sig}_{\textbf{MC}}$ from \eqref{MC Estimators} are $n \times 1$ in length, making the cost $\mathcal{O}(2nN_s)$.  
Therefore, the cost of one application of \cref{eq:IBMI}, for one set $k = 1,\ldots ,K$ in Algorithm \eqref{alg:IBMI} is: 
\begin{align*}
\text{Cost}\left(\tsig^{(r,k)}\right) &= \text{Cost}\left(\tsig_{\I, \I^c}^{(r,k)}\right)  +\text{Cost} \left(\tsig_{\I}^{(r,k)}\right)  \\ 
&=\mathcal{O}\left(\left(\frac{7}{3} + 2K(K-1)\right)m^3\right)+ \mathcal{O}\left(2(K-1)m^3\right) \\ 
&= \mathcal{O}\left(\left(\frac{1}{3} + 2K^2\right)m^3\right).
%&= \mathcal{O}\left((\frac{7}{3} + 2(K^2-1))m^3\right).
\end{align*}
Hence, the cost per iteration of \cref{alg:IBMI} is $\mathcal{O}\left((\frac{1}{3} + 2K^2)Km^3\right).$
% \begin{align*}
% \text{Cost}\left(\tsig^{(r)}_{\I}\right) &= \text{Cost}\left(\tsig_{\I, \I^c}^{(r)}\right)  +\text{Cost} \left(\tsig_{\I}^{(r)}\right)  \\ 
% &=\mathcal{O} \left( m^3+2m^2n+n^2m \right) + \mathcal{O} \left(2mn^2\right) \\ 
% &= \mathcal{O}\left(m^3+4m^2n+2n^2m \right).
% \end{align*}
The initial guess of the inverse of the Schur complement can also be considered here, but since we use the identity matrix there is no additional cost. 
%contributes less than the leading order term. For example, if the Monte Carlo estimator is used, the cost increases by $2N_s(K-1)m$ flops. % for $\text{Cost}\left(\tsig_{\I^c}^{(r)}\right)$.
\\ \\ %
%Although it is not common to compare iterative methods with direct solvers, 
When $\Q$ is partitioned according to the multi-block \textbf{non-overlapping} case, \cref{alg:IBMI} can take many iterations to converge (see \cref{sec:graphs}). However, as we will see in \cref{sec:graphs} when a small amount of overlap is added between the diagonal blocks, \cref{alg:IBMI} can take just one iteration to converge. For these cases, the cost of \cref{alg:IBMI} can be compared with the cost of a direct solver. The cost of inverting $\Q \in \mathbb{R}^{Km \times Km}$ using the Cholesky factorisation and solving $Km$ linear systems would be $\mathcal{O}(\frac{1}{3}\left(Km\right)^3)+\mathcal{O}(2\left(Km\right)^3) = \mathcal{O}\left(\frac{7}{3} (Km)^3\right)$. Comparing this cost with the leading order term for the IBMI algorithm $\mathcal{O}\left( (\frac{1}{3} + 2K^2) Km^3\right) $, when only one iteration is required, \cref{alg:IBMI} is computationally faster compared with this direct method. For cases where \cref{alg:IBMI} takes more iterations to converge, it may be slower than direct inversion. Finally, we note that if the matrix $\Q$ has additional structure, this could be incorporated in the complexity analysis above. 
%and more is known about the matrix $\Q$, the above cost analysis could be used to estimate the computational complexity which is required for a specific problem size. 

%This can be compared with leading order term in the cost of \cref{alg:IBMI} is $\mathcal{O}(2\left( Km\right)^3)$, showing that the IBMI algorithm is computationally faster.
% If $\Qdim$ is partitioned into $K$ blocks of equal size $m\times m$, then $n=(K-1)m$ and for $ k=1,\ldots K$ the cost per block update can be calculated, 
% \begin{align*}  
% \text{Cost}(\tsig_{\I_{k}})=& \mathcal{O}(m^3+4m^2n+2n^2m)\\ 
% =& \mathcal{O}(m^3+4m^2((K-1)m)+2((K-1)m)^2m)\\
% =& \mathcal{O}(m^3+4m^3K -4m^3 +2m^3(K^2-2K+1)) \\ 
% =& \mathcal{O}(m^3+4m^3K -4m^3 +2m^3K^2 -4m^3K +2m^3  ) \\ 
% =& \mathcal{O}(2m^3K^2 - m^3 ). 
% \end{align*}
% The cost per iteration after all $K$ sets have been used, it is, $\mathcal{O}(2m^3K^3 - m^3K)$.  

% \subsection{Initial Approximation * Deleted this?*} \label{sec:initial}
% A note on the initial guess for Algorithm \eqref{alg:IBMI} will now be discussed. Currently, the identity matrix $\mathbf{I} \in \mathbb{R}^{n \times n}$ is used as an initial guess for $\tsig_{\I_1^c}^{(0,1)}$ due to the low memory requirements when working with larger matrices. However, this does not have to be the case and % It is important to note that the initial guess does not need to be a `good' approximation of the lower diagonal block $\tsig_{\I_1^c}^{(0,1)}$. In fact, an identity matrix can be used, and the algorithm will still converge.
% any close approximation of $\tsig_{\I_1^c}$ can be used as an initial guess. For example, taking inspiration from \cite{Finn} the Monte Carlo estimators in \eqref{MC Estimators} can be used as an initial guess:
% \begin{equation} \label{eq:MC guess}
%      \tsig^{(0,1)}_{\I_1^c} = \frac{1}{N_{s}}\b{Z}_{\I_1^c}\b{Z}_{\I_1^c}^\top,
% \end{equation}
% where $\b{Z}$ contains $N_s$ Gaussian samples $\b{Z} =[\z_1,\z_2, \ldots \z_{N_s}]$. Additionally, Hutchison's estimator displayed in \eqref{Hutch} can also be used to approximate the diagonal of $\Q^{-1}$, which could be used as an initial approximation of $\tsig_{\I^c}$. The extra cost associated with using the Monte Carlo estimator in \eqref{eq:MC guess} is $\mathcal{O}(2nN_s)$. Finally, the cost of the Monte Carlo estimator, which is used to approximate the first Schur complement, $\tsig_{\I_1}$ is calculated. The $N_s$ vectors $\Z$ which are used in $ \hat{\sig}_{\textbf{MC}}$ from \eqref{MC Estimators} are $n \times 1$ in length, making the cost $\mathcal{O}(2nN_s)$. This initial guess does not need to be a `good' approximation of the lower diagonal block $\tsig_{\I_1^c}^{(0,1)}$, for the algorithm to converge. 

%\vic{What are the conclusions to this cost computation? A final remark is perhaps missing}
%\vic{Can we comment on potential advantages of the IBMI method when we need to invert large covariance matrices? (comments) } 
\section{Numerical Results} \label{sec:graphs}
Some numerical results to highlight the capabilities of \cref{alg:IBMI} will now be detailed. These experiments were run on a 2023 M3 MacBook Pro with 8-core CPU, 10-core GPU and 16-core Neural Engine, 16GB unified memory and 1TB SSD storage, running macOS 15.1.1, using MATLAB 2024a and OpenBLAS. (Experiments were also run with Apple's Accelerate BLAS and the results were qualitatively similar.) 
%\vic{I think we have different types of numerical results, perhaps we should introduce a first section like "Convergence vs. the dimension" then give more meaningful title to the others "Numerical vs theoretical convergence rate", "influence of the partition on the convergence". I would rather end up with a discussion section rather than conclusion and explain exactly in what context the method can be used what would be its potential, etc..}
Covariance matrices, $\Q \in \mathbb{R}^{p\times p}$, which are dense and guaranteed to be symmetric positive definite, were used for the following numerical results.  
Three covariance kernels were used to generate covariance matrices, which can be viewed in \cref{tab:kernels}. These are the exponential kernel (EXP), the radial basis function (RBF) kernel, and the inverse quadratic function kernel (IQUAD). For each, the values of $x$ and $x'$ used to generate the covariance matrix from the corresponding kernel are equally-spaced values from 0 to $p^{0.9}$. This ensured that the condition number increased moderately with the dimension. 
%These values, and the length-scale parameter for the kernels, were chosen so that the 2-norm condition number of $\Q$ increased with $p$, with condition numbers on the order of $10^7$ for the largest matrices used.
% Each covariance kernel is characterised by the length-scale parameter $\sigma$, which was fixed at $\sigma =1$. 
%The inverse of matrices larger than $2^{15}$ could not be approximated due to the memory constraints of storing a dense matrix. 
\\ \\
The partitioning of the covariance matrices is further explored in \cref{sec:overlap} but, unless otherwise stated, the covariance matrices are partitioned using a four-block partitioning with each block having a $5\%$ overlap, since these choices gave fast convergence in all cases. %The tolerance used for the stopping condition in \cref{alg:IBMI} is $10^{-8}$. 
Finally, the error estimate used as the stopping condition in \cref{alg:IBMI} is shown in \cref{appx:A} with a set tolerance of $10^{-8}$.
\renewcommand{\arraystretch}{2.25}
\begin{table}[!b]
    \centering
     \begin{tabular}{|c|c|c|} \hline
  \textbf{Kernel Type} & \textbf{Covariance Matrix }\\  \hline 
     Exponential Kernel&  $ \Q_{EXP}(\b{x},\b{x}')= \exp ^\frac{-{\mid \bf{x}-\bf{x}' \mid}}{5}$   \\ \hline
    RBF Kernel &  $\Q_{RBF}(\b{x},\b{x}')= \exp ^{- \frac{\mid \bf{x}-\bf{x}' \mid^2}{2(0.6)^2}} $ \\ \hline 
 Inverse Quadratic Kernel & $ \Q_{IQUAD}(\b{x},\b{x}') = \frac{1}{\sqrt{1+ \mid \bf{x}-\bf{x}'\mid^2}}$   \\ \hline
    \end{tabular}
   \caption{Covariance kernels used to generate dense symmetric positive definite covariance matrices.}
    \label{tab:kernels}
\end{table}

\begin{figure}[!hp]
\centering 
\subfloat[Exponential Kernel ($\Q_{EXP}$)]{%
  \includegraphics[clip,width=0.75\columnwidth]{figs/new_exp_mac_Apple_Comparision_MATLAB.png}%
}

\subfloat[RBF Kernel ($\Q_{RBF}$)]{%
  \includegraphics[clip,width=0.75\columnwidth]{figs/new_rbf_mac_Apple_Comparision_MATLAB.png}%
}

\subfloat[Inverse Quadratic Kernel ($\Q_{IQUAD}$)]{%
  \includegraphics[clip,width=0.75\columnwidth]{figs/new_iquad_mac_Apple_Comparision_MATLAB.png}
}
\caption{Dimension of $\Q$ and the time taken for \cref{alg:IBMI} to converge and approximate $\tsig$, compared to the time taken by MATLAB's \texttt{inv()} function to compute $\sig$.} \label{Figure1}
\end{figure} 
\subsection{Dimension vs Number of Iterations}
We first investigate the performance of \cref{alg:IBMI}, for the different covariance matrices as the dimension $p$, of the matrices increases. Specifically, $p = 2^\ell$, where $\ell = 8,\dotsc, 15$. (Larger covariance matrices could not be stored.) 
%The timings of how long \cref{alg:IBMI} took to converge were recorded as the dimension of the covariance matrices increased. 
The time taken for  \cref{alg:IBMI} to approximate the inverse of each covariance matrix, generated by the kernels in \cref{tab:kernels}, was compared with the time taken for MATLAB's inverse function \texttt{inv()} to invert the same matrices. 
%, is identified by the blue markers in \cref{Figure1}. The solid blue line displays the trend as the dimension of the matrix increases. This is also compared with the time taken for MATLAB's inverse function \texttt{inv()} to invert covariance matrices, of the same dimensions. The timings of these can be seen by the red markers of  \cref{Figure1} for each of the three covariance kernels in \cref{tab:kernels}. 
%As the dimension of the covariance matrices increases, 
It can be seen in \cref{Figure1} that \cref{alg:IBMI} converges faster for covariance matrices larger than $2^{10}$ in dimension, compared to the in-built function \texttt{inv()}, for all three covariance kernels.
\\ \\ 
When approximating the inverse of smaller covariance matrices, \cref{alg:IBMI} was slightly slower for some covariance kernels. For example, for $\Q_{IQUAD}$ with dimension $2^8$, 
%which can be seen in the third figure in \cref{Figure1}, 
\cref{alg:IBMI} took 0.00618 seconds to converge, compared to 0.00345 for MATLAB's \texttt{inv} function. For large covariance matrices, \cref{alg:IBMI} converged the fastest for the RBF kernel, taking 
%272.4479 
272 seconds, compared to %539.6741 
540 seconds for the exponential kernel and %710.1620
710 seconds for the inverse quadratic kernel. However, all three covariance kernels converged quicker with \cref{alg:IBMI} than \texttt{inv()} which took: %1207.5887 
1208 (RBF kernel), %1273.827 
1274 (EXP kernel), and %1339.422 
1339 (IQUAD kernel) seconds, respectively.  \\ \\ 
\renewcommand{\arraystretch}{2.2}
\begin{table}[!b]
\centering
\caption{The number of iterations for \cref{alg:IBMI} to converge, and the error in $\tsig$, for matrices generated by the three covariance kernels. } \label{tab:iters vs error}
\begin{tabular}{|l|l|l|l|l|l|l|} \hline
Dim of $\Q$ & \multicolumn{3}{|l|}{Number of Iterations} & \multicolumn{3}{|l|}{Error $ \| \tsig - \sig \|_2$} \\ \hline
& EXP         & RBF         & IQUAD        & EXP    & RBF    & IQUAD   \\ \hline
\textbf{$2^8$}   & 1    & 5   & 1   & 1.2616e-10 & 6.9947e-07 & 5.8826e-07  \\ \hline
$2^9$             & 1  & 3    & 1  &   7.9793e-10 & 4.6919e-06 & 1.147e-09  \\ \hline
$2^{10}$            & 1  & 2     & 1  &    4.33e-09   & 3.2503e-06 & 6.8218e-11 \\ \hline
$2^{11}$             & 1    & 1    & 1    &  4.4162e-08 & 0.0086078  & 5.3152e-12    \\ \hline
$2^{12}$             & 1   & 1  & 1  &  1.8036e-07 & 2.0312e-06 & 1.4117e-11       \\ \hline
$2^{13}$             & 1 & 1     & 1    & 5.0948e-07 & 1.5404e-07 & 5.8581e-11    \\ \hline
$2^{14}$            & 1 & 1  & 1    &    3.2119e-06 & 6.0552e-06 & 1.2582e-07       \\ \hline
$2^{15}$ & 1 & 1  & 1 & - & - & - \\ \hline  
\end{tabular}
\end{table}
%\subsection{Error vs number of iterations}
\cref{tab:iters vs error} displays the number of iterations taken for \cref{alg:IBMI} to converge, and $ \| \tsig - \sig \|_2$ at the last iteration, where $\| \cdot \|_{2}$ is the 2-norm,  as the dimension of the covariance matrix increases. Here, $\tsig$ is the inverse computed using MATLAB's \texttt{inv} function.
%Given that the exact inverse of each covariance matrix $\sig$ was also calculated for \cref{Figure1}, it was used in the following error estimate: $ \| \tsig - \sig \|_2$, where $\| \cdot \|_{2}$ is the 2-norm. This error was expensive to calculate in both time and memory costs, and was computed after \ref{alg:IBMI} had converged. Therefore, 
The error could not be computed for covariance matrices of dimension $2^{15}$ due to memory constraints. We note that the errors in \cref{tab:iters vs error} differ from the residual-based measure used in the stopping criterion (cf.\ \cref{appx:A}), because $\sig$ is unknown in practice. 
\\ \\ 
The exponential and inverse quadratic covariance kernels took only one iteration for \cref{alg:IBMI} to converge, irrespective of the dimension of the covariance matrix, as shown in \cref{tab:iters vs error}. The RBF kernel took more iterations to converge for smaller matrices, but did converge in one iteration for covariance matrices of dimension greater than or equal to $2^{11}$. We note here that for certain cases, such as ill-conditioned matrices, \cref{alg:IBMI} will likely require more iterations to converge, therefore performing slower compared to direct methods. 
\\ \\
%
The best approximated matrices came from the inverse quadratic kernel. %, as they gave a majority of the lowest errors in \cref{tab:iters vs error}. 
The covariance matrices produced by the exponential covariance kernel also had low errors for smaller covariance matrices, but the error did increase slightly as the dimension increased. This was not seen with the other covariance matrices produced by the other two covariance kernels, as there was no strict increase or decrease in the error. The poorest approximation was for a covariance matrix generated by the RBF kernel of dimension $2^{11}$, which gave an error of only 0.0086078. In this case, the residual-based stopping criterion is met for large matrices even when the error is somewhat larger. %This phenomenon is well known for iterative methods. 



\begin{figure}[!b]
    \centering
    \includegraphics[width=0.95\linewidth]{new_conv_proof_.png}
    \caption{Comparison of the error $\|\tsig_{\I_2}^{(r,2)} - \sig_{\I_2}\|_2$ for \cref{alg:IBMI} with $K=2$ non-overlapping blocks and the error bound in \cref{lemma} for  $\Q_{RBF}$ of dimension $p = 2^{12}$.} \label{Figure3}
\end{figure}

\subsection{Numerical vs theoretical convergence rate}
Given any symmetric positive definite matrix,   \cref{lemma} guarantees that \cref{alg:IBMI} will converge when $\Q$ is partitioned into two, non-intersecting sets. Moreover, it provides the upper bound \cref{eq:bound} on the error reduction at each iteration. We examine whether this bound is descriptive for a covariance matrix generated using the RBF kernel, of dimension $2^{12}$. 
%A reminder that the error bound for $\tsig_{\I_2}^{(r,2)}$ is:
%\begin{align} \label{eq:bound}
% \left\|{\tsig}_{\I_{2}}^{(r,2)} - {\sig}_{\I_{2}} \right\|_2 &\leq \left\| \left( \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2}\right)\right\|_{2}^{2r} \left\| \tsig^{(0,2)}_{\I_2} -  \sig_{\I_2} \right\|_{2}. 
% \end{align}
As the number of iterations increases, \cref{Figure3} confirms that the actual error decreases linearly, similarly to the upper bound \cref{eq:bound}. The convergence rate is better, but fairly similar to, the rate of 0.333 predicted by the bound, indicating that the bound is fairly descriptive in this case. 
%errors decrease linearly, and the error relating to $\tsig_{\I_2}^{(r,2)}$ is always less than the bound for each iteration after 0. The blue solid line represents the error of, $\tsig_{\I_2}^{(r,2)}$ and the red line represents the bound on the right-hand side of \cref{eq:bound}. If $\rho$ is close to 1, \cref{alg:IBMI} will take longer to convergence, highlighting the importance of a well conditioned matrix. Condition number up to $10e^6?$ have managed to converge with \cref{alg:IBMI}.


\subsection{Influence of the Partitioning on the Convergence\label{sec:overlap}}
\begin{comment}
\newcommand*\rot{\rotatebox{90}}
\begin{table}[!b]
\centering
\caption{Time taken for Algorithm \eqref{alg:IBMI} to converge (in seconds) for different numbers of blocks and different amounts of overlap between the blocks when partitioning $\Q_{RBF}$ of dimension $2^{14}.$} \label{tab:blocks & overlap}
\begin{tabular}{ll|lllll}
&   & \multicolumn{5}{c}{Overlap Fraction}      \\
&   & 0.00   & 0.05   & 0.1  & 0.15   & 0.2    \\ \hline
\multirow{5}{*}{\rot{Number of Blocks}} & 2 & 533.87 & 35.658 & 38.857 & 35.779 & 35.074 \\
 & 3 & 613.62 & 34.744 & 35.37   & 35.477 & 35.923 \\
& 4 & 634.22 & 35.003 & 34.661   & 34.81  & 35.003 \\
 & 5 & 626.95 & 35.255 & \underline{34.224} & 34.873 & 35.129 \\
 & 6 & 654.66 & 35.347 & 35.169 & 34.844 & 35.782 \\ \hline
\multicolumn{2}{l|}{Iters*}&20& 1 & 1 & 1      & 1     
\end{tabular}
\end{table}
\end{comment}

\newcommand*\rot{\rotatebox{90}}
\begin{table}[]
\centering
\caption{Time taken for \cref{alg:IBMI} to converges (in seconds) by altering the number of blocks and overlap between the blocks, when partitioning a covariance matrix $\Q_{RBF}$ of dimension $p=2^{12}$.} \label{tab:blocks & overlap}
\begin{tabular}{ll|lllll}
&   & \multicolumn{5}{c}{Overlap Fraction}  \\   &   & 0.00   & 0.05 & 0.10    & 0.15   & 0.20    \\ \hline
\multirow{5}{*}{\rot{Number of Blocks}}  & 2 & 323.180 & 1.1419 & 1.1054 & 1.1807 & 1.0588 \\
 & 3 & 398.898 & 1.1575 & 1.1486 & 1.1673 & 1.1606 \\
& 4 & 401.556 & 1.1791 & 1.1724 & 1.2099 & 1.2433 \\
& 5 & 469.355 & 1.2222 & 1.2472 & 1.2666 & 1.315  \\
& 6 & 487.718 & 1.3113 & 1.2535 & 1.2749 & 1.3823 \\ \hline
\multicolumn{2}{l|}{Iters} & 476 & 1  & 1      & 1 & 1 
\end{tabular}
\end{table}

The partitioning of the covariance matrix $\Q$ can greatly affect the convergence rate of \cref{alg:IBMI}.  \cref{lemma} details how  \cref{alg:IBMI} will converge for the two-block non-overlapping partitioning, given any symmetric positive definite matrix $\Q$. Here some numerical results are displayed which suggests that the multi-block partitioning with overlap will converge faster than the two-block partitioning. A covariance matrix $\Q$, of dimension $2^{12}$ was generated using the RBF covariance kernel. When partitioning the matrix for \cref{alg:IBMI}, the number of blocks and size of the overlap between the blocks were varied. The effect on the time taken for \cref{alg:IBMI} to converge, and the number of iterations required, was then recorded 
%and can be viewed 
in \cref{tab:blocks & overlap}. 
Note that the number of iterations was independent of the number of blocks, $K$, within the tested range of $K=2,\dotsc, 6$.
%Additionally, the number of iterations for each overlap size was recorded and can be viewed in the last row of \cref{tab:blocks & overlap}.
\\ \\ 
The overlap between the partitioned blocks varied between 0\% and 20\%.
%(0\%) as displayed in column one of Table \eqref{tab:blocks & overlap} and increasing to 20\%. %When producing these results, the overlap was increased up to 50\%, however, the results did not seem to differ much after a 20\% overlap.
\cref{tab:blocks & overlap} illustrates how even a small amount of overlap greatly decreased the number of iterations, and hence time, for \cref{alg:IBMI} to converge. 
When non-overlapping blocks were used,  \cref{alg:IBMI} took 476 iterations to converge, taking between 323 seconds (for two blocks) and 488 seconds (for six blocks). However, by introducing only a  5\% overlap, the algorithm converged in 1 iteration and between 1.14 and 1.31 seconds.  %This is reflected in the computational time: \cref{alg:IBMI} took just over 10 minutes to converge for most covariance matrices when there was no overlap between the blocks, compared to around 35 seconds when overlap between the block sizes was introduced. 
%Additionally, introducing overlap significantly reduces the number of iterations needed for \cref{alg:IBMI} to converge, reducing the number of iterations from 20 iterations without overlap to just one when overlap was introduced. %This reduction with iterations remained consistent across the different number of blocks, and the only variation in the number of iterations was when overlap was added. 
\\ \\ 
The number of blocks used when partitioning the covariance matrix was varied between $K=2$ and $K=10$, but the results were $K>6$ have been omitted due to their similarity with the $K=6$ case. 
%tarted at two and increased by one, stopping at ten blocks. Only blocks two to six are displayed in \cref{tab:blocks & overlap} due to the similarity in results for more than 6 blocks. 
%It can be seen from the first column, 
When no overlap is used, it was quicker to use a two block partitioning with Algorithm \eqref{alg:IBMI}. %When a 5\% or 20\% overlap was used, 
When overlap was introduced, only one iteration was required and the timings were very similar for all choices of $K$. 
The time for \cref{alg:IBMI} to converge increased slightly with the number of blocks and the overlap fraction and, for this particular matrix, 
% but 
%overlap was introduced, 
the smallest time was achieved for two blocks and a 10\% overlap.
%a partitioning using 4 blocks appeared to be most optimal for a $15\%,$ and $20\%$ overlap. 
However, the variation in timings for the overlapping cases was small, indicating that the algorithm is fairly insensitive to the number of blocks in the partitioning, and the amount of overlap. Although our default choices in other experiments are $K=4$ blocks and an overlap of 5\%, results are fairly similar for other partitionings. \\ \\ 
% The fastest time for \cref{alg:IBMI} to converge overall was 34.224 seconds, taken to approximate $\Q_{RBF}$ using a five block partitioning with a 10\% overlap between the blocks. 
Overall, \cref{tab:blocks & overlap} highlights how introducing overlap appears to be more effective than optimising the number of blocks when partitioning the covariance matrix to achieve faster convergence for \cref{alg:IBMI}. 




\begin{comment} 
\begin{table}[]
\centering
\caption{Time taken for Algorithm \eqref{alg:IBMI} to converges (in seconds) by altering the number of blocks and overlap between the blocks when partitioning a covariance matrix $\Q$ of size $16384 \times 16384$.}
\begin{tabular}{ll|llllllll}
 &    & \multicolumn{8}{c}{Overlap Fraction}  \\ 
  &    & 0.00   & 0.05   & 0.1    & 0.15   & 0.2   & 0.3    & 0.4    & 0.5    \\ \hline
\multirow{9}{*}{\rot{Number of Blocks}}    & 2  & 533.87 & 35.658 & 38.857 & 35.779 & 35.074  & 38.53  & 36.219 & 38.402 \\
 & 3  & 613.62 & 34.744 & 35.37  & 35.477 & 35.923  & 40.266 & 38.617 & 40.699 \\
 & 4  & 634.22 & 35.003 & 34.661 & 34.81  & 35.003  & 40.561 & 38.54  & 40.482 \\
 & 5  & 626.95 & 35.255 & \underline{34.224} & 34.873 & 35.129  & 40.306 & 37.967 & 38.971 \\
  & 6  & 654.66 & 35.347 & 35.169 & 34.844 & 35.782  & 39.803 & 39.563 & 39.516 \\
 & 7  & 635.67 & 34.761 & 34.721 & 34.618 & 34.985  & 38.406 & 38.434 & 38.338 \\
 & 8  & 650.23 & 35.47  & 35.495 & 35.367 & 35.621 & 38.364 & 38.707 & 39.606 \\
 & 9  & 636.74 & 36.114 & 38.354 & 34.692 & 34.749 & 36.971 & 37.725 & 38.981 \\
 & 10 & 651.55 & 36.642 & 37.967 & 34.544 & 35.443 & 37.313 & 38.431 & 38.967 \\ \hline 
\multicolumn{2}{l}{Iters*} & 20 &1 &1 &1 &1 &1 &1 &1 
\end{tabular}
\end{table}
\end{comment}



\begin{comment}
\section{Application to Precision Matrices}
More specifically, obtaining the covariance matrix from its inverse, also known as the precision matrix, is a well-known challenge within multivariate statistics. The covariance matrix $\sig \in \mathbb{R}^{p\times p}$ is typically dense, unlike its inverse, the precision matrix, $\Q \in \mathbb{R}^{p\times p}$ which is often sparse. The sparsity structure of $\Q$ is determined by the conditional independence of one variable given all other variables. In particular, an element $\Q_{i,j} = 0$ if and only if the corresponding variables $\z_{i}$ and $\z_j$ are conditionally independent given all other elements, given $\b{Z}=\left[ \b{z}_1,\cdots \b{z}_{Ns}\right], \b{Z} \sim \mathcal{N}(\b{\mu},\sig)$ \cite{Finn}. Consequently, many entries in the precision matrix are zero, making it computationally efficient to store.
\end{comment}

 
\section{Discussion} \label{sec:conclusions}
In this paper, we have presented a novel iterative block matrix inversion algorithm which can accurately and efficiently  approximate the inverse of a dense symmetric positive definite matrix. The IBMI algorithm serves as a way to approximate the off-diagonal elements of the inverse of a symmetric positive definite matrix, which is a known limitation for current literature. When $\Q$ is partitioned into two non-intersecting sets, \cref{alg:IBMI} will always converge, as shown in \cref{lemma}. Numerical results indicate that the multi-block partitioning with overlap accelerates the convergence of \cref{alg:IBMI}. Moreover, \cref{alg:IBMI} outperforms MATLAB's built-in inverse function, \texttt{inv()} in terms of time and computational complexity for the large dense matrices examined in \cref{sec:cost}. \\ \\ 
%
\cref{alg:IBMI} is generally applicable to any symmetric positive definite matrix, without any additional constraints such as converting $\Q$ into a hierarchical low rank matrix and therefore, has the potential to assist with a wide range of modern problems within data science, machine learning and multivariate statistics. One application which could benefit significantly is Gaussian process regression (GPR), as both the covariance matrix and its inverse (the precision matrix) are needed for prediction and uncertainty quantification. For high dimension data sets, directly inverting the covariance matrix to derive the posterior predictive equations can become computational infeasible. \cref{alg:IBMI} could offer a potential solution for obtaining the inverse, allowing GPR to be applied to these high dimensional data sets. %\\ \\
%Adaptations of \cref{alg:IBMI} could also be explored. 
Furthermore, the IBMI algorithm could potentially be altered to approximate block diagonal sub-matrices of, $\tsig$ rather than the full matrix. This partial approximation may be beneficial to methods where only a subset of the full inverse is required, such as in the literature discussed in  \cref{sec:background}  and referenced in \cite{ FIND.ALG, SelInv,32xia2013superfast, Rue_2023}. %\cref{alg:IBMI}  bridges a gap in current literature as an efficient method of approximating the inverse of a symmetric positive definite matrix, posing as a potentially valuable tool for tackling computational challenges across many mathematical disciplines. 

%\vic{Can we give a few examples where the complete inverse is needed? Perhaps say something more about future work and improvements of the current method: like potential applications or preconditioning}
\appendix
\section{Error Estimate} \label{appx:A}
The following error was used as a stopping condition for \cref{alg:IBMI}:
\vspace{-0.15cm}
\begin{equation} \label{eq:error}
    \texttt{Error} = \left\| \tsig_{\I}\Q_{\I,\I^c} + \tsig_{\I,\I^c}\Q_{\I^c}  \right\|_{2}, 
\end{equation} 
where $\|\cdot\|_2$ is the usual matrix norm induced by the Euclidean vector norm. 
The quantity \texttt{Error} measures how well the off-diagonal elements of $\tsig=\Q^{-1}$ are approximated. 
To see this, we consider $\tsig\Q$, which is the identity matrix if $\tsig=\sig$:
%take the approximated matrix is and $\Q$: 
\vspace{-0.15cm}
\begin{align*}
    \tsig\Q =& 
     \begin{bmatrix}
\Q_{\I}^{-1}+\Q_{\I}^{-1}\Q_{\I,\I^c}\tsig_{\I} \Q_{\I^c,\I}\Q_{\I}^{-1} &  -\Q_{\I}^{-1} \  \Q_{\I,\I^c}\tsig_{\I^c}\\ 
         - \tsig_{\I^c}\Q_{\I^c,\I}\Q_{\I}^{-1} &  \tsig_{\I^c}
    \end{bmatrix}
    \begin{bmatrix}
        \Q_{\I} & \Q_{\I,\I^c} \\ 
        \Q_{\I^c,\I} & \Q_{\I^c} 
    \end{bmatrix} \\ 
    = &
    \begin{bmatrix}
\mathbf{I}& \Q_{\I}^{-1}\Q_{\I,\I^c}\left[ \mathbf{I} -\tsig_{\I^c} \sig_{\I^c}^{-1} \right]\\ 
\mathbf{0} & -\tsig_{\I^c} \sig_{\I^c}^{-1}
\end{bmatrix},  
\end{align*}
where the exact Schur complement be denoted by $\sig_{\I^c}^{-1} = \Q_{\I^c,\I^c}- \Q_{\I^c,\I}\Q_{\I,\I}^{-1}\Q_{\I,\I^c}$. 
It is clear that when the approximation of the Schur complement $\tsig_{\I^c}$ is exact i.e., when $\tsig_{\I^c} = \sig_{\I^c}$, then $\tilde{\Q}^{-1} \Q = \b{I}$, as expected. Within \cref{alg:IBMI}, the upper off-diagonal block matrix is used as an error estimate in order to judge how well approximated the off-diagonal elements are. A tolerance is set by the user and if this error estimate is lower than the tolerance then \cref{alg:IBMI} will return the full approximated matrix and number of iterations.


\begin{comment}

\section{Appendix A }\label{appx:A} 

The upper diagonal block of the previous covariance matrix approximation, $\tsig_{\I_2}^{(1)}$ is kept and is used in the next iteration:
\begin{align*}
   \! \tsig_{\I_{1}}^{(2)} &= \Q_{\I_1}^{-1}\Q_{\I_{1},\I_{2}}\tsig_{\I_2}^{(1)}\Q_{\I_2,\I_1}\Q_{\I_1}^{-1} \\ 
    &= \!\Q_{\I_1}^{-1} \! + \! \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Bigg[ \Q_{\I_2}^{-1} + \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \left[ \Q_{\I_1}^{-1} + \!\Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \tilde{\sig}_{\I_{2}}^{(0)} \Q_{\I_2,\I_1}\Q_{\I_1}^{-1}   \right]  \\ &  \qquad \qquad \qquad \qquad \qquad 
  \qquad \qquad \qquad \qquad \qquad \qquad \Q_{\I_1,\I_2}\Q_{\I_2}^{-1}   \Bigg] \Q_{\I_2,\I_1}\Q_{\I_1}^{-1} \\ 
    \! &= \Q^{(\frac{1}{2})}\!+ \! 
     \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \Q_{\I_2,\I_1}  \Q_{\I_1}^{-1} \Q_{\I_1,\I_2}  
     \tsig_{\I_{2}}^{(0)} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}
     \Q_{\I_2,\I_1} \Q_{\I_1}^{-1}.
\end{align*}
and $\Q^{(\frac{1}{2})} \!= \!\Q_{\I_1}^{-1}\! +\! \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} + \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1} \Q_{\I_2,\I_1} \Q_{\I_1}^{-1} \Q_{\I_1,\I_2} \Q_{\I_2}^{-1}$. 
Then this expression for $\tsig_{\I_1}^{(2)}$ can be used in the expression for $\tsig_{\I_2}^{(2)}$ in Section \eqref{sec:conv}. 
\end{comment}


\section*{Acknowledgments}
We would like to acknowledge Professor Finn Lindgren and John Pearson for helpful discussions. 

\bibliographystyle{siamplain}
\bibliography{references}
\end{document}
