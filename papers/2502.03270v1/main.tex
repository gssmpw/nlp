%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{svg}
\usepackage{xspace}

%%% package for icons in PVR table
\usepackage{fontawesome5}

% \usepackage{ulem}
% Define custom colors
% \definecolor{lightgreen}{rgb}{0.88, 1.0, 0.88}
% \definecolor{lightred}{rgb}{1.0, 0.88, 0.88}

\definecolor{lightgreen}{HTML}{CEEAD6}
\definecolor{lightred}{HTML}{FAD2CF}
\definecolor{lightorange}{HTML}{FEEFC3}
\definecolor{lightblue}{HTML}{30CEFE}
\definecolor{darkerblue}{HTML}{5CA3FF}
\definecolor{brightpurple}{HTML}{9865FE}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{adjustbox}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}
\newcommand{\todo}[1]{{\color{red}{\bf {TODO: #1}}}} 

\def\eg{\emph{e.g.,}\xspace} 
\def\Eg{\emph{E.g.,}\xspace}
\def\ie{\emph{i.e.,}\xspace} 
\def\Ie{\emph{I.e.,}\xspace}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{On the use of PVRs in Visuo-Motor Policy Learning}
\icmltitlerunning{On the use of Pre-trained Visual Representations in Visuo-Motor Policy Learning}

\begin{document}

\twocolumn[
\icmltitle{When Pre-trained Visual Representations Fall Short:\\ Limitations in Visuo-Motor Robot Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{dagger}{\dag}

\begin{icmlauthorlist}
\icmlauthor{Nikolaos Tsagkas}{yyy}
\icmlauthor{Andreas Sochopoulos}{yyy}
\icmlauthor{Duolikun Danier}{yyy}
\icmlauthor{Chris Xiaoxuan Lu}{xxx,dagger}
\icmlauthor{Oisin Mac Aodha}{yyy,dagger}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Informatics, University of Edinburgh, UK}
\icmlaffiliation{xxx}{Department of Computer Science, University College London, UK}
% \icmlaffiliation{}{\dag Indicates equal senior authorship.}
% \icmlaffiliation{slmc}{Statistical Learning \& Machine Control group}

\icmlcorrespondingauthor{Nikolaos Tsagkas}{n.tsagkas@ed.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Visuo-motor Robot Learning, Pre-trained Visual Representations, Behaviour Cloning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % \icmlEqualContribution otherwise use the standard text.

\begin{abstract}
% C. Stachniss Abstract Recipe 
%% 1. Why should I care?
%% 2. Which problem do I address?
%% 3. How do I address the problem?
%% 4. What does my approach achieve?

The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. 
However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. 
These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. 
This work identifies these shortcomings and proposes solutions to address them. 
First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time.  
Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. 
Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations. 
Project page:~\texttt{\href{https://tsagkas.github.io/pvrobo/}{tsagkas.github.io/pvrobo}}
% ~\texttt{\url{https://tsagkas.github.io/click2grasp}} 

\end{abstract}
\begin{figure*}[!t]
%\vskip 0.2in
\begin{center}
\begin{tabular}{c}
\includegraphics[width=\linewidth]{figs/pca_bin_picking.pdf} \\
% \includesvg[width=\linewidth]{figs/traj_bin_picking.svg} \\
\includegraphics[width=\linewidth]{figs/traj_bin_picking.pdf} \\
\end{tabular}
%\vskip -0.1in
\vspace{-15pt}
% \caption{Visualisation of PCA results for the features extracted from frames of an expert demonstration trajectory in the Bin Picking task across the studied PVRs. The top row shows results for ViT-based models, while the bottom row corresponds to ResNet-based models. The sample task frames are approximately aligned with the colours shown on the colourbar below, representing their respective positions in the trajectory. The decomposition of the features into a 2D plane reveals how the features that correspond to the frames depicting the gripper's \textcolor{lightblue}{\textbf{descent}} and \textcolor{brightpurple}{\textbf{ascent}} might be entangled, due to their visual similarities. Similarly, at the stage where the \textcolor{darkerblue}{\textbf{gripper stops}} to close its fingers and pick up the block, the features form a tight cluster, due to the miniscule changes in the pixel domain. The average performance for the particular task is less than 50\%, whereas our disentangling approach increases the success rate by  $16.4$\%.}
\caption{PCA of features from an expert demonstration in Bin Picking across PVRs (Top row: ViT models; Bottom row: ResNet models). Frame colours align with trajectory stages, suggesting feature entanglement during the gripper \textcolor{lightblue}{\textbf{descent}} and \textcolor{brightpurple}{\textbf{ascent}}, and during the \textcolor{darkerblue}{\textbf{gripper stop}} phase. Our disentangling method improves success by $16.4$\% on the Bin Picking task, up from $<$50\%.}

\label{fig:pvr_trajectories}
\end{center}
%\vskip -0.2in
\vspace{-15pt}
\end{figure*}

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

\section{Introduction}
\label{sec:intro}
% \textbf{Why?}
Performing robust and accurate robotic manipulation from visual inputs necessitates informative and stable visual representations. 
The traditional paradigm for training visuo-motor policies has involved learning visual encoders from scratch alongside policy models~\cite{JMLR:v17:15-522}. 
Recently, however, the adoption of pre-trained visual representations (PVRs), \ie computer vision models trained on large and diverse visual datasets, has emerged as a compelling alternative, moving away from the tabula-rasa approach~\cite{parisi2022unsurprising}. 
This shift is driven by three key factors: the state-of-the-art performance of PVRs in computer vision tasks, their impressive generalisation capabilities derived from training on vast datasets, and the absence of robust robot-specific foundation models capable of addressing challenges unique to robotics, such as handling diverse embodiments.

% \textbf{Which Problem?} 
Despite the promising results of PVRs in downstream robotic applications, including affordance-based manipulation~\cite{li2024affgrasp}, semantically precise tasks~\cite{tsagkas2024click}, and language-guided approaches~\cite{shen2023F3RM}, their integration into visuo-motor policy learning for even basic pick-and-place tasks remains an open challenge.
Crucially, training visual encoders from-scratch or fine-tuning them with in-domain data still leads to competitive performance compared to using raw PVR features or even adapted PVRs~\cite{sharmalossless, Hansen2022pre}.
Furthermore, no single PVR, or set of characteristics, has consistently delivered optimal performance across diverse tasks and environments~\cite{NEURIPS2023_022ca1be, hu2023pretrainedvisionmodelsmotor}.
Notably, their generalisation capabilities remain underutilised, as small scene variations can destabilise policy models~\cite{caron2021emerging, 10611331}, c.f. Fig.~\ref{fig:task_vis}. 
This limitation has reignited interest in training models from scratch with augmented datasets~\cite{Hansen2022pre}, a strategy that is prohibitively expensive for many real-world applications.

% \textbf{How and What?}
We identify critical shortcomings in the current use of PVRs for visuo-motor policy learning, rooted in the inherent nature of PVR features.
First, we observe that these features are temporally entangled, primarily because widely used PVRs are designed as time-invariant models. 
Additionally, imitation learning datasets often consist of frame sequences where only minor changes occur in the pixel domain between  adjacent timesteps. 
As a result, the extracted features from these frames remain highly similar, even at transition points where the corresponding actions may differ significantly. 
This discrepancy forces policy models to map nearly identical inputs to divergent outputs, introducing a problematic one-to-many mapping that violates the Markov property (see Fig.~\ref{fig:pvr_trajectories}).
Second, policy networks tend to overfit to features corresponding to dominant but irrelevant static visual cues (e.g., background elements), making them overly sensitive to minor scene perturbations. 
These seemingly small prediction errors accumulate over time, leading to substantial performance degradation.
% For instance, as shown in Fig.~\ref{fig:teaser}, DINO's attention is often misdirected towards distractors, such as a cereal box in the background, rather than focusing on task-relevant objects.

We identify these inherent characteristics of PVRs as key factors hindering visuo-motor policy learning and argue that these issues should be addressed at the feature level. 
Attempting to resolve the temporal entanglement problem within the policy network would limit the flexibility of PVRs in general policy architectures. 
For example, an LSTM policy network could be a good candidate for that~\cite{DBLP:conf/aaaifs/HausknechtS15}, but would prevent us from conditioning other SoTA approaches, \eg diffusion policies~\cite{chi2023diffusionpolicy}.
Similarly, augmenting the dataset for improving the policy's robustness~\cite{Hansen2022pre} would be prohibitive for real-world robot applications, as it would require a great number of man-hours and the fine-tuning of PVR weights could affect the rich encoded knowledge.  

In summary, we make the following contributions:\\
\textbf{1.~Identifying PVR limitations}: We identify key characteristics of PVRs that hinder effective visuo-motor robot learning. 
Specifically, we show that they fail to encode the temporal cues and scene agnostic fine-scale visual features needed for precise manipulation tasks.\\   
\textbf{2.~Temporal disentanglement}: 
We enhance PVR features by incorporating temporal awareness and task-completion perception, without altering the policy model architecture, yielding a statistically significant improvement in downstream task performance. \\
\textbf{3.~Targeted visual features}: We introduce a module that learns to directly attend to task-relevant visual cues while ignoring scene distractors. 
Our approach does not require dataset augmentation or re-training but instead more effectively utilises  existing PVR features, particularly benefiting  masked image modelling (MIM) trained PVRs.\\

\section{Related Work}
\begin{figure*}[!t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/model.pdf}}
\vspace{-10pt}
\caption{Standard PVR-based visuo-motor policy learning via behaviour cloning (a) and our approach (b), which integrates \emph{Temporal Encoding (TE)} for temporal features (Section~\ref{ssec:method_tenc}) and \emph{Attentive Feature Aggregation (AFA)} for selective local feature attention (Section~\ref{ssec:method_abc}).}
\label{fig:model}
\end{center}
\vspace{-15pt}
\end{figure*}

\subsection{PVRs in Visuo-motor Policy Learning}
% Intro to use of PVRs in VMPL
In~\cite{parisi2022unsurprising}, frozen PVRs were evaluated across simulated environments, outperforming models trained from scratch. %random features and  
Similarly,~\cite{hu2023pretrainedvisionmodelsmotor} showed that the utility of PVRs depends on the policy training paradigm, with behaviour cloning and inverse reinforcement learning yielding robust results, while reinforcement learning exhibited higher variability. 
Furthermore,~\cite{silwal2024large} provided evidence that simulation experiments (\eg Metaworld~\cite{yu2020meta} benchmark) are indicative of real world performance for PVR-based trained policies. 

% Role of dataset in pre-training visual representations.
The dataset(s) used for pre-training plays a pivotal role in PVRs. 
While it was hypothesised that pre-training with video data featuring egocentric human-object interaction would be highly effective for learning features suitable for robot learning (due to their emphasis on object manipulation), research indicates that the diversity of images within the dataset is a more critical factor in successful robot learning~\cite{dasari2023datasets, NEURIPS2023_022ca1be}. 
Indeed, PVRs pre-trained on static datasets such as ImageNet~\cite{ridnik2021imagenet21kpretrainingmasses} have demonstrated competitive performance, underscoring the importance of dataset variability over modality. 

% On the robustness of policies trained with features from frozen PVRs. 
PVRs are favoured for their generalisation capabilities in vision tasks, but out-of-distribution generalisation remains challenging in policy deployment. 
\cite{10611331} analysed the impact of various perturbations on PVR-based policy generalisation, while~\cite{burns2024what} identified correlations between generalisation performance and inherent model traits, such as ViTs' segmentation ability.
Conversely,~\cite{Hansen2022pre} found that learning from scratch with data augmentation can yield competitive results, while~\cite{spawnet} found that adapters~\cite{houlsby2019parameter} can improve policy generalisation when training with diverse object instances. 
We focus on developing methods that achieve robustness to scene changes without relying on dataset augmentation, which can be prohibitively expensive in real-world robotics applications.
Particularly, our goal is to understand the reasons PVR-based policies struggle to generalise well and argue that there is more to be achieved with untouched frozen PVRs and that their full potential in visuo-motor policy learning is yet to be revealed. 
%%%%%%%%%% Task visualisation %%%%%%%%%%
\begin{figure*}[t]
\begin{center}
% \centerline{\includesvg[width=\linewidth]{figs/task_visualisation.svg}}
\includegraphics[width=\linewidth]{figs/task_visualisation.pdf} \\
\vspace{-10pt}
\caption{Visualisation of the 10 tasks used for evaluation. The first row illustrates representative scenes for all tasks, as seen in the frames from the expert demonstrations (in-domain). The second row shows how the scenes are modified by randomly altering the brightness, orientation and position of the light sourse. Similarly, the third row presents changes to the tabletop texture.}
\label{fig:task_vis}
\end{center}
\vspace{-15pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Time-informed Policy Training} 
In PVR-based visuo-motor policy learning, the incorporation of temporal information remains underexplored. Augmentation with temporal perception can happen either at feature level or during training time. 

\textbf{Feature Augmentation}. While early fusion methods, such as stacking multiple frames before encoding~\cite{Karpathy_2014_CVPR}, are common in training visual encoders from scratch, late fusion-processing frames individually and stacking their representations~\cite{NIPS2017_3f5ee243} has shown superior performance with fewer encoder parameters. 
Recent work~\cite{NEURIPS2021_ba3c5fe1} highlights that naive feature concatenation in latent space is insufficient; instead, approaches like FLARE~\cite{NEURIPS2021_ba3c5fe1} incorporate sequential embeddings and their differences, inspired by optical flow techniques. 
Nevertheless, concatenating sequential embeddings as input to policy networks has become standard in visuo-motor policy learning~\cite{parisi2022unsurprising} and state-of-the-art generative policies~\cite{chi2023diffusionpolicy}. 
However, a gap remains in leveraging PVR features, which are primarily designed for vision tasks, within this temporal framework.

\textbf{Loss Function Augmentation}. A major limitation of many PVRs is their inherit lack of temporal perception, as most are pre-trained on static 2D image datasets. 
Temporal perception can be added by employing loss functions that enforce temporal consistency during training (\eg R3M~\cite{nair2022rm} and VIP~\cite{ma2022vip}), when training with video data.  
However, there is no clear consensus on the superiority of this approach compared to alternatives like MIM (\eg MVP~\cite{Xiao2022, hu2023pretrainedvisionmodelsmotor} and VC-1~\cite{NEURIPS2023_022ca1be}). 
This disparity suggests that existing temporal modelling strategies may be insufficient in isolation. 

In subsequent experiments, we evaluate PVRs trained with temporal information and demonstrate that methods trained with a time-agnostic paradigm achieve comparable performance.
We hypothesise that this limitation arises from a lack of task-completion perception, which we address by incorporating positional encoding-a fundamental mechanism in many machine learning approaches.
This straightforward operation has been instrumental in the success of Transformers~\cite{NIPS2017_3f5ee243}, implicit spatial representations~\cite{mildenhall2020nerf}, and diffusion models~\cite{ho2020denoising}.

\subsection{Task-Relevant Feature Extraction}
Downstream vision tasks often make use of the output features of PVRs. 
However, these features typically encode a broad range of scene information, much of which may be irrelevant to the specific task. 
To address this challenge, attentive probing~\cite{Chen2024, danier2024depthcuesevaluatingmonoculardepth, bardes2024revisiting} has emerged as a popular evaluation technique, leveraging local tokens. 
This approach leverages a cross-attention layer with a trainable query token, treating the local features from PVRs as a sequence of key-value pairs. 
Unlike traditional evaluation methods such as linear probing, attentive probing has shown significantly different vision evaluation outcomes, particularly with PVRs trained using MIM approaches (\eg MAE~\cite{He2021MaskedAA}), where features, such as the CLS token, often include irrelevant information.
Similarly, in robot learning, task-relevant signals like joint angles may correspond to particular image regions, with unrelated cues acting as distractions. 
By prioritizing task-relevant signals, we show that attentive probing enhances task performance, especially in out-of-distribution scenarios. 

\section{Methodology}
In this section, we present our approaches to enhance the deployment of PVRs in visuo-motor policy learning. First, we introduce the general behaviour cloning framework commonly employed in this domain (Section~\ref{ssec:method_preliminaries}). Then, we describe a method to enrich features with temporal information, aimed at mitigating issues of temporal ambiguity (Section~\ref{ssec:method_tenc}). Finally, we introduce our approach to selectively attend to task-relevant components of PVR features, improving their utility for robot learning tasks (Section~\ref{ssec:method_abc}).

\subsection{Preliminaries}
\label{ssec:method_preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Useful
%  \item https://arxiv.org/pdf/2412.05265\#page=103.08
%  \item  https://arxiv.org/pdf/2407.15007
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent\textbf{Imitation Learning via Behaviour Cloning}. 
We consider an expert policy \(\pi^\star : \mathcal{P} \times \mathcal{O} \to \mathcal{A}\), which maps the current proprioceptive observation \(p \in \mathcal{P}\) of a robot manipulator and a visual observation \(o \in \mathcal{O}\) to a corresponding robot action \(a \in \mathcal{A}\). 
This expert policy is used to generate a dataset of expert demonstrations, \(D_\text{exp}\), consisting of \(N\) trajectories \(\mathcal{T}^\text{e} = \{(p_t^i, o_t^i, a_t^i)_{t=0}^{T}\}_{i=1}^N\), where each trajectory captures the sequence of observations, and actions recorded over \(T\) timesteps while solving a task.

The goal of behavioural cloning is to learn a policy \(\pi_\theta\), parameterised by \(\theta\), that closely imitates the expert policy by minimizing the discrepancy between its actions and the expert’s actions. 
This is formulated as a supervised learning problem, where the loss function measures this discrepancy across the dataset of demonstrations:
\begin{equation}
    \mathbb{E}_{(p_t^i, o_t^i, a_t^i) \sim \mathcal{T}^e} \|a_t^i - \pi_\theta(f_\text{PVR}(o_t^i), p_t^i)\|_2^2,
\end{equation}
where \(f_\text{PVR}\) represents a PVR that acts as a feature extraction function used to process visual observations \(o_t^i\).

In robot learning, the decision-making process is commonly assumed to satisfy the Markov property. 
This assumption implies that the current observation \(x_t = (p_t, o_t)\) encapsulates all the information necessary for predicting the subsequent state, \ie \(P(x_{t+1} | x_t) = P(x_{t+1} | x_t, x_{t-1}, \dots, x_0)\). 
Consequently, tasks are modelled as sequences of decisions, where each action depends solely on the current state, facilitating the application of behaviour cloning under this framework.

\subsection{Temporal Disentanglement}
\label{ssec:method_tenc}

We find that the assumption of Markovian decision-making in policies based on features extracted from frozen PVRs is frequently invalid. This arises because, at each timestep, the available information may be insufficient for the policy to confidently map the current observation to the appropriate action. 

Consider the example presented in Fig.~\ref{fig:pvr_trajectories}, where PVR-features of the same pick-and-place trajectory are projected with PCA into 2D. 
Regardless of the PVR utilised, the extracted features seem to suffer from temporal entanglement. 
First, features extracted from the frames where the robot has stopped to pick up the box form a tight cluster, since the only change is the movement of the gripper fingers, which corresponds to a very small percentage of pixels. 
Second, as the gripper moves down and subsequently ascends, the primary visual change is the cube's vertical displacement relative to the table. Consequently, the visual features extracted from the descent and ascent frames may differ only marginally, and only in dimensions affected by the small pixel region of the cube.

Fig.~\ref{fig:pvr_trajectories} also hints that including proprioceptive data in the policy input is not necessarily adequate to resolve these ambiguities. 
This is either because the high-dimensionality of visual features often dominates the lower-dimensional proprioceptive input, or because the robot may follow nearly identical trajectories while performing a task, further compounding the difficulty of disentangling visually similar states. 
At the same time, prior methods, where features from successive images were concatenated  alone~\cite{parisi2022unsurprising} or along with their differences~\cite{NEURIPS2021_ba3c5fe1}, have been effective to some extent. 
However, if frame-to-frame appearance differences are minimal, the resulting features can be very similar, and thus their differences predominantly contain near-zero values. 

Training a policy network to map $(p_t, o_t)$ to $a_t$ becomes difficult under these conditions. 
When multiple observations are nearly indistinguishable, the mapping violates the functional requirement that each input must map to exactly one output. 
To address these challenges, we propose a simple yet effective method to augment each observation with a temporal component by encoding the timestep index of each frame as a high-dimensional vector, using Eq.~\ref{eq:t_enc}. 
This augmentation can temporally disentangle similar $(p_t, o_t)$ pairs, introducing a task progression signal into the robot state, which we argue can substantially enhance policy performance.
% Empirically, this leads to policies that frequently get stuck in loops, failing to complete tasks. 

\begin{align}
\label{eq:t_enc}
\gamma(t) &= \left( 
\sin\left( \frac{2^0 \pi t}{\text{s}^0} \right), \cos\left( \frac{2^0 \pi t}{\text{s}^0} \right), \dots, \right. \nonumber \\
&\quad \left. \sin\left( \frac{2^{T-1} \pi t}{\text{s}^{T-1}} \right), \cos\left( \frac{2^{T-1} \pi t}{\text{s}^{T-1}} \right) \right)
\end{align}

% \begin{align}
% \label{eq:t_enc}
% \gamma(t) &= \left( 
% \sin\left( \frac{2^k \pi t}{\text{s}^k} \right), \cos\left( \frac{2^k \pi t}{\text{s}^k} \right) 
% \right)_{k=0}^{T-1}
% \end{align}

Eq.~\ref{eq:t_enc} encodes the timestep \( t \) into a high-dimensional vector \(\gamma(t)\) using alternating sine and cosine functions at exponentially increasing frequencies \( 2^k \). The lower-frequency terms capture coarse temporal trends, while the higher-frequency terms provide finer temporal resolution, enabling the policy to distinguish between temporally similar states.
%%%%%%%%%%%%% TE EVALUATION %%%%%%%%%%%%
\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/bar_temporal_comparison.pdf}}
\vspace{-10pt}
% \caption{Evaluation of our Temporal Encoding (TE) technique against FLARE~\cite{NEURIPS2021_ba3c5fe1} and using no temporal augmentation on the extracted PVR features. Average results are reported: (a)  per task across all models, sorted in ascending order based on the TE performance, and (b) per model across all tasks. The FLARE and TE bars report the performance boost compared to using no time information.}
\caption{Comparison of Temporal Encoding (TE) against FLARE~\cite{NEURIPS2021_ba3c5fe1} and using no temporal augmentation on PVR features. Results (sorted by TE) show (a) per-task performance and (b) per-model performance. FLARE and TE bars indicate gains over no temporal information.}
\label{fig:avg_tenc}
\end{center}
\vspace{-15pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Attending to Policy Related Features}
\label{ssec:method_abc}
We posit that training policies using the global features of PVRs (\ie CLS token for ViTs or average channel feature for ResNets) leads to overfitting to scene conditions that are irrelevant to the task at hand. 
The output features of these representations often capture visual characteristics of the scene that may be irrelevant to the policy (\eg the texture of a tabletop). 
Processing such extraneous information not only dilutes the policy network's focus but also leads to overfitting to specific scene conditions. 

This observation aligns with recent work on vision model evaluation~\cite{Chen2024}, which argues that only specific image regions carry the necessary information for solving a task. 
% To address this limitation, \emph{attentive probing} has been proposed~\cite{Chen2024}, utilizing a cross-attention layer with a trainable query token that learns to attend to local features and dimension groups. 
Building on this insight, we hypothesise that incorporating local information is particularly effective in the context of robot learning, echoing findings in PVR distillation research~\cite{shang2024theia}, though this area remains empirically underexplored.

Recognizing the importance of local information is only part of the solution; a data-driven mechanism is also required to filter irrelevant details, such as background patches, and prioritise task-relevant information. To this end, we adopt the \emph{attentive probing} methodology to implement \emph{Attentive Feature Aggregation} (AFA). Specifically, we append a cross-attention layer to the frozen PVR, modified to include a trainable query token that interacts with the sequence of local tokens produced by the model. These tokens correspond to the per-patch embeddings for ViTs and the channel embeddings for ResNets, both from the final layer. 

Following Eq.~\ref{eq:cross_attn}, the query token $q$ computes dot products with the feature sequence, with length equal to $\text{\#patches}$ and dimension $d_k$, organized as a matrix $F$. These dot products are passed through a softmax function to assign weights to the contributions of each local token to the final embedding. Also, our module consists of multiple heads, so that specific dimension groups that might be irrelevant to the policy can be filtered out. Gradients are allowed to flow through the cross-attention layer, updating the parameters of $q$ as well as the key and value projection matrices, $W_K$ and $W_V$. 

\begin{equation}
\label{eq:cross_attn}
    \text{Attention}(q, F) = \text{softmax}\left(\frac{q \cdot (F \cdot W_K)^\top}{\sqrt{d_k}}\right) F \cdot W_V  
\end{equation}

\section{Experiments}
\subsection{Setup}
\noindent\textbf{Environment}. We conduct our experiments in the widely used MetaWorld simulation environment~\cite{yu2020meta}, which is built on the MuJoCo~\cite{6386109} physics engine. From this benchmark, we select the ten tasks visualised in Fig.~\ref{fig:task_vis} and generate 25 expert demonstrations with 175 rollout steps for each using the provided heuristic policies. The primary criterion for task selection is to maintain a balanced representation of easy, medium, and hard tasks, as identified in prior work on PVR-based visuo-motor control~\cite{mete2024questselfsupervisedskillabstractions, hu2023pretrainedvisionmodelsmotor}, as well as in our empirical results.

\noindent\textbf{PVRs}. To validate our hypotheses, we deploy seven Residual Networks (ResNets)~\cite{he2016deep} and seven Vision Transformers (ViT)~\cite{dosovitskiy2021an}, as summarised in Tab.~\ref{tab:pvrs_info} in the Appendix. Our selection includes the most popular PVRs utilised in robot learning applications that have led to SoTA performance. We also focused on ensuring diversity in training strategies, datasets, and the balance between local and global perception. Despite these variations, we maintain a consistent backbone architecture of ResNet-50 or ViT-B/16, with the exception of DINOv2, which employs a smaller patch size of 14. Also, for DINOv2 we discard overlapping patches, to ensure fairness in the comparison of PVRs. The models tested include powerful representations from vision-specific approaches (\eg DINO), vision-language models (\eg CLIP), and robot-learning-focused models (\eg R3M).

\noindent\textbf{Policy training}. For all policy training, we train the policy network five times, using five different seeds, keeping the PVR frozen, and report the interquartile  mean (IQM) success rate. As is common practice in similar work~\cite{parisi2022unsurprising, nair2022rm, hu2023pretrainedvisionmodelsmotor}, our policy head consists of a small number of MLP layers (four in our case), separated by ReLUs, and outputs the predicted action. We train with mini-batches of 128 samples for 80,000 steps.

%%%%%%%%%%%%%%%% AFA EVAL %%%%%%%%%%%%%%%
\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/bar_ood.pdf}} 
\vspace{-10pt}
\caption{Robustness evaluation of policies trained with PVR-extracted features temporally augmented with TE, and with features processed using AFA and temporally augmented with TE. The evaluation is conducted under perturbed lighting conditions and tabletop texture changes. Subplot (a) shows the average performance per task, while subplot (b) shows the average performance per model.}
\label{fig:augmented_feats_avg}
\end{center}
\vspace{-15pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Temporal Encoding}
We evaluate the performance of a policy network trained for each PVR under three conditions: without any temporal component, using the three most recent past observations and their latent differences (\ie FLARE~\cite{NEURIPS2021_ba3c5fe1}), and with our proposed Temporal Encoding (TE) method. 
We select the dimensionality of TE to be $64$ and the scale parameter $100$, after tuning these hyperparameters in a subset of tasks and PVRs (see Appendix~\ref{app_sub:hyperparam_te}).  

Fig.~\ref{fig:avg_tenc} illustrates (a) the average performance per task for each of the three approaches and (b) the average performance per model. 
% Statistical analysis, including paired t-tests and Wilcoxon tests, confirms that the performance improvements achieved with TE over both FLARE and no augmentation are statistically significant.
Statistical analysis (paired t-tests, Wilcoxon tests-results in Appendix~\ref{app_sub:tenc_all_results}) confirms TE's gains over FLARE and no augmentation are significant.
While VC-1 and iBOT achieve slightly higher average scores with FLARE, all other PVRs benefit significantly from temporal augmentation, even when compared to FLARE-augmented results. 
Similarly, apart from the ``Pick and Place'' and ``Shelf Place'' tasks, TE significantly enhances the average task performance.
We attribute TE’s superiority over FLARE to the fact that FLARE's method of stacking sequential latent embeddings, along with their differences, results in embeddings that are nearly identical due to the high similarity of features extracted from consecutive observations.
Consequently, the differences between these embeddings are often near-zero vectors, limiting FLARE's ability to effectively leverage temporal information.
Table~\ref{tab:tenc_flare} provides a detailed summary of the performance of each model-task pair.

An intriguing observation is that PVRs pre-trained with a temporal component in their objective function also benefit considerably from TE. 
For instance, R3M employs time-contrastive learning~\cite{sermanet2017unsupervised} to enforce similarity between representations of temporally adjacent frames-experience substantial gains from TE. 
Notably, VIP achieves an average performance boost of approximately $15$ percentage points, making it one of the most positively impacted models. 
This finding suggests a potential reconsideration of how temporal perception is integrated into features designed for robot learning. 
It raises the possibility that existing approaches may not fully exploit the temporal structure necessary for optimal performance.

%%%%%%%%% ABLATION %%%%%%%%%
\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/bar_ablation.pdf}}
\vspace{-10pt}
% \caption{Observing the effect of the TE and AFA components on our approach (\ie PVR+TE+AFA) against directly extracting features from a PVR for training the policy network. Results are reported on in-domain scenes.}
\caption{Ablation study on our approach (\ie PVR+TE+AFA). Results are reported on in-domain scenes.}
\label{fig:ablation}
\end{center}
\vspace{-15pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Attentive Feature Aggregation}
\label{ssec:afa}
We utilise a cross-attention layer and use 12 attention heads for features extracted from ViTs and 32 for features from ResNets. 
This configuration ensures that we process 64-dimensional feature chunks in both cases, maintaining fairness between the two backbone architectures. 
Consistent with standard attentive probing methodologies~\cite{Chen2024, danier2024depthcuesevaluatingmonoculardepth}, we employ a cosine learning rate scheduler with a warm-up phase. 
In all experiments, TE is applied to augment features with a temporal component. 
Specifically, for AFA, the temporal feature is concatenated with the output of the cross-attention module. 
We validate our full model's significance via an ablation study, presented in Fig.~\ref{fig:ablation}, that evaluates the contributions of TE and AFA in our approach, comparing it against training the policy network solely with PVR-extracted features.
% perform an ablation study on our full model to evaluate the contributions of TE and AFA in our approach, comparing it against training the policy network solely with PVR-extracted features in Fig.~\ref{fig:ablation}.

We hypothesise that AFA learns to attend only to scene areas that are important to the task, disregarding features irrelevant to the policy. 
To properly evaluate this component, we do not limit our evaluation to in-domain scenes but also with environments recreated with scene perturbations, leaving the training dataset distribution unchanged. 
These perturbations include two modifications, details of which are included in Appendix~\ref{app_sup:scene_pert}. 
First, tabletop texture changes, randomly selected from $30$ distinct textures, some of which feature vibrant patterns that act as strong distractors. 
These changes affect a significant area of the frame. 
Second, variations in lighting conditions, including adjustments to the orientation, position, and brightness of the light source. 
These modifications influence the entire frame, including the robot and the object it manipulates.




We summarize the performance of policies trained directly with PVR features and those trained with features aggregated using AFA in Fig.~\ref{fig:augmented_feats_avg}, averaged across tasks and models on perturbed scenes. 
We also provide in Fig.~\ref{fig:augmented_feats} a concise summary of the performance of each task and model pair on in-domain scenes and on perturbed scenes (per perturbation category).
% The left plot illustrates the average performance per task and per model for out-of-distribution scenarios, while the right plot focuses on the average OOD performance. 
% In Fig.~\ref{fig:augmented_feats} in the Appendix, we provide a concise summary of all task and model pairs. 

From these results, several trends emerge. 
AFA consistently improves the robustness of policies across all tasks, often yielding significant gains, with some tasks showing up to a threefold improvement. 
The only exception is the ``Drawer Open'' task, which sees little benefit from AFA. 
This is likely due to its inherent simplicity, as the task involves manipulating a large object without requiring control of the gripper fingers, which remain open throughout the demonstrations. 
Consequently, attending to local observations has limited impact. 

Additionally, most models exhibit improved out-of-distribution performance with AFA, except for CLIP and ViT-B/16. 
This is reasonable, as these models are trained with objectives that emphasise global frame perception, unlike other models that incorporate supervision at the patch level. 
Notably, MIM-trained PVRs benefit the most from AFA, reflecting the alignment between AFA’s design, which is inspired by attentive probing, and the training principles of MIM-based models. 
These findings highlight AFA's ability to enhance policy performance, particularly in challenging out-of-distribution scenarios, and underscore its compatibility with models that leverage local feature representations.

The average in-domain performance remains nearly unchanged, with a slight increase from $63.1$\% to $66.4$\% when using AFA. 
The minor performance boost observed with AFA in in-domain scenarios, especially when compared to its substantial improvements in perturbed scenes, suggests that AFA does not learn a new latent space for the PVR that is more suited to the task. 
Instead, it appears to refine the use of the existing latent space by learning to effectively leverage relevant information while discarding elements that are irrelevant to the policy. 
This distinction underscores AFA’s role as a mechanism for better utilisation of pre-trained features rather than redefining or adapting the underlying feature space.
%%%%%%%%%%%%%%% Attention Maps %%%%%%%%%%
\begin{figure}[!t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/heatmap_dino.pdf}}
\vspace{-10pt}
\caption{Comparison of attention heatmaps between the DINO and DINO+AFA (ours). The latter learns to focus on task-relevant regions and ignores scene changes (\eg distractor objects). Visualisations from more PVRs are included in Fig.~\ref{fig:more_heatmaps}.}
\label{fig:teaser}
\end{center}
\vspace{-15pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
We explored how to effectively utilise PVRs for visuo-motor policy learning, identifying the issues of feature temporal entanglement and lack of robustness in scene visual changes. 
Furthermore, we proposed two approaches to address these limitations leading to a significant performance increase. 
Below we provide a set of questions with answers that we believe further highlight the contributions of our work.

\textbf{Q}:\textit{Does TE help because expert demos are time synced?}\newline
\textbf{A}: Even though the expert demos are generated by a heuristic policy, in Fig.~\ref{fig:demos_async} we show that they are actually not synchronised and the only common factor is that the task is always completed by T=175 steps. 

\textbf{Q}:\textit{Does AFA's robustness stem from the additional trainable parameters?}\newline
\textbf{A}: AFA introduces approximately 1.5M additional parameters to a policy model that originally has around 0.5M parameters. 
To determine whether the observed performance improvement is simply due to the increased model capacity, we train a policy network using raw PVR features but design a deeper policy network to match AFA's parameter count. 
The results, shown for two tasks in Fig.~\ref{fig:ablation_deeperMLP}, suggest otherwise. 
This indicates that the performance gains arise from AFA's ability to effectively attend to local information, as is suggested by Fig.~\ref{fig:teaser}.

\textbf{Q}:\textit{Does the simulation's lack of visual realism affect the perception of PVRs?}\newline
\textbf{A}: Empirical evidence has shown that simulations serve as a reliable proxy for PVR-based visuo-motor policy learning, particularly in environments like MetaWorld~\cite{silwal2024large}. 
To further validate this, we replicate the visualization in Fig.~\ref{fig:pvr_trajectories} for an expert demonstration in two real-world tasks. 
The results in Fig.~\ref{fig:pvr_trajectories_DLR} and Fig.~\ref{fig:pvr_trajectories_PUSHT}, reveal that the same feature-related issues persist, underscoring the relevance of our findings across both simulated and real-world settings.

\textbf{Q}:\textit{Why is there still a significant performance gap under scene perturbations?}\newline
\textbf{A}: We posit that the remaining performance gap largely stems from inherent limitations of behaviour cloning. 
Its supervised nature makes the policy network vulnerable to compounding errors. 
Despite improved feature attention, the models tend to overfit to specific visual settings, making it sensitive to domain shifts that progressively push the policy out-of-distribution. 



% \textbf{Q}:\textit{Why do PVR-based policies not achieve even better performance when paired with TE and AFA?}\newline
\textbf{Q}:\textit{Can we get even better policy performance?}\newline
\textbf{A}: Behaviour cloning performance is largely dependent on how well the expert policy $\pi^\star$ is sampled. 
Increasing the number of demonstrations or  utilising techniques like DAgger~\cite{pmlr-v15-ross11a} could help mitigate this issue.

% \textbf{Q}:\textit{Why not compare against other generalisation baselines?}\newline
% % \textbf{A5}: In this work we aim to provide evidence that the current use of PVR features hinders robustness in visuo-motor policy learning and propose AFA to patch this. Vision approaches for achieving robustness~\cite{InvariantRiskMinimization, hendrycks2021facesrobustnesscriticalanalysis, 9577393, bai2024hypo} usually target the dataset and are actually orthogonal to our apporach, and can be used coplentary way. Similarly, robustness in robot-learning is usually achieved via changes in the scene in the demonstration~\cite{Hansen2022pre, spawnet}. This is either unrealistic for scalable real-world robot learning or requires bridging the sim-to-real gap~\cite{9310796}, which is a difficult problem and out of the scope of this work.    
% \textbf{A}: This work aims to demonstrate that the standard use of PVR features limits robustness in visuo-motor policy learning and introduces AFA to address this issue. Existing vision-based generalisation methods~\cite{InvariantRiskMinimization, hendrycks2021facesrobustnesscriticalanalysis, 9577393, bai2024hypo} typically target the dataset and are actually complementary to our approach. Similarly, robot-learning robustness is often achieved through scene variation in demonstrations~\cite{Hansen2022pre, spawnet}, which is either impractical for scalable real-world learning or relies on bridging the sim-to-real gap~\cite{9310796}, a complex problem beyond our scope.
% \textbf{Q}:\textit{Why use image encoders instead of video encoders?}\newline
% \textbf{A}:

% {\bf Limitations}. We performed extensive experiments, evaluating multiple PVRs, and trained and tested policies across different seeds and representative tasks. 
% However, our work has limitations.
% First, while MetaWorld is a reliable proxy and widely used in robot learning experiments, we have not tested in our methods on an actual robot, to measure the effect of our methods in the real-world.
% Second, as is common practice, we rely on a simple MLP policy network to evaluate our methods, to make sure that our solutions target the feature space and are policy-architecture agnostic. Nevertheless, it would be interesting to evaluate how are solutions would affect SoTA policy methodologies (\dg diffusion models), which are known to underperform when conditioned with PVRs. 
% Future work should explore these limitations and assess our approaches with alternative policy models trained end-to-end with visual encoders. 

{\bf Limitations}. We conducted extensive experiments, evaluating multiple PVRs and training policies across different seeds and representative tasks. However, our work has certain limitations.
First, while MetaWorld serves as a reliable and widely used proxy for robot learning experiments, we have not tested our methods on a real robot. Evaluating their effectiveness in real-world scenarios remains an important direction for future research.
Second, in line with common practice, we use a simple MLP policy network to ensure our approach targets the feature space and remains agnostic to policy architecture. 
However, it would be valuable to explore how our methods impact state-of-the-art policy models, such as diffusion-based approaches, which are known to struggle when conditioned on PVRs~\cite{chi2023diffusionpolicy}.
Future work should address these limitations by assessing our approach with alternative policy architectures trained end-to-end with visual encoders.


% Second, our focus on PVRs popular in robot learning and traditional vision tasks leaves pre-trained video encoders underexplored. 
% These encoders may offer better temporal perception but potentially reduced generalisation due to limited visual variability in video datasets compared to image datasets. 



\section{Conclusion}
The use of PVRs for visuo-motor policy learning is still in its early stages, and we believe our work paves the way for further exploration of key challenges.
% In particular, we highlight characteristics that PVR features currently lack for effective policy learning, such as an inherent sense of task completion and the ability to focus on task-relevant cues. 
% Additionally, 
% Our findings underscore the crucial role of spatial tokens in this context.
Our insights contribute to the development of PVR models specifically designed for robot learning, ultimately leading to a generalist robotic system powered by large-scale vision foundation models.

% \todo{Can be short. Can mention the opportunities for future work in ML to make better use of these PVRs, \ie get the ML community excited about these robotics tasks.}

\clearpage
\clearpage
\newpage
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning and Robotics. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite



\bibliography{references}
% \input{references.bbl}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\clearpage
\newpage
\appendix

\setcounter{table}{0}   
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}


\onecolumn
\section{Evaluated PVRs}
\label{app_sub:all_pvrs}

In this section we describe the PVRs evaluated in our experiments. Table~\ref{tab:pvrs_info} summarises the architecture, training objective, and pre-training dataset for each PVR, along with its highlight that motivates our evaluation.
\input{tables/pvrs.tex}
\section{On the use of PVRs in Downstream Robotics Tasks}
\label{app_sub:pvrs_in_robo}
As discussed in Section~\ref{sec:intro}, while the application of pre-trained visual representations (PVRs) in visuo-motor policy learning is still nascent, these models have proven instrumental in other downstream robotics tasks.

For instance, in manipulation tasks utilizing dense visual descriptors without a learnable policy component, the field has transitioned from training these features from scratch~\cite{sundaresan2020learning,ganapathi2020learning,manuelli2021keypoints,florence2019selfsupervised,pmlr-v87-florence18a,nerf-supervision} to leveraging features extracted from vision foundation models such as DINO~\cite{caron2021emerging,oquab2023dinov2} and CLIP~\cite{radford2021learning}. 
A common strategy involves integrating these features into 3D spatial representations, as exemplified by works like F3RM~\cite{shen2023F3RM}, D$^3$Fields~\cite{wang2023d3fields}, and Click2Grasp~\cite{tsagkas2024click}. Similarly, PVRs have significantly contributed to semantic mapping, particularly when incorporating language components, as demonstrated in VL-Fields~\cite{tsagkas2023vlfields} and CLIP-Fields~\cite{clip-fields}.

However, PVRs have yet to gain traction in robot learning frameworks. 
The foundational study by~\cite{chi2023diffusionpolicy} shows that diffusion policies conditioned on R3M~\cite{nair2022rm} features perform worse than those conditioned on visual encoders trained end-to-end. 
Notably, policies trained with PVRs tend to produce jittery actions and are prone to getting stuck, which may be linked to an incorrect assumption of the Markov property. 
Consequently, end-to-end training of visual encoders alongside the policy remains the preferred approach (\eg~\cite{10801826, florence2021implicit}).

%% Other modalities: Mainly the language related papers 
%%% 1. https://arxiv.org/abs/2302.12766
%%% 2. https://arxiv.org/abs/2306.00958
%%% 3. https://arxiv.org/abs/2311.01378
%%% 4. https://arxiv.org/abs/2405.05852 (not really language - SD features)

Finally, an interesting side that is out of the scope of this research concerns the use of other modalities in robot learning. More specifically, a group of recent works has shifted from PVRs that utilise only image data but also language~\cite{li2023vision, gupta2024pre, 10.5555/3618408.3619378, karamcheti2023voltron}. 


\newpage
\section{Temporal Encoding}
\label{app_sub:tenc}
Figs.~\ref{fig:pvr_trajectories_PUSHT} and~\ref{fig:pvr_trajectories_DLR} extend the methodology from Fig.~\ref{fig:pvr_trajectories} to real-world data from~\cite{chi2023diffusionpolicy, zhou2023learning, zhou2023modularity}. As shown, similar issues arise, such as the temporal entanglement of features. While testing our approach on a real robot remains part of our future work, these results provide evidence that the challenges identified in simulation persist in real-world scenarios. Although it is well established that simulations like MetaWorld serve as a reliable proxy for real-world performance~\cite{silwal2024large}, these figures underscore the significance of our findings and the necessity of our proposed solutions.

\subsection{On the quality of simulation data}
\label{app_sub:tenc_sim_v_real}
\begin{figure*}[h]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.9\linewidth]{figs/pca_pusht.pdf} \\
% \includesvg[width=0.9\linewidth]{figs/traj_pusht.svg} \\
\includegraphics[width=0.9\linewidth]{figs/traj_pusht.pdf} \\
\end{tabular}
%\vskip -0.1in
\vspace{-10pt}
\caption{Visualisation of PCA results for the features extracted from frames of an expert demonstration trajectory from \textbf{the real-world} in the Push-T task~\cite{chi2023diffusionpolicy} across the studied PVRs. In this example, we see mostly a cluster forming when the robot has pushed the T block to its outline and makes very small adjustments. }
\label{fig:pvr_trajectories_PUSHT}
\end{center}
%\vskip -0.2in
\vspace{-15pt}
\end{figure*}
\begin{figure*}[h]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.9\linewidth]{figs/pca_dlr.pdf} \\
% \includesvg[width=0.9\linewidth]{figs/traj_dlr.svg} \\
\includegraphics[width=0.9\linewidth]{figs/traj_dlr.pdf} \\
\end{tabular}
%\vskip -0.1in
\vspace{-10pt}
\caption{Visualisation of PCA results for the features extracted from frames of an expert demonstration trajectory from \textbf{the real-world} in the ASU Table~\cite{zhou2023learning, zhou2023modularity} task across the studied PVRs. In this example, entanglement occurs mostly from the fact that the robot picks up the can, rotates it by 90 degrees and then places it in the exact same spot. }
\label{fig:pvr_trajectories_DLR}
\end{center}
%\vskip -0.2in
\vspace{-15pt}
\end{figure*}


\newpage
\subsection{TE Per Task Results}
\label{app_sub:tenc_all_results}
In Table~\ref{tab:tenc_flare}, we provide the IQM success rate of policies trained with each PVR-task pair, using the raw features extracted from the PVR, the features temporally augmented with the FLARE method and finally, the features temporally augmented with TE (ours). The results indicate that incorporating TE significantly improves performance compared to using no temporal information. This is supported by both the Wilcoxon test ($p < 10^{-30}$) and paired t-test ($p < 10^{-26}$). The data is not normally distributed. Additionally, TE also outperforms FLARE, with statistically significant differences observed in both the Wilcoxon test ($p \approx 4.38 \times 10^{-5}$) and paired t-test ($p \approx 1.47 \times 10^{-4}$).

\input{tables/tenc_final_colour.tex}


% \subsection{Temporal Encoding for Representation Disentangling}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one, even using the one-column format.


% \newpage
\subsection{Tuning Temporal Encoding Hyperparameters}
\label{app_sub:hyperparam_te}

% We select 4 PVRs (MAE, DINOv1, SwAV, R3M) and 4 MetaWorld (Bin Picking, Coffee Pull, Disassemble, Pick and Place) tasks to fine-tune the scale and dimensionality of TE. In Table~\ref{tab:boost_perc} we present the percentage of PVR-task pairs that gain performance boost from TE for the different scale, dimensionality hyperparameters. The $D=64$, $s=100$ pair led to the most pairs benefitting. As is apparent from Table~\ref{tab:avg_boost}, it also leads to one of the highest boost in performance (\ie the average gain in PVR-task pairs that benefit). 

We fine-tune TE's scale and dimensionality using 4 PVRs (MAE, DINOv1, SwAV, R3M) and 4 MetaWorld tasks (Bin Picking, Coffee Pull, Disassemble, Pick and Place). Table~\ref{tab:boost_perc} shows the percentage of PVR-task pairs benefiting from TE across different hyperparameters, with $D=64$, $s=100$ yielding the most improvements. As seen in Table~\ref{tab:avg_boost}, this setting also achieves one of the highest average performance gains.

\begin{table}[h!]
\label{tab:boost_perc}
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Dims} & \textbf{10} & \textbf{100} & \textbf{1000} \\
\midrule
64  & 56.25\% & 84.38\% & 75.00\% \\
128 & 81.25\% & 75.00\% & 75.00\% \\
256 & 75.00\% & 81.25\% & 81.25\% \\
\bottomrule
\end{tabular}
\caption{Percentage of PVR-Task pairs were TE led to performance increase. }
\end{table}


\begin{table}[h!]
\label{tab:avg_boost}
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Dims} & \textbf{10} & \textbf{100} & \textbf{1000} \\
\midrule
64  & $11.26 \pm 6.66$ & $11.82 \pm 5.72$ & $9.08 \pm 4.43$ \\
128 & $8.51 \pm 7.61$ & $11.97 \pm 4.79$ & $11.03 \pm 5.39$ \\
256 & $8.89 \pm 4.84$ & $11.38 \pm 6.17$ & $11.82 \pm 6.73$ \\
\bottomrule
\end{tabular}
\caption{Average boost (\ie for the PVR-Task pairs that benefited from TE) values for each scale and dimension combination.}
\end{table}
% \label{app_sub:te_hp}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figs/combined_sinusoidal_embeddings.pdf}
%     \caption{Visualisation of temporal encoding for scale values $\{10^1, 10^2, 10^3\}$ and for dimension values $\{64, 128, 256\}$.}
%     \label{fig:tenc_ablation}
% \end{figure}

\subsection{MetaWorld Temporal Variability of Expert Demos}
\label{app_sub:mw_sync}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/async_demos_proof.pdf}
    \caption{Illustration of temporal variability in Metaworld expert demonstrations. Each column represents a different task, with each row showing a frame captured at the same time-step across three separate demonstrations. The lack of perfect synchronisation between demonstrations highlights variability in task progression.}
    \label{fig:demos_async}
\end{figure}

\newpage
\section{Attentive Feature Aggregation}
\label{app_sub:afa}

\subsection{AFA Per Task Results}
In Fig.~\ref{fig:augmented_feats} we provided the results for each PVR+AFA+TE-task pair, for both in-domain scenes and visually altered scenes. 
\label{app_sub:afa_results}
\begin{figure*}[!h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/bar_afa_vs_pvr_all.pdf}}
% \centerline{\includegraphics[width=\linewidth]{figs/ood_iqm-True-20250113-110008_STACKED.pdf}}
\caption{Evaluation of the AFA module in both in-domain and out-of-domain scenarios, including tabletop texture and lighting perturbations. Sub-figures (i)-(x) illustrate policy performance for individual tasks. Sub-figure (xi) presents the average performance across all tasks, while sub-figure (xii) displays the average OOD performance, with PVRs sorted by descending ABC performance.}
\label{fig:augmented_feats}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Scene Perturbations}
\label{app_sup:scene_pert}
To test the robustness of trained policies we visualise modify the scenes in the evaluation either by changing the lighting or by randomly changing the tabletop texture. Note that all policies are evaluated in the same perturbations for fairness. 

\textbf{Randomizing the scene's lighting properties}: The brightness of the scene is altered by adjusting the diffuse light components, where each colour channel (red, green, blue) is randomly set to a value between 0.3 and 1.0. The specular highlights are similarly randomized, with lower intensity values ranging from 0.1 to 0.5. Additionally, the position of each light source is varied randomly within a 3D space, spanning horizontal and vertical shifts between -2 and 2 units and height adjustments between 0.5 and 3 units. Lastly, the direction of the lights is randomized, allowing for changes in their angular orientation, with each directional component varying between -1 and 1 for horizontal/vertical angles and up to -1 for downward angles.

\textbf{Randomizing the Tabletop's texture}: In Fig.~\ref{fig:textures} we provide the textures that we utilised in our experiments, some of which are borrowed from~\cite{10611331}. Some are visually similar to the texture used in the training demos and others are vibrant, with patterns that hold semantic information that could potentially attract the attention of a PVR. Nevertheless, by observing the evaluation rollouts, policies can fail and succeed in both out-of-distribution cases.     

\label{app_sub:ood}
\begin{figure}[h]
    \centering
    % \includegraphics[width=\linewidth]{figs/combined_sinusoidal_embeddings.pdf}
    % \includegraphics[width=1\textwidth]{example-image}
    \centerline{\includegraphics[width=\linewidth]{figs/all_textures.png}}
    \caption{Visualisation of the different table textures used in the evaluations of Section~\ref{ssec:afa}. The additional textures that are not provided by MetaWorld were borrowed from~\cite{10611331}.}
    \label{fig:textures}
\end{figure}

\subsection{Ablation: Does AFA's success stem from its increased capacity?}
Does AFA's success stem from its increased capacity? We make the policy network deeper (\ie from 594,956 to 1,645,580) to roughly match the number of trainable parameters of the AFA (\ie 1,774,336) by adding more layers. In Fig.~\ref{fig:ablation_deeperMLP} we visualise the results both in and out of domain for policies trained with AFA and with the deeper MLP. As is evident, AFA still has the better robustness performance. 


\label{app_sub:ablation_deeperMLP}
\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=\linewidth]{figs/bar_afa_vs_deep.pdf}}
    \caption{Results from policies trained with either AFA or with a deeper MLP of capacity comparable to that of AFA.}
    \label{fig:ablation_deeperMLP}
\end{figure}

% \newpage
% \subsection{Does ABC require TE?}
% \label{app_sub:te_abc}
\newpage
\subsection{Qualitatively explaining AFA's performance boost}
\label{app_sub:heatmaps}

In Fig.~\ref{fig:more_heatmaps}, we present additional comparisons between the attention heatmaps of the PVRs and their corresponding trained AFAs. These visualizations illustrate how the CLS token attends to different patches compared to the trained query token, offering a general sense of what the models prioritise. While this does not imply that trained AFAs are entirely robust to visual changes in the scene (\eg note that iBOT+AFA still allocates some attention to patches containing the robot's cast shadow), we observe a consistent trend: the attention heatmaps become more focused, particularly on regions relevant to the task.

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=\linewidth]{figs/heatmap_all.pdf}}
    \caption{Additional comparisons between PVR and PVR+AFA attention heatmaps.}
    \label{fig:more_heatmaps}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
