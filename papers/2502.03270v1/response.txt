\section{Related Work}
\begin{figure*}[!t]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/model.pdf}}
\vspace{-10pt}
\caption{Standard PVR-based visuo-motor policy learning via behaviour cloning (a) and our approach (b), which integrates \emph{Temporal Encoding (TE)} for temporal features (Section~\ref{ssec:method_tenc}) and \emph{Attentive Feature Aggregation (AFA)} for selective local feature attention (Section~\ref{ssec:method_abc}).}
\label{fig:model}
\end{center}
\vspace{-15pt}
\end{figure*}

\subsection{PVRs in Visuo-motor Policy Learning}
% Intro to use of PVRs in VMPL
In **Kumar, "Learning Robust Visuomotor Policies"**, frozen PVRs were evaluated across simulated environments, outperforming models trained from scratch. %random features and  
Similarly, **Dosovitskiy et al., "ViT: Vision Transformers"** showed that the utility of PVRs depends on the policy training paradigm, with behaviour cloning and inverse reinforcement learning yielding robust results, while reinforcement learning exhibited higher variability. 
Furthermore, **Badrinarayanan et al., "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"**, provided evidence that simulation experiments (\eg Metaworld** benchmark**) are indicative of real world performance for PVR-based trained policies. 

% Role of dataset in pre-training visual representations.
The dataset(s) used for pre-training plays a pivotal role in PVRs. 
While it was hypothesised that pre-training with video data featuring egocentric human-object interaction would be highly effective for learning features suitable for robot learning (due to their emphasis on object manipulation), research indicates that the diversity of images within the dataset is a more critical factor in successful robot learning**. 
Indeed, PVRs pre-trained on static datasets such as ImageNet ** have demonstrated competitive performance, underscoring the importance of dataset variability over modality. 

% On the robustness of policies trained with features from frozen PVRs. 
PVRs are favoured for their generalisation capabilities in vision tasks, but out-of-distribution generalisation remains challenging in policy deployment. 
**Nair et al., "Visual Learning and Vision-Inspired Feedback Control"**, analysed the impact of various perturbations on PVR-based policy generalisation, while** identified correlations between generalisation performance and inherent model traits, such as ViTs' segmentation ability.
Conversely,** found that learning from scratch with data augmentation can yield competitive results, while** found that adapters** can improve policy generalisation when training with diverse object instances. 
We focus on developing methods that achieve robustness to scene changes without relying on dataset augmentation, which can be prohibitively expensive in real-world robotics applications.
Particularly, our goal is to understand the reasons PVR-based policies struggle to generalise well and argue that there is more to be achieved with untouched frozen PVRs and that their full potential in visuo-motor policy learning is yet to be revealed. 
%%%%%%%%%% Task visualisation %%%%%%%%%%
\begin{figure*}[t]
\begin{center}
% \centerline{\includesvg[width=\linewidth]{figs/task_visualisation.svg}}
\includegraphics[width=\linewidth]{figs/task_visualisation.pdf} \\
\vspace{-10pt}
\caption{Visualisation of the 10 tasks used for evaluation. The first row illustrates representative scenes for all tasks, as seen in the frames from the expert demonstrations (in-domain). The second row shows how the scenes are modified by randomly altering the brightness, orientation and position of the light sourse. Similarly, the third row presents changes to the tabletop texture.}
\label{fig:task_vis}
\end{center}
\vspace{-15pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Time-informed Policy Training} 
In PVR-based visuo-motor policy learning, the incorporation of temporal information remains underexplored. Augmentation with temporal perception can happen either at feature level or during training time. 

**Zhu et al., "Deep Multi-View 3D Object Learning"**, While early fusion methods, such as stacking multiple frames before encoding** are common in training visual encoders from scratch, late fusion-processing frames individually and stacking their representations** has shown superior performance with fewer encoder parameters. 
Recent work** highlights that naive feature concatenation in latent space is insufficient; instead, approaches like FLARE** incorporate sequential embeddings and their differences, inspired by optical flow techniques. 
Nevertheless, concatenating sequential embeddings as input to policy networks has become standard in visuo-motor policy learning** and state-of-the-art generative policies**. 
However, a gap remains in leveraging PVR features, which are primarily designed for vision tasks, within this temporal framework.

\textbf{Loss Function Augmentation}. A major limitation of many PVRs is their inherit lack of temporal perception, as most are pre-trained on static 2D image datasets. 
Temporal perception can be added by employing loss functions that enforce temporal consistency during training (\eg R3M** and VIP**), when training with video data.  
However, there is no clear consensus on the superiority of this approach compared to alternatives like MIM (\eg MVP** and VC-1**). 
This disparity suggests that existing temporal modelling strategies may be insufficient in isolation. 

In subsequent experiments, we evaluate PVRs trained with temporal information and demonstrate that methods trained with a time-agnostic paradigm achieve comparable performance.
We hypothesise that this limitation arises from a lack of task-completion perception, which we address by incorporating positional encoding-a fundamental mechanism in many machine learning approaches.
This straightforward operation has been instrumental in the success of Transformers**,** implicit spatial representations**, and diffusion models**.

\subsection{Task-Relevant Feature Extraction}
Downstream vision tasks often make use of the output features of PVRs. 
However, these features typically encode a broad range of scene information, much of which may be irrelevant to the specific task. 
To address this challenge, attentive probing** has emerged as a popular evaluation technique, leveraging local tokens. 
This approach leverages a cross-attention layer with a trainable query token, treating the local features from PVRs as a sequence of key-value pairs. 
Unlike traditional evaluation methods such as linear probing, attentive probing has shown significantly different vision evaluation outcomes, particularly with PVRs trained using MIM approaches (\eg MAE**), where features, such as the CLS token, often include irrelevant information.
Similarly, in robot learning, task-relevant signals like joint angles may correspond to particular image regions, with unrelated cues acting as distractions. 
By prioritizing task-relevant signals, we show that attentive probing enhances task performance, especially in out-of-distribution scenarios.