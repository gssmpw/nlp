\section{Related Work}
\paragraph{Large Audio Models.} Large-scale self-supervised audio models have been used to learn generalized audio representations from extensive unlabeled datasets. Early successful approaches such as wav2vec____ and  HuBERT____ learned audio representations from scratch, achieving robust performance across many tasks when finetuned. Focused primarily on scaling data and training time, recent efforts such as Whisper____ and OWSM____ have led to extremely effective models both for transcription and speech understanding.

Recent advancements in audio models have integrated learned audio representations with text-based LLMs, enabling native audio understanding while leveraging knowledge and stylistic insights from textual resources. This has led to the emergence of Large Audio Models (LAMs)____. Such models include SpeechGPT ____ which leverages HuBERT ____ for extracting continuous speech as discrete units, LLaMA ____ as the text-LLM foundation, and HiFi-GAN ____ as the unit vocoder; LTU ____ which consists of an audio spectrogram transformer, LLaMA and a Low-rank Adapter; Qwen-Audio series ____ with Whisper-large-v2 and Whisper-large-v3 as the audio encoder and Qwen-7B as the LLM, and many other Large Audio Models ____. In our work, we evaluated nine different LAMs that are publicly available on static benchmarks and tested five best-performing ones in the interactive setting.

\paragraph{Evaluation of Large Audio Models.}
To evaluate the audio processing capability of different models, prior research has constructed a variety of audio benchmarks, targeting particular abilities. For automatic speech recognition, benchmarks such as Librispeech ____ and Commonvoice ____ are widely used, with metrics like word error rate (WER) and character error rate (CER). For speech translation tasks, there are datasets like Covost ____, Covost2 ____, and CVSS ____ with evaluation metrics such as BLEU scores. For emotion detection, benchmarks include MELD ____ and IEMOCAP ____ with speech data labeled with different emotions. In the domain of Speech Question Answering, there are SDQA ____, Social IQ 2.0 ____, and HeySquad ____.

However, one problem regarding the evaluation of LAMs is that they have reported evaluation results on different sets of benchmarks, resulting in inconsistent evaluation and difficulty in comparison ____. Therefore, there are commendable efforts to aggregate audio datasets together to evaluate LAMs in a holistic way such as AIRBench ____, AudioBench ____, and VoiceBench ____. However, they still utilize static reference-based metrics like WER and accuracy. In contrast, we interactively evaluate LAMs using user preferences.

\paragraph{Interactive Evaluation of LLMs}
Interactive evaluation can overcome many limitations in using static datasets to evaluate models. One limitation is model overfitting ____ where models are over-optimized for specific datasets and tasks, limiting their generalization capability. Moreover, static benchmarks may have data contamination ____ issues where LLMs have been trained on the data. Furthermore, static evaluation may lack the ability to incorporate real-world scenarios ____ and align with human preferences ____. Moreover, data drift ____ can happen when the environment generating the data evolves, causing a mismatch between the static datasets and the data in real-world scenarios. Thus, static datasets can fail to keep track of long-term model performance over time. These limitations strongly suggest the need for interactive evaluation of models.

As such, there are many research efforts on creating live NLP benchmarks. For example, DynaBench ____ builds an open platform for dynamic data curation, and Chatbot Arena ____ benchmarks models through chat with LLMs from a larger user base. In a similar line, there are works extending to other modalities and use cases like Wildvision-Arena ____ for vision-language models, Long Code Arena ____ for coding, and Web-Arena ____ for web-related tasks. To our knowledge, there is no similar interactive evaluation of audio-language models to investigate the gap between static benchmarks and user interactions.