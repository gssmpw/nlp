\section{Related Work}
\paragraph{Large Audio Models.} Large-scale self-supervised audio models have been used to learn generalized audio representations from extensive unlabeled datasets. Early successful approaches such as wav2vec~\citep{schneider2019wav2vec} and  HuBERT~\citep{hsu2021hubert} learned audio representations from scratch, achieving robust performance across many tasks when finetuned. Focused primarily on scaling data and training time, recent efforts such as Whisper~\citep{radford2022whisper} and OWSM~\citep{owsm} have led to extremely effective models both for transcription and speech understanding.

Recent advancements in audio models have integrated learned audio representations with text-based LLMs, enabling native audio understanding while leveraging knowledge and stylistic insights from textual resources. This has led to the emergence of Large Audio Models (LAMs)~\citep{latif2023sparks}. Such models include SpeechGPT \citep{zhang2023speechgpt} which leverages HuBERT \citep{hsu2021hubert} for extracting continuous speech as discrete units, LLaMA \citep{touvron2023llama} as the text-LLM foundation, and HiFi-GAN \citep{kong2020hifi} as the unit vocoder; LTU \citep{gong2023listen} which consists of an audio spectrogram transformer, LLaMA and a Low-rank Adapter; Qwen-Audio series \citep{chu2023qwen, chu2024qwen2} with Whisper-large-v2 and Whisper-large-v3 as the audio encoder and Qwen-7B as the LLM, and many other Large Audio Models \citep{borsos2023audiolm, liu2023audioldm, held2024distilling}. In our work, we evaluated nine different LAMs that are publicly available on static benchmarks and tested five best-performing ones in the interactive setting.

\paragraph{Evaluation of Large Audio Models.}
To evaluate the audio processing capability of different models, prior research has constructed a variety of audio benchmarks, targeting particular abilities. For automatic speech recognition, benchmarks such as Librispeech \citep{panayotov2015librispeech} and Commonvoice \citep{ardila2019common} are widely used, with metrics like word error rate (WER) and character error rate (CER). For speech translation tasks, there are datasets like Covost \citep{wang2020covost}, Covost2 \citep{wang2021covost}, and CVSS \citep{jia2022cvss} with evaluation metrics such as BLEU scores. For emotion detection, benchmarks include MELD \citep{poria2018meld} and IEMOCAP \citep{busso2008iemocap} with speech data labeled with different emotions. In the domain of Speech Question Answering, there are SDQA \citep{faisal2021sd}, Social IQ 2.0 \citep{siq2}, and HeySquad \citep{wu2023heysquad}.

However, one problem regarding the evaluation of LAMs is that they have reported evaluation results on different sets of benchmarks, resulting in inconsistent evaluation and difficulty in comparison \citep{wang2024audiobench}. Therefore, there are commendable efforts to aggregate audio datasets together to evaluate LAMs in a holistic way such as AIRBench \citep{yang2024air}, AudioBench \citep{wang2024audiobench}, and VoiceBench \citep{chen2024voicebench}. However, they still utilize static reference-based metrics like WER and accuracy. In contrast, we interactively evaluate LAMs using user preferences.

\paragraph{Interactive Evaluation of LLMs}
Interactive evaluation can overcome many limitations in using static datasets to evaluate models. One limitation is model overfitting \citep{ying2019overview} where models are over-optimized for specific datasets and tasks, limiting their generalization capability. Moreover, static benchmarks may have data contamination \citep{magar2022data} issues where LLMs have been trained on the data. Furthermore, static evaluation may lack the ability to incorporate real-world scenarios \citep{lin2024wildbench} and align with human preferences \citep{oren2023proving}. Moreover, data drift \citep{mallick2022matchmaker} can happen when the environment generating the data evolves, causing a mismatch between the static datasets and the data in real-world scenarios. Thus, static datasets can fail to keep track of long-term model performance over time. These limitations strongly suggest the need for interactive evaluation of models.

As such, there are many research efforts on creating live NLP benchmarks. For example, DynaBench \citep{kiela2021dynabench} builds an open platform for dynamic data curation, and Chatbot Arena \citep{chiang2024chatbot} benchmarks models through chat with LLMs from a larger user base. In a similar line, there are works extending to other modalities and use cases like Wildvision-Arena \citep{lu2024wildvision} for vision-language models, Long Code Arena \citep{bogomolov2024long} for coding, and Web-Arena \citep{zhou2023webarena} for web-related tasks. To our knowledge, there is no similar interactive evaluation of audio-language models to investigate the gap between static benchmarks and user interactions.