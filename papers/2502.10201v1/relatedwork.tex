\section{Related Work}
\citet{radovanovic2010hubs} showed the ubiquity of hubs in many different kinds of datasets. 
Hubness is a cause of concern, as it can negatively impact many common tasks in data analysis and machine learning, such as regression, classification, outlier detection and clustering. Hubness was also shown to hinder the performance of nearest-neighbour algorithms in speech recognition, recommendation and multimedia retrieval (see \citet{feldbauer2019comprehensive} and references therein). Problematic hubness also occurs in distributed text representations analogous to those produced by a LLM. For example \citet{Dinu2014ImprovingZL}, \citet{smith2017offline}, \citet{lample2018word}, \citet{huang2020improving} and \citet{pmlr-v233-nielsen24a} studied hubness in word and text embeddings, while \citet{bogolin2022cross}, \citet{wang-etal-2023-balance} and \citet{chowdhury-etal-2024-nearest} looked at hubness in multimodal language models and cross-modal retrieval. 

Given the problems posed by hubs, various hubness reduction methods have been proposed, for example Local Scaling \citep{zelnik2004self}, Mutual Proximity \citep{schnitzer2012local}, Globally Corrected Rank \citep{Dinu2014ImprovingZL}, Inverted Softmax \citep{smith2017offline}, Cross-domain Similarity Local Scaling \citep{lample2018word}, Hubness Nearest Neighbor Search \citep{huang2020improving}, Querybank Normalisation \citep{bogolin2022cross}, DBNorm \citep{wang-etal-2023-balance}, Dual Inverted Softmax \citep{wang-etal-2023-balance}, F-norm \citep{pmlr-v233-nielsen24a} and Nearest Neighbor Normalization \citep{chowdhury-etal-2024-nearest}. These methods have been systematically compared by \citet{feldbauer2019comprehensive} and \citet{pmlr-v233-nielsen24a}, among others. 

As shown by the plethora of hubness reduction techniques, the focus has so far been on mitigating hubness, with little attention devoted to the question of whether hubness is actually always a nuisance phenomenon to be mitigated.