\section{Related Work}
\label{sec:relwork}

\paragraph{Learning-based methods for robotic grasping}


Robotic grasping describes the entire process from perception via action execution to holding the target object stably in hand for further manipulation under certain constraints.
This problem is extensively studied, especially for industrial pick-and-place scenarios \cite{kleeberger2020survey,xie2023learning}. 
Large datasets have been created and annotated using real sensor data or simulated settings, enabling the supervised learning of static grasping configurations \cite{fang2020graspnet,dai2023graspnerf}. The learned models are accurate, efficient, and generalize well to novel objects based on vision input alone. However, their output is usually a single selected grasping pose, requiring additional path planning to execute the grasp. Reacting to perturbations or failures due to perception errors is challenging in this setup. Multiple deep reinforcement learning-based grasping methods have been proposed \cite{kalashnikov2018scalable,quillen2018deep,pavlichenko2023deep,koenig2022role} to overcome this problem using reactive policies. These methods mostly use
visual perception as sensing input to provide feedback for failure correction. In recent works, tactile sensing has been introduced to RL-based grasping pipelines to reduce the gap between visual perception and the state of the real environment. Sim-to-real manipulation tasks have utilized raw binary contacts for better transfer \cite{ding2021sim}. Binary tactile grids and proprioceptive information have been used for grasping adjustment on a 3-finger manipulation platform \cite{wu2020mat}. Vulin et al.\ use intrinsic rewards based on tactile sensing that helps exploration in RL settings \cite{vulin2021improved}. Fusing force-torque sensing and visual input using transformers has also been shown to improve grasping performance \cite{chen2022visuo}. However, the literature lacks a detailed comparison of the influence of different tactile modalities in the context of 2-finger reach-and-grasp tasks solved with reinforcement learning, which we present in this paper.

\paragraph{Tactile sensors for robotics}
Tactile sensors with different modalities have been developed for robotics, especially for providing force feedback \cite{li2020review}. Sensors like Minsight \cite{Andrussow23-AIS-Minsight}, Insight\cite{Insight}, Piacenza et al. \cite{piacenza2020disco} or DenseTact 2.0 \cite{wo2023densetact20} provide high-resolution all-around tactile perception with a sensing frequency up to 60 Hz. BioTac \cite{Lin2013EstimatingPO},  GelSight \cite{yuan2017gelsight}, and GelSlim \cite{taylor2022gelslim} provide
 tactile information on a flat or curved surface, which can be used to extract local textures, object orientations \cite{takahashi2024stable} and force information \cite{dong2021high}. Discretized sensor arrays, like USkin \cite{funabashi2020stable} and binary taxel arrays \cite{yang2023tacgnn}, are used to provide low-resolution tactile information. Force-torque sensors are also used for simple force feedback, e.g.\ in \cite{chen2022visuo} and \cite{pitz2023dextrous}. A comparison across sensing area and measured quantities can be found in \cref{fig:tactile-sensor-classification}. In this paper, we create a prototypical simulation to abstract away the specific physical properties of tactile sensors to focus on their high-level differences in sensing area, sensing resolution and representation of the measured forces. For our real-robot experiment, we use the vision-based tactile sensor Minsight~\cite{Andrussow23-AIS-Minsight}, as a representative tool that can deliver different modes of force-based tactile information.
 
                                     
\paragraph{Evaluation of tactile sensing for RL-based grasping}
Multiple force stability criteria are compared by Koenig et al.~\cite{koenig2022role} for multi-finger re-grasping setups using PPO \cite{schulman2017proximal}. However, only a single force per finger link has been presented with 3 data representations. Clustered and full tactile sensing for in-hand manipulation tasks are compared by Melnik et al.\cite{melnik2019tactile} on the Shadow Dexterous Hand. Three visual-tactile encoders are compared by Chen et al. \cite{chen2022visuo} for grasping and manipulation tasks in a model-based RL setup with force-torque-based tactile sensing. The existing works mostly focus on multi-finger setups and do not provide a systematic comparison of tactile sensing areas and sensed quantities. 
Our work closes this gap by conducting well-designed experiments to assess the importance of the sensing area, the quantities detected, the learning architectures and the memory length for robust and generalizable grasping.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%