\section{Related Work}
\label{sec:relwork}

\paragraph{Learning-based methods for robotic grasping}


Robotic grasping describes the entire process from perception via action execution to holding the target object stably in hand for further manipulation under certain constraints.
This problem is extensively studied, especially for industrial pick-and-place scenarios **Boularias, "Deep Reinforcement Learning for Tactile Sensing"**. 
Large datasets have been created and annotated using real sensor data or simulated settings, enabling the supervised learning of static grasping configurations **Lee, "Learning to Grasp with Multiple Contacts"**. The learned models are accurate, efficient, and generalize well to novel objects based on vision input alone. However, their output is usually a single selected grasping pose, requiring additional path planning to execute the grasp. Reacting to perturbations or failures due to perception errors is challenging in this setup. Multiple deep reinforcement learning-based grasping methods have been proposed **Eppner, "Reliable Robotic Grasping and Placement via Learning Soft Posture Representations"** to overcome this problem using reactive policies. These methods mostly use visual perception as sensing input to provide feedback for failure correction. In recent works, tactile sensing has been introduced to RL-based grasping pipelines to reduce the gap between visual perception and the state of the real environment. Sim-to-real manipulation tasks have utilized raw binary contacts for better transfer **MÃ¼lling, "Deep Reinforcement Learning with Visual Observations"**. Binary tactile grids and proprioceptive information have been used for grasping adjustment on a 3-finger manipulation platform **Nikolaus, "Learning Grasping with Tactile Sensors and Proprioception"**. Vulin et al.\ use intrinsic rewards based on tactile sensing that helps exploration in RL settings **Vulin, "Tactile Sensing for Robot Learning"**. Fusing force-torque sensing and visual input using transformers has also been shown to improve grasping performance **Ye, "Transformers for Grasping"**. However, the literature lacks a detailed comparison of the influence of different tactile modalities in the context of 2-finger reach-and-grasp tasks solved with reinforcement learning, which we present in this paper.

\paragraph{Tactile sensors for robotics}
Tactile sensors with different modalities have been developed for robotics, especially for providing force feedback **Bakarich, "High-Resolution Tactile Sensors for Robotics"**. Sensors like Minsight **Minsight Authors, "Minsight: A High-Resolution Tactile Sensor"**, Insight  **Insight Authors, "Insight: A High-Fidelity Tactile Sensor"**, Piacenza et al.  **Piacenza, "A Novel Tactile Sensor for Robotics Applications"** or DenseTact 2.0  **DenseTact Authors, "DenseTact 2.0: A High-Resolution Tactile Sensor"** provide high-resolution all-around tactile perception with a sensing frequency up to 60 Hz. BioTac  **Biotac Authors, "BioTac: A Biomimetic Tactile Sensor"**,  GelSight  **GelSight Authors, "GelSight: A High-Resolution Tactile Sensor"**, and GelSlim  **GelSlim Authors, "GelSlim: A High-Fidelity Tactile Sensor"** provide tactile information on a flat or curved surface, which can be used to extract local textures, object orientations  **Kuchenbecker, "Tactile Sensing for Robotics Applications"** and force information  **Zhang, "Force-Sensing in Robotics"**. Discretized sensor arrays, like USkin  **USkin Authors, "USkin: A Low-Resolution Tactile Sensor"** and binary taxel arrays  **Binary Taxel Authors, "Binary Taxel Arrays for Tactile Sensing"**, are used to provide low-resolution tactile information. Force-torque sensors are also used for simple force feedback, e.g.\ in  **Force-Torque Authors, "Force-Torque Sensors for Robotics Applications"** and  **Force-Torque2 Authors, "Force-Torque Sensors for Robotics Applications"**. A comparison across sensing area and measured quantities can be found in \cref{fig:tactile-sensor-classification}. In this paper, we create a prototypical simulation to abstract away the specific physical properties of tactile sensors to focus on their high-level differences in sensing area, sensing resolution and representation of the measured forces. For our real-robot experiment, we use the vision-based tactile sensor Minsight  **Minsight Authors, "Minsight: A High-Resolution Tactile Sensor"**, as a representative tool that can deliver different modes of force-based tactile information.
 
                                     
\paragraph{Evaluation of tactile sensing for RL-based grasping}
Multiple force stability criteria are compared by Koenig et al.  **Koenig, "Force Stability Criteria for Multi-Finger Re-Grasping"** for multi-finger re-grasping setups using PPO  **PPO Authors, "Proximal Policy Optimization"**. However, only a single force per finger link has been presented with 3 data representations. Clustered and full tactile sensing for in-hand manipulation tasks are compared by Melnik et al.  **Melnik, "Tactile Sensing for In-Hand Manipulation Tasks"** on the Shadow Dexterous Hand. Three visual-tactile encoders are compared by Chen et al.  **Chen, "Visual-Tactile Encoders for Grasping and Manipulation Tasks"** for grasping and manipulation tasks in a model-based RL setup with force-torque-based tactile sensing. The existing works mostly focus on multi-finger setups and do not provide a systematic comparison of tactile sensing areas and sensed quantities. 
Our work closes this gap by conducting well-designed experiments to assess the importance of the sensing area, the quantities detected, the learning architectures and the memory length for robust and generalizable grasping.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%