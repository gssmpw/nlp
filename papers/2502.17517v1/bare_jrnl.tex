
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/
%\usepackage{indentfirst}




\setlength{\parindent}{2em}
\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
\usepackage{multirow}
\usepackage{subfigure}
%\usepackage{multicolumn}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}%解决公式在表格中位置太小的问题
\usepackage{booktabs}%解决表格横向粗细问题
\usepackage{autobreak}
%\usepackage{upgreek}
\setcellgapes{3pt}

\setcellgapes{3pt}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[justification=centering]{caption}
\usepackage{color}

%\setlength{\abovedisplayskip}{0pt}
%\setlength{\belowdisplayskip}{0pt}
%\setlength{\abovedisplayshortskip}{0pt}
%\setlength{\belowdisplayshortskip}{0pt}



% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
\usepackage{verbatim}

% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
%\cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
%\usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
%\usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.(1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Attention-based UAV Trajectory Optimization for Wireless Power Transfer-assisted IoT Systems}
\author{Li Dong, Feibo Jiang, \textit{Senior Member, IEEE}, Yubo Peng.

\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant 41904127, Grant 41604117 and Grant 62132004, in part by the Hunan Provincial Natural Science Foundation of China under Grant 2024JJ5270, in part by the Open Project of Xiangjiang Laboratory under Grant 22XJ03011, Grant XJ2023001 and Grant XJ2022001, in part by the Scientific Research Fund of Hunan Provincial Education Department under Grant 22B0663, in part	by the Changsha Natural Science Foundation under Grant kq2402098, and Grant kq2402162, and in part by Qiyuan Lab Innovation Fund under Grant 2022-JCJQ-LA-001-088. (Corresponding author: Feibo Jiang)}
\thanks{Li Dong is with the School of Computer Science, Hunan University of Technology and Business, Changsha 410205, China, and also with the Xiangjiang Laboratory, Changsha 410205, China (e-mail: Dlj2017@hunnu.edu.cn).}
\thanks{Feibo Jiang is with the Hunan Provincial Key Laboratory of Intelligent
	Computing and Language Information Processing, Hunan Normal
	University, Changsha 410081, China (e-mail: jiangfb@hunnu.edu.cn).}
\thanks{Yubo Peng is with the School of Intelligent Software and Engineering,
Nanjing University, Suzhou 215163, China (e-mail: pengyubo@
	hunnu.edu.cn).}% <-this % stops a space

%\thanks{Kezhi Wang (kezhi.wang@northumbria.ac.uk) is with the department of Computer and Information Sciences, Northumbria University, UK}

}
%\author{Feibo Jiang, Junyan Liu
%	\thanks{
%		dadadada
%		}
%%	\thanks{The paper was submitted on \today.}\\
%}


%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%


%\author{Li Dong, Feibo Jiang, Dian He.}% <-this % stops a space
\begin{comment}
	content...	
\thanks{Li Dong (Dlj2017@hunnu.edu.cn) is with Key Laboratory of Hunan Province for New Retail Virtual Reality Technology, Hunan University of Commerce, Changsha, China.}
\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant nos. 41604117, 41904127 and 41874148.}
\thanks{Feibo Jiang (jiangfb@hunnu.edu.cn) is with Hunan Provincial Key Laboratory of Intelligent Computing and Language Information Processing, Hunan Normal University, Changsha, China.}% <-this % stops a space
\thanks{Junyan Liu (202070291644@hunnu.edu.cn) is with School of Information Science and Engineering, Hunan Normal University, Changsha, China.}% <-this % stops a space

\thanks{Kezhi Wang (kezhi.wang@northumbria.ac.uk) is with the department of Computer and Information Sciences, Northumbria University, UK}
}
\end{comment}
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Submitted for Review}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
\textcolor{black}{Unmanned Aerial Vehicles (UAVs) in Wireless Power Transfer (WPT)-assisted Internet of Things (IoT) systems face the following challenges: limited resources and suboptimal trajectory planning. Reinforcement learning-based trajectory planning schemes face issues of low search efficiency and learning instability when optimizing large-scale systems. To address these issues, we present an Attention-based UAV Trajectory Optimization (AUTO) framework based on the graph transformer, which consists of an Attention Trajectory Optimization Model (ATOM) and a Trajectory lEarNing Method based on Actor-critic (TENMA). In ATOM, a graph encoder is used to calculate the self-attention characteristics of all IoTDs, and a trajectory decoder is developed to optimize the number and trajectories of UAVs. TENMA then trains the ATOM using an improved Actor-Critic method, in which the real reward of the system is applied as the baseline to reduce variances in the critic network. This method is suitable for high-quality and large-scale multi-UAV trajectory planning. Finally, we develop numerous experiments, including a hardware experiment in the field case, to verify the feasibility and efficiency of the AUTO framework.}



%recent years, small Internet of Things (IoT) IoTDs, like sensors or detectors, become popular monitors for the environment because of their high portability and low energy consumption. However, most small IoT sensors are weak in long-distance communication. Thus, high-mobility Unmanned Aerial Vehicles (UAVs) are expected to be powerful data collection tools. Hence, UAV-assisted data collection system for IoTDs is promising in recent years. However, UAV-assisted data collection system is challenging since (1) UAVs are limited in battery and data storage capacity, and (2) it is difficult to optimize the number of UAVs and trajectories to minimize the system energy cost and meet all the constraints. In this paper, we proposed a Self-Attention Energy Consumption Optimization (UFO) framework for the UAV-assisted data collection system and overcome the challenges mentioned above. UFO contains a Self-Attention Trajectory Planning (STP) model to get the trajectories of UAVs. STP model is divided into two step. First, get the self-attention features of IoT sensors. Second, optimize the trajectories of all UAVs with limited battery and data storage capacity. In order to enhance the performance and generalization ability of STP model, Deep Reinforcement Learning (DRL) is utilized to train STP model. Finally, 
\end{abstract}

\begin{IEEEkeywords}
Unmanned Aerial Vehicle, Wireless Power Transfer, Trajectory Planning, Attention, Actor-critic
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.



With the advancement of 5G, the Internet of Things (IoT) has become widely used in a variety of fields, including environmental monitoring, healthcare, and industry 4.0, among others. However, due to limited transmitting power and battery capacity, Internet of Things Devices (IoTDs) perform poorly in long-distance communication. Furthermore, when IoTDs are positioned in remote places with limited wireless coverage and battery power, charging IoTDs and transferring sensory data from the IoTDs to the remote data center are challenging tasks.

\textcolor{black}{Fortunately, Unmanned Aerial Vehicles (UAVs) and Wireless Power Transfer (WPT) can be integrated into these IoT systems, enabling UAVs to wirelessly transfer power to IoTDs \cite{messaoudi2023survey}. Before collecting the sensory data of an IoTD, the UAV needs to charge the IoTD so that the IoTD has enough energy to transfer its sensory data to the UAV. 
Thus, UAVs are deployed in the IoT system as mobile chargers to transfer power to IoTDs and as the data collectors for sensory data collection. 
Compared to terrestrial data collection systems, UAVs can more efficiently cover large areas and fly close to IoTDs for data collection and energy transfer. This reduces data transmission latency, alleviates communication burdens, and enhances the efficiency of WPT
\cite{bouzid20235g}.}
Hence, mobility management plays a key role for UAVs in WPT-assisted IoT systems, and the shoddy design of UAV trajectory will lead to not only the waste of energy but also service delays. Efficient mobility management is demanded in trajectory design, especially considering a swarm of UAVs \cite{dong2024deep,10726905,jiang2024large}. 


There are many previous trajectory design algorithms have been proposed. 
\textcolor{black}{Messaoudi et al. \cite{messaoudi2024ugv} proposed a collaborative system based on UAVs and Unmanned Ground Vehicles (UGVs) for the data collection of IoT devices. The trajectory control of UGVs and UAVs was optimized using a multi-agent Reinforcement Learning (RL) approach. 
Lu et al. \cite{lu2021covertness} introduced a WPT system utilizing UAVs, which first wirelessly charge energy-constrained IoT devices and then enable these devices to opportunistically send collected data to the UAV. 
Oubbati et al. \cite{oubbati2022synchronizing} proposed a multi-agent deep RL method called TEAM to optimize UAV trajectories and resource allocation while minimizing UAV energy consumption. Zhu et al. \cite{zhu2022uav} introduced a machine learning algorithm based on Transformer and Weighted A*, named TWA, to address UAV trajectory optimization in UAV-aided IoT networks. 
Zhu et al. \cite{10224843} proposed an Attention-Reinforced Learning Scheme for optimizing the trajectory of UAVs in large-scale and low-power data collection tasks.}




\begin{table}[]
	\caption{Comparison with previous works}
	\centering
	\setlength{\tabcolsep}{5mm}
	\renewcommand\arraystretch{1.25}
	\begin{tabular}{|c|c|c|c|c|}
	\hline
Work & UAV & WPT & Attention & RL \\
	\hline
\cite{messaoudi2024ugv} & $\checkmark$ & $\checkmark$ &  & $\checkmark$ \\
	\hline
	\cite{lu2021covertness} & $\checkmark$ & $\checkmark$ &   &   \\
	\hline
	\cite{oubbati2022synchronizing} & $\checkmark$ & $\checkmark$ &   & $\checkmark$ \\
	\hline
	\cite{zhu2022uav} & $\checkmark$ &   & $\checkmark$ &   \\
	\hline
	\cite{10224843} & $\checkmark$ &   & $\checkmark$ & $\checkmark$ \\
	\hline
	AUTO & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
	\hline
\end{tabular}
	\label{tab:relatedwork}
\end{table}


Although high mobility is an impressive feature of UAVs, the battery and data storage capacity of UAVs always limits the data collection tasks for all IoTDs in the system. Therefore, joint resource and mobility management is essential to UAV swarms for the WPT-assisted IoT system \cite{jiang2020ai,jiang2021distributed,10638533}. 
In this paper, we propose a novel Attention-based UAV Trajectory Optimization (AUTO) framework, in which an Attention Trajectory Optimization Model (ATOM) is used to optimize the number and trajectories of UAVs, and a Trajectory lEarNing Method based on Actor-critic (TENMA) is applied to train the ATOM model. 
%\textcolor[rgb]{0,0,0}{Then, we need the performance of RSTP baseline is close to that of STP model so that the reward variance is small. Therefore, the Kullback-Leibler (KL) divergence\cite{2003KL} is utilized to determine whether the performance gap between STP model and RSTP baseline is small enough. }


\textcolor{black}{In Table \ref{tab:relatedwork}, we compare our AUTO framework with existing works, focusing on UAV, WPT, attention mechanisms, and RL. It is evident that most of the listed works consider the system from only two or three perspectives. However, the aforementioned studies do not specifically investigate the potential of integrating the attention and RL in UAV and WPT-assisted IoT systems.
Hence, unlike existing works, the contributions of the study are summarized as follows: }
\begin{enumerate}

	\item 
\textcolor{black}{	\emph{High-precision graph encoding:} In ATOM, the IoT system is mathematically expressed as a graph structure and a graph encoder is presented to extract the self-attention features of all IoTDs precisely. The novel learnable graph embedding layer and graph pooling layer are introduced to the graph encoder for enhancing self-attention features and guiding the trajectory planning.}
	
	

	\item
\textcolor{black}{	\emph{High-quality trajectory decoding:} The trajectory decoder followed by the graph encoder in ATOM is utilized to generate trajectories of UAVs. We decode the self-attention features of each IoTD and the whole graph by the proposed alignment vector and context vector, and the quantity and trajectories of UAVs are optimized according to self-attention features and remaining battery and storage capacity.}
	
	
	\item 
\textcolor{black}{	 \emph{Efficient and stable Actor-Critic learning:} The ATOM model is trained by TENMA. Specifically, a critic network is applied to evaluate the generated trajectories, and the ATOM model is introduced as the actor network to produce the trajectories. Moreover, the real reward of the system is used as the baseline to reduce the variance of the critic network and enhance the stabilization and generalization of TENMA.}
	 
	

\end{enumerate}

This paper is organized as follows: %Section \ref{sec:related} reviews some related works. 
Section \ref{sec:system_model} describes the system model and problem formulation of the WPT-assisted IoT system. 
%section \ref{sec:pre} introduces some preliminaries, 
Section \ref{sec:UFO} describes the principle of the AUTO framework in details. %Section %\ref{sec:converge} analyzes the convergence of the TACL method and the time complexity of the ATOM model.
Section \ref{sec:experiments} illustrates the experiment results. Finally, Section \ref{sec:conclusion} concludes the whole paper. 


\section{System Model and Problem Formulation }
\label{sec:system_model}

\begin{figure}[htbp]
\centering
\includegraphics[width=9cm]{1.png}
\caption{The WPT-assisted IoT system.}
\label{fig:scenario}
\end{figure}

As illustrated in Fig. \ref{fig:scenario}, the WPT-assisted IoT system consists of $N$ IoTDs, a data center, and $m$ UAVs with half-duplex access points, which can transmit power to the IoTDs and collect data from IoTDs by Time Division Duplexing (TDD) mode.
The $N$ IoTDs are denoted as a set of $\mathcal{N}=\left\{ 1,2,...,N \right\}$. We assumed that the position ($x_i, y_i$) of the $i$-th IoTD is fixed and known, and the $i$-th IoTD has $D_{i}$ data to be collected. 
% Thus, each IoTD contains three pieces of information: abscissa $x$, ordinate $y$ and data transmission amount $d$, hence the $i$-th IoTD is denoted as $\textbf{X}_i=[\textbf{X}_i, y_i, D_{i}]$. 
%UFO can optimal the number of UAVs for the data collection. 
The set of $m$ UAVs is denoted as $\mathcal{M}=\left\{ 1,2,...,m \right\}$, and each UAV has limited data storage capacity $C_{\max}$ and energy capacity $E_{\max}$. The flight height of the UAV is set to $H^F$. Each UAV can only collect data from one IoTD at one time, so the association $a_{ij}$ at the $t$-th time step can be expressed by:
\begin{equation}
\label{eq:c_uav_collect}
a_{ij}[t]=\left\{0,1\right\}, \forall i\in\mathcal{N}, \forall j\in\mathcal{M}
\end{equation}
where $a_{ij}[t]=1$ means the $j$-th UAV is collecting the data from the $i$-th IoTD at the $t$-th time step, and $a_{ij}[t]=0$ otherwise. 


\subsection{Trajectory Model}

In the proposed system, each UAV flies straightly from
one IoTD to another. The $j$-th UAV takes off from the
data center at a fixed location $r_j[0]=(0,0,H)$, and flies to the IoTD one by one, and hovers over each IoTD to collect data. The $j$-th
UAV completes the data collection task according to a predetermined flight trajectory and returns to the same data center after one flying cycle. Hence, we have
\begin{equation}
\label{eq:c_return}
r_j[s_j]=r_j[0],\forall j \in\mathcal{M}
\end{equation}
where $r_j[t]$ denotes the $t$-th hover point on the flight trajectory of the $j$-th UAV, $t \in\mathcal{T}_j=\left\{ 1,2,...,s_j \right\}$, and the $j$-th UAV serves total $s_j$ IoTDs. Hence, the $j$-th UAV has $s_j$ hover points. 
Assume that the UAV serves the $i$-th IoTD only once, then one has 
\begin{equation}
\label{eq:c_iot_transfer}
\sum_{j=1}^{m}\sum_{t=1}^{s_j} a_{ij}[t]=1, \forall i \in\mathcal{N}.
\end{equation}

Since each UAV can fly from one hover point to another point in a straight line, then the flight time of the $j$-th UAV can be expressed by:
\begin{equation}
\label{eq:uav_flight_time}
T_{j}^{F}=\sum_{t=1}^{s_j}\frac{\left\|r_j[t]-r_j[t-1]\right\|_2}{v},  \forall j \in \mathcal{M}
\end{equation}
where $\left\|r_j[t+1]-r_j[t]\right\|_2$ is the Euclidean distance between hover points $r_j[t+1]$ and $r_j[t]$, and $v$ is the flight velocity of the UAV, which is a constant value. 




\subsection{Data Collection Model}

We assume that IoTDs can be wirelessly charged by the UAV before transmitting the data to the UAV. The whole process can be divided into the WPT stage and data transmission stage. 


In the WPT stage, the UAV can transmit the energy wirelessly via Radio Frequency (RF) technologies with a fixed transmit power $P^{T}$. The power received at the $i$-th IoTD from the $j$-th UAV is denoted as $P^{R}_{ij}$, which can be calculated by
\begin{equation}
\label{eq:eh_receive}
P^{R}_{ij}=|g^{D}_{ij}|^2 P^{T}  % PT=0.5W,kappa=2,k0=-50dB,sigma2=-110dB,Pmax=24mW,a=4,b=0.001,B=1Mhz
\end{equation}
where $|g^{D}_{ij}|^2$ denotes the downlink power gain from the $j$-th UAV to the $i$-th IoTD.
%, which can be calculated by

%\begin{equation}
%\label{eq:jiang2}
 %|g^{D}_{ij}|^2=\frac{\kappa_{0}}{{H^{F}}^{\kappa}}
%\end{equation}
%where $\kappa_{0}$ denotes the reference signal power gain at the distance of 1m from the IoTD, and $\kappa$ denotes the path loss factor\cite{2019EH}. 
Assume that $\eta^{L}$ is the constant attenuation parameter in the linear energy harvesting model. 
%$P_{\max}^{D}$ denotes the maximum output direct current power. 
%$a$ and $b$ represent some properties of the energy harvest system such as the resistance, the capacitance and the circuit sensitivity\textcolor[rgb]{0,0,0}{\cite{2018non-linear-EH}}. %and we set $a=4$ and $b= 0.001$. 
Then, the received energy of the $i$-th IoTD from the $j$-th UAV can be given by
%\begin{equation}
%\label{eq:non_linear_eh_model}
%\phi(P^{R}_{ij})=\frac{P^{D}_{\max}e^{a\cdot b}-P^{D}_{\max}e^{-a\cdot (P^{R}_{ij}-b)}}{e^{a\cdot b}(1+e^{-a\cdot (P^{R}_{ij}-b)})}
%\end{equation}
\begin{equation}
\label{eq:linear_eh_model}
E^{R}_{ij}=\eta^{L}P^{R}_{ij} T_{ij}^{E}
\end{equation}
where $T_{ij}^{E}$ is the energy harvesting time of the $i$-th IoTD from the $j$-th UAV.
%$\eta^{L}$ is the attenuation parameter in linear model, $\eta^{L}\in[0,1)$ and is a constant. 
%$P_{\max}^{D}$ denotes the maximum output direct current power. 
%$a$ and $b$ represent some properties of the energy harvest system such as the resistance, the capacitance and the circuit sensitivity\textcolor[rgb]{0,0,0}{\cite{2018non-linear-EH}}. %and we set $a=4$ and $b= 0.001$. 

In the data transmission stage, the uploading data rate of the  $i$-th IoTD to the $j$-th UAV can be given by
\begin{equation}
\label{eq:jiang3}
R_{ij}=B\log_2(1+\frac{|g^{U}_{ij}|^2 E^{R}_{ij}}{\sigma^2 T^{C}_{ij}})
\end{equation}
where $|g^{U}_{ij}|^2=|g^{D}_{ij}|^2$ denotes the uplink power gain, $B$ is the bandwidth, $\sigma^2$ is Gaussian white noise power, and $T^{C}_{ij}$ is the data collection time from $i$-th IoTD to the $j$-th UAV. %Assuming that the IoTD also has a battery that can store the energy.


To ensure that IoTDs can successfully upload their data $D_{i}$ to the UAVs, one has
\begin{equation}
\label{eq:c_eh}
T^{C}_{ij}B\log_2(1+\frac{|g^{U}_{ij}|^2 E^{R}_{ij}}{\sigma^2 T^{C}_{ij}}) \geq D_{i}, \forall j\in \mathcal{M}.
\end{equation}

%\textcolor[rgb]{0,0,0}{When hovering, the UAV needs to charge the IoTD and collect the sensory data from the IoTD. Thus, the hover time of the $j$-th UAV is calculated as:}
%
%\begin{equation}
%\label{eq:uav_hover_time}
%T_j^H=\sum_{i=1}^N \sum_{t=1}^{s_j} a_{ij}[t](T^{E}_{ij}+T^{C}_{ij}), j\in \mathcal{M}
%\end{equation}


\subsection{Energy Consumption Model}

Assuming that the flight energy consumption of the $j$-th UAV is given as
\begin{equation}
\label{eq:uav_flight_energy}
E_j^F=P^FT_j^F
\end{equation}
where $P^F$ is the flight power of the UAV. 
We also assume that the power consumption is $P^{H}$ when the $j$-th UAV hovers above the IoTD, then one has
\begin{equation}
 \label{eq:jiang4}
% E_j^T={P^{T}}^{'} T^{T}_j={P^{T}}^{'} \sum_{i=1}^N \sum_{t=1}^{s_j} a_{ij}[t]T^{E}_{ij}, j\in \mathcal{M}
E_j^T=({P^{T}}+P^{H}) T^{T}_j
\end{equation}
where  $T^{T}_j$ is the power transfer time of the $j$-th UAV. 
Next, when the $j$-th UAV hovers, the energy consumption of data collection and UAV hovering is calculated as
\begin{equation}
\label{eq:uav_hover_energy}
%E_{j}^{C}={P^{C}}^{'} T_j^H={P^{C}}^{'} \sum_{i=1}^N \sum_{t=1}^{s_j} a_{ij}[t]T^{C}_{ij}, j\in \mathcal{M}
E_{j}^{C}=(P^{C}+P^{H}) T^{C}_j
\end{equation}
where $T_j^{C}$ is the data collection time of the $j$-th UAV. $P^{C}$ is the data collection power of the UAV.
Therefore, the total energy consumption of the $j$-th UAV can be given as
\begin{equation}
\label{eq:total_energy}
E_j=E_j^F+E_{j}^{C}+E_j^T, \forall j\in\mathcal{M}.
\end{equation}

Due to the limited data storage capacity and energy capacity of UAVs, it is required that the total energy consumption of the UAV does not exceed its energy capacity $E_{\max}$ and all collected data does not exceed the storage capacity $C_{\max}$. Therefore, these inequalities need to be satisfied
\begin{equation}
\label{eq:c_battery}
E_j\leq E_{\max}, \forall j\in\mathcal{M},
\end{equation}
\begin{equation}
\label{eq:c_capacity}
\sum_{i=1}^N \sum_{t=1}^{s_j}a_{ij}[t]D_{i}\leq C_{\max}, \forall j\in\mathcal{M}.
\end{equation}

\subsection{Problem Formulation}
We aim to minimize the energy consumption of all UAVs, by jointly optimizing the number and trajectories of UAVs, the user association, the energy harvesting time, and the data collection time.
The optimization problem can be mathematically formulated by
\begin{equation}
	\label{eq:problem}
	\begin{aligned}
		P0:\min_{\mathcal{A},m,\mathcal{R}, \mathcal{T}^{E}, \mathcal{T}^{C}}\sum_{j=1}^m E_j \\
		\text{s.t.}~  \eqref{eq:c_uav_collect}, \eqref{eq:c_return},\eqref{eq:c_iot_transfer}, \eqref{eq:c_eh}, \eqref{eq:c_battery},\eqref{eq:c_capacity}
	\end{aligned}
\end{equation}
where $\mathcal{A}=\left\{a_{ij}[t], \forall i \in \mathcal{N}, j \in \mathcal{M},  t \in \mathcal{T}_j \right\}$ represents the association between UAVs and IoTDs. $m$ is the optimal number of UAVs for data collection. $\mathcal{R}=\left\{r_j[t], \forall j \in \mathcal{M},  t \in \mathcal{T}_j \right\}$ represents the visiting order of hover points for UAVs. $\mathcal{T}^{E}=\left\{T_{ij}^{E}, \forall i \in \mathcal{N}, j \in \mathcal{M}, t \in \mathcal{T}_j \right\}$ represents the set of energy harvesting time and $\mathcal{T}^{C}=\left\{T_{ij}^{C}, \forall i \in \mathcal{N}, j \in \mathcal{M}, t \in \mathcal{T}_j \right\}$ represents the set of collecting data time. %Obviously, Problem $P0$ is a Capacitated Vehicle Routing Problem (CVRP). 
%where $E^F_j$ is affected by the visiting order of hover points, $E^H_j$ is influenced by the energy harvesting time and the data collection time, and $E^T_j$ is influenced by energy harvesting time. 

\subsection{Problem Decomposition}
Since the energy harvesting time and the data collection time of each IoTD are independent of trajectories of UAVs, the original problem $P0$ can be decomposed into two sub-problems: time allocation problem $P1$ and trajectory optimization problem $P2$. Problem $P1$ can be described as follows:
%
%\begin{equation}
%\label{eq:jiang6}
%\begin{aligned}
%P1:\min_{\mathcal{A},m,\mathcal{R}}\sum_{j=1}^m E^F_j \\
%s.t. 
%\eqref{eq:c_uav_collect}, \eqref{eq:c_return},\eqref{eq:c_iot_transfer}, 
%\eqref{eq:c_battery},\eqref{eq:c_capacity}
%\end{aligned}
%\end{equation}
%
%Problem $P1$ is NP-hard, and is a typical combinatorial optimization problem. Moreover, each UAV has its limitations, and the number of UAVs is also required to be optimized. Hence, it is difficult to find the optimal solution by known methods. We solve Problem $P1$ by using the proposed UFO framework, which will be described in section \ref{sec:UFO}. After solving problem $P1$ and get the UAV trajectories, we let UAVs transfer energy and collect data. Then, we optimize the energy harvesting time and data collection time by solving problem $P2$, which can be described as:
%
\begin{equation}
\label{eq:jiang5}
\begin{aligned}
P1:\min_{\mathcal{T}^{C}, \mathcal{T}^{E}}\sum_{j=1}^{m} (E^H_j+E^T_j) \\
\text{s.t.}~ \eqref{eq:c_eh}.\ \ \ \ \ \ \ \ \ \ \ \ 
\end{aligned}
\end{equation}
%%\begin{equation}
%%\label{eq:jiang5}
%%\begin{aligned}
%%P1:\min_{\mathcal{T}^{E}, \mathcal{T}^{C}}\sum_{j=1}^{m} (E^H_j+E^T_j) \\
%%s.t. \ref{eq:c_uav_collect}, \ref{eq:c_return},\ref{eq:c_iot_transfer}, \ref{eq:c_eh}
%%\end{aligned}
%%\end{equation}
%
%Problem $P2$ is a convex optimization problem. The optimal data collecting time $\mathcal{T}^{C}$ and energy harvesting time $\mathcal{T}^{E}$ are achieved when Eq. (\ref{eq:c_eh})  is satisfied with equality as follows:
%
%\begin{equation}
%\label{eq:t_cd_convex}
%{T^{C}_{ij}}^{*}B\log_2(1+\frac{|g^{D}_{ij}|^2 {E^{R}_{j}}^{*}}{\sigma^2 {T^{C}_{ij}}^{*}})=D_{i}, j \in \mathcal{M}, i \in \mathcal{N}
%\end{equation}
%
%Then, we can solve the convex optimization problem by Karush-Kuhn-Tucker (KKT) conditions \cite{2012-kkt} and obtain the optimal solutions $\left\{ {T^{C}_{ij}}^{*}, {T^{E}_{ij}}^{*} \right\}$ as follows:
%
%
%\begin{equation}
%\label{eq:t_cd_solution}
%{T^{C}_{ij}}^{*}=\frac{\ln2D_{i}}{\mathcal{W}(\frac{{\frac{|g^{D}_{ij}|^2 \phi(P^{R}_{ij})}{\sigma^2}}-1}{e})+1}, i \in \mathcal{N}, j\in \mathcal{M}
%\end{equation}
%where $\mathcal{W}(\cdot)$ is the Lambert W function\cite{2013lambert_W_function} and
%\begin{equation}
%\label{eq:t_eh_solution}
%{T^{E}_{ij}}^{*}=\frac{{T^{C}_{ij}}^{*}(2^{\frac{D_{i}}{{T^{C}_{ij}}^{*}}}-1)}{{\frac{|g^{D}_{ij}|^2 \phi(P^{R}_{ij})}{\sigma^2}}}, i \in \mathcal{N}, j\in \mathcal{M}
%\end{equation}
%
%
%After obtaining the optimal $\left\{{{T^{C}_{ij}}^{*}, {T^{E}_{ij}}^{*}}\right\}$, problem $P0$ is solved. 



One can see that Problem $P1$ is a convex optimization problem, %The optimal data collection time $\mathcal{T}^{C}$ and energy harvesting time $\mathcal{T}^{E}$ are achieved when Eq. (\ref{eq:c_eh})  is satisfied with equality as follows:
\begin{comment}
\begin{equation}
\label{eq:t_cd_convex}
{T^{C}_{ij}}^{*}B\log_2(1+\frac{|g^{D}_{ij}|^2 {E^{R}_{j}}^{*}}{\sigma^2 {T^{C}_{ij}}^{*}})=D_{i}, \forall j \in \mathcal{M}, \forall i \in \mathcal{N}.
\end{equation}
\end{comment}
 which can be solved by applying Karush-Kuhn-Tucker (KKT) conditions \cite{2012-kkt}. The optimal solutions $\left\{ {T^{C}_{ij}}^{*}, {T^{E}_{ij}}^{*} \right\}$ can be calculated as follows\cite{2020-aoi-eh}:
\begin{equation}
\label{eq:t_cd_solution}
{T^{C}_{ij}}^{*}=\frac{\ln(2\cdot D_{i})}{\mathcal{W}(\frac{{|g^{D}_{ij}|^2 \phi(P^{R}_{ij})-{\sigma^2}}}{e \cdot {\sigma^2}})+1}, \forall i \in \mathcal{N}, \forall j\in \mathcal{M}
\end{equation}
where $\mathcal{W}(\cdot)$ is the Lambert W function\cite{2020-aoi-eh} and
\begin{equation}
\label{eq:t_eh_solution}
{T^{E}_{ij}}^{*}=\frac{{T^{C}_{ij}}^{*}(2^{\frac{D_{i}}{{T^{C}_{ij}}^{*}}}-1){\sigma^2}}{{{|g^{D}_{ij}|^2 \phi(P^{R}_{ij})}}}, \forall i \in \mathcal{N},  \forall j\in \mathcal{M}.
\end{equation}

After obtaining the optimal $\left\{{{T^{C}_{ij}}^{*}, {T^{E}_{ij}}^{*}}\right\}$, Problem $P2$ can be described as follows:
\begin{equation}
\label{eq:jiang6}
\begin{aligned}
P2:\min_{\mathcal{A},m,\mathcal{R}}\sum_{j=1}^m E^F_j \\
\text{s.t.}~      
\eqref{eq:c_uav_collect}, \eqref{eq:c_return},\eqref{eq:c_iot_transfer}, 
\eqref{eq:c_battery},\eqref{eq:c_capacity}.
\end{aligned}
\end{equation}

%Problem $P2$ is NP-hard and is a typical combinatorial optimization problem. Moreover, each UAV has its limitations, and the number of UAVs is also required to be optimized. Hence, it is difficult to find the optimal solution by known methods. 


%Hence, the hovering energy of UAVs $E^H$ is related to the total data demand of IoTDs in one scenario. Then, UAVs collect data at the hover points. Therefore, when we optimize visiting order of hover points, the association is also optimized, then the total energy cost $E_j$ is minimized. 
% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol
% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

%% prelimiaries
%\section{Prelimiaries}
%\label{sec:pre}
%
%\subsection{Transformer Model}
%
%The Transformer\cite{2017transformer} model is a famous model in the DL field. Transformer is most famous for improving the performance of attention mechanism\cite{2015luong}, using several self-attention layers, which is used in text sequence processing. The ability of the model to extract text sequence features is greatly improved through self-attention Layers. The structure of Transformer is illustrated in Fig. \ref{fig:transformer}.
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=9cm]{fig/transformer.png}
%\caption{The structure of Encoder and Decoder in Transformer}
%\label{fig:transformer}
%\end{figure}
%
%\subsubsection{Encoder}
%The role of Encoder in Transformer is to perform feature extraction on natural language sequences. Positional encoding needs to be performed before the sequence is input into the model. The purpose is to identify the sequence of words in the sequence. After that, it passes through $L$ self-attention layers. A self-attention layer contains a multi-head self-attention (MSA) sublayer and a position-wise Feed-Forward (FF) sublayer. The FF sublayer can be regarded as two convolution layers with a convolution kernel size of 1. Both self-attention and FF sublayers perform residual connections, which are marked with red arrows and blue plus signs in Fig. \ref{fig:transformer}.  
%
%\subsubsection{Decoder}
%The role of the Decoder in Transformer is to obtain the self-attention feature of the original sequence output by the Encoder, which also has an $L$ layers, including a Mask attention layer, a cross attention layer and an FF layer. Decoder needs to perform mask attention operation at the beginning. For example, for the $i$-th input word in the original input text, Transformer wants it to only pay attention to the text sequence before $i$, so the mask the sequence after $i$. The gray arrows represent the features output by Encoder, which will participate in the decoder's cross-attention operation. Then the Encoder output and the Decoder are put together and input into an attention module. Finally, the current output will be used as the next input.
%
%
%This paper draws on the Encoder structure of Transformer to perform Self-Attention operation on IoTDs to extract features. It also refers to the Decoder of Transformer, to output the trajectories of UAVs. The output of Decoder is to select which IoTD to collect data. The paper also puts the scenario features and remaining resources of UAV together as scenario state to help Decoder get planning with the lowest energy consumption and meet the constraints.
%
%
%\subsection{Policy-based Reinforcement Learning}
%
%RL\cite{1998RL} algorithm is a goal-oriented algorithm. In a standard RL algorithm, the agent interacts with the environment at time step $t$ to detect the state $s_t$, perform an action $a_t$ and obtain a reward $r_t$. Assuming that $r(s_t, a_t)$ is the reward function, which represents the reward obtained after the state-behavior interacts with the environment, then the sum of the rewards of the agent from start to finish is $R=\sum_{t=1}^T \gamma r(s_t, a_t)$, $\gamma$ is the reward decay, and $T$ is the total number of states. The purpose of RL is to maximize $R$ by adjusting state $s_t$ and action $a_t$. The reward and punishment mechanism established in the environment will also affect the decision-making of the agent. Thus, a good-designed reward function is essential for RL algorithm.
%
%Policy Gradient (PG) outputs a set of decision-making actions directly, and learns through the obtained rewards. The formula of the PG is as follows
%
%\begin{equation}
%\label{eq:sbpg}
%\theta=\theta + \sum_{t=1}^T \alpha \nabla_{\theta} \log \boldsymbol{\pi}_{\theta} (s_t, a_t)v_t
%\end{equation}
%where $\theta$ is the parameter of the policy model, $\alpha$ is the learning rate, $\boldsymbol{\pi}_\theta (s_t, a_t)$ represents the probability of selecting behavior $a_t$ in state $s_t$ and $v_t$ is the reward received from the environment at the time step $t$. PG is to adjust model parameter $\theta$ to increase the probability of actions that may bring high rewards in certain states, and vice versa.

\section{Attention-based UAV Trajectory Optimization Framework}
\label{sec:UFO}


Problem $P2$ can be formulated as a well-known combinatorial optimization problem called Capacitated Vehicle Routing Problem (CVRP), which is NP-hard and difficult to be solved. %Heuristic methods are always applied to solve CVRP, but these methods have high time complexity and are easy to be trapped into the local minimum.
%ML-based methods can also be introduced to solve CVRP, but there are still several challenges yet to be addressed: First, each UAV has its own constraints (e.g., data storage capacity and energy capacity), and the number of UAVs is unknown and also required to be optimized. Hence, it is hard to determine the structure of the ML model. Second, the visiting order of IoTDs is a typical time series analysis, \textcolor[rgb]{0,0,0}{hence there are many dynamic elements in the system that need to be handled by the ML model. These dynamic elements are constantly changing during trajectory planning, e.g., remaining battery and data storage of the UAV.} %Third, since the lack of high-quality labeled data, it is hard to train the ML model.  
%due to the structure of RNN, the input order of IoTDs will have an impact on the trajectories of UAVs and association. Second, it is difficult for DL models to learn the optimal number of UAVs and trajectories with many constraints\cite{2021E2ECO}. Third, traditional SPG training DL models will be difficult to converge\cite{2010RL}. 
Self-attention can encode the whole system information (e.g., the quantity, location and data size of IoTDs) to a self-attention feature matrix, and make a global decision based on it. Hence, we propose the AUTO framework based on the graph transformer to solve Problem $P2$. %Next, we give a brief introduction of the proposed AUTO framework.

%To address the challenge above, the MSA is used in the encoding procedure to handle variable number of IoTDs, and does not have the shortcomings of RNN, because it can be calculated in parallel. Then, we put the battery and data capacity together with the hidden state to make up the scenario state. The scenario state is related to the procedure of generating trajectories of UAVs. Moreover, a baseline is used in the SBPG. With the aid of baseline, the training process is stable and convergent\cite{2010RL}. 


\subsection{AUTO Framework Overview}
In the AUTO framework, we propose a novel ATOM model to optimize the quantity, trajectory, and association by the customized graph self-attention model. Moreover, we propose a new TENMA method to train the ATOM model. %The overview of the UFO model is illustrated in Fig. \ref{fig:ufo}.
\iffalse 
\begin{figure}[htbp]
\centering
\includegraphics[width=9.1cm]{fig/overview-v13}
\caption{Overview of the UFO framework. }
\label{fig:ufo}
\end{figure}
\fi

%The ATOM model consists of a graph encoder and a trajectory decoder. The graph encoder is utilized to extract the self-attention features of all IoTDs information from a graph structure. Then, the self-attention features of each IoTD and the whole graph are sent to the trajectory decoder, and the optimal quantity and trajectories of UAVs are generated by the decoder. The details of the graph encoder and the trajectory decoder are introduced in Section \ref{sec:encoder} and \ref{sec:decoder}, respectively. The TACL

% TACL. Specifically, a critic network is applied to evaluate the generated trajectories, and the ATOM model is introduced as the actor network to produce the trajectories. Moreover, the reward is used as the baseline to reduce the variance of the critic network and enhance the stabilization and generalization of TACL.


The workflow of the AUTO framework is presented in \textbf{Algorithm \ref{alg:overview}}. 
We initialize the parameter of ATOM model $\theta_\pi$ randomly at the beginning. 
Then, in the training stage, we utilize TENMA method to train the ATOM model. %In SBPG, the RSTP baseline model with the parameter $\theta^{'}$, the REINFORCE loss \cite{1992reinforce} and \textcolor[rgb]{0,0,0}{KL divergence}\cite{2003KL} are introduced to enhance the stability of the policy gradient in the DRL, and improve the performance and generalization of the STP model. 
%Thus, the RSTP baseline with parameter $\theta^{'}$ is initialized. 
In the inference stage, %we assume all UAVs with the same constraints, where $C_{\max}$ and $E_{\max}$ are denoted as the maximum UAV storage and battery capacity, respectively. 
%Then,
%\textcolor[rgb]{0,0,0}{by inputting IoTDs information $\mathcal{X}$, which contain the coordinates and data demand of IoTDs, to the Iencoder with $L$ self-attention layers}, 
the self-attention features of all IoTDs $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$ and the graph feature $\textbf{h}_{sa}$ are obtained by the graph encoder. Next, a solution that contains the total trajectory $\boldsymbol{\pi}$ and quantity $m$ of UAVs are obtained by trajectory decoder. The solution also meets the constraints of storage and battery capacity. Finally, we split the total trajectory $\boldsymbol{\pi}$ into trajectories $\left\{\boldsymbol{\pi}_{(1)},...,\boldsymbol{\pi}_{(m)} \right\}$ of all UAVs. In the $j$-th UAV trajectory, the visiting order is denoted as $\boldsymbol{\pi}_{(j)}=[\pi_{j,1},...,\pi_{j,s_j}], $ and $\pi_{j,t}$ is the index of the IoTD whose data is collected by the $j$-th UAV in the $t$-th time step. 
Therefore, all the association \textcolor[rgb]{0,0,0}{$a_{ij}[t]$} and visiting order \textcolor[rgb]{0,0,0}{$r_{j}[t]$}  are solved in the inference stage of \textbf{Algorithm \ref{alg:overview}}. 


\begin{algorithm}
\caption{AUTO framework}
\textbf{Input:} $C_{\max}$, $E_{\max}$.

%\textbf{output:} $\boldsymbol{\pi}$, $m$. 
\textbf{Output:} \textcolor[rgb]{0,0,0}{$a_{ij}[t]$}, $m$, \textcolor[rgb]{0,0,0}{$r_{j}[t]$}. 

\begin{algorithmic}[1]

\STATE Initialize parameters $\theta_\pi$ of the ATOM model.
%association $\mathcal{A}$, visiting order $\mathcal{R}$. 
\STATE Initialize $a_{ij}[t]=0, r_{j}[t]=0$.% \forall i \in \mathcal{N}, j \in \mathcal{M}, t \in \mathcal{T}_j.$} 
%\STATE \textcolor[rgb]{0,0,0}{Initialize $r_{j}[t]=0, \forall j \in \mathcal{M}, t \in \mathcal{T}_j.$} 

\textbf{Training stage}

\STATE Train ATOM and update $\theta_\pi$ by TENMA method.
%\textbf{Algorithm \ref{alg:TENMA}}. 

\textbf{Inference stage}

\STATE Calculate the $\textbf{h}_{sa}$ and $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$ by Graph Encoder. %in \textbf{Algorithm \ref{alg:encoder}}.
\STATE Calculate $\boldsymbol{\pi}$ and $m$ by Trajectory Decoder. %in \textbf{Algorithm \ref{alg:decoder}}. 
\STATE Split $\boldsymbol{\pi}$ into $\left\{\boldsymbol{\pi}_{(1)},...,\boldsymbol{\pi}_{(m)} \right\}$. 
\FOR {$j=1,...,m$}
\FOR {$t=1,...,s_j$}
\STATE $r_j[t]=\pi_{j,t}$. 
\STATE $a_{\pi_{j,t}, j}[t]=1$. 
\ENDFOR
\ENDFOR

\end{algorithmic}
\label{alg:overview}
\end{algorithm}

% Moreover, We present the SBPG learning method to train the STP model. In SBPG, a baseline model with the same structure as STP model and a reinforcement loss are introduced to enhance the stability and generalization of the policy gradient in the DRL. The detail of SBPG is described in Section \ref{sec:train}. 


\subsection{Graph Encoder}
\label{sec:encoder}

%Our Iencoder is illustrated in Fig. \ref{fig:encoder}. 
The proposed IoT system can be mathematically expressed as a graph structure pair: $\mathcal{G}=(\mathcal{X},\mathcal{E})$, where $\mathcal{X}$ is the set of IoTD node, and $\mathcal{E}$ is the set of edges. For one IoTD, the neighborhoods in the communication range all have edges connected to it. 
We propose a novel graph encoder to extract different self-attention features from IoTDs and the whole graph, which can help decoder to generate trajectories of UAVs with minimum energy consumption. The graph encoder contains a graph embedding layer, $L$ self-attention layers and a graph pooling layer. Each self-attention layer contains a multi-head self-attention (MSA) sublayer, and a fully connected (FC) sublayer. Moreover, residual connections are introduced after every MSA sublayer and fully connected sublayer, and normalization operators are introduced before every MSA sublayer and fully connected sublayer \cite{2017transformer}. 
\textcolor{black}{The proposed graph encoder is illustrated in Fig. \ref{fig:encoder}}. 
The detailed procedure of the graph encoder can be described as follows:
\begin{figure}[htbp]
\centering
\includegraphics[width=9cm]{2.png}
\caption{Structure of the graph encoder.}
\label{fig:encoder}
\end{figure}
\subsubsection{Graph Embedding Calculation}

\textcolor{black}{We define the IoTD information contains the coordinate $\{x^c_i, y^c_i\}$ of the $i$-th IoTD and sensory data $D_{i}$ generated from the $i$-th IoTD, which is denoted as $\textbf{X}_i=[x^c_i, y^c_i, D_{i}]$ and the information of all IoTDs is denoted as $\mathcal{X}=\left\{\textbf{X}_i, \forall i \in \mathcal{N}\right\}$. The graph embedding layer is applied to preprocess the graph information $\mathbf{h}_{0,i}$ of the $i$-th IoTD.}

\subsubsection{Self Attention Calculation}


\textcolor{black}{We introduce self attention layer to calculate self-attention features of all IoTDs.}

\textcolor{black}{First, the graph encoder maps the embedding information $\textbf{h}_{0,i}$ of the $i$-th IoTD  to query $\textbf{Q}_i$, key $\textbf{K}_i$ and value $\textbf{V}_i$ with learnable matrices, respectively. }%The equation is given as

\textcolor{black}{The attention scores are computed as the dot product of the query $\textbf{Q}_i$ and key $\textbf{K}_j$, normalized by the square root of the dimension of the key vectors, followed by a softmax function to obtain the attention weights. These weights are then used to compute a weighted sum of the value $\textbf{V}_j$, producing the self-attention features $\textbf{Z}_i$ of the $i$-th IoTD for each head.}

\textcolor{black}{The outputs of all attention heads are concatenated and linearly transformed to form the MSA feature $\textbf{h}^l_{M,i}$ of the $i$-th IoTD at the $l$-th self-attention layer. The multi-head mechanism allows the self-attention layers to jointly attend to extracted information from different representation subspaces at different IoTDs \cite{2017transformer}.} The output of the $i$-th IoTD in the $l$-th self attention layer is
\begin{equation}
	\label{eq:a3}
	\textbf{h}^l_i=\operatorname{FC}(\textbf{h}^l_{M,i})+\textbf{h}^l_{M,i}
\end{equation}
where $\operatorname{FC}(\cdot)$ means the feed-forward operator of the fully connected layer.

\subsubsection{Graph Pooling Calculation}
%After the last layer of Iencoder, we get the self-attention features of all IoTDs $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$. 
%Then, the features will be used in Tdecoder to get the trajectories of UAVs. However, the number of IoTD is various for different scenarios. In order to input to the Tdecoder, 
\textcolor[rgb]{0,0,0}{After calculating the self-attention features of all IoTDs, we need to merge all attention features to a global attention feature. However, simply averaging the features may produce an unrecoverable loss of IoTDs information\cite{2014learnable-pool}.} % meanings here
\textcolor[rgb]{0,0,0}{%In the last layer of Iencoder, we merge the self-attention features of all IoTDs $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$. % Why?
Therefore, we propose a graph pooling layer to merge attention features of all IoTDs $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$ to a graph feature $\textbf{h}_{sa}$, which can be given as}
\begin{align}
	\mathbf{h}_{sa} &= \operatorname{Mean}\left(\left\{\mathbf{W}_{G,i} \cdot \operatorname{Concat}\left(\mathbf{h}^L_{i}, \mathbf{h}^L_{j}\right), \right.\right. \notag \\
	& \qquad \left.\left. \forall\ i\in\mathcal{N};\ j\in\mathcal{N}(i) \right\}\right)
	\label{eq:a2}
\end{align}
where $\mathbf{h}_{L,i}$ and $\mathbf{h}_{L,j}$ are the outputs of the $L$-th self attention layer, $\mathbf{W}_{G,i}$ is the learnable matrix. $\operatorname{Concat}(\cdot)$ means the concatenation of two vectors.
%\begin{equation}
%\label{eq:sa}
%\textcolor[rgb]{0,0,0}{\textbf{h}_{sa}=F(\textbf{h}_1^L,...,\textbf{h}_N^L).}
%\end{equation}

%\textcolor[rgb]{0,0,0}{To make full use of all self-attention features, we utilize a recursive structure to merge and preserve features. Hence, the merged feature from the first to the $i$-th IoTDs can be calculated as}

%\begin{equation}
%\label{eq:rnn}
%\textcolor[rgb]{0,0,0}{F(\textbf{h}_1^L,...,\textbf{h}_i^L)=\text{relu}(\textbf{W}^r \textbf{h}_i^L+F(\textbf{h}_1^L,...,\textbf{W}^v \textbf{h}_{i-1}^L)+\textbf{b}^r)}
%\end{equation}
\iffalse 
\begin{equation}
\label{eq:rnn}
\textcolor[rgb]{0,0,0}{
  F(\textbf{h}_1^L,...,\textbf{h}_i^L) = 
  \begin{cases}
    \text{a}(\textbf{W}^r \textbf{h}_i^L+\textbf{W}^vF(\textbf{h}_1^L,..., \textbf{h}_{i-1}^L)+\textbf{b}^r), &i>1\\
	\text{a}(\textbf{W}^r \textbf{h}_1^L+\textbf{b}^r), &i=1
  \end{cases}}
\end{equation}
\textcolor[rgb]{0,0,0}{where $\text{a($\cdot$)}$ is the activation function in feature merging layer, and we use Relu function in this layer. $\textbf{W}^r$ and $\textbf{W}^v$ are the learnable weights for current and previous features, respectively. $\textbf{b}^r$ is the learnable bias. The recursive structure enables the feature merging layer $F(\cdot)$ to merge various number of self-attention features and make use of previous merged features of IoTDs. Hence, the learnable merging layer can efficiently merge and preserve the global self-attention feature.}
 \fi
%\begin{equation}
%\label{eq:f_out}
%\textcolor[rgb]{0,0,0}{f_i=}
%\end{equation}
%\begin{equation}
%\label{eq:merge}
%\textcolor[rgb]{0,0,0}{F(\textbf{h}_1^L,...,\textbf{h}_N^L)=\sum_{i=1}^{N}(\textbf{W}^r\textbf{h}_i^L+b^r)}
%\end{equation}
%The calculation flow of the graph encoder can be provided in \textbf{Algorithm \ref{alg:encoder}}.
\iffalse
\begin{algorithm}
\caption{graph encoder}
\textbf{input:} $\mathcal{X}$, $H$. 

\textbf{output:} $\textbf{h}_{sa}$, $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$. 

\begin{algorithmic}[1]

\FOR{$l=0,1,...,L-1$}
\FOR{$i=1,2,...,N$} 
\STATE Get self-attention $\textbf{Z}_i$ of $\mathcal{X}$ by equations (\ref{eq:qkv})-(\ref{eq:oi}). 
\STATE Merge $H$ heads self-attention into $\textbf{h}_{i}^{l+1}$ by equation (\ref{eq:msa}). 
\ENDFOR 
\ENDFOR
\STATE Merge $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$ to $\textbf{h}_{sa}$ by equation (\ref{eq:sa}). 

\end{algorithmic}
\label{alg:encoder}
\end{algorithm}
\fi
%The advantages of the graph encoder in the ATOM model can be concluded as follows: (1) the self-attention layers in graph encoder are suitable to handle scenarios with various numbers of IoTDs. (2) the order in which IoTDs are input to the graph encoder does not affect self-attention features. (3) multiple heads can notice different self-attention features of IoTDs. \textcolor[rgb]{0,0,0}{(4) the learnable feature merging layer can efficiently merge all self-attention features of IoTDs and generate the scenario features to guide the trajectory planning. }

\subsection{Trajectory Decoder}
\label{sec:decoder}

In Problem $P2$, the aim is to minimize the system energy consumption by jointly optimizing the number and the trajectories of UAVs. 
The UAV takes off from the data center, collects data from IoTDs one by one, and returns to the data center. Then, the next UAV repeats the steps until all trajectories are generated. The procedure of the trajectory planning in trajectory decoder is illustrated in Fig. \ref{fig:decoder}. %in which the whole trajectory generated is $[0,4,3,0,2,1,0]$ ($0$ represents the data center). The whole trajectory can be divided into $[0,4,3,0]$ and $[0,2,1,0]$, which represent the trajectories of two UAVs, respectively. 
 The detailed process of the trajectory decoder is explained as follows: 

\begin{figure*}[htbp]
\centering
\includegraphics[width=14cm]{3.png}
%\caption{Trajectory output by decoder (top), and trajectories for each UAVs to collect data (bottom). }
\caption{The trajectories generated by the trajectory decoder. }
\label{fig:decoder}
\end{figure*}

\subsubsection{Graph State Definition}
%To make sure the constraints of UAV battery and data storage capacity are met, 
We develop the graph state for the trajectory decoder that considers the constraints of UAV battery and storage capacity in every step of UAV trajectory planning. Therefore, the trajectory can be optimized under all constraints. 
% the Tdecoder evaluates the probability of each IoTD by inputting the scenario state. 
In the $t$-th time step, the graph state is denoted as $\textbf{h}_s(t)$:
 \begin{equation}
\label{eq:scenario_state}
\textbf{h}_s(t)=[\textbf{h}_{sa}, \textbf{h}^L_{\pi_{t-1}}, C_{t}, E_{t}]
\end{equation}
where $\textbf{h}_{sa}$ is the graph feature obtained from the graph encoder, $\textbf{h}^L_{\pi_{t-1}}$ is the self-attention feature of the last selected IoTD in the trajectory of the current UAV, and $\pi_{t-1}$ is the index of the last selected IoTD. $C_{t}$ and $E_{t}$ represent the remaining data and battery capacity of the current UAV in the $t$-th time step, respectively. 


\begin{comment}
Then, the action of UAV in time step $t$ is defined as

\begin{equation}
\label{eq:action}
a_t=\left\{ 0,1,...,n \right\}
\end{equation}
where the number means the next IoTD to collect data, while $a_t=1$ represents that UAV returns data center. 
Moreover, the reward function is defined as

\begin{equation}
\label{eq:reward}
r=-\sum_{j=1}^m \sum_{i=0}^{S^F} |r_j[\boldsymbol{\pi}_{t+1}]-r_j[\boldsymbol{\pi}_{t}]|_1
\end{equation}
where $r$ is the negative number of UAV flight distance. Because we need to maximize the reward and minimize the length of trajectory of UAV. In the specific state $s_t$, 
\end{comment}

\subsubsection{Trajectory Generation}
%Second, the Tdecoder generates the trajectory. 
%we use the query of scenario state $\textbf{Q}_c(t)=W^{Q}_{c} \textbf{h}_s(t)$ and keys of IoTDs $\textbf{K}_{c,j}=W^{K}_{c} \textbf{h}^L_j$ to calculate the probability distribution of IoTDs $\textbf{P}(t)=[p_0(t), ..., p_N(t)]$, in which the $p_j(t)$ is the probability that UAV selects the $i$-th IoTD as next IoTD to collect data in time step $t$, so one has
In each time step, we use the graph state $\textbf{h}_s(t)$ and the self-attention features of the $L$-th self attention layer to calculate the alignment vector as follows:
\begin{equation}\label{eq:a4}
	a_i\left(t\right)=\frac{\exp(\mathbf{h}_s\left(t\right)^T\mathbf{W}_{i}\mathbf{h}^L_{i})}{\sum_{j=1}^{N}{\exp({\mathbf{h}}_s(t)^T\mathbf{W}_{j}\mathbf{h}^L_{j})}}
\end{equation}
where $\mathbf{W}_{i}$ and $\mathbf{W}_{j}$ are learnable matrices.

Then, we can calculate the context vector as follows:
\begin{equation}\label{eq:a5}
	\mathbf{c}\left(t\right)=\sum_{i=1}^{N}{a_i\left(t\right)}\mathbf{h}^L_{i}
\end{equation}


Next, the probability distribution of the remaining IoTDs  $\textbf{P}(t)=\left\{p_i(t),j\in \mathcal{N} \right\}$ can be calculated as
\begin{align}
	\textbf{P}\left(t\right) &= \operatorname{softmax}\left(\mathbf{W}_{p}\tanh\left(\mathbf{W}_{c}\cdot\operatorname{Concat}\left(\mathbf{c}\left(t\right),\mathbf{h}_s\left(t\right)\right)\right)\right) \notag \\
	&\quad \cdot \textbf{M}
	\label{eq:a6}
\end{align}
where $\mathbf{W}_{p}$ and $\mathbf{W}_{c}$ are learnable matrices, $\textbf{M}$ is the mask matrix. $\textbf{M}(i)=1$ means UAV can collect data of the $i$-th IoTD, and $\textbf{M}(i)=0$ otherwise. $p_i(t)$ represents the probability that the current UAV selects the $i$-th IoTD as next IoTD to collect data in the $t$-th time step.
\begin{comment}
\begin{equation}
\label{eq:prob}
p_j(t)=\text{softmax}({{\textbf{Q}_c(t)}^\text{T} \textbf{K}_{c,j}}) \textbf{M}(j)
\end{equation}
\end{comment}
%where
% $\textbf{Q}_c(t)=\textbf{W}^{Q}_{c} \textbf{h}_s(t)$ is the query of the scenario state and $\textbf{K}_{c,j}=\textbf{W}^{K}_{c} \textbf{h}^L_j$ is the keys of IoTDs. $W^{Q}_{c}$ is the learnable context query weight and $\textbf{W}^{K}_{c}$ is the learnable context key weight. 
%$\textbf{M}$ is the masking. Initially, we set $\textbf{M}=[1,...,1]$ at the beginning of trajectory planning. The length of $\textbf{M}$ is $N$. When $\textbf{M}(i)=1$ means UAV can collect data of the $i$-th IoTD. 
%After data of the $j$-th IoTD is collected, we set $\textbf{M}(j)=0$. 
%There are three cases when $\textbf{M}(i)=0$: (1) the data of the $i$-th IoTD has been collected, (2) UAV cannot fly to the position of the $i$-th IoTD due to low battery, and (3) UAV cannot collect the data of the $i$-th IoTD due to insufficient data storage capacity. 

In each time step, the current UAV selects the IoTD with max probability in $\textbf{P}(t)$ as the next IoTD $\pi_t$ to collect data, so one has 
\begin{equation}
\label{eq:next}
\pi_t= \text{argmax} (\textbf{P}(t))
\end{equation}
where $\text{argmax}(\cdot)$ returns the \textcolor[rgb]{0,0,0}{index of the IoTD with the max probability. }

 %The algorithm flow of the trajectory decoder can be described in \textbf{Algorithm \ref{alg:decoder}}. 
\begin{comment}
\begin{algorithm}
\caption{trajectory decoder}
\textbf{input:} $\mathcal{X}$, $\left\{\textbf{h}_1^L,...,\textbf{h}_N^L\right\}$, $\textbf{h}_{sa}$, $C_{\max}$, $E_{\max}$. 

\textbf{output:} $\boldsymbol{\pi}$, $m$. 

\begin{algorithmic}[1]
\STATE Initialize $\boldsymbol{\pi}, \textbf{M}$. 

\STATE $t=1$. 
\STATE $m=1$. 
\STATE $\pi_{t-1}=0$. 
%\STATE $\pi_{t}=0$. 
%\STATE Set scenario state $\textbf{h}_s(t)=[\textbf{h}_{sa}, \textbf{h}^L_{\pi_{t-1}}, C_{\max}, E_{\max}]$. 
\STATE Initialize scenario state $\textbf{h}_s(t)$ by equation \eqref{eq:scenario_state}. 
 
%\WHILE {not all data from IoTDs has been collected}
\WHILE {$t \leq T$}
\STATE Get $\textbf{P}(t)$ by equation (\ref{eq:prob}). 
% \STATE Check the availability of every IoTDs and get $\textbf{M}$. 
\STATE Get $\pi_{t}$ with $\textbf{P}(t)$ by equation (\ref{eq:next}). 
\STATE Append $\pi_{t}$ to $\boldsymbol{\pi}$. 
\IF {$\pi_{t} = 0$}
\STATE $m=m+1$. 
\STATE $\pi_{t-1}=0$. 
%\STATE Set scenario state $\textbf{h}_c=[\textbf{h}_{sa}, \textbf{h}^L_{0}, C_{\max}, E_{\max}]$. 
\STATE $C_{t}=C_{\max}$, $E_{t}=E_{\max}$. 
\ELSE
\STATE Calculate battery and data storage usage $E_{t, cost},C_{t,cost}$ between IoTD $\textbf{X}_{\pi_{t-1}}$ and IoTD $\textbf{X}_{\pi_{t}}$ in the $t$-th time step. 
\STATE $C_{t}=C_{t}-C_{t,cost}$, $E_{t}=E_{t}-E_{t,cost}$. 
% \STATE Update $\textbf{h}_c=[\textbf{h}_{sa},\textbf{h}^L_{\pi_{n}},C_{left}-C_{cost},E_{left}-E_{cost}]$. 
\STATE $\textbf{M}(\pi_{t})=0$. 
\STATE $\pi_{t-1}=\pi_{t}$. 
\ENDIF
\STATE Update $\textbf{h}_s(t)=[\textbf{h}_{sa}, \textbf{h}^L_{\pi_{t-1}}, C_t, E_t]$. 
\STATE $t=t+1$. 
\ENDWHILE 

\end{algorithmic}
\label{alg:decoder}
\end{algorithm}
\end{comment}
\subsubsection{Trajectory Segmentation}
\label{sec:split}
When all IoTDs have been selected, we can get the total trajectory $\boldsymbol{\pi}=[\pi_1, ..., \pi_T]$. $\boldsymbol{\pi}$ is the permutation of all IoTDs and the data center. $\pi_t \in \left\{ 1,...,N \right\}$ represents the IoTD index and $\pi_t=0$ represents the data center. For example in Fig. \ref{fig:decoder}, we can get the whole trajectory $\boldsymbol{\pi}=[0,4,3,0,2,1,0]$.
Then, the whole trajectory generated by trajectory decoder can be divided into several trajectories for all UAVs. For example, the trajectory $\boldsymbol{\pi}=[0,4,3,0,2,1,0]$ can be divided into $\boldsymbol{\pi}_{(1)}=[0,4,3,0]$ and $\boldsymbol{\pi}_{(2)}=[0,2,1,0]$. Therefore, the number of UAVs is set to $m=2$. Finally, all UAVs can fly and collect data in parallel according to the planned trajectories $\boldsymbol{\pi}_{(1)},...,\boldsymbol{\pi}_{(m)}$.

%The advantages of the trajectory decoder in the STP model can be concluded as follows: (1) the number of UAVs and trajectories can be carried out by trajectory decoder for scenarios with different numbers of IoTDs. (2) constraints (e.g., data storage and battery capacity) are added in the scenario state and affect the selection of IoTDs so that the output trajectories can meet all constraints. 

\subsection{Actor-Critic Trajectory Learning}
\label{sec:train}
The training method of the ATOM model is introduced in this section. 
In reinforcement learning, value-based methods, such as Q-learning and DQN, are inefficient for large-scale action space\cite{2019-np-alphago}. Traditional policy-based methods, such as vanilla policy gradient and Monte-Carlo policy gradient, are hard to converge \cite{jiang2021distributed}. 
Therefore, we utilize a novel TENMA method to train the ATOM model, in which an additional critic network is utilized to evaluate the ATOM model, and the real reward of the system is applied as the baseline to reduce the variance of the critic network, and enhance the stabilization and generalization of TENMA. The detailed procedure of the TENMA method can be described as follows:
%Moreover, the KL divergence is used to determine whether the performance difference between the STP model and the RSTP baseline is significant and then update the RSTP baseline. Hence, the STP model can be trained with the best baseline and achieve the best performance.
%To apply the TACL method, we first define the state, action, and reward. Second, we introduce the stochastic policy. Third, we optimize the parameters of the STP model. Finally, we update the parameters of the RSTP baseline. 
\subsubsection{State, Action and Reward Definition} 
In the WPT-assisted IoT system, we define the state of the system as $\text{State}=\left\{ \textbf{h}_{sa}, \textbf{h}^L_{\pi_{t-1}}, C_{t}, E_{t} \right\}$. and then the action of UAVs is defined as the whole trajectory $\boldsymbol{\pi}$.

%\begin{equation}
%\label{eq:state}

%\end{equation}
%where is the same as scenario state defined in equation (\ref{eq:scenario_state}).


\begin{comment}
\begin{equation}
\label{eq:action}
\text{Action}=\left\{ 0,1,...,N \right\}
\end{equation}
where $\left\{1,2,...,N\right\}$ represents the index of all IoTDs, and $0$ means the UAV returns to the data center. 
\end{comment}
Moreover, we draw $K$ instances from the state space and use Monte Carlo simulation to produce feasible sequences with respect to the current policy of the ATOM model. The reward of the $k$-th instance is defined as
\begin{equation}
\label{eq:reward}
R_k=-\sum_{j=1}^m \sum_{t=1}^{s_j} ||r_j[\pi_{k,t+1}]-r_j[\pi_{k,t}]||_2
\end{equation}
%where the reward equals the negative UAV flight distance. We should optimize trajectories of UAVs by maximizing the reward. 
\subsubsection{Actor Network Design}
The ATOM model with parameter $\theta_\pi$ is designed as the actor network, which defines a stochastic policy $P_{\theta_\pi} (\boldsymbol{\pi}_k \mid \textbf{s}_k)$  for selecting the trajectory $\boldsymbol{\pi}_k$ with the $k$-th instance as follows:
\begin{equation}
\label{eq:stochastic}
P_{\theta_\pi} (\boldsymbol{\pi}_k \mid \textbf{s}_k)=\prod_{j=1}^{m}\prod_{t=1}^{s_j} P_{\theta_\pi} (\pi_{k,t} \mid \textbf{s}_k, \boldsymbol{\pi}_{k,1:t-1})
\end{equation}
where $\textbf{s}_k$ is the state of the $k$-th instance. The loss function of the actor network is defined as
\begin{equation}	
	\label{eq:actorloss}
L\left(\theta_\pi\right)=\frac{1}{K}\sum_{k=1}^{K}{{-R}_k\log P_{\theta_\pi} (\boldsymbol{\pi}_k \mid \textbf{s}_k)}
\end{equation}

Finally, we optimize $L\left(\theta_\pi\right)$ using the following policy gradient with baseline\cite{1992reinforce}: 
\begin{equation}
	\label{eq:actorloss2}
d\theta_\pi\gets\frac{1}{K}\sum_{k=1}^{K}{\left(R_k-Q\left(\mathbf{s}_k,\boldsymbol{\pi}_k\middle|\theta_Q\right)\right)\nabla_{\theta_\pi}\log P_{\theta_\pi} (\boldsymbol{\pi}_k \mid \textbf{s}_k)}
\end{equation}
where $Q\left(\mathbf{s}_k, \boldsymbol{\pi}_k| \theta_Q\right)$ is the predicted reward approximation. 
\subsubsection{Critic Network Design}

We design a critic network with parameter $\theta_Q$ to predict the reward approximation for the $k$-th instance. The loss function of the critic network is defined as
\begin{equation}
	\label{eq:criticloss1}
L\left(\theta_Q\right)=\frac{1}{K} \sum_{k=1}^K\left(R_k-Q\left(\mathbf{s}_k, \boldsymbol{\pi}_k| \theta_Q\right)\right)^2
\end{equation}


The critic network is applied as a baseline to stabilize the learning process. Hence, we optimize $L\left(\theta_Q\right)$ using the following gradient estimator:
\begin{equation}
	\label{eq:criticloss2}
{d\theta}_Q\gets\frac{1}{K}\sum_{k=1}^{K}{\nabla_{\theta_Q}\left(R_k-Q(\mathbf{s}_k,\boldsymbol{\pi}_k|\theta_Q)\right)^2}
\end{equation}

The critic network is optimized in the direction of reducing the difference between the expected rewards and the true rewards during Monte Carlo rollouts. %The details of the TENMA are described in \textbf{Algorithm 2}.
\begin{comment}
	content...

\begin{algorithm}
	\caption{TENMA}
	\label{alg:TENMA}
	\begin{algorithmic}[1]
		\REQUIRE $E_m$.
		\ENSURE $\theta_Q$ and $\theta_\pi$.
		\STATE{Randomly initialize critic network and actor network with weights $\theta_Q$ and $\theta_\pi$.}
		\FOR{epoch=1,2,…,$E_m$}
		\STATE{Reset gradients ${d\theta}_Q\gets0$; $d\theta_\pi\gets0$.}
		\STATE{Sample $K$ instances from the state space.}
		\FOR{$k$=1,2,…,$K$}
		\REPEAT
		\STATE{Initialize time step $t\gets0$.}
		%	\STATE{Calculate MSA $\mathbf{h}_{L,i}$ and  graph attention  $\mathbf{h}_{sa}$ from Graph Encoder.}
		\STATE{Calculate $\boldsymbol{\pi}$ and $m$ from the ATOM model.}
		\STATE{Calculate reward $R_k$ by Eq. (\ref{eq:reward}).}
		\UNTIL{termination condition is satisfied.} 
		\ENDFOR
		\STATE{Update weights $\theta_\pi$ using Eq. (\ref{eq:actorloss})-(\ref{eq:actorloss2}).}
		\STATE{Update weights $\theta_Q$ using Eq. (\ref{eq:criticloss1})-(\ref{eq:criticloss2}).}
		
		%\STATE{$L\left(\theta_Q\right)=\frac{1}{K} \sum_{k=1}^K\left(R_k-Q\left(\mathbf{s}_k, \boldsymbol{\pi}_k| \theta_Q\right)\right)^2$,}
		%	\STATE{${d\theta}_Q\gets\frac{1}{K}\sum_{k=1}^{K}{\nabla_{\theta_Q}\left(R_k-Q(\mathbf{s}_k,\boldsymbol{\pi}_k|\theta_Q)\right)^2}$.}
		
		%	\STATE{$L\left(\theta_\pi\right)=\frac{1}{K}\sum_{k=1}^{K}{{-R}_k\log P(\boldsymbol{\pi}_k|\mathbf{s}_k)}$,}
		%	\STATE{$d\theta_\pi\gets\frac{1}{K}\sum_{k=1}^{K}{\left(R_k-Q\left(\mathbf{s}_k,\boldsymbol{\pi}_k\middle|\theta_Q\right)\right)\nabla_{\theta_\pi}log P(\boldsymbol{\pi}_k|\mathbf{s}_k)}$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
\end{comment}

\subsection{Time Complexity Analysis of the ATOM model}

The time complexity of the ATOM model is calculated based on the graph encoder and the trajectory decoder. We analyze the time complexity of each part, then compute the overall time complexity of the ATOM model. According to the transformer model\cite{2017transformer}, the time complexity of graph encoder is $O(N^2 \times D  \times L)$, where $N$ is the current number of IoTDs, $D$ is the hidden dimension,  $L$ is the number of self-attention layers. The time complexity of the trajectory decoder is $O(N \times D^2)$. When the number of IoTDs $N$ is small, $D$ dominates the complexity of graph encoder and trajectory decoder. The bottleneck of the ATOM model thus lies in trajectory decoder. However, as the number of IoTDs grows larger, $N$ gradually dominates the complexity of these modules, in which case the graph encoder becomes the bottleneck of the ATOM model.

\begin{comment}
introduce the RSTP baseline to stabilize the training. The RSTP baseline in TACL has the same structure as the STP model. 
The trajectories of the STP model $\boldsymbol{\pi}$ and the RSTP baseline $\boldsymbol{\pi}_{BL}$ are obtained by \textbf{Algorithm \ref{alg:encoder}} and \textbf{\ref{alg:decoder}}. The rewards $R(\boldsymbol{\pi})$ and $R(\boldsymbol{\pi}_{BL})$ are obtained by equation (\ref{eq:reward}). 
Then, the loss function is defined as follows:

\begin{equation}
\label{eq:loss}
%\nabla L(\theta)=\mathbb{E}_{\boldsymbol{\pi} \sim p_θ (\boldsymbol{\pi} |s)} [(R(\boldsymbol{\pi})-r_{BL} )\nabla \log p_{\theta}(\boldsymbol{\pi} |s)]
\mathcal{L}(\theta \mid \textbf{s})=\mathbb{E}_{p_{\theta}(\boldsymbol{\pi} \mid \textbf{s})} \left[-R(\boldsymbol{\pi})\right]
\end{equation}
where $\mathcal{L}(\theta \mid \textbf{s})$ is the expectation of length of trajectory $\boldsymbol{\pi}$. 
Finally, we optimize $\mathcal{L}(\theta \mid \textbf{s})$ using the REINFORCE gradient estimator with baseline\cite{1992reinforce}: 

\begin{equation}
\label{eq:reinforce}
%\nabla L(\theta)=\mathbb{E}_{\boldsymbol{\pi} \sim p_θ (\boldsymbol{\pi} |s)} [(R(\boldsymbol{\pi})-r_{BL} )\nabla \log p_{\theta}(\boldsymbol{\pi} |s)]
\nabla \mathcal{L}(\theta \mid \textbf{s})=\mathbb{E}_{p_{\theta}(\boldsymbol{\pi} \mid \textbf{s})} \left[\left(R(\boldsymbol{\pi}_{BL})-R(\boldsymbol{\pi})\right) \nabla \log p_{\theta}(\boldsymbol{\pi} \mid \textbf{s})\right].
\end{equation}
%where
%$\boldsymbol{\pi}$ is the UAV trajectory of $\textbf{s}$ generated by STP model. 
%$R(\cdot)$ is the reward calculation function of a trajectory given by equation (\ref{eq:reward}). Hence, $R(\boldsymbol{\pi})$ is the reward of trajectory generated by STP model and $R(\boldsymbol{\pi}_{BL})$ is the reward of baseline model. 
%$p_{\theta} (\boldsymbol{\pi} \mid \textbf{s})=\prod_{j=1}^{M}\prod_{t=1}^{s_j} p_{\theta} (\pi_t \mid \textbf{s}, \pi_{1:t-1})$ is the probability distribution when STP's parameter is $\theta$. 
%$\mathbb{E}_{p_{\theta}(\boldsymbol{\pi} \mid \textbf{s})} $ is the expectation reward of distribution $p_{\theta} (\boldsymbol{\pi} \mid \textbf{s})$. 

\subsubsection{Baseline Update}
%The baseline model is essential in SBPG because a good baseline can estimate the model unbiasedly and stabilize the training process of the reinforcement learning by reducing the variance of reward \cite{2021-drl-survey}.
During the training stage, the training data is generated randomly, 
% we need to estimate the performance of model. However, in our problem, we generate the scenarios instances randomly.
hence the variance of reward is high and it is difficult to estimate the performance of the STP model. 
Therefore, we utilize the RSTP baseline as the baseline in the training stage to reduce the variance of the reward and estimate the performance of the STP model accurately and unbiasedly\cite{2021-drl-survey}. Then, the training process is stable and the STP model can achieve a high reward. 

\textcolor[rgb]{0,0,0}{However, it is uncertain how to update the baseline. 
The actor critic-based methods use a trainable baseline and update the baseline in the back-propagation stage\cite{2018tsp-rl-critic}. It is hard for the critic-based baseline to estimate scenarios with various constraints, hence it is ineffective during the training stage\cite{2018attention}. 
%Therefore, in the proposed SBPG, we update the RSTP baseline by overwriting its parameter $\theta^{'}$ with that of STP model when the performance difference is significant.
Therefore, in the proposed TACL, we update the RSTP baseline when the performance difference is significant, and KL divergence is applied to determine whether the difference is significant, which does not require the reward distributions obtained by the STP model and the RSTP baseline is normal distributions. 
}
%In the training stage, we compare the result of the STP model with that of RSTP baseline at the end of every epoch, and replace the parameter $\theta^{'}$ of the RSTP 
%only if the improvement is significant according to a paired t-test \cite{1987t-test} on $10,000$ evaluation scenario instances, and the evaluation scenario instances are generated randomly. 
\textcolor[rgb]{0,0,0}{
%In the SBPG, when the average award of STP model outperforms that of RSTP baseline and the performance difference is significant. 
%We use the KL divergence of the reward distributions output by STP model and RSTP baseline to determine whether the difference is significant. 
When the performance of the STP model is better than that of the RSTP baseline and the KL divergence is higher than a preset threshold $\beta$, we will update the RSTP baseline. %by overwriting the parameter $\theta^{'}$ of the RSTP baseline with $\theta$ of the STP model.
%we can say the performance difference between STP model and RSTP baseline is significant.  
%In this way, STP model can explore all the states of IoTDs and the corresponding trajectories planning to the greatest extent in the early stage of training, and reduce the situation of trapping into local optimization. 
}
%When the $\theta^{'}$ is updated, we generate new training scenario instances to prevent overfitting\cite{2018attention}.
%\textcolor[rgb]{0,0,0}{Then, we can focus on optimizing the STP model, and the RSTP baseline can help reduce the variance of reward and stabilize the training procedure. }

%In SBPG, we utilize the paired t-test \cite{1987t-test} to distinguish whether performance of STP model is different from that of baseline model. 

%In the training, a batch of scenario instances' reward distributions $\overline{R}_{STP}$ and $\overline{R}_{BL}$ are generated by STP and baseline model. 
%The performance difference is 
%In paired t-test, $\alpha$ is the level of significance, and $\alpha=0.05$ for most time to 

%The parameter $\theta^{'}$ of baseline model is frozen, and we overwrite it with parameter of STP $\theta$ when baseline and STP can pass the paired t-test\cite{1987t-test}. In the paired t-test, $\alpha$ is the level of significance. 

%In training, STP model and baseline model will generate trajectories on batches of scenario instances and we can get the reward distribution $\overline{R}_{STP}$ and $\overline{R}_{BL}$ of STP model and baseline model. 
 
%Paired t-test is to test whether the difference between the mean of the two reward distributions, $\overline{R}_{STP}$ and $\overline{R}_{BL}$, and the population they represent is significant and $\alpha$ is used to represent the significance. In SBPG, we define that baseline and STP pass the paired t-test when $\alpha < 0.05$, and overwrite $\theta^{'}$ with $\theta$. 
 
%The training procedure of TACL is shown in \textbf{Algorithm \ref{alg:training}}, where $\text{KL}(\cdot)$ is used to calculate the KL divergence of two distributions and $\text{avg}(\cdot)$ is used to calculate the average value. 
\end{comment}


\begin{comment}
\begin{algorithm}
\caption{TACL Algorithm}

\textbf{input:} Training epochs $E_M$, $\beta$. 

\textbf{output:} Trained parameter of STP model $\theta$. 

\begin{algorithmic}[1]
\STATE {Initialize parameter of RSTP baseline $\theta^{'}$}. 
\FOR{$epoch=1,...,E_{M}$}

\STATE Get scenario feature $\textbf{h}_{sa}$ in \textbf{\textbf{Algorithm \ref{alg:encoder}}}. 
\STATE Get $\boldsymbol{\pi}, m$ with parameter $\theta$ in \textbf{Algorithm \ref{alg:decoder}}. 
\STATE Calculate the reward $r_{STP}$ of STP model with $\theta$ by equation \eqref{eq:reward}.
\STATE Calculate the reward $r_{BL}$ of RSTP baseline with $\theta^{'}$ by equation \eqref{eq:reward}. 
\STATE Calculate the differential of loss $\nabla L(\theta)$ via equations (\ref{eq:stochastic})-(\ref{eq:reinforce}). 
\STATE Update $\theta$ with $\nabla L(\theta)$. 

%\IF {$\text{paired\_t\_test}(\theta ,\theta^{'})<0.05$}
\IF {\textcolor[rgb]{0,0,0}{$\text{avg}(r_{STP})>\text{avg}(r_{BL})$}}
\IF {\textcolor[rgb]{0,0,0}{$\text{KL}(r_{STP} ,r_{BL}) > \beta$}}
\STATE $\theta^{'}=\theta$. 
\ENDIF
\ENDIF

\ENDFOR

\end{algorithmic}
\label{alg:training}
\end{algorithm}
\end{comment}
\begin{comment}
	The procedure of training can be divided into the following steps:

\begin{enumerate}
\item Generate training scenario instances. 
\item Get the reward $R(\boldsymbol{\pi})$ of STP model. 
\item Get the reward $R(\boldsymbol{\pi}_{BL})$ of baseline model. 
\item Optimize parameter $\theta$ of STP by $\nabla L(\theta)$ via equation (\ref{eq:reinforce}). 
\item If paired t-test\cite{1987t-test} is passed, overwrite parameter of baseline $\theta^{'}$ with parameter of STP $\theta$ and generate new training scenario instances. 

\end{enumerate}
\end{comment}


%For each training epoch, there are $1,000,000$ scenarios instances used to train the STP model. 
%After the training stage, the trajectory can be generated very fast with some simple algebraic calculations. The advantages of the TACL method are listed as follows:(1) the RSTP baseline can reduce the variance of reward and stabilize the training procedure. (2) we use the KL divergence to evaluate the discrepancy between the STP model and the RSTP baseline and update the RSTP baseline efficiently. Therefore, the STP model can be trained with the best baseline and achieve the best performance. 

%After the training of STP model, 
% the parameter in STP is optimized by SBPG to get the trajectories of UAVs with the lowest energy consumption and satisfy the UAV data storage and battery constraints. 
%For the Iencoder, the training process is to find the self-attention features of IoTD. 
%For the Tdecoder, the training process is to make good use of self-attention features output by Iencoder and UAV's battery and storage constraints to output UAV trajectories that meet the constraints and have the lowest energy consumption. 
%Of course, a baseline model is added in SBPG to help train STP model. Therefore, the training process is stabilized. 

\begin{comment}  %% SPG training figure

The training procedure is illustrated in Fig. \ref{fig:SPG}. (TODO: change the figure of training)

\begin{figure}[htbp]
\centering
\includegraphics[height=7.7cm,width=9.2cm]{fig/SPG.png}
\label{fig:SPG}
\caption{Train STP model with Stochastic Policy Gradient.}
\end{figure}



\section{Convergence and Complexity Analysis}
\label{sec:converge}
\subsection{Convergence Analysis of TACL }
In the training phase of TACL, we denote $\mathcal{B}$ as a batch of scenario instances. For the $k$-th training step, the parameter of the STP model is $\theta_k$. Hence, the batch loss in the $k$ step is

\begin{equation}
	\label{eq:batch_loss}
	\mathcal{J}(\theta_k)=\mathbb{E}_{\textbf{s} \in \mathcal{B}} (\textcolor[rgb]{0,0,0}{\mathcal{L}}(\theta_k \mid \textbf{s})). 
\end{equation}
%where we denote $\mathcal{L}^{'}$ as the loss 

Therefore, the batch gradient can be calculated as 

\begin{equation}
	\label{eq:batch_gradient}
	\nabla \mathcal{J}(\theta_k)=\mathbb{E}_{\textbf{s} \in \mathcal{B}} (\nabla\textcolor[rgb]{0,0,0}{\mathcal{L}}(\theta_k \mid \textbf{s})). 
\end{equation}

The parameter of the STP model at the $k+1$-th training step can be calculated as 

\begin{equation}
	\label{eq:optimize}
	\theta_{k+1}=\theta_k+\alpha_k \nabla \mathcal{J}(\theta_k)
\end{equation}
where $\alpha_k$ is the learning rate in the $k$-th training step.
We also define the following auxiliary random variable $W_k$ for the $k$-th training step as follows:

\begin{equation}
	\label{eq:auxiliary}
	W_k=\mathcal{J}\left(\theta_k\right)-L\ell^2\sum_{k}^{\infty}\alpha_k^2
\end{equation}
where $L$ is the Lipchitz constant of $\mathcal{J}(\theta)$ \cite{2015Lipschitz-mdp}, and $\ell$ is the upper bound of $|\nabla \mathcal{J}(\theta_k)|$, 
%where $||...||$ is used to calculate the magnitude. 
which is the module length of $\left\{ \nabla \mathcal{J}(\theta_k) \right\}$. %% N+ is the positive integer numbers. 
Then, we introduce the probability measure space $(\Omega, \mathcal{F}, \mathcal{P})$. $\Omega$ is the sample space, and is all hover points of IoTDs. $\mathcal{F}$ is the set of events, which contains the visiting order of hover points. $\mathcal{P}$ is the probability measure, which describes the probabilities of all events in $\mathcal{F}$.
From \cite{2019-spg-converge}, one inequality property of $W_k$ is

% cite paper!
\begin{equation}
	\label{eq:wk-inequal}
	\mathbb{E}\left(W_{k+1}\mid\mathcal{F}_k\right)\geq W_k+\alpha_k\left|\nabla\mathcal{J}\left(\theta_k\right)\right|^2.
\end{equation}
\textcolor[rgb]{0,0,0}{where $\mathcal{F}_k$ denotes the events (UAV trajectories) before the $k$-th training step. }% cite paper!
Note that $W_k$ is bounded by $\mathcal{J}^{\ast}$, where $\mathcal{J}^{\ast}$ is the global minimum $\mathcal{J}(\theta)$ and the maximum reward. So one has

\begin{equation}
	\label{eq:wk-bound}
	E\left(\mathcal{J}^{\ast}-W_{k+1}\mid\mathcal{F}_k\right)\le(\mathcal{J}^{\ast}-W_k)-\alpha_k\left|\nabla\mathcal{J}\left(\theta_k\right)\right|^2
\end{equation}
where $\left\{ \mathcal{J}^{\ast}-W_{k} \right\}$ is a nonnegative sequence. By the supermartingale convergence theorem \cite{1971supermartingales}, we have

\begin{equation}
	\label{eq:supermartingale}
	\sum_{k=1}^{\infty}{\alpha_k\left|\nabla\mathcal{J}\left(\theta_k\right)\right|^2}<\infty. 
\end{equation}

We assume that the learning rate $\alpha_k$ meets the following conditions that $\sum_{k=1}^{\infty} \alpha_k=\infty$ and $\sum_{k=1}^{\infty} \alpha_k^2 < \infty$, then we have

\begin{equation}
	\label{eq:converge}
	\lim_{k\rightarrow \infty} \left|\nabla\mathcal{J}\left(\theta_k\right)\right|=0.
\end{equation}

Now, we prove that the gradient $\nabla\mathcal{J}\left(\theta_k\right)$ will be equal to $0$ when the $k$-th training step goes to infinity. The STP model will finally converge. 
\end{comment}







\section{Results and Discussion}
\label{sec:experiments}

\subsection{Parameter Settings}

\textcolor{black}{In the simulation, we consider an area of 1000 m $\times$ 1000 m, and the locations and sensory data of IoTDs are various in the scenario. We assume there are 500 IoTDs in the area. 
The data size $D_{i}$ for each IoTD is randomly selected in $[0.2,1.5]$ MB \cite{zhang2022joint}.  
%The scenarios with $100$ IoTDs are same as the number of IoTDs for training data, while scenarios with $500$ IoTDs are not. 
Then, the detailed parameter settings of the AUTO framework are listed in Table \ref{tab:param} \cite{yu2021multi}.}
Moreover, we use PyTorch to implement the AUTO framework. All simulations are carried out in Python3.6 Environment running on Intel Xeon E5 CPU and NVIDIA Tesla T4 GPU with 32GB RAM. 

%\cite{2018attention} \cite{2017eh-param}.

%In Table \ref{tab:param}, the start-up energy consumption of UAV includes the sum of all energy consumption from the time when the UAV is in the data center to the time when UAV takes off to a certain altitude and is ready for the data collection task. And the start-up energy cost is a fixed value for all UAVs in our simulation. 

%In STP model, the number of self-attention layers is $3$. For each self-attention layer, we utilize a $8$-head self-attention, a feed-forward and a batch normalization sublayers, and the activation function is $\text{softmax}$. 
 
%We randomly generate $1,000,000$ scenario instances with $100$ IoTDs on the fly for each epoch and train the model for $100$ epochs. After training for the model, we evaluate the performance of the model on $1,000$ randomly generated scenario instances with $100$ IoTDs and $500$ IoTDs, respectively. 
%The training set contains $1,000,000$ instances of scenarios with 100 IoTDs for each training epoch. The validation set contains $1,000$ instances of scenarios with 100 IoTDs and $1,000$ instances of scenarios with 500 IoTDs. In both training and evaluation sets, the data containing locations and data amount of IoTDs is generated randomly. 

%\begin{table}[h]
%	\caption{Parameters of the UFO framework.}
%	\centering
%	\setlength{\tabcolsep}{3mm}
%	\begin{tabular}{|c|c|}\hline
%		Parameter&{Value}\\\hline
%		Number of Self-Attention layers $L$&3\\
%		Number of heads $H$&8\\
%		Dimension of keys $d_k$&128\\
%		%	Activation function in Iencoder&softmax\\
%		%	Activation function in Tdecoder&tanh\\
%		Hidden dimension of feed-forward sublayer&512\\
%		Normalization type&Batch normalization\\
%		Activation function in Self-Attention layer&Softmax\\
%		Learning rate $\alpha$&0.001\\
%		Learning rate decay $\gamma$&0.96\\
%		Training epochs $E^M$&100\\
%		KL threshold $\beta$&0.005\\
%		Batch size&256\\
%		%	Training size per epoch&1,000,000\\
%		%	Evaluation size&1,000\\
%		\hline
%	\end{tabular}
%	
%	\label{tab:model_param}
%\end{table}



\begin{table}[h]
	\caption{Parameters of the system model}
	\centering
	\setlength{\tabcolsep}{3mm}
	\renewcommand\arraystretch{1.25}
	\begin{tabular}{|c|c|}\hline
		Parameter&{Value}\\\hline
		%		Number of IoTDs $N$&100, 500\\
		%	Amount of data per IoTD (100) $D_{i}$&10$\sim$90MB\\
		%	Amount of data per IoTD (500) $D_{i}$&2$\sim$15MB\\
	%	UAV Data Storage Capacity $C_{\max}$&50MB\\
		%		Data Transfer Rate $R_{ij}$&1024KB/s\\
		%	Data Transmitting Power $$&50W\\
		%		Harvested Power $\phi(P^{R}_{ij})$ &50W\\
		
		Fly speed of UAV $v$ & 10 m/s\\
		Flight Height $H^F$ & 20 m\\
		Flight Power $P^F$&75 W\\
	%	UAV Battery Capacity $E_{\max}$&2550mAh\\
		Bandwidth $B$ & 2M Hz\\
		Transmitting power $P^{T}$ & 0.5 W\\
		Data collection power $P^{C}$ & 0.5 W\\
		Hover power $P^{H}$ & 50 W\\
		%Path loss factor $\kappa$ & 2\\
		%		Channel power gain at 1m $\kappa_{0}$ & -60dB\\
		Noise power $\sigma^{2}$ & -110 dBm\\
		%	Attenuation parameter $\eta^{L}$ & 0.9\\
		%UAV Working Voltage&11.55V\\
		Number of Self-Attention layers $L$&3\\
		Number of heads $H$&8\\
		%	Dimension of keys $d_k$&128\\
		%	Activation function in Iencoder&softmax\\
		%	Activation function in Tdecoder&tanh\\
		Hidden dimension of feed-forward sublayer&512\\
		%	Normalization type&Batch normalization\\
		%	The activation function in Self-Attention layer&Softmax\\
		%Learning rate $\alpha$&0.001\\
		\textcolor{black}{Learning rate decay}&0.98\\
		\textcolor{black}{Training epoch} &500\\
		\textcolor{black}{Experience pool capacity}&10000\\
		\textcolor{black}{Batch size}&100\\
	\textcolor{black}{Learning rate for the Critic network}&1e-3\\
	\textcolor{black}{Learning rate for the Actor network}&1e-5\\
		\hline
	\end{tabular}
	
	\label{tab:param}
\end{table}
%We utilize Pytorch to implement the UFO framework. The number of training epochs is $100$, while $1,000,000$ scenario instances are generated in each epoch for training. The training procedure is run on GPU: NVIDIA Tesla T4, CPU: Intel Xeon E5 and 32G flash memory on OS: Windows Server 2012. 

%The aspects to be verified in our experiment are:
%\begin{enumerate}
%	\item Comparison of attention mechanism. Comparison attention: non-attention, one head self-attention, dot product attention \cite{2015luong} and area attention \cite{2019area}. 
%	\item Comparison of baselines during the training. Comparison baseline: without baseline, exponential baseline \cite{2016PN-RL} and critic baseline \cite{2018tsp-rl-critic}. 
	%\item Satisfaction of constraints. Check whether the planning of UAVs meets the constraints. 
%	\item Comparison with mainstream trajectory planning methods. Comparison methods: nearest neighbor\cite{2013tsp-near}, pointer network\cite{2016PN-RL}, graph neural network\cite{2017-cvrp-gnn} and Google OR-Tools\cite{ortools}. 
%	\item Real life case evaluation. The STP model is evaluated on a real life case. 
%\end{enumerate}


\subsection{Comparison of Attention Mechanisms with Graph Pooling}
%The self-attention in STP model plays an important role in the feature extraction of IoTDs, 
This experiment is presented to evaluate the performance of different attention mechanisms with various pooling operators.
The ATOM model is compared with two attention mechanisms: Luong attention (Luong-Attn)\cite{2015luong} and Area attention (Area-Attn)\cite{2019area}, and three pooling operators mean, sum and max are also considered.
%\begin{enumerate}
%	\item Non-attention (None). %The fully connected layers are used to replace the multi-head self-attention layers in the STP model.
%	\item One-head self-attention (One-Head). %The one-head self-attention layers are used to replace the multi-head self-attention layers in the STP model.
%	\item Luong attention\cite{2015luong} (Luong-Attn). %The dot-attention layers are used to replace the multi-head self-attention layers of STP model. %Attention method that calculates the dot product attention between the output hover point and input IoTDs. 
%	\item Area attention\cite{2019area} (Area-Attn). %The area-attention layers are used to replace the multi-head self-attention layers of STP model. %Area attention utilizes different size of area memories to perform spatial attention on information of IoTDs.
%\end{enumerate}
 The minimum energy costs (Min.), average energy costs (Ave.), and standard deviation (Std.) are shown in Table \ref{tab:perform}. %The energy consumption unit is "Wh". 

\begin{table}[]
	\caption{Comparison of different attention mechanisms with different pooling operators}
	\centering
	\setlength{\tabcolsep}{3.5mm}
	\renewcommand\arraystretch{1.25}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		Attention Type& Pool operator  & Min. & Ave. & Std. \\ \hline
		\multirow{3}{*}{Luong-Attn\cite{2015luong}} &  Mean&  289.57 & 293.25 & 3.58 \\ \cline{2-5} 
		& Sum & 291.15 & 295.58 & 4.32 \\ \cline{2-5} 
		& Max & 292.37 & 296.49 & 4.05 \\ \hline
		\multirow{3}{*}{Area-Attn\cite{2019area}} &  Mean& 262.26 & 267.28 &  4.77\\ \cline{2-5} 
		&  Sum& 263.74 & 268.69 & 4.65 \\ \cline{2-5} 
		&  Max& 263.35 & 268.27 & 4.61 \\ \hline
		\multirow{3}{*}{ATOM}
		& Mean & \textbf{262.75} & \textbf{264.32} & \textbf{1.39} \\ \cline{2-5} 
		&  Sum&262.72  & 265.48 & 2.54 \\ \cline{2-5} 
		&  Max& 263.02 &265.74  & 2.43 \\ \hline
	\end{tabular}
	\label{tab:perform}
\end{table}



%Attention used for models are Non-Attention (None), One-Head Self-Attention (One-Head), Dot Attention (Luong-Attn), Area attention (Area-Attn) and the proposed STP model (STP). As is shown in \ref{tab:perform}, the performance results of attention mechanism from worst to best: Non-attention, dot product attention, one-head self-attention, our framework. 

%The DL model without attention can not get the dependency between IoTDs well, hence it is impossible to plan an excellent UAV path. After adopting the attention mechanism, the performance of the model has been greatly improved, but compared with self-attention, dot product attention only focuses on the correlation between IoTDs and lacks the global value evaluation of IoTDs in the scenario. 
%The multi-head self-attention in our STP model takes into account not only the local self-attention features, but also the global self-attention features of IoTDs information. 
\textcolor{black}{In Table \ref{tab:perform}, it is evident that the proposed ATOM model, which incorporates a multi-head self-attention layer with mean graph pooling in its graph encoder, consistently exhibits the lowest energy consumption across all scenarios. Several factors may contribute to this lower energy consumption: (1) Both luong-attn and area-atten are local attention mechanisms, whereas multi-head self-attention is a hybrid mechanism combining local and global attentions. Through multi-head self-attention, the graph encoder can capture local IoT characteristics of IoTDs at different scales as well as global characteristics of IoTDs, thereby achieving enhanced performance. (2) Graph embedding and pooling enhance attentional features from graph information. Particularly, mean pooling, compared to max pooling, preserves more information of IoTDs and is more sensitive to features of IoTDs than sum pooling, making it more suitable for our ATOM model.}

%In Table \ref{tab:perform}, it is clear that the proposed ATOM model, which has self-attention layers with mean graph pooling in the graph encoder, has the lowest energy cost in all scenarios. The reasons for low energy cost may be interpreted by the following reasons: 
%(1) With the multi-head attention mechanism, graph encoder can obtain both local IoTDs features and global IoTDs features from different scales. (2) The graph embedding and pooling can enhance the attention features by graph information.

\begin{comment}
	content...

\subsection{Comparison of Different Baselines}


This experiment is applied to investigate the feasibility of the proposed RSTP baseline model in the TACL algorithm. The RSTP baseline is compared with the following baseline methods: Non-baseline (None), Exponential baseline (EXP)\cite{2016PN-RL} and Critic baseline (Critic)\cite{2018tsp-rl-critic}. Then, we train the STP model by the TACL algorithm with different baselines, and we illustrate the rewards in Fig. \ref{fig:award}, and the energy costs of different baselines are listed in Table \ref{tab:baseline}. 

%\begin{enumerate}
%	\item Non-baseline (None). %The reward of baseline model is set to $0$ in the training phase. 
%	\item Exponential baseline\cite{2016PN-RL} (EXP). %An exponential function replaces the RSTP baseline during the training. %During the training, the reward of baseline model decreases exponentially.%Let $r_{BL}=\gamma^e M_0$, where $M_0$ is the energy cost of the initialized model, and $\gamma$ is the decay value, which is set to 0.8 in this paper. $e$ is the epoch of training. During the training, $r_{BL}$ decreases exponentially reduction.
%	\item Critic model \cite{2018tsp-rl-critic} (Critic). %A learnable DL model replaces the RSTP baseline during the training. %Let a learnable DL baseline model to predict the reward of a scenario. 
%\end{enumerate}

%This experiment compares the training with different baselines, and the structure of model and training parameter are the same. 
%The reward changes during training are illustrated in the Fig. \ref{fig:award} and the results are listed in Table \ref{tab:baseline}. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=9cm]{fig/training-v7}
	%\caption{Comparison of Award for train without baseline (W/O), Exponential function (EXP) as baseline, Critic model (Critic) as baseline model, and the proposed baseline in UFO framework (UFO).}
	\caption{The rewards of different baselines. }
	\label{fig:award}
\end{figure}

It can be seen from Fig. \ref{fig:award} that the RSTP achieves the highest reward among all compared baselines. 
\textcolor[rgb]{0,0,0}{The reason is that the RSTP can reduce the variance of the reward and stabilize the training procedure. Then, we update the parameters of RSTP only when the STP model outperforms the RSTP baseline significantly, hence the RSTP baseline is always the best baseline during the training phase, and the STP model can achieve the best performance.} 
% and the RSTP can reduce the variance of the reward during the training process. 
% Because of the low variance of the reward, the STP model can be estimated accurately, and the training process is stable. 
%Moreover, all baselines can converge at after around $30$ epochs of training while the None baseline does not converge and the reward goes down. The reason is that the baseline can stabilize the training procedure by reducing the reward variance and help the model achieve optimal results. 
%Then, RSTP converges slower than EXP and Critic, this is because


\begin{table}[h]
	\caption{Comparison of different baselines}
	\centering
	\setlength{\tabcolsep}{6mm}
	\begin{tabular}{|l|c|c|}\hline
		Baseline Method&{Cost (100)}&{Cost (500)}\\\hline
		None&305.4000&412.3866\\
		EXP\cite{2016PN-RL}&271.5173&370.2144\\
		Critic\cite{2018tsp-rl-critic}&266.4679&361.9262\\
		RSTP& \textbf{262.1566}& \textbf{335.1642}\\
		\hline
	\end{tabular}
	\label{tab:baseline}
\end{table}

%Baselines used during training are train without baseline (None), EXPonential function (EXP), Critic baseline (Critic) and our proposed baseline (UFO). 

As shown in Table \ref{tab:baseline}, 
the RSTP achieves the lowest energy cost among all compared baselines. 
The reason is that the TACL with RSTP can estimate the energy cost of all trajectories more accurately. Moreover, the estimator is unbiased. 
\end{comment}
%The reason is that the update method of RSTP allows the reward variance reach a small value when the training data possess strong randomness. 
%Then, RSTP may converge slower than EXP and Critic but achieve the highest reward. 
%Moreover, all baselines can converge at after around $30$ epochs of training while the None baseline does not converge and the reward goes down. The reason is that the baseline can stabilize the training procedure by reducing the reward variance and help the model achieve optimal results. 
%the training with proposed RSTP achieves the highest reward, and the performance of the STP model trained with RSTP baseline model gets the best among the comparison baselines methods. Because the RSTP can reduce the reward variance of STP model during training, thus the change of loss is not very large, hence the training process is stable. 
%W/O has the most unstable training procedure, and the worst result. SPG without baseline converges slower than SBPG, and the reward during the training is unstable. EXP converges faster than other methods, and has a better result than W/O. However, the hard reduction of EXP is difficult to continue to converge in the later stage of training. 
%our proposed UFO frame converges slowly in the early stage of training, but the training process converges stably and the performance of the trained model is the best among the comparison baselines methods. Because the baseline can reduce the reward variance during training, thus the change of loss is not very large, and the training process is stable. 

%\subsection{Constraint Satisfaction}
%
%In the UAV data collection problem, we need not only to get the optimal trajectories of UAVs, but also to meet several constraints. In this subsection, the planning of the scenario with 100 IoTDs is shown in Fig. \ref{fig:constraints}, and the satisfaction of constraints is shown in the Table \ref{tab:constraints}. 
%
%\begin{figure}[htbp]
%\centering
%\includegraphics[width=8cm]{fig/constraints-v2.png}
%\caption{The trajectories of UAVs of scenario with 100 IoTDs. Distance unit: km. Energy consumption unit: Wh}
%\label{fig:constraints}
%\end{figure}
%
%\begin{table}[h]
%\caption{The data storage usage and battery remain of all UAVs}
%\centering
%\begin{tabular}{|l|c|c|}\hline
%	UAV&{Data Storage Used}&{Battery Usde}\\\hline
%	0&461&1849\\
%1&467&2050\\
%2&495&1928\\
%3&502&1863\\
%4&503&2132\\
%5&498&2059\\
%6&495&2180\\
%7&506&1950\\
%8&453&1911\\
%9&330&1612\\
%	\hline
%\end{tabular}
%\label{tab:constraints}
%\end{table}
%In Table \ref{tab:constraints}, $a/b$ means that the total amount of resources is $b$, and the usage is $a$. Storage space unit: MB. Battery unit: mAh. 
%
%It can be seen from the Fig. \ref{fig:constraints} and Table \ref{tab:constraints} that all UAVs meet the constraints of storage space and battery capacity, and UAVs can fly and perform tasks in parallel. In other test scenarios, all results meet the constraints. Therefore, UFO can handle the constraint planning. 

\subsection{Comparison of Trajectory Designers}
This experiment is used to compare the overall performance of the proposed AUTO framework with different trajectory designers in the WPT-assisted IoT system, and two different evaluations are presented. 

Firstly, we compare the energy cost and computing time of the ATOM model with 
%This experiment is used to compare the energy cost and computing time of different trajectory planning algorithms for the UAV-assisted wireless powered IoT system. 
%The proposed UFO framework is compared with 
%the Nearest Neighbor (NN)\cite{2013tsp-near}, 
Pointer Network (PN)\cite{bello2016neural}, Graph Neural Network (GNN)\cite{2017-cvrp-gnn} and Graph Pointer Network (GPN)\cite{2019-gpn-rl} using different reinforcement trainers (e.g., Reinforce, AC, A2C and the proposed TENMA)\cite{li2022deep}. The average energy costs are shown in Table \ref{tab:method}. 

%The average energy cost, runtime are tested for all methods. 

%We compare our framework with other mainstream trajectory planning methods in the energy cost. 
%\begin{enumerate}
%	\item Nearest Neighbor\cite{2013tsp-near} (NN). %UAVs only select the nearest IoTD to collect and return data center when running out of battery or data storage. 
%	%\item Genetic Algorithm in\cite{2020uav-GA-data-collect}. We set the population size as 1000, the mating probability as 0.8, the mutation probability as 0.02, and 100 iterations. 
%	\item Pointer Network\cite{2016PN-RL} (PN). %Pointer network is utilized as the trajectory model to generate UAV trajectory. 
%	\item Graph Neural Network\cite{2017-cvrp-gnn} (GNN). %Graph structure is utilized to extract features of IoTDs. 
%	\item Google OR-Tools \cite{ortools}. %OR-Tools is a fast and poweful software for combinatorial optimization developed by Google. We use OR-Tools to solve the UAV trajectory planning and compare the result with that of STP model. 
%	
%\end{enumerate}

\begin{table}[h]
	\caption{Average energy costs of different trajectory designers with different reinforcement trainers}
	\centering
	\setlength{\tabcolsep}{2.5mm}
	\renewcommand\arraystretch{1.25}
	\begin{tabular}{|p{35pt}<{\centering}|p{35pt}<{\centering}|p{35pt}<{\centering}|p{35pt}<{\centering}|p{35pt}<{\centering}|}\hline
		Method&{Reinforce}&{AC}&{A2C}&{TENMA}\\\hline
	%	NN\cite{2013tsp-near}&346.0686&\textbf{0.1160s}&459.3089&2.8541s\\
		PN\cite{bello2016neural}&287.88&287.42&285.04&285.83\\
		GNN\cite{2017-cvrp-gnn}&293.46&292.73&290.31&290.72\\
		GPN\cite{2019-gpn-rl}&275.48&273.25&272.85&274.46\\
	    ATOM&266.51&265.17&264.84&\textbf{264.32}\\
		\hline
	\end{tabular}
	\label{tab:method}
\end{table}





%\begin{table}[h]
%\caption{Comparison of runtime}
%\centering
%\begin{tabular}{|l|c|c|}\hline
%	Runtime&{Average (100)}&{Average (500)}\\\hline
%	Nearest Neighbor&\textbf{0.1160s}&2.8541s\\
%PN\cite{2016PN-RL}&0.1711s&1.1046s\\
%GNN\cite{2017-cvrp-gnn}&0.1531s&\textbf{1.0093s} \\
%	OURS&0.1991s&1.2044s\\
%	\hline
%\end{tabular}
%\label{tab:runtime}
%\end{table}
We can see from Table \ref{tab:method} that the energy cost of the AUTO framework (ATOM+TENMA) is the lowest. 
The reason can be interpreted by the following reasons: (1) The ATOM model with multi-head attention and graph operator is a powerful combinatorial optimization solver with high generalization. (2) The TENMA in the training process achieves the highest reward by minimizing the variance of the reward. 

Secondly, to evaluate the generalization of the AUTO framework, we apply all neural network-based designers to scenarios with different UAV battery capacities. The range of UAV battery capacity is set from $1,700$ to $2,550$ mAh. The simulation results of the energy costs are shown in Fig. \ref{fig:change_energy}, in which we can see that GNN gets the highest energy cost, followed by PN, GPN, and AUTO. It is clear that the AUTO framework achieves the lowest energy cost among all neural network-based methods.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=9cm]{4.png}
	\caption{Energy cost with different battery capacities of UAVs.}
	\label{fig:change_energy}
\end{figure} 

\subsection{Field Case Evaluation}

In this section, we design a field case to evaluate the performance of the AUTO framework in trajectory optimization. The simulation scenario is located at Yuelu Mountain in Hunan Province of China. We use $125$ IoTDs to monitor the air quality for three months. Each IoTD has an independent monitoring frequency, hence the amounts of sensory data collected in the IoTDs are different. The data center is located near the Changsha radio tower. 


\begin{table}[h]
	\caption{DJI Matrice 100 specifications.}
	\centering
	\setlength{\tabcolsep}{6mm}
	\renewcommand\arraystretch{1.25}
	\begin{tabular}{|p{100pt}<{\centering}|p{70pt}<{\centering}|}\hline
		Parameter&{Value}\\\hline
		
		UAV type & DJI Matrice 100\\
		Hovering time & 40 min\\
		Max flight speed & 17 m/s\\
		Max power consumption & 350 W\\
		Battery capacity & 5700 mAh\\
		Max charging power & 180 W\\
		Flight control & Programmable\\
		Max data storage capability & 500 MB\\
		\hline
	\end{tabular}
	
	\label{tab:UAV}
\end{table}

The programmable DJI Matrice 100 (M100) quad-copter drone with an open-source WPT hardware platform \cite{9184913} is achieved in the field case, whose specifications are summarized in Table \ref{tab:UAV}.
Fig. \ref{fig:route} shows the locations of $125$ IoTDs (blue dots) and data center (red cross). \textcolor{black}{The computation time of the trajectory planning is 1.04 s. The energy costs for different schemes are shown in Table \ref{tab:field}. Therefore, in this field case, the ATOM model can generate the best UAV trajectories, and all UAVs make maximum use of their batteries and data storage capacity.}
The trajectories of all UAVs are illustrated in Fig. \ref{fig:route}, 
in which the AUTO framework generates $7$ trajectories for UAVs. 

\begin{table}[h]
	\caption{The energy costs of different trajectory designers with different attention mechanisms}
	\centering
	\setlength{\tabcolsep}{2.5mm}
	\renewcommand\arraystretch{1.25}
	\begin{tabular}{|p{35pt}<{\centering}|p{55pt}<{\centering}|p{55pt}<{\centering}|p{40pt}<{\centering}|}\hline
		Method&{Luong-Atten}&{Area-Atten}&{MSA}\\\hline
		%	NN\cite{2013tsp-near}&346.0686&\textbf{0.1160s}&459.3089&2.8541s\\
		PN\cite{bello2016neural}&90.45&88.24&84.37\\
		GNN\cite{2017-cvrp-gnn}&98.77&98.23&92.14\\
		GPN\cite{2019-gpn-rl}&86.43&82.67&77.43\\
		ATOM&79.48&74.65&\textbf{72.73}\\
		\hline
	\end{tabular}
	\label{tab:field}
\end{table}





\begin{figure}[htbp]
	\centering
	\includegraphics[width=8cm]{5.png}
	%\caption{Comparison of Award for train without baseline (W/O), Exponential function (EXP) as baseline, Critic model (Critic) as baseline model, and the proposed baseline in UFO framework (UFO).}
	\caption{The generated trajectories of UAVs in the field case. }
	\label{fig:route}
\end{figure}


\section{Conclusions}
\label{sec:conclusion}

In this paper, we propose the AUTO framework to optimize the energy costs of UAVs in the WPT-assisted IoT system. In the AUTO framework, we first design the ATOM model based on graph transformer to generate the UAV trajectories with the lowest energy cost. In the ATOM model, the self-attention mechanism is utilized in graph encoder to extract the self-attention features of IoTDs. Then, the trajectory decoder generates the trajectories that can meet the resource constraints of UAVs. Next, we train the ATOM model by the TENMA method. 
Finally, the experiment results show that the performance of the AUTO framework surpasses other trajectory designers. 

%Despite the contributions of this paper, there are still some limitations that can serve as a basis for future research. First, more restrictive situations in real life, such as the existence of a no-fly zone, the actual loss of UAV, and unstable data transmission speed should be considered. Second, we will consider the collision-free trajectory optimization in the multi-UAV scenarios.



%Second, we consider adjusting and adopting the techniques of the state-of-the-art neural network models for the UAV-assisted data collection problem to get UAV trajectories with lower system energy cost. 
%Second, we should combine the characteristics of the UAV-assisted data collection problem with artificial intelligence to develop a more suitable framework. 
%In this paper, an efficient UAV planning framework UFO, which realizes UAV number optimization, association optimization, path planning and data collection under constraints of UAV data storage and battery, is proposed. The UFO contains a STP model and the SBPG training method. 
%The STP model consists of an Iencoder and a Tdecoder. In the Iencoder, the self-attention layers extract the scenario feature of information of IoTDs. In the Tdecoder, the number of UAV, association and trajectories are generated by inputting the scenario features and the results meet the constraints of UAV battery and data storage. 
%The UFO framework utilize SBPG to train STP model. The baseline, RSTP baseline in UFO framework, can reduce the reward standard deviation and make the STP model converge stable. 
%Finally, in the experimental part, the performance of UFO framework surpasses other mainstream trajectory planning methods, and achieves a good balance between performance and runtime. We also evaluate the STP model on a real life case. 
%

%The future work of this paper will be (1) consider applying the UFO framework to the situations that occur in real-world scenarios, such as the actual loss of UAV, unstable data transmission speed, etc. (2) Consider more restrictive situations, such as the existence of a no-fly zone in the scene. (3) Combine the characteristics of the UAV data collection problem with artificial intelligence to develop a more suitable method. 




%\begin{table}[h]
%	\caption{The data storage usage and battery remain of all UAVs}
%	\centering
%	\setlength{\tabcolsep}{4mm}
%	\begin{tabular}{|l|c|c|}\hline
%		UAV&{Data Storage Used (MB)}&{Battery Used (mAh)}\\\hline
%		0&472 / 512&1661 / 2550\\
%		1&493 / 512&1965 / 2550\\
%		2&441 / 512&1782 / 2550\\
%		3&483 / 512&1960 / 2550\\
%		4&430 / 512&1564 / 2550\\
%		5&500 / 512&2267 / 2550\\
%		6&373 / 512&1682 / 2550\\
%		7&509 / 512&2301 / 2550\\
%		8&346 / 512&1388 / 2550\\
%		9&481 / 512&1904 / 2550\\
%		\hline
%	\end{tabular}
%	\label{tab:constraints}
%\end{table}
%In Table \ref{tab:constraints}, "$a/b$" means that the total amount of resources is $b$, and the usage is $a$. 
%
%It can be seen from Fig. \ref{fig:route} that the STP model can generate optimal UAV trajectories efficiently. 
%And it is clear in Table \ref{tab:constraints} that all UAVs meet the constraints of storage space and battery capacity. 



%Aiming at the problem of UAV data collection, this paper proposes an efficient UAV planning method, which realizes UAV number optimization, association optimization, path planning and data collection under constraints of UAV data storage and battery. Then, we propose the UAV flight optimization (UFO) framework. In UFO, a Self-Attention Trajectory Planning (STP) model is utilized to get the number of UAV and trajectories in a certain scenario with various IoTDs. Once the trajectories are optimized, the association between UAVs and IoTDs and visiting order of IoTDs are solved, which is presented in evaluation stage of \textbf{Algorithm \ref{alg:overview}}. 
%STP model consists of an Iencoder and a Tdecoder. First, the self-attention layers in Iencoder extract the self-attention feature of IoTDs, which provides features for Tdecoder to generate UAV trajectories. Second, the Tdecoder outputs the  number of UAV and trajectories, and the trajectories result satisfies the constraints. 
%In order to make the STP model have stronger generalization ability and high-performance effects in different scenarios, the UFO framework utilize stochastic baseline policy gradient (SBPG) to train STP model. The baseline can make the training process more stable and converge faster. 
%Finally, in the experimental part, the performance of UFO framework surpasses other mainstream algorithms, and achieves a good balance between performance and runtime. We also evaluate the STP model on a real life case. 


%\begin{algorithm}
%	\caption{SFLA-FCM algorithm}
%	\label{alg1}
%	\begin{algorithmic}
%		\REQUIRE SFLA parameter: $p, m,l i t e r_{l} , l i t e r_{g}$ and FCM parameter $c_{\max }, \tau$.
%		\ENSURE optimal number of cluster $C_{o p t}$,cluster centers $\mathbf{C}$ ,partition matrix $\mathbf{U}$.
%		\WHILE{$2 \leq c<c_{\max }$}
%		\STATE{Initialize the frog population randomly. Each frog represents a feasible clustering center vector.}
%		\WHILE{$T<{l i t e r}_{g}$}
%		\STATE{Calculate the fitness value of each frog and save the global best frog $\mathbf{F}_{g}(c)$.}
%		\STATE{Divide the whole population into $m$ memeplexes by equation  (\ref{eq:Shi18})}
%		\FOR{each memeplex}
%		\WHILE{$t<{l i t e r}_{l}$}
%		\STATE{Update the location of the worst frog $\mathbf{F}_{w}$ based on FCM algorithm by Eqs. (\ref{eq:Shi15})-(\ref{eq:Shi17})}
%		\STATE{$t= t+1$.}
%		\ENDWHILE
%		\ENDFOR
%		\STATE{Shuffle the population.}
%		\STATE{$T= T+1$.}
%		\ENDWHILE
%		\STATE{Calculate the partition matrix $\mathbf{U}(\mathrm{c})$ according to $\mathbf{F}_{g}(c)$.}
%		\STATE{Calculate the average BWP value $a v g B W P(c)$ according to $\mathbf{F}_{g}(c)$.}
%		\STATE{$c= c+1$.}
%		\ENDWHILE
%		\STATE{$c_{o p t}=\underset{2 \leq c<\max }{\arg \max }\{\operatorname{avg} B W P(c)\}$.}
%		\STATE{$\mathbf{U}=\mathbf{U}\left(c_{o p t}\right), \mathbf{C}=\mathbf{F}_{g}\left(c_{o p t}\right)$.}
%	\end{algorithmic}
%\end{algorithm}


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%




% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}
\bibliographystyle{IEEEtran}

\bibliography{bare_jrnl}
\section*{Biographies}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,keepaspectratio]{PaperFig/LiDong}}]{Li Dong} received the B.S. and M.S. degrees in School of Physics and Electronics from Hunan Normal University, China, in 2004 and 2007, respectively. She received her Ph.D. degree in School of Geosciences and Info-physics from Central South University, China, in 2018. She is currently an associate professor at Hunan University of Technology and Business, China. Her research interests include Industrial Internet of Things, Machine Learning, and Mobile Edge Computing.
\end{IEEEbiography}
\vspace{-20 mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,keepaspectratio]{PaperFig/FeiboJiang}}]{Feibo Jiang} received his B.S. and M.S. degrees in School of Physics and Electronics from Hunan Normal University, China, in 2004 and 2007, respectively. He received his Ph.D. degree in School of Geosciences and Info-physics from Central South University, China, in 2014. He is currently an associate professor at the Hunan Provincial Key Laboratory of Intelligent Computing and Language Information Processing, Hunan Normal University, China. His research interests include  Industrial Internet of Things, Artificial Intelligence, and Mobile Edge Computing.
\end{IEEEbiography}
\vspace{-20 mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,keepaspectratio]{PaperFig/YuboPeng}}]{Yubo Peng} received the B.S. and M.S. degrees in computer science and technology from Hunan Normal University, Changsha, China, in 2019 and 2024. He is currently working toward the doctor’s degree in software engineering from the School of Intelligent Software and Engineering, Nanjing University, Nanjing, China.
His main research interests include Federated Learning and Semantic Communication.
\end{IEEEbiography}
\vspace{-20 mm}
\newpage
\end{document}


