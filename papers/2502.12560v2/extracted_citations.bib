@inproceedings{How_good_is,
    title = "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models",
    author = "Rust, Phillip  and
      Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Ruder, Sebastian  and
      Gurevych, Iryna",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.243",
    doi = "10.18653/v1/2021.acl-long.243",
    pages = "3118--3135",
    abstract = "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model{'}s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",
}

@misc{arabic,
      title={Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models}, 
      author={Mohamed Taher Alrefaie and Nour Eldin Morsy and Nada Samir},
      year={2024},
      eprint={2403.11130},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{domain,
  title={Reducing tokenizer’s tokens per word ratio in Financial domain with T-MuFin BERT Tokenizer},
  author={Seethalakshmi Gopalakrishnan and Victor Zitian Chen and Wenwen Dou and Wlodek Zadrozny},
  booktitle={FINNLP},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263610065}
}

@article{domain2,
author = {Barrett, Neil and Weber, Jens},
year = {2011},
month = {06},
pages = {S1},
title = {Building a Biomedical Tokenizer Using the Token Lattice Design Pattern and the Adapted Viterbi Algorithm},
volume = {12 Suppl 3},
journal = {BMC bioinformatics},
doi = {10.1186/1471-2105-12-S3-S1}
}

@misc{gemmateam2024gemma,
       title={Gemma: Open Models Based on Gemini Research and Technology}, 
       author={Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
       year={2024},
       eprint={2403.08295},
       archivePrefix={arXiv},
       primaryClass={cs.CL}
 }

@misc{japanese,
      title={How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese}, 
      author={Takuro Fujii and Koki Shibata and Atsuki Yamaguchi and Terufumi Morishita and Yasuhiro Sogawa},
      year={2023},
      eprint={2306.09572},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kim2024efficient,
      title={Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models}, 
      author={Seungduk Kim and Seungtaek Choi and Myeongho Jeong},
      year={2024},
      eprint={2402.14714},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llama3modelcard,

 title={Llama 3 Model Card}

@article{llama3openko,
  title={Llama-3-Open-Ko},
  author={L, Junbum},
  year={2024},
  url={https://huggingface.co/beomi/Llama-3-Open-Ko-8B}
}

@misc{llm_tokenizer,
      title={Tokenizer Choice For LLM Training: Negligible or Crucial?}, 
      author={Mehdi Ali and Michael Fromm and Klaudia Thellmann and Richard Rutmann and Max Lübbering and Johannes Leveling and Katrin Klug and Jan Ebert and Niclas Doll and Jasper Schulze Buschhoff and Charvi Jain and Alexander Arno Weber and Lena Jurkschat and Hammam Abdelwahab and Chelsea John and Pedro Ortiz Suarez and Malte Ostendorff and Samuel Weinbach and Rafet Sifa and Stefan Kesselheim and Nicolas Flores-Herr},
      year={2024},
      eprint={2310.08754},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{turkish,
   title={Impact of Tokenization on Language Models: An Analysis for Turkish},
   volume={22},
   ISSN={2375-4702},
   url={http://dx.doi.org/10.1145/3578707},
   DOI={10.1145/3578707},
   number={4},
   journal={ACM Transactions on Asian and Low-Resource Language Information Processing},
   publisher={Association for Computing Machinery (ACM)},
   author={Toraman, Cagri and Yilmaz, Eyup Halit and Şahi̇nuç, Furkan and Ozcelik, Oguzhan},
   year={2023},
   month=mar, pages={1–21} }

@misc{unpacking,
      title={Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance}, 
      author={Omer Goldman and Avi Caciularu and Matan Eyal and Kris Cao and Idan Szpektor and Reut Tsarfaty},
      year={2024},
      eprint={2403.06265},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

