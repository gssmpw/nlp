\section{Related Work}
\subsection{Classifiers Explainability}
Class Activation Maps (CAMs)~\cite{grad_cam1,grad_cam2} are a well-known technique for explaining classifier decisions, as they highlight the most influential regions in an image that affect the classifier's output. However, these methods typically require access to the classifier's architecture and all its layers, as they involve computing gradients of the outputs with respect to the inputs. Additionally, CAMs only indicate important regions in images without explicitly identifying the affected attributes, such as shape, color, or size. This can be limiting, particularly in microscopy images where subtle variations are of interest. Counterfactual visual explanations represent another family of methods aimed at explaining classifier decisions. These methods seek to identify minimal changes that would alter the classifier's decision. Generative models have been widely used to generate such counterfactual explanations. Generative Adversarial Networks (GANs), for instance, have been employed for this purpose~\cite{Singla2020,Lang_2021_ICCV,Goetschalckx_2019_ICCV_ganalyze}. While some approaches generate counterfactual explanations all at once~\cite{Singla2020,Goetschalckx_2019_ICCV_ganalyze}, the work in~\cite{Lang_2021_ICCV} identifies a set of attributes that influence the classifier's decision. However, GANs suffer from training instability due to the simultaneous optimization of two networks: the generator and the discriminator. Recently, diffusion models have demonstrated more stable training, superior generation quality, and greater diversity~\cite{diffusion_beat_gans,improved_denoising_diffusion_models}. They have also been adopted for generating visual counterfactual explanations~\cite{dvce,time_WACV,global_counterfactual_explanations}.


\subsection{Diffusion Models} Generative models have recently achieved significant success in various tasks~\cite{gan,score_based_generative_models,diffusion_beat_gans,vae}. Diffusion models~\cite{ddpm,ddim,diffusion_beat_gans,improved_denoising_diffusion_models}, a class of generative models, have been applied to different domain~\cite{diffusion_beat_gans,Diffusion_Bioinformatics,ldm}. These models consist of two processes: a known forward process that gradually adds noise to the input data, and a learned backward process that iteratively denoises the noised input. Numerous works have proposed improvements to diffusion models~\cite{diffusion_beat_gans,ldm,improved_denoising_diffusion_models}, enhancing their performance and making them the new state-of-the-art in generative modeling across different tasks. Recently, it has been shown that diffusion models can be used to learn meaningful representations of images that facilitate image editing tasks~\cite{diffusion_ae,diffusion_semantic_space}. In~\cite{diffusion_ae}, the authors proposed adding an encoder network during the training of diffusion models to learn a semantic representation of the image space. This approach enables the model to capture high-level features that can be manipulated for various applications. In~\cite{diffusion_semantic_space}, the authors modified the reverse process—introducing an asymmetric reverse process—to discover semantic latent directions in the space induced by the bottleneck of the U-Net~\cite{Unet} used as a denoiser in the diffusion model, which they refer to as the \textit{h-space}. By exploring this space, they were able to identify directions corresponding to specific semantic attributes, allowing for targeted image modifications. These advancements demonstrate the potential of diffusion models not only for high-quality data generation but also for learning rich representations that can be leveraged for downstream tasks.
\subsection{Detecting phenotypes in microscopy images}
Capturing the visual cellular differences in microscopy images under varying conditions is essential for understanding certain diseases and the effects of treatments~\cite{cellular_profiling,cellular_profiling_2,cellular_profiling_3, bourou,phenexp,bourou_2}. Historically, hand-crafted methods were employed to measure changes between different conditions~\cite{cellprofiler}. However, these tools have limitations, especially when the observed changes are subtle or masked by biological variability~\cite{phenexp,bourou_2}. Recently, generative models have been proposed to alleviate these limitations. In~\cite{bourou}, CycleGAN~\cite{cyclegan} was used to perform image-to-image translations, aiming to discard biological variability and retain only the induced changes. By translating images from one condition to another, the model focused on the specific alterations caused by the experimental conditions, effectively highlighting phenotypic differences. In~\cite{phenexp}, a conditional StyleGAN2~\cite{stylegan2} was trained to identify phenotypes by interpolating between classes in the StyleGAN's latent space. This approach enabled the generation of high-fidelity images that represent different phenotypic expressions, facilitating the study of subtle cellular variations and providing insights into the underlying biological processes. Furthermore, recent advancements have seen the use of conditional diffusion models in image-to-image translation~\cite{bourou_2}. In this method, an image from the source condition is first inverted into a latent code, that is used to generate corresponding the image from the target condition. This technique leverages the strengths of diffusion models in capturing complex data distributions and performing realistic translations between conditions. All of these methods have proven effective in uncovering phenotypes and enhancing the understanding of cellular differences. However, they rely solely on generative models and do not integrate classifiers that can extract patterns from images and assess how a given image would be transferred to another class. Incorporating discriminative models alongside generative approaches could enhance pattern recognition and provide a more comprehensive analysis of cellular changes, ultimately improving the assessment of disease progression and treatment effects.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{images/diffex_interview.drawio.png} % Replace with your image file name
  \caption{DiffEx primarily consists of three stages: \textbf{(a)} A semantic latent space is constructed by combining the embedding obtained from an encoder with the classifier's prediction for each image. The resulting representation is used to condition the DDIM. \textbf{(b)} Directional models are learned in this semantic latent space using a self-supervised approach. \textbf{(c)} After identifying the directions that most significantly affect the classification probability, we shift the images accordingly. For example, in the accompanying figure, a single image is shifted along the identified directions, resulting in visibly different images that highlight the changes induced by these directions.}
  \label{fig:main_figure}
\end{figure*}

\subsection{Contrastive learning}

Contrastive learning is a powerful self-supervised framework that has achieved remarkable success across various domains, including computer vision and natural language processing~\cite{simclr,clip,language_1,language_2}. By contrasting positive and negative pairs, it learns rich feature representations, maximizing similarity for positive pairs while minimizing it for negative ones using a contrastive loss~\cite{simclr,INFO_NCE,triplet_loss,contrastive_learning}. This versatile approach has been integrated into diverse architectures, enabling the extraction of robust and generalizable features for a wide range of downstream tasks. Beyond traditional applications, contrastive learning has also been leveraged in generative modeling. It has been employed to enhance conditioning in GANs~\cite{contragan} and to improve style transfer in diffusion models~\cite{diffusion_contra}. Discovering interpretable directions in generative models is fundamental to various image generation and editing tasks~\cite{latent_clr,noise_clr,diffusion_semantic_space}. In this context, contrastive learning has proven highly effective. For instance, LatentCLR~\cite{latent_clr} identifies meaningful transformations by applying contrastive learning to the latent space of GANs, while NoiseCLR~\cite{noise_clr} uncovers semantic directions in pre-trained text-to-image diffusion models like Stable Diffusion~\cite{stable_diffusion}.