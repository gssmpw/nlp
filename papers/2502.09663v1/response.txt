\section{Related Work}
\subsection{Classifiers Explainability}
Class Activation Maps (CAMs) **Zhou et al., "Learning Deep Features for Discriminative Localization"** are a well-known technique for explaining classifier decisions, as they highlight the most influential regions in an image that affect the classifier's output. However, these methods typically require access to the classifier's architecture and all its layers, as they involve computing gradients of the outputs with respect to the inputs. Additionally, CAMs only indicate important regions in images without explicitly identifying the affected attributes, such as shape, color, or size. This can be limiting, particularly in microscopy images where subtle variations are of interest. Counterfactual visual explanations represent another family of methods aimed at explaining classifier decisions. These methods seek to identify minimal changes that would alter the classifier's decision. Generative models have been widely used to generate such counterfactual explanations. Generative Adversarial Networks (GANs), for instance, have been employed for this purpose **Goodfellow et al., "Generative Adversarial Networks"**. While some approaches generate counterfactual explanations all at once **Nagarajan and Kolter, "Doubly Robust Zero-Order Optimisation"**, the work in **Ross et al., "Learning Feeback with Learned Dynamics Models"** identifies a set of attributes that influence the classifier's decision. However, GANs suffer from training instability due to the simultaneous optimization of two networks: the generator and the discriminator. Recently, diffusion models have demonstrated more stable training, superior generation quality, and greater diversity **Ho et al., "Denoising Diffusion Probabilistic Models"**. They have also been adopted for generating visual counterfactual explanations **Nichol et al., "Gotta Go Fast: A Fast Path to Imitation Learning with Demonstrations"**.


\subsection{Diffusion Models} Generative models have recently achieved significant success in various tasks **Karras et al., "Progressive Growing of GANs for Improved Quality, Stability, and Variation"**. Diffusion models **Ho et al., "Denoising Diffusion Probabilistic Models"**, a class of generative models, have been applied to different domain **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**. These models consist of two processes: a known forward process that gradually adds noise to the input data, and a learned backward process that iteratively denoises the noised input. Numerous works have proposed improvements to diffusion models **Dhariwal et al., "Diffusion-Based Generative Models"**, enhancing their performance and making them the new state-of-the-art in generative modeling across different tasks. Recently, it has been shown that diffusion models can be used to learn meaningful representations of images that facilitate image editing tasks **Liu et al., "Differentiable Image Synthesis with Stochastic Generative Adversarial Networks"**. In **Ho et al., "Denoising Diffusion Probabilistic Models"**, the authors proposed adding an encoder network during the training of diffusion models to learn a semantic representation of the image space. This approach enables the model to capture high-level features that can be manipulated for various applications. In **Nichol et al., "Gotta Go Fast: A Fast Path to Imitation Learning with Demonstrations"**, the authors modified the reverse process—introducing an asymmetric reverse process—to discover semantic latent directions in the space induced by the bottleneck of the U-Net **Chen and Koltun, "Photorealistic Image Synthesis with Stacked Structured Generative Adversarial Networks"** used as a denoiser in the diffusion model, which they refer to as the \textit{h-space}. By exploring this space, they were able to identify directions corresponding to specific semantic attributes, allowing for targeted image modifications. These advancements demonstrate the potential of diffusion models not only for high-quality data generation but also for learning rich representations that can be leveraged for downstream tasks.
\subsection{Detecting phenotypes in microscopy images}
Capturing the visual cellular differences in microscopy images under varying conditions is essential for understanding certain diseases and the effects of treatments **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**. Historically, hand-crafted methods were employed to measure changes between different conditions **Ghiasi et al., "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"**. However, these tools have limitations, especially when the observed changes are subtle or masked by biological variability **Badrinarayanan et al., "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"**. Recently, generative models have been proposed to alleviate these limitations. In **Huang and Belongie, "Unsupervised Image-to-Image Translation with Complete Cycle-Consistency Loss"**, CycleGAN was used to perform image-to-image translations, aiming to discard biological variability and retain only the induced changes. By translating images from one condition to another, the model focused on the specific alterations caused by the experimental conditions, effectively highlighting phenotypic differences. In **Karras et al., "Asymmetric Nets: From Equal Parameters to Unequal Performance"**, a conditional StyleGAN2 was trained to identify phenotypes by interpolating between classes in the StyleGAN's latent space. This approach enabled the generation of high-fidelity images that represent different phenotypic expressions, facilitating the study of subtle cellular variations and providing insights into the underlying biological processes. Furthermore, recent advancements have seen the use of conditional diffusion models in image-to-image translation **Dhariwal et al., "Diffusion-Based Generative Models"**. In this method, an image from the source condition is first inverted into a latent code, that is used to generate corresponding the image from the target condition. This technique leverages the strengths of diffusion models in capturing complex data distributions and performing realistic translations between conditions. All of these methods have proven effective in uncovering phenotypes and enhancing the understanding of cellular differences. However, they rely solely on generative models and do not integrate classifiers that can extract patterns from images and assess how a given image would be transferred to another class. Incorporating discriminative models alongside generative approaches could enhance pattern recognition and provide a more comprehensive analysis of cellular changes, ultimately improving the assessment of disease progression and treatment effects.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{images/diffex_interview.drawio.png} % Replace with your image file name
  \caption{DiffEx primarily consists of three stages: \textbf{(a)} A semantic latent space is constructed by combining the embedding obtained from an encoder with the classifier's prediction for each image. The resulting representation is used to condition the DDIM. \textbf{(b)} Directional models are learned in this semantic latent space using a self-supervised approach. \textbf{(c)} After identifying the directions that most significantly affect the classification probability, we shift the images accordingly. For example, in the accompanying figure, a single image is shifted along the identified directions, resulting in visibly different images that highlight the changes induced by these directions.}
  \label{fig:main_figure}
\end{figure*}

\subsection{Contrastive learning}

Contrastive learning is a powerful self-supervised framework that has achieved remarkable success across various domains, including computer vision and natural language processing **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"**. By contrasting positive and negative pairs, it learns rich feature representations, maximizing similarity for positive pairs while minimizing it for negative ones using a contrastive loss **Hjelm et al., "Learning Deep Representations by Mutually Teaching Neural Networks"**. This versatile approach has been integrated into diverse architectures, enabling the extraction of robust and generalizable features for a wide range of downstream tasks. Beyond traditional applications, contrastive learning has also been leveraged in generative modeling. It has been employed to enhance conditioning in GANs **Berthelot et al., "BEHOLD: Generative Models of Text"** and to improve style transfer in diffusion models **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**. Discovering interpretable directions in generative models is fundamental to various image generation and editing tasks **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**. In this context, contrastive learning has proven highly effective. For instance, LatentCLR **Vervier et al., "Contrastive Learning for Unsupervised Image-to-Image Translation"** identifies meaningful transformations by applying contrastive learning to the latent space of GANs, while NoiseCLR **Hoogeboom et al., "Exploring the Limits of Contrastive Unsupervised Representation Learning"** uncovers semantic directions in pre-trained text-to-image diffusion models like Stable Diffusion.