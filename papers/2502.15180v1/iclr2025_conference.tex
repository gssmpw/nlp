
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{aux/math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% custom packages
% \usepackage[table,xcdraw]{xcolor}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{graphicx}
% \usepackage{subfig}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}

% Define a custom color for shallow gray
\definecolor{shallowgray}{rgb}{0.6, 0.6, 0.6}

\usepackage{xspace}
\def \prophet {\includegraphics[height=22pt]{figures/portrait.png}\xspace}

\title{\prophet \textcolor[HTML]{BE56BE}{Occ}\textcolor[HTML]{75C175}{Prophet}: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\author{Junliang Chen$^{*}$,~~Huaiyuan Xu$^{*}$,~~Yi Wang$^\dagger$,~~Lap-Pui Chau$^\dagger$\\ 
The Hong Kong Polytechnic University\\ 
\texttt{jun-liang.chen@connect.polyu.hk}\\
\texttt{\{huaiyuan.xu, yi-eie.wang, lap-pui.chau\}@polyu.edu.hk}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\def\customfootnotetext#1#2{{%
  \let\thefootnote\relax
  \footnotetext[#1]{#2}}}

\customfootnotetext{1}{$^{*}$ Equal contribution  \quad $^{\dagger}$ Corresponding author}

\begin{abstract}
Predicting variations in complex traffic environments is crucial for the safety of autonomous driving. Recent advancements in occupancy forecasting have enabled forecasting future 3D occupied status in driving environments by observing historical 2D images. However, high computational demands make occupancy forecasting less efficient during training and inference stages, hindering its feasibility for deployment on edge agents. In this paper, we propose a novel framework, \textit{i.e.}, OccProphet, to efficiently and effectively learn occupancy forecasting with significantly lower computational requirements while improving forecasting accuracy. OccProphet comprises three lightweight components: Observer, Forecaster, and Refiner. The Observer extracts spatio-temporal features from 3D multi-frame voxels using the proposed Efficient 4D Aggregation with Tripling-Attention Fusion, while the Forecaster and Refiner conditionally predict and refine future occupancy inferences. Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasets demonstrate that OccProphet is both training- and inference-friendly. OccProphet reduces 58\%$\sim$78\% of the computational cost with a 2.6$\times$ speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves 4\%$\sim$18\% relatively higher forecasting accuracy. Code and models are publicly available at \url{https://github.com/JLChen-C/OccProphet}.

\end{abstract}

\section{Introduction}

\begin{figure*}[ht]
    \vspace{-7pt}
    \centering
    \begin{minipage}{0.64\textwidth}
    \includegraphics[width=\textwidth]{figures/short_illustra.pdf}
    \label{fig:short_illstra}
    \vspace{-21pt}
    \caption{Illustration of OccProphet. OccProphet only receives multi-camera video input and produces future occupancies.}
    \end{minipage}\hfill\hspace{1pt}
    \begin{minipage}{0.34\textwidth}
    \includegraphics[width=\textwidth]{figures/comp_fps_iou.pdf}
    \vspace{-23pt}
    \caption{Comparison of performance between Cam4DOcc and OccProphet.}
    \label{fig:comp_fps_iou}
    \end{minipage}
    \vspace{-7pt}
\end{figure*}

Autonomous driving holds significant promise for reshaping transportation and urban mobility. Perceiving 3D surroundings is critical for autonomous vehicles. There are typically two 3D perception paradigms: detection-based perception and occupancy-based perception. The detection-based paradigm, such as monocular or multi-view 3D object detection \citep{mono3d,fcos3d,bevdet,detr3d}, equips autonomous vehicles with 3D perception capabilities by detecting traffic participants and assigning them 3D bounding boxes. However, due to limitations in pre-defined object categories and rigid detection boxes, the detection-based paradigm struggles to generalize to unknown objects and irregular structures, which are likely to appear in real-world traffic scenarios. To relieve these constraints, the occupancy-based perception paradigm \citep{tpvformer,surroundocc,openoccupancy,occ3d,occsurvey} offers a more general and fine-grained representation of the environment through learning occupied states in 3D space. This paradigm provides a stronger perception ability to autonomous vehicles, enabling them to better comprehend complex traffic environments.

Despite advancements in occupancy perception, most existing methods can only perceive the past and present states of the environment. They lack the ability to capture and understand environmental dynamics and subsequently forecast the future scene. Forecasting is essential for safe planning, which assists autonomous vehicles to avoid potential collisions \citep{ssc,pip,epsilon,gameformer,vlp}. Although some bird's-eye view (BEV) approaches have achieved object motion forecasting in the environment\citep{fiery,beverse,tbpformer,motionperceiver}, their forecasts are restricted on a 2D plane. This 2D forecasting limits the comprehensive understanding of the entire 3D dynamic scene. In light of this limitation, occupancy forecasting methods \citep{spf2,s2net,selfpoint4d,point4docc,uno} shift to predict future 3D occupancy for the whole environment. However, these methods rely on point cloud inputs from expensive LiDAR kits. To explore more cost-effective solutions, Cam4DOcc \citep{cam4docc} introduces a camera-only benchmark and baseline for occupancy forecasting.

While Cam4DOcc achieves remarkable performance in occupancy forecasting compared to its counterparts, the high computational cost makes it less efficient during training and inference. It hampers feasibility of deployment on edge agents, such as autonomous vehicles that operate under restricted computational budgets. In this paper, we propose a novel framework, dubbed as \textit{OccProphet} (shown in \Figref{fig:short_illstra}), to efficiently and effectively perform camera-only occupancy forecasting. In OccProphet, we design three lightweight components to forecast future states: the \textit{Observer}, \textit{Forecaster}, and \textit{Refiner}. The Observer adopts 4D feature aggregation and a tripling-attention fusion strategy on the reduced-resolution features to extract spatio-temporal information efficiently. The Forecaster then infers future states according to the scene condition and the Observer's outputs. Finally, the Refiner enhances the quality of the forecast results through spatio-temporal interactions. The main contributions of this paper are summarized as follows:
\vspace{-5pt}
\begin{itemize}[leftmargin=*]
\item We propose OccProphet, a novel camera-only occupancy forecasting framework, which is both efficient and effective during training and inference, towards on-vehicle deployment.
\item We design a lightweight Observer-Forecaster-Refiner pipeline for OccProphet. The Observer extracts spatio-temporal features from historical observations; the Forecaster conditionally predicts coarse future states; the Refiner promotes forecasting accuracy.
\item Experimental results demonstrate that OccProphet achieves higher forecasting accuracy with less than half the computational cost of Cam4DOcc. These improvements are consistently observed across the nuScenes \citep{nuscenes}, Lyft-Level5 \citep{lyftlevel5}, and nuScenes-Occupancy \citep{nuscenesoccupancy} datasets, highlighting the superior efficiency and effectiveness of OccProphet (shown in \Figref{fig:comp_fps_iou}).
\end{itemize}

\vspace{-12pt}
\section{Related Work}
% \vspace{-7pt}

\vspace{-5pt}
\subsection{Occupancy Prediction}
\vspace{-5pt}

Occupancy prediction aims at modeling the current 3D occupancy layout in space, by observing historical and current environments. Occupancy prediction is adept at providing 3D dense descriptions for complex traffic scenarios, thereby garnering increasing attention from academia and industry. SSCNet \citep{sscnet} was the first semantic occupancy prediction work, which simultaneously predicted occupied voxels and their semantics for an indoor scene using a depth image. MonoScene \citep{monoscene} extends SSCNet to outdoor scenarios by using an RGB image and incorporating stronger supervisions. Training and evaluating occupancy prediction networks require benchmarks with ground truth occupancy labels, which are challenging due to the complexity of densely annotating 3D outdoor driving scenes. Wang \textit{et al.} \citep{openoccupancy} propose OpenOccupancy, the first large-scale benchmark for semantic occupancy prediction, which covers multiple sensing modalities and provides high-resolution dense occupancy annotations. Occ3d, developed by Tian \textit{et al.} \citep{occ3d}, is another widely used benchmark for occupancy prediction, whose high-quality labels are from the proposed technique of image-guided occupancy label refinement.

Considering the inherently dense nature of depicting 3D environments, ideal occupancy perception emphasizes balancing efficiency and accuracy. Although some studies explore sparse queries \citep{voxformer} or tri-perspective view (TPV) representation \citep{tpvformer} to elevate the efficiency of occupancy prediction, they inevitably sacrifice fine-grained details of 3D space. In contrast, many other methods \citep{occformer,surroundocc,cotr} utilize 3D feature volumes to preserve 3D details of the scene, leading to higher occupancy prediction accuracy. Recently, COTR \citep{cotr} proposes a compact occupancy representation that preserves geometric details while reducing computational costs. Despite the significant advancements in occupancy prediction, including comprehensive benchmarks and powerful algorithms, all existing methods focus exclusively on current occupancy and overlook future occupancy, which reflects potential variations in the 3D environment.

\vspace{-3pt}
\subsection{Occupancy Forecasting}
\vspace{-3pt}

Occupancy forecasting targets to predict future occupancy, starting from the current timestamp. Previous dominant works mainly adopt the BEV perspective for occupancy forecasting, reasoning about 2D occupancy changes on a BEV plane. For example, FIERY \citep{fiery} extracts BEV features from multi-view image inputs and utilizes a temporal model with 3D convolution to capture spatio-temporal states, which are then used to recursively forecast future instances states. BEVerse \citep{beverse} introduces a unified BEV representation framework that jointly achieves object perception and occupancy forecasting using multi-task supervision. To better align spatio-temporal information, TBP-Former \citep{tbpformer} designs a pose-synchronized BEV encoder to synchronize multi-frame BEV features during occupancy forecasting. While these BEV-based methods deliver impressive performance for forecasting pre-defined semantic categories (\textit{e.g.}, vehicles), they struggle to (1) forecast the motion of out-of-distribution objects and (2) capture height information in the environment. In contrast, our method forecasts class-agnostic occupancy from a 3D perspective, rather than BEV, thus enabling autonomous vehicles to monitor, comprehend, and reason about 3D dynamics in the physical world. 

To address the limitations of BEV occupancy forecasting, researchers have recently shifted their focus toward forecasting 3D occupancy without considering semantics. 
Specifically, Khurana \textit{et al.} \citep{point4docc} treat LiDAR point cloud forecasting as a proxy task for the occupancy forecasting task, where point cloud rendering is used to bridge the two tasks. UnO \citep{uno} takes LiDAR point clouds as input and performs occupancy forecasting using the proposed unsupervised learning paradigm, in which the forecasted occupancy should align with pseudo occupancy labels generated from future LiDAR data. However, these methods rely on point clouds from expensive LiDAR kits, leading to increased costs when implemented in autonomous vehicles. 

Compared to LiDAR-based approaches, camera-only occupancy forecasting offers a promising alternative with significantly lower costs. Cam4DOcc \citep{cam4docc} introduces a comprehensive benchmark and dataset to evaluate camera-only occupancy forecasting algorithms on both movable and static objects beyond pre-defined categories. Additionally, it proposes a strong camera-only baseline for occupancy forecasting.  However, this approach is still far from real-world application due to its high computational demands. Drive-OccWorld \citep{driveoccworld} introduces extra action condition inputs and planning supervision to enhance performance. In this paper, we propose a novel end-to-end framework with faster speed, and higher accuracy, enabling efficient and effective occupancy forecasting in a pure camera-only setting.

\vspace{-3pt}
\section{OccProphet}
\vspace{-3pt}
\subsection{Overview}
\vspace{-3pt}
The overview of the proposed OccProphet is illustrated in \Figref{fig:overview}. Given multi-frame surround-view RGB images as input, 2D features are extracted using a shared image encoder. These 2D features are subsequently lifted into 3D space and aggregated into multi-frame 3D voxel features through depth estimation and voxel pooling. We design the following unshared pipeline for each of the occupancy and occupancy flow branches. Specifically, the pipeline consists of four components: the Observer, Forecaster, Refiner, and Predictor. The Observer module efficiently and effectively aggregates spatio-temporal information within multi-frame observations (\textit{i.e.}, multi-frame 3D voxel features). The Observer’s output then undergoes a Forecaster, which adaptively predicts future states, ensuring flexibility across diverse traffic conditions. The Refiner module further enhances the quality of these predictions by enabling cross-frame interactions. Finally, the Predictor module decodes the refined future states into either occupancy or occupancy flow.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \vspace{-22pt}
    \caption{Overview of OccProphet. It receives multi-frame images from surround-view cameras as input and outputs future occupancy or occupancy flow. It consists of four key components: the Observer, Forecaster, Refiner, and Predictor. The Observer module aggregates spatio-temporal information. The Forecaster module conditionally generates preliminary representations of future scenarios. These preliminary representations are refined by the Refiner module. Finally, the Predictor module produces the final predictions of future occupancy or occupancy flow.}
    \label{fig:overview}
    \vspace{-12pt}
\end{figure*}

\vspace{-7pt}
\subsection{Observer}
\vspace{-7pt}

The Observer takes the 4D motion-aware feature $F_{motion}$ as input, and generates a spacetime-aware representation. Let $\mathcal{I}_{t}=\{I_{t}^{v}\}_{v=1}^{N_{cam}}$ denote the multi-camera RGB images at timestamp $t \in \{1, \dots, T\}$, where $N_{cam}$ refers to the total number of surround-view cameras, and $T$ is the total number of input frames. A shared image encoder (\textit{i.e.}, ResNet \citep{resnet}) is applied to $\{\mathcal{I}_{t}|t=1, \dots, T\}$ to extract 2D features. For each frame, these 2D features are projected to 3D space and then aggregated into voxelized 3D features \citep{lss}. The 3D features from multiple frames are aligned into the current-frame coordinate system using 6 degrees of freedom (6-DoF) ego-vehicle poses. We then concatenate the aligned 3D features into a 4D feature $F \in \R^{ T \times C \times X \times Y \times Z}$, where $C$ is the number of channels, and $(X, Y, Z)$ represents the size of voxelized 3D feature volume. Subsequently, the motion-aware 4D feature $F_{motion} \in \R^{T \times (C+6) \times X \times Y \times Z}$ is generated by concatenating 6-DoF ego-vehicle poses. 

To generate the spacetime-aware representation, directly processing the 4D motion-aware feature using convolutional operations is intuitive. However, the direct processing imposes a substantial computational burden, and ignores the fact that a large portion of the 3D space is unoccupied, which leads to the inherent sparsity of the motion-aware feature. To address this issue, we efficiently and effectively generate the spacetime-aware feature from $F_{motion}$ using the Observer module. The Observer comprises an Efficient 4D Aggregation module and a Tripling-Attention Fusion module.

\vspace{-4pt}
\subsubsection{Efficient 4D Aggregation}
\label{sec:e4a}
\vspace{-3pt}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-12pt}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/e4a.pdf}
    \vspace{-20pt}
    \caption{Efficient 4D Aggregation (E4A).}
    \label{fig:e4a}
    \vspace{-20pt}
\end{wrapfigure}

Directly aggregating the original 4D feature $F_{motion}$ will incur a high computational cost. For efficiency, we design the Efficient 4D Aggregation (E4A) module to first produce compact features through downsampling, and then exploit spatio-temporal interactions on the compact features to achieve aggregation, followed by the upsampling process to compensate for the information loss.
The architecture of the E4A module is shown in \Figref{fig:e4a}. We first reduce the channel number of $F_{motion}$ from $(C+6)$ to $C$ through a 3D convolution, thereby forming a feature $O \in \mathbb{R}^{T \times C \times X \times Y \times Z}$. We gradually downsample $O$ to reduce computation during aggregation. However, such downsampling inevitably results in non-negligible information loss, especially for small objects. To compensate for this loss, we conduct two operations. On one hand, we perform spatio-temporal interactions on the downsampled features, that is, the Tripling-Attention Fusion module (to be illustrated in \Secref{sec:hybridfusion}). On the other hand, the post-interaction features are upsampled, and further summed with the features at the same resolution prior to downsampling. These two operations continue until the resolution of the upsampled feature matches the resolution of the initial motion-aware feature.

\begin{wraptable}{r}{0.35\textwidth}
\vspace{-22pt}
\centering
\renewcommand\arraystretch{1.37}
\caption{Comparisons with different representation styles. N$_{p}$: Total number of parameters.}
\scalebox{0.73}{
    \begin{tabular}{c|c|c|c}
    \hline
    Style & N$_{p}$ (M) & FLOPs (G) & $\tilde{\text{IoU}}_{f}$ (\%) \\
    \hline
    BEV & 102 & 1370 & 26.50 \\
    TPV & 181 & 1574 & 26.96 \\
    \rowcolor[HTML]{F9F2FE}
    E4A & 67 & 1737 & \textbf{27.50} \\
    \hline
    \end{tabular}
}
\label{tab:comp_flops_repr_styles}
\end{wraptable}

The output 4D representation of the E4A module possesses spatio-temporal awareness. It effectively preserves the 3D geometric details of the environment--details that the BEV and TPV representations are unable to capture. As shown in Table \ref{tab:comp_flops_repr_styles}, using the E4A representation achieves higher performance than the BEV and TPV representations with fewer parameters and a slight increase in computational costs.

\subsubsection{Tripling-Attention Fusion}
\label{sec:hybridfusion}

\begin{wrapfigure}{r}{0.65\textwidth}
    \vspace{-12pt}
    \centering
    \includegraphics[width=0.65\textwidth]{figures/hf_and_tripling.pdf}
    \vspace{-18pt}
    \caption{Tripling-Attention Fusion (left) and Tripling (right).}
    \label{fig:hf_and_tripling}
    \vspace{-7pt}
\end{wrapfigure}

A 4D feature can be considered as a combination of multiple 3D voxel-wise features along the temporal dimension. The Tripling-Attention Fusion (TAF, shown in \Figref{fig:hf_and_tripling}) module is specifically designed to facilitate spatio-temporal interactions across multiple 3D features, as depicted on the left of \Figref{fig:hf_and_tripling}. Notably, downsampling reduces computational cost by lowering feature resolution, while the Tripling-Attention Fusion module makes a further step through the proposed tripling operation, as illustrated on the right of \Figref{fig:hf_and_tripling}. 

The tripling operation is designed to understand the 3D space from three complementary and compact perspectives, which can retain 3D scene information with fewer computational costs. Specifically, a tripling operation decomposes a 3D feature into three distinct branches: scene, height, and BEV.
This decomposition compresses 3D features into 1D and 2D features, significantly reducing the subsequent computational overhead. The scene branch can extract the global context of the corresponding frame, providing an overall understanding of the scenario. The height branch retains vertical details, serving as complementary clues to the 2D BEV branch to enhance the representation capability of the 3D geometric information. The three branches can be computed as follows:
\begin{gather}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\label{eq:tripling}
    \mathcal{S} =  \text{Act}(\text{Norm}(\text{Linear}(\text{GAP}_{\text{3D}}(U)))),\\
    \mathcal{H} = \text{Act}(\text{Norm}(\text{Conv}_{\text{1D}}(\text{GAP}_{\text{2D}}(U)))),\\
    \mathcal{B} = \text{W-MSA}(\text{GAP}_{\text{1D}}(U)),
\end{gather}
where $U \in \R^{T \times C \times \frac{X}{2^{i}} \times \frac{Y}{2^{i}} \times \frac{Z}{2^{i}}}$ denotes the $i$-th downsampled feature input to the Tripling-Attention Fusion module. GAP$_{\text{3D}}$, GAP$_{\text{2D}}$, and GAP$_{\text{1D}}$ are 3D, 2D, and 1D global average pooling. Linear, Conv$_{\text{1D}}$, Norm, and Act refer to a single linear layer, 1D convolution, normalization, and activation, respectively. W-MSA is the window-based multi-head self-attention block \citep{swin}. $\mathcal{S} \in \R^{T \times C \times 1 \times 1 \times 1}$, $\mathcal{H} \in \R^{T \times C \times 1 \times 1 \times \frac{Z}{2^{i}}}$, and $\mathcal{B} \in \R^{T \times C \times \frac{X}{2^{i}} \times \frac{Y}{2^{i}} \times 1}$ denote the outputs of scene, height, and BEV branches. After the tripling operation, we interact and fuse scene, height, and BEV features. Specifically, we first independently apply the temporal attention along the time axis to different branches, then sum over the three branches using the broadcast technique. The process is formulated as follows:
\begin{equation}
\label{eq:ta_and_sum}
    U_{\text{HF}} = \text{TA}_{\text{scene}}(\mathcal{S}) \oplus \text{TA}_{\text{height}}(\mathcal{H}) \oplus \text{TA}_{\text{BEV}}(\mathcal{B}),
\end{equation}
where $\text{TA}_{\text{scene}}$, $\text{TA}_{\text{height}}$, and $\text{TA}_{\text{BEV}}$ denote temporal attention for scene, height, and BEV branches; $\oplus$ is broadcast-style plus; $U_{\text{HF}} \in \R^{T \times C \times \frac{X}{2^{i}} \times \frac{Y}{2^{i}} \times \frac{Z}{2^{i}}}$ is the output feature of the TAF module.

\subsection{Forecaster}

\begin{wrapfigure}{r}{0.38\textwidth}
    \vspace{-7pt}
    \centering
    \includegraphics[width=0.38\textwidth]{figures/forecaster.pdf}
    \vspace{-22pt}
    \caption{Architecture of Forecaster.}
    \label{fig:forecaster}
    \vspace{-10pt}
\end{wrapfigure}

Given a spatio-temporal representation $O_{\text{obs}} \in \R^{T \times C \times X \times Y \times Z}$ output from the Observer, the Forecaster (shown in \Figref{fig:forecaster}) is supposed to generate future states. We first reshape $O_{\text{obs}}$ by collapsing the time axis into the channel axis, resulting in the reshaped features $O_{\text{obs}}^{\prime} \in \R^{TC \times X \times Y \times Z}$. A straightforward approach to forecasting is using a single linear layer to predict features for future frames. This approach encourages the network to learn static weights to fit different traffic scenarios. However, traffic situations vary significantly across different traffic scenarios, presenting distinct challenges in predicting future changes using a single linear layer. For instance, forecasting environmental changes is more difficult in a crowded intersection with many moving objects than on an empty highway with few vehicles. The spatio-temporal complexity in the former is far greater than in the latter.

Considering these challenges, we propose forecasting occupancy with flexibility to adapt to various traffic scenarios featuring diverse spatio-temporal complexities. To achieve this, we design a novel Forecaster module that predicts future states based on the overall environmental condition. The Forecaster comprises a Condition Generator and a Conditional Forecaster.
We first use a Condition Generator comprising a 3D GAP and a shared linear layer across different frames to extract the condition from the observation $O_{\text{obs}}$:
\begin{equation}
\label{eq:cond}
    F_{\text{cond}}^{\prime} = \text{Act}(\text{Norm}(\text{Linear}(\text{GAP}_{\text{3D}}(O_{\text{obs}})))),
\end{equation}
where $F_{\text{cond}}^{\prime} \in \R^{T \times C \times 1 \times 1 \times 1}$ denotes the overall environmental condition, which is then reshaped into $F_{\text{cond}} \in \R^{TC}$ by collapsing the time axis into the channel axis. $F_{\text{cond}}$ is subsequently passed to a Conditional Forecaster to predict future states. Specifically, a linear layer is applied to $F_{\text{cond}}$ to produce adaptive weights for specific traffic scenarios. Another linear layer is then used to predict future states conditioned by these adaptive weights.
\begin{equation}
\label{eq:weights_and_forecast}
    W_{cond} = \text{Linear}(F_{\text{cond}}), F^{\prime}_{\text{future}} = \text{Linear}(O_{\text{obs}}^{\prime}|W_{cond}),
\end{equation}
where $W_{cond} \in \R^{TC \times T^{\prime}C}$ denotes the adaptive weights, $F_{\text{future}}^{\prime} \in \R^{T^{\prime}C \times X \times Y\times Z}$ is the future states of $T^{\prime}$ future frames. $F_{\text{future}}^{\prime}$ is reshaped into $F_{\text{future}} \in \R^{T^{\prime} \times C \times X \times Y \times Z}$ to recover the time axis. $F_{\text{future}}$, as the preliminary feature for future environments, is then refined by the Refiner module.

\subsection{Refiner}

Since the Forecaster module predicts $F_{\text{future}}$ using linear projection, it inevitably lacks cross-frame interactions. The Refiner is designed to enhance the forecasted results via further interactions between future frames, as well as incorporating historical frames as supplementary information.
The E4A module, described in Section \Secref{sec:e4a}, is a spatio-temporal interaction module. Furthermore, taking a review of the E4A, for any input feature $Q \in \R^{T \times C \times X \times Y \times Z }$, the function of the E4A module can be formulated as:
\begin{equation}
\label{eq:review_e4a}
    Q^{\prime} = \text{E4A}(Q) = Q + \mathcal{F}(Q),
\end{equation}
where $Q^{\prime} \in \R^{T \times C \times X \times Y \times Z}$ is the output feature of E4A, and $\mathcal{F}$ denotes the transformation function. Considering residual networks can aid in refinement and network optimization \citep{resnet}, it is reasonable to regard E4A as a \textit{refinement transformation} for features, which also reduces the learning complexity of earlier modules. Building on this insight, we further introduce a Refiner reusing the E4A architecture to refine the forecasted future states.
The Refiner is applied to the forecasted feature $F_{\text{future}}$ from the Forecaster and the feature output $O_{\text{obs}}$ from the Observer, producing an enhanced feature $V=E4A([O_{\text{obs}},F_{\text{future}}])_{T+1:T+T^{\prime},:,:,:,:} \in \R^{T^{\prime} \times C \times X \times Y \times Z}$ for subsequent forecasting of occupancy and flow.

For fair comparisons, the Predictor for occupancy and occupancy flow prediction, and the overall training loss function follow those in Cam4DOcc \citep{cam4docc}.

\section{Experiments}

\vspace{-3pt}
\subsection{Experimental Setups}
\vspace{-3pt}

\paragraph{Datasets.}

Following Cam4DOcc, 700 out of 850 scenes in nuScenes \citep{nuscenes} and nuScenes-Occupancy \citep{nuscenesoccupancy} datasets, and 130 out of 180 scenes in Lyft-Level5 \citep{lyftlevel5} with occupancy labels are used for training, while the remaining scenes are used for evaluation. For nuScenes and nuScenes-Occupancy, the numbers of sequences for training and evaluation are 23930 and 5119, respectively. For Lyft-Level5, 15720 and 5880 sequences are used for training and evaluation, respectively. The total length of the sequence is set as 7, including 3 frames as observations (2 past frames and 1 present frame) and 4 future frames for forecasting. The range of the occupancy labels is $[-51.2\ m, 51.2\ m]$ for $x$ and $y$ axes, and $[-5\ m, 3\ m]$ for $z$ axis. The voxel resolution is $0.2\ m$, and the grid size is $(512, 512, 40)$ for occupancy labels. The forecasting performances are reported with different time intervals due to the different annotated frequencies, \textit{i.e.}, 2 Hz for nuScenes and nuScenes-Occupancy, and 5 Hz for Lyft-Level5.

\vspace{-7pt}
\paragraph{Evaluation Protocol and Metrics.} To fully evaluate the forecasting performance, we follow Cam4DOcc to adopt the following three-level \textbf{camera-only} occupancy forecasting tasks: (1) \textbf{Forecasting inflated general movable objects (GMO)}: the categories for occupancy grids within bounding boxes from nuScenes and Lyft-Level5 datasets are defined as GMO, while the remaining categories are defined as others. (2) \textbf{Forecasting fine-grained GMO}: the same category definition as (1), while the bounding box labels of GMO are replaced with fine-grained voxel-wise annotations from the nuScenes-Occupancy dataset. (3) \textbf{Forecasting fine-grained GMO and fine-grained general
static objects (GSO)}: the labels of GMO and GSO are from fine-grained annotations. For Lyft-Level5, the evaluation is only conducted on task (1) due to the lack of fine-grained occupancy annotations. For all tasks, intersection over union (IoU (\%)) is adopted as the evaluation metric to evaluate occupancy estimation performance for each category of the current frame ($\text{IoU}_{c}$), and any of future frames ($\text{IoU}_{f}$), and the whole span ($\tilde{\text{IoU}}_{f}$). More details can be found in Cam4DOcc.

\vspace{-7pt}
\paragraph{Implementation Details.} The proposed OccProphet takes 6 images with $448 \times 800$ pixels from different surround views. Following Cam4DOcc, we use ResNet \citep{resnet} pre-trained on ImageNet \citep{imagenet} with FPN \citep{fpn} as the image encoder. For ablation studies, we use ResNet18 for efficiency, while ResNet34 with deformable convolution \citep{dcn} is adopted for main results. All models are trained with a batch size of 4 on 4 RTX 4090 GPUs and tested on a single RTX 4090 with 24G memory. AdamW \citep{adamw} optimizer with an initial learning rate of 3e-4 and a weight decay of 0.01 is adopted to train the models.

\vspace{-4pt}
\subsection{Main Results}
\vspace{-4pt}

\paragraph{Evaluation on forecasting inflated GMO.} The results of forecasting inflated GMO are listed in Table \ref{tab:inf-GMO}, with five comparison methods in total: OpenOccupancy-C, SPC, PowerBEV-3D, BEVDet4D, and OCFNet (Cam4DOcc). OccProphet achieves higher performance than all the compared approaches. Specifically, on the nuScenes dataset, OccProphet surpasses the BEV-based approaches PowerBEV-3D and BEVDet4D by 2.76-11.28\%, 2.07-5.69\%, 2.28-7.29\% in $\text{IoU}_{c}$, $\text{IoU}_{f}$, $\tilde{\text{IoU}}_{f}$ respectively. OccProphet also surpasses the voxel-based method Cam4DOcc in all the metrics, especially in $\tilde{\text{IoU}}_{f}$ by relatively 4.18\% and 15.92\% on nuScenes and Lyft-Level5 datasets respectively. Qualitative results in \Figref{fig:viz_comp_cam4docc_ours} demonstrate the superiority of OccProphet. The first group indicates OccProphet's adaptability in low-light conditions.

\begin{table}[h]
    \vspace{-15pt}
    \centering
    \renewcommand\arraystretch{1.37}
    \caption{Performance on forecasting inflated GMO. SPC: SurroundDepth \citep{surrounddepth} + PCPNet \citep{pcpnet} + Cylinder3D \citep{cylindrical}.}
    \renewcommand\tabcolsep{2pt}
    \scalebox{0.88}{
        \begin{tabular}{l|ccc|ccc}
        \toprule
         & \multicolumn{3}{c|}{nuScenes} & \multicolumn{3}{c}{Lyft-Level5} \\
         \cline{2-7}
         \multirow{-2}{*}{Method} & $\text{IoU}_{c}$ & $\text{IoU}_{f}$ (2\ s) & $\tilde{\text{IoU}}_{f}$ & $\text{IoU}_{c}$ & $\text{IoU}_{f}$ (0.8\ s) & $\tilde{\text{IoU}}_{f}$ \\
        \midrule
        OpenOccupancy-C \citep{openoccupancy} & 12.17 & 11.45 & 11.74 & 14.01 & 13.53 & 13.71 \\
        SPC \citep{pcpnet,surrounddepth,cylindrical} & 1.27 & - & - & 1.42 & - & - \\
        PowerBEV-3D \citep{powerbev} & 23.08 & 21.25 & 21.86 & 26.19 & 24.47 & 25.06 \\
        BEVDet4D \citep{bevdet4d}& 31.60 & 24.87 & 26.87 & -& -& -\\
        OCFNet (Cam4DOcc) \citep{cam4docc} & 31.30 & 26.82 & 27.98 & 36.41 & 33.56 & 34.60 \\
        \hline
        \rowcolor[HTML]{F9F2FE}
        OccProphet (ours) & \textbf{34.36} & \textbf{26.94} & \textbf{29.15} & \textbf{43.38} & \textbf{37.92} & \textbf{40.11}\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:inf-GMO}
    \vspace{-10pt}
\end{table}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/viz_comp_cam4docc_ours.pdf}
    \vspace{-18pt}
    \caption{Qualitative results of Cam4DOcc and OccProphet in the future 2 seconds. Black arrows denote the motion trends of moving objects. Red dashed rectangles represent that the results of OccProphet are more consistent with the ground truth than those of Cam4DOcc.}
    \label{fig:viz_comp_cam4docc_ours}
    \vspace{-14pt}
\end{figure*}

\paragraph{Evaluation on forecasting fine-grained GMO.} The second task evaluates the performance of forecasting fine-grained GMO. As shown in Table \ref{tab:fine-GMO}, when changing the GMO labels from inflated format using bounding boxes to fine-grained voxel-based labels, the performances of all approaches decrease significantly except the point cloud prediction method SPC achieving slightly better performance compared to Table \ref{tab:inf-GMO}. As pointed out in Cam4DOcc, the reason is that the fine-grained 3D structures of the moving objects are difficult to estimate using past and current continuous RGB images, while the labels for training SPC are also fine-grained and sparse. However, SPC  still performs worse than other counterparts due to the lack of shape consistency. Cam4DOcc keeps the lead over other competitors. Despite the challenging task, OccProphet surprisingly precedes all other approaches by a large margin. Specifically, the performances of OccProphet in $\text{IoU}_{c}$, $\text{IoU}_{f}$, $\tilde{\text{IoU}}_{f}$ are relatively 34.3\%, 10.43\%, 18.61\% higher than those of Cam4DOcc. The above results demonstrate the effectiveness of OccProphet in capturing the intricate 3D details of moving objects.

\begin{table}[h]
    \vspace{-14pt}
    \centering
    \renewcommand\arraystretch{1.3}
    \caption{Performance on forecasting fine-grained GMO.}
    \renewcommand\tabcolsep{6pt}
    \scalebox{0.88}{
        \begin{tabular}{l|ccc}
        \toprule
         & \multicolumn{3}{c}{nuScenes-Occupancy} \\
         \cline{2-4}
         \multirow{-2}{*}{Method} & $\text{IoU}_{c}$ & $\text{IoU}_{f}$ (2\ s) & $\tilde{\text{IoU}}_{f}$ \\
        \midrule
        OpenOccupancy-C \citep{openoccupancy} & 10.82 & 8.02 & 8.53\\
        SPC \citep{pcpnet,surrounddepth,cylindrical} & 5.85 & 1.08 & 1.12\\
        PowerBEV-3D \citep{powerbev} & 5.91 & 5.25 & 5.49\\
        OCFNet (Cam4DOcc) \citep{cam4docc} & 11.45 & 9.68 & 10.10\\
        \hline
        \rowcolor[HTML]{F9F2FE}
        OccProphet (ours) & \textbf{15.38} & \textbf{10.69} & \textbf{11.98} \\
        \bottomrule
        \end{tabular}
    }
    \label{tab:fine-GMO}
    \vspace{-14pt}
\end{table}

\begin{table}[h]
    \vspace{-14pt}
    \centering
    \renewcommand\arraystretch{1.2}
    \caption{Performance on forecasting fine-grained GMO and fine-grained GSO.}
    \renewcommand\tabcolsep{3pt}
    \scalebox{0.88}{
        \begin{tabular}{l|ccc|ccc|c}
        \toprule
         & \multicolumn{3}{c|}{$\text{IoU}_{c}$} & \multicolumn{3}{c|}{$\text{IoU}_{f}$ (2\ s)} & $\tilde{\text{IoU}}_{f}$\\
         \cline{2-8}
         \multirow{-2}{*}{Method} & GMO & GSO & mean & GMO & GSO & mean & GMO\\
        \midrule
        OpenOccupancy-C \citep{openoccupancy} & 9.62 & 17.21 & 13.42 & 7.41 & 17.30 & 12.36 & 7.86\\
        SPC \citep{pcpnet,surrounddepth,cylindrical} & 5.85 & 3.29 & 4.57 & 1.08 & 1.40 & 1.24 & 1.12\\
        PowerBEV-3D \citep{powerbev} & 5.91 & - & - & 5.25 & - & - & 5.49 \\
        OCFNet (Cam4DOcc) \citep{cam4docc} & 11.02 & 17.79 & 14.41 & 9.20 & 17.83 & 13.52 & 9.66 \\
        \hline
        \rowcolor[HTML]{F9F2FE}
        OccProphet (ours) & \textbf{13.71} & \textbf{24.42} & \textbf{19.06} & \textbf{9.34} & \textbf{24.56} & \textbf{16.95} & \textbf{10.33} \\
        \bottomrule
        \end{tabular}
    }
    \label{tab:fine-GMO_fine-GSO}
    \vspace{-10pt}
\end{table}

\paragraph{Evaluation on forecasting fine-grained GMO and fine-grained GSO.} The performances of forecasting fine-grained GMO and fine-grained GSO, are listed in Table \ref{tab:fine-GMO_fine-GSO}. OccProphet still surpasses Cam4DOcc in all metrics.

\subsection{Ablation Studies}

\begin{wraptable}{r}{0.23\textwidth}
    \vspace{-20pt}
    \centering
    \renewcommand\arraystretch{1.37}
    \caption{Effectiveness of each component.}
    \renewcommand\tabcolsep{4pt}
    \scalebox{0.89}{
        \begin{tabular}{lc}
        \toprule
        Method & $\tilde{\text{IoU}}_{f}$ \\
        \midrule
        \rowcolor[HTML]{F9F2FE}
        OccProphet & \textbf{28.24}\\
        w/o Observer & 27.25\\
        w/o Forecaster & 27.50\\
        w/o Refiner & 27.44\\
        w/o All & 26.07\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:comp_eff}
    \vspace{-7pt}
\end{wraptable}

\paragraph{Effectiveness of each component.} Table \ref{tab:comp_eff} demonstrates the ablation studies on the effectiveness of each component. Removing the Observer (Row 3) leads to around 1\% drop in $\tilde{\text{IoU}}_{f}$, showing the importance of the Observer extracting spatio-temporal information from observations. If the Forecaster is removed (Row 4), the performance decreases by 0.74\%, indicating the advantage of predicting future states adaptively based on the traffic environment compared to direct prediction. Row 5 shows that removing the Refiner brings a 0.8\% drop. If all the components are removed (the last row), where only a single linear layer is used to predict future states, the performances are sharply reduced to 26.07\%, further revealing the effectiveness of the components. Qualitative results of using the Observer or not are in \Figref{fig:viz_ablation_observer}. The integration of Observer promotes spatio-temporal representativeness, enhancing the forecasting consistency with the ground truth. Other qualitative results are shown in the appendix.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/viz_ablation_observer.pdf}
    \vspace{-20pt}
    \caption{Qualitative results of using the Observer or not. Black arrows denote the motion trends of moving objects. Red dashed rectangles represent that the results with the Observer are more consistent with the ground truth than those without the Observer.}
    \label{fig:viz_ablation_observer}
\end{figure*}

\vspace{-7pt}
\paragraph{Effectiveness of E4A representation.} To further justify the effectiveness of E4A representation, we experiment with different representation styles of the Observer. As shown in Table \ref{tab:comp_flops_repr_styles}, the UNet like E4A balances computational cost and performance well. 

\vspace{-7pt}
\subsection{Comparison of Model Complexity}
\vspace{-7pt}

\begin{wraptable}{r}{0.68\textwidth}
\vspace{-20pt}
\centering
\caption{Comparison of model complexity. N$_{\text{p}}$: Total number of parameters. Mem.: Occupied GPU memory during training.}
\renewcommand\tabcolsep{1pt}
\scalebox{0.88}{
    \begin{tabular}{c|ccccc}
    \toprule
    Method & N$_{\text{p}}$ (M) & Mem. (G) & FLOPs (G) & FPS & $\tilde{\text{IoU}}_{f}$ \\
    \midrule
    Cam4DOcc & 370 & 57 & 6434 & 1.7 & 27.98 \\
    \rowcolor[HTML]{F9F2FE}
    OccProphet & \textbf{82(78\%$\downarrow$)} & \textbf{24(58\%$\downarrow$)} & \textbf{1985(69\%$\downarrow$)} & \textbf{4.5(165\%$\uparrow$)} & \textbf{29.15 (4\%$\uparrow$)} \\
    \bottomrule
    \end{tabular}
}
\label{tab:comp_flops}
\vspace{-7pt}
\end{wraptable}


In this section, we compare the model complexity between Cam4DOcc and OccProphet. As shown in Table \ref{tab:comp_flops}, the number of parameters, memory usage, and FLOPs of OccProphet are decreased by 58-78\% compared to Cam4DOcc, while OccProphet achieves a 4\% relative increase in $\tilde{\text{IoU}}_{f}$, and has 2.6$\times$ the FPS speed of Cam4DOcc, justifying the efficiency and effectiveness of OccProphet.

\vspace{-7pt}
\section{Conclusion}
\vspace{-7pt}

This paper proposes OccProphet, a novel camera-only framework for occupancy forecasting. The framework employs an Observer-Forecaster-Refiner pipeline, specifically designed for efficient and effective training and inference. Such efficiency and effectiveness are achieved through 4D aggregation and tripling-attention fusion on reduced-resolution features.
Experimental results demonstrate OccProphet's superiority in both forecasting accuracy and efficiency. It outperforms the state-of-the-art Cam4DOcc in occupancy forecasting by relatively 4\%$\sim$18\% on three datasets, while operating 2.6× faster and reducing computational costs by 58\%-78\%. We hope OccProphet can motivate future research in efficient occupancy forecasting and its applications in on-vehicle deployment.

\section*{Acknowledgements}
The research work was conducted in the JC STEM Lab of Machine Learning and Computer Vision funded by The Hong Kong Jockey Club Charities Trust and was partially supported by the Research Grants Council of the Hong Kong SAR, China (Project No. PolyU 15215824).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}

\subsection{Additional Quantitative Results}

\textbf{Forecasting inflated GMO and fine-grained GSO}: the category definitions of GMO and GSO are the same as those in the task of \textbf{Forecasting fine-grained GMO}. The labels of inflated GMO are generated from bounding boxes, while the labels of fine-grained GSO are occupancy annotations.
The performances of forecasting inflated GMO and fine-grained GSO are listed in Table \ref{tab:inf-GMO_fine-GSO}. SPC is still the worst, where the IoU of inflated GMO remains consistent in Table \ref{tab:inf-GMO}. OccProphet dramatically outperforms other approaches including OpenOccupancy-C and Cam4DOcc by a large margin. Regarding GMO, OccProphet is 3.77\% and 0.92\% higher than Cam4DOcc in the current and future moments, respectively. For GSO, OccProphet surpasses Cam4DOcc by 6.46\% and 6.38\% in $\text{IoU}_{c}$ and $\text{IoU}_{f}$, respectively. When taking multiple future frames for evaluation, OccProphet remains the best with an impressive superiority of 2.21\% over the second best Cam4DOcc in $\tilde{\text{IoU}}_{f}$. These results demonstrate the superiority of our method of extracting spatio-temporal features and predicting future states.


\begin{table}[h]
    \centering
    \renewcommand\arraystretch{1.2}
    \caption{Performance on forecasting inflated GMO and fine-grained GSO.}
    \renewcommand\tabcolsep{3pt}
    \scalebox{0.88}{
        \begin{tabular}{l|ccc|ccc|c}
        \toprule
         & \multicolumn{3}{c|}{$\text{IoU}_{c}$} & \multicolumn{3}{c|}{$\text{IoU}_{f}$ (2\ s)} & $\tilde{\text{IoU}}_{f}$\\
         \cline{2-8}
         \multirow{-2}{*}{Method} & GMO & GSO & mean & GMO & GSO & mean & GMO\\
        \midrule
        OpenOccupancy-C \citep{openoccupancy} & 13.53 & 16.86 & 15.20 & 12.67 & 17.09 & 14.88 & 12.97\\
        SPC \citep{pcpnet,surrounddepth,cylindrical} & 1.27 & 3.29 & 2.28 & - & 1.40 & - & -\\
        PowerBEV-3D \citep{powerbev} & 23.08 & - & - & 21.25 & - & - & 21.86 \\
        OCFNet (Cam4DOcc) \citep{cam4docc} & 29.84 & 17.72 & 23.78 & 25.53 & 17.81 & 21.67 & 26.53 \\
        \hline
        \rowcolor[HTML]{F9F2FE}
        OccProphet (ours) & \textbf{33.61} & \textbf{24.18} & \textbf{28.89} & \textbf{26.45} & \textbf{24.19} & \textbf{25.32} & \textbf{28.74}\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:inf-GMO_fine-GSO}
    \vspace{-12pt}
\end{table}

\subsection{Qualitative Results of Ablation Studies}

\paragraph{Effectiveness of the Forecaster.} \Figref{fig:viz_ablation_forecaster} shows the qualitative results with or without using the Forecaster module. The Forecaster is adept at perceiving moving objects, while forecasting with only a single linear layer (w/o Forecaster) tends to miss some moving objects.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/viz_ablation_forecaster.pdf}
    \vspace{-20pt}
    \caption{Qualitative results of using the Forecaster or not. Black arrows denote the motion trends of moving objects. Red dashed rectangles represent that the results with the Forecaster are more consistent with the ground truth than those without the Forecaster.}
    \label{fig:viz_ablation_forecaster}
\end{figure*}

\paragraph{Effectiveness of the Refiner.} \Figref{fig:viz_ablation_refiner} shows the qualitative results using the Refiner or not. It is evident that using the Refiner yields better forecast results for both moving and static objects, indicating its forecast refinement capability.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/viz_ablation_refiner.pdf}
    \vspace{-20pt}
    \caption{Qualitative results of using the Refiner or not. Black arrows denote the motion trends of moving objects. Red dashed rectangles represent that the results with the Refiner are more consistent with the ground truth than those without the Refiner.}
    \label{fig:viz_ablation_refiner}
\end{figure*}

\subsection{Distinctions from the traditional encoder-decoder architecture}

The traditional encoder-decoder architecture comprises an encoder for representation extraction and a decoder for occupancy prediction, as adopted by OccNet \citep{occnet} and Cam4DOcc. However, the traditional architecture either loses 3D geometry details-\textit{e.g.}, OccNet adopts a BEV-based encoder, or introduces high computational cost-\textit{e.g.}, Cam4DOcc utilizes vanilla 3D convolutional networks as the encoder and decoder.

In OccProphet, the Observer works similarly to an encoder, while the combination of Forecaster and Refiner works similarly to a decoder. Unlike the traditional encoder-decoder architecture, OccProphet pushes the efficiency frontier of 4D occupancy forecasting. To achieve this, the Observer and Refiner enable spatio-temporal interaction using the Efficient 4D Aggregation module, and the Forecaster adaptively predicts future states using a lightweight condition mechanism. Overall, our Observer-Forecaster-Refiner framework emphasizes 4D spatio-temporal interaction and conditional forecasting, meanwhile maintaining efficiency.


\subsection{Occupancy forecasting over varying time horizons}
The performance of occupancy forecasting over varying time horizons is critical in autonomous driving scenarios.
We compare OccProphet with OpenOccupancy-C \citep{openoccupancy}, PowerBEV-3D \citep{powerbev}, and Cam4DOcc \citep{cam4docc} in terms of occupancy forecasting accuracy over varying time horizons, as shown in Table \ref{tab:forecasting-varying-time}. We can see that (1) OccProphet consistently outperforms other approaches across all time horizons on three datasets. (2) The longer the forecasting period, the lower the accuracy of all methods, indicating that forecasting becomes increasingly difficult.

\begin{table}[h]
    \vspace{-14pt}
    \centering
    \renewcommand\arraystretch{1.3}
    \caption{Performance on occupancy forecasting over varying time horizons.}
    \renewcommand\tabcolsep{3pt}
    \scalebox{0.88}{
        \begin{tabular}{l|cccc|cccc|cccc}
        \toprule
         & \multicolumn{4}{c|}{nuScenes} & \multicolumn{4}{c|}{Lyft-Level5} & \multicolumn{4}{c}{nuScenes-Occupancy}\\
         \cline{2-13}
         \multirow{-2}{*}{Method} & 0.5s &1.0s & 1.5s & 2.0s & 0.2s & 0.4s & 0.6s & 0.8s & 0.5s & 1.0s & 1.5s & 2.0s \\
        \midrule
        OpenOccupancy-C & 12.07 & 11.80 & 11.63 & 11.45 & 13.87 & 13.77 & 13.65 & 13.53 & 9.17 & 8.64 & 8.29 & 8.02\\
        PowerBEV-3D & 22.48 & 22.07 & 21.65 & 21.25 & 25.70 & 25.25 & 24.82 & 24.47 & 5.74 & 5.56 & 5.41 & 5.25\\
        OCFNet (Cam4DOcc)  & 29.36 & 28.30 & 27.44 & 26.82 & 35.58 & 34.96 & 34.28 & 33.56 & 10.64 & 10.20 & 9.89 & 9.68 \\
        \hline
        \rowcolor[HTML]{F9F2FE}
        OccProphet (ours) & \textbf{32.17} &\textbf{29.60} & \textbf{27.95} & \textbf{26.94} & \textbf{42.34} & \textbf{40.87} & \textbf{39.38} & \textbf{37.92} & \textbf{13.64} & \textbf{12.10} & \textbf{11.27} & \textbf{10.69}\\
        \bottomrule
        \end{tabular}
    }
    \label{tab:forecasting-varying-time}
    \vspace{-14pt}
\end{table}

\subsection{Failure Cases}

\subsubsection{Unseen Scenarios}

To test forecasting performance in unseen scenarios, we conduct a cross-domain experiment. Specifically, we train the model on the Lyft-Level5 dataset and test it on the nuScenes dataset. The visualization results are shown in \Figref{fig:failure_unseen}. We can observe that cross-domain occupancy forecasting performs worse than intra-domain forecasting within a single dataset. 
We consider that this performance gap may be due to the difference in sensor settings between the two datasets. In the future, research on generalized occupancy forecasting will be a valuable direction worth exploring.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/lyft-to-nuscenes.pdf}
    \vspace{-20pt}
    \caption{Failure cases in unseen scenarios. Red dashed rectangles indicate that the intra-domain results (\textit{e.g.}, nuScenes to nuScenes) are more consistent with the ground truth than cross-domain occupancy forecasting (\textit{e.g.}, Lyft-Level5 to nuScenes).}
    \label{fig:failure_unseen}
\end{figure*}

\subsubsection{Occluded Scenarios}

We investigate the forecasting performance in occluded scenarios, which frequently occur in traffic scenarios. Ground truth occupancy and forecasted future occupancy by OccProphet, over varying time horizons, are qualitatively visualized in \Figref{fig:failure_occlusion}. It can be observed that the occupancy labels of the occluded object over different time horizons are complete, ensuring the training reliability. The forecasted results of OccProphet demonstrate its capability of handling occluded scenarios to a certain degree, which can still be further improved in future research.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/occlusion.pdf}
    \vspace{-12pt}
    \caption{Failure cases in occluded scenarios. In input images, yellow rectangles indicate the object of interest, while dashed rectangles denote that the object is nearly entirely occluded. Red rectangles indicate that the occupancy ground truths of the occluded object are complete over past, current, and future frames. Blue rectangles reveal that OccProphet is able to forecast the future occupancy of the occluded object, which can mitigate the occlusion issue to a certain extent.}
    \label{fig:failure_occlusion}
    \vspace{-2pt}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/dense_scenario_failure_cases.pdf}
    \vspace{-20pt}
    \caption{Failure cases in dense scenarios. Red dashed rectangles indicate that the results of OccProphet are more consistent with the ground truth than those of Cam4DOcc.}
    \label{fig:failure_dense_scene}
    \vspace{-10pt}
\end{figure*}

\subsubsection{Dense Scenarios}

To evaluate model precision at fine granularity, we qualitatively visualize the occupancy forecasting of Cam4DOcc and OccProphet in dense scenarios, as shown in \Figref{fig:failure_dense_scene}. The visualization reveals that both Cam4DOcc and OccProphet encounter challenges in fine-grained forecasting, indicating a huge difficulty. In comparison, OccProphet's results are closer to the ground truth. We believe that this is attributed to our proposed Observer-Forecaster-Refiner framework. We regard fine-grained occupancy forecasting as a valuable direction for future research.

\end{document}