In the above section, we have characterized the pure Nash Equilibria, mixed Nash Equilibria, and Correlated Equilibria under different conditions. Next, we study the convergence of natural learning dynamics that are intended for game-theoretic settings. We focus, in particular, on both learners using the classical \emph{Exponential Weights Algorithm}~\cite{VOVK1998153,LITTLESTONE1994212}, perhaps one of the most classical no-regret algorithms in the online learning literature. 

\begin{algorithm}[!h]
\caption{Online Learning Dynamic through Exponential Weights %\gua{added the algorithm for $n>2$ in appendix}
}
\begin{algorithmic}
\label{alg:Hedge} 
\STATE \textbf{Input}: step size $\eta>0$, time period $T$
    \STATE \textbf{Initialization: $p_1^{(1)},p_2^{(1)}\in\Delta_4$}
\FOR{$t=1,\dots,T$}
\STATE \emph{Decision:} Each player $i$ plays $\theta^{(j)}$ with probability $p_{i,j}^{(t)}$;
\STATE \emph{Update Rule:} Players update their probability distributions to $p_{1}^{(t+1)},~p_{2}^{(t+1)}$ as follows:
\begin{align*}
&p^{(t+1)}_{1,j}\propto p^{(t)}_{1,j} \cdot \exp\left(\eta \sum_{k=1}^4 p^{(t)}_{2,k} \cdot u_1\left(\theta^{(j)},\theta^{(k)}\right)\right),~~\forall k\in[4]\\
&p^{(t+1)}_{2,j}\propto p^{(t)}_{2,j} \cdot \exp\left(\eta \sum_{k=1}^4 p^{(t)}_{1,k} \cdot u_2\left(\theta^{(k)},\theta^{(j)} \right)\right),~~\forall k\in[4]
\end{align*}
% $\forall i\in[4]$, $p^{(t+1)}_{1,i}\propto p^{(t)}_{1,i}\exp\left(\eta \sum_{j=1}^4 p^{(t)}_{2,j}u_1\left(\theta^{(i)},\theta^{(j)}\right)\right)$
% \STATE $\forall i\in[4]$, $p^{(t+1)}_{2,i}\propto p^{(t)}_{2,i}\exp\left(\eta \sum_{j=1}^4 p^{(t)}_{1,j}u_2\left(\theta^{(i)},\theta^{(j)}\right)\right)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

%\juba{the algo was incomplete---it only highlighted the update rule (how to pick the distribution in the next round), but not talks at all about the decision rule (how to pick $\theta$). There were also notational issues that were inconsistent with the rest of the paper. The update rule was also wrong. I fixed, but double-check for errors. }
Exponential Weights is formally described in Algorithm~\ref{alg:Hedge}.
We provide a brief description of the most salient step here: at each time step, each player $i \in \{1,2\}$ randomly picks an action according to their current probability distribution $p_i^{(t)}$. Then, each player updates their weight on each action in a manner that is exponentially proportional to the expected utility derived by playing that action (which in turn depends on the mixed strategy of the other player). For example, considering round $t$ and candidate action $\theta^{(j)}$, player $1$ obtains an expected utility of $\sum_{k=1}^4 p^{(t)}_{2,k} \cdot u_1\left(\theta^{(j)},\theta^{(k)}\right)$ under player $2$ playing $p_2^{(t)}$. An important parameter of the algorithm is the step size $\eta$, which is traditionally set as $\eta = \Theta(1/\sqrt{T})$ (with $T$ denoting a finite time horizon) or in a time-varying manner $\eta_t = \Theta(1/\sqrt{t})$ to ensure state-of-the-art no-regret guarantees.
Surprisingly, our characterizations will hold for any \emph{constant} step size $\eta \in (0, \infty)$.

%\subsection{The Full-Information Setting \gua{I think Full-Information is better than deterministic, (I think deterministic refers to the case without a distribution, not knowing the distribution)}\juba{I agree}}
\subsection{The Full-Information Setting}
\label{sec:det-hedge-convergence}

We begin by considering the idealized setting where \( D_y \) is known, allowing every learner to compute the exact utility matrix. In this scenario, we examine the online learning dynamics presented in Algorithm \ref{alg:Hedge}, where two players update their decisions using the Exponential Weights method. 
% Specifically, at each round \( t \), player \( k \in \{1,2\} \) updates their decision, denoted as \( p^{(t)}_k\in\Delta_4 \). For \( i \in [4] \), the update rule for player $1$ is given by:
% \[
% p^{(t+1)}_{1,i} = \frac{p^{(t)}_{1,i} \exp\left(\eta \sum_{j=1}^4 p^{(t)}_{2,j} u_1\left(\theta^{(i)}, \theta^{(j)}\right)\right)}{\sum_{\ell=1}^4 p^{(t)}_{1,\ell} \exp\left(\eta \sum_{j=1}^4 p^{(t)}_{2,j} u_1\left(\theta^{(\ell)}, \theta^{(j)}\right)\right)},
% \]
% where \( p^{(t)}_{1,i} \) represents the probability assigned to \( \theta^{(i)} \) by player 1 at round \( t \), and \( \eta > 0 \) is a tuning parameter controlling the update's sensitivity. 

% The following lemma shows convergence of the dynamics at an exponential rate:
First, we establish a basic property of the game dynamic that holds under mild initialization conditions; namely, that the dominated strategies $\theta^{(1)} = (\tau_{\ell},\gamma_{\ell})$ and $\theta^{(4)} = (\tau_h, \gamma_h)$ are \emph{ruled out quickly}.
Formally, the following lemma shows that the probability on each of the strategies $\theta^{(1)}$ and $\theta^{(4)}$ decays to $0$ as the number of rounds $t \to \infty$ at a rate that is exponential in $t$ for a constant step size $\eta \in (0,\infty)$.
\begin{lemma}
\label{lem:1 and 4 p}
Let $\xi_1=\min\{\frac{1}{2}f(\gamma_{\ell},\tau_{h},1), \frac{1}{2}\left|f(\gamma_{\ell},\tau_{\ell},\tau_{h})\right|,\frac{1}{2}|f(\gamma_{h},\tau_{\ell},\tau_{h})|, |\epsilon_1|, |\epsilon_2|\}>0.$ Moreover, let $c_{ini}=\max_{i\in[2]}\left\{\frac{\max_{j\in[4]}p^{(1)}_{i,j}}{\min_{i\in[4]}p^{(1)}_{i,j}}\right\}$, and $t'=\max\left\{\left\lceil\frac{\log 2c_{ini}}{\epsilon\eta}+1\right\rceil,2\right\}$ be constants. Then for any error tolerance $\omega>0$, if $t\geq t'+\frac{2}{\eta\xi_1} \log\frac{c_{ini}}{\omega}$, we have for bank $i=1,2$, 
\begin{equation*}
    \begin{split}
        {p^{(t)}_{i,1}}\leq \omega,\ \ \text{and}
\ \ p^{(t)}_{i,4}\leq \omega.
    \end{split}
\end{equation*}
\end{lemma}
Lemma~\ref{lem:1 and 4 p} is proved in Appendix~\ref{proof:Lemma: converofp_1p_4}. %\juba{as per vidya's comment, it would be good to translate this into what T is needed to get to a certain epsilon; the current version is hard to parse} \gua{In Theorem 4, we didn't use the union bound for all $T$, so we can claim convergences as $t\rightarrow \infty$, so I still think we can use the current form.}\juba{I think it is not about technical accuracy but about exposition---the flipped version of the theorem is more "standard" and easy to interpret for the EC community}
% \begin{proof}
% The full proof can be found in Appendix \ref{proof:Lemma: converofp_1p_4}.
% \end{proof}
The above lemma shows that, for Algorithm \ref{alg:Hedge}, the mass assigned to the dominated pure strategies $(\tau_{\ell},\gamma_{\ell})$ and $(\tau_{h},\gamma_{h})$ will always converge to 0 exponentially fast under the mild assumption that both learners initialize with some non-zero probability on all pure strategies. This is easily satisfied by, e.g. the uniform distribution over all $4$ pure strategies. To see why this initialization condition is necessary, note that the constant $c_{ini} < \infty$ only when $p_{i,j}^{(1)} > 0$ for all $i \in [2]$ and $j \in [4]$. %The Lemma requires the initialization to be positive: this is a very mild condition. \juba{what initialization? Not precise enough, is it $c_{ini}$? And, why is it mild? All this need be explained to the reader}
Next, we provide the convergence results for our online learning dynamic under different conditions for $\epsilon_1$ and $\epsilon_2$.\\

%\juba{we probably want to add some interpretation, show that this is again an exponential convergence rate, etc., in all our results below} \vidya{done}

\noindent \textbf{Case I: $\epsilon_1<0$, and $\epsilon_2<0$}. In this case, Algorithm \ref{alg:Hedge} converges to the symmetric pure Nash equilibrium $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$ at an exponential rate for a constant step size $\eta \in (0,\infty)$ as the number of rounds $t \to \infty$. 
The formal statement in this case is provided below.
\begin{theoremfour}{Part I}
\label{thm:converge}
If $\epsilon_1<0,\epsilon_2<0$, let 
$t''=t'+1+\left\lceil\frac{2}{\eta\xi_1}\log\frac{2(\xi_1+3)c_{ini}}{\xi}\right\rceil$ be a constant. For $i\in[2]$ and any error tolerance $\omega>0$, if $t\geq \frac{2}{\xi_1}(3+\frac{\xi_1}{2})t''+\frac{2}{\eta\xi_1}\log \frac{c_{ini}}{\omega}$, we have 
$$ p^{(t)}_{i,2}\leq \omega, \ \ \text{and}\ \ \  p^{(t)}_{i,3}\geq \max\{1-3\omega,0\}.$$
%We have for , $t\geq t''$ and 
%$$p^{(t+1)}_{i,2}\leq c_{\text{ini}}\exp\left(3\eta t''-\frac{\eta\xi_1}{2}\left(t-t''+1\right)\right), $$ 
%and  
%$$ p^{(t+1)}_{i,3}\geq \max\left\{ 1 -3c_{\text{ini}}\exp\left(3\eta t''-\frac{\eta\xi_1}{2}\left(t-t''+1\right)\right),0\right\}.$$
\end{theoremfour}

\noindent \textbf{Case II: $\epsilon_1>0$, $\epsilon_2>0$.} In this case, Algorithm \ref{alg:Hedge} again converges to the symmetric pure Nash equilibrium $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$ at an exponential rate for a constant step size $\eta \in (0,\infty)$ as the number of rounds $t \to \infty$. 
The formal statement in this case is provided below.

\begin{theoremfour}{Part II}
If $\epsilon_1>0$ and $\epsilon_2>0$, then let $t'''=\left\lceil\frac{1}{\eta\xi_1}\log\frac{2(1+\xi_1)c_{\text{ini}}}{\xi_1}\right\rceil+1$ be a constant. For $i\in[2]$ and any error tolerance $\omega>0$, if $t\geq \frac{2}{\xi_1}(1+\frac{\xi_1}{2})t''+\frac{2}{\eta\xi_1}\log \frac{c_{ini}}{\omega}$, we have 
$$ p^{(t)}_{i,3}\leq \omega, \ \ \text{and}\ \ \  p^{(t)}_{i,2}\geq \max\{1-3\omega,0\}.$$
%we have for $t\geq t'''$, for $i\in[2]$,  
%$${p^{(t+1)}_{i,3}}\leq c_{\text{ini}}\exp\left(\eta t'''-\frac{\eta\xi_1}{2}(t-t'''+1)\right), $$
%and 
%$$p_{i,2}^{(t+1)}\geq 1- \min\left\{3c_{ini}\exp\left(\eta t'''-\frac{\eta\xi_1}{2}(t-t'''+1)\right),1\right\}.$$
\end{theoremfour}

It is worth noting that neither \textbf{Case I} nor \textbf{Case II} require any additional assumptions on the initialization other than the mild condition (i.e. non-zero probability on every pure strategy) required for the dominated strategies to be ruled out (Lemma~\ref{lem:1 and 4 p}).
Both of these cases considered $\epsilon_1$ and $\epsilon_2$ (which represent certain changes in utility for each bank when they switch their respective decisions; see the discussion before Theorem~\ref{thm:pureNE}) to be of the same sign.

We now consider the more complex cases in which the signs of the parameters of the game $\epsilon_1$ and $\epsilon_2$ \emph{do not} match.
For these two cases, we assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$ to simplify the proof. Note that based on Lemma \ref{proof:Lemma: converofp_1p_4}, the weights assigned to these two decisions will decrease exponentially fast independently of initialization---hence this assumption is relatively mild.


\noindent \textbf{Case III: $\epsilon_1<0$, $\epsilon_2>0$.} 
In this case, Algorithm~\ref{alg:Hedge} \emph{under proper initialization} can be shown to converge to the asymmetric NE $((\tau_{\ell},\gamma_{h})(\tau_{h},\gamma_{\ell}))$ at an exponential rate for constant step size $\eta \in (0,\infty)$ as the number of rounds $t \to \infty$.
To be more specific, the initialization requires $p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2$ and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2$ to have different signs. 
It is easy to achieve this requirement by setting, e.g. $p^{(1)}_{2,2} = 1$ and $p^{(1)}_{1,3} = 1$; more generally, a continuum of choices of initialization would satisfy this requirement since $\epsilon_1 < 0$ and $\epsilon_2 > 0$.
The formal statement in this case is provided below.
% Note that this is a minor requirement and easy to achieve since $\epsilon_1$ and $\epsilon_2$ have different signs. 


% \juba{the proper initialization needs a short discussion, explaining that the condition is relatively mild and easy to satisfy. We want to sell the fact that there is a significant range of initializations that are expected to work well---i.e., our result is not a corner case} \gua{added}
\begin{theoremfour}{Part III}
    If $\epsilon_1<0$, and $\epsilon_2>0$, assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. Moreover, suppose the initialization 
$p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2<-\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2>\Delta$, for some constant $\Delta>0$. Then for any error tolerance $\omega>0$, if $t\geq \frac{1}{\eta\Delta}\log \frac{c'_{ini}}{\omega}+1$, we have  
\[
p^{(t)}_{1,2}\geq \frac{1}{1+\omega}, \ \ \text{and}\ \ p^{(t)}_{2,3}\geq \frac{1}{1+\omega},
\]
where $c'_{ini}=\max_{i\in[2]}\left\{\frac{\max_{j\in\{2,3\}}p^{(1)}_{i,j}}{\min_{i\in\{2,3\}}p^{(1)}_{i,j}}\right\}$. It implies that the algorithm converges to $((\tau_{\ell},\gamma_{h})(\tau_{h},\gamma_{\ell}))$ as $t \to \infty$. If $p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2>\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2<-\Delta$, for some constant $\Delta>0$. Then if $t\geq \frac{1}{\eta\Delta}\log \frac{c'_{ini}}{\omega}+1$, we have   
\[
p^{(t)}_{1,3}\geq \frac{1}{1+\omega},\ \ \text{and}\ \ p^{(t)}_{2,2}\geq \frac{1}{1+\omega}.
\]
 It implies that the algorithm converges to $((\tau_{h},\gamma_{\ell}),(\tau_{\ell},\gamma_{h}))$ as $t \to \infty$.
\end{theoremfour}

\noindent \textbf{Case IV: $\epsilon_1>0$, $\epsilon_2<0$}. In this case, Algorithm \ref{alg:Hedge}, \emph{again under proper initialization}, converges to the symmetric NE $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$ at an exponential rate for constant step size $\eta \in (0,\infty)$ as the number of rounds $t \to \infty$.
To be more specific, the initialization now requires $p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2$ and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2$ to have the same sign. 
Similar to Case III, this requirement is also easy to achieve by setting, e.g. $p^{(1)}_{2,2} = 1$ and $p^{(1)}_{1,2} = 1$; more generally, a continuum of choices of initialization\footnote{Note, however, that it is unclear whether there are ``universal" initializations that would satisfy the requisite condition \emph{simultaneously} for both Case III and Case IV.} would satisfy this requirement since $\epsilon_1 > 0$ and $\epsilon_2 < 0$.
The formal statement in this case is provided below.

\begin{theoremfour}{Part IV}
  If $\epsilon_1>0$, $\epsilon_2<0$, assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. Moreover, suppose the initialization 
$p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2<-\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2<-\Delta$, for some constant $\Delta>0$. Then for any error tolerance $\omega>0$, if $t\geq \frac{1}{\eta\Delta}\log \frac{c'_{ini}}{\omega}+1$, we have  
\[
p^{(t+1)}_{1,3}\geq \frac{1}{1+\omega},\ \ \text{and}\ \ p^{(t+1)}_{2,3}\geq \frac{1}{1+\omega}.
\]
It implies that the algorithm converges to $((\tau_{h},\gamma_{\ell}),(\tau_{h},\gamma_{\ell}))$. If 
$p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2>\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2>\Delta$, for some constant $\Delta>0$. Then if $t\geq \frac{1}{\eta\Delta}\log \frac{c'_{ini}}{\omega}+1$, we have 
$$ p^{(t)}_{1,2} \geq \frac{1}{1+\omega},\ \ \text{and}\ \ \ p^{(t)}_{2,2}\geq \frac{1}{1+\omega}.$$
%\[
%p^{(t+1)}_{1,2}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},\ \ \text{and}\ \ p^{(t+1)}_{2,2}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},
%\]
It implies that the algorithm converges to $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$.   
\end{theoremfour}



%\juba{this is not formal enough. We cannot just say ``the last iterate converges to blah''. We need to not only define the last iterate, give it a proper notation, then use tis notation in the theorem}
%\juba{``exponentially fast'' needs to be replaced by the acutal convergence rate, written in math not english + defining the equilibria}
%\begin{theorem}
%\label{thm:converge}
%We have the following convergence results:    
%\begin{itemize}
%\item If $\epsilon_1<0,\epsilon_2<0$, let $t''=t'+1+\left\lceil\frac{2}{\eta\xi_1}\log\frac{2(\xi_1+3)c_{ini}}{\xi}\right\rceil$ be a constant. We have for $i\in[2]$, $t\geq t''$,$$p^{(t+1)}_{i,2}\leq c_{\text{ini}}\exp\left(3\eta t''-\frac{\eta\xi_1}{2}\left(t-t''+1\right)\right), $$ and  $$ p^{(t+1)}_{i,3}\geq \max\left\{ 1 -3c_{\text{ini}}\exp\left(3\eta t''-\frac{\eta\xi_1}{2}\left(t-t''+1\right)\right),0\right\}.$$It implies that the algorithm converges to $((\tau_{h},\gamma_{\ell}),(\tau_{h},\gamma_{\ell}))$ when $t \to \infty$.
%\item If $\epsilon_1>0$ and $\epsilon_2>0$, then let $t'''=\left\lceil\frac{1}{\eta\xi_1}\log\frac{2(1+\xi_1)c_{\text{ini}}}{\xi_1}\right\rceil+1$ be a constant, we have for $t\geq t'''$, for $i\in[2]$,  $${p^{(t+1)}_{i,3}}\leq c_{\text{ini}}\exp\left(\eta t'''-\frac{\eta\xi_1}{2}(t-t'''+1)\right), $$and $$p_{i,2}^{(t+1)}\geq 1- \min\left\{3c_{ini}\exp\left(\eta t'''-\frac{\eta\xi_1}{2}(t-t'''+1)\right),1\right\}.$$It implies that the algorithm converges to $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$ as $t \to \infty$.
%\item If $\epsilon_1<0$, and $\epsilon_2>0$, assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. Moreover, suppose the initialization $p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2<-\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2>\Delta$, for some constant $\Delta>0$. Then 
%\[
%p^{(t+1)}_{1,2}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},\ \ \text{and}\ \ p^{(t+1)}_{2,3}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},
%\]
%where $c'_{ini}=\max_{i\in[2]}\left\{\frac{\max_{j\in\{2,3\}}p^{(1)}_{i,j}}{\min_{i\in\{2,3\}}p^{(1)}_{i,j}}\right\}$. It implies that the algorithm converges to $((\tau_{\ell},\gamma_{h})(\tau_{h},\gamma_{\ell}))$ as $t \to \infty$. If $p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2>\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2<-\Delta$, for some constant $\Delta>0$. Then 
%\[
%p^{(t+1)}_{1,3}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},\ \ \text{and}\ \ p^{(t+1)}_{2,2}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},
%\]
% It implies that the algorithm converges to $((\tau_{h},\gamma_{\ell}),(\tau_{\ell},\gamma_{h}))$ as $t \to \infty$.
%\item If $\epsilon_1>0$, $\epsilon_1<0$, assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. Moreover, suppose the initialization 
%$p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2<-\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2<-\Delta$, for some constant $\Delta>0$. Then 
%\[
%p^{(t+1)}_{1,3}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},\ \ \text{and}\ \ p^{(t+1)}_{2,3}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},
%\]
%It implies that the algorithm converges to $((\tau_{h},\gamma_{\ell}),(\tau_{h},\gamma_{\ell}))$. If 
%$p^{(1)}_{2,2}\epsilon_1+p^{(1)}_{2,3}\epsilon_2>\Delta$, and $p^{(1)}_{1,2}\epsilon_1+p^{(1)}_{1,3}\epsilon_2>\Delta$, for some constant $\Delta>0$. Then 
%\[
%p^{(t+1)}_{1,2}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},\ \ \text{and}\ \ p^{(t+1)}_{2,2}\geq \frac{1}{1+c'_{ini}\exp\left(-\eta\Delta t\right)},
%\]
%It implies that the algorithm converges to $((\tau_{\ell},\gamma_{h}),%(\tau_{\ell},\gamma_{h}))$.
%\end{itemize}
%\end{theorem}

% \begin{proof}
% The full proof can be found in Appendix \ref{proof:Theoerm:conveccc}.
% \end{proof}
Parts I-IV of Theorem 4 are proved together in Appendix~\ref{proof:Theoerm:conveccc}.
Interestingly, the proof does not appeal to regret analysis and instead characterizes from first principles properties of the (discrete-time) dynamics. 
Moreover, somewhat surprisingly and in contrast to even certain last-iterate convergence results for zero-sum games~\cite{daskalakis2019last}, a larger step size implies faster convergence, and last-iterate convergence is achieved for \emph{any} non-zero step size!

Theorem 4 shows that it is possible to converge to a specific Nash Equilibrium in all of the four cases under proper initialization. 
It is worthwhile to ask the flipped question of whether we can approach every possible Nash Equilibrium through some instantiation of the Exponential Weights dynamic.
While the result above shows that this is the case for every \emph{pure} equilibrium, none of the cases cover convergence to the \emph{strictly mixed} equilibria --- obtaining a satisfactory convergence result to the strictly mixed Nash Equilibria remains open.
% It establishes the convergence behavior of the algorithm under different conditions, which indicates that the algorithm always converges to the pure NEs of this problem. 


\subsection{The Stochastic Setting}
\label{sec:stoch-hedge-convergence}
\input{stochastic}



%The asymmetric pair of decisions $((\tau_{h},\gamma_{\ell}),(\tau_{\ell},\gamma_{h}))$ will be a pure NE if and only if the following condition holds:
%$$f(\gamma_{\ell},\tau_{h},1)\geq \frac{1}{2}f(\gamma_{h},\tau_{\ell},1),\ \ \emph{and}\ \ f(\gamma_{h},\tau_{\ell},\tau_{h})\geq \frac{1}{2}f(\gamma_{\ell},\tau_h,1).$$
%Moreover if $D_y$ is a uniform distribution, then this condition does not hold. 
%On the other hand, this condition holds for the following piece-wise uniform distribution: for $y\in\left[0,\frac{1}{\tau_{\ell}}\right],$ $p(y)=\frac{0.2}{\tau_{\ell}}$; for $y\in[\tau_{\ell},\tau_{h}]$, $p(y)=\frac{0.7}{\tau_h-\tau_{\ell}}$; for $y\in[\tau_{h},1]$, $p(y)=\frac{0.1}{1-\tau_h}$. 