Much of machine learning operates under the assumption of a static, independently and identically distributed (i.i.d.) data paradigm, where the data generating process remains fixed and unaffected by the predictions made by the model. However, in many high-stakes decision-making settings, predictions and decisions often induce strategic or behavioral responses as well as distributional shifts in the populations they affect. These shifts create feedback loops between predictions, decisions, and outcomes, potentially altering the statistical properties of the data and of the decisions made over time. This phenomenon is known as Performative Prediction~\cite{pmlr-v119-perdomo20a}. 

Performative prediction arises in diverse applications. In lending, for example, credit scoring models predict the likelihood of default based on past borrower behavior, and these predictions determine whether a loan is granted and at what interest rate. Borrowers who receive higher interest rates may find it even harder to repay their loans, altering the distribution of future borrowers and their estimated likelihood of default. Further, borrowers denied loans may take actions to increase their credit score or financial responsibility, again changing the distribution of likelihood of repayment in future time steps. In multi-player settings, for example when there is competition between different mortgage companies (here, ``learners''), the distribution of applicants seen by each learner can frequently further depend on decisions made by both learners---for example, an individual may choose the mortgage company they are eligible for with the lowest interest rate. In turn, how mortgage companies decide to update their lending policy in each time step is a function of their historical lending policies. Importantly, this performativity raises challenges: the dynamics we describe can lead to model instability, as predictions drive changes in behavior that, if not properly accounted for, may prevent the system from reaching a stable equilibrium. 

Performative prediction and dynamic settings with distribution shifts and strategic responses in the case of a single learner have been receiving significant attention over the past few years~\cite{pmlr-v119-perdomo20a, NEURIPS2020_33e75ff0, pmlr-v151-brown22a,bechavod2021gaming,shavit2020causal,mendler2020stochastic,hardt2022performative,hardt2023performative}. Here, the goal is to characterize which dynamics converge under what condition to what is called a \emph{stable} model: i.e., a model such that, if we were to deploy said model and re-train it based on the responses to the current model, we would still obtain the same model---basically, even under performativity, the model will not change even in response to its induced distribution shift.~\cite{pmlr-v119-perdomo20a} provides necessary and sufficient conditions for certain simple dynamics (namely, repeated empirical risk minimization and gradient descent) to converge to a unique, stable model. They also provide conditions in which this model is optimal, i.e. it minimizes the performative loss (that takes performative distribution shifts into account). 

However, there has been significantly less attention paid to settings with \emph{multiple learners} that may be competing against each other~\cite{dean2022multi,narang2023multiplayer}.~\cite{dean2022multi} consider a setting where each learner obtains an i.i.d. sub-sample of the total population they are make prediction on, but where the fraction of the population that is assigned to them depends on all the learners' decisions---for example, a bank or mortgage company with lower interest rates may attract more consumers. However, they do not model self-selection effects, where different types of consumers may prefer different learners depending on the learner's decisions, in which case each learner gets a biased, non-i.i.d. sub-sample of the predicted population.~\cite{narang2023multiplayer} extend the traditional performative prediction setting of~\cite{pmlr-v119-perdomo20a} to include general multi-player interactions, but must rely on several assumptions to obtain convergence of the above dynamics. The strongest of these is an \emph{insensitivity} assumption that changing the strategy of the learners by a small amount also changes the population distribution by a similarly small amount, controlled by a Lipschitz constant. Such insensitivity does not hold for very simple and classical competition models; e.g., in a Bertrand price competition, one firm or learner lowering their price by a small amount means they may suddenly capture the entire demand, completely shifting the demand distribution faced by each learner. 

% \juba{need be more careful what the worst case is and that it is for specific dynamics (repeated ERM and repeated gradient descent)} \gua{I made some edits to avoid this, based on discussion with Juba and Vidya} 

%While the insensitivity assumption is generally  required for the convergence to a unique stable solution~\cite{brown2022performative, narang2023multiplayer}, there is an open question of whether simple dynamics can converge in its absence. 
%Our main result is to answer the question by the affirmative: 


The goal of this paper is to investigate the equilibria and the convergence of simple but effective learning dynamics in settings in which i) self-selection effects are commonplace and ii) the insensitivity assumption does not hold. To this end, we propose a simple, 2-player game between two mortgages companies or ``banks'' that is reminiscent of Bertrand Competition. In this model, the banks compete by setting minimum credit score requirements and interest rates, while customers self-select by choosing the bank offering the lower interest rate, provided they meet the respective credit score requirements. This setup not only highlights the importance of self-selection effects but also inherently violates the insensitivity assumption. For this model, we show that a very simple and natural no-regret dynamic (namely, both players following exponential weights) will \emph{always} converge to a stable outcome, which is an equilibrium of the one-shot game between the two banks. Interestingly and in contrast with~\cite{pmlr-v119-perdomo20a,narang2023multiplayer} that guarantee a unique stable outcome, we note that our framework exhibits a \emph{multiplicity} of stable outcomes; which stable outcome we converge to is controlled by the initialization of the no-regret dynamic. 

\paragraph{Summary of contributions} More precisely, our contributions are as follow: 
% \juba{this needs to be updated} \gua{I made small edits} \gua{Vidya said she will also edit it later}
\begin{enumerate}
\item In Section \ref{sec1}, we introduce a new performative prediction model where mortgage companies or banks compete with each other aiming to set minimum credit score requirements and interest rates, that we call the ``Bank Game''.

\item We fully characterize the equilibria of our Bank Game in Section \ref{sec:understand NE}, highlighting diverse types of pure and mixed Nash equilibria depending on the parameters of the game.
\item We study the convergence of simple learning dynamics, when each player uses a simple no-regret algorithm, exponential weights, and provide a full characterization of convergence in Section~\ref{sec:learntoconvege}.
Surprisingly, we show that the \emph{last iterate} of the no-regret dynamics converges to one of the \emph{Nash equilibria} of the one-shot game (despite general-purpose results only guaranteeing convergence of the time-average of no-regret dynamics to the polytope of coarse correlated equilibria).
This result highlights interesting friendly structure for no-regret dynamics in the Bank Game.
We also show robustness of convergence to stochasticity in the dynamics where the banks learn from a sufficiently large batch of customer samples per round (rather than having access to the entire distribution of customers).
% While in general, two no-regret algorithms best responding to each other are not guaranteed to converge to an equilibrium of the game, even in a time-average sense, we show that in the Bank Game, our dynamics not only always converge to one of the equilibria of the one-shot game, but also converge to them in a last iterate sense! We provide a full characterization of convergence in Section \ref{sec:learntoconvege}.
\item Finally, in Section~\ref{sec: exp-results} we provide extensive experiments that validate our theoretical insights and demonstrate convergence behavior in some cases not covered by our theory: i) stochastic dynamics with only \emph{one} sample per round, ii) a more fine-grained Bank Game in which each player can choose between $n > 2$ interest rates and credit thresholds in Appendix~\ref{app:3gamma}.
% we provide experimental results to extend our results across two irst, in Section \ref{sec: exp-results}, we consider \emph{in-sample} dynamics, where at each time step each learner learns from a batch of samples rather than the entire distribution; second, we expand our insights to the case of a Bank Game where each player can choose between $n > 2$ interest rates and credit thresholds. \vidya{come back to editing this bullet point}
\end{enumerate}

\subsection{Related Work}

%\juba{find and add more general purpose references for this section} \gua{Vidya noted she will add some more game }

\paragraph{Offline learning with population responses} While our work looks at online and dynamic settings, the questions it asks are related to works on offline learning settings where the population distribution changes as a function of the deployed models. There are few major but mostly separate research directions that study such settings specifically in the context of machine learning. One such direction is \emph{strategic classification}, where agents may aim to modify their features to pass a classifier deployed by a learner~\cite{bruckner2011stackelberg, hardt2016strategic, jagadeesan2021alternative,stratindark,bechavod2022information,haghtalab2023calibrated,cohen2023sequential}. Another setting is one where learners face self-selection effects~\cite{cherapanamjeri2023makes,horowitz2024classification,gaitonde2024sample,zampetakis2022analyzing}: i.e., an agent will only face the learner in the first place if they meet some qualification criterion, or decide which learner to pick among several learners and a potential outside option, as a function of the learner(s)' deployed rules or models. These self-selection effects tend to introduce bias in the population distributions faced by the learners, as is the case in the present work. Finally, our work is related to a long line of work on distribution shifts, where testing data may follow a different distribution from training data---see~\cite{koh2021wilds} for a survey.

\paragraph{Performative Prediction} Performative prediction has received a lot of attention over the past few years~\cite{pmlr-v119-perdomo20a, NEURIPS2020_33e75ff0, pmlr-v151-brown22a,bechavod2021gaming,shavit2020causal,mendler2020stochastic,hardt2022performative,hardt2023performative}. The most seminal work on Performative Prediction is perhaps that of~\citet{pmlr-v119-perdomo20a}. They provide necessary and sufficient conditions for repeated risk minimization and gradient descent to converge to performatively stable models in the context of performative prediction, and provide additional conditions for optimality. Performatively stable models are models that do not change under re-training, i.e. the model is a best response to the agent or population distribution it induces. Performatively optimal models minimize the loss function, explicitly taking into account the agents or population's response to the deployed model.

\paragraph{Multi-player Performative Prediction} As mentioned above, to the best of our knowledge, there few papers explicitly studying performative prediction in multi-player settings. Most relevant to use are~\cite{dean2022multi} and~\cite{narang2023multiplayer}. Again,~\cite{dean2022multi} consider a setting where each learner obtains an i.i.d. sub-sample of the total population they are make prediction on, but does not model self-selection effects.~\cite{narang2023multiplayer} extend the traditional performative prediction setting to include general multi-player interactions, but does rely on insensitivity assumptions. In comparison, our model, while less general, explicitly includes self-selection effects and does not rely on insensitivity assumptions.

\paragraph{Game Dynamics and Online Learning} In this paper, we depart from the more commonly assumed choices of repeated empirical risk minimization/gradient descent and instead consider dynamics that are more conducive to multi-agent settings; namely, \emph{no-regret dynamics}~\cite{freund1999adaptive} (also commonly called \emph{adaptive heuristics} in the economics literature~\cite{hart2005adaptive}).
However, even these more friendly dynamics are not immune to pitfalls. It is well known that such dynamics (and in fact, any ``uncoupled" dynamic where learners perform updates independently) \emph{cannot} converge to any Nash equilibria even for simple general-sum games~\cite{hart2003uncoupled}.
The only general-purpose positive result stipulates that the \emph{time average} of no-regret dynamics will converge to the polytope of coarse correlated equilibria. 
This is far from a satisfactory result for our purposes, as we are concerned with Nash equilibria and, ideally, the last iterate of the dynamics.
There is recent precedent for last-iterate analysis focusing on Nash Equilibria for special classes of games.
A plethora of last-iterate convergence results have been shown for zero-sum finite games (most relevant to our setting are~\cite{daskalakis2019last,cai2022finite}), but these usually require modifying the standard Exponential Weights dynamic to include optimism and/or extragradient method, as well as a sufficiently small step size.
In contrast, our game is not zero-sum, our analysis can handle any non-zero step size, and we consider the original Exponential Weights dynamic.
Finally, it is interesting to note that our convergence results (Theorems 4 and 5) only recover \emph{pure} Nash Equilibria.
Indeed, the works~\cite{vlatakis2020no,giannou2021survival} show that for any general-sum game, some Nash Equilibrium is asymptotically stable under no-regret dynamics if and only if it is pure. Note that these results only imply \emph{local} convergence of the dynamic, i.e. if it is initialized sufficiently close to a pure Nash Equilibrium.
On the other hand, we utilize the special structure of the Bank Game to provide a \emph{global} convergence result, i.e. for a broader set of initializations that could be arbitrarily far from any Nash Equilibria.


% \section{Motivation} 

% \paragraph{Context and motivation} The motivation comes from the original performative prediction paper of~\citet{pmlr-v119-perdomo20a}. As a quick reminder, the paper shows that under distribution shifts, where the model we deploy every day impacts the distribution of input features, periodic retraining converges to stable and optimal solutions \emph{under some assumptions}. A crucial assumption this line of work makes is called \emph{insensitivity} (I think this is reminiscent of stability in adversarial contextual learning) where if the learner changes the model a bit, the input distribution also only changes by a small amount; this is controlled by an additive Lipschitzness condition. 

% Here, we note that this assumption is often unrealistic in game theoretic scenarios, in particular those involving competition between several learners. A simple example of this is Bertrand competition. Suppose two competitors set the same price $p$ for a product. In the classic Bertrand competition model, both competitors would split half the demand. However, if competitor $1$ decides to price at $p - \varepsilon$, competitor $1$ captures the entire demand. In turn, even though the competitor's parameter only changes by $\varepsilon$, that we can make arbitrarily small, the demand at each competitor changes by a large amount, breaking Lipschitzness.
