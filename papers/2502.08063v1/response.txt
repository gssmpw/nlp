\section{Related Work}
%\juba{find and add more general purpose references for this section} \gua{Vidya noted she will add some more game }

\paragraph{Offline learning with population responses} While our work looks at online and dynamic settings, the questions it asks are related to works on offline learning settings where the population distribution changes as a function of the deployed models. There are few major but mostly separate research directions that study such settings specifically in the context of machine learning. One such direction is \emph{strategic classification}, where agents may aim to modify their features to pass a classifier deployed by a learner**Mansha P., "Strategic Classification"**. Another setting is one where learners face self-selection effects: i.e., an agent will only face the learner in the first place if they meet some qualification criterion, or decide which learner to pick among several learners and a potential outside option, as a function of the learner(s)' deployed rules or models. These self-selection effects tend to introduce bias in the population distributions faced by the learners, as is the case in the present work. Finally, our work is related to a long line of work on distribution shifts, where testing data may follow a different distribution from training data---see**Kotlowski, W., "A Survey of Distribution Shift"** for a survey.

\paragraph{Performative Prediction} Performative prediction has received a lot of attention over the past few years**J. Lattimore and K. Szepesv√°ri, "Bandit Algorithms"**. The most seminal work on Performative Prediction is perhaps that of**Bartlett et al., "Performative Prediction in Reinforcement Learning"**. They provide necessary and sufficient conditions for repeated risk minimization and gradient descent to converge to performatively stable models in the context of performative prediction, and provide additional conditions for optimality. Performatively stable models are models that do not change under re-training, i.e. the model is a best response to the agent or population distribution it induces. Performatively optimal models minimize the loss function, explicitly taking into account the agents or population's response to the deployed model.

\paragraph{Multi-player Performative Prediction} As mentioned above, to the best of our knowledge, there few papers explicitly studying performative prediction in multi-player settings. Most relevant to use are**Gidel et al., "Stability Analysis of Performative Prediction"**, **Lai and Vayatis, "Performative Prediction for Dynamic Systems"**. Again,**Auer et al., "The Non-Stationary Multi-Armed Bandit Problem"** consider a setting where each learner obtains an i.i.d. sub-sample of the total population they are make prediction on, but does not model self-selection effects. **Mansha et al., "Multi-Agent Performative Prediction"** extend the traditional performative prediction setting to include general multi-player interactions, but does rely on insensitivity assumptions. In comparison, our model, while less general, explicitly includes self-selection effects and does not rely on insensitivity assumptions.

\paragraph{Game Dynamics and Online Learning} In this paper, we depart from the more commonly assumed choices of repeated empirical risk minimization/gradient descent and instead consider dynamics that are more conducive to multi-agent settings; namely, \emph{no-regret dynamics}**Mansha et al., "No-Regret Dynamics in Multi-Agent Settings"** (also commonly called \emph{adaptive heuristics} in the economics literature). However, even these more friendly dynamics are not immune to pitfalls. It is well known that such dynamics (and in fact, any ``uncoupled" dynamic where learners perform updates independently) \emph{cannot} converge to any Nash equilibria even for simple general-sum games**Marden et al., "Cooperative Control and Potential Games"**.
The only general-purpose positive result stipulates that the \emph{time average} of no-regret dynamics will converge to the polytope of coarse correlated equilibria. 
This is far from a satisfactory result for our purposes, as we are concerned with Nash equilibria and, ideally, the last iterate of the dynamics.
There is recent precedent for last-iterate analysis focusing on Nash Equilibria for special classes of games.
A plethora of last-iterate convergence results have been shown for zero-sum finite games (most relevant to our setting are**Marden et al., "Cooperative Control and Potential Games"**), but these usually require modifying the standard Exponential Weights dynamic to include optimism and/or extragradient method, as well as a sufficiently small step size.
In contrast, our game is not zero-sum, our analysis can handle any non-zero step size, and we consider the original Exponential Weights dynamic.
Finally, it is interesting to note that our convergence results (Theorems 4 and 5) only recover \emph{pure} Nash Equilibria.
Indeed, the works**Marden et al., "Cooperative Control and Potential Games"** show that for any general-sum game, some Nash Equilibrium is asymptotically stable under no-regret dynamics if and only if it is pure. Note that these results only imply \emph{local} convergence of the dynamic, i.e. if it is initialized sufficiently close to a pure Nash Equilibrium.
On the other hand, we utilize the special structure of the Bank Game to provide a \emph{global} convergence result, i.e. for a broader set of initializations that could be arbitrarily far from any Nash Equilibria.