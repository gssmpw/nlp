
\begin{algorithm}[h]
\caption{Online Learning Dynamic through Exponential Weights with Fresh Samples}
\begin{algorithmic}
\label{alg:Hedge:stoc} 
\STATE \textbf{Input}: step size $\eta>0$, time period $T$, number of fresh samples at each round $n_{samples}$
    \STATE \textbf{Initialization: $p_1^{(1)},p_2^{(1)}\in\Delta_4$}
\FOR{$t=1,\dots,T$}
\STATE \emph{Decision:} Each player $i$ plays $\theta^{(j)}$ with probability $p_{i,j}^{(t)}$;
\STATE \emph{Sampling:} Sample a batch of customers $\mathcal{C}^{(t)}=\{y_i\}_{i=1}^{n_{samples}}$, where $y_i\sim\D_y$, for $i\in[n_{samples}]$ 
\STATE \emph{Estimation:} Construct Estimated Utility function $\widehat{u}_1(\theta_1, \theta_2, \mathcal{C}^{(t)})$ and $\widehat{u}_2(\theta_1, \theta_2, \mathcal{C}^{(t)}) $ by \eqref{eq:utility:est1} and \eqref{eq:utility:est2}. 
\STATE \emph{Update Rule:} Players update their probability distributions to $p_{1}^{(t+1)},~p_{2}^{(t+1)}$ as follows:
\begin{align*}
&p^{(t+1)}_{1,j}\propto p^{(t)}_{1,j} \cdot \exp\left(\eta \sum_{k=1}^4 p^{(t)}_{2,k} \cdot \widehat{u}_1\left(\theta^{(j)},\theta^{(k)},\mathcal{C}^{(t)}\right)\right),~~\forall k\in[4]\\
&p^{(t+1)}_{2,j}\propto p^{(t)}_{2,j} \cdot \exp\left(\eta \sum_{k=1}^4 p^{(t)}_{1,k} \cdot \widehat{u}_2\left(\theta^{(k)},\theta^{(j)} ,\mathcal{C}^{(t)}\right)\right),~~\forall k\in[4]
\end{align*}
% $\forall i\in[4]$, $p^{(t+1)}_{1,i}\propto p^{(t)}_{1,i}\exp\left(\eta \sum_{j=1}^4 p^{(t)}_{2,j}u_1\left(\theta^{(i)},\theta^{(j)}\right)\right)$
% \STATE $\forall i\in[4]$, $p^{(t+1)}_{2,i}\propto p^{(t)}_{2,i}\exp\left(\eta \sum_{j=1}^4 p^{(t)}_{1,j}u_2\left(\theta^{(i)},\theta^{(j)}\right)\right)$
\ENDFOR
\end{algorithmic}
\end{algorithm}
In the previous section, we proved that the Exponential Weights dynamic always learns to converges to pure Nash Equilibria under the idealized assumption that the learners \emph{exactly knew} the customer distribution. 
In this section, we relax this assumption. 
Instead, we assume that each bank only has sample access to individuals in the population, instead of knowing the full distribution of consumers. 
This is a more natural real-world learning model; e.g. in a scenario where the banks face a fresh (but finite) batch of customers every time they update their loan policy.

Formally, at every time step $t$, a fresh batch of $n_{samples}$ i.i.d. samples from \( D_y \) arrives. We denote this batch by \( \mathcal{C}^{(t)} \). Each player $i$ then uses this batch of samples to construct an empirical estimate of the utility matrix. For Bank 1, the resulting utility matrix is given by: 


% Here, we introduce a stochastic version of Algorithm \ref{alg:Hedge}, which relies on a batch of i.i.d. samples from \( D_y \) at each round \( t \), yet still converges with high probability. The algorithm is presented in Algorithm \ref{alg:Hedge:stoc}.  Notably, the stochastic dynamic closely resembles Algorithm \ref{alg:Hedge}, with the key difference that we use a sampled set of customers, \( \mathcal{C}_t \), to estimate the utility function \( \widehat{u}_i(\theta_1, \theta_2, \mathcal{C}^{(t)}) \) for \( i \in [2] \), which is then used to update decisions. More specifically, for Bank 1, let
\begin{align}\label{eq:utility:est1}
    \widehat{u}_1(\theta_1, \theta_2, \mathcal{C}^{(t)}) 
    &=   \frac{1}{n_{samples}}\sum_{y\in\mathcal{C}^{(t)}}\bigg[\mathbb{I}\{{\tau_1 \leq y < \tau_2 \ \cup \ (\tau_1, \tau_2 \leq y \ \cap \ \gamma_1 < \gamma_2)}\} \cdot \big((1+\gamma_1)y - (1-y)\big) \nonumber \\
    & + \frac{1}{2} \mathbb{I}\{\tau_1, \tau_2 \leq y \ \cap \ \gamma_1 = \gamma_2\} \cdot \big((1+\gamma_1)y - (1-y)\big) \bigg]
\end{align}
%
Similarly, for Bank 2, we have 
\begin{align}\label{eq:utility:est2}
    \widehat{u}_2(\theta_1, \theta_2, \mathcal{C}^{(t)}) 
    &=  \frac{1}{n_{samples}}\sum_{y\in\mathcal{C}^{(t)}}\bigg[ \mathbb{I}\{{\tau_2 \leq y < \tau_1 \ \cup \ (\tau_1, \tau_2 \leq y \ \cap \ \gamma_2 < \gamma_1)}\} \cdot \big((1+\gamma_2)y - (1-y)\big) \nonumber \\
    & + \frac{1}{2} \mathbb{I}\{\tau_1, \tau_2 \leq y \ \cap \ \gamma_1 = \gamma_2\} \cdot \big((1+\gamma_2)y - (1-y)\big) \bigg]
\end{align}
We have the following claim which is easy to verify. 
\begin{claim}
$\widehat{u}_1(\theta_1, \theta_2, \mathcal{C}^{(t)})$ is an unbiased estimator of  
$ {u}_1(\theta_1, \theta_2)$. Similarly, $\widehat{u}_1(\theta_1, \theta_2, \mathcal{C}^{(t)})$ is an unbiased estimator of  
$ u_2(\theta_1, \theta_2)$.
\end{claim}
%
This follows immediately from Equations~\eqref{eq:utility} and~\eqref{eq:utility:est1}, noting that $\widehat{u}_1(\theta_1, \theta_2, \mathcal{C}^{(t)})$ is just the traditional empirical estimate of $u_1(\theta_1,\theta_2)$ over $\mathcal{C}^{(t)}$.

The full algorithm used by the learners is formally provided in Algorithm~\ref{alg:Hedge:stoc}. 
Informally, Algorithm~\ref{alg:Hedge:stoc} runs the Exponential Weight dynamic, but with utilities calculated according to the empirical estimates Equation~\eqref{eq:utility:est1} and~\eqref{eq:utility:est2} for Banks 1 and 2 respectively.
Our main result of this section highlights that Algorithm~\ref{alg:Hedge:stoc} still converges to the exact same outcomes as~\ref{alg:Hedge}, so long as the number of samples in each time step is large enough. More specifically, we provide the convergence results of Algorithm \ref{alg:Hedge:stoc} under four different possible cases for the initialization that depend on the signs of $\varepsilon_1$ and $\varepsilon_2$ as follows.
The result parallels Theorem 4, proving convergence at an exponential rate to the same set of Nash Equilibria as a function of game parameters $(\epsilon_1,\epsilon_2)$ and with similar initialization conditions.
The reader is advised to read the results side by side.
\\
\\\textbf{Case I: $\epsilon_1<0,\epsilon_2<0$:} Then, Algorithm \ref{alg:Hedge:stoc} approximately converges to $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$. Specifically:

\begin{fixedtheorem}{Part I}\label{thm:stocccc}
Suppose $\epsilon_1<0,\epsilon_2<0$. Let 
$t_s''=t_s'+1+\left\lceil\frac{4}{\eta\xi_1}\log\frac{4(\xi_1+3)c_{ini}}{\xi_1}\right\rceil$ be a constant. For Algorithm \ref{alg:Hedge:stoc}, for any error tolerance $\omega>0$ and $\delta\in(0,1)$, if\ \ $T\geq \frac{4}{\xi_1}\left(3+\frac{\xi_1}{4}\right)t^{''}_s+\frac{4}{\eta\xi_1}\log\frac{c_{ini}}{\omega}$, and $n_{samples}\geq \frac{1280\log\frac{64T}{\delta}}{\xi_1^2}$, then with probability at least $1-\delta$, for $i\in[2]$,  we have 
$$p_{i,2}^{(T)}\leq \omega,\ \  \text{and}\ \ \  p_{i,3}^{(T)}\geq 1-\min\{3\omega,1\}.$$
\end{fixedtheorem}

\noindent \textbf{Case II: $\epsilon_1>0,\epsilon_2>0$}: Algorithm \ref{alg:Hedge:stoc} approximately converges to $((\tau_{h},\gamma_{\ell}),(\tau_{h},\gamma_{\ell}))$. Specifically:

\begin{fixedtheorem}{Part II}
Suppose $\epsilon_1>0,\epsilon_2>0$. Let $t_s'''=\left\lceil\frac{2}{\eta\xi_1}\log\frac{4(1+\xi_1)c_{\text{ini}}}{\xi_1}\right\rceil+1$ be a constant.  For Algorithm \ref{alg:Hedge:stoc}, for any error tolerance $\omega>0$, and $\delta\in(0,1)$, if $T\geq \frac{4}{\xi_1}\left(1+\frac{\xi_1}{4}\right)t^{''}_s+\frac{4}{\eta\xi_1}\log\frac{c_{ini}}{\omega}$, and $n_{samples}\geq \frac{1280\log\frac{64T}{\delta}}{\xi_1^2}$, then with probability at least $1-\delta$, for $i\in[2]$, we have 
$$p_{i,3}^{(T)}\leq \omega,\ \  \text{and}\ \ \  p_{i,2}^{(T)}\geq 1-\min\{3\omega,1\}.$$
\end{fixedtheorem}

\noindent \textbf{Case III: $\epsilon_1<0$, and $\epsilon_2>0$.} Then Algorithm \ref{alg:Hedge:stoc} with proper initialization converges to the asymmetric pure NE. 

\begin{fixedtheorem}{Part III}
Suppose $\epsilon_1<0,\epsilon_2>0$. Assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. 
Moreover, let $\epsilon^{(t)}_{1,1} = \widehat{u}_1(\theta^{(2)},\theta^{(2)},\C^{(t)})-\widehat{u}_1(\theta^{(3)},\theta^{(2)},\C^{(t)})$, and $\epsilon^{(t)}_{1,2} = \widehat{u}_1(\theta^{(2)},\theta^{(3)},\C^{(t)})-\widehat{u}_1(\theta^{(3)},\theta^{(3)},\C^{(t)}),$ $\epsilon^{(t)}_{2,1} = \widehat{u}_2(\theta^{(2)},\theta^{(2)},\C^{(t)})-\widehat{u}_2(\theta^{(2)},\theta^{(3)},\C^{(t)})$, and $\epsilon^{(t)}_{2,2} = \widehat{u}_2(\theta^{(3)},\theta^{(2)},\C^{(t)})-\widehat{u}_2(\theta^{(3)},\theta^{(3)},\C^{(t)}).$
\begin{itemize}
\item Suppose the initialization 
$p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}<-\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}>\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$. For Algorithm \ref{alg:Hedge:stoc}, with any error tolerance $\omega>0$ and $\delta\in(0,1)$, if $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$, where $c'_{ini}=\max_{i\in[2]}\left\{\frac{\max_{j\in\{2,3\}}p^{(1)}_{i,j}}{\min_{i\in\{2,3\}}p^{(1)}_{i,j}}\right\}$, and $n_{samples}\geq \frac{1280\log\frac{64T}{\delta}}{\xi_1^2}$, then with probability at least $1-\delta$, we have 
$$p^{(T)}_{1,2}\geq \frac{1}{1+\omega},\ \  \text{and}\ \ \  p^{(T)}_{2,3}\geq \frac{1}{1+\omega}.$$  
\item Suppose the initialization $p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}>\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}<-\Delta$ for some constant $\Delta>\frac{\xi_1}{2}$. For Algorithm \ref{alg:Hedge:stoc}, with any error tolerance $\omega>0$ and $\delta\in(0,1)$, if $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$ and $n_{samples}\geq \frac{1280\log\frac{64T}{\delta}}{\xi_1^2}$, then with probability at least $1-\delta$, we have $$p^{(T)}_{1,3}\geq \frac{1}{1+\omega},\ \  \text{and}\ \ \  p^{(T)}_{2,2}\geq \frac{1}{1+\omega}.$$ 
\end{itemize}
\end{fixedtheorem}


\noindent\textbf{Case IV: $\epsilon_1>0$, $\epsilon_2<0$}. Then Algorithm \ref{alg:Hedge:stoc}  with proper initialization converges to symmetric pure NE.

\begin{fixedtheorem}{Part IV}
Suppose $\epsilon_1>0$, $\epsilon_2<0$. Assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. We have:
\begin{itemize}
    \item Assume $p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}<-\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}<-\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$. For Algorithm \ref{alg:Hedge:stoc}, with any error tolerance $\omega>0$ and $\delta\in(0,1)$, if $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$ and $n_{samples}\geq \frac{1280\log\frac{64T}{\delta}}{\xi_1^2}$, then with probability at least $1-\delta$, we have  
    $$p^{(T)}_{1,3}\geq \frac{1}{1+\omega},\ \ \text{and}\ \ \  p^{(T)}_{2,3}\geq \frac{1}{1+\omega}.$$
    \item  Assume  
$p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}>\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}>\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$. For Algorithm \ref{alg:Hedge:stoc}, with any error tolerance $\omega>0$ and $\delta\in(0,1)$, if $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$ and $n_{samples}\geq \frac{1280\log\frac{64T}{\delta}}{\xi_1^2}$, then with probability at least $1-\delta$, we have 
$$p^{(T)}_{1,2}\geq \frac{1}{1+\omega},\ \ \text{and}\ \ \  p^{(T)}_{2,2}\geq \frac{1}{1+\omega}.$$
\end{itemize}
\end{fixedtheorem}



%\item If $\epsilon_1<0,\epsilon_2<0$, let $t_s''=t_s'+1+\left\lceil\frac{4}{\eta\xi_1}\log\frac{4(\xi_1+3)c_{ini}}{\xi_1}\right\rceil$ be a constant. For any error tolerance $\omega>0$, if\ \ $T\geq \frac{4}{\xi_1}\left(3+\frac{\xi_1}{4}\right)t^{''}_s+\frac{4}{\eta\xi_1}\log\frac{c_{ini}}{\omega}$, then for $i\in[2]$, we have $p_{i,2}^{(T)}\leq \omega,$ and $p_{i,3}^{(T)}\geq 1-\min\{3\omega,1\}$. \emph{It implies that the algorithm approximately converges to $((\tau_{h},\gamma_{\ell}),(\tau_{h},\gamma_{\ell}))$ with an error on on the order of $O(\omega)$}.
%\item If $\epsilon_1>0$ and $\epsilon_2>0$, then let $t_s'''=\left\lceil\frac{2}{\eta\xi_1}\log\frac{4(1+\xi_1)c_{\text{ini}}}{\xi_1}\right\rceil+1$ be a constant.  For any error tolerance $\omega>0$, if $T\geq \frac{4}{\xi_1}\left(1+\frac{\xi_1}{4}\right)t^{''}_s+\frac{4}{\eta\xi_1}\log\frac{c_{ini}}{\omega}$, then for $i\in[2]$, we have $p_{i,3}^{(T)}\leq \omega,$ and $p_{i,2}^{(T)}\geq 1-\min\{3\omega,1\}$. \emph{It implies that the algorithm approximately converges to $((\tau_{\ell},\gamma_{h}),(\tau_{\ell},\gamma_{h}))$ with an error on on the order of $O(\omega)$.}

%\item If $\epsilon_1<0$, and $\epsilon_2>0$, assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. Moreover, let $\epsilon^{(t)}_{1,1} = \widehat{u}_1(\theta^{(2)},\theta^{(2)},\C^{(t)})-\widehat{u}_1(\theta^{(3)},\theta^{(2)},\C^{(t)})$, and $\epsilon^{(t)}_{1,2} = \widehat{u}_1(\theta^{(2)},\theta^{(3)},\C^{(t)})-\widehat{u}_1(\theta^{(3)},\theta^{(3)},\C^{(t)}),$ $\epsilon^{(t)}_{2,1} = \widehat{u}_2(\theta^{(2)},\theta^{(2)},\C^{(t)})-\widehat{u}_2(\theta^{(2)},\theta^{(3)},\C^{(t)})$, and $\epsilon^{(t)}_{2,2} = \widehat{u}_2(\theta^{(3)},\theta^{(2)},\C^{(t)})-\widehat{u}_2(\theta^{(3)},\theta^{(3)},\C^{(t)}).$Suppose the initialization $p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}<-\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}>\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$. Then with any error tolerance $\omega>0$, for $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$, where $c'_{ini}=\max_{i\in[2]}\left\{\frac{\max_{j\in\{2,3\}}p^{(1)}_{i,j}}{\min_{i\in\{2,3\}}p^{(1)}_{i,j}}\right\}$,  we have $p^{(T)}_{1,2}\geq \frac{1}{1+\omega}$, and $p^{(T)}_{2,3}\geq \frac{1}{1+\omega}$. \emph{It implies that the algorithm approximately converges to $((\tau_{\ell},\gamma_{h})(\tau_{h},\gamma_{\ell}))$ with an error of order $O(\omega)$}. If $p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}>\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}<-\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$, then for $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$, we have $p^{(T)}_{1,3}\geq \frac{1}{1+\omega}$, and $p^{(T)}_{2,2}\geq \frac{1}{1+\omega}$. \emph{It implies that the algorithm approximately converges to $((\tau_{h},\gamma_{\ell}),(\tau_{\ell},\gamma_{h}))$ with an error of order $O(\omega)$.}
% \item If $\epsilon_1>0$, $\epsilon_1<0$, assume $p^{(1)}_{i,1}=p^{(1)}_{i,4}=0$. Moreover, suppose the initialization $p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}<-\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}<-\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$. Then $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$, we have $p^{(T)}_{1,3}\geq \frac{1}{1+\omega}$, and $p^{(T)}_{2,3}\geq \frac{1}{1+\omega}$. \emph{It implies that the algorithm approximately converges to $((\tau_{h},\gamma_{\ell})(\tau_{h},\gamma_{\ell}))$ with an error of order $O(\omega)$.} If $p^{(1)}_{2,2}\epsilon^{(1)}_{1,1}+p^{(1)}_{2,3}\epsilon^{(1)}_{1,2}>\Delta$, and $p^{(1)}_{1,2}\epsilon^{(1)}_{2,1}+p^{(1)}_{1,3}\epsilon^{(1)}_{2,2}>\Delta$, for some constant $\Delta>\frac{\xi_1}{2}$. Then for $T\geq \frac{1}{\eta\left(\Delta-\frac{\xi_1}{2}\right)}\log\frac{c'_{ini}}{\omega}$, we have $p^{(T)}_{1,2}\geq \frac{1}{1+\omega}$, and $p^{(T)}_{2,2}\geq \frac{1}{1+\omega}$. \emph{It implies that the algorithm approximately converges to $((\tau_{\ell},\gamma_{h})(\tau_{\ell},\gamma_{h}))$ with an error of order $O(\omega)$.}
%\end{itemize}
The proof of the above theorem is given in Appendix \ref{proof:Theorem 5} and closely resembles the proof of Theorem 4, in that the the same qualitative trends in the evolution of the probabilities on various pure strategies hold---there is only a difference in the precise rate of exponential convergence due to sampling-induced error. Importantly, Theorem 5 does not hold for an arbitrary small number of samples. In particular, the required lower bound on $n_{samples}$ depends on the time horizon $T$---however, this dependence is minimal and logarithmic in $T$.
Interestingly, we observe in Section~\ref{sec: exp-results} to follow that the iterates still converge empirically when $n_{samples}$ violates the above requirement --- and even in the extreme case where $n_{samples} = 1$.

%\juba{there is no pointer to where the theorem is proved?}