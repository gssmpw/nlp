\section{Supplementary Materials for Fine-grained Analysis}

\subsection{LLM-as-Encoder: Compared with LMs}
\input{tables/encoder_comp_fullsupervised}

We supplement the comparison between LLM-as-Encoder and LM-as-Encoder under supervised settings in Table \ref{tab:encoder_comp_fullysupervised}. The key takeaway that \textbf{LLMs outperform LMs as Encoders in heterophilic graphs} remains valid. This conclusion is particularly evident on the arXiv dataset, where the performance gap between LM- and LLM-generated embeddings reaches up to $7\%$ on MLP and $3\%$ on GraphSAGE. Additionally, we observe that in supervised settings, the performance gap between LM- and LLM-as-Encoders becomes less pronounced compared to semi-supervised settings. We still consider the theoretical insights in Equation \eqref{eq:mutual_info} for explanation: Increased supervision enhances the mutual information between labels and graph structure, i.e., $I(\mathcal{E}, \mathcal{Y}_{l})$, thereby rendering the second term less significant and diminishing the advantages provided by more powerful encoders like LLMs.



\subsection{LLM-as-Predictor: Sensitivity to LLM Backbones}

\input{tables/llaga_llm}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/qwen_cora.pdf}        
        \caption{Cora}
        \label{fig:qwen_cora}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/qwen_wikics.pdf}
        \caption{WikiCS}
        \label{fig:qwen_citeseer}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/qwen_instagram.pdf}
        \caption{Instagram}
        \label{fig:qwen_instagram}
    \end{subfigure}
    \caption{\textbf{Performance trends within Qwen-series in different scales using LLaGA framework in semi-supervised settings.}}
    \label{fig:llaga_scaling}
\end{figure}


\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/qwen_cora_s.pdf}        
        \caption{Cora}
        \label{fig:qwen_cora_s}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/qwen_wikics_s.pdf}
        \caption{WikiCS}
        \label{fig:qwen_citeseer_s}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/qwen_instagram_s.pdf}
        \caption{Instagram}
        \label{fig:qwen_instagram_s}
    \end{subfigure}
    \caption{\textbf{Performance trends within Qwen-series in different scales using LLaGA framework in supervised settings.}}
    \label{fig:llaga_scaling_s}
\end{figure}

\begin{table}[!h]
    \centering
    \caption{\textbf{Training and inference times of Qwen-series models ranging from 3B to 32B parameters.}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cc|ccc|ccc|ccc}
       \toprule
        \rowcolor{COLOR_MEAN} &  & \multicolumn{3}{c|}{\textbf{Semi-supervised Training Times}} & \multicolumn{3}{c}{\textbf{Supervised Training Times}} & \multicolumn{3}{|c}{\textbf{Avg. Inference Times Per Case}} \\
       \rowcolor{COLOR_MEAN} \multirow{-2}{*}{\textbf{GPU Device}}  & \multirow{-2}{*}{\textbf{LLM}} & Cora & WikiCS & Instagram  &  Cora & WikiCS & Instagram &  Cora & WikiCS & Instagram \\  \midrule 
        &  Qwen-3B & 2.2min & 7.1min & 5.8min & 8.6min & 33.5min & 20.2min & 32.3ms & 37.3ms & 26.9ms \\ 
        \multirow{-2}{*}{1 NVIDIA A6000-48G} & Qwen-7B & 4.7min & 15.3min & 8.2min & 13.3min & 59.4min & 43.2min & 50.9ms & 55.9ms & 43.4ms \\ 
       & Qwen-14B & 8.9min & 25.5min & 15.7min & 30.8min & 2.3h & 1.5h & 97.6ms & 103.0ms & 83.2ms \\ 
        \multirow{-2}{*}{2 NVIDIA A6000-48G} & Qwen-32B & 18.9min & 43.6min & 30.7min & 52.2min & 3.3h & 2.0h & 254.7ms & 262.6ms & 232.4ms \\ 
       \bottomrule
    \end{tabular}
    }
    \label{tab:qwen_cost}
\end{table}


We evaluate LLaGA with different LLM backbones in semi-supervised settings, as detailed in Table \ref{tab:llaga_llm}. We examine two primary trends: (1) \textbf{Scaling within the same series}: Assessing whether scaling laws apply to node classification tasks by using LLMs from the same series, and (2) \textbf{Model selection at similar scales}: Identifying the most suitable LLM for node classification tasks by comparing models of similar scales.

\textbf{Scaling within the same series}: We plot performance trends across several datasets under both semi-supervised and supervised settings to clearly illustrate these dynamics. From Figure \ref{fig:llaga_scaling} and Figure \ref{fig:llaga_scaling_s}, we conclude that scaling laws generally hold: as the Qwen model size increases from 3B to 32B parameters, performance improves, indicating that larger model sizes enhance task performance. However, the 7B and 14B models are sufficiently large, typically representing the point beyond which further increases in model size yield only marginal improvements but introducing huge computational costs (Table \ref{tab:qwen_cost}). Unexpectedly, in the Instagram dataset under semi-supervised settings, the Qwen-32B model experiences a performance drop. This may be because 32B models require extensive data to train effectively, making them less robust and stable compared to smaller models. Based on these findings, we recommend the 7B or 14B models as they offer an optimal balance between performance and computational costs.

\textbf{Model selection at similar scales:} By comparing the performance of Mistral-7B, Qwen-7B, and LLaMA-8B in Table \ref{tab:llaga_llm}, we conclude that Mistral-7B outperforms the other two similarly scaled LLMs in most cases. This makes Mistral-7B the optimal choice as a backbone LLM for node classification tasks.



\clearpage
\newpage
\subsection{LLM-as-Predictor: Biased and Hallucinated Predictions}\label{sec:llm_bias_pred}

During our experiments, we found that LLM-as-Predictor methods are vulnerable to limited supervision. In addition to standard metrics such as Accuracy (Table \ref{tab:mainexp}) and Macro-F1 scores (Table \ref{tab:mainexp_f1}), their predictions also exhibit significant  \textbf{biases} and \textbf{hallucinations}.

\textbf{Biased Predictions:} For datasets with fewer labels, LLM-as-Predictor methods tend to disproportionately predict certain labels while under-predicting others. To illustrate this phenomenon, we compare the ground-truth label distributions with the predicted label distributions. Specifically, we present different LLM-as-Predictor methods, LLM\textsubscript{IT}, GraphGPT, and LLaGA, in Figure \ref{fig:instagram_predictor}, and the LLaGA method with various LLM backbones in Figure \ref{fig:instagram_llm}, using the Instagram dataset in semi-supervised settings, which has two labels.

From Figure \ref{fig:instagram_predictor}, we can directly observe that LLaGA and GraphGPT predominantly bias towards the first class, while LLM\textsubscript{IT} tends to predict the second class more frequently. The predicted label distributions of LLMs are more \textbf{polarized} compared to the ground-truth distributions, where the two labels are roughly in a $6:4$ proportion. In contrast, LLMs tend to predict in ratios such as $8:2$ or $1:9$. This observation also holds across different LLMs, as shown in Figure \ref{fig:instagram_llm}, where both Qwen-7B and LLaMA-8B tend to bias towards the first label. A similar example on the Pubmed dataset, which contains three classes in a semi-supervised setting, is shown in Figure \ref{fig:distribution_pubmed}. Here, the predictor methods tend to bias towards the third class, while LLaGA with Qwen-7B tends to predict the second class. Additionally, in the semi-supervised setting for Pubmed, the training data consists of only $60$ samples, which is insufficient to train a robust predictor model, leading to high levels of hallucinations across all methods.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/instagram_predictor.pdf}
        \caption{Different LLM-as-Predictor Methods}
        \label{fig:instagram_predictor}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/instagram_llm.pdf}
        \caption{Different LLM Backbones within LLaGA}
        \label{fig:instagram_llm}
    \end{subfigure}
    \caption{\textbf{Biased predictions by LLM-as-Predictor methods on the Instagram dataset:} Comparison of ground-truth label distributions with predictor-generated label distributions.}
    \label{fig:distribution_instagram}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.38\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/pubmed_predictor.pdf}
        \caption{Different LLM-as-Predictor Methods}
        \label{fig:pubmed_predictor}
    \end{subfigure}
    \begin{subfigure}[b]{0.38\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exp_new/pubmed_llm.pdf}
        \caption{Different LLM Backbones within LLaGA}
        \label{fig:pubmed_llm}
    \end{subfigure}
    \caption{\textbf{Biased predictions by LLM-as-Predictor methods on the Pubmed dataset.}}
    \label{fig:distribution_pubmed}
\end{figure}


\textbf{Hallucinations: }In addition to biased predictions, we observed that a certain portion of the LLMs' outputs \textbf{fall outside the valid label space} or \textbf{contain unexpected content that cannot be parsed}. In semi-supervised settings, the limited training data restricts these predictor methods from developing effective models, resulting in failures to follow instructions and understand dataset-specific classification requirements. To illustrate that, we provide both quantitative and qualitative analyses as follows:

\begin{itemize}
    \item \textbf{Quantitative Analysis: }Table \ref{tab:predictor_hall} presents the hallucination rates of each LLM-as-Predictor method across various experimental datasets in both semi-supervised and supervised settings. The hallucination rate is calculated as the proportion of outputs containing invalid labels or unexpected content among all test cases, where higher values indicate poorer classification performance. Hallucinations are most severe on the Pubmed and Citeseer datasets within semi-supervised settings, where the number of training samples does not exceed hundreds, making effective model training challenging. \textbf{This demonstrates that the number of training samples significantly impacts the mitigation of hallucinations}: even in semi-supervised settings, larger datasets like Books and Photo provide thousands of training samples, resulting in hallucination ratios consistently below $1\%$. Therefore, this further verifies that LLM-as-Predictor methods require extensive labeled data for effective model training.
    
    \item \textbf{Qualitative Analysis: }We provide several examples to facilitate the comprehension of hallucinated predictions, which we categorize into three types: (1) misspellings of existing labels, (2) generation of non-existent types, and (3) unexpected content that cannot be parsed. Illustrative examples of these types from GraphGPT's outputs on the Citeseer and Pubmed datasets are presented in Table \ref{tab:predictor_hall_example}.
    
\end{itemize}

\begin{table}[!t]
    \centering
    \caption{\textbf{Average hallucination ratios ($\%$) of LLM-as-Predictor methods.} The hallucination rate is calculated as the proportion of outputs containing invalid labels or unexpected content across all test cases, where higher values indicate poorer classification ability. Hallucinations $>1\%$ are \textcolor{brown}{\textbf{highlighted}}.}
    \vspace*{-8pt}
    \resizebox{0.9\linewidth}{!}{
      \begin{tabular}{cc|ccccccccc}
        \toprule
        \rowcolor{COLOR_MEAN} \textbf{Setting} &  \textbf{Method} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{WikiCS} & \textbf{Instagram} & \textbf{Reddit} & \textbf{Books} & \textbf{Photo} & \textbf{Computer}  \\ \midrule
       \multirow{4}{*}{\textbf{Semi-supervised}} &  \# Train Samples & 140 & 120 & 60 & 580 & 1,160 & 3,344 & 4,155 & 4,836 & 8,722 \\ 
       & {LLM\textsubscript{IT}} & 0.43 & \textcolor{brown}{\textbf{13.08}} & \textcolor{brown}{\textbf{9.24}} & 0.06 & 0.00 & 0.00 & 0.01 & 0.02 & 0.02  \\ 
       & {GraphGPT} & \textcolor{brown}{\textbf{7.56}} & \textcolor{brown}{\textbf{2.51}} & \textcolor{brown}{\textbf{15.97}} & \textcolor{brown}{\textbf{7.76}} & 0.28 & \textcolor{brown}{\textbf{1.78}} & 0.51 & 0.72 & 0.27 \\
       &  LLaGA & 0.35 & 0.20 & 0.29 & 0.00 & 0.01 & 0.02 & 0.00 & 0.00 & 0.00 \\  \midrule

       \multirow{4}{*}{\textbf{Supervised}} & \# Train Samples & 1,624 & 1,911 & 11,830 &  7,020 & 6,803 & 20,060 & 24,930 & 29,017 & 52,337 \\ % \cmidrule(r){2-11} 
       & LLM\textsubscript{IT} & 0.06 & \textcolor{brown}{\textbf{13.61}} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.01 & 0.02 \\ 
       & GraphGPT & \textcolor{brown}{\textbf{1.29}} & 0.63 & 0.08 & \textcolor{brown}{\textbf{1.64}} & 0.13 & 0.50 & 0.11 & 0.11 & 0.10 \\ 
       & LLaGA & 0.03 & 0.00 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 \\
       
         \bottomrule
    \end{tabular}
    }
    \label{tab:predictor_hall}
\end{table}

\begin{table}[!t]
    \centering
    \caption{\textbf{Examples of hallucinations in GraphGPT's outputs on the Citeseer and Pubmed datasets.}}
     \vspace*{-8pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cc|cc}
      \toprule
      \rowcolor{COLOR_MEAN}  &  \multicolumn{2}{c|}{\textbf{Citeseer}} & \multicolumn{2}{c}{\textbf{Pubmed}} \\ 
        \rowcolor{COLOR_MEAN} \multirow{-2}{*}{\textbf{Error Type}} & \textbf{Prediction} & \textbf{Ground-truth} &  \textbf{Prediction} & \textbf{Ground-truth} \\ \midrule
        \textbf{Misspelling} & AGents & Agents & Type II diabetes & Type 2 diabetes \\ \midrule

        \multirow{4}{*}{\begin{tabular}{c}
             \textbf{Non-existent} \\ \textbf{Types}
        \end{tabular}} & Logic and Mathematics & ML (Machine Learning) & Type 3 diabetes of the young (MODY) & Type 2 diabetes \\
        & Information Extraction &	IR (Information Retrieval) & Genetic Studies of Wolfram Syndrome & Type 2 diabetes\\ 
        & Pattern Recognition & ML (Machine Learning) & Experimentally induced insulin resistance & Type 2 diabetes \\ 
        & Multiagent Systems & Agents & Experimentally induced oxidative stress & Experimentally induced diabetes \\ \midrule

        \multirow{2}{*}{\begin{tabular}{c}
             \textbf{Unexpected} \\ \textbf{Contents}
        \end{tabular}} & \multicolumn{2}{c|}{H.4.1 Office Automation: Workflow Management} & \multicolumn{2}{c}{membrane is not altered by diabetes.} \\ 

        & \multicolumn{2}{c|}{The citation graph is given by the following: ...} & \multicolumn{2}{c}{What is the sensitivity and specificity of the IgA-EMA test ...} \\

        \bottomrule
    \end{tabular}
    }
    \label{tab:predictor_hall_example}
\end{table}


\clearpage 
\newpage
