\section{Prompts}

\subsection{Prompt in LLM-as-Predictor Methods}\label{sec:predictor_prompt}
For GraphGPT \cite{tang2023graphgpt} and LLaGA \cite{chen23llaga}, we utilize the prompt templates provided in their original papers for several datasets, including Cora and arXiv. For datasets not originally addressed, such as Photo, we adapt their prompt designs to create similarly formatted prompts. For LLM Instruction Tuning, we carefully craft prompt templates tailored to each dataset to directly guide the LLMs in performing classification tasks. Below is a summary of these prompt templates using Cora as an example: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}} denotes the dataset-specific label space (Table \ref{tab:dataset_detail}), \textcolor{brown}{\textbf{$\langle$graph$\rangle$}} represents the tokenized graph context, and \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}} refers to the node's original raw text.

\begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, arc=2pt, left=5pt, right=5pt]

\textbf{Illustration of Prompts Utilized by LLM-as-Predictor Methods on the Cora Dataset} \vspace*{5pt}
\small

\textbf{LLM Instruction Tuning:} { 
Given a node-centered graph with centric node description: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}, each node represents a paper, we need to classify the center node into 7 classes: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}, please tell me which class the center node belongs to?
} \vspace*{3pt}

\textbf{GraphGPT:} {
Given a citation graph: \textcolor{brown}{\textbf{$\langle$graph$\rangle$}}, where the 0-th node is the target paper, with the following information: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. Question: Which of the following specific research does this paper belong to: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}. Directly give the full name of the most likely category of this paper.
} \vspace*{3pt}


\textbf{LLaGA:} {
  Given a node-centered graph: \textcolor{brown}{\textbf{$\langle$graph$\rangle$}}, each node represents a paper, we need to classify the center node into 7 classes: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}, please tell me which class the center node belongs to?
}  
\end{tcolorbox}




\subsection{Prompt in Zero-shot Scenarios}\label{sec:zeroshot_prompt}

For LLM Direct Inference, we consider prompt templates including Direct, Chain-of-Thought \cite{Wei2022ChainOT}, Tree-of-Thought \cite{yao2023tree}, and ReACT \cite{Yao2022ReActSR}. The latter three methods are effective in enhancing LLM reasoning abilities across different tasks. Therefore, we adopt these advanced prompts for node classification to assess their continued effectiveness. Illustrations of these prompts on the Cora dataset are shown below: 
\begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, arc=2pt, left=5pt, right=5pt]
\textbf{Illustration of Advanced Prompts Utilized by LLM Inference on the Cora Dataset}
\vspace*{5pt}

\small

\textbf{Direct:}
{Given the information of the node: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. Question: Which of the following categories does this paper belong to? Here are the categories: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}. Reply only with one category that you think this paper might belong to. Only reply with the category name without any other words. 
} \vspace*{3pt}

 \textbf{Chain-of-Thought:} 
{
Given the information of the node: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. 
Question: Which of the following types does this paper belong to? 
Here are the 7 categories: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}. \textcolor{red}{\textbf{Let's think about it step by step.}} Analyze the content of the node and choose one appropriate category.
Output format: $\langle$reason: $\rangle$, $\langle$classification: $\rangle$
 } \vspace*{3pt}

\textbf{Tree-of-Thought: }{Given the information of the node: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. 
Imagine three different experts answering this question. 
All experts will write down 1 step of their thinking, and then share it with the group. 
Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave.
Question: Based on this information, which of the following categories does this paper belong to? 
Here are 7 categories: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}. \textcolor{red}{\textbf{Let's think through this using a tree of thought approach. }}
Output format: $\langle$discussion: $\rangle$, $\langle$classification: $\rangle$. The classification should only consist of one of the category names listed.
} \vspace*{3pt}

 \textbf{ReACT: }{
Given the information of the node: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. Your task is to determine which of the following categories this paper belongs to. 
Here are the 7 categories: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}.
\textcolor{red}{\textbf{Solve this question by interleaving the Thought, Action, and Observation steps.}} Thought can reason about the current situation, and Action can be one of the following:
(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.
(2) Lookup[keyword], which returns the next sentence containing the keyword in the current passage.
(3) Finish[answer], which returns the answer and finishes the task.
The output format must be $\langle$process: $\rangle$, $\langle$classification: $\rangle$. The classification should only consist of one of the category names listed.
}
\end{tcolorbox}


Additionally, we incorporate the central node's structural information into extended prompts to evaluate performance. We propose two variants for integrating neighbor information: (1) ``w. Neighbor'': we concatenate the texts of all $1$-hop neighbors of the central node as enriched context, and (2) ``w. Summary'': we first provide all neighbors' information to LLMs to generate a summary that highlights the common points among the neighbors. Then, we feed both the generated summary and the node's text to LLMs to facilitate prediction. The prompt templates for these methods on the Cora dataset are illustrated below:

\begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, arc=2pt, left=5pt, right=5pt]

\textbf{Illustration of Structure-enriched Prompts Utilized by LLM Inference on the Cora Dataset} \vspace*{5pt}
 \small
 
\textbf{w. Neighbor: }{Given the information of the node: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. Given the information of its neighbors \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}.
Here I give you the content of the node itself and the information of its 1-hop neighbors. The relation between the node and its neighbors is `citation'. Question: Based on this information, Which of the following sub-categories of AI does this paper belong to? Here are the 7 categories: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}. Reply only one category that you think this paper might belong to. Only reply with the category name without any other words. } \vspace*{4pt}

\textbf{w. Summary (Step 1): }{
The following list records papers related to the current one, with the relationship being `citation': \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}. Please summarize the information above with a short paragraph, and find some common points that can reflect the category of the paper.
} \vspace*{2pt}

\textbf{w. Summary (Step 2): }{Given the information of the node: \textcolor{blue}{\textbf{$\langle$raw\_text$\rangle$}}, \textcolor{teal}{\textbf{$\langle$summary$\rangle$}}.
Here I give you the content of the node itself and the summary information of its 1-hop neighbors. The relation between the node and its neighbors is `citation'. Question: Based on this information, Which of the following sub-categories of AI does this paper belong to? Here are the 7 categories: \textcolor{purple}{\textbf{$\langle$labels$\rangle$}}.
 Reply only one category that you think this paper might belong to. Only reply with the category name without any other words. } %\vspace*{6pt}

\end{tcolorbox}

\clearpage
\newpage

