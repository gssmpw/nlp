\section{Supplementary Materials for Computational Cost Analysis}\label{sec:detail_cost}

\begin{table}[!h]
    \centering
    \caption{\textbf{Total training times of different methods in semi-supervised settings.} All recorded experiment times are based on a single NVIDIA H100-80G GPU.}
    \resizebox{\linewidth}{!}{

      \begin{tabular}{cc|ccccccccc}
       \toprule
       \rowcolor{COLOR_MEAN}  \textbf{Type} & \textbf{Method} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{WikiCS} & \textbf{Instagram} & \textbf{Reddit} & \textbf{Books} & \textbf{Photo} & \textbf{Computer} \\ \midrule
       \multicolumn{2}{c|}{\# Training Samples} & 140 & 120 & 60 & 580 & 1,160 & 3,344 & 4,155 & 4,836 & 8,722 \\ \midrule
       \multirow{5}{*}{\textbf{Classic}} & GCN$_{\text{ShallowEmb}}$ & 2.8s & 2.7s & 2.7s & 3.2s & 1.8s & 7.7s & 12.7s & 13.8s & 33.5s \\ 
       & {GAT$_{\text{ShallowEmb}}$} & 1.9s & 2.4s & 3.8s & 3.3s & 2.0s & 6.0s & 10.5s & 12.3s & 38.0s \\ 
       & {SAGE$_{\text{ShallowEmb}}$} & 1.9s & 3.9s & 5.0s & 2.9s & 1.8s & 6.4s & 16.9s & 21.1s & 33.3s \\ 
       & {SenBERT-66M} & 8.5s & 7.9s & 5.9s & 27.9s & 14.7s & 1.2m & 1.5m & 1.8m & 3.3m \\ 
       & {RoBERTa-355M} & 21.2s & 18.9s & 12.7s & 1.2m & 2.3m & 6.5m & 8.1m & 3.8m & 6.9m \\ \midrule 

      \multirow{2}{*}{\textbf{Encoder}} & GCN$_{\text{LLMEmb}}$ & 1.2m & 1.4m & 13.4m & 7.4m & 4.5m & 16.0m & 23.5m & 26.8m & 44.7m \\ 
      & ENGINE  & 2.2m & 2.4m & 16.1m & 15.2m & 9.3m & 22.9m & 31.1m & 38.8m & 1.1h \\ \midrule

      \textbf{Reasoner} & TAPE & 25.5m & 27.8m & 5.6h & 2.7h & 2.0h & 8.0h & 9.9h & 11.7h & 14.5h \\ \midrule

      \multirow{3}{*}{\textbf{Predictor}} & {LLM$_{\text{IT}}$} & 25.6m & 22.0m & 3.9m & 1.1h & 49.1m & 1.1m & 2.0h & 2.4h & 2.7h \\ 
      & GraphGPT & 16.4m & 15.5m & 1.2h & 48.5m & 30.1m & 1.8h & 2.4h & 2.2h & 5.8h \\ 
       & LLaGA & 1.7m & 2.2m & 5.2m & 5.8m & 3.0m & 20.9m & 19.5m & 23.5m & 43.1m  \\    
        \bottomrule
      \end{tabular}
    }
    \label{tab:timecost}
\end{table}


\begin{table}[!h]
    \centering
    \caption{\textbf{Total training times of different methods in supervised settings.} All recorded experiment times are based on a single NVIDIA H100-80G GPU.}
    \resizebox{\linewidth}{!}{

      \begin{tabular}{cc|cccccccccc}
       \toprule
       \rowcolor{COLOR_MEAN}  \textbf{Type} & \textbf{Method} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{WikiCS} & \textbf{arXiv} & \textbf{Instagram} & \textbf{Reddit} & \textbf{Books} & \textbf{Photo} & \textbf{Computer} \\ \midrule
       \multicolumn{2}{c|}{\# Training Samples} & 1,624 & 1,911 & 11,830 & 7,020 &  90,941  & 6,803 & 20,060 & 24,930 & 29,017 & 52,337  \\ \midrule
       \multirow{5}{*}{\textbf{Classic}} & {GCN$_{\text{ShallowEmb}}$} & 1.8s & 1.7s & 5.2s & 5.1s & 51.2s & 19.5s & 8.5s & 14.9s & 19.7s & 25.8s \\ 
       & {GAT$_{\text{ShallowEmb}}$} & 2.1s & 1.9s & 7.9s & 5.7s & 1.5m & 2.7s & 6.9s & 16.6s & 28.0s & 44.6s \\ 
       & {SAGE$_{\text{ShallowEmb}}$} & 1.7s & 3.0s & 7.6s & 4.0s & 1.3m & 2.0s & 7.2s & 19.6s & 20.1s & 43.2s \\ 
       & {SenBERT-66M} &  35s & 41s & 2.6m & 2.5m & 7.4m & 1.2m & 4.4m & 1.8m & 2.2m & 4.1m \\ 
       & {RoBERTa-355M} & 1.3m & 1.6m & 9.2m & 5.5m & 40.8m & 5.3m & 15.9m & 9.7m & 11.9m & 22.4m \\ \midrule 

      \multirow{2}{*}{\textbf{Encoder}} & GCN$_{\text{LLMEmb}}$ & 1.2m & 1.4m & 13.4m & 7.5m & 1.4h & 4.5m & 16.1m & 23.6m & 26.8m & 44.8m  \\ 
      & ENGINE & 2.2m & 2.4m & 16.1m & 19.4m & 2.6h & 8.9m & 24.2m & 35.2m & 44.2m & 1.2h \\ \midrule

      \textbf{Reasoner} & TAPE & 27.4m & 30.3m & 5.9h & 2.8h & 37.4h &  2.1h & 8.3h & 10.0h & 12.0h & 15.0h \\ \midrule

      \multirow{3}{*}{\textbf{Predictor}} & {LLM$_{\text{IT}}$} & 1.0h & 1.3h & 9.9h & 4.2h & 36.3h & 2.7h & 3.4h & 5.7h & 7.4h & 12.4h \\ 
      & GraphGPT & 26.4m & 29.5m & 2.7h & 1.7h & 7.8h & 49.1m & 3.4h & 3.8h & 3.6h & 7.8h \\ 
       & LLaGA &  5.6m & 7.7m & 25.6m & 18.8m & 7.7h & 10.6m & 32.2m & 1.0h & 1.4h & 2.5h \\    
        \bottomrule
      \end{tabular}
    }
    \label{tab:timecost_supervised}
\end{table}


\begin{table}[!h]
    \centering
    \caption{\textbf{Inference times of different methods.} Values in brackets denote the average inference time per case in milliseconds (ms). All recorded experiment times are based on a single NVIDIA H100-80G GPU.}
    \resizebox{0.8\linewidth}{!}{
     \begin{tabular}{cc|ccccc}
     \toprule
        \rowcolor{COLOR_MEAN} \multicolumn{2}{c|}{\textbf{Method}} & \textbf{Cora} & \textbf{arXiv}  & \textbf{Instagram} & \textbf{Photo} & \textbf{WikiCS} \\ \midrule
        \multicolumn{2}{c|}{\# Test Samples} &  542 & 48,603 & 5,847 & 2,268 & 9,673 \\ \midrule
        \textbf{Classic} & GCN & 0.9ms & 21.8ms & 2.0ms & 7.5ms & 4.4ms \\ \midrule 
        \textbf{Encoder} & GCN$_{\text{LLMEmb}}$ & 14.0s (26ms) & 23.8m (29ms) & 53.6s (24ms) & 5.3m (33ms) & 3.7m (38ms) \\  \midrule
        \textbf{Reasoner} & TAPE & 5.0m (551ms) & 10.4h (767ms) & 23.7m (627ms) & 2.3h (863ms) & 1.3h (813ms) \\ \midrule
        \multirow{3}{*}{\textbf{Predictor}} & LLM$_{\text{IT}}$ & 1.2m (129ms) & 3.3h (243ms) & 2.7m (71ms) & 24.1m (149ms) & 5.8m (60ms) \\ 
        & GraphGPT & 1m (104ms) & 1.2h (87ms) & 2.0m (52ms) & 10.4m (64ms) & 11.0m (112ms) \\ 
        & LLaGA & 11.2s (21ms) & 57.1m (70ms) & 1.3m (35ms) & 4.4m (27ms) & 2.4m (25ms) \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:inference_cost}
\end{table}


