\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/LibFramework.pdf}
    \vspace*{-10pt}
    \caption{\textbf{Overview of LLMNodeBed.}}
    \vspace*{-10pt}
    \label{fig:system_implementation}
\end{figure}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.86\linewidth]{figs/LLMRole.pdf}
    \vspace*{-0.2cm}
    \caption{\textbf{Illustrations of LLM-based node classification algorithms under supervised and zero-shot settings.}}
    \vspace*{-0.2cm}
    \label{fig:llm_role}
\end{figure*}


\section{Preliminaries on Node Classification}
To leverage the language abilities of LLMs, we study the node classification task within the context of text-attributed graphs (TAGs) \cite{ma2021deep}. TAGs are represented as $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{S})$, where $\mathcal{V}$ represents the set of nodes, $\mathcal{E}$ the set of edges, and $\mathcal{S}$ the collection of textual descriptions associated with each node $v \in \mathcal{V}$. Some of the nodes are associated with labels, represented as $\mathcal{V}_l \subset \mathcal{V}$. The remaining nodes do not have labels, and are denoted as $\mathcal{V}_u$. The goal of node classification is to train a neural network based on the graph $\mathcal{G}$ and the labels of $\mathcal{V}_l$, which can predict the labels of the unlabeled nodes in $\mathcal{V}_u$.

Traditionally, the textual attributes of nodes can be encoded into shallow embeddings as $\bm{X} = [\bm{x}_1, \ldots, \bm{x}_{|\mathcal{V}|}] \in \mathbb{R}^{|\mathcal{V}| \times d}$ using naive methods like bag-of-words or TF-IDF \cite{Salton1988TermWeightingAI}, where $d$ represents the dimensionality of the embeddings. Such transformation is adopted in most GNN papers. Instead, the input for LLM-based approaches is the raw text and one may expect that the pre-trained knowledge in LLMs can improve performance.







\section{LLMNodeBed: A Testbed for LLM-based Node Classification}
In this section, we present the datasets, baselines, and learning paradigms within LLMNodeBed (Figure \ref{fig:system_implementation}).

\subsection{Datasets}

To provide guidelines for applying algorithms across diverse real-world applications, the selection of datasets in LLMNodeBed considers several key factors: (1) \textbf{Multi-domain Diversity} to reflect different contexts, (2) \textbf{Varying Scales} to examine algorithm scalability and the associated costs of leveraging LLMs, and (3) \textbf{Diverse Levels of Homophily} to understand its impact on performance. Therefore, LLMNodeBed comprises ten datasets spanning the academic, web link, social, and E-Commerce domains. These datasets vary significantly in scale, ranging from thousands of nodes to millions of edges, and exhibit differing levels of homophily. Such diversity in domain, scale, and homophily enables the assessment of algorithms across a wide range of contexts. 

For datasets where raw text has been preprocessed into vector embeddings using bag-of-words or TF-IDF techniques, we utilize collected versions including Cora and Pubmed \cite{he2023TAPE}, Citeseer \cite{chen2024exploring}, and WikiCS \cite{liu2023one}. The remaining datasets already include text attributes in their official releases, including arXiv \cite{hu2020open}, Instagram and Reddit \cite{Huang2024GraphAdapter}, and Books, Computer, and Photo \cite{yan2023comprehensive}. Detailed statistics and information for these datasets are provided in Table \ref{tab:dataset} and Table \ref{tab:dataset_detail} in the Appendix.




\subsection{Baselines} 
The initial release of LLMNodeBed includes eight LLM-based baseline algorithms alongside classic methods. We selected these LLM-based algorithms based on three key criteria: (1) \textbf{Diverse Roles of LLMs} to thoroughly evaluate their effectiveness, (2) \textbf{Straightforward Design} to facilitate clear comparisons by avoiding complex and intertwined architectures, and (3) \textbf{Representativeness} to ensure benchmark relevance by including widely recognized methods. Therefore, the LLM-based baselines include:

\textbf{LLM-as-Encoder: }We include \textbf{ENGINE} \cite{Zhu2024ENGINE} and introduce \textbf{GNN\textsubscript{LLMEmb}}. ENGINE aggregates hidden embeddings from each LLM layer to create comprehensive node representations. In contrast, GNN\textsubscript{LLMEmb} initializes node embeddings using the LLM's last hidden layer before feeding them into GNNs for classification.

\textbf{LLM-as-Reasoner: }We select \textbf{TAPE} \cite{he2023TAPE}, a representative LLM-as-Reasoner method. TAPE prompts the LLM to reason over nodes by generating predictions along with explanations, thereby enriching the node's text attributes and enhancing classification performance.

Both Encoder and Reasoner methods require processing the entire dataset, either by encoding each node's text or by performing reasoning over nodes, which introduces additional processing time before the actual model training begins.

\textbf{LLM-as-Predictor: }We select \textbf{GraphGPT} \cite{tang2023graphgpt}, \textbf{LLaGA} \cite{chen23llaga}, and implement \textbf{LLM Instruction Tuning} (\textbf{LLM\textsubscript{IT}}). GraphGPT employs a multi-stage pre-training and instruction tuning process to classify nodes based on both text and graph context. \textbf{LLaGA} integrates tokenized task instructions and graph context into LLMs to generate predictions.  We implement LLM\textsubscript{IT} to evaluate whether LLMs alone can function as effective predictors. This involves fine-tuning the LLM using task prompts and ground-truth labels formatted as $\langle \texttt{Question}, \texttt{Answer} \rangle$ pairs. 

\textbf{LLM Direct Inference: } This category refers to LLMs generating prediction labels directly from a node's text without additional training or labels. We employ two types of prompt templates: (1) \textbf{Advanced Prompts} that improve LLMs' reasoning abilities such as Chain-of-Thought (CoT) \cite{Wei2022ChainOT} and Tree-of-Thought (ToT) \cite{yao2023tree}, and (2) \textbf{Enriched Prompts} that incorporate neighboring node information to provide structural context.


\textbf{GFMs: } GFMs are foundation models trained on large-scale source graph datasets to acquire general classification knowledge, which can then be seamlessly applied to target graphs. We include \textbf{ZeroG} \cite{li2024zerog} as a representative GFM due to its superior performance in zero-shot settings. Additionally, LLM-as-Predictor methods trained with extensive graph corpora are also considered within this category for zero-shot applications.

Besides LLM-based methods, LLMNodeBed also integrates classic algorithms, including \textbf{MLPs}, \textbf{GNNs}, and \textbf{LMs}. MLPs generate predicted label matrices from node embeddings, while GNNs combine shallow embeddings with graph structures for label prediction. LMs process a node's text through hidden layers and use a classification head to produce label distributions. Further discussions of existing node classification algorithms are provided in Appendix \ref{sec:related_works}.

Prompt templates for LLM-as-Predictor and Direct Inference are listed in Appendix \ref{sec:predictor_prompt} and \ref{sec:zeroshot_prompt}, respectively. Appendix \ref{sec:hyperparam} describes the implementation details, backbone selections, and hyperparameter search spaces for all algorithms. Additionally, we highlight the distinctions of LLMNodeBed in Appendix \ref{sec:distinct_llmnodebed}.


\input{tables/mainexp_supervised}


\subsection{Learning Paradigms}

We evaluate the baselines under three learning configurations: Semi-supervised, Supervised, and Zero-shot. These configurations are defined as follows:

% [leftmargin=*, topsep=2pt]
\begin{itemize}
    \item \textbf{Semi-supervised Learning:} A small subset of nodes $\mathcal{V}_l \subseteq \mathcal{V}$ with known labels $\mathcal{Y}_l$ is provided. This setting assesses the model's ability to effectively utilize limited labeled data, reflecting real-world scenarios where labeling is scarce. For experimental datasets, we adopt the official splits designed for semi-supervised settings to ensure standardized evaluation.
   
    \item \textbf{Supervised Learning:} A larger subset of nodes $\mathcal{V}_l$ with known labels is provided, assessing the model's performance with abundant supervision. Specifically, we use a 60\% training, 20\% validation, and 20\% testing split for most datasets. This consistent split facilitates fair comparisons across baselines. Detailed data splits are provided in Table \ref{tab:dataset_detail} in the Appendix.
    
    \item \textbf{Zero-shot Learning:} No labeled data is provided for training. The model predicts labels solely based on node textual descriptions and the graph structure, assessing its ability to generalize to new, unseen data. For test samples, we follow existing literature \cite{Zhu2024GraphCLIPET} by selecting one smaller dataset from each domain and using 20\% of its nodes as test samples.
\end{itemize}
