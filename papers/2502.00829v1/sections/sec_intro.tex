\section{Introduction}

Node classification is a fundamental task in graph analysis, with a wide range of applications such as item tagging \cite{Mao2020ItemTF}, user profiling \cite{Yan2021RelationawareHG}, and financial fraud detection \cite{Zhang2022eFraudComAE}. Developing effective algorithms for node classification is crucial, as they can significantly impact commercial success. For instance, US banks lost 6 billion USD to fraudsters in 2016. Therefore, even a marginal improvement in fraud detection accuracy could result in substantial financial savings.

Given its practical importance, node classification has been a long-standing research focus in both academia and industry. The earliest attempts to address this task adopted techniques such as Laplacian regularization \cite{belkin2006manifold}, graph embeddings \cite{yang2016revisiting}, and label propagation \cite{zhu2003semi}. Over the past decade, GNN-based methods have been developed and have quickly become prominent due to their superior performance, as demonstrated by works such as \citet{kipf2017GCN}, \citet{velickovic2018GAT}, and \citet{hamilton2017SAGE}. Additionally, the incorporation of encoded textual information has been shown to further complement GNNs' node features, enhancing their effectiveness \cite{jin2023patton, zhao2022GLEM}.

Inspired by the recent success of LLMs, there has been a surge of interest in leveraging LLMs for node classification \cite{li2023survey}. LLMs, pre-trained on extensive text corpora, possess context-aware knowledge and superior semantic comprehension, overcoming the limitations of the non-contextualized shallow embeddings used by traditional GNNs. Typically, supervised methods fall into three categories: Encoder, Reasoner, and Predictor. In the Encoder paradigm, LLMs employ their vast parameters to encode nodes' textual information, producing more expressive features that surpass shallow embeddings \cite{Zhu2024ENGINE}. The Reasoner approach utilizes LLMs' reasoning capabilities to enhance node attributes and the task descriptions with a more detailed text \cite{chen2024exploring, he2023TAPE}. This generated text augments the nodes' original information, thereby enriching their attributes. Lastly, the Predictor role involves LLMs integrating graph context through graph encoders, enabling direct text-based predictions  \cite{chen23llaga,tang2023graphgpt,chai2023graphllm,Huang2024GraphAdapter}. For zero-shot learning with LLMs, methods can be categorized into two types: Direct Inference and Graph Foundation Models (GFMs). Direct Inference involves guiding LLMs to directly perform classification tasks via crafted prompts \cite{Huang2023CanLE}. In contrast, GFMs entail pre-training on extensive graph corpora before applying the model to target graphs, thereby equipping the model with specialized graph intelligence \cite{li2024zerog}. An illustration of these methods is shown in Figure \ref{fig:llm_role}. 

Despite tremendous efforts and promising results, the design principles for LLM-based node classification algorithms remain elusive. Given the significant training and inference costs associated with LLMs, practitioners may opt to deploy these algorithms only when they provide substantial performance enhancements compared to costs. This study, therefore, seeks to identify \textbf{(1) the most suitable settings for each algorithm category, and (2) the scenarios where LLMs surpass traditional LMs such as BERT}. While recent work like GLBench \cite{Li2024GLBench} has evaluated various methods using consistent data splits in semi-supervised and zero-shot settings, differences in backbone architectures and implementation codebases still hinder fair comparisons and rigorous conclusions. To address these limitations, we introduce a new benchmark that further standardizes backbones and codebases. Additionally, we extend GLBench by incorporating three new E-Commerce datasets relevant to practical applications and expanding the evaluation settings. Specifically, we assess the impact of supervision signals (e.g., supervised, semi-supervised), different language model backbones (e.g., RoBERTa, Mistral, LLaMA, GPT-4o), and various prompt types (e.g., CoT, ToT, ReAct). These enhancements enable a more detailed and reliable analysis of LLM-based node classification methods. In summary, our contributions to the field of LLMs for graph analysis are as follows:


% A fair comparison necessitates a benchmark that evaluates all methods using consistent data splitting ratios, learning paradigms, backbone architectures, and implementation codebases. A very recent work, GLBench~\cite{Li2024GLBench}, tested various methods on several datasets in a semi-supervised/zero-shot setting, maintaining the same data splits. However, differences in the underlying backbones and implementation codebases still pose challenges for a fair comparison and drawing rigorous conclusions of the above questions. This paper introduces a benchmark that further standardizes the backbones and implementation codebases. Moreover, we expand upon GLBench by providing additional datasets and evaluation settings. Specifically, we include three new datasets from the E-Commerce sector, which are more relevant for practical commercial applications. We also assess the influence of supervision signals (e.g., supervised or semi-supervised), various language model backbones (e.g., RoBERTa, Mistral, GPT-4o), and prompts (e.g., CoT, ToT, and ReAct). These datasets and settings enable a detailed analysis of the aforementioned questions. 



% However, existing works lack the necessary standardization for such comparisons. An algorithm that performs exceptionally well in its original paper might underperform when used as a baseline in subsequent studies. This discrepancy often arises from variations in data splitting, learning paradigms, backbone architectures, and implementation codebases.  The backbone architecture and implementations are adopted from the original papers, which 

% To address this issue, this paper introduces a testbed for LLM-based node classification algorithms and conducts extensive experiments to derive insights and guidelines. 

\begin{itemize}
    \item \textbf{A Testbed:} We release LLMNodeBed, a PyG-based testbed designed to facilitate reproducible and rigorous research in LLM-based node classification algorithms. The initial release includes ten datasets, eight LLM-based algorithms, and three learning configurations. LLMNodeBed allows for easy addition of new algorithms or datasets, and a single command to run all experiments, and to automatically generate all tables included in this work.
    
    \item \textbf{Comprehensive Experiments:} By training and evaluating over 2,200 models, we analyzed how the learning paradigm, homophily, language model type and size, and prompt design impact the performance of each algorithm category.
    
    \item \textbf{Insights and Tips:} Detailed experiments were conducted to analyze each influencing factor. We identified the settings where each algorithm category performs best and the key components for achieving this performance. Our work provides intuitive explanations, practical tips, and insights about the strengths and limitations of each algorithm category.
\end{itemize}




%It has been a research focus in both academia and industry due to its wide range of applications, including item tagging \cite{Mao2020ItemTF}, user profiling \cite{Yan2021RelationawareHG}, and financial fraud detection \cite{Zhang2022eFraudComAE}. 


%Building effective algorithms for node classification is a long-standing topic as it has a direct impact on commercial success \cite{Lo2022InspectionLSG}.

%Before the popularity of LLMs, node classification is typically tackled by graph neural networks (GNNs) or language models (LMs) such as BERT \cite{Devlin2019BERTPO}. GNNs \cite{kipf2017GCN,velickovic2018GAT,hamilton2017SAGE} enhance node representations by aggregating information from neighboring nodes, thereby capturing the structural context essential for accurate classification. In contrast, LMs \cite{Wang2022e5-large, Liu2019roberta} focus on semantic representations by encoding the textual information associated with each node, transforming the node classification into a text classification task. The encoded textual information can further complement GNNs' node features \cite{jin2023patton, zhao2022GLEM}. Yifei: I think the current intro is too long, to move it to related works

%Over the past decade, we have witnessed great progress in node classification algorithms. The classical ones include Graph Neural Networks (GNNs) \cite{kipf2017GCN,velickovic2018GAT,hamilton2017SAGE} and additional language modeling to enhance the node features \cite{jin2023patton, zhao2022GLEM}. Recently, there has been a surge of interest in applying LLMs for node classification \cite{li2023survey}. In these studies, the roles performed by LLMs can be primarily 


% Despite the importance of this area, the literature of LLM-based node classification is scattered: the algorithms are evaluated under different datasets, learning paradigms, baselines, and implementation codebases. The purpose of this work is to perform rigorous comparisons among algorithms, as well as to open-source our software for anyone to replicate and extend our analysis. This manuscript investigates the question: \emph{How useful are LLMs for node classification under a fair setting?}

% To answer this question, we implement and tune eight LLM-based node classification algorithms, to compare them across ten datasets and three learning paradigms.  There are four major takeaways from our investigations: (1) \textbf{LLM-as-Encoder is effective for low-homophily graphs:} These methods outperform classic LM counterparts on low-homophily graphs, with the advantages being more obvious under limited supervision.
% (2) \textbf{LLM-as-Reasoner is the most effective when LLMs have prior knowledge of the target graph:} These methods achieve superior performance on datasets where the LLMs possess prior knowledge like academic and web link datasets, and benefit from more powerful models like GPT-4o. 
% (3) \textbf{LLM-as-Predictor methods is highly effective when labeled data is abundant}: Predictor methods require extensive supervision for model training, with their performance improving as larger LLMs adhering to scaling laws \cite{Kaplan2020ScalingLF} are utilized. Among different LLMs, Mistral-7B \cite{Jiang2023Mistral7B} consistently serves as a robust backbone. (4) \textbf{Zero-shot methods are most effective when neighbor information is injected:} Although Graph Foundation Models (GFMs) \cite{liu2023one, li2024zerog, Zhu2024GraphCLIPET} outperform open-source LLMs in zero-shot settings, they still lag behind advanced models like GPT-4o. The most effective zero-shot approaches involve injecting neighbor information to guide LLMs for direct inference.

% As a result of this paper, we release LLMNodeBed, a PyTorch-based testbed designed to facilitate reproducible and rigorous research in node classification algorithms. The initial release includes ten datasets, eight algorithms, three learning configurations, and the infrastructure to run all experiments. Our experimental framework can be easily extended to include new methods and datasets. We are committed to updating this repository with new algorithms and datasets and welcome pull requests from fellow researchers to ensure its ongoing development.


%While a myriad of algorithms exists, diverse datasets, architectures, learning configurations, and implementation codebases, rendering fair and realistic comparisons difficult and conclusions inconsistent. Inspired by standardized benchmarks in computer vision like ImageNet, this paper conducts a rigorous comparison of various LLM-based node classification methods to assess the true efficacy of LLMs. This investigation addresses the following research question:

%\textit{Under What Circumstances do LLMs Help Node Classification Task?}

%At a first step, we implement LLMNodeBed, a codebase and testbed for node classification with LLMs. It includes ten multi-domain graph datasets with varying scales and levels of homophily, supports eight representative algorithms that represent diverse LLM roles, and offers three learning configurations: semi-supervised, fully-supervised, and zero-shot. Through extensive experiments, we provide empirical insights into when LLMs contribute to node classification performance: 



% In summary, we make the following contributions: 

% \begin{enumerate}
%     \item \textbf{LLMNodeBed:} We introduce LLMNodeBed, a comprehensive and extensible testbed for evaluating LLM-based node classification algorithms. It comprises ten datasets, eight representative algorithms, and three learning scenarios, and can easily accommodate new datasets, methods, and backbones.
%     \item \textbf{Comprehensive Evaluation:} We conduct extensive empirical analysis across different datasets, algorithms, and learning settings to elucidate the efficacy of different LLM roles in node classification performance. 
%     \item \textbf{Practical Guidelines:} Based on our findings, we provide actionable guidelines for effectively applying LLMs to diverse real-world node classification tasks, enhancing their performance and applicability in various scenarios.
% \end{enumerate}
