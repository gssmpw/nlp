\section{Comparisons among Algorithm Categories}
In this section, we present empirical results among algorithm categories, along with key insights derived from them.

\subsection{Semi-supervised and Supervised Performance}
\textbf{Settings: }To ensure a fair comparison of baseline algorithms, all methods are implemented with consistent components: GCN \cite{kipf2017GCN} for GNNs, RoBERTa-355M \cite{Liu2019roberta} for LMs, and Mistral-7B \cite{Jiang2023Mistral7B} for LLM components where applicable. This uniformity guarantees that performance differences are attributable to model designs rather than underlying architectures. Each experiment was conducted over \textbf{4 runs}. Based on the results of Accuracy (Table \ref{tab:mainexp}) and Macro-F1 (Table \ref{tab:mainexp_f1} in Appendix), we summarize the following takeaways:

\textbf{Takeaway 1: Appropriately incorporating LLMs consistently improves the performance.} According to the table, the best performance is often achieved by LLM-based methods compared to classic methods. It suggests that using LLM to exploit the textual information is useful. 

\textbf{Takeaway 2: LLM-based methods provide greater improvements in semi-supervised settings than in supervised settings.} By comparing the tables, we observe that performance gains are more significant in semi-supervised scenarios. From an information-theoretic perspective, the node classification task with cross-entropy loss aims to maximize the mutual information between the graph and the provided labels, denoted as $I(\mathcal{G}; \mathcal{Y}_{l})$. If we consider graph $\mathcal{G}$ as a joint distribution of node attributes $\bm{X}$ and structure $\mathcal{E}$, we have: 
\begin{equation}
\label{eq:mutual_info}
    \begin{aligned}
    I(\mathcal{G}; \mathcal{Y}_{l}) = I(\bm{X}, \mathcal{E}; \mathcal{Y}_{l}) = I(\mathcal{E}; \mathcal{Y}_{l}) + I(\bm{X}; \mathcal{Y}_{l} | \mathcal{E}).
\end{aligned}
\end{equation}

The first term represents the information encoded in the graph structure, utilized by classic GNNs, while the second term represents information from node features, leveraged by LLMs. In semi-supervised settings, the mutual information between structure and labels is relatively low, allowing LLMs to contribute more significantly to performance. 

%In the semi-supervised setting, the training of GNNs is easier to overfit due to the lack of label information. The abundant pretrained knowledge can help under such circumstance. 

\textbf{Takeaway 3: LLM-as-Reasoner methods are highly effective when labels heavily depend on text.} TAPE achieves top or runner-up performance on academic and web link datasets like Cora and WikiCS, where structural information is less relevant to labels \cite{zhang2021graphless}. However, TAPE struggles with social networks that require deeper structural understanding, such as predicting popular users (high-degree nodes) on Reddit. 


\textbf{Takeaway 4: LLM-as-Encoder methods balance computational cost and accuracy effectively.} LLM-as-Encoder methods perform satisfactorily across all datasets. Further experiments in Section \ref{exp:encoder_comp} reveal that \textbf{LLM-as-Encoder methods are more effective than their LM counterparts when graphs are heterophilic.} Regarding cost-effectiveness, LLM-as-Reasoner should generate long reasoning text, which is far more time-consuming than encoding texts in LLM-as-Encoder (see Appendix \ref{sec:detail_cost}). Therefore, LLM-as-Encoder methods strike a balance between computational efficiency and accuracy.


\textbf{Takeaway 5: LLM-as-Predictor methods are more effective when labeled data is abundant.} In supervised scenarios, LLM-as-Predictor methods enhance performance across most datasets. Especially, the LLaGA method achieves superior results among 5 of 10 datasets. Conversely, in semi-supervised settings, LLM-as-Predictor methods exhibit unstable performance, evidenced by low Macro-F1 scores, and imbalanced output distributions (detailed discussion in Appendix \ref{sec:llm_bias_pred}). These findings indicate that LLM-as-Predictor methods are most effective when ample supervision is available, with LLaGA being an especially strong choice. Furthermore, within the predictor methods, LLM Instruction Tuning typically falls behind the other two methods and incurs substantial time costs (Table \ref{tab:timecost} and Table \ref{tab:timecost_supervised} in the Appendix). This shows that standalone LLMs are weak predictors and incorporating graph context is essential for achieving satisfactory performance.



\subsection{Zero-shot Performance}
\textbf{Settings:} For LLM Direct Inference, we utilize both closed-source and open-source LLMs, including GPT-4o \cite{Achiam2023GPT4TR}, DeepSeek-Chat \cite{Shao2024DeepSeekV2AS}, LLaMA3.1-8B \cite{llama3modelcard}, and Mistral-7B. The prompt templates include \textbf{Direct}, \textbf{CoT}, \textbf{ToT}, and \textbf{ReAct} \cite{Yao2022ReActSR}. Additionally, we incorporate a node's neighboring information into extended prompts, referred to as ``\textbf{w. Neighbor}'', and have the LLMs first reason over neighbors to generate a summary that facilitates the subsequent classification task, referred to as ``\textbf{w. Summary}''. Prompt templates are listed in Appendix \ref{sec:zeroshot_prompt}. Besides, we assess the transferability of GFMs by evaluating \textbf{ZeroG} \cite{li2024zerog}, \textbf{LLM Instruction Tuning}, and \textbf{LLaGA}. GFMs are applied following the intra-domain manner: each model is pre-trained on a larger dataset within the same domain (e.g., arXiv from the academic domain) before being evaluated on the target dataset. Results for Accuracy and Macro-F1 are shown in Table \ref{tab:zeroshot} and Table \ref{tab:zeroshot_supple} in the Appendix, where we have the following takeaway: 

\input{tables/mainexp_zeroshot}



\textbf{Takeaway 6: GFMs can outperform open-source LLMs but still fall short of strong LLMs like GPT-4o.} ZeroG outperforms LLaMA-8B in most cases, achieving up to a 6\% average improvement in accuracy. However, it still falls short of GPT-4o and DeepSeek-Chat. Among GFMs, LLaGA performs poorly because it uses a projector to align the source graph's tokens with LLM input tokens. This projector may be dataset-specific, leading to reduced performance on different datasets, as also observed in \citet{Zhu2024GraphCLIPET}. These findings highlight the need for further research to improve the generalization of GFMs to match the performance of more powerful LLMs.


\textbf{Takeaway 7: LLM direct inference can be improved by appropriately incorporating structural information.} Our results reveal that advanced prompt templates such as CoT, ToT, and ReAct, offer only minor performance improvements. Specifically, models like LLaMA exhibit limited instruction-following abilities, often producing unexpected and over-length outputs when processing complex prompts such as ReAct \cite{wu2025webwalker}. This makes parsing classification results challenging and leads to suboptimal performance. The advanced prompts are generally designed for broad reasoning tasks and lack graph- or classification-specific knowledge, thereby limiting their benefits for the node classification task. In contrast, enriched prompts that incorporate structural information, i.e., ``w. Neighbor'' and ``w. Summary'', demonstrate performance enhancements across LLMs. The performance gains are particularly evident on homophilic datasets such as Cora and Photo (3\%-10\%), where neighboring nodes are likely to share the same labels as the central node. High homophily means that information from neighboring nodes provides crucial clues about a central node's label, thereby improving classification performance. Among these enriched prompts, ``\textbf{w. Summary}'' is especially effective as it not only provides structural context but also leverages the self-reflection abilities of LLMs to further utilize structural information. 

%This finding aligns with previous studies \cite{Huang2023CanLE, Hu2023BeyondTA}.

\input{tables/encoder_comp}

\subsection{Computational Cost Analysis}
 
We evaluate the training and inference times of various methods in both semi-supervised and supervised settings. Detailed training times are provided in Tables \ref{tab:timecost} and \ref{tab:timecost_supervised} in the Appendix, while inference times are presented in Table \ref{tab:inference_cost}. All measurements were conducted on a single NVIDIA H100-80G GPU to ensure consistency.

Based on the results, we can conclude that classic methods are highly efficient, with GNNs typically converging within seconds (e.g., 5.2 seconds for GCN\textsubscript{ShallowEmb} on Pubmed) and LMs fine-tuning completed within minutes. In contrast, LLM-as-Reasoner approaches are the most time-consuming (e.g., 5.9 hours for TAPE on Pubmed) because they require generating reasoning text for each node and subsequently processing this augmented text through both an LM and a GNN. This three-stage computational process significantly extends the overall computation time. LLM-as-Encoder methods are the most efficient among LLM-based approaches (e.g., 13.4 minutes for GCN$_{\text{LLMEmb}}$ on Pubmed), utilizing LLMs solely for feature encoding, which allows GNN training to remain efficient and complete within minutes. Although LLM-as-Predictor methods are more efficient than Reasoner approaches, they still require hours for effective model training. Among predictor methods, LLaGA is the most efficient (e.g., 25.6 minutes on Pubmed) as it encodes both the node's textual and structural information into embeddings instead of processing raw text.

During inference, a significant efficiency gap remains between LLM-based and classic methods. Classic methods can complete the entire inference process for thousands of cases within milliseconds, making them suitable for industrial deployments that demand real-time responses. In contrast, LLM-based methods are limited to processing one case within the same timeframe, highlighting the urgent need to improve their efficiency.