\begin{table*}[!t]
    \centering
    \caption{\textbf{Accuracy ($\%$) of LLaGA to different LLM backbones under supervised settings}.\\ \small{The best LLM backbone within \colorbox{red!10}{\textbf{each series}} and \colorbox{yellow!20}{\textbf{at similar scales}} is highlighted. Semi-supervised performance is shown in Table \ref{tab:llaga_llm}.}}
    \vspace*{-8pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccccccccccc}
      \toprule
     \rowcolor{COLOR_MEAN}  & \textbf{LLM} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{arXiv} & \textbf{WikiCS} & \textbf{Instagram} & \textbf{Reddit} & \textbf{Books} & \textbf{Photo} & \textbf{Computer} & \textbf{Avg.} \\ \midrule
      \multirow{4}{*}{\rotatebox[origin=c]{90}{\small \begin{tabular}{c}
           \textbf{Same} \\ \textbf{series}
      \end{tabular}}}  & Qwen-3B & 84.91$_{\pm \text{2.19}}$ & 74.83$_{\pm \text{2.46}}$ & 88.61$_{\pm \text{1.24}}$ & 71.82$_{\pm \text{1.37}}$ & 82.23$_{\pm \text{3.14}}$ & 62.49$_{\pm \text{0.98}}$ & 67.96$_{\pm \text{0.90}}$ & 83.56$_{\pm \text{1.86}}$ & \cellcolor{red!10} \textbf{85.20$_{\pm \text{1.63}}$} & \cellcolor{red!10} \textbf{89.37$_{\pm \text{0.29}}$} & 79.10 \\ 
     & Qwen-7B & 85.33$_{\pm \text{1.50}}$ & 70.75$_{\pm \text{5.18}}$ & \cellcolor{red!10} \textbf{90.53$_{\pm \text{0.49}}$} & 71.60$_{\pm \text{1.59}}$ & 82.57$_{\pm \text{1.67}}$ & 63.86$_{\pm \text{2.76}}$ & \cellcolor{red!10} \textbf{68.62$_{\pm \text{0.53}}$} & \cellcolor{red!10} \textbf{84.23$_{\pm \text{0.51}}$} & 83.55$_{\pm \text{1.35}}$ & 87.21$_{\pm \text{1.88}}$  & 78.82 \\ 
    & Qwen-14B & \cellcolor{red!10} \textbf{87.25$_{\pm \text{1.63}}$} & \cellcolor{red!10} \textbf{75.49$_{\pm \text{2.03}}$} & 89.93$_{\pm \text{0.27}}$ & \cellcolor{red!10} \textbf{73.15$_{\pm \text{0.74}}$} & 82.26$_{\pm \text{1.51}}$ & 63.88$_{\pm \text{2.49}}$ & 67.60$_{\pm \text{1.77}}$ & 83.94$_{\pm \text{0.41}}$ & 84.83$_{\pm \text{0.77}}$ & 87.06$_{\pm \text{0.80}}$ & 79.54 \\  
    & Qwen-32B & 85.93$_{\pm \text{0.99}}$ & 75.39$_{\pm \text{1.90}}$ & 89.97$_{\pm \text{0.26}}$ & 72.84$_{\pm \text{0.67}}$ & \cellcolor{red!10} \textbf{83.49$_{\pm \text{0.91}}$} & \cellcolor{red!10} \textbf{64.33$_{\pm \text{1.69}}$} & 68.47$_{\pm \text{0.09}}$ & 84.18$_{\pm \text{0.29}}$ & 84.77$_{\pm \text{0.23}}$ & 88.49$_{\pm \text{0.49}}$ & \cellcolor{red!10}\textbf{79.79} \\ 
    \midrule
    
    \multirow{3}{*}{\rotatebox[origin=c]{90}{\small \begin{tabular}{c}
           \textbf{Similar} \\ \textbf{scales}
      \end{tabular}}}  & Mistral-7B & \cellcolor{yellow!10}\textbf{87.55$_{\pm \text{1.15}}$} &\cellcolor{yellow!10} \textbf{76.73$_{\pm \text{1.70}}$} & 90.28$_{\pm \text{0.91}}$ & \cellcolor{yellow!10}\textbf{74.49$_{\pm \text{0.23}}$} & \cellcolor{yellow!10}\textbf{84.03$_{\pm \text{1.10}}$} & \cellcolor{yellow!10}\textbf{69.16$_{\pm \text{0.72}}$} & \cellcolor{yellow!10}\textbf{71.06$_{\pm \text{0.38}}$} & \cellcolor{yellow!10}\textbf{85.56$_{\pm \text{0.30}}$} &  \cellcolor{yellow!10}\textbf{87.62$_{\pm \text{0.30}}$} &\cellcolor{yellow!10} \textbf{90.41$_{\pm \text{0.12}}$} & \cellcolor{yellow!10} \textbf{81.69} \\ 
     & Qwen-7B & 85.33$_{\pm \text{1.50}}$ & 70.75$_{\pm \text{5.18}}$ & \cellcolor{yellow!10}\textbf{90.53$_{\pm \text{0.49}}$} & 70.47$_{\pm \text{1.12}}$ & 82.57$_{\pm \text{1.67}}$ & 63.86$_{\pm \text{2.76}}$ & 68.62$_{\pm \text{0.53}}$ & 84.23$_{\pm \text{0.51}}$ & 83.55$_{\pm \text{1.35}}$ & 87.21$_{\pm \text{1.88}}$  & 78.71\\ 
    & LLaMA-8B & 85.77$_{\pm \text{1.34}}$ & 74.84$_{\pm \text{1.09}}$ & 89.57$_{\pm \text{0.24}}$ & 72.72$_{\pm \text{0.26}}$  & 82.25$_{\pm \text{1.65}}$ & 61.12$_{\pm \text{0.45}}$ & 67.70$_{\pm \text{0.44}}$ & 84.05$_{\pm \text{0.26}}$ & 85.57$_{\pm \text{0.41}}$ & 89.42$_{\pm \text{0.12}}$ & 79.30 \\ 
      \bottomrule
    \end{tabular}
    }
    \label{tab:llaga_llm_s}
   % \vspace*{-13pt}
\end{table*}
