\section{Related Works}
\label{sec:related_works}
	
	Approximation bounds have been studied from various perspectives, including results on the approximation capacity of neural networks ____ ____ ____ ____ and the topological properties of the realization map ____, particularly focusing on the fact that this realization is Lipschitz continuous with the constant depending on the network architecture ____ ____.
	
	There also have been analytical works directly quantifying the value of \( C \) in Equation \eqref{eq_intro_1}. For example, ____ studied this constant in the context of a particular case where \( \theta' \) is obtained through controlled perturbation (i.e., perturbations that do not significantly modify the norm of the initial weights). They derived, for the \( L^2 \) norm, a constant that depends on the network depth, the norm of the weights, the data and the perturbation. However, their results do not generalize to any \( \theta' \) and  are not applicable to arbitrary quantization. Similarly, ____ expressed the constant in the case of the \( L^\infty \) norm but with uniform parameter bounds and, specifically for \( d_{\text{out}} = 1 \), which cannot be applied to every neural network tasks.
	
	More recently, ____ formalized a framework for approximation bounds of ReLU neural networks, providing a general upper-bound for the constant of a neural network in terms of its architecture, weight norms, and other properties. Specifically, their result applies to neural networks defined over general \(L^p\)-spaces, and under general constraints on the weight parameters. The generalization provided by their upper-bound generalizes prior results, which were often limited to specific cases, such as ____ for spectrally-normalized networks.
	By the same authors, in ____, there is another approach that generalizes the notion of approximation bounds to Directed Acyclic Graphs (DAGs) using \( \ell_1 \)-path norms. This formulation as graphs allows for more flexibility on the network architecture (pooling, skip connection...). This work provides general bounds with the notable feature of being invariant under parameter rescaling. They improve their previous paper results, relaxing some assumptions, notably the condition \( r \geq 1 \), but introducing new conditions such as \( \theta_i \theta'_i \geq 0 \), which is not always satisfied for general distinct parameters $(\theta,\theta')$. We further discuss these bounds  in relation to our work in Section~\ref{sec:prelim}.
	
	%Moreover, it is important to note that the bound is derived using the \( L_1 \)-norm, and as such, it can be directly compared to our bound, which is formulated in the infinity norm. The comparison with the \( L_1 \)-bound leads to the same conclusion as the bound from ____, namely that the bound obtained by our theorem is much tighter.
	
	%Besides such bounds are crucial not only in analyzing the behavior of neural networks but also in applications like post-training quantization, where the weights of a pre-trained model are modified to reduce its computational complexity.
	
	
	A directly  application of approximation bounds is quantization.  Quantization significantly reduces the storage and computational requirements of deep models ____. The impact of quantization on the approximation capabilities of neural networks has been studied in ____ and ____, where the authors provided empirical evidence for maintaining high performance even at low precision. However, formal guarantees are still limited, and existing bounds often assume either uniform quantization or specific activation functions, which limit their applicability. Recent works have introduced strategies for low-bit quantization to retain high predictive accuracy in practical implementations. For instance, ____ explored methods for efficient inference with low-bit quantization, while ____ demonstrated the effectiveness of binary weight quantization during training, opening a way for training and deploying models with significantly reduced memory and computational demands. Another important aspect in controlling the accuracy of quantized networks involves understanding singular values in convolutional layers  ____ which helps inform layer-specific quantization strategies.