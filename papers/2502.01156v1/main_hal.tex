%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}

\usepackage{subcaption} % rajout car subfigure me faisait des erreurs

\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}
\usepackage{witharrows}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
 \usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{On the impact of the parametrization of deep convolutional neural networks on post-training quantization}

\begin{document}

\twocolumn[
\icmltitle{On the impact of the parametrization of deep convolutional neural networks \\
on post-training quantization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Samy Houache}{IMB,Thales}
\icmlauthor{Jean-François Aujol}{IMB}
\icmlauthor{Yann Traonmilin}{IMB}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{IMB}{Univ. Bordeaux, Bordeaux INP, CNRS, IMB, F-33400, Talence, France}
\icmlaffiliation{Thales}{Thales AVS, France}

\icmlcorrespondingauthor{Samy Houache}{samy.houache@u-bordeaux.fr}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

	\begin{abstract}
		This paper introduces novel theoretical approximation bounds for the output of quantized neural networks, with a focus on convolutional neural networks (CNN). By considering layerwise parametrization and focusing on the quantization of weights, we provide bounds that gain several orders of magnitude compared to state-of-the art results on classical deep convolutional neural netorks such as MobileNetV2 or ResNets. These gains are achieved by improving the behaviour of the approximation bounds with respect to the depth parameter, which has the most impact on the approximation error induced by quantization. To complement our theoretical result, we provide a numerical exploration of our bounds on MobileNetV2 and ResNets.
	\end{abstract}

\section{Introduction}

Neural networks have become central to modern machine learning, driving significant advancements across a wide range of applications, including computer vision, natural language processing, and robotics \cite{lecun2015deep} \cite{goodfellow2016deep}. Due to the size of state-of-the-art models, deploying these in resource-constrained environments, such as mobile devices or embedded systems, requires model compression techniques like pruning \cite{han2015deep}, low-rank approximations \cite{denton2014exploiting} or quantization \cite{gholami2022survey}. Post-training quantization, in particular, reduces the bit-width of parameters, enabling faster inference and reduced energy consumption without retraining the model.
	
	Despite the empirical success of quantized models, one concern is the potential performance degradation introduced by  quantization. Establishing theoretical bounds on this degradation is therefore crucial, especially for safety-critical applications where robust guarantees are required \cite{forsberg2020challenges}.
	
	For a neural network  $R_{\theta}$ with parameters $\theta$ (i.e. a function parametrized by $\theta$), given an approximation $\theta'$ of $\theta$, we look for bounds of the form
	\begin{equation}\label{eq_intro_1}
		\sup_{x \in \Omega}\| R_{\theta}(x) - R_{\theta'}(x) \|_{\infty} \leq C \|\theta - \theta'\|_\infty,
	\end{equation}
	where  $\Omega$ is the domain of the considered inputs of the networks  and \( C \) is a constant (that must be explicited)  depending on the network’s architecture. Such bounds quantify the stability of an architecture with respect to parameter perturbations which is essential for understanding the impact of quantization on performance.
	
	%Traditional approaches have focused on input-output Lipschitz constants to measure sensitivity to perturbations in inputs \cite{virmaux2018lipschitz}, but our work diverges by examining how the network's realization responds to parameter quantization.
	
	Recent works, such as \cite{gonon2023approximation}, provide insightful  approximation bounds for quantization. However, these bounds are often pessimistic for practical use and come with strict assumptions. For example, \cite{gonon2023approximation} imposes a condition that the maximum parameter norm $r$ must be larger than $1$, which limits the applicability of their results, particularly when using post-training quantization where the weights are fixed.  In practice, for larger networks, we often regularize weights to prevent overfitting \cite{bejani2021systematic} \cite{santos2022avoiding}. Techniques such as L2 regularization DropConnect \cite{wan2013regularization} and weight decay \cite{krogh1991simple} actively encourage the parameters to remain small, potentially making $r $ smaller than 1. Moreover, when removing the impact of weights and input distribution, the constant $C$ in \eqref{eq_intro_1} exhibits a dependency $O(NL^2)$, where $L$ is the depth of the network and $N$ is the width, making this upper bound of little use for modern deep architectures (which have a large $L$). This opens the following question: is it possible to quantize \emph{deep} neural networks with a practical approximation bound ?
	
	In this work, we provide new theoretical approximation bounds for neural networks, as illustrated in Figure \ref{fig:ICML_bounds_comparison_Resnet18}. This bound improves several dependencies in the approximation constant $C$ of~\eqref{eq_intro_1} giving a better  analysis of the performance of quantized \emph{deep} neural networks in practical cases. We focus on theoretical guarantees for the worst-case quantization error under the \textit{infinity norm} metric, which captures the maximum deviation in network outputs. Our key contributions can be summarized as follows.
	
	\begin{figure}[htpb]
		\centering
		\includegraphics[width=0.95\columnwidth]{img/ICML_bounds_comparison_Resnet18.pdf}
		\caption{Illustration of the improvement, in log scale, over the previous bound \eqref{eq:orig_bound1} on ResNet18 without BatchNorm and without biases, with respect to the number of quantization bits, showing a $10^{8}$ times tighter error estimation.}
		\label{fig:ICML_bounds_comparison_Resnet18}
	\end{figure}
	
	
	
	\textbf{Tighter approximation bounds :} As weights typically require much higher memory usage than biases and many convolutional architectures do not use biases (for the convolutional part),  we provide  approximation bounds for quantization, when only weights are quantified.  In this case, our result enhances the state-of-the-art theorem by \cite{gonon2023approximation}, where we replace their factor of  \( N L^2 \) with the sum \( \sum_{\ell=1}^{L} N_{\ell-1}  \) where $N_\ell$ is the width of the $\ell$-th layer, which simplifies to \( N L \) when the network has a constant width \( N \). We further improve this constant for convolutional networks where only the filter sizes and the number of channels of each layer replaces the width in the approximation bound.
	
	\textbf{Relaxation of norm constraints:} We weaken the assumptions on the operator norm constraints by considering arbitrary positive values of \( r_\ell \), the $l$-th layer operator norm. This generalization makes our result applicable to networks where smaller parameter norms are present due to regularization or sparsity constraints. Also, instead of taking the maximum parameter norm $r$, as in \cite{gonon2023approximation} and [Corollary G.1]\cite{gonon2024path}, our approach replaces it with the less pessimistic expression \( r_{mean} \), which (geometrically) averages the parameter norms across layers.
	
	\textbf{Practical validation:} We validate our theoretical improvements by applying them to classical pretrained CNN models, demonstrating that our approach is orders of magnitude closer to a practical application compared to previous works.

 
	\section{Related Works} \label{sec:related_works}
	
	Approximation bounds have been studied from various perspectives, including results on the approximation capacity of neural networks \cite{devore2021neural} \cite{ding2019universal} \cite{csaji2001approximation} \cite{barron1993universal} and the topological properties of the realization map \cite{petersen2021topological}, particularly focusing on the fact that this realization is Lipschitz continuous with the constant depending on the network architecture \cite{petersen2021topological} \cite{devore2021neural}.
	
	There also have been analytical works directly quantifying the value of \( C \) in Equation \eqref{eq_intro_1}. For example, \cite{neyshabur2017pac} studied this constant in the context of a particular case where \( \theta' \) is obtained through controlled perturbation (i.e., perturbations that do not significantly modify the norm of the initial weights). They derived, for the \( L^2 \) norm, a constant that depends on the network depth, the norm of the weights, the data and the perturbation. However, their results do not generalize to any \( \theta' \) and  are not applicable to arbitrary quantization. Similarly, \cite{berner2020analysis} expressed the constant in the case of the \( L^\infty \) norm but with uniform parameter bounds and, specifically for \( d_{\text{out}} = 1 \), which cannot be applied to every neural network tasks.
	
	More recently, \cite{gonon2023approximation} formalized a framework for approximation bounds of ReLU neural networks, providing a general upper-bound for the constant of a neural network in terms of its architecture, weight norms, and other properties. Specifically, their result applies to neural networks defined over general \(L^p\)-spaces, and under general constraints on the weight parameters. The generalization provided by their upper-bound generalizes prior results, which were often limited to specific cases, such as \cite{neyshabur2017pac} for spectrally-normalized networks.
	By the same authors, in \cite{gonon2024path}, there is another approach that generalizes the notion of approximation bounds to Directed Acyclic Graphs (DAGs) using \( \ell_1 \)-path norms. This formulation as graphs allows for more flexibility on the network architecture (pooling, skip connection...). This work provides general bounds with the notable feature of being invariant under parameter rescaling. They improve their previous paper results, relaxing some assumptions, notably the condition \( r \geq 1 \), but introducing new conditions such as \( \theta_i \theta'_i \geq 0 \), which is not always satisfied for general distinct parameters $(\theta,\theta')$. We further discuss these bounds  in relation to our work in Section~\ref{sec:prelim}.
	
	%Moreover, it is important to note that the bound is derived using the \( L_1 \)-norm, and as such, it can be directly compared to our bound, which is formulated in the infinity norm. The comparison with the \( L_1 \)-bound leads to the same conclusion as the bound from \cite{gonon2023approximation}, namely that the bound obtained by our theorem is much tighter.
	
	%Besides such bounds are crucial not only in analyzing the behavior of neural networks but also in applications like post-training quantization, where the weights of a pre-trained model are modified to reduce its computational complexity.
	
	
	A directly  application of approximation bounds is quantization.  Quantization significantly reduces the storage and computational requirements of deep models \cite{gholami2022survey}. The impact of quantization on the approximation capabilities of neural networks has been studied in \cite{ding2019universal} and \cite{hubara2018quantized}, where the authors provided empirical evidence for maintaining high performance even at low precision. However, formal guarantees are still limited, and existing bounds often assume either uniform quantization or specific activation functions, which limit their applicability. Recent works have introduced strategies for low-bit quantization to retain high predictive accuracy in practical implementations. For instance, \cite{choukroun2019low} explored methods for efficient inference with low-bit quantization, while \cite{courbariaux2015binaryconnect} demonstrated the effectiveness of binary weight quantization during training, opening a way for training and deploying models with significantly reduced memory and computational demands. Another important aspect in controlling the accuracy of quantized networks involves understanding singular values in convolutional layers  \cite{sedghi2018singular} which helps inform layer-specific quantization strategies.

	\section{Preliminaries}\label{sec:prelim}
	
	
	
	In this section, we define the key concepts and notations that will be used throughout the paper and we recall reference theoretical approximation bounds from the literature.
	
	\begin{definition}\textbf{Neural network architecture.}  The architecture of a neural network is defined by the tuple \((L,  \mathbf{N})\), where \( L \in \mathbb{N} \) represents the depth of the network, and \( \mathbf{N} = (N_0, \ldots, N_L) \in \mathbb{N}^{L+1} \) is a sequence specifying the number of neurons in each layer (the width of each). We call $N_\ell$ the width of the \(\ell\)-th layer. The width of the network is defined as \( N := \max_{\ell=0,\ldots,L} N_\ell \).
    \end{definition}
	
	
	\begin{definition}
	\textbf{Parameters associated with an architecture.} Given an architecture \((L, \mathbf{N})\),  parameters associated with this architecture are  \(\theta = (\tilde{W}_1, \ldots, \tilde{W}_L)\), where \( \tilde{W}_\ell \in \mathbb{R}^{N_\ell \times ( N_{\ell-1}+1)} \) is the weight matrix for layer \(\ell=1, \dots,L\) with included bias, i.e.  the concatenation of a base weight matrix $W_\ell\in \mathbb{R}^{N_\ell \times N_{\ell-1}}$ with  associated bias \( b_\ell \in \mathbb{R}^{N_\ell} \), for layer \(\ell\). We have that $ \theta \in
	\Theta_{L,\mathbf{N}} := \mathbb{R}^{d(L,\mathbf{N})}$,
	where the dimension $d(L,\mathbf{N})$ is  defined by $d(L,\mathbf{N}) := \sum_{\ell=1}^L N_\ell(N_{\ell-1} + 1)$.
    \end{definition}
	
	\begin{definition}
	\textbf{ReLU Network.} For any vector $x$, we write $\tilde{x}=\begin{pmatrix} x \\ 1 \end{pmatrix} $. Given an architecture \((L, \mathbf{N})\) and parameter vector \(\theta = (\tilde{W}_1, \ldots, \tilde{W}_L)\), we associate the function \( R_\theta : \mathbb{R}^{N_0 + 1} \rightarrow \mathbb{R}^{N_L} \), which is recursively defined for $\ell = 0,\ldots,L$ as follows:
	
	\begin{equation}
		y_0 = x, \; y_\ell =  \sigma \left(\tilde{W}_\ell  \tilde{y}_{l-1}\right) \; \text{and} \; R_\theta(\tilde{x}) = y_L 
	\end{equation}
	
	where \(\sigma(x) = \mathrm{max}(0, x)\) is the ReLU activation function.
    \end{definition}
	
	\begin{definition}
	\textbf{Domains for parameters and input vectors.} Given an architecture \((L, \mathbf{N})\) and a parameter space \(\Theta_{L,\mathbf{N}}\), for any \(r \geq 0\), we define the set of admissible parameters:
	\[
	\Theta_{L,\mathbf{N}}(r) := \left\{(\tilde{W}_1, \ldots, \tilde{W}_L) \in \Theta_{L,\mathbf{N}} :  \right.
	\]
	\[
	\left.  \| \tilde{W}_\ell \|_{\mathrm{op},\infty} \leq r, \; \ell = 1,\ldots,L\right\}.
	\]
	where \(\|\cdot\|_{\mathrm{op},\infty}\) denotes the infinity operator norm, defined as follows, for every matrix $W$ in $\mathbb{R}^{m\times n}$:
	\begin{equation}
		\| W \|_{\mathrm{op},\infty} := \sup_{x \in \mathbb{R}^{n}, \; \|x\|_\infty = 1} \| Wx \|_\infty.
	\end{equation}
    \end{definition}
	
	In this work, we suppose that the input $x$ belongs to the domain $  \Omega = [-D,D]^{N_0} $.
	
	We can now give previous approximation bounds in the $\ell^\infty$-norm setting.
	\begin{theorem}[Previous bound from \cite{gonon2023approximation}]\label{Th_bound_article}
		For any architecture \( (L,\mathbf{N}) \), and any \( r \geq 1 \), denoting  \( N := \max_{l=0,\ldots,L} N_l \), for any $\theta,\theta' \in \Theta_{L,\mathbf{N}}(r) $, we have :
		\begin{equation}
			\label{eq:orig_bound1}
			\sup_{x \in \Omega}\| R_{\theta}(x) - R_{\theta'}(x) \|_{\infty} \leq (D+1) N L^2 r^{L-1} \| \theta - \theta' \|_\infty.
		\end{equation}
		
	\end{theorem}
	
	This shows that the quantization error depends on the architecture’s depth  $L$, width  $N$, and the maximum  norm $r$ of weight matrices. As the network depth $L$ increases, the potential error grows exponentially, increasing the sensitivity of deeper networks to small parameter changes.
	
	
	In \cite{gonon2024path}[Corollary G.1], a new bound is given as a consequence of a general approach using a path-norm metric (see Section \ref{sec:related_works}):
	\begin{equation}\label{eq:orig_bound2}
		\sup_{x \in \Omega}\| R_{\theta}(x) - R_{\theta'}(x) \|_{1}  \leq 2 \max(D,1) LN^2 r^{L-1} \| \theta - \theta' \|_\infty
	\end{equation}
	The dependency in $L^2$  of bound~\eqref{eq:orig_bound1} is reduced to a dependency in $L$ at the cost of a depency in $N^2$ for the $\ell^1$ operator norm. For the sake of clarity in our representations, we have chosen to include only the bound ~\eqref{eq:orig_bound1} in our comparisons. This choice is motivated by the fact that it is generally tighter, particularly because most networks of interest tend to have widths significantly larger than their depths (see Table \ref{tab:model_bound_comparison} for an illustration). That is, the condition \( 2LN^2 \gg NL^2 \) often holds, making the \cite{gonon2023approximation} bound  more appropriate for practical comparisons in our $\ell^\infty$ norm setting.
	
	
	\textbf{The convolutional case.}  In convolutional networks for image processing, a convolutional layer can be represented as a matrix multiplication. Consider an input image \( x \) of dimensions \( n \times m \) and a convolutional filter \( h \) of dimensions \( p \times p \). The input image \( x \) can be flattened into a column vector \( \mathbf{x} \) of size \( nm \times 1 \): $\mathbf{x} = \text{vec}(X)$ where \(\text{vec}(\cdot)\) denotes the vectorization operation. The convolution of the input image \( x \) by the filter \( h \) of size $p \times p$  followed by a subsampling/upsampling can be expressed as a matrix product $ \mathbf{y} = \mathcal{H} \mathbf{x}$
	where \( \mathbf{y} \)  of dimensions \( n_\ell \times m_\ell \) is the vectorization of the output image and $\mathcal{H}$ is a matrix  representing the 2d convolution by $h$ (hence each row contains at most $p^2$ coefficients). The weight matrix associated with  a given convolutional layer is a collection of $c_\ell$ convolutions by several filters $h_\ell$, followed by a subsampling or an upsampling. Hence the width of the $\ell$-th layer is $c_{\ell-1}\times n_{\ell-1} \times m_{\ell-1}$.
	
	
	
	%
	% \textbf{Setting for Our Analysis.} In this paper, we work with the Lebesgue measure on \(\Omega = [-D, D]^d\) and consider \(q = p = \infty\). Under this setting, the constant becomes \(c := D + 1\). We always consider $\tilde{x} = \begin{pmatrix}
		%     x \\ 1 \end{pmatrix}$ as the input of the network, and therefore for us, all the weight matrices are the concatenation of $W$ the original weight, and $b$ the associated bias %
        
\section{Theoretical results}
	
	In this section, we present our main theoretical results, which extend and improve upon the existing bounds for quantized ReLU networks by relaxing the constraints on the network parameters and considering a quantization of the weight matrices only (and not biases). Indeed, in practice, convolutional neural networks such as MobileNetV2 or Resnets without biases are used successfully for vision tasks. If we consider more general networks with biases (and a constant width for the sake of discussion), the size of the bias vector is $NL$ compared to the size of weight matrice $N^2L$. If the width $N$ of the network is large compared to the objective in terms of memory requirement, e.g. $N >>8$  for a $8\times$ memory reduction from $64$ bits to $8$ bits, then quantifying biases will add little gain in memory compared to the gain resulting from the quantization of weights (this is seen in practice when the biases often remain in full precision or are only lightly quantized \cite{finkelstein2019fighting}).
	
	We first give a general approximation bound and then specify to the convolutional case.
\begin{theorem}[General approximation bound]\label{Th:my_bound_extend_new}
For any architecture \((L,\mathbf{N})\),  define the parameters \(\theta = (\tilde{W}_1, \ldots, \tilde{W}_L)\) and \(\theta' = (\tilde{W}'_1, \ldots, \tilde{W}'_L)\), where $\tilde{W}_\ell$ and $\tilde{W}'_\ell$ are weight matrices with included bias. Assume that the two networks have same biases. Assume besides that $ \forall \ell = 1, \ldots, L$:
\begin{equation}
\quad \| \tilde{W}_\ell \|_{\mathrm{op}, \infty} \leq r_\ell \quad \text{and} \quad \| \tilde{W}'_\ell \|_{\mathrm{op}, \infty} \leq r_\ell.
\end{equation}
		Then:
\begin{equation}\begin{split}\label{eq1_main_th} &\sup_{x \in \Omega}\| R_{\theta}(\tilde{x}) - R_{\theta'}(\tilde{x}) \|_{\infty}  \\
&\leq \max(D,1) \sum_{\ell=1}^{L} N_{\ell-1} \times r_{mean}^{L-1} \|\theta - \theta'\|_\infty,\\
\end{split}\end{equation}
where  we define the mean norm parameter
\begin{equation}
r_{mean} := \sqrt[L-1]{\max_{l=1, \dots, L} \max_{i=1, \dots, l-1} \prod_{\substack{j=i \\ j \neq l}}^{L} r_j}.
\end{equation}
\end{theorem}
	
	
	Note that the maximum norm parameter  $ r $ in \eqref{eq:orig_bound1}, from \cite{gonon2023approximation}, is replaced by the term $r_{mean}$.
	This geometric mean-like term, which considers the largest of  partial products of layer-wise norm parameters, allows for a better adjustment of the bound to the variability of the norms. This variability can be significant, particularly between $ r_{\max} $ and the other \( r_l \) values. Specifically, in the least favorable case where \( r_1 = r_2 = \dots = r_L \), the largest product simplifies to \( r^{L-1} \). Conversely, the most favorable scenario would involve a distribution of \( r_l \) with high variance, particularly where \( \max(r_l) \gg 1 \) and all other \( r_l \leq 1 \). In such cases, $r_{mean}^{L-1}$ would be much smaller than \( r^{L-1} \).
	
	In Figure \ref{fig:ICML_comparison_product_all} we simulate several values of $r_\ell$ with different distributions. For the exponential distribution, there is variability across layers, most of $r_\ell$ values are small (less than 5) but the max is around 18. For this case we have the value of $r_{mean}$ around 3. The opposite case is the second histogram where most of $r_j$ values are large (more than 15), but even in this case, $r_{mean}$ allows to  better fit to the distribution, taking account of the few small values of $r_j$. Finally, the last scenario, is the best for our bound, because for all layers except the last one, $r_j \in [0,1]$ and the last one is 10. With this distribution we have that, $r_{mean}$ is equal to $1.02$.
	We will see in Section \ref{sec:experiments} practical examples, and especially MobileNetV2, that is close to the exponential distribution, making us gain  orders of magnitude as the approximation constant grows exponentially  with respect to the depth (with parameter $r_{mean}$).
	\begin{figure}
		\centering
		\includegraphics[width=0.99\columnwidth]{img/ICML_comparison_product_all.pdf}
		\caption{ Comparison between the $r_{mean}$ (green) used in Theorem \ref{Th:my_bound_extend_new} and $r$ (red) used in Theorem \ref{Th_bound_article}, for three different simulated distributions, showing a smaller value compared to $r$ for each distribution.}
		\label{fig:ICML_comparison_product_all}
	\end{figure}
	
	
	\begin{remark}
		[Improved factor \( \sum_{l=1}^L N_{l-1} \)]
		Our result introduces an improved factor, which takes into account the sum of the layer widths. If the architecture has uniform width across all layers, i.e., all layers have width \( N \), this factor becomes \( N \times L \), which is smaller than  \( N \times L^2 \) of Equation \eqref{eq:orig_bound1} .
	\end{remark}
	
	
	\begin{remark}
		[Weakened condition for the domain of parameters \(r_\ell \)] In this bound, the constants \( r_\ell \) are allowed to be arbitrary positive numbers. This weakens the condition $r \geq 1$ of previous bound, making the result more general and applicable to a wider range of network architectures.
	\end{remark}
	
	
	\begin{theorem}[Approximation bound for  CNN]\label{Th:my_bound_extend_new_conv}
		With the same settings as in Theorem \ref{Th:my_bound_extend_new}
		and for a purely convolutional network without biases, where each layer applies \( c_l \) filters of size \( p_l \times p_l \), we have:
		
		\begin{equation} \label{eq:th_conv}
			\begin{split}
				&\sup_{x \in \Omega}\| R_{\theta}(x) - R_{\theta'}(x) \|_{\infty} \\
				&\leq D \times \sum_{l=1}^{L} p_{l}^2 c_{l-1} \times r_{conv}^{L-1} \|\theta - \theta'\|_\infty
			\end{split}
		\end{equation}
		where we define
		\begin{equation}
			r_{conv} := \sqrt[L-1]{\max_{l=1, \dots, L}\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r^{\text{conv}}_k}
		\end{equation}
		
		with $r^{conv}_k$  a bound on the norm of the convolutional matrix of layer $k$ without bias (i.e. $r^{conv}_k \geq  \|\mathcal{H}_k\|_{op,\infty}$).
		
	\end{theorem}
	
	With this formulation,  the term \( \sum_{\ell=1}^{L} p_\ell^2 c_{\ell-1} \) accounts for the sparse structure of the convolution matrices \( \mathcal{H}_\ell \) rather than simply using the number of row elements  $N_{l-1} = m_{l-1}n_{l-1}c_{l-1}$ ($m_{l-1}n_{l-1}$ being the size of each channel at each layer, which can be much larger, see Table \ref{tab:model_bound_comparison}). Then notice that we use $r_{conv}$ in this bound, which corresponds to the larger product omitting one term and starting from the first layer, so $r_{conv}$ is smaller than $r_{mean}$ by construction. Finally, we also have an intermediate result between  Theorem \ref{Th:my_bound_extend_new} and Theorem \ref{Th:my_bound_extend_new_conv}, which corresponds to the case of MLP without bias, given in appendix (Theorem \ref{th:mlp_noBias}). 
	
	
	% We  notice that it could be easily  manage more accurately the term $r_{mean}$ in our Theorems by considering separate bounds on $r_\ell \leq \| \tilde{W}_\ell \|_{\mathrm{op}}$ and  $ \| \tilde{W}'_\ell \|_{\mathrm{op}}\leq r'_\ell$.  $r_l$ values and  replacing it by $ \max (r_\ell,r'_\ell)$, where $r'_\ell$ is defined as $r'_\ell = \| \tilde{W}'_\ell \|_{\mathrm{op}}$.

\section{Experiments} \label{sec:experiments}
	
	In this section, we describe the methodology and setup used to validate our theoretical findings, followed by the results of our experiments, that involve \emph{pretrained} architectures. We evaluate the performance of larger models such as \textbf{ResNet18}, \textbf{ResNet50}, and \textbf{MobileNetV2}, all pretrained on the ImageNet dataset \cite{deng2009imagenet}. These models use deep convolutional architectures without biases (except the last fully connected layer) that are frequently used in real-world applications. Note that a particular care must be taken to manage skip connections (see Lemma \ref{lem:resnet_block_representation} and Lemma \ref{lem:resnet50_block_representation} in the Annex) in the calculations. For experiments with these networks, we  removed the BatchNormalization of the networks to match the conditions of Theorem \ref{Th:my_bound_extend_new_conv}. By doing this, we still obtained similar accuracies than models with BatchNorm.
	
	
	The goal of these experiments is to empirically compare the bounds given by our new theoretical results against those derived from the work of \cite{gonon2023approximation}. The pretrained models are also used to evaluate the effects of post-training quantization on model performance. We use two widely-used image classification datasets. The MNIST dataset: 28x28 grayscale images of handwritten digits, with 60,000 training samples and 10,000 test samples across 10 classes. The CIFAR-10 dataset:  32x32 color images in 10 different classes, with 50,000 images for training and 10,000 for testing.
	
	
	\subsection{Analysis of weight distribution across layers}
	
	The results of Figure \ref{fig:rk_pretrained_comparison} highlight the key advantage of our theoretical bound \eqref{eq:th_conv} compared to prior works.  In \cite{gonon2023approximation}, the bound depends on the maximum operator norm $r$, which in these examples, is significantly larger than the "geometric mean" layer-wise term $r_{conv}$. For example, in  ResNet50, the maximum  $r$ is approximately 3 times larger than $r_{conv}$, and in MobileNetV2, it is more than 11 times larger. Specifically for MobileNetV2 we can see that the weight norm distribution looks like the exponential distribution of  Figure \ref{fig:ICML_comparison_product_all}, that is a favorable case to have a tighter bound. Lots of values are small and the maximum value $r$ is more than 100, while $r_{conv}$ is 9. This shows better consideration of network weight norms distribution. 
	
	\begin{figure*}
		\centering
		\includegraphics[width=0.95\textwidth]{img/ICML_comparison_norms_models2.pdf}
		\caption{Comparison between the maximum geometric mean term $r_{\text{conv}}$ (green) used in \eqref{eq:th_conv} and the maximum weight norm $r$ (red) used in \eqref{eq:orig_bound1}, for ResNet18, ResNet50 and MobileNetV2, without BatchNorm, showing a smaller value of $r_{\text{conv}}$ for all models.}
		\label{fig:rk_pretrained_comparison}
	\end{figure*}
	
	\subsection{Quantization}
	
	In our experiments, we consider two types of \textbf{post-training quantization}.
	We define \emph{uniform} quantization as a function \( Q^{unif} : \Theta_{L,\mathbf{N}} \rightarrow \Theta_{L,\mathbf{N}} \), where \( Q(\theta) \) represents the quantized version of the parameter \( \theta \). Specifically, we use the following definition of uniform quantization in our experiments:
	\begin{equation}
		Q^{unif}(\theta) = Q_\eta^{unif}(\theta) = \left\lfloor \frac{\theta}{\eta} \right\rfloor \eta,
	\end{equation}
	where \( \eta > 0 \) is the quantization step size. For each layer, the parameter \( \eta \) is determined by the formula:
	\[
	\eta = \frac{W_{\max}}{2^n - 1},
	\]
	where \( W_{\max} \) is the maximum absolute value of the weight values, and \( n \) is the bit width.
	
	To highlight the role of the term $\|\theta -Q(\theta)\|_\infty$ in the quantization error,  as an alternative approach to uniform quantization, we  define \( Q^{round} \) using a rounding function instead of floor truncation. This form of quantization is defined as:
	\begin{equation}
		Q^{round}(\theta) = Q_\eta^{round}(\theta) = \text{round}\left( \frac{\theta}{\eta} \right) \eta,
	\end{equation}
	where the function \( \text{round} \) rounds \( \frac{\theta}{\eta} \) to the nearest integer before scaling back by \( \eta \). This approach may reduce quantization error in cases where uniform quantization leads to excessive loss of information.
	
	
	
	\subsection{Experimental results on MobileNetV2 and Resnets}
	
	We analyze the key parameters involved in our bounds and those from \cite{gonon2023approximation} in Table \ref{tab:model_bound_comparison}.  The depth $ L$ of the considered architectures varies between \( L = 18 \) for ResNet18 and \( L = 53 \) for MobileNetV2. This variation in depth is important, as the approximation bound is exponentially dependent on \( L \). An important difference is observed in the width parameter  $N$, where the values obtained using the formulation of Theorem \ref{Th:my_bound_extend_new_conv} bound are orders of magnitude smaller than those from the previous bound. For instance, for MobileNetV2, the previous bound is calculated with a width of \( 1.2 \times 10^6 \), whereas the new bound reduces this to \(8641 \). Similarly, for ResNet50, the width decreases from \( 8 \times 10^5 \) to \( 4609 \). This significant decrease reflects the tighter characterization provided by the new bound, which avoids overly conservative estimations of \( N \). Another critical parameter is the maximum norm parameter, which is significantly smaller under the new bound, particularly for MobileNetV2 ($ r \approx 101 $) while $r_{mean}$ only equals to $9$. The reduced values of norm parameters reduce the exponential dependency on depth, which is the main pessimistic factor in the bound. These differences between the two bounds is clearly reflected in the ratio value. Even for a relatively shallow network like ResNet18, we observe in Table \ref{tab:model_bound_comparison} that our bound is \(10^{8}\) times tighter and this observation becomes even more relevant for deeper and wider networks such as MobileNetV2, where the ratio reaches \(10^{56}\).
	
	
	\begin{table*}[t]
    \caption{Comparison of parameters between our bound \eqref{eq:th_conv} and the state-of-the-art \cite{gonon2023approximation} bound on pre-trained models. The comparison is also expressed in terms of a ratio of the two bounds, where the values of the bounds in this ratio are computed exclusively for the convolutional part of the network.}
		\vskip 0.1cm
		\begin{center}
		\begin{small}
		\begin{sc}
		\renewcommand{\arraystretch}{1.5}
		\resizebox{\textwidth}{!}{%
		\begin{tabular}{lcccccc}
		\toprule
			 &  & Previous Width & Previous Norm Param & New Width & New Norm Param & Ratio \\
		\midrule
			Model & Depth (\( L \)) & $m_{l-1}n_{l-1}c_{l-1}$ & $ r $  & $p_{l}^2 c_{l-1}$ & $r_{conv}$ & $\dfrac{\text{Previous Bound (2023)}}{\text{New bound}}$ \\
		\midrule
			MobileNetV2 & 53 & $1.2 \times 10^6$ & \( \approx 101\) & 8641 & \( \approx 9 \) & \( \approx 10^{56} \) \\
			ResNet18 & 18 & $8 \times 10^5$ & \( \approx 84 \) & 4609 & \( \approx 44 \) & \( \approx 10^{8} \) \\
			ResNet50 & 50 & $8 \times 10^5$ & \( \approx 108 \) & 4609 & \( \approx 37 \) & \( \approx 10^{27} \) \\
		\bottomrule
		\end{tabular}%
					}
		\end{sc}
		\end{small}
		\end{center}
		\vskip -0.1in
        \label{tab:model_bound_comparison}
	\end{table*}
	
	
	In Figure \ref{fig:ICML_bounds_comparison_Resnet18} (in the introduction), we compare the approximation error bound and the estimated approximation error for the Resnet18 architecture. The results  were obtained by removing the final fully connected layer to keep only the convolutional part of the network. Then we use $r_{conv}$ to calculate the new bound since the network has no biases for the convolutional part. The figure demonstrates an error of approximately $ 10^{40}$, whereas the new bound is only $10^{32}$. This highlights a significant improvement, even for a shallow network (with $ L=18$). In Figure \ref{fig:ICML_bounds_comparison_Mobilnet}, we perform the same experiment for the MobileNetV2 architecture. The effect of weight norm distribution is even more apparent. The previous bound reaches a value of $ 10^{109}$, whereas the new bound is reduced to $ 10^{53}$. For both figures, we can observe that the shape of the bound is accurate as it follows the error approximation trend, up to a constant factor. Nevertheless, the constant is still large even for the new bound, since the output error is at most around $ 10^3$.
	\begin{figure}
		\centering \includegraphics[width=0.9\columnwidth]{img/ICML_bounds_comparison_Mobilnet.pdf}
		\caption{Comparison in log scale, between our bound \eqref{eq:th_conv} and previous bound \eqref{eq:orig_bound1}, on the convolutional part of MobileNetV2 without BatchNorm and without biases, with respect to the number of bits, where $\theta'$ is the quantized set of parameters deduced from $\theta$, showing a $10^{56}$ times tighter error estimation.}
		\label{fig:ICML_bounds_comparison_Mobilnet}
	\end{figure}
	We can make similar observations from Figure \ref{fig:ICML_bounds_comparison_Resnet50} because the previous bound has the same order of magnitude. However, even though MobileNetV2 and ResNet50 have similar depths, our bound has adapted much better to the specific distribution of weight norms in MobileNetV2. The bound value for MobileNetV2 is significantly lower than the ResNet50 one, being around $ 10^{26}$ times smaller. This is mainly due to the favorable distribution of weight norms in MobileNetV2, which favors $ r_{\text{conv}}$ to remain small.
	\begin{figure}
		\centering \includegraphics[width=0.9\columnwidth]{img/ICML_bounds_comparison_Resnet50.pdf}
		\caption{Comparison in log scale, between our bound \eqref{eq:th_conv} and previous bound \eqref{eq:orig_bound1}, on the convolutional part of Resnet50, without BatchNorm and without biases, with respect to the number of bits, where $\theta'$ is the quantized set of parameters deduced from $\theta$, showing a $10^{27}$ times tighter error estimation.}
		\label{fig:ICML_bounds_comparison_Resnet50}
		\vspace*{-4mm}
	\end{figure}
	
	Although our new bound significantly improves the previous state of the art bound (e.g., \cite{gonon2023approximation}, [Corollary G.1]\cite{gonon2024path}), the new bound is still orders of magnitude far from what is observed in real outputs of networks (Figures \ref{fig:ICML_bounds_comparison_Resnet18},\ref{fig:ICML_bounds_comparison_Mobilnet} and \ref{fig:ICML_bounds_comparison_Resnet50}). One reason is that such bounds are derived under theoretical worst-case scenarios, which rarely occur in practice. Another reason is the generality of the theorem itself, which leads to a very conservative estimate of the output error.
	
	
	In Figure \ref{fig:comparison_pretrained_quantized_six}, we analyze the impact of post-training quantization on the precision of various pretrained models, across two datasets: MNIST and CIFAR-10. We  clearly observe the behavior reflected by the form of the bound: depth significantly influences quantization error. For instance, in the case of ResNet50 on MNIST, uniform quantization impacts precision as the model fails to reach the baseline precision even at 24 bits.  Conversely, ResNet18 achieves the baseline precision with just 12 bits on MNIST (using uniform quantization). To a lesser extent, we can also suppose that other parameters play a role. For example, despite having very similar depths, MobileNetV2 and ResNet50 exhibit noticeably different quantized performances. This is probably due to the specific architecture of MobilNetV2 that uses residual bottlenecks and Relu6 activation function ($Relu6(x):= \min(\max(0,x),6)$ which is known to better support quantization \cite{sandler2018mobilenetv2}. Specifically, MobileNetV2 reaches baseline precision with 20-bit quantization on MNIST.
	
	
	Furthermore, the dataset influences precision as well. On CIFAR-10, ResNet50 handles quantization much better, achieving the desired precision with only 18 bits of quantization (14 bits with rounding quantization). Finally, the quantization method itself significantly affects the results: rounding quantization offers an average gain of approximately 4 bits, regardless of the dataset or model, to achieve the desired precision. This behavior is accounted for in the bound through the factor involving $ \| \theta - \theta' \|_{\infty}$.
	
	\begin{figure*}
		\centering
		% Row 1: MNIST
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\columnwidth]{img/ICML_Acc_MNIST_MobileNetV2.pdf}
			\caption{MNIST - MobileNetV2}
			\label{fig:ICML_Acc_MNIST_MobileNetV2}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\columnwidth]{img/ICML_Acc_MNIST_ResNet18.pdf}
			\caption{MNIST - ResNet18}
			\label{fig:ICML_Acc_MNIST_ResNet18}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\columnwidth]{img/ICML_Acc_MNIST_ResNet50.pdf}
			\caption{MNIST - ResNet50}
			\label{fig:ICML_Acc_MNIST_ResNet50}
		\end{subfigure}
		\\
		% Row 2: CIFAR-10
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\columnwidth]{img/ICML_Acc_CIFAR10_MobileNetV2.pdf}
			\caption{CIFAR-10 - MobileNetV2}
			\label{fig:ICML_Acc_CIFAR10_MobileNetV2}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\columnwidth]{img/ICML_Acc_CIFAR10_ResNet18.pdf}
			\caption{CIFAR-10 - ResNet18}
			\label{fig:ICML_Acc_CIFAR10_ResNet18}
		\end{subfigure}
		\begin{subfigure}{0.32\textwidth}
			\centering
			\includegraphics[width=\columnwidth]{img/ICML_Acc_CIFAR10_ResNet50.pdf}
			\caption{CIFAR-10 - ResNet50}
			\label{fig:ICML_Acc_CIFAR10_ResNet50}
		\end{subfigure}
		
		\caption{Graphs illustrating the effect of quantization on performance for two quantization functions (round and uniform). The results highlights how quantization reduces memory requirements while maintaining or approaching the base model's accuracy. The amount of quantization needed to reach the base precision depends on the quantization function used.}
		\label{fig:comparison_pretrained_quantized_six}
	\end{figure*}
	
	\subsection{Effect  of depth in the quantization of MLP without biases}
	
	In Figure \ref{fig:MLP_comparison}, we investigate the improvement in accuracy of our bound in  Theorem \ref{Th:my_bound_extend_new} with respect to depth.  We construct  four MLPs of depths: 5, 7, 9, and 11, trained on the MNIST dataset with architectures provided in the Annex. Independently of the number of bits, the ratio value is  dependent on the depth, starting from  $\approx 10^3$ for depth 5 to $\approx 10^8$ for depth 11. Additionally, it is observed that for extreme quantization (4 bits), there is a notable difference compared to other quantizations (8, 16, and 24 bits). Indeed, for 4 bits, the model accuracy is bad ($\leq 10\%$), impacting the norms of the weight matrices and making $r_{mean}$ closer to $r_{max}$. This results in a slightly lower ratio compared to other quantization levels.
	
	
	\begin{figure}[h]
		\centering \includegraphics[width=0.93\columnwidth]{img/MLP_comparison3.pdf}
		\caption{Ratio in log scale between the previous bound \eqref{eq:orig_bound1}  and our bound \eqref{eq1_main_th} depending on the model depth for different quantization bits. This graph shows that the ratio grows (exponentially) according to the depth.}
		\label{fig:MLP_comparison}
		\vspace*{-5mm}
	\end{figure}
	
	Finally, in Figure \ref{fig:MLP_both_bounds_ratios}, we compare the bounds with respect to the approximation error. First, the slope of the curves does not change. Moreover, the exponential dependence on depth is significantly reduced. Indeed, the slope given by $ r$ for the previous bound is much steeper than the slope given by $ r_{\text{mean}}$ in our bound. Thus, the new bound better adapts to the network's behavior, up to a constant. However, the ideal slope, which would perfectly match the network's behavior, would be a horizontal curve.
	
	\begin{figure}[h]
		\centering \includegraphics[width=0.95\columnwidth]{img/MLP_both_bounds_ratios.pdf}
		\caption{Ratio in log scale between bounds \eqref{eq:orig_bound1}, \eqref{eq1_main_th} and the estimated error approximation, depending on the model depth for different quantization bits. This graph shows that the exponential dependence to the depth is reduced for every quantization bits.}
		\label{fig:MLP_both_bounds_ratios}
		\vspace*{-5mm}
	\end{figure}

\section{Conclusion}
	
	In this work, we introduced a novel theoretical approximation bound for deep (convolutional) neural networks. By generalizing the infinity-norm-based bound and introducing a more flexible approach to operator norm constraints, our  significantly improves existing bounds  for a broad range of network architectures. This was validated on popular architectures such as ResNet18, ResNet50, and MobileNetV2, as well as MLP without bias quantization; showcasing improved accuracy in performance predictions for post-training quantized networks.
	
	For future research, one key direction is to refine this bound further by integrating knowledge about the network’s specific purpose, structure, and optimization path. Notably, $\theta$ and $\theta'$ are not randomly chosen parameters in an abstract space; they are the result of gradient-based optimization designed to minimize a specific loss function. One could intend to  adapt the quantization process to this fact.
	
	Another important promising lead lies in developing a probabilistic approach that reflects the network’s expected behavior under typical operating conditions, such as average or quantile-based performance, providing a complementary view to our deterministic bound. While a deterministic bound is crucial for critical applications, probabilistic insights can enhance our understanding of the network’s behavior in practical, real-world scenarios.
	


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work. In particular, it can help identify ways to reduce energy consumption, while improving the performance, and hence safety, of embedded systems which use quantized neural networks.

% Acknowledgements should only appear in the accepted version.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

		\onecolumn
		\section{Proof of main results}
		For completeness, we begin by recalling  the value of the infinity norm of a matrix.
		\begin{lemma} \label{lem:matrices}
			For all matrices $A \in \mathbb R^{m\times n}$ and all $x \in \mathbb{R}^n$ we have:
			\begin{equation}
				\| A \|_{\mathrm{op},\infty} := \sup_{x \in \mathbb{R}^{n}, \; \|x\|_\infty = 1} \| Wx \|_\infty = \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{i,j}|
			\end{equation}
		\end{lemma}
		
		
		\begin{proof}
			
			Let $A \in \mathbb R^{m\times n}$ and $x \in \mathbb R^{n}$ with $\|x\|_\infty = 1$, then
			
			\begin{align}
				\|Ax\|_\infty &= \max_{1 \leq i \leq m} \left| (Ax)_i \right| = \max_{1 \leq i \leq m} \left| \sum_{j=1}^n a_{i,j} x_j \right|\leq \max_{1 \leq i \leq m}  \sum_{j=1}^n \left| a_{i,j} \right| \|x\|_\infty = \max_{1 \leq i \leq m}  \sum_{j=1}^n \left|a_{i,j} \right|
			\end{align}
			
			
			To show that equality holds, let \( x \) such that \( x_j = \mathrm{sign}(a_{i^\star,j})\) where $i^\star \in  \arg \max_i \sum_{j=1}^n \left|a_{i,j} \right|$.  Then \( \|x\|_\infty = 1 \) and:
			\begin{equation}
				\|Ax\|_\infty =  \sum_{j=1}^n a_{i^\star j}\cdot \mathrm{sign}(a_{i^\star j}) =  \sum_{j=1}^n |a_{i^\star j} | =  \max_i \sum_{j=1}^n \left|a_{i,j} \right|.
			\end{equation}
			
			This shows that:
			\begin{equation}
				\|A\|_{\mathrm{op},\infty}= \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}|.
			\end{equation}
		\end{proof}
		
		The following Lemma bounds the difference of outputs between a network and its quantization. It is adapated from  [Lemma C.1]\cite{gonon2023approximation} to weight matrices with included identical biases.
		
		\begin{lemma}\label{Lemma_article}
			Let $(L, \mathbf{N})$ be an architecture with  $L \geq 1$, denoting
			$\theta = (\tilde{W}_1, \dots, \tilde{W}_L)$,
			$\theta' = (\tilde{W}'_1, \dots, \tilde{W}'_L) \in \Theta_{L, \mathbf{N}}$ as two  sets of parameters associated with this architecture, with each last column of the matrices composed by biases of the corresponding layer. We assume that the two networks have same biases (i.e. we do not quantize the bias) For every $\ell = 1, \dots, L-1$, define $\theta'_\ell$ as the parameter deduced from $\theta'$, associated with the architecture $(\ell, (N_0, \dots, N_\ell))$:
			\[
			\theta'_\ell = (\tilde{W}'_1, \dots, \tilde{W}'_\ell).
			\]
			Then for every $\tilde{x} = \begin{pmatrix}
				x \\ 1 \end{pmatrix} \; \text{with} \; x \in \mathbb{R}^{N_0}$, denoting by $W_k$ and $W'_k$ the  weight matrices without biases (i.e. $\tilde{W}_k$ and $\tilde{W}'_k$ with last column removed), then, for any $1$-Lipschitz activation function $\sigma$ such that $\sigma(0) = 0$, we have:
			
			\begin{equation} \label{inequality_lemmeC}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} \leq
				\sum_{\ell=1}^{L}
				\left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right)
				\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty} \| R_{\theta'_{\ell-1}} (\tilde{x}) \|_\infty,
			\end{equation}
			
			where we set by convention $R_{\theta'_{\ell-1}} (\tilde{x}) = x $ for $\ell = 1$, and $\prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} = 1$ for $\ell = L$.
		\end{lemma}
		
		
		
		\begin{proof}
			The proof of Inequality \eqref{inequality_lemmeC} follows by induction on $L \in \mathbb{N}$. For \( L = 1 \), with the Lipschitz condition and the definition of $\|\cdot\|_{\mathrm{op},\infty}$, using the fact that the last columns of $\tilde{W}_1$ and $\tilde{W}_1'$ are equal, we have $ \| \tilde{W}_1 \tilde{x} - \tilde{W}'_1 \tilde{x} \|_{\infty} = \| W_1 x - W'_1x \|_{\infty}$ and:
			
			\begin{equation}
				\left\|R_{\theta_L} (\tilde{x})  -  R_{\theta_L'} (\tilde{x}) \right\|_{ \infty} = \left\| \sigma(\tilde{W}_1 \tilde{x}) - \sigma(\tilde{W}'_1 \tilde{x}) \right\|_{\infty} \leq \left\| \tilde{W}_1 \tilde{x} - \tilde{W}'_1 \tilde{x} \right\|_{\infty} = \left\| W_1 \tilde{x} - W'_1 x \right\|_{\infty}
				\leq \|W_1 - W'_1\|_{\mathrm{op}, \infty} \left\| x\right\|_\infty.
			\end{equation}
			
			Now assume that  property \eqref{inequality_lemmeC} holds for \( L \geq 1 \). At rank \( L+1 \), using the fact that the activation function \( \sigma \) is 1-Lipschitz and satisfies \( \sigma(0) = 0 \), we have :
			\begin{DispWithArrows*}
				&\left\| R_{\theta_{L+1}} (\tilde{x}) - R_{\theta_{L+1}'} (\tilde{x}) \right\|_{\infty}\\
				&= \left\| \sigma\left( \tilde{W}_{L+1} \begin{pmatrix} R_{\theta_L} (\tilde{x})  \\ 1 \end{pmatrix}\right)
				-  \sigma\left( \tilde{W}'_{L+1} \begin{pmatrix} R_{\theta'_L} (\tilde{x})  \\ 1 \end{pmatrix}\right) \right\|_{ \infty}\Arrow{$\sigma$ is 1-Lipschitz} \\
				&\leq \left\| \tilde{W}_{L+1} \begin{pmatrix} R_{\theta_L} (\tilde{x}) \\ 1 \end{pmatrix}
				-  \tilde{W}'_{L+1} \begin{pmatrix}  R_{\theta'_L} (\tilde{x})  \\ 1 \end{pmatrix} \right\|_{ \infty}\\
				&= \left\| \tilde{W}_{L+1} \left(\begin{pmatrix}  R_{\theta_L} (\tilde{x})\\ 1 \end{pmatrix}
				- \begin{pmatrix}  R_{\theta'_L} (\tilde{x})  \\ 1 \end{pmatrix}   \right)+  \left(\tilde{W}_{L+1} - \tilde{W}'_{L+1} \right)\begin{pmatrix}  R_{\theta'_L} (\tilde{x})  \\ 1 \end{pmatrix} \right\|_{ \infty} \Arrow{triangle inequality} \\
				&\leq \left\| \tilde{W}_{L+1}  \begin{pmatrix}  R_{\theta_L} (\tilde{x})
					-  R_{\theta'_L} (\tilde{x})  \\ 0 \end{pmatrix}  \right\|_{ \infty} + \left\| \left(\tilde{W}_{L+1} - \tilde{W}'_{L+1}\right) \begin{pmatrix}  R_{\theta'_L} (\tilde{x})  \\ 1 \end{pmatrix} \right\|_{ \infty} \Arrow{ same last column \\ for $\tilde{W}_{L+1}$ and $\tilde{W}'_{L+1}$} \\
				&= \left\| W_{L+1} \left( R_{\theta_L} (\tilde{x})
				-  R_{\theta'_L} (\tilde{x}) \right)  \right\|_{ \infty} + \left\| \left(W_{L+1} - W'_{L+1}\right) R_{\theta'_L} (\tilde{x})\right\|_{ \infty}
				\Arrow{Sub-multiplicativity} \\
				&\leq \|W_{L+1}\|_{\mathrm{op}, \infty} \left\|  R_{\theta_L} (\tilde{x})
				-  R_{\theta'_L} (\tilde{x}) \right\|_\infty + \|W_{L+1} - W'_{L+1}\|_{\mathrm{op}, \infty} \left\| R_{\theta'_L} (\tilde{x}) \right\|_ \infty .
			\end{DispWithArrows*}
			
			Applying the induction hypothesis (Inequality~\eqref{inequality_lemmeC}) to the term \( \left\| R_{\theta_L} (\tilde{x}) - R_{\theta'_L} (\tilde{x}) \right\|_\infty \), we have:
			
			\begin{equation}
				\left\| R_{\theta_L} (\tilde{x}) - R_{\theta'_L} (\tilde{x})  \right\|_\infty
				\leq \sum_{\ell=1}^{L} \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right)
				\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty}
				\left\|  R_{\theta'_{\ell-1}} (\tilde{x}) \right\|_\infty.
			\end{equation}
			
			
			
			Substituting this bound back into the previous inequality and using that we  have \( \prod_{k=L+2}^{L=1} \|W_k\|_{\mathrm{op}, \infty} = 1 \) by convention, we get:
			\begin{equation}
				\begin{split}
					\left\| R_{\theta_{L+1}} (\tilde{x}) - R_{\theta_{L+1}'} (\tilde{x}) \right\|_{ \infty}
					&\leq \|W_{L+1}\|_{\mathrm{op}, \infty} \sum_{\ell=1}^{L}
					\left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right)
					\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty}
					\left\|  R_{\theta'_{\ell-1}} (\tilde{x}) \right\|_\infty \\
					&\quad + \|W_{L+1} - W'_{L+1}\|_{\mathrm{op}, \infty}
					\left\| R_{\theta'_L} (\tilde{x}) \right\|_\infty \\
					&= \sum_{\ell=1}^{L}
					\left( \prod_{k=\ell+1}^{L+1} \|W_k\|_{\mathrm{op}, \infty} \right)
					\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty}
					\left\| R_{\theta'_{\ell-1}} (\tilde{x}) \right\|_\infty \\
					&\quad + \left(\prod_{k=L+1+1}^{L+1} \|W_k\|_{\mathrm{op}, \infty}\right)\|W_{L+1} - W'_{L+1}\|_{\mathrm{op}, \infty}
					\left\| R_{\theta'_L} (\tilde{x}) \right\|_\infty.
				\end{split}
			\end{equation}
			
			We deduce
			\begin{equation}
				\begin{split}
					\left\| R_{\theta_{L+1}} (\tilde{x}) - R_{\theta_{L+1}'} (\tilde{x}) \right\|_{\infty}
					&\leq \sum_{\ell=1}^{L+1} \left( \prod_{k=\ell+1}^{L+1} \|W_k\|_{\mathrm{op}, \infty} \right)
					\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty} \left\| R_{\theta'_{\ell-1}} (\tilde{x}) \right\|_\infty.
				\end{split}
			\end{equation}
			This concludes the induction and proves the lemma.
			
		\end{proof}
		
		Note that, if we suppose that our networks have no bias, then, for all layers, we do not have to take account of the last column of weight matrices with included bias. We can do the same proof replacing $\tilde{W}_l$ by $W_l$ and $\tilde{x}$ by $x$.
		
		The result without bias also follows by the same induction, and it comes: 
		
		\begin{equation}\label{inequality_lemmeC_no_bias}
			\left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq \sum_{\ell=1}^{L} \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right)
			\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty} \left\| R_{\theta'_{\ell-1}} (x) \right\|_\infty.
		\end{equation}
		
		The following lemma allows us to upper bound the output of a network based on the operator norms of each layer, without any specific conditions on their values.
		
		\begin{lemma}\label{Lemma_max}
			Let $(L, \mathbf{N})$ be an architecture with  $L \geq 1$, denoting
			$\theta = (\tilde{W}_1, \dots, \tilde{W}_L) \in \Theta_{L, \mathbf{N}}$ a  set of parameters associated with this architecture.
			Then for every $\tilde{x} = \begin{pmatrix}
				x \\ 1
			\end{pmatrix}$ where $x \in \mathbb{R}^{N_0}$, we have:
			
			\begin{equation} \label{inequality_lemmeMax}
				\|R_{\theta_L} (\tilde{x})\|_{\infty} \leq \max \left( \max_{l=2, \dots, L} \prod_{s=\ell}^{L} \|\tilde{W}_s\|_{\mathrm{op}, \infty} ; \prod_{s=1}^{L} \| \tilde{W}_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \right).
			\end{equation}
		\end{lemma}
		
		\begin{proof}
			We prove~\eqref{inequality_lemmeMax}  by induction on $L \in \mathbb{N}$. For $L=1$, we have:
			
			\begin{equation}
				\|R_{\theta_L} (\tilde{x})\|_{\infty} = \|\sigma(\tilde{W}_1\tilde{x})\|_{\infty} \leq \| \tilde{W}_1\tilde{x}\|_{\infty} \leq \| \tilde{W}_1\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty
			\end{equation}
			
			Then by convention for $L=1$ (as in Lemma~\ref{Lemma_article}),
			
			\begin{equation}
				\max_{l=2, \dots, L} \prod_{s=\ell}^{L} \|\tilde{W}_s\|_{\mathrm{op}, \infty} = 1 .
			\end{equation}
			
			We deduce:
			\begin{equation}
				\|R_{\theta_L} (\tilde{x})\|_{\infty} \leq \| \tilde{W}_1\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \leq \max(1 ; \| \tilde{W}_1\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty) .
			\end{equation}
			
			Now assume that the property holds for \( L \geq 1 \). At rank \( L+1 \), using  that operator norm is sub-multiplicative and the fact that $ \sigma$ is 1-Lipschitz, we have:
			
			\begin{align}
				\|R_{\theta_{L+1}} (\tilde{x})\|_{\infty} &= \left\| \sigma \left( \tilde{W}_{L+1} \begin{pmatrix} R_{\theta_L} (\tilde{x}) \\ 1 \end{pmatrix}\right) \right\|_\infty \nonumber \\
				&\leq \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty} \left\| \begin{pmatrix} R_{\theta_{L}} (\tilde{x})\\ 1 \end{pmatrix} \right\|_\infty \nonumber \\
				&=\left\| \begin{pmatrix} \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty} \|R_{\theta_{L}} (\tilde{x})\|_{\infty} \\ \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty}  \end{pmatrix} \right\|_\infty
			\end{align}
			
			
			Then, applying the induction hypothesis to the term $\|R_{\theta_{L}} (\tilde{x})\|_{\infty}$, it comes:
			
			\begin{align}
				\left\| \begin{pmatrix} \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty} \|R_{\theta_{L}} (\tilde{x})\|_{\infty} \\ \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty}  \end{pmatrix} \right\|_\infty &= \max( \|\tilde{W}_{L+1}\|_{\mathrm{op}, \infty} \|R_{\theta_{L}} (\tilde{x})\|_{\infty} ; \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty}) \nonumber \\
				&\leq \max\left(\| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty} \max\left[\max_{l=2, \dots, L}\left(\prod_{s=l}^{L} \| \tilde{W}_s\|_{\mathrm{op}, \infty}\right) ; \prod_{s=1}^{L} \| \tilde{W}_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \right]; \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty}\right)  \nonumber \\
				&= \max\left( \max\left[ \max_{l=2, \dots, L}\left(\prod_{s=l}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty}\right) ; \prod_{s=1}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \right]; \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty} \right) \nonumber\\
				&=  \max \left[ \max_{l=2, \dots, L}\left(\prod_{s=l}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty}\right) ; \prod_{s=1}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty ; \| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty}\right]
			\end{align}
			
			
			Then, noticing that the term $\| \tilde{W}_{L+1}\|_{\mathrm{op}, \infty}$ can be included in $\max_{l=2, \dots, L}\left(\prod_{s=l}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty}\right)$ by adding index $\ell = L+1$, we have
			\begin{equation}
				\|R_{\theta_{L+1}} (\tilde{x})\|_{\infty} \leq \max\left[ \max_{l=2, \dots, L+1}\left(\prod_{s=l}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty}\right) ; \prod_{s=1}^{L+1} \| \tilde{W}_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \right].
			\end{equation}
			
			This concludes the induction and proves the lemma.
			
		\end{proof}
		
		We can now prove our main theorem.
		
		
		
		\begin{proof}[Proof of Theorem~\ref{Th:my_bound_extend_new}]
			
			We recall that $\theta'_\ell$ is defined as the parameter deduced from $\theta'$, associated with the architecture $(\ell, (N_0, \dots, N_\ell))$.
			
			With the convention that
			
			\begin{equation}
				\begin{split}
					\begin{cases}
						R_{\theta'_{l-1}}(\tilde{x}) = \tilde{x}
						& \text{if } l = 1, \\
						\prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op},q} = 1
						& \text{if} \; l = L,
					\end{cases}
				\end{split}
			\end{equation}
			
			
			we have, with Lemma \ref{Lemma_article}:
			
			\begin{equation} \label{eq:proof_main1}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} \leq \sum_{\ell=1}^{L}  \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right) \|W_\ell - W'_\ell\|_{\mathrm{op}, \infty} \| R_{\theta'_{\ell-1}} (\tilde{x}) \|_\infty.
			\end{equation}
			
			Thus, using Lemma \ref{Lemma_max} we can bound $\| R_{\theta'_{\ell-1}} (\tilde{x}) \|_\infty$, with the convention that an empty product is equal to 1, it follows: 
			
			\begin{equation}
				\| R_{\theta'_{\ell-1}} (\tilde{x}) \|_\infty \leq \max\left[ \max_{i=2, \dots, l-1}\left(\prod_{s=i}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty}\right) ; \prod_{s=1}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \right]
			\end{equation}
			
			Then, noticing that $\|\tilde{x}\|_\infty \geq 1$, and re-indexing the second $\max$ to include $\prod_{s=1}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty}$, we have:
			
			\begin{align}
				\| R_{\theta'_{\ell-1}} (\tilde{x}) \|_\infty &\leq \max\left[\max_{i=2, \dots, l-1}\left(\prod_{s=i}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty}\right) \| \tilde{x} \|_\infty ; \prod_{s=1}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty \right]\nonumber \\
				&= \| \tilde{x} \|_\infty \max\left[ \max_{i=2, \dots, l-1}\left(\prod_{s=i}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty}\right) ; \prod_{s=1}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty} \right] \nonumber \\
				& = \| \tilde{x} \|_\infty  \max_{i=1, \dots, l-1}\left(\prod_{s=i}^{l-1} \| \tilde{W}'_s\|_{\mathrm{op}, \infty}\right).
			\end{align}
			
			Using this bound in Equation \eqref{eq:proof_main1}, we get:
			
			\begin{equation}\label{eq:proof_main2}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} \leq \| \tilde{x} \|_\infty \sum_{\ell=1}^{L}  \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right) \|W_\ell - W'_\ell\|_{\mathrm{op}, \infty}  \max_{i=1, \dots, l-1}\left(\prod_{s=i}^{l-1} \| \tilde{W}'_s\|_
				{\mathrm{op}, \infty}\right).
			\end{equation}
			
			Then, recalling that $\tilde{x} = \begin{pmatrix}
				x \\ 1
			\end{pmatrix}$ with $x \in [-D,D]^d$, we have
			\begin{equation}
				\| \tilde{x} \|_\infty \leq \max(D;1)
			\end{equation}
			
			
			Then, we know by Lemma \ref{lem:matrices} that for every matrix in $\mathbb{R}^{m\times n}$:
			
			\begin{equation}
				\|W\|_{\mathrm{op},\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^{n} |w_{ij}|
			\end{equation}
			
			Thus, recalling that $\theta = (\tilde{W}_1, \dots, \tilde{W}_L)$ and because for all $l$, $ dim(W_l) = N_l \times N_{l-1}$, we can write:
			
			\begin{equation}
				\begin{split}
					\|W_l\|_{\mathrm{op}, \infty} \leq N_{l-1} \max_{i,j} |(W_l)_{ij}|
					& \leq N_{l-1} \|\theta\|_\infty
				\end{split}
			\end{equation}
			
			
			Using the previous inequality on $\|W_l - W'_l\|_{\mathrm{op},\infty}$ and replacing it in Inequality \eqref{eq:proof_main2}, we deduce that:
			
			\begin{align}\label{eq:proof_main3}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} &\leq \max(D;1) \sum_{\ell=1}^{L} N_{l-1} \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right) \max_{i=1, \dots, l-1}\left(\prod_{s=i}^{l-1} \| \tilde{W}'_s\|_
				{\mathrm{op}, \infty}\right)  \|\theta-\theta'\|_\infty
			\end{align}
			
			Thus, recalling that for all $l$, 
			
			\begin{equation}
				\|\tilde{W}_l\|_{\mathrm{op}, \infty} \leq r_l \quad \text{and} \quad \|\tilde{W}'_l\|_{\mathrm{op}, \infty} \leq r_l
			\end{equation}
			
			Equation \eqref{eq:proof_main3} becomes: 
			
			\begin{align}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} &\leq \max(D;1) \sum_{\ell=1}^{L} N_{l-1} \left( \prod_{k=\ell+1}^{L} r_k \right) \max_{i=1, \dots, l-1}\left(\prod_{s=i}^{l-1} r_s\right)  \|\theta-\theta'\|_\infty \nonumber\\
				& \leq \max(D;1) \sum_{\ell=1}^{L}\left( N_{l-1} \max_{i=1, \dots, l-1} \prod\limits_{\substack{j=i \\ j \neq l}}^{L} r_j\right) \|\theta-\theta'\|_\infty
			\end{align}
			
			Then taking the maximum over all layers, we finally have: 
			
			\begin{equation}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} \leq \max(D;1) \left(\max_{l=1, \dots, L} \max_{i=1, \dots, l-1}\prod\limits_{\substack{j=i \\ j \neq l}}^{L} r_j\right) \sum_{\ell=1}^{L} N_{l-1}  \|\theta-\theta'\|_\infty
			\end{equation}
			
			Then, we can rewrite this to show the geometric mean for partial products:
			
			\begin{equation}
				\|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} \leq \max(D;1) \left(\sqrt[L-1]{\max_{l=1, \dots, L} 
					\left( \max_{i=1, \dots, l-1} 
					\prod_{\substack{j=i \\ j \neq l}}^{L} r_j \right)}\right)^{L-1} \sum_{\ell=1}^{L} N_{\ell-1} \|\theta - \theta'\|_\infty
			\end{equation}
			
			
			
			
			
			
			
			
			Finally, taking the supremum  of both sides we obtain:
			
			\begin{equation}
				\sup_{x \in \Omega} \|R_\theta (\tilde{x}) - R_{\theta'} (\tilde{x})\|_{\infty} \leq \max(D;1) \left(\sqrt[L-1]{\max_{l=1, \dots, L} \left( \max_{i=1, \dots, l-1} 
					\prod_{\substack{j=i \\ j \neq l}}^{L} r_j \right)}\right)^{L-1} \sum_{\ell=1}^{L} N_{\ell-1} \|\theta - \theta'\|_\infty
			\end{equation}
			
		\end{proof}
		
		
		We now provide an intermediate Theorem (dealing with MLP without biases) to complete Theorem~\ref{Th:my_bound_extend_new} (MLP with biases not quantified) and Theorem~\ref{Th:my_bound_extend_new_conv} (CNN with no biases).
		
		\begin{theorem}[Bound for neural networks without bias]\label{th:mlp_noBias}
			With the same settings as in Theorem \ref{Th:my_bound_extend_new}, with $(b_1, \dots, b_L) = (b'_1, \dots, b'_L) = (0, \dots, 0)$
			(i.e $\forall l, \text{ we can take}\; \tilde{W}_l = W_l$ and $\tilde{W}'_l = W'_l$, the standard weight matrices without included bias). For all $x \in \Omega$ the following bound holds.
			
			\begin{equation}
				\sup_{x \in \Omega} \left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq D \left(\sqrt[L-1]{\max_{l=1, \dots, L}\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k}\right)^{L-1} \sum_{\ell=1}^{L} N_{l-1} \|\theta-\theta'\|_\infty.
			\end{equation}
		\end{theorem}
		
		\begin{proof}
			By analogy of the previous proof, we use Equation \eqref{inequality_lemmeC_no_bias} (which corresponds to Lemma \ref{Lemma_article} but without bias), and it comes:
			
			\begin{equation}
				\left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq \sum_{\ell=1}^{L} \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right)\|W_\ell - W'_\ell\|_{\mathrm{op}, \infty} \left\| R_{\theta'_{\ell-1}} (x) \right\|_\infty
			\end{equation}
			
			Now we want to bound the term $\left\| R_{\theta'_{\ell-1}} (x) \right\|_\infty$ by using \eqref{inequality_lemmeC_no_bias}. Thus, for any $\theta$ and with $\theta' = (0, \dots, 0)$, noticing that:
			\begin{equation}
				\begin{split}
					&\forall l \geq  2, \; \|R_{\theta'_{\ell-1}} (x)\|_\infty = 0, \\
					& \text{if} \; l =1  , \; \|R_{\theta'_{\ell-1}} (x)\|_\infty = \|x\|_\infty, \text{by convention}
				\end{split}
			\end{equation}
			
			We have:
			
			\begin{equation}
				\|R_{\theta_{l-1}} (x) \|_{\infty} \leq \prod_{k=2}^{l-1} \|W_k\|_{\mathrm{op}, \infty} \|W_1 - 0 \|_{\mathrm{op}, \infty} \|x \|_\infty = \prod_{k=1}^{l-1} \|W_k\|_{\mathrm{op}, \infty} \|x \|_\infty
			\end{equation}
			%
			%     We could use Lemma \ref{Lemma_max} as well, but we would get:
			%     \begin{equation}
				%         \|R_{\theta_{l-1}} (x) \|_{\infty} \leq \max[ \max_{j=2, \dots, l-1}(\prod_{s=j}^{l-1} \| W_s\|_{\mathrm{op}, \infty}) ; \prod_{s=1}^{l-1} \| W_s\|_{\mathrm{op}, \infty} \| \tilde{x} \|_\infty ]
				%     \end{equation}
			%
			%     Wich is larger than simply $\prod_{k=1}^{l-1} \|W_k\|_{\mathrm{op}, \infty} \|x \|_\infty$.
			%
			%
			
			
			Now for any $\theta, \theta'$ without bias, we can bound $\left\| R_{\theta'_{\ell-1}} (x) \right\|_\infty$ in \eqref{inequality_lemmeC_no_bias} , and it comes:
			
			\begin{equation} \label{eq1_proof_no_bias}
				\left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq \|x \|_\infty \sum_{\ell=1}^{L} \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right) \prod_{k=1}^{l-1} \|W'_k\|_{\mathrm{op},\infty} \|W_\ell - W'_\ell\|_{\mathrm{op}, \infty}
			\end{equation}
			
			Then, we use Lemma \ref{lem:matrices} to get: 
			\begin{equation}
				\begin{split}
					\|W_l\|_{\mathrm{op}, \infty} \leq N_{l-1} \max_{i,j} |(W_l)_{ij}|
					& \leq N_{l-1} \|\theta\|_\infty
				\end{split}
			\end{equation}
			
			Hence recalling that for all $k$ we have $\|W_k\|_{\mathrm{op}, \infty} \leq r_k \quad \text{and} \quad \|W'_k\|_{\mathrm{op}, \infty} \leq r_k$, it comes:
			
			\begin{align}
				\left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} &\leq \|x \|_\infty \sum_{\ell=1}^{L} N_{l-1} \left( \prod_{k=\ell+1}^{L} \|W_k\|_{\mathrm{op}, \infty} \right) \prod_{k=1}^{l-1} \|W'_k\|_{\mathrm{op},\infty} \|\theta - \theta'\|_{ \infty} \nonumber \\
				&\leq D \sum_{\ell=1}^{L} N_{l-1} \prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k \|\theta-\theta'\|_\infty 
			\end{align}
			
			Thus taking the maximum over all layers, we can rewrite the bound in terms of the geometric mean: 
			
			\begin{equation}
				\left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq D \left(\sqrt[L-1]{\max_{l=1, \dots, L}\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k}\right)^{L-1} \sum_{\ell=1}^{L} N_{l-1} \|\theta-\theta'\|_\infty
			\end{equation}
			
			Finally, taking the supremum gives the desired result: 
			
			\begin{equation}
				\sup_{x \in \Omega} \left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq D \left(\sqrt[L-1]{\max_{l=1, \dots, L}\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k}\right)^{L-1} \sum_{\ell=1}^{L} N_{l-1} \|\theta-\theta'\|_\infty.
			\end{equation}
			
			
			
			
			
			
			
		\end{proof}
		
		We now give the proof of our last theorem.
		
		
		\begin{proof}[Proof of Theorem~\ref{Th:my_bound_extend_new_conv}]
			
			Let \( \theta = (\mathcal{H}_1, \dots, \mathcal{H}_L)\) the vector of parameters where each $\mathcal{H_\ell}$ represent the convolution matrix of layer $l$.
			
			With the convention that
			
			\begin{equation}
				\begin{split}
					\begin{cases}
						R_{\theta'_{l-1}}(x) = x
						& \text{if } l = 1, \\
						\prod_{k=\ell+1}^{L} \|\mathcal{H}_k\|_{\mathrm{op},\infty} = 1
						& \text{if} \; l = L
					\end{cases}
				\end{split}
			\end{equation}
			
			We can start the proof using the same line of reasoning. Thus we can use directly Equation \eqref{eq1_proof_no_bias} that becomes for the convolutional case: 
			
			\begin{equation}
				\|R_\theta(x) - R_{\theta'}(x)\|_\infty
				\leq \|x\|_\infty \sum_{l=1}^{L} \prod_{k=\ell+1}^{L} \|\mathcal{H}_k\|_{\mathrm{op}, \infty} \prod_{k=1}^{l-1} \|\mathcal{H}'_k\|_{\mathrm{op},\infty}   \times \|\mathcal{H}_l - \mathcal{H}'_l\|_{\mathrm{op},\infty}
			\end{equation}
			
			Then by analogy we can write:
			
			\begin{equation}
				\begin{split}
					\|R_\theta(x) - R_{\theta'}(x)\|_\infty
					&\leq \|x\|_\infty \sum_{l=1}^{L} \prod_{k=\ell+1}^{L} \|\mathcal{H}_k\|_{\mathrm{op}, \infty} \prod_{k=1}^{l-1} \|\mathcal{H}'_k\|_{\mathrm{op},\infty}   \times \|\mathcal{H}_l - \mathcal{H}'_l\|_{\mathrm{op},\infty} \\
					& \leq D \sum_{l=1}^{L} \prod_{k=\ell+1}^{L} r_k \prod_{k=1}^{l-1} r_k   \times \|\mathcal{H}_l - \mathcal{H}'_l\|_{\mathrm{op},\infty} \\
					&= D \sum_{l=1}^{L} \prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k   \times \|\mathcal{H}_l - \mathcal{H}'_l\|_{\mathrm{op},\infty}
				\end{split}
			\end{equation}
			
			Then, we know by lemma \ref{lem:matrices} that for every matrix in $\mathbb{R}^{m\times n}$ it holds :
			
			\begin{equation}
				\|W\|_{\mathrm{op},\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^{n} |w_{ij}|
			\end{equation}
			
			
			Thanks to the convolutional structure, we improve the bound on $\|\mathcal{H}_l\|_{\mathrm{op},\infty}$ given by Lemma~\ref{lem:matrices}.
			Let us note the output of the previous layer as $y_{l-1}$. This output is a set of feature maps with dimensions $(n_{l-1} \times m_{l-1}) \times c_{l-1}$, where $c_{l-1}$ is the number of feature maps (i.e., the number of filters) in the previous layer.
			
			Next, recalling that,we want to express the convolution at layer $l$ as a matrix multiplication: $\mathcal{H}_l \text{vec}(y_{l-1})$.
			
			Then we write $\mathcal{H}_l$ as a block-Toeplitz matrix:
			
			\begin{equation}
				\mathcal{H}_l =
				\begin{pmatrix} H_{l,1}\\ \vdots \\ H_{l,c_{l}}\end{pmatrix}
			\end{equation}
			
			
			where each block $H_{l,i}$ is a Toeplitz matrix of size $(n_l m_l) \times (n_{l-1} m_{l-1}c_{l-1})$. These matrices are highly sparse, with each row composed of coefficients of the filters arranged in a specific pattern, while the remaining entries are zeros.
			
			Thus, each block $H_{l,i}$ in $\mathcal{H}_l$ represents the convolution operation between the $i$-th filter and the feature maps of the previous layer.
			
			Hence, the overall dimensions of $\mathcal{H}_l$ are:
			
			\begin{equation}
				\dim(\mathcal{H}_l) = (n_l m_l c_l) \times  (n_{l-1} m_{l-1} c_{l-1}).
			\end{equation}
			
			Then to bound the norm of $\mathcal{H}_l$, we consider only the non-zero coefficients in its rows, which are $p_l^2 \times c_{l-1}$, where $p_l^2$ denotes the size of the filters at layer $l$.
			
			Thus, recalling that $\theta = (\mathcal{H}_1, \dots, \mathcal{H}_L)$ we have:
			
			\begin{equation}
				\begin{split}
					\|\mathcal{H}_l\|_{\mathrm{op}, \infty} \leq c_{l-1}\times p_l^2 \max_{i,j} |(\mathcal{H}_l)_{ij}|
					& \leq c_{l-1} \times p_l^2 \|\theta\|_\infty
				\end{split}
			\end{equation}
			
			Using the previous inequality on $\|\mathcal{H}_l - \mathcal{H}'_l\|_{\mathrm{op},\infty}$ we can  bound the quantity $\|R_\theta(x) - R_{\theta'}(x)\|_\infty$, by:
			
			
			\begin{equation}
				\begin{split}
					\|R_\theta(x) - R_{\theta'}(x)\|_\infty
					&\leq D \sum_{l=1}^{L} \prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k   \times \|\mathcal{H}_l - \mathcal{H}'_l\|_{\mathrm{op},\infty} \\
					& \leq D \sum_{l=1}^{L} c_{l-1}\times p_l^2\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k  \|\theta-\theta'\|_\infty
				\end{split}
			\end{equation}
			
			Thus taking the maximum over all layers, we can rewrite the bound in terms of the geometric mean: 
			
			\begin{equation}
				\left\| R_{\theta} (x) - R_{\theta'} (x) \right\|_{\infty} \leq D \left(\sqrt[L-1]{\max_{l=1, \dots, L}\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k}\right)^{L-1} \sum_{\ell=1}^{L} c_{l-1} \times p_l^2 \|\theta-\theta'\|_\infty
			\end{equation}
			
			
			Hence, we can conclude that:
			
			\begin{equation}
				\begin{split}
					&\sup_{x \in \Omega}\| R_{\theta}(x) - R_{\theta'}(x) \|_{\infty} \leq D \left(\sqrt[L-1]{\max_{l=1, \dots, L}\prod\limits_{\substack{k=1 \\ k \neq l}}^{L} r_k}\right)^{L-1} \sum_{\ell=1}^{L} c_{l-1} \times p_l^2 \|\theta-\theta'\|_\infty.
				\end{split}
			\end{equation}
		\end{proof}
		
		
		Then, we present the 3 specific architectures of residual and bottleneck block, in Resnet18, Resnet50 \cite{he2016deep} and MobilnetV2 \cite{sandler2018mobilenetv2}.
		
		\section{Calculations for MobileNetV2 and Resnets}
		
		\subsection{Specific structure of Resnet18}
		
		% Residual Block ResNet18
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=0.5cm, auto, >=Latex]
				\node (input) {Input};
				\node[draw, right=of input] (conv1) {Conv 3x3};
				\node[draw, right=of conv1] (bn1) {BN};
				\node[draw, right=of bn1] (relu1) {ReLU};
				\node[draw, right=of relu1] (conv2) {Conv 3x3};
				\node[draw, right=of conv2] (bn2) {BN};
				\node[right=of bn2] (add) {+};
				\node[draw, right=of add] (relu2) {ReLU};
				\node[right=of relu2] (output) {Output};
				
				\draw[->] (input) -- (conv1);
				\draw[->] (conv1) -- (bn1);
				\draw[->] (bn1) -- (relu1);
				\draw[->] (relu1) -- (conv2);
				\draw[->] (conv2) -- (bn2);
				\draw[->] (bn2) -- (add);
				\draw[->] (add) -- (relu2);
				\draw[->] (relu2) -- (output);
				\draw[->] (input) -- ++(0,-1) -| node[below, pos=0.25] {$W_s$} (add);
			\end{tikzpicture}
			\caption{Structure of a Residual Block of ResNet18} \label{fig:Residual_Block_of_ResNet18}
		\end{figure}
		
		\begin{lemma}[Matrix Representation of a Residual Block in ResNet-18]\label{lem:resnet_block_representation}
			The output $ y \in \mathbb{R}^n $ of a residual block in ResNet-18, as illustrated in Figure~\ref{fig:Residual_Block_of_ResNet18}, can be expressed as:
			\begin{equation}
				y = \sigma \left( V_2 \cdot \tilde{\sigma}_1 \left( V_1 \cdot f \right) \right),
			\end{equation}
			
			where $f \in \mathbb{R}^n$ is the input, and $V_1$ and $V_2$ are defined as:
			
			\begin{equation}
				V_1 = \begin{pmatrix} W_1 \\ I \end{pmatrix} \in \mathbb{R}^{(d + n) \times n}, \quad 
				V_2 = \begin{pmatrix} W_2 & W_s \end{pmatrix} \in \mathbb{R}^{m \times (d + n)},
			\end{equation}
			
			with $W_1 \in \mathbb{R}^{d \times n}$ and $W_2 \in \mathbb{R}^{m \times d}$ as the convolutional weight matrices, $I \in \mathbb{R}^{n \times n}$ as the identity matrix and $W_s \in \mathbb{R}^{m \times n}$ represents the shortcut weight matrix. Note that, if the input and output of the block have the same dimension $W_s = I$. The function $\tilde{\sigma}_1$ applies the non-linearity $\sigma$ only to the term $W_1 \cdot f$, leaving the shortcut component $I \cdot f$ unchanged.
		\end{lemma}
		
		\begin{proof}
			The residual block computes the output $ y$ as:
			\begin{equation}
				y = \sigma\left(\text{BN}_2\left(\text{Conv}_2\left(\text{ReLU}\left(\text{BN}_1\left(\text{Conv}_1(f)\right)\right)\right)\right) + W_s f\right),
			\end{equation}
			where: $\text{Conv}_1$ and $\text{Conv}_2$ represent the convolutional layers with weight matrices $W_1$ and $W_2$, respectively, $\text{BN}_1$ and $\text{BN}_2$ are batch normalization layers (omitted in the matrix formulation because we removed them in our experiments).
			
			The first convolutional layer computes:
			\begin{equation}
				x_1 = \text{Conv}_1(f) = W_1 \cdot f,
			\end{equation}
			where $W_1 \in \mathbb{R}^{d \times n}$. This result is passed through $\text{BN}_1$ and $\sigma$, yielding:
			
			\begin{equation}
				\tilde{x}_1 = \sigma\left(\text{BN}_1(x_1)\right).
			\end{equation}
			In our matrix representation, we express this as:
			\begin{equation}
				\tilde{x}_1 = \tilde{\sigma}_1\left(V_1 \cdot f\right),
			\end{equation}
			where \(V_1 = \begin{pmatrix} W_1 \\ I \end{pmatrix} \in \mathbb{R}^{(d + n) \times n}\). The expanded computation is:
			\begin{equation}
				V_1 \cdot f = \begin{pmatrix} W_1 \cdot f \\ I \cdot f \end{pmatrix} = \begin{pmatrix} W_1 \cdot f \\ f_1 \\ \vdots \\ f_n \end{pmatrix}.
			\end{equation}
			Thus:
			\begin{equation}
				\tilde{\sigma}_1\left(V_1 \cdot f\right) = \begin{pmatrix} \sigma(W_1 \cdot f) \\ f_1 \\ \vdots \\ f_n \end{pmatrix}.
			\end{equation}
			
			
			Then the second convolutional layer computes:
			\begin{equation}
				x_2 = \text{Conv}_2\left(\sigma\left(\text{BN}_1(x_1)\right)\right) = W_2 \cdot \tilde{x}_1,
			\end{equation}
			where \(W_2 \in \mathbb{R}^{m \times d}\). Combining the result with the shortcut connection $W_sf$, we obtain:
			\begin{equation}
				y = \sigma\left(x_2 + W_sf\right).
			\end{equation}
			
			Using our matrix representation for $V_2$, where $V_2 = \begin{pmatrix} W_2 & W_s \end{pmatrix} \in \mathbb{R}^{m \times (d + n)}$, the computation expands as:
			\begin{equation}
				V_2 \cdot \begin{pmatrix} \sigma(W_1 \cdot f) \\ f_1 \\ \vdots \\ f_n \end{pmatrix} = \begin{pmatrix} W_2 & W_s \end{pmatrix} \cdot \begin{pmatrix} \sigma(W_1 \cdot f) \\ f_1 \\ \vdots \\ f_n \end{pmatrix} =  W_2 \cdot \sigma(W_1 \cdot f) + W_s \cdot f = x_2 + W_sf.
			\end{equation}
			
			So the final output of the block is:
			\begin{equation}
				y = \sigma\left(V_2 \cdot \tilde{\sigma}_1\left(V_1 \cdot f\right)\right),
			\end{equation}
			
			Thus, the residual block's output is equivalent to our matrix representation.
		\end{proof}
		
		An immediate consequence of this Lemma is:
		
		\begin{equation}
			\begin{cases}
				\|V_1\|_{\mathrm{op},\infty} = \max(1,\|W_1\|_{\mathrm{op},\infty}), \\
				\|V_2\|_{\mathrm{op},\infty} = \|W_2\|_{\mathrm{op},\infty} + 1, & \text{if $W_s = I$}, \\
				\|V_2\|_{\mathrm{op},\infty} \leq \|W_2\|_{\mathrm{op},\infty} + \|W_s\|_{\mathrm{op},\infty}, & \text{otherwise}.
			\end{cases}
		\end{equation}
		
		\subsection{Specific structure of Resnet50}
		
		% Residual Block Bottleneck ResNet50
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=0.5cm, auto, >=Latex]
				\node (input) {Input};
				\node[draw, right=of input] (conv1) {Conv 1x1};
				\node[draw, right=of conv1] (bn1) {BN};
				\node[draw, right=of bn1] (relu1) {ReLU};
				\node[draw, right=of relu1] (conv2) {Conv 3x3};
				\node[draw, right=of conv2] (bn2) {BN};
				\node[draw, right=of bn2] (relu2) {ReLU};
				\node[draw, right=of relu2] (conv3) {Conv 1x1};
				\node[draw, right=of conv3] (bn3) {BN};
				\node[right=of bn3] (add) {+};
				\node[draw, right=of add] (relu3) {ReLU};
				\node[right=of relu3] (output) {Output};
				
				\draw[->] (input) -- (conv1);
				\draw[->] (conv1) -- (bn1);
				\draw[->] (bn1) -- (relu1);
				\draw[->] (relu1) -- (conv2);
				\draw[->] (conv2) -- (bn2);
				\draw[->] (bn2) -- (relu2);
				\draw[->] (relu2) -- (conv3);
				\draw[->] (conv3) -- (bn3);
				\draw[->] (bn3) -- (add);
				\draw[->] (add) -- (relu3);
				\draw[->] (relu3) -- (output);
				\draw[->] (input) -- ++(0,-1) -| node[below, pos=0.25] {$W_s$} (add);
			\end{tikzpicture}
			\caption{Structure of a Residual Block of ResNet50} \label{fig:Residual_Block_of_ResNet50}
		\end{figure}
		
		\begin{lemma}[Matrix Representation of a Bottleneck Block in ResNet-50]\label{lem:resnet50_block_representation}
			The output $ y \in \mathbb{R}^n $ of a bottleneck block in ResNet-50, as illustrated in Figure~\ref{fig:Residual_Block_of_ResNet50}, can be expressed as:
			
			\begin{equation}
				y = \sigma \left( V_3 \cdot \tilde{\sigma}_2 \left( V_2 \cdot \tilde{\sigma}_1 \left( V_1 \cdot f \right) \right) \right),
			\end{equation}
			
			where $f \in \mathbb{R}^n$ is the input, and the matrices $V_1$, $V_2$, and $V_3$ are defined as follows:
			\begin{equation}
				V_1 = \begin{pmatrix} W_1 \\ I \end{pmatrix} \in \mathbb{R}^{(d_1 + n) \times n}, \quad
				V_2 = 
				\begin{pmatrix}
					W_2 & 0 \\
					0 & I
				\end{pmatrix} \in \mathbb{R}^{(d_2 + n) \times (d_1 + n)}, \quad
				V_3 = \begin{pmatrix} W_3 & W_s \end{pmatrix} \in \mathbb{R}^{m \times (d_2 + n)}.
			\end{equation}
			
			Here: $W_1 \in \mathbb{R}^{d_1 \times n}$, $W_2 \in \mathbb{R}^{d_2 \times d_1}$, and $W_3 \in \mathbb{R}^{m \times d_2}$ are the weight matrices of the three convolutional layers in the bottleneck block, $W_s$ is the weight matrix associated with the shortcut, $I \in \mathbb{R}^{n \times n}$ is the identity, and $0$ denotes zero matrices of appropriate dimensions.
			
			The functions $\tilde{\sigma}_1$ and $\tilde{\sigma}_2$ apply the non-linearity $\sigma$ only to specific components, leaving the shortcut components unchanged.
		\end{lemma}
		
		\begin{proof}
			The bottleneck block computes the output $ y $ as:
			\begin{equation}
				y = \sigma\left(\text{BN}_3\left(\text{Conv}_3\left(\text{ReLU}\left(\text{BN}_2\left(\text{Conv}_2\left(\text{ReLU}\left(\text{BN}_1\left(\text{Conv}_1(f)\right)\right)\right)\right)\right)\right)\right) + W_sf\right),
			\end{equation}
			where $\text{Conv}_1$, $\text{Conv}_2$, and $\text{Conv}_3$ represent the three convolutional layers with weight matrices $W_1$, $W_2$, and $W_3$, respectively, $\text{BN}_1$, $\text{BN}_2$, and $\text{BN}_3$ are batch normalization layers (omitted in the matrix formulation because we removed them in our experiments).
			
			The first convolutional layer computes:
			\begin{equation}
				x_1 = \text{Conv}_1(f) = W_1 \cdot f,
			\end{equation}
			where \(W_1 \in \mathbb{R}^{d_1 \times n}\). This result is passed through \(\text{BN}_1\) and \(\sigma\), yielding:
			\begin{equation}
				\tilde{x}_1 = \sigma\left(\text{BN}_1(x_1)\right).
			\end{equation}
			In our matrix representation, we express this as:
			\begin{equation}
				\tilde{x}_1 = \tilde{\sigma}_1\left(V_1 \cdot f\right),
			\end{equation}
			
			where $V_1 = \begin{pmatrix} W_1 \\ I \end{pmatrix} \in \mathbb{R}^{(d_1 + n) \times n}$. The expanded computation is:
			
			\begin{equation}
				V_1 \cdot f = \begin{pmatrix} W_1 \cdot f \\ I \cdot f \end{pmatrix} = \begin{pmatrix} W_1 \cdot f \\ f_1 \\ \vdots \\ f_n \end{pmatrix}.
			\end{equation}
			Thus:
			
			\begin{equation}
				\tilde{\sigma}_1\left(V_1 \cdot f\right) = \begin{pmatrix} \sigma(W_1 \cdot f) \\ f_1 \\ \vdots \\ f_n \end{pmatrix}.
			\end{equation}
			
			The second convolutional layer computes:
			\begin{equation}
				\tilde{x}_2 = \sigma(\text{Conv}_2(\tilde{x}_1)) = \sigma(W_2 \cdot \tilde{x}_1),
			\end{equation}
			where $W_2 \in \mathbb{R}^{d_2 \times d_1}$. Using $V_2$, the expanded computation is:
			
			\begin{equation}
				V_2 \cdot \begin{pmatrix} \sigma(W_1 \cdot f) \\ f_1 \\ \vdots \\ f_n \end{pmatrix} = \begin{pmatrix}
					W_2 & 0 \\
					0 & I
				\end{pmatrix}  \cdot \begin{pmatrix} \sigma(W_1 \cdot f) \\ f_1 \\ \vdots \\ f_n \end{pmatrix} =
				\begin{pmatrix}
					W_2 \cdot \sigma(W_1 \cdot f) \\
					f_1 \\
					\vdots \\
					f_n
				\end{pmatrix}.
			\end{equation}
			
			So the output of this layer with our matrix representation is: $\begin{pmatrix}
				\sigma(W_2 \cdot \sigma(W_1 \cdot f)) \\
				f_1 \\
				\vdots \\
				f_n
			\end{pmatrix}.$ 
			
			The third convolutional layer combines the outputs of the second layer and the shortcut connection:
			
			\begin{equation}
				x_3 = \text{Conv}_3(\tilde{x}_2) = W_3 \cdot \tilde{x}_2 + W_sf,
			\end{equation}
			
			and the final original output of the block is $y = \sigma(x_3)$
			
			Using $V_3$, this becomes:
			\begin{equation}
				V_3 \cdot \begin{pmatrix} \sigma(W_2 \cdot \sigma(W_1 \cdot f)) \\ f_1 \\ \vdots \\ f_n \end{pmatrix} = \begin{pmatrix} W_3 & W_s \end{pmatrix} \cdot \begin{pmatrix} \sigma(W_2 \cdot \sigma(W_1 \cdot f)) \\ f_1 \\ \vdots \\ f_n \end{pmatrix} = W_3 \cdot \sigma( W_2 \cdot \sigma(W_1 \cdot f)) + W_sf.
			\end{equation}
			
			Then the final output is:
			\begin{equation}
				y = \sigma\left(V_3 \cdot \tilde{\sigma}_2\left(V_2 \cdot \tilde{\sigma}_1\left(V_1 \cdot f\right)\right)\right),
			\end{equation}
			ending the proof of the matrix representation.
		\end{proof}
		
		An immediate consequence of this Lemma is that: 
		
		\begin{equation}
			\begin{cases}
				\|V_1\|_{\mathrm{op},\infty} = \max(1,\|W_1\|_{\mathrm{op},\infty}), \\
				\|V_2\|_{\mathrm{op},\infty} = \max(1,\|W_2\|_{\mathrm{op},\infty}), \\
				\|V_3\|_{\mathrm{op},\infty} = \|W_3\|_{\mathrm{op},\infty} + 1, & \text{if $W_s = I$}, \\
				\|V_3\|_{\mathrm{op},\infty} \leq \|W_3\|_{\mathrm{op},\infty} + \|W_s\|_{\mathrm{op},\infty}, & \text{otherwise}.
			\end{cases}
		\end{equation}
		
		
		\subsection{Specific structure of MobileNetV2}
		
		% Residual Block Inverted Bottleneck de MobileNetV2
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[node distance=0.5cm, auto, >=Latex]
				\node (input) {Input};
				\node[draw, right=of input] (conv1) {Conv 1x1};
				\node[draw, right=of conv1] (bn1) {BN};
				\node[draw, right=of bn1] (relu1) {ReLU6};
				\node[draw, right=of relu1] (dwconv) {DW Conv 3x3};
				\node[draw, right=of dwconv] (bn2) {BN};
				\node[draw, right=of bn2] (relu2) {ReLU6};
				\node[draw, right=of relu2] (conv2) {Conv 1x1};
				\node[draw, right=of conv2] (bn3) {BN};
				\node[right=of bn3] (add) {+};
				\node[right=of add] (output) {Output};
				
				\draw[->] (input) -- (conv1);
				\draw[->] (conv1) -- (bn1);
				\draw[->] (bn1) -- (relu1);
				\draw[->] (relu1) -- (dwconv);
				\draw[->] (dwconv) -- (bn2);
				\draw[->] (bn2) -- (relu2);
				\draw[->] (relu2) -- (conv2);
				\draw[->] (conv2) -- (bn3);
				\draw[->] (bn3) -- (add);
				\draw[->] (add) -- (output);
				\draw[->] (input) -- ++(0,-1) -| node[below, pos=0.25] {$I$} (add);
			\end{tikzpicture}
			\caption{Structure of a Residual Block of MobileNetV2} \label{fig:Residual_Block_of_MobilnetV2}
		\end{figure}
		
		
		\begin{remark}
			The only difference between Resnet-50 bottleneck block and MobileNetV2 bottleneck block is that MobileNetV2 uses inverted residuals bottleneck bock (expansion before convolution) and Depthpwise convolution (only one filter is applied for each input channel), without activation function at the end of the bloc. Thus the matrix representation can also be expressed as: 
			\begin{equation}
				y = V_3 \cdot \tilde{\sigma}_2 \left( V_2 \cdot \tilde{\sigma}_1 \left( V_1 \cdot f \right) \right),
			\end{equation}
			Where all matrices are defined in Lemma \ref{lem:resnet50_block_representation}.
		\end{remark}
		

\section{Experimental setup for the MLP case}
		
		
For the Figure \ref{fig:MLP_comparison}, we developed four MLPs with different depths. All of them were trained on MNIST dataset during 2 epochs,  using the Adam optimizer with a learning rate of 0.001 and a batch size of 64.  The MLP with depth 5 has four hidden layers with sizes [1024, 512, 256, 128]. The MLP with depth 7 has six hidden layers with sizes [1024, 512, 256, 128, 64, 32]. The MLP with depth 9 has eight hidden layers with sizes [1024, 512, 256, 128, 128, 64, 64, 32]. The MLP with depth 11 has ten hidden layers with sizes [1024, 512, 512, 256, 256, 128, 128, 64, 64, 32]. The models were quantized using uniform quantization with different bit widths: 4, 8, 16, and 24 bits only on the weights.


\section{Experimental Setup of Figures \ref{fig:ICML_bounds_comparison_Resnet18}, \ref{fig:ICML_bounds_comparison_Mobilnet}, \ref{fig:ICML_bounds_comparison_Resnet50}}

 The networks were initialized without pre-trained weights (pretrained=False). Each network was re-trained from scratch during 3 epochs on the MNIST dataset. During training, we used the Stochastic Gradient Descent (SGD) optimizer with a learning rate of $0.001$ and a momentum value of $0.9$. The training process used a batch size of $64$.
Moreover, the Batch Normalization layers were removed from each of the models before the training. This modification was made to match the context of Theorem \ref{Th:my_bound_extend_new_conv} and to simplify the network structure for the analysis of weight norms distribution without the regularization effects introduced by Batch Normalization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}






% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
