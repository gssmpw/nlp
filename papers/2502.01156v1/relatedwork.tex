\section{Related Works}
\label{sec:related_works}
	
	Approximation bounds have been studied from various perspectives, including results on the approximation capacity of neural networks \cite{devore2021neural} \cite{ding2019universal} \cite{csaji2001approximation} \cite{barron1993universal} and the topological properties of the realization map \cite{petersen2021topological}, particularly focusing on the fact that this realization is Lipschitz continuous with the constant depending on the network architecture \cite{petersen2021topological} \cite{devore2021neural}.
	
	There also have been analytical works directly quantifying the value of \( C \) in Equation \eqref{eq_intro_1}. For example, \cite{neyshabur2017pac} studied this constant in the context of a particular case where \( \theta' \) is obtained through controlled perturbation (i.e., perturbations that do not significantly modify the norm of the initial weights). They derived, for the \( L^2 \) norm, a constant that depends on the network depth, the norm of the weights, the data and the perturbation. However, their results do not generalize to any \( \theta' \) and  are not applicable to arbitrary quantization. Similarly, \cite{berner2020analysis} expressed the constant in the case of the \( L^\infty \) norm but with uniform parameter bounds and, specifically for \( d_{\text{out}} = 1 \), which cannot be applied to every neural network tasks.
	
	More recently, \cite{gonon2023approximation} formalized a framework for approximation bounds of ReLU neural networks, providing a general upper-bound for the constant of a neural network in terms of its architecture, weight norms, and other properties. Specifically, their result applies to neural networks defined over general \(L^p\)-spaces, and under general constraints on the weight parameters. The generalization provided by their upper-bound generalizes prior results, which were often limited to specific cases, such as \cite{neyshabur2017pac} for spectrally-normalized networks.
	By the same authors, in \cite{gonon2024path}, there is another approach that generalizes the notion of approximation bounds to Directed Acyclic Graphs (DAGs) using \( \ell_1 \)-path norms. This formulation as graphs allows for more flexibility on the network architecture (pooling, skip connection...). This work provides general bounds with the notable feature of being invariant under parameter rescaling. They improve their previous paper results, relaxing some assumptions, notably the condition \( r \geq 1 \), but introducing new conditions such as \( \theta_i \theta'_i \geq 0 \), which is not always satisfied for general distinct parameters $(\theta,\theta')$. We further discuss these bounds  in relation to our work in Section~\ref{sec:prelim}.
	
	%Moreover, it is important to note that the bound is derived using the \( L_1 \)-norm, and as such, it can be directly compared to our bound, which is formulated in the infinity norm. The comparison with the \( L_1 \)-bound leads to the same conclusion as the bound from \cite{gonon2023approximation}, namely that the bound obtained by our theorem is much tighter.
	
	%Besides such bounds are crucial not only in analyzing the behavior of neural networks but also in applications like post-training quantization, where the weights of a pre-trained model are modified to reduce its computational complexity.
	
	
	A directly  application of approximation bounds is quantization.  Quantization significantly reduces the storage and computational requirements of deep models \cite{gholami2022survey}. The impact of quantization on the approximation capabilities of neural networks has been studied in \cite{ding2019universal} and \cite{hubara2018quantized}, where the authors provided empirical evidence for maintaining high performance even at low precision. However, formal guarantees are still limited, and existing bounds often assume either uniform quantization or specific activation functions, which limit their applicability. Recent works have introduced strategies for low-bit quantization to retain high predictive accuracy in practical implementations. For instance, \cite{choukroun2019low} explored methods for efficient inference with low-bit quantization, while \cite{courbariaux2015binaryconnect} demonstrated the effectiveness of binary weight quantization during training, opening a way for training and deploying models with significantly reduced memory and computational demands. Another important aspect in controlling the accuracy of quantized networks involves understanding singular values in convolutional layers  \cite{sedghi2018singular} which helps inform layer-specific quantization strategies.