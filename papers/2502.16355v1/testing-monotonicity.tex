% !TEX root = main.tex

\section{Testing Monotonicity}

In this section, we show that using the directed and real-valued version of Talagrand's inequality, we may design an ``edge tester'' for testing monotonicity of distributions using subcube conditioning. In particular, we give the following theorem.
\begin{theorem}\label{thm:mon-ub}
    There exists an algorithm that receives as input subcube conditioning access to an unknown distribution $p$ supported on $\{-1,1\}^n$, as well as an accuracy parameter $\eps$. The algorithm makes $\tilde{O}(n/\eps^2)$ subcube conditioning queries and satisfies the following guarantees:
    \begin{itemize}
        \item If $p$ is monotone, the algorithm outputs ``accept'' with probability at least $0.9$.
        \item If $p$ is $\eps$-far from monotone, the algorithm outputs ``reject'' with probability at least $0.9$.
    \end{itemize}
\end{theorem}

The algorithm referred to in Theorem~\ref{thm:mon-ub} is given in Figure~\ref{fig:alg}.
% where we take the variable $c_0$ to be a small enough constant factor of $1/\sqrt{\log n}$. 
We break up the proof of \Thm{mon-ub}
into a few parts. First, we argue about the running time.

\begin{figure}
\begin{framed}

\textbf{Algorithm for Testing Monotonicity of Distributions}. We receive as input subcube conditioning access to an unknown distribution $p$ which is supported on $\{-1,1\}^n$. Furthermore, we receive the accuracy parameter $\eps \in (0,1)$. We let $c_0$ denote a sufficiently small constant. 
%     The algorithm proceeds in the following manner: 
\begin{enumerate}
    \item For all integers $w\geq0$ such that $2^w = \tilde{O}(n/\eps^2)$, repeat the following $t = O(2^{w} \log(n/\eps))$ times:
    \begin{itemize}
        \item Sample $\bx \sim p$ and $\bi \sim [n]$, and consider the restriction $\brho \in \{-1,1,*\}^n$ given by $\brho_j = \bx_j$ if $j \neq \bi$, and $\brho_{\bi} = *$. 
        \item Let $\eta = c_0^2\eps^2 \cdot 2^{w} / (16n \cdot \log(n/\eps) \cdot \log n)$ and take $m = O(\log (n/\eps)/\eta)$ subcube conditioning queries with restriction $\brho$ while counting the number of $1$'s and $-1$'s in coordinate $\bi$ observed. If the number of $-1$'s observed is larger than $m\left(1/2 + \sqrt{\eta}/2\right)$, output ``reject.''
    \end{itemize}
    \item If the algorithm has not rejected, output ``accept.''
\end{enumerate}

\end{framed}
\caption{Algorithm for Testing Monotonicity of Distributions} \label{fig:alg}
\end{figure}

\begin{claim}
    The query complexity is $\tilde{O}(n / \eps^2)$.
\end{claim}
\begin{proof}
    We simply upper bound the query complexity by inspecting Figure~\ref{fig:alg}. We have that (disregarding constant factors) the query complexity is the sum over all integers $w \geq 0$ such that $2^{w} = \tilde{O}(n/\eps^2)$ of 
    \[ O(2^{w} \cdot \log(n/\eps)) \cdot O\left(\dfrac{n\cdot \log^2(n/\eps) \cdot \log n}{c_0^2 \eps^2 \cdot 2^{w}} \right) = \tilde{O}(n / \eps^2). \]
    There are $O(\log(n/\eps))$ such settings of $w$, so the total complexity is still $\tilde{O}(n / \eps^2)$.
\end{proof}

\begin{lemma}
    Whenever $p$ is monotone, the algorithm outputs ``accept'' with probability at least $0.9$.
\end{lemma}

\begin{proof}
    Note that if $p$ is monotone, then for any restriction $\rho \in \{-1, 1, *\}^n$, which has one coordinate $i$ with $\rho_i = *$, a sample $\by$ from $p_{|\rho}$ must have the probability that $\by_i$ is 1 is at least the probability that it is $-1$. A standard Hoeffding bound implies that if one takes $m = O(\log(n/\eps)/\eta)$ samples of some event which is more likely to be $1$ than $-1$, the probability that the number of $1$'s observed is smaller than $m (1/2 - \sqrt{\eta}/2)$ is smaller than $\poly(\eps / n)$, for an arbitrarily large polynomial. Note that the number of times we may wrongfully reject is at most the query complexity, which is at most $\tilde{O}(n/\eps^2)$. So we may union bound as desired.
\end{proof}

\begin{lemma}\label{lem:far-case-reject}
    Whenever $p$ is $\eps$-far from monotone, the algorithm outputs ``reject'' with probability at least $0.9$.
\end{lemma}

\begin{proof}
    We show that whenever $p$ is $\eps$-far from monotone, there exists some $\gamma \in \{0, \dots, h\}$ with $h = O(\log(n/\eps))$ and a setting of $\ell \in \{ 0, \dots, r\}$ where $r = O(\log(n/\eps))$ which satisfies $2^{2\gamma + r +1} = \tilde{O}(n / \eps^2)$ and
    \begin{align*}
        \Prx_{ \substack{\bx \sim p \\ \bi \sim [n]}}\left[\left(\dfrac{(p(\bx^{(\bi\to-1)}) - p(\bx^{(\bi\to1)}))^+}{p(\bx^{(\bi\to-1)}) + p(\bx^{(\bi\to1)})}\right)^2 \geq \eta\right] \geq \frac{1}{r \cdot 2^{\gamma + \ell}}.
    \end{align*}
    for $\eta = c_0^2 \eps^2 \cdot 2^{2\gamma+\ell} / (16 h \cdot n \cdot \log n)$. When the algorithm iterates over all $w \geq 0$ such that $2^w = \tilde{O}(n/\eps^2)$, it will eventually consider $w = 2\gamma + \ell$.
    This implies that except with probability $0.01$, one of the $t$ samples $\bx \sim p$ and $\bi \sim [n]$ satisfy the above bound, since we repeat $t = O(2^{w}\log(n/\eps))$ times and $2^{w} = 2^{2\gamma+\ell}$ is larger than $2^{\gamma+\ell}$. Once that is set, with probability except $0.01$, the algorithm outputs reject; the subcube conditioning query $\brho$ is exactly sampling from $\{-1,1\}$ whose probability of being $-1$ is at least $1/2 + \sqrt{\eta}$. By a Hoeffding bound, the probability that the number of $-1$'s is smaller than $m(1/2 + \sqrt{\eta}/2)$ is at most $0.01$. From Corollary~\ref{cor:l1-tal}, for 
    small enough constant $c_0$. the fact that $p$ is $\eps$-far from monotone implies,
    \begin{align*}
        \frac{c_0\eps}{\sqrt{\log n}} &\leq \sum_{x \in \{-1,1\}^n} \sqrt{\sum_{i:x_i = -1} \left( \left(p(x^{(i\to-1)}) - p(x^{(i\to1)} \right)^+ \right)^2} \\
        &= \Ex_{\bx \sim p}\left[ \sqrt{\sum_{i:\bx_i=-1} \left(\dfrac{(p(\bx^{(i\to-1)}) - p(\bx^{(i\to1)}))^+}{p(\bx^{(i\to-1)})} \right)^2} \right].
    \end{align*}
    Furthermore, $p(\bx^{(i\to-1)}) - p(\bx^{(i\to1)}) \geq 0$ implies $p(\bx^{(i\to-1)}) + p(\bx^{(i\to 1)}) \leq 2 p(\bx^{(i\to-1)})$. So we may lower bound
    \begin{align}
        \frac{c_0 \eps}{4\sqrt{\log n}} \leq \Ex_{\bx \sim p}\left[ \sqrt{\sum_{i:\bx_i=-1} \left(\dfrac{(p(\bx^{(i\to-1)}) - p(\bx^{(i\to1)}))^+}{p(\bx^{(i\to-1)}) + p(\bx^{(i\to1)})} \right)^2} \right] \label{eq:exp1}
    \end{align}
    Notice that the maximum quantity within the expectation in (\ref{eq:exp1}) is $\sqrt{n}$, since each of the terms being added is between $0$ and $1$. Therefore, there must exist some $\gamma \in \{0,\dots, h \}$ with $h = \lceil \log_2(4\sqrt{n}/(c_0\eps))\rceil + 1 = O(\log(n/\eps))$ which satisfies
    \begin{align}
     \Prx_{\bx \sim p}\left[\sum_{i:\bx_i=-1} \left(\dfrac{(p(\bx^{(i\to-1)}) - p(\bx^{(i\to1)}))^+}{p(\bx^{(i\to-1)}) + p(\bx^{(i\to1)})} \right)^2 \geq \frac{c_0^2 \cdot \eps^2 \cdot 2^{2\gamma}}{16h\log n} \right] \geq \frac{1}{2^{\gamma}}. \label{eq:good-1}
    \end{align}
    Thus, consider any one of those values of $x$, and in order to simplify the notation, we define 
    \[ \xi \eqdef \frac{c_0^2 \cdot \eps^2 \cdot  2^{2\gamma}}{16h\log n} \qquad \nu_i = \left(\dfrac{(p(x^{(i\to-1)}) - p(x^{(i\to1)}))^+}{p(x^{(i\to-1)}) + p(x^{(i\to1)})} \right)^2,\]
    so that we assume to fix $x$ such that $\sum_{i=1}^n \nu_i \geq \xi$, and each $\nu_i \in [0, 1]$. Consider a partition of the coordinates of $[n]$ into groups $B_1, \dots, B_{r}$, such that $i \in B_{\ell}$ whenever the $i$-th coordinate contributes between $\xi 2^{\ell}/ n$ and $\xi 2^{\ell+1} / n$, and $r$ is chosen is the value $\xi 2^{r+1} / n$ is between $1$ and $2$ (note that, since $\nu_i \in [0, 1]$, $B_{r'}$ for $r' > r$ must be empty), so $r = O(\log(n/\xi))$. Then, there must be some $\ell$ with $|B_{\ell}| \geq n / (r \cdot 2^{\ell+1})$, and this implies
    \begin{align}
        \Prx_{\bi \sim [n]}\left[\left(\dfrac{(p(\bx^{(\bi\to-1)}) - p(\bx^{(\bi\to1)}))^+}{p(\bx^{(\bi\to-1)}) + p(\bx^{(\bi\to1)})} \right)^2 \geq \frac{\xi \cdot 2^{\ell}}{n}  \right] \geq \frac{1}{r \cdot 2^{\ell}}.\label{eq:good-2}
    \end{align}
    The desired bound then follows from the setting of $\eta$, and lower bounding the probability that $\bx \sim p$ satisfies the event of (\ref{eq:good-1}), and then $\bi \sim [n]$ satisfies the event of (\ref{eq:good-2}).
\end{proof}

\subsection{The Real-Valued Directed Talagrand Inequality} \label{sec:tal}

We will prove a ``directed isoperimetric theorem" for real-valued functions. This is an important
tool used for the analysis of the monotonicity tester. We define notions of the
directed boundary for Boolean functions. 

Let $f:\{-1,1\}^n \to [0,1]$ be a function defined on the $n$-dimensional hypercube. The $L_1$-distance 
of $f$ from monotonicity is defined as
\begin{equation*}
	\dist_1(f) \eqdef \min_{g~:~\text{monotone}} ~~\Ex_{\bx \sim \{-1,1\}^n}\left[ |f(\bx) - g(\bx)| \right]
\end{equation*}
where the expectation is over the uniform distribution over $\{-1,1\}^n$.
For a point $x\in \{-1,1\}^n$, define the directed derivative $\grad^-f(x)$ to be the $n$-dimensional vector defined as 
\begin{equation}\label{eq:defgrad}
	\left(\grad^-f(x)\right)_i \eqdef \begin{cases}
		0 & \textrm{if $x_i = 1$} \\
		\left(f(x) - f(x+2e_i)\right)^+ & \text{otherwise}
	\end{cases}
\end{equation}
where $(z)^+$ is a shorthand for $\max(z,0)$. For Boolean-valued $f:\{-1,1\}^n \to \{0,1\}$, the distance $\dist_1(f)$ corresponds to the ``normal'' Hamming distance notion, $\dist_0(f)$.
Based on isoperimetric theorems of Talagrand~\cite{Tal93}, the quantity $\Exp_{\bx} \norm{\grad^-f (\bx)}_2$ can be thought
of as a ``directed surface area" for the function $f$. A deep isoperimetric theorem of Khot, Minzer, and Safra~\cite{KMS18} (see, also~\cite{PRW22}, who showed how to remove the final logarithmic factor)
lower bounds this surface area by the distance to monotonicity.

\begin{theorem}[\cite{KMS18, PRW22}]\label{thm:booliso}
	There exists a universal constant $C > 0$ such that for every $f \colon \{-1,1\}^n \to \{0,1\}$,
$\Exp_{\bx} \norm{\grad^-f (\bx)}_2 \geq C\cdot \dist_0(f)$.
\end{theorem}

Theorem~\ref{thm:l1-talagrand} gives a real-valued generalization of the above theorem, with a $\sqrt{\log n}$ loss in the bound. The proof appears in Subsection~\ref{sec:proof-tal}, but we state the following corollary used in the tester's analysis.
\begin{corollary} \label{cor:l1-tal} Let $p$ be a distribution over $\{-1,1\}^n$ that is $\eps$-far
from monotone. Then 
\[
\sum_{x \in \{-1,1\}^n} \sqrt{\sum_{i:x_i = -1} \left( \left(p(x^{(i\to-1)}) - p(x^{(i\to1)} \right)^+ \right)^2} = \Omega\left(\frac{\eps}{\sqrt{\log n}}\right).
\]
\end{corollary}

\begin{proof} Let $\eps(p)$ be the distance of $p$ to monotonicity. Note that this is the distance
over distributions, while \Thm{l1-talagrand} refers to $L_1$-distance between arbitrary functions.
So we need an extra calculation to apply \Thm{l1-talagrand}.

Let $\cM$ be the set of monotone distributions. Then, $\eps(p) = \min_{q \in \cM} \dtv(p,q)
= \min_{q \in \cM} \|p-q\|_1/2$. On the other hand, $\dist_1(p) = \min_{g: \textrm{monotone}} \Exp_{\bx}|p(\bx) - g(\bx)|
= 2^{-n} \min_{g: \textrm{monotone}} \|p-g\|_1$. Note that the minimizer $g$ is non-negative, since $p$
is non-negative. Hence, the function $f = g/\|g\|_1$ is a distribution. 

By triangle inequality,
\begin{eqnarray*}
\eps(p) \leq \|p - f\|_1 \leq \|p-g\|_1 + \|f-g\|_1 = \|p-g\|_1 + \|g - g/\|g\|_1 \|_1
\end{eqnarray*}
Observe that $\|g - g/\|g\|_1 \|_1 = \sum_x |g(x) - g(x)/\|g\|_1| = |1 - 1/\|g\|_1| \cdot \sum_x |g(x)|
= | 1 - \|g\|_1|$. Since $p$ is a distribution, this expression is equal to $| \|p\|_1 - \|g\|_1|$.
And finally, $| \sum_x (|p(x)| - |g(x)|)| \leq \sum_x |p(x) - g(x)| = \|p-g\|_1$. 
Overall, we deduce that $\eps(p) \leq 2\|p-g\|_1$. Recall that $\dist_1(p)$ is defined
using an expectation over the domain, so $\eps(p) \leq 2 \cdot 2^n \dist_1(p)$. 

With our lower bound for $\dist_1(g)$, we can apply \Thm{l1-talagrand}. 
So $\Exp_{\bx} \norm{\grad^-p(\bx)} = \Omega(\dist_1(p)/\sqrt{\log n}) = \Omega(2^{-n} \eps(p)/\sqrt{\log n})$.
We expand out the expression for $\grad^-p(x)$ to wrap up the proof.
\begin{eqnarray*}
\Ex_{\bx\sim\{-1,1\}^n}\left[ \norm{\grad^-p(\bx)} \right] = 2^{-n} \sum_{x \in \{-1,1\}^n} \norm{\grad^-p(x)}
= 2^{-n}  \sum_{x \in \{-1,1\}^n} \sqrt{\sum_{i:x_i = -1} \left( (p(x) - p(x+2e_i) )^+ \right)^2}
\end{eqnarray*}
As argued above, this expression is lower bounded by $\Omega(2^{-n} \eps(p)/\sqrt{\log n})$.
The $2^{-n}$ terms ``cancel out", and noting that $\eps(p) \geq \eps$, we get the desired bound.
%$\sum_{x \in \{-1,1\}^n} \sqrt{\sum_{i:x_i = -1} \left( \left(p(x^{(i\to-1)}) - p(x^{(i\to1)} \right)^+ \right)^2} = \Omega(\eps/\sqrt{\log n})$.
\end{proof}


\subsection{The proof of \Thm{l1-talagrand}} \label{sec:proof-tal}

By a simple translation and rescaling argument, we reduce the function range to $[0,1]$.
This will make subsequent calculations easier.

\begin{claim} \label{clm:rescale} Consider $f:\{-1,1\}^n \to \R$. For positive $\alpha \in \R^+$ and
any $\beta \in \R$, define the function $\hat{f}$ where $\hat{f}(x) = \alpha f(x) + \beta$. Then,
$\EX_x[\|\nabla^- \hat{f}\|_2]/\dist_1(\hat{f}) =\EX_x[\|\nabla^- f\|_2]/\dist_1(f) $.
\end{claim}

\begin{proof} The monotonicity violations in $f$ and $\hat{f}$ are identical.
For any point $x$ and coordinate $i$, $ (\nabla^- \hat{f}(x))_i = \alpha (\nabla^- f(x))_i$.
Hence, $\EX_x[\|\nabla^- \hat{f}\|_2] = \alpha \EX_x[\|\nabla^- f\|_2]$.
For a function $g$, let $\alpha g + \beta$ be the function whose value at $x$
is $\alpha g(x) + \beta$.
\begin{align*}
 \dist_1(\hat{f}) = \min_{\hat{g}: \textrm{monotone}} \|\hat{f}-\hat{g}\|_1 &= \min_{\hat{g}: \textrm{monotone}} \| (\alpha f + \beta) - \hat{g}\|_1
\min_{\hat{g}: \textrm{monotone}} \|(\alpha f + \beta) - (\alpha (\alpha^{-1}(\hat{g} - \beta)) + \beta)\|
\end{align*}
Monotonicity is preserved by positive scaling and translation, so $\hat{g}$ is monotone iff $(\alpha^{-1}(\hat{g} - \beta))$
is monotone.
Hence,
\begin{align*}
 \dist_1(\hat{f}) = \min_{g: \textrm{monotone}} \|(\alpha f + \beta) - (\alpha g + \beta)\|_1
= \min_{g: \textrm{monotone}} \|\alpha f - \alpha g\|_1 = \alpha \ \dist_1(f)
\end{align*}
We conclude that  
$\EX_x[\nabla^- \hat{f}\|_2]/\dist_1(\hat{f}) =\EX_x[\|\nabla^- f\|_2]/\dist_1(f) $.
\end{proof}

Given $f$, we technically work with the function $\hat{f} = f/2M + 1/2$,
where $M = \max_x |f(x)|$. Observe that $\hat{f}$ has range in $[0,1]$,
and by \Clm{rescale}, the statement of \Thm{l1-talagrand} for $\hat{f}$ implies
the statement for $f$.

Abusing notation, we just assume that $f:\{-1,1\}^n \to [0,1]$.
We use the technique of Berman, Raskhodnikova, and Yaroslavtsev~\cite{BeRaYa14} of using threshold Boolean functions to relate the real-valued $f$ to Boolean functions. 

\noindent
Given $t\in [0,1]$ consider the following Boolean function (Definition 2.1 in~\cite{BeRaYa14}) $f_t : \{-1,1\}^n \to \{0,1\}$
\[
	f_t(x) = \begin{cases}
		1 & \text{if}~ f(x) \geq t; \\
		0 & \text{if}~ f(x) < t
	\end{cases}
\]
\noindent
It is easy to see that for any $x\in \{-1,1\}^n$,
\[
	f(x) = \int_0^{f(x)} dt  =  \int_0^1 f_t(x)~dt ~= \Ex_{\bt \sim [0,1]} \left[ f_{\bt}(x) \right]
\]
where the expectation is over $t$ uniformly distributed over $[0,1]$.
One can perform analogous calculations to relate the $L_1$ distance of (the real valued)
$f$ to the $L_0$ distance of (the Boolean) $f_t$s.  

\begin{lemma}[Lemma 2.1~\cite{BeRaYa14}]\label{lem:bry}
	\noindent
	For any function $f:\{-1,1\}^n \to [0,1]$, $\dist_1(f) = \int_0^1 \dist_0(f_t) dt = \Exp_{\bt} \left[ \dist_0(f_{\bt})\right]$.
\end{lemma}

The main work is in relating the (directed) gradients of $f$ to the corresponding
gradients of $f_t$. This is where we suffer a $\sqrt{\log n}$ loss.
 
% One can also similarly check that for any $x\in \{0,1\}^n$,
% we have 
% \begin{equation}\label{eq:exp-grad}
% 	\grad^-f(x) = \Exp_t \grad^- f_t(x)
% \end{equation}
% In particular, since $\grad^-f(x)$ and $\grad^-f_t(x)$ are non-negative vectors, by linearity of expectation we get
% \begin{equation}\label{eq:l1-norm}
% 	\norm{\grad^-f(x)}_1 = \norm{\Exp_t \grad^- f_t(x)}_1 = \Exp_t \norm{\grad^- f_t(x)}_1
% \end{equation}
% BRY provide the following characterization
% 
% Using~\Lem{bry} and \eqref{eq:exp-grad}, one immediately obtains the Poincare version of~\Thm{booliso}:
% \[
% 	\dist_1(f) =  \Exp_t  \dist_0(f_t)  \underbrace{\leq}_{\Thm{booliso}} \frac{1}{C}\cdot \Exp_t \Exp_x \norm{\grad^- f_t(x)}_1 \underbrace{=}_{\eqref{eq:l1-norm}} \frac{1}{C}\cdot \Exp_x \norm{\Exp_t \grad^- f_t(x)}_1 \underbrace{=}_{\eqref{eq:exp-grad}}\frac{1}{C}\cdot \Exp_x \norm{\grad^- f(x)}_1
% \]
% ndent
% 
% Talagrand's inequality doesn't {\em immediately} follow because (as usual) ``Jensen is in the wrong direction''. \eqref{eq:exp-grad} and the fact that the $\ell_2$ norm is a convex function implies that $\Exp_t \norm{\grad^- f_t(x)}_2 \geq \norm{\Exp_t \grad^- f_t(x)}_2 = \norm{\grad^- f(x)}_2$ which is counter to what would've been nice. Nevertheless one can prove the following lemma which, using the same derivation as above, proves~\Thm{l1-talagrand}.
% 

\begin{lemma}
	For all $x\in \{-1,1\}^n$, $\norm{\grad^- f(x)}_2 = \Omega(1/\sqrt{\log n}) \Exp_t \norm{\grad^- f_t(x)}_2 $.	
\end{lemma}
\begin{proof}
Fix any $x \in \{-1,1\}^n$. Let $y_1, \ldots, y_d \in \{-1,1\}^n$ denote the ``up''-neighbors of $x$ which satisfy $f(x) > f(y_j)$. In particular, there are at most $d \leq n$ points $y_1 ,\dots, y_d$ such that, for every $j \in [d]$, $y_j = x + 2e_i$ for some $i$, and in addition, $f(x) > f(y_j)$.
%(so there exists some coordinate $i$ such that $y_j = x + 2e_i$).
%Note that $d\leq n$.
Order the indices so as to assume $f(y_1) \leq f(y_2) \leq \cdots \leq f(y_d)$ and let $a_j := f(x) - f(y_j)$ (and so $a_1 \geq a_2 \geq \cdots \geq a_d$). By definition, we have defined $a_1,\dots, a_d$ to have $\norm{\grad^- f(x)}_2 = (\sum_{j=1}^d a^2_j)^{1/2}$.

For $t \in [0, 1]$, consider the function $f_t$, and let edge $(x,y_j)$ be called a violation in $f_t$ if $f_t(x) = 1$ and $f_t(y_j) = 0$.
Observe that only violated edges contribute to $\norm{\grad^-f_t(x)}$. 
Notice that for any $t \in (f(y_i), f(y_{i+1})]$, the edge $(x,y_j)$ is a violation in $f_t$ iff $j \leq i$.
Hence, if $t \in (f(y_i), f(y_{i+1})]$, then the vector $\grad^-f_t(x)$ 
has exactly $i$ non-zeros and $\norm{\grad^- f_t(x)}_2 = \sqrt{i}$. For $i < d$, the probability that $t \in (f(y_i), f(y_{i+1})]$
is exactly $y_{i+1} - y_i = a_i - a_{i+1}$. The probability that $t \in (f(y_d), x]$
is exactly $a_d$.
% 	
% Next note that $\grad^-f_t(x)$ looks as follows:
% 	\begin{equation}\label{eq:gorilla}
% 		\grad^-f_t(x) =  \begin{cases}
% 			(0,0,\ldots, 0, 0) & \text{if} ~ 0\leq t\leq f(y_1) \\
% 			(1,0,\ldots, 0, 0) & \text{if} ~ f(y_1) <  t\leq f(y_2) \\
% 			(1,1,\ldots, 0, 0) & \text{if} ~ f(y_2) <  t\leq f(y_3) \\
% 			\vdots & \\
% 			(1,1,\ldots, 1, 0) & \text{if} ~ f(y_{d-1}) <  t\leq f(y_d) \\
% 			(1,1,\ldots, 1, 1) & \text{if} ~ f(y_d) <  t\leq f(x) \\
% 			(0,0,\ldots, 0, 0) & \text{if} ~ f(x) <  t\leq 1 \\
% 		\end{cases}
% 	\end{equation}
Thus,
\[
	\Ex_{\bt\sim[0,1]}\left[ 	\norm{\grad^- f_{\bt}(x)}_2 \right] = \sum_{i=1}^{d-1} \left(a_i - a_{i+1}\right)\sqrt{i} + a_d \sqrt{d} = \sum_{i=1}^d a_i \cdot \left(\sqrt{i} - \sqrt{i-1}\right)
\]
By Cauchy-Schwarz and the following calculation, we complete the proof
\begin{equation*}
	\Ex_{\bt\sim [0,1]}\left[ 	\norm{\grad^- f_{\bt}(x)}_2 \right]\leq \norm{\grad^- f(x)}_2 \cdot \sqrt{\sum_{i=1}^d \left(\sqrt{i} - \sqrt{i-1}\right)^2}\leq O(\sqrt{\log n})\cdot \norm{\grad^- f(x)}_2
\end{equation*}
since $\sqrt{i} - \sqrt{i-1} = 1/(\sqrt{i}+\sqrt{i-1}) \leq 1/\sqrt{i}$, and so $\sum_{i \leq d} (\sqrt{i} - \sqrt{i-1})^2 \leq \sum_{i \leq d} 1/i = O(\log d)$.
% 
% To see the above, note that
% \[
% (\sqrt{i}-\sqrt{i-1})^2 = (2i - 1) - 2\sqrt{i(i-1)} = 2i\cdot \left(1 - \left(1 - \frac{1}{i}\right)^{1/2}\right) - 1
% \]
% By Taylor approximation, we get that there exists constant $A > 0$ (indeed, $A = 1$ suffices) such that
% \[
% \left(1 - \frac{1}{i}\right)^{1/2} \geq 1 - \frac{1}{2i} - \frac{A}{i^2}
% \]
% \noindent
% and so substituting above we get that
% \[
% \sum_{i=1}^d \left(\sqrt{i} - \sqrt{i-1}\right)^2 \leq \sum_{i=1}^d \frac{2A}{i} = O(\log d)\qedhere
% \]
\end{proof}

We now complete the proof of \Thm{l1-talagrand}. By the above lemma,
\begin{align*}
\Ex_{\bx\sim\{-1,1\}^n}\left[ \norm{\grad^- f(\bx)}_2\right] &= \Omega(1/\sqrt{\log n}) \Ex_{\substack{\bx \sim \{-1,1\}^n \\ \bt \sim [0,1]}} \left[ \norm{\grad^- f_{\bt}(\bx)}_2\right] \\
    &= \Omega(1/\sqrt{\log n}) \Ex_{\bt \sim [0,1]} \left[  \Ex_{\bx\sim\{-1,1\}^n}\left[ \norm{\grad^- f_{\bt}(\bx)}_2 \right] \right].
\end{align*}
By the directed Boolean isoperimetric statement of \Thm{booliso}, $\Exp_{\bx} \norm{\grad^- f_t(\bx)}_2 = \Omega(\dist_0(f_t))$.
We apply this bound and then \Lem{bry} to relate back to $f$.
\begin{align*}
\Ex_{\bx\sim\{-1,1\}^n}\left[ \norm{\grad^- f(\bx)}_2 \right] = \Omega(1/\sqrt{\log n}) \Ex_{\bt\sim[0,1]} \left[\dist_0(f_{\bt})\right] = \Omega(1/\sqrt{\log n}) \cdot \dist_1(f)
\end{align*}
