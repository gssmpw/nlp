% !TEX root = main.tex

\section{Introduction}

Distribution testing is the study of algorithms for testing properties of probability distributions \cite{GR00, BFRSW00}. A distribution testing problem is specified by a class of distributions $\calP$ 
supported on a domain $\Sigma$.  The aim is to get low-complexity algorithms that distinguish an unknown probability distribution $p$ having the property (i.e., $p \in \calP$) 
from one that is ``far" from having the property. One hallmark result is an algorithm and a matching lower bound, showing that $\Theta(\sqrt{|\Sigma|} / \eps^2)$ independent samples are necessary and sufficient for testing whether $p$ is the uniform distribution~(see, the recent survey~\cite{C22} and references therein for an overview of the area). For most problems of interest, such a polynomial dependence on the support size $\Sigma$ is intrinsic. This makes classical distribution testing algorithms intractable for high-dimensional distributions, such as those supported on the hypercube $\Sigma = \{-1,1\}^n$, where the complexity becomes exponential in the dimension.

To circumvent this issue, Bhattacharrya-Chakraborty~\cite{BC18} and Canonne-Ron-Servedio~\cite{CRS15} introduced the \emph{subcube conditional model}
for distributions supported on $\{-1,1\}^n$. An algorithm can query the distribution $p$ with a subcube $\rho \in \{-1,1, *\}^n$, and receive an independent sample $\bx \sim p$ conditioned on $\bx_i = \rho_i$ for all $i$ where $\rho_i \neq *$. We highlight some appealing aspects of this model.

% In this paper, we study testing of high-dimensional distributions with subcube conditional sampling. First suggested and studied in~\cite{CRS15, BC18}, an algorithm receives subcube conditioning access to a distribution supported on $\{-1,1\}^n$; it can query We highlight three aspects of the subcube conditioning model we find particularly appealing:
\begin{flushleft}\begin{itemize}
\item For high-dimensional distributions, the subcube conditional model may provide an appropriate analogue to a ``membership query'' in learning theory, where distribution testing algorithms can overcome exponential dependencies in the dimension (e.g., see~\cite{CJLW21b, BLMT23}).
\item There is a growing body of work seeking to make high-dimensional distribution testing practical (particularly in the context of testing software that produces samples), where one can often implement more powerful query oracles, and in particular, the subcube conditional sampling oracle (e.g., see~\cite{MPC20, PM22,BCPSS24}).
\item Subcube conditioning lends itself to an elegant mathematical analysis, often leading to query complexities polynomial in the dimension (as opposed
    to domain size). The key to an efficient testing result often involves (approximately) determining a global property of the distribution while only estimating marginals after conditioning on (random) subcubes of the domain (e.g., see~\cite{CCKLW21, BCSV23, CM24, AFL24}).
\end{itemize}\end{flushleft}

In this paper, we focus on the classic property of monotonicity of distributions. We use $\preceq$ to denote the coordinate-wise partial order on $\{-1,1\}^n$ and use $p$ to denote the probability mass function.
A distribution $p$ supported on $\{-1,1\}^n$ is monotone if, for any pair $x, y \in \{-1,1\}^n$ with $x \preceq y$, 
$p(x) \leq p(y)$. 
Monotonicity arises naturally in many scenarios and is a desirable property from an algorithmic perspective~(e.g., \cite{BLMT23}). 
For any distribution $p$, we define the \emph{distance to monotonicity} as $\min_{q } \dtv(p,q)$, 
where the minimum is over all monotone distributions $q$ and $\dtv$ is the total variation distance between distributions. We say that $p$ is \emph{$\eps$-far from monotone}
if the distance to monotonicity is at least $\eps$.
A \emph{tester for monotonicity} gets access to a distribution $p$ and a proximity parameter $\eps \in (0,1)$. 
If $p$ is monotone, the tester accepts with probability $> 2/3$. If $p$ is $\eps$-far from monotone,
the tester rejects with probability $> 2/3$.
%{\color{red} Erik: This appears in the related work below. Perhaps it is okay to remove? In the standard model, a tester gets access to samples from the distribution $p$.
%In this setting, the complexity of monotonicity testing is close to $2^n$. 
%Currently, the best upper bound is $2^n/2^{\Theta_{\eps}(n^{1/5})}$~\cite{RV20}, and the best lower bound is $2^{(1 - \Theta(\sqrt{\eps}) - o(1)) n}$~\cite{AGPRY19}. 
%}
Our main question is: what is the query complexity of monotonicity testing of distributions over the hypercube $\{-1,1\}^n$, in the subcube conditional model?


% By a reduction to uniformity testing, monotonicity cannot be tested with fewer than $\Omega(2^{n/2}/\eps^2)$ samples~\cite{BKR04}, and a naive (via learning) upper bound uses $O(2^n/\eps^2)$ samples. Perhaps surprisingly, there are better upper and lower bounds~\cite{BFRV11}; 

% 
% 
% 
% In a parallel line-of-work, there has been immense progress on testing monotonicity of functions (see Chapter 2 of the thesis~\cite{B23b} for an overview of much of the recent development), with 

\subsection{Our Contributions} \label{sec:contributions}

\paragraph{Testing Monotonicity.} Our first results are a testing algorithm and a nearly-matching lower bound which shows the query complexity of testing monotonicity in the subcube conditional model is \emph{linear} in the dimension.

\begin{theorem}[Monotonicity Testing Upper Bound]\label{thm:intro-ub}
There is an algorithm for testing monotonicity of distributions over $\{-1,1\}^n$ that uses $\tilde{O}(n/\eps^2)$ subcube conditioning queries. The algorithm works in the weaker \emph{coordinate oracle} model~\cite{BCSV23}, where queries are only made on one-dimensional subcubes.
\end{theorem}
%Theorem~\ref{thm:intro-ub} gives an algorithm whose query complexity is exponentially better than what is possible using independent samples (i.e., linear-in-dimension, whereas exponential-in-dimension samples are necessary). 

%We complement our upper bound with a nearly matching lower bound. These results show that the complexity
%of monotonicity testing is $\Theta(n/\eps^2)$, up to logarithmic factors.

\begin{theorem}[Monotonicity Testing Lower Bound]\label{thm:intro-lb}
Any monotonicity tester of distributions using subcube conditioning must use $\tilde{\Omega}(n/\eps^2)$ queries.
\end{theorem}

We will overview the proofs of \Thm{intro-ub} and \Thm{intro-lb} shortly. Roughly speaking, the upper bound will follow from exploiting a connection between monotonicity testing and directed isoperimetric inequalities, and defining an ``edge tester'' for distribution testing. For the lower bound, we construct a pair of distributions over product distributions which will ``hide'' the negative biases. 

    \noindent\textbf{Directed Isoperimetry.} The key tool enabling \Thm{intro-ub} is a directed isoperimetric inequality for real-valued functions. As we expand on in Section~\ref{sec:related}, isoperimetric inequalities relate the surface area of a geometric object to its volume. In the hypercube, the volume of a subset $A \subset \{-1,1\}^n$ is its size and the surface area is a measure of edges ``crossing'' the set. \emph{Directed} isoperimetry is the phenomenon where a non-monotone function exhibits evidence of its non-monotonicity via a directed edge boundary (points where the function value decreases). Directed isoperimetric inequalities relate the ``non-monotone edge boundary'' (the measure of surface area) to the distance to monotonicity (the measure of volume). These inequalities have played a major role in testing monotonicity of functions. In this work, we use a real-valued version of a (Boolean-valued) directed isoperimetric inequality of Khot-Minzer-Safra~\cite{KMS18} (also~\cite{PRW22}, who remove a logarithmic factor), which may be of independent interest.\footnote{As we discuss in Section~\ref{sec:related}, real-valued versions of directed isoperimetric inequalities have been studied~\cite{BKR23}, although we will need one of a different form.} Namely, let $\dist_1(f)$ denote the $\ell_1$-distance to monotonicity of any 
    $f:\{-1,1\}^n\rightarrow \mathbb{R}$, i.e., the minimum over monotone functions $g \colon \{-1,1\}^n \to \R$ of $\Ex_{\bx}[|f(\bx) - g(\bx)|]$.
\begin{theorem}\label{thm:l1-talagrand}
	For any $f:\{-1,1\}^n \to \R$, we have
 \[
	\Ex_{\bx \sim \{-1,1\}^n}\left[  \left( \sum_{i:\bx_i=-1} \left(\big(f(\bx) - f(\bx^{(i)})\big)^+\right)^2 \right)^{1/2}  \right] \geq \Omega\left(\frac{1}{\sqrt{\log n}}\right)\cdot \dist_1(f).
	\]
\end{theorem}
(We use $(z)^+$ as shorthand for $\max(z,0)$, and for $x \in \{-1,1\}^n$, $x^{(i)} \in \{-1,1\}^n$ refers to the point that agrees with $x$ on all but the $i$-th coordinate.)
The proof of this theorem is obtained by a reduction to the Boolean case, using a thresholding technique of Berman-Raskhodnikova-Yaroslavstev~\cite{BeRaYa14}. 
We note that this theorem answers a open question in~\cite{F23} 
(the question mark in Table 1, which asks for an $(L^1, \ell^2)$-Poinc\'{a}re theorem)).
It is an interesting open problem to determine whether the dependence on $n$ in \Thm{l1-talagrand} can be removed (no such dependence exists for the Boolean valued case).


%The main idea behind the proof is to detect biases in product distributions. (We explain in depth later.) We observe
%that this idea can be used to get lower bounds for other problems as well. 
\noindent\textbf{Uniformity of Monotone Distributions.} We then turn our attention to a problem introduced by Rubinfeld-Servedio, where they seek distribution testing algorithms under the promise that the underlying distributions are monotone~\cite{RS09}. %they test uniformity under the promist that the underlying distribution is monotone~\ref{RS09}. % the promise that the input distribution is monotone~\cite{RS09}. They study the classic problem
%of uniformity testing, and 
They study the classic problem of uniformity testing, and show that $\tilde{O}(n/\eps^2)$ independent samples suffice (a significant improvement over the exponential sample lower bounds). %We note that this is in the standard model,
%and a significant improvement over the $O(2^{n/2}/\eps^2)$ complexity without an input assumptions.
With subcube conditioning, one would hope for further significant improvements. In particular, uniformity testing (without any assumption on the distribution) can be done in $\tilde{O}(\sqrt{n}/\eps^2)$ queries~\cite{CCKLW21}; does the complexity decrease further when the input distribution is promised to be monotone?
%Could we get an improvement if the input is promised to be monotone? 
We prove a lower bound, showing that uniformity testing of monotone distributions requires $\tilde{\Omega}(\sqrt{n}/\eps^2)$ subcube conditioning queries, i.e., it is equally hard for general and monotone distributions. % in the subcube conditional
%model is equally hard even for monotone distributions.

\begin{theorem}[Testing Uniformity of Monotone Distributions]\label{thm:uni-of-mon}
Testing uniformity of monotone distributions requires $\tilde{\Omega}(\sqrt{n}/\eps^2)$ subcube conditioning queries.
\end{theorem}

\ignore{\paragraph{Directed Isoperimetric Theorems.} One of the key tools for the upper bound of \Thm{intro-ub} is
a \emph{directed isoperimetric theorem} for real-valued functions. The problem of property testing monotonicity
of functions over $\{-1,1\}^n$ has seen a rich history of results. 
Much of the progress is due to a connection with directed isoperimetric inequalities~\cite{GGLRS00, ChSe13-j, KMS15, BCS23, BKKM23, F23, BKR23, F24}. 
Directed isoperimetry is the phenomenon that a function that is far from monotone must exhibit evidence via a directed boundary (points where the function value decreases)
The directed isoperimetric inequalities ensure that various functionals of the directed boundary are large whenever the function is far from monotone.
Our proof of \Thm{intro-ub} shows an interesting use of directed isoperimetry for distribution testing.

We need to prove a new directed isoperimetric theorem, which is of independent interest. 
We state some notation to formally state this theorem.

Let $f:\{-1,1\}^n \to [0,1]$ be a function defined on the $n$-dimensional hypercube. The $L_1$-distance 
of $f$ from monotonicity is defined as
\begin{equation*}
	\dist_1(f) \eqdef \min_{g~:~\text{monotone}} ~~\Ex_{\bx \sim \{-1,1\}^n}\left[ |f(\bx) - g(\bx)| \right]
\end{equation*}
where the expectation is over the uniform distribution over $\{-1,1\}^n$.
For a point $x\in \{-1,1\}^n$, define the directed derivative $\grad^-f(x)$ to be the $n$-dimensional vector defined as 
\begin{equation}\label{eq:defgrad}
	\left(\grad^-f(x)\right)_i \eqdef \begin{cases}
		0 & \textrm{if $x_i = 1$} \\
		\left(f(x) - f(x+2e_i)\right)^+ & \text{otherwise}
	\end{cases}
\end{equation}
where $(z)^+$ is a shorthand for $\max(z,0)$. For Boolean-valued $f:\{-1,1\}^n \to \{0,1\}$, the distance $\dist_1(f)$ corresponds to the ``normal'' Hamming distance notion, $\dist_0(f)$.
Based on isoperimetric theorems of Talagrand~\cite{Tal93}, the quantity $\Exp_{\bx} \norm{\grad^-f (\bx)}_2$ can be thought
of as a ``directed surface area" for the function $f$. A deep isoperimetric theorem of Khot, Minzer, and Safra~\cite{KMS15} (see, also~\cite{PRW22}, who showed how to remove the final logarithmic factor)
lower bounds this surface area by the distance to monotonicity.

\begin{theorem}[\cite{KMS15, PRW22}]\label{thm:booliso}
	There exists a universal constant $C > 0$ such that for every $f \colon \{-1,1\}^n \to \{0,1\}$,
$\Exp_{\bx} \norm{\grad^-f (\bx)}_2 \geq C\cdot \dist_0(f)$.
\end{theorem}

We prove a real-valued generalization of this statement, with a $\sqrt{\log n}$ loss in the bound.
\begin{theorem}\label{thm:l1-talagrand}
	For any $f:\{-1,1\}^n \to [0,1]$, 
 \[
	\Ex_{\bx \sim \{-1,1\}^n}\left[  \norm{\grad^-f (\bx)}_2 \right] \geq \Omega\left(\frac{1}{\sqrt{\log n}}\right)\cdot \dist_1(f).
	\]
\end{theorem}

The proof of this theorem is obtained by a reduction to the Boolean case of \Thm{booliso}, using a thresholding
technique of Berman-Raskhodnikova-Yaroslavtsev~\cite{BeRaYa14}. 
We note that this theorem answers an open question in~\cite{F23} 
(the question mark in Table 1, which asks for an $(L_1, \ell_2)$-Poinc\'{a}re theorem)).
An interesting open problem is to remove
the dependence on $n$ from \Thm{l1-talagrand}.}

\subsection{Main Ideas} \label{sec:ideas}
We give a short summary of the main ideas behind our results. 

\noindent\textbf{Monotonicity Testing Upper Bound.} The algorithm behind \Thm{intro-ub} is an ``edge tester" for distribution testing. We take a sample $\bx \sim p$ and choose a direction $\bi \sim [n]$ uniformly at random, and we consider the distribution $p$ conditioned on $\smash{\{\bx, \bx^{(\bi)}\}}$, or equivalently, on the 1-dimensional subcube $\brho$ which is $\bx$ except for $\brho_{\bi} = *$. Notice that the conditional distribution is supported on one bit $\{-1,1\}$. For our algorithm, it suffices to condition on 1-dimensional subcubes, which have been studied under the name \emph{coordinate oracles}~\cite{BCSV23}. In a monotone distribution, $1$ is more likely to appear than $-1$, so if the algorithm can detect (by few independent samples from the subcube) that the probability of $-1$ is larger than that of $1$, it can safely output ``reject'' (meaning non-monotone).
%random point $x$ from the distribution $p$,
%and pick a uar hypercube edge incident to $x$. The distribution restricted to this edge is a Bernoulli,
%and subcube conditional queries allow for sampling of this Bernoulli.
%The algorithm tries to estimate the Bernoulli's bias; if the bias of edge $e$ is $\mu(e)$,
%we required $\Theta(1/\mu(e)^2)$ to determine it. The algorithm reject is the bias
%is negative, since it is a sign of non-monotonicity.
The challenge is understanding, when $p$ is $\eps$-far from monotone, how likely it is that an ``edge'' $\smash{\{ \bx, \bx^{(i)}\}}$ is biased toward $-1$, and is this bias detectable from few samples. 
%when $p$ is $\eps$-far from monotone. 
%The challenge is to understand how many (and how much) edges are (significantly) negatively biased, when $p$
%is far from monotone. 

The connection to directed isoperimetry then becomes clear:
considering the case when $p$ is \mbox{$\eps$-far} from monotone, we apply \Thm{l1-talagrand} on the probability mass function $p$, where $\dist_1(p)$ can be~shown to be $\Omega(\eps)$ (Corollary~\ref{cor:l1-tal}). Then, the left-hand side of \Thm{l1-talagrand} can be used to establish (via an averaging argument and an ``importance sampling'' trick) how large the bias toward $-1$ will be on a random draw of $\bx \sim p$ and $\bi \sim [n]$ (in the proof of Lemma~\ref{lem:far-case-reject}). We emphasize that, even though we seek lower bounds on the biases of individual edges, which a (simpler) directed Poincar\'{e} inequality would seemingly handle (namely, Theorem~1.3 in~\cite{F23}), such an argument only gives a weaker $O(n^2/\eps^2)$ complexity. The reason is the following: directed Poincar\'{e} lower bounds the sum of $(p(x) - p(x^{(i)}))^+$ across all edges $\{x, x^{(i)}\}$, and would imply an edge-wise bias of $\eps / n$ (since there are $n 2^n$ edges); however, the query complexity of detecting bias is inverse {\em quadratic} in the bias. On the other hand, \Thm{l1-talagrand} rules out such situations: if all edges had negative bias $\eps / n$, $p$ would be $\tilde{O}(\eps / \sqrt{n})$-close to monotone. 

\ignore{The negative
biased edgs are like a measure of ``surface area". The problem was basic isoperimetric theorems, like the Poincar\'{e}
inequality, is that they only bound the total sum of negative biases over all edges. The query complexity
is inverse \emph{quadratic} in the biases, so for an optimal complexity bound, we need to understand how
the negative bias is spread over the edges. We would prefer if this biased is concentrated on a smaller set 
of edges. Remarkably, the directed Talagrand inequality of \Thm{l1-talagrand} precisely asserts that. If the total
negative bias is small (close to minimum possible for $\eps$-far distributions), then this bias is concentrated
on a small set of edges. This ensure that, on sampling such an edge, the bias can be determined quickly.
Combining with a logarithmic search over bias values, one can prove that the edge tester works in $\tilde{O}(n/\eps^2)$ samples.}

\noindent\textbf{Monotonicity Testing Lower Bound.} Our starting point is to focus on proving sample-complexity lower bounds for testing the analogous problems (namely, monotonicity and uniformity of monotone distribution) in the restricted setting of \emph{product distributions}. This is a significant restriction from general distributions\footnote{Product distributions over $\{-1,1\}^n$ are fully specified by their mean vector, so in order to describe a product distribution, it suffices to specify the $n$ values of $\mathbb{E}_{\bx \sim p}[\bx_i]$. General distributions over $\{-1,1\}^n$, on the other hand, lie in the convex hull of $\{ e_i : i \in [2^n]\}$ and require $2^{n}-1$ numbers to specify.}, and these immediately imply general query lower bounds for subcube conditioning (Lemma~\ref{lem:tree-to-iid}). In particular, conditioning on subsets of coordinates (i.e., subcubes) do not change the distribution at all if coordinates were independent to begin with. The surprising aspect is that these lower bounds on product distributions will turn out to be nearly optimal (this was also the case in~\cite{CCKLW21, CJLW21b}).

Let us first focus on the monotonicity testing lower bound (\Thm{intro-lb}), where we use Yao's minimax
principle. Since a product distribution is fully specified by its mean vector, we construct a pair of distributions $\calA$ and $\calB$ supported on $[-1,1]$; and let the mean vector (and hence product distribution) have, for each $i \in [n]$, the $i$-th coordinate given by the $i$-th draw from $\calA$ or $\calB$ (Lemma~\ref{lem:one-dim-dist}). $\calA$ will always output non-negative numbers (so the corresponding product distribution will always be monotone), while $\calB$ will be $-\eps / \sqrt{n}$ with constant probability (so the corresponding product distribution will be $\Omega(\eps)$-far from monotone). The key trick is to design $\calA$ and $\calB$ in such a way so as to have their first $\log n / \log \log n$ moments being identical; in Section~\ref{sec:indist} we show how it implies that $\tilde{\Omega}(n/\eps^2)$ samples are needed to distinguish product distributions they produce.
The matching moments technique for lower bounds has been used before~\cite{RRSS09, V11}. Most recently, it was used to prove lower bounds for subcube conditioning for testing and learning $k$-juntas~\cite{CJLW21b}.

%We carefully construct two ``mean distributions" to have support in $[-1/\sqrt{n}, 1/\sqrt{n}]$,
%but only one of them (the ``no" distribution) take negative values. We sample from these one-dimensional distributions to determine the mean
%along each dimension. Using the equivalence of moments of these mean distributions, we can prove
%that $o(n)$ samples from the yes/no distributions look identical, yielding the lower bound.

\noindent\textbf{Uniformity of Monotone Distributions.} The lower bound on testing uniformity of monotone distributions also proceeds by Yao's minimax principle. We note that, the fact that testing uniformity of product distributions required $\Omega(\sqrt{n}/\eps^2)$ samples was known~\cite{CDKS17}; however, the examples to obtain the lower bound were non-monotone (each coordinate behaves independently with bias set to $\pm \eps / \sqrt{n}$). In our construction, each of the $n$ coordinates is biased with probability $1/\sqrt{n}$, but its bias becomes $\eps / n^{1/4}$ (Section~\ref{sec:dno-def}). Note that all biases are positive, so the resulting distribution is monotone. 
%
%To prove the lower bound for uniformity testing for monotone distributions, we use the same idea
%of choosing means of product distributions. The ``yes" distribution in this case is simply the uniform distribution.
%The ``no" distribution has small positive bias in some randomly chosen coordinates. The lower bound
%would be obtained by showing that $o(\sqrt{n})$ samples are distributed nearly identically in both
%cases. 
In order to prove indistinguishability, we reduce to the corresponding problem under Gaussian distributions (instead of bits). The calculation
boils down to the behavior of sum of exponentials of Gaussians, which can be determined by the (closed form) expression for the moment generating function (Section~\ref{sec:proof-mono-lb}).

% 
% 
% Perhaps surprisingly, Theorem~\ref{thm:intro-lb} follows from a sample-complexity lower bound for testing monotonicity of product distributions. We give two distributions, one of which is supported on $n$ independent Rademacher random variables with a positive bias (the monotone case), and the other also supported on $n$ independent Rademacher random variables which are collectively $\eps$-far from monotone. A (general) algorithm which uses subcube conditioning may adaptively interact with the distribution; however, if one is promised the input distribution is product, subcube conditioning queries may always be simulated by independent samples (Lemma~\ref{lem:tree-to-iid}). This enables us to argue about sample-complexity and automatically obtain (adaptive) query lower bounds. 
% 
% Finally, we consider a problem from~\cite{RS09}, which overcomes the exponential sample complexity of testing by limiting the power of the adversary. They want to test uniformity over $\{-1,1\}^n$ and are promised that an adversary can only produce monotone distributions. Given the promise that the input distribution is monotone, they show how to test uniformity with $\tilde{O}(n/\eps^2)$ samples, and show that $\tilde{\Omega}(n)$ samples are necessary for constant $\eps$.
% \begin{theorem}[Testing Uniformity of Monotone Distributions]\label{thm:uni-of-mon}
% Testing uniformity of monotone distributions requires $\tilde{\Omega}(\sqrt{n}/\eps^2)$ subcube conditioning queries.
% \end{theorem}
% Theorem~\ref{thm:uni-of-mon} is nearly-optimal, as it matches the $\tilde{O}(\sqrt{n}/\eps^2)$ uniformity testing upper bound~\cite{CCKLW21} (which does not need an assumption of monotonicity). In particular, it implies that assuming the underlying distribution is monotone does not improve the complexity of uniformity testing. As above, we prove the lower bound by showing a corresponding sample-complexity lower bound with product distributions.
% 
% 
% As we will discuss in the technical overview, the algorithm is an ``edge tester'' for distribution testing. We repeatedly sample a random $1$-dimensional subcube, specified by $\brho \in \{-1,1,*\}^n$ with a single $*$, and consider the distribution conditioned on $\brho$ which is supported on a single bit. If the distribution is monotone, the final bit is $1$ more often than it is $-1$; however, if the algorithm sees evidence that $-1$ appears more often than $1$, it may safely reject. We note that in testing of Boolean functions, the edge tester of~\cite{GGLRS00} relies on a (weaker) directed isoperimetric result, a so-called directed Poincare-like inequality, but that in this work, the Talagrand-like inequality is needed~\cite{KMS15} (the weaker inequality would result in an $\tilde{O}(n^2/\eps^2)$-query tester). In order to apply these inequalities to distributions, we first generalize the directed isoperimetric inequality, from functions mapping $\{-1,1\}^n \to \{0,1\}$ to functions $f \colon \{-1,1\}^n \to [0,1]$ (see, Theorem~\ref{thm:l1-talagrand} in Subsection~\ref{sec:tal}).


% 
% 
% \section{Preliminaries}\label{sec:prelims}
% 
% \newcommand{\Mono}{\textsc{Mono}}
% 
% Subcube conditional query access, first suggested in \cite{CRS15} and studied in \cite{BC18}, allows algorithms to specify a subcube of $\{-1,1\}^n$ and request a sample from the distribution \emph{conditioned} on the sample lying in the subcube specified. Equivalently, to request samples after fixing some of their variables. The operation is akin to the notion of \emph{restrictions} in the analysis of Boolean functions. Specifically, we identify the distribution by its probability mass function $p \colon \{-1,1\}^n \to \R_{\geq 0}$. An algorithm may then specify a subcube by a string $\rho \in \{-1,1, *\}^n$, where $*$'s denote free variables and non-$*$'s denote the values of restricted variables. Calling the oracle on such a $\rho$ results in a sample from the distribution $p_{|\rho}$ (now supported on $\{-1,1\}^{\stars(\rho)}$) given by restricting the function $\smash{p_{|\rho} \colon \{-1,1\}^{\stars(\rho)} \to \R_+}$ and re-normalizing it, so that it represents a distribution $p_{|\rho}$.\footnote{When conditioning on a subcube with zero support, one may consider models where the oracle returns a uniform sample \cite{CFGM16} or outputs ``error'' \cite{CRS15}. We note that our algorithm will never run into this scenario.}
% 
% \begin{definition}[Monotone Distributions]\label{def:mon-dist}
% For $n \in \N$, a distribution $p$ supported on $\{-1,1\}^n$ is monotone if the probability mass function $p \colon \{-1,1\}^n \to \R_{\geq 0}$ is monotone with respect to the partial order on $\{-1,1\}^n$. Namely, $p(x) \leq p(y)$ whenever $x_i \leq y_i$ for all $i \in [n]$. We let
% \begin{align*}
% \Mono = \left\{ p \colon \{-1,1\}^n \to \R_{\geq 0} : p \text{ is a monotone distribution }\right\}.
% \end{align*}
% For any $\eps \in (0, 1)$, and any property $\calP$ of distributions, we say that a distribution $p$ is $\eps$-far from $\calP$ if for all $g \in \calP$, $\dtv(p,g) \geq \eps$. 
% \end{definition}
% 
% \begin{definition}[Testing Monotonicity of Distributions]
% For any $n \in \N$ and $\eps \in (0, 1)$, a randomized algorithm is an $\eps$-test for monotonicity if it receives as input subcube conditioning access to an unknown distribution $p$ supported on $\{-1,1\}^n$.
% \begin{itemize}
% \item If $p \in \textsc{Mono}$, the algorithm should output ``accept'' with probability at least $2/3$, and
% \item If $p$ is $\eps$-far from \textsc{Mono}, the algorithm should output ``reject'' with probability at least $2/3$. 
% \end{itemize}
% \end{definition}
% 
% \paragraph{Lower Bounds for Subcube Conditioning.} 
% 

\subsection{Related Work}\label{sec:related}

%Directed Isoperimetry Results 
As mentioned above, directed isoperimetry theorems have been  crucial to the development of testers for monotonicity of Boolean functions~\cite{GGLRS00,ChSe13-j,KMS18} (as opposed to monotonicity of distributions). In particular, as we illustrate in~\Thm{booliso}, the strongest known directed isoperimetry theorem for Boolean functions due to~\cite{KMS18} (and slightly improved by~\cite{PRW22}) relates the expected $\norm{\grad^-f(x)}_2$ to the $\ell_0$-distance of $f$ from monotonicity, that is, the fraction of domain points at which $f$ must be modified to make it a monotone function. Our result,~\Thm{l1-talagrand}, is an ``$\ell_1$-version'' of the above statement for {\em real-valued} functions over the Boolean hypercube. 

The most relevant works to \Thm{l1-talagrand} are the directed isoperimetry theorems initiated by Pinto Jr.~\cite{F23, F24} who considers smooth functions $f:[0,1]^n \to \RR$. In~\cite{F23}, Pinto Jr. looks at the $\ell_1$-geometry and proves under a certain $\ell_1$-smoothness condition, the expected $\ell_1$-norm of the gradient is at least the $\ell_1$-distance of $f$ from monotonicity. In the subsequent paper~\cite{F24}, Pinto Jr. assumes $\ell_2$-smoothness and proves that the expected $\ell_2^2$-norm of the gradient is at least the square of the $\ell_2$-distance. Neither of these results imply or are implied by the Boolean setting of~\cite{KMS18} or our result,~\Thm{l1-talagrand}. As mentioned earlier, our result,~\Thm{l1-talagrand}, answers a question left open in~\cite{F23}.
Using the notation of that paper, we prove an $(L^1, \ell^2)$-Poinc\'{a}re theorem
for real-valued functions on the hypercube.


 In~\cite{BKR23}, Black, Kalemaj and Raskhodnikova consider Boolean functions $f:\{-1,1\}^n \to \mathbb{R}$ and they prove that the isoperimetry result of~\cite{KMS18} generalizes for such functions in the following sense. Instead of looking at the $\ell_2$-norm of the (directed) gradient $\grad^- f(x)$,~\cite{BKR23} considers the square-root of the ``negative influence'' at each $x$, where the ``negative influence'' counts the number of pairs which form a monotonicity violation with $x$. The {\em magnitude} of the violation is ignored. 
In this setting,~\cite{BKR23} proves that if a real-valued function is $\eps$-far from being monotone in the $\ell_0$-sense (which is usual in property testing), then the expected square-root of the negative influence is $\Omega(\eps)$ thereby generalizing~\cite{KMS18}. The authors use this result to give an $O(r\sqrt{d}/\eps^2)$-query non-adaptive tester for real-valued monotone functions, where $r$ is the cardinality of the image of $f$. Our directed isoperimetry result seems unrelated to their result, apart from the fact that both of our results are proved by reducing it to the Boolean case. 
Finally, in~\cite{BCS23}, Black, Chakrabarty and Seshadhri generalize the directed isoperimetry theorem of~\cite{KMS18} to Boolean functions defined over the {\em hypergrid}. %This is not relevant to our result, but our techniques would also imply a similar theorem as~\Thm{l1-talagrand} for $[0,1]$-range function defined over the hypergrid. 
%{\bf deepc: is the distribution testing question interesting here?}

\paragraph{Monotonicity Testing of Distributions.} Monotonicity of distributions has been studied extensively in the literature, in both low-dimensional and high-dimensional regimes~\cite{BKR04, RS09, ACS10, BFRV11,AGPRY19,RV20}. Batu, Kumar, and Rubinfeld initiated the study in~\cite{BKR04} and considered both regimes above. They described a 
tester for one-dimensional distributions (total orders) using $\tilde{O}_\eps(\sqrt{n})$-samples, and via a reduction to uniformity testing proved a tightness of this result. They also proved a $\Omega(m^{n/2})$-lower bound for distributions over $[m]^n$, and described algorithms with $\tilde{O}(m^{n - 0.5})$-samples. The one-dimensional result's dependency on $\eps$ was improved by~\cite{CDGR18} and the optimal algorithm for the low-dimensional regime was given by Acharya, Daskalakis and Kamath~\cite{ADK15} who gave a tester with sample-complexity $O\left(\frac{m^{n/2}}{\eps^2} + \frac{1}{\eps^2}\left(\frac{n\log m}{\eps^2}\right)^n\right)$. For the high-dimensional regime (which is of interest of this paper) of distributions over the hypercube $\{-1,+1\}^n$, one can get stronger lower bounds than ones found by reduction to uniformity testing: Aliakbarpour, Gouleakis, Peebles, Rubinfeld and Yodpinayee~\cite{AGPRY19} prove a lower bound of $2^{(1 - \Theta(\sqrt{\eps}) - o(1))n}$ on the sample complexity. The best upper bound is currently at $\smash{2^{n} / 2^{\Theta_{\eps}(n^{1/5})}}$ samples due to Rubinfeld and Vasilyan~\cite{RV20}.

\paragraph{Distribution Testing Beyond Sample-Complexity.} Many distribution testing problems over high-dimensional domains incur sample complexities which are exponential in the dimension. As a result, various works have sought models and techniques to overcome these lower bounds, which can be divided between those which assume structure on the input, and those which provide stronger access. Works assuming additional structure on the input include monotonicity~\cite{RS09}, low-degree Bayesian networks~\cite{CDKS17, DP17, ABDK18, DKP23}, Markov random fields~\cite{GLP18, DDK19, BBCSV20}, ``flat'' histogram structure~\cite{DKP19}, or structured truncations~\cite{DNS23, DLNS24}. On the other hand, the subcube conditional model follows the other approach on assuming stronger access.~\cite{BC18} was the first to obtain polynomial query complexities for various testing problems; for uniformity testing over $\{-1,1\}^n$,~\cite{CCKLW21} showed the complexity is $\tilde{\Theta}(\sqrt{n}/\eps^2)$~and~\cite{CM24} extended it to hypergrids. In this work, we use an approach of~\cite{CJLW21b} which studied subcube conditioning for testing and learning $k$-junta distributions (those which have at most $k$ non-uniform variables). Other accesses include (unrestricted) conditioning on the domain~\cite{CRS15,CFGM16} (see also, improvements~\cite{FJOPS15, ACK15, KT18, N21, CCK24, CCKM24}),  queries to the probability density function or cumulative distribution function~\cite{BDKR05, CR14}, conditioning on prefixes for hidden Markov models~\cite{MKKZ23}, and samples which reveal their probability~\cite{OS18}. 
