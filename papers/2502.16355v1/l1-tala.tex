\documentclass[11pt]{article}
\input{preamble}
\usepackage{times}


\begin{document}


%<*eindata>
\section{$L_1$-Distance to Monotonicity, Poincare, and Talagrand}
\hrule height 2pt
\vspace{2ex}
%Created : 16th July, 2024
%Modified:

%%%%%%%%%%%%%%%%%%%%%%%%%% Note specific preamble


%\begin{itemize}
	%\item 
	Let $f:\{0,1\}^n \to [0,1]$ be a function defined on the $n$-dimensional hypercube. The $L_1$-distance 
	of $f$ from monotonicity is defined as
	\begin{equation}
		\dist_1(f) := \min_{g~:~\text{monotone}} ~~\Exp_x |f(x) - g(x)|
	\end{equation}
	where the expectation is over the uniform distribution over $\{0,1\}^n$.
	For a point $x\in \{0,1\}^n$, define $\grad^-f(x)$ to be the $n$-dimensional vector defined as 
	\begin{equation}\label{eq:defgrad}
		\left(\grad^-f(x)\right)_i = \begin{cases}
			0 & \textrm{if $x_i = 1$} \\
			\left(f(x) - f(x+e_i)\right)^+ & \text{otherwise}
		\end{cases}
	\end{equation}
	where $(z)^+$ is a shorthand for $\max(z,0)$.

	When $f:\{0,1\}^n \to \{0,1\}$, $\dist_1(f)$ corresponds to the ``normal'' Hamming distance notion, $\dist_0(f)$, and 
	the following two are the ``directed analogues'' of Poincare and Talagrand isoperimetry theorems.
	
	\begin{theorem}\label{thm:booliso}
		Let $f:\{0,1\}^n \to \{0,1\}$. There exists a constant $C$ such that
		\begin{asparaenum}
			\item $\Exp_x \norm{\grad^-f (x)}_1 \geq C\cdot \dist_0(f)$ \cite{GGLRS00}
			\item $\Exp_x \norm{\grad^-f (x)}_2 \geq C\cdot \dist_0(f)$ \cite{KMS15,PRW22}
		\end{asparaenum}
	\end{theorem}
	\noindent
	Note that the second statement is stronger than the first.
	The objective of the note is to show similar statements for real valued functions with $\dist_0$ replaced by $\dist_1$ via a ``reduction'' to the Boolean range case using threshold Boolean functions introduced by Berman-Rashkhodnikova-Yaroslavtsev~\cite{BeRaYa14}. 
	\begin{thm}\label{thm:l1-talagrand}
		Let $f:\{0,1\}^n \to [0,1]$. Then, \[
		\Exp_x \norm{\grad^-f (x)}_2 \geq \Omega\left(\frac{1}{\sqrt{\log n}}\right)\cdot \dist_1(f)
		\]
	\end{thm}
	
	%\item 
	\noindent
	Given $t\in [0,1]$ consider the following Boolean function (Definition 2.1 in~\cite{BeRaYa14}) $f_t : \{0,1\}^n \to \{0,1\}$
	\[
		f_t(x) = \begin{cases}
			1 & \text{if}~ f(x) \geq t; \\
			0 & \text{if}~ f(x) < t
		\end{cases}
	\]
	\noindent
	It is easy to see that for any $x\in \{0,1\}^n$,
	\[
		f(x) = \int_0^{f(x)} dt  =  \int_0^1 f_t(x)~dt ~= \Exp_t f_t(x)
	\]
	where the expectation is over $t$ uniformly distributed over $[0,1]$. 
	One can also similarly check that for any $x\in \{0,1\}^n$,
	we have 
	\begin{equation}\label{eq:exp-grad}
		\grad^-f(x) = \Exp_t \grad^- f_t(x)
	\end{equation}
	In particular, since $\grad^-f(x)$ and $\grad^-f_t(x)$ are non-negative vectors, by linearity of expectation we get
	\begin{equation}\label{eq:l1-norm}
		\norm{\grad^-f(x)}_1 = \norm{\Exp_t \grad^- f_t(x)}_1 = \Exp_t \norm{\grad^- f_t(x)}_1
	\end{equation}
	BRY provide the following characterization
	\begin{lemma}(Lemma 2.1, \cite{BeRaYa14})\label{lem:bry}
		%
		\noindent
		For any function $f:\{0,1\}^n \to [0,1]$, $\dist_1(f) = \int_0^1 \dist_0(f_t) dt$
	\end{lemma}
\noindent
	Using~\Cref{lem:bry} and \eqref{eq:exp-grad}, one immediately obtains the Poincare version of~\Cref{thm:booliso}:
	\[
		\dist_1(f) =  \Exp_t  \dist_0(f_t)  \underbrace{\leq}_{\Cref{thm:booliso}} \frac{1}{C}\cdot \Exp_t \Exp_x \norm{\grad^- f_t(x)}_1 \underbrace{=}_{\eqref{eq:l1-norm}} \frac{1}{C}\cdot \Exp_x \norm{\Exp_t \grad^- f_t(x)}_1 \underbrace{=}_{\eqref{eq:exp-grad}}\frac{1}{C}\cdot \Exp_x \norm{\grad^- f(x)}_1
	\]
\noindent

	Talagrand's inequality doesn't {\em immediately} follow because (as usual) ``Jensen is in the wrong direction''. \eqref{eq:exp-grad} and the fact that the $\ell_2$ norm is a convex function implies that $\Exp_t \norm{\grad^- f_t(x)}_2 \geq \norm{\Exp_t \grad^- f_t(x)}_2 = \norm{\grad^- f(x)}_2$ which is counter to what would've been nice. Nevertheless one can prove the following lemma which, using the same derivation as above, proves~\Cref{thm:l1-talagrand}.

	
	\begin{lemma}
		For any $x\in \{0,1\}^n$, we have $\Exp_t \norm{\grad^- f_t(x)}_2 \leq O(\sqrt{\log n})\cdot \norm{\grad^- f(x)}_2$.	
	\end{lemma}
	\begin{proof}
		Fix an $x$. Let $y_1, \ldots, y_d$ be ``up''-neighbors of $x$ such that $f(x) > f(y_j)$. Note that $d\leq n$.
		Assume $f(y_1) \leq f(y_2) \leq \cdots \leq f(y_d)$ and let $a_j := f(x) - f(y_j)$ (and so $a_1 \geq a_2 \geq \cdots $).
		So,
		\[
			\norm{\grad^- f(x)}_2 = \sqrt{\sum_{i=1}^d a^2_i}
		\]
		Next note that $\grad^-f_t(x)$ looks as follows:
		\begin{equation}\label{eq:gorilla}
			\grad^-f_t(x) =  \begin{cases}
				(0,0,\ldots, 0, 0) & \text{if} ~ 0\leq t\leq f(y_1) \\
				(1,0,\ldots, 0, 0) & \text{if} ~ f(y_1) <  t\leq f(y_2) \\
				(1,1,\ldots, 0, 0) & \text{if} ~ f(y_2) <  t\leq f(y_3) \\
				\vdots & \\
				(1,1,\ldots, 1, 0) & \text{if} ~ f(y_{d-1}) <  t\leq f(y_d) \\
				(1,1,\ldots, 1, 1) & \text{if} ~ f(y_d) <  t\leq f(x) \\
				(0,0,\ldots, 0, 0) & \text{if} ~ f(x) <  t\leq 1 \\
			\end{cases}
		\end{equation}
		And so,
		\[
			\Exp_t 	\norm{\grad^- f_t(x)}_2 = \sum_{i=1}^{d-1} \left(a_i - a_{i+1}\right)\sqrt{i} + a_d \sqrt{d} = \sum_{i=1}^d a_i \cdot \left(\sqrt{i} - \sqrt{i-1}\right)
		\]
		Applying Cauchy-Schwarz and the following calculation, we complete the proof
		\begin{equation}
			\Exp_t 	\norm{\grad^- f_t(x)}_2 \leq \norm{\grad^- f(x)}_2 \cdot \sqrt{\underbrace{\sum_{i=1}^d \left(\sqrt{i} - \sqrt{i-1}\right)^2}_{= O(\log d) ~\text{explained below}}}
		\end{equation}
		To see the above, note that
		\[
		(\sqrt{i}-\sqrt{i-1})^2 = (2i - 1) - 2\sqrt{i(i-1)} = 2i\cdot \left(1 - \left(1 - \frac{1}{i}\right)^{1/2}\right) - 1
		\]
		By Taylor approximation, we get that there exists constant $A > 0$ (indeed, $A = 1$ suffices) such that
		\[
		\left(1 - \frac{1}{i}\right)^{1/2} \geq 1 - \frac{1}{2i} - \frac{A}{i^2}
		\]
		\noindent
		and so substituting above we get that
		\[
		\sum_{i=1}^d \left(\sqrt{i} - \sqrt{i-1}\right)^2 \leq \sum_{i=1}^d \frac{2A}{i} = O(\log d)\qedhere
		\]
	\end{proof}
%</eindata>

%If needed
\bibliographystyle{plain}
\bibliography{masterbib}

\end{document}
