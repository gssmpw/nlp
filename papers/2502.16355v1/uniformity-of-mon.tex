% !TEX root = main.tex

\section{Testing Uniformity of Monotone Distributions}

In this section, we prove the following theorem, which gives a query lower bound on testing uniformity of a distribution which is promised to be monotone using subcube conditioning queries. 
\begin{theorem}[Uniformity Testing of Monotone Distributions -- Lower Bound] For any $\eps > 0$,~any $\eps$-test for uniformity of distributions that are promised to be monotone must make $\tilde{\Omega}(\sqrt{n}/\eps^2)$ queries.
\end{theorem}
Similar to Section~\ref{sec:mon-lb}, we describe a distribution $\Dno$ supported on monotone product distributions over $\{-1,1\}^n$. Importantly, a distribution $\bp \sim \Dno$ will be $\Omega(\eps)$-far from the uniform distribution with probability $1-o_n(1)$ (Lemma \ref{lem:distancelemma}). Then, we show that for  $q$ which is at most $c \sqrt{n} / (\eps^2 \log^4 n)$, for a small enough constant $c > 0$, any function $\Alg \colon \{-1,1\}^{nq} \to \{\text{``accept''}, \text{``reject''} \}$ cannot output ``accept'' with probability at least $0.99$ when samples are drawn from the uniform distribution, and output ``reject'' with probability at least $0.99$ when samples are drawn from  $\Dno$.

\subsection{The distribution $\Dno$}\label{sec:dno-def}

A draw of $\bp \sim \Dno$ is generated as follows:
\begin{flushleft}\begin{itemize}
\item First, we let $\calD$ denote the distribution over vectors $\bmu$ where we 
independently set $\bmu_i$ to be $\eps/n^{1/4}$ with probability $1/\sqrt{n}$ and $0$ otherwise.
%sample a subset $\bN$ of size $\sqrt{n}$ uniformly at 
%random from $[n]$ and set $\bmu_i = 1 / n^{1/4}$ when $i\in \bN$ and $\bmu=0$ when $i\notin \bN$.}
  %{\color{red} and $\bmu_i=0$ otherwise}.
\item Then, we let $\bp$ be the monotone product distribution on $\{-1,1\}^n$ whose mean vector is $\bmu$.
\end{itemize}\end{flushleft}
The fact that $\bp \sim \Dno$ is far from the uniform distribution follows from the subsequent lemma.
\begin{lemma}\label{lem:distancelemma}
With probability at least $1 - o_n(1)$, $\bp \sim \Dno$ is $\Omega(\eps)$-far from the uniform distribution.
\end{lemma}
\begin{proof}
We consider a draw of $\bmu \sim \calD$, and note that with probability at least $1-o_n(1)$, $\bp \sim \Dno$ has a set $\bN \subset [n]$ of at least $\Omega(\sqrt{n})$ coordinates $i$ with $\bmu_i = \eps/n^{1/4}$. Fix such a draw and let $\bp$ denote the corresponding distribution. The total variation distance from $\bp$, generated with mean vector $\mu$, to the uniform distribution can be lower bounded by considering strings $x \in \{-1,1\}^n$ which have fewer $1$'s than $-1$'s in coordinates of $\bN$. 
For each such string $x$, letting $t(x)$ denote the number of coordinates in $\bN$ with $x_i = 1$, the probability of $x$ in $\bp$ is
$$
\frac{1}{2^n}\cdot \left(1 + \frac{\eps}{n^{1/4}} \right)^{t(x)} \left( 1 - \frac{\eps}{n^{1/4}}\right)^{|\bN| - t(x)} 
=\frac{1}{2^n}\cdot \left(1 - \frac{\eps^2}{\sqrt{n}}\right)^{t(x)} \left(1 - \frac{\eps}{n^{1/4}}\right)^{|\bN| - 2t(x)}< \frac{1}{2^n}.
$$
As a result, we can bounded $\dtv(\bp,\calU_n)$ from below as follows:
\begin{align}
\dtv(\bp, \calU_n) %&\geq \frac{1}{2^n} \sum_{\substack{x \in \{-1,1\}^n \\ t(x)\le |\bN|/2}} \left( 1 - \prod_{i=1}^n \left( 1 + x_i \cdot \mu_i \right)\right) \nonumber \\
     &\geq \frac{1}{2^n} \sum_{\substack{x \in \{-1,1\}^n \\ t(x) \leq |\bN|/2}} \left( 1 - \left(1 - \frac{\eps^2}{\sqrt{n}}\right)^{t(x)} \left(1 - \frac{\eps}{n^{1/4}}\right)^{|\bN| - 2t(x)}\right) \nonumber\\[0.5ex]
    &\ge
     \frac{1}{2^n} \sum_{\substack{x \in \{-1,1\}^n \\ t(x) \leq |\bN|/2}} \left( 1 -  \left(1 - \frac{\eps}{n^{1/4}}\right)^{|\bN| - 2t(x)}\right).\label{eq:hehe1}
\end{align}
%In (\ref{eq:simp-1}), we expanded the product over $i \in [n]$ using the definition of $t(x)$. Note that the fact $t(x) < |\bN| / 2$ implies $t(x) \leq |\bN| - t(x)$ and thus,
On the other hand, using $|\bN| = \Omega(\sqrt{n})$, there is a constant probability over a uniform $\bx \sim \{-1,1\}^n$ that $t(\bx)$ is bounded away from $|\bN|/2$ by $\Omega(n^{1/4})$.
For every such $\bx$, we have 
\[ %\left(1 + \frac{\eps}{n^{1/4}}\right)^{t(x)} \left(1 - \frac{\eps}{n^{1/4}}\right) = 
 \left(1 - \frac{\eps}{n^{1/4}}\right)^{|\bN| - 2t(\bx)} \leq \left(1 - \frac{\eps}{n^{1/4}}\right)^{\Omega(n^{1/4})}\le e^{-\Omega(\eps)}\le 1-\Omega(\eps). \]
Combining everything we have from (\ref{eq:hehe1}) that
$$
\dtv(\bp,\calU_n)\ge \frac{1}{2^n}\cdot \Omega(2^n)\cdot \Omega(\eps)=\Omega(\eps).
$$
This finishes the proof of the lemma.
%We now lower bound (\ref{eq:simp-1}) using the above inequality, were we have (\ref{eq:simp-%1}) is at least
%\[ \frac{1}{2^n} \sum_{x \in \{-1,1\}^n} \frac{\eps}{n^{1/4}} \left(|\bN| - 2t(x) \right)^+ %= \frac{\eps}{n^{1/4}} \Ex_{\bx \sim \{-1,1\}^n}\left[ \left(|\bN| - 2t(\bx) %\right)^+\right], \]
%which is $\Omega(\eps)$, 
\end{proof}

\def\br{\mathbf{r}}

\subsection{Indistinguishability of $\Dno$ from the uniform distribution}

Consider the task of distinguishing via $q$ independent samples, $\bx_1, \dots, \bx_q \in \R^n$, whether these samples were drawn from the standard $n$-dimensional Gaussian $\calN(0, I)$, or an $n$-dimensional Gaussian $\calN(\bmu, I)$, where $\bmu \sim \calD$. We consider the above problem because of the following simple claim, which shows how to generate a product distribution whose mean vector has $i$-th coordinate $\Omega(\bmu_i)$.

\begin{claim}
Let $\sign\colon \R^n \to \{-1,1\}^n$ denote the function which applies $\sign(\cdot)$  coordinate-wise. 
\begin{flushleft}\begin{itemize}
\item The uniform distribution over $\{-1,1\}^n$ can be generated by sampling $\bx \sim \calN(0, I)$ and outputting $\sign(\bx)$.
\item For a fixed $\mu \in [0, 1/2]^n$, consider the product distribution over $\{-1,1\}^n$ generated by sampling $\bx \sim \calN( \mu, I)$ and outputting $\sign(\bx)$. Then, the mean vector of such a distribution has the $i$-th coordinate set to $\Omega(\mu_i)$.
\end{itemize}\end{flushleft}
\end{claim}

\begin{proof}
The first condition is by symmetry of the Gaussian distribution, and the second condition is by standard Gaussian anti-concentration, whenever $\mu \in [0, 1/2]$.
\end{proof}

Hence, it suffices to prove the following lemma.

\begin{lemma}\label{lem:mainlemma1}
% Let $\calD_n'$ denote the distribution over $n$-dimensional Gaussian  $\calN(\bmu, I)$ where $\bmu \sim \calD$. 
Consider an algorithm that takes $q$ samples $\bx_1, \ldots, \bx_q\in \R^n$ and satisfies the following guarantees:
\begin{flushleft}\begin{itemize}
\item \textbf{\emph{Standard Case}}: If $\bx_1,\dots, \bx_q \sim \calN(0, I)$ and the algorithm receives those samples, then the algorithm outputs ``standard'' with probability at least $0.99$.
\item \textbf{\emph{Non-Standard Case}}: We sample $\bmu \sim \calD$,\footnote{The distribution $\calD$ is defined in the first bullet point of Subsection~\ref{sec:dno-def}.} then $\bx_1,\dots, \bx_q \sim \calN(\bmu, I)$, and the algorithm receives those samples, the algorithm outputs ``not standard'' with probability at least $0.99$.
\end{itemize}\end{flushleft}
Then, the number of samples must satisfy $q = \tilde{\Omega}(\sqrt{n}/\eps^2)$.
\end{lemma}

\subsection{Proof of \Lem{mainlemma1}} \label{sec:proof-mono-lb}

We prove by contradiction. {We assume that $q\le  \sqrt{n}/(c\eps^2 \log^4n)$ for some sufficiently large constant $c$} and show below
that the algorithm cannot distinguish between the two cases.

We set up some notation for the proof. We use $i \in [q]$ as an index over the set of queries,
while $j \in [n]$ indexes the dimension/coordinate.
We write $f_y, f_n \colon (\R^n)^q \to \R_{\geq 0}$ to denote the probability density functions of a tuple of $q$ independent samples from $\calN(0, I)$ (for $f_y$) or $\calN(\bmu, I)$ with $\bmu \sim \calD$ (for $f_n$) given by
\begin{align*}
f_y(x_1, \dots, x_q) &= \frac{1}{(2\pi)^{n/2}} \prod_{j=1}^n  \exp\left(-\frac{1}{2} \sum_{i=1}^q x_{ij}^2 \right) \qquad \text{and}\quad \\[1ex]
f_{n}(x_1,\dots, x_q) &= \frac{1}{(2\pi)^{n/2}} \prod_{j=1}^n \exp\left(-\frac{1}{2} \sum_{i=1}^q x_{ij}^2 \right) \left(1 - \frac{1}{\sqrt{n}} + \frac{1}{\sqrt{n}} \cdot \exp\left( \frac{\eps}{n^{1/4}}\sum_{i=1}^q x_{ij} -  \frac{q\eps^2}{2\sqrt{n}} \right)\right).
\end{align*}
The definition of $f_y$ comes from the product of $q$ many $n$-dimensional Gaussian p.d.fs; the definition of $f_n$ comes from the fact that each coordinate $j$ behaves independently under the draw of $\bmu \sim \calD$: with probability $1/\sqrt{n}$, $\bmu_i = \eps / n^{1/4}$ and is otherwise 0.

%We use $\bx_{ij}$ to denote the $j$th coordinate
%of $\bx_i$. Note that $\bmu$ is a vector in $\R^n$, where the $j$th coordinate is set according
%to the distribution described earlier.
%We use $f_y, f_n \colon (\R^n)^q \to \R_{\geq 0}$ to denote the probability density functions of a tuple of $q$ independent samples from $\calN(0, I)$ (for $f_y$) or $\calN(\bmu, I)$ for $\bmu \sim \calD$ (for $f_n$). We have
%\begin{align*}
%f_y(\bx_1, \dots, \bx_q) &= \frac{1}{(2\pi)^{n/2}} \prod_{i=1}^q  \exp\left(-\|\bx_i\|^2_2/2 \right) \qquad \text{and}\\[0.5ex]
%f_n(\bx_1,\dots, \bx_q) &= \frac{1}{(2\pi)^{n/2}} \prod_{i=1}^q \exp\left(-\|\bx_i - \bmu\|^2_2/2 \right)
%\end{align*}
%Note that $\bmu$ is a random vector, based on the distribution $\cD$. For a fixed choice of $\bmu$,
%we have the above probability distribution defined over $(\bx_1, \bx_2, \ldots, \bx_q)$.
The main lemma below shows that these pdfs are nearly the same with high probability over draws $\bx_1,\ldots,\bx_q\sim \cN(0,I)$.
(Technically, we only need to lower bound $f_n$ by $f_y$.)

\begin{lemma} \label{lem:gaussian-calc} Consider $q$ independent draws of $\bx_i \sim \cN(0,I)$.
%(the $n$-dimensional
%Gaussian distribution).
With probability at least $1- o_n(1)$, 
$$\frac{f_n(\bx_1, \ldots, \bx_q)}{f_y(\bx_1, \ldots, \bx_q)} \geq 1-o_n(1) .$$
\end{lemma}

\begin{proof} 
%Let us take the ratio and perform some simple manipulations.
%\begin{align}
%&\frac{f_n(\bx_1, \bx_2, \ldots, \bx_q)}{f_y(\bx_1, \bx_2, \ldots, \bx_q)}
%= \frac{\prod_{i=1}^q \exp(-\|\bx_i - \bmu\|^2_2/2 )}{\prod_{i=1}^q  \exp(-\|\bx_i\|^2_2/2 ) }
%= \prod_{i=1}^q \exp(\bx_i \cdot \bmu - \|\bmu\|^2/2) \\
%&= \prod_{i=1}^q \exp(\sum_{j=1}^n x_{ij}\mu_j - \sum_{j=1}^n \mu^2_j/2) 
%= \exp(\sum_{i=1}^q \sum_{j=1}^n x_{ij} \mu_j - \sum_{i=1}^q \sum_{j=1}^n \mu^2_j/2)\\
%&= \exp\Big(\sum_{j=1}^n (\sum_{i=1}^q x_{ij}) \mu_j - \sum_{j=1}^n q\mu^2_j/2\Big)
%= \prod_{j=1}^n \exp(X_j \mu_j - q\mu^2_j/2)
%\end{align}
We set $\bX_j := \sum_{i=1}^q \bx_{ij}$ for each $j\in [n]$. %, which is the sum of $j$th coordinates over all the pdf domain (which corresponds to the
%queries). We now take the expectation over the bias vector $\bmu$. Recall that each coordinate $\mu_i$
%is generated independently. With probability $1/\sqrt{n}$, $\mu_i = \eps/n^{1/4}$ and zero otherwise.
%Using these facts, we get the following.
Then the ratio can be written as
\begin{align}
%\EX_{\bmu}
\frac{f_n(\bx_1, \ldots, \bx_q)}{f_y(\bx_1, \ldots, \bx_q)}
%&= \prod_{j=1}^n \EX_{\mu_j}\exp(X_j \mu_j - q\mu^2_j/2) \\
&= \prod_{j=1}^n\left( 1 + \frac{1}{\sqrt{n}}\left( \exp\left(\frac{\eps\bX_j}{n^{1/4}}  - \frac{q\eps^2}{2\sqrt{n}}\right)-1\right)\right)
%\\
%&= \prod_{j=1}^n \Big[ 1 + (1/\sqrt{n})(\exp(\eps X_j/n^{1/4} - q\eps^2/2\sqrt{n}) - 1)\Big]
\end{align}
and thus, 
\begin{align}
 \ln\left( \frac{f_n(\bx_1, \ldots, \bx_q)}{f_y(\bx_1,  \ldots, \bx_q)}\right) 
&= %\sum_{j=1}^n \ln\Big[ 1 + (1/\sqrt{n})(\exp(\eps X_j/n^{1/4} - q\eps^2/2\sqrt{n}) - 1)\Big] \\
  \sum_{j=1}^n \ln\left[ 1 + \frac{\bW_j}{\sqrt{n}}\right],
  \quad \text{with\ }\bW_j \eqdef \exp\left(\frac{\eps \bX_j}{n^{1/4}} - \frac{q\eps^2}{2\sqrt{n}}\right) - 1.
  \label{eq:wj}
\end{align}
%
%We define $$.
At this point, we use the distributional information of $\bx_1, \ldots, \bx_n\sim \cN(0,I)$:

\begin{claim} \label{clm:wj} With probability at least $1-1/n$, we have 
$|\bW_j| \leq 1/\log n$ for all $j \in [n]$.
\end{claim}

\begin{proof} Note that $\bX_j = \sum_{i=1}^q \bx_{ij}$ where each $\bx_{ij} \sim \cN(0,1)$. Hence, $\bX_j \sim \cN(0,q)$.
With probability at least $1-1/n^2$, we have $|\bX_j| \le 4\sqrt{q}\log n$. By a union bound over all coordinates,~with probability at least $1-1/n$, we have $|\bX_j| \le  4\sqrt{q}\log n$ for all $j\in [n]$.

When this is the case, using $ {q\le \sqrt{n}/(c\eps^2 \log^4n)}$
  we have (when $c$ is sufficiently large)
$$\frac{\eps |\bX_j|}{n^{1/4}} \le \frac{4\eps \sqrt{q} \log n }{n^{1/4}} \le \frac{1}{4\log n}\quad\text{and}\quad
\frac{q\eps^2}{2\sqrt{n}} \le \frac{1}{4\log n}.$$ 
%We deduce that $|\eps X_j/n^{1/4} - q\eps^2/2\sqrt{n}| \leq \eps|X_j|/n^{1/4} + q\eps^2/2\sqrt{n}
%< 1/2\log n$. 
Using $1/(1-z) \geq \exp(z) \geq 1+z$ for $|z|\le 1$, we have
$$ \exp\left(\frac{\eps \bX_j}{n^{1/4}} - \frac{q\eps^2}{2\sqrt{n}}\right) \geq 1-\frac{1}{2\log n}$$
and
$$ \exp\left(\frac{\eps \bX_j}{n^{1/4}} - \frac{q\eps^2}{2\sqrt{n}}\right) \leq \frac{ 1}{1-1/(2\log n)} \leq 1 + \frac{1}{\log n}.$$
So $|\bW_j|  \leq 1/\log n$ for all $j$ and the claim follows.
\end{proof}

We go back to \Eqn{wj}, and apply the inequality $\ln(1+z) \geq z-z^2$ for $|z| \leq 1/2$.
With probability at least $1-1/n$ over $\bx_1, \ldots, \bx_n\sim \cN(0,I)$, we have $|\bW_j|\le 1/\log n$ for all $j$ and thus,
\begin{align}
\ln\left(\frac{f_n(\bx_1,  \ldots, \bx_q)}{f_y(\bx_1,  \ldots, \bx_q)} \right)\nonumber
&\geq \frac{1}{\sqrt{n}}\sum_{j=1}^n \bW_j - \frac{1}{n} \sum_{j=1}^n \bW^2_j \\
&\geq \frac{1}{\sqrt{n}}\sum_{j=1}^n \bW_j - \frac{1}{\log^2 n} \ \ \ ~~~~~~~ \textrm{(by \Clm{wj}, $\bW^2_j \leq 1/\log^2 n$)}\nonumber \\
&= \frac{1}{\sqrt{n}} \left[ \exp\left(-\frac{q \eps^2}{2\sqrt{n}}\right) \sum_{j=1}^n \exp\left(\frac{\eps \bX_j}{n^{1/4}}\right) - n \right]  - \frac{1}{\log^2 n}. \label{eq:gaussian}
\end{align}
The heart of the matter is the
next claim on the distribution of sum of exponentials of Gaussians.

\begin{claim} \label{clm:exp-gauss} Let each $\bX_j \sim \cN(0,q)$ be independent.
With probability at least $ 1-1/\sqrt{\log n}$,  we have$$\sum_{j=1}^n \exp\left(\frac{\eps \bX_j}{n^{1/4}}\right) \geq n \cdot\exp\left(\frac{q \eps^2}{2\sqrt{n}}\right) - \frac{\sqrt{n}}{\log^{0.25} n}.$$
\end{claim}

\begin{proof} Denote $\bY_j = \eps \bX_j/n^{1/4}$ and consider the random variable $\bZ_j = \exp(\bY_j)$. 
Observe~that~$\bY_j$ $\sim \cN(0, q\eps^2/\sqrt{n})$. Using the formula for the moment generating function
    of the Gaussian~\cite{gaussian-wiki}, we have $\EX[\exp(t\bY_j)] = \exp(q\eps^2 t^2/(2\sqrt{n}))$.
Hence, $$\EX[\bZ_j] = \EX[\exp(\bY_j)] = \exp\left(\frac{q\eps^2}{2\sqrt{n}}\right)\quad\text{and}\quad \EX[\bZ^2_j] = \EX[\exp(2\bY_j)] = \exp\left(\frac{2q\eps^2}{\sqrt{n}}\right).$$
So $\textrm{var}[\bZ_j] = \exp(2q\eps^2/\sqrt{n}) - \exp(q\eps^2/\sqrt{n})\leq 1/\log n$ using  $q\eps^2/\sqrt{n} = o (1/\log n)$.
Overall, we have 
$$\EX\left[\sum_{j=1}^n \bZ_j\right] = n\cdot \exp\left(\frac{q\eps^2}{ 2\sqrt{n}}\right)\quad\text{and}\quad\var\left[\sum_{j=1}^n \bZ_j\right] \leq \frac{n}{\log n}$$ since
all $\bZ_j$'s are independent. By Chebyshev's inequality, we have 
$$\Pr\left[\left|\sum_{j=1}^n \bZ_j - n\cdot \exp\left(\frac{q\eps^2}{2\sqrt{n}}\right)\right| > \frac{\sqrt{n}}{\log^{0.25} n}\right] \leq \frac{1}{\sqrt{\log n}}.$$
This finishes the proof of the claim.
\end{proof}

%We pick up from \Eqn{gaussian}, and apply \Clm{exp-gauss}.  
In particular, with probability at least $1-1/n - 1/\sqrt{\log n} = 1-o_n(1)$, we get that
\begin{align*}
\exp\left(-\frac{q \eps^2}{2\sqrt{n}}\right) \sum_{j=1}^n \exp\left(\frac{\eps \bX_j}{n^{1/4}}\right) & \geq \exp\left(-\frac{q \eps^2}{2\sqrt{n}}\right)\cdot \left( n \cdot \exp\left(\frac{q \eps^2}{2\sqrt{n}}\right) - \frac{\sqrt{n}}{\log^{0.25} n}\right)  >  n - \frac{\sqrt{n}}{\log^{0.25} n} 
    \end{align*}
where in the last inequality we used $\exp(-q \eps^2/2\sqrt{n}) < 1$. Substituting in \Eqn{gaussian}, we get
\begin{align}
\ln\left(\frac{f_n(\bx_1, \ldots, \bx_q)}{f_y(\bx_1,  \ldots, \bx_q)} \right)
& > -\frac{ 1}{    \log^{0.25} n}  - \frac{1}{\log^2 n} \label{eq:18}
\end{align}
Hence, with probability at least $1-o_n(1)$,  we have
$$\frac{f_n(\bx_1,  \ldots, \bx_q)}{f_y(\bx_1, \ldots, \bx_q)} \geq \exp\left(-\frac{ 1}{ \log^{0.25} n} - \frac{1}{\log^2n}\right) = 1-o_n(1).$$
This finishes the proof of the lemma.
\end{proof}

We can now complete the proof of \Lem{mainlemma1}. 
Consider the set $Y \subset (\R^n)^{q}$ of tuples that lead  the algorithm to output ``standard.''
Then we must have $$\Prx_{ \bx_i \sim \cN(0,I)}\big[(\bx_1, \dots, \bx_q) \in Y\big] \geq 0.99.$$ Let $Y' \subseteq Y$
be the set of tuples that also satisfy the condition of \Lem{gaussian-calc}.
By a union bound $$\Prx_{ \bx_i \sim \cN(0,I)}\big[(\bx_1, \dots, \bx_q) \in Y'\big] \geq 0.99 - o_n(1) \geq 0.98.$$
Thus, $\int_{Y'} f_y(\bx_1, \dots, \bx_q) d \bx_1 d \bx_2 \ldots d \bx_q \geq 0.98$. 
By the condition of \Lem{gaussian-calc}, we have  $$\int_{Y'}  f_n(\bx_1, \dots, \bx_q)  d \bx_1 \ldots d \bx_q \geq (1-o_n(1))\cdot0.98 \geq 0.97.$$
%Since $f_n(\cdot)$ is absolutely bounded, by Fubini's theorem, we can switch the expectation and integral. So
%$\EX_{\mu} [\int_{Y'} f_n(\bx_1, \dots, \bx_q) d \bx_1 
This is exactly the probability that we see a tuple in $Y'$,
when we generate the samples $\bx_1, \cdots, \bx_q$ from the non-standard case. Thus,
with probability at least $0.97$, the algorithm outputs ``standard'' when the samples are generated
from the non-standard case. This completes the contradiction.

\begin{comment}
\subsection{The old proof}

\begin{proof} 
We write $f_y, f_n \colon (\R^n)^q \to \R_{\geq 0}$ to denote the probability density functions of a tuple of $q$ independent samples from $\calN(0, I)$ (for $f_y$) or $\calN(\bmu, I)$ for $\bmu \sim \calD$ (for $f_n$) given by
\begin{align*}
f_y(x_1, \dots, x_q) &= \frac{1}{(2\pi)^{n/2}} \prod_{k=1}^n  \exp\left(-\frac{1}{2} \sum_{i=1}^q x_{ik}^2 \right) \qquad \text{and}\qquad \\[1ex]
f_{n}(x_1,\dots, x_q) &= \frac{1}{(2\pi)^{n/2}} \prod_{k=1}^n \exp\left(-\frac{1}{2} \sum_{i=1}^q x_{ik}^2 \right) \left(1 - \frac{1}{\sqrt{n}} + \frac{1}{\sqrt{n}} \cdot \exp\left( \frac{\eps}{n^{1/4}}\sum_{i=1}^q x_{ik} -  \frac{q\eps^2}{2\sqrt{n}} \right)\right).
\end{align*}
The definition of $f_y$ comes from the product of $q$ many $n$-dimensional Gaussian p.d.fs, and the definition of $f_n$ comes from the fact that each coordinate $k \in [n]$ behaves independently under the draw of $\bmu \sim \calD$, and with probability $1/\sqrt{n}$, $\bmu_i = \eps / n^{1/4}$ and is otherwise 0.

Letting $s = \sum_{i=1}^q x_i$ denote the vector given by the sum and $\xi = \exp(-q\eps^2 / (2\sqrt{n}))$, we write
\begin{align*}
\dfrac{f_n(x_1, \dots, x_q)}{f_y(x_1, \dots, x_q)} &= \prod_{k=1}^n \left(1 + \frac{\xi}{\sqrt{n}} \left( \exp\left( \frac{\eps s_k}{n^{1/4}} \right) - \exp\left(\frac{q\eps^2}{2\sqrt{n}}\right)  \right) \right).
\end{align*}
Applying Taylor expansion, we have
$$
\dfrac{f_n(x_1, \dots, x_q)}{f_y(x_1, \dots, x_q)}  = \prod_{k=1}^n \left(1 + \frac{\xi}{\sqrt{n}} \left( 
\sum_{\text{$\ell$ odd}} \frac{1}{\ell!}\left(\frac{\eps s_k}{n^{1/4}}\right)^\ell
+\sum_{\ell=1}^\infty \left(\frac{1}{(2\ell)!}\left(\frac{\eps s_k}{n^{1/4}}\right)^{2\ell}
-\frac{1}{\ell!}\left(\frac{q\eps^2}{2\sqrt{n}}\right)^\ell
\right)\right)\right).
$$
For each $k$, we write $A_k$ and $B_k$ to denote
\begin{align*}
A_k  \eqdef \frac{\xi}{\sqrt{n}}\sum_{\ell \text{ odd}} \frac{1}{\ell!} \left( \frac{\eps s_k}{n^{1/4}}\right)^{\ell} \quad\text{and}\quad 
B_k  \eqdef \frac{\xi}{\sqrt{n}} \sum_{\ell = 1}^{\infty} \left(\frac{\eps^2}{\sqrt{n}} \right)^{\ell} \left(\frac{ s_k^{2\ell}}{(2\ell)!} - \frac{ q^{\ell}}{2^{\ell}\ell!} \right),
\end{align*}
and $A=\sum_{k=1}^n A_k$ and $B=\sum_{k=1}^n B_k$:
\begin{align*}
    A  \eqdef \frac{\xi}{\sqrt{n}}\sum_{\ell \text{ odd}} \frac{1}{\ell!} \left( \frac{\eps}{n^{1/4}}\right)^{\ell} \sum_{k=1}^n s_k^{\ell}\quad\text{and}\quad
    B  \eqdef \frac{\xi}{\sqrt{n}} \sum_{\ell = 1}^{\infty} \left(\frac{\eps^2}{\sqrt{n}} \right)^{\ell} \left(\frac{1}{(2\ell)!}\sum_{k=1}^{n} s_k^{2\ell} - \frac{nq^{\ell}}{2^{\ell}\ell!} \right).
\end{align*}
Then we have 
\begin{equation}\label{eq:haha2}
\dfrac{f_n(x_1, \dots, x_q)}{f_y(x_1, \dots, x_q)}
=\prod_{k=1}^n \left(1+A_k+B_k\right)
\le \exp\big(A+B\big)  
\end{equation}
using $1+x\le e^x$.
We finish the proof using the following claim, which
  shows that $A$ and $B$ are both small with high probability when $\bx_1,\ldots,\bx_q$ are drawn
  from the non-standard case:

\begin{claim}\label{mainclaim1}
When $\bmu\sim \calD$ and $\bx_1,\ldots,\bx_q\sim \calN(\bmu, I)$ independently,
  we have both $A$ and $B$ are $o_n(1)$ with probability at least $1-o_n(1)$.
\end{claim}

We delay the proof of Claim \ref{mainclaim1} to the end and use it to 
  finishes the proof of Lemma \ref{mainlemma1}.

Let the set $N \subset (\R^n)^{q}$ denote the subset of tuples of samples $(x_1, \dots, x_q)$ such that, if the algorithm observes samples $(x_1, \dots, x_q)\in N$, it declares ``not standard.'' Then, we note that by assumption of the algorithm, a draw of $\bx_1, \dots, \bx_q \sim \calN(\bmu, I)$ with $\bmu\sim \calD$ must have $(\bx_1,\dots, \bx_q) \in N$ with probability at least $0.99$. Furthermore, let $N' \subseteq N$ be the set of tuples $(x_1, \dots, x_q)$ in $N$ such that $A$ and $B$ derived from it satisfy the condition of  Claim~\ref{mainclaim1}. On the one hand, we have from Claim~\ref{mainclaim1} and 
  a union bound that 
\begin{align*}
\Prx_{\substack{\bmu\sim \calD\\
\bx_1,\dots, \bx_q \sim \calN(\bmu, I)}}\left[ (\bx_1,\dots, \bx_q) \in N' \right] \geq 0.99-o_n(1)>0.98.
\end{align*}
On the other hand, for every $(x_1,\ldots,x_q)\in N'$ we have from (\ref{eq:haha2}) that
$$
\frac{f_n(x_1, \dots, x_n) } {f_y(x_1, \dots, x_n)}\le 
\exp\big(1+o_n(1)\big)\le 1+o_n(1).
$$ 
As a result, we have 
$$
\Prx_{\bx_1,\ldots,\bx_q\sim \calN(0,I)}\left[(\bx_1,\ldots,\bx_q)\in N'\right]
\ge (1-o_n(1))\cdot \Prx_{\substack{\bmu\sim \calD\\
\bx_1,\dots, \bx_q \sim \calN(\bmu, I)}}\left[ (\bx_1,\dots, \bx_q) \in N' \right] 
>0.97.
$$
However, this means
%but since $f_n(x_1, \dots, x_n) \geq f_y(x_1, \dots, x_n) (1 - o(1))$ for every $(x_1, \dots, x_n) \in A'$, we must have
%\begin{align*}
%\Prx_{\substack{\bmu \sim \calD \\ \bx_1,\dots, \bx_q \sim \calN(\bmu, I)}}\left[ (\bx_1,\dots, \bx_q) \in A' \right] \geq 0.98 \cdot (1 - o(1)) \geq 0.97.
%\end{align*}
  that the algorithm outputs ``not standard'' with probability at least $0.97$ when samples are drawn from $\calN(0,I)$, which contradicts the assumption that the algorithm outputs ``not standard'' with probability $0.99$ if the distribution is $\calN(0,I)$.
  %for $\bmu \sim \calD$.
%We will divide the computation in the following way. We show that under a certain event (which occurs often over draw of $\bx_1,\dots, \bx_q$), we have the following two inequalities holding:
%Once we establish both, then we can lower bound the ratio by expanding the Taylor expansion of both instances of $\exp(\cdot)$, and taking a union bound
%\begin{align*}
%    \dfrac{f_n(x_1, \dots, x_q)}{f_y(x_1, \dots, x_q)} \geq 1 - |A| - |B| \geq 1 - o(1).
%\end{align*}
\end{proof}

Finally we prove Claim \ref{mainclaim1}:

\begin{proof}[Proof of Claim \ref{mainclaim1}]
First, with probability at least $1-o_n(1)$, we have that the number of $i\in [n]$
  with $\bmu_i=\eps/n^{1/4}$ is at most $O(\sqrt{n})$.
Fix such a $\mu$ and without less of generality, we assume that 
  $\mu_1,\ldots,\mu_m=0$ and $\mu_{m+1},\ldots, \mu_n=\eps/n^{1/4}$
  for some $m$ with $n-m=O(\sqrt{n})$.
Then $\bs_k$ is drawn from $\calN(0,q)$ for $k\le m$ and 
  $\bs_{k}$ is drawn from $\calN(q\eps/n^{1/4},q)$ for $k>m$.
We also break $A,B$ into $A',A''$ and $B',B''$ accordingly, where
\begin{align*}
A'  &\eqdef \frac{\xi}{\sqrt{n}}\sum_{\ell \text{ odd}} \frac{1}{\ell!} \left( \frac{\eps}{n^{1/4}}\right)^{\ell} \sum_{k=1}^m \bs_k^{\ell}\\[0.6ex]
A''  &\eqdef \frac{\xi}{\sqrt{n}}\sum_{\ell \text{ odd}} \frac{1}{\ell!} \left( \frac{\eps}{n^{1/4}}\right)^{\ell} \sum_{k=m+1}^n \bs_k^{\ell} \\[0.6ex]
B'  &\eqdef \frac{\xi}{\sqrt{n}} \sum_{\ell = 1}^{\infty} \left(\frac{\eps^2}{\sqrt{n}} \right)^{\ell} \left(\frac{1}{(2\ell)!}\sum_{k=1}^{m} \bs_k^{2\ell} - \frac{mq^{\ell}}{2^{\ell}\ell!} \right)\quad \text{and}\qquad\\[0.6ex]
B''  &\eqdef \frac{\xi}{\sqrt{n}} \sum_{\ell = 1}^{\infty} \left(\frac{\eps^2}{\sqrt{n}} \right)^{\ell} \left(\frac{1}{(2\ell)!}\sum_{k=m+1}^{n} \bs_k^{2\ell} - \frac{(n-m)q^{\ell}}{2^{\ell}\ell!} \right).
\end{align*}
It suffices to show that $|A'|,|A''|,|B'|$ and $|B''|$ are all 
  $o_n(1)$ with probability $1-o_n(1)$.
For $A''$ and $B''$, we have from a standard union bound that with probability
  at least $1-o_n(1)$, every $\bs_k$ is 
  at most 
$$
\frac{q\eps}{n^{1/4}}+O\left(\sqrt{q\log n}\right)=O\left(\sqrt{q\log n}\right)
$$
for every $k>m$ using our choice of $q$.
When this holds for every $\bs_k$, $k>m$, we have
$$
|A''|\le \frac{1}{\sqrt{n}}\sum_{\text{$\ell$ odd}}\frac{1}{\ell!}
  \left(\frac{\eps}{n^{1/4}}\right)^\ell \cdot O(\sqrt{n})\cdot 
  \left(O\left(\sqrt{q\log n}\right)\right)^\ell=o_n(1),
$$
and 
$$
|B''| \le  \frac{1}{\sqrt{n}} \sum_{\ell = 1}^{\infty} \left(\frac{\eps^2}{\sqrt{n}} \right)^{\ell} \left(\frac{1}{(2\ell)!}\cdot O(\sqrt{n})\cdot 
\left(O\left(\sqrt{q\log n}\right)\right)^\ell+ \frac{1}{2^\ell \ell!}\cdot O(\sqrt{n})\cdot  q^{\ell} \right)=o_n(1).
$$

We bound $|A'|$ and $|B'|$ in the rest of the proof, where 
  we need to be more careful and take advantage of cancellations in the sums.
Below $k$ will always denote an index in $[m]$ and $\bs_k$ is always 
  drawn independently from $\calN(0,q)$.
We start with the following claim about expectations of $\bs_k^\ell$.

\begin{claim}\label{cl:exp}
For every $k\in [m]$ and $\ell \in \N$, the expectation of $\bs_k^{\ell}$ is zero if $\ell$ is odd, and
\begin{align*}
    \frac{1}{(2\ell)!} \Ex_{\bs_k\sim \calN(0, q)}\left[ \bs_k^{2\ell} \right] = \dfrac{q^{\ell}}{2^{\ell} \ell!}.
\end{align*}
\end{claim}

\begin{proof}
    The fact that odd moments are zero follows from the fact $\calN(0, q)$ is symmetric. Then, recall that the $2\ell$-th moment of the Gaussian distribution with variance $q$ is exactly $q^{\ell} (2\ell-1)!!$,    where the double-factorial $n!!$ is the product of all numbers from $1$ to $n$ which share the same parity as $n$. Evaluating $(2\ell-1)!! / (2\ell)!$, we obtain $1/(2\ell)!!$, which is $2 \cdot 4 \cdot 6 \cdot \dots \cdot 2\ell = \ell! \cdot 2^{\ell}$.
\end{proof}

We use the following claim to bound $|A'|$ and $|B'|$, where we note that $m\le n$:

\begin{claim}\label{cl:good-s}
With probability $1-o_n(1)$, 
$\bs_1,\ldots,\bs_m\sim \calN(0,q)$ satisfy the following two conditions:
\begin{itemize}
    \item For every odd $\ell \geq 1$, we have
\begin{align*}
\left| \sum_{k=1}^m \bs_k^{\ell} \right| \leq \left(\sqrt{n} \cdot q^{\ell / 2} \cdot \sqrt{(2\ell - 1)!!}\right) \cdot (\log n)^{\ell};
\end{align*}
\item For every $\ell \geq 1$, we have
\begin{align*}
    \left|\frac{1}{(2\ell)!} \sum_{k=1}^{m} \bs_k^{2\ell} - \frac{mq^{\ell}}{2^{\ell} \ell!} \right| \leq \left(\sqrt{n} \cdot q^{\ell} \cdot 2^{\ell} \right)\cdot (\log n)^{\ell}.
\end{align*}
\end{itemize}
\end{claim}

\begin{proof}
Since $\bs_k\sim \calN(0,q)$, each sum of $\bs_k^{\ell}$ has expectation $0$ by Claim~\ref{cl:exp}. We apply Chebyshev's inequality, and we have
\begin{align*}
\Prx_{\bs_1, \dots, \bs_m \sim \calN(0, q)}\left[ \left| \sum_{k=1}^m \bs_k^{\ell} \right| \geq v \right] \leq \frac{n}{v^2}  \cdot \Ex_{\bs \sim \calN(0, q)}\left[ \bs^{2\ell} \right] = \frac{n \cdot q^{\ell}}{v^2} \cdot (2\ell - 1)!! \leq \frac{1}{(\log n)^{2\ell}},
\end{align*}
for the setting of $$v = \left(\sqrt{n} \cdot q^{\ell/2} \cdot \sqrt{(2\ell - 1)!!}\right) \cdot (\log n)^{\ell}.$$ 

Now for the second item,
  consider any $\ell \in \N$, and note that by Claim~\ref{cl:exp}, the expectation of the expression is also zero, so we apply Chebyshev's inequality once more. We have
\begin{align*}
    \Prx_{\bs_1,\dots, \bs_m \sim \calN(0,q)}\left[ \left|\frac{1}{(2\ell)!} \sum_{k=1}^m \bs_k^{2\ell} - \frac{mq^{\ell}}{2^{\ell} \ell!}\right|  \geq v \right] &\leq \frac{n}{v^2} \cdot \Ex_{\bs \sim \calN(0,q)}\left[\left(\frac{\bs^{2\ell}}{(2\ell)!} - \frac{q^{\ell}}{2^{\ell}\ell!} \right)^2\right] \\[0.3ex]
        &\leq \frac{n}{v^2} \cdot \left(\frac{1}{(2\ell)!}\right)^2 \Ex_{\bs\sim\calN(0,q)}\left[\bs^{4\ell} \right] \\[0.6ex]
        &\leq \frac{n}{v^2} \cdot \left(\frac{1}{(2\ell)!}\right)^2 q^{2\ell} \cdot (4\ell-1)!! \leq \frac{nq^{2\ell}}{v^2} \cdot 2^{2\ell} \leq \frac{1}{(\log n)^{2\ell}},
\end{align*}
for the setting of 
$$
v=\left(\sqrt{n} \cdot q^{\ell} \cdot 2^{\ell} \right)\cdot (\log n)^{\ell}.
$$
The final simplification comes from the fact the square of $(2\ell)!$ multiplies every integer between $1$ and $2\ell$ twice, and $(4\ell - 1)!!$ multiplies every odd integer between $1$ and $4\ell-1$ (a total of at most $2\ell$ of them), which is at most twice an integer in multiplied in $(2\ell)!$; a very loose bound gives $(4
\ell-1)!! \leq ((2\ell)!)^2 \cdot 2^{2\ell}$.
%Setting $v = \left(\sqrt{n} \cdot q^{\ell} \cdot 2^{\ell} \right) \cdot (\log n)^{\ell}$. 
By a union bound over all terms, the probability that some inequality fails to be satisfied is at most $\sum_{\ell \in \N} 2 / (\log n)^{2\ell} = o_n(1)$.
\end{proof}

Consider any setting of $s_1,\dots, s_m$ where the event of Claim~\ref{cl:good-s} is satisfied, then
\begin{align*}
    |A'|  &\leq \frac{\xi}{\sqrt{n}} \sum_{\ell \text{ odd}} \frac{1}{\ell!}\left(\frac{\eps}{n^{1/4}} \right)^{\ell} \left(\sqrt{n} \cdot q^{\ell/2} \sqrt{(2\ell-1)!!} \right) \cdot (\log n)^{\ell} \leq  \sum_{\ell \text{ odd}} \left(\frac{2\eps \sqrt{q} \cdot \log n}{n^{1/4}} \right)^{\ell}\quad \text{and} \\[0.8ex]
    |B'| &\leq \frac{\xi}{\sqrt{n}} \sum_{\ell=1}^{\infty} \left(\frac{\eps^2}{\sqrt{n}}\right)^{\ell} \cdot \left( \sqrt{n} \cdot q^{\ell} \cdot 2^{\ell} \right) \cdot (\log n)^{\ell} \leq \sum_{\ell=1}^{\infty} \left(\frac{2\eps^2 q\cdot  \log n}{\sqrt{n}} \right)^{\ell},
\end{align*}
and both are $o_n(1)$ when $q$ is a sufficiently small constant smaller than $\sqrt{n} / (\eps^2 \log^2 n)$. 
\end{proof}
\end{comment}

