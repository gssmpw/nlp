\section{Related Work}
\label{sec:related}

%Directed Isoperimetry Results 
As mentioned above, directed isoperimetry theorems have been  crucial to the development of testers for monotonicity of Boolean functions~\cite{GGLRS00,ChSe13-j,KMS18} (as opposed to monotonicity of distributions). In particular, as we illustrate in~\Thm{booliso}, the strongest known directed isoperimetry theorem for Boolean functions due to~\cite{KMS18} (and slightly improved by~\cite{PRW22}) relates the expected $\norm{\grad^-f(x)}_2$ to the $\ell_0$-distance of $f$ from monotonicity, that is, the fraction of domain points at which $f$ must be modified to make it a monotone function. Our result,~\Thm{l1-talagrand}, is an ``$\ell_1$-version'' of the above statement for {\em real-valued} functions over the Boolean hypercube. 

The most relevant works to \Thm{l1-talagrand} are the directed isoperimetry theorems initiated by Pinto Jr.~\cite{F23, F24} who considers smooth functions $f:[0,1]^n \to \RR$. In~\cite{F23}, Pinto Jr. looks at the $\ell_1$-geometry and proves under a certain $\ell_1$-smoothness condition, the expected $\ell_1$-norm of the gradient is at least the $\ell_1$-distance of $f$ from monotonicity. In the subsequent paper~\cite{F24}, Pinto Jr. assumes $\ell_2$-smoothness and proves that the expected $\ell_2^2$-norm of the gradient is at least the square of the $\ell_2$-distance. Neither of these results imply or are implied by the Boolean setting of~\cite{KMS18} or our result,~\Thm{l1-talagrand}. As mentioned earlier, our result,~\Thm{l1-talagrand}, answers a question left open in~\cite{F23}.
Using the notation of that paper, we prove an $(L^1, \ell^2)$-Poinc\'{a}re theorem
for real-valued functions on the hypercube.


 In~\cite{BKR23}, Black, Kalemaj and Raskhodnikova consider Boolean functions $f:\{-1,1\}^n \to \mathbb{R}$ and they prove that the isoperimetry result of~\cite{KMS18} generalizes for such functions in the following sense. Instead of looking at the $\ell_2$-norm of the (directed) gradient $\grad^- f(x)$,~\cite{BKR23} considers the square-root of the ``negative influence'' at each $x$, where the ``negative influence'' counts the number of pairs which form a monotonicity violation with $x$. The {\em magnitude} of the violation is ignored. 
In this setting,~\cite{BKR23} proves that if a real-valued function is $\eps$-far from being monotone in the $\ell_0$-sense (which is usual in property testing), then the expected square-root of the negative influence is $\Omega(\eps)$ thereby generalizing~\cite{KMS18}. The authors use this result to give an $O(r\sqrt{d}/\eps^2)$-query non-adaptive tester for real-valued monotone functions, where $r$ is the cardinality of the image of $f$. Our directed isoperimetry result seems unrelated to their result, apart from the fact that both of our results are proved by reducing it to the Boolean case. 
Finally, in~\cite{BCS23}, Black, Chakrabarty and Seshadhri generalize the directed isoperimetry theorem of~\cite{KMS18} to Boolean functions defined over the {\em hypergrid}. %This is not relevant to our result, but our techniques would also imply a similar theorem as~\Thm{l1-talagrand} for $[0,1]$-range function defined over the hypergrid. 
%{\bf deepc: is the distribution testing question interesting here?}

\paragraph{Monotonicity Testing of Distributions.} Monotonicity of distributions has been studied extensively in the literature, in both low-dimensional and high-dimensional regimes~\cite{BKR04, RS09, ACS10, BFRV11,AGPRY19,RV20}. Batu, Kumar, and Rubinfeld initiated the study in~\cite{BKR04} and considered both regimes above. They described a 
tester for one-dimensional distributions (total orders) using $\tilde{O}_\eps(\sqrt{n})$-samples, and via a reduction to uniformity testing proved a tightness of this result. They also proved a $\Omega(m^{n/2})$-lower bound for distributions over $[m]^n$, and described algorithms with $\tilde{O}(m^{n - 0.5})$-samples. The one-dimensional result's dependency on $\eps$ was improved by~\cite{CDGR18} and the optimal algorithm for the low-dimensional regime was given by Acharya, Daskalakis and Kamath~\cite{ADK15} who gave a tester with sample-complexity $O\left(\frac{m^{n/2}}{\eps^2} + \frac{1}{\eps^2}\left(\frac{n\log m}{\eps^2}\right)^n\right)$. For the high-dimensional regime (which is of interest of this paper) of distributions over the hypercube $\{-1,+1\}^n$, one can get stronger lower bounds than ones found by reduction to uniformity testing: Aliakbarpour, Gouleakis, Peebles, Rubinfeld and Yodpinayee~\cite{AGPRY19} prove a lower bound of $2^{(1 - \Theta(\sqrt{\eps}) - o(1))n}$ on the sample complexity. The best upper bound is currently at $\smash{2^{n} / 2^{\Theta_{\eps}(n^{1/5})}}$ samples due to Rubinfeld and Vasilyan~\cite{RV20}.

\paragraph{Distribution Testing Beyond Sample-Complexity.} Many distribution testing problems over high-dimensional domains incur sample complexities which are exponential in the dimension. As a result, various works have sought models and techniques to overcome these lower bounds, which can be divided between those which assume structure on the input, and those which provide stronger access. Works assuming additional structure on the input include monotonicity~\cite{RS09}, low-degree Bayesian networks~\cite{CDKS17, DP17, ABDK18, DKP23}, Markov random fields~\cite{GLP18, DDK19, BBCSV20}, ``flat'' histogram structure~\cite{DKP19}, or structured truncations~\cite{DNS23, DLNS24}. On the other hand, the subcube conditional model follows the other approach on assuming stronger access.~\cite{BC18} was the first to obtain polynomial query complexities for various testing problems; for uniformity testing over $\{-1,1\}^n$,~\cite{CCKLW21} showed the complexity is $\tilde{\Theta}(\sqrt{n}/\eps^2)$~and~\cite{CM24} extended it to hypergrids. In this work, we use an approach of~\cite{CJLW21b} which studied subcube conditioning for testing and learning $k$-junta distributions (those which have at most $k$ non-uniform variables). Other accesses include (unrestricted) conditioning on the domain~\cite{CRS15,CFGM16} (see also, improvements~\cite{FJOPS15, ACK15, KT18, N21, CCK24, CCKM24}),  queries to the probability density function or cumulative distribution function~\cite{BDKR05, CR14}, conditioning on prefixes for hidden Markov models~\cite{MKKZ23}, and samples which reveal their probability~\cite{OS18}.