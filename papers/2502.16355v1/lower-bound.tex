% !TEX root = main.tex

\section{Lower Bound for Testing Monotonicity}\label{sec:mon-lb}

In this section, we prove a query complexity lower bound on testing monotonicity of distributions using subcube conditioning queries.
\begin{theorem}[Monotonicity Testing -- Lower Bound]\label{thm:mon-lb}
For any $\eps \in (0,1)$, an $\eps$-test for monotonicity of distributions must make $\tilde{\Omega}(n/\eps^2)$ queries.
\end{theorem}

\subsection{Preliminaries} \label{sec:prelims}

Our lower bound proofs proceed by Yao's method. We consider a property of distribution $\calP$ that we want to test.
We describe two distributions, $\Dyes$ and $\Dno$, supported on \emph{product} distributions over $\{-1,1\}^n$ which, in addition, satisfy the following properties:
\begin{itemize}
\item Every distribution $p$ in $\Dyes$ is a distribution over $\{-1,1\}^n$ lies in $\calP$.
\item A distribution $\bp$ drawn from $\Dno$ is a distribution over $\{-1,1\}^n$ which is $\eps$-far from $\calP$ with probability at least $0.99$ (over the draw of $\bp \sim \Dno$).   
\end{itemize}
Consider any deterministic algorithm which can $\eps$-test monotonicity which makes $q$ queries. The algorithm is specified by a depth-$q$ decision tree of subcube conditioning queries; each non-leaf node of the decision tree specifies a subcube $\rho \in \{-1,1, *\}^n$ which is the query performed at that node, and has $2^{\stars(\rho)}$ many children, corresponding to the possible completions of $\rho$ that the algorithm would receive after such a query; each leaf is labeled ``accept'', or ``reject'' corresponding to the output. In the case input distributions $p$ are promised to be \emph{product} distributions, algorithms with subcube query access significantly simplify, allowing us to prove lower bounds against the \emph{sample complexity} of $\eps$-testing algorithms.

\begin{lemma}[Decision Tree to i.i.d Samples]\label{lem:tree-to-iid}
Let $p$ be a product distribution supported on $\{-1,1\}^n$, and $\Alg$ be a deterministic $q$-query algorithm with subcube conditioning access. There exists a function $\Alg' \colon \{-1,1\}^{nq} \to \{ \text{``accept''}, \text{``reject''}\}$ which exactly simulates the algorithm on independent samples, i.e.,
\begin{align*}
\Prx\left[ \Alg(p) = \text{``accept''} \right] = \Prx_{\bx_1,\dots, \bx_q \sim p}\left[ \Alg'(\bx_1,\dots, \bx_q) = \text{``accept''}\right].
\end{align*}
\end{lemma}\begin{proof}
We describe the function $\Alg' \colon \{-1,1\}^{nq} \to \{\text{``accept''}, \text{``reject''} \}$ by specifying how it computes on independent samples $\bx_1, \dots, \bx_q$ using the depth-$q$ decision tree $\Alg$. The algorithm maintains the current node $u$, initially set to the root of $\Alg$, and a counter $c$, initially set to $1$. If $u$ is a leaf, then we output either ``accept'' or ``reject'', according to the decision held at $u$. Otherwise, $u$ contains a string $\rho \in \{-1,1, *\}^n$ which specifies a subcube conditioning query. We update $u$ to be the child corresponding to the completion $(\bx_c)_{|\stars(\rho)} \in \{-1,1\}^{\stars(\rho)}$ of the sample $\bx_c$ and we increment $c$. We note that, since the distribution $p$ is a product distribution, $(\bx_c)_{|\stars(\rho)}$ is distributed as a draw from $p_{|\rho}$, and since the counter is incremented, all the used queries are mutually independent. Hence, this is a perfect simulation of subcube conditioning queries with i.i.d samples when the input distribution $p$ is product.
\end{proof}

In the following subsection, we describe the two distribution $\Dyes$ and $\Dno$; we prove these are monotone and far-from monotone distributions in Lemma~\ref{lem:mon-dist} and Lemma~\ref{lem:far-mon-dist}, respectively. 
% As discussed in Section~\ref{sec:prelims}, we will describe two distributions $\Dyes$ and $\Dno$ supported on product distributions over $\{-1,1\}^n$. Every distribution in the support of $\Dyes$ will be
% monotone, and $\bp \sim \Dno$ will be $\Omega(\eps)$-far from monotone with probability at least $0.99$. 
As per Lemma~\ref{lem:tree-to-iid}, in order to prove a $q$-query lower bound, it suffices to show that for any function $\Alg \colon \{-1,1\}^{nq} \to \{\text{``accept''}, \text{``reject''} \}$, 
\begin{align}
\left| \Prx_{\substack{\bp \sim \Dyes \\ \bx_1,\dots, \bx_q \sim \bp}}\big[ \Alg(\bx_1,\dots, \bx_q) = \text{``accept''}\big] - \Prx_{\substack{\bp \sim \Dno \\ \bx_1,\dots, \bx_q \sim \bp}}\big[ \Alg(\bx_1,\dots, \bx_q) = \text{``accept''}\big] \right| = o(1). \label{eq:indistinguish}
\end{align}
In Section~\ref{sec:indist}, we prove that (\ref{eq:indistinguish}) holds for any $q$-sample algorithm $\Alg$ with $q$ smaller than $n / (\eps^2\cdot \polylog(n))$. This concludes the proof of Theorem~\ref{thm:mon-lb}.

\subsection{The one-dimensional mean distributions} \label{sec:mean-dist}

A distribution supported on product distributions on $\{-1,1\}^n$ may be equivalently specified by describing a distribution on vectors in $[-1,1]^n$, corresponding to the mean vectors of the distribution. 
In this section, we show the existence of one-dimensional distributions with special properties. In the next subsection, we use these
distributions to generate mean vectors, from which the hypercube distributions are constructed. The properties of the next
lemma are crucial for the main lower bound.

\begin{lemma}[One-Dimensional Bias Distributions]\label{lem:one-dim-dist}
Let $c_0$ be some absolute positive constant.
Fix any natural number parameter $\ell$. There exist two discrete distributions $\cA$ and $\cB$ with the following properties.
\begin{asparaitem}
	\item $\cA$ is supported on $\{0\} \cup [\ell^3]$ and $\cB$ is supported on $\{-1, 0\} \cup [\ell^3]$.
	\item For every $k \leq \ell$, $\EX_{i \sim \cA}[i^k] = \EX_{j \sim \cB}[j^k]$.
	\item $\Pr_{j \sim \cB}[j = -1] > c_0$.
\end{asparaitem}
\end{lemma}

The first step of the proof is to construct a solution to a linear system that captures the second point above (the equality of moments).
For convenience, let $\alpha_j= j^3$ for each $j\in [\ell]$.
Consider the following $(\ell+1) \times (\ell+1)$ matrix $A$:
\begin{align*}
A := \left[ \begin{array}{ccccc} 1 & 0 & 0 & \dots & 0 \\ 
				       1 & 1 & 1 & \dots & 1 \\
				       -1 & \alpha_1^1 & \alpha_2^1 & \dots & \alpha_{\ell}^1 \\
				       (-1)^2 & \alpha_1^2 & \alpha_2^2 & \dots & \alpha_{\ell}^2 \\
				       \vdots & \vdots & \vdots & \vdots & \vdots \\
				       (-1)^{\ell-1} & \alpha_1^{\ell-1} & \alpha_2^{\ell-1} & \dots & \alpha_{\ell}^{\ell-1} \end{array} \right].
\end{align*}

\begin{claim}\label{clm:inv-and-z}
The matrix $A$ %(\alpha_0,\dots, \alpha_{\ell})$ 
is invertible and the vector $z \in \R^{\ell+1}$ satisfying $A z = e_1$ has $\|z\|_1 = O(1)$.
\end{claim}

\begin{proof} It is convenient to index the rows/columns of $A$ with $0,1,2,\ldots,\ell$.
The proof of Claim~\ref{clm:inv-and-z} mostly follows similar calculations  in the proof of Claim~6.3 of~\cite{CJLW21b}. Let $V$ denote the transpose of the $\ell \times \ell$ Vandermonde matrix on $\alpha_1,\dots, \alpha_{\ell}$, so we may substitute for the determinant
\[ \det(A ) = \det(V ) = \prod_{\substack{i,j \in [\ell] \\ i < j}} (\alpha_j - \alpha_i) \neq  0,\]
as long as $\alpha_1,\dots, \alpha_{\ell}$ are distinct, and this means that $A$ is invertible. 

We now compute $z$ using Cramer's rule: $z_0=1$ and for each $i\in [\ell]$, $z_i$ is given by
\begin{align*}
z_i = \dfrac{\det(A_i)}{\det(A)} = (\pm 1)\cdot \dfrac{\det(V(-1; \alpha_{-i}))}{\det(V )}.
\end{align*}
The numerator $\det(A_i)$ denotes the determinant of the matrix where the $i$-th column of $A$ is replaced by $e_1$. Then, $\det(A_i)$ is the determinant of the Vandermonde matrix of the entries $-1$, and well as $\alpha_1,\dots, \alpha_{\ell}$ except for $\alpha_i$, which we refer  to as $V(-1;\alpha_{-i})$ above. By the formula of the determinant for Vandermonde matrices, we have 
\begin{align*}
\det\big(V(-1; \alpha_{-i})\big) &= \prod_{j \in [\ell] \setminus \{i \}} (\alpha_j + 1) \cdot \prod_{\substack{j_1, j_2 \in [\ell] \setminus \{i\} \\ j_1 < j_2}} (\alpha_{j_2} - \alpha_{j_1}) \qquad\text{and}\\[0.6ex]
\det(V ) &= \prod_{j > i} (\alpha_{j} - \alpha_i) \prod_{j < i} (\alpha_i - \alpha_j) \cdot \prod_{\substack{j_1, j_2 \in [\ell] \setminus \{i\} \\ j_1 < j_2}} (\alpha_{j_2} - \alpha_{j_1}),
\end{align*}
and $|z_i|$ is given by 
\begin{align*}
|z_i| = \prod_{j \in [\ell] \setminus \{i \}} \dfrac{\alpha_j + 1}{|\alpha_j - \alpha_i|} = \prod_{j \in [\ell] \setminus \{i\}} \frac{\alpha_j}{|\alpha_j - \alpha_i|} \left( 1 + \frac{1}{\alpha_j} \right) = \prod_{j \in [\ell] \setminus \{i\}} \frac{\alpha_j}{\alpha_j - \alpha_i} \prod_{j \in [\ell] \setminus \{i\}} \left( 1 + \frac{1}{\alpha_j} \right),
\end{align*}
and therefore,
\begin{align*}
    |z_i| \leq \left| \prod_{j \in [\ell]\setminus\{i\}} \frac{\alpha_j}{\alpha_j - \alpha_i} \right| \cdot \exp\left( \sum_{j=1}^{\ell} \frac{1}{j^3} \right) \leq O(1) \cdot \left| \prod_{j \in [\ell]\setminus\{i\}} \frac{\alpha_j}{\alpha_j - \alpha_i} \right|.
\end{align*}
Finally, the bound on the $\|z\|_1$ follows from summing up the right-most expression above, which is $O(1)$, with the exact derivation in Claim~6.3 of~\cite{CJLW21b}.
\end{proof}


\begin{proof} (of Lemma~\ref{lem:one-dim-dist}) Consider the vector $z \in \R^{\ell+1}$ from \Clm{inv-and-z}.
For convenience we index $z$ using $0, \ldots,\ell$. Note that $z_0 = 1$. Let $N \subset [\ell]$ be the 
set of coordinates that are negative, and $P \subseteq [\ell]$ be the set of positive coordinate. (We do 
not put the index $0$ in $P$, since we treat $z_0 = 1$ separately.

The distribution $\cA$ is supported on $\{i^3 | i \in N\}$. The probability of $i^3$ is $z_i/\|z\|_1$,
and the probability of $0$ is $1 - \sum_{i \in N} z_i/\|z\|_1$.

The distribution $\cB$ is supported on $\{-1\} \cup \{j^3 | j \in P\}$. The probability of $-1$ is $z_0/\|z\|_1$,
the probability of $j^3$ is $z_j/\|z\|_1$, and the probability of $0$ is the remainder $1 - z_0/\|z\|_1 - \sum_{j \in P} z_j/\|z\|_1$.

The first bullet of the lemma holds by the above construction. For the second lemma, consider the row of the matrix $A$
with the $k$th powers. Since $Az = 0$, that row leads to the equation $(-1)^k z_0 + \sum_{j \in P} \alpha^k_j z_j = \sum_{i \in N} \alpha^k_i z_i$.
Dividing by $\|z\|_1$, we deduce that $\EX_{j \sim \cB}[j^k] = \EX_{i \sim \cA} [i^k]$.
Finally, since $\|z\|_1 = O(1)$, $\Pr_{j \sim \cB}[j = -1] = z_0/\|z\|_1 = 1/\|z\|_1 = \Omega(1)$.
\end{proof}

\subsection{The distributions $\cD_{yes}$ and $\cD_{no}$} \label{sec:dist}

Given any ``mean" vector $\bmu = (\bmu_1, \bmu_2, \ldots, \bmu_n) \in [-1,1]^n$, we can define a product distribution
on $\{-1,1\}^n$ as follows. We set each coordinate $x_i$ to be $1$ with probability $(1+\bmu_i)/2$ and zero 
with probability $(1-\bmu_i)/2$. Note that $\EX[x_i] = \bmu_i$.

Recall the distributions $\cA$ and $\cB$ given in \Lem{one-dim-dist}. 
We set $\ell = \log n/\log\log n$.

We now define $\Dyes$ and $\Dno$, which are distributions over 
distributions on $\{-1,1\}^n$. For $\Dyes$,
we generate $n$ independent entries $(a_1, a_2, \ldots, a_n)$ from $\cA$. 
We applying a scaling to define the mean vector $\bmu$ where $\bmu_i = \eps a_i/\sqrt{n}$.
We then take the corresponding distribution over $\{-1,1\}^n$. This describes a ``draw"
from $\cD_{yes}$.
The distribution $\cD_{no}$ is generated analogously from $\cB$.

Observe that all the coordinates means for $\Dyes$ are non-negative, while
a constant fraction of the corresponding means for $\Dno$ are negative.

% 
% 
% Having set distributions $\calD_y$ and $\calD_n$ as in Lemma~\ref{lem:one-dim-dist}, we define the distributions $\Dyes$ and $\Dno$ as follows. We let $\Dyes$ be the distribution over product distributions given by first letting $\bmu = (\bmu_1,\dots, \bmu_n) \in [-\alpha,\alpha]$ where each $\bmu_i \sim \calD_y$ is sampled independently, and letting $\bp$ be the product distribution whose mean vector is $\bmu$. Similarly, $\Dno$ is the distribution over product distribution given by first letting $\bmu = (\bmu_1,\dots, \bmu_n) \in [-\alpha,\alpha]$ where each $\bmu_i \sim \calD_n$ is sampled independently, and letting $\bp$ be the product distribution whose mean vector is $\bmu$. 
% 
\begin{lemma}\label{lem:mon-dist}
Every distribution in the support of $\Dyes$ is monotone.  
\end{lemma}

\begin{proof} Consider $\cD \sim \Dyes$. Since the mean vector is non-negative, for each coordinate $x_i$,
$\Pr[x_i = 1] \geq \Pr[x_i = -1]$. Since $\cD$ is a product distribution, this means that the probability
of a point cannot decrease if we flip a $-1$ (coordinate) to a $1$. Hence, $\cD$ is monotone.
% By definition of $\Dyes$, every distribution $p$ in the support of $\Dyes$ is a product distribution whose mean vector $\mu = (\mu_1,\dots, \mu_n)$ satisfies $\mu_i \geq 0$ for all $i \in [n]$. This comes from the first item of Lemma~\ref{lem:one-dim-dist}. Thus, suppose $x \in \{-1,1\}^n$ with $x_i = -1$ and $y = x^{(i)}$. Note that
% \begin{align*}
% p(x) = \prod_{j=1}^n \left(\frac{1 + x_j \cdot \mu_j}{2} \right),
% \end{align*}
% so $p(y) = (1+ \mu_i) / (1 - \mu_i) \cdot p(x)$. Since $\mu_i \geq 0$, $p(y) \geq p(x)$. This implies $p$ is monotone.
\end{proof}

\begin{lemma}\label{lem:far-mon-dist}
A distribution $\bp \sim \Dno$ is $\Omega(\eps)$-far from monotone with probability at least $0.99$.
\end{lemma}

\begin{proof}
Let $\bp \sim \Dno$ and let $\bmu = (\bmu_1,\dots, \bmu_n)$ be the corresponding mean vector, where $\bmu_i\sim \calD_n$ for all $i \in [n]$. 
Let $\bN \subset [n]$ denote the coordinates $i \in [n]$ where $\bmu_i = -\eps / \sqrt{n}$. From the third item of Lemma~\ref{lem:one-dim-dist} and standard concentration inequalities, \smash{$|\bN| \eqdef m$} has size at least $c_0 n / 2$ with probability $1-o(1)$. We assume this is the case and that $m$ is even (so $\bN$ is a large subset of coordinates where $\bmu_i$ is negative). We will lower bound 
the distance to monotonicity by showing that there exists a matching $\calM$ of pairs $(x, y)$ in $\{-1,1\}^n \times \{-1,1\}^n$ (which depends on $\bp$) where $x_i \leq y_i$ for every $i \in [n]$ and
\begin{align*}
\sum_{(x,y) \in \calM} ( \bp(x) - \bp(y) ) = \Omega(\eps).
\end{align*}
Assuming this bound, we prove that $\|\bp - g\|_1 = \Omega(\eps)$ for any monotone $g$, which
implies that $\dtv(\bp, g)\ge \Omega(\eps)$. Consider $(x,y) \in \calM$.
Suppose $\max(|\bp(x) - g(x)|, |\bp(y) - g(y)|) < (\bp(x) - \bp(y))/2$.
Then $g(x) > \bp(x) - (\bp(x) - \bp(y))/2 = (\bp(x) + \bp(y))/2$.
And $g(y) < \bp(y) + (\bp(x) - \bp(y))/2 = (\bp(x) + \bp(y))/2$. So $g(x) > g(y)$,
contradicting the fact that $g$ is monotone. Hence, $|\bp(x) - g(x)| + |\bp(y) - g(y)| \geq (\bp(x) - \bp(y))/2$.
Summing over all pairs $(x,y) \in \calM$, $\|\bp - g\|_1 \geq \sum_{(x,y) \in \calM} ( \bp(x) - \bp(y) ) = \Omega(\eps)$.

% 
%  because, for any monotone distribution $g \colon \{-1,1\}^n \to \R_{\geq 0}$,
% \begin{align*}
% \dtv(\bp, g) = \frac{1}{2} \sum_{z \in \{-1,1\}^n} \left| \bp(z) - g(z) \right| \geq \frac{1}{2} \sum_{(x,y) \in \calM} \big( \left| \bp(x) - g(x) \right| + \left| \bp(y) - g(y) \right| \big),
% \end{align*}
% and we lower bound $|\bp(x) - g(x)| + |\bp(y) - g(y)|$ by $(\bp(x) - \bp(y))^+$ term-by-term: the case $\bp(x) \leq \bp(y)$ is trivial; in the case $\bp(x) > \bp(y)$, we have that $g(x) < g(y)$, so $\bp(x) - \bp(y) = \bp(x) - g(x) + g(x) - \bp(y) \leq \bp(x) - g(x) + g(y) - \bp(y)$, which implies the desired bound. 
% 
In order to describe the matching $\calM$, we first let $|z|$ denote the number of entries in $z \in \{-1,1\}^m$ which are set to $1$. We consider a bijection $\sigma$ which maps vectors $z \in \{-1,1\}^m$ with $|z| = m / 2 - r$ to $\sigma(z) \in \{-1,1\}^m$ with $|\sigma(z)| = m/2 + r$ for every $r \in [m/2]$ and satisfies $z \preceq \sigma(z)$. In other words, $\sigma$ maps the bottom-half of the hypercube $\{-1,1\}^m$ to a comparable element in the top half; the fact such matchings exist follows straight-forwardly from chain decompositions of $\{-1,1\}^m$. Let
\begin{align*}
\calM = \left\{ (x, x_{\ol{\bN}}\sigma(x_{\bN}) : x_{\bN} \in \{-1,1\}^m \text{ contains at most $m/2$ entries set to $1$}\right\},
\end{align*}
where we use the notation $x_{\ol{\bN}} \sigma(x_{\bN})$ to denote the string in $\{-1,1\}^n$ whose $i$-th coordinate is $x_i$ if $i \in \ol{\bN}$ and $\sigma(x_{\bN})_i$ if $i\in \bN$. The fact $z \preceq \sigma(z)$ and $\sigma$ is a bijection gives us the desired matching. Notice that, whenever $(x,y) \in \calM$ with $|x| = m/2-r$, we have
\begin{align*} 
\bp(y) &= \bp(x) \cdot \prod_{j \in \bN} \dfrac{1 + y_j \cdot \bmu_j}{1 + x_j \cdot \bmu_j}  = \bp(x) \left(1 - \frac{c_0 \cdot \eps}{\sqrt{n}} \right)^{2r} \left( 1 + \frac{c_0\cdot \eps}{\sqrt{n}}\right)^{-2r} \leq \bp(x) \left(1 - \frac{c_0\cdot \eps}{\sqrt{n}}\right)^{2r}.
\end{align*}
where the second expression is obtained by counting the number of coordinates $j \in \bN$ where $y_j$ is $1$ or $-1$, and comparing that to the number of coordinates $j \in \bN$ where $x_j$ is $1$ or $-1$, as well as the fact that every $j \in \bN$ has $\bmu_j= -c_0 \eps /\sqrt{n}$. Therefore, since $2r = 2(m/2 - |x_{\bN}|)$ for each $x$ above, we have
\begin{align*}
\sum_{(x,y) \in \calM} \left( \bp(x) - \bp(y) \right) &\geq \sum_{\substack{x \in \{-1,1\}^{n} \\ |x_{\bN}| < m/2 }} \bp(x) \left( 1 - \left( 1 - \frac{c_0 \cdot \eps}{\sqrt{n}}\right)^{2(m/2-|x_{\bN}|)}\right) \\
            &\geq \sum_{\substack{x \in \{-1,1\}^n \\ |x_{\bN}| < m/2}} \bp(x) \cdot \frac{2 c_0 \eps \cdot (m/2 - |x_{\bN}|)}{\sqrt{n}} = \frac{2c_0\eps}{\sqrt{n}}\cdot  \Ex_{\bx \sim \bp}\left[ \left(\frac{m}{2} - |\bx_{\bN}| \right)^+\right].
\end{align*}
Finally, note that because each $\bx_i$ when $\bx \sim \bp$ is independent, and for every $i \in \bN$, the probability that $\bx_i = 1$ is smaller than $1/2$ (recall $\bN$ are exactly the negatively-biased coordinates), standard anti-concentration inequalities imply that with constant probability over $\bx \sim \bp$, we have $|\bx_{\bN}| \leq m/2 - \sqrt{m} = \Omega(\sqrt{n})$. This gives the $\Omega(\eps)$ lower bound on the distance to monotonicity and finishes the proof of the lemma.
\end{proof}

% \newcommand{\mono}{\mathrm{mono}}


\subsection{Indistinguishability of $\Dyes$ and $\Dno$}\label{sec:indist}

Recall that $\ell = \log n/\log\log n$. For convenience, we set $\alpha = \eps \ell^3/\sqrt{n}$,
the largest possible value of the mean vectors.

We show (\ref{eq:indistinguish}) with the following approach. First, we note that, since the algorithm receives $q$ independent samples from an $n$-dimensional product distribution, it suffices to consider draws to a ``vector of counts''. Namely, given a sequence of samples $x_1, \dots, x_q \in \{-1,1\}^n$, let $r(x_1, \dots, x_q)$ be the $n$-dimensional vector of integers, where
\[ r(x_1,\dots, x_q)_i = \sum_{k=1}^q \ind\{ (x_k)_i = 1 \}. \]
Then, we let $\calR_y$ (resp. $\calR_n$) denote the distribution given by (i) sampling $\bp \sim \Dyes$ (or, $\bp \sim \Dno$), (ii) letting $\bx_1,\dots, \bx_q \sim \bp$, and (iii) outputting $r(\bx_1,\dots, \bx_q)$. One may equivalently sample $\bx_1,\dots, \bx_q$ from $\Dyes$ or $\Dno$ by sampling $\boldr \sim \calR_y$ or $\calR_n$ and then generating the samples $\bx_1,\dots, \bx_q$ conditioned on the vector of counts $\boldr$. Thus, we will derive (\ref{eq:indistinguish}) by showing that 
$$\dtv\big(\calR_y, \calR_n\big) = o(1),
\quad\text{when $q\le \frac{n}{\eps^2\cdot
\text{polylog}(n)}$}.$$ 
Toward this end, we define $G \subset \Z_{\geq 0}^n$ (in Lemma \ref{lemma:hehe} below) and show that
\begin{itemize}
    \item Lemma \ref{lemma:hehe}:
 $\boldr \in G$ with probability at least $1 - o(1)$ over $\boldr \sim \calR_y$ and $\calR_n$; and
 \item Lemma \ref{lemma:haha}: 
For every count vector $x \in G$, we have
\begin{align*}
\dfrac{\Prx_{\boldr \sim \calR_y}\left[ \boldr = x \right]}{\Prx_{\boldr \sim \calR_n}\left[ \boldr = x \right]} = 1 \pm o(1).
\end{align*}
\end{itemize}
These two lemmas together imply the desired bound on the total variation distance.

\begin{lemma}\label{lemma:hehe}
    For any $q \in \N$, let $G \subset \Z_{\geq 0}^n$ denote the set of vectors $r$ 
      that sum to $q$ such that 
    \[ \frac{q}{2} - \frac{q\cdot \alpha}{2} 
 - \sqrt{q} \log n \leq r_i \leq \frac{q}{2} + \frac{q\cdot \alpha}{2} 
 + \sqrt{q} \log n \]
 for all $i\in [n]$.
 Then, for $\calR$ being both $\calR_y$ and $\calR_n$, we have
    \begin{align*}
        \Prx_{\boldr \sim \calR}\big[ \boldr \notin G \big] = o(1).
    \end{align*}
\end{lemma}

\begin{proof}
    Note that, each of the $n$ coordinates of $\boldr \sim \calR$ is independent and identically distributed; it is given by letting $\bmu \sim \Dyes$ (or $\Dno$), and then letting $\boldr_i \sim \Bin(q, \frac{1 + \bmu}{2})$. Since $\bmu$ is always below $\alpha$ and above $-\alpha$, so $\boldr_i$ and $-\boldr_i$ are stochastically dominated by $\Bin(q, (1+\alpha)/2)$. The bound then follows from a union bound and standard concentration arguments, using the fact that $\alpha$ is small (say, smaller than $0.01$).
\end{proof}


\begin{lemma}\label{lemma:haha}
%Let $\Dyes$ and $\Dno$ be two distributions supported on $[-\alpha, \alpha]$ where \smash{$\alpha \leq \frac{\eps\log^3 n}{\sqrt{n}}$} which agree on the first \smash{$\ell = \frac{\log n}{\log \log n}$}. Then, for 
Assume that $q \leq n / (\eps^2 \cdot \polylog(n))$.
For any $c \in G$, we have
\begin{align*}
    \dfrac{\Prx_{\boldr \sim \calR_y}\left[ \boldr = c \right]}{\Prx_{\boldr \sim \calR_n}\left[ \boldr = c \right]} = 1 \pm o(1).
\end{align*}
\end{lemma}

\begin{proof}
 For $\calR$ being $\calR_y$ or $\calR_n$ and $\calD$ being $\calD_y$ or $\calD_n$
   accordingly, 
   we write down $\Pr_{\boldr\sim \calR} [\boldr=c]$ as
   \begin{align*}
        \Prx_{\boldr \sim \calR}\left[ \boldr = c\right] &= \prod_{i=1}^n \Ex_{\bmu \sim \calD}\left[ \binom{q}{c_i} \left(\frac{1 + \bmu}{2}\right)^{c_i} \left( \frac{1 - \bmu}{2} \right)^{q-c_i} \right] \\
                        &= \prod_{i=1}^n \binom{q}{c_i} \cdot 2^{-q} \cdot \Ex_{\bmu \sim \calD}\left[ \left(1 - \bmu^2 \right)^{q/2 - d_i} \left(1+ \sigma_i \bmu \right)^{2 d_i} \right],
    \end{align*}
where $0\le d_i\le q/2$ and $\sigma_i \in \{-1,1\}$ uniquely satisfy $c_i - \sigma_i d_i = q/2$.
%   we first evaluate the probability that $\boldr \sim \calR$ is some vector $c \in G$. For simplicity in the notation, let $d_i \in \Z_{\geq 0}$ and $\sigma_i \in \{-1,1\}$ be the parameters of $c$ so that $c_i + \sigma_i d_i = q$. 

Note $c \in G$ implies $d_i \leq q \alpha/2 + \sqrt{q} \log n \leq \sqrt{n} / (\eps \cdot \polylog(n))$. 
        Furthermore, for any $\mu \in [-\alpha, \alpha]$, % and any $d_i \leq q$, and $\sigma_i$, 
    \begin{align}
    \left(1 - \mu^2\right)^{q/2 - d_i}\left(1+ \sigma_i \mu\right)^{2d_i} = \sum_{\ell_1 = 0}^{q/2-d_i} \sum_{\ell_2 = 0}^{2d_i} \binom{q/2-d_i}{\ell_1} \binom{2d_i}{\ell_2} (-1)^{\ell_1} \sigma_i^{\ell_2} \mu^{2\ell_1 + \ell_2}. \label{eq:expression} \end{align}
    We consider the degree-$\ell$ Taylor expansion $T_{\ell}(\mu,d_i,\sigma_i)$ (with respect to $\mu$) at $0$ of (\ref{eq:expression}), and we have that the error in the expression becomes at most
    \begin{align*} 
    &\sum_{t > \ell} \sum_{\ell_1=0}^{q/2-d_i} \sum_{\ell_2=0}^{2d_i} \ind\{ 2\ell_1 + \ell_2 = t \} \cdot \binom{q/2-d_i}{\ell_1} \binom{2d_i}{\ell_2} \cdot \alpha^{t} \\
        &\qquad\qquad \leq \sum_{t > \ell} t\cdot \left( q^{1/2} + 2d_i\right)^{t} \cdot \alpha^{t} \leq \sum_{t > \ell} \left( 2\alpha \sqrt{q} + 4\alpha d_i  \right)^{t} %\lsim \left(\alpha \sqrt{q} + 2\alpha d_i \right)^{\ell+1} 
        \leq \frac{1}{n^{10}},
    \end{align*}
    as long as $\alpha \sqrt{q} + \alpha d_i < 1 / \polylog(n)$ for a large enough polynomial, given that $\ell=\log n / \log \log n $, which it is by setting of $\alpha, q$ and $d_i$. Notice, furthermore, that (\ref{eq:expression}) is at least $1 - O(q\mu^2) - O(d_i \mu) \geq 1/2$. Hence, the ratio of the two probabilities, when $\calR$ is $\calR_y$ and $\calR_n$ is
    \begin{align*}
        \prod_{i=1}^n \dfrac{\Ex_{\bmu \sim \calD_y}\left[ T_{\ell}(\bmu, d_i,\sigma_i) \pm 1/n^{10}\right]}{\Ex_{\bmu \sim \calD_n}\left[ T_{\ell}(\bmu, d_i,\sigma_i) \pm 1/n^{10}\right]}=\prod_{i=1}^n \left(1\pm O(1/n^{10})\right) = 1 \pm O(1 / n^{9}),
    \end{align*}
    where we used the fact that the expected Taylor expansion to degree $\ell$ is equal for $\calD_y$ and $\calD_n$, since it is a function of the first $\ell$ moments of $\calD_y$ and $\calD_n$. 
\end{proof}
