%\documentclass[10pt]{article}
\documentclass[journal,onecolumn]{IEEEtran}
\usepackage[margin=0.7in,footskip=0.25in]{geometry}
\date{}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{amsmath, amsthm, amssymb, dsfont}
\usepackage{graphicx}
\usepackage{comment, cite}
\usepackage{psfrag}
\usepackage{bbm}
%\usepackage{appendix}
\usepackage{csquotes}



\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}

\usetikzlibrary{fit}					% fitting shapes to coordinates
\usetikzlibrary{backgrounds}	


%% Changed
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{discussion}{Discussion}
\newtheorem{remark}{Remark}

\definecolor{darkblack}{rgb}{0, .07, .5}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{mahogany}{rgb}{0.65, 0., 0.5}

%**********************************
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}

%*******************************
\def\Pr{\text{\rm{Pr}}}
\def\T{\text{T}}
\def\det{\text{det}}


%*********************************
\newcommand{\bR}{\mathbb{R}}

\newcommand{\tr}{\text{\rm{tr}}}
\newcommand{\fR}{\mathfrak{R}}
\newcommand{\fS}{\mathfrak{S}}
\newcommand{\fC}{\mathfrak{C}}

\newcommand{\Var}{\text{\rm{Var}}}


\newcommand{\supp}{\text{\rm{supp}}}


\newcommand{\PR}{\text{\rm{PR}}}
\newcommand{\CHSH}{\text{\rm{CHSH}}}


\newcommand{\E}{\mathbb{E}}

\newcommand{\wA}{\widetilde{A}}
\newcommand{\wB}{\widetilde{B}}
\newcommand{\wX}{\widetilde{X}}
\newcommand{\wY}{\widetilde{Y}}
\newcommand{\wT}{\widetilde{T}}
\newcommand{\wS}{\widetilde{S}}

\newcommand{\wOmega}{\widetilde{ w }}
\newcommand{\wPi}{\widetilde{\Pi}}


\newcommand{\wa}{\tilde{a}}
\newcommand{\wb}{\tilde{b}}
\newcommand{\wx}{\tilde{x}}
\newcommand{\wy}{\tilde{y}}
\newcommand{\wt}{\tilde{t}}
\newcommand{\ws}{\tilde{s}}
\newcommand{\womega}{\tilde{ w }}
\newcommand{\wpi}{\tilde{\pi}}

\renewcommand{\arraystretch}{1.5}

\let\emptyset\varnothing

\setcounter{secnumdepth}{3}

\newcommand{\Nash}{\text{Nash}}
\newcommand{\RN}[1]{%
	\textup{\uppercase\expandafter{\romannumeral#1}}%
}


\title{On the Source Model Key Agreement Problem}
\author{Hamidreza Abin and Amin Gohari\\
Department of Information Engineering\\
The Chinese University of Hong Kong\\
Sha Tin, NT, Hong Kong\\
\{hamabin,agohari\}@ie.cuhk.edu.hk
}


% \author{%
%   \IEEEauthorblockN{ Hamidreza Abin and Amin Gohari}
%   \IEEEauthorblockA{Department of Information Engineering\\
% The Chinese University of Hong Kong\\
% Sha Tin, NT, Hong Kong\\
% \{hamabin,agohari\}@ie.cuhk.edu.hk}
% }
\allowdisplaybreaks
\begin{document}
	\maketitle
  
\begin{abstract}
We consider the source model key agreement problem involving two legitimate parties and an eavesdropper who observe 
$n$ i.i.d.\ samples of $X$ and $Y$ and $Z$ respectively. The best-known upper bound on the key capacity is characterized by an inf-max optimization problem that generally lacks a closed-form solution. In this paper, we solve the optimization for some class of sources, thereby providing simple expressions for the upper bound. We provide general conditions under which the upper bound reduces to $I(X;Y)$. As an example, we consider the XOR setting in which 
$X$ and $Y$ are binary, and $Z$
 is the XOR of $X$ and $Y$. The upper bound reduces to $I(X;Y)$ for this source. Next, we conjecture that the rate $I(X;Y)$ is not achievable for the XOR source, and provide some ideas that might be useful for developing a new upper bound on the source model problem.
 
 
 

\end{abstract}

\section{Introduction}
 

The two-party source model problem consists of two legitimate parties, Alice and Bob, who aim to establish a shared secret key hidden from an eavesdropper, Eve \cite{1,10}. Alice, Bob, and Eve observe 
$n$ i.i.d.\ samples of $X$ and $Y$ and $Z$ respectively. Random variables $X,Y$ and $Z$ are distributed according to $P_{X,Y,Z}$, known to all parties. Through authenticated but public communication, Alice and Bob collaborate to generate a key that remains secret from Eve (extensions to multiple parties and continuous random variables can be found in \cite{CsNar,Chanomni,Nitinawarat,Tyagi,Chan2,chan2018optimality}; please see \cite{elgamal} for a detailed treatment). 
 In a non-interactive setting, Alice and Bob locally generate messages $F_1$ and $F_2$ from $P_{F_1|X^n}$ and $P_{F_2|Y^n}$, respectively, and exchange these messages.
 Then Alice and Bob create their own key from 
$K_A\sim P_{K_A|X^n,F_1,F_2}$ and  $K_B\sim  P_{K_B|Y^n,F_1,F_2}$ respectively.  On the other hand, in the interactive setting,  Alice initiates the process by generating a public message $F_1$ using the distribution $P_{F_1|X^n}$
 and shares it with Bob. Bob then responds with $F_2$
 derived from $P_{F_2|Y^n,F_1}$. This exchange continues for $r$ rounds, during which both senders alternately generate and share public messages
 $F_3,F_4,\cdots F_r$. At the end of these interactions, Alice derives a key $K_A$	based on
 $P_{K_A|X^n,F_{1:r}}$ while Bob produces a corresponding key $K_B$ using $P_{K_B|Y^n,F_{1:r}}$.
In an
$(n,\delta)$ code, the keys must match with probability $1-\delta$:
\begin{align}
    \mathbb{P}[K_A=K_B]\geq 1-\delta,\label{pd1}
\end{align}
where 
$\delta$ is a small positive value.
The key rate is defined as $R=\frac{1}{n}H(K_A)$. $R$ must exhibit high entropy, quantified as:
\begin{align}
    R\geq \frac{1}{n}\log(|\mathcal{K}|)-\delta,\label{pd2}
\end{align}
where  
$\mathcal{K}$ denotes the alphabet of 
$K_A$ and $K_B$.  Furthermore, the keys should be nearly independent of Eve’s observations, formalized by:
\begin{align}
    \frac{1}{n}I(K_A;Z^n,F_{1:r})\leq \delta.\label{pd3}
\end{align}
Note that the value of $r$ for non-interactive regime is $2$. 

The maximum achievable key rate is known as the source model secret-key (SK) capacity. It is denoted by 
$S(X;Y\|Z)$ in the interactive setting, and by
$S_{\text{ni}}(X;Y\|Z)$ in the non-interactive setting. 
 

Finding the secret key capacity remains an open problem in general scenarios, including both the interactive and non-interactive communication settings. Conditions that imply $S(X;Y\|Z)>0$ are studied in  \cite{10,MaurerWolf99,OrlitskyWigderso,gohari2020coding}.
The exact formula for secret key capacity is known only for some instances, such as when $X,Y$ and $Z$ form a Markov chain in some order. As a side result, we derive the capacity for a new class of sources, which we call ``deterministic-erasure" sources in Appendix \ref{appndx}.

One scenario where the capacity is known occurs when communication is restricted to one-way transmission from one party to another.
 The one-way SK capacity from $X$ to $Y$ equals \cite{1}:
\begin{align}
&S_{\text{ow}}(X;Y\|Z)=\max_{U,V\rightarrow X\rightarrow YZ} I(U;Y|V) - I(U;Z|V),
\end{align}
where the conditions $|\mathcal{V}|\leq |\mathcal{X}|$ and $|\mathcal{U}|\leq |\mathcal{X}|$ can be imposed on $\mathcal{V}$, $\mathcal{U}$, and $\mathcal{X}$ representing the alphabets associated with $V$, $U$, and $X$, respectively \cite{elgamal}. It is evident that $S_{\text{ow}}(X;Y\|Z)$ establishes a lower bound on the SK capacity $S(X;Y\|Z)$. 

An early upper bound on the key capacity was \cite{10,1}
$$S(X;Y\|Z)\leq I(X;Y|Z).$$
Authors in \cite[pp. 1126, Remark 2]{1},\cite{MaurerWolf99}
%{AhlswedeCsiszar}
improved this upper bound by realizing that degrading Eve cannot increase the key rate, i.e., for any degradation $J$ of $Z$ through a Markov chain $J\rightarrow Z\rightarrow (X,Y)$ we have
$$S(X;Y\|Z)\leq S(X;Y\|J)\leq I(X;Y|J).$$
The quantity
$$\min_{J\rightarrow Z\rightarrow (X,Y)}I(X;Y|J)$$
is called  the intrinsic conditional mutual information.  
More generally, we can relax the degraded assumption $J\rightarrow Z\rightarrow (X,Y)$ to a less noisy condition. The inequality 
$$S(X;Y\|Z)\leq S(X;Y\|J)$$ holds if the channel $P_{Z|X,Y}$ is less noisy than the channel $P_{J|X,Y}$. Even more generally, for any arbitrary $P_{J|X,Y}$, we  have \cite{gohari-terminal1}
\begin{align}S(X;Y\|Z)-S(X;Y\|J)\leq \max_{U,V\rightarrow X,Y\rightarrow J,Z} I(U;J|V)-I(U;Z|V)\label{eqnd1}\end{align} 
Observe that the right hand side of the above equation vanishes when $P_{Z|X,Y}$ is less noisy than the channel $P_{J|X,Y}$, and was interpreted as ``the penalty of deviating
from the less-noisy condition" in \cite{gohari2020coding}. Using \eqref{eqnd1} and the upper bound $$
S(X;Y|J)\leq I(X;Y|J)$$
the following upper bound was proposed in \cite{gohari-terminal1}:
 \begin{align}
 %\overset{(a)}{\leq}
S(X;Y|Z)\leq \Psi(X;Y\|Z)&\triangleq\inf_J\max_{U,V\rightarrow X,Y\rightarrow J,Z} I(X;Y|J)+I(U;J|V)-I(U;Z|V)
    \label{main1}
 \end{align}
 Note that $J$ serves as an auxiliary receiver in the sense of \cite{gon21}. 
 The above bound is the best known upper on the key capacity. It involves a min-max optimization problem, which makes computing the upper bound challenging. In fact, it is not known whether the infimum over $J$ in \eqref{main1} is a minimum (see \cite{gohari-terminal1} for a discussion on the computability of this bound). A weaker, but more convenient upper bound was also proposed in  \cite{gohari-terminal1}

 \begin{proposition}[\cite{gohari-terminal1}]\label{lem1}
    For any $P_{X,Y,Z}$ we have:
    \begin{align}
   S(X;Y\|Z)&\leq \inf_J\max_{U,V\rightarrow XY\rightarrow JZ} I(X;Y|J)+I(U;J|V)-I(U;Z|V)\label{main2-1}
 \\&\leq \min_JI(X;Y|J)+I(X,Y;J|Z).\label{main2}
\end{align}
\end{proposition}
We denote $$\hat{\Psi}(X;Y\|Z)\triangleq\min_{J}I(X;Y|J)+I(X,Y;J|Z).$$ 
Notice that  the upper bound $\Psi(X;Y\|Z)$ in \eqref{main1} depends solely on 
$P_{X,Y,J}$ and $P_{X,Y,Z}$, meaning the minimization is restricted to  $P_{J|X,Y}$. However the weaker  upper bound $\hat{\Psi}(X;Y\|Z)$ in \eqref{main2}  requires minimization over 
 $P_{J|X,Y,Z}$. On the other hand, the weaker upper bound $\hat{\Psi}(X;Y\|Z)$ does not involve the auxiliary random variables $U$ and $V$. 

The main results of this paper are as follows:
\begin{itemize}
    \item We observe that $\hat{\Psi}(X;Y\|Z)=\Psi(X;Y\|Z)$ if $Z$ is a function of $(X,Y)$. This is immediate by the choice of $U=X,Y$ and $V=Z$ in \eqref{main2-1} which would reduce the expression to that in \eqref{main2}.
    \item We find sufficient conditions that imply that
    $$\hat{\Psi}(X;Y\|Z)=\Psi(X;Y\|Z)=I(X;Y).$$
    In particular, the sufficient conditions are satisfied when $X$ and $Y$ are binary random variables (with an arbitrary joint distribution) and  $Z$ is the XOR of $X$ and $Y$. We call this setting the XOR case.
    \item We conjecture that $S(X;Y\|Z)\neq I(X;Y)$ in the XOR case. Showing this would require developing a new upper bound for the source model problem. Partial results in our attempt to develop a new upper bound are provided as well.
\end{itemize}




The rest of this paper is organized as follows: Section \ref{sec:main:re} evaluates the upper bound $\Psi(X;Y\|Z)$ for some settings. Finally, Section  \ref{dis:tightness} discusses the tightness of the upper bound in the XOR case.
%Finally, we conclude the paper in section \ref{conclusion}. 
Additionally, in Appendix \ref{appndx}, we compute the secret key capacity for a class of sources where $Z$ is a function of $X$ and $P_{Y|X}$ corresponds to an erasure channel.

%-----------------------------------------------

\emph{Notation and Definitions:} Random variables are represented by uppercase letters, whereas their realizations are indicated by lowercase letters. It is assumed that all random variables possess finite alphabets. Sets are denoted by calligraphic letters. 
 The notation $X\rightarrow Y\rightarrow Z$ signifies that $X$ and $Z$ are conditionally independent given $Y$, which implies that $P_{XZ|Y}=P_{X|Y}P_{Z|Y}$. In this scenario, we assert that $X\rightarrow Y\rightarrow Z$ constitutes a Markov chain. The sequences $(U_1, U_2, \ldots, U_{i})$  and $(U_i,U_{i+1},\cdots,U_n)$ are denoted as $U^i$ and $U_{i}^{n}$ respectively. For a sequence $X^n=(X_1, X_2,\cdots, X_n)$, we use $X_{\backslash i}$ to denote $X^{i-1}X_{i+1}^{n}$. The entropy of a random variable \(X\) is represented as \(H(X)\), while the mutual information between \(X\) and \(Y\) is denoted by \(I(X;Y)\). We say $Q_{X}\ll P_{X}$ if $Q_X(x)=0$ whenever $P_X(x)=0$. 

%-------------------------------------



\section{Main Results}\label{sec:main:re}
The upper bound \begin{align}
\hat{\Psi}(X;Y\|Z)&=\min_{J}I(X;Y|J)+I(X,Y;J|Z).
 \end{align}
 satisfies \begin{align}\hat{\Psi}(X;Y\|Z)\leq \min(I(X;Y), I(X;Y|Z)).\label{eqntu}\end{align} This follows from setting $J$ constant or $J=Z$. The main results of this section identify sufficient conditions for equality in \eqref{eqntu}. Applications will be provided in the next section.

\begin{theorem}\label{Th:gen:classic}
Take some arbitrary $P_{X,Y,Z}$. Assume that one can find an auxiliary random variable $T$ on some arbitrary alphabet such that 
$I(T;Z|X,Y)=I(X;Y|T)=I(T;Z)=0$. Then 
$$\hat{\Psi}(X;Y\|Z)=I(X;Y).$$
\end{theorem}
\begin{proof}
Take an arbitrary $P_{J|X,Y,Z}$. Consider the coupling 
    $$P_{T,X,Y,Z,J}\triangleq P_{T|X,Y,Z}P_{J|X,Y,Z}.$$
    Thus, we have
        $$I(T;Z)=I(X;Y|T)=I(T;J|X,Y)=I(T;J|X,Y,Z)=0.$$
We have
\begin{align*}I(X; Y)-I(X;Y|J)&=
I(J; X)-I(J;X|Y)\nonumber
\\&\leq I(J; T, X)-I(J; X|Y)\nonumber\\&\leq I(J; T, X)+I(J; T, Y)-I(J; X, Y)\nonumber
\\&\overset{(a)}{=} I(J; T, X)+I(J; T, Y)-I(J; X, Y,T)
\\&= I(J; T, X)-I(J; X|T,Y)\nonumber
\\&\overset{(b)}{\leq} I(J; T)\\&\leq I(J, Z; T)
\\&\overset{(c)}{\leq} I(J; T|Z)\\&\leq I(J; T, X, Y|Z)\nonumber
\\&\overset{(d)}{=} I(J; X, Y|Z)
\end{align*}
where $(a)$ follows from $I(T;J|X,Y)=0$, $(b)$ follows from $I(X;Y|T)=0$,
$(c)$ follows from $I(T;Z)=0$ and $(d)$   follows from $I(T;J|X,Y,Z)=0$.
\end{proof}
\begin{remark}
$\hat{\Psi}(X;Y\|Z)=I(X;Y)$ is equivalent with the inequality 
$$I(X;Y|J)+I(X,Y;J|Z)\geq I(X;Y), \qquad\forall P_{J|X,Y,Z}.$$
When $Z$ is a function of $(X,Y)$, this inequality can be equivalently written as
    $$I(J;X,Y,Z)\geq \frac{1}{2}I(J;X)+\frac{1}{2}I(J;Y)+\frac{1}{2}I(J;Z), \qquad\forall P_{J|X,Y,Z}.$$
This condition holds if and only if $(1/2,1/2,1/2)$ belongs to the hypercontractivity ribbon for random variables $(X,Y,Z)$ (remember that the hypercontractivity ribbon for random variables $X_1, X_2, \cdots, X_k$ is defined as the set of $\lambda_i$ such that for any $P_{u|x_{[k]}}$ we have
$I(X_{[k]};U)\geq \sum_i \lambda_i I(X_i;U)$).
\end{remark}
\begin{remark} 
Theorem \ref{Th:gen:classic} readily extends to quantum systems: take quantum systems $X,Y,Z,J$ with some arbitrary joint density $\rho_{X,Y,Z,J}$. The inequality 
    $$I(X;Y|J)+I(X,Y;J|Z)\geq I(X;Y)$$
    holds if there is an extended state $\rho_{X,Y,Z,J,T}$ (whose reduced density on $X,Y,Z,J$ is the given state $\rho_{X,Y,Z,J}$) such that
    $$I(T;Z)=I(X;Y|T)=I(T;J|X,Y)=I(T;J|X,Y,Z)=0.$$
\end{remark}

The 
following lemma  provides an equivalent condition for the assumption of Theorem \ref{Th:gen:classic} for the special case of $Z$ being a function of $(X,Y)$.
\begin{lemma}\label{lem:con-hull}
Consider the special case of $Z=f(X,Y)$ for some arbitrary  function $f$. For a given joint distribution $P_{X,Y}$, there exists a random variable $T$ such that $I(Z;T|X,Y)=I(Z;T)=I(X;Y|T)=0$ if and only if $P_{X,Y}$ lies in the convex hull of the set 
     \begin{align}
        \mathcal A(P_Z)=\left\{Q_{X,Y}:I_{Q}(X;Y)=0,~Q_Z=P_Z\right\}.\label{con:formula}
    \end{align}
    \end{lemma}
    Proof of this lemma can be found in Appendix \ref{append2}.
  
Next, we find sufficient conditions that imply
$\hat{\Psi}(X;Y\|Z)=I(X;Y|Z).$
\begin{theorem} Assume that  $P_{Z|X,Y}$ is decomposed as follows 
$$P_{Z|X,Y}(z|x,y)=r(z,x)s(z,y), \qquad\forall x,y,z$$
for some functions $r(\cdot,\cdot)$ and $s(\cdot,\cdot)$. Then, 
$$\hat{\Psi}(X;Y\|Z)=I(X;Y|Z), \qquad\forall P_{X,Y}.$$


\end{theorem}
\begin{proof}
    In \cite{renner2002mutual}, it is shown that under the above condition, we have
$$I(X;Y|Z)\leq I(X;Y), \qquad\forall P_{X,Y}$$ 

Then, for every $P_{X,Y}$ and every $P_{J|X,Y,Z}$ we will have
$$I(X;Y|J)\geq I(X;Y|Z,J).$$
Therefore,
$$I(X;Y|J)+I(X,Y;J|Z)\geq I(X;Y|Z,J)+I(X;J|Z)=I(X;Y,J|Z)\geq I(X;Y|Z). $$
This shows that $J=Z$ in \eqref{main1} is an optimal choice.
\end{proof}
\begin{remark}
    It is shown in \cite[Lemma 2.3]{renner2002mutual} that the conditional distribution $P_{Z|X,Y}$ can be decomposed as 
$$P_{Z|X,Y}(z|x,y)=r(z,x)s(z,y), \qquad\forall x,y,z$$
for some functions $r(\cdot,\cdot)$ and $s(\cdot,\cdot)$ if and only if 
$$I_Q(X;Y|Z)=0$$
under $Q_{X,Y,Z}=Q_{X}Q_{Y}P_{Z|X,Y}$ where $Q_X$ and $Q_Y$ are uniform distributions.
\end{remark}
%{\color{red} can you show $\Psi(X;Y\|Z)=I(X;Y|Z)$?}

\iffalse
 
The following theorem asserts that considering a binary $J$ is sufficient to verify whether $\min_J[I(X;Y|J)+I(X,Y;J|Z)]< I(X;Y)$.

\begin{theorem}
The following statements are equivalent for any arbitrary $P_{X,Y}$:
\begin{enumerate}
    \item There is some $P_{J|X,Y}$ such that
$$\hat{\Psi}(X;Y\|Z)=\min_J[I(X;Y|J)+I(X,Y;J|Z)]<I(X;Y).$$
\item There is some $P_{J|X,Y}$ with $J$ being binary such that
$$\min_J[I(X;Y|J)+I(X,Y;J|Z)]<I(X;Y).$$
\item There is some $Q_{XY}\ll P_{X,Y}$ such that
$$D(Q_X\|P_X)+D(Q_Y\|P_Y)+D(Q_Z\|P_Z)>2D(Q_{X,Y}\|P_{X,Y})$$
\end{enumerate}
\end{theorem}
\begin{proof}
    The proof of this theorem follows from various characterizations of hypercontractivity ribbon. The equation $$\min_J[I(X;Y|J)+I(X,Y;J|Z)]\geq I(X;Y).$$
is equivalent with
$$I(J;X,Y,Z)\geq \frac12I(J;X)+\frac12I(J;Y)+\frac12I(J;Z), \qquad \forall p_{J|X,Y,Z}.$$

$$I(X;J)-I(X;J|Y)+I(J;Z)\leq I(X,Y,Z;J)$$
    The hypercontractivity ribbon for random variables $X_1, X_2, \cdots, X_k$ is defined as the set of $\lambda_i$ such that for any $p_{J|X_{[k]}}$ we have
$$I(X_{[k]};J)\geq \sum_i \lambda_i I(X_i;J).$$
Moreover, it suffices to consider binary random variable $U$.

\end{proof}





 \fi




 





\subsection{$Z$ is a function of $(X,Y)$}\label{function}
 In this subsection, we assume that $Z$ is a function of $X$ and $Y$. The following lemma is immediate by the choice of $U=X,Y$ and $V=Z$ in \eqref{main2-1} which would reduce the expression to that in \eqref{main2}.

 
 \begin{lemma}
 We have $\hat{\Psi}(X;Y\|Z)={\Psi}(X;Y\|Z)$ if $Z$ is a function of $(X,Y)$.
 \end{lemma}
 
 
 We will explore the value of $\hat{\Psi}(X;Y\|Z)$ under various scenarios based on the alphabet sizes of $X$ and $Y$.  
\subsubsection{$X$ and $Y$ are binary}
Assume that $X,Y\in \{0,1\}$ are binary random variable. Moreover, $$P_{X,Y}=\begin{bmatrix}
   p_{00}   & p_{01}     \\
    p_{10}  &p_{11}  
\end{bmatrix}.$$

Let $X\oplus Y$ denote the XOR of $X$ and $Y$, and $X\wedge Y$ denote the AND of $X$ and $Y$. 
We consider the three functions $X+Y, X\oplus Y$ and $X\wedge Y$ as follows:

$$Z_1=X+Y=\begin{cases}
        0& (X,Y)=(0,0)\\
        1& (X,Y)\in \{(0,1), (1,0)\}\\
        2& (X,Y)=(1,1)
    \end{cases}$$
    and
$$Z_2=X\oplus Y=\begin{cases}
        1& (X,Y)\in \{(0,0), (1,1)\}\\
        0& (X,Y)\in \{(0,1), (1,0)\}
    \end{cases}$$
and 
$$Z_3=X\wedge Y=\begin{cases}
        0& (X,Y)\in \{(0,1), (1,0),(0,0)\}\\
        1& (X,Y)=(1,1)
    \end{cases}.$$
We have the following Theorem.
\begin{theorem}\label{th:binary}The following statements hold for binary random variables $X$ and $Y$:
\begin{itemize}
    \item For any $P_{X,Y}$, we have
$\Psi(X;Y\|Z_2)=I(X;Y).$
\item If $Cov(X,Y)\leq 0$, then
$\Psi(X;Y\|Z_1)=\Psi(X;Y\|Z_3)=I(X;Y)$.  
\item  If $Cov(X,Y)> 0$, then
$\Psi(X;Y\|Z_1)=\Psi(X;Y\|Z_3)=0.$
\end{itemize}
\end{theorem}

Proof of this theorem can be found in Appendix \ref{append2}.



\subsubsection{Ternary-Binary}
 In this sub-section, we extend the alphabet of $Y$ to $\mathcal{Y}=\{0,1,2\}$. We give a condition for $\Psi(X;Y\|Z)=I(X;Y)$ when $Z=(X+Y)~ mod~ 2$. 
 

 
\begin{theorem}\label{th:ter-bin}Assume that $\mathcal{X}=\{0,1\}, \mathcal{Y}=\{0,1,2\}$ and $\mathcal{Z}=\{0,1\}$, and
\begin{align}\label{Z:function}
   Z=(X+Y)\mod 2=\begin{cases}
        0& (X,Y)\in \{(0,0), (1,1),(0,2)\}\\
        1& (X,Y)\in \{(0,1), (1,0),(1,2)\}\\
    \end{cases}
\end{align}
If $$\frac{P_{X,Y}(0,0)}{P_Z(0)}+\frac{P_{X,Y}(1,2)}{P_Z(1)}\leq 1$$ and $$\frac{P_{X,Y}(0,2)}{P_Z(0)}+\frac{P_{X,Y}(1,0)}{P_Z(1)}\leq 1,$$ then
$$\Psi(X;Y\|Z)=I(X;Y).$$
\end{theorem}
Proof of this theorem can be found in Appendix \ref{append2}.


\subsubsection{Arbitary Functions}
This part considers random variables $X,Y$ and $Z$  with arbitrary alphabet sizes. We aim to determine all functions $f:\mathcal{X}\times\mathcal{Y}\mapsto \mathcal{Z}$ for which the equation $\Psi(X;Y|Z)=I(X;Y)$ holds for every joint distribution $P_{X,Y}$.
\begin{theorem}\label{thm:thm5}
    Take some fixed alphabets $\mathcal{X}$ and $\mathcal{Y}$. Fix some function $f:\mathcal{X}\times\mathcal{Y}\mapsto \mathcal{Z}$ and assume that $Z=f(X,Y)$. We  have
$$\Psi(X;Y\|Z)=I(X;Y), \qquad \forall P_{X,Y}$$
if and only if either of the following holds:
\begin{itemize}
    \item $X$ and $Y$ are binary and we have the XOR mapping $f(0,0)=f(1,1)\neq f(1,0)=f(0,1)$,
    \item $f(x,y)$ is a constant function.
\end{itemize}
\end{theorem}
Proof of Theorem \ref{thm:thm5} can be found in Appendix \ref{append2}.












% $$p(z|x,y)=r(z,x)s(z,y), \qquad\forall x,y,z$$
% for some functions $r(\cdot)$ and $s(\cdot)$. Then, 
% $$\mathcal{U}(X;Y\|Z)=I(X;Y|Z), \qquad\forall P_{X,Y}.$$


% \end{theorem}
% \begin{proof}
%     In \cite{renner2002mutual}, it is shown that under the above condition we have
% $$I(X;Y|Z)\leq I(X;Y), \qquad\forall P_{X,Y}$$ 

% Then, for every $p_{X,Y}$ and every $p_{J|X,Y,Z}$ we will have
% $$I(X;Y|J)\geq I(X;Y|Z,J).$$
% Therefore,
% $$I(X;Y|J)+I(X,Y;J|Z)\geq I(X;Y|Z,J)+I(X;J|Z)=I(X;Y,J|Z)\geq I(X;Y|Z). $$
% This shows that $J=Z$ is an optimal choice.
% \end{proof}




\iffalse
\color{blue}
Consider three random variables  $X,Y$ and $Z$ defined on the set $\mathcal{X}=\{0,1\}, \mathcal{Y}=\{0,1,2\}$ and $\mathcal{Z}=\{0,1\}$ respectively.
Suppose that $p(Z=0)=\alpha, p(Z=1)=1-\alpha$ and 
$q(X=0)=a_0,q(X=1)=a_1, q(Y=0)=b_0,q(Y=1)=b_1$ and $q(Y=2)=b_2$. Assume  that $Z$ is a function of $X$ and $Y$ as follows:
\begin{align}\label{Z:function}
   Z=\begin{cases}
        0& (X,Y)\in \{(0,0), (1,1),(0,2)\}\\
        1& (X,Y)\in \{(0,1), (1,0),(1,2)\}\\
    \end{cases}
\end{align}
Under condition $I_{q}(X;Y)=0$, we conclude that $q(X,Y)=q(X)q(Y)$ and from condition $q(Z)=p(Z)$ we have:
\begin{align}
    &q_{00}+q_{02}+q_{11}=a_0b_0+a_0b_2+a_1b_1=\alpha\\
    &q_{01}+q_{10}+q_{12}=a_0b_1+a_1b_0+a_1b_2=1-\alpha
\end{align}
We show distribution $q$ in the following table:

\begin{center}
\begin{tabular}{ |c|c|c| } 
\hline
q(0,0) & q(0,1) & q(0,2) \\
\hline
q(1,0) & q(1,1) & q(1,2) \\ 
\hline
\end{tabular}
\end{center}
\fi








\iffalse
\newpage
 \subsection{Some Results}


\begin{itemize}


\item Two letter auxiliary receiver does not help in the bound $I(X;Y|J)+I(X,Y;J|Z)$:

\begin{align*}
    &I(X_AX_B;Y_AY_B|J)+I(X_AX_BY_AY_B;J|Z_AZ_B)
    \\&\geq 
I(X_A;Y_A|J)+I(X_AY_A;J|Z_A)
+I(X_B;Y_B|JX_AY_A)+I(X_BY_B;JX_AY_A|Z_B)
\end{align*}



    \item We have
\begin{align*}
    S(X;Y\|Z)&\leq \inf_J\max_{U,V\rightarrow X,Y\rightarrow J,Z} I(X;Y|J)+I(U;J|V)-I(U;Z|V)
    \\&\leq \min_J\left[I(X;Y|J)+I(X,Y;J|Z)\right]
\end{align*}
Note that the bound depends only on $P_{X,Y,J}$ and $P_{X,Y,Z}$, therefore minimization is over $P_{J|X,Y}$. The other upper bound has minimization over $P_{J|X,Y,Z}$. Therefore, we only need to consider $P_{J|X,Y}$. Therefore, in the upper bound $I(X;Y|J)+I(X,Y;J|Z)$ we can minimize over all coupling between $Z$ and $J$.

\item If for some $P_{T|X,Y}$  we have that
\[P_{X,Y}\mapsto I(X,Y;T|J)-I(X,Y;T|Z)\] is concave, we can include $T$ inside $V$. Note that the concavity implies that for any $U,V$ we have
\[
I(U;T|V,J)\geq I(U;T|V,Z)
\]
This is equivalent with
\[
I(U;T|V,J)-I(U;T|V)\geq I(U;T|V,Z)-I(U;T|V)
\]
or
\[
I(U;J|V,T)-I(U;J|V)\geq I(U;Z|V,T)-I(U;Z|V)
\]

In particular, if $H(Z|X,Y)=0$ then it is optimal to include $Z$ in $V$ in which case the bound becomes
$I(X;Y|J)+I(X,Y;J|Z)$. If $T$ is a common function of $X,Y$ and $Z$, we can also include $T$ in $V$.

\item If $H(J|X,Y)=0$, then $U=J$ is optimal. We get $I(X;Y|J)+H(J|V)-I(J;Z|V)=I(X;Y|J)+H(J|ZV)\leq I(X;Y|J)+H(J|Z)$. Thus, in this case, $U=J$ and empty $V$ is optimal. The other upper bound $I(X;Y|J)+I(X,Y;J|Z)$ will also yield $I(X;Y|J)+H(J|Z)$, so when either $H(J|X,Y)=0$ or $H(Z|X,Y)=0$, the upper bound becomes $I(X;Y|J)+I(X,Y;J|Z)$.

\item We have that 
\[I(U;J|V)-I(U;Z|V)\leq I(UX;J|V)-I(UX;Z|V)\]
if
\[0\leq I(X;J|U,V)-I(X;Z|U,V)\]
so if
\[I(X;J)\geq I(X;Z),\qquad \forall P_{X,Y}\]
we should include $X$ in $U$.

If 
\[I(X,Y;J)\geq I(X,Y;Z),\qquad \forall P_{X,Y}\]
then we can set $U=(X,Y)$ and we will have only one auxiliary $V$.

\end{itemize}



\subsubsection{Binary $J$ suffices}

The following statements are equivalent for any arbitrary $P_{X,Y}$:
\begin{enumerate}
    \item There is some $p_{J|X,Y}$ such that
$$I(X;Y|J)+I(X,Y;J|Z)<I(X;Y).$$
\item There is some $p_{J|X,Y}$ with $J$ being binary such that
$$I(X;Y|J)+I(X,Y;J|Z)<I(X;Y).$$
\item There is some $q(x,y)<< p_{X,Y}$ such that
$$D(q(x)\|p(x))+D(q(y)\|p(y))+D(q(z)\|p(z))>2D(q(x,y)\|P_{X,Y})$$
\end{enumerate}


$$I(J;X,Y,Z)\geq \frac12 I(J;X)+\frac12 I(J;Y)+\frac12 I(J;Z), \qquad\forall p_{J|X,Y,Z}$$

%To find the set of $P_{X,Y}$ where $J$ constant is enough, we need to find cases where
%$$D(q(x)\|p(x))+D(q(y)\|p(y))+D(q(z)\|p(z))\leq 2D(q(x,y)\|P_{X,Y}), \qquad \forall q(x,y)$$
\fi

\section{On the gap between $S(X;Y\|Z)$, $\hat{\Psi}(X;Y\|Z)$ and $I(X;Y)$}
Let us begin with the upper bound $\hat{\Psi}(X;Y\|Z)$ and $I(X;Y)$. It is clear that
$$\hat{\Psi}(X;Y\|Z)\leq I(X;Y), \qquad \forall P_{X,Y,Z}.$$
Assume that 
$$\hat{\Psi}(X;Y\|Z)=I(X;Y)$$
for some $P_{X,Y,Z}$. Then, we claim that if $X'=f_1(X)$, $Y'=f_2(Y)$ and $Z'=f_3(Z)$ for some functions $f_1,f_2,f_3$ such that $H(Z'|X',Y')=0$, then, we also have
$$\hat{\Psi}(X';Y'\|Z')=I(X';Y').$$
We show this by establishing a more general result: 
\begin{theorem}\label{thm6n}
For any $P_{X,Y,Z,X',Y',Z'}$, we have the following two inequalities:
\begin{align}
   I(X;Y)-\hat{\Psi}(X,Y\|Z)+T_1\geq I(X';Y')-\hat{\Psi}(X',Y'\|Z'), \label{eqnSf1}
\end{align}
and
\begin{align}
   I(X;Y)-\hat{\Psi}(X,Y\|Z)+T_2\geq I(X';Y')-\hat{\Psi}(X',Y'\|Z'), 
\end{align}
where
    \begin{align*}
T_1&=I(Z'; Z|X', Y')+I(Y, Y', Z'; X'|X)+I(X, Z'; Y'|Y)+2I(X, Y; Z'|Z)
\\&\qquad-I(Z; Z'|X, Y)-I(Z'; Y', X'|Z),
\end{align*}
and
\begin{align*}
T_2&= I(Z'; X, Y|X', Y')+I(Y, Y'; X'|X)+I(X; Y'|Y)+I(X, Y; Z'|Z).
\end{align*}
\end{theorem}
\begin{remark}
    If $X'=f_1(X)$, $Y'=f_2(Y)$ and $Z'=f_3(Z)$ for some functions $f_1,f_2,f_3$ such that $H(Z'|X',Y')=0$, the values of both $T_1$ and $T_2$ will become zero. Thus, we obtain that
    \begin{align}
   I(X;Y)-\hat{\Psi}(X,Y\|Z)\geq I(X';Y')-\hat{\Psi}(X',Y'\|Z'). 
\end{align}
If we further have $\hat{\Psi}(X,Y\|Z)=I(X;Y)$, we deduce that
\begin{align}
   \hat{\Psi}(X',Y'\|Z')\geq I(X';Y'). 
\end{align}
On the other hand, we always have \begin{align}
   \hat{\Psi}(X',Y'\|Z')\leq I(X';Y'). 
\end{align}
Thus, $\hat{\Psi}(X',Y'\|Z')= I(X';Y')$.
\end{remark}
Proof of Theorem \ref{thm6n} is given in Appendix \ref{append2}.
\subsection{Theorem \ref{thm6n} and future work}
Note that Theorem \ref{thm6n} implies that
\begin{align}
   \hat{\Psi}(X,Y\|Z)\leq \inf_{P_{X',Y',Z'|X,Y,Z}}\left(I(X;Y)+\hat{\Psi}(X',Y'\|Z')+\min(T_1,T_2)-I(X';Y')\right),
\end{align}
where the infimum is over all arbitrary $P_{X',Y',Z'|X,Y,Z}$. 


It is not known whether there exists some $P_{X,Y,Z}$ such that $S(X;Y\|Z)\neq\hat \Psi(X;Y\|Z)$. If $S(X;Y\|Z)=\hat \Psi(X;Y\|Z)$ holds for all $P_{X,Y,Z}$, we must have
\begin{align}
   S(X,Y\|Z)\leq \Phi(X;Y\|Z)\triangleq \inf_{P_{X',Y',Z'|X,Y,Z}}\left(I(X;Y)+S(X',Y'\|Z')+\min(T_1,T_2)-I(X';Y')\right).\label{eqnNN}
\end{align}
Thus, if one can refute \eqref{eqnNN} for some $P_{X,Y,Z}$, it will serve as evidence of  $S(X;Y\|Z)\neq \hat \Psi(X;Y\|Z)$ for general distributions. On the other hand, if one can establish \eqref{eqnNN} for all $P_{X,Y,Z}$, then $\Phi(X;Y\|Z)$ as defined in \eqref{eqnNN} will serve as an upper bound on $S(X;Y\|Z)$. We leave this for future investigation. To sum this up, one general approach to investigating the optimality of the upper bound \( \hat{\Psi}(X; Y \| Z) \) on \( S(X; Y \| Z) \) is as follows:
\begin{enumerate}
    \item Find inequalities that relate \( \hat{\Psi}(X; Y \| Z) \) and \( \hat{\Psi}(X'; Y' \| Z') \) for two sources $P_{X,Y,Z}$ and $P_{X',Y',Z'}$. Theorem \ref{thm6n} provides one such relation, but other relations may also exist.
    \item Form a similar expression with \( S(X; Y \| Z) \) and \( S(X'; Y' \| Z') \) replacing \( \hat{\Psi}(X; Y \| Z) \) and \( \hat{\Psi}(X'; Y' \| Z') \), and prove or disprove this relation. Here, it may be useful to point out that (as mentioned in \cite{gohari-terminal1}), a sufficient condition for $\Phi(X;Y\|Z)$ to serve as an upper bound on $S(X;Y\|Z)$ is that  $\Phi(X;Y\|Z)$ satisfies the following conditions:
    \begin{itemize}
        \item $\Phi(X^n;Y^n\|Z^n)\leq n \Phi(X;Y\|Z)$ when $P_{X^n,Y^n,Z^n}=\prod_{i=1}^{n}P_{X_i,Y_i,Z_i}$ is i.i.d,
        \item $\Phi(F,X;F,Y\|F,Z)\leq \Phi(F,X;Y\|Z)$ and $\Phi(F,X;F,Y\|F,Z)\leq \Phi(X;F,Y\|Z)$ for every arbitrary $P_{F,X,Y,Z}$,
        \item $\Phi(X';Y'\|Z)\leq  \Phi(X;Y\|Z)$ when $P_{X,Y,Z,X',Y'}=P_{X,Y,Z}P_{X'|X}P_{Y'|Y}$,
        \item $\Phi(X;Y\|Z)\geq I(X;Y)-I(X;Z)$.
    \end{itemize}

\end{enumerate}
\section{Is the upper bound $I(X;Y)$ tight for the XOR  case?}\label{dis:tightness}
Let $X$ and $Y$ be binary random variables with an arbitrary joint distribution, and set $Z=X\oplus Y$. We conclude from Theorem \ref{th:binary} that
$$S(X;Y\|Z)\leq \Psi(X;Y\|Z)=\hat{\Psi}(X;Y\|Z)=I(X;Y).$$
In other words, the upper bound $I(X;Y)$ cannot be improved using any existing upper bound. Note that $I(X;Y)$ is the maximum rate we could have achieved if Eve did not have access to the sequence $Z^n$ and could only monitor the public channel. We conjecture that the rate $I(X;Y)$ is no longer be achievable (for a general $P_{X,Y}$) when Eve has access to the XOR of $X$ and $Y$. 
\begin{conjecture}
    There exists some distribution $P_{X,Y}$ on binary random variables $X$ and $Y$ such that
    $$S(X;Y\|Z)<I(X;Y)$$
    where $Z=X\oplus Y$.
\end{conjecture}

One idea to develop a better upper bound is to note that for every $n$ we have
$$S(X;Y\|Z)=\frac1n S(X^n;Y^n\|Z^n)\leq \frac1n \Psi(X^n;Y^n\|Z^n)$$
where $(X^n,Y^n,Z^n)$ are $n$ i.i.d.\ repetitions of $(X,Y,Z)$. Therefore,
$$S(X;Y\|Z)\leq \inf_n\left[\frac1n \Psi(X^n;Y^n\|Z^n)\right].$$
The above inequality might potentially lead to better upper bounds because $\Psi(X^n;Y^n\|Z^n)$ allows for minimizing over all $P_{J|X^n,Y^n,Z^n}$ which is a larger space than that of the single-letter bound. 
The following proposition shows that at least when $Z$ is a function of $(X,Y)$, we do not obtain better upper bounds with this approach, i.e., the proposition shows that
\begin{align}
\inf_n\left[\frac1n \Psi(X^n;Y^n\|Z^n)\right]=\Psi(X;Y\|Z)\label{eqnFW}
\end{align}
as $\Psi(X;Y\|Z)=\hat{\Psi}(X;Y\|Z)$ when $Z$ is a function of $(X,Y)$ (however, we do not know if \eqref{eqnFW} holds when $Z$ is not a function of $(X,Y)$).

\begin{proposition}\label{prop2}
    For every $n$, we have
    $$\hat{\Psi}(X;Y\|Z)=\frac1n \hat{\Psi}(X^n;Y^n\|Z^n)$$
where $(X^n,Y^n,Z^n)$ are $n$ i.i.d.\ repetitions of $(X,Y,Z)$.
\end{proposition}
\begin{proof}
Take some arbitraray $P_{J|X^n,Y^n,Z^n}$ and let $J_i=(J,X^{i-1},Y^{i-1})$. Then, we have
\begin{align}
\color{black}{I(X^n;Y^n|J)}+\color{black}{I(X^nY^n;J|Z^n)}&=\sum_{i}\color{black}{I(X_i;Y_i|J_i)+I(X^{i-1};Y_i|JY^{i-1})+I(Y^{i-1};X_i|JX^{i-1})}\nonumber\\&\qquad + \color{black}{I(X_iY_i;J_i|Z_i)+I(X_i,Y_i;Z_{\backslash i}|J_iZ_i)}\nonumber\\&
\geq
\sum_{i}\color{black}{I(X_i;Y_i|J_i)}+\color{black}{I(X_iY_i;J_i|Z_i)}
\nonumber\\& \geq n\cdot \hat{\Psi}(X;Y\|Z),\label{reverse:1}
\end{align}
where the last equation follows from definition of $\hat{\Psi}(X;Y\|Z)$.

For the converse direction, assume that $J_{*}$ achieves the minimum of $I(X;Y|J)+I(X,Y;J|Z)$.
Suppose  $J$ consists of  i.i.d copies of  random variale  $J_{*}$,i.e, $J=J_{*}^{n}$. We have 
\begin{align*}
I(X^n;Y^n|J_{*}^{n})+I(X^nY^n;J_{*}^{n}|Z^n)=n(I(X;Y|J_{*})+I(X,Y;J_{*}|Z))=n\hat{\Psi}(X;Y\|Z).
\end{align*}
Thus, $\hat{\Psi}(X^n,Y^n\|Z^n)\leq n \hat{\Psi}(X,Y\|Z)$. This observation  and inequality \eqref{reverse:1} complete the proof. 
\end{proof}

\subsection{Controling $H(Z^n|F_{1:r})$}
The following lemma shows that $H(Z^n|F_{1:r})$ cannot be equal to zero for any code that achieves a positive key rate:
\begin{proposition}
    \label{lmm3d}
    Take some $P_{X,Y}(x,y)$ such that $P_{X,Y}(x,y)>0$ for all $x,y$. 
Assume that  $H(Z^n|F_{1:r})=0$
where $F_{1:r}=(F_1,F_2,\cdots,F_r)$ is the public communication.  Then, 
$$H(X^n,Y^n|F_{1:r})=0.$$
\end{proposition} 


We need the following definition:
\begin{definition}\cite{ishwar,verdu} \label{defpreceq}
	Given two pmfs $P_{X,Y}$ and $Q_{X,Y}$ on the alphabets $\mathcal X \times \mathcal Y$, the relation $Q_{X,Y}\preceq P_{X,Y}$ represents existence of some functions $a:\mathcal{X}\rightarrow\mathbb{R}_{+}\cup\{0\}$ and $b:\mathcal{Y}\rightarrow\mathbb{R}_{+}\cup\{0\}$ such that for all $(x,y)\in\mathcal{X}\times\mathcal{Y}$ we have
	\begin{align}
		Q_{X,Y}(x,y)=a(x)b(y)P_{X,Y}(x,y)(x,y).\label{eq:defforPart1proof}
	\end{align}
\end{definition} 

\begin{proof}[Proof of Proposition \ref{lmm3d}] Let us fix $F_{1:r}=f_{1:r}$. 
It is known that for any interactive communication,\footnote{This property is similar to the rectangle property of communication
complexity.} we have
$$P_{X^nY^n|F_{1:r}=f_{1:r}}\preceq P_{X^nY^n}.$$
In other words, 
$$P(x^n,y^n|F_{1:r}=f_{1:r})=P(x^n,y^n)a(x^n)b(y^n)
$$
for some functions $a(\cdot)$ and $b(\cdot)$. 

Since $H(Z^n|F_{1:r}=f_{1:r})=0$, the value of $z^n$ must be fix when we fix $F_{1:r}=f_{1:r}$. This shows that for any $x_1^n,y_1^n$ and $x_2^n,y_2^n$ such that
$x_1^n+y_1^n\neq x_2^n+y_2^n$, the probability of $p(x_1^n,y_1^n|F_{1:r}=f_{1:r})$ and $p(x_2^n,y_2^n|F_{1:r}=f_{1:r})$
cannot be positive at the same time. Thus, 
$$0=P(x_1^n,y_1^n|F_{1:r}=f_{1:r})P(x_2^n,y_2^n|F_{1:r}=f_{1:r})=P(x_1^n,y_1^n)P(x_2^n,y_2^n)a(x_1^n)b(y_1^n)a(x_2^n)b(y_2^n).
$$
Since $P_{X,Y}(x,y)>0$ for all $x,y$, we must have
$a(x_1^n)b(y_1^n)a(x_2^n)b(y_2^n)=0$.

Let $y^n$ be such that $b(y^n)>0$. Take two distinct sequences $x_1^n,x_2^n$ and set $y_1^n=y_2^n=y^n$. This choice is valid since $x_1^n+y_1^n\neq x_2^n+y_2^n$. Thus, we get
$$a(x_1^n)a(x_2^n)b(y^n)=0.$$
Since we assumed $b(y^n)>0$, we get that either $a(x_1^n)$ or $a(x_2^n)$ must be zero. Therefore,  $a(x^n)=0$ for all but one sequence $x^n$, and similarly we can show that $b(y^n)=0$ for all but one sequence $y^n$. Thus, 
$$P(x^n,y^n|F_{1:r}=f_{1:r})=P(x^n,y^n)a(x^n)b(y^n)
$$
is positive for only one pair $(x^n,y^n)$. Thus, $$H(X^n,Y^n|F_{1:r}=f_{1:r})=0.$$
\color{black}



\end{proof}

Proposition \ref{lmm3d} motivates the following definition: 
\begin{definition}
    $S_{\Delta}(X;Y\|Z)$ is supremum of secret key rate corresponding to protocols
  satisfying  
$$\lim_{n\rightarrow \infty}\frac{1}{n}H(Z^n|F_1,F_2,\cdots,F_r)\geq \Delta.$$
The non-interactive capacity $S_{\text{ni},\Delta}(X;Y\|Z)$ is defined similarly.
\end{definition}
\begin{remark}
Observe that $\Delta\mapsto S_{\Delta}(X;Y\|Z)$ is a non-increasing function.
    Moreover, $$S(X;Y\|Z)=\lim_{\Delta\rightarrow 0}S_{\Delta}(X;Y\|Z).$$
\end{remark}


Our strategy for improving the upper bound on $S(X;Y\|Z)$ is as follows:
\begin{enumerate}

\item Compute an upper bound for $S_{\Delta}(X;Y\|Z)$ for any given $\Delta>0$. We will provide such an upper bound in Theorem \ref{tight:the1} below.
    \item Show that $S(X;Y\|Z)=S_{\Delta^*}(X;Y\|Z)$ for some $\Delta^*>0$. Intuitively we expect this to be true because of Proposition \ref{lmm3d}. We only show this part for the non-interactive setup for the XOR example.
    \item Use the upper bound on $S_{\Delta^*}(X;Y\|Z)$ to obtain an upper bound on $S(X;Y\|Z)$. We are unable to perform this last step for the non-interactive setting because, even though the upper bound on $S_{\Delta^*}(X;Y\|Z)$ is theoretically computable, unfortunately, the number of free variables is large and it is not easy to obtain reliable numerical results.  
\end{enumerate}

We begin by the task 1:

\begin{theorem}\label{tight:the1}
    $S_{\Delta}(X;Y\|Z)$ has an upper bound as follows:
    \begin{align}
       S_{\Delta}(X;Y\|Z)\leq \Psi_{\Delta}(X;Y\|Z)\triangleq \inf_J\max_{U,V\rightarrow X,Y\rightarrow J,Z} I(X;Y|J)+I(U;J|V)-I(U;Z|V),\label{main:delta1}
    \end{align}
    where $P_{X,Y,Z,J,U,V}$ satisfy the following constraint
    \begin{align}
      I(V;Z)\leq I(V;J)+H(Z)-\Delta ,\label{cons1}
    \end{align}
    where in the inner maximum $U$ and $V$ satisfy the cardinality bounds 
    $|\mathcal{V}|\leq |\mathcal{X}||\mathcal{Y}|+1$ and $|\mathcal{U}|\leq|\mathcal{X}||\mathcal{Y}|$.
    
    Moreover, if $Z=f(X,Y)$, in computing the upper bound we can assume that $H(X,Y|U,V,Z)=0$.
\end{theorem}
\begin{remark}\label{re:2} 
Observe that $\Delta\mapsto \Psi_{\Delta}(X;Y\|Z)$ is a non-increasing function. When $\Delta=0$, $\Psi_{\Delta}(X;Y\|Z)$ reduces to $\Psi(X;Y\|Z)$ as the only difference between the optimization problems in \eqref{main1} and \eqref{main:delta1} is the additional constraint defined in \eqref{cons1}. This constraint holds trivially when $\Delta=0$. This constraint imposes a  limitation on the distribution $P_{U,V|X,Y}$. 


 As observed earlier in computing $\Psi(X;Y\|Z)$ when $Z$ is a function of $(X,Y)$, an optimizer for $$\max_{U,V\rightarrow X,Y\rightarrow J,Z} I(U;J|V)-I(U;Z|V)$$ is $ U=X,Y$ and 
$V=Z$, which yields the value $I(X,Y;J|Z)$. However, for a positive 
$\Delta$, this choice of $(U,V)$ may not be feasible in \eqref{cons1}.  
\end{remark}
Proof of Theorem \ref{tight:the1} is given in Appendix \ref{append2}.

We now turn to the second task. The following theorem provides a lower bound on $H(Z^n|F_1,F_2)$ for a capacity achieving code. Some of the manipulations inside the proof are similar to the one given in \cite{nair2020} in the context of a different distributed source coding problem.
\begin{theorem}\label{sd=s} Suppose that 
$X$ and $Y$ are binary random variables, and let 
$Z=X\oplus Y$. Let
\begin{align}
  \Delta_1&=\min_{U\rightarrow X\rightarrow YZ}H(Z|U)-H(X|U),~\Delta_2=\min_{V\rightarrow Y\rightarrow XZ}H(Z|V)-H(X|V),\\
  \bar{\Delta}&=\max\{\Delta_1,\Delta_2\}.
\end{align}
   Then, every non-interactive code with public messages $F_1$ and $F_2$ satisfies
   \begin{align}
    \frac{H(Z^n|F_1,F_2)}{n}\geq \frac{\bar{\Delta}}{2}+\frac{I(K_A;K_B|F_1,F_2)}{2n}.
\end{align}

\end{theorem}
\begin{corollary} %Without loss of generality we can assume that the exchanged messages on the public channel are almost independent of each other, i.e., $\frac1nI(F_1;F_2)\rightarrow 0$ in a capacity-achieving sequence of codes. To see this, given a code $(F_1, F_2,X^n,Y^n)$, let us take $m$ i.i.d.\ copies of the code $(F_1^m, F_2^m, X^{nm}, Y^{nm})$. The second user can randomly bin $F_2^m$ at rate $H(F_2|F_1)$ and simply send the Slepian-Wolf bin index of $F_2^m$ instead of $F_2^m$ on the public channel. The first user can utilize $F_1^m$ and the Slepian-Wolf bin index to recover $F_2^m$.
%From here
Let
$$\Delta=\frac{\bar{\Delta}+S_{\text{ni}}(X;Y\|Z)}{2}.$$
    Then, we can conclude:
\begin{align}
    S_{\text{ni}}(X;Y\|Z)=S_{\text{ni},\Delta}(X;Y\|Z).
\end{align}
\end{corollary}
\begin{proof}
  We have
  \begin{align*}
    I(K_A;K_B|F_1,F_2)&\leq I(X^n;Y^n|F_1,F_2)
    \\&\leq H(X^nY^n|F_1,F_2)
    \\&=nH(X,Y)-I(F_1,F_2;X^n,Y^n)
    %\\&=nH(X,Y)-I(F_1;X^n)-I(F_2;Y^n)+I(F_1;F_2)
    %\\&=nH(X,Y)-nR_{F_1}-nR_{F_2}+I(F_1;F_2).
  \end{align*}   
  This yields
  $$\frac1nI(F_1,F_2;X^n,Y^n)\leq H(X,Y)-\frac{1}{n}I(K_A;K_B|F_1,F_2).$$
  To prove the desired inequality, it suffices to show the following inequalities:
    \begin{align}
%H(F_1)+H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|V_i)\\
%H(F_2)+H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|U_i)\\
\frac1nI(F_1,F_2;X^n,Y^n)+\frac{2H(Z^n|F_1,F_2)}{n} &\geq 
H(X,Y)+\min_{U\rightarrow X\rightarrow Y,Z}H(Z|U)-H(Y|U),\label{ineq:11}\\
\frac1nI(F_1,F_2;X^n,Y^n)+\frac{2H(Z^n|F_1,F_2)}{n} &\geq 
H(X,Y)+\min_{V\rightarrow Y\rightarrow X,Z}H(Z|V)-H(X|U),\label{ineq:12}
\end{align}
as combining inequalities \eqref{ineq:11} and \eqref{ineq:12} gives:
\begin{align}
  \frac1nI(F_1,F_2;X^n,Y^n)+\frac{2H(Z^n|F_1,F_2)}{n}\geq H(X,Y)+\bar{\Delta} \label{ineq:13}. 
\end{align}

We only prove inequality \eqref{ineq:11}, as the proof of \eqref{ineq:12} is similar.
Let $U_i=(F_1, Y^{i-1},Z_{i+1}^n)$, $V_i=(F_2, X^{i-1},Z_{i+1}^n)$ be new auxiliary random variables. We have:
\begin{align}
I(F_1,F_2;X^n,Y^n)+2H(Z^n|F_1,F_2)
&=
I(F_1;X^n,Y^n)+I(F_2;Y^n,X^n|F_1)+2H(Z^n|F_1,F_2)\nonumber
\\&= I(F_1;X^n)+I(F_2;Y^n,Z^n|F_1)+2H(Z^n|F_1,F_2)
\nonumber\\&=I(F_1;X^n)+I(F_2;Z^n|F_1)+I(F_2;Y^n|F_1,Z^n)+2H(Z^n|F_1,F_2)
\nonumber\\&=I(F_1;X^n)+H(Z^n|F_1)+I(F_2;Y^n|F_1,Z^n)+H(Z^n|F_1,F_2)
\nonumber\\&\overset{(a)}{=}I(F_1;X^n)+H(Y^n|F_1)+H(Z^n|F_1,Y^n)+I(F_2;Y^n|F_1,Z^n)
\nonumber\\&\qquad+H(Z^n|F_1)-H(Y^n|F_1)+I(Y^n;Z^n|F_1,F_2)
\nonumber\\&\overset{(b)}{=}H(X^n,Y^n)+I(F_2;Y^n|F_1,Z^n)+H(Z^n|F_1)-H(Y^n|F_1)
\nonumber\\&\qquad+I(Z^n;Y^n|F_1,F_2)\nonumber\\
&\overset{(c)}{\geq} nH(X,Y)+H(Z^n|F_1)-H(Y^n|F_1)\nonumber\\
&\overset{(d)}{=}nH(X,Y)+\sum_iH(Z_i|F_1,Y^{i-1},Z_{i+1}^{n})-H(Y_i|F_1,Y^{i-1},Z_{i+1}^{n})\nonumber\\
&=nH(X,Y)+\sum_iH(Z_i|U_i)-H(Y_i|U_i),\label{ineq:inq15}
\end{align}
where $(a)$ follows from $H(Z^n|F_1,F_2,Y^n)=H(Z^n|F_1,Y^n)$, which is equivalent with $H(X^n|F_1,F_2,Y^n)=H(X^n|F_1,Y^n)$ which holds due to the Markov chain $F_2,Y^n\rightarrow X^n\rightarrow F_1$, and for $(b)$  we use the following
\begin{align*}
I(F_1;X^n)+H(Y^n|F_1)+H(Z^n|F_1,Y^n)&=I(F_1;X^n)+H(Y^n|F_1)+H(X^n|F_1,Y^n)
\\&=
I(F_1;X^n)+H(X^n,Y^n|F_1)
\\&=
H(X^n,Y^n).
\end{align*}
Also, in the above, $(c)$ follows from the non-negativity of conditional mutual information, and $(d)$ follows from the Marton-Körner identity.

Let $Q$ be the uniform distribution over the set $\{1,2,\cdots,n\}$, independent of 
$X,Y$ and $F_{1:r}$, and let $X=X_{Q}, Y=Y_Q$ and 
$U=(U,Q)$. It is clear that we have the Markov chain 
$U\rightarrow X \rightarrow Y$. Thus, inequality \eqref{ineq:inq15} simplifies to the desired bound.
\end{proof}
We note that the optimization problems  $\min_{U\rightarrow X\rightarrow Y}H(Z|U)-H(Y|U)$ and $\min_{V\rightarrow Y\rightarrow X}H(Z|V)-H(X|V)$ also arise in many places, including the ``modulo 2 sum" problem in \cite{nair2020}. For instance, the following is known:
\begin{example}[Proposition  $1$ and $2$ of \cite{nair2020}]
Let $P_{X}(0)=x,P_{X|Y}(0|0)=c$ and $P_{X|Y}(1|1)=d$.     The value of $\bar{\Delta}$ is $0$  if $(1-2c)(1-2d)\leq 0$. Additionally, by setting $x=\frac{\sqrt{d(1-d)}}{\sqrt{d(1-d)}+\sqrt{c(1-c)}}$, the value of  $\bar{\Delta}$
 is $2H(Z)-H(X,Y)$ if either $c=d$ or $(1-2c)(1-2d)>0$ and  $c\neq d$.    
\end{example}




\section{Acknowgement}
Prior to the first author's involvement in this problem, the second author designated this project as the undergraduate thesis topic for Yicheng Cui. During the initial stages of this research, Cui made some progress on the XOR and AND patterns, employing different (and weaker) techniques than those presented in this work.

The authors would like to also thank Mr. Chin Wa Lau for his help in proving Proposition \ref{prop2} and for helping Yicheng Cui in his undergraduate thesis.


\appendix
\section{Capacity of a class of source distributions}
\label{appndx}
Assume that $Z=g(X)$ and the channel from $X$ to $Y$ is erasure, i.e., $Y=X$ with probability $1-\epsilon$ and $\{e\}$ otherwise. 
Note that random variables $X,Y$ and $Z$ do not always form any Markov chain in any order, and the key capacity for this class is not known in the literature. 

We claim that $S(X;Y\|Z)=I(X;Y|Z)=(1-\epsilon)H(X|Z)$ is achievable by one-way communication from Alice to Bob. Note that 
\begin{align}
&S_{\text{ow}}(X;Y\|Z)=\max_{U,V\rightarrow X\rightarrow Y,Z}I(U;Y|V)-I(U;Z|V).
\end{align}
Consider $V$ constant, and by the functional representation lemma, let $U$ be such that $I(Z;U)=0$ and $H(X|Z,U)=0$. Then, note that $I(U;Z|Y)=0$ since when $Y$ is $X$ it holds and when $Y$ is $e$, it also holds. Then, we have
\begin{align}
I(U;Y|V)-I(U;Z|V)&=I(U;Y)-I(U;Z)\\
&=I(U;Y)
\\&=I(U;YZ)
\\&=I(U;Y|Z)
\\&=I(X;Y|Z)-I(X;Y|Z,U)
\\&=I(X;Y|Z).
\end{align}
This completes the proof. 
\section{Some of the proofs}\label{append2}
\subsection{Proof of Lemma \ref{lem:con-hull}}
\begin{proof}[Proof of Lemma \ref{lem:con-hull}]
Note that $I(Z;T|X,Y)=0$ holds trivially when $Z$ is a function of $(X,Y)$. Assume that for given $P_{X,Y}$ there exists a random variable $T$ such that $I(Z;T|X,Y)=I(Z;T)=I(X;Y|T)=0$. Define $Q_{X,Y}^{(t)}=P(X,Y|T=t)$. From $I(X;Y|T)=0$,  it follows that $I_{Q^{(t)}}(X;Y)=0$. Additionally, for any $t\in \mathcal{T}$ and $z\in \mathcal{Z}$, we have 
\begin{align*}
    Q^{(t)}(Z=z)=\sum_{x,y\in \mathcal{R}_{z}}
    Q^{(t)}(X=x,Y=y)=\sum_{x,y\in \mathcal{R}_{z}}
    P(X=x,Y=y|T=t)=P(Z=z|T=t)\overset{(a)}{=}P(Z=z),
\end{align*}
 where $\mathcal{R}_{z}=\left\{(x,y)\in \mathcal{X}\times \mathcal{Y}: z=f(x,y)\right\}$ and $(a)$ follows from $I(Z;T)=0$. Hence, we conclude that $P_{X,Y}\in conv(A)$.
 
For the converse direction, assume that $P_{X,Y}\in conv(A)$. Therefore, we can represent $P_{X,Y}$ as a convex combination of elements from $A$:
 \begin{align}
     P_{X,Y}=\sum_{t}\omega_{t}Q_{X,Y}^{(t)}, ~Q_{X,Y}^{(t)}\in A,
 \end{align}
 where $\omega_t\geq 0$ and $\sum_{t}\omega_{t}=1$.
 Define $P(X,Y|T=t)=Q_{X,Y}^{(t)}$. Since $Q^{(t)}\in A$, it follows that $I_{P}(X;Y)=I_{Q^{(t)}}(X;Y)=0$. Next, 
\begin{align*}
    P(Z=z|T=t)=\sum_{x,y\in \mathcal{R}_{z}}P(X=x,Y=y|T=t)=
    \sum_{x,y\in \mathcal{R}_{z}}Q^{(t)}(X=x,Y=y)=Q(Z=z)\overset{(a)}{=}P(Z=z)
\end{align*}
where $(a)$ holds  since $Q^{(t)}\in A$. Thus form above we have $I_{P}(Z;T)=0$. This completes the proof.  
\end{proof}
\subsection{Proof of Theorem \ref{th:binary}}
\begin{proof}[Proof of Theorem \ref{th:binary}]
The condition $Cov(X,Y)\leq 0$ is equivalent with $p_{00}p_{11}\leq p_{01}p_{10}$. 


    For the first part, since
    $$\Psi(X;Y\|Z_3)\geq \Psi(X;Y\|Z_1)$$
it suffices to show that
if $p_{00}p_{11}\leq p_{01}p_{10}$, then
    $\Psi(X;Y\|Z_1)=I(X;Y)$
     and if $p_{00}p_{11}>p_{01}p_{10}$, then
    $\Psi(X;Y\|Z_3)=0.$

Proof of 
    $\Psi(X;Y\|Z_1)=I(X;Y)$ if $p_{00}p_{11}\leq p_{01}p_{10}$: 
    The condition $p_{00}p_{11}\leq p_{01}p_{10}$ implies that
the polynomial
    $$x^2-(p_{01}+p_{10})x+p_{00}p_{11}=0$$
    has two non-negative real roots. Let $x_0\leq x_1$ denote the roots. Note that $x_0x_1=p_{00}p_{11}$ and $x_0+x_1=p_{01}+p_{10}$. One can verify that $p_{01}$ belongs to $[x_0,x_1]$. Then, one can find $\lambda\in[0,1]$ such that
    $$p_{01}=\lambda x_0+(1-\lambda)x_1.$$
    since $x_0+x_1=p_{01}+p_{10}$, we obtain
    $$p_{10}=\lambda x_1+(1-\lambda)x_0.$$
    
    Since $\Psi(X;Y\|Z_1)=\hat{\Psi}(X;Y\|Z_1)$, to show that $\hat{\Psi}(X;Y\|Z_1)=I(X;Y)$, we use Theorem \ref{Th:gen:classic}. It suffices to identify a $T$ such that $I(T;Z_1|X,Y)=I(T;Z_1)=I(X;Y|T)=0$. The condition $I(T;Z_1|X,Y)=0$ holds for any $P_{T|X,Y}$ because $Z_1$ is a function of $(X,Y)$. 
    Let $T$ be a binary random variable satisfying $P(T=1)=\lambda$ and $P(T=0)=1-\lambda$ and
$$P(X=1,Y=1|T=t)=p_{11}, \qquad t\in\{0,1\}$$
$$P(X=0,Y=0|T=t)=p_{00}, \qquad t\in\{0,1\}$$
$$P(X=0,Y=1|T=0)=P(X=1,Y=0|T=1)=x_1.$$
$$P(X=0,Y=1|T=1)=P(X=1,Y=0|T=0)=x_0$$
This yields the desired result.


Proof of 
    $\Psi(X;Y\|Z_3)=0$ if $p_{00}p_{11}>p_{01}p_{10}$: we identify a binary random variable $J^*$ such that 
$I(X;Y|J^*)=I(X,Y;J^*|Z_3)=0$. This would imply that
$$\Psi(X;Y\|Z_3)\leq I(X;Y|J^*)+I(X,Y;J^*|Z_3)=0.$$
Let
$$\alpha=\dfrac{p_{00}}{p_{00}-(p_{00}p_{11}-p_{01}p_{10})}.$$ Under condition $p_{00}p_{11}-p_{01}p_{10}>0$ we have $\alpha \geq 1$. Suppose that 
$$P_{X,Y|J^*=0}=\begin{bmatrix}
   0   & 0     \\
    0  &1  
\end{bmatrix}$$ and $$P_{X,Y|J^*=1}=\begin{bmatrix}
   \alpha p_{00}   & \alpha p_{01}     \\
    \alpha p_{10}  &1-\alpha(1-p_{11}) 
\end{bmatrix}$$ 
and
$P(J^*=1)=\frac{1}{\alpha}$. One can directly verify that
$$P_{X,Y}=P(J^*=1)P_{X,Y|J^*=1}+P(J^*=0)P_{X,Y|J^*=0}.$$
Moreover, for this choice of $J^*$, we have $I(X;Y|J^*)=I(X,Y;J^*|Z)=0$.

For the second part, we consider two cases: if $p_{00}p_{11}\leq p_{01}p_{10}$, we use the fact that $\Psi(X;Y\|Z_1)=I(X;Y)$ and 
$$\Psi(X;Y\|Z_2)\geq \Psi(X;Y\|Z_1)=I(X;Y).$$
In the case of $p_{00}p_{11}\leq p_{01}p_{10}$, consider the following random variable:
$$\hat Z_1=\begin{cases}
        0& (X,Y)=(0,1)\\
        1& (X,Y)=(1,0)\\
        2& (X,Y)\in \{(0,0), (1,1)\}
    \end{cases}$$
    
    Then, by symmetry, from the first part we deduce that 
    in the case of $p_{00}p_{11}\leq p_{01}p_{10}$, we have
    $\Psi(X;Y\|\hat Z_1)=I(X;Y)$. Since
    $$\Psi(X;Y\|Z_2)\geq \Psi(X;Y\|\hat Z_1)=I(X;Y)$$
    we get the desired result.
\end{proof}
\subsection{Proof of Theorem \ref{th:ter-bin}}
\begin{proof}[Proof of Theorem \ref{th:ter-bin}]
Note that when $P(Y=2)=0$, i.e., when $Y\in\{0,1\}$, the random variable $Z$ can be expressed as $Z=X\oplus Y$. In this case, proof of Theorem 
\ref{th:binary} shows that one can find a random variable $T$ such that $I(Z;T|X,Y)=I(Z;T)=I(X;Y|T)=0$. Lemma \ref{lem:con-hull} would then imply any $P_{X,Y}$ satisfying $P_{Y}(2)=0$ belongs to the convex hull of the set $\mathcal{A}(P_Z)$ as defined in \eqref{con:formula}. More specifically, let 
$\alpha=P_Z(0)$ and $\bar{\alpha}=P_Z(1)=1-\alpha$ and let us denote $\mathcal{A}(P_Z)$ by $\mathcal{A}(\alpha)$. Then, for any $r,t\in[0,1]$, the following joint distribution belongs to 
the convex hull of the set $\mathcal{A}(\alpha)$:
\begin{align}
  B_{r,t}(x,y)=\begin{tabular}{|c|c|c|c|}
 \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$ & $\mathbf{2}$   \\
  \hline
 $\mathbf{0}$&$\alpha r$& $\bar{\alpha}\bar{t}$ &0\\
\hline
 $\mathbf{1}$&$\bar{\alpha}t$ & $\alpha\bar{r}$ &0 \\ 
\hline
\end{tabular}
 \end{align}

Observe that when $P(Y=0)=0$, the mapping from $X$ and $Y\in\{1,2\}$ to $Z$ is again an XOR map. A similar argument shows that then imply any $P_{X,Y}$ satisfying $P_{Y}(0)=0$ belongs to the convex hull of the set $\mathcal{A}(\alpha)$.
In other words, for any $s,\ell\in[0,1]$ the following joint distribution belongs to the convex hull of  the set $\mathcal{A}(\alpha)$:
\begin{align}
 C_{s,\ell}(x,y)=\begin{tabular}{|c|c|c|c|}
 \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$ & $\mathbf{2}$  \\
 \hline
$\mathbf{0}$& 0& $\bar{\alpha}\bar{s}$ & $\alpha \ell$  \\
\hline
$\mathbf{1}$&0& 
$\alpha\bar{\ell}$ & $\bar{\alpha}s$ \\ 
\hline
\end{tabular}
 \end{align}
Observe that for any $\omega\in[0,1]$ the linear combination
$$\omega B_{r,t}(x,y)+\bar{\omega}C_{s,\ell}(x,y)$$
must also belong to the convex hull of  the set $\mathcal{A}(\alpha)$.


Let $p_{ij}=P_{X,Y}(i,j)$. We have $\alpha=p_{00}+p_{02}+p_{11}$. Set 
$$r=\min\left\{1, \frac{\bar{\alpha}p_{00}}{\alpha p_{10}} \right\}$$
$$\omega=\frac{ p_{00}}{\alpha r},~ t=\frac{\alpha p_{10}}{\bar{\alpha}p_{00}}r,~ 
s=\frac{\alpha p_{12}r}{\bar{\alpha}(\alpha r-p_{00})},~
\ell=\frac{p_{02}r}{\alpha r-p_{00}}.$$
One can then verify that
$$P_{X,Y}(x,y)=\omega B_{r,t}(x,y)+\bar{\omega}C_{s,\ell}(x,y)$$
belongs to the convex hull of  the set $\mathcal{A}(\alpha)$.
The conditions $$\frac{p_{00}}{\alpha}+\frac{p_{12}}{\bar{\alpha}}\leq 1,~ \frac{p_{02}}{\alpha}+\frac{p_{10}}{\bar{\alpha}}\leq 1. $$  
imply that 
$$\omega,t,r,s,\ell\in[0,1].$$ Consequently, one can identify a random variable $T$ such that $I(Z;T|X,Y)=I(Z;T)=I(X;Y|T)=0$ holds for $P_{X,Y}$ provided that
the conditions $$\frac{p_{00}}{\alpha}+\frac{p_{12}}{\bar{\alpha}}\leq 1,~ \frac{p_{02}}{\alpha}+\frac{p_{10}}{\bar{\alpha}}\leq 1$$  
are satisfied.

\end{proof}

\subsection{Proof of Theorem \ref{thm:thm5}}
\begin{proof}[Proof of Theorem \ref{thm:thm5}]
 Let  the alphabets of $X$ and $Y$  take values in   $\{0,1,2,\cdots,s_{1}-1\}$ and $\{0,1,2,\cdots,s_{2}-1\}$   respectively. Let  $Z=f(X,Y)$.  
 The function $f$ corresponds to a table
%  \backslashbox{$X$}{$Y$}
\begin{align}
  \begin{tabular}{|c|c|c|c|c|}
 \hline
  $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$ &$\cdots$ & $\mathbf{s_2-1}$ \\
  \hline
  $\mathbf{0}$ &  $f(0,0)$&  $f(0,1)$& $\cdots$ &  $f(0,s_2-1)$  \\
  $\mathbf{1}$ &  $f(1,0)$ &  $f(1,1)$ &  $\cdots$ &$f(1,s_2-1)$ \\
  $\vdots$ &  $\vdots$ &  $\vdots$ &  $\cdots$ & $\vdots$  \\
   $\mathbf{s_1-1}$ &  $f(s_1-1,0)$ & $f(s_1-1,1)$ & $\cdots$ & $f(s_1-1,s_2-1)$  \\
  \hline
\end{tabular}
\end{align}

A table is considered valid if the equality $\Psi(X;Y|Z)=I(X;Y)$ holds for all distributions $P_{X,Y}$. Since any distribution over a subset of $\mathcal{X} \times \mathcal{Y}$ can be extended to a distribution over $\mathcal{X} \times \mathcal{Y}$, it follows that if a table is valid, all of its sub-tables will also be valid.  Consider a $2\times 2$ table.
As stated in Theorem \ref{th:binary}, the equality $\Psi(X;Y|Z)=I(X;Y)$ does not hold for any arbitrary distribution $P_{X,Y}$ when the functions are AND, OR, or $Z_1$. It is also evident that if $Z=f(X,Y)$ is a one-to-one map, $\Psi(X;Y|Z)\leq I(X;Y|Z)=0$ and the equality $\Psi(X;Y|Z)=I(X;Y)$ would not hold  for every $P_{X,Y}$. Consequently, for $2\times 2$ tables, the following combinations are  invalid if $A,B,C$ and $D$ are distinct symbols:
 \begin{align}
\begin{tabular}{|c|c|c|}
  \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$  \\
  \hline
  $\mathbf{0}$ &  $A$&  $B$  \\
  $\mathbf{1}$ &  $B$ &  $C$\\  
  \hline
\end{tabular}
~~
\begin{tabular}{|c|c|c|}
  \hline
 $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$  \\
  \hline
  $\mathbf{0}$ &  $A$&  $B$  \\
  $\mathbf{1}$ &  $C$ &  $A$\\  
  \hline
\end{tabular}
~~
\begin{tabular}{|c|c|c|}
  \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$   \\
  \hline
  $\mathbf{0}$ &  $A$&  $B$  \\
  $\mathbf{0}$ &  $A$ &  $A$\\  
  \hline
\end{tabular}
~~
\begin{tabular}{|c|c|c|}
  \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$  \\
  \hline
  $\mathbf{0}$ &  $A$&  $B$  \\
  $\mathbf{1}$ &  $C$ &  $D$\\  
  \hline
  \end{tabular}
\end{align}
and the valid $2 \times 2$ tables are limited to the following:
\begin{align}\label{valid:2*2:tables}
 T_1=\begin{tabular}{|c|c|c|}
  \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$  \\
  \hline
  $\mathbf{0}$ &  $A$&  $B$  \\
  $\mathbf{0}$ &  $B$ &  $A$\\  
  \hline
\end{tabular}
\qquad\qquad T_2=\begin{tabular}{|c|c|c|}
  \hline
   $\mathbf{X \backslash Y}$& $\mathbf{0}$ & $\mathbf{1}$ \\
  \hline
  $\mathbf{0}$ &  $A$&  $A$  \\
  $\mathbf{1}$ &  $A$ &  $A$\\  
  \hline
\end{tabular}.
\end{align}
Now, consider a general table $T$ of arbitrary size $s_1\times s_2$ where $(s_1,s_2)\neq (2,2)$. Since every $2\times 2$ sub-table must be valid, we conclude that for every $i_1\neq i_2$ and $j_1\neq j_2$, we must either have 
$$T(i_1,j_1) = T(i_1,j_2)=T(i_2,j_1) = T(i_2,j_2)$$
or
$$T(i_1,j_1) = T(i_2,j_2)\neq T(i_2,j_1) = T(i_1,j_2)$$
In both cases $T(i_1,j_1)=T(i_2,j_2)$ for every $i_1\neq i_2$ and $j_1\neq j_2$. In other words, if we move from one cell to another cell by changing both the row and the column, the value of the new cell must be equal to the value of the old cell. Note that in a  $s_1\times s_2$ table where $(s_1,s_2)\neq (2,2)$, we can move from a cell to all the other cells by a sequence of such moves. For instance, in a $2\times 3$ table, we can move from $(1,1)$ to $(1,2)$ by the following sequence:
$$(1,1)\rightarrow (2,2)\rightarrow (3,1)\rightarrow (1,2).$$
This shows that when $(s_1,s_2)\neq (2,2)$, $T(i_1,j_1)=T(i_2,j_2)$ for all $i_1,i_2,j_1,j_2$. 
\end{proof}
\subsection{Proof of Theorem \ref{thm6n}}
\begin{proof}[Proof of Theorem \ref{thm6n}]
Let $P_{J|X'Y'Z'}$ be a minimizer for the optimization problem in $\hat{\Psi}(X',Y'\|Z')$, i.e., $$\hat{\Psi}(X',Y'\|Z')=I(X';Y'|J)+I(X',Y';J|Z').$$ 
We know that such a minimizer exists because of the cardinality bound on $J$ is available for $\hat{\Psi}(X',Y'\|Z')$.

We define the joint distribution between $J$ and $(X,Y,Z,X',Y',Z')$ as 
$P_{J|X'Y'Z'}P_{X'Y'Z'XYZ}$.
This construction implies the   Markov chain
\begin{align}
 X,Y,Z\rightarrow X',Y',Z'\rightarrow J.  \label{mar:J} 
\end{align}
 Let $\alpha=\min\{T_1, T_2\}$.
We claim that it suffices to show that
 \begin{align}
       I(X;Y)-I(X;Y|J)-I(X,Y;J|Z)+\alpha&\geq I(X';Y')-I(X';Y'|J)-I(X',Y';J|Z')\label{inq:ak1}.
      \end{align}
To see this, observe that if \eqref{inq:ak1} holds, we can write
      \begin{align}
      I(X;Y)-\hat{\Psi}(X;Y\|Z)+\alpha&
       \geq I(X;Y)-I(X;Y|J)-I(X,Y;J|Z)+\alpha\\&\geq I(X';Y')-I(X';Y'|J)-I(X',Y';J|Z')\\&\geq 
       I(X';Y')-\hat{\Psi}(X';Y'\|Z').
      \end{align}
      Thus, we obtain the desired inequality. 
It remains to establish inequality \eqref{inq:ak1}, i.e., we need to show the following two inequalities:
\begin{align}
       I(X;Y)-I(X;Y|J)-I(X,Y;J|Z)+T_1&\geq I(X';Y')-I(X';Y'|J)-I(X',Y';J|Z'),\label{inq:ak1a1}
       \\
       I(X;Y)-I(X;Y|J)-I(X,Y;J|Z)+T_2&\geq I(X';Y')-I(X';Y'|J)-I(X',Y';J|Z').\label{inq:ak1a2}
      \end{align}
      
For \eqref{inq:ak1a1}, the following equality holds for any arbitrary random variables $X,Y,Z,X',Y',Z',J$ with no assumptions on their joint distribution:
    \begin{align}
        I(X;Y)-I(X;Y|J)-I(X,Y;J|Z)+T_1&= I(X';Y')-I(X';Y'|J)-I(X',Y';J|Z')+A_1
    \end{align}
    where
\begin{align*}
    A_1=&I(J;Y'|X, Y, Z, X', Z')+I(X;Z'|X', Y', J)+I(Y;Z'|X, X', Y', J)\\
    &\quad +I(X; Y'|Y, J)+
  I(X; Z'|Y, Z, J)+ I(X; J|X')+
  I(Y; X'|X, J) +
  I(Y; Z'|Z, J)+
  I(Y; J|Y') 
  \\&\quad +I(Z; J|Z')+
  I(X'; Y'|X, Y, J)+
  I(X'; J|X, Y, Z, Z')
  \\&\quad-I(J;X,Y,Z|X',Y',Z')-I(J;X,Y|X',Y',Z')
\end{align*}
    Clearly, $A_1\geq 0$ when \eqref{mar:J} holds because the mutual information terms with negative sign vanish under the assumption in \eqref{mar:J}. Thus,
\eqref{inq:ak1a1} follows. We remark that if \eqref{eqnSf1} holds with equality, there must exist a minimizer for $\Psi(X;Y\|Z)$ satisfying \eqref{mar:J}, and all mutual information terms with positive weight in $A_1$ must vanish. This is a very restricting condition. 

Next, consider \eqref{inq:ak1a2}. Let
\begin{align*}
      A_2&=I(X; Y'|Y, J)+  I(X; Z'|Y, Z, J)+  I(X; J|X')+  I(Y; X'|X, J)+  I(Y; Z'|Z, J)
      \\&\qquad+  I(Y; J|Y')+I(Z; J|Z') +  I(X'; Y'|X, Y, J) +  I(X'; J|X, Y, Z, Z') 
      \\&\qquad+ I(J; Y'|X, Y, Z, X', Z')+I(X; Z'|X', Y', J)+I(Y; Z'|X, X', Y', J)
      \\&\qquad-I(J;X,Y,Z|X',Y',Z')-I(J;X,Y|X',Y',Z')
\end{align*}
The following identity holds for any arbitrary random variables $X,Y,Z,X',Y',Z',J$ with no assumptions on their joint distribution:
    \begin{align}
        I(X;Y)-I(X;Y|J)-I(X,Y;J|Z)+T_2&= I(X';Y')-I(X';Y'|J)-I(X',Y';J|Z')+A_2.
    \end{align}
    Similar to the earlier case,
    $A_2\geq 0$ when \eqref{mar:J} , and we are done with the proof.
\end{proof}
\subsection{Proof of Theorem \ref{tight:the1}}
\begin{proof}[Proof of Theorem \ref{tight:the1}]
Let $F_{1:r}=(F_1,\cdots,F_r)$. Take some arbitrary $P_{J|X,Y,Z}$ and suppose $(X^n,Y^n,Z^n,J^n)$ are $n$ i.i.d.\ copies according to $P_{X,Y,Z}P_{J|X,Y,Z}$. 
Define the auxiliary random variables as  $V_i=(J^{i-1},Z_{i+1}^n,F_{1:r})$ and $U_i=K_A$. The following chains of equations follows:
\begin{align}
H(K_A)&=I(K_A;Z^n,F_{1:r})+H(K_A|Z^n,F_{1:r})\\&\overset{(a)}{\leq} n\epsilon_{n}+H(K_A|Z^n,F_{1:r})\nonumber\\
&= n\epsilon_{n}+ H(K_A|Z^n,F_{1:r})-H(K_A|J^n,F_{1:r})+H(K_A|J^n,F_{1:r})\nonumber\\
&\overset{(b)}{=}  n\epsilon_{n}+\sum_{i=1}^{n}I(K_A;J_i|J^{i-1},Z_{i+1}^{n},F)-I(K_A;Z_i|J^{i-1},Z_{i+1}^{n},F)+H(K_A|J^n,F_{1:r})\nonumber\\
&\overset{(c)}{=}n\epsilon_{n}+n\delta_n+\sum_{i=1}^{n}I(U_i;J_i|V_i)-I(U_i;Z_i|V_i)+I(K_A;K_B|J^n,F_{1:r})\nonumber\\
&\overset{(d)}{\leq} n\epsilon_{n}+n\delta_n+\sum_{i=1}^{n}I(U_i;J_i|V_i)-I(U_i;Z_i|V_i)+I(X^n;Y^n|J^n,F_{1:r})\nonumber\\
&\overset{(e)}{\leq}  n\epsilon_{n}+n\delta_n+\sum_{i=1}^{n}I(U_i;J_i|V_i)-I(U_i;Z_i|V_i)+I(X^n;Y^n|J^n)\nonumber\\
&\leq n\epsilon_{n}+n\delta_n+\sum_{i=1}^{n}I(U_i;J_i|V_i)-I(U_i;Z_i|V_i)+nI(X;Y|J)\label{ineq:1}
\end{align}
where $\epsilon_n,\delta_n\rightarrow 0$ as $n\rightarrow\infty$; steps (a) and (c) are derived using Fano's inequality, while (b) follows from the Marton-Körner identity, and (d) is a consequence of the Data Processing Inequality (DPI). To establish 
(e), we proceed as follows:
\begin{align}
    I(X^n;Y^n|J^n)&\overset{(a)}{=} I(X^n,F_1;Y^n|J^n)\overset{(b)}{\geq} I(X^n;Y^n|J^n,F_1)\overset{(c)}{=} I(X^n;Y^n,F_2|J^n,F_1)\overset{(d)}{\geq}
    I(X^n;Y^n|J^n,F_1,F_2)\nonumber\\&= I(X^n,F_3;Y^n|J^n,F_1,F_2)\geq I(X^n;Y^n|J^n,F_1,F_2,F_3)= \cdots\geq I(X^n;Y^n|J^n,F_{1:r}),\nonumber
\end{align}
where $(a)$ and $(b)$ follow from Markov chains $Y^n,F^{i-1}\rightarrow X^n\rightarrow F_i$ for odd $i$ and $X^n,F^{i-1}\rightarrow Y^n\rightarrow F_i$  for even $i$ respectively. Also $(b)$ and $(d)$ are derived from chain rule.

  Let $Q$ be uniformly  distributed over $\{1,2,\cdots,n\}$ and independent of $X,Y,Z,J,F_{1:r}$. Define  the random variables $V=(V_Q,Q), U=U_Q,Z=Z_Q$ and $J=J_Q$. Under these definitions, inequality \eqref{ineq:1} simplifies  to:  
  \begin{align}
      \frac{H(K_A)}{n}\leq \epsilon_{n}+\delta_n+
      I(X;Y|J)+I(U;J|V)-I(U;Z|V).\label{eq:q1}
  \end{align}
  Additionally, the auxiliary random variables    $V_i=(J^{i-1},Z_{i+1}^n,F_{1:r})$ and $U_i=K_A$ imply the Markov chain relationship $U,V\rightarrow X,Y\rightarrow J,Z$. 
 Assuming that $n$ is sufficiently large, to prove constraint \eqref{cons1} for the random variables
 $V_i,J_i,Z_i$  and $F_{1:r}$ as defined above, we proceed as follows:
  \begin{align}
      I(V;Z)=\frac{1}{n}\sum_{i=1}^{n}I(V_i;Z_i)&=\frac{1}{n}\sum_{i=1}^{n}I(J^{i-1},F_{1:r};Z_i|Z_{i+1}^{n})
      \\&=\frac{1}{n}\sum_{i=1}^{n}I(F_{1:r};Z_i|Z_{i+1}^{n})+\frac{1}{n}\sum_{i=1}^{n}I(J^{i-1};Z_i|Z_{i+1}^{n},F_{1:r})\nonumber\\
      &\overset{(a)}{=}\frac{1}{n}\sum_{i=1}^{n}I(F_{1:r};Z_i|Z_{i+1}^{n})+\frac{1}{n}\sum_{i=1}^{n}I(Z_{i+1}^{n};J_i|J^{i-1},F_{1:r})\nonumber\\ & \overset{(b)}{\leq}
    \frac{1}{n}I(Z^n;F_{1:r})+ \frac{1}{n}\sum_{i=1}^{n}I(J^{i-1},Z_{i+1}^{n},F;J_i)\nonumber\\
      &=\frac{1}{n}I(Z^n;F_{1:r}) +I(V;J)
      =I(V;J)+H(Z)-\frac{1}{n}H(Z^n|F_{1:r})\nonumber\\
       &\overset{(c)}{\leq} I(V;J)+H(Z)-\Delta+\epsilon'_n\nonumber,
  \end{align}
  where step 
(a) follows from Marton-Körner  identity, 
(b) is derived using the chain rule, and 
(c) utilizes the assumption
$$\frac{1}{n}H(Z^n|F_{1:r})\geq \Delta-\epsilon'_n$$
for a vanishing sequence $\epsilon'_n$.
Thus, for any $J$ and a specific choice of auxiliary random variables 
$U$ and $V$ that satisfy the Markov chain 
$U,V\rightarrow X,Y\rightarrow J,Z$
 and inequality \eqref{cons1}, by taking the limit as $n\rightarrow\infty $ in equation \eqref{eq:q1}, we obtain:
  \begin{align*}
      S_{\Delta}(X;Y\|Z)\leq 
      I(X;Y|J)+I(U;J|V)-I(U;Z|V).
  \end{align*}

Proof of the cardinality bounds are standard and we skip them.
The second part of this theorem for the special case of $Z=f(X,Y)$ follows from Lemma \ref{helper} (after applying the proper change of variables).
\end{proof}

\begin{lemma}\label{helper}
      Suppose that $Z=g(X)$. For every joint distribution $P_{V,X,Y}$, the maximizer of $I(U;Y|V)-I(U;Z|V)$ over all $P_{U|V,X}$ is achieved when $H(X|U,V,Z)=0$.
  \end{lemma}
\begin{proof}[Proof of Lemma \ref{helper}] 
   Take some arbitrary tuple $P_{U|V,X}$.  By the functional representation lemma, we can find a conditional distribution $P_{W|U,V,Z,X}$ such that $I(Z,U,V;W)=0$ and $H(X|Z,U,V,W)=0$. Now,  consider the joint distribution $P_{W|U,V,Z,X}P_{Y|X}$.
Let $U'=(U,W)$. We claim that changing $U$ to $(U,W)$ would not decrease the expression, and moreover $H(X|Z,U',V)=0$. This would complete the proof. We need to show that
\begin{align}
    I(U;Y|V)-I(U;Z|V)\leq I(U';Y|V)-I(U';Z|V)
\end{align}
Equivalently,
\begin{align}
    0\leq I(W;Y|U,V)-I(W;Z|U,V)\label{conn}
\end{align}
which holds since $I(W;U,V,Z)=0$.   
\end{proof}

      

\iffalse
In the proof of \eqref{main1} in \cite{}, the authors used auxiliary random variables 
$V_i=J^{i-1}Z_{i+1}^nF_1\cdots F_r$ and $U_i=K_A$,
to construct $$S(X;Y\|Z)\leq I(X;Y|J)+I(U;J|V)-I(U;Z|V).$$Moreover, they proved
\begin{align}
   {\color{cyan}\frac1nI(F_1\cdots F_r;Z^n)\leq} I(V;Z)\leq I(V;J)+\frac1nI(F_1\cdots F_r;Z^n)\label{f33}. 
\end{align}



Thus, at first step we construct upper bound on 
$I(F_1\cdots F_r;Z^n)$ or lower bound on $H(Z^n|F_1\cdots F_r)$.












Letting  we have
$$S(X;Y\|Z)\leq I(X;Y|J)+I(U;J|V)-I(U;Z|V).$$
Moreover, we have
\begin{align}
   {\color{cyan}\frac1nI(F_1\cdots F_r;Z^n)\leq} I(V;Z)\leq I(V;J)+\frac1nI(F_1\cdots F_r;Z^n)\label{f33} 
\end{align}
Therefore, we need to find an upper bound on $\frac1nI(F_1\cdots F_r;Z^n)$.

Upper bounds:
From
    \begin{align}
      H(Z^n|F_1,F_2,\cdots, F_r)\geq \sum_i H(Z_i|U_{1i},U_{2i},\cdots, U_{ri},A_i)  
    \end{align}
we obtain
$$I(F_1\cdots F_r;Z^n)\leq nH(Z)-\sum_i H(Z_i|U_{1i},U_{2i},\cdots, U_{ri},A_i)$$
We also have
  \begin{align}
     n\sum_{j=1}^rR_j\geq I(W^n;F_1,F_2,F_3,\cdots, F_r)+\sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots,U_{ri}|A_i,W_i).
     \label{f22}
  \end{align}
So, we need lower bounds on $I(W^n;F_1,F_2,F_3,\cdots, F_r)$ and upper bounds on $n\sum_{j=1}^rR_j$.

\begin{align}
    I(W^n;F_1,F_2,\cdots, F_r)&\geq \max[0,\epsilon_{n}+ I(Z^n;F_1,F_2,\cdots, F_r)-nH(Z|W)+\sum_i H(X_i,Y_i|U_{1i}, U_{2i}, \cdots, U_{ri}, A_i, W_i)].\label{bound3}
\end{align}
We get the following:
\begin{align}
     n\sum_{j=1}^rR_j\geq \sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots,U_{ri}|A_i,W_i). 
  \end{align}
\begin{align}
     n\sum_{j=1}^rR_j\geq I(Z^n;F_1,F_2,F_3,\cdots, F_r)-nH(Z|W)+nH(X,Y|W)
  \end{align}
Moreover, 
the following observation builds a connection between $I(W^n;F_1,F_2,\cdots, F_r)$ and $S(X;Y\|Z))$:
\begin{align}
    &I(W^n;F_1,F_2,\cdots, F_r) =n(H(X,Y|Z)-S(X;Y\|Z))+
    I(Z^n;F_1,F_2,\cdots, F_r)
    -\sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots, U_{ri}|W_i,A_i).\label{f2}
\end{align}
Thus, inequalities \eqref{f22} and \eqref{f2} imply 
\begin{align}
    n\sum_{j=1}^rR_j\geq n(H(X,Y|Z)-S(X;Y\|Z))+
    I(Z^n;F_1,F_2,\cdots, F_r).
\end{align}

Using lower bound zero for $I(Z^n;F_1,F_2,\cdots, F_r)$ in above, we have:
\begin{align}
     n\sum_{j=1}^rR_j&\geq n(H(X,Y|Z)-S(X;Y\|Z))
    . 
  \end{align}
If we use lower bound $I(Z^n;F_1,F_2,\cdots, F_r)\geq n(I(V;Z)-I(V,J))$(from equation \eqref{f33})
we get
\begin{align}
     \sum_{j=1}^rR_j&\geq H(X,Y|Z)-S(X;Y\|Z)+
    I(V;Z)-I(V;J)
    . 
  \end{align}

  we need either upper bound for $\sum_{j=1}^rR_j$ or upper bound on $I(Z^n;F_1,F_2,\cdots, F_r)$ through  other ways.


$$H(K)+n\sum_{j=1}^rR_j\approx H(K)+H(F)\approx H(KF)\leq H(X^n,Y^n)$$
Therefore,
$$S(X;Y\|Z)+\sum_{j=1}^rR_j\leq H(X,Y)$$
Therefore,
\begin{align}
     H(X,Y)-S(X;Y\|Z)\geq\sum_{j=1}^rR_j&\geq H(X,Y|Z)-S(X;Y\|Z)+
    I(V;Z)-I(V;J)
    . 
  \end{align}
or
\begin{align}
     H(Z)\geq
    I(V;Z)-I(V;J)
    . 
  \end{align}

\newpage

Let us consider all interactive protocols satisfying
$$\frac1nH(Z^n|F_1\cdots F_r)>\Delta$$
for some given $\Delta>0$.

Letting $V_i=J^{i-1}Z_{i+1}^nF_1\cdots F_r$ and $U_i=K_A$, we have
$$S(X;Y\|Z)\leq I(X;Y|J)+I(U;J|V)-I(U;Z|V).$$
Moreover, we have
\begin{align}
   I(V;Z)\leq I(V;J)+H(Z)-\Delta 
\end{align}


\newpage



{\color{blue}
$$H(X^nY^n|F_1,F_2,\cdots, F_r)\geq H(X^nY^n|F_1,F_2,\cdots, F_rZ^n)\geq nS(X;Y\|Z)$$

Then,
$$\frac1nI(X^nY^n;F_1,F_2,\cdots, F_r)\leq H(X,Y)-S(X;Y\|Z)$$
}




\textbf{ABINNN}


\section{Intution}
If we have an interactive communication where $H(Z^n|F)=0$, then we claim that
$$H(X^n,Y^n|F)=0.$$

For any $x_1^n,y_1^n$ and $x_2^n,y_2^n$ such that
$x_1^n+y_1^n\neq x_2^n+y_2^n$, the probability of $p(x_1^n,y_1^n|F=f)$ and $p(x_2^n,y_2^n|F=f)$
cannot be positive at the same time. However, 
$$p(x_1^n,y_1^n|F=f)=p(x_1^n,y_1^n)a(x_1^n)b(y_1^n)
$$
for some functions $a(\cdot)$ and $b(\cdot)$. Therefore, we must have
$a(x_1^n)b(y_1^n)a(x_2^n)b(y_2^n)=0$.

Show from here that $a(x^n)=0$ for all $x^n$ except for one sequence $x^n$ and similarly for $y^n$. Suppose that $a(x_1^n)>0$ and $a(x_2^n)>0$. Then, take some $y^n$ such that $b(y^n)>0$. Then, choose $y_1^n=y_2^n=y^n$. 



\section{One-way capacity when $Z=f(X)$}
Consider the one-way capacity from $X$ to $Y$ when $Z=g(X)$. Let's impose the assumption that $H(Z^n|F)\geq \Delta$. 

We claim that the one-way key rate
\begin{align}
&S_{\text{ow}}(X;Y\|Z)=\max_{UV\rightarrow X\rightarrow YZ}I(U;Y|V)-I(U;Z|V).
\end{align}
is achieved when $H(X|UVZ)=0$. 

Take some arbitrary $(U,V,X)$. By the functional representation lemma, we can find some $P_{W|UVZX}$ such that $I(ZUV;W)=0$ and $H(X|ZUV)=0$. Considering the joint distribution
$$P_{W|UVZX}P_{Y|X}$$
we have that $WUV\rightarrow X\rightarrow Y$ forms a Markov chain. Then, we claim that changing $U$ to $(U,W)$ would not decrease the expression, i.e.,
\begin{align}
    I(U;Y|V)-I(U;Z|V)\leq I(UW;Y|V)-I(UW;Z|V)
\end{align}
Equivalently,
\begin{align}
    0\leq I(W;Y|UV)-I(W;Z|UV)
\end{align}
which is clear sine $I(W;UVZ)=0$. 

\newpage

\section{Capacity of erasure-function channels}
Assume that $Z=g(X)$ and the channel from $X$ to $Y$ is erasure, i.e., $Y=X$ with probability $1-\epsilon$ and $\{e\}$ otherwise. We claim that $S(X;Y\|Z)=I(X;Y|Z)=(1-\epsilon)H(X|Z)$ is achievable by one-way communication from Alice to Bob. Note that 
\begin{align}
&S_{\text{ow}}(X;Y\|Z)=\max_{UV\rightarrow X\rightarrow YZ}I(U;Y|V)-I(U;Z|V).
\end{align}
Consider $V$ constant, and by the functional representation lemma, let $U$ be such that $I(Z;U)=0$ and $H(X|ZU)=0$. Then, note that $I(U;Z|Y)=0$ since when $Y$ is $X$ it holds and when $Y$ is $e$, it also holds. Then, we have
\begin{align}
I(U;Y|V)-I(U;Z|V)&=I(U;Y)-I(U;Z)\\
&=I(U;Y)
\\&=I(U;YZ)
\\&=I(U;Y|Z)
\\&=I(X;Y|Z)-I(X;Y|ZU)
\\&=I(X;Y|Z)
\end{align}

\begin{corollary} Assume that $Z=f(X,Y)$.     If we choose an auxiliary receiver $J$ such that the channel from $(X,Y)$ to $J$ is erasure, then the upper bound
$$I(X;Y|J)+S_{\text{ow}}(X,Y;J\|Z)=I(X;Y|J)+I(X,Y;J|Z)$$
reduces to the simpler upper bound. 
\end{corollary}
\newpage
\section{Non-Interactive Scenario}
In this section we will find some new upper bound when $Z$ is a function of $X$ and $Y$ and we have only one round of communication. 

Let $U_i=(F_1, X^{i-1})$ and $V_i=(F_2, Y^{i-1})$ be random angularities variables. Then, it is easy to verify the following Markov chains:
\begin{align}
 &U_i\rightarrow X_i\rightarrow (Y_i,Z_i),\\
&V_i\rightarrow Y_i\rightarrow (X_i,Z_i),\\
&(U_i,V_i)\rightarrow (X_i,Y_i)\rightarrow Z_i.  
\end{align}
Using above Markov chains we can obtain lower bound on $H(F_1),H(F_2)$ and $H(F_1,F_2)$ in the following lemma.
\begin{lemma}
  $H(F_1),H(F_2)$ and $H(F_1,F_2)$ have a lower bound as follows:
  \begin{align}
     H(F_1)&\geq \sum_i I(X_i,Y_i;U_i|V_i),\\
     H(F_2)&\geq  \sum_i I(X_i,Y_i;V_i,U_i)-\sum_i I(U_i;X_i),\\
     H(F_1,F_2)&\geq \sum_i
    I(X_i,Y_i;U_i,V_i),\\
    H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|U_i,V_i)
  \end{align}

  \begin{align*}
    \sum_i H(Z_i|U_i,V_i)&=
    \sum_i H(Z_i|F_1,F_2,X^{i-1},Y^{i-1})
    \\&=\sum_i H(Z_i|F_1,F_2,X^{i-1},Y^{i-1},Z^{i-1})
    \\&\leq \sum_i H(Z_i|F_1,F_2,Z^{i-1})
    \\&=H(Z^n|F_1,F_2)
\end{align*}
\end{lemma}

\begin{proof}
    For $H(F_1)$ we have following chains:
    \begin{align*}
    H(F_1)&\geq I(F_1;X^n,Y^n|F_2)
    \\&=\sum_i H(X_i,Y_i|F_2,X^{i-1},Y^{i-1})-\sum_i H(X_i,Y_i|F_1,F_2,X^{i-1},Y^{i-1})
    \\&=\sum_i H(X_i,Y_i|F_2,Y^{i-1})-\sum_i H(X_i,Y_i|F_1,F_2,X^{i-1},Y^{i-1})
    \label{eqnb2}
    \\&=\sum_i I(X_i,Y_i;U_i|V_i).
\end{align*}
Similarly for $H(F_2)$ we have:
\begin{align*}
    H(F_2)&\geq \sum_i I(X_i,Y_i;V_i|U_i)
    \\&=
    \sum_i I(X_i,Y_i;V_i,U_i)-\sum_i I(U_i;X_i,Y_i)
    \\&=
    \sum_i I(X_i,Y_i;V_i,U_i)-\sum_i I(U_i;X_i).
    \end{align*}
    Finally for $H(F_1,F_2)$, we have:
    \begin{align*}
    H(F_1,F_2)&\geq I(F_1,F_2;X^n,Y^n)
    \\&=\sum_i 
    I(F_1,F_2;X_i,Y_i|X^{i-1},Y^{i-1})
    \\&=\sum_i 
    I(F_1,F_2,X^{i-1},Y^{i-1};X_i,Y_i)
    \\&=\sum_i
    I(X_i,Y_i;U_i,V_i).
    \end{align*}
    

\end{proof}
% where \eqref{eqnb2} follows from
% % \begin{align*}
% %     I(X_i,Y_i;X^{i-1}|Y^{i-1},F_2)&\leq I(X_i,Y_i,F_2;X^{i-1}|Y^{i-1})\\
% %     &\leq I(X_i,Y_i,Y^n;X^{i-1}|Y^{i-1})=0.
% % \end{align*}
% Similarly, we have
% \begin{align}
%     H(F_2)&\geq \sum_i I(X_i,Y_i;V_i|U_i)
%     \\&=
%     \sum_i I(X_i,Y_i;V_i,U_i)-\sum_i I(U_i;X_i,Y_i)
%     \\&=
%     \sum_i I(X_i,Y_i;V_i,U_i)-\sum_i I(U_i;X_i)
%     \end{align}
% Moreover,
% \begin{align}
%     H(F_1,F_2)&\geq I(F_1,F_2;X^n,Y^n)
%     \\&=\sum_i 
%     I(F_1,F_2;X_i,Y_i|X^{i-1},Y^{i-1})
%     \\&=\sum_i 
%     I(F_1,F_2,X^{i-1},Y^{i-1};X_i,Y_i)
%     \\&=\sum_i
%     I(X_i,Y_i;U_i,V_i)
%     \end{align}
% Finally,
% $$\sum_i I(U_i;X_i)=\sum_iI(F_1;X_i|X^{i-1})=I(F_1;X^n)$$

% To sum this up, for an optimal code where $F_1$ is independent of $F_2$, we get
\begin{corollary}
  Assume that $F_1$ and $F_2$ are independent distributions on $\{1,2,\cdots,2^{nR_{F_{1}}}\}$ and $\{1,2,\cdots,2^{nR_{F_{2}}}\}$, respectively, then 
\end{corollary}
\begin{align*}
    R_{F_1}&\geq I(X,Y;U|V)=I(U;X),\\
    R_{F_2}&\geq I(X,Y;V|U)=I(V;Y),\\
    R_{F_1}+R_{F_2}&\geq I(X,Y;U,V).
\end{align*}
% Finally,
% \begin{align*}
%     \sum_i H(Z_i|U_i,V_i)&=
%     \sum_i H(Z_i|F_1,F_2,X^{i-1},Y^{i-1})
%     \\&=\sum_i H(Z_i|F_1,F_2,X^{i-1},Y^{i-1},Z^{i-1})
%     \\&\leq \sum_i H(Z_i|F_1,F_2,Z^{i-1})
%     \\&=H(Z^n|F_1,F_2)
% \end{align*}

\textbf{Sefidgaran:}
Assume that $Z$ is the xor of $X$ and $Y$. The first identification comes from Sefidgaran's paper. This identification is not computable. Thus, he relaxed expressions as follows. He  used Fano's inequality, thus, in  following expression  he assumed that  
$H(Z^n|F_1,F_2)=0$.
\begin{theorem}
    Consider the Markov chain $U\rightarrow X\rightarrow Y$, we can relax Lemma 3 as follows: \begin{align}
        H(F_1)&\geq I(X,Y;U|V)+I(X;V|Y),\\
        H(F_2)&\geq I(X,Y;V|U),\\
        H(F_1,F_2)&\geq I(X,Y;U,V).
    \end{align}
   Moreover auxiliary  random variables $U$ and $V$ satisfy the following conditions:
   \begin{align}
       &I(U;X)+I(V;Y)\geq I(U,V;X,Y),\\
       &H(Z|U,V)=0.
   \end{align}
   Furthermore, the cardinality of $U$ and $V$ are bounded as $|\mathcal{U}|\leq 5$ and $|\mathcal{V}|\leq 23$. 
\end{theorem}

\textbf{Second Identification:}
Let $U_i=(F_1, Y^{i-1},Z_{i+1}^n)$, $V_i=(F_2, X^{i-1},Z_{i+1}^n)$ be new axillary random variables.
We have the following Theorem.
\begin{theorem}
The lower bounds on the sum of $H(F_1),H(F_2)$ and $H(Z^n|F_1,F_2)$ are as follows: \begin{align}
H(F_1)+H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|V_i)\\
H(F_2)+H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|U_i)
\\H(F_1)+H(F_2)+2H(Z^n|F_1,F_2)&\geq 
nH(X,Y)+\sum_iH(Z_i|U_i)-H(Y_i|U_i)
\\H(F_1)+H(F_2)+2H(Z^n|F_1,F_2)&\geq 
nH(X,Y)+\sum_iH(Z_i|V_i)-H(X_i|V_i).
\end{align} 
\end{theorem}
\begin{proof}
% Using Marton's  expansion theorem we have: 
% \begin{align*}
%     I(F_1;Z^n)-I(F_1;Y^n)=\sum_i I(U_i;Z_i)-I(U_i;Y_i),
% \end{align*}  
\end{proof}
We begin by obtaining lower bound on $H(Z^n|F_1)$ as follows: 
\begin{align*}
   H(Z^n|F_1)=\sum_i H(Z_i|F_1,Z_{i+1}^n)\geq \sum_i H(Z_i|U_i). 
\end{align*}
Moreover, we have:
\begin{align*}
    H(F_2)&\geq 
    I(F_2;Y^n,Z^n|F_1)
    \\&\geq 
    I(F_2;Z^n|F_1)
    \\&= -H(Z^n|F_1,F_2)+\sum_i H(Z_i|U_i)
\end{align*}


Next, we can write
\begin{align*}
H(F_1)+H(F_2)+2H(Z^n|F_1,F_2)&\geq I(F_1;X^n)+I(F_2;Y^n,Z^n|F_1)+2H(Z^n|F_1,F_2)
\\&=I(F_1;X^n)+I(F_2;Z^n|F_1)+I(F_2;Y^n|F_1,Z^n)+2H(Z^n|F_1,F_2)
\\&=I(F_1;X^n)+H(Z^n|F_1)+I(F_2;Y^n|F_1,Z^n)+H(Z^n|F_1,F_2)
\\&=I(F_1;X^n)+H(Y^n|F_1)+H(Z^n|F_1,Y^n)+I(F_2;Y^n|F_1,Z^n)
\\&\qquad+H(Z^n|F_1)-H(Y^n|F_1)+I(Y^n;Z^n|F_1,F_2)
\\&=H(X^n,Y^n)+I(F_2;Y^n|F_1,Z^n)+H(Z^n|F_1)-H(Y^n|F_1)
\\&\qquad+I(Z^n;Y^n|F_1,F_2)
\end{align*}
where in the last step we use
\begin{align*}
I(F_1;X^n)+H(Y^n|F_1)+H(Z^n|F_1,Y^n)&=I(F_1;X^n)+H(Y^n|F_1)+H(X^n|F_1,Y^n)
\\&=
I(F_1;X^n)+H(X^n,Y^n|F_1)
\\&=
H(X^n,Y^n)
\end{align*}
Thus,
\begin{align*}
H(F_1)+H(F_2)+2H(Z^n|F_1,F_2)&\geq 
H(X^n,Y^n)+I(F_2;Y^n|F_1,Z^n)+H(Z^n|F_1)-H(Y^n|F_1)
\\&\qquad+I(Z^n;Y^n|F_1,F_2)
\\&
\geq 
H(X^n,Y^n)+H(Z^n|F_1)-H(Y^n|F_1)
\\&
=
nH(X,Y)+\sum_iH(Z_i|U_i)-H(Y_i|U_i)
\end{align*}


To sum this up, we get the desired inequalities.

\textbf{Chandra:}
Assume that $Z$ is the xor of $X$ and $Y$. If we assume $H(Z^n|F_1,F_2)=0$ then for $\lambda\geq 1$ we have:

\begin{align}
    &H(F_1)+\lambda H(F_2)\geq H(X,Y)+min_{U:U\rightarrow X\rightarrow Y}\lambda H(Z|U)-H(Y|U),\\
    &\lambda H(F_1)+ H(F_2)\geq H(X,Y)+min_{V:V\rightarrow Y\rightarrow X}\lambda H(Z|V)-H(X|V).
\end{align}
From above,  under assumption $H(Z^n|F_1,F_2)=0$, one can argue the following inequalities
\begin{align}
    H(F_2)\geq min_{U:U\rightarrow X\rightarrow Y} H(Z|U)>0,\\
    H(F_1)\geq min_{V:V\rightarrow Y\rightarrow X} H(Z|V)>0.
\end{align}






% \begin{align}
% H(F_1)+H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|V_i)\\
% H(F_2)+H(Z^n|F_1,F_2)&\geq \sum_i H(Z_i|U_i)
% \\H(F_1)+H(F_2)+2H(Z^n|F_1,F_2)&\geq 
% nH(X,Y)+\sum_iH(Z_i|U_i)-H(Y_i|U_i)
% \\H(F_1)+H(F_2)+2H(Z^n|F_1,F_2)&\geq 
% nH(X,Y)+\sum_iH(Z_i|V_i)-H(X_i|V_i)
% \end{align}




\textbf{Third identification:} This idea is similar to idea in the Section \ref{}. 
We have the following Lemma:
\begin{lemma}
    Take some $W^n$ such that $X^n\rightarrow W^n\rightarrow Y^n$ forms a Markov chain. We have:
    \begin{align}
       n(R_{F_1}+R_{F_2}) \geq I(F_1,F_2;W^n)+\sum_{i=1}^{n}
I(X_iY_i;F_2Y^{i-1}|W^{n})+
\sum_{i=1}^{n}
I(X_iY_i;F_1X^{i-1}|W^{n}).
    \end{align}
\end{lemma}

\begin{proof}
We have the following chains of inequalities:
   \begin{align*}
    n(R_{F_1}+R_{F_2})&\geq H(F_1,F_2)
    \\&\geq I(F_1,F_2;X^n,Y^n,W^n)
    \\&=I(F_1,F_2;W^n)+
    I(F_1,F_2;X^n,Y^n|W^n)
        \\&=I(F_1,F_2;W^n)+
I(F_1;X^n,Y^n|W^n)+I(F_2;X^n,Y^n|W^n,F_1)
     \\&=I(F_1,F_2;W^n)+
    I(F_1;X^n,Y^n|W^n)+
        I(F_2;X^n,Y^n|W^n)    
\\&=I(F_1,F_2;W^n)+\sum_{i=1}^{n}
I(X_iY_i;F_2Y^{i-1}|W^{n})+
\sum_{i=1}^{n}
I(X_iY_i;F_1X^{i-1}|W^{n})
\end{align*} 
\end{proof}
  

\begin{align*}
    n(R_1+R_2)&\geq H(F_1,F_2)
    \\&\geq I(F_1,F_2;X^n,Y^n,W^n)
    \\&=I(F_1,F_2;W^n)+
    I(F_1,F_2;X^n,Y^n|W^n)
        \\&=I(F_1,F_2;W^n)+
I(F_1;X^n,Y^n|W^n)+I(F_2;X^n,Y^n|W^n,F_1)
     \\&=I(F_1,F_2;W^n)+
    I(F_1;X^n,Y^n|W^n)+
        I(F_2;X^n,Y^n|W^n)    
\\&=I(F_1,F_2;W^n)+\sum_{i=1}^{n}
I(X_iY_i;F_2Y^{i-1}|W^{n})+
\sum_{i=1}^{n}
I(X_iY_i;F_1X^{i-1}|W^{n})
\end{align*}


% \begin{align}
% &nR_2+H(W^n|F_1F_2)\geq I(Y^nW^n;F_2|F_1)+H(W^n|F_1F_2)\nonumber\\
% &\overset{(a)}{=}I(W^n;F_2|F_1)+I(Y^n;F_2|F_1W^n)+H(W^n|F_1F_2)=H(W^n|F_1)+I(Y^n;F_2|F_1W^n)\nonumber\\
% &\overset{(b)}{=}H(W^n|F_1)+H(Y^n|F_1W^n)-H(Y^n|F_2F_1W^n)=H(W^n|F_1)+H(Y^n|W^n)-H(Y^n|F_2W^n)\nonumber\\
% &\overset{(c)}{=}
% H(W^n|F_1)+H(Y^n|W^n)-H(Y^nW^n|F_2)+H(W^n|F_2)
% \nonumber\\
% &\overset{(d)}{=}
% H(Y^n|W^n)-H(Y^nW^n|F_2)+H(W^n|F_2)+H(W^n|F_1)-
% H(X^n|F_1)+H(X^n|F_1)\nonumber\\
% &\overset{(e)}{=}nH(Y|W)+\sum_{i=1}^{n}H(W_i|F_2Y^{i-1}W^{n/i})-
% H(W_iY_i|F_2Y^{i-1}W^{n/i})\nonumber\\&+
% \sum_{i=1}^{n}H(W_i|F_1X^{i-1}W^{n/i})-
% H(X_i|F_1X^{i-1}W^{n/i})+H(X^n|F_1)
% \end{align}
% The above it is not good, because there exist term $H(X^n|F_1)$.

% {\color{red}
% We have
% $$H(X^n|F_1)=\sum_{i}H(X_i|F_1,X^{i-1})$$

% }

% But from the above we conclude that 
% \begin{align}
%     &nR_1+nR_2+H(W^n|F_1F_2)\geq\nonumber\\
%     &nH(X)+nH(Y|W)+\sum_{i=1}^{n}H(W_i|F_2Y^{i-1}W^{n/i})-
% H(W_iY_i|F_2Y^{i-1}W^{n/i})\nonumber\\&+
% \sum_{i=1}^{n}H(W_i|F_1X^{i-1}W^{n/i})-
% H(X_i|F_1X^{i-1}W^{n/i})
% \\&=nH(X)+nH(Y|W)-\sum_{i=1}^{n}
% H(Y_i|F_2Y^{i-1}W^{n})\nonumber\\&+
% \sum_{i=1}^{n}H(W_i|F_1X^{i-1}W^{n/i})-
% H(X_i|F_1X^{i-1}W^{n/i})
% \\&=nH(X)+nH(Y|W)+nH(W|X)-\sum_{i=1}^{n}
% H(Y_i|F_2Y^{i-1}W^{n})\nonumber\\&+
% \sum_{i=1}^{n}H(W_i|F_1X^{i-1}W^{n/i})-
% H(X_i,W_i|F_1X^{i-1}W^{n/i})
% \\&=nH(X)+nH(Y|W)+nH(W|X)-\sum_{i=1}^{n}
% H(Y_i|F_2Y^{i-1}W^{n})-
% \sum_{i=1}^{n}
% H(X_i|F_1X^{i-1}W^{n})
% \\&=nH(X,Y,W)-\sum_{i=1}^{n}
% H(Y_i|F_2Y^{i-1}W^{n})-
% \sum_{i=1}^{n}
% H(X_i|F_1X^{i-1}W^{n})
% \\&=H(W^n)+\sum_{i=1}^{n}
% I(Y_i;F_2Y^{i-1}|W^{n})+
% \sum_{i=1}^{n}
% I(X_i;F_1X^{i-1}|W^{n})
% \end{align}
% So, the above means that
% \begin{align}
%     nR_1+nR_2&\geq I(W^n;F_1,F_2)+\sum_{i=1}^{n}
% I(Y_i;F_2Y^{i-1}|W^{n})+
% \sum_{i=1}^{n}
% I(X_i;F_1X^{i-1}|W^{n})
% \\&=I(W^n;F_1,F_2)+\sum_{i=1}^{n}
% I(X_iY_i;F_2Y^{i-1}|W^{n})+
% \sum_{i=1}^{n}
% I(X_iY_i;F_1X^{i-1}|W^{n})
% \end{align}

% Here is a direct proof:

% \begin{align*}
%     n(R_1+R_2)&\geq H(F_1,F_2)
%     \\&\geq I(F_1,F_2;X^n,Y^n,W^n)
%     \\&=I(F_1,F_2;W^n)+
%     I(F_1,F_2;X^n,Y^n|W^n)
%         \\&=I(F_1,F_2;W^n)+
% I(F_1;X^n,Y^n|W^n)+I(F_2;X^n,Y^n|W^n,F_1)
%      \\&=I(F_1,F_2;W^n)+
%     I(F_1;X^n,Y^n|W^n)+
%         I(F_2;X^n,Y^n|W^n)    
% \\&=I(F_1,F_2;W^n)+\sum_{i=1}^{n}
% I(X_iY_i;F_2Y^{i-1}|W^{n})+
% \sum_{i=1}^{n}
% I(X_iY_i;F_1X^{i-1}|W^{n})
% \end{align*}






\color{cyan}
This Section will give us better bounds on $S(X;Y\|Z)$.

Take some $W^{n}$ such that $X^{n}\rightarrow W^{n}\rightarrow Y^{n}$ forms a Markov chain.
We assume that the joint distribution of $(X^{n},Y^{n},Z^{n},F_{1},\cdots,F_{r},K_A,K_B)$ is as follows:
 % $X^n, Y^n, Z^n, F_1, F_2, \cdots, F_r, K_A,K_B$ satisfying
 \begin{align}
\prod_{i}p(x_i,y_i,z_i)p(f_1|x^n)p(f_2|y^n,f_1)p(f_3|f_1,f_2,x^n)\cdots p(k_A|F_{1:r},x^n)p(k_B|F_{1:r},y^n).  
 \end{align}
In the following Lemma, we prove some new Markov chains.


\begin{lemma}\label{mar1:lem}
 Let
$A_i=W^{n\backslash i}$, $U_{t,i}=(F_t,X^{i-1})$ for odd $t$, and $U_{t,i}=(F_t,Y^{i-1})$ for even $t$.Then, for odd $t$ we have the following  Markov chain  
\begin{align}
U_{t,i}\rightarrow (X_i,U_i^{t-1},A_i)\rightarrow Y_i,\label{mar1}
\end{align}
Moreover for even $t$, we have:
\begin{align}
X_i\rightarrow (Y_i,U_i^{t-1},A_i)\rightarrow U_{t,i}. \label{mar2} 
\end{align}
\end{lemma}
\begin{proof}
    Using the chain rule and Markov chain $X^{n}\rightarrow W^{n}\rightarrow Y^{n}$ we have:
    \begin{align}
       &I(X^n;Y^n|X_i,W^{n\backslash i})=0,\\
       &I(X^n;Y^n|Y_i,W^{n\backslash i})=0. 
    \end{align}
    Moreover, since $F_t$ is a function of either $(X^n, F^{t-1})$ or $(Y^n, F^{t-1})$, we get that for odd $t$ 
\begin{align}
 &I(X^nF_t;Y^n|F^{t-1},X_i,W^{n\backslash i})=0,\label{mar:3}\\
&I(X^nF_t;Y^{n}|F^{t-1},Y_i,W^{n\backslash i})=0.   
\end{align}
Similarly, for even $t$, we have
\begin{align}
&I(X^n;Y^nF_t|F^{t-1},X_i,W^{n\backslash i})=0,\\
&I(X^n;F_t,Y^n|F^{t-1},Y_i,W^{n\backslash i})=0.\label{mar:4}
\end{align}
Thus, equations \eqref{mar:3}-\eqref{mar:4} imply Markov chains in 
\eqref{mar1} and \eqref{mar2}.
\end{proof}
\begin{corollary}
    Using Markov chains in Lemma \ref{mar1:lem} we have
    \begin{align}
      H(Z^n|F_1,F_2,\cdots, F_r)\geq \sum_i H(Z_i|U_{1i},U_{2i},\cdots, U_{ri},A_i)  
    \end{align}
\end{corollary}
\begin{proof}
 Using Chain rule and Markov chains in Lemma \ref{mar1:lem} we get chains   of inequalities as follows: 
  \begin{align*}
    H(Z^n|F_1,F_2,\cdots, F_r)&=\sum_i H(Z_i|F_1,F_2,\cdots, F_r,Z^{i-1})
    \\&\geq \sum_i H(Z_i|F_1,F_2,\cdots, F_r,Z^{i-1},X^{i-1},Y^{i-1},W^{n\backslash i})
    \\&=\sum_i H(Z_i|F_1,F_2,\cdots, F_r,X^{i-1},Y^{i-1},W^{n\backslash i})
    \\&=\sum_i H(Z_i|U_{1i},U_{2i},\cdots, U_{ri},A_i)
\end{align*}
\end{proof}
The rate of $i$ th random variable $F_i$ is denoted by $R_i$. The following Lemma gives a lower bound of the sum rate of random variables $F_1,F_2,\cdots,F_r$.
\begin{lemma}
  For every $1\leq t\leq r$, we have
  \begin{align}
     n\sum_{j=1}^tR_j\geq I(W^n;F_1,F_2,F_3,\cdots, F_t)+\sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots,U_{ti}|A_i,W_i). \label{f1}
  \end{align}
\end{lemma}
\begin{proof}
    We proceed as follows
    \begin{align*}
    n\sum_{j=1}^tR_j&\geq H(F_1,F_2,\cdots,F_t)
    \\&\geq I(F_1,F_2,\cdots,F_t;X^n,Y^n,W^n)
    \\&=I(F_1,F_2,\cdots,F_t;W^n)+
    I(F_1,F_2,\cdots,F_t;X^n,Y^n|W^n)
    \\&=I(F_1,F_2,\cdots,F_t;W^n)+
    \sum_{i=1}^nI(F_1,F_2,\cdots,F_t;X_i,Y_i|W^n,X^{i-1},Y^{i-1})
    \\&=I(F_1,F_2,\cdots,F_t;W^n)+
    \sum_{i=1}^nI(F_1,F_2,\cdots,F_t,X^{i-1},Y^{i-1};X_i,Y_i|W^n)
\end{align*}
\end{proof}
The lower bound of sum rate $\sum_{j=1}^tR_j$ consists of two terms. The first one is $I(F_1,F_2,\cdots,F_t;W^n)$ and the second is $\sum_{i=1}^nI(F_1,F_2,\cdots,F_t,X^{i-1},Y^{i-1};X_i,Y_i|W^n)$. We want to obtain some inequalities related to $I(F_1,F_2,\cdots,F_t;W^n)$.
The following Lemma builds a connection between 
$H(X^n,Y^n|F_1,F_2,\cdots, F_r,W^n)$ and $H(Z^n|F_1,F_2,\cdots, F_r,W^n)$
\begin{lemma}
There is a sequence $\epsilon_{n}$ with property $\lim_{n\rightarrow\infty}\epsilon_{n}=0$ such that
\begin{align}
 H(X^n,Y^n|F_1,F_2,\cdots, F_r,W^n)=H(Z^n|F_1,F_2,\cdots, F_r,W^n)+\epsilon_{n}  \label{approx1} 
\end{align}
\end{lemma}
\begin{proof}
     Since $S(X;Y\|W)=0$, $W^n$ must be able to recover the secret keys, and by using omniscience property, we have :
\begin{align*}
H(X^n,Y^n|F_1,F_2,\cdots, F_r,W^n)&= H(X^n,Y^n|F_1,F_2,\cdots, F_r,W^n,K_A,K_B)+\epsilon_{1,n}
\\&= H(Z^n|F_1,F_2,\cdots, F_r,W^n,K_A,K_B)+\epsilon_{2,n}
\\&= H(Z^n|F_1,F_2,\cdots, F_r,W^n)+\epsilon_{n}
\end{align*}
where $\lim_{n\rightarrow\infty}\epsilon_{1,n}=\lim_{n\rightarrow\infty}\epsilon_{2,n}=\lim_{n\rightarrow\infty}\epsilon_{n}=0$.
\end{proof}
Using \eqref{approx1} we have:
\begin{align}
    I(W^n;F_1,F_2,\cdots, F_r)&\geq I(Z^n;F_1,F_2,\cdots, F_r)-I(Z^n;F_1,F_2,\cdots, F_r|W^n)
    \nonumber\\
    &\geq  I(Z^n;F_1,F_2,\cdots, F_r)-nH(Z|W)+H(Z^n|F_1,F_2,\cdots, F_r,W^n)
    \nonumber\\
    & I(Z^n;F_1,F_2,\cdots, F_r)-nH(Z|W)+H(X^n,Y^n|F_1,F_2,\cdots, F_r,W^n)+\epsilon_{n}
    \nonumber\\&= I(Z^n;F_1,F_2,\cdots, F_r)-nH(Z|W)+\sum_i H(X_i,Y_i|U_{1i}, U_{2i}, \cdots, U_{ri}, A_i, W_i)+\epsilon_{n}
    \label{bound1}
\end{align}


   
Using chain rule and Markov chains in Lemma \ref{mar1:lem}, we have
\begin{align}
    I(W^n;F_1,F_2,\cdots, F_r)&\geq I(W^n;Z^n)-I(Z^n;W^n|F_1,F_2,\cdots, F_r)
    \nonumber\\
    &\geq nI(W;Z)-H(Z^n|F_1,...,F_r)+\sum_i H(Z_i|U_{1i},U_{2i},\cdots, U_{ri},A_i)\label{bound2}
\end{align}
Thus, by combination  \eqref{bound1} and \eqref{bound2} we get
\begin{align}
    I(W^n;F_1,F_2,\cdots, F_r)&\geq \max[0,\epsilon_{n}+ I(Z^n;F_1,F_2,\cdots, F_r)-nH(Z|W)+\sum_i H(X_i,Y_i|U_{1i}, U_{2i}, \cdots, U_{ri}, A_i, W_i)].\label{bound3}
\end{align}
Moreover, 
the following observation builds a connection between $I(W^n;F_1,F_2,\cdots, F_r)$ and $S(X;Y\|Z))$:
\begin{align}
    &I(W^n;F_1,F_2,\cdots, F_r)
    \nonumber\\&=I(X^nY^n;F_1,F_2,\cdots, F_r)-I(X^nY^n;F_1,F_2,\cdots, F_r|W^n)
    \nonumber\\&=I(X^nY^n;F_1,F_2,\cdots, F_r)-\sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots, U_{ri}|W_i,A_i)
    \nonumber\\&=I(X^nY^n;F_1,F_2,\cdots, F_r|Z^n)+
    I(Z^n;F_1,F_2,\cdots, F_r)
    -\sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots, U_{ri}|W_i,A_i)
   \nonumber \\&=n(H(X,Y|Z)-S(X;Y\|Z))+
    I(Z^n;F_1,F_2,\cdots, F_r)
    -\sum_i I(X_i,Y_i;U_{1i},U_{2i},\cdots, U_{ri}|W_i,A_i).\label{f2}
\end{align}
Thus, inequalities \eqref{f1} and \eqref{f2} imply 
\begin{align}
    n\sum_{j=1}^rR_j\geq n(H(X,Y|Z)-S(X;Y\|Z)+
    I(Z^n;F_1,F_2,\cdots, F_r).
\end{align}
This gives a lower bound on $S(X;Y\|Z)$.

The authors in  \cite{} have shown for 
\begin{align}
   S(X;Y\|Z)\leq \max_{U,V\rightarrow X,Y\rightarrow J,Z} I(X;Y|J)+I(U;J|V)-I(U;Z|V) 
\end{align}

\color{black}

\textbf{ABINNN}














  





\color{black}


\iffalse
\subsection{Conditions for a Positive SK Capacity}
In an early work, Maurer gives an example where multiple rounds of communication are necessary to achieve a positive SK capacity  \cite[Section V]{Maurer93}. Orlitsky and Wigderson show in \cite{OrlitskyWigderson} that if the SK capacity is positive, only two rounds of communication suffice to realize a positive key rate. In fact, they give a necessary and sufficient condition for the SK capacity to be positive. We begin by reviewing their results. 

Consider some natural number $n$, and some sets $\mathcal{A}\subset \mathcal{X}^n$ and $\mathcal{B}\subset \mathcal{Y}^n$. Denote the pmf of $(X^n, Y^n, Z^n)$ conditioned on the events $X^n\in \mathcal{A}$ and $Y^n\in \mathcal{B}$ by $p_r(\cdot)$ so that $p_r(x^n, y^n, z^n)=0$ if $(x^n, y^n)\notin \mathcal{A}\times \mathcal{B}$; otherwise, we have
\begin{align}p_r(x^n, y^n, z^n)=\frac{p_{X^nY^nZ^n}(x^n, y^n,z^n)}{\mathbb{P}[X^n\in \mathcal{A}, Y^n\in \mathcal{B}]}\label{eqn:AAAd13}\end{align}
where $P_{X^nY^nZ^n}(x^n, y^n,z^n)=\prod_{i=1}^n P_{X,Y,Z}(x_i, y_i, z_i)$ is a product distribution, whereas $p_r(x^n, y^n, z^n)$ is not necessarily a product distribution.



\begin{definition}[Orlitsky-Wigderson \cite{OrlitskyWigderson}]\label{def:advantage}
Given sets $\mathcal{A}\subset \mathcal{X}^n$ and $\mathcal{B}\subset \mathcal{Y}^n$, the legitimate users have a simple entropic advantage over the eavesdropper in $\mathcal{A}\times \mathcal{B}$ if for some (binary) function $f(X^n)$ we have
\begin{align}I_{p_r}(f(X^n); Y^n)> I_{p_r}(f(X^n); Z^n)\end{align}
where the mutual information expressions are calculated according to
$p_r(x^n, y^n, z^n)$.
\end{definition}

\begin{theorem} [Orlitsky-Wigderson \cite{OrlitskyWigderson}] \label{OrlitskyWigdersonThm}
The following three claims are equivalent.
\begin{enumerate}
\item $S(X;Y\|Z)>0$.
\item There exists some natural number $n$, and sets $\mathcal{A}\subset \mathcal{X}^n$ and $\mathcal{B}\subset \mathcal{Y}^n$, such that the legitimate users have a simple entropic advantage over the eavesdropper in $\mathcal{A}\times \mathcal{B}$. 
\item  A positive rate is achievable with only two rounds of communication.
\end{enumerate}
\end{theorem}
\begin{proof} The work \cite{OrlitskyWigderson} does not include proofs. We therefore give a sketch of a proof based on personal communication with the authors. 



We first prove that 1) implies 2). As mentioned in the introduction, an interactive communication $\mathbf{F}$ without private randomization suffices to achieve positive key rates, i.e., without loss of generality we may assume $H(F_1|X^n)=H(F_2|F_1Y^n)=H(F_3|F_{1:2}X^n)=...=0$. Orlitsky and Wigderson observe that for any $\mathbf{F}$ without private randomization, the conditional pmf of $(X^n, Y^n, Z^n)$ given $\mathbf{F}=\mathbf{f}$ has the form $p_r(x^n, y^n, z^n)$ for some sets $\mathcal{A}$ and $\mathcal{B}$ that depend on $\mathbf{f}$. This follows from the rectangle property of communication complexity, e.g., see \cite[p. 10]{kushilevitz_nisan_1996}. Given $S(X;Y\|Z)>0$, from 1) there is an $(n,\delta)$ code with a positive rate $R>0$ and a sufficiently small $\delta$. Observing that $\frac 1n I(K_A;Y^n|\mathbf{F})\geq \frac 1n I(K_A;K_B|\mathbf{F})\approx R$  and $\frac 1n I(K_A;Z^n|\mathbf{F})\approx 0$, we have
\begin{align}I(K_A;Y^n|\mathbf{F})-I(K_A;Z^n|\mathbf{F})>0\end{align}
so there exists an $\mathbf{F}=\mathbf{f}$ such that 
\begin{align}I(K_A;Y^n|\mathbf{f})-I(K_A;Z^n|\mathbf{f})>0.\end{align}
Note that given a $\mathbf{F}=\mathbf{f}$, $K_A$ is a function of $X^n$. 

We next prove that 2) implies 3). Suppose that Alice and Bob observe $N$ independent blocks, each of which consists of $n$ i.i.d.\ realizations of $(X,Y)$. Consider one of the blocks. Suppose that Alice observes $X^n$ and Bob observes $Y^n$ in that block. Alice declares on the public channel whether or not $X^n\in\mathcal{A}$, and Bob declares whether or not $Y^n\in\mathcal{B}$.  If $X^n\notin\mathcal{A}$ or $Y^n\notin\mathcal{B}$, they discard the block. Otherwise, they use the block for key agreement. The fraction of used blocks is asymptotically equal to $\mathbb{P}[X^n\in \mathcal{A}, Y^n\in \mathcal{B}]$. In the used blocks, the conditional pmf of the source is $p_r(x^n, y^n, z^n)$, and Alice has a simple entropic advantage over Eve, so she can apply a one-way SK generation scheme to produce a shared key with Bob. Alice's and Bob's declarations count as two rounds of communications. However, if Alice declares that $X^n\in\mathcal{A}$ and if $Y^n\in\mathcal{B}$, then Bob hears Alice's response and attaches the necessary public information for SK generation to his declaration so that no more than two rounds of communication are required. 

Since 1) implies 2) and 2) implies 3), we have that 1) also implies 3). Finally, going from 3) to 1) is immediate, and going from 3) to 2) is possible by going from 3) to 1) and then 1) to 2), as shown above.
\end{proof}

Finally, a sufficient condition for $S(X;Y\|Z)=0$ for the special class of erasure sources is given in \cite{MaurerWolf99}.
\begin{definition}\cite[Definition 4]{MaurerWolf99}\label{def:Fpxy}
Given a joint pmf $P_{X,Y}$ on discrete sets $\mathcal{X}$ and $\mathcal{Y}$, let 
\begin{align}
&F(p_{XY})=\min_{q_{XY}}\left(\max_{x,y}\left(\frac{p_{XY}(x,y)}{q_{XY}(x,y)}\right)\cdot\max_{x,y}\left(\frac{q_{XY}(x,y)}{p_{XY}(x,y)}\right)\right)
\end{align}
where the minimum is taken over all product pmfs $q_{XY}=q_X q_Y$ and where we set $0/0:=1$, $c/0:=\infty$ for $c>0$. 
Further, the \emph{``deviation from independence of $p_{XY}$"} is defined as
\begin{align}
   d_{\emph{ind}}(p_{XY})=1-\frac{1}{F(p_{XY})}.
\end{align}
\label{def-Maurer}
\end{definition}
For example, for the DSBE we have \cite[p. 509]{MaurerWolf99}
\begin{align}
	d_{\text{ind}}(p_{XY})= 1- \frac{p}{1-p}. 
\end{align}



\begin{theorem} \cite[Theorems 14 and 15]{MaurerWolf99}\label{theo:MaurerWolftheo1415}
For the erasure source $p_{XY}(x,y)p_{Z|XY}(z|x,y)$ with erasure probability $\epsilon$, we have $S(X;Y\|Z)=B_0(X;Y\|Z)=0$ if 
$\epsilon\leq 1-d_{\emph{ind}}(p_{XY})$. Furthermore, for the special case of the DSBE source the converse is also true, namely $S(X;Y\|Z)>0$ if $\epsilon>1-d_{\emph{ind}}(p_{XY})$. \label{theoremWolf} 
\end{theorem}
In this paper, we generalize Theorem~\ref{theo:MaurerWolftheo1415} with Theorem \ref{theoremeq:eps12} below.



\subsection{Classification of Sources with Zero Secret Key Capacity}
We introduce a new quantity $\Delta(X;Y\|Z)$ that provides insight into sources with zero key capacity. In particular, it allows one to compare sources with zero key capacity with each other. When $S(X;Y\|Z)=0$, it is not possible for Alice and Bob to agree on a shared key. However, it may be still possible for Alice and Bob to agree on a key that is \emph{approximately} secret. The quantity $\Delta(X;Y\|Z)$ quantifies the goodness of the approximate secret key.

Suppose Alice and Bob wish to agree on a \emph{single secret bit}. That is, instead of achieving a key \emph{rate}, they produce \emph{bits} $K_A\in\{1,2\}$ and $K_B\in\{1,2\}$ respectively. Alice and Bob wish to maximize $\mathbb{P}[K_A=K_B]$ while minimizing leakage to Eve who has $Z^n$ and the public discussion transcript $\mathbf{F}$. We can measure the quality of the keys $K_A$ and $K_B$ via the total variation distance
\begin{align}
\|p_{K_A K_B Z^n \mathbf{F}} - q_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  \|_{TV} \label{quality}
\end{align}
where \begin{align}q_{K_AK_B}(k_A, k_B)=\frac{1}{2}\mathds{1}[k_A=k_B]\label{quality-q}\end{align} is the ideal distribution on $\mathcal K_A\times \mathcal K_B=\{1,2\}^2$. If the total variation distance given in \eqref{quality} vanishes, then Alice and Bob have perfect secret bits. The same total variation distance as in \eqref{quality-q} has been previously utilized in \cite[Eq. 3]{tyagi2014bound}.

We are interested in the  minimum of \eqref{quality} over all public discussion protocols as we let the number of source observations $n$ tend to infinity.

\begin{definition} Given a source $P_{X,Y,Z}$, let $\Delta(X;Y\|Z)$ be the infimum of (\ref{quality}) over all public discussion schemes $\mathbf{F}$ (of arbitrary length) that produce single bits $K_A$ and $K_B$ by Alice and Bob, respectively, and where the number $n$ of source observations tends to infinity. In other words, we let
\begin{align}
   &\Delta(X;Y\|Z)=\inf \|p_{K_A K_B Z^n \mathbf{F}} - q_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  \|_{TV}
\end{align}
where the infimum is over all $n\in\mathbb{N}$, arbitrary finite sets $\mathcal{F}_1$, $\mathcal{F}_2, \cdots, \mathcal{F}_k$ for some $k\in\mathbb{N}$, arbitrary conditional pmfs $p_{F_1|X^n}, 
p_{F_2|F_1Y^n}, p_{F_3|F_1F_2X^n},\ldots,$ and arbitrary conditional pmfs $p_{K_A|X^n\mathbf{F}}$ and $p_{K_B|Y^n\mathbf{F}}$, where $K_A$ and $K_B$ are binary and $q_{K_AK_B}$ is as given in \eqref{quality-q}. 
\end{definition}


\section{Main Results}\label{main-result}

We give generic results about the positivity of the SK capacity in Section \ref{sec:general}. We then restrict attention to erasure sources in Section \ref{sec:erasure}. 

\subsection{General Sources}\label{sec:general}
\begin{theorem}\label{genertlsd} Let $p_{XY}$ and $q_{XY}$ be two pmfs satisfying $q_{XY}\preceq p_{XY}$ (as defined in Definition \ref{defpreceq}). Consider a channel $p_{Z|XY}$ and let 
\begin{align*}
	&p_{XYZ}=p_{XY}\, p_{Z|XY} \\ &q_{XYZ} = q_{XY}\, p_{Z|XY}.
\end{align*}	
If the SK capacity $S(X;Y\|Z)$ under $q_{XYZ}$ is positive, then the SK capacity $S(X;Y\|Z)$ under $p_{XYZ}$ is also positive.
\end{theorem}
Theorem~\ref{genertlsd}  is proved in Section~\ref{eq:genertlsd}. Intuitively, the condition $q_{XY}\preceq p_{XY}$ allows simulating the source $q_{XY}$ from $p_{XY}$ via rejection sampling, i.e., each observation $(X,Y)$ from $p_{XY}$ is either accepted or rejected by Alice and Bob via discussion on the public channel. 
If it is possible to generate a secret key under $q_{XYZ}$, then Alice and Bob simulate $q_{XYZ}$ from $p_{XYZ}$ and utilize the protocol for $q_{XYZ}$ to generate a SK with positive rate.


The following theorem gives a condition to achieve a positive SK rate in terms of the R{\'e}nyi divergence of order $1/2$.

\begin{theorem}\label{gentheoremeq:eps12} Consider the source $p_{XY}\,p_{Z|XY}$. Then the following conditions are equivalent.
\begin{enumerate}[label=(\roman*)]
\item  The key capacity $S(X;Y\|Z)$ is positive.
\item  There is an integer $n$, disjoint non-empty sets $\mathcal{A}_1, \mathcal{A}_2\subset \mathcal{X}^n$, and disjoint non-empty sets $\mathcal{B}_1, \mathcal{B}_2\subset \mathcal{Y}^n$ such that (see Definition~\ref{def:renyidivv}) for $(X^n, Y^n, Z^n)$ that are i.i.d. with pmf $p_{X YZ}$ we have
\end{enumerate}
\begin{align}
	&D_{\frac 12}\Big(p_{Z^n}(\cdot | X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1)\Big\| p_{Z^n}(\cdot  |X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2)\Big)\nonumber\\
	&<
	\log\Bigg(\frac{
	\mathbb{P}[X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1]
}{
\mathbb{P}[X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_2]
}\frac{
	\mathbb{P}[X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2]
}{
\mathbb{P}[X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_1]
}\Bigg)\label{eq:nproductnonzeroSKrate}
\end{align}
where $p_{Z^n}(\cdot | \mathcal{E})$ is the pmf of $Z^n$ conditioned on the event $\mathcal{E}$.	
\begin{enumerate}[label=(\roman*)]
\setcounter{enumi}{2}
\item $\Delta(X;Y\|Z)=0$.
\item $\Delta(X;Y\|Z)<\frac{3-\sqrt{5}}{8}\approx 0.095$.
\end{enumerate}
\end{theorem}
\begin{remark} Both the characterization given in item 2 of Theorem \ref{OrlitskyWigdersonThm} and  the characterization in item (ii) of Theorem \ref{gentheoremeq:eps12} are $n$-letter characterizations. Neither  makes the problem of identifying sources with positive SK rate decidable for general sources, \emph{i.e.,} the characterizations are not computable. However, for 
the special case of erasure sources when  either $X$ or $Y$ is binary, the characterization in Theorem \ref{gentheoremeq:eps12} turns out to be helpful, while it is not clear how to make use of the characterization in Theorem \ref{OrlitskyWigdersonThm}. Moreover,   in contrast to the characterization given in item 2 of Theorem \ref{OrlitskyWigdersonThm}, the characterization in Theorem \ref{gentheoremeq:eps12} considers all probabilities with respect to the unconditional product pmf $\prod_{i=1}^n p_{XYZ}(x,y,z)$. Furthermore, \eqref{eq:nproductnonzeroSKrate} can be equivalently expressed as
\begin{align}\nonumber
	&\sum_{z^n}\!\Big(\mathbb{P}[Z^n=z^n, X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1]^{\frac{1}{2}}\times \mathbb{P}[Z^n=z^n, X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2]^{\frac 12}\Big)\nonumber
	\\[0.5ex]&\;>
\mathbb{P}[X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_2]^{\frac 12}\;
\mathbb{P}[X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_1]^{\frac 12}
\label{eq:nproductnonzeroSKrate23}.
\end{align}
Equation \eqref{eq:nproductnonzeroSKrate23} involves only a product of unconditional probability terms, while  the characterization given in item 2 of Theorem \ref{OrlitskyWigdersonThm} is based on mutual information for conditional expressions. 
\end{remark}
\begin{remark} 
The constant $\frac{3-\sqrt{5}}{8}$ in Theorem \ref{gentheoremeq:eps12} is not necessarily optimal. 
It is an interesting question to find the best possible constant, i.e., the
minimum possible value of $\Delta(X;Y\|Z)$ over all sources that satisfy $S(X;Y\|Z)=0$.
\end{remark}
\begin{corollary}\label{corollary-new-1} Considering item (ii) of Theorem~\ref{gentheoremeq:eps12} for $n=1$, the SK capacity $S(X;Y\|Z)$ is positive  if one can find distinct symbols $x_1, x_2 \in\mathcal{X}$ and distinct symbols $y_1, y_2 \in\mathcal{Y}$ such that 
\begin{align}
	&D_{\frac 12}(p_{Z|XY}(\cdot |x_1,y_1)\| p_{Z|XY}(\cdot |x_{2},y_{2}))<
	\log\left(\frac{
		p_{XY}(x_1, y_{1})p_{XY}(x_2, y_{2})}{p_{XY}(x_1, y_{2})p_{XY}(x_2, y_{1})}\right).
\label{eqn:gentheoremeqA2}
\end{align}	

\begin{corollary} 
Fix some pmfs $q_{X,1}$, $q_{X,2}$, $q_{Y,1}$, $q_{Y,2}$, and $q_Z$. 
For $i,j=1,2$, we define
\begin{align}
  \kappa_{i,j}&=\min_{r_{XY}:\: r_X=q_{X,i}, r_Y=q_{Y,j}}D(r_{XY}\|p_{XY})
\end{align}
and
\begin{align}
\theta_{i,j}&=\min_{r_{XYZ}} D(r_{XYZ}\|p_{XYZ}) \label{eqnDR}
\end{align}
where the minimum in \eqref{eqnDR} is over $r_{XYZ}$ satisfying $r_X=q_{X,i}, r_Y=q_{Y,j}, r_Z=q_Z$. 
Then, $S(X;Y\|Z)>0$  if  
\begin{align}&\theta_{1,1}+\theta_{2,2}<
\kappa_{1,2}+\kappa_{2,1}.\label{cond1}
\end{align}
To see this, assume that $q_Z$, $q_{X,1}$, $q_{X,2}$, $q_{Y,1}$, and $q_{Y,2}$ are types, and $\mathcal{A}_i$ and $\mathcal{B}_j$ are the set of typical sequences with types $q_{X,i}$ and $q_{Y,j}$, respectively, \emph{i.e.,} $\mathcal{A}_i=\mathcal{T}^{(n)}_{q_{X,i}}$ and $\mathcal{B}_j=\mathcal{T}^{(n)}_{q_{Y,j}}$. 

Positivity of the SK capacity follows if \eqref{eq:nproductnonzeroSKrate23} holds. Using Lemma \ref{LemmaMT} and the simple inequality
\begin{align}\nonumber
	&\sum_{z^n}\!\Big(\mathbb{P}[Z^n=z^n, X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1]^{\frac{1}{2}}\times \mathbb{P}[Z^n=z^n, X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2]^{\frac 12}\Big)\nonumber
	\\[0.5ex]&\;\geq 
\sum_{z^n\in \mathcal{T}^{(n)}_{q_Z}}\!\Big(\mathbb{P}[Z^n=z^n, X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1]^{\frac{1}{2}}\times \mathbb{P}[Z^n=z^n, X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2]^{\frac 12}\Big)
\end{align}
we observe that \eqref{eq:nproductnonzeroSKrate23} holds as $n\rightarrow \infty$ if \eqref{cond1} holds. 

Consider an erasure source, and the special case of $q_Z(z)=\mathbf{1}[z=e]$. Then we have
\begin{align}
\theta_{i,j}=\kappa_{i,j}+\log\frac{1}{\epsilon}.
\end{align}
In other words, the SK rate is positive if 
\begin{align}&\log(\epsilon)>
\frac{1}{2}\left(\kappa_{1,1}+\kappa_{2,2}-\kappa_{1,2}-\kappa_{2,1}\right).
\end{align}
In particular if $q_{X,1}(x)=\mathbf{1}[x=x_1]$, $q_{X,2}(x)=\mathbf{1}[x=x_2]$, $q_{Y,1}(y)=\mathbf{1}[y=y_1]$, $q_{Y,2}(y)=\mathbf{1}[y=y_2]$, we obtain
\begin{align}
  \kappa_{i,j}=\log\frac{1}{p_{XY}(x_i, y_j)}
\end{align}
and hence the SK key is positive if
\begin{align}
  \epsilon>\left(\frac{p_{XY}(x_1, y_2) p_{XY}(x_2, y_1)}{p_{XY}(x_1,y_1)p_{XY}(x_2,y_2)}\right)^{\frac 12}.\label{eq:conditionforerasuresourcenonzerorate}
\end{align}
Alternatively, the condition (\ref{eq:conditionforerasuresourcenonzerorate}) can be deduced also from \eqref{eqn:gentheoremeqA2} (see \eqref{eqnN149} for the calculation). 
\end{corollary}
\end{corollary} 


Theorem~\ref{gentheoremeq:eps12} is proved in Section~\ref{proofgentheoremeq:eps12} by using a hypothesis testing approach. The left hand side of \eqref{eq:nproductnonzeroSKrate} is the error exponent of the adversary in a hypothesis testing problem while the right hand side is the error exponent of the legitimate parties. The theorem shows that key agreement is feasible when the legitimate parties have a better exponent than the adversary.



\subsection{Erasure Sources}\label{sec:erasure}
We illustrate the condition (\ref{eq:nproductnonzeroSKrate}) for erasure sources and relate it to previously known bounds. Suppose we are given a joint pmf $p_{XY}$. Without loss of generality, we assume that $p_X(x)>0$ and $p_Y(y)>0$ for all $(x,y)\in \mathcal{X}\times\mathcal{Y}$ throughout this section. 
We define a path, which is used in the proofs of the theorems given below.
\begin{definition}\label{def:path}
A sequence $(x_1, y_1, x_2, y_2, \cdots, x_k, y_k)$ forms a path if all $x_i$'s with $x_i\in\mathcal{X}$ are distinct and also all $y_i$'s with $y_i\in\mathcal{Y}$ are distinct. We say the length of the path is $2k$ and we assign the following value to the path 
\begin{align}
	\left(\frac{\prod_{i=1}^kp_{XY}(x_i,y_i)}{p_{XY}(x_1, y_k) \prod_{i=2}^kp_{XY}(x_i,y_{i-1})}\right)^{1/k}.\label{eq:assignedvalue}
\end{align}
Let $\epsilon_{1}$ be the minimum  value of all possible paths and $\epsilon_{2}$ be the minimum  value of all possible paths of length at most four. In particular, we have
\begin{align}
	\epsilon_2=\min_{x_1\neq x_2, y_1\neq y_2}\left(\frac{p_{XY}(x_1,y_1)p_{XY}(x_2,y_2)}{p_{XY}(x_1, y_2) p_{XY}(x_2, y_1)}\right)^{\frac 12}.
\end{align}
\end{definition}

\begin{example} \label{example2n}
\normalfont	
Suppose that $X$ and $Y$ are binary with a joint pmf $p_{XY}$. Then paths of length two are of the form $(x_1, y_1)$ for some $x_1,y_1\in\{0,1\}$, and, by definition, are assigned the value $1$. Because the $x_i$'s and $y_i$'s are distinct in a path, paths of length more than four do not exist. There are multiple paths of length four. For instance, $(x_1=0, y_1=1, x_2=1, y_2=0)$ is assigned the value
$$\frac{\sqrt{p_{XY}(0,1)p_{XY}(1,0)}}{\sqrt{p_{XY}(0,0)p_{XY}(1,1)}}$$
and $(x_1=0, y_1=0, x_2=1, y_2=1)$ is assigned the value
$$\frac{\sqrt{p_{XY}(0,0)p_{XY}(1,1)}}{\sqrt{p_{XY}(0,1)p_{XY}(1,0)}}.$$ 
All other paths have one of the above two values and therefore
\begin{align}
&\epsilon_1=\epsilon_2=\min\Bigg\{\frac{\sqrt{p_{XY}(0,1)p_{XY}(1,0)}}{\sqrt{p_{XY}(0,0)p_{XY}(1,1)}},\frac{\sqrt{p_{XY}(0,0)p_{XY}(1,1)}}{\sqrt{p_{XY}(0,1)p_{XY}(1,0)}}\Bigg\}.\label{eqn:binbi}
\end{align}
Note that one of the terms inside the minimum is less than or equal to one. We therefore do not need to consider paths of length two whose values are one.      
\end{example}
\begin{example} \label{example3n}
As mentioned at the beginning of this section, we assume positive marginal distributions $p_X(x)>0$ and $p_Y(y)>0$ for all $(x,y)\in \mathcal{X}\times\mathcal{Y}$.
Assume that $p_{XY}(x^*,y^*)=0$ for some $(x^*,y^*)\in\mathcal{X}\times\mathcal{Y}$.   Then $\epsilon_1=\epsilon_2=0$. This can be seen by starting the path with $x_1=x^*, y_1=y^*$.
\end{example}

We now give lower and upper bounds on the maximum erasure probability for which the SK capacity is zero for an erasure source. 
 

\begin{theorem}\label{theoremeq:eps12} For the erasure source $p_{XY}\, p_{Z|XY}$ with erasure probability $\epsilon$, we have $S(X;Y\|Z)=0$ if $\epsilon\leq \epsilon_1$, and $S(X;Y\|Z)>0$ if $\epsilon> \epsilon_2$, where $\epsilon_1$ and $\epsilon_2$ are as in Definition~\ref{def:path}. Moreover, $\epsilon_1=\epsilon_2$ if $X$ or $Y$ is binary. We also have $\epsilon_1=\epsilon_2=0$ when $p_{XY}(x^*,y^*)=0$ for some $(x^*,y^*)\in\mathcal{X}\times\mathcal{Y}$. For these special cases, $S(X;Y\|Z)>0$ if and only if $\epsilon>\epsilon_1=\epsilon_2$.
\label{mmthm1s}
\end{theorem}

Theorem~\ref{theoremeq:eps12} is proved in Section~\ref{eq:maintheoremsproof}. Positivity of $S(X;Y\|Z)$ for $\epsilon> \epsilon_2$ is derived by using the result of Theorem \ref{gentheoremeq:eps12}. 

\begin{remark} Parameter $1-\epsilon$ quantifies the information leakage to Eve. Intuitively speaking, the condition $\epsilon>\epsilon_2$ or $1-\epsilon<1-\epsilon_2$ states that key agreement is possible if the correlation between $X$ and $Y$ (as measured by $1-\epsilon_2$) is larger than the leakage to Eve. In fact, the quantities $\epsilon_1$ and $\epsilon_2$ can be used to define measures of correlation. For instance, Maurer and Wolf take 
\begin{align}
	d_{\emph{ind}}(p_{XY})=1-\frac{1}{F(p_{XY})}=1-\epsilon_1
\end{align}	
as a measure of correlation. We propose 
\begin{align}
	\log F(p_{XY})=\log\frac{1}{\epsilon_1}
\end{align}
as yet another measure. Observe that $\log F(p_{XY})$ can be expressed in terms of Renyi-divergence of order infinity:
\begin{align}
	\log\frac{1}{\epsilon_1}&=\log F(p_{XY})=\min_{q_X,q_Y}\!\bigg(D_\infty(p_{XY} \| q_X\, q_Y)+D_\infty(q_X\,q_Y \| p_{XY})\bigg).
\end{align}
We define the Renyi-Jeffrey's divergence (RJ divergence) between two distributions $p$ and $q$ as
\begin{align}
	D^{RJ}_{\alpha}(p\|q)\triangleq \frac{1}{2}\left(D_\alpha(p\|q)+D_\alpha(q\|p)\right)
\end{align}
which is just Jeffrey's divergence (symmetrized KL divergence) in its R{\'e}nyi form. We next define the RJ information of order $\alpha$ as the minimum RJ divergence between a given joint distribution and all product distributions:
\begin{align}
	RJ_\alpha(X;Y)\triangleq \min_{q_Xq_Y}D^{RJ}_{\alpha}(p_{XY} \| q_X\, q_Y).\label{eq:RJalpha}
\end{align}
Observe that $RJ_\infty(X;Y)=\frac12\log F(p_{XY})$. The way RJ information is defined in (\ref{eq:RJalpha}) parallels the way $\alpha$-R{\'e}nyi mutual information is defined in \cite{lapidoth2019two} and \cite{tomamichel2017operational}[Equation (58)].

Similarly for $\epsilon_2$, we propose 
\begin{align}
	\log\frac{1}{\epsilon_2}
\end{align}
as a new measure of correlation (see (\ref{eqn:J134}) below) and study its properties in Appendix \ref{appendixB}. 
\end{remark}
\begin{remark}
Theorem~\ref{theoremeq:eps12} generalizes \cite[Theorems 14 and 15]{MaurerWolf99} which was summarized in Theorem \ref{theoremWolf}. In the proof, we show (via the duality theorem for linear programs) that $\epsilon_1$ (of Definition \ref{def:path}) is the same quantity as $1-d_{\emph{ind}}(p_{XY})$ (of Definition \ref{def-Maurer}). Observe that Theorem~\ref{theoremeq:eps12} claims $S(X;Y\|Z)>0$ if $\epsilon> \epsilon_2$ for any pmf $p_{XYZ}$, while \cite[Theorem 14]{MaurerWolf99} considers only the DSBE source. We remark that (i) the code we use to prove $S(X;Y\|Z)>0$ for general distributions differs from the one used by \cite{MaurerWolf99} for the DSBE source. Our code applies the swapping concept  and works for general sources, (ii) for the special case of the DSBE source, our code and the one used in \cite{MaurerWolf99} give the same bound on $\epsilon$ for the positivity of the SK capacity, 
(iii) for the DSBE source, the code used in \cite{MaurerWolf99} achieves higher secret key rates for $\epsilon>\epsilon_2$. However, the code of \cite{MaurerWolf99} exchanges more information on the public channel.
\end{remark}


We now study the one-way SK rate and the lower bound $\bar{L}(X;Y\|Z)$ obtained from (\ref{eq:lowerbound}) for an erasure source. 




\begin{theorem}\label{theorem-lowerbounds} For an erasure source $p_{XY}\,p_{Z|XY}$ with erasure probability $\epsilon$ such that $p_{X}(x)>0,\; p_Y(y)>0$ for all $(x,y)\in\mathcal{X}\times\mathcal{Y}$, the following statements hold.
\begin{enumerate}
\item The  one-way SK rate from Alice to Bob vanishes if and only if
\begin{align}
	\epsilon\leq 1-\eta(p_{Y|X})\label{eq:Theorem6part1}
\end{align}	
where $\eta(\cdot)$ is defined in (\ref{eqnAG687rr657}). A similar statement holds for the one-way SK rate from Bob to Alice. 
\item We have $\bar{L}(X;Y\|Z)=0$ if and only if 
\begin{align}
	\epsilon\leq 1-\max_{\substack{q_{XY}:\: q_{XY}\preceq p_{XY}}}\rho^2_m(q_{XY})\label{eq:theorem4lowerbound}
\end{align}
where $\rho_m(\cdot)$ is defined in (\ref{eq:max-correlatoin-31}). 
\item The upper bound $ B_0(X;Y\|Z)$ in (\ref{eq:B0upperbound}) is zero  if and only if $\epsilon\leq \epsilon_3$ where
\begin{align}
	\epsilon_3=\max \sum_{t=1}^{|\mathcal{Z}|}\min_{x,y:\: p_{X,Y}(x,y)>0}\delta_{x,y,t} \label{theoremeq:maxmin2}
\end{align} 
and the maximum is over all $\delta_{x,y,t}$ such that $\delta_{x,y,t}\geq 0$, the matrix $[p_{XY}(x,y)\delta_{x,y,t}]$  has rank 1 for all $t$, and $\sum_{t=1}^{|\mathcal{Z}|}\delta_{x,y,t}=1$ for all $x,y$. Here, for every $t$, $[p_{XY}(x,y)\delta_{x,y,t}]$  is
a matrix with dimensions ${|\mathcal{X}|\times|\mathcal{Y}|}$ whose rows  and columns are indexed by the realizations of $X$ and $Y$, respectively, and whose $(x,y)$ entry is $p_{XY}(x,y)\delta_{x,y,t}$ for all $(x,y)\in\mathcal{X}\times\mathcal{Y}$.
\item Assume that $P_{XY}>0$ for all $x,y$ (the case where $P_{XY}=0$ for some $x,y$ was discussed in Theorem \ref{theoremeq:eps12}). The upper bound $ B_1(X;Y\|Z)$ in (\ref{eq:B1upperbound}) is zero  if and only if 
$\epsilon\leq \epsilon_4$ where
\begin{align}
	\epsilon_4 =1- \inf\eta(p_{J|XY}).
\end{align}
Here the infimum is taken over channels $p_{J|XY}$ 
for which $I(X;Y|J)=0$ for $p_{XYJ}=p_{XY}p_{J|XY}$. 
\end{enumerate}

\end{theorem}

The proof of Theorem~\ref{theorem-lowerbounds} is given in Section~\ref{subsec:proofdsbe22}.
\begin{remark}
Since $B_1(X; Y ||Z) \!\leq\! B_0(X; Y ||Z)$, we have $\epsilon_3 \leq\epsilon_4$.
We show below that $\epsilon_1 \leq \epsilon_3$, where $\epsilon_1$ is as given in Definition \ref{def:path}.
This implies that $\epsilon_1 \leq \epsilon_3 \leq \epsilon_4$.
Therefore, the bound in terms of $\epsilon_4$ given in the last part of Theorem \ref{theorem-lowerbounds} is tighter than the bound in terms of $\epsilon_1$ given in Theorem \ref{theoremeq:eps12}.
The reason for stating Theorem \ref{theoremeq:eps12} with $\epsilon_1$ instead of $\epsilon_4$ is that the definition of $\epsilon_1$ is explicit, computable and can be readily related to $\epsilon_2$.
On the other hand, $\epsilon_4$ is not computable in general since we do not have a cardinality bound on $J$.
However, for any particular choice of $p_{J|XY}$ we deduce that $S(X;Y\|Z)=0$ if  $I(X;Y|J)=0$ and $\epsilon\leq 1- \eta(p_{J|XY})$, but we do not know the best value for $1- \eta(p_{J|XY})$ as we vary over all $p_{J|XY}$ satisfying $I(X;Y|J)=0$.

The value of $\epsilon_3$ given in part 3 of Theorem \ref{theorem-lowerbounds} is greater than or equal to $\epsilon_1$ given in Definition \ref{def:path}. To see this,
observe from Theorem \ref{lemma-generaldis1} that one can find some $\tilde{ \delta}_{x,y}\in [0,1]$ such that 
\begin{align}
	\epsilon_1= \min_{x,y}\tilde{\delta}_{x,y}
\end{align}
and the matrix $[p_{XY}(x,y)\tilde{\delta}_{x,y}]$ has rank one. To prove that $\epsilon_1\leq \epsilon_3$,  we need to find appropriate $\delta_{x,y,t}$ for $t=1,2,\cdots, |\mathcal{Z}|$ such that 
\begin{align}
	\min_{x,y}\tilde{\delta}_{x,y}\leq \sum_{t=1}^{|\mathcal{Z}|}\min_{x,y:\: p_{XY}(x,y)>0}\delta_{x,y,t}.
\end{align}

As shown in the proof of part 3 of Theorem \ref{theorem-lowerbounds}, the quantity $\epsilon_3$ remains the same if we allow  $t$ to take values in a larger set $\{1,2,\cdots, M\}$ for some $M>|\mathcal{Z}|$. We define $\delta_{x,y,t}$ for 
$t\in\{0\}\cup \mathcal{X}\times\mathcal Y$ as follows: $\delta_{x,y,0}=\tilde{\delta}_{x,y}$ and for any $(x',y')\in \mathcal{X}\times\mathcal Y$, we have
\begin{align}
	\delta_{x,y, t=(x',y')}=\mathds{1}[x'=x, y'=y](1-\tilde{\delta}_{x,y}).
\end{align}
We have $\sum_t \delta_{x,y,t}=1$ and $[p_{XY}(x,y)\delta_{x,y,t}]$  has rank 1 for all $t\in\{0\}\cup \mathcal{X}\times\mathcal Y$. Furthermore, we have
\begin{align}
	\min_{x,y: p_{XY}(x,y)>0}\delta_{x,y,t=(x',y')}=0 \qquad \forall x',y'.
\end{align}
Therefore, we compute
\begin{align}
	&\sum_{t}\min_{x,y:\: p_{X,Y}(x,y)>0}\delta_{x,y,t}=\min_{x,y:\: p_{X,Y}(x,y)>0}\delta_{x,y,0}=\min_{x,y:\: p_{X,Y}(x,y)>0}\tilde{\delta}_{x,y}\geq \min_{x,y}\tilde{\delta}_{x,y}.
\end{align}
\end{remark}
\vspace{0.4cm}

Computing the bounds on $\epsilon$ given in Theorem \ref{theorem-lowerbounds}  is cumbersome for general distributions. Thus, we next focus on the DSBE source and illustrate the suboptimality of the lower bound $\bar{L}(X;Y\|Z)$ obtained from (\ref{eq:lowerbound}) with a DSBE source example.



\subsection{DSBE Source Example}
Using (\ref{eqn:binbi}) and Theorem~\ref{theoremeq:eps12}, the SK capacity $S(X;Y\|Z)$ is zero if and only if 
\begin{align}
\epsilon\leq\frac{\min\{p,1-p\}}{\max\{p,1-p\}}.\label{eqfger2}
\end{align}
We now study the  lower bound $\bar{L}(X;Y\|Z)$ in (\ref{eq:lowerbound}). The main result of this subsection is to show that $S(X;Y\|Z)\neq \bar{L}(X;Y\|Z)$ for a DSBE$(p,\epsilon)$ source if
\begin{align}\frac{\min\{p,1-p\}}{\max\{p,1-p\}}<\epsilon\leq 4p(1-p).\end{align}
In fact, we show that $\bar{L}(X;Y\|Z)=0$ for erasure probabilities $\epsilon$ in the above interval since we know from \eqref{eqfger2} that $S(X;Y\|Z)>0$ in this interval. This result illustrates that the {lower bound $\bar{L}(X;Y\|Z)$} is loose. 



We remark that the lower bound is tight, i.e., $S(X;Y\|Z)=\bar{L}(X;Y\|Z)$, for all previously considered joint pmfs $p_{XYZ}$ for which the SK capacity $S(X;Y\|Z)$ is known. For instance, if $X\rightarrow Y\rightarrow Z$ forms a Markov chain, then assigning $U_1=X$ and $k=1$ in (\ref{eq:lowerbound}) recovers the SK capacity $S(X;Y\|Z)=I(X;Y|Z)$ achieved by one-way communication from $X$ to $Y$. Similarly, consider the reversely degraded example from \cite{AhlswedeCsiszar}. Let $X = (X_1, X_2)$, $Y = (Y_1, Y_2)$, and $Z = (Z_1, Z_2)$, where all $(X_i,Y_i,Z_i)$ tuples for $i=1,2$ are mutually independent. If $X_{1}\rightarrow Y_{1}\rightarrow Z_{1}$ and $Y_{2}\rightarrow X_{2}\rightarrow Z_{2}$ form Markov chains, assigning $U_1=X_1$ and $U_{2}=Y_{2}$ in (\ref{eq:lowerbound}) recovers the SK capacity $S(X;Y\|Z)=I(X;Y|Z)$. 




We give the condition for $\bar{L}(X;Y\|Z)=0$ for a DSBE source in the following theorem and prove it in Section~\ref{subsec:proofdsbe}.

\begin{theorem}\label{thm:A5}
 Let $(X,Y,Z)$ be a DSBE  source with parameters $(p,\epsilon)$. Then $\bar{L}(X;Y\|Z)=0$ if and only if the one-way SK rate from Alice to Bob (or Bob to Alice) vanishes, i.e., if and only if
\begin{align}\epsilon\leq 4p(1-p).\end{align}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.85\textwidth, height=1\textheight, keepaspectratio]{Newp06figure_withMaurer_v3}
	\caption{SK capacity bounds for a DSBE$(0.4,\epsilon)$ source. The curve labeled $S_{\text{ow}}(X;Y\|Z)$ is zero if and only if $\epsilon\leq 4p(1-p)$.}\label{fig:SK06}
\end{figure*} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%



In Fig.~\ref{fig:SK06}, we plot the known lower and upper bounds on $S(X;Y\|Z)$ to illustrate the gaps between them. Consider a DSBE source $(X,Y,Z)$ with parameters $(p=0.4,\epsilon)$. This source has
\begin{align}
  I(X;Y)=\frac{I(X;Y|Z)}{\epsilon}.
\end{align}
Therefore, we plot only $I(X;Y|Z)$ and do not consider the upper bound $I(X;Y)$. We also plot the improved upper bound (see (\ref{eq:B0upperbound}))
\begin{align}B_0^{\text{sub}}(X;Y\|Z)=I(X;Y|J)\end{align}
where 
\begin{align}
	J=\begin{cases}
0 & \text{ if } Z=(0,0),\\
1&\text{ if }Z=(1,1),\\
\mathtt{e}&\text{otherwise}
\end{cases}
\end{align}
which takes on non-zero values for $\epsilon>\epsilon_2=\frac{p}{1-p}$, as in Theorem~\ref{theoremeq:eps12}. In Fig.~\ref{fig:SK06}, $S_{\text{ow}}(X;Y\|Z)$ denotes the one-way communication capacity. This curve is calculated as follows: for every fixed value of $\epsilon \in [0,1]$, $S_{\text{ow}}(X;Y\|Z)$ is the maximum of
$I(U;Y|V)-I(U;Z|V)$
over all $p_{UV|X}$. The above expression is the upper concave envelope of the curve
\begin{align}
   p(X=0)\mapsto  \max_{p_{U|X}} I(U;Y)-I(U;Z)
\end{align}
at $p(X=0)=0.5$. Since the distribution $p_{YZ|X}$ is symmetric and $X$ is uniform in a DSBE source, using the symmetrization idea of \cite{nair2013upper} we obtain
\begin{align}
  S_{\text{ow}}(X;Y\|Z)=\max_{p_{UX}}I(U;Y)-I(U;Z). \label{nsym}
\end{align}
In fact, simulations indicate that the maximum in \eqref{nsym} is achieved when $X$ is uniform, indicating that auxiliary variable $V$ is not necessary to compute $S_{\text{ow}}(X;Y\|Z)$ for the DSBE source. The curve for $S_{\text{ow}}(X;Y\|Z)$ attains non-zero values for $\epsilon> 4p(1-p)$, which is the case also for $\bar{L}(X;Y\|Z)$ due to Theorem~\ref{thm:A5}. Similarly, we plot the rates achieved by the repetition codes of \cite{Maurer93} that are multi-letter and multi-round protocols. The $N$-repetition code achieves the SK rate
\begin{align}
	&R_N(X;Y\|Z) = \frac{p^N+{(1-p)}^N}{N}\max\Bigg\{0,\epsilon^N-h\Bigg(\frac{p^N}{p^N+{(1-p)}^N}\Bigg)\Bigg\}.
\end{align} 
In Fig.~\ref{fig:SK06}, we plot the rates for $N=2,3,4,5,6$. Fig.~\ref{fig:SK06} illustrates that there is a large gap between the lower bounds $R_N(X;Y\|Z)$ for $N=2,3,4,5,6$ and $B_0^{\text{sub}}(X;Y\|Z)$ for $\frac{p}{1-p}\leq \epsilon<1$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs}\label{sec:Proofs}

\subsection{Proof of Theorem \ref{genertlsd}}\label{eq:genertlsd}
Without loss of generality, suppose the symbol $0$ is in both $\mathcal{X}$ and $\mathcal{Y}$. Suppose that $q_{XYZ}(x,y,z)=a(x)\,b(y)\,p_{XYZ}(x,y,z)$. Let $\bar{a}=\max_{x}a(x)>0$ and $\bar{b}=\max_{y}b(y)>0$. 

We define $X'$ and $Y'$ on $\mathcal{X}$ and $\mathcal{Y}$, respectively, as follows:
\begin{align}
	p_{XYZX'Y'} = p_{XYZ} \, p_{X'|X} \, p_{Y'|Y} 
\end{align}
where $p_{X'|X}(x'|x)$ and $p_{Y'|Y}(y'|y)$ satisfy
\begin{align}
\begin{array}{l}
  p_{X'|X}(0|x) = a(x) / \bar{a} \\
  p_{Y'|Y}(0|y) = b(y) / \bar{b}.
\end{array}
\label{genertlsdv1}
\end{align}
 The values of $p_{X'|X}(x'|x)$ and $p_{Y'|Y}(y'|y)$ 
for $x',y'\neq 0$ are not important for the proof. Observe that \eqref{genertlsdv1} implies 
\begin{align}
	p_{X'Y'}(0,0) & =\sum_{x,y,z}\frac{a(x)}{\bar{a}}\frac{b(y)}{\bar{b}} p_{XYZ}(x,y,z) = \frac{1}{\bar{a}\bar{b}}\sum_{x,y,z}q_{XYZ}(x,y,z)=\frac{1}{\bar{a}\bar{b}}>0
\end{align}
and
\begin{align}
	p_{XYZ|X'Y'}(x,y,z|0,0)
	& =a(x)b(y)p_{XYZ}(x,y,z) =q_{XYZ}(x,y,z).
\end{align}

Suppose that Alice, Bob and Eve observe  i.i.d.\ repetitions of $X,Y,Z$ according to $p_{XYZ}$. We now show that they can simulate i.i.d.\ repetitions according to $q_{XYZ}$. Alice has access to $X^n$. She passes $X^n$ through $\prod_{i=1}^np_{X_i'|X_i}$ to produce a sequence $X^{'n}$. Alice then puts into the public channel the list of indices $i$ such that $X'_i=0$. Similarly, Bob passes $Y^n$ through $\prod_{i=1}^np_{Y_i'|Y_i}$ to produce $Y^{'n}$ and puts into the public channel the list of indices $i$ such that $Y'_i=0$. Alice and Bob then consider the observations $(X_i, Y_i)$ for indices $i$ where $(X'_i,Y'_i)=(0,0)$, and discard their observations for other indices. The induced pmf on $(X_i, Y_i, Z_i)$ given the event $(X'_i,Y'_i)=(0,0)$ is $q_{XYZ}$. Alice and Bob can now proceed with any key agreement protocol for $q_{XYZ}$ that achieves a positive key rate. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{gentheoremeq:eps12} }\label{proofgentheoremeq:eps12} 
We prove the equivalence by showing that (ii) implies (i), (i) implies (iii), (iii) implies (iv), and (iv) implies (ii). The fact that (iii) implies (iv) is trivial, so we prove the other three implications in the following subsubsections. 

\subsubsection{  (ii) implies (i) }
We claim that proving Corollary \ref{corollary-new-1}  establishes the claim that  (ii) implies (i). To see this, assume that \eqref{eq:nproductnonzeroSKrate} holds for some  integer $n$ and disjoint non-empty sets $\mathcal{A}_1, \mathcal{A}_2\subset \mathcal{X}^n$, and disjoint non-empty sets $\mathcal{B}_1, \mathcal{B}_2\subset \mathcal{Y}^n$. Let $X'\in\{1,2,3\}$ be a function of $X^n$ defined as follows: $X'=1$ if $X^n\in\mathcal{A}_1$, $X'=2$ if $X^n\in\mathcal{A}_2$ and $X'=3$  otherwise. We defined $Y'$ as a function of $Y^n$ in a similar manner using $\mathcal{B}_1$ and $\mathcal{B}_2$. Finally, let $Z'=Z^n$. We have, $S(X;Y\|Z)>0$ if $S(X';Y'\|Z')>0$ since Alice and Bob can produce $X'$ and $Y'$ from $X^n$ and $Y^n$ respectively. Equation \eqref{eqn:gentheoremeqA2} for $(X', Y', Z')$ with the choice $x'_1=1, x'_2=2, y'_1=1, y'_2=2$ is equivalent to \eqref{eq:nproductnonzeroSKrate} for the triple $(X^n, Y^n, Z^n)$ with the sets $\mathcal{A}_1, \mathcal{A}_2, \mathcal{B}_1$ and $\mathcal{B}_2$. 

It remains to prove Corollary \ref{corollary-new-1}. In other words, we wish to prove that $S(X;Y\|Z)>0$ if (\ref{eqn:gentheoremeqA2}) holds for distinct symbols $x_1, x_2 \in\mathcal{X}$ and distinct symbols $y_1, y_2 \in\mathcal{Y}$. Let $p_{ij} = p_{XY}(x_i, y_{j})$ for $i,j=1,2$. By \eqref{eqn:gentheoremeqA2}, we have
\begin{align}
  \frac12 \log\left(\frac{p_{11}p_{22}}{p_{12}p_{21}}\right)>0
\end{align}
or equivalently
\begin{align}
p_{11}p_{22} > p_{12}p_{21}.\label{eqdfrtgdf4545}
\end{align} 
Consider some even natural number $n$ and the sets
\begin{align}
	\mathcal{A}=\{\mathbf{x}_1, \mathbf{x}_2\},\quad \mathcal{B}=\{\mathbf{y}_1, \mathbf{y}_2\}
\end{align}
where
\begin{align}
	&\mathbf{x}_1=(x_1,x_1,\cdots, x_1, x_2,x_2,\cdots, x_2), \nonumber\\
	&\mathbf{x}_2= (x_2,x_2,\cdots, x_2,x_1,x_1,\cdots, x_1 ),\nonumber\\
	&\mathbf{y}_1=(y_1,y_1,\cdots, y_1,\, y_2,\, y_2,\cdots,\, y_2),\nonumber\\
	&\mathbf{y}_2= (\underbrace{y_2,y_2,\cdots, y_2}_{n/2},\,\underbrace{y_1,\, y_1,\cdots,\, y_1}_{n/2} ).
\end{align}

As in the proof of Theorem \ref{OrlitskyWigdersonThm} and Maurer's example in \cite[p.~740]{Maurer93}, suppose that Alice and Bob observe $N$ independent blocks, each having i.i.d.\ realizations of $(X,Y)$. For each block, Alice declares whether  $X^n\in\mathcal{A}$ and Bob declares whether $Y^n\in\mathcal{B}$. If $X^n\notin\mathcal{A}$ or $Y^n\notin\mathcal{B}$, they discard the block. Otherwise, they keep the block and use it for SK agreement. To prove that key generation is feasible, it suffices to show that 
\begin{align}
&I(X^n;Y^n|X^n\in\mathcal{A}, Y^n\in \mathcal{B})> I(X^n;Z^n|X^n\in\mathcal{A}, Y^n\in \mathcal{B}) \label{fkjbeg34345}
\end{align}
for large $n$. Equation (\ref{fkjbeg34345}) implies that  the legitimate users have a simple entropic advantage over the eavesdropper (see Definition \ref{def:advantage}) and hence a positive key rate can be achieved. We now show that (\ref{fkjbeg34345}) is satisfied. For any three random variables $X,Y,Z$ we have
\begin{align}
&I(X;Y)-I(X;Z)= H(X,Y|Z)-H(Y|X,Z)-H(X|Y)\geq H(X,Y|Z)-H(Y|X)-H(X|Y).
\end{align}
Thus, it suffices to show that for large $n$ we have
\begin{align}
&H(X^n,Y^n|Z^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})>H(Y^n|X^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})+H(X^n|Y^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B}).\label{eqjstrtg}
\end{align}



We compute
\begin{align}
	&\mathbb{P}[X^n=\mathbf{x}_1, Y^n=\mathbf{y}_1]=\mathbb{P}[X^n=\mathbf{x}_2, Y^n=\mathbf{y}_2]=p_{11}^{n/2}p_{22}^{n/2} \\
	&\mathbb{P}[X^n=\mathbf{x}_2, Y^n=\mathbf{y}_1]=\mathbb{P}[X^n=\mathbf{x}_1, Y^n=\mathbf{y}_2]=p_{12}^{n/2}p_{21}^{n/2}.
\end{align}
The conditional pmf of $(X^n, Y^n)$ given that $X^n\in\mathcal{A}$ and $Y^n\in \mathcal{B}$ is 
\begin{align}
\mathbb{P}[X^n=\mathbf{x}_1, Y^n=\mathbf{y}_1|X^n\in\mathcal{A}, Y^n\in \mathcal{B}]\nonumber&=\mathbb{P}[X^n=\mathbf{x}_2, Y^n=\mathbf{y}_2|X^n\in\mathcal{A}, Y^n\in \mathcal{B}]
\nonumber\\
&=\frac{p_{11}^{n/2}p_{22}^{n/2}}{2(p_{11}^{n/2}p_{22}^{n/2}+p_{12}^{n/2}p_{21}^{n/2})}\label{eq:symmetricforposSK1}
\end{align}
\begin{align}
\mathbb{P}[X^n=\mathbf{x}_2, Y^n=\mathbf{y}_1|X^n\in\mathcal{A}, Y^n\in \mathcal{B}]
&=\mathbb{P}[X^n=\mathbf{x}_1, Y^n=\mathbf{y}_2|X^n\in\mathcal{A}, Y^n\in \mathcal{B}]
\nonumber\\
&=\frac{p_{21}^{n/2}p_{12}^{n/2}}{2(p_{11}^{n/2}p_{22}^{n/2}+p_{12}^{n/2}p_{21}^{n/2})}.\label{eq:symmetricforposSK2}
\end{align}
If $X^n\in\mathcal{A}$ and $Y^n\in \mathcal{B}$, then we can model the conditional joint pmf of $(X^n,Y^n)$ as a DSBS with parameter
\begin{align}
  \tilde{p}_n=\frac{p_{11}^{n/2}p_{22}^{n/2}}{p_{11}^{n/2}p_{22}^{n/2}+p_{12}^{n/2}p_{21}^{n/2}}
\end{align}
due to symmetry in (\ref{eq:symmetricforposSK1}) and (\ref{eq:symmetricforposSK2}). Thus, we obtain
\begin{align}
	&H(X^n|Y^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})=H(Y^n|X^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})=h(\tilde{p}_n)
\end{align}
where $h(\cdot)$ is the binary entropy function. We have $h(p)\leq -2(1-p)\log(1-p)$ for any $p\in[0.5,1]$. Using \eqref{eqdfrtgdf4545}, we have $\tilde{p}_n\in [0.5,1]$ and 
\begin{align}
  \lim_{n\rightarrow\infty}(1-\tilde{p}_n)^{\frac{1}{n}}= \left(\frac{
  p_{12}p_{21}}{p_{11}p_{22}}\right)^{\frac 12}.
\end{align}
Hence, we have
\begin{align}
	&\lim_{n\rightarrow\infty} h(\tilde{p}_n)^{\frac{1}{n}}\leq \lim_{n\rightarrow\infty}\left(-2(1-\tilde{p}_n)\log(1-\tilde{p}_n)\right)^{\frac{1}{n}}\nonumber=\left(\frac{
	p_{12}p_{21}}{p_{11}p_{22}}\right)^{\frac 12}
\end{align}
and we obtain
\begin{align}
\lim_{n\rightarrow\infty}H(X^n|Y^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})^{\frac{1}{n}}&=\lim_{n\rightarrow\infty}H(Y^n|X^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})^{\frac{1}{n}}\leq 
\left(\frac{
 p_{12}p_{21}}{p_{11}p_{22}}\right)^{\frac 12}\label{eqdf23erNN}.
\end{align}
This equation implies that
\begin{align}
&\lim_{n\rightarrow\infty}\Big[H(X^n|Y^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})+H(Y^n|X^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})\Big]^{\frac{1}{n}}\leq 
\left(\frac{
 p_{12}p_{21}}{p_{11}p_{22}}\right)^{\frac 12}\label{eqdf23er}
\end{align}
which gives a bound on the asymptotics of the right hand side in \eqref{eqjstrtg}. We now consider the term on the left hand side in (\ref{eqjstrtg}). Our aim is to show that
\begin{align}
&\liminf_{n\rightarrow\infty}H(X^n, Y^n|Z^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})^{\frac{1}{n}}\geq \exp\Big(\!-\frac12D_{\frac 12}\big(p_{Z|XY}(\cdot |x_1,y_1)\big\| p_{Z|XY}(\cdot |x_{2},y_{2})\big)\Big).
\label{eqn34df45gd}
\end{align}
This equation together with \eqref{eqdf23er} show that \eqref{eqjstrtg} holds for large values of $n$ if
\begin{align}
	&\left(\frac{p_{12}p_{21}}{p_{11}p_{22}}\right)^{\frac 12} <\exp\Big(-\frac12D_{\frac 12}\big(p_{Z|XY}(\cdot |x_1,y_1)\big\| p_{Z|XY}(\cdot |x_{2},y_{2})\big)\Big)
\end{align}
which is equivalent to the condition
\begin{align}
	&D_{\frac 12}\big(p_{Z|XY}(\cdot |x_1,y_1)\big\| p_{Z|XY}(\cdot |x_{2},y_{2})\big)< \log\left(\frac{p_{11}p_{22}}{p_{12}p_{21}}\right).
\end{align}


It remains to prove \eqref{eqn34df45gd}.  From the perspective of Eve who observes $Z^n$, there are four possibilities of $(X^n,Y^n)=(\mathbf{x}_i, \mathbf{y}_j)$ for $i,j\in\{1,2\}$. Eve can view this as a hypothesis testing problem. For example, given $(X^n,Y^n)=(\mathbf{x}_1, \mathbf{y}_1)$, the conditional pmf of $(Z_i, Z_{\frac{n}{2}+i})$ satisfies
\begin{align}
  p_{Z|XY}(z_i|x_1,y_1)\cdot p_{Z|XY}(z_{\frac{n}{2}+i}|x_2,y_2)
\end{align}
for all $1\leq i\leq \frac{n}{2}-1$. Furthermore, $Z_i$ and $Z_{\frac{n}{2}+i}$ are conditionally independent given $(X^n,Y^n)=(\mathbf{x}_1, \mathbf{y}_1)$ for all $1\leq i\leq \frac{n}{2}-1$. Therefore, given the hypothesis $(X^n,Y^n)=(\mathbf{x}_1, \mathbf{y}_1)$, Eve observes $n/2$ i.i.d.\ repetitions
\begin{align}
  q^{(11)}_{Z_a Z_b}(z_a, z_b)= p_{Z|XY}(z_a|x_1,y_1)p_{Z|XY}(z_{b}|x_2,y_2).
\end{align}
More generally, given the hypothesis $(X^n,Y^n)=(\mathbf{x}_i, \mathbf{y}_j)$, Eve observes $n/2$ i.i.d.\ repetitions
\begin{align}
  q^{(ij)}_{Z_a Z_b}(z_a, z_b)=p_{Z|XY}(z_a|x_i,y_j)p_{Z|XY}(z_{b}|x_{3-i},y_{3-j})
\end{align}
for $i,j\in\{1,2\}$. 

We remark that the prior probability of the hypothesis $(X^n,Y^n)=(\mathbf{x}_i, \mathbf{y}_j)$ depends on $n$; see \eqref{eq:symmetricforposSK1} and \eqref{eq:symmetricforposSK2}. Therefore, we cannot directly apply results from the hypothesis testing literature, where fixed prior hypothesis probabilities are assumed. We use the following lemma.
\begin{lemma}\label{lem:UVlemma}\cite[Eq. (10)]{kanaya1995asymptotics}
For any $p_{UV}$ and any two distinct symbols $u_1, u_2$, we have
\begin{align}
   &\frac{H(U|V)}{ \log(2)}\geq\Bigg( p_{U}(u_1)\sum_{v\in\mathcal{D}}p_{V|U}(v|u_1) +p_U(u_2)\sum_{v\in\mathcal{D}^c}p_{V|U}(v|u_2) \Bigg) 
\end{align}
where $\mathcal{D}=\{v:~p_{UV}(u_1,v)<p_{UV}(u_2,v)\}$.
\end{lemma}

We apply Lemma~\ref{lem:UVlemma} with $U=(X^n, Y^n)$, $V=Z^n$, and
\begin{align}
&p_{UV}((x^n, y^n), z^n)=p_{X^n, Y^n, Z^n}(x^n, y^n, z^n|X^n\in\mathcal{A}, Y^n\in \mathcal{B}) \\
&u_1=(\mathbf{x}_1,\mathbf{y}_1) \\
&u_2=(\mathbf{x}_2,\mathbf{y}_2).
\end{align}
Using \eqref{eq:symmetricforposSK1}, we have
\begin{align}
&p_U(u_1)=p_U(u_2)=\frac{p_{11}^{n/2}p_{22}^{n/2}}{2(p_{11}^{n/2}p_{22}^{n/2}+p_{12}^{n/2}p_{21}^{n/2})}
\end{align}
and we obtain
\begin{align}
&H(X^n,Y^n|Z^n, X^n\in\mathcal{A}, Y^n\in \mathcal{B})^{\frac{1}{n}}\nonumber\\
&\geq  
\left(\log(2)\frac{p_{11}^{n/2}p_{22}^{n/2}}{p_{11}^{n/2}p_{22}^{n/2}+p_{12}^{n/2}p_{21}^{n/2}}\right)^{\frac{1}{n}}\nonumber\\
&\;
\times\!\Bigg\{\frac12 \!\sum_{z^n\in\mathcal{D}}p_{Z^n|X^n, Y^n}\!(z^n|\mathbf{x}_1, \mathbf{y}_1, X^n\!\in\!\mathcal{A}, Y^n\!\in\! \mathcal{B})  +\!\frac12\! \sum_{z^n\in~\mathcal{D}^c}\!p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_2, \mathbf{y}_2, X^n\!\in\!\mathcal{A}, Y^n\!\in\! \mathcal{B}) \Bigg\}^{\frac{1}{n}}.\label{eqn34fef45ewf}
\end{align} 
Using \eqref{eqdfrtgdf4545}, we have \begin{align}\lim_{n\rightarrow\infty}\left(\log(2)\frac{p_{11}^{n/2}p_{22}^{n/2}}{p_{11}^{n/2}p_{22}^{n/2}+p_{12}^{n/2}p_{21}^{n/2}}\right)^{\frac{1}{n}}=1.\label{fdrkjebef1}\end{align}

Next, observe that 
\begin{align}
	&\mathcal{D}=\Big\{z^n: p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_1, \mathbf{y}_1)<p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_2, \mathbf{y}_2)\Big\}
\end{align}
is the maximum a-posteriori probability (MAP) decision region for a new binary hypothesis testing problem with two equiprobable hypotheses, i.e., $(X^n, Y^n)=(\mathbf{x}_1, \mathbf{y}_1)$ and $(X^n, Y^n)=(\mathbf{x}_2, \mathbf{y}_2)$. In this problem, 
Eve observes $n/2$ i.i.d.\ repetitions of $q^{(11)}_{Z_a Z_b}$ under the first hypothesis, and $n/2$ i.i.d.\ repetitions of $q^{(22)}_{Z_a Z_b}$ under the second hypothesis. The expression
\begin{align}
&\frac12 \sum_{z^n\in\mathcal{D}}p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_1, \mathbf{y}_1, X^n\in\mathcal{A}, Y^n\in \mathcal{B}) +\frac12 \sum_{z^n\in\mathcal{D}^c}p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_2, \mathbf{y}_2, X^n\in\mathcal{A}, Y^n\in \mathcal{B}) 
\end{align}
is the error probability, which is asymptotically equal to $\exp(-\frac{n}{2}E)$, where
\begin{align}
E&=C\Big(q^{(11)}_{Z_a Z_b}\big\| q^{(22)}_{Z_a Z_b}\Big)=D_{\frac 12}\Big(p_{Z|XY}(\cdot |x_1,y_1)\big\| p_{Z|XY}(\cdot |x_2,y_2)\Big).
\label{eqnEval}
\end{align}
Equation \eqref{eqnEval} follows from the argument given in Example \ref{RnyiHalf}. 
Therefore, we obtain
\begin{align}
&\lim_{n\rightarrow\infty}\!\Bigg\{\frac12 \sum_{z^n\!\in\!\mathcal{D}}p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_1, \mathbf{y}_1, X^n\!\in\!\mathcal{A}, Y^n\in \mathcal{B}) +\frac12 \sum_{z^n\!\in\!\mathcal{D}^c}p_{Z^n|X^n, Y^n}(z^n|\mathbf{x}_2, \mathbf{y}_2, X^n\!\in\!\mathcal{A}, Y^n\!\in\! \mathcal{B}) \Bigg\}^{\frac{1}{n}}\nonumber
\\&=\exp\Big(-\frac12D_{\frac 12}\big(p_{Z|XY}(\cdot |x_1,y_1)\| p_{Z|XY}(\cdot |x_2,y_2)\big)\Big).\label{fdkjbretdv435}
\end{align}
Combining \eqref{eqn34fef45ewf}, \eqref{fdrkjebef1}, and \eqref{fdkjbretdv435}  establishes \eqref{eqn34df45gd}.


\subsubsection{   (i) implies (iii) }

Suppose that $S(X;Y\|Z)>0$. Because Alice and Bob can produce a key at a positive rate, they can also produce a key of length one bit. Maurer and Wolf \cite{maurer2000information} show the equivalence of the strong and weak notions of security for the source model problem. More specifically, from \cite{maurer2000information} and using $S(X;Y\|Z)>0$, we conclude that, given any $\delta>0$, there is an interactive communication protocol yielding  bits $K_A$ and $K_B$ for Alice and Bob such that $\mathbb{P}[K=K_A=K_B]\geq 1-\delta$ for some uniform bit $K\in\{1,2\}$. Furthermore, we have
\begin{align}
   I(K;Z^n\mathbf{F})\leq \delta.
\end{align}

The triangle inequality gives
\begin{align}\nonumber
\|p_{K_A K_B Z^n \mathbf{F}} - q_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  \|_{TV}&\leq
\|p_{K_A K_B Z^n \mathbf{F}} - p_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  \|_{TV}\!+\!
\|p_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  - q_{K_AK_B}\cdot p_{Z^n \mathbf{F}} \|_{TV}\nonumber
\\&=
\|p_{K_A K_B Z^n \mathbf{F}} - p_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  \|_{TV}+
\|p_{K_A K_B} - q_{K_AK_B}  \|_{TV}\label{j34bgkuret}
\end{align}
and by Fano's inequality we have
\begin{align}
	&I(KK_AK_B;Z^n\mathbf{F})\leq I(K;Z^n\mathbf{F})+H(K_AK_B|K)\leq \delta+h(\delta)+3\delta.
\end{align}
Therefore, we have the bound
\begin{align}
	&D(p_{K_AK_BZ^n\mathbf{F}}\|p_{K_AK_B}p_{Z^n\mathbf{F}})=I(K_AK_B;Z^n\mathbf{F})\leq 4\delta+h(\delta).
\end{align}
By Pinsker's inequality, we have
\begin{align}
	&\|p_{K_AK_BZ^n\mathbf{F}}-p_{K_AK_B}p_{Z^n\mathbf{F}}\|_{TV}\nonumber\leq \sqrt{2\delta+\frac 12 h(\delta)}\label{lkjbrtjbdf342}.
\end{align}
Next, from uniformity of $K$ and 
$\mathbb{P}[K=K_A=K_B]\geq 1-\delta$, we have
\begin{align}
	\left|p_{K_AK_B}(i,i)-\frac12\right|\leq \delta\; \text{  for } i=1,2
\end{align}
and
\begin{align}
	p_{K_AK_B}(i,j)\leq \delta\; \text{  for } i\neq j.
\end{align}
Therefore, we can write
\begin{align}
\|p_{K_A K_B} - q_{K_AK_B}  \|_{TV}\leq 2\delta.\label{lkjbrtjbdf34}
\end{align}
From \eqref{j34bgkuret}, \eqref{lkjbrtjbdf342} and \eqref{lkjbrtjbdf34}, we obtain
\begin{align}
&\|p_{K_A K_B Z^n \mathbf{F}} - q_{K_AK_B}\cdot p_{Z^n \mathbf{F}}  \|_{TV}\leq
\sqrt{2\delta+\frac 12 h(\delta)}+2\delta.
\end{align}
The right hand side of the above equation tends to zero as $\delta$ tends to zero.


\subsubsection{ (iv) implies (ii) }

It suffices to prove Lemma  \ref{lemma0-bnwe} below. This lemma  identifies sets $\mathcal{A}_i$ and $\mathcal{B}_i$ such that (\ref{eq:nproductnonzeroSKrate}) holds if 
\begin{align}
-\log\left(1-4\delta\right)<\log\frac{\frac 12 -2\delta}{2\delta}.\label{kjbsdf,mbr234343}
\end{align}
Equation \eqref{kjbsdf,mbr234343} holds for $\delta< \frac{3-\sqrt{5}}{8}$. This completes the proof.

%%%%%%%%%%%

\begin{lemma}\label{lemma0-bnwe} Consider a code with source sequences of length $n$,  interactive communication $\mathbf{F}=(F_1, F_2, ...)$ satisfying \eqref{eqn-=deertg}, and secret key bits $K_A\in\{1,2\}$ and $K_B\in\{1,2\}$, created by Alice and Bob after public discussion. We define
\begin{align}
	\delta=\|p_{K_A K_B Z^n \mathbf{F}} - q_{K_AK_B}\cdot p_{Z^n, \mathbf{F}}  \|_{TV}.
\end{align}
One can find disjoint non-empty subsets $\mathcal{A}_1, \mathcal{A}_2\subset\mathcal{X}^n$ and disjoint non-empty subsets $\mathcal{B}_1, \mathcal{B}_2\subset \mathcal{Y}^n$ such that 
\begin{align}
	&\frac12D_{\frac 12}\Big(p_{Z^n}(\cdot |X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1)\big\|  p_{Z^n}(\cdot |X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2)\Big)\leq -\log\left(1-4\delta\right)\label{eq:IamOutofNamesforEquations----}
\end{align}	
and
\begin{align}
&\frac12
	\log\left(\frac{
	\mathbb{P}[X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1]\mathbb{P}[X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2]
}{
\mathbb{P}[X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_2]\mathbb{P}[X^n\in\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_1]
}\right)\geq 
	\log\frac{\frac 12 -2\delta}{2\delta}.\label{eqn:lkjbkzmsdjbf----}
\end{align}
\end{lemma}
\begin{proof}
There is a realization $\mathbf{F}=\mathbf{f}$ such that $\mathbb{P}[\mathbf{F}=\mathbf{f}]>0$ and
\begin{align}
	\Big\|p_{K_A K_B Z^n|\mathbf{F}}(\cdot |\mathbf{f})
	  -  q_{K_AK_B} \cdot p_{Z^n|\mathbf{F}}(\cdot |\mathbf{f}) \Big\|_{TV}\leq \delta.\label{eqn:slfjbkjbt}
\end{align}
From the data processing property of the total variation distance, we have
\begin{align}
	\big\|p_{K_A K_B|\mathbf{F}}(\cdot |\mathbf{f}) \!-\! q_{K_AK_B} \big\|_{TV}
	\le \delta\label{eqnrefklh23}
\end{align}
and therefore
\begin{align}\mathbb{P}[K_A=1, K_B=1|\mathbf{F}=\mathbf{f}]&\geq \frac 12 -2\delta\label{eqnrefklh231}\\
\mathbb{P}[K_A=2, K_B=2|\mathbf{F}=\mathbf{f}]&\geq \frac 12 -2\delta\label{eqnrefklh232}\\
\mathbb{P}[K_A=1, K_B=2|\mathbf{F}=\mathbf{f}]&\leq 2\delta \label{eqnrefklh233}\\
\mathbb{P}[K_A=2, K_B=1|\mathbf{F}=\mathbf{f}]&\leq 2\delta. 
\label{eqnrefklh234}
\end{align}

As in the proof of Theorem \ref{OrlitskyWigdersonThm},  the conditional pmfs of $(X^n, Y^n, Z^n)$ given $\mathbf{F}=\mathbf{f}$ have the form $p_r(x^n, y^n, z^n)$ given in \eqref{eqn:AAAd13} for some sets $\mathcal{A}$ and $\mathcal{B}$ that depend on $\mathbf{f}$. Furthermore, given $\mathbf{F}=\mathbf{f}$, the key $K_A$ is a function of $X^n$. We partition $\mathcal{A}$ into  $\mathcal{A}_1\cup \mathcal{A}_2$ as follows:
 $\mathcal{A}_i=\{x^n: K_A(x^n, \mathbf{f})=i\}$ for $i=1,2$. We define $\mathcal{B}_i$ similarly using $K_B$. 

Observe for $i,j\in\{1,2\}$ that
\begin{align}
	&\mathbb{P}[K_A=i, K_B=j|\mathbf{F}=\mathbf{f}]=\frac{\mathbb{P}[X^n\in\mathcal{A}_i,Y^n\in\mathcal{B}_j]}{ \mathbb{P}[X^n\in\mathcal{A},Y^n\in\mathcal{B}]}.\label{lasnkljeb}
\end{align}
From \eqref{eqnrefklh231}-\eqref{lasnkljeb}, we obtain 
\begin{align}
&\frac12
	\log\left(\frac{
	\mathbb{P}[X^n\in\mathcal{A}_1,Y^n\in\mathcal{B}_1]\mathbb{P}[X^n\in\mathcal{A}_2,Y^n\in\mathcal{B}_2]
}{
\mathbb{P}[X^n\in\mathcal{A}_1,Y^n\in\mathcal{B}_2]\mathbb{P}[X^n\in\mathcal{A}_2,Y^n\in\mathcal{B}_1]
}\right)\nonumber\\
&\quad=\frac12
	\log\Bigg(
	\frac{
	\mathbb{P}[K_A\!=\!1, K_B\!=\!1|\mathbf{F}\!=\!\mathbf{f}]
}{
\mathbb{P}[K_A\!=\!1, K_B\!=\!2|\mathbf{F}\!=\!\mathbf{f}]
}\times
\frac{
	\mathbb{P}[K_A\!=\!2, K_B\!=\!2|\mathbf{F}\!=\!\mathbf{f}]
}{
\mathbb{P}[K_A\!=\!2, K_B\!=\!1|\mathbf{F}\!=\!\mathbf{f}]
}\Bigg)\nonumber\\
&\quad\geq \frac12
	\log\frac{(\frac 12 -2\delta)^2}{(2\delta)^2}= 
	\log\frac{\frac 12 -2\delta}{2\delta}.\label{eqn:lkjbkzmsdjbf}
\end{align}

Next, from \eqref{eqn:slfjbkjbt} we have \eqref{eqn:slfjbkjbt23} given at the top of this page,
\begin{figure*}
\begin{align}
	&\sum_{z^n} \Bigg|\mathbb{P}[K_A\!=\!1, K_B\!=\!1|\mathbf{F}\!=\!\mathbf{f}] \times p_{Z^n|K_A, K_B,\mathbf{F}}(z^n|k_A\!=\!1, k_B\!=\!1,\mathbf{f})-
\frac 12 p_{Z^n|\mathbf{F}}(z^n|\mathbf{f}) 
\Bigg|\nonumber\\
&+
	\sum_{z^n}\! \Bigg|\mathbb{P}[K_A\!=\!2, K_B\!=\!2|\mathbf{F}\!=\!\mathbf{f}] \times p_{Z^n|K_A, K_B,\mathbf{F}}(z^n|k_A\!=\!2, k_B\!=\!2,\mathbf{f}) - \frac 12 p_{Z^n|\mathbf{F}}(z^n|\mathbf{f}) \Bigg|
\nonumber\\[0.5ex]
&\quad\leq 2\delta.\label{eqn:slfjbkjbt23}
\\
\hline \nonumber
\\ \nonumber
	&\sum_{z^n} \Bigg|\mathbb{P}[K_A\!=\!1, K_B\!=\!1|\mathbf{F}\!=\!\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A\!=\!1, k_B\!=\!1,\mathbf{f}) \nonumber
\\&\qquad - \mathbb{P}[K_A\!=\!2, K_B\!=\!2|\mathbf{F}\!=\!\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A\!=\!2, k_B\!=\!2,\mathbf{f})  \Bigg|\nonumber\\[0.5ex]
&\quad
\leq 2\delta.\label{eqn:slfjbkjbt2323}
\\
\hline\nonumber
\end{align}
\end{figure*}
and the triangle inequality gives \eqref{eqn:slfjbkjbt2323} also given at the top of this page. The triangle inequality further implies equation \eqref{eqjklbgkb} given at the top of the next page, where we use \eqref{eqnrefklh23} and \eqref{eqn:slfjbkjbt2323} in the last step of the derivation. 
\begin{figure*}
\begin{align}\nonumber
&\sum_{z^n} \Bigg|\frac 12 p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=1, k_B=1,\mathbf{f})- \frac 12 p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=2, k_B=2,\mathbf{f})  \Bigg|\nonumber 
\\&
\leq\sum_{z^n} \Bigg|\mathbb{P}[K_A=1, K_B=1|\mathbf{F}=\mathbf{f}]
\times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=1, k_B=1,\mathbf{f}) \nonumber
\\&\qquad\quad - \mathbb{P}[K_A=2, K_B=2|\mathbf{F}=\mathbf{f}]
\times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=2, k_B=2,\mathbf{f})  \Bigg|\nonumber
\\&\quad+
\sum_{z^n} \Bigg|\mathbb{P}[K_A=1, K_B=1|\mathbf{F}=\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=1, k_B=1,\mathbf{f})  -\frac 12 p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=1, k_B=1,\mathbf{f})\Bigg|\nonumber
\\&\quad+
\sum_{z^n} \Bigg|\mathbb{P}[K_A=2, K_B=2|\mathbf{F}=\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=2, k_B=2,\mathbf{f}) -\frac 12 p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=2, k_B=2,\mathbf{f})\Bigg|\nonumber\\
&=\sum_{z^n} \Bigg|\mathbb{P}[K_A=1, K_B=1|\mathbf{F}=\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=1, k_B=1,\mathbf{f}) \nonumber
\\&\qquad\quad - \mathbb{P}[K_A=2, K_B=2|\mathbf{F}=\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=2, k_B=2,\mathbf{f})  \Bigg|\nonumber
\\&\quad+
 \left|\mathbb{P}[K_A=1, K_B=1|\mathbf{F}=\mathbf{f}] -\frac 12 \right|+
\left|\mathbb{P}[K_A=2, K_B=2|\mathbf{F}=\mathbf{f}] -\frac 12\right|\nonumber\\
&\leq \sum_{z^n} \Bigg|\mathbb{P}[K_A=1, K_B=1|\mathbf{F}=\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=1, k_B=1,\mathbf{f}) \nonumber
\\[0.5ex]&\qquad\quad - \mathbb{P}[K_A=2, K_B=2|\mathbf{F}=\mathbf{f}] \times p_{Z^n|K_A, K_B\mathbf{F}}(z^n|k_A=2, k_B=2,\mathbf{f})  \Bigg|\nonumber
\\&\quad+
2\|p_{K_A K_B|\mathbf{F}}(k_A, k_B|\mathbf{f}) - q_{K_AK_B}(k_A, k_B) \|_{TV}\nonumber
\\[0.7ex]&\leq 2\delta+2\delta\label{eqjklbgkb}
\\
\hline \nonumber
\end{align}
\end{figure*}
Observe for $i\in\{1,2\}$ that
\begin{align}
	&p_{Z^n|K_AK_B\mathbf{F}}(z^n|k_A=i,k_B=i,\mathbf{f}) =p_{Z^n}(z^n|X^n\in\mathcal{A}_{i},Y^n\in\mathcal{B}_{i}).
\end{align}
Therefore, \eqref{eqjklbgkb} shows that 
\begin{align}
	&\Big\|p_{Z^n}(\cdot |X^n\in\mathcal{A}_{1},Y^n\!\in\!\mathcal{B}_{1})-p_{Z^n}(\cdot |X^n\!\in\!\mathcal{A}_{2},Y^n\!\in\!\mathcal{B}_{2})\Big\|_{TV}\leq 4\delta
\end{align}
and by \eqref{TV-Chernoff} we have
\begin{align}
	&\frac 12 D_{\frac 12}\Big(p_{Z^n}(\cdot |X^n\!\in\!\mathcal{A}_1,Y^n\!\in\!\mathcal{B}_1)\Big\|  p_{Z^n}(\cdot |X^n\!\in\!\mathcal{A}_2,Y^n\!\in\!\mathcal{B}_2)\Big)\leq -\log\left(1-4\delta\right).\label{eq:IamOutofNamesforEquations}
\end{align}	
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{mmthm1s}}\label{eq:maintheoremsproof}
\label{proof-lower-bound1}

In light of Examples \ref{example2n} and \ref{example3n}, we only need to to prove the first part of the theorem, namely $S(X;Y\|Z)=0$ if $\epsilon\leq \epsilon_1$, and 
$S(X;Y\|Z)>0$ if $\epsilon> \epsilon_2$. The fact that $S(X;Y\|Z)=0$ if $\epsilon\leq \epsilon_1$ follows from Theorem \ref{theoremWolf} if we can show that $\epsilon_1=1-d_{\emph{ind}}=\frac{1}{F(p_{XY})}$.
Observe that  $F(p_{XY})$ is (see Definition~\ref{def:Fpxy}) the minimum over all product measures $q_{XY}=q_X\, q_Y$ of 
\begin{align}
	\max_{x,y}\left(\frac{p_{XY}(x,y)}{q_{XY}(x,y)}\right)\cdot\max_{x,y}\left(\frac{q_{XY}(x,y)}{p_{XY}(x,y)}\right).\label{eq:Icouldnotfindagoodname}
\end{align}
This is because (\ref{eq:Icouldnotfindagoodname}) would not change if we multiply $q_{XY}(x,y)$ by a positive constant. Moreover, by the same argument, we can restrict attention to product measures $q_{XY}=q_X\, q_Y$ satisfying 
\begin{align}
	\max_{x,y}\left(\frac{q_{XY}(x,y)}{p_{XY}(x,y)}\right)=1.
\end{align}
The equality $\epsilon_1=\frac{1}{F(p_{XY})}$ follows because of the following alternative characterization of $\epsilon_1$.
\begin{theorem}\label{lemma-generaldis1}
Define a matrix $[p_{XY}(x,y)\delta_{x,y}]$ of dimensions $|\mathcal{X}|\times|\mathcal{Y}|$ whose rows  and columns are indexed by the realizations of $X$ and $Y$, respectively, and whose $(x,y)$ entry is $p_{XY}(x,y)\delta_{x,y}$ for all $(x,y)\in\mathcal{X}\times\mathcal{Y}$. We have 
\begin{align}
	\epsilon_1=\max \min_{x,y}\delta_{x,y} \label{theoremeq:maxmin}
\end{align}
where the maximum is over all $\delta_{x,y}\in [0,1]$ such that the matrix $[p_{XY}(x,y)\delta_{x,y}]$ has rank one. 
\end{theorem}
The proof of Theorem \ref{lemma-generaldis1} is given in Section~\ref{eq:lemma-generaldis1proof}.

It remains to show that $S(X;Y\|Z)>0$ if $\epsilon > \epsilon_2$.
Suppose that $\epsilon_2$ is obtained with the minimizer path $(x_1, y_1, x_2, y_2)$ so that
\begin{align}\epsilon_2=\left(\frac{p_{XY}(x_1,y_1)p_{XY}(x_2,y_2)}{p_{XY}(x_1, y_2)p_{XY}(x_2, y_1)}\right)^{1/2}.\label{eqn:92384y}\end{align}
We prove that $S(X;Y\|Z)>0$ for $\epsilon > \epsilon_2$. Since the value of the path $(x_1, y_1, x_2, y_2)$ is less than or equal to the value of the path $(x_1, y_2, x_2, y_1)$, we have
\begin{align}
  \epsilon_2=\left(\frac{
  \min\{p_{11}p_{22}, p_{12}p_{21}\}}{\max\{p_{11}p_{22}, p_{12}p_{21}\}}\right)^{1/2}
\end{align}
where $p_{ij} = p_{XY}(x_i, y_{j})$ for $i,j=1,2$. From Theorem \ref{gentheoremeq:eps12}, we have $S(X;Y\|Z)>0$ if 
\begin{align}
  &\frac 12 D_{\frac 12}\Big(p_{Z|XY}(\cdot |x_1,y_1)\big\| p_{Z|XY}(\cdot |x_{2},y_{2})\Big) <-\log\epsilon_2.
\end{align}
But observe that
\begin{align} 
&\frac12D_{\frac 12}\Big(p_{Z|XY}(\cdot |x_1,y_1)\big\| p_{Z|XY}(\cdot |x_2,y_2)\Big)=-\log\Big(\sum_{z} p_{Z|XY}(z|x_1,y_1)^{\frac 12}\times
p_{Z|XY}(z|x_2,y_2)^{\frac 12}\Big)
\nonumber\\&\quad\leq-\log\Big(p_{Z|XY}(\mathtt{e}|x_1,y_1)^{\frac 12}\times
p_{Z|XY}(\mathtt{e}|x_2,y_2)^{\frac 12}\Big)=-\log(\epsilon)\label{eqnN149}
\end{align}
which proves that $S(X;Y\|Z)>0$ if $\epsilon > \epsilon_2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Theorem~\ref{lemma-generaldis1}} \label{eq:lemma-generaldis1proof}
Let 
\begin{align}
	\tilde{\epsilon}_1=\max \min_{x,y}\delta_{x,y}\label{eq:epsilon3}
\end{align}
where the maximization is over all $\delta_{x,y}\in [0,1]$ such that the matrix $[p_{XY}(x,y)\delta_{x,y}]$ has rank one. We prove that $\tilde{\epsilon}_1=\epsilon_1$. 

Observe that if $p_{XY}(x^*,y^*)=0$ for some $x^*,y^*\in\mathcal{X}\times\mathcal{Y}$, then $\epsilon_1=0$, which follows from Definition~\ref{def:path}. We now prove that $\tilde{\epsilon}_1$ is also zero. Consider some arbitrary $\delta_{x,y}$ such that $[p_{XY}(x,y)\delta_{x,y}]$ has rank one. Since $p_{XY}(x^*,y^*)\delta_{x^*,y^*}=0$, we must have either $p_{XY}(x^*,y)\delta_{x^*,y}=0$ for all $y\in\mathcal{Y}$ or $p_{XY}(x,y^*)\delta_{x,y^*}=0$ for all $x\in\mathcal{X}$. Assume that $p_{XY}(x^*,y)\delta_{x^*,y}=0$ for all $y\in\mathcal{Y}$. Since there exists a $y$ such that $p_{XY}(x^*,y)>0$, we obtain  $\delta_{x^*,y}=0$ for some $y\in\mathcal{Y}$. Hence, $\min_{x,y}\delta_{x,y}=0$ and $\tilde{\epsilon}_1=0$. 

Based on the discussions above, we may assume that $p_{XY}(x,y)>0$ for all $(x,y)\in\mathcal{X}\times\mathcal{Y}$. In this case, it follows that $\epsilon_1>0$. We also have $\tilde{\epsilon}_1>0$ since one valid choice for $\delta_{x,y}$ is $\delta_{x,y}=\frac{k}{p_{XY}(x,y)}$, where $k= \min_{x,y}p_{XY}(x,y)$. Since $\tilde{\epsilon}_1>0$, we take the maximum in (\ref{eq:epsilon3}) only over positive $\delta_{x,y}$.

Consider some $\delta_{x,y}\in (0,1]$ such that $[p_{XY}(x,y)\delta_{x,y}]$ has rank one. In other words, $p_{XY}(x,y)\delta_{x,y}=e^{m(x)}e^{n(y)}$ has a product form for some $m(x)$ and $n(y)$. Taking logarithms, we obtain 
\begin{align}\log p_{XY}(x,y)+\log\delta_{x,y}=m(x)+n(y).\end{align} We can express the problem as finding the maximum value of $\tilde{\epsilon}_1\in(0,1]$ such that for some $m(x)$ and $n(y)$, we have for $\forall x,y$
\begin{align}
m(x)\!+\!n(y)\!\leq\! \log p_{XY}(x,y)\!\leq\! m(x)\!+\!n(y)\!-\!\log\tilde{\epsilon}_1.
\end{align}
We can view this as a linear programming problem to minimize $A\triangleq-\log\tilde{\epsilon}_1$ subject to
\begin{align}
	&m(x)+n(y)\leq \log p_{XY}(x,y)\label{eq:topequation}\\
	&\log p_{XY}(x,y)\leq m(x)+n(y)+A.\label{eq:bottomequation}
\end{align}
We consider the dual of this linear programming problem. Multiplying (\ref{eq:topequation}) by some $\gamma(x,y)\geq 0$ and (\ref{eq:bottomequation}) by some $\mu(x,y)\geq 0$, we obtain
\begin{align}
	&\sum_{x,y}\gamma(x,y)\left( m(x)+n(y)\right) + \sum_{x,y}\mu(x,y)\log p_{XY}(x,y)\nonumber\\
	&\quad\leq \sum_{x,y}\gamma(x,y)\log p_{XY}(x,y)+\sum_{x,y}\mu(x,y)\Big( m(x)+n(y)+A\Big).
\end{align}
Since we are interested in the best lower bound on $A$, we should choose $\gamma$ and $\mu$ such that the  coefficient of $A$ is equal to one and the coefficients of free variables $m(x)$ and $n(y)$ vanish. The coefficient of $A$ is one only if  $\sum_{x,y}\mu(x,y)=1$. Furthermore,  to cancel out the auxiliary variables $m(x)$ and $n(y)$ from both sides, we must have $\sum_x \gamma(x,y)=\sum_x \mu(x,y)$ for all $y\in\mathcal{Y},$ and $\sum_y \gamma(x,y)=\sum_y \mu(x,y)$ for all $x\in\mathcal{X}$. This implies that $\gamma(x,y)$ and $\mu(x,y)$ are \emph{probability distributions} with the same marginals. We denote their marginal probabilities by $\mu(x)=\gamma(x) $ and $\mu(y)=\gamma(y)$. 

The dual of the linear programming problem is 
\begin{align}
A\!=\!\max \!\sum_{x,y}\!(\mu(x,y)\!-\!\gamma(x,y))\log p_{XY}(x,y)\label{eqn:34re}
\end{align}
where the maximization is over all pmfs $\mu, \gamma$ with the same marginals. Since  $A=-\log\tilde{\epsilon}_1$, we have
\begin{align}
\tilde{\epsilon}_1=\min \prod_{x,y}p_{XY}(x,y)^{-\mu(x,y)+\gamma(x,y)}\label{eqn:34wewere}
\end{align}
where the minimization is over all pmfs $\mu, \gamma$ with the same marginals. 

The requirement that the pmfs $\mu, \gamma$ should have the same marginals imposes a number of linear constraints on $\mu$ and $\gamma$. This indicates that the set of all pmfs $\mu, \gamma$ with the same marginals is a polytope. We maximize a linear equation over this polytope in \eqref{eqn:34re}.

We first list three claims. These are proved below and used to show that $\tilde{\epsilon}_1=\epsilon_1$.


\textbf{Claim 1:} \emph{If $(\mu, \gamma)$ is a minimizer for (\ref{eqn:34wewere}) and 
\begin{align}\mu(x,y)&=\lambda \mu_1(x,y)+(1-\lambda) \mu_2(x,y)
\\\gamma(x,y)&=\lambda \gamma_1(x,y)+(1-\lambda) \gamma_2(x,y)\end{align}
where $\mu_i, \gamma_i$ are pmfs with the same marginals for $i=1,2$ and $0<\lambda<1$, then $\mu_i, \gamma_i$ are also minimizers for $i=1,2$.}

Given any pmf $\mu$, we define $\textit{Support}(\mu)$ as the set of realizations with positive occurrence probability.

\textbf{Claim 2:} \emph{Given pmfs $(\mu, \gamma)$ with the same marginals, if one can find pmfs $\mu_1(x,y), \gamma_1(x,y)$ with the same marginals such that $\text{Support}(\mu_1)\subseteq \text{Support}(\mu)$ and
$\text{Support}(\gamma_1)\subseteq \text{Support}(\gamma)$, then there is a $\lambda$ with $0<\lambda<1$ and a $(\mu_2, \gamma_2)$ with the same marginals such that \begin{align}\mu(x,y)&=\lambda \mu_1(x,y)+(1-\lambda) \mu_2(x,y)\\ \gamma(x,y)&=\lambda \gamma_1(x,y)+(1-\lambda) \gamma_2(x,y).\end{align}}
\textbf{Claim 3:} \emph{Given pmfs $\mu, \gamma$ with the same marginals, one can find a path  $(x_1, y_1, x_2, y_2, \cdots, x_k, y_k)$ (as in Definition \ref{def:path}) such that $\gamma(x_i, y_i)>0$ for $1\leq i\leq k$, and $\mu(x_1, y_k)>0$ and $\mu(x_i,y_{i-1})>0$ for $2\leq i\leq k$. }

\vspace{0.5cm}

Consider a minimizer $(\mu, \gamma)$ and the path given in Claim 3 for $(\mu, \gamma)$. Define $\mu_1$ and $\gamma_1$ as follows: \begin{align}
\gamma_1(x,y)=\begin{cases}\frac{1}{k}&  \text{if } (x,y)=(x_i, y_i) \text{ for } 1\leq i\leq k,\\
0&  \text{otherwise},
\end{cases}\label{eq:claim31}
\end{align}
\begin{align}
\mu_1(x,y)=\begin{cases}\frac{1}{k}&  \text{if } (x,y)\!=\!(x_1, y_k),\\
\frac{1}{k}&  \text{if } (x,y)\!=\!(x_i, y_{i-1})  \text{ for } 2\!\leq\! i\!\leq\! k,\\
0&  \text{otherwise.} 
\end{cases}\label{eq:claim32}
\end{align}

Observe that $\gamma_1$ and $\mu_1$ have the same marginals and satisfy the conditions $\text{Support}(\mu_1)\subseteq \text{Support}(\mu)$ and $\text{Support}(\gamma_1)\subseteq \text{Support}(\gamma)$. Using Claim 1 and 2, we conclude that $(\gamma_1, \mu_1)$ must also be a minimizer. Using \eqref{eqn:34wewere}, we therefore obtain 
\begin{align}
\tilde{\epsilon}_1&=\prod_{x,y}p_{XY}(x,y)^{-\mu_1(x,y)+\gamma_1(x,y)}
\end{align}
which evaluates to the value assigned to the path $(x_1, y_1, x_2, y_2, \cdots, x_k, y_k)$ according to \eqref{eq:assignedvalue}. Since $\epsilon_1$ is the minimum assigned value of all possible paths, we obtain $\epsilon_1\leq \tilde{\epsilon}_1$. 

To show that $\tilde{\epsilon}_1\leq \epsilon_1$, suppose that $\epsilon_1$ is obtained for the minimizer path $(x'_1, y'_1, x'_2, y'_2, \cdots, x'_k, y'_k)$. We  construct $\mu'_1$ and $\gamma'_1$ for this path, similar to \eqref{eq:claim31} and \eqref{eq:claim32}, and the value of this path is
\begin{align}
\epsilon_1=\prod_{x,y}p_{XY}(x,y)^{-\mu'_1(x,y)+\gamma'_1(x,y)} .
\end{align}
Using \eqref{eqn:34wewere}, we obtain $\tilde{\epsilon}_1\leq \epsilon_1$. This proves $\tilde{\epsilon}_1=\epsilon_1$. 

It remains to prove the claims given above.

\emph{Proof of Claim 1:} The value of $\sum_{x,y}(\mu_i(x,y)-\gamma_i(x,y))\log p_{XY}(x,y)$ must be less than or equal to $\sum_{x,y}(\mu(x,y)-\gamma(x,y))\log p_{XY}(x,y)$ for $i=1,2$ since $(\mu,\gamma)$ is a maximizer for (\ref{eqn:34re}). On the other hand, by the linearity of \eqref{eqn:34re}, we have
\begin{align}
&\sum_{x,y}(\mu(x,y)-\gamma(x,y))\log p_{XY}(x,y)\nonumber\\
&\quad=\lambda\sum_{x,y}(\mu_1(x,y)-\gamma_1(x,y))\log p_{XY}(x,y)\!+\!(1\!-\!\lambda)\sum_{x,y}(\mu_2(x,y)\!-\!\gamma_2(x,y))\log p_{XY}(x,y).
\end{align}
We thus have for $i=1,2$ that
\begin{align}
	&\sum_{x,y}(\mu_i(x,y)-\gamma_i(x,y))\log p_{XY}(x,y)=\sum_{x,y}(\mu(x,y)-\gamma(x,y))\log p_{XY}(x,y).
\end{align}



\emph{Proof of Claim 2:} Assign 
\begin{align}
\lambda\!=\!\min\left\{1, \min_{\substack{x,y:\\ \gamma_1(x,y)>0}}\frac{\gamma(x,y)}{\gamma_1(x,y)}, \min_{\substack{x,y:\\ \mu_1(x,y)>0}}
\frac{\mu(x,y)}{\mu_1(x,y)}\right\}.
\end{align}
Observe that $\lambda>0$ since $\gamma_1(x,y)>0$ implies that $\gamma(x,y)>0$, and $\mu_1(x,y)>0$ implies that $\mu(x,y)>0$. Assigning the values
\begin{align}
\gamma_2(x,y)&=\frac{\gamma(x,y) -\lambda \gamma_1(x,y)}{1-\lambda}\\
\mu_2(x,y)&=\frac{\mu(x,y) -\lambda \mu_1(x,y)}{1-\lambda}
\end{align}  proves the claim.


\emph{Proof of Claim 3:} Consider some $x_1\in\mathcal{X}$ such that $\gamma(x_1)>0$. Then there is some $y_1\in\mathcal{Y}$ such that $\gamma(x_1, y_1)>0$. Hence $\gamma(y_1)>0$, which implies that $\mu(y_1)>0$. This also implies that there is some $x_2\in\mathcal{X}$ such that $\mu(x_2, y_1)>0$. Hence, $\mu(x_2)>0$ implies $\gamma(x_2)>0$ and that there is some $y_2$ such that $\gamma(x_2, y_2)>0$. We continue this process and obtain a sequence $(x_1, y_1, x_2, y_2, ....)$. While applying the process, we must observe at some point for the first time a previously occurred symbol. Suppose that this happens at time $m$. If $x_m=x_i$ for some $i<m$, then we consider the sequence $(x_i, y_i, x_{i+1}, y_{i+1}, ..., x_{m-1}, y_{m-1})$ as our path. This is a desirable path since $\mu(x_i, y_{m-1})=\mu(x_m, y_{m-1})>0$. Similarly, if $y_m=y_i$ for some $i<m$, we consider the sequence $(x_{i+1}, y_{i+1}, ..., x_{m}, y_{m})$ as our path. This is a desirable path since $\mu(x_{i+1}, y_{m})=\mu(x_{i+1}, y_{i})>0$. This proves the existence of such a path.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{theorem-lowerbounds}}\label{subsec:proofdsbe22}
\textbf{Part 1:} We first show that the  one-way SK rate from Alice to Bob  is positive if and only if
\begin{align}
	\epsilon> 1-\eta(p_{Y|X})
\end{align}	
where $\eta(\cdot)$ is as defined in (\ref{eqnAG687rr657}). 

The one-way SK rate is positive if and only if one can find auxiliary random variables $U$ and $V$ that satisfy the Markov chain $UV\rightarrow X\rightarrow YZ$ such that 
\begin{align}
	&I(U;Y|V)>I(U;Z|V)=\!(1\!-\!\epsilon)I(U;XY|V)\!=(1-\epsilon)I(U;X|V).
\end{align}
Thus, the one-way SK rate is positive if and only if
\begin{align}
	&\epsilon>1-\sup_{UV\rightarrow X\rightarrow Y}\frac{I(U;Y|V)}{I(U;X|V)}\overset{(a)}{=} 1-\eta(p_{Y|X}) 
\end{align}
where $(a)$ follows by Lemma \ref{lemma2fdret}, proved in Appendix \ref{appendix-lemmas}.

\noindent
\textbf{Part 2:} We next prove that $\bar{L}(X;Y\|Z)=0$  if and only if
\begin{align}
	\epsilon\leq 1-\max_{\substack{q_{XY}:\: q_{XY}\preceq p_{XY}}}\rho^2_m(q_{XY}).
\end{align}

From the definition of $\bar{L}(X;Y\|Z)=0$, one can deduce that $\bar{L}(X;Y\|Z)=0$ if and only if, for any $U_1, U_2, \cdots, U_k$ satisfying \eqref{eq:LowerboundoddiMarkov} and \eqref{eq:LowerboundeveniMarkov}, any $i$ with $1\leq i\leq k$, and any $u_{1:i-1}$ such that $\mathbb{P}[U_{1:i-1}=u_{1:i-1}]>0$, we have for odd and even $i$, respectively,
\begin{align}
& I(U_i; Y|U_{1:i-1}\!=\!u_{1:i-1}) \le \!I(U_i;Z|U_{1:i-1}\!=\!u_{1:i-1}) \label{eq231}\\
 & I(U_i; X|U_{1:i-1}\!=\!u_{1:i-1})\le \!I(U_i;Z|U_{1:i-1}\!=\!u_{1:i-1})\label{eq232}.
 \end{align}
The reason is that if either \eqref{eq231} or \eqref{eq232} fails, we can construct a valid $U_i$
by setting $U_i$ to a constant if $U_{1:i-1}\neq u_{1:i-1}$ so that 
\begin{align}
    &I(U_i;Y|U_{1:i-1})=\mathbb{P}[U_{1:i-1}=u_{1:i-1}] I(U_i; Y|U_{1:i-1}\!=\!u_{1:i-1}).
\end{align}
We can then compute a non-zero lower bound ${L}(X;Y\|Z)$ by  considering $k=\zeta=i$ in 
\eqref{eq:lowerbound}.

Equivalently, one can show that $\bar{L}(X;Y\|Z)=0$ if and only if for any $U_1, U_2, \cdots, U_k$ satisfying \eqref{eq:LowerboundoddiMarkov} and \eqref{eq:LowerboundeveniMarkov}, any $i$ with $1\leq i\leq k$, and any $u_{1:i-1}$ such that $\mathbb{P}[U_{1:i-1}=u_{1:i-1}]>0$, the one-way SK rates for the distribution
\begin{align}
  r_{XY}(\cdot)=p_{XY|U_{1:i-1}}(\cdot | u_{1:i-1})
\end{align}
are zero. This is because for any $r_{UVXY}=r_{UV|X}\, r_{Y|X}$ such that $I_r(U;Y|V)-I_r(U;Z|V)>0$, there exists some $v$ such that $I_r(U;Y|V=v)-I_r(U;Z|V=v)>0$. 
If $i$ is odd, we can then append to $U_{1:i-1}$ the choices $U_i=V$ and $U_{i+1}=U$. Considering $U_{1:i}=(u_{1:i-1}, v)$, we have
\begin{align}
   I(U_{i+1}; Y|U_{1:i}\!=\!u_{1:i})\!-\!I(U_{i+1};Z|U_{1:i}\!=\!u_{1:i})> 0.
\end{align}
For even $i$, we can set $U_i=\text{constant}$, $U_{i+1}=V$, and $U_{i+2}=U$, and proceed similarly.

To complete the proof, we need to characterize the class of pmfs $r_{XY}$ that arises when we condition the joint pmf of $(X,Y)$ on  $U_{1:i-1}=u_{1:i-1}$. The authors of \cite{MaIshwar2013,Verdu} consider this problem, where they search for the set of  conditional pmfs  $p_{XY|U_{1:k}}$ that one can obtain with some auxiliary random variables $U_1, U_2, \cdots, U_k$ satisfying
\begin{align*}
	&U_i\rightarrow XU_{1:i-1}\rightarrow Y \quad \text{for odd $i$},\nonumber\\
	&U_i\rightarrow YU_{1:i-1}\rightarrow X \quad \text{for even $i$}
\end{align*}
for some arbitrary $k$ and arbitrary realization $u_{1:k}$ of $U_{1:k}$ satisfying $\mathbb{P}[U_{1:k}=u_{1:k}]>0$. This set of pmfs can be expressed as $q_{XY}(x,y)=a(x)b(y)p_{XY}(x,y)$ for some functions $a:\mathcal{X}\rightarrow \mathbb{R}$ and $b:\mathcal{Y}\rightarrow \mathbb{R}$ \cite{MaIshwar2013, Verdu}, i.e., $q_{XY}\preceq p_{XY}$. Combining this observation with (\ref{eq:Theorem6part1}) proves that $\bar{L}(X;Y\|Z)=0$  if and only if
\begin{align}
  \epsilon & \le 1-\max_{\substack{q_{XY}:\: q_{XY}\preceq p_{XY}}}\eta(q_{Y|X}) \overset{(a)}{=}1-\max_{\substack{q_{XY}:\: q_{XY}\preceq p_{XY}}}\rho^2_m(q_{XY})\end{align}
where $(a)$ follows by \eqref{eqnAG687rr657} and because if $q_{XY}\preceq p_{XY}$, then for any 
$r_{XY}=r_{X}\, q_{Y|X}$ we also have $r_{XY}\preceq p_{XY}$. A similar argument is used in \cite[Eq. (80)]{Verdu}.

\noindent
\textbf{Part 3:} Since $p_{Z|XY}$ is an erasure channel with probability $\epsilon$, Lemma \ref{lemmmasthm1}  in Appendix \ref{appendix-lemmas} shows that  a given conditional pmf $p_{\bar{Z}|X,Y}$ can be produced with a degradation $p_{\bar{Z}|Z}$ on random variable $Z$, if and only if 
\begin{align}
\epsilon\leq \sum_{\bar{z}} \min_{x,y:\: p_{XY}(x,y)>0} p_{\bar{Z}|X,Y}(\bar{z}|x,y).
\end{align}
As a result, the intrinsic mutual information $B_0(X;Y\|Z)$ in (\ref{eq:B0upperbound}) is zero if and only if 
\begin{align}
\epsilon\leq \sup_{\substack{p_{\bar{Z}|X,Y}:\\ I(X;Y|\bar{Z})=0}}\sum_{\bar{z}} \min_{x,y:\: p_{X,Y}(x,y)>0} p_{\bar{Z}|X,Y}(\bar{z}|x,y).
\end{align}
In computing $B_0(X;Y\|Z)$, it suffices to restrict to random variables $\bar{Z}$ with cardinality at most $|\mathcal{Z}|$ \cite{christandl2003property}. Therefore, we assume that $\bar{z}\in\{1,2,\cdots, |\mathcal{Z}|\}$.
Finally, observe that the condition $I(X;Y|\bar{Z})=0$ is equivalent to the condition that the matrix 
$$[p_{XY}(x,y)p_{\bar{Z}|X,Y}(\bar{z}|x,y)]_{x,y}$$
has rank one  for all $\bar{z}$.


\noindent
\textbf{Part 4:} Consider an arbitrary $p_{J|XY}$. The bound $ B_1(X;Y\|Z)$ in (\ref{eq:B1upperbound}) is zero only if $I(X;Y|J)=0$. Thus, assume that for $p_{X,Y,J}=p_{X,Y}p_{J|X,Y}$, we have $X\rightarrow J\rightarrow Y$ forming a Markov chain. Since $Z$ is the result of $XY$ passing through an erasure channel, for any $p_{UVXY}\, p_{Z|XY}$ we have
\begin{align}
  I(U;Z|V)=(1-\epsilon)I(U;XY|V).
\end{align}
Thus, $ B_1(X;Y\|Z)$ is zero if and only if for any $p_{UVXY}p_{J|X,Y}$ we have
\begin{align}(1-\epsilon)I(U;XY|V)\geq I(U;J|V).\label{eqnAAAcv1}
\end{align}
We claim that this is equivalent with the following condition:  for any $q_{U,X,Y}p_{J|X,Y}$ we must have
\begin{align}(1-\epsilon)I(U;XY)\geq I(U;J).\label{eqnAAAcv2}
\end{align}
Clearly \eqref{eqnAAAcv2} implies \eqref{eqnAAAcv1} for \eqref{eqnAAAcv2} implies that 
\begin{align}
  (1-\epsilon)I(U;XY|V=v)\geq I(U;J|V=v), \quad \forall v.
\end{align}
For the other direction, assume that \eqref{eqnAAAcv1} holds for  any $p_{UVXY}p_{J|X,Y}$. Take some $v\in\mathcal{V}$ and assume that $U$ is a constant when $V\neq v$, while we let $p(u|V=v)$ to be arbitrary. Then, \eqref{eqnAAAcv1} implies that 
\begin{align}(1-\epsilon)I(U;XY|V=v)\geq I(U;J|V=v).\label{eqnAAAcv1p}
\end{align}
Since $P_{XY}>0$, for any $q_{XY}$ one can find $p_{VXY}$ such that $p(x,y|V=v)=q_{XY}$. Since 
$p(u|V=v)$ was arbitrary, we obtain \eqref{eqnAAAcv2}. 

The condition \eqref{eqnAAAcv2} can be expressed in terms the strong data processing constant and Renyi's maximal correlation as
\begin{align}
	1-\epsilon \geq \max_{q_{XY}\, p_{J|XY}}s^*(q_{XYJ})=  \eta(p_{J|X,Y}).
\end{align}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:A5}}\label{subsec:proofdsbe}
Using Theorem~\ref{theorem-lowerbounds},  the one-way SK rate for a DSBE source is positive if
\begin{align}
  \epsilon>1-\max_{\substack{q_{XY}:\: q_{XY}=q_{X}p_{Y|X}}}\rho^2_m(q_{XY}).
\end{align}
In particular, the one-way SK rate for a DSBE source is positive if
\begin{align}
  \epsilon>1-\rho^2_m(p_{XY})=1-{(1-2p)}^2=4p(1-p).
\end{align}
We next prove that $\bar{L}(X;Y\|Z)=0$ if $\epsilon\leq 4p(1-p)$, which completes the proof of Theorem~\ref{thm:A5}. Note that the lower bound $\bar{L}(X;Y\|Z)$ obtained from (\ref{eq:lowerbound}) includes the one-way SK rate. Using \cite[Theorem 6]{Verdu}, we obtain
\begin{align}
\max_{\substack{q_{XY}:\: q_{XY}\preceq p_{XY}}}\rho_m^2(q_{XY})&=(1-2p)^2 \label{eq:maximalcorrforDSBE}.
\end{align}
Combining (\ref{eq:theorem4lowerbound}) with (\ref{eq:maximalcorrforDSBE}) gives  the desired result.






\appendices
\section{A New Interpretation of the Upper Bound $B_1(X;Y\|Z)$}\label{appendixA}
In this section, we give a new interpretation of the upper bound $B_1(X;Y\|Z)$. In \cite{ourpaper}, the bound $S(X;Y\|Z)\leq B_1(X;Y\|Z)$ follows by showing that
\begin{align}
S(X;Y\|Z)&\leq S(X;Y\|J)+S_{\text{ow}}(XY; J\|Z)\nonumber
\\&= S(X;Y\|J)+\!\max_{UV\rightarrow XY\rightarrow ZJ}I(U;J|V)\!-\!I(U;Z|V)\label{less-noisy-djf}
\end{align}
where $S_{\text{ow}}(XY; J\|Z)$ is the one-way SK rate from $XY$ to $J$. The interpretation of \eqref{less-noisy-djf} given in \cite{ourpaper} is to split the key shared between $X$ and $Y$ (and hidden from $Z$) into two parts: a part independent of $J$ (i.e., the term $S(X;Y\|J)$) and another part shared with $J$ (i.e., the term $S_{\text{ow}}(XY; J\|Z)$). 

We now give a new interpretation for (\ref{less-noisy-djf}). To do this, we begin by revisiting the intrinsic mutual information upper bound $B_0(X;Y\|Z)$. For this bound, note that making Eve weaker by passing $Z$ through a channel $p_{J|Z}$ does not decrease the SK capacity. Thus, $S(X;Y\|Z)\leq S(X;Y\|J)$ if $J\rightarrow Z\rightarrow XY$ forms a Markov chain. We now replace the \emph{degradation} of $Z$ to $J$ with the \emph{less noisy} condition. Consider a broadcast channel with input $(X,Y)$ and two outputs $Z$ and $J$. We have the following proposition:
\begin{proposition} 
	\label{prop134}
	If the channel $p_{Z|XY}$ is less noisy than the channel $p_{J|XY}$, then 
	\begin{align}
	S(X;Y\|Z)\leq S(X;Y\|J).\label{prop:lessnoisy}
	\end{align}
\end{proposition}

\begin{proof}[Proof 1]
	The proposition follows from \eqref{less-noisy-djf} since if $Z$ is less noisy than $J$, then $I(U;J|V)-I(U;Z|V)$ vanishes in (\ref{less-noisy-djf}). We give a direct proof of Proposition \ref{prop134} below. 
\end{proof}

\begin{proof}[Proof 2]
	Suppose that we have a code for $(X^n, Y^n, Z^n)$ with public communication $F_1, F_2, \cdots, F_k$, and keys $K_A$ and $K_B$. We then have
\begin{align}
   \frac 1n I(K_A;Z^nF_{1:k})\leq \epsilon.
\end{align}
	We now show that
	\begin{align}
	I(K_A;J^nF_{1:k})\leq I(K_A;Z^nF_{1:k})\label{eq:claimlessnoisy}
	\end{align}
	which shows that the code is secure also for an eavesdropper that has i.i.d.\ repetitions of $J$ instead of $Z$. To prove (\ref{eq:claimlessnoisy}), we need to show that 
\begin{align}
   I(K_A;J^n|F_{1:k})\leq I(K_A;Z^n|F_{1:k}).
\end{align}
	It therefore suffices to show for all $f_{1:k}$ that
	\begin{align}
	&I(K_A;J^n|F_{1:k}=f_{1:k})\leq I(K_A;Z^n|F_{1:k}=f_{1:k}).\label{eqn:rev1}\end{align}
	Since we have the Markov chain $F_{1:k}K_A\rightarrow X^nY^n\rightarrow Z^nJ^n$, when we condition on $F_{1:k}=f_{1:k}$, we also have the Markov chains
	\begin{align}
	    \begin{array}{l}
	      K_A \rightarrow X^nY^n\rightarrow Z^nJ^n \\
	      F_{1:k} \rightarrow X^nY^n\rightarrow Z^nJ^n.
	    \end{array}
	\end{align}
Since $p_{Z|XY}$ is less noisy than $p_{J|XY}$, the $n$-letter product channel $p_{Z^n|X^nY^n}$ is also less noisy than the channel $p_{J^n|X^nY^n}$. Thus, we have the bound \eqref{eqn:rev1}.
\end{proof}

Proposition \ref{prop134} gives the following interpretation of \eqref{less-noisy-djf}: the term $S_{\text{ow}}(XY; J\|Z)$ is the penalty of deviating from the less-noisy condition when we replace $Z$ with $J$ in $S(X;Y\|Z)$ and $S(X;Y\|J)$.

\section{A New Measure of Correlation}\label{appendixB}
The quantity $\epsilon_2$ given in Definition \ref{def:path} motivates a new measure of correlation. 
Suppose we are given a joint pmf $p_{XY}$. 
Let
\begin{align*}
   q_{X_1Y_1X_2Y_2}(x_1,y_1,x_2,y_2) & =p_{XY}(x_1,y_1)p_{XY}(x_2,y_2) \\
   r_{X_1Y_1X_2Y_2}(x_1,y_1,x_2,y_2) & =p_{XY}(x_1, y_2)p_{XY}(x_2, y_1)
\end{align*}
and define the new correlation measure
\begin{align}
	&J_\alpha(X;Y)\triangleq D_{\alpha}\Big({q_{X_1Y_1X_2Y_2}}\big\|{r_{X_1Y_1X_2Y_2}}\Big)\label{eq:Japlhadef}
\end{align}
where $D_\alpha$ is the R{\'e}nyi divergence of order $\alpha$. We have
\begin{align}
&\exp(J_{\infty}(X;Y))=\max_{\substack{x_1, x_2, y_1, y_2:\\
\substack{p_X(x_1), p_X(x_2)>0,\\ p_Y(y_1), p_Y(y_2)>0}}
}\left(\frac{p_{XY}(x_1,y_1)p_{XY}(x_2,y_2)}{p_{XY}(x_1, y_2) p_{XY}(x_2, y_1)}\right) \label{eqn:J134}
\nonumber\\
&\quad= \max_{\substack{x_1\neq x_2, y_1\neq y_2:\\
\substack{p_X(x_1), p_X(x_2)>0,\\ p_Y(y_1), p_Y(y_2)>0}}}
\left(\frac{p_{XY}(x_1,y_1)p_{XY}(x_2,y_2)}{p_{XY}(x_1, y_2) p_{XY}(x_2, y_1)}\right)=\frac{1}{\epsilon_2^2}.
\end{align}
Some properties of $J_\alpha$ are as follows.
\begin{itemize}
\item \emph{Faithfulness}: $J_{\alpha}(X;Y)\geq 0$ with equality $J_{\alpha}(X;Y)=0$ if and only if $X$ and $Y$ are independent. Equality  follows from
\begin{align}
   & q_{X_1Y_1X_2Y_2}(x_1,y_1,x_2,y_2)  = r_{X_1Y_1X_2Y_2}(x_1,y_1,x_2,y_2)
\end{align}
for all $x_1,x_2,y_1,y_2$.
{Data Processing}: If $A\rightarrow X\rightarrow Y\rightarrow B$, then we have $J_{\alpha}(X;Y)\geq J_{\alpha}(A;B)$. In particular, we have
\begin{align}
  J_{\alpha}(X_1X_2; Y_1Y_2)\geq J_{\alpha}(X_1;Y_1).
\end{align}
To show that $A\rightarrow X\rightarrow Y\rightarrow B$ implies $J_{\alpha}(X;Y)\geq J_{\alpha}(A;B)$,
consider some $p_{A|X}$ and $p_{B|Y}$. Let
\begin{align}
    &V(a_1a_2b_1b_2|x_1x_2y_1y_2) = p_{A|X}(a_1|x_1)  p_{A|X}(a_2|x_2) p_{B|Y}(b_1|y_1) p_{B|Y}(b_2|y_2)
\end{align}
and define
\begin{align}
&q_{A_1B_1A_2B_2}(a_1,b_1,a_2,b_2)=\sum_{x_1,x_2,y_1,y_2}\Big(q_{X_1Y_1X_2Y_2}(x_1,y_1,x_2,y_2) V(a_1a_2b_1b_2|x_1x_2y_1y_2)\Big)
\nonumber\\&\quad =p_{AB}(a_1,b_1)p_{AB}(a_2,b_2)
\end{align}
and
\begin{align}
&r_{A_1B_1A_2B_2}(a_1,b_1,a_2,b_2)=\sum_{x_1,x_2,y_1,y_2}\Big(r_{X_1Y_1X_2Y_2}(x_1,y_1,x_2,y_2) V(a_1a_2b_1b_2|x_1x_2y_1y_2)\Big)
\nonumber\\&\quad=p_{AB}(a_1,b_2)p_{AB}(a_2,b_1).
\end{align}
The bound $J_{\alpha}(X;Y)\geq J_{\alpha}(A;B)$ follows from the data processing property of $D_\alpha$.

\item \emph{Symmetry}: The definition (\ref{eq:Japlhadef}) implies that 
\begin{align}
  J_{\alpha}(X;Y)=J_{\alpha}(Y;X).
\end{align}
\item \emph{Additivity}: If $(X_1, Y_1)$ and $(X_2, Y_2)$ are independent, then
\begin{align}
  J_{\alpha}(X_1X_2; Y_1Y_2)=J_{\alpha}(X_1;Y_1)+J_{\alpha}(X_2;Y_2)
\end{align}
which follows from (\ref{eq:Japlhadef}).
\end{itemize}

Furthermore, $J_\infty(X;Y)$ has the following properties:
\begin{itemize}
\item By \eqref{eqn:J134} we see that $J_\infty(X;Y)$ depends only on $p_{Y|X}$ and the support set of the distribution on $X$. Thus, with abuse of notation, we may write
\begin{align}
J_\infty(p_{Y|X}(y|x), \mathcal{X}')\triangleq J_\infty(X;Y)
\end{align}
where
$\mathcal{X}'=\{x:p_X(x)>0\}$.
\item For a pmf $p_{XYZ}$, let 
\begin{align}
    J_{\infty}(X;Y|Z)\triangleq \max_{z: p_Z(z)>0} J_\infty(X;Y|Z=z).
\end{align}
Then, if $F-X-Y$ forms a Markov chain, we have
\begin{align}
  J_{\infty}(X;Y)\geq J_\infty(X;Y|F).
\end{align}
\end{itemize}
An application of $J_{\infty}$ is given in the next subsection. As another application, 
consider a key agreement protocol of blocklength $n$ with public messages $F_1, F_2, \ldots, F_k$ to agree on single bits $K_A, K_B\in\{0,1\}$. Then we can write
\begin{align}n J_{\infty}(X;Y|Z)&=J_{\infty}(X^n;Y^n|Z^n) \nonumber
\\&\geq J_{\infty}(X^n;Y^n|F_1Z^n) \nonumber
\\&\geq J_{\infty}(X^n;Y^n|F_1F_2Z^n) \nonumber
\\&\quad\ldots \nonumber
\\&\geq J_{\infty}(X^n;Y^n|F_{1:k}Z^n) \nonumber
\\&\geq J_{\infty}(K_A;K_B|F_{1:k}Z^n) \nonumber
\\&\geq \max_{f_{1:k},z^n} \log\Bigg(\frac{p_{K_AK_B|f_{1:k},z^n}(0,0)}{p_{K_AK_B|f_{1:k},z^n}(0,1)}\frac{p_{K_AK_B|f_{1:k},z^n}(1,1)}{p_{K_AK_B|f_{1:k},z^n}(1,0)}\Bigg) 
\end{align}
providing a bound on how fast Alice and Bob can approach the ideal distribution 
\begin{align}
   q_{K_AK_B Z^n F_{1:k}}=\frac{1}{2}\mathds{1}[k_A=k_B]\,p_{Z^n F_{1:k}}
\end{align}
as given in \eqref{quality}.

\subsection{An ``Uncertainty Principle" for Channel Coding}
Consider a point to point communication to send a message $M$ over a channel $p_{Y|X}(y|x)$ with $n$ channel uses. Then, the chain $M\rightarrow X^n\rightarrow Y^n\rightarrow \hat{M}$ is Markov so that the data-processing property of the new correlation measure implies
\begin{align}
   J_{\infty}(M;\hat{M})\leq J_{\infty}(X^n;Y^n).
\end{align}
However, $J_{\infty}(X^n;Y^n)$ depends only on the channel $p(y^n|x^n)$ and the support set of $X^n$, that is $\{x^n:p(x^n)>0\}$. Because increasing the support set can only increase $J_{\infty}$, we conclude that 
\begin{align}
  J_{\infty}(X^n;Y^n) & \le J_{\infty}(p(y^n|x^n), \mathcal{X}^n)  = n\times J_{\infty}(p(y|x), \mathcal{X}).
\end{align}
Thus, we have
\begin{align}
  J_{\infty}(M;\hat{M})\leq n\times J_{\infty}(p(y|x), \mathcal{X})
\end{align}
which implies that for any $m_1\neq m_2$ we have
\begin{align}\log &\left(\frac{p_{\hat{M}|M}(m_1|m_1)}{p_{\hat{M}|M}(m_2|m_1)}\times \frac{p_{\hat{M}|M}(m_2|m_2)}{p_{\hat{M}|M}(m_1|m_2)}\right)\leq n\times J_{\infty}(p(y|x), \mathcal{X}).\label{eqwesdn1f}\end{align}
Equivalently, we have the following inequality
\begin{align} &\frac{p_{\hat{M}|M}(m_2|m_1)}{p_{\hat{M}|M}(m_1|m_1)}\times \frac{p_{\hat{M}|M}(m_1|m_2)}{p_{\hat{M}|M}(m_2|m_2)}\geq \exp\left(-n\times J_{\infty}(p(y|x),    \mathcal{X})\right).\label{eqwesdn1f2}\end{align}
Define
\begin{align*}
	\frac{P_{\hat{M}|M}(m_2|m_1)}{P_{\hat{M}|M}(m_1|m_1)}
\end{align*}
as the ``uncertainty of $M=m_1$" against $M=m_2$ if the true value of $M$ is $m_1$. Similarly, define the ``uncertainty of $M=m_2$" against $M=m_1$ if the true value of $M$ is $m_2$ as
\begin{align*}
	\frac{P_{\hat{M}|M}(m_1|m_2)}{P_{\hat{M}|M}(m_2|m_2)}.
\end{align*}
Then \eqref{eqwesdn1f2} gives a lower bound on the product of the uncertainty of $M=m_1$ and the uncertainty of $M=m_2$. \eqref{eqwesdn1f2} states that ``if the uncertainty when the true value of $M$ is $m_1$ is very small, then the uncertainty when the true value of $M$ is $m_2$ cannot be small." This may be considered as an uncertainty principle. 

\section{Lemmas Used in the Proof of Theorem~\ref{theorem-lowerbounds}}\label{appendix-lemmas}
\begin{lemma}\label{lemma2fdret}
For any $p_{X}$ such that $p_X(x)>0$ for all $x$, and any $p_{Y|X}$, we have
\begin{align}
  \eta\big(p_{Y|X}\big) =\sup_{UV\rightarrow X\rightarrow Y}\frac{I(U;Y|V)}{I(U;X|V)}.
\end{align}

\end{lemma}
\begin{proof}
For any $(U,V)$ satisfying the Markov chain $UV\rightarrow X\rightarrow Y$, we have 
\begin{align}
\frac{I(U;Y|V)}{I(U;X|V)} & \leq \max_{v}\frac{I(U;Y|V=v)}{I(U;X|V=v)}\nonumber
\\&\quad\overset{(a)}{\leq} s^*_r(X;Y)\nonumber
\\&\quad\leq \max_{\substack{q_{XY}:\: q_{XY}=q_{X}p_{Y|X}}}s_q^*(X;Y)\nonumber
\\&\quad=\eta\big(p_{Y|X}\big)\label{eqref12356f}
\end{align}
where $(a)$ follows because $s^*_r(X;Y)$ is the strong data processing constant evaluated according to
\begin{align}r_{XY}(\cdot) = p_{XY|V}(\cdot | v) = p_{X|V}(\cdot | v) p_{Y|X} (\cdot | \cdot).
\end{align}
Therefore, we have
\begin{align}
  \eta\big(p_{Y|X}\big) \geq \sup_{UV\rightarrow X\rightarrow Y}\frac{I(U;Y|V)}{I(U;X|V)}.
\end{align}
On the other hand, consider an arbitrary $p_{V|X}$.
Fix some $v^*$ such that $P_V(v^*)>0$ and let $U$ be a constant when $V\neq v^*$. Then we have
\begin{align}
  \frac{I(U;Y|V)}{I(U;X|V)}= \frac{I(U;Y|V=v^*)}{I(U;X|V=v^*)}.
\end{align}
Thus, we obtain 
\begin{align}
  \sup_{UV\rightarrow X\rightarrow Y}\frac{I(U;Y|V)}{I(U;X|V)}\geq s^*(X;Y|V=v^*).
\end{align}
Since $p_X(x)>0$ for all $x\in\mathcal{X}$, for any pmf $q_X$ on $\mathcal{X}$ one can find a channel $P_{V|X}$ and a value $v^*$ such that $P_{X,Y|V}(\cdot |v^*)=q_X(\cdot) p_{Y|X}(\cdot | \cdot)$. Thus, we obtain
\begin{align}
 \sup_{UV\rightarrow X\rightarrow Y}\frac{I(U;Y|V)}{I(U;X|V)}
 & \ge \max_{q_X}s^*\big(q_X\, p_{Y|X}\big)  = \eta\big(p_{Y|X}\big).
\end{align}
\end{proof}

\begin{lemma}\label{lemmmasthm1}
Let $\mathcal{A}$ and $\mathcal{R}$ be arbitrary discrete sets. Let $p_{A}$ be a pmf on $\mathcal{A}$ such that $p_{A}(a)>0$ for all $a\in\mathcal{A}$. Let $\mathcal{B}=\mathcal{A}\cup\{\mathtt e\}$ and 
assume that $p_{B|A}$ is an erasure channel with probability $\epsilon$ (here $\mathtt e$ is the erasure symbol). Consider an arbitrary $q_{R|A}$. There exists  a conditional distribution $p_{R|B}$ such that
\begin{align}
	\sum_bp_{A,B}(a,b)p_{R|B}(r|b)=p_{A}(a)q_{R|A}(r|a)\label{eqn:sderf3r3}
\end{align}
for all $a,r$ if and only if
\begin{align}\sum_{r} \min_{a} q_{R|A}(r|a)&\geq \epsilon.\label{eqn:reg0}\end{align}
\end{lemma}

\begin{remark}
	The term  $\sum_{r} \min_{a} q_{R|A}(r|a)$ is known as  Doeblin's coefficient of ergodicity of the channel $q_{R|A}$ \cite[Section 5]{Cohen}. One direction of Lemma~\ref{lemmmasthm1} is mentioned in \cite{makur2018comparison} and \cite[Remark 3.2]{Maxim}; the other direction seems to be new. 
\end{remark}

\begin{proof}
Suppose that we have  a conditional distribution $p_{R|B}$ such that \eqref{eqn:sderf3r3} holds. Since $p_{B|A}$ is an erasure channel, given an input $A=a$, $B$ has two possibilities $B\in\{a,\mathtt{e}\}$. We have
\begin{align}
	p_{A}(a)q_{R|A}(r|a)&=\sum_bp_{A,B}(a,b)p_{R|B}(r|b) \!=\!\epsilon\, p_{A}(a)p_{R|B}(r|\mathtt e)\!+\!(1\!-\!\epsilon)p_{A}(a)p_{R|B}(r|a) \nonumber
\\&\geq\! \epsilon\, p_{A}(a)p_{R|B}(r|\mathtt e).
\end{align}
Thus, from $p_{A}(a)>0$ for all $a\in\mathcal{A}$ we have
$
q_{R|A}(r|a)\geq \epsilon p_{R|B}(r|\mathtt e).
$
We obtain
\begin{align}
\min_{a}q_{R|A}(r|a)\geq \epsilon \, p_{R|B}(r|\mathtt e).
\end{align}
Therefore, we observe that
\begin{align}
\sum_{r}\min_{a}q_{R|A}(r|a)\geq \epsilon \sum_{r}p_{R|B}(r|\mathtt e)=\epsilon
\end{align}
which proves the correctness of \eqref{eqn:reg0}.

Conversely, take some arbitrary $q_{R|A}$ satisfying  \eqref{eqn:reg0} and let $q_{AR}=p_A\, q_{R|A}$. Then for all $r$ we have
\begin{align}
  q_R(r)=\sum_{a}p_A(a)q_{R|A}(r|a)>0
\end{align}
and 
\begin{align}
  \sum_{a}p_A(a)\frac{q_{R|A}(r|a)}{q_{R}(r)}=1.
\end{align}
Thus, we conclude that for any $r$ we have
\begin{align}
  \min_{a}\frac{q_{R|A}(r|a)}{q_R(r)}\leq 1.
\end{align}
Furthermore, using \eqref{eqn:reg0}, we have
\begin{align}\sum_{r}q_R(r)\min_{a}\frac{q_{R|A}(r|a)}{q_R(r)}&\geq \epsilon.\label{eqn:reg0-34wsd}\end{align}
Thus, one can find 
$\lambda(r)\in[0,1]$ such that
\begin{align}\lambda(r)&\leq \min_{a}\frac{q_{R|A}(r|a)}{q_R(r)}  \label{eqn:reg1}
\end{align}
and
\begin{align}
\epsilon&=\sum_{r} q_R(r)\lambda(r).\label{eqn:reg2}\end{align}

Define $p_{R|B}$ as follows:
\begin{align}
p_{R|B}(r|\mathtt{e})&=\frac{q_R(r)\lambda(r)}{\epsilon}\label{eqn:reg3}
\\p_{R|B}(r|a)&=\frac{q_{R|A}(r|a)\!-\!q_{R}(r)\lambda(r)}{1\!-\!\epsilon}, \quad\forall a\in\mathcal{A}\label{eqn:reg4}.
\end{align}
The conditional probability in \eqref{eqn:reg3} is well defined by \eqref{eqn:reg2}. The conditional probability in \eqref{eqn:reg4} is non-negative and well-defined by \eqref{eqn:reg1}. Finally, observe that with the choice of $p_{R|B}(r|b)$ in (\ref{eqn:reg3}) and (\ref{eqn:reg4}), the marginal pmf of $A,R$ has
\begin{align}
p_{AR}(a,r)&=\sum_{b}p_{A,B}(a,b)p_{R|B}(r|b)
\nonumber\\&=\epsilon\, p_A(a)p_{R|B}(r|\mathtt{e})+(1-\epsilon)p_A(a)p_{R|B}(r|a)
\nonumber\\&=p_A(a)q_{R}(r)\lambda(r)\!+\!p_A(a)\big(q_{R|A}(r|a)\!-\!q_R(r)\lambda(r)\big)
\nonumber\\&=p_A(a)q_{R|A}(r|a)
\end{align}
which proves the converse.
\end{proof}
\fi
\fi

\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,References}
\bibliography{mybiblio}
%\bibliography{refrences}
\iffalse
\begin{IEEEbiography}[{}]{Amin Gohari}
received the B.Sc. degree from Sharif University of Technology, Tehran, Iran, in 2004 and his Ph.D. degree in Electrical Engineering from the University of California, Berkeley in 2010. From 2010-2011, he was a postdoc at the Chinese University of Hong Kong, Institute of Network Coding. From 2011-2020 he was with the Electrical Engineering department of Sharif University of Technology. He joined the Tehran Institute for Advanced Studies in 2020. Dr. Gohari received the 2010 Eli Jury Award from UC Berkeley, Department of Electrical Engineering and Computer Sciences, for “outstanding achievement in the area of communication networks,” and the 2009-2010 Bernard Friedman Memorial Prize in Applied Mathematics from UC Berkeley, Department of Mathematics, for “demonstrated ability to do research in applied mathematics.” He also received the Gold Medal from the 41st International Mathematical Olympiad (IMO 2000) and the First Prize from the 9th International Mathematical Competition for University Students (IMC 2002). He was a finalist for the best student paper award at IEEE International Symposium on Information Theory (ISIT) in three consecutive years, 2008, 2009 and 2010. He was also a co-author of a paper that won the ISIT 2013 Jack Wolf student paper award, and two that were finalists in 2012 and 2014. He was selected as an exemplary reviewer for Transactions on Communications in 2016. Dr. Gohari is currently serving as an Associate Editor for the IEEE Transactions on Information Theory.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.09in,height=1.22in]{photoonur}}]{Onur G\"unl\"u}
	(S'10\textendash M'18) received the B.Sc. degree in Electrical and Electronics Engineering from Bilkent University, Turkey in 2011; M.Sc. and Dr.-Ing. degrees in Communications Engineering both from the Technical University of Munich (TUM), Germany in October 2013 and November 2018, respectively. He worked as a Research and Teaching Assistant at TUM between February 2014 –May 2019. He is a Research Associate and Dozent at TU Berlin since June 2019. He visited the Information and Communication Theory (ICT) Lab at the TU Eindhoven, The Netherlands during February-March 2018. His research interests include information-theoretic privacy and security, coding theory, statistical signal processing for biometric secrecy systems and physical unclonable functions (PUFs), federated learning (FL) with differential privacy(DP) and information privacy guarantees, and secure identification. Among his publications is the recent book Key Agreement with Physical Unclonable Functions and Biometric Identifiers published by Dr. Hut Verlag. He organized workshops and special sessions in the privacy and security areas.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1.07in,height=1.2in]{kramer}}]{Gerhard Kramer}
	(S'91\textendash M'94\textendash SM'08\textendash F'10) received the Dr. sc. techn. degree from ETH Zurich in 1998. From 1998 to 2000, he was with Endora Tech AG, Basel, Switzerland, and from 2000 to 2008 he was with the Math Center, Bell Labs, Murray Hill, NJ, USA. He joined the University of Southern California, Los Angeles, CA, as a Professor of electrical engineering in 2009. He joined the Technical University of Munich (TUM) in 2010 as Alexander von Humboldt Professor and Chair of Communications Engineering. Since October 2019, he is Senior Vice President for Research and Innovation at TUM. His research interests include information theory and communications theory, with applications to wireless, copper, and optical fiber networks. He served as the 2013 President of the IEEE Information Theory Society.
\end{IEEEbiography}
\fi

\end{document}
















