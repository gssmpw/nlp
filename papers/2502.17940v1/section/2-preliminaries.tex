\section{Preliminaries}
\label{sec:sec-Preliminary}




\input{tables/tab-1-space-complexity}

\subsection{Notations and Basic Concepts}

In this paper, we use bold uppercase letters to denote matrices (e.g., $\boldsymbol{A}$), bold lowercase letters to denote vectors (e.g., $\boldsymbol{x}$), and lowercase letters to denote scalars (e.g., $w$).


Given a real matrix \(\boldsymbol{A} \in \mathbb{R}^{n\times m}\), its condensed singular value decomposition (SVD) is given by \(\boldsymbol{A} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^\top\), where \(\boldsymbol{U} \in \mathbb{R}^{n \times r}\) and \(\boldsymbol{V} \in \mathbb{R}^{m \times r}\) are matrices with orthonormal columns, and \(\boldsymbol{\Sigma} = \operatorname{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)\) is a diagonal matrix containing the singular values of \(\boldsymbol{A}\) arranged in non-increasing order, i.e., \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\), with \(r\) denoting the rank of \(\boldsymbol{A}\). We define several matrix norms as follows: the Frobenius norm of \(\boldsymbol{A}\) is given by 
\(
\|\boldsymbol{A}\|_F = \sqrt{\sum_{i,j} A_{i,j}^2} = \sqrt{\sum_{i=1}^r \sigma_i^2},
\)
the spectral norm is 
\(
\|\boldsymbol{A}\|_2 = \sigma_1,
\)
and the nuclear norm is 
\(
\|\boldsymbol{A}\|_* = \sum_{i=1}^r \sigma_i.
\)
For a matrix \(\boldsymbol{A}\), we denote its \(i\)-th column by \(\boldsymbol{a}_i\); hence, if \(\boldsymbol{A} \in \mathbb{R}^{n \times l}\) and \(\boldsymbol{B} \in \mathbb{R}^{m \times l}\), then their product can be expressed as: 
\(
\boldsymbol{A}\boldsymbol{B}^T = \sum_{i=1}^l \boldsymbol{a}_i \boldsymbol{b}_i^T.
\)
Finally, \(\boldsymbol{I}_n\) denotes the \(n \times n\) identity matrix and \(\boldsymbol{0}^{n \times m}\) denotes the \(n \times m\) zero matrix.








\subsection{Problem Definition}
The Approximate Matrix Multiplication (AMM) problem has been extensively studied in the literature \cite{YeLZ16,MrouehMG17,drineas2006fast,FrancisR22}. Given two matrices \(\boldsymbol{X} \in \mathbb{R}^{d_x \times n}\) and \(\boldsymbol{Y} \in \mathbb{R}^{d_y \times n}\), the AMM problem aims to obtain two smaller matrices \(\boldsymbol{B}_X \in \mathbb{R}^{d_x \times l}\) and \(\boldsymbol{B}_Y \in \mathbb{R}^{d_y \times l}\), with \(l \ll n\), so that
\(
\left\|\boldsymbol{X}\boldsymbol{Y}^\top - \boldsymbol{B}_X \boldsymbol{B}_Y^\top\right\|_2
\) is sufficiently small. Next, we formally define the problem of AMM over a Sliding Window.

\vspace{-1mm}
\begin{definition}[AMM over a Sliding Window]\label{def:amm}
Let \(\{(\boldsymbol{x}_t, \boldsymbol{y}_t)\}_{t \ge 1}\) be a sequence of data items (a data stream), where for each time \(t\) we have \(\boldsymbol{x}_t \in \mathbb{R}^{d_x}\) and \(\boldsymbol{y}_t \in \mathbb{R}^{d_y}\). For a fixed window size \(N\) and for any time \(T \ge N\), define the sliding window matrices
\[
\boldsymbol{X}_W = \begin{bmatrix} \boldsymbol{x}_{T-N+1} & \boldsymbol{x}_{T-N+2} & \cdots & \boldsymbol{x}_T \end{bmatrix} \in \mathbb{R}^{d_x \times N},\]
\[
\boldsymbol{Y}_W = \begin{bmatrix} \boldsymbol{y}_{T-N+1} & \boldsymbol{y}_{T-N+2} & \cdots & \boldsymbol{y}_T \end{bmatrix} \in \mathbb{R}^{d_y \times N}.
\]
A streaming algorithm (or matrix sketch) \(\kappa\) is said to yield an \(\epsilon\)-approximation for AMM over the sliding window if, at every time \(T \ge N\), it outputs matrices \(\boldsymbol{A}_W \in \mathbb{R}^{d_x \times m}\) and \(\boldsymbol{B}_W \in \mathbb{R}^{d_y \times m}\) (with \(m \le N\), typically \(m \ll N\)) satisfying
\[
\left\|\boldsymbol{X}_W \boldsymbol{Y}_W^\top - \boldsymbol{A}_W \boldsymbol{B}_W^\top\right\|_2 \le \epsilon\, \|\boldsymbol{X}_W\|_F \, \|\boldsymbol{Y}_W\|_F.
\]
That said, the product \(\boldsymbol{A}_W \boldsymbol{B}_W^\top\) produced by the sketch \(\kappa\) approximates the true product \(\boldsymbol{X}_W \boldsymbol{Y}_W^\top\) with a spectral norm error that is at most an \(\epsilon\)-fraction of the product of the Frobenius norms of \(\boldsymbol{X}_W\) and \(\boldsymbol{Y}_W\).
\end{definition}
\vspace{-1mm}



In \cite{WeiLLSDW16}, it is shown that one may assume without loss of generality that the squared norms of the columns of \(\boldsymbol{X}\) and \(\boldsymbol{Y}\) are normalized to lie in the intervals \([1, R_x]\) and \([1, R_y]\), respectively, which is a mild assumption. We denote \(R = \max(R_x, R_y)\).







Next, we present two key techniques that underpin our solution: the Co-occurring Directions (COD) algorithm \cite{MrouehMG17} and the \(\lambda\)-snapshot method \cite{LeeT06}. While each technique was originally developed for a different problem, we show later that their careful and nontrivial integration yields a solution with optimal space to gain $\epsilon$-approximation guarantee.




\subsection{Co-Occurring Directions}


Co-occurring Directions (COD) \cite{MrouehMG17} is a deterministic streaming algorithm for approximate matrix multiplication. Given matrices \(\boldsymbol{X} \in \mathbb{R}^{d_x \times n}\) and \(\boldsymbol{Y} \in \mathbb{R}^{d_y \times n}\) whose columns arrive sequentially, COD maintains sketch matrices \(\boldsymbol{B}_X \in \mathbb{R}^{d_x \times l}\) and \(\boldsymbol{B}_Y \in \mathbb{R}^{d_y \times l}\) (with \(l \ll n\)) so that 
\(
\boldsymbol{X}\boldsymbol{Y}^\top \approx \boldsymbol{B}_X \boldsymbol{B}_Y^\top.
\)
Algorithm \ref{alg:cod} shows the pseudo-code of COD algorithm. In essence, each incoming column pair \((\boldsymbol{x}_i, \boldsymbol{y}_i)\) is inserted into an available slot in the corresponding sketch (Lines 3-4). When a sketch becomes full, a compression step is performed (Lines 5-12): the sketches are first orthogonalized via QR decomposition (Lines 6-7); then, an SVD is applied to the product of the resulting factors (Line 8); finally, the singular values are shrunk by a threshold \(\delta\) (typically chosen as the \(\ell/2\)-th singular value) to update the sketches (Lines 9-12). This process effectively discards less significant directions while preserving the dominant correlations, thereby controlling the approximation error. 

\begin{algorithm}[t]
    \caption{Co-occurring Directions (COD)}
    \label{alg:cod}
    \DontPrintSemicolon
    \KwInput{\(\boldsymbol{X}\in \mathbb{R}^{d_x\times n}\), \(\boldsymbol{Y}\in \mathbb{R}^{d_y\times n}\), sketch size \(l\)}
    \KwOutput{\(\boldsymbol{A}\in \mathbb{R}^{d_x\times l}\) and \(\boldsymbol{B}\in \mathbb{R}^{d_y\times l}\)}
    
    Initialize \(\boldsymbol{A} \leftarrow \boldsymbol{0}^{d_x\times l}\) and \(\boldsymbol{B} \leftarrow \boldsymbol{0}^{d_y\times l}\)\;
    
    \For{\(i = 1, \dots, n\)}{
        Insert \(\boldsymbol{x}_i\) into an empty column of \(\boldsymbol{A}\)\;
        
        Insert \(\boldsymbol{y}_i\) into an empty column of \(\boldsymbol{B}\)\;
        
        \If{\(\boldsymbol{A}\) or \(\boldsymbol{B}\) is full}{
            \((\boldsymbol{Q}_x, \boldsymbol{R}_x) \leftarrow \text{QR}(\boldsymbol{A})\)\;
            
            \((\boldsymbol{Q}_y, \boldsymbol{R}_y) \leftarrow \text{QR}(\boldsymbol{B})\)\;
            
            \([\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}] \leftarrow \text{SVD}(\boldsymbol{R}_x \boldsymbol{R}_y^\top)\)\;
            
            \(\delta \leftarrow \sigma_{l/2}(\boldsymbol{\Sigma})\), \(\hat{\boldsymbol{\Sigma}} \leftarrow \max(\boldsymbol{\Sigma} - \delta\,\boldsymbol{I}_l, \boldsymbol{0})\)\;
            
            Update \(\boldsymbol{A} \leftarrow \boldsymbol{Q}_x\,\boldsymbol{U}\,\sqrt{\hat{\boldsymbol{\Sigma}}}\)\;
            Update \(\boldsymbol{B} \leftarrow \boldsymbol{Q}_y\,\boldsymbol{V}\,\sqrt{\hat{\boldsymbol{\Sigma}}}\)\;
        }
    }
    
    \Return \(\boldsymbol{A}, \boldsymbol{B}\)
\end{algorithm}






\vspace{-1mm}
\begin{theorem} \label{thm:cod}
The output of Co-occurring Directions (Algorithm~\ref{alg:cod}) provides correlation sketch matrices \((\boldsymbol{B}_X \in \mathbb{R}^{d_x \times l}, \boldsymbol{B}_Y \in \mathbb{R}^{d_y \times l})\) for \((\boldsymbol{X} \in \mathbb{R}^{d_x \times n}, \boldsymbol{Y} \in \mathbb{R}^{d_y \times n})\), where \(l \leq \min(d_x, d_y)\), satisfying:
\[
\left\|\boldsymbol{X} \boldsymbol{Y}^\top - \boldsymbol{B}_X \boldsymbol{B}_Y^\top\right\|_2 \leq \frac{2\|\boldsymbol{X}\|_F \|\boldsymbol{Y}\|_F}{l}.
\]
Algorithm~\ref{alg:cod} runs in \(O(n(d_x + d_y)l)\) time using \(O((d_x + d_y)l)\) space.
\end{theorem}

\subsection{$\boldsymbol{\lambda}$-Snapshot Method}

We explain the key idea of the $\lambda$-snapshot method \cite{LeeT06} by beginning with a bit stream
\(
f = \{b_1, b_2, b_3, \dots\}
\)
where 
\(
b_i \in \{0,1\},
\)
and the goal is to approximate the number of 1-bits in a sliding window. The \(\lambda\)-snapshot method achieves this by “sampling” every \(\lambda\)-th 1-bit. That is, if we index the 1-bits in order of appearance, the \(\lambda\)-th, \(2\lambda\)-th, \(3\lambda\)-th, etc., are stored. The stream is conceptually divided into blocks of \(\lambda\) consecutive positions (called \(\lambda\)-blocks), and a block is deemed \emph{significant} if it intersects the current sliding window and contains at least one sampled 1-bit. The algorithm maintains a \(\lambda\)-counter that tracks: {\em (i)} a queue \(Q\) holding the indices of significant \(\lambda\)-blocks; {\em (ii)} a counter \(\ell\) recording the number of 1-bits seen since the last sample; and {\em (iii)} auxiliary variables for the current block index and the offset within that block. When a new bit arrives, the offset is incremented and any block that no longer falls within the sliding window is removed from \(Q\). If the offset reaches \(\lambda\), the block index is incremented. Upon encountering a 1-bit, \(\ell\) is incremented; when \(\ell\) reaches \(\lambda\), that 1-bit is sampled (i.e., \(\ell\) is reset to 0) and the current block index is \underline{\em registered} and appended to \(Q\). The estimated count of 1-bits in the current window \(W_p\) is given by
\(
v(S) = |Q|\lambda + \ell.
\)
If the true count is \(m\), then it is guaranteed that:
\(
m \le v(S) \le m + 2\lambda,
\)
so that the error is bounded by \(2\lambda\). As the window slides, blocks falling entirely out of range are removed from \(Q\), ensuring that the estimate \(v(S) = |Q|\lambda + \ell\) remains valid with an error of at most \(2\lambda\). 

An example of how the $\lambda$-snapshot method works to count the number of 1-bits in a sliding window is provided in Appendix \ref{app:examples}.





To support frequency estimation over sliding window, a naive approach is to apply the $\lambda$-snapshot method to every distinct element in the stream. For any given element $e$, the stream can be represented as a bit stream, where each new bit is set to 1 if the stream element equals $e$ and 0 otherwise. However, maintaining a separate $\lambda$-snapshot structure for each element would lead to unbounded space usage. Lee et al.\ \cite{LeeT06} show that by maintaining only $O(1/\epsilon)$ such $\lambda$-snapshot structures and combine with the well known frequency estimation algorithm MG-sketch \cite{MisraG82}, an $\epsilon$-approximation for the frequency of each element can be achieved while using just $O(1/\epsilon)$ space. Although there are $O(1/\epsilon)$ $\lambda$-snapshot structures, they collectively track a stream containing at most $N$ ones, ensuring that the overall space cost remains bounded. For each element $e$, let $f(e)$ be its true frequency and $\hat{f}(e)$ be the estimated frequency derived using the $\lambda$-snapshot method, then
\(
f(e)-\hat{f}(e) \leq \epsilon N.
\)







