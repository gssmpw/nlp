

\section{Our Solution}
\label{sec:sec-prim}




\input{algorithms/alg-2-initialize}



In this section, we introduce our method, \oursolution (\underline{S}pace \underline{O}ptimal \underline{C}o-\underline{O}ccurring \underline{D}irections). As we mentioned earlier, our algorithm is inspired by the \(\lambda\)-snapshot method for \(\epsilon\)-approximation frequency estimation over sliding windows~\cite{LeeT06}.
The key idea is simple yet powerful: as new data arrives, we continuously monitor the dominant (i.e., most significant) co-occurring direction. When the product of the norms of the corresponding vectors exceeds a preset threshold \(\theta\), we “register” a snapshot capturing that direction and remove it from the sketch. This selective registration guarantees that only directions with substantial contributions are maintained—ensuring both accuracy and space efficiency.

 In this section, we focus on the \emph{normalized sliding window} case, where every incoming vector is unit-norm (\(\|\boldsymbol{x}_i\| = \|\boldsymbol{y}_i\| = 1\)). In this case, we have \(\|\boldsymbol{A}_W\|_F^2 = \|\boldsymbol{B}_W\|_F^2 = N\) (with \(N\) being the window length). In Section \ref{sec:unnormalized-setting}, we extend the method to handle the general case (\(R\geq 1\)), referred to as the unnormalized sliding window.


\input{algorithms/alg-3-simple-update}


\subsection{Algorithm Description}

\textbf{Sketch Structure.}
To handle the sliding window efficiently, our proposed \oursolution maintains two separate COD sketches:
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt] 
  \item A {\em primary sketch} \((\hat{\boldsymbol{A}}, \hat{\boldsymbol{B}})\) along with its snapshot queue \(S\), which summarizes data within the current sliding window.
  \item An {\em auxiliary sketch} \((\hat{\boldsymbol{A}}', \hat{\boldsymbol{B}}')\) with its snapshot queue \(S'\), which accumulates the most recent updates.
\end{itemize}

Algorithm~\ref{alg:initialize} provides the pseudo-code for initializing the two sketches. It sets both the primary and auxiliary sketches to zero matrices and initializes the queue as empty. 




\htitle{Update Algorithm.} When a new pair \((\boldsymbol{x}_i,\boldsymbol{y}_i)\) arrives, we update both sketches using the COD routine. The update procedure, detailed in Alg.\ \ref{alg:simple-update}, involves three main steps:

  
  {\em (1) Window refresh} (Lines 1-4): Every \(N\) timestamps, the auxiliary sketch and its snapshots replaces that of the primary sketch to discard expired data outside the current sliding window. This swap mitigates the accumulated influence of outdated data outside the sliding window; see the theoretical analysis for details.
  
  {\em (2) Snapshot expiration} (Lines 5-6): It removes any snapshots from both the primary queue \(S\) and the auxiliary queue \(S'\) that have fallen outside the window.
  
  {\em (3) Registering dominant directions} (Lines 7-12): After updating a sketch, we check if the product of the norms of its first (i.e., most significant) column vectors exceeds \(\theta\). When 
  \(
  \|\hat{\boldsymbol{a}}_1\|_2 \cdot \|\hat{\boldsymbol{b}}_1\|_2 \ge \theta,
  \)
  a snapshot capturing these vectors (and the current timestamp information) is appended to the appropriate queue, and the corresponding column is removed from the sketch. This is the core mechanism of our method: only when a co-occurring direction is sufficiently significant do we register it.



\htitle{Complexity Analysis.} Next, we analyze the running cost for \oursolution. For each incoming pair $(\boldsymbol{x}_i,\boldsymbol{y}_i)$, we need to feed them to the COD algorithm (Line 7 of Alg.\ \ref{alg:simple-update}). It needs to perform Lines 6-12 in Alg.\ \ref{alg:cod} every time to obtain the singular values of $\boldsymbol{R}_X\boldsymbol{R}_Y^T$. Performing QR decomposition on $\hat{\boldsymbol{A}}$ and $\hat{\boldsymbol{B}}$ takes $O(d_xl^2)$ and $O(d_yl^2)$ time, respectively. And then computing $\boldsymbol{R}_X\boldsymbol{R}_Y^T$ and performing SVD on matrix $\boldsymbol{R}_X\boldsymbol{R}_Y^T$ cost $O(l^3)$ time. Thereby, the computational bottleneck of the algorithm is the QR decomposition performed within each COD update on the \(d_x \times l\) and \(d_y \times l\) matrices. Hence, each update runs in \(O((d_x+d_y)l^2)\) time.

For space cost, by setting the sketch size $l = min(\lceil \frac{1}{\epsilon}\rceil,d_x,d_y)$, the memory cost of the two COD sketches is bounded by $O(\frac{d_x+d_y}{\epsilon})$. Additionally, setting the register threshold $\theta = \epsilon N$ results in the number of snapshots being $O(\frac{1}{\epsilon})$. In total, the space cost for the whole DS-COD sketch is $O(\frac{d_x+d_y}{\epsilon})$. For the choice of $l$ and $\theta$, we will include more discussions in our later analysis. 



\subsection{A Fast Update Algorithm}


A major computational bottleneck in the basic \oursolution update (see Alg.\ \ref{alg:simple-update}) is the repeated full matrix decomposition, which costs \(O((d_x+d_y)l^2)\) per update. In fact, the original COD method defers a full decomposition until the sketch accumulates \(l\) columns, resulting in an amortized cost of \(O((d_x+d_y)l)\) per column. However, in our sliding window scenario, we must process every column to ensure that no significant direction is missed.

We observe that the COD analysis only requires the maintained matrices \(\boldsymbol{Q}_X\) and \(\boldsymbol{Q}_Y\) to be orthonormal; the specific upper triangular structure of \(\boldsymbol{R}_X\) and \(\boldsymbol{R}_Y\) is not essential. Motivated by this observation, we aim to incrementally maintain the decomposition by ensuring that \(\boldsymbol{Q}_X\) and \(\boldsymbol{Q}_Y\) remain orthonormal without recomputing a full decomposition from scratch each time. This leads to our novel \emph{FastUpdate Algorithm} (Alg.\ \ref{alg:fast-update}), which avoids unnecessary full decompositions by updating the existing orthonormal basis incrementally, in a manner akin to the Gram–Schmidt process.

To design this strategy, we additionally maintain four matrices:
\(
\boldsymbol{Q}_X,\ \boldsymbol{R}_X\) and \(\boldsymbol{Q}_Y,\ \boldsymbol{R}_Y,
\)
which capture the decomposition of the primary sketches \(\hat{\boldsymbol{A}}\) and \(\hat{\boldsymbol{B}}\), respectively.


\input{algorithms/alg-4-incremental_QR}


\htitle{Incremental Decomposition.}  When a new vector \(\boldsymbol{x}\) (or \(\boldsymbol{y}\)) arrives, we update the current decomposition using the procedure described in Alg.\ \ref{alg:ID} (IncDec). Briefly, we compute the residual:
\(
\boldsymbol{x}' = \boldsymbol{x} - \sum_{i=1}^l \langle \boldsymbol{q}_i,\boldsymbol{x} \rangle\, \boldsymbol{q}_i,
\)
where \(\{\boldsymbol{q}_i\}_{i=1}^l\) are the columns of the current basis \(\boldsymbol{Q}\) (see Alg.\ \ref{alg:ID}, Line~1). Normalizing \(\boldsymbol{x}'\) produces a new unit vector orthogonal to the existing basis, and the inner products \(\langle \boldsymbol{q}_i,\boldsymbol{x} \rangle\) form a vector \(\boldsymbol{v}\) that updates the upper triangular matrix \(\boldsymbol{R}\) (Lines~2--3 of Alg.\ \ref{alg:ID}). This incremental update requires only \(O(dl)\) time per update, which is dramatically lower than a full decomposition.

\htitle{Fast Update Algorithm.} With the incremental decomposition procedure in place, we now present the complete details of our FastUpdate Algorithm in Alg.\ \ref{alg:fast-update}. The first two steps—window refresh and snapshot expiration—are identical to those in Alg.\ \ref{alg:simple-update}. For the update step, we first invoke Alg.\ \ref{alg:ID} to incrementally update and obtain the decompositions \(\boldsymbol{Q}_X\), \(\boldsymbol{R}_X\), \(\boldsymbol{Q}_Y\), \(\boldsymbol{R}_Y\) (Lines~9--10 in Alg.\ \ref{alg:fast-update}).



\input{algorithms/alg-5-fast-update}



Once these updated decompositions are available, we compute the product \(\boldsymbol{R}_X\boldsymbol{R}_Y^T\) and perform a singular value decomposition (SVD) on it (which takes \(O(l^3)\) time, as shown in Line~11). The SVD yields singular values \(\{\sigma_i\}\) and the associated singular vectors. We then check whether any singular value \(\sigma_i\) meets or exceeds the snapshot threshold \(\theta\). For each \(\sigma_i \ge \theta\), a snapshot is generated immediately by computing:
\(
\hat{\boldsymbol{a}}_i = \boldsymbol{Q}_X\,\boldsymbol{u}_i\sqrt{\sigma_i}\) and  \(\hat{\boldsymbol{b}}_i = \boldsymbol{Q}_Y\,\boldsymbol{v}_i\sqrt{\sigma_i},
\)
where \(\boldsymbol{u}_i\) and \(\boldsymbol{v}_i\) are the singular vectors corresponding to \(\sigma_i\), and it holds that
\(
\|\hat{\boldsymbol{a}}_i\|_2 \,\|\hat{\boldsymbol{b}}_i\|_2 = \sigma_i.
\)
This snapshot computation requires \(O((d_x+d_y)l)\) time. Note that our snapshot registration process differs from that in Alg.\ \ref{alg:cod}. Here, we must update \(\hat{\boldsymbol{A}}\), \(\hat{\boldsymbol{B}}\), \(\boldsymbol{R}_X\), and \(\boldsymbol{R}_Y\) so that the invariant
\(
\hat{\boldsymbol{A}} = \boldsymbol{Q}_X\boldsymbol{R}_X\) and \(\hat{\boldsymbol{B}} = \boldsymbol{Q}_Y\boldsymbol{R}_Y
\)
remains preserved after the update. Although this update strategy is different, we will show that it still produces the correct result. Finally, as detailed in Lines~15–19 of Alg.\ \ref{alg:fast-update}, the corresponding snapshot vectors are removed from \(\hat{\boldsymbol{A}}\), \(\hat{\boldsymbol{B}}\), \(\boldsymbol{R}_X\), and \(\boldsymbol{R}_Y\) using our incremental decomposition results; the correctness of these removals is supported by Lemma~\ref{lem:fast-equivalent}. Finally, we apply the same update strategy for the auxiliary sketch (Alg. \ref{alg:fast-update} Line 20).  

By replacing full decompositions with an efficient incremental update strategy, our FastUpdate algorithm achieves an amortized update cost of \(O((d_x+d_y)l + l^3)\) per column. This novel improvement retains key properties required for COD while substantially reducing computational overhead, making our approach particularly well-suited for high-dimensional streaming data.






\begin{lemma} \label{lem:fast-equivalent}
    if $\hat{\boldsymbol{A}}=\boldsymbol{Q}_X \boldsymbol{R}_X$, $\hat{\boldsymbol{B}}=\boldsymbol{Q}_Y\boldsymbol{R}_Y$, $(\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}) = SVD(\boldsymbol{R}_X\boldsymbol{R}_Y^T)$, $\hat{\boldsymbol{A}}{'}= \hat{\boldsymbol{A}} - \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X$, $\hat{\boldsymbol{B}}{'} = \hat{\boldsymbol{B}} - \boldsymbol{Q}_Y\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y$, $\boldsymbol{R}_X' = \boldsymbol{R}_X - \boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X$ and $\boldsymbol{R}_Y' = \boldsymbol{R}_Y - \boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y$, then $(\boldsymbol{Q}_X,\boldsymbol{R}_X')$ is an orthogonal decomposition of $\hat{\boldsymbol{A}}{'}$, $(\boldsymbol{Q}_Y,\boldsymbol{R}_Y')$ is the orthogonal decomposition of $\hat{\boldsymbol{B}}{'}$ and $(\hat{\boldsymbol{A}}{'}, \hat{\boldsymbol{B}}{'})$ is the same as removing $(\hat{a_i},\hat{b_i})$ from $(\hat{\boldsymbol{A}},\hat{\boldsymbol{B}})$, that is, $\hat{\boldsymbol{A}}{'}\hat{\boldsymbol{B}}{'}^T = \hat{\boldsymbol{A}}\hat{\boldsymbol{B}}^T - \hat{a_i}\hat{b_i}^T$.
\end{lemma}

All omitted proofs can be found in Appendix \ref{app:proofs}.



\htitle{Query algorithm.}
The query procedure for \oursolution\ operate through a direct aggregation of the COD sketch matrices and snapshots from the primary sketch. Specifically, the current sketches $(\hat{\boldsymbol{A}},\hat{\boldsymbol{B}})$ are horizontally concatenated with matrices $(\hat{\boldsymbol{C}},\hat{\boldsymbol{D}})$ stacked by the non-expiring snapshots, constructing an augmented representation that preserves historical data integrity within the window while incorporating current COD sketches.


\input{algorithms/alg-6-query}


\htitle{Theoretical Analysis.} To compute $\boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X$, we should first compute $\boldsymbol{Q}_X\boldsymbol{u}_i$ in $O(d_xl)$ time, $\boldsymbol{u}_i^T\boldsymbol{R}_X$ in $O(l^2)$ time and then multiply $\boldsymbol{Q}_X\boldsymbol{u}_i$ by $\boldsymbol{u}_i^T\boldsymbol{R}_X$ in $O(d_xl)$ time to obtain $\boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X$. Similarly, we can compute $\boldsymbol{Q}_Y\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y$ in $O(d_yl)$ time. For $\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X$, we can first calculate $\boldsymbol{u}_i^T\boldsymbol{R}_X$ in $O(l^2)$ time and then multiply $\boldsymbol{u}_i$ by $\boldsymbol{u}_i^T\boldsymbol{R}_X$ to attain $\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X$ in $O(l^2)$ time. We can also compute $\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y$ in $O(l^2)$ time. As a result, extracting a snapshot from $(\hat{\boldsymbol{A}},\hat{\boldsymbol{B}})$ takes $O((d_x+d_y)l)$ time. However, it is noteworthy that after executing Lines 17 and 18, $\boldsymbol{R}_X$ and $\boldsymbol{R}_Y$ are not necessarily upper triangular matrices. What can be guaranteed is that $(\boldsymbol{Q}_X,\boldsymbol{R}_X')$ and $(\boldsymbol{Q}_Y,\boldsymbol{R}_Y')$ are the orthogonal column decomposition of $\hat{\boldsymbol{A}}$ and $\hat{\boldsymbol{B}}$ respectively. However, as we mentioned earlier, a closer examination of the COD algorithm's proof in \cite{MrouehMG17} reveals that the upper triangular structure of $\boldsymbol{R}_X$ and $\boldsymbol{R}_Y$ is nonessential to the algorithm's validity. The algorithm's correctness hinges solely on $\boldsymbol{Q}_X$ and $\boldsymbol{Q}_Y$ being column orthogonal. Notably, our proposed Algorithm \ref{alg:ID} is designed to handle decompositions with non-triangular $\boldsymbol{R}_X$ and $\boldsymbol{R}_Y$. It remains valid for the pairs $(\boldsymbol{Q}_X,\boldsymbol{R}_X')$ and $(\boldsymbol{Q}_Y,\boldsymbol{R}_Y')$ as long as $\boldsymbol{Q}_X$ and $\boldsymbol{Q}_Y$ are column orthogonal. Consequently, decompositions $(\boldsymbol{Q}_X,\boldsymbol{R}_X')$ and $(\boldsymbol{Q}_Y,\boldsymbol{R}_Y')$ remain valid for the COD algorithm and Algorithm \ref{alg:ID}, regardless of the upper triangular structure of $\boldsymbol{R}_X$ and $\boldsymbol{R}_Y$.



Suppose that there exist $m$ singular values surpassing the threshold $\theta$ in Line 13 from the beginning to current timestamp $t$, aligned as $\sigma_1\geq \sigma_2 \geq \sigma_3 \geq \cdots \geq \sigma_m \geq \theta$, extracting these $m$ snapshots takes $O(m(d_x+d_y)l)$ time in total. Since $m\theta \leq \sum_{i=1}^m\sigma_i \leq \left\| \boldsymbol{A}_{1,t}\boldsymbol{B}_{1,t}^T\right\|_{*} \leq \sum_{i=1}^t \left\|\boldsymbol{x}_i\boldsymbol{y}_i^T\right\|_{*} \leq t$, we have $m \leq t/\theta$. As a result, it takes $O(\frac{t(d_x+d_y)l}{\theta})$ over $t$ timestamps, and the amortized time per column is $O(\frac{(d_x+d_y)l}{\theta})$. With $\theta = \epsilon N = N / l$, this amortized time is $O((d_x+d_y)l^2/N)$. Assuming $N = \Omega(l)$, this amortization for a single update remains $O((d_x+d_y)l + l^3)$, which is a rational assumption. Thus, compared to Alg.\ \ref{alg:simple-update} costing $O((d_x+d_y)l^2)$ for each update, Alg.\ \ref{alg:fast-update} only takes amortized $O((d_x+d_y)l + l^3)$ time for each update, which is faster.

Finally, we present the following theorem about the space cost, update cost and error guarantee of our \oursolution.

\begin{theorem}\label{thm:socod-normalized}
Let \(\{(\boldsymbol{x}_t,\boldsymbol{y}_t)\}_{t\ge1}\) be a stream of data with \(\|\boldsymbol{x}_t\|=\|\boldsymbol{y}_t\|=1\) for all \(t\), and let \(\boldsymbol{X}_W\) and \(\boldsymbol{Y}_W\) denote the sliding window matrices as defined in Definition~\ref{def:amm}. Given a window size \(N\) and relative error \(\epsilon\), the \oursolution algorithm outputs matrices \(\boldsymbol{A}_{aug}\in\mathbb{R}^{d_x\times O(\frac{1}{\epsilon})}\) and \(\boldsymbol{B}_{aug}\in\mathbb{R}^{d_y\times O(\frac{1}{\epsilon})}\) such that, if the register threshold is set to \(\theta = \epsilon N\) and 
\(
l = \min\Bigl(\Bigl\lceil \frac{1}{\epsilon}\Bigr\rceil,\, d_x,\, d_y\Bigr),
\)
then
\[
\Bigl\|\boldsymbol{X}_W\boldsymbol{Y}_W^\top - \boldsymbol{A}_{aug}\boldsymbol{B}_{aug}^\top\Bigr\|_2 \le 8\epsilon N = 8\epsilon\,\|\boldsymbol{X}_W\|_F\,\|\boldsymbol{Y}_W\|_F.
\]
Furthermore, the \oursolution sketch uses \(O\Bigl(\frac{d_x+d_y}{\epsilon}\Bigr)\) space and supports each update in \(O((d_x+d_y)l+l^3)\) time with Alg.\ \ref{alg:fast-update}.
\end{theorem}

