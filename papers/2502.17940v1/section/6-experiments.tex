\section{Experiments}
\label{sec:sec-Experiment}

In this section, we present a comprehensive evaluation of our proposed algorithms against three different baseline algorithms on both synthetic and real-world datasets. 



\subsection{Baseline Algorithms}


\begin{figure*}[t]
	\centering
		\begin{tabular}{cccc}
	 			\multicolumn{4}{c}{\hspace{-8mm} \includegraphics[height=2.8mm]{figures/socod_legend.eps}}   \\[-1mm]
                 \includegraphics[height=26mm]{figures/uniform-max-error.eps} &
			 \includegraphics[height=26mm]{figures/normal-max-error.eps} &
			\ \includegraphics[height=26mm]{figures/multi-modal-max-error.eps} &
			 \includegraphics[height=26mm]{figures/hpmax-max-error.eps}
			\\[-3mm]
             (a) Uniform Random &
                 (b) Random Noisy &
			 (c) Multimodal Data &
			 (d) HPMaX  \\[-1mm]
		\end{tabular}
		\vspace{-3mm}
		\caption{Maximum Sketch Size vs. Maximum Error.} \label{fig:max-error}
		\vspace{-1mm}
\end{figure*}

\begin{figure*}[t]
	\centering
 \vspace{-2mm}
		\begin{tabular}{cccc}
	 	%		\multicolumn{4}{c}{\hspace{-8mm} \includegraphics[height=2.8mm]{figures/socod_legend.pdf}}   \\
			\includegraphics[height=26mm]{figures/uniform-avg-error.eps} &
             \includegraphics[height=26mm]{figures/normal-avg-error.eps} &
			 \includegraphics[height=26mm]{figures/multi-modal-avg-error.eps} &
			 \includegraphics[height=26mm]{figures/hpmax-avg-error.eps}
			\\[-3mm]
                 (a) Uniform Random &
                (b) Random Noisy &
			 (c) Multimodal Data &
			 (d) HPMaX  \\[-1mm]
		\end{tabular}
		\vspace{-3mm}
		\caption{Maximum Sketch Size vs. Average Error.} \label{fig:avg-error}
		% \vspace{-1mm}
\end{figure*}

\begin{figure*}[t]
	\centering
 \vspace{-2mm}
		\begin{tabular}{cccc}
	 	%		\multicolumn{4}{c}{\hspace{-8mm} \includegraphics[height=2.8mm]{figures/socod_legend.pdf}}   \\
			 \includegraphics[height=26mm]{figures/uniform-sketch-size.eps} &
            		 \includegraphics[height=26mm]{figures/normal-sketch-size.eps} &
			 \includegraphics[height=26mm]{figures/multi-modal-sketch-size.eps} &
			 \includegraphics[height=26mm]{figures/hpmax-sketch-size.eps}
			\\[-3mm]
             (a) Uniform Random &
                (b) Random Noisy &
			 (c) Multimodal Data &
			(d) HPMaX  \\[-1mm]
		\end{tabular}
		\vspace{-2mm}
		\caption{$\log_{10}(1/\epsilon)$ vs. Maximum Sketch Size ($\log_{10}(\frac{1}{0.25}) \approx 0.6$, \text{and} $\log_{10}(\frac{1}{0.016}) \approx 1.8$).}\label{fig:sketch-size}
		\vspace{-1mm}
\end{figure*}

\htitle{Sampling method.} For the AMM problem, the algorithm samples a small proportion of the matrices. Specifically, each pair of columns $(x_i,y_i)$ is assigned to a priority $\rho=u^{1/(\xinorm\yinorm)}$, where $u$ is uniformly sampled from the interval $(0,1)$ \cite{efraimidis2006weighted}. This priority-based sampling strategy ensures that columns with larger norms are more likely to be selected, thereby preserving the most significant contributions to the matrix product. To achieve an $\epsilon$-approximation guarantee, the algorithm requires $O(\frac{1}{\epsilon^2})$ independent samples selected based on the highest priorities \cite{drineas2006fast, efraimidis2006weighted}. To extend the priority sampling on the sliding window, we use the technique from \cite{babcock2001sampling}, leading to a space complexity of $O(\frac{d_x+d_y}{\epsilon^2}\log{N})$ for the normalized model and $O(\frac{d_x+d_y}{\epsilon^2}\log{NR})$ for general unnormalized model.

\htitle{DI-COD.} DI-COD applied the Dyadic Interval approach \cite{arasu2004approximate} to Co-Occurring Directions, maintaining a hierarchical structure with $L=\log{\frac{R}{\epsilon}}$ parallel levels, each of which contains a dynamic number of blocks. For $i$-th level, the window is segmented into at most $2^{L-i+1}$ block, and each block maintains a COD sketch. The space cost for DI-COD is $O(\frac{(d_x+d_y)R}{\epsilon}\log^2{\frac{R}{\epsilon}})$. 

\htitle{EH-COD.} Exponential Histogram Co-occurring Directions (EH-COD) combines the Exponential Histograms technique \cite{DatarGIM02} and incorporates the COD algorithm for efficiently approximating matrix multiplication within the sliding window model. The space cost for EH-COD is $O(\frac{d_x+d_y}{\epsilon^2}\log{\epsilon NR})$.



\subsection{Experiments Setup}
\htitle{Datasets.} Experiments are conducted on both synthetic and real-world datasets widely used in matrix multiplication \cite{YaoLCWC24,YeLZ16,MrouehMG17,GhashamiDP14,KangKK20}. All datasets are \emph{unnormalized}.
The details are listed below:
\begin{itemize}[leftmargin=10pt]
\item \textbf{Uniform Random \cite{YaoLCWC24,YeLZ16}.} We generate two random matrices: one of size $2000 \times 10000$ and another of size $1000 \times 10000$. The entries of both matrices are drawn uniformly at random from the interval $[0, 1)$. The window size for this dataset is $N = 4000$.
    
\item \textbf{Random Noisy \cite{MrouehMG17,GhashamiDP14}.} 
We generate the input matrix $\boldsymbol{X}^T = \boldsymbol{SDU} + \boldsymbol{F} / \zeta \in \mathbb{R}^{n \times d_x}$. Here, the term $\boldsymbol{SDU}$ represents an $m$-dimensional signal, while the other part $\boldsymbol{F} / \zeta$ is a Gaussian noise matrix, with scalar parameter $\zeta$ controlling the noise-to-signal ratio.
Specifically, $\boldsymbol{S}\in\mathbb{R}^{n\times m}$ is a random matrix where each entry is drawn from a standard normal distribution. $\boldsymbol{D}\in\mathbb{R}^{m\times m}$ is a diagonal matrix with entries $\boldsymbol{D}_{i,i}=1-(i-1)/m$, and $\boldsymbol{U}\in\mathbb{R}^{m\times d_x}$ is a random rotation which represents the row space of the signal and satisfies that $\boldsymbol{U}^T\boldsymbol{U}=I_m$. $\boldsymbol{F}$ is again a Gaussian matrix with each entries generated i.i.d. from a normal distribution $N(0,1)$. Matrix $\boldsymbol{Y}$ is generated in the same manner as $\boldsymbol{X}$. We set $d_x = 2000$, $d_y=1000$, $m = 400$, $\zeta = 100$, and the window size $N = 4000$.

 \item \textbf{Multimodal Data \cite{MrouehMG17}.} We study the empirical performance of the algorithms in approximating correlation between images and captions. Following \cite{MrouehMG17}, we consider Microsoft COCO dataset \cite{LinMBHPRDZ14}. For visual features we use the residual CNN Resnet101 \cite{HeZRS16} to generate a feature vector of dimension $d_x = 2048$ for each picture. For text we use the Hierarchical Kernel Sentence Embedding \cite{mroueh2015asymmetrically}, resulting in a feature vector of dimensions $d_y = 3000$. We construct the matrices $\boldsymbol{X}$ and $\boldsymbol{Y}$ with sizes $2048 \times 123287$ and $3000 \times 123287$, respectively, where each column represents a feature vector. The window size is set to $N = 10000$.

 \item \textbf{HPMaX \cite{KangKK20}:} We also include the dataset HPMaX, which is used to test the performance of heterogenous parallel algorithms for matrix multiplication. In this dataset, both of $\boldsymbol{X}$ and $\boldsymbol{Y}$  have size of $16384\times 32768$. The window size $N$ is $10000$.
\end{itemize}


\htitle{Evaluation Metrics.} Recall that our \oursolution achieves the optimal space complexity while providing an $\epsilon$-approximation guarantee. Therefore, we design the experiments to explicitly demonstrate the trade-off between space consumption and empirical accuracy across different datasets. Specifically, we tune the parameters of each algorithm and report both the maximum sketch size and the empirical relative correlation error.  
\begin{itemize}[topsep=0.5mm, partopsep=0pt, itemsep=0pt, leftmargin=10pt] 
    \item \textbf{Maximum sketch size}. This metric is measured by the \textit{maximum} number of column vectors maintained by a matrix sketching algorithm. The maximum sketch size metric represents the peak space cost of a matrix sketching algorithm. 
    \item \textbf{Relative correlation error}. This metric is used to assess the approximation quality of the output matrices. It is defined as $\left\| \boldsymbol{X}_W \boldsymbol{Y}_W^T - \boldsymbol{A}_W \boldsymbol{B}_W^T\right\|_2 /\left\|\boldsymbol{X}_W\right\|_F\left\|\boldsymbol{Y}_W\right\|_F$, where $\boldsymbol{X}_W$ and $\boldsymbol{X}_W$ denotes the exact matrices covered by the current window, and $\boldsymbol{A}_W$ and $\boldsymbol{B}_W$ denotes sketch matrices for $\boldsymbol{X}_W\boldsymbol{Y}_W^T$. 
\end{itemize} 





\subsection{Experimental Results}
We first adjust the error parameter $\epsilon$ for each algorithm to analyze the trade-off between space efficiency and empirical accuracy. Generally, when the error parameter $\epsilon$ decreases, the maximum sketch size increases. As shown in Figures~\ref{fig:max-error}--\ref{fig:avg-error}, we report the maximum sketch size, as well as the maximum and average relative correlation errors, for each algorithm. Both the x-axis and y-axis are displayed on a logarithmic scale to encompass the wide range of values. 

First, we observe that the curve representing our solution \oursolution consistently resides in the lower-left corner compared to other baselines, in terms of both maximum and average errors.  This implies that for a given space cost (i.e., maximum sketch size), our \oursolution consistently produces matrices with much lower correlation errors. Therefore, our solution demonstrates a superior space-error trade-off, aligning with its optimal space complexity as discussed in Section~\ref{sec:unnormalized-setting}. Second, on certain datasets (e.g., Multimodal Data and HPMax), the second-best algorithm, EH-COD, produces results comparable to our solution when the maximum sketch size is small (i.e., the error parameter $\epsilon$ is large). However, the gap between the two curves widens as the maximum sketch size increases (i.e., the error parameter $\epsilon$ decreases). This is also aligned with the theoretical result that the suboptimal space complexity $O(\frac{d_x + d_y}{\epsilon^2}\log{\epsilon NR})$ of EH-COD is outperformed by our optimal complexity $O(\frac{d_x + d_y}{\epsilon}\log{R})$. Finally, we observe that the EH-COD baseline performs better than the DI-COD baseline in almost all cases, which aligns with the observations in \cite{YaoLCWC24}.

Then, we examine the impact of the error parameter on the space cost of each algorithm. We vary the parameter $\epsilon$ and report the maximum value of sketch size. The results are shown in Figure~\ref{fig:sketch-size}.  The curve of our solution \oursolution consistently remains the lowest. This indicates that, for a given error parameter, \oursolution requires the least space, thereby confirming the conclusion of space optimality. One may note that as $\log_{10}(1/\epsilon)$ increases, the space growth of the EH-COD algorithm gradually slows down. This occurs because, as $\epsilon$ decreases, the storage capacity of EH-COD increases, and the entire sketch becomes sufficient to store the entire window without significant COD compression operations. Consequently, the maximum sketch size approaches the window size.


In summary, when space is the primary concern, our \oursolution is the preferred choice, delivering the best accuracy under space constraints compared to all competitors.
