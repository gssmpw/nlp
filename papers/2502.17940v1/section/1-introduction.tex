\section{Introduction}
\label{sec:sec-Introduction}

Matrix multiplication is a core operation across machine learning, data analysis, signal processing, and computer graphics. However, as data volumes grow, exact matrix multiplication becomes increasingly expensive, especially in large-scale scenarios demanding real-time or near-real-time processing. To address these challenges, approximate matrix multiplication (AMM) has emerged as an attractive alternative, offering substantial reductions in computational and storage overhead while preserving high accuracy.


In real-world applications, data often arrives in a streaming fashion, as exemplified by social media analytics (where posts or tweets are continuously generated), user behavior analysis (where search queries or web content are continuously updated), and financial market monitoring (where new transactions or price quotes are streamed in). This continuous flow of data has spurred interest in extending Approximate Matrix Multiplication (AMM) algorithms to streaming models \cite{MrouehMG17, YeLZ16}. Among various techniques, from sampling \cite{DrineasKM06} and random projections \cite{CohenNW16,MagenZ11,Sarlos06} to hashing \cite{ClarksonW13}, Co-occurring Directions (COD) \cite{MrouehMG17} has gained prominence for its robust approximation guarantee and strong empirical performance. 

However, a key limitation of traditional streaming methods is that they treat all incoming data uniformly, without prioritizing more recent information. In many real-world applications, the most valuable insights stem from the most recent data, necessitating a focus on a sliding window of the most current columns rather than retaining the entire historical dataset. For instance, in social media analysis, matrix \( \boldsymbol{X} \) could represent a stream of recent tweets, while matrix \(\boldsymbol{Y} \) could represent user interests. The multiplication \( \boldsymbol{X}  \boldsymbol{Y}^T \) captures the relevance of each tweet to user preferences. Similarly, in user behavior analysis, matrix \( \boldsymbol{X} \) might represent recent search queries, and matrix \( \boldsymbol{Y} \) could represent the relevance of advertisements or content. By using a sliding window, the approach prioritizes recent data, ensuring that the most current user activities influence the recommendations. Thus, matrix multiplication over a sliding window in a streaming setting is a natural choice, as it aligns with the data evolving nature and emphasizes the timeliness of insights, which is crucial for real-world applications.



To address this challenge, a line of research has focused on developing more efficient sketches that use less space while maintaining the same approximation guarantees. For example, by integrating techniques for sliding-window sampling with sampling-based matrix multiplication (see \cite{efraimidis2006weighted, drineas2006fast, babcock2001sampling}), we can discard outdated samples and draw new ones from the most recent rows or columns of the input matrix. However, the space cost of the sampling-based method remains relatively high, scaling with \( O\left(\frac{1}{\epsilon^2} \log N\right) \), where \( N \) is the number of rows and columns involved in the multiplication, and \( \epsilon \) is the relative error guarantee for approximate matrix multiplication (refer to Def.\ \ref{def:amm}). 
Yao et al.~\cite{YaoLCWC24} present two algorithms, EH-COD and DI-COD, which reduce the dependence on \( N \). However, they still incur a space cost that involves either a quadratic dependence on \( \frac{1}{\epsilon} \) or an extra factor of \( \log^2\left(\frac{1}{\epsilon}\right) \).
Thus, although progress has been made, the space complexity of existing sketching methods remains suboptimal, indicating room for further improvement.


Motivated by these limitations, we introduce {\em \oursolution} (\underline{S}pace \underline{O}ptimal \underline{C}o-\underline{O}ccuring \underline{D}irections), a novel algorithm specifically designed for approximate matrix multiplication in the sliding window setting. By maintaining a compact yet representative summary of the most recent data, {\oursolution} not only achieves optimal space complexity—matching the best-known theoretical bounds from the streaming literature—but also effectively meets the demands of time-sensitive applications.


At a high level, our approach draws inspiration from the classic sliding window frequency estimation problem \cite{LeeT06}, where one estimates the frequency of individual elements over the most recent \(N\) items by recording snapshot events when an element’s count reaches a threshold and expiring outdated snapshots as the window slides. In our work, we extend this snapshot concept to the significantly more challenging task of approximate matrix multiplication (AMM) for the matrices \(\boldsymbol{X} \in \mathbb{R}^{d_x \times n}\) and \(\boldsymbol{Y} \in \mathbb{R}^{d_y \times n}\). Rather than tracking individual elements, we register the key directions that contribute substantially to the product \(\boldsymbol{X}\boldsymbol{Y}^T\) as snapshots, and we expire these snapshots once they fall outside the sliding window. This extension is non-trivial since matrix multiplication involves complex interactions that far exceed the simplicity of frequency estimation. It is crucial to emphasize that, although Yin et al.\ \cite{YinWLWZHL24} also utilize a \(\lambda\)-snapshot framework, their work is confined to approximating covariance matrices (i.e., \(\boldsymbol{A}\boldsymbol{A}^T\)), which represents only a special case of the AMM problem. In contrast, our method is designed to address the full generality of AMM, overcoming challenges that their approach cannot.

We consider two settings. First, when every column of \(\boldsymbol{X}\) and \(\boldsymbol{Y}\) is normalized (i.e., each column has an \(L_2\) norm of 1), our method requires space \(O\left(\frac{d_x+d_y}{\epsilon}\right)\). We then extend our solution to the unnormalized case, where the squared \(L_2\) norm of each column lies within \([1, R]\), and demonstrate that the space cost increases to \(O\left(\frac{d_x+d_y}{\epsilon}\log R\right)\). In both cases, we prove that these bounds are tight by matching known space lower bounds for an \(\epsilon\)-approximation guarantee (see Def.\ \ref{def:amm}). Table \ref{tab:table-comparsion} summarizes the space complexity of {\oursolution} compared to existing solutions, clearly demonstrating its advantage. We evaluate the performance of {\oursolution} with extensive experiments on both synthetic and real-world datasets, showing its accuracy and space efficiency in comparison to existing methods. The results confirm that {\oursolution} effectively balances the space cost and approximation errors, making it a practical choice for large-scale streaming matrix multiplication in dynamic environments.


































