\appendix
\section{Proofs}\label{app:proofs}

\subsection{Proof of Lemma \ref{lem:fast-equivalent}.} 
We note that:

\begin{align}
    \hat{\boldsymbol{A}}{'} & = \hat{\boldsymbol{A}} - \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X \nonumber \\
    & = \boldsymbol{Q}_X(\boldsymbol{R}_X - \boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X) = \boldsymbol{Q}_X \boldsymbol{R}_X' \nonumber  \\
    \hat{\boldsymbol{B}}{'} & = \hat{\boldsymbol{B}} - \boldsymbol{Q}_Y\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y \nonumber \\
    & = \boldsymbol{Q}_Y(\boldsymbol{R}_Y - \boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y) = \boldsymbol{Q}_Y\boldsymbol{R}_Y' \nonumber
\end{align}
Then, we further have that:
\begin{align}
    \hat{\boldsymbol{A}}{'}{\hat{\boldsymbol{B}}{'}}^T & = (\hat{\boldsymbol{A}} - \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X)(\hat{\boldsymbol{B}} - \boldsymbol{Q}_Y\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{R}_Y)^T \nonumber \\ 
    & = \hat{\boldsymbol{A}}\hat{\boldsymbol{B}}^T - \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X\hat{\boldsymbol{B}}^T - \hat{\boldsymbol{A}}\boldsymbol{R}_Y^T\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    &\ \ \ \ + \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X\boldsymbol{R}_Y^T\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    & = \hat{\boldsymbol{A}}\hat{\boldsymbol{B}}^T - \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X\boldsymbol{R}_Y^T\boldsymbol{Q}_Y^T - \boldsymbol{Q}_X \boldsymbol{R}_X
    \boldsymbol{R}_Y^T\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    &\ \ \ \ + \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^T\boldsymbol{R}_X\boldsymbol{R}_Y^T\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    & = \hat{\boldsymbol{A}}\hat{\boldsymbol{B}}^T - \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^TU\Sigma V^T\boldsymbol{Q}_Y^T - \boldsymbol{Q}_XU\Sigma V^T\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    &\ \ \ \ + \boldsymbol{Q}_X\boldsymbol{u}_i\boldsymbol{u}_i^TU\Sigma V^T\boldsymbol{v}_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    & = \hat{\boldsymbol{A}}\hat{\boldsymbol{B}}^T - \boldsymbol{Q}_X\boldsymbol{u}_i\sigma_i\boldsymbol{v}_i^T\boldsymbol{Q}_Y^T \nonumber \\
    & = \hat{\boldsymbol{A}}\hat{\boldsymbol{B}}^T - \hat{a_i}\hat{b_i}^T \nonumber
\end{align}
This finishes the proof. 

\subsection{Proof of Theorem \ref{thm:socod-normalized}.}

We denote the matrices of snapshot vectors generated between timestamps $a$ and $b$ as $\boldsymbol{C}_{a,b}$ and $\boldsymbol{D}_{a,b}$. Specifically, $\boldsymbol{C}_{a,b}$ if formed by stacking $s_j.u$ for all $s_j.t\in [a,b]$ and $\boldsymbol{D}_{a,b}$ is formed by stacking $s_j.v$ for all $s_j.t\in [a,b]$. Similarly, let $\boldsymbol{X}_{a,b}$ and $\boldsymbol{Y}_{a,b}$ represent the matrices stacked by all $\boldsymbol{x}_i$ and $\boldsymbol{y}_i$, respectively, where $i\in[a,b]$. We denote $P$ as the moment right before the primary sketch begins to receive updates. We have $P = (k-1)N$, where $k = \max(1,\lfloor (T-1)/N\rfloor)$. And we define $(\hat{\boldsymbol{A}}_T,\hat{\boldsymbol{B}}_T)$ as the primary COD sketch at moment $T$. Next, we have

\begin{align}
    & \left\| \boldsymbol{X}_W\boldsymbol{Y}_W^T - \boldsymbol{A}_{aug}\boldsymbol{B}_{aug}^T\right\|_2 \nonumber \\
    & = \left\| \boldsymbol{X}_{T-N+1,T}\boldsymbol{Y}_{T-N+1,T}^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T - \boldsymbol{C}_{T-N+1,T}\boldsymbol{D}_{T-N+1,T}^T\right\|_2 \nonumber \\
    & = || \boldsymbol{X}_{P,T}\boldsymbol{Y}_{P,T}^T - \boldsymbol{X}_{P,T-N}\boldsymbol{Y}_{P,T-N}^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T \nonumber \\
    & - \boldsymbol{C}_{P,T}\boldsymbol{D}_{P,T}^T + \boldsymbol{C}_{P,T-N}\boldsymbol{D}_{P,T-N}^T ||_2 \nonumber \\
    & \leq \left\| \boldsymbol{X}_{P,T}\boldsymbol{Y}_{P,T}^T - \boldsymbol{C}_{P,T}\boldsymbol{D}_{P,T}^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T \right\| + \nonumber \\
    & \left\| \boldsymbol{X}_{P,T-N}\boldsymbol{Y}_{P,T-N}^T - \boldsymbol{C}_{P,T-N}\boldsymbol{D}_{P,T-N}^T \right\|_2 \nonumber
\end{align}

\begin{align}
    & = \left\| [\boldsymbol{X}_{P,T},-\boldsymbol{C}_{P,T}][\boldsymbol{Y}_{P,T},\boldsymbol{D}_{P,T}]^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T \right\| \nonumber \\
    & + \left\| \boldsymbol{X}_{P,T-N}\boldsymbol{Y}_{P,T-N}^T - \boldsymbol{C}_{P,T-N}\boldsymbol{D}_{P,T-N}^T - \hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T + \hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T \right\|_2 \nonumber \\
    & \leq \left\| [\boldsymbol{X}_{P,T},-\boldsymbol{C}_{P,T}][\boldsymbol{Y}_{P,T},\boldsymbol{D}_{P,T}]^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T \right\| + \left\|\hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T \right\|_2 \nonumber \\
    & + \left\| [\boldsymbol{X}_{P,T-N},- \boldsymbol{C}_{P,T-N}][\boldsymbol{Y}_{P,T-N},\boldsymbol{D}_{P,T-N}]^T  - \hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T\right\|  \nonumber \\
\end{align}

where $(\hat{\boldsymbol{A}}_{T-N},\hat{\boldsymbol{B}}_{T-N})$ and $(\hat{\boldsymbol{A}}_T,\hat{\boldsymbol{B}}_T)$ can be considered as the COD sketches of matrices $([\boldsymbol{X}_{P,T-N},- \boldsymbol{C}_{P,T-N}], [\boldsymbol{Y}_{P,T-N},\boldsymbol{D}_{P,T-N}])$ and $([\boldsymbol{X}_{P,T},-\boldsymbol{C}_{P,T}],$ $[\boldsymbol{Y}_{P,T},\boldsymbol{D}_{P,T}])$, respectively. Extracting a snapshot with vectors $(v,u)$ from $(\hat{\boldsymbol{A}},\hat{\boldsymbol{B}})$ can be seen as a COD sketch update with pair of columns $(-v,u)$. It is also known that $\left\|\boldsymbol{C}_{P,T} \right\|_F\left\|\boldsymbol{D}_{P,T} \right\|_F\leq \left\|\boldsymbol{X}_{P,T}\boldsymbol{Y}_{P,T}^T\right\|_*\leq \left\|\boldsymbol{X}_{P,T} \right\|_F\left\|\boldsymbol{Y}_{P,T} \right\|_F = T - P$. Similarly, we also have $\left\|\boldsymbol{C}_{P,T-N} \right\|_F\left\|\boldsymbol{D}_{P,T-N} \right\|_F\leq T - N - P$.

By the approximation error guarantee of COD \cite{MrouehMG17}, we have:
    \begin{align}
        & \left\| [\boldsymbol{X}_{P,T},-\boldsymbol{C}_{P,T}][\boldsymbol{Y}_{P,T},\boldsymbol{D}_{P,T}]^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T \right\|_2 \nonumber \\
        & \leq \epsilon \left\| [\boldsymbol{X}_{P,T},-\boldsymbol{C}_{P,T}]\right\|_F\left\| [\boldsymbol{Y}_{P,T},-\boldsymbol{D}_{P,T}]\right\|_F = 2\epsilon(T-P) \nonumber \\
        & \left\| [\boldsymbol{X}_{P,T-N},-\boldsymbol{C}_{P,T-N}][\boldsymbol{Y}_{P,T-N},\boldsymbol{D}_{P,T-N}]^T - \hat{\boldsymbol{A}}_{T-N}\hat{\boldsymbol{B}}_{T-N}^T \right\|_2 \nonumber \\
        & \leq \epsilon \left\| [\boldsymbol{X}_{P,T-N},-\boldsymbol{C}_{P,T-N}]\right\|_F\left\| [\boldsymbol{Y}_{P,T-N},-\boldsymbol{D}_{P,T-N}]\right\|_F = 2\epsilon(T-N-P) \nonumber \\
        & \left\|\hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T \right\|_2\leq \epsilon\left\|\boldsymbol{X}_{P,T-N} \right\|_F \left\|\boldsymbol{Y}_{P,T-N} \right\|_F = \epsilon(T - N - P)
    \end{align}
\noindent
Combining Inequality (2), the approximation error is bounded by

\begin{align}
    & \left\| \boldsymbol{X}_W\boldsymbol{Y}_W^T - \boldsymbol{A}_{aug}\boldsymbol{B}_{aug}^T\right\|_2 \nonumber \\
    & \leq \left\| [\boldsymbol{X}_{P,T},-\boldsymbol{C}_{P,T}][\boldsymbol{Y}_{P,T},\boldsymbol{D}_{P,T}]^T - \hat{\boldsymbol{A}}_T\hat{\boldsymbol{B}}_T^T \right\| + \left\|\hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T \right\|_2 \nonumber \\
    & + \left\| [\boldsymbol{X}_{P,T-N},- \boldsymbol{C}_{P,T-N}][\boldsymbol{Y}_{P,T-N},\boldsymbol{D}_{P,T-N}]^T  - \hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T\right\|  \nonumber \\
    & \leq 2\epsilon(T-P) + 2\epsilon(T-N-P) + \epsilon(T-N-P) \nonumber \\
    & \leq \epsilon(5T - 5P - 3N)  \leq 8\epsilon N
\end{align}

    Next, we prove the number of snapshots is bounded by $O(\frac{1}{\epsilon})$. Suppose that at time $T$ the queue has $w$ snapshots of vector pairs $(\boldsymbol{u}_1,\boldsymbol{v}_1),(\boldsymbol{u}_2,\boldsymbol{v}_2),\cdots,(\boldsymbol{u}_k,\boldsymbol{v}_k)$, and the corresponding singular values are $\sigma_1,\sigma_2,\cdots,\sigma_k$. Then we know $k\epsilon N\leq $$\sum_{i=1}^k\left\|\boldsymbol{u}_i\right\|_2\left\|\boldsymbol{v}_i\right\|_2$$=\sum_{i=1}^k\sigma_i\leq \left\|\hat{\boldsymbol{A}}_{T-N}{\hat{\boldsymbol{B}}_{T-N}}^T\right\|_{*}$$ + \left\|\boldsymbol{X}_W\boldsymbol{Y}_W^T\right\|_* \leq l\epsilon N + N = 2N$. Hence, we have $k\epsilon N \leq 2N$, which implies $k\leq \frac{2}{\epsilon}$. Thus, the space cost of \oursolution for normalized model is dominated by the COD sketch and snapshot storage, yielding a total space cost of $O((d_x+d_y)/\epsilon)$.

 \subsection{Proof of Theorem \ref{thm:socod-unnormalized}.}

    For $i$-th level of \newsolution, the correlation error is $2^{i+3}\epsilon N$ by the theorem \ref{thm:socod-normalized}. To ensure the correlation error $\left\| \boldsymbol{X}_W\boldsymbol{Y}_W^T - \boldsymbol{A}_{aug}\boldsymbol{B}_{aug}^T\right\|_2$ is bounded by $4\epsilon \left\| \boldsymbol{X}_W\right\|_F\left\| \boldsymbol{Y}_W\right\|_F$, we need to find the level $i$ such that $2^{i+3}\epsilon N \leq 4\epsilon \left\| \boldsymbol{X}_W\right\|_F\left\| \boldsymbol{Y}_W\right\|_F < 2^{i+4}\epsilon N $, thus we have

    \begin{align}
        log_2{\frac{\left\| \boldsymbol{X}_W\right\|_F\left\| \boldsymbol{Y}_W\right\|_F}{2N}} - 1 < i \leq  log_2{\frac{\left\| \boldsymbol{X}_W\right\|_F\left\| \boldsymbol{Y}_W\right\|_F}{2N}} \label{ine:level-i-range}
    \end{align}

    At the same time, we must ensure level $i$ contains sufficient snapshots. Suppose at time $T$, the snapshot queue $M[i].S$ holds $k$ snapshots $(u_1,v_1),(u_2,v_2),\cdots,(u_k,v_k)$. Given the register threshold $\theta = 2^i\epsilon N$, we have $\sum_{i=1}^k\left\|u_i \right\|_2\left\|v_i\right\|_2 \geq k\times2^i\epsilon N$. This sum consists of contributions from both the residue sketch matrix and the term $\sum_{i=T-N+1}^T \left\| \boldsymbol{x}_i \boldsymbol{y}_i^T\right\|_*$. From this, we derive the upper bound:
    
    \begin{align}
      &  k\times2^i\epsilon N  \leq \sum_{i=1}^k\left\|u_i \right\|_2\left\|v_i\right\|_2  \leq \sum_{i=T-N+1}^T \left\| \boldsymbol{x}_i \boldsymbol{y}_i^T\right\|_* + l2^i\epsilon N  \nonumber \\
        & = 2^iN + \sum_{i=T-N+1}^T \left\| \boldsymbol{x}_i\right\|_2 \left\| \boldsymbol{y}_i\right\|_2  \leq 2^iN + \left\| \boldsymbol{X}_W\right\|_F \left\| \boldsymbol{Y}_W\right\|_F \label{ine:snapshot-comtribute}
    \end{align}
    
    Combining (\ref{ine:level-i-range}) and (\ref{ine:snapshot-comtribute}), we deduce that $k\times2^i\epsilon N < 6\times 2^iN$, which implies $k<\frac{6}{\epsilon}$. As a result, storing at most $\frac{6}{\epsilon}$ snapshots per level suffices. Thus, \newsolution derives a space complexity of $O(\frac{d_x+d_y}{\epsilon}\log{R})$.
    


\subsection{Proof of Theorem \ref{thm:lower-bound-amm-sw}.} Without loss of generality, we suppose that $d_y\geq d_x$. We partition a window of size $N$ consisting of $(d_x+1)$-
dimensional $\boldsymbol{X}_W$ and $(d_y+1)$-dimensional $\boldsymbol{Y}_W$ into $\log{R}+ 2$ blocks, and we label the leftmost $\log{R}+1$ blocks as $\log{R},\cdots,2,1,0$ from left to right. The construction of these $\log{R}+2$ blocks are as follow: 

\begin{enumerate}[leftmargin=16pt]
    \item Choose $\log{R}+1$ matrix pairs from the set $\hat{\mathcal{Z}}_{l/4}$ from the lemma \ref{lem:cardinality}, and by setting $\delta = 1/8$, we have $M = 2^{\Omega(d_yl)}$ and $\left\|\boldsymbol{X}_i\boldsymbol{Y}_i^T-\boldsymbol{X}_j\boldsymbol{Y}_j^T\right\|_2 > 1/2$ for any $i\neq j$ and $i,j\in[1,M]$. Thereby the total number of distinct arrangements $L = \binom{2^{\Omega(d_yl)}}{\log{R}+1}$, consequently resulting that $\log{L} = \Omega(d_yl\log{R})$.

    \item For block $i$, multiply the chosen $(\boldsymbol{X}_i\in \mathbb{R}^{d_x\times (l/4)},\boldsymbol{Y}_i\in \mathbb{R}^{d_y\times (l/4)})$ by a scalar of $\sqrt{\frac{2^iN}{l}}$, making $\left\|\boldsymbol{X}_i\right\|_F\left\|\boldsymbol{Y}_i\right\|_F = 2^{i-2}N$.

    \item For block $i$ where $i>\log{\frac{lR}{N}}$, we replace the scalar $\sqrt{\frac{2^iN}{l}}$ with $\sqrt{R}$, and increases the number of columns from $l/4$ to $(l/4)\cdot 2^{i - \log{\frac{lR}{N}}}$, ensuring $1\leq\left\|\boldsymbol{x}_i\right\|_2^2, \left\|\boldsymbol{y}_i\right\|_2^2\leq R$. This configuration preserves the condition $\left\|\boldsymbol{X}_i\right\|_F\left\|\boldsymbol{Y}_i\right\|_F = 2^{i-2}N$. To bound the total number of columns by $N$, the condition $H = \frac{l}{4}\log{\frac{lR}{N}} +  \sum_{i = \log{\frac{lR}{N}}}^{\log{R}} \frac{l}{4}\cdot 2^{i - \log{\frac{lR}{N}}} \leq N$ must hold, implying $N \geq \frac{l}{2}\log{\frac{lR}{N}}$. 

    \item If the total number of columns $H$ of these blocks is less than $N$, to fill up the whole window, we set the last ($\log{R}+2$)-th block as all zeros matrix pair $(\boldsymbol{X}_{\log{R}+2},\boldsymbol{Y}_{\log{R}+2})=(0^{d_x\times(N-H)},0^{d_y\times(N-H)})$.

    \item Add one dimension to all matrices in all $\log{R}+2$ blocks and set the newly-added ($d_x+1$)-th dimension of each $\boldsymbol{X}_i$ and ($d_y+1$)-th dimension of each $\boldsymbol{Y}_i$ as $1$.
    
\end{enumerate}

We assume the algorithm receives one of these $L$ predefined arrangements of length $N$, followed by a sequence of $N$ one-hot vector pairs. In these pairs, only the ($d_x+1$)-th dimension in $\boldsymbol{X}$ and the ($d_y+1$)-dimension in $\boldsymbol{Y}$ are set to $1$. Let $(\boldsymbol{X}_W^i, \boldsymbol{Y}_W^i)$ denote the matrices over the sliding window of length $N$ when $\log{R},\cdots,i+1,i$ blocks have exactly expired. Assume the algorithm can provide AMM estimation $(\boldsymbol{A}_W^i, \boldsymbol{B}_W^i)$ and $(\boldsymbol{A}_W^{i-1}, \boldsymbol{B}_W^{i-1})$ for $(\boldsymbol{X}_W^i, \boldsymbol{Y}_W^i)$ and $(\boldsymbol{X}_W^{i-1}, \boldsymbol{Y}_W^{i-1})$, respectively, with a correlation coefficient $\frac{1}{6l}$, implying:
$$
\left\|\boldsymbol{X}_W^i {\boldsymbol{Y}_W^i}^T - \boldsymbol{A}_W^i {\boldsymbol{B}_W^i}^T\right\|_2
\leq \frac{1}{6l}
\left\|\boldsymbol{X}_W^i\right\|_F\left\|\boldsymbol{Y}_W^i\right\|_F = \frac{1}{6l}(\frac{N}{4}\cdot 2^{i+1}+\frac{3N}{4}),
$$

$$
\left\|\boldsymbol{X}_W^{i-1} {\boldsymbol{Y}_W^{i-1}}^T - \boldsymbol{A}_W^{i-1} {\boldsymbol{B}_W^{i-1}}^T\right\|_2
\leq \frac{1}{6l}
\left\|\boldsymbol{X}_W^{i-1}\right\|_F\left\|\boldsymbol{Y}_W^{i-1}\right\|_F = \frac{1}{6l}(\frac{N}{4}\cdot 2^{i}+\frac{3N}{4})
$$

Then We can compute the AMM for block $i$ using $(\boldsymbol{A}_i',\boldsymbol{B}_i')$, where $\boldsymbol{A}_i'{\boldsymbol{B}_i'}^T = \boldsymbol{A}_W^{i} {\boldsymbol{B}_W^{i}}^T - \boldsymbol{A}_W^{i-1} {\boldsymbol{B}_W^{i-1}}^T$ as described below:

\begin{align}
    & \left\|\boldsymbol{X}_i\boldsymbol{Y}_i^T- \boldsymbol{A}_i'{\boldsymbol{B}_i'}^T\right\|_2 \nonumber \\
    = & \left\|(\boldsymbol{X}_W^i {\boldsymbol{Y}_W^i}^T - \boldsymbol{A}_W^i {\boldsymbol{B}_W^i}^T) - (\boldsymbol{X}_W^{i-1} {\boldsymbol{Y}_W^{i-1}}^T - \boldsymbol{A}_W^{i-1} {\boldsymbol{B}_W^{i-1}}^T)\right\|_2 \nonumber \\
    \leq & \left\|\boldsymbol{X}_W^i {\boldsymbol{Y}_W^i}^T - \boldsymbol{A}_W^i {\boldsymbol{B}_W^i}^T\right\|_2 + \left\|\boldsymbol{X}_W^{i-1} {\boldsymbol{Y}_W^{i-1}}^T - \boldsymbol{A}_W^{i-1} {\boldsymbol{B}_W^{i-1}}^T\right\|_2 \nonumber \\
    \leq & \frac{1}{6l}(\frac{3}{4}\cdot 2^i+ \frac{3}{2}N) \leq \frac{1}{l}\left\|\boldsymbol{X}_i\right\|_F\left\|\boldsymbol{Y}_i\right\|_F \nonumber
\end{align}

The algorithm can estimate blocks for all layers $0\leq i\leq \log{R}$. By lemma \ref{lem:lower-bound-amm}, estimating of each block requires $\Omega((d_x+d_y)l)$ bits of space. Consequently, we establish the fundamental space complexity lower bound for any deterministic AMM algorithm over a sliding window as $\Omega(\frac{d_x+d_y}{\epsilon}\log{R})$. 

In the special case $X=Y$, the problem reduces to matrix sketching over a sliding window, for which the lower bound $\Omega(\frac{d}{\epsilon}\log{R})$ has been established in \cite{YinWLWZHL24}, consistent with our findings.

\section{Additional Examples}\label{app:examples}

Here, we provide an example of how the $\lambda$-snapshot method works for counting the number of 1-bits in a sliding window.
\begin{example}
Consider a bit stream 
\(
\{1,0,1,1,0,1,1,1,0,1,0\}
\)
with 11 elements and let \(\lambda=3\). The 1-bits occur at positions 1, 3, 4, 6, 7, 8, and 10; hence, the 3\textsuperscript{rd} and 6\textsuperscript{th} 1-bits (positions 4 and 8) are sampled. We partition the stream into \(\lambda\)-blocks of length 3:
\[
\text{Block 1: } [1,3],\quad \text{Block 2: } [4,6],\quad \text{Block 3: } [7,9],\quad \text{Block 4: } [10,12].
\]
(Note that Block 4 is still incomplete.) At time \(T=11\), assume a sliding window of size \(N=5\):
\(
W_{11} = [7,11].
\)
Within \(W_{11}\), the 1-bits occur at positions 7, 8, and 10. Block 2 lies entirely outside \(W_{11}\) and is dropped, and although Block 4 overlaps \(W_{11}\), it contains no sampled bit. Thus, only Block 3 (spanning 7–9 and containing the sampled 1-bit at position 8) remains registered in the queue \(Q\). Moreover, the 1-bit at position 10, arriving after the last sampled bit, yields a leftover count \(\ell=1\). The \(\lambda\)-snapshot estimate is hence
\(
v(S)=|Q|\cdot\lambda + \ell = 1\cdot3 + 1 = 4.
\)
Since the true count of 1-bits in \(W_{11}\) is 3, the error is \(4-3=1\).
\end{example}
