\section{Toolkit Usage}




%\subsection{Usage steps}


Figure~\ref{fig:recommendation_yamls} provides an overview of the three main steps for utilizing our toolkit, FairDiverse. We will describe each step of usage in detail. The detailed configuration file parameters can be found in~\url{https://xuchen0427.github.io/FairDiverse/}.

\subsection{Usage Steps}
\textbf{Step 1.} First, download the dataset you wish to test, as described in Section~\ref{sec:datasets}, and store it in the \texttt{/dataset} directory. Then, specify the parameters in \texttt{/properties/dataset/\{data\_name\}.yaml.}
Next, you need to review all default parameters for data processing, model hyperparameters, and evaluation settings.

\noindent\textbf{Step 2.} Then, you need to create your own configuration file, specifying the selected models and the log directory path. If you want to modify the default pipeline parameters, you can specify them directly in your own configuration file, which will override the values in the default configuration files.

\noindent\textbf{Step 3.} Enter \texttt{/fairdiverse} dictionary and execute the command

\begin{lstlisting}[style=shell]
python main.py --task "recommendation" 
--stage "in-processing" --dataset "steam" 
--train_config_file "In-processing.yaml"
\end{lstlisting}

\noindent%
The args should specify the task (recommendation/search), stage (pre-processing, in-processing, post-processing), dataset, and your custom configuration file as defined in Step 2. Finally, the evaluation results and item/user utility allocations will then be recorded in your specified log file.

\subsection{Usage Example}
Besides our provided \texttt{main.py} file and shell command, you can also utilize the following test codes to run the toolkit.

\noindent\textbf{Recommendation.}
Our repository includes an example dataset, Steam.\footnote{\url{http://cseweb.ucsd.edu/~wckang/Steam_games.json.gz}} We provide the simple code snippets for running the in-processing and post-processing models, which are listed below. The user needs to specify the chooses ``model,'' ``dataset'' and ``log\_name'' for training and testing. 

\begin{lstlisting}[language=Python]
from recommendation.trainer import RecTrainer

config = {'model': 'BPR', 'data_type': 'pair', 'fair-rank': True, 'rank_model': 'APR', 'use_llm': False, 'log_name': "test", 'dataset': 'steam'}

trainer = RecTrainer(train_config=config)
trainer.train()
\end{lstlisting}

\begin{lstlisting}[language=Python]
from recommendation.reranker import RecReRanker

config = {'ranking_store_path': 'steam-base-mf', 'model': 'CPFair', 'fair-rank': True, 'log_name': 'test', 'fairness_metrics': ["GINI"], 'dataset': 'steam'}

reranker = RecReRanker(train_config=config)
reranker.rerank()
\end{lstlisting}

\noindent\textbf{Search.}
Our repository includes a running example of the pre-processing models on the COMPAS dataset. We provide the simple code snippet for running the pre-processing models, listed as follows. One can set the ``preprocessing\_model'' field to any of the supported models: CIFRank, LFR, gFair and iFair. Each pre-processing model has its own config file under \texttt{search/properties/models}  which is automatically loaded based on your choice. 

\begin{lstlisting}[language=Python]
from search.trainer_preprocessing_ranker import RankerTrainer
 
config={"train_ranker_config": {"preprocessing_model": "iFair", "name": "Ranklib", "ranker": "RankNet", "lr": 0.0001, "epochs": 10}}
 
reranker=RankerTrainer(train_config=config)
reranker.train()
\end{lstlisting}

For the post-processing models, our repository also provides a running example on the ClueWeb09 dataset. The simple code snippet for running these models is shown as follows. %The user needs to specify ``model'' for training and testing.

\begin{lstlisting}[language=Python]
from search.trainer import SRDTrainer
   
config={'model':'xquad', 'dataset':'clueweb09', 'log_name': 'test', 'model_save_dir': "model/", 'tmp_dir': "tmp/", 'mode': "train",}
 
trainer = SRDTrainer(train_config=config)
trainer.train()
\end{lstlisting}



%, for the recommendation task, along with template configuration files for testing the toolkit. The toy dataset provided for the search task includes the COMPAS dataset \cite{bias2016there}. You can directly execute the provided shell command to run the test.