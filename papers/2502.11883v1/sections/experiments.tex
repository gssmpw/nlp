\begin{table*}[t]
%\setlength{\tabcolsep}{2pt}
%\small
\caption{Partial benchmark results for recommendation tasks on Steam datasets for different ranking sizes $K$. They are evaluated on BPR ranking models with in-processing fairness-aware and diversity-aware approaches. $\downarrow$ and $\uparrow$ indicate that a smaller or larger metric value, respectively, corresponds to better model performance. It is important to note that the reported results are based on default parameters. }
\label{tab:exp:rec_in_processing}
\small
\setlength{\tabcolsep}{1.2mm}
\begin{tabular}{l cccccc cccccc}
\toprule
\multicolumn{1}{@{}l}{\multirow{2}{*}{Models/Metric}} & \multicolumn{6}{c}{$K=10$} & \multicolumn{6}{c}{$K=20$} \\
\cmidrule(r){2-7}
\cmidrule{8-13}
& NDCG$\uparrow$ & MRR$\uparrow$ & HR$\uparrow$ & MMF$\uparrow$ & GINI$\downarrow$ & Entropy$\uparrow$  & NDCG$\uparrow$ & MRR$\uparrow$ & HR$\uparrow$ & MMF$\uparrow$ & GINI$\downarrow$ & Entropy$\uparrow$ \\ 
\midrule
Llama3-FairPrompts  & 0.0304 & 0.0565 & 0.0265 & 0.0364 & 0.7332 & 3.8800  & 0.0444 & 0.1083 & 0.0308 & 0.0980 & 0.6201 & 4.3738  \\
Qwen2-FairPrompts   & 0.0312 & 0.0546 & 0.0284 & 0.0324 & 0.7503 & 3.7629  & 0.0455 & 0.1070 & 0.0329 & 0.0871 & 0.6453 & 4.2590  \\
Mistral-FairPrompts & 0.0323 & 0.0559 & 0.0303 & 0.0315 & 0.7494 & 3.7751  & 0.0481 & 0.1132 & 0.0355 & 0.0861 & 0.6455 & 4.2602  \\
APR  & 0.2925 & 0.2934 & 0.4085 & 0.0324 & 0.7257 & 3.9513  & 0.3193 & 0.2999 & 0.5058 & 0.0590 & 0.6485 & 4.2997  \\
FairDual    & 0.3204 & 0.3073 & 0.4727 & 0.0330 & 0.7123 & 4.0301  & 0.3479 & 0.3136 & 0.5702 & 0.0563 & 0.6577 & 4.2745  \\
IPS  & 0.3073 & 0.3090 & 0.4213 & 0.0249 & 0.7314 & 3.9183  & 0.3347 & 0.3155 & 0.5196 & 0.0570 & 0.6517 & 4.2824  \\
Minmax-SGD  & 0.2672 & 0.2604 & 0.3961 & 0.0218 & 0.7501 & 3.7252  & 0.2958 & 0.2679 & 0.4991 & 0.0470 & 0.6936 & 3.9982  \\
SDRO & 0.3009 & 0.3056 & 0.4081 & 0.0350 & 0.7212 & 3.9754  & 0.3298 & 0.3124 & 0.5137 & 0.0619 & 0.6451 & 4.3156  \\
FairNeg     & 0.2964 & 0.2975 & 0.4125 & 0.0673 & 0.6671 & 4.2222  & 0.3208 & 0.3036 & 0.5020 & 0.0778 & 0.6158 & 4.4067  \\
FOCF & 0.2879 & 0.2879 & 0.4041 & 0.0294 & 0.7272 & 3.9460  & 0.3141 & 0.2942 & 0.4992 & 0.0579 & 0.6472 & 4.3086  \\
Reg  & 0.2979 & 0.2981 & 0.4162 & 0.0306 & 0.7270 & 3.9465  & 0.3245 & 0.3043 & 0.5127 & 0.0584 & 0.6497 & 4.2917  \\ 
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
%\setlength{\tabcolsep}{2pt}
%\small
 \caption{Partial benchmark results for recommendation tasks on ClueWeb datasets for different ranking sizes $K$. They can be evaluated using the shell command provided in our GitHub repository. It is important to note that the reported results are based on default parameters.}
\label{tab:exp:rec_post_processing}
\small
\setlength{\tabcolsep}{1.2mm}
\begin{tabular}{l cccccc cccccc}
\toprule
\multicolumn{1}{@{}l}{\multirow{2}{*}{Models/Metric}} & \multicolumn{6}{c}{$K=10$} & \multicolumn{6}{c}{$K=20$} \\
\cmidrule(r){2-7}
\cmidrule{8-13}
\multicolumn{1}{@{}l}{} & R-NDCG$\uparrow$ & u-loss$\downarrow$ & MMF$\uparrow$ & GINI$\downarrow$ & Entropy$\uparrow$ & MMR$\uparrow$  & R-NDCG$\uparrow$ & u-loss$\downarrow$ & MMF$\uparrow$ & GINI$\downarrow$ & Entropy$\uparrow$ & MMR$\uparrow$ \\
\midrule
CP-Fair & 0.9981 & 0.0035 & 0.2135 & 0.4424 & 4.8441 & 0.0196 & 0.9969 & 0.0055 & 0.2118 & 0.4349 & 4.9064 & 0.0301 \\
min-regularizer & 0.9272 & 0.1234 & 0.3984 & 0.1373 & 5.3796 & 0.2740 & 0.9359 & 0.0896 & 0.4004 & 0.1326 & 5.3818 & 0.2761 \\
RAIF & 0.9881 & 0.0233 & 0.2937 & 0.3293 & 5.0080 & 0.0248 & 0.9829 & 0.0302 & 0.3333 & 0.2585 & 5.1558 & 0.0358 \\
P-MMF & 0.9691 & 0.0536 & 0.3140 & 0.2792 & 5.1911 & 0.0685 & 0.9675 & 0.0482 & 0.3289 & 0.2429 & 5.2597 & 0.0987 \\
FairRec & 0.9529 & 0.1088 & 0.1866 & 0.5098 & 4.5372 & 0.0157 & 0.9497 & 0.0990 & 0.1779 & 0.5179 & 4.5234 & 0.0175 \\
FairRec+ & 0.9773 & 0.0540 & 0.1758 & 0.5254 & 4.5108 & 0.0119 & 0.9750 & 0.0510 & 0.1609 & 0.5463 & 4.4594 & 0.0134 \\
FairSync & 0.9816 & 0.0353 & 0.2477 & 0.3951 & 4.9152 & 0.0237 & 0.9785 & 0.0383 & 0.2553 & 0.3705 & 5.0165 & 0.0312 \\
TaxRank & 0.9438 & 0.1015 & 0.2421 & 0.3791 & 4.9846 & 0.0149 & 0.9303 & 0.1080 & 0.2907 & 0.3078 & 5.1287 & 0.0209 \\
Welf & 0.9668 & 0.0575 & 0.3216 & 0.2638 & 5.2254 & 0.0820 & 0.9682 & 0.0467 & 0.3322 & 0.2383 & 5.2674 & 0.1112 \\ \bottomrule
\end{tabular}
\end{table*}


\section{Benchmark Results Analysis}
In this section, we present an analysis of partial benchmark results derived from our toolkit FairDiverse. Note that our goal is not to compare different models but to highlight the analytical direction of these results, helping researchers interpret and understand the findings more effectively and efficiently.

\subsection{Recommendation} 

In recommendation, we primarily evaluate the performance of commonly used in-processing models, which integrate fairness constraints during training, and post-processing models, which adjust rankings after predictions to enhance fairness.

\noindent\textbf{In-processing models.}
Table~\ref{tab:exp:rec_in_processing} presents the performance of the implemented in-process fairness and diversity-aware models in terms of accuracy (NDCG, HR, MRR) and fairness/diversity (MMF, GINI, Entropy) under the default parameters of our toolkit. The benchmark results are obtained using the Steam dataset and BPR~\cite{BPR} ranking models. The fairness/diversity metric is calculated at the Steam game category level.

First, from Table~\ref{tab:exp:rec_in_processing}, we observe that LLM-based models generally exhibit higher fairness and diversity but lower ranking performance. In contrast, non-LLM-based models achieve better ranking performance but struggle with the long-tail problem. 
Secondly, different methods often exhibit significant variance in accuracy and fairness/diversity performance, excelling in some metrics while underperforming in others. Moreover, the trends across different fairness metrics are not always consistent. 

Our toolkit provides researchers with a unified and convenient tool to compare various methods, explore trade-offs between different metrics, and analyze the reasons behind performance gaps. Researchers can use our toolkit to analyze results and validate their ideas across different base models and datasets.

\noindent\textbf{Post-processing models.} Table~\ref{tab:exp:rec_post_processing} shows the results of implemented post-process fairness and diversity-aware models in terms of re-ranking accuracy (NDCG, u-loss) and fairness/diversity (MMF, GINI, Entropy, and MMR). The benchmark results are obtained using the Steam dataset and the ranking lists provided from DMF~\cite{DMF} models. The fairness/diversity metric is also calculated at the Steam game category level.

First, from Table~\ref{tab:exp:rec_in_processing}, we observe that post-processing models outperform in-processing methods in fairness and diversity. However, they often face an accuracy-fairness trade-off, sacrificing accuracy to enhance the fairness and diversity of item categories. With our toolkit, FairDiverse, researchers can explore this trade-off across different parameters, base models, and datasets.



\begin{figure*}[t]  
    \centering    
    \includegraphics[width=\linewidth]{img/APIs.pdf}
    \caption{The custom steps for fairness and diversity-aware search and recommender models named \textit{YourModel}. The differently colored areas indicate the code you need to add when developing  different types of model. Generally, you can follow three steps: (1) define custom model parameters, (2) develop your model based on its type, and (3) integrate it into the pipeline. }
    \label{fig:rec_APIs}
    %\vspace*{-3mm}
\end{figure*}

%$\downarrow$ and $\uparrow$ indicate that a smaller or larger metric value, respectively, corresponds to better model performance. 

\subsection{Search}
\begin{table*}[t]
%\setlength{\tabcolsep}{2pt}
%\small
 \caption{Benchmark results for search task on the COMPAS dataset for different ranking sizes $K$, obtained on RankNet ranking models with pre-processing fairness-aware approaches. Evaluated using the shell command provided in our GitHub repository. \%D: diversity of the Female-Black group (the disadvantaged intersectional group); IGF: in-group-fairness measure as an average over the groups; yNN: individual fairness; NDCG-loss: NDCG loss. The reported results are based on default parameters.}
\label{tab:exp:rec_pre_processing}
\small
\setlength{\tabcolsep}{1.3mm}
\begin{tabular}{l cccc cccc }
\toprule
\multicolumn{1}{@{}l}{\multirow{2}{*}{Models/Metric}} & \multicolumn{4}{c}{$K=100$}      & \multicolumn{4}{c}{$K=300$} \\
\cmidrule(r){2-5}
\cmidrule{6-9}
\multicolumn{1}{@{}l}{} & \%D$\uparrow$ & IGF$\uparrow$ & yNN$\uparrow$ & NDCG-loss$\uparrow$ & \%D$\uparrow$ & IGF$\uparrow$ & yNN$\uparrow$ & NDCG-loss$\uparrow$ \\ 
\midrule
RankNet & 0.10 & - & 0.86 & 0.92 & 0.11 & - & 0.86 & 0.95  \\
CIFRank & 0.13 & 1.00 & 0.86 & 0.78 & 0.10 & 1.00 & 0.86 & 0.84 \\
LFR & 0.09 & 1.00 & 0.86 & 0.93 & 0.09 & 0.93 & 0.86 & 0.72 \\
gFair & 0.14 & 1.00 & 0.86 & 0.39 & 0.10 & 1.00 & 0.86 & 0.52 \\
iFair & 0.42 & 0.55 & 0.86 & 0.85 & 0.19 & 0.30 & 0.86 & 0.89 \\ \bottomrule
\end{tabular}
%\vspace*{-2mm}
\end{table*}

\begin{table*}[t]
%\setlength{\tabcolsep}{2pt}
%\small
 \caption{Benchmark results for the post-processing search result diversification models on the ClueWeb09 datasets with different ranking sizes $K$. We evaluate the performance using the shell command provided by the official Web Track which is also available in our GitHub repository. A larger metric value indicates superior model performance. The reported results are based on default parameter settings. }
\label{tab:exp:search_post_processing}
\small
\setlength{\tabcolsep}{1.2mm}
\begin{tabular}{l ccc ccc ccc}
\toprule
\multicolumn{1}{@{}l}{\multirow{2}{*}{Models/Metric}} & \multicolumn{3}{c}{$K=5$} & \multicolumn{3}{c}{$K=10$} & \multicolumn{3}{c}{$K=20$} \\
\cmidrule(r){2-4}
\cmidrule(r){5-7}
\cmidrule{8-10}
\multicolumn{1}{@{}l}{} & ERR-IA & $\alpha$-nDCG & S-rec & ERR-IA & $\alpha$-nDCG & S-rec & ERR-IA & $\alpha$-nDCG & S-rec \\ 
\midrule
PM2 & 0.2626 & 0.3292 & 0.4793 & 0.2824 & 0.3684 & 0.5708 & 0.2913 & 0.3989 & 0.6407 \\
xQuAD & 0.2002 & 0.2511 & 0.3961 & 0.2166 & 0.2838 & 0.4701 & 0.2272 & 0.3230 & 0.5761\\
DiversePrompts (GPT-4o) & 0.2890 & 0.3514 & 0.4972 & 0.3054 & 0.3833 & 0.5791 & 0.3131 & 0.4099 & 0.6396\\
DiversePrompts (Claude 3.5) & 0.3136 & 0.3800 & 0.4981 & 0.3292 & 0.4079 & 0.5741 & 0.3372 & 0.4348 & 0.6486 \\
DESA & 0.3497 & 0.4226 & 0.5195 & 0.3642 & 0.4452 & 0.5914 & 0.3703 & 0.4655 & 0.6438\\
DALETOR & 0.2770 & 0.3362 & 0.4609 & 0.2948 & 0.3732 & 0.5644 & 0.3047 & 0.4085 & 0.6581\\
%0.3534 & 0.4234 & 0.5336 & 0.3679 & 0.4467 & 0.6054 & 0.3754 & 0.4724 & 0.6605 \\
\bottomrule
\end{tabular}
%\vspace*{-2mm}
\end{table*}


As for the search task, we first evaluate the output of a ranking model trained on data that was debiased by a pre-processing model and then observe the diversity of the final ranking results achieved by post-processing models.

%evaluate the diversity of the final ranking results of post-processing models, or of the output of a ranking model trained on data that was debiased by a pre-processing model.

\noindent\textbf{Pre-processing models.} The results of implemented pre-pro\-cessing models applied on the input of a ranking model are denoted in Table~\ref{tab:exp:rec_pre_processing}. In this setting the ranking model is RankNet using the implementation provided by the Ranklib Library. We evaluate the performance on the COMPAS dataset. NDCG-loss represents the loss in utility with respect to the original ranking and the original scores. 

All models, except LFR, manage to improve or maintain the diversity in top-k of the Female-Black group, which is the most disadvantaged intersectional group. Out of all models gFair has the biggest loss in utility, while individual fairness (yNN) is not affected. In-group-fairness (IGF) is measured on the transformed representations, not on the output ranking, to check whether the transformed data respects the order within a group. It can be observed that CIFRank and gFair obtain perfect IGF. Using FairDiverse researchers can compare the impact of pre-processing models on the output ranking given the trade-offs between group fairness, individual fairness and utility loss. 

\noindent\textbf{Post-processing models.} The results of implemented post-process search result diversification models are denoted in Table~\ref{tab:exp:search_post_processing}. We evaluate the performance based on the ClueWeb09 dataset. The initial ranking list is provided by Lemur.\footnote{\url{https://lemurproject.org/clueweb09.php/}} We utilize the top 50 documents from the initial ranking list for testing these diversified ranking models' performance.

From the results, we can observe that, first, supervised diversified search models demonstrate superior performance compared to unsupervised models. Moreover, diversified rankers based on LLMs consistently outperform traditional unsupervised methods. This observation suggests that the knowledge acquired by LLMs during the pre-training stage significantly enhances the diversity of search results. To facilitate the exploration of these models, we present FairDiverse, a comprehensive toolkit that enables researchers to analyze various parameters, base models, and datasets.
