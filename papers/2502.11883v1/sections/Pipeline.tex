\vspace*{-2mm}
\section{Toolkit Overview}

In this section, we illustrate the overview pipelines of FairDiverse, as shown in Figure~\ref{fig:pipline}. Generally, search and recommendation tasks in IR can be considered ranking tasks with similar pipelines~\cite{yao2021user, xie2024unifiedssr}. Next, we will detail the IR pipeline steps, incorporating fairness- and diversity-aware algorithms.

% MART \cite{friedman2001greedy}, RankNet \cite{burges2005learning}, RankBoost \cite{freund2003efficient}, AdaRank \cite{xu2007adarank}, Coordinate Ascent \cite{metzler2007linear}, LambdaMART \cite{wu2010adapting}, ListNet \cite{cao2007learning}, Random Forests \cite{breiman2001random}. 

%Ranklib~\cite{dang2013lemur} (including: 8 popular search algorithms such as MART \cite{friedman2001greedy}, RankNet \cite{burges2005learning})

\begin{table*}[t]
\setlength{\tabcolsep}{1.1pt}
\caption{The models implemented in FairDiverse. }
\label{tab:models}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll}
\toprule
Model types & Models \\ \hline
\multicolumn{2}{c}{\emph{Recommendation}} \\
\midrule
Base model & MF~\cite{DMF}, BPR~\cite{BPR}, GRU4Rec~\cite{tan2016improved}, SASRec~\cite{SASRec}, Llama3~\cite{dubey2024llama}, Qwen2~\cite{bai2023qwen}, Mistral~\cite{jiang2023mistral7b} \\
In-processing & APR~\cite{APR}, DPR~\cite{DPR}, FairDual~\cite{FairDual}, FairNeg~\cite{FairNeg}, FOCF~\cite{yao2017beyond}, IPS~\cite{jiang2024item}, Reg~\cite{Reg}, Minmax-SGD~\cite{Minmax-SGD}, SDRO~\cite{SDRO}, FairPrompts~\cite{xu-etal-2024-study} \\
Post-processing & P-MMF~\cite{xu2023p}, CP-Fair~\cite{cpfair}, FairRec~\cite{fairrec}, FairRec+~\cite{fairrecplus}, FairSync~\cite{fairsync}, min-regularizer~\cite{xu2023p}, Tax-Rank~\cite{TaxRank}, Welf~\cite{nips21welf}, RAIF~\cite{liu2025repeat} \\
\midrule
\multicolumn{2}{c}{\emph{Search}} \\
\midrule
Base model & MART \cite{friedman2001greedy}, RankNet \cite{burges2005learning}, RankBoost \cite{freund2003efficient}, AdaRank \cite{xu2007adarank}, Coordinate Ascent \cite{metzler2007linear}, LambdaMART \cite{wu2010adapting}, ListNet \cite{cao2007learning}, Random Forests \cite{breiman2001random} \\
Pre-processing & CIF-Rank~\cite{yang2020causal}, LFR~\cite{zemel2013learning}, gFair, iFair~\cite{lahoti2019ifair} \\
Post-processing & PM2~\cite{PM2_12_sigir}, xQuAD~\cite{xQuAD_10_www}, DESA~\cite{DESA_20_cikm}, DALETOR~\cite{DALETOR_21_WWW}, DiversePrompts (based on GPT-4o~\cite{openai2024gpt4technicalreport} and Claude 3.5~\cite{Claude3.5}) \\ 
\bottomrule
\end{tabular}%
}
\end{table*}

\noindent\textbf{IR data collection.}
First, we collect the user and item information. Let $\mathcal{U}$ denote the set of users, and $\mathcal{I}$ the set of items. Each user $u\in\mathcal{U}$ may have a different user profile $\mathcal{P}_u$ such as age, gender, occupation, etc. We will record the user's browsing historical item list $H_u=[i_1, i_2, \cdots, i_n]$.
Each item $i\in\mathcal{I}$ is associated with specific attributes, such as categories, descriptions, and other metadata.

When a user $u$ interacts with an IR system, they may actively input a query $q_u$ to explicitly express their information need, a scenario commonly referred to as a search task. Alternatively, the user may not provide a query; instead, they rely on the IR system to infer their information needs and deliver relevant content, which characterizes a recommendation task.

Meanwhile, the collected data should also capture user behaviors, such as clicks, ratings, and other interactions.
For example, click behavior $c_{u,i}=1$ indicates that the user has clicked on the item, while $c_{u,i}=0$ signifies that the user did not interact with the item on the browser or recommender platform. Rating behaviors $r\in [0,5]$ denotes the preference degree of the 
These interaction behaviors are typically regarded as labels to train the IR models.

\noindent\textbf{Data processing.}
After collecting the IR data, it is essential to preprocess the dataset by filtering out noisy data samples, such as removing users with very few interaction histories, to ensure the quality of the data~\cite{recbole}. Then, we integrate user and item information with interaction behaviors and partition the data into training, validation, and test sets for model training and evaluation.

The pre-processing algorithms are primarily applied at this stage, aiming to mitigate biases present in the model input before training~\cite{rus2024study}. Specifically, certain features may enhance model performance but are influenced by sensitive attributes such as user race, and pre-processing methods aim to mitigate such effects by adjusting certain item or user features to ensure fairness and diversity.

Pre-processing methods are typically simple, easy to integrate with existing IR systems and offer good generalizability. However, these methods are independent of the model and may remove certain features that are useful for the model.

\noindent\textbf{Model training.}
After preparing the training data, we first transform the raw data into vectorized representations (\ie embeddings) suitable for model input. Then, we design the IR models, assign appropriate loss functions, and optimize the models based on the defined loss functions.

The in-processing methods are mainly applied to the model training phase. Typically, they incorporate a fairness- and diversity-aware constraint or regularizer into the IR loss function, optimizing it to enhance ranking accuracy while ensuring fairness or diversity in the results~\cite{APR, FairNeg}. 

\noindent\textbf{Result evaluation.}
Finally, after training the Information Retrieval (IR) models, we apply them to evaluate their performance. We use the trained IR model to infer relevance scores for all user-item pairs in the test set. Based on these scores, we  generate a ranked list by selecting items with the highest relevance scores for each user.

Post-processing methods are often based on a given set of relevance scores and re-rank the items to form a new ranked list. They formulate the problem as a constrained linear programming optimization~\cite{fairrec, xu2023p, TaxRank}. The objective is to maximize the sum of relevance scores while fairness- and diversity-aware constraints will be incorporated to ensure a fair and diverse ranked list. 

Typically, post-processing methods are considered the most effective approach, but their performance is often impacted by errors propagated from earlier stages in the pipeline.
