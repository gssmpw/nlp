\section{Toolkit Details}

For package details, we introduce the datasets used, the implemented models, and the evaluation metrics adopted.

\subsection{Datasets}\label{sec:datasets}
We provide details of each used dataset of recommendation and search tasks in the following parts.

\begin{figure*}[t]  
    \centering    
    \includegraphics[width=\linewidth]{img/all_yamls.pdf}
    \caption{The usage of FairDiverse with three steps: (1) Download the datasets and check the default parameters of the four stages of pipelines; (2) Set custom configuration file to execute the pipeline. The $*$tuning\_variables allow you to define variable values for the default settings across the four pipeline stages, with the In-processing configuration file overriding these default values when specified; (3) Run the shell command, with the task, stage, dataset, and your custom configuration file. }
    \label{fig:recommendation_yamls}
\end{figure*}

% The detailed configuration file parameters can be found in~\url{https://xuchen0427.github.io/FairDiverse/}.

\subsubsection{Recommendation} 
Any recommendation dataset can be used for the recommendation task. Specifically, we use the RecBole~\cite{recbole} dataset,\footnote{\url{https://github.com/RUCAIBox/RecSysDatasets}} which includes 43 commonly used datasets, all fully supported by our toolkit FairDiverse. The datasets span more than ten diverse domains, including games, products, and music. FairDiverse offers a comprehensive and fair comparison across all datasets.

To use them, researchers can simply download the datasets, place them in the \verb#~/recommendation/dataset# directory, and configure the settings in \verb#/properties/dataset/{dataset_name}.yaml#. The configuration files should specify which column names correspond to user ID, item ID, group ID, and other relevant fields. Once set up, running the command will enable algorithm evaluation on different datasets.

\subsubsection{Search.} 
Any dataset can be used with the pre-processing fairness models, however, in this framework we provide a working example on the COMPAS dataset \cite{bias2016there}. This dataset evaluates the bias in the COMPAS risk assessment tool by Northpointe (now Equivant), which predicts recidivism. It includes criminal history, recidivism probability, and sensitive attributes (gender and race). Following \cite{yang2020causal}, we provide a subset of 4,162 individuals distributed as 25\% White males, 59\% Black males, 6\% White females, and 10\% Black females.


For post-processing settings, we use ClueWeb09 Category B data collection~\cite{clueweb09_09_data} for our experiments.\footnote{\url{https://lemurproject.org/clueweb09.php/}} The ClueWeb09 dataset consists of 200 queries and 40,537 unique documents from the Web Track 2009-2012 dataset. Notably, queries \#95 and \#100 are not included in our experiments due to the lack of diversity judgments. The remaining 198 queries are associated with 3 to 8 manually annotated user intents, each accompanied by binary relevance ratings assigned at the intent level. %with binary relevance ratings assigned at the intent level. %To align with TREC Web Track, we adopt the top 50 results from Lemur as the prior relevance ranking. 


\subsection{Models}\label{sec:models}
We provide all the implemented models in Table~\ref{tab:models}. Then we will delve into the details of each model. Note that to integrate different models into our toolkit architecture, some models may be re-implemented. As a result, their performance may vary due to differences in implementation and experimental settings.

\subsubsection{Recommendation.}
In the recommendation task, many models focus on in-processing and post-processing methods.

Firstly, we will categorize the base recommendation models into non-LLMs (Large Language Models) and LLMs-based models.
non-LLMs-based models mainly utilize user-item interaction behaviors to learn a good representation of users and items.
LLMs-based models rely on the prompts to rank the items according to their textual information such as item titles~\cite{LLMs_Fairness_Survey, DaiUncovering}. The  models are:
\begin{itemize}[leftmargin=*]
    \item Non-LLMs-based models: 
    \begin{itemize}[leftmargin=*]
        \item  \textbf{DMF}~\cite{DMF}: which optimizes the matrix factorization with the deep neural networks.
        \item \textbf{BPR}~\cite{BPR}: optimizes pairwise ranking via implicit feedback. \item \textbf{GRU4Rec}~\cite{tan2016improved}: employs gated recurrent units (GRUs) for session-based recommendations. \item \textbf{SASRec}~\cite{SASRec}: leverages self-attention mechanisms to model sequential user behavior.
    \end{itemize}
    
    \item LLMs-based models: \textbf{Llama3}~\cite{dubey2024llama}, \textbf{Qwen2}~\cite{bai2023qwen}, \textbf{Mistral}~\cite{jiang2023mistral7b}: utilizing rank-specific prompts to conduct ranking tasks under LLMs~\cite{DaiUncovering}.
\end{itemize}

\noindent%
We categorize in-processing models into re-weight, re-sample, regularizer, and prompt-based methods. 
The re-weighting and re-sample-based method adjusts sample weights/ratios during loss calculation, assigning higher weights/radios to underperforming item groups to enhance their support. Regularizer-based methods incorporate fairness- and diversity-aware regularization terms into the original loss function. In contrast, prompt-based methods, designed for LLM-based models, introduce fairness-aware prompts to enhance support for underperforming item groups. They are:

%APR~\cite{APR}, DPR~\cite{DPR}, FairDual~\cite{FairDual}, FairNeg~\cite{FairNeg}, FOCF~\cite{yao2017beyond}, IPS~\cite{jiang2024item}, Reg~\cite{Reg}, Minmax-SGD~\cite{Minmax-SGD}, SDRO~\cite{SDRO}
\begin{itemize}[leftmargin=*]
    \item Re-weight-based models: 
    \begin{itemize}[leftmargin=*]
        \item \textbf{APR}~\cite{APR}: an adaptive reweighing method that dynamically prioritizes samples near the decision boundary to mitigate distribution shifts.
        \item \textbf{FairDual}~\cite{FairDual}: applies dual-mirror gradient descent to dynamically compute the weight for each sample to support the worst-off groups.
        \item \textbf{IPS}~\cite{jiang2024item}: employs the reciprocal of the sum popularity of items within the group as the weight assigned to that group.
        \item \textbf{Minmax-SGD}~\cite{Minmax-SGD}: applies optimizing techniques to dynamically sample groups.
        \item \textbf{SDRO}~\cite{SDRO}: Improves DRO with the distributional shift to optimize group MMF.
    \end{itemize}
   
    \item Re-sample-based models: \textbf{FairNeg}~\cite{FairNeg}: adjusts the group-level negative sampling distribution in the training process.
    \item Regularizer-based models: 
    \begin{itemize}[leftmargin=*]
        \item \textbf{DPR}~\cite{DPR}: applies a fair-aware adversarial loss based on statistical parity and equal opportunity.
        \item \textbf{FOCF}~\cite{yao2017beyond}: applies a fair-aware regularization loss of different groups.
        \item \textbf{Reg}~\cite{Reg}: applies a penalty on the squared difference between the scores of two groups across all positive user-item pairs.
    \end{itemize}
    
    \item Prompt-based models: \textbf{FairPrompts}~\cite{xu-etal-2024-study}.\footnote{We do not fine-tune the LLMs but only use manually designed prompts.}
\end{itemize}

\noindent%
We categorize the post-processing models into heuristic and learning-based models. Heuristic models primarily use algorithms like greedy search to re-rank items, while learning-based models dynamically generate fairness- and diversity-aware scores, which are incorporated into the original relevance score for re-ranking. They are:

%P-MMF~\cite{xu2023p}, CP-Fair~\cite{cpfair}, FairRec~\cite{fairrec}, FairRec+~\cite{fairrecplus}, FairSync~\cite{fairsync}, min-regularizer~\cite{xu2023p}, Tax-Rank~\cite{TaxRank}, Welf~\cite{nips21welf}

\begin{itemize}[leftmargin=*]
    \item Heuristic models: 
    \begin{itemize}[leftmargin=*]
        \item \textbf{CP-Fair}~\cite{cpfair}: applies a greedy solution to optimize the knapsack problem of fair ranking.
        \item \textbf{min-regularizer}~\cite{xu2023p}: adds an additional fairness score to the ranking scores, capturing the gap between the current utility and the worst-off utility.
        \item \textbf{RAIF}~\cite{liu2025repeat}: a model-agnostic repeat-bias-aware item fairness optimization algorithm based on mixed-integer linear programming.\footnote{Note that we remove repeat bias term, change the item fairness objective to make the exposure of each group closer, and extend RAIF into multi-group cases.}
    \end{itemize}
    
    \item Learning-based methods: 
    \begin{itemize}[leftmargin=*]
        \item \textbf{P-MMF}~\cite{xu2023p}: applies a dual-mirror gradient descent method to optimize the accuracy-fairness trade-off problem.
        \item \textbf{FairRec}~\cite{fairrec}, \textbf{FairRec+}~\cite{fairrecplus}: proposes leveraging Nash equilibrium to guarantee Max-Min Share of item exposure.
        \item \textbf{FairSync}~\cite{fairsync}: proposes to guarantee the minimum group utility under distributed retrieval stages.
        \item \textbf{Tax-Rank}~\cite{TaxRank}: applies the optimal transportation (OT) algorithm to trade-off fairness-accuracy.
        \item \textbf{Welf}~\cite{nips21welf}: use the Frank-Wolfe algorithm to maximize the Welfare functions of worst-off items.
    \end{itemize}
    
\end{itemize}







\subsubsection{Search.}
Our framework makes use of the Ranklib library to offer a variety of ranking models, including 8 popular algorithms: MART \cite{friedman2001greedy}, RankNet \cite{burges2005learning}, RankBoost \cite{freund2003efficient}, AdaRank \cite{xu2007adarank}, Coordinate Ascent \cite{metzler2007linear}, LambdaMART \cite{wu2010adapting}, ListNet \cite{cao2007learning}, Random Forests \cite{breiman2001random}. 

We divide pre-processing models into two categories causal based models and probabilistic mapping clustering models. All model implementations are adapted to optimize for multiple sensitive attributes and for non-binary groups, including intersectional groups.
\begin{itemize}[leftmargin=*]
    \item Causal based models:
    \begin{itemize}[leftmargin=*]
        \item \textbf{CIF-Rank}~\cite{yang2020causal} estimates the causal effect of the sensitive attributes on the data and makes use of them to correct for the bias encoded.
    \end{itemize}
    \item Probabilistic mapping clustering models:
    create representations which are independent of the available sensitive attributes.
    \begin{itemize}[leftmargin=*]
        \item \textbf{LFR}~\cite{zemel2013learning} optimizes for group fairness by making sure that the probability of a group to be mapped to a cluster is equal to the probability of the other group. 
        \item \textbf{iFair}~\cite{lahoti2019ifair} optimizes for individual fairness by making sure that the distance between similar individuals is maintained in the new space. 
        \item \textbf{gFair} based on iFair~\cite{lahoti2019ifair}, optimizes for group fairness by making sure that the distance between similar individuals from a group are close to similar individuals from the other group. It constraints the optimization to maintain the relative distance between individuals belonging to the same group.
    \end{itemize}
\end{itemize}
        
\noindent%
For post-processing search models, we often utilize the diversity-aware re-ranking models. These models can be roughly categorized into unsupervised methods and supervised methods. 

\begin{itemize}[leftmargin=*]
    \item Unsupervised methods:
    \begin{itemize}[leftmargin=*]
        \item  \textbf{PM2}~\cite{PM2_12_sigir}: optimizes proportionality by iteratively determining the topic that best maintained the overall proportionality. 
        \item \textbf{xQuAD}~\cite{xQuAD_10_www}: utilizes sub-queries representing pseudo user intents and diversifies document rankings by directly estimating the relevance of the retrieved documents to each sub-queries. 
        \item \textbf{DiversePrompts}: a diversity ranking model based on large language models. We design specific prompts tailored for search result diversification based on two latest closed-source LLMs: GPT-4o~\cite{openai2024gpt4technicalreport} and Claude 3.5~\cite{Claude3.5}.
    \end{itemize}
    \item Supervised methods: 
    \begin{itemize}[leftmargin=*]
        \item \textbf{DESA}~\cite{DESA_20_cikm}: employs the attention mechanism to model the novelty of documents and the explicit subtopics. 
        \item \textbf{DALETOR}~\cite{DALETOR_21_WWW}: proposes diversification-aware losses to approach the optimal ranking. 
    \end{itemize}
\end{itemize}




\subsection{Evaluation Metrics}\label{sec:evaluation}
We will delve into the details of each used evaluation metrics in the following parts.

\noindent\textbf{Recommendation.} In recommendation, evaluation metrics are generally categorized into two types: 
\begin{itemize}[leftmargin=*]
    \item Ranking accuracy-based metric: Mean Reciprocal Rank (MRR), Hit Radio (HR), and Normalized Discounted Cumulative Gain (NDCG)~\cite{IRbook}, utility loss (\ie Regret)~\cite{xu2023p}~\footnote{Note that evaluation metric NDCG in post-processing is slightly different compared to common definition: NDCG in post-processing means the re-ranking quality compared to original ranking quality~\cite{xu2023p}.}.
    \item Fairness- and diversity-based metric:  MMF~\cite{xu2023p}, GINI index~\cite{nips21welf}, Entropy~\cite{jost2006entropy}, and MinMaxRatio~\cite{rehman2018selection}.
\end{itemize}

\noindent\textbf{Search.} For the search task, we adopt the official diversity evaluation metrics of the Web Track, including ERR-IA~\cite{erria_09_cikm}, $\alpha$-nDCG~\cite{andcg_08_sigir}, and the diversity measure Subtopic Recall (denoted as S-rec)~\cite{srec15sigir}. These metrics assess the diversity of document rankings by explicitly rewarding novelty while penalizing redundancy. We follow the Web Track and utilize the provided shell command to evaluate model performance.

Furthermore, we provide support for fairness metrics, including group fairness measures such as demographic parity, ensuring proportional representation of groups, as well as proportional exposure of groups. Additionally, one can compute the in-group fairness metric proposed by \citet{yang2019balanced}, which computes the ratio between the lowest accepted score and the highest rejected score within a group. On top of group fairness, one can compute individual fairness by doing a pairwise comparison between candidates' distance in the features space and their achieved exposure~\cite{dwork2012fairness}. %These evaluation metrics are all computed based on the top 5, 10, and 20 ranking results. 



