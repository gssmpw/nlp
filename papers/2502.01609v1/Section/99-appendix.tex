\section{Experiment Details}
\label{appendix:settings}

% \subsection{Models}
% \label{appendix:model_settings}
% As shown in \autoref{tab:all_models}, we used four high-performance proprietary models in our experiments: GPT-4o \citep{hurst2024gpt}, GPT-4o-mini \citep{openai2024gpt4omini}, Claude-3.5-Sonnet \citep{anthropic2024claude35}, and o1-mini \citep{jaech2024openai}. In addition, we included eight open-weight models: Gemma-2-2B, Gemma-2-27B \citep{gemma_2024}, Qwen2.5-1.5B, Qwen2.5-7B, Qwen2.5-72B \citep{qwen2,qwen2.5}, Llama-3.1-8B \citep{meta2024llama31_8b}, Llama-3.1-70B \citep{meta2024llama31_70b} and Phi-3.5-mini \citep{abdin2024phi}.

\definecolor{headerbg}{RGB}{44,62,80}
\definecolor{rowgray}{RGB}{245,245,245}
\definecolor{rowblue}{RGB}{234,242,248}

\begin{table}[htbp]
\vspace{-1em}
\centering
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{0.55mm}
\caption{Models used in our experiments along with their versions, organizations, licenses, and purposes. \textit{Gen}: Model used for generating questions (as a proxy or victim); \textit{Eval}: Model used for evaluating datasets; \textit{Clf}: Model used as a classifier to filter questions.}
\label{tab:all_models}
\rowcolors{2}{rowgray}{white}  % Alternate row colors
\begin{tabular}{lcccccc}
    \toprule[1.5pt]
    \rowcolor{headerbg}
    \textcolor{white}{\textbf{Model}} & 
    \textcolor{white}{\textbf{Version}} & 
    \textcolor{white}{\textbf{Organization}} & 
    \textcolor{white}{\textbf{License}} & 
    \textcolor{white}{\textbf{Gen}} & 
    \textcolor{white}{\textbf{Eval}} & 
    \textcolor{white}{\textbf{Clf}} \\
    \midrule[0.8pt]
    GPT-4o-mini       & gpt-4o-mini-2024-07-18       & OpenAI      & Proprietary            & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} & \\
    GPT-4o            & gpt-4o-2024-08-06            & OpenAI      & Proprietary            & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} & \\
    Gemma-2-2B        & Gemma-2-2B-it                & Google      & Gemma License          &  & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} \\
    Gemma-2-27B       & Gemma-2-27B-it               & Google      & Gemma License          & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} & \\
    Llama-3.1-8B      & Meta-Llama-3.1-8B-Instruct   & Meta        & Llama 3.1 Community    &  & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} \\
    Llama-3.1-70B     & Meta-Llama-3.1-70B-Instruct  & Meta        & Llama 3.1 Community    & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} & \\
    Qwen2.5-1.5B      & Qwen2.5-1.5B-Instruct        & Alibaba     & Qwen License           &  &  & \textcolor{red}{\checkmark} \\
    Qwen2.5-7B        & Qwen2.5-7B-Instruct          & Alibaba     & Qwen License           &  & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} \\
    Qwen2.5-72B       & Qwen2.5-72B-Instruct         & Alibaba     & Qwen License           & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} & \\
    o1-mini           & o1-mini-2024-09-12           & OpenAI      & Proprietary            &  & \textcolor{red}{\checkmark} & \\
    Phi-3.5-mini      & Phi-3.5-mini-instruct        & Microsoft   & MIT                    &  & \textcolor{red}{\checkmark} & \textcolor{red}{\checkmark} \\
    Claude-3.5-Sonnet & claude-3-5-sonnet-20241022 & Anthropic   & Proprietary            &  & \textcolor{red}{\checkmark} & \\
    \bottomrule[1.5pt]
\end{tabular}
\vspace{-10pt}
\end{table}



\subsection{Experiment Settings}
\label{appendix:experiment_settings}
In all experiments, we adopt the same parameter settings. Specifically, we set the length threshold \(\lambda = 10\), the semantic threshold \(\tau_C = 0.5\), the number of simulation times \(n = 5\), and the diversity limit \(n_1 = 3\). Additionally, we use the same model as both the proxy model and the victim model.

\textbf{Experimental details of different victim models.} We selected five victim models with varying capabilities: GPT-4o, GPT-4o-mini, Llama-3.1-70B, Qwen2.5-72B, and Gemma-2-27B. From each of the four datasets, namely MMLU, CommonsenseQA, OpenbookQA, and TruthfulQA, we randomly sampled 100 original questions. Each victim model enhanced these questions via our search framework, creating five distinct enhanced datasets. To evaluate the effectiveness of these enhanced questions, we tested the performance of seven different models: GPT-4o-mini, Gemma-2-27B, Llama-3.1-8B, Qwen2.5-72B, o1-mini, GPT-4o, and Claude-3.5-Sonnet. All models were evaluated using a zero-shot approach with CoT prompting templates. This setup allowed us to systematically analyze the relationship between victim model capability and the difficulty of the generated enhanced questions.
The results of this experiment are summarized in \autoref{fig:multi_results}.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/main_results.pdf}
    \caption{Overall results between 4 datasets.}
    \label{fig:main_results}
\end{figure*}

\textbf{Experimental details of scale-up experiment.} We selected GPT-4o-mini as the victim model for question enhancement due to its balance between perturbation effectiveness and computational efficiency. From the same four datasets, we sampled 300 questions per dataset, resulting in a total of 1200 original questions. Similar to the first experiment, the enhanced questions were tested across the same seven models: GPT-4o-mini, Gemma-2-27B, Llama-3.1-8B, Qwen2.5-72B, o1-mini, GPT-4o, and Claude-3.5-Sonnet. All evaluations were conducted using zero-shot CoT prompting templates. This larger-scale experiment provided a more comprehensive analysis of the generalizability of our perturbation methodology.
The results of this experiment are summarized in \autoref{fig:main_results} and \autoref{tab:main_results}.

\textbf{Experimental details of baseline methods.} To validate the effectiveness of our tree-based search framework, we implemented two baseline perturbation approaches for comparison. The \textbf{Elaborated} method performed semantic-preserving length augmentation by expanding original questions with explanatory clauses and redundant contextual information while maintaining core semantics. The \textbf{Prompt-only} baseline utilized our perturbation prompt template (details in \autoref{appendix:prompt_template}) through Claude-3.5-Sonnet for automatic disturbance generation without subsequent search optimization. For a fair comparison, all baseline methods processed the same 100 original questions from four datasets using Claude-3.5-Sonnet as the executor. The enhanced questions were evaluated under identical zero-shot CoT settings across seven target models. This demonstrates the crucial role of our tree-based search mechanism in identifying optimal perturbation combinations rather than relying on simple length expansion or single-pass prompt perturbations.
The results of this experiment are summarized in \autoref{tab:baseline}.

\textbf{Experimental details of classifier.} We used the 1200 original questions from the scale-up experiment, splitting them into training, test, and validation sets. Specifically, 80 percent of the data was allocated to training, with 10 percent of the training set reserved for validation, and the remaining 10 percent was used for testing. For the prompt-based classifiers, we designed specific prompts to guide the models in determining whether a problem was hard to perturb. We evaluated the classification performance of seven models: GPT-4o-mini, GPT-4o, Llama-3.1-8B, Gemma-2-27B, Gemma-2-2B, Qwen2.5-1.5B, and Qwen2.5-7B. A baseline configuration without any classifier was also included for comparison. The effectiveness of these classifiers was measured using the F1-score with beta equal to 0.5, which prioritizes precision over recall. For the training-based classifiers, we used supervised fine-tuning with LoRA on four open-source models: Llama-3.1-8B, Gemma-2-2B, Qwen2.5-1.5B, and Qwen2.5-7B. The training was conducted on a single RTX 4090 GPU, with a learning rate set to 1e-4 and a total of five epochs. The performance of these fine-tuned classifiers was also evaluated using the F1-score on the test set. This experimental design allowed us to compare the utility of prompt-based and training-based classifiers in identifying hard-to-perturb questions. The results of this experiment are summarized in \autoref{fig:classifier_matrix}, \autoref{tab:cross_model_precision}, \autoref{fig:f1_score} and \autoref{tab:classifier_results}.

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.0}
\small
\setlength{\tabcolsep}{3.6mm}
\caption{Performance of the classifier under Prompt-Based and Fine-Tuned methods. The table reports Precision, Recall, and F\textsubscript{0.5} scores for both Prompt-Based (left) and Fine-Tuned (right) classifiers. Fine-tuned models are marked in the Fine-Tuned columns. The baseline represents the performance of the system without using the classifier.}
\label{tab:classifier_results}
    \begin{tabular}{lcccccc}
    \toprule[1pt]
     \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Prompt-Based}} & \multicolumn{3}{c}{\textbf{Fine-Tuned}} \\
     \cmidrule(lr){2-4} \cmidrule(lr){5-7}
     & \textbf{Precision} & \textbf{Recall} & \textbf{F\textsubscript{0.5}} & \textbf{Precision} & \textbf{Recall} & \textbf{F\textsubscript{0.5}} \\
    \midrule
    \centering
    GPT-4o-mini & 0.606 & 0.940 & 0.652 & - & - & - \\
    GPT-4o & \textbf{0.685} & 0.910 & \textbf{0.721} & - & - & - \\
    Llama-3.1-8B & 0.555 & 0.985 & 0.608 & \textbf{0.812} & 0.836 &\textbf{ 0.816} \\
    Gemma-2-27B & 0.568 & 1.000 & 0.622 & - & - & - \\
    Gemma-2-2B & 0.558 & 0.866 & 0.678 & 0.712 & 0.776 & 0.724 \\
    Qwen2.5-1.5B & 0.534 & 0.463 & 0.518 & 0.719 & 0.687 & 0.712 \\
    Qwen2.5-7B & 0.526 & 0.149 & 0.350 & 0.797 & 0.821 & 0.802 \\
    \midrule
    Baseline & 0.558 & 1.000 & 0.612 & 0.558 & 1.000 & 0.612 \\
    \bottomrule[1pt]
    \end{tabular}
\end{table}


\textbf{Experimental details of mitigating CDV.} We curated approximately 1200 preference data pairs. Each preference pair consisted of a question, a correct answer, and an incorrect answer collected from model responses in prior experiments. To ensure a fair evaluation, we guaranteed that enhanced questions originating from the same original question did not appear in both the training and test sets. The data was split into training, validation, and test sets, with 80 percent of the data used for training, 10 percent of the training set reserved for validation, and 20 percent allocated to testing. For prompt-based enhancement, we designed new prompt templates aimed at improving model focus on the core question content and tested them on seven models: GPT-4o-mini, Gemma-2-27B, Llama-3.1-8B, Qwen2.5-72B, o1-mini, GPT-4o, and Claude-3.5-Sonnet. For training-based enhancement, we fine-tuned three open-source models, namely Gemma-2-2B, Qwen2.5-7B, and Phi-3.5-mini. Using the Direct Preference Optimization algorithm, the fine-tuning was performed on two RTX 4090 GPUs with a learning rate set to 2e-4 and five epochs. The preference loss was implemented with a sigmoid activation function. The fine-tuned models were evaluated against three high-performance baseline models, specifically GPT-4o, GPT-4o-mini, and Qwen2.5-72B, using the original zero-shot with CoT prompting templates on the test set. This experiment provided insights into the effectiveness of both prompt-based and training-based approaches in improving model robustness against enhanced questions. The results of this experiment are summarized in \autoref{tab:prompt_enhanced} and \autoref{tab:dpo_enhanced}.

\subsection{Experiment Analysis}
\textbf{Distribution Analysis of Enhanced Questions.} Our analysis of the search process reveals interesting patterns in both the depth of perturbation chains and the length ratios of enhanced questions across different datasets. As shown in \autoref{fig:length_depth}, the majority of successful perturbations were found at relatively shallow depths, particularly for CommonsenseQA and OpenbookQA, where approximately 85\% and 80\% of effective perturbations were discovered within the first three levels. However, MMLU exhibited a notably different pattern, with nearly 30\% of perturbations requiring five or more steps to achieve effectiveness. This suggests that questions testing specialized knowledge often require more sophisticated and layered perturbations to successfully challenge model performance. The length ratios of enhanced questions also varied significantly across datasets. OpenbookQA showed a tendency toward longer perturbations, with about 70\% of enhanced questions being more than five times longer than their original versions. In contrast, MMLU questions maintained relatively compact perturbations, with nearly half of the enhanced questions staying within three times the original length. These distributions reflect the varying complexity required to effectively perturb different types of questions and highlight how the nature of the underlying task influences the perturbation process.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/length_depth.pdf}
    \caption{Distribution analysis of perturbation chain depth and enhanced question length ratio across four datasets.}
    \label{fig:length_depth}
\end{figure}

\section{Human Evaluation} 
To verify that the perturbations \( \Delta Q \) do not introduce significant semantic shifts and that the answers remain consistent, we conducted a human evaluation study. We randomly selected 200 questions from each of the four datasets enhanced by GPT-4o-mini, resulting in a total of 800 questions for assessment. Five undergraduate students majoring in computer science with good English were divided into two groups to participate in the evaluation. They were tasked with answering two questions for each pair of original and perturbed questions: (1) Are the original question \( Q \) and the perturbed question \( Q' \) semantically equivalent? (2) Does the answer to the perturbed question remain consistent with the original question's answer? The evaluators provided simple "Yes" or "No" responses. The results are summarized in \autoref{tab:human_evaluation}.

\begin{table}[htbp]
    \centering
    \small
    \setlength{\tabcolsep}{4pt} 
    \renewcommand{\arraystretch}{1.1}
    \caption{Results of human evaluation on semantic equivalence (Semantic Eq.) and answer consistency (Answer Consis.) between original and perturbed questions.}
    \label{tab:human_evaluation}
    \scalebox{0.9}{
    \begin{tabular}{p{2.8cm}cc} 
    \toprule[1pt]
    \textbf{Dataset} & \textbf{Semantic Eq. (\%)} & \textbf{Answer Consis. (\%)} \\
    \midrule
    \textbf{MMLU}          & 93.5 & 98.5 \\
    \textbf{OpenbookQA}    & 90.5 & 94.0 \\
    \textbf{CommonsenseQA} & 87.0 & 91.0 \\
    \textbf{TruthfulQA}    & 94.0 & 99.0 \\
    \bottomrule[1pt]
    \end{tabular}}
\end{table}




% \section{Overall Algorithm}
% \label{app:alg}

% We show the overall algorithm in Algorithm \ref{alg:perturbation_optimization}.

% \begin{algorithm}[h!]
% \caption{Overall Algorithm}
% \label{alg:perturbation_optimization}
% \KwIn{Dataset \( D = \{P_1, P_2, \dots, P_N\} \), Proxy model \( P_\text{proxy} \), Victim model \( M \), Thresholds \(\lambda, \tau_C \), Diversity limit \( n_1 \)}
% \KwOut{Candidate problem list \( L \)}

% Initialize priority queue \( \mathcal{Q} \gets \emptyset \) and candidate list \( L \gets \emptyset \)\;

% \ForEach{\( P = \langle Q, A_\text{gt}, \mathcal{A}_\text{inc} \rangle \in D \)}{
%     \If{\( p(y=1 \mid Q) = C(Q) < \tau_C \)}{
%         \textbf{continue} \tcp*[h]{Filter low-potential questions using classifier}\;
%     }
%     \If{\( r_M(P) \) = 0}{
%         Add \(P\) to \(L\)\;
        
%         \textbf{continue}
%     }\;
%     Add root node \( P \) to \( \mathcal{Q} \)\;
% }

% \While{\( \mathcal{Q} \neq \emptyset \)}{
%     Pop \( P' = \arg\max_{P \in \mathcal{Q}} \mathcal{V}(P) \), \( \mathcal{Q} \gets \mathcal{Q} \setminus \{P'\} \)\;

%     Generate \( k = |\mathcal{A}_\text{inc}| \) child nodes for \( P' \) using \( P_\text{proxy} \)\;
    
%     \For{each child node \( P'_j \)}{
%         Compute semantic shift \( S(P, P'_j) \) and length ratio \( \text{len}(P'_j)/\text{len}(P) \) \tcp*[h]{Semantic shift check and computing length ratio}\; 
    
%         \If{\( S(P, P'_j) = 1 \) and \( \text{len}(P'_j)/\text{len}(P) \leq \lambda \)}{
%             Compute value \( \mathcal{V}(P'_j) \)\;
%         } \Else{
%             Discard \( P'_j \)\;
%         }
%     }

    
%     \If{\( |\mathcal{C}_0(P')| > n_1 \)}{
%         Add top \( n_1 \) nodes from \( \mathcal{C}_0(P') \) to \( L \)
        
%         terminate branch\;
        
%         \textbf{continue}\;
%     }
    
%     \If{\( r_M(P'_j) = 1, \, \forall P'_j \)}{
%         \textbf{continue} \tcp*[h]{Skip nodes where all children are unpromising}\;
%     }
    
%     Add \( P'_j \) nodes to \( \mathcal{Q} \)\;
    
%     \If{\( m_{i+1} > m_i, \, \forall i \in \{1, 2, \dots, l-1\} \)}{
%         \textbf{continue} \tcp*[h]{Prune monotonically increasing success rate branches}\;
%     }
% }

% \Return \( L \)\;
% \end{algorithm}






\section{Case Study}
\label{appendix:case_study}
Figures \ref{fig:main4omini}, \ref{fig:mainclaude}, \ref{fig:main4o}, \ref{fig:mainllama}, \ref{fig:maino1}, \ref{fig:maingemma}, and \ref{fig:mainqwen} showcase the specific response performances of various models when confronted with both original and enhanced questions.

From Figures \ref{fig:dpogemma}, \ref{fig:dpophi} and \ref{fig:dpoqwen}, we present cases illustrating the changes in responses to enhanced questions by the Gemma-2-2B, Phi-3.5-mini, and Qwen2.5-7B following training-based improvements.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/dpo_gemma.pdf}
    \caption{Case study: Gemma-2-2B's responses to enhanced question before and after DPO.}
    \label{fig:dpogemma}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/dpo_phi35.pdf}
    \caption{Case study: Phi-3.5-mini's responses to enhanced question before and after DPO.}
    \label{fig:dpophi}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/dpo_qwen.pdf}
    \caption{Case study: Qwen2.5-7B's responses to enhanced question before and after DPO.}
    \label{fig:dpoqwen}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_4omini.pdf}
    \caption{Case study: GPT-4o-mini's responses to original questions and enhanced questions.}
    \label{fig:main4omini}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_claude.pdf}
    \caption{Case study: Claude-3.5-Sonnet's responses to original questions and enhanced questions.}
    \label{fig:mainclaude}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_gpt4o.pdf}
    \caption{Case study: GPT-4o's responses to original questions and enhanced questions.}
    \label{fig:main4o}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_llama38b.pdf}
    \caption{Case study: Llama-3.1-8B's responses to original questions and enhanced questions.}
    \label{fig:mainllama}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_o1mini.pdf}
    \caption{Case study: o1-mini's responses to original questions and enhanced questions.}
    \label{fig:maino1}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_gemma.pdf}
    \caption{Case study: Gemma-2-27B's responses to original questions and enhanced questions.}
    \label{fig:maingemma}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/main_qwen2572pptx.pdf}
    \caption{Case study: Qwen2.5-72b's responses to original questions and enhanced questions.}
    \label{fig:mainqwen}
\end{figure*}

\onecolumn
\section{Prompt Template}
\label{appendix:prompt_template}
\begin{promptbox}{Template for generating distraction from wrong option}
[Instruction] \newline
You are an expert at crafting subtle, contextually relevant distractions to enhance the challenge of multiple-choice questions. Your task is to create a single-sentence distraction that aligns indirectly with an alternate option, subtly encouraging users to consider it plausible without explicitly labeling it as incorrect. Follow these rules carefully:\newline

1. Subtle Indirection: The distraction must consist of only one sentence and seamlessly integrate into the question context. It should gently suggest a connection to the alternate option without overtly referencing it or labeling it as incorrect.

2. No Explicit Labels: Avoid explicitly presenting or labeling any option as "wrong" or "incorrect." Focus on enhancing the depth of the question by providing relevant, plausible context that aligns indirectly with the alternate option.

3. Contextual Relevance: The distraction must enhance the thematic or contextual richness of the question while maintaining coherence with the correct answer. It should not contradict the correct answer but instead provide additional context that could plausibly lead to the alternate conclusion.

4. Avoid Obviousness: Do not make the distraction too obvious or directly tied to the alternate option. Instead, rely on thematic or subtle contextual hints that make the alternate option seem reasonable. \newline

[The Start of the Question] \newline
\{question\} \newline
[The End of the Question] \newline

[The Start of the Alternate Option] \newline
\{wrong\_answer\} \newline
[The End of the Alternate Option] \newline

[Output Format] \newline
Generated Distraction: \textless Provide a concise, contextually relevant single-sentence distraction that indirectly aligns with the alternate option and subtly encourages consideration of it.\textgreater
\end{promptbox}


\begin{promptbox}{Template for answering questions (zero-shot + CoT)}
Please carefully read the question below and provide a solution from the choices. You must choose the model's final answer from one of the choices. Let's think step by step! \newline

[The Start of the Question] \newline
\{question\} \newline
[The End of the Question] \newline

[The Start of the Choices] \newline
\{choices\} \newline
[The End of the Choices] \newline
\end{promptbox}

\begin{promptbox}{Template for prompt-based enhancement}
Please carefully read the question below and provide a solution from the choices. You must choose the model's final answer from one of the choices. Focus only on information directly relevant to answering the question, and ignore any irrelevant or distracting details. Let's think step by step! \newline

[The Start of the Question] \newline
\{question\} \newline
[The End of the Question] \newline

[The Start of the Choices] \newline
\{choices\} \newline
[The End of the Choices] \newline
\end{promptbox}

\begin{promptbox}{Template for measuring semantic shift}
[Instruction] \newline
You are a linguistics expert. Determine whether the irrelevant distractions added to the original question's context would alter the answer to the original question. If the distractions do not affect the answer, respond with "Yes." If the distractions affect the answer, respond with "No." Let's think step by step! \newline

[The Start of Original Question] \newline
\{ori\_question\} \newline
[The End of Original Question] \newline

[The Start of Question with Distractions] \newline
\{question\_with\_distractions\} \newline
[The End of Question with Distractions] \newline

[Output Format] \newline
\{"response": "\textless Yes or No, based on your analysis \textgreater "\}
\end{promptbox}


\begin{promptbox}{Template for extracting the model's answer}
[Instruction] \newline
You are an expert in answer selecting. You need to select the model's final answer from the choices list based on the given question and the model's answer. \newline

[The Start of the Question] \newline
\{question\} \newline
[The End of the Question] \newline

[The Start of the Model's Answer] \newline
\{answer\} \newline
[The End of the Model's Answer] \newline

[The Start of the Choices] \newline
\{choices\} \newline
[The End of the Choices] \newline

[Output Format] \newline
\{"final\_answer": \textless Your extracted answer, strictly the same as the option in choices\textgreater\}
\end{promptbox}

\begin{promptbox}{Template for prompt-based classifier}
[Instruction] \newline
You are an expert at analyzing linguistic complexity and reasoning patterns. Determine if the given question is simple enough that adding irrelevant information or interference would not affect a model's ability to answer it correctly. If the question is too clear to be enhanced (i.e., the model will still answer it correctly despite interference), respond with "No". If the question can be enhanced (i.e., adding interference might confuse the model), respond with "Yes". \newline

[The Start of Question] \newline
\{question\} \newline
[The End of Question] \newline

[Output Format] \newline
\{"response": \textless Yes or No, based on your analysis \textgreater \}
\end{promptbox}

\begin{promptbox}{Template for elaborated question generation}
[Instruction] \newline
You are a language expert. Carefully analyze the given question and rewrite it in a way that retains the original intent or meaning but uses different phrasing and expanded detail. Ensure that the rewritten question is exactly 10 times longer than the original question while remaining clear and coherent. \newline

[The Start of the Question] \newline
\{question\} \newline
[The End of the Question] \newline

[Output Format] \newline
New question: \textless Your expanded and rephrased question here \textgreater
\end{promptbox}

\begin{promptbox}{Template for adding non-essential contextual information to questions}
[Instruction] \newline
You are a test design expert. Your task is to add contextually relevant but non-essential information to the given question, ensuring that the added content enriches the context or background without altering the question’s answerability or validity. \newline

[The Start of the Question] \newline
\{question\} \newline
[The End of the Question] \newline

[Requirements] \newline
1. Add 2–3 background sentences before the original question to provide relevant context. \newline
2. Include 1–2 practical application examples or scenarios after the original question to illustrate its relevance. \newline
3. Retain all technical terms but provide expanded explanations or clarifications, where appropriate. \newline
4. Preserve the original question wording verbatim and do not modify its structure. \newline
5. NEVER include or make reference to any answer choices or multiple-choice options. \newline
6. Ensure the final output omits any mention of "choices" or "options." \newline

[Output Format] \newline
New question: \textless Your modified question with added context and examples here \textgreater
\end{promptbox}
