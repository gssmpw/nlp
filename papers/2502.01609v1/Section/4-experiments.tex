\section{Experiment}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/main_results_multi.pdf}
    \caption{Performance of victim models on original questions and perturbed questions generated by different proxy LLMs.}
    %Overall results between questions enhanced by different models.}
    \label{fig:multi_results}
    \vspace{-1em}
\end{figure*}


\subsection{Experiment Setup}
\textbf{Selected Datasets.} We selected four widely used datasets for CDV investigation: %as the source data for enhancement: 
MMLU \citep{hendryckstest2021,hendrycks2021ethics}, CommonsenseQA \citep{talmor-etal-2019-commonsenseqa}, OpenbookQA \citep{OpenBookQA2018}, and TruthfulQA \citep{lin2021truthfulqa}. These datasets are widely used to evaluate LLMs on various dimensions, including commonsense reasoning and elementary science knowledge.


\textbf{Models.} As shown in \autoref{tab:all_models}, we used four proprietary models in our experiments: GPT-4o \citep{hurst2024gpt}, GPT-4o-mini \citep{openai2024gpt4omini}, Claude-3.5-Sonnet \citep{anthropic2024claude35}, and o1-mini \citep{jaech2024openai}. Additionally, we included eight open-weight models: Gemma-2-2B, Gemma-2-27B \citep{gemma_2024}, Qwen2.5-1.5B, Qwen2.5-7B, Qwen2.5-72B \citep{qwen2,qwen2.5}, Llama-3.1-8B \citep{meta2024llama31_8b}, Llama-3.1-70B \citep{meta2024llama31_70b} and Phi-3.5-mini \citep{abdin2024phi}.

\textbf{Hyperparameter Setting.} We set the temperature to 0.7 during the perturbation generation phase to encourage more creative and reproducible responses. For the evaluation phase, we lowered the temperature to 0.001 to ensure the stability of the responses, with a maximum output length of 1,024 tokens. Additionally, we set $\alpha$ to 2 and $\gamma$ to 1 in all experiments. For other detailed hyperparameter settings, please refer to \hyperref[appendix:experiment_settings]{Appendix A.2}.

\textbf{Prompt Template.} We employ prompt-based approaches to perform the following tasks: generating perturbations, evaluating semantic similarity, model assessment (zero-shot + CoT), enhancing problems as a baseline, identifying hard-to-perturb problems, and mitigating CDV. The specific prompt templates are provided in \autoref{appendix:prompt_template}.


\subsection{Main Results} 
% \yue{General question: It seems not description/ref about the experimental details of Table 1 and Figure 2. What's the dataset you used in Figure 2? What are the differences in experiments between Table 1 and Figure 2?}
We conducted extensive evaluation experiments and mitigation experiments on CDV. The detailed configurations of each experiment and their correspondence with all the figures can be found in the \autoref{appendix:settings}.

% \textbf{LLMs can automatically generate CDV examples that themselves will fail.}
\textbf{Our method enables LLMs to autonomously generate  CDV examples and  effectively engage in self-challenging.} In \autoref{fig:multi_results}, the same model was configured to serve as both the proxy and victim model during the process of CDV investigation, we can observe that all tested models exhibit a significant decline in accuracy on perturbed questions when compared to their performance on the original questions. For instance, GPT-4o-mini's performance significantly drops by more than 40\%.

Furthermore, it also reveals an intriguing pattern the vulnerabilities identified by a model itself tend to be more challenging than those discovered by other models. For instance, the GPT-4o-mini exhibits a significantly lower accuracy of 0.185 on perturbed questions generated through its self-identified vulnerabilities, compared to its performance of 0.235 on perturbations created by the more sophisticated GPT-4o. This indicates that \textbf{the model itself is more aware of its weaknesses}, aligning with the justifiability of previous works about self-alignment \citep{sun2024principle} and self-correction \citep{pan2023automatically}.

\begin{table*}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Model performance on original and perturbed datasets, and the corresponding performance decline.}
\label{tab:main_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{CommonsenseQA}} & \multicolumn{3}{c}{\textbf{OpenbookQA}} & \multicolumn{3}{c}{\textbf{TruthfulQA}} & \multicolumn{3}{c}{\textbf{MMLU}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}  \cmidrule(lr){8-10}  \cmidrule(lr){11-13} 
& Original &  Perturbed & $\Delta$ & Original &  Perturbed & $\Delta$ & Original &   Perturbed & $\Delta$ & Original &  Perturbed & $\Delta$ \\
\hline
\textbf{GPT-4o-mini}       & 0.857 & 0.220 & \cellcolor[HTML]{FF9999}0.637 & 0.897 & 0.228 & \cellcolor[HTML]{FF9999}0.668 & 0.607 & 0.160 & \cellcolor[HTML]{FF9999}0.447 & 0.787 & 0.255 & \cellcolor[HTML]{FF9999}0.532 \\
\textbf{Llama-3.1-8B}      & 0.753 & 0.230 & \cellcolor[HTML]{FF9999}0.524 & 0.807 & 0.212 & \cellcolor[HTML]{FF9999}0.595 & 0.570 & 0.283 & \cellcolor[HTML]{FFFFCC}0.288 & 0.697 & 0.300 & \cellcolor[HTML]{FFCCCC}0.397 \\
\textbf{Gemma-2-27B }      & 0.857 & 0.249 & \cellcolor[HTML]{FF9999}0.607 & 0.867 & 0.231 & \cellcolor[HTML]{FF9999}0.636 & 0.782 & 0.449 & \cellcolor[HTML]{FFCCCC}0.332 & 0.753 & 0.340 & \cellcolor[HTML]{FF9999}0.413 \\
\textbf{o1-mini}           & 0.856 & 0.296 & \cellcolor[HTML]{FF9999}0.560 & 0.897 & 0.377 & \cellcolor[HTML]{FF9999}0.519 & 0.748 & 0.523 & \cellcolor[HTML]{FFFFCC}0.226 & 0.803 & 0.451 & \cellcolor[HTML]{FFCCCC}0.352 \\
\textbf{Qwen2.5-72B}      & 0.880 & 0.304 & \cellcolor[HTML]{FF9999}0.576 & 0.917 & 0.325 & \cellcolor[HTML]{FF9999}0.592 & 0.790 & 0.442 & \cellcolor[HTML]{FFCCCC}0.348 & 0.807 & 0.412 & \cellcolor[HTML]{FFCCCC}0.395 \\
\textbf{GPT-4o}            & 0.890 & 0.277 & \cellcolor[HTML]{FF9999}0.613 & 0.950 & 0.375 & \cellcolor[HTML]{FF9999}0.575 & 0.757 & 0.494 & \cellcolor[HTML]{FFFFCC}0.263 & 0.870 & 0.552 & \cellcolor[HTML]{FFCCCC}0.318 \\
\textbf{Claude-3.5-sonnet} & 0.873 & 0.345 & \cellcolor[HTML]{FF9999}0.529 & 0.953 & 0.529 & \cellcolor[HTML]{FF9999}0.424 & 0.840 & 0.734 & \cellcolor[HTML]{FFFFCC}0.106 & 0.877 & 0.645 & \cellcolor[HTML]{FFFFCC}0.232 \\
\bottomrule[1pt]
\end{tabular}%
}
\end{table*}

\textbf{All models are susceptible to challenges posed by CDV examples, regardless of their capabilities.}
Our findings reveal that CDV examples generated by stronger models effectively challenge weaker models, while those generated by weaker models also significantly impact stronger ones. As shown in \autoref{fig:multi_results}, despite their superior language, reasoning, and knowledge capabilities, the stronger models have not achieved complete dominance over smaller models in handling CDV examples. Even the most advanced LLMs\footnote{We refer to the leaderboard at \url{https://lmarena.ai/} for model performance comparisons.}, such as GPT-4o and Claude-3.5-Sonnet, achieve less than 50\% accuracy on questions perturbed by Gemma-2-27B. This underscores a fundamental concern, showing that no model is fully immune to such vulnerability.
% \textcolor{red}{Please check here "reliability-level challenge" and "utility-related challenge". what is the meaning of them? how this "indication" is reached? This is the same or different from what was mentioned earlier about "reasoning-level challenge rather than a knowledge-level issue,"? }
% This indicates that CDV examples present a reliability-level challenge rather than a utility-related challenge\footnote{Here, utility refers to the functional effectiveness of the model in natural language processing tasks, including abilities in logical reasoning, content summarization, text generation, and so on \citep{huang2024trustllm}.} and that the state-of-the-art models have not made fundamental leaps over weaker models in patching CDV examples.

\begin{wrapfigure}{r}{0.61\textwidth}
\vspace{-1em}
\centering
\scriptsize
\renewcommand{\arraystretch}{1.1} % 调整行间距
\setlength{\tabcolsep}{3mm} % 减小列间距
\caption{Accuracy on original and perturbed samples by different methods on the sampled dataset. \textbf{Elaborated}: Semantic-preserving length augmentation, \textbf{Prompt-only}: Auto-generated perturbations without search, \textbf{DyVal2}: Dynamic evaluation framework \citep{zhu2024dynamic}. Lower scores indicate stronger CDV challenge effectiveness.} 
\label{tab:baseline}
\begin{tabular}{l@{\hskip 3mm}c@{\hskip 3mm}c@{\hskip 3mm}c@{\hskip 3mm}c@{\hskip 3mm}c}
    \toprule[1pt]
    \textbf{Model} & \textbf{Original} & \textbf{Elaborated} & \textbf{Prompt-only} & \textbf{DyVal2} & \textbf{Ours} \\
    \midrule
    \textbf{GPT-4o-mini}       & 0.890 & 0.727 & 0.760 & 0.630 & 0.185 \\
    \textbf{Gemma-2-27B}       & 0.860 & 0.788 & 0.790 & 0.650 & 0.237 \\
    \textbf{Llama-3.1-8B}      & 0.667 & 0.657 & 0.620 & 0.640 & 0.247 \\
    \textbf{Qwen2.5-72B}       & 0.820 & 0.737 & 0.810 & 0.697 & 0.334 \\
    \textbf{o1-mini}           & 0.860 & 0.694 & 0.770 & 0.697 & 0.374 \\
    \textbf{GPT-4o}            & 0.940 & 0.818 & 0.850 & 0.740 & 0.390 \\
    \textbf{Claude-3.5-sonnet} & 0.879 & 0.838 & 0.820 & 0.780 & 0.495 \\
    \midrule
    \textbf{Average}           & 0.843 & 0.757 & 0.774 & 0.691 & \textbf{0.323} \\
    \bottomrule[1pt]
\end{tabular}
\end{wrapfigure}




\textbf{The extent of performance degradation varies significantly across tasks.} For example, as shown in \autoref{tab:main_results}, the average performance drop on the TruthfulQA dataset is consistently smaller than that observed on OpenbookQA for all evaluated models. This trend holds true across models, indicating that model sensitivity against CDV varies in different tasks. Tasks that emphasize domain-specific knowledge like OpenbookQA, appear to expose model vulnerabilities more effectively than trustworthiness-related tasks like TruthfulQA.


\textbf{Our method outperforms existing perturbation techniques by a significant margin.} We consider two traditional perturbation approaches: length-based augmentation (Elaborated), which expands the original question's length while preserving its core meaning, and simple auto-generated perturbations (Prompt-only), which uses a single prompt to add interference to the question. However, these methods lead to only minimal performance drops. As shown in \autoref{tab:baseline}, even advanced methods like DyVal2 achieve only a 15.2\% average accuracy reduction. In contrast, our proposed perturbation method introduces a dramatic 52.0\% average accuracy decline, outperforming existing methods by 2 to 4 times. Notably, even highly capable models such as Claude-3.5-Sonnet exhibit a substantial 49.5\% absolute accuracy drop under our perturbations, compared to just 10.0\% under DyVal2. These results highlight the unique strength of our tree-based search framework in systematically targeting the model's ability to discern and resist distraction across diverse architectures, rather than merely increasing input complexity.

\subsection{Classifier: Filtering Hard-to-Perturb Problems}

A critical challenge in CDV generation lies in distinguishing between \textbf{hard-to-perturb problems} (the questions are quite easy and naive for LLMs so that it's highly impossible to be the CDV examples, like elementary arithmetic or factual questions) and \textbf{easy-to-perturb problems} (questions susceptible to semantic-preserving perturbations). Our analysis reveals that approximately 37\% of computational resources are typically wasted on enhancing hard-to-perturb problems. To address this inefficiency, we develop classifiers that predict the perturbation possibility of each problem.

\begin{wraptable}{r}{0.5\textwidth}
\centering
\scriptsize
\caption{The impact of the classifier on the perturbation success rate of the LLMs. The full model names are: GPT-4o, Gemma-2-27B, Llama-3.1-70B, and Qwen2-5-72B. The rows display the perturbation success rate with and without the classifier.}
\label{tab:cross_model_precision}
\begin{tabular}{lcccc}
    \toprule
    \textbf{Mode} & \textbf{GPT-4o} & \textbf{Gemma-2} & \textbf{Llama-3.1} & \textbf{Qwen2.5} \\
    \midrule
    \textbf{w/o classifier} & 0.527 & 0.592 & 0.581 & 0.563 \\
    \textbf{w/ classifier} & 0.723 & 0.791 & 0.754 & 0.735 \\
    \bottomrule
\end{tabular}
\end{wraptable}



Firstly, to validate the classifier's generalization, it is crucial to examine whether hard-to-perturb questions exhibit nearly consistency across most LLMs. The confusion matrix in \autoref{fig:classifier_matrix} demonstrates significant overlap in perturbation difficulty across models: about 82\% of problems are either perturbable or non-perturbable for both models in any pairwise comparison. This consistency across models enables the development of universal classifiers for the prediction of perturbation difficulty.

\begin{wrapfigure}{r}{0.5\textwidth}
    \includegraphics[width=\linewidth]{images/matrix.pdf}
    \caption{The result of whether the samples are perturbable by two models, A and B. Here, A=1 indicates that the sample is easy-to-perturb for model A, while A=0 means it is hard-to-perturb for model A. The numbers in each cell represent the percentage of samples in each category.}
    \label{fig:classifier_matrix}
    \vspace{-2em}
\end{wrapfigure}




We implement two classifier paradigms: (1) \textit{Prompt-based classifiers} using zero-shot LLM judgments, and (2) \textit{Fine-tuned classifiers} trained on 1,080 annotated samples with 120 held-out test cases. Notably, as shown in \autoref{tab:cross_model_precision}, classifiers trained on GPT-4o-mini-derived data generalize robustly to diverse LLMs, maintaining high precision even on stronger models. The evaluation focuses on precision-recall tradeoffs using the F$_{\beta}$ metric ($\beta=0.5$), prioritizing precision to minimize false positives in perturbable problem selection. The F$_{\beta}$ score is computed as:

\begin{equation}
\text{F}_{\beta} = (1 + \beta^2) \times \frac{\text{Precision} \times \text{Recall}}{(\beta^2 \times \text{Precision}) + \text{Recall}}
\end{equation}


The results shown in \autoref{fig:f1_score} and \autoref{tab:classifier_results} reveal that fine-tuned classifiers substantially improve perturbation efficiency through precise problem selection. The fine-tuned classifiers achieve 83\% precision in identifying perturbable problems, outperforming the best prompt-based approach (GPT-4o at 68\%). We will discuss the details of computational cost saved by the trained classifier in \autoref{sec:ablation_study}.




% The optimized pipeline reduces the computational resources required by 37\% through the reduction of the targeted search space, demonstrating that the selection of problem guided by the classifier maintains adversarial effectiveness while significantly improving scalability.


\subsection{Ablation Study}
\label{sec:ablation_study}

\textbf{Impact of the value function.} We evaluate the effectiveness of the designed value function \(\mathcal{V}(P')\) and its components introduced in \autoref{sec:value_function} about guiding the search process. This function incorporates two key factors: the success rate \( r_M(P') \) of the victim model on the perturbed problem \( P' \), and a depth penalty \( \text{depth}^{-\gamma} \), designed to balance exploration and computational efficiency.


To assess the individual contributions of the success rate \( r_M(P') \) and the depth penalty to the overall effectiveness of the perturbations, we randomly selected questions from four different datasets for testing and calculated the perturbation success rate (the percentage of the questions that achieves 0\% accuracy (i.e., \( r_M(P')=0 \))). The results are as follows: using the complete value function, the perturbation success rate was 59\%; without the depth penalty term \( \text{depth}^{-\gamma} \), the perturbation success rate dropped to 57\%; and without \( r_M(P') \), the perturbation success rate further decreased to 53\%. These results demonstrate that both the success rate term and the depth penalty term significantly enhance the effectiveness of the search process.

\textbf{Cost saved by classifier.} To evaluate the impact of incorporating the fine-tuned classifier into the pipeline, we experimented with comparing the performance with and without the classifier.  We randomly selected questions from four datasets for this study. For the condition utilizing the classifier, we applied our fine-tuned classifier to filter and select 100 questions identified as easy-to-perturb. For the condition without the classifier, we directly sampled 100 questions at random without any prior filtering. 


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/f1.pdf}
    \caption{Comparison of classification performance using $F_{0.5}$ Scores. \textbf{Left:} $F_{0.5}$ scores of seven prompt-based classifiers compared to the baseline without a classifier (recall is 1 when all problems are enhanced directly). \textbf{Right:} $F_{0.5}$ scores of four fine-tuned classifiers after training, showing significant improvements over prompt-based classifiers.}
    \label{fig:f1_score}
    \vspace{-1em}
\end{wrapfigure}

The experimental results are presented in \autoref{tab:classifier_comparison}. By effectively filtering out hard-to-perturb questions, the classifier allowed us to focus resources on those that were more susceptible to successful perturbation. This led to a significant increase in the perturbation success rate and minimized the costs associated with unsuccessful attempts (the perturbation success rate increase by 38.9\% and the average cost decreased by 22\%).



\subsection{Mitigation of CDV}

Notably, models that perform well on original questions but fail on perturbed ones show they have the necessary knowledge but are susceptible to contextual information. To address this, we explored both \textbf{prompt-based (\emph{i.e.}, training-free)} and \textbf{training-based strategies} to improve their performance on these challenging questions.

\textbf{Prompt-based mitigation is unable to patch CDV.} Since our perturbed questions introduce context-based perturbations while preserving the core question content, we explored whether explicit prompt instructions could help models focus on the essential information and filter out the added distractions. We tested this approach by modifying the original prompts to include specific guidance on identifying and focusing on the key question components, with detailed prompt templates provided in Appendix~\ref{appendix:prompt_template}. As shown in \autoref{tab:prompt_enhanced}, the prompt-based approach did not yield significant improvements in mitigating CDV. Some models, such as o1-mini and Claude-3.5-Sonnet, even showed slightly decreased accuracy with the modified prompts. This suggests that the distractions introduced by our perturbed questions cannot be effectively addressed through simple prompt refinements alone.

\begin{table*}[t]
\centering
\small
\renewcommand{\arraystretch}{1.2}
\caption{Comparison of cost with and without classifier. \textbf{Inp. Tok.}: Number of input tokens, \textbf{Out. Tok.}: Number of output tokens, \textbf{Pert. Ques.}: Number of successfully perturbed questions, \textbf{Pert. Rate (\%)}: Perturbation success rate, \textbf{Cost/Ques. (\$)}: USD Cost per perturbed question.}
\label{tab:classifier_comparison}

    \begin{tabular}{cccccc}
    \toprule[1pt]
    \textbf{Mode} & \textbf{Inp. Tok.} & \textbf{Out. Tok.} & \textbf{Pert. Ques.} & \textbf{Pert. Rate (\%)} & \textbf{Cost (\$)} \\
    \midrule
    \textbf{\textit{w/o} classifier} & 3.69M & 1.48M & 175 & 59\% & 0.0082 \\
    \textbf{\textit{w/} classifier}  & 3.81M & 1.57M & 236 & 82\% ($\uparrow$ 38.9\%) & 0.0064 ($\downarrow$ 21.9\%) \\
    \bottomrule[1pt]
    \end{tabular}
\end{table*}


\textbf{Training-based mitigation brings significant benefits.} The limited success of prompt-based mitigation inspired us to explore whether training-based approaches (i.e., fine-tuning) could work well. We leveraged about 1,200 preference data pairs curated from our dataset, where each pair consisted of a question, a chosen answer (correct answer), and a rejected answer (wrong answer). These pairs were derived from model responses in our main experiments and were split into training and test sets. We selected three open-weight models for training, and the training process was conducted using the DPO \citep{rafailov2024direct}.

\begin{wraptable}{l}{0.5\textwidth}
\vspace{-1em}
\centering
\scriptsize
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{4mm}
\caption{Model accuracy before and after prompt-based mitigation (Original vs \textbf{Enh.}).}
\label{tab:prompt_enhanced}
\begin{tabular}{lrrr}
    \toprule[1pt]
    \textbf{Model} & \textbf{Orig.} & \textbf{Enh.} & \textbf{Diff.} \\
    \midrule
    \textbf{GPT-4o-mini}       & 0.185 & 0.211 & \textcolor{green!40!black}{$+$0.026} \\
    \textbf{Llama-3.1-8B}      & 0.247 & 0.251 & \textcolor{green!40!black}{$+$0.003} \\
    \textbf{Gemma-2-27B}       & 0.237 & 0.255 & \textcolor{green!40!black}{$+$0.018} \\
    \textbf{o1-mini}           & 0.374 & 0.366 & \textcolor{red!40!black}{$-$0.008} \\
    \textbf{Qwen2.5-72B}      & 0.334 & 0.343 & \textcolor{green!40!black}{$+$0.009} \\
    \textbf{GPT-4o}            & 0.390 & 0.391 & \textcolor{green!40!black}{$+$0.002} \\
    \textbf{Claude-3.5-sonnet} & 0.495 & 0.481 & \textcolor{red!40!black}{$-$0.013} \\
    \bottomrule[1pt]
\end{tabular}
\vspace{-1em}
\end{wraptable}

As shown in \autoref{tab:dpo_enhanced}, the results indicate that the performance of the original models was comparable to GPT-4o-mini, while significantly behind more powerful models like GPT-4o and Qwen2.5-72B. However, after fine-tuning, all three models demonstrated substantial improvements in accuracy on the enhanced questions. Most remarkably, Phi-3.5-mini achieved exceptional progress, surpassing all comparison models, including GPT-4o. Through detailed case studies provided in \autoref{appendix:case_study}, we found that fine-tuned models exhibited markedly improved ability to mitigate CDV rather than simply gaining new knowledge. Notably, a large fraction of the originally incorrect answers remained incorrect after training (e.g., 82.1\% for Phi-3.5-mini), suggesting that the improvements were driven by a stronger ability to identify and ignore irrelevant contextual information, maintaining focus on the key points required to answer questions correctly.

\begin{wraptable}{l}{0.5\textwidth}
\centering
\scriptsize
\vspace{-1em}
\renewcommand{\arraystretch}{1}
\setlength{\tabcolsep}{3mm}
\caption{Model accuracy before and after DPO training. \textbf{Retain}: fraction of original incorrect answers that remain incorrect after training.}
\label{tab:dpo_enhanced}
    \begin{tabular}{p{2cm}cccc}
    \toprule[1pt]
    \textbf{Model} & \textbf{Orig.} & \textbf{Enh.} & \textbf{Diff.} & \textbf{Retain} \\
    \midrule
    \textbf{Gemma-2-2B}       & 0.257 & 0.432 & \textcolor{green!40!black}{+0.175} & 0.788 \\
    \textbf{Qwen2.5-7B}      & 0.212 & 0.440 & \textcolor{green!40!black}{+0.228} & 0.763 \\
    \textbf{Phi-3.5-mini}     & 0.195 & 0.680 & \textcolor{green!40!black}{+0.485} & 0.821 \\
    \midrule
    \textit{GPT-4o}           & 0.568 & -     & -                                  & -    \\
    \textit{Qwen2.5-72B}     & 0.519 & -     & -                                  & -    \\
    \textit{GPT-4o-mini}      & 0.232 & -     & -                                  & -    \\
    \bottomrule[1pt]
    \end{tabular}
    \vspace{-1em}
\end{wraptable}






These findings shed light on the nature of CDV and strategies for its mitigation. The limited effectiveness of prompt-based mitigation and significant improvement brought by training indicates that the interference caused by our enhanced questions impacts the models at a deeper decision-making level, rather than merely introducing superficial distractions (i.e., LLMs are unable to be aware of such distractions simply guided by prompt engineering). \textbf{This highlights the importance of incorporating CDV mitigation into the initial training pipeline}, and we urge the community to prioritize these aspects during model development, rather than relying on post-hoc solutions, which may be less effective and computationally expensive.

