\section{Introduction}

% \yue{Should we give a name for the vulnerability that we study? Some examples for reference: ``Focus Disruption Vulnerability'', ``Focus Disruption Vulnerability'', or ``Contextual Distraction Vulnerability''}


% \yue{Large Language Model (LLM) has shown exceptional capabilities in the Natural Language Process (NLP) domain. It has been widely used in various downstream applications including xxxx, xxxx, xxxx, and xxxx.

% Despite their strong abilities, researchers found LLMs are prone to be fragile. For instance, many studies show even the latest LLMs can be jailbroken by carefully designed prompts \citep{zou2023universal}. Some studies also reveal the hallucination \citep{xu2024hallucination}, inconsistency \citep{huang-etal-2024-1} and ``Reversal Curse'' \citep{berglund2023reversal} of LLMs.

% In this study, we will unveil a vulnerability of LLMs: by modifying the questions without changing their semantics, LLMs' performance can be significantly compromised. Different from the noise generated by adversarial learning (e.g., the noise is meaningless in its semantic), the modification add extra context for the question so that LLMs are easy to be distracted and respond with an incorrect answer.

% Specifically, in this study, we propose a methodology to introduce a novel and automatic pipeline to launch attacks on such vulnerability. The pipeline xxxxxxxxxxxx (introduce your method). The experiments show that xxxxxxxxxxx (introduce your result).

% To further explore the vulnerability, we put efforts on patching or mitigating such vulnerability. Specifically, xxxxxxxxxxxxxxx (introduce how you improved it and what findings you obtained)

% In summary, our contributions are three-folds: 1) We reveil the vulnerability and propose a method. 2) We conduct extensive experiments to validate the effectiveness of our method. 3) We study how to mitigate such vulnerability.

% }

Large Language Models (LLMs) \citep{zhou2023survey} have demonstrated remarkable capabilities across various Natural Language Processing (NLP) tasks, revolutionizing wide downstream applications such as medicine \citep{zhao2023survey}, education \citep{kasneci2023chatgpt}, and science \citep{li2024quantifying, guo2023can, huang2024social}. Despite their impressive performance, recent studies have exposed various vulnerabilities in LLMs, including susceptibility to jailbreaking attacks \citep{zou2023universal}, hallucination issues \citep{xu2024hallucination}, and consistency problems \citep{liang2024internal, huang-etal-2024-1}. These vulnerabilities highlight the limitations of LLMs in handling nuanced and adversarial scenarios, making it critical to uncover and analyze additional weaknesses to improve their reliability.


In this work, we investigate a novel vulnerability termed \textbf{Contextual Distraction Vulnerability (CDV)}, where semantically coherent but \textit{non-essential contextual} additions to a question degrade LLM performance. For instance, a customer service chatbot might miss a refund request hidden in a short story about discovering products through social media influencers. Similarly, a technical query about machine learning could be misunderstood if it's preceded by a student's emotional account of exam preparation anxiety. 
Unlike adversarial attacks that inject semantically meaningless noise into inputs \citep{zou2023universal, 10.1145/3658644.3690291} and distraction brought by long-context input \citep{bai2023longbench}, for CDV, our study demonstrates that semantically coherent without a long context yet contextually distracting modifications are sufficient to disrupt the decision-making process of even the most advanced LLMs. This vulnerability underscores a critical weakness in LLMs' ability to filter out irrelevant information and prioritize core knowledge, which is essential for robust reasoning.

Recent studies have demonstrated the powerful generative capabilities of LLM \cite{xu2024magpie, wu2024unigen}, To systematically investigate this vulnerability, we propose a methodology for efficiently automating the generation of questions that may trigger CDV in LLMs. This is a non-trivial problem, as the primary challenge lies in identifying the most effective distraction (a perturbation to the original input) within a vast search space while ensuring that the added distraction remains semantically coherent with the original question. Our proposed method employs a tree-based search algorithm, 
%Specifically, our approach focuses on enhancing a given question into one that has a high probability of exploiting CDV in LLMs. By employing an error-guided generation strategy that maintains semantic consistency, our method explores perturbations of the original question using a tree-based search algorithm. To streamline the search process, the method 
incorporates early stopping strategies and leverages a simple but effective classifier to pre-filter problematic candidates. Through extensive experimentation across multiple datasets on the latest LLMs, we demonstrate the effectiveness of our method in producing effective  CDV question examples.


Beyond this, we investigate potential causes and mitigation strategies for CDV. Our findings reveal that 1) while simple prompt engineering provides limited benefits, targeted fine-tuning (e.g., DPO \citep{rafailov2024direct})  significantly enhances model robustness against contextual distractions. 2) CDV is a fundamental challenge in LLM development that requires ability-level  rather than knowledge-level enhancement.

In summary, our contributions are three-folds: 1) We identify and characterize CDV, a significant weakness in current LLMs that affects their reliability in question-answering tasks. 2) We propose an efficient automated pipeline to generate semantically valid contextual distractions that expose this vulnerability, achieving substantial performance degradation across various models and datasets. 3) We conduct comprehensive experiments to evaluate different mitigation strategies, providing insights into the nature of CDV and potential approaches to overcome CDV. %improve the reliability of the model.

