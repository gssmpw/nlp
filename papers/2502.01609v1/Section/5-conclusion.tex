\section{Conclusion}

In this work, we identify a critical weakness, termed Contextual Distraction Vulnerability (CDV), where models struggle with irrelevant but semantically coherent context. Using an efficient tree-based search method, we generate CDV examples across four datasets, causing a significant performance drop in the state-of-the-art models. Mitigation strategies, particularly post-targeted training, show promise in improving robustness. This highlights CDV as an ability-level challenge, emphasizing the need to address this issue to ensure model reliability.
