\section{Methodology}
\subsection{Overview}

% Our goal is to systematically uncover vulnerabilities in LLMs by generating adversarial examples that preserve semantic integrity while revealing subtle failure modes. 

As illustrated in \autoref{fig:pipeline}, our objective is to systematically identify and analyze the vulnerabilities of LLMs by generating high-quality CDV examples that challenge models while maintaining the semantic integrity of the original questions.

Generating effective CDV examples poses three significant challenges: First, it is critical to design an effective reward signal that guides modifications toward high-quality CDV examples. Second, the process must address the computational overhead caused by the vast search space associated with generating and evaluating these examples. Third, the generation must mitigate semantic shifts and account for the impact introduced by long-context inputs to ensure the original intent of the examples.

To overcome these challenges, we introduce a multi-step framework. Initially, a \emph{classifier-based filtering} mechanism is employed to identify  original samples that are easy to perturb, substantially reducing the computational burden by narrowing down the search space. For the filtered samples, we apply a \emph{tree-based search} strategy that systematically explores contextual distractions using simulation-driven evaluation. This process generates controlled perturbations through a \emph{proxy model} and guides the search using rewards based on the outcomes of the \emph{victim model} during simulation. Iterative semantic validation and length control are applied to ensure that the generated examples align closely with the original intent. Furthermore, we incorporate efficient pruning and early stopping techniques to balance the trade-off between uncovering meaningful CDV examples and maintaining computational efficiency.

% Two principal challenges arise in this process: (1) preventing \emph{semantic shift}, where modifications may alter the original intent of the questions, and (2) balancing \emph{exploration efficiency}, as exhaustive search within the perturbation space is computationally intensive. We address these challenges by enforcing semantic constraints and implementing efficient pruning strategies, thereby achieving an optimal balance between uncovering CDV and ensuring computational feasibility.


% length constraints
% Although this multi-stage approach provides thorough coverage, three principal challenges arise: (1) preventing \emph{semantic shift}, where overzealous modifications alter the core meaning, (2) controlling the \emph{computational cost} of exploring and validating a large number of adversarial examples, and (3) maintaining \emph{perturbation diversity} to expose different types of vulnerabilities. 
% We address these challenges through a comprehensive strategy: enforcing similarity thresholds to retain critical context, implementing efficient early stopping mechanisms (diversity control and performance-based pruning), and utilizing a simulation-based reward system to evaluate perturbation quality. 
% Our method employs both breadth-first exploration to discover diverse vulnerabilities and depth-first pruning to maintain computational efficiency, achieving a balance between coverage and semantic fidelity.


%to be refine
% Discovering vulnerabilities in LLMs involves several challenges, including avoiding semantic drift and curbing computational overhead. 
% To address the challenge of avoiding semantic drift, we impose a semantic similarity metric and a length ratio constraint, ensuring the perturbed question retains its original meaning without excessively inflating context. 
% For computational efficiency, we implement early stopping strategies such as diversity control and performance-based pruning to terminate unproductive exploration, and integrate a pretrained classifier to pre-filter trivial or low-potential questions, focusing resources on more promising candidates.


% Uncovering vulnerabilities in LLMs is crucial for improving their robustness and evaluation methodologies, as it enables researchers to identify specific weaknesses in the models \citep{huang2024trustllm, ye2024justice}. Pinpointing these weaknesses allows targeted enhancements in model performance by addressing specific failure modes and improving robustness under adversarial or ambiguous conditions \citep{chaudhary2024towards}. In addition, this process offers a fresh perspective on traditional LLM evaluation \citep{raina2024llm}. Specifically, assessing a model's understanding of a particular knowledge point solely through standard, directly related questions can lead to misdirected or biased results.


% Uncovering vulnerabilities in LLMs involves the following challenges. 1) Relying on human experts to identify weaknesses is both costly and time-intensive \citep{das2024under}. Experts might spend hours manually crafting adversarial prompts or analyzing model outputs to uncover biases or logical fallacies \citep{wei2024jailbroken,zeng2024johnny}. To address this, we leverage LLMs' powerful capabilities \citep{minaee2024large} to explore and expose their own vulnerabilities autonomously. 2) Standard vulnerability detection approaches using single-step interactions with LLMs often fail to generate sufficiently challenging problems \citep{zhu2024dynamic}, as vulnerabilities may only become apparent through continuous probing that builds upon previous responses \citep{sun2024multi}. Our framework addresses this through an iterative refinement process for thorough vulnerability identification. 3) The identification process requires extensive exploration across different question types and scenarios \citep{chen2024tree}. To address this, our framework incorporates heuristic-driven optimization strategies to efficiently reduce resource consumption while ensuring the generation of high-quality problems.

% \yue{Where is the method overview?}

\subsection{Problem Formulation}

Let \( D = \{P_1, P_2, \dots, P_N\} \) denote a dataset consisting of \( N \) multiple-choice problem instances, where each instance is represented as a tuple:
\begin{equation}
    P = \langle Q, A_\text{gt}, \mathcal{A}_\text{inc} \rangle,
\end{equation}
with \( Q \) denoting the question, \( A_\text{gt} \) the ground truth answer, and \( \mathcal{A}_\text{inc} \) a set of incorrect answers. Given a victim model \( M \), our goal is to construct a perturbed dataset \( D' = \{P'_1, P'_2, \dots, P'_N\} \), where each perturbed instance \( P' = \langle Q', A_\text{gt}, \mathcal{A}_\text{inc} \rangle \) is obtained by applying a perturbation \( \Delta Q \) to the question \( Q \), such that:
\begin{equation}
\label{equ:Q}
Q' = Q + \Delta Q.
\end{equation}
Our  aim is to optimize the perturbation \( \Delta Q \) to minimize the accuracy of \( M \) on \( D' \), while ensuring semantic consistency and length constraints between \( Q \) and \( Q' \). Here, semantic consistency is determined by a binary classifier \( S \), which outputs \( S(Q, Q') \in \{0, 1\} \), where \( S(Q, Q') = 1 \) indicates no semantic shift. Formally, the problem is expressed as:
\begin{equation}
\label{equ:constraint}
\begin{aligned}
\min_{\Delta Q} & \quad \mathbb{E}_{P \sim D} \Big[ \mathcal{L}_\text{accuracy}(M, Q') \Big], \\
\text{s.t.} & \quad S(Q, Q') = 1, \quad \frac{\text{len}(Q')}{\text{len}(Q)} \leq \lambda,
\end{aligned}
\end{equation}

Here, \( S \) ensures that the perturbation \( \Delta Q \) does not lead to a semantic shift, while the length ratio constraint \(\lambda\) ensures \( Q' \) remains within acceptable bounds compared to the original~\( Q \). This   constraint is necessary  because  recent studies show that LLMs experience performance degradation in long context scenarios \citep{bai2023longbench}.  To  prevent excessive length expansion in  $Q'$, we introduce a length constraint, where \( \lambda \) is an upper bound on the relative length of \( Q' \) compared to \( Q \).

If the output \( Q' \) does not satisfy the constraints in \autoref{equ:constraint}, it is discarded, and a new perturbation \( \Delta Q \) is generated by re-prompting the proxy model.

% \textbf{Human evaluation.} \yue{Move this paragraph into experiment section.} To verify that the perturbations \( \Delta Q \) do not introduce significant semantic shifts and that the answers remain consistent, we conducted a human evaluation study. We randomly selected 200 questions from each of the four datasets enhanced by GPT-4o-mini, resulting in a total of 800 questions for assessment. Twenty human experts (each holding at least a bachelor's degree) were divided into two groups to participate in the evaluation. They were tasked with answering two questions for each pair of original and perturbed questions: (1) Are the original question \( Q \) and the perturbed question \( Q' \) semantically equivalent? (2) Does the answer to the perturbed question remain consistent with the original question's answer? The evaluators provided simple "Yes" or "No" responses. The results are summarized in \autoref{tab:human_evaluation}.

% The evaluation demonstrates that our method effectively preserves the semantic content of the original questions after perturbation, with high percentages of semantic equivalence and answer consistency across all datasets.


\subsection{Error-Guided Perturbation Generation}

The perturbation \( \Delta Q \) is generated using a \textbf{proxy model}, denoted as \( P_\text{proxy} \). The proxy model is prompted with the original problem instance \( P = \langle Q, A_\text{gt}, \mathcal{A}_\text{inc} \rangle \) and tasked with generating a modified question \( Q' \) defined in \autoref{equ:Q}, where \( \Delta Q \) represents the perturbation introduced by \( P_\text{proxy} \). The perturbation process is formalized as:
\begin{equation}
\Delta Q = P_\text{proxy}(Q, A_\text{gt}, \mathcal{A}_\text{inc}),
\end{equation}
where \( P_\text{proxy} \) generates \( \Delta Q \) based on a predefined prompt designed to guide the proxy model in producing modifications that increase the likelihood of the victim model \( M \) selecting an incorrect answer \( a_\text{inc} \in \mathcal{A}_\text{inc} \) (i.e., lead model to response with an error).




\subsection{Tree-Based Perturbation Exploration}
\label{sec:value_function}
We employ a tree-based simulation-driven method to optimize perturbations by heuristically exploring the search space. A priority queue is maintained to store nodes ordered by their value \( \mathcal{V}(P') \), with the highest-value node dequeued and expanded iteratively using \( P_\text{proxy} \) to identify high-potential vulnerabilities.

\textbf{Simulation For Measuring Perturbation Quality.} Firstly, we aim to design the {reward of perturbed questions to measure their value. For a given problem instance \( P = \langle Q, A_\text{gt}, \mathcal{A}_\text{inc} \rangle \), the simulation process evaluates the quality of a perturbation by estimating the success rate of the victim model \( M \) on \( P \). Let \( y \sim M(y \mid P) \) represent the output of the model \( M \) when queried on \( P \). During a single simulation, the success rate \( r_M(P) \) is computed by sampling \( n \) model outputs:
\begin{equation}
r_M(P) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}\{y_i = A_\text{gt}\}, \quad y_i \sim M(y \mid P),
\end{equation}
where \( \mathbb{I}\{\cdot\} \) is an indicator function that returns 1 if the model’s output \( y_i \) matches the ground truth answer \( A_\text{gt} \), and 0 otherwise. The success rate \( r_M(P) \) quantifies the likelihood of the model producing the correct answer under the given perturbation.

A perturbed problem \( P' = \langle Q', A_\text{gt}, \mathcal{A}_\text{inc} \rangle \) is considered successfully perturbed if \( r_M(P') = 0\), indicating that the model fails to produce the correct answer in all sampled outputs. The simulation process computes a value \( \mathcal{V}(P') \) for the node corresponding to \( P' \) in the tree-based search:
\begin{equation}
\mathcal{V}(P') = \exp\left(\frac{\alpha}{r_M(P')}\right) \cdot \text{depth}^{-\gamma}, \quad \text{s.t. } r_M(P') \neq 0,
\end{equation}
where \( \alpha \) and \( \gamma \) are scaling constants, \( r_M(P') \) is the success rate of the victim model \( M \) on \( P' \), and \( \text{depth} \) represents the recursion depth of the node in the search tree. For the question with \( r_M(P')=0 \), we add it into the candidate problem list $L$ for storing the problems considered to be successfully ``frustrate'' LLMs. 

The simulation process systematically estimates \( \mathcal{V}(P') \), prioritizing perturbed problems with lower success rates \( r_M(P') \), which correspond to higher potential vulnerabilities in the model. Simultaneously, the factor \( \text{depth}^{-\gamma} \) discourages deeper recursions in the search tree, ensuring computational efficiency. High-value nodes with large \( \mathcal{V}(P') \) scores are prioritized in the following tree-based search. 


\textbf{Tree-Based Search.} For the tree-based search, the process begins with a root \( P_\text{root} = \langle Q, A_\text{gt}, \mathcal{A}_\text{inc} \rangle \). A priority queue \( \mathcal{Q} \) is maintained, where each node \( P' \) is ordered by its value \( \mathcal{V}(P') \) in descending order. Initially, the root node is added to the queue as $\mathcal{Q} \gets \mathcal{Q} \cup \{P_\text{root}\}$. At each iteration, the node \( P' \) with the highest value \( \mathcal{V}(P') \) is dequeued for exploration:
\begin{equation}
P' = \arg\max_{P \in \mathcal{Q}} \mathcal{V}(P), \quad \mathcal{Q} \gets \mathcal{Q} \setminus \{P'\}.
\end{equation}

The proxy model \( P_\text{proxy} \) generates \( k = |\mathcal{A}_\text{inc}| \) child nodes for \( P' \), corresponding to perturbations \( \Delta Q_j \) derived from each incorrect candidate answer \( a_\text{inc} \in \mathcal{A}_\text{inc} \):
\begin{equation}
Q'_j = Q' + \Delta Q_j, \quad P'_j = \langle Q'_j, A_\text{gt}, \mathcal{A}_\text{inc} \rangle, \quad j = 1, 2, \dots, k.
\end{equation}
Each child node \( P'_j \) is evaluated by a simulation-driven method to compute its value \( \mathcal{V}(P'_j) \), and the child nodes are added to the priority queue:
\begin{equation}
\mathcal{Q} \gets \mathcal{Q} \cup \{P'_1, P'_2, \dots, P'_k\}.
\end{equation}
The search iteratively repeats this searching process, dynamically expanding the highest-value node and exploring the perturbation space. 

\textbf{Why not Monte Carlo Tree Search?} Monte Carlo Tree Search (MCTS) \citep{browne2012survey} has been widely used in recent studies to perform simulations powered by LLMs, achieving remarkable performance \citep{zhang2024rest, wang2024seed, xie2024monte, guan2025rstar}. However, MCTS is not suitable for our task due to its focus on balancing exploration (searching broadly across the tree) and exploitation (focusing on promising branches). In our context, such a balance is unnecessary because the width of the tree is inherently fixed, dictated by the number of incorrect answer candidates $|\mathcal{A}_\text{inc}|$. Moreover, MCTS introduces computational overhead by maintaining dynamic exploration strategies, which is impractical given the predefined structure and requirements of our method. Therefore, we opt for a simpler and more task-specific tree design that aligns directly with the properties of our problem.


% \PYB{I think we should write another paragraph on "Why not use LLMs to create CDV examples" }

\subsection{Efficiency Strategies}

\textbf{Early Stopping Strategies.} To reduce computational costs during the search process, we employ two early stopping strategies: diversity control and performance-based pruning.

The first strategy, diversity control, limits the number of child nodes considered at each search step. For a node \( P' \), if the number of child nodes \( P'_j \) satisfying \( r_M(P'_j) = 0 \) exceeds a predefined threshold \( n_1 \), we add the top \( n_1 \) child nodes to the candidate problem list \( L \) and directly pass this branch without further exploration. Formally, let \( \mathcal{C}(P') \) represent the set of child nodes of \( P' \), and define:
\begin{equation}
\mathcal{C}_0(P') = \{P'_j \in \mathcal{C}(P') \mid r_M(P'_j) = 0\}.
\end{equation}
If \( |\mathcal{C}_0(P')| > n_1 \), we update the candidate problem list \( L \) as:
\begin{equation}
L \gets L \cup \mathcal{C}_0(P')[1:n_1],
\end{equation}
where \( [1:n_1] \) indicates the top \( n_1 \) nodes according to their values \( \mathcal{V}(P'_j) \). The branch corresponding to \( P' \) is then terminated.


The second strategy is performance-based pruning, which bypasses nodes where further exploration is unlikely to yield meaningful results. For a node \( P' \), if all its child nodes satisfy \( r_M(P'_j) = 1 \), the node \( P' \) is skipped. Formally, if:
\begin{equation}
r_M(P'_j) = 1, \quad \forall P'_j \in \mathcal{C}(P'),
\end{equation}
then \( P' \) is pruned from the search.



Additionally, if for \( l \) consecutive levels of the search tree, the minimum success rate \( \min(r_M(P')) \) at each level increases monotonically from the top level to the bottom level, the corresponding node is bypassed. Let \( \text{level}_i \) represent the set of nodes at level \( i \) of the search tree, and define $m_i = \min_{P' \in \text{level}_i} r_M(P')$. If:
\begin{equation}
m_{i+1} > m_i, \quad \forall i \in \{1, 2, \dots, l-1\},
\end{equation}
then the corresponding branch of the search tree is pruned.



\textbf{Problem Filtering via Classifier.} To reduce search costs, a classifier \( C(Q) \) is used to filter out questions with low potential to become effective problem candidates (e.g., the extremely easy question ``What is the highest mountain in the world?''). The classifier is trained on previously searched questions, \( \mathcal{D}_\text{train} = \{(Q_i, y_i)\}_{i=1}^{N} \), where \( y_i \in \{0, 1\} \) indicates whether \( Q_i \) successfully exposes a vulnerability in the victim model \( M \).

For each new question \( Q \), the classifier computes \( p(y=1 \mid Q) = C(Q) \). Questions satisfying \( p(y=1 \mid Q) < \tau_C \), where \( \tau_C \) is a predefined threshold, are discarded:
\begin{equation}
\mathcal{Q} \gets \mathcal{Q} \setminus \{Q \mid p(y=1 \mid Q) < \tau_C\}
\end{equation}


We show the overall algorithm in Algorithm \ref{alg:perturbation_optimization}.

\begin{algorithm}[h!]
\small
\caption{Overall Algorithm}
\label{alg:perturbation_optimization}
\KwIn{Dataset \( D = \{P_1, P_2, \dots, P_N\} \), Proxy model \( P_\text{proxy} \), Victim model \( M \), Thresholds \(\lambda, \tau_C \), Diversity limit \( n_1 \)}
\KwOut{Candidate problem list \( L \)}

Initialize priority queue \( \mathcal{Q} \gets \emptyset \) and candidate list \( L \gets \emptyset \)\;

\ForEach{\( P = \langle Q, A_\text{gt}, \mathcal{A}_\text{inc} \rangle \in D \)}{
    \If{\( p(y=1 \mid Q) = C(Q) < \tau_C \)}{
        \textbf{continue} \tcp*[h]{Filter low-potential questions using classifier}\;
    }
    \If{\( r_M(P) \) = 0}{
        Add \(P\) to \(L\)\;
        
        \textbf{continue}
    }\;
    Add root node \( P \) to \( \mathcal{Q} \)\;
}

\While{\( \mathcal{Q} \neq \emptyset \)}{
    Pop \( P' = \arg\max_{P \in \mathcal{Q}} \mathcal{V}(P) \), \( \mathcal{Q} \gets \mathcal{Q} \setminus \{P'\} \)\;

    Generate \( k = |\mathcal{A}_\text{inc}| \) child nodes for \( P' \) using \( P_\text{proxy} \)\;
    
    \For{each child node \( P'_j \)}{
        Compute semantic shift \( S(P, P'_j) \) and length ratio \( \text{len}(P'_j)/\text{len}(P) \) \tcp*[h]{Semantic shift check and computing length ratio}\; 
    
        \If{\( S(P, P'_j) = 1 \) and \( \text{len}(P'_j)/\text{len}(P) \leq \lambda \)}{
            Compute value \( \mathcal{V}(P'_j) \)\;
        } \Else{
            Discard \( P'_j \)\;
        }
    }

    
    \If{\( |\mathcal{C}_0(P')| > n_1 \)}{
        Add top \( n_1 \) nodes from \( \mathcal{C}_0(P') \) to \( L \)
        
        terminate branch\;
        
        \textbf{continue}\;
    }
    
    \If{\( r_M(P'_j) = 1, \, \forall P'_j \)}{
        \textbf{continue} \tcp*[h]{Skip nodes where all children are unpromising}\;
    }
    
    Add \( P'_j \) nodes to \( \mathcal{Q} \)\;
    
    \If{\( m_{i+1} > m_i, \, \forall i \in \{1, 2, \dots, l-1\} \)}{
        \textbf{continue} \tcp*[h]{Prune monotonically increasing success rate branches}\;
    }
}

\Return \( L \)\;
\end{algorithm}