\section{Related Work}

% \textcolor{red}{
% \textbf{LLM Vulnerability.}
% I think it is more relevant to discuss the previous studies of  LLM Vulnerability, instead of "LLM Evaluation". Especially, the related work of jailbreaks, prompt injection, and any other related work of adversarial prompts.


\textbf{LLM Vulnerability.} Previous research has extensively explored various vulnerabilities in LLMs. Jailbreak attacks \citep{wei2024jailbroken, zou2023universal, huang2024obscureprompt} have highlighted security risks, driving efforts to develop more trustworthy LLMs \citep{huang2024trustllm, liu2023trustworthy, huang2023trustgpt}. Hallucinations \citep{li2023halueval} remains a persistent issue in various scenarios like LLM-based agents \citep{zhang2024toolbehonest, huang2023metatool}, leading to incorrect or misleading responses \cite{10.1145/3589335.3651509}. Additionally, LLMs are highly susceptible to prompt injection attacks \citep{liu2024promptinjection}, where adversarial prompts disguise malicious instructions as benign inputs. \citet{berglund2023reversal} identify the reversal curse in LLMs, exposing their failure to generalize in auto-regressive settings. \citet{sharma2023towards} investigate sycophancy, revealing that human feedback may inadvertently encourage LLMs to align with user beliefs rather than provide truthful responses. Furthermore, studies on LLM honesty \citep{yang2023alignment, gaohonestllm} suggest that LLMs often lack self-awareness regarding their capabilities, contributing to hallucinations and unreliable outputs.


Our study focuses on a novel vulnerability--CDV, in which LLMs lose focus due to contextual distractions. We identify CDV as a fundamental ability-level challenge for LLMs, parallel to the aspects previously explored. Addressing CDV is therefore crucial for enhancing LLMs' reliability alongside the already assessed capabilities.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/pipeline.pdf}
    \caption{The overview of the proposed method. Given a problem, our goal is to automatically transform it into a CDV example. Initially, a classifier filters the problems to identify potential candidates that are easy to perturb. Next, the method employs a tree-based search, supported by a priority queue to manage the search priorities of individual nodes. Using Error-Guided Perturbation (EGP) generation, the pipeline efficiently and automatically produces effective CDV examples.}
    \label{fig:pipeline}
    \vspace{-1em}
\end{figure*}


% %\textbf{LLM Evaluation.}
% Extensive efforts have been made to evaluate the capabilities of LLMs across various domains. These evaluations are from holistic assessment \citep{liang2022holistic, huang2024trustllm} to specific tasks. Previous studies mainly focus on traditional NLP tasks such as sentiment analysis \citep{wang2023chatgpt,zhang2023sentiment} and translation \citep{yao2023benchmarking,zhang2023prompting}, mathematical problem-solving 

% \citep{hendrycks2021measuring,liu2024mathbench}, and code generation \citep{jain2024livecodebench}. Comprehensive utlity-driven benchmarks like MMLU \citep{hendryckstest2021,hendrycks2021ethics}, HellaSwag \citep{zellers2019hellaswag}, and HumanEval \citep{chen2021evaluating} have been established and widely used to assess the foundational abilities of LLM. Lots of recent works are studying the reliability and trustworthiness evaluation of LLMs, including jailbreak attack \citep{mazeika2024harmbench}, hallucination \citep{li2023halueval}, and robustness \citep{zhu2023promptbench, bai2024longbench2}.

%In addition to these evaluated capabilities, 
% \textcolor{red}{Our study focuses on a novel vulnerability in which LLMs lose focus due to contextual distractions.}


% \textbf{LLM Evaluation.}
% Extensive evaluation efforts have been conducted to comprehensively understand LLM capabilities. These evaluations encompass traditional NLP tasks, including sentiment analysis \citep{wang2023chatgpt,zhang2023sentiment} and translation \citep{yao2023benchmarking,zhang2023prompting}. In the mathematical domain, numerous benchmarks have emerged \citep{hendrycks2021measuring,liu2024mathbench}. Various benchmarks for evaluating code generation capabilities have been proposed, including HumanEval \citep{chen2021evaluating} and LiveCodeBench \citep{jain2024livecodebench}. Comprehensive capability assessments have also been established, including MMLU \citep{hendryckstest2021,hendrycks2021ethics} and LongBench \citep{bai2024longbench2}. With the introduction of LLM-as-a-Judge \citep{zheng2023judging}, researchers have begun leveraging LLMs to evaluate other LLMs, significantly reducing the costs associated with traditional human evaluation.

% \textbf{Robustness of LLMs.}
% The robustness of LLMs refers to their ability to maintain performance levels across diverse scenarios \citep{huang2024trustllm}. Several comprehensive benchmarks have been developed to evaluate this crucial aspect, including TrustLLM \citep{huang2024trustllm}, HEIM \citep{liang2022holistic}, DecodingTrust \citep{wang2023decodingtrust}, and PromptBench \citep{zhu2023promptbench}, which investigate the resilience of LLMs to adversarial prompts. Researchers have also examined robustness in various practical applications, spanning domains such as embodied intelligence \citep{liu2024exploring}, code generation \citep{zhong2023study,sarker2024syntactic}, and LLM-as-a-Judge \citep{raina2024llm}. Furthermore, efforts have been made to understand and enhance LLM robustness \citep{zhao2024improving,fan2024towards}.
% In this work, we address the robustness of LLMs in handling contextual distractions.

% \PYB{Please make sure you leave a space between text and ref}

\textbf{Tree-Based Search for LLMs.}
Tree search methods have shown considerable potential in enhancing the exploratory capabilities of LLMs \citep{zhang2023planning,hu2024uncertainty}. Tree Prompting \citep{singh2023tree} offers a novel approach that solves problems by constructing decision prompt trees. \citet{zhou2023language} and \citet{koh2024tree} treat the language model as a general agent, utilizing tree-based search to enhance the model capability of both reasoning and planning. Building upon the Chain-of-Thought (CoT) \citep{wei2022chain}, Tree of Thoughts (ToT) \citep{yao2024tree} extends this idea by decomposing reasoning tasks into sequential steps and exploring potential thoughts at each stage. Moreover, \citet{li2024codetree} proposes CodeTree to efficiently explore the search space in different stages during code generation.

% Our method constructs a search tree where the nodes do not contain any of the LLM's responses or thought steps. Instead, the state of each node corresponds to the continually evolving question itself. In our search procedure, we assess the value of the questions at each node based on the LLM's responses, using this evaluation to heuristically guide the search.

% \textcolor{red}{Our usage of tree-based search ...Please add a brief discussion to highlight the difference between this work and other related work using trees.}

Our proposed method employs a tree-based search to systematically explore potential perturbations of the original input, aiming to automatically generate CDV samples that effectively challenge LLMs.




% \textbf{Tree-Based Search and Reasoning.}
% Tree-based search algorithms have played a crucial role in artificial intelligence, from classical methods like Monte Carlo Tree Search (MCTS) \citep{browne2012survey} to their modern applications with LLMs. In the context of LLMs, tree-based search has been widely adopted for reasoning enhancement. Tree Prompting \citep{singh2023tree} introduced a novel approach that solves problems by constructing decision prompt trees. Tree of Thoughts \citep{yao2024tree} proposed a tree-structured reasoning framework that explores diverse solution paths and outcomes. This methodology has inspired various extensions, including Graph of Thoughts \citep{besta2024graph}, Reasoning on Graphs \citep{luo2023reasoning}, and Skeleton of Thought \citep{ning2023skeleton}, which further enhance search efficiency and LLMs' reasoning capabilities.

