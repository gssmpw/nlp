@article{wang2024epistemic,
  title={Epistemic Uncertainty Quantification For Pre-trained Neural Network},
  author={Wang, Hanjing and Ji, Qiang},
  journal={arXiv preprint arXiv:2404.10124},
  year={2024}
}

@misc{yang2024spectralconditionfeaturelearning,
      title={A Spectral Condition for Feature Learning}, 
      author={Greg Yang and James B. Simon and Jeremy Bernstein},
      year={2024},
      eprint={2310.17813},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17813}, 
      note={},
}

@misc{yang2022featurelearninginfinitewidthneural,
      title={Feature Learning in Infinite-Width Neural Networks}, 
      author={Greg Yang and Edward J. Hu},
      year={2022},
      eprint={2011.14522},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.14522}, 
}

@misc{everett2024scalingexponentsparameterizationsoptimizers,
      title={Scaling Exponents Across Parameterizations and Optimizers}, 
      author={Katie Everett and Lechao Xiao and Mitchell Wortsman and Alexander A. Alemi and Roman Novak and Peter J. Liu and Izzeddin Gur and Jascha Sohl-Dickstein and Leslie Pack Kaelbling and Jaehoon Lee and Jeffrey Pennington},
      year={2024},
      eprint={2407.05872},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.05872}, 
}

@misc{yang2023tensorprogramsvifeature,
      title={Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks}, 
      author={Greg Yang and Dingli Yu and Chen Zhu and Soufiane Hayou},
      year={2023},
      eprint={2310.02244},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2310.02244}, 
}

@misc{bordelon2023depthwisehyperparametertransferresidual,
      title={Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit}, 
      author={Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
      year={2023},
      eprint={2309.16620},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2309.16620}, 
}

@misc{large2024scalableoptimizationmodularnorm,
      title={Scalable Optimization in the Modular Norm}, 
      author={Tim Large and Yang Liu and Minyoung Huh and Hyojin Bahng and Phillip Isola and Jeremy Bernstein},
      year={2024},
      eprint={2405.14813},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14813}, 
}

@misc{hoff2010separablecovariancearraystucker,
      title={Separable covariance arrays via the Tucker product, with applications to multivariate relational data}, 
      author={Peter D. Hoff},
      year={2010},
      eprint={1008.2169},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1008.2169}, 
}

@article{manceur2013tensor,
title = {Maximum likelihood estimation for the tensor normal distribution: Algorithm, minimum sample size, and empirical bias and dispersion},
journal = {Journal of Computational and Applied Mathematics},
volume = {239},
pages = {37-49},
year = {2013},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2012.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0377042712003810},
author = {Ameur M. Manceur and Pierre Dutilleul},
keywords = {Empirical bias and dispersion, Maximum likelihood estimation, Minimum sample size, Multi-stage algorithm, Separable variance–covariance structure, Tensor normal distribution},
abstract = {Recently, there has been a growing interest in the analysis of multi-dimensional data arrays (e.g. when a univariate response is sampled in 3-D space or when a multivariate response is sampled in time and 2-D space). In this article, we scrutinize the problem of maximum likelihood estimation (MLE) for the tensor normal distribution of order 3 or more, which is characterized by the separability of its variance–covariance structure; there is one variance–covariance matrix per dimension. In the 3-D case, the system of likelihood equations for the three variance–covariance matrices has no analytical solution, and therefore needs to be solved iteratively. We studied the convergence of an iterative three-stage algorithm (MLE-3D) that we propose for this, determined the minimum sample size required for matrix estimates to exist, and computed by simulation the empirical bias and dispersion of the Kronecker product of the three variance–covariance matrix estimators in eight scenarios. We found that the standardized bias and a matrix measure of dispersion decrease monotonically and tend to vanish with increasing sample size, so the Kronecker product estimator is consistent. An example with 3-D spatial measures of glucose content in the brain is also presented. Finally, results are discussed and the 4-D case is presented with simulation results in an appendix. Software is available for interested users.}
}

@misc{cottier2024risingcoststrainingfrontier,
      title={The rising costs of training frontier AI models}, 
      author={Ben Cottier and Robi Rahman and Loredana Fattorini and Nestor Maslej and David Owen},
      year={2024},
      eprint={2405.21015},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2405.21015}, 
}

@misc{bernstein2021distanceneuralnetworksstability,
      title={On the distance between two neural networks and the stability of learning}, 
      author={Jeremy Bernstein and Arash Vahdat and Yisong Yue and Ming-Yu Liu},
      year={2021},
      eprint={2002.03432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.03432}, 
}

@misc{bordelon2022selfconsistentdynamicalfieldtheory,
      title={Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2022},
      eprint={2205.09653},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.09653}, 
}

@article{Mei_2018,
   title={A mean field view of the landscape of two-layer neural networks},
   volume={115},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.1806579115},
   DOI={10.1073/pnas.1806579115},
   number={33},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
   year={2018},
   month=jul }

@misc{yang2021tensorprogramsiwide,
      title={Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes}, 
      author={Greg Yang},
      year={2021},
      eprint={1910.12478},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1910.12478}, 
}

@misc{yang2020tensorprogramsiineural,
      title={Tensor Programs II: Neural Tangent Kernel for Any Architecture}, 
      author={Greg Yang},
      year={2020},
      eprint={2006.14548},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2006.14548}, 
}

@misc{yang2021tensorprogramsiiineural,
      title={Tensor Programs III: Neural Matrix Laws}, 
      author={Greg Yang},
      year={2021},
      eprint={2009.10685},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2009.10685}, 
}

@misc{yang2022tensorprogramsvtuning,
      title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}, 
      author={Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
      year={2022},
      eprint={2203.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.03466}, 
}

@misc{haas2024boldsymbolmumathbfp2effectivesharpnessaware,
      title={$\boldsymbol{\mu}\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling}, 
      author={Moritz Haas and Jin Xu and Volkan Cevher and Leena Chennuru Vankadara},
      year={2024},
      eprint={2411.00075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.00075}, 
}
@misc{bordelon2023dynamicsfinitewidthkernel,
      title={Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2023},
      eprint={2304.03408},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2304.03408}, 
}

@misc{bordelon2024infinitelimitsmultiheadtransformer,
      title={Infinite Limits of Multi-head Transformer Dynamics}, 
      author={Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan},
      year={2024},
      eprint={2405.15712},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.15712}, 
}

@misc{yaida2022metaprincipledfamilyhyperparameterscaling,
      title={Meta-Principled Family of Hyperparameter Scaling Strategies}, 
      author={Sho Yaida},
      year={2022},
      eprint={2210.04909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.04909}, 
}

@article{Rotskoff_2022,
   title={Trainability and Accuracy of Artificial Neural Networks: An Interacting Particle System Approach},
   volume={75},
   ISSN={1097-0312},
   url={http://dx.doi.org/10.1002/cpa.22074},
   DOI={10.1002/cpa.22074},
   number={9},
   journal={Communications on Pure and Applied Mathematics},
   publisher={Wiley},
   author={Rotskoff, Grant and Vanden‐Eijnden, Eric},
   year={2022},
   month=jul, pages={1889–1935} }

@misc{sirignano2021meanfieldanalysisdeep,
      title={Mean Field Analysis of Deep Neural Networks}, 
      author={Justin Sirignano and Konstantinos Spiliopoulos},
      year={2021},
      eprint={1903.04440},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/1903.04440}, 
}

@article{Geiger_2020,
   title={Disentangling feature and lazy training in deep neural networks},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc4de},
   DOI={10.1088/1742-5468/abc4de},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
   year={2020},
   month=nov, pages={113301} }

@misc{bernstein2024modulardualitydeeplearning,
      title={Modular Duality in Deep Learning}, 
      author={Jeremy Bernstein and Laker Newhouse},
      year={2024},
      eprint={2410.21265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.21265}, 
}

@misc{bernstein2024oldoptimizernewnorm,
      title={Old Optimizer, New Norm: An Anthology}, 
      author={Jeremy Bernstein and Laker Newhouse},
      year={2024},
      eprint={2409.20325},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.20325}, 
}

@article{wang2023generative,
  title={Generative AI for Math: Part I--MathPile: A Billion-Token-Scale Pretraining Corpus for Math},
  author={Wang, Zengzhi and Xia, Rui and Liu, Pengfei},
  journal={arXiv preprint arXiv:2312.17120},
  year={2023}
}

@misc{cold-french-law,
  author = {Harvard Library Innovation Lab, Casetext - Part of Thomson Reuters},
  title = {COLD French Law Dataset},
  month = May,
  year = 2024,
  url = {https://huggingface.co/datasets/harvard-lil/cold-french-law}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@misc{vaswani2017attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{dehghani2023scalingvisiontransformers22,
      title={Scaling Vision Transformers to 22 Billion Parameters}, 
      author={Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Patrick Collier and Alexey Gritsenko and Vighnesh Birodkar and Cristina Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Pavetić and Dustin Tran and Thomas Kipf and Mario Lučić and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},
      year={2023},
      eprint={2302.05442},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.05442}, 
}

@misc{merity2016pointersentinelmixturemodels,
      title={Pointer Sentinel Mixture Models}, 
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1609.07843}, 
}

@misc{wolf2020huggingfacestransformersstateoftheartnatural,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.03771}, 
}

@misc{he2015delvingdeeprectifierssurpassing,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1502.01852}, 
}

@misc{ba2016layernormalization,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}

@misc{ioffe2015batchnormalizationacceleratingdeep,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.03167}, 
}

@Techreport{krizhevsky2009learning,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
 address = {Toronto, Ontario},
 institution = {University of Toronto},
 number = {0},
 publisher = {Technical report, University of Toronto},
 title = {Learning multiple layers of features from tiny images},
 year = {2009},
 title_with_no_special_chars = {Learning multiple layers of features from tiny images}
}

@misc{wang2024setadamwsweightdecay,
      title={How to set AdamW's weight decay as you scale model and dataset size}, 
      author={Xi Wang and Laurence Aitchison},
      year={2024},
      eprint={2405.13698},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13698}, 
}

@book{gupta1999matrix,
  title={Matrix Variate Distributions},
  author={Gupta, A.K. and Nagar, D.K.},
  isbn={9781584880462},
  lccn={99040291},
  series={Monographs and Surveys in Pure and Applied Mathematics},
  url={https://books.google.co.uk/books?id=PQOYnT7P1loC},
  year={1999},
  publisher={Taylor \& Francis}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@misc{noci2024superconsistencyneuralnetwork,
      title={Super Consistency of Neural Network Landscapes and Learning Rate Transfer}, 
      author={Lorenzo Noci and Alexandru Meterez and Thomas Hofmann and Antonio Orvieto},
      year={2024},
      eprint={2402.17457},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.17457}, 
}

@misc{martens2015kfac,
      title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
      author={James Martens and Roger Grosse},
      year={2015},
      eprint={1503.05671},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1503.05671}, 
}

@misc{hayou2021stableresnet,
      title={Stable ResNet}, 
      author={Soufiane Hayou and Eugenio Clerico and Bobby He and George Deligiannidis and Arnaud Doucet and Judith Rousseau},
      year={2021},
      eprint={2010.12859},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.12859}, 
}

@misc{hayou2023widthdepthlimitscommute,
      title={Width and Depth Limits Commute in Residual Networks}, 
      author={Soufiane Hayou and Greg Yang},
      year={2023},
      eprint={2302.00453},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2302.00453}, 
}