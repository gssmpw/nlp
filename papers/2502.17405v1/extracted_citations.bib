@article{Geiger_2020,
   title={Disentangling feature and lazy training in deep neural networks},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc4de},
   DOI={10.1088/1742-5468/abc4de},
   number={11},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
   year={2020},
   month=nov, pages={113301} }

@article{Mei_2018,
   title={A mean field view of the landscape of two-layer neural networks},
   volume={115},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.1806579115},
   DOI={10.1073/pnas.1806579115},
   number={33},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
   year={2018},
   month=jul }

@article{Rotskoff_2022,
   title={Trainability and Accuracy of Artificial Neural Networks: An Interacting Particle System Approach},
   volume={75},
   ISSN={1097-0312},
   url={http://dx.doi.org/10.1002/cpa.22074},
   DOI={10.1002/cpa.22074},
   number={9},
   journal={Communications on Pure and Applied Mathematics},
   publisher={Wiley},
   author={Rotskoff, Grant and Vanden‐Eijnden, Eric},
   year={2022},
   month=jul, pages={1889–1935} }

@misc{bernstein2021distanceneuralnetworksstability,
      title={On the distance between two neural networks and the stability of learning}, 
      author={Jeremy Bernstein and Arash Vahdat and Yisong Yue and Ming-Yu Liu},
      year={2021},
      eprint={2002.03432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.03432}, 
}

@misc{bernstein2024modulardualitydeeplearning,
      title={Modular Duality in Deep Learning}, 
      author={Jeremy Bernstein and Laker Newhouse},
      year={2024},
      eprint={2410.21265},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.21265}, 
}

@misc{bernstein2024oldoptimizernewnorm,
      title={Old Optimizer, New Norm: An Anthology}, 
      author={Jeremy Bernstein and Laker Newhouse},
      year={2024},
      eprint={2409.20325},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.20325}, 
}

@misc{bordelon2022selfconsistentdynamicalfieldtheory,
      title={Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2022},
      eprint={2205.09653},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.09653}, 
}

@misc{bordelon2023depthwisehyperparametertransferresidual,
      title={Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit}, 
      author={Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
      year={2023},
      eprint={2309.16620},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2309.16620}, 
}

@misc{bordelon2023dynamicsfinitewidthkernel,
      title={Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2023},
      eprint={2304.03408},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2304.03408}, 
}

@misc{bordelon2024infinitelimitsmultiheadtransformer,
      title={Infinite Limits of Multi-head Transformer Dynamics}, 
      author={Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan},
      year={2024},
      eprint={2405.15712},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.15712}, 
}

@misc{everett2024scalingexponentsparameterizationsoptimizers,
      title={Scaling Exponents Across Parameterizations and Optimizers}, 
      author={Katie Everett and Lechao Xiao and Mitchell Wortsman and Alexander A. Alemi and Roman Novak and Peter J. Liu and Izzeddin Gur and Jascha Sohl-Dickstein and Leslie Pack Kaelbling and Jaehoon Lee and Jeffrey Pennington},
      year={2024},
      eprint={2407.05872},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.05872}, 
}

@misc{haas2024boldsymbolmumathbfp2effectivesharpnessaware,
      title={$\boldsymbol{\mu}\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling}, 
      author={Moritz Haas and Jin Xu and Volkan Cevher and Leena Chennuru Vankadara},
      year={2024},
      eprint={2411.00075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.00075}, 
}

@misc{large2024scalableoptimizationmodularnorm,
      title={Scalable Optimization in the Modular Norm}, 
      author={Tim Large and Yang Liu and Minyoung Huh and Hyojin Bahng and Phillip Isola and Jeremy Bernstein},
      year={2024},
      eprint={2405.14813},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14813}, 
}

@misc{yaida2022metaprincipledfamilyhyperparameterscaling,
      title={Meta-Principled Family of Hyperparameter Scaling Strategies}, 
      author={Sho Yaida},
      year={2022},
      eprint={2210.04909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.04909}, 
}

@misc{yang2020tensorprogramsiineural,
      title={Tensor Programs II: Neural Tangent Kernel for Any Architecture}, 
      author={Greg Yang},
      year={2020},
      eprint={2006.14548},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2006.14548}, 
}

@misc{yang2021tensorprogramsiiineural,
      title={Tensor Programs III: Neural Matrix Laws}, 
      author={Greg Yang},
      year={2021},
      eprint={2009.10685},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2009.10685}, 
}

@misc{yang2021tensorprogramsiwide,
      title={Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes}, 
      author={Greg Yang},
      year={2021},
      eprint={1910.12478},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1910.12478}, 
}

@misc{yang2022featurelearninginfinitewidthneural,
      title={Feature Learning in Infinite-Width Neural Networks}, 
      author={Greg Yang and Edward J. Hu},
      year={2022},
      eprint={2011.14522},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.14522}, 
}

@misc{yang2022tensorprogramsvtuning,
      title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer}, 
      author={Greg Yang and Edward J. Hu and Igor Babuschkin and Szymon Sidor and Xiaodong Liu and David Farhi and Nick Ryder and Jakub Pachocki and Weizhu Chen and Jianfeng Gao},
      year={2022},
      eprint={2203.03466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.03466}, 
}

@misc{yang2023tensorprogramsvifeature,
      title={Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks}, 
      author={Greg Yang and Dingli Yu and Chen Zhu and Soufiane Hayou},
      year={2023},
      eprint={2310.02244},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/2310.02244}, 
}

@misc{yang2024spectralconditionfeaturelearning,
      title={A Spectral Condition for Feature Learning}, 
      author={Greg Yang and James B. Simon and Jeremy Bernstein},
      year={2024},
      eprint={2310.17813},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17813}, 
      note={},
}

