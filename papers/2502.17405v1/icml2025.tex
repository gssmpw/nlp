%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xcolor}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage{placeins}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2025}


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\newcommand{\bracket}[3]{\left#1 #3 \right#2}
\newcommand{\mbracket}[5]{\left#1 #4 \middle#2 #5 \right#3}
\renewcommand{\b}{\bracket{(}{)}}
\newcommand{\sqb}{\bracket{[}{]}}
\newcommand{\abs}{\bracket{\lvert}{\rvert}}
\newcommand{\ab}{\bracket{\langle}{\rangle}}
\newcommand{\cb}{\bracket{\{}{\}}}
\newcommand{\bc}{\mbracket{(}{\vert}{)}}

\newcommand{\lrms}{\bracket{\lVert}{\rVert_\text{RMS}}}
\newcommand{\frob}{\bracket{\lVert}{\rVert_{\mathcal{F}}}}

\newcommand{\tsum}{{\textstyle \sum}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Function-Space Learning Rates}% for Deep Neural Networks}

\begin{document}

\twocolumn[
%\icmltitle{Function-space Learning Rate Matching for Hyperparameter Transfer}
% \icmltitle{Hyperparameter transfer made easy: Function-space Learning Rate Matching}
\icmltitle{Function-Space Learning Rates}% for Deep Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Edward Milsom}{bristol}
\icmlauthor{Ben Anson}{bristol}
\icmlauthor{Laurence Aitchison}{bristol}
\end{icmlauthorlist}

\icmlaffiliation{bristol}{University of Bristol}

\icmlcorrespondingauthor{Edward Milsom}{edward.milsom@bristol.ac.uk}
\icmlcorrespondingauthor{Laurence Aitchison}{laurence.aitchison@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\newcommand{\flerm}{FLeRM}

\begin{abstract}
We consider layerwise function-space learning rates, which measure the magnitude of the change in a neural network's output function in response to an update to a parameter tensor. This contrasts with traditional learning rates, which describe the magnitude of changes in parameter space. We develop efficient methods to measure and set function-space learning rates in arbitrary neural networks, requiring only minimal computational overhead through a few additional backward passes that can be performed at the start of, or periodically during, training. We demonstrate two key applications: (1) analysing the dynamics of standard neural network optimisers in function space, rather than parameter space, and (2) introducing FLeRM (Function-space Learning Rate Matching), a novel approach to hyperparameter transfer across model scales. FLeRM records function-space learning rates while training a small, cheap base model, then automatically adjusts parameter-space layerwise learning rates when training larger models to maintain consistent function-space updates. FLeRM gives hyperparameter transfer across model width, depth, initialisation scale, and LoRA rank in various architectures including MLPs with residual connections and transformers with different layer normalisation schemes.

\end{abstract}


\section{Introduction}
The fundamental purpose of neural network training is to learn a function that maps inputs to desired outputs. However, we typically understand optimisation methods as acting in parameter space, e.g.\ traditional learning rates tell us how much the parameters change during each step rather than the functional impact of those changes. This raises an important question: can we meaningfully quantify and control learning in function space?

We consider the concept of layerwise function-space learning rates which measure the magnitude of change in network output induced by updates to individual parameter tensors. 
Unfortunately, naive approaches to measuring function-space learning rates would be computationally prohibitive. 
We solve this problem by developing a Monte-Carlo estimate to measure function-space learning rates using only a single additional backward pass, which can be performed a handful of times at the start of training (e.g.\ 40), or periodically during training (e.g., once every 100 steps), resulting in negligible computational overhead. We then consider two immediate applications of function-space learning rates.

First, function-space learning rates provide a novel lens for analysing the behavior of standard neural network optimisers \citep[e.g.\ Adam,][]{kingma2014adam}, giving important insights into how different parts of the network contribute to functional changes during training. Second, function-space learning rates enable a new approach to hyperparameter transfer \citep[for previous work see e.g.][]{yang2022featurelearninginfinitewidthneural,bordelon2023depthwisehyperparametertransferresidual, large2024scalableoptimizationmodularnorm}. 
As large language model pre-training can cost millions of dollars in compute \citep{cottier2024risingcoststrainingfrontier}, running extensive hyperparameter sweeps at full scale is impractical. 
Instead, one might hope to optimise hyperparameters on smaller models and transfer them to larger ones. 
However, this is complicated by the fact that optimal learning rates change with model width and depth \citep{yang2022featurelearninginfinitewidthneural,noci2024superconsistencyneuralnetwork, yang2023tensorprogramsvifeature, bordelon2023depthwisehyperparametertransferresidual}.
We address this challenge with FLeRM (Function-space Learning Rate Matching), which maintains consistent function-space learning rates as models are scaled up by automatically adjusting the parameter-space learning rates, thereby keeping the optimal value of the user-defined learning rate hyperparameter stable.

A key advantage of our approach is its flexibility: our methods for measuring and controlling function-space learning rates work with any network architecture and at any point during training. This contrasts with traditional approaches to hyperparameter transfer that often make restrictive assumptions about architectures or initialisation schemes \citep{everett2024scalingexponentsparameterizationsoptimizers, yang2022featurelearninginfinitewidthneural, bordelon2023depthwisehyperparametertransferresidual, large2024scalableoptimizationmodularnorm}. We demonstrate FLeRM's utility across a range of scenarios, including model width scaling, depth scaling, initialisation scale variation, and even LoRA rank adjustment.

\section{Related work}

As far as we are aware, this work is unique in proposing methods to measure and set function-space learning rates in arbitrary neural networks far from initialisation, and using this approach to study the dynamics of optimizers.

There is an existing body of literature on hyperparameter transfer \citep[e.g.\ ][]{yang2022featurelearninginfinitewidthneural,bordelon2023depthwisehyperparametertransferresidual,large2024scalableoptimizationmodularnorm, yaida2022metaprincipledfamilyhyperparameterscaling}. Broadly speaking, these works analytically derive scaling laws for e.g.\ the initialisations and parameter-space learning rates, such that the function-space learning rates do not change as e.g.\ width is increased.
This is radically different from our approach to hyperparameter scaling.
In particular, none of these works provide a mechanism to empirically measure the function-space learning rates in an arbitrary neural network.
% At a high level, prior work takes the approach of theoretically deriving the scaling of the initialisation and learning rates necessary to give similar function-space learning rates for a small set of optimizers .
Furthermore, prior works tend to rely on rigid assumptions such as being close to a specific random initialisation, which we do not make.

Perhaps the earliest and best-known work on hyperparameter scaling is \textmu{}P \citep{yang2022featurelearninginfinitewidthneural}.
\textmu{}P derives how to scale random initialisations and learning rates as you increase model width, such that the function-space learning rates remain asymptotically constant (i.e.\ the magnitude of the activations does not blow up to infinity or shrink to zero as the model gets wider). \textmu{}P has since been extended to depth-scaling \citep{yang2023tensorprogramsvifeature,bordelon2023depthwisehyperparametertransferresidual} and networks trained with sharpness-aware minimisation \citep{haas2024boldsymbolmumathbfp2effectivesharpnessaware}, and is closely related to the mean-field analysis of neural networks grounded in statistical mechanics \citep{Mei_2018,Geiger_2020,bordelon2022selfconsistentdynamicalfieldtheory,Rotskoff_2022}.
Because this approach derives the function-space learning rates analytically, (in contrast to our approach of measuring them), it requires restrictive assumptions, including that the network is wide, and close to a random initialisation. Extending these results to more general cases is made complicated by the fact that they typically rely on heavy-duty mathematical machinery such as Tensor-Programs \citep{yang2021tensorprogramsiwide, yang2020tensorprogramsiineural, yang2021tensorprogramsiiineural, yang2022featurelearninginfinitewidthneural, yang2022tensorprogramsvtuning, yang2023tensorprogramsvifeature} or dynamical mean-field theory \citep{bordelon2022selfconsistentdynamicalfieldtheory, bordelon2023dynamicsfinitewidthkernel, bordelon2023depthwisehyperparametertransferresidual, bordelon2024infinitelimitsmultiheadtransformer}

The full \textmu{}P scheme can be complex to apply in practice, because it e.g.\ requires distinct treatment of the initialisation and learning rates for the embedding weights and output heads.
Later work \citep{large2024scalableoptimizationmodularnorm} sought to address this issue, by providing a library (Modula) of modules, such that when the modules are combined, the overall network naturally exhibits hyperparameter scaling. Rather than studying scaling asymptotically, Modula follows a metrisation-based approach \citep[for other metrisation works, see e.g.][]{yang2024spectralconditionfeaturelearning,bernstein2021distanceneuralnetworksstability, bernstein2024modulardualitydeeplearning, bernstein2024oldoptimizernewnorm}. Its carefully designed modules allow the computation of the network's Lipschitz constant, which can be used to normalise updates to enable hyperparameter scaling.
However, one important difficulty with Modula is that it requires setting the ``mass'' of each parameter.
This mass can be seen as analagous to the layerwise function-space learning rate in our work, for the first step of the optimiser (i.e.\ at initialisation).
This introduces a large number of new hyperparameters that must be tuned.
In contrast, our approach to hyperparameter transfer, \flerm{}, does not require the user to specify masses / function-space learning rates for each parameter, because it directly measures the function-space learning rates in a base model, and then uses them in a scaled model. We show in Section \ref{sec:experiments:flerm:naivecomparison} that using function-space learning rates measured directly from a base model leads to better training loss than simplistic user-defined function-space learning rates.
Furthermore, \flerm{} can be applied to any existing neural network in Pytorch, whereas Modula requires the user to rewrite their network architecture using the library's modules.

Finally, \citet{everett2024scalingexponentsparameterizationsoptimizers} recently evaluated the ``alignment" assumptions (concerning the size of dot products between activations and updates across different layers) made by \textmu{}P and mean-field parametrisations \citep{yang2022featurelearninginfinitewidthneural,Mei_2018,bordelon2022selfconsistentdynamicalfieldtheory}, and found that they are often violated in practice. They showed empirically that alignment in real models is highly dynamic and complex throughout training, meaning that static parametrisations like \textmu{}P and mean-field could be problematic.
By contrast, we propose methods that can directly measure the function-space learning rates throughout training, avoiding the need for such assumptions.

\section{Methods} % show how we can exploit the tensor structure of the parameters to improve the quality of our estimates with negligibly increased computation. 
In Section \ref{sec:methods:deltaphi} we describe how we can empirically measure the layerwise function-space learning rates using a Monte-Carlo estimate. 
Then, in Section \ref{sec:methods:kronecker}, we propose using Kronecker factorisation to reduce the variance of our estimates. Finally, in Section \ref{sec:methods:flerm}, we introduce FLeRM, which modifies the parameter-space learning rates of scaled models so their function-space learning rates match a small base model, thereby enabling hyperparameter transfer.

\subsection{Monte-Carlo estimation of the layerwise function-space learning rate}
\label{sec:methods:deltaphi}
At the core of our contributions is the estimation of the layerwise function-space learning rates, i.e.\ the magnitude of the change in output logits arising from a particular change in the $\ell^\text{th}$ parameter tensor.
% The ability to compute such a quantity allows us to analyse the importance of particular layers of neural networks from a function-space perspective (Section \ref{sec:experiments:analysing}) as well as rectify problems that prevent hyperparameter transfer (Sections \ref{sec:methods:flerm} and \ref{sec:experiments:flerm}).
We begin by considering the full change in the function output, $f_{nk}$.
Here, $n$ indexes the $N$ datapoints in the minibatch, and $k$ indexes the $K$ output features.
We use $\Delta_\ell f_{nk}$ to denote a first-order Taylor approximation of the change in the outputs due to a particular change, $\Delta \mathbf{W}_\ell$, in the $\ell$th parameter, $\mathbf{W}_\ell$,
\begin{align}
  \label{eq:Delta_f}
   \Delta_\ell f_{nk} &= \sum_{ij} \Delta W_{ij}^\ell \frac{d f_{nk}}{d W^\ell_{ij}}.
\end{align}
Here, $W^\ell_{ij}$ is the ${ij}$th element of $\mathbf W_\ell$ and $\frac{d f_{nk}}{d W^\ell_{ij}}$ is the gradient of the output w.r.t. $W^\ell_{ij}$. Note that for ease of notation we assume $\mathbf W_\ell$ is a matrix, but $\mathbf W_\ell$ can be a tensor of any rank.
We are interested in the \textbf{layerwise function-space learning rate}, i.e.\ the RMS norm of $\Delta_\ell \mathbf f$,
%the change in output induced by $\Delta \mathbf{W}^\ell$,
\begin{align}\label{eq:fslr}
  \lrms{\Delta_\ell \mathbf{f}}^2 &= \tfrac{1}{NK} \tsum_{nk} (\Delta_\ell f_{nk})^2,
\end{align}
(L2-norm can be used if preferred, the only difference is the term $\tfrac{1}{NK}$).
Naïvely computing Eq. (\ref{eq:fslr}) via Eq. (\ref{eq:Delta_f}) is intractable as it requires $NK$ backward passes, one for each $\frac{d f_{nk}}{d W^\ell_{ij}}$.
\renewcommand{\comment}{\textcolor{gray}}
\newcommand{\different}{\textcolor{red}}
\begin{algorithm}[t]
   \caption{\textcolor{red}{Recording (red)} or \textcolor{blue}{setting (\flerm{}, blue)} function-space learning rates in a training loop.}
   \label{alg:combinedrecordandset}
   
   \begin{algorithmic}
       \STATE \textcolor{blue}{\textbf{Input:} $\lrms{\Delta_\ell \mathbf{f}}^\text{(base,$:$)}$} \hfill \comment{Base model function-space LRs}
      \STATE $\mathtt{EMA\_Z2}[\ell], \mathtt{EMA\_ZZT}[\ell], \mathtt{EMA\_ZTZ}[\ell] = 0$
      \FOR{$t=1$ {\bfseries to} $T$}
      \STATE $\mathbf{f} \leftarrow f(\mathbf{X})$ \hfill \comment{Std. forward pass}
      \STATE $\mathcal{L} \leftarrow \text{loss}(\text{targets}, \mathbf{f})$ \hfill \comment{Std. loss}
      \STATE $g^\ell_{ij} \leftarrow d\mathcal{L} / dW_{ij}^\ell$ \hfill \comment{Std. backward pass}
      \STATE $\mathbf{W}^\ell_\text{buffer} \leftarrow \mathbf{W}^\ell$ \hfill \comment{Save weights before update}
      \STATE \textcolor{red}{$\mathbf{W}^\ell \leftarrow \text{optimiser}(\eta_0, \mathbf{W}^\ell, g^\ell)$} \hfill \comment{Std. update (base LR)}
      \STATE \textcolor{blue}{$\mathbf{W}^\ell \leftarrow \text{optimiser}(\cb{\eta^\ell}_{\ell=1}^L, \mathbf{W}^\ell, g^\ell)$} \hfill \comment{(or FLeRM LR)}
      \STATE \comment{EMA warmup: run below code for a few different $\mathbf X'$}
      \IF{t \% 100 == 0}
        \STATE $\omega_{nk} \sim \mathcal{N}(0,1)$
        \STATE $\mathbf f' = f(\mathbf X')$ \hfill \comment{Fresh batch of data for $\phi$}
        \STATE $\phi \leftarrow \tfrac{1}{\sqrt{NK}} \tsum_{nk} \omega_{nk} f'_{nk}$ \hfill \comment{Compute $\phi$ (Eq. \ref{eq:phi})}
        \STATE $g^\ell_{ij} \leftarrow d\phi / dW_{ij}^\ell$ \hfill \comment{Backward pass for $\phi$}.
        \STATE $\Delta \mathbf{W}^\ell \leftarrow \tfrac{1}{\eta^\ell}\b{\mathbf{W}^\ell - \mathbf{W}^\ell_\text{buffer}}$ \hfill \comment{LR=1 update}
        \STATE $Z^\ell_{ij} \leftarrow g^\ell_{ij} \Delta W_{ij}^\ell$ \hfill \comment{Compute $Z_{ij}$ as in Eq. (\ref{eq:delta_phi_z})}
        \STATE \comment{Update EMAs}
        \STATE $\mathtt{EMA\_Z2}[\ell] \leftarrow (1{-}\beta)\mathtt{EMA\_Z2}[\ell] {+} \beta \sum_{ij} Z_{ij}^2$
        \STATE $\mathtt{EMA\_ZZT}[\ell] {\leftarrow} (1{-}\beta)\mathtt{EMA\_ZZT}[\ell] {+} \beta \sum_{k} (\sum_i Z_{ik})^2$
        \STATE $\mathtt{EMA\_ZTZ}[\ell] {\leftarrow} (1{-}\beta)\mathtt{EMA\_ZTZ}[\ell] {+} \beta \sum_{k} (\sum_j Z_{kj})^2$
        \STATE \comment{Function-space LR (EMA bias correction hidden)}
        \STATE $\lrms{\Delta_\ell \mathbf{f}}^{(t)} \leftarrow \sqrt{\mathtt{EMA\_ZZT}[\ell] \mathtt{EMA\_ZTZ}[\ell] / \mathtt{EMA\_Z2}[\ell]}$
        \STATE \comment{Set parameter-space learning rates (\flerm{})}
        \textcolor{blue}{\STATE $\eta^\ell \leftarrow \eta_0 \lrms{\Delta_\ell \mathbf{f}}^{(t)} / \lrms{\Delta_\ell \mathbf{f}}^\text{(base,$t$)}$}
        % \STATE \comment{Record functional learning rates,}
        % \STATE \textcolor{red}{$\text{save}(\lrms{\Delta_\ell \mathbf{f}}^{(t)})$}
      \ENDIF
      \ENDFOR
      \textcolor{red}{\STATE \textbf{Output:} $\lrms{\Delta_\ell \mathbf{f}}^\text{($:$)}$} \comment{Recorded function-space LRs}
   \end{algorithmic}
\end{algorithm}
Instead, we exploit the fact that we only need the magnitude $\lrms{\Delta_\ell \mathbf{f}}^2$, not the full change $\Delta \mathbf f$. 
Specifically, we use a Monte-Carlo approach. 
Consider the following scaled random combination of outputs,
\begin{align}
  \label{eq:phi}
  \phi &= \tfrac{1}{\sqrt{NK}}{\textstyle \sum}_{nk} \omega_{nk} f_{nk} & \omega_{nk} \sim \mathcal{N}(0,1).
\end{align}
As in Eq. (\ref{eq:Delta_f}) we can write the change in $\phi$ arising from a change in the $\ell$th parameter,
\begin{align}
    \label{eq:Delta_phi}
    \Delta_\ell \phi &= \sum_{ij} \Delta W_{ij}^\ell \frac{d \phi}{d W^\ell_{ij}}.
\end{align}
Importantly, note that we can compute $\Delta_\ell \phi$ in a single backward pass.
To see how $\Delta_\ell \phi$ helps us compute $\lrms{\Delta_\ell \mathbf{f}}$, we substitute the definition of $\phi$ (Eq.~\ref{eq:phi}) into Eq.~\eqref{eq:Delta_phi},
%\begin{align}
%    \Delta_\ell \phi &= \sum_{ij} \Delta W_{ij}^\ell \frac{d}{d W^\ell_{ij}} \sum_{nk} \omega_{nk} f_{nk}.
%\end{align}
%And rearranging,
\begin{align}
    \Delta_\ell \phi &= \sum_{nk} \omega_{nk} \tfrac{1}{\sqrt{NK}} \sum_{ij} \Delta W_{ij}^\ell \frac{d f_{nk}}{d W^\ell_{ij}}
\end{align}
and note that the inner sum is $\Delta_\ell f_{nk}$ (Eq.~\ref{eq:Delta_f}), so simplifying,
\begin{align}
  \Delta_\ell \phi &= \sum_{nk} \omega_{nk} \tfrac{1}{\sqrt{NK}} \Delta_\ell f_{nk},
\end{align}
and since $\omega_{nk}$ are IID standard Gaussian (Eq.~\ref{eq:phi}), we have
\begin{align}\label{eq:deltaphi}
  \Delta_\ell \phi &\sim \mathcal{N}\b{0, \lrms{\Delta_\ell \mathbf{f}}^2}.
\end{align}
Hence we can estimate $\lrms{\Delta_\ell \mathbf{f}}^2$ by computing $\Delta_\ell \phi$ with multiple samples of $\omega_{nk}$, and estimating the variance.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/fslr_vs_time_reduced.pdf}
    \\[-2ex]
    \caption{Function-space learning rates over time, measured using our approach, for the ResMLP model (top) and the Transformer (PostNorm) model (bottom). ``QK Weights" refers to $W_\text{Q}$ or $W_\text{K}$ (query and key weight matrices), whilst ``VO Weights" refers to $W_\text{V}$ or $W_\text{O}$ (values and head-concatenation projection weight matrices).}
    \label{fig:fslr_vs_time_mainpaper}
\end{figure}

\subsection{More efficient function-space learning rate estimates using Kronecker factorisation}
\label{sec:methods:kronecker}

The approach in Sec.~\ref{sec:methods:deltaphi} requires one backward pass per sample, which could still be inefficient if we need multiple samples for a good estimate.
We remedy this by exploiting the structure of $\phi$. 
Noting that $i,j$ index the rows and columns of $\mathbf W_\ell$, we rewrite Eq.~\eqref{eq:Delta_phi} as,
\begin{align}\label{eq:delta_phi_z}
  \Delta_\ell \phi &= \sum_{ij} Z_{ij} & Z_{ij} &= \Delta W_{ij}^\ell \frac{d \phi}{d W_{ij}^\ell}
\end{align}
% Note that all the $z_{ij}$'s can be computed in a single backward pass, and
Hence the function-space learning rate can be written as
\begin{align}\label{eq:onlyneedsumovercov}
  \lrms{\Delta_\ell \mathbf{f}}^2 &= \text{Var}\sqb{\Delta_\ell \phi} = \sum_{ij, i'j'} \text{Cov}\sqb{Z_{ij}, Z_{i'j'}}.
\end{align}
Also note that, substituting the definition of $\phi$ (Eq.~\ref{eq:phi}) into the definition of $Z_{ij}$ (Eq.~\ref{eq:delta_phi_z}), we see that the $Z_{ij}$'s are zero-mean Gaussian, as they are a linear combination of zero-mean Gaussian terms $\omega_{nk}$ (we will use this fact later),
\begin{align}
  Z_{ij} &= \sum_{nk} \omega_{nk} \tfrac{1}{\sqrt{NK}} \Delta W_{ij}^\ell \frac{d f_{nk}}{d W_{ij}^\ell}.
\end{align}

From Eq. (\ref{eq:onlyneedsumovercov}) we can see that instead of directly estimating the variance of samples of $\Delta_\ell \phi$, we could instead first estimate the covariance of the $Z_{ij}$'s and sum over them. This opens up the possibility of assuming some covariance structure over the $Z_{ij}$'s to reduce the variance of our estimate for $\text{Var}\sqb{\Delta_\ell \phi}$, at the expense of some bias. One possibly restrictive example is to assume that all the $Z_{ij}$'s are IID, in which case $\text{Var}\sqb{\Delta_\ell \phi} = {\textstyle \sum}_{ij} \text{Var}\sqb{Z_{ij}}$.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/widthtransfer_onlyfirststepflerm.pdf}
    \\[-2ex]
    \caption{\flerm{} dramatically improves optimal learning rate transfer across widths. Top: standard practice. Bottom: \flerm{}.}
    \label{fig:widthtransfer_onlyfirststepflerm_mainpaper}
\end{figure*}
Instead, we assume a Kronecker-factored covariance matrix \citep{martens2015kfac}. Specifically, we assume that we have a covariance matrix $\mathbf U$ over rows and $\mathbf V$ over columns, giving
\begin{align}
  \text{Cov}\sqb{Z_{ij},Z_{i'j'}} = U_{ii'}V_{jj'}.
\end{align}

Under this assumption, Eq. (\ref{eq:onlyneedsumovercov}) becomes
\begin{align}
  \nonumber
  \lrms{\Delta_\ell \mathbf{f}}^2 &= \tsum_{ij, i'j'} \text{Cov}\sqb{Z_{ij}, Z_{i',j'}}\\
  \nonumber
  &= \tsum_{ij, i'j'} U_{ii'} V_{jj'}\\
  \label{eq:kfac_ltwo}
  &= \b{\tsum_{ii'} U_{ii'}} \b{\tsum_{jj'}V_{jj'}}.
\end{align}
We will now show how to compute this efficiently from $\mathbf Z$, which itself can be computed with a single backwards pass (Eq.~\ref{eq:delta_phi_z}).
Since the $Z_{ij}$'s are zero-mean Gaussian, and we have assumed $\text{Cov}\sqb{Z_{ij},Z_{i'j'}} = U_{ii'}V_{jj'}$, $\mathbf{Z}$ is zero-mean Matrix-Normal distributed, and so \citep{gupta1999matrix}
\begin{subequations}
\label{eq:matrixnormalsecondordermoments}
\begin{align}
  \label{eq:matrixnormalsecondordermoments:U}
  \mathbb E[\mathbf{ZZ}^T] &= \mathbf U \text{tr}(\mathbf V)\\
  \label{eq:matrixnormalsecondordermoments:V}
  \mathbb E[\mathbf{Z}^T\mathbf Z] &= \mathbf V \text{tr}(\mathbf U).
\end{align}
\end{subequations}
Dividing Eq.~\eqref{eq:matrixnormalsecondordermoments:U} by $\text{tr}(\mathbf{V})$ and Eq.~\eqref{eq:matrixnormalsecondordermoments:V} by $\text{tr}(\mathbf{U})$, we obtain $\mathbf{U}$ and $\mathbf{V}$ and can substitute them into Eq.~\eqref{eq:kfac_ltwo},
\begin{align}\label{eq:kfac_ltwo_denom}
  \lrms{\Delta_\ell \mathbf{f}}^2 &= \frac{\b{\tsum_{ii'} \mathbb{E}[\mathbf{Z Z}^T]_{ii'}} \b{\tsum_{jj'} \mathbb{E}[\mathbf{Z}^T \mathbf{Z}]_{jj'}}}{\text{tr}(\mathbf{U}) \text{tr}(\mathbf{V})}
\end{align}
To obtain the denominator, we take the trace of both sides of Eq.~\eqref{eq:matrixnormalsecondordermoments:U} or Eq.~\eqref{eq:matrixnormalsecondordermoments:V}, giving us
\begin{align}
  \label{eq:kfac_ltwo:inter}
  \text{tr}(\mathbf{U}) \text{tr}(\mathbf{V}) &= \text{tr}\left(\mathbb{E}[\mathbf{ZZ}^T]\right)\\
  \nonumber
  &= \mathbb{E}\sqb{\tsum_{ij} Z_{ij}^2} = \mathbb{E}\sqb{\frob{\mathbf{Z}}^2}.
\end{align}
%Taking the trace on both sides of these expressions gives an identity,
%\begin{equation}\label{eq:matrixnormaltraceproduct}
%    \text{tr}(\mathbf U)\text{tr}(\mathbf V) = \text{tr}(\mathbf U \text{tr}(\mathbf V)) = \text{tr}\left(\mathbb E[\mathbf{XX}^T]\right) = \mathbb E[\text{tr}(\mathbf X \mathbf X^T)] = \mathbb E[||\mathbf X||_\mathcal{F}^2].
%\end{equation}
%
%The sum over all elements of this kronecker factored covariance matrix is given by all pairwise products of elements between $\mathbf U$ and $\mathbf V$:
Substituting this into Eq.~\eqref{eq:kfac_ltwo_denom}, we obtain
\begin{align}
  \label{eq:kfac_ltwo:final}
    \lrms{\Delta_\ell \mathbf{f}}^2 &= 
    %&= \frac{\mathbb E\left(\sum_{ij} [\mathbf X \mathbf X^T]_{ij}\right)\mathbb E\left(\sum_{i'j'} [\mathbf X \mathbf X^T]_{i'j'}\right)}{\text{tr}(\mathbf V) \text{tr}(\mathbf U)}\\
	&= \frac{\mathbb E\left(\sum_{ii} [\mathbf Z \mathbf Z^T]_{ii'}\right)\mathbb E\left(\sum_{jj'} [\mathbf Z^T \mathbf Z]_{jj'}\right)}{\mathbb E[||\mathbf Z||_\mathcal{F}^2]}
\end{align}
and we note that the sums in the numerator can be computed in quadratic time (not cubic as $\mathbf Z \mathbf Z^T$ or $\mathbf Z^T \mathbf Z$ suggests), e.g.
\begin{align}
  \nonumber
  \tsum_{ii'} [\mathbf Z \mathbf Z^T]_{ii'} &= \tsum_{ii'k} Z_{ik} Z_{i'k}\\
  \nonumber
  &= \tsum_{k} \b{\tsum_i Z_{ik}} \b{\tsum_{i'} Z_{i'k}}\\
  &= \tsum_{k} \b{\tsum_i Z_{ik}}^2.
\end{align}
Hence to compute the layerwise function-space learning rate $\lrms{\Delta_\ell \mathbf{f}}^2$, we only need to estimate 3 scalar expectations.
\begin{align}
\label{eq:emas}
&\mathbb E[||\mathbf Z||_\mathcal{F}^2], &
&\mathbb E[\tsum_{k} (\tsum_i Z_{ik})^2],&
&\mathbb E[\tsum_{k} (\tsum_j Z_{kj})^2]
\end{align}
Since we usually measure $\lrms{\Delta_\ell \mathbf{f}}^2$ as it changes over time, we estimate these expectations using exponential moving averages (EMAs) to further reduce the variance.
 
This approach can be generalised to tensor-valued parameters (see Appendix \ref{sec:app:tensorkronecker}); the resulting algorithm has very similar computational cost and memory; in particular, we only need to store $R+1$ scalar EMAs for each parameter tensor, where $R$ is the rank of the tensor (e.g.\ 2 for a matrix).
In summary (see Algorithm \ref{alg:combinedrecordandset}, red text), estimating the layerwise function-space learning rates involves computing a sample $\mathbf Z$ (Eq.~\ref{eq:delta_phi_z}), which requires a single forward pass and backward pass using a fresh batch of data to compute $\frac{d\phi}{dW_{ij}^\ell}$ (in addition to the usual training step which gives $\Delta W_{ij}^\ell$), and updating 3 scalar EMAs (for matrix parameters). 
We warm up the EMAs by computing several samples of $\mathbf Z$ at the start of training, and only use one sample of $\mathbf Z$ every e.g.\ $100^\text{th}$ iteration thereafter.

\subsection{Function-space learning rate matching (FLeRM)}
\label{sec:methods:flerm}

\flerm{} uses the machinery developed above to record the function-space learning rates in a small, cheap base model. 
Then, in the larger, more expensive model, \flerm{} sets the parameter-space learning rates such that the function-space learning rates match those in the base model (see Alg.~\ref{alg:combinedrecordandset}, blue text, for details).

There are two extra details worth discussing. First, recall that we use EMAs for the three scalars in Eq.~\eqref{eq:emas}.
These EMA estimates will have considerable bias if the parameter-space learning rates vary (e.g. due to scheduling, or \flerm{}), as previous updates of the EMA may have used a very different learning rate.
Hence, in Algorithm  (\ref{alg:combinedrecordandset}), we always consider a learning rate of $1$ when computing $\lrms{\Delta_\ell \mathbf{f}}$, rather than the actual learning rates.
Computing the $\Delta \mathbf{W}^\ell$ implied by a learning rate of $1$ ensures that the learning rate seen by the EMA is always consistent. This also applies when recording the base function-space learning rates $\lrms{\Delta_\ell \mathbf{f}}^\text{base}$, so the modified layerwise learning rate $\eta^\ell$ becomes
\begin{equation}
    \eta^\ell = \frac{\eta_0\lrms{\Delta_\ell \mathbf{f}}^\text{base}}{\lrms{\Delta_\ell \mathbf{f}}}
\end{equation}
where $\eta_0$ is the learning rate of the base model.

Second, if the scaled model has more layers than the base model, it is not possible to ``match" the layerwise function-space learning rates one-to-one. Since we scale depth by increasing the number of ``residual blocks" in our experiments, we use the heuristic of sharing the base model's function-space learning rates between the new blocks. For example, if a parameter in the first residual block has $\lrms{\Delta_\ell \mathbf{f}}^\text{(base)}=1$ in the base model, and the scaled model has $4 \times$ as many blocks, the corresponding parameters in the first 4 blocks of the scaled model will use a base function-space learning rate of $\lrms{\Delta_\ell \mathbf{f}}^\text{(base)} = \tfrac{1}{4}$. 

\section{Experiments}
\label{sec:experiments}
In this section we analyse function-space learning rates for concrete neural networks (Section \ref{sec:experiments:analysing}), and investigate the use of FLeRM to enable hyperparameter transfer when scaling model width, depth, initialisation scale, and LoRA rank (Section \ref{sec:experiments:flerm}).

Full details of the models\footnote{We provide our code at \url{https://github.com/edwardmilsom/function-space-learning-rates-paper}} used in the experiments can be found in Appendix \ref{sec:app:modeldetails}. The base ResMLP is an MLP with 4 hidden layers, each with residual connections, trained for 50 epochs on flattened CIFAR10 images \citep{krizhevsky2009learning}.
The base transformer is decoder-only, has two self-attention + feedforward blocks \citep{vaswani2017attentionneed}, and is trained on a subset of the Wikitext-103 dataset \citep{merity2016pointersentinelmixturemodels}. 
We compare 3 different types of Layernorm \citep{ba2016layernormalization} in the transformers, with affine transformations disabled: PostNorm is $\text{Norm}(x + f(x))$
, PreNorm is $x + f(\text{Norm}(x)$, and PreNormPostMod is $x + \text{Norm}(f(x))$. 
In the ResMLP, scaling width increases the hidden dimension, whilst in the transformers, we scale the embedding dimension, the feedforward hidden dimension, and the number of heads. 
In all models, depth scaling increases the number of ``residual blocks" that form the hidden layers of the model. Both ResMLP and the transformers used the Adam optimiser~\citep{kingma2014adam} with a constant learning rate schedule. In all plots, train loss is averaged over the last 200 batches of training.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/depthtransfer_onlyfirststepflerm.pdf}
    \\[-2ex]
    \caption{\flerm{} improves or maintains optimal learning rate transfer across depth.  Top: standard practice. Bottom: (\flerm{}).}
    \label{fig:depthtransfer_onlyfirststepflerm_mainpaper}
\end{figure*}

\subsection{Analysing function-space learning rates}
\label{sec:experiments:analysing}
In Figure \ref{fig:fslr_vs_time_mainpaper} we measure the function-space learning rates using the techniques presented in Sections \ref{sec:methods:deltaphi} and \ref{sec:methods:kronecker} for the ResMLP and PostNorm transformer models. Plots for the PreNorm and PreNormPostMod transformer can be found in Figure \ref{fig:fslr_vs_time_appendix}, though they are very similar. We use 40 batches of data to warm up the EMAs as suggested Section \ref{sec:methods:kronecker}, then measure the function-space learning rate every 100 iterations. We use an EMA decay rate of $\beta=0.9$. that in both models, the function-space learning rates change over time, despite the parameter-space learning rates being fixed. 
The most obvious pattern is that the function space learning rates fall monotonically for all parameters, except the input embedding layer, revealing an implicit scheduling of the function-space learning rates under standard Adam training. 
Interestingly, in the ResMLP, whilst the function-space learning rates initially decay, the hidden layers and input layer eventually start increasing again, whilst the output layer plateaus. 
In the transformer, the embedding layer's function-space learning rate actually increases over time. 
One possible explanation is that the noisy initialisation in hidden and output layers effectively scrambles the signal from the input layer, but as these layers are trained, the input embedding can have a clearer, stronger effect on the output.

We also observe that the different types of layers, such as feedforward weights or QK weights, form very clear ``groups" or ``bands" in these plots. From this, we can see that the second feedforward weight matrices in self-attention have the strongest influence over the transformer's learned function, with function-space learning rates an order of magnitude larger than those of the readout layer. This is a surprising discovery, since one might naively expect the readout layer, whose weights directly project to the output logits, to have the largest effect on the learned function.

\subsection{\flerm{} hyperparameter transfer experiments}
\label{sec:experiments:flerm}

We evaluated the effect of \flerm{} on hyperparameter transfer when scaling model width, depth, parameter initialisation scale, and LoRA rank. 
We ensure width invariance at initialisation by using Kaiming initialisation \citep{he2015delvingdeeprectifierssurpassing}, and depth invariance at initialisation by introducing a factor of $\tfrac{1}{\sqrt{L}}$ into the residual stream \citep{hayou2021stableresnet, hayou2023widthdepthlimitscommute}, using \flerm{} to achieve invariance throughout training. We first record the function-space learning rates of the base models as in Section \ref{sec:experiments:analysing} using 8 random seeds, and then average over these seeds. This process is repeated for every learning rate used in our plots. When running the ``scaled" models, i.e. those with altered width, depth, initialisation scale, or LoRA rank, use 40 batches of data to warm up the EMAs, and then modify the initial learning rate as specified in Algorithm (\ref{alg:combinedrecordandset}).

We then have a choice. We can either use these learning rates for the rest of training, or we can periodically update them every 100 iterations with a single batch of data as in Algorithm (\ref{alg:combinedrecordandset}). We found these approaches to give very similar results, so we present the results for the fixed learning rates here, and provide plots for the periodically updated learning rates in Appendix \ref{sec:app:extraplots:periodicupdate}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/inittransfer_onlyfirststepflerm.pdf}
    \\[-2ex]
    \caption{\flerm{} allows us to train initialisation scale invariant networks. Top: standard practice. Bottom: \flerm{}.}
    \label{fig:initscaletransfer_onlyfirststepflerm_mainpaper}
\end{figure}

\subsubsection{Model Width}
\label{sec:experiments:flerm:modelwidth}
Figure \ref{fig:widthtransfer_onlyfirststepflerm_mainpaper} shows the effect of scaling the width on the optimal learning rate for the ResMLP and transformer models. In agreement with e.g.\ \citet{yang2022featurelearninginfinitewidthneural}, we find that in standard practice, there is a significant shift in the optimal learning rate for all models as width increases, but when using \flerm{} to normalise the layerwise parameter-space learning rates, this shift is either entirely removed or dramatically reduced. Additionally, in the transformer models, the use of \flerm{} does seem to improve the loss at high widths, compared to standard practice.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures_lora/loss_lr_lora_fix_a.pdf}
    \\[-2ex]
    \caption{\flerm{} improves optimal learning rate transfer when changing LoRA rank. The plots show the behaviour of training loss under varying the learning rate of $B$ and LoRA rank for two continual pretraining tasks. Top: standard AdamW optimiser. Bottom: (\flerm{}). Some lines end abruptly for larger learning rates, indicating a numerical instability.}
    \label{fig:lora_rank_transfer}
\end{figure*}


\subsubsection{Model Depth}
\label{sec:experiments:flerm:modeldepth}
Figure \ref{fig:depthtransfer_onlyfirststepflerm_mainpaper} shows the effect of scaling depth on the optimal learning rate for the ResMLP and transformer models. Under standard practice, the behaviour of the optimal learning rate as we increase depth varies by model. In the ResMLP, there is a significant shift in the optimal learning rate, and we observe that \flerm{} brings the optimal learning rates much closer together, though for higher depths there is a slight shift towards towards larger values.

The PostNorm transformer is relatively unstable, with the loss shooting up once a certain learning rate threshold is passed.
In standard practice, the location of this instability shifts to lower learning rates as depth is increased, meaning that deeper models actually have a worse optimal loss.
However, with \flerm{}, these instabilities all occur around the same place as the base model (either at the same learning rate or the one before it, suggesting the ``true" location of the instability could be somewhere in between, at a learning rate we did not evaluate).
Thus, in this setting, \flerm{} gives dramatic improvements in performance for the deeper models.

In standard practice, the PreNorm transformer shows a small shift in the optimal learning rate, which is rectified by \flerm{}. %In addition, there is a moderate improvement in the best training loss achieved when using \flerm{}.

The PreNormPostMod transformer is an interesting case. When scaling depth, the optimal learning rate in the standard setting is already depth-invariant, so in this setting, there is arguably no reason to apply \flerm{}. 
Reassuringly, when we do apply \flerm{}, the location of the optimal learning rate is preserved.
In practice, we will usually be scaling width and depth together, so \flerm{} is likely useful overall.

\subsubsection{Parameter Initialisation Scale}
\label{sec:experiments:flerm:initscale}

\flerm{}'s uses are not constrained to width and depth scaling. In the PreNormPostMod transformer, which uses residual connections of the form $x + \text{Norm}(f(x))$ and, like all 3 transformer variants, uses QK normalisation (layernorms applied after $\mathbf W_\text{Q}\mathbf X$ and $\mathbf W_\text{K}\mathbf X$), all learnable functions in the network (with the exception of the input embedding and the readout layer) are followed by a layernorm. The network should be invariant to the magnitude of these parameters, in the sense that multiplying such a weight matrix by a constant does not change the output of the network. 
However, as shown in Figure \ref{fig:initscaletransfer_onlyfirststepflerm_mainpaper}, the loss of networks trained in the standard setting varies wildly with initialisation scale, and the optimal learning rate even shifts. 
With \flerm{}, however, we ensure that the updates are invariant in function-space, and so the loss vs. learning rate curves line up very closely for all initialisation scales, removing the need to tune this hyperparameter.

\subsubsection{LoRA Adapter Rank}
\label{sec:experiments:lora}
We also investigated the use of FLeRM in a fine-tuning setting. For this, we trained autoregressively using LoRA adapters~\citep{hu2021lora} on two LLMs, GPT-2~\citep{radford2019language} and Llama-3.2-1B~\citep{dubey2024llama}. We experimented with {$\small\sim$}4M token subsets of two datasets: Cold French Law~\citep{cold-french-law} and Mathpile~\citep{wang2023generative}. For LoRA experiments, we used Adam as our base optimiser with a constant learning rate schedule, sweeping over adapter rank and the learning rate of each of the two LoRA parameters separately. We measure the base function-space learning rates for a single seed and use these with \flerm{} in the scaled models.
%We used \flerm{} periodically throughout training, updating the learning rates every 50 iterations, with an EMA warm-up of 40 batches.
As before, we run 40 EMA warm-up iterations and then normalise the learning rates using \flerm{} at the first iteration, fixing those learning rates for the rest of training.
Figure~\ref{fig:lora_rank_transfer} shows results after sweeping over the learning rate of the $B$ parameter in LoRA ($\Delta W = BA$), and  in Appendix~\ref{app:lora_extra_exp} we show results for sweeping over the learning rate of $A$. See Appendix~\ref{app:lora} for further experimental details.

We find that in the standard setting, the optimal learning rate {\it increases\/} as we increase the LoRA rank (Figure~\ref{fig:lora_rank_transfer}); for example, the optimal learning rate for the Llama model increases by more than an order of magnitude on both datasets as we increase the LoRA rank from 2 to 32. There is also a corresponding shift in learning rate instabilities (i.e. where the learning rate is too high). However, when using \flerm{} with $r$ (Rank) equal to $2$ as the ``base model", the shift is either eliminated or greatly reduced, and the instabilities mostly align. Note that some data points are missing when learning rates are too high (for example for $=2^4$ and $r=2^5$ in the learning rate $\small \gtrapprox 10^{-2}$ range in the bottom left plot), but this is expected because this instability is inherited from the base model.

\subsubsection{Comparison to naïvely chosen function-space learning rates}
\label{sec:experiments:flerm:naivecomparison}

One might ask whether the matching the function-space learning rates of a base model is important, or if we could replace them with something simpler. To test this, we repeated the width, depth, and initialisation scale experiments from earlier, but replaced the base model's recorded function-space learning rates with uniform vectors that sum to 1. The results are shown in Figures \ref{fig:widthtransfer_equalmassbutstillsplittingdepthproperlyablation}, \ref{fig:depthtransfer_equalmassbutstillsplittingdepthproperlyablation}, and \ref{fig:initscaletransfer_equalmassbutstillsplittingdepthproperlyablation} (in Appendix \ref{sec:app:equalmassbutstillsplittingdepthproperlyablation}). Whilst hyperparameter transfer is still retained, the training loss is slightly worse than the equivalent experiments in Section \ref{sec:experiments:flerm}, suggesting that the function-space learning rates induced by the Adam optimiser give benefits to performance. 
This ablation still shares the base function-space learning rates across new hidden blocks as depth is scaled (Section \ref{sec:methods:flerm}). 
Importantly, this means that the function-space learning rates for the embedding and readout layers are held constant even as we scale depth, because these layers are not replicated.  
We also ran a further ablation where all function-space learning rates are completely equal at all depths (Figure \ref{fig:depthtransfer_EQUALmassablation}, Appendix \ref{sec:app:EQUALmassablation}). In this case, the PreNormPostMod transformer no longer exhibits depthwise hyperparameter transfer, which matches the findings of \citet{large2024scalableoptimizationmodularnorm}, who observed that the ``importance" of the input and output layers must be retained as depth is scaled to achieve hyperparameter transfer. Interestingly, although hyperparameter transfer is lost, performance in the deepest PreNormPostMod model in this setting is slightly better than the standard setting, suggesting that with a more complex heuristic for matching function-space learning rates to models with new layers, it might be possible to achieve greater performance with \flerm{}.

\section{Conclusion}
In this paper, we developed an efficient way to estimate the magnitude of changes in function-space caused by an update to a neural network's parameters. We used this method to analyse the dynamics of existing models, and then to modify layerwise parameter-space learning rates (\flerm{}) for scaled models so that updates in function-space are scale invariant, enabling transfer of the optimal learning rate across width, depth, initialisation scale, and LoRA rank. 
Such a method could very useful for training very large foundation models, where scaling laws are currently derived on a case-by-case basis \citep[e.g. see the LLama 3 technical report][]{dubey2024llama}. 
In terms of future work, it was noted in Section \ref{sec:experiments:flerm:naivecomparison} that a more sophisticated scheme for matching function-space learning rates to models with more layers than the base model could further enhance the performance of \flerm{}. One possible approach would be to use the methods presented in this paper to study the relationship between function-space learning rates in models of various depth. However, this is beyond the scope of this work.
% ... we found that computing the modified learning rate for scaled models at the first step of training was sufficient to enable hyperparameter transfer. Since this only requires a handful of extra backwards passes at the start of training, this represents an easier method for setting learning rates than empirically measuring the trend of the optimal learning rate as is often done in modern large scale training (CITATIONS)
% ... whilst the work of \citet{everett2024scalingexponentsparameterizationsoptimizers} suggests that normalising the learning rate based only on the dynamics at the start of training could miss important information, we found that periodically recomputing the learning rate throughout training had little effect on hyperparameter transfer or performance in the settings we tested, though it would be interesting to explore more complex settings like full-finetuning in future work. 

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Model Details}
\label{sec:app:modeldetails}

\subsection{ResMLP}
The base residual MLP model has 4 residual blocks, each containing a single linear layer. Every hidden layer has dimension 128, which is multiplied by the width multiplier in width-scaling experiments. In depth-scaling experiments, the number of residual blocks is multiplied by the depth multiplier. We do not use Layernorms or Batchnorms in the ResMLP \citep{ba2016layernormalization,ioffe2015batchnormalizationacceleratingdeep}. 

We initialise all weight matrices using Kaiming / He initialisation \citep{he2015delvingdeeprectifierssurpassing}, that is, IID Gaussian weights with $\frac{1}{\texttt{fan\_in}}$ variance, multiplied by an activation-function-specific scalar (2 in the case of ReLU, 1 for no activation). Biases are initialised to 0.

To ensure depth invariance at initialisation, we introduce a factor of $\frac{1}{\sqrt{L}}$ into each layer's weight matrix. At initialisation, this is equivalent to multiplying by $\frac{1}{\sqrt{L}}$ in the forward pass as in \citet{bordelon2023depthwisehyperparametertransferresidual}, but does not continue to affect the forward passes during training, allowing us to better isolate the effect of \flerm{}.

We optimise the model using Adam with default settings (other than the learning rate which we set using the methods detailed in this paper). We train for 50 epochs.

\subsection{Transformer}
The base transformer model is a decoder-only transformer with 2 self-attention + feedforward layers, query-key normalisation \citep[applying layernorm to the queries and keys before using them in multihead attention]{dehghani2023scalingvisiontransformers22}, and layernorms with affine transformations disabled. We compare 3 different types of layernorm in our experiments: PostNorm is $\text{Norm}(x + f(x))$
, PreNorm is $x + f(\text{Norm}(x)$, and PreNormPostMod is $x + \text{Norm}(f(x))$.

When scaling width, we multiply the number of heads, the embedding dimension $d_\text{model}$, and the feedforward hidden dimension $d_\text{ff}$ by the width multiplier. The base model has $d_\text{model} = 128, d_\text{ff} = 512$, and 2 attention heads per layer. When scaling depth, we multiply the number of transformer ``blocks", consisting of self-attention and a feedforward network, as detailed in \citet{vaswani2017attentionneed}, by the depth multiplier. 

We initialise all weight matrices using Kaiming / He initialisation \citep{he2015delvingdeeprectifierssurpassing}, that is, IID Gaussian weights with $\frac{1}{\texttt{fan\_in}}$ variance, multiplied by an activation-function-specific scalar (2 in the case of ReLU, 1 for no activation). Biases are initialised to 0. 

To ensure the model is invariant to depth at initialisation, we multiply the residual branch by $\frac{1}{\sqrt{L}}$ during the forward pass, as in \citet{bordelon2023depthwisehyperparametertransferresidual}. More specifically, PostNorm becomes $\text{Norm}(x + \frac{1}{\sqrt{L}} f(x))$, PreNorm becomes $x + \frac{1}{\sqrt{L}} f(\text{Norm}(x)$, and PreNormPostMod becomes $x +  \frac{1}{\sqrt{L}} \text{Norm}(f(x))$. Note that with PostNorm and PreNorm we could have absorbed this factor into the weights of the module $f$ and therefore avoided altering the forward pass computation, because $f$ is at the end of the residual stream. However, in PreNormPostMod, the layernorm is at the end of the residual stream, so its initialisation will always have unit size no matter how we initialise $f$, therefore requiring us to use the factor $\frac{1}{\sqrt{L}}$ during the forward pass. We therefore decided to treat all transformer variants the same in this regard.

We train the transformer on roughly $\frac{1}{10}$ of the Wikitext-103 dataset \citep{merity2016pointersentinelmixturemodels}, using a batch size of 20 and a sequence length of 256. We tokenise the dataset using the GPT2 tokeniser from the HuggingFace transformer library \citep{wolf2020huggingfacestransformersstateoftheartnatural}. We optimise the model using Adam with default settings (other than the learning rate which we set using the methods detailed in this paper). We train for 1 epoch (i.e. we only observe each token once).

\subsection{LoRA adapters}\label{app:lora}
For the LoRA experiments in Section~\ref{sec:experiments:lora}, we trained LoRA adapters on continual pretraining tasks. The LoRA adapters were initialized using the \texttt{gaussian} initialization provided by Hugging Face's \texttt{peft} library~\citep{peft}. We experimented with two models, GPT2 and Llama-3.2-1B.
We added LoRA adapters to the default modules of GPT2, and the \texttt{q/v/k/o\_proj} modules of Llama-3.2-1B.
We trained for 500 iterations with a batchsize of 8 and sequence length 512. The datasets used were {$\small\sim$}4M token subsets of Cold French Law~\citep{cold-french-law} and Mathpile~\citep{wang2023generative}.

A LoRA adapter is formed of two parameters $B$ and $A$, with $\Delta W = BA$. When sweeping learning rates, we vary the learning rates of each parameter ($A$/$B$) separately, while keeping the learning rate of the other parameter fixed. When sweeping for $B$, we used a fixed learning rate of $10^{-4}$ for $A$. When sweeping for $A$, we fixed the learning rate of $B$ as follows: $10^{-3}$ for GPT-2, $10^{-4}$ for Llama-3.2-1B / Cold French Law, and $5\times 10^{-5}$ for Llama-3.2-1B/Mathpile .

As in the transformer and ResMLP experiments, we wish to warm-up the EMAs of \flerm{} with 40 batches of data, and then use \flerm{} to normalise the learning rates at the initial iteration, fixing these learning rates for the rest of training. However, we cannot immediately use our method FLeRM when using LoRA adapters. This is due to the way LoRA adapters (following the original work of~\citet{hu2021lora}) are initialized. Since the parameter ${B}$ is initialized to zero (to ensure that $\Delta{W}={0}$ at the beginning of training), the gradient of ${A}$ is ${0}$ for the first iteration. This is problematic because it implies $\lrms{\Delta_\ell {f}}=0$, leading to a division by zero in Algorithm \ref{alg:combinedrecordandset}. As a workaround, we warm-up FLeRM on the fifth iteration, and record / set the learning rates on the 6th iteration. We find that in practice, this still gives adequate hyperparameter transfer. In the LoRA experiments, we measure the base function-space learning rates for a single seed and use these with \flerm{} in the scaled models. This is because the base model is a lot more expensive than in the other experimental setups. Only using one seed for the base function-space learning rates did not appear to hinder \flerm{}'s ability to enabled LoRA-rank transfer.

\section{Kronecker factored covariance approximation for tensor-valued parameters}
\label{sec:app:tensorkronecker}
Previous work has extended the matrix-normal distribution to tensors, with one covariance matrix per dimension \citep[e.g.][]{manceur2013tensor,hoff2010separablecovariancearraystucker}. We can use this to extend the methods in Section \ref{sec:methods:kronecker} to tensor-valued parameters.

Suppose we have a $D$-dimensional tensor $\mathbf X \in \mathbb R^{n_1 \times \cdots \times n_D}$, where we will use $X_{i_1,\dots,i_D}$ to denote the $(i_1,\dots,i_D)^\text{th}$ element. Extend the $\text{vec}(\cdot)$ operation to tensors in the obvious way, i.e. form a vector by taking elements $(1,1,\dots,1,1)$ to $(n_1,1,\dots,1,1)$, then $(1,2,\dots,1,1)$ to $(n_1,2,\dots,1,1)$ etc. systematically iterating along the dimensions until you reach the final element $(n_1,\dots,n_D)$. We say the vector $\mathbf X$ is \textbf{tensor-normal distributed} if
\begin{equation}
    \text{vec}(\mathbf X) \sim \mathcal N(\text{vec}(\mathbf M), \mathbf U^{(D)} \otimes \cdots \otimes \mathbf U^{(1)})
\end{equation}
where $\mathbf M \in \mathbb R^{n_1,\dots,n_D}$ is the mean tensor, and $\mathbf U^{(1)} \in \mathbb R^{n_1 \times n_1}, \dots, \mathbf U^{(D)} \in \mathbb R^{n_D \times n_D}$ are covariance matrices for the $D$ different dimensions. In other words, assuming $\mathbf M=0$, the second-order moments / covariance of any two elements in the tensor factorises across dimensions:
\begin{equation}
    \mathbb E(X_{i_1,\dots,i_D}X_{j_1,\dots,j_D}) = \prod_{d=1}^D U^{(d)}_{i_d,j_d}.
\end{equation}

For matrix-normal distributions, we had identities for the second order matrix products $\mathbb E[\mathbf{XX}^T] = \mathbf U \text{tr}(\mathbf V) \text{ and } \mathbb E[\mathbf{X}^T\mathbf X] = \mathbf V \text{tr}(\mathbf U)$, which we used in Section \ref{sec:methods:kronecker} to figure out how to estimate the covariance matrices (or more specifically, the sum over all pairs of elements). Do similar identities hold for the tensor normal distribution? It turns out yes \citep[e.g. Proposition 2.1 of][]{hoff2010separablecovariancearraystucker}.

Consider the following shorthand for ``contracting" two tensors over all but one of their dimensions $d$, kind of like a generalisation of matrix multiplication
\begin{equation}
    (\mathbf A \times_\text{d} \mathbf B)_{ij} = \sum_{k_1 \dots k_{d-1}k_{d+1}\dots k_D} A_{k_1 \dots k_{d-1} i k_{d+1}\dots k_D} B_{k_1 \dots k_{d-1}jk_{d+1}\dots k_D}
\end{equation}
i.e. assuming $\mathbf A$ and $\mathbf B$ have conformable shapes, you take ``dot products" over all dimensions except $d$. Then for a tensor-normal distributed $\mathbf X$ we have the ``second-order moments"
\begin{subequations}
    \begin{align}
        \mathbb E(\mathbf X \times_d \mathbf X)_{ij} &= \sum_{k_1 \dots k_{d-1}k_{d+1}\dots k_D} \mathbb E(X_{k_1 \dots k_{d-1}ik_{d+1}\dots k_D} X_{k_1 \dots k_{d-1}jk_{d+1}\dots k_D})\\
        &= \sum_{k_1 \dots k_{d-1}k_{d+1}\dots k_D} U^{(d)}_{ij}\prod_{d' \neq d} U^{(d')}_{k_{d'},k_{d'}}\\
        &= U^{(d)}_{ij} \prod_{d' \neq d} \left(\sum_{k_{d'}} U^{(d')}_{k_{d'},k_{d'}}\right)\\
        &= U^{(d)}_{ij} \prod_{d' \neq d} \text{tr}(\mathbf U^{(d')})
    \end{align}
\end{subequations}
and so we have
\begin{equation}
    \mathbb E(\mathbf X \times_d \mathbf X) = \mathbf U^{(d)} \prod_{d' \neq d} \text{tr}(\mathbf U^{(d')}).
\end{equation}

As before, we want to compute the sum over all elements of the covariance matrix. The covariance matrix for the vectorised tensor is $\mathbf U^{(D)} \otimes \cdots \otimes \mathbf U^{(1)}$, which means it contains precisely all the possible products of D elements, one from each $\mathbf U^{(d)}$. Hence we wish to compute
\begin{subequations}
    \begin{align}
        \prod_{d=1}^D \left(\sum_{i_d,j_d} U^{(d)}_{i_dj_d}\right) &= \prod_{d=1}^D \frac{\sum_{i_d,j_d}\mathbb E(\mathbf X \times_d \mathbf X)_{i_dj_d}}{\prod_{d' \neq d} \text{tr}(\mathbf U^{(d')})}\\
        &= \frac{\prod_{d=1}^D \sum_{i_d,j_d}\mathbb E(\mathbf X \times_d \mathbf X)_{i_dj_d}}{\prod_{d=1}^D \text{tr}(\mathbf U^{(d)})^{D-1}}.
    \end{align}
\end{subequations}

As before, we can express the denominator in terms of $\mathbf X$. Note that
\begin{subequations}
    \begin{align}
        \prod_{d=1}^D \text{tr}(\mathbf U^{(d)}) &= \text{tr}\left( \mathbf U^{(1)} \prod_{d=2}^D \text{tr}(\mathbf U^{(d)})\right)\\
        &= \text{tr}\left(\mathbb E(\mathbf X \times_1 \mathbf X)\right)\\
        &= \sum_i \mathbb E(\mathbf X \times_1 \mathbf X)_{ii}\\
        &= \sum_i \sum_{k_2\dots k_D} \mathbb E (X_{i k_2 \dots k_D} X_{i k_2 \dots k_D})\\
        &= \mathbb E \sum_{k_1\dots k_D} X_{k_1 \dots k_D}^2\\
        &= \mathbb E ||\mathbf X||_\mathcal{F}^2
    \end{align}
\end{subequations}
where we have used the Frobenius norm to represent the sum of all squared elements of the tensor. So the denominator will be $\mathbb E( ||\mathbf X||_\mathcal{F}^2)^{D-1}$.

Also similar to the matrix-normal case, the numerator is very cheap to compute. Observe that
\begin{subequations}
    \begin{align}
        \sum_{i_d,j_d}\mathbb E(\mathbf X \times_d \mathbf X)_{i_dj_d} &= \mathbb E \sum_{i_d,j_d} \sum_{k_1 \dots k_{d-1}k_{d+1}\dots k_D} X_{k_1 \dots k_{d-1}i_d k_{d+1}\dots k_D} X_{k_1 \dots k_{d-1}j_d k_{d+1}\dots k_D}\\
        &= \mathbb E \sum_{k_1 \dots k_{d-1}k_{d+1}\dots k_D} \sum_{i_d,j_d} X_{k_1 \dots k_{d-1}i_d k_{d+1}\dots k_D} X_{k_1 \dots k_{d-1}j_d k_{d+1}\dots k_D}\\
        &= \mathbb E \sum_{k_1 \dots k_{d-1}k_{d+1}\dots k_D} \left( \sum_{i_d} X_{k_1 \dots k_{d-1}i_d k_{d+1}\dots k_D} \right)^2
    \end{align}
\end{subequations}
which in pseudocode can be written as $\mathbb E (\texttt{X.sum(d).square().sum()})$.

Hence, using pseudocode to make things clearer, our normaliser can be estimated as
\begin{equation}
    ||\Delta_\ell \mathbf f||_\text{RMS} \approx \sqrt{\frac{\prod_{d=1}^D \mathbb E (\texttt{Z.sum(d).square().sum()})}{\mathbb E(\texttt{Z.square().sum()})^{D-1}}}
\end{equation}
where \texttt{Z} is our tensor of updates times phi-gradients, and the expectation symbols $\mathbb E$ tell us what we're taking EMAs of. Remarkably, we can see that we only have to track $D+1$ scalar EMAs, and updating these EMAs only requires simple \texttt{sum()} and \texttt{square()} operations. For numerical stability, it is wise to compute this quotient in log-domain. 

\section{Extra Plots}

\subsection{Full function-space learning rate vs time plot}
Figure \ref{fig:fslr_vs_time_appendix} shows the function-space learning rates over time as in Figure \ref{fig:fslr_vs_time_mainpaper} in the main text, but also with the Transformer (PreNorm) and Transformer (PreNormPostMod) models. The plots for the three transformer layernorm variants all show very similar behaviour.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/fslr_vs_time_full.pdf}
    \caption{Function-space learning rates over time, measured using our approach, for the ResMLP model (row 1) and the transformer models with different layernorm strategies (rows 2, 3, and 4).}
    \label{fig:fslr_vs_time_appendix}
\end{figure}

\subsection{\flerm{} with periodic updates to the learning rate}
\label{sec:app:extraplots:periodicupdate}
In the FLeRM experiments in the main text (Section \ref{sec:experiments:flerm}) we only used FLeRM to modify the learning rate during the first step of training, and then used that learning rate for the rest of training. There is nothing to stop us from periodically updating the learning rate throughout training using FLeRM if necessary, e.g. if we think the dynamics of training might be affected by width or depth in a time-dependent manner. Here we present plots similar to Figures \ref{fig:widthtransfer_onlyfirststepflerm_mainpaper}, \ref{fig:depthtransfer_onlyfirststepflerm_mainpaper}, and \ref{fig:initscaletransfer_onlyfirststepflerm_mainpaper} from the main text, but in addition to using FLeRM to compute the learning rate at the start, we also update the learning rate every 100 iterations using a single batch of data and an EMA decay rate $\beta=0.9$. The resultant width, depth, and initialisation scale transfer plots are given in Figures \ref{fig:widthtransfer_periodicupdate_appendix}, \ref{fig:depthtransfer_periodicupdate_appendix}, and \ref{fig:initscaletransfer_periodicupdate_appendix}. The plots are very similar to those using the static learning rate, suggesting that the effects of increasing width, depth, or initialisation scale can be effectively cancelled-out using a learning rate computed purely at the start of training. This agrees with prior work like \textmu{}P \citep{yang2022featurelearninginfinitewidthneural} and Module \citep{large2024scalableoptimizationmodularnorm}, which both achieve hyperparameter transfer in width and depth scaling settings using static learning rates.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/widthtransfer_onlyfirststepflerm.pdf}
    \caption{Normalising function-space learning rates periodically using FLeRM gives very similar width transfer to normalising using FLeRM at the first iteration only (see Figure \ref{fig:widthtransfer_onlyfirststepflerm_mainpaper}). Top: standard practice. Bottom: our scheme (\flerm{}). PreNorm, PostNorm, and PreNormPostMod are different Layernorm configurations described in Section \ref{sec:app:modeldetails}}.
    \label{fig:widthtransfer_periodicupdate_appendix}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/depthtransfer_onlyfirststepflerm.pdf}
    \caption{Normalising function-space learning rates periodically using FLeRM gives very similar depth transfer to normalising using FLeRM at the first iteration only (see Figure \ref{fig:depthtransfer_onlyfirststepflerm_mainpaper}). Top: standard practice. Bottom: our scheme (\flerm{}).}
    \label{fig:depthtransfer_periodicupdate_appendix}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/inittransfer_onlyfirststepflerm.pdf}
    \caption{Normalising function-space learning rates periodically using FLeRM gives very similar initialisation scale transfer to normalising using FLeRM at the first iteration only (see Figure \ref{fig:initscaletransfer_onlyfirststepflerm_mainpaper}). Top: standard practice. Bottom: our scheme (\flerm{}).}
    \label{fig:initscaletransfer_periodicupdate_appendix}
\end{figure}

\subsection{\flerm{} with equal base model function-space learning rates (but still splitting them in the same way as depth scales}
\label{sec:app:equalmassbutstillsplittingdepthproperlyablation}
Here we replace the recorded base model function-space learning rates with equal values for each layer, that add up to 1. As depth is increased, with still split the function-space learning rates up between the new hidden blocks (described in Section \ref{sec:methods:flerm}) as we did in the main experiments. See Figures (\ref{fig:widthtransfer_equalmassbutstillsplittingdepthproperlyablation}), (\ref{fig:depthtransfer_equalmassbutstillsplittingdepthproperlyablation}), and (\ref{fig:initscaletransfer_equalmassbutstillsplittingdepthproperlyablation}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/widthtransfer_equalmassbutstillsplittingwithdepthablation_onlyfirststepflerm.pdf}
    \caption{Equal base FSLR ablation (width): Using equal base model function-space learning rates results in a degradation in performance in some of the models, compared to the main experiments. Top: standard practice. Bottom: our scheme (\flerm{}) with equal base model function-space learning rates.}
    \label{fig:widthtransfer_equalmassbutstillsplittingdepthproperlyablation}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/depthtransfer_equalmassbutstillsplittingwithdepthablation_onlyfirststepflerm.pdf}
    \caption{Equal base FSLR ablation (depth): Using equal base model function-space learning rates results in a degradation in performance in some of the models, compared to the main experiments. Top: standard practice. Bottom: our scheme (\flerm{}) with equal base model function-space learning rates.}
    \label{fig:depthtransfer_equalmassbutstillsplittingdepthproperlyablation}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/inittransfer_equalmassbutstillsplittingwithdepthablation_onlyfirststepflerm.pdf}
    \caption{Equal base FSLR ablation (init. scale): Using equal base model function-space learning rates results in a degradation in performance in some of the models, compared to the main experiments. Top: standard practice. Bottom: our scheme (\flerm{}) with equal base model function-space learning rates.}
    \label{fig:initscaletransfer_equalmassbutstillsplittingdepthproperlyablation}
\end{figure}

\subsection{\flerm{} with equal base function-space learning rates that are ALWAYS equally divided across layers, even with scaled depth}
\label{sec:app:EQUALmassablation}
This ablation is similar to the equal base function-space learning rates ablation in Appendix \ref{sec:app:equalmassbutstillsplittingdepthproperlyablation}, but instead of sharing the values across the new hidden layer blocks as described in Section \ref{sec:methods:flerm}, we simply set all the ``base" function-space learning rates for the deeper model to be equal and sum to 1. Note that a key difference in this approach is that the input embedding and readout layer's base function-space learning rate is going to shrink as the model gets deeper, whereas in the ablation in Appendix \ref{sec:app:equalmassbutstillsplittingdepthproperlyablation}, the base function-space learning rates for the input embedding and readout layer remained constant, whilst only the hidden layers got diluted when scaling depth. As suggested in previous work \citep{large2024scalableoptimizationmodularnorm}, ensuring input embeddings and readout layers retain their ``importance" during training in deeper models is critical for achieving depth transfer, and we observe in this ablation (Figure \ref{fig:depthtransfer_EQUALmassablation}, only depth scaling is changed from the other equal function-space learning rate ablation) that we no longer have hyperparameter transfer across depth. Interestingly, however, the performance of the deepest PreNormPostMod model is better in this ablation than in the other \flerm{} experiments, actually marginally beating the standard practice model too, suggesting further refine of the scheme for splitting base FSLRs as depth increases could be beneficial. However, performance in this ablation is also far worse on other models like PostNorm at high depths.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures/depthtransfer_EQUALmassablation_onlyfirststepflerm.pdf}
    \caption{Equal base FSLR ablation where we don't carefully retain the importance of the input embedding and readout layers (depth): Just setting the base model FSLRs to be equal and sum to one, no matter what the depth is, means that we no longer have depthwise hyperparameter transfer in the PreNormPostMod model. Top: standard practice. Bottom: our scheme (\flerm{}) with equal base model function-space learning rates that don't try to account for depth increase.}
    \label{fig:depthtransfer_EQUALmassablation}
\end{figure}

\subsection{LoRA}\label{app:lora_extra_exp}
In Section~\ref{sec:experiments:lora}, we showed results for sweeping over the learning rate of the LoRA adapters' B parameter, with a fixed learning rate for A. Here, we show results for the reverse: sweeping over the learning rate of A with a fixed learning rate for B. Figure~\ref{fig:lora_rank_transfer_fix_b} shows the results. Interestingly, we achieve learning rate transfer with the Standard AdamW optimiser, mainly because varying the learning rate of $A$ does not change the performance of the model (until it becomes too large). This suggests that selecting the learning rate of $A$ is not very important compared to selecting the learning rate of $B$. We find that with FLeRM, we mostly preserve the learning rate transfer, and we also observe an instability shift to the left.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.999\linewidth]{figures_lora/loss_lr_lora_fix_b.pdf}
    \caption{Behaviour of training log likelihood loss under varying the learning rate of $A$ and LoRA rank for two continual pretraining tasks. The top row shows results from a standard setup with AdamW, the bottom row shows our method, FLeRM.}
    \label{fig:lora_rank_transfer_fix_b}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
