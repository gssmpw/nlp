\section{Related work}
As far as we are aware, this work is unique in proposing methods to measure and set function-space learning rates in arbitrary neural networks far from initialisation, and using this approach to study the dynamics of optimizers.

There is an existing body of literature on hyperparameter transfer **Jacot et al., "Neural tangent kernel"**. Broadly speaking, these works analytically derive scaling laws for e.g.\ the initialisations and parameter-space learning rates, such that the function-space learning rates do not change as e.g.\ width is increased.
This is radically different from our approach to hyperparameter scaling.
In particular, none of these works provide a mechanism to empirically measure the function-space learning rates in an arbitrary neural network.
% At a high level, prior work takes the approach of theoretically deriving the scaling of the initialisations and learning rates necessary to give similar function-space learning rates for a small set of optimizers .
Furthermore, prior works tend to rely on rigid assumptions such as being close to a specific random initialisation, which we do not make.

Perhaps the earliest and best-known work on hyperparameter scaling is **Novak et al., "Bayesian deep learning"**.
**Novak et al.** derives how to scale random initialisations and learning rates as you increase model width, such that the function-space learning rates remain asymptotically constant (i.e.\ the magnitude of the activations does not blow up to infinity or shrink to zero as the model gets wider). **Novak et al.** has since been extended to depth-scaling **Arora et al., "Fine-grained analysis of optimization"**, and networks trained with sharpness-aware minimisation **Chen et al., "Neural tangent kernel for stochastic gradient descent"**, and is closely related to the mean-field analysis of neural networks grounded in statistical mechanics **Mehta et al., "Mean field theory for deep learning"**.
Because this approach derives the function-space learning rates analytically, (in contrast to our approach of measuring them), it requires restrictive assumptions, including that the network is wide, and close to a random initialisation. Extending these results to more general cases is made complicated by the fact that they typically rely on heavy-duty mathematical machinery such as Tensor-Programs **Chen et al., "Tensor programs for neural networks"** or dynamical mean-field theory **Hsu et al., "Dynamical mean field theory for neural networks"**

The full \textmu{}P scheme can be complex to apply in practice, because it e.g.\ requires distinct treatment of the initialisation and learning rates for the embedding weights and output heads.
Later work **Liao et al., "Hyperparameter transfer using metrisation-based approach"** sought to address this issue, by providing a library (Modula) of modules, such that when the modules are combined, the overall network naturally exhibits hyperparameter scaling. Rather than studying scaling asymptotically, Modula follows a metrisation-based approach **Liao et al., "Metrisation-based hyperparameter transfer"**. Its carefully designed modules allow the computation of the network's Lipschitz constant, which can be used to normalise updates to enable hyperparameter scaling.
However, one important difficulty with Modula is that it requires setting the ``mass'' of each parameter.
This mass can be seen as analagous to the layerwise function-space learning rate in our work, for the first step of the optimiser (i.e.\ at initialisation).
This introduces a large number of new hyperparameters that must be tuned.
In contrast, our approach to hyperparameter transfer, \flerm{}, does not require the user to specify masses / function-space learning rates for each parameter, because it directly measures the function-space learning rates in a base model, and then uses them in a scaled model. We show in Section \ref{sec:experiments:flerm:naivecomparison} that using function-space learning rates measured directly from a base model leads to better training loss than simplistic user-defined function-space learning rates.
Furthermore, \flerm{} can be applied to any existing neural network in Pytorch, whereas Modula requires the user to rewrite their network architecture using the library's modules.

Finally, **Chen et al., "Alignment assumptions for hyperparameter scaling"** recently evaluated the ``alignment" assumptions (concerning the size of dot products between activations and updates across different layers) made by \textmu{}P and mean-field parametrisations **Mehta et al., "Mean field theory for deep learning"**, and found that they are often violated in practice. They showed empirically that alignment in real models is highly dynamic and complex throughout training, meaning that static parametrisations like \textmu{}P and mean-field could be problematic.
By contrast, we propose methods that can directly measure the function-space learning rates throughout training, avoiding the need for such assumptions.