\section{Related Work}
We review the works that are mostly related to our project in this section.

\paragraph{Self-rewarding alignment.} Our work aligns with research on self-rewarding alignment **Brown et al., "Meta Reinforcement Learning"**, where both of our project and their methods share similar spirits that we can unify the generation ability and evaluation ability into a single LLM. These methods leverage iterative DPO-type algorithms, where the model labels its own generated responses to provide training signals for subsequent iterations, enabling \textit{self-improvement}. In contrast, our approach does not focus on self-improvement during training. Instead, we rely on an external ground-truth reward model to provide learning signals in training. Our study emphasizes inference-time alignment for reasoning-focused LLMs, where self-rewarding signals are employed solely to guide inference rather than training.

\paragraph{Self-correction.} Our work is closely related to self-correction in LLMs. We refer interested readers to the survey **Henderson et al., "A Survey of Self-Improving Neural Networks"** for a more comprehensive review and only review some representative approaches that are mostly related to our project. **Bansal et al., "Robust Training through Adversarial Self-Reflection"** demonstrated that incorporating teacher model reflections into SFT data enhances students' self-reflection abilities in general-purpose conversation tasks. However, for reasoning tasks, **Meng et al., "Self-Correction in Language Models"** found that current LLMs—without additional training—fail to self-correct purely through intrinsic reasoning (i.e., prompting). This observation is also validated in **Brown et al., "Meta Reinforcement Learning"**. A more in-depth analysis shows that most prior successful studies in this domain depend on \textbf{external} (ground-truth) reward models to determine when to initiate and terminate self-correction ____. Currently, there is no major work demonstrating that intrinsic self-correction (via prompting or fine-tuning) is reliably effective. Furthermore, because external reward models are typically LLM-based, these methods introduce additional computational overhead by requiring a multi-agent system for inference.

Recognizing this challenge, our study explores how LLMs can autonomously evaluate response quality and correct errors without external reward models. Specifically, we introduce a self-rewarding reasoning framework that enables a single LLM to perform error detection and self-correction effectively. Among the works in self-correction, the most relevant work is the recent **Rajani et al., "Self-Correcting Models for Language Understanding"**, which employed a multi-turn deep RL approach to train self-correcting models. In comparison, this work introduces a new and general self-rewarding formulation for reasoning-focused LLMs, with self-correction as a representative application. Compared to the intrinsic correction and the framework in **Henderson et al., "A Survey of Self-Improving Neural Networks"**, one major difference is that our framework equips models with self-rewarding ability, enabling our models to intelligently scale inference compute by \textit{selectively} revising the first attempts, which helps to reduce computational overhead by avoiding unnecessary iterations. We will also design experiments to illustrate this idea.

Algorithmically, our approach also differs from **Rajani et al., "Self-Correcting Models for Language Understanding"**. We first use sequential rejection sampling to construct long CoT trajectories with both self-rewarding and self-correction patterns, which serve as warm-up fine-tuning data. We then enhance these behaviors through reinforcement learning (using either DPO-type algorithms or PPO) with rule-based signals. In contrast,  **Henderson et al., "A Survey of Self-Improving Neural Networks"** employed RLOO ____ with a specialized reward function for a two-turn self-correction task. While their no-public models (Gemini) and implementation details (parameters, codes) do not enable comparison, we believe that the multi-turn RL methods proposed by **Rajani et al., "Self-Correcting Models for Language Understanding"** could also complement the proposed self-rewarding framework, and achieve better reasoning performance compared to the standard reasoning models.
\vspace{-8pt}
\paragraph{Rule-based RL for LLMs mathematical reasoning.} Rule-based reinforcement learning has received significant attention following the success of DeepSeek-R1 **Rajani et al., "Self-Correcting Models for Language Understanding"**. Open-source efforts have since attempted to replicate its performance using Qwen models ____**, including works such as ____**. These methods train LLMs using only the correctness score (whether the final answer is correct or not) and a format score (whether the final answer is output in a pre-determined format), in contrast to the previous works with the neural network-based reward model ____**.  
In particular, ____** observed that self-correction naturally emerges during RL training (referred to as an AHA moment in their report). However, our preliminary experiments, along with open-source replications using Qwen-2.5-Math ____**, suggest that (i) the base models already exhibit some self-correction ability, though it is quite sparse. (ii) vanilla rule-based RL \textbf{cannot} consistently enhance self-correction without additional design.

Interestingly, even when using the same algorithms and data, similar improvements in mathematical reasoning are not observed in models such as Llama ____**. We hypothesize that Qwen-2.5-Math ____** and DeepSeek-R1 **Rajani et al., "Self-Correcting Models for Language Understanding"** benefit from extensive pre-training on high-quality mathematical corpora (e.g., 1T tokens for Qwen-2.5-Math ____**), and that the AHA moment may stem from carefully curated data containing self-correction patterns in pre-training or a cool-down stage. Since these datasets are non-public, the exact details remain unknown.

In contrast, our study shows that a warm-up stage using a carefully curated SFT dataset (collected via sequential rejection sampling) enables models to learn self-correction patterns more reliably. This foundation allows rule-based RL to further enhance these behaviors in a stable manner. We also remark that our two-stage framework and most of the associated experiments are performed prior to the release of DeepSeek-R1.


\begin{table*}[t]
\centering
\begin{minipage}{2.0\columnwidth}\vspace{0mm}    \centering
\begin{sectionbox}[]%{An example of the self-rewarding reasoning path under the two-turn conversation framework.} 
    \centering
    %  \hspace{-10mm}
      \small
\begin{tabular}{p{0.97\columnwidth} c}
    \textbf{User:} A particular star has an absolute magnitude $M=-7$. If this star is observed in a galaxy that is at a distance of $3 \mathrm{Mpc}$, what will its apparent magnitude be? \\ Please reason step by step, and put your final answer within $\boxed{}$. \textcolor{blue}{(prompt $x$)}\\\\

    \textbf{Assistant:} To determine the apparent magnitude of the star, we can use the distance modulus formula : $ m - M = 5 \log_{10}(d) - 5 $ where:$ m $ is the apparent magnitude,$ M $ is the absolute magnitude,$ d $ is the distance to the star in parsecs.... The apparent magnitude of the star, when observed from a distance of 3 Mpc, is approximately $ 7.58 $. So the final answer is: $\boxed{7.58}$. \textcolor{blue}{(initial response $a^1$)}\\\\
    To verify the correctness of the solution, let's recheck the steps ... So the final answer is: $\boxed{20.39}$. The self-evaluation is: [VERIFY] wrong. \textcolor{blue}{(self-rewarding evaluation $y^1$)} \\\\
    
    Let's correct it step by step. Given: Absolute magnitude ... The correct apparent magnitude is approximately $ 20.39 $. So the final answer is: $\boxed{20.39}$. \\\\ \textcolor{blue}{(revised response $a^2$)}.

    Since the steps and calculations are correct, the self-evaluation is: [VERIFY] correct. \textcolor{blue}{(self-rewarding evaluation $y^2$)}
    \end{tabular}
\end{sectionbox}
\caption{An example of the self-rewarding reasoning path. We omit the detailed reasoning path for a clear presentation. The full trajectory is available at Table~\ref{tab:example_hh_rlhf_dataset} in Appendix.} 
    \label{fig:example0}
\end{minipage}
\end{table*}