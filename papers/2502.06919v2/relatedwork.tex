\section{Related Work}
\subsection{Temporal Abstraction}
Temporal Abstraction is proposed in the semi-MDP formulation~\citep{sutton1999between,precup2000temporal} and commonly implemented based on the \emph{options} framework~\citep{stolle2002learning,bacon2017option,harutyunyan2018learning}.
Each option describes a low-level policy and is defined as $\langle\mathcal{I},\pi,\beta\rangle $, where $\mathcal{I}$ denotes the admissible states for the option initialization, $\pi$ is the policy that the option follows, and $\beta$ determines when the option is terminated.
High-level policies are trained to solve tasks utilizing temporally extended actions provided in the options, rather than one-step actions without action persistence.
Plenty of Hierarchical RL methods are proposed based on temporal abstraction, achieving faster exploration and higher sample efficiency in various sequential decision tasks~\citep{lin2021juewu,yang2021hierarchical}.
Some works are proposed to learn to design options through various techniques, including discovering state connectedness~\citep{chaganty2012learning}, replay buffer analysis~\citep{eysenbach2019search}, and learning termination criteria~\citep{vezhnevets2016strategic,harutyunyan2019termination}.
However, designing options is still a challenging task, which requires prior knowledge and handcraft tunning~\citep{pateria2021hierarchical,yu2021taac,lee2024learning}. 


\subsection{Action Repetition}
One simple option strategy is repeating a primitive action for a number of steps, which is similar to the frame-skipping utilized in RL for video games~\citep{bellemare2013arcade,braylan2015frame}.
Recently, action repetition has been actively researched and widely adopted in RL, which can achieve deeper exploration~\citep{dabney2021temporallyextended}, improve sample efficiency by reducing control granularity~\citep{biedenkapp2021temporl}, and reduce action oscillations~\citep{chen2021addressing}.
Existing repetition works are classified as two categories: \emph{open-loop} and \emph{closed-loop} manners.
Open-loop methods force the agent to repeat actions for a predicted number of steps without opportunity of early terminations, such as DAR~\citep{lakshminarayanan2017dynamic}, FiGAR~\citep{sharma2017learning}, TempoRL~\citep{biedenkapp2021temporl}, and UTE~\citep{lee2024learning}.
In contrast, closed-loop methods conduct \emph{act-or-repeat} binary decision to decide if the previous action should be repeated, such as PIC~\citep{chen2021addressing} and TAAC~\citep{yu2021taac}.
Compared to open-loop methods, closed-loop methods examine whether to repeat based on the current state, which is more flexible and improves performance in emergency situations.
In this work, we propose a new closed-loop method to conduct act-or-repeat selections for each actuator individually given current state, which is more flexible and achieves higher action persistence with sufficient action diversity.