\section{Related Work}
\subsection{Temporal Abstraction}
Temporal Abstraction is proposed in the semi-MDP formulation____ and commonly implemented based on the \emph{options} framework____.
Each option describes a low-level policy and is defined as $\langle\mathcal{I},\pi,\beta\rangle $, where $\mathcal{I}$ denotes the admissible states for the option initialization, $\pi$ is the policy that the option follows, and $\beta$ determines when the option is terminated.
High-level policies are trained to solve tasks utilizing temporally extended actions provided in the options, rather than one-step actions without action persistence.
Plenty of Hierarchical RL methods are proposed based on temporal abstraction, achieving faster exploration and higher sample efficiency in various sequential decision tasks____.
Some works are proposed to learn to design options through various techniques, including discovering state connectedness____, replay buffer analysis____, and learning termination criteria____.
However, designing options is still a challenging task, which requires prior knowledge and handcraft tunning____. 


\subsection{Action Repetition}
One simple option strategy is repeating a primitive action for a number of steps, which is similar to the frame-skipping utilized in RL for video games____.
Recently, action repetition has been actively researched and widely adopted in RL, which can achieve deeper exploration____, improve sample efficiency by reducing control granularity____, and reduce action oscillations____.
Existing repetition works are classified as two categories: \emph{open-loop} and \emph{closed-loop} manners.
Open-loop methods force the agent to repeat actions for a predicted number of steps without opportunity of early terminations, such as DAR____, FiGAR____, TempoRL____, and UTE____.
In contrast, closed-loop methods conduct \emph{act-or-repeat} binary decision to decide if the previous action should be repeated, such as PIC____ and TAAC____.
Compared to open-loop methods, closed-loop methods examine whether to repeat based on the current state, which is more flexible and improves performance in emergency situations.
In this work, we propose a new closed-loop method to conduct act-or-repeat selections for each actuator individually given current state, which is more flexible and achieves higher action persistence with sufficient action diversity.