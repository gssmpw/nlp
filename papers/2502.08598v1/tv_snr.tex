\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{caption} 
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{makecell}
\usepackage{array}

\usepackage{hyperref}

\usepackage{microtype}
\usepackage[]{graphicx}
\usepackage{booktabs}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{authblk}
\renewcommand\Authfont{\bfseries}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[thinc]{esdiff}

\input{math_commands.tex}

\usepackage{nkj}

\renewcommand{\eqref}[1]{(\ref{#1})}
% Define the \rebuttal command
\newcommand{\rebuttal}[1]{\textcolor{blue}{#1}}
\newcommand*{\dv}{\mathrm{d}}
\renewcommand{\qed}{\triangle}

\newcommand{\memo}[1]{{\color{red}#1}}

\title{Enhancing Diffusion Models Efficiency by Disentangling Total-Variance and Signal-to-Noise Ratio}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author[1,2]{Khaled Kahouli\thanks{correspondence to \texttt{khaled.kahouli@tu-berlin.de}}\hspace{0.13cm}}
\author[1,2]{Winfried Ripken}
\author[1,2]{Stefan Gugler} 
\author[5]{Oliver T. Unke } 
\author[1,2,3,4,5]{\\Klaus-Robert M\"uller}
\author[1,2,6]{Shinichi Nakajima}

\affil[1]{BIFOLD â€“ Berlin Institute for the Foundations of Learning and Data}
\affil[2]{Machine Learning Group, Technische Universit\"at Berlin}
\affil[3]{Department of Artificial Intelligence, Korea University}
\affil[4]{Max-Planck Institute for Informatics}
\affil[5]{Google DeepMind}
\affil[6]{RIKEN Center for Advanced Intelligence Project}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{TV/SNR Diffusion}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={Enhancing Diffusion Models Efficiency by Disentangling Total-Variance and Signal-to-Noise Ratio},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\begin{abstract}
    The long sampling time of diffusion models remains a significant bottleneck, which can be mitigated by reducing the number of diffusion time steps. 
    However, the quality of samples with fewer steps is highly dependent on the noise schedule, i.e., the specific manner in which noise is introduced and the signal is reduced at each step. 
    Although prior work has improved upon the original variance-preserving and variance-exploding schedules, these approaches \emph{passively} adjust the total variance, without direct control over it. 
    In this work, we propose a novel total-variance/signal-to-noise-ratio disentangled (TV/SNR) framework, where TV and SNR can be controlled independently. Our approach reveals that different existing schedules, where the TV explodes exponentially, can be \emph{improved} by setting a constant TV schedule while preserving the same SNR schedule.
    Furthermore, generalizing the SNR schedule of the optimal transport flow matching significantly improves the performance in molecular structure generation, achieving few step generation of stable molecules.
    A similar tendency is observed in image generation, where our approach with a uniform diffusion time grid performs comparably to the highly tailored EDM sampler.
\end{abstract}


% keywords can be removed
% \keywords{Generative modeling \and Diffusion models \and Signal-to-noise-ratio \and Flow matching \and Differential equations}



\begin{section}{Introduction}

 \begin{figure*}[t]
     \centering
     \includegraphics[width=.95\textwidth]{figures/overview_plot.pdf}
     \caption{Our TV/SNR schedule generates stable molecules with diffusion models using far fewer steps $T$ than established methods like EDM (top).  For image generation, it matches the optimized EDM sampler's performance without image-specific changes (bottom).}
     \label{fig:overview}
 \end{figure*}

 With the development of diffusion models~\citep{diff_mod_sohl,DDPM_Ho,song2020score,song2021score}, generative modeling has witnessed great progress in recent years.
 They have demonstrated remarkable capabilities across various traditional domains such as image synthesis \citep{diffbeatsgan, nichol2022glide,rombach2022high, DiT, SiT} and audio generation \citep{kong2021diffwave, chen2020wavegrad, audioLDM}.
In computational chemistry, diffusion models have been increasingly utilized as an efficient alternative 
%to conventional techniques
for tasks like molecule generation~\citep{gebauer2022inverse,edm, diffbridges, mdm, geoldm, moldiff, vignac2023midi, eqgat-diff}, conformer search~\citep{geodiff}, and molecular graph generation~\citep{vignac2023digress, autoregressivediff}. Beyond generation, \citet{kahouli2024morered} introduced MoreRed, which employs reverse diffusion to learn a pseudo-potential energy surface instead of the physical one, thereby enabling denoising from arbitrary noisy states and retrieving ground states.

Despite their superior generative capabilities, such as achieving state-of-the-art Fr\'{e}chet Inception Distance (FID) scores in image generation, diffusion models are computationally expensive during inference. Generating samples requires iteratively solving a reverse stochastic process, where a learned \emph{score function}, generally a deep neural network, must be evaluated at each step. Therefore, reducing the sampling cost without degrading the sample quality requires solving the reverse process with minimal score function evaluations while avoiding high discretization errors. To address this challenge, prior work has explored both training-based improvements~\citep{imp_ddpm, watson2022learning, salimans2022progressive, cm} and advanced sampling techniques, including higher-order solvers, to replace conventional first-order methods~\citep{dpm-solver, dpm-solver-v3, liu2022pseudo, zhang2023fast, dockhorn2022genie, jolicoeur-martineau2022gotta}. Moreover, the critical role of the noise schedule in determining the performance of diffusion models was highlighted in previous work \citep{chen2023importance, lin2024common}. Complementary to these approaches, the seminal work of \citet{diff_edm} introduced EDM, which modifies the reverse process to follow straighter trajectories and employs a non-uniform time grid to reduce accumulated discretization errors. Additionally, by leveraging a second-order ODE solver, EDM further enhances both sampling efficiency and quality.
Building on the concept of straight paths in probability flows, flow matching was proposed as a simulation-free approach to train continuous normalizing flows and as a generalization of diffusion models~\citep{liu2023flow, lipman2023flow}. In this framework, the particular case of linear interpolation by using optimal transport as a conditional probability path has been explored to enforce straight ODE trajectories \citep{albergo2023building, pmlr-v202-pooladian23a, tong2024improvingflow}. Flow matching has also been successfully applied to molecular tasks~\citep{klein2023equivariant, song2023equivariant, irwin2024efficient}. Besides, previous work emphasised the importance of the noise schedule for the performance of diffusion models.

Given a data sample $\rvx(0)$, the forward diffusion process employing a standard Wiener process can be modeled with a time-dependent perturbation kernel:
    \begin{align}
             p(\rvx(t)|\rvx(0)) = \mathcal{N} \left(\rvx(t); a(t) \rvx(0), b^2(t) \mI \right),
             \label{eq:PerturbationKernel}
         \end{align}
where $t \in [0, 1]$, $a(t)$ controls the signal strength, $b(t)$ controls the noise level at each diffusion step, and $\bfI$ denotes the identity matrix. $a(t)$ and $b(t)$ are smooth non-negative functions that satisfy the following conditions: for small $\Delta t$, $a(\Delta t) \approx 1$ and $b(\Delta t) \approx 0$, ensuring that $\rvx(\Delta t) \approx \rvx(0)$. Additionally, $a(1) \ll b(1)$ to ensure that $\rvx(1)$ follows an isotropic
%standard
Gaussian distribution.
The two common diffusion processes are Variance-Preserving (VP)~\citep{DDPM_Ho} and Variance-Exploding (VE)~\citep{song2019score} each corresponding to specific choices of $a(t)$ and $b(t)$, determined by a pre-defined noise schedule. In particular, EDM adopts the VE variant with $a(t) = 1$ and $b(t) = t$, achieving state-of-the-art performance in diffusion-based image generation \citep{diff_edm}.

In this paper, we aim to further improve noise scheduling by introducing a total-variance/signal-to-noise ratio (TV/SNR) disentangled framework, where the TV defined as $\tau^2(t) =a^2(t) + b^2(t)$, and the SNR as $\gamma(t) = a(t)/ b(t)$ \citep{kingma2021} 
%with both quantities
are
independently controlled. The reverse process is then solved using the corresponding ODE/SDE.
Notably, our empirical results indicate that for  commonly used scheduling methods, where the total variance (TV) explodes exponentially, modifying the TV schedule to follow a VP trajectory (i.e., $\tau(t) = 1$) while keeping the SNR schedule $\gamma(t)$ unchanged significantly improves the performance of fast sample generation {(see \Cref{fig:ComparisonMolecule} left)}.
Based on this observation, we assume that the constant TV schedule, $\tau(t)=1$, is already a sufficiently effective choice,
although we do not fully explore all possibilities of more sophisticated TV schedules $\tau(t)$.
Instead, we focus on optimizing the SNR schedule. Specifically, we propose an exponential inverse sigmoid function to schedule the SNR, which allows rapid SNR decay both at the beginning ($t \approx 0$) and the end ($t \approx 1$) of the diffusion process.
This SNR schedule can be seen as a generalization of optimal transport flow matching (OTFM), and shows
state-of-the-art performance in molecular structure generation.  We observe a similar tendency in image generation tasks, where 
our SNR schedule shows comparable performance to the state-of-the-art methods
while requiring minimal parameter tuning and using a uniform diffusion time grid. 
By conducting a numerical analysis of ODE trajectories, we associate the effectiveness of our approach with the curvature of the trajectories when $ t \ll 1$, meaning \emph{close to the data space}, and the time evolution of the marginal distribution path. This provides a new insight into fast sample generation using diffusion models. Our source code can be accessed at \href{https://github.com/khaledkah/tv-snr-diffusion}{https://github.com/khaledkah/tv-snr-diffusion}.

% \textbf{Contributions:}
The key contributions of this paper are summarized as follows:
 \begin{itemize}
     \item We introduce a novel TV/SNR disentangled framework, where TV and SNR are independently controlled. Many existing diffusion models can be subsumed within this unifying framework (see \Cref{tab:ExistingTVSNR}).

     
     \item For commonly used schedules exhibiting exponential TV behavior, we empirically show that their VP versions lead to significant improvements in sample generation performance for both molecular structure and image generation tasks. 
     %\item We derive the forward and reverse SDEs for this unified framework and provide analytical expressions
           %\item Time step prediction explanation in SDE framework.
           %\item Dynamic time-dependent stochasticity control in the reverse diffusion process.
           %\item Diffusion guidance with time step prediction.
     \item We propose to schedule the SNR using an exponential inverse sigmoid function, which shows state-of-the-art sampling performance.
           %\item Accurate (rare-class) conditional denoising for MEP's products and reactants identification.
     % \item We identify the score estimation error as another factor that affects optimal noising schedules. 
     \item We numerically analyze the ODE trajectories near the data space and the time evolution of marginal distribution path, and discuss their importance for fast sample generation.

 \end{itemize}

\end{section}


\begin{section}{Background}

\subsection{Diffusion Models}

 Let $p_\mathrm{data}(\rvx(0))$ be the data distribution of interest defined on the support $\mathcal{X} \subseteq \R^d$.
 We assume that the data are standardized, i.e., $\mathbb{E}_{p_\mathrm{data}}[\rvx(0)] = \bfzero$ and 
 $\mathrm{Var}_{p_\mathrm{data}}[\rvx(0)] = \bfone$. 
 Let $\{\rvx(t)\}_{t \in [0,1]}$ be the forward diffusion process describing the stochastic flow of the marginal probability density path $\{p_t(\rvx)\}_{t \in [0,1]}$ resulting from iterative injection of Gaussian noise (starting from $p_0 := p_\mathrm{data}$ and ending at a tractable Gaussian prior $p_1(\rvx)$). 
 Once we define the perturbation kernel in Eq.~\eqref{eq:PerturbationKernel}, the marginal density path for $t \in [0, 1]$ is fixed as
 \begin{align}
     p_t(\rvx(t))  =  \int p(\rvx(t)|\rvx(0))p_\mathrm{data}(\rvx(0))\ \dv\rvx(0),
   %  \text{ for }   t \in [0, 1]
     \label{eq:GenGaussPerturbKernel}
 \end{align}
 and the corresponding SDE is 
 \begin{align}
     \dv \rvx & =  \rvx f(t)   \dv t + {g(t)}  \dv \rvw, \quad \mbox{ where }
     \label{eq:GenForwSDE}\\
     f(t) &= \frac{\dot{a}(t)}{a(t)} , 
     \;\;
     %g(t) =\sqrt{2 \frac{ b(t)}{ a(t)} \left( \dot{a}(t) b(t) - a(t) \dot{b}(t)\right)} .
     g(t) =\sqrt{2 \frac{ b(t)}{ a(t)} \left(a(t) \dot{b}(t) - \dot{a}(t) b(t)\right)} .
     \notag
 \end{align}     
 Here, $\rvw$ is the standard Wiener process, $f(t)$ and $g(t)$ are the drift and diffusion coefficients, respectively, and we use Newton's notation for time derivatives, e.g., $\dot{a}(t) = \frac{\dv}{\dv t} a(t)$ \citep{song2021score}. 

The reverse SDE for Eq.~\eqref{eq:GenForwSDE} is given by 
 \begin{align}
     \dv \tilde{\rvx} = \left[ \tilde{\rvx} f(t)   - \frac{1+ \lambda^2}{2} {g(t)}^2 \nabla_{\tilde{\rvx}} \log p_t(\tilde{\rvx})\right] \dv \tilde{t} + \lambda {g(t)} \, \dv \tilde{\rvw}
     \label{eq:GenRevSDE}
 \end{align}
 for $\lambda = 1$, where the tilde indicates time-reversal, i.e. $\tilde{t}=1-t$, and $\nabla_{\rvx} \log p_t(\rvx)$ is the score function of the marginal density at time $t$.
Eq.~\eqref{eq:GenRevSDE} describes the general reverse SDE for $\lambda \geq 0$, including the ones with less ($\lambda < 1$) or more ($\lambda > 1$) stochasticity with the same marginal distribution -- the extreme case where $\lambda = 0$ corresponds to the probability flow ODE \citep{zhang2021diffusion}.

Training diffusion models amounts to approximating the unknown score function of the marginal density in Eq.~\eqref{eq:GenGaussPerturbKernel} for $t= [0, 1]$
by a parametrized neural network, $\rvs_{\theta}(\rvx(t),t) \approx \nabla_{\rvx} \log p_t(\rvx)$.  
This can be performed by
a re-weighted version of denoising score-matching (DSM) \citep{vincent,song2019score,song2021score}:
\begin{align}
    \theta^* 
    & = \argmin_{\theta}
     \E_{p_{\mathrm{data}(\rvx(0))}} \E_{p(\rvx(t)|\rvx(0))} \bigl[ \| \rvepsilon_t - \rvepsilon_{\theta}(\rvx(t), t)\|^2  \bigr],
     \label{eq:DSM} 
 \end{align}
where $\rvepsilon_t \!=\! \frac{\rvx(t) - a(t) \rvx(0)}{b(t)}$ and 
$\rvepsilon_{\theta}(\rvx(t), t) \!=\!- b(t) \rvs_{\theta}(\rvx(t),t)$.
The expectation over $p_\mathrm{data}(\rvx(0))$ is approximated by the average over the training data samples. The expectation over $p(\rvx(t)|\rvx(0))$, which corresponds to simulating the forward process, is numerically performed by applying the perturbation kernel in Eq.~\eqref{eq:PerturbationKernel} to $\rvx(0)$.

\end{section}

         \begin{table*}[t]
         \small
             \centering
             \caption{TV $\tau(t)$ and SNR $\gamma(t)$ schedules corresponding to commonly used diffusion processes within our framework.}
             \renewcommand{\arraystretch}{2}
             \begin{tabular}{lccc}
                 \toprule
                 \textbf{Method}  & $\tau^2(t)$ & $\gamma^2(t)$   & time grid \\
                 \midrule
                 \makecell[l]{\textbf{SMLD} \\[-0.7ex] \scriptsize{\citep{song2019score}}}      & $1 + \sigma_\mathrm{min}^{2}\left( \frac{\sigma_\mathrm{max}}{\sigma_\mathrm{min}}\right)^{2t}$    & $\sigma_\mathrm{min}^{-2}\left( \frac{\sigma_\mathrm{min}}{\sigma_\mathrm{max}}\right)^{2t}$ & uniform   \\
                 \makecell[l]{\textbf{EDM} \\[-0.7ex] \scriptsize{\citep{diff_edm}}}     & $1 + \sigma^2(t)$    & $\sigma^{-2}(t)$ & Eq.\eqref{eq:EDMTimeGrid}  \\
                 \makecell[l]{\textbf{EDM-UT} \\[-0.7ex] \scriptsize{}}    & $1 + \left( \sigma_{\text{max}}^{\frac{1}{\rho}} + (1 - t)  \left( \sigma_{\text{min}}^{\frac{1}{\rho}} - \sigma_{\text{max}}^{\frac{1}{\rho}} \right) \right)^{2 \rho}$    & $\left( \sigma_{\text{max}}^{\frac{1}{\rho}} + (1 - t)  \left( \sigma_{\text{min}}^{\frac{1}{\rho}} - \sigma_{\text{max}}^{\frac{1}{\rho}} \right) \right)^{- 2 \rho}$ & uniform \\
                 \makecell[l]{\textbf{OTFM} \\[-0.7ex] \scriptsize{\citep{lipman2023flow}}} & 
                $1 - 2t (1 - t) (1-\sigma_{\mathrm{min}}) +(1 - t)^2 \sigma_{\mathrm{min}}^2$


                 &
                 $\left(\frac{1-t}{(1-t)\sigma_{\mathrm{min}} + t} \right)^2$
                 & uniform 
                 \\    
                  \makecell[l]{\textbf{DDPM-linear} \\[-0.7ex] \scriptsize{\citep{DDPM_Ho}}} & $1$    & $\left( e^{ \frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) + t \beta_\mathrm{min}} -1 \right)^{-1}$   & uniform         \\  
                 \makecell[l]{\textbf{DDPM-cos} \\[-0.7ex] \scriptsize{\citep{imp_ddpm}}}   & $1$    & $ \left( \left( \frac{\cos\left(\frac{s}{1 + s} \frac{\pi}{2}\right)}{\cos\left(\frac{t^\nu + s}{1 + s} \frac{\pi}{2}\right)} \right)^2 -1 \right)^{-1}$    & uniform        \\     \hline
         
                 \makecell[l]{\textbf{VP-SMLD} \\[-0.7ex] \scriptsize{(Ours)}}     & 1    & $\sigma_\mathrm{min}^{-2}\left( \frac{\sigma_\mathrm{min}}{\sigma_\mathrm{max}}\right)^{2t}$ & uniform   \\                 
                 \makecell[l]{\textbf{VP-EDM-UT} \\[-0.7ex] \scriptsize{(Ours)}}     & 1    & $\left( \sigma_{\text{max}}^{\frac{1}{\rho}} + (1 - t) \left( \sigma_{\text{min}}^{\frac{1}{\rho}} - \sigma_{\text{max}}^{\frac{1}{\rho}} \right) \right)^{- 2 \rho}$ & uniform \\                                  
                 \makecell[l]{\textbf{VP-OTFM} \\[-0.7ex] \scriptsize{(Ours)}}      & 
                 1
                 &
                 $\left(\frac{1-t}{(1-t)\sigma_{\mathrm{min}} + t} \right)^2$                  & uniform 
                 \\ 
                  \makecell[l]{\textbf{VP-ISSNR } \\[-0.7ex] \scriptsize{(Ours)}}      & $1$      & 
                  $\left( \frac{1}{t \, (t_{\mathrm{max}} - t_{\mathrm{min}}) + t_{\mathrm{min}}} -1 \right)^{2 \eta}
\exp( 2 \kappa)$
                  & uniform 
                 \\
                \bottomrule
             \end{tabular}
             \label{tab:ExistingTVSNR}
         \end{table*}


   \begin{figure*}[t]
             %\vspace{-2mm}
             \centering
             
             
             \includegraphics[width=.32\textwidth]{figures/scale_schedules.pdf}
             \includegraphics[width=.35\textwidth]{figures/snr_schedules.pdf}
             \includegraphics[width=.32\textwidth]{figures/inv_sigmoid_schedules.pdf}
             

             
             %\vspace{-3mm}
             \caption{
                 TV (left) and SNR (middle) schedules of the different methods reported in \Cref{tab:ExistingTVSNR}. Right: Inverse Sigmoid SNR schedule. %Eq.~\eqref{eq:SigmoidSNRScheduling}.
             }
             \label{fig:ExistingTVSNR}

    \end{figure*}

         
\subsection{Existing Noising Schedules and Elucidating the Design-space of Diffusion Models (EDM)}


Two popular noising schedules are the Variance-Exploding (VE) \citep{song2019score, song2021score} 
and
Variance-Preserving (VP) \citep{DDPM_Ho} schedules.  
The original VE schedule uses $a(t)=1$ and $b^2(t) = \sigma^2(t)$ to define the perturbation kernel in Eq.~\eqref{eq:PerturbationKernel}, and control $\sigma(t)$ by, e.g., setting $\sigma^2(t) =  \sigma_\mathrm{min}^{2}\left( \frac{\sigma_\mathrm{max}}{\sigma_\mathrm{min}}\right)^{2t}$, where $\sigma_\mathrm{min}$ and $\sigma_\mathrm{max}$ are the minimum and maximum noise level, respectively.
In contrast, the VP schedule uses $a(t) = \sqrt{\bar{\alpha}(t)}$ and $b^2(t) = 1 - a^2(t)$, 
and controls $\bar{\alpha}(t)$ by e.g.,
$\bar{\alpha}(t) = e^{- \beta_{\mathrm{min}} t - \frac{1}{2}(\beta_{\mathrm{max}} - \beta_{\mathrm{min}}) t^2}$, where $\beta_{\mathrm{min}}$ and $ \beta_{\mathrm{max}}$ control the start and end points of the schedule.

Besides, \citet{diff_edm} introduced the EDM framework, which optimizes the reverse process to minimize the number of function evaluations while preserving sample quality. They design the noising schedule using the scale factor $s(t)$ and the noise level $\sigma(t)$, defined as:
\begin{align}
             p(\rvx(t)|\rvx(0)) = \mathcal{N} \left(\rvx(t); s(t) \rvx(0), s^2(t) \sigma^2(t) \mI \right),
             \label{eq:PerturbationKernelEDM}
 \end{align}
 and adopt a non-uniform time grid discretization over $N$ steps, $t_i = \sigma^{-1}(\sigma_i)$, where 
 \begin{align}
     \sigma_{i < N} =
     \left( 
     \sigma_{\mathrm{max}}^{1/\rho}
     + \frac{i}{N-1} \left(\sigma_{\mathrm{min}}^{1/\rho} - \sigma_{\mathrm{max}}^{1/\rho}\right)
     \right)^{\rho} %\mbox{ and } \sigma_N=0,
     \label{eq:EDMTimeGrid}
 \end{align}
and $\sigma_N=0$ for $\rho = 7$, combined with the second-order solver Heun.
They argue that setting $s(t) = 1$ and $\sigma(t) = t$, which coincides with the DDIM sampler \cite{ddim}, 
leads to flat probability flow trajectories. Specifically, they argued that, when $\lambda = 0$ in Eq.~\eqref{eq:GenRevSDE}, this schedule minimizes the discretization error under some assumptions.

\begin{section}{Proposed Methods}

In this section, we first propose our novel unifying framework for noise scheduling, where the total variance (TV) and the signal-to-noise ratio (SNR) are controlled independently.
Then, we cast established schedules into our framework, and apply simple modifications.  We empirically show that, for  common existing schedules, where the TV explodes exponentially, their constant TV (i.e., VP) variants \emph{improve} the performance.  Finally, we propose an SNR scheduling strategy based on the exponential of the inverse sigmoid function, further enhancing the sampling efficiency and quality.


\subsection{TV/SNR Disentangled Scheduling}

We first define the perturbation kernel as:
  \begin{align}
  p(\rvx(t)|\rvx(0))
  &= 
             \mathcal{N}\left(\rvx(t);  \sqrt{\frac{\tau^2(t) \gamma^2(t)}{1 + \gamma^2(t)}) }\rvx(0) ,  \frac{\tau^2(t)}{1 + \gamma^2(t)} \mI \right),
             \label{eq:ProposedMarginal}
         \end{align}
         where $\tau(\cdot): [0, 1] \mapsto \mathbb{R}_{++}$ is a TV controlling function, and $\gamma(\cdot): [0, 1] \mapsto \mathbb{R}_{++}$ is an SNR controlling function.  $\tau(t)$ can be an arbitrary positive function, while $\gamma(t)$ is monotonically decreasing from $\gamma(0) = \gamma_{\mathrm{max}}< \infty$ to $\gamma(1) = \gamma_{\mathrm{min}}>0$.  From Eq.~\eqref{eq:ProposedMarginal} one can easily confirm that 
         \begin{align}
            \mathrm{TV} &= \sqrt{ a^2(t) + b^2(t)} =\tau(t),
%            \notag\\
\quad \quad
            \mathrm{SNR} = \frac{ a(t)}{ b(t)} =\gamma(t),
            \notag
         \end{align}
and therefore, $\tau(t)$ does not affect SNR, and $\gamma(t)$ does not affect TV, respectively---they can be independently controlled.
The forward and the reverse SDEs corresponding to the kernel in Eq.~\eqref{eq:ProposedMarginal} are given by Eqs.~\eqref{eq:GenForwSDE} and \eqref{eq:GenRevSDE}, respectively, with the following drift and diffusion coefficients:
 \begin{align}
     f(t) &= \frac{\dot{\tau}(t)}{\tau(t)} + \frac{\dot{\gamma}(t)}{\gamma(t) \left( 1 + \gamma^2(t) \right)},
     \quad \quad
     g(t) = \sqrt{\frac{-2 \tau^2(t) \dot{\gamma}(t)}{\gamma(t) \left( 1 + \gamma^2(t) \right)}}.
     \notag
 \end{align} 
 The full derivation can be found in \Cref{app:SNR-SDE}.

\subsection{VP variants of Established Non-VP Schedules}

For common established schedules, including the original linear VP schedule in DDPM \citep{DDPM_Ho,song2021score} and its cosine alternative \citep{imp_ddpm}, the original VE schedule in SMLD \citep{song2019score,song2021score}, EDM \citep{diff_edm}, and FM \citep{lipman2023flow}, \Cref{tab:ExistingTVSNR} 
 provides a summary of their corresponding TV and SNR schedules, while \Cref{fig:ExistingTVSNR} (left and middle) offers a visual representation. The derivations are given in \Cref{sec:A.SCLSNRExpressionExisting}.
Since EDM uses a non-uniform time grid, we also consider EDM with a uniform time grid (EDM-UT), where we incorporate the original non-uniform time grid into the TV/SNR schedules. Note that, although EDM and EDM-UT effectively use the same schedules and their respective ODEs are equivalent in the continuous time case, they perform differently when using numerical integration with discretization to solve the ODE/SDE.

In the table, the top three entries, SMLD, EDM, and EDM-UT use the VE schedule, where the TV increases exponentially to a large value $\sigma_{\mathrm{max}}$ when $t \to 1$.  
The TV schedule of the optimal transport flow matching (OTFM) is modulated---i.e., it is a variance modulated (VM) schedule--- although it does not grow exponentially, as depicted in \Cref{fig:ExistingTVSNR} (left).
In \Cref{sec:Experiments}, we examine whether exploding or modulated TV schedules are essential for achieving good performance. To this end, we introduce their VP counterparts, VP-SMLD, VP-EDM-UT, and VP-OTFM, as defined in the \cref{tab:ExistingTVSNR}.


     
    \begin{figure*}[t]
        \centering
        \includegraphics[width=.33\linewidth]{figures/stab_ve_vp_Euler_ODE_forward_model_poly.pdf}
         \includegraphics[width=.33\linewidth]{figures/stab_Euler_ODE_forward_model_poly.pdf}
         \includegraphics[width=.33\linewidth]{figures/stab_Euler_SDE_forward_model_poly.pdf}
        \caption{
        Stability rate ({\bf higher is better}) as a function of the number of function evaluations (NFE) for molecular structure generation on the QM9 dataset. Left: Comparison between commonly established non-VP schedules (i.e., $\tau(t)$ non-constant) and their VP counterparts in our framework. Middle: Comparison of our proposed VP-ISSNR schedule against various baselines, including the VP analogs from the left plot. Right: Same as the middle plot but using the reverse SDE, instead of the reverse ODE. The best-performing ODE schedule, VP-ISSNR, is highlighted in black for reference.
        }
        \label{fig:ComparisonMolecule}

      \end{figure*}
      
\subsection{Variance-preserving Inverse Sigmoid SNR (VP-ISSNR) Schedule}

         We propose to schedule TV and SNR with the following functions:
            \begin{align}
            \tau^2(t) &=1,
                    \label{eq:ConstantTVScheduling}\\
                 \gamma^2(t) 
                 % &= \exp \left(2\eta \log \left( \frac{1 - t}{t} \right) +2 \kappa \right), \quad t \in [ \varepsilon_1,  \varepsilon_2] 
                  &=  \exp \left(2 \eta \log \left( \frac{1}{t \, (t_{\mathrm{max}} - t_{\mathrm{min}}) + t_{\mathrm{min}}} -1 \right) + 2\kappa \right)
                 %\quad t \in [ 0,  1]
                 \notag\\
                 &=  
\left( \frac{1}{t \, (t_{\mathrm{max}} - t_{\mathrm{min}}) + t_{\mathrm{min}}} -1 \right)^{2 \eta}
\exp( 2 \kappa) .
%\quad t \in [\varepsilon_1, \varepsilon_2]
             \label{eq:SigmoidSNRScheduling}
            \end{align}
            Namely, we set TV to be constant, and schedule SNR with the exponential of the inverse sigmoid function.
            The parameters $\eta > 0$ and $\kappa \in \mathbb{R}$ control the steepness and the offset of the inverse sigmoid function, respectively, and the two constants,
            $0 < t_{\mathrm{min}} \approx 0$ and $ 1 > t_{\mathrm{max}} \approx 1$, adjust the starting $\gamma_{\mathrm{max}}$ and final $\gamma_{\mathrm{min}}$ SNR values as 
            \begin{align}
                \gamma^2(0) &=  \exp \left(2 \eta \log \left( \frac{1}{t_{\mathrm{min}}} -1 \right) + 2 \kappa \right) = \gamma^2_{\mathrm{max}}, \\
                \gamma^2(1) &= \exp \left(2 \eta \log \left( \frac{1}{t_{\mathrm{max}}} -1 \right) + 2 \kappa \right) = \gamma^2_{\mathrm{min}}.
            \end{align}
            The right plot in \Cref{fig:ExistingTVSNR} shows the inverse sigmoid function from Eq.~\eqref{eq:SigmoidSNRScheduling} with a few sets of parameters.
            With this SNR function, we can allocate more steps to specific SNR levels using $\kappa$, while $\eta$ controls the relative emphasis on the most critical SNR levels compared to other regions of the diffusion process.


\end{section}

\paragraph{Relation to OTFM}

Using optimal transport (OT) as conditional probability path in flow matching (FM) \citep{lipman2023flow} results in a linear interpolation between the prior and the target data distribution,
\begin{align}
    p_t(\rvx_t|\rvx_0) = \mathcal{N}\left(\rvx(t); (1-t) \rvx(0) ,  ((1 - t) \sigma_\mathrm{min} + t)^2 \mI \right),
    \label{eq:OTFMKernel}
\end{align}
%for $t \in [0,1]$.%
where $\sigma_\mathrm{min}$ is chosen to be sufficiently small, ensuring that the Gaussian distribution is concentrated around the target data point $x(0)$%
\footnote{Note that in \citet{lipman2023flow}, the time is reversed, with $\rvx(1)$ and $\rvx(0)$ corresponding to the samples in the target domain and latent domain, respectively. In this paper, we always define $t$ in the forward diffusion direction.}.
Consider a generalization of Eq.~\eqref{eq:OTFMKernel}:
\begin{align}
    p_t(\rvx_t|\rvx_0) =  \mathcal{N}\left(\rvx(t); (1-t)^\eta \rvx(0) ,  t^{2\eta} \exp(-2\kappa) \mI \right).
    \label{eq:OTFMKernelGeneral}
\end{align}
where $\sigma_{\mathrm{min}} = 0$ for simplicity. Then, the corresponding TV and SNR schedules are
\begin{align}
\tau^2(t) &= (1-t)^{2\eta} + t^{2\eta} \exp(-2\kappa),
\notag\\
\gamma^2(t) &=  
\left(\frac{1}{t} - 1\right)^{2 \eta}
\exp( 2 \kappa).
\notag
\end{align}
Comparing our proposed schedules, defined in Eqs.~\eqref{eq:ConstantTVScheduling} and \eqref{eq:SigmoidSNRScheduling} for $t_{\mathrm{min}} = 0$ and $t_{\mathrm{max}} = 1$,
we observe that the SNR schedule of our method is a generalization of the OTFM, as described in Eq.~\eqref{eq:OTFMKernelGeneral}, with a constant TV schedule, where setting $\eta=0.5$ and $\kappa=0$ recovers the SNR of OTFM. This generalization allows for more control over the generated probability flow.
     
     \begin{figure*}[t]
        \centering
        \includegraphics[width=.33\linewidth]{figures/CIFAR_10_fid_vs_nfe0.pdf}           \includegraphics[width=.33\linewidth]{figures/CIFAR_10_fid_vs_nfe1.pdf}
        \includegraphics[width=.33\linewidth]{figures/FFHQ_fid_vs_nfe1.pdf}
       \caption{
        FID score ({\bf lower is better}) as a function of the number of function evaluations (NFE) in image generation on CIFAR-10 and FFHQ.
        Left: comparison between existing non-VP and their VP variants on CIFAR-10. Middle: comparison between our proposed VP-ISSNR and baselines on CIFAR-10. Right: comparison between our proposed VP-ISSNR and baselines on FFHQ.
        }
        \label{fig:ComparisonImage}
     \end{figure*}

  \section{Experiments}
  \label{sec:Experiments}

  In this section, we empirically evaluate our proposed TV/SNR scheduling framework on molecular structure and image generation tasks. We also discuss the conditions for good schedules in terms of ODE trajectories and time evolution of the marginal density, through toy numerical investigation.
  
  \subsection{Molecular Structure Generation}
  \label{sec:ExperimentsMolecular}


  
  \paragraph{Problem setting: }

The goal is to predict an equilibrium state $R$ given a molecular composition $Z$, i.e. $\rvx(0) \sim p(R|Z)$. Note the difference from the general molecule generation task, where the composition $Z$ is also predicted.
Our experiments systematically evaluate different scheduling techniques used in state-of-the-art diffusion and flow matching models for molecular tasks~\citep{edm, geoldm, vignac2023midi, kahouli2024morered, song2023equivariant, eqgat-diff}. Common schedules include the DDPM-cos schedule with $\nu=1$ and $\nu=2.5$ and OTFM. We use the QM9 dataset~\citep{qm9}, a widely used benchmark comprising $\sim 130k$ equilibrium molecules with up to 9 heavy atoms (C, O, N, and F). Following \citet{kahouli2024morered}, we use a training/validation split of 55k/10k molecules and the remainder for testing.
For training, we adopt the noise model architecture used in ~\citet{kahouli2024morered}, 
and minimize
the DSM loss \eqref{eq:DSM} 
using the DDPM-cos schedule with $\nu=1.0$.
For sample generation,
we solve the reverse ODE, i.e., Eq.\eqref{eq:GenRevSDE} for $\lambda = 0$, using first-order Euler integration. For varying computational budgets, defined by the number of function evaluations (NFE), we report stability rates \citep{gebauer2022inverse} (higher is better) over 2.5k generated structures with compositions $Z$ sampled from the test split. 
More experimental details are given in \Cref{sec:A.AlgorithmDetails}.

\paragraph{VP-variants of Existing Schedules: }
% \label{sec:Exp.Molecule.VPImprovement}

\Cref{fig:ComparisonMolecule} (left) presents a comparison of the stability rates 
%based on the NFE of 
%various 
of
VE schedules, SMLD and EDM-UT, as well as the VM schedule OTFM, against their VP counterparts: VP-SMLD, VP-EDM-UT, and VP-OTFM.  Importantly, across all schedules, the VP variants outperform or match the performance of their original versions. Specifically, the VP versions lead to substantial enhancements for SMLD and EDM-UT, where the TV increases exponentially, whereas the OTFM with a smooth TV modulation, exhibits comparable performance to its VP analog. These findings suggest that exponentially increasing TV can be detrimental, thereby validating our choice to adopt a constant TV schedule.

\paragraph{VP-ISSNR Schedule: }
\Cref{fig:ComparisonMolecule} (middle) presents results for all methods listed in \Cref{tab:ExistingTVSNR}, excluding the original VE schedules, which showed worse performance compared to their VP analogs (shown in the left plot). The figure also includes the original EDM approach with its non-uniform time grid and our proposed VP-ISSNR schedule, using parameters $\eta = 1.0$, $\kappa = 2.0$, $t_\mathrm{min}=0.01$ and $t_\mathrm{max}=0.99$.
Strikingly, with our VP-ISSNR schedule, the diffusion model generates stable molecules in as few as 4 NFEs.
%with the Euler solver. 
The stability rate surpasses $74\%$ with only 8 steps and reaches nearly $87\%$ with 128 steps, outperforming all other schedules.
%employed for molecular generation. 
Additionally, 
\Cref{fig:ComparisonMolecule} (right) shows that,
when solving the reverse SDE ($\lambda=1$ in Eq.\eqref{eq:GenRevSDE}) using our VP-ISSNR schedule,
%with the Euler-Maruyama integration method, 
the stability rate increases significantly to $93.16\%$ with 32 NFEs and $95.82\%$ with 64 NFEs. Considering the number of NFEs, our approach achieves state-of-the-art results to the best of our knowledge. 
While the ODE achieves a higher stability rate than the SDE at very low NFEs (e.g., $74\%$ vs. $70\%$ at 8 NFEs), the SDE achieves superior stability as NFEs increase, suggesting that stochasticity introduces a corrective effect that enhances sample quality in molecules, albeit with a slight increase in sampling time. We also investigate the quality of the generated samples by running DFT relaxations to identify the nearest reference structure and report the results in \cref{fig:AllComparisonMolecule_Heun_RMSD}
in \Cref{sec:A.AdditionalExperimentalResults}, where we see trends similar to those observed in the stability rate results.
In \Cref{sec:A.AdditionalExperimentalResults}, we also present additional experimental results using the second-order Heun method and diffusion models trained with different schedules.
% yielded only marginal improvements at higher NFEs \memo{(Appendix x)}.

  \subsection{Image Generation}
  \label{sec:ExperimentsImage}
  
\paragraph{Problem setting: }     

We evaluate the performance of different schedules for unconditional image generation. Following the setup in \citet{diff_edm}, we use their pre-trained diffusion models trained on CIFAR-10 \citep{cifar10} and FFHQ \citep{Karras2018ASG}. Samples are generated by solving the reverse ODE using the second-order Heun method. We assess sample quality using the average FID score \citep{fid} computed over 50K generated images as a function of the NFE.

\paragraph{VP-variants of Existing Schedules: }
Similar to the experiment with molecules in \Cref{fig:ComparisonMolecule} (left),  
we first compare the original non-VP schedules to their VP analogs. 
\Cref{fig:ComparisonImage} (left) summarizes the performance on CIFAR (results on FFHQ, reported in \Cref{sec:A.AdditionalExperimentalResults}, show similar trends).
Consistent with our findings in molecular structure generation, we observe that both SMLD and EDM-UT, which feature exploding TV schedules, benefit significantly from adopting a constant TV schedule. In contrast, 
% the difference between OTFM and its VP counterpart is less pronounced. While the original OTFM performs better at lower NFEs, VP-OTFM eventually catches up.
the VP-OTFM does not perform as well as the original OTFM with modulated TV, which is different from what we observe for molecular structure generation.  This  implies the possibility of further improving the fast sampling by optimizing the TV control, which we leave as future work.

\paragraph{VP-ISSNR Schedule: }

\Cref{fig:ComparisonImage} (middle) and \Cref{fig:ComparisonImage} (right) compare different schedules, including the original EDM
%with a non-uniform time grid 
and our proposed VP-ISSNR schedule with $\eta=1.5$, $\kappa=1.0$, $t_\mathrm{\mathrm{min}}=0.03$ and $t_{\mathrm{max}}=0.973$.
%with a uniform grid.
Unlike in molecular structure generation, the original EDM outperforms other methods in image generation. This is expected, as EDM is highly optimized for these tasks and relies on a carefully tuned non-uniform time grid, which is crucial for its strong performanceâ€”evidenced by the poor results for its uniform-time variant, EDM-UT, in \Cref{fig:ComparisonImage} (left). Nonetheless, our VP-ISSNR schedule, which adopts a simpler strategy with a 
%simple 
uniform time grid, a constant TV, and an inverse sigmoid SNR,
%that generalizes OTFM, 
achieves comparable performance to EDM. 
Importantly, our VP-ISSNR reduces the number of tunable hyperparameters, simplifying the optimization process dependent on the dataset used. Given its superior performance in molecular structure generation, our framework appears robust across different domains, highlighting its effectiveness.

\begin{figure*}[t]
    \centering
\includegraphics[width=\linewidth]{figures/toy_example1_3_deltas.pdf}          
\includegraphics[width=\linewidth]{figures/toy_example0_3_deltas.pdf}        
   \caption{
    ODE trajectories (black solid curves) and the marginal density path (red shadows) when the data distribution is a mixture of 3 uniformly spaced delta peaks. The inline plots focus on the neighborhood of one of the peaks at $x = \frac{3}{2}$ and $ t \ll 1$.
    }
   \label{fig:toy_example_3deltas_ours}
  \end{figure*}


\subsection{Discussion: Curvature of ODE Trajectories and the Support of Marginal Density}
\label{sec:Discussion}

  \citet{diff_edm} argue that, if the ODE trajectories are straight, crude time discretization does not produce substantial errors, allowing fast sample generation.
  They further argue from a theoretical point of view that 
  the ODE trajectories of EDM are straight, by using 
  Tweedie's formula \citep{Efron22011}, 
$      \nabla_{\rvx} \log p_t(\rvx(t)) = \frac{\textcolor{black}{a(t)} \E[\rvx(0)|\rvx(t)] - \rvx(t)}{\textcolor{black}{b^2(t)}}
$.
However, their argument relies on the assumption that 
the conditional expectation $\E[\rvx(0)|\rvx(t)]$ in the formula approximates the data point $\rvx(0)$ that $\rvx(t)$ reaches at time $t=0$ in the reverse ODE process,
which does not necessarily hold due to the \emph{interaction between trajectories}---an ODE trajectory generating a particular data point never crosses the trajectory of another data point.
To numerically observe such trajectory interactions, we depict ODE trajectories in \Cref{fig:toy_example_3deltas_ours}, assuming that the data distribution $p_\mathrm{data}(\rvx(0))$ is a mixture of three uniformly spaced delta peaks (at $x = -\sqrt{\frac{3}{2}}, 0, \sqrt{\frac{3}{2}}$ such that the mean and variance are standardized) with uniform weights.
  The figure
    shows ODE trajectories of different schedules, which were computed by solving the reverse ODE with an analytically computed score function. 
  At first glance, the trajectories of EDM (top-left) seem straight.  However, when we focus on the neighborhood of a single delta peak ($x = \sqrt{\frac{3}{2}}$) for $ t \ll 1$, i.e., close to the data space, its trajectories are highly curved, as shown in the inline plot.

We hypothesize that discretization errors at time $ t \ll 1$ are more severe than errors around $ t \approx 1$, especially when the goal is to generate high-quality samples.  More specifically, the error at $t$ when the marginal $p_t(\rvx)$ has a large support (due to the Gaussian diffusion) should not seriously degrade the generated sample quality at $t = 0$.  
This is because errors around $t \approx 1$ do not push the latent sample $\rvx(t)$ into the out-of-distribution region of $p_t(\rvx)$.  Instead, such errors steer the latent sample onto an incorrect trajectory, potentially violating bijectiveness. However, if the remaining reverse process is solved accurately, the sample following this incorrect trajectory can still reach a high-quality point at $t=0$.
Thus, we hypothesize that a good schedule should i) have straight trajectories close to the data space ($ t \ll 1$), and ii) the marginal density $p_t(\rvx)$ should have a large support for small $t$.

Revisiting \Cref{fig:toy_example_3deltas_ours}, we observe that for schedules with exploding TV $\tau(t)$, i.e., SMLD and EDM-UT, the support of the marginal $p_t(\rvx)$ (relative to the latent space variance at $t=1$) is small until $t$ becomes large. In contrast, schedules with non-exploding $\tau(t)$, like OTFM and the VP schedules, exhibit broader support close to the data space. This explains why our VP variants of existing VE schedules improve the sample quality.
Furthermore,
by carefully observing the inline plots, we find that OTFM, our VP-OTFM, and our VP-ISSNR have straight trajectories for $ t \ll 1$, which is consistent with our experimental results on molecule and image generation.
Note that the original EDM performs best in image generation even though its trajectories have high curvatures for $ t \ll 1$, because it adopts a non-uniform time grid that assigns many integration steps exactly for $ t \ll 1$ (see \Cref{fig:EDMTimeGrid} in \Cref{sec:A.SCLSNRExpressionExisting}).
Although our hypothesis about the requirements for good schedules needs to be supported mathematically, it explains our main observations in the molecular and image generation experiments.



\begin{section}{Conclusion}
 \label{sec:conclusion}
Diffusion models have learned to master the intricate interplay between noising and denoising. Long sampling times can  be alleviated by reducing the number of diffusion time steps. A successful reduction, however, crucially depends on the chosen noise schedule, which balances introducing noise on one side and signal reduction on the other. So far, schedules have been improved only implicitly as, e.g., variance-preserving and variance-exploding, controlling variance without exerting direct explicit control over the balance between noise introduction and signal reduction. 

In this work, we contributed the following novel direct control strategy: a total-variance/signal-to-noise-ratio disentangled  (TV/SNR) framework, where TV and SNR can be controlled {\em independently}. Note that SNR is a well-known tool in signal processing. 

We can see theoretically and empirically that existing schedules with the TV exploding exponentially can be \emph{improved} by setting the TV schedule to be constant, while keeping the SNR schedule unchanged. While our novel framework with a uniform diffusion time grid is on par with the highly tailored  EDM sampler for image generation, surprisingly clear progress in performance is observed when generating molecules. 
Specifically, we find that our SNR schedule, as a generalization of optimal transport flow matching, drastically improves the performance in molecular structure generation by up to {\bf 30-fold}. Specifically, stable molecules are generated after only {\bf 4 steps} (much less than SOTA). 
An interesting side observation is that our proposed procedure allows improving even on the EDM sampler which was previously considered `optimal'. To explain this unexpected empirical finding, we provided some theoretical insight and an intuitive conceptual illustration of possible mechanisms.

In conclusion, by unifying diffusion processes through our TV/SNR framework and improving reverse diffusion speed, we take a meaningful step toward advancing applications of diffusion models in quantum chemistry and beyond. 

\end{section}

\section*{Impact Statement}

This paper aims to make progress in the fields of machine learning and computational chemistry. Although our work may have numerous societal implications, we believe none of these require explicit mention at this time.

\section*{Acknowledgments}
This work was partly funded by the German Ministry for Education and Research (BMBF) as BIFOLD â€“ Berlin Institute for the Foundations of Learning and Data - under Grants 01IS14013A-E, 01GQ1115, 01GQ0850, 01IS18025A, 031L0207D, and 01IS18037A.
%(under refs 01IS14013A-E, 01GQ1115, 01GQ0850, 01IS18056A). 
Furthermore, K.R.M. was partly supported by the Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grants funded by the Korean Government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program, Korea University and No. 2022-0-00984, Development of
Artificial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of
Explanation).
S.G. was supported by the Postdoc.Mobility fellowship by the Swiss National Science Foundation (project no. 225476).

\bibliographystyle{unsrtnat}
\bibliography{tv_snr}

\newpage

\appendix
% reset figure counter and use format A1, A2, etc.
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}




\section{Derivation of TV/SNR Exressions of Existing Schedules}
\label{sec:A.SCLSNRExpressionExisting}

In this section, we show the derivations of the TV, $\tau(t)$, and SNR, $\gamma(t)$, for existing diffusion model schedules.

\subsection{Variance-Exploding (VE) Schedules}

Given is the VE perturbation kernel, originally introduced by \citet{song2019score} as Denoising Score Matching with Langevin Dynamics (SMLD) and then reframed in the SDE framework by \citet{song2021score} as VE-SDE,
\begin{align}
\boxed{
    p(\rvx(t)|\rvx(0)) = \mathcal{N}\left(\rvx(t); \rvx(0), b^2(t) \mathbf{I}\right)}
\end{align}
with
\begin{align}
    a(t) &= 1, \label{eq:VE_a} \\
    b^2(t) &= \sigma_{\mathrm{min}}^2 \left( \frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}} \right)^{2t}. 
\end{align}
Here, $\sigma_{\mathrm{min}}$ and $\sigma_{\mathrm{max}}$ are the minimum and maximum noise scales, respectively.

\paragraph{SNR:}

\begin{align}
    \gamma^2(t) 
    = \frac{a^2(t)}{b^2(t)} 
    = \frac{1}{\sigma_{\mathrm{min}}^2 \left( \frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}} \right)^{2t}} 
    = \sigma_{\mathrm{min}}^{-2} \left( \frac{\sigma_{\mathrm{min}}}{\sigma_{\mathrm{max}}} \right)^{2t}.
\end{align}

\paragraph{TV:}

\begin{align}
    \tau^2(t) &= a^2(t) + b^2(t) \notag\\
    &= 1 + \left[ \sigma_{\mathrm{min}} \left( \frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}} \right)^{t} \right]^2  \notag\\
    &=1 + \sigma_{\mathrm{min}}^2 \left( \frac{\sigma_{\mathrm{max}}}{\sigma_{\mathrm{min}}} \right)^{2t}.
\end{align}

\subsection{Elucidating Design Space of Diffusion Models (EDM)}

EDM \citep{diff_edm} introduces a noise schedule with a scaling factor $s(t)$ and noise level $\sigma(t)$. 
The perturbation kernel is
\begin{align}
\boxed{
    p(\rvx(t)|\rvx(0)) = \mathcal{N}\left(\rvx(t); s(t) \rvx(0), [s(t) \sigma(t)]^2 \mathbf{I}\right)
    }
\end{align}
where they use,
\begin{align}
    a(t) &= s(t) = 1, \\
    b^2(t) &= \sigma^2(t).
\end{align}

The time discretization \eqref{eq:EDMTimeGrid} of the original EDM is illustrated in \Cref{fig:EDMTimeGrid}.


    \begin{figure}[t]
        \centering
        \includegraphics[width=.49\linewidth]{figures/kve_schedule.pdf}
        \caption{
        The time discretization \eqref{eq:EDMTimeGrid} of the original EDM sampler \citep{diff_edm} for various parameters $\rho$. At $\rho = 1$, linear time discretization is recovered, i.e. $t(i) = i$. For larger $\rho$, time steps get considerably shorter close to the data distribution. EDM uses $\rho = 7$.}
        \label{fig:EDMTimeGrid}
     \end{figure}

     

\paragraph{SNR:}

\begin{align}
    \gamma^2(t) = \frac{a^2(t)}{b^2(t)} = \frac{1}{\sigma^2(t)}.
\end{align}

\paragraph{TV:}

\begin{align}
    \tau^2(t) &= a^2(t) + b^2(t) \notag\\
    &= 1 + \sigma^2(t).
\end{align}

\subsection{Optimal Transport Flow Matching (FM)}
The perturbation kernel of OTFM \citep{liu2023flow, lipman2023flow} is given by:

\begin{align}
\boxed{
        p_t(\rvx_t|\rvx_0) =  \mathcal{N}\left(\rvx(t); (1-t) \rvx(0) ,  ((1 - t) \sigma_\mathrm{min} + t)^2 \mI \right)
        }
\end{align}
where $\sigma_\mathrm{min}$ is sufficiently small, resulting in a Gaussian distribution concentrated around $x(0)$. Assuming $\sigma_\mathrm{min} = 0$ and incorporating the boundary constraint on $t \in [\varepsilon_{\mathrm{min}},1]$, we can define:

\paragraph{SNR:}
\begin{align}
    \gamma^2(t) = \frac{(1-t)^2}{t^2} = \left(\frac{1}{t} -1\right)^2 .
\end{align}

\paragraph{TV:}
\begin{align}
    \tau^2(t) = (1-t)^2 + t^2 = 1-2t(1-t).
\end{align}

\subsection{Denoising Diffusion Probabilistic models (DDPM)}
The original DDPM model \citep{DDPM_Ho} used the perturbation kernel:
\begin{align}
\boxed{
    p(\rvx(t)|\rvx(0)) = \mathcal{N}\left(\rvx(t); \sqrt{\bar{\alpha}(t)} \rvx(0), (1-\bar{\alpha}(t)) \mathbf{I}\right)
    }
\end{align}
\paragraph{SNR}

\begin{align}
    \gamma^2(t) = \frac{a^2(t)}{b^2(t)} = \frac{\bar{\alpha}(t)}{1-\bar{\alpha}(t)}.
\end{align}

\paragraph{TV}

\begin{align}
    \tau^2(t) &= a^2(t) + b^2(t) \notag\\
    &= 1.
\end{align}

While the TV is always constant in the VP case, different schedules were adopted for $\bar{\alpha}(t)$. The most common are:
\begin{itemize}
    \item {\bf DDPM-linear} the original linear schedule introduced by \citet{DDPM_Ho} and adopted by \citet{song2021score} as VP-SDE, where:\\
    \begin{align*}
        \bar{\alpha}(t) =  e^{ -\frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) - t \beta_\mathrm{min}},
    \end{align*}
    and therefore 
    \begin{align}
        \gamma^2(t) &= \frac{ e^{ -\frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) - t \beta_\mathrm{min}}}{1 -  e^{ -\frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) - t \beta_\mathrm{min}}} \notag \\
        &= \left( \frac{1 - e^{ -\frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) - t \beta_\mathrm{min}}}{e^{ -\frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) - t \beta_\mathrm{min}}} \right)^{-1} \notag \\  
        &= \left( e^{ \frac{1}{2} t^2 (\beta_\mathrm{max} - \beta_\mathrm{min}) + t \beta_\mathrm{min}} -1 \right)^{-1}.
    \end{align}
    \item {\bf DDPM-cos} First introduced by \citet{imp_ddpm} as a better alternative to the linear schedule:
    %, where
    \begin{align*}
        \bar{\alpha}(t) = \frac{u(t)}{u(0)}, \quad \text{where} \quad u(t)=\cos\left(\frac{t^\nu + s}{1 + s} \frac{\pi}{2}\right)^2.
    \end{align*}
    Note that the parameter $\nu$ does not exist in the original formulation \citep{imp_ddpm} but we adopt it from \citet{vignac2023midi}. The SNR is therefore:
    \begin{align}
        \gamma^2(t) &= \frac{u(t)}{u(0) \left( 1 - \frac{u(t)}{u(0)} \right)} \notag \\
        &= \frac{u(t)}{u(0)-u(t)} \notag \\
        &= (\frac{u(t)}{u(0)} -1)^{-1} \notag \\
        &= \left( \left( \frac{\cos\left(\frac{s}{1 + s} \frac{\pi}{2}\right)}{\cos\left(\frac{t^\nu + s}{1 + s} \frac{\pi}{2}\right)} \right)^2 -1 \right)^{-1}.
    \end{align}
    
\end{itemize}
    

\section{Derivation of the SDE}

     \begin{subsection}{Derivation of the perturbation kernel for a given affine SDE}
         \label{app:SDEtoKernel}
         As shown by \citet{song2021score}, a diffusion process can be described by a continuous stochastic differential equation (SDE) describing an ItÃ´ process:
         \begin{equation}
             \dv \rvx =  \mathbf{f}(\rvx, t) \, \dv t + \mathbf{g}(\rvx, t) \, \dv \rvw,
             \label{eq:A.GenForwSDE}
         \end{equation}
         where $\rvx$ is the state variable, $\rvw$ is the standard Wiener process, and $\mathbf{f}(\rvx, t)$ and $\mathbf{g}(\rvx, t)$ are predefined functions describing the drift and diffusion coefficients, respectively.
    
         While \citet{song2021score} derived the perturbation kernel using the differential equations for the mean and covariance of an SDE, we take a different approach. By exploiting the affine nature of the SDE in our case, we first solve the SDE and then derive the perturbation kernel parameters, arriving at the same solution. Specifically, for the affine case where
         \begin{equation}
             \dv \rvx =  \textcolor{black}{f(t)} \rvx \, \dv t + \textcolor{black}{g(t)} \, \dv \rvw,
             \label{eq:AffineForwSDE_si}
         \end{equation}
         the integral of the ItÃ´ process is described by Eq. (4.28) in \citet{sarkka_applied} as
         \begin{equation}
             \rvx(t) = \phi(t, 0) \rvx(0) + \int_0^t \phi(t, s) g(s) \, \dv \rvw(s)
             \label{eq:solutionForwSDE}
         \end{equation}
         with an integrating factor $\phi(t, s)$ and initial condition $\rvx(0)$. In the following we use the integrating factor $\phi(t, s) = \exp\left( \int_s^t f(u) \, \dv u \right)$.
    
         The solution $\rvx(t)$ involves a deterministic part dependent on $\rvx(0)$ and a stochastic part independent of $\rvx(0)$ and involving a standard Wiener process. Thus, we can derive a Gaussian perturbation kernel for this process following the general form defined in Eq.~\eqref{eq:PerturbationKernel}:
    
         \begin{align*}
             a(t) \rvx(0) & = \mathbb{E}[\rvx(t)|\rvx(0)]                                                                                                                                                   \\
                          & =  \mathbb{E}[\phi(t, 0) \rvx(0) |\rvx(0)] + \underbrace{\mathbb{E}\left[\int_0^t \phi(t, s) g(s) \, \dv \rvw(s) |\rvx(0)\right]}_{=0, \text{ since } \mathbb{E}[\dv \rvw] = 0} \\
                          & = \phi(t,0) \rvx(0),
         \end{align*}
         and
         \begin{align*}
             b^2(t) & = \text{Var} \left( \rvx(t) |  \rvx(0)\right)                                                                                                                                                                                           \\
                    & = \underbrace{\text{Var} \left( \phi(t, 0) \rvx(0) |\rvx(0) \right) }_{=0} + \text{Var} \left( \int_0^t \phi(t, s) g(s) \, \dv \rvw(s) |\rvx(0)\right)                                                                                  \\
                    & = \E \left[ \left( \int_0^t \phi(t, s) g(s) \, \dv \rvw(s) \right)^2 | \rvx(0)\right] +  \underbrace{\left( \E \left[ \int_0^t \phi(t, s) g(s) \, \dv \rvw(s) | \rvx(0)\right] \right)^2}_{=0, \text{ since } \mathbb{E}[\dv \rvw] = 0} \\
                    & \overset{\text{(ItÃ´ isometry)}}{=} \E \left[ \int_0^t \phi(t, s)^2 g(s)^2 \, \dv s | \rvx(0) \right]                                                                                                                                    \\
                    & = \int_0^t \phi(t, s)^2 g(s)^2 \, \dv s.
         \end{align*}
    
         Summarizing, the perturbation kernel parameters are given by:
         \begin{tcolorbox}[colframe=black, colback=white, boxrule=1pt, rounded corners, valign=center]
             \begin{align}
                 \textcolor{black}{a(t)} & = \phi(t,0) = \exp\left( \int_0^t f(u) \, \dv u \right),
                 \label{eq:MeankernelSDE}\\
                 \textcolor{black}{b^2(t)} & = \int_0^t \phi(t, s)^2 g(s)^2 \, \dv s = \int_0^t \exp\left( 2 \int_s^t f(u) \, \dv u \right) g(s)^2 \, \dv s.
                 \label{eq:VarkernelSDE}
             \end{align}
         \end{tcolorbox}
         
     \end{subsection}


 \begin{subsection}{Derivation of the SDE for a given perturbation kernel}
     \label{app:KerneltoSDE}
     In the previous section (Appendix~\ref{app:SDEtoKernel}), we derived a perturbation kernel with the general form specified in Eq.~\eqref{eq:PerturbationKernel} from a given SDE. In this section, we do the opposite and derive the SDE that results in a given perturbation kernel, where we use the results from the previous section. Starting from the definition of $a(t)$ in Eq.~\eqref{eq:MeankernelSDE} and $b^2(t)$ in Eq.~\eqref{eq:VarkernelSDE}, we can first derive the drift $f(t)$ of the affine SDE (Eq.~\eqref{eq:AffineForwSDE_si}):
     \begin{align*}
         \exp\left( \int_0^t f(u) \, \dv u \right) & =    a(t)                                                 \\
\therefore \quad         f(t)                                      & = \frac{\dv[\log a(t)]}{\dv t}  = \frac{\dot{a}(t)}{a(t)}.
     \end{align*}
     where $\dot{a}(t) = \frac{\dv [a(t)]}{\dv t}$ denotes the derivative of $a(t)$ with respect to time. Next, we derive the diffusion coefficient $g(t)$:
     \begin{align*}
         b^2(t)                           & = \int_0^t \exp\left( 2 \int_s^t f(u) \, \dv u \right) g(s)^2 \, \dv s                          \\
                                          & = \int_0^t \exp\left( 2 \int_s^t \frac{\dv[\log{a(u)}]}{\dv u} \, \dv u \right) g(s)^2 \, \dv s \\
                                          & = \int_0^t \exp \left( 2 \left(\log a(t) -  \log a(s) \right) \right) g(s)^2 \, \dv s           \\
                                          & = \int_0^t \frac{a(t)^2}{a(s)^2} \, g(s)^2 \, \dv s                                             \\
                                          & = a^2(t) \int_0^t \frac{g(s)^2}{a(s)^2} \, \dv s                                                \\
         \therefore \quad \left(\frac{b(t)}{a(t)}\right)^2 & = \int_0^t \frac{g(s)^2}{a(s)^2} \, \dv s.
     \end{align*}
     Deriving both sides with respect to $t$ and solving for $g(t)$, we get
     \begin{align*}
         \frac{g(t)^2}{a(t)^2} & = 2 \, \frac{b(t)}{a(t)} \, \frac{\dv}{\dv t} \left(\frac{b(t)}{a(t)}\right)   \\
\therefore \quad         g(t)                  & = \sqrt{2 \, a(t) \, b(t) \, \frac{\dv}{\dv t} \left(\frac{b(t)}{a(t)}\right)}.
     \end{align*}
     Thus, the SDE parameters are given by
     \begin{tcolorbox}[colframe=black, colback=white, boxrule=1pt, rounded corners, valign=center]
         \begin{align}
             \textcolor{black}{f(t)} &= \frac{\dv[\log a(t)]}{\dv t} = \frac{\dot{a}(t)}{a(t)},
             \label{eq:DriftSDE} \\
             \textcolor{black}{g(t)} &= \sqrt{2 \, a(t) \, b(t) \, \frac{\dv}{\dv t} \left(\frac{b(t)}{a(t)}\right)}.
             \label{eq:DiffusionSDE}
         \end{align}
     \end{tcolorbox}
     When defining the perturbation kernel to explicitly include a scaling factor, i.e., when  $b^2(t) = a^2(t) \, c(t)^2 $, $ p(\rvx(t)|\rvx(0)) = \mathcal{N}(\rvx(t);  a(t) \rvx_0, a^2(t) \, c(t)^2 \mI )$ and therefore $\tilde{\rvx}(t) = \left(\rvx(t) / a(t)\right) \sim \mathcal{N}(\rvx_0, c(t)^2 \mI )$, we get the special case of Eq.~\eqref{eq:DiffusionSDE}:
     \begin{align}
         g(t) = \sqrt{2 \, a(t) \, a(t) \, c(t) \, \frac{\dv}{\dv t} \left(\frac{a(t) \, c(t)}{a(t)}\right) } = a(t) \sqrt{2 \, c(t) \, \dot{c}(t)}
         \label{eq:ScaledDiffusionSDE}
     \end{align}

 \end{subsection}
 
 
 \begin{subsection}{Derivation of our TV/SNR SDE}
         \label{app:SNR-SDE}

Using the results from sections \ref{app:SDEtoKernel} and \ref{app:KerneltoSDE}, we derive our TV/SNR SDE. To this end, we first define the perturbation kernel as
  \begin{align}
  p(\rvx(t)|\rvx(0))
  &=
             \mathcal{N}\left(\rvx(t);  \sqrt{\frac{\tau^2(t) \gamma^2(t)}{1 + \gamma^2(t)}) }\rvx(0) ,  \frac{\tau^2(t)}{1 + \gamma^2(t)} \mI \right).
             \label{eq:A.ProposedMarginal}
         \end{align}
         
         Given the TV/SNR perturbation kernel in Eq.~\eqref{eq:A.ProposedMarginal} and the results from Appdix~\ref{app:KerneltoSDE}, we can derive the SDE that results in this perturbation kernel. First, using Eq.~\eqref{eq:DriftSDE} we derive $f(t)$, where
         \begin{equation*}
             a^2(t) = \frac{\tau^2(t) \gamma^2(t)}{1 + \gamma^2(t)}.
         \end{equation*}


%    For simplicity, we use $\tau$ = $\tau(t)$ and $\gamma = \gamma(t)$:
Abbreviating as $\tau$ = $\tau(t)$ and $\gamma = \gamma(t)$ to avoid clutter, we have
    \begin{align*}
        \frac{\dv}{\dv t}a(t)^2 = 2 \dot{a}(t) a(t)
    \end{align*}
    and
    \begin{align*}
    \frac{\dv}{\dv t}a(t)^2 = \frac{\dv}{\dv t}  \left( \frac{\tau^2 \gamma^2}{1 + \gamma^2} \right) &= 
    \frac{(1 + \gamma^2)(2 \tau \dot{\tau} \gamma^2 + 2 \gamma \dot{\gamma} \tau^2) - \tau^2 \gamma^2 (2 \gamma \dot{\gamma})}{(1 + \gamma^2)^2} \\
    &= \frac{2 \tau \dot{\tau} \gamma^2 (1 + \gamma^2) + 2 \gamma \dot{\gamma} \tau^2}{(1 + \gamma^2)^2}.
    \end{align*}
    Therefore
    \begin{align*}
         \dot{a(t)} &= \frac{\frac{\dv}{\dv t} a^2(t)}{2 a(t)} \\
     &= \frac{\tau \dot{\tau} \gamma^2 (1 + \gamma^2) + \gamma \dot{\gamma} \tau^2}{a(t) (1 + \gamma^2)^2}
    \end{align*}
    and    
    \begin{align*}
    f(t) &= \frac{\dot{a}(t)}{a(t)} \\
    &= \frac{\tau \dot{\tau} \gamma^2 (1 + \gamma^2) + \gamma \dot{\gamma} \tau^2}{a^2(t) (1 + \gamma^2)^2} \\
    &= \frac{\tau \dot{\tau} \gamma^2 (1 + \gamma^2) + \gamma \dot{\gamma} \tau^2}{\tau^2 \gamma^2 (1 + \gamma^2)}.
    \end{align*}
     Consequently we have
     \begin{align}
              f(t) &= \frac{\dot{\tau}(t)}{\tau(t)} + \frac{\dot{\gamma}(t)}{\gamma(t) \left( 1 + \gamma^2(t) \right)}.
     \end{align}
         
         Now, we can derive the diffusion coefficient $g(t)$, where we can use the special case of Eq.~\eqref{eq:DiffusionSDE}, when the variance is explicitly scaled by the mean factor $a(t)$:
         \begin{align*}
             b^2(t) = a^2(t) \, c(t)^2 & = \frac{\tau^2(t) \gamma^2(t)}{1 + \gamma^2(t)} \, \frac{1}{\gamma^2(t)} \\
             \Rightarrow          c(t) & = \gamma(t)^{-1} .
         \end{align*}
         Thus, we use Eq.~\eqref{eq:ScaledDiffusionSDE} to solve for $g(t)$, where
         \begin{align*}
             \dot{c}(t) = \frac{\mathrm{d}}{\mathrm{d}t} \left( \gamma(t)^{-1} \right) = - \frac{\dot{\gamma}(t)}{\gamma^2(t)}
         \end{align*}
         and hence
         \begin{align}
             g(t) &= a(t) \sqrt{2 \, c(t) \, \dot{c}(t)} \notag \\
             &= \sqrt{ \frac{\tau^2(t) \gamma^2(t)}{1 + \gamma^2(t)}} \sqrt{-2 \frac{1}{\gamma(t)}\frac{\dot{\gamma}(t)}{\gamma^2(t)}} \notag \\
             &= \sqrt{\frac{-2 \tau^2(t) \dot{\gamma}(t)}{\gamma(t) \left( 1 + \gamma^2(t) \right)}}.
         \end{align}
        Note that $\gamma(t)$ needs to be differentiable, monotonically decreasing and positive for all $t \in [0, 1]$ to ensure that the SDE is well-defined, i.e., the square root in the diffusion coefficient $g(t)$ is well-defined and the dominant term in the drift $f(t)$ is non-zero.
    
     \end{subsection}

 \label{app:SDE}

\section{Experimental Details}
\label{sec:A.AlgorithmDetails}


For training a diffusion model, we use the loss in Eq.~\eqref{eq:DSM} to keep a unit variance of the model output for all $t$, and adopt the same noise model architecture $\rvepsilon_{\theta}$ from ~\citet{kahouli2024morered}, but use 9 interaction blocks, train on continuous time and condition the model on a scaled SNR instead of the time $t$, i.e., $\rvepsilon_{\theta}(\hat{\rvx}(t), c_\mathrm{snr}(\gamma^2(t)))$, where $\hat{\rvx}(t)$ is a scaled version of $\rvx(t)$ to unit variance. This is achieved by first scaling the training data by $\sigma_\mathrm{data}$, which is approximately $\sqrt{2}$ for the QM9 dataset, and always setting $\tau(t)=1$ during training, independent of the training SNR schedule. This has the benefit of making the model compatible with various TV and SNR schedules during sampling without retraining, and avoiding model stability issues due to large cutoff distances in the Graph Neural Network when using non-constant $\tau(t)$. We define $c_\mathrm{snr}(\gamma^2(t)) = \omega \log(\gamma^2(t)) + \xi$ to linearize the SNR input, keeping it in a stable, normalized range, with $\omega=0.35$ and $\xi=-0.125$ providing good performance. 
During sampling with a TV schedule $\tau(t) \neq 1$, we scale the model input to $\hat{\rvx}(t) = \tau(t)^{-1} \rvx(t)$ to maintain unit variance for all $t$. Note that the reverse trajectory itself will not become constant. The generated samples $\rvx(0)$ are then scaled back to the target data variance by multiplying by $\sigma_\mathrm{data}$.

We tuned $t_\mathrm{max}$ such that $\gamma(t)^{-1}$ approximates the dataset's maximum pairwise Euclidean distance. For molecules with different number of atoms we choose the average. This ensures that all the modes of the distribution are mixed at $t_\mathrm{max}$. We tune $t_\mathrm{min}$ to the largest value producing almost noiseless samples, avoiding extra reverse steps near the data manifold. 

 We trained two models using different schedules: (i) DDPM-cos with $\nu=1.0$ and (ii) the EDM SNR schedule with $\tau(t)=1$ for the reasons discussed before. We then sampled from each model using all schedules and found that the model trained with DDPM-cos consistently outperformed the EDM-trained model, even when using the EDM schedule for sampling, as depicted in \cref{fig:AllComparisonMolecule_EulerODE_AnotherTraining}. Therefore, we report only the results using the model trained on the DDPM-cos in the main text, while results for the model trained on the EDM schedule are included in \Cref{sec:A.AdditionalExperimentalResults}.

\section{Additional Experimental Results}
\label{sec:A.AdditionalExperimentalResults}


\subsection{Molecular structure generation}


    \begin{figure}
        \centering
        \includegraphics[width=.49\linewidth]{figures/stab_Heun_ODE_forward_model_poly.pdf}
        \caption{Stability rate ({\bf higher is better}) as a function of the number of function evaluations (NFE) for molecular structure generation on the QM9 dataset, using the Heun sampler.}
        \label{fig:AllComparisonMolecule_Heun}
     \end{figure}

     

     \begin{figure}
        \centering
        \includegraphics[width=.49\linewidth]{figures/rmsd_vs_nfe.pdf}
         \caption{Root mean square deviation (RMSD, {\bf lower is better}) between the generated structures and reference structures obtained from geometry relaxations using DFT calculations at the B3LYP/6-31G(2df,p) level of theory, the same method used for generating the structures in QM9 \cite{qm9}, which the model was trained on. We see a similar trend to the stability rate results in \cref{fig:ComparisonMolecule}, where our VP-ISSNR consistently outperforms other approaches. This reveals that our method can generate physically plausible molecules that are structurally similar to ground truth reference structures.
        }
        \label{fig:AllComparisonMolecule_Heun_RMSD}
     \end{figure}


     
    \begin{figure}
        \centering
        \includegraphics[width=.49\linewidth]{figures/model_effect_stab_Euler_ODE_forward_model_poly.pdf}
        \caption{
        Effect of the schedule used during training. Using the same schedule for both training and sampling does not enhance results for EDM, whereas the Cosine schedule emphasizes sampling in more challenging regions during training. Our VP-ISSNR sampling still outperforms other baselines, even when the model is trained using the EDM schedule.}
        \label{fig:AllComparisonMolecule_EulerODE_AnotherTraining}
     \end{figure}

\Cref{fig:AllComparisonMolecule_Heun} shows the stability rate of the generated molecular structures with the second-order integration method, Heun. We 
%can see that the is 
observe
only a marginal improvement with high NFEs, compared to the performance achieved by Euler in \cref{fig:ComparisonMolecule}.

\Cref{fig:AllComparisonMolecule_Heun_RMSD} shows molecular structure generation performance evaluated by running DFT to relax the generated structures,
which further validates the stability rate results. We see similar trend with this evaluation criterion.

\Cref{fig:AllComparisonMolecule_EulerODE_AnotherTraining} compares the sample generation performance with the diffusion model trained on different schedules, DDPM-cosine with $\nu=1$ and EDM. We can see that the model trained on the cosine schedule achieves consistently better results than the model trained on the EDM schedule, even when using the EDM schedule during sampling. This suggests that the cosine schedule samples more points on the relevant SNR region.  


     
\subsection{Image Generation}


\Cref{fig:ComparisonFFHQImage.VPComparison} compares 
existing non-VP with their VP variants
in image generation on FFHQ. 

     \begin{figure*}
        \centering
\includegraphics[width=.49\linewidth]{figures/FFHQ_fid_vs_nfe0.pdf}         
% \includegraphics[width=.49\linewidth]{figures/FFHQ_fid_vs_nfe1.pdf}        
       \caption{
        FID score {\bf(lower is better)} as a function of the number of function evaluations (NFE) in image generation on FFHQ, comparing existing non-VP schedules with their VP variants.  
        }
       \label{fig:ComparisonFFHQImage.VPComparison}
      \end{figure*}


      


\end{document}
