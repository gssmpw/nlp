

\section{Method}
%\wenzhen{In general I think this section is unclear. You need to explain specifically your pipeline of estimating CoM. The definition of the poses to grasp objects is also part of the method}

%\wenzhen{start with a high-level overview of your method -- assuming people forgot most of things in introduction}
Targeting a generalized and robust CoM estimation framework, we propose U-GRAPH: Uncertainty-Guided Rotational Active Perception with Haptics. This system incorporates a BNN that processes 6-dimensional force-torque readings and 2-dimensional orientation data to yield a 3-dimensional CoM estimation. U-GRAPH also features ActiveNet, which utilizes the output from the BNN to determine the next best action. Assuming that the robot has already grasped the object, we perform two measurements at different orientations to accurately estimate its CoM. The BNN supplies both prior predictions and quantifies uncertainty through the standard deviation. The ActiveNet takes in prior estimation distribution and uses grid search to calculate a score for each action to determine the best one. Specifically, the action space is the 2-dimensional orientation of the grasping pose. We define the action executed as changing the pose. In the subsequent subsections, we discuss an intuitive physics model, introduce the individual modules of this framework, and present the implementation of online inference.


%To obtain a robust and generalizable estimation of the center of mass, we propose U-GRAPH: Uncertainty-Guided Rotational Active Perception with Haptic.
%U-GRAPH features a Bayesian Neural Network with mean estimation and uncertainty quantification to provide both the prior and secondary estimation of the CoM of the grasping object from the Force Torque reading. This network is trained with a 6-dimensional force-torque reading input, and 2-dimensional orientation input, with the supervised label as the 3-dimensional center of mass. We also believe that this can be generalized to other physical property problems with various input and output modalities. Our system also includes a second part that focuses on determining the next best action. We refer to this part as the ActiveNet, which trains from the result of the first networks, with data of the same label but went through different actions. In our setup, the action is the 2-dimensional rotation orientation of the grasping pose. The second part can also be substituted for any continuous action space. A visualization of the system can be found in Fig. \ref{active}, and we will explain the two parts in detail in the following subsections.
\subsection{Intuitive Model of Arbitrary Object's Center Of Mass}
\label{model}
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.4]{fig/modeling3.png}
\end{center}
\caption{Illustration of the simplified model of CoM on a real-world object. In our setup, we try to estimate the $\mathrm{d}x$, $\mathrm{d}y$, and $\mathrm{d}z$ which are the displacement of CoM away from the grasping point.}
\label{model_figure}
\vspace{-3mm}
\end{figure}

%\wenzhen{Shoudl this be in the method section?}
After grasping the object, we define its CoM by some displacement $dx$, $dy$, and $dz$ away from the grasping point. These axes are defined in the world coordinates, as illustrated in Fig. \ref{model}.
Ideally, we could directly employ an analytical solution using the 6-dimensional F/T reading from an F/T sensor to determine the CoM. However, real-world complications, such as measurement noise and potential in-hand slipping of the object, complicate this process. To counteract these issues, a second measurement is necessary. %This follow-up allows for recalibration and adjustment based on any discrepancies noted from the initial data, providing a more accurate and reliable estimation of the CoM. 
Our method aims to reduce the effect of real-world challenges towards a more robust and accurate prediction.


\begin{figure*}[htbp]
\vspace{2mm}
\begin{center}
\includegraphics[scale=0.26]{fig/active.png}
\end{center}
\caption{a) Flowchart for training Bayesian Neural Network. We train BNN with Markov Chain Monte Carlo and No U-Turn Sampler iteratively. b) Flowchart for training an active perception module. We calculate the score from two orientations as the supervised label of the ActiveNet. We use the first prediction's mean and standard deviation along with the second angle as the input to the network. c) Flowchart for inferencing with U-GRAPH. The robot first grasps with a fixed orientation, then passes the F/T reading with (0, 0) as orientations into the BNN. We use ActiveNet and grid search to find the second action. We pass the second F/T reading with the orientation through BNN to get a secondary prediction and join that with the first prediction to form the posterior prediction.}
%\wenzhen{This caption is too long}}
\label{active}
\vspace{-3mm}
\end{figure*} 

\subsection{Bayesian Neural Network for Uncertainty Qualification}
%\wenzhen{It's unclear how this section is relevant to your method. At least somewhere you need to explain you are using this network to estimate CoM}
%\wenzhen{Start your subsection with directly what you are doing here, or what problem you are trying to solve. Don't start with the background or motivation. Check with other method sections too}

%\wenzhen{It's not clear what do you want to show in the paragraph. It's always nice to start with a sentence for explaining what you want to show, and then goes to details. }
%A common approach to data-driven solutions is to use a Neural Network such as a Multi-Layer Perceptron (MLP) to regress from a large dataset. However, in our scenario, the challenge extends beyond mere predictions; we also aim to quantify the uncertainty associated with these predictions. For this purpose, we utilize a Bayesian Neural Network (BNN), which maintains a similar structure to a traditional MLP but operates under non-deterministic principles. The variability in these outputs allows us to determine the uncertainty of predictions, quantifying it as the standard deviation of the estimated center of mass.


The purpose of using BNN is to get a standard deviation for its output value. Instead of training to specify the exact weight of each network node, in the BNN framework, 
we want to learn a posterior distribution $p(w|D)$ given the input dataset $D$. Each node in our BNN will have a distribution instead of a deterministic value. Given this distribution, we can obtain the estimated distribution of unseen data     $P(\hat{\mathbf{y}}|\hat{\mathbf{x}})$ by getting the expectation of the predictive distribution: $P(\hat{\mathbf{y}}|\hat{\mathbf{x}}) = \mathbb{E}_{P(\mathbf{w}|\mathcal{D})}[P(\hat{\mathbf{y}}|\hat{\mathbf{x}},\mathbf{w})]$, $\mathbf{w}$ denotes the posterior distributions of the nodes in the BNN, $\hat{\mathbf{x}}$ denotes the input testing data and $\hat{\mathbf{y}}$ denotes the output prediction.

However, to evaluate this expectation value, we will need an infinite ensemble of networks as mentioned in \cite{weightuncertainty}. To practically approximate this, Monte Carlo sampling methods, particularly Markov Chain Monte Carlo (MCMC), are employed to reduce training and inference costs. MCMC provides unbiased samples from the posterior, facilitating effective posterior inference and backpropagation.  
Further improving this approach, the Hamiltonian Monte Carlo (HMC) and No U-Turn Sampler (NUTS) are incorporated to avoid the inefficient random walk behavior and dynamically determine the optimal number of steps in the HMC. This automatically adjusts the BNN parameters after each sample to enhance convergence and accuracy \cite{hoffman2011nouturnsampleradaptivelysetting} \cite{Brooks_2011}. 

To implement the BNN and MCMC with NUTS, we used Pyro \cite{pyro} to construct the network, train on our dataset, and evaluate its predictive function. This method gives us reliable uncertainty of the regression prediction of our MLP for active perception. The illustration of the BNN training process can be found in Fig. \ref{active} a).

\subsection{ActiveNet: Action Selection Network}
As mentioned before, our actions have 2 degrees of freedom, the last two joints on the robot are free to move, while all other joints are fixed during perception.
We always keep the orientation [0,0] as the first orientation. This is the orientation where the gripper points straight down, as shown in Fig. \ref{active} c).
To find the best second orientation that improves the prediction result, we consequently design ActiveNet and use grid search to find such orientation. The most intuitive way to generate a new action is to directly estimate from the prediction of the BNN and train the network to predict the best subsequent orientation. In our case, there are usually multiple orientations that the robot can take to minimize the error of CoM prediction. A simple regression model explicitly predicts a single ``best" action, but it can overlook other ``good" actions, especially if these are localized away from the highest peak. 

We therefore try to perform a grid search through the action space and estimate a score to determine how good each action is. For simplicity, we define this score as the error of estimation obtained by the BNN after we perform a specific rotation that results in the orientation $a$. 
%During data collection, we did not collect data explicitly according to the grid we used for data collection, and our grid size (2500) is much larger than our dataset size (100), proving that our network can do robust interpolation of data points for score estimation. 
As a result, the input of our ActiveNet as illustrated in Fig. \ref{active} b) has three parts, the estimation from BNN, the standard deviation from BNN, and a new proposed action that be scored on. 
The output of the ActiveNet is a score of this new proposed action. %For simplicity, we defined this as the error of the mean of the joint prior and posterior distribution after the second orientation. Both estimations are obtained from the pre-trained BNN.
%During training, the proposed action and ground truth action score is from a randomly selected action, and its error when passed through a pre-trained BNN. 

\subsection{Inference}
Our inference pipeline is illustrated in Fig. \ref{active} c). We first use the fixed orientation to generate a prior estimation of the CoM location. Then ActiveNet performs a grid search over the entire action space and calculates the score for each action with prior estimation as input. It uses the action with the minimum action score to proceed. %\wenzhen{The following part is not about action selection. You should use a new sub-section, or merge it with the overview, or find some other ways to accommodate it.}
After we obtain the new F/T reading from the second orientation, we then predict the CoM again with the same BNN. Finally, we treat each orientation as an independently observed measurement of CoM. Since our network can provide a quantified uncertainty, we assume the two measurements are Gaussian. Therefore we can obtain the posterior estimation with: $$
%with the following formula:$$
\mu_{final} =\frac{ (\frac{\mu_1}{\sigma_1^2} + \frac{\mu_2}{\sigma_2^2})}{(\frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2})}$$
%\yuchen{$\sigma_{final}$?}

\section{Experiment Setup}

In this section, we discuss the hardware setup of the CoM estimation problem. We also explain how to set up the hardware, collect training data, and implement models.

\subsection{Hardware Setup}
\label{hardware}
The hardware system features a 6-DoF UR5e robot arm. Attached to the robot's wrist is a 6-axis NRS-6050-D80 F/T sensor from Nordbo Robotics with a sampling rate of 1000 Hz. The arm is also equipped with a WSG-50 2-fingered gripper from Weiss Robotics with customized 3D-printed PLA fingers. The system is shown in Fig. \ref{setup} a).

For data collection, we designed and 3D-printed two objects with dimensions of 15cm $\times$ 15cm $\times$ 8cm, each including two holders sized 4cm $\times$ 4cm for placing AprilTags \cite{olson2011tags}. The plate object weighs 127.36 grams and allows grasping onto the center. The box object weighs 185.36 grams and is designated to be grasped on the side. We utilize standard laboratory weights for the experiments, specifically two 100-gram weights and one 200-gram weight. Fig. \ref{setup} b) shows the printed version of these objects, as well as weights that are randomly placed on them. 

\begin{figure}[htbp]
\vspace{2mm}
\begin{center}
\includegraphics[scale=0.36]{fig/setup.png}
\end{center}
\caption{a) Example of a data collection robot grasping with the location of F/T Sensor and gripper. b) Printed data collection objects in the real world, and standard lab weights for training data collection. AprilTags are placed on all of the objects. We refer to the object on the left as Plate and the object on the right as Box. }
\label{setup}
\vspace{-5mm}
\end{figure}

\subsection{Data Collection}
%\wenzhen{I think you should highlight the experiment design part-- the creation of those standard objects and your design thoughts are very interesting. You can mention this in the introduction. Also in this section you should emphasize it. E.g. separate the data collection part; add it in the subsection title or the opening sentence}

As mentioned in Sec. \ref{hardware}, we only collect data from the two customized objects for CoM estimation. Our model is based on the premise that the CoM of any grasped object can be fundamentally described by the offsets $\mathrm{d}x$, $\mathrm{d}y$, $\mathrm{d}z$, and the gravitational force $G$ acting on the object. During each trial, we randomly select 0 to 2 weights to be fixed onto one of the objects. On the software side, our data collection algorithm first uses the overhead camera to detect AprilTag and calculate the CoM while figuring out the graspable zone on the object. It randomly generates a valid grasping point from the graspable zone and calculates the $dx$, $dy$, and $dz$ from the CoM. Finally, the robot moves to the location and grasps the object to start a trial of data collection. This data collection algorithm saves the trouble of intensive human labor and allows us to do a larger scale of data collection. 
%\yuchen{``This'' refers to?}

After securing a grasp, the robot rotates the object to 100 different orientations (excluding the default [0,0]), recording the F/T readings at each position. We use the difference between the F/T reading with the object gripped and the F/T reading with nothing between fingers. Then, we loop through the entire 100 different orientations to calculate the action score for each of them. In total, we spent about 150 hours on data collection to generate a dataset from 204 different grasps, comprising 18,893 F/T readings. 

%For the active perception part, we assume the first action is fixed. We use the rotational orientation of $[0,0]$ as the fixed first orientation, which is the orientation that the gripper points straight down, as shown in Fig. \ref{active}. ActiveNet utilizes the variety of rotational data collected from identical grasps but differing orientations to train for better predictive performance. During the evaluation and testing phases, we employ a grid search across a 50 by 50 grid to comprehensively explore a rotational space spanning $2\pi$ by $1\pi$ radians. This method allows us to systematically assess the potential reorientation angles, ensuring optimal grasp and manipulation strategies are developed based on the CoM predictions from the BNN.
\subsection{Model Implementation}
The BNN has a backbone with a hidden size of [256, 128, 64]. To speed up the training process, we first train with PyTorch for a deterministic MLP with RAdam \cite{liu2019radam} optimizer and a learning rate of 0.001 for 500 epochs. Then we use the pre-trained weights as mean and standard deviation of 0.5 as the initialization for our BNN. We train the BNN using Pyro with 1000 samples and 200 warmup steps. 

The ActiveNet is a 5-layer MLP with a hidden size of [1024, 1024, 512, 64]. We train the ActiveNet with RAdam optimizer with a learning rate of 0.0001 for 500 epochs.