we step into a new era of computing, each leap in processor technology not only boosts performance and efficiency but alters the landscape of cybersecurity threats~\cite{pandorasbox}. 
Optimizations can introduce side channels, potentially exposing sensitive data to adversaries. Simultaneously, artificial intelligence (AI) computations are increasingly becoming a significant part of the workload for various types of chips.

Matrix operations are fundamental to many AI and machine learning (ML) algorithms, especially in the training and inference processes of neural networks. Consequently, there is significant emphasis on optimizing these operations and enhancing the hardware to support them efficiently. This has led to the development and refinement of specialized processors such as GPUs, TPUs, and other AI-specific chips designed to handle these intensive computational tasks more effectively.

In a major change, AI accelerators are no longer solely standalone, discrete units. Instead, they are directly integrated into CPU architectures\cite{jeong2021rasa,jeong2023vegeta,nassif2022sapphire, elster2022nvidia}. This integration creates fast data paths and reduces delays. As a result, it boosts performance and improves energy efficiency compared to separate units.

Intel Advanced Matrix Extensions (AMX) is an on-chip accelerator designed to enhance the computational capabilities of modern processors. Integrated into Intel's latest processor architectures, specifically starting with the 4th Gen Intel Xeon Scalable processors code-named Sapphire Rapids, Intel AMX provides specialized hardware support for high-throughput operations required for matrix multiplication. By significantly boosting the efficiency and speed of these operations, Intel AMX enables faster training and inference times, leading to more responsive AI applications and more efficient use of system resources ~\cite{intel2022amx}. 



In this paper, we demonstrate that the execution times of AMX multiplication are not fixed and that AMX multiplication operates in five distinct power states, ranging from 16 to 20,000 cycles. %(Figure~\ref{fig:Power_Stages}). 
We also discovered that the execution time of AMX multiplication in high-latency power states depends on the specific values of the data it processes. This creates exploitable timing differences that can be used to infer sensitive information, such as the weights of a neural network model or the input data. We present the techniques required to practically exploit this side-channel vulnerability in neural networks. We show that it is possible to leak the 64 hidden binary weights of a neural network solely by measuring the execution time, achieving 100\% accuracy after a duration of 50 minutes.