\section{Related Work}
\subsection{Mathematical Reasoning Tasks}

Mathematical reasoning tasks comprise a series of tasks developed to examine the ability of machine learning systems to solve mathematical problems and conduct abstract, logical, and quantitative reasoning.
Early studies in this field started with collecting and solving applied mathematical problems described in natural languages, which are also known as math word problems.
Several representative datasets for math word problems include MAWPS \cite{mawps}, Math23K \cite{math23}, and MathQA \cite{mathqa}.
Pushing the boundaries beyond applied mathematical problems, 
Saxton et al. constructed a large-scale dataset offering mathematical problems generated in a wide spectrum of mathematical areas \cite{math_dm},
and other work has also introduced datasets specialized to geometric problems \cite{math_g0} and theorem proving \cite{math_atp,ldj}.
Upon entering the era of LLMs, recent work published datasets collating mathematical problems together with their solutions annotated in natural languages \cite{gsm8k}.
Such tasks have also become an important benchmark in the evaluation of LLMs \cite{mmlu}.

\subsection{Mathematical Problem Solvers}

While it is feasible to train machine learning models to solve mathematical problems through end-to-end predictions, 
existing studies have revealed significant limitations in their generalization capacity,
which is generally because end-to-end inference inevitably receives and predicts rational numbers by digits or subwords, and thus results in the absence of the concept of numbers in their entirety \cite{math_dm}.
In contrast, an alternative approach that has been broadly adopted contemporarily is to make use of intermediate labels named equations or formulas \cite{mawps,math23,mathqa}.
This approach carried out a neuro-symbolic paradigm in which these intermediate labels, usually predicted by neural networks, can be interpreted as specific computations and executed by external tools to acquire problem answers.
Most of the research adopting this approach requires these intermediate labels to be prepared in the training data to permit straightforward training on the inference model.
Some other studies have also developed methods to learn these labels utilizing weak supervision derived from questions and answers \cite{math_fix,coling}.


On the other hand, with the recent advancements in large language models (LLMs), recent studies have also investigated the feasibility of prompting LLMs to solve mathematical problems. 
This approach is also known as Chain-of-Thought (CoT) because of LLMs' proficiency in generating the solution to problems as a chain that formulates the sequential reasoning steps \cite{palm,cot}.
This practice can be realized either through fine-tuning pretrained language models or utilizing few-shot or even zero-shot prompting \cite{zs1,zs2,zsr}.
% 
Being aware of the precision issue exposed in mathematical calculations performed by LLMs, researchers also integrated CoT with neuro-symbolic computations by implementing CoT to generate the expressions for calculations and delegating the computations to external tools \cite{gsm8k}.
Similarly, Wolfram developed a solution equipping GPTs with the APIs of Wolfram, which merged the comprehensive reasoning capability of LLMs and the computational prowess of Wolfram \cite{wolfram}.