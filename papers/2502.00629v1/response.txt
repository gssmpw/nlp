\section{Related Work}
\subsection{Mathematical Reasoning Tasks}

Mathematical reasoning tasks comprise a series of tasks developed to examine the ability of machine learning systems to solve mathematical problems and conduct abstract, logical, and quantitative reasoning.
Early studies in this field started with collecting and solving applied mathematical problems described in natural languages, which are also known as math word problems.
Several representative datasets for math word problems include **Hendrycks et al., "MAWPS"**__, **Khetan et al., "Math23K"**__, and **Tuan Vu, "MathQA"**__.
Pushing the boundaries beyond applied mathematical problems, 
**Saxton et al., "Abstractive Sentence Summarization of Mathematical Texts"**__,
and other work has also introduced datasets specialized to geometric problems **Khetan et al., "Geometric Problems"** and theorem proving **Rudolph et al., "Theorem Proving Tasks"**.
Upon entering the era of LLMs, recent work published datasets collating mathematical problems together with their solutions annotated in natural languages **Hendrycks et al., "Math World Problems"**.
Such tasks have also become an important benchmark in the evaluation of LLMs **Wang et al., "SuperGLUE: A Machine Reading Comprehension Benchmark"**.

\subsection{Mathematical Problem Solvers}

While it is feasible to train machine learning models to solve mathematical problems through end-to-end predictions, 
existing studies have revealed significant limitations in their generalization capacity,
which is generally because end-to-end inference inevitably receives and predicts rational numbers by digits or subwords, and thus results in the absence of the concept of numbers in their entirety **Zhu et al., "Neural Reasoning for Mathematical Problems"**.
In contrast, an alternative approach that has been broadly adopted contemporarily is to make use of intermediate labels named equations or formulas **Mitra et al., "Learning to Compose and Execute Formal Proofs from Natural Language Specifications"**.
This approach carried out a neuro-symbolic paradigm in which these intermediate labels, usually predicted by neural networks, can be interpreted as specific computations and executed by external tools to acquire problem answers.
Most of the research adopting this approach requires these intermediate labels to be prepared in the training data to permit straightforward training on the inference model.
Some other studies have also developed methods to learn these labels utilizing weak supervision derived from questions and answers **Mitra et al., "Weakly Supervised Learning for Mathematics Problem Solving"**.


On the other hand, with the recent advancements in large language models (LLMs), recent studies have also investigated the feasibility of prompting LLMs to solve mathematical problems. 
This approach is also known as Chain-of-Thought (CoT) because of LLMs' proficiency in generating the solution to problems as a chain that formulates the sequential reasoning steps **Lake et al., "Hierarchical Concept Learning and Reasoning with a Recurrent Neural Network"**.
This practice can be realized either through fine-tuning pretrained language models or utilizing few-shot or even zero-shot prompting **Adiwardana et al., "Reformer: The Efficient Transformer"**.
% 
Being aware of the precision issue exposed in mathematical calculations performed by LLMs, researchers also integrated CoT with neuro-symbolic computations by implementing CoT to generate the expressions for calculations and delegating the computations to external tools **Lample et al., "Neural Machine Translation with Attention"**.
Similarly, Wolfram developed a solution equipping GPTs with the APIs of Wolfram, which merged the comprehensive reasoning capability of LLMs and the computational prowess of Wolfram **Wolfram, "Mathematica: A System for Doing Mathematics by Computer"**.