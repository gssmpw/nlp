\section{Experiments}

\subsection{Datasets and Metrics}
\begin{table}[h]
\small
\centering
\begin{tabular}{rrr}
\toprule
\textbf{Token Length} & \textbf{\# of Samples} & \textbf{Proportion (\%)} \\ \midrule
6K $\leq$ Data $<$ 8K & 410,876 & 36.64 \\
8K $\leq$ Data $<$ 16K & 418,332  & 37.30 \\
16K $\leq$ Data $<$ 32K & 207,503 & 18.50 \\
32K $\leq$ Data $<$ 64K & 69,396 & 6.19 \\
64K $\leq$ Data $<$ 128K & 13,659 & 1.22 \\
128K $\leq$ Data & 1,679 & 0.15 \\ \midrule
\textbf{Total} & 1,121,445 & 100.00 \\ \bottomrule
\end{tabular}
\caption{
% \sumin{
% \textbf{Data distribution in FineWeb-Edu dataset}, used for training, categorized by data length. This table highlights the focus on long-context modeling and provides a percentage breakdown for each data length category.
\textbf{Data distribution of the training set extracted from FineWeb-Edu.} Our training set is constructed focusing on long-context modeling.
% }
}
\label{tab:data_stats}
\end{table}

\paragraph{FineWeb-Edu}
FineWeb-Edu~\cite{finewebedu} comprises 1.3T tokens, filtered from the FineWeb dataset, which contains 15T tokens. To facilitate long-form context modeling, we selected texts with a minimum length of 4K tokens, resulting in a dataset that includes texts up to 339K tokens in length. The statistics of the training data are provided in the Table~\ref{tab:data_stats}. For evaluation, we curated 1,000 texts of 128K tokens each to assess perplexity.

\paragraph{FineWeb-LQA}
FineWeb-LQA, a long-form QA dataset, was automatically generated from FineWeb-Edu to support the training of our query-dependent model, following a data construction process similar to \citet{in2}. We extract random 128-token text segments and utilize the Llama-3.1-70B-Instruct-FP8 model to generate corresponding QA pairs. The extracted segments are then reintegrated into the original long-form context, which serves as the basis for evaluating long-form QA tasks.

\paragraph{InfiniteBench}
InfiniteBench~\cite{infinitebench} is a benchmark suite designed to assess the capability of LLMs in handling ultra-long contexts, exceeding 100K tokens. We focus on two tasks: LongBook QA (En.QA) and LongBook Multiple Choice (En.MC), both of which test the model's ability to answer identical questions in open-ended and multiple-choice formats, respectively. Evaluation is conducted using the F1 score for En.QA and accuracy for En.MC.

\paragraph{LongBench}
LongBench~\cite{longbench} evaluates long-form context modeling through a suite of tasks across real-world and synthetic categories. In this study, we focus on six English tasks, consisting of both single-document and multi-document QA tasks. The average context length is 18K tokens, with a maximum of 82K tokens. Performance is measured using the F1 score.

\paragraph{L-Eval}
L-Eval~\cite{leval} includes a dataset of 508 long documents spanning various domains, divided into 20 subtasks. It comprises over 2K human-annotated query-response pairs, with context lengths ranging from 3K to 200K tokens. We evaluate the models on four open-ended QA tasks using the F1 score and four multiple-choice QA tasks using accuracy.

\subsection{Models}
We build upon Llama2-7B~\cite{llama2} as our baseline model, augmenting its long-form context modeling capabilities. In line with \cite{autocompressor}, we implement an extended version of Llama2 that supports full attention over longer contexts (ExtendedFA), extending the token length limit to 8K by modifying the RoPE $\theta$~\cite{rope}. Due to the quadratic complexity of full attention, ExtendedFA is restricted to a maximum of 8K tokens. We compare this approach against AutoCompressor~\cite{autocompressor}, a state-of-the-art method that models context through prompt compression by recurrently feeding segmented inputs to the model while leveraging compressed tokens for subsequent segments. Additionally, we evaluate our proposed method with and without query dependent modeling.

% LCIRC and the baseline models (ExtendedFA ) are  trained on FineWeb-Edu to ensure fair comparisons in language modeling.
% For QD-LCIRC, we initialize the model with the pre-trained weights of LCIRC on FineWeb-Edu and fine-tune it on FineWeb-LQA for query dependent modeling. 

All models are trained on FineWeb-Edu to ensure fair comparisons in language modeling. For QD-LCIRC, we initialize the model with the pre-trained weights of LCIRC on FineWeb-Edu and fine-tune it on FineWeb-LQA for query dependent modeling. 
% \sumin{Moreover, ExtendedFA and AutoCompressor are fine-tuned on FineWeb-LQA for fair comparisons. However, unlike QD-LCIRC, they are unable to model context in a query dependent manner.}
Note that FineWeb-LQA is automatically generated from FineWeb-Edu for this purpose.

For baseline models, context exceeding the token length limit is truncated. Llama2 and ExtendedFA handle up to 4K and 8K tokens, respectively, while AutoCompressor supports up to 84K tokens, based on the experimental setup from \citet{autocompressor}. In contrast, our method imposes no explicit length limit, as the compressed context is injected via cross-attention, allowing us to process sequences up to 815K tokens in length.

\subsection{Implementation Details}
All models are based on Llama2-7B and trained using a batch size of 64 with the Adam optimizer \cite{adam}.
QD-LCIRC is trained with a learning rate of 2e-5 and 300 warmup steps,
% \sumin{
utilizing Selective State BPTT with 8 random selections.
% }
Other models are trained with a learning rate of 5e-5.
% \sumin{
The length ($K$) of the initial learnable queries $\vh^{(0)}$, which also corresponds to the length of each compressed features $\vh^{(i)}$, is set to 64 based on our preliminary experiments with a smaller model OPT-2.7B.
In these experiments, we observed no significant performance difference between $K=64$ and $K=256$, leading us to adopt the more efficient setting of $K=64$.
% When we conducted preliminary experiments based on OPT-2.7B~\cite{zhang2022opt}, there was no significant performance difference between $K$ = 64 and $K$ = 256.}
All training procedures are conducted using eight NVIDIA H100 80GB GPUs.

\begin{table}[t]
\small
\centering
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{4}{c}{Total Token Length ($N$)}\\
 \cmidrule(lr){2-5}
Models & 4k & 8k & 64k & 128k\\ \midrule
Llama-2-7B & 5.472 & - & - & - \\
ExtendedFA & \textbf{5.442} & 5.319 & - & - \\
AutoCompressor & 6.127 & 6.010 & 6.188 & -\\
LCIRC (Ours) & 5.472 & 5.313 & 5.312 & 5.312\\
QD-LCIRC (Ours) & 5.472 & \textbf{5.299} & \textbf{5.298} & \textbf{5.298}\\ \bottomrule
\end{tabular}
\caption{
\textbf{Perplexity scores on the FineWeb-Edu test set.} 
Each long-form text is truncated from the beginning to adhere to the total token length, which encompasses both the context and the last 2K target tokens used for measuring perplexity.
}
\label{table:ppl}
\end{table}

\begin{table}[t]
\small
\centering
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{4}{c}{Total Token Length ($N$)}\\
 \cmidrule(lr){2-5}
Models & 4k & 8k & 64k & 128k\\ \midrule
% Llama-2-7B & 5.472 & - & - & - \\
ExtendedFA & 63 & 143 & 3,118 & 10,739 \\
AutoCompressor & 61 & 125 & 1,350 & -\\
% LCIRC (Ours) & 36 & 37 & 57 & 79\\
LCIRC (Ours) & 63 & 77 & 97 & 120\\
% QD-LCIRC (Ours) & 36 & 38 & 58 & 81\\
QD-LCIRC (Ours) & 63 & 77 & 98 & 122\\ \bottomrule
\end{tabular}
\caption{
\textbf{Computational complexities for different models in TeraFLOPs.}
We compute the TFLOPs of ExtendedFA under the assumption that the model is extended to process input tokens of the specified length.
AutoCompressor is unable to process inputs with 128K tokens.
}
\label{table:cost}
\end{table}

\begin{table*}[t]
\centering
\begingroup
\setlength{\heavyrulewidth}{0.10em}  
\setlength{\lightrulewidth}{0.055em}
\resizebox{\textwidth}{!}{
\begin{tabular}{l cc ccc ccccccc}
\toprule
 & & & \multicolumn{3}{c}{\textbf{InfiniteBench}} & \multicolumn{7}{c}{\textbf{LongBench}}\\
 \cmidrule(lr){4-6}  \cmidrule(lr){7-13}
 & \textbf{FW-LQA} & \textbf{QD} & \textbf{En.MC} & \textbf{En.QA} & \textbf{Avg} & \textbf{NQA} & \textbf{Qasper} & \textbf{MFQA} & \textbf{HQA} & \textbf{2WQA} & \textbf{MSQ} & \textbf{Avg}\\ \midrule
Llama-2-7B & \ding{55} & \ding{55} & 6.99 & 3.95 & 5.47 & \textbf{13.04} & 12.08 & 14.68 & 16.27 & 7.10 & 4.41 & 11.26\\
ExtendedFA & \ding{55} & \ding{55} & 15.72 & 3.88 & 9.80 & 12.21 & 18.23 & 18.23 & 17.81 & 14.18 & 8.25 & 14.82\\
AutoCompressor & \ding{55} & \ding{55} & 18.34 & 4.46 & 11.40 & 12.60 & 16.89 & 19.93 & 19.00 & 16.36 & 8.84 & 15.60\\
LCIRC (Ours) & \ding{55} & \ding{55} & \textbf{21.40} & \textbf{5.26} & \textbf{13.33} & 10.67 & \textbf{18.32} & \textbf{21.71} & \textbf{21.66} & \textbf{16.55} & \textbf{9.09} & \textbf{16.33}\\ \midrule
ExtendedFA & \ding{51} & \ding{55} & 28.38 & 4.55 & 16.47 & \textbf{18.96} & 13.73 & 23.48 & 20.78 & 17.24 & 8.29 & 17.08\\
AutoCompressor & \ding{51} & \ding{55} & 31.00 & 5.35 & 18.18 & 13.69 & 18.63 & \textbf{33.55} & 15.01 & 14.13 & 8.98 & 17.33\\
QD-LCIRC (Ours) & \ding{51} & \ding{51} & \textbf{38.86} & \textbf{5.80} & \textbf{22.33} & 15.31 & \textbf{20.57} & 33.25 & \textbf{28.19} & \textbf{19.00} & \textbf{12.39} & \textbf{21.45}\\ \bottomrule
\end{tabular}
}
\endgroup
\caption{\textbf{Per-task performance on InfiniteBench and LongBench.} The following abbreviations are used: \textbf{NQA} denotes NarrativeQA, \textbf{MFQA} represents MultiFieldQA-en, \textbf{HQA} refers to HotpotQA, \textbf{2WQA} to 2WikiMQA, and \textbf{MSQ} to MuSiQue. \textbf{Avg} indicates the average score across all subtasks within respective benchmarks.
\textbf{FW-LQA} indicates whether the model is fine-tuned on FineWeb-LQA.
Our QD-LCIRC consistently outperforms competing methods, achieving the highest average score by incorporating query dependent modeling, as indicated in the \textbf{QD} column.
}
\label{tab:infinite_long_bench}

\end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{l c ccc ccccccc}
% \toprule
%  & \multirow{2}{*}{LQA} & \multicolumn{3}{c}{\textbf{InfiniteBench}} & \multicolumn{7}{c}{\textbf{LongBench}}\\
%  \cmidrule(lr){3-5}  \cmidrule(lr){6-12}
%  & & \textbf{En.MC} & \textbf{En.QA} & \textbf{Avg} & \textbf{NQA} & \textbf{Qasper} & \textbf{MFQA} & \textbf{HQA} & \textbf{2WQA} & \textbf{MSQ} & \textbf{Avg}\\ \midrule
% Llama-2-7B & \ding{55} & 6.99 & 3.95 & 5.47 & \textbf{13.04} & 12.08 & 14.68 & 16.27 & 7.10 & 4.41 & 11.26\\
% ExtendedFA & \ding{55} & 15.72 & 3.88 & 9.80 & 12.21 & 18.23 & 18.23 & 17.81 & 14.18 & 8.25 & 14.82\\
% AutoCompressor & \ding{55} & 18.34 & 4.46 & 11.40 & 12.60 & 16.89 & 19.93 & 19.00 & 16.36 & 8.84 & 15.60\\
% LCIRC (Ours) & \ding{55} & \textbf{21.40} & \textbf{5.26} & \textbf{13.33} & 10.67 & \textbf{18.32} & \textbf{21.71} & \textbf{21.66} & \textbf{16.55} & \textbf{9.09} & \textbf{16.33}\\ \midrule
% ExtendedFA & \ding{51} & 28.38 & 4.55 & 16.47 & \textbf{18.96} & 13.73 & 23.48 & 20.78 & 17.24 & 8.29 & 17.08\\
% AutoCompressor & \ding{51} & 31.00 & 5.35 & 18.18 & 13.69 & 18.63 & \textbf{33.55} & 15.01 & 14.13 & 8.98 & 17.33\\
% QD-LCIRC (Ours) & \ding{51} & \textbf{38.86} & \textbf{5.80} & \textbf{22.33} & 15.31 & \textbf{20.57} & 33.25 & \textbf{28.19} & \textbf{19.00} & \textbf{12.39} & \textbf{21.45}\\ \bottomrule
% \end{tabular}
% \caption{\textbf{Per-task performance on InfiniteBench and LongBench.} The following abbreviations are used: \textbf{NQA} denotes NarrativeQA, \textbf{MFQA} represents MultiFieldQA-en, \textbf{HQA} refers to HotpotQA, \textbf{2WQA} to 2WikiMQA, and \textbf{MSQ} to MuSiQue. \textbf{Avg} indicates the average score across all subtasks within respective benchmarks. \sumin{Our QD-LCIRC consistently outperforms competing methods, achieving the highest scores across the majority of evaluated tasks. ExtendedFA-LQA and AutoCompressor-LQA are models fine-tuned with FineWeb-LQA.}}
% \label{tab:infinite_long_bench}

% \end{table*}

\begin{table*}[t]
\small
\centering
\begingroup
\setlength{\heavyrulewidth}{0.10em}  
\setlength{\lightrulewidth}{0.055em}
\resizebox{\textwidth}{!}{
\begin{tabular}{l ccccccccccc}
\toprule
 & \textbf{FW-LQA} & \textbf{QD} & \textbf{CS} & \textbf{QALIT} & \textbf{TOEFL} & \textbf{SF} & \textbf{LFQA} & \textbf{NQA} & \textbf{NQ} & \textbf{Qasper} & \textbf{Avg} \\ \midrule
Llama-2-7B & \ding{55} & \ding{55} & 10.47 & 25.74 & 0.00 & 29.69 & 22.30 & 8.66 & 22.64 & 2.29 & 15.22\\
ExtendedFA & \ding{55} & \ding{55} & 16.86 & 33.66 & 17.10 & 22.66 & 13.89 & 10.76 & 33.24 & 10.96 & 19.89\\
AutoCompressor & \ding{55} & \ding{55} & 18.60 & 31.68 & 17.47 & 22.66 & 11.73 & 4.82 & 26.42 & 6.64 & 17.50 \\
LCIRC (Ours) & \ding{55} & \ding{55} & 22.09 & 27.23 & 10.04 & 27.34 & 14.66 & 9.89 & 26.40 & 9.44 & 18.39\\ \midrule
ExtendedFA & \ding{51} & \ding{55} & 15.70 & \textbf{34.16} & 13.75 & 20.31 & 28.27 & 16.91 & \textbf{35.68} & 8.51 & 21.66 \\
AutoCompressor & \ding{51} & \ding{55} & 20.35 & 32.18 & \textbf{26.39} & 16.41 & 29.33 & 19.08 & 28.67 & 15.38 & 23.47 \\
QD-LCIRC (Ours) & \ding{51} & \ding{51} & \textbf{25.58} & 30.20 & 12.27 & \textbf{37.50} & \textbf{31.63} & \textbf{20.92} & 34.37 & \textbf{16.92} & \textbf{26.17}\\ \bottomrule
\end{tabular}
}
\endgroup
\caption{\textbf{Per-task performance on L-Eval}. The following abbreviations are used: \textbf{CS} denotes Coursera, \textbf{QALIT} refers to QuALITY, \textbf{SF} represents SFiction, \textbf{LFQA} refers to LongFQA, and \textbf{NQA} to NarrativeQA. \textbf{Avg} indicates the mean performance score across all subtasks within the respective benchmark. \textbf{FW-LQA} indicates whether the model has been fine-tuned on FineWeb-LQA, while \textbf{QD} denotes whether query dependent modeling.}
\label{tab:leval}
\end{table*}

\begin{table}[t]
\small
\centering
\begin{tabular}{lcccc}
\toprule
 & InfBench & LongBench & L-Eval \\ \midrule
% Vanilla BPTT & 23.10 & 20.31 &  \\
Truncated BPTT & 21.26 & 20.73 & 25.45\\
Selective State BPTT & \textbf{22.33} & \textbf{21.45} & \textbf{26.17}\\ \bottomrule
\end{tabular}
\caption{\textbf{Impact of different BPTT variations on benchmark performance.} This table presents the performance of the model trained with Truncated BPTT, and Selective State BPTT ($T=8$) across three benchmarks: InfiniteBench, LongBench, and L-Eval. The results highlight the differences in benchmark scores for each method, demonstrating the effectiveness of Selective State BPTT.}
\label{table:cos}
\end{table}
\vspace{-0.2cm}


\subsection{Results on Language Modeling}
Following~\citet{autocompressor}, we first measure the perplexity of the last 2K tokens given its context while varying the total token length between 4K and 128K, which refers to the combined length of both the context and target tokens.
To ensure that the same tokens are used for perplexity measurement for comparisons, we truncate the inputs from the beginning based on the total context length ($N$), retaining the last $N$ tokens, of which the final 2K tokens are used for the perplexity computation.
Note that all the models except ours impose limits on context lengths. 

Table~\ref{table:ppl} presents the perplexity results of the models on the FineWeb-Edu test set.
We first observe that all the methods, except for AutoCompressor, perform similarly when $N=4$K.
AutoCompressor significantly alters Llama's generation mechanism resulting in a notable drop in perplexity.
This finding aligns with the observations from experiments conducted with Llama in \citet{autocompressor}.
In contrast, our method preserves the original strong capabilities of Llama with short contexts, as the frozen LLM remains unchanged.

With $N>4$K, Llama is unable to process the full context.
In contrast, all other models exhibit improved perplexity scores by utilizing additional context compared to their scores with $N=4$K. 
Note, however, that both ExtendedFA and AutoCompressor still impose strict limits on the total context length. 
Moreover, the perplexity of AutoCompressor increases at $N=64$K, indicating challenging optimization for long-form context modeling. 
In contrast, the proposed LCIRC model consistently improves perplexity and maintains this improved performance even as the context further lengthens.

% Table \ref{table:ppl} shows the perplexity results of each model for the last 2K tokens in documents ranging from 4K to 128K in length. Compared to other models, our model shows the best language modeling performance for long context tokens of 4K or more. All models except ours have limitations on the number of tokens that can be processed. 
% Llama2-7B baseline model and ExtendedFA model can only process tokens of a given context window size, and even the AutoCompressor model that performs prompt compression cannot process 126K context tokens. However, our model does not have a limit on the number of tokens that can be processed. 
% Our model is trained on Llama2-7B with 4K size of the context window as other models, but can process 126K context tokens with the best perplexity performance. 
% The AutoCompressor model that performs prompt compression like us shows that perplexity increases when processing contexts of 30K or more, resulting in failure to model long contexts. On the other hand, our model shows the results of perplexity that does not increase as the context tokens become longer. Our model generates compressed history tokens only for context tokens that exceed the context window size of the pre-trained language model, in order to preserve the good language understanding and inference ability of the pre-trained language model as much as possible. Therefore, when looking at the results for 2K context tokens, AutoCompressor shows lower performance than the baseline model Llama2-7B, while our model shows the same performance.

% \begin{table}[t]
% \small
% \centering
% \begin{tabular}{lcccc}
% \toprule
%  & \multicolumn{4}{c}{Total Token Length ($N$)}\\
%  \cmidrule(lr){2-5}
% Models & 4k & 8k & 64k & 128k\\ \midrule
% % Llama-2-7B & 5.472 & - & - & - \\
% ExtendedFA & 63 & 143 & 3,118 & 10,739 \\
% AutoCompressor & 61 & 125 & 1,350 & -\\
% % LCIRC (Ours) & 36 & 37 & 57 & 79\\
% LCIRC (Ours) & 63 & 77 & 97 & 120\\
% % QD-LCIRC (Ours) & 36 & 38 & 58 & 81\\
% QD-LCIRC (Ours) & 63 & 77 & 98 & 122\\ \bottomrule
% \end{tabular}
% \caption{
% \textbf{Computational complexities for different models in TeraFLOPs.}
% We compute the TFLOPs of ExtendedFA under the assumption that the model is extended to process input tokens of the specified length.
% AutoCompressor is unable to process inputs with 128K tokens.
% }
% \label{table:cost}
% \end{table}

We evaluate the computational complexities of the models for processing long-term contexts with varying total token lengths at inference, as shown in Table~\ref{table:cost}. 
As the token length increases, the results show a prohibitively large increase in complexity with ExtendedFA, which requires full attention across all tokens.
In contrast, AutoCompressor effectively reduces complexity compared to ExtendedFA; 
a reduction rate increases with the number of tokens and showing 66\% reduction with 64K tokens. 
However, AutoCompressor cannot process tokens longer than 64K with segments of 2K token length.
In contrast, LCIRC can handle 128K tokens, while achieving a 99\% complexity reduction compared to ExtendedFA. 
Note that LCIRC improved perplexity while maintaining this significant reduction in complexity.
Finally, QD-LCIRC introduces some additional complexities, but they are marginal compared to the overall reduction rate.


% We show the inference cost for each model in Table \ref{table:cost}. Although all models process the same length of 8,192 tokens, our model shows the cheapest inference cost. ExtendedFA that does not perform prompt compression shows the 

% \begin{table}[ht]
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lccccccc}
% \toprule
%  & \multicolumn{7}{c}{Context Token Lengths}\\
%  \cmidrule(lr){2-8}
% Models & 2k & 4k & 6k & 14k & 30k & 62k & 126k\\ \midrule
% Llama-2-7B & 5.47 & - & - & - & - & - & -\\
% Extended FA & \textbf{5.44} & 5.37 & 5.32 & - & - & - & - \\
% AutoCompressor & 6.13 & 6.02 & 6.01 & 6.02 & 6.07 & 6.19 & -\\
% Ours & 5.47 & \textbf{5.30} & \textbf{5.30} & \textbf{5.30} & \textbf{5.30} & \textbf{5.30} & \textbf{5.30}\\ \bottomrule
% \end{tabular}
% }
% \caption{Perplexity score measured for the last 2K tokens of data for each model.}
% \label{table:ppl}
% \end{table}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{l ccc ccccccc}
% \toprule
%  & \multicolumn{3}{c}{\textbf{InfiniteBench}} & \multicolumn{7}{c}{\textbf{LongBench}}\\
%  \cmidrule(lr){2-4}  \cmidrule(lr){5-11}
%  & \textbf{En.MC} & \textbf{En.QA} & \textbf{Avg} & \textbf{NQA} & \textbf{Qasper} & \textbf{MFQA} & \textbf{HQA} & \textbf{2WQA} & \textbf{MSQ} & \textbf{Avg}\\ \midrule
% Llama-2-7B & 6.99 & 3.95 & 5.47 & 13.04 & 12.08 & 14.68 & 16.27 & 7.10 & 4.41 & 11.26\\
% ExtendedFA & 15.72 & 3.88 & 9.80 & 12.21 & 18.23 & 18.23 & 17.81 & 14.18 & 8.25 & 14.82\\
% AutoCompressor & 18.34 & 4.46 & 11.40 & 12.60 & 16.89 & 19.93 & 19.00 & 16.36 & 8.84 & 15.60\\
% LCIRC (Ours) & 21.40 & 5.26 & 13.33 & 10.67 & 18.32 & 21.71 & 21.66 & 16.55 & 9.09 & 16.33\\
% QD-LCIRC (Ours) & \textbf{38.86} & \textbf{5.80} & \textbf{22.33} & \textbf{15.31} & \textbf{20.57} & \textbf{33.25} & \textbf{28.19} & \textbf{19.00} & \textbf{12.39} & \textbf{21.45}\\ \bottomrule
% \end{tabular}
% \caption{\textbf{Per-task performance on InfiniteBench and LongBench.} The following abbreviations are used: \textbf{NQA} denotes NarrativeQA, \textbf{MFQA} represents MultiFieldQA-en, \textbf{HQA} refers to HotpotQA, \textbf{2WQA} to 2WikiMQA, and \textbf{MSQ} to MuSiQue. \textbf{Avg} indicates the average score across all subtasks within respective benchmarks. Our QD-LCIRC consistently outperforms competing methods, achieving the highest scores across all evaluated tasks.}
% \label{tab:infinite_long_bench}

% \end{table*}

% \begin{table*}[t]
% \small
% \centering
% \begin{tabular}{l ccccccccc}
% \toprule
%  & \textbf{CS} & \textbf{QALIT} & \textbf{TOEFL} & \textbf{SF} & \textbf{LFQA} & \textbf{NQA} & \textbf{NQ} & \textbf{Qasper} & \textbf{Avg} \\ \midrule
% Llama-2-7B & 10.47 & 25.74 & 0.00 & 29.69 & 22.30 & 8.66 & 22.64 & 2.29 & 15.22\\
% ExtendedFA & 16.86 & \textbf{33.66} & 17.10 & 22.66 & 13.89 & 10.76 & 33.24 & 10.96 & 19.89\\
% AutoCompressor & 18.60 & 31.68 & \textbf{17.47} & 22.66 & 11.73 & 4.82 & 26.42 & 6.64 & 17.50 \\
% LCIRC (Ours) & 22.09 & 27.23 & 10.04 & 27.34 & 14.66 & 9.89 & 26.40 & 9.44 & 18.39\\
% QD-LCIRC (Ours) & \textbf{25.58} & 30.20 & 12.27 & \textbf{37.50} & \textbf{31.63} & \textbf{20.92} & \textbf{34.37} & \textbf{16.92} & \textbf{26.17}\\ \bottomrule
% \end{tabular}
% \caption{Per-task performance on \textbf{L-Eval}. The following abbreviations are used: \textbf{CS} denotes Coursera, \textbf{QALIT} refers to QuALITY, \textbf{SF} represents SFiction, \textbf{LFQA} refers to LongFQA, and \textbf{NQA} to NarrativeQA. \textbf{Avg} indicates the mean performance score across all subtasks within the respective benchmark.}
% \label{tab:leval}
% \end{table*}

% \begin{table}[t]
% \small
% \centering
% \begin{tabular}{lcccc}
% \toprule
%  & InfBench & LongBench & L-Eval \\ \midrule
% % Vanilla BPTT & 23.10 & 20.31 &  \\
% Truncated BPTT & 21.26 & 20.73 & 25.45\\
% Selective State BPTT & \textbf{22.33} & \textbf{21.45} & \textbf{26.17}\\ \bottomrule
% \end{tabular}
% \caption{\textbf{Impact of different BPTT variations on benchmark performance.} This table presents the performance of the model trained with Truncated BPTT, and Selective State BPTT ($T$=8) across three benchmarks: InfiniteBench, LongBench, and L-Eval. The results highlight the differences in benchmark scores for each method, demonstrating the effectiveness of Selective State BPTT.}
% \label{table:cos}
% \end{table}
% \vspace{-0.2cm}

\subsection{Results on Long Context Benchmarks}
We also evaluate the methods on multiple QA benchmarks that require long-form context understanding.
In these benchmarks, models are asked to answer a question, requiring to understand the input text under the context of the question. 
For QD-LCIRC, we use the input question as the query.

Table~\ref{tab:infinite_long_bench} presents performances on InfiniteBench and LongBench.
Since the QA instances in these benchmarks require an understanding of long-form context documents, Llama exhibits poor performance across all tasks.
ExtendedFA improves this by leveraging additional context, but it is still limited to 8K tokens, sharing the same underlying issue as Llama.
Both AutoCompressor and LCIRC further enhance performance over ExtendedFA by enabling access to much longer contexts, with LCIRC achieving much greater improvements.
When LCIRC is combined with our query-dependent modeling technique (QD-LCIRC), it leads to significant performance gains, yielding approximately 308\% and 90\% relative improvements in average scores over the base Llama model on InfiniteBench and LongBench, respectively.
To ensure fair comparisons, we further fine-tuned ExtendedFA and AutoCompressor on FineWeb-LQA.
Despite this, our QD-LCIRC still achieves significant improvements over these models, consistently delivering the best performance on most tasks and resulting in the highest average score.
This further underscores the effectiveness of incorporating query dependent modeling.


% \sumin{
% Furthermore, When LCIRC is combined with our query-dependent modeling technique, it yields significant improvements compared to Extended-LQA and AutoCompressor-LQA.
% As a result, QD-LCIRC outperforms all other models, including Extended-LQA and AutoCompressor-LQA, across most tasks.
% Notably, it achieves the highest average performance, demonstrating approximately 308\% and 90\% relative gains over the base Llama model on InfiniteBench and LongBench, respectively.
% }

We also evaluate the models on L-Eval in Table~\ref{tab:leval}.
Note that, although L-Eval is a benchmark designed to assess long-form context understanding capabilities, its context lengths are relatively shorter, with an average length of 19K compared to 218K in InfiniteBench. 
Based on this, Llama, which can process up to 4K tokens, demonstrates strong performance on several tasks. 
When extended to capture longer contexts using various methods—namely ExtendedFA, AutoCompressor, and LCIRC—all models show improved average performance compared to the base model.
% \sumin{
Finally, our QD-LCIRC achieves the highest average performance on L-Eval as well, demonstrating significant gains through query dependent modeling, with a relative improvement of approximately 11.5\% compared to the best-performing baseline, ExtendedFA finetuned on FineWeb-LQA.
% }
% Finally, our QD-LCIRC achieves significant performance gains through query-dependent modeling, with a relative improvement of approximately 32\% compared to the best-performing baseline (ExtendedFA).

In Table~\ref{table:cos}, we compare the proposed selective state BPTT with truncated BPTT for QD-LCIRC on InfiniteBench, LongBench and L-Eval.
The results show that our selective state BPTT allows higher scores compared to truncated BPTT across all three benchmarks.
Note that truncated BPTT only backpropagates gradients to a limited number of timesteps in recurrence, restricting optimization for long-term context modeling.
In contrast, our selective state BPTT enables the model to receive gradients from any timesteps and in consequence, the trained model better models long inputs.


% We also conduct an ablation study on to investigate the impact of different BPTT. Table~\ref{table:cos} is the performance of the model trained with Truncated BPTT and Selective BPTT for long-form context benchmark: InfiniteBench, LongBench, and L-Eval. Between the models trained with Truncated BPTT and Selective State BPTT, the result of Selective BPTT is better than the other. The result shows that LCIRC can be optimized more effectively for modeling query-relevant information in long-form context through Selective State BPTT.

% \junyoung{
% \junyoung{We evaluate LCIRC and query-dependent context compression by comparing them against existing methods on long-context language modeling benchmarks like InfiniteBench, LongBench, and LEval.} \\
% To evaluate the effectiveness of our LCIRC, along with the query-dependent context compression approach, we conduct a comprehensive comparison against existing methodologies using a diverse set of tasks and benchmarks. 
% These tasks specifically target long-context language modeling and the comprehension of query-relevant information, with question answering serving as a key example. In this study, we utilize InfiniteBench, LongBench, and LEval as our benchmarks. Given the large number of subtasks, detailed descriptions of each are provided in the Appendix \ref{}.\\
% \junyoung{LCIRC demonstrates strong performance across benchmarks, consistently outperforming ExtendedFA on InfiniteBench and LongBench, while remaining competitive on LEval.} \\
% As shown in Table \ref{}, our LCIRC demonstrates strong performance across various benchmarks. Specifically, LCIRC achieves an average score of 13.33 on InfiniteBench, which represents an absolute improvement of 3.53 points over ExtendedFA. On LongBench, LCIRC scores 16.33, outperforming ExtendedFA by an absolute margin of 1.51 points. Moreover, LCIRC achieves competitive results on LEval, where it records an average score of 18.39, slightly behind ExtendedFA's 19.89 by only 1.5 points. These results underscore LCIRC's robust ability to process and comprehend long-context inputs while performing competitively with other state-of-the-art models.\\
% \junyoung{Query-Dependent LCIRC outperforms all baselines as well as LCIRC, with its largest gain on InfiniteBench, demonstrating its effectiveness in handling long-context inputs through query-dependent compression.} \\
% Query-Dependent LCIRC extends these improvements, consistently outperforming all baseline models across a range of challenging benchmarks. Specifically, it achieves an absolute improvement of 12.63 points on InfiniteBench, 5.76 points on LongBench, and 5.66 points on LEval compared to ExtendedFA. Notably, the largest improvement is observed on InfiniteBench, which has a significantly longer average context length compared to the other two benchmarks. These results emphasize the strength of our approach in handling long-context inputs and highlight the added benefit of query-dependent compression.}


% \subsection{Needle In A Haystack}