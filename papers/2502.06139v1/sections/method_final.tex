\section{Efficient Long-form Context Modeling}
In this section, we present an efficient approach to handle long-form inputs in pretrained LLMs. We introduce a method that enables models to process lengthy contexts by compressing and injecting the relevant information back into the model in a computationally efficient manner. Additionally, we outline the training strategy used to optimize the proposed components while maintaining the foundational capabilities of the model.


\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vs}{\mathbf{s}}

% \subsection{Preliminaries: Transformer Based LLMs}
% Modern LLMs adopts a transformer architecture which autoregressively estimates a probability for next tokens $x_{n+1:N}$ given previous tokens $x_{1:n}$ as context, \ie
% \begin{align}
%     P(x_{n+1:N}|x_{1:n})=\prod_{i=n+1}^{N} P(x_{i}|x_{1:i-1})
% \end{align}
% where $x_{j:k}$ represents a subsequence of tokens $(x_j,x_{j+1},\dots, x_{k-1},x_{k})$ and $x_{1:N}$ is the entire token sequence.
% For generation, the model samples a single token $x_i$ at each timestep based on the estimated distribution $P(x_i|x_{1:i-1})$ and the sampled token is appended to the input sequence for generating the next token $x_{i+1}$.

% In this autoregressive generation, modeling token order is important because it plays a key role in understanding language structure and meaning.
% Modern transformer-based LLMs learn $M$ position embeddings to capture token positions within a sequence.
% However, these learned position embeddings also impose a limit on the input token length that the model can process in a single input, as the number of position embeddings directly determines this limit.
% For this reason, it is technically infeasible for a transformer to process input sequence with its length exceeding the length limit ($N>M$).

% Incorporating long-form context into an LLM, however, is crucial for generating outputs that are grounded in information distributed across long spans of input text.
% For long-form context modeling, the entire pre-trained LLMs need to be re-trained to extend the maximum token length.
% Furthermore, the computational costs of LLMs increase quadratically with the context length due to their transformer architecture, making the processing of extended inputs highly expensive.

\subsection{Preliminaries: Transformer-Based LLMs}
Modern LLMs are built on the transformer architecture, where the model autoregressively estimates the probability of each subsequent token $x_{n+1:N}$ given the preceding tokens $x_{1:n}$, as formulated by:
\begin{align}
    P(x_{n+1:N}|x_{1:n})=\prod_{i=n+1}^{N} P(x_{i}|x_{1:i-1})
\end{align}
Here, $x_{j:k}$ refers to a subsequence of tokens $(x_j, x_{j+1}, \dots, x_{k})$, and $x_{1:N}$ denotes the full token sequence. During generation, the model samples one token at a time, appending it to the input to predict the next token.

Accurately modeling token order is essential in autoregressive generation because it enables the model to effectively capture the underlying structure and meaning of language. Transformer-based LLMs employ learned positional embeddings to represent token positions, but these embeddings impose a fixed input length, limiting the model’s ability to handle sequences longer than the maximum number of embeddings ($M$). Consequently, transformers are constrained in their capacity to process input sequences exceeding this limit ($N > M$).

Incorporating long-form context into LLMs is critical for generating outputs that remain grounded in extended input sequences. However, extending the input length requires full retraining of the model, and the computational cost scales quadratically with the context length, making long-form input processing computationally prohibitive in standard transformer architectures.







% \subsection{Long-form Context Injection with Recurrent Compression}
% To address the above limitations and enable pretrained LLMs to process lengthy inputs ($N \gg M$) with long-form context, we propose Long-form Context Injection with Recurrent Compression (\model{}), a novel method that extends pretrained LLMs for efficient long-form input processing. 
% A na\"ive approach to handling input that exceeds the length limit is to truncate the first $N-M$ tokens, completely cutting off access to the discarded context. Our method efficiently restores access to this lost context through recurrent context compression and compressed context injection, which are detailed in the following parts. 
% An overview of the proposed method is shown in Figure~\ref{fig:overview}.

% \subsubsection{Recurrent Context Compression}
% \label{subsubsection:RCC}
% This section first describes recurrent context compressor that efficiently reduces the excessively long context into a small sequence of embeddings.

% We first separate beginning of the prompts that exceeds context length $M$ which will be truncated otherwise without long-form injection.
% Let $x_{1:N-M}$ be denoted as $x_C$, the truncated context from the input sequence $x_{1:N}$.

% Note that since $N$ is often substantially larger than $M$, e.g., $N = 192\mathrm{K}$ in InfiniteBench~\cite{infinitebench} vs. $M = 4\mathrm{K}$ in Llama~\cite{llama2}, the length of $x_C$ may still be prohibitively large.
% For this reason, our compressor computes a compact feature sequence $(h_1,...,h_K)\in \vh$ from $x_C$, and its length is significantly shorter with $K\ll N-M$.

% We adopt the Perceiver architecture~\cite{perceiver, perceiverio} for building the compressor;
% it is structured by stacking Perceiver blocks, which consist of a cross-attention layer followed by a two-layered MLP with residual connections as depicted in Figure~\ref{fig:overview}.
% Note that a cross-attention layer operates by taking a set of query features and a set of input features, aggregating the projected input features into the query features~\cite{attention}.
% This mechanism allows the Perceiver to effectively compress a large sequence of features into a compact representation by using the compact sequence as the query features and the longer sequence as the input features.

% Specifically, we provide the token embeddings $\ve_C$ of $x_C$ as the input features\footnote{We use the initial token embeddings $\ve_C$ as inputs for compression, based on the empirical observation that the initial token embeddings show performances comparable to those with the last hidden states  while the efficiency is significantly improved.} and a short learnable query vectors $\vh^{(0)}$ of length $K$ as the query features to a Perceiver module and obtain a compressed features $\vh$ computed by
% \begin{align}
%     \vh = \mathrm{Perceiver}(\vh^{(0)}, \ve_C)
% \end{align}
% where $\mathrm{Perceiver}(q, x)$ is the Perceiver module with query features $q$ and input features $x$.

% However, compressing an extremely long context all at once is computationally challenging and significantly increases space complexity, and thus we introduce recurrence into the compression process.

% The long context $\ve_C$ is divided into $S$ disjoint segments $\vs_1, \dots, \vs_S$ where $\vs_i=\ve_{n_{i-1}+1:n_i}$ with $n_i$ as the last token index of $i$-th segment. 
% Note that we omit the sequence indices `$a\mathrm{:}b$' for segments for notational simplicity.
% The segments are fed to the Perceiver recurrently transferring compressed features as a hidden state.
% Specifically, a compressed feature for $i$-th segment $\vs_i$ is obtained by
% \begin{align}
%     \vh^{(i)}=\mathrm{Perceiver}(\vh^{(i-1)}, \vs_i) \label{eq:hidden_state}
% \end{align}

% The long context $\ve^{(0)}_{1:\tau}$ is divided into $S$ disjoint segments $\vs_1, \dots, \vs_S$ where $\vs_i=\ve^{(0)}_{n_{i-1}+1:n_i}$ with $n_i$ as the last token index of $i$-th segment. 
% Note that we omit the sequence indices `$a\mathrm{:}b$' for segments for notational simplicity.
% The segments are fed to the Perceiver recurrently transferring compressed features as a hidden state.
% Specifically, a compressed feature for $i$-th segment $\vs_i$ is obtained by
% \begin{align}
%     \vh^{(i)}=\mathrm{Perceiver}(\vh^{(i-1)}, \vs_i) \label{eq:hidden_state}
% \end{align}
% where $\vh^{(i)}$ compresses the segment of a larger length and is used as the hidden state transferred through the recurrence, and the initial hidden state $\vh^{(0)}$ is a set of learnable parameters.
% Note that $\vh^{(i)}$ does not only compresses the current segment $\vs_i$ but also contains information from previous segments thanks to the recurrence.
% Finally, we concatenate $\vh^{(i)}$ of all segments as the compressed representation of the entire long-form context, \ie $\vh=[\vh^{(1)},...,\vh^{(S)}]$ where $[\cdots]$ denotes a concatenation operation.



\subsection{Long-form Context Injection with Recurrent Compression}
To address the limitations of processing lengthy inputs ($N \gg M$) in pretrained LLMs, we propose Long-form Context Injection with Recurrent Compression (\model{}), an approach that enables efficient handling of long-form contexts. A simple truncation of the first $N-M$ tokens would discard essential contextual information, whereas our method restores access to this context through recurrent context compression and compressed context injection, detailed below. An overview of the proposed method is provided in Figure~\ref{fig:overview}.

\subsubsection{Recurrent Context Compression}
\label{subsubsection:RCC}
We introduce a recurrent context compression mechanism that effectively reduces the long-form context into a compact sequence of embeddings, which can be efficiently processed by the model.

Given an input sequence $x_{1:N}$, where $x_{1:N-M}$ represents the truncated context $x_C$, it is often the case that $N \gg M$, for example, $N = 192\mathrm{K}$ in InfiniteBench~\cite{infinitebench} compared to $M = 4\mathrm{K}$ in Llama~\cite{llama2}. To handle this, the recurrent compressor produces a compact feature sequence $(h_1, \dots, h_K) \in \vh$ from $x_C$, where $K \ll N-M$.

We employ the Perceiver architecture~\cite{perceiver, perceiverio} for the compressor, which consists of stacked Perceiver blocks. Each block includes a cross-attention layer followed by a two-layer MLP with residual connections (Figure~\ref{fig:overview}). The cross-attention mechanism aggregates input features based on query features, enabling efficient compression by using a compact sequence as the query and the longer context as the input features.

In particular, we use the token embeddings $\ve_C$ of the truncated context $x_C$ as the input features and a set of learnable query vectors $\vh^{(0)}$ of length $K$ as the query features. The compressed features $\vh$ are obtained through the Perceiver module as follows:
\begin{align}
    \vh = \mathrm{Perceiver}(\vh^{(0)}, \ve_C)
\end{align}
where $\mathrm{Perceiver}(q, x)$ represents the Perceiver module with query $q$ and input features $x$.

However, compressing such an extensive context in a single step is computationally expensive, thus we introduce a recurrent compression process. The long context $\ve_C$ is split into $S$ disjoint segments $\vs_1, \dots, \vs_S$, where $\vs_i = \ve_{n_{i-1}+1:n_i}$ represents the $i$-th segment. These segments are sequentially fed into the Perceiver module, with the compressed features from the previous segment serving as the query features for the next segment:
\begin{align}
    \vh^{(i)} = \mathrm{Perceiver}(\vh^{(i-1)}, \vs_i)
    \label{eq:vanillaRCC}
\end{align}
Here, $\vh^{(i)}$ compresses both the current segment $\vs_i$ and the cumulative information from all previous segments, enabled by the recurrent mechanism. The initial query vectors $\vh^{(0)}$ consists of learnable parameters.

Finally, the compressed representations $\vh^{(1)}, \dots, \vh^{(S)}$ of all segments are concatenated to form the overall compressed representation of the long-form context:
\begin{align}
    \vh = [\vh^{(1)}, \dots, \vh^{(S)}]
\end{align}
where $[\cdots]$ denotes concatenation. This recurrent approach ensures efficient long-form context representation, enabling LLMs to process extended inputs beyond their native length limitations.

% \subsubsection{Compressed Context Injection}
% \label{subsubsection:CCI}
% Once the compressed representation for the long-form context $h$ is obtained, we inject this compressed information into the pretrained transformer through gated cross-attention layers with a residual connection~\cite{flamingo}.
% Given the truncated input sequence of the length $M$, we obtain the embedding sequence $\ve^{(l)}_{\tau+1:N}$ at $l$-th transformer block.
% Then it is contextualized by
% \begin{align}
%     \hat{\ve}^{(l)}_{\tau+1:N}&=\alpha^{(l)} \cdot\mathrm{CA}(\ve^{(l)}_{\tau+1:N}, \vh)+\ve^{(l)}_{\tau+1:N} \\
%     \alpha^{(l)} & =\mathrm{tanh}(a^{(l)})
%     \label{eq:CCI}
% \end{align}
% where $\mathrm{CA}(q, x)$ is a cross attention layer with queries $q$ and key value inputs $x$.
% We pass $\ve^{(l-1)}_{\tau+1:N}$ to subsequent transformer block instead of $\hat{\ve}^{(l-1)}_{\tau+1:N}$.
% Note that $a^{(l)}$ is a learnable scalar parameter, 
% initialized to 0 to preserve the pretrained LLM's capabilities at the beginning of training. 
% Thanks to the compressed context representation the context injection is processed efficiently.

% \subsubsection{Training}
% To fully utilize the foundational capability of the pretrained LLM, we tune only the additional components with a corpus containing long-form texts.

% Given a long-form training text $x_{1:N}$, we train our model by minimizing the negative log-likelihood (NLL) loss.
% \begin{align}
%     \mathcal{L}&= -\frac{1}{N} \sum_{i=1}^{N}\mathrm{log}P(x_i|x_{1:i-1}).
%     \label{eq:training_objective}    
% \end{align}
% We randomly segment the long-form text with a maximum segment length $R$ and use the same segmentation to estimate $P(x_i|x_{1:i-1})$ for every $i$.
% To compute $P(x_i|x_{1:i-1})$, 
% previous segments are processed by the recurrent context compressor, while $x_{k:i-1}$, where $k$ is the first index of the current segment, is inputted as regular input to the LLM.

% Although our recurrent architecture allows memory efficient inference, the space complexity during training increases linearly with the input length $N$ using backpropagation through time (BPTT)~\cite{}.
% To overcome this challenge, we adopt the truncated BPTT~\cite{} where gradients are computed up to last $T$ segments.
% Note that, since gradient computation is not required for previous segments except for the $T$ last segments at each iteration, we cache compressed features $\vh_{1:K}^{(i)}$ and reuse them when predicting tokens in subsequent segments.


\subsubsection{Compressed Context Injection}
\label{subsubsection:CCI}
After obtaining the compressed representation $\vh$ for the long-form context, we inject this compressed information into the pretrained transformer using gated cross-attention layers with residual connections~\cite{flamingo}. For the truncated input sequence $x_{N-M+1:N}$ of length $M$, denoted as $x_{I}$
,we first compute the embedding sequence $\ve^{(l)}_{I}$ at the $l$-th transformer block. The embeddings are then contextualized through Gated Cross Attention Block (GCA Block in Figure \ref{fig:overview}) as follows:
\begin{align}
\begin{split}
    \Dot{\ve}^{(l)}_{I} &= \alpha^{(l)} \cdot \mathrm{CA}(\ve^{(l)}_{I}, \vh) + \ve^{(l)}_{I} \\
    \alpha^{(l)} &= \mathrm{tanh}(a^{(l)}) \\
    \Ddot{\ve}^{(l)}_{I} &= \beta^{(l)} \cdot \mathrm{MLP}(\Dot{\ve}^{(l)}_{I}) + \Dot{\ve}^{(l)}_{I} \\
    \beta^{(l)} &= \mathrm{tanh}(b^{(l)})
\end{split}
\label{eq:GCAB}
\end{align}
where $\mathrm{CA}(q, x)$ denotes the cross-attention layer with queries $q$ and key-value inputs $x$. Unlike standard transformer layers, we pass the modified embeddings $\Ddot{\ve}^{(l)}_{I}$ to the next transformer block instead of the original embeddings $\ve^{(l)}_{I}$. The scalar parameters $a^{(l)}$ and $b^{(l)}$ are learnable and initialized to 0, preserving the pretrained LLM’s performance at the start of training. The compressed context representation enables efficient context injection, minimizing computational overhead.

\subsubsection{Optimization Strategy}
\label{subsubsection:optimization}
To fully leverage the pretrained LLM's foundational capabilities, we optimize only the additional components using a corpus of long-form texts.

Given a long-form training sequence $x_{1:N}$, the model is trained by minimizing the negative log-likelihood (NLL) loss:
\begin{align}
    \mathcal{L} &= -\frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_{1:i-1}).
    \label{eq:training_objective}
\end{align}
The long-form input is randomly segmented, with each segment limited to a maximum length $R$. The probability $P(x_i | x_{1:i-1})$ is estimated within each segment, and prior segments are processed by the recurrent context compressor. The input $x_{k:i-1}$, where $k$ is the starting index of the current segment, is treated as the regular input for the LLM.

Although the recurrent architecture enables memory-efficient inference, the space complexity during training scales linearly with the input length $N$ due to backpropagation through time (BPTT). To mitigate this, we employ truncated BPTT, where gradient computation is restricted to the last $T$ segments. Since gradient calculation is unnecessary for earlier segments beyond the last $T$, we cache the compressed features $\vh$ and reuse them for predicting subsequent tokens within each segment.

\subsubsection{Inference}
\label{sec:inference}
% \sumin{
LLMs perform inference by conditioning on a given input prompt and generating a coherent and contextually relevant output sequence.
Given an input token sequence of length $N$ and an output token sequence of maximum length $P$, if their combined length remains within the context window $M$ of the underlying LLM ($N+P \le M$), the our recurrent compression mechanism is not activated, as LCIRC is specifically designed for long-form contexts.
In this case, the model operates in the same manner as the inference process of the pretrained LLMs.
In contrast, when the total maximum length of input and output sequences exceeds the context window of the LLM ($N+P > M$), two distinct cases must be considered.
When the maximum output length remains within the context window of the LLM ($P \le M$), the LLM can generate the entire output sequence in a single pass given the compressed input sequence by the proposed recurrent compression mechanism.
Conversely, if the maximum output length exceeds the context window ($P > M$), our model iteratively compresses the earlier portions of the generated tokens (e.g., the first ${M}/{2}$ tokens) by performing additional recurrent steps using the compression mechanism.
This process removes the compressed tokens from the context of the underlying LLM, thereby creating space for generating new next tokens while preserving coherence and continuity. 
Notably, this additional process remains highly efficient, as it only involves the compression steps while maintaining the original window size of the underlying LLM.
% If the maximum output length remains within the context window of the LLM ($P \le M$), the excess input tokens are recurrently compressed, after which the LLM generates the entire output sequence in a single step.
% the excess input tokens are segmented and compressed into features $\vh$ via recurrent compression. The output tokens are then generated by processing both the overall compressed features and the uncompressed input tokens within the context window using GCA blocks.
% Conversely, if the maximum output length exceeds the context window ($P > M$), our model compresses the earlier portion of the generated tokens (e.g., the first 2K tokens) by performing an additional recurrent step using the compression mechanism. This process removes the compressed tokens from the context of the underlying LLM, thereby creating space for generating new tokens while preserving coherence and continuity. Notably, this additional process remains highly efficient, as it only involves the compression step while maintaining a smaller context window size ($<$ 4K) for the underlying LLM.
% all input tokens undergo segmentation and recurrent compression. The output tokens are then generated up to the context limit.
% Subsequently, additional recurrent compression is applied to an earlier portion of the generated tokens, allowing for the iterative generation of new tokens from the compressed representation. This process continues until the entire sequence of output tokens has been produced.
% }

% \sumin{In contrast, when the input text exceeds the maximum context length of the LLM ($N > M$), the input tokens $x_C$ of length  $N-M$ are segmented and efficiently compressed into an overall compressed representation  $\vh$ through the Recurrent Context Compression mechanism, as described above.
% During this process, each segment is compressed to a fixed length of $K$, which corresponds to the length of $\vh^{(0)}$.
% Subsequently, the truncated input tokens $x_I$ of length $M$ and the compressed representation $\vh$ are processed through Compressed Context Injection, enabling inference over a long-form context of length $N(>M)$.
% This design ensures that LCIRC preserves the original generalization capabilities of the underlying LLM for shorter input sequences while simultaneously enabling effective processing of long-form contexts.}
% \sumin{LCIRC is applicable in scenarios involving multi-turn conversations or when the length of generated tokens exceeds the maximum context length of the pretrained LLM.
% If the length of a multi-turn conversation or the generated tokens surpasses the LLM’s maximum context length, LCIRC performs an additional recurrent compression step to compress the earlier portion of these tokens.
% This process allows the model to process additional context equivalent to the length of the compressed tokens while preserving coherence and continuity.
% By continuously applying the Recurrent Context Compression mechanism, LCIRC maintains computational efficiency and effectively models context, even in cases where multi-turn conversations or generated token sequences are exceptionally long.}


% \begin{figure}[t]
%     \centering  
%     \begin{subfigure}{.47\linewidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/RCC.pdf}
%         % \subcaption{Recurrent compression module}
%     \end{subfigure}\hspace{0.3cm}
%     \begin{subfigure}{.47\linewidth}
%         \centering
%         \includegraphics[width=1\linewidth]{figures/QDRCC.pdf}
%         % \subcaption{Query dependent RCC}
%     \end{subfigure}
%     \caption{\textbf{Comparison of the recurrent context compression module with and without query dependent modeling.} 
%     In addition to the regular context compression module (left), we add additional cross attention module (blue box) to inject query information into the compressed feature $\vh^{(i-1)}$ (right).}
%     % Recurrent Context Compression (RCC), referred to as (a), and the query dependent RCC, denoted as (b), are distinguished accordingly. The distinguishing feature of this approach is the integration of the user query, which is used to generate the query vector for the Perceiver module. This process conditions the compression on both the recurrent hidden state, $\vh^{(i-1)}$, and the user query embedding, $\ve_{\mathrm{query}}$. Gated MLPs and gated cross-attention mechanisms are employed to construct the query vector, which is then passed to the Perceiver module for further processing.}
%     \label{fig:QDRCC}
% \end{figure}

% \begin{figure*}[t]
%     \centering  
%     \includegraphics[width=\linewidth]{figures/BPTT.pdf}
%     \caption{
%     \textbf{Comparisons of the proposed Selective State BPTT with vanilla and truncated BPTT.}
%     Green boxes represent timesteps where gradients are computed in BPTT whereas the light green ones indicate the timesteps without gradient computation. 
%     Finally, dotted red lines illustrate the gradient flows.
%     (a) Vanilla BPTT computes the full gradients through the entire timesteps in recurrence but is computationally infeasible with a large $N$. The gradients for $\vh^{(i)}$ receives upstream gradients both through the recurrent connection and through the direct connection from $\vh$. 
%     (b) Truncated BPTT backprobagates gradients to the last $T$ timesteps only significantly reducing computational costs.
%     However, it does not transfer gradient flows to timesteps further than $T$ (marked with light green color) and fails to learn long-term QD modeling.
%     (c) Our proposed Selective State BPTT selects several random timesteps and transfer gradient flows directly through the direct connection from $\vh$, which enables efficient learning of long-term QD modeling capabilities.
%     }
%     \label{fig:bptt}
% \end{figure*}


% \section{Query Dependent Context Modeling}
% LLMs are often tasked with processing context in response to instructions or questions.
% Therefore, we further extend our method to understand long-form context based on the query; this is achieved through query dependent context compression.
% Unlike the vanilla context compressor where the entire information within the current segment $\vs_i$ is merged into the compressed features $\vh^{(i-1)}_{1:K}$ in Eq.~\eqref{eq:hidden_state}, 
% the query dependent compression selectively adds information that is closely relevant to the query.

% We achieve that by incorporating an additional cross attention layer to our compressor.
% \phseo{As Figure~\ref{} shows, the ...write this part once the figure is added.}
% \sumin{
% At each step of the Recurrent Context Compression, the learnable latent queries or the previously compressed features, which is used as query feature for Perceiver, are transformed into query dependent input query through the newly added gated cross attention layer before being fed into Perceiver.
% The newly added gated cross attention layer receives the learnable latent queries or the previous compressed features $\vh^{(i-1)}_{1:K}$ as the query, and user query $\ve_{\mathrm{query}}$ as the key-value, \junyoung{Edit} generating query dependent query $\Dot{\vh}^{(i-1)}_{1:K}$ as follows:
% \begin{align}
%     \Dot{\vh}^{(i-1)}_{1:K}&=\alpha \cdot\mathrm{CA}(\vh^{(i-1)}_{1:K}, \ve_{\mathrm{query}})+\vh^{(i-1)}_{1:K} \\
%     \alpha & =\mathrm{tanh}(a)
% \end{align}
% After that, the query dependent compressed feature $\Dot{\vh}^{(i)}_{1:K}$ is generated by the same manner as before except for using query dependent query $\Dot{\vh}^{(i-1)}_{1:K}$ as follows.
% \begin{align}
%     \vh^{(i)}_{1:K}=\mathrm{Perceiver}(\Dot{\vh}^{(i-1)}_{1:K}, \vs_i)
% \end{align}
% Finally, we can inference using these query dependent compressed features $\vh$ as same as Eq. \eqref{eq:CCI}.
% }
% \subsection{Training}
% Query Dependent LCIRC is a model that adds a single gated cross attention layer for query dependency to pre-trained LCIRC.
% Given the query tokens $x_{\mathrm{query}}$, the following objective function is minimized to train Query Dependent LCIRC.
% \begin{align}
%     \mathcal{L}&= -\frac{1}{N} \sum_{i=1}^{N}\mathrm{log}P(x_i|x_{1:i-1}, x_{\mathrm{query}}).
% \end{align}
% As shown in Figure \ref{fig:bptt}, we adopt Random Selective BPTT to train Query Dependent LCIRC efficiently.
% Through Random Selective BPTT, LCIRC can learn query dependent compression for long-form context and at the same time, LCIRC can be trained more efficiently than Vanilla BPTT.


\section{Query Dependent Context Modeling}
LLMs frequently process context based on specific instructions or queries. To enhance the ability of LLMs to handle long-form context, we extend our method to incorporate query dependent context compression. This allows the model to selectively focus on the context most relevant to the given query, thus improving the overall efficiency and relevance of the model's responses.

Unlike the vanilla recurrent context compression, which merges all the information in the current segment $\vs_i$ into the compressed features $\vh^{(i)}$ as shown in Eq.~\eqref{eq:vanillaRCC}, query dependent compression selectively injects information that is most relevant to the user query. This selective compression is achieved through the addition of a Gated Cross Attention Block to our recurrent context compression method as shown in Figure~\ref{fig:QDRCC}.

\begin{figure}[t]
    \centering  
    \begin{subfigure}{.47\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/RCC.pdf}
        % \subcaption{Recurrent compression module}
    \end{subfigure}\hspace{0.3cm}
    \begin{subfigure}{.47\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{figures/QDRCC.pdf}
        % \subcaption{Query dependent RCC}
    \end{subfigure}
    \caption{\textbf{Comparison of the recurrent context compression module with and without query dependent modeling.} 
    In addition to the regular context compression module (left), we add additional cross attention module (blue box) to inject query information into the compressed feature $\vh^{(i-1)}$ (right).}
    % Recurrent Context Compression (RCC), referred to as (a), and the query dependent RCC, denoted as (b), are distinguished accordingly. The distinguishing feature of this approach is the integration of the user query, which is used to generate the query vector for the Perceiver module. This process conditions the compression on both the recurrent hidden state, $\vh^{(i-1)}$, and the user query embedding, $\ve_{\mathrm{query}}$. Gated MLPs and gated cross-attention mechanisms are employed to construct the query vector, which is then passed to the Perceiver module for further processing.}
    \label{fig:QDRCC}
\end{figure}

\begin{figure*}[t]
    \centering  
    \includegraphics[width=\linewidth]{figures/BPTT.pdf}
    \caption{
    \textbf{Comparisons of the proposed Selective State BPTT with vanilla and truncated BPTT.}
    Green boxes represent timesteps where gradients are computed in BPTT whereas the light green ones indicate the timesteps without gradient computation. 
    Finally, dotted red lines illustrate the gradient flows.
    (a) Vanilla BPTT computes the full gradients through the entire timesteps in recurrence but is computationally infeasible with a large $N$. The gradients for $\vh^{(i)}$ receives upstream gradients both through the recurrent connection and through the direct connection from $\vh$. 
    (b) Truncated BPTT backprobagates gradients to the last $T$ timesteps only significantly reducing computational costs.
    However, it does not transfer gradient flows to timesteps further than $T$ (marked with light green color) and fails to learn long-term QD modeling.
    (c) Our proposed Selective State BPTT selects several random timesteps and transfer gradient flows directly through the direct connection from $\vh$, which enables efficient learning of long-term QD modeling capabilities.
    }
    \label{fig:bptt}
\end{figure*}

As illustrated in Figure~\ref{fig:QDRCC}, the model integrates the user query into the compression pipeline. At each compression step, the learnable query vectors or the previously compressed features $\vh^{(i-1)}$ used as the query features for the Perceiver module are transformed into a query dependent representation. This is done through the newly introduced gated cross attention block, which takes $\vh^{(i-1)}$ as the query features, and the user query embedding $\ve_{\mathrm{query}}$ as the input features. The query dependent features $\Ddot{\vh}^{(i-1)}$ are computed through the same process in Eq.~\eqref{eq:GCAB} with $\vh^{(i-1)}$ and $\ve_{\mathrm{query}}$.

The query dependent compressed feature $\vh^{(i)}$ is then produced using the Perceiver module, with the query dependent feature $\Ddot{\vh}^{(i-1)}$ and the segment embeddings $\vs_i$ through the same process in Eq.~\eqref{eq:vanillaRCC}.
Subsequently, these query dependent compressed features $\vh$ are then used during inference as described in Eq.~\eqref{eq:GCAB}, enabling the model to focus on information relevant to the query while handling long-form inputs.

\paragraph{Training and Efficiency Enhancements}
Query Dependent LCIRC (QD-LCIRC) builds upon the pre-trained LCIRC architecture by adding a gated cross-attention layer to introduce query dependency. To train QD-LCIRC, we minimize the following negative log-likelihood (NLL) loss:
\begin{align}
    \mathcal{L} &= -\frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_{1:i-1}, x_{\mathrm{query}})
\end{align}
This objective ensures that the model learns to predict each token in the input sequence conditioned on both the preceding tokens and the query, facilitating query dependent context modeling.

As shown in Figure~\ref{fig:bptt}, we employ Random Selective BPTT to train the QD-LCIRC efficiently. Unlike vanilla BPTT, which computes gradients across all timesteps, or truncated BPTT, which only computes gradients for the last $T$ timesteps, Random Selective BPTT randomly selects a subset of timesteps for gradient computation. This allows the model to efficiently learn long-term query dependent context modeling without excessive computational overhead. Additionally, we cache the compressed features $\vh$ from earlier segments, further reducing the memory and computational requirements for training.

\paragraph{Inference}
% \sumin{
The only distinction between QD-LCIRC and LCIRC lies in the incorporation of query dependent compression, facilitated by the GCA block, within the recurrent compression step. Consequently, the inference process of QD-LCIRC differs from that of LCIRC only by the addition of query dependent modeling through an extra GCA block in the recurrent compression step, as demonstrated in Section~\ref{sec:inference} and Figure~\ref{fig:QDRCC}.
% }

% \sumin{QD-LCIRC introduces a Gated Cross Attention Block within the Recurrent Context Compression process of LCIRC to enhance the efficiency of query dependent compression.
% As a result, the inference process of QD-LCIRC remains consistent with that of LCIRC, as detailed in Section~\ref{sec:inference}, with the sole addition of a query dependent modeling step.
% For input texts shorter than the maximum context length of the underlying LLM ($N < M$), inference is performed in the same manner as in the pretrained LLM.
% For input texts that exceed the maximum context length of the LLM ($N > M$), as depicted in Figure~\ref{fig:QDRCC}, the input tokens  $x_C$  of length  $N-M$  undergo an additional processing step through the Gated Cross Attention Block before being compressed by the Perceiver within the Recurrent Context Compression mechanism.
% Subsequently, the Compressed Context Injection process proceeds in the same manner as in LCIRC.
% QD-LCIRC facilitates recurrent and query dependent compression, thereby enabling more focused modeling of long-form contexts based on the user query.}
