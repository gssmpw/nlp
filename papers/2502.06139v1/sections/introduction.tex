\section{Introduction}
% \junyoung{Recent advancements in large language models (LLMs) have shown strong language understanding and reasoning across various tasks, largely due to techniques like in-context learning, chain-of-thought prompting, and retrieval augmentation. While these methods improve performance, they struggle with tasks requiring long-context inputs, such as processing lengthy documents or multi-hop question answering. LLMs like LLaMA, constrained by fixed-length position embeddings, truncate earlier input when the context window is exceeded, limiting their effectiveness on long-form content. Additionally, processing longer sequences increases computational cost quadratically, making simple context window extensions impractical. While models exist to handle longer contexts, they often increase memory usage and slow down inference, making them less efficient for real-time or resource-constrained situations.
% }


Large language models (LLMs) have demonstrated remarkable capabilities in generating coherent and contextually rich outputs~\cite{zhao2023survey,wang2024beyond}. However, a major limitation remains in their ability to handle long-form contexts efficiently~\cite{liu2024lost,li2024long}. Transformer-based architectures, which underlie most LLMs, are constrained by fixed-length position embeddings, limiting the number of tokens they can process in a single pass~\cite{chen2021simple,lin2022survey}. Once this limit is exceeded, crucial information from earlier parts of the input is truncated, leading to reduced performance in tasks requiring extended context, such as document summarization~\cite{koh2022empirical} or long-form question answering~\cite{fan2019eli5}. Also, the computational cost of processing longer input sequences grows quadratically with sequence length, making it impractical to simply increase the context window~\cite{fournier2023practical,chen2023extending}.

To address these challenges, we propose Long-form Context Injection with Recurrent Compression (\model), a method designed to extend LLM's ability to process long-form inputs efficiently. LCIRC compresses the context beyond the model's length limit into compact representations, which are injected back into the model, allowing it to retain essential information without retraining the entire model. By recurrently compressing the input sequence, our approach maintains a balance between preserving context and minimizing computational overhead, enabling the model to generate outputs grounded in long-span contexts~\cite{valmeekam2023llmzip,chen2024melodi,lester2024training}.

In many real-world applications, LLMs need to process input in response to specific queries or instructions~\cite{ouyang2022training,naveed2023comprehensive}. Compressing all available information indiscriminately can lead to irrelevant data being retained, reducing the modelâ€™s effectiveness~\cite{mulc2024compressing,franceschelli2024training}. To address this, we introduce query dependent context modeling, which selectively compresses information based on its relevance to the query. This ensures that the model focuses on the most pertinent parts of the input, enhancing its performance in tasks like multiple choice and long-form question answering, where query relevance is crucial.

Our results show that \model, combined with query dependent modeling, significantly improves the ability of LLMs to handle long-form contexts in a scalable and efficient manner, making it well-suited for applications that require both extensive context understanding and precise query relevance.

In summary, our primary contributions are as follows: (1) We propose LCIRC, a method that extends the context window of LLMs through recurrent compression, allowing efficient handling of long-form inputs without retraining the entire model. (2) We introduce query dependent context modeling, which selectively compresses query-relevant information, enhancing performance in tasks that require the comprehensive understanding of extended contexts. (3) Our approach significantly improves the efficiency of LLMs in long-form contexts, reducing computational costs while demonstrating performance improvements quantitatively through various benchmarks.



% \sumin{
% In summary, our primary contributions are as follows:
% (1) We introduce LCIRC, a method that extends the context window of LLMs indefinitely, while maintaining their strong reasoning capabilities and minimizing computational overhead through recurrent compression. In particular, LCIRC achieves a 99\% reduction in computational cost relative to baseline model.
% (2) We further propose Query-Dependent Context Modeling for the efficient compression of query-relevant information within comprehensive long-form contexts. Additionally, we introduce Selective State BPTT to enhance the learning efficiency of Query-Dependent Context Modeling.
% (3) In long-form contexts, Query-Dependent LCIRC demonstrates superiority in language modeling and real-world applications such as question answering and multiple choice tasks, while requiring very low computational costs.
% }

% Our contributions are threefold: 
% \begin{itemize}
%     % \item \junyoung{We present LCIRC, which extends LLMs' context length indefinitely while preserving performance and minimizing computational costs through recurrence.}
%     \item \junyoung{We introduce Query-Dependent Context Modeling, a method that enhances LLM functionality by efficiently compressing query-relevant information in real-world applications.}
%     \item \junyoung{Our method allows LLMs to comprehend information beyond predefined context limits, a previously unattainable capability, as demonstrated by our experiments.}
% \end{itemize}
