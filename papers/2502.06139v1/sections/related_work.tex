\section{Related Work}

\begin{figure*}[t]
    \centering  
    \includegraphics[width=0.95\linewidth]{figures/LCIRC.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{The overall process of the proposed Long-form Context Injection with Recurrent Compression (LCIRC)}. LCIRC comprises two components: Recurrent Context Compression (left) and Compressed Context Injection (right). In the $i$-th step of Recurrent Context Compression, the previously compressed features $\mathbf{h}^{(i-1)}$ and the segment embeddings $\mathbf{s}_{i}$ are fed into the Perceiver module as query and input features, respectively. The compressed features $\mathbf{h}^{(i)}$ are then generated and reinjected as query features for the subsequent recurrence step. The initial query features $\mathbf{h}^{(0)}$ are learnable parameters. In Compressed Context Injection, the concatenated compressed features $\mathbf{h}$ serve as input to the Gated Cross Attention layer. Layers indicated with a fire symbol represent trained layers, while layers marked with a snow symbol denote frozen layers.}
    \vspace{-0.3cm}

    \label{fig:overview}
\end{figure*}

% \paragraph{Recurrent Transformers}
% Several attempts have been made to extend transformer-based language models to handle longer input sequences using recurrence mechanisms. These approaches split input into segments, storing and reusing information recurrently. Transformer-XL \cite{transformer-xl} and Compressive Transformer \cite{compressive} apply segment-level recurrence, while Recurrent Memory Transformer \cite{rmt} and Memformer \cite{memformer} use memory modules for past information. Recurrent Attention Network \cite{RAN} and Segmented Recurrent Transformer \cite{SRformer} aggregates information across segments with recurrent attention. However, they require full model re-pretraining, which is resource-intensive.

% \paragraph{Sparse Attention}
% As the length of the input sequence grows, the computational cost associated with full attention increases quadratically. To address this challenge, sparse attention mechanisms have been introduced as a more efficient solution for managing long contexts. Several notable works, including Sparse Transformer \cite{sparse-transformer}, Longformer \cite{longformer}, Reformer \cite{reformer}, Big Bird \cite{bigbird}, and Routing Transformer \cite{routing-transformer}, have incorporated sparse attention to reduce computational overhead. However, despite these efficiency gains, sparse attention inherently leads to partial information loss, which ultimately limits its ability to achieve performance on par with full attention mechanisms.

% \paragraph{Prompt Compression}
% Prompt compression techniques aim to alleviate the computational challenges posed by processing large prompts by creating more compact representations. For example, Gisting \cite{gist} compresses the entire user instruction into "gist tokens" all at once. However, these approaches are generally optimized for shorter contexts, limiting its ability to handle contexts exceeding the window size of the backbone LLM. ICAE \cite{icae} and AutoCompressor \cite{autocompressor} alleviate this issue by segmenting the long context into smaller chunks and compressing each segment individually. While these approaches represent progress, they do not demonstrate compression capabilities for sequences comprising hundreds of thousands of tokens, and still have limitations in inference cost and context window by performing inference and compression simultaneously with LLMs.


\paragraph{Recurrent Transformers}
Various methods have been explored to extend the capacity of transformer-based language models for handling longer input sequences through the use of recurrence mechanisms. These approaches typically divide the input into segments and recurrently store and reuse information across them. Transformer-XL~\cite{transformer-xl} and Compressive Transformer~\cite{compressive} employ segment-level recurrence, while models such as Recurrent Memory Transformer~\cite{rmt} and Memformer~\cite{memformer} utilize memory modules to retain past information. Additionally, Recurrent Attention Network (RAN)~\cite{RAN} and Segmented Recurrent Transformer (SRformer)~\cite{SRformer} integrate recurrent attention to aggregate information across segments. Despite these advancements, a common limitation of these methods is the need for full model re-pretraining, which is computationally costly and resource-intensive.

\paragraph{Sparse Attention}
As input sequence lengths increase, the computational overhead associated with full attention mechanisms grows quadratically. Sparse attention mechanisms have been proposed as an efficient alternative to mitigate this computational burden. Notable models such as Sparse Transformer~\cite{sparse-transformer}, Longformer~\cite{longformer}, Reformer~\cite{reformer}, Big Bird~\cite{bigbird}, Routing Transformer~\cite{routing-transformer}, and MInference~\cite{minference} implement sparse attention to reduce computational costs for longer contexts.
While these methods improve efficiency, they do so at the cost of partial information loss, resulting in performance that often falls short of full attention mechanisms, and they are still constrained by the limited context length of LLMs, making it challenging to process long-form sequences.
%inputs that exceed this boundary.
% }

\paragraph{Prompt Compression}
Prompt compression techniques aim to reduce the computational challenges of processing large prompts by creating more compact representations. Gisting~\cite{gist} compresses the entire user instruction into a set of "gist tokens" in one step, providing a compact representation of the prompt. However, this approach is typically optimized for shorter contexts, limiting its effectiveness when handling inputs that exceed the context window of the underlying LLM.
% \sumin{
Additionally, several studies have explored 
% methods such as
KV cache compression~\cite{dmc, pyramidkv} to enhance the efficiency of long-form context inference. 
While these approaches improve computational efficiency, they are primarily designed to optimize context modeling within the context length of the underlying LLM and have limited capability in extending the sequence length.
% As a result, they remain constrained by the modelâ€™s predefined context window and struggle to process inputs that significantly exceed this limit.
% }
ICAE~\cite{icae} and AutoCompressor~\cite{autocompressor} address this limitation by segmenting long contexts into smaller chunks and compressing each individually. Although these methods represent progress, they have yet to demonstrate compression capabilities for extremely long sequences, such as those involving hundreds of thousands of tokens, and still face limitations in terms of inference costs and context window size due to the simultaneous inference and compression processes.
