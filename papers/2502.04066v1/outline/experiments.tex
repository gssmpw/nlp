
\input{figures/eval_data}
\input{figures/cooccur_frequency}

\input{figures/mmlu}
\input{tables/main_table}

\input{figures/main_figures}
\input{figures/smi_figures}

% \input{tables/case_study}


\section{Experiments}
\label{experiments}
We first complete pre-training and achieve results comparable to the Llama2 series~\cite{DBLP:journals/corr/abs-2307-09288} on benchmarks such as GSM8K~\cite{cobbe2021gsm8k}, MMLU~\cite{DBLP:conf/iclr/HendrycksBBZMSS21}, C-Eval~\cite{huang2023ceval}, and GaoKao~\cite{DBLP:journals/corr/abs-2305-12474}. Subsequently, we conduct evaluations on four LLMs: our 1.6B, 7B, and 13B models, as well as the open-source model TinyLlama-1.1B-intermediate-step-1431k-3T~\cite{DBLP:journals/corr/abs-2401-02385}, which is pre-trained on a subset of our pre-training data.


\subsection{Evaluation Setup}
\label{evaluation_setup}

\paragraph{Evaluation data construction.} 
We make use of the high-quality knowledge triple dataset, Pararel~\cite{DBLP:journals/tacl/ElazarKRRHSG21}, which is commonly employed to assess the alignment of pre-trained language models with factual knowledge. To adapt this dataset for evaluating decode-only structured LLMs, we filter out templates in Pararel where the object label does not appear at the end of the sentence. Additionally, we select 15 knowledge triple relations that have larger data volumes and clearer semantics. This refinement results in an evaluation set comprising 12,468 knowledge triples, as detailed in Figure~\ref{eval_data}.

\paragraph{Pre-training data retrieval.} 
We divide the pre-training data into 2.3 billion paragraphs. For each knowledge triple and paragraph, we retrieve the occurrence frequency of the subject, the occurrence frequency of the object, and their co-occurrence frequency. Using the retrieval results across all paragraphs, we compute the co-occurrence, MI, and SMI metrics for each knowledge triple. The data retrieval process is conducted using 600 CPUs and takes approximately one week to complete.

\paragraph{Evaluation settings.} 
During the evaluation process, we set the temperature to 0.7, the maximum number of new tokens to 32, and the precision to bfloat16. For a modelâ€™s response $o$, we consider $o$ a valid instance of the correct answer set \(\mathbf{O}\) if the object appears in $o$. To ensure stable evaluation, we perform inference using each of the 20 templates on every knowledge triple 20 times, averaging the ACC over the resulting 400 inferences.

The retrieval results from the pre-training data reveal a highly uneven co-occurrence frequency distribution of knowledge triples (Figure~\ref{cooccur_frequency}). Nearly half of the data exhibit co-occurrence frequencies between $2^5$ and $2^{10}$, while knowledge triples with high co-occurrence frequencies are rare. To address this imbalance and effectively evaluate data with varying frequencies, we group knowledge triples that have similar metric values. For the co-occurrence and MI metrics, we group the data into intervals of 0.2 (after logarithm and before normalization) and calculate the average ACC for each group. The SMI metric is then derived from the MI metric.

We perform linear regression on the data to calculate the $\text{R}^2$ value. To evaluate the predictive performance of the regression equation for individual knowledge triples, we compute the mean squared error (MSE) for each knowledge triple and then average the results.


% \input{figures/mmlu}

\subsection{Experimental Results}
\label{experimental_results}

\paragraph{Results of pre-training.}
We utilize a total of 112 A100 GPUs for pre-training. The 1.6B model is trained over two weeks, while the 7B model requires two months. For the 13B model, the memory of a single GPU is insufficient, so we employ Tensor Parallelism, with each pair of GPUs handling a model replica. Training the 13B model takes approximately four months.

Our models demonstrate strong performance across the GSM8K, MMLU, C-Eval, and Gaokao benchmarks (Figure~\ref{mmlu}). Notably, our models significantly outperform the Llama2 series models of the same sizes on the C-Eval and Gaokao benchmarks. This can be attributed to our pre-training data, which includes high-quality Chinese data closely aligned with the LLM capabilities.

However, on the GSM8K math benchmark and the MMLU multi-task benchmark, our models underperform the Llama2 series models of the same sizes. This discrepancy may be due to the Llama2 series upsampling the most factual data sources, thereby enhancing knowledge retention and reducing hallucinations~\cite{DBLP:journals/corr/abs-2307-09288}.

% \input{tables/main_table}
% \input{figures/main_figures}
% \input{figures/smi_figures}

\paragraph{Results of LLM capabilities prediction}
The main results are presented in Figure~\ref{main_figures} and Table~\ref{main_table}. Generally, we can find that: (1) The SMI metric demonstrates strong performance on the evaluation set containing 12,468 knowledge triples, achieving $\text{R}^2$ values greater than 0.84 across four different LLM sizes. This highlights a robust linear relationship, effectively predicting the ACC (Figure~\ref{smi_figures}). (2) The SMI metric achieves an MSE of less than 0.06 across all four LLM sizes, indicating its ability to accurately predict not only overall knowledge retention levels but also individual knowledge triples. (3) The MI metric outperforms the baseline co-occurrence metric, while the SMI metric further surpasses the MI metric, demonstrating the effectiveness of our progressively improved approach.

The advantage of the SMI metric over the MI metric is more pronounced in smaller models, becoming less significant as model size increased. For the 1.6B model, the $\text{R}^2$ value differs by 0.053, and the MSE differs by 0.002 between the two metrics. In contrast, for the 13B model, the $\text{R}^2$ value differs by just 0.002, and the MSE differs by less than 0.001. This suggests that larger models may already possess sufficient knowledge retention capabilities, reducing the impact of further size increases.

% \paragraph{Case study}
% Four counterintuitive cases are highlighted in Table \ref{case_study}. We can observe that: (1) In the first two cases, the subject and object rarely co-occur, suggesting a seemingly weak relationship. However, whenever the subject appears, the object almost always appears as well, resulting in high accuracy. (2) In the last two cases, despite frequent co-occurrence between the subject and object, their individual occurrence frequencies are significantly higher. This leads to lower accuracy. For instance, given ``Paris is located in'', the model is more likely to respond with ``France'' rather than ``Europe''.

% These examples underscore the critical role of knowledge specificity in the memory of LLMs. It is not merely the frequency of knowledge occurrence that enhances knowledge retention, but the specificity of knowledge that proves to be more influential. This insight suggests that in downstream task training, even if the domain-specific data is much smaller than the pre-training data, reducing redundancy with the pre-training data can yield a higher SMI, making it easier for LLMs to retain the relevant knowledge.


