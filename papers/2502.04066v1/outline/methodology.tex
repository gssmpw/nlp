

\input{figures/pretraining_data}

\input{tables/model_architecture}


\section{Methodology}
\label{methodology}
To address the three challenges outlined in Section \ref{introduction} and develop a method to predict LLM capabilities prior to pre-training, we meticulously construct 1.5 trillion tokens of high-quality pre-training data and train base models at three different sizes. We then utilize a multi-template complementation mechanism to accurately evaluate the model’s knowledge retention capbility on CBQA tasks. Furthermore, we retrieve these knowledge triples from the pre-training data, introduce the SMI metric and establish a predictive equation that maps SMI to the ACC on CBQA tasks using linear regression. Finally, we compute $\text{R}^2$ and MSE to assess the predictive performance.


\subsection{LLM Pre-training}
\label{3-1}

\paragraph{Pre-training data.} 
Our pre-training data is composed of four main categories: English, Chinese, multilingual, and code. We utilize four high-quality and widely used open-source datasets: Falcon RefinedWeb~\cite{refinedweb}, SlimPajama~\cite{cerebras2023slimpajama}, Wikipedia~\cite{wikidump}, and Starcoderdata~\cite{DBLP:journals/tmlr/LiAZMKMMALCLZZW23}. For Chinese data, we collect diverse content from the internet, including blogs, books, chats, encyclopedias, and other categories.

To ensure data quality, we implement a rigorous cleaning process. First, during data filtering, we remove short texts, special characters, and texts lacking punctuation, and we use Toxigen~\cite{hartvigsen2022toxigen} to filter out toxic content. Second, for data deduplication, we apply Locality-Sensitive Hashing to eliminate duplicate documents and employ strict matching techniques to remove duplicate sentences. Finally, for data inspection, we sample 1\% of the data and use the Qwen2-1.5B~\cite{DBLP:journals/corr/abs-2407-10671} to calculate the loss, followed by manual review of the high-loss data to ensure it met our quality standards. The detailed composition of our pre-training data is provided in Figure~\ref{pre-training_data}.

\paragraph{Model architectures.} 
Our models are built on Transformer architectures similar to Llama2 (Table~\ref{model_architecture}). The 1.6B model consists of 24 layers with a maximum sequence length of 2048 tokens. The 7B model features 32 layers with a maximum sequence length of 4096 tokens, while the 13B model includes 40 layers, also supporting a maximum sequence length of 4096 tokens. We train the tokenizer using ten thousand tokens from minority languages, leveraging the Wikipedia dataset and the Byte Pair Encoding method. These tokens are then integrated with the tokenizer from InternLM~\cite{DBLP:journals/corr/abs-2403-17297}.


\subsection{Evaluating LLM capabilities}
\label{3-2}

\paragraph{Knowledge triples.}
We center our research on the ability of LLMs to memorize knowledge triples. A knowledge triple is represented as a tuple $t = (s, r, o)$, where $s$ is the subject, $r$ is the relation, and $o$ is the object \cite{DBLP:conf/acl/JuCY0DZL24}. For a given LLM, denoted as $F$, we define $F$ as mastering the knowledge triple $t$ if the following condition holds:

\begin{equation}
    F(q_{s,r}) = o, \quad (q \in \mathbf{Q}, o \in \mathbf{O}).
\end{equation}

Here, \(q_{s,r}\) represents the combined representation of the subject \(s\) and the relation \(r\), while \(o\) denotes the representation of the object. For instance, if \(s = \text{Apple}\) and \(r = \text{headquarter}\), then \(q_{s,r}\) can be expressed as statements like ``The headquarters of Apple is in...'', ``Apple is headquartered in...'', or ``Apple's head office is based in...''. Similarly, \(o\) corresponds to the object representation, such as ``Cupertino'', ``Cupertino, California'', or ``Cupertino city''. The sum of all \(q_{s,r}\) representations is denoted as \(\mathbf{Q}\), and the sum of all \(o\) representations is denoted as \(\mathbf{O}\).

\paragraph{Multi-template complementation mechanism.}
Accurately evaluating LLMs’ memory of knowledge is challenging due to the diverse range of queries formed by the subject and relation of a knowledge triple. To address this, we implement a multi-template complementation mechanism, which has demonstrated strong performance in CBQA tasks~\cite{DBLP:journals/corr/abs-2409-15825}. Experimental results indicate that memory levels assessed using this mechanism closely align with the model’s actual CBQA task performance. Moreover, the distribution of memory levels is strongly correlated with the model’s performance after fine-tuning, further validating the effectiveness of the multi-template complementation mechanism in evaluating LLMs’ knowledge retention.

Specifically, we generate a large set of query templates that are semantically similar but vary in form. From this set, we selecte 20 templates with diverse lengths and structures for each type of knowledge triple. Each template $q$ represents a specific instance within the query set \(\mathbf{Q}\), and the 20 selected $q$’s are used to approximate the entire \(\mathbf{Q}\).


% \input{figures/eval_data}
% \input{figures/cooccur_frequency}

\subsection{Predicting LLM capabilities}
\label{3-3}

\paragraph{MI metric.}
We are committed to investigating the relationship between the capabilities of LLMs and their pre-training data. Existing research has explored how the co-occurrence of questions and answers within pre-training data influences model ACC, identifying a general trend: higher co-occurrence frequencies are typically associated with improved response ACC in LLMs~\cite{DBLP:journals/corr/abs-2404-05405, DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23}. However, co-occurrence metrics alone fail to account for the specificity of knowledge. When the subject and object of a knowledge triple are common words that frequently appear in pre-training data, the specificity is low, making it more challenging for the model to recall the information accurately. In contrast, when the subject and object are less frequent, the specificity increases, thereby improving the model’s ability to retrieve the knowledge.

For instance, the headquarters address of NVIDIA is easier to recall than that of Apple because ``Apple'' often appears in diverse contexts, reducing its specificity. According to information theory, Mutual Information (MI) accounts for both the co-occurrence of two variables and their individual specificity \cite{DBLP:journals/bstj/Shannon48}. Thus, we propose leveraging MI to address the challenge of predicting LLM capabilities.

We introduce the MI metric in the pre-training data. Given a pre-training data \(\mathbf{P}\) consisting of \(\mathbf{N}\) paragraphs and a knowledge triple $t = (s, r, o)$, we first calculate three indicators: P(s), the proportion of paragraphs containing the subject $s$, P(o), the proportion of paragraphs containing the object $o$, and P(s, o), the proportion of paragraphs containing both $s$ and $o$. The MI between $s$ and $o$ is then defined as:

\begin{equation}
    I(s, o) = P(s, o) \log \left( \frac{P(s, o)}{P(s)P(o)} \right).
\end{equation}

In this formula, $P(s, o)$ measures the co-occurrence frequency of the subject and object, serving as our baseline. The denominator in the logarithmic term, $P(s)$ and $P(o)$, penalizes the individual occurrences of the subject and object, respectively, highlighting their mutual connection. The formula, in its entirety, quantifies the amount of information shared between $s$ and $o$, which can also be interpreted as the reduction in uncertainty about $o$ given $s$. In the context of LLMs, this shared information corresponds to the likelihood of the model generating the object given the subject. Given the highly skewed distribution of MI values, we apply a logarithm to this metric and normalize it to the range between 0 and 1.

\begin{equation}
    MI(s, o) = Norm(\log(I(s, o))).
\end{equation}

\paragraph{Size-dependent MI metric.}
The MI metric focuses solely on the distribution of pre-training data, without considering the memory capacity of LLMs. However, knowledge retention in LLMs is influenced by both data distribution and model size~\cite{DBLP:journals/corr/abs-2403-00510}. OpenAI’s research highlights that the capabilities of LLMs improve as model size increases \cite{DBLP:journals/corr/abs-2001-08361}. To address this limitation, we propose an enhancement to the MI metric by incorporating model size \(\Phi\) (measured in billions of parameters).

We introduce the SMI metric, an exponential function where the MI metric serves as the base and $1 + \frac{1}{\Phi}$ is the exponent. The MI metric quantifies the informational content of a knowledge triple in the pre-training data, while the model size \(\Phi\) reflects the memory capacity of LLMs. Together, these two factors govern the model’s knowledge retention:

\begin{equation}
    SMI(s, o, \Phi) = Norm(\log(I(s, o)))^{1 + \frac{1}{\Phi}}.
\end{equation}

Since the MI metric is less than 1, as the model size increases, $1 + \frac{1}{\Phi}$ decreases, leading to an increase in the SMI metric value, which indicates stronger memory capabilities of the model.

\paragraph{Predicting LLM capabilities.}
We are committed to linking SMI metric with the ACC of the model. For each knowledge triple, we retrieve the entire pre-training data and calculate three key metrics: the co-occurrence metric, the MI metric, and the SMI metric. Similar to the MI metric, the co-occurrence metric is logarithmically transformed and normalized. Next, we use the knowledge triples to construct questions and test them on LLMs to determine their ACC. Finally, we fit a predictive equation that captures the relationship between the metrics for all knowledge triples in the evaluation set and the observed ACC.

