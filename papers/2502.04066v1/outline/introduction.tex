

\input{figures/head}

\input{figures/intro}

\section{Introduction}
\label{introduction}

The GPT-4 technical report of OpenAI states, ``We registered predictions for GPT-4’s performance on HumanEval before training completed, using only information available prior to training''~\cite{DBLP:journals/corr/abs-2107-03374, DBLP:journals/corr/abs-2303-08774}. However, the report lacks detailed explanations of the methodologies underlying this prediction technique.

This technique is critical for the pre-training of Large Language Models (LLMs). On the one hand, the cost of a single pre-training run can reach hundreds of millions of dollars, making re-pre-training a highly challenging endeavor~\cite{DBLP:journals/corr/abs-2401-04088, DBLP:journals/corr/abs-2407-10671, DBLP:journals/corr/abs-2407-21783}. The prediction technique, which allows for accurate performance forecasting on certain tasks based only on the pre-training data, helps mitigate the resource waste associated with unnecessary re-pre-training aimed at improving a model’s knowledge retention. On the other hand, it can guide the construction of pre-training data that are more aligned with target tasks, optimizing resource usage. This is particularly valuable when pre-training LLMs for specialized domains, as it reduces the creation of redundant data and minimizes unnecessary expenditures~\cite{Taoli-LLama, xiong2023doctorglm, li2024deeplearningllmbasedmethods}.

To achieve this vision, we focus on make predictions for the CBQA task, as it is closely linked to both the pre-training data and the model's knowledge retention~\cite{DBLP:conf/acl/WangL020, DBLP:conf/iclr/Sun0TYZ23}.
However, we encounter three major challenges (Figure~\ref{head}):

\textbf{Challenge1: Mastering the entire pre-training process, especially the construction of pre-training data.} Currently, most open-source base LLMs do not fully disclose their pre-training data, making it challenging to gain a comprehensive understanding of the datasets’ contents. Starting pre-training from scratch is prohibitively expensive, requiring a vast amount of data collection and substantial computational resources. For instance, Meta’s technical report reveals that pre-training the 405B-parameter Llama 3 model on 15.6 trillion tokens involved the use of 16k H100 GPUs, with the total cost approaching several hundred million dollars~\cite{DBLP:journals/corr/abs-2407-21783}.

\textbf{Challenge2: Evaluating whether the pre-trained model retains specific knowledge.} Based on the characteristics of the CBQA tasks, we can assess the model’s accuracy (ACC) on these tasks to evaluate its retention of knowledge. However, most evaluation methods face challenges, such as being overly sensitive to specific in-context examples and having coarse granularity in test data segmentation~\cite{DBLP:conf/emnlp/MinLHALHZ22, DBLP:conf/acl/SrivastavaGD024}. These issues make it difficult to accurately assess whether the pre-trained model has effectively retained specific knowledge.

\textbf{Challenge3: Predicting task-specific knowledge retention using only information available prior to training.} Solving target tasks relies on the model’s ability to learn world knowledge during pre-training, with this retention being strongly influenced by the data~\cite{DBLP:conf/icml/Allen-ZhuL24, DBLP:conf/emnlp/WangYXQD00GJX0C24}. During pre-training, the model generalizes knowledge by learning from terabytes of unlabeled data. However, there is currently no effective method to predict the retention of specific knowledge prior to training. Research from Meta and Palm suggests that analyzing the co-occurrence of questions and answers within the pre-training data might aid in making such predictions~\cite{DBLP:journals/corr/abs-2404-05405, DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23}. Nevertheless, this approach fails to account for the specificity of the knowledge and the impact of the model’s memory capacity on knowledge retention. For example, despite having the same co-occurrence frequency, a model might struggle to recall the location of Apple’s headquarters while easily recalling the location of NVIDIA’s headquarters.

Our work is primarily focused on effectively addressing these three challenges. An overview of our approach is provided in Figure~\ref{intro}. Specifically:

First, we allocate substantial resources to pre-training three base models with parameter sizes of 1.6B, 7B, and 13B, utilizing 1.5 trillion tokens of data. This process incurs a cost of 560k dollars and consumes 520k GPU hours. A detailed overview of the models’ overall performance is provided in Section~\ref{experimental_results}. With full access to the pre-training data, we are able to conduct an in-depth analysis and thoroughly evaluate the models’ performance on specific tasks.

Second, we utilize knowledge triples to perform a comprehensive retrieval and analysis of the pre-training data, focusing on specific CBQA tasks. This enables a detailed examination of the information embedded within the data. Additionally, inspired by the work of~\citet{DBLP:journals/corr/abs-2409-15825}, we implement a robust multi-template complementation mechanism to precisely assess the model’s knowledge retention.

Finally, we introduce the Size-dependent Mutual Information (SMI) metric, an information-theoretic approach to predict a model’s retention of specific knowledge using only information available prior to training. This method considers both the occurrence frequency and specificity of knowledge, as well as the model’s memory capacity. We conduct experiments on our three pre-trained models and the TinyLlama-1.1B model~\cite{DBLP:journals/corr/abs-2401-02385}. Our experiments show that the SMI metric effectively predicts knowledge retention, with a strong linear correlation between the SMI and ACC on CBQA tasks across various LLM sizes (i.e., 1.1B, 1.6B, 7B, and 13B), with Coefficient of Determination ($\text{R}^2$) values over 0.84.

% In addition, based on the characteristics of the SMI mertric and experimental results, we provide several recommendations for the pre-training phase. Regarding data composition, it is essential to consider not only the occurrence frequency of the data but also its specificity. Furthermore, in selecting the parameter size of the pre-trained model, attention should be given to the quality of the data. For details, please refer to Section~\ref{discussion}.

Overall, our contributions are threefold:
% Overall, our contributions are fourfold:
\begin{enumerate}
    \item We allocate substantial resources to pre-train three base models of varying sizes and release the weights and most of the pre-training data for the 1.6B model to support further research in this field.
    \item We propose an information-theoretic method and introduce the SMI metric, which accurately reflects a model's ability to retain task-specific knowledge solely based on the information available prior to training.
    \item Across LLMs of various sizes, the SMI metric demonstrates a strong linear correlation with ACC on CBQA tasks, achieving $\text{R}^2$ values greater than 0.84.
    % \item Based on our experimental results, we provide practical recommendations regarding the composition and quality of pre-training data, as well as the selection of model sizes.
\end{enumerate}