\section{Methodology}
%%%%%%%%%%% Subsection
\subsection{Database Overview}
We use the UofT database (UofTDB) \cite{Wahabi2014}, which was collected in an off-the-person setting and includes recordings in various positions. This database is designed to facilitate evaluation in practical environments and under a wide range of challenging conditions that biometric systems might encounter. UofTDB contains recordings from participants across up to six different sessions and five conditions: sitting, standing, supine, tripod, and exercise. The distribution of subjects across these conditions is summarized in Table~\ref{tab:subject_distribution}.

%%%%%%%%%%%%%%% Table Start %%%%%%%%%%%%%%%
\begin{table}[!t]
    \centering
    \caption{Subject Count by Condition and Recording Session in UofT Database}
    \label{tab:subject_distribution}
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Session} & \textbf{Sit} & \textbf{Stand} & \textbf{Exercise} & \textbf{Supine} & \textbf{Tripod} \\
        \midrule
        S1 & 1012 & 0   & 0  & 0  & 0   \\
        S2 & 72   & 72  & 0  & 0  & 0   \\
        S3 & 76   & 5   & 71 & 0  & 0   \\
        S4 & 63   & 0   & 0  & 0  & 0   \\
        S5 & 0    & 0   & 0  & 63 & 63  \\
        S6 & 65   & 65  & 0  & 0  & 0   \\
        \bottomrule
    \end{tabular}
\end{table}
%%%%%%%%%%%%%%% Table End %%%%%%%%%%%%%%%

To effectively analyze the impact of heart rate dynamics on system performance, we aim to minimize the influence of other sources of variability by integrating them into the training data.
To achieve this, recordings from multiple sessions are included to capture day-to-day variations.
Additionally, the training dataset incorporates recordings from both sitting and standing positions, representing the range of realistic and practical postures for ECG acquisition.
This approach ensures a more robust evaluation while accounting for common physiological and environmental factors.
The \textit{Target} set, which serves as the primary dataset for training and evaluation, includes subjects recorded in all five positions.
In addition, subjects with exercise recordings but missing data in other positions, which were excluded from the Target set, form an \textit{Auxiliary} set of 15 subjects, which is utilized in later stages.

%%%%%%%%%%% Subsection
\subsection{Signal Preprocessing}
The preprocessing workflow is presented in \figref{preprocess_overview}.
%%%%%%%%%%%%%%% Figure Start %%%%%%%%%%%%%%%
\begin{figure}[!t]
\myhyperlabel{preprocess_overview}
\centering
\includegraphics[width=2.5in]{abusa1.pdf}
\caption{Overview of the preprocessing flow, with an average size of $W=3$ used for illustration.}
\label{fig:preprocess_overview}
\end{figure}
%%%%%%%%%%%%%%% Figure End %%%%%%%%%%%%%%%
%%%%%%%%%%% Subsection
\subsubsection{Signal Filtering}
Following previous studies \cite{Jyotishi2022, 8392675} and the database authors \cite{Wahabi2014}, the raw ECG signals are filtered using a 4th order Butterworth band-pass filter with cutoff frequencies of 0.5 Hz and 40 Hz to remove baseline wander, power line interference, and other artifacts.
%%%%%%%%%%% Subsection
\subsubsection{R-peak Detection}
Following filtering, R-peaks are detected using the neurokit2\cite{Makowski2021neurokit} library and the biopeaks \cite{Brammer2020} method. These tools were chosen for their robustness and accuracy in detecting peaks in noisy signals compared to the commonly used Pan-Tompkins method \cite{4122029}, which is critical for precise heartbeat segmentation.
However, due to noise and artifacts in off-the-person databases, sharp spikes are sometimes incorrectly identified as R-peaks. Inter-quartile range (IQR) method \cite{Dekking2005} was used to remove heartbeats with R-peak amplitude falling below the lower threshold $(Q_1 - 1.5 * IQR)$ or above the upper threshold $(Q_3 + 1.5 * IQR)$, with the quantiles calculated for each recording separately.
%%%%%%%%%%% Subsection
\subsubsection{Heartbeat Segmentation}
The signals are segmented into 550 milliseconds (ms) heartbeats, with 175 ms before and 375 ms after the R-peak. These segmentation parameters balance capturing key waveform features while minimizing overlap between heartbeats, particularly in high heart rate signals.

Off-the-person ECG datasets often suffer from a low signal-to-noise ratio (SNR) due to various factors such as movement artifacts and environmental noise. To mitigate this issue, consecutive heartbeats are averaged using a sliding window technique with a window size $W$. This averaging process improves the SNR by approximately $\sqrt{W}$, although it reduces the number of available heartbeats in each recording.

The window size $W$ is a hyper-parameter that can be adjusted based on the signal quality of the ECG device, providing flexibility in preprocessing both the training and testing sets. This adaptability enables the use of single heartbeats, which is particularly important in scenarios that demand minimal acquisition time.

%%%%%%%%%%% Subsection
\subsubsection{T-wave Peak Detection}
For each segmented heartbeat, we locate the T-wave peak (T-peak) by finding the point of maximum amplitude within the last 300 ms of the heartbeat. To enhance the accuracy of T-peak detection, an outlier removal process based on Z-scores is applied to the T-peak locations in the time domain.
Z-score values are calculated separately for the entire rest-state training data and the exercise data.
As the exercise data in the Target set is reserved exclusively for evaluation, the Auxiliary set is used to derive thresholds for exercise samples.
We set the thresholds to be three standard deviations from the mean location for each group, effectively filtering out erroneous T-peak detections and enhancing the reliability of the T-peak identification process.

%%%%%%%%%%% Subsection
\subsection{Personalized Augmentation}

To increase the variability of the training data and compensate for the lack of exercise or elevated heart rate samples, we aim to generate synthetic data that simulates heartbeats across a range of heart rates.
Specifically, to simulate heartbeats at different heart rates, we augment the ST interval in the time domain, as this is the most variable part due to heart rate changes.

Building on prior work in ST interval augmentation \cite{Kim2022} and normalization \cite{Hwang2021}, we observed that regressing the location of the T-wave peak (T-peak) against the heart rate exhibits the expected linear relationship. This aligns with some of the QT corrections used in medical diagnosis \cite{hodges1983bazett, SAGIE1992797} and supports the findings in \cite{Hwang2021}.
While the linear relationship holds across the general population, our analysis reveals that when analyzing each individual separately, the slope of the linear curve differs among subjects, providing a more precise reflection of each subject's unique characteristics.

Therefore, we calculate the linear slope for each subject individually, based on their training data in sit and stand positions. This approach allows us to predict the individual range of T-peak locations for each subject. Subsequently, we perform ST interval augmentation within these personalized ranges, rather than using a global fit or applying a fixed range across all subjects.

Given that the number of sitting recordings is significantly larger than the standing recordings, we calculate both balanced and unbalanced linear fits for each individual.

The unbalanced linear fit assigns a uniform weighting to all the data points, while the balanced linear fit distributes the weights so that the overall weight of the sit data points equals the overall weight of the stand data points.

Heart rate variability is low for some subjects even when considering both sit and stand positions, which can cause one of the linear fits to deviate.
To address this issue, we calculate the minimal T-peak value for the global, balanced, and unbalanced linear fits. The closest value to the global fit among the balanced and unbalanced fits is then selected as the lower T-peak limit, as detailed in \hyperlink{individual_augmentation}{Algorithm 1}.
The upper T-peak limit corresponds to the lowest heart rates. Since resting data is included in the training set, there is no need to generate augmented data up to the subjects' maximum range. Instead, we aim to generate data only in the range where it is lacking; therefore, the upper limit is set as the median of the T-peak locations from the standing position in the training data.

%%%%%%%%%%%%%%% Figure Start %%%%%%%%%%%%%%%
\begin{figure}[!t]
\hrule\noindent
\myhyperlabel{individual_augmentation}
\begin{algorithmic}[1]
    \STATE \COMMENT{--- Initialization ---}
    \STATE $HR_{\text{limit}} \gets 140$ \COMMENT{Upper bound heart rate for augmentation}
    \STATE $T_{g_{\text{min}}} \gets 29$ \COMMENT{$T_{peak}$ location at $HR_{\text{limit}}$ for global fit}
    \STATE $T_{p_{\text{min}}} \gets 25$ \COMMENT{Physiological minimal $T_{peak}$ location}
    \STATE $S_k$: Signals for each individual $k$

    \STATE \COMMENT{--- Main Algorithm ---}
    \FOR{each individual $k \in \{0, \ldots, N-1\}$}
        \STATE Compute $T$-wave location at $HR_{\text{limit}}$ using individual linear fit \label{lst:line:pa_start}
        \STATE \hspace{0.5em} $T_b[k] \gets \text{Balanced fit result}$
        \STATE \hspace{0.5em} $T_{ub}[k] \gets \text{Unbalanced fit result}$

        \IF{$(T_b[k] - T_{g_{\text{min}}})^2 \leq (T_{ub}[k] - T_{g_{\text{min}}})^2$}
            \STATE $T_{peak_{\text{min}}}[k] \gets T_b[k]$
        \ELSE
            \STATE $T_{peak_{\text{min}}}[k] \gets T_{ub}[k]$
        \ENDIF
        \STATE $T_{peak_{\text{min}}}[k] \gets \max(T_{peak_{\text{min}}}[k], T_{p_{\text{min}}})$
        \STATE $T_{peak_{\text{max}}}[k] \gets \text{median}(\{T_{peak} \mid T_{peak} \in \text{individual } k\})$ \label{lst:line:pa_end}

        \FOR{$s \in S_k$}
            \FOR{$T_{new} \in [T_{peak_{\text{min}}}[k], T_{peak_{\text{max}}}[k]]$}
                \STATE $s^{PQRS} \gets \text{PQRS interval of }s$
                \STATE $s^{ST} \gets \text{ST interval of }s$
                \STATE $s^{ST}_{res} \gets \text{Resample } s^{ST}$ so that its T-peak is aligned with  $T_{new}$
                \STATE $s_{res} \gets \text{Concatenate }[s^{PQRS}, s^{ST}_{res}]$
            \ENDFOR
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\noindent\hrule
\algcaption{Personalized Augmentation Algorithm}
\label{alg:individual_augmentation}
\end{figure}
%%%%%%%%%%%%%%% Figure End %%%%%%%%%%%%%%%
%%%%%%%%%%% Subsection
\subsection{Standard CNN Architecture}
We employ a Convolutional Neural Network (CNN) as our feature extraction backbone, followed by fully connected layers for classification. The architecture is adapted from \cite{9211012}, with modifications to accommodate a smaller input size and reduce the number of Max-pooling layers. Additionally, batch normalization layers were added to stabilize and speed up training, as illustrated in \figref[a]{standard_dual_models}. After the convolutional layers, the feature maps are flattened and passed into a Multi-Layer Perceptron (MLP) with a single hidden layer, for classification. The output dimension of the MLP corresponds to the number of classes.


%%%%%%%%%%% Subsection
\subsection{Dual Expert Model}

Deep learning models generally perform well when test data matches the distribution of the training data, assuming sufficient data is available.
However, generalizing to exercise conditions without including exercise data in the training set introduces additional challenges.
Several previous studies have split ECG heartbeats into segments, addressing some of the segments separately \cite{Chun2016, Saechia}, but these studies relied on traditional processing methods. 
However, studies employing deep learning models typically rely on the model to extract the necessary features, thereby avoiding manual segmentation of the heartbeat.

CNNs are designed to learn convolutional kernels that span the entire input, yet distinct features naturally appear across different segments of the heartbeat, such as the PQRS and ST intervals, as illustrated in \figref{pqrs_st}. These differences limit the effectiveness of shared kernels across segments. Additionally, CNNs exhibit translation equivariance \cite{app10093161}, allowing them to recognize features despite minor shifts, which is valuable in capturing variations in the ST interval caused by heart rate fluctuations.
To leverage this structural knowledge of the ECG and the unique characteristics of CNNs, we propose a novel CNN-based architecture that processes the PQRS and ST intervals independently.

%%%%%%%%%%%%%%% Figure Start %%%%%%%%%%%%%%%
\begin{figure}[!t]
\myhyperlabel{pqrs_st}
\centerline{\includegraphics[width=\columnwidth]{abusa2.pdf}}
\caption[PQRS and ST intervals split for DE model]{The input signal is split to PQRS and ST intervals for the Dual Expert model with a 50 ms overlap.}
\label{fig:pqrs_st}
\end{figure}
%%%%%%%%%%%%%%% Figure End %%%%%%%%%%%%%%%

The Dual Expert (DE) Model consists of two Standard CNN models, each processing a different segment of the input signal. The first model processes the PQRS interval, which corresponds to the first 250 ms of the input signal, and is referred to as the PQRS model. The second model processes the ST interval, the last 350 ms of the input signal, and is referred to as the ST model.

The training of the DE model involves two training stages. In stage \uppercase\expandafter{\romannumeral 1}, each model is trained independently on the Target set data for classification. After training, the Fully Connected (FC) layers are removed, and the backbone CNN models serve as feature extractors for the DE model, as illustrated in \figref[b]{standard_dual_models}.
In stage \uppercase\expandafter{\romannumeral 2}, the features extracted by the PQRS and ST backbones are flattened and fused through concatenation into the final feature vector, which is passed into the MLP classifier. At this stage, the weights of the backbone networks are frozen, and only the FC layers of the classifier are trained.

%%%%%%%%%%%%%%% Figure Start %%%%%%%%%%%%%%%
\begin{figure}[!t]
    \myhyperlabel{standard_dual_models}
    \centering
    \includegraphics[width=\columnwidth]{abusa3.pdf}
    \caption{(a) The architecture of the Standard CNN. The CNN backbone consists of 1D convolutional layers (in\_ch$\times$out\_ch$\times$kernel/size/stride/padding),
    batch normalization, MaxPooling, and ReLU activations. The feature maps are flattened and passed to the MLP classifier, which includes Fully Connected (FC) layers, ReLU activation, dropout, and a final softmax layer for classification. (b) The DE model includes the pre-trained PQRS and ST backbones, which share the architecture with the Standard model backbone. The MLP classifiers of the Standard and DE models differ only by the input dimension.}
    \label{fig:standard_dual_models}
\end{figure}
%%%%%%%%%%%%%%% Figure End %%%%%%%%%%%%%%%
%%%%%%%%%%% Subsection
\subsection{Domain Adaptation}
We utilize the Auxiliary set, which includes recordings under various conditions, including exercise. Since the Auxiliary data is not part of our evaluation set (Target data), its exercise data can be used for training. While augmentation accounts for ST interval variability in the time domain, it does not fully capture the variability associated with heart rate elevation, such as amplitude changes, which limits the classifier's ability to generalize to genuine exercise data.

To direct the classifier toward learning condition-invariant features, in stage \uppercase\expandafter{\romannumeral 2} we train the DE model using genuine (non-augmented) data from both the Target and Auxiliary sets.
To accommodate the additional 15 subjects from the Auxiliary data, we expand the output layer of the classifier and train the model to classify all the subjects.
After training, we remove the additional neurons for inference, retaining only the classes corresponding to the subjects in our Target data as illustrated in \figref{domain_adaptation_clf}.
%%%%%%%%%%%%%%% Figure Start %%%%%%%%%%%%%%%
\begin{figure}[!t]
    \myhyperlabel{domain_adaptation_clf}
    \centering
    \includegraphics[width=\columnwidth]{abusa4.pdf}
    \caption{Classifier output dimensions during domain adaptation. (a) During training the output dimension includes subjects from Target and Auxiliary sets. (b) After training the classes corresponding to the Auxiliary set are removed.}
    \label{fig:domain_adaptation_clf}
\end{figure}
%%%%%%%%%%%%%%% Figure End %%%%%%%%%%%%%%%