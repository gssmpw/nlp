%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{times}  % DO NOT CHANGE THIS


\usepackage[table]{xcolor}
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{bm}
\usepackage{booktabs}
% \usepackage{stfloats}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{amsfonts}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{breakurl}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Retrieval-Augmented Visual Question Answering\\ via Built-in Autoregressive Search Engines}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    Xinwei Long\textsuperscript{1},
    Zhiyuan Ma\textsuperscript{1},
    Ermo Hua\textsuperscript{1},
    Kaiyan Zhang\textsuperscript{1},
    Biqing Qi\textsuperscript{2},
    Bowen Zhou\textsuperscript{1}\thanks{Corresponding author.}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Department of Electronic Engineering, Tsinghua University\\
    \textsuperscript{\rm 2}Shanghai Artificial Intelligence Laboratory\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript
    % email address must be in roman text type, not monospace or sans serif
    longxw22@mails.tsinghua.edu.cn
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
% Retrieval-augmented generation (RAG) have emerged to address knowledge-intensive visual question answering tasks.
% Existing methods utilize seperate retrieval and generation modules to acquire external knowledge and generate answers respectively.
% We propose a ReAuSE, an alternative to the traditional RAG model, in which knowledge retrieval is seamlessly integrated into the answer generation model, serving as a built-in search engine.
Retrieval-augmented generation (RAG) has emerged to address the knowledge-intensive visual question answering (VQA) task.
Current methods mainly employ separate retrieval and generation modules to acquire external knowledge and generate answers, 
respectively.
% which suffer from the 
We propose ReAuSE, an alternative to the previous RAG model for the knowledge-based VQA task, which seamlessly integrates knowledge retriever into the generative multi-modal large language model, serving as a built-in search engine.
% We propose ReAuSE as an alternative to seamlessly integrates knowledge retriever into the generative multi-modal large language model (MLLM), serving as a built-in search engine.
% ReAuSE takes advantage of the fact that MLLMs can work as virtual knowledge warehouses, aware of which documents a given query can be associated with.
% Specifically, our model operates both as a generative retriever, directly producing identifiers for relevant documents (where each identifier corresponds to a document within the knowledge base), and as an accurate answer generator based on the retrieved documents.
Specifically, our model functions both as a generative retriever and an accurate answer generator. It not only helps retrieve documents from the knowledge base by producing identifiers for each document, but it also answers visual questions based on the retrieved documents.
Furthermore, we also propose a reinforced retrieval calibration module from relevance feedback to improve retrieval performance and align with the preferences for accurate answer generation.
Extensive experiments on two representative OKVQA and A-OKVQA datasets demonstrate significant improvements ranging from 2.9\% to 9.6\% across all evaluation metrics when compared to strong baselines. 
% The code will be available at \url{https://github.com/xinwei666/ReAuSE}
% Specifically, Our model can work both as a generative retriever, which directly generates the identifier of relevant documents (i.e., each identifier can be linked to a document in the knowledge base), and can provide accurate answers based on the retrieved documents.


% AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}
The Visual Question Answering (VQA) task aims to answer questions based on a user-provided image, which has received significant attention from CV and NLP community~\cite{Antol_2015_ICCV,hu2017learning,shen2023git,sunprogram,DBLP:conf/naacl/ZhuQZLLZ24}.
Early VQA methods~\cite{mascharka2018transparency,gao2019dynamic} mainly focus on understanding visual elements within the image.
Recently, the research trend of VQA has shifted towards knowledge-intensive scenarios~\cite{Shah_Mishra_Yadati_Talukdar_2019}, requiring the incorporation of external knowledge and joint reasoning over multi-modal content to generate accurate answers.
However, existing methods generally face challenges in effectively acquiring relevant information from large-scale knowledge bases using multi-modal queries~\cite{DBLP:conf/nips/LinCMCB23}.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{./fig1_modify3.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Comparing with the Paradigm of Previous Knowledge-Based VQA Methods.}
\label{fig1}
\end{figure}

Retrieval-augmented generation (RAG)~\cite{chan2024rq,chen2024benchmarking} has recently emerged as a promising approach for knowledge-based visual question answering (KBVQA) tasks~\cite{DBLP:conf/cvpr/0013PTRWN22,DBLP:conf/emnlp/LinB22,DBLP:conf/emnlp/ChenHCVC22}.
RAG-based approaches typically consist of two separate stages: retrieval and generation. 
In the first retrieval stage, these methods usually integrate multiple discriminative retrievers, each designed for specific purposes such as image-to-text or text-to-text retrieval.
Afterward, in the second answer generation stage, these methods typically use generative multi-modal large language models (MLLM) to produce the final result.
Despite achieving success in some benchmarks~\cite{DBLP:conf/cvpr/MarinoRFM19,DBLP:conf/eccv/SchwenkKCMM22}, this workflow still encounters several limitations.
1) Current methods sequentially invoke models in the pipeline for feature engineering, retrieval, and answer generation, requiring the integration of multiple heterogeneous models.
2) Moreover, these methods typically combine generative answer generators with discriminative retrievers. The disparate model architectures make it challenging for retrievers to further optimize their performance based on the feedback from the answer generator.
% these methods utilize discriminative retrievers and generative answer generators, and the heterogeneous model architectures result in the retrievers being challenged to co-optimize through the feedback signals from the answer generators.
Consequently, the research question arises: \textbf{\textit{how can we integrate knowledge retrieval and answer generation into a homogeneous generative model?}}
% Current methods ensemble multiple heterogeneous models within the workflow, each serving distinct purposes such as feature engineering, retrieval, and generation. 
% This integration of diverse models leads to a redundant and potentially inefficient pipeline.
% Moreover, existing approaches often treat retrieval and generation as separate processes, hindering the retriever's ability to adapt effectively to downstream answer generation tasks.

% Recently, we have witnessed a wide range of discriminative tasks being transformed into seq2seq tasks. 
% Among these, generative retrieval has been proposed, aiming to directly generate relevant document identifiers based on a query, rather than calculating the similarity between the query and documents one by one. 
% Through the identifiers of relevant documents, we can associate with documents that actually exist in the knowledge base. 
% However, existing generative retrieval methods still struggle to outperform discriminative retrievers in general scenarios.

To address the above issue, we propose ReAuSE, a novel \underline{Re}trieval-augmented framework with built-in \underline{Au}toregressive \underline{S}earch \underline{E}ngines for knowledge-based VQA tasks, which seamlessly integrates knowledge retrieval into the generative MLLM.
ReAuSE takes advantage of the fact that MLLMs can serve as virtual knowledge warehouses~\cite{pan2024unifying}, 
%aware of which documents the multi-modal query can be associated with.
recognizing the documents that a multi-modal query can be linked to.
Therefore, ReAuSE abandons the discriminative retrieval paradigm that computing the similarity between the query and document one by one,
whereas directly generates the document identifier in an autoregressive manner, where each identifier corresponds to a document within the knowledge base.
We define the document identifiers as a sequence of tokens that appears at least once within a document in the knowledge base, thus enabling effective and efficient mapping to the document.
Subsequently, we propose a reinforced retrieval calibration method based on relevance feedback to further enhance retrieval performance.
To collect relevance preference data, we employ a MLLM as a reward model, which inputs sampled documents and questions into this model and assesses document relevance based on the VQA scores~\cite{Antol_2015_ICCV} of the generated answers. 
To align with relevance preference, we employ a direct preference optimization (DPO) algorithm~\cite{rafailov2024direct} to further refine the generative retrieval model.
In the answer generation stage, we input the retrieved documents one by one, and the model obtains the final prediction based on the joint probability of retrieval and answer generation.

We conduct primary experiments on two representative knowledge-based VQA benchmarks, OKVQA and A-OKVQA.
The experimental results show significant improvements of 2.9\%-9.6\% across all metrics compared to strong baselines.
Additionally, we perform knowledge retrieval experiments on three datasets to further validate the performance of the generative knowledge retrievers. 
Our model consistently outperforms other discriminative knowledge retrievers and the improvements become more apparent when applied to large-scale knowledge bases.
This outcome illustrates our modelâ€™s capability
to retrieve knowledge from large-scale knowledge sources. The code will be available at \url{https://github.com/xinwei666/ReAuSE}
% These experiments demonstrate that our model can meet the demands of knowledge-based VQA tasks for large-scale knowledge acquisition.
% Later, we propose reinforced retrieval calibration fromrelevance feedbacks to further improve the retrieval performance. 
% To collect relevance preference data, we use a MLLM that has not seen the training data as a reward model. We input the sampled documents and questions, and measure the relevance of the documents based on the VQA score of the generated answers. We use a direct preference optimization algorithm to further optimize the generative retrieval model.

% which can be used to directly locate a specific document in the knowledge base.

% ReAuSE builds on recent studies that apply generative language models to text retrieval, but extends these insights to support knowledge acquisition for knowledge-based VQA tasks.
% Moreover, ReAuSE takes advantage of the fact that MLLMs can work as virtual knowledge warehouses, aware of which documents the multi-modal query can be associated with.


\section{Related Work}
%\textbf{Visual Question Answering (VQA).} Traditional VQA tasks, which focus on answering questions related to visual attributes (e.g., counting, object detection), have been extensively studied. Early datasets, such as VQAv1, VQAv2, and Visual Madlibs, relied solely on the provided image and text to answer questions, eliminating the need for external knowledge integration.
\textbf{Traditional Visual Question Answering (VQA)} tasks~\cite{johnson2017clevr,mishra2019ocr}, which focus on answering questions related to visual elements (e.g., simple counting, visual attributes), have been extensively studied. 
Several studies~\cite{DBLP:conf/cvpr/MarinoRFM19} have revealed that over 78\% of questions can be answered by people under ten years old, indicating that traditional VQA tasks require little background knowledge to answer a vast majority of questions.

% Early datasets, such as VQAv1, VQAv2, and Visual Madlibs, require little background knowledge to answer the vast majority question.
% relied solely on the provided image and text to answer questions, eliminating the need for background knowledge.
% The analysis 

\noindent \textbf{Knowledge-based VQA.} To assess models' capacity to leverage world knowledge instead of relying solely on input data, knowledge-based VQA tasks have emerged, such as OKVQA~\cite{DBLP:conf/cvpr/MarinoRFM19}, and A-OKVQA~\cite{DBLP:conf/eccv/SchwenkKCMM22}. 
OKVQA and A-OKVQA datasets pose challenges in acquiring the necessary knowledge from an outside source and performing reasoning over multi-modal contexts and knowledge. 
% Later, A-OKVQA is proposed as a successor of OKVQA, it focuses on visual-grounded reasoning instead of knowledge acquisition.
Recently, Infoseek~\cite{DBLP:conf/emnlp/ChenHLSCRC23} has been proposed, featuring visual questions about detailed properties of factual knowledge in Wikipedia. The above datasets all highlight the importance of retrieving knowledge from external sources and underscore that current state-of-the-art methods still have significant room for improvement in this task.

Existing approaches have been proposed to incorporate knowledge in two ways to address knowledge-based VQA tasks.
% can be categorized into two types. 
One line of research~\cite{DBLP:conf/emnlp/XenosSPT23,chen2023see,gui2021kat} leverages implicit knowledge from LLMs.
This approach involves converting images into text or directly feeding multi-modal contexts into LLMs (e.g. GPT-3~\cite{brown2020language}, GPT-4V~\cite{achiam2023gpt}, etc.) to generate text that serves as augmented knowledge, but hallucinated information produced by LLMs poses risks to the overall pipeline. 
Another research direction~\cite{DBLP:conf/nips/LinX0X0Y22,DBLP:journals/corr/abs-2403-10037,DBLP:conf/nips/LinCMCB23} aims to retrieve explicit knowledge from structured or unstructured KB.
This approach, known as retrieval augmentation, often uses off-the-shelf tools to generate visual tags and captions, thereby boosting the performance of knowledge retrievers.
% There have been preliminary efforts to combine both ways by simply using the results of LLMs and retrievers as external knowledge, without bringing about fundamental differences compared to the previous methods.
Several studies~\cite{DBLP:conf/cvpr/0013PTRWN22,DBLP:conf/cvpr/HuI0WCSSRF23} have tried to combine both ways by simply using the results of LLMs and retrievers but led to limited improvements over baselines.

\noindent \textbf{Knowledge Retrieval.} As a crucial component of retrieval-augmented approaches, knowledge retrievers face challenges in handling multi-modal queries~\cite{DBLP:conf/emnlp/LuoZBB21,luo2023end,shen2023pbsl}. Several methods~\cite{lin2022retrieval,DBLP:conf/emnlp/LinB22,DBLP:conf/cvpr/0013PTRWN22}, which employ separate text-to-text and image-to-text retrievers, struggle to capture cross-modal interactions. 
To bridge this gap, Reviz~\cite{luo2023end} leverages visual-language models to unify the encoding of image and text queries, and FMLR~\cite{DBLP:conf/nips/LinCMCB23} proposes a fine-grained late-interaction framework to fuse cross-modal features at the token level.
PreFLMR~\cite{lin2024preflmr} explores scaling laws for knowledge retrieval based on the FLMR model.
Although these methods achieve improvements over previous approaches, they require training on large-scale datasets containing millions of image-text pairs, which incurs high computational costs.

Recently, some studies~\cite{bevilacqua2022autoregressive,ziems2023large,li2023multiview,li2024survey,DBLP:conf/acl/LongZMZZ24,jain-etal-2024-rag} have introduced generative pipelines in information retrieval tasks, instead of discriminative retrievers.
These methods~\cite{DBLP:conf/nips/Tay00NBM000GSCM22} are based on the assumption that all documents are memorized by generative language models, and the language model directly generates the identifiers of relevant documents based on the query.
While prior research~\cite{li2024generative,long2024generative} has investigated generative retrieval for multi-modal tasks, such methods have demonstrated only marginal gains over traditional methods when applied to general tasks.
Different from them, we are the first work to seamlessly integrate generative retrieval and retrieval-augmented VQA tasks, and use the feedback from the QA module to enhance the retrieval performance, thereby achieving better retrieval and QA results simultaneously.
% Different from them, our research is the first to seamlessly integrate generative retrieval with retrieval-augmented VQA tasks. We leverage feedback from the QA module to enhance retrieval performance, thereby achieving simultaneous improvements in both retrieval and QA accuracy.

% \begin{itemize}
% \item You must use the 2025 AAAI Press \LaTeX{} style file and the aaai25.bst bibliography style files, which are located in the 2025 AAAI Author Kit (aaai25.sty, aaai25.bst).
% \item You must complete, sign, and return by the deadline the AAAI copyright form (unless directed by AAAI Press to use the AAAI Distribution License instead).
% \item You must read and format your paper source and PDF according to the formatting instructions for authors.
% \item You must submit your electronic files and abstract using our electronic submission form \textbf{on time.}
% \item You must pay any required page or formatting charges to AAAI Press so that they are received by the deadline.
% \item You must check your paper before submitting it, ensuring that it compiles without error, and complies with the guidelines found in the AAAI Author Kit.
% \end{itemize}
\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{./fig2_modify2.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The architecture of ReAuSE. ReAuSE contains three components: Built-in Autoregressive Search Engine for knowledge retrieval, Reinforced Retrieval Calibration via Relevance Feedback to align retrievers with relevance preferences, and Retrieval-Augmented Generation for answer prediction.}
\label{fig2}
\end{figure*}

\section{Methodology}
We introduce ReAuSE, a \textbf{Re}trieval-\textbf{Au}gmented framework utilizing built-in \textbf{Au}toregressive \textbf{S}earch \textbf{E}ngines tailored for knowledge-based VQA tasks.
ReAuSE is designed as
%proposes 
a unified model to facilitate both effective knowledge retrieval and question-answering tasks.
%As depicted in Fig.2, ReAuSE contains three components: Built-in Autoregressive Search Engine, Reinforced Retrieval Calibration via Relevance Feedback, and Retrieval-augmented Answer Generation.

\subsection{Problem Formulation}
Formally, let $\mathcal{D}=\{D_1, ..., D_k\}$ denotes a knowledge base used for the knowledge-based VQA task, $D_i=\{d_1, ..., d_{|D|}\}$ denotes a document with its title and textual contexts, and $R_i=\{r_1, r_2, ..., r_{|R|}\}$ denotes an identifier of the document $D_i$.
Given a multi-modal query $X$, the generative knowledge retrieval can be formulated as a Seq2Seq task, as Eq.~\ref{eq1},
\begin{equation}
  \mathcal{P}(R_i|X) = \prod_{j=1}\mathcal{P}(r_j|\bm {r}_{<j},X,\Theta) . 
  \label{eq1}
\end{equation}
where $\mathcal{P}$ denotes the standard auto-regressive language modeling probability and $\bm \Theta$ are the paramters of our model. During inference, the model employs a constrained strategy to guide the decoder in generating valid identifiers, which maintains a deterministic mapping relationship $\varphi$ between identifier and document, as Eq.~\ref{eq2},
\begin{equation}
  \varphi: R_i \to D_i, {\rm where}\  D_i \in \mathcal{D} .
  \label{eq2}
\end{equation}
Finally, we obtain a subset $\hat{D}=\{D_1, ..., D_{|K|}\}$ from $\mathcal{D}$ to improve answer generation. The overall likelihood of generating the answer $Y$ is given by Eq.~\ref{eq3},
\begin{equation}
  \mathcal{P}(Y|X) =\sum_{D_i \in \hat{D}} \underbrace{\mathcal{P}(R_i|X)}_{retrieval} \cdot \underbrace{\mathcal{P}(Y|X, D_i)}_{generation} . 
  \label{eq3}
\end{equation}

% ensures that each $R_i$ can be definitively mapped to a document 
% we first retrieve $K$ possibly relevant documents $D=\{d_1, ..., d_k\}$ from the knowledge base $\mathcal{D}$. Each $d_i$ contains its title and textual contexts, and $R=\{r_1, ..., r_k\}$
% , which employs a unified model to perform effective knowledge retrieval and retrieval-augmented question answering.
% For knowledge retrieval, our objective is to acquire relevant knowledge documents by directly generating identifiers of such documents.
% For QA tasks, our goal is to generate accurate answers based on retrieved knowledge and visual knowledge.
% To implement such a unified approach, we 
% aims at integrating retrieval and the QA task into a unified model. 

\subsection{Built-in Autoregressive Search Engines}
% Knowledge-based VQA tasks require extensive knowledge access due to the insufficiency of vital information within their contexts.
We introduce a novel autoregressive search engine for knowledge-based VQA tasks to facilitate retrieval from external knowledge bases.
% The generative retriever leverages an architecture homologous to common question answering models, enabling its seamless integration and functioning as a built-in search engine.
The autoregressive search engine leverages a generative architecture similar to that of common multimodal large language models, instead of discriminative models, enabling its seamless integration and functioning as a built-in module.

Given a multi-modal input $X=\{Q, V\}$, the autoregressive search engine aims to generate the relevant identifier directly in a seq2seq manner as Eq.~\ref{eq1}. 
For example, Fig.~\ref{eq2} shows how our model generates the corresponding identifier for a document related to the ``palm tree" based on the input image and question. 
To achieve such a generative retriever, we mainly elaborate on the three aspects as follows:
% The generative knowledge retriever is based on the assumption that large language models serve as virtual knowledge repositories, aware of which documents a user's given query can be associated with.

\noindent \textbf{Document Identifier.} 
Based on the assumption in ~\cite{pan2024unifying} that large language models are aware of the content within each document, we define any document's identifier as subsequences that appear only in that specific document.
Unlike the one-to-one relationship in DSI~\cite{DBLP:conf/nips/Tay00NBM000GSCM22}, We assign more than one identifier to each document, as long as these identifiers are unique for this document.
% To represent each document effectively and flexibly, we define the identifier as any subsequence within a document that appears exclusively in that particular document.
% This approach is based on the assumption that large language models can serve as virtual knowledge bases, which are aware of the content within each document.
Consequently, our model does not require additional memory steps as in existing studies~\cite{DBLP:conf/nips/Tay00NBM000GSCM22,li2024generative} to associate documents with identifiers.
%Existing methods use static identifiers (e.g. numeric IDs and URLs), which require additional memory steps to associate each identifier with a document.

\noindent \textbf{Supervised Fine-tuning} teaches our model to generate relevant identifiers based on the autoregressive probability for each given multi-modal query. 
To sample the most relevant sub-sequences from the given ground-truth document as identifiers, we employ a large language model~\cite{touvron2023llama} as an extractive summarizer, which uses a fixed-length original text to answer a given question.
Later, we filter the obtained set of identifiers and select the identifier containing the most answer keywords as the target identifier.
Note that our model is model-agnostic, allowing it to be applied to any generative multi-modal large language model. 
The generative loss function can be formalized as maximizing the likelihood of the target identifier using the teacher forcing strategy, as Eq.~\ref{eq4}.
\begin{equation}
  \mathcal{L}_{retrieval} = \sum_{j=1} {\rm log} \mathcal{P} (r_j|\bm {r}_{<j},X) . 
  \label{eq4}
\end{equation}
To avoid overfitting and catastrophic forgetting, we freeze all the parameters of the MLLM and adopt the Low-Rank Adaptation (LoRA) method~\cite{hu2021lora} to efficiently fine-tune our model, with only the parameters of LoRA being updated.


\noindent \textbf{Constrained Decoding and FM-Index.} 
A valid identifier is defined as a generated sequence that appears at least once within a document in the knowledge base, ensuring that each generated identifier can be directly linked to a specific document.
To help the model generate valid identifiers during inference, we implement a beam decoding strategy constrained by knowledge bases.

Specifically, we use the previously generated sequence $R_i^{t-1}=\{r_1,...,r_{t-1}\}$ as the prefix condition to search for all matching strings in the knowledge base.
We then extract the subsequent tokens from these strings to form a feasible token set $\mathcal{S}$.
The model's next token, $r_t$, is restricted to selection from $\mathcal{S}$, guaranteeing that all generated sequences exist within the knowledge base.
To support fast substring search, we utilize an FM-Index database~\cite{ferragina2000opportunistic,bevilacqua2022autoregressive} to store the knowledge base. FM-Index is an efficient indexing structure tailored for substring search. The time complexity for obtaining the next allowed token is nearly $\mathcal{O}(V)$, where $V$ is the vocabulary size, independent of the size of the knowledge base.


% During inference, we use a constrained decoding strategy to guide the model in generating valid identifiers. Here, a valid identifier refers to a generated sequence that has appeared at least once in a document within the knowledge base. In other words, the generated identifier can be associated with a specific document.
% To ensure the generation of valid identifiers during inference, we implement a constrained decoding strategy. A valid identifier is defined as a generated sequence that appears at least once within a document in the knowledge base. This constraint ensures that each generated identifier can be directly linked to a specific document.
% To access external knowledge bases, we present a novel Generative Knowledge Retrievers for knowledge-based VQA tasks. The generative knowledge retriever adopts an isomorphic structure and parameters to the VQA model, except for specific Lora adapter parameters. Therefore, our proposed knowledge retriever can be considered natively integrated with the QA model and can be seen as a built-in search engine.
% The generative knowledge retriever is based on the assumption that large language models serve as virtual knowledge repositories, aware of which documents a user's given query can be associated with.
% All papers submitted for publication by AAAI Press must be accompanied by a valid signed copyright form. They must also contain the AAAI copyright notice at the bottom of the first page of the paper. There are no exceptions to these requirements. If you fail to provide us with a signed copyright form or disable the copyright notice, we will be unable to publish your paper. There are \textbf{no exceptions} to this policy. You will find a PDF version of the AAAI copyright form in the AAAI AuthorKit. Please see the specific instructions for your conference for submission details.

\subsection{Reinforced Retrieval Calibration via Relevance Feedback}
Despite teaching our model through supervised fine-tuning to generate relevant document identifiers based on user queries, the retrieved documents exhibit varying degrees of relevance. 
Even when documents are provided, the QA model may struggle to provide accurate responses.
Optimally, the generative retriever should retrieve documents that: (1) strongly correlate with the multi-modal query, and (2) minimize extraneous content.
Consequently, it is essential to further improve retrieval performance through feedback from the QA model.

As the first step towards this goal, we sample a set of identifiers $\{R_1,...,R_k\}$ for each $X$ using the generative retriever $\pi_{sft}$ that has been supervised fine-tuned.
% To accomplish this goal, we first use the generative retriever that has been supervised fine-tuned as the policy model $\pi_{sft}$ to sample a set of identifiers $\{R_1,...,R_k\}$ for each $X$. 
% Then, we collect relevance preferences through the following three aspects:
Then, we score the collected samples by evaluating their relevance from three aspects:
\begin{itemize}
\item \textbf{Contributions to VQA performance.} 
A document is deemed relevant if a model can produce the correct answer using it. To evaluate this relevance, we employ an MLLM that has not been fine-tuned on downstream data as the reward model, with the VQA score serving as the reward value $v_{vqa} \in [0, 1]$.
% If a model can generate the correct answer based on this document, then the document may be relevant. To measure this relevance, we use an MLLM that has not been fine-tuned on downstream data as the reward model, and the VQA score as the reward value.
% \item The Relevance between Documents and Answers. If a model can generate the correct answer based on this document, then the document may be relevant. To measure this relevance, we use an MLLM that has not been fine-tuned on downstream data as the reward model, and the VQA score as the reward value.
\item \textbf{Keyword Hit Count.} If an identifier includes keywords from the answer set, it is likely to be relevant. To quantify this relevance, we employ an exact matching function as the reward function, with matching signals serving as the reward values $v_{hit}\in \{0, 1\}$.
% \item The Relevance between Identifiers and Answers. If the identifier contains the keywords from answer set, it is likely to be relevant. To measure this relevance, we use exact matching function as the reward function, and matching signals as the reward values.
\item \textbf{Semantic Similarity.} Higher semantic similarity between an identifier and a document indicates that the identifier better represents the document's semantics, thereby suggesting a lower presence of irrelevant content within the document. To measure this relevance, we use the BERT model to calculate the cosine similarity between identifiers and documents as the reward values $v_{sim} \in [0, 1]$.
% \item The Relevance between Identifiers and Documents. If the semantic similarity is closer, then the document contains less content irrevelant to the identifier. To measure this relevance, we use the BERT model to calculate the cosine similarity between identifiers and documents
\end{itemize}

The overall reward can be obtained by taking a weighted sum of the scores from different aspects.
Then, we build a triplet $<X, R^{+}, R^{-}>$ for each $X$ by treating the identifiers with the highest/lowest reward as positive/negative samples, respectively. 
Using the triplets reflecting the QA model's preference, the retriever can be further aligned by preference-based reinforcement learning.
As one of the typical methods, direct preference optimization (DPO)~\cite{rafailov2024direct} is widely used for its efficiency and effectiveness.
% We obtain the final relevance preference by taking a weighted sum of the above reward values. We select the identifier with the highest reward value and the identifier with the lowest reward value to form a preference triplet as $<X, R^{+}, R^{-}>$. 
% Due to the success of the direct alignment from preference methods, direct preference optimization (DPO)~\cite{rafailov2024direct} has emerged as an alternative to reinforcement learning from human feedback.
Therefore, we employ the DPO loss to further optimize our autoregressive knowledge retriever as Eq.~\ref{eq7},
\begin{equation}
  \mathcal{L}_{dpo} = - {\rm log} \sigma\Bigg( \beta {\rm log} \frac{\pi_{\Theta}(R^+|X) \pi_{sft}(R^-|X)}{\pi_{sft}(R^+|X) \pi_{\Theta}(R^-|X)} \Bigg)  . 
  \label{eq7}
\end{equation}
where $\pi_{sft}$ is the original model used as reference, and $\pi_{\Theta}$ is the model being optimized. As before, we only update the parameters of LoRA.

\subsection{Answer Generation}
Utilizing built-in autoregressive knowledge retrievers, we extract the top-K relevant documents from extensive knowledge bases to serve as external knowledge.
For our answer generation model, we employ a model architecture homologous to that of the retrieval module.
As illustrated in Fig. 2, we construct a prompt template, filling the slots with the image, question, and each retrieved document.
The multi-modal contexts are then fed into the model, and the training loss of the answer generation follows that of the generative retrieval model, as Eq.~\ref{eq5},
\begin{equation}
  \mathcal{L}_{gen} = \sum_{j=1} {\rm log} \mathcal{P} (y_j|\bm {y}_{<j},X, D_i) . 
  \label{eq5}
\end{equation}
where $y_j$ denotes the $j-th$ token of the ground-truth answer $Y$. As before, we freeze all the parameters of the MLLM, but introduce another LoRA, and only update the parameters of this new LoRA.
\begin{equation}
\begin{split}
	\hat{Y}, \hat{D} &= \mathop{\rm arg max}\limits_{Y, D_i} \mathcal{P} (Y, D_i|X) \\
	&= \mathop{\rm arg max}\limits_{Y, D_i} \mathcal{P} (Y|X, D_i) \cdot \mathcal{P} (R_i|X).
\label{eq6}	
\end{split}
\end{equation}
During inference, We use the same MLLM and parameters for both the retrieval and answer generation stages, except for the two LoRA adapters. 
After retrieving the relevant document set, we switched to the LoRA adapter for answer generation, and obtain
the final prediction through the joint probability of retrieval and answer generation, as Eq.~\ref{eq6}.
% \begin{equation}
%   \hat{Y}, \hat{d} = \mathop{\rm arg max}\limits_{Y, D_i} \mathcal{P} (Y, D_i|X)= \mathop{\rm arg max}\limits_{Y, D_i} \mathcal{P} (Y|X, D_i)\mathcal{P} (R_i|X). 
%   \label{eq5}
% \end{equation}
% \begin{equation}


\section{Experiments}
\subsection{Experiment Setup}
\textbf{Datasets and Knowledge Bases.}
We focus on the knowledge-based VQA benchmarks, OKVQA~\cite{DBLP:conf/cvpr/MarinoRFM19} and A-OKVQA~\cite{DBLP:conf/eccv/SchwenkKCMM22}. 
Previous work provided two retrieval corpora, GS112K~\cite{DBLP:conf/emnlp/LuoZBB21} and Wiki21M~\cite{karpukhin2020dense}, for the OKVQA dataset. GS112K contains 112K passages collected through Google Search, while Wiki21M is a subset of Wikipedia, containing 21M Wikipedia entries.
Moreover, we also conduct retrieval experiments on these two corpora and introduce a new information-seeking dataset, InfoSeek~\cite{DBLP:conf/emnlp/ChenHLSCRC23}, to evaluate the model's retrieval performance. Since InfoSeek's KB is not publicly available, we use the KB provided by PreFLMR~\cite{lin2024preflmr} and follow the same experimental setup.

\noindent \textbf{Evaluation Metrics.} 
We strictly follow the settings of the original papers, using the corresponding metrics for each dataset.
For the OKVQA dataset and the ``direct answer" setting of the A-OKVQA dataset, we use the VQA score to evaluate the model's performance.
For the ``multi-choice" setting of the A-OKVQA dataset, we use accuracy for evaluation.
To evaluate the performance of knowledge retrieval, we use the Pseudo-relevance Recall@K (PRR@K)~\cite{DBLP:conf/emnlp/LuoZBB21}, consistent with the baselines.

\noindent \textbf{Baselines.} 
We adopt several baseline methods for comparison, categorized as follows:
1) multi-modal large language models: 
LLaVA-13B~\cite{liu2024visual}, PALm-E-562B ~\cite{chenpali}, and GPT-4V~\cite{achiam2023gpt}.
2) knowledge-enhanced methods via GPT-3/4 APIs: Prophet~\cite{shao2023prompting}, Promptcap~\cite{hu2023promptcap}, FillingGap~\cite{wang2023filling} and REVIVE~\cite{DBLP:conf/nips/LinX0X0Y22}.
3) retrieval-augmented methods:  TwO~\cite{si2023combo}, ReVeaL~\cite{DBLP:conf/cvpr/HuI0WCSSRF23}, GeMKR~\cite{long2024generative}, and FLMR~\cite{DBLP:conf/nips/LinCMCB23}.
For the A-OKVQA dataset, we also add the advanced GPV-2~\cite{DBLP:conf/eccv/SchwenkKCMM22}, SimVQA~\cite{xenos2023simple}, Cola-FT(11B+3B)~\cite{chen2024large} and CKR-VQA~\cite{DBLP:journals/corr/abs-2403-10037} as baselines.

\noindent \textbf{Implementation Details.}
Our framework is model-agnostic. 
In our main experiments, we utilize MiniGPT4-v2-7B as the base model, which employ ViT-L/14 from pre-trained CLIP as the image encoder and LLaMa-v2-7B (Touvron et al. 2023) as the text encoder.
% In the ablation studies, we also use the MLLMs employed by other baselines as the base models, such as MiniGPT-4-v1~\cite{zhu2023minigpt}, InstructBLIP~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}, and BLIP-2~\cite{li2023blip}.
We freeze all parameters of the MLLM, allowing updates only to the LoRA parameters. 
\textbf{We use the same MLLM in the three stages but apply two sets of LoRA parameters to optimize the model respectively: one for retrieval and alignment, and the other for answer generation.} 
Our model is implemented in PyTorch, utilizing version 0.3.0 of the PEFT library, which supports efficient switching between two LoRA adapters during inference. 
Similar to baselines, we use image captions as features to enhance the model's performance.
Each training stage is performed on four NVIDIA A6000 48G GPUs and completed within three hours.
% Due to space constraints, we have moved the discussion of other hyperparameter details to the appendix, where the code will also be available.

\begin{table}[]
\centering
\small
\begin{tabular}{lcc}
\toprule
Model                       & PRR@K         & Score        \\ \midrule
\multicolumn{3}{c}{\textbf{\textit{Multi-modal Large Language Models}}}               \\
% Minigpt4v2-7B~\cite{chen2023minigpt}             & -                  & 57.8             \\
% Minigpt4v2-7B~\cite{chen2023minigpt}             & -                  & 61.9             \\
% InstructBLIP-7B~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}             & -                  & 57.6             \\
% InstructBLIP-7B~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}             & -                  & 62.1             \\
LLaVA-13B~\cite{liu2024visual}                    & -                  & 61.9             \\
% Flamin.-80B~\cite{alayrac2022flamingo}                & -                  & 57.8             \\
Minigpt4-v2-7B~\cite{chen2023minigpt}                & -                  & 57.8             \\
Minigpt4-v2-7B (FT)~\cite{chen2023minigpt}                & -                  & 61.9             \\
% Flamin.-80B~\cite{alayrac2022flamingo}                & -                  & 57.8             \\
PaLM-E-562B~\cite{driess2023palm}                 & -                  & 66.1             \\
GPT-4V~\cite{achiam2023gpt}                      & -                  & 64.3             \\ \midrule
\multicolumn{3}{c}{\textbf{\textit{Knowledge-enhanced Methods via GPT-3/4v APIs}}} \\
ReVIVE~\cite{DBLP:conf/nips/LinX0X0Y22}                      & -                  & 58.0             \\
Prophet~\cite{shao2023prompting}                     & -                  & 61.1             \\
Promptcap~\cite{hu2023promptcap}                   & -                  & 60.4             \\
FillingGap~\cite{wang2023filling}                  & -                  & 61.3             \\
MM-Reasoner~\cite{khademi2023mm}                 & -                  & 60.8             \\ \hline
\multicolumn{3}{c}{\textbf{\textit{Retrieval-augmented Generation Methods}}}                  \\
TRiG~\cite{DBLP:conf/cvpr/0013PTRWN22}                        & 45.8                  & 50.5             \\
RA-VQA~\cite{DBLP:conf/emnlp/LinB22}                      & 82.8               & 54.5             \\
TwO~\cite{si2023combo}                         & -                  & 56.7             \\
ReVeaL~\cite{DBLP:conf/cvpr/HuI0WCSSRF23}                      & -                  & 59.1             \\
FLMR~\cite{DBLP:conf/nips/LinCMCB23}                        & 89.3               & 62.1             \\
FLMR~\cite{DBLP:conf/nips/LinCMCB23} $\ast$                        & 88.3               & 62.7             \\
KSVQA~\cite{hao2024boter}                       & -                  & 62.8             \\
GeMKR~\cite{long2024generative} $\ast$                       & 78.6               & 61.8             \\ \midrule
\textbf{ReAuSE (Ours)}          & \textbf{92.6}               & \textbf{65.7}             \\ \bottomrule
\end{tabular}
\caption{Performance on the OKVQA benchmark. PPR@K applies only to RAG baselines; ``-" denotes inapplicability or unavailable results. ``$\ast$" indicates the results we reproduced using the official code and the same answer generator as our model.\protect\footnotemark}
\label{tab_mr_okvqa}
\end{table}

\footnotetext{We first use officially released checkpoints to obtain retrieved documents and then feed these documents into the answer generator (fine-tuned Minigpt4-v2) to acquire the corresponding answers.}
% \begin{table*}[!th]
% \centering
% \begin{tabular}{ccccc}
% \hline
% \# & Model                & Knowledge Source          & PRRecall@5     & VQA Score \\ \hline
% \multicolumn{5}{c}{\textbf{\textit{Multi-modal Large Language Models}}}                                  \\
% 1  & MiniGPT-4-v2-7B~\cite{chen2023minigpt}      & MiniGPT-4-v2 (7B)           & -               & 57.80     \\
% 2  & MiniGPT-4-v2-7B (FT)~\cite{chen2023minigpt} & MiniGPT-4-v2 (7B)           & -               & 61.87         \\
% 3  & InstructBLIP-7B~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}      & InstructBLIP (7B)           & -               & 57.6         \\
% 4  & InstructBLIP-7B (FT)~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & InstructBLIP (7B)           & -               & 62.1         \\
% 5  & LLaVA-13B~\cite{liu2024visual}            & LLaVA (13B)                 & -               & 61.93     \\
% 6  & Flamingo-80B~\cite{alayrac2022flamingo}         & Flamingo (80B)              & -               & 57.80     \\
% 7  & PaLM-E-562B~\cite{driess2023palm}        & PaLM-E (562B)             & -               & 66.10      \\
% 8  & GPT-4V~\cite{achiam2023gpt}       & GPT-4V          & -             & 64.28     \\ \hline
% \multicolumn{5}{c}{\textbf{\textit{Knowledge-enhanced Approaches via LLMs APIs}}}                        \\
% 9  & ReVIVE~\cite{DBLP:conf/nips/LinX0X0Y22}               & Wikidata+GPT-3            & -               & 58.0      \\
% 10 & Prophet~\cite{shao2023prompting}              & GPT-3                     & -               & 61.1      \\
% 11 & Promptcap~\cite{hu2023promptcap}            & GPT-3                     & -               & 60.4      \\
% 12 & FillingGap~\cite{wang2023filling}           & GPT-3                     & -               & 61.3      \\
% 13 & MM-Reasoner~\cite{khademi2023mm}          & GPT-4V                     & -               & 60.8      \\ \hline
% \multicolumn{5}{c}{\textbf{\textit{Retrieval-augmented Approaches}}}                                     \\
% 14 & TRiG~\cite{DBLP:conf/cvpr/0013PTRWN22}                 & Wikipedia+GPT-3           & -           & 50.50     \\
% 15 & RA-VQA~\cite{DBLP:conf/emnlp/LinB22}               & Google Search Corpus      & 82.84       & 54.48     \\
% 16 & TwO~\cite{si2023combo}                  & Wikipedia+OFA+VQAv2+GPT-3 & -           & 56.67     \\
% 17 & ReVeaL~\cite{DBLP:conf/cvpr/HuI0WCSSRF23}               & WIT + Wikidata            & -               & 59.10     \\
% 18 & FLMR~\cite{DBLP:conf/nips/LinCMCB23}                 & Google Search Corpus      & 89.32       & 62.08     \\
% 19 & KSVQA~\cite{hao2024boter}                & Google Search Corpus      & -               & 62.83     \\
% 20 & GeMKR~\cite{long2024generative}                & Google Search Corpus      & 78.6            & 61.83         \\ \hline
% 21 & ReAuSE (Our Model)   & Google Search Corpus      & 92.6        & 65.73     \\ \hline
% \end{tabular}
% \caption{Performance on the OK-VQA benchmark. TBD,TBD,TBD.Adjusting the bounding box instead of actually removing the unwanted data resulted multiple layers in this paper. It also needlessly increased the PDF size. In this case, the size of the unwanted layer doubled the paper's size, and produced the following surprising results in final production. Crop your figures properly in a graphics program. Don't just alter the bounding box.}
% \label{tab_mr_okvqa}
% \end{table*}

% \begin{table*}[!th]
% \centering
% \begin{tabular}{cccccc}
% \hline
% \# & Model                & Knowledge Source          & PRRecall@5 & EM    & VQA Score \\ \hline
% \multicolumn{6}{c}{\textbf{\textit{Multi-modal Large Language Models}}}                                  \\
% 1  & MiniGPT-4-v2-7B~\cite{chen2023minigpt}      & MiniGPT-4-v2 (7B)           & -          & -     & 57.80     \\
% 2  & MiniGPT-4-v2-7B (FT)~\cite{chen2023minigpt} & MiniGPT-4-v2 (7B)           & -          & -     & 61.87         \\
% 3  & InstructBLIP-7B~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}      & InstructBLIP (7B)           & -          & -     & 57.6         \\
% 4  & InstructBLIP-7B (FT)~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & InstructBLIP (7B)           & -          & -     & 62.1         \\
% 5  & LLaVA-13B~\cite{liu2024visual}            & LLaVA (13B)                 & -          & -     & 61.93     \\
% 6  & Flamingo-80B~\cite{alayrac2022flamingo}         & Flamingo (80B)              & -          & -     & 57.80     \\
% 7  & PaLM-E-562B~\cite{driess2023palm}        & PaLM-E (562B)             & -          & -     & 66.10      \\
% 8  & GPT-4V~\cite{achiam2023gpt}       & GPT-4V          & -          & -     & 64.28     \\ \hline
% \multicolumn{6}{c}{\textbf{\textit{Knowledge-enhanced Approaches via LLMs APIs}}}                        \\
% 9  & ReVIVE~\cite{DBLP:conf/nips/LinX0X0Y22}               & Wikidata+GPT-3            & -          & -     & 58.0      \\
% 10 & Prophet~\cite{shao2023prompting}              & GPT-3                     & -          & -     & 61.1      \\
% 11 & Promptcap~\cite{hu2023promptcap}            & GPT-3                     & -          & -     & 60.4      \\
% 12 & FillingGap~\cite{wang2023filling}           & GPT-3                     & -          & -     & 61.3      \\
% 13 & MM-Reasoner~\cite{khademi2023mm}          & GPT-4V                     & -          & -     & 60.8      \\ \hline
% \multicolumn{6}{c}{\textbf{\textit{Retrieval-augmented Approaches}}}                                     \\
% 14 & TRiG~\cite{DBLP:conf/cvpr/0013PTRWN22}                 & Wikipedia+GPT-3           & -          & 54.73 & 50.50     \\
% 15 & RA-VQA~\cite{DBLP:conf/emnlp/LinB22}               & Google Search Corpus      & 82.84      & 59.41 & 54.48     \\
% 16 & TwO~\cite{si2023combo}                  & Wikipedia+OFA+VQAv2+GPT-3 & -          & 61.32 & 56.67     \\
% 17 & ReVeaL~\cite{DBLP:conf/cvpr/HuI0WCSSRF23}               & WIT + Wikidata            & -          & -     & 59.10     \\
% 18 & FLMR~\cite{DBLP:conf/nips/LinCMCB23}                 & Google Search Corpus      & 89.32      & 62.01 & 62.08     \\
% 19 & KSVQA~\cite{hao2024boter}                & Google Search Corpus      & -          & -     & 62.83     \\
% 20 & GeMKR~\cite{long2024generative}                & Google Search Corpus      & 78.6       & ?     & 61.83         \\ \hline
% 21 & ReAuSE (Our Model)   & Google Search Corpus      & 92.6       & 70.75 & 65.73     \\ \hline
% \end{tabular}
% \caption{Performance on the OK-VQA benchmark. TBD,TBD,TBD.Adjusting the bounding box instead of actually removing the unwanted data resulted multiple layers in this paper. It also needlessly increased the PDF size. In this case, the size of the unwanted layer doubled the paper's size, and produced the following surprising results in final production. Crop your figures properly in a graphics program. Don't just alter the bounding box.}
% \label{tab_mr_okvqa}
% \end{table*}

\subsection{Main Results}
We compare our ReAuSE with the aforementioned baselines for knowledge-based VQA tasks in Tab.~\ref{tab_mr_okvqa} and Tab.~\ref{tab_mr_aokvqa}. The experimental results illustrate
that ReAuSE achieves significant improvements over the competitive baselines on the challenging OKVQA and A-OKVQA datasets.

From Tab.~\ref{tab_mr_okvqa}, we can observe that ReAuSE outperforms the competitive baseline FLMR on both retrieval and VQA metrics, which consistently demonstrates the effectiveness of our method in integrating both knowledge retrieval and answer generation into a unified multi-modal large language model framework. 
ReAuSE achieves an advanced VQA score on OKVQA when compared to models with similar parameter scales, surpassing the previous best retrieval-augmented method by more than 2.9\% and outperforming methods that use LLM-APIs for knowledge enhancement by 4.6\%. Moreover, our method exceeds GPT-4V by 1.45\% in VQA score. Even compared with the closed-source PALM-E-562B, which is over 80 times larger than ours, our method is only 0.5\% behind.
% Compared to models with similar parameter scales, ReAuSE achieves
% the state-of-the-art VQA score on OKVQA, surpassing the previous best retrieval-augmented method by
% more than 2.9\% and outperforming methods
% that use LLMs API for knowledge enhancement by 4.6\%.
% Even compared to GPT-4V, our method exceeds it by 1.45\% in VQA score. When compared to the closed-source PALM-E, which is over 80 times larger, our method is only 0.5\% behind.

The OKVQA benchmark poses a challenging issue of 
retrieving relevant knowledge from extensive knowledge bases or directly generating useful information about multi-modal contexts. 
Despite using GPT-3 or GPT-4V to acquire knowledge or directly adopting GPT-3 as the backbone, MM-Reasoner and FillingGap fail to achieve obvious improvements compared to retrieval-augmented methods. In contrast, retrieval-augmented methods, such as FLMR and KSVQA, achieve better VQA performance by incorporating manually designed feature engineering and integrating multiple retrievers and selectors.

\begin{table}[]
\small
\centering
\begin{tabular}{lcccc}
\toprule
\setlength{\tabcolsep}{5pt}
\multirow{2}{*}{Models} & \multicolumn{2}{c}{Multi-Choice} & \multicolumn{2}{c}{Direct-Answer} \\
                        & val              & test             & val             & test            \\ \hline
LLaVA-1.5-7B            & 77.1             & 74.5             & 63.7            & 58.6            \\
InstructBLIP-7B(FT)         & 73.0             & 71.1             & 62.4            & 58.7            \\
Minigpt4-v2-7B(FT)        & -                & -                & 61.3               & -               \\
GPV-2                   & 60.3             & 53.7             & 48.6            & 40.7            \\
PromptCap               & 73.2             & 73.1             & 56.3            & 59.6            \\
Prophet                 & 76.4             & 73.6             & 58.2            & 55.7            \\
FillingGap              & -                & -                & 59.8            & -               \\
SimVQA                  & -                & -                & 58.6            & 57.5            \\
REVEAL                  & -                & -                & 52.2            & -               \\
Cola-FT                 & 78.1             & 76.7             & -               & -               \\
CKR-VQA                 & 76.2             & 75.4             & 58.1            & 60.1            \\ \midrule
\textbf{ReAuSE (Ours)}      & \textbf{85.0}                & \textbf{80.3}            & \textbf{67.7}               & \textbf{65.8}           \\ \bottomrule
\end{tabular}
\caption{Performance on the A-OK-VQA benchmark.}
\label{tab_mr_aokvqa}
\end{table}

From Tab.~\ref{tab_mr_aokvqa}, 
ReAuSE demonstrates more significant performance improvements on A-OKVQA, with accuracy and VQA scores increasing by 4.9\% to 9.6\% compared to baselines of similar parameter scales\footnote{See the submission at: \url{https://leaderboard.allenai.org/a-okvqa/submission/cqp56m03c8g0k0quidj0}}.
Our approach demonstrates consistent improvements, which can be attributed to two key factors. First, we leverage large language models as virtual knowledge bases by replacing traditional discriminative pipelines with generative retrievers. Second, we implement reinforced retrieval calibration to align the search engine with the answer generator, enabling the retriever to incorporate relevance feedback for refinement, thereby yielding more relevant results.
In the following sections, we will examine the performance of the autoregressive search engine and analyze the impact of search results on the answer generation process.



\begin{table}[t]
\small
\centering
\begin{tabular}{lcc}
\toprule
Ablation Setting                        & PRRecall@5 & Score \\ \midrule
\textbf{Full Model (Ours)}                            & \textbf{92.6}          & \textbf{65.7}      \\ \midrule
\textit{w/o} Search Engines          & -          & 61.9         \\
\textit{w/o} Fine-tuning Search Engine         & 33.1          & 61.6         \\
\textit{w/o} Constrained Decoding              & -          & 63.2         \\
\hline
\textit{w/o} Retrieval Calibration & 88.7          & 62.5         \\
\textit{w/o} VQA Reward Model                 & 91.0          & 63.3         \\
\textit{w/o} EM Reward Func.                  & 89.9          & 64.5         \\
\textit{w/o} Sim. Reward Model          & 91.7          & 65.3         \\ \bottomrule
\end{tabular}
\caption{Ablation Studies. \textit{w/o} denotes ``without".}
\label{tab_as}
\end{table}


\subsection{Ablation Study}
We conduct a series of ablation studies by gradually removing each module of our framework and the corresponding
results are presented in Tab.~\ref{tab_as}.

To evaluate the impact of retrieval augmentation, we first remove the built-in autoregressive search engine, using the MLLM as an answer generator without access to external knowledge. This operation results in a 3.8\% decrease in the VQA score, indicating that external knowledge retrieval is crucial for knowledge-based VQA tasks.
% Next, we disable the constrained decoding strategy, allowing the MLLM to generate image-related knowledge without restrictions.
% This approach led to a 2.5\% decline in the VQA score, likely due to the MLLM generating erroneous or hallucinated content, which contributed to inaccurate outputs from the answer generator.
% To assess the impact of retrieval augmentation, we first removed the built-in autoregressive search engine, which is equivalent to directly using MLLM as an answer generator without external knowledge.
% We can observe a decrease of 3.8\% in VQA score without the search engine, which suggests that retrieving external knowledge is crucial in knowledge-based VQA tasks.
Next, if we do not supervised fine-tune the MLLMs, it cannot effectively serve as a generative search engine to retrieve knowledge from the KB.
% Furthermore, we disable the constrained decoding strategy, allowing the MLLM to freely generate image-related knowledge.
% However, because the free-style content cannot be linked to the document in the KB, it is directly used as external knowledge to support the answer generator. 
Moreover, we disable the constrained decoding strategy, allowing the MLLM to generate image-related knowledge without restrictions. However, since this freely generated content cannot be linked to the document in the KB, it is used directly as external knowledge to support the answer generation process.
This approach leads to a 2.5\% decrease in the VQA score, likely due to the MLLM producing erroneous or hallucinated information, which results in inaccurate outputs from the answer generator.

To evaluate the effectiveness of the Reinforced Retrieval Calibration (RRC) module, we employ the generative search engine after supervised fine-tuning but remove the reinforced calibration module. We observe a 3.9\% decrease in retrieval performance, which is slightly below that of the strongest baseline, FLMR. This suggests that the autoregressive retriever can be further optimized through the RRC module by leveraging relevance feedback from reward models. 
Furthermore, we disable each reward model to assess its effectiveness. We find that the VQA reward model enables the generative retriever to retrieve documents that align with the answer generator's preferences, thereby improving VQA performance. Conversely, the EM reward model ensures that the generated identifiers include answer keywords, leading to enhanced retrieval performance.




\begin{table*}[!t]
\small
\centering
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\#} & \multirow{2}{*}{Retrievers} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}OKVQA- GS112K\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}OKVQA- WK21M\end{tabular}} & InfoSeek-100K \\
                    &                             & PRRecall@5                          & PRRecall@10                          & PRRecall@5                          & PRRecall@10                         & PRRecall@5    \\ \midrule
1                   & DPR~\cite{karpukhin2020dense}                       & 83.4                               & 90.3                                & 66.9                               & 76.4                               & -             \\
2                   & RA-VQA~\cite{DBLP:conf/emnlp/LinB22}                      & 82.8                               & 89.0                                & -                                   & -                                   & -             \\
3                   & ReViz-ICT~\cite{luo2023end}                   & 73.4                                & 83.2                                 & 61.9                                & 72.6                                & -             \\
4                   & GeMKR~\cite{long2024generative}                       & 78.6                                & 86.2                                 & 70.8                                & 79.1                                & 48.9             \\
5                   & FLMR~\cite{DBLP:conf/nips/LinCMCB23}                        & 89.3                               & 94.0                                & 68.1                               & 78.0                               & 47.1         \\
6                   & Pre-FLMR~\cite{lin2024preflmr}                    & -                                   & -                                    & 68.6                                & -                                   & 57.8         \\ \midrule
7                   & \textbf{ReAuSE (Ours)}                      & \textbf{92.6}                                   & \textbf{95.8}                                    & \textbf{88.0}                                   & \textbf{91.3}                                   & \textbf{59.5}             \\ \bottomrule
\end{tabular}
\caption{Retrieval Performance on Three Retrieval Corpora.}
\label{rs_retr}
\end{table*}
%The text retrieval baseline DPR model enhances cross-modal understanding capabilities by obtaining textual descriptions of images, such as OCR, captions, objects, and attributes.
% \section{Analysis}

\subsection{Effects of Retrieval Performance}
% To evaluate our model's ability to retrieve knowledge from large-scale knowledge bases and to explore how our answer generator benefits from retrieval, we conducted retrieval experiments for the OKVQA dataset on two retrieval corpora Google Search (GS112K)~\cite{DBLP:conf/emnlp/LuoZBB21} and Wikipedia (Wiki21M)~\cite{karpukhin2020dense}, and introduce an additional dataset, Infoseek~\cite{DBLP:conf/emnlp/ChenHLSCRC23}, with the size of the knowledge bases ranging from 12K to 21M entries.
To assess our model's capability in retrieving knowledge from large-scale knowledge bases, we conduct experiments on the OKVQA dataset using two retrieval corpora: Google Search (GS112K)~\cite{DBLP:conf/emnlp/LuoZBB21} and Wikipedia (Wiki21M)~\cite{karpukhin2020dense}, with knowledge bases ranging in size from 112K to 21M documents. 
Additionally, we introduce a new dataset, Infoseek~\cite{DBLP:conf/emnlp/ChenHLSCRC23}, consisting of 100K documents, which poses challenges for visual entity retrieval.
As shown in Tab.~\ref{rs_retr},
our proposed approach consistently outperforms the leading
state-of-the-art baselines FLMR and Pre-FLMR across all evaluated metrics.
Specifically, our model outperforms FLMR by 3.3\% in PRRecall@5 on the GS112K corpus. 
This improvement explains why our answer generation model surpasses the FLMR model by 3.6\% in the VQA score, as shown in Tab.~\ref{tab_mr_okvqa}.
Moreover, our method outperforms FLMR by 10.6\% on the Infoseek dataset and surpasses PreFLMR by 1.7\%, indicating the effectiveness of ReAuSE in handling visual entity retrieval tasks.

We observe a significant performance drop of over 20\% in the FLMR model when applied to the Wiki21M corpus, while our model exhibits only a 4.6\% decrease. This indicates that our model demonstrates stronger generalization capabilities for retrieving from large-scale corpora.
This can be attributed to the advantages of generative search engines, which generate document identifiers through token-level search rather than relying on one-to-one matches at the document level (i.e., document-level search).
Although the number of documents increases, the size of the token set (i.e., vocabulary) does not expand proportionally. Consequently, generative search engines are less affected by the scale of the knowledge base, whereas the performance of discriminative methods degrades as the corpus size increases.
% Additionally, we observe a significant performance drop of over 20\% in the FLMR model when applied to the Wiki21M corpus, whereas our model shows only a 4.6\% decrease. This suggests that our model possesses stronger generalization for retrieving from large-scale corpora.
% This phenomenon can be attributed to the advantage of generative search engines, which generate document identifiers via token-level search rather than conducting one-to-one matches within the retrieval corpus (i.e., document-level search). 
% Although the number of documents increases, the size of the token set (i.e., vocabulary) does not expand proportionally. 
% As a result, generative search engines are less impacted by the scale of the knowledge base, whereas performance of discriminative methods degrade as corpus size increases.


% \begin{table}[]
% \centering
% \begin{tabular}{lc}
% \toprule
% MLLMs          & VQA Score \\ \midrule
% Minigpt4-v1-7B~\cite{zhu2023minigpt} (FT)      & 60.3     \\
% Minigpt4-v2-7B~\cite{chen2023minigpt} (FT)      & 61.9      \\
% InstructBLIP-7B~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}  (FT)      & 62.1      \\ \midrule
% \textbf{Our Model}      & \textbf{65.7}      \\ \midrule
% \textit{w/} MiniGPT-4-v1~\cite{zhu2023minigpt} & 63.7         \\
% \textit{w/} BLIP-2~\cite{li2023blip}       & 64.2         \\
% \textit{w/} InstructBLIP~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & 64.5         \\
%  \bottomrule
% \end{tabular}
% \caption{Effects of Different MLLMs. \textit{w/} denotes ``with"}
% \label{tab_mllms}
% \end{table}

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{./fig3_modify.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Case Studies. The highlighted text represents the document identifier generated by our model.}
\label{fig3}
\end{figure}

% \subsection{Effects of Different MLLMs}
% As shown in Tab~\ref{tab_mllms}, we demonstrate the common MLLMs employed by both our method and the baselines. We then fine-tune these models and evaluate their performance without retrieval augmentation, serving as reference values.
% We sequentially replace our fundamental model (Minigpt-4-v2) with Minigpt-4-v1 (used by GeMKR), BLIP-2 (used by FLMR), and InstructBLIP, and display the VQA scores in the Tab.~\ref{tab_mllms}.

% Although differences in performance exist among various MLLMs, these disparities are relatively minor.
% Notably, our method still outperforms other baselines in Tab.~\ref{tab_mr_okvqa}, even when we utilize weaker MLLMs. 
% Moreover, the comparison with the base model without retrieval augmentation reveals significant performance gains when leveraging the built-in autoregressive search engine to facilitate retrieval augmentation.
% Even when using weaker MLLMs, our method still obviously outperforms the performance of other baselines in Tab.~\ref{tab_mr_okvqa}.
% Furthermore, compared to the base model without retrieval augmentation, we achieve significant performance improvements by leveraging the built-in autoregressive search engine for retrieval augmentation.

\subsection{Efficiency}
\begin{table}[]
\small
\centering
\begin{tabular}{cccc}
\toprule
  \textbf{Models}  & TopK & GS112K  & Wiki21M \\ \midrule
DPR & 5 & 370.4ms & 518.4ms \\
FLMR & 5  & 758.4ms & - \\
ReAuSE (Ours) & 5 & 751.3ms & 962.1ms \\ 
% DPR~\cite{karpukhin2020dense} & 10 & 370.4ms & 518.4ms \\
% FLMR~\cite{karpukhin2020dense} & 10  & 758.4ms & - \\
ReAuSE (Ours) & 10 & 1023.3ms & 1273.2.1ms \\ 
\bottomrule
\end{tabular}
\caption{The Retrieval Time in the Inference Stage.}
\label{tab:ie}
\end{table}
ReAuSE requires fewer resources than other retrieval-augmented baselines.
ReAuSE unifies three stages into a single MLLM, whereas other baselines require an MLLM for generation and at least one traditional retriever for retrieval. 
ReAuSE uses LoRA fine-tuning, requiring 9K (OKVQA) to 17K (A-OKVQA) training data to train 0.49\% of the parameters, with both SFT and RRC completing within 3 hours across 4 GPUs. In contrast, traditional retrievers such as PreFLMR and ReViz-ICT necessitate additional millions of data for full-scale fine-tuning.

We record the inference time of ReAuSE, FLMR, and the most efficient baseline, DPR to provide a qualitative result. As shown in Tab.~\ref{tab:ie}, ReAuSE has comparable efficiency to traditional retrievers. 
ReAuSE generates top-K document identifiers with $l$ tokens for each query through a $l$-steps decoding ($l=10$). In contrast, for each query, traditional retrievers need to calculate the similarity with many documents. 
% We also record the inference time of ReAuSE and the most efficient baseline, DPR (with FAISS) in Tab.~\ref{tab:ie}. 
% Given the nearly 20\% improvement in PRRecall@5 over DPR, we contend that the 440-millisecond latency of ReAuSE is acceptable.
When TopK=5, our model is only 440 milliseconds slower than DPR. Considering the nearly 20\% performance improvement, we argue that such latency is acceptable.
% Considering the nearly 20\% improvement in PRRecall@5 compared to DPR, we argue that the 440-millisecond latency of ReAuSE is acceptable.

% \noindent \textbf{Model Efficiency.} We unify retrieval, alignment, and generation into a single MLLM. Other RAG baselines require an MLLM for generation and at least one traditional retriever for retrieval, whereas ReAuSE uses fewer models than theirs.

% \noindent \textbf{Training Efficiency.} We employ LoRA fine-tuning, requiring 9K (OKVQA) to 17K (A-OKVQA) training data to train 0.49\% of the parameters, with both SFT and RRC completing within 3 hours across 4 GPUs. In contrast, traditional retrievers like PreFLMR necessitate additional millions of data for full-scale fine-tuning. Consequently, our model training is more efficient.

% \noindent \textbf{Inference Efficiency.} The generative retriever and traditional retriever have comparable efficiency. ReAuSE generates only one fixed-length $l$ document identifier (l=10 in our paper) for each query. The efficiency does not obviously change with the number of documents (see Appendix B.1 for derivation). In contrast, traditional retrievers require calculating the similarity between the query vector and the vectors of multiple documents. The efficiency decreases as the number of documents increases. To provide a qualitative analysis, we measured the inference time of our model and DPR (with FAISS) when retrieving from the GS112K and Wiki21M corpora.

\subsection{Case Study}
% Finally, we perform a qualitative analysis to gain an intuitive understanding of the proposed ReAuSE. 
As illustrated in Fig.~\ref{fig3}, ReAuSE accurately generates the correct answers for all three samples. 
ReAuSE directly generates document identifiers associated with the image-text pairs using its built-in search engine. 
Each document identifier is a sequence of tokens representing a document, and it can be linked to a corresponding document that potentially contains information to answer the given question.
% Document identifiers are sequences of tokens representing a document and can be linked to documents that contain information to answer the given questions.
% % capable of answering the given questions. 
% Document identifiers are sequences of tokens representing a document and can occur anywhere within it. 
% For varying question-image pairs, the same document may be represented by distinct document identifiers. 
% Interestingly, in the examples depicted in Fig.~\ref{fig3}, 
What's more, we observe that all generated document identifiers contain answer keywords, suggesting that generated document identifiers are highly relevant to the question.
% Better performance of the answer generation model is achieved when the retrieved documents are more relevant.



\section{Conclusion}
In this paper, we introduce ReAuSE, a novel KBVQA approach by integrating knowledge retrieval and generation within a unified generative multi-modal large language model (MLLM) framework. Extensive experimental results have shown that ReAuSE consistently outperforms existing methods, achieving significant improvements across various evaluation metrics on two benchmarks. 
% Our proposed model, ReAuSE, presents a novel approach to knowledge-based VQA, seamlessly integrating knowledge retrieval within a generative MLLM. This model has demonstrated superior performance compared to existing methods, achieving notable improvements across various evaluation metrics on OKVQA and A-OKVQA benchmarks. %The effectiveness of ReAuSE highlights the potential of integrating retrieval and generation within a unified framework for knowledge-intensive VQA tasks. 
Future work will focus on extending the application of ReAuSE to domains such as biomedicine and education~\cite{gao2021rcd}.
%, exploring its potential to address complex questions in diverse fields.





% \begin{tabular}{cccccc}
% \hline
% \# & Model                & Knowledge Source          & PRRecall@5 & EM    & VQA Score \\ \hline
% \multicolumn{6}{c}{\textbf{\textit{Multi-modal Large Language Models}}}                                  \\
% 1  & MiniGPT-4-v2-7B      & MiniGPT-4-v2-7B           & -          & -     & 57.80     \\
% 2  & MiniGPT-4-v2-7B (FT) & MiniGPT-4-v2-7B           & -          & -     & -         \\
% 3  & InstructBLIP-7B      & InstructBLIP-7B           & -          & -     & -         \\
% 4  & InstructBLIP-7B (FT) & InstructBLIP-7B           & -          & -     & -         \\
% 5  & LLaVA-13B            & LLaVA-13B                 & -          & -     & 61.93     \\
% 6  & Flamingo-80B         & Flamingo-80B              & -          & -     & 57.80     \\
% 7  & PaLM-E-562B       & PaLM-E (562B)             & -          & -     & 66.1      \\
% 8  & GPT-4V       & GPT-4V            & -          & -     & 64.28     \\ \hline
% \multicolumn{6}{c}{\textbf{\textit{Knowledge-enhanced Approaches via LLMs APIs}}}                        \\
% 9  & ReVIVE               & Wikidata+GPT-3            & -          & -     & 58.0      \\
% 10 & Prophet              & GPT-3                     & -          & -     & 61.1      \\
% 11 & Promptcap            & GPT-3                     & -          & -     & 60.4      \\ \hline
% \multicolumn{6}{c}{\textbf{\textit{Retrieval-augmented Approaches}}}                                     \\
% 12 & TRiG                 & Wikipedia+GPT-3           & -          & 54.73 & 50.50     \\
% 13 & RA-VQA               & Google Search Corpus      & 82.84      & 59.41 & 54.48     \\
% 14 & TwO                  & Wikipedia+OFA+VQAv2+GPT-3 & -          & 61.32 & 56.67     \\
% 15 & ReVeaL               & WIT + Wikidata            & -          & -     & 59.10     \\
% 16 & FLMR                 & Google Search Corpus      & 89.32      & 62.01 & 62.08     \\
% 17 & GeMKR                & Google Search Corpus      & 78.6       & ?     & ?         \\ \hline
% 18 & ReAuSE (Our Model)   & Google Search Corpus      & 92.6       & 70.75 & 65.73     \\ \hline
% \end{tabular}





% \subsubsection{Appendices.}
% Any appendices must appear after the main content. If your main sections are numbered, appendix sections must use letters instead of arabic numerals. In \LaTeX{} you can use the \texttt{\textbackslash appendix} command to achieve this effect and then use \texttt{\textbackslash section\{Heading\}} normally for your appendix sections.


% \subsubsection{Algorithms.}
% Algorithms and/or programs are a special kind of figures. Like all illustrations, they should appear floated to the top (preferably) or bottom of the page. However, their caption should appear in the header, left-justified and enclosed between horizontal lines, as shown in Algorithm~\ref{alg:algorithm}. The algorithm body should be terminated with another horizontal line. It is up to the authors to decide whether to show line numbers or not, how to format comments, etc.

% In \LaTeX{} algorithms may be typeset using the {\tt algorithm} and {\tt algorithmic} packages, but you can also use one of the many other packages for the task.

% \begin{algorithm}[tb]
% \caption{Example algorithm}
% \label{alg:algorithm}
% \textbf{Input}: Your algorithm's input\\
% \textbf{Parameter}: Optional list of parameters\\
% \textbf{Output}: Your algorithm's output
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Let $t=0$.
% \WHILE{condition}
% \STATE Do some action.
% \IF {conditional}
% \STATE Perform task A.
% \ELSE
% \STATE Perform task B.
% \ENDIF
% \ENDWHILE
% \STATE \textbf{return} solution
% \end{algorithmic}
% \end{algorithm}
\section{Acknowledgments}
This work was supported by the National Key Research and Development Program of China (2022ZD0160603), the NSFC (No. 62406161), CPSF (No. 2023M741950), and the Postdoctoral Fellowship Program of CPSF (No. GZB20230347).
\bibliography{aaai25}
% \clearpage
\appendix
\clearpage
% \section{Reproducibility Checklist}

% This paper:
% \begin{itemize}
%     \item Includes a conceptual outline and/or pseudocode description of AI methods introduced (\textbf{yes})
%     \item Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (\textbf{yes})
%     \item Provides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (\textbf{yes})
% \end{itemize}

% Does this paper make theoretical contributions? (\textbf{yes})

% If yes, please complete the list below.
% \begin{itemize}
%     \item All assumptions and restrictions are stated clearly and formally. (\textbf{yes})
%     \item All novel claims are stated formally (e.g., in theorem statements). (\textbf{yes})
%     \item Proofs of all novel claims are included. (\textbf{yes})
%     \item Proof sketches or intuitions are given for complex and/or novel results. (partial)
%     \item Appropriate citations to theoretical tools used are given. (\textbf{yes})
%     \item All theoretical claims are demonstrated empirically to hold. (\textbf{yes})
%     \item All experimental code used to eliminate or disprove claims is included. (\textbf{yes})
% \end{itemize}

% Does this paper rely on one or more datasets? (\textbf{yes})

% If yes, please complete the list below.
% \begin{itemize}
%     \item A motivation is given for why the experiments are conducted on the selected datasets (\textbf{yes})
%     \item All novel datasets introduced in this paper are included in a data appendix. (NA)
%     \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{yes})
%     \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations. (\textbf{yes})
%     \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available. (\textbf{yes})
%     \item All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (NA)
% \end{itemize}

% Does this paper include computational experiments? (\textbf{yes})

% If yes, please complete the list below.
% \begin{itemize}
%     \item Any code required for pre-processing data is included in the appendix. (\textbf{yes}).
%     \item All source code required for conducting and analyzing the experiments is included in a code appendix. (\textbf{yes})
%     \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{yes})
%     \item All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (\textbf{yes})
%     \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (NA)
%     \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (\textbf{yes})
%     \item This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (\textbf{yes})
%     \item This paper states the number of algorithm runs used to compute each reported result. (no)
%     \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (\textbf{yes})
%     \item The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (no)
%     \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments. (\textbf{yes})
%     \item This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (\textbf{yes})
% \end{itemize}

% \clearpage
% \section{Technical Details}
% \subsection{FM-Index Technique}
% FM-Index~\cite{ferragina2000opportunistic} is a compact data
% structure used in string searching and pattern-matching algorithms.
% It is based on the Burrows-Wheeler Transform (BWT)~\cite{fenwick1996burrows} and the concept of the suffix array.
% The FM-Index efficiently compresses the information from the original text while still allowing for rapid pattern searches and substring queries.
% Moreover, the memory usage of FM-Index is linear in the size of the corpus with a small vocabulary table. The time complexities for obtaining the next allowed token is nearly $\mathcal{O}(V)$, which is efficient, as V is the size of the vocab file, independent of the size of the knowledge base.
% Please refer to~\cite{ferragina2000opportunistic} for mathematical derivations.

% The Succinct Data Structure Library (SDSL)~\cite{gbmp2014sea} provides a powerful and flexible C++ 11 implementation for FM-Index. SDSL provides a series of interfaces for adding, deleting, modifying, and querying FM-Index databases. The primary interfaces utilized in our method are as follows:
% \begin{itemize}
% \item \textbf{\textit{getNext($\cdot$)}} uses the previously generated sequence $R_i^{t-1}=\{r_1,...,r_{t-1}\}$ as the prefix condition to search for all matching strings in the knowledge base.
% It then extracts the subsequent tokens from these strings to form a allowable token set $\mathcal{S}$ as the return value.
% \item 
% \textbf{\textit{lookupDoc($\cdot$)}} uses the generated document identifier as the input to search for the matching document in the knowledge base. It returns the document corresponding to the identifier as the return value.
% \end{itemize}

% The execution times for FM-Index database interfaces are as follows:

% \begin{table}[h]
% \centering
% \begin{tabular}{ccc}
% \toprule
% FM-Index Calls & Time (GS112K) & Time (WK21M)\\ \midrule
% \textit{getNext($\cdot$)}        & $2.7\times 10^{-4}$ s &  $6.1\times 10^{-4}$ s    \\
% \textit{lookupDoc($\cdot$)}      & $3.6\times 10^{-5}$ s  &  $5.8\times 10^{-5}$ s   \\ \bottomrule
% \end{tabular}
% \caption{Execution Time of FM-Index Database Interfaces on Our Environments.}
% \label{tb_rt}
% \end{table}

% As shown in the Tab.~\ref{tb_rt}, execution times for all interfaces are less than 1 millisecond. 
% Despite the WK21M corpus being 200 times larger than the GS112K corpus, their query efficiency shows no significant difference. The vocabulary size of the GS112K corpus is 17,232, while that of the WK21M corpus is 31,182. Although the WK21M corpus is substantially larger, its distinct token count (vocabulary size) has only increased by a factor of 1.8. Consequently, the query efficiency in the FM-Index database does not differ significantly between the two corpora.


% \subsection{Constrained Decoding}
% To ensure that the generated identifier appears at least once within a document in the knowledge base, we implement a constrained decoding strategy that guides the decoder in generating content within a restricted token space at each step, as shown in Alg.~\ref{alg:algorithm}. The algorithm iteratively invokes the \textit{getNext} and \textit{ConstrainedBeamSearch} functions, ultimately obtaining an identifier of length $l$.

% \begin{algorithm}[tb]
% \textbf{Input}: X\\
% \textbf{Parameter}: Fixed length l\\
% \textbf{Output}: DocIds $R_i$
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Let $t=0$, $R_i\leftarrow [ ]$ .
% \WHILE{$t<l$}
% \STATE $\mathcal{S}\leftarrow\{\}$. \# next allowed token set $\mathcal{S}$
% \IF {t $\neq$ 0}
% \STATE $\mathcal{S}\leftarrow getNext(R_i)$
% \ELSE
% \STATE $\mathcal{S}\leftarrow V$. \# all distinct tokens
% \ENDIF
% \STATE $R_i\leftarrow ConstrainedBeamSearch(R_i, \mathcal{S})$
% \ENDWHILE
% \STATE \textbf{return} $R_i$
% \end{algorithmic}
% \caption{KB-guided Constrained Decoding Algorithm}
% \label{alg:algorithm}
% \end{algorithm}

% The \textit{ConstrainedBeamSearch} method, implemented by GENRE~\cite{deautoregressive}, has been integrated into the transformer library. This approach constructs a binary mask based on the set $\mathcal{S}$ of allowable next tokens, assigning a value of 1 to tokens in $\mathcal{S}$ and 0 to others. The mask is then applied to the language model's predicted vocabulary distribution, effectively nullifying the probabilities of tokens outside $\mathcal{S}$. This process ensures that subsequent token selection is confined to set $\mathcal{S}$. For further technical details, please refer to the transformer library documentation.%~\footnote{\url{https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate}}


% \section{Experimental Details}
% \subsection{Dataset Settings}
% \begin{table}[h]
% \centering
% \begin{tabular}{ccccccc}
% \toprule
% Dataset / KB      & Train / Val / Test & KB size  \\ \midrule
% OKVQA / GS112K     & 8,062/896/5,046 & 112,724                  \\
% OKVQA / WK21M    & 8,062/896/5,046 & 21,015,324               \\
% A-OKVQA / WK21M    & 17,056/1,145/6,702 & 21,015,324               \\
% InfoSeek / Pre-FLMR            & 100,000/ - /4,708 & 100,000                   \\
% \bottomrule
% \end{tabular}
% \caption{Dataset statistics.}
% \label{tab_ds}
% \end{table}

% As shown in the Tab.~\ref{tab_ds}, we present the scale of each dataset and knowledge base.
% We focus on the knowledge-based VQA benchmarks, OKVQA~\cite{DBLP:conf/cvpr/MarinoRFM19} and A-OKVQA~\cite{DBLP:conf/eccv/SchwenkKCMM22}. 
% Previous work provided two retrieval corpora, GS112K~\cite{DBLP:conf/emnlp/LuoZBB21} and Wiki21M~\cite{karpukhin2020dense}, for the OKVQA dataset. GS112K contains 112K passages collected through Google Search, while Wiki21M is a subset of Wikipedia, containing 21M Wikipedia entries.
% Moreover, we also conduct retrieval experiments on these two corpora and introduce a new information-seeking dataset, InfoSeek~\cite{DBLP:conf/emnlp/ChenHLSCRC23}, to evaluate the model's retrieval performance. Since InfoSeek's KB is not publicly available, we use the KB provided by PreFLMR~\cite{lin2024preflmr} and follow the same experimental setup.
% \subsection{Evaluation Metrics}
% We strictly follow the settings of the original papers, using the corresponding metrics for each dataset.
% For OKVQA dataset and the ``direct answer" setting of A-OKVQA dataset, we use the VQA score~\cite{Antol_2015_ICCV} to evaluate the model's performance, as Eq.~\ref{eq_vqascore},
% \begin{equation}
%   VQAscore(y,A) = {\rm min}\Bigg(\frac{\#A(y)}{3}, 1\Bigg).
%   \label{eq_vqascore}
% \end{equation}
% where $y$ denotes the predicted answer, and $\#A(y)$ represents the occurrence of $y$ in the human-annotated answer set $A$.

% For the ``multi-choice" setting of A-OKVQA dataset, the model chooses its answer from one of four options and we
% compute accuracy as the evaluation metric, consistent with the resource paper~\cite{DBLP:conf/eccv/SchwenkKCMM22}.

% As mentioned by VRR~\cite{DBLP:conf/emnlp/LuoZBB21}, common retrieval metrics such as Recall are based on the assumption that any retrieved knowledge that contains any of the answers annotated in the OK-VQA dataset is relevant. This assumption is made because it is unknown which knowledge is relevant to a question-image pair.
% Consequently, we replace the Recall@K with the Pseudo-relevance Recall@K (PRR@K), which assesses whether the top K retrieved documents contain at least one human-annotated answer, aligning with the baselines.~\cite{DBLP:conf/emnlp/LuoZBB21,DBLP:conf/nips/LinX0X0Y22,DBLP:conf/nips/LinCMCB23,lin2024preflmr}, as Eq.~\ref{eq_prr},
% % To evaluate the performance of knowledge retrieval, we use the Pseudo-relevance Recall@K (PRR@K)~\cite{DBLP:conf/emnlp/LuoZBB21}, consistent with the baselines, as Eq.~\ref{eq_prr},
% \begin{equation}
%   PRR@K(A, D) = {\rm min}\Bigg(\sum_{i=1}^{K} \mathbb{I}(D_i, A), 1\Bigg).
%   \label{eq_prr}
% \end{equation}
% where $A$ is the answer set, and $\mathbb{I}$ is the indicator function that evaluates to 1 if the retrieved document $D_i$ contains any answer in $A$, and 0 otherwise.

% \subsection{Hyper-parameter Settings}
% In training the generative knowledge retriever, we use learning rate $5\times10^{-5}$, batch size 24, gradient accumulation steps 2 for 12.5k steps.
% When refining retrieval performance through the reinforced retrieval calibration module, we use learning rate $5\times10^{-6}$, batch size 4, gradient accumulation steps 4 for 10k steps. For the hyper-parameters in the DPO algorithm, we borrow the default values from the Transformer Reinforcement Learning library~\cite{vonwerra2022trl}.
% To traing the retrieval-augmented answer generator, we use learning rate $5\times10^{-5}$, batch size 16, gradient accumulation steps 2 for 7k steps.

% During inference, we employ a $beam\_size$ of 10 to generate document identifiers. The generative retriever products $beam\_size$ document identifiers, each of a fixed length $l=10$. These identifiers are mapped to corresponding documents within the knowledge base (KB), with the top 5 documents being utilized as external knowledge for the retrieval-augmented answer generation.




% \subsection{Development Environments}
% Our ReAuSE requires no more than 3 GPU hours on 4 Nvidia A6000 (40G) for each training stage. 
% During Inference, our model also supports the Nvidia 3090 GPU, with a memory usage of only 16GB.
% Our model is implemented in PyTorch, utilizing version 12.2 of CUDA, version 0.3.0 of the PEFT
% library and version 4.28.0 of the transformers.

% \section{Extended Qualitative Analysis}
% We conduct extended qualitative analysis to reveal how our ReAuSE works, including the following aspects: 
% 1) Demonstration of the employed prompt templates.
% 2) Acquisition of sampled document identifiers for supervised fine-tuning. 
% 3) Workflow of knowledge retrieval via built-in autoregressive search engines.
% 4) Illustrative cases of retrieval-augmented answer generation.

% \subsection{Prompt Templates}
% Tab~\ref{tab:instruction_format_1} presents the prompt templates utilized in our model, wherein $<$ImageHere$>$ and $\{*\}$ denote placeholders for image and text content, respectively. The structural similarity between the templates for both tasks, differing primarily in their instructions, facilitates the integration of knowledge retrieval and answer generation within a unified MLLM.

% \begin{table*}[htbp]
% % \setlength{\tabcolsep}{2.5pt}
% \small
% \centering
% \begin{tabular}{p{15cm}l}
% \toprule
% \rowcolor{gray!20} \textit{Knowledge Retrieval via Built-in Autoregressive Search Engines} \\
% \midrule
% $<$Img$>$ $<$ImageHere$>$ $<$/Img$>$ \\ 
% Question: \{Name the type of plant this is?\} \\
% Instruction: Refer to the image, the Wikipedia paragraph to answer the above question is: \\
% \textit{\{indoor ficus trees are so popular is\}} \\
% \midrule
% \midrule
% \rowcolor{gray!20} \textit{Retrieval-augmented Answer Generation} \\
% \midrule
% $<$Img$>$ $<$ImageHere$>$ $<$/Img$>$ \\
% Instruction: Using the provided image and passage, respond to this question with a short answer: \\
% Passage: \{the reason why indoor ficus trees are so popular is that they are very versatile and suit a wide range of interior design needs. if you are looking for a superb tropical bonsai to keep indoors in the uk you cannot go far wrong with a ficus, they are great and provide all year round interest and green in the home.\} \\
% Question: \{Name the type of plant this is?\} \\
% Answer: \textit{\{ficus\}} \\
% \bottomrule
% \end{tabular}
% \small
% \caption{Prompt Format for Knowledge Retrieval and Answer Generation.}
% \label{tab:instruction_format_1}
% \end{table*}


% \subsection{Sampled Document Identifiers}
% As described in the Sec.3.2, We employ a large language model~\cite{touvron2023llama} as an extractive summarizer, which extract a fixed-length original sequence of sub-tokens to answer a given question. 
% % As shown in the Fig.~\ref{fig4}, we use these texts as the supervised document identifier.
% We present three examples where we use a summarizer to extract key substrings from the original documents. We find that the extracted text is highly relevant to our questions. Therefore, using these extracted texts as supervised signals can guide our model to generate document identifiers that are strongly correlated with the multimodal context.
% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.8\textwidth]{fig4_1.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Acquisition of Sampled Document Identifiers.}
% \label{fig4}
% \end{figure*}

% \subsection{Workflow of Knowledge Retrieval}
% Fig.~\ref{fig5} demonstrates the workflow of the generative knowledge retriever. For a given image-text pair, our generative retriever produces a document identifier, which is a sequence of tokens appearing in a document within the knowledge base. We can locate the corresponding document using this document identifier. From these examples, we observe that our generated document identifiers are strongly correlated with the content of the documents, demonstrating the effectiveness of our Supervised Fine-tuning and Reinforced Retrieval Calibration modules in improving retrieval relevance.
% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{fig5.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Workflow of Knowledge Retrieval via Built-in Autoregressive Search Engines.}
% \label{fig5}
% \end{figure*}

% \subsection{Illustrative Cases of Retrieval-Augmented Answer Generation}
% Fig.~\ref{fig6} presents three illustrative cases demonstrating how our model addresses knowledge-intensive questions.
% This process involves invoking the built-in search engine and generating answers grounded in the retrieved results.
% These examples highlight the model's proficiency in retrieving documents highly relevant to the multimodal context and generating accurate answers based on the retrieved information.
% % In Fig.~\ref{fig6}, we present three illustrative cases to demonstrate how our model answers a knowledge-intensive question by invoking the built-in search engine and generating answers based on the retrieved results.
% % The three provided examples demonstrate our model's proficiency in retrieving documents highly pertinent to the multimodal context and generating accurate answers based on the retrieved information.
% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{fig6.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Illustrative Cases of Retrieval-Augmented Answer Generation.}
% \label{fig6}
% \end{figure*}


\end{document}
