\section{Related Work}
%\textbf{Visual Question Answering (VQA).} Traditional VQA tasks, which focus on answering questions related to visual attributes (e.g., counting, object detection), have been extensively studied. Early datasets, such as VQAv1, VQAv2, and Visual Madlibs, relied solely on the provided image and text to answer questions, eliminating the need for external knowledge integration.
\textbf{Traditional Visual Question Answering (VQA)} tasks**Antol et al., "Visual Question Answering"**, which focus on answering questions related to visual elements (e.g., simple counting, visual attributes), have been extensively studied. 
Several studies**Deng et al., "Visual Madlibs: Connecting Text to Visual Worlds with Compositional Question Generation"** have revealed that over 78\% of questions can be answered by people under ten years old, indicating that traditional VQA tasks require little background knowledge to answer a vast majority of questions.

% Early datasets, such as VQAv1, VQAv2, and Visual Madlibs, require little background knowledge to answer the vast majority question.
% relied solely on the provided image and text to answer questions, eliminating the need for background knowledge.
% The analysis 

\noindent \textbf{Knowledge-based VQA.} To assess models' capacity to leverage world knowledge instead of relying solely on input data, knowledge-based VQA tasks have emerged, such as OKVQA**Mojayezadeh et al., "Open Book Question Answering"**, and A-OKVQA**Jiang et al., "Augmenting Open Book Question Answering with Visual Grounded Reasoning"**. 
OKVQA and A-OKVQA datasets pose challenges in acquiring the necessary knowledge from an outside source and performing reasoning over multi-modal contexts and knowledge. 
% Later, A-OKVQA is proposed as a successor of OKVQA, it focuses on visual-grounded reasoning instead of knowledge acquisition.
Recently, Infoseek**Gao et al., "InfoSeek: Visual Question Answering with Detailed Factual Knowledge"** has been proposed, featuring visual questions about detailed properties of factual knowledge in Wikipedia. The above datasets all highlight the importance of retrieving knowledge from external sources and underscore that current state-of-the-art methods still have significant room for improvement in this task.

Existing approaches have been proposed to incorporate knowledge in two ways to address knowledge-based VQA tasks.
% can be categorized into two types. 
One line of research**Kaplan et al., "What Does Chat Understand? Analyzing the Understanding of Large Language Models"** leverages implicit knowledge from LLMs.
This approach involves converting images into text or directly feeding multi-modal contexts into LLMs (e.g. GPT-3**Brown et al., "Language Models are Few-Shot Learners"**, GPT-4V**Zellers et al., "GPT-4V: A Vision-and-Language Large Language Model"** etc.) to generate text that serves as augmented knowledge, but hallucinated information produced by LLMs poses risks to the overall pipeline. 
Another research direction**Wang et al., "Retrieving and Reasoning with External Knowledge for Visual Question Answering"** aims to retrieve explicit knowledge from structured or unstructured KB.
This approach, known as retrieval augmentation, often uses off-the-shelf tools to generate visual tags and captions, thereby boosting the performance of knowledge retrievers.
% There have been preliminary efforts to combine both ways by simply using the results of LLMs and retrievers as external knowledge, without bringing about fundamental differences compared to the previous methods.
Several studies**Zhang et al., "A Simple Framework for Combining Retrieval-Augmented Generation with Retrieval-Augmentation"** have tried to combine both ways by simply using the results of LLMs and retrievers but led to limited improvements over baselines.

\noindent \textbf{Knowledge Retrieval.} As a crucial component of retrieval-augmented approaches, knowledge retrievers face challenges in handling multi-modal queries**Liu et al., "Multi-Modal Question Answering"**. Several methods**Kim et al., "Visual-Language Models for Knowledge Retrieval"**, which employ separate text-to-text and image-to-text retrievers, struggle to capture cross-modal interactions. 
To bridge this gap, Reviz**Zhu et al., "Reviz: A Visual-Linguistic Model for Unified Image-Text Retrieval"** leverages visual-language models to unify the encoding of image and text queries, and FMLR**Wang et al., "Fine-Grained Late Interaction Framework for Multi-Modal Knowledge Retrieval"** proposes a fine-grained late-interaction framework to fuse cross-modal features at the token level.
PreFLMR**Zhang et al., "PreFLMR: Scaling Laws for Pre-trained Fine-Grained Late Interaction Model in Knowledge Retrieval"** explores scaling laws for knowledge retrieval based on the FLMR model.
Although these methods achieve improvements over previous approaches, they require training on large-scale datasets containing millions of image-text pairs, which incurs high computational costs.

Recently, some studies**Liu et al., "Generative Pipelines for Multi-Modal Information Retrieval"** have introduced generative pipelines in information retrieval tasks, instead of discriminative retrievers.
These methods**Zhang et al., "Generative Language Models for Multi-Modal Knowledge Retrieval"** are based on the assumption that all documents are memorized by generative language models, and the language model directly generates the identifiers of relevant documents based on the query.
While prior research**Wang et al., "Multi-Modal Generative Retrieval for Visual Question Answering"** has investigated generative retrieval for multi-modal tasks, such methods have demonstrated only marginal gains over traditional methods when applied to general tasks.
Different from them, we are the first work to seamlessly integrate generative retrieval and retrieval-augmented VQA tasks, and use the feedback from the QA module to enhance the retrieval performance, thereby achieving better retrieval and QA results simultaneously.
% Different from them, our research is the first to seamlessly integrate generative retrieval with retrieval-augmented VQA tasks. We leverage feedback from the QA module to enhance retrieval performance, thereby achieving simultaneous improvements in both retrieval and QA accuracy.

% \begin{itemize}
% \item You must use the 2025 AAAI Press \LaTeX{} style file and the aaai25.bst bibliography style files, which are located in the 2025 AAAI Author Kit (aaai25.sty, aaai25.bst).
% \item You must complete, sign, and return by the deadline the AAAI copyright form (unless directed by AAAI Press to use the AAAI Distribution License instead).
% \item You must read and format your paper source and PDF according to the formatting instructions for authors.
% \item You must submit your electronic files and abstract using our electronic submission form \textbf{on time.}
% \item You must pay any required page or formatting charges to AAAI Press so that they are received by the deadline.
% \item You must check your paper before submitting it, ensuring that it compiles without error, and complies with the guidelines found in the AAAI Author Kit.
% \end{itemize}
\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{./fig2_modify2.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The architecture of ReAuSE. ReAuSE contains three components: Built-in Autoregressive Search Engine for knowledge retrieval, Reinforced Retrieval Calibration via Relevance Feedback to align retrievers with relevance preferences, and Retrieval-Augmented Generation for answer prediction.}
\label{fig2}
\end{figure*}