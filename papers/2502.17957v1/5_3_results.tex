\begin{table}[t]
    \centering
    \small
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
     & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10}\\
     \midrule
     Chunk &43.64& 66.65& 13.98& 31.14\\
     +Sent &61.64 &81.69 &22.13 &47.20 \\
     \bottomrule
    \end{tabular}
    }
    \caption{Ablation study on the effect of synthetic queries generated at a sentence-level granularity of context on Multihop-RAG.}
    \label{tab:granularity}
    \vspace{-0.8em}
\end{table}


\subsection{Results}
We will discuss our experiment results for each of the stages. In the supervised fine-tuning stage, we will discuss the effects of multi-granular synthetic queries, synthetic data with domain-specific constraints, and the use of Context2ID data. For the preference learning stage, we will discuss using different candidates for preference learning.

\subsubsection{Supervised Fine-Tuning Stage}

\paragraph{Effects of multi-granular synthetic queries.} We conduct an analysis on the effects of incorporating synthetic queries generated from the context at different levels of granularity on MultiHop-RAG. We train the generative retrieval model based on semantic identifiers on chunk-level Query2ID data (Chunk), comparing it with the model trained on chunk-level and sentence-level Query2ID data (+Sent), and both models use Context2ID data.
The results are shown in Table~\ref{tab:granularity}. We find that sentence-level synthetic queries can significantly improve retrieval performance, indicating that synthetic query generation with a small context can help capture more details from the document.

\begin{table*}[htbp]
    \centering
    \small
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccccccccccc}
        \toprule
        & \multicolumn{4}{c}{\textbf{MultiHop-RAG}} & \multicolumn{3}{c}{\textbf{AllSides}} &\multicolumn{3}{c}{\textbf{AGNews}}\\\cmidrule(lr){2-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}
        & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10} & \textbf{HIT@1} & \textbf{HIT@5} & \textbf{HIT@10} & \textbf{HIT@1} & \textbf{HIT@5} & \textbf{HIT@10}\\
        \midrule
        w/o constraints &61.64 &81.69 &22.13 &47.20 & 10.19 & 29.63 & 47.22 & 59.91& 83.94& 88.11\\
        w/ constraints &69.98& 88.34 &24.85 & 52.29 & 14.20& 38.58 & 51.85 & 62.19& 83.78& 88.24\\
        \bottomrule
        \end{tabular}
    }
    \caption{Ablation study on generative retrieval performances with or without the constraints-based synthetic queries.}
    \label{tab:constraints}
\end{table*}
\begin{table*}[htbp]

    \centering
    \small
    \begin{tabular}{lcccccccc}
    \toprule
    & \multicolumn{4}{c}{\textbf{MultiHop-RAG}} & \multicolumn{4}{c}{\textbf{Natural Questions}} \\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
    & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10} & \textbf{HIT@1} & \textbf{HIT@5} & \textbf{HIT@10} & \textbf{MRR@10} \\
    \midrule
    w/o Context2ID &41.33 &69.31 &14.45 &31.25 & 69.72 & 85.58 & 89.01 & 76.57\\
    w/ Context2ID &69.98& 88.34 &24.85 & 52.29 & 70.71 & 86.48 & 89.85 & 77.54\\
    \bottomrule
    \end{tabular}
    \caption{Ablation study on generative retrieval performance trained with or without Context2ID data. The results demonstrate the helpfulness of Context2ID data and learning to memorize the context for generative retrieval.}
    \label{tab:context2id}
\end{table*}

\paragraph{Effects of constraints-based synthetic queries.} We further study the use of constraints-based synthetic queries that are customized for each domain-specific setting. We conduct experiments on three domain-specific corpora, MultiHop-RAG, AllSides, and AGNews. We compare the semantic identifier-based generative retrieval model trained with or without constraints-based synthetic queries, combing with the corresponding Context2ID data. The results are shown in Table~\ref{tab:constraints}. The results show that constraints-based synthetic queries can further improve retrieval performance, indicating that it is helpful to use LLM-produced synthetic queries for domain customization.


\begin{table}[t]
    \centering
    \small
    \resizebox{.48\textwidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
         & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10}\\
         \midrule
         Concat &44.30& 72.77& 15.64& 33.59\\
         Interleave &69.98& 88.34 &24.85 & 52.29 \\
         \bottomrule
        \end{tabular}
    }
    \caption{Analysis on different ways of combining Query2ID and Context2ID data on Multihop-RAG. We compare simple concatenation (Concat) and interleaving (Interleave) that inherently upsamples the Context2ID data.} %
    \label{tab:context_concat}
    \vspace{-0.5em}
\end{table}

\paragraph{Effects of Context2ID data.} Existing work~\citep{DBLP:journals/corr/abs-2206-10128,DBLP:conf/icml/KishoreWLAW23} debates whether Context2ID data are useful for generative retrieval training. In this work, we consider Context2ID data as an important part of the data recipe, and also include the memorization of the context as part of the supervised fine-tuning objective. Therefore, we conduct an analysis that removes the Context2ID data on MultiHop-RAG and NQ, and the results are shown in Table~\ref{tab:context2id}. We can find that Context2ID data consistently improve generative retrieval performance. We also include the comparison of the strategies to combine Query2ID and Context2ID data, including simple concatenation or interleaving Context2ID and Query2ID that will upsample Context2ID data (\textit{i.e.,} smaller size dataset) on MultiHop-RAG in Table~\ref{tab:context_concat}, again illustrating the importance of Context2ID and learning to memorize context may strengthen the effects on Context2ID.

\begin{table*}[htbp]
    \centering
    \small
    \begin{tabular}{lcccccccc}
    \toprule
    & \multicolumn{4}{c}{\textbf{MultiHop-RAG}} & \multicolumn{4}{c}{\textbf{Natural Questions}} \\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
     & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10}& \textbf{HIT@1} & \textbf{HIT@5} & \textbf{HIT@10} & \textbf{MRR@10} \\
     \midrule
     docT5query &50.86& 73.30& 17.60& 37.73 & 63.30 & 79.12 & 85.18 & 70.30\\
     Mixtral 8x7b &61.64 &81.69 &22.13 &47.20 & 70.71 & 86.48 & 89.85 & 77.54\\
     \bottomrule
    \end{tabular}
    \caption{Generative retrieval performance with synthetic queries from Mixtral 8x7b and docT5query. The results show that queries from Mixtral 8x7b can help train a better generative retrieval model.}
    \label{tab:qg_model}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/synth_data_winrates.pdf}
    \caption{Jaccard similarity post-analysis on MultiHop-RAG test set. Synthetic queries from Mixtral 8x7b are generally closer to the test set than those from docT5query. Besides, incorporating granularity and domain-specific attributes further helps with getting queries that are closer to the test set.}
    \label{fig:jaccard_similarity}
    \vspace{-0.5em}
\end{figure}

\paragraph{Different query generation models.} As we primarily use LLMs to produce synthetic queries, it is important to understand the performance and effects of using an LLM compared to a specialized query generation model. To this end, we conduct a comparison between synthetic queries generated by Mixtral 8x7b and those from docT5query, as shown in Table~\ref{tab:qg_model}. For a fair comparison, we exclude constraints-based queries because docT5query cannot produce these types of queries. The results show generative retrieval models trained with queries from Mixtral 8x7b consistently outperform models trained on queries from docT5query.

Following \citet{pradeep-etal-2023-generative}, we use Jaccard similarity to evaluate the semantic similarity between test queries and synthetic queries as a post-analysis. 
For each test query, we compute the maximum Jaccard similarity among all synthetic queries generated for the corresponding chunk or document. We then compute the win rate between two synthetic query sets (e.g., Mixtral 8x7b versus docT5query) as the proportion of test queries for which one set exhibits higher Jaccard similarity.
Let \(Q\) be the set of test queries. Then, the win rate for a synthetic query set \(\mathcal{S}\) over another set \(\mathcal{T}\) is defined as:
\begin{align*}
& \text{Win Rate}(\mathcal{S}, \mathcal{T}) = \\
& \frac{1}{|Q|} \sum_{q \in Q} \mathbb{I}\left[\max_{s \in \mathcal{S}(q)} J(q, s) > \max_{t \in \mathcal{T}(q)} J(q, t)\right],
\end{align*}
where $J(q, s)$ denotes the Jaccard similarity between the token sequences of the test query $q$ and a synthetic query $s$, and $\mathcal{S}(q)$ represents the subset of synthetic queries for the corresponding chunk.

Figure~\ref{fig:jaccard_similarity} illustrates that synthetic queries from Mixtral 8x7b generally have a higher similarity to test queries. Moreover, both sentence-level synthetic queries and constraints-based queries contribute to improved distribution matching with the test queries.

\begin{table}[t]
    \centering
    \small
    \resizebox{.48\textwidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
         & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10}\\
         \midrule
         all &74.32& 88.03& 29.71& 59.26\\
         \hdashline
         w/o Context2ID& 72.15& 86.21& 28.54&57.50\\
         w/o Sent &58.40&75.17&21.51&44.76\\
         w/o constraints &68.34&83.73&26.28&53.83\\
         \bottomrule
        \end{tabular}
    }
    \caption{Ablation study on atomic identifier-based generative retrieval performance on MultiHop-RAG.}
    \label{tab:identifier}
\end{table}
\paragraph{Generalization to different identifiers.} We further study the generalizability of our data strategies across different types of document identifiers. In this analysis, we use atomic identifier, which are arbitrary unique IDs assigned to each document or chunk. We conduct experiments on MultiHop-RAG and the results are shown in Table~\ref{tab:identifier}. The findings align with our observations using semantic identifiers, highlighting the critical role of all three data types in generative retrieval. Among them, sentence-level synthetic queries contribute the most to performance improvements.


\begin{table*}[htbp]
    \centering
    \small
    \begin{tabular}{lcccccccc}
    \toprule
    & \multicolumn{4}{c}{\textbf{MultiHop-RAG}} & \multicolumn{4}{c}{\textbf{Natural Questions}} \\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
     & \textbf{HIT@4} & \textbf{HIT@10} & \textbf{MAP@10} & \textbf{MRR@10}& \textbf{HIT@1} & \textbf{HIT@5} & \textbf{HIT@10} & \textbf{MRR@10} \\
     \midrule
     SFT & 69.98& 88.34 &24.85 & 52.29 & 70.71 & 86.48 & 89.85 & 77.54 \\
     Random 5 & 58.94 & 82.88 & 20.88 & 43.53 & 70.19 & 86.48 & 89.50 & 77.17 \\
     Top-5 negative & 71.53 & 89.62 & 26.36 & 55.40 & 71.02 & 87.32 & 90.04 & 78.02 \\
     Top-10 negative & 71.88 & 89.80 & 26.23 & 54.94 & 71.22 & 87.41 & 89.97 & 78.14 \\
     \bottomrule
    \end{tabular}
    \caption{Preference learning with different numbers of negative candidates. The results show that it is an effective strategy to select negative candidates with ranks higher than the positive candidate, while different numbers of negative candidates may optimize the retrieval performance in different ways.}
    \label{tab:neg_cand_number}
\end{table*}


\subsubsection{Preference Learning Stage}

\paragraph{Effects of negative candidate sources.} We first study the strategies of candidates selection for preference learning. We compare random selection from the corpus with using the top candidates from the generative retrieval model after the supervised fine-tuning stage. The results are shown in Table~\ref{tab:neg_cand_number}, which illustrates that candidate selection has an impact on preference learning, and random candidates may have a negative impact.

\paragraph{Effects of negative candidate number.} We also study the effects of using different negative candidate numbers for each query. We experiment with selecting Top-5 and Top-10 negative candidates with a rank higher than the positive candidate from the retrieval results. The results are shown in Table~\ref{tab:neg_cand_number}. In general, it is effective to use the strategy, which includes high-quality candidates with ranks higher than the corresponding positive candidates. We also see some slight differences when including different numbers of negative candidates. We can find that a large number of negative candidates helps better in metrics such as HIT@1 and HIT@4.


\subsubsection{Comparison to Off-The-Shelf Retrievers}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.9\linewidth, trim={0cm 0.5cm 0cm 0.3cm}, clip]{figures/dense_comparison.pdf}
    \caption{Performance comparison between generative retrieval using semantic identifiers and off-the-shelf-retrieval models. We use HIT@4 for MultiHop-RAG and HIT@1 for other datasets as the evaluation metric. For the full experiment results, please refer to Appendix \ref{sec:detailed_dense_comparison} (Table \ref{tab:dense_retrieval_comparison}).}
    \label{fig:dense_comparison}
\end{figure*}
We also compare the performance of our generative retrieval with several off-the-shelf retrievers, including BM25~\citep{DBLP:conf/sigir/RobertsonW94}, bge-large-en-v1.5~\citep{bge_embedding},  Contriever-msmarco~\citep{izacard2021contriever},  E5-mistral-7b-instruct~\citep{wang-etal-2024-improving-text} and GTE-Qwen2-7B-instruct~\citep{li2023towards}. The results are shown in Figure~\ref{fig:dense_comparison} and more detailed results can be found in Appendix~\ref{sec:detailed_dense_comparison}. The results demonstrate that generative retrieval models, which rely solely on in-domain synthetic data training without retrieval pre-training, can achieve competitive performance compared to those retrievers.
These results indicate the potential of generative retrieval, as well as the effectiveness of using LLMs to generate synthetic data tailored to domain-specific needs.


