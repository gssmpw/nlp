\section{Related Work}
\paragraph{Generative retrieval modeling.}
Previous work has explored various aspects of generative retrieval. One line of research aims to find appropriate document identifiers for generation, such as numerical or atomic identifier~\citep{DBLP:conf/nips/Tay00NBM000GSCM22,DBLP:journals/corr/abs-2206-10128,DBLP:journals/corr/abs-2208-09257}, N-grams~\citep{DBLP:conf/nips/BevilacquaOLY0P22,DBLP:conf/sigir/Chen0GR0FC23}, titles or URLs~\citep{DBLP:journals/ipm/LiYWWL23,lee-etal-2022-generative,DBLP:conf/cikm/ChenZG0FC22,ziems-etal-2023-large}, keywords-based or summary-based semantic identifiers~\citep{lee-etal-2023-glen,DBLP:conf/kdd/Tang0GCZWYC23}, codebook~\citep{DBLP:conf/nips/ZhangWCCZMHDMWP23,yang-etal-2023-auto,DBLP:conf/www/Zeng0JSWZ24}, and full passages themselves~\citep{tang2024selfretrievalendtoendinformationretrieval}. There are also efforts to combine the advantages of different identifiers~\citep{li-etal-2023-multiview}. Another line of work tackles the optimization of generative retrieval models, such as incorporating ranking losses~\citep{zhou-etal-2023-enhancing-generative,DBLP:conf/aaai/00010WWL24,DBLP:journals/tois/TangZGRCC24}, or using auxiliary tasks to enhance training~\citep{DBLP:conf/sigir/LiD0L24}. During retrieval, different constrained decoding methods have been explored to obtain valid identifiers, such as FM-Index~\citep{DBLP:conf/nips/BevilacquaOLY0P22}, Trie-based~\citep{DBLP:journals/corr/abs-2010-00904}, and set-based inference~\citep{DBLP:journals/tois/TangZGRCC24}.

\paragraph{Synthetic query generation.}
Alongside the progress in generative retrieval modeling and optimization, synthetic query generation has emerged as a pivotal technique for enhancing retrieval systems, particularly in domains with limited annotated data.
In dense retrieval, synthetic queries have been used extensively to improve cross-domain performance.
For instance, \citet{ma2021zero} generated synthetic questions for target-domain documents with a question generation model trained on general-domain data, thereby improving retrieval performance in zero-shot settings.
Similarly, \citet{wang2022gpl} introduced generative pseudo labeling, which combines query generation with pseudo labeling using a cross-encoder to capture finer-grained ranking signals.
Further advancements include \citet{bonifacio2022inpars} and \citet{jeronymo2023inpars}, which leverage large language models to generate synthetic queries in a few-shot manner, and then combine with top K documents ranked by the conditional question generation probability, to train a domain-specific reranker.

Despite these successes in dense retrieval, the potential of synthetic data for generative retrieval has been underexplored.
Existing studies typically rely on passage-level synthetic queries generated by docT5query \cite{nogueiradoc2query}, following the DSI-QG paradigm \cite{DBLP:journals/corr/abs-2206-10128}.
\citet{chen-etal-2023-understanding} explores breaking documents into text fragments for query generation and memorization. However, there still lacks a comprehensive discussions on effective strategies for generating synthetic data tailored to domain-specific corpora, especially with LLMs.
This work investigates data strategies from multiple perspectives, including the generation of synthetic queries using multi-granularity contexts, incorporating search constraints, and exploring the impact of context data. For preference learning, \citet{zhou-etal-2023-enhancing-generative} proposes using preference learning objectives for generative retrieval with specialized reward models, though acquiring such models in a domain-specific setting can be challenging.
In contrast, our proposed preference learning strategy directly uses the retrieval results to obtain the preference data, offering a more streamlined approach for domain-specific applications.