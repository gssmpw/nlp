\begin{table*}[htbp]
    \small
    \centering
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{1cm}{\textbf{Dataset}} & \multirow{2}{1.1cm}{\textbf{Context}} & \multicolumn{3}{c}{\textbf{Queries}} \\
     & &\textbf{Chunk-Level} & \textbf{Sentence-Level} & \textbf{Constraints-Based}\\\midrule
     MultiHop-RAG & 7,724 & 72,090 & 472,193 & 51,212\\
     AllSides & 645 & 6,313 & 173,898 & 6,091 \\
     AGNews & 1,050 & 10,355 & 80,524 & 20,875 \\
     NQ & 98,748 & 1,459,031 & - & - \\\bottomrule %
    \end{tabular}
    \caption{Dataset Statistics}
    \label{tab:dataset_stats}
\end{table*}

\begin{table}[htbp]
    \small
    \centering
    \begin{tabular}{lp{4.8cm}}
    \toprule
      \textbf{Dataset} & \textbf{Attributes} \\\midrule
        MultiHop-RAG & author, publish time, source, category, title \\
        AllSides & political polarity \\
        AGNews & location, topic \\\bottomrule
    \end{tabular}
    \caption{Attributes used in each dataset for constraints-based query generation.}
    \label{tab:dataset_attributes}
\end{table}

\section{Details of Experiment Setup}
We use \texttt{Mistral-7B-Instruct-v0.3} as the base model for generative retrieval with the semantic identifier, while use \texttt{Mistral-7B-v0.3} as the base model for atomic identifier as it is closer to a classification setting.

For supervised fine-tuning, we train the models with 2 epochs, with a learning rate of 2e-5 and a warmup ratio of 0.1. The batch size is set as 256. We use sequence packing to put multiple examples in one forward pass~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}. We use \texttt{bfloat16} for our training.

For preference learning, we mainly conduct experiments on MultiHop-RAG and NQ with semantic identifiers. We train the models with 1 epoch. The learning rate is set as 1e-7, batch size is set as 64, $\beta$ is set as 0.5, $\alpha$ is set as 1.0. 

The training infrastructure includes TRL~\citep{vonwerra2022trl}, Accelerate~\citep{accelerate}, Transformers~\citep{wolf-etal-2020-transformers}, DeepSpeed~\cite{DBLP:conf/kdd/RasleyRRH20} and FlashAttention-2~\citep{DBLP:conf/iclr/Dao24}. We use 8x Nvidia A100-SXM4-40GB for our experiments. Each training or inference procedure can be completed in 1 day.

Statistics of the numbers of the documents, different synthetic queries can be found in Table~\ref{tab:dataset_stats}. Attributes used for constraints-based synthetic queries can be found in Table~\ref{tab:dataset_attributes}. All the experiment results are obtained with single run.
\label{app:data_specific_setup}
\subsection{MultiHop-RAG}
On MultiHop-RAG, we split the documents into chunks with maximum length of 256 without overlap and conduct retrieval on individual chunks. For synthetic query generation, $m_c$, $m_s$ and $m_i$ are set as 10, and the temperature for LLM inference on synthetic data generation is set as 0.7. We interleave the Context2ID and Query2ID data as the full dataset for model supervised fine-tuning. The maximum sequence length is set as 700. For synthetic queries for preference learning, we ask the LLM to generate 10 queries. We perform the retrieval with beam size as 10 and retrieve the top-10 candidates for each query to construct the candidate pairs.
\subsection{AllSides}
On AllSides, we conduct document-level retrieval. For synthetic query generation, $m_c$, $m_s$ and $m_i$ are set as 10, and the temperature for LLM inference on synthetic data generation is set as 0.7. For Context2ID data, as there are some long documents in the corpus, we will split the long context into chunks with maximum length of 256 without overlap. The Context2ID data is constructed to use all chunks in the document to predict its corresponding document identifier. We interleave the Context2ID and Query2ID data as the full dataset for model supervised fine-tuning. The maximum sequence length is set as 700.
\subsection{AGNews}
On AllSides, we conduct document-level retrieval. For synthetic query generation, $m_c$, $m_s$ and $m_i$ are set as 10, and the temperature for LLM inference on synthetic data generation is set as 0.7. Queries constructed by \citet{DBLP:journals/corr/abs-2405-02714} uses two different perspectives. The first perspective is either the location of the desired news or the topic, while the second perspective is that the news is similar to another given news in the query. As we mentioned Section~\ref{sec:experiment_setup}, we replace the second perspective with the another field so that each query consists of both location and topic perspectives. The topic and location information used for instruction-based synthetic query generation is extracted with Mixtral 8x7b. We interleave the Context2ID and Query2ID data as the full dataset for model supervised fine-tuning. The maximum sequence length is set as 700.
\subsection{NQ}
On NQ, we conduct document-level retrieval. We use the document prefixes from~\cite{DBLP:conf/icml/KishoreWLAW23} to produce the semantic identifiers. For synthetic query generation, we perform truncation on pages when they are too long so that we always have at least 1024 token space for model output. We set $m_c$ as 15 and temperature as 0.7. We do not include sentence-level synthetic queries as the number of those queries are too large to be included in training within a reasonable time. Instead, we include sentence-level Context2ID as the approximation, and use the sentences from the document prefixes from~\cite{DBLP:conf/icml/KishoreWLAW23} to predict corresponding document identifiers. In NQ, we have high quality human annotated training queries, which we also include as part of the Query2ID data and therefore we do not include instruction-based synthetic queries. We concatenate the Context2ID and Query2ID data as the full dataset for model supervised fine-tuning, as interleaving will produce a much larger dataset that cannot be trained within a reasonable time. The maximum sequence length is set as 450. For synthetic queries for preference learning, we also perform truncation as for supervised fine-tuning, and ask the LLM to generate 10 queries. As the generated query number is quite large for inference, we use the first 2 generated queries for each documents for preference learning. We perform the retrieval with beam size as 10 and retrieve the top-10 candidates for each query to construct the candidate pairs.
