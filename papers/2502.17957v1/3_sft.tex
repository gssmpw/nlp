\section{Supervised Fine-Tuning Data Strategy}
In a typical domain-specific setup, we often assume access to a corpus with limited or no labeled data for domain-specific training~\citep{DBLP:conf/ictir/HashemiZKPMC23}. Therefore, it is crucial to create high-quality synthetic data that thoroughly covers the entire corpus for generative retrieval training.

Our synthetic data comprises two main components: Context2ID data and Query2ID data. Context2ID involves training the model to retrieve the document identifiers given the document's content. Query2ID focuses on teaching the model to retrieve relevant document identifiers from a given query.
Combining these two data types encourages the model to learn both content memorization and retrieval given a query.

\subsection{Supervised Fine-Tuning Objective}
At this stage, we train the model to generate relevant document identifiers by maximizing the probability of each individual token. While typical supervised fine-tuning (SFT), especially with encoder-decoder architectures such as T5, focuses on optimizing the output sequence (\textit{i.e.} document identifiers), it's also part of the training goal for generative retrieval models to comprehend and memorize the context. To this end, we also optimize the model for learning to decode the input.
Specifically, for a given query-document pair $(q,d)$, where $q$ could be an actual query or a text chunk from the document, the model maximizes the likelihood of the combined input and output sequence:
\begin{align*}
\mathcal{L}_\text{sft}\left(q,d\right) = &-\log P\left(d', q; \theta\right) \\
= &-\sum_i \log P(q_i \mid q_{<i}; \theta) \\
 &- \sum_i \log P(d'_i \mid d'_{<i}, q; \theta).
\end{align*}

\begin{table*}[htbp]
    \centering
    \small
    \begin{tabular}{lp{0.749\textwidth}}
        \toprule
        \textbf{Data Type} & \textbf{Example} \\
        \midrule
        Context & title: Christmas Day preview: \colorbox{Apricot}{49ers}, \colorbox{Salmon}{Ravens} square off in potential Super Bowl sneak peek\ldots source: \colorbox{GreenYellow}{Yardbarker} \ldots \colorbox{Apricot}{San Francisco} has racked up an NFL-leading 25 turnovers and has given up the second-fewest rushing \colorbox{Goldenrod}{yards (1,252)}, \ldots \\
        Chunk-Level Query & What is the potential implication of this matchup between the \colorbox{Apricot}{49ers} and \colorbox{Salmon}{Ravens}? \\
        Sentence-Level Query &  Where does the \colorbox{Apricot}{49ers}' defense stand in terms of \colorbox{Goldenrod}{total yards} allowed per game? \\
        Constraints-Based Query & \underline{According to the \colorbox{GreenYellow}{Yardbarker} article}, which team has the league's most effective running game?\\% as of 2023-12-24?\\
        \bottomrule
    \end{tabular}
    \caption{Examples of different synthetic queries generated from MultiHop-RAG corpus.} %
    \label{tab:data_type_example}
    \vspace{-0.5em}
\end{table*}

\subsection{Context2ID}
Context2ID data is created by pairing each chunk of text in the corpus with its corresponding document identifier. The goal of Context2ID data is to help the generative retrieval model associate each document's content with its unique identifier, i.e., ``memorizing'' the text.

\subsection{Query2ID}
Query2ID is designed to teach the model to retrieve the relevant document identifiers given a query. It helps the model to learn the core retrieval task and also further comprehend content from the query perspective.

Previous work~\citep{DBLP:journals/corr/abs-2206-10128} finds it effective to use a query generation model (\textit{e.g.,} docT5query, \citealp{nogueiradoc2query}) to produce synthetic queries from documents using multiple independent samplings. In this work, we instead use an LLM for synthetic query generation. Specifically, given a context (e.g., a document chunk), the LLM is instructed to generate a diverse set of $m$ queries, thereby covering a  wider range of semantic variations compared to the sampling-based approach with a specialized query generation model.


\subsubsection{Multi-Granular Query Generation} We first generate queries with context at different levels of granularity: \textit{chunk-level} and \textit{sentence-level}. Chunk-level synthetic queries are produced by providing the entire chunk as input to the LLM to capture higher-level semantics or facts, while sentence-level synthetic queries are produced by only providing individual sentences to focus on more specific details within the document
Concretely, for each chunk, we ask the LLM to produce $m_c$ chunk-level queries. We then split the chunk into individual sentences and ask the LLM to generate $m_s$ sentence-level queries for each sentence.

\subsubsection{Constraints-Based Query Generation}

A key advantage of using an LLM for query generation is its ability to incorporate domain-specific instructions.
For instance, we can prompt the LLM to include metadata constraints, such as the \textit{author name} or \textit{political polarity} of a document, in the generated queries.
Although the specific constraint types depends on the metadata available and can be domain or dataset specific, they are common in real-world scenarios such as enterprise data. Table~\ref{tab:dataset_attributes} in Appendix specifies the attributes that we use to produce constraints-based synthetic queries for each dataset. We ask the LLM to generate $m_i$ queries for each document that incorporate these constraints, allowing our generative retrieval model to handle more specialized or domain-specific queries.
