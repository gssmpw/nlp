\section{Introduction}

Generative retrieval is emerging as a promising paradigm for information retrieval (IR), leveraging generative models (\textit{e.g.,} Transformers, \citealp{DBLP:conf/nips/VaswaniSPUJGKP17}) to directly produce ranked lists of potentially relevant document identifiers for a user query.
Although prior work has made progress on various fronts, including training strategies (\textit{e.g.,} identifier choices)~\citep{DBLP:conf/nips/Tay00NBM000GSCM22,DBLP:journals/corr/abs-2208-09257,DBLP:conf/nips/BevilacquaOLY0P22,DBLP:conf/nips/0001YCWZRCYRR23}, modeling techniques~\citep{chen-etal-2023-understanding,zhou-etal-2023-enhancing-generative,DBLP:conf/aaai/00010WWL24}, and inference methods~\citep{DBLP:journals/corr/abs-2010-00904,lee-etal-2023-nonparametric,DBLP:conf/sigir/ZhangL0DLC24}, the role of \emph{data strategies} in training generative retrieval models, particularly when dealing with domain-specific corpora, remains relatively underexplored.
This gap is critical: as generative retrieval models internalize entire corpus within their parametric memory, the choice and quality of training data are likely to play a critical role in their performance.

To mitigate the high cost and scalability challenges of in-domain annotation, most studies have adopted DSI-QG \cite{DBLP:journals/corr/abs-2206-10128}, which uses passage-level synthetic queries generated by docT5query \cite{nogueiradoc2query} (a model trained on MS-MARCO data).
However, applying such off-the-shelf synthetic data strategies to new domains may not suffice.
Unlike dense retrieval approaches, which focuses on strong text representation \cite{karpukhin2020dense,izacard2021contriever}, 
generative retriever must develop three key capabilities: (1) \textbf{memorization} (storing the content of the corpus (\textit{e.g.,} documents) and mapping them to their assigned identifiers), (2) \textbf{generalization} (inferring beyond explicit textual cues from user queries), and (3) \textbf{relevance scoring} (accurately ranking document identifiers by relevance to a given query).
Domain-specific corpora can amplify these challenges, as the model must adapt its internal representations to reflect domain nuances while maintaining robust generalization and ranking accuracy.
In this work, we systematically investigate data strategies that can foster these core capabilities.

We introduce a two-stage training framework.
The first stage focuses on mapping an input directly to document identifiers via supervised fine-tuning on synthetic data. The second stage uses preference learning to further enhance the ranking performance~\citep{zhou-etal-2023-enhancing-generative,DBLP:conf/aaai/00010WWL24}. Here we adopt Regularized Preference Optimization (\citealp[RPO]{DBLP:journals/corr/abs-2404-19733}), an effective alternative to PPO-based reinforcement learning \citep{DBLP:conf/nips/Ouyang0JAWMZASR22}.
We study the data strategies for both stages.

The first stage focuses on the memorization and generalization ability. 
We examine two data sources as the input for decoding document identifiers during training: the \emph{context} data (\textit{e.g.,} chunks) directly extracted from the corpus, and \emph{synthetic queries} that represent various relevance signals. For synthetic queries, we investigate query generation using multi-granular context (\textit{e.g.,} sentence-level, chunk-level) to capture both local and global information from the corpus. We also explore adding constraints derived from available metadata or domain-specific knowledge when generating synthetic queries to enhance the model's ability of handling complex, domain-relevant queries.


In the first stage, models are optimized to produce a single positive candidate, without considering relevance between different candidates. In the second stage, we further create data to enhance the model's ranking capability through preference learning~\citep{zhou-etal-2023-enhancing-generative,DBLP:conf/aaai/00010WWL24}. 
We study the selection of negative candidate documents for preference learning. Rather than relying on static offline data, we collect preference data online from the model's own top-ranked candidates after the first stage, and compare it to random sampling from the corpus.
We further investigate the choices and impact of varying the number of negative candidates to the ranking performance.


We conduct experiments on datasets covering various aspects of relevance, including the widely adopted Natural Questions (\citealp[NQ]{kwiatkowski-etal-2019-natural}), a multi-hop dataset MultiHop-RAG~\citep{DBLP:journals/corr/abs-2401-15391}, and two perspective-based retrieval datasets: AllSides~\citep{baly-etal-2020-detect} and AGNews~\citep{DBLP:conf/nips/YuZZMRKSZ23} from \citet{DBLP:journals/corr/abs-2405-02714}.
We show that queries with different aspects, such as multi-granular and constrains-based queries, significantly improve the retrieval performance compared to relying solely on chunk-level synthetic queries from query generation models. Additionally, upsampling context data further improves the performance.
Moreover, we show that these data strategies generalize well to other types of document identifiers, such as atomic identifiers.
Finally, we demonstrate that RPO effectively improve the ranking performance of generative retrieval, and the key lies in the selection of high-quality negative candidates: high-quality hard negative candidates improve the performance while random negatives may have an adverse impact.

In summary, this work offers a comprehensive investigation of data strategies for building scalable and effective domain-specific generative retrieval systems. Our findings emphasizes the importance of creating high-quality and diverse synthetic queries that capture multiple levels of granularity within the corpus, as well as informed negative selection strategies for ranking optimization.
