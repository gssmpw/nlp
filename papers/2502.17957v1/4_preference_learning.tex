\section{Preference Learning Data Strategy}
Previous work~\citep{zhou-etal-2023-enhancing-generative,DBLP:conf/aaai/00010WWL24} have shown that incorporating ranking tasks can further enhance the relevance modeling of generative retrieval models. However, when generative retrieval models are based on large language models, complex ranking objectives -- such as listwise optimization -- often become computationally inefficient due to multiple forward passes. In this work, we instead use a simpler method and adopt the regularized preference optimization algorithm to perform the preference optimization, a technique widely applied in optimizing large language models. We will first briefly introduce the preference optimization method, and then turn our focus on the synthetic data construction, which consists of the synthetic queries along with their corresponding preferred or rejected candidates.

\subsection{Preference Optimization Objective}
We use Regularized Preference Optimization (\citealp[RPO]{DBLP:journals/corr/abs-2404-19733}) as our optimization method for preference learning. It is an extended version of Directed Preference Optimization (\citealp[DPO]{DBLP:conf/nips/RafailovSMMEF23}), including additional supervised fine-tuning loss to alleviate the over-optimization issues on negative responses. It takes an input query $q$, a positive candidate $d_p$, and a negative candidate $d_n$ as input. The loss is in favor of the positive candidate while against the negative candidate
\begin{align*}
    \mathcal{L}_\text{rpo}\left(q, d_p, d_n\right) = &- \log \delta \left(  \beta\log \frac{P\left(d'_p\mid q; \theta\right)}{P\left(d'_p\mid q; \theta_\text{ref}\right)} \right.\\
    & \quad\quad\ \ \ \left. -\beta\log \frac{P\left(d'_n\mid q; \theta\right)}{P\left(d'_n\mid q; \theta_\text{ref}\right)} \right) \\
    & - \alpha\frac{\log P(d'_p \mid q; \theta)}{\left|d'_p\right|},
\end{align*}
where $\theta_\text{ref}$ is the parameter of the reference model, \textit{i.e.,} the supervised fine-tuned model from the first stage training. $d'_p$ and $d'_n$ are the identifiers of the positive and negative candidate, respectively.

\subsection{Synthetic Queries}
Similar to the previous section, in a domain-specific setup, we assume that we do not have enough data for model training. Therefore, after the supervised fine-tuning stage, we need a batch of new synthetic queries for preference learning.

We still adopt the LLM-based query generation as with the supervised fine-tuning stage. However, there are a few key differences in the instructions. First of all, we ask the LLM to make queries as difficult as possible. At the same time, we ask the LLM to provide not only the synthetic queries but also their corresponding answers. This is to ensure that, while making difficult queries, those synthetic queries are still answerable using the given context.

These changes make the new batch of synthetic queries different from queries used during supervised fine-tuning so that the model will not be over-optimized to the same batch of data. Intensifying the difficulties also increases the likelihood that the initial generative retrieval model makes mistakes, and therefore the model will benefit from the preference learning by learning from those mistakes.

\subsection{Candidate Selection}
After producing the synthetic queries, the next step is to select document candidate pairs for RPO optimization. For each training instance, we need one positive candidate and one negative candidate. As we always produce synthetic queries based on a document, the positive candidate can be naturally assigned. Therefore, the focus will be on selecting negative candidates for each synthetic query.

To increase the hardness of the negative candidates, we choose to select negative candidates from the retrieval results. Specifically, after the supervised fine-tuning stage, we will use the generative retrieval model to perform retrieval on the synthetic queries for preference learning. Our strategy mainly focuses on selecting the top-$k$ negative candidates with ranks higher than the positive candidate from the retrieval results. In this way, if the positive candidate ranks in the top-1, we will not use the query for preference learning. If the rank of the positive candidate is higher than $k$, then there will be different numbers of negative candidates, depending on the rank. If the rank is lower than $k$, there will be $k$ different negative candidates. When there are multiple negative candidates, we pair each negative candidate with the positive one to form a candidate pair instance for preference learning.
