\section{Generative Retrieval Framework}
A typical generative retrieval framework takes a query as input, and generates the corresponding relevant document identifiers as the retrieval results~\citep{DBLP:conf/nips/Tay00NBM000GSCM22}. Because each document in the corpus has a unique identifier, one can then use these identifiers to retrieve the corresponding documents for downstream tasks.

\subsection{Document Identifiers}
We primarily use \textit{semantic} document identifiers in our experiments due to their superior performance and better scalability to larger corpora.
Instead of using corpus-specific semantic identifiers like titles or urls, we adopt a more general, keyword-based approach that can be applied to a wide variety of corpora~\citep{DBLP:journals/corr/abs-2208-09257}.
Specifically, we instruct an LLM to produce a list of keywords that describes the content of a document, and use this keyword list as its semantic identifier.

In addition, we extended our synthetic data strategies to other types of identifiers to validate its generalizability, such as atomic identifiers ~\citep{DBLP:conf/nips/Tay00NBM000GSCM22}, which are unique tokens that can be generated through a one-step decoding or classification process.

\subsection{Generative Modeling}
The generative retrieval model learns to generate the identifier of a relevant document given a query. Formally, for a query $q$ and a relevant document $d$ with identifier $d'$, generative retrieval aims to produce $d'$ given $q$, which can be represented as:
\newcommand{\score}[1]{\operatorname{score}(#1)}
\begin{align*}
\score{q,d} &= P\left(d'\mid q; \theta\right) \\
&= \prod_{i}P\left(d'_i \mid d'_{<i}, q; \theta \right),
\end{align*}
where $d'_{i}$ is the $i^\text{th}$ token of the identifier. To ensure the generated identifiers are valid during inference, we use constrained beam search with Trie~\citep{DBLP:journals/corr/abs-2010-00904} to restrict the output token space at each decoding step. The top-$k$ output from the beam search serves as the final retrieval results.

Compared to dense retrieval models~\cite{karpukhin2020dense}, generative retrieval bypasses the need for an external index by directly producing relevant document identifiers. However, there are distinct challenges in learning a generative retrieval model.
As it solely relies on parametric knowledge, the model must not only learn the retrieval task, but also capture and encode document content in a way that associates each document with its identifier. Therefore, generative retrieval often requires training on the entire corpus to enable the model to memorize and comprehend the necessary information effectively. 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth, trim={0.6cm 0.2cm 0.6cm 0.3cm}, clip]{figures/generative_retrieval_workflow.pdf}
    \caption{The overall workflow of the generative retrieval training and synthetic data utilization at each stage.}
    \label{fig:workflow}
\end{figure*}
