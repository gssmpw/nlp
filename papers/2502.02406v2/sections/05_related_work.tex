\section{Related Work}
\textbf{Multimodal Large Langauge Models} There are two main classes of MLLM designs. The first design concatenates tokenized visual inputs with text tokens and feeds them into the LLM. Models such as LLaVA~\cite{liu2023llava}, InternVL~\cite{chen2024internvl}, NVILA~\cite{liu2024nvila}, and MiniGPT-4~\cite{zhu2024minigpt} follow this paradigm. Although this approach naturally extends text-only LLMs, the large number of tokens significantly slows down both training and inference~\cite{ye2024mplugowl3, grattafiori2024llama3v}. The second design relies on cross-attention mechanisms, where cross-attention layers are inserted between LLM layers to incorporate visual features into its intermediate representations. Models such as Flamingo~\cite{alayrac2022flamingo}, IDEFICS~\cite{laurencon2023idefics}, Otter~\cite{li2023otter}, mPLUG-Owl3~\cite{ye2024mplugowl3} and Llama 3-V~\cite{grattafiori2024llama3v} adopt this strategy. While this method avoids processing a large number of visual tokens through the LLM backbone, cross-attention layers remain computationally expensive with current sequence-parallel approaches~\cite{grattafiori2024llama3v}. In this work, we introduce LV-XAttn to address this bottleneck.

\textbf{Memory-efficient Attention} The attention operation has a memory complexity that scales quadratically with sequence length, limiting its scalability for longer contexts. Approximate methods~\cite{kitaev2020reformer, zaheer2020bigbird, beltagy2020longformer, choromanski2021performer, ding2023longnet} and compression techniques~\cite{chevalier2023autocompressors, munkhdalai2024infini-attention} reduce memory requirements by sacrificing some model quality. For exact attention, FlashAttention~\cite{dao2022fa, dao2023fa2} proposes block-wise computation, which reduces memory complexity to linear while providing runtime speedups by minimizing I/O between GPU HBM and SRAM. However, for applications like long video understanding, which require context lengths exceeding a single GPU's memory capacity, a distributed setup is necessary.

\textbf{Parallelism} Distributed attention can be classified into head-parallelism approaches, such as Megatron-LM~\cite{korthikanti2023megatron-lm} and Deepspeed-Ulysses~\cite{jacobs2024ds}, and sequence-parallelism approaches like Ring Attention, its variants DistFlashAttn~\cite{li2024distflashattn}, and Striped Attention~\cite{brandon2023striped}. As discussed in Section~\ref{sec:background}, these methods face significant communication. Our work proposes a solution with minimal communication overhead for cross-attention in MLLMs. General parallelism approaches, such as data parallelism~\cite{dean2012data-parallel}, pipeline parallelism~\cite{narayanan2019pipedream}, tensor parallelism~\cite{shoeybi2019megatron}, and FSDP~\cite{rajbhandari2020zero}, can be combined with our approach.
