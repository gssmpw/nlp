\section{Evaluation}
\label{sec:eval}
\newcommand{\highlight}[1]{{\leavevmode\textbf{#1}}}
\subsection{Experimental Setup}
\label{sec:eval/setup}
\begin{table}[tbp]
    \centering
    \caption{Evaluated models.}
    \begin{tabular}{l|c|c}
        \hline
         Model & \makecell{Num. of\\ CA Layers} & \makecell{Num. of\\ LM Blocks}\\ \hline
         mPLUG-Owl3-7b & 4 & 28 \\
         mPLUG-Owl3-2b & 4 & 28 \\
         mPLUG-Owl3-1b & 4 & 24 \\
         OpenFlamingo-9b & 8 & 32 \\
         OpenFlamingo-3b & 24 & 24 \\ \hline         
    \end{tabular}
    \label{tab:evaluated-models}
    \vspace{-2ex}
\end{table}
\textbf{Model Setup} We evaluate our methods and baselines on 5 models shown in Table~\ref{tab:evaluated-models}. Following their default configuration, each frame is encoded into 729 visual tokens for the mPLUG-Owl3 models and 64 visual tokens for the OpenFlamingo models. This implies that given the same amount of memory capacity, we can fit more frames to OpenFlamingo models than mPLUG-Owl3 models. 

For all models, a special token \texttt{<image>} must be included in the text prompt for each frame. Consequently, the length of the text prompt must be at least equal to the number of frames in the visual input. 

We use a batch size of 1 and fully sharded tensor parallelism for all models to enable a larger context length.

\textbf{Cluster Setup} We evaluate our method and baselines on the following configurations: (1) A 16-GPU cluster, each node equipped with 4 A100 80GB GPUs, with the GPUs within a node interconnected via NVLink and a cross-node bandwidth of 25 GB/s, representing a typical setting for cross-node training of up to millions of tokens. (2) An 8-GPU cluster, each node equipped with 1 A30 24GB GPU, with a cross-node bandwidth of 1.25 GB/s, representing a more resource-constrained setup with slower interconnect bandwidth. (3) A 12-GPU cluster, each node equipped with 3 A100 40GB GPUs, with the GPUs interconnected via 64 GB/s PCIe and a cross-node bandwidth of 25 GB/s, used for smaller-scale case studies and ablation studies.

\textbf{Baselines} For our method, we use LV-XAttn for the cross-attention layers and Ring Attention for the LM blocks. Our primary baseline is the setup where Ring Attention is used for both the cross-attention layers and LM blocks. We apply our activation recomputation technique to both of these settings for enabling longer context length. We also compare against Deepspeed-Ulysses~\cite{jacobs2024ds}, which employs sequence parallelism for non-attention layers and head parallelism for attention layers. All methods uses Flash Attention.

\subsection{Comparison with Ring Attention}
\begin{table*}[ht]\centering
\caption{Per iteration wall-clock time (in seconds) on 16 A100 80GB GPUs with Ring Attention and LV-XAttn. ``CA" represents the time spent on cross-attention operations. As $S_Q$ doubles, the cross-attention speedup nearly halves because the runtime for Ring Attention, which is communication-bound, remains constant, while the runtime for LV-XAttention, which is computation-bound, doubles. On the other hand, as $S_{KV}$ doubles, both communication and computation also double, so the speedup remains roughly the same.}
\begin{tabular}{|l|ll|ll|ll|ll|ll|}
\hline
\multirow{2}{*}{Model} & Text & Frame & \multirow{2}{*}{$S_Q$} & \multirow{2}{*}{$S_{KV}$} & \multicolumn{2}{|c|}{Ring Attention} & \multicolumn{2}{|c|}{LV-XAttn} & \multicolumn{2}{|c|}{Speedup} \\ \cline{6-11}
& length  & count & & & CA (s) & Total (s) & CA (s) & Total (s) & CA & Total \\ \hline
\multirow{3}{*}{mPLUG-Owl3-7b} & 8K & 4K & 8K & 2916K & 174.73 & 202.84 & 24.08 & 42.79 & 7.26$\times$ & 4.74$\times$ \\
& 8K & 2K & 8K & 1458K & 89.88 & 112.28 & 12.14 & 32.72 & 7.41$\times$ & 3.43$\times$ \\
& 4K & 2K & 4K & 1458K & 92.48 & 107.01 & 6.45 & 19.5 & \highlight{14.33$\times$} & \highlight{5.49$\times$} \\
% & 4K & 1K & 4K & 729K & 47.72 & 64.07 & 3.28 & 19.44 & 14.55$\times$ & 3.3$\times$ \\
\hline\multirow{3}{*}{mPLUG-Owl3-2b} & 8K & 4K & 8K & 2916K & 83.41 & 90.1 & 10.33 & 17.39 & 8.07$\times$ & 5.18$\times$ \\
& 8K & 2K & 8K & 1458K & 36.66 & 45.42 & 5.21 & 11.28 & 7.04$\times$ & 4.03$\times$ \\
& 4K & 2K & 4K & 1458K & 37.78 & 44.8 & 2.79 & 8.25 & \highlight{13.52$\times$} & \highlight{5.43$\times$} \\
% & 4K & 1K & 4K & 729K & 19.31 & 24.62 & 1.44 & 5.29 & 13.42$\times$ & 4.65$\times$ \\
\hline\multirow{3}{*}{mPLUG-Owl3-1b} & 8K & 4K & 8K & 2916K & 47.12 & 55.1 & 5.17 & 12.69 & 9.12$\times$ & 4.34$\times$ \\
& 8K & 2K & 8K & 1458K & 22.63 & 28.81 & 2.62 & 7.99 & 8.64$\times$ & 3.6$\times$ \\
& 4K & 2K & 4K & 1458K & 23.26 & 29.24 & 1.52 & 5.24 & \highlight{15.32$\times$} & \highlight{5.58$\times$} \\
% & 4K & 1K & 4K & 729K & 11.2 & 14.22 & 0.81 & 3.3 & 13.89$\times$ & 4.31$\times$ \\
\hline\multirow{3}{*}{OpenFlamingo-9b} & 64K & 64K & 64K & 4096K & 95.13 & 165.17 & 62.4 & 126.71 & 1.52$\times$ & 1.3$\times$ \\
& 64K & 32K & 64K & 2048K & 47.86 & 101.01 & 31.44 & 86.89 & 1.52$\times$ & 1.16$\times$ \\
& 32K & 32K & 32K & 2048K & 33.58 & 69.53 & 16.0 & 49.5 & \highlight{2.1$\times$} & \highlight{1.4$\times$} \\
% & 32K & 16K & 32K & 1024K & 17.47 & 48.9 & 8.07 & 36.36 & 2.16$\times$ & 1.35$\times$ \\
\hline\multirow{3}{*}{OpenFlamingo-3b} & 64K & 64K & 64K & 4096K & 276.69 & 306.51 & 187.45 & 226.04 & 1.48$\times$ & 1.36$\times$ \\
& 64K & 32K & 64K & 2048K & 138.05 & 166.36 & 94.1 & 120.09 & 1.47$\times$ & 1.39$\times$ \\
& 32K & 32K & 32K & 2048K & 102.82 & 118.01 & 47.98 & 61.57 & \highlight{2.14$\times$} & \highlight{1.92$\times$} \\
% & 32K & 16K & 32K & 1024K & 50.49 & 62.67 & 24.18 & 35.38 & 2.09$\times$ & 1.77$\times$ \\
\hline
\end{tabular}
\label{tab:end_to_end_nersc}
\end{table*}

\begin{table*}[ht]\centering
\caption{Per iteration wall-clock time (in seconds) on 8 A30 24GB GPUs with Ring Attention and LV-XAttn. ``CA" represents the time spent on cross-attention operations.}
\begin{tabular}{|l|ll|ll|ll|ll|ll|}
\hline
\multirow{2}{*}{Model} & Text & Frame & \multirow{2}{*}{$S_Q$} & \multirow{2}{*}{$S_{KV}$} & \multicolumn{2}{|c|}{Ring Attention} & \multicolumn{2}{|c|}{LV-XAttn} & \multicolumn{2}{|c|}{Speedup} \\ \cline{6-11}
& length  & count & & & CA (s) & Total (s) & CA (s) & Total (s) & CA & Total \\ \hline
\multirow{3}{*}{mPLUG-Owl3-7b} & 1K & 512 & 1K & 364K & 42.41 & 74.28 & 1.42 & 33.32 & 29.96$\times$ & 2.23$\times$ \\
& 1K & 256 & 1K & 182K & 20.81 & 50.61 & 0.66 & 30.63 & 31.31$\times$ & 1.65$\times$ \\
& 512 & 256 & 512 & 182K & 20.84 & 49.89 & 0.45 & 28.95 & \highlight{45.85$\times$} & \highlight{1.72$\times$} \\
% & 512 & 128 & 512 & 91K & 10.53 & 39.11 & 0.27 & 28.53 & 39.28$\times$ & 1.37$\times$ \\
\hline\multirow{3}{*}{mPLUG-Owl3-2b} & 1K & 512 & 1K & 364K & 17.85 & 25.94 & 0.78 & 8.71 & 22.89$\times$ & 2.98$\times$ \\
& 1K & 256 & 1K & 182K & 9.06 & 16.56 & 0.44 & 7.86 & 20.6$\times$ & 2.11$\times$ \\
& 512 & 256 & 512 & 182K & 9.15 & 16.34 & 0.3 & 7.44 & \highlight{30.39$\times$} & \highlight{2.19$\times$} \\
% & 512 & 128 & 512 & 91K & 4.64 & 11.18 & 0.2 & 6.61 & 23.42$\times$ & 1.69$\times$ \\
\hline\multirow{3}{*}{mPLUG-Owl3-1b} & 1K & 512 & 1K & 364K & 10.6 & 14.41 & 0.44 & 4.18 & 24.25$\times$ & 3.45$\times$ \\
& 1K & 256 & 1K & 182K & 5.38 & 8.4 & 0.25 & 3.36 & 21.19$\times$ & 2.5$\times$ \\
& 512 & 256 & 512 & 182K & 5.31 & 8.22 & 0.18 & 3.03 & \highlight{29.44$\times$} & \highlight{2.71$\times$} \\
% & 512 & 128 & 512 & 91K & 2.7 & 5.16 & 0.12 & 2.53 & 22.05$\times$ & 2.04$\times$ \\
\hline\multirow{3}{*}{OpenFlamingo-9b} & 8K & 8K & 8K & 512K & 17.28 & 65.75 & 3.99 & 53.22 & 4.33$\times$ & 1.24$\times$ \\
& 8K & 4K & 8K & 256K & 8.74 & 54.09 & 2.2 & 52.17 & 3.97$\times$ & 1.04$\times$ \\
& 4K & 4K & 4K & 256K & 8.87 & 52.04 & 1.23 & 44.18 & \highlight{7.2$\times$} & \highlight{1.18$\times$} \\
% & 4K & 2K & 4K & 128K & 4.49 & 44.04 & 0.69 & 41.14 & 6.54$\times$ & 1.07$\times$ \\
\hline\multirow{3}{*}{OpenFlamingo-3b} & 8K & 8K & 8K & 512K & 52.26 & 69.45 & 12.25 & 32.71 & 4.27$\times$ & 2.12$\times$ \\
& 8K & 4K & 8K & 256K & 26.09 & 41.73 & 6.43 & 22.22 & 4.06$\times$ & 1.88$\times$ \\
& 4K & 4K & 4K & 256K & 25.84 & 40.59 & 3.62 & 18.28 & \highlight{7.14$\times$} & \highlight{2.22$\times$} \\
% & 4K & 2K & 4K & 128K & 13.18 & 29.17 & 2.19 & 16.36 & 6.03$\times$ & 1.78$\times$ \\
\hline
\end{tabular}
\label{tab:end_to_end_cl}
\end{table*}

Table~\ref{tab:end_to_end_nersc} shows the per iteration time of 5 models using LV-XAttn and Ring Attention on 16 A100 80GB GPUs. For the mPLUG-Owl3 models, LV-XAttn speeds up the cross-attention operation by 7.04 -- 15.32$\times$. Since the cross-attention operation accounts for the majority of the total iteration time when using Ring Attention, this reduction results in a significant total iteration speedup of 3.3 -- 5.58$\times$. For the OpenFlamingo models, which process a larger number of frames and thus have longer text lengths (due to the inclusion of a special token \texttt{<image>} per frame) and larger $S_Q$, the speedup is less pronounced, LV-XAttn achieves 1.47 -- 2.16$\times$ speedup on the cross-attention operation and 1.16 -- 1.92$\times$ speedup on the total iteration time. Additionally, OpenFlamingo-3b, with denser cross-attention layers, spends a larger portion of its time in cross-attention compared to OpenFlamingo-9b when using Ring Attention. Consequently, the speedup in cross-attention translates to a more substantial end-to-end speedup for OpenFlamingo-3b.

Table~\ref{tab:end_to_end_cl} shows the same experiment on 8 A30 24GB GPUs. We have smaller text lengths and fewer frames due to the smaller memory capacity. In this setup, the speedup for cross-attention operation is greater than that on 16 A100 GPUs: 20.6 -- 45.85$\times$ for the mPLUG-Owl3 models and 3.97 -- 7.2$\times$ for the OpenFlamingo models. This is due to smaller query block sizes $\frac{S_Q}{n}$ (shorter computations favors computation-bound LV-XAttn) and slower interconnect bandwidth (longer communication hurts communication-bound Ring Attention), as shown in Table~\ref{tab:runtime-formula}. However, the larger cross-attention speedups do not translate into a larger total speedup, as the portion of time spent on cross-attention layers decreases due to slower self-attention layers in LM blocks (caused by the slower interconnect). Despite this, the total speedup remains 1.37 -- 3.45$\times$ for the mPLUG-Owl3 models and 1.04 -- 2.22$\times$ for the OpenFlamingo models.

\subsection{Comparison with DeepSpeed-Ulysses}
\begin{table}[tb]
\centering
\caption{Per iteration wall-lock time (in seconds) of mPLUG-Owl3-2b ran on A100 80GB GPUs. The model uses multi-head attention with 12 heads.}
\begin{tabular}{|c|cc|c|c|} \hline
\makecell{Cluster\\Config.} & \makecell{Text /\\worker}& \makecell{Frame /\\worker} & DS (s) & LV-XAttn (s) \\ \hline
\multirow{3}{*}{12 GPUs} & 512 & 256 & OOM   & \textbf{13.38} \\ 
& 512 & 128 & 12.15 & \textbf{8.71} \\ 
& 256 & 128 & 9.32  & \textbf{6.1} \\ \hline
% 12 GPUs & 256 & 64  & 256 & 46K  & 5.82  & \textbf{4.78} \\ 
\multirow{3}{*}{6 GPUs}  & 512 & 256 & 16.36 & \textbf{10.58} \\ 
& 512 & 128 & 10.41 & \textbf{7.09} \\ 
& 256 & 128 & 8.81  & \textbf{5.83} \\ \hline
% 6 GPUs  & 256 & 64  & 256 & 46K  & 6.4   & \textbf{4.68} \\ 
\multirow{3}{*}{3 GPUs}  & 512 & 256 & 15.64 & \textbf{10.11} \\ 
 & 512 & 128 & 10.61 & \textbf{7.8} \\ 
 & 256 & 128 & 9.91  & \textbf{7.37} \\
% 3 GPUs  & 256 & 64  & 256 & 46K  & 7.71  & \textbf{6.39} \\ 
\hline
\end{tabular}
\label{tab:ds-owl}
\end{table}

\begin{table}[tb]
\centering
\caption{Per iteration wall-lock time (in seconds) of OpenFlamingo-3b ran on A30 24GB GPUs. The model uses multi-head attention with 8 heads.}
\begin{tabular}{|c|cc|c|c|} \hline
\makecell{Cluster\\Config.} & \makecell{Text /\\worker}& \makecell{Frame /\\worker} & DS (s) & LV-XAttn (s) \\ \hline
\multirow{3}{*}{8 GPUs} & 1K & 1K & OOM  & \textbf{32.71} \\ 
& 512 & 512 & OOM & \textbf{18.28} \\ 
& 256 & 256 & OOM & \textbf{14.46} \\ \hline
% 12 GPUs & 256 & 64  & 256 & 46K  & 5.82  & \textbf{4.78} \\ 
\multirow{3}{*}{4 GPUs}  & 1K & 1K & OOM & \textbf{19.71} \\ 
& 512 & 512 & OOM & \textbf{13.34} \\ 
& 256 & 256 & 11.65  & \textbf{11.29} \\ \hline
% 6 GPUs  & 256 & 64  & 256 & 46K  & 6.4   & \textbf{4.68} \\ 
\multirow{3}{*}{2 GPUs}  & 1K & 1K & OOM & \textbf{13.75} \\ 
 & 512 & 512 & 10.19 & \textbf{9.24} \\ 
 & 256 & 256 & 8.04  & \textbf{7.87} \\
% 3 GPUs  & 256 & 64  & 256 & 46K  & 7.71  & \textbf{6.39} \\ 
\hline
\end{tabular}
\label{tab:ds-flamingo}
\end{table}

% \begin{table*}[ht]\centering
% \caption{Per iteration wall-lock time (in seconds) of mPLUG-Owl3-2b ran on A100 80GB GPUs. The model uses multi-head attention with 12 heads. Since Deepspeed-Ulysses requires the number of heads to be divisible by the number workers, we run (TODO)}
% \begin{tabular}{|cc|cc|c|c|c|c|c|c|} \hline
% Text length & Frame count & \multirow{2}{*}{$\frac{S_Q}{n}$} & \multirow{2}{*}{$\frac{S_{KV}}{n}$} & \multicolumn{2}{c|}{3 GPUs} & \multicolumn{2}{c|}{6 GPUs} & \multicolumn{2}{c|}{12 GPUs} \\ \cline{5-10}
% per worker & per worker & & & DS & LV-XAttn & DS & LV-XAttn & DS & LV-XAttn \\ \hline
% 512 & 256 & 512 & 182K & 15.64 & \textbf{10.11} & 16.36 & \textbf{10.58} & OOM & \textbf{13.38} \\ 
% 512 & 128 & 512 & 91K & 10.61 & \textbf{7.8} & 10.41 & \textbf{7.09} & 12.15 & \textbf{8.71} \\ 
% 256 & 128 & 256 & 91K & 9.91 & \textbf{7.37} & 8.81 & \textbf{5.83} & 9.32 & \textbf{6.1} \\ 
% % 256 & 64 & 256 & 46K & 7.71 & \textbf{6.39} & 6.4 & \textbf{4.68} & 5.82 & \textbf{4.78} \\ 
% \hline
% \end{tabular}
% \label{tab:ds-owl}
% \end{table*}

% \begin{table*}[ht]\centering
% \caption{Per iteration wall-clock time (in seconds) of mPLUG-Owl3-2b ran on A100 80GB GPUs. The model uses multi-head attention with 12 heads. Since Deepspeed-Ulysses requires the number of heads to be divisible by the number of workers, we run (TODO)}
% \begin{tabular}{|cc|cc|c|c|c|c|c|c|c|} \hline
% Text length & Frame count & \multirow{2}{*}{$\frac{S_Q}{n}$} & \multirow{2}{*}{$\frac{S_{KV}}{n}$} & \multicolumn{3}{c|}{Deepspeed-Ulysses} & \multicolumn{3}{c|}{LV-XAttn} \\ \cline{5-10}
%  per worker & per worker & & & 3 GPUs & 6 GPUs & 12 GPUs & 3 GPUs & 6 GPUs & 12 GPUs \\ \hline
% 512 & 256 & 512 & 182K & 15.64 & 16.36 & OOM & \textbf{10.11} & \textbf{10.58} & \textbf{13.38} \\ 
% 512 & 128 & 512 & 91K & 10.61 & 10.41 & 12.15 & \textbf{7.8} & \textbf{7.09} & \textbf{8.71} \\ 
% 256 & 128 & 256 & 91K & 9.91 & 8.81 & 9.32 & \textbf{7.37} & \textbf{5.83} & \textbf{6.1} \\ 
% % 256 & 64 & 256 & 46K & 7.71 & 6.4 & 5.82 & \textbf{6.39} & \textbf{4.68} & \textbf{4.78} \\ 
% \hline
% \end{tabular}
% \label{tab:ds-owl}
% \end{table*}

% \begin{table*}[ht]\centering
% \caption{OpenFlamingo-3b on A30 cluster. 8 heads}
% \begin{tabular}{|cc|cc|c|c|c|c|c|c|c|} \hline
% Text length & Frame count & \multirow{2}{*}{$\frac{S_Q}{n}$} & \multirow{2}{*}{$\frac{S_{KV}}{n}$} & \multicolumn{3}{c|}{Deepspeed-Ulysses} & \multicolumn{3}{c|}{LV-XAttn} \\ \cline{5-10}
% per worker & per worker & & & 2 GPUs & 4 GPUs & 8 GPUs & 2 GPUs & 4 GPUs & 8 GPUs \\ \hline
% 1K & 1K & 1K & 64K & OOM & OOM & OOM & 13.75 & 19.71 & 32.71 \\ 
% 512 & 512 & 512 & 32K & 10.19 & OOM & OOM & 9.24 & 13.34 & 18.28 \\ 
% 256 & 256 & 256 & 16K & 8.04 & 11.65 & OOM & 7.87 & 11.29 & 14.46 \\ 
% 128 & 128 & 128 & 8K & 7.09 & 10.32 & 12.02 & 7.22 & 10.27 & 12.94 \\ 
% \hline\end{tabular}
% \label{tab:ds-flamingo}
% \end{table*}
For Deepspeed-Ulysses, each attention operation involves two all-to-all communications: one before the computation to gather input query, key and value blocks, and another afterward to distribute attention output along the sequence dimension. The first all-to-all is expensive as it involves communicating the large key-value blocks. To see this, we compare Deepspeed-Ulysses with LV-XAttn on mPLUG-Owl3-2b using the cluster with A100 80GB GPUs. As shown in Table~\ref{tab:ds-owl}, LV-XAttn achieves 1.21 -- 1.55$\times$ speedup compared to Deepspeed-Ulysses.

In addition, without activation recomputation, the larger memory footprint of Deepspeed-Ulysses limits its ability to process large visual inputs. When using the OpenFlamingo-3b model on the cluster with A30 24GB GPUs, Table~\ref{tab:ds-owl} shows that LV-XAttn is able to process up to $4\times$ longer text and visual inputs compared to Deepspeed-Ulysses.

Notably, the head parallelism in Deepspeed-Ulysses restricts both its scalability and flexibility: the maximum degree of parallelism is limited by the number of heads, and the number of heads has to be divisible by the number of workers.
\subsection{Ablation Study}
\label{sec:eval/ablation}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_overlap.pdf}
    \caption{Ablation study on the effect of overlapping communication and computation with 6 A100 40GB GPUs. The frame count is set to 2048 per worker. Since processing the same total number of frames on a single GPU is not feasible due to memory constraints, the ``no communication'' runtime is derived by running the same per-worker input size on a single GPU and then scaling the result by 6. LV-XAttn incurs an overhead of less than 0.42\% compared to the no-communication baseline.}
    \label{fig:ablation_overlap}
\end{figure}
\textbf{Overlapping Communication and Computation} Figure~\ref{fig:ablation_overlap} shows the time spent on cross-attention in OpenFlamingo-3b using Ring Attention and LV-XAttn, with and without overlapping communication and computation, on 6 A100 40GB GPUs. While overlapping reduces the runtime for Ring Attention, its effect is limited as the large communication overhead of key-value blocks cannot be fully hidden by computation. In contrast, LV-XAttn reduces communication time by transmitting significantly smaller query, output, and softmax statistics blocks. The overlapping further hides the communication time, enabling distributed attention with no communication overhead.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.97\linewidth]{figures/ablation_memory_owl.pdf}
        \caption{mPLUG-Owl-7b}
        \label{fig:ablation_mem_owl}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=0.97\linewidth]{figures/ablation_memory_flamingo.pdf}
        \caption{OpenFlamingo-3b}
        \label{fig:ablation_mem_flamingo}
    \end{subfigure}
    \caption{Ablation study on the effect of activation recomputation for cross-attention layers with 3 A30 24GB GPUs. Text length is set to $2K$ and $8K$ for mPLUG-Owl-7b and OpenFlamingo-3b, respectively.}
    \label{fig:ablation_mem}
    \vspace{-3ex}
\end{figure}
\textbf{Activation Recomputation} Figure~\ref{fig:ablation_mem_owl} and \ref{fig:ablation_mem_flamingo} show the iteration time for running mPLUG-Owl-7b and OpenFlamingo-3b on a single node with 3 A100 40GB GPUs, with and without employing activation recomputation for cross-attention layers. By omitting the saving of large key-value blocks, the reduced memory consumption enables the processing of a larger number of frames, increasing by 1.6$\times$ and 1.5$\times$ for mPLUG-Owl-7b and OpenFlamingo-3b, respectively, with a negligible overhead of less than 8\%.


% \begin{table}[ht]\centering
% \caption{Owl Activation Saving}
% \begin{tabular}{|c|c|c|} \hline
% $\frac{S_Q}{n}$ & Save & No Save \\ \hline
% 3072 & 64 & 144 \\ 
% 2536 & 96 & 160 \\ 
% 2048 & 128 & 208 \\ 
% \hline\end{tabular}
% \end{table}



% \begin{table*}[ht]\centering
% \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
% \hline
% \multirow{2}{*}{Model} & \multirow{2}{*}{Q Length} & \multirow{2}{*}{KV Length} & \multicolumn{3}{|c|}{Forward Time (sec)} & \multicolumn{3}{|c|}{Backward Time (sec)} \\ \cline{4-9}
%  &  &  & Ring & SplitQ & DupQ & Ring & SplitQ & DupQ \\ \hline
% \multirow{6}{*}{owl-7b} & \multirow{2}{*}{512} & \multirow{2}{*}{72900} & 13.09 (1.0x) & 1.26 (10.39x) & 1.43 (9.16x) & 45.72 (1.0x) & 15.82 (2.89x) & 16.56 (2.76x) \\ \cline{4-9}
%  & & & 23.09 (1.0x) & 11.13 (2.07x) & 10.91 (2.12x) & 60.15 (1.0x) & 27.93 (2.15x) & 28.91 (2.08x) \\ \cline{2-9}
% & \multirow{2}{*}{512} & \multirow{2}{*}{36450} & 6.61 (1.0x) & 0.71 (9.25x) & 1.01 (6.53x) & 27.32 (1.0x) & 11.67 (2.34x) & 11.91 (2.29x) \\ \cline{4-9}
%  & & & 14.87 (1.0x) & 8.79 (1.69x) & 9.32 (1.6x) & 40.65 (1.0x) & 24.29 (1.67x) & 25.16 (1.62x) \\ \cline{2-9}
% & \multirow{2}{*}{256} & \multirow{2}{*}{36450} & 6.45 (1.0x) & 0.45 (14.27x) & 0.52 (12.39x) & 27.07 (1.0x) & 9.72 (2.79x) & 10.14 (2.67x) \\ \cline{4-9}
%  & & & 14.27 (1.0x) & 8.3 (1.72x) & 8.5 (1.68x) & 38.73 (1.0x) & 21.12 (1.83x) & 21.53 (1.8x) \\ \cline{2-9}
% \hline\hline\multirow{6}{*}{owl-2b} & \multirow{2}{*}{512} & \multirow{2}{*}{72900} & 5.62 (1.0x) & 0.5 (11.23x) & 0.64 (8.81x) & 12.87 (1.0x) & 4.36 (2.95x) & 4.58 (2.81x) \\ \cline{4-9}
%  & & & 8.34 (1.0x) & 3.21 (2.6x) & 3.37 (2.47x) & 13.96 (1.0x) & 5.27 (2.65x) & 5.47 (2.55x) \\ \cline{2-9}
% & \multirow{2}{*}{512} & \multirow{2}{*}{36450} & 2.86 (1.0x) & 0.29 (9.85x) & 0.44 (6.5x) & 7.59 (1.0x) & 3.16 (2.41x) & 3.35 (2.27x) \\ \cline{4-9}
%  & & & 5.1 (1.0x) & 2.69 (1.89x) & 2.75 (1.86x) & 8.24 (1.0x) & 3.83 (2.15x) & 4.06 (2.03x) \\ \cline{2-9}
% & \multirow{2}{*}{256} & \multirow{2}{*}{36450} & 2.81 (1.0x) & 0.21 (13.1x) & 0.23 (12.0x) & 7.43 (1.0x) & 2.35 (3.16x) & 2.42 (3.06x) \\ \cline{4-9}
%  & & & 5.13 (1.0x) & 2.92 (1.76x) & 2.58 (1.99x) & 8.0 (1.0x) & 2.93 (2.73x) & 3.0 (2.67x) \\ \cline{2-9}
% \hline\hline\multirow{6}{*}{owl-1b} & \multirow{2}{*}{512} & \multirow{2}{*}{72900} & 3.34 (1.0x) & 0.6 (5.52x) & 0.49 (6.81x) & 7.2 (1.0x) & 1.7 (4.24x) & 1.71 (4.21x) \\ \cline{4-9}
%  & & & 5.41 (1.0x) & 2.47 (2.19x) & 2.29 (2.36x) & 7.49 (1.0x) & 1.92 (3.89x) & 1.93 (3.88x) \\ \cline{2-9}
% & \multirow{2}{*}{512} & \multirow{2}{*}{36450} & 1.67 (1.0x) & 0.33 (5.11x) & 0.32 (5.17x) & 3.88 (1.0x) & 1.14 (3.4x) & 1.18 (3.27x) \\ \cline{4-9}
%  & & & 3.37 (1.0x) & 1.78 (1.9x) & 1.88 (1.79x) & 4.11 (1.0x) & 1.36 (3.01x) & 1.4 (2.93x) \\ \cline{2-9}
% & \multirow{2}{*}{256} & \multirow{2}{*}{36450} & 1.62 (1.0x) & 0.19 (8.41x) & 0.17 (9.47x) & 3.74 (1.0x) & 0.78 (4.78x) & 0.78 (4.77x) \\ \cline{4-9}
%  & & & 3.29 (1.0x) & 2.05 (1.61x) & 2.4 (1.37x) & 3.96 (1.0x) & 0.99 (3.99x) & 0.99 (4.02x) \\ \cline{2-9}
% \hline\hline\multirow{6}{*}{flamingo-9b} & \multirow{2}{*}{128} & \multirow{2}{*}{131072} & 14.02 (1.0x) & 7.37 (1.9x) & 7.58 (1.85x) & 23.19 (1.0x) & 11.33 (2.05x) & 11.27 (2.06x) \\ \cline{4-9}
%  & & & 18.49 (1.0x) & 12.78 (1.45x) & 12.75 (1.45x) & 25.89 (1.0x) & 12.49 (2.07x) & 12.5 (2.07x) \\ \cline{2-9}
% & \multirow{2}{*}{128} & \multirow{2}{*}{65536} & 10.67 (1.0x) & 7.66 (1.39x) & 7.46 (1.43x) & 17.09 (1.0x) & 10.47 (1.63x) & 10.34 (1.65x) \\ \cline{4-9}
%  & & & 13.56 (1.0x) & 10.68 (1.27x) & 10.55 (1.29x) & 18.82 (1.0x) & 12.38 (1.52x) & 12.27 (1.53x) \\ \cline{2-9}
% & \multirow{2}{*}{64} & \multirow{2}{*}{65536} & 10.65 (1.0x) & 7.6 (1.4x) & 7.33 (1.45x) & 17.35 (1.0x) & 9.63 (1.8x) & 9.59 (1.81x) \\ \cline{4-9}
%  & & & 15.44 (1.0x) & 10.41 (1.48x) & 11.2 (1.38x) & 18.69 (1.0x) & 11.97 (1.56x) & 11.92 (1.57x) \\ \cline{2-9}
% \hline\hline\multirow{6}{*}{flamingo-3b} & \multirow{2}{*}{128} & \multirow{2}{*}{131072} & 22.46 (1.0x) & 4.18 (5.38x) & 2.86 (7.86x) & 45.24 (1.0x) & 5.34 (8.47x) & 5.13 (8.83x) \\ \cline{4-9}
%  & & & 26.23 (1.0x) & 8.01 (3.28x) & 7.15 (3.67x) & 45.47 (1.0x) & 5.51 (8.26x) & 5.26 (8.64x) \\ \cline{2-9}
% & \multirow{2}{*}{128} & \multirow{2}{*}{65536} & 11.78 (1.0x) & 3.15 (3.74x) & 2.44 (4.83x) & 23.72 (1.0x) & 4.36 (5.44x) & 4.26 (5.56x) \\ \cline{4-9}
%  & & & 14.51 (1.0x) & 6.64 (2.18x) & 5.78 (2.51x) & 23.86 (1.0x) & 4.51 (5.29x) & 4.4 (5.43x) \\ \cline{2-9}
% & \multirow{2}{*}{64} & \multirow{2}{*}{65536} & 12.18 (1.0x) & 3.12 (3.9x) & 2.24 (5.43x) & 24.23 (1.0x) & 3.89 (6.23x) & 3.71 (6.54x) \\ \cline{4-9}
%  & & & 14.85 (1.0x) & 5.78 (2.57x) & 5.33 (2.79x) & 24.37 (1.0x) & 4.04 (6.03x) & 3.85 (6.33x) \\ \cline{2-9}
% \hline
% \end{tabular}
% \caption{For each Q, KV length combination and method, two numbers are reported. The top row is the time spent on cross attention during a model pass, while the bottom row is that of the total model pass.)}
% \label{tab:your_label}
% \end{table*}
% \begin{table*}[ht]\centering
% \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
% \hline
% \multirow{2}{*}{Model} & \multirow{2}{*}{Q Length} & \multirow{2}{*}{KV Length} & \multicolumn{3}{|c|}{Forward Time (sec)} & \multicolumn{3}{|c|}{Backward Time (sec)} \\ \cline{4-9}
%  &  &  & Ring & SplitQ & DupQ & Ring & SplitQ & DupQ \\ \hline
% \multirow{6}{*}{Flamingo-3b} & \multirow{2}{*}{4096} & \multirow{2}{*}{262144} & 69.57 (1.0x) & 45.22 (1.54x) & OOM & 209.59 (1.0x) & 142.5 (1.47x) & OOM \\ \cline{4-9}
%  & & & 102.56 (1.0x) & 71.85 (1.43x) & OOM & 226.38 (1.0x) & 154.48 (1.47x) & OOM \\ \cline{2-9}
% & \multirow{2}{*}{4096} & \multirow{2}{*}{131072} & 34.44 (1.0x) & 22.66 (1.52x) & OOM & 104.57 (1.0x) & 71.59 (1.46x) & OOM \\ \cline{4-9}
%  & & & 46.11 (1.0x) & 33.03 (1.4x) & OOM & 117.65 (1.0x) & 83.35 (1.41x) & OOM \\ \cline{2-9}
% & \multirow{2}{*}{2048} & \multirow{2}{*}{131072} & 32.72 (1.0x) & 11.88 (2.75x) & OOM & 68.7 (1.0x) & 36.17 (1.9x) & OOM \\ \cline{4-9}
%  & & & 42.75 (1.0x) & 18.82 (2.27x) & OOM & 76.31 (1.0x) & 42.27 (1.81x) & OOM \\ \cline{2-9}
% & \multirow{2}{*}{2048} & \multirow{2}{*}{65536} & 16.21 (1.0x) & 6.02 (2.69x) & OOM & 34.05 (1.0x) & 18.26 (1.87x) & OOM \\ \cline{4-9}
%  & & & 22.52 (1.0x) & 11.06 (2.04x) & OOM & 40.6 (1.0x) & 24.17 (1.68x) & OOM \\ \cline{2-9}
% & \multirow{2}{*}{1024} & \multirow{2}{*}{65536} & 17.21 (1.0x) & 3.72 (4.62x) & 3.71 (4.64x) & 35.25 (1.0x) & 9.26 (3.81x) & 9.51 (3.71x) \\ \cline{4-9}
%  & & & 22.57 (1.0x) & 8.45 (2.67x) & 8.88 (2.54x) & 39.92 (1.0x) & 13.05 (3.06x) & 13.4 (2.98x) \\ \cline{2-9}
% \hline
% \end{tabular}
% \caption{262144 KV = 4096 images. With 16 nodes, this corresponds to 64K images. }
% \label{tab:your_label}
% \end{table*}
% \begin{table*}[ht]\centering
% \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
% \hline
% \multirow{2}{*}{Model} & \multirow{2}{*}{Q Length} & \multirow{2}{*}{KV Length} & \multicolumn{3}{|c|}{Forward Time (ms)} & \multicolumn{3}{|c|}{Backward Time (ms)} \\ \cline{4-9}
%  &  &  & Ring & SplitQ & DupQ & Ring & SplitQ & DupQ \\ \hline
% \multirow{6}{*}{Flamingo-9b} & \multirow{2}{*}{4096} & \multirow{2}{*}{262144} & 24.32 (1.0x) & 15.04 (1.62x) & 70.81 (1.0x) & 47.36 (1.5x) & nan & nan \\ \cline{4-9}
%  & & & 60.4 (1.0x) & 45.29 (1.33x) & 104.77 (1.0x) & 81.42 (1.29x) & nan & nan \\ \cline{2-9}
% & \multirow{2}{*}{4096} & \multirow{2}{*}{131072} & 12.22 (1.0x) & 7.55 (1.62x) & 35.63 (1.0x) & 23.89 (1.49x) & nan & nan \\ \cline{4-9}
%  & & & 32.77 (1.0x) & 30.71 (1.07x) & 68.24 (1.0x) & 56.18 (1.21x) & nan & nan \\ \cline{2-9}
% & \multirow{2}{*}{2048} & \multirow{2}{*}{131072} & 10.91 (1.0x) & 3.96 (2.75x) & 22.67 (1.0x) & 12.04 (1.88x) & nan & nan \\ \cline{4-9}
%  & & & 28.34 (1.0x) & 19.88 (1.43x) & 41.19 (1.0x) & 29.62 (1.39x) & nan & nan \\ \cline{2-9}
% & \multirow{2}{*}{2048} & \multirow{2}{*}{65536} & 5.68 (1.0x) & 2.0 (2.84x) & 11.79 (1.0x) & 6.07 (1.94x) & nan & nan \\ \cline{4-9}
%  & & & 18.25 (1.0x) & 12.95 (1.41x) & 30.66 (1.0x) & 23.41 (1.31x) & nan & nan \\ \cline{2-9}
% \hline
% \end{tabular}
% \caption{For each Q, KV length combination and method, two numbers are reported. The top row is the time spent on cross attention during a model pass, while the bottom row is that of the total model pass.)}
% \label{tab:your_label}
% \end{table*}
