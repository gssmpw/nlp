\section{Comparison of LV-XAttn and Ring Attention for General Use Case}
\label{appendix:analysis}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.3]{figures/speedup_roof.pdf}
    \caption{The theoretical speedup of LV-XAttn over Ring Attention for on a cluster with 4 nodes, each equipped with 4 A100 GPUs. The uncolored region indicates a speedup of less than 1, meaning LV-XAttn performs slower than Ring Attention. Top-left and bottom-left quadrant represents the typical use case of MLLM with large visual inputs: $S_{KV} \gg S_Q$.}
    \label{fig:speedup_appdx}
\end{figure}
We have shown that for applications with large $S_{KV}$ and small $S_Q$ such as long video understanding, LV-XAttn achieves significant speedup over Ring Attention. Here, we provide a more in-depth analysis that generalizes to a broader range of cases.

Figure~\ref{fig:speedup_appdx} plots the theoretical speedup of LV-XAttn over Ring Attention for general $S_Q$ and $S_{KV}$. When $S_{KV}$ is large enough (above the horizontal bright red line), the transmission of $O_i, Q_i, m_i$ and $l_i$ in LV-XAttn is hidden by computation, making LV-XAttn compute-bound. On the other hand, when $S_{Q}$ is not too large (to the left of the vertical dark red line), the transmission of $K_i$ and $V_i$ are too large to be hidden by computation, making Ring Attention compute-bound. Their intersection (the top-left quadrant) represents the typical MLLM use case with large visual inputs and small text prompt.

For smaller visual inputs, the reduced $S_{KV}$ causes LV-XAttn to also become communication-bound (the bottom-left quadrant). When both LV-XAttn and Ring Attention are communication-bound, their relative speed depends on communication volume: LV-XAttn sends $2\cdot \frac{S_Qd}{n} + 2\cdot\frac{S_Q}{n}$, while Ring Attention sends $2\cdot \frac{S_{KV}d}{n}$. Roughly, when $S_{KV} > S_{Q}$, LV-XAttn still remains faster than Ring Attention. For MLLMs, each image is encoded into a large number of visual tokens -- e.g., 729 for mPLUG-Owl3 and 64 for OpenFlamingo -- so this condition is typically satisfied, making LV-XAttn faster for MLLMs in general.

This also suggests that for self-attention, where $S_Q = S_{KV}$, Ring Attention is preferable. This is why in our experiments, we apply Ring Attention to LM blocks for all baselines. However, when the context length is very large (top-right quadrant), both LV-XAttn and Ring Attention become compute-bound, resulting in identical iteration times, making the choice between them effectively irrelevant.