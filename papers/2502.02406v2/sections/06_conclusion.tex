\section{Conclusion}
We introduced LV-XAttn, a distributed, exact cross-attention mechanism for MLLMs with minimal communication overhead. By storing large key-value blocks locally on each worker and transmitting only smaller query blocks, LV-XAttn significantly reduces communication volume, which can be fully hidden by computation. Additionally, the activation recomputation technique reduces memory usage, enabling the processing of longer visual inputs with minimal overhead. Our evaluation demonstrates that LV-XAttn speeds up MLLM iteration by up to 5.58× and enables the processing of visual inputs up to 1.6× longer.