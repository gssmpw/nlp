\section{Background}
\subsection{Long Context Self-Attention}
\textbf{Self-Attention} In self-attention, the input is a sequence $x \in \mathbb{R}^{S \times d_{\text{embed}}}$, where $S$ is the sequence length and $d_{\text{embed}}$ is the embedding dimension. The input sequence is transformed into the query, key, and value matrices $Q, K, V \in \mathbb{R}^{S \times d}$ by multiplying $x$ with the projection matrices $W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{embed}} \times d}$, where $d$ is the hidden dimension. The attention output $O \in \mathbb{R}^{S \times d}$ is computed as:
\begin{equation} \label{eqn:attn}
  O = \text{softmax}(\frac{QK^T}{\sqrt{d}})V
\end{equation}
The standard implementation of self-attention involves materializing the matrix product $QK^T \in \mathbb{R}^{S \times S}$, resulting in memory complexity that scales quadratically with the sequence length. While memory-efficient methods like FlashAttention~\cite{dao2022fa, dao2023fa2} reduce the memory footprint of attention operations to linear complexity and enable handling longer context lengths, tasks such as long-document summarization and repository-level code understanding still demand context lengths that surpass the memory capacity of a single worker.

\textbf{Ring Attention} To address this, Ring Attention~\cite{liu2024ring} proposes distributing the attention operation across multiple workers along the sequence dimension. Specifically, with $n$ workers, each worker $i$ is responsible for storing one block of query, key and value $Q_i, K_i, V_i \in \mathbb{R}^{\frac{S}{n} \times d}$ and computing the attention block $O_i \in \mathbb{R}^{\frac{S}{n} \times d}$. $O_i$ can thus be decomposed to
\begin{equation} \label{eqn:ring-attn}
  O_i = \text{softmax}(\frac{Q_i[K_1, ..., K_n]^T}{\sqrt{d}})[V_1, .., V_n]
\end{equation}
$O_i$ is computed iteratively by transmitting key-value blocks among workers in a ring-like fashion. To facilitate the block-wise computation of $O_i$, worker $i$ has to maintain necessary softmax statistics $m_i \in \mathbb{R}^{\frac{S}{n} \times d}$ and $l_i \in \mathbb{R}^{\frac{S}{n}}$. During round $r$, worker $i$ computes partial attention using the blocks $Q_i$, $K_{(i-r) \mod n}$, and $V_{(i-r) \mod n}$ and updates $O_i, m_i$ and $l_i$. The key-value block is then sent to worker $i+1$ while a new key-value block is received from worker $i-1$.
% In the next iteration, worker $i$ computes partial attention with blocks $Q_i$, $K_{j-1}$ and $V_{j-1}$. As long as the communication of the key-value blocks is fully overlapped with the attention computation, Ring Attention incurs no communication overhead.

\subsection{Cross-Attention in Multimodal Large Language Models}
\textbf{Cross Attention} As LLMs continue to evolve, researchers are investigating the incorporation of vision and other modalities into these models. One common way is to embed cross-attention layers into the language model. In cross-attention, the input consists of two sequences from different modalities: $x \in \mathbb{R}^{S_Q \times d_{\text{embed}}}$ and $y \in \mathbb{R}^{S_{KV} \times d_{\text{embed}}}$, where $S_Q$ and $S_{KV}$ denote the sequence lengths of $x$ and $y$, respectively. The query matrix $Q \in \mathbb{R}^{S_Q \times d}$ is derived from $x$, while the key matrix $K \in \mathbb{R}^{S_{KV} \times d}$ and the value matrix $V \in \mathbb{R}^{S_{KV} \times d}$ are obtained from $y$. The attention output $O \in \mathbb{R}^{S_Q \times d}$ is then calculated in the same manner as in Equation~\ref{eqn:attn}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mllm.pdf}
    \caption{Multimodal LLMs with cross-attention (TODO: add caption. Adjust font size)}
    \label{fig:mllm-architecture}
\end{figure}
MLLMs that uses cross-attention, such as Flamingo~\cite{alayrac2022flamingo}, Otter~\cite{li2023otter}, mPLUG-Owl3~\cite{ye2024mplugowl3}, and IDEFICS~\cite{laurencon2023idefics} follow the model architecture illustrated in Figure~\ref{fig:mllm-architecture}. These models consist of a visual encoder, a projection layer, and an LLM. Cross-attention layers are sparsely interleaved between the layers of the LLM. Language inputs are fed directly into the LLM and the resulting intermediate representation are passed to the cross-attention layers as $x$. Visual inputs are processed by the visual encoder and projection layer to produce visual tokens, which are then passed to the cross-attention layers as $y$, enabling the incorporation of visual information for next-token prediction in LLM.

\textbf{Challenges with Large Visual Inputs} 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/runtime_breakdown.pdf}
    \caption{Runtime breakdown for a single iteration of mPLUG-Owl-7b and OpenFlamingo-3b with Ring Attention and LV-XAttn on 16 A100 GPUs. mPLUG-Owl-7b has four cross-attention layers distributed throughout its 28 LM blocks, while OpenFlamingo-3b inserts one cross-attention layer after each of its 24 LM blocks. LV-XAttn reduces the time spent on cross-attention computation by 93\% for mPLUG-Owl-7b and by 53\% for OpenFlamingo-3b, compared to Ring Attention.}
    \label{fig:runtime_breakdown}
\end{figure}
For applications with large visual inputs, such as long video understanding, where $S_{KV}$ is substantial, the memory required to compute cross-attention often surpasses the capacity of a single worker. For instance, in mPLUG-Owl3, a 23-minute video sampled at 1 frame per second is encoded to a visual input with one million tokens. Combined with a language input of 2048 tokens, the cross-attention operation demands over 100 GB of memory. 

While Ring Attention can be used to distribute the cross-attention operation, as is done for self-attention, the presence of large key-value blocks makes Ring Attention communication-bound, resulting in highly inefficient cross-attention operations. As illustrated in Figure~\ref{fig:runtime_breakdown}, despite being sparsely embedded in the LLM, cross-attention operations can account for up to 87\% of the iteration time when using Ring Attention.

% \textbf{Extending to Long Context: Ring Attention (or maybe just talk about techniques for long context including head parallelism?}
% \begin{itemize}
%     \item to process longer visual inputs, need more memory
%     \item talk about ring attention. partition k, v
%     \item talk about imbalance of q and kv, using real world dataset like MMIU and Video-MMeE. this causes non-overlap of comp and comm
% \end{itemize}