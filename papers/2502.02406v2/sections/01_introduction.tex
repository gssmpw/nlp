\section{Introduction}
Large language models (LLMs) have shown exceptional performance in language processing tasks that involves long context, such as long document understanding~\cite{sun-2024-pearl, bertsch2023unlimiformer} and repository-level code completion~\cite{shrivastava23repollm, zhang2023repocoder}. Their strong reasoning capabilities have motivated efforts to expand beyond language inputs, giving rise to multimodal large language models (MLLMs). These models can process and reason about other modalities, such as visual inputs, enabling applications like video understanding~\cite{qian2024momentor, islam2024videorecap, he2024malmm} and image processing~\cite{qugo2024llava-uhd, li2023blip2}.

A common approach to integrating visual inputs into LLMs is through \textit{cross-attention}~\cite{alayrac2022flamingo, laurencon2023idefics, li2023otter, ye2024mplugowl3, grattafiori2024llama3v}, where queries derived from the text input interact with keys and values derived from the visual inputs. This enables effective fusion of multimodal information. In MLLMs, cross-attention layers are inserted between language model blocks, enabling the LLM to process intermediate representations that are integrated with visual information.

However, the memory requirement of cross-attention layers is a limiting factor for applications involving large visual inputs, such as long video understanding. For example, in mPLUG-Owl3~\cite{ye2024mplugowl3}, cross-attention applied to a text of sequence length 2048 and a 23-minute video sampled at 1 frame per second (fps) requires over 220$GB$ of memory. This exceeds the memory capacity of existing accelerators, necessitating the distributed computation of the attention operation across multiple workers. 

Existing distributed attention approaches can be categorized as two classes: head-parallelism and sequence-parallelism. Head-parallelism methods such as Deepspeed-Ulysses~\cite{jacobs2024ds} and Megatron-LM~\cite{korthikanti2023megatron-lm} partition the computation along the head dimension of multi-head attention. Consequently, maximum degree of parallelism is capped by the number of heads used in multi-head attention. This translates to an upper bound in terms of memory capacity, preventing them from processing longer visual inputs which have memory demands beyond this (Table~\ref{tab:ds-flamingo} in Section~\ref{sec:eval}). In addition, the number of workers must be divisible by the total number of heads to ensure a balanced load across workers. Otherwise, resource underutilization may occur due to stragglers. On the other hand, sequence parallel methods such as Ring Attention~\cite{liu2024ring} partition the computation along the input sequence dimension, overcoming the limitation of head-parallelism methods. However, when applied to cross-attention with large visual inputs, these approaches suffer from large communication overheads even after overlapping computation and communication. Figure~\ref{fig:runtime_breakdown} shows that cross-attention operations distributed with Ring Attention~\cite{liu2024ring} can account for up to 87\% of the iteration time, despite comprising only 2\% of the total parameters.

In this work, we present LV-XAttn, a distributed, exact cross-attention mechanism that employs sequence-parallelism \new{\emph{with minimal communication overhead}}. Our main observation is that while keys and values derived from visual inputs are large, the queries derived from text input are typically small in MLLMs. For example, in the video understanding benchmark Video-MME~\cite{fu2024video-mme}, an input processed with mPLUG-Owl3 models results in an average sequence length of 1,739,394 for keys and values and 5,514 for queries, when frames are sampled at 1 fps. Based on this, LV-XAttn organizes each worker to locally store a partition of the large key and value blocks, while small query blocks are transmitted between workers to compute the attention output in a blockwise fashion. This significantly reduces the communication volume compared to Ring Attention. For instance, with the Video-MME benchmark, LV-XAttn reduces communication volume to just 0.48\% of that required by Ring Attention. Furthermore, the reduced communication can be effectively overlapped by computation, allowing distributed cross-attention to be performed without incurring any communication overhead.

To further enable the processing of longer visual inputs, we employ an activation recomputation technique that is specific to MLLMs. In standard attention implementations, activations including queries, keys, and values need to be saved for backward pass~\cite{korthikanti2023reducing}. Storing the large key and value tensors for every cross-attention layer introduces additional memory pressure. We observe that since cross-attentions in MLLMs share input visual tokens, we can maintain a single copy of the visual tokens accessible to all cross-attention layers and recompute activations during the backward pass. This allows us to process up to 1.6$\times$ longer visual inputs with just less than 8\% overhead.
% This provides significant memory savings and involves minimal overhead as it only involves projection. 

We perform comprehensive evaluation of LV-XAttn on mPLUG-Owl3 models and OpenFlamingo~\cite{awadalla2023openflamingo} models across multiple cluster configurations, including setups with A100 and A30 GPUs. LV-XAttn speeds up the cross-attention operation by up to 45.85$\times$ and overall model iteration time by up to 5.58$\times$ compared to Ring Attention. By minimizing communication volume and further overlapping communication with computation, we demonstrate that LV-XAttn incurs less than 0.42\% overhead compared to a no-communication baseline.

% We show that the reduced memory footprint from not saving the large activations allow us to process inputs up to 1.6$\times$ longer with overhead of less than 8$\%$.

% As a result, in LV-XAttn, we do not store these activations during forward pass.

% employ an activation recomputation technique based on the unique characteristics of cross-attentions in MLLMs. The observation is that

% However, existing distributed attention~\cite{liu2024ring, jacobs2024ds} suffer from large communication overhead, resulting in high inefficient cross-attention operation. For example, Figure~\ref{fig:runtime_breakdown} shows that cross-attention operations distributed with Ring Attention~\cite{liu2024ring} can account for up to 87\% of the iteration time, despite comprising only 2\% of the total parameters.

% For applications with large visual inputs such as long video understanding, the large amount of visual tokens causes the memory requirements of cross-attention operations to exceed the capacity of a single worker. This calls for distributing the attention operation across multiple workers. T

% Visual information is fused to the intermediate features through attention operation.  

% the attention operation has queries derived from the language inputs, and keys and values derived from the visual inputs~\cite{alayrac2022flamingo, laurencon2023idefics, li2023otter, grattafiori2024llama3v}. Cross-attention layers are interleaved between language model (LM) blocks to facilitate interaction between modalities, enabling the LLM to process intermediate representations that are fused with visual information. Despite effective, e

% % visual inputs are encoded by a visual encoder into visual tokens, which are used to derive keys and values; text inputs are processed through the language model blocks to produce intermediate features, which are used to derive queries. The queries, keys, and values are inputs to the attention operation~\cite{vaswani2017attention}, allowing interaction between modalities.

% cross attention -> long context large memory requirement requires distributed, along sequence
% existing long context approach for self attention. suffer from comm overhead
% lv-xattn + activation reduction
% good results