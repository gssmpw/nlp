\section{LV-XAttn: Distributed Cross-Attention with Minimal Communication Overhead}
In this section, we introduce LV-XAttn, our method for efficiently distributing the cross-attention operation with minimal communication overhead. We also present an activation recomputation technique specific to MLLMs to reduce memory pressure, enabling the processing of longer visual contexts.
\subsection{Method}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth,height=1.6in]{figures/LV-XAttn.pdf}
    \caption{LV-XAttn with 4 workers. We partition the KV blocks and each worker  stores their respective large key-value blocks $K_i, V_i$. We also partition the query ($Q_i$), output ($O_i$), and softmax statistics ($m_i$ and $l_i$ omitted in the figure). The query and output are rotated among workers to compute the attention.}
    \label{fig:lv-xattn}
\end{figure*}
\begin{algorithm}[tb]
   \caption{LV-XAttn Forward Pass for Worker $i$}
   \label{alg:lv-xattn}
    \begin{algorithmic}
       \STATE {\bfseries Input:} data $Q_i$, $K_i$, $V_i$
       \STATE Initialize $O_i, m_i, l_i \gets 0$
       \FOR{$round=0$ {\bfseries to} $n-1$}
            \STATE $j_{\text{prev}} \gets (i-round+1) \bmod n$
            \STATE $j \gets (i-round) \bmod n$
            \STATE $j_{\text{next}} \gets (i-round-1) \bmod n$
            \STATE \textbf{do in parallel:}
            \STATE \hspace{1em} Send $O_{j_{\text{prev}}}, m_{j_{\text{prev}}}, l_{j_{\text{prev}}}, Q_{j}$ to worker $(i+1) \bmod n$
            \STATE \hspace{1em} Recv $O_{j}, m_{j}, l_{j}, Q_{j_{\text{next}}}$ from worker $(i-1) \bmod n$
            \STATE \hspace{1em} $\Delta m, \Delta l, \Delta O \gets \text{FlashAttention}(Q_{j}, K_i, V_i)$
            \STATE $O_{j}, m_{j}, l_{j} \gets \text{Rescale}(O_{j}, m_{j}, l_{j}, \Delta O, \Delta m, \Delta l)$
       \ENDFOR
    \end{algorithmic}
\end{algorithm}
The primary observation that motivates our work is that in applications involving large visual inputs, the size of the \emph{query block is typically much smaller} than that of the key-value blocks. For instance, in the widely-used video understanding benchmark, Video-MME~\cite{fu2024video-mme}, the average text prompt for long videos consists of $S_Q = 3128$ words, including the question, options, answer, and subtitles. On the other hand, videos have an average duration of 2,386 seconds; each frame is encoded by the visual encoder and projection layer in an MLLM into multiple visual tokens. For example, mPLUGOwl-3 generates 729 visual tokens per frame. With a sampling rate of 1 fps, each video results in a sequence length of $S_{KV} = 1739394$. As a result, distributed attention mechanisms that involves movement of key-value blocks incur substantial communication overhead.

To address this, we propose LV-XAttn, which keeps the large key-value blocks locally on each worker, while smaller query blocks, attention blocks, and necessary softmax statistics are exchanged among workers in a ring-style fashion. This is illustrated in Figure~\ref{fig:lv-xattn}. During each round, each worker $i$ computes attention using its local key-value blocks $K_i$ and $V_i$ and query blocks $Q_j$ received from peers. This computation generates partial attention blocks $\Delta O$ and partial softmax statistics $\Delta m$ and $\Delta l$. The worker then updates the received attention block $O_j$ and softmax statistics $m_j$ and $l_j$ by rescaling them using $\Delta O$, $\Delta m$, and $\Delta l$. The worker then sends $Q_j$, $O_j$, $m_j$, and $l_j$ to the next worker in the ring topology and receives $Q_{j-1}$, $O_{j-1}$, $m_{j-1}$, and $l_{j-1}$ from the previous worker. After $n$ rounds, the computed attention block $O_i$ and softmax statistics $m_i$ and $l_i$ are returned to worker $i$.

\textbf{Overlapping Computation and Communication} To further reduce communication overhead, we can overlap the attention computation with data transmission between workers. While performing attention computation with $Q_j$, $K_i$ and $V_i$, worker $i$ also does the following in parallel
\begin{itemize}
    \item Receive $O_j$, $m_j$ and $l_j$ from worker $i-1$, which are needed for rescaling in this round.
    \item Receive $Q_{j-1}$ from worker $i-1$, which is needed for attention computation in the next round.
    \item Send $O_{j+1}$, $m_{j+1}$, $l_{j+1}$ computed in the previous round to worker $i+1$.
    \item Send the already present $Q_j$ to worker $i+1$.
\end{itemize}
After receiving $O_j$, $m_j$ and $l_j$, and computing $\Delta O, \Delta m$ and $\Delta l$, we can perform rescaling to update $O_j$, $m_j$ and $l_j$. We describe our distributed attention procedure in Algorithm~\ref{alg:lv-xattn}. As demonstrated in Section~\ref{sec:eval/ablation}, the substantial reduction in communication volume enables complete overlap with computation, effectively eliminating any communication overhead.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/speedup.pdf}
    \caption{The theoretical speedup of LV-XAttn over Ring Attention for cross-attention on a 4-node cluster. Each node is equipped with 4 A100 GPUs, and nodes are interconnected by a 25 GB/s network. The markers represent processing a 2,386-second video and a 3,128-word text prompt—average values for long videos in Video-MME~\cite{fu2024video-mme}—using mPLUG-Owl3 and OpenFlamingo models at different frame rates. Note that for each frame, a special token \texttt{<image>} have to be added to the text-prompt, resulting in $S_Q = 2386+3128=5514$.}
    \label{fig:speedup}
\end{figure}
\begin{table}[tbp]
    \centering
    \caption{Runtime analysis for Ring Attention and LV-XAttn. $h$ represents the number of head in multi-head attention. The attention FLOPs are calculated similar to previous work~\cite{dao2023fa2}. }
    \begin{tabular}{l|c|c}
        \hline
         & Forward & Backward \\ \hline 
        Ring Attention & $\frac{2\cdot\frac{S_{KV}}{n}\cdot h\cdot d}{\text{Net Bandwidth}}$ & $\frac{4\cdot\frac{S_{KV}}{n}\cdot h\cdot d}{\text{Net Bandwidth}}$ \\[7pt]
        LV-XAttn & $\frac{4\cdot\frac{S_Q}{n}\cdot\frac{S_{KV}}{n}\cdot h\cdot d}{\text{GPU FLOPS}}$ & $\frac{10\cdot\frac{S_Q}{n}\cdot\frac{S_{KV}}{n}\cdot h\cdot d}{\text{GPU FLOPS}}$ \\[7pt]
        Speedup & $\frac{1}{2\cdot\frac{S_Q}{n}}\frac{\text{GPU FLOPS}}{\text{Net Bandwidth}}$ & $\frac{2}{5\cdot\frac{S_Q}{n}}\cdot\frac{\text{GPU FLOPS}}{\text{Net Bandwidth}}$ \\ [7pt]\hline
    \end{tabular}
    \label{tab:runtime-formula}
    \vspace{-2ex}
\end{table}
\textbf{Runtime Analysis} Let $f(\text{query size}, \text{key-value 
size})$ represent the time required to perform the forward-pass attention computation, and let $comm(\text{tensor size})$ denote the time to transmit a tensor. In \text{LV-XAttn}, $Q_i, O_i\in \mathbb{R}^{\frac{S_Q}{n} \times d}$ and $m_i, l_i \in \mathbb{R}^{\frac{S_Q}{n}}$ are transmitted during each round. This results in a per-round runtime of:
\begin{equation} \label{eqn:lv-xattn}
    \max\left(f(\frac{S_Qd}{n}, \frac{S_{KV}d}{n}), comm(2\cdot\frac{S_Qd}{n} + 2\cdot\frac{S_Q}{n})\right)
\end{equation}
In contrast, Ring Attention transmits key-value blocks $K_i, V_i \in \mathbb{R}^{\frac{S_{KV}}{n} \times d}$ in each round, leading to a per-round runtime of:
\begin{equation} \label{eqn:ring}
    \max\left(f(\frac{S_Qd}{n}, \frac{S_{KV}d}{n}), comm(2\cdot\frac{S_{KV}d}{n})\right)
\end{equation}
Figure~\ref{fig:speedup} shows the theoretical speedup of LV-XAttn over Ring Attention across different $S_Q$ and $S_{KV}$. For MLLM with large visual inputs, where $S_{KV} \gg S_Q$, and slow cross-node interconnect -- an unavoidable constraint for inputs exceeding single-node memory capacity -- \text{LV-XAttn} is compute-bound, while Ring Attention is communication-bound. Consequently, the runtime for \text{LV-XAttn} in Equation~\ref{eqn:lv-xattn} reduces to $f\left(\frac{S_Qd}{n}, \frac{S_{KV}d}{n}\right)$, while the runtime for Ring Attention in Equation~\ref{eqn:ring} becomes $comm\left(2\cdot\frac{S_{KV}d}{n}\right)$. A similar analysis applies to the backward pass. Table~\ref{tab:runtime-formula} summarizes the runtime and corresponding speedup for multi-head cross-attention. More in-depth discussion is in Appendix~\ref{appendix:analysis}.
% Intuitively, with a smaller per-device query block size, attention computation becomes less time-consuming, while the communication of key-value blocks remains constant. This leads to a more significant speedup of of \text{LV-XAttn} over Ring Attention.

% (TODO: more on this)

% TODO: talk about backward pass? Ring Attention becomes $comm\left(4\frac{S_{KV}d}{n}\right)$, while LV-XAttn is $b\left(\frac{S_Qd}{n}, \frac{S_{KV}d}{n}\right)$
% TODO: Do we want a trace graph here?

% Let $f(x, y)$ denote the time it takes to run forward pass attention computation with query block size $x$ and key-value block sizes $y$, and $comm(x)$ denote the time it takes to send tensor of size $x$. By transmitting $Q_j, m_j, O_j \in \mathbb{R}^{\frac{S_Q}{n} \times d}$ and $L_j \in \mathbb{R}^{\frac{S_Q}{n}}$, each round in \text{LV-XAttn} takes $\max\left(f(\frac{S_Qd}{n}, \frac{S_{KV}d}{n}), comm(3\frac{S_Qd}{n} + \frac{S_Q}{n})\right)$. On the other hand, Ring Attention transmits key value blocks $K_j, V_j \in \mathbb{R}^{\frac{S_{KV}}{n} \times d}$, resulting in each round taking $\max\left(f(\frac{S_Qd}{n}, \frac{S_{KV}d}{n}), comm(2\frac{S_{KV}d}{n})\right)$. With large visual inputs where $S_{Q} \ll S_{KV}$ and slow inter-connect between GPUs, which is typical in cross-node setup, LV-XAttn's communication can be easily overlapped by computation while Ring Attention's cannot, and thus the term reduces to $f(\frac{S_Qd}{n}, \frac{S_{KV}d}{n})$ and $comm(2\frac{S_{KV}d}{n})$, respectively.
\subsection{Activation Recomputation for MLLM} In standard attention implementation, during forward pass computation, input tensors $Q_i, K_i, V_i$ and output tensors $O_i$ and $L_i$ are saved for backward pass, where $L_i = m_i + \log{l_i}$~\cite{dao2023fa2}. However, storing large key-value blocks $K_i$ and $V_i$ increases memory usage, thereby limiting the maximum number of visual inputs that can be processed. For instance, in the case of mPLUG-Owl3-7b with a 4096-frame video, storing $K_i$ and $V_i$ takes $79.73$GB per cross-attention layer.

To address this, we observe that while language features $x$ differ across cross-attention layers as they pass through various LM blocks, the visual features $y$ remain unchanged throughout all cross-attention layers, as they are only fed into the cross-attention layers. Thus, instead of storing key-value blocks for each cross-attention layer, we propose to keep a single copy of visual features $y$ that can be accessed by all cross-attention layers. During the backward pass, $y$ is projected to recompute key-value blocks $K_i$ and $V_i$. With $Q_i$ also being recomputed, we only need to save $x$, $O_i$, and $L_i$ during each cross-attention forward pass.

As demonstrated in the ablation study in Section~\ref{sec:eval/ablation}, this approach incurs an overhead of less than $8\%$ while enabling the system to handle $1.6\times$ more visual inputs.


% \subsection{Support for arbitrary mask?}
% \subsection{Generalize to longer Q?}

