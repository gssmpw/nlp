\section{Related Work}
\textbf{Multimodal Large Langauge Models} Large language models (LLMs) have shown remarkable abilities in language understanding, generation, and reasoning. Building on these capabilities, researchers have extended LLMs to process multimodal inputs, including text, images, and videos. This has given rise to two primary designs for multimodal large language models:

The first design employs an auto-regressive approach, where tokenized visual inputs are concatenated with text tokens and processed by the LLM. Models such as LLaVA~\cite{liu2023llava}, InternVL~\cite{chen2024internvl}, NVILA~\cite{liu2024nvila}, and MiniGPT-4~\cite{zhu2024minigpt} follow this paradigm. Although this approach naturally extends text-only LLMs, the large number of tokens significantly slows down both training and inference~\cite{ye2024mplugowl3, grattafiori2024llama3v}.

The second design relies on cross-attention mechanisms, where cross-attention layers are inserted between LLM layers to incorporate visual features into its intermediate representations. Models such as Flamingo~\cite{alayrac2022flamingo}, IDEFICS~\cite{laurencon2023idefics}, Otter~\cite{li2023otter}, mPLUG-Owl3~\cite{ye2024mplugowl3} and Llama 3-V~\cite{grattafiori2024llama3v} adopt this strategy. While this method avoids processing a large number of visual tokens through the LLM backbone, cross-attention layers remain computationally expensive with current sequence-parallel approaches~\cite{grattafiori2024llama3v}. In this work, we introduce LV-XAttn to address this bottleneck.

\textbf{Memory-efficient Attention} The attention operation has a memory complexity that scales quadratically with sequence length, limiting its scalability for longer contexts. Approximate methods~\cite{kitaev2020reformer, zaheer2020bigbird, beltagy2020longformer, choromanski2021performer, ding2023longnet} and compression techniques~\cite{chevalier2023autocompressors, munkhdalai2024infini-attention} reduce memory requirements by sacrificing some model quality. For exact attention, FlashAttention~\cite{dao2022fa, dao2023fa2} proposes block-wise computation, which reduces memory complexity to linear while providing runtime speedups by minimizing I/O between GPU HBM and SRAM. However, for applications like long video understanding, which require context lengths exceeding a single GPU's memory capacity, a distributed setup is necessary.

\textbf{Sequence Parallelism} Megatron-LM~\cite{korthikanti2023megatron-lm} and Deepspeed-Ulysses~\cite{jacobs2024ds} parallelize the attention computation along the head dimension and the non-attention computation along the sequence dimension. However, this approach requires expensive collective communication, leading to inefficiencies in cross-node settings. Additionally, the degree of parallelism is constrained by the number of heads. On the other hand, Ring Attention~\cite{liu2024ring} extends FlashAttention to the distributed setting, parallelizing the attention computation along the sequence dimension. Variants of Ring Attention, such as DistFlashAttn~\cite{li2024distflashattn} and Striped Attention~\cite{brandon2023striped}, address workload imbalance in attention operations with causal masks. Despite this, all these methods face significant communication overhead due to the transmission of large key-value blocks in cross-attention operations with large visual inputs. LongVila~\cite{chen2024longvila} employs intra-node head parallelism and inter-node sequence parallelism but still encounters the same issue as Ring Attention. Our method effectively addresses the communication bottleneck encountered by these approaches in cross-attention for MLLMs.

\textbf{Other Parallelism} Data parallelism~\cite{dean2012data-parallel} partitions samples within a batch across workers but cannot handle scenarios where the memory capacity is insufficient for even a single sample, which is central to our use case. Pipeline parallelism~\cite{narayanan2019pipedream} divides models by layers into multiple stages, while tensor parallelism~\cite{shoeybi2019megatron} distributes parameter and activation tensors within layers across workers. FSDP~\cite{rajbhandari2020zero} shards parameters and gradients across workers, gathering them only when needed. Our work focuses on sequence parallelism, which is complementary to these approaches for handling larger visual inputs. For instance, our experiments in Section~\ref{sec:eval} utilize FSDP.