
Diffusion models are probabilistic generative models that generate new data similar to those they are trained on~\citep{song2019generative,ho2020denoising,song2020score}.
These models have recently gained significant attention due to their impressive performance    at image generation, video synthesis, text-to-image translation, and molecular design~\citep{dhariwal2021diffusion,ho2022video,ramesh2022hierarchical,xu2022geodiff}.


A diffusion generative model is based on two stochastic processes: 
\begin{enumerate}
    \item A forward process 
$ \Fx_0\to \Fx_1\to \dots \to \Fx_{\T}$ that starts from a sample $\Fx_0 \in \R^{\Dim}$  from a target data distribution~$\q_0$ ($\Fx_0\sim \q_0$) and then diffuses this sample in $\T$ steps into  pure noise $\Fx_{\T}\in  \R^{\Dim}$ ($\Fx_{\T}\sim\StandNormal$).
\item 
A reverse process $\BY_{\T}\to \BY_{\T-1}\to \dots \to \BY_0$ that starts from  pure noise  $\BY_{\T} \in\R^{\Dim}$ ($\BY_{\T}\sim\StandNormal$) and then converts this noise in $\T$ steps into a new sample $\BY_0\in \R^{\Dim}$ ($\BY_0\sim p_0$) that is similar in  distribution to the target sample $\x_0\in \R^{\Dim}$.
%, that is, the reverse process aims at  $\BY_0\stackrel{d}{\approx}\x_0$.
\end{enumerate}
\noindent 
Making the data noisy is easy.
Therefore, fitting a good reverse process is the key to successful diffusion modeling.

The three predominant formulations of diffusion models are denoising diffusion probabilistic models
(DDPM)~\citep{ho2020denoising}, score-based generative models (SGM)~\citep{song2019generative}, and score-based stochastic differential equations (SDE)~\citep{song2020score,song2021maximum}.
DDPM include two Markov chains: 
a forward process that transforms data into noise, and a reverse process that recovers the data from the noise. 
The objective is to train a function  (usually a deep neural network) for denoising the data over time.
Sample generation then  takes random Gaussian noise through the trained denoising function. 
SGM, which is the setting adopted in  this paper,  also  perturb data with a sequence of  Gaussian noise but then try to estimate the score functions, the gradient of the log probability density, for the noisy data. 
Sampling combines the trained scores with score-based sampling approaches like Langevin dynamics. 
While DDPM and SGM focus on discrete time steps,~\sde~consider infinitely many time steps or unbounded noise levels. In~\sde, the desired score functions are solutions of stochastic differential equations. 
Once the desired score functions are trained,  sampling can be reached using stochastic or ordinary differential equations.


Much research efforts are geared toward non-asymptotic rates of convergence, particularly in the number of steps $\T$ needed to achieve a desired level of reconstruction accuracy.
Typical measures of accuracy are Kullbackâ€“Leibler
divergence,  total variation, and Wasserstein distance between the true distribution $\Fd_0$ and the approximated counterpart $\Bd_0$. 
For example, one tries to ensure $\TV(\Fd_0,\Bd_0)\le \error$ for a fixed error level~$\error\in (0,\infty)$, where $\TV(\Fd_0,\Bd_0)\deq \sup_{A\subset \R^d}  |\Fd_0(A)-\Bd_0(A)|$ is called the total variation~\citep{Sara2000}.
The many very recent papers on this topic highlight the large interest in this topic~\citep{block2020generative,de2021diffusion,de2022convergence,lee2022convergence,chen2023probability,chen2023improved,li2023towards,chen2023restoration,liang2024nonN}.
Results like~\citet[Theorem~13]{block2020generative} provide rates of convergence for diffusion models in terms of Wasserstein distance employing Langevin dynamics, 
 but they suffer from the curse of dimensionality in that the rates depend exponentially on the dimensions of the data $\Dim$, that is, the number of input features. 
 Improved convergence rates in terms of $\Dim$ are proposed by \citet{chen2023probability,li2023towards}, showing polynomial growth in~$\Dim$. 
Recently~\citet{liang2024nonN} proposed a new Hessian-based accelerated sampler for the stochastic diffusion processes.
They  achieve  accelerated rate  for the total variation convergence for \ddpm\ of the order $\Dim^{1.5}/\error$ for any target distributions having finite variance and  assuming a uniform bound over the accuracy of the estimated score function (see our Section~\ref{sec:ReWork} for an overview of recent works).
 While these results are a major step forward, they involve a strong dependence the dimensionality of the data, which is problematic as images, text, and so forth a typically high dimensional. The key question is whether improving these rates with respect to $\Dim$ is possible at all.

 \paragraph{Contribution}  
 This paper aims to enhance the efficiency of diffusion models through the incorporation of regularization techniques commonly used in high-dimensional statistics~\cite{Lederer2021HD}.

The contributions of this work are as follows:  
\begin{itemize}  
    \item We theoretically demonstrate that $\ell_1$-regularization can enhance the convergence rates of diffusion models to  the order of $s^2/\error$, where $s \ll \Dim$, compared to the standard order of $\Dim^2/\error$ (Theorem~\ref{the:maindS}).
    \item We validate our theoretical findings through  simulations on image datasets (Section~\ref{sec:sim}).   
    \item We additionally demonstrate that $\ell_1$-regularization can make sampling more balanced and avoid oversmoothing (Section~\ref{sec:fmnist}).
\end{itemize}
\noindent 
Thus, our research is a step forward in the whole field's journey of improving our understanding of  diffusion models and of making diffusion modeling more efficient.

\paragraph{Paper outline}
Section~\ref{sec:disc-setting} introduces score matching and the discrete-time diffusion process.
Section~\ref{Sec:main} presents our proposed estimator along with the main results (Theorem~\ref{the:maindS}).
Section~\ref{sec:ThecRe} includes some technical results and Section~\ref{sec:ReWork} provides an overview of related work.
We support our theoretical findings with numerical observations over image datasets in Section~\ref{sec:sim}.
Finally, we conclude the paper in Section~\ref{sec:conc}.
Additional simulations, technical results, and detailed proofs are provided in the Appendix.


\section{Preliminaries of score matching and discrete-time diffusion process}\label{sec:disc-setting}
In this section, we provide a brief introduction to score matching and  discrete-time diffusion process.
\paragraph{Notations}\label{Notations}
    For a vector $\boldsymbol{z}\in \R^{\Dim}$, we use the notation $\norm{\boldsymbol{z}}_1\deq \sum_{i=1}^{\Dim} |z_i|$, $\norm{\boldsymbol{z}}^2\deq \sum_{i=1}^{\Dim} (z_i)^2$, $\norm{\boldsymbol{z}}_{\infty}\deq \sup_{i\in \{1,\dots,\Dim\}} |z_i|$, and $\norm{\boldsymbol{z}}_0\deq\sum_{i=1}^{\Dim} \mathbf{1}(z_i \neq 0)$. 
   
\subsection{Score matching}
Assume a dataset $\dataset\deq\{\x^1,\dots, \x^{\dn}\}$ of $\dn$ training data  samples $\x^i\in \R^{\Dim}$ with an unknown target distribution $\q_0$ ($\x^i\sim \q_0$ for $i\in\{1,\dots,\dn\}$). 
The goal of probabilistic generative modeling is to use the dataset \dataset\ to learn a model that can sample from~$\q_0$.
The score of a probability density $\q(\x)$, 
the gradient of the log-density with respect to~\x~
 denoted as $\nabla_{\x} \log \q(\x)$,
 are the key components for generating new samples from $\Fdd$.
The score network $\scoref : \R^{\Dim} \to \R^{\Dim}$ is then a neural network parameterized by $\NetP\in \paramspace$, which will be trained to approximate the unknown score $\nabla_{\x} \log \q_0(\x)$. 
The corresponding objective functions for learning scores in \sgm~\citep{song2019generative} is then based on~\citet{hyvarinen2005estimation,hyvarinen2007some}
\begin{equation}\label{scorematch}
 \NetPS\in \arg \min_{\NetP\in \paramspace} \E_{\q_0(\x)}\bigl[\norm{\scoref(\x)-\nabla_{\x}\log \q_0(\x)}^2\bigr]\,,   
\end{equation}
which yields the parameters of a neural network  
% $\NetPS\in \paramspace$ (network is called
$\scorefS(\x)$ that approximates the unknown score function $\nabla_{\x}\log \q_0(\x)$.
Of course, 
the objective function in~\eqref{scorematch} entails (i)~an expectation over $\q_0$ and (ii)~the true score~$\nabla_{\x}\log \q_0(\x)$,
which are both not accessible in practice.
The expectation can readily be approximated by an average over the data samples $\dataset$;
replacing the score needs more care~\citep{vincent2011connection,song2020sliced}.
We come back to this point later  in Section~\ref{sec:distime} by representing  a time dependent form of denoising score matching~\citep{vincent2011connection}.
Once the score function is trained, there are various approaches to generate new samples from the target distribution $\q_0$ employing the approximated score.
These include deterministic and stochastic samplers (see~\citet{li2023towards} for an overview), Langevin dynamics among the most popular one~\cite{song2019generative}.




\subsection{Discrete-time diffusion process}\label{sec:distime}
Let's  $\xz\in \R^{\Dim}$ be an initial data sample  and $\xt\in \R^{\Dim}$  for a  discrete time step $t\in\{1,\dots,T\}$  be the latent variable  in the diffusion process. 
Let $\Fd_0$ be the initial data distribution, that is, the distribution belonging to the data's density $\Fdd_0$, and let $\Fd_t$ be the marginal latent distribution in time $t$ in the forward process.
We also use the notation $\Fd_{t,t+1} $ as the  joint distribution over the time $t$ to $t+1$ and $\Fd\deq \Fd_{0,\dots,T} $ as the overall joint distribution over the time $T$. 
In the forward process, white Gaussian noise is gradually added to the data with $\x_t=\sqrt{1-\beta_t} \x_{t-1}+\sqrt{\beta_t}\boldsymbol{w}_t$, where $\boldsymbol{w}_t\sim \mathcal{N}(\zerov,\Identity)$ and  $\beta_t\in (0,1)$ captures the ``amount of noise'' that is injected at time step $t$ and are called the noise schedule. 
This can be written as the conditional distribution 
\begin{equation*}\label{eq:forwardP}
    \Fd_{t|t-1}(\x_t|\x_{t-1})=\mathcal{N}(\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t\Identity)\,.
\end{equation*}
An immediate result is that
\begin{equation*}
    \Fd_{t}(\x_t|\x)=\mathcal{N}\bigr(\x_t;\sqrt{\bar\alpha_t}\x,(1-\bar\alpha_t)\Identity\bigl)\,,
\end{equation*}
for $\alpha_t=1-\beta_t$ and $\bar\alpha_t=\prod_{i=1}^{t} \alpha_i$. 
For large enough $T$   we have $\Fd_T\approx \mathcal{N}(\zerov,\Identity)$.
We also denote $\Fdd_t(\x_t|\x)$ as the corresponding density of $\Fd_t(\x_t|\x)$ and that 
$\Fdd_t(\x_t)=\int \Fdd_t(\x_t|\x)\Fdd_{0}(\x)d\x$, in which, $\Fdd_0$ is the unknown target density for $\x$.
We also assume that $\Fd_0$ is absolutely continuous w.r.t. the Lebesgue measure and so the absolute continuity is preserved for all 
$t\in \{1,\dots,T\}$ due to the Gaussian nature of the noise. 
The goal of the reverse process in diffusion models is then  to generate samples (approximately) from the distribution~$\Fd_0$
starting from the Gaussian distribution $\x_T\sim \mathcal{N}(\zerov,\Identity)\eqd\Bd_T$. 
% To achieve effective sampling, each forward step is approximated by a reverse sampling step.  
Let's first define 
\begin{equation*}
    \ut(\x_t)\deq\frac{1}{\sqrt{\alpha_t}}\bigl(\x_t+(1-\alpha_t)\nabla_{\x_t}\log \Fdd_t(\x_t)\bigr)\,.
\end{equation*}
At each time step, we then consider the reverse process (for sampling), specifically Langevin dynamics, which can generate samples from a probability density using the true score function, as follows:
\begin{multline*}
    \x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\bigl(\x_t+(1-\alpha_t)\nabla_{\x_t}\!\log \Fdd_t(\x_t)\bigr)+\sqrt{\frac{1-\alpha_t}{\alpha_t}}\boldsymbol{z}_t\\
    =\ut(\x_t)+\sigma_t
\boldsymbol{z}_t
\end{multline*}
for $\boldsymbol{z}_t\sim \mathcal{N}(\zerov,\Identity)$ and $\sigma_t\deq \sqrt{1-\alpha_t/\alpha_t}$.
Let's $\Bd_t$ be the marginal distribution of $\x_t$ in the true reverse process, which is the reverse process by employing the true scores $\nabla_{\x_t}\log \Fdd_t(\x_t)$,  and $\Bdd_t$ be the corresponding density. 
Then, above statement can be written as 
$\Bd_{t-1|t}=\mathcal{N}(\x_{t-1};\ut(\x_t),\sigma_t^2\Identity)$.
But in practice, one does not have access to the true scores $\nabla_{\x_t} \log \Fdd_t(\x_t)$, 
instead, an estimate of it namely $\scoref(\cdot,t)$, which corresponds to a neural network parameterized with a tuple  $\Theta\in \mathcal{B}$ (tuple of weight matrices),  implying  
\begin{equation*}\label{eq:uhat}
 \uth(\x_t)\deq\frac{1}{\sqrt{\alpha_t}}\bigl(\x_t+(1-\alpha_t)\scoref(\x_t,t)\bigr)\,.
\end{equation*}
Let's $\BdA_t$ be the marginal distribution of $\x_t$ in the estimated reverse process implying $\BdA_{t-1|t}=\mathcal{N}(\x_{t-1};\uth(\x_t),\sigma_t^2\Identity)$.
For $\Fd_0$   absolutely continuous, we are then interested measuring the mismatch between $\Fd_0$ and $\BdA_0$ through the
Kullbackâ€“Leibler divergence
\begin{equation*}
 \KL(\Fd_0 ||\Bd_0)\deq  \E_{X\sim \Fd_0} \Bigl[\log\frac{\Fdd_0(X)}{\Bdd_0(X)}\Bigr] \ge 0\,. 
\end{equation*}


	


\section{Regularizing denoising score matching}\label{Sec:main}
A promising avenue for accelerating sampling in diffusion models is high-dimensional statistics~\citep{Lederer2021HD}.  
High-dimensional statistics is a branch of statistics that deals with many variables.
A key idea in high-dimensional statistics is the concept of sparsity;
broadly speaking,
it means that among those many variables, only few are relevant to a problem at hand.
There are different sparsity-related approaches in deep learning,
such as dropout~\citep{Hinton2012,Molchanov2017,labach2019survey,gomez2019learning} or explicit regularization~\citep{Alvarez16,Feng17}. 
The latter approach adds prior functions (``penalties'', ``regularization'') to the objective functions of the estimators.
These penalties push the estimators toward specific parts of the parameter space that correspond to certain assumptions, for example, sparsity in $\ell_0$-norm and/or $\ell_1$-norm~\citep{Lederer2021HD}. 
The benefits of sparsity are well-documented in regression, deep learning, and beyond~\citep{Eldar:2012,Hastie2015,Neyshabur2015,Golowich17,Hieber2017,HEBIRI2025106195,golestaneh2024many}.
However, sparsity-inducing prior functions are abundant in statistics and machine learning, 
they are rarely employed for generative models~\citep{lin2016estimation}. 
In this paper, we examine the advantages of incorporating regularization into the objective functions of score-based diffusion models.
Additionally, we leverage techniques from empirical process theory~\cite{Sara2000,Vershynin2018} to analyze regularized objectives and calibrate the tuning parameter.

\subsection{$\ell_1$-regularized denoising score matching}\label{sec:elloneReg}
Here we propose an $\ell_1$-regularized estimator for diffusion models, inspired by the concept of ``scale regularization''  in deep learning~\citep{taheri2021}. Consider the parameter space
\begin{multline}
\label{eq:paramspace}
  \paramspaceone \deq \bigl\{\NetP \in \R^{p}:\norm{\scoref(\x_t,t)}_1~\le~1~~~\forall \x_t\in \R^{\Dim},\\ t\in \{1,\dots,T\} \bigr\}\,,
\end{multline}
where $\scoref(\cdot,\cdot): (\R^{\Dim},\mathbb{N}) \to \R^{\Dim}$ is modeled as a neural network with two inputs, parameterized by a tuple $\NetP = (W_0, \dots, W_L)$. Here, $\NetP$ collects all the weight matrices of the network, which has $L$ hidden layers and input and output dimensions in $\R^{\Dim}$. 
We consider $\scoref(\cdot,\cdot)$ as a time-dependent score-based model approximating $\nabla_{\x_t} \log \Fdd_t(\x_t)$, which is crucial for sample generation in the backward process of diffusion models. 
The parameter space $\paramspaceone$ corresponds to sparse score functions, more specifically, the outputs of these networks are sparse. 
Motivated by the denoising score matching objective, a scalable alternative to the objective function in~\eqref{scorematch}~\citep{vincent2011connection}, we define a regularized denoising score-matching estimator as
\begin{align}\label{dscorematchR}
 (\DnSMR,\scaleM)\in \argmin_{\substack{\NetP\in \paramspaceone \\ \scale\in (0,\infty)}}\E_{\substack{t\sim \mathcal{U}_{[0,T]}\\X_t\sim\Fd_t}}&\norm{\scale\scoref(X_t,t)-\nabla_{X_t}\!\log\Fdd_t(X_t)}^2\notag\\
 &+\tuning \scale^2 \,, 
\end{align}
where $\scale \in (0,\infty)$ represents the scale of the score function, $\tuning \in [0,\infty)$ is a tuning parameter that balances the penalty between scale and score matching, 
% $\lambda(t) \in \R^+$ is a positive weighting function that we ignore it for simplicity in our computations, 
and $\mathcal{U}_{[0,T]}$ denotes the uniform distribution over $[0,T]$.
The fixed constraint $\NetP\in \paramspaceone$ enforces $\ell_1$-norm regularization, while the actual regularization concerns only on the scale $\scale\in (0,\infty)$.
Additionally, we regularize $\scale^2$  to simplify our proofs.
We will further elaborate in Section~\ref{sec:TrainingAlg}  on how the objective function in~\eqref{dscorematchR} can be computed in practice in terms of expectation and score functions. 



Our main contribution in this paper is to theoretically and numerically demonstrate that our proposed regularized estimator in~\eqref{dscorematchR} can accelerate the sampling process of diffusion models, specifically increasing the rate of convergence in  Kullbackâ€“Leibler divergence from $\Dim^2/\error$~\citep{li2023towards} or $\Dim^{1.5}/\error$~\citep{liang2024nonN} to $s^2/\error$, where $s \ll \Dim$.


We are now ready to introduce some assumptions and present our main theorem for our proposed estimator in~\eqref{dscorematchR}.


We first set the learning rates to be used for our theory and analyses.
For sufficiently large $T$, we set the step size $\alpha_t$ as
\begin{align}\label{eq:stepsize}
    1-\alpha_t\lessapprox \frac{\log T}{T}\,,~~\forall t\in \{1,\dots,T\}\,.
\end{align}

We then impose some standard assumptions on the true density function.
    
\begin{assumption}[Finite Second Moment]\label{Assum:FSM}
There exists a constant $\FSM<\infty$ 
such that $\E_{X_0\sim \Fd_0}\norm{X_0}^2\le \FSM$.    
\end{assumption}
Assumption~\ref{Assum:FSM} be employed in the proof of our main theorem for the convergence of the forward process ensuring that the distribution is not excessively heavy-tailed.
The same assumption is also utilized in previous works including~\citet{chen2023improved}, \citet{chen2022sampling}, \citet{benton2023linear}, and~\citet{liang2024nonN}.

\begin{assumption}[Absolute Continuity]\label{assum:conAna}
    We assume that $\Fd_0$ is absolutely continuous w.r.t. the Lebesgue measure, and thus $\Fdd_0$
exists. 
% Also, suppose that $\Fdd_0$ is analytic.
\end{assumption}

We then push an assumption over the true gradient vectors $\nabla_{\x_t}\log\Fdd_t(\x_t)$ for $t\in \{1,\dots,T\}$, that is, assuming they can be well approximated by some sparse versions.
More precisely, we assume that only a small subset of features or directions in the high-dimensional space contributes significantly to the score functions.
Assumption~\ref{Assum:approxs} is well-motivated in high-dimensional statistics and is central to our main Theorem~\ref{the:maindS}. 
\begin{assumption}[Sparsity]\label{Assum:approxs}
There is a sparsity level $s\in\{1,2,\dots\}$ and an accuracy $\epsilon\in(0,\infty)$, 
$\epsilon\leq1/T$,
%with $s\ll\Dim$ and $\epsilon\ll1$,
such that for all $t \in \{1,\dots,T\}$, there is an analytic auxiliary function $\Fdds_t(\x_t)$ and the corresponding score 
% there is a score 
$\nabla_{\x_t}\log\Fdd_t^s(\x_t)$ that is $s$-sparse and $\epsilon$-accurate:
\begin{multline*}
\E_{X_t \sim \Fd_t}\norm{\nabla_{X_t}\log\Fdd_t^s(X_t)}_0~\leq~s~~\text{and} \\
\frac{1}{T} \sum_{t=1}^T
   \sqrt{\E_{X_t \sim \Fd_t} \norm{\nabla_{\x_t}\log\Fdd_t(X_t) - \nabla_{\x_t}\!\log\Fdds_t(X_t)}^2}\leq\Esparsity\,.
\end{multline*}
% Consequently, $\norm{\nabla_{\x_t}\log\Fdds_t(\cdot)}_2 \leq s \scaleS$, for a $\scaleS \in (0,\infty)$ referred to as the optimal scaling factor.
\end{assumption}


We then assume that the derivatives of true log densities are regular, that is, they are bounded by a constant $\DerBound\in (0,\infty)$.
\begin{assumption}[Regular Derivatives]\label{assum:ReDe}
For all $t \in \{1,\dots,T\}$ and $l\in \{1,2,\dots\}$ and $\boldsymbol{a}\in [d]^p$ such that $|\boldsymbol{a}|_1=p\in \{1,2,\dots\}$, it holds that
\begin{multline*}
    \E_{X_t\sim \Fd_t}|\partial^p_{\boldsymbol{a}}\log \Fdd_t(X_t)|^\ell~\le~B~~\text{and} \\
    \E_{X_t\sim \Fd_t}|\partial^p_{\boldsymbol{a}}\log \Fdd_t\bigl(\ut(X_t)\bigr)|^\ell~\le~B 
\end{multline*}
 for a constant $\DerBound\in (0,\infty)$.    
\end{assumption}
The regularity Assumption~\ref{assum:ReDe} is required for our analysis in Lemma~\ref{lem:RevError} and  also is utilized in previous works like~\citet{huang2024convergence}. As discussed extensively in~\citet[Section~5]{liang2024nonN}, this assumption is relatively mild, for example for distributions with finite variance or   Gaussian mixtures. 




\begin{theorem}[Non-asymptotic rates of convergence for regularized diffusion models]\label{the:maindS}
Under the Assumptions~\ref{Assum:FSM},~\ref{assum:conAna},~\ref{Assum:approxs}, and~\ref{assum:ReDe} and
    for $\tuning\ge \tuningorc\deq 8\sqrt{2\log (n)/n}$, our estimator proposed in~\eqref{dscorematchR}  generates samples with
    \begin{align*}
     &\KL(\Fd_0 ||\BdA_0)
     ~\le~\frac{\COr \FSM}{T^2}+\frac{\COr}{T}\max\{1,(s\DerBound)^2\} \\%{T}~~~O\Bigl(\frac{1}{T}\Bigr)+O\Bigl(\frac{s \Lc^2}{T}\Bigr)+O\Bigl(\frac{s \Lc^2 \epsilon}{T}\Bigr)+O(s\Lc^2\serror)\\
     &+\frac{\log T}{T}  \sum_{t=1}^{T}\inf_{\substack{\NetP\in \paramspaceone\\ \scale\in (0,\infty)}}\biggl\{\frac{1}{n} \sum_{i=1}^{n}  \norm{\scale\scoref(\x_t^i,t)-\nabla_{\x_t}\!\log\Fdd_t(\x_t^i)}^2\\
      &~+\tuning\scale^2\biggr\}
      % +\frac{1}{n} \sum_{i=1}^{n}  \norm{\nabla_{\x_t} \log q_t(\x_t^i)-\nabla_{\x_t}\log q^s_t(\x_t^i)}^2\\
      + s\DerBound^2\frac{\sqrt{2\log (nT)}}{n}
      +\Delta_{T} (\log \Fdd,\log \Fdds)
      % &~~+\E_{X_t\sim Q_t}\norm{\nabla_{\x_t}\log q_t(X_t)-\nabla_{\x_t}\log q^s_t(X_t)}_2^2\\
  %     &~~+\sum_{t=1}^{T} \biggl(\E_{X_{t}\sim \Fd_{t}}\bigl[\E_{X_{t-1}\sim \Fd_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]\notag\\
  % &~~-\E_{X_{t-1}\sim \Bds_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]\bigr]\biggr)
  % &~~+\frac{\COr s \scaleS}{T} 
\end{align*}
% \begin{align*}
%     \Delta_{T} &(\log \Fdd_{t-1},\log \Fdds_{t-1})\\
%     &\deq \sum_{t=1}^{T} \biggl(\E_{X_{t}\sim \Fd_{t}}\bigl[\E_{X_{t-1}\sim \Fd_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]\notag-\E_{X_{t-1}\sim \Bds_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]\bigr]\biggr)
% \end{align*}
for a constant $\COr\in (0,\infty)$ and $\Delta_{T} (\log \Fdd,\log \Fdds)
    \deq \sum_{t=1}^{T} (\E_{X_{t}\sim \Fd_{t}}[\E_{X_{t-1}\sim \Fd_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]\notag-\E_{X_{t-1}\sim \Bds_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]])$ with probability at least $1-1/n^2$.
\end{theorem}
\begin{corollary}
[Parametric setting]\label{cor:maindS}
Assume that $\tuning=\tuningorc$ and that there exists a pair $(\Theta^*,\kappa^*)\in \paramspaceone \times (0,\infty)$ such that $\kappa^*\scorefS(\x_t^i,t)=\nabla_{\x_t}\log q_t(\x_t^i)$ for all $i\in\{1,\dots,n\}$ and $t\in \{1,\dots,T\}$. Then, under the Assumptions of Theorem~\ref{the:maindS}, our estimator proposed in~\eqref{dscorematchR}  generates samples with
    \begin{align*}
     &\KL(\Fd_0 ||\BdA_0)
     ~\le~\frac{\COr}{T}\max\{1,(s\DerBound)^2\}\\
     &~~+ \frac{\log (nT)}{n}\max\{(s\DerBound)^2,(\scale^*)^2\}
      +\Delta_{T} (\log \Fdd,\log \Fdds)
\end{align*}
for a constant $\COr\in (0,\infty)$  with probability at least $1-1/n^2$.
\end{corollary}
Our Theorem~\ref{the:maindS} reveals that if the true gradient vectors $\nabla_{\x_t} \log q_t(\x_t)$ can be well approximated by $s$-sparse vectors $\nabla_{\x_t}\log q^s_t(\x_t)$ and for sufficiently large tuning parameter, the rates of convergence of diffusion models scale with $s$, where $s \ll \Dim$ (and not $\Dim$).  
The term $\Delta_{T}(\log \Fdd,\log \Fdds)$ in the bound of Theorem~\ref{the:maindS} measures how close the log density  $\log \Fdd_t(\x_t)$ is to the auxiliary log density $\log \Fdds_t(\x_t)$ across the entire sample space and time steps.  
Note that we use the notation $\Bds_t$ for the distribution of the latent steps in the reverse process, utilizing the sparse scores of Assumption~\ref{Assum:approxs} (see also Section~\ref{sec:Aux} for more details). 
Following the intuition behind score matching, we argue that Assumption~\ref{Assum:approxs} also promotes closeness between these log densities.
Furthermore, our empirical observations in Section~\ref{sec:sim} support the practical validity of our assumptions.
Theorem~\ref{the:maindS}  also directly implies a bound on the the total-variation distance between $\Fd_0$ and $\BdA_0$ in view of Pinsker's inequality.
Detailed proof of Theorem~\ref{the:maindS} is provided in Appendix~\ref{proof:mainR}. 
Corollary~\ref{cor:maindS} follows directly from Theorem~\ref{the:maindS}, so we omit its proof.

Assuming the bound $\sum_{t=1}^T\E_{X_t\sim \Fd_t}\norm{\score(X_t,t)-\nabla_{X_t}\log \Fdd_t(X_t)}^2/T\le \varsigma$,
works like~\citet{li2023towards} and~\citet{liang2024nonN}
show that the reverse diffusion process produces a sample with error roughly at the order of $\varsigma$. But whether such accurate estimators are available in practice remains unclear.
For example,
\citet[Theorem 3.5, Corollary 3.7]{zhang2024minimax} upper bounds the estimation error of the score (using $n$ training samples) of the order of $n^{-2\beta/(2\beta+\Dim)}$ assuming the true data distribution $\Fdd_0$ is 1.~$\sigma_0$-sub-Gaussian and 2.~in the Sobolev class of density functions with the order of smoothness $\beta \le 2$, see also \citet{wibisono2024optimal,dou2024optimal}. 
This result highlights that the original score matching method suffers from the curse of dimensionality (note that $\beta \le 2\ll \Dim$). 
Thus, regularization might not only  accelerate the reverse process but also help in the estimation of the scores---compare to~\citet{lederer2023extremes}. 



\section{Technical results}\label{sec:ThecRe}
Here we provide some auxiliary results  used in the proof of Theorem~\ref{the:maindS}. 

\begin{lemma}[Reverse-step error]\label{lem:RevError}
Under the Assumption~\ref{Assum:approxs} and Assumption~\ref{assum:ReDe} we have for a constant $\COr\in (0,\infty)$ 
 \begin{align*}
    &\sum_{t=1}^{T}~\E_{X_t,X_{t-1}\sim \Fd_{t,t-1}}\biggl[\log\frac{\Fdd_{t-1|t}(X_{t-1}|X_t)}{\Bdd^s_{t-1|t}(X_{t-1}|X_t)}\biggr]\notag\\
    &\le \frac{\COr}{T}\bigl(s\DerBound^2+  s^2\DerBound^2+ s^2\DerBound^2\epsilon\bigr)+\COr\sqrt{s}\DerBound \epsilon\notag\\
 &~~~~~~+\Delta_{T} (\log \Fdd,\log \Fdds)\,.
 % +\sum_{t=1}^{T}\Bigl(\E_{X_{t}\sim \Fd_{t}}\bigl[ \E_{X_{t-1}\sim \Fd_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1}) \notag\\
 %   &~~-\log \Fdds_{t-1}(X_{t-1})]\notag\\
 %  &~~-\E_{X_{t-1}\sim \Bds_{t-1|t}}[\log \Fdd_{t-1}(X_{t-1})-\log \Fdds_{t-1}(X_{t-1})]\bigr]\Bigr)\,.
\end{align*}
 
\end{lemma}
Lemma~\ref{lem:RevError} helps upper bounding the reverse-step error for the backward process of diffusion models and 
its detailed proof is provided in Appendix~\ref{proof:lemReverror}. 

We then present a lemma that aids in determining the optimal rates for the tuning parameter.
\begin{lemma}[Empirical processes]\label{lem:emp}
Under the Assumption~\ref{Assum:approxs} 
% and for $\tuning\ge 2\tuningorc$ 
we obtain 
\begin{align*}
   \sup_{\NetP\in \paramspaceone}\Bigl|&\E_{X_t\sim Q_t}\norm{\scaleh\scoref(X_t,t)-\nabla\log q^s_t(X_t)}^2\\
     &~~~~-\frac{1}{n} \sum_{i=1}^{n}  \norm{\scaleh\scoref(\x_t^i,t)-\nabla\log q^s_t(\x_t^i)}^2\Bigr|\\
     &\le 8 (\scaleh^2+s \DerBound^2) \sqrt{\frac{2\log n}{n}}
\end{align*}
with probability at least $1-32/n^2$.
\end{lemma}
Lemma~\ref{lem:emp} is employed in the proof of Theorem~\ref{the:maindS} to calibrate the tuning parameter. 
Its detailed proof  is provided in Appendix~\ref{proof:lememp}. 
