In this section, we provide additional simulation supporting our theories on further image dataset Butterflies (Section~\ref{sec:BF}).
We also  introduce our training and sampling approach in Section~\ref{sec:TrainingAlg} and provide details about network architecture and training settings in Section~\ref{sec:netstr}. 
\subsection{Butterflies}\label{sec:BF}
We also compare original diffusion and  regularized analog on \ButF~dataset (smithsonian-butterflies) including $n=10\,000$ training samples.
We consider regularization $\tuning=0.0001$ for $T=1000$ and $\tuning=0.0005$ for $T\in \{200,150\}$.
Results are provided in Figure~\ref{fig:BF}. 
Again, our results show that our approach perform better than the original score matching  for small values of $T$. 

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Butterfly/BFT.png}
        % Caption or additional description for the centered image
    \end{minipage}
    
    \vspace{0.1em} % Adjust vertical space between rows
    % First row
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Butterfly/BFO1000.png}
        % \caption{Caption for figure 1}
    \end{minipage}
    \hspace{13em}
     \begin{minipage}{0.29\textwidth}
        \centering      \includegraphics[width=\textwidth]{Figures/Butterfly/BFR1000.png}
        % \caption{Caption for figure 4}
    \end{minipage}
    
    \vspace{0.1em}
    
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Butterfly/BFO200.png}
        % \caption{Caption for figure 2}
    \end{minipage}
    \hspace{13em}
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Butterfly/BFR200.png}
        % \caption{Caption for figure 5}
    \end{minipage}

     \vspace{0.1em} 
    
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Butterfly/BFO150.png}
        % \caption{Caption for figure 3}
    \end{minipage}  
    \hspace{13em} % Adjust vertical space between rows 
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Butterfly/BFR150.png}
        % \caption{Caption for figure 6}
    \end{minipage}
    
    \caption{Image generation using the original denoising score matching (left column) versus the regularized version (right column) for different time steps,  $T=1000, T=200$, and $T=150$ (from top to bottom).  
    The middle column displays $81$ original samples from the
\ButF~dataset for comparison.
The dataset consists of images with dimensions $\Dim=28\times28\times 3=2352$. As shown in the images, our regularized version  generates high-quality images for  
$T=1000$ (comparable to the original denoising score matching) and still perform better than original denoising score matching for  
$T=200$ and $T=150$.}\label{fig:BF}
\end{figure*}







\subsection{Training and sampling algorithms}\label{sec:TrainingAlg}
Here we provide details about how we solve the objective function~\eqref{dscorematchR} in practice, that is, how we deal with the expected values and score functions. 

Let's first define the objective function over a batch  of training examples $\x_{\batchS}$ (a batch of size $\batchS\in \{1,2,\dots\}$) and for a batch of random time steps $\boldsymbol{t}_{\batchS}\in (0,1]^{\batchS}$: 
\begin{equation}\label{eq:simobj}
f(\scale,\Theta,\x_{\batchS},\boldsymbol{t}_{\batchS})\deq \frac{1}{\batchS}\sum_{i=1}^{\batchS}\norm{\scale\scoref(\x^i_{t_i},t_i)-\nabla_{\x_{t}}\log \Fdd_t(\x^i_{t_i}|\x^i)}^2\bigr]
 +\tuning \scale^2 
\end{equation}
with 
\begin{equation}\label{eq:pertx}   \Fd_t(\x^i_{t_i}|\x^i)=\mathcal{N}\bigl(\x^i,\sigma_{t_i}\Identity\bigr) 
\end{equation}
with $\sigma_t\deq (\sigma^{2t}-1)/(2\log \sigma)$ for $t\in (0,1]$ and a large enough $\sigma\in (0,\infty)$ (we set $\sigma=5$ for \ButF~and $\sigma=25$ for other datasets). 
Here $\x^i_t$ corresponds to a perturbed version of the training sample $\x^i$ ($i$th sample of the batch) in  time step $t$. As stated in~\eqref{eq:pertx}, once $\sigma$ is large, $\x_{1}$ ($t=1$) goes to a mean-zero Gaussian. 
And as shown in~\citet{vincent2011connection}, the optimization objective  $\E_{\Fdd_t(\x_t|\x)\Fdd_{0}(\x)}[\norm{\scale\scoref(\x_{t},t)-\nabla_{\x_{t}}\log \Fdd_t(\x_{t}|\x)}^2]$ for a fixed variance~$\varnoise_t$ is equivalent to the optimization  objective $\E_{\Fdd_t(\x_t)}[\norm{\scale\scoref(\x_{t},t)-\nabla_{\x_{t}}\log \Fdd_t(\x_{t})}^2]$ and, therefore, satisfies $\scale^*\scorefS(\x_{t},t)=\nabla_{\x_t}\log \Fdd_t(\x_t)$.
We then provide Algorithm~\ref{alg:training} for solving the objective function in~\eqref{dscorematchR}. 
Note that we can easily compute the score functions in~\eqref{eq:simobj} since there is a  closed form solution for them as densities are just Gaussian conditional on $\x^i$. 
\begin{algorithm}[H]
\caption{Training algorithm}\label{alg:training}
\begin{algorithmic}[1] % The [1] adds line numbering
\STATE \textbf{Inputs:} $\sigma$, $n_{\operatorname{epochs}}$ (number of epochs), $\batchS(\operatorname{batch-size})$, $\operatorname{eps}=0.00001$
\STATE \textbf{Outputs:} $(\DnSMR,\scaleM)$
\STATE Initialize parameters $(\DnSMR,\scaleM)$ 
\FOR{$i = 1$ to $n_{\operatorname{epochs}}$}
\FOR{$\x_{\batchS}  $ in data-loader}
\STATE $\boldsymbol{t}_{\batchS}=\{\mathcal{U}_{[0,1]}\}^{\batchS}  (1 - \operatorname{eps}) + \boldsymbol{\operatorname{eps}}$ 
    \STATE One step optimization minimizing   $f(\scale,\Theta,\x_{\batchS},\boldsymbol{t}_{\batchS})$ in~\eqref{eq:simobj} employing a random batch of time steps $\boldsymbol{t}_{bs}\in (0,1]^{bs}$  and updating $(\DnSMR,\scaleM)$
\ENDFOR
\ENDFOR


\end{algorithmic}
\end{algorithm}
Parameter $\operatorname{eps}$ in Algorithm~\ref{alg:training}  is introduced for numerical stability and to refuse $t=0$. 
For a sufficiently large number of epochs, we expect to learn the scores accurately for different time steps. 
For sampling process, we employ a naive sampler as proposed in Algorithm~\ref{alg:samp}
employing Langevin dynamics~\citep[Section~2.2]{song2019generative} to align with our theory.  
\begin{algorithm}[H]
\caption{Sampling algorithm}\label{alg:samp}
\begin{algorithmic}[1] % The [1] adds line numbering

\STATE \textbf{Inputs:} $\sigma$, $\operatorname{eps}=0.00001$, T (Time steps)
\STATE \textbf{Output:} \x
\STATE $\x=\x_{\operatorname{init}}=\StandNormal\sigma_1$ 
\STATE $\boldsymbol{t} = linspace(1., eps, T)$ (make a grid of time steps)
\STATE $\eta = \boldsymbol{t}[0] - \boldsymbol{t}[1]$ (set step size)
\FOR{t in $\boldsymbol{t}$}
\STATE $\x=\x+\eta \scaleM\score_{\DnSMR}(\x,t)+\sqrt{2\eta}\StandNormal$ (update $\x$)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Network architecture and training settings}\label{sec:netstr}
Our model is a U-Net architecture with 4 downsampling and 4 upsampling layers, each comprising residual blocks. The network starts with a base width of 32 channels, doubling at each downsampling step to a maximum of 256 channels, and mirrors this in the decoder. A bottleneck layer with 256 channels connects the encoder and decoder. Time information is encoded using Gaussian Fourier projections and injected into each residual block via dense layers. Group normalization is applied within the residual blocks, and channel attention mechanisms are included selectively to enhance feature representations.
For training, we used the Adam optimizer with a learning rate of 0.001, and for sampling, we employed a signal-to-noise ratio of 0.1.
We used a batch size of $128$ and trained for $2000$ epochs on the \ButF~dataset and less than $1000$ epochs on the other datasets.