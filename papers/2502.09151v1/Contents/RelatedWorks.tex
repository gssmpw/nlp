
The non-asymptotic rates of convergence for diffusion models established very recently~\citep{block2020generative,de2021diffusion,de2022convergence,lee2022convergence,chen2023probability,chen2023improved,li2023towards,chen2023restoration,huang2024convergence} show the large interest in this topic and are an important step forward but do not fully explain the success of generative models either.
For example, results like~\citet[Theorem~13]{block2020generative} provide rates of convergence for diffusion models in terms of Wasserstein distance employing the tools from empirical-process theory, 
but they suffer from the curse of dimensionality in that the rates depend exponentially on the dimensions of the data, that is, the number of input features. 
Recent works then concentrate on improving convergence guarantees to grow polynomially in the number of input features under different assumptions on the  original and estimated scores ($L_2$-accurate score estimates, Lipschitz or smooth scores, scores with bounded moments)~\citep{lee2022convergence,wibisono2022convergence,chen2022sampling,chen2023probability,chen2023improved,chen2023restoration,lee2023convergence,huang2024convergence}. 
 For example,~\citet{lee2022convergence} prove a convergence guarantee  in terms of total variation for~\sgm, 
 which has a polynomial dependence on the number of input features if the score estimate is $L_2$-accurate for any smooth distribution satisfying the log-Sobelev inequality.
 %Their rates grow polynomially in the dimension of the data under some additional assumptions. 
A very recent work by~\citet{li2023towards} proposes improved convergence rates in terms of total variation for~\ddpm~with ordinary differential equations and stochastic differential equations samplers that are proportional to $\Dim^2/\error$,
where $\Dim$ is the number of input features and $\error$ the error in the measure under consideration.
They assumed 1.~finite support  assumption, 2.~$L_2$-accurate score estimates, and 3.~accurate Jacobian matrices. 
\citet[Theorem~3]{li2023towards} also provides rates growing by~$\Dim^3/\sqrt{\error}$ for an accelerated ordinary differential equations samplers. 

While works like~\citet{li2023towards} and \citet{li2024accelerating} concentrate more on improving the rates in~$\error$,~\citet{chen2023probability} focus on improving the rates in~$\Dim$ for denoising diffusion implicit models.
\citet{chen2023probability} use a specially chosen corrector step based on the underdamped Langevin diffusion to achieve their improvements, namely rates  proportional to $L^2\sqrt{\Dim}/\error$
by assuming: 1.~the score function along the forward process is $L$-Lipschitz,  2.~finite second moments of the data distribution, and  3.~$L_2$-accurate score estimates. 
\citet{chen2023improved,benton2023linear} then  relaxed the assumptions over the data distribution and  proposed  the  rates of convergence for DDPM  proportional to $\sqrt{\Dim^2/\tau}$ and $\sqrt{\Dim/\tau}$ under 1.~finite second moments of the data distribution, and  2.~$L_2$-accurate score estimates. 
Further research directions may also build upon~\citet{chen2023score}, who assume that the data lie on a low-dimensional linear subspace. They demonstrate that, in this setting, the convergence rates depend on the dimension of the subspace.
