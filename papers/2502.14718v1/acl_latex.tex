% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.


\documentclass[11pt]{article}

\usepackage{pgfplotstable}
\usepackage{multirow}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}

\usepackage[utf8]{inputenc}
\usepackage{tcolorbox}
\usepackage{parskip} % Adds space between paragraphs
% \usepackage{newtxtext}    % Times New Roman-like font
\usepackage{graphicx}
\usepackage{pgf}

% Define the tcolorbox style
\tcbset{
  colframe=black,
  colback=white,
  fonttitle=\bfseries,
  sharp corners,
  boxrule=0.5pt,
  width=\columnwidth,
  left=4pt,
  right=4pt,
  top=4pt,
  bottom=4pt
}

\usepackage{svg}        % For including SVG files
\usepackage{caption}    % For customizing captions
\usepackage{subcaption}    % For customizing captions
\usepackage{tikz}
\usetikzlibrary{shapes.callouts}
\usetikzlibrary{positioning}
\usepackage{fancyvrb}      
\usepackage{framed}        
\usepackage{xcolor}        
\usepackage{graphicx}      
\usepackage{caption}       
\usepackage{amsmath}  
% \usepackage{newtxtext,newtxmath}
% Command to create labeled entity mentions
% \newcommand{\labeledentity}[3]{%
%   \tikz[baseline=(word.base)]{
%     % Label above the entity mention
%     \node[anchor=south, inner sep=2pt, fill=#3, rounded corners=2pt, text=white] (label) {\tiny #1};
%     % Entity mention with a box
%     \node[below=1pt of label, draw, thick, rounded corners=2pt, inner sep=2pt] (word) {#2};
%   }%
% }
% Define custom colors
\definecolor{lightbrown}{rgb}{0.71, 0.40, 0.11}
\definecolor{darkblue}{rgb}{0.00, 0.20, 0.60}
\definecolor{lightblue}{rgb}{0.20, 0.60, 0.86}

% Define text commands for convenience
\newcommand{\lightbrowntext}[1]{\textcolor{lightbrown}{#1}}
\newcommand{\darkbluetext}[1]{\textcolor{darkblue}{#1}}
\newcommand{\lightbluetext}[1]{\textcolor{lightblue}{#1}}
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\definecolor{darkred}{rgb}{0.7, 0, 0.1}
\definecolor{darkblue}{rgb}{0, 0, 0.5}

% Labeled entity command
\newcommand{\labeledentity}[3]{%
  \tikz[baseline=(word.base)]{
    \node[anchor=south, inner sep=2pt, fill=#3, rounded corners=2pt, text=white, font=\scriptsize] (label) {\tiny #1};
    \node[below=1pt of label, draw, thick, rounded corners=2pt, inner sep=2pt, font=\small] (word) {#2};
  }%
}
% \usepackage{tcolorbox}
% \usepackage{lipsum} % For placeholder text

% % Define the box style
% \tcbset{
%   colframe=black,
%   colback=white,
%   fonttitle=\bfseries\small,
%   sharp corners,
%   boxrule=0.5pt,
%   width=\textwidth,
%   left=2pt,
%   right=2pt,
%   top=2pt,
%   bottom=2pt
% }
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Entity Framing and Role Portrayal in the News}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Tarek Mahmoud$^1$, 
  \textbf{Zhuohan Xie}$^1$,
  \textbf{Dimitar Dimitrov}$^2$,
  \textbf{Nikolaos Nikolaidis}$^3$, 
  \textbf{Purificação Silvano}$^4$, \\
  \textbf{Roman Yangarber}$^5$,
  \textbf{Shivam Sharma}$^6$,
  \textbf{Elisa Sartori}$^{7}$,
  \textbf{Nicolas Stefanovitch}$^{8}$, \\
  \textbf{Giovanni Da San Martino}$^{7}$
  \textbf{Jakub Piskorski}$^9$,
  \textbf{Preslav Nakov}$^1$,
  \\
  %{\small\tt~\Letter~\hspace{-0.12cm}jpiskorski@gmail.com}\\
	$^1$MBZUAI, 
    $^2$Sofia University "St. Kliment Ohridski", 
    $^3$Athens University of Economics and Business, \\
 $^4$University of Porto, 
    $^5$University of Helsinki,
    $^6$Indian Institute of Technology Delhi,
    $^{7}$University of Padova, \\
    $^{8}$European Commission Joint Research Centre,
    $^9$Institute of Computer Science, Polish Academy of Science \\
\href{tarek.mahmoud@mbzuai.ac.ae}{\{tarek.mahmoud, preslav.nakov\}@mbzuai.ac.ae}
%{\texttt dasan@math.unipd.it, jpiskorski@gmail.com, nicolas.stefanovitch@ec.europa.eu, preslav.nakov@mbzuai.ac.ae}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\pgfplotsset{compat=1.18} 
\begin{document}
\maketitle
\begin{abstract}
We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: \emph{protagonist}, \emph{antagonist}, and \emph{innocent}. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence. 

% Compared to naive single-step prompting, hierarchical prompting reduces costs by a staggering $40.15\%$. It also marginally improves accuracy for main role predictions by $0.6\%$, though it results in a $3.38\%$ drop in F1 for fine-grained role predictions across all languages.
\end{abstract}

\begin{figure*}[!htbp]
\centering
\footnotesize
\fbox{%
     \parbox{1\textwidth}{%\
    % \fontsize{6}\selectfont
     {\centering \textbf{Putin says what Russia needs to do to win special operation in Ukraine } \\[1em]}

    
    Russia will win the special operation in Ukraine if the society shows consolidation and composure to the enemy, President Vladimir Putin said during a visit to the Ulan-Ude Aviation Plant on March 14, Rossiya 24 TV channel said.
    
    Russia is not improving its geopolitical position in Ukraine. Instead, \labeledentity{Underdog}{Russia}{darkblue} is fighting "for the survival of Russian statehood, for the future development of the country and our children."
    
    "In order to bring peace and stability closer, we, of course, need to show the consolidation and composure of our society. When the enemy sees that our society is strong, internally braced up, consolidated, then, without any doubt we will come to reach what we are striving for — both success and victory," Putin said.
    
    According to him, many of the current problems began after the collapse of the Soviet Union, when they tried to put pressure on \labeledentity{Victim}{Russia}{darkgreen} to "destabilise the internal political situation.” "Hordes of international terrorists" new sent to the purpose to accomplish this goal, Putin said.
    
    Afterwards, the West decided to start rehabilitating Nazism in Russia's neighbouring states, including in Ukraine.
    
    Nevertheless, Putin continued, Russia had long tried to build partnerships with both Western countries and Ukraine. However, after 2014, when the West contributed to the coup in Ukraine, the state of affairs changed dramatically. It was then when they started exterminating those who advocated the development of normal relations with Russia, he said.
    
    According to Putin, \labeledentity{Guardian}{Russia}{darkblue} was forced to launch the special operation to protect the population. \labeledentity{Saboteur}{Western countries}{darkred} were hoping to break Russia quickly, but they were wrong, he said adding that \labeledentity{Virtuous}{Russia}{darkblue} managed to raise its economic sovereignty since 2022.
    
    }%
}
\caption{Annotated example color-coded according to the main roles in the taxonomy: \textcolor{darkred}{red} for \emph{antagonist}, \textcolor{darkblue}{blue} for \emph{protagonist}, and \textcolor{darkgreen}{green} for \emph{innocent}.}
\label{fig:annotated_example_text2}
\end{figure*}


\section{Introduction\label{sec:intro}}
The rapid proliferation of social media has dramatically transformed the information landscape, providing immediate access to news, and allowing anyone to propagate their narratives across the globe. While this connectivity functions as a convenient avenue for information dissemination, it also heightens the risk of exposure to biased reporting, propaganda, and narrative manipulation. These risks are particularly pronounced during periods of conflict and political upheavals, where the framing of entities—individuals, organizations, or groups—can profoundly influence public perception and decision-making. Understanding how entities are portrayed in the news is essential for fostering media literacy, identifying bias, and ensuring transparent news consumption.

Social science highlights the role of emotion in framing—selecting elements that evoke affective responses to shape perceptions \cite{https://doi.org/10.1111/j.1467-9221.2004.00354.x}. Emotional framing often leverages language that elicits specific feelings, such as fear, anger, or compassion, to influence how entities and events are understood \cite{iyengar_is_1991, nabi_exploring_2003,https://doi.org/10.1111/j.1460-2466.2000.tb02843.x,brader_campaigning_2006}. For instance, referring to a group as ``freedom fighters'' versus ``terrorists'' not only frames their role but also activates distinct emotional reactions. Research has shown that emotional appeals are powerful tools for reinforcing or challenging public attitudes \cite{westen_political_2008,lerner_emotion_2015}. Such framing can manifest through specific linguistic cues and includes the portrayal of entities also defined by \citet{schneider_social_1993} as the \emph{Social Construction of Target Populations}.

Natural language processing research has increasingly been applied to analyze the emotional dimensions of framing \cite{troiano-etal-2023-relationship}, including the identification of sentiment \cite{zhang-etal-2024-sentiment,app13074550} and emotion-laden narratives \cite{mousavi-etal-2022-emotion}. Understanding these emotional components provides deeper insight into how media narratives construct and perpetuate particular representations of entities, ultimately shaping public perception and societal discourse.

Given the large scale and complexity of the modern news ecosystem, effective analysis of entity framing requires automated tools, which depend on high-quality annotated data. In this context, we introduce a new multilingual dataset designed to develop tools for the study of entity framing and role portrayal in news articles. Our dataset uses a unique, hierarchical taxonomy inspired by elements of storytelling containing a set of 22 carefully defined archetypes nested under three main roles: \emph{protagonist}, \emph{antagonist}, and \emph{innocent}.

% \jp{Our dataset is not meant for studying news articles directly but for training models to facilitate carrying out such studies on representative corpora. Our corpus might not be represenative to draw any conclusions. I suggest to reword accordingly. The non-representiveness of our corpus should be mentioned in Limitations} 
% Agreed. Done

The corpus spans 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) and focuses on two globally significant domains: the Ukraine-Russia war and climate change. We annotated over 5,800 entity mentions with detailed role labels.


Our contributions can be summarized as follows:

\begin{itemize}
    \item We release a novel multilingual dataset annotated for entity framing and role portrayal, complete with detailed annotation guidelines.
    \item We introduce a comprehensive hierarchical taxonomy for entity roles validated on a large set of documents, supporting analysis at both the coarse and the fine-grained levels.
    \item We provide comprehensive dataset statistics, exploring entity portrayals across languages and topics.
    \item We set benchmarks using state-of-the-art multilingual transformer models, and hierarchical zero-shot learning with LLMs.
\end{itemize}



\section{Related Work\label{sec:related_work}}

\citet{sharma-etal-2023-characterizing} introduced a dataset for identifying heroes, villains, and victims in memes, focusing on visual content. In contrast, our dataset focuses on textual content. While both share similar coarse-level roles, our work adds an additional layer of granularity with a hierarchical taxonomy of 22 archetypes nested within these roles. 

\citet{card-etal-2016-analyzing} addressed a different aspect of framing. Their contribution is developing a model that makes use of personas to infer the article framing as defined in the Media Frames Corpus (MFC)  \cite{card-etal-2015-media}. MFC focuses on identifying how an article is framed along nine dimensions, such as Economic or Political. In their work, topic modeling is used to identify 50 personas. However, the results are noisy, with only a few informative personas, namely \emph{refugee} and \emph{immigrant}, while the 48 other personas, such as Job, Worker, and Year, are less informative. In contrast, our work focuses on \emph{entity} framing rather than article framing. While their work identifies personas in a weakly supervised manner, we develop a hierarchical taxonomy containing a richer set of roles, validated through human annotation of news articles across two diverse domains, ensuring higher quality and broader applicability. There is more research on news framing that focuses on article-level framing \cite{Pastorino2024DecodingNN, DBLP:conf/acl/0001KF24, piskorski-etal-2023-multilingual, liu-etal-2019-detecting, card-etal-2015-media}. In contrast, our work centers on entity-level framing.

Aspect-Based Sentiment Analysis \cite{chebolu-etal-2024-oats, DBLP:journals/corr/abs-2203-01054, orbach-etal-2021-yaso, jiang-etal-2019-challenge, saeidi-etal-2016-sentihood} is also related. It involves identifying targets of specific opinions and determining the polarity of the sentiment associated with particular aspects of these targets. Typically, the polarity is binary, and multiple aspects of the target entity are examined. Our work on entity framing is different as we do not define aspects nor do we assign polarities. Instead, we introduce a hierarchical taxonomy for news, which contains a rich set of roles inspired by elements of storytelling, and entities can be classified into any subset of roles within that taxonomy.




\section{The Entity Framing Task}
Entity framing focuses on analyzing how a text portrays a specific entity through word choice and narrative structure. More concretely, given a news article and a list of entity mentions (i.e., entity mentions, along with their span offsets), we assign to each of them one or more roles based on the taxonomy shown in Figure \ref{fig:taxonomy_roles}. We developed this taxonomy specifically for this task and the roles were inspired by storytelling elements. The taxonomy includes 22 archetypes, or fine-grained roles nested under three main roles: \emph{protagonist}, \emph{antagonist}, and \emph{innocent}. The role an entity plays in a given article may differ from one context in that article to another depending on the portrayal. See Figure~\ref{fig:annotated_example_text2} for a complete, annotated example from the corpus.

% \textbf{Protagonist} is an entity portrayed in a favorable or sympathetic light, often seen as standing up for a noble cause, protecting others, or striving for justice. 

% \textbf{Antagonist} is an entity framed in an unfavorable or adversarial manner, often depicted as contributing to conflict, harm, or injustice. Antagonists are commonly portrayed as oppressive, deceitful, corrupt, or threatening.


% \textbf{Innocent} is an entity presented as neutral, victimized, or undeserving of harm or blame. Innocents are often characterized as vulnerable, exploited, or caught in a conflict through no fault of their own.


Entity framing can be formalized mathematically as follows. Let $R$ be a tree structure representing the taxonomy of roles. Let $S$ be a string of length $|S|$ characters with the content of the full article. The goal of entity framing is to learn a function
\begin{equation}
f: (S, [i,j]) \rightarrow \{r_1, r_2, \ldots, r_k\} \subseteq R
\end{equation}

\noindent where $0 \leq i < j \leq |S|$ and $\{r_1, r_2, \ldots, r_k\}$ is the set of roles assigned to the span $[i,j]$.





\input{taxonomy}


% In summary, the task involves finding a function $f$ that maps each character position in the span $(i,j)$ of an entity mention in the string $S$ to a subset of roles from the taxonomy $R$.


\section{Corpus Description}

\subsection{Domains}
\label{sec:corpus_domains}
The articles used in the task cover the following domains: (1) \emph{Ukraine-Russia War}, which includes articles about the war that started in February 2022 when Russia launched a full-scale invasion of Ukraine and began occupying parts of the country, and (2) \emph{Climate Change}, which encompasses both climate change denial (characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change), and climate change activism.
% \paragraph{Climate Change (CC):} encompasses both climate change denial (characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change), and climate change activism,
% \paragraph{Ukraine-Russia war (URW):} includes articles about the war that started in February 2022 when Russia launched a full-scale invasion of Ukraine and began occupying parts of the country.

\subsection{Article Selection}
% For each language, articles were primarily selected from links provided by the \textbf{Europe Media Monitor}~\footnote{https://emm.newsbrief.eu/} on the basis of multilingual keyword based categories already developed by the project \jp{I am not sure we should refer to EMM here at submission time due to potential revealing the identity. Say instead, large-scale in-house news aggregation .... bla bla . 
% Also we did select specific sources, which needs to be mentioned too}, 

For each language, articles were primarily selected from links we obtained from a large-scale in-house news aggregation tool. We performed the first candidate article selection based on multilingual keyword-based filters and we perform several steps, which we follow by to enrich the selection to match the criteria discussed below. To select the articles, we followed these steps:

\paragraph{Initial Collection:}  
Articles were scraped and filtered based on criteria such as word count (e.g.,~only articles exceeding 250 words were selected). For duplicate articles, the version with the higher number of words was preferred.

\paragraph{Filtering:}  
Each article was manually reviewed to determine its relevance to the annotation task. The articles were categorized into four groups: Perfect Fit, Average Fit, Uncertain (requiring further validation by language coordinators), or Unfit (excluded from annotation). Only articles classified as \emph{Perfect Fit} and \emph{Average Fit} were considered for annotation. 
% \nn{In EN, we included very few not directly relevant but adjacent topics to make the inclusion a bit more complete which was important for ST2.}
Afterwards, we used a zero-shot classifier with selected key phrases, and a persuasiveness score using the persuasion technique classifier as described in \citet{nikolaidis-etal-2024-exploring}. These scores were used to further enrich the selection with relevant articles.

% \paragraph{Language-Specific Sources:}  
% In addition to JRC-provided\jp{anonymity issue again!} articles, we used additional sources to capture diverse perspectives:
Additional sources were also incorporated to ensure diversity of perspectives for two languages: for Hindi, we selected articles from mainstream and bias-specific outlets (e.g., NDTV, The Hindu, OpIndia), and for Portuguese, from newspapers and political websites (e.g., \emph{O Diabo}, Esquerda, Folha Nacional) that had more controversial opinion texts about the relevant topics.
% \begin{itemize}
%     % \item \textbf{Bulgarian (BG):} Articles were selected both from mainstream and alternative media outlets.
%     % \jp{this is entirely unclear what this means} \dd{Nothing more to be added for Bulgarian}
%     % \nn{I assume texts == posts.}
%     \item \textbf{Hindi (HI):} Articles were selected from mainstream and bias-specific outlets (e.g., NDTV, The Hindu, OpIndia).
%     \item \textbf{Portuguese (PT):} Articles were sourced from newspapers and political websites (e.g., \emph{O Diabo}, Esquerda, Folha Nacional) which had more controversial opinion texts about the relevant topics.
%     % \jp{add urls? say something about them?}.
%     %\item \textbf{Russian (RU):} Articles were filtered based on loaded language and relevant keyword filters.
%     % \jp{in what sense? more loaded language means more suitable? Clarify}
%     % \nn{ I would remove RU here alltoghether, since I described the general approach above.}
%     % \nn{Please avoid references to EMM  
%     % due to anonymity concerns.}
    
% \end{itemize}

% \jp{The description above is somewhat unclear, ie. how and why were the additional sources selected?}
% \nn{On EN we did not follow any more steps than that.}


\subsection{Annotation Process}

Given that our corpus contains articles in five languages, we had one annotation team per language, each led by a language coordinator. Each language team included 3 to 5 annotators with prior experience in linguistics, social science, international relations, or prior annotation work. The annotators studied our detailed guidelines, attended live demonstrations, and completed real-time annotation exercises. During weekly meetings, teams clarified any uncertainties, resolved conflicts, improved consistency, and revised the annotation guidelines.
% \jp{and tuned the guidelines?} Yes

Each article was annotated by two annotators. Designated curators, often language coordinators or experienced annotators, reviewed and consolidated all annotations. They resolved the discrepancies through discussions with the respective teams. Over time, as the annotation quality improved, the curators reduced the frequency of checks but continued to perform random quality checks to ensure that annotations were of high quality. We used the Inception tool~\cite{tubiblio106270} for annotating and curating the corpus. See Appendix~\ref{sec:annotation_tool} for more details.
% \jp{consolidated?}\jp{annotations?} yes.

%Some challenges arise during annotation. Annotators initially struggled with spurious labels due to the annotation bias surrounding the topics under consideration. For example, the preconceived notion that \emph{entity A} was a natural offender (and not \emph{entity B}) in an ongoing conflict may cause annotation bias. Annotators also found richer content for URW-related topics due to the abundance of relevant narratives and entities. In contrast, CC articles lacked clear entity roles and argumentative content, resulting in sparser annotations. Weekly meetings were instrumental in refining and converging to the current annotation guidelines outlined in Appendix \ref{sec:annotation_guidelines}. 

The annotation guidelines (Appendix \ref{sec:annotation_guidelines}) were refined during initial weekly meetings between language coordinators and annotators. From these guidelines, we emphasize key aspects of entity selection for annotation. We annotated traditional named entities, extending this to also include eponym-derived entities (e.g., \emph{Putin supporters}) and toponym-derived entities (e.g., \emph{Western countries}, as illustrated in Figure~\ref{fig:annotated_example_text2}). Additionally, we focused on annotating entities that are central to the narrative conveyed by the article. For example, in Figure~\ref{fig:annotated_example_text2}, entities such as \emph{Ulan-Ude Aviation Plant} and \emph{Rossiya 24 TV} were not annotated because they were not considered central to the narrative. For a detailed explanation of how centrality was defined, refer to the guidelines in Appendix~\ref{sec:annotation_guidelines}.

\input{"eda/corpus_stats.tex"}

\begin{table}[!h]
\small
    \centering
\begin{tabular}{ccccccc}
\toprule
Lang. & EN & RU & BG & PT & HI & All\\
\hline
$\alpha$ & .460 & .436 & .733 & .467 & .461 & .558\\
\bottomrule
\end{tabular}
    \caption{Inter-annotator agreement computed using Krippendorff's $\alpha$.}
    \label{tab:iaa}
\end{table}

\begin{table}[!h]
    \small
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lllllll}
        \toprule
        \textbf{Freq.} & \textbf{1} & \textbf{2-5} & \textbf{5-10} & \textbf{10-20} & \textbf{20-50} & \textbf{50-500} \\
        \midrule
        count (\%) & 1513 (74) & 374 (18) & 76 (4) & 46 (2) & 22 (1) & 14 (1) \\
\bottomrule
\end{tabular}
}
\caption{Proportion of entities of a given frequency in the corpus.}
    \label{tab:proportion}
\end{table}


% \jp{to converge abd} Abd?



\subsection{Inter-Annotator Agreement}

To assess the inter-annotator agreement (IAA), we used Krippendorff's alpha. We compared the annotations at the span level: they were approximately matched if they shared at least 50\% of their length, to account for minor differences.
The IAA values are shown in Table~\ref{tab:iaa}. The results indicate a moderate agreement (above 0.45), which we consider acceptable due to the span-based nature of the task. We can see that the IAA is similar across the languages, except for Bulgarian, for which it is notably higher (0.73), which can be explained by the low count of entities in Table~\ref{tab:corpus_stats}.  %The overall IAA is a little below the recommended value of 0.667. Nevertheless, one needs to take into account, first, that given the complex annotation scheme, these values are not outside the range for tasks of similar complexity~\cite{}\jp{reference?}, second, that IAA reflects agreement at the level of annotation, therefore not accounting for the quality increase\jp{improvement?} after to the curation step.\jp{Do we say anything about entity mentions annotated only by single annotators? What fraction of the annotations do they cover? This would be interesting to know.}
%\nn{Indeed, but in EN at least it was a large portion and it could be quite tough to defend.}
\



%Please note that because the specific entities roles depend on a context which is not captured by the span, it is not possible to evaluate agreement per entity, neither to perform a cross-lingual check of coherency.\jp{but we know the context. Hence this sentence doe not read well :-)}
%\nn{I agree, it sounds like, we did not do a good job making a tool that could highlight that. I would remove the paragraph altoghther.}
\subsection{Corpus Analysis}

\subsubsection{Statistics}

Table \ref{tab:corpus_stats} provides overall statistics about the corpus, including a breakdown per language, the number of annotated entity mentions, the number of unique entities, as well as the number of annotations. Figure~\ref{fig:fine_roles_histogram} displays the distribution of the main and fine-grained roles in the corpus. We can see that there is a significant imbalance between the fine-grained roles, while the distribution of the main roles is relatively balanced. Notably, within the \emph{innocent} category, the majority of the instances, $83.6\%$, are labeled as \emph{victim}, with fewer occurrences of \emph{exploited}, \emph{forgotten}, and \emph{scapegoat} roles. More details about the proportions of main roles and fine-grained roles across different languages can be found in Figure~\ref{fig:proportions_of_roles} in Appendix~\ref{sec:appendix_stats}. 

% Table \ref{"eda/corpus_domain_distribution} outlines the distribution of our corpus across both domains CC and URW.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/fine_roles_histogram.pdf}
    \caption{Distribution of fine-grained roles color-coded according to the main role. For the fine-grained roles, the percentages inside the bars indicate the proportion of each fine-grained role relative to its corresponding main role category. The counts on top of the bars show the total occurrences of each fine-grained role. In the mini histogram, the percentages inside the bars reflect the distribution of the main roles, with the counts displayed above the bars. }
    \label{fig:fine_roles_histogram}
\end{figure*}

Table~\ref{tab:proportion} presents %\jp{maybe a graphical representation of the table would make more sense}
the number and the proportion of entities within a given frequency range considering exact string matching. We can see that 74\% of the entities were annotated only once, while 14 entities have been annotated more than 50 times
%\jp{Could we list them here, give examples, maybe in a separate table. We have space for that!}. \dd{I agree with listing at least some interesting ones. Like those with a lot of mentions}
These numbers consider only the surface string, not accounting for different grammatical and name variants and different languages. In Appendix~\ref{sec:appendix_stats}, we matched the most frequent entities accounting for these differences and presented detailed statistics at the level of entities. 




\subsubsection{Co-occurrence of Roles}

In our definition of entity framing, entity mentions can be assigned one or more fine-grained roles. Our corpus contains on average 1.12 annotations per entity mention. Out of the 1,378 articles, 353 articles contain 638 instances where at least one entity mention has multiple annotations. For this set of articles, the average and the maximum are 2.1 and 3 annotations per entity mention, respectively. We further observe that roles such as \emph{peacemaker} frequently accompany \emph{guardian}. Similarly, entities portrayed as \emph{scapegoats} are often framed as being \emph{exploited}. More details about the co-occurrence matrix for entity mentions with multiple annotations are shown in Figures \ref{fig:frequent_roles_combined} and \ref{fig:fine_roles_co_occurence_normalized} in Appendix~\ref{sec:appendix_stats}.
% Table \ref{tab:multiple_annotation_stats} shows per-language statistics for the  instances.

% \begin{table}
%     \small
%     \centering
%     \resizebox{0.5\columnwidth}{!}{%
% \begin{tabular}{l|rrc}
% \toprule
%  Lang. & \#ENT & AVG_a & MAX_a \\
% \midrule
% BG & 71 & 2.21 & 3 \\
% EN & 68 & 2.00 & 2 \\
% HI & 404 & 2.04 & 3 \\
% PT & 70 & 2.04 & 3 \\
% RU & 25 & 2.04 & 3 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Statistics for entity mentions containing 2 or more annotations. 

% % (\#ENT) is the number of such instances, while (AVG\textsubscript{a}) and (MAX\textsubscript{a}) are computed on the number of annotations.
% }
%     \label{tab:multiple_annotation_stats}
% \end{table}



\section{Experiments}
We experimented with classifying entities into main roles and fine-grained roles. As we performed the entity framing annotations at the span level, we framed the problem as a multi-class multi-label classification task. Given an article, an entity mention, and the span offsets, the goal was to predict the framing of that entity mention. We provide two sets of baselines and experiments to benchmark state-of-the-art models, as well as to assess the complexity of the entity framing task. The first set evaluates fine-tuning multilingual transformer models in various settings, while the second set explores hierarchical zero-shot learning using LLMs. %We investigate the impact of multilingual data on the classification of entity framing and role portrayal.
% \jp{complexity of the task, not dataset}\jp{exploiting}\jp{performance} Agreed

\subsection{Fine-Tuning Pre-trained Multilingual Transformers}
For the first set of experiments, we designed our experiments to address the following aspects:
\begin{itemize}
    \item \textbf{Granularity of context}--predicting role labels for entity mentions using the full document or narrowing the model's focus to only look at the pertinent paragraph or sentence containing the entity of interest.
    \item \textbf{Multilingual} comparison of the performance in the monolingual setting versus the multilingual setting trained on data in all five languages: Bulgarian, English, Hindi, European Portuguese, and Russian.    
\end{itemize}

For both granularity-level classification and multilingual performance, we made predictions at two levels: main role (3 labels) and fine-grained role (22 labels). We fine-tuned the multilingual pre-trained transformer XLM-R \cite{conneau2020unsupervisedcrosslingualrepresentationlearning} and adapted its final layers for our tasks, applying $softmax$ for multiclass classification and $sigmoid$ for multi-label classification. To handle spans within potentially long documents, we addressed the 512-token limitation of XLM-R by narrowing the context to the paragraph or the sentence where the entity appeared. We constructed the input text using the following format:

\texttt{input = entity mention + [SEP] + title + [SEP] + context}.

In this setup, \texttt{[SEP]} is the separator token, and the context can vary based on the granularity level, ranging from the full text to just the paragraph or sentence containing the entity mention. We placed the entity mention first, followed by the title and context, to maintain consistent positional encodings for the entity mention across different inputs. We used Stanza \cite{qi2020stanza} for sentence splitting for all languages.

% For both multilingual performance and granularity-level classification, we classify entities at the main role level (3 labels) and fine-grained role level (22 labels). We used the multilingual pre-trained transformer XLM-R \cite{conneau2020unsupervisedcrosslingualrepresentationlearning} and adapted the final layers for our classification tasks (using softmax for multiclass and sigmoid for multilabel classification). Given the need to classify spans within potentially long documents, we addressed the 512-token limitation of transformer models by limiting the field of view to the specific context in which the entity appears at both paragraph and sentence levels. We process the input text by first including the entity mention, followed by the title of the article and the context with the separator token in between as shown here: \texttt{input = entity mention + [SEP] + title + [SEP] + context}, where context depends on the granularity we train on ranging from the full text to just the paragraph or sentence containing the entity mention of interest. The reason we arrange the entity mention followed by the title and the context in this order is to ensure the associated positional encodings for the entity mention do not vary across different inputs.



To further support span-level multi-label classification, we modified the output layer to include a $sigmoid$ activation and optimized the model using \emph{Binary Cross-Entropy loss}. This setup allowed the model to predict multiple overlapping roles for a given entity span. See Appendix~\ref{sec:appendix_experiments} for more details.




\subsection{Hierarchical Zero-Shot Learning with LLMs}

We experimented with two prompting approaches: \emph{single-step} and \emph{hierarchical multi-step}. The former aimed to predict both the main role and the fine-grained role simultaneously within a single prompt. It assumed that both tasks can be handled together, relying on the model's ability to process them in one go. On the other hand, the multi-step approach separated the prediction into two distinct stages. First, the main role was predicted, and based on that output, the fine-grained role was predicted in a second step, using the information from the initial prediction to refine the second task. This stepwise process involved an intermediate prediction, which allowed the model to focus on each task individually. See Appendix~\ref{sec:appendix_experiments}, and~\ref{sec:appendix_zeroshot} for more details on experimental settings and prompt structure.

\input{results/xlmr_tables}

\subsection{Results}

We report standard evaluation metrics, including \emph{micro-average precision}, \emph{recall}, and $F1$ score, along with the \emph{macro-average} $F1$ score for fine-grained roles to address class imbalance. We further provide \emph{accuracy} and \emph{balanced accuracy} for predictions on the main role granularity.

Table~\ref{tab:xlmr_granularity_results} shows the performance of XLM-R across different context granularities (document, paragraph, and sentence) and training configurations (main roles vs. fine-grained roles). For models trained and evaluated on main roles, paragraph-level contexts perform best, followed closely by sentence-level contexts, while document-level contexts perform the worst.  When models trained on fine-grained roles are evaluated on main roles, paragraph-level contexts again yield the best performance, with document and sentence-level contexts slightly behind. This indicates that training on fine-grained roles provides an advantage. For models trained and evaluated on fine-grained roles, sentence-level contexts perform best, followed by paragraph-level contexts, with document-level contexts showing the weakest performance. These results highlight that context granularity significantly impacts performance, with localized contexts outperforming document-level contexts for both main role and fine-grained role classification tasks.

Table~\ref{tab:xlmr_language_results} offers interesting insights into the performance of XLM-R when fine-tuned on monolingual and multilingual data for multi-label fine-grained role classification at the paragraph level. The monolingual setting exhibits varying performance across languages, with the highest scores achieved for Portuguese, while English, Bulgarian, and Russian show notably lower performance; the model's performance on Hindi is moderate. These differences stem from the quantity and quality of training data, linguistic variations, and the complexity of entity mentions across languages. The consistently low Macro F1 scores across all languages indicate difficulty in predicting rare roles. In contrast, the multilingual setting consistently outperforms the monolingual setting, demonstrating its ability to better capture diverse fine-grained roles through cross-lingual transfer.



% \jp{There is a missing discussion of the results in Table 4 and 5, which are not referred to in the text.} Added

\begin{table*}[t]
\small
\centering
% Subtable (a)
%\begin{subtable}[h]{\columnwidth}
\centering
\resizebox{0.87\textwidth}{!}{%

\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Lang.}} & \multicolumn{2}{c}{\textbf{Main Role}} & \multicolumn{4}{c}{\textbf{Fine Grained Role}} & \multirow{2}{*}{\textbf{Cost (USD)}} \\
\cmidrule(lr){3-4}
\cmidrule(lr){5-8}
 & &  \textbf{Accuracy} & \textbf{Balanced Accuracy} & \textbf{P} & \textbf{R} & \textbf{Micro F1} & \textbf{Macro F1} &  \\
\midrule
\multirow{5}{*}{\shortstack{\textbf{Single-Step} \\ \textbf{LLM Prompting}}}
& EN & .8346 & .6756 & .2692 & .4632 & .3405 & .2171 & 0.7989 \\
& BG & .8065 & .7380 & .3725 & .5588 & .4471 & .3481 & 0.2751 \\
& HI & .6327 & .6247 & .2753 & .4000 & .3262 & .2196 & 2.4696 \\
& PT & .7812 & .7455 & .5167 & .6643 & .5813 & .2891 & 1.0200 \\
& RU & .7558 & .6719 & .3939 & .5843 & .4706 & .4644 & 0.7587 \\
& All & .7030 & .6957 & \underline{.3211} & \underline{.4726} & \underline{.3824} & \underline{\textbf{.3103}} & 5.3223 \\
\midrule
\multirow{5}{*}{\shortstack{\textbf{Multi-Step} \\ \textbf{LLM Prompting}}} 
& EN & .8031 & .6799 & .2887 & .4118 & .3394 & .2383 & 0.5130 \\
& BG & .8031 & .6799 & .4318 & .5588 & .4872 & .3601 & 0.5130 \\
& HI & .6367 & .6284 & .2676 & .2868 & .2769 & .1771 & 1.4581 \\
& PT & .8125 & .7882 & .3895 & .2643 & .3149 & .2498 & 0.5634 \\
& RU & .7442 & .6680 & .4118 & .4719 & .4398 & .3774 & 0.4769 \\
& All & \underline{.7053} & \underline{.7017} & .3051 & .3294 & .3168 & .2765 & \underline{\textbf{3.1852}} \\
\midrule \midrule
\multirow{5}{*}{\textbf{XLM-R}} 
& EN  & 0.6889 & 0.5276 & .1854 & .2828 & .2240 & .1327 & --\\
& BG  & 0.7333 & 0.5791 & .3030 & .3030 & .3030 & .1349 & --\\
& HI  & 0.7025 & 0.7046 & .3234 & .4951 & .3912 & .2043 & --\\
& PT  & 0.8957 & 0.8840 & .6259 & .7480 & .6815 & .2040 & --\\
& RU  & 0.8000 & 0.7604 & .4831 & .4886 & .4859 & .2364 & --\\
& All & \textbf{0.7529} & \textbf{0.7553} & \textbf{.3649} & \textbf{.4985} & \textbf{.4213} & .2392 & --\\
\bottomrule
\end{tabular}
}
\caption{Consolidated results comparing fine-tuning XLM-R and zero-shot learning with GPT-4o. The table shows performance and cost comparisons between single-step and multi-step LLM prompting approaches, where the highest scores between these two approaches across all languages are \underline{underlined}. The top results across all three methods and languages are highlighted in \textbf{bold}.}
\label{tab:promptingmethods}
\end{table*}

Table \ref{tab:promptingmethods} consolidates the results from fine-tuning XLM-R and hierarchical zero-shot learning, showing that the multi-step approach achieves slightly better performance on main role prediction compared to the single-step approach. 
% \jp{it can be inferred from the text earlier, but is not clear that this is the paragrpah version of the task. please mention it explicitly} 
This performance improvement in multi-step prompting can be attributed to the structured decomposition of the task. By explicitly isolating the main role prediction into a dedicated step, the model can focus on high-level role identification without the distraction of fine-grained details.
While the multi-step approach enhances performance for main role prediction, it underperforms compared to the single-step approach in predicting fine-grained roles. We hypothesize two potential reasons for this discrepancy:
\begin{enumerate}
\item \textbf{Error Propagation:} In the multi-step approach, the model first predicts the main role and then proceeds to predict fine-grained roles. Errors introduced during the main role prediction step can propagate to subsequent steps, thereby reducing the overall accuracy of fine-grained predictions.
\item \textbf{Loss of Joint Context:} The single-step approach enables the model to reason jointly about both main roles and fine-grained roles, allowing it to better capture dependencies between role labels. This integrated reasoning leads to more precise and consistent fine-grained predictions.
Another finding is that the multi-step approach is significantly more cost-effective than the single-step approach. This efficiency may stem from token efficiency, as multi-step prompts are designed to be more concise, with each step addressing a specific sub-task (e.g., main role followed by fine-grained role). Consequently, this results in fewer tokens per prompt.
\end{enumerate}

Table~\ref{tab:promptingmethods} additionally compares the performance of zero-shot approaches and XLM-R, both using similar-sized input contexts. We can see that XLM-R outperforms zero-shot methods on all evaluation measures except for Macro F1, where it shows the lowest performance among all approaches. We hypothesize that this discrepancy arises because XLM-R had limited training instances for rare roles, preventing effective learning for these categories. In contrast, zero-shot approaches do not rely on training data and thus are not constrained by this limitation.


% Streamlined Task Execution: While the Multi-Step approach involves multiple queries, the focused nature of each step reduces redundancy and unnecessary context. In contrast, Single-Step prompting processes all role predictions jointly, which can lead to more verbose and costly outputs.



% \jp{again something missing on XLM performance vis-a-vis LLMs} Done

\section{Conclusion and Future Work}
We presented a novel multilingual and hierarchical dataset for characterizing entity framing and role portrayal in news articles. Our dataset introduces a unique taxonomy inspired by storytelling elements, featuring 22 fine-grained archetypes nested within three main categories: \emph{protagonist}, \emph{antagonist}, and \emph{innocent}. The dataset covers 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian), spanning two globally significant domains: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been thoroughly annotated with role labels, capturing nuanced portrayals. We evaluated the dataset using fine-tuned state-of-the-art multilingual transformer models and explored hierarchical zero-shot learning with LLMs at document, paragraph, and sentence level. Our experiments highlight the potential of multilingual representations and hierarchical approaches for entity-framing tasks. We intend to release the dataset to the community freely for research purposes. We hope that this dataset will serve as a valuable resource for developing methods and tools to enhance the analysis of entity portrayals in news media. 

In future work, we plan to extend the annotations to additional languages and explore other sources of text, such as social media posts. This expansion aims to provide a broader understanding of role portrayal across diverse linguistic and contextual settings. We also intend to perform a more in-depth dataset analysis to examine formulations such as central entity identification. We will also consider evaluating the potential biases in the dataset to ensure robustness in downstream tasks and end-user applications.

\section{Limitations}

\paragraph{Corpus} Our dataset focuses on two domains: the Ukraine-Russia War and Climate Change, and covers news articles in five languages (Bulgarian, English, Hindi, Portuguese, and Russian). While these domains and languages provide a diverse foundation for entity framing analysis, the corpus contains 1,378 articles and it should not be considered representative of all news coverage or media landscapes in any specific country. Additionally, the dataset is not perfectly balanced with respect to topics, entities, or languages. Moreover, human annotation of entity framing and role portrayal inevitably is subjective and annotators may subconsciously have biases that could influence the quality. Despite providing detailed annotation guidelines and conducting quality control measures such as double annotation and adjudication through the curation process, some level of subjectivity may remain in the dataset. 

\paragraph{Baseline Models}
Our reported experiments utilize state-of-the-art baselines covering a range of fine-tuned multilingual transformer models and hierarchical zero-shot learning with LLMs. However, we have not yet explored alternative architectures or advanced techniques such as few-shot, instruction-based evaluation, or multitask learning. Future work could investigate these approaches to improve model efficiency and performance. 

Additionally, our zero-shot learning experiments rely on OpenAI's GPT-4o, a closed-source model that is subject to changes over time and may be deprecated in the future. This dependency may impact the reproducibility and interpretability. To address these challenges, future research should prioritize improving open-source models to ensure greater accessibility, transparency, and reproducibility.





\section{Ethics and Broader Impact}
% Authors are encouraged to devote a section of their paper to concerns about the ethical impact of the work and to a discussion of broader impacts of the work, which will be taken into account in the review process. This discussion may extend into a 5th page (short papers) or 9th page (long papers). https://www.acm.org/code-of-ethics
%In addition, we provide a responsible NLP research checklist, which authors must complete as part of their paper submission. https://aclrollingreview.org/responsibleNLPresearch/
\paragraph{Biases}
Our dataset aims to capture a balanced range of perspectives on the Ukraine-Russia War and Climate Change, covering five languages: Bulgarian, English, Hindi, European Portuguese, and Russian. While our goal is to incorporate diverse news sources and viewpoints, achieving perfect balance is not always possible. Consequently, inherent biases in the original media sources may be present in the annotations. To reduce unwanted annotation biases, the corpus is annotated with clear instructions to annotators to focus strictly on the framing of entities, setting aside their personal opinions. All annotations are performed by subject-matter experts, and we did not use crowd-sourcing.

\paragraph{Intended Use and Misuse Potential}
The primary goal of this corpus is to facilitate research on entity framing, role portrayal, and media analysis. These tools can help researchers, journalists, and the general public identify framing patterns and biases in news content. However, there is a risk that the corpus could be misused for malicious purposes, such as manipulating news narratives. We urge users to employ this resource responsibly and remain aware of potential ethical risks associated with its misuse.

\paragraph{Environmental Impact}
The use of LLMs requires substantial computational power, contributing to carbon emissions. Even though we used LLMs in a zero-shot in-context-learning setting rather than training models from scratch, the LLMs still rely on GPUs for inference, which has an environmental impact.

\paragraph{Fairness}

Most of our annotators and curators come from the institutions of the co-authors of this manuscript and were fairly paid as part of their job duties. Few annotators were experienced analysts with full-time consulting roles and rates set by their contracting institutions. A fraction 
of the annotators were students from the respective
academic organizations. For two languages, a professional annotation company was contracted on rates based on country of residence. At the same time, some of the remaining annotators were researchers working primarily as linguists and lexicographers at their institute of affiliation and were all compensated according to local standards and their employment contracts.  


%\section*{Acknowledgments}

%This work is (partially) supported by the  Project SERICS (PE00000014) under the NRRP MUR program funded by the EU – NGEU. 

% Bibliography entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom,shared}
\input{acl_latex.bbl}
% Custom bibliography entries only
%\bibliography{custom}
\clearpage
\appendix
\section{Detailed Taxonomy with Examples}
\label{sec:detailed_taxonomy}
\input{detailed_taxonomy}

\section{Annotation Guidelines}
\label{sec:annotation_guidelines}

\input{annotation_guidelines}

\section{Experimental Settings}
\label{sec:appendix_experiments}

All fine-tuning experiments were conducted on a single NVIDIA RTX 4090 GPU with 24 GB of memory. We fine-tuned XLM-R (XLM-RoBERTa) in a single run, using a fixed random seed to ensure reproducibility. When the input context was at the sentence granularity, we performed sentence splitting using Stanza pipelines for each one of our five languages. For XLM-R, default settings were applied, with the following configurations:

\begin{itemize}
    \item Model: XLM-R\textsubscript{base} (125M parameters) 
    \item Learning Rate: 2e-5
    \item Batch Size: 8
    \item Epochs: 20 (with early stopping of 3 based on validation loss)
    \item Random Seed: 42
    \item Weight Decay: 0.01
\end{itemize}

To optimize performance, the sigmoid thresholds for fine-grained role predictions were tuned on the validation set. These optimized thresholds were then applied to generate predictions on the test set.

To prevent data leakage, we created train/dev/test splits based on entire articles rather than individual entity-mention annotations. The details of these splits are provided in Table~\ref{tab:train_dev_test_split}.

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%

\begin{tabular}{lccccc|c}
\toprule
 & BG & EN & HI & PT & RU & All \\
\midrule
Train & 165 (389) & 133 (440) & 203 (1347) & 206 (833) & 89 (252) & 796 (3261) \\
Dev & 94 (237) & 69 (245) & 139 (983) & 100 (417) & 44 (114) & 446 (1996) \\
Test & 15 (30) & 27 (90) & 35 (279) & 31 (115) & 28 (85) & 136 (599) \\ \midrule
Total & 274 (656) & 229 (775) & 377 (2609) & 337 (1365) & 161 (451) & 1378 (5856) \\
\bottomrule
\end{tabular}
}
\caption{Distribution of articles and entity mentions by language and split. The number of entity mentions is shown in parentheses}
\label{tab:train_dev_test_split}
\end{table}

For the zero-shot experiments, we used OpenAI's GPT-4o (gpt-4o-2024-11-20) with a temperature setting of 0.2 to produce more conservative responses. To ensure the outputs conformed to our defined data types, we employed OpenAI's Structured Outputs API, which returned results in the expected JSON format.




\clearpage
\onecolumn

\section{Dataset Statistics}

\label{sec:appendix_stats}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=1\textwidth]{images/fine_role_co_occurence_normalized.pdf}
    \caption{Normalized co-occurrence of fine-grained roles.}
    \label{fig:fine_roles_co_occurence_normalized}
\end{figure*}

\begin{figure}[!h]  % 't' ensures the figure is placed at the top of the page
    \centering

    % First Subfigure
    \begin{subfigure}[t]{0.9\textwidth}  % Width for the first subfigure
        \centering
        \includegraphics[width=\textwidth]{images/frequent_pairs.pdf}
        \caption{}
        \label{fig:frequent_pairs}
    \end{subfigure}
    \hfill  % Horizontal space to push subfigures apart
    
    % Second Subfigure
    \begin{subfigure}[t]{\textwidth}  % Width for the second subfigure
        \centering
        \includegraphics[width=\textwidth]{images/frequent_triplets.pdf}
        \caption{}
        \label{fig:frequent_triplets}
    \end{subfigure}

    % Main Caption for the Entire Figure
    \caption{The 10 most frequent co-occurring (a) pairs and (b) triplets of fine-grained roles.}
    \label{fig:frequent_roles_combined}
\end{figure}

\begin{figure}  
    % \captionsetup{font=scriptsize}

    % First Subfigure
    \begin{subfigure}[b]{0.9\textwidth}  % Width for the first subfigure
        \centering
        % \captionsetup{font=scriptsize}
        \includegraphics[width=\textwidth]{images/proportion_of_main_roles_per_language.pdf}
        \caption{}
        \label{fig:main_roles_per_language}
    \end{subfigure}
    % \hfill  % Horizontal space to push subfigures apart
    
    % Second Subfigure
    \begin{subfigure}[b]{\textwidth}  % Width for the second subfigure
        \centering
        % \captionsetup{font=scriptsize}
        \includegraphics[width=\textwidth]{images/proportion_of_fine_roles_per_language.pdf}
        \caption{}
        \label{fig:fine_roles_per_language}
    \end{subfigure}

    % Main Caption for the Entire Figure
    \caption{Proportions of (a) main roles and (b) fine-grained roles per language.}
    \label{fig:proportions_of_roles}
\end{figure}




% \begin{figure}[htbp]
%     \centering

%     % First Row: English and Portuguese
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/word_cloud_EN.pdf}
%         \caption{}
%         \label{fig:wordcloud_en}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/word_cloud_PT.pdf}
%         \caption{}
%         \label{fig:wordcloud_pt}
%     \end{subfigure}

%     \vspace{0.5cm}

%     % Second Row: Bulgarian and Russian
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/word_cloud_BG.pdf}
%         \caption{}
%         \label{fig:wordcloud_bg}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/word_cloud_RU.pdf}
%         \caption{}
%         \label{fig:wordcloud_ru}
%     \end{subfigure}

%     % \vspace{0.5cm}

%     % % Third Row: Hindi (centered)
%     % \begin{subfigure}[b]{0.45\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{images/word_cloud_HI.pdf}
%     %     \caption{}
%     %     \label{fig:wordcloud_hi}
%     % \end{subfigure}

%     % Main Caption for the Entire Figure
%     \caption{Word clouds for annotated entity mentions in different languages: (a) English, (b) Portuguese, (c) Bulgarian, and (d) Russian.}
%     \label{fig:wordclouds_all_languages}
% \end{figure}



\begin{figure}
    \centering
    \includegraphics[scale=0.75]{images/hist_ent.png}
    \caption{Top entities counts after multilingual linking was manually performed to link surface string to unique identifiers. The entities selected are all the ones for which at least one surface string has a count of a least 10 in any language}
    \label{fig:hist_ent}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=1.5]{images/heathmap_ent_2nd.png}
    \caption{Heathmap of the raw count of mention of entity and 2nd level role, where entities are defined and selected as in Figure~\ref{fig:hist_ent}}
    \label{fig:entity_2nd}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.22]{images/graph_topent_large.png}
    \caption{Graph of the top entities, nodes are a pair of entity and 2nd level role, node size is relative to the count of mentions, node color codes the 1st level role, there is and edge between two nodes, if they appear in the same document. This graph illustrate how group of entity+role pairs can be used to identify potential narratives.}
    \label{fig:enter-label}
\end{figure}



\clearpage
\section{Prompts for Hierarchical Zero-Shot Experiments}
\label{sec:appendix_zeroshot}


\begin{figure*}[htbp]
\centering
\begin{framed}
\begin{minipage}{0.9\textwidth}
\begin{Verbatim}[fontsize=\scriptsize,commandchars=\\\{\}]

\lightbrowntext{You are an expert at identifying entity framing and role portrayal in news articles. Analyze the following entity} 
\lightbrowntext{mention in context, and predict its main role and fine-grained role(s) from the taxonomy below.} 

\lightbrowntext{Taxonomy:} \darkbluetext{\{}\lightbluetext{detailed taxonomy with definitions and examples}\darkbluetext{\}}

\lightbrowntext{Context Around Entity:} \darkbluetext{\{}\lightbluetext{context}\darkbluetext{\}}

\lightbrowntext{Entity Mention:} \darkbluetext{\{}\lightbluetext{entity mention}\darkbluetext{\}}

\lightbrowntext{Task: Based on the provided context, assign to the entity mention at least one fine-grained role and}
\lightbrowntext{exactly one main role.}

\lightbrowntext{Return a JSON that has below attributes:}
\lightbrowntext{- \textbf{main role}: either one of Protagonist, Antagonist, or Innocent}
\lightbrowntext{- \textbf{fine grained roles}: a list of all your predicted fine-grained roles}

\end{Verbatim}
\end{minipage}
\end{framed}
\caption{\textbf{Single-Step Prompt Template.} The detailed taxonomy is the same one shown in \ref{sec:detailed_taxonomy}. The context is the text consisting of entity mention along with the 20 words before and after the entity mention.}
\label{fig:single_step_prompt_template}
\end{figure*}

\begin{figure*}[htbp]
\centering
\begin{framed}
\begin{minipage}{0.9\textwidth}
\begin{Verbatim}[fontsize=\scriptsize,commandchars=\\\{\}]
\darkbluetext{First Step (LLM Call 1): Predict the Main Role}

\lightbrowntext{You are an expert at identifying entity framing and role portrayal in news articles. Analyze the following entity }
\lightbrowntext{mention in context, and predict its main role from the taxonomy below.} 

\lightbrowntext{Taxonomy:} \darkbluetext{\{}\lightbluetext{list of fine-grained roles per main role}\darkbluetext{\}}

\lightbrowntext{Context Around Entity:} \darkbluetext{\{}\lightbluetext{context}\darkbluetext{\}}

\lightbrowntext{Entity Mention:} \darkbluetext{\{}\lightbluetext{entity mention}\darkbluetext{\}}

\lightbrowntext{Task: Based on the provided context, assign to the entity mention exactly one main role.}

\lightbrowntext{Return a JSON that has this attribute:}
\lightbrowntext{- \textbf{main role}: either one of Protagonist, Antagonist, or Innocent}


\darkbluetext{Second Step (LLM Call 2): Predict the Fine-Grained Role}

\lightbrowntext{You are an expert at identifying entity framing and role portrayal in news articles. This entity is}
\lightbrowntext{portrayed as a(n)} \darkbluetext{\{}\lightbluetext{main role}\darkbluetext{\}}\lightbrowntext{ and your task is to analyze the entity mention in context}
\lightbrowntext{and predict its fine-grained role(s) from the taxonomy below.}

\lightbrowntext{Taxonomy:} \darkbluetext{\{}\lightbluetext{pertinent portion of the detailed taxonomy with definitions and examples}\darkbluetext{\}}

\lightbrowntext{Context Around Entity:} \darkbluetext{\{}\lightbluetext{context}\darkbluetext{\}}

\lightbrowntext{Entity Mention:} \darkbluetext{\{}\lightbluetext{entity mention}\darkbluetext{\}}

\lightbrowntext{Task: Based on the provided context, assign to the entity mention at least one fine-grained role.}

\lightbrowntext{Return a JSON that has this attribute:}
\lightbrowntext{- \textbf{fine grained roles}: a list of all your predicted fine-grained roles}
\end{Verbatim}
\end{minipage}
\end{framed}
\caption{\textbf{Multi-Step Prompt Template.} In the first step, the taxonomy is only the tree structure of the taxonomy and does not include any definitions or examples. In the second step, the detailed taxonomy only includes the branch under the predicted main role in the first step. The context is as defined in Figure \ref{fig:single_step_prompt_template}. }
\label{fig:multi_step_prompt_template}
\end{figure*}


% \fbox{%
%     \parbox{\textwidth}{%
% \bf{\textcolor{red}{Single Step Prompt}}
% \\

%  Taxonomy Definitions: \\
% Main Roles: \{Main Roles Definitions\} \\
% Fine-Grained Roles: \{Fine Grained Roles Definitions\} \\
% You are an expert at identifying entity framing and role portrayal in news articles. Analyze the following entity mention in context, and predict its main role and fine-grained role(s) from the taxonomy below. \\
% \{taxonomy section\} \\
% \{document section\} \\
% Context Around Entity:
% \{context\} \\
% Entity Mention:
% \{entity mention\} \\
% Task: Based on the provided context, assign to the entity mention at least one fine-grained role and exactly one main role. Return a JSON that has below attributes: \\
% ``main role'': either one of Protagonist, Antagonist, or Innocent \\
% ``fine grained roles'': this is a list of all of your predicted fine-grained roles \\
% ... \\

%     }%
% }

% \fbox{%
%     \parbox{\textwidth}{%
% \bf{\textcolor{red}{Multi Step Prompts}} 
% \\

% \textcolor{blue}{First Step (LLM call): Define the main role}

%  Taxonomy Definitions: \\
% Main Roles: \{Main Roles Definitions\} \\
% You are an expert at identifying entity framing and role portrayal in news articles. Analyze the following entity mention in context, and predict its main role from the taxonomy below.\\
% \{taxonomy section\} \\
% \{document section\} \\
% Context Around Entity:
% \{context\} \\
% Entity Mention: \\
% \{entity mention\} \\
% Task:
% Based on the provided context, assign to the entity mention exactly one main role. Return a JSON that has below attributes: \\
% ``main role'': either one of Protagonist, Antagonist, or Innocent \\
% ... \\
% \textcolor{blue}{Second step (LLM call): Define the fine grained role (protagonist/antagonist/innocent)}

% You are an expert at identifying entity framing and role portrayal in news articles. This entity is portrayed as a protagonist/antagonist/innocent and your task is to analyze the following entity mention in context, and predict its role from the taxonomy below.\\
% \{taxonomy section\} \\
% \{document section\} \\
% Context Around Entity:
% \{context\} \\
% Entity Mention: \\
% \{entity mention\} \\
% Task:
% Based on the provided context, assign to the entity mention at least one fine-grained role. Return a JSON that has below attributes: \\
% "fine grained roles": this is a list of all of your predicted protagonist/antagonist/innocent fine-grained roles
%     }%
% }

%\clearpage
%\section{Top Entities}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale=0.3]{images/st1_language-1st_normX.png}
%     \caption{language-1st role, normalised over language}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale=0.3]{images/matrix_entity-language_normY.png}
%     \caption{top entity - language, normalised over language}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale=0.3]{images/matrix_entity-language_normX.png}
%     \caption{top entities - language, normalised over entities}
%     \label{fig:enter-label}
% \end{figure}

\clearpage
\section{Annotation Tool}
\label{sec:annotation_tool}
We used the Inception~\cite{tubiblio106270} platform\footnote{https://inception-project.github.io/} to annotate our corpus because it has a rich set of features that extends beyond mere annotation to also include useful tools such as the ability to perform annotation adjudication through curation, monitoring the annotation progress, and calculating agreement between annotators. Inception allows to assign the following roles to users: annotator, curator, and manager.

To annotate a mention of an entity with a role, annotators should go to the part of the article where the entity is mentioned and select it. After selecting an entity mention, annotators can then assign roles as shown in \ref{fig:INC_entity}.


\begin{figure}[!htpb]
\centering
\includegraphics[width=1\textwidth]{images/Entity_layer.PNG}
\caption{Annotating entity framing using Inception.}
\label{fig:INC_entity}
\end{figure}






\end{document}



