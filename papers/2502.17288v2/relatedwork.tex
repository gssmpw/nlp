\section{Related Work}
\subsection{Occupancy Estimation}
The 3D semantic occupancy estimation task has become an important area in the autonomous driving research community in recent years.
Numerous voxel-based occupancy benchmarks have been introduced for datasets such as SemanticKITTI~\cite{behley2019semantickitti} and nuScenes~\cite{tian2023occ3d, wang2023openoccupancy}.
Early works in 3D occupancy estimation utilize established techniques from Birds-Eye-View (BEV) perception and object detection \cite{huang2021bevdet,li2022bevformer} to lift multi-view camera images into a unified 3D voxel grid~\cite{huang2023tri, huang2022bevdet4d, tong2023scene, cao2022monoscene, li2023voxformer}.
Subsequent approaches have improved model efficiency \cite{yu2023flashocc, wang2024opus, lu2023octreeocc, liu2024fully, shi2025occupancy, tang2024sparseocc, huang2024gaussianformer}, optimized the training procedure and label efficiency \cite{pan2023renderocc, boeder2024occflownet, gan2023simple, hayler2024s4c, sun2024gsrender}, and improved occupancy estimation performance through architectural innovations \cite{li2023fb, zhang2023occformer, jiang2023symphonize, tan2024geocc, Zhao_2024_CVPR, ma2024cotr, ma2024cam4docc}.

Despite these advancements, the majority of existing 3D occupancy estimation methods require costly voxel-based 3D ground truth labels for training.
Therefore, a parallel line of research has emerged that focuses on self- and weakly supervised learning for occupancy estimation models, leveraging only 2D labels.
SelfOcc~\cite{huang2023selfocc} and OccNeRF~\cite{zhang2023occnerf} employ volume rendering inspired by NeRF~\cite{mildenhall2021nerf} to render estimated 3D occupancy back to the 2D image space. 
Spatial and temporal photometric consistency losses can be used to train the geometry estimation, while semantic information is incorporated using pretrained vision foundation models like OpenSeeD~\cite{zhang2023simple} or GroundedSAM~\cite{ren2024grounded}.
GaussianOcc~\cite{gan2024gaussianocc} replaces the volume rendering pipeline with 3D Gaussian Splatting to accelerate training, yet it continues to model the scene with dense occupancy grids, thus failing to exploit the benefits of a fully sparse Gaussian representation.
Also, while these methods eliminate the need for 3D labels, they fail to address scene dynamics, a critical aspect when relying on temporal consistency losses.

A growing body of research aims to align 3D occupancy with feature spaces of strong foundation models.
OccFeat~\cite{sirko2024occfeat} distills features of CLIP~\cite{radford2021learning} and DINO~\cite{caron2021emerging, oquab2023dinov2} into an occupancy representation for model pretraining.
POP-3D~\cite{vobecky2024pop}, LOcc~\cite{yu2024language} and OVO~\cite{tan2023ovo} perform open-vocabulary occupancy estimation by aligning voxel-based predictions with vision-language features extracted from pretrained encoders like MaskCLIP~\cite{zhou2022extract}.
VEON~\cite{zheng2025veon} and LangOcc~\cite{boeder2024langocc} follow up on the self-supervised methods and directly use volume rendering of vision-language features to train open-vocabulary models.

\subsection{Differentiable Rendering and 3D Gaussian Splatting}
Differentiable rendering has emerged as a powerful technique for learning 3D scene representations by projecting them into 2D views, followed by an optimization based on photometric or semantic consistency.
Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf} have been particularly influential, modeling scenes as volumetric representations that encode radiance and density, enabling novel view synthesis through differentiable volume rendering.
Recently, 3D Gaussian Splatting (GS)~\cite{kerbl20233d} has introduced a novel paradigm for 3D scene reconstruction by representing scenes as a collection of 3D Gaussians.
This approach significantly reduces computational overhead while preserving expressive scene modeling, making it highly efficient for tasks requiring dynamic or large-scale reconstructions.
NeRF and GS approaches were originally designed to reconstruct individual scenes for novel-view synthesis, with research focusing on improving efficiency~\cite{muller2022instant, chen2025mvsplat}, rendering quality~\cite{barron2021mip, barron2022mip} or feature enrichment~\cite{qin2024langsplat, kerr2023lerf, ye2023featurenerf, zhou2024feature}.
Several works have further explored the modeling of dynamic scenes for video reconstruction \cite{wu20244d, fridovich2023k, pumarola2021d}.
Another line of work incorporates different priors like depth, stereo-matching, or additional data like LiDAR, to train generalizable reconstruction models~\cite{xu2022point,chang2022rc,chen2021mvsnerf,yu2021pixelnerf, wimbauer2023behind, liu2025mvsgaussian, zheng2024gps}.
As mentioned above, similar ideas have been adapted to train occupancy estimation models.
Methods like OccNeRF~\cite{zhang2023occnerf}, SelfOcc~\cite{huang2023selfocc} and GaussianOcc~\cite{gan2024gaussianocc} employ volume rendering or GS for weakly supervised training.
However, they still rely on voxel grids for scene representation, which limits efficiency and scalability.
GSRender~\cite{sun2024gsrender} additionally introduces a ray compensation method to mitigate duplicate predictions along camera rays, a common issue among rendering-based approaches.
Lastly, the recent approach GaussTR~\cite{jiang2024gausstr} shares similarities with our method by adopting 3D Gaussians as the scene representation.
However, GaussTR uses multiple pretrained feature encoders (e.g., CLIP, Metric3D, SAM) during inference, making the pipeline computationally expensive.
Furthermore, GaussTR employs standard attention layers, which significantly limits the number of Gaussians it can handle.