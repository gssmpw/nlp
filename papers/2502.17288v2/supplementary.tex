\clearpage
\setcounter{page}{1}
\maketitlesupplementary
\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}
\setcounter{figure}{0}

\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{table}{0}

\section{Detailed Quantitative Results}
For completeness, we provide per-class IoU scores for all evaluated methods on the Occ3D-nuScenes dataset in \cref{table:main_complete}.
Our proposed GaussianFlowOcc consistently outperforms all prior methods trained with pseudo labels, both with and without depth supervision.
The most significant performance gains can be observed in small object classes, such as motorcycles and traffic cones, but also in categories.
Notably, our method achieves the best or second best score across nearly all categories. 
The only exception are the the LiDAR-based labels, where one method surpasses our performance by a small margin.
These results further highlight the effectiveness of our Gaussian-based scene representation and temporal modeling, particularly in handling fine-grained object details that are often challenging for voxel-based approaches.

\begin{table*}[!ht]
    \begin{center}
        \caption{
            \textbf{Occupancy estimation performance on the Occ3D-nuScenes validation set.}
            The \textit{Mode} indicates the source of the 2D labels.
            $C$ refers to camera images, $D$ refers to usage of pseudo depth.
            Best performing per column and \textit{Mode} section in \textbf{bold}, second best in \textit{italics}.
            Methods that use pseudo labels ignore the \textit{others} and \textit{other flat} classes.}
        \label{table:main_complete}
        
        \resizebox{\textwidth}{!}{
            \addtolength{\tabcolsep}{2pt}
            \begin{tabular}{ll|c|ccccccccccccccccc}
                \hline
                \noalign{\smallskip}
                 Method & \rotatebox{90}{Mode} & mIoU & \rotatebox{90}{others} & \rotatebox{90}{barrier} & \rotatebox{90}{bicycle} & \rotatebox{90}{bus} & \rotatebox{90}{car} & \rotatebox{90}{cons. vehicle} & \rotatebox{90}{motorcycle} & \rotatebox{90}{pedestrian} & \rotatebox{90}{traffic cone} & \rotatebox{90}{trailer} & \rotatebox{90}{truck} & \rotatebox{90}{driv. surf.} & \rotatebox{90}{other flat} & \rotatebox{90}{sidewalk} & \rotatebox{90}{terrain} & \rotatebox{90}{manmade} & \rotatebox{90}{vegetation}\\
                
                \noalign{\smallskip}
                \hline
                \noalign{\smallskip}
                
                SelfOcc \cite{huang2023selfocc} & C & 10.54 & - & 0.15 & 0.66 & 5.46 & \textit{12.54} & 0.00 & 0.80 & 2.10 & 0.00 & 0.00 & 8.25 & \textbf{55.49} & - & \textit{26.30} & \textit{26.54} & \textit{14.22} & 5.60 \\
                OccNeRF \cite{zhang2023occnerf} & C & 10.81 & - & 0.83 & 0.82 & 5.13 & 12.49 & \textit{3.50} & 0.23 & 3.10 & 1.84 & 0.52 & 3.90 & 52.62 & - & 20.81 & 24.75 & \textbf{18.45} & \textbf{13.19} \\
                GaussianOcc \cite{zhang2023occnerf} & C & \textit{11.26} & - & \textit{1.79} & \textit{5.82} & \textbf{14.58} & \textbf{13.55} & 1.30 & \textit{2.82} & \textbf{7.95} & \textbf{9.76} & \textit{0.56} & \textit{9.61} & 44.59 & - & 20.10 & 17.58 & 8.61 & \textit{10.29} \\
                
               \textit{GaussianFlow (Ours)} & C & \textbf{13.20} & - & \textbf{6.27} & \textbf{8.54} & \textit{13.36} & 12.38 & \textbf{4.92} & \textit{10.05} & \textit{6.84} & \textit{8.75} & \textbf{1.12} & \textbf{10.43} & \textit{53.40} & - & \textbf{26.44} & \textbf{28.89} & 10.39 & 9.33 \\
               
                \noalign{\smallskip}
                \hline
                \noalign{\smallskip}
                
                GaussTR \cite{jiang2024gausstr} & C+D & \textit{13.26} & - & \textit{2.09} & \textit{5.22} & \textit{14.07} & \textbf{20.43} & \textbf{5.70} & \textit{7.08} & \textit{5.12} & \textit{3.93} & \textit{0.92} & \textbf{13.36} & \textit{39.44} & - & \textit{15.68} & \textit{22.89} & \textbf{21.17} & \textbf{21.87} \\
                \textit{GaussianFlow (Ours)} & C+D & \textbf{16.02} & - & \textbf{7.23} & \textbf{9.33} & \textbf{17.55} & \textit{17.94} & \textit{4.50} & \textbf{9.32} & \textbf{8.51} & \textbf{10.66} & \textbf{2.0} & \textit{11.80} & \textbf{63.89} & - & \textbf{31.11} & \textbf{35.12} & \textit{14.64} & \textit{12.59} \\
                \noalign{\smallskip}
                \hline
            \end{tabular}
            \addtolength{\tabcolsep}{2pt}
        }
    \end{center}
\end{table*}

\section{Qualitative Results}

\subsection{Predicted Occupancy}
In \cref{fig:qualitative1}, we present qualitative examples of GaussianFlowOcc's predicted occupancy compared to the ground truth, visualized from a third-person perspective.
These examples demonstrate our model's ability to precisely reconstruct the 3D scene geometry using 3D Gaussians, despite the absence of explicit 3D supervision.
The Gaussians align naturally to capture the shapes and details of objects, effectively representing scene structures without requiring per-scene optimization.
This highlights the flexibility and efficiency of our approach in modeling complex environments.
Additionally, we provide a set of videos in our \href{https://github.com/boschresearch/GaussianFlowOcc}{repository} showcasing inference results on full validation scenes:
\begin{itemize}
    \item \textit{scene-0346.mp4} and \textit{scene-0790.mp4} visualize the predicted Gaussians from three perspectives: input camera view, third-person view, and Bird's-Eye View.
    \item \textit{scene-0521\_gt.mp4} and \textit{scene-0925\_gt.mp4} compare our model’s predictions to the ground truth occupancy, both from a third-person perspective.
\end{itemize}

These results clearly demonstrate the model’s ability to accurately represent flat surfaces (e.g., streets and walls), thin objects (e.g., poles), and small objects (e.g., traffic cones).
Unlike voxel-based methods, which are constrained by coarse resolution, our approach captures continuous geometric details with far greater fidelity, enabling a more precise and realistic 3D scene understanding.

\begin{figure*}
    \centering
    \includegraphics[page=4, trim=0cm 5.47cm 11.11cm 0cm, clip, width=1\textwidth]{figures/Figures.pdf}
    \caption{
        \textbf{Qualitative results on the Occ3D-nuScenes dataset.}
        We show the estimated 3D Gaussians, how the predictions look when voxelized, and the ground truth occupancy.
        Best viewed when zoomed in.
    }
    \label{fig:qualitative1}
\end{figure*}

\subsection{Rendered Depth and Semantics}
In \cref{fig:qual_render}, we present examples of rendered depth and semantics obtained by applying Gaussian Splatting to the 3D Gaussians estimated from our trained model.
For comparison, we also show the pseudo labels used during training, which are generated by GroundedSAM~\cite{ren2024grounded} for semantics and Metric3D~\cite{yin2023metric3d} for depth.
The rendered outputs demonstrate a high level of detail, with most objects accurately segmented.
Notably, even thin structures such as trees and pedestrians are well-preserved, showcasing the model's ability to preserve fine geometric details; something voxel-based approaches often struggle with due to grid resolution constraints.

In \cref{fig:qual_render_temporal}, we further illustrate the effect of our \emph{Temporal Gaussian Splatting} by comparing predicted semantic segmentation maps across consecutive frames:
The first row shows the rendered semantics for the current input frame.
The second row depicts the segmentation map generated by rendering into the next frame and using the 3D flow estimated by the \emph{Temporal Module} to compensate the object motion.
The third row shows the same next-frame projection but without dynamic object handling — simulating the approach taken by previous works.
The last row provides the pseudo-semantic labels of the next frame for comparison.
The comparison clearly reveals the importance of dynamic object modeling.
Without applying the 3D flow (Row 3), the rendered semantics fail to align with the dynamic objects in the next frame, as these objects have already moved.
This misalignment would introduce incorrect supervisory signals if used to compute the loss.
In contrast, when applying the \emph{Temporal Module}’s flow estimation (Row 2), the rendered semantics better match the pseudo labels of the next frame.
This demonstrates that our \emph{Temporal Module} has successfully learned to approximate object motion, improving the alignment of dynamic objects across frames and ensuring more accurate supervision during training.

\begin{figure*}[t]
  \centering
      \includegraphics[page=5, trim=0cm 4.89cm 7.29cm 0cm, clip, width=1\textwidth]{figures/Figures.pdf}
  \caption{
  \textbf{Qualitative results on the Occ3D-nuScenes validation set.}
  Each column shows an \textit{Input Image}, \textit{Rendered Depth} and \textit{Rendered Semantics} generated by using GS to render predicted Gaussians into input cameras (as is done during training).
  We additionally show the pseudo labels used during training for the relevant sample.}
\label{fig:qual_render}
\end{figure*}

\section{Voxelization}
\noindent
As explained in the main paper, for benchmarking and comparison, we convert the estimated 3D Gaussian distributions into a voxelized representation.
This process begins with defining a voxel grid over the scene.
Each Gaussian distribution is then evaluated at the center of every voxel, where its opacity and semantic logits are accumulated to determine the final voxel values.
The formulation for this voxelization is as follows:
\begin{equation}
    \begin{aligned}
        v_o(p; \mathcal{G}) & = \sum_{i=1}^PG_i(p; \mu_i, s_i, r_i,o_i) \\
        & =  \sum_{i=1}^P \text{exp}\left(- \frac{1}{2}\left( p -\mu_i \right)^T \Sigma^{-1}_i \left( p-\mu_i \right) \right) o_i \\
        \end{aligned}
        \end{equation}
        \begin{equation}
        \begin{aligned}
        v_c(p; \mathcal{G}) & =  \sum_{i=1}^PG_i(p; \mu_i, s_i, r_i,c_i) \\ 
        & = \sum_{i=1}^P \text{exp}\left(- \frac{1}{2}\left( p -\mu_i \right)^T \Sigma^{-1}_i \left( p-\mu_i \right) \right) c_i  ,
    \end{aligned}
\end{equation}
where $\Sigma_i$ represents the covariance matrix of each Gaussian, derived from its rotation quaternion and scale parameters.
To ensure efficiency in handling large numbers of Gaussians and voxels, we limit the influence of each Gaussian to a fixed local neighborhood.
This is justified by the fact that the contribution of each Gaussian typically decays rapidly with distance, making it computationally unnecessary to evaluate its impact on distant voxels.
\clearpage
\begin{figure*}
  \centering
      \includegraphics[page=6, trim=0cm 4.99cm 6.11cm 0cm, clip, width=1\textwidth]{figures/Figures.pdf}
  \caption{
  \textbf{Qualitative results on the Occ3D-nuScenes validation set.}
  Each column shows an input image and the rendered semantics generated by using GS to render predicted Gaussians into the input camera, as well as rendered semantics when rendering the predictions into the next frame, one with using our \emph{Temporal Module} to compensate motion and one without.
  The last row displays the pseudo labels of the next frame to show how correct motion compensation should look like.}
\label{fig:qual_render_temporal}
\end{figure*}