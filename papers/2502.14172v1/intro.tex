Distributional policy evaluation
\citep{morimura2010nonparametric,bellemare2017distributional,bdr2022}, which aims to estimate the return distribution of a policy in a Markov decision process (MDP), is crucial for many uncertainty-aware or risk-sensitive tasks \citep{lim2022distributional,kastner2023distributional}.
Unlike classic policy evaluation that only focuses on expected returns (\ie\ value functions), distributional policy evaluation captures uncertainty and risk by considering the full distributional information.
To solve distributional policy evaluation problem, the seminal work of \cite{bellemare2017distributional} proposed distributional temporal difference (TD) learning, which can be viewed as an extension of classic TD learning \citep{sutton1988learning}.

While classic TD learning was extensively studied \citep{bertsekas1995neuro, tsitsiklis1996analysis, bhandari2018finite, dalal2018finite, patil2023finite,li2024q,li2024high, chen2024lyapunov,samsonov2024gaussian,samsonov2024improved, wu2024statistical}, the theoretical understanding of distributional TD learning remains relatively underdeveloped.
Recent works \cite{rowland2018analysis,speedy,zhang2023estimation,rowland2024analysis,rowland2024nearminimaxoptimal,peng2024statistical} have analyzed distributional TD learning (or its model-based variants) in the tabular setting.
Especially, \cite{rowland2024nearminimaxoptimal} and \cite{peng2024statistical} demonstrated that in the tabular setting, statistically speaking, learning the return distribution (in terms of the $1$-Wasserstein distance\footnote{Solving distributional policy evaluation $\varepsilon$-accurately in the $1$-Wasserstein
distance sense is harder than solving classic policy evaluation $\varepsilon$-accurately, as the absolute difference of value functions is always bounded by the $1$-Wasserstein distance between return distributions. See \citep[Appendix~B][]{peng2024statistical} for more details.}) is as easy as learning its expectation.
However, in practical scenarios, where the state space is often extremely large or continuous, function approximation \citep{bellemare2017distributional,dabney2018distributional,dabney2018implicit,yang2019fully,freirich2019distributional,yue2020implicit,nguyen2021distributional,ijcai2021p476,luo2022distributional,wenliang2024distributional,sun2024distributional} becomes indispensable.
This raises a new open question: \emph{When function approximation is introduced, does learning the return distribution remain as statistically efficient as learning its expectation?}

To answer this question, we consider the simplest form of function approximation, \ie\  linear function approximation, and investigate the finite-sample performance of linear distributional TD learning.
In distributional TD, we need to represent the infinite-dimensional return distributions with some finite-dimensional parametrizations to make the algorithm tractable.
Prior works \citep{bellemare2019distributional,lyle2019comparative,bdr2022} have proposed various linear distributional TD learning algorithms under different parameterizations, namely, categorical and quantile parametrizations.
In this paper, we consider the categorical parametrization and propose an improved version of categorical linear TD learning algorithm (\LCTD).
We then analyze the non-asymptotic convergence rate of {\LCTD}.
Our analysis reveals that, with Polyak-Ruppert tail averaging \citep{ruppert1988efficient,polyak1992acceleration} and constant step size, the sample complexity of {\LCTD} matches that of the classic linear TD learning (\LTD) \citep{li2024high,samsonov2024improved}.
This result confirms that learning the return distribution is statistically no more difficult than learning its expectation when linear function approximation is introduced.

% In our analysis, we utilize exponential stability arguments for analyzing general linear stochastic approximation (LSA) proposed by the recent paper \citep{samsonov2024improved}, which can provide instance-independent tight convergence results for LSA.
% \zly{Add more technical contributions? Or move to the ``technical contributions" part in the contribution section?}
\paragraph{Notations.}
In the following parts of the paper, $(x)_+:=\max\brc{x,0}$ for any $x\in\RB$.
$\lesssim$ (resp. $\gtrsim$) means no larger (resp. smaller) than up to a multiplicative universal constant, and $a\simeq b$ means $a\lesssim b$ and $a\gtrsim b$ hold simultaneously.
The asymptotic notation $f(\cdot)=\wtilde{O}\prn{g(\cdot)}$ (resp. $\wtilde{\Omega}\prn{g(\cdot)}$)
means that $f(\cdot)$ is order-wise no larger (resp. smaller) than $g(\cdot)$, ignoring logarithmic factors of polynomials of $(1-\gamma)^{-1}, \lambda_{\min}^{-1}, \alpha^{-1}$, $\varepsilon^{-1}$, $\delta^{-1}$, $K$, $\norm{\bpsi^\star}_{\bSigma_{\bphi}}$, $\norm{\btheta^{\star}}_{\bI_K\otimes\bSigma_{\bphi}}$. 
We will explain the meaning of the notations once we first encounter them.
% See Appendix~\ref{Appendix_notation_table} for a quick reference for notations appearing in this paper.
In addition,re the sample complexity more carefully, sometimes we will not ignore the polynomials of specific logarithmic terms in $\wtilde{O}\prn{\cdot}$ (resp. $\wtilde{\Omega}\prn{\cdot}$), we will emphasize this point when necessary.

% Throughout this paper, 
We denote by $\otimes$ the Kronecker product (see Appendix~\ref{Appendix_kronecker} for properties of Kronecker product), $\bm{1}_K\in\RB^K$ the all-ones vector, $\bm{0}_K\in\RB^K$ the all-zeros vector, $\bI_K\in\RB^{K\times K}$ the identity matrix, $\norm{\bu}$ the Euclidean norm of any vector $\bu$, $\norm{\bB}$ the spectral norm of any matrix $\bB$. 
Let $\norm{\bu}_B:=\sqrt{\bu^{\intercal}\bB\bu}$ for any vector $\bu$ and positive semi-definite (PSD) matrix $\bB$.
$\bm B_1 \preccurlyeq \bm B_2$ stands for $\bm B_2-\bm B_1$ is PSD for any symmetric matrices $\bm B_1, \bm B_2$.
And $\prod_{k=1}^t \bm{B}_k$ is defined as $\bm{B}_t\bm{B}_{t-1}\cdots \bm{B}_1$ for any matrices $\brc{\bm{B}_k}_{k=1}^t$.
For any matrix $\bB=(\bb(1),\cdots,\bb(n))\in\RB^{m\times n}$, we define its vectorization as $\vect(\bB)=\prn{\bb(1)^{\intercal},\cdots,\bb(n)^{\intercal}}^{\intercal}\in\RB^{mn}$.
Suppose $A$ is some set. We denote by $\Delta(A)$ the set of all probability distributions over $A$.
And we denote $\Delta\prn{\brk{0,(1-\gamma)^{-1}}}$ by $\sP$.

\subsection{Contributions}
Our contribution includes two folds.

Algorithmically, we propose an improved version of linear categorical TD learning algorithm (\LCTD).
Instead of using stochastic semi-gradient descent (SSGD) to update the parameter as in \citep{bellemare2019distributional,lyle2019comparative,bdr2022}, we directly translate the linear-categorical projected Bellman equation into a linear system and apply linear stochastic approximation (LSA) to solve it.
The resulting {\LCTD} can be viewed as a preconditioned version of the vanilla linear categorical TD learning algorithm proposed in \citep[Section~9.6][]{bdr2022}. 
By introducing a preconditioner, our {\LCTD} achieve a finite-sample rate independent of the number of supports $K$ in the categorical parameterization, a result the vanilla form cannot attain.
We also note that our proposed \LCTD\ is similar to the algorithm in \citep{lyle2019comparative}, with the distinction that our approach ensures the total mass of the estimated return distributions always be $1$.

Theoretically, we establish the first non-asymptotic guarantees for distributional TD learning with linear function approximation.
Specifically, we show that, in the generative model setting, with Polyak-Ruppert tail averaging and
% instance-independent constant step size, 
% \begin{equation*}
%    T=\wtilde{O}\prn{\frac{\frac{1}{K(1-\gamma)^2}\norm{\btheta^{\star}}^2_{\bI_K\otimes\bSigma_{\bphi}}+1}{\varepsilon^2(1-\gamma)^2\lambda_{\min}^2}}
% \end{equation*}
instance-dependent constant step size, 
\begin{equation*}
   T=\wtilde{O}\prn{\frac{\frac{1}{K(1-\gamma)^2}\norm{\btheta^{\star}}^2_{\bI_K\otimes\bSigma_{\bphi}}+1}{(1-\gamma)^2\lambda_{\min}}\prn{\frac{1}{\varepsilon^2}+\frac{1}{\lambda_{\min}}}}
\end{equation*}
% \begin{equation*}
%    T=\wtilde{O}\prn{\frac{\frac{1}{K(1-\gamma)^2}\norm{\btheta^{\star}}^2_{\bI_K\otimes\bSigma_{\bphi}}+1}{\varepsilon^2(1-\gamma)^2\lambda_{\min}}}
% \end{equation*}
\footnote{${K^{-1}(1-\gamma)^{-2}}\norm{\btheta^{\star}}^2_{\bI_K\otimes\bSigma_{\bphi}}$ has the same scale as $\norm{\bm{\psi}^\star}_{\bSigma_\bphi}^2$, where $\btheta^{\star},\bm{\psi}^\star$ are the optimal solutions of {\LCTD} and {\LTD} respectively, because the cumulative distribution function is bounded by $1$ and the value function is bounded by $(1-\gamma)^{-1}$.See Remark~\ref{remark:theory_match} for a detailed discussion.}
online interactions with the environment are sufficient to guarantee {\LCTD} to yield an $\varepsilon$-accurate estimator with high probability, when the error is measured by the $\mu_\pi$-weighted $1$-Wasserstein distance.
We also provide sample complexity bounds when we take the 
instance-independent (\ie\ not dependent on unknown quantity) step size,
% optimal instance-dependent (\ie\ dependent on unknown quantity) step size, 
and in the Markovian setting.
Our sample complexity bounds match those of classic {\LTD} with constant step size \citep{li2024high,samsonov2024improved}, confirming the equal statistical tractability of distributional and classic value-based policy evaluation.
To establish these theoretical results, we analyze  the linear-categorical Bellman equation in details, and apply the exponential stability argument proposed in \citep{samsonov2024improved}.
% The fine-grained analysis of the linear-categorical Bellman equation improves the understanding of distributional reinforcement learning with function approximation.
Our analysis of the linear-categorical Bellman equation lays the foundation for subsequent algorithmic and theoretical advances in distributional reinforcement learning with function approximation.

\subsection{Related Work}
\paragraph{Distributional Reinforcement Learning.}
Distributional TD learning was first proposed in \citep{bellemare2017distributional}.
Following the distributional perspective in \citep{bellemare2017distributional}, \cite{pmlr-v97-qu19b} proposed a distributional version of the gradient TD learning algorithm,
\cite{tang2022nature} proposed a distributional version of multi-step TD learning, \cite{tang2024off} proposed a distributional version of off-policy Q($\lambda$) and TD($\lambda$) algorithms, and \citet{pmlr-v202-wu23s} proposed a distributional version of fitted Q evaluation to solve the distributional offline policy evaluation problem.
\cite{wiltzer2024dsm} proposed an approach for evaluating the return distributions for all policies simultaneously when the reward is deterministic or in the finite-horizon setting. 
\cite{wiltzer2024foundations} studied distributional policy evaluation in the multivariate reward setting and proposed corresponding TD learning algorithms.
Beyond the tabular setting, \cite{bellemare2019distributional,lyle2019comparative,bdr2022} proposed various distributional TD learning algorithms with linear function approximation under different parametrizations.

A series of recent studies have focused on the theoretical properties of distributional TD learning.
\cite{rowland2018analysis,speedy,zhang2023estimation,rowland2024analysis,rowland2024nearminimaxoptimal,peng2024statistical} analyzed the asymptotic and non-asymptotic convergence of distributional TD learning (or its model-based variants) in the tabular setting.
Among these works, \cite{rowland2024nearminimaxoptimal,peng2024statistical} established that in the tabular setting, learning the full return distribution is statistically as easy as learning its expectation, in the model-based and model-free setting respectively.
And \cite{bellemare2019distributional} provided an asymptotic convergence result for categorical TD learning with linear function approximation.

Beyond the problem of distributional policy evaluation,
\cite{rowland2023statistical,NEURIPS2023_06fc38f5, wang2024more} showed that, theoretically, classic value-based reinforcement learning could benefit from distributional reinforcement learning.
\cite{bauerle2011markov,chow2014algorithms,marthe2023beyond,noorani2023exponential,pires2025optimizing} considered optimizing statistical functionals of the return, and proposed algorithms to solve this harder problem.

\paragraph{Stochastic Approximation.}
Our {\LCTD} falls into the category of LSA. 
The classic TD learning, as one of the most classic LSA problems, has been extensively studied \citep{bertsekas1995neuro, tsitsiklis1996analysis, bhandari2018finite, dalal2018finite, patil2023finite,duan2023finite,li2024q,li2024high,samsonov2024gaussian, wu2024statistical}. 
Among these works, \cite{li2024high,samsonov2024improved} provided the tightest bounds for {\LTD} with constant step sizes, which is also considered in our paper.
While \cite{wu2024statistical} established the tightest bounds for {\LTD} with polynomial-decaying step sizes.

For general stochastic approximation problems, extensive works including \citep{lakshminarayanan2018linear,srikant2019finite,mou2020linear,mou2022optimal,huo2023bias,li2023online,durmus2024finite,samsonov2024improved, chen2024lyapunov} have provided solid theoretical understandings.
% \zly{list more result? at least list the sample complexity bounds of linear TD matching our \LCTD}

\subsection{Organization}
The remainder of this paper is organized as follows. 
In Section~\ref{Section:background}, we recap {\LTD}, categorical parametrization of return distributions and categorical TD learning.
% we introduce some backgrounds of  and  
In Section~\ref{Section:linear_ctd}, we introduce the linear-categorical parametrization, and use the linear-categorical projected Bellman equation to derive the {\LCTD} algorithm.
In Section~\ref{Section:analysis_linear_ctd}, we employ the exponential stability arguments to analyze the statistical efficiency of {\LCTD} in both the generative model setting and the Markovian setting.
In Section~\ref{Section:conclusion}, we conclude our work. 
The details of our proof are given in the appendices.