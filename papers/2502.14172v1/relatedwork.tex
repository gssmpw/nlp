\section{Related Work}
\paragraph{Distributional Reinforcement Learning.}
Distributional TD learning was first proposed in \citep{bellemare2017distributional}.
Following the distributional perspective in \citep{bellemare2017distributional}, \cite{pmlr-v97-qu19b} proposed a distributional version of the gradient TD learning algorithm,
\cite{tang2022nature} proposed a distributional version of multi-step TD learning, \cite{tang2024off} proposed a distributional version of off-policy Q($\lambda$) and TD($\lambda$) algorithms, and \citet{pmlr-v202-wu23s} proposed a distributional version of fitted Q evaluation to solve the distributional offline policy evaluation problem.
\cite{wiltzer2024dsm} proposed an approach for evaluating the return distributions for all policies simultaneously when the reward is deterministic or in the finite-horizon setting. 
\cite{wiltzer2024foundations} studied distributional policy evaluation in the multivariate reward setting and proposed corresponding TD learning algorithms.
Beyond the tabular setting, \cite{bellemare2019distributional,lyle2019comparative,bdr2022} proposed various distributional TD learning algorithms with linear function approximation under different parametrizations.

A series of recent studies have focused on the theoretical properties of distributional TD learning.
\cite{rowland2018analysis,speedy,zhang2023estimation,rowland2024analysis,rowland2024nearminimaxoptimal,peng2024statistical} analyzed the asymptotic and non-asymptotic convergence of distributional TD learning (or its model-based variants) in the tabular setting.
Among these works, \cite{rowland2024nearminimaxoptimal,peng2024statistical} established that in the tabular setting, learning the full return distribution is statistically as easy as learning its expectation, in the model-based and model-free setting respectively.
And \cite{bellemare2019distributional} provided an asymptotic convergence result for categorical TD learning with linear function approximation.

Beyond the problem of distributional policy evaluation,
\cite{rowland2023statistical,NEURIPS2023_06fc38f5, wang2024more} showed that, theoretically, classic value-based reinforcement learning could benefit from distributional reinforcement learning.
\cite{bauerle2011markov,chow2014algorithms,marthe2023beyond,noorani2023exponential,pires2025optimizing} considered optimizing statistical functionals of the return, and proposed algorithms to solve this harder problem.

\paragraph{Stochastic Approximation.}
Our {\LCTD} falls into the category of LSA. 
The classic TD learning, as one of the most classic LSA problems, has been extensively studied \citep{bertsekas1995neuro, tsitsiklis1996analysis, bhandari2018finite, dalal2018finite, patil2023finite,duan2023finite,li2024q,li2024high,samsonov2024gaussian, wu2024statistical}. 
Among these works, \cite{li2024high,samsonov2024improved} provided the tightest bounds for {\LTD} with constant step sizes, which is also considered in our paper.
While \cite{wu2024statistical} established the tightest bounds for {\LTD} with polynomial-decaying step sizes.

For general stochastic approximation problems, extensive works including \citep{lakshminarayanan2018linear,srikant2019finite,mou2020linear,mou2022optimal,huo2023bias,li2023online,durmus2024finite,samsonov2024improved, chen2024lyapunov} have provided solid theoretical understandings.
% \zly{list more result? at least list the sample complexity bounds of linear TD matching our \LCTD}