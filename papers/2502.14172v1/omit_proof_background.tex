\subsection{Linear Projected Bellman Equation}\label{subsection:linear_projected_bellman_equation}
It is worth noting that, $\bPi_{\bphi}^{\pi}\colon \prn{\RB^\gS,\norm{\cdot}_{\mu_\pi}}\to\prn{\sV_\bphi,\norm{\cdot}_{\mu_\pi}}$ is an orthogonal projection.

We aim to derive Eqn.~\eqref{eq:linear_TD_equation}.
It is easy to check that, for any $\bV\in \RB^{\gS}$, $\bPi_{\bphi}^{\pi}\bV$ is uniquely give by $ \bV_{\tilde\bpsi}$ where
\begin{equation*}
    \tilde\bpsi=\bSigma_{\bphi}^{-1}\EB_{s\sim\mu_\pi}\brk{\bphi(s)V(s)}.
\end{equation*}
Hence, by the definition of Bellman operator (Eqn.~\eqref{eq:Bellman_equation}), $\bpsi^{\star}$ is the unique solution to the following system of linear equations for $\bpsi\in\RB^{d}$
\begin{equation*}
    \begin{aligned}
    \bpsi=&\bSigma_{\bphi}^{-1}\EB_{s\sim\mu_{\pi}}\brk{\bphi(s)\brk{\bT^\pi \bV_{\bpsi}}(s)}\\
    =&\bSigma_{\bphi}^{-1}\EB_{s\sim\mu_{\pi}}\brk{\bphi(s)\prn{\EB\brk{r_0\mid s_0=s }+\gamma\EB\brk{ \bphi(s_1)^{\intercal}\mid s_0=s }\bpsi}}\\
    =&\bSigma_{\bphi}^{-1}\EB_{s,s^\prime}\brk{\bphi(s)\bphi(s^\prime)^{\intercal}}\bpsi+\bSigma_{\bphi}^{-1}\EB_{s,r}\brk{\bphi(s)r},
    \end{aligned}
\end{equation*}
or equivalently,
\begin{equation*}
    \begin{aligned}
    \prn{\bSigma_{\bphi}-\gamma\EB_{s,s^\prime}\brk{\bphi(s)\bphi(s^\prime)^{\intercal}}}\bpsi=\EB_{s,r}\brk{\bphi(s)r}.
    \end{aligned}
\end{equation*}

\subsection{Convergence Results for Linear TD Learning}\label{subsection:convergence_linear_TD}
It is worthy noting that, {\LTD} is equivalent to the stochastic semi-gradient descent (SSGD) update.

In {\LTD}, our goal is to find a good estimator $\hat{\bpsi}$ such that $\norm{\bV_{\hat{\bpsi}}-\bV_{\bpsi^\star}}_{\mu_\pi}=\norm{\hat{\bpsi}-\bpsi^\star}_{\bSigma_{\bphi}}\leq \varepsilon$.
\cite{samsonov2024improved} considered the Polyak-Ruppert tail averaging $\bar{\bpsi}_{T}:=\prn{T/2+1}^{-1}\sum_{t=T/2}^T\bpsi_t$, and showed that in the generative model setting
with constant step size $\alpha\simeq (1-\gamma)\lambda_{\min}$, 
\begin{equation*}
   T=\wtilde{O}\prn{\frac{\norm{\bpsi^\star}^2_{\bSigma_{\bphi}}+1}{(1-\gamma)^2\lambda_{\min}}\prn{\frac{1}{\varepsilon^2}+\frac{1}{\lambda_{\min}}}}
\end{equation*}
% with instance-independent constant step size $\alpha\simeq (1-\gamma)$, 
% \begin{equation*}
%    T=\wtilde{O}\prn{\frac{\norm{\bpsi^\star}^2_{\bSigma_{\bphi}}+1}{\varepsilon^2(1-\gamma)^2\lambda_{\min}^2}}
% \end{equation*}
is sufficient to guarantee that $\norm{\bV_{\bar{\bpsi}_{T}}-\bV_{\bpsi^\star}}_{\mu_\pi}\leq \varepsilon$.
They also provided sample complexity bounds when taking the instance-independent (\ie\ not dependent on unknown quantity) 
% optimal instance-dependent (\ie\ dependent on unknown quantity) 
step size, and in the Markovian setting.

\subsection{Categorical Parametrization is an Isometry}\label{appendix:PK_isometric}
\begin{proposition}\label{prop:PK_isometric}
The affine space $\prn{\sP^{\sgn}_K, \ell_2}$ is isometric with $\prn{\RB^{K}, \sqrt{\iota_K}\norm{\cdot}_{\bC^\intercal \bC}}$, in the sense that, for any $\nu_{\bp_1},\nu_{\bp_2}\in\sP^{\sgn}_K$, it holds that $\ell_2^2(\nu_{\bp_1},\nu_{\bp_2})=\iota_K\norm{\bp_1-\bp_2}^2_{\bC^{\intercal}\bC}$, where
\begin{equation}\label{eq:def_C}
    \bC = 
\begin{bmatrix}
1 & 0 & \cdots & 0 & 0 \\
1 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
1 & 1 & \cdots & 1 & 0 \\
1 & 1 & \cdots & 1 & 1
\end{bmatrix}\in\RB^{K\times K}.
\end{equation}
\end{proposition}
\begin{proof}
\begin{equation*}
    \begin{aligned}
    \ell_2^2(\nu_{\bp_1},\nu_{\bp_2})=&\int_{0}^{\prn{1{-}\gamma}^{-1}}\prn{F_{\nu_{\bp_1}}(x)-F_{\nu_{\bp_2}}(x)}^2 d x\\
    =&\iota_K\sum_{k=0}^{K-1}\prn{F_{\nu_{\bp_1}}(x_k)-F_{\nu_{\bp_2}}(x_k)}^2\\
    % =&\iota_K\norm{\bF_{\nu_1}-\bF_{\nu_2}}^2\\
    =&\iota_K\norm{\bC\prn{\bp_{1}-\bp_{2}}}^2\\
    =&\iota_K\norm{\bp_{1}-\bp_{2}}^2_{\bC^{\intercal}\bC}.
\end{aligned}
\end{equation*}
\end{proof}

\subsection{Categorical Projection Operator is Orthogonal Projection}\label{appendix:cate_project_orth}
\begin{proposition} \cite[Lemma~9.17]{bdr2022} \label{prop:orthogonal_decomposition}
For any $\nu\in\sP^{\sgn}$ and $\nu_{\bp}\in\sP^{\sgn}_K$, it holds that
\begin{equation*}
    \ell_2^2\prn{\nu,\nu_\bp}=\ell_2^2\prn{\nu,\bm{\Pi}_K\nu}+\ell_2^2\prn{\bm{\Pi}_K\nu,\nu_\bp}.
\end{equation*}
\end{proposition}

\subsection{Categorical Projected Bellman Operator}
The following lemma characterizing $\bm{\Pi}_K\gT^\pi$ is useful for both practice and theoretical analysis.
\begin{proposition}\label{prop:categorical_projection_operator}
For any ${\bm{\eta}}\in\prn{\sP^{\sgn}}^\gS$ and $s\in\gS$, it holds that
% \begin{equation*}
% \begin{aligned}
%      p_k\prn{\brk{\gT^\pi\bm{\eta}}(s)}=&\EB_{X\sim \brk{\gT^\pi\bm{\eta}}(s)}\brk{\prn{1-\abs{\frac{X-x_k}{\iota_K}}}_+}\\
%      =&\EB\brk{g_{K,k}(r_0)+\sum_{j=0}^{K-1}p_j\prn{\eta(s_1)}\prn{g_{j,k}(r_0)-g_{K,k}(r_0)}\Big| s_0=s }.
% \end{aligned}
% \end{equation*}
\begin{equation*}
\begin{aligned}
     \bp_{\gT^\pi\bm{\eta}}(s)=&\EB\brk{\bg_K(r_0)+\prn{\bG(r_0)-\bm{1}_K^{\intercal}\otimes\bg_K(r_0)}  \bp_{\bm{\eta}}(s_1)  \Big| s_0=s }\\
     =&\EB\brk{\tilde\bG(r_0) \prn{\bp_{\bm{\eta}}(s_1)-\frac{1}{K+1}\bm{1}_{K}}  \Big| s_0=s }+\frac{1}{K+1}\sum_{j=0}^K\EB\brk{\bg_j(r_0) \Big| s_0=s }.
\end{aligned}
\end{equation*}
And in the same way, for any $r\in[0,1]$ and $s^\prime\in\gS$, it holds that
\begin{equation*}
\begin{aligned}
\bp_{\prn{b_{r,\gamma}}_\#{\eta}(s^\prime)}&=\tilde\bG(r) \prn{\bp_{\bm{\eta}}(s^\prime)-\frac{1}{K+1}\bm{1}_{K}}  +\frac{1}{K+1}\sum_{j=0}^K\bg_j(r),
\end{aligned}
\end{equation*}
    where $\tilde{\bG}$ and $\bg$ is defined in Theorem~\ref{thm:linear_cate_TD_equation}.
\end{proposition}
This proposition is a special case of Proposition~\ref{prop:Pi_K_T}, whose proof can be found in Appendix~\ref{appendix:proof_Pi_K_T}.