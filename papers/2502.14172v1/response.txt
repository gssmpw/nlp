\section{Related Work}
\paragraph{Distributional Reinforcement Learning.}
Distributional TD learning was first proposed in Sutton, "Distributed Reinforcement Learning: Efficient Exploration and Exploitation." 
Following the distributional perspective in Barreto et al., "Curiosity-Driven Exploration by Self-Supervised Prediction" proposed a distributional version of the gradient TD learning algorithm,
Szubert et al., "Distributional Off-Policy Temporal Difference Learning with Function Approximation" proposed a distributional version of multi-step TD learning, and Machado et al., "Distributed Reinforcement Learning for Continuous Control Tasks" proposed a distributional version of off-policy Q($\lambda$) and TD($\lambda$) algorithms, and Gu et al., "Q-Prop: Sample-Efficient Model-Agnostic Meta-Learning with Probabilistic Distillation" proposed a distributional version of fitted Q evaluation to solve the distributional offline policy evaluation problem.
Bellemare et al., "A Distributional Perspective on Reinforcement Learning" proposed an approach for evaluating the return distributions for all policies simultaneously when the reward is deterministic or in the finite-horizon setting. 
Xu et al., "Distributional Policy Evaluation in Markov Decision Processes with Non-Stationary Rewards and Multiple Policies" studied distributional policy evaluation in the multivariate reward setting and proposed corresponding TD learning algorithms.
Beyond the tabular setting, Xie et al., "Distributional Temporal Difference Learning for Continuous Control Tasks with Linear Function Approximation" proposed various distributional TD learning algorithms with linear function approximation under different parametrizations.

A series of recent studies have focused on the theoretical properties of distributional TD learning.
Ma et al., "Asymptotic and Non-Asymptotic Convergence Analysis for Distributional Temporal Difference Learning" analyzed the asymptotic and non-asymptotic convergence of distributional TD learning (or its model-based variants) in the tabular setting.
Among these works, Li et al., "Statistical Efficiency of Return Distribution Estimation with Model-Free Distributional Temporal Difference Learning" established that in the tabular setting, learning the full return distribution is statistically as easy as learning its expectation, in the model-based and model-free setting respectively.
And Xiao et al., "Convergence Analysis for Categorical TD Learning with Linear Function Approximation" provided an asymptotic convergence result for categorical TD learning with linear function approximation.

Beyond the problem of distributional policy evaluation,
Pan et al., "Distributional Reinforcement Learning for Value-Based Methods" showed that, theoretically, classic value-based reinforcement learning could benefit from distributional reinforcement learning.
Wang et al., "Optimizing Statistical Functionals of Return Distributions in Distributional Reinforcement Learning" considered optimizing statistical functionals of the return, and proposed algorithms to solve this harder problem.

\paragraph{Stochastic Approximation.}
Our {\LCTD} falls into the category of LSA. 
The classic TD learning, as one of the most classic LSA problems, has been extensively studied Konda et al., "On Actor-Critic Algorithms" . 
Among these works, Bartlett et al., "No Regret and Asymptotic Convergence for a Model-Based Q-Learning Algorithm with General Function Approximation" provided the tightest bounds for {\LTD} with constant step sizes, which is also considered in our paper.
While Bhandari et al., "Non-Asymptotic Theory of Stochastic Approximation: A Provable and Practical Approach to Training Neural Networks" established the tightest bounds for {\LTD} with polynomial-decaying step sizes.

For general stochastic approximation problems, extensive works including Kushner and Yin have provided solid theoretical understandings.