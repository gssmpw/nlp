\subsection{Policy Evaluation and TD learning}\label{Subsection:policy_eval_and_TD}
A discounted MDP is defined by a 5-tuple $M=\<\gS,\gA,\gP_R,P,\gamma\>$.
We assume the state space $\gS$ and action space $\gA$ are both Polish spaces, namely a complete separable metric space.
${\gP_R}$ is the distribution of rewards,  \ie\ $\gP_R(\cdot|s,a)\in\Delta\prn{[0,1]}$ for any state-action pair $(s,a)\in\gS\times\gA$ (we assume all rewards is bounded).
${P}$ is the transition dynamics, \ie\ $P(\cdot|s,a)\in\Delta\prn{\gS}$ for any $(s,a)\in\gS\times\gA$.
And $\gamma\in(0,1)$ is the discount factor.

Given a policy $\pi\colon\gS\to\Delta\prn{\gA}$ and an initial state $s_0=s\in\gS$, a random trajectory $\brc{\prn{s_t,a_t,r_t}}_{t=0}^\infty$ can be sampled: $a_t\mid s_t\sim\pi(\cdot\mid s_t)$, $r_t\mid (s_t,a_t)\sim \gP_R({\cdot}\mid s_t,a_t)$, ${s_{t+1}}\mid{(s_t,a_t)}\sim P({\cdot}\mid{s_t,a_t})$ for any $t\in\NB$.
We assume the Markov chain $\brc{s_t}_{t=0}^\infty$ has a unique stationary distribution $\mu_\pi\in\Delta(\gS)$.
We define the return of the trajectory by $G^\pi(s):=\sum_{t=0}^\infty \gamma^t r_t$.
% \in\brk{0,(1-\gamma)^{-1}}$.
The value function $V^\pi(s)$ is the expectation of $G^\pi(s)$, and ${\bm{V}}^\pi:=\prn{V^\pi(s)}_{s\in\gS}$.
It is known that ${\bm{V}}^\pi$ satisfies the Bellman equation: 
% That is, for any $s\in\gS$,
\begin{equation}\label{eq:Bellman_equation}
            V^\pi(s)=\EB_{a\sim\pi(\cdot\mid s),r\sim \gP_R({\cdot}\mid s,a),s^\prime\sim P(\cdot\mid s,a)}\brk{r+\gamma V^\pi(s^\prime)},\quad\forall s\in\gS,
\end{equation}
or in a compact form ${\bm{V}}^\pi={\bm{T}}^\pi{\bm{V}}^\pi$, where ${\bm{T}}^\pi\colon \RB^\gS\to \RB^\gS$ is called the Bellman operator.

In the task of policy evaluation, we aim to find ${\bm{V}}^\pi$ for some given policy $\pi$.
Since ${\bm{V}}^\pi$ is the unique solution of the Bellman equation, the problem is reduced to solving the Bellman equation.
However, in practical applications ${\bm{T}}^\pi$ is usually unknown and the agent only has access to the streaming data $\brc{\prn{s_t,a_t,r_t}}_{t=0}^\infty$.
% a sample trajectory $\brc{\prn{s_t,a_t,r_t}}_{t=0}^\infty$ generated in a streaming manner.
In this circumstance, we may solve the Bellman equation through LSA.
% stochastic approximation.
Specifically, at the $t$-th time-step, the updating scheme is 
$$
{V}_{t}(s_t)=V_{t-1}(s_{t})-\alpha\prn{V_{t-1}(s_{t})-r_t-\gamma V_{t-1}(s_{t+1})}.
$$
We expect ${\bm{V}_t}$ will converge to ${\bm{V}}^\pi$ as $t$ tends to infinity.
This algorithm is known as the TD learning.

\subsection{Linear Function Approximation and Linear TD Learning}\label{subsection:linear_td}
In this section, we introduce linear function approximation and briefly review {\LTD}.
% linear TD learning (\LTD).
% We adopt the perspective of using LSA to solve the linear projected Bellman equation.
To be concrete, we assume there is a $d$-dimensional feature for each state $s\in\gS$, which is given by the feature map $\bphi\colon\gS\to\RB^d$.
We consider the linear function approximation of value functions:
\begin{equation*}
    \sV_{\bphi} := \left\{\bV_{\bpsi}=\prn{V_{\bpsi}(s)}_{s\in\gS}\colon  V_{\bpsi}(s)=\bpsi^{\intercal}\bphi(s),\bpsi\in\RB^{d} \right\}\subset \RB^\gS,
\end{equation*}
$\mu_\pi$-weighted norm $\norm{\bV}_{\mu_\pi}:=(\EB_{s\sim\mu_{\pi}}[V(s)^2])^{1/2}$, and linear projection operator $\bPi_{\bphi}^{\pi}\colon \RB^{\gS}\to\sV_{\bphi}$:
\begin{equation*}
    \bPi_{\bphi}^{\pi}\bV:=\argmin\nolimits_{\bV_{\bpsi}\in\sV_{\bphi}}\norm{\bV-\bV_{\bpsi}}_{\mu_\pi},\quad \forall\bV\in\RB^\gS.
\end{equation*}
One can check that the linear projected Bellman operator $\bPi_{\bphi}^{\pi}\bm{T}^{\pi}$ is a $\gamma$-contraction in the Polish space $(\sV_{\bphi},\norm{\cdot}_{\mu_\pi})$.
% Hence, the linear projected Bellman equation $\bV_{\bpsi}=\bPi_{\bphi}^{\pi}\bm{T}^{\pi}\bV_{\bpsi}$ admits a unique solution $\bV_{\bpsi^\star}$, 
Hence, $\bPi_{\bphi}^{\pi}\bm{T}^{\pi}$ admits a unique fixed point $\bV_{\bpsi^\star}$, 
which satisfies $\|\bV^\pi-\bV_{\bpsi^\star} \|_{\mu_\pi}\leq(1-\gamma^2)^{-1/2}\|\bV^\pi-\bPi_{\bphi}^{\pi}\bV^\pi \|_{\mu_\pi}$ \citep[Theorem~9.8][]{bdr2022}.
In Appendix~\ref{subsection:linear_projected_bellman_equation}, we show that $\bpsi^\star\in\RB^d$ is the unique solution to the linear system for $\bpsi\in\RB^d$:
\begin{equation}\label{eq:linear_TD_equation}
    \prn{\bSigma_{\bphi}-\gamma\EB_{s,s^\prime}\brk{\bphi(s)\bphi(s^\prime)^{\intercal}}}\bpsi=\EB_{s,r}\brk{\bphi(s)r}.
\end{equation}
In the subscript of the expectation, we abbreviate $s\sim\mu_{\pi}(\cdot), a\sim\pi(\cdot|s), r\sim \gP_R(\cdot|s,a), s^\prime\sim P(\cdot|s,a)$ as $s,a,r,s^\prime$.
For brevity, we will continue to use such abbreviations in this paper when there is no ambiguity.
We can use LSA to solve the linear projected Bellman equation (Eqn.~\eqref{eq:linear_TD_equation}).
As a result, at the $t$-th time-step, the updating scheme of {\LTD} is 
\begin{equation}\label{eq:linear_TD}
    \bpsi_t=\bpsi_{t-1}-\alpha\bphi(s_t)\brk{\prn{\bphi(s_t)-\gamma\bphi(s_{t+1})}^{\intercal}\bpsi_{t-1} - r_t}.
\end{equation}
See Appendix~\ref{subsection:convergence_linear_TD} for convergence results for {\LTD}.

\subsection{Distributional Policy Evaluation and Distributional TD Learning}
In certain applications, we are not only interested in finding the expectation of the random return $G^\pi(s)$ but want to find the whole distribution of $G^\pi(s)$.
This task is called distributional policy evaluation.
We use $\eta^\pi(s)\in\sP$ to denote the distribution of $G^\pi(s)$ and let ${\bm{\eta}}^\pi:=(\eta^\pi(s))_{s\in\gS}\in\sP^\gS$.
Then ${\bm{\eta}}^\pi$ satisfies the distributional Bellman equation:
\begin{equation}
\label{eq:distributional_Bellman_equation}
% \begin{aligned}
        \eta^\pi(s)=\EB_{a\sim\pi(\cdot\mid s),r\sim \gP_R({\cdot}\mid s,a),s^\prime\sim P(\cdot\mid s,a)}[\prn{b_{r,\gamma}}_\#\eta^\pi(s^\prime)],\quad\forall s\in\gS.
% \end{aligned}
\end{equation}
% \begin{equation}\label{eq:distributional_Bellman_equation}
% % \begin{aligned}
%         \eta^\pi(s)=\EB_{a\sim\pi(\cdot\mid s),r\sim \gP_R({\cdot}\mid s,a),s^\prime\sim P(\cdot\mid s,a)}\brk{\prn{b_{r,\gamma}}_\#\eta^\pi(s^\prime)},\quad\forall s\in\gS.
% % \end{aligned}
% \end{equation}
Here $b_{r,\gamma}\colon \RB\to\RB$ is the affine function defined by $b_{r,\gamma}(x)=r+\gamma x$, and $f_\#\nu$ is the push forward measure of $\nu$ through any function $f\colon \RB\to\RB$, so that $f_\#\nu(A)=\nu(f^{-1}(A))$ for any Borel set $A$, where $f^{-1}(A):=\brc{x\colon f(x)\in A}$.
The distributional Bellman equation can also be written as ${\bm{\eta}}^\pi={\bm{\gT}}^\pi{\bm{\eta}}^\pi$.
The operator ${{\gT}}^\pi\colon \sP^\gS\to \sP^\gS$ is called the distributional Bellman operator and ${\bm{\eta}}^\pi$ is the associated fixed point.

In analogy to TD learning, we can solve the distributional Bellman equation by LSA
% stochastic approximation 
and get the distributional TD learning algorithm given the streaming data  $\brc{\prn{s_t,a_t,r_t}}_{t=0}^\infty$:
\begin{equation*}
    \eta_{t}(s_t)=\eta_{t-1}(s_{t})-\alpha[\eta_{t-1}(s_{t})-\prn{b_{r_t,\gamma}}_\#\eta_{t-1}(s_{t+1})].
\end{equation*}
We comment the algorithm above is not computationally feasible as we need to manipulate an infinite-dimensional object at each iteration.
 
\subsection{Categorical Parametrization of Return Distributions and Categorical TD Learning}
% \zly{\textbf{Too messy. Maybe we should explain like (1) Originally all distributions lie in $\sP^{\sgn}$.
% (2) For every $\eta$ in $\sP^{\sgn}$ we can find a categorical approximation $\eta_K$.(How it defined?)
% (3) All such $\eta_K$s forms the categorical sign measure space.
% And the approximation is indeed the projection to that space.
% (4) The projection of $\eta^\pi$ satisfies the categorical Bellman euqation, and using LSA to solve that equation is called categorical TD.
% All redundant irrelevant details should be avoided.
% }}
In order to deal with the infinite-dimensional return distributions in a computational tractable manner, we consider the finite-dimensional categorical parametrization as in \citep{bellemare2017distributional,rowland2018analysis,bellemare2019distributional,rowland2024nearminimaxoptimal,peng2024statistical}.
To incorporate function approximation in the distribution parametrization which cannot guarantee non-negative outputs, we will work with the signed measure space $\sP^{\sgn}$ with total mass $1$ instead of the standard probability space $\sP\subset \sP^{\sgn}$:
\begin{equation*}
    \sP^{\sgn}:= \left\{\nu\colon\abs{\nu}(\RB)< \infty ,\nu(\RB)=1,\operatorname{supp}(\nu)\subseteq \left[0,(1-\gamma)^{-1}\right] \right\}.
\end{equation*} 
For any $\nu\in\sP^{\sgn}$, we define its cumulative distribution function (CDF) as $F_\nu(x):=\nu[0,x]$. 
We can naturally define the $L^2$ and $L^1$ distance between CDFs as Cram\'er distance $\ell_2$ and $1$-Wasserstein distance $W_1$ in $\sP^{\sgn}$ respectively.
The distributional Bellman operator (see Eqn.~\eqref{eq:distributional_Bellman_equation}) can also be extended to the product space $(\sP^{\sgn})^{\gS}$ without modifying its definition.

The space of all categorical parametrized signed measures with total mass $1$ is defined as
\begin{equation*}
    \sP^{\sgn}_K := \left\{ \nu_\bp=\sum_{k=0}^K p_k \delta_{x_k} \colon  \bp=\prn{p_0,\cdots,p_{K-1}}^{\intercal}\in \RB^K, p_K=1-\sum_{k=0}^{K-1}p_k \right\}, 
\end{equation*}
which is an affine subspace of $\sP^{\sgn}$, where $K\in \NB$, $\brc{x_k=k\iota_K}_{k=0}^K$ are equally-spaced points of the support, and $\iota_K=\brk{K(1-\gamma)}^{-1}$ is the gap between two adjacent points.
We say $\bp$ is the probability mass function (PMF) representation of $\nu_\bp\in\sP^{\sgn}_K$.
% We denote by $\bp_\nu=\prn{p_k(\nu)}_{k=0}^{K-1}\in\RB^K$ the PMF representation of $\nu=\sum_{k=0}^{K} p_k(\nu) \delta_{x_k}\in\sP^{\sgn}_K$ when $\nu$ is not identified with a vector $\bp\in\RB^K$ in the subscript explicitly.
In fact, this representation is an isometry (see Proposition~\ref{prop:PK_isometric}).
We define the categorical projection operator $\bPi_{K}\colon \sP^{\sgn}\to\sP^{\sgn}_K$ as
% For any $\nu\in \sP^{\sgn}$, 
\begin{equation*}
   \bPi_{K}\nu:=\argmin\nolimits_{\nu_\bp\in\sP^{\sgn}_{K}}\ell_{2}\prn{\nu, \nu_\bp},\quad \forall\nu\in \sP^{\sgn}. 
\end{equation*}
Following \citep[Proposition~5.14][]{bdr2022}, one can show that $\bm{\Pi}_K\nu\in\sP^{\sgn}_K$ is uniquely identified with a vector $\bp_\nu=\prn{p_k(\nu)}_{k=0}^{K-1}\in\RB^K$, where
% and $p_K(\nu)=1-\sum_{k=0}^{K-1}p_k(\nu)$.
% \begin{equation}\label{eq:categorical_prob}
%      p_k(\nu)=\EB_{X\sim \nu}\brk{\prn{1-\abs{\frac{X-x_k}{\iota_K}}}_+}:=\int_{\brk{0,(1-\gamma)^{-1}}}\prn{1-\abs{\frac{x-x_k}{\iota_K}}}_+ \nu(dx).
% \end{equation}
\begin{equation}\label{eq:categorical_prob}
     p_k(\nu)=\EB_{X\sim \nu}[(1-\abs{(X-x_k)/{\iota_K}})_+]:=\int_{\brk{0,(1-\gamma)^{-1}}}(1-\abs{(x-x_k)/{\iota_K}})_+ \nu(dx).
\end{equation}
$\bm{\Pi}_K$ is in fact an orthogonal projection (see Proposition~\ref{prop:orthogonal_decomposition}), and thus is non-expansive.

For any ${\bm{\eta}}\in\prn{\sP^{\sgn}}^\gS$, $s\in\gS$, we define $\bp_{\bm{\eta}}(s):=\bp_{\eta(s)}$ and lift $\bPi_K$ to the product space by defining
$\brk{\bm{\Pi}_K{\bm{\eta}}}(s) := \bm{\Pi}_K\eta(s)$.
One can check that the categorical Bellman operator $\bPi_{K}{\gT}^{\pi}$ (see Proposition~\ref{prop:categorical_projection_operator} for the characterization of $\bm{\Pi}_K\gT^\pi$) is a $\sqrt\gamma$-contraction in the Polish space $((\sP_K^{\sgn})^\gS,\ell_{2,\mu_\pi})$, where $\ell_{2,\mu_\pi}(\bm{\eta}_1,\bm{\eta}_2):=(\EB_{s\sim\mu_{\pi}}[\ell_2^2(\eta_1(s),\eta_2(s))])^{1/2}$ is the $\mu_\pi$-weighted Cram\'er distance between $\bm{\eta}_1,\bm{\eta}_2\in(\sP^{\sgn})^\gS$.
Hence, the categorical projected Bellman equation $\bm{\eta}=\bPi_{K}\gT^{\pi}\bm{\eta}$ admits a unique solution $\bm{\eta}^{\pi,K}$, which satisfies $\ell_{2,\mu_\pi}(\bm{\eta}^\pi,\bm{\eta}^{\pi,K})\leq(1-\gamma)^{-1/2}\ell_{2,\mu_\pi}(\bm{\eta}^\pi,\bPi_K\bm{\eta}^{\pi})$ \citep[Proposition~3][]{rowland2018analysis}.
Applying LSA to solving the equation yields categorical TD learning, and the iteration rule is given by
\begin{equation*}
    \eta_{t}(s_t)=\eta_{t-1}(s_{t})-\alpha[\eta_{t-1}(s_{t})-\bPi_K\prn{b_{r_t,\gamma}}_\#\eta_{t-1}(s_{t+1})].
\end{equation*}

