\subsection{Linear-Categorical Parametrization of Return Distributions}\label{subsection:linear_cate_para}
Now, we combine linear function approximation with categorical parametrization, and introduce the space of linear-categorical parametrized signed measures with total mass $1$:\footnote{Compared with the parametrization in \citep[Section~9.6][]{bdr2022}, our model is identifiable because we remove redundant parameters in their model, which can be linearly represented by other parameters.}
% , which is defined with the PMF parametrization:
% \begin{equation*}
%     \sP^{\sgn}_{\bphi,K} := \left\{\bm{\eta}_{\bw}=\prn{\eta_{\bw}(s)}_{s\in\gS}=\prn{\sum_{k=0}^Kp_k(s;\bw)\delta_{x_k}}_{s\in\gS}\colon  \bw\in\RB^{dK} \right\},
% \end{equation*}
\begin{equation*}
    \sP^{\sgn}_{\bphi,K} := \left\{\bm{\eta}_{\bw}=\prn{\eta_{\bw}(s)}_{s\in\gS}\colon \eta_{\bw}(s)=\sum_{k=0}^Kp_k(s;\bw)\delta_{x_k},  \bw\in\RB^{dK} \right\},
\end{equation*}
which is an affine subspace of $(\sP^{\sgn}_{K})^{\gS}$, where $\bw=(\bw(0)^{\intercal},\cdots,\bw(K-1)^{\intercal})^{\intercal}$ and
\begin{equation}\label{eq:def_linear_parametrize}
            p_k(s;\bw)= \begin{cases} \bphi(s)^{\intercal}\bw(k)+\frac{1}{K+1}, & \text{ for}\ k\in\brc{0,1,\cdots K-1};\\
            -\bphi(s)^{\intercal}\sum_{j=0}^{K-1} \bw(j)+\frac{1}{K+1}, & \text{ for}\ k=K.
            \end{cases}
\end{equation}
Again, this representation is an isometry (see Proposition~\ref{prop:linear_cate_isometric}).
In many cases, it is more convenient to work with the matrix version of the parameter $\bW:=\prn{\bw(0),\cdots,\bw(K-1)}\in\RB^{d\times K}$ instead of $\bw=\vect\prn{\bW}$.
We abbreviate $\bp_{\bm{\eta}_\bw}$ as $\bp_{\bw}$, then by Lemma~\ref{lem:vec_and_KP}, for any $s\in\gS$, it holds that
\begin{equation}\label{eq:PMF_linear_parametrization}
    \bp_{\bw}(s)=\bW^{\intercal}\bphi(s)+(K+1)^{-1}\bm{1}_{K}.
    % =\prn{\bI_K\otimes\bphi(s)}^{\intercal}\bw+\frac{1}{K+1}\bm{1}_{K},
\end{equation}
% where $\bI_K$ is the identity matrix.
We define the linear-categorical projection operator $\bPi_{\bphi, K}^{\pi}\colon (\sP^{\sgn})^{\gS}\to\sP^{\sgn}_{\bphi,K}$ as follow:
\begin{equation*}
    \bPi_{\bphi, K}^{\pi}\bm{\eta}:=\argmin\nolimits_{\bm{\eta}_{\bw}\in\sP^{\sgn}_{\bphi,K}}\ell_{2,\mu_\pi}\prn{\bm{\eta}, \bm{\eta}_{\bw}},\quad\forall\bm{\eta}\in (\sP^{\sgn})^{\gS}.
\end{equation*}
$\bPi_{\bphi, K}^{\pi}$ is in fact an orthogonal projection (see Proposition~\ref{prop:orthogonal_decomposition_linear_approximation}), and thus is non-expansive.
% Therefore, $\bPi_{\bphi, K}^{\pi}$ is non-expansive w.r.t.\ the $\mu_\pi$-weighted Cram\'er metric, namely $\ell_{2,\mu_{\pi}}\prn{\bPi_{\bphi, K}^{\pi}\bm{\eta},\bPi_{\bphi, K}^{\pi}\bm{\eta}^{\prime}}\leq\ell_{2,\mu_{\pi}}(\bm{\eta},\bm{\eta}^{\prime})$ for any $\bm{\eta},\bm{\eta}^{\prime}\in\prn{\sP^{\sgn}}^{\gS}$.
% In addition, $\bm{\eta}\in\prn{\sP^{\sgn}}^{\gS}$, $\bPi_{\bphi, K}^{\pi}\bm{\eta}=\bPi_{\bphi, K}^{\pi}\bPi_{K}\bm{\eta}$, for any $\bm{\eta}\in\prn{\sP^{\sgn}}^{\gS}$.
The following proposition characterizes $\bPi_{\bphi, K}^{\pi}$, whose proof can be found in Appendix~\ref{appendix:linear-cate-project-op}.
\begin{proposition}\label{prop:linear_projection}
For any $\bm{\eta}\in(\sP^{\sgn})^{\gS}$, $\bPi_{\bphi, K}^{\pi}\bm{\eta}$ is uniquely give by $ \bm{\eta}_{\tilde\bw}$, where
% the matrix version of the parameter $\tilde \bW$ is given by
                    \begin{equation*}
      \tilde{\bw}=\vect(\tilde\bW),\quad \tilde\bW=\bSigma_{\bphi}^{-1}\EB_{s\sim\mu_{\pi}}[\bphi(s)\prn{\bp_{\bm{\eta}}(s)-(K+1)^{-1}\bm{1}_{K}}^{\intercal}].
    \end{equation*}
\end{proposition}

\subsection{Linear-Categorical Projected Bellman Equation}
Since $\bPi_{\bphi, K}^{\pi}\gT^{\pi}$ is a $\sqrt{\gamma}$-contraction in the Polish space $(\sP_{\bphi,K}^{\sgn},\ell_{2,\mu_\pi})$, we have the following theorem, whose proof can be found in Appendix~\ref{subsection:proof_linear_cate_TD_equation}.
\begin{theorem}\label{thm:linear_cate_TD_equation}
The linear-categorical projected Bellman equation  $\bm{\eta}_{\bw}=\bPi_{\bphi, K}^{\pi}\gT^{\pi}\bm{\eta}_{\bw}$ admits a unique solution $\bm{\eta}_{\bw^{\star}}$, where the matrix version of the parameter $\bW^{\star}\in\RB^{d\times K}$ is the unique solution to the linear system for $\bW\in\RB^{d\times K}$
\begin{equation}\label{eq:fixed_point_equation}
    \bSigma_{\bphi}\bW-\EB_{s,s^\prime,r}\brk{\bphi(s)\bphi(s^\prime)^{\intercal}\bW\tilde{\bG}^{\intercal}(r)}=\frac{1}{K+1}\EB_{s,r}\brk{\bphi(s)\prn{\sum_{j=0}^K\bg_j(r)-\bm{1}_{K}}^{\intercal}},
\end{equation}
where for any $r\in[0,1]$ and $j,k\in\brc{0,1\cdots,K}$,
\begin{equation*}
     g_{j,k}(r):=\prn{1-\abs{(r+\gamma x_j-x_k)/{\iota_K}}}_+, \quad \bg_j(r):=\prn{g_{j,k}(r)}_{k=0}^{K-1}\in\RB^{K},
\end{equation*}
% \begin{equation*}
%      g_{j,k}(r):=\prn{1-\abs{\frac{r+\gamma x_j-x_k}{\iota_K}}}_+, \quad \bg_j(r):=\prn{g_{j,k}(r)}_{k=0}^{K-1}\in\RB^{K},
% \end{equation*}
\begin{equation*}
    \bG(r):=\begin{bmatrix}
\bg_0(r), & \cdots, & \bg_{K-1}(r)
\end{bmatrix}\in\RB^{K\times K},\quad \tilde{\bG}(r):=\bG(r)-\bm{1}_K^{\intercal}\otimes\bg_K(r)\in\RB^{K\times K}.
\end{equation*}
\end{theorem}
In analogy to approximation results of $\|\bV^\pi-\bV_{\bpsi^\star} \|_{\mu_\pi}$ and $\ell_{2,\mu_\pi}(\bm{\eta}^\pi,\bm{\eta}^{\pi,K})$, 
the following lemma answers how close $\bm{\eta}_{\bw^{\star}}$ is to $\bm{\eta}^\pi$, whose proof can be found in Appendix~\ref{appendix:proof_approx_error}.
\begin{proposition}
[Approximation Error of $\bm{\eta}_{\bw^{\star}}$]\label{prop:approx_error}
Consider the $\mu_\pi$-weighted $1$-Wasserstein error $W_{1,\mu_{\pi}}^2\prn{\bm{\eta}^\pi,\bm{\eta}_{\bw^{\star}}}:=(\EB_{s\sim\mu_{\pi}}[W_1^2({\eta}^\pi(s),{\eta}_{\bw^{\star}}(s))])^{1/2}$, it holds that
%    \begin{equation*}
%     \begin{aligned}
%         \ell_{2,\mu_{\pi}}^2\prn{\bm{\eta}^\pi,\bm{\eta}_{\bw^{\star}}}\leq& K^{-1}(1-\gamma)^{-2}+(1-\gamma)^{-1}\ell_{2,\mu_{\pi}}^2\prn{\bPi_K\bm{\eta}^{\pi},\bPi_{\bphi, K}^{\pi}\bm{\eta}^{\pi}},
%     \end{aligned}
% \end{equation*}
   \begin{equation*}
    \begin{aligned}
        W_{1,\mu_{\pi}}^2\prn{\bm{\eta}^\pi,\bm{\eta}_{\bw^{\star}}}\leq& K^{-1}(1-\gamma)^{-3}+(1-\gamma)^{-2}\ell_{2,\mu_{\pi}}^2\prn{\bPi_K\bm{\eta}^{\pi},\bPi_{\bphi, K}^{\pi}\bm{\eta}^{\pi}},
    \end{aligned}
\end{equation*}
where the first error term $K^{-1}(1-\gamma)^{-3}$ is due to the categorical parametrization, and the second error term $(1-\gamma)^{-2}\ell_{2,\mu_{\pi}}^2(\bPi_K\bm{\eta}^{\pi},\bPi_{\bphi, K}^{\pi}\bm{\eta}^{\pi})$ is due to the additional linear function approximation. 
\end{proposition}
% Since we desire a small error $\varepsilon\in(0,1)$, we will assume $K\geq (1-\gamma)^{-3}$ from now on.
\subsection{Update Scheme of Linear-Categorical TD Learning}
As before, we solve Eqn.~\eqref{eq:fixed_point_equation} by LSA and get {\LCTD} given the streaming data  $\brc{\prn{s_t,a_t,r_t}}_{t=0}^\infty$:
% \begin{equation}\label{eq:linear_CTD}
% \begin{aligned}
% \bW_t=&\bW_{t-1}-\alpha\bphi(s_t)\brk{\bphi(s_t)^{\intercal}\bW_{t-1}-\bphi(s_{t+1})^{\intercal}\bW_{t-1}\tilde{\bG}^{\intercal}(r_t)-\frac{1}{K+1}\prn{\sum_{j=0}^K\bg_j(r_t)-\bm{1}_{K}}^{\intercal}}\\
% =&\bW_{t-1}-\alpha\bphi(s_t)\prn{\bp_{\bw_{t-1}}(s_t)-\bp_{\brk{\prn{b_{r_t,\gamma}}_\#{{\eta}_{\bw_{t-1}}}}(s_{t+1})}}^{\intercal},
% \end{aligned}
% \end{equation}
\begin{equation}\label{eq:linear_CTD}
\begin{aligned}
\bW_t{=}\bW_{t{-}1}{-}\alpha\bphi(s_t)\!\!\brk{\bphi(s_t)^{\intercal}\bW_{t{-}1}{-}\bphi(s_{t{+}1})^{\intercal}\bW_{t{-}1}\tilde{\bG}^{\intercal}(r_t)-\frac{1}{K{+}1}\!\prn{\sum_{j=0}^K\bg_j(r_t){-}\bm{1}_{K}}^{\intercal}},
\end{aligned}
\end{equation}
for any $t\geq 1$, where $\alpha$ is the constant step size.
% and the second equality is justified by Eqn.~\eqref{eq:project_bellman_w}.
In this paper, we also consider the Polyak-Ruppert tail averaging $\bar{\bw}_{T}:=\prn{T/2+1}^{-1}\sum_{t=T/2}^T\bw_t$ as in the analysis of {\LTD} in \citep{samsonov2024improved}.
Standard theory of LSA \citep{mou2020linear} tells us, under some conditions, if we take an appropriate constant step size, $\bar{\bw}_{T}$ will converges to the solution $\bw^\star$ with rate $T^{-1/2}$ as $T\to\infty$.
\begin{remark}[Comparison with Existing Works]
Our {\LCTD} can be regarded as a preconditioned version of SSGD with the PMF representation considered in \citep{bellemare2019distributional} and \citep[Section~9.6][]{bdr2022}.
The preconditioning technique \citep{chen2005matrix, li2017preconditioned} is a commonly used methodology to accelerate solving optimization problems by reducing condition number.
We precondition the vanilla SSGD with the PMF representation by removing the matrix $\bC^{\intercal}\bC$ on the right of Eqn.~\eqref{eq:ssgd_pmf}, whose condition number scales with $K^2$ (Lemma~\ref{lem:spectra_of_CTC}).
By introducing the preconditioner $\prn{\bC^{\intercal}\bC}^{-1}$, our {\LCTD} (Eqn.~\eqref{eq:linear_CTD}) can achieve a convergence rate independent of $K$, which the vanilla form cannot achieve (see Appendix~\ref{Appendix:convergece_ssgd_pmf}).
It is worth noting that our {\LCTD} (Eqn.~\eqref{eq:linear_CTD}) is equivalent to the stochastic semi-gradient descent (SSGD) with CDF representation, which was also considered in \citep{lyle2019comparative}.
See Appendix~\ref{Appendix:cdf_representation} for the CDF representation, and Appendix~\ref{appendix:equiv_ssgd_lctd} for a self-contained derivation of SSGD with different representations.
We further comment that our {\LCTD} can guarantee that the total mass of return distributions always be $1$, while the algorithms proposed in \citep{lyle2019comparative} cannot.
\end{remark}

