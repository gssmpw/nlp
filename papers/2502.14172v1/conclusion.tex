In this paper, we have bridged a critical theoretical gap in distributional reinforcement learning by establishing the non-asymptotic sample complexity of distributional TD learning with linear function approximation. 
% Specifically, we have reformulated distributional TD learning with linear function approximation, derived directly via linear stochastic approximation of the linear-categorical projected Bellman equation.
Specifically, we have proposed {\LCTD}, derived via solving the linear-categorical projected Bellman equation with linear stochastic approximation.
By carefully analyzing the Bellman equation and using the exponential stability arguments, we have shown tight sample complexity bounds for the proposed algorithm in both generative model setting and Markovian setting. 
Our finite-sample rates match the state-of-the-art sample complexity bounds for the classic TD learning.
These theoretical findings demonstrate that learning the full return distribution under linear function approximation can be statistically as easy as classic TD learning for value function estimation.
Finally, we noted that it is possible to improve the convergence rates by applying variance-reduction techniques or using polynomial-decaying step sizes, which we leave as a future work.