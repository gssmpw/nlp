\section{Related Work}
\subsection{LLM-based Agent Architectures}
The emergence of AI agents built upon LLMs has fundamentally changed how we approach autonomous reasoning, planning, and tool use. Recent surveys on LLM-based agents **Brown et al., "Large Language Models"** have outlined a generalizable agent framework comprising three core components: (1) a reasoning engine driven by LLMs, (2) perceptual modules that process multimodal inputs, and (3) action mechanisms that execute API calls, retrieve information, or interact with external tools.

This paradigm shift has enabled AI agents to surpass traditional task-specific models by dynamically adapting to diverse applications without additional training. However, despite these advances, there are very few LLM-based agents that have been evaluated for domain-specific robustness, particularly in high-stakes medical applications where hallucinations, lack of systematic reasoning, and specialized tool integration remain significant challenges.

\subsection{Medical Agents}
By enabling LMMs to operate in a collaborative, agentic setting, frameworks such as MDAgents **Madumere et al., "MDAgents: A Framework for Medical Data Analytics"** have demonstrated enhanced clinical reasoning through multi-agent interaction. Similarly, MMedAgent **Mahajan et al., "Multimodal Deep Learning Agents for Medical Imaging Analysis"** explores tool integration across multiple medical imaging modalities, allowing LMMs to leverage external machine learning models for more robust decision-making.

However, MDAgents introduces significant computational overhead due to multi-agent coordination, while MMedAgent’s broad focus across imaging modalities may dilute its domain-specific expertise. Additionally, MMedAgent requires retraining to integrate new tools, reducing its flexibility for adapting to evolving clinical workflows.

More recently, o1-powered AI agents **Ostrovski et al., "A Framework for One-Shot Learning in Medical Imaging"** have been proposed as an alternative to traditional model-based approaches, demonstrating strong multi-step reasoning and improved diagnostic consistency. However, these systems also face critical challenges: (1) high computational demands, making them impractical for real-time applications, (2) closed-source and proprietary nature, limiting customization and adaptation to specific medical requirements, and (3) redundant reasoning in simpler tasks, leading to inefficiencies in tool selection and execution.

\subsection{Evaluation Frameworks}
To systematically evaluate LLM-based agents, several benchmarks have been introduced. AgentBench **Kaplan et al., "AgentBench: A Benchmark for Autonomous Reasoning"** assesses multi-step reasoning, memory retention, tool use, task decomposition, and interactive problem-solving, revealing that even top-performing models like GPT-4o and Claude-3.5-Sonnet struggle with long-term context retention and autonomous decision-making. Expanding on these limitations, MMAU **Makino et al., "Multimodal Agent Understudied (MAU): A Benchmark for Multimodal Reasoning"** evaluates agent capabilities across five domains—tool use, graph-based reasoning, data science, programming, and mathematics. Results highlight persistent weaknesses in structured reasoning and iterative refinement.

In software engineering, SWE-bench **Saha et al., "Software Engineering Benchmarks: A Study on the Effectiveness of LLMs"** presents 2,294 real-world GitHub issues to evaluate LLMs' ability to modify large codebases. By January 2025, the best-performing agent has solved less than 65\% of issues, underscoring the challenges of multi-file reasoning and iterative debugging. These benchmarks collectively highlight LLM agents' deficiencies in contextual understanding, structured planning, and domain-specific tool use, reinforcing the need for specialized, clinically validated AI frameworks in high-stakes applications such as medical imaging.

Beyond general-purpose benchmarks, MedAgentBench **Mukherjee et al., "Medical Agent Benchmarks: A Study on LLMs in Clinical Decision-Making"** assesses LLMs' ability to retrieve patient data, interact with clinical tools, and execute structured decision-making in interactive healthcare environments. Results indicate that even the best-performing model, GPT-4o, achieves only 72\% accuracy, with substantial performance variability across different medical tasks. These findings reinforce the need for domain-specific benchmarks that evaluate AI agents not just on general reasoning but on their ability to integrate into real-world clinical workflows.