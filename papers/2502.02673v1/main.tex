\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem} % for customizing lists

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MedRAX: Medical Reasoning Agent for Chest X-ray}

\begin{document}

\twocolumn[
\icmltitle{MedRAX: Medical Reasoning Agent for Chest X-ray}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{advising}{\dag}

\begin{icmlauthorlist}
\icmlauthor{Adibvafa Fallahpour}{equal,tor,vec,uhn}
\icmlauthor{Jun Ma}{equal,vec,uhn}
\icmlauthor{Alif Munim}{equal,uhn,coh}
\icmlauthor{Hongwei Lyu}{uhn}
\icmlauthor{Bo Wang}{advising,tor,vec,uhn,lmp}

\end{icmlauthorlist}
\icmlaffiliation{tor}{Department of Computer Science, University of Toronto, Toronto, Canada}
\icmlaffiliation{vec}{Vector Institute, Toronto, Canada}
\icmlaffiliation{uhn}{University Health Network, Toronto, Canada}
\icmlaffiliation{coh}{Cohere For AI, Toronto, Canada}
\icmlaffiliation{lmp}{Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, Canada}
\icmlcorrespondingauthor{Adibvafa Fallahpour}{adibvafa.fallahpour@mail.utoronto.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML, healthcare, medical, agent, multimodal, chest X-ray, benchmark}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art  CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at \url{https://github.com/bowang-lab/MedRAX}.
\end{abstract}

\section{Introduction}
\label{Introduction}

Chest X-rays (CXRs) have been widely used to make critical decisions in disease detection, diagnosis, and monitoring, comprising the largest proportion of over 4.2 billion diagnostic radiology procedures performed annually worldwide~\cite{unscear2022}. However, the systematic evaluation of key anatomical structures places a significant time burden on radiologists, often requiring hours of careful analysis \cite{bahl2020interpretation}. \\

The gradual introduction of AI into clinical practice has demonstrated promising potential to alleviate this burden. Task-specific AI models have shown success in automating various aspects of CXR interpretation, from classification and segmentation to automated report generation \cite{yang2017chexnet, huang2023generative, tanno2024collaboration, ouis2024chestbiox}. When integrated into clinical workflows, these tools have improved report turnaround times and interobserver agreement \cite{baltruschat2021smart, ahn2022association, pham2022accurate, shin2023impact}. However, the fragmented nature of these solutions—each operating in isolation—has hindered their widespread adoption in practical clinical settings \cite{erdal2023integrationimplementationstrategiesai, fallahpour2024ehrmambageneralizablescalablefoundation}.

Foundation models (FMs), including large language models (LLMs) and large multimodal models (LMMs), have emerged as a promising solution to this fragmentation, enabling unified, scalable AI-driven image-text reasoning for medical tasks. OpenAI's GPT-4 played a pivotal role in establishing the dominance of this approach with its unprecedented scale. Trained on an enormous volume of multimodal data, it has shown exceptional performance in medical understanding and reasoning without explicit training \cite{nori2023capabilities, yan2023multimodal, javan2024gpt, eriksen2024use}. LLaVA-Med \cite{li2024llava}, trained on 15 million biomedical figure-caption pairs, established new benchmarks in medical visual question answering (VQA), showcasing strong generalization in zero-shot image interpretation. CheXagent \cite{chen2024chexagent} focused specifically on CXR analysis, achieving performance comparable to GPT-4 despite using significantly fewer parameters. 

While FMs have significantly advanced the field, they face critical limitations that hinder their direct clinical application. LMMs frequently experience hallucinations and inconsistencies in their reasoning, particularly concerning for medical applications where accuracy is paramount. They also struggle with the complex multi-step reasoning required for diagnostic tasks, often failing to systematically evaluate all relevant anatomical structures or integrate findings across different regions of the image. Their end-to-end architecture, while elegant, lacks the transparency and specialization that comes from purpose-built medical AI tools. These limitations suggest the need for a more structured, tool-based approach that can combine the flexibility of foundation models with the reliability of clinical AI systems.

To bridge this gap, we present MedRAX, the first specialized AI agent framework for CXR interpretation. Our key contributions include:
\begin{itemize}
    \item MedRAX, a specialized AI agent framework that seamlessly integrates multiple CXR analysis tools without additional training, dynamically orchestrating specialized components for complex medical queries.
    
    \item ChestAgentBench, a comprehensive evaluation framework with 2,500 complex medical queries across 7 categories, built from 675 expert-curated clinical cases to assess multi-step reasoning in CXR interpretation.
    
    \item Experiments show that MedRAX outperforms both general-purpose and biomedical specialist models, demonstrating substantial improvements in complex reasoning tasks while maintaining transparent workflows.
    
    \item Development of a user-friendly interface, enabling flexible deployment options from local to cloud-based solutions that address healthcare privacy requirements.
\end{itemize}


\section{Related Work}

\subsection{LLM-based Agent Architectures}
The emergence of AI agents built upon LLMs has fundamentally changed how we approach autonomous reasoning, planning, and tool use. Recent surveys on LLM-based agents \cite{xi2025rise, zhao2023depth, masterman2024landscape} have outlined a generalizable agent framework comprising three core components: (1) a reasoning engine driven by LLMs, (2) perceptual modules that process multimodal inputs, and (3) action mechanisms that execute API calls, retrieve information, or interact with external tools.

This paradigm shift has enabled AI agents to surpass traditional task-specific models by dynamically adapting to diverse applications without additional training. However, despite these advances, there are very few LLM-based agents that have been evaluated for domain-specific robustness, particularly in high-stakes medical applications where hallucinations, lack of systematic reasoning, and specialized tool integration remain significant challenges.

\subsection{Medical Agents}
By enabling LMMs to operate in a collaborative, agentic setting, frameworks such as MDAgents \cite{kim2024mdagents} have demonstrated enhanced clinical reasoning through multi-agent interaction. Similarly, MMedAgent \cite{li2024mmedagent} explores tool integration across multiple medical imaging modalities, allowing LMMs to leverage external machine learning models for more robust decision-making.

However, MDAgents introduces significant computational overhead due to multi-agent coordination, while MMedAgent’s broad focus across imaging modalities may dilute its domain-specific expertise. Additionally, MMedAgent requires retraining to integrate new tools, reducing its flexibility for adapting to evolving clinical workflows.

More recently, o1-powered AI agents \cite{jaech2024openai} have been proposed as an alternative to traditional model-based approaches, demonstrating strong multi-step reasoning and improved diagnostic consistency. However, these systems also face critical challenges: (1) high computational demands, making them impractical for real-time applications, (2) closed-source and proprietary nature, limiting customization and adaptation to specific medical requirements, and (3) redundant reasoning in simpler tasks, leading to inefficiencies in tool selection and execution.

\subsection{Evaluation Frameworks}
To systematically evaluate LLM-based agents, several benchmarks have been introduced. AgentBench \cite{liu2023agentbench} assesses multi-step reasoning, memory retention, tool use, task decomposition, and interactive problem-solving, revealing that even top-performing models like GPT-4o and Claude-3.5-Sonnet struggle with long-term context retention and autonomous decision-making. Expanding on these limitations, MMAU \cite{yin2024mmau} evaluates agent capabilities across five domains—tool use, graph-based reasoning, data science, programming, and mathematics. Results highlight persistent weaknesses in structured reasoning and iterative refinement.

In software engineering, SWE-bench \cite{jimenez2023swe} presents 2,294 real-world GitHub issues to evaluate LLMs' ability to modify large codebases. By January 2025, the best-performing agent has solved less than 65\% of issues, underscoring the challenges of multi-file reasoning and iterative debugging. These benchmarks collectively highlight LLM agents' deficiencies in contextual understanding, structured planning, and domain-specific tool use, reinforcing the need for specialized, clinically validated AI frameworks in high-stakes applications such as medical imaging.

Beyond general-purpose benchmarks, MedAgentBench \cite{2501.14654} assesses LLMs' ability to retrieve patient data, interact with clinical tools, and execute structured decision-making in interactive healthcare environments. Results indicate that even the best-performing model, GPT-4o, achieves only 72\% accuracy, with substantial performance variability across different medical tasks. These findings reinforce the need for domain-specific benchmarks that evaluate AI agents not just on general reasoning but on their ability to integrate into real-world clinical workflows.


\section{MedRAX}
\label{sec:MedRAX}
We present MedRAX, an open-source agent-based framework that can dynamically reason, plan, and execute multi-step CXR workflows. Compared to previous approaches \cite{chexagent, bansal2024medmaxmixedmodalinstructiontuning}, MedRAX integrates multimodal reasoning abilities with structured tool-based decision-making, allowing real-time CXR interpretation without unnecessary computational overhead. By balancing computational efficiency with domain specialization and eliminating the need for retraining when incorporating new tools, MedRAX offers greater adaptability to evolving clinical needs.
Our framework integrates heterogeneous machine learning models—from lightweight classifiers to large LMMs—specialized for diverse downstream tasks, allowing it to decompose and solve complex medical queries by reasoning across multiple analytical skills (\autoref{fig:react-loop}).



\begin{figure}[t!]
\centering
\includegraphics[width=0.95\linewidth]{figures/fig1_medrax.png}
\caption{\textbf{Architecture of MedRAX.} The framework implements a ReAct loop that processes user queries by integrating short-term memory (LangChain) and specialized medical tools for visual QA (CheXagent \cite{chexagent}, LLaVA-Med \cite{li2024llava}), segmentation (MedSAM \cite{medsam, sam}, ChestX-Det \cite{ChestX-Det, pspnet}), grounding (Maira-2 \cite{maira2}), report generation (model trained on CheXpert Plus \cite{chexpert, chexpert-plus}), classification (TorchXRayVision \cite{torchxrayvision1, torchxrayvision2}), and image generation (RoentGen \cite{roentgen}).}
\label{fig:react-loop}
\end{figure}


\begin{algorithm}[h!]
    \caption{MedRAX ReAct Framework}
    \label{alg:algorithm1}
\begin{algorithmic}
    \STATE {\bfseries Input:}
    \STATE $Q$: User query
    \STATE $I$: Set of input CXR images (can be empty)
    \STATE $T$: Available medical AI tools
    \STATE $M$: Memory buffer
    \STATE $t_{max}$: Maximum allowed time
    \STATE {\bfseries Output:}
    \STATE $R$: Final response to query
    \STATE \textbf{Initialize:}
    \STATE $t_{start} = \text{GetCurrentTime}()$
    \STATE state $= \text{Observe}(Q, I, M)$
    \WHILE{$\text{GetCurrentTime}() - t_{start} < t_{max}$}
        \STATE thoughts $= \text{Reason}(state, M)$
        \IF{$\text{RequiresUserInput}(thoughts)$}
            \STATE {\bfseries return} $\text{GenerateUserPrompt}(thoughts, M)$
        \ENDIF
        \IF{$\text{CanGenerateResponse}(thoughts)$}
            \STATE {\bfseries return} $\text{GenerateResponse}(thoughts, M)$
        \ENDIF
        \STATE tool $= \text{SelectTool}(thoughts, T, M)$
        \STATE result $= \text{Execute}(tool, state)$
        \STATE $M = M \cup \{(thoughts, tool, result)\}$
        \STATE state $= \text{Observe}(state, result, M)$
    \ENDWHILE
    \STATE {\bfseries return} $\text{GenerateTimeoutResponse}(state, M)$
\end{algorithmic}
\end{algorithm}


\subsection{LLM Driven Agent}
MedRAX employs a LLM as the core to drive a ReAct (Reasoning and Acting) loop, which breaks down complex medical queries into sequential analytical steps \cite{react}. The system processes a user query through iterative cycles of (1) observation - analyzing the current state and query, (2) thought - determining required actions, and (3) action - executing relevant tools and integrating findings from previous steps to inform subsequent reasoning. Throughout this process, the system maintains a short-term memory of user interactions, tool outputs, and images to support multi-turn interactions. The reasoning loop continues until the system either generates a response or asks the user for additional input (Algorithm \autoref{alg:algorithm1}).

\subsection{Flexible Tool Integration}
MedRAX integrates state-of-the-art models for various downstream CXR interpretation tasks:
\begin{itemize}[leftmargin=10pt,
                topsep=2pt,
                partopsep=0pt,
                itemsep=2pt]
    \item \textbf{Visual Question Answering (VQA)}. \\
    Answering free-form questions about CXR images by combining visual understanding with medical knowledge.
    
    \textit{Models}: CheXagent, a vision-language foundation model trained on CheXinstruct, with over 8.5M samples across 35 tasks, capable of fine-grained visual reasoning and CXR interpretation \cite{chexagent}.
    
    LlaVA-Med, a biomedical 7B VLM, trained on 600K biomedical image-caption pairs from PMC-15M and 60K instruction-tuning data \cite{li2024llava}.
    
    \item \textbf{Segmentation}. \\ Partitioning CXR images into semantically meaningful regions by assigning each region to anatomical structures.
    
    \textit{Models}: MedSAM, a state-of-the-art biomedical segmentation model trained on 1,570,263 medical image-mask pairs, covering 10 imaging modalities and over 30 cancer types \cite{medsam, sam}.
    
    PSPNet model trained on ChestX-Det dataset, consisting of 3,578 images from NIH ChestX-14, annotated with 13 common categories of diseases or abnormalities \cite{ChestX-Det, pspnet}.
    
    \item \textbf{Grounding}. \\ Localizing specific visual regions in medical images that correspond to given textual descriptions or findings.
    
    \textit{Model}: Maira-2, a 7B VLM trained on MIMIC-CXR, PadChest, and USMix datasets, excellent in grounding specific phrases or generating findings of a radiology report with or without grounding \cite{maira2}.

    \item \textbf{Report Generation}. \\
    Writing radiology reports with findings and impressions.
    
    \textit{Model}: A SwinV2 Transformer with a two-layer BERT decoder trained on 223K expert-annotated reports from CheXpert Plus dataset to generate findings and impressions \cite{chexpert, chexpert-plus}.
    
    \item \textbf{Disease Classification}. \\ Detecting and classifying pathologies and abnormalities.
    
    \textit{Model}: A DenseNet-121 model from the TorchXRayVision library, trained on NIH ChestX-ray, CheXpert, MIMIC-CXR, and PadChest datasets. It can predict 18 pathology classes including Pneumonia, Pneumothorax, Edema, Effusion, and Nodule \cite{torchxrayvision1, torchxrayvision2}.
    
    \item \textbf{Chest X-ray Generation}. \\ Synthesizing realistic CXR images from text descriptions of anatomical features and pathologies.
    
    \textit{Model}: RoentGen, a medical vision-language model adapted from Stable Diffusion, trained on the MIMIC-CXR dataset, generates diverse, high-fidelity chest X-rays given text prompts \cite{roentgen}.

    \item \textbf{Utilities}. \\ Processing DICOM images, generating custom plots, and visualizing figures to user.

\end{itemize}

The agent continuously monitors tool outputs and errors, incorporating these results into its reasoning loop to inform subsequent tool selection. Through its memory, MedRAX caches tool outputs to prevent redundant computations, optimizing performance in multi-step analyses that might reference the same intermediate results.

The framework supports parallel execution of independent tools while providing flexible deployment configurations - tools can be quantized for efficiency and distributed across CPU or GPU resources. \autoref{fig:interface} shows an example user interaction with MedRAX.


\begin{figure}[t!]
\includegraphics[width=0.99\linewidth]{figures/fig2_demo.png}
\caption{\textbf{MedRAX Interaction Flow.} An example of how MedRAX handles a multi-turn conversation through its ReAct loop (\textless thought\textgreater, \textless action\textgreater, \textless observation\textgreater) along with tool outputs and final response. For clarity, the production interface shows only tool outputs and agent responses.}
\label{fig:interface}
\end{figure}


\subsection{Modularity}
MedRAX is built on the LangChain and LangGraph frameworks. The reasoning engine can be any LLM, accommodating both text-only and multimodal models, from open-source to proprietary. This flexibility enables deployments ranging from local installations to cloud-based solutions, addressing diverse healthcare privacy requirements. Our reference implementation uses GPT-4o with vision, while supporting integration of alternative models.

Each tool operates as an independent module with defined loading and inference. Tools can be modified, replaced, or repurposed for multiple tasks without affecting other components. Integration of new tools requires only a class definition specifying the tool's input/output formats and capabilities, with the LLM learning its usage without any training. The framework decouples tool creation from agent instantiation, enabling multiple agents to share tools and allowing each to access its own customized set of tools.


\subsection{User-friendly Interface}
MedRAX includes a production-ready interface built with Gradio that facilitates seamless deployment in clinical settings. The interface supports uploading of radiological images in all standard formats, including DICOM, and maintains an interactive chat session for natural multi-turn interactions. The interface further provides transparency into tool execution by tracking and displaying intermediate outputs. This end-to-end implementation enables quick integration of MedRAX into existing clinical workflows.


\begin{figure*}[t!]
   \centering
    \includegraphics[width=0.99\linewidth]{figures/fig3_benchmark.png}
   \caption{\textbf{Overview of ChestAgentBench.} (a) Benchmark creation pipeline that uses GPT-4o to generate 2,500 six-choice questions from 675 Eurorad clinical cases. (b) Gender distribution, showing 55.4\% male, 44.1\% female, and 0.45\% unknown. (c) Age distribution, a bimodal with a mean age of 46.0 years (SD=20.4, median=47.0 years). (d) Distribution of anatomical areas of interest across cases, with lung (51.2\%), thorax (42.8\%), and mediastinum (15.8\%) representing the most frequently examined regions from 53 unique areas.}
   \label{fig:benchmark}
\end{figure*}



\section{ChestAgentBench}
\label{sec:benchmark}

Existing medical VQA benchmarks typically focus on simple, single-step reasoning tasks. In contrast, ChestAgentBench offers several distinctive advantages:
\begin{itemize}[leftmargin=10pt, itemsep=0pt, topsep=0pt]
    \item It represents one of the largest medical VQA benchmarks, with 2,500 questions derived from expert-validated clinical cases, each with comprehensive radiological findings, detailed discussions, and multi-modal imaging data.
    
    \item The benchmark combines complex multi-step reasoning assessment with a structured six-choice format, enabling both rigorous evaluation of advanced reasoning capabilities and straightforward, reproducible evaluation.
    
    \item The benchmark features diverse questions across seven core competencies in CXR interpretation, requiring integration of multiple visual findings and reasoning to mirror the complexity of real-world clinical decision-making.
\end{itemize}


\subsection{Dataset}
We utilize \href{https://www.eurorad.org/}{Eurorad}, the largest peer-reviewed radiological case report database maintained by the European Society of Radiology (ESR). This database contains detailed clinical cases consisting of patient histories, clinical presentations, and multi-modal imaging findings. Each case includes detailed radiological interpretations across different modalities, complemented by in-depth discussions that connect findings with clinical context, and concludes with reasoned interpretations, differential diagnosis list and a final diagnoses.

From its chest imaging section, we curated 675 patient cases with associated chest X-rays and complete clinical documentation. These cases covered 53 unique areas of interest including lung, thorax, and mediastinum. \autoref{fig:benchmark} provides an overview of the benchmark, showing (a) the creation pipeline, (b) patient gender distribution, (c) age distribution, and (d) most frequent anatomical areas of interest.

\subsection{Benchmark Creation}
ChestAgentBench comprises six-choice questions, each designed to evaluate complex CXR interpretation capabilities.

We first established seven core competencies alongside reasoning that are essential for CXR interpretation:

\begin{itemize}[leftmargin=10pt, itemsep=0pt, topsep=0pt]
   \item \textbf{Detection:} Identifying specific findings. (e.g., ``Is there a nodule present in the right upper lobe?")
   \item \textbf{Classification:} Classifying specific findings. (e.g., ``Is this mass benign or malignant in appearance?")   
   \item \textbf{Localization:} Precise positioning of findings. (e.g., ``In which bronchopulmonary segment is the mass located?")
   \item \textbf{Comparison:} Analyzing relative sizes and positions. (e.g., ``How has the pleural effusion volume changed compared to prior imaging?")
   \item \textbf{Relationship:} Understanding relationship of findings. (e.g., ``Does the mediastinal lymphadenopathy correlate with the lung mass?")
   \item \textbf{Diagnosis:} Interpreting findings for clinical decisions. (e.g., ``Given the CXR, what is the likely diagnosis?")
   \item \textbf{Characterization:} Describing specific finding attributes. (e.g., ``What are the margins of the nodule - smooth, spiculated, or irregular?")
   \item \textbf{Reasoning:} Explaining medical rationale and thought. (e.g., ``Why do these findings suggest infectious rather than malignant etiology?")
\end{itemize}

These competencies are combined into five question types, each designed to evaluate specific combinations of core competencies while requiring medical reasoning:
\begin{itemize}[leftmargin=10pt, itemsep=0pt]
   \item \textbf{Detailed Finding Analysis:}  detection, localization, and characterization
   \item \textbf{Pattern Recognition \& Relations:}  detection, classification, and relationships
   \item \textbf{Spatial Understanding:} localization, comparison, and relationships 
   \item \textbf{Clinical Decision Making:}  classification, comparison, and diagnosis
   \item \textbf{Diagnostic Characterization:}  classification, characterization, and diagnosis
\end{itemize}

For each clinical case and question type, we first prompted GPT-4o to analyze the case and generate a six-choice question that would best assess the target analytical skills of that question type. We then instructed it to ensure the question has the necessary context from the clinical case and its correct answer could be explicitly verified from the case's radiological findings and discussion.

The benchmark uses a straightforward accuracy metric (percentage of correct answers) to enable easy evaluation across different agent architectures. All questions underwent quality check, during which we removed questions that exhibited issues such as ungrounded answers or missing information.


\section{Experiments}
\label{Experiments}


\begin{table*}[t!]
\label{tab:results-chestagentbench}
\centering
\caption{Model Performance on ChestAgentBench. Performance of five vision-language models (LLaVA-Med \cite{li2024llava}, CheXagent \cite{chen2024chexagent}, Llama-3.2-90B, GPT-4o, and MedRAX) compared across seven categories of our 2,500-question benchmark. MedRAX significantly outperforms both general-purpose models and specialized biomedical models across all categories.\\}
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}lccccc@{}}
\toprule
\textbf{Categories} & \textbf{LLaVA-Med} & \textbf{CheXagent}  & \textbf{Llama-3.2-90B} & \textbf{GPT-4o} & \textbf{MedRAX} \\
\midrule
Detection & 32.4 & 38.7 &  58.1 & \underline{58.7} & \textbf{64.1} \\
Classification & 30.8 & 34.7 &  \underline{56.5} & 54.6 & \textbf{62.9} \\
Localization & 30.2 & 42.5  & \underline{59.9} & 59.0 & \textbf{63.6} \\
Comparison & 30.6 & 38.5  & \underline{57.5} & 55.5 & \textbf{61.8} \\
Relationship & 31.8 & 39.8  & \underline{59.3} & 59.0 & \textbf{63.1} \\
Diagnosis & 29.3 & 33.5  & \underline{55.9} & 52.6 & \textbf{62.5} \\
Characterization & 28.8 & 34.2  & \underline{58.0} & 56.1 & \textbf{64.0} \\
\midrule
\textbf{Overall} & 28.7 & 39.5  & \underline{57.9} & 56.4 & \textbf{63.1} \\
\bottomrule
\end{tabular*}
\end{table*}


\begin{table*}[h!]
\label{tab:results-chexbench}
\centering
\caption{Model Performance on CheXbench. Performance of five vision-language models (LLaVA-Med \cite{li2024llava}, CheXagent \cite{chen2024chexagent}, Llama-3.2-90B, GPT-4o, and MedRAX) compared on 238 Visual QA (Rad-Restruct and SLAKE) and 380 Image-Text Reasoning questions (OpenI). MedRAX excels in VQA while achieving the best overall performance. \\}
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}lccccc@{}}
\toprule
\textbf{Categories} & \textbf{LLaVA-Med} & \textbf{CheXagent} & \textbf{Llama-3.2-90B} & \textbf{GPT-4o} & \textbf{MedRAX} \\
\midrule
Visual QA & & & & & \\
\hspace{5mm} Rad-Restruct & 34.9 & 57.1 & \underline{62.6} & 53.9 & \textbf{68.7} \\
\hspace{5mm} SLAKE & 55.5 & 78.1 & 74.0 & \textbf{85.4} & \underline{82.9} \\
Fine-Grained Reasoning & 45.8 & \textbf{59.0} & 49.2 & 51.1 & \underline{52.6} \\
\midrule
\textbf{Overall} & 45.4 & \underline{64.7} & 61.9 & 63.5 & \textbf{68.1} \\
\bottomrule
\end{tabular*}
\end{table*}



\subsection{Implementations}
MedRAX uses GPT-4o as its backbone LLM and is deployed on a single NVIDIA RTX 6000 GPU, using the same configuration as described in Section 3. It integrates CheXagent \cite{chexagent} and LLaVA-Med \cite{li2024llava} for visual QA, Maira-2 for grounding \cite{maira2}, a model trained on ChestX-Det for segmentation \cite{ChestX-Det}, TorchXRayVision for classification \cite{torchxrayvision1}, and a model trained on CheXpert Plus for report generation \cite{chexpert-plus}.

MedRAX implements tool execution with structured JSON API calls, where the agent formulates precise requests with required arguments (e.g., image paths, text prompts) to call target tools. All baseline models are evaluated using their official implementations and recommended configurations.

Model responses are processed using regex to extract letter choices. For unclear responses, errors, or timeouts, we retry up to three times. Responses that remain invalid or do not choose a single choice are marked incorrect.

\subsection{Experimental Setup}
We evaluate MedRAX against four models: LLaVA-Med, a finetuned LLaVA-13B model for biomedical visual question answering (\cite{li2024llava}, CheXagent, a Vicuna-13B VLM trained for CXR interpretation \cite{chexagent}, along with GPT-4o and Llama-3.2-90B Vision as popular closed and open-source multimodal LLMs respectively.

We evaluate models on two complementary benchmarks:

(1) ChestAgentBench, our proposed benchmark described in Section \ref{sec:benchmark}, which assesses comprehensive CXR reasoning through 2,500 six-choice questions across seven categories: detection, classification, localization, comparison, relationship, characterization, and diagnosis. Model performance is measured by accuracy across all questions.

(2) CheXbench, a popular benchmark that evaluates seven clinically-relevant CXR interpretation tasks. We specifically focus on the visual question answering (238 questions from Rad-Restruct \cite{pellegrini2023radrestructnovelvqabenchmark} and SLAKE \cite{liu2021slakesemanticallylabeledknowledgeenhanceddataset} datasets) and fine-grained image-text reasoning (380 questions from OpenI dataset) subsets, as they most closely mirror complex clinical workflows that require precise differentiation between similar findings.


\subsection{Quantitative Analysis}
\textbf{ChestAgentBench.} Shown in Table~\hyperref[tab:results-chestagentbench]{1}, MedRAX achieves consistently state-of-the-art performance (63\%) across all seven categories, a significant improvement over the baseline models. There is a clear performance hierarchy among models, with GPT-4o (56.4\%) and Llama-3.2-90B (57.9\%) performing notably better than specialized medical models like CheXagent (39.5\%) \cite{chen2024chexagent} and LLaVA-Med (28.7\%) \cite{li2024llava}. Interestingly, general-purpose VLMs outperform domain-specific ones across all categories, with particularly large gaps in characterization and diagnosis tasks.

\textbf{CheXbench.} Shown in Table~\hyperref[tab:results-chexbench]{2}, we observe distinct performance patterns across different task types. On visual QA tasks, MedRAX demonstrates strong performance on Rad-Restruct (68.7\%) and SLAKE (82.9\%). This notably surpasses both domain-specific CheXagent (57.1\%, 78.1\%) \cite{chen2024chexagent} and larger general-purpose models like GPT-4o (53.9\%, 85.4\%), suggesting that our tool-based approach particularly excels at fine-grained visual understanding. However, on image-text reasoning tasks, we observe a significant performance drop across all models, with even the best-performing CheXagent achieving only 59.0\% accuracy, almost equal to random performance (50\% baseline).


\begin{figure*}[h!]
    \centering
    \label{fig:case}
    \includegraphics[width=\linewidth]{figures/fig4_case.png}
    \caption{\textbf{MedRAX and GPT-4o Case Study.} (Case 17576) Correct answer is chest tube. GPT-4o incorrectly identifies as endotracheal tube based on position, while MedRAX correctly identifies chest tube by integrating multiple tool outputs, even resolving conflicting tool suggestions. (Case 16703) Correct answer is left pneumothorax. GPT-4o misdiagnoses as right-sided pneumonia/edema, while MedRAX correctly identifies left pneumothorax through sequential tool application for disease detection and comparative lung analysis.}
\end{figure*}

\subsection{Case Studies}
We present two representative cases that compare MedRAX to GPT-4o (Figure~\ref{fig:case}).

\textbf{Medical Device Identification (Eurorad Case 17576)}. \\
This question asks the model to determine the type of tube present in the CXR. GPT-4o incorrectly suggests an endotracheal tube based on the central positiong of the tube alone. MedRAX, integrated findings from multiple tools like report generation and visual QA, and correctly identifies a chest tube despite one tool (LLaVA-Med \cite{li2024llava}) suggesting otherwise. This demonstrates MedRAX's ability to resolve conflicting tool outputs through systematic reasoning.

\textbf{Multi-step Disease Diagnosis (Eurorad Case 16703)}. \\
This questions asks about diagnosing the predominant disease and comparing its severity across lungs. GPT-4o misinterprets the CXR as showing pneumonia with right lung predominance. MedRAX, through sequential tool application of report generation for disease identification and segmentation for lung opacity analysis, correctly determines left pneumothorax as the main finding. This demonstrates MedRAX's ability to break down complex queries into targeted analytical steps.


\section{Discussion}
\label{Discussion}

Our experiments demonstrate that MedRAX achieves state-of-the-art performance in complex CXR interpretation tasks, outperforming both general-purpose and specialized medical models. We discover valuable insights about structured tool orchestration in medical AI, suggesting that a hybrid approach—leveraging both large-scale reasoning capabilities and domain-specific expertise—offers superior performance over purely end-to-end models.

\textbf{Task Decomposition}. MedRAX demonstrates that a ReAct-based architecture dynamically composes complex reasoning chains while maintaining computational efficiency. The performance gap between MedRAX and end-to-end models suggests that explicit decomposition of reasoning steps provides advantages that scale alone cannot achieve. The process produces clear decision traces, enhancing transparency and interpretability, with implications beyond medical imaging for structured model-tool integration.

\textbf{Generalists Versus Specialists}. A key insight is the superior performance of general-purpose models (GPT-4o, Llama-3.2-90B) over specialized medical models (LLaVA-Med \cite{li2024llava}, CheXagent \cite{chen2024chexagent}). This suggests that medical model specialization may sacrifice broader reasoning capabilities provided by large-scale pretraining. MedRAX bridges this gap by integrating domain-specific tools while maintaining generalist reasoning, demonstrating the benefits of hybrid architectures.

\textbf{Limitations.} While MedRAX excels in structured reasoning, it sometimes struggles with resolving contradictory tool outputs, particularly in fine-grained visual tasks when classification and segmentation tools provide conflicting interpretations of subtle patterns. Additionally, the system's computational overhead from running multiple specialized tools can impact response times compared to end-to-end models. The framework also lacks robust uncertainty quantification mechanisms. 

\textbf{Future Work.} Our initial observations suggest the importance of balanced tool utilization, where neither complete reliance on tools nor their complete absence produced optimal results. While formal analysis is needed, our findings indicate that prompting strategies encouraging critical evaluation of tool outputs may play a key role in system performance. This presents an interesting direction for understanding the interaction between LLM reasoning and tool utilization. Future work would consider applying reinforcement learning to boost reasoning ability, as verified in DeepSeek-R1 \cite{guo2025deepseek}. Additionally, comprehensive clinical validation will be crucial for establishing MedRAX's practical utility in real-world settings.

\section{Conclusion}
\label{Conclusion}
MedRAX establishes a new benchmark in AI-driven CXR interpretation by integrating structured tool orchestration with large-scale reasoning. Our evaluation on ChestAgentBench demonstrates its superiority over both general-purpose and domain-specific models, reinforcing the advantages of explicit stepwise reasoning in medical AI. These findings highlight the potential of combining foundation models with specialized tools, a principle that could be applied to broader domains in healthcare and beyond. Future work should focus on optimizing tool selection, uncertainty-aware reasoning, and expanding MedRAX’s capabilities to multimodal medical imaging for greater clinical impact.

\clearpage
\section{Acknowledgements}
We thank Mohammed Baharoon for their assistance in preprocessing the Eurorad dataset. We are particularly grateful to Karan Singhal and Shekoofeh Azizi for their thorough feedback on ChestAgentBench's design, which significantly improved the benchmark's clinical relevance and evaluation methodology. We also thank Yubin Kim for their insightful perspectives on medical AI agent architectures. This work was supported by the University of Toronto, University Health Network (UHN) and Vector Institute.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\bibliography{main}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
