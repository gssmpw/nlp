\section{Related Work}
{\bf Physical Commonsense Reasoning.}~Commonsense knowledge, embedded in a variety of data, is acquired by humans and used for reasoning about unseen things____. Hespos {\it et al.}____ show that infants' commonsense aids in reasoning about knowledge, and machines can similarly learn and perform well on physical knowledge____. Machines can acquire commonsense from various data types, including visual____, textual____, audio____, and multimodal data____. 
Zellers~{\it et al.}____ constructed a visual question-answering~(VQA) dataset for visual commonsense reasonng~(VCR), guiding models to utilize learned commonsense knowledge for high-level cognition and reasoning beyond images. Wang~{\it et al.}____ proposed an unsupervised approach to mine visual commonsense, enhancing model performance on visual captioning and VQA. Zareian~{\it et al.} ____ proposed the first method to automatically acquire visual commonsense such as affordance and intuitive physics from data for scene graph generation. Li~{\it et al.}____ further introduced a video-based VQA dataset, \emph{Video-VQA}, which not only involves reasoning questions about video content but also generates appropriate justifications based on commonsense knowledge. Bisk~{\it et al.}____ firstly proposed the task of learning physical commonsense from text and constructed a corresponding benchmark dataset, \emph{PIQA}. Lin~{\it et al.}____ explored the usage of commonsense knowledge in human-like chatbots with multi-modal context. However, most work focused on learning visual and audio commonsense knowledge, with a lack of learning the physics from visual objects. Yu~{\it et al.}____ introduced a multimodal physical commonsense knowledge dataset based on visual, audio, and text, \emph{PACS}, and performed the VQA task related to the physical commonsense in a fusion manner. In contrast, our proposed method decouples physical commonsense into static and dynamic aspects and introduces causal learning to enhance reasoning ability for physical problems. 

\noindent
{\bf Disentangled Representation Learning~(DRL).}~DRL aims to learn various hidden explanatory factors behind observable data____, and it has been widely applied in computer vision____, including image recognition____, visual reasoning____, and generation____,. Tran~{\it et al.}____ employed a Generative Adversarial Network (GAN)____ to explicitly disentangle facial variations, addressing face recognition across diverse human poses.  Similarly, Wei~{\it et al.}____ utilized a Variational Autoencoder~(VAE) to disentangle actions within videos, enhancing unsupervised cross-domain action recognition by decoupling videos into domain-specific and domain-invariant features. Moreover, disentangled representation has been leveraged in image generation. Ma~{\it et al.} ____ disentangled images into foreground, background, and pose information, generating new person images based on these manipulated factors through a multi-branch reconstruction network and adversarial training. Differing from static image processing, Bai~{\it et al.}____ and Zhu~{\it et al.}____ investigated video generation by disentangling and merging static and dynamic character information. Wang~{\it et al.}____ addressed the visual semantic ambiguity problem by decoupling questions into region-related, spatial-related, and semantic-related features. Contrary to previous methods that explicitly model disentangled factors, our work centers on learning the relationships of physical knowledge across different samples. We utilize this knowledge to assist in answering relevant questions, thereby enhancing the model's interpretability.


\noindent{\bf Causal learning.}~Due to the "language prior"____ or "visual  bias"____ in traditional VQA datasets, current methods rely heavily on inherent biases in language or visual features, leading to inaccurate answers. Recently, counterfactual causal reasoning have been utilized in VQA____, scene graph generation____, image recognition____, and video understanding____.  These techniques not only mitigate the impact of data biases on results____, but also enhance model interpretability during inference____. Different from the current work____ focusing VQA with cross-modal modeling, our approach distinctively concentrates on constructing physical knowledge relationships among different samples and employing them as confounders in causal reasoning.

\noindent{\bf Roubst multimodal learning.}~Multimodal learning encompasses various types of data, including visual-text____, visual-audio____, text-audio____, and visual-text-audio____. However in practical applications, data from different modalities may exhibit varying degrees of missing information____, which can lead to the performance decrease of multimodal systems, sometimes even inferior to those of the single-modal approach. In this work, we propose a new robust multi-modal learning that aligns the shared semantic information across different modalities and then utilizes this information to complete the missing modality.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.82\linewidth]{Figures/Fig_2.pdf}
    \vspace{-4mm}
    \caption{The illustration of our proposed DCL model: Part (a) presents the overall structure, which begins with the input of videos accompanied by audio. These are initially encoded via the respective visual and audio encoders. Subsequently, the Disentangled Sequence Encoder in Part (b) is employed to segregate video features into static and dynamic elements utilizing an LSTM-based Variational Autoencoder (VAE). The Counterfactual Learning Module in Part (c) is then used to construct the affinity matrix `A', which acts as a confounder, and to derive the prediction $\hat{Y}_{X, A_X}$ and the counterfactual outcome $\hat{Y}_{X, A^*_X}$. Ultimately, we compute $\hat{Y}_{TIE}$ by subtracting these two outcomes and optimizing the model.}
    \label{fig: an overview of model}
    \vspace{-3mm}
\end{figure*}