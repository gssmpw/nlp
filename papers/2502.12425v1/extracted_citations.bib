@InProceedings{Rao_2021_ICCV,
    author    = {Rao, Yongming and Chen, Guangyi and Lu, Jiwen and Zhou, Jie},
    title     = {Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-Identification},
    booktitle = {Proc. IEEE Int. Conf. Comput. Vis.},
    year      = {2021},
    pages     = {1025-1034}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2425--2433},
  year={2015}
}

@article{bai2021contrastively,
  title={Contrastively disentangled sequential variational autoencoder},
  author={Bai, Junwen and Wang, Weiran and Gomes, Carla P},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={10105--10118},
  year={2021}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{chen2021curriculum,
  title={Curriculum disentangled recommendation with noisy multi-feedback},
  author={Chen, Hong and Chen, Yudong and Wang, Xin and Xie, Ruobing and Wang, Rui and Xia, Feng and Zhu, Wenwu},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={26924--26936},
  year={2021}
}

@article{deshmukh2023pengi,
  title={Pengi: An audio language model for audio tasks},
  author={Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={36},
  pages={18090--18108},
  year={2023}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6904--6913},
  year={2017}
}

@inproceedings{goyal2019counterfactual,
  title={Counterfactual visual explanations},
  author={Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={2376--2384},
  year={2019},
}

@article{hespos2016five,
  title={Five-month-old infants have general knowledge of how nonsolid substances behave and interact},
  author={Hespos, Susan J and Ferry, Alissa L and Anderson, Erin M and Hollenbeck, Emily N and Rips, Lance J},
  journal={Psychological Science},
  volume={27},
  number={2},
  pages={244--256},
  year={2016},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{hu2021class,
  title={Class-aware Sounding Objects Localization via Audiovisual Correspondence},
  author={Hu, Di and Wei, Yake and Qian, Rui and Lin, Weiyao and Song, Ruihua and Wen, Ji-Rong},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2021},
  publisher={IEEE}
}

@inproceedings{li2022learning,
  title={Learning to answer questions in dynamic audio-visual scenarios},
  author={Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={19108--19118},
  year={2022}
}

@inproceedings{li2022representation,
  title={From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering},
  author={Li, Jiangtong and Niu, Li and Zhang, Liqing},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={21273--21282},
  year={2022}
}

@inproceedings{li2023progressive,
  title={Progressive spatio-temporal perception for audio-visual question answering},
  author={Li, Guangyao and Hou, Wenxuan and Hu, Di},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={7808--7816},
  year={2023}
}

@inproceedings{lin2023tiktalk,
  title={TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World},
  author={Lin, Hongpeng and Ruan, Ludan and Xia, Wenke and Liu, Peiyu and Wen, Jingyuan and Xu, Yixin and Hu, Di and Song, Ruihua and Zhao, Wayne Xin and Jin, Qin and others},
  booktitle={Proc. ACM Int. Conf. on Multimedia},
  pages={1303--1313},
  year={2023}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={32},
  year={2019}
}

@inproceedings{ma2018disentangled,
  title={Disentangled person image generation},
  author={Ma, Liqian and Sun, Qianru and Georgoulis, Stamatios and Van Gool, Luc and Schiele, Bernt and Fritz, Mario},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={99--108},
  year={2018}
}

@article{piloto2022intuitive,
  title={Intuitive physics learning in a deep-learning model inspired by developmental psychology},
  author={Piloto, Luis S and Weinstein, Ari and Battaglia, Peter and Botvinick, Matthew},
  journal={Nature human behaviour},
  volume={6},
  number={9},
  pages={1257--1267},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{qi2019attentive,
  title={Attentive relational networks for mapping images to scene graphs},
  author={Qi, Mengshi and Li, Weijian and Yang, Zhengyuan and Wang, Yunhong and Luo, Jiebo},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={3957--3966},
  year={2019}
}

@inproceedings{qi2019ke,
  title={KE-GAN: Knowledge embedded generative adversarial networks for semi-supervised scene parsing},
  author={Qi, Mengshi and Wang, Yunhong and Qin, Jie and Li, Annan},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={5237--5246},
  year={2019}
}

@article{qi2020stc,
  title={STC-GAN: Spatio-temporally coupled generative adversarial networks for predictive scene parsing},
  author={Qi, Mengshi and Wang, Yunhong and Li, Annan and Luo, Jiebo},
  journal={IEEE Trans. Image Process.},
  volume={29},
  pages={5420--5430},
  year={2020},
  publisher={IEEE}
}

@article{qi2021semantics,
  title={Semantics-aware spatial-temporal binaries for cross-modal video retrieval},
  author={Qi, Mengshi and Qin, Jie and Yang, Yi and Wang, Yunhong and Luo, Jiebo},
  journal={IEEE Trans. Image Process.},
  volume={30},
  pages={2989--3004},
  year={2021},
  publisher={IEEE}
}

@article{sun2023unbiased,
  title={Unbiased scene graph generation via two-stage causal modeling},
  author={Sun, Shuzhou and Zhi, Shuaifeng and Liao, Qing and Heikkil{\"a}, Janne and Liu, Li},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  volume={45},
  number={10},
  pages={12562--12580},
  year={2023},
  publisher={IEEE}
}

@inproceedings{tang2020unbiased,
  title={Unbiased scene graph generation from biased training},
  author={Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={3716--3725},
  year={2020}
}

@inproceedings{tran2017disentangled,
  title={Disentangled representation learning gan for pose-invariant face recognition},
  author={Tran, Luan and Yin, Xi and Liu, Xiaoming},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={1415--1424},
  year={2017}
}

@article{van2019disentangled,
  title={Are disentangled representations helpful for abstract visual reasoning?},
  author={Van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber, J{\"u}rgen and Bachem, Olivier},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={32},
  year={2019}
}

@inproceedings{wang2020visual,
  title={Visual commonsense r-cnn},
  author={Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10760--10770},
  year={2020}
}

@article{wang2023disavr,
  title={DisAVR: Disentangled Adaptive Visual Reasoning Network for Diagram Question Answering},
  author={Wang, Yaxian and Wei, Bifan and Liu, Jun and Zhang, Lingling and Wang, Jiaxin and Wang, Qianying},
  journal={IEEE Trans. Image Process.},
  year={2023},
  publisher={IEEE}
}

@inproceedings{wang2023multi,
  title={Multi-modal learning with missing modality via shared-specific feature modelling},
  author={Wang, Hu and Chen, Yuanhong and Ma, Congbo and Avery, Jodie and Hull, Louise and Carneiro, Gustavo},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={15878--15887},
  year={2023}
}

@article{wang2024rdfc,
  title={RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion},
  author={Wang, Haowen and Che, Zhengping and Yang, Yufan and Wang, Mingyuan and Xu, Zhiyuan and Qiao, Xiuquan and Qi, Mengshi and Feng, Feifei and Tang, Jian},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2024},
  publisher={IEEE}
}

@article{wei2024unsupervised,
  title={Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective},
  author={Wei, Pengfei and Kong, Lingdong and Qu, Xinghua and Ren, Yi and Xu, Zhiqiang and Jiang, Jing and Yin, Xiang},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={36},
  year={2024}
}

@article{xu2022unintentional,
  title={Unintentional action localization via counterfactual examples},
  author={Xu, Jinglin and Chen, Guangyi and Lu, Jiwen and Zhou, Jie},
  journal={IEEE Trans. Image Process.},
  volume={31},
  pages={3281--3294},
  year={2022},
  publisher={IEEE}
}

@inproceedings{xue2023variational,
  title={Variational Causal Inference Network for Explanatory Visual Question Answering},
  author={Xue, Dizhan and Qian, Shengsheng and Xu, Changsheng},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2515--2525},
  year={2023}
}

@inproceedings{yu2022pacs,
  title={PACS: A Dataset for Physical Audiovisual CommonSense Reasoning},
  author={Yu, Samuel and Wu, Peter and Liang, Paul Pu and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  booktitle={Proc. Eur. Conf. Comput. Vis},
  pages={292--309},
  year={2022},
}

@inproceedings{zareian2020learning,
  title={Learning visual commonsense for robust scene graph generation},
  author={Zareian, Alireza and Wang, Zhecan and You, Haoxuan and Chang, Shih-Fu},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={642--657},
  year={2020},
}

@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6720--6731},
  year={2019}
}

@inproceedings{zellers2022merlot,
  title={Merlot reserve: Neural script knowledge through vision and language and sound},
  author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={16375--16387},
  year={2022}
}

@inproceedings{zhu2020s3vae,
  title={S3vae: Self-supervised sequential vae for representation disentanglement and data generation},
  author={Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6538--6547},
  year={2020}
}

