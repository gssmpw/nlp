\section{Related Work}
{\bf Physical Commonsense Reasoning.}~Commonsense knowledge, embedded in a variety of data, is acquired by humans and used for reasoning about unseen things~\cite{piloto2022intuitive}. Hespos {\it et al.}~\cite{hespos2016five} show that infants' commonsense aids in reasoning about knowledge, and machines can similarly learn and perform well on physical knowledge~\cite{piloto2022intuitive}. Machines can acquire commonsense from various data types, including visual~\cite{zellers2019recognition,li2022representation}, textual~\cite{bisk2020piqa}, audio\cite{zellers2022merlot}, and multimodal data~\cite{yu2022pacs}. 
Zellers~{\it et al.}~\cite{zellers2019recognition} constructed a visual question-answering~(VQA) dataset for visual commonsense reasonng~(VCR), guiding models to utilize learned commonsense knowledge for high-level cognition and reasoning beyond images. Wang~{\it et al.}\cite{wang2020visual} proposed an unsupervised approach to mine visual commonsense, enhancing model performance on visual captioning and VQA. Zareian~{\it et al.} ~\cite{zareian2020learning} proposed the first method to automatically acquire visual commonsense such as affordance and intuitive physics from data for scene graph generation. Li~{\it et al.}~\cite{li2022representation} further introduced a video-based VQA dataset, \emph{Video-VQA}, which not only involves reasoning questions about video content but also generates appropriate justifications based on commonsense knowledge. Bisk~{\it et al.}~\cite{bisk2020piqa} firstly proposed the task of learning physical commonsense from text and constructed a corresponding benchmark dataset, \emph{PIQA}. Lin~{\it et al.}~\cite{lin2023tiktalk} explored the usage of commonsense knowledge in human-like chatbots with multi-modal context. However, most work focused on learning visual and audio commonsense knowledge, with a lack of learning the physics from visual objects. Yu~{\it et al.}~\cite{yu2022pacs} introduced a multimodal physical commonsense knowledge dataset based on visual, audio, and text, \emph{PACS}, and performed the VQA task related to the physical commonsense in a fusion manner. In contrast, our proposed method decouples physical commonsense into static and dynamic aspects and introduces causal learning to enhance reasoning ability for physical problems. 

\noindent
{\bf Disentangled Representation Learning~(DRL).}~DRL aims to learn various hidden explanatory factors behind observable data~\cite{bengio2013representation}, and it has been widely applied in computer vision~\cite{chen2021curriculum}, including image recognition~\cite{wei2024unsupervised,qi2020stc}, visual reasoning~\cite{van2019disentangled,qi2021semantics}, and generation~\cite{ma2018disentangled, bai2021contrastively, wang2024rdfc,qi2019attentive,qi2019ke},. Tran~{\it et al.}~\cite{tran2017disentangled} employed a Generative Adversarial Network (GAN)~\cite{goodfellow2020generative} to explicitly disentangle facial variations, addressing face recognition across diverse human poses.  Similarly, Wei~{\it et al.}\cite{wei2024unsupervised} utilized a Variational Autoencoder~(VAE) to disentangle actions within videos, enhancing unsupervised cross-domain action recognition by decoupling videos into domain-specific and domain-invariant features. Moreover, disentangled representation has been leveraged in image generation. Ma~{\it et al.} ~\cite{ma2018disentangled} disentangled images into foreground, background, and pose information, generating new person images based on these manipulated factors through a multi-branch reconstruction network and adversarial training. Differing from static image processing, Bai~{\it et al.}~\cite{bai2021contrastively} and Zhu~{\it et al.}~\cite{zhu2020s3vae} investigated video generation by disentangling and merging static and dynamic character information. Wang~{\it et al.}\cite{wang2023disavr} addressed the visual semantic ambiguity problem by decoupling questions into region-related, spatial-related, and semantic-related features. Contrary to previous methods that explicitly model disentangled factors, our work centers on learning the relationships of physical knowledge across different samples. We utilize this knowledge to assist in answering relevant questions, thereby enhancing the model's interpretability.


\noindent{\bf Causal learning.}~Due to the "language prior"~\cite{goyal2017making} or "visual  bias"~\cite{antol2015vqa} in traditional VQA datasets, current methods rely heavily on inherent biases in language or visual features, leading to inaccurate answers. Recently, counterfactual causal reasoning have been utilized in VQA~\cite{goyal2019counterfactual}, scene graph generation~\cite{tang2020unbiased}, image recognition~\cite{Rao_2021_ICCV}, and video understanding~\cite{xu2022unintentional}.  These techniques not only mitigate the impact of data biases on results~\cite{sun2023unbiased}, but also enhance model interpretability during inference~\cite{xue2023variational}. Different from the current work~\cite{li2023progressive} focusing VQA with cross-modal modeling, our approach distinctively concentrates on constructing physical knowledge relationships among different samples and employing them as confounders in causal reasoning.

\noindent{\bf Roubst multimodal learning.}~Multimodal learning encompasses various types of data, including visual-text~\cite{lu2019vilbert}, visual-audio~\cite{hu2021class}, text-audio~\cite{deshmukh2023pengi}, and visual-text-audio~\cite{li2022learning}. However in practical applications, data from different modalities may exhibit varying degrees of missing information~\cite{wang2023multi}, which can lead to the performance decrease of multimodal systems, sometimes even inferior to those of the single-modal approach. In this work, we propose a new robust multi-modal learning that aligns the shared semantic information across different modalities and then utilizes this information to complete the missing modality.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.82\linewidth]{Figures/Fig_2.pdf}
    \vspace{-4mm}
    \caption{The illustration of our proposed DCL model: Part (a) presents the overall structure, which begins with the input of videos accompanied by audio. These are initially encoded via the respective visual and audio encoders. Subsequently, the Disentangled Sequence Encoder in Part (b) is employed to segregate video features into static and dynamic elements utilizing an LSTM-based Variational Autoencoder (VAE). The Counterfactual Learning Module in Part (c) is then used to construct the affinity matrix `A', which acts as a confounder, and to derive the prediction $\hat{Y}_{X, A_X}$ and the counterfactual outcome $\hat{Y}_{X, A^*_X}$. Ultimately, we compute $\hat{Y}_{TIE}$ by subtracting these two outcomes and optimizing the model.}
    \label{fig: an overview of model}
    \vspace{-3mm}
\end{figure*}