
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{float}
\usepackage{colortbl}
\usepackage{graphicx} 
\usepackage{subcaption}
\def\etal{\emph{et al.}}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\newcommand{\LCS}[1]{\textcolor{orange}{[LCS: #1]}}
\begin{document}
\renewcommand\arraystretch{1.5}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{\title{Supplementary Materials of  \\ \emph{Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning}}}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{
        Mengshi Qi,~\IEEEmembership{Member,~IEEE},
        Changsheng Lv,
        Huadong Ma,~\IEEEmembership{Fellow,~IEEE}
\thanks{This work is partly supported by the Funds for the NSFC Project under Grant 62202063, Beijing Natural Science Foundation (L243027). (\emph{Corresponding author: Mengshi Qi~(email:~qms@bupt.edu.cn)})}
\thanks{M. Qi, C. Lv, and H. Ma are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China.}
}
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Transactions on Pattern Analysis and Machine Intelligence}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
% \begin{abstract}
% In this paper, we propose a Disentangled Counterfactual Learning~(DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. 
% To address these issues, our proposed DCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model's reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. Our proposed method is a plug-and-play module that can be incorporated into any baseline. In experiments, we show that our proposed method improves baseline methods and achieves state-of-the-art performance. 
% \end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Physical Commonsense Reasoning, Robust Multimodal Learning, Disentangled Representation, Counterfactual Analysis.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


In this supplementary material, we provide a comprehensive algorithm underlying our proposed model, encompassing both the DCL and RDCL in Section~\ref{Sec. Algorithm of DCL and RDCL}. Section~\ref{Sec: Drivations and more experimental results} includes derivations and supplementary experimental results. Additionally, Section~\ref{Sec: VLM-Assisted Reasoning Dataset} presents more samples and statistical analyses of the VLM-Assisted Reasoning Dataset introduced in our main paper.
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\section{Algorithm of DCL and RDCL}
In this section, we introduce the detailed algorithms for Disentangled Counterfactual
Learning (DCL) in Section~\ref{Sec. Algorithm of DCL} and Robust Disentangled Counterfactual
Learning (RDCL) in Section~\ref{Sec. Algorithm of RDCL} for Physical Commonsense Reasoning.  
\label{Sec. Algorithm of DCL and RDCL}
\subsection{DCL}
\label{Sec. Algorithm of DCL}
The overall framework of the proposed DCL algorithm is outlined in Algorithm~\ref{algorithm: DCL}. The model takes as input a training batch consisting of paired video-audio data along with associated physical knowledge questions. It outputs the final prediction, denoted as $\hat{Y}_{TIE}$.  

\subsection{RDCL}
\label{Sec. Algorithm of RDCL}
Unlike DCL, which processes complete multimodal inputs, RDCL is designed to handle incomplete modalities. As an illustrative example, we consider scenarios where audio data are missing. The corresponding algorithm is presented in Algorithm~\ref{algorithm: RDCL}.
\begin{algorithm}[ht]
\caption{Disentangled Counterfactual Learning~(DCL) Batch-Wise Training}
\label{algorithm: DCL}
\SetAlgoLined
\KwIn{
    Training batch $\{ \langle v_1, v_2 \rangle_i, \langle a_1, a_2 \rangle_i, q_i \}_{i=1}^{B}$, \\
    Batch size $B$, \\
    Pretrained image encoder $\mathcal{E}_{\text{img}}(\theta)$, \\
    Pretrained audio encoder $\mathcal{E}_{\text{aud}}(\theta)$, \\
    Pretrained text encoder $\mathcal{E}_{\text{text}}(\theta)$, \\
    Labels $\{Y_{GT,i}\}_{i=1}^B$, \\
    Number of frames $T$
}
\KwOut{
    Predicted answers $\{\hat{Y}_{TIE,i}\}_{i=1}^B$
}


Encode features: \\
  \For{$j \in \{1, 2\}$}{ 
   \quad $X^{v_j} = \{X_1^{v_j}, X_2^{v_j}, \cdots, X_T^{v_j}\} \gets \mathcal{E}_{\text{img}}(v_j)$ \\ 
   \quad $X^{a_j} \gets \mathcal{E}_{\text{aud}}(a_j)$
   }
   $X^t \gets \mathcal{E}_{\text{text}}(q)$ \\

\For{each sample in the batch}{
    Disentangle static factors $X^v_s$ and dynamic factors $X^v_z$ from $X^v$ via DSE in Section 4.2. \\
}

Compute the adjacency matrix $A_X$ using Eq. (15), (16), and (17). \\

Obtain the fused feature $F_1$, $F_2$ using Eq.(14). \\

Construct intervened features $X^*$ using Eq. (20), and compute the intervened adjacency matrix ${A}^*$ using Eqs. (15), (16) and (17). \\

Predict the $\hat{Y}_{X, A_X}$ and $\hat{Y}_{X^*, A_{X^*}}$ using Eq.(18)\\

Use $\hat{Y}_{TIE}$ obtained from Eq.(19) as the output.
\end{algorithm}

\section{Drivations and more experimental results}
\label{Sec: Drivations and more experimental results}
\subsection{Approximate Estimation of the Objective Function}

In Section 4.2 Disentangled Sequential Encoder of our main paper, our goal is to maximize the log-likelihood of \( x_{1:T} \). However, due to the computational complexity associated with high-dimensional integrals, directly obtaining \( \log p(x_{1:T}) \) is challenging. To address this issue, we employ the Evidence Lower Bound (ELBO) as an approximation to the log-likelihood. 

{\bf For the input sequence $x_{1:T}$}, Eq.(~\ref{eq:elbo_derivation}) in Figure~\ref{Seq-ELBO} shown adapted from the standard VAE framework \cite{bai2021contrastively}, noticing that either the prior or the approximate posterior factorizes over $s$ and $z_{1:T}$.
{\bf For the entire dataset}, let \( p_D \) represent the empirical data distribution, which assigns a probability mass of \( 1/N \) to each of the \( N \) training sequences in \( D \). The aggregated posteriors are defined as shown in Eq.(~\ref{Eq.2}),  Eq.(~\ref{Eq.3}), and  Eq.(~\ref{Eq.4}) in Figure~\ref{proof_3}. By rearranging terms and applying similar operations to \( x \), we arrive at Eq.~(\ref {Eq.6}) and Eq.~(\ref{Eq.7}) in Figure~\ref{proof_3}. Finally, integrating the above derivations, we obtain the dataset ELBO by subtracting a distinct KL divergence from the data log-likelihood, as illustrated in Eq.~\ref{eq:elbo} in Figure~\ref{proof-4}.
\begin{figure*}
\begin{equation}
\begin{aligned}
& \log p(x_{1:T}) \\
\ge& -KL[q(s,z_{1:T}|x_{1:T})||p(s,z_{1:T}|x_{1:T})]+\log p(x_{1:T})\\
=&\mathbb E_{q(s, z_{1:T}|x_{1:T})} \left[ \log p(s,z_{1:T}|x_{1:T}) - \log q(s,z_{1:T}|x_{1:T}) + \log p(x_{1:T}) \right] \\
=&\mathbb E_{q(s,z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T})-\log q(s,z_{1:T}|x_{1:T})+\log p(s,z_{1:T})]\\
=&\mathbb E_{q(s,z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T})-\log q(s|x_{1:T}) - \log p(z_{1:T}|x_{1:T})+\log p(s) + \log p(z_{1:T})]\\
=&\mathbb E_{q(z_{1:T}, s|x_{1:T})} \left[ \underbrace{\log p(x_{1:T}|s, z_{1:T})}_{\text{Reconstruction term}}-\underbrace{KL[q(s|x_{1:T})||p(s)]}_{s\text{-regression}}-\underbrace{KL[q(z_{1:T}|x_{1:T})||p(z_{1:T})]}_{z\text{-regression}} \right].
\end{aligned}  
\label{eq:elbo_derivation}
\end{equation}
\caption{The ELBO derivation for the input sequence \( x_{1:T} \).}
\label{Seq-ELBO}
\end{figure*}

\begin{figure*}
\centering
\begin{align}
q(s) & = \mathbb E_{x_{1:T}\sim p_D} [q(s|x_{1:T})] = \frac{1}{N} \sum_{x_{1:T}\in D} q(s|x_{1:T}), \label{Eq.2} \\
q(z_{1:T}) & = \mathbb E_{x_{1:T}\sim p_D} [q(z_{1:T}|x_{1:T})] = \frac{1}{N} \sum_{x_{1:T}\in D} q(z_{1:T}|x_{1:T}), \label{Eq.3} \\
q(s,z_{1:T}) & = \mathbb E_{x_{1:T}\sim p_D} [q(s|x_{1:T}) q(z_{1:T}|x_{1:T})] = \frac{1}{N} \sum_{x_{1:T}\in D} q(s|x_{1:T}) q(z_{1:T}|x_{1:T}). \label{Eq.4}
\end{align}
\begin{equation}
\begin{aligned}
& \mathbb E_{x_{1:T}\sim p_D}[KL[q(s|x_{1:T})||p(s)]] \\
=& \mathbb E_{x_{1:T}\sim p_D}\mathbb E_{q(s|x_{1:T})}[\log q(s|x_{1:T}) - \log q(s) + \log q(s) - \log p(s)] \\
=& \mathbb E_{q(x_{1:T}, s)} \log \left[ \frac{q(s|x_{1:T})}{q(s)} \right] + \mathbb E_{q(x_{1:T}, s)} [\log q(s)-\log p(s)] \\
=& I_q (x_{1:T};s) + KL [q(s)||p(s)]. 
\end{aligned}
\label{}
\end{equation}
\begin{equation}
KL [q(s)||p(s)] = \mathbb E_{x_{1:T}\sim p_D}[KL[q(s|x_{1:T})||p(s)]] - I_q(x_{1:T};s). 
\label{Eq.6}
\end{equation}
\begin{gather}
KL[q(z_{1:T})||p(z_{1:T})] = \mathbb E_{x_{1:T}\sim p_D}[KL[q(z_{1:T}|x_{1:T})||p(z_{1:T})]] - I_q(x_{1:T};z_{1:T}). 
\label{Eq.7}
\end{gather}
\caption{Aggregated equations and their relationships.}
\label{proof_3}
\end{figure*}

\begin{figure*}
\begin{equation}
\begin{aligned} 
&\frac{1}{N} \sum_{x_{1:T}\in D} \log p(x_{1:T}) = \mathbb E_{x_{1:T}\sim p_D}[\log p(x_{1:T})] \\
\ge& \mathbb E_{x_{1:T}\sim p_D}[\log p(x_{1:T}) - KL[q(s, z_{1:T})||p(s, z_{1:T}|x_{1:T})]] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}) - \log q(s, z_{1:T}) + \log p(s, z_{1:T}|x_{1:T})]] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}) - \log q(s, z_{1:T}) + \log p(x_{1:T}|s, z_{1:T}) + \log p(s, z_{1:T}) - \log p(x_{1:T})]] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T}) - \log q(s, z_{1:T}) + \log p(s, z_{1:T})]] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T})]] - \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log q(s, z_{1:T}) - \log p(s, z_{1:T})]] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T})]] - KL[q(s, z_{1:T})||p(s, z_{1:T})] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T})]] - I_q(s;z_{1:T}) - KL[q(s)||p(s)] - KL[q(z_{1:T})||p(z_{1:T})] \\
=& \mathbb E_{x_{1:T}\sim p_D}[\mathbb E_{q(s, z_{1:T}|x_{1:T})}[\log p(x_{1:T}|s, z_{1:T})]] \\
&\hspace{5em} - \mathbb E_{x_{1:T}\sim p_D}[KL[q(s|x_{1:T})||p(s)]] - \mathbb E_{x_{1:T}\sim p_D}[KL[q(z_{1:T}|x_{1:T})||p(z_{1:T})]] \\
&\hspace{5em} + I_q(s;x_{1:T}) + I_q(z_{1:T};x_{1:T}) - I_q(s;z_{1:T}).
\end{aligned}
\label{eq:elbo}
\end{equation}
\caption{Derivation of the ELBO for a dataset by subtracting a KL-divergence term from the data log-likelihood.}
\label{proof-4}
\end{figure*}

\subsection{Sensitivity Analysis of Parameters}  
We conducted a sensitivity analysis on the parameters $\gamma$ and $\theta$ as defined in Eq. (6) of the main paper, with results presented in Figure~\ref{fig: hyperparameters}. Specifically, we evaluated $\gamma$ over the range $\{0.01, 0.1, 1, 10\}$ and $\theta$ over the range $\{0.5, 5, 50, 500\}$. The results denote that our proposed DCL method exhibits strong robustness to variations in both $\gamma$ and $\theta$, achieving consistent and stable performance across all tested parameter configurations.
\subsection{Analysis of dynamic factors}
Our DSE+ method separates video features into static (time-invariant) and dynamic (time-varying) factors. Figure~\ref{fig: t-sne} shows t-SNE visualizations of these factors alongside raw video features. While raw features appear scattered, dynamic factors extracted by DSE+ exhibit clear clustering, as highlighted by red circles. For example, Figure~\ref{fig: t-sne}(b) shows objects with similar dynamic characteristics, such as small size and lightweight, positioned adjacently. The upper portion of Figure~\ref{fig: t-sne} illustrates a cluster where actions consistently depict a hand grasping and striking the object, reflecting their lightweight characteristics. In contrast to raw features, DSE+ successfully captures this dynamic information. Similarly, the lower section highlights another cluster with shared thickness-related properties, further demonstrating DSE+’s ability to extract dynamic physical characteristics.  
\subsection{Additional Qualitative Results}
As shown in Figure~\ref{fig: Qualitative Results_1}, we present more visualization results comparing our proposed method with other baseline models. It can be seen from the figures that our proposed DCL method outperforms the original process.
\subsection{Impact of visual bias}
As illustrated in Figure~\ref{fig: visual bias}, we show the absolute accuracy differences for specific object pairs. Accuracy for pairs in the lowest 25\% of occurrence frequency improves notably after applying DCL, demonstrating its effectiveness in reducing visual bias for less frequent pairs. However, for some high-frequency pairs ({\it e.g.}, ``paper-foam'' and ``paper-textiles''), a slight accuracy decrease occurs after DCL. This is because dominant visual bias previously led to correct but unreliable predictions, while DCL mitigates this bias, revealing the model’s robust performance.  

\section{VLM-Assisted Reasoning Dataset}
\label{Sec: VLM-Assisted Reasoning Dataset}
\subsection{More examples of VLM-Assisted Reasoning Dataset}
Figure \ref{fig: VLM} illustrates the prompts used for the Vision-Language Model (VLM), with subfigures (a)–(f) showcasing its assisted reasoning outputs across various samples. A failure case is evident in Figure~\ref{fig: VLM}(f), where the VLM excessively emphasizes object-specific details ({\it e.g.}, identifying the type of wine) while overlooking the physical characteristics of the glass bottle. Future work will focus on developing more targeted prompting strategies to address such limitations.
\subsection{Dataset Statistics}
We obtained corresponding VLM descriptions for each object in the PACS dataset, resulting in 1,526 descriptions. The average length of these descriptions is 74.05 words, with a maximum length of 118 words and a minimum length of 41 words. The corresponding word cloud is illustrated in Figure~\ref{fig: Word Cloud} and the Top-50 Material Types in the Object Pair are shown in Figure~\ref{fig: Frequency of Material Types}. The generated data is available at https://github.com/MICLAB-BUPT/DCL.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/Fig_11.pdf}
    \caption{Absolute differences in accuracy scores between two configurations: AudioCLIP with DCL using solely video input (V w/ DCL) and AudioCLIP utilizing only video input (V). The parenthetical value indicates the frequency of occurrence, measured at 11.5 instances within the final 25\% of the training dataset.}
    \label{fig: visual bias}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/T-SNE.pdf}
    \caption{T-SNE visualization of video features before applying DSE+ (a), along with dynamic factors (b) and static factors (c) obtained after DSE+. The \textcolor{red}{red circles} indicate clusters that have been manually identified as containing samples with similar physical properties. We provide examples of these clusters based on shared attributes, including weight and thickness.}
    \label{fig: t-sne}
\end{figure}
\begin{figure}[b]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/Fig_12.pdf}
   \caption{Word Cloud for VLM-Assisted Reasoning}
    \label{fig: Word Cloud}
\end{figure}
\begin{algorithm}[ht]
\caption{Robust Disentangled Counterfactual Learning (RDCL) Batch-Wise Training}
\label{algorithm: RDCL}
\SetAlgoLined
\KwIn{
    Training batch $\{ \langle v_1, v_2 \rangle_i, \langle a_1, a_2 \rangle_i, q_i \}_{i=1}^{B}$, \\
    Batch size $B$, \\
    Pretrained image encoder $\mathcal{E}_{\text{img}}(\theta)$, \\
    Pretrained audio encoder $\mathcal{E}_{\text{aud}}(\theta)$, \\
    Pretrained text encoder $\mathcal{E}_{\text{text}}(\theta)$, \\
    Labels $\{Y_{GT,i}\}_{i=1}^B$, \\
    Number of frames $T$, proportion of missing data in object 1's video $a_{v1}.$ 
}
\KwOut{
    Predicted answers $\{\hat{Y}_{TIE,i}\}_{i=1}^B$
}

\textbf{Encode features:} \\
\For{$j \in \{1, 2\}$}{ 
   \quad $X^{v_j} = \{X_1^{v_j}, X_2^{v_j}, \cdots, X_T^{v_j}\} \gets \mathcal{E}_{\text{img}}(v_j)$ \\ 
   \quad $X^{a_j} \gets \mathcal{E}_{\text{aud}}(a_j)$
}
$X^t \gets \mathcal{E}_{\text{text}}(q)$ \\

Obtain the set of missing data set $B_{{miss}}$ and the complete data set $B_{{com}}$ using Eq.(27). \\

For each sample $i$ in the batch:

\If{$i \in B_{com}$}{
    \For{each sample in the $B_{{com}}$}{
        Disentangle static factors $X^v_s$ and dynamic factors $X^v_z$ from $X^v$ via DSE in Section 4.2. \\
    }
    
    Use unique encoder and shared encoder to encode $X^v_s$, $X^v_z$, and $X^a$ using Eqs.(24) and (25), obtaining $r_m^{unique}$ and $r_m^{share}, m \in \{a,z,s\}$.
}
\Else{
    Use Eqs. (29) and (30) to complete the missing information.
}

Compute the adjacency matrix $A_X$ using Eqs. (15), (16), and (17). \\

Obtain the fused features $F_1$ and $F_2$ using Eq.(14). \\

Construct the intervened features $X^*$ using Eq. (20), and compute the intervened adjacency matrix ${A}^*$ using Eqs. (15), (16), and (17). \\

Predict $\hat{Y}_{X, A_X}$ and $\hat{Y}_{X^*, A_{X^*}}$ using Eq.(18). \\

Use $\hat{Y}_{TIE}$ obtained from Eq.(19) as the output.
\end{algorithm}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/Fig_13.pdf}
    \caption{Frequency of Material Types for Object Pairs in the PACS Training Set.}
    \label{fig: Frequency of Material Types}
\end{figure}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/Fig_7.pdf}
    \caption{Performance comparisons of various hyperparameters in Eq. (6) are presented. Figures (a) and (b) display the performance of AudioCLIP with different values of $\gamma$ in $\mathcal{L}_{DSE}$ on the PACS and PACS-Material datasets. Figures (c) and (d) show the performance of AudioCLIP with varying $\theta$ in $\mathcal{L}_{DSE}$ on the same datasets.}
    \label{fig: hyperparameters}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/Fig_8.pdf}
    \caption{Qualitative Results of baseline w/ and w/o our proposed method, where “Material” refers to the material of the object. The correct answers are depicted in green while the incorrect ones are depicted in red.}
    \label{fig: Qualitative Results_1}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/Fig_9.pdf}
    \caption{Qualitative Results of baseline w/ and w/o our proposed method, where “Material” refers to the material of the object. The correct answers are depicted in green while the incorrect ones are depicted in red.}
    \label{fig: Qualitative Results_2}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/Fig_10.pdf}
    \caption{Prompt Text, Input Image, and Corresponding Response of the VLM ({\it i.e.}, Doubao-1.5-Vision-Pro)}
    \label{fig: VLM}
\end{figure*}


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{IEEEtran}
\bibliography{reference}
\end{document}


