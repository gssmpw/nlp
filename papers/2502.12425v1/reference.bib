@inproceedings{li2023progressive,
  title={Progressive spatio-temporal perception for audio-visual question answering},
  author={Li, Guangyao and Hou, Wenxuan and Hu, Di},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={7808--7816},
  year={2023}
}

@inproceedings{li2022learning,
  title={Learning to answer questions in dynamic audio-visual scenarios},
  author={Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={19108--19118},
  year={2022}
}

@inproceedings{yun2021pano,
  title={Pano-avqa: Grounded audio-visual question answering on 360deg videos},
  author={Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2031--2041},
  year={2021}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2425--2433},
  year={2015}
}
@inproceedings{shrestha2019answer,
  title={Answer them all! toward universal visual question answering models},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10472--10481},
  year={2019}
}
@inproceedings{zhou2021trar,
  title={Trar: Routing the attention spans in transformer for visual question answering},
  author={Zhou, Yiyi and Ren, Tianhe and Zhu, Chaoyang and Sun, Xiaoshuai and Liu, Jianzhuang and Ding, Xinghao and Xu, Mingliang and Ji, Rongrong},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2074--2084},
  year={2021}
}
@inproceedings{yu2019deep,
  title={Deep modular co-attention networks for visual question answering},
  author={Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6281--6290},
  year={2019}
}
@inproceedings{teney2018tips,
  title={Tips and tricks for visual question answering: Learnings from the 2017 challenge},
  author={Teney, Damien and Anderson, Peter and He, Xiaodong and Van Den Hengel, Anton},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={4223--4232},
  year={2018}
}
@article{jin2021adaptive,
  title={Adaptive spatio-temporal graph enhanced vision-language representation for video QA},
  author={Jin, Weike and Zhao, Zhou and Cao, Xiaochun and Zhu, Jieming and He, Xiuqiang and Zhuang, Yueting},
  journal={IEEE Trans. Image Process.},
  volume={30},
  pages={5477--5489},
  year={2021},
  publisher={IEEE}
}
@inproceedings{yang2021just,
  title={Just ask: Learning to answer questions from millions of narrated videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={1686--1697},
  year={2021}
}
@inproceedings{yang2020bert,
  title={Bert representations for video question answering},
  author={Yang, Zekun and Garcia, Noa and Chu, Chenhui and Otani, Mayu and Nakashima, Yuta and Takemura, Haruo},
  booktitle={Proc. IEEE Winter Conf. Appl. Comput. Vis.},
  pages={1556--1565},
  year={2020}
}

@inproceedings{azuma2022scanqa,
  title={ScanQA: 3D question answering for spatial scene understanding},
  author={Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={19129--19139},
  year={2022}
}
@inproceedings{yu2022pacs,
  title={PACS: A Dataset for Physical Audiovisual CommonSense Reasoning},
  author={Yu, Samuel and Wu, Peter and Liang, Paul Pu and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  booktitle={Proc. Eur. Conf. Comput. Vis},
  pages={292--309},
  year={2022},
}
@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}
@inproceedings{lee2018diverse,
  title={Diverse image-to-image translation via disentangled representations},
  author={Lee, Hsin-Ying and Tseng, Hung-Yu and Huang, Jia-Bin and Singh, Maneesh and Yang, Ming-Hsuan},
  booktitle={Proc. Eur. Conf. Comput. Vi},
  pages={35--51},
  year={2018}
}
@article{zhu2018visual,
  title={Visual object networks: Image generation with disentangled 3D representations},
  author={Zhu, Jun-Yan and Zhang, Zhoutong and Zhang, Chengkai and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Josh and Freeman, Bill},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={31},
  year={2018}
}
@inproceedings{he2017unsupervised,
  title={An unsupervised neural attention model for aspect extraction},
  author={He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={388--397},
  year={2017}
}

@article{chen2021curriculum,
  title={Curriculum disentangled recommendation with noisy multi-feedback},
  author={Chen, Hong and Chen, Yudong and Wang, Xin and Xie, Ruobing and Wang, Rui and Xia, Feng and Zhu, Wenwu},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={26924--26936},
  year={2021}
}
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6904--6913},
  year={2017}
}
@inproceedings{chen2020counterfactual,
  title={Counterfactual samples synthesizing for robust visual question answering},
  author={Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10800--10809},
  year={2020}
}
@inproceedings{teney2020learning,
  title={Learning what makes a difference from counterfactual examples and gradient supervision},
  author={Teney, Damien and Abbasnedjad, Ehsan and van den Hengel, Anton},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={580--599},
  year={2020},
}
@inproceedings{goyal2019counterfactual,
  title={Counterfactual visual explanations},
  author={Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={2376--2384},
  year={2019},
}
@inproceedings{hendricks2018grounding,
  title={Grounding visual explanations},
  author={Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={264--279},
  year={2018}
}
@inproceedings{wang2020scout,
  title={Scout: Self-aware discriminant counterfactual explanations},
  author={Wang, Pei and Vasconcelos, Nuno},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={8981--8990},
  year={2020}
}

@inproceedings{tang2020unbiased,
  title={Unbiased scene graph generation from biased training},
  author={Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={3716--3725},
  year={2020}
}
@inproceedings{chen2019counterfactual,
  title={Counterfactual critic multi-agent training for scene graph generation},
  author={Chen, Long and Zhang, Hanwang and Xiao, Jun and He, Xiangnan and Pu, Shiliang and Chang, Shih-Fu},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={4613--4623},
  year={2019}
}
@inproceedings{abbasnejad2020counterfactual,
  title={Counterfactual vision and language learning},
  author={Abbasnejad, Ehsan and Teney, Damien and Parvaneh, Amin and Shi, Javen and Hengel, Anton van den},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10044--10054},
  year={2020}
}
@article{pandeya2021deep,
  title={Deep learning-based late fusion of multimodal information for emotion classification of music video},
  author={Pandeya, Yagya Raj and Lee, Joonwhoan},
  journal={Multimedia Tools and Applications},
  volume={80},
  pages={2887--2905},
  year={2021},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={8748--8763},
  year={2021},
}

@inproceedings{guzhov2022audioclip,
  title={Audioclip: Extending clip to image, text and audio},
  author={Guzhov, Andrey and Raue, Federico and Hees, J{\"o}rn and Dengel, Andreas},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={976--980},
  year={2022},
  organization={IEEE}
}
@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  year={2020},
  pages={104--120},
}
@inproceedings{zellers2022merlot,
  title={Merlot reserve: Neural script knowledge through vision and language and sound},
  author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={16375--16387},
  year={2022}
}
@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6077--6086},
  year={2018}
}
@article{lu2016hierarchical,
  title={Hierarchical question-image co-attention for visual question answering},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={29},
  year={2016}
}
@inproceedings{wu2021hierarchical,
  title={Hierarchical Semantic Enhanced Directional Graph Network for Visual Commonsense Reasoning},
  author={Wu, Mingyan and Qi, Shuhan and Rao, Jun and Zhang, Jiajia and Liao, Qing and Wang, Xuan and Liao, Xinxin},
  booktitle={Proceedings of the 1st International Workshop on Trustworthy AI for Multimedia Computing},
  pages={27--36},
  year={2021}
}
@inproceedings{liu2022umt,
  title={Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection},
  author={Liu, Ye and Li, Siyuan and Wu, Yang and Chen, Chang-Wen and Shan, Ying and Qie, Xiaohu},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={3042--3051},
  year={2022}
}
@article{kriegeskorte2015deep,
  title={Deep neural networks: a new framework for modeling biological vision and brain information processing},
  author={Kriegeskorte, Nikolaus},
  journal={Annual review of vision science},
  volume={1},
  pages={417--446},
  year={2015},
  publisher={Annual Reviews}
}
@article{ma2022principles,
  title={On the principles of parsimony and self-consistency for the emergence of intelligence},
  author={Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={23},
  number={9},
  pages={1298--1323},
  year={2022},
}
@article{an2024etpnav,
  title={Etpnav: Evolving topological planning for vision-language navigation in continuous environments},
  author={An, Dong and Wang, Hanqing and Wang, Wenguan and Wang, Zun and Huang, Yan and He, Keji and Wang, Liang},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2024},
  publisher={IEEE}
}
@inproceedings{zhao2025decomposed,
  title={Decomposed Vector-Quantized Variational Autoencoder for Human Grasp Generation},
  author={Zhao, Zhe and Qi, Mengshi and Ma, Huadong},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={447--463},
  year={2025},
}
@inproceedings{lugrin2007making,
  title={Making sense of virtual environments: action representation, grounding and common sense},
  author={Lugrin, Jean-Luc and Cavazza, Marc},
  booktitle={Proceedings of the 12th international conference on Intelligent user interfaces},
  pages={225--234},
  year={2007}
}
@inproceedings{chen2019holistic++,
  title={Holistic++ scene understanding: Single-view 3d holistic scene parsing and human pose estimation with human-object interaction and physical commonsense},
  author={Chen, Yixin and Huang, Siyuan and Yuan, Tao and Qi, Siyuan and Zhu, Yixin and Zhu, Song-Chun},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={8648--8657},
  year={2019}
}
@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={Int. J. Comput. Vis.},
  volume={123},
  pages={32--73},
  year={2017},
}
@article{storks2021tiered,
  title={Tiered reasoning for intuitive physics: Toward verifiable commonsense language understanding},
  author={Storks, Shane and Gao, Qiaozi and Zhang, Yichi and Chai, Joyce},
  journal={arXiv preprint arXiv:2109.04947},
  year={2021}
}
@inproceedings{mottaghi2016happens,
  title={“What happens if...” learning to predict the effect of forces in images},
  author={Mottaghi, Roozbeh and Rastegari, Mohammad and Gupta, Abhinav and Farhadi, Ali},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={269--285},
  year={2016},
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={30},
  year={2017}
}
@book{glymour2016causal,
  title={Causal inference in statistics: A primer},
  author={Glymour, Madelyn and Pearl, Judea and Jewell, Nicholas P},
  year={2016},
  publisher={John Wiley \& Sons}
}

@incollection{pearl2022direct,
  title={Direct and indirect effects},
  author={Pearl, Judea},
  booktitle={Probabilistic and causal inference: the works of Judea Pearl},
  pages={373--392},
  year={2022}
}
@article{vanderweele2013three,
  title={A three-way decomposition of a total effect into direct, indirect, and interactive effects},
  author={VanderWeele, Tyler J},
  journal={Epidemiology},
  pages={224--232},
  year={2013},
  publisher={JSTOR}
}


@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}
@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}
@inproceedings{zhao2020learning,
  title={Learning physical common sense as knowledge graph completion via BERT data augmentation and constrained tucker factorization},
  author={Zhao, Zhenjie and Papalexakis, Evangelos and Ma, Xiaojuan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={3293--3298},
  year={2020}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{zhu2020s3vae,
  title={S3vae: Self-supervised sequential vae for representation disentanglement and data generation},
  author={Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6538--6547},
  year={2020}
}
@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={1597--1607},
  year={2020},
}
@article{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={33},
  pages={18661--18673},
  year={2020}
}
@inproceedings{suhr2019corpus,
  title={A Corpus for Reasoning about Natural Language Grounded in Photographs},
  author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6418--6428},
  year={2019}
}
@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={9694--9705},
  year={2021}
}
@article{bobrow1984qualitative,
  title={Qualitative reasoning about physical systems: an introduction},
  author={Bobrow, Daniel G},
  journal={Artificial intelligence},
  volume={24},
  number={1-3},
  pages={1--5},
  year={1984},
  publisher={Elsevier}
}
@article{forbus1984qualitative,
  title={Qualitative process theory},
  author={Forbus, Kenneth D},
  journal={Artificial intelligence},
  volume={24},
  number={1-3},
  pages={85--168},
  year={1984},
  publisher={Elsevier}
}
@article{hespos2016five,
  title={Five-month-old infants have general knowledge of how nonsolid substances behave and interact},
  author={Hespos, Susan J and Ferry, Alissa L and Anderson, Erin M and Hollenbeck, Emily N and Rips, Lance J},
  journal={Psychological Science},
  volume={27},
  number={2},
  pages={244--256},
  year={2016},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}
@article{bliss2008commonsense,
  title={Commonsense reasoning about the physical world},
  author={Bliss, Joan},
  journal={Studies in Science Education},
  volume={44},
  number={2},
  pages={123--155},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{forbes2019neural,
  title={Do neural language representations learn physical commonsense?},
  author={Forbes, Maxwell and Holtzman, Ari and Choi, Yejin},
  journal={arXiv preprint arXiv:1908.02899},
  year={2019}
}
@article{li2022hake,
  title={Hake: a knowledge engine foundation for human activity understanding},
  author={Li, Yong-Lu and Liu, Xinpeng and Wu, Xiaoqian and Li, Yizhuo and Qiu, Zuoyu and Xu, Liang and Xu, Yue and Fang, Hao-Shu and Lu, Cewu},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2022},
  publisher={IEEE}
}
@article{wilcox2006shake,
  title={Shake, rattle, and... one or two objects? Young infants' use of auditory information to individuate objects},
  author={Wilcox, Teresa and Woods, Rebecca and Tuggy, Lisa and Napoli, Roman},
  journal={Infancy},
  volume={9},
  number={1},
  pages={97--123},
  year={2006},
  publisher={Taylor \& Francis}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{fang2022concept,
  title={Concept propagation via attentional knowledge graph reasoning for video-text retrieval},
  author={Fang, Sheng and Wang, Shuhui and Zhuo, Junbao and Huang, Qingming and Ma, Bin and Wei, Xiaoming and Wei, Xiaolin},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={4789--4800},
  year={2022}
}
@inproceedings{ding2022mukea,
  title={Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering},
  author={Ding, Yang and Yu, Jing and Liu, Bang and Hu, Yue and Cui, Mingxin and Wu, Qi},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={5089--5098},
  year={2022}
}
@article{liu2021indigo,
  title={Indigo: Gnn-based inductive knowledge graph completion using pair-wise encoding},
  author={Liu, Shuwen and Grau, Bernardo and Horrocks, Ian and Kostylev, Egor},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={2034--2045},
  year={2021}
}
@article{liu2021auto,
  title={Auto-encoding knowledge graph for unsupervised medical report generation},
  author={Liu, Fenglin and You, Chenyu and Wu, Xian and Ge, Shen and Sun, Xu and others},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={16266--16279},
  year={2021}
}
@inproceedings{li2022counterfactual,
  title={Counterfactual Intervention Feature Transfer for Visible-Infrared Person Re-identification},
  author={Li, Xulin and Lu, Yan and Liu, Bin and Liu, Yating and Yin, Guojun and Chu, Qi and Huang, Jinyang and Zhu, Feng and Zhao, Rui and Yu, Nenghai},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={381--398},
  year={2022},
}
@book{pearl2018book,
  title={The book of why: the new science of cause and effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic books}
}
@article{he2021debertav3,
  title={Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2111.09543},
  year={2021}
}
@article{gong2021ast,
  title={Ast: Audio spectrogram transformer},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  journal={arXiv preprint arXiv:2104.01778},
  year={2021}
}
@inproceedings{gemmeke2017audio,
  title={Audio set: An ontology and human-labeled dataset for audio events},
  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={776--780},
  year={2017},
  organization={IEEE}
}
@inproceedings{guzhov2021esresne,
  title={Esresne (x) t-fbsp: Learning robust time-frequency transformation of audio},
  author={Guzhov, Andrey and Raue, Federico and Hees, J{\"o}rn and Dengel, Andreas},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@inproceedings{purushwalkam2021audio,
  title={Audio-visual floorplan reconstruction},
  author={Purushwalkam, Senthil and Gari, Sebastia Vicenc Amengual and Ithapu, Vamsi Krishna and Schissler, Carl and Robinson, Philip and Gupta, Abhinav and Grauman, Kristen},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={1183--1192},
  year={2021}
}
@inproceedings{chen2021semantic,
  title={Semantic audio-visual navigation},
  author={Chen, Changan and Al-Halah, Ziad and Grauman, Kristen},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={15516--15525},
  year={2021}
}
@inproceedings{gan2022finding,
  title={Finding fallen objects via asynchronous audio-visual integration},
  author={Gan, Chuang and Gu, Yi and Zhou, Siyuan and Schwartz, Jeremy and Alter, Seth and Traer, James and Gutfreund, Dan and Tenenbaum, Joshua B and McDermott, Josh H and Torralba, Antonio},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10523--10533},
  year={2022}
}
@inproceedings{gan2020look,
  title={Look, listen, and act: Towards audio-visual embodied navigation},
  author={Gan, Chuang and Zhang, Yiwei and Wu, Jiajun and Gong, Boqing and Tenenbaum, Joshua B},
  booktitle={IEEE International Conference on Robotics and Automation},
  pages={9701--9707},
  year={2020},
  organization={IEEE}
}

@inproceedings{qi2019attentive,
  title={Attentive relational networks for mapping images to scene graphs},
  author={Qi, Mengshi and Li, Weijian and Yang, Zhengyuan and Wang, Yunhong and Luo, Jiebo},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={3957--3966},
  year={2019}
}

@inproceedings{qi2020few,
  title={Few-shot ensemble learning for video classification with SlowFast memory networks},
  author={Qi, Mengshi and Qin, Jie and Zhen, Xiantong and Huang, Di and Yang, Yi and Luo, Jiebo},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={3007--3015},
  year={2020}
}

@article{qi2019sports,
  title={Sports video captioning via attentive motion representation and group relationship modeling},
  author={Qi, Mengshi and Wang, Yunhong and Li, Annan and Luo, Jiebo},
  journal={IEEE Trans. Circuits Syst. Video Technol.},
  volume={30},
  number={8},
  pages={2617--2633},
  year={2019},
  publisher={IEEE}
}

@article{qi2021semantics,
  title={Semantics-aware spatial-temporal binaries for cross-modal video retrieval},
  author={Qi, Mengshi and Qin, Jie and Yang, Yi and Wang, Yunhong and Luo, Jiebo},
  journal={IEEE Trans. Image Process.},
  volume={30},
  pages={2989--3004},
  year={2021},
  publisher={IEEE}
}

@inproceedings{qi2019ke,
  title={KE-GAN: Knowledge embedded generative adversarial networks for semi-supervised scene parsing},
  author={Qi, Mengshi and Wang, Yunhong and Qin, Jie and Li, Annan},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={5237--5246},
  year={2019}
}

@inproceedings{qi2020imitative,
  title={Imitative non-autoregressive modeling for trajectory forecasting and imputation},
  author={Qi, Mengshi and Qin, Jie and Wu, Yu and Yang, Yi},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={12736--12745},
  year={2020}
}

@article{qi2020stc,
  title={STC-GAN: Spatio-temporally coupled generative adversarial networks for predictive scene parsing},
  author={Qi, Mengshi and Wang, Yunhong and Li, Annan and Luo, Jiebo},
  journal={IEEE Trans. Image Process.},
  volume={29},
  pages={5420--5430},
  year={2020},
  publisher={IEEE}
}

@inproceedings{qi2018stagnet,
  title={stagnet: An attentive semantic rnn for group activity recognition},
  author={Qi, Mengshi and Qin, Jie and Li, Annan and Wang, Yunhong and Luo, Jiebo and Van Gool, Luc},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={101--117},
  year={2018}
}

@inproceedings{bao-etal-2019-generating,
    title = "Generating Sentences from Disentangled Syntactic and Semantic Spaces",
    author = "Bao, Yu  and
      Zhou, Hao  and
      Huang, Shujian  and
      Li, Lei  and
      Mou, Lili  and
      Vechtomova, Olga  and
      Dai, Xin-yu  and
      Chen, Jiajun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1602",
    doi = "10.18653/v1/P19-1602",
    pages = "6008--6019",
    abstract = "Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE{'}s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.",
}
@inproceedings{CLEVRER2020ICLR,
  author    = {Kexin Yi and
               Chuang Gan and
               Yunzhu Li and
               Pushmeet Kohli and
               Jiajun Wu and
               Antonio Torralba and
               Joshua B. Tenenbaum},
  title     = {{CLEVRER:} Collision Events for Video Representation and Reasoning},
  booktitle = {ICLR},
  year      = {2020}
}
@inproceedings{Suhr2017ACO,
  title={A Corpus of Natural Language for Visual Reasoning},
  author={Alane Suhr and Mike Lewis and James Yeh and Yoav Artzi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:19435386}
}
@article{piloto2022intuitive,
  title={Intuitive physics learning in a deep-learning model inspired by developmental psychology},
  author={Piloto, Luis S and Weinstein, Ari and Battaglia, Peter and Botvinick, Matthew},
  journal={Nature human behaviour},
  volume={6},
  number={9},
  pages={1257--1267},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{Lake_Ullman_Tenenbaum_Gershman_2017, 
    title={Building machines that learn and think like people},
    volume={40}, 
    DOI={10.1017/S0140525X16001837}, 
    journal={Behavioral and Brain Sciences}, 
    author={Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
    year={2017}, 
    pages={e253}
}
@book{smith2019promise,
  title={The promise of artificial intelligence: reckoning and judgment},
  author={Smith, Brian Cantwell},
  year={2019},
  publisher={Mit Press}
}
@article{baillargeon2008innate,
  title={Innate ideas revisited: For a principle of persistence in infants' physical reasoning},
  author={Baillargeon, Ren{\'e}e},
  journal={Perspectives on Psychological Science},
  volume={3},
  number={1},
  pages={2--13},
  year={2008},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@article{spelke1992origins,
  title={Origins of knowledge.},
  author={Spelke, Elizabeth S and Breinlinger, Karen and Macomber, Janet and Jacobson, Kristen},
  journal={Psychological review},
  volume={99},
  number={4},
  pages={605},
  year={1992},
  publisher={American Psychological Association}
}
@article{baillargeon1985object,
  title={Object permanence in five-month-old infants},
  author={Baillargeon, Renee and Spelke, Elizabeth S and Wasserman, Stanley},
  journal={Cognition},
  volume={20},
  number={3},
  pages={191--208},
  year={1985},
  publisher={Elsevier}
}
@inproceedings{park2023dive,
  title={DIVE: Towards Descriptive and Diverse Visual Commonsense Generation},
  author={Park, Jun-Hyung and Park, Hyuntae and Kang, Youjin and Jeon, Eojin and Lee, SangKeun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9677--9695},
  year={2023}
}
@inproceedings{storks-etal-2021-tiered-reasoning,
    title = "Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding",
    author = "Storks, Shane  and
      Gao, Qiaozi  and
      Zhang, Yichi  and
      Chai, Joyce",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.422",
    doi = "10.18653/v1/2021.findings-emnlp.422",
    pages = "4902--4918",
}
@inproceedings{sap2020commonsense,
  title={Commonsense reasoning for natural language processing},
  author={Sap, Maarten and Shwartz, Vered and Bosselut, Antoine and Choi, Yejin and Roth, Dan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
  pages={27--33},
  year={2020}
}
@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={6720--6731},
  year={2019}
}
@inproceedings{singh-etal-2023-viphy,
    title = "{VIPHY}: Probing {``}Visible{''} Physical Commonsense Knowledge",
    author = "Singh, Shikhar  and
      Qasemi, Ehsan  and
      Chen, Muhao",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.473",
    doi = "10.18653/v1/2023.findings-emnlp.473",
    pages = "7113--7128",
}
@inproceedings{kondo-etal-2023-probing,
    title = "Probing Physical Reasoning with Counter-Commonsense Context",
    author = "Kondo, Kazushi  and
      Sugawara, Saku  and
      Aizawa, Akiko",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.53",
    doi = "10.18653/v1/2023.acl-short.53",
    pages = "603--612",
}
@inproceedings{zhang-etal-2023-heuristic,
    title = "From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning",
    author = "Zhang, Zheyuan  and
      Storks, Shane  and
      Hu, Fengyuan  and
      Sohn, Sungryull  and
      Lee, Moontae  and
      Lee, Honglak  and
      Chai, Joyce",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.456",
    doi = "10.18653/v1/2023.emnlp-main.456",
    pages = "7354--7379",
}
@article{kuipers1984commonsense,
  title={Commonsense reasoning about causality: deriving behavior from structure},
  author={Kuipers, Benjamin},
  journal={Artificial intelligence},
  volume={24},
  number={1-3},
  pages={169--203},
  year={1984},
  publisher={Elsevier}
}
@inproceedings{wang2020visual,
  title={Visual commonsense r-cnn},
  author={Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10760--10770},
  year={2020}
}
@inproceedings{kant2022housekeep,
  title={Housekeep: Tidying virtual households using commonsense reasoning},
  author={Kant, Yash and Ramachandran, Arun and Yenamandra, Sriram and Gilitschenski, Igor and Batra, Dhruv and Szot, Andrew and Agrawal, Harsh},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={355--373},
  year={2022},
}
@inproceedings{zareian2020learning,
  title={Learning visual commonsense for robust scene graph generation},
  author={Zareian, Alireza and Wang, Zhecan and You, Haoxuan and Chang, Shih-Fu},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={642--657},
  year={2020},
}

@inproceedings{fang-etal-2020-video2commonsense,
    title = "{V}ideo2{C}ommonsense: Generating Commonsense Descriptions to Enrich Video Captioning",
    author = "Fang, Zhiyuan  and
      Gokhale, Tejas  and
      Banerjee, Pratyay  and
      Baral, Chitta  and
      Yang, Yezhou",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.61",
    doi = "10.18653/v1/2020.emnlp-main.61",
    pages = "840--860",
}
@inproceedings{lin2023tiktalk,
  title={TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World},
  author={Lin, Hongpeng and Ruan, Ludan and Xia, Wenke and Liu, Peiyu and Wen, Jingyuan and Xu, Yixin and Hu, Di and Song, Ruihua and Zhao, Wayne Xin and Jin, Qin and others},
  booktitle={Proc. ACM Int. Conf. on Multimedia},
  pages={1303--1313},
  year={2023}
}
@inproceedings{tran2017disentangled,
  title={Disentangled representation learning gan for pose-invariant face recognition},
  author={Tran, Luan and Yin, Xi and Liu, Xiaoming},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={1415--1424},
  year={2017}
}
@article{denton2017unsupervised,
  title={Unsupervised learning of disentangled representations from video},
  author={Denton, Emily L and others},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={30},
  year={2017}
}
@inproceedings{ma2018disentangled,
  title={Disentangled person image generation},
  author={Ma, Liqian and Sun, Qianru and Georgoulis, Stamatios and Van Gool, Luc and Schiele, Bernt and Fritz, Mario},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={99--108},
  year={2018}
}
@article{eom2019learning,
  title={Learning disentangled representation for robust person re-identification},
  author={Eom, Chanho and Ham, Bumsub},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={32},
  year={2019}
}
@inproceedings{lu2024hierarchical,
  title={Hierarchical diffusion autoencoders and disentangled image manipulation},
  author={Lu, Zeyu and Wu, Chengyue and Chen, Xinyuan and Wang, Yaohui and Bai, Lei and Qiao, Yu and Liu, Xihui},
  booktitle={Proc. IEEE Winter Conf. Appl. Comput. Vis.},
  pages={5374--5383},
  year={2024}
}
@article{han2024revcolv2,
  title={RevColV2: Exploring disentangled representations in masked image modeling},
  author={Han, Qi and Cai, Yuxuan and Zhang, Xiangyu},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={36},
  year={2024}
}
@article{bai2021contrastively,
  title={Contrastively disentangled sequential variational autoencoder},
  author={Bai, Junwen and Wang, Weiran and Gomes, Carla P},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={34},
  pages={10105--10118},
  year={2021}
}
@article{wei2024unsupervised,
  title={Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective},
  author={Wei, Pengfei and Kong, Lingdong and Qu, Xinghua and Ren, Yi and Xu, Zhiqiang and Jiang, Jing and Yin, Xiang},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={36},
  year={2024}
}
@article{van2019disentangled,
  title={Are disentangled representations helpful for abstract visual reasoning?},
  author={Van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber, J{\"u}rgen and Bachem, Olivier},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={32},
  year={2019}
}
@article{wang2023disavr,
  title={DisAVR: Disentangled Adaptive Visual Reasoning Network for Diagram Question Answering},
  author={Wang, Yaxian and Wei, Bifan and Liu, Jun and Zhang, Lingling and Wang, Jiaxin and Wang, Qianying},
  journal={IEEE Trans. Image Process.},
  year={2023},
  publisher={IEEE}
}
@inproceedings{amizadeh2020neuro,
  title={Neuro-symbolic visual reasoning: Disentangling},
  author={Amizadeh, Saeed and Palangi, Hamid and Polozov, Alex and Huang, Yichen and Koishida, Kazuhito},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={279--290},
  year={2020},
}
@article{liu2023video,
  title={Video Question Answering with Semantic Disentanglement and Reasoning},
  author={Liu, Jin and Wang, Guoxiang and Xie, Jialong and Zhou, Fengyu and Xu, Huijuan},
  journal={IEEE Trans. Circuits Syst. Video Technol.},
  year={2023},
  publisher={IEEE}
}
@InProceedings{Wang_2024_CVPR,
    author    = {Wang, Tan and Li, Linjie and Lin, Kevin and Zhai, Yuanhao and Lin, Chung-Ching and Yang, Zhengyuan and Zhang, Hanwang and Liu, Zicheng and Wang, Lijuan},
    title     = {DisCo: Disentangled Control for Realistic Human Dance Generation},
    booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
    year      = {2024},
    pages     = {9326-9336}
}
@article{yi2018neural,
  title={Neural-symbolic vqa: Disentangling reasoning from vision and language understanding},
  author={Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Josh},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={31},
  year={2018}
}
@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={235--251},
  year={2016},
}
@inproceedings{xiao2021next,
  title={Next-qa: Next phase of question-answering to explaining temporal actions},
  author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={9777--9786},
  year={2021}
}
@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}
@article{burgess2018understanding,
  title={Understanding disentangling in $\beta$-VAE},
  author={Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1804.03599},
  year={2018}
}
@inproceedings{kazemi2019style,
  title={Style and content disentanglement in generative adversarial networks},
  author={Kazemi, Hadi and Iranmanesh, Seyed Mehdi and Nasrabadi, Nasser},
  booktitle={Proc. IEEE Winter Conf. Appl. Comput. Vis.},
  pages={848--856},
  year={2019},
  organization={IEEE}
}
@inproceedings{wang2021causal,
  title={Causal attention for unbiased visual recognition},
  author={Wang, Tan and Zhou, Chang and Sun, Qianru and Zhang, Hanwang},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={3091--3100},
  year={2021}
}
@article{qin2021causal,
  title={Causal interventional training for image recognition},
  author={Qin, Wei and Zhang, Hanwang and Hong, Richang and Lim, Ee-Peng and Sun, Qianru},
  journal={IEEE Transactions on Multimedia},
  volume={25},
  pages={1033--1044},
  year={2021},
  publisher={IEEE}
}
@inproceedings{liu2022contextual,
  title={Contextual debiasing for visual recognition with causal mechanisms},
  author={Liu, Ruyang and Liu, Hao and Li, Ge and Hou, Haodi and Yu, TingHao and Yang, Tao},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={12755--12765},
  year={2022}
}
@inproceedings{yang2021causal,
  title={Causal attention for vision-language tasks},
  author={Yang, Xu and Zhang, Hanwang and Qi, Guojun and Cai, Jianfei},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={9847--9857},
  year={2021}
}
@inproceedings{mao2021generative,
  title={Generative interventions for causal learning},
  author={Mao, Chengzhi and Cha, Augustine and Gupta, Amogh and Wang, Hao and Yang, Junfeng and Vondrick, Carl},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={3947--3956},
  year={2021}
}
@inproceedings{wang2022counterfactual,
  title={Counterfactual cycle-consistent learning for instruction following and generation in vision-language navigation},
  author={Wang, Hanqing and Liang, Wei and Shen, Jianbing and Van Gool, Luc and Wang, Wenguan},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={15471--15481},
  year={2022}
}
@inproceedings{qi2020two,
  title={Two causal principles for improving visual dialog},
  author={Qi, Jiaxin and Niu, Yulei and Huang, Jianqiang and Zhang, Hanwang},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={10860--10869},
  year={2020}
}
@article{yang2021deconfounded,
  title={Deconfounded image captioning: A causal retrospect},
  author={Yang, Xu and Zhang, Hanwang and Cai, Jianfei},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  volume={45},
  number={11},
  pages={12996--13010},
  year={2021},
  publisher={IEEE}
}
@inproceedings{li2024image,
  title={Image content generation with causal reasoning},
  author={Li, Xiaochuan and Fan, Baoyu and Zhang, Runze and Jin, Liang and Wang, Di and Guo, Zhenhua and Zhao, Yaqian and Li, Rengang},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={38},
  number={12},
  pages={13646--13654},
  year={2024}
}
@article{kocaoglu2017causalgan,
  title={Causalgan: Learning causal implicit generative models with adversarial training},
  author={Kocaoglu, Murat and Snyder, Christopher and Dimakis, Alexandros G and Vishwanath, Sriram},
  journal={arXiv preprint arXiv:1709.02023},
  year={2017}
}
@inproceedings{zang2023discovering,
  title={Discovering the real association: Multimodal causal reasoning in video question answering},
  author={Zang, Chuanqi and Wang, Hanqing and Pei, Mingtao and Liang, Wei},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={19027--19036},
  year={2023}
}
@inproceedings{huang2022deconfounded,
  title={Deconfounded visual grounding},
  author={Huang, Jianqiang and Qin, Yu and Qi, Jiaxin and Sun, Qianru and Zhang, Hanwang},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={36},
  number={1},
  pages={998--1006},
  year={2022}
}
@inproceedings{xue2023variational,
  title={Variational Causal Inference Network for Explanatory Visual Question Answering},
  author={Xue, Dizhan and Qian, Shengsheng and Xu, Changsheng},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2515--2525},
  year={2023}
}
@inproceedings{kim2017interpretable,
  title={Interpretable learning for self-driving cars by visualizing causal attention},
  author={Kim, Jinkyu and Canny, John},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={2942--2950},
  year={2017}
}
@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={32},
  year={2019}
}
@article{hu2021class,
  title={Class-aware Sounding Objects Localization via Audiovisual Correspondence},
  author={Hu, Di and Wei, Yake and Qian, Rui and Lin, Weiyao and Song, Ruihua and Wen, Ji-Rong},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2021},
  publisher={IEEE}
}
@article{deshmukh2023pengi,
  title={Pengi: An audio language model for audio tasks},
  author={Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={36},
  pages={18090--18108},
  year={2023}
}
@article{xia2023balanced,
  title={Balanced Audiovisual Dataset for Imbalance Analysis},
  author={Xia, Wenke and Zhao, Xu and Pang, Xincheng and Zhang, Changqing and Hu, Di},
  journal={arXiv preprint arXiv:2302.10912},
  year={2023}
}
@inproceedings{peng2022balanced,
  title={Balanced multimodal learning via on-the-fly gradient modulation},
  author={Peng, Xiaokang and Wei, Yake and Deng, Andong and Wang, Dong and Hu, Di},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={8238--8247},
  year={2022}
}
@inproceedings{wang2020makes,
  title={What makes training multi-modal classification networks hard?},
  author={Wang, Weiyao and Tran, Du and Feiszli, Matt},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={12695--12705},
  year={2020}
}
@inproceedings{wang2023multi,
  title={Multi-modal learning with missing modality via shared-specific feature modelling},
  author={Wang, Hu and Chen, Yuanhong and Ma, Congbo and Avery, Jodie and Hull, Louise and Carneiro, Gustavo},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={15878--15887},
  year={2023}
}
@article{lv2024disentangled,
  title={Disentangled counterfactual learning for physical audiovisual commonsense reasoning},
  author={Lv, Changsheng and Zhang, Shuai and Tian, Yapeng and Qi, Mengshi and Ma, Huadong},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={36},
  year={2023}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={770--778},
  year={2016}
}
@inproceedings{ganin2015unsupervised,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={1180--1189},
  year={2015},
}
@inproceedings{tfeat2016bmvc,
  title={Learning local feature descriptors with triplets and shallow convolutional neural networks},
  author={V. Balntas and E. Riba and D. Ponsa and  K. Mikolajczyk},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2016}
}
@inproceedings{yang2018semi,
  title={Semi-Supervised Multi-Modal Learning with Incomplete Modalities.},
  author={Yang, Yang and Zhan, De-Chuan and Sheng, Xiang-Rong and Jiang, Yuan},
  booktitle={IJCAI},
  pages={2998--3004},
  year={2018}
}
@inproceedings{ma2021smil,
  title={Smil: Multimodal learning with severely missing modality},
  author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Tulyakov, Sergey and Wu, Cathy and Peng, Xi},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={35},
  number={3},
  pages={2302--2310},
  year={2021}
}
@inproceedings{yingzhen2018disentangled,
  title={Disentangled sequential autoencoder},
  author={Yingzhen, Li and Mandt, Stephan},
  booktitle={Proc. Inter. Conf. on Machine Learning},
  pages={5670--5679},
  year={2018},
}
@article{hou2019deep,
  title={Deep multimodal multilinear fusion with high-order polynomial pooling},
  author={Hou, Ming and Tang, Jiajia and Zhang, Jianhai and Kong, Wanzeng and Zhao, Qibin},
  journal={Proc. Adv. Neural Inf. Process. Syst.},
  volume={32},
  year={2019}
}
@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={Proc. Inter. Conf. on Mach. Learn.},
  pages={689--696},
  year={2011}
}
@inproceedings{pham2019found,
  title={Found in translation: Learning robust joint representations by cyclic translations between modalities},
  author={Pham, Hai and Liang, Paul Pu and Manzini, Thomas and Morency, Louis-Philippe and P{\'o}czos, Barnab{\'a}s},
  booktitle={AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6892--6899},
  year={2019}
}
@inproceedings{li2022representation,
  title={From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering},
  author={Li, Jiangtong and Niu, Li and Zhang, Liqing},
  booktitle={Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
  pages={21273--21282},
  year={2022}
}
@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{lv2023counterfactual,
  title={Counterfactual cross-modality reasoning for weakly supervised video moment localization},
  author={Lv, Zezhong and Su, Bing and Wen, Ji-Rong},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6539--6547},
  year={2023}
}
@InProceedings{Rao_2021_ICCV,
    author    = {Rao, Yongming and Chen, Guangyi and Lu, Jiwen and Zhou, Jie},
    title     = {Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-Identification},
    booktitle = {Proc. IEEE Int. Conf. Comput. Vis.},
    year      = {2021},
    pages     = {1025-1034}
}
@article{xu2022unintentional,
  title={Unintentional action localization via counterfactual examples},
  author={Xu, Jinglin and Chen, Guangyi and Lu, Jiwen and Zhou, Jie},
  journal={IEEE Trans. Image Process.},
  volume={31},
  pages={3281--3294},
  year={2022},
  publisher={IEEE}
}
@article{sun2023unbiased,
  title={Unbiased scene graph generation via two-stage causal modeling},
  author={Sun, Shuzhou and Zhi, Shuaifeng and Liao, Qing and Heikkil{\"a}, Janne and Liu, Li},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  volume={45},
  number={10},
  pages={12562--12580},
  year={2023},
  publisher={IEEE}
}
@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}
@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@inproceedings{zong2024toward,
  title={Toward Explainable Physical Audiovisual Commonsense Reasoning},
  author={Zong, Daoming and Ding, Chaoyue and Chen, Kaitao},
  booktitle={Proc. ACM Int. Conf. on Multimedia},
  pages={7288--7297},
  year={2024}
}
@article{wang2024rdfc,
  title={RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion},
  author={Wang, Haowen and Che, Zhengping and Yang, Yufan and Wang, Mingyuan and Xu, Zhiyuan and Qiao, Xiuquan and Qi, Mengshi and Feng, Feifei and Tang, Jian},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  year={2024},
  publisher={IEEE}
}