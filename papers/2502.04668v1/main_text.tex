\documentclass[aps,physrev,twocolumn,groupedaddress,superscriptaddress]{revtex4-2}
\usepackage{amssymb}
%\DeclareUnicodeCharacter{00FC}{\"u} % 将 "ü" 定义为 LaTeX 命令
\usepackage{bm}
\usepackage{amsmath}
%\documentclass[aps,physrev,preprint,superscriptaddress]{revtex4-2}
%\documentclass[aps,prl,preprint,superscriptaddress]{revtex4-2}
%\documentclass[aps,prl,reprint,groupedaddress]{revtex4-2}
%\documentclass[aps,rmp,preprint,superscriptaddress]{revtex4-2}
%\documentclass[aps,rmp,reprint,groupedaddress]{revtex4-2}

% You should use BibTeX and apsrev.bst for references
% Choosing a journal automatically selects the correct APS
% BibTeX style file (bst file), so only uncomment the line
% below if necessary.
%\bibliographystyle{apsrev4-2}
\usepackage{graphicx}
\usepackage[cmyk]{xcolor}
\newcommand{\tcb}[1]{\textcolor{blue}{#1}}
\newcommand{\tcr}[1]{\textcolor{red}{Note: #1}}
\usepackage{xr}
% In your preamble
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\makeatletter
\newcommand*{\addFileDependency}[1]{% argument=file name and extension
  \typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother
\newcommand*{\myexternaldocument}[1]{%
    \externaldocument{#1}%
    \addFileDependency{#1.tex}%
    \addFileDependency{#1.aux}%
}
\myexternaldocument{SupplementaryMaterial}

%\usepackage{xcite}
%\externalcitedocument{SupplementaryMaterial}

\begin{document}

% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}

%Title of paper
\title{Machine-Learning Interatomic Potentials for Long-Range Systems} 

% repeat the \author .. \affiliation  etc. as needed
% \email, \thanks, \homepage, \altaffiliation all apply to the current
% author. Explanatory text should go in the []'s, actual e-mail
% address or url should go in the {}'s for \email and \homepage.
% Please use the appropriate macro foreach each type of information

% \affiliation command applies to all authors since the last
% \affiliation command. The \affiliation command should follow the
% other information[]
% \affiliation can be followed by \email, \homepage, \thanks as well.
\author{Yajie Ji}
\email{jiyajie595@sjtu.edu.cn}
\affiliation{School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai 200240, China}
\author{Jiuyang Liang}
\email[Contact author: ]{jliang@flatironinstitute.org}
\affiliation{School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai 200240, China}
\affiliation{
Center for Computational Mathematics, Flatiron Institute, Simons Foundation, New York 10010, USA}
\author{Zhenli Xu}
\email[Contact author: ]{xuzl@sjtu.edu.cn}
%\email{xuzl@sjtu.edu.cn}
\affiliation{School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai 200240, China}
\affiliation{CMA-Shanghai and MOE-LSC, Shanghai Jiao Tong University, Shanghai 200240, China}

%\thanks{Y. Ji acknowledges the support from the NSFC (No. 124B2023).}
%\author{Yajie Ji}
%\affiliation{School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai 200240, China.}
%\author{Jiuyang Liang}
%\affiliation{School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai 200240, China.}
%\affiliation{Center for Computational Mathematics, Flatiron Institute, Simons Foundation, New York 10010, USA.}
%\email{Contact author: jliang@flatironinstitute.org}
%\thanks{J. Liang acknowledges the support from the China Postdoctoral Science Foundation (grant No. 2024M751948).}

%\author{Zhenli Xu}
%\affiliation{School of Mathematical Sciences, CMA-Shanghai and MOE-LSC, Shanghai Jiao Tong University, Shanghai 200240, China.}
%\email{xuzl@sjtu.edu.cn}
%\thanks{Z. Xu acknowledges the support from the NSFC (grant Nos. 12325113 and 12426304) and the HPC center of Shanghai Jiao Tong University.}

%\email[]{Your e-mail address}
%\homepage[]{Your web page}
%\thanks{}
%\altaffiliation{}
%\affiliation{}

%Collaboration name if desired (requires use of superscriptaddress
%option in \documentclass). \noaffiliation is required (may also be
%used with the \author command).
%\collaboration can be followed by \email, \homepage, \thanks as well.
%\collaboration{}
%\noaffiliation

%\date{\today}

\begin{abstract}
Machine-learning interatomic potentials have emerged as a revolutionary class of force-field models in molecular simulations, delivering quantum-mechanical accuracy at a fraction of the computational cost and enabling the simulation of large-scale systems over extended timescales. However, they often focus on modeling local environments, neglecting crucial long-range interactions. 
We propose a Sum-of-Gaussians Neural Network (SOG-Net), a lightweight and versatile framework for integrating long-range interactions into machine learning force field. The SOG-Net employs a latent-variable learning network that seamlessly bridges short-range and long-range components, coupled with an efficient Fourier convolution layer that incorporates long-range effects. By learning sum-of-Gaussian multipliers across different convolution layers, the SOG-Net adaptively captures diverse long-range decay behaviors while maintaining close-to-linear computational complexity during training and simulation via non-uniform fast Fourier transforms. The method is demonstrated effective for a broad range of long-range systems. 
\end{abstract}

% insert suggested keywords - APS authors don't need to do this
%\keywords{}

%\maketitle must follow title, authors, abstract, and keywords
\maketitle

Machine-learning interatomic potentials (MLIPs) have become essential tools in atomic force field modeling, bridging the gap between the high accuracy of computationally expensive quantum mechanical calculations and the efficiency of empirical force fields~\cite{friederich2021machine,unke2021machine,zhang2018deep}. Most MLIPs assume that the potential energy surface of an atom is determined by its local environment within a finite cutoff radius, an assumption rooted in the ``nearsightedness principle'' of electronic structure theory~\cite{kohn1996density,PNAS2005}. This approximation simplifies the model, enabling predictions to scale linearly with system size. However, it systematically neglects long-range (LR) interactions such as Coulomb, dispersion, and dipole forces, which are crucial for accurately modeling the heterogeneous structures and functionalities of polar materials and biological systems in molecular dynamics (MD) simulations~\cite{zhou2018electrostatic,PhysRevLett.132.228101}. Incorporating LR interactions into MLIP models while preserving efficiency and scalability remains a significant challenge in the field.

To capture LR effects in MLIP models, most efforts focused on electrostatic interactions. Early approaches~\cite{bartok2010gaussian,Unke2021} assigned fixed point charges with empirical force-field corrections; however, these methods struggled with charge-transfer effects and the challenge of defining accurate corrections. Recent advancements include the use of virtual charge sites, as employed in deep potential LR~\cite{zhang2022deep} and self-consistent field neural networks~\cite{gao2022self} as well as methods that predict effective partial charges while enforcing global charge conservation~\cite{unke2019physnet,ko2021fourth,shaidu2024incorporating}. In these models, once latent variables such as virtual sites or partial charges are determined, electrostatic interactions are computed using classical algorithms such as Ewald summation~\cite{ewald1921berechnung} or self-consistent field iterations. These approaches have demonstrated success in applications such as phase diagram predictions~\cite{zhang2021phase,bore2023realistic}. 
However, accurately and efficiently modeling diverse LR decay behaviors in MLIPs remains far from settled. The classical Ewald summation~\cite{ewald1921berechnung}, commonly used to account for LR effects, was originally designed for $1/r$ Coulomb potentials and is ill-suited for other LR decay rates, such as $1/r^p$ (with $p>1$ or non-integer), $e^{-\mu r}/r$ (with $\mu>0$), or more complex forms involving combinations of decay exponents~\cite{israelachvili2011intermolecular}.
%Even for electrostatic interactions, the physical decay rates often deviate from the idealized $1/r$, as atoms in density functional theory (DFT) are not ideal point particles. 
Approaches such as the long-distance equivariant (LODE) method~\cite{huguenin2023physics,Grisafi2019JCP} improve the accuracy for inverse power-law decays but rely heavily on the accuracy of the initial LR tail guess. Similarly, message passing neural networks~\cite{schutt2017schnet,batzner20223,batatia2022mace,kosmala2023ewald,deng2023chgnet} capture nonlocal interactions through stacked graph convolution layers but involve prohibitive computational costs for slowly decaying LR potentials.

\begin{figure*}[!ht] 
    \centering
    \includegraphics[width=0.93\textwidth]{./LRC-SOG.pdf}
    \caption{Schematic illustration of the SOG-Net model. The upper section depicts the entire network structure, whereas the lower section provides a detailed view of the LR convolution layer. Here, \(\{\bm{q}_{i,\eta}\}\), \(E^{SR,i}\), and \(E^{LR,i}\) denote the latent variables and SR and LR component of atomic energy, respectively.}
    \label{fig:3DSketch}
\end{figure*}

This paper proposes a sum-of-Gaussians neural network (SOG-Net) for accurately learning interatomic potentials with general LR tails. It separates atomic interactions into short-range (SR) and LR components, which are connected through a network that learns latent variables. These variables are processed by an efficient LR convolution layer that accounts for LR contributions, where the SOG multiplier in each layer is represented as a sum of Gaussians with trainable variances and amplitudes. The resulting SOG-Net effectively captures LR effects while maintaining nearly linear computational complexity during both training and simulation through the use of non-uniform fast Fourier transform (NUFFT)~\cite{dutt1993fast,greengard2004accelerating}. Unlike existing works relying on either various forms of Ewald summation or self-consistent iterations to account for LR contributions, our method adaptively captures diverse LR decay tails present in the system by learning Gaussian multipliers across different convolution layers. 

Consider a system of $N$ atoms at $\bm{r}_{i}$, for $i=1, \cdots, N$, in a three-dimensional space with periodic boundary conditions. One assumes that the total potential energy can be written into SR and LR parts, 
\begin{equation}
E=E^{SR}+E^{LR}.
\end{equation}
As is standard in most MLIPs~\cite{ko2023recent}, the SR energy is summed over the atomic contribution of each atom:
\begin{equation}
E^{SR}=\sum_{i=1}^{N}E^{SR,i}=\sum_{i=1}^{N}f_{\bm{\theta}_{SR}}(\bm{D}_{i}),
\end{equation}
where $\bm{D}_i$ is the SR descriptor, comprising invariant features of the $i$-th atom. The neural network $f_{\bm{\theta}_{SR}}(\cdot)$, parameterized by $\bm{\theta}_{SR}$, maps $\bm{D}_i$ to the atomic SR energy $E^{SR,i}$. The descriptor \(\bm{D}_i\) is a function of the local atomic environment $\bm{\mathcal{R}}_i \in \mathbb{R}^{\mathcal{N}_i \times 3}$, where $\mathcal{N}_i$ represents the number of neighbors of atom $i$ within a predefined cutoff $r_c$. To ensure physical consistency, $\bm{D}_i$ preserves symmetries such as translation, rotation, and permutation. Further details on its practical construction are provided in the Supplementary Materials (SM), Section II~\cite{supplementary_information}.

\begin{figure*}[!ht] 
    \centering
    \includegraphics[width=0.85\textwidth]{./PinTu.pdf}
    \caption{
(a) Plot of the test errors as a function of the size of training set. Data are shown for the short-range component (SR) and the full SOG-Net model (LR). The insets at the upper right and bottom left display a snapshot of the system and a plot of the learned SOG multiplier $g_{\bm{\theta}_{\eta}}$, respectively. The solid line represents the reference LR decaying tail in the force field.  (b) Plot of the test error, represented by the relative root mean square error (RMSE), as a function of the number of Fourier grids \( I_{\text{FFT}} \). The data are shown for different real-space cutoff radii \( r_c \). }
    \label{fig:3DFFTgrid}
\end{figure*}

Leveraging the rapid decay of LR interactions in Fourier space, we model the LR energy using multiple Fourier convolution layers:
\begin{equation}\label{eq::long-rangepotential}
E^{LR}= \frac{1}{2V}\sum_{i=1}^{N}\sum_{\eta=1}^{P}q_{i,\eta}\sum_{\bm{k}}  \widehat{g}_{\bm{\theta}_{\eta}}(\bm{k})\widehat{\rho}_{\eta}(\bm{k})e^{i\bm{k}\cdot\bm{r}_i},
\end{equation}
where $P\in \mathbb{Z}^+$ is the total number of layers, \begin{equation}
\bm{q}_{\eta}=(q_{1,\eta},\cdots,q_{N,\eta}),\quad \eta=1,\cdots,P
\end{equation}
denotes the $\eta$-th layer of latent variables for the LR energy, $\widehat{g}_{\bm{\theta}_{\eta}}$ is the SOG multiplier operator parametrized by $\bm{\theta}_{\eta}$ for the $\eta$-th layer, $\bm{k}=2\pi (m_x/L_x, m_y/L_y,m_z/L_z)$ denotes the Fourier modes with $m_x,m_y,m_z\in\mathbb{Z}$, and  
\begin{equation}
\widehat{\rho}_{\eta}(\bm{k}) = \sum_{j=1}^{N} q_{j,\eta} e^{-i \bm{k} \cdot \bm{r}_j}
\end{equation}
denotes the structure factor of the latent variables for the $\eta$-th layer. In the LR model, the latent variables $\bm{q}_{\eta}$ in each layer can be interpreted as the weights assigned to each atom for different types of LR interactions, such as partial charges in electrostatic interactions or atomic sizes in dispersion interactions. These latent variables are obtained by mapping the invariant features $\bm{D}_i$ to $\bm{q}_{\eta}$ using a latent neural network parameterized by $\bm{\theta}_{lnn}$, i.e., $\{q_{i,\eta}\}_{\eta=1}^{P}=f_{\bm{\theta}_{lnn}}(\bm{D}_i)$. A schematic diagram of the basic network structure is presented in the upper half of Figure~\ref{fig:3DSketch}. Further explanation on the motivation behind Eq.~\eqref{eq::long-rangepotential} is provided in the SM (Sections I A and I B)~\cite{supplementary_information}.


In our LR model, two key aspects are the parameterization of the SOG multiplier operator $\widehat{g}_{\bm{\theta}_{\eta}}$ and the efficiency in computing Eq.~\eqref{eq::long-rangepotential}. These factors will be addressed sequentially as follows. We parameterize $\widehat{g}_{\bm{\theta}_{\eta}}$ as a sum of Gaussians,
\begin{equation}\label{eq::FourierMultiplier}
\widehat{g}_{\bm{\theta}_{\eta}}(\bm{k}) = \sum_{\ell=0}^{M} w_{\ell,\eta} e^{-k^2 / s_{\ell,\eta}^2},
\end{equation}
where $\bm{\theta}_{\eta} = \{w_{\ell,\eta}, s_{\ell,\eta}\}_{\ell=0}^{M}$ is the set of trainable parameters. The SOG multipliers $\{\widehat{g}_{\bm{\theta}_{\eta}}\}_{\eta=1}^{P}$ serve as a multilayer approximation of the LR tail of the potential in Fourier space. Gaussians exhibit excellent symmetry and smoothness and are renowned for minimizing the Heisenberg uncertainty principle for $L^2$ functions~\cite{folland1997uncertainty}. These properties allows the SOG multipliers to achieve an optimal trade-off between spatial and frequency localization, facilitating the effective representation of LR tails. The form of LR energy, as constructed in Eqs.~\eqref{eq::long-rangepotential}-\eqref{eq::FourierMultiplier}, inherently preserves translation, rotation, and permutation invariants. Additional constraints can  be imposed either on the coefficients \( \bm{\theta}_{\eta} \) or by projecting the learned potential onto a set of symmetric basis functions~\cite{huguenin2023physics,faller2024density}.

%on the coefficients \( \bm{\theta}_{\eta} \) , for instance, 
%by solving an equation for some certain coefficients after training the others to enforce other symmetries.
%Additional constraints can be imposed on the coefficients \( \bm{\theta}_{\eta} \) to enforce other symmetries. 

In practice, the weights $\{w_{\ell,\eta}\}_{\ell=0}^{M}$ in the SOG are initialized as a vector of ones for $\eta=1,\cdots,P$, and the bandwidths are initialized as
\begin{equation}
s_{\ell,\eta} = e^{b_{\text{min}} + \ell(b_{\text{max}} - b_{\text{min}}) / M}, \quad \ell = 0, \dots, M,
\end{equation}
where \( b_{\text{min}}, b_{\text{max}} \in \mathbb{R} \), and \( b_{\text{min}} < b_{\text{max}}.\) This logarithmically spacing generates $s_{\ell,\eta}$ as points in the range \( [e^{b_{\text{min}}}, e^{b_{\text{max}}}] \), effectively capturing the multiscale nature of LR potentials and significantly reducing the number of Gaussians needed for accurate representation. Additionally, after completing the training, $M$ is further reduced using model reduction techniques~\cite{benner2015survey} to improve the prediction efficiency.
Further details on the parameter initialization and model reduction method are provided in the SM (Section I C and I D)~\cite{supplementary_information}. 

Here, we further propose a fast algorithm to accelerate our model, achieving near-optimal complexity and improved scalability. 
The method begins by mapping each layer of latent variables, $q_{i,\eta}$, onto a uniform grid through convolution with a shape function $W$, referred to as the gridding operator $\mathcal{G}_{\eta}$. The FFT operator $\mathcal{F}$ is then applied to compute $\widehat{\rho}_{\eta}(\bm{k})$. For each layer, the diagonal scaling operator $\mathcal{S}_{\eta}$  performs pointwise multiplication $(\widehat{g}_{\bm{\theta}_{\eta}}\circ \widehat{\rho}_{\eta})(\bm{k})$ in the Fourier space. Subsequently, the inverse FFT operator $\mathcal{F}^{-1}$ recovers the atomic LR energy at each grid point. Finally, the gathering operator $\mathcal{G}^{-1}_{\eta}$ maps the atomic energy back to the atomic positions $\{\bm{r}_i\}_{i=1}^{N}$, summing contributions across all layers. It is worth noting that the gridding, scaling, and gathering operators can be applied species-wise in practical implementations. The entire process of the LR convolution layer is summarized as
\begin{equation}\label{eq::operator}
E^{LR,i}=\mathcal{G}^{-1}_{\eta}\mathcal{F}^{-1}\mathcal{S}_{\eta}\mathcal{F}\mathcal{G}_{\eta}{\Big[}\{\bm{q}_{i,\eta}\}_{\eta=1}^{P}{\Big]},
\end{equation}
with a schematic illustration provided in the lower half of Figure~\ref{fig:3DSketch}. The detailed method is provided in the SM, Section~III~\cite{supplementary_information}, where we leverage the FINUFFT package~\cite{Barnett2019SISC} to accelerate these operators via NUFFTs. As a result, each operator in Eq.~\eqref{eq::operator} can be executed in a complexity of either $O(N+N_{\text{FFT}}\log N_{\text{FFT}})$, where $N_{\text{FFT}}$ represents the average number of grid points per layer. This highlights a key advantage of the SOG-Net: after the learning process is complete, the model inherently supports acceleration through a fast algorithm, thereby enhancing the simulation process. 

\begin{figure*}[!ht] 
    \centering
\includegraphics[width=0.90\textwidth]{./dimer.pdf}
    \caption{Comparison of the binding energy curves for the short-range component (SR) and the full SOG-Net model (LR) across six dimer classes: charged-charged (CC), charged-polar (CP), polar-polar (PP), charged-apolar (CA), polar-apolar (PA), and apolar-apolar (AA). The binding energy curve represents the potential energy difference between the dimer and its isolated monomers. Insets display  system snapshots with charge states labeled, along with the RMSE of energy for both models.}
    \label{fig:sixdimer}
\end{figure*}

To evaluate the model, we start by applying SOG-Net to a toy model of NaCl electrolytes. This system  consists of $1000$ particles interacting through Coulomb and dispersion forces. The training set contains at most $4000$ configurations, and the test set includes $200$ configurations, both sampled every $1000$ steps from a long MD trajectory using LAMMPS~\cite{thompson2022lammps} in the NVT ensemble at $T=300~K$. Further details on the training setup for the SR component are provided in the SM, Section~IV~\cite{supplementary_information}. For the LR component, we use a single layer of latent variables ($P=1$) and six Gaussians ($M=6$) in the LR convolution layer.
In Figure~\ref{fig:3DFFTgrid}(a), we plot the test error as a function of the training set size, keeping the number of Fourier grids fixed as $N_{\text{FFT}}=21^3$. The results indicate that using either the SR or LR parts alone results in limitations in the representational capacity. However, combining the two components improves the accuracy by nearly 1–2 orders of magnitude. The inset on the bottom left of Figure~\ref{fig:3DFFTgrid}(a) demonstrates that our SOG multiplier effectively captures the LR tail, which is represented by a mix of electrostatic and Lennard-Jones interactions. In Figure~\ref{fig:3DFFTgrid}(b), we plot the test error of the complete network against the number of Fourier grids. When the cutoff radius of the SR network exceeds \(2^{1/6}\), the equilibrium distance for dispersion interactions, the SOG-Net achieves a relative error of \(10^{-3}\) using only $21$ grid points per dimension. Conversely, when the cutoff radius is less than $2^{1/6}$, incorporating an LR layer fails to improve the fitting accuracy due to insufficient representation of repulsive interactions. These findings suggest that the choice of parameters for the SR and LR parts must be carefully balanced to optimize the accuracy. 

Next, we evaluate the SOG-Net by examining the binding curves of dimer pairs formed from charged (C), polar (P), and apolar (A) relaxed molecules at various separations (ranging from approximately $5\mathring{A}$ to $15\mathring{A}$) within a periodic cubic box of side length $L=30\mathring{A}$. These three molecular categories result in six distinct dimer classes (CC, CP, PP, CA, PA, and AA), each characterized by ideal power-law decay constants $1/r^p$ for their interactions: $p_{\text{CC}}=1$, $p_{\text{CP}}=2$, $p_{\text{PP}}=3$, $p_{\text{CA}}=4$, $p_{\text{PA}}=5$, and $p_{\text{AA}}=6$. Snapshots of the dimer configurations are provided as insets in Figure~\ref{fig:sixdimer}. Our benchmark utilizes data from the BioFragment Database~\cite{burns2017biofragment}, computed using the HSE06 hybrid functional~\cite{heyd2003hybrid} with a high fraction of exact exchange. For each molecular pair, the dataset is randomly split into training and testing sets in a $9:1$ ratio. This benchmark is designed to evaluate the ability of the SOG-Net model to accurately capture the LR behavior of various interaction types.
In the LR component of the SOG-Net, we use a single layer of latent variables, $M=12$ Gaussians, and $N_{\text{FFT}}=31^3$ Fourier grids. The construction of the SR component and the training setup are detailed in the SM, Section~IV~\cite{supplementary_information}. Figure~\ref{fig:sixdimer} presents the binding energy curve. The pure SR models fail to accurately capture the training data, as evidenced by the noticeable flattening of the binding energy curves at larger separations between the two molecules. The improvement achieved by our LR models is particularly remarkable for the CA, PA, and AA cases, where the energy scales are much smaller (below $0.1$ meV). Moreover, the RMSE of forces are presented in Figure~SI in the SM~\cite{supplementary_information}. The results demonstrate that the model incorporating the LR component significantly outperforms the pure SR model in all cases, with the RMSE of the forces reduced by approximately an order of magnitude. These observations highlight the critical role of the LR component in accurately modeling atomic interactions.  

Below, we highlight the advantages of the SOG-Net compared to previous methods tested on the same dataset. In the generalized LODE method~\cite{huguenin2023physics}, the training accuracy depends heavily on guessing the potential exponent $p$ for the LR tail of each dimer class. In contrast, the SOG-Net eliminates the need for such manual tuning, as its SOG multiplier adaptively fits the LR tail during training. Additionally, the recently proposed latent Ewald method~\cite{cheng2024latent} also learns latent variables from invariant features but employs the Ewald summation for evaluating the LR energy. The latent Ewald approach is well-suited for interactions with a $1/r$ LR tail, offering benefits for the CC class; however, it yields only modest improvements for non-CC classes. The SOG-Net is able to accurately capture diverse LR tails across both CC and five non-CC cases. For example, in the CC, CP, and PP classes, the force RMSEs are $4.5$, $4.1$, and $0.51$ meV, respectively, compared to approximately $18$, $20$, and $5$ meV for the LODE method (with optimal $p$) and $5.9$, $16.5$, and $3.0$ meV for the latent Ewald approach. These results demonstrate the outstanding performance of the SOG-Net.

Finally, we applied the SOG-Net to a dataset of $1900$ liquid-water configurations, each containing $300$ atoms. The system has a density of $1~g/ml$ at $T = 300~K$. The dataset was generated via DFT calculations using the projector-augmented wave method~\cite{kresse1996efficient}. The electronic exchange-correlation energy was described using the generalized gradient approximation with the Perdew–Burke–Ernzerhof functional~\cite{payne1992iterative}. Details of the training setup for the SR component are provided in the SM, Section~IV~\cite{supplementary_information}. For the LR component, we use $P=16$ layers of latent variables, $M=12$ Gaussians for the SOG multiplier, and $31$ Fourier modes for each direction. The learning curves of SOG-Net, shown in Figure~SII in the SM~\cite{supplementary_information}, demonstrate that incorporating the LR component into the SOG-Net significantly reduces the error compared with the pure SR component. 

We performed MD simulations under the NVT ensemble. Figure~\ref{fig:water_rdf}(a) displays the oxygen-oxygen radial distribution function (RDF). RDFs for the oxygen-hydrogen and hydrogen-hydrogen pairs are provided in the SM (Figure~SIII)~\cite{supplementary_information}. All the computed RDFs are nearly identical and show excellent agreement with the DFT results. %, suggesting that the SOG-Net framework can accurately predict the structural properties of bulk liquid water. 
In Figure~\ref{fig:water_rdf}(b), we present the longitudinal component of the charge structure factor along the $z$-axis, defined as
\begin{equation}
S(\bm{k})=\frac{1}{V}\sum_{ij=1}^{N}q_iq_j\langle e^{i\bm{k}\cdot \bm{r}_{ij}}\rangle,
\end{equation}
where $\langle\cdot\rangle$ denotes the ensemble average. The charge structure factor in the long-wavelength limit reflects the accuracy of reproducing LR correlations in water~\cite{hu2022symmetry}. The results show that our LR model could adequately describe the charge structure factors, whereas pure SR models diverge sharply as $k\rightarrow 0$, consistent with observations in prior studies on SR and LR electrostatics~\cite{cox2020dielectric,schlaich2016water}. Additionally, we compute the average number of hydrogen bonds per water molecule using the pure SR model and the full SOG-Net model, as shown in the inset of Figure~\ref{fig:water_rdf}(b). The results confirm that incorporating the LR component significantly enhances the accuracy.
The time performance of the SOG-Net during MD simulations is given in Figure~SIV of the SM~\cite{supplementary_information}, demonstrating the linear scaling of the model. Although the theoretical complexity of the FFT and IFFT operators is $O(N\log N)$, their contribution to the overall computation time in the LR convolution layer is minimal. The CPU time of the full SOG-Net model is only slightly higher ($10\%-40\%$, depending on the system size) than that of the pure SR model, indicating that SOG-Net achieves significant improvements in fitting accuracy with only a modest increase in computational cost.

\begin{figure*}[!htbp]
    \centering    \includegraphics[width=0.85\textwidth]{./water_rdf.pdf}
    \caption{(a) Predicted radial distribution functions (RDFs) of oxygen-oxygen (O-O) atom pairs of water at $T=300 ~K$ and $1 g/mL$, using SR or LR models compared with the DFT results. The inset provides a snapshot of the simulated system, and the test errors are listed in the lower right corner. (b) Predicted charge-charge structure correlations over charge density in reciprocal
    space as a function of $k$ values, using SR or LR models. The inset presents a plot of the average number of hydrogen bonds per water molecule.}
    \label{fig:water_rdf}
\end{figure*}

In conclusion, we have developed an efficient and versatile framework, SOG-Net, for modeling LR interactions in atomistic systems. Unlike existing LR methods, the SOG-Net requires no user-defined electrostatics or dispersion baseline corrections and does not rely on the classical Ewald summation, which assumes a predefined decay rate of $1/r$. It exhibits spectral convergence in learning and testing errors with the number of Fourier modes. Moreover, the LR MLIP incurs only a modest computational overhead compared to the pure SR network. This cost can be further reduced for both SR and LR components by leveraging acceleration techniques, such as random batch-type schemes~\cite{liang2021random,liang2023random}. By addressing these challenges, the SOG-Net provides a robust foundation for large-scale simulations with first-principles accuracy that account for LR interactions.
Notably, the LR convolution layer in the SOG-Net can also be integrated into existing methods such as HDNNP~\cite{behler2007generalized}, ACE~\cite{drautz2019atomic}, MACE~\cite{batatia2022mace}, CACE~\cite{cheng2024cartesian}, GAP~\cite{bartok2010gaussian}, and MTPs~\cite{shapeev2016moment}. In these approaches, after latent variables, such as partial charges, Wannier centers, or electronegativity, are learned through existing frameworks~\cite{unke2019physnet,ko2021fourth,gao2022self,zhang2022deep,rappe1991charge}, the SOG-Net can be readily integrated to enable adaptive learning of LR decay tails. This integration provides a simple mathematical formulation and achieves near-optimal computational complexity.


\bigskip
\textit{Acknowledgments}---This work was supported by the National Natural Science Foundation of China (Grants No. 12325113, 12401570, 12426304 and 124B2023) and the Science and Technology Commission of Shanghai Municipality (Grant No. 23JC1402300). The work of J. L. is partially supported by the China Postdoctoral Science Foundation (grant No. 2024M751948). The authors would like to thank the support from the SJTU Kunpeng \& Ascend Center of Excellence.

% Create the reference section using BibTeX:
%\bibliography{apssamp}
%apsrev4-2.bst 2019-01-14 (MD) hand-edited version of apsrev4-1.bst
%Control: key (0)
%Control: author (8) initials jnrlst
%Control: editor formatted (1) identically to author
%Control: production of article title (0) allowed
%Control: page (0) single
%Control: year (1) truncated
%Control: production of eprint (0) enabled
\begin{thebibliography}{49}%
\makeatletter
\providecommand \@ifxundefined [1]{%
 \@ifx{#1\undefined}
}%
\providecommand \@ifnum [1]{%
 \ifnum #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \@ifx [1]{%
 \ifx #1\expandafter \@firstoftwo
 \else \expandafter \@secondoftwo
 \fi
}%
\providecommand \natexlab [1]{#1}%
\providecommand \enquote  [1]{``#1''}%
\providecommand \bibnamefont  [1]{#1}%
\providecommand \bibfnamefont [1]{#1}%
\providecommand \citenamefont [1]{#1}%
\providecommand \href@noop [0]{\@secondoftwo}%
\providecommand \href [0]{\begingroup \@sanitize@url \@href}%
\providecommand \@href[1]{\@@startlink{#1}\@@href}%
\providecommand \@@href[1]{\endgroup#1\@@endlink}%
\providecommand \@sanitize@url [0]{\catcode `\\12\catcode `\$12\catcode `\&12\catcode `\#12\catcode `\^12\catcode `\_12\catcode `\%12\relax}%
\providecommand \@@startlink[1]{}%
\providecommand \@@endlink[0]{}%
\providecommand \url  [0]{\begingroup\@sanitize@url \@url }%
\providecommand \@url [1]{\endgroup\@href {#1}{\urlprefix }}%
\providecommand \urlprefix  [0]{URL }%
\providecommand \Eprint [0]{\href }%
\providecommand \doibase [0]{https://doi.org/}%
\providecommand \selectlanguage [0]{\@gobble}%
\providecommand \bibinfo  [0]{\@secondoftwo}%
\providecommand \bibfield  [0]{\@secondoftwo}%
\providecommand \translation [1]{[#1]}%
\providecommand \BibitemOpen [0]{}%
\providecommand \bibitemStop [0]{}%
\providecommand \bibitemNoStop [0]{.\EOS\space}%
\providecommand \EOS [0]{\spacefactor3000\relax}%
\providecommand \BibitemShut  [1]{\csname bibitem#1\endcsname}%
\let\auto@bib@innerbib\@empty
%</preamble>
\bibitem [{\citenamefont {Friederich}\ \emph {et~al.}(2021)\citenamefont {Friederich}, \citenamefont {H{\"a}se}, \citenamefont {Proppe},\ and\ \citenamefont {Aspuru-Guzik}}]{friederich2021machine}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Friederich}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {H{\"a}se}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Proppe}},\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Aspuru-Guzik}},\ }\bibfield  {title} {\bibinfo {title} {Machine-learned potentials for next-generation matter simulations},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Mater.}\ }\textbf {\bibinfo {volume} {20}},\ \bibinfo {pages} {750} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Unke}\ \emph {et~al.}(2021{\natexlab{a}})\citenamefont {Unke}, \citenamefont {Chmiela}, \citenamefont {Sauceda}, \citenamefont {Gastegger}, \citenamefont {Poltavsky}, \citenamefont {Sch{\"u}tt}, \citenamefont {Tkatchenko},\ and\ \citenamefont {M{\"u}ller}}]{unke2021machine}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {O.~T.}\ \bibnamefont {Unke}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Chmiela}}, \bibinfo {author} {\bibfnamefont {H.~E.}\ \bibnamefont {Sauceda}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Gastegger}}, \bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Poltavsky}}, \bibinfo {author} {\bibfnamefont {K.~T.}\ \bibnamefont {Sch{\"u}tt}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Tkatchenko}},\ and\ \bibinfo {author} {\bibfnamefont {K.~R.}\ \bibnamefont {M{\"u}ller}},\ }\bibfield  {title} {\bibinfo {title} {Machine learning force fields},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Chem. Rev.}\ }\textbf {\bibinfo {volume} {121}},\ \bibinfo {pages} {10142} (\bibinfo {year} {2021}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhang}\ \emph {et~al.}(2018)\citenamefont {Zhang}, \citenamefont {Han}, \citenamefont {Wang}, \citenamefont {Car},\ and\ \citenamefont {E}}]{zhang2018deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Zhang}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Han}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Car}},\ and\ \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {E}},\ }\bibfield  {title} {\bibinfo {title} {Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {120}},\ \bibinfo {pages} {143001} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kohn}(1996)}]{kohn1996density}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Kohn}},\ }\bibfield  {title} {\bibinfo {title} {Density functional and density matrix method scaling linearly with the number of atoms},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {76}},\ \bibinfo {pages} {3168} (\bibinfo {year} {1996})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Prodan}\ and\ \citenamefont {Kohn}(2005)}]{PNAS2005}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {Prodan}}\ and\ \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {Kohn}},\ }\bibfield  {title} {\bibinfo {title} {Nearsightedness of electronic matter},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {P. Nat. Acad. Sci.}\ }\textbf {\bibinfo {volume} {102}},\ \bibinfo {pages} {11635} (\bibinfo {year} {2005})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhou}\ and\ \citenamefont {Pang}(2018)}]{zhou2018electrostatic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {H.-X.}\ \bibnamefont {Zhou}}\ and\ \bibinfo {author} {\bibfnamefont {X.}~\bibnamefont {Pang}},\ }\bibfield  {title} {\bibinfo {title} {Electrostatic interactions in protein structure, folding, binding, and condensation},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Chem. Rev.}\ }\textbf {\bibinfo {volume} {118}},\ \bibinfo {pages} {1691} (\bibinfo {year} {2018})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Yuan}\ and\ \citenamefont {Tanaka}(2024)}]{PhysRevLett.132.228101}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Yuan}}\ and\ \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Tanaka}},\ }\bibfield  {title} {\bibinfo {title} {Charge regulation effects in polyelectrolyte adsorption},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {132}},\ \bibinfo {pages} {228101} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bart{\'o}k}\ \emph {et~al.}(2010)\citenamefont {Bart{\'o}k}, \citenamefont {Payne}, \citenamefont {Kondor},\ and\ \citenamefont {Cs{\'a}nyi}}]{bartok2010gaussian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~P.}\ \bibnamefont {Bart{\'o}k}}, \bibinfo {author} {\bibfnamefont {M.~C.}\ \bibnamefont {Payne}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Kondor}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Cs{\'a}nyi}},\ }\bibfield  {title} {\bibinfo {title} {Gaussian approximation potentials: {T}he accuracy of quantum mechanics, without the electrons},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {104}},\ \bibinfo {pages} {136403} (\bibinfo {year} {2010})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Unke}\ \emph {et~al.}(2021{\natexlab{b}})\citenamefont {Unke}, \citenamefont {Chmiela}, \citenamefont {Gastegger}, \citenamefont {Sch{\"u}tt}, \citenamefont {Sauceda},\ and\ \citenamefont {M{\"u}ller}}]{Unke2021}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {O.~T.}\ \bibnamefont {Unke}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Chmiela}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Gastegger}}, \bibinfo {author} {\bibfnamefont {K.~T.}\ \bibnamefont {Sch{\"u}tt}}, \bibinfo {author} {\bibfnamefont {H.~E.}\ \bibnamefont {Sauceda}},\ and\ \bibinfo {author} {\bibfnamefont {K.~R.}\ \bibnamefont {M{\"u}ller}},\ }\bibfield  {title} {\bibinfo {title} {{SpookyNet: Learning force fields with electronic degrees of freedom and nonlocal effects}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {7273} (\bibinfo {year} {2021}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhang}\ \emph {et~al.}(2022)\citenamefont {Zhang}, \citenamefont {Wang}, \citenamefont {Muniz}, \citenamefont {Panagiotopoulos}, \citenamefont {Car} \emph {et~al.}}]{zhang2022deep}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Zhang}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {M.~C.}\ \bibnamefont {Muniz}}, \bibinfo {author} {\bibfnamefont {A.~Z.}\ \bibnamefont {Panagiotopoulos}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Car}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {A deep potential model with long-range electrostatic interactions},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {156}},\ \bibinfo {pages} {124107} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Gao}\ and\ \citenamefont {Remsing}(2022)}]{gao2022self}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Gao}}\ and\ \bibinfo {author} {\bibfnamefont {R.~C.}\ \bibnamefont {Remsing}},\ }\bibfield  {title} {\bibinfo {title} {Self-consistent determination of long-range electrostatics in neural network potentials},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {13}},\ \bibinfo {pages} {1572} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Unke}\ and\ \citenamefont {Meuwly}(2019)}]{unke2019physnet}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {O.~T.}\ \bibnamefont {Unke}}\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Meuwly}},\ }\bibfield  {title} {\bibinfo {title} {{PhysNet: A neural network for predicting energies, forces, dipole moments, and partial charges}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Theory Comput.}\ }\textbf {\bibinfo {volume} {15}},\ \bibinfo {pages} {3678} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ko}\ \emph {et~al.}(2021)\citenamefont {Ko}, \citenamefont {Finkler}, \citenamefont {Goedecker},\ and\ \citenamefont {Behler}}]{ko2021fourth}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.~W.}\ \bibnamefont {Ko}}, \bibinfo {author} {\bibfnamefont {J.~A.}\ \bibnamefont {Finkler}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Goedecker}},\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Behler}},\ }\bibfield  {title} {\bibinfo {title} {A fourth-generation high-dimensional neural network potential with accurate electrostatics including non-local charge transfer},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {12}},\ \bibinfo {pages} {398} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Shaidu}\ \emph {et~al.}(2024)\citenamefont {Shaidu}, \citenamefont {Pellegrini}, \citenamefont {K{\"u}{c}{\"u}kbenli}, \citenamefont {Lot},\ and\ \citenamefont {de~Gironcoli}}]{shaidu2024incorporating}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Shaidu}}, \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Pellegrini}}, \bibinfo {author} {\bibfnamefont {E.}~\bibnamefont {K{\"u}{c}{\"u}kbenli}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Lot}},\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {de~Gironcoli}},\ }\bibfield  {title} {\bibinfo {title} {Incorporating long-range electrostatics in neural network potentials via variational charge equilibration from shortsighted ingredients},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {NPJ Comput. Mater.}\ }\textbf {\bibinfo {volume} {10}},\ \bibinfo {pages} {47} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ewald}(1921)}]{ewald1921berechnung}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.~P.}\ \bibnamefont {Ewald}},\ }\bibfield  {title} {\bibinfo {title} {{Die Berechnung optischer und elektrostatischer Gitterpotentiale}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Ann. Phys.}\ }\textbf {\bibinfo {volume} {369}},\ \bibinfo {pages} {253} (\bibinfo {year} {1921})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Zhang}\ \emph {et~al.}(2021)\citenamefont {Zhang}, \citenamefont {Wang}, \citenamefont {Car},\ and\ \citenamefont {E}}]{zhang2021phase}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Zhang}}, \bibinfo {author} {\bibfnamefont {H.}~\bibnamefont {Wang}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Car}},\ and\ \bibinfo {author} {\bibfnamefont {W.}~\bibnamefont {E}},\ }\bibfield  {title} {\bibinfo {title} {Phase diagram of a deep potential water model},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {126}},\ \bibinfo {pages} {236001} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Bore}\ and\ \citenamefont {Paesani}(2023)}]{bore2023realistic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.~L.}\ \bibnamefont {Bore}}\ and\ \bibinfo {author} {\bibfnamefont {F.}~\bibnamefont {Paesani}},\ }\bibfield  {title} {\bibinfo {title} {Realistic phase diagram of water from ``first principles'' data-driven quantum simulations},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {14}},\ \bibinfo {pages} {3349} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Israelachvili}(2011)}]{israelachvili2011intermolecular}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.~N.}\ \bibnamefont {Israelachvili}},\ }\href@noop {} {\emph {\bibinfo {title} {{Intermolecular and Surface Forces}}}}\ (\bibinfo  {publisher} {Academic Press},\ \bibinfo {year} {2011})\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Huguenin-Dumittan}\ \emph {et~al.}(2023)\citenamefont {Huguenin-Dumittan}, \citenamefont {Loche}, \citenamefont {Haoran},\ and\ \citenamefont {Ceriotti}}]{huguenin2023physics}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.~K.}\ \bibnamefont {Huguenin-Dumittan}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Loche}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Haoran}},\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ceriotti}},\ }\bibfield  {title} {\bibinfo {title} {Physics-inspired equivariant descriptors of nonbonded interactions},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Phys. Chem. Lett.}\ }\textbf {\bibinfo {volume} {14}},\ \bibinfo {pages} {9612} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Grisafi}\ and\ \citenamefont {Ceriotti}(2019)}]{Grisafi2019JCP}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Grisafi}}\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ceriotti}},\ }\bibfield  {title} {\bibinfo {title} {{Incorporating long-range physics in atomic-scale machine learning}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {151}},\ \bibinfo {pages} {204105} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Sch{\"u}tt}\ \emph {et~al.}(2017)\citenamefont {Sch{\"u}tt}, \citenamefont {Kindermans}, \citenamefont {Sauceda~Felix}, \citenamefont {Chmiela}, \citenamefont {Tkatchenko},\ and\ \citenamefont {M{\"u}ller}}]{schutt2017schnet}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Sch{\"u}tt}}, \bibinfo {author} {\bibfnamefont {P.-J.}\ \bibnamefont {Kindermans}}, \bibinfo {author} {\bibfnamefont {H.~E.}\ \bibnamefont {Sauceda~Felix}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Chmiela}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Tkatchenko}},\ and\ \bibinfo {author} {\bibfnamefont {K.~R.}\ \bibnamefont {M{\"u}ller}},\ }\bibfield  {title} {\bibinfo {title} {Schnet: A continuous-filter convolutional neural network for modeling quantum interactions},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Adv. Neural Inf. Process. Syst. (NeurIPS)}\ }\textbf {\bibinfo {volume} {30}},\ \bibinfo {pages} {992} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Batzner}\ \emph {et~al.}(2022)\citenamefont {Batzner}, \citenamefont {Musaelian}, \citenamefont {Sun}, \citenamefont {Geiger}, \citenamefont {Mailoa}, \citenamefont {Kornbluth}, \citenamefont {Molinari}, \citenamefont {Smidt},\ and\ \citenamefont {Kozinsky}}]{batzner20223}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Batzner}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Musaelian}}, \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Sun}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Geiger}}, \bibinfo {author} {\bibfnamefont {J.~P.}\ \bibnamefont {Mailoa}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kornbluth}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Molinari}}, \bibinfo {author} {\bibfnamefont {T.~E.}\ \bibnamefont {Smidt}},\ and\ \bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Kozinsky}},\ }\bibfield  {title} {\bibinfo {title} {E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Commun.}\ }\textbf {\bibinfo {volume} {13}},\ \bibinfo {pages} {2453} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Batatia}\ \emph {et~al.}(2022)\citenamefont {Batatia}, \citenamefont {Kovacs}, \citenamefont {Simm}, \citenamefont {Ortner},\ and\ \citenamefont {Cs{\'a}nyi}}]{batatia2022mace}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {I.}~\bibnamefont {Batatia}}, \bibinfo {author} {\bibfnamefont {D.~P.}\ \bibnamefont {Kovacs}}, \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Simm}}, \bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Ortner}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Cs{\'a}nyi}},\ }\bibfield  {title} {\bibinfo {title} {{MACE: Higher order equivariant message passing neural networks for fast and accurate force fields}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Adv. Neural Inf. Process. Syst. (NeurIPS)}\ }\textbf {\bibinfo {volume} {35}},\ \bibinfo {pages} {11423} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kosmala}\ \emph {et~al.}(2023)\citenamefont {Kosmala}, \citenamefont {Gasteiger}, \citenamefont {Gao},\ and\ \citenamefont {G{\"u}nnemann}}]{kosmala2023ewald}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Kosmala}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Gasteiger}}, \bibinfo {author} {\bibfnamefont {N.}~\bibnamefont {Gao}},\ and\ \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {G{\"u}nnemann}},\ }\bibfield  {title} {\bibinfo {title} {Ewald-based long-range message passing for molecular graphs},\ }in\ \href@noop {} {\emph {\bibinfo {booktitle} {International Conference on Machine Learning}}}\ (\bibinfo {organization} {ICML},\ \bibinfo {year} {2023})\ pp.\ \bibinfo {pages} {17544--17563}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Deng}\ \emph {et~al.}(2023)\citenamefont {Deng}, \citenamefont {Zhong}, \citenamefont {Jun}, \citenamefont {Riebesell}, \citenamefont {Han}, \citenamefont {Bartel},\ and\ \citenamefont {Ceder}}]{deng2023chgnet}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Deng}}, \bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Zhong}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Jun}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Riebesell}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Han}}, \bibinfo {author} {\bibfnamefont {C.~J.}\ \bibnamefont {Bartel}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Ceder}},\ }\bibfield  {title} {\bibinfo {title} {{CHGNet} as a pretrained universal neural network potential for charge-informed atomistic modelling},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Mach. Intell.}\ }\textbf {\bibinfo {volume} {5}},\ \bibinfo {pages} {1031} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Dutt}\ and\ \citenamefont {Rokhlin}(1993)}]{dutt1993fast}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Dutt}}\ and\ \bibinfo {author} {\bibfnamefont {V.}~\bibnamefont {Rokhlin}},\ }\bibfield  {title} {\bibinfo {title} {Fast {F}ourier transforms for nonequispaced data},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {SIAM J. Sci. Comput.}\ }\textbf {\bibinfo {volume} {14}},\ \bibinfo {pages} {1368} (\bibinfo {year} {1993})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Greengard}\ and\ \citenamefont {Lee}(2004)}]{greengard2004accelerating}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {Greengard}}\ and\ \bibinfo {author} {\bibfnamefont {J.-Y.}\ \bibnamefont {Lee}},\ }\bibfield  {title} {\bibinfo {title} {Accelerating the nonuniform fast {F}ourier transform},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {SIAM Rev.}\ }\textbf {\bibinfo {volume} {46}},\ \bibinfo {pages} {443} (\bibinfo {year} {2004})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Ko}\ and\ \citenamefont {Ong}(2023)}]{ko2023recent}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {T.~W.}\ \bibnamefont {Ko}}\ and\ \bibinfo {author} {\bibfnamefont {S.~P.}\ \bibnamefont {Ong}},\ }\bibfield  {title} {\bibinfo {title} {Recent advances and outstanding challenges for machine learning interatomic potentials},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Nat. Comput. Sci.}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {998} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{sup()}]{supplementary_information}%
  \BibitemOpen
  \href@noop {} {\bibinfo {title} {See supplementary materials for detailed mathematical derivations, numerical quadrature scheme, its error analysis, and numerical validations}},\ {includes references [8,10,13,14,19-28,31,32,36,46,47]}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Folland}\ and\ \citenamefont {Sitaram}(1997)}]{folland1997uncertainty}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.~B.}\ \bibnamefont {Folland}}\ and\ \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Sitaram}},\ }\bibfield  {title} {\bibinfo {title} {The uncertainty principle: a mathematical survey},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Fourier Anal. App.}\ }\textbf {\bibinfo {volume} {3}},\ \bibinfo {pages} {207} (\bibinfo {year} {1997})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Faller}\ \emph {et~al.}(2024)\citenamefont {Faller}, \citenamefont {Kaltak},\ and\ \citenamefont {Kresse}}]{faller2024density}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {C.}~\bibnamefont {Faller}}, \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Kaltak}},\ and\ \bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Kresse}},\ }\bibfield  {title} {\bibinfo {title} {Density-based long-range electrostatic descriptors for machine learning force fields},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {161}} (\bibinfo {year} {2024})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Benner}\ \emph {et~al.}(2015)\citenamefont {Benner}, \citenamefont {Gugercin},\ and\ \citenamefont {Willcox}}]{benner2015survey}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {P.}~\bibnamefont {Benner}}, \bibinfo {author} {\bibfnamefont {S.}~\bibnamefont {Gugercin}},\ and\ \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Willcox}},\ }\bibfield  {title} {\bibinfo {title} {A survey of projection-based model reduction methods for parametric dynamical systems},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {SIAM Rev.}\ }\textbf {\bibinfo {volume} {57}},\ \bibinfo {pages} {483} (\bibinfo {year} {2015})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Barnett}\ \emph {et~al.}(2019)\citenamefont {Barnett}, \citenamefont {Magland},\ and\ \citenamefont {af~Klinteberg}}]{Barnett2019SISC}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~H.}\ \bibnamefont {Barnett}}, \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Magland}},\ and\ \bibinfo {author} {\bibfnamefont {L.}~\bibnamefont {af~Klinteberg}},\ }\bibfield  {title} {\bibinfo {title} {A parallel nonuniform fast {F}ourier transform library based on an ``exponential of semicircle'' kernel},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {SIAM J. Sci. Comput.}\ }\textbf {\bibinfo {volume} {41}},\ \bibinfo {pages} {C479} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Thompson}\ \emph {et~al.}(2022)\citenamefont {Thompson}, \citenamefont {Aktulga}, \citenamefont {Berger}, \citenamefont {Bolintineanu}, \citenamefont {Brown}, \citenamefont {Crozier}, \citenamefont {In't~Veld}, \citenamefont {Kohlmeyer}, \citenamefont {Moore}, \citenamefont {Nguyen} \emph {et~al.}}]{thompson2022lammps}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~P.}\ \bibnamefont {Thompson}}, \bibinfo {author} {\bibfnamefont {H.~M.}\ \bibnamefont {Aktulga}}, \bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Berger}}, \bibinfo {author} {\bibfnamefont {D.~S.}\ \bibnamefont {Bolintineanu}}, \bibinfo {author} {\bibfnamefont {W.~M.}\ \bibnamefont {Brown}}, \bibinfo {author} {\bibfnamefont {P.~S.}\ \bibnamefont {Crozier}}, \bibinfo {author} {\bibfnamefont {P.~J.}\ \bibnamefont {In't~Veld}}, \bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Kohlmeyer}}, \bibinfo {author} {\bibfnamefont {S.~G.}\ \bibnamefont {Moore}}, \bibinfo {author} {\bibfnamefont {T.~D.}\ \bibnamefont {Nguyen}}, \emph {et~al.},\ }\bibfield  {title} {\bibinfo {title} {{LAMMPS}-a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Comput. Phys. Commun.}\ }\textbf {\bibinfo {volume} {271}},\ \bibinfo {pages} {108171} (\bibinfo
  {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Burns}\ \emph {et~al.}(2017)\citenamefont {Burns}, \citenamefont {Faver}, \citenamefont {Zheng}, \citenamefont {Marshall}, \citenamefont {Smith}, \citenamefont {Vanommeslaeghe}, \citenamefont {MacKerell}, \citenamefont {Merz},\ and\ \citenamefont {Sherrill}}]{burns2017biofragment}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {L.~A.}\ \bibnamefont {Burns}}, \bibinfo {author} {\bibfnamefont {J.~C.}\ \bibnamefont {Faver}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Zheng}}, \bibinfo {author} {\bibfnamefont {M.~S.}\ \bibnamefont {Marshall}}, \bibinfo {author} {\bibfnamefont {D.~G.}\ \bibnamefont {Smith}}, \bibinfo {author} {\bibfnamefont {K.}~\bibnamefont {Vanommeslaeghe}}, \bibinfo {author} {\bibfnamefont {A.~D.}\ \bibnamefont {MacKerell}}, \bibinfo {author} {\bibfnamefont {K.~M.}\ \bibnamefont {Merz}},\ and\ \bibinfo {author} {\bibfnamefont {C.~D.}\ \bibnamefont {Sherrill}},\ }\bibfield  {title} {\bibinfo {title} {{The BioFragment Database (BFDb): An open-data platform for computational chemistry analysis of noncovalent interactions}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {147}},\ \bibinfo {pages} {161727} (\bibinfo {year} {2017})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Heyd}\ \emph {et~al.}(2003)\citenamefont {Heyd}, \citenamefont {Scuseria},\ and\ \citenamefont {Ernzerhof}}]{heyd2003hybrid}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Heyd}}, \bibinfo {author} {\bibfnamefont {G.~E.}\ \bibnamefont {Scuseria}},\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Ernzerhof}},\ }\bibfield  {title} {\bibinfo {title} {Hybrid functionals based on a screened {C}oulomb potential},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {118}},\ \bibinfo {pages} {8207} (\bibinfo {year} {2003})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cheng}(2024{\natexlab{a}})}]{cheng2024latent}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Cheng}},\ }\bibfield  {title} {\bibinfo {title} {Latent {E}wald summation for machine learning of long-range interactions},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {arXiv:2408.15165}\ } (\bibinfo {year} {2024}{\natexlab{a}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Kresse}\ and\ \citenamefont {Furthm{\"u}ller}(1996)}]{kresse1996efficient}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {G.}~\bibnamefont {Kresse}}\ and\ \bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Furthm{\"u}ller}},\ }\bibfield  {title} {\bibinfo {title} {Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. B}\ }\textbf {\bibinfo {volume} {54}},\ \bibinfo {pages} {11169} (\bibinfo {year} {1996})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Payne}\ \emph {et~al.}(1992)\citenamefont {Payne}, \citenamefont {Teter}, \citenamefont {Allan}, \citenamefont {Arias},\ and\ \citenamefont {Joannopoulos}}]{payne1992iterative}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {M.~C.}\ \bibnamefont {Payne}}, \bibinfo {author} {\bibfnamefont {M.~P.}\ \bibnamefont {Teter}}, \bibinfo {author} {\bibfnamefont {D.~C.}\ \bibnamefont {Allan}}, \bibinfo {author} {\bibfnamefont {T.}~\bibnamefont {Arias}},\ and\ \bibinfo {author} {\bibfnamefont {a.~J.}\ \bibnamefont {Joannopoulos}},\ }\bibfield  {title} {\bibinfo {title} {Iterative minimization techniques for ab initio total-energy calculations: molecular dynamics and conjugate gradients},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Rev. Mod. Phys.}\ }\textbf {\bibinfo {volume} {64}},\ \bibinfo {pages} {1045} (\bibinfo {year} {1992})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Hu}(2022)}]{hu2022symmetry}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Hu}},\ }\bibfield  {title} {\bibinfo {title} {The symmetry-preserving mean field condition for electrostatic correlations in bulk},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {156}} (\bibinfo {year} {2022})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cox}(2020)}]{cox2020dielectric}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {S.~J.}\ \bibnamefont {Cox}},\ }\bibfield  {title} {\bibinfo {title} {Dielectric response with short-ranged electrostatics},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {P. Nat. Acad. Sci.}\ }\textbf {\bibinfo {volume} {117}},\ \bibinfo {pages} {19746} (\bibinfo {year} {2020})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Schlaich}\ \emph {et~al.}(2016)\citenamefont {Schlaich}, \citenamefont {Knapp},\ and\ \citenamefont {Netz}}]{schlaich2016water}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.}~\bibnamefont {Schlaich}}, \bibinfo {author} {\bibfnamefont {E.~W.}\ \bibnamefont {Knapp}},\ and\ \bibinfo {author} {\bibfnamefont {R.~R.}\ \bibnamefont {Netz}},\ }\bibfield  {title} {\bibinfo {title} {Water dielectric effects in planar confinement},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {117}},\ \bibinfo {pages} {048001} (\bibinfo {year} {2016})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liang}\ \emph {et~al.}(2021)\citenamefont {Liang}, \citenamefont {Xu},\ and\ \citenamefont {Zhao}}]{liang2021random}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Liang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Xu}},\ and\ \bibinfo {author} {\bibfnamefont {Y.}~\bibnamefont {Zhao}},\ }\bibfield  {title} {\bibinfo {title} {Random-batch list algorithm for short-range molecular dynamics simulations},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Chem. Phys.}\ }\textbf {\bibinfo {volume} {155}},\ \bibinfo {pages} {044108} (\bibinfo {year} {2021})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Liang}\ \emph {et~al.}(2023)\citenamefont {Liang}, \citenamefont {Xu},\ and\ \citenamefont {Zhou}}]{liang2023random}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Liang}}, \bibinfo {author} {\bibfnamefont {Z.}~\bibnamefont {Xu}},\ and\ \bibinfo {author} {\bibfnamefont {Q.}~\bibnamefont {Zhou}},\ }\bibfield  {title} {\bibinfo {title} {{Random batch sum-of-Gaussians method for molecular dynamics simulations of particle systems}},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {SIAM J. Sci. Comput.}\ }\textbf {\bibinfo {volume} {45}},\ \bibinfo {pages} {B591} (\bibinfo {year} {2023})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Behler}\ and\ \citenamefont {Parrinello}(2007)}]{behler2007generalized}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {J.}~\bibnamefont {Behler}}\ and\ \bibinfo {author} {\bibfnamefont {M.}~\bibnamefont {Parrinello}},\ }\bibfield  {title} {\bibinfo {title} {Generalized neural-network representation of high-dimensional potential-energy surfaces},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. Lett.}\ }\textbf {\bibinfo {volume} {98}},\ \bibinfo {pages} {146401} (\bibinfo {year} {2007})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Drautz}(2019)}]{drautz2019atomic}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {R.}~\bibnamefont {Drautz}},\ }\bibfield  {title} {\bibinfo {title} {Atomic cluster expansion for accurate and transferable interatomic potentials},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Phys. Rev. B}\ }\textbf {\bibinfo {volume} {99}},\ \bibinfo {pages} {014104} (\bibinfo {year} {2019})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Cheng}(2024{\natexlab{b}})}]{cheng2024cartesian}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {B.}~\bibnamefont {Cheng}},\ }\bibfield  {title} {\bibinfo {title} {Cartesian atomic cluster expansion for machine learning interatomic potentials},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {NPJ Comput. Mater.}\ }\textbf {\bibinfo {volume} {10}},\ \bibinfo {pages} {157} (\bibinfo {year} {2024}{\natexlab{b}})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Shapeev}(2016)}]{shapeev2016moment}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~V.}\ \bibnamefont {Shapeev}},\ }\bibfield  {title} {\bibinfo {title} {Moment tensor potentials: {A} class of systematically improvable interatomic potentials},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {Multiscale Model. Simul.}\ }\textbf {\bibinfo {volume} {14}},\ \bibinfo {pages} {1153} (\bibinfo {year} {2016})}\BibitemShut {NoStop}%
\bibitem [{\citenamefont {Rappe}\ and\ \citenamefont {Goddard~III}(1991)}]{rappe1991charge}%
  \BibitemOpen
  \bibfield  {author} {\bibinfo {author} {\bibfnamefont {A.~K.}\ \bibnamefont {Rappe}}\ and\ \bibinfo {author} {\bibfnamefont {W.~A.}\ \bibnamefont {Goddard~III}},\ }\bibfield  {title} {\bibinfo {title} {Charge equilibration for molecular dynamics simulations},\ }\href@noop {} {\bibfield  {journal} {\bibinfo  {journal} {J. Phys. Chem.}\ }\textbf {\bibinfo {volume} {95}},\ \bibinfo {pages} {3358} (\bibinfo {year} {1991})}\BibitemShut {NoStop}%
\end{thebibliography}%

   
\end{document}
%
% ****** End of file apstemplate.tex ******

