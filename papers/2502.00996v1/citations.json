[
  {
    "index": 0,
    "papers": [
      {
        "key": "tang2023large",
        "author": "Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan",
        "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners"
      },
      {
        "key": "Li2023Deceiving",
        "author": "Bangzheng Li and Ben Zhou and Fei Wang and Xingyu Fu and Dan Roth and Muhao Chen",
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?"
      },
      {
        "key": "li2024famicom",
        "author": "Li, Bangzheng and Zhou, Ben and Fu, Xingyu and Wang, Fei and Roth, Dan and Chen, Muhao",
        "title": "FamiCom: Further Demystifying Prompts for Language Models with Task-Agnostic Performance Estimation"
      },
      {
        "key": "mirzadeh2024gsm",
        "author": "Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad",
        "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2022program",
        "author": "Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W",
        "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks"
      },
      {
        "key": "gao2023pal",
        "author": "Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham",
        "title": "Pal: Program-aided language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "feng2024bird",
        "author": "Feng, Yu and Zhou, Ben and Lin, Weidong and Roth, Dan",
        "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lyu2023faithful",
        "author": "Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris",
        "title": "Faithful Chain-of-Thought Reasoning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Hu2023InContextAR",
        "author": "Xiaoyang Hu and Shane Storks and Richard L. Lewis and Joyce Yue Chai",
        "title": "In-Context Analogical Reasoning with Pre-Trained Language Models"
      },
      {
        "key": "Yasunaga2023LargeLM",
        "author": "Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and Jure Leskovec and Percy Liang and Ed Huai-hsin Chi and Denny Zhou",
        "title": "Large Language Models as Analogical Reasoners"
      },
      {
        "key": "Yuan2023ANALOGYKBUA",
        "author": "Siyu Yuan and Jiangjie Chen and Changzhi Sun and Jiaqing Liang and Yanghua Xiao and Deqing Yang",
        "title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base"
      },
      {
        "key": "Zhou2023Symbolic",
        "author": "Zhou, Ben and Zhang, Hongming and Chen, Sihao and Yu, Dian and Wang, Hongwei and Peng, Baolin and Roth, Dan and Yu, Dong",
        "title": "Conceptual and Unbiased Reasoning in Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhu2024pad",
        "author": "Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xinwei and Lin, Zhouhan and Zhou, Bowen",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning"
      },
      {
        "key": "zelikman2024quiet",
        "author": "Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
        "title": "Quiet-star: Language models can teach themselves to think before speaking"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "khattab2022demonstrate",
        "author": "Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei",
        "title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"
      },
      {
        "key": "hao2023reasoning",
        "author": "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua and Wang, Zhen and Wang, Daisy and Hu, Zhiting",
        "title": "Reasoning with Language Model is Planning with World Model"
      },
      {
        "key": "yaoreact",
        "author": "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tang2023large",
        "author": "Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan",
        "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners"
      },
      {
        "key": "Li2023Deceiving",
        "author": "Bangzheng Li and Ben Zhou and Fei Wang and Xingyu Fu and Dan Roth and Muhao Chen",
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2022program",
        "author": "Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W",
        "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks"
      },
      {
        "key": "gao2023pal",
        "author": "Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham",
        "title": "Pal: Program-aided language models"
      },
      {
        "key": "Zhou2023Symbolic",
        "author": "Zhou, Ben and Zhang, Hongming and Chen, Sihao and Yu, Dian and Wang, Hongwei and Peng, Baolin and Roth, Dan and Yu, Dong",
        "title": "Conceptual and Unbiased Reasoning in Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhou2020temporal",
        "author": "Zhou, Ben and Richardson, Kyle and Ning, Qiang and Khot, Tushar and Sabharwal, Ashish and Roth, Dan",
        "title": "Temporal reasoning on implicit events from distant supervision"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Hu2023InContextAR",
        "author": "Xiaoyang Hu and Shane Storks and Richard L. Lewis and Joyce Yue Chai",
        "title": "In-Context Analogical Reasoning with Pre-Trained Language Models"
      },
      {
        "key": "Yasunaga2023LargeLM",
        "author": "Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and Jure Leskovec and Percy Liang and Ed Huai-hsin Chi and Denny Zhou",
        "title": "Large Language Models as Analogical Reasoners"
      },
      {
        "key": "Yuan2023ANALOGYKBUA",
        "author": "Siyu Yuan and Jiangjie Chen and Changzhi Sun and Jiaqing Liang and Yanghua Xiao and Deqing Yang",
        "title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base"
      }
    ]
  }
]