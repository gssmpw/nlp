@inproceedings{Hu2023InContextAR,
  title={In-Context Analogical Reasoning with Pre-Trained Language Models},
  author={Xiaoyang Hu and Shane Storks and Richard L. Lewis and Joyce Yue Chai},
  booktitle={ACL},
  year={2023}
}

@article{Li2023Deceiving,
  title={Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?},
  author={Bangzheng Li and Ben Zhou and Fei Wang and Xingyu Fu and Dan Roth and Muhao Chen},
  journal={NAACL},
  year={2023},
}

@article{Yasunaga2023LargeLM,
  title={Large Language Models as Analogical Reasoners},
  author={Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and Jure Leskovec and Percy Liang and Ed Huai-hsin Chi and Denny Zhou},
  journal={ICLR},
  year={2024}
}

@article{Yuan2023ANALOGYKBUA,
  title={ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base},
  author={Siyu Yuan and Jiangjie Chen and Changzhi Sun and Jiaqing Liang and Yanghua Xiao and Deqing Yang},
  journal={ACL},
  year={2024}
}

@article{Zhou2023Symbolic,
  title={Conceptual and Unbiased Reasoning in Language Models},
  author={Zhou, Ben and Zhang, Hongming and Chen, Sihao and Yu, Dian and Wang, Hongwei and Peng, Baolin and Roth, Dan and Yu, Dong},
  journal={arXiv preprint arXiv:2404.00205},
  year={2024}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{feng2024bird,
  title={BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models},
  author={Feng, Yu and Zhou, Ben and Lin, Weidong and Roth, Dan},
  journal={arXiv preprint arXiv:2404.12494},
  year={2024}
}

@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@inproceedings{hao2023reasoning,
  title={Reasoning with Language Model is Planning with World Model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua and Wang, Zhen and Wang, Daisy and Hu, Zhiting},
  booktitle={EMNLP},
  year={2023}
}

@article{khattab2022demonstrate,
  title={Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@article{li2024famicom,
  title={FamiCom: Further Demystifying Prompts for Language Models with Task-Agnostic Performance Estimation},
  author={Li, Bangzheng and Zhou, Ben and Fu, Xingyu and Wang, Fei and Roth, Dan and Chen, Muhao},
  journal={arXiv preprint arXiv:2406.11243},
  year={2024}
}

@inproceedings{lyu2023faithful,
  title={Faithful Chain-of-Thought Reasoning},
  author={Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
  booktitle={Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={305--329},
  year={2023}
}

@article{mirzadeh2024gsm,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@article{tang2023large,
  title={Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners},
  author={Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan},
  journal={arXiv preprint arXiv:2305.14825},
  year={2023}
}

@inproceedings{yaoreact,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{zelikman2024quiet,
  title={Quiet-star: Language models can teach themselves to think before speaking},
  author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
  journal={arXiv preprint arXiv:2403.09629},
  year={2024}
}

@article{zhou2020temporal,
  title={Temporal reasoning on implicit events from distant supervision},
  author={Zhou, Ben and Richardson, Kyle and Ning, Qiang and Khot, Tushar and Sabharwal, Ashish and Roth, Dan},
  journal={arXiv preprint arXiv:2010.12753},
  year={2020}
}

@inproceedings{zhu2024pad,
  title={PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning},
  author={Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xinwei and Lin, Zhouhan and Zhou, Bowen},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={2571--2597},
  year={2024}
}

