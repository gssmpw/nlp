\section{Related Work}
Our work builds on analytical works that discusses language model's inconsistency~\cite{tang2023large, Li2023Deceiving, li2024famicom, mirzadeh2024gsm}, and \framework{} is a learning scheme that mitigate such issues by encouraging consistency high-level solutions among relevant questions. \framework{} is related to previous works that use LLMs to generate programmatic solutions~\cite{chen2022program, gao2023pal} or other structures~\cite{feng2024bird}. Some of them also aims to improve model consistency~\cite{lyu2023faithful} via programs, but our work considers abstraction and high-level solutions, which can transfer better. Several works have also been using analogy to encourage generalization \cite{Hu2023InContextAR, Yasunaga2023LargeLM, Yuan2023ANALOGYKBUA, Zhou2023Symbolic}. Our work shares a similar motivation, but we are the first to propose automatic self-supervision methods and improve model performances in the training step. Our work is also related and motivated by other works that use self-supervision or distillation signals~\cite{zhu2024pad, zelikman2024quiet}. Decomposition-based inference pipelines~\cite{khattab2022demonstrate, hao2023reasoning, yaoreact} also inspires our work, as our programmatical solutions resembles a decomposed inference process.

% Our work builds on a shared belief among multiple works \cite{tang2023large, Li2023Deceiving} that they rely on superficial word biases. Some works try to acquire symbolic signals in the form of computer programs \cite{chen2022program, gao2023pal, Zhou2023Symbolic}, and our work borrows this formulation as the generation target, which is both controllable and explainable. Distant and self-supervision \cite{zhou2020temporal} are also relevant to our work since we acquire automatic signals that save time and money compared with human annotation. Several works have also been using analogto encourage generalization \cite{Hu2023InContextAR, Yasunaga2023LargeLM, Yuan2023ANALOGYKBUA}, and our work is the first to propose automatic self-supervision methods.