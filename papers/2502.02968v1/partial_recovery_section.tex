\section{\texorpdfstring{\hyperref[Problem:Partial Coverage]{Partial Recovery in the \ekecd[k]}}{Partial Recovery in the \ekecd[k]}}
\label{sec:partial_recovery}
We start by noting that the \kecd{1}{n} is equivalent to the standard \ccp{n} and therefore 
$\T{\kecd{1}{n}}{r}$ and $\ST{\kecd{1}{n}}{r}$ have the same distributions as their known standard counterparts.

We now address a connection between different \ekecd models.
\begin{lemma}
\label{lemma:n-k and k}
    For any integers $ 0 < k < n$ it holds that $\T{\kecd{k}{n}}{n}$ and $\T{\kecd{(n-k)}{n}}{n}$ have the same distribution.
\end{lemma}
\begin{proof}
    Denoting the problem's graph as $G=(C,V)$. We can use the same graph for both models as the amount of coupons is equal in both models.
    Let  $S=(S_C,S_V)=(\{c_1,\dots,c_k\},\{v_1,\dots,v_k\})$ be a sample in the \kecd{k}{n} model,
    such that $C$ is the sampled coupons and $V$ are their labels.
    We can define the equivalent sample in the \kecd{(n-k)}{n} model as $S'=(S'_C,S'_V)=(\{c_{k+1},\dots,c_n\},\{v_{k+1},\dots,v_n\})$.
    The probabilities for both samples is equal as $\binom{n}{n-k}=\binom{n}{k}$ and they give the same information if we know the graph vertices beforehand, as one can easily calculate $(S'_C,S'_V) = (S'\setminus C,V \setminus S'_V)$.
    For complete recovery, we need to sample each vertex at least once, so we surely have $(C,V)$.
\end{proof}


We turn to the \kecd{2}{} model, starting with a useful claim.
\begin{lemma}
\label{claim:knowing_c__size_3}
    Let $G(n) = (C\cup L, E)$ and $H=(C,S)$ graphs of \kecd{2}{}.
    We know the label of coupon $c\in C$, $L(c)$, if and only if $c$ is part of a connected component of size at least $3$ in the graph $H$. 
\end{lemma}

\begin{proof}   
We prove this considering both directions:

\begin{itemize}
    \item[$\Rightarrow$] 
    Suppose, for contradiction, that $c$ belongs to a component of size less than 3. There are two possibilities:
    \begin{enumerate}
        \item $c$ is an isolated coupon. In this case, $c$ has not been sampled, and thus we cannot deduce its label.
        \item $c$ is connected to exactly one other coupon. This is also insufficient to determine its label since the sample gives the 2 labels and 2 coupons without matching them.
    \end{enumerate}
    In both cases, we cannot deduce $L(c)$, contradicting the assumption that $L(c)$ is known. Therefore, if we know $L(c)$, $c$ must be part of a component of size at least 3.
    
    \item[$\Leftarrow$] 
    Now assume $c$ belongs to a connected component of size at least 3. We can deduce the label of $c$ by considering the following:
    
    Let $c'$ be a vertex in the same component as $c$ such that $d(c') \geq 2$. This means $c'$ appears in at least two samples, each containing different coupons. From this information, we can deduce the label of $c'$.
    
    Since the component is connected, there exists a path from $c'$ to $c$. After deducing the label of $c'$, we can propagate this information along the path to $c$, allowing us to infer $L(c)$ as well.
\end{itemize}

Thus, the claim is proved.
\end{proof}

As a result of \autoref{claim:knowing_c__size_3} we deduce the two following claims, 
in the context of any total number of coupons, $n$.
\begin{claim}
In the \kecd{2}{} model, recovering the first coupon implies the recovery of two additional coupons. That is,
    \vspace{-1ex}
    $$\T{\kecd{2}{n}}{r=1}  = \T{\kecd{2}{n}}{r=2} = \T{\kecd{2}{n}}{r=3}.$$
\end{claim}

\begin{claim}
\label{claim:minimum_2ECD}
In the \kecd{2}{} model, one must sample at least $\left\lceil \frac{2n}{3}\right\rceil$ for complete recovery:
$\Min{\kecd{2}{n}}{n} = \left\lceil \frac{2n}{3}\right\rceil$.
\end{claim}
This follows by observing that the number of edges (samples) in a graph $H$ 
is at least the total number of vertices (coupons) minus the number of connected components. 
For $H$ with $\left\lfloor \frac{n}{3}\right\rfloor$ components partition the vertices into triplets \note{last - 4 or 5} and put two edges in each.
\note{full proof in proposal}

\begin{theorem}
\label{theorem:expected_2ecd_specific_partialrecover_1}

For recovering a specific coupon in the \kecd{2}{} model, it holds that
$$\E[\ST{\kecd{2}{n}}{1}]  = \frac{n \left(n^3 + n^2 + 5n - 5\right)}{\left(n + 3\right)\left(5n - 4\right)}
= O(n^2).$$
\end{theorem}
\begin{proof}
    Let $t \in \mathbb{N}$ and denote by $c$ the specific coupon we aim to recover.
    We are interested in
    $P(\ST{\kecd{2}{n}}{1}>t)$
    because 
    $\E[\ST{\kecd{2}{n}}{1}] = \sum_{t=0}^{\infty}P(\ST{\kecd{2}{n}}{1}>t)$.
    One can notice that there are $2$ distinct events in which $c$ isn't recovered after $t$ samples:
    \begin{itemize}
        \item Event A: 
        We did not see the coupon $c$ in the $t$ draws.
        The probability is
        $P(A)=\frac{\binom{n-1}{2} ^t }{\binom{n}{2} ^t} = \left(\frac{n-2}{n}\right)^t$.
        \item Event B: 
        We have seen the coupon $c$ but not able to recover its label.
        This scenario happens if and only if the coupon only appeared with another coupon $c'$
        which itself did not appear with any other coupon.
        Denote by $\ell$ the number of times the sample $\{c,c'\}$ appeared.
        There are $n-1$ options to choose the coupon $c'$,
        $\binom{n-2}{2}$ options to choose the coupons for each of the $t-\ell$ samples 
        and $\binom{t}{\ell}$ options to choose which of the samples is $\{c,c'\}$.
        We can sum over all the options for $\ell$ and use the binomial expansion to get:
        $
        P(B) 
        =
        \frac{\sum_{\ell=1}^t {\binom{n-2}{2}^{t-\ell} \cdot \binom{t}{\ell} \cdot (n-1)}}{\binom{n}{2} ^t} 
        = 
        \frac{\left(\binom{n-2}{2} + 1 \right)^t - \binom{n-2}{2}^t}{\binom{n}{2} ^t} \cdot (n-1) 
        = 
        \left(\frac{(n-2)(n-3)+2}{n(n-1)}\right) ^t \cdot (n-1)
        -
        \left(\frac{(n-2)(n-3)}{n(n-1)}\right) ^t \cdot (n-1)
        $.
    \end{itemize}
    We denote $q_1 = \frac{n-2}{n}$,  $q_2 = \frac{(n-2)(n-3)+2}{n(n-1)}$ and $q_3 = \frac{(n-2)(n-3)}{n(n-1)}$.
    Since $A$ and $B$ are disjoint:
    \[\begin{array}{lcl}
    \hspace{-0.06cm}
    P(\ST{\kecd{2}{n}}{1}>t)  
    \hspace{-0.08cm}=\hspace{-0.08cm}
    P(A) \hspace{-0.05cm}+\hspace{-0.05cm} P(B) 
    \hspace{-0.08cm}=\hspace{-0.08cm}
    q_1^t
    \hspace{-0.02cm}+\hspace{-0.02cm}
    (q_2 ^t
    \hspace{-0.05cm}-\hspace{-0.05cm}
    q_3 ^t)
    (n\hspace{-0.05cm}-\hspace{-0.05cm}1)
    \end{array}
    \]
    Therefore,
    \begin{align*}
    \E[\ST{\kecd{2}{n}}{1}] 
    &=
    \sum_{t=0}^{\infty}
    \left(
    q_1^t
    +
    q_2 ^t (n-1)
    -
    q_3 ^t (n-1)
    \right)
    \\&=
    \frac{n}{2}
    +
    \frac{n(n-1)^2}{5n-4}
    -
    \frac{n(n-1)}{2(n+3)}
    \\&=
    \frac{n \left(n^3 + n^2 + 5n - 5\right)}{\left(n + 3\right)\left(5n - 4\right)} 
     \vspace{-2ex}
    \end{align*}
     \vspace{-1.5ex}
\end{proof}
\begin{theorem}
\label{theorem:expected_2ecd_nonSpecific_partialrecovered_1}
Denoting $m = (n-2)(n+1)/2$, for recovering any coupon in the \kecd{2}{}:
$$\E[\T{\kecd{2}{n}}{1}]  
        = 
        1
        +
        \sum_{i=1}^{\frac{n}{2}}
            \frac{n!}{(n-2i)! 2^i}
            \frac{\left(m- i\right)!}{m!}
        = 
        O(n)
            $$
\end{theorem}
The proof is similar to the proof of \autoref{theorem:expected_2ecd_specific_partialrecover_1}
and given in the journal version.
\note{the proof is commented}
