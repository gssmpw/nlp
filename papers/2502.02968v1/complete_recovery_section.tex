% \section{\autoref{Problem:complete_recovery}: Complete Recovery in the \efmecd}
\section{\texorpdfstring{\hyperref[Problem:complete_recovery]{Complete Recovery in the \efmecd}}{Complete Recovery in the \efmecd}}
\label{sec:complete_recovery}

Now, we shift our focus to the expected number of samples needed for complete recovery.

\begin{theorem}
    Let a RV $K$ be the sample size distribution 
    in both the \fmecd{K}{}{} and the \ccpg{K}{},
    the following holds:
    \begin{equation}
    \label{theorem:expected:MECD_vs_CCP}
        \E[\T{\fmecd{K}{P}{n}}{r=n}] > \E[\T{\ccpg{K}{n}}{r=n-1}].
    \end{equation}
\end{theorem}


\begin{proof}
% \noindent This proof uses the reduction in \autoref{fig:graph_reduction}.
Let \fmecd{K}{P}{n}
and $\ccpg{K}{n}$ be two models with the same sample size distribution, $K$.
Also, $\mathcal{P}(C)$ be the set of all possible samples of $C$.
Moreover, $\mathcal{S}(\mathcal{P}(C))$ is the set of all possible finite sequences of samples.
Every sequence with $i$ samples $A_i \in \mathcal{S}(\mathcal{P}(C))$ may recover one of the models, both or none.
To show \autoref{theorem:expected:MECD_vs_CCP}, we are interested in showing the following:

\[\forall i  \in \mathbb{N} ; P(\T{\fmecd{K}{P}{n}}{n} \leq i) \leq P(\T{\ccpg{K}{n}}{n-1} \leq i)\]
\[\exists j \in \mathbb{N} ; P(\T{\fmecd{K}{P}{n}}{n} \leq j) < P(\T{\ccpg{K}{n}}{n-1} \leq j)\]

Let $M \in \{\efmecd, \eccpg\}$. 
We say that $A\in M(r)$ 
if and only if 
the event $A$ in the model $M$ suffices a recovery of $r$ coupons. 
Now, we can separate to disjoint events: 
% \vspace{-1ex}
\[ P(\T{\fmecd{K}{P}{n}}{n} \leq i) =
\hspace{-0.6cm}
\sum_{A_i \in \fmecd{K}{P}{n}(n)} \hspace{-0.6cm}  P(A_i)
\]
\vspace{-1ex}
\[ P(\T{\ccpg{K}{n}}{n-1} \leq i) =
\hspace{-0.6cm} 
\sum_{A_i \in \ccpg{K}{n}(n-1)} \hspace{-0.6cm} P(A_i)
\]

The probabilities of the events are the same in both models as they share the same sample size distribution.
Therefore, we can use the graph equivalence of \autoref{fig:graph_reduction} 
between \ccpg{K}{n} and \fmecd{K}{P}{n} 
and notice that if an event $A_i$ recovers all the $n$ edges in \fmecd{K}{P}{n} 
it means we collected at least $n-1$ edges (and may have inferred the last one) 
and therefore recovered at least $n-1$ coupons in the \ccpg{K}{n} model.
Therefore:
\vspace{-1ex}
\begin{align*}
P(\T{\fmecd{K}{P}{n}}{n} \leq i) = & 
\sum_{A_i \in \fmecd{K}{P}{n}(n)} P(A_i) \\
\leq & \sum_{A_i \in \ccpg{K}{n}(n-1)} P(A_i)\\
= & P(\T{\ccpg{K}{n}}{n-1} \leq i)
\end{align*}

Now, we show that the inequality is strict by looking at the minimal number of samples for recovery in both models.
Denote $k_m=\arg\min_{k}\{|k - \frac{n}{2}| : P(K=k)>0\}$.
We now deduce $\Min{\ccpg{K}{n}}{n-1} \leq \left\lceil\frac{n-1}{k_m}\right\rceil$.

Since $\Min{\fmecd{K}{P}{n}}{n} = \Min{\kecd{k_m}{n}}{n} = \left\lceil\frac{2}{k_m+1} \cdot n \right\rceil >
\left\lceil\frac{n-1}{k_m}\right\rceil \geq \Min{\ccpg{K}{n}}{n-1}$,
we can infer $\Min{\fmecd{K}{P}{n}}{n} > \Min{\ccpg{K}{n}}{n-1}$, 
meaning that there is an event $A$ that recovers $n-1$ coupons in \ccpg{K}{n} 
but does not recover $n$ edges in \fmecd{K}{P}{n}.

This implies $P(\T{\fmecd{K}{P}{n}}{n}\hspace{-0.018cm} \leq \hspace{-0.018cm} i) < P(\T{\ccpg{K}{n}}{n-1} \hspace{-0.018cm} \leq \hspace{-0.018cm} i)$, and
therefore,
        $\E[\T{\fmecd{K}{P}{n}}{n}] >  \E[\T{\ccpg{K}{n}}{n-1}]$. 
\end{proof}

%%%%%%%%%%%%%%%

% \subsection{Mixed-ECD as a Markov Process}
\label{Markov:Finite-MECD}
We now shift our focus to the practical aspects of working with these models, demonstrating their representation as a Markov process.
We have already seen the sampling process in the \eccpg~\cite{Survey_ccp} and the \kecd{2}{n}~\cite{barlev2024} models
can be approached using Markov chains,
implying a process for calculating $\E[\T{\kecd{2}{n}}{n}]$.
We extend this to the \fmecd{K(p)}{}{n} model to account for the variability in random sample sizes,
extending the approach to calculating $\E[\T{\fmecd{K(p)}{}{n}}{n}]$.
\begin{theorem}
For any $p \in [0,1]$ the $\fmecd{K(p)}{}{n}$ model can be represented as a Markov chain. 
\label{Markov:(1;2)-MECD}
\end{theorem}
\begin{proof}
In the $\fmecd{K(p)}{}{n}$ model, each state of the Markov chain represents a configuration of collected coupons.  
We define the state as $s = (\alpha, \beta, \gamma)$, where:
$\alpha$ is the number of ``unknown'' coupons,
$\beta$ of ``partly known'' coupons (collected but labels still unknown) and
$\gamma$ of ``known'' coupons (collected and identified).
% The process $\{(\alpha_m, \beta_m, \gamma_m), m \in \mathbb{N}\}$, where $m$ is the number of samples taken, forms a Markov chain
% with the state space
To summarize, the Markov chain state space is
$\Sigma = \{(\alpha, \beta, \gamma) \mid \alpha + \beta + \gamma = n, \, \alpha, \beta, \gamma \in [0,n],  \beta \text{ is even}\}$,
and the transition matrix $M$, where $M_{s \to \ns}$ gives the probability of transitioning from state $s = (\alpha, \beta, \gamma)$ to $\ns = (\alpha', \beta', \gamma')$.

The transition matrix is calculated by considering all possible samples under the abstraction of the Markov process.
Using \autoref{claim:knowing_c__size_3}, $(\alpha, \beta, \gamma)$ are numbers of vertices in connected components of sizes $(1,2, \geq3)$.
Sample of size $1$ is viewed as moving this coupon to a connected component of size $3$, 
even if there is no such connected component.
One can add $3$ coupons to the system, assume they are already known, connected to each other and connect a coupon sampled alone to it.
We denote
$\mu = \frac{1-p}{\binom{n}{2}}, \lambda = \frac{p}{n}$,
as the coefficients for probability normalizing when counting the number of possible samples.

\vspace{-2.5ex}
\begin{equation}
\label{transistion_matrix_markov_(1;2)-MECD}
M_{s \to \ns}
\hspace{-0.1cm}=\hspace{-0.1cm}
% \ldots\\ \ldots=
\begin{cases}
%
\mu \binom{\alpha}{2}, & 
\ns = (\alpha - 2, \beta + 2, \gamma)\\
%
\mu \alpha \beta, &
\ns = (\alpha - 1, \beta - 2, \gamma + 3) \\
%
\mu \cdot 4 \binom{\beta/2}{2}, &
\ns = (\alpha,\beta - 4, \gamma + 4) \\
%
\mu \alpha \gamma + \lambda \alpha, & 
\ns = (\alpha - 1,\beta, \gamma + 1) \\
%
\mu \beta \gamma + \lambda \beta, & 
\ns = (\alpha, \beta - 2, \gamma + 2) \\
%
\mu \left(\binom{\gamma}{2} + \frac{\beta}{2}\right) + \lambda \gamma, & 
\ns = (\alpha, \beta, \gamma) \\
%
\ 0, & \text{otherwise}
\end{cases}
\end{equation}
To compute the expected number of samples required to collect all edges,
we analyze the hitting times of this Markov chain.
Specifically, we calculate the expected number of transitions (representing samples)
needed to reach the absorbing state $j=(0,0,n)$,
where all edges are known.
Standard Markov chain theory techniques are used to compute hitting times,
applying the recursive equation:
\[
E_{ij} = \begin{cases} 
0, & \text{if } i = j \\
\frac{1}{1 - P_{ii}} \left( 1 + \sum_{k \neq i} P_{ik} E_{kj} \right), & \text{if } i \neq j 
\end{cases}
\]
\end{proof}
% \vspace{-3ex}
\begin{claim}
    Let a RV $K$ be the sample size distribution. 
    The $\fmecd{K}{}{n}$ model can be represented as a Markov chain.
\end{claim}
As above, we do this by adding complexity to the fixed \ekecd[k] process.
In each transition the sample size is determined as a first step and then the transition probabilities of the relevant \ekecd[k] can be used, as a second step.
Namely:
\begin{enumerate}
    \item 
    Sample Size Decision: From the current 
    ``old state'', the process transitions to one of $\frac{n}{2}$ potential states, each representing a specific sample size.
    \item 
    Coupon Collection Transition: After the sample size is determined, the system evolves as in the \ekecd[k] Markov process, updating the different ``partly known'' counters (like $\beta$ in the proof for $\fmecd{K(p)}{}{}$, but more).
\end{enumerate}

Using the Markov approach,
we demonstrate that there is no simple convex solution to $\E[\T{\fmecd{K(p)}{}{n}}{n}]$.
According to our simulations and based on exact calculations for any specific $n$,
we conjectured that $\E[\T{\kecd{2}{n}}{n}] \approx \frac{n H_n}{2}$.
This is also independently observed in a parallel work~\cite{barlev2024}.
\label{NoSimpleConvexSolution}
Building on this conjecture, intuition may suggest that
% the following holds
        \[\begin{array}{lcl}
        \hspace{-0.16cm}
        \E[\T{\fmecd{K(p)}{}{n}}{n}] 
        &\hspace{-0.28cm}\stackrel{?}{=}&
        \hspace{-0.28cm}p \E[\T{\kecd{1}{n}}{n}] \hspace{-0.08cm} + \hspace{-0.08cm} (1-p) \E[\T{\kecd{2}{n}}{n}] 
        \\&\hspace{-0.28cm}\stackrel{?}{\approx}&
        \hspace{-0.28cm}p nH_n + (1-p) \frac{n H_n}{2}
        =
        p \frac{nH_n}{2}+\frac{nH_n}{2}
        \end{array}\]
    For a fixed number of coupons, the expression $nH_n$ is a constant, and therefore we would expect a linear relationship between $\E[\T{\fmecd{K(p)}{}{n}}{n}]$ and $p$. 
    However, as seen in \autoref{fig:avgTbyP} this is not typically the situation.
    % \vspace{-25ex}
    \begin{figure}[t]
    \centering
    \vspace{-2.6ex}
    \includegraphics[width=1\linewidth]{C2000S1000p10/box_plot_simulation_vs_markov_plot_20250116_150721.png}
    \caption{\footnotesize The normalized value of $\T{\fmecd{K(p)}{}{n}}{n}$ as a function of $p\in[0,1]$, for $n=2000$. %}
    %\descriptionlabel{
    % \raggedright
    %\parbox{\linewidth}{
    For any $p \in \{0,0.1,\ldots,1\}$, the exact value of $\E[\T{\fmecd{K(p)}{}{n}}{n}]$, 
    calculated by the Markov approach (\autoref{transistion_matrix_markov_(1;2)-MECD})
    is presented by the green curve, 
    while the dashed red line represents the convex combination.
    Additionally, we conducted simulations of $\T{\fmecd{K(p)}{}{n}}{n}$ and present, as box plots, results for $1000$ instances per all relevant values of $p$.\vspace{-4.5ex}}
    \label{fig:avgTbyP}
    \end{figure}
% \vspace{-1ex}
% To demonstrate this, we calculate the expected value for $n=3$, as shown in \autoref{theorem:K(p)-ECD(3)}.
% \ssb{line bellow}
\\
\autoref{theorem:K(p)-ECD(3)} provides a closed form solution for $n=3$.
\begin{theorem}
    \label{theorem:K(p)-ECD(3)}
    For any $p\in[0,1]$,
    we have that
    $\E[\T{\fmecd{K(p)}{}{3}}{r=n=3}] =
    \frac{13p^4+63p^3+74p^2-200p-120}{4(p+2)^2(3-p)}$.
\end{theorem}
\begin{proof}
Let $T \triangleq \T{\fmecd{K(p)}{}{3}}{3}$ for simplicity. 
We have that:
% \vspace{-3ex}
\[
\E[T] \hspace{-0.04cm}= \hspace{-0.08cm}\sum_{t=0}^\infty P(T > t) \hspace{-0.04cm}= \hspace{-0.08cm}\sum_{t=0}^\infty \sum_{s = 0}^t P(S=s) P(T > t \mid S\hspace{-0.04cm}=\hspace{-0.04cm}s)
\]
where $0 \leq S \leq T$ governs the number of samples of size-$1$ obtained in the first $t$ samples.

Note that $P(s) = \binom{t}{s} p^s (1-p)^{t-s}$ 
and the boundary cases:
\begin{itemize}
    \item $P(T > 0) = P(T > 1) = 1$.
    \item $P(T > t \mid S = 0) = \frac{1}{3^{t-1}}, P(T > t \mid S = 1) = \frac{3}{3^{t-1}}.$
    \item $P(T > t \mid S = t) = \frac{t}{3^{t-1}}.$
\end{itemize}

Additionally, for $t > 1$ and $1 < S < t$ there exist two cases where we fail to recover the graph with $3$ coupons.

\textbf{Case A:} All the $S$ samples of size-$1$ are of one specific coupon 
and all the $t-S$ samples of size-$2$ are of the same pair of $2$ coupons 
(this pair of coupons may contain the unique coupon that corresponds with the samples of size-$1$).
Thus, the relevant probability is $P(A^{t,s}) 
\triangleq
P(A \mid T=t, S=s)
= 
\frac{3^2}{3^t}.$

\textbf{Case B:} All $S$ samples of size-$1$ are of two specific coupons (excluding a specific coupon).
All $t-S$ samples of size-$2$
are of the same pair that contains the $2$ coupons allowed in the samples of size-$1$.
Note that there is freedom regarding how the $S$ samples are divided between the coupons. 
Thus, the relevant probability is
$
P(B^{t,s}) 
\triangleq 
P(B \mid T=t, S=s) 
=
\frac{3s}{3^t}.
$

Combining the cases for $t > 1$ and $1 < S < t$:
\vspace{-1ex}
\[
P(T > t \mid S = s) 
=
P(A^{t,s}) 
+
P(B^{t,s}) 
=
\frac{s +3}{3^{t-1}}.
\]

Finally, summing over all cases, we obtain:
%\begin{align*}
\[\begin{array}{l@{\hspace{0.4em}}c@{\hspace{0.4em}}l}
\mathbb{E}[T] 
&=& 
\sum_{t=0}^\infty P(T > t) 
\\& = &
2
+ 
\sum_{t=2}^\infty 
    \left(
    \sum_{s=2}^{t-1}
    \binom{t}{s} p^s (1-p)^{t-s} \frac{s +3}{3^{t-1}}
        \right. \\&& \left. \hspace{1.8cm}
        + \frac{(1-p)^t}{3^{t-1}} + \frac{t p (1-p)^{t-1} \cdot 3}{3^{t-1}} + \frac{p^t t}{3^{t-1}} 
    \right)
\\&\stackrel{(*)}{=}& 
\frac
{13p^4 + 63p^3 + 74p^2 - 200p - 120}
{4(p+2)^2(3-p)},
\end{array} 
\]
where (*) in the last equality follows from basic algebra and known series sums.
\end{proof}

Already knowing that $\Min{\kecd{2}{n}}{n} = \left\lceil \frac{2n}{3}\right\rceil$ (\autoref{claim:minimum_2ECD}) and $\Min{\kecd{1}{n}}{n} = n$ (sampling each coupon once), a question arises as to whether allowing samples of both models, from both sizes, reduces the minimal number of samples needed for a complete recovery. We show that for $n \geq 3$ this value in \fmecd{K(p)}{}{n} inherits the value in the \kecd{2}{n} model, formally, for any $n \geq 3$, we have that: 
% \vspace{-1ex}
$$
\Min{\fmecd{K(p)}{}{n}}{n} = \Min{\kecd{2}{n}}{n} = \left\lceil \frac{2n}{3}\right\rceil.
$$
Moreover, we conjectured that the minimal number of samples needed for complete recovery in \ekecd follows a linear relationship with $\frac{1}{k+1}$:
$
        \Min{\kecd{k}{n}}{n} = \left\lceil\frac{2}{k+1} \cdot n \right\rceil 
$.
\note{the proof for \kecd{2}{n} is commented}

This conjecture
was proven for values of $n$ that are divisible by $\binom{k+1}{2}$ in a parallel work~\cite{barlev2024}.
However, proving or refuting it for any $n > k$,  remains an open problem.

We extended this result and present the behavior in the \efmecd model and its relation to the \ekecd.
\begin{theorem}
\label{conjecture:minimal:MECD}
    Let a RV $K$ be the sample size distrubtion.
    The following holds:
    $\Min{\fmecd{K}{}{n}}{n} = \Min{\kecd{k_m}{n}}{n}$,
    where $k_m=\arg\min_{k}\{|k - \frac{n}{2}| : P(K=k)>0\}$.
\end{theorem}
Note that
$\arg\min_{k}\{|k - \frac{n}{2}| : P(K=k)>0 \}$
is equal to
$\arg\min_{k}\{\Min{\kecd{k}{n}}{n} : P(K=k)>0\}$
using the conjecture on the minimal number of samples needed for complete recovery in \ekecd and the \autoref{lemma:n-k and k}.
This theorem is proved using the Pigeonhole Principle and the proof is available in the journal version.

From the relation between the \ekecd and \ccpg{k}{} and the behavior of \Min{\kecd{k}{}}{n}, 
we conjecture a relationship between \ekecd[k] with different sample sizes and the general case \efmecd.

\begin{conjecture}
\label{conjecture:expected:(k1,k2)-MECD relation with k-ECD}
    Let $K = \{k_1:p, k_2:1-p\}$ and assume w.l.o.g. $k_1 < k_2 \leq \frac{n}{2}$. 
    The models $\fmecd{K}{}{n}$, \kecd{k_1}{n} and \kecd{k_2}{n} satisfy:
    $$
    \E[\T{\kecd{k_2}{n}}{n}] \leq \E(\T{\fmecd{K}{}{n}}{n}) \leq \E[\T{\kecd{k_1}{n}}{n}].
    $$
\end{conjecture}
Note that if $\max(k_1,k_2) > \frac{n}{2} $, \autoref{lemma:n-k and k} can be used.
