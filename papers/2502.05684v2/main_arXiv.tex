\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage[abbrvbib, preprint]{jmlr2e}
\setcitestyle{square,comma, open={[}, close={]}}
\setcitestyle{numbers}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\hypersetup{hidelinks}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{mathrsfs}
\usepackage{pgfplots}
\usepackage[skip=2pt,font=scriptsize]{caption}
\usepackage{float}
\usepackage{subfig} % Use the subfig package for subfigures
\usepackage{bbm}
\usepackage{bm}

\usepackage{lipsum}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}

% Definitions of handy macros can go here

\newtheorem{prob}{Problem}

\newtheorem{thm}{Theorem}[section]{\bfseries}{\itshape}
\newtheorem{lem}{Lemma}[section]{\bfseries}{\itshape}
\newtheorem{prop}{Proposition}[section]{\bfseries}{\itshape}
\newtheorem{rema}{Remark}[section]{\bfseries}{\itshape}
\newtheorem{coro}{Corollary}[section]{\bfseries}{\itshape}
\newtheorem{defi}{Definition}[section]{\bfseries}{\itshape}
\newtheorem{exam}{Example}[section]{\bfseries}{\itshape}

%Algorithms stuff
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\algnewcommand\algorithmicinput{\textbf{Input:}}
%\algnewcommand\algorithmicoutput{\textbf{Output:}}
%\algnewcommand\Input{\item[\algorithmicinput]}%
%\algnewcommand\Output{\item[\algorithmicoutput]}

% Definitions of handy macros can go here

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\trace}{Trace}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\card}{Card}
\DeclareMathOperator{\dom}{Domain}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\bary}{Bary}

\renewcommand{\epsilon}{{\varepsilon}}

\newcommand{\eps}{{\epsilon}}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
%\nefwcommand{\edit}[1]{\textcolor{blue}{[Version 2: #1]}}
\newcommand{\edit}[1]{\textcolor{blue}{[#1]}}

\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.5\hsize}X}
\newcolumntype{t}{>{\hsize=.3\hsize}X}
\newcommand{\heading}[1]{\multicolumn{1}{c}{#1}}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\newcommand{\ts}[1]{\textcolor{red}{[TS: #1]}}

\jmlrheading{}{}{}{}{}{}{Shizhou Xu}

% Short headings should be running head and authors last names

\ShortHeadings{Machine Unlearning via Information Theoretic Regularization}{Xu, Strohmer}
\firstpageno{1}

\begin{document}

\title{Machine Unlearning via Information Theoretic Regularization}

\author{\name Shizhou Xu \email shzxu@ucdavis.edu \\
       \addr Department of Mathematics\\
       University of California Davis\\
       Davis, CA 95616-5270, USA
       \AND
       \name Thomas Strohmer \email strohmer@math.ucdavis.edu \\
       \addr Department of Mathematics\\
       Center of Data Science and Artificial Intelligence Research\\
       University of California Davis\\
       Davis, CA 95616-5270, USA}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
How can we effectively remove or “unlearn” undesirable information, such as specific features or individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a mathematical framework based on information-theoretic regularization to address both feature and data point unlearning. For feature unlearning, we derive a unified solution that simultaneously optimizes diverse learning objectives, including entropy, conditional entropy, KL-divergence, and the energy of conditional probability. For data point unlearning, we first propose a novel definition that serves as a practical condition for unlearning via retraining, is easy to verify, and aligns with the principles of differential privacy from an inference perspective. Then, we provide provable guarantees for our framework on data point unlearning. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications.
\end{abstract}

\begin{keywords}
Machine Unlearning, Feature Unlearning, Data Privacy, Information-Theoretic Regularization, Optimal Transport Methods
\end{keywords}

\vfill
\newpage

\tableofcontents
\newpage


\section{Introduction}\label{S:1}

As machine learning models become more common in sensitive domains, removing specific features or data points from trained models has become increasingly important. Sensitive information such as gender, ethnicity, or private data can perpetuate biases and lead to unfair results. Simply deleting these attributes in raw data is often insufficient, since their influence may persist through correlated or latent variables. Retraining models without these attributes can  be impractical due to high computational costs. Machine unlearning provides a systematic way to “forget” specific features or data points, ensuring legal compliance and ethical alignment. It is critical to develop provable methods so that models can adapt to evolving demands for privacy and fairness.

\begin{itemize}
    \item \textbf{Feature Unlearning:} In 2014, Amazon built a machine learning based recruitment tool~\cite{dastin2022amazon,christianalignment}. The system favored male candidates because the training data came predominantly from men, penalizing words commonly associated with women. Removing explicit gender references failed, as gender correlated with other features (e.g., all-female colleges). Unable to resolve these biases, Amazon discontinued the tool. This example shows that merely dropping a sensitive feature is not enough if correlated information remains.
    \item \textbf{Data Point Unlearning:} The General Data Protection Regulation (GDPR) \cite{GDPR2016a} introduced the “right to be forgotten,” letting individuals request deletion of their personal data. Although removing raw data is straightforward, it does not eliminate that data’s effect on a trained model’s parameters. The challenge is how to unlearn specific data points while preserving overall model performance, highlighting the need for robust frameworks offering provable compliance and minimal utility loss.
\end{itemize}
% The question we address in this work
In this work, we introduce a framework for machine unlearning within a probabilistic and information-theoretic setting, addressing the following question:

\begin{itemize}
    \item[] \textit{Given a dataset denoted by $X$, an undesirable attribute $Z$, and a target variable $Y$, how can we optimally modify the information in $(X, Z)$ to produce an unlearning outcome $S$ which retains minimal information about $Z$ while preserving as much information in $X$ about $Y$ as possible?}
\end{itemize}

For machine unlearning, we consider $S = \hat{X}$ if the unlearning happens on the data space $\mathcal{X}$ and $S = \hat{Y}$ if the unlearning happens on the learning outcome space $\mathcal{Y}$ (via unlearning on the model parameter space). Note that the feature or label $Z$ may also represent identity information, which blurs the line between feature unlearning and data point unlearning, making the framework applicable to both. Further discussion on applying the framework to data point unlearning can be found in Section \ref{s:data_points_unlearning} below. In the remainder of this work, we focus on unlearning in settings where relational data $(X, Z)$ and target data $Y$ are available.

To address this problem, we connect modern machine unlearning to classic rate-distortion theory  and data compression~\cite{shannon1959coding}. Here, we use data compression in the sense of minimizing \(I(S;Z)\), thereby reducing shared information and effectively removing unwanted details of \(Z\). We
introduce two core concepts inspired by this information-theoretic framework:
\begin{itemize}
%\setlength{\itemsep}{-0.5ex}
%\setlength{\parsep}{-5ex}
\item \textit{Unlearning}, achieved by compressing the relational data $(X, Z)$ into an unlearning outcome $S$ such that the compression rate (quantified by mutual information in this work) between $S$ and the undesirable information $Z$ is minimized.
\item \textit{Utility}, preserved by minimizing the distortion (or maximizing the utility quantification) guided by $Y$, such as the mutual information between $S$ and $Y$ when $S = \hat{Y}$.
\end{itemize}
Following the above information-theoretic perspective, we propose a unified machine unlearning framework applicable to both feature and data point unlearning for a variety of downstream tasks.

\subsection{Application to Feature Unlearning}

For feature unlearning, we define $Z$ as the feature(s) to be unlearned, $X$ as the remaining available data features, and $Y$ as the target variable to estimate using the information in $(X, Z)$. Since feature unlearning often involves removing a feature from a dataset intended for multiple downstream tasks, it is more practical to modify $(X, Z)$ directly in the data space, producing an unlearning outcome $S = \hat{X}$. The objective is to generate $\hat{X}$ that maximizes utility (considering multiple $Y$), while minimizing or compressing the information related to $Z$ to achieve effective unlearning.

Alternatively, if feature unlearning is performed for a specific $Y$ or a specific model, one may set $S = \hat{Y}$. See Algorithm \ref{alg:feature_reg} in Section \ref{S:Algorithm}.

\subsection{Application to Data Point Unlearning}\label{s:data_points_unlearning}

For data point unlearning, the application is less straightforward and requires a more detailed explanation. Here, we discuss the key distinctions between feature unlearning and data point unlearning and clarify how the proposed unlearning method can be applied to data point unlearning by defining a probabilistic data point unlearning guarantee.

First, since data point unlearning operates on the learning outcome space (through unlearning on the parameter space), we define $S = \hat{Y}$ for data point unlearning.

It is crucial to emphasize that data point unlearning typically aims to “forget” the marginal effect of adding/removing a data point on the rest of the dataset, rather than removing all the information associated with that data point. For example, the baseline {\em unlearning via retraining approach} aims to remove the marginal effect (on training) by directly removing the data to unlearn from training. Here, we apply information theoretic regularization and the existing model to directly estimate the marginal effect on learning outcome and thereby penalize it during training.

A straightforward application of an information theoretic framework, such as via the information bottleneck, might minimize $I(X; \hat{Y})$, the mutual information between $X$ and $\hat{Y}$ (see Appedix \ref{a:utility_motivation} for more details), while maximizing $I(X \setminus \{x_u\}; \hat{Y})$ \cite{han2024unlearning}, where $x_u$ represents the data belonging to the individual requiring unlearning. However, enforcing perfect unlearning by setting $I(X; \hat{Y}) = 0$ would leave almost no utility in $\hat{Y}$. This approach unnecessarily targets removing all data information rather than focusing solely on unlearning the marginal effect of $x_u$, leading to an excessive loss of information and failing to balance unlearning with utility.

It is thus more reasonable to pursue a probabilistic, information-theoretic approach which constructs relational data to indicate whether a data point is included in the training step: I.e., we consider $(X_{train}, Z)$, where $X_{train}|_{Z = 1} := X_1 := \{x_i\}_{i = 1}^N$ with $x_i \sim \text{Uniform}(X \setminus \{x_u\})$ and $X_{train}|_{Z = 0} := X_0 := \{x_j\}_{j = N+1}^{2N-1} \cup \{x_u\}$ with $x_j \sim \text{Uniform}(X \setminus \{x_u\})$. In this formulation, perfect unlearning of the marginal effect of $x_u$ can be achieved by finding a measurable function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that minimizes $I(Z; \hat{Y}_{train})$, where $\hat{Y}_{train} = f(X_{train})$, without directly conflicting with utility objectives $\mathcal{U}(Y; \hat{Y})$, such as $I(Y; \hat{Y})$ or $||Y - \hat{Y}||_{\ell^2}$.

It is important to highlight that the data point unlearning we propose is fundamentally distinct from existing approaches (exact or approximate unlearning, see more details in Section \ref{s:related_work}) that aim to achieve the same result as a retrained model on a dataset excluding the individual record. Instead, our approach will provide a probabilistic guarantee that aligns with the principles of differential privacy from an inference perspective. Specifically, we formalize the data point unlearning guarantee as follows:
\begin{defi}[$\epsilon$-Differential Unlearning]\label{d:epsilon_DU}
    Given a dataset $X$ and an individual record $\{x_i\}$ that requires unlearning, an unlearning output $f_{\theta(X,Y,Z)}$ satisfies {\em $\epsilon$-differential unlearning} if
    \begin{equation}
        \sup_{D \in \mathcal{B}_{\hat{Y}}} \Big| \log\left(\frac{\mathbb{P}(\{X_{train} = X_0\} \mid \hat{Y}_{train} \in D)}{\mathbb{P}(\{X_{train} = X_1\} \mid \hat{Y}_{train} \in D)}\right) \Big| \le \epsilon.
    \end{equation}
\end{defi}
That is, given any observation of the unlearning output $\hat{Y}_{train} = f_{\theta}(X_{train})$ and the knowledge that the training process uses either the original dataset $X$ or the dataset excluding $x_u$, it is impossible (up to $\epsilon$ inference capability) to determine whether the individual record $x_u$ was included in the training. We briefly summarize the advantages of the proposed definition:
\begin{itemize}
    \item \textbf{Necessary Condition for Exact Unlearning}: Under mild assumptions, one can show that any unlearned model that is assumed to estimate an exact unlearning outcome (via retraining) can fail the $\epsilon$-differential unlearning definition only if it is non-smooth (in particular, violates a Lipschitz condition) and hence suffers from low generalizability for unseen data (See Lemma \ref{l:DU_as_condition_for_good_model} and its discussion below). This establishes $\epsilon$-differential unlearning as a foundational requirement for exact unlearning.
    \item \textbf{Ease of Verification}: The definition relies solely on the unlearned model and the original dataset $X$, without requiring access to the exact retrained outcome. As a result, $\epsilon$-differential unlearning can serve as an efficient preliminary test to assess unlearning outcomes.
\end{itemize}
This unlearning concept is inspired by differential privacy \cite{dwork2006differential}. It is instructive to compare $\epsilon$-differential unlearning to differential privacy to highlight their differences:

\begin{itemize}
    \item \textit{$\epsilon$-Differential Unlearning (Remediation via Inference Guarantee)}: Requires that the unlearning outcome (via data compression) provides no inference capability about whether the training process included the individual record requiring unlearning. Since unlearning is a {\em remedial} process, it is more reasonable to consider it from an inference perspective: $\sup_{D \in \mathcal{B}_{\mathcal{Y}}} | \log \left(\frac{\mathbb{P}(\{X_{train} = X_1\} \mid \hat{Y}_{train} \in D)}{\mathbb{P}(\{X_{train} = X_0\} \mid \hat{Y}_{train} \in D)}\right) | \leq \epsilon$. Here, $(\mathcal{Y}, \mathcal{B}_{\mathcal{Y}})$ is a measurable space for the learning outcome, where $\mathcal{B}_{\mathcal{Y}}$ denotes the sigma-algebra containing all possible events $D$ that can occur in the outcome space $\mathcal{Y}$.

    \item \textit{$\epsilon$-Differential Privacy (Prevention via Generative Guarantee)}: Requires that the learning outcome (via a randomized algorithm) does not change significantly if an unknown individual's data is added to or removed from the training dataset. Since privacy focuses on {\em prevention}, it is more reasonable to provide guarantees from a generative perspective: $\sup_{D \in \mathcal{B}_{\mathcal{Y}}}  | \log\left(\frac{\mathbb{P}(\hat{Y} \in D \mid X)}{\mathbb{P}(\hat{Y} \in D \mid X' )}\right) | \leq \epsilon,$ where $X'$ is the dataset that differs from $X$ by one (unknown) data point.
\end{itemize}

It is worth noting that differential unlearning can also be framed as a straightforward extension of the original differential privacy definition from a generative perspective: $| \log\left(\frac{\mathbb{P}(\hat{Y} \mid X)}{\mathbb{P}(\hat{Y} \mid X \setminus \{x_u\})}\right) | < \epsilon$ implies that the unlearning outcome $\hat{Y}$ should remain largely unchanged if $X$ is replaced with $X \setminus \{x_i\}$ during training or vice versa. While this perspective offers valuable insights and practical tools for achieving unlearning, we argue that the generative approach is better understood as a {\em means} to unlearning rather than a {\em definition}, because a definition of unlearning should capture the remedial nature inherent to machine unlearning.

We will demonstrate in Section~\ref{S:2} that the proposed framework naturally produces unlearning outputs that satisfy the $\epsilon$-differential unlearning guarantee when applied to data point unlearning tasks.

\subsection{Related Work}\label{s:related_work}

The existing body of work on unlearning can be separated into feature unlearning and machine unlearning:

\subsubsection{Machine Unlearning}

Machine Unlearning focuses on removing data points' influence during training on the model and thereby the learning outcome, mainly to comply with regulations such as GDPR’s “right to be forgotten.” The current study can be separated into exact unlearning and approximate unlearning:

\textit{Exact unlearning} \cite{bourtoule2021machine, brophy2021machine, yan2022arcane, graves2021amnesiac, schelter2021hedgecut, ginart2019making, chen2022graph, chen2022recommendation} approaches ensure the removal of data influence from the model by separating the model into sub-models (via subsets of training data in SISA training, parameters, or training steps in amnesiac unlearning) which allow fast retraining by only retraining the affected sub-models or training steps. While effective, these methods are computationally expensive, particularly in scenarios with multiple data removal requests. 

\textit{Approximate unlearning} methods try to keep the probabilistic similarity quantification between the unlearning outcome and the retraining outcome (also known as data point influence quantification). The main current approaches include: (1) using the influence function \cite{weisberg1982residuals} to estimate the influence of the data point and thus remove its effect on the model \cite{guo2020certified, sekhari2021remember, suriyakumar2022algorithms, mehta2022deep, wu2022puma, tanno2022repairing, warnecke2021machine}; (2) using the scrubbing function to enforce similarity between the network weights and the retraining weights \cite{golatkar2020eternal, golatkar2020forgetting}.

While these methods are effective in approximating the retrained model, each has practical limitation(s), such as significant performance degradation, high computational cost, limited compatibility with learning objectives, or restricted evaluation capability on simple datasets \cite{nguyen2022survey, wang2024machine}. More importantly, due to the aim to estimate the unknown (due to the complexity of algorithms, objective functions, and data influence) retraining outcome, the provable unlearning guarantees rely heavily on impractical assumptions such as convexity of the objective function and Lipschitz condition on Hessian matrices. \cite{xu2024machine}

In comparison, the proposed method can be considered as using mutual information to quantify the marginal effect of adding or removing the data point to unlearn in training, thereby more effectively removing the influence of the data to unlearn by diminishing the marginal effect. Furthermore, since the proposed unlearning definition (Definition \ref{d:epsilon_DU}) does not explicitly depend on the unknown retraining outcome, its provable guarantees avoid strong assumptions and become more practical to achieve. Finally, we establish a connection to retraining by demonstrating that Definition \ref{d:epsilon_DU} serves as a necessary condition for a “good” retrained model (Lemma \ref{l:DU_as_condition_for_good_model} and the discussion below).

Recent unlearning methods have leveraged the information bottleneck \cite{li2021unlearning, han2024unlearning} to avoid utility degradation by addressing both the utility and influence function. While at first glance these information bottleneck-based methods may seem similar to our proposed method, there is a fundamental difference. In particular, these information bottleneck methods aim to unlearn the information of that data point itself, such as minimizing the mutual information between the remaining target variable and the model parameter \cite{han2024unlearning} or the mutual information between the data to unlearn and the unlearning latent representations \cite{li2021unlearning}. In comparison, our regularization tries to help unlearn (or learn) the marginal effect of adding (removing) the data point.

\subsubsection{Feature Unlearning and Statistical Parity in Machine Learning Fairness}

Feature Unlearning and Statistical Parity in Machine Learning Fairness aim to remove the influence of a feature in the learning outcome~\cite{warnecke2021machine, han2024unlearning}. It is closely related to machine learning fairness when fairness is defined as statistical parity \cite{dwork2012fairness}. In particular, our framework when applied to feature unlearning is closely related to the Wasserstein barycenter characterization of both the optimal statistical parity learning outcome \cite{chzhen2020fair, gouic2020projection} and the optimally fair data representation \cite{xu2023fair}.

%\textit{Differential Privacy}:

\subsection{Our Contributions}

Our contributions in this paper are as follows:
\begin{itemize}
\item We propose \textit{$\epsilon$-Differential Unlearning}, a novel probabilistic guarantee for data point(s) unlearning that reflects the remedial nature of machine unlearning, inspired by the protection guarantees of differential privacy.
\item We propose a unified machine unlearning framework, inspired by {\em rate-distortion theory} in {\em data compression}, for both feature and data point unlearning. The framework is compatible with diverse target information and utility quantifications, and hence with a range of downstream tasks for the unlearned output.
\item For feature unlearning, we demonstrate the existence of a unified analytic solution across multiple utility objectives and arbitrary target variables.
\item For data point unlearning, we provide a provable $\epsilon$-differential unlearning guarantee using compression rate and, thereby, provide a sufficient condition for the proposed unlearning framework to generate an $\epsilon$-differential unlearning guaranteed result. 
\end{itemize}

\iffalse
The rest of the paper is organized as follows: Section \ref{S:2} introduces the proposed framework and its motivation from data compression. Section \ref{S:3} presents the analytical solution and theoretical guarantees. Section \ref{S:4} demonstrates the framework’s practical effectiveness in unlearning.
\fi

\subsection{Some Tools and Notation}


Before we proceed, we introduce some tools used later and fix some notation.
Let \(\mathcal{P}(\mathcal{X})\) be the set of probability measures on a metric space \((\mathcal{X},d)\). The \emph{p-Wasserstein distance} between \(\mu, \nu \in \mathcal{P}(\mathcal{X})\) is defined as 
$$\mathcal{W}_{d,p}(\mu,\nu) = \Big( \inf_{\lambda \in \Pi(\mu,\nu)} 
    \int_{\mathcal{X} \times \mathcal{X}} d(x_1,x_2)^p \,d\lambda(x_1,x_2) \Big)^{\frac{1}{p}},$$
where \(\Pi(\mu,\nu)\) denotes the set of all couplings of \(\mu\) and \(\nu\). Let \(\mathcal{P}_{2,ac}(\mathcal{X})\) be the subset of measures with finite second moments that are absolutely continuous w.r.t.\ the Lebesgue measure. For random variables \(X_1, X_2\), we write \(\mathcal{W}_{d,p}(X_1,X_2) := \mathcal{W}_{d,p}(\mathcal{L}(X_1),\mathcal{L}(X_2))\), where \(\mathcal{L}(X)\) denotes the law of \(X\). For simplicity, we let $\mathcal{W}_{d} := \mathcal{W}_{d,1}$. Also, $\mathcal{W}_2 := \mathcal{W}_{l^2,2}$ when $d(x,x') := ||x-x'||_{l^2}$. Given a family of measures \(\{\mu_z\}_{z\in\mathcal{Z}} \subset \mathcal{P}_{2,ac}(\mathcal{X})\) with weights \(\lambda\), their \emph{Wasserstein barycenter} \(\bar{\mu}\) is the minimizer of $\int_{\mathcal{Z}}\mathcal{W}_2^2(\mu_z,\mu)\,d\lambda(z)$.
We define \(\bar{X}\) as the random variable distributed according to \(\bar{\mu}\) and satisfy \(\bar{X}_z = T_z(X_z)\), where \(T_z\) is the optimal transport map from \(\mathcal{L}(X_z)\) to \(\bar{\mu}\). More details can be found in Appendix \ref{a:OT_intro}.



\section{A Unified Unlearning Framework}\label{S:2}

We first introduce our machine unlearning framework, followed by a concrete example illustrating the data compression motivation behind this approach and a brief review of key technical tools underlying the definitions of utility, unlearning, and admissibility. Next, we extend the motivational example from feature unlearning (Section \ref{S:2.1}) to the data point unlearning (Section \ref{S:2.2}) setting. Finally, we generalize the framework to accommodate different choices of target information and utility quantification for both feature and data point unlearning.

\begin{defi}[Optimal Feature Unlearning]\label{d:optimal_unlearning}
Given relational data $(X, Z)$ and a target variable $Y$, optimal feature unlearning is defined as the solution, if it exists, to:
\begin{equation}
\label{unlearning}
\sup_{f: \mathcal{X} \times \mathcal{Z} \rightarrow \mathcal{X}} \{\mathcal{U}(Y; \hat{X}): \hat{X} \perp Z\},
\end{equation}
where $\hat{X} := f(X, Z)$ is the unlearning outcome, and $\mathcal{U}: \mathcal{P}(\mathcal{Y}) \times \mathcal{P}(\mathcal{X}) \rightarrow \mathbb{R}$ quantifies the utility retained in $\hat{X}$ relevant to $Y$. The optimization is over all measurable functions $f$ that remove the information of $Z$ while preserving the information of $Y$ from $X$.
\end{defi}

While setting $S = \hat{X}$ instead of $\hat{Y}$ may seem unconventional, this choice ensures compatibility with multiple target variables $Y$, making it practical for feature unlearning. Moreover, Theorem \ref{th:optimal_solution} shows that under a wide range of utility functions, the optimal solution is independent of $Y$.

To relax the strict independence constraint in~\eqref{unlearning}, we introduce a soft probabilistic constraint using mutual information as a compression rate:
\begin{equation}
\label{unlearning:relaxed}
\sup_{f: \mathcal{X} \times \mathcal{Z} \rightarrow \mathcal{X}} \mathcal{U}(Y; \hat{X}) - \gamma I(\hat{X} ; Z),
\end{equation}
for some $\gamma \geq 0$, allowing a trade-off between utility preservation and unlearning effectiveness.

For data point unlearning, we follow the setting developed in Section \ref{s:data_points_unlearning}:
\begin{defi}[Optimal Differential Unlearning]\label{d:optimal_unlearning}
Given data $X$, a data point $\{x_u\}$ to unlearn, and a target variable $Y$, the optimal differential unlearning is defined as the optimal solution, if it exists, to the following problem:
\begin{equation}
\label{eq:optimal_point_unlearning}
\sup_{f: \mathcal{X} \rightarrow \mathcal{Y}} \{\mathcal{U}(Y; \hat{Y}): \hat{Y}_{train} \perp Z\},
\end{equation}
where $\hat{Y}_{train} := f(X_{train})$ is the unlearning outcome. The relational data $(X_{train}, Z)$ is defined by $X_{train}|_{Z = 0} = X_0$ and $X_{train}|_{Z = 1} = X_1$. The optimization is over all measurable functions  $f: \mathcal{X} \rightarrow \mathcal{Y}$ of which the goal is to leverage the relational data $(X_{train}, Z)$ to remove the information of the training data set indicator $Z$ but retain information of $Y$ based on $X$.
\end{defi}

Similarly to above, we can also relax equation~\eqref{eq:optimal_point_unlearning} to the following version: $$\sup_{f: \mathcal{X} \rightarrow \mathcal{Y}} \mathcal{U}(Y; \hat{Y}) - \gamma I(\hat{Y}_{train} ; Z).$$

\begin{rema}[Alternative Regularization to Mutual Information]\label{rema:regularization_choice}
The regularization term using mutual information in our framework serves to relax the strict independence constraint through a soft penalization. While mutual information is a natural choice due to its strong theoretical foundations in information theory and its connection to data compression that motivates the proposed framework (See Section \ref{S:2.1} below), it is by no means the only option. More generally, any quantification of statistical dependence between two random variables can serve as an alternative regularizer. 

For instance, in the binary $Z$ case, one could use the Kullback-Leibler (KL) divergence between $\hat{Y}_0$ and $\hat{Y}_1$, i.e., $D_{KL}(\hat{Y}_0||\hat{Y}_1)$. Alternatively, if the objective is to account for both distributional differences and geometric distance in the learning outcome space, the Wasserstein distance, $\mathcal{W}_{d,p}(\hat{Y}_0,\hat{Y}_1)$, provides a viable alternative. A systematic investigation into the optimal choice of regularizer for specific utility objectives is an interesting direction for future research.
\end{rema}

\subsection{Motivation: Feature Unlearning \& Data Compression}\label{S:2.1}

To illustrate our framework’s link to data compression, consider the special case where the target variable is the original data (\(Y = X\)) and utility is quantified by mutual information (\(\mathcal{U}(X;\hat{X}) := I(X;\hat{X})\)). We then solve: $$\sup_{\hat{X} = f(X,Z)} \bigl\{ I(X;\hat{X}) : \hat{X} \perp Z \bigr\}.$$ Here, \(I(X;\hat{X})\) measures unlearning quality: larger values indicate \(\hat{X}\) retains more of \(X\). Meanwhile, \(\hat{X} \perp Z\) enforces complete removal of \(Z\) by making \(I(Z;\hat{X}) = 0\). Next, we discuss how utility, unlearning, and admissibility fit into this context.

%In this work, we use ``compressing \((S,Z)\)'' to denote minimizing \(I(S;Z)\), thereby reducing shared information and effectively removing unwanted details of \(Z\).\ts{We probably need to mention this much earlier, since we already repeatedly talk about compression before, without explaining there what we mean.}
%\ts{Perhaps this whole subsection should go right before Subsection 1.1.}\edit{But I am afraid it will make the introduction section too long.}

\begin{itemize}

    \item \textbf{Compression Rate via Mutual Information}: Mutual information measures shared information between two variables and is fundamental in data compression~\cite{CT05}. For dataset $X$ encoded as $\hat{X}$, the information contained in $X$ is quantified by $H(X)$ where $H(X)$ is the entropy of $X$. The conditional entropy $H(X|\hat{X})$ quantifies the remaining uncertainty in $X$ given $\hat{X}$, with higher values indicating less explanatory power of $\hat{X}$. The compression ratio, $2^{H(X) - H(X|\hat{X})} = 2^{I(X;\hat{X})}$, reflects how well $\hat{X}$ retains information about $X$. Higher mutual information implies $\hat{X}$ generates a finer partition on $X$, while lower values indicate better compression. See Appendix \ref{a:utility_motivation} for details.
    
    \item \textbf{Feature Unlearning}: To unlearn a feature \(Z\) from data \(X\), we minimize \(I(Z;\hat{X})\), measuring how much information \(\hat{X}\) retains about \(Z\). Perfect unlearning occurs if \(I(Z;\hat{X}) = 0\), i.e., \(\hat{X} \perp Z\). In practice, partial unlearning is more realistic, balancing the rate-distortion-like trade-off between removing \(Z\) and preserving utility. By bounding \(I(Z;\hat{X}) < \epsilon\) (or equivalently introducing a Lagrange multiplier \(\gamma\)), we obtain \(\sup_{\hat{X} = f(X,Z)}\{\,I(X;\hat{X}) - \gamma\,I(Z;\hat{X})\}\), where \(\gamma>0\) governs the trade-off between minimizing unwanted information and retaining the utility in \(\hat{X}\).
    
    \item \textbf{Admissibility}: Since we unlearn the information of $Z$ by compressing relational data $(X,Z)$, it is natural to require the resulting compressed data $\hat{X}$ to be measurable with respect to $(X,Z)$. In particular, for every event or observation $A$ of the compressed $\hat{X}$, the information represented by the observation $\hat{X}^{-1}(A) := \{\omega: \hat{X}(\omega) \in A\}$ comes from the knowledge of $f^{-1}(A) := \{(x,z): f(x,z) \in A\}$ based on $(X,Z)$:
\begin{align}
\{\omega: \hat{X}(\omega) \in A\} = \hat{X}^{-1}(A) 
= (X,Z)^{-1}(f^{-1}(A)) \nonumber = \{\omega: (X(\omega),Z(\omega)) \in f^{-1}(A)\}.
\label{eq:long_equation}
\end{align}
Here, $\omega \in \Omega$ is the smallest unit of information we can have from the measure space $(\Omega, \mathcal{F},\mathbb{P})$. From a probability-theoretical perspective, since $\hat{X}$ is a compression of $(X,Z)$, it generates a coarser partition (or, more technically, sigma-algebra) than the original information $(X,Z)$ and we say $\hat{X}$ is measurable with respect to $(X,Z)$, denoted by $\sigma(\hat{X}) \subset \sigma((X,Z)).$ This is equivalent to the existence of a $\mathcal{B}_\mathcal{X} \otimes \mathcal{B}_\mathcal{Z}/\mathcal{B}_\mathcal{X}$-measurable map, denoted by $f$, such that $\hat{X} = f(X,Z)$. That is, our admissibility is equivalent to the assumption that the data compression process does not create information or randomness by itself. Therefore, we define the admissible unlearning outcome in our framework as follows: $\mathcal{A}(X,Z) := \{\hat{X} = f(X,Z): f \text{ is }
 \mathcal{B}_\mathcal{X} \otimes \mathcal{B}_\mathcal{Z}/\mathcal{B}_\mathcal{X} \text{-measurable} \}$ and we use $\hat{X} = f(X,Z)$ and $\hat{X} \in \mathcal{A}(X,Z)$ interchangeably.
\end{itemize}





\subsection{Motivation: Data Point Unlearning \& Data Compression}\label{S:2.2}

We now connect the feature unlearning constraint using mutual information to $\epsilon$-differential unlearning (Definition~\ref{d:epsilon_DU}) and demonstrate that the proposed feature unlearning framework with a mutual information constraint can directly ensure $\epsilon$-differential unlearning for data points.

In particular, we apply the following framework to estimate the optimal differential unlearning solution:
\begin{equation}\label{eq:estimate_point_unlearning}
    \sup_{\hat{Y} = f(X)} \{I(Y;\hat{Y}) - \gamma I(\hat{Y}_{train};Z) \},
\end{equation}
and then use the following result to provide a provable unlearning guarantee:
\begin{lem}[Mutual Information Bound on Unlearning Inference Log Ratio]\label{l:epsilon_DU_bound_mutual_info}
    Let the proposed unlearning framework (equation \eqref{eq:estimate_point_unlearning})  compress $(\hat{Y}_{train}, Z)$. Then, for any given $\epsilon > 0$, we have
    \begin{equation*}
    \begin{aligned}
    \mathbb{P}\Bigl(
    &\Bigl|\log\!\Bigl(\frac{\mathbb{P}(\{X_{train} = X_0\} \mid \hat{Y}_{train})}%
                         {\mathbb{P}(\{X_{train} = X_1\} \mid \hat{Y}_{train})}\Bigr)\Bigr| 
    \;\le\; \log\!\Bigl(\frac{1+\epsilon}{1-\epsilon}\Bigr)
    \Bigr) \ge \;
    1 \;-\; \frac{1}{\epsilon}\,\biggl(\sqrt{\frac{1}{2}\,I(Z;\hat{Y}_{train})}\biggr).
    \end{aligned}
    \end{equation*}
    
\end{lem}
See proof in Appendix \ref{a:proof_DU_bound_mutual_info}. That is, if the proposed unlearning framework is applied to penalize the mutual information term $I(Z; \hat{Y}_{train})$ such that it remains relatively small compared to $\frac{\exp(\epsilon) - 1}{\exp(\epsilon) + 1}$, then the probability of observing an event in $\hat{Y}_{train}$ that grants more than $\frac{1+\epsilon}{1-\epsilon}$ inference capability becomes extremely low. By the construction of $\hat{Y}_{train}$, it is clear that even if we observe the learning outcomes from both the data set including data point to unlearn and the one excluding the point, one can still not tell whether or not the model is trained using $X_0 \sim X$ or $X_1 \sim X\setminus \{x_u\}$. Therefore, to ensure $\epsilon$-differential unlearning with high probability, it suffices to adjust the regularization weight $\gamma$ such that $\sqrt{I(Z;\hat{Y}_{train})}$ is small relative to $\frac{\exp(\epsilon) - 1}{\exp(\epsilon) + 1}$.

Lastly, we connect the proposed $\epsilon$-differential unlearning definition to the unlearning via retraining by showing that it serves as a practical necessary condition for a ``good'' retraining model. In particular, one can show that if a model, denoted by $f: \mathcal{X} \rightarrow \mathcal{Y}$, can minimize training loss on the training data, then either the unlearned model satisfies the $\epsilon$-differential unlearning definition or it suffers from low generalizability by violating a Lipschitz condition as described below.

\begin{lem}[$\epsilon$-DU as a Condition for ``Good" Retrained Model]\label{l:DU_as_condition_for_good_model}
    Assume $f$ is a retrained model with $f(X_0)$ and $f(X_1)$ both absolutely continuous, then either $f$ violates $\epsilon$-DU: $\sup_{y} |\log(\frac{\mathcal{L}(f(X_1))(y)}{\mathcal{L}(f(X_0))(y)})| \leq \epsilon$ or $f$ violates the $L$-Lipschitz condition for any $L \leq L^*$ where
    \begin{equation}
    \label{eq:Lstar}
    \begin{aligned}
    L^*(\epsilon) 
    := \frac{\delta \,\bigl|f(X_1)(y^*) - f(X_0)(y^*)\bigr|}
          {\,\mathcal{W}_{d_\mathcal{X}}(X_1,X_0)} 
    = \frac{\delta\,\bigl(\exp(\epsilon) - 1\bigr)\,
       \min\{f(X_0)(y^*),\,f(X_1)(y^*)\}}
          {\,\mathcal{W}_{d_\mathcal{X}}(X_1,X_0)}.
    \end{aligned}
    \end{equation}
    Here, $y^*$ is the point where the $\epsilon$-differential unlearning definition is violated, $\mathcal{W}_{d_\mathcal{X}}(X_0,X_1)$ is the Wasserstein distance between $X_0$ and $X_1$ on the metric space $(\mathcal{X},d_{\mathcal{X}})$, and $\delta$ is the radius around $y^*$ where $\sign(\log (\frac{\mathcal{L}(f(X_0))(y)}{\mathcal{L}(f(X_1))(y)})) = \sign(\log (\frac{\mathcal{L}(f(X_0))(y^*)}{\mathcal{L}(f(X_1))(y^*)}))$.
\end{lem}

See proof in Appendix \ref{a:proof_DU_as_good}. The above result implies that, if we assume that there is a significant marginal effect of adding/removing $(x_u,y_u)$ relative to the original dataset (otherwise there is no significant need for unlearning) and the retrained model $f$ achieves a low training loss on the remaining dataset, then one cannot simultaneously have all of the following: (1)~truthful revealing of $(x_u,y_u)$: $f(x_u) = y_u$; (2)~$\epsilon$-DU, (3)~good generalizability of $f$ with low Lipschitz constant. That is, if we require (2), then a retrained model with good generalizability cannot reveal $(x_u,y_u)$.

\begin{prop}[$\epsilon$-DU Reduces Utility on the Data to Unlearn]\label{prop:DU_reduce_utility}
    Assume $f(X_1) = Y_1$ and $\sup_{y} |\log(\frac{\mathcal{L}(f(X_0))(y)}{\mathcal{L}(f(X_1))(y)})| \leq \epsilon$ such that $L^*(\epsilon) > 1$, but $Y_0$ satisfies $\delta|Y_0(y) - Y_1(y)| > \mathcal{W}_{d_\mathcal{X}}(X_0,X_1)$ for some $y \in \mathcal{Y}$, then $f(X_0) \neq Y_0$.
\end{prop}

\begin{proof}
    This is a direct corollary of Lemma \ref{l:DU_as_condition_for_good_model}.
\end{proof}

In other words, if a retrained model achieves good training performance on the remaining data set ($f(X_1) = Y_1$) and satisfies the proposed $\epsilon$-differential unlearning for small enough $\epsilon$ ($L^*(\epsilon) > 1$), but there is a significant marginal training signal ($\delta |Y_0(y) - Y_1(y)| > 2\mathcal{W}_{d_\mathcal{X}}(X_0,X_1)$) resulting from adding/removing the data to unlearn (to the existing data set), then $f(X_0)$ cannot reveal $Y_0$ truthfully. That further implies $f(x_u)$ cannot reveal $y_u$ truthfully by the construction of $(X_0,Y_0)$. 

Finally, by enforcing $\epsilon$-DU by mutual information regularization while preserving utility on the remaining data, the proposed framework achieves unlearning by only diminishing the marginal utility at the data point to be unlearned. But we further notice that the utility via our approach can decrease further than the retrained (from scratch) model in practice.

\subsection{Versatile Utility}\label{s:utility}
In our feature unlearning setting, often the target variable \(Y\) lives in a different space from \(X\), and mutual information may not suffice to capture how \(\hat{X}\) relates to \(Y\). Thus, we consider
\begin{equation}\label{eq:general_formulation}
\sup_{\hat{X} = f(X,Z)} \{\mathcal{U}(Y;\hat{X}): \hat{X} \perp Z\},
\end{equation}
where \(\mathcal{U}(Y;\hat{X})\) is a user-defined utility. Below are some widely used objectives, each with a short motivation. For more detailed explanation of each utility below, see Appendix \ref{a:utility_appendix} and Appendix \ref{a:formulation_constraint_optimization}.

\begin{itemize}
\item \textbf{Entropy Maximization:} $\mathcal{U}(Y;\hat{X}) \;=\; H(\hat{X})$ preserves the total uncertainty in \(\hat{X}\).

\item \textbf{Mutual Information Maximization:} $\mathcal{U}(Y;\hat{X}) \;=\; I(Y;\hat{X})$ maximizes how much \(\hat{X}\) reveals about \(Y\).

\item \textbf{KL-Divergence Maximization:} $\mathcal{U}(Y;\hat{X}) 
    \;=\; D_{KL}\bigl(\mathbb{P}(Y \mid \hat{X}) \,\|\, \mathbb{P}(Y)\bigr)$ makes \(\mathbb{P}(Y\mid \hat{X})\) more deterministic relative to \(\mathbb{P}(Y)\).

\item \textbf{Conditional Probability Energy Maximization:}
  \begin{equation}
    \mathcal{U}(Y;\hat{X}) 
    \;=\; 
      \begin{cases}
       \sqrt{\mathbb{E}[\,\mathbb{P}(Y\in A\mid \hat{X})^2\,]}, & \text{classification}, \\
       -\|\,Y - \mathbb{E}(Y\mid \hat{X})\|_2,        & \text{regression},
      \end{cases}
  \end{equation}
improves classification boundaries or reduces mean squared error.
\end{itemize}

We note that the proposed (feature and data point) unlearning framework supports a wide range of utility and objective functions beyond the four listed here. The four listed objectives are selected for their shared analytical optimal solution. As shown in Theorem \ref{th:optimal_solution}, this optimal solution is of particular interest for its independence from the choice of $Y$, even though three of the objectives explicitly involve $Y$.

\section{Theoretical Guarantees}\label{S:3}

In this section, we provide theoretical guarantees for both feature unlearning and data point unlearning. Specifically, we leverage optimal transport to derive a unified analytic solution for feature unlearning under all the listed objectives. For data point unlearning, we establish a provable guarantee by ensuring that the unlearning outcome satisfies $\epsilon$-differential unlearning through the compression rate quantified by mutual information.

\if 0
\subsection{Wasserstein Distance and Wasserstein Barycenter}

Before presenting our main results, we need some preparation.
Let \(\mathcal{P}(\mathcal{X})\) be the set of probability measures on a metric space \((\mathcal{X},d_\mathcal{X})\). The \emph{p-Wasserstein distance} between \(\mu, \nu \in \mathcal{P}(\mathcal{X})\) is defined as 
$$\mathcal{W}_{d,p}(\mu,\nu) = \Big( \inf_{\lambda \in \Pi(\mu,\nu)} 
    \int_{\mathcal{X} \times \mathcal{X}} d(x_1,x_2)^p \,d\lambda(x_1,x_2) \Big)^{\frac{1}{p}},$$
where \(\Pi(\mu,\nu)\) denotes the set of all couplings of \(\mu\) and \(\nu\). Let \(\mathcal{P}_{2,ac}(\mathcal{X})\) be the subset of measures with finite second moments that are absolutely continuous w.r.t.\ the Lebesgue measure. For random variables \(X_1, X_2\), we write \(\mathcal{W}_{d,p}(X_1,X_2) := \mathcal{W}_{d,p}(\mathcal{L}(X_1),\mathcal{L}(X_2))\), where \(\mathcal{L}(X)\) denotes the law of \(X\). For simplicity, we let $\mathcal{W}_{d} := \mathcal{W}_{d,1}$. Also, $\mathcal{W}_2 := \mathcal{W}_{l^2,2}$ when $d(x,x') := ||x-x'||_{l^2}$. Given a family of measures \(\{\mu_z\}_{z\in\mathcal{Z}} \subset \mathcal{P}_{2,ac}(\mathcal{X})\) with weights \(\lambda\), their \emph{Wasserstein barycenter} \(\bar{\mu}\) is the minimizer of $\int_{\mathcal{Z}}\mathcal{W}_2^2(\mu_z,\mu)\,d\lambda(z)$.
We define \(\bar{X}\) as the random variable distributed according to \(\bar{\mu}\) and express it as \(\bar{X} = T_z(X_z)\), where \(T_z\) is the optimal transport map from \(\mathcal{L}(X_z)\) to \(\bar{\mu}\). More details can be found in Appendix \ref{a:OT_intro}.
\fi

\subsection{Feature Unlearning: Unified Optimal Solution for Multiple Objectives}

The utility objectives outlined earlier are commonly used across fields such as biology, physics, and AI. See Appendix \ref{a:utility_appendix} for more details. Despite their diverse forms, these objectives can be unified within a single framework by focusing on the sigma-algebra generated by the unlearning outcome $\hat{X}$. This approach allows for a cohesive solution to seemingly distinct optimization problems, demonstrating the versatility and practicality of the proposed framework.

We start with the following result, which establishes that the Wasserstein-2 barycenter generates the finest sigma-algebra among all admissible outcomes:

\begin{lem}[Wasserstein-2 Barycenter Generates the Finest Sigma-Algebra]\label{l:Finest_Sigma_Algebra}
Let $\{X_z\}_{z \in \mathcal{Z}} \subset \mathcal{P}_{2,ac}(\mathcal{X})$. We have $\sigma(\hat{X}) \subset \sigma(\Bar{X})$ for all $\hat{X} = f(X, Z)$.
\end{lem}

The proof is in the Appendix \ref{a:proof_finest_sigma_algebra}. The above result demonstrates one feasible optimal solution. Under the assumption of absolute continuity of marginals, the barycenters of convex costs also satisfy the invertibility of transport maps, ensuring optimality.

Now, the importance of the above result lies in the monotonicity of the objective functions listed earlier w.r.t.\ the sigma-algebra generated by random variables. That is, the fineness of the sigma-algebra is equivalent to the amount of information contained by the random variable in probability theory. See the following remark for a more detailed explanation. 

\begin{rema}[Sigma-Algebra and Information]\label{r:Sigma-Algebra and Information}
In probability theory, a probability space is often represented as a triple $(\Omega, \Sigma, \mathbb{P})$, where $\Omega$ is the sample space, $\Sigma$ is the sigma-algebra (a collection of subsets of $\Omega$), and $\mathbb{P}: \Sigma \rightarrow [0,1]$ is a probability measure that assigns probabilities to each event in $\Sigma$.

The same sample space can be associated with different sigma-algebras, resulting in different probability spaces. We say that a sigma-algebra $\Sigma_1$ is finer than $\Sigma_2$, denoted $\Sigma_2 \subset \Sigma_1$, if $\Sigma_1$ contains all events in $\Sigma_2$. Conversely, we say $\Sigma_1$ is coarser than $\Sigma_2$ if $\Sigma_1$ contains fewer events than $\Sigma_2$.

A random variable or random vector $X$ is a measurable function from the probability space to $\mathbb{R}^d$ (or $\mathbb{C}^d$), $X: \Omega \rightarrow \mathbb{R}^d$. The sigma-algebra generated by $X$, denoted by $\sigma(X)$, comprises all possible events that could be defined based on the image of $X$ in $\mathbb{R}^d$ (or $\mathbb{C}^d$). Thus, if $X$ generates a finer sigma-algebra than another variable $X’$, denoted $\sigma(X’) \subset \sigma(X)$, then $X$ contains more events and, therefore, more information than $X’$.

In modern probability theory, sigma-algebras facilitate the construction of probability measures, especially in countably or uncountably infinite spaces (as the concept is trivial in finite spaces). They satisfy certain axioms, including countable additivity, that link the set algebra of events in the space to the algebra of their probabilities, particularly through continuity properties.
\end{rema}

As a result, the information quantification discussed in Section~\ref{s:utility} are naturally monotone w.r.t.\ the fineness of the generated sigma-algebra. In particular,

\begin{lem}[Monotonicity of Information Measures w.r.t.\ Sigma-Algebra]\label{l:monotonicity}
If $\sigma(X_1) \subset \sigma(X_2)$, then:
\begin{itemize}
    \item $H(X_1) \leq H(X_2)$,
    \item $H(Y|X_2) \leq H(Y|X_1)$ for any $Y: \Omega \to \mathcal{Y}$,
    \item $I(Y;X_1) \leq I(Y;X_2)$ for any $Y: \Omega \to \mathcal{Y}$,
    \item $D_{KL}(\mathbbm{P}(Y|X_1) || \mathbbm{P}(Y)) \leq D_{KL}(\mathbbm{P}(Y|X_2) || \mathbbm{P}(Y))$,
    \item $\|\mathbbm{P}(Y \in A | X_1)\|_2^2 \leq \|\mathbbm{P}(Y \in A | X_2)\|_2^2$ for any $A \in \sigma(Y)$.
\end{itemize}
\end{lem}

Informally, Lemma~\ref{l:monotonicity} shows entropy increases as the sigma-algebra becomes finer, while conditional entropy decreases, indicating reduced uncertainty. Similarly, mutual information and conditional probability energy increase, reflecting enhanced informativeness and predictive utility. Furthermore, as the sigma-algebra generated by $X$ becomes finer, the conditional prediction of $Y$ given $X$ becomes more deterministic, resulting in a larger KL-divergence between the conditional distribution and the original distribution of $Y$.

Combining Lemma \ref{l:Finest_Sigma_Algebra} with Lemma \ref{l:monotonicity}, we deduce that $\Bar{X}$ is the optimal solution to all utility objectives discussed in Section~\ref{S:2} and further specified in Problems~\ref{prob:Entropy_Maximized}–\ref{prob:Energy-Maximized} in the Appendix.

\begin{thm}[Unified Optimal Feature Unlearning Solutions]\label{th:optimal_solution}
Assume $\{X_z\}_{z \in \mathcal{Z}} \subset \mathcal{P}_{2,ac}(\mathcal{X})$. Then the following statements are equivalent:
\begin{itemize}
    \item $\sigma(\hat{X}) = \sigma(\Bar{X})$,
    \item $\hat{X} \in \argmax_{\hat{X}=f(X,Z)} \{ H(\hat{X}): \hat{X} \perp Z \}$,
    \item $\hat{X} \in \argmin_{\hat{X}=f(X,Z)} \{ H(Y|\hat{X}): \hat{X} \perp Z \}$ for all $Y$,
    \item $\hat{X} \in \argmax_{\hat{X}=f(X,Z)} \{ I(Y;\hat{X}): \hat{X} \perp Z \}$ for all $Y$,
    \item $\hat{X} \in \argmax_{\hat{X}=f(X,Z)} \{ D_{KL}(\mathbbm{P}(Y|\hat{X}) || \mathbbm{P}(Y)): \hat{X} \perp Z \}$ for all $Y$,
    \item $\hat{X} \in \argmax_{\hat{X}=f(X,Z)} \{ \|\mathbbm{P}(Y \in A|\hat{X})\|_2^2: \hat{X} \perp Z \}$ for all $A$ and $Y$.
\end{itemize}
\end{thm}

\begin{proof}
    This follows from Lemma \ref{l:Finest_Sigma_Algebra} and Lemma \ref{l:monotonicity}.
\end{proof}

\subsection{Data Point Unlearning: Fine-tuning Guided by Compression Rate}

For data point unlearning, we adopt the following framework to fine-tune the trained model parameters under the guidance of mutual information regularization:
$$\sup_{f: \mathcal{X} \rightarrow \mathcal{Y}} \mathcal{U}(Y; \hat{Y}) - \gamma I(\hat{Y}_{train} ; Z)$$

The following theorem is now a direct consequence of Lemma \ref{l:epsilon_DU_bound_mutual_info}.

\begin{thm}[$\epsilon$-Differential Unlearning Guarantee via Compression Rate]\label{th:DU Guarantee via Compression Rate}
    Assume the unlearning outcome $f_{\theta}$ satisfies $I(f_{\theta}(X_{train}) ; Z) \leq \mu$, then $f_{\theta}$ satisfies $\epsilon$-differential unlearning with probability at least $1 - \frac{\exp(\epsilon) + 1}{\exp(\epsilon) - 1}\sqrt{\frac{\mu}{2}}$.
\end{thm}

Therefore, one can achieve $\epsilon$-differential unlearning based on the compression rate and choose the hyperparameter $\gamma$ in the framework according to the required $\epsilon$ and the resulting trade-off between utility and mutual information.

\section{Algorithm Design}\label{S:Algorithm}

Here, we summarize the pseudo-code implementation of the feature and data point unlearning framework we propose:

\subsection{Feature Unlearning}

\subsubsection{Feature Unlearning on Data via Theorem \ref{th:optimal_solution}}

\begin{algorithm}[H]
\caption{Feature Unlearning via Wasserstein Barycenter}
\label{alg:bary}
\begin{algorithmic}[1]
\REQUIRE Dataset $D = (X,Z) = \{(x_i, z_i)\}_{i=1}^{N}$, Maximum number of iterations $T$, Convergence threshold $\epsilon$.
\ENSURE Estimated Wasserstein barycenter $\bar{X}$.

\STATE \textbf{Initialize:} $\bar{X} \leftarrow \bar{X}_0$ \COMMENT{Random initialization of barycenter}

\FOR{$t = 1$ to $T$} 
    \FOR{each unique value of $Z$: $z \in \text{unique}(Z)$}
        \STATE Compute the optimal transport map $T_z$ that maps $\bar{X}$ to $X_z := X |_{Z=z}$.
    \ENDFOR
    \STATE Compute updated barycenter: $\bar{X}_{\text{new}} = \sum_{z \in \text{unique}(Z)} \frac{|X_z|}{|X|} T_z(\bar{X})$
    \STATE Compute convergence criterion: $\epsilon_t = \mathcal{W}_2(\bar{X},\bar{X}_{\text{new}})$.
    \STATE Update barycenter: $\bar{X} \leftarrow \bar{X}_{\text{new}}$.
    \IF{$\epsilon_t < \epsilon$}
        \STATE \textbf{break} \COMMENT{Terminate loop if convergence threshold is met}
    \ENDIF
\ENDFOR

\STATE \textbf{Return} Estimated Wasserstein barycenter $\bar{X}$.

\end{algorithmic}
\end{algorithm}

\subsubsection{Feature Unlearning on Modal via Regularization}

\begin{algorithm}[H]
\caption{Feature Unlearning on Model via Regularization}
\label{alg:feature_reg}
\begin{algorithmic}[1]
\REQUIRE Dataset $D = (X,Y,Z) = \{(x_i, y_i, z_i)\}_{i=1}^{N}$, loss function $-\mathcal{U}$, learning rate $\eta$, batch size $B$, number of epochs $T$, regularization parameter $\gamma$.\\
\textbf{(Optional:)} Pre-trained neural network $f_{\theta_{\text{origin}}}$ with parameters $\theta_{\text{origin}}$.
\ENSURE Unlearned model parameters $\theta$.

\STATE \textbf{Initialize:} $\theta \leftarrow$ random initialization
\IF{pre-trained model available}
    \STATE Load $\theta \leftarrow \theta_{\text{origin}}$
\ENDIF

\FOR{$t = 1$ to $T$} 
    \STATE Shuffle dataset $D$
    \FOR{each mini-batch $d \subset D$ with $d = (X_d,Y_d,Z_d)$}
        \STATE Compute predictions: $\hat{Y}_d = f_{\theta}(X_d)$
        \STATE Compute loss: $\mathcal{L}_{\text{reg}} = -\mathcal{U}(\hat{Y}_d;Y_d) + \gamma I(\hat{Y}_d;Z_d)$
        \STATE Compute gradients: $\nabla_{\theta} \mathcal{L}_{\text{reg}}$
        \STATE Update parameters: $\theta \leftarrow \theta - \eta \nabla_{\theta} \mathcal{L}_{\text{reg}}$
    \ENDFOR
\ENDFOR

\STATE \textbf{Return} Unlearned model parameters $\theta$

\end{algorithmic}
\end{algorithm}

\subsection{Data Point Unlearning on the Model Parameter Space}

\begin{algorithm}[H]
\caption{Data Point Unlearning via Regularization}
\label{alg:point_reg}
\begin{algorithmic}[1]
\REQUIRE Remaining dataset $R = \{(x_i, y_i)\}_{i=1}^{N}$, Unlearning dataset $U = \{(x_i, y_i)\}_{i=N+1}^{N+K}$, trained neural network $f_{\theta_{\text{origin}}}$ with parameters $\theta_{\text{origin}}$, loss function $-\mathcal{U}$, learning rate $\eta$, batch size $B$, number of epochs $T$, regularization parameter $\gamma$.
\ENSURE Unlearned model parameters $\theta$.

\STATE \textbf{Initialize:} Load pre-trained parameters $\theta \leftarrow \theta_{\text{origin}}$.
\FOR{$t = 1$ to $T$}
    \STATE Shuffle datasets $R$ and $U$.
    \FOR{each mini-batch $r \subset R$ and $u \subset U$, where $r = (X_r, Y_r)$ and $u = (X_u, Y_u)$}
        \STATE Compute predictions: $\hat{Y}_r = f_{\theta}(X_r)$ and $\hat{Y}_u = f_{\theta}(X_u)$.
        \STATE Construct $\hat{Y}_0 = \text{concat}(\hat{Y}_r, \hat{Y}_u, \dim = 0)$ and $\hat{Y}_1 = \hat{Y}_r$.
        \STATE Define relational dataset $(\hat{Y}_{train}, Z)$ where: $\hat{Y}_{train} |_{Z=0} = \hat{Y}_0$ and $\hat{Y}_{train} |_{Z=1} = \hat{Y}_1$.
        \STATE Compute regularized loss: $\mathcal{L}_{\text{reg}} = -\mathcal{U}(\hat{Y}_r, Y_r) + \gamma I(\hat{Y}_{train}; Z)$.
        \STATE Compute gradients: $\nabla_{\theta} \mathcal{L}_{\text{reg}}$.
        \STATE \textbf{Update} model parameters: $\theta \leftarrow \theta - \eta \nabla_{\theta} \mathcal{L}_{\text{reg}}$.
    \ENDFOR
\ENDFOR

\STATE \textbf{Return} unlearned model parameters $\theta$.

\end{algorithmic}
\end{algorithm}

\section{Numerical Experiments}\label{S:4}

We conduct numerical experiments on synthetic and real-world datasets to validate the proposed framework, focusing on its theoretical guarantees and explainability rather than benchmarking against other methods. A more comprehensive evaluation, including improved barycenter estimation for feature unlearning, optimal regularization selection, and refined mutual information estimation for data point unlearning, is left for future work.
%The code will be released on GitHub upon paper acceptance.
The code is available at \url{https://github.com/xushizhou/Machine_Unlearn_via_Info_Reg}.

\subsection{Feature Unlearning}

Here, we demonstrate the unified solution using the Wasserstein barycenter on image data (Celeba \cite{liu2015faceattributes}) with the following features to unlearn: {\em gender} and {\em smile}.

\begin{figure}[H]
    \centering
    \vspace{-3mm}  % Reduce space above the figure
    \includegraphics[width=\columnwidth]{smile_unlearn.png}
    \includegraphics[width=\columnwidth]{gender_unlearn.png}
    \vspace{-5mm}  % Reduce space above the figure
    \caption{The above plot shows the Wasserstein Barycenter (Bary(X)) characterization of the optimal feature unlearning result. In particular, we apply neural optimal transport \cite{korotin2022neural} to learn the optimal transport map from the smile (female) faces to non-smile (male) faces if smile (gender) is the feature to unlearn, then generate the barycenter using McCann interpolation \cite{mccann1997convexity} with $t=0.5$.}
    \label{plot:unlearn_smile}
    \vspace{-3mm}  % Reduce space above the figure
\end{figure}

We note that the Wasserstein-2 barycenter provides one solution to the optimal feature unlearning under the utility objectives outlined in Section \ref{s:utility}. But alternative solutions can be obtained by selecting a more appropriate metric space, instead of the $\ell^2(\mathbb{R}^{d\times d})$ space used here, to better align the Wasserstein geodesic path with the natural data manifold (e.g., the face manifold).

For tabular data, the solution and corresponding experiments closely align with those in machine learning fairness. We refer interested readers to fairness studies such as \cite{chzhen2020fair, xu2023fair} for detailed numerical evaluations of the Wasserstein barycenter approach on datasets including UCI Adult (unlearn gender), COMPAS (race), LSAC (race), and CRIME (race), along with comparisons to other fairness-driven feature unlearning methods.

\subsection{Data Point Unlearning}

To assess the effectiveness and efficiency of the proposed framework for data point unlearning, we conduct experiments on both synthetic and real-world datasets. First, we generate synthetic data of size 5000 from a Gaussian mixture model (GMM) with two distinct means and train a neural network for classification. We then select a data point for unlearning and compare the predictions of four models: (1) the original classifier, (2) the classifier retrained from scratch, (3) the classifier retained from the original one, (4) the unlearned model via our mutual information regularization. See Table~\ref{tab:gmm_unlearning} for the results averaged over 100 repetitions.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6pt} % Adjust column spacing for readability
\renewcommand{\arraystretch}{1.2} % Adjust row spacing for clarity
\resizebox{\columnwidth}{!}{ % Ensures table fits within column width
\begin{tabular}{|l|c|c|c|}
\hline
%\rowcolor{gray!20} % Light gray header row
\textbf{Method} & \textbf{Epochs} & \textbf{Avg. (Std.) Acc. Remain} & \textbf{Acc. Unlearn} \\ 
\hline
Original Model  & 100  & 0.9334 (0.0017)  & 0.9676 (0.0300) \\ 
\hline
Retrain from Scratch & 100  & 0.9334 (0.0018)  & 0.7848 (0.1901) \\ 
\hline
Retrain on Original Model & 100  & 0.9452 (0.0000)  & 0.9999 (0.0000) \\ 
\hline
Mutual Info Reg. ($\gamma = 4.2$) & \textbf{5} & 0.9244 (0.0000)  & 0.7805 (0.0000) \\ 
\hline
\end{tabular}}
\caption{\textbf{Gaussian Mixture Model (GMM) Unlearning.} The proposed method effectively removes the marginal effect of the unlearned data while maintaining high accuracy on the remaining dataset. In contrast, retraining on the original model fails to remove the effect even after extensive training.}
\label{tab:gmm_unlearning}
\end{table}

Next, we extend the evaluation to the MNIST \cite{lecun2010mnist} dataset. To ensure a significant difference between the predictions of the original model and those of the retrained-from-scratch model, we construct the unlearning dataset such that 75\% of the samples belong to the digit 3 class. See Table~\ref{tab:mnist_unlearning} for the results averaged over 10 repetitions.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6pt} % Adjust column spacing for readability
\renewcommand{\arraystretch}{1.2} % Adjust row spacing for clarity
\resizebox{\columnwidth}{!}{ % Ensures table fits within column width
\begin{tabular}{|l|c|c|c|c|}
\hline
%\rowcolor{gray!20} % Light gray header row
\textbf{Method} & \textbf{Epochs} & \textbf{Avg. (Std.) Acc. Remain} & \textbf{Acc. Unlearn} & \textbf{Acc. Test} \\ 
\hline
Original Model  & 100  & 0.9989 (0.0001)  & 0.9998 (0.0004)  & 0.9781 (0.0011) \\ 
\hline
Retrain from Scratch & 100  & 0.9986 (0.0007)  & 0.9389 (0.0112)  & 0.9764 (0.0016) \\ 
\hline
Retrain on Original Model & 100  & 0.9996 (0.0003)  & 0.9702 (0.0131)  & 0.9797 (0.0016) \\ 
\hline
Mutual Info Reg. ($\gamma = 3.5$) & \textbf{5} & 0.9976 (0.0013)  & 0.9418 (0.0309)  & 0.9736 (0.0045) \\ 
\hline
\end{tabular}}
\caption{\textbf{MNIST Data Point Unlearning.} The proposed method effectively removes the marginal effect of the unlearned dataset while maintaining strong generalization (high test accuracy). In contrast, retraining from the original model treats the dataset to unlearn as test data after extensive training.}
\label{tab:mnist_unlearning}
\end{table}

As illustrated in Figure \ref{plot:MI_ACC}, achieving performance comparable to retraining from scratch requires careful tuning of the relaxation parameter $\gamma$ and continuous monitoring of mutual information. Notably, mutual information exhibits a strong correlation with model accuracy on the MNIST unlearning dataset. By adjusting $\gamma$, we can control the rate at which mutual information decays over training epochs, with larger (smaller) values of $\gamma$ leading to faster (slower) unlearning. This enables controlled utility removal from the unlearning dataset while preserving overall training and test accuracy.

Importantly, a smaller $\gamma$ provides finer control over the compression rate (i.e., mutual information decay), enabling stable and precise unlearning, as demonstrated in the lower-right plot of Figure \ref{plot:MI_ACC}. Conversely, a larger $\gamma$ accelerates compression, making it particularly advantageous for time-sensitive unlearning tasks, as shown in the lower-left plot of Figure \ref{plot:MI_ACC}.

\begin{figure}[H]
    \centering
    \subfloat{%
        \includegraphics[width=0.48\textwidth]{MI_ACC_3.5_5.png}
    }
    \subfloat{%
        \includegraphics[width=0.48\textwidth]{MI_ACC_3.5_20.png}
    } \\
    \subfloat{%
        \includegraphics[width=0.48\textwidth]{MI_ACC_5_2.png}
    }
    \subfloat{%
        \includegraphics[width=0.48\textwidth]{MI_ACC_1.5_20.png}
    }
    \caption{\textbf{Mutual Information vs. Utility in MNIST Unlearning.} Mutual information and model utility on the MNIST unlearning dataset exhibit a strong correlation. The rate of utility reduction can be precisely controlled by adjusting the regularization parameter $\gamma$, allowing for flexible unlearning speed while maintaining accuracy.}
    \label{plot:MI_ACC}
\end{figure}




\section*{Acknowledgement}

The authors acknowledge support from NSF DMS-2208356,  NIH R01HL16351,  
P41EB032840, and DE-SC0023490.


\newpage
%\clearpage

\iffalse
\section*{Impact Statement}
 We have introduced a flexible mathematical framework for unlearning undesirable information in machine learning.  As our approach combines rigorous theoretical guarantees with practical solutions,  we expect our work to have significant impact on both theory and practice of machine unlearning. Since machine unlearning plays a crucial ethical role in AI by enabling data privacy, fairness, accuracy, and responsible development, the broader impact of our paper is that it contributes positively to building trustworthy and ethical AI. We do not see any negative ethical or societal impact of our work.
\fi
 
\clearpage
\bibliography{References_Unlearning}
\bibliographystyle{icml2025_conference}



\newpage
\appendix
\onecolumn

\iffalse

\section{Appendix: Categories of Machine Unlearning}\label{A:Categorty}

Current research in machine unlearning generally distinguishes between two main types:
\begin{itemize}
\item \textit{Data Point Unlearning}: The learning outcome should be invariant under the inclusion or exclusion of specific data points during the training process.
\item \textit{Feature and Label Unlearning}: The learning outcome should be invariant under the inclusion or exclusion of specific features or labels in the training data.
\end{itemize}

However, these two types are not necessarily distinct from a mathematical perspective. For instance, if each learning outcome is associated with a label indicating whether specific data points were involved in training and inference, which is a common scenario in feature unlearning, then unlearning this label effectively reduces to data point unlearning. Conversely, if point-specific labels are not associated with the learning outcome, as is typical in data point unlearning, the task resembles traditional data point unlearning, even if it is formally equivalent to unlearning features or labels.

Thus, the fundamental distinction between these two types of unlearning does not lie in the form of data being unlearned. Instead, it depends on whether the conditional distribution of the learning outcome or dataset with respect to the undesirable information is accessible.

\begin{itemize}
    \item \textit{Unlearning with Known Information}: Information is considered “known” to the learning outcome if it is accessible for each outcome. Specifically, when the undesirable information and learning outcome are represented by random vectors $Z$ and $X$, respectively, we say $Z$ is known to $X$ if the pairing $(X, Z)$ is available.\ts{It would be great to illustrate the difference between the two cases with the AI/hiring example. }
    \item \textit{Unlearning with Unknown Information}: Information is considered “unknown” to the learning outcome if, for some outcomes, the corresponding information is unavailable. In other words, the pairing $(X, Z)$, which represents the relational relationship between $X$ and $Z$, is not fully accessible.
\end{itemize}

To illustrate the fundamental difference between unlearning known and unknown information, we now consider them from intuitive, concrete, and probabilistic perspectives:
\begin{itemize}
    \item \textit{Intuitive}: If the undesirable information associated with the learning outcome is known (i.e., the relational data is available), we can attempt to “remove” this information directly, without requiring additional data or estimation. In contrast, if this information is unknown to the learning outcome, additional data or estimations (introducing noise) are needed to fill in the gap in the relational information between $X$ and $Z$ and thereby approximate the removal of the relational information.
    \item \textit{Concrete}: Consider two face images for which we aim to blur the identity information. If we have access to the identity information corresponding to each image, we can “remove” or blur the identity by merging the two faces, preserving common features while obscuring distinctions. Conversely, if the identity information we wish to remove is not linked to the images—perhaps we are unsure whether the images even depict the individual whose identity we seek to obscure—then we must introduce noise to blur both images. This added noise serves to obscure features that could potentially be associated with the identity we aim to remove.
    \item \textit{Probabilistic}: For readers familiar with probability theory, the availability of relational data $(X, Z)$ enables the “removal” of $Z$ from $X$ in a way that is measurable with respect to $(X, Z)$. In this scenario, since all relational information between $X$ and $Z$ is available, we can eliminate the undesirable information in $X$ by coarsening the sigma-algebra generated by $(X, Z)$ to a coarser one generated by $(\hat{Y}, Z)$, thereby obtaining an unlearned version $\hat{Y}$. Conversely, when $(X, Z)$ is unavailable, certain relational information is missing, making it impossible to perform the removal in a way that is measurable with respect to $(X, Z)$. Consequently, additional data or noise must be introduced, requiring us to work within a sigma-algebra finer than that generated by $(X, Z)$. The difference lies in removing the information of $Z$ by coarsening the sigma-algebra or making it finer by introducing external randomness.
\end{itemize}

\newpage

\fi

\section{Appendix: Supplementary Material for Section 2}

\subsection{Utility Motivation}\label{a:utility_motivation}

Mutual information is a widely used quantification of the common information shared by two random variables. In particular, given a data set $X$ with a goal to compress $X$ by an encoding $\hat{Y}$, the volume of code needed to encode $X$ is $2^{H(X)}$ where $H(X)$ is the entropy of $X$. Furthermore, from the Chapman-Kolmogorov equation $p(\hat{Y}) = \sum_x p(\hat{Y}|x)p(x),$ the average volume of $x$ mapped to individual $\hat{Y}$ is equal to $2^{H(X|\hat{Y})}$. Here,
\begin{equation}
    H(X|\hat{Y}) := -\sum_x p(x) \sum_{\hat{Y}} p(\hat{Y}|x) \log(p(\hat{Y}|x) ) 
\end{equation}
is the conditional entropy of $X$ on $\hat{Y}$. Intuitively, a higher conditional entropy means more volume of $x$ are expected to be mapped to individual $\hat{Y}$, which implies more randomness of $X$ remained given the observation of $\hat{Y}$. In other words, less $X$ is explained by $\hat{Y}$.

Since the volume of code for $X$ is $2^{H(X)}$ and the average volume of code mapped to each $\hat{Y}$ is $2^{H(X|\hat{Y})}$, the average cardinality of the partition generated by the values of $\hat{Y}$ on the values of $X$ is the ratio:
\begin{equation}
    \frac{2^{H(X)}}{2^{H(X|\hat{Y})}} = 2^{I(X;\hat{Y})}.
\end{equation}
Here, $I(X;\hat{Y}) = H(X) - H(X|\hat{Y})$ is the mutual information between $X$ and $\hat{Y}$. On the one hand, higher mutual information implies that  $\hat{Y}$ generates a partition with higher cardinality (or usually finer partition) on $X$, which further implies more common information is shared between $X$ and $\hat{Y}$. On the other hand, from a data compression perspective, lower mutual information means $\hat{Y}$ generates a partition on $Z$ with lower cardinality, which further implies a better data compression rate, because $\hat{Y}$ can compress $X$ into a partition of smaller cardinality.

As discussed in Section~\ref{S:2}, we adopt mutual information to quantify the common information and compression rate between random variables. For  unlearning quality purposes, we hope to maintain as much information of $X$ as possible in generating $\hat{Y}$. Therefore, to maximize utility, we should maximize mutual information $I(X;\hat{Y})$ or, equivalently, minimize the compression rate.

\subsection{Admissibility}\label{a:admissibility}

Since we are unlearning the information of $Z$ by compressing relational data $(X,Z)$, it is natural to require the resulting compressed data $\hat{X}$ to be measurable with respect to $(X,Z)$. Intuitively, the compression output $\hat{X}$ should have its ``root'' from $(X,Z)$ without introducing additional randomness by the compression map $f$ itself. Technically speaking, the ``root'' here means that for every event or observation $A$ of the compressed $\hat{X}$, the information or pre-image represented by the observation $\hat{X}^{-1}(A) := \{\omega: \hat{X}(\omega) \in A\}$ comes from the knowledge of $f^{-1}(A) := \{(x,z): f(x,z) \in A\}$ based on $(X,Z)$:
\begin{align}
\{\omega: \hat{X}(\omega) \in A\} 
&= \hat{X}^{-1}(A) \nonumber \\
&= (X,Z)^{-1}(f^{-1}(A)) \nonumber \\
&= \{\omega: (X(\omega),Z(\omega)) \in f^{-1}(A)\}.
\label{eq:long_equation}
\end{align}
Here, $\omega \in \Omega$ is the smallest unit of information we can have from the measure space $(\Omega, \mathcal{F},\mathbbm{P})$. From a probability-theoretical perspective, since $\hat{X}$ is a compression of $(X,Z)$, it generates a coarser partition (or, more technically, sigma-algebra) than the original information $(X,Z)$ and we say $\hat{X}$ is measurable with respect to $(X,Z)$, denoted by
\begin{equation}
    \sigma(\hat{X}) \subset \sigma((X,Z)).
\end{equation}
This is equivalent to the existence of a $\mathcal{B}_\mathcal{X} \otimes \mathcal{B}_\mathcal{Z}/\mathcal{B}_\mathcal{X}$-measurable map, denoted by $f$, such that $\hat{X} = f(X,Z)$. That is, our admissibility is equivalent to the assumption that the data compression process does not create information or randomness by itself. Therefore, we define the admissible unlearning outcome in our framework as follows:
\begin{equation}
\label{eq:admissible}
\begin{aligned}
\mathcal{A}(X,Z) \;:=\; \Bigl\{\hat{X} = f(X,Z):
 &\; f \text{ is }
 \mathcal{B}_\mathcal{X} \otimes \mathcal{B}_\mathcal{Z}/\mathcal{B}_\mathcal{X} \\
 &\quad \text{-measurable} \Bigr\}.
\end{aligned}
\end{equation}
and we use $\hat{X} = f(X,Z)$ and $\hat{X} \in \mathcal{A}(X,Z)$ interchangeably.

\subsection{Proof of Lemma \ref{l:epsilon_DU_bound_mutual_info}}\label{a:proof_DU_bound_mutual_info}

\begin{proof}
    First, notice that it follows from the construction of relational data $(X_{train},Z)$ that $\{Z = 0\} = \{X_{train} = X_0\}$ and $\{Z = 1\} = \{X_{train} = X_1\}$. Also, we have 
    \begin{align}
        |\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 1|\hat{Y})| & = |2 \mathbb{P}(Z = 0|\hat{Y}) - 1|\\
    & \leq 2 |\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 0)| + 2|\mathbb{P}(Z = 0) - 0.5|\\
    & \leq ||\mathbb{P}(Z|\hat{Y}) - \mathbb{P}(Z)||_{TV},
    \end{align}
    where the third line follows from the definition of total variation distance and the prior information $\mathbb{P}(Z = 0) = \frac{1}{2}$. By taking the expectation over $\hat{Y}$, we have
    \begin{align*}
        \mathbb{E}_{\hat{Y}} ( |\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 1|\hat{Y})| ) & \leq \mathbb{E}_{\hat{Y}} ( ||\mathbb{P}(Z|\hat{Y}) - \mathbb{P}(Z)||_{TV} )\\
    & \leq \mathbb{E}_{\hat{Y}} \left( \sqrt{\frac{1}{2}KL(\mathbb{P}(Z|\hat{Y})||\mathbb{P}(Z))} \right)\\
    & \leq \frac{1}{2} \sqrt{\mathbb{E}_{\hat{Y}} \left( KL(\mathbb{P}(Z|\hat{Y})||\mathbb{P}(Z)) \right)}\\
    & = \frac{1}{2}\sqrt{I(Z;\hat{Y})}.
    \end{align*}
    Here, the second line follows from Pinsker's inequality, the third from Jensen's inequality, and the fourth from the definition of mutual information. Now, for any fixed $\epsilon > 0$, it follows from Markov's inequality that
    \begin{equation*}
        \mathbb{P}\left( \{ |\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 1|\hat{Y})| \leq \epsilon \} \right) \geq 1-\frac{1}{\epsilon}\left( \sqrt{\frac{1}{2} I(Z;\hat{Y})} \right).
    \end{equation*}
    Finally, it follows from 
$$|\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 1|\hat{Y})| \leq \epsilon \Longrightarrow \log\left(\frac{\mathbb{P}(Z = 0 \mid \hat{Y})}{\mathbb{P}(Z = 1 \mid \hat{Y})}\right) \leq \log\Big(\frac{1+\epsilon}{1-\epsilon}\Big)$$
that 
$$\big\{|\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 1|\hat{Y})| \leq \epsilon\big\} \subset \left\{ |\log\left(\frac{\mathbb{P}(Z = 0 \mid \hat{Y})}{\mathbb{P}(Z = 1 \mid \hat{Y})}\right)| \leq \log\Big(\frac{1+\epsilon}{1-\epsilon}\Big) \right\},$$ and
    \begin{align*}
        \mathbb{P}\left( \{ |\log\left(\frac{\mathbb{P}(Z = 0 \mid \hat{Y})}{\mathbb{P}(Z = 1 \mid \hat{Y})}\right)| \leq \log(\frac{1+\epsilon}{1-\epsilon}) \} \right) & \geq \mathbb{P}\left( \{ |\mathbb{P}(Z = 0|\hat{Y}) - \mathbb{P}(Z = 1|\hat{Y})|\leq \epsilon \} \right)\\
    & \geq 1-\frac{1}{\epsilon}\left( \sqrt{\frac{1}{2} I(Z;\hat{Y})} \right).
    \end{align*}
    Since $\{Z = 0\} = \{X_{train} = X_0\}$ and $\{Z = 1\} = \{X_{train} = X_1\}$ by construction, the proof is complete.
\end{proof}

\subsection{Proof of Lemma \ref{l:DU_as_condition_for_good_model}}\label{a:proof_DU_as_good}

\begin{proof}
    Assume for contradiction that there exists a $y^* \in \mathcal{Y}$ such that $|\log(\frac{f(X_0)(y^*)}{f(X_0)(y^*)})| > \epsilon$, then let $\delta$ be the radius around $y^*$ that satisfies $\sign(\log (\frac{f(X_0)(y)}{f(X_1)(y)})) = \sign(\log (\frac{f(X_0)(y^*)}{f(X_1)(y^*)}))$, we have
    \begin{align*}
        \mathcal{W}_{d_{\mathcal{Y}}}(f(X_0), f(X_1)) & > \delta \int_{B_{\delta}(y^*)} |f(X_0) - f(X_1)|(y)dy\\
        & \geq \delta |f(X_0) - f(X_1)|(y^*)
    \end{align*}
    Now, if there exits a $L \leq L^*(\epsilon)$ such that $f$ is L-Lipschitz, then we have
    \begin{equation}
        d_{\mathcal{Y}}(f(x),f(x')) \leq Ld_{\mathcal{X}}(x,x') \leq L^*(\epsilon)d_{\mathcal{X}}(x,x').
    \end{equation}
    But that implies
    \begin{align*}
        \mathcal{W}_{d_\mathcal{Y}}(f(X_0),f(X_1)) & \leq L^*(\epsilon)\mathcal{W}_{d_\mathcal{X}}(X_0,X_1)\\
        & = \frac{\delta \,\bigl|f(X_1)(y^*) - f(X_0)(y^*)\bigr|}{\,\mathcal{W}_{d_\mathcal{X}}(X_1,X_0)} \mathcal{W}_{d_\mathcal{X}}(X_0,X_1)\\
          & = \delta \,\bigl|f(X_1)(y^*) - f(X_0)(y^*)\bigr|
    \end{align*}
    which contradicts $\mathcal{W}_{d_{\mathcal{Y}}}(f(X_0), f(X_1)) > \delta |f(X_0) - f(X_1)|(y^*)$. Therefore, $f$ must violate L-Lipschitz for any $L \leq L^*(\epsilon)$.
\end{proof}

\subsection{Details on Considered Utility Quantifications}\label{a:utility_appendix}

In practical machine unlearning, the utility of unlearning may need to be evaluated with respect to a different target variable $Y$ rather than the original dataset $X$. Moreover, mutual information, while often a natural choice, is not the only metric for quantifying the relationship between the unlearning outcome $\hat{X}$ and the target variable $Y$. To accommodate diverse objectives, we extend our framework to the general formulation:
\begin{equation}\label{eq:general_formulation}
\sup_{\hat{X} = f(X,Z)} \{ \mathcal{U}(Y;\hat{X}): \hat{X} \perp Z \},
\end{equation}
where $\mathcal{U}(Y;\hat{X})$ represents a utility quantification, and the constraint $\hat{X} \perp Z$ ensures that the unwanted information $Z$ is fully removed from $\hat{X}$. Below, we introduce several commonly used utility objectives and their corresponding constrained optimization problems, for which we provide a unified analytic feature unlearning solution in the next section.

\medskip
\noindent
{\bf Entropy Maximization:} The utility is defined as the entropy of the unlearning output, $\mathcal{U}(Y;\hat{X}) = H(\hat{X})$, which quantifies the information in $\hat{X}$. Entropy is commonly used to balance exploitation and exploration, such as in classifier training \cite{pereyra2017regularizing}. The optimization problem, \textit{Entropy-Maximized Feature Unlearning}, is given by $\sup_{\hat{X} = f(X,Z)} \{H(\hat{X}): \hat{X} \perp Z\}$. Alternatively, it can be interpreted as entropy-regularized mutual information minimization: $\sup_{\hat{X} = f(X,Z)} \{-I(Z;\hat{X}) + \frac{1}{\beta} H(\hat{X})\}$, where $\beta$ controls the trade-off.

\medskip
\noindent
{\bf Mutual Information Maximization:} The utility is defined as $\mathcal{U}(Y;\hat{X}) = I(Y;\hat{X})$, measuring the shared information between the target variable $Y$ and the unlearning outcome $\hat{X}$. This objective is widely applied in classification methods such as decision trees \cite{criminisi2012decision} and in deep learning techniques, including Deep InfoMax \cite{hjelm2018deepinfomax} and information bottleneck methods \cite{alemi2017vib}. The corresponding optimization problem, \textit{Mutual-Information-Maximized Feature Unlearning}, is given by $\sup_{\hat{X} = f(X,Z)} \{I(Y;\hat{X}): \hat{X} \perp Z\}$. Since $I(Y;\hat{X}) = H(Y) - H(Y|\hat{X})$, this problem is equivalent to minimizing the conditional entropy $H(Y|\hat{X})$. As a result, it provides an optimal solution for utility preservation with respect to any target variable $Y$.

\medskip
\noindent
{\bf KL-Divergence Maximization:} The utility is  $\mathcal{U}(Y;\hat{X}) = D_{KL}(\mathbbm{P}(Y|\hat{X}) || \mathbbm{P}(Y))$, where $D_{KL}$ measures the divergence between the predicted and prior distributions of $Y$. This objective is commonly applied in generative models such as Variational Autoencoders (VAEs) \cite{higgins2017betaVAE}. The corresponding optimization problem, \textit{KL-Divergence-Maximized Feature Unlearning}, is formulated as $\sup_{\hat{X} = f(X,Z)} \{D_{KL}(\mathbbm{P}(Y|\hat{X}) || \mathbbm{P}(Y)): \hat{X} \perp Z\}$. This problem seeks to make $\mathbbm{P}(Y|\hat{X})$ as deterministic as possible relative to the prior $\mathbbm{P}(Y)$, thereby enhancing the predictive power of $\hat{X}$ for $Y$.

\medskip
\noindent
{\bf Conditional Probability Energy Maximization:} The utility is defined as the $L^2$-norm of the conditional probability $\mathbbm{P}(Y \in A | \hat{X})$ for classification or the negative mean squared error (MSE) for regression:
\begin{equation*}
    \mathcal{U}(Y;\hat{X}) =
    \begin{cases}
         \sqrt{\mathbb{E}_{\hat{X}} \left[ \mathbb{P}(Y \in A | \hat{X})^2 \right]} & \text{for classification,}\\
        -||Y - \mathbb{E}(Y|\hat{X})||_2 & \text{for regression}.
    \end{cases}
\end{equation*}
The corresponding optimization problem, \textit{Energy-Maximized Feature Unlearning}, is formulated as $\sup_{\hat{X} = f(X,Z)} \{||\mathbbm{P}(\{Y \in A_Y\}|\hat{X})||_2^2 : \hat{X} \perp Z\}$. A higher $L^2$-norm indicates a more precise prediction of the event $\{Y \in A_Y\}$ based on $\hat{X}$, leading to reduced Bayes error and improved decision boundaries.

As we show in the next section, when the above-listed objectives are applied, there exists a universal optimal feature unlearning solution to the general formulation: equation \eqref{eq:general_formulation} for arbitrary target variable $Y$.



\subsection{Formulation of Constrained Optimization Problems}\label{a:formulation_constraint_optimization}

Our goal here is to provide theoretical solutions to the following constrained optimization problems under mild assumptions, thereby developing a unified mathematical framework for machine unlearning of features and labels under various utility objectives:

\begin{prob}[Entropy-Maximized Feature Unlearning]\label{prob:Entropy_Maximized}
    \begin{equation*}
        \sup_{\hat{Y} \in \mathcal{A}(X,Z)} \{H(\hat{Y}): \hat{Y} \perp Z\}.
    \end{equation*}
    \end{prob}
    Here, $H$ denotes the entropy (or differential entropy), which measures the information contained in the unlearning outcome $\hat{Y}$. As discussed, entropy is a fundamental metric in information theory and probability for quantifying information and randomness. Thus, Problem~\ref{prob:Entropy_Maximized} seeks to optimally compress $(X, Z)$ to produce $\hat{Y}$ with the information about $Z$ effectively removed.

\begin{prob}[Conditional-Entropy-Minimized Feature Unlearning]\label{prob:Conditional_Entropy_Minimized}
    \begin{equation*}
        \inf_{\hat{Y} \in \mathcal{A}(X,Z)} \{H(Y|\hat{Y}): \hat{Y} \perp Z\}.
    \end{equation*}
    \end{prob}
    In many cases, an unlearning output $\hat{Y}$ may be used to generate inferences or predictions for some random variable $Y$. Thus, it is also desirable to solve Problem \ref{prob:Conditional_Entropy_Minimized} for some dependent variable $Y$. Notice that, due to $I(X;Y) = H(Y) - H(Y|X)$, the above problem shares the same solution as the maximization of mutual information between $Y$ and $\hat{Y}$:

\begin{prob}[Mutual-Information-Maximized Feature Unlearning]\label{prob:Mutual_Information_Maximized}
    \begin{equation*}
        \sup_{\hat{Y} \in \mathcal{A}(X,Z)} \{I(Y;\hat{Y}): \hat{Y} \perp Z\}.
    \end{equation*}
    \end{prob}
    Notably, the optimal solution to Problem \ref{prob:Conditional_Entropy_Minimized} and \ref{prob:Mutual_Information_Maximized} does not depend on the specific choice of $Y$ due to the monotonicity of the functional $H(Y|\cdot)$ with respect to the sigma-algebra generated by $\hat{Y}$. Thus, despite the explicit presence of $Y$ in Problem \ref{prob:Conditional_Entropy_Minimized}, it provides a generalized solution for any choice of $Y$.

\begin{prob}[KL-Divergence-Maximized Feature Unlearning]\label{prob:KL-Divergence-Maximized}
    \begin{equation*}
        \sup_{\hat{Y} \in \mathcal{A}(X,Z)} \{D_{KL}(\mathbbm{P}(Y|\hat{Y}) || \mathbbm{P}(Y) ): \hat{Y} \perp Z\}.
    \end{equation*}
    \end{prob}
    Given a variable of interest denoted by $Y$, a general downstream machine learning or AI task may aim to estimate the conditional probability using the unlearning outcome $\hat{Y}$. Therefore, it is desirable to make $\mathbbm{P}(Y|\hat{Y})$ as deterministic as possible relative to the original distribution of $Y$. To quantify this determinism, we use the KL-divergence of $\mathbbm{P}(Y|\hat{Y})$ relative to $\mathbbm{P}(Y)$, leading to the optimization problem above. Intuitively, a more accurate prediction of $\mathbbm{P}(Y|\hat{Y})$ implies less randomness relative to $\mathbbm{P}(Y)$, which increases the KL-divergence of $\mathbbm{P}(Y|\hat{Y})$ relative to $\mathbbm{P}(Y)$. Thus, maximizing the KL-divergence enhances the predictive power of $\hat{Y}$ for $Y$. We later show that Problem \ref{prob:KL-Divergence-Maximized} is also independent of the choice of $Y$.

\begin{prob}[Energy-Maximized Feature Unlearning]\label{prob:Energy-Maximized}
    \begin{equation*}
        \sup_{\hat{Y} \in \mathcal{A}(X,Z)} \{ ||\mathbbm{P}(\{Y \in A_Y\}|\hat{Y})||_2^2 : \hat{Y} \perp Z\}.
    \end{equation*}
    \end{prob}
    Finally, from the perspective of conditional probability estimation, for a given $Y$ and event $A_Y \in \mathcal{B}_Y$, it is natural to maximize the energy (or equivalently, the $L^2$ norm) of the conditional probability $\mathbbm{P}(\{Y \in A_Y\}|\hat{Y})$. Here, a larger $L^2$ norm of the conditional probability indicates a more precise prediction of the event $\{Y \in A_Y\}$ based on the information provided by $\hat{Y}$.

%sNote that the objectives listed above are widely used in scientific fields such as biology, chemistry, physics, medical science, social science, and AI. %In the next section, we demonstrate that focusing on the sigma-algebra generated by $\hat{Y}$ allows us to simultaneously solve these four unlearning optimization problems, despite their seemingly different objective functions.

\newpage

\section{Appendix: Supplementary Material for Section 3}

\subsection{Wasserstein Distance and Barycenter}\label{a:OT_intro}

Given $\mu, \nu \in \mathcal{P}(\mathbb{R}^d)$ where $\mathcal{P}(\mathbb{R}^d)$ denotes the set of all the probability measures on $\mathbb{R}^d$, $$\mathcal{W}_2(\mu,\nu) := \left(\inf_{ \lambda \in \prod (\mu,\nu) } \Big\{\int_{\mathbb{R}^d \times \mathbb{R}^d} ||x_1 - x_2||^2 d \lambda(x_1,x_2)\Big\}\right)^{\frac{1}{2}}.$$ Here, $\prod (\mu,\nu) := \{\pi \in \mathcal{P}((\mathbb{R}^d)^2): \int_{\mathbb{R}^d} d\pi(\cdot,v) = \mu, \int_{\mathbb{R}^d} d\pi(u,\cdot) = \nu \}$.
$(\mathcal{P}_2(\mathbb{R}^d),\mathcal{W}_2)$ is called the Wasserstein space, where $\mathcal{P}_2(\mathbb{R}^d):= \Big\{\mu \in \mathcal{P}(\mathbb{R}^d): \int_{\mathbb{R}^d} ||x||^2 d\mu < \infty\Big\}$. Also, we use $\mathcal{P}_{2,ac}(\mathbb{R}^d)$ to denote the set of probability measures with finite second moments and are absolute continuous w.r.t. the Lebesgue measure. To simplify notation, we often denote $$\mathcal{W}_2(X_1,X_2) := \mathcal{W}_2(\mathcal{L}(X_1),\mathcal{L}(X_2)),$$ where $\mathcal{L}(X) := \mathbb{P} \circ X^{-1} \in \mathcal{P}(\mathbb{R}^d)$ is the law or distribution of $X$, $X: \Omega \rightarrow \mathcal{X} := \mathbb{R}^d$ is a random variable (or vector) with an underlying probability space $(\Omega, \mathcal{F}, \mathbb{P})$. Intuitively, one can consider the Wasserstein distance as $L^2$ distance after optimally coupling two random variables whose distributions are $\mu$ and $\nu$. That is, if the pair $(X_1,X_2)$ is an optimal coupling \cite{villani2008optimal}, then $$\mathcal{W}_2(X_1,X_2) = ||X_1 - X_2||_{L^2} = \int_{\Omega} ||X_1(\omega) - X_2(\omega)||^2 d\mathbb{P}(\omega).$$
Given $\{\mu_z\}_{z \in \mathcal{Z}} \subset (\mathcal{P}_2(\mathbb{R}^d),\mathcal{W}_2)$ for some index set $\mathcal{Z}$, their Wasserstein barycenter~\cite{agueh2011barycenters} with weights $\lambda \in \mathcal{P}(\mathcal{Z})$ is 
\begin{equation}\label{barycenter}
\bar{\mu} := \text{argmin}_{\mu \in \mathcal{P}_2(\mathbb{R}^d)} \Big\{\int_{\mathcal{Z}} \mathcal{W}_2^2(\mu_z,\mu) d\lambda(z)\Big\}.
\end{equation}

If there is no danger of confusion, we will refer to the Wasserstein barycenter simply as barycenter. Also, we use $\bar{X}$ to denote the random variable such that $\mathcal{L}(\bar{X}_z) = (T_z)_{\sharp}\mathcal{L}(X_z)$ where $T_z$ is the optimal transport map from $\mathcal{L}(X_z)$ to $\bar{\mu}$. $\bar{\mu}$ is the barycenter of $\mu_z = \mathcal{L}(X_z)$ for all $z$.

\subsection{Proof of Lemma \ref{l:Finest_Sigma_Algebra}}\label{a:proof_finest_sigma_algebra}

\begin{proof}
    See Lemma 5.2 in \cite{xu2023fair} for the full proof. We provide a sketch here: Under the absolute continuity assumption of $X_z$'s, we have the Wasserstein-2 barycenter is also absolute continuous. Therefore, the optimal transport maps between the barycenter and each of the $X_z$ are invertible (almost everywhere). This implies that there exists invertible measurable maps between $(\bar{X},Z)$ and $(X,Z)$ and hence $\sigma((\hat{Y},Z)) \subset \sigma((X,Z)) = \sigma((\bar{X},Z))$. Now, by the independence constraint, we also have $\sigma((\bar{X},Z)) = \sigma(\bar{X}) \otimes \sigma(Z)$ and $\sigma((\hat{Y},Z)) = \sigma(\hat{Y}) \otimes \sigma(Z)$. Therefore, it follows from $\sigma(\hat{Y}) \otimes \sigma(Z) \subset \sigma(\bar{X}) \otimes \sigma(Z) \implies \sigma(\hat{Y}) \subset \sigma(\bar{X})$ that $\sigma(\hat{Y}) \subset \sigma(\bar{X})$ for all admissible $\hat{Y}$. That completes the proof.
\end{proof}

\iffalse
\subsection{Sigma-Algebra and Information}

\begin{rema}[Sigma-Algebra and Information]\label{r:Sigma-Algebra and Information}
In probability theory, a probability space is often represented as a triple $(\Omega, \Sigma, \mathbb{P})$, where $\Omega$ is the sample space, $\Sigma$ is the sigma-algebra (a collection of subsets of $\Omega$), and $\mathbb{P}: \Sigma \rightarrow [0,1]$ is a probability measure that assigns probabilities to each event in $\Sigma$.

The same sample space can be associated with different sigma-algebras, resulting in different probability spaces. We say that a sigma-algebra $\Sigma_1$ is finer than $\Sigma_2$, denoted $\Sigma_2 \subset \Sigma_1$, if $\Sigma_1$ contains all events in $\Sigma_2$. Conversely, we say $\Sigma_1$ is coarser than $\Sigma_2$ if $\Sigma_1$ contains fewer events than $\Sigma_2$.

A random variable or random vector $X$ is a measurable function from the probability space to $\mathbb{R}^d$ (or $\mathbb{C}^d$), $X: \Omega \rightarrow \mathbb{R}^d$. The sigma-algebra generated by $X$, denoted by $\sigma(X)$, comprises all possible events that could be defined based on the image of $X$ in $\mathbb{R}^d$ (or $\mathbb{C}^d$). Thus, if $X$ generates a finer sigma-algebra than another variable $X’$, denoted $\sigma(X’) \subset \sigma(X)$, then $X$ contains more events and, therefore, more information than $X’$.

In modern probability theory, sigma-algebras facilitate the construction of probability measures, especially in countably or uncountably infinite spaces (as the concept is trivial in finite spaces). They satisfy certain axioms, including countable additivity, that link the set algebra of events in the space to the algebra of their probabilities, particularly through continuity properties.
\end{rema}
\fi

\subsection{Proof of Lemma \ref{l:monotonicity}}

\iffalse
\begin{lem}[Monotonicity of Information Measures w.r.t.\ Sigma-Algebra]\label{l:monotonicity}
If $\sigma(X_1) \subset \sigma(X_2)$, then:
\begin{itemize}
    \item $H(X_1) \leq H(X_2)$,
    \item $H(Y|X_2) \leq H(Y|X_1)$ for any $Y: \Omega \to \mathcal{Y}$,
    \item $I(Y;X_1) \leq I(Y;X_2)$ for any $Y: \Omega \to \mathcal{Y}$,
    \item $D_{KL}(\mathbbm{P}(Y|X_1) || \mathbbm{P}(Y)) \leq D_{KL}(\mathbbm{P}(Y|X_2) || \mathbbm{P}(Y))$,
    \item $\|\mathbbm{P}(Y \in A | X_1)\|_2^2 \leq \|\mathbbm{P}(Y \in A | X_2)\|_2^2$ for any $A \in \sigma(Y)$.
\end{itemize}
\end{lem}
\fi

\begin{proof}
    Assume $\sigma(X_1) \subset \sigma(X_2)$. For entropy, we have
    $H(X_2) - H(X_1) = H(X_2 | X_1) \geq 0$. Therefore, $H(X_2) - H(X_1)$.

    For mutual information, it follows from the assumption $\sigma(X_1) \subset \sigma(X_2)$ that there exists a measurable function $g(X_2) = X_1$. Therefore, given any $Y$, we have $Y$ is conditionally independent of $X_1$ given $X_2$ because $X_1$ is a constant given $X_2$. That is, $Y \rightarrow X_2 \rightarrow X_1$ forms a Markov chain. It follows from the data-processing inequality~\cite{CT05} that $I(Y;X_1) \leq I(Y;X_2)$.

    For the conditional entropy, we have $H(Y|X_s) = H(Y) - I(Y;X_s)$ for $s \in \{1,2\}$. Therefore, it follows from $I(Y;X_1) \leq I(Y;X_2)$ that $H(Y|X_2) \leq H(Y|X_1)$.

    For KL-divergence, it follows from the convexity of the divergence in the first argument, the $\mathbb{P}(Y|X_1) = \mathbb{E}(\mathbb{P}(Y|X_2)|X_1)$, and Jensen's inequality that $$D_{KL}(\mathbb{P}(Y|X_1)||\mathbb{P}(Y)) \leq \mathbb{E}_{X_2}(D_{KL}(\mathbb{P}(Y|X_2)||\mathbb{P}(Y))|X_1).$$ Finally, by taking expectation w.r.t. $X_1$ on both sides, we have $$D_{KL}(\mathbb{P}(Y|X_1)||\mathbb{P}(Y)) \leq D_{KL}(\mathbb{P}(Y|X_2)||\mathbb{P}(Y)).$$

    For the conditional probability energy, it follows directly from $\mathbbm{P}(\{Y \in  A_{\mathcal{Y}}|X_1\}|X) = \mathbbm{E}(\mathbbm{1}_{Y \in A_{\mathcal{Y}}}|X)$ and the tower property of conditional expectation: If $\sigma(X_1) \subset \sigma(X_2)$,
    \begin{equation*}
        \mathbbm{E}(\mathbbm{1}_{Y \in A_{\mathcal{Y}}}|X_1) = \mathbbm{E}(\mathbbm{E}(\mathbbm{1}_{Y \in A_{\mathcal{Y}}}|X_2)|X_1).
    \end{equation*}

\end{proof}

\subsection{Intuitive Insights into Theorem \ref{th:optimal_solution}}

\begin{itemize}
    \item Among all admissible $\hat{Y}$ outcomes, $\Bar{X}$ maximizes randomness or information when quantified by entropy.
    \item Given $\Bar{X}$, the conditional probability distribution of $Y$ retains the least randomness among all admissible $\hat{Y}$ outcomes, with randomness measured by conditional entropy.
    \item From the information-theoretical perspective, since $\bar{X}$ contains the most information among the admissible, it contains the most mutual information to $Y$ compared to other admissible.
    \item The conditional probability $\mathbbm{P}(Y|\Bar{X})$ provides the greatest certainty (or least randomness) relative to $\mathbbm{P}(Y)$, with the reduction in randomness measured by KL-divergence.
    \item Assuming sufficiently regularized sensitive distributions with a density function, $\Bar{X}$ achieves higher or equal energy (or $L^2$-norm) in $\mathbbm{P}(Y|\Bar{X})$ for any random variable $Y$ and any event generated by $Y$, compared to other admissible $\hat{Y}$ outcomes.
\end{itemize}

\end{document}

