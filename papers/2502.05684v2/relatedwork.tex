\section{Related Work}
\label{s:related_work}

The existing body of work on unlearning can be separated into feature unlearning and machine unlearning:

\subsubsection{Machine Unlearning}

Machine Unlearning focuses on removing data points' influence during training on the model and thereby the learning outcome, mainly to comply with regulations such as GDPR’s “right to be forgotten.” The current study can be separated into exact unlearning and approximate unlearning:

\textit{Exact unlearning} \cite{bourtoule2021machine, brophy2021machine, yan2022arcane, graves2021amnesiac, schelter2021hedgecut, ginart2019making, chen2022graph, chen2022recommendation} approaches ensure the removal of data influence from the model by separating the model into sub-models (via subsets of training data in SISA training, parameters, or training steps in amnesiac unlearning) which allow fast retraining by only retraining the affected sub-models or training steps. While effective, these methods are computationally expensive, particularly in scenarios with multiple data removal requests. 

\textit{Approximate unlearning} methods try to keep the probabilistic similarity quantification between the unlearning outcome and the retraining outcome (also known as data point influence quantification). The main current approaches include: (1) using the influence function \cite{weisberg1982residuals} to estimate the influence of the data point and thus remove its effect on the model \cite{guo2020certified, sekhari2021remember, suriyakumar2022algorithms, mehta2022deep, wu2022puma, tanno2022repairing, warnecke2021machine}; (2) using the scrubbing function to enforce similarity between the network weights and the retraining weights \cite{golatkar2020eternal, golatkar2020forgetting}.

While these methods are effective in approximating the retrained model, each has practical limitation(s), such as significant performance degradation, high computational cost, limited compatibility with learning objectives, or restricted evaluation capability on simple datasets \cite{nguyen2022survey, wang2024machine}. More importantly, due to the aim to estimate the unknown (due to the complexity of algorithms, objective functions, and data influence) retraining outcome, the provable unlearning guarantees rely heavily on impractical assumptions such as convexity of the objective function and Lipschitz condition on Hessian matrices. \cite{xu2024machine}

In comparison, the proposed method can be considered as using mutual information to quantify the marginal effect of adding or removing the data point to unlearn in training, thereby more effectively removing the influence of the data to unlearn by diminishing the marginal effect. Furthermore, since the proposed unlearning definition (Definition \ref{d:epsilon_DU}) does not explicitly depend on the unknown retraining outcome, its provable guarantees avoid strong assumptions and become more practical to achieve. Finally, we establish a connection to retraining by demonstrating that Definition \ref{d:epsilon_DU} serves as a necessary condition for a “good” retrained model (Lemma \ref{l:DU_as_condition_for_good_model} and the discussion below).

Recent unlearning methods have leveraged the information bottleneck \cite{li2021unlearning, han2024unlearning} to avoid utility degradation by addressing both the utility and influence function. While at first glance these information bottleneck-based methods may seem similar to our proposed method, there is a fundamental difference. In particular, these information bottleneck methods aim to unlearn the information of that data point itself, such as minimizing the mutual information between the remaining target variable and the model parameter \cite{han2024unlearning} or the mutual information between the data to unlearn and the unlearning latent representations \cite{li2021unlearning}. In comparison, our regularization tries to help unlearn (or learn) the marginal effect of adding (removing) the data point.

\subsubsection{Feature Unlearning and Statistical Parity in Machine Learning Fairness}

Feature Unlearning and Statistical Parity in Machine Learning Fairness aim to remove the influence of a feature in the learning outcome~\cite{warnecke2021machine, han2024unlearning}. It is closely related to machine learning fairness when fairness is defined as statistical parity \cite{dwork2012fairness}. In particular, our framework when applied to feature unlearning is closely related to the Wasserstein barycenter characterization of both the optimal statistical parity learning outcome \cite{chzhen2020fair, gouic2020projection} and the optimally fair data representation \cite{xu2023fair}.

%\textit{Differential Privacy}: