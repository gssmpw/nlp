\subsection{Data Collection and Preprocessing}

% LawPal’s effectiveness relies on a structured data pipeline ensuring accurate, legally valid, and contextually appropriate responses. It collects legal texts from authoritative sources like government websites, Supreme Court archives, and legal journals, updating the dataset through automated web scraping. 

% The data is preprocessed with cleaning, OCR digitization, and text normalization before being segmented into chunks using LangChain’s Recursive Character Text Splitter. These chunks are converted into dense vector embeddings using DeepSeek’s model and indexed with FAISS for fast retrieval. Hierarchical indexing categorizes texts by legal domains like Criminal Law and Civil Law for improved precision.

% LawPal optimizes performance through caching of frequent queries, parallelized FAISS search, and continuous quality assurance to maintain dataset accuracy. This pipeline ensures efficient, semantically accurate, and contextually aware legal responses, enhancing access to legal knowledge in India.
 
Effective legal information retrieval relies on a well-structured and comprehensive dataset. For this study, data was collected from diverse sources, including publicly available legal repositories, court case archives, statutory databases, the Constitution, and academic legal literature. The dataset comprises structured and unstructured legal texts, including case summaries, judicial opinions, statutes, and legal articles. Documents were categorized based on jurisdiction, legal domain, and citation frequency to ensure a balanced and representative dataset. Additionally, API-based data retrieval and web scraping techniques incorporated recent legal developments, ensuring the system remains updated with evolving case law and statutory amendments.  

Preprocessing plays a crucial role in refining the raw text data for efficient retrieval and query processing. The initial phase involves tokenization, stopword removal, and stemming/lemmatization to normalize textual data while preserving key legal terminologies. Named Entity Recognition (NER) is utilized to extract critical legal entities such as case names, statutory references, and legal principles. To enhance retrieval efficiency, FAISS-based vector indexing is employed for semantic representation of legal texts. Additional steps, such as spell correction, deduplication, and noise filtering, further refine the dataset to improve accuracy and reduce redundancy. Moreover, semantic retrieval and retrieval-augmented generation (RAG) techniques are integrated to handle complex legal queries by leveraging both keyword-based and contextual search mechanisms. These preprocessing methodologies collectively enhance the system’s ability to deliver precise, contextually relevant legal information while maintaining computational efficiency.

\subsection{Model Building}

The model construction of LawPal follows a Retrieval-Augmented Generation (RAG) approach, integrating DeepSeek-R1:5B for embedding generation and response synthesis while using FAISS (Facebook AI Similarity Search) for efficient document retrieval. This architecture ensures that responses are legally accurate, contextually relevant, and computationally efficient by leveraging both semantic search and generative AI.  

The first step in the process is embedding generation, where legal texts are converted into high-dimensional vector representations. Given an input text segment \( T \), the embedding vector \( E_T \) is computed as follows:  

\[
E_T = f(T)
\]

where \( f \) is the DeepSeek embedding function that encodes text into a 1,024-dimensional vector. These embeddings enable semantic similarity searches rather than simple keyword matching, improving retrieval accuracy.  

When a user submits a query \( Q \), it undergoes a similar transformation into an embedding \( E_Q \):  

\[
E_Q = f(Q)
\]

To retrieve the most relevant legal information, FAISS performs a vector similarity search between \( E_Q \) and stored legal document embeddings \( E_T \), using cosine similarity as the distance metric:  

\[
S(E_Q, E_T) = \frac{E_Q \cdot E_T}{||E_Q|| ||E_T||}
\]

FAISS identifies the top-\( k \) most relevant text chunks \( R = \{T_1, T_2, ..., T_k\} \), ensuring that retrieved documents are contextually relevant to the legal query.  

In the response generation phase, the retrieved legal context \( R \) is concatenated with the user query \( Q \) and passed to the DeepSeek-R1:5B model to generate a legally coherent answer \( A \):  

\[
A = G(Q, R)
\]

where \( G \) represents the DeepSeek generative model, fine-tuned to legal domain knowledge. The model is prompt-engineered to maintain factual accuracy, ensuring that responses are aligned with constitutional laws, statutory provisions, and legal precedents.  

By leveraging DeepSeek embeddings, FAISS retrieval, and generative AI, LawPal efficiently bridges the gap between complex legal texts and user-friendly explanations. This hybrid system allows for rapid legal consultation while ensuring responses remain grounded in authoritative legal sources.

\subsection{Framework}\label{AA} 

The framework of LawPal is designed as a modular Retrieval-Augmented Generation (RAG) system, integrating DeepSeek-R1:5B for language understanding, FAISS for efficient vector-based retrieval, and Streamlit for an interactive user interface. This structured pipeline ensures seamless data processing, retrieval, and response generation, making legal assistance accessible and accurate.  

The data ingestion module collects legal documents from sources such as statutory laws, Supreme Court judgments, government legal databases, and research papers. These documents undergo preprocessing, including cleaning, OCR correction, and chunking, to ensure high-quality retrieval. The text is segmented into 500–750 character chunks with an overlap of 50–100 characters to maintain contextual integrity.  

In the embedding module, these text chunks are converted into 1,024-dimensional vector embeddings using DeepSeek-R1:5B, which captures the semantic relationships between legal texts. The generated embeddings are stored in FAISS (Facebook AI Similarity Search), where they are indexed for efficient similarity search. The FAISS index is structured hierarchically, grouping legal topics into categories such as criminal law, contract law, and constitutional law, allowing for more domain-specific retrieval.  

When a user submits a legal query, it is converted into an embedding and compared against stored vectors using cosine similarity to retrieve the most relevant legal text chunks. These top-k retrieved chunks are then passed to the DeepSeek-R1:5B model, which generates a coherent, legally sound response based on the extracted context.  

Finally, the Streamlit-based UI presents the response in a structured format, offering an intuitive interface where users can input legal queries, receive instant answers, and access relevant legal documents. This integrated RAG framework allows LawPal to provide fast, accurate, and well-referenced legal assistance, ensuring reliability and accessibility for users seeking legal guidance.