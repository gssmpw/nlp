% !TEX root = main.tex
Writing comments and keeping them up to date during software maintenance and evolution is a challenging task and requires effort \cite{fluri2007code,fluri2009analyzing,linares2015developers,wen2019large}--yet, 
high-quality comments and documentation are essential for understanding code \cite{rani2023decade,de2005study}. Thus, automatically generating code comments from code has always been a long-lasting dream of developers and practitioners willing to bolster program comprehension in a cost-effective way \cite{haque2020improved,hu2020deep,leclair2019neural,zhang2020retrieval,mastropaolo2021studying,gao2023code}. A dream that came reality, thanks to the recent advancements in Deep Learning (DL) models and particularly Large Language Models (LLMs) that pushed the boundaries of Software Engineering (SE) automation to the next level.

Modern DL-based approaches based on Transformers \cite{vaswani2017attention,devlin2018bert,raffel2020exploring} rely on transfer learning. First, a basic model is pre-trained to acquire knowledge of the programming language. Then, several specialized models can be fine-tuned from it to tackle specific tasks.
This approach has been proven effective for several SE tasks, including code generation \cite{wei2019code,svyatkovskiy2020intellicode,liu2024your,ugare2024improving}, program repair \cite{jin2023inferfix,chen2022neural,chen2019sequencer,tufano2018empirical}, and, indeed, code summarization \cite{hu2018summarizing,hu2018deep,zhang2020retrieval,gao2023code,leclair2020improved,mastropaolo2021studying}.

Fine-tuning a DL model still requires plenty of examples, which can hardly be produced or curated manually. Thus, most state-of-the-art datasets for training DL models for coding tasks (including code summarization) are built by automatically mining open-source software repositories. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{low-side.pdf}
	\caption{Example of a $\langle code, summary \rangle$ pair exhibiting a misalignment after the coarse-grained filtering by CAT.}
	\label{fig:intro-example}
\end{figure}

However, datasets created by mining software repositories tend to be noisy, containing several low-quality instances \cite{BirdBADBFD09,HerzigJZ13,BachmannBRDB10}. More specifically, Shi \etal \cite{shi2022we} recently proposed a heuristic-based approach named CAT that can automatically clean up low-quality $\langle summary, code \rangle$ pairs from code summarization datasets. This approach removes or fixes \textit{structural} quality issues at comment- (\eg commented code) and code-level (\eg empty methods). The results of their empirical study show that removing noisy instances does not reduce and even improves the model's ability to produce meaningful code summaries. 
Even though CAT streamlines coarse-grained filtering, the $\langle code, summary \rangle$ pairs that are not discarded can still have low quality and thus negatively impact the ability of the model to produce good code summaries. 
Let us consider the instance in \figref{fig:intro-example} from the state-of-the-art \textit{Funcom} dataset. While no structural problem exists (CAT does not discard it), the instance is characterized by a summary completely incoherent with the source code. Exposing the model to instances with such inconsistencies can cause hallucinations \cite{dziri2022origin}, which may lead to decreased performance.

For this reason, we conjecture that a finer-grained selection of $\langle code, summary \rangle$ pairs in code summarization datasets is not only possible, but even desirable to (i) reduce the training time, and (ii) possibly improve the model effectiveness. More specifically, we conjecture that code-comment coherence, which is a well-known quality attribute of $\langle code, summary \rangle$ pairs \cite{corazza2015coherence}, might play a crucial role in achieving this goal.

In this paper, we present an empirical investigation in which we study the impact of fine-grained filtering based on code-comment coherence on code summarization models in terms of \textit{effectiveness} (\ie correctness of the inferences) and \textit{efficiency} (\ie training time). 
To measure the code-comment coherence, we rely on SIDE, a metric recently introduced by Mastropaolo \etal \cite{mastropaolo2024evaluating}. The authors show that SIDE strongly correlates with human evaluations of summary quality, surpassing established metrics including BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge}.
We consider two state-of-the-art datasets for code summarization, \ie TL-CodeSum \cite{hu2018summarizing} and Funcom \cite{leclair2019neural}, already filtered with CAT \cite{shi2022we}. Then, we further filter the instances in terms of SIDE value by considering different thresholds (\ie $\{0.5, 0.6, 0.7, 0.8, 0.9\}$). Finally, we use each resulting training set (including the original one) to fine-tune \emph{CodeT5+} \cite{wang2023codet5+}.

We test the models on two manually-curated datasets from the literature \cite{yu2024codereval, mastropaolo2023robustness}. We observe that reducing the size of the training set, even with the most restrictive filter (SIDE$_{0.9}$, which selects $\sim$50\% of the instances), has a negligible impact on the model's effectiveness. On the other hand, reducing the number of training instances results in a significantly lower training time (up to $\sim$111 saved hours).
To further validate our original hypothesis that code-comment coherence is a suitable quality attribute for filtering instances in code summarization datasets, we compared the most restrictive filter (SIDE$_{0.9}$) with a filter that keeps the same number of instances, but by simply choosing them randomly. Surprisingly, we observed that the random filter achieves negligibly worse results than SIDE$_{0.9}$.

Our results provide two clear insights. First, code-comment coherence is marginally important for selecting suitable instances for code summarization. Second, regardless of this, removing instances (even randomly!) does not impact the effectiveness of code summarization models.
This suggests that additional code-comment quality attributes should be investigated, and researchers should prioritize relevance over quantity to better select the most informative instances to build code summarization datasets.

The paper is organized as follows. Section \ref{sec:selection_strategies} provides backgrounds on the existing selection technique for code summarization (CAT \cite{shi2022we}) and overviews SIDE \cite{mastropaolo2024evaluating}. Section \ref{sec:study} details the study definition and planning. Results are reported in Section \ref{sec:result}, while Section \ref{sec:implication} discusses its implications, and Section \ref{sec:threats} the threats to its validity. Section \ref{sec:related} discusses related work about data quality. Finally, Section \ref{sec:conclusion} concludes the paper and outlines directions for future work.
