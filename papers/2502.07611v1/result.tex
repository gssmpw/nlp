In this section, we present the outcomes of our study, addressing the research questions formulated in Section \ref{sec:study}.

\subsection{RQ$_{0}$ Consistency Analysis}
\label{rq0}
\figref{fig:rq0_training_distribution} reports the SIDE score distributions for the training instances of the examined datasets. We can observe that most of the instances of the two datasets exhibit high-quality features, according to SIDE. In detail, the mean SIDE scores for the training sets are 0.83 and 0.81 for \textit{TL-CodeSum} and \textit{Funcom}, respectively. This result denotes a high coherence in the $\langle code, summary \rangle$ pairs. Also, we can notice that, for more than 90\% of the instances (93\% for \textit{TL-CodeSum} and 91\% for \textit{Funcom}), the SIDE score is greater than 0.5. While such a percentage is quite high, it is worth noting that, as a consequence, a large number of instances (3,525 and 103,789 for \textit{TL-CodeSum} and \textit{Funcom}, respectively) exhibit a SIDE score lower than 0.5. Besides, 51\% and 54\% of the instances from \textit{TL-CodeSum} and \textit{Funcom}, respectively, have a SIDE score lower than 0.9. While many of such instances are not necessarily detrimental to the model, they might not be beneficial either. Such distributions show a sufficiently high margin of improvement for both datasets.

\begin{tcolorbox}[
	colframe=black!100!white, colback=black!10!white,
	coltitle=white, colbacktitle=black!,
	title=Answer to RQ$_{0}$,
	fonttitle=\bfseries,
	rounded corners,
	boxrule=0.5mm,
	width=\linewidth,
	breakable
	]
	
	The training sets from both \textit{TL-CodeSum} and \textit{Funcom} exhibit a high SIDE score, on average (> 0.8 for both). Still, over half the instances have a sub-optimal SIDE score (< 0.9). This distribution suggests that a dataset reduction approach could  improve the performance of an automated code summarization model trained on it.
\end{tcolorbox}


\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{pareto.pdf}
	\caption{Pareto front for TL-CodeSum \cite{hu2018summarizing} (\textbf{top}) and Funcom \cite{leclair2019recommendations} (\textbf{bottom}). From left to right, the chart shows the number of tokens for $SIDE_{0.9}$, $SIDE_{0.8}$, $SIDE_{0.7}$, $SIDE_{0.6}$, $SIDE_{0.5}$, and the full dataset.}
	\label{fig:pareto}
\end{figure*}

\subsection{RQ$_{1}$. Selection Strategy Impact}
\label{rq1}
\tabref{tab:performance_metrics_codereval} reports the results obtained from the different models fine-tuned on the full and filtered training sets, while the detailed results of the Wilcoxon signed-rank tests are in our replication package for space reasons \cite{replicationpackage}. The column ``Dataset'' indicates the training set from which the selections were performed, while the ``Selection'' column reports the SIDE threshold used to filter the training sets \ie \textit{full}, and \side{0.5} up to \side{0.9}. For example, the first line in correspondence of \textit{TL-CodeSum} (\ie Full) represents the model fine-tuned on the full training set of \textit{TL-CodeSum}, while the one below \ie (\side{0.5}) represents the model fine-tuned on the training set filtered with SIDE score greater than 0.5.
Furthermore, we report the number of tokens seen during the fine-tuning process (``Tokens'' column) and the percentage of the saved training instances with respect to the complete training sets (``Saving'' column).
Finally, the columns ``BLEU-4'', ``METEOR'', and ``ROUGE'' report the performance in terms of percentage for the golden sets including \textit{CoderEval-Original}, \textit{CoderEval-Human}, and the one by Mastropaolo \etal~\cite{mastropaolo2023robustness}.


In the following, we discuss results achieved through the SIDE-based selection by comparing them against the \textit{CoderEval-Original} ground truth.
Looking at the rows corresponding to \textit{TL-CodeSum}, we observe that the model trained with the complete training set achieves 11.72\% BLEU-4, 17.84\% METEOR, and 35.01\% ROUGE-L. Surprisingly, even though such a model underwent fine-tuning with the full training set, it is not the one exhibiting the best results in terms of these metrics. Instead, the model fine-tuned with the selection strategy with \side{0.6} is the one that achieves the best results for all metrics despite being trained with $\sim$1M fewer tokens during training than the one leveraging the whole corpus of code tokens. If we look at the ``minimal'' selection strategy \ie \side{0.9}, we observe a limited drop in performance ($\downarrow$ 0.12, $\downarrow$ 0.77, and $\downarrow$ 1.34) while saving the 51\% of the training instances.

Looking at the rows corresponding to \textit{Funcom}, we observe the same trend as for \textit{TL-CodeSum}. The model fine-tuned on the whole training set (108.7M tokens) performs worse than the models fine-tuned with less training data. We can notice a negligible drop in performance for \side{0.9} (49.0M tokens) on BLEU-4 metric ($\downarrow$ 0.60), while achieving slightly better performance on METEOR ($\uparrow$ 0.47) and ROUGE-L ($\uparrow$ 0.59).

The Wilcoxon signed-rank tests indicate that there is no statistically significant difference between the results obtained by the model fine-tuned on the full training set and those fine-tuned on the filtered training sets (minimum \textit{p}-value~$> 0.2$ for \textit{TL-CodeSum}, and \textit{p}-value~$> 0.08$ for \textit{Funcom}). Also, the Cliff's $\delta$ effect size is always negligible for the same comparisons. Such results occur despite the most aggressive selection only leveraging 46\% of the original training instances. In other words, there is little to lose in terms of summary quality when the model training set is heavily optimized.

Similar observations apply to the \textit{CoderEval-Human} dataset. With no statistically significant differences, (minimum \textit{p}-value $> 0.23$ for \textit{TL-CodeSum}, and \textit{p}-value $> 0.15$ for \textit{Funcom}), according to the Wilcoxon signed-rank test and a negligible Cliff's $\delta$ effect size, the models specialized to produce meaningful code summaries with the filtered dataset have comparable performance to those fine-tuned with the original dataset.

For example, the model instructed with ``high-quality'' examples included in the \side{0.9} dataset --- even if trained with 4.2M fewer tokens --- obtains a slightly higher BLEU-4 ($\uparrow$ 0.15) and a slightly lower METEOR ($\downarrow$ 0.09) and ROUGE-L ($\uparrow$ 0.02). We also observed no substantial differences in the context of \textit{Funcom}. In this case, however, the models fine-tuned with the filtered training sets perform slightly better than the model fine-tuned on the full-training set in terms of all the metrics ($\uparrow$ 0.85 BLEU-4, $\uparrow$ 0.66 METEOR, and $\uparrow$ 1.71 ROUGE-L).

Finally, the conclusions above (no statistically significant differences, negligible Cliff's $\delta$ effect sizes) are further confirmed on the dataset by Mastropaolo \etal \cite{mastropaolo2023robustness}.
The model that underwent fine-tuning with the full-sized \textit{TL-CodeSum} training set performs only slightly better than the models fine-tuned on the filtered datasets. At the same time, it is interesting to notice how---although differences are still not statistically significant---the model fine-tuned on the filtered \textit{Funcom} training set with \side{0.6} exhibits marginally better performance compared to the model that was exposed to the largest amount of code tokens, \ie the full-sized \textit{Funcom} dataset, during training. This suggests that there may be contexts for which the selection based on the SIDE metric helps to prune out instances worsening the summarization quality.

\figref{fig:pareto} depicts the Pareto fronts for the models' performance and training dataset size across the different selections of the \textit{TL-CodeSum} and \textit{Funcom} datasets (\ie \textit{Full}, SIDE$_{0.5}$, SIDE$_{0.6}$, SIDE$_{0.7}$, SIDE$_{0.8}$, SIDE$_{0.9}$), evaluated on the three benchmarks \ie \textit{CoderEval-Original}, \textit{CoderEval-Human}, and Mastropaolo \etal.

For the models trained on the \textit{TL-CodeSum} selections, we observe that as the training dataset size increases from SIDE$_{0.9}$ with 3.7M tokens to SIDE$_{0.7}$ with 6.6M tokens, the ROUGE scores remain almost constant. A slight improvement in ROUGE scores can be observed at 7.1M tokens (SIDE$_{0.6}$) on \textit{CoderEval-Original} and \textit{CoderEval-Human}. However, at larger sizes with 7.4M tokens (SIDE$_{0.5}$) and the 7.9M tokens (\textit{Full}) the scores decline. At the same time, the METEOR and BLEU-4 scores remain almost constant across all dataset sizes, showing minimal improvements. In the Mastropaolo \etal dataset, the three metrics remain constant across all filtering levels, evidencing that increasing dataset size does not meaningfully impact the models' effectiveness.

The models trained on \textit{Funcom} selections exhibit similar results. As the dataset size decreases from the \textit{Full} set of 108.7M tokens through SIDE thresholds down to SIDE$_{0.9}$ with 49.0M, the ROUGE scores exhibit slight improvement up to SIDE$_{0.6}$ but remain relatively stable across the golden sets, while METEOR and BLEU-4 scores follow the trend of the ROUGE scores although in a less noticeable way. For \textit{CoderEval Human}, ROUGE scores exhibit minor improvements, while METEOR and BLEU-4 scores show no significant changes. For the Mastropaolo \etal dataset, the effectiveness remains stable across the three metrics and training sizes, further underscoring that increasing the training dataset size has a small impact on performance for high-quality test sets.

Although we observe slight improvements with larger dataset selections (specifically at SIDE$_{0.6}$ and SIDE$_{0.7}$), the gains are minimal, resulting in a generally flat trend on the Pareto front. Finally, as shown in \figref{fig:pareto}, increasing the training dataset size does not lead to substantial improvements in downstream performance.
Based on our results, we can state that increasing dataset size provides negligible improvements, with SIDE$_{0.9}$ achieving an optimal balance between performance and resource efficiency.

\begin{tcolorbox}[
	colframe=black!100!white, colback=black!10!white,
	coltitle=white, colbacktitle=black!,
	title=Answer to RQ$_{1}$,
	fonttitle=\bfseries,
	rounded corners,
	boxrule=0.5mm,
	width=\linewidth,
	breakable
	]
	
	Fine-tuning neural code summarization models with coherent code-comment instances selected through SIDE leads to performances comparable to those obtained when fine-tuning the model on the complete training sets, with a reduction of up to 50\% of the training instances.
\end{tcolorbox}


\begin{table*}[t]
	\centering
	\caption{Performance metrics on Top-1 predictions for \side{0.9} and \textit{Random}.}
	\label{tab:performance_metrics_codereval_comp}
	\resizebox{0.8\linewidth}{!}{%
		\begin{tabular}{c|l|rrr|rrr|rrr}
			\toprule
			\rowcolor{black}
			&  & \multicolumn{3}{c}{\textcolor{white}{\textbf{CoderEval-Original \cite{yu2024codereval}}}} & \multicolumn{3}{c}{\textcolor{white}{\textbf{CoderEval-Human \cite{yu2024codereval}}}} & \multicolumn{3}{c}{\textcolor{white}{\textbf{Mastropaolo \etal \cite{mastropaolo2023robustness}}}} \\
			\rowcolor{gray!20}
			\textbf{Dataset} & \textbf{Selection} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE} \\
			\midrule
			\multirow{2}{*}{\textit{TL-CodeSum \cite{hu2018summarizing}}}
			& \side{0.9} & 11.60 & 17.07 & 33.67 & 6.56 & 14.19 & 30.53 & 5.62 & 12.75 & 27.26   \\
% 			\cmidrule(r){2-11}
			& \emph{Random} & 10.77 & 16.36 & 32.67 & 6.12 & 13.44 & 28.78 & 5.85 & 12.89 & 27.13   \\
			\midrule
			\multirow{2}{*}{\textit{Funcom \cite{leclair2019neural}}}
			& \side{0.9} & 13.65 & 18.08 & 37.12 & 6.78 & 13.61 & 30.07 & 6.81 & 12.95 & 28.07 \\
			& \emph{Random} & 13.67 & 17.95 & 36.09 & 6.88 & 13.95 & 29.90 & 5.84 & 12.32 & 27.12 \\
			\bottomrule
	\end{tabular}}
\end{table*}

\begin{table}[h!]
	\centering
	\caption{Results Comparison for codesum and funcom}
	\label{tab:tests_comp}
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{l|l|r|r|r|r|r|r}
			\toprule
			\rowcolor{black}
			 & & \multicolumn{2}{c}{\textcolor{white}{\textbf{CoderEval-Original \cite{yu2024codereval}}}} & \multicolumn{2}{c}{\textcolor{white}{\textbf{CoderEval-Human \cite{yu2024codereval}}}} & \multicolumn{2}{c}{\textcolor{white}{\textbf{Mastropaolo \etal  \cite{mastropaolo2023robustness}}}} \\
			\rowcolor{gray!20}
			\textbf{Dataset} & \textbf{Metric} & \textbf{$\boldsymbol{p}$-value} & \textbf{Cliff $\boldsymbol{\delta}$} & \textbf{$\boldsymbol{p}$-value} & \textbf{Cliff $\boldsymbol{\delta}$} & \textbf{$\boldsymbol{p}$-value} & \textbf{Cliff $\boldsymbol{\delta}$} \\
			\midrule
			\multirow{3}{*}{\textit{TL-CodeSum \cite{hu2018summarizing}}} & BLEU-4  & 1.0 &   0.03 & 0.5 &   0.03 &   1.0 &   0.02 \\
																		  & METEOR  & 1.0 &   0.02 & 0.5 &  0.014 &   0.4 & -0.013 \\
																		  & ROUGE-L & 1.0 &   0.03 & 0.5 &  0.052 &   1.0 &   0.01 \\
			\midrule
			\multirow{3}{*}{\textit{Funcom \cite{leclair2019neural}}}     & BLEU-4  & 1.0 & -0.004 & 1.0 & -0.021 & 0.017 &  0.042 \\
																	      & METEOR  & 1.0 &   0.01 & 1.0 &  0.014 &  0.11 &  0.025 \\
																	      & ROUGE-L & 1.0 &   0.03 & 1.0 &  0.013 &  0.07 &  0.021 \\
			\bottomrule
		\end{tabular}
	}
\end{table}


\subsection{RQ$_2$. Comparison with the Random Baseline}
\label{rq2}
\tabref{tab:performance_metrics_codereval_comp} reports the performance achieved by the models trained on the \side{0.9} and \textit{Random} selections for both datasets (\ie \textit{TL-CodeSum} and \textit{Funcom}). We report such information as described in \secref{rq1}. Instead, \tabref{tab:tests_comp} reports the results of the Wilcoxon signed-rank tests with the adjusted $p$-values (column \textit{$p$-value}), and the corresponding Cliff's $\delta$ effect sizes (column Cliff's $\delta$) for each golden set, including \textit{CoderEval-Original}, \textit{CoderEval-Human}, and Mastropaolo \etal

The results are quite comparable for the models trained on the \textit{TL-CodeSum} dataset selections. In detail, on \textit{CoderEval-Original}, the model trained on the \side{0.9} selection achieves slightly better results ($\uparrow$ 0.83, $\uparrow$ 0.71, $\uparrow$ 1.0), even though without statistically significant statistical differences ($p$-value = 1.0), and negligible effect sizes for all metrics ($\sim$ 0.00). We obtain consistent results for \textit{CoderEval-Human}. Instead, we observe slight differences in performance for the Mastropaolo \etal golden set. Specifically, the \textit{Random} selection allows to achieve better BLEU-4 ($\uparrow$ 0.23) and METEOR ($\uparrow$ 0.14), but lower ROUGE ($\downarrow$ 0.13). Again, we observe no statistically significant differences and negligible effect sizes.

What was observed above is also generally true for the models trained on the \textit{Funcom}. The model trained on the \side{0.9} selection, achieves better ROUGE ($\uparrow$ 1.03, $\uparrow$ 0.17, $\uparrow$ 0.95) on all the three golden sets. Conversely, for the BLEU and METEOR metrics, the models show alternate better performance, except on the Mastropaolo \etal golden set. \tabref{tab:tests_comp} reports a statistically significant difference between the performance of the two models for the BLEU-4 metric, although with negligible effect sizes.

\begin{tcolorbox}[
	colframe=black!100!white, colback=black!10!white,
	coltitle=white, colbacktitle=black!,
	title=Answer to RQ$_{2}$,
	fonttitle=\bfseries,
	rounded corners,
	boxrule=0.5mm,
	width=\linewidth,
	breakable
	]
	Filtering training instances based on code-comment coherence provides models with comparable effectiveness to those trained on randomly selected instances.
\end{tcolorbox}
