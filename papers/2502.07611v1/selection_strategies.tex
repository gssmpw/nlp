% !TEX root = main.tex
In this section, we provide backgrounds about (i) a state-of-the-art strategy for repairing or removing poor instances from code summarization datasets (CAT), and (ii) the SIDE metric, which we use to streamline a data-centric, quality-aware instance filtering.

\subsection{Code-comment cleAning Tool}
CAT (\textbf{C}ode-comment cle\textbf{A}ning \textbf{T}ool) is an approach and tool by Shi \etal \cite{shi2022we} to detect and handle noisy instances given the pairs of \textit{<code, summary>} from code-summarization datasets. The development of CAT was preceded by a manual investigation involving 9 participants who examined 1,600 \textit{<code, summary>} pairs. This manual analysis aimed to define a taxonomy of noisy data categories.

The taxonomy features comments- and code-related noisy data, such as \textit{commented-out method} or \textit{empty function}. Based on such categories, Shi \etal defined a set of heuristic rules and implemented them in the CAT tool. CAT works with two possible strategies based on the issues found: On the one hand, it fixes instances with minor issues. For example, it removes block-level comments. On the other hand, it completely drops instances where no fix is possible, including, for example, getters and setters.

\subsection{Fine-Grained Filtering: SIDE}
\label{sec:side-aware}
SIDE (\textbf{S}ummary al\textbf{I}gnment to co\textbf{D}e s\textbf{E}mantics) is a novel quality-aware metric presented by Mastropaolo \etal \cite{mastropaolo2024evaluating}. SIDE addresses the shortcomings of traditional metrics such as BLEU \cite{papineni2002bleu}, ROUGE \cite{lin:tsbo2004}, and METEOR \cite{banerjee:acl2005} used in code summarization tasks. 

SIDE employs a contrastive learning approach to determine the accuracy with which a code summary documents the underlying code, explicitly focusing on Java methods. Contrastive learning aims to maximize the distance between the reference code and inappropriate comments while minimizing the distance to suitable comments. The model that implements SIDE, MPNet \cite{Song2020MPNetMA}, provides a continuous score ranging from -1 to 1. Scores closer to~-1 indicate poor alignment between the code summary and the actual code, whereas scores closer to~1 suggest a strong alignment. SIDE showed a high correlation with human evaluations of summary quality, outperforming established metrics like BLEU, ROUGE, and METEOR. 

The benefits of a quality-aware metric like SIDE extend beyond evaluating code summarization techniques, as Mastropaolo \etal \cite{mastropaolo2024evaluating} noted. SIDE can be valuable when distinguishing high-quality code documentation from subpar examples is essential. We conjecture that SIDE ensures that only the instances most likely to enhance the training procedure are used, enabling the model to converge faster without a drop in performance by filtering out low-quality elements.

Our study relies on SIDE to select $\langle code, summary \rangle$ pairs with high coherence. Specifically, given a training set $T$ and a threshold $t$, we select the training instances $\{ p_{i} \in T \mid \text{SIDE}(p_{i}) \ge t \}$. 
