% !TEX root = main.tex

We presented an empirical investigation in which we studied how filtering out incoherent code-comment pairs in code summarization datasets affects the DL models trained to tackle this task in terms of effectiveness and removed training instances. 

On the one hand, our results show that reducing the number of instances in the training set using a coherence-based approach driven by the SIDE metric \cite{mastropaolo2024evaluating} does not significantly impact the models' effectiveness, even when applying the strictest filter that reduces the training set of over 50\%. On the other hand, we found that randomly selecting the same number of instances instead of selecting them based on their code-comment coherence allows us to achieve comparable results. 

Our future research agenda includes investigating other quality aspects for selecting training data, such as instance diversity and readability. Furthermore, we plan to investigate how far we can go in randomly removing instances without affecting the quality of generated code summaries.
