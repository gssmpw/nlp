% !TEX root = main.tex
The \emph{goal} of this study is to empirically evaluate how code-comment coherence, through a quality-aware selection strategy grounded on SIDE, impacts the effectiveness and training efficiency of neural code summarization models.

More specifically, the study aims to address the following research questions:

\begin{itemize}[itemindent=0.25cm]
	\item[\textbf{RQ$_{0}$}:] \textit{How do code summarization datasets measure up in terms of code-comment coherence?}
	In this preliminary question, we assess the coherence of code-comment pairs of datasets commonly used in code summarization. As we aim to use a coherence-aware strategy to optimize training sets, first of all, we would like to see how the coherence is distributed.
	\item[\textbf{RQ$_{1}$}:] \textit{How does a coherence-aware strategy selection impact the performance of neural code summarization models?}
	In this research question, we investigate how a targeted selection of training data based on code-comment coherence impacts the performance of neural code summarization models.
	\item[\textbf{RQ$_{2}$}:] \textit{How does the coherence-aware strategy selection compare with a random baseline?}
	In this research question, we test our hypothesis that code-comment coherence is a quality attribute that can be used to select training instances.
\end{itemize}

\subsection{Context Selection}
\label{subsec:context_selection}
The \emph{context} of our study consists of datasets containing pairs of \java methods with the associated summaries. 
For fine-tuning the models, we consider the two most important datasets from the state of the art: TL-CodeSum \cite{hu2018summarizing}, and Funcom \cite{leclair2019neural}.

The \textit{TL-CodeSum} dataset \cite{hu2018summarizing} is specifically designed for the code summarization task. It consists of $\sim$87k instances $ \langle code, summary \rangle$ extracted from GitHub repositories created from 2015 to 2016, and having at least 20 stars. In detail, Hu \etal \cite{hu2018summarizing} extracted the first sentence---likely to describe the overall method functionality---from the doc of each pair.

Similarly to TL-CodeSum, the \textit{Funcom} dataset \cite{leclair2019neural} is also specifically designed for code summarization. \textit{Funcom} consists of over 2.1M $ \langle code, summary \rangle$ pairs collected from the Sourcerer repository. As for TL-CodeSum, LeClair \etal \cite{leclair2019neural} only consider methods with their javadoc, extracting the first sentence as corresponding \textit{summary}.

Shi \etal \cite{shi2022we} found many noisy instances and duplicates in the above-described datasets and cleaned them up using their heuristic-based dataset-cleaning approach. For this reason, we use the cleaned versions of \textit{TL-CodeSum} and \textit{Funcom} provided by Shi \etal \cite{shi2022we}. The cleaned \textit{TL-CodeSum} contains 53,597 training instances, while the cleaned \textit{Funcom} contains 1,184,438 training instances.

The above datasets are built automatically, and no manual check was performed, \ie there is no guarantee of their quality. For this reason, we use two additional, manually curated datasets to test the models. The first one is \textit{CoderEval} \cite{yu2024codereval}, which consists of 230 Python and 230 \java code generation problems collected from open-source, high-starred projects which include \textit{original} and \textit{human-labeled} docstrings that should act as prompt for Code Generation models to generate the corresponding \textit{code}. 
The instances have been subject to manual screening, for which the main criterion is the probability of appearing in real-development scenarios. We focus on the \java set of problems, inverting the input and the output \ie from $\langle docstring, code \rangle$ to $\langle code, docstring \rangle$. 
To align the format of the pairs format, we performed an additional manual analysis in which one of the authors checked all the triplets with a second author to confirm the analysis. 
We found that some of the \textit{docstring}(s) contained more than a sentence. Therefore, to make them consistent with the previous dataset format (\eg single sentence), we extracted the first sentence from each \textit{docstring}. Still, we found 12 occurrences in which the corresponding \textit{original docstring} does not describe the \textit{code} (\eg ``\texttt{{@inheritDoc}}'', ``\texttt{@param modelName model name of the entity}'', and similar). We also excluded \textit{docstring}: ``\texttt{Computes floor(\$log\_2 (n)\$) \$+ 1\$.}'' since it includes a formula not explained in natural language.
Again, to appropriately align the evaluation, we do not evaluate such instances, ending up with 218 \textit{original} instances.

The second manually-curated dataset we use is the one by Mastropaolo \etal \cite{mastropaolo2023robustness}. The dataset consists of 892 methods associated with their summary (\ie first sentence of the method documentation), collected from non-fork GitHub \java repositories with at least 300 commits, 50 contributors, and 25 stars. 
Such instances are in the form $\langle summary, code\rangle$ and, as for CoderEval \cite{yu2024codereval}, we inverted the input and the output \ie $\langle code, summary\rangle$. Mastropaolo \etal  analyzed such pairs to ensure their quality. We manually analyzed and cleaned them further (\eg ``\texttt{Adds an {@link CarrierService} to the {@linkCarrier}}'' into ``\texttt{Adds an CarrierService to the Carrier}''), as we had done for CoderEval. No instances were removed during such a manual analysis.

We remove the instances from the test sets which appear in the training sets of \textit{TL-CodeSum} and \textit{Funcom}. As a result, we remove ten instances from CoderEval, which are present only in the \textit{TL-CodeSum} training set.

\subsection{Study Methodology}
\label{subsec:exp_proc}
To answer RQ$_{0}$, we use SIDE to compute the degree to which the summaries of the studied datasets document their corresponding code. We did this for each instance of the training sets included in \textit{TL-CodeSum} and \textit{Funcom}. To understand the coherence of the training sets, we analyze the average and the distributions of the SIDE scores of the instances.\\

\addtolength{\extrarowheight}{\belowrulesep}
\aboverulesep=0pt
\belowrulesep=0pt
\begin{table}[t]
	\centering
	\caption{Different selections for \textit{TL-CodeSum} and \textit{Funcom} training sets.}
	\label{tab:dataset_w_strategies}
	\resizebox{0.6\columnwidth}{!}{%
		\begin{tabular}{lrrr}
			\toprule
			\cellcolor{black}\textcolor{white}{\textbf{Selection}} &  \cellcolor{black}\textcolor{white}{\textbf{TL-CodeSum}} & \cellcolor{black}\textcolor{white}{\textbf{Funcom}} \\
			\midrule
			Full & 53,597 & 1,184,438 \\
			\midrule
			\side{0.5} & 50,073 & 1,080,649 \\
			\side{0.6} & 48,146 & 1,031,647 \\
			\side{0.7} & 44,853 & 952,265 \\
			\side{0.8} & 38,733 & 813,998 \\
			\side{0.9} & 26,258 & 540,170 \\
			\bottomrule
	\end{tabular}}
\end{table}

To answer RQ$_{1}$, we use the SIDE-based filter we define in \secref{sec:selection_strategies}. We use five threshold values, \ie 0.5, 0.6, 0.7, 0.8, and 0.9. We do not use thresholds lower than 0.5 because they would result in negligible dataset reductions (lower than 10\% for both), as we will observe in the results of RQ$_{0}$.
We report information about the different datasets in \tabref{tab:dataset_w_strategies}. 
We apply each filter on the training sets of \textit{TL-CodeSum} and \textit{Funcom}. Such filtering leads to the definition of five new versions of both datasets.

We fine-tune a pre-trained Transformer-based model for each dataset version, \ie both the base one and its six filtered versions, producing 12 fine-tuned models.
We choose to leverage the pre-trained \emph{CodeT5+} \cite{wang2023codet5+} since it has been largely used for code-related tasks \cite{ahmed2024automatic,phan2024repohyper,yang2024important} and, more important, in the code summarization approaches described above. This model is built on the backbone of the well-known T5 model by Raffel \etal \cite{raffel2020exploring}, yet it benefits from specific enhancements tailored for code understanding and generation tasks. During the pre-training phase, \emph{CodeT5+} is first trained on unimodal data, which includes code and comments, employing a combination of pre-training objectives such as span-denoising \cite{raffel2020exploring} and Causal Language Modeling \cite{soltan2022alexatm,tay2022ul2}. Then, it is pre-trained on bi-modal data where pre-training objectives such as text-code contrastive learning, text-code matching, and text-code causal language modeling are employed. It comes with different variants: (i) \emph{CodeT5+} 220M, (ii) \emph{CodeT5+} 770M, (iii) \emph{CodeT5+} 2B, (iv) \emph{CodeT5+} 6B, and (vi) \emph{CodeT5+} 16B.
Since our experimental design would require training, validating and testing 12 models, we decided to fine-tune the \emph{CodeT5+} variant featuring 220M trainable parameters.
This choice aligns with the goal of our investigation: Rather than proposing a new code summarization technique, we aim to use a model that offers a favorable balance between size and training time while still allowing us to observe the relevant phenomenon (if present).

Considering the extensive array of our experiments, we fine-tune for 20 epochs using a batch size of 16. Additionally, we restrict the input length to 512 tokens and the output to 128 tokens, consistent with previous studies leveraging the two datasets we used \cite{mastropaolo2022using,zhou2022automatic,tufano2023automating}. In addition, we conduct the fine-tuning using the standard hyperparameters for \emph{CodeT5+}, which include the AdamW optimizer \cite{loshchilov2017decoupled} and a learning rate of 2e-5, which is the one recommended for (Code)T5 and also used in works leveraging such models \cite{mastropaolo2023towards,ciniselli2024generalizability,mastropaolo2024vul}.

To prevent overfitting, we employ early stopping \cite{prechelt2002early}. After each epoch, we assess the performance of the models by computing the number of correct predictions on the validation set. 
In line with similar research \cite{mastropaolo2023towards,ciniselli2024generalizability}, we implement early stopping with patience of 5 epochs and a delta of 0.01. This means that training will stop if the model's performance does not improve by at least 0.01 for five consecutive epochs. We then select the best-performing checkpoint before early stopping.
We fine-tune a \emph{CodeT5+} model for each training set derived from the selection strategy \ie 12 (2 datasets $\times$ six variants).

After training the models, we assess their performance on the test set dataset that, as previously explained, are the \textit{CoderEval} \cite{yu2024codereval}, and the one from Mastropaolo \etal \cite{mastropaolo2023robustness} which we refer to as the \textit{golden sets}.

In the inference phase, we employ a beam search decoding strategy. In detail, with $k \in \{1, 3, 5\}$, we allow each model to generate the $k$ most probable candidate \textit{summaries} for the given \textit{code}.
To evaluate the generated summaries of each model, we compute the following metrics: BLEU \cite{papineni2002bleu}, METEOR \cite{banerjee:acl2005}, and ROUGE-L \cite{lin2004rouge}.
\textbf{BLEU} is a metric that expresses, within a range from 0 to 1, the similarity between a generated text (candidate) and the target one (oracle). It computes the percentage of $n$-grams of the generated text that appear in the target, where $n \in \{1, 2, 3, 4\}$. 
\textbf{METEOR} is computed as the harmonic mean of unigram precision and recall, with the latter weighted higher than the former. It ranges from 0 to 1. 
\textbf{ROUGE-L} is computed as the length of the longest common subsequence (LCS) between the generated text and the target one and measures the recall by considering the proportion of the LCS relative to the length of the target text.
We do not use SIDE \cite{mastropaolo2024evaluating} as it was employed for selecting training instances and could therefore be unnaturally biased in favor of models trained on filtered datasets.
Also, we do not compute the percentage of exact matches for three reasons. First, exact matches might underestimate the actual performances of the model. Indeed, an exact match implies a correct summary, but many alternative summaries might be as correct (or even more correct, in theory) as the ones in the ground truth for the very nature of this task. Second (also related to the previous point), \textit{CoderEval} \cite{yu2024codereval} provides two summaries for each coding instance, namely \textit{original} (\ie the docstring collected from the original source code), and \textit{human} (\ie the docstring written from scratch by developers during the benchmark creation \cite{yu2024codereval}). The model could have correctly generated only one of them, which are, by definition, both correct alternatives, thus leading to inconsistent results. Third, the dataset provided by Mastropaolo \etal~\cite{mastropaolo2023robustness} includes three different yet semantically equivalent code summaries for each \java method. As previously noted, each of these alternative descriptions is a valid candidate summary.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.65\linewidth]{distributions-plot.pdf}
	\caption{Distribution of SIDE scores for \textit{TL-CodeSum} and \textit{Funcom} training instances.}
	\label{fig:rq0_training_distribution}
\end{figure}

\begin{table*}[t]
	\centering
	\caption{Performance metrics on Top-1 predictions for CoderEval.}
	\label{tab:performance_metrics_codereval}
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{c|l|r|r|rrr|rrr|rrr}
			\toprule
			\rowcolor{black}
			&  &  &  & \multicolumn{3}{c}{\textcolor{white}{\textbf{CoderEval-Original \cite{yu2024codereval}}}} & \multicolumn{3}{c}{\textcolor{white}{\textbf{CoderEval-Human \cite{yu2024codereval}}}} & \multicolumn{3}{c}{\textcolor{white}{\textbf{Mastropaolo \etal \cite{mastropaolo2023robustness}}}} \\
			\rowcolor{gray!20}
			\textbf{Dataset} & \textbf{Selection} & \textbf{\#Tokens} & \textbf{(\%) Saving} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE} \\
			\midrule
			\multirow{6}{*}{\textit{TL-CodeSum \cite{hu2018summarizing}}}
			
			& \emph{Full}      & \cellcolor[HTML]{656565}\color[HTML]{FFFFFF} $\uparrow$ 7.9M &  \cellcolor[HTML]{a3070c}\color[HTML]{FFFFFF} --   & 11.72 & 17.84 & 35.01 & 6.41 & 14.28 & 30.51 & 6.37 & 13.04 & 27.09 \\
			\cmidrule(r){2-13}
			& \side{0.5} & 7.4M & 7\%  & 12.41 & 17.73 & 35.41 & 6.32 & 14.28 & 31.08 & 6.36 & 12.93 & 27.47\\
			& \side{0.6} & 7.1M & 10\% & 13.22 & 17.98 & 36.49 & 6.98 & 14.03 & 31.06 & 6.61 & 13.07 & 27.49\\
			& \side{0.7} & 6.6M & 16\% & 13.17 & 17.93 & 36.14 & 6.79 & 14.83 & 31.90 & 6.30 & 12.95 & 27.60\\
			& \side{0.8} & 5.6M & 28\% & 11.99 & 17.54 & 34.60 & 6.23 & 13.94 & 29.72 & 6.02 & 13.01 & 27.63\\
			& \side{0.9} & \cellcolor[HTML]{656565}\color[HTML]{FFFFFF}  $\downarrow$ 3.7M & \cellcolor[HTML]{026329}\color[HTML]{FFFFFF} 51\% & 11.60 & 17.07 & 33.67 & 6.56 & 14.19 & 30.53 & 5.62 & 12.75 & 27.26   \\
			\midrule
			\rowcolor[gray]{.85} & & & & & & & & & & & & \\
			\midrule
			\multirow{6}{*}{\textit{Funcom \cite{leclair2019neural}}} & \emph{Full}      & \cellcolor[HTML]{656565}\color[HTML]{FFFFFF} $\uparrow$ 108.7M  & \cellcolor[HTML]{a3070c}\color[HTML]{FFFFFF} --   & 14.25 & 17.61 & 36.53 & 5.93 & 12.95 & 28.36 & 6.77 & 12.94 & 28.00 \\
			\cmidrule(r){2-13}
			& \side{0.5} & 99.5M & 9\%  & 15.04 & 18.16 & 37.08 & 6.15 & 13.14 & 29.05 & 6.84 & 12.87 & 27.93 \\
			& \side{0.6} & 95.0M & 13\% & 16.19 & 19.30 & 38.38 & 7.02 & 14.07 & 30.25 & 7.03 & 12.99 & 28.33 \\
			& \side{0.7} & 87.7M & 20\% & 14.77 & 18.92 & 37.39 & 7.49 & 14.19 & 30.14 & 6.62 & 12.79 & 27.78 \\
			& \side{0.8} & 74.2M & 31\% & 14.10 & 18.34 & 37.04 & 6.53 & 13.38 & 28.45 & 6.93 & 12.78 & 27.99 \\
			& \side{0.9} & \cellcolor[HTML]{656565}\color[HTML]{FFFFFF} $\downarrow$ 49.0M & \cellcolor[HTML]{026329}\color[HTML]{FFFFFF} 54\% & 13.65 & 18.08 & 37.12 & 6.78 & 13.61 & 30.07 & 6.81 & 12.95 & 28.07 \\
			\bottomrule
	\end{tabular}}
\end{table*}

We also perform statistical hypothesis tests (Wilcoxon signed-rank test) \cite{wilcoxon1992individual} and Cliff's delta effect size \cite{grissom2005effect} to compare the distributions of the BLEU-4, METEOR, and ROUGE-L of the predictions generated by the different models trained on the filtered training sets with those of the models trained on the full training sets. We use Holm's correction \cite{holm1979simple} to adjust the \textit{p}-values for the multiple tests. We reject the \textit{null hypothesis} (there is no difference between the effectiveness of two given models) if the \emph{p}-value is lower than 0.05.

Finally, we study the Pareto front to analyze the cost-benefit trade-offs between the effectiveness of the models trained on the different selections of \textit{TL-CodeSum} and \textit{Funcom} (benefit, measured with \ie, BLEU, METEOR, and ROUGE-L) and the corresponding training dataset size (cost).

To answer RQ$_{2}$ we compare the selection strategy with \side{0.9} (\ie the most restrictive selection), with a \textit{Random} baseline. In detail, we randomly sample the same number of training instances as those selected with \side{0.9} from each dataset. We compare the effectiveness of the models trained with the training instances selected with \side{0.9} and \textit{Random} measured in terms of the previously described metrics (\ie BLEU-4, METEOR, and ROUGE-L). Again, we perform statistical hypothesis tests (Wilcoxon signed-rank test) \cite{wilcoxon1992individual} and compute the Cliff's delta effect size \cite{grissom2005effect} to compare the distributions of BLEU-4, METEOR, and ROUGE-L of the predictions generated by the \side{0.9} model and the \textit{Random} baseline. We use Holm's correction \cite{holm1979simple} to adjust the \textit{p}-values for the multiple tests. We reject the \textit{null hypothesis} (there is no difference between the two models) if the \emph{p}-value is lower than 0.05.
