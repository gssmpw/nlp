\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{amsmath,amsfonts, amssymb}
\usepackage{algorithm}
\usepackage{adjustbox}
\usepackage{lscape}
\usepackage{siunitx}
\usepackage[noend]{algpseudocode}
\usepackage{textcomp}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{soul}
\usepackage[many]{tcolorbox}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pbox}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{xspace}
\usepackage{url}
\usepackage{tikz}
\usepackage{tabularx}
\usepackage{fontawesome}
\usepackage{lscape}
\usepackage{listings}
\usepackage{color}
\usepackage{showexpl}
\usepackage{anyfontsize}
\usepackage{comment}
\usepackage{soul}
\usepackage{multibib}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{footnote}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{balance}
\usepackage[normalem]{ulem}

\usepackage{hhline}

% MACROs for stylized Tcolorbox
\definecolor{main}{HTML}{5989cf}    % setting main color to be used
\definecolor{sub}{HTML}{cde4ff}     % setting sub color to be used



\tcbset{
	sharp corners,
	colback = white,
	before skip = 0.2cm,    % add extra space before the box
	after skip = 0.5cm      % add extra space after the box
}                           % setting global options for tcolorbox

\newtcolorbox{boxM}{
	fontupper = \color{white},
	rounded corners,
	arc = 6pt,
	colback = main!80,
	colframe = main,
	boxrule = 0pt,
	bottomrule = 4.5pt,
	enhanced,
	fuzzy shadow = {0pt}{-3pt}{-0.5pt}{0.5pt}{black!35}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcolumntype{N}{>{\centering\arraybackslash}m{.85in}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newboolean{showcomments}

\setboolean{showcomments}{true}

\ifthenelse{\boolean{showcomments}}
{\newcommand{\nb}[2]{
		\fbox{\bfseries\sffamily\scriptsize#1}
		{\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
	}
	\newcommand{\cvsversion}{\emph{\scriptsize$-$Id: macro.tex,v 1.9 2005/12/09 22:38:33 xxx Exp \$}}
}
{\newcommand{\nb}[2]{}
	\newcommand{\cvsversion}{}
}

\makeatletter
\newcommand{\linebreakand}{%
	\end{@IEEEauthorhalign}
	\hfill\mbox{}\par
	\mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etc}{etc.\xspace}
\newcommand{\etal}{\emph{et~al.}\xspace}
\newcommand{\secref}[1]{Section~\ref{#1}\xspace}
\newcommand{\figref}[1]{Fig.~\ref{#1}\xspace}
\newcommand{\listref}[1]{Listing~\ref{#1}\xspace}
\newcommand{\tabref}[1]{Table~\ref{#1}\xspace}
\newcommand{\TBD}[1]{\textcolor{red}{#1}\xspace}
\newcommand{\java}{\emph{Java}\xspace}
\newcommand{\tool}{\emph{SATDBailiff}\xspace}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,fill,inner sep=0.8pt] (char) {\textcolor{white}{#1}};}}
\newcommand{\side}[1]{SIDE$_{{#1}}$\xspace}
\newcommand{\green}{green AI\xspace}


\definecolor{lightergray}{rgb}{0.9,0.9,0.9}
\newtcolorbox{resultbox}{colback=lightergray, arc=0.5mm, top=2mm, bottom=2mm, left=2mm, right=2mm}

\definecolor{arsenic}{rgb}{0.23, 0.27, 0.29}
\definecolor{darkgray}{rgb}{0.33, 0.33, 0.33}

\newcommand\TODO[1]{\textcolor{red}{#1}}
\newcommand\REMOVE[1]{\textcolor{red}{\st{#1}}}
\newcommand\ANTONIO[1]{\textcolor{blue}{\nb{ANTONIO}{#1}}}
\newcommand\MAX[1]{\textcolor{green}{\nb{MAX}{#1}}}
\newcommand\VITALE[1]{\textcolor{orange}{\nb{VITALE}{#1}}}
\newcommand\SIMONE[1]{\textcolor{red}{\nb{SIMONE}{#1}}}

\newcommand\rev[1]{\textcolor{black}{#1}}
\pagenumbering{arabic}
\pagestyle{empty}

\begin{document}

\title{Optimizing Datasets for Code Summarization:\\ Is Code-Comment Coherence Enough?}

\author{
\IEEEauthorblockN{Antonio Vitale\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}, Antonio Mastropaolo\IEEEauthorrefmark{2}, Rocco Oliveto\IEEEauthorrefmark{3}, Massimiliano Di Penta\IEEEauthorrefmark{4}, and Simone Scalabrino\IEEEauthorrefmark{4}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Politecnico di Torino, Italy, antonio.vitale@polito.it}
\IEEEauthorblockA{\IEEEauthorrefmark{2}William \& Mary, USA, amastropaolo@wm.edu}
\IEEEauthorblockA{\IEEEauthorrefmark{3}University of Molise, Italy, \{rocco.oliveto, simone.scalabrino\}@unimol.it}
\IEEEauthorblockA{\IEEEauthorrefmark{4}University of Sannio, Italy, dipenta@unisannio.it}
}


\maketitle

\thispagestyle{empty}


\begin{abstract}
Automated code summarization is a long-standing goal for code comprehension. This task automatically generates documentation using a given method. Deep Learning (DL)-based approaches have been proven beneficial for various software engineering (SE) tasks, including this one. 
Most state-of-the-art datasets for code summarization are automatically mined from GitHub and, thus, might contain erroneous or sub-optimal examples. Previous work showed that using a simple rule-based approach for removing noisy instances allows for a tangible reduction of the training set size while not reducing the effectiveness of the trained models. Motivated by this finding, we conjecture that it is possible to further reduce the dataset size by removing instances that contain different issues.
In this paper, we explore the extent to which code-comment coherence, a specific quality attribute of code summaries, can be used to optimize code summarization datasets. Specifically, we hypothesize that removing incoherent code-comment pairs might positively impact the effectiveness of the models. To do this, we rely on SIDE, a recently introduced metric for code-summary coherence. We examine multiple selectivity levels of training instances from two state-of-the-art datasets (TL-CodeSum and Funcom) and evaluate the resulting models on three manually curated test sets. The results show that even halving the training set sizes does not significantly affect the model's ability to generate summaries. However, when comparing the most restrictive selection strategy with a simpler one that randomly selects the training instances, we observe that the resulting accuracy of the model also does not change.
This result suggests that (i) current datasets contain many irrelevant examples, and (ii) different quality attributes should be explored for optimizing code summarization datasets.
\end{abstract}

\begin{IEEEkeywords}
	Code Summarization, Data Quality, Code-Comment Coherence, Empirical Study
\end{IEEEkeywords}


\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Selection Strategies for Code Summarization}
\label{sec:selection_strategies}
\input{selection_strategies}

\section{Study Definition, Design and Planning}
\label{sec:study}
\input{study}

\section{Results}
\label{sec:result}
\input{result}

\section{Discussions}
\label{sec:implication}
\input{discussions}

\section{Threats to Validity}
\label{sec:threats}
\input{threats}

\section{Related Work}
\label{sec:related}
\input{related}


\section{\rev{Conclusion}}
\label{sec:conclusion}
\input{conclusion}

\section{Data Availability}
\label{sec:data}
The study dataset and scripts used for the analysis are available and documented in our online replication package~\cite{replicationpackage}.


\section{Acknowledgments}
\label{sec:ack}
\input{ack}


\balance
\bibliographystyle{IEEEtran}
\bibliography{main}


\end{document}
\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
