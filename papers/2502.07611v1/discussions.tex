Our findings challenge the conjecture that code-comment coherence, as measured by SIDE \cite{mastropaolo2024evaluating}, is a critical quality attribute for filtering instances of code summarization datasets. By selecting $\langle code, summary \rangle$ pairs with high-coherence for training allow to achieve the same results that would be achieved by randomly selecting such a number of instances. At the same time, we observed that reducing the datasets size up to 50\% of the training instances does not significantly affect the effectiveness of the models, even when the instances are randomly selected. These results have several implications.

First, code-comment consistency might not be a problem in state-of-the-art datasets in the first place, as also suggested in the results of RQ$_0$. Also, the DL models we adopted (and, probably, bigger models as well) are not affected by inconsistent code-comment pairs, even when these inconsistencies are present in the training set.
Despite the theoretical benefits of filtering by SIDE \cite{mastropaolo2024evaluating}, that is the state-of-the-art metric for measuring code-comment alignment, our results indicate its limitations in improving the \textit{overall} quality of the training sets for code summarization task.
Nevertheless, other quality aspects of code and comments that have not been explored yet (such as readability) may be important for smartly selecting the training instances.
Future work should explore such quality aspects further.

Our results clearly indicate that state-of-the-art datasets contain instances that do not contribute to improving the models' effectiveness. This finding is related to a general phenomenon observed in Machine Learning and Deep Learning. Models reach convergence when they are trained for a certain amount of time (epochs). Additional training provides smaller improvements and increases the risk of overfitting. We show that the same is true for data. In terms of effectiveness, model convergence is achieved with fewer training instances than previously assumed. Limiting the number of epochs may make it possible to reach model convergence with a subset of training data, maintaining model effectiveness, reducing resource demands and minimizing the risk of overfitting.
Future work could explore different criteria for data selection that identify the most informative subsets for training.
Conversely, this insight suggests that currently available datasets suffer from poor diversity (thus causing the previously discussed phenomenon).
This latter insight constitutes a clear warning for researchers interested in building code summarization datasets, which should include instances that add relevant information instead of adding more data, which might turn out to be useless.

Finally, it is worth pointing out that another benefit of the reduction we performed is the environmental impact. Reducing the number of training instances implies a reduced training time, which, in turn, lowers the resources necessary to perform training and, thus, energy consumption and CO$_2$ emissions.
We performed a rough estimation of the training time across different selections of \textit{TL-CodeSum} and \textit{Funcom} datasets and estimated a proxy of the CO$_2$ emissions for each model training phase by relying on the ML CO$_2$ impact calculator\footnote{\url{https://mlco2.github.io/impact/\#compute}} \cite{lacoste2019quantifying}. Such a calculator considers factors such as the total training time, the infrastructure used, the carbon efficiency, and the amount of carbon offset purchased. The estimation of CO$_{2}$ emissions needed to train the model with the \textit{Full} selection of \textit{Funcom} ($\sim$ 200 hours) is equal to 26.05 Kg, while with the optimized training set, \ie $SIDE_{0.9}$ ($\sim$ 90 hours), the estimation is 11.69 Kg of $CO_2$ (-55\% emissions).
While we recognize that this method provides an estimation rather than a precise measurement, it offers a glimpse into the environmental impact of applying data reduction.
