% !TEX root = main.tex

This section describes the threats that could affect the findings of our study.

\textbf{Construct validity} threats concern the relationship between theory and observation. A first threat is related to how we assess the quality of the generated summaries. We leverage metrics widely used in literature and, specifically, BLEU-4, METEOR, and ROUGE. We are aware that such metrics may not fully reflect the developers' perception of a summary quality.

Another threat could be related to the choice of SIDE as a driver for selecting a training set based on summary coherence. As shown in previous work \cite{mastropaolo2024evaluating}, this metric correlates with human-based summary evaluation better than other state-of-the-art metrics.

\textbf{Internal validity} threats concern factors internal to our study that could affect our findings. One factor is related to the hyperparameter calibration of the performed training. As explained in \secref{sec:study}, our choices are based on those of previous studies \cite{mastropaolo2023towards,ciniselli2024generalizability}.

Another factor is the choice of the SIDE cut thresholds. We have chosen five thresholds varying from 0.5 to 0.9 and reported the findings for such levels of dataset filtering. While we did not explore the full range of SIDE values, we have evaluated, with a discretization of 0.1, the whole range from 0.5 above. We conjecture that lower values would not show results much different from the full datasets because the 0.5 threshold indicates a very small reduction (7\%-9\% on the two datasets).

Finally, we are aware that a more accurate computation of training times would require multiple runs. However, this was unfeasible given the number of configurations to evaluate and the training time required for each of them. 

\textbf{Conclusion validity} threats concern the relationship between the experimentation and outcome. The study is mostly observational. Therefore, we report and discuss results through descriptive statistics. However, wherever appropriate---and in particular for RQ$_1$ and RQ$_2$, we complement them with suitable statistical procedures (Wilcoxon rank-sum test and Cliff's $d$ effect size). Also, since multiple tests have been performed, we adjust the $p$-values using Holm's correction procedure \cite{holm1979simple}.

\textbf{External validity} threats concern the generalizability of our findings. We have considered two datasets for the training. These datasets have been specifically designed for code summarization and cleaned up using the CAT approach by Shi \etal \cite{shi2022we}. 
As for the test set, we have considered two datasets that (i) do not overlap with the training set and (ii) have been evaluated by humans. 
Although these datasets are particularly suitable for the study reported in this paper, we cannot exclude that applying the selection strategy to other datasets, particularly those related to different programming languages or application domains, might lead to different results.
