% !TEX root = main.tex
In this section, we examine key literature concerning the impact of data quality on neural models for automating software engineering practices, with a particular focus on code summarization.
Following this, we provide a section covering the main approaches in code summarization literature.

\subsection{The Importance of Data Quality for DL-Based Methods in Code Summarization}
\label{sec:dataset-cleaning}

Data quality is essential for the success of DL methods designed to automate software engineering tasks. Models trained on noisy data frequently suffer significant performance degradation when deployed in real-world settings. A recent study by Shi \etal \cite{shi2022evaluation} examines various factors that could impact the evaluation of code summarizers, including metrics, code preprocessing operations, and datasets. Their empirical study showed that these elements significantly influence the evaluation of code summarization models. Specifically, Shi \etal observed that certain code preprocessing operations (\eg converting all tokens to lowercase) could substantially affect performance. This can either enhance the model's effectiveness or lead to a decline in performance, showcasing the fundamental role of preprocessing strategies to either boost or impair model outcomes. 

LeClair and McMillan \cite{leclair2019recommendations} also explored the effects of different preprocessing decisions on datasets used for code summarization. Specifically, they analyzed how splitting training and test datasets by function versus by project influences model performance.

Gros \etal \cite{gros2020code} investigate the relationship between code comments and code itself, particularly exploring the premise that generating code comments is akin to translating between natural languages. This conceptual alignment has facilitated using models and evaluation metrics from Natural Language Processing (NLP) in tasks like code summarization. Gros \etal assessed how code-comment datasets, 
compare with datasets used for natural language translation. Specifically, they contrasted each code-comment dataset against WMT19~\cite{barrault2019findings}, a renowned corpus utilized for training natural language translators. The outcomes of their research indicated that code comments are more repetitive than English sentences found in natural language, a characteristic that influences performance evaluation metrics.


Sun \etal \cite{sun2022importance} illustrate the issue of poor natural language data quality within the context of code searchâ€”specifically, retrieving the ``closest'' code matching a natural language query in a codebase. By implementing a semantic query cleaning module for code search datasets, Sun \etal showed that their filtering framework not only enhances the accuracy of models trained on the filtered dataset but also conserves computing resources. Along these lines, Li \etal~\cite{li2023commit} investigated the quality of commit messages and found that (i) the quality of commit messages plays a fundamental role when it comes to software defects, and (ii) the overall quality of these messages declines over time, despite the developers' belief who think their commit messages are improving.

Xu \etal \cite{xu2023data} explored the impact of data quality on just-in-time obsolete comment detection.
They empirically showed that applying a set of manually derived rules could enhance accuracy by up to 10.7\%.

Shi \etal \cite{shi2022we} implemented a similar approach to address data quality issues in code summarization benchmarks by introducing CAT (Code-comment Cleaning Tool). This tool can identify noisy data in various programming languages, including Java and Python. Specifically, the rule-based method (\ie CAT) developed by Shi \etal is designed to detect specific patterns of noisy data at both the comment and code levels, based on a taxonomy of data preprocessing noises identified across four popular datasets. An initial evaluation with CAT showed that commonly used code summarization benchmark datasets for Python and Java include noisy data pairs $\langle code, comment \rangle$, ranging from 31\% to 66\%. These noisy elements were either ``fixed'' to decrease their noise level or completely removed. Following this cleanup, state-of-the-art neural code summarization techniques were retrained from the ground on these refined benchmarks. The performances of the different models were then compared against the cleaned test datasets processed using CAT. This comparison indicated that the optimized training dataset significantly improved summarization accuracy and overall performance.

\subsection{Automated Code Summarization}
Different studies have explored the automation of code summarization, with three primary approaches emerging in the literature: Information Retrieval (IR), Deep Learning (DL), and hybrid methods combining both techniques.

One of the first approaches to DL-based code summarization is the work by Iyer \etal \cite{iyer2016summarizing}, who introduced an RNN-based model with an attention mechanism for generating code summaries. They used an encoder-decoder architecture, proposing CODE-NN to generate summaries. It was trained on code-description pairs from StackOverflow and it demonstrated substantial improvements over traditional approaches in generating summaries, highlighting one of the first evidence of DL effectiveness for code summarization tasks.

Zhang \etal \cite{zhang2020retrieval} proposed a retrieval-based neural model, Rencos, which combines information retrieval (IR) techniques with neural machine translation (NMT) specifically for code summarization. They first train an encoder-decoder model. Then, during the testing phase, they retrieve the most similar snippets from the training set in terms of syntax and semantics and encode them with the input. Finally, after fusing them it predicts the summary.

One of the first works exploring Transformer-based approaches for code summarization is the one by Ahmad \etal \cite{ahmad2020transformer}. In their work, they leverage the self-attention mechanism within Transformers to model the complex, long-distance dependencies in source code, aiming to generate appropriate summaries. Their results demonstrated that Transformers are effective for code summarization tasks.

With the advent of Large Language Models (LLMs), substantial progress has been made in automating code summarization. LLMs are capable of few-shot learning \ie providing task-specific examples in the prompt, allowing the model to perform the requested task.
As first steps for LLM-based code summarization, Ahmed \etal \cite{ahmed2024automatic} introduced the Automatic Semantic Augmentation of Prompts (ASAP) technique. This latter enhances LLMs performance by adding structured semantic information in the prompts. ASAP inserts (i) repository context, (ii) tagged identifiers, and (iii) data flow graphs directly into the prompt, guiding the model toward a wider knowledge of the code structure and functionality. Such technique allowed models like Code-Davinci and GPT-3.5 to achieve state-of-the-art performance on multiple programming languages for the code summarization task.

As part of advancing LLM-based code summarization, Sun \etal \cite{sun2024source} conducted a study examining various prompting strategies and model configurations for improving performance in code summarization tasks. They evaluated prompting techniques such as zero-shot, few-shot, and chain-of-thought, finding that simpler approaches often performed as well as more complex methods.

\subsection{Code-comment Coherence}
Detecting code comments that result to be inconsistent with respect to the given code has been the subject of several works.
Tan \etal \cite{tan2007icomment} proposed \textit{iComment}, a tool that leverages NLP properties, machine learning models, and program analysis techniques to extract rules for a decision tree classifier able to detect potential inconsistencies between comments and code. They evaluated their tool for large-scale projects such as Linux, Mozilla, Apache, and Wine, finding several inconsistencies, including bugs and bad comments.

Liu \etal \cite{liu2018automatic}, instead, focused on detecting outdated comments during code changes.
The authors proposed a machine learning-based approach that leverages 64 features related to code, comments, and their relationship before and after code changes to identify outdated comments. Using a random forest classifier, they evaluated their approach on several open-source projects and demonstrated its effectiveness in detecting outdated comments, achieving high precision and recall rates.

Wang \etal \cite{wang2019deep} proposed DComment, a framework to evaluate the quality of comments in source code. By analyzing both the code and its associated comments, DComment identifies meaningful patterns and relationships to determine whether a comment is coherent and relevant. DComment showed a strong ability to generalize across different projects on multiple datasets.

Xu \etal \cite{xu2024code} proposed MCCL, which combines method comment detection and confidence learning to identify and mitigate inconsistencies. The proposed approach uses advanced encoding techniques, including multi-head attention and graph neural networks, to analyze the relationships between code and comments. Additionally, a denoising component addresses noisy data and labeling errors, improving the detection process. MCCL was evaluated on over 1,500 open-source projects, demonstrating superior performance compared to existing methods, significantly improving precision, recall, and F1-score.

In this work, we adopt SIDE \cite{mastropaolo2024evaluating} as an approach for measuring code-comment coherence instead of the previously-mentioned approaches because, differently from them, (i) it has been shown to highly correlate with human judgment, and (ii) it allows us to filter instances based on a threshold (multiple selectivity levels).
