
\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{\TABVSPACE} % vertical spacing
    \setlength{\tabcolsep}{3pt} % horizontal spacing
    \caption{Comparison of RoI-based methods over vanilla VPCC. Outperforming results are highlighted in bold.}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
     Detector      & \multicolumn{6}{c|}{CenterPoint} & \multicolumn{6}{c}{BEVFusion-Lidar} \\
     
    \hline
    \multirow{2}{*}{Method}  & \multicolumn{3}{c|}{Naive RoI} & \multicolumn{3}{c|}{\methodname{}} &  \multicolumn{3}{c|}{Naive RoI} & \multicolumn{3}{c}{\methodname{}} \\ 
                               \cline{2-6}         \cline{5-7}            \cline{8-10}          \cline{11-13}
                &$\roiqp=15$&$\roiqp=20$&$\roiqp=25$ &$\roiqp=15$&$\roiqp=20$&$\roiqp=25$ &$\roiqp=15$&$\roiqp=20$&$\roiqp=25$ &$\roiqp=15$&$\roiqp=20$&$\roiqp=25$\\
    \hline
        $\areabetwcurve_{mAP}\uparrow$ & -0.26 & 1.34 & 1.94 & \textbf{1.92} & \textbf{3.51} & \textbf{3.61} & -1.61 & 0.21 & 1.24 & \textbf{0.29} & \textbf{2.33} & \textbf{2.80} \\ 
        $\areabetwcurve_{NDS}\uparrow$ & -1.00 & 1.20 & 2.26 & \textbf{1.34} & \textbf{2.90} & \textbf{3.69} & -1.44 & -0.18 & 0.75 & \textbf{0.33} & \textbf{1.65} & \textbf{2.23} \\ 
        $\areabetwcurve_{ATE}\downarrow$ & -0.22 & -2.81 & -3.80 & \textbf{-1.75} & \textbf{-3.92} & \textbf{-4.40} & \textbf{0.49} & -0.05 & -0.38 & 0.66 & \textbf{-0.07} & \textbf{-0.84} \\ 
        $\areabetwcurve_{ASE}\downarrow$ & -0.72 & -1.92 & -2.82 & \textbf{-1.44} & \textbf{-2.50} & \textbf{-3.32} & \textbf{-0.02} & \textbf{-0.37} & \textbf{-0.41} & 0.09 & 0.06 & -0.24 \\ 
        $\areabetwcurve_{AOE}\downarrow$ & 4.56 & 1.64 & \textbf{-3.54} & \textbf{1.35} & \textbf{-0.27} & -0.95 & 0.00 & -0.73 & 0.28 & \textbf{-2.14} & \textbf{-1.21} & \textbf{-0.26} \\ 
        $\areabetwcurve_{AVE}\downarrow$ & 5.60 & 0.60 & 0.56 & \textbf{0.30} & \textbf{-2.13} & \textbf{-5.89} & 5.63 & 4.11 & -0.25 & \textbf{-0.01} & \textbf{-2.73} & \textbf{-5.58} \\ 
        $\areabetwcurve_{AAE}\downarrow$ & -0.56 & \textbf{-2.81} & -3.32 & \textbf{-2.27} & -2.65 & \textbf{-4.29} & 0.27 & -0.06 & -0.57 & \textbf{-0.44} & \textbf{-0.90} & \textbf{-1.34} \\ 
    \hline
    \end{tabular}
    \label{tab-main-results}
    \vspace{-0.2cm}
\end{table*}


\section{Experiments}
\subsection{Experiment Settings}

\subsubsection{Dataset}
\label{sec-dataset}
Experiments are conducted on the nuScenes~\cite{caesarnuScenesMultimodalDataset2020} dataset, which comprises 1000 autonomous driving scenes, each lasting 20 seconds, collected in urban environments such as Boston and Singapore. Each scene sample includes data from a lidar sensor, which provides high-resolution 3D point clouds. The point clouds are sampled by a frequency of 20~Hz and annotated by a frequency of 2~Hz with 3D bounding boxes for 10 classes of objects. 

For 3D object detection, nuScenes assigns 750 scenes for training, 150 scenes for validation, and 100 for testing. However, the annotations for the test set nuScenes dataset are not directly accessible~\cite{nusceneswebsite}. As we need to conduct a vast number of rounds of evaluation on the test set, we re-split the training set to 525 for training and 125 for validation, and the original 150 scenes in the validation set are used for testing. The new splitting is applied to the training of both RoI detectors and back-end detectors. 

As frames in the same scenes have close object distributions, we select clips of 40-point cloud frames indexing from 90 to 130 from each scene to accelerate the encoding process. Note that the input to the RoI detector is the enhanced point cloud that aggregates 10 historical lidar frames following common practices~\cite{mmdet3d2020}. Thus, RoI detectors predict RoI mask $\roimaskpc$ on 4 out of the 40 frames. The RoI mask for the rest of the frames is transformed from $\roimaskpc$ based on the ego-motion provided by nuScenes.

\subsubsection{Back-end Detectors}
\label{back-end-detectors}
We select the leading 3D object detectors, CenterPoint~\cite{yin2021center} and BEVFusion-Lidar~\cite{liu2023bevfusion} as our back-end detectors. We first pre-train the detectors using the dataset splitting in ~\cref{sec-dataset}. To accommodate the back-end detectors with the compressed dataset, we fine-tune the pre-trained models on a mixed dataset comprising both lossy and lossless point clouds for one epoch. Details can be found in Suppl.~\ref{sup-backend-detector}.

\subsubsection{Baselines}
We provide two baselines for the evaluation:

\textbf{Vanilla VPCC}: The original VPCC using a uniform compression scheme. The bitrate control is also implemented during the transcoding phase, except the whole point cloud is encoded with non-RoI QP $\backqp$.

\textbf{Naive RoI}: Replace the GMM-based RoI detector of \methodname{} with a naive RoI detector directly built based on the CenterPoint~\cite{yin2021center} 3D object detector. The output 3D bounding boxes are transformed into point-level RoI indexes at the test time. The detector is trained on the same dataset described in ~\cref{sec-dataset}. Ground removal and RoI encoding follow the same procedure as \methodname{}.

To fairly compare the GMM-based RoI detector and naive RoI detector, the network capacity of the two detectors is comparable, as shown in Table.~\ref{tab-detector-capacity}. The FLOPs and parameter number of the naive RoI detector are slightly larger than the GMM-based RoI detector.

\subsubsection{Implementation Details}
\label{sec-implementation-details}
We set the number of GMM components $K=5$ and the test-time heatmap binarization threshold $\gamma=0.4$. We use MPEG-TMC2 test model v15.0~\footnote{https://github.com/MPEGGroup/mpeg-pcc-tmc2} as the base implementation of VPCC. Details about the implementation and training of the RoI detector can be found in Suppl.~\ref{sup-roi-detector}.


\subsubsection{Metrics}

\begin{figure}[t]
% \setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{-0.2cm}
  \centering
    \includegraphics[width=\linewidth]{figs/illus_advantage.pdf}
    % \fbox{\rule{0pt}{5cm} \rule{\linewidth}{0pt}}
  \caption[]{Illustration of the evaluation metric.\protect\footnote{}}
  \label{fig-metric-illus}
\vspace{-0.4cm}
\end{figure}
\footnotetext{Result is encoded with $\roiqp=25$, the rest of the settings follows \cref{sec-implementation-details}.}

For each $\roiqp\in\{15,20,25\}$, we iterate $\backqp\in\{30,33,36,39,42,45\}$ to encode the test set. Then, we run back-end 3D object detectors on the encoded test set and record their performances. The performance metrics~\cite{caesarnuScenesMultimodalDataset2020} include mean average precision (mAP), nuScenes detection score (NDS), and true-positive metrics: average translation error (ATE), average scale error (ASE), average orientation error (AOE), average velocity error (AVE) and average attribute error (AAE). We also record the averaged bitrate of encoded bitstreams across test scenes. As shown in Fig.~\ref{fig-metric-illus}, we can draw metric-bitrate curves where the y-axis is the performance metric, and the x-axis is the averaged bitrates in megabits per second (Mbps). 

% To comprehensively evaluate the performance boost brought by RoI encoding, 
% we calculate the difference between the averaged value of metric-volume function of vanilla VPCC and the RoI-based methods:
We define \textit{averaged advantage} $\areabetwcurve$ to evaluate the performance boost brought by RoI encoding against vanilla VPCC:
% \begin{equation}
\begin{align}
    \areabetwcurve &= \frac{\int_{x_{min}}^{x_{max}} \metric(x) - \metric_{b}(x) \, dx }{x_{max}-x_{min}}\\
                   &\approx \frac{1}{N} \sum_{i=1}^{N} \metric(x_i) - \metric_{b}(x_i) \nonumber
\end{align}
% \end{equation}
where $\metric$ and $\metric_{b}$ are the metric-bitrate curves of the RoI-based method and vanilla VPCC, respectively. $ x_{min} $ and $ x_{max} $ define the region on the x-axis where the domains of the two curves overlap. We linearly interpolate the metric-volume line chart to approximate $\areabetwcurve$, where $N=100$ is the number of samples after interpolation. In the following experiments, $\areabetwcurve$ is recorded in percentage value.

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{\TABVSPACE} % vertical spacing
\setlength{\tabcolsep}{6pt} % horizontal spacing
\caption{Class-wise performance of \methodname{} over vanilla VPCC. Results outperforming VPCC are highlighted in bold.}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
\hline
    Class & Car & Truck & Bus & Trailer & Constr Veh & Pedestrian & Motorcycle & Bicycle & Traffic Cone & Barrier \\ 
    \hline
    $\areabetwcurve_{mAP}\uparrow$ & -0.28 & \textbf{2.87} & \textbf{2.53} & \textbf{4.85} & \textbf{1.13} & \textbf{10.00} & \textbf{1.79} & -0.50 & \textbf{1.05} & \textbf{5.76} \\ 
    $\areabetwcurve_{ATE}\downarrow$ & \textbf{-0.09} & 0.81 & 0.25 & \textbf{-1.53} & \textbf{-1.85} & \textbf{-0.57} & \textbf{-0.51} & \textbf{-13.19} & \textbf{-1.31} & \textbf{-1.97} \\ 
    $\areabetwcurve_{ASE}\downarrow$ & \textbf{-0.13} & 0.07 & 0.40 & \textbf{-1.02} & \textbf{-1.76} & 0.26 & \textbf{-0.06} & \textbf{-9.42} & \textbf{-0.25} & \textbf{-0.30} \\ 
    $\areabetwcurve_{AOE}\downarrow$ & \textbf{-0.66} & \textbf{-1.03} & \textbf{-0.25} & 7.39 & \textbf{-3.95} & \textbf{-2.39} & 2.96 & \textbf{-8.48} & \tabNA & \textbf{-0.25} \\ 
    $\areabetwcurve_{AVE}\downarrow$ & \textbf{-0.84} & \textbf{-1.97} & \textbf{-3.94} & \textbf{-12.09} & 0.14 & \textbf{-1.32} & 2.47 & \textbf{-1.85} & \tabNA & \tabNA \\ 
    $\areabetwcurve_{AAE}\downarrow$ & \textbf{-0.43} & 0.04 & \textbf{-0.82} & \textbf{-1.06} & 0.49 & \textbf{-2.41} & 1.82 & \textbf{-11.83} & \tabNA & \tabNA \\ 
\hline
\end{tabular}
\label{tab-abla-class-wise-perf}
\vspace{-0.4cm}
\end{table*}

\subsection{Main Results}
\label{sec-main-results}
Table.~\ref{tab-main-results} shows the main results of the experiment. It can be seen that: 1) \textit{\methodname{} consistently improves VPCC across different RoI QPs and back-end detectors.} 2) \textit{The GMM-based RoI detector significantly outperforms naive RoI detector in most cases.} This shows that GMM-based RoI is more efficient for finding critical regions potentially containing objects. For the naive RoI detector, 3D object detection redundantly requires the network to classify the objects, which wastes the network's capacity. 3)  Compared with vanilla VPCC and naive RoI, \textit{\methodname{} consistently improves the true-positive metrics.} This implies that with GMM-based RoI encoding, the details of objects are well-preserved so that the back-end detector can easily identify the boundary and pose of objects. 4) \textit{The advantage over vanilla VPCC for both RoI-based methods enlarges when $\roiqp$ increases.} This is partly because when the RoI quality is degraded, the domain of the metric-bitrate curves of RoI-based methods will shift towards low bitrates, where the advantage of RoI encoding is more significant. However, when $\roiqp$ is too large, the absolute performance of the RoI-based methods will be less satisfying. Thus, we empirically keep $\roiqp$ no larger than 25.


\subsection{Additional Experiments}
We conduct additional experiments to get a detailed understanding of \methodname{}'s performance. Unless specially mentioned, we set $\roiqp=20$, $K=5$, $\gamma=0.4$, and $\areabetwcurve$ is averaged across back-end detectors. 

\vspace{-0.2cm}
\subsubsection{Class-wise Analysis}

Table.~\ref{tab-abla-class-wise-perf} demonstrates the class-wise performance of \methodname{} over vanilla VPCC. It can be seen that: 1) \textit{The smaller the object, the greater the performance improvement achieved by \methodname{} over VPCC.} For instance, the mAP advantage of \textit{pedestrain} class improves significantly by 10\%; the mAP advantages of \textit{barrier} class improves by 5.76\%; the ATE and AAE advantages of \textit{bicycle} class improve over 10\%. These results indicate that \methodname{} preserves much more object details than vanilla VPCC, given the same bitrate consumption. 2) \textit{\methodname{} help distinguish between visually similar classes.} For instance, mAP advantages of \textit{truck} and \textit{bus} both improve by approximately 3\%. 3) Interestingly, \textit{the performances of \textit{car} class are less significant compared to other classes.} It may be that the key features for identifying the \textit{car} class are more resilient to VPCC compression.


\vspace{-0.2cm}
\subsubsection{Number of GMM Components}

\begin{table}[ht]
\vspace{-0.4cm}
\centering
\renewcommand{\arraystretch}{\TABVSPACE} % vertical spacing
\setlength{\tabcolsep}{10pt} % horizontal spacing
\caption{Impact of the number of GMM Components}
\begin{tabular}{c|c|c|c|c}
\hline
$K$ & 1 & 3 & 5 & 7 \\
\hline
        $\areabetwcurve_{mAP}\uparrow$ & 0.80 & 2.89 & 2.92 & 2.48 \\ 
        $\areabetwcurve_{NDS}\uparrow$ & 2.05 & 2.54 & 2.28 & 2.10 \\ 
        $\areabetwcurve_{ATE}\downarrow$ & -2.16 & -1.92 & -2.00 & -1.41 \\ 
        $\areabetwcurve_{ASE}\downarrow$ & -2.43 & -1.21 & -1.22 & -1.01 \\ 
        $\areabetwcurve_{AOE}\downarrow$ & -3.06 & -1.59 & -0.74 & -2.02 \\ 
        $\areabetwcurve_{AVE}\downarrow$ & -5.51 & -4.19 & -2.43 & -2.22 \\ 
        $\areabetwcurve_{AAE}\downarrow$ & -3.75 & -2.08 & -1.78 & -1.93 \\ 
\hline
\end{tabular}
\label{tab-abla-GMM-K}
\vspace{-0.2cm}
\end{table}


Intuitively, increasing the number of GMM components $K$ allows the RoI detector to better fit the object point clouds (the learning objective will be the same as point cloud segmentation when $K\to \infty$), so RoIs will contain fewer background points. However, as shown in Table~\ref{tab-abla-GMM-K}, the performance of \methodname{} starts to decline when $K=7$. This can be attributed to the observation that a simpler object representation tends to stabilize the learning process~\cite{yin2021center, zhou2019objects}. Additionally, the benefits of a more precise fit diminish when $K$ becomes large, as the number of background points involved in heatmaps is already minimized.


\vspace{-0.2cm}
\subsubsection{Heatmap Binarization Threshold}

\begin{table}[ht]
\vspace{-0.4cm}
\centering
\renewcommand{\arraystretch}{\TABVSPACE} % vertical spacing
\setlength{\tabcolsep}{12pt} % horizontal spacing
\caption{Impact of heatmap binarization threshold}
\begin{tabular}{c|c|c|c|c}
\hline
$\gamma$ & 0.2 & 0.3 & 0.4 & 0.5 \\
\hline
        $\areabetwcurve_{mAP}\uparrow$ & -0.69 & 1.67 & 2.92 & 2.80 \\ 
        $\areabetwcurve_{NDS}\uparrow$ & -1.06 & 1.37 & 2.28 & 2.64 \\ 
        $\areabetwcurve_{ATE}\downarrow$ & 0.96 & -1.19 & -2.00 & -2.10 \\ 
        $\areabetwcurve_{ASE}\downarrow$ & 0.00 & -0.82 & -1.22 & -1.54 \\ 
        $\areabetwcurve_{AOE}\downarrow$ & 2.49 & -0.68 & -0.74 & -2.31 \\ 
        $\areabetwcurve_{AVE}\downarrow$ & 4.04 & -1.50 & -2.43 & -4.29 \\ 
        $\areabetwcurve_{AAE}\downarrow$ & -0.38 & -1.17 & -1.78 & -2.20 \\ 
\hline
\end{tabular}
\label{tab-abla-heatmap-thrd}
\vspace{-0.2cm}
\end{table}


Table.~\ref{tab-abla-heatmap-thrd} shows the impact of the heatmap binarization threshold $\gamma$. It can be seen that $\gamma=0.4$ achieves the best mAP and NDS performance. When $\gamma$ is too small, the predicted RoI regions will include too many non-RoI points, leading to poor compression efficiency. On the contrary, when $\gamma$ is too large, potential objects may be ignored, leading to low detection accuracy.
It is worth noting that $\gamma=0.5$ shows improvement on true-positive metrics. It is expected because regions with higher heatmap scores are inherently easier to identify and localize.


\begin{figure*}[t!]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.2cm}
  \centering
    \includegraphics[width=\linewidth]{figs/qualitative.pdf}
  \caption[]{Qualitative results. The first and second rows are the detection results of CenterPoint on point clouds encoded with DetVPCC and VPCC, respectively, where $\roiqp=20$ and $\backqp=45$. The third row shows RoIs (colored in green) predicted by the RoI detector.}
  \label{fig-qualitative}
\vspace{-0.2cm}
\end{figure*}


\vspace{-0.2cm}
\subsubsection{Ablation on Ground Removal}

\begin{table}[ht]
\vspace{-0.4cm}
\centering
\renewcommand{\arraystretch}{\TABVSPACE} % vertical spacing
\setlength{\tabcolsep}{12pt} % horizontal spacing
\caption{Ablation on ground removal}
\begin{tabular}{c|c|c}
\hline
Ground Removal & w/ & w/o \\
\hline
        $\areabetwcurve_{mAP}\uparrow$ & 2.92 & 1.82 \\ 
        $\areabetwcurve_{NDS}\uparrow$ & 2.28 & 1.15 \\ 
        $\areabetwcurve_{ATE}\downarrow$ & -2.00 & -0.55 \\ 
        $\areabetwcurve_{ASE}\downarrow$ & -1.22 & -0.76 \\ 
        $\areabetwcurve_{AOE}\downarrow$ & -0.74 & -0.74 \\ 
        $\areabetwcurve_{AVE}\downarrow$ & -2.43 & 0.62 \\ 
        $\areabetwcurve_{AAE}\downarrow$ & -1.78 & -0.96 \\ 
\hline
\end{tabular}
\label{tab-abla-ground-removal}
\vspace{-0.2cm}
\end{table}


Table.~\ref{tab-abla-ground-removal} shows the contribution of ground removal in \methodname{}. Ground removal assists the performance of \methodname{} to some extent as it compensates for the 2D heatmap on the z-axis and helps filter the ground points. However, DetVPCC still outperforms vanilla VPCC without ground removal, proving its effectiveness. In contrast, the naive RoI detector, which is also equipped with ground removal, exhibits compromised performance as mentioned in \cref{sec-main-results}.


\subsubsection{Find Points in Bounding Boxes}
\vspace{-0.4cm}
\begin{table}[ht]
    \centering
    \caption{Speed test of Alg.~\ref{alg-points-in-roi}}
    \begin{tabular}{c|c|c}
    \hline
        Method & Ours & nuScenes Toolkit \\ \hline
        Time per Box (ms) & 0.70 & 2.15 \\ \hline
    \end{tabular}
    \label{tab-alg1-speed}
\vspace{-0.2cm}
\end{table}

Table.~\ref{tab-alg1-speed} evaluates the speed of Alg.~\ref{alg-points-in-roi}. 
The baseline method is the \textit{points\_in\_box} function provided by nuScenes 
toolkit~\footnote{https://github.com/nutonomy/nuscenes-devkit}, 
which examines each point's relative position to the bounding box's three axes.
Experiments are conducted on Intel Xeon Gold 6226R CPUs. As shown, our method is $\sim3\times$ faster than the baseline method. The speed-up is attributed to the reusable K-D tree that drastically narrows the search space and cuts unnecessary evaluation between points and boxes that are too far away.


\subsection{Qualitative Results}


Fig.~\ref{fig-qualitative} shows visualized detection results between \methodname{} and vanilla VPCC, along with heatmaps predicted by the RoI detector. It can be seen that RoI-based encoding significantly aids 3D object detection, especially for small objects that are easily lost by vanilla VPCC.

