\section{Methodology}
\subsection{Overview}
As shown in Fig.~\ref{fig-system-diagram}, in the first stage, the RoI detector identifies RoIs within point clouds, while VPCC converts raw point clouds into lossless 2D depth images. Subsequently, the RoI encoder applies RoI-based lossy transcoding to these lossless depth images, effectively compressing the data volume. The encoded point clouds can then be archived or streamed to the cloud for further analysis by back-end 3D object detectors. This enables various applications, such as cloud-based 3D object detection, fault detection, optimization of on-device 3D object detectors, and efficient 3D content retrieval.

\subsection{RoI-based point cloud Sequence Encoder}
\label{sec-roi-encoder}
\subsubsection{Preliminary on VPCC}
To encode a point cloud sequence, VPCC first clusters point clouds into small patches grouped by point-wise surface normals. Then, it projects and arranges them into three sequences of 2D images, namely attribute, occupancy, and geometry images~\cite{graziosiOverviewOngoingPoint2020}. 
Attribute images store color and auxiliary information. Occupancy images are binary images indicating the occupancies in geometry images.
Geometry (Depth) images encode point positions into pixel values proportional to the distance between the projection plane and correlated points, as shown in Fig.~\ref{fig-roi-encoding}.
These 2D images are then compressed to separate bitstreams using standard 2D video codecs like H.264. Typically, geometry images account for most of the volume ~\cite{rudolph2023rabbit}.

To reconstruct point clouds, the compressed bitstreams are decoded and projected back to 3D point clouds using projection information encoded in the bitstream.

\subsubsection{Bitrate Control and RoI Encoding}
VPCC provides bitrate control options at both point and image levels. For the point-level bitrate control, VPCC provides options to limit the maximum number of points in a patch and the maximum distance between a selected point to the projection plane~\cite{graziosiOverviewOngoingPoint2020}. At the image level, bitrate control follows the underlying 2D video encoder. 

Following the practice of existing works~\cite{rudolph2023rabbit, shen2021rate, wang2023vqba}, we control bitrate at the image level by \textit{transcoding the lossless geometry (depth) images} with H.264~\cite{wiegand2003overview} video encoder to take advantage of the mature bitrate control mechanisms of 2D video encoders. Unlike previous works that uniformly compress geometry images, we implement non-uniform quality assignments by leveraging the macroblock-level quality control feature of 2D video encoders.

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.2cm}
  \centering
    \includegraphics[width=\linewidth]{figs/roi_encoding.pdf}
    % \fbox{\rule{0pt}{5cm} \rule{\linewidth}{0pt}}
  \caption[]{Illustration of RoI Encoding.\protect\footnote{}\ The right image compares point clouds with RoI and non-RoI encoding. The red points represent the points that appear in the non-RoI encoded frame but not in the RoI encoded frame, and blue represents the contrary. Black points represent common parts.}
  \label{fig-roi-encoding}
% \vspace{-0.4cm}
\end{figure}
\footnotetext{For better visualization, the depth image is cropped, and its sharpness is lowered by 25\%; The sample is encoded with $\roiqp=25$ and $\backqp=30$.}

% As illustrated in Fig.~\ref{fig-roi-encoding}, 
Specifically, H.264 processes geometry images in units of macroblocks consisting of $16\times16$ pixels. Each macroblock will be further partitioned and transformed into the frequency domain by $4\times4$ 2D discrete cosine transform (DCT)~\cite{wiegand2003overview}:
\begin{align*}
    \mathbf{Y}&=\mathbf{A}\mathbf{X}\mathbf{A}^T
\end{align*}
where $\mathbf{X}$ is the $4\times4$ pixel matrix, $\mathbf{Y}$ is the DCT coefficient matrix, $\mathbf{A}$ is the orthogonal DCT transform matrix.
% and could be factorized as $\mathbf{B}\mathbf{C}$ where $\mathbf{B}$ is diagonal. 
The above equation could be transformed into the following equivalent form~\cite{Richardson2003H2P}:

\vspace{-0.4cm}
\begin{align*}
    % \mathbf{Y}=(\mathbf{B}\mathbf{C})\mathbf{X}(\mathbf{B}\mathbf{C})^T=\mathbf{B}\mathbf{C}\mathbf{X}\mathbf{C}^T\mathbf{B}=(\mathbf{C}\mathbf{X}\mathbf{C}^T)\otimes\mathbf{E}
    \mathbf{Y}=(\mathbf{C}\mathbf{X}\mathbf{C}^T)\otimes\mathbf{E}
\label{eq-dct-1}
\end{align*}
where $\mathbf{E}$ is a scaling matrix, and $\otimes$ represents element-wise multiplication. $\mathbf{W}=\mathbf{C}\mathbf{X}\mathbf{C}^T$ is the unscaled coefficient matrix, which is quantized and scaled by:
\begin{equation}
\mathbf{Z}_{ij}=round\left(\mathbf{W}_{ij}\frac{\mathbf{M}_{ij}}{2^{(15+floor(\frac{QP}{6})}}\right)
\label{eq-dct-1}
\end{equation}
where $\mathbf{Z}$ is the quantized frequency coefficient matrix, $\mathbf{M}$ is a scaling matrix derived from $\mathbf{E}$. QP, i.e., the quantization parameter, is an integral value that controls bitrate by adjusting the quantization step. \textit{The larger the QP is, the less frequency information is reserved during the quantization, the lower the image quality, and the smaller the volume.} In H.264, QP ranges from 0 to 51.

To implement non-uniform QP assignments, we use the emphasis map feature provided by NVENCODE API~\cite{NVENCVideoEncoder}. This feature allows \textit{macroblock-level control of QP}, which enables the encoder to enhance the quality of geometry images in specified partitions. As illustrated in Fig.~\ref{fig-roi-encoding}, by setting a lower QP for RoIs of depth images, the quality of RoIs in the decoded point clouds is enhanced. In \methodname{}, we use a binary quality assignment strategy by applying two distinct QPsâ€”a low QP for RoIs and a high QP for non-RoI (background) areas.

\begin{figure*}[ht]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.2cm}
  \centering
    % \fbox{\rule{0pt}{5cm} \rule{\linewidth}{0pt}}
    \includegraphics[width=0.9\linewidth]{figs/roi_detector.pdf}
  \caption{Design of the GMM-based RoI detector.}
  \label{fig-roi-detector}
% \vspace{-0.2cm}
\end{figure*}

\subsubsection{Encoding Objective}
\label{sec-encoding-objective}
The objective of the RoI encoder can be formalized as:
\begin{equation}
\max_{\roimaskimg}\sum_{\backqp\in\backqpset}\accfunc(\backqp;\roiqp,\roimaskimg)
\label{eq_encobj}
\end{equation}
$\roimaskimg\in \{0,1\}^{\framembnum \times \framel}$ is the binary RoI macroblock indicator where $\framembnum, \framel$ denote the number of macroblocks in each frame and total number of frames of the geometry image sequence to be encoded; $\accfunc$ denotes the 3D object detector's accuracy function;  $\roiqp$ is the pre-defined QP of RoI; the non-RoI (background) QP $\backqp$ is sampled from a pre-defined QP value set $\backqpset$ to comprehensively evaluate $\roimaskimg$ under different background point cloud qualities.

Typically, RoIs are detected in 3D space. A RoI mask of a given point cloud can be denoted by $\roimaskpc \in \{0,1\}^{\pointnum}$ where $\pointnum$ is the number of points in the point cloud and $\roimaskpc[n]=1$ indicates the $n^{th}$ point belongs to RoI.  Typically, $\pointnum>\framembnum$. $\roimaskimg$ can be solved by:
\begin{equation}
    \ptpixmap_t\pixmbmap_t\roimaskimg[:,t] = {\roimaskpc_t}^T 
\end{equation}
where $\framepixnum$ is the number of pixels in the frame; $t$ is the frame index; $\ptpixmap \in \{0,1\}^{\pointnum \times \framepixnum}$ is the VPCC's point-to-pixel map encoded within the bitstream and $\pixmbmap \in \{0,1\}^{\framepixnum \times \framembnum}$ is the pixel-to-macroblock index map.

As such, the core of solving Eq.~\ref{eq_encobj} is to identify the RoI mask $\roimaskpc$ for any given point cloud. In the next section, we will elaborate on the design of our RoI detector, which efficiently detects 3D RoIs.

\subsection{RoI Detector}
An intuitive design for the RoI detector is based on a lightweight 3D object detector, which uses the predicted 3D bounding box as the RoIs.
% For instance, we tune the filtering threshold of non-maximum suppression~(NMS) to gain high-recall RoI bounding boxes. 
However, while object detectors try to precisely regress the bounding box parameters, our goal is to gain point-level importance scores \textit{without the need to distinguish between instances or predict the object classes.} This observation enables the RoI detector to be designed in a lightweight yet efficient way. 

% We conducted extensive experiments based on this solution, as shown in Table.~1. The performance of this method is even worse than the vanilla VPCC. The reason behind this is that the goal to ``detect objects'' is misaligned with our objective to ``find RoI points''. 


\subsubsection{GMM-based RoI}
Inspired by previous heatmap-based object predictors~\cite{yin2021center,zhao2023ada3d,law2018cornernet,zhou2019objects}, the RoI detector is designed to predict a heatmap indicating the possibility an object appears in a given region. 

As shown in Fig.~\ref{fig-roi-detector}, due to irregular object shapes and occlusion, points belonging to an object tend to be \textit{distributed in clusters}, and each cluster could be interpreted as a distinct component of that object. Thus, instead of using simple Gaussian distributions to model the object centers~\cite{yin2021center, zhou2019objects}, we propose to model the points of objects with 3D Gaussian mixture models (GMMs). Specifically, we assume the point cloud $\pointcloud = \{\mathbf{p}_\idxpoints\}_{\idxpoints=1}^\numpoints$ of an object is sampled from a 3D GMM:
\begin{equation}
\mathbf{p}_\idxpoints \sim \sum_{c=1}^{C} \pi_c \, \mathcal{N}(\mathbf{p} \mid \boldsymbol{\mu}_c, \boldsymbol{\Sigma}_c)
\end{equation}
where $C$ is the number of Gaussian components in the mixture. $\pi_c$ is the weight of the $c$-th Gaussian component, with $\sum_{c=1}^C \pi_k = 1$ and $\pi_c \geq 0$. We let $\pi_c=\frac{1}{C},\forall c\in\{1,\cdots,C\}$ as a general assumption for the priors. At training time, the component centers $\boldsymbol{\mu}_c \in \mathbb{R}^3$ and covariances $\boldsymbol{\Sigma}_c \in \mathbb{R}^{3 \times 3}$ is fitted to the points located inside of each ground-truth bounding box. 

\begin{algorithm}[t]
\caption{Find Points in Bounding Boxes}
\begin{algorithmic}[1]
\REQUIRE Point cloud $\pointcloud = \{\mathbf{p}_\idxpoints\}_{\idxpoints=1}^\numpoints$; A set of bounding boxes $\listofbboxs = \{B_\idxbbox\}_{\idxbbox=1}^\numbbox$, where $B_\idxbbox$ is defined by width $w_\idxbbox$, length $l_\idxbbox$, height $h_\idxbbox$ and box center $\mathbf{c}_\idxbbox$.
\ENSURE List of inner point clouds $\listofpointcloud_{in}=\{\pointcloud_\idxbbox\}_{\idxbbox=1}^\numbbox$
\STATE $\listofpointcloud_{in} \leftarrow \emptyset$
\STATE $\kdtree \leftarrow BuildKDTree(\pointcloud)$
\FOR{each bounding box $B_\idxbbox \in \listofbboxs$}
    \STATE $r_\idxbbox=\frac{1}{2}\sqrt{w_\idxbbox^2 + l_\idxbbox^2 + h_\idxbbox^2 }$
    \STATE $\pointcloud_{r} \leftarrow \kdtree.QueryBallPoints(\mathbf{c}_\idxbbox,r_\idxbbox)$
    \STATE $\convexhull_\idxbbox \leftarrow Delaunay(Corners(B_\idxbbox))$
    \STATE $\pointcloud_\idxbbox \leftarrow \convexhull_\idxbbox.PointsInAnySimplex(\pointcloud_r)$
    \STATE $\listofpointcloud_{in} \gets \listofpointcloud_{in} \cup \{\pointcloud_\idxbbox\}$
\ENDFOR
\RETURN $\listofpointcloud_{in}$
\end{algorithmic}
\label{alg-points-in-roi}
\end{algorithm}

To accelerate the labeling process, we provide an efficient algorithm to find points inside a set of 3D bounding boxes in Alg.~\ref{alg-points-in-roi}. The algorithm essentially leverages a reusable K-D tree to narrow the search space of each bounding box down to its circumscribed sphere. Then, we adopt Quickhull algorithm~\cite{barber1996quickhull} to accelerate the searching process of finding points inside the sphere. Specifically, it performs Delaunay triangulation to separate the bounding box into a set of simplexes $\convexhull$, i.e., tetrahedrons in 3D space. Then, Alg.~\ref{alg-points-in-roi} iterate through $\pointcloud_{r}$ and check whether the point falls in any of the simplexes and get the points inside of the RoI bounding boxes $\listofpointcloud_{in}$.

% GMM projected to heatmap
To enable a lightweight network design of the RoI detector, we transform 3D GMMs to 2D heatmaps $Y\in \mathbb{R}^{\pillarhight\times\pillarwidth}$. Specifically, we first project 3D GMMs on the x-y plane. Then, for heatmap grid $(i,j)$, its value is set to the maximum sampled probability across GMMs:
\begin{equation}
    \heatmap_{ij} = \max_{k} \sum_{c=1}^{C}  \frac{1}{C} \,  \mathcal{N} (\boldsymbol{\mu}_{ij} \mid \boldsymbol{\mu}^{'}_{kc}, \boldsymbol{\Sigma}^{'}_{kc})
\end{equation}
where $k\in\{1,\cdots,K\}$, $K$ is the total number of GMMs in the frame. $\boldsymbol{\mu}_{ij}$ is the center coordinate of the grid $(i,j)$, $\boldsymbol{\mu}^{'}$ and $\boldsymbol{\Sigma}^{'}$ are parameters of projected 2D GMMs. 




\subsubsection{Network Design}


\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{\TABVSPACE} % vertical spacing
    \setlength{\tabcolsep}{2pt} % horizontal spacing
    \caption{Complexities of RoI detectors.}
    \begin{tabular}{c|c|c|c|c}
    \hline
     Model      & \multicolumn{2}{c|}{GMM-based RoI} & \multicolumn{2}{c}{Naive RoI} \\
     
    \hline
    Method  & FLOPs (G) & Param (M) &  FLOPs (G) & Param (M)  \\ 
\cline{2-3}         \cline{4-5}
    \hline
        Backbone & 28.51 & 4.21 & 37.50 & 4.26 \\ 
        Header & 20.49 & 0.51 & 28.22 & 1.72 \\ 
        Total & 49.00 & 4.72 & 65.72 & 5.98 \\ 
    \hline
    \end{tabular}
    \label{tab-detector-capacity}
    \vspace{-0.4cm}
\end{table}


As shown in Fig.~\ref{fig-roi-detector}, we borrowed the backbone of PointPillar~\cite{lang2019pointpillars} due to its lightweight design and efficiency. Following the design of CenterPoint~\cite{yin2021center}, the head regresses on a pillar-wise RoI heatmap of $C\times200\times200$, where $C$ is the number of object classes. The regressed heatmap is supervised by Gaussian focal loss~\cite{law2018cornernet} separately on each class channel. 
At runtime, the heatmap $\heatmap$ produced by the network is binarized by a pre-defined threshold $\gamma$ to rule out low-confident regions. Then, the heatmap takes union across class channels to aggregate class-wise RoIs. The aggregated binary heatmap $\heatmap_b\in \{0,1\}^{200\times200}$ is then transformed to point-wise RoI mask $\roimaskpc_\heatmap$. 

Table.~\ref{tab-detector-capacity} lists the FLOPs and number of parameters of the proposed network. \textit{The overhead is below 50G FLOPs, and the number of parameters is less than 5M.} 

To suppress ground points included in $\roimaskpc_\heatmap$, we utilize the ground removal algorithm Patchwork++~\cite{lee2022patchworkpp} to generate the foreground mask $\roimaskpc_{f}$. Then, the final RoI mask is given by:
\begin{equation}
    \roimaskpc=\roimaskpc_\heatmap \cdot \roimaskpc_{f}
\end{equation}

