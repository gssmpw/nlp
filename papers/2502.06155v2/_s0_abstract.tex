\begin{abstract}
Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. 
For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; 
We identify a prevalent \emph{tile-style repetitive pattern} in the 3D attention maps for video data, and advocate %retaining only the tile structures to build 
a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 
2) Shorten the sampling process by adopting existing
%based on 
multi-step consistency distillation; 
We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. 
% method from Image DiTs with our sparse pattern to further accelerate the inference process. 
% It is empirically observed that naively merging such architectural and algorithmic notifications can lead to unstable training, so 
We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. 
% Our proposed method, ~\methodname turns a pre-trained 3D Full Attention DiT model into a DiT with linear compute complexity to the number of frames, and with fewer sampling steps. 
%modify the strong multi-step consistency distillation method from image DiTs 
%Consistency Models (CMs) addresses this problem for previous models by reducing redundancy between denoising steps. However, we find that the recent architecture, 3D Full Attention DiT, suffers from a new source of redundancy that CMs do not address. 
%In this paper, we first demonstrate that although patterns differ for each attention head, they remain static to different data inputs within one single attention head. Based on this analysis, we develop a framework~\methodname that enables a pre-trained 3D full attention DiT to compute fewer denoising step, while having linear complexity with respect to the number of frames generated. ~\methodname 
% Our approach can be data-efficiency, e.g., only 0.1\% pretraining data is required to obtain a low-cost video generator without compromising quality. %, and features a hardware-friendly sparse attention pattern during inference time. 
% Using~\methodname, w
Notably, with 0.1\% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is $7.4\times-7.8\times$ faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional $3.91 \times$ speedup when running on 4 GPUs with sequence parallelism.

%\dacheng{Add distributed result}
% Our method is also $1.76\times - 2.67\times$ faster than a pure Consistency-based approaches.
%\dacheng{Pick number 09.21.} 

%We show that~\methodname speed ups the inference Open-Sora-Plan 1.2 model for 8.8x

%To address this, we propose a hardware-efficient attention mechanism, ~\methodname, with linear complexity in sequence length. Through knowledge distillation, we preserve video quality with only 0.1% of the pretraining compute. Our method achieves a 1.48x speedup over pure CM and a 9x speedup over the original DiTs.

%Diffusion Transformers (DiTs) based models have shown great performance in synthesizing long and high fidelity videos, but also incurs a high inference cost due to the iterative sampling procedure. Consistency Models (CMs) accelerates the process by exploiting the redundancy between denosing steps. However, we discover that the recent architecture, 3D Full Attention DiTs, stills suffers from significant redundancy issue. Specifically, we discover a pattern,~\patternname, where the 3D attention maps exhibits tile-style repetitive pattern. Moreover, the pattern exhibited by different attention heads, remain the same for different inputs. Based on this observation, we develop a hardware-friendly attention~\methodname with linear complexity to the sequence length. We develop a knowledge distillation based method to preserve the quality the with only $0.1\%$ the pretraining compute. Using~\methodname, we achieve 1.48x speedup compared to a pure CM approach, and 9x speedup compared to the original DiTs.
\end{abstract}