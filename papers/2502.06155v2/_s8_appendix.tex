\appendix

\section{Extened layerwise search algorithm}
\label{appendix:search}
In this section, we explore how to balance the trade-off between inference speedup and output image quality. Intuitively, as the attention map becomes sparser, the inference time decreases, but the output image quality also degrades. With this model, we can answer the key question: \textbf{Given a target speedup or inference time, how can we achieve the highest possible image quality?} 

This problem is well-suited to latency constrained case because, in real-world applications, speedup can be precisely measured. Adjusting the generation quality within these constraints is therefore meaningful. Additionally, solving this problem allows us to approximate continuous speedup ratios as closely as possible using discrete masks, further validating the robustness of our algorithm.

\subsection{Estimation and Quantitative Analysis} 
The inference time can be quantitatively computed. Given time limitation $T_{\text{target}}$. Suppose we have a series of masks $M_1, M_2, \ldots, M_k$. For each mask, we can pre-profile its runtime as $T_1, T_2, \ldots, T_k$. If layer $j$ uses mask $a_j \in [1, k]$, the total inference time is given by $T = \sum_{j} T_{a_j} \le T_{\text{target}}$.

On the other hand, quantifying image quality is challenging. To address this, we make an assumption: the impact of different layers on image quality is additive. We use the loss as the value function, representing the output image quality as $\mathcal{L} = \sum_{j} \mathcal{L}_{j, a_j}$, where $\mathcal{L}_{j, a_j}$ denotes the loss value when layer $j$ uses mask type $a_j$.

\subsection{Lagrangian Relaxation Method}

By introducing a Lagrange multiplier $\lambda$, we construct the Lagrangian function:

\begin{equation}
L(\lambda) = \sum_j \mathcal{L}_{j,a_j} + \lambda \left( \sum_j T_{a_j} - T_{\text{target}} \right).
\end{equation}

Our goal is to minimize $L(\lambda)$, that is:

\begin{equation}
\min_{a_j} L(\lambda) = \min_{a_j} \left( \sum_j \mathcal{L}_{j,a_j} + \lambda \sum_j T_{a_j} \right) - \lambda T_{\text{target}}.
\end{equation}

Since $T_{\text{target}}$ is a constant, the optimization problem can be simplified into independent subproblems for each layer $j$:

\begin{equation}
\min_{a_j} \left( \mathcal{L}_{j,a_j} + \lambda T_{a_j} \right).
\end{equation}


\subsection{Lagrangian Subgradient Method}

\noindent\textbf{Input:} Initial Lagrange multiplier $\lambda^{(0)}$, learning rate $\alpha_t$, maximum iterations $N$.\\
\textbf{Output:} Approximate optimal solution $\{a_j\}$ and Lagrange multiplier $\lambda$.

\begin{enumerate}
    \item \textbf{Initialization:} Set iteration counter $t = 0$.
    \item \textbf{While} $t < N$ and not converged:
    \begin{enumerate}
        \item \textbf{Step 1: Solve Subproblems} \\
        For each layer $j$, solve the subproblem:
        \begin{equation}
        a_j^{(t)} = \arg\min_{a_j} \left( \mathcal{L}_{j,a_j} + \lambda^{(t)} T_{a_j} \right).
        \end{equation}
        \item \textbf{Step 2: Calculate Subgradient} \\
        Compute the subgradient:
        \begin{equation}
        g^{(t)} = \sum_j T_{a_j^{(t)}} - T_{\text{target}}.
        \end{equation}
        \item \textbf{Step 3: Update Lagrange Multiplier} \\
        Update $\lambda$ using the subgradient:
        \begin{equation}
        \lambda^{(t+1)} = \lambda^{(t)} + \alpha_t g^{(t)}.
        \end{equation}
        \item Update $t = t + 1$.
    \end{enumerate}
\end{enumerate}

\noindent\textbf{Output:} Return the approximate solution $\{a_j\}$ and the final Lagrange multiplier $\lambda$.

\section{FlexAttention implementation details}
\label{appendix:flex_attention}
%In this section, we aim to demonstrate that our method is \textbf{hardware-friendly}. 
The attention we design can be efficiently implemented by the native block-wise computation design in FlexAttention. %we designed operates at the block level. 
Compared to a dynamic implementations, our computations are static, allowing us to leverage static CUDA graphs for capturing or use PyTorch's \texttt{compile=True} feature. % In our implementation, we chose FlexAttention as our backend. 

FlexAttention employs a block-based mechanism that allows for efficient handling of sparse attention patterns. Specifically, when an empty block is encountered, the module automatically skips the attention computation, leveraging the sparsity in the attention matrix to accelerate calculations. The ability to skip computations in this manner results in significant speedups while maintaining efficient memory usage. % We tested our approach on a single A100-SXM 80GB GPU, and as shown in Table \ref{tab:flexattention_times_kernel}, the speedup of individual kernels increases as the sparsity level rises.

Additionally, FlexAttention is optimized by avoiding the need to materialize the entire mask. This mechanism enables FlexAttention to operate efficiently on large-scale models without incurring significant memory costs. For example, the additional memory usage of a model with 32 layers and a 29 frames mask is only 0.278GB, while a 93 frames mask requires 0.715GB of additional memory, which is considered minimal for large-scale models. By not needing to store or process the full mask, we save both memory and computation time, leading to improved performance, especially in scenarios where the attention matrix is highly sparse.

\begin{comment}
\begin{table}[h]
\centering
\caption{Results on Open-Sora-Plan with 93 frames and 720p resolution. We select motion smoothness and temporal flickering from VBench as they measure frame transition, which are crucial for sparse attention methods.}
\resizebox{0.40\columnwidth}{!}{
\begin{tabular}{cccc}
    \toprule
    \textbf{Model} & \makecell{\textbf{Motion} \\ \textbf{Smoothness}} & \makecell{\textbf{Temporal} \\ \textbf{Flickering}}  & \textbf{Speedup}  \\
    \midrule
    Base & 99.15\% & 98.76\%  & 1.00$\times$  \\ 
    MLCD & \textbf{99.30\%} & 99.22\%  & 5.00$\times$  \\ 
    \midrule
    $\text{Ours}_{r\text{=0.150}}$ & 99.08\% & \textbf{99.31}\%  & \textbf{7.40$\times$}   \\ 
    \bottomrule
\end{tabular}}
\label{tab:open_sora_results}
\end{table}
\end{comment}
% In Table \ref{tab:flexattention_times_kernel}, we present the kernel execution times of FlexAttention under varying sparsity levels, demonstrating the impact of the key-diff pattern mask on both memory usage and performance. When the sparsity level reaches 51.88\%, our kernel achieves more than a twofold speedup, fully utilizing the benefits of sparsity.

\section{Supplemental Vbench Evaluation}

\begin{comment}
\begin{table*}[h]

    \scriptsize \centering
\setlength{\tabcolsep}{4pt}
\caption{Supplemental VBench evaluation for main result.} 
\begin{tabular}{ccccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} &  \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color}  & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} \\
\midrule
Base & 23.25\% & 54.00\% & 94.47\% & 43.49\% & 18.60\% & 19.88\% & 18.45\% & 19.69\% & 97.64\% \\ 
MLCD & 19.21\% & 56.00\% & 94.12\% & 40.57\% & 22.67\% & 20.46\% & 18.21\% & 19.77\% & 97.98\% \\ 
\midrule
%\cmidrule{2-10}
$\text{Ours}_{r\text{=0.025}}$ & 18.83\% & 55.00\% & \textbf{96.25\%} & \textbf{46.02\%} & 12.35\% & \textbf{20.31\%} & 18.17\% & 19.11\% & 97.70\% \\ 
$\text{Ours}_{r\text{=0.050}}$ & 11.74\% &  \textbf{58.00\%} & 92.11\% & 39.81\% & \textbf{22.31\%} & 20.25\% & 17.71\% & \textbf{19.45\%} & \textbf{97.71\%} \\ 
$\text{Ours}_{r\text{=0.100}}$ & \textbf{18.98\%} & 56.00\% & 93.65\% & 43.88\% & 15.77\% & 20.20\% & 17.98\% & 19.29\% & 97.55\% \\ 
$\text{Ours}_{r\text{=0.200}}$ & 17.99\% & 53.00\% & 51.82\% & 36.14\% & 13.88\% & 20.29\% & 17.97\% & 18.97\% & 97.62\% \\ 
$\text{Ours}_{r\text{=0.400}}$ & 15.32\% & 54.00\% & 92.64\% & 37.05\% & 12.06\% & 20.24\% & \textbf{18.19\%} & 19.22\% & 97.66\% \\ 
\bottomrule
\end{tabular}
\label{tab::all_vbench}
\end{table*}
\end{comment}


\begin{table*}[h]
\scriptsize \centering
\setlength{\tabcolsep}{4pt}
\caption{Supplemental VBench evaluation for main result.} 
\begin{tabular}{cccccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} & \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color} & \makecell{\textbf{Dynamic} \\ \textbf{Degree}} & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} & \makecell{\textbf{Imaging} \\ \textbf{Quality}} \\
\midrule
Base & 23.25\% & 54.00\% & 94.47\% & 34.72\% & 43.49\% & 18.60\% & 19.88\% & 18.45\% & 19.69\% & 97.64\% & 64.75\% \\ 
MLCD & 19.21\% & 56.00\% & 94.12\% & 41.67\% & 40.57\% & 22.67\% & 20.46\% & 18.21\% & 19.77\% & 97.98\% & 65.55\% \\ 
\midrule
$\text{Ours}_{r\text{=0.025}}$ & 18.83\% & 55.00\% & \textbf{96.25\%} & 52.78\% & \textbf{46.02\%} & 12.35\% & \textbf{20.31\%} & 18.17\% & 19.11\% & 97.70\% & \textbf{58.90\%} \\ 
$\text{Ours}_{r\text{=0.050}}$ & 11.74\% & \textbf{58.00\%} & 92.11\% & 58.33\% & 39.81\% & \textbf{22.31\%} & 20.25\% & 17.71\% & \textbf{19.45\%} & \textbf{97.71\%} & 56.86\% \\ 
$\text{Ours}_{r\text{=0.100}}$ & \textbf{18.98\%} & 56.00\% & 93.65\% & 63.89\% & 43.88\% & 15.77\% & 20.20\% & 17.98\% & 19.29\% & 97.55\% & 54.88\% \\ 
$\text{Ours}_{r\text{=0.200}}$ & 17.99\% & 53.00\% & 51.82\% & 59.72\% & 36.14\% & 13.88\% & 20.29\% & 17.97\% & 18.97\% & 97.62\% & 54.07\% \\ 
$\text{Ours}_{r\text{=0.400}}$ & 15.32\% & 54.00\% & 92.64\% & \textbf{65.28\%} & 37.05\% & 12.06\% & 20.24\% & \textbf{18.19\%} & 19.22\% & 97.66\% & 54.36\% \\ 
\bottomrule
\end{tabular}
\label{tab::all_vbench}
\end{table*}


\newpage
\section{Ablation study}

\subsection{Ablation study of the effect of MLCD and layerwise search}
\label{appendix:abl}

%\updated{In this section, we conduct two ablation studies to thoroughly evaluate our method. First, we investigate the order of knowledge distillation and consistency distillation to verify their orthogonality. Second, we validate the generalizability of our attention distillation method by applying it to the CogVideoX model, which employs a different attention architecture. These ablation studies aim to demonstrate both the flexibility in our distillation pipeline and the broad applicability of our method across different model architectures.}

\textbf{Effect of MLCD} We conduct tests on VBench and CD-FVD, first comparing the differences between the Base model and the MLCD model, and then evaluating the compatibility of CM with the attention mask. As shown in Table \ref{tab:mlcd}, the MLCD model performs as well as or better than the Base model across most dimensions on VBench, achieving an overall VBench score of 76.81\%. Due to the MLCD model requiring fewer sampling steps than the Base model, it achieves a 5.00$\times$ speedup. Furthermore, we observe that the MLCD model, even after undergoing knowledge distillation, maintains performance without any drop in quality. The VBench score and CD-FVD trends are consistent, indicating that the MLCD model supports attention mask operations effectively, similar to the original model. Therefore, the MLCD model continues to deliver high-quality performance while offering significant acceleration benefits.

\begin{table*}[ht]
\scriptsize \centering
\caption{Ablation experiments on the effect of MLCD.}
\label{tab:mlcd}
\setlength{\tabcolsep}{4pt}

% 第一个表格（调整后）
\begin{tabular}{ccccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Final} \\ \textbf{Score}} $\uparrow$&  \makecell{\textbf{Aesthetic} \\ \textbf{Quality}} & \makecell{\textbf{Motion} \\ \textbf{Smoothness}} & \makecell{\textbf{Temporal} \\ \textbf{Flickering}} & \makecell{\textbf{Object} \\ \textbf{Class}} & \makecell{\textbf{Subject} \\ \textbf{Consistency}} & \makecell{\textbf{Imaging} \\ \textbf{Quality}} & \textbf{CD-FVD} $\downarrow$ & \textbf{Speedup} \\
 
\midrule
Base & 76.12\% & 58.34\% & \textbf{99.43}\% & 99.28\% & 64.72\% & \textbf{98.45\%} & 64.75\% & 172.64 & 1.00$\times$ \\ 
$\text{Base}_{4:4}$& 76.57\% & 58.64\% & 99.38\% & 99.20\% & \textbf{66.38\%} & 98.26\% & 63.56\% & \textbf{171.62} & 1.16$\times$ \\ 
$\text{Base}_{3:5}$ & 75.53\% & 55.47\% & 99.01\% & 98.96\% & 62.26\% & 97.42\% & 59.67\% & 197.35 & 1.26$\times$ \\ 
$\text{Base}_{2:6}$& 76.33\% & 57.14\% & 99.06\% & 99.02\% & 56.17\% & 97.58\% & 61.10\% & 201.61 & 1.45$\times$ \\ 
$\text{Base}_{1:7}$ & \textbf{77.15\%} & 57.53\% & 98.67\% & 98.66\% & 60.68\% & 96.96\% & 61.91\% & 322.28 & 1.77$\times$ \\ 
\midrule
MLCD & 76.81\% & \textbf{58.92}\% & 99.41\% & 99.42\% & 63.37\% & 98.37\% & \textbf{65.55}\%& 190.50 & 5.00$\times$ \\ 
$\text{MLCD}_{4:4}$ & 75.90\% & 57.84\% & 99.38\% & \textbf{99.50}\% & 63.03\% & 98.21\% & 58.47\%& 175.47 & 5.80$\times$ \\ 
$\text{MLCD}_{3:5}$  & 75.41\% & 57.19\% & 99.36\% & 99.50\% & 57.04\% & 98.12\% & 58.84\%& 190.92 & 6.30$\times$ \\ 
$\text{MLCD}_{2:6}$  & 75.23\% & 57.45\% & 99.29\% & 99.48\% & 54.59\% & 98.37\% & 57.35\%& 213.72 & 7.25$\times$ \\ 
$\text{MLCD}_{1:7}$  & 75.84\% & 56.83\% & 98.99\% & 99.23\% & 52.77\% & 97.54\% & 56.42\%& 294.09 & \textbf{8.85$\times$} \\ 
\bottomrule
\end{tabular}

\vspace{+5mm}
% 第二个表格（调整后）
\begin{tabular}{ccccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} &  \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color} & \makecell{\textbf{Dynamic} \\ \textbf{Degree}} & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} \\
\midrule
Base & 23.25\% & 54.00\% & \textbf{94.47\%} & 34.72\% & 43.49\% & 18.60\% & 19.88\% & \textbf{18.45\%} & 19.69\% & 97.64\% \\ 
$\text{Base}_{4:4}$ & \textbf{32.01\%} & 55.00\% & 90.94\% & 43.06\% & \textbf{45.42\%} & 17.30\% & 20.21\% & 18.41\% & 19.48\% & 97.17\% \\ 
$\text{Base}_{3:5}$ & 15.85\% & 53.00\% & 88.88\% & 58.33\% & 44.38\% & 14.53\% & 20.13\% & 17.46\% & 18.43\% & 97.28\%  \\ 
$\text{Base}_{2:6}$ & 21.65\% & 56.00\% & 93.27\% & 56.94\% & 49.90\% & 18.31\% & 19.87\% & 18.23\% & 18.94\% & 97.27\% \\ 
$\text{Base}_{1:7}$& 17.76\% & 54.00\% & 93.02\% & 75.00\% & 44.75\% & 19.99\% & 19.95\% & 18.25\% & 19.41\% & 97.30\%  \\ 
\midrule
MLCD & 19.21\% & \textbf{56.00\%} & 94.12\% & 41.67\% & 40.57\% & \textbf{22.67\%} & \textbf{20.46\%} & 18.21\% & \textbf{19.77\%} & \textbf{97.98\%} \\ 
$\text{MLCD}_{4:4}$ & 22.79\% & 53.00\% & 92.69\% & 50.00\% & 39.80\% & 17.51\% & 19.89\% & 18.32\% & 19.06\% & 97.30\% \\ 
$\text{MLCD}_{3:5}$ & 22.10\% & 50.00\% & 90.82\% & 43.06\% & 43.48\% & 21.44\% & 19.97\% & 17.68\% & 19.75\% & 97.47\% \\ 
$\text{MLCD}_{2:6}$ & 18.60\% & 53.00\% & 92.52\% & 44.44\% & 43.36\% & 16.21\% & 19.89\% & 17.84\% & 20.12\% & 97.70\%  \\ 
$\text{MLCD}_{1:7}$ & 16.92\% & 53.00\% & 91.92\% & 63.89\% & 43.27\% & 17.22\% & 19.94\% & 18.56\% & 19.85\% & 97.45\% \\ 
\bottomrule
\end{tabular}
\end{table*}

% \begin{tabular}{ccccccccccc}
% \toprule 
% \textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} &  \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color}  & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} \\
% \midrule
% Base & 23.25\% & 54.00\% & 94.47\% & 43.49\% & 18.60\% & 19.88\% & 18.45\% & 19.69\% & 97.64\% \\ 
% $\text{Base}_{4:4}$ & 32.01\% & 55.00\% & 90.94\% & 45.42\% & 17.30\% & 20.21\% & 18.41\% & 19.48\% & 97.17\% \\ 
% $\text{Base}_{3:5}$ & 15.85\% & 53.00\% & 88.88\% & 44.38\% & 14.53\% & 20.13\% & 17.46\% & 18.43\% & 97.28\% \\ 
% $\text{Base}_{2:6}$ & 21.65\% & 56.00\% & 93.27\% & 49.90\% & 18.31\% & 19.87\% & 18.23\% & 18.94\% & 97.27\%\\ 
% $\text{Base}_{1:7}$& 17.76\% & 54.00\% & 93.02\% & 44.75\% & 19.99\% & 19.95\% & 18.25\% & 19.41\% & 97.30\% \\ 
% %\cmidrule{1-10}
% \midrule
% MLCD & 19.21\% & 56.00\% & 94.12\% & 40.57\% & 22.67\% & 20.46\% & 18.21\% & 19.77\% & 97.98\%\\ 
% $\text{MLCD}_{4:4}$ & 22.79\% & 53.00\% & 92.69\% & 39.80\% & 17.51\% & 19.89\% & 18.32\% & 19.06\% & 97.30\%\\ 
% $\text{MLCD}_{3:5}$ & 22.10\% & 50.00\% & 90.82\% & 43.48\% & 21.44\% & 19.97\% & 17.68\% & 19.75\% & 97.47\%\\ 
% $\text{MLCD}_{2:6}$ & 18.60\% & 53.00\% & 92.52\% & 43.36\% & 16.21\% & 19.89\% & 17.84\% & 20.12\% & 97.70\% \\ 
% $\text{MLCD}_{1:7}$ & 16.92\% & 53.00\% & 91.92\% & 43.27\% & 17.22\% & 19.94\% & 18.56\% & 19.85\% & 97.45\%\\ 
% \bottomrule
% \end{tabular}

\textbf{Effect of Layerwise Search} We conduct tests on VBench and CD-FVD, selecting the MLCD model as the baseline. We compare applying a uniform mask across all layers (e.g., 4:4, 3:5) with the layerwise mask from Algorithm \ref{alg:search}. As shown in Table \ref{tab::ablation_layer}, in VBench, using the layerwise mask with ($r$ = 0.025, 0.050, 0.100) achieve a score exceeding 76.00\%, significantly outperforming the results without layerwise masking, while also providing a better speedup (7.05$\times$ vs. 5.80$\times$). In CD-FVD, the layerwise mask consistently results in scores below 250. However, as sparsity increases, the score without layerwise masking exceeds 250, indicating a decrease in video generation quality. Therefore, the layerwise approach enhances the quality of generated videos.

\begin{table*}[h]
    \scriptsize \centering
    \setlength{\tabcolsep}{4pt}
    \caption{Ablation experiments on the effect of our layerwise searching algorithm.}
    \label{tab::ablation_layer}
    
    % 第一部分表格（移除Dynamic Degree列）
    \begin{tabular}{ccccccccccc}
    \toprule 
    \textbf{Model} & \makecell{\textbf{Final} \\ \textbf{Score}} $\uparrow$&  \makecell{\textbf{Aesthetic} \\ \textbf{Quality}} & \makecell{\textbf{Motion} \\ \textbf{Smoothness}} & \makecell{\textbf{Temporal} \\ \textbf{Flickering}} & \makecell{\textbf{Object} \\ \textbf{Class}} & \makecell{\textbf{Subject} \\ \textbf{Consistency}} & \makecell{\textbf{Imaging} \\ \textbf{Quality}} & \textbf{CD-FVD} $\downarrow$ & \textbf{Speedup} \\
    \midrule
    MLCD & \textbf{76.81\%} & \textbf{58.92\%} & \textbf{99.41\%} & 99.42\% & \textbf{63.37\%} & \textbf{98.37\%} & \textbf{65.55\%} & 190.50 & 5.00$\times$ \\ 
    $\text{MLCD}_{4:4}$ & 75.90\% & 57.84\% & 99.38\% & 99.50\% & 63.03\% & 98.21\% & 58.47\%& \textbf{175.47} & 5.80$\times$ \\ 
    $\text{MLCD}_{3:5}$  & 75.41\% & 57.19\% & 99.36\% & 99.50\% & 57.04\% & 98.12\% & 58.84\%& 190.91 & 6.30$\times$ \\ 
    $\text{MLCD}_{2:6}$  & 75.23\% & 57.45\% & 99.29\% & 99.48\% & 54.59\% & 98.37\% & 57.35\%& 213.71 & 7.25$\times$ \\ 
    $\text{MLCD}_{1:7}$  & 75.84\% & 56.83\% & 98.99\% & 99.23\% & 52.77\% & 97.54\% & 56.42\%& 294.09 & \textbf{8.85$\times$} \\ 
    \midrule
    $\text{Ours}_{r\text{=0.025}}$& 76.14\% & 57.21\% & 99.37\% & 99.49\% & 60.36\% & 98.26\% & 58.90\%& 186.84 & 5.85$\times$ \\ 
    $\text{Ours}_{r\text{=0.050}}$& 76.01\% &  57.57\% & 99.15\% & \textbf{99.56\%} & 58.70\% & 97.58\% & 56.86\%& 195.55 & 6.60$\times$ \\ 
    $\text{Ours}_{r\text{=0.100}}$ & 76.00\% & 56.59\% & 99.13\% & 99.54\% & 57.12\% & 97.73\% & 54.88\%& 204.13 & 7.05$\times$ \\ 
    $\text{Ours}_{r\text{=0.200}}$ & 75.02\% & 55.71\% & 99.03\% & 99.50\% & 55.22\% & 97.28\% & 54.07\%& 223.75 & 7.50$\times$ \\ 
    $\text{Ours}_{r\text{=0.400}}$& 75.30\% & 55.79\% & 98.93\% & 99.46\% & 54.98\% & 97.71\% & 54.36\%& 231.68 & 7.80$\times$ \\ 
    \bottomrule
    \end{tabular}

    \vspace{+5mm}
    
    % 第二部分表格（Dynamic Degree在Color列之后）
    \begin{tabular}{ccccccccccccc}
    \toprule 
    \textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} &  \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color} & \makecell{\textbf{Dynamic} \\ \textbf{Degree}} & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} \\
    \midrule
    MLCD & 19.21\% & 56.00\% & 94.12\% & 41.67\% & 40.57\% & \textbf{22.67\%} & \textbf{20.46\%} & 18.21\% & 19.77\% & \textbf{97.98\%} \\ 
    $\text{MLCD}_{4:4}$ & \textbf{22.79\%} & 53.00\% & 92.69\% & 50.00\% & 39.80\% & 17.51\% & 19.89\% & 18.32\% & 19.06\% & 97.30\%  \\ 
    $\text{MLCD}_{3:5}$ & 22.10\% & 50.00\% & 90.82\% & 43.06\% & 43.48\% & 21.44\% & 19.97\% & 17.68\% & 19.75\% & 97.47\%  \\ 
    $\text{MLCD}_{2:6}$ & 18.60\% & 53.00\% & 92.52\% & 44.44\% & 43.36\% & 16.21\% & 19.89\% & 17.84\% & \textbf{20.12\%} & 97.70\%  \\ 
    $\text{MLCD}_{1:7}$ & 16.92\% & 53.00\% & 91.92\% & 63.89\% & 43.27\% & 17.22\% & 19.94\% & \textbf{18.56\%} & 19.85\% & 97.45\% \\ 
    \midrule 
    $\text{Ours}_{r\text{=0.025}}$ & 18.83\% & 55.00\% & \textbf{96.25\%} & 52.78\% & \textbf{46.02\%} & 12.35\% & 20.31\% & 18.17\% & 19.11\% & 97.70\% \\ 
    $\text{Ours}_{r\text{=0.050}}$ & 11.74\% &  \textbf{58.00\%} & 92.11\% & 58.33\% & 39.81\% & 22.31\% & 20.25\% & 17.71\% & 19.45\% & 97.71\% \\ 
    $\text{Ours}_{r\text{=0.100}}$ & 18.98\% & 56.00\% & 93.65\% & 63.89\% & 43.88\% & 15.77\% & 20.20\% & 17.98\% & 19.29\% & 97.55\% \\ 
    $\text{Ours}_{r\text{=0.200}}$ & 17.99\% & 53.00\% & 51.82\% & 59.72\% & 36.14\% & 13.88\% & 20.29\% & 17.97\% & 18.97\% & 97.62\%  \\ 
    $\text{Ours}_{r\text{=0.400}}$ & 15.32\% & 54.00\% & 92.64\% & \textbf{65.28\%} & 37.05\% & 12.06\% & 20.24\% & 18.19\% & 19.22\% & 97.66\%  \\ 
    \bottomrule
    \end{tabular}
\end{table*}


%\subsection{Ablation study of knowledge distillation and consistency distillation order}
%\label{appendix:order}

\begin{table*}[h]
\scriptsize \centering
\setlength{\tabcolsep}{4pt}
\caption{VBench evaluation result for ablation study on distillation order for MLCD and layerwise knowledge distillation.}
\begin{tabular}{cccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Final} \\ \textbf{Score}} $\uparrow$&  \makecell{\textbf{Aesthetic} \\ \textbf{Quality}} & \makecell{\textbf{Dynamic} \\ \textbf{Degree}}  & \makecell{\textbf{Motion} \\ \textbf{Smoothness}} & \makecell{\textbf{Temporal} \\ \textbf{Flickering}} & \makecell{\textbf{Object} \\ \textbf{Class}} & \makecell{\textbf{Subject} \\ \textbf{Consistency}} & \makecell{\textbf{Imaging} \\ \textbf{Quality}} & \textbf{CD-FVD} $\downarrow$ \\
\midrule
 MLCD + KD & 76.00\% & 56.59\% & 63.88\% & 99.13\% & 99.54\% & 57.12\% & 97.73\% & 54.88\% & 204.13 \\
 KD + MLCD & 75.50\% & 56.38\% & 54.16\% & 99.12\% & 99.40\% & 54.67\% & 97.71\% & 57.97\% & 203.52  \\
\bottomrule
\end{tabular}

\vspace{+5mm}

\begin{tabular}{cccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} &  \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color}  & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} \\
\midrule
MLCD + KD & 18.97\% & 0.56\% & 93.65\% & 43.87\% & 15.77\% & 20.20\% & 17.98\% & 19.29\% & 97.55\% \\
KD + MLCD & 17.22\% & 0.53\% & 93.14\% & 39.87\% & 17.65\% & 20.11\% & 18.01\% & 19.17\% & 97.69\% \\
\bottomrule
\end{tabular}

\label{tab:ablation_order}
\end{table*}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/prompt_sample/ablation.jpg}
  %\vspace{-5pt}
  \caption{Qualitative samples of ablation of distillation order. sampled from VBench prompts. We show that both MLCD  and~\methodname~model can simliar quality on these samples. In two consecutive videos, the top shows results from MLCD + CD model followed by KD + MLCD model.}
  \label{fig:vbench_abl}
  %\vspace{-5pt}
\end{figure}

\clearpage

%\begin{comment}
\section{Attention distill on CogVideoX model}
\label{appendix:cogvideo}

We show that attention distillation also works well on the CogVideoX \cite{yang2024cogvideox} model. CogVideoX is based on the MM-DiT architecture, where its attention module concatenates text tokens with video tokens, which differs from Open-Sora-Plan's cross attention module. This demonstrates that our method works effectively on both MM-DiT and cross attention architectures. Our experiments are conducted on the CogVideoX-5B model with 49-frame generation capability.

\textbf{Implementation Details} CogVideoX-5B is profiled using Algorithm \ref{alg:search}. For training, the model is trained for a total of 10,000 steps, equivalent to 10 epochs of the dataset. The learning rate is set to 1e-7, and the gradient accumulation step is set to 1. The diffusion scale factor $\lambda$ is set to 1.

\textbf{Kernel Performance} We analyze the computation time for a single sparse attention kernel in Table \ref{tab:cog_kernel}. The results show that as sparsity increases, computation time decreases significantly. For instance, with a 2:11 attention mask, the execution time reduces to 15.16ms, achieving a 1.72$\times$ speedup compared to the full mask.

\begin{table*}[h]
\small \centering
\caption{CogvideoX-5B model speedup with different masks.}
\begin{tabular}{cccc}
\toprule
\textbf{Mask} & \textbf{Sparsity (\%)} & \textbf{Time(ms)} & \textbf{Speedup} \\
\midrule 
full & 0.00 & 26.03 & 1.00$\times$ \\
1 & 14.50 & 24.12 & 1.08$\times$ \\
2 & 29.29 & 23.68 & 1.10$\times$ \\
3 & 38.30 & 20.51 & 1.27$\times$ \\
4 & 48.66 & 17.77 & 1.47$\times$ \\
6 & 60.15 & 14.08 & 1.85$\times$ \\
12 & 74.11 & 9.99 & 2.60$\times$ \\
\bottomrule
\vspace{-2mm}
\label{tab:cog_kernel}
\end{tabular}
\end{table*}

\textbf{Evaluation} For quantitative analysis, we show the VBench evaluation results of the knowledge distillation model in Table \ref{tab:cog_vbench}. The results of our model are within 1\% of the final score with no noticeable drop in several key dimensions. Our model achieves comparable performance to the original model. For qualitative analysis, we present sample visualizations in Figure \ref{fig:cog} to demonstrate the video generation quality. These evaluations show that our method maintains similar video quality while achieving significant speedup, validating its effectiveness across different video diffusion model architectures.

\begin{table*}[h]
\scriptsize \centering
\setlength{\tabcolsep}{4pt}
\caption{CogVideoX-5B with 49 frames and 480p resolution results on VBench. `$r$=4.0' indicates that this checkpoint was trained using the layerwise search strategy described in Algorithm \ref{alg:search}, with a threshold of $r$=4.0.}
\begin{tabular}{cccccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Final} \\ \textbf{Score}} $\uparrow$ & \makecell{\textbf{Aesthetic} \\ \textbf{Quality}} & \makecell{\textbf{Dynamic} \\ \textbf{Degree}}  & \makecell{\textbf{Motion} \\ \textbf{Smoothness}} & \makecell{\textbf{Temporal} \\ \textbf{Flickering}} & \makecell{\textbf{Object} \\ \textbf{Class}} & \makecell{\textbf{Subject} \\ \textbf{Consistency}} & \makecell{\textbf{Imaging} \\ \textbf{Quality}} & \textbf{Speedup} \\
\midrule
Base & 77.91\% & 57.91\% & 76.39\% & 97.83\% & 97.34\% & 71.99\% & 92.27\% & 57.78\% & 1.00$\times$ \\
$\text{Ours}_{r\text{=5}}$ & 77.15\% & 51.18\% & 86.11\% & 96.67\% & 97.18\% & 77.06\% & 90.89\% & 55.75\% & 1.34$\times$\\
%$\text{Ours}_{r\text{=50}}$ & & & & & & & & & & 1.73$\times$\\
\bottomrule
\vspace{+0.5mm}
\end{tabular}

\begin{tabular}{cccccccccc}
\toprule 
\textbf{Model} & \makecell{\textbf{Multiple} \\ \textbf{Objects}} &  \makecell{\textbf{Human} \\ \textbf{Action}} & \textbf{Color}  & \makecell{\textbf{Spatial} \\ \textbf{Relationship}} & \textbf{Scene} & \makecell{\textbf{Appearance} \\ \textbf{Style}} & \makecell{\textbf{Temporal} \\ \textbf{Style}} & \makecell{\textbf{Overall} \\ \textbf{Consistency}} & \makecell{\textbf{Background} \\ \textbf{Consistency}} \\
\midrule
Base & 48.62\% & 84.00\% & 86.71\% & 48.47\% & 38.01\% & 22.99\% & 23.22\% & 26.13\% & 95.01\% \\
$\text{Ours}_{r\text{=5}}$ & 39.17\% & 90.00\% & 83.58\% & 46.00\% & 36.92\% & 23.20\% & 23.40\% & 26.02\% & 93.95\% \\
%$\text{Ours}_{r\text{=50}}$ & & & & & & & & &  \\
\bottomrule
\end{tabular}
\label{tab:cog_vbench}
\end{table*}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,height=0.90\textheight]{figures/prompt_sample/cogvideo_batch.jpg}
  %\vspace{-5pt}
  \caption{Qualitative samples of CogvideoX-5B \cite{yang2024cogvideox} distillation from its sample prompts. We show that our attention distill is capable of MM-DiT model architecture. In two consecutive videos, the top shows results from the base model, followed by the distillation model.}
  \label{fig:cog}
  %\vspace{-5pt}
\end{figure}

%\end{comment}



\section{Qualitative samples of dynamic scenes and large-scale motion }
\label{appendix:sample}

In this section, we compare the generation quality between the base model and the distilled model. For a better demonstration of \methodname, we highly recommend viewing the video file in the supplementary material.

For the figures listed below, in Fig. \ref{fig:vis2}, we demonstrate that our model is capable of generating large-scale motion effects such as centralized radiating explosions.
In Figs. \ref{fig:vbench1} and \ref{fig:vbench3}, we show a series of samples from VBench prompts, demonstrating our model's motion generation capabilities. % and providing better insights into the VBench scoring results.

%In Fig.\ref{fig:vis3}, we showcase our model's ability to generate dramatic physical movements, such as superhumans unleashing power in mid-air.

\begin{figure}[t]
  \centering
  \includegraphics[page=5,width=\linewidth]{figures/prompt_sample/QuanlitiveResult.pdf}
  \includegraphics[page=6,width=\linewidth]{figures/prompt_sample/QuanlitiveResult.pdf}
  %\vspace{-5pt}
  \caption{Based on Open-Sora's examples \cite{opensora}  , we selected dynamic prompts featuring centralized explosions and radiating energy, demonstrating dramatic transitions from focal points to expansive environmental transformations, emphasizing large-scale motion.}
  % \hangliang {@runlong, update qualitative results}
  % \hangliang {@runlong, make it tight}
  \label{fig:vis2}
  %\vspace{-5pt}
\end{figure}

\begin{comment}
\begin{figure}[t]
  \centering
  \includegraphics[page=4,width=\linewidth]{figures/prompt_sample/Quanlitive Result.pdf}
  \includegraphics[page=3,width=\linewidth]{figures/prompt_sample/Quanlitive Result.pdf}
  %\vspace{-5pt}
  \caption{Dynamic prompts featuring forceful physical movements (like kicking) and swirling environmental effects (like waves), transitioning from calm to intense states.}
  % \hangliang {@runlong, update qualitative results}
  % \hangliang {@runlong, make it tight}
  \label{fig:vis3}
  %\vspace{-5pt}
\end{figure}
\end{comment}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,height=0.90\textheight]{figures/prompt_sample/output_batch_1.jpg}
  %\vspace{-5pt}
  \caption{Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD  and~\methodname~model can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and the~\methodname~model.}
  \label{fig:vbench1}
  %\vspace{-5pt}
\end{figure}

\begin{comment}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,height=0.90\textheight]{figures/prompt_sample/output_batch_2.jpg}
  %\vspace{-5pt}
  \caption{Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD  and~\methodname~model can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and the~\methodname~model.}
  \label{fig:vbench2}
  %\vspace{-5pt}
\end{figure}
\end{comment}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth,height=0.90\textheight]{figures/prompt_sample/output_batch_3.jpg}
  %\vspace{-5pt}
  \caption{Qualitative samples of dynamic scenes from VBench prompts. We show that both MLCD  and~\methodname~model can generate dynamic videos while maintaining video quality. In three consecutive videos, the top shows results from the base model, followed by the MLCD model, and the~\methodname~model.}
  \label{fig:vbench3}
  %\vspace{-5pt}
\end{figure}

