\section{Conclusion}
In this paper, we first describe the phenomenon of~\patternname, and dive into its characteristics of repetitive, large diagonals, locality, and data independent. Then we describe a class of sparse attention pattern tailored to address the efficiency problem in~\patternname. Lastly, we introduce our overall framework that leveraged this class of sparse attention, which further leverages multi-step consistency distillation, layerwise searching, and knowledge distillation for faster generation and high performance. Experiments on two varaints of the Open-Sora-Plan model has demonstrated that our method can achieve similar performance, with 0.1\% the pre-training data, and up to $7.8\times$ speedup. Further ablation study has shown that our method can be natively integrated with advanced parallelism method to achieve further speedup.

\section{Impact Statement}

This paper presents work whose goal is to advance the field
of Machine Learning. As highlighted in ~\citep{mirsky2020creation}, such generative technologies can impact media authenticity, privacy, and public trust. We acknowledge these potential impacts and emphasize that our research is intended to advance the scientific understanding of machine learning while encouraging responsible development and deployment of these technologies.

% \hangliang{@DL, add conclusion here to fill up 10 pages.}