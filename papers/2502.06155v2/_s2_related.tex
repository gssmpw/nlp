
\section{Related Work}

\begin{figure*}[ht]
    \centering
        \includegraphics[width=\textwidth]{figures/method.pdf}
        \caption{~\methodname~takes in a pre-trained 3D Full Attention video diffusion transformer(DiT), with slow inference speed and high fidelity. It then operates on three stages to greatly accelerate the inference while maintaining the fidelity. In Stage 1, we modify the multi-step consistency distillation framework from~\citep{heek2024multistep} to the video domain, which turned a DiT model to a CM model with \textit{stable} training. In Stage 2,~\methodname~performs a searching algorithm to find the best sparse attention pattern for each layer. In stage 3,~\methodname~performs a knowledge distillation procedure to optimize the fidelity of the sparse DiT. At the end,~\methodname~outputs a DiT with linear attention, high fidelity and fastest inference speed.}
     %~\hangliang {using 8x8 to show pattern is better?}
    %promptA:An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt, he wears a brown beret and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm film.
    %promptB:a close-up shot of a woman standing in a dimly lit room. she is wearing a traditional chinese outfit, which includes a red and gold dress with intricate designs and a matching headpiece. the woman has her hair styled in an updo, adorned with a gold accessory. her makeup is done in a way that accentuates her features, with red lipstick and dark eyeshadow. she is looking directly at the camera with a neutral expression. the room has a rustic feel, with wooden beams and a stone wall visible in the background. the lighting in the room is soft and warm, creating a contrast with the woman's vibrant attire. there are no texts or other objects in the video. the style of the video is a portrait, focusing on the woman and her attire.
    \label{fig:method}
    \vspace{-5mm}
\end{figure*}


\textbf{Video Diffusion Transformers} There is a rich line of research in diffusion based models for video generation~\citep{ho2022video, he2022latent, VideoFusion, wang2023lavie, ge2023preserve, chen2024videocrafter2, guo2023sparsectrl, guo2023animatediff}. More recently, ~\citet{peebles2023scalable} introduces the architecture of Diffusion Transformers (DiTs), and several popular video generation models have been developed using the DiTs backbone, for instance, ~\citet{ma2024latte, opensora, pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109, yang2024cogvideox}. More specifically, ~\citet{ pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109, yang2024cogvideox} has explored the use of 3D Full Attention Transformers, which jointly model spatial and temporal relationship, instead of previous models that separately model spatial and temporal relationship (e.g. one Transformer layer with spatial attention and the other with temporal attention~\citep{opensora, ma2024latte}). The design of 3D full attention has gained increasing popularity due to their promising performance. In this work, we tackle the efficiency problem specifically for 3D full attention diffusion Transformers. In addition, there is a line of research that combines video diffusion model with sequential or autoregressive generation. These methods may also achieve speedup due to their use of shorter sequence length. ~\methodname~aims to speedup in a single diffusion forward, which is compatible with orthogonal to autoregressive manner methods~\citep{henschel2024streamingt2v, xiang2024pandora, chen2024diffusion, valevski2024diffusion}. 

\textbf{Accelerating diffusion inference} 
Many work in diffusion models have been proposed to reduce the number of sampling steps to accelerate diffusion inference~\citep{song2020denoising, lu2022dpm, lu2022dpm++}~\citep{liu2024scott}.  ~\citet{song2023consistency} proposes the consistency models which distills multiple steps ODE to one step. ~\citet{wang2023videolcm} extends CMs to video generation model. ~\citet{li2024t2v} further extends the idea with reward model to speed up video diffusion model inference. 
Another line of research that accelerates diffusion models inference utilize multiple devices~\citep{li2024distrifusion, wang2024pipefusion, chen2024asyncdiff, zhao2024real}. These works exploit the redundancy between denoising steps and use stale activations in distributed inference to hide communication overhead, and are naturally incompatible with work that reduce the redundancy between steps. In this work, we exploit the redundancy in attention computation, which is orthogonal to works that leverage distributed acceleration and redundancy between denoising steps. Our pipeline integrates a multi-step CM approach~\citep{xie2024mlcm} by default, and in experiment, we show that it can also seaminglessly integrate with parallel inference.
% \dacheng{Add DDIM line of work and PAB}

\textbf{Sparsity in Transformer inference} has been investigated in the context of Large Language Models (LLMs) inference, which can be decomposed into pre-filling and decoding stages~\citep{yu2022orca}. StreamingLLM discovers the pattern of Attention Sink, and keeps a combination of first few tokens and recent decoded tokens during decoding phrase~\citep{xiao2023efficient}. ~\citet{zhang2024q, zhang2024h2o} adaptively identify the most significant tokens during test time. Video DiTs have different workload than LLMs, where DiTs perform a single forward in each diffusion step without a decoding phrase. In particular, our paper is among the first to explore sparse attention in the context of 3D Full Attention DiTs. In addition, our finding that ~\patternname~is data-independent motivates us to design a solution which does not require inference time adaptive searching, which is a bottleneck in work such as~\citet{zhang2024h2o}. Sparsity has also been studied in Gan and other diffusion-based models, yet we focus on the new architecture 3D DiT~\citep{li2020gan, li2022efficient}. A recent paper~\citep{wang2024qihoo} also discusses the redundancy in DiTs models, but no performance has been shown.% by the date of this paper is written.


%\dacheng{add QIHOO-T2X: AN EFFICIENCY-FOCUSED DIFFUSION
%TRANSFORMER VIA PROXY TOKENS FOR TEXT-TOANY-TASK to bib when it is ready}
%QIHOO-T2X uses redundancy in visual information in DiT. ~\citep{jiang2024minference} studies the sparse pattern in LLM inference.

% \textbf{Sparsity in Transformer Inference}

% \textbf{2D versus 3D attention} community finds 3d is better.

%\dacheng{In related work, we can discuss about the shortcomings in PAB, distrifusion}