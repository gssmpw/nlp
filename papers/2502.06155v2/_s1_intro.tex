
\section{Introduction}
\label{sec:intro}
%\dacheng{Paragrah 1 - DiT based video generation mdoels and the problem.}
%\dacheng{Pattern characteristics: The attention map exhibits patch by patch pattern, where each patch is mostly the same except the main diagonal.}
Diffusion Transformers (DiTs) based video generators can synthesize long-horizon, high-resolution, and high-fidelity videos~\citep{peebles2023scalable, openai_sora, kuaishou_kling, pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109, opensora, esser2023structure, yang2024cogvideox}. 
The 3D attention is a core module of such models. It flattens both the spatial and temporal axes of the video data into one long sequence for attention computation and reports state-of-the-art generation quality~\citep{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109, yang2024cogvideox, huang2024vbench}. 
Computation of 3D attention often consumes the majority of the time during the entire forward propagation of a 3D DiT, especially with long sequences when generating extended videos. 
Thus, existing 3D DiTs suffer from prohibitively slow inference due to the slow attention computation and the multi-step diffusion sampling procedure. 

% However, DiTs suffer from slow inference due to the iterative sampling process. 

% To accelerate this, ~\citet{song2023consistency} introduces Consistency Models (CMs), which reduce the number of sampling iterations by exploiting the redundancy between denoising steps.

%\dacheng{Talk about Attention Tile first, then say CM is a thing, and then say overall;} 
% While CMs have been successful for previous video DiTs models that \textit{separately} model temporal and spatial relationship~\citep{li2024t2v}, we find that it has not sufficiently address the efficiency problem in the recent better DiTs with 3D full attention (3D DiTs), which \textit{jointly} models both spatial and temporal relationship in a single attention compution~\citep{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109, yang2024cogvideox}. For instance, after apply CMs, ~\citep{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109} still takes xxx seconds to generate a video of 3 seconds 30 fps video. This is because 3D attention DiT is majorly bottlenecked by the attention computation, whose efficiency issue has not been addressed by CM based approach.
%This is because 3D attention suffers from slow attention computation, which is not addressed by


%Due to the 3D Attention, these DiTs consume excessive number of tokens, whose computation is quadratic to the number of tokens. For instance, we found that a single diffusion step in the Open-Sora-Plan v1.2 model takes 86k tokens for a 93 frame and 720p video, counting for x\% the total computation time. We also found that using CMs, which do not reduce the attention computation, this is xxx second for one video, calling a method to reduce the redundancy in this dimension.


%However, these long and high-resolution videos consume excessive number of tokens in DiTs, whose inference time is asymptotically quadratic to the number of tokens. Moreover, diffusion models require an iterative denoising process, typically 20-50 steps~\citep{li2024distrifusion}, further slowing down the inference process. For instance, one leading open-source DiT models~\citet{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109} consumes xxx tokens in 93 frames and 720p video generation, consuming xxx seconds in a single xxx GPU.

\begin{figure*}[!ht]
    \centering
    % Diagonal subplot
    %\begin{subfigure}[t]
    %{0.27\textwidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/layer16_0.pdf}
    %    \subcaption{Repetitive.}
    %    \label{fig:repete}
    %\end{subfigure}
    %\hfill
    %\begin{subfigure}[t]{0.69\textwidth}
    %    \centering
    \includegraphics[width=\linewidth]{figures/combined_plots_with_captions.pdf}
    %\end{subfigure}
    %\begin{subfigure}[t]{0.23\textwidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/average_diagonal.pdf}
    %    \caption{Large Diagonal.}
    %    \label{fig:diagonal}
    %    ~\hangliang{average attention score}
    %\end{subfigure}
    %\hfill
    % Locality subplot
    %\begin{subfigure}[t]{0.23\textwidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/average_locality.pdf}
    %    \caption{Locality.}
    %    \label{fig:locality}
    %\end{subfigure}
    % Data indepdendent subplot
    %\begin{subfigure}[t]{0.23\textwidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/average_prompt.pdf}
    %    \caption{Data Independent.}
    %    \label{fig:data_indepdendent}
    %\end{subfigure}
    \caption{We observe the~\patternname~pattern in 3D DiTs. (a) the attention map can be broken down into smaller repetitive blocks. (b) These blocks can be classified into two types, where attention weights on the diagonal blocks are noticeably larger than on off-diagonal ones. (c) These blocks exhibit locality, where the attention score differences between the first frame and later frames gradually increases. (d) The block structure is stable across different data points, but varies across layers. We randomly select 2 prompts (one landscape and one portrait) and record the important positions in the attention map that accounted for 90\% (95\%, 99\%) of the total. We printed the proportion of stable overlap of important positions across layers.}
    \label{fig:main_combined}
    \vspace{-3mm}
\end{figure*}


This paper tackles the issue by simultaneously sparsifying 3D attention and reducing sampling steps to accelerate 3D DiTs. 
To explore the redundancies in video data (recall that by nature videos can be efficiently compressed), we examine  3D DiT attention states and identify an intriguing phenomenon, referred to as the \patternname. 
As shown in Fig.~\ref{fig:main_combined}a, the attention maps exhibit uniformly distributed and repetitive \textit{tile blocks}, where each tile block represents the attention between latent frames\footnote{we use the term latent because DiTs compute in the latent space of VAEs~\citep{rombach2022high}.}. % This pattern reflects the correlation indicates frame-frame correlations, with significant sparsity in off-diagonal blocks indicating that \emph{not every frame needs to attend to all others}. 
This repetitive pattern suggests that \emph{not every latent frame needs to attend to all others}.
Moreover, the \patternname~pattern is almost independent of specific input (Fig.~\ref{fig:main_combined}d).
With these, we propose a solution that replaces the original attention with a fixed set of sparse attention mask during inference (\S \ref{sec:method_layerwise}).
%to profile the sparse patterns among a search space for each layer and use the resultant sparse attention to approximate the original attention when inference. 
%{\spaceskip=0.2em
Specifically, we constrain each latent frame to only attend to a constant number of other latent frames, which reduces the complexity of attention computation from quadratic to linear.
%\par
%}

% The above observation suggest an opportunity to approximate the original attention with a sparse attention to accelerate the inference. Furthermore, since the attention pattern remains the same across different inputs, the model can employ a \textbf{fixed} set of sparse pattern during inference time. 
% This allows a hardware-efficient implementation, where the model does not need to adaptively search for a suitable sparse pattern during inference~\citep{zhang2024h2o}. 
% Following these analysis, we design a class of sparse attention patterns for 3D full attention DiT where frames only attend to a constant of other frames. This design reduces the attention computation from quadratic complexity to linear complexity. Building on this class of patterns, we develop a searching algorithm to find the optimal pattern for each layer. 
% advocate retaining only the tile structures to build a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 

% With the insight that video data contains numerous redundancies and can be effectively compressed, we take an inspection on the attention states in 3D DiTs and find an interesting phenomenon, namely~\patternname~.
% As shown in Figure~\ref{fig:attention_tile_method}, the attention maps display uniformly distributed blocks, which correspond to individual frame-frame correlation, and sparsity in the off-diagonal blocks is significant, which implies that \textit{not every frame needs to attend to all other frames}. 
% Besides, the \patternname~ pattern is nearly data-agnostic but can vary across heads and layers. 
% and the contents of off-diagonal blocks are largely \textbf{repetitive}.
% This pattern suggests that there remains a large redundancy in the latter category: \textit{not every frame needs to attend to all other frames}. 
% We further find that each attention head exhibits a different pattern for a single block. However, the pattern for each attention head remains the same across different inputs (Figure~\ref{fig:attention_tile_method}).

% Intuitively, in 3D DiTs, each small block in the attention map correspond to the attention of one latent frame to another latent frame. Each attention map can be decomposed to two categories, where a frame either attend to itself or another frame.% 

% aspects: 1) Prune the 3D full attention based on the redundancy within video data. 
% We identify a prevalent \emph{tile-style repetitive pattern} in the 3D attention maps for video data, and advocate retaining only the tile structures to build a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 
We then consider shortening the sampling process of a video from 3D DiT to further amplify the acceleration effect. 
Inspired by the recent advance in diffusion distillation~\citep{salimans2022progressive,song2023consistency,kim2023consistency,liu2023instaflow,sauer2023adversarial,yin2024one,heek2024multistep,xie2024mlcm}, we 
adopt
%propose to introduce the 
a simple yet effective multi-step consistency distillation (MCD)~\citep{heek2024multistep} technique into our approach to achieve the efficient generation of compelling videos. 
In particular, we split the entire sampling trajectory into adjacent segments and perform consistency distillation within each one. 
We also progressively decrease the number of segments to improve the generation quality at rare steps. 
% Unlike acceleration method that achieves speedup based on the redundancy between diffusion steps~\citep{zhao2024real, li2024distrifusion}, 

% our sparse attention pattern is orthogonal to the redundancy between diffusion steps. To further accelerate the inference for 3D DiTs, we adopt the powerful multi-step consistency distillation method built in image DiTs~\citep{heek2024multistep}.
% method from Image DiTs with our sparse pattern to further accelerate the inference process. 
% It is empirically observed that naively merging such architectural and algorithmic notifications can lead to unstable training, so we devise a three-stage training pipeline to better conjoin the low-complexity attention and few-step generation capacities. 

% In this paper, we analyze the attention pattern in 3D full attention DiTs, and find it exhibits significant redundancy. Specifically, we find an interesting phenomenon, namely~\patternname~, where attention maps consist of regular and smaller blocks, whose contents are largely \textbf{repetitive} for off-diagonal ones (Figure~\ref{fig:attention_tile_method}).
%The contents of these blocks are largely $\textbf{repetitive}$ except for the main diagonal ones. %We term this phenomenon~\patternname~ (Figure~\ref{fig:attention_tile_method}). 
% Intuitively, in 3D DiTs, each small block in the attention map correspond to the attention of one latent frame to another latent frame. Each attention map can be decomposed to two categories, where a frame either attend to itself or another frame.% ~\hangliang {this is not real frame. we can call it frame of latent (after VAE). }
%This pattern shows that each frame compute attention in two different mechanism. The attention maps can be thus decomposed to two mechanism: (1) each frame attends to itself, and (2) each frame attends to other frame. 
% This pattern suggests that there remains a large redundancy in the latter category: \textit{not every frame needs to attend to all other frames}. %This implies an opportunity to develop a sparse attention based mechanism. 
% We further find that each attention head exhibits a different pattern for a single block. However, the pattern for each attention head remains the same across different inputs (Figure~\ref{fig:attention_tile_method}).% This implies that during inference time, the model 
%a sparse attention based method may simply employ the same pattern for an attention head, without the need to adaptively select a different pattern given the data, which has been shown to be hardware unfriendly.
%This implies that we can train a model with a fixed attention pattern
%In addition, we find that for each attention head, these patterns are relatively static across different data points (Figure~\ref{fig:attention_tile_method}).
%\dacheng{Improve the flow to: data-independent, knowledge distillation, and layerwise adaptive design.}

% The above observation suggest an opportunity to approximate the original attention with a sparse attention to accelerate the inference. Furthermore, since the attention pattern remains the same across different inputs, the model can employ a \textbf{fixed} set of sparse pattern during inference time. This allows a hardware-efficient implementation, where the model does not need to adaptively search for a suitable sparse pattern during inference~\citep{zhang2024h2o}. 
% %the original pattern remains the same across different inputs, the model can use a fixed pattern for different inputs during inference time.
% Following these analysis, we design a class of sparse attention patterns for 3D full attention DiT where frames only attend to a constant of other frames. This design reduces the attention computation from quadratic complexity to linear complexity. Building on this class of patterns, we develop a searching algorithm to find the optimal pattern for each layer. 


% We further apply this masking on top of the general consistency models framework~\citep{song2020denoising} to achieve a further speedup. In particular, we 
% To preserve the performance of the original model, we develop a knowledge distillation method to efficiently transfer the knowledge learned in the full attention DiT to the sparse one. 


Due to the orthogonality between sparse attention and MCD, a naive combination is possible, such as directly distilling a sparse student 3D DiT from a pre-trained model. 
However, %empirical observations indicate that this approach can lead to unstable training due to 
the initial gap between the sparse student and the teacher can be large so that the training suffers from a cold start. 
To tackle this issue, 
we introduce a more refined model acceleration process named ~\methodname. Initially, MCD is utilized to generate a student model with the same architecture but fewer sampling steps than the teacher. Subsequently, we determine the optimal sparse attention pattern for each head of the student and then apply a knowledge distillation procedure to the sparse model to maintain performance.
With 0.1\% the pretraining data, we train Open-Sora-Plan-1.2 models into variants that are $7.8\times$ and $7.4\times$ faster, with a marginal performance trade-off in VBench.~\citep{huang2024vbench}.
%with less than $1\%$ performance loss in VBench~\citep{huang2024vbench}. 
In addition, we provide evidence that our approach is amenable to advances in distributed inference systems, achieving an additional $3.91\times$ speedup when running on 4 GPUs.

% A sentence summarizes the performance and data efficiency. 
% Putting these components together, we introduce the framework~\methodname~, which turns a pre-trained 3D DiT to an efficient variant. ~\methodname~ 
% first extends the multi-step consistency distillation procedure developed for image diffusion models ~\citep{heek2024multistep} to video diffusion models. It then searches for the optimal sparse attention pattern for each layer of the consistency distilled model, and finally employs a knowledge distillation procedure to the sparse model to preserve the performance.

%to a variant that compute for fewer denoising steps and hardware-friendly sparse attentions with a similar performance, and in a data-efficient way. 

%that has linear compute complexity with respect to sequence length. Building on top of this sparse attention, we build a framework ~\methodname~ that takes in a trained 3D DiT model, and output an DiT that has linear compute complexity with respect to sequence length, with almost no performance loss. ~\methodname~ operates in three steps. Firstly, we perform a multi-step CM training. Secondly, we search for the best attention pattern for each layer. Since the attention pattern remains static over different data examples, we train the same sets of attention masks. In the last step, we perform knowledge distillation, where the CM model is used as the teacher to guide the generation of the sparse model. During the inference time, the user can uses this model, which is consistency distilled, wit sparse attention, to achieve fast 3D DiT synthesis. 
In summary, our contribution are:
\begin{enumerate}
    \item We discover and analyze the phenomenon of~\patternname~in 3D full attention DiTs, and propose a family of sparse attention mask with linear complexity to address the redundancy.
    % and analyze its characteristics, namely, repetitive, large diagonals, locality and data independence.
    \item We design a framework~\methodname~based on our analysis of~\patternname, which turns pre-trained 3D DiT to a fast variant in a data efficient manner.%We develop a sparse attention kernel that is amenable to various parallelism strategies. %\dacheng{make kernel availble, with strong scalability}
    \item We evaluate on two Open-Sora-Plan 1.2 models for 29 frames and 93 frames generation. ~\methodname~achieves up to $7.8\times$ speedup with little performance trade-off on VBench and CD-FVD. We further demonstrate the potential of integrating our method with advanced distributed inference techniques, achieving additional $3.91\times$ with 4 GPUs.
\end{enumerate}

\begin{comment}
\begin{figure}[t]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/prompt1.pdf}
        \subcaption{Attention map for prompt A.}
    \end{minipage}
    \hfill
    \begin{minipage}{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/prompt2.pdf}
        \subcaption{Attention map for prompt B.}
    \end{minipage}
    \hfill
    \caption{(a): Attention map consists of tile-like repetitive pattern. (b) This pattern is the same across different prompts. (c) Our method reduces such redundancy.} \dacheng{Maybe not show our pattern here, seems not the major message.}%~\hangliang {using 8x8 to show pattern is better?}
    %promptA:An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt, he wears a brown beret and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm film.
    %promptB:a close-up shot of a woman standing in a dimly lit room. she is wearing a traditional chinese outfit, which includes a red and gold dress with intricate designs and a matching headpiece. the woman has her hair styled in an updo, adorned with a gold accessory. her makeup is done in a way that accentuates her features, with red lipstick and dark eyeshadow. she is looking directly at the camera with a neutral expression. the room has a rustic feel, with wooden beams and a stone wall visible in the background. the lighting in the room is soft and warm, creating a contrast with the woman's vibrant attire. there are no texts or other objects in the video. the style of the video is a portrait, focusing on the woman and her attire.
    \label{fig:attention_tile_method}
\end{figure}
\end{comment}

% \hangliang{need to show that attention takes a lot of time}

%\begin{table}[h]
%\centering
%\caption{Runtime Comparison of Self-Attention and MLP in 3D-DiT. \dacheng{Convert this to a curve, use wrapfigure.}}
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
%\textbf{h/w} & \textbf{seq\_len} & \textbf{attn\_time} & \textbf{mlp\_time} & \textbf{attn\_percent} & \textbf{mlp\_percent} \\
%\hline
%256 & 4352 & 0.005148 & 0.003349 & 60.5 & 39.4 \\
%512 & 17408 & 0.042987 & 0.012810 & 77.0 & 22.9 \\
%1024 & 69632 & 0.557357 & 0.052235 & 91.4 & 8.56 \\
%2048 & 278528 & 8.706761 & 0.954199 & 90.1 & 9.87 \\
%\hline
%\label{fig:attn_time}
%\end{tabular}
%\end{table}

%\dacheng{Discuss 3D Attention.}
%\dacheng{Paragraph 2 - The solution lies in two directions. Yet, lacking a co-design of these two dimensions missed opportunities.}
%Existing optimizations on video generation DiT can be categorized into two directions, yet, neither of them sufficiently addresses the latency problem. The first direction reduces the number of iterative denoising steps. For instance, T2V-Turbo uses consistency distillation with 4-8 denoising steps and outperforms a 50 step DDIM sampler~\citep{li2024t2v, song2023consistency, song2020denoising}. However, this solution does not address the quadratic computation in a single denoising step, still resulting in a high latency (e.g. xxx second) when generating long and high-resolution videos. The second direction reduces the amount of attention computation. In particular, PAB~\citet{zhao2024real} leverages similarity of attention matrix across denoising steps and occasionally reuse attention computation from previous steps. However, this method does not reduce the number of denoising steps, resulting in xxx seconds.
% Paragraph 3: Our method.

% In this paper, we address the high latency of video generation DiTs model inference by simultaneously tackling both problems. To reduce the number of denosing steps, we leverage multi-step consistency distillation~\citep{song2023consistency, heek2024multistep, xie2024mlcm}. When reducing the quadratic attention computation, we find that existing PAB method is not compatible with the CM method whose attention matrices are no longer similar across denoising steps ~\dacheng{Add an illustrative figure on this problem.} Thus, we purpose to use Sparse Attention framework, generally studied in the context of text-only Transformer models~\citep{beltagy2020longformer, choromanski2020rethinking}. In particular, we split video frames into key frames and non-key frames, where the features of non-key frames only attend to the features of key frames. To effectively leverage a pretrained full attention DiT, we use knowledge distillation that formulates a pretrained full attention DiT as the teacher, and the sparse attention DiT as the student~\citep{hinton2015distilling}.