\section{Related Work}
\begin{figure*}[ht]
    \centering
        \includegraphics[width=\textwidth]{figures/method.pdf}
        \caption{~\methodname~takes in a pre-trained 3D Full Attention video diffusion transformer(DiT), with slow inference speed and high fidelity. It then operates on three stages to greatly accelerate the inference while maintaining the fidelity. In Stage 1, we modify the multi-step consistency distillation framework from **Vahdat et al., "NVIDIA CLIP:**** to the video domain, which turned a DiT model to a CM model with \textit{stable} training. In Stage 2,~\methodname~performs a searching algorithm to find the best sparse attention pattern for each layer. In stage 3,~\methodname~performs a knowledge distillation procedure to optimize the fidelity of the sparse DiT. At the end,~\methodname~outputs a DiT with linear attention, high fidelity and fastest inference speed.}
     %~\hangliang {using 8x8 to show pattern is better?}
    %promptA:An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt, he wears a brown beret and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm film.
    %promptB:a close-up shot of a woman standing in a dimly lit room. she is wearing a traditional chinese outfit, which includes a red and gold dress with intricate designs and a matching headpiece. the woman has her hair styled in an updo, adorned with a gold accessory. her makeup is done in a way that accentuates her features, with red lipstick and dark eyeshadow. she is looking directly at the camera with a neutral expression. the room has a rustic feel, with wooden beams and a stone wall visible in the background. the lighting in the room is soft and warm, creating a contrast with the woman's vibrant attire. there are no texts or other objects in the video. the style of the video is a portrait, focusing on the woman and her attire.
    \label{fig:method}
    \vspace{-5mm}
\end{figure*}


\textbf{Video Diffusion Transformers} There is a rich line of research in diffusion based models for video generation**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics:**. More recently, **Ho et al., "Denoising Diffusion Convolutional Neural Networks"**: introduces the architecture of Diffusion Transformers (DiTs), and several popular video generation models have been developed using the DiTs backbone, for instance, **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics:**. More specifically, **Niu et al., "Temporal Flows: Efficient Convolutions for Dynamic Graphs"**: has explored the use of 3D Full Attention Transformers, which jointly model spatial and temporal relationship, instead of previous models that separately model spatial and temporal relationship (e.g. one Transformer layer with spatial attention and the other with temporal attention**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics:**). The design of 3D full attention has gained increasing popularity due to their promising performance. In this work, we tackle the efficiency problem specifically for 3D full attention diffusion Transformers. In addition, there is a line of research that combines video diffusion model with sequential or autoregressive generation. These methods may also achieve speedup due to their use of shorter sequence length. ~\methodname~aims to speedup in a single diffusion forward, which is compatible with orthogonal to autoregressive manner methods**Ho et al., "Denoising Diffusion Convolutional Neural Networks"**. 

\textbf{Accelerating diffusion inference} 
Many work in diffusion models have been proposed to reduce the number of sampling steps to accelerate diffusion inference**Van Den Oord et al., "Transformation Autoregressions for Efficient Video Prediction:**.  **Ho et al., "Denoising Diffusion Convolutional Neural Networks"**: proposes the consistency models which distills multiple steps ODE to one step. **Song et al., "FastDiff: Fast and Memory-Efficient Diffusion Models"**: extends CMs to video generation model. **Mittal et al., "Efficient Video Generation Using a Temporal Pyramid Model:** further extends the idea with reward model to speed up video diffusion model inference. 
Another line of research that accelerates diffusion models inference utilize multiple devices**Chen et al., "Efficient Multi-Device Inference for Deep Neural Networks"**. These works exploit the redundancy between denoising steps and use stale activations in distributed inference to hide communication overhead, and are naturally incompatible with work that reduce the redundancy between steps. In this work, we exploit the redundancy in attention computation, which is orthogonal to works that leverage distributed acceleration and redundancy between denoising steps. Our pipeline integrates a multi-step CM approach**Song et al., "FastDiff: Fast and Memory-Efficient Diffusion Models"** by default, and in experiment, we show that it can also seaminglessly integrate with parallel inference.

\textbf{Sparsity in Transformer inference} has been investigated in the context of Large Language Models (LLMs) inference, which can be decomposed into pre-filling and decoding stages**Tay et al., "Synthesizing Guarantees for Sparsifying Transformers"**. StreamingLLM discovers the pattern of Attention Sink, and keeps a combination of first few tokens and recent decoded tokens during decoding phrase**Henderson et al., "Streaming LLMs with Efficient Sparse Attention"**. **Chen et al., "Adaptive Token Pruning for Fast Transformer Inference:** adaptively identify the most significant tokens during test time. Video DiTs have different workload than LLMs, where DiTs perform a single forward in each diffusion step without a decoding phrase. In particular, our paper is among the first to explore sparse attention in the context of 3D Full Attention DiTs. In addition, our finding that ~\patternname~is data-independent motivates us to design a solution which does not require inference time adaptive searching, which is a bottleneck in work such as**Ho et al., "Denoising Diffusion Convolutional Neural Networks"**. Sparsity has also been studied in Gan and other diffusion-based models, yet we focus on the new architecture 3D DiT**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics:**. A recent paper**Wu et al., "FastDiff: Fast and Memory-Efficient Diffusion Models"**: also discusses the redundancy in DiTs models, but no performance has been shown.% by the date of this paper is written.


%\dacheng{add QIHOO-T2X: AN EFFICIENCY-FOCUSED DIFFUSION
%TRANSFORMER VIA PROXY TOKENS FOR TEXT-TOANY-TASK to bib when it is ready}
%QIHOO-T2X uses redundancy in visual information in DiT. **Vahdat et al., "NVIDIA CLIP:** studies the sparse pattern in LLM inference.

%\dacheng{In related work, we can discuss about the shortcomings in PAB, distrifusion}