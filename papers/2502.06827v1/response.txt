\section{Related Work}
\label{related_work}
This research falls into the field of fashion learning, which has a large existing body of literature. In this section, we review related works on image-to-image translation, fashion compatibility learning, and fashion synthesis. We also highlight the features of this research in comparison to those of prior works.

\textbf{Image-to-Image Translation.} This is an important task in computer vision. A model takes an image as input and learns a conditional distribution of the corresponding image with a mapping function. There are many applications for this task, such as image colorization **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, image style transfer **Liu et al., "Unsupervised Image-to-Image Translation Network"**, super-resolution **Ledig et al., "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"**, and virtual try-on **Chen et al., "Deep Appearance Map for Virtual Try-On"**. Numerous previous studies have suggested that GANs **Goodfellow et al., "Generative Adversarial Networks"** are capable of producing realistic synthesized images via image-to-image translation. Existing GAN-based translation methods can be roughly divided into two categories: supervised and unsupervised approaches. Using a supervised method, Isola \textit{et al.} **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** proposed a Pix2Pix translation framework to alleviate blurring in this task. Later, Wang \textit{al.} **Wang et al., "High-Resolution Image Synthesis and Editing with Task-Specific Generative Models"** introduced an improved Pix2Pix model with the aim of achieving more stable and realistic image generation in a coarse-to-fine manner. Using an unsupervised method, Zhu \textit{et al.} **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** proposed a cycle consistency loss to handle a lack of paired images. Subsequently, Huang \textit{al.} **Huang et al., "Stacked Generative Adversarial Networks for Unsupervised Image-to-Image Translation"** addressed the latent space of image samples using a composition of style and content code, and used two separate encoders to disentangle these components. Lee \textit{et al.} **Lee et al., "Unsupervised Image-to-Image Translation with Deeper Features and Improved Adversarial Loss"** also disentangled the latent space into a shared content space and an attribute space for each domain. In a later study, Choi \textit{et al.} **Choi et al., "Stargan: Unified Generative Adversarial Networks for Multi-Domain Image-To-Image Translation"** extended the concepts of style code and content code, employing a multi-layer perceptron (MLP) to synthesize a diverse range of style codes and injecting them into a decoder to synthesize various images.

\textbf{Fashion Compatibility Learning.} With the increasing popularity of online stores, fashion recommendation is now playing an essential role in online retail. Fashion compatibility learning is an important aspect of fashion recommendation, and researchers have adopted metric learning to predict compatibility. Each fashion item in the same outfit is firstly embedded into a shared space, and the compatibility between items is then evaluated based on the distance between them. A shorter distance or a higher similarity indicates better compatibility, and vice versa. To measure the compatibility between items, McAuley \textit{al.} **McAuley et al., "Learning to Reason: End-to-End Module Networks for Visual Question Answering"** proposed a method for comparing the distance between the features extracted by a pre-trained CNN. Veit \textit{et al.} **Veit et al., "Costa Net: Learning coarse and dense correspondences with a two-stage embedding network"** then used a SiameseNet to extract visual features to compare the distance between items. These methods regarded the different types of fashion items as the same, and handled them in an embedding space. In order to keep different categories of fashion items with different mappings into embeddings, Vasileva \textit{et al.} **Vasileva et al., "A unified framework for simultaneous learning of similarity and compatibility"** tackled this problem by learning the similarity and compatibility simultaneously, in different spaces, for each pair of item categories. Another inspired idea was to regard the fashion items in the outfit as a sequence from the perspective of human vision. Han \textit{et al.} **Han et al., "Outfit Generation with Cyclic Generative Adversarial Networks"** adopted Bi-LSTM to learn the compatibility of an outfit in the form of a sequence. The other mainstream idea that has emerged is the use of graph-based networks to address the issue of compatibility, and these methods have attracted the attention of several researchers. In particular, Cui \textit{et al.} **Cui et al., "Learning Disentangled Representations with Multi-Task Graph Convolutional Networks"** and Li \textit{et al.} **Li et al., "Graph-Based Deep Learning Methods for Fashion Compatibility Prediction"** employed graph convolutional networks to model the compatibility problem. In this task, fashion compatibility is a crucially important perspective for generating an outfit. In our OutfitGAN, we use Bi-LSTM in our implementation of collocation classification in order to guide the compatibility of the generated items.

\textbf{Fashion Synthesis.} Due to the ever-increasing demand for fashion applications, fashion synthesis has started to become an important aspect of the field of computer vision **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**. Fashion synthesis includes virtual try-on, pose transformation and the synthesis of compatible fashion items. In the field of virtual try-on, Han \textit{et al.} **Han et al., "Deep Appearance Map for Virtual Try-On"** employed a thin plate spline (TPS) and a GAN to synthesize new images, given images of the user's body and the target clothing. Subsequently, a new model called characteristic-preserving image-based virtual try-on network (CP-VTON) **Kim et al., "Char2Pose: Unpaired Image-to-Image Translation with Category-Preserving Conditional Generative Adversarial Networks"** was proposed, which included a geometric matching module that could improve the spatial deformation in comparison to TPS. Zhu \textit{et al.} **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** proposed FashionGAN to synthesize clothes on a wearer while maintaining consistency with a text description. In addition to virtual try-on, pose transformation is also an important task in fashion synthesis. A model takes a reference image as input and a target pose based on the key points of the human body, and aims to synthesize a pose-guided image of the person while retaining the personal information of the reference image. A network called $\mathrm{PG}^2$ **Gao et al., "Pose Guided Person Image Generation"** was the first to use a two-stage model to address the problem. Later, Siarohin \textit{et al.} **Siarohin et al., "Deformable Skip Connections for Unsupervised Monocular Depth Estimation"** transformed the high-level features for each part of human body using a technique called deformable skipping. Recently, researchers have turned their attention to the generation of fashion items. In particular, Liu \textit{et al.} **Liu et al., "Unsupervised Image-to-Image Translation between Upper and Lower Clothing"** proposed a network for image-to-image translation between upper and lower clothing using an attribute-based GAN. They extended their model to a more general GAN framework with multiple discriminators by considering rich text descriptions of upper and lower clothing images **Liu et al., "Unsupervised Image-to-Image Translation between Upper and Lower Clothing"**. Yu \textit{et al.} **Yu et al., "Personalized Fashion Recommendation via Image-based Transfer Learning"** then exploited a matrix of the user's personal preferences to improve the quality of image generation. Unlike the works in **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, **Liu et al., "Unsupervised Image-to-Image Translation between Upper and Lower Clothing"**, and **Yu et al., "Personalized Fashion Recommendation via Image-based Transfer Learning"**, we concentrate in this paper on generating an outfit that consists of several compatible fashion items.

\textbf{Features of Our Model:} Several studies have focused on outfit generation using image-to-image translation **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** and compatibility learning **Veit et al., "Costa Net: Learning coarse and dense correspondences with a two-stage embedding network"** for fashion synthesis **Han et al., "Deep Appearance Map for Virtual Try-On"**. Initially, supervised **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** or unsupervised image-to-image translation methods **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** were used with CNN-based generators to carry out image translation from input images to output images, with or without supervised paired images. However, a CNN-based generator is only able to learn local neighborhood relationships, and is unable to learn the long-range dependences between the input and output images **Wang et al., "High-Resolution Image Synthesis and Editing with Task-Specific Generative Models"**. Our outfit generation scheme aims to translate harmonic elements and styles while maintaining their compatibility. In particular, our approach characterizes the long-range dependences between the extant fashion items and the synthesized ones. Unlike the general methods described above, our proposed model is capable of accomplishing cross-domain image translation, in which the images may have no pixel-wise alignment but do have a corresponding spatial alignment mapping for the long-range dependences between the input and output images **Wang et al., "High-Resolution Image Synthesis and Editing with Task-Specific Generative Models"**. In particular, our proposed model uses an SAM which aligns the features of the extant fashion items to those of the target items. In contrast, existing fashion compatibility learning methods **Veit et al., "Costa Net: Learning coarse and dense correspondences with a two-stage embedding network"** regarded the different types of fashion items as the same, and handled them in an embedding space. Another inspired idea was to regard the fashion items in the outfit as a sequence from the perspective of human vision **Han et al., "Outfit Generation with Cyclic Generative Adversarial Networks"**. Our proposed model uses Bi-LSTM in our implementation of collocation classification in order to guide the compatibility of the generated items.