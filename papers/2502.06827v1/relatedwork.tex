\section{Related Work}
\label{related_work}
This research falls into the field of fashion learning, which has a large existing body of literature. In this section, we review related works on image-to-image translation, fashion compatibility learning, and fashion synthesis. We also highlight the features of this research in comparison to those of prior works.

\textbf{Image-to-Image Translation.} This is an important task in computer vision. A model takes an image as input and learns a conditional distribution of the corresponding image with a mapping function. There are many applications for this task, such as image colorization \cite{pix2pix2017}, image style transfer \cite{Gatys_2016_CVPR}, super-resolution \cite{ledig2017photo}, and virtual try-on \cite{Han_2018_viton,tileimage}. Numerous previous studies have suggested that GANs \cite{NIPS2014_gan} are capable of producing realistic synthesized images via image-to-image translation. Existing GAN-based translation methods can be roughly divided into two categories: supervised and unsupervised approaches. Using a supervised method, Isola \textit{et al.} \cite{pix2pix2017} proposed a Pix2Pix translation framework to alleviate blurring in this task. Later, Wang \textit{et al.} \cite{wang2018pix2pixHD} introduced an improved Pix2Pix model with the aim of achieving more stable and realistic image generation in a coarse-to-fine manner. Using an unsupervised method, Zhu \textit{et al.} \cite{CycleGAN2017} proposed a cycle consistency loss to handle a lack of paired images. Subsequently, Huang \textit{et al.} \cite{huang2018munit} addressed the latent space of image samples using a composition of style and content code, and used two separate encoders to disentangle these components. Lee \textit{et al.} \cite{DRIT_plus} also disentangled the latent space into a shared content space and an attribute space for each domain. In a later study, Choi \textit{et al.} \cite{choi2020starganv2} extended the concepts of style code and content code, employing a multi-layer perceptron (MLP) to synthesize a diverse range of style codes and injecting them into a decoder to synthesize various images.

\textbf{Fashion Compatibility Learning.} With the increasing popularity of online stores, fashion recommendation is now playing an essential role in online retail. Fashion compatibility learning is an important aspect of fashion recommendation, and researchers have adopted metric learning to predict compatibility. Each fashion item in the same outfit is firstly embedded into a shared space, and the compatibility between items is then evaluated based on the distance between them. A shorter distance or a higher similarity indicates better compatibility, and vice versa. To measure the compatibility between items, McAuley \textit{et al.} \cite{mcauley2015image} proposed a method for comparing the distance between the features extracted by a pre-trained CNN. Veit \textit{et al.} \cite{veit2015learning} then used a SiameseNet to extract visual features to compare the distance between items. These methods regarded the different types of fashion items as the same, and handled them in an embedding space. In order to keep different categories of fashion items with different mappings into embeddings, Vasileva \textit{et al.} \cite{vasileva2018learning} tackled this problem by learning the similarity and compatibility simultaneously, in different spaces, for each pair of item categories. Another inspired idea was to regard the fashion items in the outfit as a sequence from the perspective of human vision. Han \textit{et al.} \cite{han2017learning} adopted Bi-LSTM to learn the compatibility of an outfit in the form of a sequence. The other mainstream idea that has emerged is the use of graph-based networks to address the issue of compatibility, and these methods have attracted the attention of several researchers. In particular, Cui \textit{et al.} \cite{cui2019dressing} and Li \textit{et al.} \cite{li2020hierarchical} employed graph convolutional networks to model the compatibility problem. In this task, fashion compatibility is a crucially important perspective for generating an outfit. In our OutfitGAN, we use Bi-LSTM in our implementation of collocation classification in order to guide the compatibility of the generated items. 

\textbf{Fashion Synthesis.} Due to the ever-increasing demand for fashion applications, fashion synthesis has started to become an important aspect of the field of computer vision \cite{2020arXiv200313988C}. Fashion synthesis includes virtual try-on, pose transformation and the synthesis of compatible fashion items. In the field of virtual try-on, Han \textit{et al.} \cite{Han_2018_viton} employed a thin plate spline (TPS) and a GAN to synthesize new images, given images of the user's body and the target clothing. Subsequently, a new model called characteristic-preserving image-based virtual try-on network (CP-VTON) \cite{cp_vton} was proposed, which included a geometric matching module that could improve the spatial deformation in comparison to TPS. Zhu \textit{et al.} \cite{fashion_gan} proposed FashionGAN to synthesize clothes on a wearer while maintaining consistency with a text description. In addition to virtual try-on, pose transformation is also an important task in fashion synthesis. A model takes a reference image as input and a target pose based on the key points of the human body, and aims to synthesize a pose-guided image of the person while retaining the personal information of the reference image. A network called $\mathrm{PG}^2$ \cite{ma2017pose} was the first to use a two-stage model to address the problem. Later, Siarohin \textit{et al.} \cite{siarohin2018deformable} transformed the high-level features for each part of human body using a technique called deformable skipping. Recently, researchers have turned their attention to the generation of fashion items. In particular, Liu \textit{et al.} \cite{attribute_gan} proposed a network for image-to-image translation between upper and lower clothing using an attribute-based GAN. They extended their model to a more general GAN framework with multiple discriminators by considering rich text descriptions of upper and lower clothing images \cite{multi_dis_fashion}. Yu \textit{et al.} \cite{Yu_2019_ICCV} then exploited a matrix of the user's personal preferences to improve the quality of image generation. Unlike the works in \cite{attribute_gan}, \cite{multi_dis_fashion} and \cite{Yu_2019_ICCV}, we concentrate in this paper on generating an outfit that consists of several compatible fashion items.

\textbf{Features of Our Model:} Several studies have focused on outfit generation using image-to-image translation \cite{pix2pix2017,wang2018pix2pixHD,CycleGAN2017,huang2018munit,DRIT_plus,choi2020starganv2} and compatibility learning \cite{mcauley2015image,veit2015learning,vasileva2018learning,han2017learning,cui2019dressing,li2020hierarchical} for fashion synthesis \cite{attribute_gan,multi_dis_fashion,Yu_2019_ICCV}. Initially, supervised \cite{pix2pix2017,wang2018pix2pixHD} or unsupervised image-to-image translation methods \cite{CycleGAN2017,huang2018munit,DRIT_plus,choi2020starganv2} were used with CNN-based generators to carry out image translation from input images to output images, with or without supervised paired images. However, a CNN-based generator is only able to learn local neighborhood relationships, and is unable to learn the long-range dependences between the input and output images \cite{wang2018non}. Our outfit generation scheme aims to translate harmonic elements and styles while maintaining their compatibility. In particular, our approach characterizes the long-range dependences between the extant fashion items and the synthesized ones. Unlike the general methods described above, our proposed model is capable of accomplishing cross-domain image translation, in which the images may have no pixel-wise alignment but do have a corresponding spatial alignment mapping for the long-range dependences between the input and output images. In particular, our proposed model uses an SAM which aligns the features of the extant fashion items to those of the target items. In contrast, existing fashion compatibility learning methods \cite{mcauley2015image,veit2015learning,vasileva2018learning,han2017learning,cui2019dressing,li2020hierarchical} are used to predict outfit compatibility and give outfit recommendations for an extant fashion database with discriminative models, and rarely consider the synthesis of new compatible outfits based on extant fashion items. Our proposed model synthesizes compatible fashion items based on extant ones, using a generative model. Finally, although several fashion synthesis methods \cite{attribute_gan,multi_dis_fashion,Yu_2019_ICCV} have been used to synthesize complementary fashion items based on extant ones, these methods only carry out image translation between upper and lower clothing, and cannot synthesize an entire outfit. Our proposed model uses multiple generators to synthesize suitable fashion items for the generation of entire outfits. In addition, a CCM is proposed to supervise the compatibility of the synthesized outfit during the generation process.