\documentclass[sigconf]{acmart}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28--May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28--May 2, 2025, Sydney, NSW, Australia}
\acmISBN{979-8-4007-1274-6/25/04}
\acmDOI{10.1145/3696410.3714646}
\settopmatter{printacmref=true}

\usepackage{graphicx}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{color}
\usepackage{textcomp}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{bm}
\usepackage{multirow}
\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{balance}

\begin{document}

\title[Revisiting Dynamic Graph Clustering via Matrix Factorization]{Revisiting Dynamic Graph Clustering via Matrix Factorization}


%%
\author{Dongyuan Li}
\affiliation{\institution{The University of Tokyo}
\city{Tokyo}
\country{Japan}}
\orcid{0000-0002-4462-3563}
\email{lidy@csis.u-tokyo.ac.jp}

\author{Satoshi Kosugi}
\affiliation{%
  \institution{Institute of Science Tokyo}
  \city{Tokyo}
  \country{Japan}}
\orcid{0000-0001-7556-9072}
\email{kosugi@cvm.t.u-tokyo.ac.jp}

\author{Ying Zhang}
\affiliation{%
  \institution{RIKEN Center for Advanced Intelligence Project}
  \city{Tokyo}
  \country{Japan}
}
\orcid{0009-0000-9627-8768}
\email{ying.zhang@riken.jp}

\author{Manabu Okumura}
\affiliation{%
  \institution{Institute of Science Tokyo}
  \city{Tokyo}
  \country{Japan}}
\orcid{0009-0001-7730-1536}
\email{oku@pi.titech.ac.jp}

\author{Feng Xia}
\affiliation{%
 \institution{RMIT University}
 \city{Melbourne}
 \country{Australia}}
 \orcid{0000-0002-8324-1859}
 \email{f.xia@ieee.org}


\author{Renhe Jiang}
\authornote{Corresponding author.}
\affiliation{%
  \institution{The University of Tokyo}
  \city{Tokyo}
  \country{Japan}}
\orcid{0000-0003-2593-4638}
\email{jiangrh@csis.u-tokyo.ac.jp}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Li et al.}




\begin{abstract}
Dynamic graph clustering aims to detect and track time-varying clusters in dynamic graphs, revealing the evolutionary mechanisms of complex real-world dynamic systems. Matrix factorization-based methods are promising approaches for this task; however, these methods often struggle with scalability and can be time-consuming when applied to large-scale dynamic graphs. Moreover, they tend to lack robustness and are vulnerable to real-world noisy data. To address these issues, we make three key contributions. First, to improve scalability, we propose temporal separated matrix factorization, where a single matrix is divided into multiple smaller matrices for independent factorization, resulting in faster computation. Second, to improve robustness, we introduce bi-clustering regularization, which jointly optimizes graph embedding and clustering, thereby filtering out noisy features from the graph embeddings. Third, to further enhance effectiveness and efficiency, we propose selective embedding updating, where we update only the embeddings of dynamic nodes while the embeddings of static nodes are fixed among different timestamps. Experimental results on six synthetic and five real-world benchmarks demonstrate the scalability, robustness and effectiveness of our proposed method. Source code is available at \url{https://github.com/Clearloveyuan/DyG-MF}.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10010309</concept_id>
<concept_desc>Computing methodologies~Factorization methods</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computing methodologies~Factorization methods}

% \keywords{Dynamic Graph Clustering, Temporal Networks, Matrix Factorization, Dynamic Community Detection.}

\keywords{Graph Clustering, Temporal Networks, Community Detection.}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Main Body of the draft
\maketitle





% \input{Main Paper/1-Introduction}
\begin{figure}[t]
\centering 
\includegraphics[scale=0.4]{Figure/case122.pdf}
\caption{Running time and performance on noisy data of our proposed method and three matrix factorization-based methods, \textit{i.e.,} DYNMOGA~\cite{Folina2013}, jLMDC~\cite{9531337}, and RDMA~\cite{DBLP:journals/www/RanjkeshMH24}.}
\label{main_case-study}
\end{figure}


\section{Introduction}

Dynamic graph clustering, also known as dynamic community detection, aims to leverage graph topological structures and temporal dependencies to detect and track evolving communities~\cite{DBLP:journals/www/RanjkeshMH24,DBLP:conf/ijcnn/LiuWZY19,DBLP:journals/tkde/ChenJLLLCYH24}. 
As an effective tool to reveal the complex evolutionary rules behind complex real-world systems,
dynamic graph clustering has drawn great attention in various fields, such as social analysis~\cite{zhang2022robust,LiYicong,zhong2024efficient,ji2024memmap}, recommendation~\cite{Jiasu,zhang2023dyted,WangWLGLZ22,zhang2023tiger}, and AI4Science~\cite{JiShuo,shi2024graph,sun2024fast,9388900}. 

Much research in recent years has been proposed for dynamic graph clustering, which can be broadly classified into two classes~\cite{DBLP:journals/csur/RossettiC18}. \textbf{\textit{(i) Neural Network-based methods}}~\cite{DBLP:conf/aaai/YaoJ21,DBLP:journals/tcyb/GaoZZWL23,DBLP:conf/kdd/YouDL22,DBLP:conf/iclr/001400T00024} generally focus on learning dynamic node embedding, with clustering methods often applied as a post-processing step. These methods separate node embedding learning and clustering into two independent steps, leading to sub-optimal performance~\cite{DBLP:conf/ijcai/DongZSLC18,9531337}. 
To relieve this issue, \textbf{\textit{(ii) Matrix Factorization}}-based methods~\cite{Chakrabarti,DBLP:journals/tec/MaWWL24,Danon_2005,9531337,LiuF1801,Liu2019} have been widely proposed.
These methods can jointly optimize clustering and dynamic node embedding learning simultaneously, \textit{i.e.,} they cluster nodes at each timestamp while maintaining temporal smoothness of node embedding among different timestamps, achieving overall optimal performance on many benchmarks. 

Despite the great success of matrix factorization-based methods, there are still two challenges.  
\textbf{\textit{(i) Weak Scalability.}} 
Matrix factorization is an NP-hard problem with a time complexity of approximately $\mathcal{O}(n^{3})$ and a space complexity of $\mathcal{O}(n^{2})$ for a single graph containing $n$ nodes~\cite{DBLP:conf/icacs2/MouhahFB23,Vavasis09}. A pre-experiment is shown in Figure~{\ref{main_case-study}(a)}, best-performing matrix factorization-based baselines require approximately 40,000 seconds to process the arXiv dataset, which contains about 30,000 nodes, limiting their applicability to real-world dynamic graphs with millions of nodes~\cite{LiLM21}.
\textbf{\textit{(ii) Low Robustness.}} Real-world dynamic graphs contain noise and missing data, which disrupt their regular evolution patterns and pose significant challenges for dynamic graph clustering~\cite{Yuanhaonan,DBLP:conf/cikm/ZhangXZSCJZ23}. 
A case study is shown in Figure~{\ref{main_case-study}(b)}, adding random noisy edges to dynamic graphs leads to a sharp performance drop of these baselines. 

To address these issues, 
we propose a scalable and robust dynamic graph clustering framework via seperated matrix factorization, called \textbf{\textit{DyG-MF}}, containing three key contributions. 
Firstly, to enhance scalability, we propose \textbf{\textit{(i) Temporal Separated Matrix Factorization}}.
We apply temporal matrix factorization in a ``divide and conquer'' manner~\cite{li2019sepne,DBLP:journals/tkde/SongZLL22}, where we randomly divide the nodes into subsets and transform the original large-scale matrix factorization problem into several independent matrix factorization of these subsets. Since matrix factorization is applied separately to these smaller subsets, it also reduces computational cost. 
To achieve this, we design the temporal landmark selection, ensuring coherence of node embeddings across different subsets at current timestamp and maintaining consistency of node embedding between different timestamps. 
Secondly, to improve robustness, we introduce \textbf{\textit{(ii) Bi-clustering Regularization}}, which reduces the impact of noisy features on dynamic graph clustering by optimizing the rank of the matrix. We further proof this regularization can be spreadable and applied as a constraint in the matrix factorization of each node subset.
%%%%%%
Finally, to further enhance effectiveness and efficiency, we propose \textbf{\textit{(iii) Selective Embedding Updating}}. 
We first divide the nodes into dynamic and static groups by jointly considering their topological and embedding changes. We then only update node embeddings of the dynamic group while keeping the node embeddings in the static group fixed across different timestamps.
The main contributions of this study can be summarized as follows.

\begin{itemize}
\item To enhance scalability and efficiency of matrix factorization-based methods, we design a temporal separated matrix factorization framework, where we divide a single large matrix into multiple smaller matrices for independent factorization. 
\item To improve robustness, we adopt separable bi-clustering regularization to filter out noisy features from node embeddings.
\item To further enhance effectiveness and efficiency, we propose selective embedding updating, where only the node embeddings of the dynamic group are updated at each timestamp.
\item Experimental results on 11 benchmarks show the scalability, robustness, efficiency, and effectiveness of DyG-MF. 
\end{itemize}


%\input{Main Paper/2-Related Work}

\section{Related Work}
\label{related}
% Dynamic graph clustering methods are broadly categorized into two classes~\cite{DBLP:journals/csur/RossettiC18,DBLP:journals/tnn/SuXLWYZHPNJSY24}:  
% \textbf{\textit{(i) Neural Network}}-based methods and \textbf{\textit{(ii) Matrix Factorization}}-based methods.

%\vspace{3pt}
\textbf{Neural Network-based Methods.} 
Some neural network-based methods employ coupled approaches, which first condense dynamic graphs into one static graph and then apply clustering methods, such as CNN-based~\cite{9314087, DBLP:journals/kbs/SantoGMS21} and GNN-based 
methods~\cite{DBLP:journals/pami/ZhangNL23,zheng2022instant}, to identify clusters. 
Other methods employ two-stage approaches, which first learn dynamic graph embeddings~\cite{beer2023connecting,DBLP:conf/kdd/ZhangCFXZSC23,guo2022subset} and then apply clustering methods to these  embeddings to identify clusters~\cite{zhao2023spatial,DBLP:conf/KDD/CrossCityTransfer22,DBLP:journals/tnn/CuiLWZLWA24,Namyong,yang2024effective,chen2022efficient}.   
For example, RNNGCN~\cite{DBLP:conf/aaai/YaoJ21} and DGCN~\cite{DBLP:journals/tcyb/GaoZZWL23} use RNNs or LSTM to capture temporal dependencies for graph embeddings, which are then clustered using graph convolutional layers. 
CI-GCL~\cite{tan2024community} adopts a community invariance graph contrastive learning framework for graph clustering and classification.  
ROLAND~\cite{DBLP:conf/kdd/YouDL22} extends static GNN-based graph embedding methods to dynamic graphs by using gated recurrent units to capture temporal information. 
To reduce time consumption, SpikeNet~\cite{DBLP:conf/aaai/LiYZC0ZTWM23} uses spiking neural networks to model the evolving dynamics of graph embeddings, achieving better performance with lower computational costs. For more related work, refer to~\cite{cen2023cogdl,ZhangQianru,LiBolian,zhu2023wingnn}. 
The main issue with NN-based methods is their separation of dynamic graph embedding and clustering into two independent processes, making it difficult to ensure that graph embedding provides the most suitable features for clustering~\cite{DBLP:conf/ijcai/DongZSLC18,9531337}. Furthermore, most of them face weak scalability and interpretability issues on large-scale graphs~\cite{Hyenonsoo,WangCLZNL23}. 
%cannot consider topological and temporal information simultaneously. 
Thus, we focus on separated matrix factorization, jointly optimizing dynamic graph embedding and clustering, and improving scalability and interpretability. %(Please see details in Fig.{\ref{Hyper_parameter_tuning}(C)}).

%\vspace{3pt}
\noindent \textbf{Matrix Factorization-based Methods.} 
Matrix factorization-based methods cluster nodes at each timestamp using matrix factorization while optimizing the temporal smoothness of node embeddings among different timestamps. 
Recently, numerous methods with different strategies have been proposed to improve temporal smoothness. 
For example, sE-NMF~\cite{Ma17b}, jLMDC~\cite{9531337}, and NE2NMF~\cite{DBLP:journals/kbs/LiZDGM21} estimate temporal smoothness by analyzing topology changes between graphs at the current and previous timestamps, while PisCES~\cite{LiuF1801} smooths clusters by considering topology changes across the entire dynamic graph. 
%%%%
In contrast, other methods use clustering metrics or reconstruction loss to measure temporal smoothness. For example, DynaMo~\cite{DBLP:journals/tkde/ZhuangCL21} improves temporal smoothness by incrementally maximizing modularity between successive graphs, and PMOEO~\cite{DBLP:journals/kbs/ShenYTG22} and MODPSO~\cite{DBLP:journals/isci/YinZLD21} employ evolutionary algorithms to minimize the NMI of clusters across different timestamps. %ePMCL~\cite{WangWLGLZ22} and HMM-MODCD~\cite{DBLP:journals/air/AbboodAHEP23} use hidden Markov models to reconstruct clusters from previous timestamps and capture evolutionary patterns.
%%%%%%%%%%%%%%%%
Although these methods can simultaneously optimize clustering accuracy and temporal smoothness, they often suffer from low robustness and lack fine-grained node-level temporal smoothing strategies. In this study, we address these issues and enhance robustness, scalability, and practicality for large-scale real-world dynamic graphs. 

%\input{Main Paper/3-Preliminary}

\begin{figure*}[t]
\centering
\includegraphics*[clip=true,width=0.9\textwidth]{Figure/MFa6.pdf}
\caption{Overview architecture of proposed DyG-MF. 
Our method (a) first selects temporal landmarks and (b) randomly divides nodes into several groups for (c) separated matrix factorization ((a)-(c) introduced in Sec~{\ref{main_TSMF}}). 
In addition, we apply (d) bi-clustering regularization (Sec~{\ref{bi-clustering-module}})
and (e) selective embedding updating (Sec~{\ref{topological_dynamics}}) to dynamic graph clustering. 
}\label{figure:illustrate_smftii2}
%\vspace{-8pt}
\end{figure*}



\section{Preliminary}
\label{preliminaries}

%In this section, we introduce notations, task definition, and some necessary prior background knowledge of our proposed DyG-MF.
%======================

%\vspace{3pt}
\noindent \textbf{Dynamic Graph Clustering.} 
We consider a dynamic graph as a sequence of snapshots and the $t$-th snapshot $\mathcal{G}_{t}=(\mathcal{V}_{t}, \mathcal{E}_{t}$), defined for $0\leq t\leq \tau$.
Here, $\mathcal{V}_{t}$ and $\mathcal{E}_{t}$ represent the set of nodes and edges in the $t$-th snapshot. Let the graph contain $n$ nodes
and $W_{t} \in \mathbb{R}^{n \times n}$ and $M_{t} \in \mathbb{R}^{n \times n}$ represent the weighted adjacency matrix and pointwise mutual information matrix~\cite{Qiu1} for the $t$-th snapshot $\mathcal{G}_{t}$, respectively. 
In $M_{t}$, each element $m_{ij}=\log \frac{w_{ij}\sum_{k}d_{k}}{d_{i}d_{j}}$ with $d_{i}$ as the degree of the $i$-th node.
%
Dynamic graph clustering seeks to detect a set of non-overlapping clusters for $\mathcal{G}_{t}$, which corresponds to a partition of $\mathcal{V}_{t}$. This partition is represented as $\mathcal{V}_{t}=\{V_{i,t}\}_{i=1}^{\varrho_{t}}$, where $\varrho_{t}$ represents the number of clusters. 

\noindent \textbf{Matrix Factorization.} 
We first introduce a matrix factorization-based baseline, which we refer to as temporal matrix factorization.
Inspired by Qiu et al.~\cite{Qiu1}, factorizing pointwise mutual information (PMI) matrix $M_{t}$ is equivalent to Skip-gram-based graph embedding~\cite{perozzi2014deepwalk}, which can encode graph topology information. %(More details in Appendix~{\ref{section:embedding_as_matrix_factorization}}). 
Using matrix factorization, graph embedding can be formulated as:
\begin{equation}\label{equ1}
\mathcal{L}^{\text{Sc}}_{t}=\|M_{t}-C_{t}H_{t}\|_{\text{F}}^{2},
\end{equation}
where $\mathcal{L}^{\text{Sc}}_{t}$ represents the snapshot clustering (Sc) cost, each row of $C_{t} \in \mathbb{R}^{n \times r}$ denotes the embedding of the corresponding node, each column of $H_{t} \in \mathbb{R}^{r \times n}$ denotes the embedding of the corresponding node when it is considered as context for other nodes, and $r \ll n$ is the number of selected features at timestamp $t$.

Jointly considering node embedding learning and clustering can mutually reinforce each other, \textit{e.g.,} node embedding learning can select the most suitable features for clustering. 
Inspired by Chris Ding et al.~\cite{DBLP:conf/sdm/DingH05}, 
adding non-negativity and normalization constraints to matrix factorization of Eq.{(\ref{equ1})} makes it equivalent to spectral clustering.
Therefore, $\mathcal{L}^{\text{Sc}}_{t}$ can be re-formulated as follows: 
\begin{equation}\label{equ2}
\mathcal{L}^{\text{Sc}}_{t}=\|M_{t}-C_{t}H_{t}\|_{\text{F}}^{2},\quad s.t.\,\, C_{t}\geq 0, \,\, H_{t}\geq 0, \,\, C_{t}\bm{1}=\bm{1}, 
\end{equation}
where each row of $C_{t}$ not only represents the embedding of the corresponding node, but also represents the clustering index of the node, \textit{i.e.,} the rank of the maximum value in each row represents its corresponding clustering category. For example, the $i$-th node with three dimensions $C_{i,:}$=[0.2, 0.7, 0.1] belongs to the second cluster. 

Incorporating temporal information to constrain node community changes between consecutive timestamps consistently can always enhance the accuracy of graph clustering. 
We can simply define the temporal smoothing (Ts) cost as follows: 
\begin{equation}\label{equ3}
\mathcal{L}^{\text{Ts}}_{t}=\|C_{t}-C_{t-1}\|_{\text{F}}^{2},\quad s.t.\,\, C_{t}\geq 0, \,\, C_{t}\bm{1}=\bm{1}.
\end{equation}

\noindent Based on Eqs.{(\ref{equ2})} and {(\ref{equ3})}, we obtain overall objective function as:
\begin{equation}\label{equ4}
\mathcal{O}_{t} = \mathcal{L}^{\text{Sc}}_{t} + \alpha \mathcal{L}^{\text{Ts}}_{t},\quad s.t.\,\, C_{t}\geq 0, \,\, H_{t}\geq 0, \,\, C_{t}\bm{1}=\bm{1}, 
\end{equation}
where $\mathcal{L}^{\text{Sc}}_{t}$ measures the cluster quality of the $t$-th snapshot, $\mathcal{L}^{\text{Ts}}_{t}$ measures the differences of clustering results between the $t$-th and the $(t-1)$-th snapshots, and $\alpha$ is a hyperparameter to balance the importance of there two items (we set $\alpha$=0 when $t$=1). Please note that a higher $\mathcal{O}_{t}$ indicates worse clustering quality or smoothness. 

%\input{Main Paper/4-Methodology}
\section{Methodology}
\label{methodology}

As shown in Figure~{\ref{figure:illustrate_smftii2}}, our method DyG-MF consists of three main components: \textbf{\textit{(i) temporal separated matrix factorization}} jointly learns graph embedding and clustering in Sec {\ref{main_TSMF}}, \textbf{\textit{(ii) bi-clustering regularization}} reduces noise and enhance robustness in Sec {\ref{bi-clustering-module}}, and \textbf{\textit{(iii) selective embedding updating}} aims at better embedding alignment in Sec {\ref{topological_dynamics}}. We will introduce each component in order.


\subsection{Temporal Separated Matrix Factorization}
\label{main_TSMF}


Directly optimizing Eq.{(\ref{equ4})} is unacceptable time-consumption for large-scale dynamic graphsm, since its time and space complexity is $\mathcal{O}(n^{3})$ and $\mathcal{O}(n^{2})$ for a graph with $n$ nodes. 
To solve this issue, we propose temporal separated matrix factorization which transforms one large matrix factorization problem into several small matrix factorization sub-problems. The key point is to select a few nodes, called landmarks, to ensure consistency and coherence in node embeddings across all small matrix factorization.
% Here, we propose a novel landmark selection strategy for dynamic graph clustering and prove its effectiveness in Sec.{\ref{sec:ablation_study}}.

%\vspace{3pt}
\noindent \textbf{Temporal Landmark Selection in Fig~{\ref{figure:illustrate_smftii2}(a)}.}  
% Previous studies either select landmarks randomly or use a greedy search~\cite{li2019sepne}. However, 
% % they focus on graph embedding rather than clustering and are restricted to static graphs, 
% they are overlooking temporal information. 
% To solve this issue, we propose a temporal landmark selection strategy. 
% The core idea behind matrix factorization-based graph embedding is to identify a more suitable set of basis vectors, such as an orthogonal basis, to better represent the nodes. Specifically, each node, initially represented by a row in $M_{t}$, is re-embedded into a new space, where it is represented by a corresponding row in $C_{t}$.
% %%%%%%%%%%%%%%%%%%%%%%%%
% According to the above findings, before obtaining new graph embedding of nodes, we can first consider each row of $M_{t}$ as node embedding. We then give the formal definition of temporal landmarks in Definition.{\ref{definition-1}}.
% %%%%%%%%%%%%%%%%%%%%%%
% \begin{definition}\label{definition-1}
% Let $M_{t-1}$ and $M_{t}$ share the same hidden embedding space, \textit{i.e.,} node embeddings are represented using the same set of basis vectors. Then, the temporal landmarks $U_{t}$ are defined as a set of the most representative nodes that can serve as approximate basis vectors to re-represent other nodes in both $\mathcal{G}_{t-1}$ and $\mathcal{G}_{t}$.
% \end{definition}
%%%%%%%%%%%%%%
A simple idea is to select the nodes closest to each cluster center as landmarks, ensuring that these nodes are representative at the current timestamps. 
If we follow K-means clustering, the $l$-th cluster centers at the $t$-th timestamp $\bm{\theta}_{l,t}$ can be found by repeating the following process:
\begin{equation}\label{landmark_}
%\bm{\theta}_{l,t} \leftarrow
\mathop{\arg\min}\limits_{\{\Theta_{l,t}\}_{l=1}^{\varrho_{t}}}\,\,\, \sum_{l=1}^{\varrho_{t}}\sum_{a \in \Theta_{l,t}} (\|\bm{m}_{a.,t}-\bm{\theta}_{l,t}\|^{2}_{2} ), 
\end{equation}
where $\bm{m}_{a.,t}$ is the $a$-th row vector of $M_{t}$, $\varrho_{t}$ is the number of cluster centers, automatically determined by the elbow method~\cite{thorndike1953belongs},  $\{\Theta_{l,t}\}_{l=1}^{\varrho_{t}}$ represent current nodes' clusters, $ a \in \Theta_{l,t}$ indicates that the $a$-th row vector $\bm{m}_{a.,t}$ is closest to the $l$-th cluster center $\bm{\theta}_{l,t}$.

The problem with the above strategy is that it does not consider successive timestamps.
To solve this issue, we propose a temporal landmark selection strategy. We re-formulate Eq.{(\ref{landmark_})} as follows: 
% A natural idea is to select the nodes closest to each cluster center as landmarks, ensuring that these nodes are representative at both current and previous timestamps. 
% To achieve this, we first minimize the following objective function to determine the cluster centers: 
\begin{equation}\label{landmark}
%\bm{\theta}_{l,t} \leftarrow{\rm argmin}
\mathop{\arg\min}\limits_{\{\Theta_{l,t}\}_{l=1}^{\varrho_{t}}}\,\,\, \sum_{l=1}^{\varrho_{t}}\sum_{a \in \Theta_{l,t}} (\|\bm{m}_{a.,t}-\bm{\theta}_{l,t}\|^{2}_{2} + \lambda \|\bm{m}_{a.,t-1}-\bm{\theta}_{l,t} \|^{2}_{2}), 
\end{equation}
where the second term ensures that the selected landmarks are still representative in consecutive timestamps,
% $\varrho_{t}$ is the number of cluster centers, automatically determined by the elbow method~\cite{thorndike1953belongs},  $\{\Theta_{l,t}\}_{l=1}^{\varrho_{t}}$ represent the clusters, $ a \in \Theta_{l,t}$ indicates that the $a$-th row vector $\bm{m}_{a.,t}$ is closest to the $l$-th cluster center $\bm{\theta}_{l,t}$, 
and $\lambda$ serves as a hyperparameter to balance the importance of these items. 
%to control the importance of two items. 
Fortunately, we can efficiently derive an analytical solution of Eq.{(\ref{landmark})} as follows: 
% \begin{equation}\label{equ6}
% \bm{\theta}_{l,t} \leftarrow \frac{\sum_{a \in \Theta_{l,t}} (1+\lambda)(\bm{m}_{a.,t}+\bm{m}_{a.,t-1})}{|\Theta_{l,t}|}, \quad t >1, 
% \end{equation}
\begin{equation}\label{equ6}
\bm{\theta}_{l,t}= 
\begin{cases}
 \frac{\sum_{a \in \Theta_{l,t}} \bm{m}_{a.,t}}{|\Theta_{l,t}|}, & \text{ if } t = 1, \\
 \\
 \frac{\sum_{a \in \Theta_{l,t}} (1+\lambda)(\bm{m}_{a.,t}+\bm{m}_{a.,t-1})}{|\Theta_{l,t}|}, & \text{ if } t > 1.
\end{cases}
\end{equation}
where $|\Theta_{l,t}|$ denotes the number of samples in the $l$-th cluster. By iteratively updating Eq.{(\ref{equ6})} until convergence, we assign the nearest $|U_{t}|/\varrho_{t}$ nodes to the $l$-th cluster center $\bm{\theta}_{l,t}$ ($l\in\{1,\dots,\varrho_{t}\}$) to $U_{t}$, where $U_{t}$ denotes the temporal landmarks at the $t$-th timestamp and $|U_{t}|$ denotes the number of samples in $U_{t}$.

%Landmarks Matrix Factorization.
We then define the PMI matrix for temporal landmarks $|U_{t}|$ as $M^{00}_{t}$, which requires being factorized first as follows:
\begin{equation}\label{equ7}
\mathcal{L}^{\text{Lm}}_{t}=  \|M^{00}_{t}-\Phi_{t} \Psi_{t}\|_{\text{F}}^{2},\quad s.t., \Phi_{t} \ge 0, \Psi_{t} \ge 0, \Phi_{t} \textbf{1} = \textbf{1},
\end{equation}
where $\Phi_{t},\Psi_{t}$ are basis and coefficient matrices, respectively. These matrices are kept fixed during the following processes to serve as references, ensuring node embeddings' coherence and consistency. 


%\vspace{3pt}
\noindent \textbf{Randomly Subset Separation in Fig~{\ref{figure:illustrate_smftii2}(b)}.} We randomly divide all nodes, except those selected as landmarks of $\mathcal{G}_{t}$, into $s$ subsets. We find that $s = 50$ is suitable for all large-scale dynamic graphs.

%\vspace{3pt}
\noindent \textbf{Separated Matrix Factorization in Fig~{\ref{figure:illustrate_smftii2}(c)}.} 
% We define the PMI matrix for landmarks as $M^{00}_{t}$, which requires being factorized first as follows:
% \begin{equation}\label{equ7}
% \mathcal{L}^{\text{Lm}}_{t}=  \|M^{00}_{t}-\Phi_{t} \Psi_{t}\|_{\text{F}}^{2},
% \end{equation}
% where $\Phi_{t},\Psi_{t}$ are basis and coefficient matrices, respectively. These matrices are kept fixed during temporal separated matrix factorization to serve as references, ensuring consistency of the embedding.  
After dividing all nodes into $s$ subsets, Eq.{(\ref{equ2})} can be re-formulated as follow:
\begin{equation}\label{equ8}
\footnotesize
M_{t}=
\left(
\begin{array}{ccc}
M^{11}_{t} & \cdots & M^{1s}_{t} \\
\vdots  & \ddots & \vdots \\
M^{s1}_{t} &\cdots & M^{ss}_{t} \\
\end{array}
\right) \approx 
\left(
\begin{array}{ccc}
C^{1}_{t}H^{1}_{t} & \cdots & C^{1}_{t} H^{s}_{t} \\
\vdots  & \ddots & \vdots \\
C^{s}_{t}H^{1}_{t} &\cdots & C^{s}_{t} H^{s}_{t} \\
\end{array}
\right), \,\,s.t.
\left[
\begin{array}{ccc}
C_{t}\geq 0\\
H_{t}\geq 0,\\ 
C_{t}\bm{1}=\bm{1}\\
\end{array}
\right]
\end{equation}
where $M^{ii}_{t}$ repents intra-subset information within the $i$-th subset, while $M^{ij}_{t}$ captures inter-subset information between two subsets. % $i$-th and $j$-th subsets. 
Independently factorizing these matrices can result in a significant embedding drift between $C_{t}^{i}$ and $C_{t}^{j}$, \textit{i.e.,} nodes in these subsets may be projected into different hidden spaces with distinct basis vectors. 

\begin{theorem}\label{theo-1}
For $\forall i$ satisfying $1 \leq i \leq s$, assuming $C_{t}^{i}$ and $H_{t}^{i}$ in Eq.{(\ref{equ8})} can be linearly represented by the basis and coefficient matrices of the landmarks, i.e., $C^{i}_{t}=P^{i}_{t}\Phi_{t}$ and $H^{i}_{t}=\Psi_{t} Q^{i}_{t}$. Then, jointly considering the matrix factorization of the landmarks $M_{t}^{00}$ with each sub-matrix ensures embedding consistency between subsets of nodes.
\end{theorem}



\noindent According to Theorem~{\ref{theo-1}}, to ensure embedding consistency of intra-subsets, intra-subsets matrix factorization is formulated as follows:
\begin{small}
\begin{align}\label{equ9}
\mathcal{L}_{t}^{\text{intra}}
&=\sum_{i=1}^{s}(\|M^{ii}_{t}-C^{i}_{t}H^{i}_{t}\|^{2}_{\text{F}}+\|M^{0i}_{t}-\Phi_{t}H_{t}^{i} \|^{2}_{\text{F}} + \|M^{i0}_{t}-C^{i}_{t}\Psi_{t} \|^{2}_{\text{F}}) \\ %\label{equ9}\\
&=\sum_{i=1}^{s}(\|M^{ii}_{t}-P^{i}_{t}M_{t}^{00} Q^{i}_{t}\|^{2}_{\text{F}}+\|M^{0i}_{t}-M_{t}^{00} Q^{i}_{t} \|^{2}_{\text{F}} + \|M^{i0}_{t}-P^{i}_{t}M_{t}^{00} \|^{2}_{\text{F}}). \nonumber%\label{equ10}
\end{align}
\end{small}

\noindent And inter-subsets matrix factorization can be formulated as follows:
\begin{equation}\label{equ10}
\small
\mathcal{L}_{t}^{\text{inter}}=
\sum_{1\leq i\leq s, i\neq j}^{s} \|M^{ij}_{t}- P^{i}_{t}M^{00}_{t}Q^{i}_{t}\|^{2}_{\text{F}}+\|M^{ji}_{t}-P^{j}_{t}M^{00}_{t}Q^{i}_{t}\|^{2}_{\text{F}}.
\end{equation}

% where we set $C_{i,t}=P_{i,t}\Phi_{t},$ and $ H_{i,t}=\Psi_{t} Q_{i,t}.$

\noindent Finally, the overall objective function can be formulated as follows:
\begin{equation}\label{equ11}
\footnotesize
\mathcal{O}_{t} = \mathcal{L}_{t}^{\text{intra}} + \mathcal{L}_{t}^{\text{inter}} + \alpha \sum_{i=1}^{s}\|C_{t}^{i}-C_{t-1}^{i}\|^{2}_{\text{F}},
\,\, s.t. C^{i}_{t},\,H^{i}_{t} \ge 0, \,C^{i}_{t} \bm{1}=\bm{1}
\end{equation}
% For the proof and optimization process, please refer to Appendix~{\ref{append_equ11}}.




\subsection{Bi-clustering Regularization}
\label{bi-clustering-module}

Real-world dynamic graphs always contain much noise and irregular evolution patterns, directly obtaining communities from $C_{t}$ will be easily affected by noisy data (Figure~{\ref{fig:noisy}}). 
To improve robustness against noise and jointly optimize graph embedding and clustering, 
we introduce bi-clustering theory~\cite{nie2019k} as a regularization item into our overall objective function Eq.{(\ref{equ11})}. To realize this goal, we first introduce the nuclear norm theory as follows.

\begin{theorem}\label{lemma-2}
Let $L_{S_{t}}$=$I-D^{-1/2}S_{t}D^{-1/2}$ be the normalized Laplacian matrix, where $D$ is the degree matrix of $S_{t}$. 
The multiplicity $k$ of the eigenvalue 0 of $L_{S_{t}}$$\in$ $\mathbb{R}^{n \times n}$ is equal to the number of connected components of the bipartite graph $S_{t}$ =$
\begin{scriptsize}
\begin{pmatrix}
0  & C_{t} \\
C^{T}_{t}  & 0
\end{pmatrix}    
\end{scriptsize}
$, where $n$ denotes the dimension of $L_{S_{t}}$ and $T$ indicates the matrix transpose operation. 
\end{theorem}

\noindent Theorem~{\ref{lemma-2}} indicates that if $rank(L_{S_{t}}) = n-k$, $S_{t}$ has $k$ purity connected components (clusters), \textit{i.e.,}  we need to minimize the $k$ smallest eigenvalues of $L_{S_{t}}$ to be 0.  
Suppose $\sigma_{i}(L_{S_{t}})$ is the $i$-th smallest eigenvalue of $L_{S_{t}}$ and $\sigma_{i}(L_{S_{t}}) \geq 0$ since $L_{S_{t}}$ is positive semi-defined. Then, the issue can be formulated as $\sum_{i=1}^{k}\sigma_{i}(L_{S_{t}}) \approx 0$, however, optimizing this item is difficult. 
According to KyFan's Theorem~\cite{KyFan50}, minimizing the sum of $k$ smallest eigenvalues can be transformed into an easy trace optimization issue, $\sum_{i=1}^{k}\sigma_{i}(L_{S_{t}})$$\iff$$Tr(F_{t}^{T}L_{S_{t}}F_{t})$, where $Tr()$ denotes the trace of the matrix and $F_{t}$ is a learnable parameter matrix with orthogonality constraint. 

\begin{theorem}\label{theo-3}
The bi-clustering regularization on the $i$-th subset 
$S_{t}^{i}$ 
%=$
% \begin{footnotesize}
% \begin{pmatrix}
% 0  & C_{t}^{i} \\
% (C^{i}_{t})^{T}  & 0
% \end{pmatrix}    
% \end{footnotesize}
% $ 
is equal to the imposing constraints on $C_{t}^{i}$, \textit{i.e.,} when $S_{t}^{i}$ contains $k$ pure clusters, 
$C_{t}^{i}$ will also exhibit $k$ pure clusters. And bi-clustering regularization is decomposable, $\textit{i.e.,}$ the constraint on the matrix $M_{t}$ is equal to the constraint on each of its node subsets.
\end{theorem}


\noindent According to Theorem~{\ref{theo-3}}, we can add the bi-clustering regularization (Bcr) to each subset. Then, Eq.{(\ref{equ11})} can be re-formulated as follows: 
\begin{small}
\begin{align}\label{equ12}
&\mathcal{O}_{{t}} =\mathcal{L}_{t}^\text{intra}+\mathcal{L}_{t}^{\text{inter}} +\alpha \sum_{i=1}^{s}\|C_{t}^{i}-C_{t-1}^{i}\|^{2}_{\text{F}} + \beta \mathcal{L}_{t}^\text{Bcr},\\
&s.t.\, C^{i}_{t}, H^{i}_{t} \ge 0, C^{i}_{t} \bm{1}=\bm{1}, \mathcal{L}_{t}^\text{Bcr} = \sum_{i=1}^{s}Tr((F^{i}_{t})^{T}L_{S^{i}_{t}}F^{i}_{t}),(F^{i}_{t})^{T} F^{i}_{t}=\text{I},\nonumber
\end{align}
\end{small}where I is the identity matrix,
and $\alpha,\beta$ are hyperparameters. 
%Proof is shown in Appendix~{\ref{append_proof}}.
%and Appendix~{\ref{Appendix_Parameters}}. 




\subsection{Selective Embedding Updating}
\label{topological_dynamics}

The item $\sum_{i=1}^{s}\|C^{i}_{t}-C^{i}_{t-1}\|_{\text{F}}^{2}$ in Eq. {(\ref{equ12})} ensures temporal smoothness between timestamps; however, the primary focus is on overall smoothness, and the fine-grained smoothness between individual node pairs is overlooked. 
This results in heterogeneity in node embedding between successive snapshots, thus severely undermining interpretability and visualizability during the analysis of dynamic community trajectories.
% Furthermore,  using only previous and current snapshots to estimate temporal smoothness limits the model's ability to fully use the temporal information.
%
To avoid this issue and further improve clustering efficiency and accuracy, 
we devise a fine-grained node-level temporal smoothing strategy. We first separating nodes into static and dynamic groups and then update only the embeddings of those dynamically changing nodes, while the embeddings of static nodes are
fixed and shared between each timestamp. 
% We introduce them in order as follows.


% \begin{definition}\label{define-2}
Most nodes in dynamic graphs follow gradual and stable evolution patterns, maintaining their embeddings relatively unchanged over time~\cite{Liu_Zhou_Zhu_Gu_He_2020}. The remaining dynamic nodes are defined as those whose topological structures undergo significant changes or whose positions shift considerably relative to dynamic landmarks.
% \end{definition}

Based on this assumption,  dynamic nodes can be defined as:
\begin{equation}\label{equ13}
\small
\Delta \epsilon_{a,t} = \underbrace{\|\bm{w}_{a.,t}-\overline{\bm{w}}_{a.,t} \|^{2}_{2}}_{\textbf{topological changes}} + \underbrace{\|(\bm{w}_{a.,t}-\bm{\theta}_{a,t})-(\bm{w}_{a.,t}-\overline{\bm{\theta}}_{a,t})\|_{2}^{2}}_{\textbf{relative positions shift}},
\end{equation}
where $\bm{w}_{a.,t}$ is the $a$-th row of weighted adjacency matrix $W_{t}$, 
$\overline{\bm{w}}_{a.,t}$ is the average among three successive snapshots: $\overline{\bm{w}}_{a.,t}=(\bm{w}_{a.,t-1}+\bm{w}_{a.,t}+\bm{w}_{a.,t+1})/3$, $\bm{\theta}_{a,t}$ is the clustering center closest to $\bm{w}_{a.,t}$, and $\overline{\bm{\theta}}_{a.,t}$ is the averaged among three clustering centers closest to $\bm{w}_{a.,t}$, \textit{i.e.,} $\overline{\bm{\theta}}_{a.,t}=(\bm{\theta}_{a.,t-1}+\bm{\theta}_{a.,t}+\bm{\theta}_{a.,t+1})/3$.
When $t$=1 or $t$=$\tau$, we ignore $\bm{w}_{a.,0}$/$\bm{\theta}_{a.,0}$ and $\bm{w}_{a.,\tau +1}$/$\bm{\theta}_{a.,\tau+1}$, respectively.
$\Delta \epsilon_{.,t}$ can be considered as a threshold, measuring the dynamics of each node, to divide the nodes into a dynamic set $X_{t}$ with $\mu$\% nodes and a static set $Y_{t}$. 

We then fix the static node embeddings in $Y_{t}$ unchanged and only update the $\mu\%$ dynamic node embeddings in $X_{t}$. Then, landmarks factorization of Eq.{(\ref{equ7})} can be re-formulated as follows:
\begin{equation}\label{equ14}
\small\tilde{\mathcal{L}}^{\text{Lm}}_{t}=\left\|
\left(
\begin{array}{cc}
M^{00}_{xx,t} & M_{xy,t}^{00} \\
M_{yx,t}^{00} & M_{yy,t}^{00}
\end{array}
\right)-
\left(
\begin{array}{cc}
\Phi_{x,t} \\
\Phi_{y,t}
\end{array}
\right) (\Psi_{x,t},\Psi_{y,t}) \right\|_{\text{F}}^{2},
\end{equation}
\begin{equation*}
s.t. \quad \Phi_{y,t} = \Phi_{y,t-1}, \,\, \Psi_{y,t} = \Psi_{y,t-1},
\end{equation*}
where $M_{xx,t}^{00}$ and $M_{yy,t}^{00}$ represent the PMI matrix of 
static and dynamic nodes in $M_{t}^{00}$, respectively. $\Phi_{x,t}$ and $\Phi_{y,t}$ denote the sub-blocks of $\Phi_{t}$ for the dynamic and static landmarks, respectively. 

Following Eq.{(\ref{equ14})}, the overall objective function Eq.{(\ref{equ12})} can be factorized by only updating dynamic node embeddings while removing the temporal smoothing item, re-formulated as follows:
\begin{equation}\label{equ15}
\mathcal{O}_{t}= \tilde{\mathcal{L}}_{t}^\text{intra}+\tilde{\mathcal{L}}_{t}^{\text{inter}} +\beta\tilde{\mathcal{L}}_{t}^\text{Bcr},
\end{equation}
where $\tilde{\mathcal{L}}_{t}^\text{intra}$, $\tilde{\mathcal{L}}_{t}^{\text{inter}}$, and $\tilde{\mathcal{L}}_{t}^\text{Bcr}$ are the versions where the selective embedding updating has been applied. By applying the constraint of fixing static node embeddings, we gain two advantages: \textbf{\textit{(i)} it prevents updates to static node embeddings from introducing noise}, allowing us to better leverage historical information to improve model performance; \textbf{\textit{(ii)} it allows us to remove the smoothness term} $\|C^{i}_{t}-C^{i}_{t-1}\|_{\text{F}}^{2}$ in Eq.{(\ref{equ12})}, which significantly reduces
computational cost. %Please refer to Appendix~{\ref{append_optimization}} for a detailed definition and optimization of Eq.{(\ref{equ15})}.











\subsection{Complexity Analysis}

\noindent \textbf{Time Complexity.}
The time complexity of selecting $|U_{t}|$ landmarks with $\varrho_{t}$ clustering centers of all nodes $|V_{t}|$ using Eq.{(\ref{equ6})} is $\mathcal{O}(|V_{t}|\varrho_{t}l_{1})$, where $l_{1}$ is the number of iterations for converging to the optimal global solution. 
The time complexity of performing $\Phi_{t}$ and $\Psi_{t}$ %by using %Eq.{(\ref{C-12})} and Eq.{(\ref{C-14})} 
is $\mathcal{O}(|U_{t}|^{2}rl_{2})$, 
where r is the number of dimensions and $l_{2}$ is the number of iterations to optimize Eq.{(\ref{equ14})} by gradient descent. 
The time complexity of updating $F^{i}_{t}$ %in Eq.{(\ref{C-27})} 
for block $C^{i}_{t}$ with $|\Gamma^{i}_{t}|$ nodes is $\mathcal{O}(|U_{t}|^{3}+|U_{t}|^{2}|\Gamma^{i}_{t}|)$~\cite{nie2019k}. 
Taking into account the above complexity, the total time complexity of using our method for dynamic graphs with $\tau$ timestamps is $\mathcal{O}(|U_{t}|^{3}+|U_{t}|^{2}|\Gamma^{i}_{t}|+ |U_{t}|^{2}rl_{2}+|V_{t}|\varrho_{t}l_{1})=\mathcal{O}(\max\{\,|U_{t}|,|\Gamma^{i}_{t}|, rl_{2}\,\}\,|U_{t}|^{2}),$ while standard matrix factorization methods need time complexity of $\mathcal{O}(|V_{t}|^{3})$. 
Compared to standard matrix factorization methods, our method is more efficient.
%has much lower time complexity,  demonstrating its efficiency and practicability.

%\vspace{3pt}
\noindent  \textbf{Space Complexity.} 
Since our method takes only snapshots $\mathcal{G}_{t-1}$, $\mathcal{G}_{t}$, and $\mathcal{G}_{t+1}$ as input to identify communities in the $t$-th timestamp, the space complexity is $\mathcal{O}(|V_{t}|^{2})$ including the space $\mathcal{O}(|V_{t}|r)$ to store the matrices $C_{t}$ and $H_{t}$ with $r$ as the dimensions of matrix. 
%\input{Main Paper/5-Experiment}

%\input{Main Paper/5-Experiment-2}
\section{Experiment}
\label{experiments}

\noindent \textbf{Datasets.} 
In Table~{\ref{table1}}, we evaluate baselines on six synthetic dynamic graphs and five real-world dynamic graphs. % with varying number of nodes and edges. 
Synthetic dynamic graphs are generated following regular evolution rules. SYN-FIX/SYN-VAR~\cite{KimH09} randomly exchange communities of some nodes. Green datasets~\cite{Folina2013} consider four evolution events including  \textit{Birth-Death},  \textit{Expand-Contract}, \textit{Hide}, and \textit{Merge-Split}.
We also evaluate on real-world domains, including \textit{Academic Graphs}: arXiv~\cite{DBLP:conf/kdd/LeskovecKF05}; \textit{Social Graphs}: Dublin~\cite{ISELLA2011166} and Flickr~\cite{mislove-2008-flickr}
and \textit{Website Interaction Graphs}: Wikipedia~\cite{DBLP:conf/icwsm/LeskovecHK10} and Youtube~\cite{mislove-2008-flickr}. Appendix{~\ref{Appendix of Datasets}} provides more details.


\renewcommand\arraystretch{1}
\begin{table}[t]
\footnotesize
\centering 
\caption{Detailed statistics of dynamic graph benchmarks.}
%\vspace{-5pt}
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{@{}lccc@{}}
\toprule 
\textbf{Dynamic Graphs} & \textbf{\# of Nodes} & \textbf{\# of Edges} & \textbf{\# of Snapshots} \\
\midrule 
\midrule

% \multirow{5}{*}{\begin{tabular}[c]{@{}l@{}} \textbf{\textit{Artificial}} \end{tabular}}

%\multicolumn{4}{c}{\small{\lidy{Artificial dynamic graphs}}} \\
%\midrule 
SYN-FIX & 128    & 1,248,231 & 10    \\
SYN-VAR & 256    & 6,259,526 & 10    \\
Birth-Death & 30K\,\&\,100K   & 12M\,\&\,24M    & 10\,\&\,20 \\
Expansion & 30K\,\&\,100K   & 13M\,\&\,26M    & 10\,\&\,20 \\
Hide & 30K\,\&\,100K   & 13M\,\&\,28M    & 10\,\&\,20 \\
Merge-Split & 30K\,\&\,100K   & 14M\,\&\,29M    & 10\,\&\,20 \\
\midrule
% \multicolumn{4}{c}{\small{\lidy{Real-World dynamic graphs}}} \\
% \midrule
% \multirow{5}{*}{\begin{tabular}[c]{@{}l@{}} \textbf{\textit{Real-World}} \end{tabular}}
Wikipedia   & 8,400 & 162,000   & 5 \\
Dublin  & 11,000    & 415,900   & 5 \\
arXiv   & 28,100    & 4,600,000 & 5 \\
Flickr  & 2,302,925 & 33,100,000    & 5    \\
Youtube & 3,200,000 & 12,200,000    & 5    \\
\bottomrule
\end{tabular}}
\label{table1}
%\vspace{-0.3cm} 
\end{table}

%\vspace{3pt}
%remove deepwalk, add TGC jLMDC and RDMA_NET
\noindent \textbf{Baselines.} We compare our method with 14 best-performing baselines, \textit{i.e.,} Neural Network-based methods: 
CSEA~\cite{DBLP:journals/eswa/FeiWHLL23}, DSCPCD~\cite{10017356}, 
% \textit{Two-stage methods}: 
SepNE~\cite{li2019sepne}, node2vec~\cite{grover2016node2vec}, 
LINE~\cite{tang2015line}, 
RNNGCN~\cite{DBLP:conf/aaai/YaoJ21}, 
ROLAND~\cite{DBLP:conf/kdd/YouDL22}, and TGC~\cite{DBLP:conf/iclr/001400T00024}; 
and Matrix Factorization-based methods: PisCES~\cite{LiuF1801}, DYNMOGA~\cite{Folina2013}, NE2NMF~\cite{DBLP:journals/kbs/LiZDGM21}, 
RTSC~\cite{DBLP:conf/cikm/YouHKFO21}, 
RDMA~\cite{DBLP:journals/www/RanjkeshMH24},  and jL-MDC~\cite{9531337}.
Appendix~{\ref{Appendix-baselines}} provides more details about these baselines.


%\vspace{3pt}
\noindent \textbf{Implementation Details.} Following previous works~\cite{LiuF1801,Ma17b}, we use normalized mutual information (NMI)~\cite{Danon_2005} and normalized F1-score (NF1)~\cite{rossetti2017graph} to measure clustering accuracy.
We reproduced the baselines using their optimal parameters and reported average performance over five repeated runs with different random seeds. 
We conducted multiple t-tests to assess the statistical significance of the performance. %Refer to Appendix~{\ref{criteria}} for more details. 
%
We took Birth-Death-30K as validation datasets for hyperparameter tuning. With grid search, DyG-MF achieves the best performance when $s=50$, dimension of embeddings $r=1,000$, percentage of landmarks $\|U_{t}\|=0.5$, percentage of dynamic nodes $\mu=0.16$, $\lambda=0.2$ in Eq.{(\ref{landmark})} and $\beta=20$ in Eq.{(\ref{equ15})}. %Appendix~{\ref{Appendix_Parameters}} provides more details.















%========================================================
%========================================================

% \section{Discussion and Analysis}
% \label{discussion}

%\renewcommand\arraystretch{1}



\begin{table*}[t]
\footnotesize
\centering
\caption{
Overall performances on dynamic graphs.
% Methods in first, second, and third blocks are Coupled, Two-stage, and Evolutionary methods, respectively.
\textbf{Bold} and \underline{\textit{Underline}} indicates the best and second-best performing methods.
% Avg.Rank is calculated by {Pinc datasets}.
Symbol $\dag$ indicates that DyG-MF significantly surpassed all baselines with a p-value$<0.005$. The top eight methods are neural network-based methods, and the other methods are matrix factorization-based methods. N/A means that it cannot be executed due to memory and running time constraints. DyG-MF w/o TSMF, w/o BR, and w/o SEU are introduced in Sec~{\ref{sec:ablation_study}}.}
% \vspace{-6pt}
 \setlength{\tabcolsep}{0.45mm}{
\label{main_table:performance_on_artificial}
\begin{tabular}{@{}lcccccccccccccccccccccccc@{}}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{\textit{Methods}}}}
& \multicolumn{2}{c}{\textbf{\textcolor{black}{SYN-FIX}}}
& \multicolumn{2}{c}{\textbf{\textcolor{black}{SYN-VAR}}}
& \multicolumn{2}{c}{\textcolor{black}{\textbf{Birth-30K}}}
& \multicolumn{2}{c}{\textcolor{black}{\textbf{Expand-30K}}} 
& \multicolumn{2}{c}{\textbf{Hide-100K}}
& \multicolumn{2}{c}{\textbf{Merge-100K}}
& \multicolumn{2}{c}{\textcolor{black}{\textbf{Wikipedia}}} 
& \multicolumn{2}{c}{\textcolor{black}{\textbf{Dublin}}} 
& \multicolumn{2}{c}{\textcolor{black}{\textbf{arXiv}}}  
& \multicolumn{2}{c}{\textbf{Flickr}}
& \multicolumn{2}{c}{\textbf{Youtube}}
% &\multirow{2}{*}{\textbf{\textit{Avg. Rank}}}  
\\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}  
\cmidrule(lr){14-15} \cmidrule(lr){16-17}  \cmidrule(lr){18-19}  
\cmidrule(lr){20-21} \cmidrule(lr){22-23} 
\multicolumn{1}{c}{} & \textit{NMI} & \textit{NF1}         & \textit{NMI} & \textit{NF1}        
& \textit{NMI}  & \textit{NF1}  & \textit{NMI}  & \textit{NF1} & \textit{NMI}  & \textit{NF1}        & \textit{NMI}  & \textit{NF1}  & \textit{NMI}  & \textit{NF1}  & \textit{NMI}  & \textit{NF1}  & \textit{NMI}  & \textit{NF1}  & \textit{NMI}  & \textit{NF1} & \textit{NMI}  & \textit{NF1} \\
\midrule
\midrule
CSEA~\cite{DBLP:journals/eswa/FeiWHLL23} 
&70.8 &73.1 &72.4 &74.2 &88.6 &68.8 &87.3 &67.2 
&74.9 &62.3 &76.6 &61.9 
&28.6 &7.5 &29.6 &10.2 &28.3 &9.8 &30.2 &13.6 &29.6 &14.4 \\ 
DSCPCD~\cite{10017356} 
&73.2 &75.2 &76.1 &77.3 &89.2 &69.8 &88.9 &70.3 
&79.6 &63.5 &81.0 &64.4 
&28.2 &7.3 &32.4 &11.9 &31.8 &10.9  &34.3 & 18.2 & 32.6 &15.9 \\
% \midrule
SepNE~\cite{li2019sepne}
& 96.9 & 96.8 & 91.3 & 88.8 & 92.5 & \underline{89.4} & 92.1 & 81.8 
& 89.0 & 81.1 & 89.2 & 78.6  
& 31.4 & 9.8  & 49.2 & 21.0 & 42.8 & 25.8 & 40.3 & 22.4 & 39.5 & 21.0\\
node2vec~\cite{grover2016node2vec}  &98.3 &97.9 &92.6 &91.3 & 93.8 & 85.8 & 93.2 & 83.6 
& 91.2 & 85.6 & 89.0 & 78.2    
& 33.5 & 10.2 & 50.3 & 23.2 & 44.3 & 26.8 & 42.4 & 25.5 & 42.8 & 24.5\\
LINE~\cite{tang2015line}  &97.8 &97.6 &91.2 &89.3 & 92.1 & 85.9 & 92.3 & 84.8 
& 89.5 & 82.2 & 88.0 & 77.6  
& 31.2 & 9.6  & 49.8 & 21.2 & 43.2 & 26.0 & 41.5 & 24.6 & 41.9 & 23.8\\
RNNGCN~\cite{DBLP:conf/aaai/YaoJ21}   &99.2 &98.8 &95.5 &90.3 & 96.6 & 84.5 & 95.9 & {85.1} 
& 92.1 & 84.3 & 91.2 &80.9   
& 40.3 & \underline{22.5} & 52.2 & \underline{31.6} & 45.4 & 26.2 & \underline{48.5} & 30.2 & 47.6 & 32.3\\
ROLAND~\cite{DBLP:conf/kdd/YouDL22}   &98.2 &97.7 &93.8 &89.2  & 95.5 & 83.2 & 94.1 & 83.8 
& \underline{93.3} & \underline{85.8} & \underline{92.8} & \underline{81.6}
& 42.2   
& 22.3 & \underline{53.6} & \underline{31.6} & \underline{46.8} & \underline{27.6} & {47.5} & {31.4} & \underline{48.4}  & \underline{33.2} \\
TGC~\cite{DBLP:conf/iclr/001400T00024} & 98.3 & 97.9 & 93.5 & 90.6 & 95.3 & 83.0 & 93.8 & 83.6 
& 92.8 & 85.4 & 91.5 & 81.2
& 41.3 & 22.3 & 52.8 & 31.3 & 45.8 & 26.8 & 47.8 & \underline{31.6} & 47.9 & 32.6\\
\midrule
PisCES~\cite{LiuF1801} & 99.0 & 99.7 & 88.1 & 56.6 & 91.2 & 41.6 & 92.6 & 49.0 
&N/A &N/A &N/A &N/A  
& 32.1 & 9.9  & 46.3 & 16.2 & 38.2 & 14.5 & N/A & N/A  & N/A & N/A     \\
DYNMOGA~\cite{Folina2013} & 92.5 & 95.6 & 84.2 & 61.6 & \underline{98.1} & 78.1 & \underline{98.2} & 65.3 
&N/A &N/A &N/A &N/A
& 36.2 & 9.9  & 49.8 & 20.1  & 39.1 & 24.5 & N/A   & N/A & N/A & N/A \\
NE2NMF~\cite{DBLP:journals/kbs/LiZDGM21} & 97.8 & 95.9 & 94.2 & 93.6 & 97.1 & 76.1 & 97.5 & 63.1 
&N/A &N/A &N/A &N/A
& 34.1 & 8.2  & 47.9 & 18.9  & 38.2 & 22.9 & N/A   & N/A & N/A & N/A\\
RTSC~\cite{DBLP:conf/cikm/YouHKFO21} & {99.2} & {99.0} & {98.7} & {98.2} & 92.8 & 55.3 & 92.1 & 53.2 
&N/A &N/A &N/A &N/A
& 30.6 & 11.3 & 46.6 & 19.3 & 38.2 & 20.2  & N/A   & N/A & N/A & N/A \\
jLMDC~\cite{9531337} & \underline{99.7} & \underline{99.9} & \underline{99.9} & \underline{98.4} & 98.0 & 77.4 & 97.6 & 66.6 & N/A & N/A & N/A & N/A & \underline{44.6} & 22.1 & 48.3 & 21.9 & 45.6 & 26.9 & N/A & N/A &N/A &N/A \\
RDMA~\cite{DBLP:journals/www/RanjkeshMH24} & 98.4 & 97.8 & 95.5 & 94.8 & 95.3 & 69.8 & 94.8 & \underline{85.5} &N/A &N/A &N/A &N/A & 33.8 &10.2 & 47.2 & 18.6 &41.6 & 25.2 &N/A &N/A &N/A &N/A\\
% \midrule
  DyG-MF (Ours)  
& $\textbf{100}$\,\,\, 
& $\textbf{100}$\,\,\, 
& $\textbf{100}$\,\,\,  
& $\textbf{100}$  
& $\textbf{\,\,\,99.9}^{\dag}$  
& $\textbf{\,\,\,90.2}^{\dag}$  
& $\textbf{\,\,\,99.2}^{\dag}$  
& $\textbf{\,\,\,90.9}^{\dag}$  

& $\textbf{\,\,\,94.3}^{\dag}$ 
& $\textbf{\,\,\,86.5}^{\dag}$  
& $\textbf{\,\,\,94.4}^{\dag}$  
& $\textbf{\,\,\,83.2}^{\dag}$  

& $\textbf{\,\,\,50.4}^{\dag}$ &$\textbf{\,\,\,25.8}^{\dag}$ 
& $\textbf{\,\,\,56.1}^{\dag}$ & $\textbf{\,\,\,33.7}^{\dag}$ 
& $\textbf{\,\,\,51.8}^{\dag}$ & $\textbf{\,\,\,30.2}^{\dag}$ 
& $\textbf{\,\,\,52.3}^{\dag}$ & $\textbf{\,\,\,33.6}^{\dag}$ 
& $\textbf{\,\,\,51.8}^{\dag}$ & $\textbf{\,\,\,34.5}^{\dag}$ 
\\ 

\,\,\,{w/o} TSMF    & 100 & 100 & 100 & 100   & 99.9 & 90.3 & 99.3 & 90.9 
& N/A & N/A & N/A & N/A  
& 50.6 & 25.9 & 56.4 & 33.9 & 52.0 & 30.5 
& N/A & N/A & N/A & N/A \\

\,\,\,{w/o} BR     &99.0 &99.5 &88.9 &71.8   & 97.3 & 78.2 & 97.8 & 82.6 
& 90.8 & 84.5 & 88.7 & 78.1    
& 45.8 & 21.5 & 52.1 & 30.6 & 45.8 & 25.8 & 48.2 & 29.6 & 48.5 & 31.6 \\

\,\,\,{w/o} SEU & 99.8 & 99.8 & 96.5 & 94.8 & 98.9 & 88.2 & 98.9 & 86.8 
& 93.1 & 85.2 & 92.3 & 81.4       
&48.2 &23.9 &54.8 &32.4 &48.6 &28.5 &50.3 &32.8 &50.6 &33.3 
\\ \bottomrule
\end{tabular}}
\end{table*}

\begin{figure*}[h]
\centering
\includegraphics[scale=0.45]{Figure/myplot_1.pdf}
%\caption{Performance of baselines on various networks, which can show robustness of our proposed DyG-MF.}
\caption{Performance on varying timestamps of selected best-performing baselines on four real-world datasets.}
\label{main_fig:SMFT2_accuracy}
%\vspace{-0.3cm}
\end{figure*}
%

%\subsection{Comparison with State-of-the-art Baselines}
%\label{main_sota}

\subsection{Performance Evaluation} 
%\noindent \textbf{\textit{Effectiveness Evaluation.}}
The performance of various baselines in terms of NMI and NF1 scores on synthetic and real-world dynamic graphs is shown in Table~{\ref{main_table:performance_on_artificial}}. 
We observe that DyG-MF achieves the highest NMI and NF1 scores across all dynamic graphs. This can be attributed to its temporal separated matrix factorization, bi-clustering regularization, and selective embedding updating. 
Specifically, compared to the neural network-based methods RNNGCN and ROLAND, DyG-MF improves NMI scores by 5\% and 3.8\% on Flickr and Youtube, since DyG-MF jointly optimizes node embeddings and clustering, ensuring that node embeddings provide the most suitable features for clustering. 
In comparison with matrix factorization-based baselines, DyG-MF outperforms them by leveraging fine-grained temporal smoothness to capture dynamics at the node level (w/o SEU versus DyG-MF in Table~{\ref{main_table:performance_on_artificial}}) and utilizing bi-clustering regularization to reduce noise in real-world dynamic graphs (w/o BR versus DyG-MF in Table~{\ref{main_table:performance_on_artificial}}). 
%
Figure~{\ref{main_fig:SMFT2_accuracy}} further shows the performance of the baselines at each timestamp, showing that DyG-MF consistently outperforms baselines and can be effectively applied to real world.
%



% NMI and NF1 require ground-truth labels to evaluate detected clusters.  To alleviate the incorrect labels that affect the evaluation, we also employ two additional metrics, Modularity~\cite{Newman06} and Density~\cite{chen2013measuring}, to evaluate the quality of detected dynamic communities. Specifically, Modularity evaluates the quality of inter-connections between nodes within a community, while Density measures outer-connections among communities without relying on labels.
% Figure~{\ref{scalability2}} shows that DyG-MF outperforms three best-performing baselines on large-scale real-world dynamic graphs, indicating the effectiveness of DyG-MF in identifying high-quality communities.

Moreover, we also employ two additional metrics, Modularity~\cite{Newman06} and Density~\cite{chen2013measuring}, to evaluate the quality of detected dynamic communities. This is necessary because NMI and NF1 rely on ground-truth labels, which can be easily affected by incorrect labeling. 
Specifically, Modularity evaluates the quality of inter-connections between nodes within a community, while Density measures outer-connections among communities without relying on labels.
Figure~{\ref{scalability2}} shows that DyG-MF outperforms three best-performing baselines on real-world dynamic graphs, indicating the effectiveness of DyG-MF in identifying high-quality communities.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{Figure/modularity-1.pdf}
\caption{
Modularity and Density on large dynamic graphs.}\label{scalability2}
%\vspace{-10pt}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{Figure/myplot_41.pdf}
\caption{
Scalability w.r.t. varying snapshots and nodes.}\label{scalability}
%\vspace{-8pt}
\end{figure}

\begin{table*}[t]
\footnotesize
\centering
\caption{Running Time for DyG-MF and baselines on large-scale synthetic and real-world dynamic graphs (sec). N/A indicates that the corresponding methods could not be executed due to memory constraints or exceeded the time limit.}
\setlength{\tabcolsep}{2.3mm}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Methods}}} 
& \multicolumn{5}{c}{\textbf{Synthetic dynamic graphs}
 ($\downarrow$)}      
& \multicolumn{6}{c}{\textbf{Real-world dynamic graphs} ($\downarrow$)} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-12} 
& \multicolumn{1}{c}{\textbf{Bir-Dea-100K}} 
& \multicolumn{1}{c}{\textbf{Expand-100K}}   
& \multicolumn{1}{c}{\textbf{Hide-100K}}    
& \multicolumn{1}{c}{\textbf{Mer-Spl-100K}}
& \multicolumn{1}{c}{\textbf{\textit{Avg.}}}
&\multicolumn{1}{c}{\textbf{Wikipedia}}
&\multicolumn{1}{c}{\textbf{Dublin}}
&\multicolumn{1}{c}{\textbf{arXiv}}
&\multicolumn{1}{c}{\textbf{Flickr}}
&\multicolumn{1}{c}{\textbf{Youtube}}
& \multicolumn{1}{c}{\textbf{\textit{Avg.}}}\\  
\midrule
\midrule
% PisCES &485,403 & 472,153 & N/A & N/A & 12,382 &16,096 & 451,221 & N/A & N/A   \\
% DYNMOGA &241,214 &262,193 &N/A &N/A & 13,443 & 14,787 & 232,699 & N/A & N/A  \\
% NE2NMF &226,343 &223,168 &  N/A & N/A & 12,770 & 13,160 & 210,803 &  N/A & N/A  \\
CSEA &11,355  &12,233 &13,211 & 12,122 &12,230 &9,342 & 10,242 & 10,211 & 85,363 & 96,299 &42,291\\
DSCPCD &10,255 &11,323 & 10,232 &11,211 & 10,755 & 8,882 & 9,323 & 9,299 & 82,242 & 94,233 & 40,795\\ 
% \midrule
SepNE                & 7,232          & 7,599          & 7,104          & 7,562 &7,374   & 3,519 & 3,974 &5,602 &40,752 &58,608      &22,491  \\
% DeepWalk                           & 38,355         & 37,837          & 38,419          & 39,852 &38,615 & 20,332 & 25,552 &33,875 &304,200 & 381,480   &153,087        \\
LINE                                          & 23,633          & 22,566          & 20,963          & 21,555 &22,179 & 7,820 &9,464 & 13,029   & 117,000 & 132,000    &55,862    \\
node2vec                                  & 16,963          & 17,070          & 17,799          & 17,705  &17,384
& 5,474  & 7,098 & 10,032 & 99,450 & 121,440 &48,698 \\
RNNGCN            &25,342           & 24,983          & 24,518          & 25,388 &25,057 & 15,512 & 17,035 & 20,846   & 263,250 & 294,360   & 122,200       \\
ROLAND                              & 25,983          & 25,268           & 24,399         & 23,598  &24,812 & 8,602 & 10,883  & 15,374 & 146,250 & 172,920   &70,805        \\
TGC &21,252 & 20,488 &19,458 & 20,269 & 20,366 & 8,420 &10,232 & 14,535 & 138,455  &168,345 & 67,997\\
\midrule
PisCES & N/A & N/A & N/A & N/A & N/A & 44,365 & 50,287 & 98,382 & N/A & N/A &N/A \\
DYNMOGA & N/A & N/A & N/A & N/A & N/A & 24,582 & 33,442 & 62,579 & N/A & N/A &N/A  \\
NE2NMF & N/A & N/A & N/A & N/A & N/A & 39,482 & 44,377 & 79,255 & N/A & N/A  &N/A\\
RTSC & N/A & N/A & N/A & N/A & N/A & 42,242 & 43,345 & 81,334 & N/A & N/A &N/A \\
jLMDC & N/A & N/A & N/A & N/A & N/A & 18,541 & 25,233 & 38,433 & N/A &  N/A &N/A \\
RDMA & N/A & N/A & N/A & N/A & N/A & 33,482 & 48,257 & 66,598 & N/A & N/A &N/A\\
% Eq.(\ref{seclustering})                 & 6,842          & 6,638          & 6,820          & 6,529  &6,707  & 3,131 & 3,656  & 5,209 &41,325 & 59,236  &22,511     \\
% Eq.(\ref{SMFT-II}) & 7,521         &  7,852         & 7,635    & 7,923  & 7,732  & 3,922 & 4,355  & 6,032 & 42,332 & 60,215  &23,371 \\
  DyG-MF                                  & \textbf{3,434} & \textbf{3,523} & \textbf{3,425} & \textbf{3,693} &
\textbf{3,518} & \textbf{1,829} & 
\textbf{2,304} & 
\textbf{2,717} & 
\textbf{32,460} & 
\textbf{40,752} &
\textbf{16,012}
\\ 
\,\,\,{w/o} TSMF
& N/A & N/A & N/A & N/A & N/A & 31,363 & 33,595 & 55,282 & N/A &N/A &N/A
\\
\bottomrule
\end{tabular}
\label{table:performance_running_time}
}
\end{table*}



\subsection{Scalability Evaluation} 
%\vspace{3pt}
%\noindent \textbf{\textit{Scalability Evaluation.}}
Table~{\ref{table:performance_running_time}} shows the detailed running time of DyG-MF and baselines on large-scale dynamic graphs. 
%
Compared to the fastest baseline, SepNE, 
DyG-MF reduces the running time by 44.61\% across all dynamic graphs, with a 52.27\% reduction on synthetic dynamic graphs and 37.00\% on real-world ones. 
%

%
To further investigate the scalability of DyG-MF, we
conduct additional experiments on the Birth-Death dataset with varying numbers of snapshots and nodes, as shown in Figure~{\ref{scalability}}.  
Specifically, DyG-MF's running time increases linearly as the number of snapshots and nodes grows, while the baselines show nearly exponential growth. This demonstrates DyG-MF's strong scalability for larger-scale real-world dynamic graphs, which can be attributed to its separated matrix factorization and selective embedding updating. 
Specifically, the temporal separated matrix factorization strategy breaks down the large-scale matrix factorization problem into smaller, more manageable subproblems without compromising clustering accuracy. %(jLMDC vs. DyG-MF in Tables~{\ref{main_table:performance_on_artificial}} and {\ref{table:performance_running_time})}. 
Moreover, compared to other matrix factorization-baselines like DYNMOGA, which update the embeddings of all nodes at each timestamp, DyG-MF updates only a small fraction of dynamically evolving nodes (16\%), showing its potential applicability in real-world scenarios.  Figure~{\ref{scalability}(C-D)} also confirms that higher efficiency and scalability do not reduce the NMI scores. 
%


\begin{figure}[h]
\centering
\includegraphics[scale=0.35]{Figure/noisy-1.pdf}
\caption{Noise Attacks. NMI w.r.t. percentage of noisy edges.}
\label{fig:noisy}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[scale=0.4]{Figure/parameters.pdf}
\caption{(A)-(C) and show the number of clusters, and hyperparameter tuning of $r$, $\beta$ and $s$ on the first snapshot of Birth-Death-30K. (D)-(E) show the percentage of dynamic nodes and landmarks of four synthetic datasets with 30K nodes.}
\label{Hyper_parameter_tuning}
\end{figure*}

\subsection{Robustness Evaluation} 

Real-world dynamic graphs often contain much noise and exhibit irregular evolution patterns. Table~{\ref{main_table:performance_on_artificial}} and Figure~{\ref{main_fig:SMFT2_accuracy}} show that DyG-MF outperforms all baselines on real-world dynamic graphs, demonstrating its ability to filter out noise and capture more complex evolution patterns. To further support our statement, as shown in Figure~{\ref{fig:noisy}}, we contaminate dynamic graphs by adding 5\%$\sim$30\% noisy edges in each snapshot, following Tan et al.~\cite{DBLP:conf/cikm/TanYL22}. 
Compared to the best-performing baselines, DyG-MF shows a less performance degradation, indicating its robustness against temporal noisy edges.  We also observe that w/o bi-clustering regularization significantly decreases the NMI score, showing that bi-clustering regularization serves as the main component of DyG-MF in maintaining robustness against noise attacks in dynamic graphs.



\begin{table}[h!]
\footnotesize
\centering
\caption{Ablation study on landmark selection strategies. Dynamic means landmarks are updated at each timestamp.}
%\vspace{-3pt}
\setlength{\tabcolsep}{1.3mm}{ %Change weight of table
\label{table:landmark-selection}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{\textit{Strategies}}
& \multicolumn{2}{c}{Bir-Dea-30K} &\multicolumn{2}{c}{Hide-30k} &\multicolumn{2}{c}{Wikipedia}
\\
% \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7} 
%&NMI &NF1 &NMI &NF1 &NMI &NF1\\
\midrule
\midrule
%\midrule
DyG-MF\,+\,\, &NMI &NF1 &NMI &NF1 &NMI &NF1\\
\midrule
\quad Fixed Random Selection &90.2 & 83.2 & 89.2 & 80.1 & 42.2 & 19.5\\
\quad Fixed Greedy Selection &92.6 & 85.5 & 92.1 & 82.3 & 44.8 & 21.3\\
\quad Fixed $K$-means Selection &94.2 &86.9 &94.5 &84.5 & 46.5 & 23.1 \\
\midrule
\quad Dynamic Random Selection &88.2 & 81.5 & 90.3 & 82.3 & 41.8 &17.9\\
\quad Dynamic Greedy Selection &93.5 & 86.2 & 94.1 & 83.6 & 45.7 & 22.7\\
\quad Dynamic $K$-means Selection (Eq. {(\ref{landmark_})})&96.3 &87.5  &96.2  &86.1  &47.5  &23.8 \\
\midrule
  \quad Our Selection (Eq. {(\ref{landmark})})&\textbf{99.9} &\textbf{90.2} &\textbf{98.9} &\textbf{89.9} &\textbf{50.4} &\textbf{25.8}\\
\bottomrule
\end{tabular}}
\end{table}


\subsection{Ablation Study}
\label{sec:ablation_study}

We conduct an ablation study to evaluate the necessity of each component of DyG-MF. 
We consider the following three variants of DyG-MF: \textbf{(i) without Temporal Separated Matrix Factorization (w/o TSMF):} remove separated matrix factorization introduced in Sec {\ref{main_TSMF}} and replace it with Eq.({\ref{equ4}}); \textbf{(ii) without Bi-clustering Regularization (w/o BR):} remove bi-clustering regularization introduced in Sec {\ref{bi-clustering-module}};  \textbf{(iii) without Selective Embedding Updating (w/o SEU):} remove fine-grained temporal smoothing strategy introduced in Sec {\ref{topological_dynamics}}. 
As shown in Table~{\ref{main_table:performance_on_artificial}}, removing any of these components negatively impacts overall performance on dynamic graph clustering, demonstrating their effectiveness and necessity.








To understand the role of temporal landmarks, as shown in Table~{\ref{table:landmark-selection}}, we selecte random sampling, greedy search~\cite{li2019sepne} and $K$-means as potential strategies. We have three observations. 
\textbf{(i) Temporal landmarks selection is crucial}, as it can significantly affect performance on dynamic graph clustering. If landmarks cannot cover the entire feature space, there can be severe information loss for certain samples, leading to clustering errors. 
\textbf{(ii) Greedy sampling may not be as effective} as the $K$-means method, while it is faster. %We recommend using Facebook's open-source Faiss to accelerate $K$-means. 
\textbf{(iii) Fixed setting underperforms the dynamic one} because the core landmarks will change over time. Thus, dynamically updating landmark selection can further improve performance.











% \subsection{Effectiveness Analysis}
% NMI and NF1 require ground-truth labels to evaluate detected clusters. 
% To avoid incorrect labels affecting evaluation, we used two extra metrics, Modularity~\cite{Newman06} and Density~\cite{chen2013measuring}, to evaluate the quality of detected dynamic communities, \textit{i.e.,} Modularity measures the quality of inter-connections between nodes within a community and Density measures outer-connections among communities without relying on labels.
% As shown in Figure~{\ref{scalability2}}, DyG-MF outperforms three neural network-based best-performing baselines on large-scale real-world dynamic graphs, indicating the effectiveness of DyG-MF in identifying high-quality communities.



\subsection{Hyperparameters Analysis}

We use the first snapshot of four synthetic event datasets as validation data to tune the hyperparameters $\{s, r, \beta, \mu, |U_{t}|\}$, where $s$ represents the number of separated subsets, $r$ is the dimension of the node embeddings, $\beta$ is the balanced parameter in Eq.{(\ref{equ15})}, $\mu$ indicates the number of dynamic nodes, and $|U_{t}|$ refers to the number of landmarks.
Following previous studies~\cite{DBLP:conf/aaai/YaoJ21,DBLP:conf/kdd/YouDL22}, we adopt a grid search method to tune each hyperparameter while keeping the other parameters fixed. In Figure~{\ref{Hyper_parameter_tuning}(A)}, the number of clusters $\varrho$ is automatically determined by the elbow method. 
Figure~{\ref{Hyper_parameter_tuning}(B)-(C)} shows that with $r=1,000$, $\beta=20$ and $s=50$, DyG-MF achieves the highest NMI scores on the validation dataset. We do not display these parameters for the other three synthetic datasets,  as they follow a similar trend. 
Figure~{\ref{Hyper_parameter_tuning}(D)-(E)} shows that when $\mu \in [16,20]$ and $|U_{t}| \in [0.48,0.52]$, DyG-MF achieves the best performance. Thus, we set $\mu = 16$ and $|U_{t}| = 0.5$ for the rest experiments. Note that for small-scale datasets like SYN-FIX/SYN-VAR, we set $s=1$.

\begin{figure}[h]
\centering
\includegraphics[scale=0.36]{Figure/case-study.pdf}
\caption{t-SNE of the 2nd snapshot of Wikipedia: (A) initialized and (B) DyG-MF learned node embeddings.
Communities of 3rd (C1) and 4th (C2) snapshots in SYN-VAR.}
\label{fig:visualization}
% \vspace{-0.3cm}
\end{figure}



\subsection{Case Study}
To clearly demonstrate the effectiveness of DyG-MF, we present a visualization of the detected clusters using the t-SNE plot of the second snapshot from Wikipedia. 
As shown in Figure{~\ref{fig:visualization}(A)}, the initialized node embeddings are randomly distributed in the two-dimensional space, without any discernible community structures. 
After optimizing by DyG-MF, we learn representative and suitable node embeddings that can automatically cluster nodes into distinct clusters, as illustrated in Figure~{\ref{fig:visualization}(B)}.

To further illustrate the effectiveness, interpretability, and robustness of DyG-MF, we provide a case study using a Sankey plot to show community structures in the 3rd and 4th snapshots of the SYN-VAR, as shown in Figure~{\ref{fig:visualization}(C1)-(C2)}. 
DyG-MF can effectively track the evolution patterns of individual node, \textit{i.e.,} nodes from the second cluster in the 3rd snapshot are split into the second and third clusters in the 4th snapshot, highlighting DyG-MF's effectiveness and interpretability. 
Benefiting from the bi-clustering regularization, we can easily obtain the community evolution of nodes, where each diagonal block represents a community and the middle parts illustrate the transitions and changes between communities.

%\input{Main Paper/6-Conclusion}
\section{Conclusion}
\label{conclusion}


In this study, we proposed a novel scalable and robust temporal separated matrix factorization method to reveal the evolution mechanism of complex real-world complex systems. 
By jointly estimating graph embedding and clustering
with Bi-clustering regularization and selective embedding updating, 
our method can achieve SOTA performance on synthetic and real-world dynamic graphs, illustrating its scalability, robustness, and effectiveness. 
%
In the future, we will design more separated matrix factorization strategies to preserve more global information, use incremental clustering to reduce time complexity during landmark selection, introduce diffusion models to enhance robustness, 
and extend our method to continuous-time dynamic graphs to enhance its flexibility. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Citation Format.
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{Ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If your work has an appendix, this is the place to put it.
\appendix

%\twoline
\section{Limitations}

The first limitation of this study is that DyG-MF only addresses non-overlapping clustering, while its performance on overlapping clustering remains underexplored.  The second limitation is that DyG-MF has only been evaluated on large-scale real-world datasets containing up to 3,200,000 nodes, leaving its performance on even larger datasets still unexamined. Finally, with the advancement of natural language processing, many graph foundation models have been proposed. Exploring how to integrate these graph foundation models to obtain well-initialized node embeddings for improved performance is a promising area for future research.


\section{Pseudocode of DyG-MF}

We give a Pseudocode of DyG-MF in Algorithm~{\ref{algorithm:smft_2}}. 

% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[h]
\small
	\caption{Pseudocode of our method.}
    \label{algorithm:smft_2}
	\LinesNumbered
	
	\KwIn{
		$\mathcal{G}_{\{1,\cdots, \tau\}}$: Dynamic Graphs; \,\, $s, r, \beta , \mu, \lambda$: Hyperparameters.
	}
	\KwOut {$\{V_{l}\}_{l=1}^{k_{t}} (t\in \{1 \dots,\tau\})$: Dynamic Communities.}

	\For{ $t\in \{1,\ldots,\tau\}$}
	{

	\textbf{{Part I}: Dynamic Graphs Separation and Processing.}
  
{Randomly partition $\mathcal{V}_{t}$ into $s$ subsets}; 

{Temporal landmarks selection of $U_{t}$; by Eq.{(\ref{equ6})};}

{Partition nodes into static set $Y_{t}$ and dynamic set $X_{t}$ by Eq.{(\ref{equ13})};}


		\textbf{{Part II}: Landmarks Matrix Factorization of Eq.{(\ref{equ14})}}
  
		\Repeat{converge}{
	
			Update $\Phi_{x,t}$;% by using Eq.{(\ref{C-12})};
			
			Update $\Psi_{x,t}$;% by using Eq.{(\ref{C-14})};
			
		}
		
		\textbf{{Part III}: Separated Matrix Factorization of Eq.{(\ref{equ15})}}

        \For{ $i \in \{1,\ldots,s \}$}
        {
    		\Repeat{converge}{
    			Fix other variables, update $F_{x,t}^{i}$;% by using Eq.{(\ref{C-27})};
    			
    			Fix other variables, update $Q_{x,t}^{i}$;% by using Eq.{(\ref{C-29})};
    			
    			Fix other variables, update $P_{x,t}^{i}$;% by using Eq.{(\ref{C-33})};
    		}

            {Calculate $C^{i}_{t}=[P_{x,t}^{i}\Phi_{t}; P_{y,t}^{i}\Phi_{t}]$}
      
        }

	{Recognizing clusters from $C_{t}^{i}$ for $\forall i$ satisfying $1\leq i \leq s$;}
 
	}
\end{algorithm}





% \setcounter{equation}{0}
% \renewcommand\theequation{B.\arabic{equation}}
% \section{Graph Representation Learning}
% \label{section:embedding_as_matrix_factorization}

% In natural language processing, a natural word can be represented by a dense vector, called word embedding. 
% Skip-gram with negative sampling is one of the most effective methods for constructing word embedding~\cite{GoldbergL14}. Given a word from a text, Skip-gram with negative sampling learns the embedding of this word by using its surrounding words as context information. Formally, for a given vocabulary with size $n$, let $C \in \mathbb{R}^{n \times r}$ and $H \in \mathbb{R}^{r \times n}$ with dimension $r$, be the embedding matrices of the word and the embedding matrix of the transposed context, respectively. We define that the $a$-th row $\bm{c}_{a.}$ of $C$ represents the semantic embedding of the word $w_{a}$, and the $b$-th column $\bm{h}_{.b}$ of $H$ represents the contextual embedding of the word $w_{b}$. For the word $w_{a}$ and its context $w_{b}$ as a pair, previous studies assumed that the dot product $\bm{c}_{a.} \cdot \bm{h}_{.b} $ reflected the association strength between $w_{a}$ and $w_{b}$.
% Their dot product equaled the point-wise mutual information $m_{ab}$=$\bm{c}_{a.} \cdot \bm{h}_{.b} $ when sampling parameter equals to 1~\cite{DBLP:conf/nips/LevyG14}:
% \begin{equation}\label{PMI_matrix}
% m_{ab} = \log{(\frac{\#(w_{a}, w_{b}) |\mathcal{N}| }{\#(w_{a}) \#(w_{b})})},  
% \end{equation}
% where $\mathcal{N}$ denotes the collection of word and context pairs, $|\mathcal{N}|$ is the number of pairs in $\mathcal{N}$, $\#(w_{a}, w_{b})$ denotes the number of times the pair $(w_{a}, w_{b})$ appeared in $\mathcal{N}$. 
% Thus, the objective function of the Skip-gram with negative sampling for optimizing $C$ and $H$ can be rewritten to a matrix factorization problem as $M \approx CH$, where $m_{ab}$ denotes the element in the $a$-th row and $b$-th column of $M$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{More Details about Datasets}
\label{Appendix of Datasets}


%\subsection{Dataset Description}
%\label{section:experimental_settings}

We conducted experiments on 11 widely used datasets, including six synthetic and five real-world datasets, as shown in Table~{\ref{table1}}.

%\vspace{3pt}
\noindent \textbf{SYN Datasets.} 
SYN-FIX and SYN-VAR were constructed with different dynamic settings for vertices and communities~\cite{KimH09}.
SYN-FIX fixes the number of communities at four and generates snapshots by randomly moving three vertices from each original community to new communities from the second to the final timestamp. 
In contrast, SYN-VAR consists of 256 vertices belonging to four equal-sized communities, randomly moving eight vertices from each of the four communities to form a new community with 32 vertices from the second to the fifth timestamp. 
The generated snapshots are then copied and reversed to create the final five snapshots.

%\vspace{3pt}
\noindent \textbf{Green Datasets.} 
Considering the network sizes and the limited dynamic evolution of SYN-FIX/VAR, we generated four event-based temporal networks starting from the second timestamp~\cite{Folina2013}:
\begin{itemize}
\item \textbf{\textit{Birth-Death}}: 5\% existing communities are removed/generated by randomly selecting vertices from other communities; 
%
\item \textbf{\textit{Expansion}}: 10\% of communities are expanded or contracted by 50\% of their original size;
%
\item \textbf{\textit{Hide}}: 10\% of the communities are randomly hidden; 
% 
\item \textbf{\textit{Merge-Split}}: 20\% of communities are split or merged.
\end{itemize}
We repeated the above process ($\tau$-1) times to construct the corresponding temporal networks, setting the number of timestamps to 10, the number of vertices to 30K/100K, the average degree of each snapshot at 100, the maximum degree at 200, the number of communities in the range [40, 60], and the mixing parameter to 0.2. As a result, we obtained Green datasets with 30K and 100K vertices, while we evaluated the evolutionary methods only on the 30K temporal networks.


%\vspace{3pt}
\noindent \textbf{Real-world Datasets.} Following previous studies~\cite{DBLP:journals/kbs/LiZDGM21,DBLP:conf/cikm/YouHKFO21}, we conducted experiments on five widely used real-world temporal networks covering multiple applications.  
(1) \textbf{Academic graphs}: The arXiv dataset~\cite{DBLP:conf/kdd/LeskovecKF05} is a collaboration graph that describes the authors of scientific papers, covering papers from January 1993 to April 2003 (124 months) and consisting of 28,100 papers with 4,600,000 edges. 
(2) \textbf{Social networks}:  The Dublin dataset~\cite{ISELLA2011166} contains dynamic person-contact networks with 20-second intervals collected during the Infectious SocioPatterns event at the Science Gallery in Dublin. The Flickr dataset~\cite{mislove-2008-flickr} is a dynamic social network with data collected over three months, featuring 950,143 new users and more than 9.7 million new links, focusing on how new links are formed. 
(3) \textbf{Website interaction networks}: The Wikipedia dataset~\cite{DBLP:conf/icwsm/LeskovecHK10} is a bipartite editing network that contains temporal edits by users of Wikipedia pages. The Youtube dataset~\cite{mislove-2008-flickr} includes a list of user-to-user links from the video-sharing website Youtube. To evaluate clustering accuracy, gold community labels are necessary for each vertex. We obtained gold community labels during the generation of synthetic temporal networks. 

For real-world temporal networks, following previous studies~\cite{Folina2013} by aggregating all edges across all timestamps into a single graph and applying DYNMOGA to compute a soft modularity score $\text{Q}$~\cite{PhysRevE.69.026113}, where the highest $\text{Q}$ was considered the gold label for all vertices.



\section{More Details about Baselines}\label{Appendix-baselines}


We compare our method with 14 best-performing baselines, which can be classified mainly into the following three classes:

\noindent \textbf{\textit{Coupling Baselines}:}
\begin{itemize}
\item \underline{CSEA}~\cite{DBLP:journals/eswa/FeiWHLL23} first uses the Variational Autoencoder to reduce the dimension of the adjacency matrix and extracts the core structure of the coupling network. 
Then, $K$-means clustering is used to obtain information about the community structure.

%\vspace{3pt}
\item \underline{DSCPCD}~\cite{10017356} detects community structures by maximizing the dual structural consistency of the coupling network, \textit{i.e.,} the original explicit graph and the potential implicit graph have a consistent community structure.
\end{itemize}



\noindent \textbf{\textit{Two-stage Baselines:}} 
\begin{itemize}
\item \underline{SepNE}~\cite{li2019sepne} ignores the temporal information and estimates the clustering accuracy of separated matrix factorization in a proximity matrix from the given dynamic graphs.
$K$-means is then used in the factorized matrix to obtain dynamic clusters.

%\vspace{3pt}
\item \underline{node2vec}~\cite{grover2016node2vec} uses a biased random walk procedure to explore neighborhoods in a breadth-first and depth-first sampling method so that neighborhood information can be maximally preserved. After obtaining graph embedding, $K$-means is used to capture dynamic clusters. 

%\vspace{3pt}
\item \underline{LINE}~\cite{tang2015line} is a breadth-first edge sampling method and considers both adjacent and deep interactions between vertices to learn graph embedding instead of using random walks. After obtaining graph embedding, $K$-means is used to capture dynamic clusters.

%\vspace{3pt}
\item \underline{RNNGCN}~\cite{DBLP:conf/aaai/YaoJ21} uses an RNN to learn the decay rates of each edge over timestamps to characterize the importance of historical information for current clustering. 
A two-layer graph convolutional network is used for dynamic graph clustering.

%\vspace{3pt}
\item \underline{ROLAND}~\cite{DBLP:conf/kdd/YouDL22} extends the GNN to dynamic scenes by viewing the node representation at different layers as hierarchical node states and using GRUs to update these hierarchical vertex states based on newly observed vertices and edges. 

%\vspace{3pt}
\item \underline{TGC}~\cite{DBLP:conf/iclr/001400T00024} propose a general framework for deep Temporal Graph Clustering, which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. They then discuss differences between temporal graph clustering and static graph clustering from several levels. 
\end{itemize}


\noindent \textbf{\textit{Evolutionary Baselines}:} 
\begin{itemize}
\item \underline{PisCES}~\cite{LiuF1801} globally estimates and optimizes clustering drift on all snapshots.
It uses NMF, which is equal to spectral clustering, for dynamic graph clustering.

%\vspace{3pt}
\item \underline{DYNMOGA}~\cite{Folina2013} estimates the clustering drift by minimizing the NMI between the community structures detected between two successive snapshots. 
And it maximizes clustering precision by directly decomposing the adjacency matrix and uses $K$-means for dynamic graph clustering.

%\vspace{3pt}
\item \underline{NE2NMF}~\cite{DBLP:journals/kbs/LiZDGM21} uses previous and current snapshots to characterize cluster drift and locally optimizes drift at each timestamp. 
After decomposing the adjacency matrix by NMF to obtain a vertex representation, it continues to detect communities by decomposing the vertex representation matrix.

%\vspace{3pt}
\item \underline{RTSC}~\cite{DBLP:conf/cikm/YouHKFO21} 
uses the previous, current, and subsequent graphs to measure clustering drift. It applies NMF to the common feature matrix of three successive graphs to estimate clustering precision. Finally, RTSC uses $K$-means as post-processing for dynamic graph clustering. 

%\vspace{3pt}
\item \underline{jLMDC}~\cite{9531337} propose a novel joint learning model for dynamic community detection through joint feature extraction and clustering. This model is formulated as a constrained optimization problem. Vertices are classified into dynamic and static groups by exploring the topological structure of temporal networks to fully exploit their dynamics at each time step. Then, jLMDC updates the features of dynamic vertices by preserving features of static ones during optimization. The advantage of jLMDC is that the features are extracted under the guidance of clustering, promoting performance, and saving running time.

%\vspace{3pt}
\item \underline{RDMA}~\cite{DBLP:journals/www/RanjkeshMH24} propose the robust memetic method and use the idea to optimize the detection of dynamic communities in complex networks. They work with dynamic data that affect the two main parts of the initial population value and the calculation of the evaluation function of each population, and no need to determine the community number in advance. 
\end{itemize}



% \section{Introduction of Criteria and Significant Tests}
% \label{criteria}

% Following previous studies~\cite{LiuF1801,Ma17b}, we used normalized mutual information (NMI)~\cite{Danon_2005} and normalized F1 score (NF1)~\cite{rossetti2017graph} to measure the precision of clustering.
% %
% Let $k^{*}$ and $k$ be the gold and the predicted number of communities. 
% We denote a confusion matrix $N \in \mathbb{R}^{k^{*} \times k}$, where the rows correspond to the gold communities $\{\mathcal{V}_{l}^{*}\}_{l=1}^{k^{*}}$, columns correspond to the predicted communities $\{\mathcal{V}_{l}\}_{l=1}^{k}$,
% and $n_{ij}$ is the number of vertices overlapped between the 
% $i$-th real and $j$-th predicted communities. 
% NMI can be formulated
% \begin{equation}
%     \textrm{NMI}
%     %(\{\mathcal{V}_{l}\}_{l=1}^{k}, \{\mathcal{V}_{l}^{*}\}_{l=1}^{k^{*}})
%     =  \frac{-2 \sum_{i=1}^{k^{*}}\sum_{j=1}^{k} n_{ij} \textrm{log}(\frac{n_{ij} |N|}{|\textbf{n}_{i.}| |\textbf{n}_{.j}|})}{\sum_{i=1}^{k^{*}} |\textbf{n}_{i.}| \textrm{log}(\frac{|\textbf{n}_{i.}|}{|N|})+\sum_{j=1}^{k} |\textbf{n}_{.j}| \textrm{log}(\frac{|\textbf{n}_{.j}|}{|N|})}, \nonumber
% \end{equation}
% where the sum of all elements of $N$ is denoted as $|N|$, sum over the $i$-row $\textbf{n}_{i.}$ and $j$-th column $\textbf{n}_{.j}$ are denoted as $|\textbf{n}_{i.}|$ and  $|\textbf{n}_{.j}|$.

% Before introducing NF1, we first define that Precision is the percentage of vertices in $\mathcal{V}_{i}$ labelled as $\mathcal{V}_{j}^{*}$:
% \begin{equation}
%     \textrm{Precision}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*}) = \frac{|\mathcal{V}_{i} \cap \mathcal{V}_{j}^{*}|}{|\mathcal{V}_{i}|} \in [0,1],
% \end{equation}
% and Recall is the percentage of vertices in $\mathcal{V}^{*}_{j}$ covered by $\mathcal{V}_{i}$:
% \begin{equation}
%         \textrm{Recall}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*}) = \frac{|\mathcal{V}_{i} \cap \mathcal{V}_{j}^{*}|}{|\mathcal{V}_{j}^{*}|} \in [0,1].
% \end{equation}

% Following Rossetti et al.~\cite{RossettiPR16}, 
% we combine precision and recall into their harmonic mean to define the F1 score as
% \begin{equation}
% \textrm{F1}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*})
%     = 2 \frac{\textrm{Recall}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*}) * \textrm{Precision}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*})}{\textrm{Recall}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*}) + \textrm{Precision}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*})} \in [0,1], \nonumber
% \end{equation}
% where we then averaged $\textrm{F1}(\mathcal{V}_{i}, \mathcal{V}_{j}^{*})$ across all the identified pairs between the $\{\mathcal{V}_{l}^{*}\}_{l=1}^{k^{*}}$ and $\{\mathcal{V}_{l}\}_{l=1}^{k}$.

% Following Rossetti~\cite{rossetti2017graph}, NF1 can be defined as
% \begin{equation}
%     \textrm{NF1} = \frac{\textrm{F1} * \textrm{Coverage}}{\textrm{Redundancy}} \in (0,1], 
% \end{equation}
% where Coverage identified the percentage of communities in $\mathcal{V}^{*}$ that were matched by at least one object in $\mathcal{V}$:
% \begin{equation}
%     \textrm{Coverage} = \frac{|\mathcal{V}_{id}^{*}|}{|\mathcal{V}^{*}|} \in [0,1],
% \end{equation}
% where $\mathcal{V}_{id}^{*}$ was the subset of communities in $\mathcal{V}^{*}$ matched by $\mathcal{V}$.
% \textrm{Redundancy} is the percentage of unmatched real communities:
% \begin{equation}
%     \textrm{Redundancy} = \frac{|\mathcal{V}|}{|\mathcal{V}_{id}^{*}|} \in [1,+\infty).
% \end{equation}


% \noindent \textbf{Benjamini-Hochberg Correction} (\textit{B-H}) is a powerful tool that decreases the false discovery rate~\cite{benjam}.
% Considering the reproducibility of the multiple significant test, we introduce how we adopt the \textit{B-H} correction and give the hyperparameter values that we used.

% We first performed a t-test with the default parameters\footnote{\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html}{https://docs.scipy.org/scipy.stats.ttest\_ind.html}} to calculate the p-value between each of the comparison methods with our method.
% We then put the individual p-values in ascending order as input to calculate the corrected p-value using the \textit{B-H} correction. 
% We directly use the ``\textit{\textbf{multipletests(*args)}}'' function from Python package\footnote{\href{https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html}{https://www.statsmodels.org/statsmodels.stats.multitest.multipletests.html}} and set the hyperparameter of the false discovery rate $Q = 0.05$, which is a widely used default value. 
% Finally, we obtain a cutoff value as the output of the \textbf{\textit{multipletests}} function, where cut-off is a dividing line that distinguishes whether two groups of data are significant.
% If the p-value is smaller than the cut-off value, we can conclude that two groups of data are significantly different.

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%