\section{Related Work}
\label{sec:relatedWork}


%%%%%%%%%%%%%%%%  Naive Bayes
\subsection{Naive Bayes}\label{subsec:NB}

Naive Bayes (NB) ____ is a probabilistic classifier based on Bayes' theorem with the assumption of conditional independence between every pair of features given the class variable. This assumption allows factorizing the parameters

\begin{equation*}
\probability(y, \x) = \probability(y,x_1,\dots,x_n) = \probability(y) \prod_{j = 1}^{n} \probability(x_j \mid y).
\end{equation*}

Despite this simplification, which is often violated in practice, NB frequently obtains competitive accuracy across various classification tasks. The independence assumption provides NB with several advantages, such as efficient parameter learning, the absence of structure learning, and the ability to function effectively with a relatively small amount of data, as it only estimates bi-variate statistics. These advantages contribute to the widespread use of NB.

Like most probabilistic classifiers, NB models follow the maximum a posteriori (MAP) principle, meaning they return the most probable class label \(\hat{y} \in \Y\) given the input instance \(\x \in \X\) as evidence

\begin{equation*}
    \hat{y} = \C(\x) = \argmax_{y \in \Y} \probability(y \mid \x) \propto \argmax_{y \in \Y} \probability(y, \x).
\end{equation*}

%%%%%%%%%%%%%%%%  Learning NB
\subsection{Generative and Discriminative Learning of Parameters}\label{subsec:genDiscNB}

The parameter learning of NB classifiers can be approached using either generative or discriminative methods.

\subsubsection{Generative Learning}

It is the usual way to estimate NB parameters. Generative learning methods ____ focus on modeling the joint probability distribution of the class and features, $\probability(y, \x)$. To do so, NB estimates the parameters that maximize the likelihood of the observed data $\D$. In particular, the log-likelihood (LL) function given the NB parameters $\parameters$ is

\begin{equation*}
LL(\parameters \mid \D) = \sum_{i = 1}^{m} \log \probability(y^{(i)}, \x^{(i)} \mid \parameters).
\end{equation*}

In NB, and, in general, Bayesian models, one of the classical methods to estimate parameters is maximum likelihood estimation (MLE). The probability of a class label $\probability(Y = y_k)$ and the conditional probabilities $\probability(X_j = x_l \mid Y = y_k)$ are computed as

\begin{equation*}
\probability(Y = y_k) = \frac{\#(y_k)}{m} \quad ; \quad  \probability(X_j = x_l \mid Y = y_k) = \frac{\#(x_l, y_k)}{\#(y_k)},
\end{equation*} %
%
where $\#(y_k)$ denotes the number of instances where the class variable \(Y\) has label $y_k$, and $\#(x_l, y_k)$ denotes the number of instances where feature \(X_j\) has value \(x_l\) and class has label $y_k$.

Generative methods are computationally efficient and straightforward to implement. However, they may not always provide the best classification performance.


\subsubsection{Discriminative Learning}

Discriminative learning methods ____ directly model the conditional probability distribution of the class given the features, $\probability(y \mid \x)$. This approach generally leads to better classification performance by addressing the problem of interest. The objective is to maximize the conditional log-likelihood (CLL) function
%
\begin{equation*}
CLL(\parameters \mid \D) = \sum_{i = 1}^{m} \log \probability(y^{(i)} \mid \x^{(i)}, \parameters),
\end{equation*}
%
where
%
\begin{equation*}
    \log \probability(y \mid \x) =  \left(\log\theta_y + \sum_{j = 1}^{n} \log \theta_{y, x_j}\right) - \log \sum_{k = 1}^{o} \left(\theta_{y_k} \prod_{i = 1}^n \theta_{y_k, x_j}\right).
\end{equation*}

Contrary to the LL function, numerical optimization techniques are used to estimate the parameters for the CLL, as there is no direct solution. A common technique is to map the parameters of NB to logistic regression models to obtain discriminatively trained parameters ____.

Although potentially more complex and computationally expensive, discriminative methods can yield better results as they directly optimize the classification criterion. If the model structure is correct, maximizing LL or CLL functions leads to the same results ____. In NB, which assumes independence among all variables, optimizing CLL offers significant improvement compared to the generative version.

\subsubsection{Hybrid approach} Weighted Naive Bayes (NB$^w$) ____ offers a hybrid approach to parameter learning for NB models. This method extends the NB framework by introducing a weight vector $\w \in \mathbb{R}^{|\parameters|}$, assigning an extra parameter \(w \in \w\) to each parameter \(\theta \in \parameters\). The model learning involves two phases:
%
\begin{enumerate*}[label=(\alph*)]
    \item estimating $\parameters$ in a generative manner by maximizing the LL function while setting $\mathbf{w} = \mathbf{1}$ and
    \item fixing $\mathbf{\Theta}$ and optimizing $\w$ discriminatively using the L-BFGS-M optimizer ____ (although another optimizer may be used).
\end{enumerate*}

The log probability in the NB$^w$ algorithm is calculated according to

\begin{equation*}
    \log \probability(y \mid \x) =  \left(w_y \log\theta_y + \sum_{j = 1}^{n} w_{y, x_j} \log \theta_{y, x_j}\right) - \log \sum_{k = 1}^{o} \left(\theta_{y_k}^{w_y} \prod_{i = 1}^n \theta_{y_k, x_j}^{w_y, x_j}\right).
\end{equation*}

It is worth noticing that this approach encompasses the traditional NB model as a particular case when only the first phase is applied.

The experimental evaluation in ____ demonstrates the superiority of this model over the generative version and various discriminative variants, as it reduces runtime while maintaining similar accuracy. Additionally, NB$^w$ competes with the generative version of semi-naive Bayes models and tree-based methods, particularly on small datasets, yielding lower runtime results ____.


%%%%%%%%%%%%%%%%  Decentralized Naive Bayes
\subsection{Previous Attempts on Federated Naive Bayes}\label{subsec:relatedWorkNB}

The privacy-preserving distributed learning of generative NB has been explored in the literature prior to the formalization of the FL scenario. Early efforts in this direction include ____, which utilize different cryptographic protocols to learn NB parameters for categorical and numerical variables on horizontally and vertically partitioned data\footnote{Horizontally partitioned data refers to data divided by instances (rows). In contrast, vertically partitioned data refers to data divided by features (columns).} while maintaining privacy in communications. Furthermore, these studies demonstrate that, although these processes typically result in a loss of accuracy and increased temporal complexity of the algorithms, they are feasible for practical application. More recent works have explicitly addressed FL for generative NB models. In ____, the authors use differential privacy to federate the learning of an NB model with horizontally partitioned data.

These proposals address privacy concerns in model training by focusing on protecting local updates and anonymizing data sources. However, they still pose a security risk if the server hosting the model is compromised or attacked, as it stores all the probability tables. In contrast, the parameters federated in the NB$^w$ model are inherently meaningless in revealing sensitive information. Therefore, this model provides a clear advantage over traditional methods in terms of privacy preservation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%              Federated Discriminative NB
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%