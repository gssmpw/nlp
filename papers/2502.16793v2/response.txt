\section{Related Work}
\label{S2}
In this section, we briefly summarize existing works on graph federation learning, graph contrastive learning, and graph adversarial attack.


\subsection{ Graph Federation Learning}\label{HypergraphLearning}

GFL trains models across multiple data owners without exchanging the original data and can effectively protect user privacy **Ni et al., "FedVGCN: A Federated Privacy-Preserving Node GCN Learning Framework"**. Ni et al. **"Federated Node Classification via Graph Neural Networks"** proposed a federated privacy-preserving node GCN learning framework (FedVGCN), which is suitable for the case where data is vertically distributed. FedVGCN splits the computational graph data into two parts. For each iteration of training, both sides pass intermediate results under homomorphic encryption. The literature **"A Survey on Federated Learning: A Topological View"** proposed vertical federated graph neural network (VFGNN), VFGNN keeps the private data (i.e., edges, node features, and labels) on the clients, and the rest of the information is given to be uploaded to a semi-honest server for training. Liu et al. **"Federated Learning of Subgraphs with Global Graph Reconstruction"** proposed a federated learning of subgraphs with global graph reconstruction (FedGGR).  For the data silo problem, Zhang et al. **"FedEgo: Federated Edge Graph Neural Networks"** proposed FedEgo, FedEgo uses GraphSAGE on ego-graphs to fully exploit the structural information and Mixup to address the privacy issues. Xue et al. **"Federated Learning of Personality Graphs via Variational Graph Self-Encoders"** proposed a new framework for federated learning of personality graphs based on variational graph self-encoders (FedVGAE). Du et al. **"Efficient Federated Graph Neural Networks with Hierarchical Features"** proposed a new efficient GFL framework (FedHGCN), FedHGCN is able to be co-trained in high-dimensional space to obtain graph-rich hierarchical features.  In addition, FedHGCN uses a node selection strategy to remove nodes with redundant information from the graph representation to improve efficiency.  Huang et al. **"Federated Cross-Domain Knowledge Graph Embedding Model"** proposed a federated learning cross-domain knowledge graph embedding model (FedCKE) in which entity/relationship embeddings between different domains can interact securely without data sharing. Zheng et al. **"Cross-Firm Recommendation GNNs Training Framework"** proposed a cross-firm recommendation GNNs training framework (FL-GMT), which no longer uses traditional federation learning training methods (e.g., averaged federation), and designs a loss-based federation aggregation algorithm to improve the sample quality. The literature **"Federated Multi-Task Graph Learning for Scalable Privacy Preservation"** proposed a federated multi-task graph learning (FMTGL) framework to address issues in privacy preserving and scalable schemes.

\subsection{Graph Contrastive Learning}\label{2.2}
Graph contrastive learning is an unsupervised learning method that aims to learn effective representations from graph data **Meng et al., "Informative Contrastive Learning via Graph Augmentation"**.
Meng et al. **"Informative Contrastive Learning for Graph Neural Networks"** proposed an informative contrastive learning (IMCL) which uses a graph augmentation generator for distinguishing the augmented view from the original view. Besides, IMCL uses a pseudo-label generator to generate pseudo-labels as a supervisory signal to ensure that the results of the augmented view classification are consistent with the original view.  Feng et al. **"Adversarial Augmentation and Regularization for Graph Contrastive Learning"** proposed the ArieL method, which introduces an adversarial graph view for data augmentation and also uses information regularization methods for stable training. In addition, ArieL uses subgraph sampling to extend to different graphs. Jiang et al. **"Probabilistic Graph Complementary Contrastive Learning"** proposed probabilistic graph complementary contrastive learning (PGCCL) for adaptive construction of complementary graphs, which employs a Beta mixture model to distinguish intraclass similarity and interclass similarity, and solves the problem of inconsistent similarity distributions of data. Yang et al. **"Graph Knowledge Contrastive Learning via Multilevel Graph Knowledge"** proposed a graph knowledge contrastive learning (GKCL), which uses exploits multilevel graph knowledge to create noise-free contrastive views that can alleviate the problem of introducing noise and generating samples that require additional storage space during graph augmentation. The literature **"Implicit Graph Contrastive Learning for Efficient Graph Representation"** proposed an implicit graph contrastive learning (IGCL), which avoids the situation where changing certain edges or nodes may accidentally change the graph features by reconfiguring the topology of the graph. Li et al. **"Line Graph Contrastive Learning via Transformation of Graph Topology"** proposed a line graph contrastive learning (Linegcl), the core of which is to transform the original graph into the corresponding line graph, solving the deficiencies of the existing methods in understanding the overall features and topology of the graph. Since the similarity-based methods are defective in terms of node information loss and similarity metric generalization ability.
The literature **"Linear Graph Contrastive Learning via Mutual Information Maximization"** proposed a linear graph contrastive learning (LGCL), which obtains subgraph views by sampling h-hop subgraphs of target node pairs, and then maximizes mutual information after transforming the sampled subgraphs into linear graphs. The literature **"Dyadic Contrastive Learning Network for Efficient Graph Representation"** proposed a dyadic contrastive learning network (DCLN), which is based on a self-supervised learning approach to enhance the model performance through the pairwise reduction of redundant information about the learned latent variables.

\subsection{Graph Adversarial Attack}\label{2.3}

GNNs have achieved significant success in many domains and are vulnerable to adversarial attacks due to their high dependence on graph structure and node features. Graph Adversarial Attacks are defined as small modifications to the input graph that can cause GNNs to output incorrect predictions or classification results **Pang et al., "Graph Adversarial Attack via Graph Activation Mapping"**.
The literature **"Generalized Adversarial Framework for Graph Neural Networks"** proposed a generalized attack framework (CAMA) that generates the importance of nodes through graph activation mapping and its variants.
Zhang et al. **"Distributed Adversarial Training on GNNs via Gradient Synchronization Disruption"** proposed the first framework for training adversarial attacks on distributed GNNs (Disttack), which centers on disrupting the gradient synchronization between computational nodes by injecting adversarial attacks into individual computational nodes. Ennadir et al. **"Unbound Attack: Generating Adversarial Perturbations via Graph Generation"** proposed a model for generating adversarial perturbations by generating entirely new nodes (UnboundAttack), which uses advances in graph generation to generate subgraphs. Zhao et al. **"Gradient-Based Adversarial Attack on Dynamic Graph Neural Networks"** proposed a new gradient-based attack method for the robustness of dynamic graph neural networks from an optimization point of view, which centers on using gradient dynamics to attack the structure of the graph. Zhu et al. **"Partial Graph Attack via Hierarchical Target Selection and Cost-Effective Node Selection"** proposed a partial graph attack (PGA) which uses a hierarchical target selection strategy that allows the attacker to focus only on vulnerable nodes. Then, the optimal perturbed edges are selected by a cost-effective node selection strategy. Aburidi et al. **"Structural Attack via Convex Relaxation and Projected Momentum Optimization"** proposed an attack based on training time optimization, which first optimizes the graph as hyperparameters and then uses convex relaxation and projected momentum optimization techniques to generate structural attacks. Since existing attackers need to access the target model without considering the budget allocation, the literature **"Targeted Labeling Attack for Graph Neural Networks"** proposed a targeted labeling attack (ETLA), which allocates the attack budget in terms of both the search space and the optimized target, allowing the attack to achieve the best performance. The literature **"Backdoor Attack on Link Prediction via Trigger Nodes and Poisoned Pairs"** proposed a backdoor attack for the link prediction task that uses individual nodes as triggers and selects poisoned pairs of nodes in a training graph, and then embeds the backdoor in the training process of their GNNs.