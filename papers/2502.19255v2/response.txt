\section{Related Work}
%

We only summarize the most relevant results on sample complexity and transfer RL here due to space limitation. Other closely related topics can be found in Appx.~\ref{appx:related_workds}.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.55]{Pictures/Outline.pdf}
    \caption{The standard online RLHF pipeline only involves learning from online human feedback (left).
    %
    Our setting \emph{additionally} leverages available imperfect reward models via transfer learning (right).
    Inspired by the structure induced by KL regularization, we propose novel principles for transfer learning in online RLHF: (1) selecting transfer policy $\pi_\text{Transfer}$ with the highest policy value; (2) self-transfer learning---involving as a candidate the policy $\pi_\SELF$ \emph{distilled} from online collected data by offline learning techniques.
    }\label{fig:outline}
\end{figure}
\textbf{Sample Complexity in RLHF}~
Online RLHF emphasizes strategic exploration for sample-efficient learning in tabular and linear settings **Mnih et al., "Human-level control through deep reinforcement learning"** __**, as well as more general function approximation cases **Sutton et al., "Temporal Difference Methods for General Control Problems"** ____.
%
Our work further improves sample efficiency by leveraging imperfect reward models that are readily available in a variety of practical scenarios. 
%
%
%
As an alternative, offline RLHF **Lange et al., "Batch construction for on-policy deep reinforcement learning"** ____ focuses on exploiting pre-collected datasets without exploration.
%
%
What lies in between online/offline RL is hybrid RL **Duan et al., "Benchmarking Deep Reinforcement Learning for Continuous Control"** ____.
These methods harness online feedback, while assuming the reference policy provides good coverage and only engaging in passive exploration.


\textbf{Transfer Learning in RL and RLHF}~
Transfer learning in pure-reward maximization RL has been extensively investigated in previous literature **Finn et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** __**, and theoretical guarantees have been established under various conditions **Raghu et al., "Transferring Knowledge across Neural Nets"** ____ . Unlike previous works, this paper unveils new insights for transfer learning enabled by the KL regularization in RLHF. In particular, it enables us to design a policy-value-based transfer policy selection strategy, and identify a unique regime, i.e., ``self-transfer learning'', that can significantly improve sample efficiency.
We defer more discussions to Sec.~\ref{sec:new_insights}.
%
%
%

Most works on transfer learning in RLHF focus on empirical approaches.
For example, **Chu et al., "Multitask Reinforcement Learning"** ____ investigate the cross-lingual reward transfer.
To our knowledge, our empirical algorithm (Alg.~\ref{alg:empirical}) is novel in that it studies active transfer policy selection, which is still underexplored in existing literature.
%
We further distinguish our work from RLAIF**Dulac-Arnold et al., "Deep Reinforcement Learning in a Handful of Trials"** ____ or reward model selection literature **Gu et al., "Value Prediction Network for Temporal Difference Methods"** ____ . Their goal is to align LLM policies with surrogate reward models, while we study how to leverage those surrogates to accelerate the alignment with ground-truth human rewards.