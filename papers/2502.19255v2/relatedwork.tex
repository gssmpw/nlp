\section{Related Work}
%

We only summarize the most relevant results on sample complexity and transfer RL here due to space limitation. Other closely related topics can be found in Appx.~\ref{appx:related_workds}.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.55]{Pictures/Outline.pdf}
    \caption{The standard online RLHF pipeline only involves learning from online human feedback (left).
    %
    Our setting \emph{additionally} leverages available imperfect reward models via transfer learning (right).
    Inspired by the structure induced by KL regularization, we propose novel principles for transfer learning in online RLHF: (1) selecting transfer policy $\pi_\text{Transfer}$ with the highest policy value; (2) self-transfer learning---involving as a candidate the policy $\pi_\SELF$ \emph{distilled} from online collected data by offline learning techniques.
    }\label{fig:outline}
\end{figure}
\textbf{Sample Complexity in RLHF}~
Online RLHF emphasizes strategic exploration for sample-efficient learning in tabular and linear settings \citep{xu2020preference, novoseller2020dueling, pacchiano2021dueling, du2024exploration}, as well as more general function approximation cases \citep{ye2024theoretical, chen2022human, wang2023rlhf,xie2024exploratory,cen2024value,zhang2024self,xiong2024iterative}.
%
Our work further improves sample efficiency by leveraging imperfect reward models that are readily available in a variety of practical scenarios. 
%
%
%
As an alternative, offline RLHF \citep{zhan2023provable, liu2024provably, huang2024correcting} focuses on exploiting pre-collected datasets without exploration.
%
%
What lies in between online/offline RL is hybrid RL \citep{chang2024dataset, gao2024rebel}.
These methods harness online feedback, while assuming the reference policy provides good coverage and only engaging in passive exploration.


\textbf{Transfer Learning in RL and RLHF}~
Transfer learning in pure-reward maximization RL has been extensively investigated in previous literature \citep{taylor2009transfer, zhu2023transfer}, and theoretical guarantees have been established under various conditions \citep{mann2013directed, huang2022tiered,huang2023robust, golowich2022can}. Unlike previous works, this paper unveils new insights for transfer learning enabled by the KL regularization in RLHF. In particular, it enables us to design a policy-value-based transfer policy selection strategy, and identify a unique regime, i.e., ``self-transfer learning'', that can significantly improve sample efficiency.
We defer more discussions to Sec.~\ref{sec:new_insights}.
%
%
%

Most works on transfer learning in RLHF focus on empirical approaches.
For example, \citep{wu2024reuse, hong2024cross} investigate the cross-lingual reward transfer.
To our knowledge, our empirical algorithm (Alg.~\ref{alg:empirical}) is novel in that it studies active transfer policy selection, which is still underexplored in existing literature.
%
We further distinguish our work from RLAIF~\citep{lee2023rlaif,ji2023ai} or reward model selection literature \citep{nguyen2024laser}. Their goal is to align LLM policies with surrogate reward models, while we study how to leverage those surrogates to accelerate the alignment with ground-truth human rewards.