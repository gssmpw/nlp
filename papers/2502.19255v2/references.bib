@article{wu2024reuse,
  title={Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment},
  author={Wu, Zhaofeng and Balashankar, Ananth and Kim, Yoon and Eisenstein, Jacob and Beirami, Ahmad},
  journal={arXiv preprint arXiv:2404.12318},
  year={2024}
}

@article{zhao2025nearly,
  title={Nearly Optimal Sample Complexity of Offline KL-Regularized Contextual Bandits under Single-Policy Concentrability},
  author={Zhao, Qingyue and Ji, Kaixuan and Zhao, Heyang and Zhang, Tong and Gu, Quanquan},
  journal={arXiv preprint arXiv:2502.06051},
  year={2025}
}

@inproceedings{azar2024general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@article{foster2025good,
  title={Is a Good Foundation Necessary for Efficient Reinforcement Learning? The Computational Role of the Base Model in Exploration},
  author={Foster, Dylan J and Mhammedi, Zakaria and Rohatgi, Dhruv},
  journal={arXiv preprint arXiv:2503.07453},
  year={2025}
}

@article{zhao2025logarithmic,
  title={Logarithmic Regret for Online KL-Regularized Reinforcement Learning},
  author={Zhao, Heyang and Ye, Chenlu and Xiong, Wei and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2502.07460},
  year={2025}
}

@article{huang2022tiered,
  title={Tiered reinforcement learning: Pessimism in the face of uncertainty and constant regret},
  author={Huang, Jiawei and Zhao, Li and Qin, Tao and Chen, Wei and Jiang, Nan and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={679--690},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{volske-etal-2017-tl,
    title = "{TL};{DR}: Mining {R}eddit to Learn Automatic Summarization",
    author = {V{\"o}lske, Michael  and
      Potthast, Martin  and
      Syed, Shahbaz  and
      Stein, Benno},
    booktitle = "Proceedings of the Workshop on New Frontiers in Summarization",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4508",
    doi = "10.18653/v1/W17-4508",
    pages = "59--63",
    abstract = "Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a {``}TL;DR{''} to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls.",
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}


@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav},
  year={2023}
}


@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{auer2002nonstochastic,
  title={The nonstochastic multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Freund, Yoav and Schapire, Robert E},
  journal={SIAM journal on computing},
  volume={32},
  number={1},
  pages={48--77},
  year={2002},
  publisher={SIAM}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rosset2024direct,
  title={Direct nash optimization: Teaching language models to self-improve with general preferences},
  author={Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang},
  journal={arXiv preprint arXiv:2404.03715},
  year={2024}
}

@article{zhang2024self,
  title={Self-exploring language models: Active preference elicitation for online alignment},
  author={Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.19332},
  year={2024}
}

@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{cen2024value,
  title={Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF},
  author={Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo},
  journal={arXiv preprint arXiv:2405.19320},
  year={2024}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{huang2023robust,
  title={Robust Knowledge Transfer in Tiered Reinforcement Learning},
  author={Huang, Jiawei and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52073--52085},
  year={2023}
}

@inproceedings{golowich2022can,
  title={Can Q-learning be improved with advice?},
  author={Golowich, Noah and Moitra, Ankur},
  booktitle={Conference on Learning Theory},
  pages={4548--4619},
  year={2022},
  organization={PMLR}
}

@article{gupta2022unpacking,
  title={Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity},
  author={Gupta, Abhishek and Pacchiano, Aldo and Zhai, Yuexiang and Kakade, Sham and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15281--15295},
  year={2022}
}

@article{rame2024rewarded,
  title={Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards},
  author={Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jang2023personalized,
  title={Personalized soups: Personalized large language model alignment via post-hoc parameter merging},
  author={Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2310.11564},
  year={2023}
}


@article{xie2024exploratory,
  title={Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF},
  author={Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2405.21046},
  year={2024}
}

@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={6683--6694},
  year={2021}
}

@article{amortila2024scalable,
  title={Scalable Online Exploration via Coverability},
  author={Amortila, Philip and Foster, Dylan J and Krishnamurthy, Akshay},
  journal={arXiv preprint arXiv:2403.06571},
  year={2024}
}

@article{sason2016f,
  title={$ f $-divergence Inequalities},
  author={Sason, Igal and Verd{\'u}, Sergio},
  journal={IEEE Transactions on Information Theory},
  volume={62},
  number={11},
  pages={5973--6006},
  year={2016},
  publisher={IEEE}
}

@article{huang2022towards,
  title={Towards deployment-efficient reinforcement learning: Lower bound and optimality},
  author={Huang, Jiawei and Chen, Jinglin and Zhao, Li and Qin, Tao and Jiang, Nan and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2202.06450},
  year={2022}
}

@inproceedings{dani2008stochastic,
  title={Stochastic Linear Optimization under Bandit Feedback.},
  author={Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
  booktitle={COLT},
  volume={2},
  pages={3},
  year={2008}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{swamy2024minimaximalist,
  title={A minimaximalist approach to reinforcement learning from human feedback},
  author={Swamy, Gokul and Dann, Christoph and Kidambi, Rahul and Wu, Zhiwei Steven and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2401.04056},
  year={2024}
}

@article{liu2024provably,
  title={Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer},
  author={Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.16436},
  year={2024}
}

@article{gibbs2002choosing,
  title={On choosing and bounding probability metrics},
  author={Gibbs, Alison L and Su, Francis Edward},
  journal={International statistical review},
  volume={70},
  number={3},
  pages={419--435},
  year={2002},
  publisher={Wiley Online Library}
}

@article{xie2022role,
  title={The role of coverage in online reinforcement learning},
  author={Xie, Tengyang and Foster, Dylan J and Bai, Yu and Jiang, Nan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2210.04157},
  year={2022}
}

@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{Narayan2018DontGM,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.08745}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}


@article{dong2405rlhf,
  title={Rlhf workflow: From reward modeling to online rlhf, 2024},
  author={Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong},
  journal={URL https://arxiv. org/abs/2405.07863},
  year={2024}
}


@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{hong2024cross,
  title={Cross-lingual Transfer of Reward Models in Multilingual Alignment},
  author={Hong, Jiwoo and Lee, Noah and Mart{\'\i}nez-Casta{\~n}o, Rodrigo and Rodr{\'\i}guez, C{\'e}sar and Thorne, James},
  journal={arXiv preprint arXiv:2410.18027},
  year={2024}
}


@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@book{ziebart2010modeling,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010},
  publisher={Carnegie Mellon University}
}

@inproceedings{geist2019theory,
  title={A theory of regularized markov decision processes},
  author={Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={2160--2169},
  year={2019},
  organization={PMLR}
}


@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

@article{zhan2023provable,
  title={Provable offline preference-based reinforcement learning},
  author={Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2305.14816},
  year={2023}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}

@inproceedings{novoseller2020dueling,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={1029--1038},
  year={2020},
  organization={PMLR}
}

@article{pacchiano2021dueling,
  title={Dueling rl: reinforcement learning with trajectory preferences},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  journal={arXiv preprint arXiv:2111.04850},
  year={2021}
}

@article{du2024exploration,
  title={Exploration-driven policy optimization in rlhf: Theoretical insights on efficient data utilization},
  author={Du, Yihan and Winnicki, Anna and Dalal, Gal and Mannor, Shie and Srikant, R},
  journal={arXiv preprint arXiv:2402.10342},
  year={2024}
}

@article{ye2024theoretical,
  title={A theoretical analysis of nash learning from human feedback under general kl-regularized preference},
  author={Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2402.07314},
  year={2024}
}

@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@article{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL?},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  journal={arXiv preprint arXiv:2306.14111},
  year={2023}
}

@article{chang2024dataset,
  title={Dataset reset policy optimization for rlhf},
  author={Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Brantley, Kiant{\'e} and Misra, Dipendra and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2404.08495},
  year={2024}
}

@article{gao2024rebel,
  title={Rebel: Reinforcement learning via regressing relative rewards},
  author={Gao, Zhaolin and Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Swamy, Gokul and Brantley, Kiant{\'e} and Joachims, Thorsten and Bagnell, J Andrew and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2404.16767},
  year={2024}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey.},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  year={2009}
}

@incollection{lazaric2012transfer,
  title={Transfer in reinforcement learning: a framework and a survey},
  author={Lazaric, Alessandro},
  booktitle={Reinforcement Learning: State-of-the-Art},
  pages={143--173},
  year={2012},
  publisher={Springer}
}

@article{zhu2023transfer,
  title={Transfer learning in deep reinforcement learning: A survey},
  author={Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K and Zhou, Jiayu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@inproceedings{mann2013directed,
  title={Directed exploration in reinforcement learning with transferred knowledge},
  author={Mann, Timothy A and Choe, Yoonsuck},
  booktitle={European Workshop on Reinforcement Learning},
  pages={59--76},
  year={2013},
  organization={PMLR}
}


@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}

@article{russo2013eluder,
  title={Eluder dimension and the sample complexity of optimistic exploration},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}



@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low bellman rank are pac-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@article{foster2021statistical,
  title={The statistical complexity of interactive decision making},
  author={Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2112.13487},
  year={2021}
}




@inproceedings{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={2826--2836},
  year={2021},
  organization={PMLR}
}


@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@article{jiang2020minimax,
  title={Minimax value interval for off-policy evaluation and policy optimization},
  author={Jiang, Nan and Huang, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2747--2758},
  year={2020}
}

@inproceedings{uehara2020minimax,
  title={Minimax weight and q-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={9659--9668},
  year={2020},
  organization={PMLR}
}

@article{yang2020off,
  title={Off-policy evaluation via the regularized lagrangian},
  author={Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6551--6561},
  year={2020}
}

@inproceedings{zhan2022offline,
  title={Offline reinforcement learning with realizability and single-policy concentrability},
  author={Zhan, Wenhao and Huang, Baihe and Huang, Audrey and Jiang, Nan and Lee, Jason},
  booktitle={Conference on Learning Theory},
  pages={2730--2775},
  year={2022},
  organization={PMLR}
}

@article{guo2024direct,
  title={Direct language model alignment from online ai feedback},
  author={Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others},
  journal={arXiv preprint arXiv:2402.04792},
  year={2024}
}

@article{xu2023some,
  title={Some things are more cringe than others: Preference optimization with the pairwise cringe loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682},
  year={2023}
}

@article{pang2024iterative,
  title={Iterative reasoning preference optimization},
  author={Pang, Richard Yuanzhe and Yuan, Weizhe and Cho, Kyunghyun and He, He and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2404.19733},
  year={2024}
}

@inproceedings{tiapkin2023fast,
  title={Fast rates for maximum entropy exploration},
  author={Tiapkin, Daniil and Belomestny, Denis and Calandriello, Daniele and Moulines, Eric and Munos, Remi and Naumov, Alexey and Perrault, Pierre and Tang, Yunhao and Valko, Michal and Menard, Pierre},
  booktitle={International Conference on Machine Learning},
  pages={34161--34221},
  year={2023},
  organization={PMLR}
}

@article{munos2023nash,
  title={Nash learning from human feedback},
  author={Munos, R{\'e}mi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Michi, Andrea and others},
  journal={arXiv preprint arXiv:2312.00886},
  year={2023}
}


@article{yue2012k,
  title={The k-armed dueling bandits problem},
  author={Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten},
  journal={Journal of Computer and System Sciences},
  volume={78},
  number={5},
  pages={1538--1556},
  year={2012},
  publisher={Elsevier}
}


@article{zhao2024sharp,
  title={Sharp Analysis for KL-Regularized Contextual Bandits and RLHF},
  author={Zhao, Heyang and Ye, Chenlu and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2411.04625},
  year={2024}
}

@article{shi2024crucial,
  title={The Crucial Role of Samplers in Online Direct Preference Optimization},
  author={Shi, Ruizhe and Zhou, Runlong and Du, Simon S},
  journal={arXiv preprint arXiv:2409.19605},
  year={2024}
}

@inproceedings{jaques2017sequence,
  title={Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control},
  author={Jaques, Natasha and Gu, Shixiang and Bahdanau, Dzmitry and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Turner, Richard E and Eck, Douglas},
  booktitle={International Conference on Machine Learning},
  pages={1645--1654},
  year={2017},
  organization={PMLR}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{jaques2019way,
  title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@article{huang2024correcting,
  title={Correcting the mythos of kl-regularization: Direct alignment without overparameterization via chi-squared preference optimization},
  author={Huang, Audrey and Zhan, Wenhao and Xie, Tengyang and Lee, Jason D and Sun, Wen and Krishnamurthy, Akshay and Foster, Dylan J},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{nguyen2024laser,
  title={LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits},
  author={Nguyen, Duy and Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit},
  journal={arXiv preprint arXiv:2410.01735},
  year={2024}
}

@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}