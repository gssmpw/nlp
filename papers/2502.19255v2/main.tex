%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
%
\usepackage{booktabs} %

%
%
%
%
\usepackage{hyperref}

\usepackage[utf8]{inputenc}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{caption}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{hhline}
\usepackage{cite}
\usepackage[table]{xcolor}
\usepackage{rotating} 
\usepackage{natbib}
\usepackage{bm}
\usepackage{diagbox}
\usepackage{cancel}
\usepackage{subcaption}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
\usepackage[accepted]{icml2025}

%
%

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}


%
%
\icmltitlerunning{Can RLHF be More Efficient with Imperfect Reward Models?  A Policy Coverage Perspective}

\include{notations}

\begin{document}

\twocolumn[
\icmltitle{Can RLHF be More Efficient with Imperfect Reward Models? \\ A Policy Coverage Perspective}

%
%
%
%

%
%
%
%

%
%
%
%

\begin{icmlauthorlist}
\icmlauthor{Jiawei Huang}{ethz}
\icmlauthor{Bingcong Li}{ethz}
\icmlauthor{Christoph Dann}{google}
\icmlauthor{Niao He}{ethz}
%
%
%
%
%
%
%
%
\end{icmlauthorlist}

\icmlaffiliation{ethz}{Department of Computer Science, ETH Zurich}
\icmlaffiliation{google}{Google Research}
%

\icmlcorrespondingauthor{Jiawei Huang}{jiawei.huang@inf.ethz.ch}
%

%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%

%
%
%
%
%

\printAffiliationsAndNotice{}  %
%




\allowdisplaybreaks

\begin{abstract}
    Sample efficiency is critical for online Reinforcement Learning from Human Feedback (RLHF). While existing works investigate sample-efficient online exploration strategies, the potential of utilizing misspecified yet relevant reward models to accelerate learning remains underexplored. This paper studies how to transfer knowledge from those imperfect reward models in online RLHF. We start by identifying a novel property of the KL-regularized RLHF objective: \emph{a policy's coverability of the optimal policy is captured by its sub-optimality}. Building on this insight, we propose novel transfer learning principles and a theoretical algorithm with provable benefits compared to standard online learning. Our approach achieves low regret in the early stage by quickly adapting to the best available source reward models without prior knowledge of their quality, and over time, it attains an $\tilde{O}(\sqrt{T})$ regret bound \emph{independent} of structural complexity measures. Empirically, inspired by our theoretical findings, we develop a win-rate-based transfer policy selection method with improved computational efficiency. Moreover, our empirical transfer learning technique is modular and can be integrated with various policy optimization methods, such as DPO, IPO and XPO, to further enhance their performance. We validate the effectiveness of our method through experiments on summarization tasks.

    %
    %
    %
    %
    %
    %
    %
\end{abstract}

\input{Introduction}

\input{Preliminary}

%
%
\input{PolicyCoverage}

\input{Advanced_TaskSelection.tex}

\input{EmpiricalAlgorithms}

\input{Experiments}

\section{Conclusion}
This paper studies reward transfer in the context of online RLHF.
We contribute $\TPO$, a provable and efficient transfer learning algorithm that leverages the structure induced by the KL regularizer.
Based on that, we further develop a UCB-based empirical alternative and evaluate its effectiveness through LLM experiments.
Several promising directions remain for future exploration.
Firstly, an interesting avenue is to develop transfer learning strategies beyond RLHF setting, for example, the Nash Learning from Human Feedback setting.
Secondly, while we focus on policy-level transfer, a finer-grained prompt-wise knowledge transfer may be possible, which allows transfer from different policies in different states.
Thirdly, due to resource limitations, we leave the examination of our methods in fine-tuning much larger-scale language models to the future work.


%
%
%
%
%
%
%
%
%
%
%
%

\newpage
\section*{Acknowledgement}
The work is supported by ETH research grant and Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343 and SNSF Starting Grant.
%
%

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.

\section*{Reproducibility Statement}
The code of all the experiments and the running instructions can be found in \url{https://github.com/jiaweihhuang/RLHF_RewardTransfer}.

\bibliography{references}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section*{Outline of the Appendix}
\begin{itemize}
    \item Appx.~\ref{appx:freq_notations}: Frequently Used Notation.
    \item Appx.~\ref{appx:missing_details}: Missing Details in the Main Text.
    \item Appx.~\ref{appx:adaption_offline}: Offline Learning Results in Previous Literature.
    \item Appx.~\ref{appx:online_oracle}: Verification for Online Learning Oracle Example in Sec.~\ref{sec:main_theory}.
    \item Appx.~\ref{appx:coverage_related}: Proofs for Results in Section~\ref{sec:transfer_coverage_perspective}.
    \item Appx.~\ref{appx:proof_task_selection}: Proofs for the Main Algorithm and Results in Sec.~\ref{sec:main_theory}.
    \item Appx.~\ref{appx:win_rate_and_coverage}: Connection between Win Rate and Policy Coverage Coefficient.
    \item Appx.~\ref{appx:basic_lemma}: Useful Lemmas.
    \item Appx.~\ref{appx:experiment}: Missing Experiment Details.
\end{itemize}
\newpage

\input{Appendix/FreqNotations.tex}


\input{Appendix/ExtendedPreliminary.tex}

\input{Appendix/OfflinePrevResults.tex}

\input{Appendix/OnlineOracle.tex}

\input{Appendix/CoverageRelated}

\input{Appendix/Proof_MainResults}

\input{Appendix/WinRate_and_Coverage}

\input{Appendix/BasicLemma.tex}

\input{Appendix/MissingExpDetails.tex}

%

\end{document}
