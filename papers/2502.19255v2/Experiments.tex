\section{Experiments}\label{sec:experiment}
In this section, we evaluate Alg.~\ref{alg:empirical} on the summarization task using the XSum dataset \citep{Narayan2018DontGM}.
We consider T5 series models \citep{raffel2020exploring} and choose T5-small (80M) as the base model for fine-tuning.
Human reward $r^*$ is simulated by the reward model \citep{dong2405rlhf} distilled from Llama3-8B \citep{dubey2024llama}.
For the source rewards in transfer learning, we consider a collection of imperfect reward models and LLM policies, including, (a) ROUGE-Lsum score \citep{lin2004rouge}, (b) BERTScore \citep{zhang2019bertscore}, (c) T5-base (250M), (d) T5-large (770M).
As illustrated in Sec.~\ref{sec:transfer_setting}, for LLM policies (c) and (d), we treat their log-probability predictions on the given prompt $s$ and response $a$ as the reward scores.

For $\text{Alg}_{\text{PO}}$, we consider three instantiations: DPO~\citep{rafailov2024direct}, IPO~\citep{azar2024general} and XPO~\citep{xie2024exploratory}.
To save space, we present and interpret the results with DPO as the choice below and defer other experiment results and also the concrete setups to Appx.~\ref{appx:experiment}.

\textbf{Experiment Results and Interpretation}~
We run Alg.~\ref{alg:empirical} for $K=3$ iterations and compare its performance with three baselines: (I) vanilla iterative-$\DPO$ without transfer learning (i.e., setting $\text{Alg}_{\text{PO}}$ = DPO and $W = 0$); (II) purely exploiting the worst source reward---ROUGE score; (III) purely exploiting the best source reward---T5-large.
Concretely, baseline (I) removes the transfer learning component in Alg.~\ref{alg:empirical} by assigning $\pi^{k,n} = \pi_\base^k$ for all $n\in[N]$.
For baselines (II) and (III), the worst (ROUGE-LSum) and best (T5-Large) reward models from the candidates (a)-(d) are selected, and pure transfer learning is then performed using the responses recommended by the chosen reward models, i.e., $\pi^{k,n} = \pi^*_{r^w}$ in Alg.~\ref{alg:empirical} for the selected $r^w$.
Here the worst and best reward models are selected based on the final policy value when aligning with the given reward model.

Table.~\ref{tab:experiment} reports the win rates of the policies learned by Alg.~\ref{alg:empirical} competing with the three baselines.
As shown in Column 1, comparing with normal online learning, our transfer strategy demonstrates clear advantages.
Furthermore, Column 2 and 3 suggest that, without prior knowledge of source tasks quality, our method avoids being misled by low-quality tasks and achieves competitive performance compared to exploiting the best reward candidate.

Notably, as suggested by the additional results in Appx.~\ref{appx:experiment}, $\pi^k_\base$ improves over time and $\mP_{r^*}(\pi^*_{r^w} \succ \pi^k_\base)$ for any $w\in[W]$ continuously decreases.
In iteration 3, our empirical TPO automatically switches back to online learning and avoids being restricted by the sub-optimality of source reward models. In the end, it results in higher win rates than purely exploiting the best source reward model T5-Large over 3 iterations.
\begin{table}[t]
    \centering
    \begin{tabular}{cccc}
        \hline
                & \makecell{Without \\ Transfer} & \makecell{Purely Exploit \\ ROUGE-Lsum} & \makecell{Purely Exploit \\ T5-Large} \\
                \hline
         Iter 1 &  $52.1\pm1.2$ & $53.1\pm1.1$ & $49.5\pm0.9$\\
        %
         Iter 2 &  $53.3\pm1.6$ & $54.5\pm1.3$ & $49.1\pm0.4$\\
        %
         Iter 3 &  $54.0\pm1.2$ & $53.3\pm1.5$ & $50.6\pm0.3$\\\hline
    \end{tabular}
    \caption{Win rates (\%) of the policies trained by empirical $\TPO$ (Alg.~\ref{alg:empirical}) are competed with 3 baselines, presented across 3 columns. {Baseline (I)}: without transfer, i.e., iterative-$\DPO$. {Baseline (II)}:  purely utilizing ROUGE-LSum (the lowest-quality source task) in transfer learning. {Baseline (III)}: purely utilizing T5-Large (the highest-quality source task) in transfer learning. Results are averaged with 3 random seeds and 95\% confidence levels are reported.
    }
    \label{tab:experiment}
\end{table}