\section{The Blessing of Regularization: A Policy Coverage Perspective}\label{sec:transfer_coverage_perspective}
Unlike classical pure-reward maximization RL, the RLHF objective in \eqref{eq:rlhf_obj} incorporates regularization with respect to $\pi_\textref$.
We start by identifying distinctive properties associated with such regularization in Sec.~\ref{sec:new_structure}, and discuss their implications on transfer learning in RLHF in Sec.~\ref{sec:new_insights}.
%
%
%
%
%

\subsection{Structural Property Induced by Regularization}\label{sec:new_structure}
%
%
%
%
%
%
%
\begin{restatable}{lemma}{LemCovValGap}\label{lem:coverage_and_value_gap}
    Under Assump.~\ref{assump:policy} and assume $r^w\in[0,R]$ for all $w\in[W]$, then, for any policy $\pi\in\conv(\Pi)\cup\{\pi^*_{r^w}\}_{w=1}^W$,
    %
    %
    %
    \begin{align}
        \cov^{\pi^*_{r^*}|{\pi}} \leq 1 + \kappa(e^{\frac{2{\Rmax}}{\beta}}) \cdot \frac{J_\beta(\pi^*_{r^*}) - J_\beta({\pi})}{\beta},\label{eq:cov_value_gap}
    \end{align}
    where $\kappa(x) := \frac{(x-1)^2}{x-1- \log x} = O(x)$.
\end{restatable}
The key insight of Lem.~\ref{lem:coverage_and_value_gap} is that: for any prospective candidates of the optimal policy (i.e., $\conv(\Pi)$\footnote{Here we consider the convex hull in order to incorporate all possible uniform mixture policies induced by $\Pi$.}) or any transfer candidates (i.e., $\{\pi^*_{r^w}\}_{w=1}^W$), \emph{its coverability of $\pi^*_{r^*}$ is controlled by its policy value gap}.
Intuitively, $\cov^{\tpi|\pi}$ becomes extremely large or even unbounded if there is a significant distribution shift between $\pi$ and $\tpi$.
However, in the presence of regularization ($\beta > 0$), we should only consider policies with bounded policy ratio relative to $\pi_\textref$ (see Assump.~\ref{assump:policy}-(II)), and exclude those (near-)deterministic ones from our policy candidate class $\Pi$, because none of them can be (near-)optimal.
%
In other words, regularization leverages prior knowledge from $\pi_\textref$ and enables a free policy filtration step before learning begins, ensuring that the remaining policies exhibit a favorable structure (Lem.~\ref{lem:coverage_and_value_gap}).

%
%
%

To understand why such property is uniquely arising from regularization, consider a bandit instance with a single optimal arm and multiple suboptimal arms yields rewards $\Rmax$ and $\Rmax - 2\epsilon$, respectively.
In pure reward maximization RL ($\beta = 0$), the optimal policy $\pi^*_{r^*}$ is deterministic.
A policy class $\Pi$ satisfying Assump.~\ref{assump:policy} may include several suboptimal deterministic policies.
The coverage coefficient between any of them and $\pi^*_{r^*}$ is infinity, while their suboptimal gaps are $2\epsilon$ and can be arbitrarily small.

%
%
%
%
%


\subsection{New Insights for Transfer Learning in RLHF}\label{sec:new_insights}
In the online RLHF, the primary goal of exploration is to discover high-reward regions, i.e., the states and actions covered by the optimal policy.
Therefore, in our reward transfer setup, we propose to \textbf{\emph{transfer from the policy with the best coverage of $\pi^*_{r^*}$}}.
Inspired by Lem.~\ref{lem:coverage_and_value_gap}, we identify two novel principles for transfer learning for RLHF objective, which we will further explore in later sections.

\textbf{Principle 1: Select Transfer Policies with High Policy Value}~By Lem.~\ref{lem:coverage_and_value_gap}, exploiting a policy with high value for data collection could ``help'' exploration, because such a policy inherently provides good coverage for $\pi^*_{r^*}$.
In other words, regularization reconciles the trade-off between exploration and exploitation.
This insight allows us to use policy value as a criterion and transfer from the policy achieving the highest value among all candidates.
This strategy is also practical given that policy values can be estimated well.

We emphasize that this principle is unique in the regularized setting.
As exemplified by the bandit instance before, near-optimality does not imply good coverage for $\pi^*_{r^*}$ in the absence of regularization.
To avoid negative transfer in pure reward maximization setting, previous algorithms typically rely on additional assumptions about task similarity and employ sophisticated strategies to balance exploiting good source tasks with exploration~\citep{golowich2022can,huang2023robust}, which can be challenging to generalize beyond the tabular setting.
In contrast, regularization enables us to filter transfer policies directly with their policy value, facilitating the applicablity beyond the tabular setup.


\textbf{Principle 2: Transfer from the Policy Distilled from Online Data---the ``Self-Transfer Learning''}~
We first introduce a key result by combining Lem.~\ref{lem:coverage_and_value_gap} and offline RLHF result in Lem.~\ref{lem:offline_RLHF}.
\begin{restatable}{theorem}{ThmOnlineOffline}\label{thm:general_val_gap}
    Under Assump.~\ref{assump:policy}, w.p. $1-\delta$, given an online dataset $\cD$ generated\footnote{See Cond.~\ref{cond:seq_data} for the definition of data generation process.} by a policy series $\{\pi^t\}_{t=1}^T \in \conv(\Pi)$, running $\RPO$ with $\conv(\Pi)$ and $\cR^\Pi$ on $\cD$ yields a distilled policy $\pi_\SELF$, such that,
    \begin{align*}
        & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\SELF) \leq \numberthis\label{eq:offline_policy_covergence} \\
        &\tilde{O}\Big(e^{2{\Rmax}} \Big(1 + \kappa(e^{\frac{2{\Rmax}}{\beta}})  \sum_{t=1}^T\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\pi^t)}{\beta T}\Big)\sqrt{\frac{1}{T}}\Big).
        %
    \end{align*}
\end{restatable}
To understand the significance of Thm.~\ref{thm:general_val_gap}, consider the case when $\{\pi^t\}_{t\in[T]}$ are produced by a no-regret online learning algorithm, such as $\XPO$ in Lem.~\ref{lem:online_RLHF}.
As a result, the term $\sum_{t=1}^T\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\pi^t)}{\beta T}$ in Eq.~\eqref{eq:offline_policy_covergence} diminishes to 0 as $T$ increases. This implies that the policy $\pi_\SELF$ distilled from online data by offline learning techniques converges to $\pi^*_{r^*}$ at a rate of $O(T^{-\frac{1}{2}})$, which \textbf{\emph{does not depend on}} $|\cS|,|\cA|$ or other complexity measures such as $\cov_{\infty}(\Pi)$ in Lem.~\ref{lem:online_RLHF}.
This result not only strictly improves the sample complexity bounds\footnote{Beyond sample complexity, a regret bound improved to $\tilde{O}(\sqrt{T})$ for online RLHF can be established. We defer it to Coro.~\ref{coro:sqrtT_reg} after presenting our main results.} for existing online RLHF algorithms \citep{xiong2024iterative, xie2024exploratory,cen2024value,zhang2024self}, but also reveals a fundamental difference from the pure reward maximization setting, where lower bounds depending on those structural complexity factors have been established \citep{auer2002nonstochastic,dani2008stochastic}.
We defer detailed discussions to Appx.~\ref{appx:proof_offline_policy_gap}.

More importantly, the faster rate of the convergence of $\pi_\SELF$ to $\pi^*_{r^*}$ also indicates the potential of using $\pi_\SELF$ as a candidate for policy transfer.
%
We term this regime as ``\emph{self-transfer learning}'', and refer $\pi_\SELF$ as the ``\emph{self-transfer policy}''.
Notably, $\pi_\SELF$ continuously improves and converges to $\pi^*_{r^*}$ as the dataset grows, while the source policies $\{\pi^*_{r^w}\}_{w=1}^W$ retain fixed non-zero value gaps due to the imperfections in reward models $\{r^w\}_{w=1}^W$.
This reveals another benefit of self-transfer learning: it helps to avoid being restricted by suboptimal source reward models.

%
%


%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%

%
%


%
%
%
%

%

%

%


%
%
%

%
%

%
%



%
%
%
%

%

\iffalse
Moreover, Thm.~\ref{thm:general_val_gap} also suggests one may regard the offline policy as another candidates used for transfer, which we call the ``\emph{\textbf{self-transfer learning}}''.
More concretely, suppose at step $t$, the accumulative regret by the previous policies $\pi^1,...,\pi^t$ is low, e.g. $\sum_{i=1}^t J_\beta(\pi^*_{r^*}) - J_\beta(\pi^i) = O(\sqrt{t})$. 
By running an offline algorithm (RPO) on the collected dataset, Thm.~\ref{thm:general_val_gap} suggests we will get a good policy $\pi^t_\SELF$ with value gap $\tilde{O}((1 + e^{\frac{{\Rmax}}{\beta}}\sqrt{\frac{1}{\beta t}})\cdot \sqrt{\frac{1}{t}}) \approx \tilde{O}(\frac{1}{\sqrt{t}})$.
When $t$ is relatively large, $\pi^t_\SELF$ would be a better policy to transfer than $\pi^*_{r^w}$ since it has diminishing policy value gap.
Moreover, the convergence rate of $\pi^t_\SELF$ does not have the coefficient depending on $\Complexity(\Pi)$, which suggests its possible superiority over the policies for exploration-based online learning.


\blue{
    Intuition about the structure: the regularization smooths the optimal policy, and therefore, near-optimal policy implies a good coverage.
}

Next, we interpret what Thm.~\ref{thm:general_val_gap} reveals for the main focus of this paper: the reward transfer setting.
As we can see, the coefficient term $1 + e^{\frac{{\Rmax}}{\beta}} \sqrt{\sum_{t=1}^T\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\pi^t)}{2\beta T}}$ is directly related to the accumulative regret of the policy sequence $\pi^1,...,\pi^T$.
This suggests the goal of the reward transfer should be reducing the accumulative regret.
If we have multiple source reward models $\{r^w\}_{w=1}^W$, it also suggests we should consider the one whose corresponding optimal policy achieves the highest regularized policy value $w^* \gets \arg\max_{w\in[W]} J_\beta(\pi^*_{r^w})$.
\fi

%
%


\iffalse
\discussion{
    \paragraph{Comparison with Pure Online Learning from Preferences}
    Suppose we directly run XPO for $T$ steps, which results in a policy
    \begin{align}
        J(\pi^*_{r^*}) - J(\hpi^*_{r^*}) \leq \EE_{s\sim\rho}[\blue{\frac{1}{T} \cdot \XPOReg(T)}] = \tilde{O}(\EE_{s\sim\rho}[\blue{\sqrt{\frac{\cov^\Pi}{T}}}]). \label{eq:no_transfer}
    \end{align}
    Comparing with Eq.~\eqref{eq:transfer} and Eq.~\eqref{eq:no_transfer}, the benefits of the transfer is reflected by, for those transferable states, the gap is reduced from $\sqrt{\cov^\Pi}$ to $O(\exp(\frac{\xi}{\beta}))$ (suppose $W$ and $\alpha$ are constants).
    In the worst case, $\cov^\Pi = \Theta(e^{\frac{{\Rmax}}{\beta}})$, which can be potentially much larger than $O(\exp(\frac{\xi}{\beta}))$.

    Intuitively, for those $\xi$-transferable states, because some of $\pi^*_{r^w}$ already provide good coverage for $\pi^*_{r^*}$ than those exploration policies $\pi_{\mix}$. By separating some sample budgets for $\pi^*_{r^w}$ can results in better performance on those states.

    On the other hand, for those non-transferable states, $\pi^*_{r^w}$ are useless. Therefore, in the worst case, if $\cS^\xi_\transfer = \emptyset$, Eq.~\eqref{eq:transfer} will pay $\frac{1}{\alpha}$ factor because of the waste of samples on $\pi^*_{r^w}$.


    %
    %

    %
    %
    %

    \paragraph{Without regularization, bounded reward gap does not imply policy coverage between optimal policies}
    Consider a Bernoulli bandit setting with $A$ arms, where the true mean reward $r^*$ is unknown.
    Suppose we have a reward model $r$ predicting the mean reward $r(a_1) = 0.5$ and $r(a_i) = 0.5 - \xi$ for all arm $i > 1$.
    In the pure reward maximization without regularization (i.e. $\beta = 0$), Eq.~\eqref{eq:universal_gap} does not rule out the probability of being the optimal action for any $a\in\cA$, and therefore, it does not imply bounded coverage between $\pi^*_{r^*}$ and $\pi^*_{\hat r}$.

    %

}
\fi