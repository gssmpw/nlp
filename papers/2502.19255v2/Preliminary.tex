\section{Preliminary}
In this section, we review the mathematical formulation of RLHF for LLMs and introduce our reward transfer setting.
\subsection{Mathematical Formulation of RLHF}
We adopt the contextual bandits framework \citep{lattimore2020bandit, ouyang2022training}, where we treat the prompt space as the state space $\cS$ and the response space as the action space $\cA$.
Without loss of generality, we assume that both $|\cS|$ and $|\cA|$ are finite.
We denote $\rho \in \Delta(\cS)$ as the prompt distribution.
An LLM can be modeled by a policy $\pi : \cS\rightarrow \Delta(\cA)$, where, given a prompt $s\in\cS$, the responses are generated from the conditional distribution $a\sim\pi(\cdot|s)$.
Throughout this paper, we assume that all considered LLM policy $\pi$ have positive support over the entire state-action space, that is, $\min_{s,a}\pi(a|s) > 0$. This is ensured in practice by the softmax layer in LLM architectures. 


\textbf{Reward Model and Preference Model}~
A reward model is a function $r:\cS\times\cA\rightarrow[0, {\Rmax}]$, where $\Rmax$ is a constant indicating the largest possible reward value.
In the RLHF setting, the reward $r$ is unobservable and we can only access its induced preference model, denoted by $\mP_{r}$. $\mP_{r}(y|s,a,\ta)$ denotes the probability that $a$ is preferable to $\ta$ given $s$ ($y=1$) or not ($y=0$), by reward model $r$.
Following previous works, we consider the Bradley-Terry (BT) model \citep{bradley1952rank}:
%
    $
    \mP_{r}(y=1|s,a,\ta) = \sigma(r(s,a) - r(s,a')),
    $
%
where $\sigma(x) := 1/(1+e^{-x})$ is the sigmoid function.

\textbf{RLHF Learning Setting}~
Given a reward model $r$, in the context of RLHF for LLM fine-tuning, we are interested in optimizing the following the KL-regularized objective:
\begin{align}
    \pi^*_r \gets \argmax_{\pi} & J_\beta(\pi;r) \nonumber\\
    \text{with~}J_\beta(\pi;r):=& \EE_{s\sim \rho, a\sim\pi}[r(s,a)] - \beta \KL(\pi \| \pi_\textref),\label{eq:rlhf_obj}
\end{align}
where we use $\pi_\textref$ to denote the pretrained reference policy and $\KL(\pi\|\pi_\textref) := \EE_{s\sim\rho,a\sim\pi(\cdot|s)}[\log\frac{\pi(a|s)}{\pi_\textref(a|s)}]$.
The above optimization problem yields a closed-form solution:
\begin{align}
    \pi^*_r(a|s) \propto \pi_\textref(a|s)e^{\frac{r(s,a)}{\beta} }. \label{eq:closed_form}
\end{align}
Here $\beta>0$ is a moderate constant and is critical for RLHF in practice, for detailed reasons discussed in Appx.~\ref{appx:extend_prelim}.
%
%
%
%

We use $r^*:\cS\times\cA\rightarrow[0, {\Rmax}]$ to denote the unknown true reward function determining the human's preference $\mP_{r^*}$. 
For simplicity, we omit $r^*$ and use $J_\beta(\pi)$ as a short note for $J_\beta(\pi;r^*)$.
Following previous works \citep{xie2024exploratory,zhang2024self}, we consider the function approximation setting with access to a policy candidates class $\Pi$ ($|\Pi| < +\infty$) satisfying standard assumptions as follows:
%
\begin{assumption}\label{assump:policy} The policy class $\Pi$ satisfies:
        \textbf{(I) Realizability}: The optimal policy $\pi^*_{r^*} \in \Pi$.
        \textbf{(II) Bounded Policy Ratio}: 
            For any $\pi\in\Pi$, $
            \max_{s,a} |\log\frac
            {\pi(a|s)}{\pi_\textref(a|s)}| \leq \frac{{\Rmax}}{\beta}.
            $
\end{assumption}
Note that $\max_{s,a}|\log\frac{\pi^*_{r}(a|s)}{\pi_\textref(a|s)}| \leq \frac{R}{\beta}$ holds as long as $r\in[0, \Rmax]$ (see Lem.~\ref{lem:bounded_ratio} in Appx.~\ref{appx:extend_prelim}).
Thus, Assump.~\ref{assump:policy}-(II) is \emph{not} actually an assumption, but can be interpreted as an additional filtering step by leveraging the boundary of $r^*$.
%
%


%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%

\textbf{Additional Notation}
We use the standard big-O notations and use $\tilde{(\cdot)}$ (e.g. $\tilde{O}$) to suppress logarithmic terms, \textbf{\emph{including log-covering numbers}}.
Given $\Pi$ satisfying Assump.~\ref{assump:policy}, $\conv(\Pi)$ denotes its convex hull, and $\cR^{\Pi}$ denotes the reward class induced by $\Pi$ via Eq.~\eqref{eq:closed_form}, s.t., (1) $\forall r\in\cR^\Pi, r\in[0, R]$; (2) $\exists r\in\cR^{\Pi}, \pi_r^* = \pi^*_{r^*}$.
We defer to Appx.~\ref{appx:extend_prelim} for detailed converting process.
%
Besides, we denote $[n] := \{1,2,...,n\}$ and $a\wedge b := \min\{a,b\}$.
We refer the reader to Appx.~\ref{appx:freq_notations} for a table of commonly used notation in this paper.


\subsection{Reward Transfer Setting}\label{sec:transfer_setting}
We assume there are $W$ source reward models available, denoted by $\{r^w\}_{w=1}^W$, s.t. $\forall w, s,a$, $r^w(s,a) \in [0, {\Rmax}]$.
As motivated in Sec.~\ref{sec:intro}, those reward models are accessible in many scenes.
%
%
%
Given a source reward $r^w$, let $\pi^*_{r^w}$ be the corresponding source policy and denote $\Delta(w) := J_\beta(\pi^*_{r^*}) - J_\beta(\pi^*_{r^w})$ as its policy value gap.
We define $\Delta_{\min} := \min_{w\in[W]} \Delta(w)$ to be the minimal gap for all source policies.
Note that $\Delta_{\min} \geq 0$ and $\Delta_{\min} = 0$ implies $r^* \in \{r^w\}_{w\in[W]}$.
We \textbf{\emph{do not assume prior knowledge on}} $\{\Delta(w)\}_{w\in[W]}$.
%
%
%
%

\textbf{LLM Policy as Reward Model}~
Eq.~\eqref{eq:closed_form} implies that there is a way to convert a given LLM policy $\pi$ to a reward model. Concretely, by choosing an arbitrary distribution $\pi_0$, we can interpret $\beta\log\frac{\pi(\cdot|\cdot)}{\pi_0(\cdot|\cdot)}$ as a reward function that $\pi$ aligns with given $\pi_0$ as the reference policy \citep{rosset2024direct}.
Therefore, while we consistently use the term ``reward transfer'' throughout the paper for clarity, our framework is general to handle transfer learning from any LLM policy through the underlying reward function it aligns with.


%
%
%
%
%


\subsection{Background on Policy Coverability}\label{sec:background_policy_coverage}
We first introduce the formal definition of the policy coverage coefficient, which measures how well a given policy distribution covers the other.
\begin{definition}\label{def:cov_between_policies}
    Given any $\pi,\tpi$, the coverage coefficient for $\tpi$ by $\pi$ is defined by
    %
    $
        \cov^{\tpi|\pi} := \EE_{s\sim\rho,a\sim\tpi(\cdot|s)}[\frac{\tpi(a|s)}{\pi(a|s)}].
    $
    %
\end{definition}
The concept of policy coverage originally emerged from offline RL \citep{chen2019information, yang2020off, zhan2022offline}, where one aims to find a good policy given a fixed dataset. The coverage of the optimal policy by the data-generating policy naturally governs the size of the dataset required to find the optimal policy.
Policy coverage was later extended to online RL, inducing novel complexity measures to characterize intrinsic statistical difficulty of the underlying MDP \citep{xie2022role, amortila2024scalable}.
%
%
%
%

%
Compared to alternative complexity measures, policy coverage is particularly suited for studying sample complexity in RLHF, since optimizing or exploring directly on the policy (LLM) space is preferred for its computational tractability.
%
%
We use coverage as an analytical tool for reward transfer, and our results are based on $\RPO$ \citep{liu2024provably} and $\XPO$ \citep{xie2024exploratory} for offline and online RLHF, respectively:
\begin{lemma}[Offline RLHF; Thm.~5.3 in \citep{liu2024provably}; Informal]\label{lem:offline_RLHF}
    Given a dataset $\cD$ generated by a policy $\pi^\cD$, running $\RPO$ with any $\cR$ including $r^*$ and $\tPi$ yields $\hpi$, s.t.,
    $
        \forall \pi\in\tPi,J_\beta(\pi) - J_\beta(\hpi) = \tilde{O}( e^{2{\Rmax}} \cov^{\pi|\pi^\cD} |\cD|^{-\frac{1}{2}}).
    $
    %
    %
    %
\end{lemma}
\begin{lemma}[Online RLHF; Thm.~3.1 in \citep{xie2024exploratory}; Informal]\label{lem:online_RLHF}
    Running $\XPO$ with $\Pi$ satisfying Assump.~\ref{assump:policy} for $T$ steps yields a policy sequence $\{\pi^t\}_{t=1}^T$, s.t.
    %
    $
        \sum_{t=1}^T J_\beta(\pi^*_{r^*}) - J_\beta(\pi^t) = \tilde{O}({\Rmax}\cdot e^{2{\Rmax}} \cdot\sqrt{\cov_\infty(\Pi) T}).
    $
    %
\end{lemma}
Lem.~\ref{lem:offline_RLHF} states that it is possible to compute an offline policy competitive with any policy well-covered by the dataset.
%
Lem.~\ref{lem:online_RLHF} suggests that the sample efficiency of online RLHF can be characterized by the $L_\infty$ coverability $\cov_\infty(\Pi)$.
Here, $\cov_\infty(\Pi)$ by \citet{xie2022role} is a worst-case version of our Def.~\ref{def:cov_between_policies} as it takes the maximum over $s, a$ and $\tpi$. See Appx.~\ref{appx:online_oracle} for the formal definition.
