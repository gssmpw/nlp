\section{Missing Details in the Main Text}\label{appx:missing_details}

\subsection{Extended Preliminary}\label{appx:extend_prelim}
\paragraph{More Elaborations on the Necessity of Regularization} The RLHF objective Eq.~\eqref{eq:rlhf_obj} typically involves a regularization term $\beta\neq 0$. This regularization is critical in practice for several reasons.
Firstly, it prevents overfitting to human preferences, which can possibly be noisy and biased \citep{gao2023scaling, ouyang2022training}.
%
Moreover, pure reward maximization prefers near-deterministic policies, potentially causing mode collapse. In contrast, regularization encourages the fine-tuned model to retain diversity from the reference policy \citep{jaques2017sequence, jaques2019way}.
Thirdly, reference policies are pretrained on a significantly larger corpus than the post-training data, enabling them to encode more general-purpose knowledge. Regularization helps mitigate catastrophic forgetting, ensuring the model retains this broad knowledge base.

\paragraph{Formal Definition for $\cR^\Pi$}
Given a policy class $\Pi$ satisfying Assump.~\ref{assump:policy}, we use $\cR^{\Pi}$ to denote the reward function class converted from $\Pi$, such that (1) $\forall r\in\cR^\Pi$, $r(\cdot,\cdot)\in[0, R]$; (2) $\exists r\in\cR^{\Pi}, \pi_r^* = \pi^*_{r^*}$.
A possible construction satisfying this is given by
\begin{align}
    \cR^{\Pi} := \{r_{|\pi}|r_{|\pi}(s,a):=\text{Clip}_{[0,{\Rmax}]}[\beta\log\frac{\pi(a|s)}{\pi_\textref(a|s)} - \min_{a'\in\cA}\beta \log \frac{\pi(a'|s)}{\pi_\textref(a'|s)}],~\pi\in\Pi\}.\label{eq:reward_class_conversion}
\end{align}
The rationale behind such a construction lies in that $r_{|\pi^*_{r^*}}$ provably differs from $r^*$ by at most of an action-independent constant under Assump.~\ref{assump:policy}.
We prove this in the following.

For any $s\in\cS$, we denote $a_s := \argmin_{a'\in\cA} \log \frac{\pi^*_{r^*}(a'|s)}{\pi_\textref(a'|s)}$.
According to Eq.~\eqref{eq:closed_form} and the fact that $r^* \in [0, {\Rmax}]$, for any $s\in\cS$ and $a,a'\in\cA$, we have:
\begin{align*}
    0 \leq \beta\log\frac{\pi^*_{r^*}(a|s)}{\pi_\textref(a|s)} - \min_{a'\in\cA}\beta \log \frac{\pi^*_{r^*}(a'|s)}{\pi_\textref(a'|s)} = r^*(s,a) - r^*(s,a_s) \leq {\Rmax},
\end{align*}
where the first inequality is because $a_s$ takes the minimal over $\cA$.
Therefore, $r_{|\pi^*_{r^*}}(s,a) \in [0, {\Rmax}]$ and $r_{|\pi^*_{r^*}}(s,a) - r^*(s,a) = r^*(s,a_s)$, which is action-independent.
In another word, $r_{|\pi^*_{r^*}}$ induces the same optimal policy $\pi^*_{r^*}$.

Under the objective in Eq.~\eqref{eq:rlhf_obj}, the realizability assumption in \citep{liu2024provably} can be relaxed and the reward model class $\cR^{\Pi}$ can be used in their $\RPO$ objective. Because any per-state action-independent shift on the reward space does not change the induced policy in Eq.~\eqref{eq:closed_form}.
As a result, throughout this paper, we will not distinguish between $r^*$ and $r_{|\pi^*_{r^*}}$.

\paragraph{Remarks on Assumption~\ref{assump:policy}-(II)}
\begin{lemma}\label{lem:bounded_ratio}
    If $r^*(s,a)\in[0, R]$ for all $(s,a)\in\cS\times\cA$, we have:
    \begin{align*}
        \max_{s\in\cS,a\in\cA} |\log\frac{\pi^*_{r^*}(a|s)}{\pi_{\textref}(a|s)}| \leq \frac{R_{\max}}{\beta}.
    \end{align*}
\end{lemma}
\begin{proof}
    By definition, for any $s$, 
    \begin{align*}
        \forall a\in\cA,\quad \pi^*_{r^*}(a|s) = \pi_{\textref}(a|s) e^{\frac{r^*(s,a)}{\beta}} / Z(s),
    \end{align*}
    where $Z(s) := \sum_{a\in\cA} \pi_{\textref}(a|s) e^{\frac{r^*(s,a)}{\beta}}$.
    Because $r^*(s,a) \in [0, \Rmax]$, obviously, $1 \leq Z(s) \leq e^{\frac{\Rmax}{\beta}}$. Therefore,
    \begin{align*}
        \forall a\in\cA,\quad |\log\frac{\pi^*_{r^*}(a|s)}{\pi_{\textref}(a|s)}| = |\frac{r^*(s,a)}{\beta} - \log Z(s)| \leq \max\{\frac{\Rmax}{\beta} - \log Z(s), \log Z(s)\} \leq \frac{\Rmax}{\beta}.
    \end{align*}
\end{proof}

\subsection{Other Related Works}\label{appx:related_workds}

\paragraph{Other Related RLHF Literature}
%
Various approaches have been developed for reward-model-free online exploration. For example, DPO \citep{rafailov2024direct} implicitly optimizes the same objective as RLHF without explicit reward modeling. DPO is further extended to different settings; see e.g., online DPO \citep{guo2024direct}, iterative DPO \citep{xu2023some, pang2024iterative, dong2405rlhf}, etc.

%

Another direction is to go beyond Bradley-Terry reward model assumption. A particularly promising set of techniques formulates RLHF as a two-player zero-sum game \citep{yue2012k}, aiming to select policies preferred by the rater to others \citep{rosset2024direct, ye2024theoretical, munos2023nash, swamy2024minimaximalist}.
Investigating knowledge transfer within this framework is an exciting direction for future work.

\paragraph{RL Theory in Pure-Reward Maximization Setting}~
In the classical pure-reward maximization RL setting, sample efficiency is a central topic, with extensive research dedicated to strategic exploration and fundamental complexity measures for online learning \citep{russo2013eluder, jiang2017contextual, jin2021bellman, foster2021statistical, du2021bilinear}.

Besides the literature already mentioned in Sec.~\ref{sec:background_policy_coverage}, there is a rich literature \citep{uehara2020minimax,jiang2020minimax,jin2021pessimism,xie2021bellman} investigating the role of policy coverage (or density ratio) in offline learning.

\paragraph{Regularized RL}
Sample complexity in regularized RL has also been studied in previous works \citep{ziebart2008maximum,ziebart2010modeling,geist2019theory,tiapkin2023fast}.
Nonetheless, most of them focus on tabular settings and do not consider the transfer learning.



%
%
%
%
%


%
%
%


%
%