\section{Experiment Details and Additional Results}\label{appx:experiment}

\subsection{Details in Experiment Setup}
\paragraph{Setup of $r^*$}
Due to the high cost of collecting real human feedback, we use preferences generated by Llama3-8B \citep{dubey2024llama} to simulate the ground-truth human annotations.
More concretely, we adopt \texttt{sfairXC/FsfairX-LLaMA3-RM-v0.1} \citep{dong2405rlhf} as the true reward model $r^*$, which is distilled from \texttt{meta-llama/Meta-Llama-3-8B-Instruct}.
This reward model can be queried with prompt-response pairs and returns reward scores for each of them.


\paragraph{Best-of-N as an Approximation of $\pi^*_{r^w}$}
We recall that we consider 4 source tasks for transfer learning, including, (a) ROUGE-Lsum score \citep{lin2004rouge}, (b) BERTScore \citep{zhang2019bertscore}, (c) T5-base (250M) $\pi_{\text{base}}$, (d) T5-large (770M) $\pi_{\text{large}}$.

To reduce computational complexity, instead of explicitly training the optimal policies $\{\pi^*_{r^w}\}_{w\in[W]}$ associated with each source reward model, we use Best-of-N (BoN) approach, a.k.a. rejection sampling, to approximate the responses generated by $\pi^*_{r^w}$.
Specifically, we generate $\texttt{N}$\footnote{To distinguish with $N$ used to denote the block size in Alg.~\ref{alg:empirical}, we use $\texttt{N}$ to denote the size of BoN.} responses by the online learning policy $\pi^k_\base$ in Alg.~\ref{alg:empirical}, rank them according to the reward model, and select the top-ranked ones.

Furthermore, even for source LLM policies (3) and (4), we find that transferring from BoN-selected responses generated by $\pi^k_\base$ actually outperforms directly using the responses generated by T5-base/large.
We hypothesize that it is because the responses by T5-base/large usually have quite low probability of being generated by the online learning policy, leading to a distribution shift that complicates learning.
In contrast, BoN-selected responses maintain non-trivial probability of being sampled, without significant distributional mismatch.

Next, we elaborate the BoN process with more details.
In our experiment, we choose $\texttt{N}=32$.
For source reward models (1) and (2), we generate $\texttt{N}$ responses, and we compute the ROUGE-Lsum/BERTScore between the generated response and the human-provided summary in XSum dataset as the reward value.
For source policies (3) and (4), motivated by the closed-form solution in Eq.~\eqref{eq:closed_form}, given any prompt $s$ and response $a$, we infer the log-probability of T5-base/large predicting $a$ given $s$ as the reward score, i.e. $\log \pi_{\text{base}}(a|s)$ or $\log \pi_{\text{large}}(a|s)$.
In another word, we interpret T5-base/large as the optimal policies fine-tuned from uniform distribution to align with some reward models, which we treated as source rewards for transfer learning.


\paragraph{Training Details}
Our training setup is based on and adapted from \citep{xiong2024iterative,xie2024exploratory}.
We run for 3 iterations ($K=3$), and in each iteration, we sample a training dataset of size 10k (i.e., the block size $N=$10k). For each prompt, we collect 8 responses as follows.
Firstly, we generate $\texttt{N} + 4$ responses by the online learning policy $\pi^k_\base$.
The initial $\texttt{N}$ responses are used for Best-of-N (BoN) selection, where we choose a source reward model via the UCB strategy in Alg.~\ref{alg:empirical}, and then pick the top 4 from $\texttt{N}$ responses with the highest source rewards.
These 4 responses are merged with the remaining 4 responses and we get a total of 8 responses.
After that, we query $r^*$ to label the reward for those responses, and record the ones with the highest and lowest rewards to serve as positive and negative samples for $\DPO$ training.

In contrast to the procedure presented in Alg.~\ref{alg:empirical}, we utilize 8 responses for each prompt.
Therefore, the win rates are computed in a relatively different way.
Specially, we set $y^{k,n} = 1$ if the response achieving the highest reward comes from the 4 responses selected by the BoN step, and $y^{k,n} = 0$ otherwise.
The updates of the win rates estimation and the computation of UCB bonus terms align with Alg.~\ref{alg:empirical}, except that we set $\hat{\text{WR}}^{\pi^k_\base} = 0.55$ instead of 0.5.
This adjustment establishes a higher threshold for enabling transfer learning, requiring source tasks to outperform the baseline policy $\pi^k_\base$ by a larger margin before being considered.
We believe it enhances overall performance.


Regarding other hyperparameters during the training, the learning rate is 5e-5 with a cosine annealing schedule.
Training is conducted on 4 H-100 GPUs with total batch size 64.
We set the constant parts in the UCB bonus $c \sqrt{\log\frac{1}{\delta}} = 1.0$ in practice, considering the value range of win rates is [0, 1].


\paragraph{Evaluation Details}
During the evaluation phase, we randomly sample 10k prompts from XSum test dataset without repetition. 
For each prompt, we generate one response for each of the policies being compared, and query their reward values from $r^*$ (i.e., the \texttt{sfairXC}\texttt{/FsfairX-LLaMA3-RM-v0.1} reward model).
The win rate is then estimated as the frequency of that one policy generates a response with higher rewards than the other across the 10k prompts.

\subsection{Additional Experiment Results}\label{appx:additional_results}

\paragraph{Results under Other Choices for $\text{Alg}_{\text{PO}}$ in Alg.~\ref{alg:empirical}}
In the following, we report the results with two alternative instaniations of $\text{Alg}_{\text{PO}}$: by optimizing the XPO loss \citep{xie2024exploratory} or the IPO loss \citep{azar2024general}.
All the training setups are the same as the experiments where $\text{Alg}_{\text{PO}}$ is DPO, except that we choose a smaller learning rate 1e-5 for $\text{Alg}_{\text{PO}}$ is IPO.

\begin{remark}
    Different from DPO and IPO, XPO is an online algorithm itself and in their original design, the pairs of online data are generated by an online exploration policy and another fixed base policy, respectively.
    However, empirically, \citet{xie2024exploratory} follow the iterative-DPO and utilize the same online learning policy to generate pairs of online data.
    This exactly aligns with the no transfer baseline we compete with---instantiating $\text{Alg}_{\text{PO}}$ with XPO in Alg.~\ref{alg:empirical} and setting $W=0$, which we refer as iterative-XPO in this paper.
\end{remark}

\begin{table}[h]
    \begin{subtable}{0.52\textwidth}
    \begin{tabular}{cccc}
        \hline
                & \makecell{Without \\ Transfer} & \makecell{Purely Exploit \\ ROUGE-Lsum} & \makecell{Purely Exploit \\ T5-Large} \\
                \hline
         Iter 1 &  $52.3\pm1.0$ & $50.4\pm1.6$ & $49.9\pm0.4$\\
        %
         Iter 2 &  $55.2\pm1.4$ & $52.3\pm0.3$ & $50.1\pm0.3$\\
        %
         Iter 3 &  $55.3\pm1.1$ & $51.8\pm0.5$ & $50.3\pm0.5$\\\hline
    \end{tabular}
    \caption{IPO as $\text{Alg}_{\text{PO}}$ in Alg.~\ref{alg:empirical}}
    \label{tab:IPO}
    \end{subtable}
    \begin{subtable}{0.52\textwidth}
    \begin{tabular}{cccc}
        \hline
                & \makecell{Without \\ Transfer} & \makecell{Purely Exploit \\ ROUGE-Lsum} & \makecell{Purely Exploit \\ T5-Large} \\
                \hline
         Iter 1 &  $52.3\pm 1.1$ & $53.4\pm0.8$ & $50.2\pm0.3$\\
         Iter 2 &  $51.6\pm1.3$ & $54.7\pm1.6$ & $49.1\pm1.3$\\
         Iter 3 &  $52.2\pm1.6$ & $53.8\pm2.9$ & $49.2\pm1.1$\\\hline
    \end{tabular}
    \caption{XPO as $\text{Alg}_{\text{PO}}$ in Alg.~\ref{alg:empirical}}
    \label{tab:XPO}
    \end{subtable}
    \caption{Similar to Table~\ref{tab:experiment}, we report the win rates (\%) of the policies trained by empirical $\TPO$ (Alg.~\ref{alg:empirical}) competed with 3 baselines, presented across 3 columns. {Baseline (I)}: without transfer, i.e., iterative-IPO or iterative-XPO. {Baseline (II)}:  purely utilizing ROUGE-LSum (the lowest-quality source task) in transfer learning. {Baseline (III)}: purely utilizing T5-Large (the highest-quality source task) in transfer learning. Results are averaged with 3 random seeds and 95\% confidence levels are reported.}
\end{table}



\paragraph{Investigation on Source Task Selection}
Fig.~\ref{fig:selection_details} provides further investigations on the source task selection process.
For each iteration $k=1,2,3$, we count the number of times that $\pi^{k,n}$ is occupied by different transfer policies $\{\pi^*_{r^w}\}_{w\in[W]}$ or the online learning policy $\pi^k_\base$ (i.e. without transfer), and provide the results on the top sub-figure in Fig.~\ref{fig:selection_details}.
Besides, in the bottom sub-figure, we report the win rates $\mP_{r^*}(\pi \succ \pi^k_\base)$ for all $\pi \in \{\pi^*_{r^w}\}_{w\in[W]} \cup \{\pi^k_\base\}$.
As illustrated, the UCB sub-routine efficiently explores and identify the source task with the highest win rates against the learning policy $\pi^k_\base$.

Notably, as the improvement of $\pi^k_\base$ over the three iterations, we can observe the transition from transfer learning by leveraging high-quality source tasks to standard online learning.
In other words, our method can automatically switch back to online learning and avoid being restricted by source reward models.


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.6]{Pictures/Results_Num.pdf}
    \includegraphics[scale=0.6]{Pictures/Results_WR.pdf}
    \caption{Deeper investigation on the source reward models selection process. We report the allocation of transfer budgets on each source tasks averaged over 3 trials (top figure) and the win rates $\mP_{r^*}(\cdot\succ\pi^k_\base)$ (bottom figure) for iterations $k=1,2,3$.
    Due to space limit, we use abbreviation rather than the full name of source tasks.
    \text{R}, \text{B}, \text{TB}, \text{TL} and \text{NT} stand for ROUGE-Lsum, BERTScore, T5-Base, T5-Large and No Transfer, respectively.
    }\label{fig:selection_details}
\end{figure}