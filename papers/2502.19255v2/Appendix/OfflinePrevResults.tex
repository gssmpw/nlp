\section{Offline Learning Results in Previous Literature}\label{appx:adaption_offline}
In this section, we recall and adapt some results from \citep{liu2024provably}, which are useful for proofs in other places.
\paragraph{$\RPO$ Optimization Objective}
For completeness, we provide the optimization objective of $\RPO$.
Given a policy class $\tPi$ and a reward function class $\cR$, the $\RPO$ objective solves a mini-max optimization problem defined as follows:
\begin{align}
    \RPO(\tPi,\cR,\cD,\eta) = \arg\max_{\pi\in\tPi}\min_{r\in\cR} L_{\cD}(r) + \eta \EE_{s\sim\rho,a\sim\pi,\ta\sim\pi_\textref}[r(s,a)-r(s,\ta)] - \beta \KL(\pi\|\pi_\textref), \label{eq:RPO_objective} 
\end{align}
where we choose $\pi_\textref$ as the base policy in \citep{liu2024provably}.
In Alg.~\ref{alg:transfer_policy_computing}, we set $\tPi = \conv(\Pi)$ and $\cR = \cR^\Pi$.
\begin{condition}[Sequential Data Generation]\label{cond:seq_data}
    We say a dataset $\cD := \{(s^i,a^i,\ta^i,y^i,\pi^i)\}_{i\leq |\cD|}$ is generated sequentially, if it is generated following:
    \begin{align*}
        \forall i\leq |\cD|,\quad & \pi^i \sim \text{Alg}(\cdot|\{(s^j,a^j,\ta^j,y^j,\pi^j)\}_{j<i}),\\
        & s^i\sim\rho,~a^i\sim\pi^i(\cdot|s^i),~\ta^i\sim\pi_\textref(\cdot|s^i),~y^i\sim \mP_{r^*}(\cdot|s^i,a^i,\ta^i),
    \end{align*}
    where $\text{Alg}$ denotes an algorithm computing the next policy only with the interaction history.
\end{condition}


%
\begin{restatable}{lemma}{LemOfflineLearning}\label{lem:offline_learning}[Adapted from Thm.~5.3 in \citep{liu2024provably}]
    Under Assump.~\ref{assump:policy}, given any $\delta \in (0,1)$, by running $\RPO$ (Eq.~\eqref{eq:RPO_objective}) with $\conv(\Pi), \cR^{\Pi}, \delta$ and a dataset $\cD := \{(s^i,a^i,\ta^i,y^i,\pi^i)\}_{i\leq |\cD|}$ satisfying Cond.~\ref{cond:seq_data}, by choosing $\eta = (1+e^{{\Rmax}})^2 \sqrt{24|\cD|\log\frac{|\Pi|}{\delta}}$, we have:
    \begin{align*}
        \forall \pi \in \conv(\Pi),\quad J_\beta(\pi) - J_\beta(\pi_\SELF) \leq C_\Offline e^{2{\Rmax}}\cdot \cov^{\pi|\pi_\mix^\cD}\sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|}{\delta}},
    \end{align*}
    where we use $\pi_\mix^\cD := \frac{1}{|\cD|}\sum_{i\leq |\cD|} \pi^{i}$ as a short note of the uniform mixture policy.
\end{restatable}
\begin{proof}
    The main difference comparing with \citep{liu2024provably} is that we consider sequentially generated dataset while they study dataset generated by a fixed dataset distribution.
    In the following, we show how to extend their results to our setting.

    Firstly, we check the assumptions. Note that we consider feed $\RPO$ \citep{liu2024provably} by the reward function class $\cR^{\Pi}$ converted from a policy class $\Pi$ satisfying Assump.~\ref{assump:policy}, through Eq.~\eqref{eq:reward_class_conversion}. 
    Therefore, the optimal reward is also realizabile in $\cR^{\Pi}$, and the basic assumptions required by $\RPO$ \citep{liu2024provably} are satisfied.

    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %

    Next, we adapt the proofs in \citep{liu2024provably}. Note that we can directly start with their Eq.~(D.4), because their bounds in Eq.~(D.2) and Eq.~(D.3) only involve optimality of the choice of $\pi_\SELF$ and realizability.
    We move the KL-regularization terms to the LHS and merge to $J_\beta(\pi)$ and $J_\beta(\pi_\SELF)$, and we choose $\pi_\textref$ as the base policy in $\RPO$. The adapted results to our notations would be:
    \begin{align*}
        \forall \pi \in \conv(\Pi),~ J_\beta(\pi) & - J_\beta(\pi_\SELF) \\
        \leq & \max_{r\in\cR^{\Pi}} \EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[(r^*(s,a) - r^*(s,\ta)) - (r(s,a) - r(s,\ta))] \\
        & + \eta^{-1} (\cL_{\cD}(r^*) - \cL_{\cD}(r)).
    \end{align*}
    %
    Recall $\cL_\cD$ is the (unnormalized) negative log-likelihood (NLL) loss, defined in Eq.~\eqref{eq:def_likelihood}.
    Since the dataset $\cD$ is generated sequentially (Cond.~\ref{cond:seq_data}), we can apply the concentration results in Lem.~\ref{lem:MLE_Estimation}, which is a variant of Lemma D.1 in \citep{liu2024provably} for sequentially generated data:
    \begin{align*}
        \text{w.p.}~1-\delta,\quad \forall \pi\in \conv(\Pi),~ J_\beta(\pi) & - J_\beta(\pi_\SELF) \\
        \leq & \EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[(r^*(s,a) - r^*(s,\ta)) - (r_{\gets\pi}(s,a) - r_{\gets\pi}(s,\ta))] \\
        & + \eta^{-1} (\cL_{\cD}(r^*) - \cL_{\cD}(r_{\gets\pi})) \\
        \leq &  \EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[(r^*(s,a) - r^*(s,\ta)) - (r_{\gets\pi}(s,a) - r_{\gets\pi}(s,\ta))] \\
        & - \frac{1}{\eta|\cD|}\sum_{i \leq |\cD|} \EE_{s\sim\rho,a\sim\pi^i(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[\mH^2(\mP_{r_{\gets\pi}}(\cdot|s,a,\ta)\| \mP_{r^*}(\cdot|s,a,\ta))] + \frac{2}{\eta|\cD|}\log\frac{|\Pi|}{\delta}\\
        = &  \EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[(r^*(s,a) - r^*(s,\ta)) - (r_{\gets\pi}(s,a) - r_{\gets\pi}(s,\ta))] \\
        & - \frac{1}{\eta} \EE_{s\sim\rho,a\sim\pi^\cD_\mix(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[\mH^2(\mP_{r_{\gets\pi}}(\cdot|s,a,\ta)\| \mP_{r^*}(\cdot|s,a,\ta))] + \frac{2}{\eta|\cD|}\log\frac{|\Pi|}{\delta},
    \end{align*}
    where we denote $r_{\gets\pi} := \argmax_{r\in\cR^{\Pi}} \EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[(r^*(s,a) - r^*(s,\ta)) - (r(s,a) - r(s,\ta))]$.

    The rest of the proofs in \citep{liu2024provably} can be adapted here, and by choosing $\eta = (1 + e^{{\Rmax}})^{-2} \sqrt{\frac{24}{|\cD|}\log\frac{|\Pi|}{\delta}}$, we can inherit the following guarantee:
    \begin{align*}
        \text{w.p.}~1-\delta,\quad \forall \pi\in \conv(\Pi),~ J_\beta(\pi) - J_\beta(\pi_\SELF) \leq \frac{\sqrt{6}}{4}\cdot (1+e^{{\Rmax}})^2(C_{\pi_\mix^\cD}(\cR^{\Pi};\pi;\pi_\textref)^2 + 1) \sqrt{\frac{1}{|\cD|} \log\frac{|\Pi|}{\delta}}.
    \end{align*}
    Here $C_{\pi_\mix^\cD}(\cR^{\Pi};\pi;\pi_\textref)$ is the coverage coefficient (adapted from Assump. 5.2 \citep{liu2024provably}) with $\pi_\mix^\cD$, which can be upper bounded by:
    \begin{align*}
        C_{\pi_\mix^\cD}(\cR^{\Pi};\pi;\pi_\textref) \leq & \max_{r\in\cR^{\Pi}}\frac{\EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[|(r^*(s,a) - r^*(s,\ta)) - (r(s,a) - r(s,\ta))|]}{\sqrt{\EE_{s\sim\rho,a\sim\pi_\mix^\cD(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[|(r^*(s,a) - r^*(s,\ta)) - (r(s,a) - r(s,\ta))|^2]}} \\
        \leq & \sqrt{\EE_{s\sim\rho,a\sim\pi(\cdot|s)}[\frac{\pi(a|s)}{\pi_\mix^\cD(a|s)}]} \tag{AM-GM inequality; Holds for any $r$ and therefore including the one achieves the maximum}\\
        = & \sqrt{\cov^{\pi|\pi_\mix^\cD}}.
    \end{align*}
    %
    Therefore, we finish the proof. We simplify the upper bound by using $\cov^{\pi|\pi_\mix^\cD} \geq 1$ and $e^{{\Rmax}} \geq 1$.
\end{proof}


