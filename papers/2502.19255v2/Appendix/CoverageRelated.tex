\section{Proofs for Results in Section~\ref{sec:transfer_coverage_perspective}}\label{appx:coverage_related}
\subsection{Proof for Lemma~\ref{lem:coverage_and_value_gap}}
We first introduce some useful results from \citep{sason2016f}. Given two probability distribution $P, Q \in \Delta(\cA)$, we use $D_{+\infty}(P\|Q)$ to denote the Renyi divergence of order $\alpha = +\infty$. 
We follow the definition of $\chi^2$-divergence in \citep{sason2016f} as follows:
\begin{align*}
    \chi^2(P\|Q) = \EE_{s\sim P}[\frac{P(x)}{Q(x)}] - 1.
\end{align*}
%
\begin{lemma}[Theorem 7 in \citep{sason2016f}]\label{lem:KL_reverse_KL}
    Given $P,Q\in\Delta(\cA)$, such that $P\neq Q$ and $P(a),Q(a) > 0$ for all $a\in\cA$, we have:
    \begin{align*}
        \KL(P\|Q) \leq \kappa_1(e^{D_{+\infty}(P\|Q)}) \cdot \KL(Q\|P),
    \end{align*}
    where $\kappa_1:(0,1)\cup(1,+\infty) \rightarrow (0,+\infty),~\kappa_1(t) = \frac{t\log t + (1-t)}{(t-1) - \log t}$.
\end{lemma}
\begin{lemma}[Eq. 182; Theorem 9 in \citep{sason2016f} for $\alpha= 2$]\label{lem:chi_KL}
    Under the same condition as Lem.~\ref{lem:KL_reverse_KL}, 
    \begin{align*}
        \chi^2(P\|Q) \leq \frac{\KL(P\|Q)}{\kappa_2(e^{D_{+\infty}(P\|Q)})},
    \end{align*}
    where $\kappa_2(t) := \frac{t\log t + (1-t)}{(t-1)^2}$.
\end{lemma}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%


%
\begin{lemma}\label{lem:KL_as_value_gap}
    For any policy $\pi$,
    \begin{align*}
        J_\beta(\pi^*_{r^*}) - J_\beta(\pi) = \beta \EE_{s\sim\rho}[\KL(\pi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))].
    \end{align*}
\end{lemma}
\begin{proof}
    A shorter proof can be done by directly assigning $\nu = \pi$ in Lemma 3.1 of \citep{xie2024exploratory}, and here we provide another one without detouring through it.
    \begin{align*}
        &J_\beta(\pi^*_{r^*}) - J_\beta(\pi) \\
        =& \EE_{s\sim\rho,a\sim\pi^*_{r^*}}[r^*(s,a)] - \EE_{s\sim\rho,a\sim\pi}[r^*(s,a)] - \beta \EE_{s\sim\rho,a\sim\pi^*_{r^*}}[\log\frac{\pi^*_{r^*}(a|s)}{\pi_\textref(a|s)}] + \beta \EE_{s\sim\rho,a\sim\pi}[\log\frac{\pi(a|s)}{\pi_\textref(a|s)}] \\
        =& \cancel{\beta \EE_{s\sim\rho,a\sim\pi^*_{r^*}}[\log\frac{\pi^*_{r^*}(a|s)}{\pi_\textref(a|s)}]} - \beta \EE_{s\sim\rho,a\sim\pi}[\log\frac{\pi^*_{r^*}(a|s)}{\pi_\textref(a|s)}] - \cancel{\beta \EE_{s\sim\rho,a\sim\pi^*_{r^*}}[\log\frac{\pi^*_{r^*}(a|s)}{\pi_\textref(a|s)}]} + \beta \EE_{s\sim\rho,a\sim\pi}[\log\frac{\pi(a|s)}{\pi_\textref(a|s)}] \\
        =& \beta \EE_{s\sim\rho,a\sim\pi}[\log\frac{\pi(a|s)}{\pi^*_{r^*}(a|s)}] \\
        =& \beta \EE_{s\sim\rho}[\KL(\pi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))].
    \end{align*}
    where the second equality holds because for any $s,a$
    \begin{align*}
        r^*(s,a) = \beta \log\frac{\pi^*_{r^*}(a|s)}{\pi_\textref(a|s)} + Z(s)
    \end{align*}
    for some $Z(s)$ independent w.r.t. $a$.
\end{proof}


\LemCovValGap*
We prove a stronger result in Lem.~\ref{lem:cov_value_gap_stronger} below, where we consider the policy class including all the policy having bounded ratio with $\pi_\textref$.
\begin{align*}
    \Pi_{\leq\frac{\Rmax}{\beta}} := \{\pi:\cS\rightarrow\Delta(\cA)| \max_{s,a} |\log\frac{\pi(a|s)}{\pi_\textref(a|s)}| \leq \frac{\Rmax}{\beta}\}.
\end{align*}
Lem.~\ref{lem:coverage_and_value_gap} then holds directly as a corollary by combining with Lem.~\ref{lem:convex_hull_property}, Lem.~\ref{lem:bounded_ratio} and the fact that $r^w \in [0, R]$ for all $w\in[W]$.
\begin{lemma}\label{lem:cov_value_gap_stronger}
    For any policy $\pi \in \Pi_{\leq\frac{\Rmax}{\beta}}$,
    \begin{align}
        \cov^{\pi^*_{r^*}|{\pi}} \leq 1 + \kappa(e^{\frac{2{\Rmax}}{\beta}}) \cdot \frac{J_\beta(\pi^*_{r^*}) - J_\beta({\pi})}{\beta},
    \end{align}
    where $\kappa(x) := \frac{(x-1)^2}{x-1- \log x} = O(x)$.
\end{lemma}
\begin{proof}
    Given any $\pi \in \Pi_{\leq\frac{\Rmax}{\beta}}$, we consider a fixed $s > 0$, and apply Lem.~\ref{lem:KL_reverse_KL} and Lem.~\ref{lem:chi_KL} with $P = \pi^*_{r^*}(\cdot|s)$ and $Q = \pi(\cdot|s)$. Since those two lemmas holds when $P \neq Q$, we first check the case when $\pi^*_{r^*}(\cdot|s) \neq \pi(\cdot|s)$:
    \begin{align*}
        \EE_{a\sim\pi^*_{r^*}(a|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}] - 1 =& \chi^2(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s)) \leq \frac{1}{\kappa_2(\zeta)} \KL(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s)) \\
        \leq & \frac{1}{\kappa_2(\zeta)} \cdot \kappa_1(\zeta) \cdot \KL(\pi(\cdot|s)\|\pi^*_{r^*}(\cdot|s)) \\
        =& \frac{(\zeta - 1)^2}{\zeta - 1 - \log \zeta} \cdot \KL(\pi(\cdot|s)\|\pi^*_{r^*}(\cdot|s)).
    \end{align*}
    where we use $\zeta := e^{D_{+\infty}(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s))} > 1$ as a short note.

    We define $\kappa(x) = \frac{(x - 1)^2}{x - 1 - \log x}$. Note that,
    \begin{align*}
        \kappa'(x) =& \frac{2(x - 1)}{x - 1 - \log x} - \frac{(x - 1)^2(1 - x^{-1})}{(x - 1 - \log x)^2} \\
        =&\frac{x - 1}{x - 1 - \log x} \frac{2(x - 1) - 2\log x - (x - 1)(1 - x^{-1})}{x - 1 - \log x} \\
        =&\frac{x - 1}{x - 1 - \log x} \frac{x - x^{-1} - 2\log x }{x - 1 - \log x}.
    \end{align*}
    Now, we consider $g(x) := x - x^{-1} - 2\log x$ for $x \in (1, +\infty)$. Note that, $g(1) = 0$ and
    \begin{align*}
        g'(x) = 1 + \frac{1}{x^2} - \frac{2}{x} \geq 0.
    \end{align*}
    Therefore, $\kappa'(x) \geq 0$, which implies $\kappa(x)$ is increasing for all $x > 1$.

    %
    %
    %
    %
    %
    %
    %
    %
    %

    %
    %
    %
    %
    %
    %
    %
    %
    %

    Under Assump.~\ref{assump:policy},
    \begin{align*}
        D_{+\infty}(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s)) = \log \exp(\max_a \frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}) \leq \frac{2{\Rmax}}{\beta},
    \end{align*}
    which implies $\zeta \leq e^{\frac{{2\Rmax}}{\beta}}$.
    Therefore,
    \begin{align*}
        \EE_{a\sim\pi^*_{r^*}(a|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}] - 1 \leq & \kappa(e^{\frac{2{\Rmax}}{\beta}})  \cdot \KL(\pi(\cdot|s)\|\pi^*_{r^*}(\cdot|s)).
    \end{align*}
    Note that the above inequality also holds when $\pi(\cdot|s) = \pi^*_{r^*}(\cdot|s)$. Therefore, combining with Lem.~\ref{lem:KL_as_value_gap}, we have:
    \begin{align*}
        \cov^{\pi^*_{r^*}|\pi} =& \EE_{s\sim\rho,a\sim\pi^*_{r^*}(a|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}] \leq 1 + \kappa(e^{\frac{2{\Rmax}}{\beta}}) \cdot \EE_{s\sim\rho}[\KL(\pi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))] \\
        = & 1 + \kappa(e^{\frac{2{\Rmax}}{\beta}}) \cdot \frac{J_\beta(\pi^*_{r^*}) - J_\beta(\pi)}{\beta}. 
    \end{align*}

    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
\end{proof}




%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Another Bound for Policy Coverage Coefficient}
In the following, we provide another bound for the coverage coefficient between the optimal policies induced by different reward models.
Although we do not use this lemma in the proofs for other results in this paper, it indicates a different upper bound, and possibly, it is tighter than the one in Lem.~\ref{lem:coverage_and_value_gap} in some cases.
\begin{restatable}{lemma}{LemUBCov}\label{lem:UB_Cov}
    Under Assump.~\ref{assump:policy}, given any bounded reward model $r$, and the associated optimal policy $\pi^*_r$ (defined by Eq.~\eqref{eq:rlhf_obj}), the coverage coefficient between $\pi^*_r$ and $\pi^*_{r^*}$ can be controlled by:
    \begin{align*}
        \cov^{\pi^*_{r^*}|\pi^*_r} \leq \min_{b\in\mR}\EE_{s\sim\rho}[\EE^2_{a\sim\pi^*_{r^*}}[\exp(\frac{|r^*(s,a) - r(s,a)-b|}{\beta})]].
    \end{align*}
\end{restatable}
\begin{proof}
    By definition, the state-wise coverage coefficient
    \begin{align*}
        \cov^{\pi^*_{r^*}|\pi^*_{r}}(s) :=& \EE_{a\sim \pi^*_{r^*}(\cdot|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi^*_{r}(a|s)}] \\
        =&\EE_{a\sim \pi^*_{r^*}(\cdot|s)}[\exp(\frac{r^*(s,a) - r(s,a)}{\beta})] \cdot \frac{Z_{r}(s)}{Z_{r^*}(s)}
    \end{align*}
    Here we denote $Z_r(s) = \sum_{a} \pi_\textref(a|s) \exp(\frac{1}{\beta} r(s,a))$ and similar for $Z_{r^*}(s)$. Therefore,
    \begin{align*}
        \frac{Z_{r}(s)}{Z_{r^*}(s)} = \sum_{a} \frac{\pi_\textref(a|s)\exp(\frac{1}{\beta} r(s,a))}{Z_{r^*}(s)} = \sum_{a} \pi^*_{r^*}(s) \cdot \exp(\frac{1}{\beta}({r}(s,a) - r^*(s,a))) = \EE_{a\sim \pi^*_{r^*}}[\exp(\frac{r(s,a) - r^*(s,a)}{\beta})].
    \end{align*}
    We remark that one important fact we leverage in the second equality is that $\pi^*_{r^*}(a|s) > 0$ for all $a\in\cA$.
    Considering introducing an arbitrary $b \in \cR$, we should have:
    \begin{align*}
        \cov^{\pi^*_{r^*}|\pi^*_{r}} =& \EE_{a\sim \pi^*_{r^*}(\cdot|s)}[\exp(\frac{r^*(s,a) - {r}(s,a) + b}{\beta})] \cdot \EE_{a\sim \pi^*_{r^*}(\cdot|s)}[\exp(\frac{{r}(s,a) - \tilde  r(s,a) - b}{\beta})]\\
        \leq & \EE_{a\sim \pi^*_{r^*}(\cdot|s)}^2[\exp(\frac{|r^*(s,a) - r(s,a) + b|}{\beta})]
    \end{align*}
    Given that $b$ is arbirtary, we can pick the best one:
    \begin{align*}
        \cov^{\pi^*_{r^*}|\pi^*_{r}} \leq \min_{b\in\mR}\EE^2_{a\sim\pi^*_{r^*}}[\exp(\frac{|r^*(s,a) - {r}(s,a)-b|}{\beta})].
    \end{align*}
\end{proof}


\subsection{Proof for Theorem~\ref{thm:general_val_gap}}\label{appx:proof_offline_policy_gap}

\ThmOnlineOffline*
We refer to Lem.~\ref{lem:offline_learning} for the detailed hyperparameter setups.
\begin{proof}
    By Lem.~\ref{lem:offline_learning}, w.p. $1-\delta$,
    \begin{align*}
        \quad J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\SELF) \leq C_\Offline e^{2{\Rmax}}\cdot \cov^{\pi^*_{r^*}|\pi_\mix^T}\sqrt{\frac{1}{T}\log\frac{|\Pi|}{\delta}},
    \end{align*}
    where $\pi_\mix^T:=\frac{1}{T}\sum_{t\in[T]}\pi^t$ is the uniform mixture policy, and the coverage coefficient can be upper bounded by:
    \begin{align*}
        \cov^{\pi^*_{r^*}|\pi_\mix^T}\leq &  1 + \kappa(e^{\frac{2{\Rmax}}{\beta}}) \cdot \frac{J_\beta(\pi^*_{r^*}) - J_\beta({\pi^T_\mix})}{\beta} \tag{Lem.~\ref{lem:coverage_and_value_gap}}\\
        \leq & 1 + \kappa(e^{\frac{2{\Rmax}}{\beta}}) \cdot \sum_{t=1}^T \frac{J_\beta(\pi^*_{r^*}) - J_\beta({\pi^t_\mix})}{\beta T}
    \end{align*}
    %
    %
    %
    %
    Here in the last step, we use the fact that KL divergence is convex, and therefore, $\KL(\pi^T_\mix\|\pi_\textref) \leq \frac{1}{T}\sum_{t=1}^T \KL(\pi^t \| \pi_\textref)$, which implies $J_\beta(\pi^T_\mix) \geq \frac{1}{T} \sum_{t=1}^T J_\beta(\pi^t)$.
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %

    %
    %
    %
\end{proof}


\paragraph{Implication for Online RLHF}
If we consider the policy sequence generated by a no-regret online learning algorithm, we have the following corollary.
\begin{corollary}\label{coro:offline_gap}
    Under Assump.~\ref{assump:policy}, suppose $\pi^1,...,\pi^T$ is generated by a no-regret online learning algorithm with $\sum_{t=1}^T J_\beta(\pi^*_{r^*}) - J_\beta(\pi^t) = \tilde{O}(\Complexity(\Pi)\sqrt{T})$ for some structural complexity measure $\Complexity(\Pi)$, as long as $T = \tilde{\Omega}(\beta^{-2}\Complexity(\Pi)^2\kappa^2(e^{\frac{2{\Rmax}}{\beta}}))$, running $\RPO$ yields an offline policy s.t. $J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\SELF) = \tilde{O}(e^{2{\Rmax}} T^{-\frac{1}{2}})$.
\end{corollary}
The proof is straightforward by noting that $\sum_{t=1}^T\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\pi^t)}{\beta T} = O(\cC(\Pi)\sqrt{T}/T)$, which decays to 0 as $T$ increases.
Coro.~\ref{coro:offline_gap} is remarkable as it implies an $\tilde{O}(\epsilon^{-2})$ sample complexity bound to learn an $\epsilon$-optimal policy for online RLHF (for $\epsilon$ smaller than a threshold), which \textbf{\emph{does not depend on}} the number of states and actions or other complexity measures.
In contrast, in previous online RLHF literature \citep{xiong2024iterative, xie2024exploratory,cen2024value,zhang2024self}, for the uniform mixture policy $\pi^T_\mix := \frac{1}{T}\sum_{t=1}^T \pi^t$, the regret-to-PAC conversion implies a value gap $J_\beta(\pi^*_{r^*}) - J_\beta(\pi^T_\mix) = \tilde{O}(\sqrt{\frac{\Complexity(\Pi)}{T}})$, which has an additional factor $\Complexity(\Pi)$ regarding the complexity of the function class.
This suggests a strict improvement.
%


Moreover, this marks a fundamental difference from the pure reward maximization setting, where lower bounds depending on those factors has been established \citep{auer2002nonstochastic,dani2008stochastic}.
%


\paragraph{Other Previous Works Reporting Faster Convergence Rate}
Several recent works also report faster convergence rate than the information-theoretic lower bounds for online pure reward maximization RL, by exploiting the structure induced by KL regularization.
\citep{shi2024crucial} investigates the tabular softmax parametrization setting and establishes quadratic convergence results.
In contrast, our result is more general, applying to arbitrary policy class.

The work of \citep{zhao2024sharp} is more related to ours. They consider general reward function classes and derive an $O(\epsilon^{-1} \text{Poly}(D))$ sample complexity bound, where $D$ is a coefficient related to the coverage of the distribution $\rho\times\pi_\textref$.
While their dependence on $\epsilon$ is better than ours, their definition of $D$ is not always satisfactory. For example, in the worst case one would have $D = \Omega(\frac{1}{\min_{s\in\cS}\rho(s)})$. This indicates that their bound scales with the number of states, once noticing that $\frac{1}{\min_{s\in\cS}\rho(s)}$ is no smaller than $|\cS|$.
In contrast, the largest coverage-related coefficient in our result is $O(\kappa^2(e^{\frac{2R}{\beta}})) = O(e^{\frac{4R}{\beta}})$, which remains small and is free of $|\cS|$.
Therefore, our Coro.~\ref{coro:offline_gap} can outperform the bound in \citep{zhao2024sharp} in many scenarios.

More importantly, the primary focus of our work is on reward transfer, which is orthogonal to these studies.
%
%