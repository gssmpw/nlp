\section{Proofs for the Main Algorithm and Results in Sec.~\ref{sec:main_theory}}\label{appx:proof_task_selection}
\subsection{Additional Algorithm Details}\label{appx:main_alg_details}
\paragraph{Missing Details for $\TPO$ (Alg.~\ref{alg:main_algorithm})}
For any given $(k,n) \in [K]\times[N]$, we use $\cD^{k,n} := \cup_{i< k \text{ or } i = k, j<n}\{s^{i,j},a^{i,j},\ta^{i,j},y^{i,j},\pi^{i,j}\}$ to denote all the collected data up to step $(k,n)$; $\cD^{k,n}_\Online := \cup_{i<k,j\leq\alpha N\text{ or }i=k,j\leq n\wedge \alpha N}\{s^{i,j},a^{i,j},\ta^{i,j},y^{i,j},\pi^{i,j}\}$ denotes the data collected by $\AlgOnline$ up to step $(k,n)$.


\subsection{Some Useful Lemmas}
\begin{lemma}[MLE Reward Estimation Error]\label{lem:reward_est_error}
    In each call of Alg.~\ref{alg:transfer_policy_computing} with a policy class $\Pi$ satisfying Assump.~\ref{assump:policy} and a dataset $\cD$ generated by a sequence of policies $\pi^1,...,\pi^{|\cD|}$, then, for any policy $\pi$, given any $\delta\in(0,1)$, with probability at least $1-\delta$, for all $w\in[W]$, we have:
    \begin{align*}
        \Big|\Big(\EE_{\rho,\pi}[r^*] - \EE_{\rho,\pi_\textref}[r^*]\Big) - \Big(\EE_{\rho,\pi}[\hr_\MLE] - \EE_{\rho,\pi_\textref}[\hr_\MLE]\Big)\Big| \leq 16e^{2{\Rmax}} \sqrt{\frac{\cov^{\pi|\pi^\cD_\mix}}{|\cD|}\cdot \log\frac{|\Pi|}{\delta}},
    \end{align*}
    where we use $\pi_\mix^\cD := \frac{1}{|\cD|} \sum_{i \leq |\cD|} \pi^i$ as a short note.
\end{lemma}
\begin{proof}
    For any policy $\pi\in\Pi$, by applying Lem.~\ref{lem:r_err_to_Hellinger} with $\pi_\mix^\cD$ and $r \gets \hr_\MLE$, we have:
    \begin{align*}
        &\Big|\Big(\EE_{\rho,\pi}[r^*] - \EE_{\rho,\pi_\textref}[r^*]\Big) - \Big(\EE_{\rho,\pi}[\hr_\MLE] - \EE_{\rho,\pi_\textref}[\hr_\MLE]\Big)\Big| \\
        \leq & \EE_{s\sim\rho,a\sim\pi(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[|\Big(r^*(s,a) - r^*(s,\ta)\Big) - \Big(\hr_\MLE(s,a) - \hr_\MLE(s,\ta)\Big)|] \\
        \leq& 8\sqrt{2}e^{2{\Rmax}} \sqrt{\cov^{\pi|\pi^\cD_\mix} \cdot \frac{1}{|\cD|} \cdot \sum_{i\leq|\cD|} \EE_{s\sim\rho,a\sim\pi^i(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[\mH^2(\mP_{\hr_\MLE}(\cdot|s,a,\ta)\|\mP_{r^*}(\cdot|s,a,\ta))]}. 
    \end{align*}
    By applying Lem.~\ref{lem:MLE_Estimation}, and the fact that $\hr_\MLE, r^* \in \cR^{\Pi}$, for any $\delta\in(0,1)$, w.p. $1-\delta$, we have:
    \begin{align*}
        & \frac{1}{|\cD|} \sum_{i\leq |\cD|} \EE_{s\sim\rho,a\sim\pi^i(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[\mH^2(\mP_{\hr_\MLE}(\cdot|s,a,\ta)\|\mP_{r^*}(\cdot|s,a,\ta))] \\
        \leq & L_{\cD}(\hr_\MLE) - L_{\cD}(r^*) + \frac{2}{|\cD|}\log\frac{|\Pi|}{\delta} \\
        \leq & \frac{2}{|\cD|} \log\frac{|\Pi|}{\delta} \tag{Assump.~\ref{assump:policy} and $\hr_\MLE$ minimizes the negative log-likelihood}.
    \end{align*}
    %
    %
    %
    %
    %
    %
    %
    Therefore, we finish the proof.
\end{proof}
%
%
%
%
%
%
%
%

\LemOptismValErr*
\begin{proof}
    Note that $\frac{\cov^{\pi^*_{r^w}|\pi^\cD_\mix}}{|\cD|} \leq \frac{1}{N(w;\cD)}$, where we recall that $N(w;\cD) := \sum_{i\leq|\cD|} \mI[\pi^i = \pi^*_{r^w}]$ denotes the number of occurrences of $\pi^*_{r^w}$ in the dataset. By Lem.~\ref{lem:reward_est_error}, w.p. $1-\delta'$, for all $w\in[W]$, and any $(k,n)\in[K]\times[N]$ occurs in the call of Alg.~\ref{alg:main_algorithm} such that $n>\alpha N$:
    \begin{align*}
        \Big|\Big(\EE_{\rho,\pi^*_{r^w}}[r^*] - \EE_{\rho,\pi_\textref}[r^*]\Big) - \Big(\EE_{\rho,\pi^*_{r^w}}[\hr_\MLE] - \EE_{\rho,\pi_\textref}[\hr_\MLE]\Big)\Big| \leq & 16e^{2{\Rmax}} \sqrt{\frac{1}{N(w;\cD^{k,n})} \log\frac{|\Pi|W}{\delta'}}.
    \end{align*}
    Recall
    \begin{align*}
        \hV(\pi^*_{r^w};\cD) :=& \EE_{\rho,\pi^*_{r^w}}[\hr_\MLE] - \EE_{\rho,\pi_\textref}[\hr_\MLE] - \beta \KL(\pi^*_{r^w}\|\pi_\textref) + 16e^{2{\Rmax}} \sqrt{\frac{1}{N(w;\cD^{k,n})} \log\frac{|\Pi|WT}{\delta}}.
    \end{align*}
    By taking the union bound for all $T$ iterations (choosing $\delta' = \delta/T$), we finish the proof.
\end{proof}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{lemma}[Estimation Error for Self-Transfer Policy]\label{lem:est_error_self_transfer}
    For any $k > 1$ and $\alpha N < n \leq N$, in each call of Alg.~\ref{alg:transfer_policy_computing} in the iteration $(k,n)$ of Alg.~\ref{alg:main_algorithm} with a dataset $\cD := \{(s^i,a^i,\ta^i,y^i,\pi^i)\}_{i\leq |\cD|}$ satisfying Cond.~\ref{cond:seq_data}, then, given any $\delta\in(0,1)$, w.p. $1-\delta$:
    \begin{align*}
        \hat{V}(\pi_\SELF; \cD) \leq & J_\beta(\pi_\SELF) - J_\beta(\pi_\textref) \\
        \hat{V}(\pi_\SELF; \cD) \geq & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - c'\cdot {\Rmax} e^{2{\Rmax}}\cdot \Big(\cov^{\pi^*_{r^*}|\pi_\mix^\cD} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big) \cdot \sqrt{\frac{1}{|\cD|}\log^{c_0}\frac{|\Pi|T}{\delta}},
    \end{align*}
    where we use $\pi_\mix^\cD := \frac{1}{|\cD|}\sum_{i\leq |\cD|} \pi^{|\cD|}$ as a short note, and $c'$ is some absolute constant.
\end{lemma}
\begin{proof}
    Recall that
    \begin{align*}
        \hat{V}(\pi_\SELF; \cD) :=& \EE_{\rho,\pi_\SELF}[\hr_\SELF] - \EE_{\rho,\pi_\textref}[\hr_\SELF] - \beta \KL(\pi_\SELF\|\pi_\textref) \\
         & + \frac{1}{\eta} L_{\cD}(\hr_\SELF) - \frac{1}{\eta} L_{\cD}(\hr_\MLE) - \bonus,
    \end{align*}
    Here we use $\bonus := 2c\cdot e^{2{\Rmax}} \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}$ as a short note of the bonus term.
    By definition,
    %
    \begin{align*}
        &\hat{V}(\pi_\SELF; \cD) \\
        \leq & \EE_{\rho,\pi_\SELF}[r^*] - \EE_{\rho,\pi_\textref}[r^*] - \beta \KL(\pi_\SELF\|\pi_\textref) + \frac{1}{\eta} L_{\cD}(r^*) - \frac{1}{\eta} L_{\cD}(\hr_\MLE) - \bonus \tag{Pessimistic estimation of $\hr_\SELF$ in Eq.~\eqref{eq:RPO_objective}}\\
        \leq & J_\beta(\pi_\SELF) - J_\beta(\pi_\textref) + \frac{2}{\eta|\cD|}\log\frac{|\Pi|}{\delta} - \bonus \tag{Lem.~\ref{lem:MLE_Estimation}} \\
        \leq & J_\beta(\pi_\SELF) - J_\beta(\pi_\textref) + 2c\cdot e^{2{\Rmax}}\sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}} - \bonus.\numberthis\label{eq:Vhat_Offline_upper_bound}
    \end{align*}
    The last step is because of our choice of $\eta = (1+e^{{\Rmax}})^{-2} \sqrt{\frac{24}{|\cD|}\log\frac{|\Pi|T}{\delta}}$.

    For the lower bound, note that for any policy $\pi \in \conv(\Pi)$, we have:
    \begin{align*}
        & J_\beta(\pi) - J_\beta(\pi_\textref) - \hat{V}(\pi_\SELF; \cD)\\
        = & \Big(\EE_{\rho,\pi}[r^*] - \EE_{\rho,\pi_\textref}[r^*] - \beta \KL(\pi\|\pi_\textref) \Big) \\
        & - \Big(\EE_{\rho,\pi_\SELF}[\hr_\SELF] - \EE_{\rho,\pi_\textref}[\hr_\SELF]- \beta \KL(\pi_\SELF\|\pi_\textref) + \frac{1}{\eta} L_{\cD}(\hr_\SELF)\Big) + \frac{1}{\eta} L_{\cD}(\hr_\MLE) + \bonus \\
        \leq & \Big(\EE_{\rho,\pi}[r^*] - \EE_{\rho,\pi_\textref}[r^*] - \beta \KL(\pi\|\pi_\textref) \Big) - \min_{r\in\cR^{\Pi}}\Big(\EE_{\rho,\pi}[r] - \EE_{\rho,\pi_\textref}[r]- \beta \KL(\pi\|\pi_\textref) + \frac{1}{\eta} L_{\cD}(r)\Big) \tag{Optimality of $\pi_\SELF$ in $\RPO$;}\\
        & + \frac{1}{\eta} L_{\cD}(r^*)  + \bonus \tag{$\hr_\MLE$ minimizes $L_{\cD}$ }\\
        \leq & \EE_{s\sim\rho,a\sim\pi,\ta\sim\pi_\textref}[|r^*(s,a) - r^*(s,\ta) - r_{\pi;\cD}(s,a) + r_{\pi;\cD}(s,\ta)|] + \frac{1}{\eta} L_{\cD}(r^*) - \frac{1}{\eta} L_{\cD}(r_{\pi;\cD}) + \bonus \tag{We use $r_{\pi;\cD}$ to denote the reward achieves the above minimum} \\
        \leq & \frac{2}{\eta|\cD|} \log\frac{|\Pi|}{\delta} + 8\sqrt{2}e^{2{\Rmax}} \sqrt{\frac{\cov^{\pi|\pi_\mix^\cD}}{|\cD|} \cdot \sum_{i\leq|\cD|} \EE_{s\sim\rho,a\sim\pi^i(\cdot|s),\ta\sim\pi_\textref(\cdot|s)}[\mH^2(\mP_{r_{\pi;\cD}}(\cdot|s,a,\ta)\|\mP_{r^*}(\cdot|s,a,\ta))]} \\
        & - \frac{1}{\eta} \sum_{i \leq |\cD|} \EE_{s\sim\rho,a\sim\pi^i,\ta\sim\pi_\textref}[\mH^2(\mP_{r_{\pi;\cD}}(\cdot|s,a,\ta) \| \mP_{r^*}(\cdot|s,a,\ta))] \tag{Lem.~\ref{lem:MLE_Estimation} and Lem.~\ref{lem:r_err_to_Hellinger}}  + \bonus \\
        \leq & \frac{2}{\eta|\cD|} \log\frac{|\Pi|}{\delta} + 64 \eta e^{4{\Rmax}} \frac{\cov^{\pi|\pi_\mix^\cD}}{|\cD|} \tag{$ax - b x^2 \leq \frac{a^2}{4b}$}  + \bonus\\
        \leq & 4 c_2 \cdot e^{2{\Rmax}}\cdot \cov^{\pi|\pi_\mix^\cD} \cdot \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}  + \bonus. \numberthis\label{eq:V_pi_off_LB}
    \end{align*}
    where the last step is because of our choice of $\eta = c\cdot (1+e^{{\Rmax}})^{-2} \sqrt{\frac{24}{|\cD|}\log\frac{|\Pi|T}{\delta}}$.

    Next, we evaluate some choice of $\pi$. We first consider $\pi = \pi^*_{r^*} \in \conv(\Pi)$, the above result implies,
    \begin{align*}
        J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \hat{V}(\pi_\SELF; \cD) \leq 4 c_2 \cdot e^{2{\Rmax}}\cdot \cov^{\pi^*_{r^*}|\pi_\mix^\cD} \cdot \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}  + \bonus. \numberthis\label{eq:Vhat_Offline_lower_bound_1}
    \end{align*}
    Secondly, we consider the mixture policy $\pi = \pi_{\mix}^{k-1} := \frac{1}{\alpha (k-1)N}\sum_{i=1}^{k-1} \pi^{i,j} \in \conv(\Pi)$. Because of the convexity of KL divergence, $J(\pi)$ is concave in $\pi$, by Jensen's inequality, we have:
    \begin{align*}
        J_\beta(\pi^*_{r^*}) -  J_\beta(\pi^{k-1}_{\mix}) =&J_\beta(\pi^*_{r^*}) - \EE_{s\sim\rho,a\sim\pi^{k-1}_\mix(\cdot|s)}[r^*(s,a)] + \beta \KL(\pi^{k-1}_\mix\|\pi_\textref) \\
        \leq & J_\beta(\pi^*_{r^*}) -  \frac{1}{\alpha (k-1)N}\sum_{i=1}^{k-1} \sum_{1\leq j\leq \alpha N} \Big(\EE_{s\sim\rho,a\sim\pi_\Online^i(\cdot|s)}[r^*(s,a)] - \beta \KL(\pi^{i,j}\|\pi_\textref)\Big)\\
        \leq & C_\Online {\Rmax} e^{2{\Rmax}} \sqrt{\frac{\Complexity(\Pi)}{\alpha (k-1)N} \log^{c_0} \frac{|\Pi|T}{\delta}} \leq C_\Online {\Rmax} e^{2{\Rmax}} \sqrt{\frac{2\Complexity(\Pi)}{\alpha kN} \log^{c_0} \frac{|\Pi|T}{\delta}} \tag{Cond.~\ref{def:online_oracle}}.
    \end{align*}
    Note that $|\cD|\pi^\cD_\mix \geq \alpha(k-1)N\pi^{k-1}_\mix$, which implies $\cov^{\pi^{k-1}_\mix|\pi_\mix^\cD} \leq \frac{|\cD|}{\alpha(k-1)N} \leq \frac{kN}{\alpha(k-1)N}\leq \frac{2}{\alpha}$.
    Therefore, by Eq.~\eqref{eq:V_pi_off_LB},
    \begin{align*}
        J_\beta(\pi^{k-1}_\mix) - J_\beta(\pi_\textref) - \hat{V}(\pi_\SELF; \cD) \leq 4 c_2 \cdot e^{2{\Rmax}}\cdot \frac{2}{\alpha} \cdot \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}.
    \end{align*}
    Combining the above two inequalities together, we have:
    \begin{align*}
        &J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \hat{V}(\pi_\SELF; \cD) \\
        \leq & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_{\mix}^{k-1}) + J_\beta(\pi_{\mix}^{k-1}) - J_\beta(\pi_\textref) - \hat{V}(\pi_\SELF; \cD)\\
        \leq &  C_\Online {\Rmax} e^{2{\Rmax}} \sqrt{\frac{2\Complexity(\Pi)}{\alpha kN} \log^{c_0} \frac{|\Pi|T}{\delta}} + 4 c_2 \cdot e^{2{\Rmax}}\cdot \frac{2}{\alpha} \cdot \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}  + \bonus \\
        \leq & c_3 {\Rmax} \cdot e^{2{\Rmax}} \sqrt{\frac{\Complexity(\Pi)}{\alpha^2|\cD|} \log^{c_0}\frac{|\Pi|T}{\delta}} + \bonus.\numberthis\label{eq:Vhat_Offline_lower_bound_2}
    \end{align*}
    Therefore, under our choice of $\bonus = 2c\cdot\sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}$, Eq.~\eqref{eq:Vhat_Offline_upper_bound}, Eq.~\eqref{eq:Vhat_Offline_lower_bound_1} and Eq.~\eqref{eq:Vhat_Offline_lower_bound_2} imply,
    \begin{align*}
        \hat{V}(\pi_\SELF; \cD) \leq & J_\beta(\pi_\SELF) - J_\beta(\pi_\textref) \\
        \hat{V}(\pi_\SELF; \cD) \geq & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - c' {\Rmax} e^{2{\Rmax}}\cdot \Big(\cov^{\pi^*_{r^*}|\pi_\mix^\cD} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big) \cdot \sqrt{\frac{1}{|\cD|}\log^{c_0}\frac{|\Pi|T}{\delta}}.
    \end{align*}
\end{proof}


%
%
%
%
%
%
%
%
\LemSelfTransErr*
\begin{proof}
    By applying Lem.~\ref{lem:est_error_self_transfer} with appropriate constants, and taking the union bound over all iterations, we can finish the proof.
\end{proof}


\subsection{Proof for Thm.~\ref{thm:regret_guarantees}}

\ThmMainReg*
Throught the proof, we follow the convention that $1/0 = +\infty$.
\begin{proof}
    Since we divide the total budget $T$ to $K$ batches with batch size $N$, we will use two indices $\tK\in[K]$ and $\tN\in[N]$ to represent the current iteration number, i.e. the $\tN$-th iteration in the $\tK$-th batch.
    We will divide the indices of previous iterations to two parts, depending on whether we conduct normal online learning (the first $\alpha N$ samples in each batch) or do transfer learning (the rest $(1-\alpha) N$ samples in each batch):
    \begin{align*}
        &\cI^{\Online}_{\tK,\tN}:=\{(k,n)|k< \tK, n\leq \alpha N,\text{~or~}k=\tK, n\leq \tN \wedge \alpha N\},\\
        &\cI^{\Transfer}_{\tK,\tN}:=\{(k,n)|k< \tK, \alpha N < n\leq N,\text{~or~}k=\tK, \alpha N < n\leq \tN \}, \\
        &\cI_{\tK,\tN} := \cI^{\Online}_{\tK,\tN} \cup \cI^{\Transfer}_{\tK,\tN} = \{(k,n)|k< \tK, n\leq N,\text{~or~}k=\tK, n\leq \tN \}.
    \end{align*}
    For the policies generated by online algorithm, under the condition in Def.~\ref{def:online_oracle}, w.p. $1-\delta$, for any $\tK\in[K], \tN\in[N]$ we have:
    \begin{align}
        \sum_{(k,n)\in\cI^{\Online}_{\tK,\tN}} J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n}) \leq C_\Online {\Rmax} e^{2{\Rmax}} \sqrt{\Complexity(\Pi) |\cI^{\Online}_{\tK,\tN}| \log^{c_0}\frac{|\Pi|T}{\delta}}.\label{eq:online_regret}
    \end{align}
    Next, we focus on the performance of transfer policies. We first introduce a few notation for convenience.

    \paragraph{Additional Notations}
    We use $\pi^{k,n}_\SELF$ to denote the offline policy computed by Alg.~\ref{alg:transfer_policy_computing} called by Alg.~\ref{alg:main_algorithm} at iteration $(k,n)$ for some $\alpha N < n \leq N$.
    We denote $\cE^{k,n}_\SELF := \{\pi^{k,n}_\SELF = \pi^{k,n}\}$ to be the event that Alg.~\ref{alg:transfer_policy_computing} returns $\pi^{k,n}_\SELF$ as the policy, and use $\cE^{k,n}_w := \{\pi^*_{r^w} = \pi^{k,n}\}$ to denote the event that Alg.~\ref{alg:transfer_policy_computing} pick and return $\pi^*_{r^w}$.
    Besides, we use $\neg\cE^{k,n}_\SELF := \bigcup_{w\in[W]} \cE^{k,n}_w$ as a short note for the event that Alg.~\ref{alg:transfer_policy_computing} does not return the offline policy $\pi^{k,n}_\SELF$.
    Recall the definition $\Delta(w) := J_\beta(\pi^*_{r^*}) - J_\beta(\pi^*_{r^w})$, and $\Delta_{\min} = \min_{w\in[W]} \Delta(w)$. 
    We will use $w^*$ to denote the index of the task achieves $\Delta_{\min}$ (or any of the tasks if multiple maximizers exist).
    Given the dataset $\cD^{k,n}$ we use $\pi^{k,n}_\mix := \frac{1}{|\cD^{k,n}|} \sum_{i,j\in \cD^{k,n}} \pi^{i,j}$ to be the uniform mixture policy from $\cD^{k,n}$.

    Then, we decompose the accumulative value gap depending on whether $\cE^{k,n}_\SELF$ is true or not. We use $\mathbb{I}[\cE]$ as the indicator function, which takes value 1 if $\cE$ happens and otherwise 0.
    For any $\tK\in[K], \tN\in[N]$, we have:
    \begin{align}
        &\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n}) \nonumber\\
        =& \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\cE^{k,n}_\SELF] (J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n})) + \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\neg\cE^{k,n}_\SELF](J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n})). \label{eq:value_gap_decomposition}
    \end{align}
    \paragraph{Part-(1) Upper Bound the First Part in Eq.~\eqref{eq:value_gap_decomposition}}
    We first bound the accumulative error when $\mathbb{I}[\cE^{k,n}_\SELF] = 1$.
    On the good events in Lem.~\ref{lem:formal_optism_val_est_error} and Lem.~\ref{lem:formal_val_est_error} (which holds w.p. $1-\delta$), $\mathbb{I}[\cE^{k,n}_\SELF] = 1$ implies
    \begin{align*}
        \hV(\pi_\SELF; \cD^{k,n}) \geq \max_{w\in[W]} \hV(\pi^*_{r^w};\cD^{k,n}) \geq \max_{w\in[W]} J_\beta(\pi^*_{r^w}) - J_\beta(\pi_\textref) = J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \Delta_{\min},
    \end{align*}
    and as implied by Lem.~\ref{lem:formal_val_est_error}
    \begin{align*}
        J_\beta(\pi^*_{r^*}) -  J_\beta(\pi_\SELF) \leq & \Delta_{\min},\\
        J_\beta(\pi^*_{r^*}) -  J_\beta(\pi_\SELF) \leq & c_2 {\Rmax} e^{2{\Rmax}}\cdot \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{k,n}} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big) \cdot \sqrt{\frac{1}{|\cD^{k,n}|}\log^{c_0}\frac{|\Pi|T}{\delta}}.
    \end{align*}
    Combining all the results above, we conclude that
    \begin{align*}
        J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\SELF) \leq & \Delta_{\min} \wedge c_2 {\Rmax} e^{2{\Rmax}}\cdot \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{k,n}} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big) \cdot \sqrt{\frac{1}{|\cD^{k,n}|}\log^{c_0}\frac{|\Pi|T}{\delta}} \\
        =& \Delta_{\min} \wedge \iota^{k,n}.
    \end{align*}
    Here for simplicity, we use 
    $$
    \iota^{k,n} := c_2 {\Rmax} e^{2{\Rmax}}\cdot \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{k,n}} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big) \cdot \sqrt{\frac{1}{|\cD^{k,n}|}\log^{c_0}\frac{|\Pi|T}{\delta}}
    $$
    as a short note, indexed by $k,n$.
    Therefore, 
    \begin{align}
        \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\cE^{k,n}_\SELF] (J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n})) = \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\cE^{k,n}_\SELF] (\Delta_{\min} \wedge \iota^{k,n}).\label{eq:offline_accum_gap}
    \end{align}

    \paragraph{Part-(2) Upper Bound the Second Part in Eq.~\eqref{eq:value_gap_decomposition}}
    Next, we bound the accumulative error when $\mathbb{I}[\neg\cE^{k,n}_\SELF] = 1$.
    Note that,
    \begin{align*}
        & \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\neg\cE^{k,n}_\SELF](J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n})) = \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}}\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \mathbb{I}[\cE^{n,k}_w] \Delta(w)
    \end{align*}
    Here we only focus on those source tasks with $\Delta(w) > 0$, since transferring from $\pi^*_{r^w}$ with $\Delta(w) = 0$ does not incur regret.
    We separate source tasks into two sets $\cW_{\leq 2\Delta_{\min}} := \{w\in[W]|\Delta(w) \leq 2\Delta_{\min}\}$ and $\cW_{> 2\Delta_{\min}} := \{w\in[W]|\Delta(w) > 2\Delta_{\min}\}$.
    For $w\in \cW_{> 2\Delta_{\min}}$, on the same good events in Lem.~\ref{lem:formal_optism_val_est_error} and Lem.~\ref{lem:formal_val_est_error}, $\mathbb{I}[\cE^{n,k}_w] = 1$ implies
    \begin{align*}
        \Delta_{\min} =& J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - J_\beta(\pi^*_{r^{w^*}}) + J_\beta(\pi_\textref)\\
        \geq & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \hV^{k,n}(\pi^*_{r^{w^*}};\cD{}^{k,n-1}) \\
        \geq & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \hV^{k,n}(\pi^*_{r^w};\cD{}^{k,n-1}) \\
        \geq & J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - J_\beta(\pi^*_{r^{w}}) + J_\beta(\pi_\textref) - 32\cdot e^{2{\Rmax}}\sqrt{\frac{1}{N(w;\cD^{k,n})}\log\frac{|\Pi|WT}{\delta}} \\
        = & \Delta(w) - 32\cdot e^{2{\Rmax}}\sqrt{\frac{1}{N(w;\cD^{k,n})}\log\frac{|\Pi|WT}{\delta}}.
    \end{align*}
    In the following, we use $c_1 = 32$ as a short note, then the above implies
    \begin{align*}
        N(w;\cD^{k,n}) \leq \frac{c_1^2 e^{4{\Rmax}}}{(\Delta(w) - \Delta_{\min})^2} \log\frac{|\Pi|WT}{\delta} \leq \frac{4c_1^2 e^{4{\Rmax}}}{\Delta(w)^2} \log\frac{|\Pi|WT}{\delta}
    \end{align*}
    and therefore,
    \begin{align*}
        \forall w\in\cW_{>2\Delta_{\min}},\quad \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\cE^{n,k}_w] \Delta(w) \leq \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)} \log\frac{|\Pi|WT}{\delta},
        %
    \end{align*}
    %
    For $w\in\cW_{\leq 2\Delta_{\min}}$, we introduce a new event $\cE^{n,k}_{2\iota < \Delta_{\min}} := \{2\iota^{k,n} \leq \Delta_{\min}\}$. Note that, when $\mI[\cE^{n,k}_{2\iota < \Delta_{\min}}]=0$, i.e. $2\iota \geq \Delta_{\min}$, we automatically have:
    \begin{align}
        \forall w\in\cW_{\leq 2\Delta_{\min}},\quad \mI[\cE^{n,k}_w] \Delta(w) \leq 2\mI[\cE^{n,k}_w]\Delta_{\min} \leq 4\mI[\cE^{n,k}_w] \cdot (\Delta_{\min} \wedge \iota^{n,k}).\label{eq:iota_cases}
    \end{align}
    On the other hand, on the good events of Lem.~\ref{lem:formal_optism_val_est_error} and Lem.~\ref{lem:formal_val_est_error}, when $\mI[\cE^{n,k}_w\cap\cE^{n,k}_{2\iota < \Delta_{\min}}]=1$, we must have:
    \begin{align*}
        J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \iota^{k,n} \leq & \hV^{k,n}(\pi^{k,n}_\SELF;\cD^{k,n-1}{}) \tag{Lem.~\ref{lem:formal_optism_val_est_error} and Lem.~\ref{lem:formal_val_est_error}}\\
        \leq & \hV^{k,n}(\pi^*_{r^{w}};\cD^{k,n-1}{}) \tag{$w$ is chosen}\\
        \leq & J_\beta(\pi^*_{r^w}) - J_\beta(\pi_\textref) + 32\cdot e^{2{\Rmax}}\sqrt{\frac{1}{N(w;\cD^{k,n})}\log\frac{|\Pi|WT}{\delta}},
    \end{align*}
    which implies,
    \begin{align*}
        N(w;\cD^{k,n}) \leq \frac{c_1^2 e^{4{\Rmax}}}{(\Delta_{\min} - \iota^{k,n})^2}\log\frac{|\Pi|WT}{\delta} \leq \frac{4c_1^2 e^{4{\Rmax}}}{\Delta_{\min}^2}\log\frac{|\Pi|WT}{\delta}.
    \end{align*}
    Therefore,
    \begin{align*}
        \forall w\in \cW_{\leq 2\Delta_{\min}}, \quad \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w \cap \cE^{n,k}_{2\iota < \Delta_{\min}}] \Delta(w) \leq \frac{8c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}.
    \end{align*}
    %
    Combining with Eq.~\eqref{eq:iota_cases}, we have:
    \begin{align*}
        \forall w\in \cW_{\leq 2\Delta_{\min}}, \quad \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w] \Delta(w) \leq 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w] \cdot (\Delta_{\min} \wedge \iota^{n,k}) + \frac{8c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}.
    \end{align*}
    By merging the analysis for $w\in\cW_{\leq 2\Delta_{\min}}$ and $w\in\cW_{>2\Delta_{\min}}$, we have:
    \begin{align*}
        \forall w\in[W],\quad \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w] \Delta(w) \leq & \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta} + 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\Delta_{\min} \wedge \iota^{n,k},\numberthis\label{eq:reg_1}
    \end{align*}
    %
    %
    %
    %
    Note that for those $\Delta(w) \leq 4c_1 e^{2_{\max}} \cdot \sqrt{\frac{1}{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w ]}\log\frac{|\Pi|WT}{\delta}}$, we automatically have 
    %
    %
    %
    \begin{align*}
        \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w ] \Delta(w) \leq & 4 c_1 e^{2_{\max}} \cdot \sqrt{\frac{1}{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w ]}\log\frac{|\Pi|WT}{\delta}} \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w ]\\
        =&4 c_1 e^{2_{\max}} \cdot \sqrt{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\log\frac{|\Pi|WT}{\delta}}.
    \end{align*}
    On the other hand, when $\Delta(w) > 4c_1 e^{2_{\max}} \cdot \sqrt{\frac{1}{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w ]}\log\frac{|\Pi|WT}{\delta}}$, the bound in Eq.~\eqref{eq:reg_1} is tighter, since
    \begin{align*}
        \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta} \leq 4c_1 e^{2_{\max}} \sqrt{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\log\frac{|\Pi|WT}{\delta}}.
    \end{align*}
    Combining the above discussions,
    \begin{align*}
        &\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\neg\cE^{k,n}_\SELF](J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n})) = \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}}\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \mI[\cE^{n,k}_w] \Delta(w) \\
        \leq & \sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \min\{\frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}, 4c_1 e^{2_{\max}} \sqrt{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\log\frac{|\Pi|WT}{\delta}}\} \\
        & + 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\Delta_{\min} \wedge \iota^{n,k} \\
        \leq & \min\{\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}, \sum_{\substack{w\in[W] \\ \Delta(w) > 0}} 4c_1 e^{2_{\max}} \sqrt{\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\log\frac{|\Pi|WT}{\delta}}\} \tag{$\min\{a,b\} + \min\{x,y\} \leq \min\{a+x, b+y\}$}\\
        & + 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\Delta_{\min} \wedge \iota^{n,k} \\
        \leq & \min\{\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}, 4c_1 e^{2_{\max}} \sqrt{W |\cI^{\Transfer}_{\tK,\tN}|\log\frac{|\Pi|WT}{\delta}}\} \tag{Cauchy-Schwarz inequality and $\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \mI[\cE^{n,k}_w] \leq |\cI^{\Transfer}_{\tK,\tN}|$}\\
        & + 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mI[\cE^{n,k}_w]\Delta_{\min} \wedge \iota^{n,k}.\numberthis\label{eq:reg_2}
    \end{align*}
    %
    %
    %
    %
    %
    \paragraph{Merge Everything Together}
    Combining Eq.~\eqref{eq:offline_accum_gap} and Eq.~\eqref{eq:reg_2}, we have:
    \begin{align*}
        &\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n}) \\
        =& \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\cE^{k,n}_\SELF](J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n})) + \sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \mathbb{I}[\neg\cE^{k,n}_\SELF](J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n}))\\
        \leq & \min\{\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}, 4c_1 e^{2_{\max}} \sqrt{W |\cI^{\Transfer}_{\tK,\tN}|\log\frac{|\Pi|WT}{\delta}}\} \\
        & + 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \Delta_{\min} \wedge \iota^{n,k}.
    \end{align*}
    Combining the value gap for the online parts, we have:
    \begin{align*}
        & \sum_{(k,n)\in\cI_{\tK,\tN}} J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n}) \\
        \leq & C_\Online {\Rmax} e^{2{\Rmax}} \sqrt{\Complexity(\Pi)|\cI^{\Online}_{\tK,\tN}| \log^{c_0}\frac{|\Pi|\alpha T}{\delta}} + 4\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \Delta_{\min} \wedge \iota^{n,k} \\
        & + \min\{\sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \frac{16c_1^2 e^{4{\Rmax}}}{\Delta(w)}\log\frac{|\Pi|WT}{\delta}, 4c_1 e^{2_{\max}} \sqrt{W |\cI^{\Transfer}_{\tK,\tN}|\log\frac{|\Pi|WT}{\delta}}\}.
        %
    \end{align*}
    Note that for $\tK > 1$, denote $t_{\tK,\tN} := (\tK - 1)N + \tN$, we have:
    \begin{align*}
        |\cI^{\Online}_{\tK,\tN}| \leq \alpha \tK N \leq 2\alpha t_{\tK,\tN},\quad |\cI^{\Transfer}_{\tK,\tN}| \leq (1-\alpha) \tK N \leq 2(1-\alpha) t_{\tK,\tN}.
    \end{align*}
    Therefore,
    \begin{align*}
        \sum_{(k,n)\in\cI_{\tK,\tN}} J_\beta(\pi^*_{r^*}) - & J_\beta(\pi^{k,n}) = \tilde{O}\Big(\sum_{(k,n)\in \cI^{\Transfer}_{\tK,\tN}} \Delta_{\min} \wedge \iota^{n,k} +  {\Rmax} e^{2{\Rmax}} \sqrt{\alpha\Complexity(\Pi)t_{\tK,\tN}} + e^{2_{\max}} \sqrt{(1-\alpha)Wt_{\tK,\tN}} \wedge \sum_{\substack{w\in[W] \\ \Delta(w) > 0}} \frac{e^{4{\Rmax}}}{\Delta(w)}\Big),
    \end{align*}
    where in the last step, we omit the constant and logarithmic terms.
    By replacing $t_{\tK,\tN} \gets t$, $\sum_{(k,n) \in \cI_{\tK,\tN}} \gets \sum_{\tau \leq t} $, $k \gets k(\tau) := \lceil \frac{\tau}{N} \rceil$ and $n \gets n(\tau) := \tau \% N$, we finish the proof.
    

    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %


    \iffalse
    Next, we try to simplify
    \begin{align*}
        \sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \Delta_{\min} \wedge \iota^{n,k} \leq \min \{\Delta_{\min} T, \sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \iota^{n,k}\}.
    \end{align*}
    By Cauchyâ€“Schwarz inequality,
    \begin{align*}
        \sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \iota^{n,k} \leq & c_2 {\Rmax} e^{2{\Rmax}} \cdot \sqrt{\Big(\sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{k,n}} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big)^2\Big) \cdot \Big(\sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \frac{1}{|\cD^{k,n}|}\Big)} \log^{c_0}\frac{|\Pi|T}{\delta}  \\
        = & \tilde{O}\Big({\Rmax} e^{2{\Rmax}} \cdot \sqrt{\frac{\sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{k,n}} \Big)^2}{T}} \wedge \sqrt{\frac{(1-\alpha)\Complexity(\Pi)}{\alpha}} \sqrt{T})
    \end{align*}
    By merging them together, we have:
    \begin{align*}
        & \sum_{k=1}^K \sum_{1 < n\leq N} J_\beta(\pi^*_{r^*}) - J_\beta(\pi^{k,n}) \\
        \leq & \tilde{O}\Big(\min\{\Delta_{\min}(1-\alpha) T, {\Rmax} e^{2{\Rmax}} \cdot \sqrt{\sum_{\substack{1\leq k\leq K,\\ \alpha N < n \leq N}} \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{k,n}} \Big)^2}, \sqrt{\frac{(1-\alpha)\Complexity(\Pi)T}{\alpha}}\}\Big) \\
        & + \tilde{O}\Big({\Rmax} e^{2{\Rmax}}\sqrt{\alpha \Complexity(\Pi)T} + \sqrt{(1-\alpha)}e^{2{\Rmax}}\min\{\frac{e^{2{\Rmax}}W}{\Delta_{\min}}, \sqrt{WT}\}\Big)
    \end{align*}
    \fi

    %
    %
    %
    %
    %
    %
    %
    %
    %

    %

    %
    %
    %
    %
    %
    %
    %
    %

\end{proof}


