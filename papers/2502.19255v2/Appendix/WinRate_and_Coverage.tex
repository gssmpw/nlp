\section{Connection between Win Rate and Policy Coverage Coefficient}\label{appx:win_rate_and_coverage}

\begin{lemma}\label{lem:diff_and_preference}
    Given two probability vector $u, v \in \Delta(\cA)$ and a reward function $r:\cA\rightarrow\mR$, consider a preference model based on $r$, satisfying,
    \begin{align*}
        \mP_r(y=1|a,a') \geq \frac{1}{2},
    \end{align*}
    for any $a,a'\in\cA$ satisfying $r(a) \geq r(a')$. Then,
    \begin{align*}
        \sum_a \sqrt{u(a) v(a)} \leq \min_{\gamma > 0} \sqrt{(\gamma + 2\mP_r(v\succ u))\log\frac{1 + \gamma}{\gamma}},
    \end{align*}
    where $\mP_r(u \succ v) := \EE_{a\sim u,a'\sim v}[\mP_r(y=1|a, a')]$.
\end{lemma}
\begin{proof}
    We first of all sort the action space $\cA$ to $\cA_{\sorted} := \{a_1,a_2,...,a_{|\cA|}\}$ according to reward function $r$, such that, for any $1\leq i<j \leq |\cA|$, $r(a_i) \leq r(a_j)$.
    Besides, we use $F^u(\cdot)$ to denote the cumulative distribution function regarding $u$:
    $$
        \forall 1\leq i \leq |\cA|,\quad F^u(a_i) := \sum_{j=1}^{i} u(a_i),
    $$
    and $F^{v}$ is defined similarly. Then we have:
    \begin{align*}
        \sum_{a\in\cA} \sqrt{u(a)v(a)} =& \sum_{i=1}^{|\cA|} \sqrt{u(a_i)v(a_i)} = \sum_{i=1}^{|\cA|} \sqrt{\frac{u(a_i)}{\gamma + F^u(a_i)}} \cdot \sqrt{(\gamma + F^u(a_i))v(a)} \tag{Introducting a parameter $\gamma > 0$}\\
        \leq & \sqrt{\sum_{i=1}^{|\cA|} \frac{u(a_i)}{\gamma + F^u(a_i)}} \cdot \sqrt{\sum_{i=1}^{|\cA|}  (\gamma + F^u(a_i))v(a_i)} \tag{Cauchy–Schwarz inequality}\\
        \leq & \sqrt{\sum_{i=1}^{|\cA|} \frac{u(a_i)}{\gamma + F^u(a_i)}} \cdot \sqrt{\gamma + 2\mP_r(v\succ u)},
    \end{align*}
    where in the last step, we use the fact that $\gamma\sum_{i=1}^{|\cA|} v(a_i) = \gamma$ and 
    \begin{align*}
        \mP_r(v\succ u) =& \EE_{a\sim v,a' \sim u}[\mP_r(y=1|a,a')] \geq \sum_{i=1}^{|\cA|} v(a_i) \sum_{j=1}^i u(a_j) \mP_r(y=1|a_i,a_j) \geq \frac{1}{2}\sum_{i=1}^{|\cA|} v(a_i) F^u(a_i).
    \end{align*}
    For the first part, we can upper bound by the following:
    \begin{align*}
        \sum_{i=1}^{|\cA|} \frac{u(a_i)}{\gamma + F^u(a_i)} = & \sum_{i=1}^{|\cA|} \frac{u(a_i)}{\gamma + \sum_{j=1}^{i} u(a_j)} = \sum_{i=1}^A 1 - \frac{\gamma + \sum_{j=1}^{i-1} u(a_j)}{\gamma + \sum_{j=1}^{i} u(a_j)} \\
        \leq & \sum_{i=1}^A \log \frac{\gamma + \sum_{j=1}^{i} u(a_j)}{\gamma + \sum_{j=1}^{i-1} u(a_j)} \tag{$1 - x \leq \log \frac{1}{x}$} \\
        \leq & \log \frac{1+\gamma}{\gamma}
    \end{align*}
    Therefore, $\sum_{a\in\cA}\sqrt{u(a)v(a)} \leq \sqrt{(\gamma + 2\mP_r(v\succ u))\log\frac{1+\gamma}{\gamma}}$.
    Since $\gamma$ is arbitrary, we can take the minimum over $\gamma > 0$, and finish the proof.
    %
    %
    %
    %
    %
\end{proof}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{lemma}\label{lem:TV_to_Preference}
    %
    %
    %
    For any policy $\pi, \tpi$,
    \begin{align*}
        &1 - \TV(\pi(\cdot|s)\|\tpi(\cdot|s)) \leq \min_{\gamma > 0} \sqrt{(\gamma + 2\mP_{r^*}(\pi(\cdot|s) \succ \tpi(\cdot|s))) \log \frac{1 + \gamma}{\gamma}},\\
        &1 - \EE_{s\sim\rho}[\TV(\pi(\cdot|s)\|\tpi(\cdot|s))] \leq \min_{\gamma > 0} \sqrt{(\gamma + 2 \mP_{r^*}(\pi\succ \tpi)) \log \frac{1 + \gamma}{\gamma}}.
    \end{align*}
\end{lemma}
\begin{proof}
    \begin{align*}
        1 - \TV(\pi(\cdot|s)\|\tpi(\cdot|s)) \leq & 1 - \mH^2(\pi(\cdot|s)\|\tpi(\cdot|s)) = \sum_{a\in\cA} \sqrt{\pi(a|s) \tpi(a|s)} \\
        \leq & \min_{\gamma > 0}\sqrt{\Big(\gamma + 2 \mP_{r^*}(\pi(\cdot|s) \succ \tpi(\cdot|s)) \Big)\log\frac{1+\gamma}{\gamma}}.
        %
        %
        %
        %
        %
    \end{align*}
    where in the last step we apply Lem.~\ref{lem:diff_and_preference} with $\pi(\cdot|s)$ as $v(\cdot)$, $\tpi(\cdot|s)$ as $u(\cdot)$, $r^*(s,\cdot)$ as the reward function.
    %
    %
    Then, we finish the proof for the first inequality.

    For the second inequality, by taking the expectation over $s\sim\rho$ and the concavity of $\sqrt{\cdot}$ function, we have:
    \begin{align*}
        1 - \EE_{s\sim\rho}[\TV(\pi(\cdot|s)\|\tpi(\cdot|s))] \leq \min_{\gamma > 0}\sqrt{\Big(\gamma + 2\mP_{r^*}(\pi\succ\tpi)\Big) \log\frac{1 + \gamma}{\gamma}}.
    \end{align*}
    By choosing $\gamma = \mP_{r^*}(\pi\succ\tpi)$ (note that this choice ensures $\gamma > 0$ since $\tpi(\cdot|\cdot) > 0$), we finish the proof.
\end{proof}

\begin{restatable}{lemma}{LemLBCoverage}[The Complete Version of Lem.~\ref{lem:BT_LB_coverage}]\label{lem:LB_coverage_formal}
    Given any policy $\pi$, under the assumption that $\mP_{r^*}(y=1|s,a,a') \geq \frac{1}{2}$ for any $a,a'\in\cA$ satisfying $r^*(s,a) \geq r^*(s,a')$, we have:
    \begin{align}
        \cov^{\pi^*_{r^*}|\pi}\geq & \max_{\gamma > 0, \bpi} \Big(\sqrt{\gamma + 2\mP_{r^*}(\bpi\succ\pi)\log\frac{1+\gamma}{\gamma}} +\sqrt{\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\bpi)}{2\beta}}\Big)^{-1} \label{eq:LB_2}\\
        \cov^{\pi^*_{r^*}|\pi} \geq & \max_{\gamma > 0, \bpi} \Big(\sqrt{\gamma + 2\mP_{r^*}(\pi\succ\bpi)\log\frac{1+\gamma}{\gamma}}+\sqrt{\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\bpi)}{2\beta}}\Big)^{-1}. \label{eq:LB_1}
    \end{align}
    where $\bar{\pi}$ is an arbitrary intermediate policy, $\mP_{r^*}(\pi\succ \tpi) := \EE_{s\sim\rho,a\sim\pi,a'\sim\tpi}[\mP_{r^*}(y=1|s,a,a')]$ and $\mP_{r^*}(\tpi\succ \pi) = 1 - \mP_{r^*}(\pi\succ \tpi) = \EE_{s\sim\rho,a\sim\tpi,a'\sim\pi}[\mP_{r^*}(y=1|s,a,a')]$.
\end{restatable}
\begin{proof}
    We have:
    \begin{align*}
        \EE_{a\sim \pi^*_{r^*}(\cdot|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}] - 1=& \chi^2(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s)) \\
        \geq & \exp(\KL(\pi^*_{r^*}(\cdot|s) \| \pi(\cdot|s))) - 1 \tag{Theorem 5 in \citep{gibbs2002choosing}}\\
        \geq & \frac{1}{2} \cdot \frac{1}{1 - \TV(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s))} - 1 \tag{Bretagnolle–Huber inequality}.
         %
    \end{align*}
    Now, we introduce an arbitrary intermediate policy $\tpi$, 
    \begin{align*}
        \TV(\pi^*_{r^*}(\cdot|s)\|\pi(\cdot|s))
        \geq & \TV(\bpi(\cdot|s)\|\pi(\cdot|s)) - \TV(\bpi(\cdot|s)\|\pi^*_{r^*}(\cdot|s)) \tag{Reverse triangle inequality}\\
        %
        \geq & \TV(\bpi(\cdot|s)\|\pi(\cdot|s)) - \sqrt{\frac{1}{2} \KL(\bpi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))} \tag{Pinsker's inequality}.
    \end{align*}
    %
    %
    %
    %
    Applying Lem.~\ref{lem:TV_to_Preference} with $(\pi, \tpi) \gets (\pi, \pi)$, we have:
    \begin{align*}
        %
        \EE_{a\sim \pi^*_{r^*}(\cdot|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}] \geq & \frac{1}{1 - \TV(\bpi(\cdot|s)\|\pi(\cdot|s)) + \sqrt{\frac{1}{2} \KL(\bpi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))}} \\
        \geq & \frac{1}{\sqrt{(\gamma + 2\mP_{r^*}(\bpi(\cdot|s) \succ \pi(\cdot|s))) \log \frac{1+\gamma}{\gamma}} + \sqrt{\frac{1}{2} \KL(\bpi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))}}.
    \end{align*}
    By taking the expectation over $s\sim\rho$, and leveraging the convexity of $1/x$ and the concavity of $\sqrt{\cdot}$ functions, we have:
    \begin{align*}
        \EE_{s\sim\rho,a\sim \pi^*_{r^*}(\cdot|s)}[\frac{\pi^*_{r^*}(a|s)}{\pi(a|s)}] \geq& \frac{1}{\EE_{s\sim\rho}[\sqrt{(\gamma + 2\mP_{r^*}(\bpi(\cdot|s) \succ \pi(\cdot|s))) \log \frac{1+\gamma}{\gamma}}] + \EE_{s\sim\rho}[\sqrt{\frac{1}{2} \KL(\bpi(\cdot|s)\|\pi^*_{r^*}(\cdot|s))}]} \\
        \geq & \frac{1}{\sqrt{(\gamma + 2\mP_{r^*}(\bpi\succ\pi))\log\frac{1+\gamma}{\gamma}}+\sqrt{\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\pi)}{2\beta}}}.
        %
        %
    \end{align*}
    %
    Note that the above results hold for any $\gamma > 0$ and any $\bpi$, we finish the proof by taking the maximum over them.
    %

    The second inequality in Lem.~\ref{lem:LB_coverage_formal} can be proved similarly by applying Lem.~\ref{lem:TV_to_Preference} with $(\pi, \tpi) \gets (\pi, \bpi)$. All the discussion are the same.
\end{proof}

\begin{remark}\label{remark:LB_coverage}
    We provide some remarks about Lem.~\ref{lem:LB_coverage_formal}.
    \begin{itemize}
        \item Lem.~\ref{lem:BT_LB_coverage} is a direct corollary of Eq.~\eqref{eq:LB_1}.

        \item Notably, Eq.~\eqref{eq:LB_2} has a different implication compared with Eq.~\eqref{eq:LB_1} that we follow in the algorithm design in Sec.~\ref{sec:empirical_alg}.
        More concretely, Eq.~\eqref{eq:LB_2} suggests we should also disregard those source policies that strongly dominate $\bpi$ when $\bpi$ is close to $\pi^*_{r^*}$.
        This makes sense because those source policies may achieve high rewards or win rates by incurring a high KL divergence with $\pi_\textref$, and therefore, they may not provide good coverage for $\pi^*_{r^*}$.
    
        \item However, in Alg.~\ref{alg:empirical}, we intentionally do not filter out but instead prioritize source policies with exceptionally high win rates. 
        Because they likely provide good coverage for high-reward regions and can be advantageous for practical LLM training.
        Nonetheless, for completeness, we bring this theory-practice gap into attention.
    \end{itemize}
\end{remark}