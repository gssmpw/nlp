\section{Frequently Used Notation}\label{appx:freq_notations}

\begin{table}[h]
    \centering
    \def\arraystretch{1.2}
    %
    \begin{tabular}{ll}
        \hline
        \textbf{Notation} & \textbf{Description} \\
        \hline
        $\cS,\cA$ & State space and action space \\
        $\rho$ & Prompt distribution (initial state distribution) \\
        $r$ & Reward model \\
        $r^*$ & Ground-truth reward model (reflecting human preferences) \\
        $\mP_r(y|s,a,a')$ & Preference under $r$ \\
        $\mP_r(\pi\succ\tpi)$ & Win rate of $\pi$ over $\tpi$ under $r$ \\
        $\{r^w\}_{w\in[W]}$ & Imperfect source reward model \\
        $\pi$ & LLM policy \\
        $\pi^t_\mix$ & \makecell[tl]{Uniform mixture policy $\frac{1}{t}\sum_{i\leq t}\pi^i$ of a policy sequence $\pi^1,...,\pi^t$. \\ Sometimes, given a dataset $\cD=\{(x^i,\pi^i)\}_{i\leq |\cD|}$, with a bit abuse of notation, \\ we use $\pi^\cD_\mix$ to refer the mixture policy $\frac{1}{|\cD|}\sum_{i\leq|\cD|} \pi^i$.} \\
        $\cov^{\tpi|\pi}$ & Coverage coefficient \\
        %
        $\Pi$ & The policy class \\
        $\cR^\Pi$ & The reward function class converted from $\Pi$, see Appx.~\ref{appx:extend_prelim}\\
        $\conv(\Pi)$ & Convex hull of $\Pi$ \\
        $\beta$ & Regularization coefficient in RLHF objective \\
        $J_\beta(\cdot)$ & Regularized policy value (Eq.~\eqref{eq:rlhf_obj})\\
        $\Delta(w)$ & Value gap for $\pi^*_{r^w}$, i.e. $J_\beta(\pi^*_{r^*}) - J_\beta(\pi^*_{r^w})$\\
        $\Delta_{\min}$ & Minimal value gap $\min_{w\in[W]} \Delta(w)$ \\
        $a \wedge b$ & $\min\{a,b\}$ \\
        $[n]$ & $\{1,2,...,n\}$ \\
        $O(\cdot),\Omega(\cdot),\Theta(\cdot),\tilde{O}(\cdot),\tilde{\Omega}(\cdot),\tilde{\Theta}(\cdot)$ & Standard Big-O notations, $\tilde{(\cdot)}$ omits the log terms.\\
        \hline
        
    \end{tabular}
\end{table}

For completeness, we provide the definition of convex hull here.
\begin{definition}[Convex Hull]\label{def:convex_hull}
    Given a policy class $\Pi$ with finite cardinality (i.e. $|\Pi| < +\infty$), we denote $\conv(\Pi)$ as its convex hull, such that, $\forall n \in [\mN^*],~\forall \lambda^1,...,\lambda^n \geq 0$ with $\sum_{i=1}^n \lambda^i = 1$, and any $\pi^1,...,\pi^n \in \Pi$, we have:
    \begin{align*}
        \sum_{i=1}^n \lambda^i \pi^i \in \conv(\Pi).
    \end{align*}
\end{definition}


\begin{remark}
    Note that in the contextual bandit setting, the state action density induced by a policy and the policy distribution collapse with each other.
    Therefore, given a policy sequence $\pi^1,...,\pi^t$, the uniform mixture policy $\pi_\mix^t(\cdot|\cdot) = \frac{1}{t}\sum_{i\leq t} \pi^t(\cdot|\cdot)$ is directly a valid policy as a mapping from $\cS$ to $\Delta(\cA)$, which induces the state-action density $\pi_\mix^t(\cdot|\cdot)$.

    Besides, we will use $\Online$ and $\Offline$ as abbreviations of ``online learning'' and ``offline learning'', respectively.
    %
\end{remark}