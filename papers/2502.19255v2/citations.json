[
  {
    "index": 0,
    "papers": [
      {
        "key": "xu2020preference",
        "author": "Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur",
        "title": "Preference-based reinforcement learning with finite-time guarantees"
      },
      {
        "key": "novoseller2020dueling",
        "author": "Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel",
        "title": "Dueling posterior sampling for preference-based reinforcement learning"
      },
      {
        "key": "pacchiano2021dueling",
        "author": "Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan",
        "title": "Dueling rl: reinforcement learning with trajectory preferences"
      },
      {
        "key": "du2024exploration",
        "author": "Du, Yihan and Winnicki, Anna and Dalal, Gal and Mannor, Shie and Srikant, R",
        "title": "Exploration-driven policy optimization in rlhf: Theoretical insights on efficient data utilization"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ye2024theoretical",
        "author": "Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong",
        "title": "A theoretical analysis of nash learning from human feedback under general kl-regularized preference"
      },
      {
        "key": "chen2022human",
        "author": "Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei",
        "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation"
      },
      {
        "key": "wang2023rlhf",
        "author": "Wang, Yuanhao and Liu, Qinghua and Jin, Chi",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "key": "cen2024value",
        "author": "Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      },
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      },
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhan2023provable",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen",
        "title": "Provable offline preference-based reinforcement learning"
      },
      {
        "key": "liu2024provably",
        "author": "Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran",
        "title": "Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer"
      },
      {
        "key": "huang2024correcting",
        "author": "Huang, Audrey and Zhan, Wenhao and Xie, Tengyang and Lee, Jason D and Sun, Wen and Krishnamurthy, Akshay and Foster, Dylan J",
        "title": "Correcting the mythos of kl-regularization: Direct alignment without overparameterization via chi-squared preference optimization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chang2024dataset",
        "author": "Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Brantley, Kiant{\\'e} and Misra, Dipendra and Lee, Jason D and Sun, Wen",
        "title": "Dataset reset policy optimization for rlhf"
      },
      {
        "key": "gao2024rebel",
        "author": "Gao, Zhaolin and Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Swamy, Gokul and Brantley, Kiant{\\'e} and Joachims, Thorsten and Bagnell, J Andrew and Lee, Jason D and Sun, Wen",
        "title": "Rebel: Reinforcement learning via regressing relative rewards"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "taylor2009transfer",
        "author": "Taylor, Matthew E and Stone, Peter",
        "title": "Transfer learning for reinforcement learning domains: A survey."
      },
      {
        "key": "zhu2023transfer",
        "author": "Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K and Zhou, Jiayu",
        "title": "Transfer learning in deep reinforcement learning: A survey"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mann2013directed",
        "author": "Mann, Timothy A and Choe, Yoonsuck",
        "title": "Directed exploration in reinforcement learning with transferred knowledge"
      },
      {
        "key": "huang2022tiered",
        "author": "Huang, Jiawei and Zhao, Li and Qin, Tao and Chen, Wei and Jiang, Nan and Liu, Tie-Yan",
        "title": "Tiered reinforcement learning: Pessimism in the face of uncertainty and constant regret"
      },
      {
        "key": "huang2023robust",
        "author": "Huang, Jiawei and He, Niao",
        "title": "Robust Knowledge Transfer in Tiered Reinforcement Learning"
      },
      {
        "key": "golowich2022can",
        "author": "Golowich, Noah and Moitra, Ankur",
        "title": "Can Q-learning be improved with advice?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2024reuse",
        "author": "Wu, Zhaofeng and Balashankar, Ananth and Kim, Yoon and Eisenstein, Jacob and Beirami, Ahmad",
        "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment"
      },
      {
        "key": "hong2024cross",
        "author": "Hong, Jiwoo and Lee, Noah and Mart{\\'\\i}nez-Casta{\\~n}o, Rodrigo and Rodr{\\'\\i}guez, C{\\'e}sar and Thorne, James",
        "title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      },
      {
        "key": "ji2023ai",
        "author": "Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others",
        "title": "Ai alignment: A comprehensive survey"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "nguyen2024laser",
        "author": "Nguyen, Duy and Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit",
        "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits"
      }
    ]
  }
]