
\section{From Theory to an Empirical Algorithm}\label{sec:empirical_alg}
In terms of computational overheads, $\TPO$ requires solving multiple minimax optimization problems, which restricts its applicability to fine-tune LLMs in practice.
To address this, adhering to the design principles of $\TPO$, we introduce a more computationally efficient alternative in Alg.~\ref{alg:empirical}.

\textbf{Key Insight: Estimating Win Rates instead of Policy Values}~
As discussed in Sec.~\ref{sec:main_theory}, several optimization steps are designed to estimate policy values used for transfer policy selection, because they help to identify the policies' coverability for optimal policy (i.e. $\cov^{\pi^*_{r^*}|\cdot}$).
The key insight in our empirical algorithm design is to \emph{find a more accessible indicator to infer $\cov^{\pi^*_{r^*}|\cdot}$}.
This leads us to the policy win rates, i.e., the probability that human prefer the generation by one policy over another. Formally, given two policies $\pi, \tpi$, the win rate of $\tpi$ over $\pi$ is defined by:
$
    \mP_{r^*}(\tpi \succ \pi) := \EE_{s\sim\rho,a\sim\tpi,a'\sim\pi}[\mP_{r^*}(y=1|s,a,a')].
$

Win rates between two policies can be unbiasedly estimated by querying human preferences with their generated responses.
Moreover, win rates can be used to construct a lower bound for $\cov^{\pi^*_{r^*}|\cdot}$, as stated in Lem.~\ref{lem:BT_LB_coverage} below.
\begin{lemma}\label{lem:BT_LB_coverage}
    Under BT-model\footnote{
        Lem.~\ref{lem:BT_LB_coverage} can be generalized beyond BT-model.
        Besides, it is possible to construct a lower bound involving $\mP_{r^*}(\bpi\succ\pi)$ instead.
        See Lem.~\ref{lem:LB_coverage_formal} and Remark~\ref{remark:LB_coverage} in Appx.~\ref{appx:win_rate_and_coverage} for more details.
        }, for any $\pi$:
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    $\displaystyle\cov^{\pi^*_{r^*}|\pi}\geq \!$$
    \displaystyle\max_{\gamma > 0, \bpi} $$({\sqrt{(\gamma\! +\! 2\mP_{r^*}(\pi\!\succ \!\bpi))  \log \frac{1+\gamma}{\gamma}} + \sqrt{\frac{J_\beta(\pi^*_{r^*}) - J_\beta(\bar{\pi})}{2\beta}}})^{-1}.$
    %
    %
    %
\end{lemma}
Note that we may not identify the policy with the best coverage for $\pi^*_{r^*}$ through the lower bound above.
However, it still provides useful guidance for practice: we can filter out policies yielding high lower bound.
In Lem.~\ref{lem:BT_LB_coverage}, for any fixed $\gamma$ and comparator $\bpi$, the lower bound for $\cov^{\pi^*_{r^*}|\pi}$ increases as $\mP_{r^*}(\pi \succ \bpi)$ decay to 0, suggesting prioritizing transferring from policies with high win rates.

The key question now is how to choose the comparator $\bpi$. According to Lem.~\ref{lem:BT_LB_coverage}, ideally, the comparator should be close to $\pi^*_{r^*}$, so that $J_\beta(\pi^*_{r^*}) - J_\beta(\bar{\pi})$ becomes negligible, allowing the win rate term to dominate the lower bound.
Since we do not know $\pi^*_{r^*}$ in advance, empirically, we can choose the learning policy as the comparator, which is optimized and progressively converges to $\pi^*_{r^*}$.
%
%
%

\textbf{From Insights to Practice}~
Next, we walk through empirical $\TPO$ in Alg.~\ref{alg:empirical} and explain how we integrate these insights into the algorithm design.
Alg.~\ref{alg:empirical} utilizes an iterative online learning framework, which repeatedly collects online data and optimizes the policy.
We start by initializing the online learning policy $\pi^1_\base$ with the reference policy $\pi_\textref$.
For computational efficiency, in each iteration $k$, we avoid separately computing online exploration policies and self-transfer learning policies as done in $\TPO$. Instead, we only compute one policy $\pi^k_\base$ (updated from $\pi^{k-1}_\base$) by $\text{Alg}_{\text{PO}}$. Here $\text{Alg}_{\text{PO}}$ is a placeholder for an arbitrary \textbf{P}olicy \textbf{O}ptimization algorithm, and we do not restrict the concrete choice.
Such a design increases the modularity of our empirical TPO, making it possible to combine with various policy optimization methods and enhance their performance.
For example, $\text{Alg}_{\text{PO}}$ may be instantiated by DPO \citep{rafailov2024direct}, resulting in a transfer learning framework built on iterative-DPO  \citep{xiong2024iterative, yuan2024self}.
Besides, one may consider other advanced (online) methods, such as XPO \citep{xie2024exploratory}, IPO \citep{azar2024general}, etc.

As the core ingredients of our empirical TPO, during data collection, the algorithm selects the policy $\pi^{k,n} \in \{\pi^*_{r^*}\}_{w=1}^W \cup \{\pi^k_\base\}$ with the highest win rate when competing against $\pi^k_\base$.
Intuitively, we encourage transfer learning if $\{\pi^*_{r^*}\}_{w=1}^W$ includes high-quality candidates; otherwise, the algorithm conducts standard iterative policy optimization with $\text{Alg}_{\text{PO}}$ by default.
This strategy also aligns with the heuristic principle: \textbf{\emph{learn from an expert until surpassing it}}.
Lastly, since the win rates are unknown in advance, the selection process is formulated as a multi-armed bandit problem. We employ a UCB subroutine (line~\ref{line:UCB}) to balance the exploration and exploitation during the win rates estimation.
%
%



%
%
%
%
%
%
%
%

%
%
%
%
%




%
%

%

%
%
%
%

%
%

%
%
%
%
%
%


%

%
%

%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
    
%
%
%
%
%
%

%
%
%

%
%
%
%

%
%
%
%
%


%
%

%

%
%
%

%

%
%
%

\iffalse
\subsubsection{Theoretical Investigation}
\paragraph{No-regret behaviors by EXP3}
We use vector $l^k := \{\Pr(\pi^*_{r^w} \succ \pi^k|r^*)\}_{w=1}^W \cup \{0.5\}$ to denote the probability of win rates.
\begin{align*}
    \max_{p\in\Delta([0,1]^{W+1})} \EE[\sum_{b=1}^B \langle p - p^{k,n}, l^k \rangle] = o(B).
\end{align*}
Suppose there exists $w\in[W]$, such that $\Pr(\pi^*_{r^w} \succ \pi^k|r^*) \leq 0.5 - \xi$, consider the vector $p$, such that $p(w') = \frac{1}{B} \sum_{b=1}^B p^{k,n}(w')$ for $w'\not\in\{w,W+1\}$ and $p(w) = 0$ and $p(W+1) = \frac{1}{B} \sum_{b=1}^B p^{k,n}(w) + p^{k,n}(W+1)$, we have:
\begin{align*}
    \xi \cdot \sum_{b=1}^B p^{k,n}(w)  \leq o(B).
\end{align*}
%
%
%
%
\paragraph{Turning back to ``self-transfer learning'' Eventually}
As the training iteration increases, the learned policy will gradually converge to the true policy. In theory, the value gap reflects the KL divergence, which can be used to control the coverage coefficient, by the Lem.~\ref{lem:upper_bound_coverage}:
As a result, for any fixed soruce tasks, as long as they do not cover the true optimal policy (non-zero reward gap), the online policy (which continuously getting improved) would be a better choice to cover the optimal policy.

On the one hand, it suggests that we should give up all the source tasks eventually (which also makes sense).
On the other hand, as a by-product, such results reflects a way to improve the basic gaurantees by XPO, because of the ``self-transfer'' learning.

\red{
TODO: how to understand the benefits of the transfer learning in the early stages? 
Would our theorems still useful in some sense? using preference as an indicator of the coverage coefficient.
}

In some sense, as the policy improves, the learning policy will beat the others, so we will eventually turn to this mode. However, there is no guarantee, if there are some policy which achieves high KL divergence, even if the reward value is high.


\paragraph{}
From Lem.~\ref{lem:upper_bound_coverage}, we know that, a meaningful source task selection technique is to find the policy which results in the lowest policy gap.

We can just treat $J_\beta(\pi)$ as reward function of the bandit, and trying to estimate it.
Note that, we can get access to the entire policy, so the KL value can be easily computed.
If we can get access to the reward of the responses, the expected return part can be unbiasedly estimated, then, it is a standard bandit setting with stochastic feedback.

However, in the worst case, we can only get access to preference-based feedback, and we need to estimate the reward from it.

%
%
%
%
%
%
%
%

%
%


\red{In practice, computing the $\hat{r}$ can be chanllenging (?)}, so we use win rate instead. Or maybe we can just use $\log \pi$ as the estimated reward?
\fi

\iffalse
\newpage
\subsection{A Refined Reward Models Selection Algorithm}
In the second algorithm, we do a refined state-wisely reward model selection, which requires additional estimation of the performance for each state.
We introduce another scalar function class $\cF$, such that $\forall, f\in\cF$, $f:\cS\times\cA \rightarrow [0,1]^W \cup \{\frac{1}{2}\} \in \cR^{W+1}$. $\cF$ will be used to approximate the success rate for $\pi^*_{r^w}$ in each state.

\begin{algorithm}[h]
    \textbf{Input}: Iteraction number $K$; Batch size $B$; Imperfect reward models $\{r^w\}_{w\in[W]}$; Hyper-parameter $\alpha$ and $\eta$; Scalar function class $\cF$\\
    $\cD \gets \emptyset$; Initialize $p^0(\cdot|\cdot) = (\frac{1}{W+1},\frac{1}{W+1}...,\frac{1}{W+1})\in\mR^{W+1}$.\\
    \For{$k=1,...,K$}{
        $\pi^k \gets \texttt{Online-Alg}(\cD)$; \\
        \For{$b=1,..., B$}{
            Sample $s^{k,n}\sim\rho$. \\
            \textbf{If} $b \leq \alpha B$ \textbf{then} $w^{k,n} \gets 0$  
                \textbf{else} $w^{k,n} \sim p^k(\cdot|s^{k,n})$ \\
            Sample responses $(a^{k,n},\tilde{a}^{k,n}) \sim \pi^*_{r_{w^{k,n}}}(\cdot|s^{k,n}) \times \pi^k(\cdot|s^{k,n})$. \\
            Query the human preference $y^{k,n} \sim \Pr(a^{k,n} \succ \tilde{a}^{k,n}|s^{k,n})$. \\
                $\cD \gets \cD \cup \{(s^{k,n},a^{k,n},\tilde{a}^{k,n},y^{k,n})\}$ 
        }
        \blue{// train $f^k(\cdot|s^{k,n})$ to predict $\Pr(\pi^*_{r^w}\succ \pi^k|s^{k,n})$.} \\
        \blue{// Here $\texttt{CE}(y,p) := y\log p + (1-y) \log(1-p)$} \\
        %
        $\forall w\in[W],~ f^k_w \gets \argmin_{f\in\cF} \sum_{b= \alpha B + 1}^B \mathbb{I}[w^{k,n} = w] \texttt{CE}(y^{k,n}, f(\cdot|s^{k,n}))$ \\
        Update $p^k(w|\cdot) \propto p^{k-1}(w|\cdot) \cdot \exp(\eta \cdot f^{k}_w(\cdot))$ \\
    }
    \Return $\hat{\pi}^*_{r^*} \gets \texttt{Offline-Alg}(\cD)$.
    \caption{A Refined Reward Models Selection Algorithm}
\end{algorithm}

\fi