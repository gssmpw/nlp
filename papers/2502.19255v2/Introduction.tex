\section{Introduction}\label{sec:intro}
Reinforcement Learning from Human Feedback (RLHF) has achieved remarkable success in fine-tuning Large-Language Models (LLMs) to align with human preferences \citep{christiano2017deep,bai2022training,ouyang2022training}.
Using datasets annotated with human preferences reflecting human intrinsic reward model, RLHF optimizes LLM policies with reinforcement learning (RL) techniques.
Due to the high cost of collecting large amounts of human preference labels, there has been significant attention in reducing the \emph{sample complexity}---the amount of data required for training---of online RLHF through efficient exploration strategies \citep{wang2023rlhf,xie2024exploratory,cen2024value,zhang2024self}.
However, a largely overlooked opportunity is to additionally leverage existing reward models for annotation, which have already aligned partially with the human preferences.
The growing number of available open-source and high-quality reward models trained on diverse tasks provide a rich pool of candidates for transfer learning.
Harnessing guidance embedded in such reward models holds great potential for improving sample efficiency.
%
%
%
%

There are a variety of practical scenarios where such source reward models can be effectively utilized.
%
Firstly, reward models trained on relevant tasks often prove to be valuable in similar tasks.
A notable example is cross-lingual reward transfer \citep{wu2024reuse, hong2024cross}, where reward models in one language can provide effective guidance for tasks in another.
%
Secondly, informative evaluation can also be obtained from well-trained LLMs, such as GPT, LLaMA and Gemini \citep{achiam2023gpt,dubey2024llama,team2024gemini}. For certain tasks, such models can provide evaluation closely aligned with human preferences; see e.g., \citet{lee2023rlaif,ji2023ai}.
Lastly, there are scenarios where rule-based or heuristic reward functions---built upon experts knowledge and accumulated experience---are inexpensive to obtain and instructive in evaluating the LLM.
Taking summarization tasks as an example, expert summaries are available on datasets such as XSum \citep{Narayan2018DontGM} and TL-DR \citep{volske-etal-2017-tl}. Similarity with those expert solutions can be measured through metrics such as ROUGE \citep{lin2004rouge} and BERTScore \citep{zhang2019bertscore}, and be employed for scoring the LLM generations.
%
%
%

%
%

%
%
%
%
%
%
%
%

Motivated by these considerations, this paper studies how imperfect reward models can be used to learn a near-optimal policy with fewer human annotations. We consider the case where several source reward models are available, yet their quality, i.e., the similarity to human rewards, is \emph{unknown a priori}.
%
%
%
%
%
%
%
%
%
Our contributions are summarized as follows.
%
\begin{enumerate}[leftmargin=0.2cm, itemsep=0.3pt]
\item[\textbullet] In Sec.~\ref{sec:transfer_coverage_perspective}, we identify a distinctive property of RLHF arising from its KL regularization: \emph{for any prospective policy candidates, its coverability for the optimal policy improves as its policy value increases}.
Enlightened by this, we propose two new principles for transfer learning in the context of RLHF (illustrated in Fig.~\ref{fig:outline}).

Firstly, policy value serves as a crucial criterion to select the transfer policy, since exploiting policies with high values does not conflict with exploration.
Secondly, combining insights from offline RL theory, we prove that the policy distilled by offline learning techniques from the online data generated by any no-regret online algorithm, converges to the optimal one at a rate of $\tilde{O}(T^{-\frac{1}{2}})$ after a finite time.
Such a bound improves existing sample complexity results and regret bounds in online RLHF by eliminating the dependencies on the size of the state and action space, or the complexity of the policy class (up to log-covering number).
This suggests the offline policy computed with the data from the online learning process is a promising candidate to transfer from, which we term as ``self-transfer learning''.

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%


%
%
%
%

%
%
%
%

%
%

%
%
%
%


\item[\textbullet] In Sec.~\ref{sec:main_theory}, following above principles, we design a transfer learning algorithm named \textbf{T}ransfer \textbf{P}olicy \textbf{O}ptimization ($\TPO$; Alg.~\ref{alg:main_algorithm}) with provable benefits.
At the core of $\TPO$ is the self-transfer learning and an adaptive policy selection strategy that picks transfer policies based on estimated value gaps.
In the early stage, the cumulative online regret of $\TPO$ can be significantly reduced, as long as one of the source rewards is of high quality.
After finite time, the self-transfer mechanism ensures the regret of $\TPO$ grows only at a rate of $\tilde{O}(\sqrt{T})$, independent of the standard structural complexity measures. 
Compared with transfer learning in the pure-reward maximization RL, our result is novel in that it exploits the policy coverage property induced by the regularization term in RLHF.


\item[\textbullet] To reduce computational overheads and improve scalablity to practical RLHF scenarios, in Sec.~\ref{sec:empirical_alg}, we further propose an empirical algorithm that selects transfer policy based on the win rates competing with the learning policy, rather than relying on the estimated policy values.
On the one hand, win rates can be estimated much more efficiently than the policy values. On the other hand, as justified by our theory, win rates help to identify \emph{lower bounds for the coverability of the optimal policy}.
Notably, our empirical TPO is general: its core transfer learning technique can be modularized and combined with various reward-model-free policy optimization methods, such as DPO~\citep{rafailov2024direct}, IPO~\citep{azar2024general}, XPO~\citep{xie2024exploratory}, to boost their performance.
The effectiveness of our approach is demonstrated in fine-tuning T5 models on summarization tasks in Sec.~\ref{sec:experiment}.

%
%
%
%
%
%

\end{enumerate}

%
%
%
%
%
%
%
%


\subsection{Related Work}
%

We only summarize the most relevant results on sample complexity and transfer RL here due to space limitation. Other closely related topics can be found in Appx.~\ref{appx:related_workds}.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.55]{Pictures/Outline.pdf}
    \caption{The standard online RLHF pipeline only involves learning from online human feedback (left).
    %
    Our setting \emph{additionally} leverages available imperfect reward models via transfer learning (right).
    Inspired by the structure induced by KL regularization, we propose novel principles for transfer learning in online RLHF: (1) selecting transfer policy $\pi_\text{Transfer}$ with the highest policy value; (2) self-transfer learning---involving as a candidate the policy $\pi_\SELF$ \emph{distilled} from online collected data by offline learning techniques.
    }\label{fig:outline}
\end{figure}
\textbf{Sample Complexity in RLHF}~
Online RLHF emphasizes strategic exploration for sample-efficient learning in tabular and linear settings \citep{xu2020preference, novoseller2020dueling, pacchiano2021dueling, du2024exploration}, as well as more general function approximation cases \citep{ye2024theoretical, chen2022human, wang2023rlhf,xie2024exploratory,cen2024value,zhang2024self,xiong2024iterative}.
%
Our work further improves sample efficiency by leveraging imperfect reward models that are readily available in a variety of practical scenarios. 
%
%
%
As an alternative, offline RLHF \citep{zhan2023provable, liu2024provably, huang2024correcting} focuses on exploiting pre-collected datasets without exploration.
%
%
What lies in between online/offline RL is hybrid RL \citep{chang2024dataset, gao2024rebel}.
These methods harness online feedback, while assuming the reference policy provides good coverage and only engaging in passive exploration.


\textbf{Transfer Learning in RL and RLHF}~
Transfer learning in pure-reward maximization RL has been extensively investigated in previous literature \citep{taylor2009transfer, zhu2023transfer}, and theoretical guarantees have been established under various conditions \citep{mann2013directed, huang2022tiered,huang2023robust, golowich2022can}. Unlike previous works, this paper unveils new insights for transfer learning enabled by the KL regularization in RLHF. In particular, it enables us to design a policy-value-based transfer policy selection strategy, and identify a unique regime, i.e., ``self-transfer learning'', that can significantly improve sample efficiency.
We defer more discussions to Sec.~\ref{sec:new_insights}.
%
%
%

Most works on transfer learning in RLHF focus on empirical approaches.
For example, \citep{wu2024reuse, hong2024cross} investigate the cross-lingual reward transfer.
To our knowledge, our empirical algorithm (Alg.~\ref{alg:empirical}) is novel in that it studies active transfer policy selection, which is still underexplored in existing literature.
%
We further distinguish our work from RLAIF~\citep{lee2023rlaif,ji2023ai} or reward model selection literature \citep{nguyen2024laser}. Their goal is to align LLM policies with surrogate reward models, while we study how to leverage those surrogates to accelerate the alignment with ground-truth human rewards.