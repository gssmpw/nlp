\section{Provably Efficient Transfer Learning}\label{sec:main_theory}
In this section, we develop provably efficient transfer learning algorithms based on the principles in Sec.~\ref{sec:new_insights}.

\textbf{Outline of Main Algorithm}~
Our main algorithm $\TPO$, short for \textbf{T}ransfer \textbf{P}olicy \textbf{O}ptimization, is provided in Alg.~\ref{alg:main_algorithm}, which leverages Alg.~\ref{alg:transfer_policy_computing} ($\TPS$, short for \textbf{T}ransfer \textbf{P}olicy \textbf{S}election) as a subroutine to select source policies to transfer from.
$\TPO$ can be regarded as a mixture of standard online learning and transfer learning, balanced through a hyper-parameter $\alpha \in (0, 1)$.
Motivated by the implication of Thm.~\ref{thm:general_val_gap}, $\TPO$ returns the offline policy computed with all the data collected.
For convenience, we divide the total number of iterations $T$ into $K=T/N$ blocks, each containing $N$ sub-iterations.
In each block, we first run $\alpha N$ iterations of an \textbf{{O}}n\textbf{{L}}ine learning algorithm $\AlgOnline$, followed by $(1-\alpha)N$ iterations of transfer learning with policy selected by Alg.~\ref{alg:transfer_policy_computing}.
Here, $\AlgOnline$ can be any online algorithm with per-step no-regret guarantees, for example, $\XPO$ in Lem.~\ref{lem:online_RLHF}.
To save space, we defer to Appx.~\ref{appx:online_oracle} the formal behavior assumption on $\AlgOnline$ (Def.~\ref{def:online_oracle}) and concrete examples with verifications.

Now we are ready to take a closer look at the transfer policy selection steps in Alg.~\ref{alg:transfer_policy_computing} in Sec.~\ref{sec:alg_explanation}, after which, the provable merits of proposed approach are showcased in Sec.~\ref{sec:alg_main_results}.
\begin{algorithm*}[t]
    \textbf{Input}: Block size $N$; Number of blocks $K = T / N$; $\{r^w\}_{w=1}^W$; $\Pi$; $\alpha \in (0,1)$; $\delta\in(0,1)$ \\
    For all $(k,n)$, 
    $\cD^{k,n}$ denotes all the data collected up to $(k,n)$, and $\cD^{k,n}_\Online$ only includes those collected by $\AlgOnline$. See detailed definitions in Appx.~\ref{appx:main_alg_details}.\\
    \For{$k=1,2...,K$}{
        %
        \For{$n=1,...,N$}{
            \lIf{$n \leq \alpha N$}{
                $\pi^{k,n} \gets \AlgOnline(\alpha T,\Pi,\delta;\cD^{k,n}_\Online)$\label{line:online_learning}
            }
            \lElse{
                ~$\pi^{k,n} \gets \TPS(T,\Pi,\delta,\{r^w\}_{w=1}^W;\cD^{k,n})$\label{line:transfer_learning}
            }
            %
            Collect data $(s^{k,n}, a^{k,n},\ta^{k,n}, y^{k,n}) \sim \rho\times\pi^{k,n}\times\pi_\textref\times\mP_{r^*}(\cdot|\cdot,\cdot,\cdot)$. 
        }
        %
        %
        %
        %
    }
    \Return $\hat{\pi}^*_{r^*}$ computed by $\RPO$ with $\cD^{K,N+1}$.
    \caption{\textbf{T}ransfer \textbf{P}olicy \textbf{O}ptimization (\TPO)}\label{alg:main_algorithm}
\end{algorithm*}

%
%


\subsection{Details for Alg.~\ref{alg:transfer_policy_computing}: The Transfer Policy Selection}\label{sec:alg_explanation}
%
%
%
%
%

%
%

%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
The design of Alg.~\ref{alg:transfer_policy_computing} follows the two principles in Sec.~\ref{sec:new_insights}, which are: 
%
%
%
%
(1) transfer the policy with the highest (estimated) policy value, because higher policy value implies better coverage for $\pi^*_{r^*}$;
(2) include the self-transfer policy as a candidate, because it progressively converges to $\pi^*_{r^*}$ at a faster rate than the best-known ones for online policies.

Before diving into details, we first clarify some notation.
Given a dataset $\cD:=\{(s^i,a^i,\ta^i,y^i,\pi^i)\}_{i=1}^{|\cD|}$, $L_{\cD}(r)$ denotes the average negative log-likelihood (NLL) loss regarding the reward model $r$:
\begin{align*}
        L_{\cD}(r) := & \frac{1}{|\cD|}\sum_{i \leq |\cD|} -y^i \log \sigma\Big(r(s^i,a^i) - r(s^i,\ta^i)\Big) \\
        & - (1 - y^i) \log \sigma\Big(r(s^i,\ta^i) - r(s^i,a^i)\Big). \numberthis\label{eq:def_likelihood}
    %
    %
    %
\end{align*}
We will use $\EE_{\rho, \pi}[r] := \EE_{s\sim\rho, a\sim\pi}[r(s,a)]$ as a short note.
In Line~\ref{line:counter_N} of Alg.~\ref{alg:transfer_policy_computing}, $N(w;\cD) := \sum_{i\leq|\cD|} \mI[\pi^i = \pi^*_{r^w}]$ denotes the number samples collected with $\pi^*_{r^w}$ in the dataset, following the convention that $1/N(\cdot,\cdot) = +\infty$ if $N(\cdot,\cdot)=0$.
In Line~\ref{line:RPO}, we leverage $\RPO$ \citep{liu2024provably} to compute the self-transfer policy $\pi_\SELF$ and a reward function $\hr_\SELF$.
To save space, we defer the details of $\RPO$ to Appx.~\ref{appx:adaption_offline}.
%
%
%
%

\begin{algorithm*}[t]
    \textbf{Input}: Source tasks $\{r^w\}_{w=1}^W$; Policy class $\Pi$; $T$, $\delta$; Dataset $\cD:=\{(s^i,a^i,\ta^i,y^i,\pi^i)\}_{i\leq|\cD|}$. \\
    // \blue{Optimistic estimation for $J_\beta(\pi^*_{r^w}) - J_\beta(\pi_\textref)$.} \\
    $\hr_\MLE \gets \argmin_{r\in\cR^\Pi} L_{\cD}(r)$. \label{line:MLE}\\
    $\forall w\in [W],~\hat{V}(\pi^*_{r^w};\cD) \gets \EE_{\rho,\pi^*_{r^w}}[\hr_\MLE] - \EE_{\rho,\pi_\textref}[\hr_\MLE] - \beta \KL(\pi^*_{r^w}\|\pi_\textref) + 16 e^{2{\Rmax}} \sqrt{\frac{1}{N(w;\cD)} \log\frac{|\Pi|WT}{\delta}}.$ \label{line:counter_N}\\
    %
    // \blue{Pessimistic estimation for $J_\beta(\pi_\SELF) - J_\beta(\pi_\textref)$.} \\
    $\pi_\SELF, \hr_\SELF \gets \RPO(\conv(\Pi),\cR^{\Pi},\cD,\eta)$ with $\eta = c\cdot (1+e^{{\Rmax}})^{-2} \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}.$
    %
    \label{line:RPO}\\
    $\hV(\pi_\SELF;\cD) \gets \EE_{\rho,\pi_\SELF}[\hr_\SELF] - \EE_{\rho,\pi_\textref}[\hr_\SELF] - \beta \KL(\pi_\SELF\|\pi_\textref) + \frac{1}{\eta} L_{\cD}(\hr_\SELF) - \frac{1}{\eta} L_{\cD}(\hr_\MLE) - 2c e^{2{\Rmax}} \sqrt{\frac{1}{|\cD|}\log\frac{|\Pi|T}{\delta}}.$ \label{line:LB_pi_value}\\
    %
    \Return $\argmax_{\pi \in \{\pi^*_{r^w}\}_{w\in[W]} \cup \{\pi_\SELF\}} \hat{V}(\pi;\cD) $. // \blue{Selecting Transfer Policy by Estimated Value}
    \caption{\textbf{T}ransfer \textbf{P}olicy \textbf{S}election (\TPS)}\label{alg:transfer_policy_computing}
    %
\end{algorithm*}

Next, we explain our value estimation strategy.
Note that in RLHF setting, we cannot access $r^*$ directly but only the preference comparison samples following the BT model.
Thus, we instead estimate the value gain relative to $J_\beta(\pi_\textref)$.

\textbf{Optimistic Estimation for} $J_\beta(\pi^*_{r^w}) - J_\beta(\pi_\textref)$~
For policies induced by imperfect source reward models, we adopt UCB-style optimistic policy evaluation to efficiently balance exploration and exploitation.
Intuitively, by utilizing the MLE reward estimator $\hr_\MLE$, the estimation error $\hr_\MLE - r^*$ under the distribution of $\pi^*_{r^w}$ is related to the number of samples from $\pi^*_{r^w}$ occurring in the dataset. Therefore, we can quantify the value estimation error as follows.
%
\begin{restatable}[Value Est Error for $\{\pi^*_{r^w}\}_{w\in[W]}$]{lemma}{LemOptismValErr}\label{lem:formal_optism_val_est_error}
    Under Assump.~\ref{assump:policy} and Def.~\ref{def:online_oracle}, w.p. $1-\delta$, in each call of Alg.~\ref{alg:transfer_policy_computing}:
    \begin{align*}
        \forall w\in[W],\quad & J_\beta(\pi^*_{r^w}) - J_\beta(\pi_\textref) \leq \hat{V}(\pi^*_{r^w};\cD) \\
        \leq & J_\beta(\pi^*_{r^w}) - J_\beta(\pi_\textref) + \tilde{O}(\frac{e^{2{\Rmax}}}{\sqrt{N(w;\cD)}}).
    \end{align*}
\end{restatable}

%
%
%
\textbf{Pessimistic Estimation for} $J_\beta(\pi_\SELF)-J_\beta(\pi_\textref)$~
The main challenge in estimating the value of $\pi_\SELF$ is that, $\pi_\SELF$ is not fixed but changing and improving.
The previous optimistic strategy is not applicable here, since the coverage of $\pi_\SELF$ by the dataset is unclear, making it difficult to quantify the uncertainty in estimation via count-based bonus term.
Fortunately, given that $\pi_\SELF$ is improving over time, it is more important when it surpasses all the other source policies. Therefore, it suffices to construct a tight lower bound for $J_\beta(\pi_\SELF)-J_\beta(\pi_\textref)$; see line~\ref{line:LB_pi_value}. 
By leveraging $\hat{r}_\MLE$ and the optimality of $(\pi_\SELF, \hr_\SELF)$ for the $\RPO$ loss, we can show:
%
%
%
%
%
%
%
%
%

%
%
%

\begin{restatable}[Value Est Error for $\pi_\SELF$]{lemma}{LemSelfTransErr}\label{lem:formal_val_est_error}
    Under Assump.~\ref{assump:policy} and Def.~\ref{def:online_oracle}, w.p. $1-\delta$, in each call of Alg.~\ref{alg:transfer_policy_computing}:
    %
    %
    %
    %
    %
    \begin{align*}
        &J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\textref) - \tilde{O}\Big(\frac{{\Rmax} e^{2{\Rmax}}}{\sqrt{|\cD|}} \cdot (\cov^{\pi^*_{r^*}|\pi_\mix^{\cD}} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha})\Big)\\
        & \leq \hat{V}(\pi_\SELF; \cD) \leq J_\beta(\pi_\SELF) - J_\beta(\pi_\textref) \numberthis\label{eq:offline_est_err}.
    \end{align*}
\end{restatable}
Here $\pi_\mix^{\cD}:=\frac{1}{|\cD|}\sum_{i\leq|\cD|}\pi^i$ denotes the mixture policy.
In the LHS, the coefficient takes minimum over two factors $\cov^{\pi^*_{r^*}|\pi_\mix^{\cD}}$ and $\sqrt{\cC(\Pi)}/\alpha$, resulting from two different ways to estimate the value gap of $\pi_\SELF$.
According to offline RLHF theory (see Lem.~\ref{lem:offline_learning}), $\pi_\SELF$ is competitive with any $\pi \in \conv(\Pi)$ well-covered by the dataset distribution, or equivalently, $J_\beta(\pi^*_{r^*}) - J_\beta(\pi_\SELF) = J_\beta(\pi^*_{r^*}) - J_\beta(\pi) + \tilde{O}(|\cD|^{-\frac{1}{2}}\cov^{\pi|\pi_\mix^{\cD}})$.
By choosing $\pi = \pi^*_{r^*}$, we obtain the first bound with the factor $\cov^{\pi^*_{r^*}|\pi_\mix^D}$.
Next, considering $\pi = \frac{1}{\alpha kN}\sum_{i\leq k, j\leq \alpha N}\pi^{i,j}$, the uniform mixture of policies generated by $\AlgOnline$ so far, leads to the second bound involving $\sqrt{\cC(\Pi)}/\alpha$.
This also explains why we still involve normal online learning in $\TPO$---to provide another safeguard for the quality of the transfer policy.
%

%
%

%

%
%
%
%
%

%
%
%
%



\subsection{Main Results and Interpretation}\label{sec:alg_main_results}


We establish the per-step regret bound for $\TPO$ below.
%
\begin{restatable}[Total Regret]{theorem}{ThmMainReg}\label{thm:regret_guarantees}
    Suppose $\AlgOnline$ is a no-regret instance satisfying Def.~\ref{def:online_oracle}, whose regret grows as $\tilde{O}(\Rmax e^{2\Rmax} \sqrt{\cC(\Pi)\tilde{T}})$ for any intermediate step $\tilde{T}$ and some policy class complexity measure $\cC(\Pi)$.
    Then, w.p. $1-2\delta$, for any $T/K \leq t \leq T$, running $\TPO$ yields a regret bound:
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    \begin{align*}
        \sum_{\tau \leq t} J_\beta(\pi^*_{r^*}) -&  J_\beta(\pi^{k(\tau),n(\tau)}) = \Regret_\Online^{(t)} + \Regret_\Transfer^{(t)}\\
        \Regret_\Online^{(t)} :=& \tilde{O}({\Rmax} e^{2{\Rmax}} \sqrt{\alpha\Complexity(\Pi)t}),\numberthis\label{eq:transfer_regret_bound_1}\\
        \Regret_\Transfer^{(t)} :=& \tilde{O}\Big(\sum_{\substack{\tau\leq t: \alpha N < n(\tau) \leq N}} \Delta_{\min} \wedge \iota^{k(\tau),n(\tau)}\numberthis\label{eq:transfer_regret_bound_2}\\
         &+ e^{2{\Rmax}} \sqrt{(1-\alpha)Wt} \wedge  \sum_{w:\Delta(w) > 0} \frac{e^{4{\Rmax}}}{\Delta(w)} \Big).
         %
    \end{align*}
    Here we denote $k(\tau) := \lceil \frac{\tau}{N} \rceil$ and $n(\tau) := \tau~\text{mod}~N$ to be the block index and inner iteration index for step $\tau$;
    $\iota^{k(\tau),n(\tau)} := \tilde{O}({\Rmax} e^{2{\Rmax}} \Big(\cov^{\pi^*_{r^*}|\pi_\mix^{\tau}} \wedge \frac{\sqrt{\Complexity(\Pi)}}{\alpha}\Big) \sqrt{\frac{1}{\tau}})$, where $\pi_\mix^{\tau} := \frac{1}{\tau}\sum_{i\leq \tau} \pi^{k(i),n(i)}$ is the mixture policy up to $\tau$; $\Delta(w)$ and $\Delta_{\min}$ denote value gaps as defined in Sec.~\ref{sec:transfer_setting}.
    %
    %
    %
    %
    %
    %
\end{restatable}
We decompose the total regret into two parts depending on their origins. $\Regret_\Online^{(t)}$ comes from the regret by running the online algorithm $\AlgOnline$. It is weighted by $\alpha$ since we only allocate $\alpha$-proportion of the samples for $\AlgOnline$.
$\Regret_\Transfer^{(t)}$ represents the regret from transfer policies.
The first term in Eq.~\eqref{eq:transfer_regret_bound_2} reflects the benefits of utilizing transfer policy over online learning.
Here $\Delta_{\min}$ is contributed by source reward models $\{r^w\}_{w\in[W]}$, and the term $\iota^{k(\tau),n(\tau)}$ is due to the ``self-transfer policy'' $\pi_\SELF$, as we derived in Lem.~\ref{lem:formal_val_est_error}.
The second term in Eq.~\eqref{eq:transfer_regret_bound_2} results from the imperfection of source reward models: without prior knowledge on their quality, additional cost has to be paid during exploration.

Next, we elaborate the benefits of transfer learning by taking a closer look at $\Regret_\Transfer^{(t)}$ in Eq.~\eqref{eq:transfer_regret_bound_2}.
Note that the lower $\Regret_\Transfer^{(t)}$ is, the faster $\hat{\pi}^*_{r^*}$ in $\TPO$ converges to $\pi^*_{r^*}$.
When $\Delta_{\min} = 0$, i.e. $r^*$ is realizable in $\{r^w\}_{w\in[W]}$, we have $\Regret_\Transfer^{(t)} = \tilde{O}(\sqrt{Wt} \wedge \sum_{w:\Delta(w)>0} \frac{1}{\Delta(w)})$ and the benefit of transfer learning is clear.
Thus, in the following, we only focus on the case $\Delta_{\min} > 0$. We separately consider two scenarios, according to the relationship between $t$ and $\Delta_{\min}$. For clarity, we will omit the constant terms ${\Rmax}$ and $e^{{\Rmax}}$.
%
%
%
%
%
%
%

%
\textbf{Stage 1: $t<\frac{W^2}{\Delta^2_{\min}}$}~
%
%
This corresponds to the early learning stage, when $t$ is relatively small.
In this case, Thm.~\ref{thm:regret_guarantees} implies the following regret bound:
%
%
%
%
\begin{align*}
    \Regret_\Transfer^{(t)} = \tilde{O}(\sqrt{1-\alpha} (\sqrt{Wt} + \Delta_{\min}t)) = \tilde{O}(W\sqrt{t}),\numberthis\label{eq:case_1}
\end{align*}
%
which can be further improved to $\tilde{O}(\sqrt{Wt})$ if $t < \frac{W}{\Delta_{\min}^2}$.
This suggests at the earlier stage, the benefits of transfer is contributed mostly by the source reward models $\{r^w\}_{w\in[W]}$.
In general, we can expect the number of source tasks $W$ much lower than the policy class complexity measure $\Complexity(\Pi)$.
Therefore, Eq.~\eqref{eq:case_1} implies a significant improvement over the typical online learning regret bound without transfer.
%
%
%

%
\textbf{Stage 2: $\Tt\geq \frac{W^2}{\Delta^2_{\min}}$}~
In this case, the second term in Eq.~\eqref{eq:transfer_regret_bound_2} is controlled by $O(\sum_{w\in[W]}\frac{1}{\Delta(w)}) = O(\frac{W}{\Delta_{\min}}) = O(\sqrt{\Tt})$, and we have the following regret bound:
%
%
%
%
%
%
%
%
%
%
%
%
\begin{align}
    \textstyle \Regret_\Transfer^{(t)} = \tilde{O}\Big(\sqrt{\frac{\Complexity(\Pi)\Tt}{\alpha^2} \wedge \sum_{\tau\leq\Tt} (\cov^{\pi^*_{r^*}|\pi_\mix^{\tau}})^2}\Big).\numberthis\label{eq:case_2}
\end{align}
%
%
At the first glance, the RHS is controlled by $\tilde{O}(\sqrt{\Complexity(\Pi)\Tt}/\alpha)$, which implies transfer learning at most suffer a factor of $1/\alpha$ larger regrets than no transfer.
However, in fact, the term $\sqrt{\sum_{\tau\leq\Tt} (\cov^{\pi^*_{r^*}|\pi_\mix^{\tau}})^2}$ yields a much tighter bound, which only grows as $\tilde{O}(\sqrt{\Tt})$ after finite time, and is independent of $\Complexity(\Pi)$.
%
To see this, by Lem.~\ref{lem:coverage_and_value_gap} and the concavity of $J_\beta(\cdot)$, we have $\cov^{\pi^*_{r^*}|\pi_\mix^{t}} = 1 + \tilde{O}({\kappa(e^{\frac{2{\Rmax}}{\beta}}) }\cdot \frac{\Regret_\Online^{(t)} + \Regret_\Transfer^{(t)}}{\beta t})$.
Note that Eq.~\eqref{eq:case_1} and~\eqref{eq:case_2} already indicate a regret upper bound $\Regret_\Transfer^{(t)}=\tilde{O}(\Coeff\sqrt{t})$, where $\Coeff$ is a short note of a coefficient depending on $\alpha,~W,~\{\Delta(w)\}_{w\in[W]}$ and $\Complexity(\Pi)$, but not $t$.
%
%
%
%
%
This implies $\cov^{\pi^*_{r^*}|\pi_\mix^{t}}$ converges to 1 at the rate of $O(1/\sqrt{t})$, and $\Regret_\Transfer^{(t)} = \tilde{O}(\sqrt{t})$ after finite time.
%
%
%

Although the above provable benefits in Stage 2 result primarily from ``self-transfer learning'', high-quality source reward models also play an important role here.
According to Eq.~\eqref{eq:case_1}, small $\Delta_{\min}$ can lead to small $\Coeff$ and therefore, accelerate the convergence of $\cov^{\pi^*_{r^*}|\pi_\mix^{t}}$ towards $1$.
%
%
%
%

%
%
%



%
\begin{algorithm*}[t]
    \textbf{Input}: $K$, $N$ and $\{r^w\}_{w\in[W]}$; \\
    %
    %
    %

    For all $(k,n) \in [K]\times[N]$, and all $w\in[W]$,
    $\cD^{k,n} := \cup_{j=1}^{n-1}\{(s^{k,j}, a^{k,j},\ta^{k,j}, y^{k,j})\}$, $N^{k,n}(\cdot) := \sum_{j<n}\mathbb{I}[\cdot = \pi^{k,j}]$, and $\hat{\mP}_{r^*}^{k,n}(\cdot \succ \pi^k_\base) := \frac{1}{N^{k,n}(\cdot)} \sum_{j<n}\mathbb{I}[\cdot = \pi^{k,j}]y^{k,j}$. \\
    Initialize $\pi^1_\base \gets \pi_\textref$; \\
    \For{$k=1,2...,K$}{
        \For{$n=1,... N$}{
            %
            %
            %
            %
                %
            %
            $\forall w\in[W],~\hat{\text{WR}}^{\pi^*_{r^w}} \gets \hat{\mP}_{r^*}^{k,n}(\pi^*_{r^w} \succ \pi^k_\base) + c \sqrt{\frac{1}{N^{k,n}(\pi^*_{r^w})}\log\frac{1}{\delta}}$; \\
            $\hat{\text{WR}}^{\pi^k_\base} \gets \mP_{r^*}(\pi^k_\base\succ\pi^k_\base) = 0.5.$ \blue{// $\hat{\text{WR}}^{\pi^k_\base}$ can be treated as a hyperparameter taking value other than 0.5.} \\
            $\pi^{k,n} \gets \argmax_{\pi \in \{\pi^*_{r^w}\}_{w=1}^W \cup \{\pi_\base^k\}} \hat{\text{WR}}^\pi$. \label{line:UCB} \\ 
            %
            %
            %
            %
            %
            %
            Collect online data $(s^{k,n}, a^{k,n},\ta^{k,n}, y^{k,n}) \sim \rho\times\pi^{k,n}\times\pi^k_\Online\times\mP_{r^*}(\cdot|\cdot,\cdot,\cdot)$. \\
            %
            %
            %
            %
        }
        $\pi^{k+1}_{\base} \gets \text{Alg}_{\text{PO}}(\pi^{k}_{\base},\cD^{k,N+1})$; \\
    }
    \Return $\pi^{K+1}_{\base}$.
    \caption{Empirical $\TPO$}\label{alg:empirical}
\end{algorithm*}

\textbf{Choice of $\alpha$ and the total regret of $\TPO$}~
Our algorithm treats $\alpha$ as a hyperparameter. 
Based on the discussion above, with proper choice of $\alpha$ we have the following corollary.
\begin{corollary}[Total Regret of $\TPO$]\label{coro:total_regret}
    By choosing XPO \citep{xie2024exploratory} as $\AlgOnline$ and setting $\alpha = e^{-\frac{R}{\beta}} \leq \cov_{\infty}^{-1}(\Pi)$, Thm.~\ref{thm:regret_guarantees} implies that $\TPO$ achieves $\tilde{O}(W\sqrt{T})$ regret if $T < \frac{W^2}{\Delta^2_{\min}}$, or $\tilde{O}(\sqrt{T})$ regret if $T > \frac{W^2}{\Delta^2_{\min}}$ and large enough.
\end{corollary}
Depending on the concrete scenarios, if we have stronger prior beliefs that one of $\{r^w\}_{w\in[W]}$ is similar to $r^*$, we should prefer larger $\alpha$.
Besides, one may gradually decay $\alpha$ to 0 when the iteration number is large enough, which can also result in a total regret growing with $\tilde{O}(\sqrt{T})$ over time.

\textbf{Improved regret bound in standard online RLHF without source rewards}~Although our focus is transfer learning, our results can be extended to the standard online RLHF setting where no source tasks are present (i.e., $W=0$).
As implied by Corollary~\ref{coro:total_regret}, only utilizing self-transfer learning can result in an $\tilde{O}(\sqrt{T})$ online regret, thereby strictly improving existing results \citep{xiong2024iterative, xie2024exploratory,cen2024value,zhang2024self}.
%
%
%
%
%
%
%
%
%

\iffalse
%
%
Our discussion for Case 1 enlightens the benefits when the source reward models have high quality, i.e. $\Delta_{\min}$ is small.
Note that the sub-optimality of $\pi^t_{\mix}$ depends on the accumulative regret up to step $t$. A lower $\Delta_{\min}$ implies a lower $\Coeff$ in the above analysis, which implies $\cov^{\pi^*_{r^*}|\pi_\mix^{t}}$ can have a faster convergence to 1.
%
%


%
%
\fi



%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%