@article{cen2024value,
  title={Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF},
  author={Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo},
  journal={arXiv preprint arXiv:2405.19320},
  year={2024}
}

@article{chang2024dataset,
  title={Dataset reset policy optimization for rlhf},
  author={Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Brantley, Kiant{\'e} and Misra, Dipendra and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2404.08495},
  year={2024}
}

@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@article{du2024exploration,
  title={Exploration-driven policy optimization in rlhf: Theoretical insights on efficient data utilization},
  author={Du, Yihan and Winnicki, Anna and Dalal, Gal and Mannor, Shie and Srikant, R},
  journal={arXiv preprint arXiv:2402.10342},
  year={2024}
}

@article{gao2024rebel,
  title={Rebel: Reinforcement learning via regressing relative rewards},
  author={Gao, Zhaolin and Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Swamy, Gokul and Brantley, Kiant{\'e} and Joachims, Thorsten and Bagnell, J Andrew and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2404.16767},
  year={2024}
}

@inproceedings{golowich2022can,
  title={Can Q-learning be improved with advice?},
  author={Golowich, Noah and Moitra, Ankur},
  booktitle={Conference on Learning Theory},
  pages={4548--4619},
  year={2022},
  organization={PMLR}
}

@article{hong2024cross,
  title={Cross-lingual Transfer of Reward Models in Multilingual Alignment},
  author={Hong, Jiwoo and Lee, Noah and Mart{\'\i}nez-Casta{\~n}o, Rodrigo and Rodr{\'\i}guez, C{\'e}sar and Thorne, James},
  journal={arXiv preprint arXiv:2410.18027},
  year={2024}
}

@article{huang2022tiered,
  title={Tiered reinforcement learning: Pessimism in the face of uncertainty and constant regret},
  author={Huang, Jiawei and Zhao, Li and Qin, Tao and Chen, Wei and Jiang, Nan and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={679--690},
  year={2022}
}

@article{huang2023robust,
  title={Robust Knowledge Transfer in Tiered Reinforcement Learning},
  author={Huang, Jiawei and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52073--52085},
  year={2023}
}

@article{huang2024correcting,
  title={Correcting the mythos of kl-regularization: Direct alignment without overparameterization via chi-squared preference optimization},
  author={Huang, Audrey and Zhan, Wenhao and Xie, Tengyang and Lee, Jason D and Sun, Wen and Krishnamurthy, Akshay and Foster, Dylan J},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav},
  year={2023}
}

@article{liu2024provably,
  title={Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer},
  author={Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.16436},
  year={2024}
}

@inproceedings{mann2013directed,
  title={Directed exploration in reinforcement learning with transferred knowledge},
  author={Mann, Timothy A and Choe, Yoonsuck},
  booktitle={European Workshop on Reinforcement Learning},
  pages={59--76},
  year={2013},
  organization={PMLR}
}

@article{nguyen2024laser,
  title={LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits},
  author={Nguyen, Duy and Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit},
  journal={arXiv preprint arXiv:2410.01735},
  year={2024}
}

@inproceedings{novoseller2020dueling,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={1029--1038},
  year={2020},
  organization={PMLR}
}

@article{pacchiano2021dueling,
  title={Dueling rl: reinforcement learning with trajectory preferences},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  journal={arXiv preprint arXiv:2111.04850},
  year={2021}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey.},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  year={2009}
}

@article{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL?},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  journal={arXiv preprint arXiv:2306.14111},
  year={2023}
}

@article{wu2024reuse,
  title={Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment},
  author={Wu, Zhaofeng and Balashankar, Ananth and Kim, Yoon and Eisenstein, Jacob and Beirami, Ahmad},
  journal={arXiv preprint arXiv:2404.12318},
  year={2024}
}

@article{xie2024exploratory,
  title={Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF},
  author={Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2405.21046},
  year={2024}
}

@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}

@article{ye2024theoretical,
  title={A theoretical analysis of nash learning from human feedback under general kl-regularized preference},
  author={Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2402.07314},
  year={2024}
}

@article{zhan2023provable,
  title={Provable offline preference-based reinforcement learning},
  author={Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2305.14816},
  year={2023}
}

@article{zhang2024self,
  title={Self-exploring language models: Active preference elicitation for online alignment},
  author={Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.19332},
  year={2024}
}

@article{zhu2023transfer,
  title={Transfer learning in deep reinforcement learning: A survey},
  author={Zhu, Zhuangdi and Lin, Kaixiang and Jain, Anil K and Zhou, Jiayu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

