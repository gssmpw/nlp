\section{Related work}
This section reviews relevant works addressing the problem outlined in section \ref{eq:problem-formulation}, including common data-driven methods and recent transformer-based approaches.

\subsection{Data-driven techniques}

Discovering dynamical system models from data is crucial in fields such as science and engineering.
While traditional models are derived from first principles, this approach can be highly challenging in areas like climate science, finance, and biology. 
Data-driven methods for physical law discovery are an evolving field, 
with various techniques including
linear methods ____, 
dynamic mode decomposition (DMD)____, 
nonlinear autoregressive models____, 
neural networks____, 
Koopman theory____,
nonlinear Laplacian spectral analysis____, 
Gaussian process regression____, 
diffusion maps____, 
genetic programming____, and 
sparse regression____, among other recent advances.
Sparse regression techniques, such as SINDy____, have proven to be an effective method, offering high computational efficiency and a straightforward methodology. 
SINDy has demonstrated strong performance across various fields____. 
However, in order to implement effectively, it relies on prior knowledge and technical expertise. 
In this work, we leverage foundation models to supply this missing prior knowledge and expertise.

\subsection{Transformer-based discovery}

The introduction of transformer models____ has made it possible to learn sequence-to-sequence tasks in a broad range of domains. 
Combined with large-scale pre-training, transformers have been successfully applied to symbolic tasks, including 
function integration____, 
logic____, and theorem proving____.

Recent studies have applied transformers to symbolic regression____, where transformers are used to predict function structures from measurement data. 
The latest contribution in this line of work is ODEFormer____, a transformer-based model designed to infer multidimensional ordinary differential equations in symbolic form from a single trajectory of observational data. 
In our work, we include ODEFormer in our benchmark comparisons and show our framework is able to find $60\%$ more models with an $R2$ score of above 0.99 on two benchmarks.

In ____, an LLM is used for symbolic regression to sample candidate functions $f_c$, optimize their parameters, and score them based on test data. 
This score refines the candidate functions through reflection on prior samples. 
Similar to our approach, ____ also employs language models in equation discovery, but we enhance it by incorporating system observations like plots and descriptions. 
Additionally, we generate representations not only for the candidate model $\widehat{f}$ but also for the optimizer $A_\psi$ that tunes model parameters. 
Our method is evaluated on a wider range of benchmark problems, showing robustness across diverse data-driven dynamics tasks. 
Importantly, all experiments rely solely on pre-trained, open-source models, without using commercial models like \textsc{GPT-4o}.