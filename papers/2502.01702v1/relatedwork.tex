\section{Related work}
This section reviews relevant works addressing the problem outlined in section \ref{eq:problem-formulation}, including common data-driven methods and recent transformer-based approaches.

\subsection{Data-driven techniques}

Discovering dynamical system models from data is crucial in fields such as science and engineering.
While traditional models are derived from first principles, this approach can be highly challenging in areas like climate science, finance, and biology. 
Data-driven methods for physical law discovery are an evolving field, 
with various techniques including
linear methods \cite{nellesnonlinear, ljung2010perspectives}, 
dynamic mode decomposition (DMD)~\cite{schmid2010dynamic, kutz2016dynamic}, 
nonlinear autoregressive models~\cite{akaike1969autoregressive,billings2013nonlinear}, 
neural networks~\cite{yang2020physics,wehmeyer2018time,mardt2018vampnets,vlachas2018data,pathak2018model,lu2021deepxde,raissi2019physics,champion2019data,raissi2020hidden}, 
Koopman theory~\cite{budivsic2012applied,mezic2013analysis,williams2015data,klus2018data},
nonlinear Laplacian spectral analysis~\cite{giannakis2012nonlinear}, 
Gaussian process regression~\cite{raissi2017machine,raissi2018hidden}, 
diffusion maps~\cite{yair2017reconstruction}, 
genetic programming~\cite{daniels2015automated,schmidt2009distilling,bongard2007automated}, and 
sparse regression~\cite{Brunton2016,rudy2017data,schaeffer2017learning}, among other recent advances.
Sparse regression techniques, such as SINDy~\cite{Brunton2016}, have proven to be an effective method, offering high computational efficiency and a straightforward methodology. 
SINDy has demonstrated strong performance across various fields~\cite{Shea_2021,MESSENGER2021110525,kaheman2020sindy,Fasel_2022}. 
However, in order to implement effectively, it relies on prior knowledge and technical expertise. 
In this work, we leverage foundation models to supply this missing prior knowledge and expertise.

\subsection{Transformer-based discovery}

The introduction of transformer models~\cite{Vaswani17} has made it possible to learn sequence-to-sequence tasks in a broad range of domains. 
Combined with large-scale pre-training, transformers have been successfully applied to symbolic tasks, including 
function integration~\cite{Lample2020Deep}, 
logic~\cite{hahn2021teaching}, and theorem proving~\cite{polu2020}.

Recent studies have applied transformers to symbolic regression~\cite{kamienny2022endtoend, Landajuela22, Vastl24}, where transformers are used to predict function structures from measurement data. 
The latest contribution in this line of work is ODEFormer~\cite{ascoli2024odeformer}, a transformer-based model designed to infer multidimensional ordinary differential equations in symbolic form from a single trajectory of observational data. 
In our work, we include ODEFormer in our benchmark comparisons and show our framework is able to find $60\%$ more models with an $R2$ score of above 0.99 on two benchmarks.

In \cite{Mengge24}, an LLM is used for symbolic regression to sample candidate functions $f_c$, optimize their parameters, and score them based on test data. 
This score refines the candidate functions through reflection on prior samples. 
Similar to our approach, \cite{Mengge24} also employs language models in equation discovery, but we enhance it by incorporating system observations like plots and descriptions. 
Additionally, we generate representations not only for the candidate model $\widehat{f}$ but also for the optimizer $A_\psi$ that tunes model parameters. 
Our method is evaluated on a wider range of benchmark problems, showing robustness across diverse data-driven dynamics tasks. 
Importantly, all experiments rely solely on pre-trained, open-source models, without using commercial models like \textsc{GPT-4o}.