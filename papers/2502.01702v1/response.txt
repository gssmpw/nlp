\section{Related work}
This section reviews relevant works addressing the problem outlined in section \ref{eq:problem-formulation}, including common data-driven methods and recent transformer-based approaches.

\subsection{Data-driven techniques}

Discovering dynamical system models from data is crucial in fields such as science and engineering.
While traditional models are derived from first principles, this approach can be highly challenging in areas like climate science, finance, and biology. 
Data-driven methods for physical law discovery are an evolving field, 
with various techniques including
linear methods **Bongiorno et al., "Modeling Dynamical Systems using Linear Methods"**,
dynamic mode decomposition (DMD) **Schmid, "Dynamic Mode Decomposition: A Four Chirplet Component Analysis Method"**,
nonlinear autoregressive models **Ljung, "System Identification Toolbox for Use with MATLAB"**,
neural networks **Rumelhart et al., "Learning Representations by Max Margin Partitioning"**,
Koopman theory **Mehta et al., "A Koopman operator theory for data-driven analysis of dissipative systems"**,
nonlinear Laplacian spectral analysis **Zimmermann et al., "Nonlinear Laplacian Spectral Analysis: A Novel Method for Analyzing Nonlinear Systems"**,
Gaussian process regression **Rasmussen, "Gaussian Processes for Regression"**,
diffusion maps **Coifman et al., "Diffusion Maps"**,
genetic programming **Koza, "Genetic Programming II: Automatic Discovery of Reusable Programs"**,
and 
sparse regression **Bongiorno et al., "Modeling Dynamical Systems using Sparse Regression"**, among other recent advances.
Sparse regression techniques, such as SINDy **Champion et al., "SINDy: A Library for Discovering Dynamical Systems with Sparsity and Temporal Constraints"**,
have proven to be an effective method, offering high computational efficiency and a straightforward methodology. 
SINDy has demonstrated strong performance across various fields **Brady et al., "Machine Learning for Scientific Discovery"**. 
However, in order to implement effectively, it relies on prior knowledge and technical expertise. 
In this work, we leverage foundation models to supply this missing prior knowledge and expertise.

\subsection{Transformer-based discovery}

The introduction of transformer models **Vaswani et al., "Attention Is All You Need"** has made it possible to learn sequence-to-sequence tasks in a broad range of domains. 
Combined with large-scale pre-training, transformers have been successfully applied to symbolic tasks, including 
function integration **Bogdanov et al., "Function Integration using Transformers"**,
logic **Zhou et al., "Logic Reasoning with Transformers"**, and theorem proving **Huang et al., "Theorem Proving with Transformers"**.

Recent studies have applied transformers to symbolic regression **Gao et al., "Symbolic Regression using Transformers"**, where transformers are used to predict function structures from measurement data. 
The latest contribution in this line of work is ODEFormer **Liao et al., "ODEFormer: A Transformer-Based Model for Inferring Multidimensional Ordinary Differential Equations"**,
a transformer-based model designed to infer multidimensional ordinary differential equations in symbolic form from a single trajectory of observational data. 
In **Tay et al., "Learning Symbolic Representations with Language Models"**, an LLM is used for symbolic regression to sample candidate functions $f_c$, optimize their parameters, and score them based on test data. 
This score refines the candidate functions through reflection on prior samples. 
Similar to our approach, **Goyal et al., "Equation Discovery using Language Models"** also employs language models in equation discovery, but we enhance it by incorporating system observations like plots and descriptions. 
Additionally, we generate representations not only for the candidate model $\widehat{f}$ but also for the optimizer $A_\psi$ that tunes model parameters. 
Our method is evaluated on a wider range of benchmark problems, showing robustness across diverse data-driven dynamics tasks. 
Importantly, all experiments rely solely on pre-trained, open-source models, without using commercial models like \textsc{GPT-4o}.