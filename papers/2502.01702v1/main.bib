

@article{Brunton2016,
author = {Steven L. Brunton  and Joshua L. Proctor  and J. Nathan Kutz },
title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
journal = {Proceedings of the National Academy of Sciences},
volume = {113},
number = {15},
pages = {3932-3937},
year = {2016},
doi = {10.1073/pnas.1517384113},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1517384113},
}

@misc{liu2023improvedllava,
          author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
          title={Improved Baselines with Visual Instruction Tuning}, 
          publisher={arXiv:2310.03744},
          year={2023},
}

@article{rudy2017data,
  title={Data-driven discovery of partial differential equations},
  author={Rudy, Samuel H and Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Science advances},
  volume={3},
  number={4},
  pages={e1602614},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{schaeffer2017learning,
  title={Learning partial differential equations via data discovery and sparse optimization},
  author={Schaeffer, Hayden},
  journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={473},
  number={2197},
  pages={20160446},
  year={2017},
  publisher={The Royal Society Publishing}
}

@inproceedings{liu2023llava,
    author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title       = {Visual Instruction Tuning},
    booktitle   = {NeurIPS},
    year        = {2023}
}

@misc{almazrouei2023,
      title={The Falcon Series of Open Language Models}, 
      author={Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
      year={2023},
      eprint={2311.16867},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{lee2024nv,
  title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@misc{nussbaum2024nomicembedtrainingreproducible,
      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, 
      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},
      year={2024},
      eprint={2402.01613},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@misc{gao2024,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{NEURIPS2020_6b493230,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 volume = {33},
 year = {2020}
}


@article{Lan2020Albert,
    author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
    title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    journal = {International Conference on Learning Representations (ICLR)},
    year = 2020
}


@misc{chen2024,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks}, 
      author={Zhe Chen and Jiannan Wu and Wenhai Wang and Weijie Su and Guo Chen and Sen Xing and Muyan Zhong and Qinglong Zhang and Xizhou Zhu and Lewei Lu and Bin Li and Ping Luo and Tong Lu and Yu Qiao and Jifeng Dai},
      year={2024},
      eprint={2312.14238},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@misc{bai2023,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{kaiser2018sparse,
  title={Sparse identification of nonlinear dynamics for model predictive control in the low-data limit},
  author={Kaiser, Eurika and Kutz, J Nathan and Brunton, Steven L},
  journal={Proceedings of the Royal Society A},
  volume={474},
  number={2219},
  pages={20180335},
  year={2018},
  publisher={The Royal Society Publishing}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{mower2024ros,
  title={Ros-llm: A ros framework for embodied ai with task feedback and structured reasoning},
  author={Mower, Christopher E and Wan, Yuhui and Yu, Hongzhan and Grosnit, Antoine and Gonzalez-Billandon, Jonas and Zimmer, Matthieu and Wang, Jinlong and Zhang, Xinyu and Zhao, Yao and Zhai, Anbang and others},
  journal={arXiv preprint arXiv:2406.19741},
  year={2024}
}

@inproceedings{brohan2023can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle={Conference on robot learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{CowenRivers2022,
author = {Cowen-Rivers, Alexander and Lyu, Wenlong and Tutunov, Rasul and Wang, Zhi and Grosnit, Antoine and Griffiths, Ryan-Rhys and Maravel, Alexandre and Hao, Jianye and Wang, Jun and Peters, Jan and Bou Ammar, Haitham},
year = {2022},
month = {07},
pages = {},
title = {HEBO: Pushing The Limits of Sample-Efficient Hyperparameter Optimisation},
volume = {74},
journal = {Journal of Artificial Intelligence Research}
}

@article{Tianlong24,
author = {Xu, Tianlong and Tong, Richard and Liang, Jing and Fan, Xing and Li, Haoyang and Wen, Qingsong and Pang, Guansong},
title = {Foundation Models for Education: Promises and Prospects},
year = {2024},
issue_date = {May-June 2024},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {39},
number = {3},
issn = {1541-1672},
doi = {10.1109/MIS.2024.3398191},
abstract = {With the advent of foundation models like ChatGPT, educators are excited about the transformative role that artificial intelligence (AI) might play in propelling the next education revolution. The developing speed and the profound impact of foundation models in various industries force us to think deeply about the changes they will make to education, a domain that is critically important for the future of humans. In this article, we discuss the strengths of foundation models, such as personalized learning, education inequality, and reasoning capabilities, as well as the development of agent architecture tailored for education, which integrates AI agents with pedagogical frameworks to create adaptive learning environments. Furthermore, we highlight the risks and opportunities of AI overreliance and creativity. Finally, we envision a future where foundation models in education harmonize human and AI capabilities, fostering a dynamic, inclusive, and adaptive educational ecosystem.},
journal = {IEEE Intelligent Systems},
month = jun,
pages = {20–24},
numpages = {5}
}

@article{henderson2023foundation,
  title={Foundation models and fair use},
  author={Henderson, Peter and Li, Xuechen and Jurafsky, Dan and Hashimoto, Tatsunori and Lemley, Mark A and Liang, Percy},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={400},
  pages={1--79},
  year={2023}
}

@Article{Moor2023,
author={Moor, Michael
and Banerjee, Oishi
and Abad, Zahra Shakeri Hossein
and Krumholz, Harlan M.
and Leskovec, Jure
and Topol, Eric J.
and Rajpurkar, Pranav},
title={Foundation models for generalist medical artificial intelligence},
journal={Nature},
year={2023},
month={Apr},
day={01},
volume={616},
number={7956},
pages={259-265},
abstract={The exceptionally rapid development of highly flexible, reusable artificial intelligence (AI) models is likely to usher in newfound capabilities in medicine. We propose a new paradigm for medical AI, which we refer to as generalist medical AI (GMAI). GMAI models will be capable of carrying out a diverse set of tasks using very little or no task-specific labelled data. Built through self-supervision on large, diverse datasets, GMAI will flexibly interpret different combinations of medical modalities, including data from imaging, electronic health records, laboratory results, genomics, graphs or medical text. Models will in turn produce expressive outputs such as free-text explanations, spoken recommendations or image annotations that demonstrate advanced medical reasoning abilities. Here we identify a set of high-impact potential applications for GMAI and lay out specific technical capabilities and training datasets necessary to enable them. We expect that GMAI-enabled applications will challenge current strategies for regulating and validating AI devices for medicine and will shift practices associated with the collection of large medical datasets.},
issn={1476-4687},
doi={10.1038/s41586-023-05881-4},
}

@article{lu2021deepxde,
  title={DeepXDE: A deep learning library for solving differential equations},
  author={Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  journal={SIAM review},
  volume={63},
  number={1},
  pages={208--228},
  year={2021},
  publisher={SIAM}
}

@article{bongard2007automated,
  title={Automated reverse engineering of nonlinear dynamical systems},
  author={Bongard, Josh and Lipson, Hod},
  journal={Proceedings of the National Academy of Sciences},
  volume={104},
  number={24},
  pages={9943--9948},
  year={2007},
  publisher={National Acad Sciences}
}

@article{schmidt2009distilling,
  title={Distilling free-form natural laws from experimental data},
  author={Schmidt, Michael and Lipson, Hod},
  journal={science},
  volume={324},
  number={5923},
  pages={81--85},
  year={2009},
  publisher={American Association for the Advancement of Science}
}

@article{daniels2015automated,
  title={Automated adaptive inference of phenomenological dynamical models},
  author={Daniels, Bryan C and Nemenman, Ilya},
  journal={Nature communications},
  volume={6},
  number={1},
  pages={8133},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{yair2017reconstruction,
  title={Reconstruction of normal forms by learning informed observation geometries from data},
  author={Yair, Or and Talmon, Ronen and Coifman, Ronald R and Kevrekidis, Ioannis G},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={38},
  pages={E7865--E7874},
  year={2017},
  publisher={National Acad Sciences}
}

@article{giannakis2012nonlinear,
  title={Nonlinear Laplacian spectral analysis for time series with intermittency and low-frequency variability},
  author={Giannakis, Dimitrios and Majda, Andrew J},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={7},
  pages={2222--2227},
  year={2012},
  publisher={National Acad Sciences}
}

@article{raissi2018hidden,
  title={Hidden physics models: Machine learning of nonlinear partial differential equations},
  author={Raissi, Maziar and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={357},
  pages={125--141},
  year={2018},
  publisher={Elsevier}
}

@article{raissi2017machine,
  title={Machine learning of linear differential equations using Gaussian processes},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={348},
  pages={683--693},
  year={2017},
  publisher={Elsevier}
}

@article{raissi2020hidden,
  title={Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
  author={Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
  journal={Science},
  volume={367},
  number={6481},
  pages={1026--1030},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@article{champion2019data,
  title={Data-driven discovery of coordinates and governing equations. arXiv e-prints, art},
  author={Champion, Kathleen and Lusch, Bethany and Kutz, J Nathan and Brunton, Steven L},
  journal={arXiv preprint arXiv:1904.02107},
  year={2019}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{pathak2018model,
  title={Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach},
  author={Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  journal={Physical review letters},
  volume={120},
  number={2},
  pages={024102},
  year={2018},
  publisher={APS}
}
@article{budivsic2012applied,
  title={Applied koopmanism},
  author={Budi{\v{s}}i{\'c}, Marko and Mohr, Ryan and Mezi{\'c}, Igor},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={22},
  number={4},
  year={2012},
  publisher={AIP Publishing}
}

@article{aizawa1982,
  title={Topological Aspects in Chaos and in 2 k-Period Doubling Cascade},
  author={Aizawa, Yoji and Uezu, Tatsuya},
  journal={Progress of Theoretical Physics},
  volume={67},
  number={3},
  pages={982--985},
  year={1982},
  publisher={Oxford University Press}
}

@article{vlachas2018data,
  title={Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks},
  author={Vlachas, Pantelis R and Byeon, Wonmin and Wan, Zhong Y and Sapsis, Themistoklis P and Koumoutsakos, Petros},
  journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={474},
  number={2213},
  pages={20170844},
  year={2018},
  publisher={The Royal Society Publishing}
}

@inproceedings{Landajuela22,
 author = {Landajuela, Mikel and Lee, Chak Shing and Yang, Jiachen and Glatt, Ruben and Santiago, Claudio P and Aravena, Ignacio and Mundhenk, Terrell and Mulcahy, Garrett and Petersen, Brenden K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {33985--33998},
 publisher = {Curran Associates, Inc.},
 title = {A Unified Framework for Deep Symbolic Regression},
 volume = {35},
 year = {2022}
}


@ARTICLE{Vastl24,
  author={Vastl, Martin and Kulhánek, Jonáš and Kubalík, Jiří and Derner, Erik and Babuška, Robert},
  journal={IEEE Access}, 
  title={SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture}, 
  year={2024},
  volume={12},
  number={},
  pages={37840-37849},
  keywords={Transformers;Mathematical models;Vectors;Symbols;Decoding;Optimization;Predictive models;Neural networks;Genetic programming;Computational complexity;Benchmark testing;Regression analysis;Symbolic regression;neural networks;transformers},
  doi={10.1109/ACCESS.2024.3374649}}

@inproceedings{
kamienny2022endtoend,
title={End-to-end Symbolic Regression with Transformers},
author={Pierre-Alexandre Kamienny and St{\'e}phane d'Ascoli and Guillaume Lample and Francois Charton},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
}

@misc{polu2020,
      title={Generative Language Modeling for Automated Theorem Proving}, 
      author={Stanislas Polu and Ilya Sutskever},
      year={2020},
      eprint={2009.03393},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{
hahn2021teaching,
title={Teaching Temporal Logics to Neural Networks},
author={Christopher Hahn and Frederik Schmitt and Jens U. Kreber and Markus Norman Rabe and Bernd Finkbeiner},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{
Lample2020Deep,
title={Deep Learning For Symbolic Mathematics},
author={Guillaume Lample and François Charton},
booktitle={International Conference on Learning Representations},
year={2020},
}

@article{wehmeyer2018time,
  title={Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics},
  author={Wehmeyer, Christoph and No{\'e}, Frank},
  journal={The Journal of chemical physics},
  volume={148},
  number={24},
  year={2018},
  publisher={AIP Publishing}
}

@article{mardt2018vampnets,
  title={VAMPnets for deep learning of molecular kinetics},
  author={Mardt, Andreas and Pasquali, Luca and Wu, Hao and No{\'e}, Frank},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={5},
  year={2018},
  publisher={Nature Publishing Group UK London}
}

@article{yang2020physics,
  title={Physics-informed generative adversarial networks for stochastic differential equations},
  author={Yang, Liu and Zhang, Dongkun and Karniadakis, George Em},
  journal={SIAM Journal on Scientific Computing},
  volume={42},
  number={1},
  pages={A292--A317},
  year={2020},
  publisher={SIAM}
}

@misc{brunton2021sindy,
  author       = {S. L. Brunton},
  title        = {Sparse Identification of Nonlinear Dynamics (SINDy): Sparse Machine Learning Models 5 Years Later!},
  year         = {2021},
  month        = aug,
  day          = {27},
  note         = {Video},
}

@book{billings2013nonlinear,
  title={Nonlinear system identification: NARMAX methods in the time, frequency, and spatio-temporal domains},
  author={Billings, Stephen A},
  year={2013},
  publisher={John Wiley \& Sons}
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{akaike1969autoregressive,
  title={Autoregressive models for prediction},
  author={Akaike, Hirotugu},
  journal={Ann Inst Stat Math},
  volume={21},
  pages={243--247},
  year={1969}
}

@article{williams2015data,
  title={A data--driven approximation of the koopman operator: Extending dynamic mode decomposition},
  author={Williams, Matthew O and Kevrekidis, Ioannis G and Rowley, Clarence W},
  journal={Journal of Nonlinear Science},
  volume={25},
  pages={1307--1346},
  year={2015},
  publisher={Springer}
}

@article{klus2018data,
  title={Data-driven model reduction and transfer operator approximation},
  author={Klus, Stefan and N{\"u}ske, Feliks and Koltai, P{\'e}ter and Wu, Hao and Kevrekidis, Ioannis and Sch{\"u}tte, Christof and No{\'e}, Frank},
  journal={Journal of Nonlinear Science},
  volume={28},
  pages={985--1010},
  year={2018},
  publisher={Springer}
}

@article{mezic2013analysis,
  title={Analysis of fluid flows via spectral properties of the Koopman operator},
  author={Mezi{\'c}, Igor},
  journal={Annual review of fluid mechanics},
  volume={45},
  number={1},
  pages={357--378},
  year={2013},
  publisher={Annual Reviews}
}

@article{schmid2010dynamic,
  title={Dynamic mode decomposition of numerical and experimental data},
  author={Schmid, Peter J},
  journal={Journal of fluid mechanics},
  volume={656},
  pages={5--28},
  year={2010},
  publisher={Cambridge University Press}
}

@book{kutz2016dynamic,
  title={Dynamic mode decomposition: data-driven modeling of complex systems},
  author={Kutz, J Nathan and Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L},
  year={2016},
  publisher={SIAM}
}

@book{brunton2022data,
  title={Data-driven science and engineering: Machine learning, dynamical systems, and control},
  author={Brunton, Steven L and Kutz, J Nathan},
  year={2022},
  publisher={Cambridge University Press}
}

@article{ljung2010perspectives,
  title={Perspectives on system identification},
  author={Ljung, Lennart},
  journal={Annual Reviews in Control},
  volume={34},
  number={1},
  pages={1--12},
  year={2010},
  publisher={Elsevier}
}

@article{nellesnonlinear,
  title={Nonlinear System Identification [electronic resource]: From Classical Approaches to Neural Networks and Fuzzy Models},
  author={Nelles, Oliver and others},
  journal={Berlin, Heidelberg: Springer Berlin Heidelberg: Imprint: Springer},
  year=2013,
}

@misc{christianos2023,
      title={Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning}, 
      author={Filippos Christianos and Georgios Papoudakis and Matthieu Zimmer and Thomas Coste and Zhihao Wu and Jingxuan Chen and Khyati Khandelwal and James Doran and Xidong Feng and Jiacheng Liu and Zheng Xiong and Yicheng Luo and Jianye Hao and Kun Shao and Haitham Bou-Ammar and Jun Wang},
      year={2023},
      eprint={2312.14878},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{Cranmer23,
      title={Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl}, 
      author={Miles Cranmer},
      year={2023},
      eprint={2305.01582},
      archivePrefix={arXiv},
      primaryClass={astro-ph.IM},
}



@INPROCEEDINGS{Chakraborty17,
  author={Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and Kelley, Troy D. and Braines, Dave and Sensoy, Murat and Willis, Christopher J. and Gurram, Prudhvi},
  booktitle={2017 IEEE SmartWorld, Ubiquitous Intelligence \& Computing, Advanced \& Trusted Computed, Scalable Computing \& Communications, Cloud \& Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, 
  title={Interpretability of deep learning models: A survey of results}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  keywords={Machine learning;Training;Training data;Data models;Brain modeling;Predictive models;Neurons},
  doi={10.1109/UIC-ATC.2017.8397411}}



@misc{radwan2024,
      title={A Comparison of Recent Algorithms for Symbolic Regression to Genetic Programming}, 
      author={Yousef A. Radwan and Gabriel Kronberger and Stephan Winkler},
      year={2024},
      eprint={2406.03585},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}


@article{ALEKSEEVA2021102002,
title = {The demand for AI skills in the labor market},
journal = {Labour Economics},
volume = {71},
pages = {102002},
year = {2021},
issn = {0927-5371},
doi = {https://doi.org/10.1016/j.labeco.2021.102002},
author = {Liudmila Alekseeva and José Azar and Mireia Giné and Sampsa Samila and Bledi Taska},
keywords = {Artificial intelligence, Machine learning, Wage premium, Technology diffusion},
abstract = {Using detailed data on skill requirements in online vacancies, we estimate the demand for AI specialists across occupations, sectors, and firms. We document a dramatic increase in the demand for AI skills over 2010–2019 in the U.S. economy across most industries and occupations. The demand is highest in IT occupations, followed by architecture and engineering, scientific, and management occupations. Firms with larger market capitalization, higher cash holdings, and higher investments in R&D have a higher demand for AI skills. We also document a wage premium of 11% for job postings that require AI skills within the same firm and 5% within the same job title. Managerial occupations have the highest wage premium for AI skills. Firms demanding AI skills more intensively also offer higher salaries in non-AI jobs.}
}

@misc{touvron2023,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@article{ma2023eureka,
  title={Eureka: Human-level reward design via coding large language models},
  author={Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2310.12931},
  year={2023}
}

@inproceedings{ma2024dreureka,
    title   = {DrEureka: Language Model Guided Sim-To-Real Transfer},
    author  = {Yecheng Jason Ma and William Liang and Hungju Wang and Sam Wang and Yuke Zhu and Linxi Fan and Osbert Bastani and Dinesh Jayaraman},
    year    = {2024},
  booktitle = {Robotics: Science and Systems (RSS)}
}

@inproceedings{sahoo2018learning,
  title={Learning equations for extrapolation and control},
  author={Sahoo, Subham and Lampert, Christoph and Martius, Georg},
  booktitle={International Conference on Machine Learning},
  pages={4442--4450},
  year={2018},
  organization={Pmlr}
}

@misc{fountas2024,
      title={Human-like Episodic Memory for Infinite Context {LLMs}}, 
      author={Zafeirios Fountas and Martin A Benfeghoul and Adnan Oomerjee and Fenia Christopoulou and Gerasimos Lampouras and Haitham Bou-Ammar and Jun Wang},
      year={2024},
      eprint={2407.09450},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}



@article{Schmidt09,
author = {Michael Schmidt  and Hod Lipson },
title = {Distilling Free-Form Natural Laws from Experimental Data},
journal = {Science},
volume = {324},
number = {5923},
pages = {81-85},
year = {2009},
doi = {10.1126/science.1165893},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1165893},
abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the “alphabet” used to describe those systems.}}



@article{xia2024chartx,
  title={ChartX \& ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning},
  author={Xia, Renqiu and Zhang, Bo and Ye, Hancheng and Yan, Xiangchao and Liu, Qi and Zhou, Hongbin and Chen, Zijun and Dou, Min and Shi, Botian and Yan, Junchi and others},
  journal={arXiv preprint arXiv:2402.12185},
  year={2024}
}

@inproceedings{Langenkamp22,
author = {Langenkamp, Max and Yue, Daniel N.},
title = {How Open Source Machine Learning Software Shapes AI},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3514094.3534167},
abstract = {If we want a future where AI serves a plurality of interests, then we should pay attention to the factors that drive its success. While others have studied the importance of data, hardware, and models in directing the trajectory of AI, we argue that open source software is a neglected factor shaping AI as a discipline. We start with the observation that almost all AI research and applications are built on machine learning open source software (MLOSS). This paper presents three contributions. First, it quantifies the outsized impact of MLOSS by using Github contributions data. By contrasting the costs of MLOSS and its economic benefits, we find that the average dollar of MLOSS investment corresponds to at least $100 of global economic value created, corresponding to $30B of economic value created this year. Second, we leverage interviews with AI researchers and developers to develop a causal model of the effect of open sourcing on economic value. We argue that open sourcing creates value through three primary mechanisms: standardization of MLOSS tools, increased experimentation in AI research, and creation of communities. Finally, we consider the incentives for developing MLOSS and the broader implications of these effects. We intend this paper to be useful for technologists and academics who want to analyze and critique AI, and policymakers who want to better understand and regulate AI systems.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {385–395},
numpages = {11},
keywords = {machine learning, open source},
location = {Oxford, United Kingdom},
series = {AIES '22}
}



@article{huang2024programming,
  title={Programming Teaching in the Era of Artificial Intelligence},
  author={Huang, Chien-Hsing},
  journal={Eximia},
  volume={13},
  pages={583--589},
  year={2024}
}




@article{Thompson23,
author = {Thompson, Neil and Manso, Gabriel and Ge, Shuning},
title = {The Importance of (Exponentially More) Computing Power},
journal = {Academy of Management Proceedings},
volume = {2023},
number = {1},
pages = {18961},
year = {2023},
doi = {10.5465/AMPROC.2023.365bp},

eprint = { 
    
        https://doi.org/10.5465/AMPROC.2023.365bp
    
    

}
,
    abstract = { Denizens of Silicon Valley have called Moore’s Law “the most important graph in human history,” and economists describe the Moore’s Law-powered I.T. revolution as one of the most important sources of national productivity. But data substantiating these claims tend to either be abstracted – for example by examining spending on I.T., rather than I.T. itself – or anecdotal. In this paper, we assemble direct evidence of the impact that computing power has had on five domains: two computing bellwethers (Chess and Go), and three economically important applications (weather prediction, protein folding, and oil exploration). Computing power explains 48\%-94\% of the performance improvements in these domains. Moreover, in line with economic theory, we find that exponential increases in computing power are needed to get linear improvements in outcomes, which helps clarify why Moore’s Law has been so important. We also discuss how this dependence on computation means that performance improvements in these domains (and presumably many others) are becoming economically tenuous as Moore’s Law breaks down. }
}




@article{Nordhaus_2007, title={Two Centuries of Productivity Growth in Computing}, volume={67}, DOI={10.1017/S0022050707000058}, number={1}, journal={The Journal of Economic History}, author={Nordhaus, William D.}, year={2007}, pages={128–159}}

@article{Bongard07,
author = {Josh Bongard  and Hod Lipson},
title = {Automated reverse engineering of nonlinear dynamical systems},
journal = {Proceedings of the National Academy of Sciences},
volume = {104},
number = {24},
pages = {9943-9948},
year = {2007},
doi = {10.1073/pnas.0609476104},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0609476104},
abstract = {Complex nonlinear dynamics arise in many fields of science and engineering, but uncovering the underlying differential equations directly from observations poses a challenging task. The ability to symbolically model complex networked systems is key to understanding them, an open problem in many disciplines. Here we introduce for the first time a method that can automatically generate symbolic equations for a nonlinear coupled dynamical system directly from time series data. This method is applicable to any system that can be described using sets of ordinary nonlinear differential equations, and assumes that the (possibly noisy) time series of all variables are observable. Previous automated symbolic modeling approaches of coupled physical systems produced linear models or required a nonlinear model to be provided manually. The advance presented here is made possible by allowing the method to model each (possibly coupled) variable separately, intelligently perturbing and destabilizing the system to extract its less observable characteristics, and automatically simplifying the equations during modeling. We demonstrate this method on four simulated and two real systems spanning mechanics, ecology, and systems biology. Unlike numerical models, symbolic models have explanatory value, suggesting that automated “reverse engineering” approaches for model-free symbolic nonlinear system identification may play an increasing role in our ability to understand progressively more complex systems in the future.}}



@article{mower2024optimal,
  title={Optimal Control Synthesis from Natural Language: Opportunities and Challenges},
  author={Mower, Christopher E and Yu, Hongzhan and Grosnit, Antoine and Peters, Jan and Wang, Jun and Bou-Ammar, Haitham},
  year={2024}
}

@article{Chartrand11,
author = {Chartrand, Rick},
title = {Numerical Differentiation of Noisy, Nonsmooth Data},
journal = {International Scholarly Research Notices},
volume = {2011},
number = {1},
pages = {164564},
doi = {https://doi.org/10.5402/2011/164564},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.5402/2011/164564},
abstract = {We consider the problem of differentiating a function specified by noisy data. Regularizing the differentiation process avoids the noise amplification of finite-difference methods. We use total-variation regularization, which allows for discontinuous solutions. The resulting simple algorithm accurately differentiates noisy functions, including those which have a discontinuous derivative.},
year = {2011}
}



@article{zolman2024sindy,
  title={SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning},
  author={Zolman, Nicholas and Fasel, Urban and Kutz, J Nathan and Brunton, Steven L},
  journal={arXiv preprint arXiv:2403.09110},
  year={2024}
}

@inproceedings{
gilpin2021chaos,
title={Chaos as an interpretable benchmark for forecasting and data-driven modelling},
author={William Gilpin},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
}

@article{deSilva2020, doi = {10.21105/joss.02104}, year = {2020}, publisher = {The Open Journal}, volume = {5}, number = {49}, pages = {2104}, author = {Brian M. de Silva and Kathleen Champion and Markus Quade and Jean-Christophe Loiseau and J. Nathan Kutz and Steven L. Brunton}, title = {PySINDy: A Python package for the sparse identification of nonlinear dynamical systems from data}, journal = {Journal of Open Source Software} }

@article{Kaptanoglu2022, doi = {10.21105/joss.03994}, year = {2022}, publisher = {The Open Journal}, volume = {7}, number = {69}, pages = {3994}, author = {Alan A. Kaptanoglu and Brian M. de Silva and Urban Fasel and Kadierdan Kaheman and Andy J. Goldschmidt and Jared Callaham and Charles B. Delahunt and Zachary G. Nicolaou and Kathleen Champion and Jean-Christophe Loiseau and J. Nathan Kutz and Steven L. Brunton}, title = {PySINDy: A comprehensive Python package for robust sparse system identification}, journal = {Journal of Open Source Software} }

@inproceedings{Vaswani17,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@inproceedings{
ascoli2024odeformer,
title={{ODEF}ormer: Symbolic Regression of Dynamical Systems with Transformers},
author={St{\'e}phane d'Ascoli and S{\"o}ren Becker and Philippe Schwaller and Alexander Mathis and Niki Kilbertus},
booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
year={2024},
}


@Article{Jumper2021,
author={Jumper, John
and Evans, Richard
and Pritzel, Alexander
and Green, Tim
and Figurnov, Michael
and Ronneberger, Olaf
and Tunyasuvunakool, Kathryn
and Bates, Russ
and {\v{Z}}{\'i}dek, Augustin
and Potapenko, Anna
and Bridgland, Alex
and Meyer, Clemens
and Kohl, Simon A. A.
and Ballard, Andrew J.
and Cowie, Andrew
and Romera-Paredes, Bernardino
and Nikolov, Stanislav
and Jain, Rishub
and Adler, Jonas
and Back, Trevor
and Petersen, Stig
and Reiman, David
and Clancy, Ellen
and Zielinski, Michal
and Steinegger, Martin
and Pacholska, Michalina
and Berghammer, Tamas
and Bodenstein, Sebastian
and Silver, David
and Vinyals, Oriol
and Senior, Andrew W.
and Kavukcuoglu, Koray
and Kohli, Pushmeet
and Hassabis, Demis},
title={Highly accurate protein structure prediction with {AlphaFold}},
journal={Nature},
year={2021},
month={Aug},
day={01},
volume={596},
number={7873},
pages={583-589},
abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50 years9. Despite recent progress10--14, existing methods fall farshort of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
issn={1476-4687},
doi={10.1038/s41586-021-03819-2},
}

@misc{zolman2024sindyrlinterpretableefficientmodelbased,
      title={SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning}, 
      author={Nicholas Zolman and Urban Fasel and J. Nathan Kutz and Steven L. Brunton},
      year={2024},
      eprint={2403.09110},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{arora2022modelbased,
title={Model-Based Reinforcement Learning with {SIND}y},
author={Rushiv Arora and Eliot Moss and Bruno Castro da Silva},
booktitle={Decision Awareness in Reinforcement Learning Workshop at ICML 2022},
year={2022},
}

@article{Brunton16b,
title = {Sparse Identification of Nonlinear Dynamics with Control (SINDYc)**SLB acknowledges support from the U.S. Air Force Center of Excellence on Nature Inspired Flight Technologies and Ideas (FA9550-14-1-0398). JLP thanks Bill and Melinda Gates for their active support of the Institute of Disease Modeling and their sponsorship through the Global Good Fund. JNK acknowledges support from the U.S. Air Force Office of Scientific Research (FA9550-09-0174).},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {18},
pages = {710-715},
year = {2016},
note = {10th IFAC Symposium on Nonlinear Control Systems NOLCOS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.249},
author = {Steven L. Brunton and Joshua L. Proctor and J. Nathan Kutz},
keywords = {Dynamical systems, control, system identification, sparse regression},
abstract = {Abstract:
Identifying governing equations from data is a critical step in the modeling and control of complex dynamical systems. Here, we investigate the data-driven identification of nonlinear dynamical systems with inputs and forcing using regression methods, including sparse regression. Specifically, we generalize the sparse identification of nonlinear dynamics (SINDY) algorithm to include external inputs and feedback control. This method is demonstrated on examples including the Lotka-Volterra predator-prey model and the Lorenz system with forcing and control. We also connect the present algorithm with the dynamic mode decomposition (DMD) and Koopman operator theory to provide a broader context.}
}

@article{Mengge24,
    author = {Du, Mengge and Chen, Yuntian and Wang, Zhongzheng and Nie, Longfeng and Zhang, Dongxiao},
    title = {Large language models for automatic equation discovery of nonlinear dynamics},
    journal = {Physics of Fluids},
    volume = {36},
    number = {9},
    pages = {097121},
    year = {2024},
    month = {09},
    abstract = {Equation discovery aims to directly extract physical laws from data and has emerged as a pivotal research domain in nonlinear systems. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require handcrafted representation rules and complex optimization algorithms. In this paper, we introduce a novel framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically extracting governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse candidate equations in string form and then evaluate the generated equations based on observations. The best equations are preserved and further refined iteratively using the reasoning capacity of LLMs. We propose two alternately iterated strategies to collaboratively optimize the generated equations. The first strategy uses LLMs as a black-box optimizer to achieve equation self-improvement based on historical samples and their performance. The second strategy instructs LLMs to perform evolutionary operations for a global search. Experiments are conducted on various nonlinear systems described by partial differential equations, including the Burgers equation, the Chafee–Infante equation, and the Navier–Stokes equation. The results demonstrate that our framework can discover correct equations that reveal the underlying physical laws. Further comparisons with state-of-the-art models on extensive ordinary differential equations showcase that the equations discovered by our framework possess physical meaning and better generalization capability on unseen data.},
    issn = {1070-6631},
    doi = {10.1063/5.0224297},
    eprint = {https://pubs.aip.org/aip/pof/article-pdf/doi/10.1063/5.0224297/20152367/097121\_1\_5.0224297.pdf},
}





@misc{grosnit2024largelanguagemodelsorchestrating,
      title={Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level}, 
      author={Antoine Grosnit and Alexandre Maraval and James Doran and Giuseppe Paolo and Albert Thomas and Refinath Shahul Hameed Nabeezath Beevi and Jonas Gonzalez and Khyati Khandelwal and Ignacio Iacobacci and Abdelhakim Benechehab and Hamza Cherkaoui and Youssef Attia El-Hili and Kun Shao and Jianye Hao and Jun Yao and Balazs Kegl and Haitham Bou-Ammar and Jun Wang},
      year={2024},
      eprint={2411.03562},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@Article{Mnih2015,
author={Mnih, Volodymyr
and Kavukcuoglu, Koray
and Silver, David
and Rusu, Andrei A.
and Veness, Joel
and Bellemare, Marc G.
and Graves, Alex
and Riedmiller, Martin
and Fidjeland, Andreas K.
and Ostrovski, Georg
and Petersen, Stig
and Beattie, Charles
and Sadik, Amir
and Antonoglou, Ioannis
and King, Helen
and Kumaran, Dharshan
and Wierstra, Daan
and Legg, Shane
and Hassabis, Demis},
title={Human-level control through deep reinforcement learning},
journal={Nature},
year={2015},
month={Feb},
day={01},
volume={518},
number={7540},
pages={529-533},
abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
issn={1476-4687},
doi={10.1038/nature14236},
}



@article{Fang18,
author = {Fang Ren  and Logan Ward  and Travis Williams  and Kevin J. Laws  and Christopher Wolverton  and Jason Hattrick-Simpers  and Apurva Mehta },
title = {Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments},
journal = {Science Advances},
volume = {4},
number = {4},
pages = {eaaq1566},
year = {2018},
doi = {10.1126/sciadv.aaq1566},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aaq1566},
abstract = {Coupling artificial intelligence with high-throughput experimentation accelerates discovery of amorphous alloys. With more than a hundred elements in the periodic table, a large number of potential new materials exist to address the technological and societal challenges we face today; however, without some guidance, searching through this vast combinatorial space is frustratingly slow and expensive, especially for materials strongly influenced by processing. We train a machine learning (ML) model on previously reported observations, parameters from physiochemical theories, and make it synthesis method–dependent to guide high-throughput (HiTp) experiments to find a new system of metallic glasses in the Co-V-Zr ternary. Experimental observations are in good agreement with the predictions of the model, but there are quantitative discrepancies in the precise compositions predicted. We use these discrepancies to retrain the ML model. The refined model has significantly improved accuracy not only for the Co-V-Zr system but also across all other available validation data. We then use the refined model to guide the discovery of metallic glasses in two additional previously unreported ternaries. Although our approach of iterative use of ML and HiTp experiments has guided us to rapid discovery of three new glass-forming systems, it has also provided us with a quantitatively accurate, synthesis method–sensitive predictor for metallic glasses that improves performance with use and thus promises to greatly accelerate discovery of many new metallic glasses. We believe that this discovery paradigm is applicable to a wider range of materials and should prove equally powerful for other materials and properties that are synthesis path–dependent and that current physiochemical theories find challenging to predict.}}



@Article{LeCun2015,
author={LeCun, Yann
and Bengio, Yoshua
and Hinton, Geoffrey},
title={Deep learning},
journal={Nature},
year={2015},
month={May},
day={01},
volume={521},
number={7553},
pages={436-444},
abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
issn={1476-4687},
doi={10.1038/nature14539},
}



@Article{Davies2021,
author={Davies, Alex
and Veli{\v{c}}kovi{\'{c}}, Petar
and Buesing, Lars
and Blackwell, Sam
and Zheng, Daniel
and Toma{\v{s}}ev, Nenad
and Tanburn, Richard
and Battaglia, Peter
and Blundell, Charles
and Juh{\'a}sz, Andr{\'a}s
and Lackenby, Marc
and Williamson, Geordie
and Hassabis, Demis
and Kohli, Pushmeet},
title={Advancing mathematics by guiding human intuition with AI},
journal={Nature},
year={2021},
month={Dec},
day={01},
volume={600},
number={7887},
pages={70-74},
abstract={The practice of mathematics involves discovering patterns and using these to formulate and prove conjectures, resulting in theorems. Since the 1960s, mathematicians have used computers to assist in the discovery of patterns and formulation of conjectures1, most famously in the Birch and Swinnerton-Dyer conjecture2, a Millennium Prize Problem3. Here we provide examples of new fundamental results in pure mathematics that have been discovered with the assistance of machine learning---demonstrating a method by which machine learning can aid mathematicians in discovering new conjectures and theorems. We propose a process of using machine learning to discover potential patterns and relations between mathematical objects, understanding them with attribution techniques and using these observations to guide intuition and propose conjectures. We outline this machine-learning-guided framework and demonstrate its successful application to current research questions in distinct areas of pure mathematics, in each case showing how it led to meaningful mathematical contributions on important open problems: a new connection between the algebraic and geometric structure of knots, and a candidate algorithm predicted by the combinatorial invariance conjecture for symmetric groups4. Our work may serve as a model for collaboration between the fields of mathematics and artificial intelligence (AI) that can achieve surprising results by leveraging the respective strengths of mathematicians and machine learning.},
issn={1476-4687},
doi={10.1038/s41586-021-04086-x},
}

@article{MESSENGER2021110525,
title = {Weak SINDy for partial differential equations},
journal = {Journal of Computational Physics},
volume = {443},
pages = {110525},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110525},
author = {Daniel A. Messenger and David M. Bortz},
keywords = {Data-driven model selection, Partial differential equations, Weak solutions, Sparse recovery, Galerkin method, Convolution},
abstract = {Sparse Identification of Nonlinear Dynamics (SINDy) is a method of system discovery that has been shown to successfully recover governing dynamical systems from data [6], [39]. Recently, several groups have independently discovered that the weak formulation provides orders of magnitude better robustness to noise. Here we extend our Weak SINDy (WSINDy) framework introduced in [28] to the setting of partial differential equations (PDEs). The elimination of pointwise derivative approximations via the weak form enables effective machine-precision recovery of model coefficients from noise-free data (i.e. below the tolerance of the simulation scheme) as well as robust identification of PDEs in the large noise regime (with signal-to-noise ratio approaching one in many well-known cases). This is accomplished by discretizing a convolutional weak form of the PDE and exploiting separability of test functions for efficient model identification using the Fast Fourier Transform. The resulting WSINDy algorithm for PDEs has a worst-case computational complexity of O(ND+1log⁡(N)) for datasets with N points in each of D+1 dimensions. Furthermore, our Fourier-based implementation reveals a connection between robustness to noise and the spectra of test functions, which we utilize in an a priori selection algorithm for test functions. Finally, we introduce a learning algorithm for the threshold in sequential-thresholding least-squares (STLS) that enables model identification from large libraries, and we utilize scale invariance at the continuum level to identify PDEs from poorly-scaled datasets. We demonstrate WSINDy's robustness, speed and accuracy on several challenging PDEs. Code is publicly available on GitHub at https://github.com/MathBioCU/WSINDy_PDE.}
}

@article{Shea_2021,
   title={SINDy-BVP: Sparse identification of nonlinear dynamics for boundary value problems},
   volume={3},
   ISSN={2643-1564},
   DOI={10.1103/physrevresearch.3.023255},
   number={2},
   journal={Physical Review Research},
   publisher={American Physical Society (APS)},
   author={Shea, Daniel E. and Brunton, Steven L. and Kutz, J. Nathan},
   year={2021},
   month=jun }

@article{Fasel_2022,
   title={Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control},
   volume={478},
   ISSN={1471-2946},
   DOI={10.1098/rspa.2021.0904},
   number={2260},
   journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
   publisher={The Royal Society},
   author={Fasel, U. and Kutz, J. N. and Brunton, B. W. and Brunton, S. L.},
   year={2022},
   month=apr }


@article{kaheman2020sindy,
  title={SINDy-PI: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics},
  author={Kaheman, Kadierdan and Kutz, J Nathan and Brunton, Steven L},
  journal={Proceedings of the Royal Society A},
  volume={476},
  number={2242},
  pages={20200279},
  year={2020},
  publisher={The Royal Society Publishing}
}

@Article{Degrave2022,
author={Degrave, Jonas
and Felici, Federico
and Buchli, Jonas
and Neunert, Michael
and Tracey, Brendan
and Carpanese, Francesco
and Ewalds, Timo
and Hafner, Roland
and Abdolmaleki, Abbas
and de las Casas, Diego
and Donner, Craig
and Fritz, Leslie
and Galperti, Cristian
and Huber, Andrea
and Keeling, James
and Tsimpoukelli, Maria
and Kay, Jackie
and Merle, Antoine
and Moret, Jean-Marc
and Noury, Seb
and Pesamosca, Federico
and Pfau, David
and Sauter, Olivier
and Sommariva, Cristian
and Coda, Stefano
and Duval, Basil
and Fasoli, Ambrogio
and Kohli, Pushmeet
and Kavukcuoglu, Koray
and Hassabis, Demis
and Riedmiller, Martin},
title={Magnetic control of tokamak plasmas through deep reinforcement learning},
journal={Nature},
year={2022},
month={Feb},
day={01},
volume={602},
number={7897},
pages={414-419},
abstract={Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak {\`a} Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
issn={1476-4687},
doi={10.1038/s41586-021-04301-9},
}

