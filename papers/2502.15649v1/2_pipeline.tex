\section{Proposed Reinforcement Learning Pipeline}
\label{sec:pipeline}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth, trim=0cm 0cm 0cm 0.4cm, clip]{figures/training_pipeline_2.pdf}
    \caption{The proposed training pipeline involves three stages with increasing levels of complexity from left to right. Each stage is optional and can be revisited with different parameters until they pass a predefined performance criteria. %Each stage follows the RL data flow illustrated in Fig.\ \ref{fig:core_RL_diagram}. However,
    Each stage receives as input a policy and outputs a modified policy, allowing incremental improvement until the final policy is achieved.}
    \label{fig:training_pipeline}
\end{figure*}


The pipeline is proposed as a multi-stage process to turn RL policies into real-world applications as presented in Fig.~\ref{fig:simplified_diagram}, with four phases: system identification, core simulation training, high-fidelity training, and real-life training, which are explained further in this section.
Fig.\ \ref{fig:training_pipeline} describes the data flow and interactions between the RL agent and the environments during training. This design allows the customization of each phase to meet the specific requirements of a task. For example, tasks that involve manipulating physical objects (e.g., a Rubikâ€™s cube~\cite{akkaya2019solving}) may benefit from utilizing all stages of training. In contrast, more straightforward tasks with no interaction with other objects, like the position control of a wheeled vehicle, might only require the system identification and core simulation stages. This flexibility is valuable, especially for researchers and industry practitioners new to RL.

The following subsections provide a detailed description of each component and their interactions in the proposed pipeline.



%The proposed pipeline, as shown in Fig.\ \ref{fig:simplified_diagram} and \ref{fig:training_pipeline}, is designed as a multi-stage process that transitions RL policies from simplified simulation environments to high-fidelity simulations and ultimately to real-world applications. Each phase can be customized to meet the specific requirements of a robotic task or even omitted, allowing practitioners to modify each component based on task complexity and available resources. This structured methodology can be valuable for researchers and industry practitioners new to RL. 

%The following subsections provide a detailed description of each component in the proposed pipeline.

\subsection{System Identification}

This stage focuses on obtaining a data-based model of the robot to be applied in the following stages. Although optional, this step can be crucial to reduce the reality gap by considering the robot's physical parameters in the simulators used in the following stages. The system identification field is vast, and techniques include identifying linear or nonlinear models that convert inputs to outputs, frequency response analysis, machine learning, and many others. %It could also be used to identify noise distributions and uncertainty levels. % in the robot. 
A good resource on system identification is provided in~\cite{brunton2022data}. Section \ref{sec:case_study} provides further details on the system identification technique used in the presented case study. 

\subsection{Core Simulation Training}

This is the first training stage in Fig.\ \ref{fig:training_pipeline}, and it allows the RL agent to learn within a simplified simulation environment. This phase could involve modelling either the kinematic or dynamic equations of motions. For example, an RL agent could be trained to control a differential-drive robot by modelling the kinematic equations while ignoring factors like friction, motor mismatch, and other sources of error, such as in \cite{hu2021sim}, where RL is applied to control a wheeled robot to navigate in rough terrain. While this simplified approach might allow the policy to transfer to the actual robot with minimal training, the transfer could also result in policy degradation. Incorporating system identification parameters into the simulation could ensure smoother transfer and more reliable performance.


The passing criteria in this and subsequent stages can be a numerical function that determines if the model passes a predefined performance score or based on practitioners' prior experience to assess model feasibility. Examples of performance metrics include the success rate for goal-based problems or the accumulated reward.
% or a decision made by the practitioner about whether to continue improving the model with further changes in the training process or to move to a future stage. 
The tools used in this stage include the gymnasium API developed for creating RL simulators and physics simulators, such as Bullet and MuJoCo, that can be used to implement rigid body dynamics.

\subsection{High-Fidelity Training}

The second training stage in Fig.\ \ref{fig:training_pipeline} involves high-fidelity simulation, incorporating realism such as gravity, friction, actuator latency and dead zones, sensor noise, and realistic renderings. This reduces the sim-to-real gap, critical for complex RL tasks. With high-fidelity simulators, domain randomization is a powerful technique to further reduce the sim-to-real gap. For example, domain randomization (e.g., cube size, friction, force ranges, inertia, and action latency) enhanced the training process in the Rubik's cube robot manipulation task~\cite{akkaya2019solving}, where it enabled successful transfer into the real robot.

Common tools used in this stage, such as Gazebo, CoppeliaSim and Isaac Sim, offer advanced features like accurate sensor modelling, real-robot input/output matching and Robot Operating System (ROS) support, enabling near-seamless transitions between simulated and real environments.

\subsection{Real-Life Training}

In the final phase, the RL model is deployed on the physical robot, and the performance degradation is evaluated. If performance is inadequate, fine-tuning the model with real-world data or addressing discrepancies through high-fidelity simulation, using either domain randomization or domain adaptation, can reduce the sim-to-real gap. This iterative process continues until the model meets the desired performance criteria.

% \subsection{Incorporating other RL methodologies in our pipeline}
% \textcolor{red}{I was planning to explain how to apply the following concepts while following the pipeline:
% Domain randomization
% Domain adaptation
% Curriculum learning
% . However, I am running out of space and I think this is not essential for the paper so I am thinking of ignoring this subsection for now.}


\subsection{Debugging RL Applications with the Pipeline}

RL requires careful integration of components, including actions, observations, reward signals, simulation, RL algorithms, and well-tuned hyperparameters. Complex robotics tasks with continuous observations and actions often require multiple neural networks, further increasing the number of hyperparameters that must be optimized.

More often than not, the initial training fails to converge to a useful policy, with issues stemming from suboptimal hyperparameters, insufficient observations, or neural networks that need more neurons or layers. In these situations, a common solution is to start with a simplified simulation and minimal observations and actions, making it easier to find workable parameters. This process can then iterate, gradually increasing task complexity or progressing through the pipeline stages until the desired performance is obtained.



