\section{Introduction}
\label{sec:intro}

In recent years, Reinforcement Learning (RL) has achieved high-level performance in solving challenging robotic tasks. %robot manipulation tasks~\cite{akkaya2019solving}.
While RL itself is not new (with roots dating back to the 1960s~\cite{minsky1961steps}), its recent successes can be attributed to several factors: the availability of powerful and affordable processing units (such as CPUs and GPUs), advancements in deep learning techniques, and the development of new deep RL methods.

The RL methodology relies on three core concepts: the agent, the reward signal and the environment. The agent is the decision-maker that learns by interacting with the environment. The reward signal defines the goal of the RL problem, serving as feedback to encourage desirable behaviours and discourage undesirable ones. The environment is where the agent operates, providing observations in response to the agent's actions~\cite{sutton2018reinforcement}. The agent gathers experience to learn the appropriate actions and solve the RL problem. This is an iterative approach that can be directly applied to a physical robot. % Fig.\ \ref{fig:core_RL_diagram} illustrates the interactions between these concepts.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth, trim=0cm 0cm 0cm 0.35cm, clip]{figures/RL_diagram.pdf}
%     \caption{Core reinforcement learning diagram representing the interaction between the agent and the environment.}
%     \label{fig:core_RL_diagram}
% \end{figure}

%In robotic applications, the environment includes not only the space where the robot operates but also the robot itself, and the agent is the decision-making algorithm or policy that sends commands via the robot.  However, it is common to refer to the robot along with the learning algorithm as the agent and the environment as everything external to the robot. This paper adopts this latter convention to distinguish the robot from its surroundings.

Unfortunately, applying RL directly in real-life applications is often prohibitive, requiring the robot to collect data by iteratively acting on the environment~\cite{levine2020offline}. This iterative process can be costly and time-consuming. There may be safety concerns because the robot may take unexpected actions during training, possibly causing damage to itself or its surroundings.

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/simplified_diagram_2.pdf}
    \caption{Simplified diagram describing the components of the proposed RL pipeline. Each component is optional, and the combination of the stages depends on the problem's complexity. 
    %A simple RL problem could only utilize the core simulation training stage, while more complicated approaches could take advantage of the whole pipeline.
    }
    \label{fig:simplified_diagram}
\end{figure*}

The success of RL in robotics is partly due to the development of a methodology known as off-policy RL, which enables offline training~\cite{levine2020offline} where the RL training process happens after data collection. Algorithms such as Q-learning~\cite{sutton2018reinforcement}, SAC~\cite{haarnoja2018soft}, and TD3~\cite{fujimoto2018addressing} are examples of off-policy RL. Another advantage of off-policy RL is that it allows for the use of simulated data during training. Unfortunately, this approach introduces other challenges, because discrepancies between the simulated and the real world may cause the robot to perform worse when compared to its simulated counterpart. This problem, often referred to by various terms such as the reality gap, sim-to-real gap, and sim2real gap~\cite{jakobi1995noise}, arises because off-policy RL is usually trained mostly, if not entirely, in simulation. Some studies focus on reducing this problem~\cite{chebotar2019closing, calderon2024deep, zhao2020sim}, and strategies they apply include (\textit{i}) creating high-fidelity simulations with parameters modelled directly from the robot, and (\textit{ii}) randomizing the simulation parameters to generate a policy that is robust to environmental variations. %More details on common approaches are discussed in Section \ref{sec:related}.

Considering these methodologies, this work presents a pipeline to facilitate the development of RL models on real-world robotic systems. %We introduce the pipeline through two diagrams: 
A simplified overview of each phase's components is shown in Fig.\ \ref{fig:simplified_diagram}. %, and a more detailed block diagram outlining inputs, outputs, and data flow in Section \ref{sec:pipeline}. 
The proposed pipeline combines established methodologies from the RL literature into a staged process that involves system identification, training with different levels of simulation complexity, and real-world deployment. 
% Each stage is optional and introduces increasing complexity. 
~While the individual techniques included in the pipeline are not novel, this paper's contribution lies in a systematic approach to the learning process. Unlike most studies, which often detail only the specific steps for their application, this pipeline offers a modular framework to guide practitioners in adapting RL models to real-world scenarios. To support this, a case study demonstrating the successful deployment of an RL policy on a mobile robot is also presented.

The remainder of the paper is organized as follows. Section~\ref{sec:related} presents common approaches to reduce the reality gap; Section \ref{sec:pipeline} describes the proposed pipeline and discusses its application in robotics; Section \ref{sec:case_study} describes the steps taken to train an RL policy using the proposed pipeline; Section \ref{sec:application} illustrates the results in a surveillance application. Finally, Section \ref{sec:conclusion} presents the final remarks.

\section{Related Work}
\label{sec:related}

This section presents previous studies on the simulation-to-reality gap and links these concepts to the proposed pipeline. For this purpose, it is necessary to define the concept of environment used in this paper. In robotic RL applications, the environment includes not only the space where the robot operates but also the robot itself, and the agent is the decision-making algorithm or policy that sends commands via the robot.  However, it is also common to refer to the robot along with the learning algorithm as the agent and the environment as everything external to the robot. This paper adopts this latter convention to distinguish the robot from its surroundings.

Research on RL in robotic systems has led to various methodologies for improving sample efficiency of RL algorithms and reducing the sim-to-real problem. The sample efficiency in RL refers to the number of samples (i.e., experiences) needed to achieve a certain performance level. A comprehensive survey on the topic is presented by Calder√≥n-Cordova et al.~\cite{calderon2024deep}. They present many techniques, frameworks, tools, and a practical guide for developing RL control applications focused on robotic manipulators. Even though their work is extensive, they propose a simple pipeline containing a single simulation stage with no discussion of the various levels of simulation complexity and their effects on the reality gap. 

In contrast,~\cite{zhu2021survey} presents a survey on RL applied to bio-inspired robots that categorize RL methodologies into four main groups: 1) methods that rely on accurate simulators; 2) approaches that only use simplified kinematic or dynamic models; 3) techniques that apply RL on top of hierarchical controllers; and 4) methods that leverage human demonstrations. They also mention that while methodologies in group 1 often perform better after a sim-to-real transfer, they are less sample-efficient than the others. Our proposed pipeline supports not only a single simulation stage, as presented in~\cite{calderon2024deep} but also allows for multi-level simulation complexities. This includes options to integrate high-fidelity simulators and simplified kinematic or dynamic models as core simulators.

In another survey, Zhao et al.~\cite{zhao2020sim} discuss not only the potential benefits of high-fidelity simulation in improving the sim-to-real transfer but also present three other approaches: system identification to create a simulator tailored to a specific robot, domain randomization, and domain adaptation. Domain randomization methods involve modelling parameters from reality and randomizing their values in the simulator to cover the actual distribution of these parameters in the real world. On the other hand, domain adaptation involves the combination of two or more environments during the training process. For example, one could train a model in a high-fidelity simulator and then transfer the model to training on real-world data.

% Successful examples of domain randomization include the works of Huber et al.~\cite{huber2024domain}, Peng et al.~\cite{peng2018sim}, and Akkaya et al.~\cite{akkaya2019solving}. 
Several works successfully use domain randomization~\cite{huber2024domain,peng2018sim,akkaya2019solving}. In~\cite{huber2024domain}, a feedback approach is used to change the simulation parameters based on how the real robot performs with the transferred policy, thus allowing for automatic randomization to achieve high performance on the transferred policy. In contrast,~\cite{peng2018sim} performs a comprehensive randomization of parameters (e.g., table size, arm dynamics, controller gains, links' mass, friction, noise levels, and time steps) on a robotic arm application, %such as modifying table size, dynamics of the arm, controller gains, mass of the links,  friction coefficients, noise levels, time step variations, 
totalling 95 randomized parameters, resulting in a successful sim-to-real transfer without additional training on the physical system. Using a combination of system identification techniques, domain randomization and curriculum learning, \cite{akkaya2019solving} obtained a remarkable level of dexterity in a robotic hand when trained to solve the Rubik's cube.

% The task of controlling a robotic hand to solve the Rubik's cube is discussed in~\cite{akkaya2019solving}.The level of dexterity obtained was remarkable due to a combination of system identification techniques, domain randomization and curriculum learning .%, where they increased the task's difficulty as training progressed. 

Domain adaptation techniques involve converting input data from one domain into another, where most training is performed. For example,~\cite{james2019sim} develops a machine-learning model that converts the visual feedback data from the real world into a format that resembles simulated inputs. This translation technique allows a model trained in simulation to perform well on physical robots. Similar strategies are used in~\cite{bousmalis2018using}.

Note that these methodologies are not always necessary. For example,~\cite{hu2021sim} presents a sim-to-real pipeline to address the challenge of robot navigation in 3D cluttered environments. Their pipeline includes a simulation stage that matches the inputs and outputs to the real robot, and uses simulated sensors and state estimation techniques that closely matched the real ones. These steps achieved successful real-world transfer without requiring any adaptation or additional training. While control problems like these could be solved with classical control techniques, RL provides a data-based adaptive methodology that is independent of the robot model.

Ultimately, the requirements of the RL process depend on the complexity of the task. If the robot is passively stable and accepts simple commands, such as linear and angular velocities, it is possible to transfer the learned model without further adaptation~\cite{hu2021sim}. However, more complex problems, such as solving a Rubik's cube with 24 degrees of freedom, require more steps to achieve acceptable performance in real applications~\cite{akkaya2019solving}. %The proposed pipeline includes many components used in their applications, supporting various levels of system complexity.





