\section{Case Study on a Mobile Robot}
\label{sec:case_study}

This section presents a case study on applying the discussed concepts to obtain an RL policy for the Boston Dynamics Spot robot. Spot, an agile legged robot, autonomously computes gait and foot placement on linear ($a_x$), lateral ($a_y$) and angular body velocities ($a_\theta$) commands. The objective was to train an RL model to control the robot's position and orientation to reach a desired configuration while optimizing a cost function. This controller was then used in a surveillance application.

\subsection{System Identification}
\label{sec:system_id}

In this stage, the objective was to reduce the sim-to-real gap by modelling the robot's motion constraints. The robot cannot perfectly execute commanded velocities because converting desired speeds into leg movements and actuation limits introduces errors. Training an RL agent solely on commanded velocities $\mathbf{a} = \left(a_x,a_y,a_\theta\right)$ risks poor control because these may not align with what the robot can physically achieve.

The first step was identifying the velocities the robot could execute. To do so, a grid of commanded body velocities $\mathbf{a}$  was created (Fig.\ \ref{fig:action_set_a}), and the executed body velocities $\mathbf{v} = \left(v_x,v_y,v_\theta\right)$ were measured using a motion capture system, with grid ranges shown in Table \ref{tab:system_params}. Note that $a_x$ is not symmetrical because the robot moves faster forward than backward, adding control complexity, which the RL agent can still learn to handle.

\begin{figure}%[t] 
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth, trim=5.2cm 1.3cm 3.2cm 2.0cm, clip]{figures/action_set_input.pdf}
    \caption{} 
    \label{fig:action_set_a} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth, trim=5.0cm 1.3cm 3.4cm 1.0cm, clip]{figures/action_set_output.pdf} 
    \caption{} 
    \label{fig:action_set_b}
  \end{subfigure}%%
  \caption{Action set of the Spot robot. (a) Nominal and (b) feasible velocities with overlaid approximated velocities. }
  \label{fig:action_set} 
\end{figure}

\begin{figure}%[t]
\centering\includegraphics[width=0.9\linewidth]{figures/system_id_spot_h.pdf}
    \caption{The executed velocities of the real robot are approximated using a polynomial function approximator.}
    \label{fig:fuction_approximation}
\end{figure}

The system identification process involved finding a function that approximates the executed velocities from commanded velocities (Fig.~\ref{fig:fuction_approximation}), represented as
\begin{equation}
    \hat{\mathbf{v}} = \hat{\mathbf{f}}(\mathbf{a}),
\end{equation}
where  $\hat{\mathbf{v}}$ is the approximated body velocity and $\hat{\mathbf{f}}(\mathbf{a})$ is a multivariate function with a third-order polynomial with no bias term in each dimension of $\mathbf{a}$, resulting in:

\begin{equation}
    \hat{\mathbf{v}} = \begin{bmatrix}
           \hat{v}_x \\
           \hat{v}_y \\
           \hat{v}_{\theta}
         \end{bmatrix}
     =
     \begin{bmatrix}
         \hat{f}_x(\mathbf{a})\\
         \hat{f}_y(\mathbf{a})\\
         \hat{f}_\theta(\mathbf{a})
     \end{bmatrix},
\end{equation}
where $\hat{f}_x(\mathbf{a})$, $\hat{f}_y(\mathbf{a})$, and $  \hat{f}_\theta(\mathbf{a})$ are the polynomial functions that approximate the executed velocities in each dimension. For clarity, the approximation for $\hat{v}_x$ is given by
\begin{equation}
    \hat{v}_x = \hat{f}_x(\mathbf{a}) = \sum_{0 < i+j+l \leq 3} c_{x,ijl} (a^i_x a^j_y a^l_\theta),
\end{equation}
 where $i, j, l \in \mathbb{Z}_{\geq 0}$ and the coefficients $c_{x,ijl}$ are computed using the least squares method that minimizes the total squared error
\begin{equation}
    J_x = \sum_{n=0}^{N-1} (v_{x,n} - \hat{v}_{x,n})^2,
    % J = (\mathbf{v} - \hat{\mathbf{v}})(\mathbf{v} - \hat{\mathbf{v}})^\top.
\end{equation}
where $N$ represents the total number of samples in the grid (Fig. \ref{fig:action_set_a}), and the $n$ index represents the $n$-th element. The coefficients of $\hat{v}_y$ and $\hat{v}_{\theta}$ are computed similarly. 

Fig.\ \ref{fig:action_set_b} shows the linear regression results with the approximated velocities overlaid on the executed ones, illustrating a good fit from the polynomial regression.

\subsection{RL Formulation}

The problem was modelled as a goal-conditioned Markov Decision Process (MDP) $\langle S, G, A, T, R \rangle$, where $S$ is the state space, $G$ the goal space, $A$ the action space, $T: S\times A \rightarrow S$ the state transition function, and $R(\mathbf{a}, \mathbf{s}, \mathbf{g}),~R: S \times A\rightarrow\mathbb{R}$ the reward function that returns a scalar when taking action $\mathbf{a}$ and arriving at state $\mathbf{s}$ with goal $\mathbf{g}$. The goal is to find a policy $\pi(\mathbf{a}|\mathbf{s},\mathbf{g})$ that maximizes the agent's cumulative reward~\cite{antonyshyn2023multiple}. The policy $\pi(\mathbf{a}|\mathbf{s},\mathbf{g})$ defines a probability distribution over the action space $A$ given a combination of state $\mathbf{s}$ and goal $\mathbf{g}$. The policy can be applied probabilistically, where each action $\mathbf{a}$ is a sample of the probability distribution, or deterministically, where $\mathbf{a}$ is the distribution mean.

\subsubsection{State and Goal Spaces}

Since the goal of the RL agent is to control the position and orientation of the robot, the state was defined as
\begin{equation}
    \mathbf{s} = (x, y, \theta)\in\mathbb{R}^2\times[-\pi,\pi).
\end{equation}
Similarly, the goal is defined as
\begin{equation}
    \mathbf{g} = (x_g,y_g,\theta_g) \in[r_{\rm min},r_{\rm max}]^2\times[-\pi,\pi),
\end{equation}
where $r_{\rm min}$ and $r_{\rm max}$ represent the lower and upper limits of $x_g$ and $y_g$. Note that the state and goal combination form the observation $\mathbf{o} = (\mathbf{s},\mathbf{g})$, but separating them into state and goal vectors is more intuitive.

\subsubsection{Action Space}

The commanded action was defined as 
\begin{equation}
    \mathbf{a} = (a_x, a_y, a_\theta) \in \mathbb{R}^3.
\end{equation}
They represent the linear, lateral and angular body velocities commands sent to the robot. 

\subsubsection{Simulation and Transition Function}

%This section presents the simulator used in our pipeline's core simulation stage. 
The simulator used in the core simulation stage converts the desired action at time step $k$ into the identified executed action, as shown in Section \ref{sec:system_id}, and computes a new state resulting from the applied action. The new state is obtained through a first-order integration of the executed velocity as
\begin{align}
    \hat{\mathbf{v}}_k &= \hat{f}(\mathbf{a}_k) \\
    \mathbf{s}_k &= \mathbf{s}_{k-1} + \hat{\mathbf{v}}_k\Delta t,
\end{align}
with $\Delta t$ being the step duration.

Since this is a goal-conditioned MDP, our simulator must also inform when the robot reaches $\mathbf{g}$. To do so, the error in position and orientation are computed
\begin{align}
    e_{p,k} &= \|\mathbf{p}_g-\mathbf{p}_{s,k}\|_2, \\
    e_{\theta,k} &= |\theta_g - \theta_{k}|,
\end{align}
where $\mathbf{p}_g = (x_g, y_g)$, $\mathbf{p}_{s,k} =(x_k, y_k)$, and $||\cdot||_2$ represents the Euclidean norm. Then, the robot successfully reaches $\mathbf{g}$ when $e_{p,k} < \epsilon_p$ and $e_{\theta,k} < \epsilon_\theta$, where $\epsilon_p$ and $\epsilon_\theta$ are tolerance parameters that define the required proximity to the goal.

\subsubsection{Reward Function}

The reward function for this study was chosen based on common costs used in model predictive control applications. For interested readers on this topic, the authors of \cite{song2023reaching} present an insightful comparison between optimal control and reinforcement learning techniques and their costs. Our objective was to optimize a policy that minimizes control actions, action smoothness, and time, resulting in the following cost function:
\begin{equation}
\label{eq:cost_function}
J = \sum_{k=0}^{N-1}\left(\|\mathbf{u}_k\|_{\mathbf{R}}^2 + \|\mathbf{u}_k-\mathbf{u}_{k-1}\|_{\mathbf{S}}^2 + \lambda_k\right),
\end{equation}
where the notation $\|\mathbf{z}\|^2_\mathbf{M}= \mathbf{z}^\top \mathbf{M} \mathbf{z}$ represents the weighted Euclidean norm, with $\mathbf{z}$ as a column vector and $\mathbf{M}$ a square matrix matching the dimension of $\mathbf{z}$. The parameters $\mathbf{R}$, $\mathbf{S}$ prioritize action magnitude and variation, respectively, while $\lambda_k$ is a time-varying factor that encourages faster task completion. For example, the parameters $\lambda_k = 1$ and $\mathbf{R} = \mathbf{S} = \mathbf{0}$ define the minimum time optimal control problem \cite{rosolia2021minimum}.

Since RL maximizes a reward rather than minimizing a cost, the cost function in \eqref{eq:cost_function} is converted into a reward as follows,
\begin{equation}
\label{eq:reward}
    r_k =  -\left(\|\mathbf{u}_k\|_{\mathbf{R}}^2 + \|\mathbf{u}_k-\mathbf{u}_{k-1}\|_{\mathbf{S}}^2 + \lambda_k\right).
\end{equation}

Maximizing \eqref{eq:reward} over time corresponds to minimizing the cost in \eqref{eq:cost_function}. However, to further encourage the RL agent to reach the goal state, the time penalty is removed once the robot is within a specified proximity to the goal. Thus, we define:
\begin{equation}
\lambda_k = 
\begin{cases} 
      0, & \text{if }  e_{p,k} < \epsilon_p \text{ and } e_{\theta,k} < \epsilon_\theta \\ 
      1, & \text{otherwise} 
   \end{cases}
\end{equation}


\subsubsection{Neural Network Architecture}

In recent years, several algorithms, like DDPG~\cite{lillicrap2015continuous}, SAC~\cite{haarnoja2018soft}, and TD3~\cite{fujimoto2018addressing}, have been created for continuous state and action spaces.  Considering the pool of deep RL algorithms, we chose the Soft Actor-Critic Algorithm (SAC)~\cite{haarnoja2018soft} due to being easy to find hyperparameters that lead to good policies. The stochastic nature of SAC also helps with environment exploration, which reduces the number of parameters to tune. The RL training parameters and network architectures used are listed in Table~\ref{tab:system_params}, and a visual representation of the network architecture for the actor-critic framework is presented in Fig.\ \ref{fig:actor_critic_architecture}.

\begin{figure} 
  % \begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth, trim=0cm 0cm 0cm 0cm, clip]{figures/actor_critic_architecture_2.pdf}
  %   \caption{} 
  %   \label{fig:a} 
  % \end{subfigure}%% 
  % \vspace{0.3cm} % Adjust vertical spacing as needed
  % \begin{subfigure}{0.99\linewidth}
  %   \centering
  %   \includegraphics[width=0.5\linewidth, trim=0.8cm 0.6cm 0.5cm 1.5cm, clip]{figures/RL_actor_critic_diagram.pdf} 
  %   \caption{} 
  %   \label{fig:c}
  % \end{subfigure}%%
  \caption{Actor-critic network architecture}
  \label{fig:actor_critic_architecture} 
\end{figure}

\begin{figure*}
  \begin{subfigure}[b]{0.33\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth, trim=0.3cm 0.3cm 0.2cm 0.1cm, clip]{figures/Gymnasium_simulation__theta_g__0.pdf}
    \caption{Core simulation} 
    \label{fig:policy_results_a} 
  \end{subfigure}%% 
  \begin{subfigure}[b]{0.33\linewidth}
    \centering
    \includegraphics[width=0.71\linewidth, trim=0.3cm 0.3cm 0.2cm 0.1cm, clip]{figures/Gazebo_simulation__theta_g__0.pdf} 
    \caption{High-fidelity simulation} 
    \label{fig:policy_results_b}
  \end{subfigure}%%
 \begin{subfigure}[b]{0.33\linewidth}
    \centering
    \includegraphics[width=0.71\linewidth, trim=0.3cm 0.3cm 0.2cm 0.1cm, clip]{figures/Actual_robot__theta_g__0.pdf}
    \caption{Robot execution} 
    \label{fig:policy_results_c} 
  \end{subfigure}%% 
  \caption{Execution of the policy at multiple goals with $\theta_g = 0$ with (a) showing the core simulation trajectories, (b) the high-fidelity simulation trajectories, and (c) the trajectories executed by the real robot.}
  \label{fig:policy_results} 
\end{figure*}
In SAC, two networks are used: the actor and the critic. The actor network is the policy itself, taking the observation (state and the goal) as input and outputting action probabilities in terms of mean and deviation for each element. The critic network takes both the observation and a sampled action as input, and outputs estimates of the expected return of that action given the current policy (Q-value). This Q-value guides the actor’s updates, helping SAC refine the policy by assessing the value of actions to improve the long-term rewards.

\begin{table}
\caption{Simulation, RL, and neural network parameters.}
\begin{center}
\begin{threeparttable}
\begin{tabular}{l c}
    \toprule
     \textbf{Parameter} & \textbf{Value}\\ \midrule
    
    Simulation frequency & $30$ Hz \\
    Policy frequency & $10$ Hz \\
    Observation standard deviation & $0.01$ \\
    % $\epsilon_p$  &   0.1 m   \\
    % $\epsilon_\theta$  &   $\pi/8$ rad   \\
    $\mathbf{R}$ & $\textup{diag}(0.0, 0.8, 0.8)$\\
    $\mathbf{S}$ & $\textup{diag}(0.2, 0.2, 0.2)$\\
    Range of $a_x$ & $[-0.8, 1.1]$ m/s\\
    Range of $a_y$ & $[-0.7, 0.7]$ m/s\\
    Range of $a_\theta$ & $[-1.1, 1.1]$ rad/s\\
    $[r_{\rm min}, r_{\rm max}]$ & $[-2,2]$ \\
    \midrule
   
    Batch size   &   $512$\\
    $\tau$ & $0.0045$ \\
    Discount factor   &  $0.999$\\
    Learning rate   &   \SI{2e-4}{} \\
    Buffer size   &  \SI{1e6}{} \\
    Number of training steps & \SI{3e5}{} \\
    \midrule
    Actor neurons & $16$ \\
    Actor hidden layers & $2$\\
    Critic neurons & $128$ \\
    Critic hidden layers & $2$ \\
    Hidden layer activation function & ReLu\\
    Output layer function & linear \\

    \bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\label{tab:system_params}
\end{table}  

\subsection{Training and Verification Process}

The core simulation training stage was performed by using Gymnasium~\cite{towers2024gymnasium}, which is a library that simplifies the creation of simulators tailored for RL applications, and the SAC neural network was sourced from the stable-baselines 3 library~\cite{stable-baselines3}. The simulation parameters are shown in Table~\ref{tab:system_params}. To accelerate learning, another concept called Hindsight Experience Replay (HER)~\cite{andrychowicz2017hindsight} was also applied. HER is a technique that generates additional training data from failed episodes by redefining the goal to a point the robot reached and adjusting rewards accordingly. This approach enables the agent to learn not only from successes but also from failures.

Two additional training strategies were applied in this study: curriculum learning and randomization. Curriculum learning was applied to the goal tolerances $\epsilon_p$ and $\epsilon_\theta$, starting with the tolerances covering $80$~\% of the training region. When the robot reached a $95$~\% success rate, the tolerances were reduced by $20$~\% until it reached $\epsilon_p = 0.05$ m and $\epsilon_\theta = \ang{1}$. Looking at the pipeline in Fig.\ \ref{fig:training_pipeline}, this can be viewed as following the feedback loop in the core simulator until the RL agent achieves the desired tolerances. Randomization was applied to the robot's state, introducing noise in the robot's position and orientation to encourage the RL agent to learn how to control the robot under uncertain conditions, reducing the reality gap.

Once the policy achieved a $100$~\% success rate in the core simulator stage with the final tolerance levels, it was transferred to the high-fidelity simulation stage using the Gazebo simulator, which provides a physics engine, realistic sensor emulation, and a communication layer with ROS. Because the problem did not involve interacting with physical objects, there was no necessity to continue training the policy on the Gazebo simulator. However, running the policy on Gazebo was still crucial, as integration with ROS enabled the use of the same code, and localization and navigation architecture as on the real robot, allowing the algorithm to be tested under conditions that closely replicate the real-world environment. %More details on the navigation system are shown in Section~\ref{sec:application}.

After testing the RL agent in Gazebo, the policy was deemed ready for deployment on the real robot. Initial deployments yielded positive results, though some discrepancies were observed compared to the Gazebo simulation. Specifically, the RL agent struggled to stop the robot in certain scenarios, such as the goal at $x = 0$ m and $y = 2$ m in Fig.~\ref{fig:policy_results_c}., leading to overshooting and oscillations around the goal position. This was caused by the robot's latency in stopping due to inertia and a minimum action duration not accounted for in the core simulator. Thus, the tolerance to reach the goal was increased to $\epsilon_p = 0.3$ m and $\epsilon_\theta = \ang{17}$. This modification eliminated the oscillations and proved sufficient for our application, allowing us to avoid further training. The need for increased tolerances is a consequence of the reality gap. If more precise positioning is required, the pipeline allows for more training using either real-life data during training or by incorporating the robot oscillations in the high-fidelity simulator.

Fig.~\ref{fig:policy_results} shows the robot trajectories across all three stages. The  Gymnasium simulation (Fig.\ \ref{fig:policy_results_a}) shows accurate trajectories, as anticipated. Note that trajectories requiring lateral movement include some  forward/backward motion due to the RL agent being optimized for faster executions rather than shorter trajectories. In the Gazebo runs (Fig.\ \ref{fig:policy_results_b}), the tolerances were adjusted to match those used in the actual robot stage, and minor variations in the robot's starting positions were introduced to create a more realistic setup. Despite these differences, the trajectories were similar to the Gymnasium stage. Finally, in the real robot execution (Fig.\ \ref{fig:policy_results_c}), the trajectories closely matched the Gazebo results, except for a delay in the robot stopping upon reaching the goal regions.

