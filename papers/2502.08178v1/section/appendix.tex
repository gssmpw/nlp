\section{System Prompt} \label{appendix:prompt}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
The following is the system prompt used to let a LLM generate an answer without any information:

\begin{mdframed}[
    backgroundcolor=gray!10, % Background color
    linecolor=black, % Border color
    linewidth=0.5pt, % Border thickness
    roundcorner=5pt, % Rounded corners
    innertopmargin=10pt, % Top inner margin
    innerbottommargin=10pt, % Bottom inner margin
    innerleftmargin=10pt, % Left inner margin
    innerrightmargin=10pt, % Right inner margin
    skipabove=10pt, % Vertical space above
    skipbelow=10pt % Vertical space below
]
\setlength{\parskip}{2pt}
You are a helpful assistant. 
Answer the question as concisely as possible, using only the specific phrase, entity, or number that directly answers the question. Within five words. \\
\noindent \textbf{Query: }[question]  \\
\noindent \textbf{Short Answer:}
\end{mdframed}


The following is the system prompt used in RAG to let a LLM generate a NQ answer based on the given context:


\begin{mdframed}[
    backgroundcolor=gray!10, % Background color
    linecolor=black, % Border color
    linewidth=0.5pt, % Border thickness
    roundcorner=5pt, % Rounded corners
    innertopmargin=10pt, % Top inner margin
    innerbottommargin=10pt, % Bottom inner margin
    innerleftmargin=10pt, % Left inner margin
    innerrightmargin=10pt, % Right inner margin
    skipabove=10pt, % Vertical space above
    skipbelow=10pt % Vertical space below
]
\setlength{\parskip}{2pt}
You are a knowledgeable assistant tasked with answering questions based on the Natural Questions dataset. 
Each question is accompanied by contexts extracted from Wikipedia. 
Answer the question by providing only the specific phrase, entity, or number that directly answers the question. Within five words.

\noindent\textbf{Contexts:} [context]

\noindent\textbf{Query:} [question]

\noindent\textbf{Short Answer:}
\end{mdframed}

The following is the system prompt used in RAG to let a LLM generate a MS answer based on the given context:


\begin{mdframed}[
    backgroundcolor=gray!10, % Background color
    linecolor=black, % Border color
    linewidth=0.5pt, % Border thickness
    roundcorner=5pt, % Rounded corners
    innertopmargin=10pt, % Top inner margin
    innerbottommargin=10pt, % Bottom inner margin
    innerleftmargin=10pt, % Left inner margin
    innerrightmargin=10pt, % Right inner margin
    skipabove=10pt, % Vertical space above
    skipbelow=10pt % Vertical space below
]
\setlength{\parskip}{2pt}
You are a knowledgeable assistant tasked with answering questions based on the MS-marco dataset. 
Answer the question given the information in those contexts.
Answer the question in a single, brief sentence.

\noindent\textbf{Contexts:} [context]

\noindent\textbf{Query:} [question]

\noindent\textbf{Answer:}
\end{mdframed}

The following is the system prompt used in RAG to let a LLM generate a HotpotQA based on the given context:

\begin{mdframed}[
    backgroundcolor=gray!10, % Background color
    linecolor=black, % Border color
    linewidth=0.5pt, % Border thickness
    roundcorner=5pt, % Rounded corners
    innertopmargin=10pt, % Top inner margin
    innerbottommargin=10pt, % Bottom inner margin
    innerleftmargin=10pt, % Left inner margin
    innerrightmargin=10pt, % Right inner margin
    skipabove=10pt, % Vertical space above
    skipbelow=10pt % Vertical space below
]
\setlength{\parskip}{2pt}
You are a knowledgeable assistant tasked with answering questions based on the HotPotQA dataset. 
Each question is accompanied by contexts extracted from Wikipedia. 
Answer the question as concisely as possible, using only the specific phrase, entity, or number that directly answers the question. Within five words.

\noindent\textbf{Contexts:} [context]

\noindent\textbf{Query:} [question]

\noindent\textbf{Short Answer:}
\end{mdframed}

\section{Calculation of Document Retrieval Ratio for Input Size Consistency} \label{Appendix: Input Size Consistency}

% 在原始语料库上进行检索时，前30篇召回文档的平均段落长度为80.8575个Token。而经过Sentence RAG处理后分割的语料库中，前30篇召回文档的平均段落长度减少至23.85个Token。因此，为了在输入文档总长度上保持一致，当使用Sentence RAG时，理论上需要召回前 80.8575/23.85≈3480.8575/23.85≈34 篇文档，才能匹配naive-RAG召回前10篇文档的Token规模。虽然严格计算建议召回约34篇文档，但选择30篇既能维持实验结果的有效性，又在数字上更易于解读和展示。
In the original corpus, the average token count per paragraph for the top 30 retrieved paragraphs is 80.8575 tokens. After applying the Sentence-RAG, the average token count per paragraph for the top 30 retrieved paragraphs decreases to 23.85 tokens. Therefore, to maintain consistency in the total token count of input paragraphs, Sentence-RAG would theoretically need to retrieve the top $80.8575/23.85 \approx 34$ paragraphs to match the token scale of the top 10 paragraphs retrieved by naive-RAG. While strict calculations suggest retrieving approximately 34 paragraphs, selecting 30 paragraphs strikes a balance between maintaining the validity of experimental results and ensuring clarity and simplicity in presentation.


\section{Statistics of Datasets}
\begin{table}[!ht]
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{cccc}
    \hline
        ~ & NQ & HotpotQA & MS-MARCO  \\ \hline
        Passages & 2,681,468 & 5,233,329 & 8,841,823  \\ 
        Ours & 9,320,506 & 12,425,366 & 30,137,968  \\ \hline
    \end{tabular}
    }
    \label{table:statistic-dataset}
    \caption{Statistics of text unit counts before and after ParetoRAG encoding. }
    \label{table:statistic-dataset}
\end{table}

\begin{figure*}[ht!]
    \centering
    % 左侧子图
    \begin{subfigure}[b]{0.48\textwidth} % 左右各占 48%，留一些间隙
        \centering
        \includegraphics[width=\linewidth]{figure/weightRecall-hotpotqa.pdf}
        \caption{HotpotQA}
        \label{fig:HotpotQA Core Sentence}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    % 右侧子图
    \begin{subfigure}[b]{0.48\textwidth} % 左右各占 48%，留一些间隙
        \centering
        \includegraphics[width=\linewidth]{figure/weightRecall-msmarco.pdf}
        \caption{MS-MARCO}
        \label{fig:MS-Marco Core Sentence}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    \caption{Impact of Core Sentence Weight on Recall across HotpotQA and MS-Marco Dataset. }
    \label{fig:impact of topk size appendix}
\end{figure*}


Table \ref{table:statistic-dataset} shows statistics of text unit counts before and after ParetoRAG encoding.

\section{Statistics of Models} \label{appendix: Statistics of models}
All model weights are derived from Hugging Face, which were used without additional training. In the following, we list the specific hugging face model names corresponding to the weights used in the experiment:

\subsection{Model Weights}
\begin{itemize}[leftmargin=*]
    \small
    \item \textbf{DPR}:
    \begin{itemize}
        \item \path{facebook/dpr-question_encoder-multiset-base}
        \item \path{facebook/dpr-ctx_encoder-multiset-base}
    \end{itemize}
    \item \textbf{Contriever}:
    \begin{itemize}
        \item \path{facebook/contriever}
    \end{itemize}
    \item \textbf{ANCE}:
    \begin{itemize}
        \item \path{sentence-transformers/msmarco-roberta-base-ance-firstp}
    \end{itemize}
    \item \textbf{RECOMP}:
    \begin{itemize}
        \item \path{fangyuan/nq_abstractive_compressor}
        \item \path{fangyuan/hotpotqa_abstractive_compressor}
    \end{itemize}
    \item \textbf{Llama2}:
    \begin{itemize}
        \item \path{meta-llama/Llama-2-7b-chat-hf}
        \item \path{meta-llama/Llama-2-13b-chat-hf}
    \end{itemize}
    \item \textbf{Viccuna}:
    \begin{itemize}
        \item \path{lmsys/vicuna-7b-v1.3}               
        \item \path{lmsys/vicuna-13b-v1.3}
    \end{itemize}
    \item \textbf{RetRobust}:
    \begin{itemize}
        \item \path{Ori/llama-2-13b-peft-nq-retrobust}               
        \item \path{Ori/llama-2-13b-peft-hotpotqa-retrobust}
    \end{itemize}

\end{itemize}

\subsection{Model Hyperparameter} 
The model's configuration is as follows: \path{max_output_tokens} is set to 150, limiting the maximum number of tokens in the generated output; \path{temperature} is set to 0.1, which controls the randomness of the generation process, ensuring more deterministic and focused outputs; \path{seed} is fixed at 100 to ensure reproducibility of the results across different runs; and \path{per_gpu_batch_size} is set to 16, specifying the number of samples processed per GPU in each batch during training or inference.

\section{Evaluation Metrics} \label{appendix: Evaluation Metrics}
Following the experimental setup in \cite{asaiSelfRAGLearningRetrieve2023a}, we use MAUVE\cite{pillutlaMAUVEMeasuringGap2021} and ROUGE-L \cite{linROUGEPackageAutomatic2004} as evaluation metrics for long-form generation. For short-form generation, we use accuracy (ACC). For each question, if the standard answer is contained within the generated answer and the length of the generated answer is less than or equal to 15, it is counted as 1. Here’s a brief explanation of the evaluation metrics: 
\begin{itemize}
    \item Accuracy: Measures the percentage of correct predictions made by the model. It's a basic metric that indicates how well the model is performing on a classification or question-answering task.
    \item ROUGE: Evaluates text summarization or generation by comparing the overlap between generated text and reference text. It focuses on recall, ensuring the generated text captures key information from the reference. Common variants include ROUGE-N (n-gram overlap) and ROUGE-L (longest common subsequence).
    \item MAUVE:  Assesses text generation quality by comparing the distribution of generated text to reference text in an embedding space. It uses divergence measures to evaluate semantic and structural alignment, making it particularly useful for open-ended tasks like story or dialogue generation.
\end{itemize}

\section{Impact of core sentence weight} \label{appendix: Impact of core sentence weight}
%  如图所示，HotpotQA 和 MS-MARCO 在整体趋势上符合 Section 5.2.1 的分析，即核心句子权重的增加通常会提升召回率。然而，在召回率变化的细节上，两者仍存在一定的区别，特别是在 MS-MARCO 数据集中，ParetoRAG-DPR 和 ParetoRAG-ANCE 在核心句子权重 0.3 ~ 0.6 之间的召回率下降最为明显。这一现象可能由以下几个关键因素导致：
    % 检索模型的训练方式差异：
    %     ANCE 依赖异步更新的全局索引（asynchronous global index updates），而 DPR 和 Contriever 采用局部正负对比学习（local contrastive learning）。这一差异使得 ANCE 在核心句子和上下文的权重接近（0.3 ~ 0.6）时，更容易受到上下文噪声的干扰，导致检索不够精准，进而影响召回率。
    %     相比之下，DPR 和 Contriever 在训练过程中主要依赖 局部的正负样本对比，它们不会受到全局索引更新的滞后影响，因此在 0.3 ~ 0.6 之间的召回率下降幅度相对较小。

    % 任务类型与信息需求的不同：
    %     MS-MARCO 是单跳问答任务（Single-hop QA），其查询往往只需匹配文本中的某个核心句子即可找到答案，而段落级信息可能包含大量冗余内容。因此，当核心句子权重处于 0.3 ~ 0.6 之间时，段落信息对模型造成一定干扰，导致召回率下降。
    %     HotpotQA 是多跳问答任务（Multi-hop QA），查询通常需要整合多个段落的信息来完成推理。因此，即使核心句子权重较低，模型仍然能够利用其他段落的信息进行召回，不会像 MS-MARCO 那样出现明显的下降趋势。
As shown in the Figure \ref{fig:impact of topk size appendix}, HotpotQA and MS-MARCO generally follow the trends analyzed in Section 5.2.1, where increasing the core sentence weight typically improves recall performance. However, there are noticeable differences in the details of recall variations between these two datasets. Specifically, in the MS-MARCO dataset, the recall rates of ParetoRAG-DPR and ParetoRAG-ANCE decrease significantly in the core sentence weight range of 0.3 to 0.6. This phenomenon can be attributed to the following key factors:

\subsection{Differences in Retrieval Model Training Approaches}
\begin{itemize}
    \item \textbf{Asynchronous Global Index Updates versus Local Contrastive Learning}: ANCE relies on \textit{asynchronous global index updates}, whereas DPR and Contriever adopt \textit{local contrastive learning} with positive and negative samples. This distinction makes ANCE more susceptible to contextual noise when the core sentence and contextual sentence weights are close (0.3–0.6), leading to less precise retrieval and subsequently lower recall performance.
    \item In contrast, DPR and Contriever primarily depend on \textit{local contrastive learning} during training. Since they do not suffer from the lag introduced by global index updates, their recall rate decline in the 0.3–0.6 weight range is relatively less pronounced.
\end{itemize}

\subsection{Differences in Task Types and Information Requirements}
\begin{itemize}
    \item \textbf{MS-MARCO (Single-hop QA)}: In this dataset, queries typically require matching a specific \textit{core sentence} in the text to retrieve the correct answer, while paragraph-level information may contain substantial redundant content. Consequently, when the core sentence weight falls within the 0.3–0.6 range, paragraph-level information introduces interference in the retrieval process, leading to a decline in recall performance.
    \item \textbf{HotpotQA (Multi-hop QA)}: In contrast, HotpotQA involves multi-hop reasoning, where queries require integrating information from multiple paragraphs to derive the final answer. As a result, even when the core sentence weight is relatively low, the model can still leverage other paragraphs to improve retrieval performance. Therefore, unlike MS-MARCO, HotpotQA does not exhibit a sharp decline in recall within the 0.3–0.6 weight range.
\end{itemize}

\section{Impact of Model Size on Accuracy with Varying Top K in ParetoRAG} \label{appendix: Impact of Model Size on Accuracy with Varying Top K in ParetoRAG}

% 对于较小的模型（如 Vicuna-7b），其处理大量文档的能力较弱，因此在 Top K 较大时准确率下降更快。然而，这种下降并非因 ParetoRAG 的召回质量不足，而是因为较小模型无法充分利用这些更加丰富的信息。此外，对于较大的模型（如 Vicuna-13b），其参数量和推理能力更强，能够在更大范围内处理更多的信息，因此即使 Top K 扩大到一定程度时（如 Top 20、Top 30），依然有较高的准确率。特别地，较大的 Top K 设置（如 Top 20）相比 Top 10 和 baseline 展现了更好的性能，这表明 ParetoRAG 能够提供更加丰富的信息召回，为语言模型提供更有效的上下文。
As show in Figure \ref{fig:impact of topk size}, for smaller models (such as Vicuna-7b), their ability to process a large number of documents is weaker, leading to a faster decline in accuracy as the Top K increases. However, this decline is not due to a lack of retrieval quality by ParetoRAG, but rather because smaller models are unable to fully utilize the richer information provided. On the other hand, for larger models (such as Vicuna-13b), their greater parameter size and reasoning capability enable them to handle more information within a larger scope. As a result, even when the Top K is increased to a certain extent (e.g., Top 20 or Top 30), they still maintain high accuracy. Notably, larger Top K settings (e.g., Top 20) outperform Top 10 and the baseline, demonstrating that ParetoRAG can provide richer information retrieval, offering more effective context for language models.