\begin{abstract}

% 尽管检索增强生成（RAG）系统通过引入外部知识增强了大型语言模型（LLMs）的能力，但它们仍然面临着检索效率低下以及LLMs无法过滤无关信息的持续挑战。
% 我们提出了\textbf{ParetoRAG}，一种无监督框架，通过\textbf{帕累托原则}指导的句子级优化来改进RAG系统。
%  通过将段落分解为句子，并在保持上下文连贯性的同时动态重新加权核心内容，ParetoRAG在检索精度和生成质量上实现了双重提升，且无需额外的训练或API资源。该框架已在多种数据集、LLMs和检索器上进行了实证验证。此外，我们的分析揭示了将句子级检索与强大的语言模型结合时产生的协同效应。
While Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by incorporating external knowledge, they still face persistent challenges in retrieval inefficiency and the inability of LLMs to filter out irrelevant information.
We present \textbf{ParetoRAG}, an unsupervised framework that optimizes RAG systems through sentence-level refinement guided by the \textbf{Pareto principle}. 
By decomposing paragraphs into sentences and dynamically re-weighting core content while preserving contextual coherence, ParetoRAG achieves dual improvements in both retrieval precision and generation quality without requiring additional training or API resources. This framework has been empirically validated across various datasets, LLMs, and retrievers. 
% Furthermore, we explore and analyze the synergy between ParetoRAG and adaptive noise-robust model, demonstrating how their combination may enhance generation quality. This analysis provides preliminary insights that could inform future research on integrating retrieval-augmented methods with robust training strategies.
% 此外，我们证明了ParetoRAG的架构改进与自适应噪声鲁棒模型正交兼容，使检索增强优化和鲁棒训练能够相互提高生成质量。这突出了架构改进和噪声缓解的互补性，为将检索增强与鲁棒性增强相结合提供了见解。
Futhermore, we show that ParetoRAG's architectural improvements are orthogonally compatible with adaptive noise-robust models, enabling retrieval-augmented optimization and robust training to mutually enhance generation quality. This highlights architectural refinements and noise mitigation as complementary, offering insights for integrating retrieval augmentation with robustness enhancement.
\end{abstract}