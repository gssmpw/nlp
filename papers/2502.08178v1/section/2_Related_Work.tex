\section{Related Work}

% 该方法主要利用信息检索为生成型 LLMs 提供包含相关知识的文档。早期研究通常在预训练语言模型的前端采用稀疏或稠密检索器，专注于响应生成。然而，这些方法往往忽略了一个关键问题：如果检索出现错误怎么办？引入检索的目的是确保生成型 LLMs 能获取相关且准确的知识。如果检索到的文档是无关的，检索系统甚至可能加剧 LLMs 的事实性错误。 这部分没有用上

% 然而，这些方法往往忽略了而一个关键问题：如果检索文档质量较低，含有过多相关但无用的信息怎么办？

% 适应 RAG 的 LLM 调优研究

% 以往的研究探索了通过指令调优来适应 LLM 在 RAG 中的应用（Zhang 等, 2023）。RetRobust（Yoran 等, 2024）通过微调 LLM 使用一条检索到的相关片段或随机负片段，使其对无关片段更具鲁棒性。RA-DIT（Lin 等, 2024）进行双重指令调优，使 LLM 能更有效地利用检索信息，同时使检索结果更符合 LLM 的偏好。Self-RAG（Asai 等, 2024）引入了一个框架，用于训练一个动态检索片段、生成内容并评估检索片段的语言模型，以提升性能。RAFT（Zhang 等, 2024）通过训练 LLM 提升其在“开放书”领域内问题回答的能力。RankRAG（Yu 等, 2024）最近调优了一种用于 RAG 的 LLM，其目标是同时优化上下文排序和答案生成。InstructRAG（Wei 等, 2024）通过微调 LLM，使其生成自我综合的推理，而不是直接回答问题。

%  RAG
% 检索增强生成（RAG）（Lewis 等, 2020；Guu 等, 2020）被认为是一种解决上述问题的有效方法，通过检索文档增强生成型大型语言模型（LLMs）的输入问题。RAG 通常从特定语料库（如 Wikipedia）中提供额外的知识来源，从而显著提升 LLMs 在多种任务中的表现，尤其是在知识密集型任务中的性能。
% 但是, 由于检索器本身的性能问题, RAG系统不可避免会引入无关或者部分相关的内容至模型.
% 近年来，噪声信息对检索增强生成（RAG）系统性能的影响引起了广泛关注。一些研究表明，无关噪声的引入会显著降低模型性能 \cite{Jia2017, Creswell2022}。进一步的分析显示，随着检索上下文中噪声比例的增加，大型语言模型（LLMs）的性能会显著下降 \cite{Chen2023}。类似现象也被其他研究报告，指出检索噪声是影响 RAG 系统生成质量的关键因素 \cite{Yoran2023, Thakur2023}。此外，有研究探讨了不同类型噪声对 RAG 系统的影响，并发现 Counterfactual Retrieval Noise 对召回系统的不利影响最大。
%  Recomp 可以提一下

\textbf{Retrieval-Augmented Generation with Noisy Context} RAG \cite{guuREALMRetrievalaugmentedLanguage2020, lewisRetrievalaugmentedGenerationKnowledgeintensive2020} is considered a useful method to address hallucinations, which improves the input questions of generative LLM with retrieved documents. It usually provides an extra knowledge source from a specific corpus, i.e., Wikipedia, which greatly improves the performance of LLM in a variety of tasks, especially in the knowledge-intensive ones \cite{ramInContextRetrievalAugmentedLanguage2023}. However, due to the limitation of retrieval capabilities, retrieval-augmented systems inevitably introduce irrelevant or partially relevant knowledge to the models \cite{yinALCUNALargeLanguage2023}. In recent years, the impact of noisy information on the performance of RAG systems has received increasing attention \cite{zhuFreeLBEnhancedAdversarial2019, yuChainofNoteEnhancingRobustness2024}.  Some studies \cite{jiaAdversarialExamplesEvaluating2017, creswellSelectionInferenceExploitingLarge2022} have shown that the introduction of irrelevant noise significantly degrades model performance. Further analyses \cite{chenBenchmarkingLargeLanguage2025} indicate that as the proportion of noise in the retrieval context increases, the performance of large language models (LLMs) deteriorates significantly. In addition, research \cite{fangEnhancingNoiseRobustness2024a} has explored the effects of different types of noise on RAG systems and found that counterfactual retrieval noise has the most detrimental impact on retrieval systems.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{figure/figure3_ParetoRAG_Encode.pdf}
    \caption{The example of ParetoRAG encodes core sentence M and core sentence (M+1). The content within the same dashed box is split from the same passage. The context of a core sentence consists of sentences from the same passage, excluding the core sentence itself.}
    \label{fig:Pareto Encode Example}
\end{figure} 

\textbf{Advanced RAG} Many advanced approaches have been developed from the original RAG in recent years \cite{kimSuReSummarizingRetrievals2023, zhang2024raft, liuRAISFLearningAnswer2024, patilGorillaLargeLanguage2024}. Considering that retrieval is sometimes unnecessary for some queries, conversely, responses without retrieval are even more accurate in many situations. SelfRAG \cite{asaiSelfRAGLearningRetrieve2023a} is proposed to selectively retrieve knowledge and introduce a critic model to decide whether to retrieve. SAIL \cite{luoSearchAugmentedInstruction2023} is tuned on instructions to insert the retrieved documents before the instructions. RECOMP \cite{xu2024recomp} is designed to refine the retrieved passages by either abstractively or extractively summarizing them with additional models.


Compared with recent studies \cite{hwangDSLRDocumentRefinement2024, chenDenseRetrievalWhat2024a} that are the most relevant to our work, a main difference should be highlighted. Dense X reduces information redundancy by degrading documents into proposition sentences. To decompose paragraphs into propositions, the authors trained a fine-tuned text generation model to supplement contextual information. In contrast, our approach utilizes the Sentence-Context Weighted Attention mechanism to supplement contextual information without requiring any fine-tuning.
% Dense X 通过把 Document 降级成原子命题 Sentence 减少信息冗余，为了将段落分解成原子命题（propositions），作者们训练一个经过微调的文本生成模型以补充上下文信息，而我们的方法则是通过Sentence-Context Weight Attention机制以补充上下文信息，不需要经过任何的微调。