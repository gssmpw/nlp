
\section{Method}
In this section, we introduce a novel framework, ParetoRAG, for improving the accuracy of retrieval results by leveraging sentence-level weighting inspired by the Pareto Principle, integrated into the RAG system. Notably, ParetoRAG requires neither additional training resources nor extra API calls.

\subsection{Encoding Step}
% 编码步骤涉及从段落中提取核心句子及其对应的上下文，并将其编码为稠密的向量表示。此过程确保核心句子的关键信息和其周围上下文的补充信息都得以包含，从而为后续的检索提供语义丰富的表示。
The encoding step involves extracting the core sentence and its corresponding context from the passages and encoding them into dense vector representations. This process ensures the inclusion of both the key information from the core sentence and the supplementary information from its surrounding context, enabling semantically rich representation for subsequent retrieval.

% 检索语料库 C 中的段落被切分为句子，使用NLTK的句子分割工具完成操作。每个段落 P∈CP∈C 被表示为一个句子序列：其中，si 表示段落中的第 i 个句子。
% 对 Passage Segmentation 用数学化的表达
\textbf{Passage Segmentation. }The passages in the retrieval corpus $\mathcal{C}$ are segmented into sentences using NLTK. The retrieval corpus is represented as a collection of passages:
\[
\mathcal{C} = \{P_1, P_2, \dots, P_m\},
\]
where $m$ is the total number of passages. Each passage $P_j \in \mathcal{C}$ is represented as a sequence of sentences:
\[
P_j = \{s_1^j, s_2^j, \dots, s_{n_j}^j\},
\]
where $s_i^j$ represents the $i$-th sentence in the $j$-th passage, and $n_j$ is the total number of sentences in $P_j$.

% 对 Core Sentence and Context Extraction 用数学化的表达
\textbf{Core Sentence and Context Extraction. }For each passage $P_j \in \mathcal{C}$, every sentence $s_i^j$ is considered a core sentence, and its corresponding context is defined as the concatenation of all other sentences in the same passage, excluding $s_i^j$ itself. Formally, for a passage $P_j = \{s_1^j, s_2^j, \dots, s_{n_j}^j\}$, the context for the core sentence $s_i^j$ is defined as:
\\[\bigskipamount]
    \resizebox{0.5\textwidth}{!}{
    $
    \text{Context}(s_i^j) = 
    \begin{cases} 
    \{s_1^j, \dots, s_{i-1}^j, s_{i+1}^j, \dots, s_{n_j}^j\}, & \text{if } n_j > 1, \\
    \text{NULL}, & \text{if } n_j = 1.
    \end{cases}
    $
    }
\\[\bigskipamount]
Here, $\text{Context}(s_i^j)$ captures the surrounding information in the passage $P_j$ without including the core sentence $s_i^j$ itself. This ensures that each core sentence $s_i^j$ can be analyzed independently while still being informed by its contextual sentences. If a passage consists of only one sentence ($n_j = 1$), the context is defined as $\text{NULL}$.

%  对 Encode Core Sentence and Context 进行数学化的表达
\textbf{Encode Core Sentence, Context and Query. }For each core sentence $s_i^j$ and its corresponding context $\text{Context}(s_i^j)$, a configurable encoder $\text{Enc}_\theta(\cdot)$ is applied to obtain their vector representations. Here, $\theta$ represents the model selection parameter, which determines the specific encoder to be used (e.g., Contriever, ANCE, or DPR). 
% The core sentence $s_i^j$ is encoded into a vector representation:
% \[
% \mathbf{h}_{\text{core}}^i = \text{Enc}_\theta(s_i^j),
% \]
% where $\mathbf{h}_{\text{core}}^i \in \mathbb{R}^d$ is the $d$-dimensional vector representation of the core sentence.

% The context $\text{Context}(s_i^j)$ is encoded into another vector representation:
% \[
% \mathbf{h}_{\text{context}}^i = \text{Enc}_\theta(\text{Context}(s_i^j)),
% \]
% where $\mathbf{h}_{\text{context}}^i \in \mathbb{R}^d$ is the $d$-dimensional vector representation of the context.

% The query is also encoded into a vector representation:
% \[
% \mathbf{q} = \text{Enc}_\theta(\text{Query}),
% \]
% where $\mathbf{q} \in \mathbb{R}^d$ is the $d$-dimensional vector representation of the query.
The core sentence $s_i^j$, its context $\text{Context}(s_i^j)$, and the query are encoded into $d$-dimensional vector representations using the same encoder $\text{Enc}_\theta(\cdot)$. The encoding process is as follows:
\[
\begin{aligned}
\mathbf{h}_{\text{core}}^i &= \text{Enc}_\theta(s_i^j), \\
\mathbf{h}_{\text{context}}^i &= \text{Enc}_\theta(\text{Context}(s_i^j)), \\
\mathbf{q} &= \text{Enc}_\theta(\text{Query}),
\end{aligned}
\]
where $\mathbf{h}_{\text{core}}^i$, $\mathbf{h}_{\text{context}}^i$, and $\mathbf{q}$ are all $d$-dimensional vectors. These representations are used for similarity computation and ranking.

These vector representations $\mathbf{h}_{\text{core}}^i$, $\mathbf{h}_{\text{context}}^i$, and $\mathbf{q}$ are then used to calculate the similarity and rank in subsequent steps. Figure \ref{fig:Pareto Encode Example} shows the example of ParetoRAG encodes core sentence M and core sentence (M+1). 


\subsection{Retrieval Step}
The retriever step takes the encoded core sentence, context, and query vectors to compute their similarity and rank the results for retrieval. This process consists of the following key substeps:

\textbf{Sentence-Context Weight Adjustment} 
To balance the contributions of the core sentence and its context, an attention mechanism assigns weights based on a hyperparameter $\alpha$. The weighted representation is computed as: 
\\[\bigskipamount]
    \resizebox{0.5\textwidth}{!}{
    $
    \mathbf{h}_{\text{weighted}}^i =
    \begin{cases}
    \mathbf{h}_{\text{core}}^i, & \text{if } \text{Context}(s_i^j) = \text{NULL}, \\
    \alpha \cdot \mathbf{h}_{\text{core}}^i + (1 - \alpha) \cdot \mathbf{h}_{\text{context}}^i, & \text{otherwise}.
    \end{cases}
    $
    }
\\[\bigskipamount]
This mechanism ensures that both the key information from the core sentence and the supplementary information from its context are considered during similarity computation.





\textbf{Similarity Computation}
Following previous studies \cite{lewisRetrievalaugmentedGenerationKnowledgeintensive2020, zouPoisonedRAGKnowledgeCorruption2024}, the similarity between the weighted sentence representation $\mathbf{h}_{\text{weighted}}^i$ and the query vector $\mathbf{q}$ is computed using dot similarity by default.
    \[
    \text{Sim}(s_i^j, \mathbf{q}) = \mathbf{h}_{\text{weighted}}^i \cdot \mathbf{q}.
    \]
This step quantifies how closely each sentence-context pair matches the semantic meaning of the query.

\textbf{Top-$k$ Sentence Ranking} 
The top-$k$ sentences are ranked based on their similarity scores in descending order:
\[
\text{Top-}k = \operatorname{arg\,top}_k \left( \text{Sim}(s_i^j, \mathbf{q}) \right),
\]
where $\operatorname{arg\,top}_k$ returns the indices of the $k$ sentences with the highest similarity scores. These top-$k$ sentences are selected as the retrieval results, providing the most relevant information based on the query.

\begin{table*}[ht!]
\resizebox{\textwidth}{!}{ % 调整宽度为页面宽度，高度自动缩放
\begin{tabular}{lccccccccccccccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Model}} & \multicolumn{4}{c}{NQ(acc)} & \multicolumn{4}{c|}{Hotpot(acc)} & \multicolumn{4}{c|}{MS(mauve)} & \multicolumn{3}{c}{MS(rouge)} \\
\multicolumn{1}{c|}{} & \# tok & Contriever & ANCE & DPR & \# tok & Contriever & ANCE & \multicolumn{1}{c|}{DPR} & \# tok & Contriever & ANCE & \multicolumn{1}{c|}{DPR} & Contriever & ANCE & DPR \\ \hline
\multicolumn{16}{c}{Without RAG} \\ \hline
Vicuna-7B & \multirow{4}{*}{/} & \multicolumn{3}{c}{23.2} & \multirow{4}{*}{/} & \multicolumn{3}{c}{16.1} & \multirow{4}{*}{/} & \multicolumn{3}{c}{88.3} & \multicolumn{3}{c}{40.5} \\
Vicuna-13B &  & \multicolumn{3}{c}{28.2} &  & \multicolumn{3}{c}{20.2} &  & \multicolumn{3}{c}{82.1} & \multicolumn{3}{c}{40.7} \\
Llama2-7B-chat &  & \multicolumn{3}{c}{20.9} &  & \multicolumn{3}{c}{16.0} &  & \multicolumn{3}{c}{85.6} & \multicolumn{3}{c}{36.2} \\
Llama2-13B-chat &  & \multicolumn{3}{c}{29.9} &  & \multicolumn{3}{c}{18.4} &  & \multicolumn{3}{c}{90.1} & \multicolumn{3}{c}{39.6} \\ \hline
\multicolumn{16}{c}{Naive RAG} \\ \hline
Vicuna-7B & \multirow{4}{*}{895} & 33.2 & 36.1 & 41.9 & \multirow{4}{*}{757} & 25.0 & 22.3 & 23.9 & \multirow{4}{*}{576} & 84.1 & 84.9 & 87.9 & 35.8 & 35.6 & 37.5 \\
Vicuna-13B &  & 37.4 & 41.0 & 45.6 &  & 22.6 & 20.2 & 22.7 &  & 86.8 & 87.7 & 87.0 & 37.8 & 36.9 & 36.9 \\
Llama2-7B-chat &  & 33.2 & 37.9 & 40.8 &  & 23.6 & 23.4 & 23.3 &  & 85.0 & 88.6 & 89.2 & 33.6 & 34.4 & 35.4 \\
Llama2-13B-chat &  & 38.3 & 39.6 & 42.7 &  & 27.1 & 25.3 & 26.8 &  & 77.5 & 87.0 & 88.8 & 33.2 & 33.5 & 34.7 \\ \hline
\multicolumn{16}{c}{Recomp \cite{xu2024recomp}} \\ \hline
% Vicuna-7B & \multirow{4}{*}{26} & 35.9 & 39.3 & 43.4 & \multirow{4}{*}{41} & 29.0 & 25.5 & 27.9 & \multirow{4}{*}{26} & 79.2 & 83.8 & 85.3 & 40.4 & 42.3 & 41.2 \\
% Vicuna-13B &  & 36.8 & 41.5 & 42.5 &  & 28.9 & 25.4 & 25.6 &  & 85.4 & 85.7 & 87.2 & 40.5 & 41.6 & 40.3 \\
% Llama2-7B-chat &  & 24.3 & 31.5 & 33.9 &  & 26.7 & 22.4 & 25.2 &  & 25.7 & 41.2 & 38.5 & 36.7 & 39.2 & 37.6 \\
% Llama2-13B-chat &  & 31.5 & 36.2 & 39.5 &  & {\ul 30.1} & 25.2 & {\ul 28.9} &  & 40.4 & 45.9 & 41.6 & 37.0 & 39.0 & 37.7 \\ \hline
Vicuna-7B 
& \multirow{4}{*}{26} & \poscell{8}{35.9} & \poscell{9}{39.3} & \poscell{4}{43.4} 
& \multirow{4}{*}{41} & \poscell{16}{29.0} & \poscell{14}{25.5} & \poscell{17}{27.9} 
& \multirow{4}{*}{26} & \negcell{6}{79.2} & \negcell{1}{83.8} & \negcell{3}{85.3} 
& \poscell{13}{40.4} & \poscell{19}{42.3} & \poscell{10}{41.2} \\ 

Vicuna-13B 
&  & \negcell{2}{36.8} & \poscell{1}{41.5} & \negcell{7}{42.5} 
&  & \poscell{28}{28.9} & \poscell{25}{25.4} & \poscell{13}{25.6} 
&  & \negcell{2}{85.4} & \negcell{2}{85.7} & \poscell{0}{87.2} 
& \poscell{7}{40.5} & \poscell{13}{41.6} & \poscell{9}{40.3} \\ 

Llama2-7B-chat 
&  & \negcell{27}{24.3} & \negcell{17}{31.5} & \negcell{17}{33.9} 
&  & \poscell{13}{26.7} & \negcell{4}{22.4} & \poscell{8}{25.2} 
&  & \negcell{70}{25.7} & \negcell{54}{41.2} & \negcell{57}{38.5} 
& \poscell{9}{36.7} & \poscell{14}{39.2} & \poscell{6}{37.6} \\ 

Llama2-13B-chat 
&  & \negcell{18}{31.5} & \negcell{9}{36.2} & \negcell{7}{39.5} 
&  & \poscell{11}{\ul{30.1}} & \negcell{0}{25.2} & \poscell{8}{28.9} 
&  & \negcell{48}{40.4} & \negcell{47}{45.9} & \negcell{53}{41.6} 
& \poscell{12}{37.0} & \poscell{17}{39.0} & \poscell{9}{37.7} \\ \hline

% \multicolumn{16}{c}{ParetoRAG} \\ \hline
% Vicuna-7B & \multirow{4}{*}{232} & {\graybackground{20} 36.6} & {\graybackground{40} 43.4} & {\graybackground{22} 46.7} & \multirow{4}{*}{229} & {\graybackground{4} 25.4} & {\graybackground{26} 25.3} & {\graybackground{6} 24.7} & \multirow{4}{*}{183} & {\graybackground{10} 88.1} & {\graybackground{6} 87.4} & {\graybackground{8} 91.5} & {\graybackground{10} 48.6} & {\graybackground{30} 52.9} & {\graybackground{14} 50.0} \\
% Vicuna-13B &  & {\graybackground{10} 39.1} & {\graybackground{16} 44.2} & {\graybackground{12} 48.2} &  & {\graybackground{36} 26.7} & {\graybackground{56} 25.9} & {\graybackground{30} 26.0} &  & {\graybackground{0} 84.3} & {\graybackground{2} 88.8} & {\graybackground{0} 86.7} & {\graybackground{8} 49.9} & {\graybackground{34} 55.2} & {\graybackground{18} 51.5} \\
% Llama2-7B-Chat &  & {\graybackground{4} 34.0} & {\graybackground{20} 41.7} & {\graybackground{8} 42.3} &  & {\graybackground{8} 24.6} & {\graybackground{12} 25.0} & {\graybackground{6} 24.0} &  & {\graybackground{16} 92.0} & {\graybackground{2} 89.9} & {\graybackground{6} 92.6} & {\graybackground{6} 50.0} & {\graybackground{22} 53.2} & {\graybackground{12} 51.6} \\
% Llama2-13B-Chat &  & {\graybackground{0} 36.1} & {\graybackground{12} 41.8} & {\graybackground{22} 47.4} &  & {\graybackground{8} 25.9} & {\graybackground{6} 26.1} & {\graybackground{6} 25.2} &  & {\graybackground{36} 92.2} & {\graybackground{6} 90.1} & {\graybackground{8} 92.0} & {\graybackground{12} 50.9} & {\graybackground{38} 55.1} & {\graybackground{26} 52.1} \\ \hline
\multicolumn{16}{c}{ParetoRAG (Ours)} \\ \hline
% Vicuna-7B & \multirow{4}{*}{232} & 36.6 & 43.4 & 46.7 & \multirow{4}{*}{229} & 25.4 & 25.3 & 24.7 & \multirow{4}{*}{183} & 88.1 & 87.4 & 91.5 & {\ul 42.5} & {\ul 44.4} & {\ul 43.4} \\
% Vicuna-13B &  & {\ul 39.1} & {\ul 44.2} & {\ul 48.2} &  & 26.7 & 25.9 & 26.0 &  & 84.3 & 88.8 & 86.7 & 41.6 & 43.6 & 42.6 \\
% Llama2-7B-chat &  & 34.0 & 41.7 & 42.3 &  & 24.6 & 25.0 & 24.0 &  & 92.0 & 89.9 & {\ul 92.6} & 40.4 & 43.3 & 41.9 \\
% Llama2-13B-chat &  & 36.1 & 41.8 & 47.4 &  & 25.9 & {\ul 26.1} & 25.2 &  & {\ul 92.2} & {\ul 90.1} & 92.0 & 39.0 & 42.2 & 40.8 
Vicuna-7B 
& \multirow{4}{*}{232} & \poscell{10}{36.6} & \poscell{20}{43.4} & \poscell{11}{46.7} 
& \multirow{4}{*}{229} & \negcell{2}{25.4} & \poscell{13}{25.3} & \negcell{3}{24.7} 
& \multirow{4}{*}{183} & \poscell{5}{88.1} & \poscell{3}{87.4} & \poscell{4}{91.5} 
& \poscell{19}{42.5} & \poscell{25}{\ul{44.4}} & \poscell{15}{43.4} \\ 

Vicuna-13B 
&  & \poscell{5}{39.1} & \poscell{8}{44.2} & \poscell{6}{\ul{48.2}} 
&  & \poscell{18}{26.7} & \poscell{28}{25.9} & \poscell{15}{26.0} 
&  & \negcell{3}{84.3} & \poscell{1}{88.8} & \negcell{1}{86.7} 
& \poscell{10}{41.6} & \poscell{18}{43.6} & \poscell{15}{42.6} \\ 

Llama2-7B-chat 
&  & \poscell{2}{34.0} & \poscell{10}{41.7} & \poscell{4}{42.3} 
&  & \poscell{4}{24.6} & \poscell{7}{25.0} & \poscell{3}{24.0} 
&  & \poscell{8}{92.0} & \poscell{1}{89.9} & \poscell{4}{92.6} 
& \poscell{20}{40.4} & \poscell{26}{43.3} & \poscell{18}{41.9} \\ 

Llama2-13B-chat 
&  & \negcell{6}{36.1} & \poscell{6}{41.8} & \poscell{11}{47.4} 
&  & \negcell{4}{25.9} & \poscell{3}{26.1} & \negcell{6}{25.2} 
&  & \poscell{19}{\ul{92.2}} & \poscell{4}{90.1} & \poscell{4}{92.0} 
& \poscell{17}{39.0} & \poscell{26}{42.2} & \poscell{18}{40.8} \\ \hline
\end{tabular}
}
\caption{Overall experiment results of three retrievers on three tasks, based on top 30 recall contents. The deeper the \redtext{red background}, the lower the relative improvement. In contrast, the deeper the \bluetext{blue background}, the higher the relative improvement compared to Naive RAG. The \underline{underlined numbers} indicate the best-performing results on the current dataset.}
\label{table: main results}
\end{table*}


% Vicuna-7B & \multirow{4}{*}{232} & 36.6 & 43.4 & 46.7 & \multirow{4}{*}{229} & 25.4 & 25.3 & 24.7 & \multirow{4}{*}{183} & 88.1 & 87.4 & 91.5 & 48.6 & 52.9 & 50.0 \\
% Vicuna-13B &  & 39.1 & 44.2 & 48.2 &  & 26.7 & 25.9 & 26.0 &  & 84.3 & 88.8 & 86.7 & 49.9 & 55.2 & 51.5 \\
% Llama2-7B-Chat &  & 34.0 & 41.7 & 42.3 &  & 24.6 & 25.0 & 24.0 &  & 92.0 & 89.9 & 92.6 & 50.0 & 53.2 & 51.6 \\
% Llama2-13B-Chat &  & 36.1 & 41.8 & 47.4 &  & 25.9 & 26.1 & 25.2 &  & 92.2 & 90.1 & 92.0 & 50.9 & 55.1 & 52.1 \\ \hline
% Vicuna-7B & \multirow{4}{*}{232} & {\graybackground{10} 36.6} & {\graybackground{20} 43.4} & {\graybackground{11} 46.7} & \multirow{4}{*}{229} & {\graybackground{2} 25.4} & {\graybackground{13} 25.3} & {\graybackground{3} 24.7} & \multirow{4}{*}{183} & {\graybackground{5} 88.1} & {\graybackground{3} 87.4} & {\graybackground{4} 91.5} & {\graybackground{5} 48.6} & {\graybackground{15} 52.9} & {\graybackground{7} 50.0} \\
% Vicuna-13B &  & {\graybackground{5} 39.1} & {\graybackground{8} 44.2} & {\graybackground{6} 48.2} &  & {\graybackground{18} 26.7} & {\graybackground{28} 25.9} & {\graybackground{15} 26.0} &  & {\graybackground{0} 84.3} & {\graybackground{1} 88.8} & {\graybackground{0} 86.7} & {\graybackground{4} 49.9} & {\graybackground{17} 55.2} & {\graybackground{9} 51.5} \\
% Llama2-7B-Chat &  & {\graybackground{2} 34.0} & {\graybackground{10} 41.7} & {\graybackground{4} 42.3} &  & {\graybackground{4} 24.6} & {\graybackground{6} 25.0} & {\graybackground{3} 24.0} &  & {\graybackground{8} 92.0} & {\graybackground{1} 89.9} & {\graybackground{3} 92.6} & {\graybackground{3} 50.0} & {\graybackground{11} 53.2} & {\graybackground{6} 51.6} \\
% Llama2-13B-Chat &  & {\graybackground{0} 36.1} & {\graybackground{6} 41.8} & {\graybackground{11} 47.4} &  & {\graybackground{4} 25.9} & {\graybackground{3} 26.1} & {\graybackground{3} 25.2} &  & {\graybackground{18} 92.2} & {\graybackground{3} 90.1} & {\graybackground{4} 92.0} & {\graybackground{6} 50.9} & {\graybackground{19} 55.1} & {\graybackground{13} 52.1} \\ \hline
% \end{tabular}
% }



\subsection{Generation Step}
After the ranking step, the top-$k$ ranked sentences, denoted as $\text{Top-}k$, are passed to the language model $M$ to generate the final answer $a$ for the given query $\mathbf{q}$. The generation process integrates the query and the retrieved sentences to produce a response that is both accurate and contextually relevant. The generation step can be formalized as:
\[
a = \text{Generate}(\mathbf{q}, \text{Top-}k; M),
\]
where $\text{Generate}(\cdot)$ represents the generation function that combines the query $\mathbf{q}$, the retrieved top-$k$ sentences $\text{Top-}k$, and the language model $M$ to produce the output $a$.

%  密集检索器通常更加合适召回场景，因此我们测试了三个较为经典的密集检索器

% 图中的上下部分分别表示滑动窗口聚焦在句子 M 和 M+1的场景。处于同一虚线框内的内容表示切分自同一个文档。一个核心句子的上下文由同一文档中除核心句子本身以外的句子拼接而成。
