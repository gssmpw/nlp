\section{Experiment Setups}

In this section, we describe the experimental setup for evaluating ParetoRAG across various scenarios. The specific model parameters can be found in Appendix \ref{appendix: Statistics of models}. The selection and meaning of the evaluation metrics can be found in Appendix \ref{appendix: Evaluation Metrics}.

% 这段也是照抄 记得降重
\subsection{Datasets.} 
%  介绍了 NQ, HotpotQA, Msmarco,  每个数据集有多少数据, 然后我随即采样了1000条进行测试 



We experiment on three different open-domain QA datasets as the retrieval source: Natural Questions (NQ) \cite{kwiatkowskiNaturalQuestionsBenchmark2019}, HotpotQA \cite{yangHotpotQADatasetDiverse2018}, and MS-MARCO \cite{nguyen2016ms}, where each dataset has a knowledge database. 
% The knowledge databases of NQ and HotpotQA are collected from Wikipedia. The knowledge database of MS-MARCO is collected from web documents using the MicroSoft Bing search engine.
These datasets encompass different tasks, such as open-domain question answering, multi-hop reasoning, and long-form answer generation. 
Each dataset also contains a set of questions. We randomly sampled 1,000 data paris for testing. Table \ref{table:statistic-dataset} shows statistics of text unit counts before and after ParetoRAG encoding.



% \begin{table*}[ht!]
% \resizebox{\textwidth}{!}{ % 调整宽度为页面宽度，高度自动缩放
% \begin{tabular}{lccccccccccccccc}
% \hline
% \multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{4}{c}{NQ(acc)} & \multicolumn{4}{c}{Hotpot(acc)} & \multicolumn{4}{c}{MS(mauve)} & \multicolumn{3}{c}{MS(rouge)} \\
% \multicolumn{1}{c}{} & \# tok & Contriever & ANCE & \multicolumn{1}{c}{DPR} & \# tok & Contriever & ANCE & \multicolumn{1}{c}{DPR} & \# tok & Contriever & ANCE & \multicolumn{1}{c}{DPR} & Contriever & ANCE & DPR \\ \hline
% \multicolumn{16}{c}{Without RAG} \\ \hline
% Vicuna-7B & \multirow{4}{*}{-} & \multicolumn{3}{c}{23.2} & \multirow{4}{*}{-} & \multicolumn{3}{c}{16.1} & \multirow{4}{*}{-} & \multicolumn{3}{c}{88.3} & \multicolumn{3}{c}{44.9} \\
% Vicuna-13B &  & \multicolumn{3}{c}{28.2} &  & \multicolumn{3}{c}{20.2} &  & \multicolumn{3}{c}{82.1} & \multicolumn{3}{c}{47.6} \\
% Llama2-7B-Chat &  & \multicolumn{3}{c}{20.9} &  & \multicolumn{3}{c}{16.0} &  & \multicolumn{3}{c}{85.6} & \multicolumn{3}{c}{50.0} \\
% Llama2-13B-Chat &  & \multicolumn{3}{c}{29.9} &  & \multicolumn{3}{c}{18.4} &  & \multicolumn{3}{c}{90.1} & \multicolumn{3}{c}{49.4} \\ \hline
% \multicolumn{16}{c}{Naive RAG} \\ \hline
% Vicuna-7B & \multirow{4}{*}{895} & 33.2 & 36.1 & 41.9 & \multirow{4}{*}{757} & 25.0 & 22.3 & 23.9 & \multirow{4}{*}{576} & 84.1 & 84.9 & 87.9 & 46.5 & 45.9 & 46.7 \\
% Vicuna-13B &  & 37.4 & 41.0 & 45.6 &  & 22.6 & 20.2 & 22.7 &  & 86.8 & 87.7 & 87.0 & 47.8 & 46.8 & 46.9 \\
% Llama2-7B-Chat &  & 33.2 & 37.9 & 40.8 &  & 23.6 & 23.4 & 23.3 &  & 85.0 & 88.6 & 89.2 & 48.5 & 48.0 & 48.3 \\
% Llama2-13B-Chat &  & 38.3 & 39.6 & 42.7 &  & 27.1 & 25.3 & 26.8 &  & 77.5 & 87.0 & 88.8 & 47.8 & 46.1 & 46.0 \\ \hline
% \multicolumn{16}{c}{ParetoRAG} \\ \hline
% Vicuna-7B & \multirow{4}{*}{232} & {\graybackground{20} 36.6} & {\graybackground{40} 43.4} & {\graybackground{22} 46.7} & \multirow{4}{*}{229} & {\graybackground{4} 25.4} & {\graybackground{26} 25.3} & {\graybackground{6} 24.7} & \multirow{4}{*}{183} & {\graybackground{10} 88.1} & {\graybackground{6} 87.4} & {\graybackground{8} 91.5} & {\graybackground{10} 48.6} & {\graybackground{30} 52.9} & {\graybackground{14} 50.0} \\
% Vicuna-13B &  & {\graybackground{10} 39.1} & {\graybackground{16} 44.2} & {\graybackground{12} 48.2} &  & {\graybackground{36} 26.7} & {\graybackground{56} 25.9} & {\graybackground{30} 26.0} &  & {\graybackground{0} 84.3} & {\graybackground{2} 88.8} & {\graybackground{0} 86.7} & {\graybackground{8} 49.9} & {\graybackground{34} 55.2} & {\graybackground{18} 51.5} \\
% Llama2-7B-Chat &  & {\graybackground{4} 34.0} & {\graybackground{20} 41.7} & {\graybackground{8} 42.3} &  & {\graybackground{8} 24.6} & {\graybackground{12} 25.0} & {\graybackground{6} 24.0} &  & {\graybackground{16} 92.0} & {\graybackground{2} 89.9} & {\graybackground{6} 92.6} & {\graybackground{6} 50.0} & {\graybackground{22} 53.2} & {\graybackground{12} 51.6} \\
% Llama2-13B-Chat &  & {\graybackground{0} 36.1} & {\graybackground{12} 41.8} & {\graybackground{22} 47.4} &  & {\graybackground{8} 25.9} & {\graybackground{6} 26.1} & {\graybackground{6} 25.2} &  & {\graybackground{36} 92.2} & {\graybackground{6} 90.1} & {\graybackground{8} 92.0} & {\graybackground{12} 50.9} & {\graybackground{38} 55.1} & {\graybackground{26} 52.1} \\ \hline
% \end{tabular}
% }
% \caption{Overall experiment results of three retrievers on three tasks. The darker the \graytext{gray background}, the higher the relative improvement compared to Naive RAG. }
% \label{table: main results}
% \end{table*}





\subsection{Dense Retrieval Models}
%  总起句: 介绍这三种 retriever models, 然后说明了默认用的是 dot
We compare the performance of the three following unsupervised, semi-supervised,  or supervised dense retriever models. Following previous studies~\cite{lewisRetrievalaugmentedGenerationKnowledgeintensive2020a}, by default, we use the dot product between the embedding vectors of a question and a text in the knowledge database to calculate their similarity score. 

\textbf{Contriever} \cite{izacardUnsupervisedDenseInformation2021} is an unsupervised retriever implemented using a BERT-base encoder. Contriever is contrastively trained on segment pairs constructed from unlabeled documents in Wikipedia and web crawl data.

\textbf{ANCE} \cite{xiongApproximateNearestNeighbor2020} is a dual-encoder BERT-base model designed for dense retrieval tasks. It is trained using weakly supervised signals from query-document pair labels, typically sourced from datasets such as MS-MARCO.

\textbf{DPR} \cite{karpukhinDensePassageRetrieval2020} is a dual-encoder  BERT-base model fine-tuned on passage retrieval tasks directly using the question-passage pair labels from NQ, TQA \cite{joshiTriviaQALargeScale2017}, SQuAD \cite{rajpurkarSQuAD100000Questions2016} and WebQ \cite{berantSemanticParsingFreebase2013}.

\begin{figure*}[ht!]
    \centering
    % 左侧子图 (a) 和 (b)
    \begin{subfigure}[b]{0.45\textwidth} % 左侧子图占一半宽度
        \centering
        % 子图 (a)
        \begin{subfigure}[b]{0.48\textwidth} % 每个子图占一半宽度
            \centering
            \includegraphics[width=\linewidth]{figure/ParetoRAGforRobustLLM-a.pdf} % 替换为图 (a) 的路径
            \caption{NQ (Top 10)}
            \label{fig:PareroRAGforRobustLLM-a}
        \end{subfigure}
        % 子图 (b)
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figure/ParetoRAGforRobustLLM-b.pdf} % 替换为图 (b) 的路径
            \caption{HotpotQA (Top 10)}
            \label{fig:PareroRAGforRobustLLM-b}
        \end{subfigure}
    \end{subfigure}
    % 右侧子图 (c) 和 (d)
    \begin{subfigure}[b]{0.45\textwidth} % 右侧子图占一半宽度
        \centering
        % 子图 (c)
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figure/ParetoRAGforRobustLLM-c.pdf} % 替换为图 (c) 的路径
            \caption{NQ (400 words)}
            \label{fig:PareroRAGforRobustLLM-c}
        \end{subfigure}
        % 子图 (d)
        \begin{subfigure}[b]{0.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figure/ParetoRAGforRobustLLM-d.pdf} % 替换为图 (d) 的路径
            \caption{HotpotQA (400 words)}
            \label{fig:PareroRAGforRobustLLM-d}
        \end{subfigure}
    \end{subfigure}
\caption{Comparison of ParetoRAG and Naive RAG on the adaptive noise-robust LLM (\texttt{llama-2-13b-peft-nq-retrobust} and \texttt{llama}-\texttt{2-13b-peft-hotpotqa-retrobust} \cite{yoran2024making}): (a)(b) show performance under the same recall size (Top 10), while (c)(d) illustrate performance under the same input word count (400).}
\label{fig:ParetoRAGforRobustLLM}
\end{figure*}


\subsection{Baselines}
% 在这三个基线（baseline）上，我们分别测试了多种公开可用的指令微调模型（如 Vicuna-7B、Vicuna-13B），以及使用私有数据进行训练和强化的模型（如 Llama2-7B-chat 和 Llama2-13B-chat）。

For these three baselines, we evaluated publicly available instruction-tuned models, such as Vicuna-7B and Vicuna-13B \cite{zhengJudgingLLMasaJudgeMTBench2023} , as well as models trained and reinforced with private data, including Llama2-7B-Chat and Llama2-13B-Chat \cite{touvronLlama2Open2023}. 

%  我们对多种未采用RAG技术的LLMs在多种数据集上的性能进行了评估。
\textbf{Baselines without retrievals. }We evaluate the performance of various LLMs without employing RAG technology across multiple datasets.

% Standard RAG 指的是仅使用最基础的RAG技术，未采用复杂的检索优化方法（如嵌入检索、向量检索）或高级生成机制，仅使用基础的检索-生成流程。
\textbf{Baselines with Naive RAG.} We employ the most basic RAG technique, without incorporating complex retrieval optimization methods or advanced generation mechanisms, relying solely on the fundamental retrieval-generation workflow.

%  SOTA 方法的基线。我们比较了两种先进的方法：(1) RECOMP(Xu et al., 2024) 仅使用生成式摘要模型（排除抽取式变体）对检索段落进行语义重构，通过专用模型生成精炼内容；以及（2）使用对抗性噪声训练的 Retrobust LLM 以提高鲁棒性。
\textbf{Baselines with SOTA methods.} We implement two advanced approaches: (1) Recomp \cite{xu2024recomp}  using abstractive summarization (excluding extractive variants) to synthesize retrieved passages with dedicated models. (2) LLM trained with adversarial noise to improve robustness \cite{yoran2024making}. 







% “为了在句子级别和段落级别方法之间实现公平比较，我们在句子级别检索中对输入的总token数进行控制，使其与段落级别检索的输入token数近似相当。由于对文档进行了句子细粒度的划分，导致文档规模发生了显著变化，例如，在NQ数据集中，文档行数从2681468行增加到了9320506行。这样的变化使得设置相同的top k已不适用于当前的召回场景。通过限制输入的总token数，我们能够在句子级别和段落级别方法之间实现更公平的对比。尽管句子和段落级别方法在信息表达的形式和上下文完整性上可能存在差异，这种设计在实验设置中保证了两种方法的输入规模一致性，避免了因输入长度差异引入的性能偏倚。此外，这也确保了输入给LLM的内容在长度和处理预算上的一致性，从而更便于对两种方法在下游QA任务中的表现进行对比和分析。”

% \textbf{Fairness in Comparsion.} In order to facilitate a fair comparison between [xxx] sentence-level and paragraph-level approaches, we controll the total number of input tokens in sentence-level retrieval to approximate that of the paragraph-level retrieval. Given the granular division of documents into sentences, there has been a significant change in the scale of documents. For instance, in the NQ, the number of document lines increased from 2,681,468 to 9,320,506. Such changes render the previous setting of the same top-k inapplicable for current retrieval scenarios. 
% \par
% \textit{By limiting the total number of input tokens}, we are able to achieve a more equitable comparison between [xxx] sentence-level and paragraph-level methods. Although there may be differences in the form of information expression and the integrity of context between sentence- and paragraph-level methods, this design ensures consistency in input scale between the two methods in experimental setups, thus avoiding performance bias introduced by differences in input length. Furthermore, this also ensures consistency in the length and processing budget of content fed to LLMs, thereby facilitating comparative analysis of the performance of the two methods in downstream QA tasks. 
% % 为保证输入规模的一致性，当Naive RAG召回前10篇文档时，Sentence RAG应召回前30篇文档。具体的计算和分析过程详见附录（Appendix）。
% \par
% To ensure consistency in input size, ParetoRAG should retrieve the top 30 documents when naive-RAG retrieves the top 10 documents. The detailed calculation process is provided in Appendix \ref{Appendix: Input Size Consistency}

% \textbf{Hyperparameter setting.}