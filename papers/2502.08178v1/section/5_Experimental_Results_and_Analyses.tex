\section{Experimental Results and Analysis}
In this section, we show the overall experimental results with in-depth analyses of our framework.

\subsection{Main Results}

Table \ref{table: main results} presents the results of three retrievers on three datastes, based on top-30 recall contents. Figure \ref{fig:ParetoRAGforRobustLLM} illustrates the performance of ParetoRAG on \path{llama2-13b-retrobust}. From these results, we can conclude the following findings:

% \textbf{ParetoRAG 在仅消耗原先约 30% 的 token 成本的情况下，依然实现了显著的准确率和流畅度提升。} 如表 \ref{table: main results} 所示，例如在 NQ 数据集上，Vicuna-7B + ANCE 的准确率从 36.1% 提升至 43.4%（+7.3%）。同样地，Llama2-13B-Chat + DPR 的准确率从 42.7% 提升至 47.4%（+4.7%），同时 token 数从 895 减少至 232（约为 26%）。在 HotpotQA 数据集上，Vicuna-13B + ANCE 的准确率从 20.2% 提升至 25.9%（+5.7%），而 token 数从 757 减少至 229（约为 30%）。
% 此外，在 MS-Marco 数据集上，ParetoRAG 在mauve(流畅度)和rouge(正确性)指标上也实现了可观提升。例如，基于 Llama2-13B-Chat + Contriever 的流畅度分数从 77.5 提升至 92.2(+14.7%)， 同时 token 数减少至约为原先的 32%。；基于 Vicuna-7B + DPR的流畅度分数从 87.9 提升至 91.5(+3.6%)。在正确性（rouge）方面，Vicuna-13B + ANCE 的 rouge 分数从 46.8 提升至 55.2(+8.4%)，而 Llama2-13B-Chat + ANCE 的 rouge 分数从 46.1 提升至 55.1(+9%)。
% 这些结果进一步凸显了 ParetoRAG 的能力，即使在显著减少 token 消耗的情况下，依然能够在流畅度和准确率方面实现稳定且可量化的提升。
\textbf{ParetoRAG, while consuming only about 30\% of the original token cost, still delivers  notable  improvements in accuracy and fluency.} Specifically,  as shown in Table \ref{table: main results}, in NQ, the accuracy of Vicuna-7B + ANCE increases from 36.1\% to 43.4\% (+7.3\%), with the token count reduced to 26\% of the original. Similarly, the accuracy of Llama2-13B-Chat + DPR increases from 42.7\% to 47.4\% (+4.7\%), with the token count reduced to approximately 30\% of the original. In HotpotQA, the accuracy of Vicuna-13B + ANCE improves from 20.2\% to 25.9\% (+5.7\%), with the token count reduced to approximately 30\% of the original. 

In addition, in MS-Marco, ParetoRAG achieves notable improvements in both mauve (fluency) and rouge (correctness) metrics. For example, the fluency score of Llama2-13B-Chat + Contriever increases from 77.5 to 92.2 (+14.7\%), while the token count is reduced to approximately 32\% of the original. Similarly, the fluency score of Vicuna-7B + DPR improves from 87.9 to 91.5 (+3.6\%). In terms of correctness (rouge), the rouge score of Vicuna-13B + ANCE increases from 46.8 to 55.2 (+8.4\%), while Llama2-13B-Chat + ANCE improves from 46.1 to 55.1 (+9\%). These results further highlight ParetoRAG's capability to deliver consistent and measurable improvements in both fluency and accuracy, even with significantly reduced token consumption. 

% 我的方法有很强的泛用性, 主要从三个方面来看: 1. 在多个数据集上均有效; 2. 在不同种类的召回器上均有效 3. 对多个LLM均有效.
% 在多个数据集上的有效性证明了 ParetoRAG 的鲁棒性。它在 NQ、HotpotQA 和 MS-Marco 等多样化的数据集上持续提升了性能。这些数据集涵盖了不同的任务，例如开放域问答、多跳推理以及长文本回答生成，展现了该方法的多功能性。
% 

\begin{figure*}[ht!]
    \centering
    % 左侧子图
    \begin{subfigure}[b]{0.31\textwidth} % 左右各占 48%，留一些间隙
        \centering
        \includegraphics[width=\linewidth]{figure/data-distribution/data-distribution-nq.pdf}
        \caption{NQ distribution}
        \label{fig:nq distribution}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    % 右侧子图
    \begin{subfigure}[b]{0.31\textwidth} % 左右各占 48%，留一些间隙
        \centering
       \includegraphics[width=\linewidth]{{figure/data-distribution/data-distribution-hotpotqa.pdf}}
        \caption{HotpotQA distribution}
        \label{fig:hotpotqa distribution}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    \begin{subfigure}[b]{0.31\textwidth} % 左右各占 48%，留一些间隙
        \centering
        \includegraphics[width=\linewidth]{{figure/data-distribution/data-distribution-msmarco.pdf}}
        \caption{MS-MARCO distribution}
        \label{fig:msmarco distribution}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    \caption{Correct answer rank distributions across different datasets under the the same input word count (400).}
    \label{fig:data-distribution}
\end{figure*}

\textbf{ParetoRAG  demonstrates strong generalizations.} We analyze its effectiveness from three perspectives:


\textit{Effectiveness across multiple datasets: }ParetoRAG consistently improves performance across a diverse range of datasets, including NQ, HotpotQA, and MS-Marco. 
% These datasets encompass different tasks, such as open-domain question answering, multi-hop reasoning, and long-form answer generation, showcasing the versatility of the method. 
In contrast, Recomp does not have a specialized abstract model for the MS-MARCO task, resulting in a significant drop in performance on MAUVE (fluency) and a smaller improvement on ROUGE-L (correctness) compared to ParetoRAG.

\textit{Compatibility with different types of retrievers:} ParetoRAG proves effective with various retriever types, including Contriever, ANCE, and DPR. This shows that the method is not tied to a specific retriever and adapt well to different retrieval methods. Specific analysis of the impact on retrievers can be found in \ref{sec:impact-of-retriever}.
    
\textit{Applicability across multiple LLMs:} ParetoRAG achieves improvements when applied to large language models, such as Vicuna-7B, Vicuna-13B, Llama2-7B-Chat, and Llama2-13B-Chat. Notably, we also test the method on models trained with anti-noise techniques. As shown in Figure \ref{fig:ParetoRAGforRobustLLM}, the results still show improvements. More detailed analysis can be found in \ref{sec: detailed-analysis-on-robust-llm}. 





\subsection{Ablation Study}
We study the impact of core sentence weight, retriever types, and top k size on ParetoRAG. The variation of core sentence weight on HotpotQA and MS-MARCO can be found in Appendix \ref{appendix: Impact of core sentence weight}, while the impact of model parameters on ParetoRAG is detailed in Appendix \ref{appendix: Impact of Model Size on Accuracy with Varying Top K in ParetoRAG}.

\subsubsection{Impact of core sentence weight}

 From the Figure \ref{fig:the influence of core sentence weight between different retrievers}  it can be observed that when the weight of core sentences is adjusted to approximately 0.80, the Mean Recall@30 for ANCE, DPR and Contriever methods reaches optimal performance. This phenomenon reflects the impact of weight adjustment on the balance between contextual information and core sentences, which can be analyzed as follows:
 
\textbf{Performance Improvement at Optimal Weight (Around 0.80):} When the core sentence weight is set to approximately 0.80, the model effectively integrates contextual information with the content of core sentences. This balance enables the model to preserve semantic integrity while more accurately capturing key information relevant to the retrieval task, thereby achieving optimal recall performance.

\textbf{Performance Decline with Increased Weight (Beyond 0.80):} As the core sentence weight increases further toward 1.0, contextual information in the text is progressively diminished or even neglected, causing the model to rely more heavily on core sentences for retrieval. However, excessively weakening contextual information leads to a loss of semantic completeness, which adversely affects the accuracy of retrieval results. Consequently, performance declines beyond the 0.80 threshold.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/weightRecall.pdf}
    \caption{Impact of Core Sentence Weight on Recall across NQ Dataset.}
    \label{fig:the influence of core sentence weight between different retrievers}
\end{figure}


\textbf{High Weight Still Outperforms the Baseline (At 1.0):} Even when the core sentence weight reaches 1.0, resulting in the complete disregard of contextual information, the model's performance remains superior to the baseline of paragraph-level retrieval. This indicates that paragraph-level information often contains significant redundancy, while core sentences play a pivotal role in enhancing retrieval performance. By adjusting the weighting, ParetoRAG effectively reduces the spatial burden of paragraph content while incorporating more core sentences, thereby improving retrieval precision and optimizing efficiency simultaneously.


\subsubsection{Impact of retriever} 
\label{sec:impact-of-retriever}
% 图 \ref{fig:data-distribution} 比较了在不同数据集（NQ、HotpotQA 和 MS-MARCO）上使用 ParetoRAG 和 Naive RAG 时正确答案排名分布的差异。y 轴表示正确答案在检索结果中的百分比位置，x 轴表示正确答案在检索结果中的拟合密度分布。20% 表示正确答案出现在检索结果的前 20%。较高的百分比值对应于排名较低的位置，而接近 -1 的值表示未检索到正确答案的情况。
Figure \ref{fig:data-distribution} compares the ranking distribution of correct answers across different datasets (NQ, HotpotQA, and MS-MARCO) when using ParetoRAG and Naive RAG. The y-axis represents shows the percentage position of the correct answer within the ranked retrieval results, and the x-axis shows the fitted density distribution of the correct answer positions in the retrieval results. 20\% indicates that the correct answer appears in the top 20\% of the retrieval results. Higher percentage correspond to lower positions in the ranking, and values near -1 represent cases where the correct answer is not retrieved.

% 经过 ParetoRAG 的优化后，DPR、ANCE 和 Contriever 这三个召回器表现出以下共同趋势：首先，所有召回器的正确答案排名在 Rank 1（第一名）附近形成了更高的峰值，表明 ParetoRAG 能有效将正确答案推至检索结果的更高位置。其次，在 Rank -1 附近的分布密度显著降低，说明 ParetoRAG 减少了未能检索到正确答案的情况，从而提升了召回的全面性。此外，ParetoRAG 的分布曲线（蓝色虚线）整体比 Naive RAG（红色实线）更平滑，尤其是在中高排名区域（如 Rank 2~5），这表明 ParetoRAG 能更稳定地优化召回器性能并减少错误分布。

\begin{figure}[ht!]
    \centering
    % 左侧子图
    \begin{subfigure}[b]{0.23\textwidth} % 左右各占 48%，留一些间隙
        \centering
        \includegraphics[width=\linewidth]{figure/topkInfluence-vicuna7b.pdf}
        \caption{Vicuna-7B}
        \label{fig:vicuna-7b topk}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    % 右侧子图
    \begin{subfigure}[b]{0.23\textwidth} % 左右各占 48%，留一些间隙
        \centering
        \includegraphics[width=\linewidth]{figure/topkInfluence-vicuna13b.pdf}
        \caption{Vicuna-13B}
        \label{fig:vicuna-13b topk}
        \vspace{-5pt} % 减少间距
    \end{subfigure}
    \caption{Comparison of accuracy and recall rates of different retrievers under various top k conditions.}
    \label{fig:impact of topk size}
\end{figure}


After being optimized by ParetoRAG, the three retrievers, DPR, ANCE, and Contriever, exhibit the following common trends: First, the correct answer rankings for all retrievers form a higher peak around 20\%, indicating that ParetoRAG effectively pushes correct answers to higher positions in the retrieval results. Second, the density near -1 is significantly reduced, demonstrating that ParetoRAG decreases the cases where correct answers are not retrieved, thus improving the retrieval comprehensiveness. 

Lastly, the distribution curves of ParetoRAG (blue lines) are smoother compared to Naive RAG (red lines), particularly in the mid-to-high ranking regions (e.g., 40\% to 80\%). This indicates that ParetoRAG stabilizes the performance of retrievers and reduces erroneous distributions.



% 从图中可以观察到，当核心句子权重调整至 0.75 左右时，DPR 和 Contriever 方法的 Mean Recall@30 达到了性能最优。这一现象反映了权重调整对上下文信息与核心语句之间平衡的影响，具体可以分为以下几点：
%     最优权重的性能提升（0.75 附近）：
%     在核心句子权重为 0.75 时，模型能够有效融合文本中的上下文信息与核心语句的关键内容，充分利用段落级别和句子级别信息的互补特性。这种平衡使得模型能够在保留语义完整性的同时更准确地捕捉到检索任务中的关键信息，从而实现最佳召回性能。
%     权重增大后的性能下降（0.75 之后）：
%     当核心句子权重进一步增大至接近 1.0 时，文本中的上下文信息逐渐被弱化甚至忽略，模型更多依赖核心语句来完成检索。然而，过度削弱上下文信息会导致语义完整性下降，使得检索结果的精准度受到影响，因此性能在 0.75 之后出现下滑。
%     高权重依然优于基准线（1.0）：
%     尽管在核心句子权重为 1.0 时，模型完全丢弃了上下文信息，但其性能仍高于段落级别检索的基准线。这表明，段落级别的信息往往包含大量冗余，而核心语句在检索任务中对性能提升起到了关键作用。通过调整权重，我们的方法有效节省了段落内容空间，并引入了更多核心语句，从而在提升检索精准性的同时优化了效率。

% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figure/recall_compare.png}
%     \caption{the influence of core sentence weight between different retrievers}
%     \label{fig:the influence of core sentence weight between different retrievers}
% \end{figure}



\subsubsection{Impact of wider top k size}

% 由于ParetoRAG 在 Top 30 与 Naive RAG 在 Top 10 的输入规模相似，我们将 Naive RAG 的 Top 10 设置作为基准（baseline），并分别对 ParetoRAG 在 Top 10、Top 20 和 Top 30 设置下的表现进行评估，以探讨不同 Top K 设置对模型性能的影响。如图2所示，我们的主要观察如下：
Since the input size of ParetoRAG at Top 30 is similar to that of Naive RAG at Top 10 (more detailed can be seen in Appendix \ref{Appendix: Input Size Consistency}), we set the Top 10 performance of Naive RAG as the baseline. We then evaluate the performance of ParetoRAG in the top 10, top 20 and top 30 settings to investigate the impact of different top k configurations on model performance. As shown in Figure \ref{fig:impact of topk size}, our key observations are as follows:
\par
% 传统的基于段落的检索方法（Naive RAG）虽然能够实现较高的文档覆盖率，但往往包含大量无关信息，这些无关信息可能干扰语言模型（LLM）在回答问题时的准确性。而采用细粒度检索的 ParetoRAG 方法后，尽管在相同的 Top K（如 Top 10）设置下，其召回率相对较低，但语言模型的回答准确率却显著提升。这表明，ParetoRAG通过句子级别的文档召回，有效删减了大量无关内容，降低了模型的推理复杂度，从而帮助语言模型更高效地找到正确答案。
Although Naive RAG can achieve high document coverage, they often include a large amount of irrelevant information, which can interfere with the accuracy of LLM when answering questions. In contrast, with the fine-grained retrieval approach of ParetoRAG, although the recall rate is relatively lower under the same top k settings (e.g., Top 10), the accuracy of the language model's responses is significantly improved. This suggests that ParetoRAG, by performing document retrieval at the sentence level, effectively eliminates a substantial amount of irrelevant content, reducing LLM's inference complexity and enabling LLM to more efficiently identify the correct answer.
\par
% 对于较小的模型（如 Vicuna-7b），其处理大量文档的能力较弱，因此在 Top K 较大时准确率下降更快。然而，这种下降并非因 ParetoRAG 的召回质量不足，而是因为较小模型无法充分利用这些更加丰富的信息。此外，对于较大的模型（如 Vicuna-13b），其参数量和推理能力更强，能够在更大范围内处理更多的信息，因此即使 Top K 扩大到一定程度时（如 Top 20、Top 30），依然有较高的准确率。特别地，较大的 Top K 设置（如 Top 20）相比 Top 10 和 baseline 展现了更好的性能，这表明 ParetoRAG 能够提供更加丰富的信息召回，为语言模型提供更有效的上下文。
% For smaller models (such as Vicuna-7b), their ability to process a large number of documents is weaker, leading to a faster decline in accuracy as the Top K increases. However, this decline is not due to a lack of retrieval quality by ParetoRAG, but rather because smaller models are unable to fully utilize the richer information provided. On the other hand, for larger models (such as Vicuna-13b), their greater parameter size and reasoning capability enable them to handle more information within a larger scope. As a result, even when the Top K is increased to a certain extent (e.g., Top 20 or Top 30), they still maintain high accuracy. Notably, larger Top K settings (e.g., Top 20) outperform Top 10 and the baseline, demonstrating that ParetoRAG can provide richer information retrieval, offering more effective context for language models.

 \subsubsection{Complementary effect of ParetoRAG on adaptive noise-robust LLM}
 \label{sec: detailed-analysis-on-robust-llm}
%  即使对于已经经过抗噪训练、能够忽略无关或噪声上下文的模型(llama-2-13b-peft-nq-retrobust, llama-2-13b-peft-hotpotqa-retrobust)，ParetoRAG 依然能够持续提升性能。例如，在 NQ 数据集中, Top 10 检索设置下，使用 ANCE 时，准确率从 44.80% 提升至 48.20%（+3.4%）；在 HotpotQA 数据集中，输入文档字数为400的情况下，准确率从 23.3% 提升至 25.3%（+2%）。这些结果表明，ParetoRAG 能够在抗噪训练的基础上进一步提供性能改进。
% 尽管抗噪训练增强了模型的鲁棒性，但在处理长文本或多跳推理任务中仍可能因冗余或密集信息而表现受限。ParetoRAG 通过句子级表征减少冗余、提高信息密度，从而使模型更专注于相关内容，实现了对抗噪模型的有效补充。
% ParetoRAG 与抗噪训练之间的互补作用表明，结合这两种方法可以进一步优化检索和生成质量.未来的工作可以探索将 ParetoRAG 与其他训练技术相结合，以进一步增强其在更广泛场景中的性能。
In this section, we evaluate the impact of ParetoRAG on robustly trained models, which are fine-tuned for the NQ and HotpotQA datasets respectively. These models are trained to enhance robustness against irrelevant context. As shown in Figure \ref{fig:ParetoRAGforRobustLLM}, in NQ, under the Top 10 retrieval setting, the accuracy improved from 44.80\% to 48.20\% (+3.4\%) when using ANCE. In HotpotQA, with input words length limited to 400 words, the accuracy increases from 23.3\% to 25.3\% (+2.0\%). These results demonstrate that ParetoRAG can further enhance performance in addition to robustly trained models.

While robust training improves the model's resilience to noisy contexts, it may still struggle with redundant or dense information in tasks involving long texts or multi-hop reasoning. ParetoRAG mitigates this limitation by reducing redundancy and increasing information density through sentence-level representations, allowing the model to focus more effectively on relevant content, thereby serving as a valuable complement to robustly trained models.

The complementary effect between ParetoRAG and robust training LLM indicates that combining these two approaches can further optimize retrieval and generation quality. Future work could explore integrating ParetoRAG with other training techniques to further enhance its performance across broader scenarios.
% \begin{figure}[htbp]
%     \centering
%     % 第一张子图
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includesvg[width=\linewidth]{figure/topkInfluence-vicuna7b.svg}
%         \caption{Description of (a).}
%         \label{fig:subfig1}
%     \end{subfigure}
%     \hfill
%     % 第二张子图
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includesvg[width=\linewidth]{figure/topkInfluence-vicuna13b.svg}
%         \caption{Description of (b).}
%         \label{fig:subfig2}
%     \end{subfigure}
%     \caption{Main caption for the combined figure.}
%     \label{fig:main_figure}
% \end{figure}






% \textbf{Impact of LLMs.} Tabel [xxx] also shows the results of [xxx] for different LLMs in RAG.


