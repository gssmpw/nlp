\section{Validating CREDAL}
\label{sec:eval}

Incorporating human participants into the evaluation process provides valuable insights into the methodology's ability to achieve our intended objectives. This allows for an evaluation of whether the methodology effectively facilitates a deeper critical understanding of data models. Additionally, engaging a larger audience in the testing phase helps mitigate biases by capturing a range of perspectives, revealing potential ambiguities in seemingly straightforward concepts, and clarifying those that may initially appear complex. This process aids in validating both the content and the structure of the methodology, determining if it is accessible, appropriately concise, or comprehensive for the subject matter, and identifying other aspects that can only be revealed through application and feedback.

\paragraph{Audience selection.} To validate \credal, we conducted interviews with 11 students from applied science programs. Specifically, we combined two academic groups, interviewing 4 students from an MS program in data science and 7 students from a BS program in computer science at Ukrainian Catholic University in Lviv, Ukraine.\footnote{Our study received approval from the institution's ethics review board.}  Considering that \credal is intended for technical students and practitioners involved in data modeling and related tasks, including dataset creation, data analysis, and database design, this selection allows us to assess the relevance and utility of the methodology for its primary target audience.

\subsection{Interview process}
\label{sec:eval:process}

We started by conducting a workshop with participants to introduce the \credal methodology with the help of materials described in Section~\ref{sec:credal:guide}.  We then gathered their feedback on \credal using a semi-structured interview protocol.  The basic interview structure consisted of 14 questions, listed in Appendix~\ref{sec:questions}.  We supplemented this structure with follow-up prompts when necessary, particularly when clarification was needed.  Questions were divided into four categories:

\begin{itemize}    
    \item Participant data modeling background and experience, and their experience with \credal.  General feedback on the methodology, including its strengths and weaknesses, perceived change in own understanding of data modeling, and confidence in own data modeling skills. 

    \item Feedback on supporting materials, including a video tutorial and close reading example.  Feedback on the usefulness of the literary close reading analogy for learning \credal.

    \item Perceived effectiveness of \credal, including its practical applicability and the likelihood that participant would use the methodology in their work or studies.
        
    \item Feedback on how \credal may be improved in the future.
\end{itemize}

\subsection{Interview coding}
\label{sec:eval:coding}

We recorded and transcribed the interviews and then coded them using the codebook presented in Appendix~\ref{sec:Codebook}.  This codebook was created manually, based on consensus among researchers over multiple rounds of independent coding and follow-up discussions.  We also used Atlas.ti to help organize interview data (i.e., associate the manually generated codes with quotes in the interview transcripts). 
Note that Atlas.ti offers the capability to use Generative AI to generate relevant codes, including the option to align these codes with specific research questions for enhanced contextual relevance.  However, we opted for manual coding in this study. Given the novelty of our research thesis, manual coding ensured that no critical details were overlooked and allowed for more accurate identification of recurring themes.

To mitigate bias, we independently tagged interview transcripts using two approaches, \emph{questions-based} and \emph{context-based}.
\begin{itemize}
    \item[1.] \textbf{Questions-based coding method}:
    To develop a set of codes, we first reviewed the interview questions and established corresponding code groups. Based on participants' responses, we identified the most frequently mentioned themes, created relevant codes, and assigned them to the predefined code groups. With this approach, we generated 8 groups with 31 codes.

    \item[2.] \textbf{Context-based coding method:}
    Under this approach, we began by analyzing all interviews to identify the most frequently occurring topics. Codes were created for these phrases based on the context, and using these codes, we derived the names for the respective code groups. The codes were fine-grained, making them more specific and less repetitive. With this approach, we generated 5 groups with 63 codes.
\end{itemize}

After using these two approaches, we decided to analyze their similarities and differences. This process was also carried out manually, with the authors of the two previous approaches looking for corresponding codes that were assigned to the same quotation (marked place in the interview text) or had identical meanings.\\

In the case of code groups, we decided to keep four groups that best corresponded to our objectives and to the research questions:
\begin{itemize}
    \item[A:] Participant background (3 codes)
    \item[B:] Methodology effectiveness (3 codes), relevant for research questions \textbf{RQ1.2}, \textbf{RQ1.3}, \textbf{RQ2.1}, \textbf{RQ2.2}, and \textbf{RQ2.3}
    \item[C:] Methodology improvement (6 codes)
    \item[D:] Methodology strengths (3 codes), relevant for research question \textbf{RQ1.1}
\end{itemize}

All codes, code groups, and their explanation can be found in Appendix~\ref{sec:Codebook}. 