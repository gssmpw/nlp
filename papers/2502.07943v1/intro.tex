\section{Introduction}
\label{sec:intro}

As the saying goes, knowledge is power.  Through knowledge, communities make their worlds.  Yet, as has been highlighted since Foucault, the converse also holds: power is knowledge \cite{foucault}. It is primarily those with power who are enabled to resource the data systems that drive contemporary data-driven decision and knowledge making.  This, in turn, leads to a perpetuation of the status quo, where what is known is predominantly for and by those with power.  There is a vicious cycle in data systems of power begetting more power.  Consequently, data systems are too often key enablers and amplifiers of the cruelties of the status quo \cite{ansorge,becker,benjamin,bode,costanza,couldry,eubanks,stevens,LewinskiBS24,biopower,weizenbaum}.

In this work, we highlight an emerging thread in the broader conversation of how to open up and intervene in cycles of data for more equitable and just data futures.  We focus on a seemingly mundane yet critically vital aspect of data and information system design and engineering: data modeling.\footnote{Here, a data model is also known as a conceptual model, an ontology, a knowledge graph, an entity-relationship diagram, a T-box, a set of features, a set of variables of an experiment, a semantic network; basically, a collection of classes and relationships between these classes. See Figure \ref{fig:example} for an example.}  Every information system (every algorithm, every machine learning model, every statistical model, every AI solution, every database) has an underlying data model without which the system would not be usable.  Data models, whether implicit or explicit, are present at the birth of data and are necessary for the design, development, and use of any data system. Hence, data modeling is a core topic taught in computer science, data science, software engineering, information systems, and information science degree programs.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/sn_example.png}
    \caption{Data model for Example~\ref{ex:1}.
   In this data model, \texttt{Person} is a class and \texttt{Knows} is a relationship between elements of this class; furthermore, instances of class \texttt{Person} have ``name'', ``dob'',  and ``race''  attributes, and \texttt{Knows} relationships have a ``since'' attribute. 
    As an example data instance of this model: Saori is a \texttt{Person} (name ``Saori'', race ``Asian'', and date of birth 2001) who \texttt{Knows} Kotaro (who is also a \texttt{Person}, name ``Kotaro'', race ``Native Hawaiian or Other Pacific Islander'', and date of birth 2002) since 2018.    }
    \label{fig:example}
\end{figure}

\begin{example}
\label{ex:1}
Consider the data model visualized in Figure \ref{fig:example}. 
Here, ``race'' takes a single value drawn from a fixed list, standardized ``for all Federal reporting purposes'' in the USA: American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or Other Pacific Islander, and White.\footnote{\url{https://www.doi.gov/pmb/eeo/directives/race-data}}
    What does it mean that race is single-valued and drawn from a fixed domain?  In which worlds is this meaningful?  How does it conflict with other worlds, where race is multi-valued and open-ended?  To what ends does the data system even model race in the first place?  Why is race an attribute of people, i.e., modeled as inherent (objectively) in a person?  Or are people racialized by other people, in which case race would better be modeled as a separate class or as a relationship between people?  In what world is race a fixed, limited, single, unmovable, inherent feature of people?  Why is the data system provider perpetuating this world?~\footnote{See 
    Chapter 6: The case of race classification and reclassification under apartheid, 
    Bowker and Star~\cite{sorting}. }
\end{example}

How can we systematically unpack a data model, to interpret it as the contextualized creative cultural artifact that it is?  Drawing inspiration from literary criticism,  our proposal is to closely read data models in the same spirit we read literature.   Close reading is a well-known approach to literary analysis that involves careful and sustained interpretation of textual artifacts \cite{frank}. Questions to consider during close reading include the speaker, the audiences (intended or otherwise), the reader's assumed knowledge, the purpose of included details, ambiguous words or phrases, patterns, rhythm, movement, and anything that raises questions or requires clarification.  

Students are often taught step-by-step approaches for conducting a close reading of a literary passage.\footnote{e.g., \href{https://writing.wisc.edu/handbook/closereading/}{U. Wisconsin Madison Writer's Handbook}, \href{https://www.york.ac.uk/english/about/writing-at-york/writing-resources/close-reading/}{U. York -- Writing at York}}
This typically begins with selecting a short section of text and paying attention to unusual or repetitive imagery and themes. The reader is then guided to read the passage, taking notes on language use, repetitions, and changes in tone. The analysis phase involves examining elements such as diction, narrative voice, and rhetorical devices. Following this, a descriptive thesis is formulated based on language observations, and finally, an argument is developed, connecting language use to the broader themes of the text. The method emphasizes the importance of understanding not just how language is used but why, encouraging a thoughtful exploration of the author's intentions and the text's overall significance.

\subsection{Why close readings of data models?}
While working with (and within) data models, we bracket off the worlds behind the model's abstraction, often to the point that we lose contact. This bracketing is, after all, the purpose of modeling: to highlight some descriptions of the world over other descriptions.  For many doing the technical work in computing and data science, this contact is never made (or held in abeyance, as ``low value'' messy labor, the dirty wrangling work of data  \cite{SambasivanKHAPA21}).  

Close readings of data models reconnect us with, among other things:

\paragraph{The materiality of data models}  
As Brian Cantwell Smith points out in early work on the limits of modeling in information systems,   ``one of the most important facts about computers ... is that we plug them in'' \cite{Smith85}.  This highlighting of the materiality of computing was taken up by Paul Dourish's study of data systems,  underscoring that ``the material arrangements of information ... matters significantly for our experience of information and information systems'' \cite{dourish}.  Too often, data is discussed as though it were immaterial, abstractions floating above the physical world. Close readings of data models can draw our attention to the grounding of data in the physical world and infrastructures, with physical effects and side effects in our bodies, embodied communities, and environments \cite{hogan}:
{\em  Where does this data model (and its data) reside?  Is it hosted in the cloud?  Where are the data centers for the cloud provider?  What are the reasons for this and the implications (organizational, legal, social, political, environmental, ethical, ...)? ...}

\paragraph{The closure of data models}  
Sabina Leonelli's study of data journeys has underscored the relational nature of data, namely, that data exists only in relation to a particular context, narrative-making community, and moment of inquiry
\cite{datajourney,Leonelli2015}.
As such, a data model is an interpretation, a biased window onto life \cite{feinberg,maleve}, a closure, a forgetting of the limitless alternatives, the unbounded space of interpretations not taken \cite{parrish,onuoha,azoulay,sherman}.  
And the information systems built around the model and their deployment live within the closure,  enabling and enacting the world of the closure, for better and for worse \cite{ansorge,bopp,martin,LewinskiBS24}.
As a rich example, von Lewinski et al.\ have carefully demonstrated this worlding power of data models in the legal domain \cite{LewinskiBS24}.
Close readings help us to open the closure, to find the seams and stitches that hold it together, and to surface what has been left out, and why, and what has been enclosed, and why:
{\em Why do we have this class?  Why these attributes?  Who views the world this way?  Who doesn't?  What does it mean to view the world this way?  Who benefits?  Who is left out? ...}

\paragraph{The genealogies of data models} 
Technologies such as data, data models, and data systems have histories, histories which we inhabit as we work and live with and within concrete data models.  For example, recent scholarship has continued to deepen our understanding of how datafication, data analytics, and data systems have been intertwined with the imperatives of slavery, eugenics, war, colonialism, and state and industrial surveillance and control of citizens and workers (e.g.,  \cite{penn,valdivia,whittaker,wiggins}). 
Scholarship in programming languages and software engineering is also highlighting the genealogies of these technologies (e.g., \cite{parrish,hermans}).   Close readings help us to connect the data model at hand to the broader worldviews and ideologies of datafication: 
{\em Why is this a data problem in the first place?  What are alternatives to datafying this problem?  Why is this a problem at all?  What are the implications of problematizing this activity/inquiry?  What are the implications of taking non-data-centric approaches?  Are we constraining future possibilities by taking a data-driven approach now? Which futures does this data model imagine? ...}

\paragraph{The techne of data models} Close readings can help us to unpack and understand the production and engineering of the model, namely, the tools, methods, and practices employed during the life of the model:  
{\em Which software tools were used to create the model?  Was the model influenced by this tooling?  Is a data system or platform used to implement the model?  If so, what are the modeling functionalities of the system (e.g., schema language, constraint language, logical modeling language)?  Are these functionalities and capabilities suitable for the data model?  If not, what are the workarounds?  Is there an expected or known query or analytics workload?  Can the system support this workload? Will the data be governed?  What governance infrastructure is available? ...}

\paragraph{The design of data models} Shining light on the materiality, genealogy, closure, and techne of data models, close readings help us to shift registers, from the classical textbook view of modeling as a description and documentation of a given fixed reality, to a view of modeling as design, as a creative socio-political activity of diplomacy, compromise, power struggles, and consensus making \cite{Feinberg17,shaw,simsion}.  Close readings lead us to ask: {\em Who was involved in making this model?  What are their roles?  What are their professional, organizational, and personal relationships?  Who led the process?  Who initiated the process?  Was the process hindered -- technically, organizationally, legally, politically, ...? Are there open research challenges here to remove (or introduce) friction or facilitate (or inhibit) design activities? ...}

\subsection{The challenge: How to read data models?}
To our knowledge, a systematic methodology for reading data models currently does not exist.  While recognizing from literary theory that there is no one correct way to read \cite{frank}, and that in fact every encounter of a reader with a text is a non-repeatable experiment, it is nonetheless critical to have a starting point for those unfamiliar with close readings.  This is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside the socio-political aspects of data work.

\subsection{Goals and Contributions}
In our work, we have developed \credal, a structured methodology for the close reading of data models.  Such a methodology must be designed to facilitate a nuanced exploration of language, relationships, and patterns within the data models, aiming to uncover the biases within modeling choices and contribute to the creation of fair and just knowledge representations.
The goal of this research extended beyond the mere development of \credal; it was equally focused on ensuring its effectiveness, usefulness, and applicability in practice. Therefore, we guided our research using the following research questions:

\begin{enumerate}
    \item [\textbf{RQ1:}] Is \credal usable and useful? 
    \begin{enumerate}
        \item [\textbf{RQ1.1:}] Is \credal easy to learn and apply?
        \item [\textbf{RQ1.2:}] Does \credal improve data modeling proficiency?
        \item [\textbf{RQ1.3:}] Are learners likely to use \credal in the future?
    \end{enumerate}
    \item [\textbf{RQ2:}] Does \credal help understand, design and critically evaluate data models?

        \begin{enumerate}
            \item [\textbf{RQ2.1:}] Does \credal help with understanding data models?
            \item [\textbf{RQ2.2:}] Does \credal alter the approach to structuring and modeling data within modeling tasks?
            \item [\textbf{RQ2.3:}] Do learners experience a change 
            in their data modeling perspectives after working with \credal?
        \end{enumerate}
\end{enumerate}

We will present the results of a qualitative study towards answering each of these questions,  highlighting the \emph{perceived benefits} of \credal, and providing concrete indications for further research and development of the methodology. 