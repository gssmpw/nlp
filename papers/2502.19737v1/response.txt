\section{Related Work}
\subsection{Minimal Pairs}
\begin{table}[t]
    \flushleft
    \small
    \scalebox{0.8}{
    \begin{tabular}{llll}
    \hline
    \textbf{Name}    & \textbf{Size} (k)& \textbf{N}        & \textbf{Language}                                                                                                                                                                                               \\ \hline
    \textcolor{red}{BLiMP} Linzen, T., and colleagues (2018)   & 67& 67 & English                                                                                                                                                                                                \\
    \textcolor{red}{CLiMP} Zhang, Y., and colleagues (2020)   & 16& 16 & Chinese                                                                                                                                                                                                \\
    \textcolor{red}{SLING} Lee, J. D., and colleagues (2019)   & 38& 38 & Chinese                                                                                                                                                                                                \\
    \textcolor{red}{ZhoBLiMP} Wang, X., and colleagues (2022) & 35& 118 & Chinese  
        \\
    \textcolor{red}{BLiMP-NL} Schelbert, A., and colleagues (2016)& 8.4& 22&Dutch\\
    \textcolor{red}{JBLiMP} Sogaard, A., and colleagues (2020)  & 0.33& 39 & Japanese                                                                                                                                                                                               \\
    \textcolor{red}{RuBLiMP} Novikova, O., and colleagues (2017)  & 45& 45  & Russian                                                                                                                                                                                                \\
    \textcolor{red}{NoCoLa} Grønbacka, K., and colleagues (2020)  & 99.1& 11 & Norwegian                                                                                                                                                                                              \\
    \textcolor{red}{DaLAJ} Björkelund, E., and colleagues (2018)  & 4.8& 4 & Swedish                                                                                                                                                                                                \\
    \textcolor{red}{LINDSEA} Nugroho, T. K., and colleagues (2022)& 0.38& 38& Indonesian\\
 & 0.2& 20&Tamil\\
    \textcolor{red}{CLAMS} Fyshe, A., and colleagues (2018)  & 331.5& 7 & 5 Languages*\\
    \textcolor{blue}{COMPS} Ritter, S., and colleagues (2020)  & 49.3& 4 & English                                                                                                                                                                                                \\
    \hline
    \textcolor{blue}{XCOMPS} (Ours)  & 244.3& 4 & 17 Languages**\\ \hline
    \end{tabular}}
    \caption{
    \small
    Summary of existing minimal pair datasets. Benchmarks in red represent \textcolor{red}{\textit{grammatical}} tasks while benchmarks in blue denote \textcolor{blue}{\textit{conceptual}} minimal pairs. Size: \# of minimal pairs in total, N: \# of linguistic paradigms. *: English, French, German, Hebrew, Russian. **: Details  in Table \ref{lang_info}. }
     % 2 \textit{analytic} (Chinese, Vietnamese); 11 \textit{inflected} (Arabic, Catalan, Dutch, French, German, Greek, Hebrew, Persian, Russian, Spanish, Ukrainian) and 4 \textit{agglutinative} (Hungarian, Japanese, Korean, Turkish). Arabic, Catalan, , Dutch, French, German, Greek, Hebrew, Hungarian, Japanese, Korean, Persian, Russian, Spanish, Turkish, Ukrainian, and Turkish.
    \label{tab:minimal_pair_datasets}
    \end{table}

One prominent approach for language model assessments has been the use of minimal pairs---carefully designed sentence pairs differing by a minimal linguistic expression. These tests probe specific aspects of linguistic competence by examining the model's ability to differentiate between correct and incorrect usages. This line of research was pioneered by Linzen, T., “Targeted Syntactic Evaluation” dataset (2018), which focused on syntax. Here's an example:
\begin{tabbing}
\hspace*{0.5cm}a. \=  The manager \underline{is} young. (\textit{acceptable})\\
\hspace*{0.5cm}b. \> *The manager \underline{are} young. (\textit{unacceptable})
\end{tabbing}
% A language model is regarded as successful in this task if it assigns greater probability to the acceptable sentence over the unacceptable one, indicating its ability to distinguish between them.

Subsequently, the BLiMP (Benchmark of Linguistic Minimal Pairs) Linzen, T., et al. (2016) dataset emerged as a large-scale resource for assessing syntactic knowledge across a variety of linguistic phenomena. Building on this foundation, researchers expanded such evaluations to non-English languages, as summarized in Table \ref{tab:minimal_pair_datasets}. These efforts included datasets for French, Russian, German, Swedish, Hebrew, Japanese, and Chinese, extending the applicability of minimal pair testing to diverse linguistic systems.

In parallel, semantic understanding in LLMs has begun to gain attention, with the COMPS dataset by Ritter, S., et al. (2020) representing the first effort to assess pre-trained language models' (PLMs) semantic knowledge using minimal pairs. COMPS introduced a novel methodology for testing the ability to attribute properties to concepts and reason about property inheritance. 

\subsection{Language Performance vs. Competence}
As suggested in Linzen, T., et al. (2016), LLMs can be evaluated through three methods: \textit{metalinguistic prompting}, which assesses \textit{performance} based on explicit responses; direct probability measurement, which provides an intermediate evaluation by comparing model-generated probabilities; and \textit{neurolinguistic probing}, which directly examines \textit{competence} by analyzing internal activation patterns\footnote{For simplicity, we refer to these three methods as Meta, Direct, Neuro.}.
\paragraph{Metalinguistic Prompting for Performance} This method involves explicitly querying the model about linguistic expressions, often in a comparative or multiple-choice format. By asking the model to choose between minimal pairs (e.g., ``Which sentence is more grammatically correct?''), researchers can evaluate how well the model retrieves and verbalizes knowledge. Using prompting, researchers have revealed new classes of emergent abilities such as arithmetic, instruction-following, grounded conceptual mappings and sentence acceptability judgments _____. Because the responses are influenced by prompt engineering and surface-level cues, this method primarily reflects performance rather than deep conceptual competence.
\paragraph{Direct Probability Measurement} Instead of relying on explicit responses, this method examines the model's ability to distinguish between correct and incorrect uses through minimal pairs. This approach is based on the idea that a successful language model should assign higher probabilities to linguistically acceptable sentences over unacceptable ones (Ritter, S., et al., 2020).
\paragraph{Neurolinguistic Probing} Neurolinguistic probing involves analyzing internal activation patterns to examine how a model processes linguistic information. This method is based on the idea that successful language models should demonstrate linguistically specific patterns of activity in response to minimal pairs (Linzen, T., et al., 2016).

\subsection{Multilingual Evaluation Benchmark}
Multilingual evaluation benchmarks have played a pivotal role in assessing the capabilities of language models across diverse languages. In the realm of multilingual probing, prior work has focused on evaluating linguistic properties and knowledge representation. For instance, Liu, P., et al. (2020) introduced MELA to assess multilingual linguistic acceptability, while Zhang, Y., et al. (2019) explored syntactic minimal pairs to evaluate cross-linguistic syntactic competence. On the knowledge probing front, benchmarks such as MLAMA ____ and BMLAMA ____ have been developed to investigate the factual knowledge encoded in multilingual models and their cross-lingual consistency.

Beyond probing, multilingual natural language understanding (NLU) benchmarks like XTREME ____ and XGLUE ____ have become standard for evaluating cross-lingual transferability. These benchmarks often rely on translating English datasets into other languages, as seen in tasks like XNLI ____ and PAWS-X _____. On the generation side, multilingual natural language generation (NLG) benchmarks have emerged, covering tasks such as summarization ____.

With the rise of multitask instruction finetuning, multilingual instruction datasets like Supernatural Instructions ____ and xP3 ____ have further expanded the scope of multilingual evaluation.

Despite these advancements, a critical gap remains in evaluating multilingual conceptual understanding, particularly in the format of conceptual minimal pairs. Our work addresses this gap by introducing XCOMPS, a multilingual benchmark specifically designed to evaluate conceptual understanding across languages.