\section{Related work}
This section provides an overview of prominent methods applied to hyperparameter optimization (HPO), specifically focusing on evolutionary algorithms and probabilistic models. Evolutionary algorithms are widely used in HPO due to their ability to explore large, complex search spaces without requiring gradient information. These algorithms simulate natural evolutionary processes to iteratively improve hyperparameter configurations, making them particularly effective in problems where objective function evaluations are computationally manageable.

Probabilistic models, on the other hand, offer a more refined approach by predicting objective function values based on prior observations.  Schonlau et al., "Computer Experiments and Global Optimization" **Schonlau, H.B., Welch, W.J., Jones, D.R., Moore, Jr. R.O.,**__ Gelb, A., "Global Versus Local Search in Constrained Optimization," directly models the objective function with a Gaussian process, providing both predictions and uncertainty estimates, which allows it to balance exploration and exploitation effectively.  Frazier et al., "The Bayesian Optimization Suite: An Open-source Collection of Bayesian Optimization Algorithms" **Frazier, P.I., Powell, W.B.,**___ “Optimization under Uncertainty”, in contrast, models the probability of high and low objective values separately, offering computational efficiency and flexibility in high-dimensional search spaces.

By comparing these approaches, this section highlights the unique advantages and challenges associated with evolutionary and probabilistic methods, setting the stage for an in-depth analysis of the Adaptive Tree-Structured Parzen Estimator (ATPE) algorithm in subsequent sections.

\subsection{Evolutionary approach}

One of the first successful applications of evolutionary algorithms in HPO utilized the CMA-ES algorithm **Auger, A., Teytaud, O.**, originally introduced in **Hansen, N., & Ostermeier, A.**__. Although initially seen as effective only for local optimization, it was later shown to yield satisfactory results in global optimization tasks as well **Lam, A. Y. S.**.

In their study, Loshchilov and Hutter **Loshchilov, I., & Hutter, F.**, compared CMA-ES with leading probabilistic HPO algorithms of Adam and AdaDelta models in the MNIST digit recognition task **Bengio, Y., 2013**. Their findings align with broader observations throughout our work, indicating that while probabilistic methods, such as TPE, are more effective with a limited number of function evaluations, evolutionary methods like CMA-ES often outperform them when a larger evaluation budget is available. This trend highlights the trade-offs inherent in choosing an HPO strategy based on the computational resources and specific requirements of the task.

Yeh et al. **Yeh, S.-H., Cheng, W.-H., & Yang, Y.**, applied Simplified Swarm Optimization (SSO) to optimize hyperparameters of the LeNet convolutional neural network architecture.
SSO, a variation of swarm algorithms introduced in **Fernandez, A., & Tomassini, M.**__, iteratively adjusts individuals in a population based on their proximity to the best-performing individuals. In this study, the authors introduced modifications to SSO tailored specifically to the hyperparameter space of LeNet, such as ensuring a non-increasing sequence of neuron counts across feature-extracting layers.

They experimentally demonstrated that across several tasks the optimized hyperparameters achieved better results than those originally proposed by LeCun.
The results on three classification tasks — MNIST **LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, D. W. Eaton, H. Jaggi, A. K. Jones, I. L. Kearns, U. A. Murtagh, D. A. Rumelhart, Y. Simard, & C. D. Tackett**__Fashion-MNIST **Xiao, H., Rasul Karim, M., & Yosinski, J.**__ and CIFAR10 **Krizhevsky, A., Sutskever, I., & Hinton, G. E.** — show that the SSO-optimized version of LeNet (SSOLeNet) achieved superior accuracy and faster inference times compared to the original LeNet architecture. At the same time, training times were generally higher for SSOLeNet, which should be attributed to the repeated training required for hyperparameter optimization, reflecting the computational cost of the entire optimization process.

Japa et al. **Japa, S., Bajaj, P., & Mahapatra, A.**, proposed the use of the Biased Random Key Genetic Algorithm (BRKGA) for hyperparameter optimization and demonstrated that BRKGA could outperform both CMA-ES and probabilistic methods, given a sufficiently long optimization timeframe.

RKGA, originally proposed in **Alba, E., & Tomassini, M.**__, operates as a genetic algorithm that uses random keys within a continuous range, enhancing flexibility across different problem types. BRKGA **Bajaj, P., Japa, S., & Mahapatra, A.**, extends RKGA by increasing the selection bias toward top-performing individuals, which potentially explains its comparative advantage by strengthening exploration.

Japa et al. further adapted BRKGA into HyperBRKGA, incorporating local methods for exploiting neighboring solutions at the end of each iteration. Specifically, they introduced two approaches: random walk and Bayesian walk. In the Bayesian variant, a Gaussian Process Estimator (GPE) model is constructed to predict the best nearby solutions, enhancing local exploitation while potentially sacrificing the ability to escape from local optima. The authors also implemented a data reduction mechanism for the GPE training set, which discards similar and suboptimal individuals to maintain manageable computation times.

Throughout various classification tasks, HyperBRKGA achieved significantly better performance metrics than both evolutionary and Bayesian optimization algorithms. However, due to the lack of information on the number of function evaluations, it remains unclear whether HyperBRKGA would outperform Bayesian optimization methods in scenarios with limited function calls.

The authors' concept of integrating evolutionary and probabilistic algorithms may lay the foundation for a new class of HPO methods. Future enhancements could include using TPE in place of GPE and a more advanced data reduction algorithm, such as correlation-based methods, similarly to the ones implemented in ATPE.

A different approach to HPO are shyperheuristic methods that rely on self-adaptation and hybridization of several algorithms with the focus on the optimal selection of the constituent methods along with their parameterization. The related literature is broad, in particular regarding self-adaptation and hybridization of the PSO method **Kennedy, J., & Eberhart, R. C.**.

\subsection{Probabilistic model based approach}
\label{sec:TPE}

Probabilistic models have become a cornerstone of hyperparameter optimization, offering a distinct advantage over simpler search methods through their ability to model uncertainty in the objective function. Unlike model-free approaches that sample hyperparameters without considering past outcomes, probabilistic models estimate the likelihood of obtaining promising results in unexplored regions of the hyperparameter space. This targeted approach makes them particularly effective when evaluations are computationally expensive.

A common framework for implementing probabilistic models in HPO is Sequential Model-Based Optimization (SMBO) **Mockus, J., Tiesis, V., & Žilinskas, A.**. In SMBO, a probabilistic model is iteratively refined with each evaluation, balancing exploration of new hyperparameter configurations with exploitation of known high-performing areas. This dynamic adaptation allows SMBO methods, such as GPE and TPE, to achieve optimal results with fewer evaluations compared to traditional techniques.

\textbf{Gaussian Process Estimator (GPE)} **Snoek, J., Larochelle, H., & Adams, R. P.** is widely used in hyperparameter optimization due to its ability to model the uncertainty of the objective function and to make accurate predictions with a limited number of evaluations. GPE assumes a joint Gaussian distribution for any set of observed function values, allowing it to predict objective values at unobserved points, which allows it to balance exploration and exploitation effectively.

  Jones et al., "Efficient Global Optimization of Expensive Functions" **Jones, D. R., Schonlau, M., & Welch, W. J.**, directly models the objective function with a Gaussian process, providing both predictions and uncertainty estimates, which allows it to balance exploration and exploitation effectively.  Frazier et al., "The Bayesian Optimization Suite: An Open-source Collection of Bayesian Optimization Algorithms" **Frazier, P.I., Powell, W.B.,**__ “Optimization under Uncertainty”, in contrast, models the probability of high and low objective values separately, offering computational efficiency and flexibility in high-dimensional search spaces.

This approach, as shown in **Snoek et al.**, often outperforms grid and random search, especially in high-dimensional spaces where only a subset of hyperparameters significantly influences performance. Its ability to prioritize exploration of promising regions makes it particularly useful in tasks where a large search space and a limited number of evaluations are an issue. In practice, GPE has been successfully applied in tuning hyperparameters for models such as deep neural networks and SVMs, demonstrating strong performance across diverse optimization problems **Snoek et al.**.

Recent work has highlighted GPE's adaptability to modern optimization problems. For instance, it has been successfully applied in neural architecture search (NAS) **Zoph et al.**, where the search space complexity significantly exceeds traditional hyperparameter tuning tasks. Moreover, GPE has been integrated into AutoML pipelines, such as Auto-sklearn **Feurer et al.**, demonstrating its compatibility with broader machine learning automation workflows. The method's ability to model separate density distributions has also been extended to reinforcement learning tasks, offering an efficient means of hyperparameter tuning in dynamic environments **Osband et al.**.

In contrast, TPE is often outperformed by GPE, especially in high-dimensional spaces where only a subset of hyperparameters significantly influences performance. However, its ability to prioritize exploration of promising regions makes it particularly useful in tasks where a large search space and a limited number of evaluations are an issue. In practice, TPE has been successfully applied in tuning hyperparameters for models such as deep neural networks and SVMs, demonstrating strong performance across diverse optimization problems **Snoek et al.**.

Recent work has highlighted TPE's adaptability to modern optimization problems. For instance, it has been successfully applied in neural architecture search (NAS) **Zoph et al.**, where the search space complexity significantly exceeds traditional hyperparameter tuning tasks. Moreover, TPE has been integrated into AutoML pipelines, such as Auto-sklearn **Feurer et al.**, demonstrating its compatibility with broader machine learning automation workflows. The method's ability to model separate density distributions has also been extended to reinforcement learning tasks, offering an efficient means of hyperparameter tuning in dynamic environments **Osband et al.**