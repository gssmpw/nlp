\documentclass[11pt, letterpaper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib}

\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{appendix}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnotetext{#1}%
  \endgroup
}


\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\Proj}{\mathop{\Pi}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\RegVar}{\mathrm{RegVar}}
\newcommand{\Reg}{\mathrm{Reg}}
\newcommand{\RegSE}{\mathrm{RegSE}}
\newcommand{\RegMG}{\mathrm{RegMG}}
\newcommand{\RegVarMG}{\mathrm{RegVarMG}}
\newcommand{\RegOCO}{\mathrm{RegOCO}}
\newcommand{\Var}{\mathop{\mathrm{Var}}}
\newcommand{\Cov}{\mathop{\mathrm{Cov}}}
\newcommand{\Varhat}{\widehat{\mathrm{VB}}}
\newcommand{\Varalg}{\mathrm{V}_T}
\newcommand{\Aone}{\widehat{A_T(1)}}
\newcommand{\Azero}{\widehat{A_T(0)}}
\newcommand{\hinv}{h_\mathrm{inv}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\ClipOGDSC}{ClipOGD$^\mathrm{SC}$\,}

\usepackage[english]{babel}

\usepackage{bbm}
\usepackage{tcolorbox}
\usepackage[normalem]{ulem}

\allowdisplaybreaks

\title{Stronger Neyman Regret Guarantees \\for Adaptive Experimental Design}
\author{Georgy Noarov$^\dag$, Riccardo Fogliato$^\ddagger$, Martin Bertran$^\ddagger$, Aaron Roth$^{\dag\ddagger}$}

\begin{document}

\maketitle

\begin{abstract}
We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering \emph{sublinear Neyman regret}, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design.
Recent work \citep{dai2023clip} introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. 
In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. 
Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual ``multigroup'' Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments. 
\end{abstract}

\section{Introduction}


Randomized control trials (RCTs) play a central role in a variety of settings where causal effects need to be accurately measured, spanning healthcare and epidemiology, policymaking, the social sciences, econometrics, e-commerce, and beyond. In the classic potential outcomes framework \citep{neyman1923applications,rubin1974estimating}, a central estimand is the average treatment effect (ATE) -- the average individual causal effect across experimental units. To obtain precise estimates of the ATE, we generally seek estimators that are unbiased and have low variance.

\blfootnote{Georgy Noarov conducted part of this work as an intern at Amazon Web Services.}\blfootnote{$^\dagger$Department of Computer Science, University of Pennsylvania. $^\ddagger$Amazon Web Services.}

In many cases, RCTs are run sequentially: Experimental units arrive one by one, and each unit is assigned to treatment or control adaptively, based on previous outcomes or auxiliary information. The data-driven nature and flexibility of these experiments suggest that such adaptive trials can achieve substantial efficiency gains over standard fixed designs, as shown in domains ranging from political science \citep{offer2021adaptive,blackwell2022batch} to medicine \citep{chow2008adaptive,villar2015multi,fda2019adaptive}.
However, so far adaptive experiments have received limited attention \citep{hu2006theory} and have been rarely used in practice due to concerns that adaptivity could invalidate standard statistical guarantees \citep{van2008construction}. Indeed, classic solutions for improving estimator efficiency in the batch setting, such as Neyman allocation \citep{neyman1992two}, can be nontrivial to extend to the sequential setting.

Recently, a growing body of work \citep{hahn2011adaptive, kato2020efficient, li2023double, dai2023clip, cook2023semiparametric} has made progress on this front by introducing multi-stage adaptive designs that estimate the ATE via inverse-probability weighting (IPW)-type estimators with adaptively adjusted propensity scores. 
\footnote{
In parallel, studies that fall into the multi-armed bandits literature have developed adaptive designs for finding \emph{reward-maximizing} treatments (arms) or policies, which is a distinct, and conflicting, objective than estimation efficiency \citep{zhang2020inference,zhang2021statistical,hadad2021confidence,xu2016subgroup,xu2024fallacy}.
}
%
Our work contributes to this literature by developing novel adaptive sequential designs for IPW-based ATE estimation with efficiency guarantees. 
Crucially, our methods --unlike most existing work-- are developed within the finite-population setting \citep{wager2024causal}, where the ATE is defined as a deterministic function of the observed population rather than a superpopulation parameter. This distinction ensures robustness to treatment effect heterogeneity and temporal data drift, challenges that can undermine conventional superpopulation-based designs. 

\paragraph{Our contributions}

We focus on the design of adaptive RCTs to estimate the ATE as efficiently as the best-in-hindsight IPW design from some benchmark class, up to error terms. Specifically, we aim to minimize the \emph{Neyman regret} \citep{kato2020efficient,dai2023clip} -- a measure comparing the variance of our adaptive estimator to that of the variance-minimizing nonadaptive Bernoulli trial where units are treated with some fixed probability.
 Currently, to our knowledge \citet{dai2023clip}'s ClipOGD method is the only adaptive design achieving sublinear Neyman regret in the finite-population setting. This method guarantees $\widetilde{O}(\sqrt{T})$ expected regret for any $T$-unit trial under moment-bounded potential outcomes.
%
However, two important questions arise: 
\begin{enumerate}[I.]
    \item Can we develop designs with better regret rates? \citet{dai2023clip} conjectured that $\widetilde{O}(\sqrt{T})$ is the minimax Neyman rate.
    \item Can we develop context-aware designs that use pre-treatment covariates to improve efficiency?
\end{enumerate}
In this work, we answer both of these questions affirmatively as follows.

\paragraph{Contribution I: Exponentially improved noncontextual Neyman regret bound.} We show that, under a natural strengthening of \citet{dai2023clip}'s assumptions on the outcomes,
we can modify ClipOGD to attain an anytime-valid Neyman regret bound of $\widetilde{O}(\log T)$.\footnote{In fact, a lower bounding construction in the very recent work of~\citet{li2024optimal} shows that the best possible Neyman regret is $\Omega(1)$ even in the more relaxed superpopulation setting --- and so our method achieves a \emph{best-of-both-worlds} guarantee, up to logarithmic factors.} To achieve this speedup, we leverage the strong convexity of the Neyman objective under our stricter lower-bounding assumption on the outcomes, which as we show leads to near-logarithmic regret via techniques introduced by~\citep{hazan2007logarithmic}. Moreover, it can be shown that even under the weaker outcome lower bound assumption of \citet{dai2023clip}, our adaptive design can be tweaked to have the asymptotic efficiency of $ \smash{\left(1 + \epsilon \right) V^* + \widetilde{O} \left(\frac{\log T}{T} \right) }$ for any $\epsilon > 0$, where $V^*$ denotes the optimal nonadaptive design variance; the interpretation is that any $(1+\epsilon)$-multiplicative approximation to the optimal variance can be attained at this fast rate. We validate the greater efficiency of our proposed design against that of ClipOGD through a suite of experiments on synthetic and real-world data.


\paragraph{Contribution II: Adaptive designs with contextual Neyman regret guarantees.} 
We next develop a novel adaptive design MGATE (Multi-Group ATE) that leverages pre-treatment covariates to improve efficiency relative to the non-contextual setting. 
In a nutshell, given an arbitrary predefined finite collection $\cG \subseteq 2^\cX$ of contextual groups defined by the covariates (e.g., demographics), we propose a no \mbox{$\cG$-multigroup-Neyman-regret} adaptive design that obtains sublinear regret simultaneously on all subsequences of experimental units corresponding to the groups in $\cG$. Critically, we also allow for overlapping groups, i.e., units can simultaneously belong to multiple groups. 
A key challenge here is to balance the treatment probabilities in a way that balances the efficiency of the ATEs estimates across groups. Our proposed design leverages a variation of the ``sleeping experts'' approach \citep{blum2020advancing,acharyaoracle} used in the online learning literature \citep{lee2022online,deng2024group}, that deals with the limited feedback and the fact that the observed objective values do not live in an a-priori bounded range. 
The method achieves $\widetilde{O}(\sqrt{T})$ multigroup Neyman regret. 
We also empirically validate its performance. 

Our multigroup guarantees can be interpreted through the lens of group ATE (GATE) estimation \citep{chernozhukov2017fisher,semenova2021debiased,zimmert2019nonparametric}. GATE occupies a middle ground between ATE, which measures the average effect over the entire sequence, and CATE (conditional ATE), which measures the ATE conditionally on each covariate vector. 
 Existing  works on GATE, however, are mainly focused on learning data-driven disjoint groups to improve overall ATE estimation. In contrast, our objective is to simultaneously ensure efficient GATE inference for any family of arbitrarily overlapping groups. This is related in motivation (though distinct in technique) to the recent work of \cite{kern2024multi} who use ``multiaccuracy'' to make CATE inference robust to certain kinds of distribution shift.

We expect that such multigroup efficiency guarantees can be broadly useful, and hope future work will study multigroup adaptive designs beyond the sequential finite-population setting that we focus on in this paper.

\paragraph{Organization}

In \cref{sec:prelims}, we introduce our general setting and objectives. In \cref{sec:noncontextual}, we focus on the (vanilla) non-contextual setting, and present and analyze our adaptive design \ClipOGDSC, which achieves near-logarithmic Neyman regret. We prove the main regret bound in \Cref{thm:regret} and then demonstrate further guarantees on the adaptive design.

In \cref{sec:multigroup}, we introduce the notion of multigroup Neyman regret, and present our multigroup adaptive design MGATE (\cref{alg:AMGATE}), which achieves $\widetilde{O}(\sqrt{T})$ multigroup Neyman regret as shown in \Cref{thm:multigroup}. Furthermore, in \cref{app:multigroup} we provide a general multigroup design (\cref{alg:multigroup_general}) that significantly generalizes MGATE. 
In \cref{sec:experiments}, we compare the empirical performance of our adaptive designs to the \citet{dai2023clip} ClipOGD design on an array of real-world and synthetic sequential experimental design tasks.

\section{Preliminaries} \label{sec:prelims}

\paragraph{Setting} We work in the design-based, sequential variant of the potential outcomes setting \citep{neyman1923applications, rubin1974estimating, imbens2015causal}. A finite number of experimental units in the population arrive one by one at rounds $t\in \mathbb{N}_+$. 
Each unit has two associated fixed potential outcomes, only one of which can be observed: treatment outcome $y_t(1) \in \R$ and control outcome $y_t(0) \in \R$. 

In the basic setting, the observed outcome is the only information the experimenter receives about the units. A richer setting is one where before choosing treatment or control for unit $t$, the Experimenter is given access to \emph{pre-treatment covariate} $x_t \in \cX$, where $\cX$ is a feature space of arbitrary nature (e.g.\ $\cX$ may be a finite-dimensional vector space).
In this paper, we will study both settings: the noncontextual setting in \cref{sec:noncontextual} and the contextual one in \cref{sec:multigroup}.

\paragraph{Adaptive design} In a randomized controlled trial (RCT), the experimenter (randomly) decides whether to apply treatment or control to each unit, and observes the corresponding outcome but not the counterfactual. These randomized decisions for all units constitute the experimental design. We study adaptive experimental designs, described as follows.

\begin{tcolorbox}
\begin{center}
\uline{\textbf{ $T$-round Adaptive Design Protocol}}
\end{center}

Potential outcomes $\{(y_t(1), y_t(0))\}_{t \in [T]}$ are generated upfront (but not shown to Experimenter).

Then, sequentially for each unit $t = 1 \ldots T$: 
\begin{enumerate}
    \item (\emph{Contextual} setting only) Experimenter observes pre-treatment covariate $x_t \in \cX$.
    \item Experimenter sets treatment probability $p_t$.
    \item Experimenter flips bias-$p_t$ coin to obtain realized treatment decision: $Z_t \sim \mathrm{Bernoulli}(p_t)$.
    \item Experimenter observes outcome $Y_t = y_t(Z_t)$.
\end{enumerate}
\end{tcolorbox}

By contrast, the standard nonadaptive (Bernoulli) trial fixes upfront the same treatment probability $p_t = p$ for all units $t$, and uses it throughout the experiment without any adjustments. 

Our estimand of interest is the average treatment effect (\emph{ATE}), which corresponds to the difference between the average outcomes of treatment and control units in the population. We provide the formal definition below. 
%
\begin{definition}[ATE]
The \emph{average treatment effect} for potential outcomes $\smash{\{(y_t(1), y_t(0))\}_{t = 1}^{T}}$ is: 
\[
    \tau_T = \frac{1}{T} \sum_{t=1}^T y_t(1) - y_t(0).
\] 
\end{definition}
%
A classical estimator of the ATE is the adaptive IPW estimator \citep{horvitz1952generalization}, which employs inverse probability weighting. We define it next. 
 
\begin{definition}[Adaptive IPW Estimator]
The \emph{adaptive IPW estimator} of the ATE $\tau_T$ is:
\[
\hat{\tau}_T = \frac{1}{T} \sum_t Y_t \left( \frac{Z_t}{p_t} - \frac{1-Z_t}{1-p_t} \right). 
\]
\end{definition}
This estimator is unbiased, meaning that 
for any outcomes $\{(y_t(0), y_t(1)\}_{t = 1}^T$ and any adaptive design $(p_t)_{t=1}^T$ with all $p_t \in (0, 1)$, we have 
$\E[\hat{\tau}_T] = \tau_T$. 
%
Thus, no matter what adaptive design Experimenter employs, the induced adaptive IPW estimator will always be unbiased. However, the estimator's variance will vary based on the design, making some designs more efficient than others.

\paragraph{Objective: minimize variance of ATE estimator}
Our main goal will be to construct adaptive designs that asymptotically approach the variance of the best-in-hindsight experimental design in some benchmark class. 
A basic class of designs is that of nonadaptive designs, parameterized by the choice of fixed propensity $p \in (0, 1)$.
Formally, we measure the \emph{Neyman regret} \citep{kato2020efficient, dai2023clip} of any proposed adaptive design as the (time-rescaled) difference between its IPW estimator variance and the variance of same estimator under the most efficient nonadaptive design.

To define Neyman regret, note (see Proposition~2.2 of \citet{dai2023clip}) that $\Var[\hat{\tau}_T] = \sum_{t=1}^T\E\left[ f_t(p_t) \right]/T^2 - k_\mathrm{ATE}$, where $f_t(p) := y_t(1)^2/p + y_t(0)^2/(1-p)$ is the variance of the propensity-$p$ IPW estimator at unit $t$, and $k_\mathrm{ATE} = \sum_{t=1}^T (y_t(1)-y_t(0))^2/T^2$ is a design-independent term. We are now ready to provide the formal definition.

\begin{definition}[Neyman Regret \citep{kato2020efficient, dai2023clip}] \label{def:regret}
    The Neyman regret of adaptive design $(p_t)_{t=1}^T$ on a potential outcomes sequence $\{(y_t(1), y_t(0))\}_{t=1}^T$ is:\footnote{``Var'' stands for variance, as Neyman regret captures the rescaled estimator variance associated with the design.}
    \begin{align*}\label{eq:neyman_regret}
    \RegVar_T = \max_{p_T^* \in (0, 1)} \sum_{t=1}^T f_t(p_t) - f_t(p_T^*).
    \end{align*}
\end{definition}
%
Thus the variance of the IPW estimator for a design $(p_t)_{t=1}^T$ differs from that of the best nonadaptive design by exactly $\RegVar_T/T^2$, justifying the Neyman regret definition.

Our goal will be to develop adaptive designs with sublinear expected Neyman regret: $\E \left[\RegVar_T \right] = o(T)$, or equivalently with vanishing average expected Neyman regret: $\E\left[\RegVar_T/T  \right]= o(1)$. We call any design that satisfies this a no-regret design.


\section{Efficient Non-Contextual ATE Estimation} \label{sec:noncontextual}
We now present our first contribution: An adaptive design that achieves $\widetilde{O}(\log T)$ Neyman regret under natural assumptions on the outcomes. We begin by discussing the $\widetilde{O}(\sqrt{T})$-Neyman regret design ClipOGD of \citet{dai2023clip}, and then modifying it to better exploit the strongly convex structure of the Neyman objective. Next, we discuss further guarantees on our method's performance.

\subsection{Adaptive Design with Logarithmic Neyman Regret}

\paragraph{Meta-Design: ClipOGD} The first finite-population design that achieves sublinear Neyman regret, ClipOGD, was introduced by \citet{dai2023clip}. Leveraging the fact that the per-round Neyman objectives $f_t(p)$ are convex in $p$, it performs a modified version of online gradient descent (OGD) on $f_t$ to adaptively modify the treatment probabilities $p_t$. 

The complicating factor is that the gradients of $f_t$ diverge when $p$ is close to 0 or 1: standard OGD analyses typically require explicit or implicit bounds on the gradients of the objective \citep{hazan2016introduction}, so vanilla projected OGD on the entire interval $[0, 1]$ will not work without modification. ClipOGD solves this problem by clipping the OGD iterates $\{p_t\}_{t\in \mathbb{N}_+}$ to be within a nested family $\{[\delta_t, 1-\delta_t]\}_{t\in \mathbb{N}_+}$ of subintervals of $(0, 1)$, which gradually expand to cover the whole interval in the infinite time limit (i.e.,\ $\lim_{t\to\infty} \delta_t=0$). The expansion is needed to handle cases when $p^*_T$ is close to the boundary. 
In view of this, we let $\delta_t = 1 / h(t)$ for all $t \in \mathbb{N}_+$, where $h: \mathbb{N}_+ \to \R_{>0}$ is some strictly increasing function with $\lim_{t\to \infty} h(t) = \infty$. We call $\delta_t$
the \emph{clipping rate}, $h$ the clipping function, and refer to any adaptive design $(p_t)_{t \in \mathbb{N}_+}$ that satisfies $1/h(t) \leq p_t \leq 1-1/h(t)$ for all $t$ as $h$-clipped. \Cref{alg:strong} gives the pseudocode for ClipOGD. Here, $\Proj_S(x)$ denotes the projection of $x$ onto interval $S \subset (0, 1)$.

\begin{algorithm}[ht]
\begin{algorithmic}
\STATE Initialize $p_0 \gets 0.5$ and $g_0 \gets 0$
\FOR{units $t=1, 2, \ldots$}
    \STATE Set step size $\eta_t > 0$ and clipping rate $\delta_t \in (0, 0.5)$
    \STATE Set  treatment probability $p_t \gets \!\!\!\! \Proj\limits_{[\delta_t, 1-\delta_t]}(p_{t-1} - \eta_t \cdot g_{t-1})$
    \STATE Set treatment decision $Z_t \sim \mathrm{Bernoulli}(p_t)$
    \STATE Observe outcome $Y_t \gets y_t(Z_t)$
    \STATE Set gradient estimate: $g_t \gets Y_t^2 \left( -\frac{Z_t}{p_t^3} + \frac{1-Z_t}{(1-p_t)^3} \right)$
\ENDFOR
\end{algorithmic}
\caption{ClipOGD\; \citep{dai2023clip}}
\label{alg:strong}
\end{algorithm}

\paragraph{ClipOGD$^\mathrm{0}$: A $\widetilde{O}(\sqrt{T})$ regret design} In their paper, \citet{dai2023clip} analyzed and provided guarantees for a specific
instantiation of ClipOGD, where $\eta_t = \sqrt{1/T}$ and $\delta_t = 0.5 \cdot t^{-1/\alpha}$ where $\alpha = \sqrt{5 \log T}$ for all $t=1, \dots, T$. For clarity, we call this design ClipOGD$^0$.
Their main result proves that ClipOGD$^0$ has $\widetilde{O}(\sqrt{T})$ Neyman regret under a moment assumption on the outcomes: $\smash{0 < c \leq (\frac{1}{T} \sum_{t=1}^{T} y_i(t)^2)^{1/2}}$ and 
$\smash{(\frac{1}{T} \sum_{t=1}^{T} y_i(t)^4)^{1/4} \leq C}$ for $i \in \{0, 1\}$ and some $c \leq C$. However, the learning rate of ClipOGD$^0$ has several drawbacks. First, it is too conservative, precluding improvement in Neyman regret beyond $\widetilde{O}(\sqrt{T})$. Second, it is horizon-dependent, making it necessary to know (or commit to) $T$ upfront. Finally, it is constant rather than decreasing, so the design probabilities will jump around (rather than gradually converge) during any given run of ClipOGD$^0$.

\paragraph{\ClipOGDSC: Our $\widetilde{O}(\log T)$ regret design} We now present an adaptive design called \ClipOGDSC that addresses these issues: It uses the learning rate $\eta_t \sim 1/t$ that, under \Cref{ass:bounds}, (1) achieves an exponentially improved Neyman regret bound,  (2) is \emph{anytime}, i.e., does not require advance knowledge of the time horizon $T$, and (3) its propensities converge in $L_2$ to the hindsight-best propensity. 
Our Neyman regret bound relies on a stricter assumption than the one made by \citet{dai2023clip}'s, which we detail below. 
%
\begin{assumption}[Bounds on Potential Outcomes] \label{ass:bounds}
There exist positive constants $c, C$ such that outcomes $\{(y_t(0), y_t(1))\}_{t \geq 1}$ satisfy for all time horizons $T$: 
\begin{equation*}
    \max_{t \geq 1} \{|y_t(0)|, |y_t(1)|\} \leq C, \quad
    c \leq \min \left\{\min_{t \geq 1} \, \left( y_t(0)^2 + y_t(1)^2 \right)^{1/2}, \min_{i \in \{0, 1\}} \left( \frac{1}{T} \sum_{t=1}^T y(i)^2\right)^{1/2} \right\}.
\end{equation*}
\end{assumption}
Next,
let $\hinv$ be the inverse function of $h$, defined via the identity $\hinv \circ h = h \circ \hinv = \mathrm{Id}$. Our main result is the following Neyman regret bound in terms of $T$, $h$, and $\hinv$. 

\begin{theorem}[Stronger Neyman Regret Bound] \label{thm:regret}
Suppose \Cref{ass:bounds} is satisfied with $C$, $c$ the corresponding constants. Let $h: \mathbb{N}_+ \to \R_{> 0}$ be strictly increasing. 
Let \ClipOGDSC be the adaptive design
that instantiates \Cref{alg:strong} with learning rate $\eta_t = 1/(2c^2t)$ and clipping rate $\delta_t = 1/h(t)$. 
Then, \ClipOGDSC attains the following anytime-valid 
Neyman regret bound:
{
\begin{align} \label{eq:main_bound}
\E[\RegVar_T] = O \! \left( \left(h(T)\right)^5 \! \cdot \! \log(T)  \! + \!  \left( \hinv \left(1 \!+\! C/c \right) \right)^2 \right).
\end{align}
}
Since $h$ can be chosen to grow arbitrarily slowly, we can get:
$
\E[\RegVar_T] = \widetilde{O}(\log T).
$
\end{theorem}

The proof is contained in \Cref{app:proof-noncontextual}. It exploits the strong convexity of the Neyman objectives $f_t$ enabled by \cref{ass:bounds} (hence the `SC' in \ClipOGDSC), by applying the techniques for analyzing strongly convex gradient descent~\citep{hazan2007logarithmic, rakhlin2011making}.

Compared to the analysis in \citet{dai2023clip}, we make explicit the dependence of the regret of ClipOGD on the clipping rate. Note that the choice of $h$ is flexible in the sense that any $h(t) = o(t^{0.2-\varepsilon})$ for any $\varepsilon > 0$ will result in a regret bound that is sublinear in $T$. 
From a practical standpoint, however, picking $h$ may be a nontrivial affair, as a slower-growing $h$ will have a faster-growing inverse mapping $\hinv$. While the $\hinv$-dependent term in the regret bound is constant in $T$, it can still be large in the constants of the problem.
Intuitively, if $C/c$ is large, the optimal propensity $p^*_T$ may be near the boundary and convergence may be slow. We hope future work will further explore the `well-conditioning' properties of Neyman regret.

\subsection{Convergence of Adaptive Treatment Probabilities}

We now investigate the trajectory of treatment probabilities $(p_t)_{t\geq 1}$ produced by ClipOGD$^\textrm{SC}$. Ideally, these propensities would converge to the optimal probabilities $(p^*_T)_{T\geq 1}$ as $T$ grows large. By tweaking the arguments used in establishing our Neyman regret bounds of \Cref{thm:regret}, we can obtain convergence in squared means (and hence in probability). The next claims formalize this result. 
In particular, we first establish a quantitative bound on the $L_2$ convergence of our propensities to the benchmark ones. (See \Cref{app:proof-noncontextual} for the derivation.)
\begin{lemma}[$L_2$-Deviation from Benchmark Design] \label{lemma:l2deviation}
The deviation of the design probabilities of \ClipOGDSC from the best nonadaptive design probabilities is $L_2$-bounded for all $T$ as:
{\small
\begin{align*}
    \E\left[\left(p_{T} - p^*_T \right)^2\right] \leq -\Theta\left(\frac{\E[\RegVar_T]}{T}\right) + O\left(\frac{ \left(h(T) \right)^2 \log T}{T}\right).
\end{align*}
}
\end{lemma}
This implies the following $L_2$-convergence result, subject to an assumption on the Neyman regret of \ClipOGDSC which asks for it to not consistently outperform the optimal nonadaptive design. 
%
\begin{corollary}[$L_2$-Convergence to Benchmark Design]\label{thm:convergenceinl2}
Assume \ClipOGDSC has asymptotically nonnegative Neyman regret: $\liminf_{T \to \infty} \frac{\E[\RegVar_T]}{T} \geq 0$. Then, its propensities $(p_t)_{t\geq 1}$ will converge to the benchmark nonadaptive propensities $(p^*_T)_{T\geq 1}$ in squared means: $\E \left[(p_{T} - p^*_T)^2 \right] \to 0$ as $T\rightarrow\infty$.
\end{corollary}
%
In the special case of sequences of potential outcomes that are (i.i.d.) samples from a superpopulation, 
the regret nonnegativity holds automatically, implying that our adaptive design will necessarily converge to the best nonadaptive design without further assumptions.

\begin{corollary}[Convergence in the Superpopulation Setting]
Suppose that the outcomes are drawn i.i.d.\ from a superpopulation: $(y_t(0), y_t(1)) \sim \cD$ for all $t \geq 1$ and any fixed distribution $\cD$. Then, \ClipOGDSC guarantees that
$\E \left[(p_{T} - p^*)^2 \right] \to 0$ at the rate $\widetilde{O}(\log T/T)$, and thus in particular that $p_T \to p^*$ in probability.
\end{corollary}
\begin{proof}
    In the superpopulation setting, \emph{any} adaptive design will have nonnegative Neyman regret: $f_t(p) = f(p) = \E[y(1)^2]/p + \E[y(0)^2]/(1-p)$ has the same optimum $p^* = \left(1 + \E[(y_t(0))^2] / \E[(y_t(1))^2]\right)^{-1}$ for all units $t$, 
    so $\E[\RegVar_T] = \E \left[\sum_{t=1}^T \left(f(p_t) - f(p^*) \right) \right] \geq 0$.
\end{proof}


\subsection{Valid CIs for the Adaptive IPW Estimator} 

We now turn to the issue of endowing the IPW estimator $\hat{\tau}_T$ induced by our adaptive design with asymptotically valid confidence intervals (CIs). In general, the existence and construction of valid CIs for $\hat{\tau}_T$ delicately depends on the choice of the design. 
However, we will now see that a construction of \citet{dai2023clip} lends conservative CIs to all $h$-clipped adaptive designs with vanishing regret.

To formalize this result, we make a standard assumption: that the outcome sequences are not perfectly anti-correlated. To state it, define ``empirical second raw moments'' of the two outcome populations as:
$S_T(i)^2 := \frac{1}{T} \sum_{t=1}^T (y_t(i))^2 \text{ for } i \in \{0, 1\}.$

\begin{assumption}[Correlation of Outcome Populations \citep{dai2023clip}] \label{ass:correlation}
For a constant $c_\rho > 0$ and all $T \geq 1$, the running 
correlation 
$\rho_T$ of the sequences $\{(y_t(0), y_t(1))\}_{t \geq 1}$ satisfies:
\[\rho_T \geq -1 + c_\rho, \text{ where } \rho_T := \frac{\frac{1}{T} \sum_{t=1}^T y_t(1) y_t(0)}{S_T(1) S_T(0)}.\]
\end{assumption}

\begin{theorem}[CIs for Clipped Adaptive Designs] \label{thm:confidence}
    Suppose the potential outcomes satisfy \Cref{ass:bounds} and \Cref{ass:correlation}.
    Consider any $h$-clipped adaptive design $(p_t)_{t\geq 1}$  with vanishing Neyman regret: $\lim_{T \to \infty}\RegVar_T = 0$. Let $\mathrm{VB} = \frac{4}{T} S_T(1) S_T(0)$ be a conservative upper bound on the hindsight-best nonadaptive variance. 
    Then, letting $(Z_t)_{t\geq 1}$ be the treatment decisions, the estimator of \citet{dai2023clip} given by: 
    \begin{equation*}
        \Varhat = \frac{4}{T} \sqrt{\left( \frac{1}{T} \sum_{t=1}^T (y_t(1))^2 \frac{Z_t}{p_t} \right) \left( \frac{1}{T} \sum_{t=1}^T (y_t(0))^2 \frac{1-Z_t}{1 - p_t} \right)}
    \end{equation*}
    converges to $\mathrm{VB}$ in probability at rate $O_p \left(\sqrt{h(T)/T} \right)$. 

    Consequently, $\Varhat$ can be used to construct asymptotically valid Chebyshev-type confidence intervals for the adaptive IPW estimator $\hat{\tau}_T$ under any adaptive design satisfying the above conditions. Specifically, for any confidence level $\alpha \in (0, 1]$: 
    \[
    \liminf_{T \to \infty} \Pr\left[\tau_T \in \left[\hat{\tau}_T \pm \alpha^{-1/2} \sqrt{\Varhat} \right]\right] \geq 1 - \alpha.
    \]
\end{theorem}

The proof for \Cref{thm:confidence} is outlined in \Cref{app:confidence}.




\section{Efficient Multigroup ATE Estimation} \label{sec:multigroup}

\paragraph{The contextual setting} 

\cref{sec:noncontextual} covers non-contextual adaptive designs that only observe outcomes. A contextual adaptive design, however, also observes pre-treatment covariates $x_t \in \cX$ at the start of each round, which can help predict potential outcomes $(y_t(0), y_t(1))$. We can leverage this extra information to improve treatment assignments and outcome estimation.

\paragraph{A multigroup formulation} 
We frame the contextual setting in a multigroup way. Before the experiment, we have a finite set of context-defined groups $\cG = \{G_1, G_2, \ldots\}$, each $G \subseteq \cX$, where $\mathcal{X}$ is the feature space. Any covariate vector $x_t$ can belong to none, one, or more groups. The group definition is dependent on the specifics of the task, e.g., in a medical application the features $x_t$ could represent a patient's health history. 

Our objective in a multigroup setting, informally, is to design an adaptive scheme that offers ATE estimation efficiency guarantees (such as Neyman regret guarantees) not only on average over the entire sequence of units but also on each subsequence that results from conditioning on units belonging to a group $G$, simultaneously for all groups $G \in \cG$. 

\subsection{A New Metric: Multigroup Neyman Regret}

We introduce multigroup Neyman regret as a strengthening of (vanilla) Neyman regret. Specifically, given any contextual group collection $\cG$, $\cG$-multigroup Neyman regret will be the maximum Neyman regret that an adaptive design achieves over any group $G$ in the collection. We formalize it next.

\begin{definition}[$\mathcal{G}$-Multigroup Neyman Regret] Given any group collection $\cG \subseteq 2^\cX$, the group-conditional Neyman regret of an adaptive design $\cA$ on any group $G \in \cG$ is defined as: 
\begin{equation*}
\RegVar_T(\cA; G) := \E\left[\max_{p^* \in (0, 1)} \sum_{t=1}^T \mathbbm{1}[x_t \in G] \left(f_t(p_t) - f_t(p^*) \right)\right].
\end{equation*}
The $\cG$-multigroup Neyman regret of $\cA$ is then defined as its maximum group-conditional Neyman regret over all groups $G \in \cG$: 

\[
    \RegVarMG_T(\cA; \cG) := \max_{G \in \cG} \RegVar_T(\cA;G).
\]
\end{definition}


\subsection{Achieving $\widetilde{O}(\sqrt{T})$ Multigroup Neyman Regret}

We now present in \Cref{alg:AMGATE} an adaptive design which we call MGATE (for Multi-Group ATE) and achieves the $\widetilde{O}(\sqrt{T})$ multigroup Neyman regret bound. 

\textbf{Additional Notation:} We use $\odot$ to denote elementwise vector multiplication, and let $\textbf{1}^d, \textbf{0}^d$ be $d$-dimensional all-ones and all-zeros vectors. Also note that the update of $w'_{t+1}$ takes an \emph{elementwise} maximum of the vectors, and assumes that $0/0=0$ to account for the corner case $q_t = 0$.
%
\begin{algorithm}[ht]
\caption{$\cA_{MGATE}$: Multigroup Adaptive Design}
\label{alg:AMGATE}
\begin{algorithmic}
\STATE Receive clipping function $h: \mathbb{N}_+ \to \R_{>0}$
\STATE Receive number of groups $d = |\cG|$ 
\STATE Set group counts $n_0 \gets \textbf{0}^d$
\STATE Initialize $p_1 \gets 0.5 \cdot \textbf{1}^d$ \texttt{// At round $t$, $p_t = (p_{t, G})_{G \in \cG}$ will contain group propensities}
\STATE Initialize $w'_1 \gets \textbf{1}^d, L_0 \gets \textbf{0}^d, q_0 \gets 0$ \texttt{// Parameters used to update group weights}
\FOR{$t=1, 2, \ldots$}
    \STATE Receive covariate vector $x_t \in \cX$, determine the set of active groups $\cG_t = \{G: x_t \in G, G \in \cG\}$ 
    \STATE Cast $\cG_t$ as indicator vector $a_t \in \{0, 1\}^d$ ($a_{t, G}=1 \iff G \in \cG_t$). Set group counts: $n_{t} \!\gets\! n_{t-1} + a_t$
    \STATE Normalize group weights: $w_{t, \mathrm{eff}} \gets \frac{a_{t} \odot w'_{t}}{\langle a_t, w'_t \rangle}$ \texttt{// Set inactive group weights to $0$}
    \STATE Set effective treatment probability: $p_{t, \mathrm{eff}} \gets \langle w_{t, \mathrm{eff}}, p_t\rangle$ \texttt{// Aggregate group propensities}
    \STATE Set treatment decision: $Z_{t} \sim \mathrm{Bernoulli}(p_{t, \mathrm{eff}})$
    \STATE Receive realized outcome: $Y_t \gets y_t(Z_{t})$
    \FOR{active groups $G \in \cG_t$}
        \STATE \texttt{/* Update group propensities using group-specific \ClipOGDSC-type update */}
        \STATE Set estimated Neyman gradient as: \newline
        $\widetilde{g}_{t, G} \gets Y_t^2 \left( \frac{Z_{t}}{p_{t, \mathrm{eff}}} + \frac{1-Z_{t}}{1-p_{t, \mathrm{eff}}} \right) \left( - \frac{Z_{t}}{p_{t, G}^2} + \frac{1-Z_{t}}{(1-p_{t, G})^2} \right)$
        \STATE Update $p_{t+1, G} \gets \Proj\limits_{[\delta_{t, G}, 1-\delta_{t, G}]}(p_{t, G} - \eta_{t, G} \cdot \widetilde{g}_{t, G})$, where $\eta_{t, G} \gets \frac{1}{2 c^2 \cdot n_{t, G}}$ and $\: \delta_{t, G} \gets \frac{1}{h(n_{t, G})}$
        \STATE  \texttt{/* Get losses used to update group weights */}
        \STATE Set estimated Neyman loss as: \newline
        $\widetilde{\ell}_{t, G} \gets Y_t^2 \left( \frac{Z_{t}}{p_{t, \mathrm{eff}}} + \frac{1-Z_{t}}{1-p_{t, \mathrm{eff}}} \right) \left( \frac{Z_{t}}{p_{t, G}} + \frac{1-Z_{t}}{1-p_{t, G}} \right)$
    \ENDFOR
    \FOR{inactive groups $G \not\in \cG_t$}
        \STATE Set $p_{t+1, G} \gets p_{t, G}$ and $\widetilde{\ell}_{t,G} \gets 0$ \texttt{// Inactive groups are not updated}
    \ENDFOR
    \STATE \texttt{/* Update group weights: Higher cumulative group losses $\to$ larger weights */}
    \STATE Set surrogate loss: $\ell_{t} \gets a_t \odot \left(\widetilde{\ell}_{t} - \langle \widetilde{\ell}_t, w_{t, \mathrm{eff}} \rangle\right)$
    \STATE Set $L_t \gets L_{t-1} + \ell_t$ and $q_t \gets q_{t-1} + \norm{\ell_{t}}^2_2$
    \STATE Update group weights: $w'_{t+1} \gets \max\limits_\mathrm{per-coordinate} \left\{ \textbf{0}^d , - \frac{1}{\sqrt{q_{t}}}L_{t} \right\} $
\ENDFOR
\end{algorithmic}
\end{algorithm}


Given a collection $\cG$ of $d$ groups, in each round MGATE reads off the currently active groups $\cG_t \subseteq \cG$, i.e., those groups that contain $x_t$ ($G \ni x_t$), and then proceeds to determine the new treatment probability by aggregating the `best-guess' probabilities for all active groups $G \in \cG_t$ determined based on the past performance of those groups.
To do so, MGATE maintains group weights $w'_{t, G}$ and group-specific propensities $p_{t, G}$. It comes up with a single effective treatment probability: $p_{t, \mathrm{eff}} \sim \sum_{G \in \cG_t} w'_{t, G} p_{t, G}$ in each round by reweighing the group specific propensities of the active groups. This effective treatment probability should simultaneously satisfy the interests of all active groups. The treatment decision $Z_t$ is then generated according to $p_{t, \mathrm{eff}}$. After the outcome is revealed, MGATE updates all group weights, as well as the propensities of groups that were active. 

We can show that MGATE achieves the following multigroup Neyman regret guarantee. We note that MGATE is anytime valid, meaning that just like our noncontextual design \ClipOGDSC, it does not require advance knowledge of the time horizon $T$. 
\begin{theorem}[Guarantees for \cref{alg:AMGATE}] \label{thm:multigroup}
    Fix any context space $\cX$ and finite group family $\cG \subseteq 2^\cX$. Suppose\footnote{By replacing the \ClipOGDSC propensity updates in MGATE with ClipOGD$^0$-style updates, we can straightforwardly obtain a multigroup design which only relies on the assumptions of \citet{dai2023clip} while keeping $\widetilde{O}(\sqrt{T})$ multigroup Neyman regret. This follows from the generality of our multigroup meta-design presented in \cref{app:multigroup}, which can use a wide variety ``ClipOGD-style'' updates while still obtaining $\widetilde{O}(\sqrt{T})$ multigroup regret.}  \cref{ass:bounds} holds with lower bound constant $c > 0$. Then, for any clipping function $h$, the expected multigroup regret of \cref{alg:AMGATE} will be bounded as:
    \[
    \RegVarMG_T(\cA; \cG)  = O \left( \sqrt{|\cG|} \cdot (h(T))^5 \cdot \sqrt{T} \right).
    \]
\end{theorem}

\subsection{Technical Overview}
The full analysis of \cref{alg:AMGATE} is contained in \cref{app:multigroup}. It builds on several tools recently developed in the online learning literature, which are formally introduced in \cref{app:multigroup-se}, and we briefly survey them here. The central tool is the sleeping experts algorithmic framework \citep{blum2020advancing}, which has recently been shown to be able to combine the wisdom of multiple sub-learners (or experts) into a meta-algorithm with performance on par with each of the sub-learners. The key difference from typical online aggregation schemes is that each sub-learner is allowed to be inactive (asleep) on some rounds, on which it does not give advice to the meta-algorithm. At a high level, to obtain multigroup Neyman regret, we would thus like to use a sleeping experts algorithm to aggregate propensities suggested by $|\cG| = d$ copies of \ClipOGDSC that are respectively active on all groups $G \in \cG$; the aggregated design would then perform comparably to each copy of \ClipOGDSC on its group $G$. Then, since that copy of \ClipOGDSC will have no regret on group $G$, neither will the aggregated design.

\paragraph{Challenges and solutions} Past work on sleeping experts does not fully address the combination of difficulties present in our setting: (1) stochastic (realized outcome) feedback rather than full-information (both outcomes) feedback; (2) the need to perform clipping of the iterates (propensities) to explicitly restrict them from approaching the feasible set's boundary too fast; and (3) the fact that the gradient feedback magnitude grows unboundedly as $T \to \infty$, even with clipping. 

While there are a limited number of ``sleeping bandits'' algorithms in the literature (e.g., see \citet{nguyen2024near}) that address the stochastic feedback, they don't naturally extend to cover both of the latter two issues. Therefore, we design from scratch a new sleeping experts algorithm tailored to all of these challenges. It employs \emph{scale-free} updates of the group weights $w'_t$ so as to control the loss and gradient feedback magnitudes; we achieve this by deploying an instance of the seminal scale-free SOLO FTRL algorithm of \citet{orabona2018scale} and endowing it with sleeping experts regret guarantees via a recent reduction of \citet{SleepingExpertsOrabona}. To clip the effective probability magnitudes, our algorithm aggregates over the suggested per-group probabilities via convex combinations rather than via sampling from their mixture. Finally, to ensure that the per-group propensity updates remain valid under stochastic gradient feedback and despite the aggregator using a different propensity than the suggested per-group one, MGATE uses a combination of unbiased first-order ($\widetilde{g}_{t,G}$) and zeroth-order ($\widetilde{\ell}_{t, G}$) per-group feedback estimators, which depend on both $p_{t, \mathrm{eff}}$ and $p_{t,G}$.

\paragraph{A generalized meta-design} Our analysis in \cref{app:multigroup} generalizes beyond MGATE (\cref{alg:AMGATE}). Indeed, our approach more generally allows the use of any scale-free sleeping experts algorithm to update group weights, and any ClipOGD-style (see \cref{app:multigroup-first_order}) no-regret adaptive designs to update the groupwise treatment probabilities. Thus, we more generally provide a meta-design that reduces multigroup designs to a broad class of non-contextual, no-regret designs. This generalized meta-design is given as \cref{alg:multigroup_general} in \cref{app:multigroup_general}, and \cref{thm:multigroup_general} contains its regret bound, of which \cref{thm:multigroup} above is a corollary. 

\begin{figure*}[t]
    \includegraphics[width=0.99\textwidth]{plots/gaussian_varying_sigma_fullpaper.pdf}
    \caption{
         \textbf{Treatment probabilities and Neyman regret of ClipOGD on Gaussian data} for different noise ($\sigma$) levels. As $\sigma$ increases, ClipOGD$^\textrm{SC}$ converges more slowly. Its regret remains high, and the treatment probabilities do not settle within the observed time horizon ($T\approx 50{,}000$). The black line in the treatment probabilities indicates the Neyman optimal probability.
         }
        \label{fig:gaussian}
        \vspace{-1em}
\end{figure*}


\section{Experimental Results} \label{sec:experiments}

We first present the results for the non-contextual setting and then turn to the analysis of the performance for the contextual algorithm. Our code will be made available at the following link: \href{https://github.com/amazon-science/adaptive-abtester}{https://github.com/amazon-science/adaptive-abtester}. 

\subsection{Non-Contextual Experiments}

\paragraph{Tasks} We compare our method ClipOGD$^\textrm{SC}$ with ClipOGD$^\textrm{0}$ \citep{dai2023clip} on multiple tasks. Below, we show two key datasets (one synthetic and one real-world) used in our experiments, with full details in \cref{app:datasets}. The first is a synthetic dataset is generated as follows: $\smash{y_t(i) \overset{\text{iid}}{\sim} \mathcal{N}(\mu_i, \sigma^2)}$ for $t=1, \dots, T$ and $i=0,1$ with $\mu_0=1$ and $\mu_1=2$. We vary $\sigma_i\in\mathbb{R}_+$ to showcase where our method succeeds and where it struggles. 
The second dataset comes from Egypt’s largest microfinance organization \citep{groh2016macroinsurance}, covering 2,961 clients. Here, the treatment is a new insurance product, and the outcome is how much individuals invest in machinery. Following \citet{dai2023clip}, we fill missing values with Gaussian noise and resample each unit five times to increase the population size. We also present experiments 
on the ASOS Digital Experiments Dataset \citep{liu2021datasets},
 and on question-answering tasks for large language models (e.g., BigBench \citep{srivastava2022beyond}) in the Appendix. 

\paragraph{Experimental setup} In our simulation, each unit is randomly assigned to treatment or control using the treatment probability from our method or ClipOGD$^\textrm{0}$. We repeat this process 10,000 times, generating many different treatment-control paths. We then measure the Neyman regret  by averaging the regret across these probabilities obtained at each time step.

\paragraph{Hyperparameter choices} Throughout the experiments, we use the following hyperparameters. For our method, we set $\eta_t = 2/t$, and we set the clipping rate $\delta_t = 1/h(t)$, where the clipping function is $h(t) = \exp\bigl((\log (t+2))^{1/4}\bigr)$. For ClipOGD$^\textrm{0}$, we follow \citet{dai2023clip} with a constant learning rate $\eta_t = 1/\sqrt{T}$ and clipping rate $\delta_t = 0.5 \cdot t^{-1/\sqrt{5 \log T}}$.

\paragraph{Results} We analyze three synthetic data settings where we vary $\sigma$ as $\{0.1,1,10\}$. As $\sigma$ increases, the ratio $C/c$ also grows, so by \cref{eq:main_bound}, we expect slower convergence of our algorithm. We set $T=50{,}000$. \Cref{fig:gaussian} shows the Neyman regret across these settings, matching our theoretical expectations: when $\sigma=0.1$, the regret of ClipOGD$^\textrm{SC}$ drops to 0 quickly, but for larger $\sigma$, the regret remains high and converges later. The regret of ClipOGD$^\textrm{0}$ instead keeps increasing with time. Nonetheless, in line with \Cref{thm:convergenceinl2}, \Cref{fig:gaussian} also shows that our method's adaptively chosen propensities ultimately converge to the Neyman optimal probability in all three cases. By contrast, the propensities of ClipOGD$^0$ only converge when $\sigma=10$, which happens to match the initial probability of 0.5. Next, we turn to examine the results on the microfinance data. \Cref{fig:microfinance} illustrates the treatment probabilities and Neyman regret for both algorithms. On average, each design assigns probabilities near the Neyman probability. However, those of ClipOGD$^\textrm{0}$ exhibit higher variance compared to ClipOGD$^\textrm{SC}$. This translates into greater Neyman regret in later rounds, which never converges to 0. The probabilities assigned by our method, instead, converge to the Neyman probability, yielding vanishing average Neyman regret. 


\begin{figure}[t]
    \includegraphics[width=0.99\columnwidth]{plots/microfinance_sidebyside.pdf}
    \caption{
        \textbf{Treatment probabilities and Neyman regret of ClipOGD on microfinance data} for $T\approx 15{,}000$ rounds.
    }
        \label{fig:microfinance}
        \vspace{-1em}
\end{figure}


\subsection{Contextual Experiments}

\begin{figure}[t]
    \includegraphics[width=.99\columnwidth]{plots/microfinance_multigroup_regret_fullpaper.pdf}
    \caption{
         \textbf{Group-conditional Neyman regret of ClipOGD and MGATE on microfinance data}. MGATE produces the lowest \emph{$\cG$-multigroup} Neyman regret as desired, and in this case dominates the non-contextual ClipOGD variants for each group, including the noncontextual group $G_0=\mathcal{X}$.
         }
        \label{fig:microfinance_grouped}
        \vspace{-1em}
\end{figure}

Here we present our contextual results using \cref{alg:AMGATE} over the previously-described datasets. To standardize the contextual groups in each experiment, we design simple, synthetic post-hoc groups by scoring each sample as $\smash{s_t = 1 / \left(1 + \frac{y_t(0)^2}{y_t(1)^2+\epsilon}\right)}$ (the optimal Neyman sampling probability for the single sample). Our groups are computed by checking whether sample $t$ belongs to some predetermined quantile of the score function $\smash{G_0=\mathcal{X}, G_1=\mathbbm{1}[F^{-1}(s_t) \le \frac{2}{3}], G_2=\mathbbm{1}[\frac{1}{3}\le F^{-1}(s_t)]}$. We note that these groups are overlapping and informative since $G_1$ is guaranteed to have lower or equal optimal sampling probability than $G_2$.

We stress that these groups are included for illustrative purposes and rely on information that would be unobservable in a real ATE experiment, but nonetheless showcase the potential for high-quality contextual information for multi-group ATE. \Cref{fig:microfinance_grouped} shows the Neyman regret for ClipOGD$^\textrm{0}$, ClipOGD$^\textrm{SC}$, and MGATE on the microfinance dataset on each group; our MGATE method achieves the lowest group-conditional regret out of all the methods, effectively minimizing the \emph{$\cG$-multigroup} Neyman regret, and thereby validating our theoretical results. Additional contextual experiments are provided in the Appendix.

\section{Conclusion}

In this paper, we studied adaptive designs for unbiased ATE estimation with finite-population guarantees. We introduced a modification of the ClipOGD algorithm that provably yields vanishing Neyman regret, achieving an anytime-valid $\widetilde{O}(\log T)$ Neyman regret, improving upon previous \(\widetilde{O}(\sqrt{T})\) guarantees. We also extend our framework to incorporate contextual information by introducing a multigroup formulation. Our proposed multigroup adaptive design ensures \(\widetilde{O}(\sqrt{T})\) regret for each predefined group, enabling efficiency improvements for subgroup ATE estimation. Experimental results corroborate these findings. 

Overall, these results suggest that adaptive experimentation can achieve strong finite-population efficiency guarantees, offering practical advantages for a wide range of applications. Future work could explore extensions to other experimental designs and further reductions in regret rates.

\section*{Acknowledgments}
G.N. thanks Vanessa Murdock for the support throughout this project. The authors thank Lorenzo Masoero, Blake Mason, and James McQueen for useful feedback.


\bibliographystyle{plainnat}
\bibliography{sample}

\appendix

\onecolumn
\input{appendix}


\end{document}