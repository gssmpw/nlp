\section{Related Work}
% \textbf{Uncertainty Quantification in LLMs:}
% The quantification of current uncertainty in deep neural networks is primarily based on methods such as Bayesian neural networks and deep ensembles **Gal, "Deep Convolutional Neural Networks"** __**Lakshminarayanan et al., "Simple and Scalable Predictive Uncertainty Estimation at Deployment Time"**. These approaches effectively estimate uncertainty in classification tasks but face limitations in the context of open-ended language generation. Techniques like information-theoretic metrics, iterative prompting **Guo et al., "On Calibration of Modern Neural Networks"**, and conformal prediction **Janz et al., "Scalable Bayesian Optimization using Deep Neural Networks"** have advanced uncertainty estimation in multiple-choice scenarios but struggle with open-ended outputs.

% \textbf{Uncertainty-aware Selections in LLMs:}

% Recent studies show that increasing test-time computation, such as through repeated sampling, can significantly improve reasoning performance in LLMs. Repeated sampling has been shown to scale coverage (pass@k), allowing smaller models to outperform larger ones in some cases.

% Building on this, search-based approaches have emerged to guide computation toward more effective reasoning paths. These methods narrow down possible solutions by selecting the most promising candidates based on a scoring mechanism. Search algorithms typically rely on score models, such as value models, reward models, and advantage models, to evaluate the quality of reasoning paths.

% Despite their potential, current search methods struggle as computation scales. The involved score models may fail to identify high-quality paths accurately, leading to improper pruning due to imbalanced training data or limited exposure to diverse data. As a result, the effectiveness of current search-based methods diminishes as sample sizes grow, limiting their scalability.