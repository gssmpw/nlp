%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[table,dvipsnames]{xcolor}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[most]{tcolorbox}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
%\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\newcommand{\crefrangeconjunction}{--}
\crefformat{section}{#2\S#1#3}
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Tab.}{Tabs.}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption} 
\usepackage{comment}

% if you use cleveref..
%\usepackage[capitalize,noabbrev]{cleveref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%\renewcommand{\tableautorefname}{Tab.} 
%\renewcommand{\figureautorefname}{Fig.} 
%\renewcommand{\sectionautorefname}{Sec.} 
%\renewcommand{\subsectionautorefname}{Sec.}
%\renewcommand{\subsubsectionautorefname}{Sec.} 

\definecolor{californiagolden}{RGB}{255, 203, 5}
\definecolor{berkeleyblue}{HTML}{002676}

% Define the command with California Golden
\newcommand{\dacheng}[1]{\textcolor{californiagolden}{[Dacheng: #1]}}

\newcommand{\shishir}[1]{\textcolor{orange}{[Shishir: #1]}}
\newcommand{\tyler}[1]{\textcolor{blue}{[Tyler: #1]}}
\newcommand{\shiyi}[1]{\textcolor{purple}{[Shiyi: #1]}}
\newcommand{\shu}[1]{\textcolor{olive}{[Shu: #1]}}
\newcommand{\ion}[1]{\textcolor{blue}{[Ion: #1]}}
\newcommand{\joey}[1]{\textcolor{cyan}{[Joey: #1]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
% \icmltitle{\centering
%    Rethinking Learning for Reasoning: \\
%    Why Simple Supervised Fine-tuning Achieves O1 Performance?
%}
\icmltitle{\centering
{LLMs Can Easily Learn to Reason from Demonstrations}\\ %\\ \joey{Cute but I don't know if this is an RL or SFT paper.}
Structure, not content, is what matters!
}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dacheng Li}{equal,yyy}
\icmlauthor{Shiyi Cao}{equal,yyy}
\icmlauthor{Tyler Griggs}{equal,yyy}
\icmlauthor{Shu Liu}{equal,yyy}
\icmlauthor{Xiangxi Mo}{yyy}
\icmlauthor{Eric Tang}{xxx}
\icmlauthor{Sumanth Hegde}{xxx}
\icmlauthor{Kourosh Hakhamaneshi}{xxx}
\icmlauthor{Shishir G. Patil}{yyy}
\icmlauthor{Matei Zaharia}{yyy}
\icmlauthor{Joseph E. Gonzalez}{yyy}
\icmlauthor{Ion Stoica}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{Anyscale}
\icmlaffiliation{yyy}{Department of Electrical Engineering and Computer Sciences, University of California, Berkeley}

\icmlcorrespondingauthor{Ion Stoica}{istoica@berkeley.edu}
\vskip 0.3in
]
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution


\begin{abstract}
%Large language models (LLMs) can tackle complex reasoning tasks by thinking through long chain-of-thoughts (CoT) that incorporates reflection and backtracking. However, the training techniques and data required to elicit long CoT remain poorly understood. In this paper, we show that, firstly, an instruct-tuned model can easily learn to perform long CoT to greatly improve reasoning capability through supervised fine-tuning (SFT) or even parameter-efficient Low-rank Adaptation (LoRA) fine-tuning. With as few as \textbf{6.6k} long CoT training samples, Qwen2.5-32B-Instruct model learns to perform long CoTs and achieve significant improvement on both coding and math tasks: achieving 40.0\% (+23.7\%) on AIME 2024, 77.5\% (+ 10.0\%) on AMC 2023, 57.2\% (+9.8\%) on OlympiadBench. Noticeably, these results are close to the propriety OpenAI o1-preview model.
%achieve competitive performance to the OpenAI o1-Preview model. Specifically, the model a

%Secondly, we find that the key to the learning process is the \textbf{structure} of the long CoT rather than \textbf{contents} in individual reasoning steps: perturbations that alter content, such as introducing incorrect or corrupted samples or removing reasoning keywords, have minimal impact on performance, whereas structural modifications, such as shuffling or deleting reasoning steps, significantly degrade performance. For example, model trained on long CoT samples that lead to incorrect solution achieves 63.1\% performance on an average of five math datasets, a mere 3.2\% degradation from training with correct long CoT samples.
% \shishir{need to highlight results at end..}
% We perform a wide range of experiments on mathematical and coding reasoning tasks, where our conclusions are drawn from models near OpenAI O1-series model. 
% Together, our paper improves the level of understanding of what is needed to add reasoning capabilities to an instruction-following model and starts the discussion on what is needed to efficiently train the next generation of reasoning models.
%Together, our work deepens the understanding of what is required to instill reasoning capabilities in instruction-following models and opens a discussion on how to efficiently train the next generation of reasoning models.

%\dacheng{(1) Small data size works, (2) wrong/corrupted traces work (3) LoRA works} \shishir{agree on these 3. But we should think about making 1 (small data) subset of point 2 (corrupted traces), and of-course emphasize 3 - LoRA}
Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. 
However, the training techniques and data requirements to elicit Long CoT remain poorly understood. 
In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). 
With just \textbf{17k} long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including \textbf{56.7\%} (+40.0\%) on AIME 2024 and \textbf{57.0\%} (+8.1\%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6\% and 59.1\%.%, respectively. 

More importantly, we find that \textbf{the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact}. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. 
For example, a model trained on Long CoT samples with incorrect answers still achieves only \textbf{3.2\%} lower accuracy compared to training with fully correct samples. 
% \textbf{63.1\%} across five math benchmarks, 
These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. \color{berkeleyblue}{This is the academic paper of our previous released Sky-T1-32B-Preview model}. Codes are available at \url{https://github.com/NovaSky-AI/SkyThought}.
% Codes and artifacts will be open-sourced upon acceptance.
% These findings improve our understanding of how to elicit reasoning capabilities in instruction-following models and highlight key considerations for efficiently training the next generation of reasoning models.
\end{abstract}

\section{Introduction}
%\dacheng{Help needed
%(1) Lower tone in some places; a strong attack is we only train on qwen-32b-instruct. \shishir{A: use compute resource limitation argument for qwen only} (2) Add conclusion from ablations.}
% \begin{tcolorbox}[colback=gray!30, colframe=gray]
%Draft PIC (1.29): Dacheng
%\end{tcolorbox}
\label{sec:intro}
% Paragraph 1: Reasoning models unlock new capabilities beyond instruction following capabilities. They perform long CoTs before replying, and reflect backtrack in the response to perform complex reasoning. However, we have limited understanding on how to go from instruction follow model to reasoning models.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{\textwidth}  % Proper width definition
        \centering
        \includegraphics[width=.99\linewidth]{figures/Sky-T1-teaser.pdf}
        \caption{Responses of the base model, with Long CoT SFT, and with Long CoT LoRA.} %The trained model efficiently learns to produce long CoT that includes reflection and backtracking.}
        \label{fig:response_demo}
    \end{subfigure}
    \hfill
    \vspace{0.1em}
    \begin{subfigure}[b]{\textwidth}  % Proper width definition
        \centering
        \includegraphics[width=.95\linewidth]
        {figures/teaser-fig1-new.pdf}
        \caption{Performance of different models on five difference reasoning benchmarks.}
        \label{fig:intro-performance}
    \end{subfigure}
    \caption{\textbf{Learning to reason is data- and parameter-efficient.} When fine-tuned on a small amount (17k) of Long CoT samples distilled and reject-sampled from DeepSeek-R1 with either LoRA or full-parameter tuning, the model easily learns to perform reflection and backtracking by using keywords such as ``However'' and ``Alternatively'' (Top). Consequently, the fine-tuned models improve significantly across five popular math and coding benchmarks (Bottom). For fine-tuning, the base model is Qwen2.5-32B-Instruct.}
    \label{fig:longcot_teaser}
    \vspace{-2mm}
\end{figure*}

Large reasoning models (LRMs) leverage long chain-of-thoughts (Long CoTs) with reflection, backtracking, and self-validation to tackle challenging reasoning tasks~\citep{jaech2024openai, guo2025deepseek, team2024qwq}. 
However, the process of eliciting Long CoTs from available LLMs remains unclear, as existing methods are either closed-sourced~\citep{jaech2024openai, team2024qwq} or expensive to replicate~\citep{guo2025deepseek}.
% Dacheng 
% Despite strong reasoning performance, the community has limited understanding of how to elicit long CoT from available instruction-tuned LLMs, as the methodology is either closed-sourced~\citep{jaech2024openai, team2024qwq} or is expensive to replicate~\citep{guo2025deepseek}.

%unlock new capabilities beyond instruction-following large language models (LLMs). They address complex tasks by performing long chain-of-thoughts with reflection, backtracking, and self-validation before presenting a final answer~

% Paragraph 2: Surprisingly, we find such capabilities can be achieved in a data-efficient and parameter-efficient way through distillation, implying that reasoning capabilities can be elicited easily, instead of fundamentally new capabilities. 
% In this paper, we first show that a simple distillation method is surprisingly effective in adding reasoning capabilities to an existing instruction-following model. In particular, using as little as 4.6k training data set generated by LRM QwQ-32B-Preview~\citep{team2024qwq}, we fine-tune an instruction following LLM Qwen2.5-32B-Instruct~\citep{yang2024qwen2} to achieve performance close to the OpenAI o1-Preview model. The fine-tuned model learns to use more reasoning keywords such as "However", "Alternatively" to reflect and revise its previous answer, and learns to use longer chain-of-thoughts (CoT) for difficult problems (Figure~\ref{fig:longcot_teaser}).

% In this paper, we first find that instruct-tuned LLMs can be surprisingly easy to be taught to produce long CoT responses and improve reasoning capabilities. %\shishir{It's hard for me to tell from the writing if this is your contribution, or something you assume, or somethig someone else has proved. You ened ot be more explicity}
% In particular, the learning process can be both \textbf{data-efficient} and \textbf{parameter-efficient}.
% dacheng: see above 
%\shishir{It's hard for me to tell from the writing if this is your contribution, or something you assume, or somethig someone else has proved. You ened ot be more explicity}
In this paper, we first show that, surprisingly, an LLM can be cheaply and easily taught to produce Long CoT responses, significantly improving its reasoning capabilities. 
In particular, we find that this learning process can be both \textbf{data-efficient} and \textbf{parameter-efficient}.
%a simple distillation method is surprisingly effective at instilling long CoT reasoning capabilities to an instruction-following model.
By performing fully supervised fine-tuning (SFT) with only \textbf{17k} samples generated by DeepSeek R1, the Qwen2.5-32B-Instruct model achieves performance competitive with OpenAI o1-preview across a wide range of math and coding tasks~\citep{team2024qwq, yang2024qwen2, jaech2024openai}. 
In particular, it achieves 90. 8\% in Math-500 (+6.0\%), 56.7\% in AIME 2024 (+40.0\%), 85.0\% in AMC 2023 (+17.5\%), 60.3\% in OlympiadBench (+12.7\%) and 57.0\% in LiveCodeBench (+8.1\%)
~\citep{jain2024livecodebench}.
%Using 6.6k %\shu{number inconsistent with abstract}
%training samples generated by QwQ-32B-Preview, we fine-tune the Qwen2.5-32B-Instruct LLM~\citep{yang2024qwen2} using full supervised fine-tuning (SFT), achieving performance close to the OpenAI o1-Preview model on a wide range of math and coding tasks~\citep{team2024qwq, yang2024qwen2, jaech2024openai}. 
Even further, the model \textbf{can achieve o1-preview performance by updating fewer than $5\%$ parameters with LoRA fine-tuning}~\citep{hu2021lora}.
%using the parameter-efficient LoRA fine-tuning, the student model effectively learns to use 
% \joey{More relative to QwQ or to the Qwen-instruct full fine-tune?  Are you saying that LoRA is somehow an improvement over full SFT in terms of capabilities?}
% In the learning process, the model learns to use reasoning keywords such as ``However'' and ``Alternatively'' to reflect and revise its previous answers, as well as adopting longer chain-of-thoughts (CoT) for difficult problems (\cref{fig:longcot_teaser}).
We show that the model successfully learns to reflect and revise its intermediate thoughts (e.g., frequently using reasoning keywords such as ``Alternatively'' and ``Wait, but'') and adopts long, coherent CoTs to tackle challenging problems (\cref{fig:longcot_teaser}).

Moreover, we identify the Long CoT \textit{structure} as the key characteristic of distilled data for eliciting strong reasoning performance rather than the \textit{specific contents} of individual reasoning steps within the Long CoT.
To test this, we conduct two sets of controlled studies by altering either the content of individual reasoning steps or the overall logical structure. 
To alter content, we perturb samples by replacing numbers with random digits or deleting reasoning keywords. Surprisingly, we find that these perturbations have little impact on the model performance: even when 50\% of numbers in training samples are randomly changed, the model only observes 3.3\% lower accuracy on the most challenging math benchmark, AIME 2024, as compared to training with correct samples.
To alter the global reasoning structure, we separate responses into reasoning steps and randomly shuffle, insert, or delete these steps. We observe that the trained model is much more sensitive to structural perturbations that break logical coherency in the long CoT. For example, when $67\%$ of the training samples' reasoning steps are shuffled, accuracy drops by $13.3\%$ on AIME 2024 problems relative to training with correct samples. 
% \tyler{We could alternatively highlight average performance, which might make the benefits appear more general? Not sure. Will come back to this.}

% Paragraph 3: More surprisingly, we found that even the correctness of the traces do not matter much. Even more, we can achieve this by only updating partial model weights through LoRA.

% Paragraph 4: So far, we have understand that reasoning is elicited rather than a new capabilities. We analyze what matters to elicit such capabilities, e.g. the structure of the reasoning traces.

% Thirdly, we further provide evidence that the key to reasoning is the structure rather than new knowledge \ion{This sounds funny; need to reformulat.}, by training low-rank adapters(LoRAs) to add reasoning capabilities to an existing instruction-following model. \dacheng{@Shiyi @Shu could you add the LoRAs conclusion?} 

% In the end, we further discuss the performance between long CoTs and repeadly sampling from shorter CoTs, indicating that th\dacheng{@Anyscale ongining}

% Paragraph 5: In the end, we perform visualization of the elicit procedure, and also upper bound analysis to open new research.
In summary, our key contributions are:
 \begin{enumerate} 
 \item We demonstrate that an LLM can learn Long CoT reasoning in a data-efficient and parameter-efficient manner (i.e., LoRA). With fewer than 17k samples, we fine-tune the Qwen2.5-32B-Instruct model to be competitive with o1-preview. 
 % a LLM can learn to produce 
 % amount of data (5K) through supervised fine-tuning or even parameter-efficient fine-tuning (i.e., LoRA) is sufficient for teaching LLMs to reason.
 \item We identify the structure of Long CoT as critical to the learning process rather than the content of individual reasoning steps. To validate this finding, we perform two groups of controlled experiments that modify either the structure or contents of samples.
 % \item We show that the structure of the chain-of-thought is crucial for performance, rather than the content.
 \item We conduct comprehensive ablations across model sizes and architectures, dataset sizes, data generation models (DeepSeek R1 and QwQ-32B-Preview), and on five popular math and coding reasoning benchmarks.
 % \item Finally, we include blat findings open new avenues for efficiently training reasoning AIs \shu{next generation reasoning models} and improving their capabilities. 
 \end{enumerate}


%\begin{figure*}[h!]
%    \centering
%    \includegraphics[width=0.95\textwidth]{figures/all_stats.pdf}
%    \caption{Performance, average number of keywords, and average output length for four different datasets. Datasets are ordered from left to right, according to the accuracy of the instruction fine-tuned Qwen2.5-32B-Instruct model~\citep{yang2024qwen2}). With 4.6k long CoT training data, the model improves significantly, learns to use more keywords to respond such as "However", "Alternatively", and consequently longer chain-of-thoughts. In addition, the model learns to use more keywords and longer CoTs for more difficult benchmarks. Short CoT denotes an ablation setting where the model is only trained with the available short CoTs annotation in the Numina dataset~\citep{numina_math_datasets}}\label{fig:all_stats} 
%\end{figure*}

%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.5\textwidth]{figures/intro.pdf}
%    \caption{An example model response comparison with the original instruction fine-tuned model and the long CoT fine-tuned one. The complete responses can be found at Appendix~\ref{ref:figure_shortcot}.\dacheng{Making it visually better}}\label{fig:example} 
%\end{figure}

%\begin{table}[ht]
%    \centering
%    \resizebox{0.45\textwidth}{!}{%
%    \begin{tabular}{lccc}
%        \toprule
%& Math-500 & AIME 2024 & AMC 23  \\
%        \midrule
%        Qwen2.5-32B-Instruct & 84.8 & 16.7 & 46.5 \\
%        \toprule
%        Teacher=QwQ-32B-Preview \\
%        \midrule
%        Distill-5k   & 89.2 (+4.8) & 40.0 (+23.3) & 77.5 (+31)       \\
%        Distill-15k   &  &  &        \\
%        Distill-30k   &  &  &        \\
%        \midrule
%        Teacher=DeepSeek-R1-32B\\
%        \midrule
%        Distill-5k   & & &    \\
%        Distill-15k   &  &  &        \\
%        Distill-30k   &  &  &        \\
%        \midrule
%        QwQ-32B-Preview  & 85.4 & 50.0 &   \\
%        OpenAI O1-Preview & 81.4 & 40.0 &   \\
%        \toprule
%        & OlympiadBench  & Average \\
%        \toprule
%        Qwen2.5-32B-Instruct & 47.6 & 48.9  \\
 %       \toprule
 %       Teacher=QwQ-32B-Preview \\
 %       \midrule
 %       Distill-5k   & 58.5 &    66.3 (+17.4)    \\
 %       Distill-15k   &  &  &        \\
 %       Distill-30k   &  &  &        \\
 %       \toprule
 %       Teacher=DeepSeek-R1-32B \\
 %       \midrule
 %       Distill-5k   & &   \\
 %       Distill-15k   &  &  &        \\
 %       Distill-30k   &  &  &        \\
 %       \midrule
 %       QwQ-32B-Preview     \\
 %       OpenAI O1-Preview    \\
 %       \bottomrule
 %   \end{tabular}%
 %   }
    
 %   \caption{Simple distillation improves Qwen-32B-Instruct results to match OpenAI O1-Preview. \dacheng{(1) Update O1-Preview and qwq results on qwen official report (2) Update an ablation on using the short-CoT to train. (3) Launch more with R1-32B}. }
  %  \label{tab:distill_32b}
%\end{table}

\begin{comment}
\begin{table*}[h]
    \centering
    \caption{Effect of training on different versions of Pertubations data in Section~\ref{sec:key}. All models are trained with \textbf{Qwen2.5-32B-Instruct}. In particular, we study (1) traces that change contents: wrong answer traces, digits corrupted traces, keywords removed traces, and (2) traces that change structure: (1) steps shuffled traces, (2) steps deleting traces, and (3) steps insertion traces. \textbf{We found that traces that modify the overall structure is more detrimental to the performance than traces that modify contents.}}
    %Qwen2.5-32B-Instruct under different conditions. Teacher model = Qwen2.5-32B-Instruct.\dacheng{@Matei: Make the teacher model more clear}}
    \begin{tabular}{lccccc}
        \toprule
        & Math-500 & AIME 2024 & AMC 23 & OlympiadBench & Average \\
        \midrule
        \textbf{Baselines} \\
        Qwen2.5-32B-Instruct & 84.8 & 16.7 & 46.5 & 47.6 & 48.9 \\
        Train with correct trace & 89.2 & 40	& 77.5	& 58.5 & 66.3 \\
        \midrule
        \textbf{Wrong answer Traces} & 88.6 & 30.0 & 77.5 & 56.1 & 63.05 \\
        \midrule
        \textbf{Digit corrupted Traces} \\
        100\%  & 5.4  & 0.0  & 2.5  & 2.8  & 2.675 \\
        70\%   & 85.6 & 30.0 & 77.5 & 54.8 & 61.975 \\
        50\%   & 87.6 & 36.7 & 77.5 & 55.0 & 64.2 \\
        20\%   & 88.4 & 30.0 & 82.5 & 57.2 & 64.525 \\
        \midrule
        \textbf{keywords removed Traces} \\
        100\%    & 86.6 & 33.3 & 77.5 & 54.4 & 62.95 \\
        50\%     & 87.6 & 36.7 & 82.5 & 56.7 & 65.875 \\
        20\%     & 87.2 & 33.3 & 72.5 & 56.1 & 62.275 \\
        \midrule
        \textbf{Shuffle Traces} \\
        100\%    & 81.8 & 23.3 & 70.0 & 49.1 & 56.05 \\
        67\%     & 82.0 & 26.7 & 72.5 & 47.6 & 57.2 \\
        33\%     & 85.6 & 33.3 & 75.0 & 55.3 & 62.3 \\
        \midrule
        \textbf{Deleting Traces} \\
        100\%    & 79.2 & 13.3 & 60.0 & 45.4 & 49.475 \\
        67\%     & 84.2 & 26.7 & 55.0 & 48.1 & 53.5 \\
        33\%     & 88.2 & 23.3 & 80.0 & 57.7 & 62.3 \\
        \midrule
        \textbf{Inserting Traces} \\
        100\%    & 77.0 & 10.0 & 50.0 & 41.1 & 44.5 \\
        67\%     & 81.8 & 20.0 & 60.0 & 46.0 & 52.0 \\
        33\%     & 86.6 & 33.3 & 77.5 & 57.2 & 63.7 \\
        \bottomrule
    \end{tabular}
    \label{tab:perturbations}
\end{table*}
\end{comment}



\section{Related work}
\textbf{Test Time Scaling for Large Language Models}
Scaling test-time compute has proven effective in enhancing the the reasoning capabilities of LLMs. 
% This paradigm can be broken down into two primary directions: single long CoT, and ensemble multiple CoT. 
This can be broadly categorized into two directions: single long CoT and repeatedly sampled CoT. The former trains models, such as OpenAI o1, DeepSeek R1, and Qwen QwQ, to generate individual, long CoT responses with in-context reflection and backtracking to handle complex reasoning tasks~\citep{guo2025deepseek, jaech2024openai, team2024qwq}. 
Alternatively, repeated sampling methods, such as Best-of-N or search-guided generation (e.g., MCTS), improve reasoning performance by invoking multiple responses from the model, sometimes guided by search algorithms and reward models~\citep{snell2024scaling, brown2024large}. In this paper, we focus on distilling the ability to generate individual, Long CoTs, and show it can be done in a data- and parameter-efficient manner. 

\textbf{Training to improve reasoning capabilities of LLMs}
LLM reasoning capabilities can be improved by approaches such as iterative self-improvement and reinforcement learning (RL)~\citep{zelikman2022star, lightman2023let, lambert2024t, yuan2024free, guo2025deepseek}. More recently, Tulu-3 introduces Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance in tasks such as math and coding~\citep{hendrycksmath2021,jain2024livecodebench,numina_math_datasets}. PRIME proposes a RL-based method without process labels~\citep{yuan2024free}. The recent release of DeepSeek R1~\cite{guo2025deepseek} demonstrates that LLMs can learn to produce long CoT and improve reasoning using a pure RL-based approach. 
% This paper provides a non-RL and non-self-improvement-based approach to improve reasoning capabilities while achieving state-of-the-art performance on reasoning tasks. \shu{Not sure what does it mean by state-of-the-art performance? feels like we are comparing against RL}
Instead of bootstrapping reasoning ability, this paper focuses on the surprising data- and parameter-efficiency of distilling reasoning abilities from an existing reasoning model to an LLM. 

% There is a rich line of research that improves the by self-improving and reinforcement learning (RL). For example, 

% Prompting or training Large Language Models to perform complex reasoning enables the solving of challenging problems. In the prompting direction, chain-of-thoughts (CoT)~\citep{wei2022cot} allows the model to output an intermediate chain of tokens before the final answer. In 

% \joey{Note on related work section.  These days related work is also the section you highlight the novelty of your work. (think of it as the novelty section) Ideally at the end of each paragraph or cluster of related work you say what we did and how it is novel.  For example, maybe you can end the first paragraph with: "In contrast to the above work, here we focus on understanding the minimal amount of CoT demonstration data are required and what apsects of that data are essential for learning reasoning behavior.  Actually, you nailed this on the last paragraph in section 2.}

%\begin{tcolorbox}[colback=gray!30, %colframe=gray]
%PIC: Dacheng
%\end{tcolorbox}
%\paragraph{Reasoning for large language models} Mathematics reasoning tasks are important indicators of cognitive intelligence of humans~\citep{ahn2024large}. Numerous math datasets have been proposed to evaluate whether large language models process such reasoning capabilities~\citep{li2024numinamath, amini2019mathqa, hendrycksmath2021, cobbe2021training, miao2021diverse}. On the other hand, coding tasks often also require reasoning capabilities for code generation or code analysis. For instance, ~\citep{jain2024livecodebench, li2023taco} include algorirthmic problems such as LeetCode problems, and~\citep{gu2024cruxeval} also tests large language models for predicting code execution capabilities. From the modeling perspective, chain-of-thought like prompting techniques have been shown effective in eliciting the reasoning capability of large language models by producing thoughts before the final answers. Self-consistency further enhance chain-of-thought approach by inference multiple times and choose the majority votes for the answer~\citep{wang2022self}.\dacheng{@Matei delete these math benchmark, you don't need tons of related work on LRMs in general I think, but talk about the attempts to train them and the recent concurrent works}

%\paragraph{Test-Time scaling}
%More recently, scaling the test-time compute as opposed to scaling training compute has shown to be an effective way of improving reasoning capabilities. This paradigm can be majorly broke down into two directions: single long CoT based, and ensemble multiple CoT based. OpenAI o1, DeepSeek R1, Qwen QwQ models trains the model to reply with long CoT that includes reflection and backtracking, to better address reasoning tasks~\citep{guo2025deepseek, jaech2024openai, team2024qwq}. On the other hand, approaches such as repetitive sampling and further combination with search-based approaches during inference can also improve performance by choosing the best answer~\citep{snell2024scaling, brown2024large}. In this paper, our approach falls into the first category of test-time scaling. \joey{Say what we do that is novel...}
%\dacheng{@Matei: "There are a bunch of recent related works, and I think it's safe to call those "concurrent". Then there are the older ones that were much more complex and got worse results. Probably lead with the old ones and then talk about the very recent ones (TinyZero, HKUST SimpleRL, and the R1 paper itself). Maybe this is one advantage of talking about QwQ in the intro, but I would also mention that we see a similar phenomenon with R1 in there."}

\paragraph{Distillation}
Distilling the outputs or logits generated by a larger or more capable model has become a standard technique to enhance model performance~\citep{hinton2015distilling}.  Typically, responses generated by higher-quality models are used to perform supervised fine-tuning on smaller models~\citep{lambert2024t}. 
The Vicuna model, for instance, demonstrates that ChatGPT-generated responses can be used to effectively and cheaply distill high-quality chatting capabilities~\citep{zheng2023judging}. In this paper, we show that \textbf{reasoning capabilities can also be cheaply distilled}. We note that concurrent work has also observed similar trends in distilling reasoning capability~\citep{min2024imitate, huang2024o1}. 
Our paper differs from these recent works by demonstrating that reasoning distillation can be achieved efficiently with minimal parameter updates. We also provide an in-depth analysis of the key factors driving reasoning improvements, including the roles of the reasoning structure and content, as well as comprehensive evaluations and ablations across different data sizes and teacher models.

% The Vicuna model, for instance, demonstrated that ChatGPT-generated responses can effectively distill high-quality conversational capabilities\citep{zheng2023judging}. In this work, we extend this paradigm to reasoning, showing that \textbf{reasoning capabilities can be distilled efficiently and cost-effectively}. Concurrent studies have also observed similar trends in reasoning distillation~\citep{min2024imitate, huang2024o1}. However, our approach differs by demonstrating that reasoning distillation can be achieved with minimal parameter updates while maintaining strong performance. Furthermore, we conduct an in-depth analysis of the factors driving reasoning improvements, including the impact of reasoning structure and content. Our study provides comprehensive evaluations and ablations across varying data scales and teacher models, offering deeper insights into effective reasoning distillation strategies.





% learning process can be parameter efficient, and providing an in-depth study of the key factors of distilling reasoning capability. In particular, we show a breakdown of the role of structure and content in learning to reason as well as comprehensive evaluation and ablations such as different teacher models and data sizes. % \joey{fill me!}
\vspace{-1mm}
\section{Simple distillation is effective}
\label{sec:distill}
%\begin{tcolorbox}%[colback=gray!30, colframe=gray]
%PIC: Dacheng, Shiyi (LoRA)
%\end{tcolorbox}
% Shu: below 
In this section, we present our distillation process and show that a small amount of \emph{well-curated} data, along with a simple parameter-efficient fine-tuning method (e.g., LoRA), can effectively improve reasoning capabilities in a large language model. %\shu{double check}

% In this section, we present our distillation process and show how a simple distillation method with a small amount of 
% \emph{well curated} \joey{I added this because I want to claim we did a little more than just run QwQ 5k times to train our model. We might want to propagate this idea -- that data filtering is still important -- back to the intro?  It will help boost the contribution of this work (make it appear less trivial...).}
% data can be effective in adding reasoning capabilities to an instruction-following model~\citep{hinton2015distilling}. \shu{We should add LoRA into the sentence as well: i.e., we present our distillation process and show "how a small amount of \emph{well curated} data, and a simple distillation method (e.g. LoRA) can effectively add reasoning capabilities to an instruction-following model"}

\subsection{Experiments Setup}

% that supervised-finetuning on reasoning rollouts by a reasoning model. Specifically 

\paragraph{Distillation data curation.} We use DeepSeek-R1~\citep{guo2025deepseek} and QwQ-32B-Preview~\citep{team2024qwq}, two open-source models with reasoning capabilities, to generate our distillation data. We select difficult prompts from the AMC/AIME~\footnote{\scriptsize These prompts are from previous years of competition, which do not include AIME 2024 and AMC 2023 in our evaluation suite.}, Math, and Olympiad subset from the Numina-Math dataset~\citep{numina_math_datasets}, as~\citet{min2024imitate} implies that hard problems can improve performance. We also incorporate coding problems from APPS~\citep{hendrycksapps2021} and TACO~\citep{li2023taco} datasets.
% \joey{Before we jump to how we select difficult problems could we begin with motivating why?  We could say, prior work suggested starting with challenging problemts to get meaningful long CoT traces.}
Specifically, we use GPT-4o-mini to classify the difficulty of the prompts according to the AoPS standard~\citep{achiam2023gpt}, and select math problems of difficulty higher than Leval 3, Olympiad higher than Level 8, and all AIME/AMC problems. We verify the correctness of the traces by checking against ground truth solutions using exact matching for math problems and code execution for coding problems. In total, we curated 12k math and 5k coding problems with correct responses from QwQ to serve as our training data. For R1 samples, we directly use the public R1-17k reasoning dataset\footnote{\scriptsize huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k.} that is curated following a similar procedure.
% \joey{Could we say how many generations were required to reach 4618 examples. We generated over 7k examples but after filtering on correctness obtained only 4618 valid examples.}\shiyi{rewrite this part for 17k data}

\textbf{Training details.} We perform training using Llama-Factory~\citep{zheng2024llamafactory}. We train the Qwen2.5-32B-Instruct using a batch size of 96, learning rate 1e-5 with a warm-up ratio of 0.1 and linear learning rate decay~\citep{yang2024qwen2}, following similar hyperparameters in~\citep{min2024imitate}. We use the next token prediction loss as the training objective~\citep{radford2018improving}. We use the same hyper-parameters except a 1e-4 learning rate for LoRA fine-tuning.

\textbf{Evaluation setup.}
We evaluate our models on five popular reasoning benchmarks for math and coding, including Math-500, OlympiadBench, AIME-2024\footnote{\scriptsize 
huggingface.co/datasets/AI-MO/aimo-validation-aime.}, AMC23\footnote{\scriptsize huggingface.co/datasets/AI-MO/aimo-validation-amc.}~\citep{hendrycksmath2021, he2024OlympiadBench} and LiveCodeBench~\citep{jain2024livecodebench}. For LiveCodeBench, we report a weighted average accuracy across its easy, medium, and hard difficulty levels.
% We use temperature=0 to avoid randomness in evaluation and implementation from~\citet{yang2024qwen2}.

\subsection{Key Insights}
% The results are presented in Figure~\ref{fig:longcot_teaser}. The fine-tuned model improves significantly on all benchmarks, with 4.4, 31.0, 10.9, and 23.3 improvements on Math-500, AMC 2023, OlympiadBench and AIME 2024 respectively. In contrast, training only with short CoTs on the same sets of problems, available in Numina Dataset~\citep{numina_math_datasets}, decreases the performance. The model achieves performance close to OpenAI o1-preview model with only 4618 fine-tuning examples.

% \shu{I feel like the following two things can be separated into two subsection; insight 1: data-efficient, insight-2: parameter-efficient}

\textbf{Small amount of data is enough.}
In \cref{fig:intro-performance}, we present the performance of models fine-tuned with the 17k R1 trained samples. Both the supervised fine-tuned (SFT) and LoRA fine-tuned models learn to generate Long CoT responses and improve significantly on all benchmarks with just 17k training samples. 
% \shu{Talk about data efficiency with purely SFT flows better, then talk about LoRA separately. Otherwise, control variables are more than one? (i.e. why don't we do this data size variation in Table 2 for Lora as well?)}
% \begin{table}[ht!]
%    \centering
%    \caption{\textbf{Model accuracy with different data sizes.} The teacher model is QwQ-32B-Preview, and the student model is Qwen-32B-Instruct trained with full parameter fine-tuning. ``Olympiad.'' is short for ``OlympiadBench''. We find the student model can effectively learn to produce Long CoT with significant improvement with only 3k samples.}
%    \label{tab:qwq_performance}
%    \setlength{\tabcolsep}{4pt}
%    \resizebox{0.95\linewidth}{!}{
%    \begin{tabular}{lccccc}
%        \toprule
%        \textbf{Size} & \textbf{MATH500} & \textbf{AIME24} & \textbf{AMC23} & \textbf{Olympiad.} & \textbf{Avg.} \\
%        \midrule
%        200   & 23.0 & 20.0 & 17.5 & 16.7 & 19.3 \\
%        1k    & 80.2 & 20.0 & 75.0 & 51.7 & 56.7 \\
%        3k    & 87.6 & 33.3 & 87.5 & 57.0 & 66.4 \\
%        5k    & 89.2 & 40.0 & 77.5 & 58.5 & 66.3 \\
%        15k   & 89.0 & 30.0 & 80.0 & 57.5 & 64.1 \\
%        30k   & 89.2 & 46.7 & 77.5 & 57.3 & \textbf{67.7} \\
%        \bottomrule
%    \end{tabular}
%    }
%    \label{tab:data_scale}
% \end{table}
\begin{figure}[htbp]
\centering
\includegraphics[width=.45\textwidth]{figures/scaling.pdf}
\caption{\textbf{Model accuracy with different data sizes, and comparison to DeepSeek R1.} The teacher model is DeepSeek R1, and the student model is Qwen-32B-Instruct trained with full parameter fine-tuning. While the student model continues to benefits from more SFT data from DeepSeek R1, a small amount of data, e.g., 16k is sufficient to significantly boost the average performance by 15.2\%.}
\label{fig:data_scale}
\vspace{-4mm}
\end{figure}
We investigate the effect of distillation data size, ranging from 4k to 64k samples from R1, 
% We investigate the effect of distillation data size, ranging from 200 to 30k samples from QwQ-32B-Preview, to determine whether more data improves performance and if a smaller dataset can achieve comparable results\footnote{For this ablation study, we use QwQ for data curation and focus solely on math tasks for faster generation and evaluation.}.
The results, presented in \cref{fig:data_scale}, shows that a small amount of data, e.g. 16k is enough to significantly improve the model performance (from average 54.2 to 69.4).
% indicate that 3k distilled samples are sufficient to improve reasoning performance substantially. However, scaling up the training data by 10$\times$ to 30k does not lead to significant additional gains.
% We further investigate the impact of varying the amount of distillation data from 200 samples to up to 30k samples from the QwQ-32B-Preview model, and examine whether additional data enhances performance and whether a smaller dataset can achieve comparable results\footnote{For this ablation study, we use QwQ for data curation and focus solely on math tasks for faster generation and evaluation.}.



% We further vary the amount of distillation data with QwQ-32B-Preview model to study whether further scaling the data helps and whether there could be a smaller amount of data that achieves the same performance.\footnote{For this ablation study we only focus on the math tasks for faster evaluation.}
% The results are shown in \cref{tab:data_scale}. We find that 3k distillation data is sufficient to significantly improve the performance, whereas we do not further observe a significant improvement when scaling up the training data by 10x.



\paragraph{LoRA fine-tuning without performance degradation.} We next investigate the extent to which distilling Long CoT reasoning is knowledge-intensive. In addition to the results using 17k R1 samples as demonstrated in \cref{fig:intro-performance}, we also report the results for both SFT and LoRA fine-tuning with 7k and 17k QwQ samples in \cref{tab:sft-lora-qwq}.

Prior work~\citep{ghosh2024limitations, biderman2024lora} suggests that LoRA fine-tuning substantially under-performs full fine-tuning for knowledge-intensive tasks, and is limited to learning response initiation and style tokens. However, our results in \cref{fig:intro-performance} and \cref{tab:sft-lora-qwq} show that LoRA fine-tuned models achieve similar or even superior reasoning performance compared to full-parameter SFT across math and coding benchmarks. 
Additionally, we find that a model fine-tuned with LoRA using just 7k QwQ samples performs comparably to one trained on 17k QwQ-distilled samples. This demonstrates that reasoning distillation can be achieved efficiently with both minimal parameter updates and minimal data. 
As shown in \cref{fig:response_demo}, the LoRA fine-tuned model easily learns to generate Long CoT responses with reflection and self-verification. These observations suggest that Long CoT reasoning ability may not rely on deep knowledge acquisition but rather on learning structured reasoning patterns, which can be effectively distilled in a parameter-efficient manner. This also aligns with prior findings that methods such as Chain-of-Thought prompting elicit Short CoT reasoning primarily by shaping response structure rather than instilling deep factual knowledge~\citep{wei2022cot, yao2023tree_of_thoughts}. 

\begin{table}[ht!]
    \setlength{\tabcolsep}{4pt}
    \centering
    \caption{\textbf{Model accuracy with SFT and LoRA (rank=64).} Fine-tuning performed on Qwen2.5-32B-Instruct with QwQ samples. ``Olympiad.'' is short for ``OlympiadBench'', ``LCB.'' is short for ``LiveCodeBench''. We find that the learning process of Long CoT can be parameter efficient.}
    \label{tab:sft-lora-qwq}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        & \textbf{MATH500} & \textbf{AIME24} & \textbf{AMC23} & \textbf{Olympiad.} & \textbf{LCB.} \\
        \midrule
        Qwen2.5-32B-Inst. & 84.8 & 16.7 & 67.5 & 47.6 & 48.9 \\
        QwQ & 90.4 & 33.3 & 75.0 & 58.1 & 59.1 \\ 
        o1-preview & 85.5 & 44.6 & 87.5 & 59.2 & 59.1 \\
        \midrule
        \rowcolor{black!10} \multicolumn{6}{l}{\textbf{7k QwQ Samples}} \\
        SFT & 87.8 & 33.3 & 77.5 & 57.3 & 57.5 \\
        LoRA (r=64)   & 86.6 & 40.0 & 77.5 & 57.2 & 56.6 \\
        \midrule
        \rowcolor{black!10} \multicolumn{6}{l}{\textbf{17k QwQ Samples}} \\
        SFT & 87.8 & 33.3 & 70.0 & 56.7 & 57.9 \\
        LoRA (r=64)   & 86.6 & 33.3 & 90.0 & 56.0 & 56.2 \\
        % SFT-R1-17k & 90.8 & 56.7 & 85.0 & 60.3 & 57.0 \\
        % Qwen-32B-LoRA-R1-17k   & 90.6 & 50.0 & 82.5 & 56.0 & 55.2 \\
        
        \bottomrule
        
    \end{tabular}
    }
\end{table}

% This also aligns with prior findings that eliciting short CoT with methods such as Chain-of-Thought prompting primarily shape response structure rather than provide deep factual knowledge \cite{wei2022cot, yao2023tree_of_thoughts}. 

% eliciting short CoT with methods such as Chain-of-Thought prompting primarily shape response structure rather than provide deep factual knowledge \cite{wei2022cot, yao2023tree_of_thoughts}. 

% These findings collectively indicate that LongCoT reasoning ability is more aligned with structured response generation, making it well-suited for extreme data-efficient and parameter-efficient fine-tuning techniques like LoRA.

% \begin{table}[ht!]
%     \centering
%     \caption{Performance of different models on non-reasoning tasks.}
%     \label{tab:lora-non-reasoning}
%     \footnotesize % Adjust text size
%     \setlength{\tabcolsep}{3.5pt} % Adjust column spacing
%     \resizebox{0.45\textwidth}{!}{
%     \begin{tabular}{lcccc}
%         \toprule
%         & MMLU & ARC-C & IEval  & MGSM  \\
%         \midrule
%         Qwen2.5-32B-Inst. & 74.1 & 49.4 & 78.7 & 42.3 \\
%         QwQ & 71.2 & 49.7  & 42.5 & 19.1 \\ 
%         \midrule
%         Qwen-32B-LoRA-QwQ-7k & 79.1 & 47.4 & 75.4 & 31.1\\
%         SFT-QwQ-7k & 79.8 & 48.6 & 70.6 & 30.1 \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}

\section{Long CoT: Structure Is The Key}
\label{sec:key}
% Inspired by the fact that a small number of samples can significantly improve model performance with SFT or LoRA fine-tuning, we explore the key factors in this learning process. In particular, we identify two dimensions of factors that possibly provide learning signals: 
Motivated by the observation that fine-tuning with a small number of samples can significantly enhance model reasoning performance, we investigate the key factors driving this improvement. Specifically, we explore the contributions of two dimensions to the learning process:
\begin{enumerate}
    \item \textbf{The local content within a reasoning step}, including the correctness of the final answer, numbers in math derivations, and the use of reasoning keywords.
    \item \textbf{The global reasoning structure}, including reflection, self-validation, and backtracking across multiple reasoning steps to form a logically coherent long CoT.
\end{enumerate}

To understand their impact, we conduct two studies: (1) we perturb the content within individual reasoning steps -- such as the final answer, numerical digits, and reasoning keywords(\cref{sec:traces_change_contents}), and (2) we modify the global reasoning structure by inserting, deleting, and shuffling reasoning steps(\cref{sec:traces_change_structure}). We compare the performance of models trained on perturbed samples against both the base Qwen2.5-32B-Instruct model (i.e., Original) and model trained on correct, unperturbed samples (i.e., Correct), as shown in~\cref{tab:perturbations}. Our findings show that \textbf{the learning process is highly sensitive to modifications in the global reasoning structure, but remarkably tolerant to errors in the local contents.} 

% \dacheng{Make sure we have talked about how these 17k, 7k, 5k is curated.}
\paragraph{Experiment setup} In this section, we use QwQ-32B-Preview to produce the distillation data and select a subset of 4618 correct responses as the training set (out of the 12k math data in~\cref{sec:distill}). All perturbations in this section are performed on this dataset unless otherwise stated. We train models on each separate variant of the dataset with the same hyperparameters as in~\cref{sec:distill} and report performance in~\cref{tab:perturbations}.

\subsection{Wrong or Corrupted Local Content}
\label{sec:traces_change_contents}
To study the importance of local content within individual steps, we preserve the overall reasoning structure while systematically perturbing the local content in training samples with different approaches. 

% use a set of 4618 math problems to study the effectiveness of each dimension, where results of training on the corr

% : choosing wrong answer samples, corrupting numerical digits, and removing reflection and reasoning keywords. 

% (1) using training samples that lead to incorrect final answers, (2) corrupting numerical digits in mathematical derivations, and (3) removing reflection or reasoning-related keywords
% choosing wrong answer samples, corrupting numerical digits, and removing reasoning keywords.

% We start with traces where only the final answers are wrong. Then we artificially perturb the local contents in every step of the process, by changing the digits to random numbers, or removing reasoning keywords such as "However", "But wait".  \joey{Aren't these keywords what defines the overall structure as well?  }

\paragraph{Wrong Answer Samples.} 
% \tyler{Consider moving this to the end, or pull out earlier.} % \dacheng{Wait update on sec3 on data.}
During our training data curation process in~\cref{sec:distill}, we only include samples that yield correct final answers. To assess whether correctness of the final answer is necessary for learning reasoning patterns, we instead train the model using an equivalent number of samples (4.6k) that lead to the \textit{wrong} answer. % 4618
Surprisingly, we find that training the base model \textbf{without any samples that reach a correct final answer} still achieves an average accuracy of 63.1\% across benchmarks, only 3.2\% lower than training with entirely correct samples. 
% During our training data curation process in~\autoref{sec:distill}, we only include samples that lead to the correct answers. In this experiment, we instead select an equivalent number of samples (4618) that lead to the \textit{wrong} answer. Surprisingly, we find that training the base model \textbf{without any samples that reach a correct final answer} still achieves an average accuracy of 63.1\% across benchmarks, a mere 3.2\% degradation from training with all correct samples. \shu{I'm not very clear on which exact configuration you are referring to as the completely correct traces above, can you call it out more directly? i.e. from Table 2, which data sizes?} 

\paragraph{Digits Corrupted Samples.} 
% Building on the findings from training with samples of incorrect final answers, we next investigate the importance of correctness at intermediate steps. 
Building on the previous experiment, we next examine the role of correctness in the intermediate reasoning steps.
To evaluate this, we corrupt correct samples by replacing each digit with a random number between 0 and 9. Note that this is a severe corruption that can lead to nonsensical statements such as ``1+1=3''. Surprisingly, even when 70\% of the digits are corrupted, the model still maintains an average performance of 62\%, only 4.3\% below the correct sample baseline, demonstrating robustness to incorrect content. However, when all digits are corrupted, the average performance plunges to 2.7\%. 

% \paragraph{Reflection keywords removal.}
% In particular, we use GPT-4o-mini to detect sentences that contain these keywords (full list of keywords in~\S\ref{sec:keywords}), and randomly drops them. We find that even when all such keywords are dropped, the model still achieves 63.0\% average performance, which is within 3\% of the correct baseline. This suggests that these keywords themselves are not important. 

\textbf{Reasoning Keyword Removal.}
Given the prevalence of reasoning \textit{keywords} in responses from LRMs (e.g., 'wait', 'let me think again', 'but'), one theory is that these specific phrases may invoke the reflection and back-tracking necessary to elicit strong reasoning performance. 
To evaluate it, we use GPT-4o-mini to identify sentences with occurrences of these reasoning keywords and randomly remove a fraction of them (e.g., 20\%, 50\%, 100\%). Our results show that even after removing all (100\%)    
  such keywords, the model still achieves an average accuracy of 63\%, which is within 3.3\% of accuracy from the model trained with correct samples. This suggests that these particular keywords do not fundamentally impact the model reasoning performance. 

\textbf{Conclusion.} We find that errors in local content -- such as incorrect mathematical derivations or missing reasoning keywords -- have minimal impact on overall performance. 
% We find that local content with reasoning steps, such as the presence of incorrect math derivations and the absence of reasoning keywords, has minimal impact on downstream reasoning performance. 

\begin{table}[t]
    \centering
    \caption{\textbf{Effect of trace perturbations on reasoning performance~\cref{sec:key}.} 
    All models are trained with base Qwen2.5-32B-Instruct. ``Olympiad.'' is short for ``OlympiadBench''. In particular, we study (1) traces with modified reasoning step contents: wrong answers, corrupted digits, and removed reasoning keywords, and (2) traces with modified structure: deleted, inserted, or shuffled steps. 
    \textbf{We find that structural perturbations are far more detrimental to model accuracy than content perturbations.}}
    \label{tab:perturbations}
    \footnotesize % Adjust text size
    \setlength{\tabcolsep}{3pt} % Adjust column 
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        & \textbf{MATH500} & \textbf{AIME24} & \textbf{AMC23} & \textbf{Olympiad.} & \textbf{Avg.} \\
        \midrule
        \rowcolor{black!10} \multicolumn{6}{l}{\textbf{Baselines}} \\
        Original & 84.8 & 16.7 & 67.5 & 47.6 & 56.7 \\
        Correct & 89.2 & 40.0 & 77.5 & 58.5 & 66.3 \\
        \midrule
        \rowcolor{black!10} \multicolumn{6}{l}{\textbf{Content Modifications}} \\
        \midrule
        \textbf{Wrong Answers} & 88.6 & 30.0 & 77.5 & 56.1 & 63.1 \\
        \midrule
        \textbf{Corrupted Digits} \\
        100\%  & 5.4  & 0.0  & 2.5  & 2.8  & 2.7 \\
        70\%   & 85.6 & 30.0 & 77.5 & 54.8 & 62.0 \\
        50\%   & 87.6 & 36.7 & 77.5 & 55.0 & 64.2 \\
        20\%   & 88.4 & 30.0 & 82.5 & 57.2 & 64.5 \\
        \midrule
        \textbf{Removed keywords} \\
        100\%    & 86.6 & 33.3 & 77.5 & 54.4 & 63.0 \\
        50\%     & 87.6 & 36.7 & 82.5 & 56.7 & 65.9 \\
        20\%     & 87.2 & 33.3 & 72.5 & 56.1 & 62.3 \\
        \midrule
        \rowcolor{black!10} \multicolumn{6}{l}{\textbf{Structure Modifications}} \\
        \midrule
        \textbf{Shuffled Steps} \\
        100\%    & 81.8 & 23.3 & 70.0 & 49.1 & 56.1 \\
        67\%     & 82.0 & 26.7 & 72.5 & 47.6 & 57.2 \\
        33\%     & 85.6 & 33.3 & 75.0 & 55.3 & 62.3 \\
        \midrule
        \textbf{Deleted Steps} \\
        100\%    & 79.2 & 13.3 & 60.0 & 45.4 & 49.5 \\
        67\%     & 84.2 & 26.7 & 55.0 & 48.1 & 53.5 \\
        33\%     & 88.2 & 23.3 & 80.0 & 57.7 & 62.3 \\
        \midrule
        \textbf{Inserted Steps} \\
        100\%    & 77.0 & 10.0 & 50.0 & 41.1 & 44.5 \\
        67\%     & 81.8 & 20.0 & 60.0 & 46.0 & 52.0 \\
        33\%     & 86.6 & 33.3 & 77.5 & 57.2 & 63.7 \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-5mm}
\end{table}

\begin{figure*}[htbp]
\centering
\includegraphics[width=.85\textwidth]{figures/step_modifications.pdf}
\caption{\textbf{Reasoning step modifications.} To evaluate perturbations to global structure across reasoning steps, we perform three modifications: deletion, insertion, and shuffling. These modifications break logical consistency across steps and degrade model accuracy far more than changes to local content within reasoning steps.}
\label{fig:step_modifications}
\vspace{-4mm}
\end{figure*}

\subsection{Corrupted Global Reasoning Structure}
\label{sec:traces_change_structure}
Next, we examine the importance of reasoning \textit{structure} by performing three modifications to the reasoning traces: deletion, insertion, and shuffle. We first note that our system prompt (\cref{sec:system_prompt}) instructs the model to generate responses with thoughts enclosed in the tags `begin\_of\_thought' and `end\_of\_thought' and the final solution and step-by-step explanation in `begin\_of\_solution' and `end\_of\_solution'. All modifications are performed on the \textit{thoughts}, while the solution block is left unmodified.

We use Llama-3.3-70B-Instruct~\citep{dubey2024llama} to separate each reasoning trace into distinct reasoning steps, with boundaries determined by occurrences of backtracking, self-validation, reflection, or other breaks from a linear sequence of thoughts. We then generated nine modified variants of the dataset by applying each modification (insertion, deletion, and shuffle -- illustrated in~\cref{fig:step_modifications}) to 33\%, 67\%, or 100\% of reasoning steps in the 4,618 correct traces. Each variant is used to train the base model, Qwen2.5-32B-Instruct, and we report the resulting performance in \cref{tab:perturbations} and response lengths and reasoning keyword counts in~\cref{sec:lens_and_keyword_counts}.

\paragraph{Deleted reasoning steps.} As reasoning steps are increasingly deleted from the training data, model accuracy steadily declines and eventually regresses to the base model performance. Notably, retaining only the final solution and extensive step-by-step explanation (i.e., 100\% deletion case) does not suffice to learn strong reasoning capabilities. This suggests that correct long CoT demonstrations alone are insufficient. Instead, examples of handling errors and dead ends with backtracking, reflection, and self-validation are important for eliciting robust reasoning.

At 67\% deletion, the model imitates reasoning keywords (relative to the base model, keyword usage increases 45$\times$, and output token increases 9$\times$), but its accuracy does not improve accordingly. Consistent with ~\cref{sec:traces_change_contents}, this validates that merely adopting reasoning keywords and long responses is insufficient. We note, however, that as more steps are deleted, the response lengths also decrease significantly, which could contribute to reduced accuracy. We hypothesize that it is the breaking of logical consistency \textit{between} steps that causes accuracy degradation and validate this further in the following analysis. 

\paragraph{Inserted reasoning steps.} To further validate the importance of logical structure, we replace a subset of each trace's reasoning steps with a random sample of reasoning steps from other samples in the training set that lead to correct results. Unlike deletion, this approach generally preserves the original length of the reasoning trace, ensuring that accuracy degradation is not due simply to producing fewer steps. Relative to model variants trained with deleted reasoning steps, variants trained on inserted steps generate longer responses with more reasoning keywords, yet accuracy nonetheless deteriorates to, and even below, the level of the base model.

Interestingly, each inserted step is itself coherent and originates from a correct reasoning trace in the training data. Yet these internally-coherent steps appear in sequences that lack logical consistency and often from a separate domain (e.g., a combinatorics step may be inserted into a geometry solution), leading to contradictions and disjointed reflections. For instance, the model trained with inserted reasoning steps frequently references earlier steps that do not exist (e.g., {Alternatively, consider a different approach without specifying the prior approach) or enumerates edge cases in an inconsistent order (e.g., declaring a Case 2 without Case 1). 

While the model readily produces coherent individual steps that reflect on a problem, the CoT fails to exhibit continuity \textit{across} reasoning steps. 
This aligns with the observations in the deletion setting: a mere increase in reasoning steps or keywords is insufficient for robust reasoninglogical consistency across steps is a critical factor.

\paragraph{Shuffled reasoning steps.} We next examine whether preserving the domain of each reasoning step, eliminating potential cross-domain confusion, but randomizing their order likewise impacts the models ability to reason. 

As the amount of shuffling increases, response length and reasoning keyword usage remain high, and in fact exceed the model trained on correctly ordered traces, yet accuracy declines sharply. Similar to the insertion experiments, the model imitates the syntax of per-step reasoning but loses logical consistency across steps. For instance, we find that over 92\% of model responses begin with a backtracking or self-validation keyword (e.g., \emph{``Alternatively,'' or ``Wait''}), even though there is no preceding content to correct or reconsider. The model also references prior calculations or cases that were never actually introduced in any preceding step. Thus, while the shuffled traces still contain valid domain-specific reasoning steps, their rearrangement leads to incoherent overall solutions. In other words, domain alignment alone does not prevent logical breakdown.

\paragraph{Conclusion.} Taken together, these findings show that providing error-free CoT demonstrations, increasing response lengths, imitating reasoning keywords and correct short CoT within individual steps, and preserving domain relevance for each step are \textit{not} sufficient to produce effective reasoning %\shu{Not 100\% sure how to draw this conclusion}. 
Further, our experiments on incorrect traces (\cref{sec:traces_change_contents}) demonstrate that learning reasoning capability is largely robust to local inaccuracies or miscalculations. Instead, global structural consistency is essential to elicit coherent long CoTs with the reflection, revision, and validation behaviors that produce strong reasoning performance. 

% \section{Low rank adaptation is enough}
% \begin{table*}[h]
%     \centering
%     \caption{LoRA with QwQ and R1 Traces}
%     \label{tab:lora-qwq-r1}
%     \small
%     \begin{tabular}{lccccccc}
%         \toprule
%         & Math-500 & AIME 2024 & AMC 23 & OlympiadBench & LCB-Easy & LCB-Medium & LCB-Hard \\
%         \midrule
%         \textbf{Baselines} \\
%         % Qwen2.5-32B-Instruct & 81.4 & 16.7 & 46.5 & 47.6 & 84.6 & 40.8 & 9.8\\

%         Qwen2.5-32B-Instruct & 83.8 & 20.0 & 67.5 & 46.7 & 84.6 & 40.8 & 9.8\\
        
%         % QwQ & 91.6 & 43.3	& 90.0	& 61.0 & 90.7 & 56.3 & 17.1\\ 
%         QwQ & 90.4 & 33.3 & 75.0 & 58.1 & 90.7 & 56.3 & 17.1\\ 
        
%         o1-preview & 85.5 & 44.6 & 87.5 & 59.2 & 92.9 & 54.9 & 16.3\\
%         \midrule
        
%         \textbf{LoRA (R1 Traces)} \\
%         SFT-R1-17k & 90.8 & 56.7 & 85.0 & 60.3 & 86.8 & 57.8 & 11.4 \\
        
%         Qwen-32B-LoRA-R1-17k   & 90.6 & 50.0 & 82.5 & 56.0 & 88.5 & 50.0 & 14.6 \\
%         Qwen-32B-LoRA-R1-7k & 83.4 & 36.7 & 77.5 & 55.3 & 87.9 & 47.6 & 15.4\\
%         \bottomrule
%         \textbf{LoRA (QwQ Traces)} \\
%         SFT-QwQ-17k & 87.8 & 33.3 & 70.0 & 56.7 & 86.3 & 56.8 & 17.9 \\
        
%         Qwen-32B-LoRA-QwQ-17k   & 86.6 & 33.3 & 90.0 & 56.0 & 84.1 & 54.9 & 17.1 \\
%         Qwen-32B-LoRA-QwQ-7k     & 86.6 & 40.0 & 77.5 & 57.2 & 84.1 & 54.9 & 18.7\\
%         \midrule 
        
%     \end{tabular}
% \end{table*}

% \begin{table}[h]
%     \centering
%     \caption{LoRA on Gemma2-27B-It \shu{RERUN NOW}}
%     \label{tab:lora-gemma}
%     \resizebox{0.5\textwidth}{!}{
%     \begin{tabular}{lcc}
%         \toprule
%         Benchmark & Gemma2-27B-It-Base & Gemma2-27B-It-LoRA-QwQ-7k \\
%         \midrule
%         Math-500        & 58.2  & 66.6 (+8.4\%)\\
%         AIME 2024       & 6.7    & 10.0 (+3.3\%)  \\
%         AMC 23          & 27.5 & 42.5 (+15\%) \\
%         OlympiadBench  & 20.0 &  31.1 (+8.9\%) \\
%         LCB-Easy        & 63.2   & 72.5 (+9.3\%)  \\
%         LCB-Medium      & 15   & 14.6  \\
%         LCB-Hard        & 2.4   & 4.1 (+1.7\%)  \\
%         \bottomrule
%     \end{tabular}}
% \end{table}
% \begin{tcolorbox}[colback=gray!30, colframe=gray]
% Writing PIC: Shiyi, Shu \newline
% Experiment PIC: Shu
% \end{tcolorbox}

% @shiyi Lora is working well, indicating that reasoning capabilities is not something newly learned, but rather something "activated".

% To further confirm our conjecture that the model learns some form of formatting, we apply low-rank adaptation to the base model.\dacheng{add cite to papers that state lora learns format}


% \begin{enumerate}
%     \item QwQ 7k + 17k results
%     \begin{enumerate}
%         \item reasoning tasks
%         \item non-reasoning tasks
%     \end{enumerate}
%     \item R1 17k results (optional 7k results)
% \end{enumerate}



% Our experiment demonstrates that reasoning capabilities can be effectively added to instruction-following base models through parameter-efficient fine-tuning methods such as LoRA. 
% We apply LoRA to fine-tune Qwen2.5-32B-Instruct with 17k reasoning samples from QwQ and DeepSeek-R1, each with a mix of around 12k math problems and 5k coding problems. Results across seven benchmarks on math and coding domains are shown in Table~\ref{tab:lora-qwq-r1}. 

% \paragraph{Parameter efficiency} 
% With 17k reasoning samples from QwQ, LoRA fine-tuning (i.e., Qwen-32B-LoRA-QwQ-17k) significantly boosts the reasoning performance of Qwen2.5-32B-Instruct by 5.2\% -- 43.5\% on math problems and 0 -- 14.1\% on coding problems from different difficulty levels.  

% Fine-tuning with LoRA also achieves comparable results within 0.7 - 2.3\% accuracy to full supervised fine-tuning (SFT-QwQ-17k), while maintaining a much smaller parameter footprint. We note that there is a variance on the AMC23 benchmark, where Qwen-32B-LoRA-QwQ-17k achieves \shu{this might be because ...}

% Our results generalize to R1 traces. With 17k reasoning samples from R1, LoRA fine-tuning (i.e., Qwen-32B-LoRA-R1-17k) boosts the reasoning performance of Qwen2.5-32B-Instruct by 6.2 -- 30\% on math benchmarks and 3.9\% to 9.2\% on coding benchmarks. This is comparable to the full supervised fine-tuning (SFT-R1-17k) model within 7.8\%.  

% \paragraph{Data efficiency} We further show LoRA fine-tuning remains effective even with significantly reduced training samples. When decreasing the number of problems from 17k to 5k,  the fine-tuned model (Qwen-32B-LoRA-QwQ-7k) maintains strong performance across benchmarks. This highlights that LoRA not only enables parameter-efficient fine-tuning but also benefits from data efficiency, suggesting that high-quality reasoning capabilities can be learned with relatively small datasets.



% \subsection{Effectiveness on different models}
% \shu{This can be moved to ablation or just remove}
% To assess the generalizability of LoRA-based method for reasoning, we extend our experiments to another base model: Gemma2-27B-It-Base. The results in Table~\ref{tab:lora-gemma} demonstrate that LoRA fine-tuning consistently improves reasoning performance across benchmarks for different base models, though the degree of improvements varies depending on the base model's initial ability. 

% On benchmarks like Math-500, OlympiadBench, and LCB-Easy, LoRA fine-tuning significantly enhances performance, achieving up to 15\% improvements on math benchmarks and up to 9.3\% on coding benchmarks. This suggests that even for models with little reasoning ability, LoRA adaptation can effectively activate and enhance reasoning skills. On some of the benchmark like AMC 23, the performance remains unchanged. 


% \subsection{Effect of different rank sizes}
% We explore the impact of rank sizes  on LoRA perforamnce in our experiments. Results in Table~\ref{} indicate that 
% \begin{enumerate}
%     \item QwQ: r=64 works 
%     \item R1: r=64 not sufficient, r=256 
% \end{enumerate}



%\begin{table}[h]
%    \centering
%    \caption{Performance of Different Data Sizes for QwQ and R1}
%    \resizebox{0.45\textwidth}{!}{
%    \begin{tabular}{lccccc}
%        \toprule
%        & Math-500 & AIME 2024 & AMC 23 & OlympiadBench & Average \\
%        \midrule
%        \textbf{QwQ Data} \\
%        200  & -    & 20.0 & 17.5  & -    & -    \\
%        1k   & -    & -    & -     & -    & -    \\
%        3k   & 87.6 & 33.3 & 87.5  & 57.0 & 66.35 \\
%        5k   & 89.2 & 40.0 & 77.5  & 58.5 & 66.3  \\
%        15k  & 89.0 & 30.0 & 80.0  & 57.5 & 64.125 \\
%        30k  & 89.2 & 46.7 & 77.5  & 57.3 & 67.675 \\
%        \midrule
%        \textbf{R1 Data Generation} \\
%        3k   & -    & -    & -     & -    & -    \\
%        5k   & -    & -    & -     & -    & -    \\
%        15k  & -    & -    & -     & -    & -    \\
%        30k  & -    & -    & -     & -    & -    \\
%        \bottomrule
%    \end{tabular}}
%\end{table}

\section{Ablation Study}
In this section, we conduct a series of ablation studies to answer the following questions:
\begin{enumerate}
    \item (\cref{sec:ablation_non_reasoning}) Does fine-tuning on Long CoT data lead to degraded performance on non-reasoning tasks?
    \item (\cref{sec:ablation_student}) How much does the Long CoT fine-tuning enhance the performance of different student models?
    \item (\cref{sec:ablation_test_time}) How does Long CoT model performance compare to the Best-of-N sampling performance of the base model?
    \item (\cref{sec:ablation_short}) How does Long CoT fine-tuning compare to Short CoT fine-tuning with the same dataset?
\end{enumerate}

% We conduct a series of ablation studies to measure model performance on non-reasoning benchmarks and comparison to Best-of-N setup, and effect of different student models. \shu{TODO: connect these three sub-parts together} 
\subsection{Performance on Non-Reasoning Benchmarks}
\label{sec:ablation_non_reasoning}

\begin{table}[ht!]
    \centering
    \caption{\textbf{Distilled Model Performance on Non-Reasoning Tasks.} The teacher model is  QwQ-32B-Preview, and the student model is Qwen2.5-32B-Instruct. Compared to QwQ, distilled models retain most of the base model's capabilities.}
    \label{tab:lora-non-reasoning}
    \footnotesize % Adjust text size
    \setlength{\tabcolsep}{6pt} % Adjust column spacing
    \resizebox{0.46\textwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{MMLU} & \textbf{ARC-C} & \textbf{IEval}  & \textbf{MGSM}  \\
        \midrule
        % \rowcolor{black!10} \multicolumn{6}{l}{\textbf{Structure Modifications}} \\
        Qwen2.5-32B-Inst. & 74.1 & 49.4 & 78.7 & 42.3 \\
        QwQ & 71.2 & 49.7  & 42.5 & 19.1 \\ 
        \midrule
        \rowcolor{black!10} \multicolumn{5}{l}{ \textbf{17k R1 Samples}} \\
        SFT & 73.0 & 49.0 & 77.8 & 33.7 \\
        LoRA (r=256)   & 75.5  & 47.3 & 78.4 & 38.7  \\
        \midrule
        % \textbf{SFT \& LoRA (R1-7k)} \\
        % Qwen-32B-LoRA-R1-7k & \\
        % SFT-R1-7k  \\
        % \midrule
        \rowcolor{black!10} \multicolumn{5}{l}{ \textbf{17k QwQ Samples}} \\
        SFT & 78.4 & 49.5 & 75.8 & 33.0  \\
        LoRA (r=64)   &  78.5 & 46.7 & 74.1 & 30.6\\
        \midrule
        \rowcolor{black!10} \multicolumn{5}{l}{ \textbf{7k QwQ Samples}} \\
        SFT & 79.8 & 48.6 & 70.6 & 30.1 \\
        LoRA (r=64) & 79.1 & 47.4 & 75.4 & 31.1\\
        \bottomrule
    \end{tabular}
    }
\end{table}

While simple distillation enhances reasoning capabilities, it is essential to ensure that these improvements do not come at the cost of catastrophic forgetting or a decline in general language understanding and instruction-following abilities, which are crucial for broader task generalization. 

To assess this, we evaluate the performance of our SFT and LoRA fine-tuned models mentioned in~\cref{sec:distill} on a diverse set of benchmarks: MMLU (multi-task language understanding), ARC-C (science exam question), IEval (instruction-following), and MGSM (multilingual grade-school math problems)~\citep{hendrycks2021measuring,clark2018think,mitchell2023ieval,cobbe2021training}. 

As shown in~\cref{tab:lora-non-reasoning}, the base instruction model (Qwen2.5-32B-Instruct) performs well in all these tasks. The QwQ model, despite its strong reasoning capabilities, suffers significant degradation in 
instruction-following (i.e., 42.5\% on IEval) and multilingual tasks (i.e., 19.1\% on MGSM). 
% This highlights a potential issue \shu{is it?} where intensive reasoning-focused training can lead to catastrophic forgetting of general capabilities.
In contrast, fine-tuning (through both SFT and LoRA) only on a small amount of Long CoT reasoning data from R1 or QwQ allows the distilled models to retain most of the base instruction models capabilities, avoiding the drastic performance drop seen in QwQ.
% In contrast, our distilled models fine-tuned on a small amount of R1 or QwQ reasoning data can largely preserve these general abilities from the base model. LoRA fine-tuning also balances both general and reasoning capabilities effectively. 
% Both LoRA-trained models (Qwen-32B-LoRA-R1-17k and Qwen-32B-LoRA-QwQ-17k) maintain strong performance on MMLU, ARC-C, and IEval, with minimal degradation compared to the base instruction model. This suggests that LoRA efficiently integrates reasoning capabilities without compromising the model's pre-existing general knowledge. 

% We investigate whether enhancing the reasoning capabilities with our approach comes at the expense of general language understanding and instruction-follow capabilities.




% \begin{table}[h]
%     \centering
%     \caption{Performance with various data sizes as the distillation training data. The teacher model is QwQ-32B-Preview, and the student model is Qwen-32B-Instruct. }
%     \label{tab:qwq_performance}
%     \resizebox{0.5\textwidth}{!}{
%     \begin{tabular}{lccccc}
%         \toprule
%         \textbf{Data size} & \textbf{Math-500} & \textbf{AIME 2024} & \textbf{AMC 23} & \textbf{OlympiadBench} & \textbf{Avg.} \\
%         \midrule
%         200   & 23.0 & 20.0 & 17.5 & 16.7 & 19.3 \\
%         1k    & 80.2 & 20.0 & 75.0 & 51.7 & 56.7 \\
%         3k    & 87.6 & 33.3 & 87.5 & 57.0 & 66.4 \\
%         5k    & 89.2 & 40.0 & 77.5 & 58.5 & 66.3 \\
%         15k   & 89.0 & 30.0 & 80.0 & 57.5 & 64.1 \\
%         30k   & 89.2 & 46.7 & 77.5 & 57.3 & \textbf{67.7} \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:data_scale}
% \end{table}

% \subsection{Effect of distillation data size}\shiyi{moved to sec3}
% We further vary the amount of distillation data with QwQ-32B-Preview model to study whether further scaling the data helps, and whether there could be a smaller amount of data that achieves the same performance. The results are shown in Table~\ref{tab:data_scale}. We find that 3k distillation data is sufficient to significantly improves the performance, where we do not further observe a significant improvement when scaled up the training data by 10x.
% \begin{tcolorbox}[colback=gray!30, colframe=gray]
% PIC: Dacheng \newline
% cc: Shiyi
% \end{tcolorbox}
% \subsection{Comparison between teacher models}
% \begin{tcolorbox}[colback=gray!30, colframe=gray]
% PIC: Shu \newline
% cc: Dacheng (Running data scaling with R1 in parallel), Shiyi
% \end{tcolorbox}
% \begin{table}[h]
%     \centering
%     \caption{(Different Teacher Models)}
%     \label{tab:teacher}
%     \scriptsize % Reduce font size slightly
%     \resizebox{0.48\textwidth}{!}{ % Adjust to fit within a single column
%     \begin{tabular}{lcccccc}
%         \toprule
%         & Math-500 & AIME  & AMC & Olympiad & Minerva & Average \\
%         \midrule
%         \textbf{Base Model} \\ 
%         Qwen2.5-32B-Inst & 84.8 & 16.7 & 46.5  &  47.6 & 41.5 & 47.4 \\
%         \midrule

%         \textbf{17k SFT } \\
%         SFT-R1-17k & 90.8 & 56.7 & 85.0 & 60.3 & 42.3 & 67.0 \\
%         SFT-QwQ-17k & 87.8 & 33.3 & 70.0 & 56.7  & 35.7 & 56.7 \\
        
%         \midrule
%         \textbf{17k LoRA} \\
%         LoRA-R1-17k & 90.6 & 50.0 & 82.5 & 56.0 & 43.0 & 64.4 \\
%         LoRA-QwQ-17k & 86.6 & 33.3 & 90.0 & 56.0 & 36.0 & 60.4 \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}

% We investigate the impact of different teacher models by comparing fine-tuning results using QwQ and R1 as the source of the reasoning samples. The results in ~\cref{tab:teacher} demonstrate that fine-tuned with R1 teacher model consistently leads to stronger reasoning performance across SFT and LoRA adaptation than QwQ teacher model. 

% For SFT, models fine-tuned with R1 traces (SFT-R1-17k) achieve an average accuracy of 67.0\% across five different math benchmarks, outperforming SFT-QwQ-17k by 10.3\%. The most significant improvements appear in AIME (56.7\% vs. 33.3\%) and AMC (85.0\% vs. 70.0\%), suggesting that R1 provides richer reasoning traces for challenging mathematical problems. Similarly, for LoRA, the model fine-tuned with R1 traces (LoRA-R1-17k) achieves an average accuracy of 64.4\%, surpassing LoRA-QwQ-17k (60.4\%) by 4.0\%. These results indicate that the choice of the teacher model plays a crucial role in the effectiveness of different fine-tuning strategies, with LoRA and SFT benefiting differently from teacher model quality. 

% We experiment with R1-Distill-Qwen-32B as a teacher model.

% We identify three potential factors that contribute to the learning procedure
%\begin{enumerate}
%    \item The reflection components in the traces. @dacheng
%    \item The teacher capability of the traces. @shiyi
%    \item The backtracking components in the trace. \dacheng{Unknown whether todo}
%    \item Existance of the longer CoT \dacheng{Is this worth doing? would this be too obvious?} - contrast with a setting that only distills on the final short cot.
%\end{enumerate}

\subsection{Effect on Different Student Models}
\label{sec:ablation_student}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/others.pdf}
    \caption{
    \textbf{Generalization to other models.} Accuracy for models of different sizes and architectures without SFT (green) and with SFT (blue). Most models show significant improvements when fine-tuned with 17k samples from R1-Preview, showing that the Long CoT fine-tuning is beneficial across models.} % \shiyi{add value annotations for each bar}} %\dacheng{Want to re-confirm whether the 7B-math and qwen-14b results are correct.}}\label{fig:other_pdf} % Optional: Add a label for referencing
    \label{fig:other_models}
\end{figure}

% In this section, we examine whether Long CoT can improve fine-tuning for all models.
In this section, we examine whether Long CoT reasoning capabilities can be elicited with different student models via fine-tuning (as described in~\cref{sec:distill}). 
Specifically, we train with the 17k samples on Qwen2.5-7B-Math, Qwen2.5-7-Base, Qwen2.5-7B-Instruct, Llama-3.1-8B, Qwen2.5-14B-Base, Qwen2.5-14B-Instruct, Gemma2-27B-it and Qwen2.5-32B-Instruct~\citep{yang2024qwen2, dubey2024llama, team2024gemma}. 
We find that seven out of eight models improve noticeably across multiple benchmarks, showing the effect of Long CoT as a general improvement across models.
However, not all models have showed the same degree of improvements as for Qwen2.5-32B-Instuct.
%However, some improvements are not as significant as those for Qwen2.5-32B-Instruct (\cref{fig:intro-performance}), and none can approach the performance of QwQ-32B-Preview. 
%(1) not all models demonstrate the same degree of improvement as Qwen-2.5-32B-Instruct, and (2) not all models achieve equivalent performance to the teacher QwQ-32B-Preview model. 
These findings suggest promising future directions for understanding the performance upper bound and data efficiency with various teacher and student models in the space of reasoning.

% \tyler{Is there any more insight or explanation we can provide here?}

% observe improvements except models except Qwen2.5-14B-Instruct model, where Qwen-2.5-32B-Instruct observes the most significant 

%\begin{tcolorbox}%[colback=gray!30, colframe=gray]
%PIC: Dacheng
%\end{tcolorbox}
% \dacheng{@Matei: Try more epochs, is this "every base model can be taught to model?"}

\subsection{Comparison to Best-of-N} 
\label{sec:ablation_test_time}
% Will this simple imitation exceed Best-of-N? 
% \dacheng{@Anyscale is running this}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/test_time_r1.pdf}
    \caption{\textbf{SFT with Long CoT vs Best-of-N.} Accuracy of Qwen2.5-32B-Instruct before SFT (Zero-Shot), after SFT on 17k R1 samples (Fine-tuned), and Best-of-N samples on OlympiadBench. We find that fine-tuning on Long CoT achieves performance similar to Best of 2 to 16 samples.} %\dacheng{Want to re-confirm whether the 7B-math and qwen-14b results are correct.}}\label{fig:other_pdf} % Optional: Add a label for referencing
    \label{fig:test_time}
\end{figure}
%\begin{tcolorbox}%[colback=gray!30, colframe=gray]
%Writing PIC: Dacheng \newline
%Experiment PIC: Eric, Sumanth
%\end{tcolorbox}
As discussed in~\cref{sec:ablation_student}, not all student models achieve significant performance improvements through Long CoT fine-tuning. We hypothesize that this variation is influenced by several factors, such as the extent to which the training data distribution differs from that of the student models and the inherent capabilities of the student models in these tasks. In this section, we compare the test-time scaling~\citep{ahn2024large, snell2024scaling} performance of the base model with its performance after Long CoT fine-tuning to understand the relationship between a model's ability to benefit from Long CoT fine-tuning and its intrinsic capabilities.

% The models fine-tuned with Long CoT samples effectively leverage reflection and backtracking to enhance their reasoning capability. This naturally invites comparison to an alternative approach that improves reasoning by repeated sampling~\citep{ahn2024large, snell2024scaling}.
Specifically, we compare the performance of Long CoT fine-tuning against a Best-of-N sampling approach, where we generate 128 samples per prompt using an oracle verifier to select the best response. To introduce diversity, we employ a temperature of 0.5 and top-p sampling with a threshold of 0.8. The results, presented in~\cref{fig:test_time}, show that the Long CoT fine-tuned model performs comparably to Best-of-N sampling with 2 to 16 instances across all student models. 
Notably, the test-time scaling trends closely align with the improvements observed from Long CoT fine-tuning. For example, with eight parallel samples, Llama-3.1-8B achieves less than 10\% accuracy on OlympiadBench, and similarly, fine-tuning with correct Long CoT traces results in only marginal improvement. A comparable trend is observed in Gemma2-27B-it and Qwen2.5-Math-7B, reinforcing the relationship between test-time sampling efficiency and the benefits of Long CoT fine-tuning.

The performance of Best-of-N sampling continues to improve beyond 128 samples, suggesting that further gains are possible. This highlights the potential for enhancing Long CoT models through context scaling or by leveraging a broader range of reasoning paths inherent to the original model, potentially unlocking even higher performance.

% \dacheng{Add temperature details}

% In contrast to scaling the length a single CoT to tackle complex reasoning tasks, another popular line of research combine models with short-CoTs and test-time searching~\citep{ahn2024large, snell2024scaling}. 


%\textbf{The relationship of the pre-train capabilities and reasoning traces} @dacheng
%This is to say reasoning is just another different type of data that is activated, but very different from instruction-finetuning data coverage.

%\textbf{What is needed for next-generation reasoning models} (Proposed by Shiyi) Use 7B and 32B QwQ as a teacher to demonstrate: the performance is bottlenecked by the base model capability, show Pass@N curve for 7B model, and demonstrate it only approaches its Pass@N.



\begin{table}[ht!]
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\small\centering
\caption{%\joey{Use a bold highly descriptive title for tables: \textbf{Comparison of Reasoning Lengths.} 
\textbf{Comparison of number of output tokens reasoning keywords, and the performance between training with Short or Long CoT.}
%Reasoning Lengths and number of keywords used.}followed by a description with the key takeaway.  Which is?  Also what is ShortCoT and LongCoT?  Is this the key result for the intro?} Statistics of model response after fine-tuned on long chain-of-thoughts. 
The original model is Qwen2.5-32B-Instruct. Benchmarks are ordered from easy to hard, where the model trained with Long CoT learns to produce longer CoTs and uses more keywords for harder problems.} % \shiyi{move to next section}}
\vspace{-1em}

\resizebox{0.46\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Original} & \textbf{Short CoT} & \textbf{Long CoT} \\
\midrule
\rowcolor{black!10} \multicolumn{4}{l}{\textit{Avg. output tokens}} \\
MATH500           & 684   & 515   & 3972  \\
AMC23       & 728   & 605   & 5037  \\
OlympiadBench & 1275  & 948   & 8616  \\
AIME24      & 825   & 687   & 15902  \\
\midrule
\rowcolor{black!10} \multicolumn{4}{l}{\textit{Avg. keywords per response}} \\
MATH500          & 0.00  & 0.00  & 41.75  \\
AMC23      & 0.00  & 0.00  & 39.20  \\
OlympiadBench & 0.01  & 0.01  & 97.20  \\
AIME24      & 0.00  & 0.07  & 260.90  \\
\rowcolor{black!10} \multicolumn{4}{l}{\textit{Performance}} \\
MATH500          & 84.8  & 70.4 (-14.4) & \textbf{89.2} (+4.4) \\
AMC23      & 67.5  & 55.0 (-12.5) & \textbf{77.5} (+10.0) \\
OlympiadBench & 47.6  & 36.4 (-11.2) & \textbf{58.5} (+10.9) \\
AIME24      & 16.7  & 13.3 (-3.4) & \textbf{40.0} (+23.3) \\
\bottomrule
\end{tabular}
}
\label{tab:longcot_stats}
\end{table}
\subsection{Comparison to Short CoT Fine-tuning}
\label{sec:ablation_short}
In this section, we provide a direct comparison to training with short CoT. In particular, we compare results training on the 4.6k samples of Long CoT generated by QwQ-32B-Preview (\cref{sec:key}), and the short CoT denoted in the NuminaMath-CoT dataset~\citep{numina_math_datasets}. \cref{tab:longcot_stats} summaries the statistics. Training with Long CoT enables the model to use more reasoning keywords (full list in~\cref{sec:keywords}), produces longer responses to harder problems, and is the key to improved performance.

\section{Conclusion}
Large reasoning models unlock new capabilities by using a longer chain of thoughts that involves reflection and backtracking to answer challenging problems. In this paper, we show that such capability can be easily fine-tuned using a few thousand examples and with low-rank adapters. We further show that the key to the learning process is the logical structure of the samples rather than the content of individual reasoning steps. Finally, we discuss several ablations with various teacher-student models and compare them to the best-of-N approach. Together, our work deepens the understanding of what is needed to instill large language models with strong reasoning capabilities and identify potential future directions.

%\begin{tcolorbox}%[colback=gray!30, colframe=gray]
%Draft PIC: Dacheng
%\end{tcolorbox}
%\label{sec:related}


\section*{Impact Statement}
This paper aims to contribute to the advancement of Machine Learning. While our work may have various societal implications, we do not find any that require specific emphasis currently.

\section*{Acknowledgement}
% \dacheng{Check with faculty}
This work is funded by the Sky Computing Lab at UC Berkeley. We extend our sincere gratitude to Sumanth Hegde, Eric Tang, and Kourosh Hakhamaneshi from AnyScale for their invaluable feedback, assistance in building the evaluation infrastructure, and and contributions to the best-of-n experiments. We are grateful for the generous compute resources support from Databricks, Lambda Labs, and Anyscale. In particular, we thank Jonathan Frankle (Databricks) and Chuan Li (Lambda Labs) for facilitating access to these resources.
% We also thank Richard Liaw and Amjad Almahairi for their insightful comments. Finally, we are grateful to Jonathan Frankle (Databricks), Chuan Li (Lambda Labs), and the Anyscale team for generously providing the compute resources that made this research possible.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Short and Long CoT Response Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[colback=white,colframe=gray,title=Question]
For how many integers \( n \) does the expression
\[
\sqrt{\frac{\log (n^2) - (\log n)^2}{\log n - 3}}
\]
represent a real number, where \(\log\) denotes the base \(10\) logarithm?
\end{tcolorbox}

\begin{tcolorbox}
[colback=white,colframe=gray,title=Short CoT answer (1/2)]
\label{ref:figure_shortcot}
To solve the problem, we start by analyzing the given expression and the conditions under which it represents a real number. The expression is:

\[
\sqrt{\frac{\log (n^2) - (\log n)^2}{\log n - 3}}
\]

For the square root to represent a real number, the argument of the square root, which is the fraction inside, must be nonnegative. Additionally, since we are dealing with logarithms, \( n \) must be a positive integer. Let's break down the solution into detailed steps:

1. Condition for the Denominator
The denominator of the fraction is \( \log n - 3 \). For the fraction to be nonnegative, we need \( \log n - 3 > 0 \). This implies that \( \log n > 3 \), which translates to 

\[
n > 10^3 = 1000.
\]

Therefore, \( n \) must be greater than \( 1000 \).

2. Simplifying the Numerator
The numerator of the fraction is \( \log (n^2) - (\log n)^2 \). Using logarithmic properties, we can rewrite \( \log (n^2) \) as \( 2\log n \). Thus, the numerator becomes 

\[
2\log n - (\log n)^2.
\]

3. Analyzing the Fraction
Substituting the simplified numerator into the fraction, we get:

\[
\frac{2\log n - (\log n)^2}{\log n - 3}
\]

For this fraction to be nonnegative, given that the denominator is positive (from step 1), the numerator \( 2\log n - (\log n)^2 \) must also be nonnegative. This leads to the inequality:

\[
2\log n - (\log n)^2 \geq 0.
\]

4. Solving the Inequality
The inequality 

\[
2\log n - (\log n)^2 \geq 0
\]

can be rewritten as 

\[
(\log n)^2 - 2\log n \leq 0.
\]

Factoring,

\[
\log n (\log n - 2) \leq 0.
\]

Solving this inequality, we find that 

\[
0 \leq \log n \leq 2.
\]

Converting back to \( n \), this implies:

\[
1 \leq n \leq 100.
\]

5. Combining Conditions
From step 1, we have \( n > 1000 \), and from step 4, we have \( 1 \leq n \leq 100 \). However, these two conditions are contradictory. The correct interpretation should be that \( n \) must satisfy both the condition of being greater than \( 1000 \) (from the denominator analysis) and the condition derived from the inequality, which was misinterpreted in the combination step. The correct combination should focus on the range derived from the inequality, which is \( 1 \leq n \leq 100 \), and disregard the incorrect application of the denominator condition in this context.
\end{tcolorbox}
\begin{tcolorbox}[colback=white,colframe=gray,title=Short CoT answer (2/2)]
6. Conclusion
The integers \( n \) that satisfy the given conditions are those for which 

\[
1 \leq n \leq 100.
\]

Counting these integers, we find there are 

\[
\boxed{100}
\]

such integers.
\end{tcolorbox}

\begin{tcolorbox}[colback=white,colframe=gray,title=Long CoT answer (1/4)]
\label{ref:figure_longcot}
\texttt{\textless|}begin\_of\_thought\texttt{|>}
So I've got this problem here:
\[
\sqrt{\frac{\log (n^2) - (\log n)^2}{\log n - 3}}
\]

And I need to find out for how many integer values of \( n \) this expression represents a real number. The logarithm here is base 10.

First off, since it's a square root, the expression inside the square root has to be non-negative for the whole thing to be a real number. So, that means:

\[
\frac{\log (n^2) - (\log n)^2}{\log n - 3} \geq 0
\]

Additionally, the argument of the logarithm has to be positive because logarithms are only defined for positive real numbers. So, \( n^2 > 0 \), which is true for all \( n \neq 0 \), and \( n > 0 \) since \( n^2 > 0 \) is always true for \( n \neq 0 \), but we need to ensure \( n > 0 \) because the logarithm of a negative number is undefined in real numbers.

So, \( n \) has to be a positive integer.

Now, let's simplify the expression inside the square root.

I know that \( \log(n^2) = 2 \log n \), because of the logarithm power rule. So, substituting that in:

\[
\frac{2 \log n - (\log n)^2}{\log n - 3}
\]

Let me denote \( \log n = x \) to make it easier. So, the expression becomes:

\[
\frac{2x - x^2}{x - 3}
\]

I can factor the numerator:

\[
2x - x^2 = x(2 - x)
\]

So, the expression is:

\[
\frac{x(2 - x)}{x - 3}
\]

I need to analyze when this fraction is non-negative.

To do that, I can look at the sign of each factor:

- \( x = \log n \)

- \( 2 - x = 2 - \log n \)

- \( x - 3 = \log n - 3 \)

So, the expression is:

\[
\frac{\log n (2 - \log n)}{\log n - 3}
\]

I need to determine when this fraction is greater than or equal to zero.

Let me consider the critical points where each factor is zero:

1. \( \log n = 0 \) \( \Rightarrow n = 10^0 = 1 \)

2. \( 2 - \log n = 0 \) \( \Rightarrow \log n = 2 \) \( \Rightarrow n = 10^2 = 100 \)

3. \( \log n - 3 = 0 \) \( \Rightarrow \log n = 3 \) \( \Rightarrow n = 10^3 = 1000 \)

These points divide the number line into intervals where the expression doesn't change sign.

So, the intervals to consider are:

- \( 1 < n < 100 \)

- \( 100 < n < 1000 \)

- \( n > 1000 \)
\end{tcolorbox}


\begin{tcolorbox}[colback=white,colframe=gray,title=Long CoT answer (2/4)]
Wait, but I need to consider \( n \) as positive integers, and \( n \geq 1 \).

But actually, \( n \) has to be at least 1, but \( n = 1 \) makes \( \log n = 0 \), which is a critical point.
So, let's consider the intervals:

- \( 1 < n < 100 \)

- \( n = 100 \)

- \( 100 < n < 1000 \)

- \( n = 1000 \)

- \( n > 1000 \)

I need to test the sign of the expression in each interval.

First, let's consider \( n = 1 \):

\( \log 1 = 0 \), so the numerator is \( 0 \), and the denominator is \( 0 - 3 = -3 \), so the fraction is \( 0 / -3 = 0 \), which is acceptable since it's zero.

Next, for \( 1 < n < 100 \):

Let's pick \( n = 10 \), for example.

\( \log 10 = 1 \)

So, \( x = 1 \)

Then, \( 2 - x = 1 \)

\( x - 3 = -2 \)

So, \( \frac{1 \cdot 1}{-2} = -\frac{1}{2} < 0 \), which is not acceptable.

Wait, but earlier I thought the expression inside the square root needs to be non-negative. So, this interval doesn't work.

Next interval: \( n = 100 \)

\( \log 100 = 2 \)

Numerator: \( 2 \cdot (2 - 2) = 0 \)

Denominator: \( 2 - 3 = -1 \)

So, \( 0 / -1 = 0 \), which is acceptable.

Then, \( 100 < n < 1000 \)

Let's pick \( n = 1000^{0.5} = 10^{2.5} \approx 316.2 \), but since \( n \) is integer, let's take \( n = 300 \)

\( \log 300 \approx 2.477 \)

So, \( x \approx 2.477 \)

\( 2 - x \approx -0.477 \)

\( x - 3 \approx -0.523 \)

So, \( \frac{2.477 \cdot (-0.477)}{-0.523} \approx \frac{-1.182}{-0.523} \approx 2.26 > 0 \), which is acceptable.

Next, \( n = 1000 \)

\( \log 1000 = 3 \)

Denominator: \( 3 - 3 = 0 \), so the expression is undefined. Not acceptable.

Finally, \( n > 1000 \)

Let's pick \( n = 10000 \)

\( \log 10000 = 4 \)

So, \( x = 4 \)

\( 2 - x = -2 \)

\( x - 3 = 1 \)

So, \( \frac{4 \cdot (-2)}{1} = -8 < 0 \), which is not acceptable.

So, summarizing:

- \( n = 1 \): acceptable

- \( 1 < n < 100 \): not acceptable

- \( n = 100 \): acceptable

- \( 100 < n < 1000 \): acceptable

- \( n = 1000 \): not acceptable

- \( n > 1000 \): not acceptable

Therefore, the acceptable ranges are:

- \( n = 1 \)

\end{tcolorbox}

\begin{tcolorbox}
[colback=white,colframe=gray,title=Long CoT answer (3/4)]

- \( n = 100 \)


- \( 100 < n < 1000 \)

Wait, but \( n = 100 \) is included in \( 100 < n < 1000 \)? No, \( n = 100 \) is a separate point.

Actually, \( n = 100 \) is the upper limit of \( 100 < n < 1000 \), but since \( n = 100 \) is acceptable, I need to include it.

Wait, no, \( n = 100 \) is a separate point where the expression is zero, which is acceptable.

So, the acceptable \( n \) are:

- \( n = 1 \)

- \( 100 \leq n < 1000 \)

Wait, but earlier I thought \( n = 1000 \) is not acceptable, but \( n = 1000 \) is not included in \( 100 \leq n < 1000 \), which is correct.

So, now, I need to count the integer values of \( n \) in these ranges.

First, \( n = 1 \): 1 value

Second, \( 100 \leq n < 1000 \): that's from 100 to 999, inclusive.

So, \( 999 - 100 + 1 = 900 \) values.

Therefore, total integers are \( 1 + 900 = 901 \).

But wait, earlier I thought \( n = 100 \) is acceptable, and it is included in \( 100 \leq n < 1000 \), so no need to count it separately.

Wait, no, \( n = 100 \) is included in \( 100 \leq n < 1000 \), which already includes 100 to 999, so total is 900 values.

Plus \( n = 1 \), so total is 901.

But, I need to make sure that for all \( n \) in \( 100 \leq n < 1000 \), the expression is positive.

Wait, except for \( n = 100 \), where it's zero, which is acceptable.

But in the interval \( 100 < n < 1000 \), it's positive, as I tested earlier.

So, yes, total is 901.

Therefore, the number of integer values of \( n \) that satisfy the condition is 901.

**Final Answer**

\[
\boxed{901}
\]

\texttt{\textless|}end\_of\_thought\texttt{|>}

\texttt{\textless|}begin\_of\_solution\texttt{|>}

To determine for how many integer values of \( n \) the expression

\[
\sqrt{\frac{\log (n^2) - (\log n)^2}{\log n - 3}}
\]

represents a real number, we first note that the expression inside the square root must be non-negative:

\[
\frac{\log (n^2) - (\log n)^2}{\log n - 3} \geq 0
\]

Additionally, since logarithms are only defined for positive real numbers, we require \( n > 0 \).

1. Simplifying the expression

Using the logarithm power rule, we have:

\[
\log(n^2) = 2 \log n
\]

Thus, the expression becomes:

\[
\frac{2 \log n - (\log n)^2}{\log n - 3}
\]

Let \( x = \log n \). Then, the expression simplifies to:

\[
\frac{x(2 - x)}{x - 3}
\]

\end{tcolorbox}

\begin{tcolorbox}
[colback=white,colframe=gray,title=Long CoT answer (4/4)]

2. Finding critical points

The critical points are where the numerator or denominator equals zero:

- \( x = 0 \) \( \Rightarrow n = 1 \)
- \( 2 - x = 0 \) \( \Rightarrow x = 2 \Rightarrow n = 100 \)
- \( x - 3 = 0 \) \( \Rightarrow x = 3 \Rightarrow n = 1000 \)

3. Analyzing intervals

The intervals to consider are:

- \( 1 < n < 100 \)
- \( n = 100 \)
- \( 100 < n < 1000 \)
- \( n = 1000 \)
- \( n > 1000 \)

Testing the sign of the expression in each interval:

- For \( n = 1 \): The expression equals 0, which is acceptable.
- For \( 1 < n < 100 \): The expression is negative, not acceptable.
- For \( n = 100 \): The expression equals 0, acceptable.
- For \( 100 < n < 1000 \): The expression is positive, acceptable.
- For \( n = 1000 \): The expression is undefined, not acceptable.
- For \( n > 1000 \): The expression is negative, not acceptable.

4. Counting acceptable integers

The acceptable ranges are:

- \( n = 1 \)
- \( 100 \leq n < 1000 \)

The number of integers in \( 100 \leq n < 1000 \) is \( 900 \) (from 100 to 999 inclusive). Adding \( n = 1 \) gives a total of:

\[
1 + 900 = 901
\]

Thus, the number of integer values of \( n \) that satisfy the condition is:

\[
\boxed{901}
\]
\texttt{\textless|}end\_of\_solution\texttt{|>}
\end{tcolorbox}

\section{Reasoning keywords}
\label{sec:keywords}
We create a bank of reasoning keywords by manual inspection of reasoning responses from QwQ and R1. We identify these following words and phrases as frequent indicators of reflection, backtracking, and self-validation: ``Alternatively'', ``Wait'', ``Just to be thorough'', ``Just to make sure'', ``Let me just double-check'', ``Let me try another'', ``Let me verify'', ``Let me check'', ``Hmm'', ``But'', ``Maybe I should consider'', ``Maybe I can consider''.

\section{Long CoT System Prompt}
\label{sec:system_prompt}
We adopt the following system prompt from Still-2~\citep{min2024imitate}:
\begin{tcolorbox}
[colback=white,colframe=gray,title=Long CoT System Prompt]
Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: \texttt{\textless |}begin\_of\_thought\texttt{|\textgreater} thought with steps separated with \texttt{\textbackslash n\textbackslash n}\} \texttt{\textless |}end\_of\_thought\texttt{|\textgreater} Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: \texttt{\textless |}begin\_of\_solution\texttt{|\textgreater} {final formatted, precise, and clear solution} \texttt{\textless |}end\_of\_solution\texttt{|\textgreater} Now, try to solve the following question through the above guidelines:
\end{tcolorbox}

\section{Average response lengths and keyword counts}
\label{sec:lens_and_keyword_counts}
\begin{table}[h]
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\small\centering
\caption{Average keyword counts and output tokens for deleted steps.}
\vspace{-1em}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{0\%} & \textbf{33\%} & \textbf{67\%} & \textbf{100\%} \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{\textit{Avg. output tokens}} \\
Math           & 3551   & 2979   & 2078  & 482 \\
AMC 2023       & 4838  & 6612   & 4623  & 609 \\ 
OlympiadBench & 7234   & 6802   & 4978 & 595 \\ 
AIME 2024      & 13088   & 11889   & 6798  & 620 \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{\textit{Avg. keywords per response}} \\
Math           & 32   & 28   & 20  & 0.017 \\
AMC 2023       & 39  & 85.6   & 77.8  & 0 \\ 
OlympiadBench & 77   & 70   & 56 & 0.009 \\ 
AIME 2024      & 143   & 143   & 90  & 0 \\
\bottomrule
\end{tabular}
\label{tab:delete_stats}
\end{table}


\begin{table}[h]
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\small\centering
\caption{Average keyword counts and output tokens for inserted steps.}
\vspace{-1em}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{0\%} & \textbf{33\%} & \textbf{67\%} & \textbf{100\%} \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{\textit{Avg. output tokens}} \\
Math           & 3551   & 4189   & 3900  & 5383 \\
AMC 2023       & 4838  & 7089   & 5464  & 5137 \\ 
OlympiadBench & 7234   & 7558   & 6990 & 5407 \\ 
AIME 2024      & 13088   & 12858   & 12864  & 5304 \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{\textit{Avg. keywords per response}} \\
Math           & 32   & 39   & 39  & 41 \\
AMC 2023       & 39  & 98   & 44  & 35 \\ 
OlympiadBench & 77   & 76   & 80 & 38 \\ 
AIME 2024      & 143   & 127   & 165  & 44 \\
\bottomrule
\end{tabular}
\label{tab:insert_stats}
\end{table}

\begin{table}[h]
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\small\centering
\caption{Average keyword counts and output tokens for shuffled steps.}
\vspace{-1em}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{0\%} & \textbf{33\%} & \textbf{67\%} & \textbf{100\%} \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{\textit{Avg. output tokens}} \\
Math           & 3551   & 4284   & 5784  & 5613 \\
AMC 2023       & 4838  & 6802   & 10198  & 8661 \\ 
OlympiadBench & 7234   & 8942   & 12154 & 12167 \\ 
AIME 2024      & 13088   & 13451   & 16221  & 18054 \\
\midrule
\rowcolor{black!10} \multicolumn{5}{l}{\textit{Avg. keywords per response}} \\
Math           & 32   & 45   & 61  & 70 \\
AMC 2023       & 39  & 65   & 74  & 67 \\ 
OlympiadBench & 77   & 111   & 166 & 137 \\ 
AIME 2024      & 143   & 161   & 201  & 210 \\
\bottomrule
\end{tabular}
\label{tab:shuffle_stats}
\end{table}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
