\section{Related work}
\textbf{Test Time Scaling for Large Language Models}
Scaling test-time compute has proven effective in enhancing the the reasoning capabilities of LLMs. 
% This paradigm can be broken down into two primary directions: single long CoT, and ensemble multiple CoT. 
This can be broadly categorized into two directions: single long CoT and repeatedly sampled CoT. The former trains models, such as OpenAI o1, DeepSeek R1, and Qwen QwQ, to generate individual, long CoT responses with in-context reflection and backtracking to handle complex reasoning tasks~\citep{guo2025deepseek, jaech2024openai, team2024qwq}. 
Alternatively, repeated sampling methods, such as Best-of-N or search-guided generation (e.g., MCTS), improve reasoning performance by invoking multiple responses from the model, sometimes guided by search algorithms and reward models~\citep{snell2024scaling, brown2024large}. In this paper, we focus on distilling the ability to generate individual, Long CoTs, and show it can be done in a data- and parameter-efficient manner. 

\textbf{Training to improve reasoning capabilities of LLMs}
LLM reasoning capabilities can be improved by approaches such as iterative self-improvement and reinforcement learning (RL)~\citep{zelikman2022star, lightman2023let, lambert2024t, yuan2024free, guo2025deepseek}. More recently, Tulu-3 introduces Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance in tasks such as math and coding~\citep{hendrycksmath2021,jain2024livecodebench,numina_math_datasets}. PRIME proposes a RL-based method without process labels~\citep{yuan2024free}. The recent release of DeepSeek R1~\cite{guo2025deepseek} demonstrates that LLMs can learn to produce long CoT and improve reasoning using a pure RL-based approach. 
% This paper provides a non-RL and non-self-improvement-based approach to improve reasoning capabilities while achieving state-of-the-art performance on reasoning tasks. \shu{Not sure what does it mean by state-of-the-art performance? feels like we are comparing against RL}
Instead of bootstrapping reasoning ability, this paper focuses on the surprising data- and parameter-efficiency of distilling reasoning abilities from an existing reasoning model to an LLM. 

% There is a rich line of research that improves the by self-improving and reinforcement learning (RL). For example, 

% Prompting or training Large Language Models to perform complex reasoning enables the solving of challenging problems. In the prompting direction, chain-of-thoughts (CoT)~\citep{wei2022cot} allows the model to output an intermediate chain of tokens before the final answer. In 

% \joey{Note on related work section.  These days related work is also the section you highlight the novelty of your work. (think of it as the novelty section) Ideally at the end of each paragraph or cluster of related work you say what we did and how it is novel.  For example, maybe you can end the first paragraph with: "In contrast to the above work, here we focus on understanding the minimal amount of CoT demonstration data are required and what apsects of that data are essential for learning reasoning behavior.  Actually, you nailed this on the last paragraph in section 2.}

%\begin{tcolorbox}[colback=gray!30, %colframe=gray]
%PIC: Dacheng
%\end{tcolorbox}
%\paragraph{Reasoning for large language models} Mathematics reasoning tasks are important indicators of cognitive intelligence of humans~\citep{ahn2024large}. Numerous math datasets have been proposed to evaluate whether large language models process such reasoning capabilities~\citep{li2024numinamath, amini2019mathqa, hendrycksmath2021, cobbe2021training, miao2021diverse}. On the other hand, coding tasks often also require reasoning capabilities for code generation or code analysis. For instance, ~\citep{jain2024livecodebench, li2023taco} include algorirthmic problems such as LeetCode problems, and~\citep{gu2024cruxeval} also tests large language models for predicting code execution capabilities. From the modeling perspective, chain-of-thought like prompting techniques have been shown effective in eliciting the reasoning capability of large language models by producing thoughts before the final answers. Self-consistency further enhance chain-of-thought approach by inference multiple times and choose the majority votes for the answer~\citep{wang2022self}.\dacheng{@Matei delete these math benchmark, you don't need tons of related work on LRMs in general I think, but talk about the attempts to train them and the recent concurrent works}

%\paragraph{Test-Time scaling}
%More recently, scaling the test-time compute as opposed to scaling training compute has shown to be an effective way of improving reasoning capabilities. This paradigm can be majorly broke down into two directions: single long CoT based, and ensemble multiple CoT based. OpenAI o1, DeepSeek R1, Qwen QwQ models trains the model to reply with long CoT that includes reflection and backtracking, to better address reasoning tasks~\citep{guo2025deepseek, jaech2024openai, team2024qwq}. On the other hand, approaches such as repetitive sampling and further combination with search-based approaches during inference can also improve performance by choosing the best answer~\citep{snell2024scaling, brown2024large}. In this paper, our approach falls into the first category of test-time scaling. \joey{Say what we do that is novel...}
%\dacheng{@Matei: "There are a bunch of recent related works, and I think it's safe to call those "concurrent". Then there are the older ones that were much more complex and got worse results. Probably lead with the old ones and then talk about the very recent ones (TinyZero, HKUST SimpleRL, and the R1 paper itself). Maybe this is one advantage of talking about QwQ in the intro, but I would also mention that we see a similar phenomenon with R1 in there."}

\paragraph{Distillation}
Distilling the outputs or logits generated by a larger or more capable model has become a standard technique to enhance model performance~\citep{hinton2015distilling}.  Typically, responses generated by higher-quality models are used to perform supervised fine-tuning on smaller models~\citep{lambert2024t}. 
The Vicuna model, for instance, demonstrates that ChatGPT-generated responses can be used to effectively and cheaply distill high-quality chatting capabilities~\citep{zheng2023judging}. In this paper, we show that \textbf{reasoning capabilities can also be cheaply distilled}. We note that concurrent work has also observed similar trends in distilling reasoning capability~\citep{min2024imitate, huang2024o1}. 
Our paper differs from these recent works by demonstrating that reasoning distillation can be achieved efficiently with minimal parameter updates. We also provide an in-depth analysis of the key factors driving reasoning improvements, including the roles of the reasoning structure and content, as well as comprehensive evaluations and ablations across different data sizes and teacher models.

% The Vicuna model, for instance, demonstrated that ChatGPT-generated responses can effectively distill high-quality conversational capabilities\citep{zheng2023judging}. In this work, we extend this paradigm to reasoning, showing that \textbf{reasoning capabilities can be distilled efficiently and cost-effectively}. Concurrent studies have also observed similar trends in reasoning distillation~\citep{min2024imitate, huang2024o1}. However, our approach differs by demonstrating that reasoning distillation can be achieved with minimal parameter updates while maintaining strong performance. Furthermore, we conduct an in-depth analysis of the factors driving reasoning improvements, including the impact of reasoning structure and content. Our study provides comprehensive evaluations and ablations across varying data scales and teacher models, offering deeper insights into effective reasoning distillation strategies.





% learning process can be parameter efficient, and providing an in-depth study of the key factors of distilling reasoning capability. In particular, we show a breakdown of the role of structure and content in learning to reason as well as comprehensive evaluation and ablations such as different teacher models and data sizes. % \joey{fill me!}
\vspace{-1mm}