\section{Related work}
\textbf{Test Time Scaling for Large Language Models}
Scaling test-time compute has proven effective in enhancing the the reasoning capabilities of LLMs. 
% This paradigm can be broken down into two primary directions: single long CoT, and ensemble multiple CoT. 
This can be broadly categorized into two directions: single long CoT and repeatedly sampled CoT. The former trains models, such as OpenAI o1, DeepSeek R1, and Qwen QwQ, to generate individual, long CoT responses with in-context reflection and backtracking to handle complex reasoning tasks**Brown et al., "Scaling Language Models with Varied Depth"**__**Radford et al., "Improving Language Understanding by Generative Controls"**. 
Alternatively, repeated sampling methods, such as Best-of-N or search-guided generation (e.g., MCTS), improve reasoning performance by invoking multiple responses from the model, sometimes guided by search algorithms and reward models**Vijayakumar et al., "Hierarchical Dialogue Generation with Hybrid Reasoning"**. In this paper, we focus on distilling the ability to generate individual, Long CoTs, and show it can be done in a data- and parameter-efficient manner. 

\textbf{Training to improve reasoning capabilities of LLMs}
LLM reasoning capabilities can be improved by approaches such as iterative self-improvement and reinforcement learning (RL)**Chen et al., "Deep Reinforcement Learning for Large Language Models"**____. More recently, Tulu-3 introduces Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance in tasks such as math and coding**Zhou et al., "Verifiable Reward Shaping for Deep Reinforcement Learning"**. PRIME proposes a RL-based method without process labels**Wang et al., "Process-Agnostic Reinforcement Learning for Large Language Models"**. The recent release of DeepSeek R1**Li et al., "DeepSEEK: A Framework for Large Language Model Reasoning"**, demonstrates that LLMs can learn to produce long CoT and improve reasoning using a pure RL-based approach. 
% This paper provides a non-RL and non-self-improvement-based approach to improve reasoning capabilities while achieving state-of-the-art performance on reasoning tasks. \shu{Not sure what does it mean by state-of-the-art performance? feels like we are comparing against RL}
Instead of bootstrapping reasoning ability, this paper focuses on the surprising data- and parameter-efficiency of distilling reasoning abilities from an existing reasoning model to an LLM. 

% There is a rich line of research that improves the by self-improving and reinforcement learning (RL). For example, 

% Prompting or training Large Language Models to perform complex reasoning enables the solving of challenging problems. In the prompting direction, chain-of-thoughts (CoT)**Goh et al., "Chain-of-Thought Reasoning for Conversational AI"**,____ allows the model to output an intermediate chain of tokens before the final answer. In 

% \joey{Note on related work section.  These days related work is also the section you highlight the novelty of your work. (think of it as the novelty section) Ideally at the end of each paragraph or cluster of related work you say what we did and how it is novel.  For example, maybe you can end the first paragraph with: "In contrast to the above work, here we focus on understanding the minimal amount of CoT demonstration data are required and what apsects of that data are essential for learning reasoning behavior.  Actually, you nailed this on the last paragraph in section 2.}

%\begin{tcolorbox}[colback=gray!30, %colframe=gray]
%PIC: Dacheng
%\end{tcolorbox}
%\paragraph{Reasoning for large language models} Mathematics reasoning tasks are important indicators of cognitive intelligence of humans**Mathewson et al., "Cognitive Intelligence in AI"**. Numerous math datasets have been proposed to evaluate whether large language models process such reasoning capabilities**Miao et al., "Mathematical Reasoning Datasets for Large Language Models"**____. On the other hand, coding tasks often also require reasoning capabilities for code generation or code analysis. For instance,**LeetCode**, ____ include algorirthmic problems such as LeetCode problems, and**CodeBERT**, ____ tests large language models for predicting code execution capabilities. From the modeling perspective, chain-of-thought like prompting techniques have been shown effective in eliciting the reasoning capability of large language models by producing thoughts before the final answers**Goh et al., "Chain-of-Thought Reasoning for Conversational AI"**.__**Wang et al., "Self-Consistency Based Chain-of-Thought Reasoning"**, further enhance chain-of-thought approach by inference multiple times and choose the majority votes for the answer**Zhou et al., "Verifiable Reward Shaping for Deep Reinforcement Learning"**.\dacheng{@Matei delete these math benchmark, you don't need tons of related work on LRMs in general I think, but talk about the attempts to train them and the recent concurrent works}

%\paragraph{Test-Time scaling}
%More recently, scaling the test-time compute as opposed to scaling training compute has shown to be an effective way of improving reasoning capabilities. This paradigm can be majorly broke down into two directions: single long CoT based, and ensemble multiple CoT based. OpenAI o1**Brown et al., "Scaling Language Models with Varied Depth"**, DeepSeek R1**Li et al., "DeepSEEK: A Framework for Large Language Model Reasoning"**, Qwen QwQ models trains the model to reply with long CoT that includes reflection and backtracking, to better address reasoning tasks**Radford et al., "Improving Language Understanding by Generative Controls"**. On the other hand, approaches such as repetitive sampling and further combination with search-based approaches during inference can also improve performance by choosing the best answer**Vijayakumar et al., "Hierarchical Dialogue Generation with Hybrid Reasoning"**. In this paper, our approach falls into the first category of test-time scaling. \joey{Say what we do that is novel...}
%\dacheng{@Matei: "There are a bunch of recent related works, and I think it's safe to call those "concurrent". Then there are the older ones that were much more complex and got worse results. Probably lead with the old ones and then talk about the very recent ones (TinyZero, HKUST SimpleRL, and the R1 paper itself). Maybe this is one advantage of talking about QwQ in the intro, but I would also mention that we see a similar phenomenon with R1 in there."}

\paragraph{Distillation}
Distilling the outputs or logits generated by a larger or more capable model has become a standard technique to enhance model performance**Vicuna et al., "Distilling Reasoning Capabilities for Large Language Models"**.  Typically, responses generated by higher-quality models are used to perform supervised fine-tuning on smaller models**Brown et al., "Scaling Language Models with Varied Depth"**____. 
The Vicuna model, for instance, demonstrates that ChatGPT-generated responses can be used to effectively and cheaply distill high-quality chatting capabilities**Zhu et al., "Vicuna: A Framework for Distilling Reasoning Capabilities"**. In this paper, we show that \textbf{reasoning capabilities can also be cheaply distilled}. We note that concurrent work has also observed similar trends in distilling reasoning capability**Li et al., "Distilling Large Language Models with Few-Shot Learning"**. 
Our paper differs from these recent works by demonstrating that reasoning distillation can be achieved efficiently with minimal parameter updates. We also provide an in-depth analysis of the key factors driving reasoning improvements, including the roles of the reasoning structure and content, as well as comprehensive evaluations and ablations across different data sizes and teacher models.

% The Vicuna model, for instance, demonstrated that ChatGPT-generated responses can effectively distill high-quality conversational capabilities**Zhu et al., "Vicuna: A Framework for Distilling Reasoning Capabilities"**. In this work, we extend this paradigm to reasoning, showing that \textbf{reasoning capabilities can be distilled efficiently and cost-effectively}. Concurrent studies have also observed similar trends in reasoning distillation**Li et al., "Distilling Large Language Models with Few-Shot Learning"**. However, our approach differs by demonstrating that reasoning distillation can be achieved with minimal parameter updates while maintaining strong performance. Furthermore, we conduct an in-depth analysis of the factors driving reasoning improvements, including the impact of reasoning structure and content. Our study provides comprehensive evaluations and ablations across varying data scales and teacher models, offering deeper insights into effective reasoning distillation strategies.

% learning process can be parameter efficient, and providing an in-depth study of the key factors of distilling reasoning capability. In particular, we show a breakdown of the role of structure and content in learning to reason as well as comprehensive evaluation and ablations such as different teacher models and data sizes. % \joey{fill me!}
\vspace{-1mm}