\section{Experimental Results}
\subsection{Short-context Attribute Abstraction}


As shown in \Cref{tab:patient-structuring}, \ours with GPT-4o delivers the best overall performance across six short-context attribute abstraction tasks, achieving an average F1 score improvement of approximately 2 points compared to the state-of-the-art (SOTA) baseline. This improvement is driven by a notable 3-point gain in Recall. Notably, for the PD-L1 biomarker and performance status abstraction tasks, \ours with GPT-4o already reaches ceiling performance. 
Across the individual tasks, \ours with GPT-4o either outperforms or matches the baseline apart from case finding where the supervised baseline is trained on tens of thousands of labels. 
The overall performance of \ours is impressive given that the baseline methods rely on meticulously crafted heuristic rules or large labeled datasets whereas \ours does not have such data-specific training signals.

We also applied \ours to other \acp{LLM} besides GPT-4o. While \ours is adaptable to various \acp{LLM}, it performs best with the latest and most advanced models like GPT-4o and GPT-4, which significantly outperform other \acp{LLM}. These results demonstrate that \ours can effectively leverage the improvement from the base \acp{LLM} and \ours provides a stronger alternative to traditional attribute extraction methods.

\begin{table}[!ht]
    \footnotesize
    \centering
    \caption{Testing \ours on short-context attribute abstraction tasks in oncology. We show zero-shot \ours with GPT-4o achieves an overall 2 point F1 improvement compared with the conventional baseline approaches.
}
    \begin{tabular}{llrrr}
    \toprule
      {\bf Patient Attributes} & {\bf Approach} & {\bf Precision} & {\bf Recall} & {\bf F1} \\ 
      \midrule
        PD-L1 biomarker & Heuristics & 97.5 & 89.6 & 93.4  \\ 
        & \ours~(Llama 2) & 54.6 & 68.2 & 60.7 \\
        & \ours~(Mixtral) & 90.4 & 82.1 & 86.1 \\
        & \ours~(GPT-3.5)  & 90.0 & 83.2 & 86.5   \\
        & \ours~(GPT-4)  & 98.2 & 93.1 & 95.5   \\
       & {\bf \ours~(GPT-4o)} & {\bf 97.7} & {\bf 98.8} & {\bf 98.3} \\
         \midrule
        Performance Status & Heuristics & 100.0 & 97.5 & 98.7  \\ 
        (ECOG, KPS, Lansky, PPS)
        & \ours~(Llama 2) & 18.8 & 36.7 & 24.9 \\
        & \ours~(Mixtral) & 85.7 & 75.9 & 80.5 \\
        & \ours~(GPT-3.5-Turbo) & 95.8 & 87.3 & 91.4  \\
        & \ours~(GPT-4) & {\bf 100.0} & {\bf 98.7} & {\bf 99.4}  \\ 
  & {\bf \ours~(GPT-4o)} &  {\bf 100.0} & {\bf 98.7} & {\bf 99.4} \\

        \midrule
         Treatment & Heuristics & 85.0 &  85.0& 85.0\\ 
         & \ours~(Llama 2) & 81.2 & 77.6 & 79.4 \\
         & \ours~(Mixtral) & 81.8&75.4 &78.3 \\
        & \ours~(GPT-3.5)  & 84.6 & 73.3 & 78.5 \\
        & \ours~(GPT-4)  & 83.3 & {\bf 89.5} & 86.3   \\
        & {\bf \ours~(GPT-4o)} & {\bf 91.9} & 85.1 & {\bf 88.4} \\
       
         \midrule
          Progression & Supervised Model & {\bf 75.9} & 81.5 & 78.6 \\ 
        & \ours~(Llama 2) & 42.6 & 85.2 & 53.2 \\
        & \ours~(Mixtral) & 61.3 & 70.4 & 65.5 \\
        & \ours~(GPT-3.5)  & 66.7 & 44.4 & 53.3 \\
        & \ours~(GPT-4)  & 67.5 & {\bf 100} & 80.6 \\
        & {\bf \ours~(GPT-4o)}& 69.2 & {\bf 100} & {\bf 81.8} \\
         \midrule
          Response & Supervised model & 68.4 & {\bf 92.9} & {\bf 78.8} \\
        & \ours~(Llama 2) & 37.9 & 89.3 & 53.2 \\
        & \ours~(Mixtral) & 68.2 & 53.6 & 60.0 \\
        & \ours~(GPT-3.5)  & 50.0 & 60.7 & 54.8  \\
        & \ours~(GPT-4)  & 67.6 & 89.3 & 76.9 \\
        &{\bf \ours~(GPT-4o)} & {\bf 69.4} & 89.3 & 78.1 \\
         \midrule
          Case Finding & Supervised Baseline & 88.5 & {\bf 94.3} & {\bf 91.3} \\
        & \ours~(Llama 2) & {\bf 89.2} & 56.4 & 69.1 \\
        & \ours~(Mixtral) & 87.4 & 86.6 & 87.0 \\
        & \ours~(GPT-3.5) & 85.7 & 86.1 & 85.9 \\
        & \ours~(GPT-4) &  86.9 &  89.7 & 88.3 \\
        & {\bf \ours~(GPT-4o)} & 86.9 & 90& 88.4 \\
        \midrule
        \midrule
          Average & SOTA baseline & {\bf 85.9}	&90.1	&87.6
                   
                   \\
                   





        & \ours~(Llama 2) &54.1	& 68.9	& 56.8
\\
        & \ours~(Mixtral) &79.1&	74.0	&76.2 \\
        & \ours~(GPT-3.5) &78.8 &	72.5&	75.1 \\
        & \ours~(GPT-4) & 85.3	&93.5&	88.6
 \\
        &{\bf \ours~(GPT-4o)} & {\bf 85.9} &	{\bf 93.7}&	{\bf 89.1}\\
        \bottomrule
        
    \end{tabular}
    \label{tab:patient-structuring} 
\end{table}











\subsection{Long-context Attribute Abstraction: Cancer Staging}




Table~\ref{table:prov_results} presents the comparison of \ours and the supervised baselines for the long-context cancer staging attribute abstraction tasks. In this case, \ours with GPT-4o, despite being zero-shot \footnote{Although no full examples are given, some prompts contain fixed example snippets to clarify guideline application.},  achieves an overall improvement of 2 absolute points in accuracy over the supervised model. In particular, for the pathologic T attribute, \ours achieves a 20 absolute point improvement. 
For the other tasks, \ours achieves performance close to the supervised baseline. However, we should note that for primary site fine, histology and clinical T, \ours still has a significant gap compared with the supervised baseline.  This is nonetheless impressive performance from \ours given that \ours is zero-shot and the supervised models used separate splits from the same Providence dataset for training, and were therefore able to learn details of the annotation process specific to the data distribution. 

\begin{table}
    \centering
    \caption{Performance comparison, measured in accuracy, of Supervised models and zero-shot \ours on the long-context cancer staging attribute abstractions in the Providence dataset.}\begin{tabular}{lrrr}
        \toprule
        & Supervised & \ours & \ours  \\
        & & (GPT-4) & (GPT-4o)\\
        \midrule
        Primary Site Coarse & 92.6 & 91.9 & {\bf 93.2} \\
        Primary Site Fine & {\bf 71.7} & 64.3 & 68.6 \\
        Histology & {\bf 80.9} & 67.5 & 74.3 \\
        Clinical T & {\bf 58.6} & 50.9 & 51.4 \\
        Clinical N & {\bf 91.0} & 85.3 & 86.4 \\
        Clinical M & {\bf 95.2} & 92.9 & 92.4 \\
        Pathologic T & 55.0 & 73.9 & {\bf 75.4} \\
        Pathologic N & 62.4 & 65.0 & {\bf 75.8} \\
        Pathologic M & 93.8 & 91.6 & {\bf 94.1} \\
        \cmidrule(r){1-4}
        Average & 77.9 & 75.9 & {\bf 79.1} \\
        \bottomrule
    \end{tabular}
    
    \label{table:prov_results}
\end{table}

\eat{
\begin{table}[ht]  
\centering  
\begin{tabular}{lcccc}
\hline  
                     & {Supervised}    & \ours & \ours\\  
& & (GPT-4) & (GPT-4o) \\
\hline
Primary Site Coarse  & 0.826           & 0.878 & 0.895 \\  
Primary Site Fine    & 0.676           & 0.684 & 0.716 \\  
Histology            & 0.598           & 0.644 & 0.611 \\  
Clinical T           & 0.498           & 0.495 & 0.557 \\  
Clinical N           & 0.598           & 0.654 & 0.679 \\  
Clinical M           & 0.831           & 0.865 & 0.834 \\  
Pathologic T         & 0.731           & 0.747 & 0.777 \\  
Pathologic N         & 0.527           & 0.584 & 0.780 \\  
Pathologic M         & 0.715           & 0.856 & 0.811 \\  
\midrule
Average              & 0.667           & 0.712 & 0.740 \\  
\hline  
\end{tabular}  
\caption{Performance comparison, measured in accuracy, of Supervised models (trained with the labels from the Providence data) and \ours on the long-context cancer staging attribute abstractions in the held-out John Hopkins Medicine dataset.}
\label{table:jh_results}
\end{table}  
}
 
\eat{
Table~\ref{table:jh_results} shows the generalisability of \ours to the unseen John Hopkins dataset in comparison to the same supervised baseline trained from the Providence dataset. 
we can see that in this case \ours significantly outperforms the supervised models, showing \ours is much more robust to distribution changes and it is a more reliable choice to be used across different datasets and for new datasets with no training labels. We hypothesize that this is due to the generalizability of \ours as opposed to the supervised model; the \ours results rely only on the structuring guidelines and attribute definitions in the prompt, whereas the supervised models might overfit to the labels in a specific dataset. One potential concern for the experiment is that John Hopkins and Providence datasets differ significantly in the cancer type distributions. To confirm our hypothesis, we conduct an additional experiment filtering to only the patients with lung as the primary cancer site which is common to both two datasets. Our observation that \ours outperforms supervised baselines still hold in the latter setup as shown in \Cref{table:performance_comparison_lung}.}







\begin{table}[htb]
\centering
\caption{Ablating the key components (summarization, evidence, reasoning) in \ours for abstracting long-context attributes (fine-grained primary site) on the Providence dataset. ``All'' includes all tumor sites, while ``Breast'' includes only the more challenging abstraction tasks for `breast' cancer patients.}

\begin{tabular}{lcc}
\toprule
\textbf{Configuration}                 & \textbf{All}   & \textbf{Breast} \\
\midrule
\ours (GPT-4-32k)            & 63.5 & 52.5 \\
\quad no summarization               & 62.1 & 48.7 \\
\ours (GPT-4)                & 64.3& 52.0 \\
\quad no evidence                    & 62.4 & 50.7 \\
\quad no reasoning or evidence       & 62.9 & 49.8 \\
\bottomrule
\end{tabular}
\label{table:ablation_study}
\end{table}

To understand the contribution of the components in \ours, we performed ablation studies on \ours's summarization component, the evidence descriptor and the reasoning descriptor in attribute definition block (\Cref{table:ablation_study}). 
While the summarization step mainly aims to improve efficiency and to address the context window bottleneck, the ablation study helps us evaluate how this step affects performance for the patient notes that can fit into the context window. For this ablation experiment, we use gpt-4-32k as the underlying \ac{LLM} as it has a longer context window. We provide two setups: one setup is our proposed setup with the summarization step, and the other setup is where we pass a concatenation of all notes to the model as the patient history. We remove any patients whose concatenated notes do not fit into the 32k token context. We see that the summarization step is not only efficient but it also improves performance. This is likely because summarization reduces irrelevant information and therefore enhances the extraction accuracy.  
We also ablate the attribute definition setup in the prompt. In our proposed setting for some of the attributes, we instruct \ac{LLM} to extract not only the attribute values but also two additional descriptors: evidence and model reasoning. We show that removing these two descriptors decreases the performance. The performance drop is more significant for the breast cancer datasets, which typically involve more complex rules and demand greater reasoning capabilities. The reason for the significant drop is that these descriptors act as essential grounding elements—similar to a chain-of-thought reasoning process—providing context that enhances the accuracy of particularly the attribute abstraction tasks that require complex reasoning.







 
