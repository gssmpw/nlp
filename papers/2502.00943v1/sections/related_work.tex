\section{Related Work} 





\subsection{Conventional approaches to automate medical abstraction}
Automated data extraction using natural language processing (NLP) and machine learning has long been investigated in oncology-related research. In \citet{Gauthier2022-wk}, automated extraction from electronic health records (EHRs) of advanced lung cancer patients was found to be highly accurate and faster than manual abstraction despite the challenges of poorly structured EHRs and the use of analogous terms beyond the accepted gold standard definition. \cite{Preston2023-wb} explored cross-document medical information extraction using registry-derived, patient-level supervision to train deep NLP methods. The model achieved high performance in extracting core tumor attributes and showed potential for accelerating registry curation. \citet{kefeli2024generalizable} proposed to automate the classification of TNM cancer stages directly from pathology report text, using a BERT-based model trained on publicly available pathology reports.






\subsection{LLMs in medical abstraction}
In recent literature, LLMs like GPT-4 have been employed to organize and abstract clinical attributes from clinical notes within the field of oncology. These models have demonstrated considerable efficacy in medical information extracton tasks, including Named Entity Recognition (NER) and relation extraction (\cite{zhou2024universalner}, \cite{bhattarai2024leveraging}, \cite{goel2023llms}, \cite{10.1093/jamia/ocad259}, \citep{wong2023scaling}).
 Typically, these approaches leverage prompt engineering alongside few-shot learning technique to identify entity mentions, their text spans, entity types, and relations, including attributes and connections with other entities. Despite these advancements, there is a notable gap in the literature concerning how to scale the prompting across many attributes and how to end-to-end evaluate the results.
 Moreover, there is an absence of work focusing on the task of classifying medical entities according to guidelines and then aggregating evidence from one or several clinical notes to support this classification. Our study addresses this critical gap. 
