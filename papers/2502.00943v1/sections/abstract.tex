\begin{abstract} 
The vast majority of real-world patient information resides in unstructured clinical text, and the process of medical abstraction seeks to extract and normalize structured information from this unstructured input. 
However, traditional medical abstraction methods can require significant manual efforts that can include crafting rules or annotating training labels, limiting scalability.
In this paper, we propose \oursfull, a zero-shot medical abstraction framework leveraging \acp{LLM} through a modular and customizable prompt template. We refer to our approach as universal abstraction as it can quickly scale to new attributes through its universal prompt template without curating attribute-specific training labels or rules. We evaluate \ours for oncology applications, focusing on fifteen key attributes representing the cancer patient journey, from short-context attributes (e.g., performance status, treatment) to complex long-context attributes requiring longitudinal reasoning (e.g., tumor site, histology, TNM staging).
Experiments on real-world data show \ours's strong performance and generalizability. Compared to supervised and heuristic baselines, \ours with GPT-4o achieves on average an absolute 2-point F1/accuracy improvement for both short-context and long-context attribute abstraction. For pathologic T staging, \ours even outperforms the supervised model by 20 points in accuracy. 


\end{abstract}

\eat{
The vast majority of patient information resides in unstructured clinical text, such as progress notes and radiology reports. Medical abstraction seeks to extract and normalize this information, which is essential for precision health applications such as clinical trial matching and post-market surveillance. 
Standard medical abstraction methods require attribute-specific expert efforts such as crafting extraction rules or annotating examples for supervised learning, which is extremely expensive and time-consuming. 
In this paper, we explore the intrinsic structuring capability of large language models (i.e., without any specialized training for individual attributes). We conduct a case study on oncology, where medical abstraction is especially challenging. We use fifteen key attributes as representatives for the longitudinal cancer patient journey, including diagnostics (e.g., tumor site, histology, staging), health status (e.g., ECOG), treatment, outcome (e.g., treatment response and disease progression). 
Experimental results on real-world data from a large health network appear promising. GPT-4 displays amazing emergent universal structuring capability: it can simply read the definition of an attribute and then extract it from longitudinal patient records comprising up to hundreds of clinical documents, substantially outperforming other representative models such as BioGPT, Flan-T5, and GPT-3.5. 
In some cases such as pathologic T, GPT-4 even outperforms the supervised model by over 30 absolute points. We conduct thorough ablation study on prompt configuration and explore best practice in handling various challenges such as combating context length limits.

}


\eat{
The vast reservoir of high-value data encapsulated within the unstructured text of clinical documentation, such as pathology reports, imaging reports, progress notes, discharge summaries, surgical reports, and free text fields of Electronic Health Record (EHR) systems, remains largely untapped. These sources house a wealth of actionable patient data, albeit in an unstructured format, juxtaposed with more readily accessible structured data. In this study, we systematically explore extracting structured, comprehensive longitudinal patient histories from unstructured and semi-structured clinical free text, employing cutting-edge large language models (LLMs).

The enormity of unstructured data within oncology clinical text poses a formidable challenge in deriving meaningful and actionable insights. Our methodology harnesses the capabilities of large language models to comprehend the context, semantics, and subtleties of the clinical text, thereby facilitating the scalable extraction of structured data.

We employ a variety of advanced LLMs such as GPT-4, GPT-3.5, and FLAN-T5 to extract crucial oncology information, including tumor site, histology, stage, treatment, biomarkers, adverse drug events, progression, and response events. Our methodology enables the extraction of longitudinal data, offering a holistic view of the patient's journey over time.

Our findings illustrate that our methodology necessitates zero to few-shot examples and competes favorably with conventional methods in terms of precision and recall. Moreover, our approach allows for the extraction of structured data in a fraction of the time required by manual methods or labeling data to train a traditional Natural Language Processing (NLP) model, thereby enhancing efficiency and minimizing costs.

This research holds substantial implications for the field of oncology, as it enables the efficient extraction of structured patient histories, thereby aiding applications such as clinical trial matching, real-world evidence to develop personalized treatment planning, improving patient outcomes, and propelling cancer research and drug development. We posit that our approach can be extrapolated to other medical domains, thereby unveiling new opportunities for leveraging the power of LLMs in healthcare.
}
