
\section{Tasks and Experiment Setup}
As a testbed, we apply \ours for abstracting a representative set of oncology attributes. These attributes cover short-context attribute which can be extracted within the context of a note and the more challenging long-context attributes which require inference over a history of notes and long clinical guidelines. \Cref{tab:prompt-attribute-keys} shows the full list of the attributes and the associated descriptors we have defined in the attribute definition block in the prompt. 

The datasets for these attribute abstraction tasks are collected from the Providence health system, covering various types of patient documents. These documents may exist as free text or scanned files in the portable document format (PDFs). In the case of scanned PDFs, we used Azure AI Document Intelligence to convert them to free text prior to structuring. The composition of the datasets used to evaluate each attribute are different due to the availability of manual ground-truth annotations (details can be found in \Cref{tab:note-level-dev-dataset}). For the best performance, we apply \ours to state-of-the-art \acp{LLM} including \eat{GPT-3.5-turbo}, GPT-4 (version:2024-05-13) and GPT-4o (version:2024-05-13). We also demonstrate that \ours is generalizable for other \acp{LLM} and we also report results from open-source \acp{LLM} including Llama 2 (Llama 2 70B chat) and Mixtral (Mixtral 8x7B chat) for the short-context attribute abstraction tasks. All \ours experiments are zero-shot without accessing any training labels.
With regard to the baselines, for the tasks where train labels are available, we provide supervised baselines which are BERT base models trained on the labels. For the tasks where no train labels are available, we provide heuristics baselines following the domain-specific rules defined in \citet{gonzalez2023trialscope}.

Below, we provide detailed information on the datasets and experimental setup for each attribute, beginning with short-context attributes and proceeding to long-context attributes.

\begin{table}[!ht]
    \footnotesize
    \centering
        \caption{Description of the test sets from Providence. Path. = Pathology.}

     \scalebox{0.7}{
    \begin{tabular}{lrrrrr}
    \toprule
        {\bf Attributes} & {\bf \# patients} & {\bf \# notes} & {\bf \# Attribute Occurrences} & {\bf Note and Report Types} & {\bf Tumor Types} \\ 
      \midrule
        PD-L1 biomarker & 298 & 298 & 173 & Path. Reports + Progress Notes & All \\ 
        Performance Status & 565 & 565 & 79 & Progress Notes & All\\
        Treatment & 18 &431 & 203& Progress Notes & Lung Cancer \\
        Progression & 70 & 243 & 27 & Imaging Reports & Lung Cancer  \\
        Response & 70 & 243 & 28 & Imaging Reports & Lung Cancer \\
        Case Finding & 10,501 & 59,618 & 10,501 & Path. and Imaging \&Reports & All \\
        Cancer staging attributes & 2,918 & 33,293 & 2,918 & Path., imaging and surgical reports & All\\
        
    \bottomrule
    \end{tabular}}
    \label{tab:note-level-dev-dataset}
\end{table}



{\renewcommand{\arraystretch}{1.5}
\begin{table}[!ht]
    \centering
    \caption{Oncology attributes and the associated event descriptors that can be defined as part of the attribute definition block in \ours prompt
    }
    \scalebox{0.6}{
        \begin{tabular}{|l|r|r|r|}
    \hline
    {\bf Attribute} & {\bf Descriptors} & {\bf Definition} & {\bf Extracted Example Values} \\ \hline
    
    \multirow{ 2}{*}{Canse Finding} & cancer diagnosis & tumor histology & lung adenocarcinoma\\ \cline{2-4}
    & cancer diagnosis status & status of the diagnosis & positive, negative, suspicious, historical \\ \cline{2-4} 
         & date & cancer diagnosis date & 2016-12-15\\ \cline{2-4}
    \hline
    
    
    \multirow{ 2}{*}{PD-L1 Biomarker}
    & biomarker measurement type & specifies PD-L1 IHC measurement type & CPS, TPS, expression \\\cline{2-4}
    & biomarker variant & biomarker's variant or test value& 10\%, 5, High, T790M \\  
    \hline

    \multirow{ 3}{*}{Performance Status} & performance status & performance status snippet & ECOG 1, KPS 90\%  \\\cline{2-4}
    & performance status measurement type & the scale used for performance status & ECOG, KPS, Lansky \\\cline{2-4}
    & performance status value & the value of the performance status & CPS, TPS, expression \\ 
    \hline

    \multirow{2}{*}{Treatment} & treatment & treatment name & pembrolizumab, carboplatin, radiation \\\cline{2-4}
    & treatment date & treatment start date & 2014-02-03 \\
    \hline

    \multirow{ 3}{*}{ \begin{tabular}{@{}c@{}}Response \\ Progression \end{tabular}} & response & tumor response events & \begin{tabular}{@{}c@{}}complete response, partial response, \\ progressive disease, stable disease\end{tabular}  \\ \cline{2-4}
    & response disease & the disease or organ associated with the response & brain, lung, colon \\ 
    \cline{2-4}
    & response disease type & the type of disease with regard to cancer & tumor, lymph node, or non-cancerous tissues\\ 

    \hline
        Primary Site Coarse & Primary Site Coarse & Body site of primary tumor & C50 (Breast), C34 (Lung)\\
    \hline
        Primary Site Fine & Primary Site Coarse & Body site of primary tumor & C50.4 (Upper-outer quadrant of breast)\\ 
    \hline
        Histology & Histology & Cell type of tumor & 8046 (non-small cell lung cancer) \\
    \hline
        Clinical T & Clinical T & Clinical tumor staging & None, cT1, cT2, cT3, cT4 \\
    \hline
        Clinical N & Clinical N & Clinical nodal staging & None, cN0, cN1, cN2b \\
    \hline
        Clinical M & Clinical M & Clinical metastatic staging & None, cM0, cM1 \\
    \hline
        Pathologic T & Pathologic T & Pathologic tumor staging & None, pT1, pT2b \\
    \hline
        Pathologic N & Pathologic N & Pathologic nodal staging & None, pN0, pN1, pN2b \\
    \hline
        Pathologic M & Pathologic M & Pathologic metastatic staging & None, pM1 \\
        
\hline
    \end{tabular}
    }
    \label{tab:prompt-attribute-keys} 
\end{table}}


\subsection{Short-context Attribute Abstraction Tasks}
We test \ours on six short-context attributes that can be understood and abstracted in the immediate context within a single note. 
\Cref{fig:gpt-universal-prompt-template-simple} shows the template for abstracting short-context attributes. For the per-task configurations, we provide the attribute definitions according to \Cref{tab:prompt-attribute-keys}. 
For the per-patient input block, we input each note separately, and then collect the outputs for each patient for the final postprocessing step. For evaluation, we attach patient ID as an additional key to each attribute occurrence and count as positive when all the keys and values of the attribute correctly match the groundtruth. We report precision, recall and F1. 

\paragraph{PD-L1} PD-L1 protein expression is an important biomarker used to predict immunotherapy outcome as a high PD-L1 level may respond well to certain immune checkpoint inhibitor. Being able to extract the PD-L1 biomarker attribute can significantly facilitate the patient recruiting process in clinical trial matching. To configure the attribute definition block, we define the PD-L1 attribute by identifying two descriptors: the biomarker measurement types (eg. Combined positive score (CPS) or tumor proportion score (TPS)) and the biomarker variant descriptor that outputs the measurement values. To create the evaluation dataset, we manually curated 173 labels from 298 patients from the Providence data. 


\paragraph{Performance Status} 
Performance status is a standard criterion for measuring the patient's ability to perform routine daily activities. This is a required attribute that is a standard criterion for most clinical trials. For the task-specific configurations in the template, we define the two descriptors the \ac{LLM} needs to extract for a performance status attribute occurrence: performance status measurement types (which can be ECOG, KPS, PPS and etc.) and the measure value in numerical forms. To create the evaluation dataset, we manually curated 79 labels from 565 patients from the Providence data.  














\paragraph{Treatment}
The treatment attribute is a fundamental attribute of patient data, providing critical information about the timing and nature of treatments a patient has received. This data is essential for various downstream applications, such as predicting treatment outcomes and matching patients to clinical trials, which often require participants with specific prior treatments. In the task configuration section of the template, we define two key descriptors for the treatment attribute: the date of treatment and the treatment name. To create the evaluation set, we leverage existing treatment metadata from Providence. However, we observed that this structured data does not always capture all the treatments mentioned in the reports. As a result, we reviewed and manually corrected the data from a randomly selected subset to create a gold-standard test set.



\paragraph{Response and Progression}
Response and progression are important attributes to assess the treatment outcome of a clinical trial. In general, response indicates that the patient is showing improvement with the treatment, while progression signifies a worsening of the patient's condition. The response and progression attributes are extracted in one prompt. To provide the task-specific configurations in the template, we define response and progression based on the RECIST guideline\citep{nishino2010revised}\footnote{To curate the annotations from the reports, we relaxed the RECIST criteria to accommodate the level of details commonly available in standard follow-up radiology reports.}. Specifically, we require \ac{LLM} to list the specific response labels (choosing from: partial response, complete response, progressive disease or stable disease) and the corresponding response disease for each attribute occurrence. For each note, we collect the response attributes and progression attributes separately from the same \ac{LLM} output: if there is an occurrence of partial response or complete response for a disease in the note, we assign a response label. If there is an occurrence of progressive disease event in the note, we have a progression label. We manually curated the labels which are divided into 261 train labels and 55 test labels (28 response labels and 27 progression labels).




\paragraph{Case Finding}
Case finding is a system for locating patients who is diagnosed at a particular time. Case finding is essential for ensuring that cancer registries provide comprehensive, accurate, and timely data, which is critical for research, public health planning, and improving patient outcomes. To extract the case finding attribute, we define the diagnosis time as the key descriptor in the attribute definition block of our template as the ultimate goal of the task is to identify the moment of cancer diagnosis. We evaluate case finding extraction with labels collected from the cancer registry following the method in \citet{preston2023toward}. With around 50k \eat{50330} train labels, we provide a supervised baseline BERT model that predicts a binary label of whether the diagnosis happens given the note date following the setup in \citet{preston2023toward}. 



\subsection{Long-context Attribute Abstraction: Cancer Staging}

Cancer staging is a process used to determine the extent of cancer in the body. It involves the abstraction of multiple long-context attributes that requires a model to follow the rules and definitions set up in the lengthy clinical guidelines, and make inferences across multiple notes from the entire patient history.
For example, tumor measurements from imaging reports must be correlated with pathology findings to confirm a primary tumor site, and understanding the timing of diagnosis and treatments is crucial for determining their relevance to clinical or pathological staging values. \ours can effectively address these challenges. In the task configuration part, we offer the flexibility to take in long clinical guidelines and provide summarization specific to each attribute. To configure the attribute definition block, we define the attribute as the main descriptor to be extracted and since cancer staging involves complex reasoning and grounding, we also define two additional descriptors including the model reasoning descriptor and the evidence descriptor that enforce the \ac{LLM} to generate rationale and the supporting evidence (i.e. the piece of text from the patient note that supports the extracted attributes) before generating the attribute value. 
As to patient input, given the cancer staging attributes require reasoning over the patient history, we offer the solution to generate attribute-specific summaries from the patient history and pass the summaries into the patient input block. Alongside unstructured notes, we also provide the flexibility to incorporate other existing structured data such as treatment date. The previously extracted cancer staging attributes can also be chained for the best result. For example, we extract tumor site first and then we use the tumor site information to guide the extraction of the T/N/M attributes. \Cref{fig:gpt-universal-prompt-template-complex} shows the full instantiation of \ours for this case.

In this study on cancer staging, we focus on abstracting the following eight attributes that are key in the staging process. 





\paragraph{Primary site coarse/fine-grained}: The primary site attribute extracts the primary site of the tumor and we extract two attributes with different granularity. In the attribute definition block in our universal template, we define the primary site coarse attribute as the coarse-grained tumor site with a finite set of choices (eg. C34 for ``Lung''). For the fine-grained primary site attribute, we instruct the model to extract a more specific location (e.g. C34.1 for ``Upper lobe of the lung''). 

\paragraph{Histology} Histology describes the type of cells or tissues from which the cancer originates. We define the task requirements in the attribute definition block and instructs the model to output four-digit ICD-O-3 histology code.

\paragraph {Clinical T/N/M} We follow the conventional TNM system in determining cancer staging \citep{rosen2023tnm}. We first define the clinical staging attributes which describe staging results determined before treatment initiation. Specifically, we define clinical T (tumor) attribute as describing the size and extent of the primary tumor and it is a multi class classification task.  Similarly, we define clinical N (nodule) attribute which describes the involvement of regional lymph nodes, and we define clinical M (Metatasis) as describing whether the cancer has metastasized. The exact definitions of the output values come from the attribute-specific summarization of the clinical guideline that lays out the detailed rules and requirements for determining the staging outcome. To prepare the patient input, we also pass in other structured data alongside the unstructured notes. For example, we pass in treatment date which is essential for guiding the \ac{LLM} to extract staging information only from records prior to the treatment date. We also input primary site attributes that we have previously extracted as additional structured data context to guide \ac{LLM} to provide the correct staging results.

\paragraph{Pathologic T/N/M} The pathologic T/N/M attributes have the same setup to clinical T/N/M apart from the different sources of staging information, allowing the use of staging information collected during treatment including excised tumor tissue.








    










































\paragraph{Dataset and baselines}
To evaluate the abstraction of the cancer staging attributes, we obtained manually abstracted cancer registry staging data from Providence Health \eat{. Our primary dataset was obtained from Providence Healthcare, } where we collected electronic patient records and linked them to their cancer registry. We excluded cases lacking a pathology report within 30 days of the diagnosis date listed in the cancer registry. Additionally, patients with multiple primary cancer diagnoses were excluded from the study.

\eat{
To provide a more comprehensive evaluation and test the generalization abilities of our approach, in addition to the data from Providence Healthcare, we collaborated with members of Johns Hopkins Medicine to compile a similar validation dataset from their cancer registry and patient records. This process involved collecting data from the cancer registry, integrating it with electronic patient records, and filtering based on the presence of a pathology report near the diagnosis date. 
Characteristics of the two datasets prepared for cancer staging evaluation are detailed in Table~\ref{tab:prov_jh_dataset_comparison} and Table~\ref{tab:prov_jh_patient_stats}.
We note that in Hopkins, a patient generally has more imaging reports and the notes are usually longer with about three times the token count. In addition to differences in clinical notes, the staging datasets have different distributions of primary tumor sites, shown in Figure~\ref{fig:primary_site_dist}. While the Providence dataset is dominated by breast cancer, the Johns Hopkins datasets has a much higher portion of lung, blood, pancreatic, and brain cancer.}

To establish an upper bound performance from conventional methods, we train a supervised baseline using the dataset (with 23438 patients' medical records as the train and dev set) and follow the methods described in \citet{preston2023toward}. We modify the original tasks to make the evaluation more realistic by (1) including a "None" category for each prediction, allowing the model to indicate there is not enough information to make a predictions; (2) predicting standard four-digit histology codes; and (3) using the standard nodal staging instead of simplified N0/N+ classification. 
\eat{For the Hopkins dataset, we only have the test labels and we directly run the supervised model on the Providence dataset. For \ours, we provide a zero-shot setting and we use the same prompt template for the two datasets except for passing in different clinical guidelines that are used in Providence and Hopkins.} 
As to the underlying \acp{LLM} for \ours, we use GPT-4 and GPT-4o as they are the most competitive \acp{LLM} as verified by the short-context attribute absraction experiments. We report accuracy rather than F1 for all the tasks, because each patient will have a prediction for each attribute, even if the prediction is \emph{None} (i.e. not enough information to determine).

In addition to the patient dataset, the \ours prompts for cancer staging attributes include information from cancer staging guidelines. These guidelines include the International Classification of Diseases for Oncology (ICD-O) manual, the American Joint Committee on Cancer (AJCC) Cancer Staging Manual, and the Standards for Oncology Registry Entry (STORE) manual. These sources were structured via a semi-automated process using GPT-4 to organize and summarize guidelines relevant to a particular abstraction task.
\eat{
\begin{table}[h]
\centering
\begin{tabular}{lr@{\hspace{3pt}}r}
\toprule
 & \multicolumn{1}{c}{Providence} & \multicolumn{1}{c}{Hopkins} \\
\midrule
\# of patients & 2,918 & 3,760 \\
\# of notes (all) & 33,293 & 110,314 \\
\hspace{5mm}\# of imaging reports & 21,936 & 93,426 \\
\hspace{5mm}\# of pathology reports & 11,357 & 16,888 \\
\hdashline
\# median per-patient tokens & 3984 & 11461\\
\# median per-patient notes & 7 & 23 \\
\# median per-patient imaging reports & 6 & 19\\
\# median per-patient pathology reports & 2 & 3 \\
\bottomrule
\end{tabular}
\caption{Number of patients, notes and per-patient counts in the Providence and Hopkins datasets.}
\label{tab:prov_jh_dataset_comparison}
\end{table}
}
\eat{
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{figures/cancer_type_mix_barh.png}
\caption{Distribution of primary sites in Providence and Johns Hopkins datasets.}
\label{fig:primary_site_dist}
\end{figure}
}




\eat{
\green{ask sam or cliff: do we need to change anything here? ask Tristan for Providence ID. Sam is following up with Hopkins to IRB code}
This work was performed under the auspices of the independent institutional review board (IRB)-approved research protocols (Providence protocol ID \red{2019000204} \eat{; Johns Hopkins Medicine \red{0000000000})} and
was conducted in compliance with human subjects research and clinical
data management procedures—as well as cloud information security policies
and controls—administered within each institution. All study
data were integrated, managed, and analyzed exclusively and solely on respective institute cloud infrastructures. All study personnel completed and
were credentialed in training modules covering human subjects research,
use of clinical data in research, and appropriate use of IT resources and
IRB-approved data assets.}
