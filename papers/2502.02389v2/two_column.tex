\documentclass[journal]{IEEEtran}

\usepackage[titles]{tocloft}

\usepackage{braket}
\usepackage{amsmath,amssymb,amsfonts,dsfont,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref}
\usepackage{orcidlink}

\usepackage[style=ieee,giveninits=false,sorting=none,minnames=3,maxnames=10,dashed=false]{biblatex}

\addbibresource{ID.bib}





%Redefined theorem-like numbering, since we have only 11 items in the whole paper
%\newtheorem{theorem}{Theorem}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\newcommand\myfrac[2]{\genfrac{}{}{0pt}{}{#1}{#2}}

\newlength{\blank}
\settowidth{\blank}{\emph{~}}
\newenvironment{proofof}[1][{\hspace{-\blank}}]{{\medskip\noindent\textit{Proof~{#1}}.\ }}{\qed}


\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}

\newcommand{\1}{\openone} %identity matrix on Hilbert space
\newcommand{\id}{\operatorname{id}} %identity channel on trace class

\newcommand{\ox}{\otimes}
\newcommand{\proj}[1]{\ket{#1}\!\bra{#1}}
\DeclareMathOperator{\Tr}{Tr}



\DeclareFontFamily{OMX}{yhex}{}
\DeclareFontShape{OMX}{yhex}{m}{n}{<->yhcmex10}{}
\DeclareSymbolFont{yhlargesymbols}{OMX}{yhex}{m}{n}
\DeclareMathAccent{\wideparen}{\mathord}{yhlargesymbols}{"F3}

\newcommand{\aw}[1]{{\color{blue}#1}}
\newcommand{\pau}[1]{{\color{purple}#1}}
\newcommand{\new}[1]{{\color{teal}#1}}
\newcommand{\cd}[1]{{\color{green}#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Rate-reliability functions\protect\\ for deterministic identification
\thanks{A preliminary version of the present work has been accepted 
for presentation at the 2025 IEEE International Conference on Communications, Montreal (Canada) 8-12 June 2025 \cite{CDBW:Reliability_ICC}.
HB and CD are supported by the German Federal Ministry of Education and Research (BMBF) within the national initiative on 6G Communication Systems through the research hub 6G-life, grants 16KISK002 and 16KISK263, within the national initiative on Post Shannon Communication (NewCom), grants 16KIS1003K and 16KIS1005, within the national initiative ``QuaPhySI'', 
%-- Quantum Physical Layer Service Integration, 
grants 16KISQ1598K and 16KIS2234, and within the national initiative ``QTOK'', 
%-- Quantum tokens for secure authentication in theory and practice'', 
grant 16KISQ038. 
%
HB has further received funding from the German Research Foundation (DFG) within Germany’s Excellence Strategy EXC-2092-390781972. 
%
AW is supported by the European Commission QuantERA grant ExTRaQT (Spanish MICIN project PCI2022-132965), by the Spanish MICIN (projects PID2019-107609GB-I00 and PID2022-141283NB-I00) with the support of FEDER funds, by the Spanish MICIN with funding from European Union NextGenerationEU (PRTR-C17.I1) and the Generalitat de Catalunya, and by the Alexander von Humboldt Foundation.
%
PC and AW are furthermore supported by the Institute for Advanced Study of the Technical University Munich (IAS-TUM).}}
%
\author{Pau~Colomer\textsuperscript{\orcidlink{0000-0002-0126-4521}}\thanks{P. Colomer (pau.colomer@tum.de) is with the Chair of Theoretical Information Technology, Technische Universit\"at M\"unchen (LTI-TUM), Theresienstra{\ss}e 90, 80333 M\"unchen, Germany; and IAS-TUM, Lichtenbergstra{\ss}e 2a, 85748 Garching, Germany.},~\IEEEmembership{Student Member,~IEEE}, 
Christian~Deppe\textsuperscript{\orcidlink{0000-0003-3047-3549}}\thanks{C. Deppe (christian.deppe@tu-bs.de) is with the Institute for Communications Technology and the 6G-life research hub in Technische Universit\"at Braunschweig, Schleinitzstra{\ss}e 22, 38106 Braunschweig, Germany; and was with the Institute for Communications Engineering, 
TUM.},~\IEEEmembership{Member,~IEEE},\protect\\
%Technische Universit\"at M\"unchen, Theresienstra{\ss}e 90, 80333 M\"unchen, Germany.},~\IEEEmembership{Member,~IEEE},\protect\\
Holger~Boche\textsuperscript{\orcidlink{0000-0002-8375-8946}}\thanks{H. Boche (boche@tum.de) is with LTI-TUM and the 6G-life research hub in Theresienstra{\ss}e 90, 80333 M\"unchen, Germany; Munich Center for Quantum Science and Technology, Schellingstra{\ss}e 4, 80799 München, Germany; and Munich Quantum Valley, Leopoldstra{\ss}e 244, 80807 München, Germany.},~\IEEEmembership{Fellow,~IEEE},
and~Andreas~Winter\textsuperscript{\orcidlink{0000-0001-6344-4870}}
\thanks{A. Winter (andreas.winter@uab.cat) is with ICREA and Grup d'informaci\'o qu\`antica, Departament de F\'isica, Universitat Aut\`onoma de Barcelona, 08193 Bellaterra (Barcelona), Spain, as well as with IAS-TUM.}}
\maketitle

\thispagestyle{empty}

\begin{abstract}
We investigate deterministic identification over arbitrary memoryless channels under the constraint that the error probabilities of first and second kind are exponentially small in the block length $n$, controlled by reliability exponents $E_1,E_2 \geq 0$. In contrast to the regime of slowly vanishing errors, where the identifiable message length scales as $\Theta(n\log n)$, here we find that for positive exponents linear scaling is restored, now with a rate that is a function of the reliability exponents.
We give upper and lower bounds on the ensuing rate-reliability function in terms of (the logarithm of) the packing and covering numbers of the channel output set, which for small error exponents $E_1,E_2>0$ can be expanded in leading order as the product of the Minkowski dimension of a certain parametrisation the channel output set and $\log\min\{E_1,E_2\}$. These allow us to recover the previously observed slightly superlinear identification rates, and offer a different perspective for understanding them in more traditional information theory terms. 
We further illustrate our results with a discussion of the case of dimension zero, and extend them to classical-quantum channels and quantum channels with tensor product input restriction.
\end{abstract}

\begin{IEEEkeywords}
Shannon theory;
identification via channels;
finite block length;
memoryless systems;
quantum information.
\end{IEEEkeywords}

\newpage

\section{Introduction}
\label{sec:intro}
%\aw{General remarks: This is the long article, so we can afford full explanations, which should be added where appropriate to the parts taken from the ICC article, and given in the new parts. (Of course, i agree that we do not need to repeat everything from \cite{CDBW:DI_classical}, but enough for the present exposition to be understandable.)}
%{\color{green}Please make sure that we provide full and complete references, as we do in our previous work: e.g. the definition of the dimension, but please review the whole text, otherwise I will have to do it. I was a bit surprised that the bib file apparently is not an upgraded copy of the previous \texttt{ID.bib} but a fresh collection; you could have saved ourselves some copy-paste work.}

%\pau{The ID.bib is now an extended copy of the previous .bib files. The references for the classical part have been checked. There are no quantum references yet, as in the first version I just referred the reader to the quantum section in \cite{CDBW:DI_classical}. We probably need to find a compromise between the full quantum intro in our previous work and the current one which is indeed too light and inconvenient for the reader.}

\IEEEPARstart{I}{n} Shannon's fundamental model of communication \cite{Shannon:TheoryCommunication}, the receiver aims to faithfully recover an original message sent through $n$ uses of a memoryless noisy channel $W:\cX\rightarrow\cY$, given by a probability transition kernel between measurable spaces $\cX$ and $\cY$. The maximum number $M$ of transmittable messages scales exponentially with $n$, and thus we define the \emph{(linear-scale) rate} of a transmission code as $R=\frac1n\log M$, with $\log$ the binary logarithm. The maximum rate for asymptotically faithful transmission -- that is, with arbitrarily small probability of error for sufficiently long code words -- is known as the \emph{capacity} $C$ of the channel \cite{Shannon:TheoryCommunication,Wolfowitz:converse}. 

This model contrasts with the \emph{identification} task in which instead of recovering the initial message, the receiver is only interested in knowing whether it is equal to one particular message of their choice or not. 
Jaja demonstrated in \cite{Ja:ID_easier} that this task involves lower communication complexity than Shannon's original transmission scenario. Building on this result, Ahlswede and Dueck \cite{AD:ID_ViaChannels} fully characterized the identification problem and analyzed its performance over general discrete memoryless channels (DMCs) using a randomized encoder.
%This task has less communication complexity than Shannon's original transmission problem \cite{Ja:ID_easier} and was fully characterized in \cite{AD:ID_ViaChannels}. 
They showed that identification codes can achieve doubly exponential growth of the number $N$ of messages as a function of the block length $n$, i.e.~$\log N \sim 2^{nR}$. In other words, we can identify exponentially more messages than we can transmit. As this result relies crucially on randomized encoding, it is natural to ask what happens if we impose deterministic encoding. 

\begin{definition}
\label{def:DI-code}
An $(n,N,\lambda_1,\lambda_2)$ \emph{deterministic identification (DI) code} is a family $\{(u_j,\cE_j) : j\in[N]\}$ consisting of code words $u_j\in\cX^n$ and subsets $\cE_j\subset \cY^n$, such that 
%for all $j\neq k\in[N]$:
\[
    \forall j\neq k\in[N] \quad
    W_{u_j}(\cE_j) \geq 1-\lambda_1,\quad 
    W_{u_j}(\cE_k) \leq \lambda_2,
\]
where we use the notation $W_{u_j}:=W^n(\cdot|u_j)$ for the conditional probability distributions at the output. 
\end{definition}

It was initially observed that deterministic identification leads to much poorer results in terms of code size scaling in the block length \cite{AD:ID_ViaChannels, AC:DI} than the unrestricted randomized identification. Indeed, DI over discrete memoryless channels can only lead to linear scaling of the message length, $\log N \sim Rn$ \cite{SPBD:DI_power} as in Shannon's communication paradigm (albeit with a higher rate $R$). 
Despite this poorer performance, interest in deterministic codes has recently been renewed, as they have proven to be easier to implement and simulate \cite{DI_simpler_impl}, to explicitly construct \cite{DI_explicit_construction}, and offer reliable single-block performance \cite{AD:ID_ViaChannels}; see in particular \cite{VDTB:practical-DI} for identification codes.
Also, surprisingly, certain channels with continuous input alphabets have DI codes governed by a slightly superlinear scaling in the block length: $\log N\sim Rn\log n$. 
This was first observed for Gaussian channels \cite{SPBD:DI_power}, Gaussian channels with both fast and slow fading \cite{DI-fading, VDB:DI-fading-new}, and Poisson channels \cite{DI-poisson,DI-poisson_mc}.
This behaviour differs from that of randomized identification, where the scaling remains the same as for DMCs \cite{Han:book,LDB:mc}.

In our recent work \cite{CDBW:DI_classical,CDBW:DI_proceedings} we have generalized all these particular DI results, proving that the superlinear scaling observed in these special cases is actually a feature exhibited by general channels with discrete output. For the rest of the paper we will in fact assume that $\cY$ is finite. The corresponding superlinear pessimistic and optimistic capacities 
(see \cite{Ahlswede2006} or the discussion in \cite[Sect.~7.1]{CDBW:DI_classical} for further insight on this topic) 
can be bounded respectively in terms of the lower and upper covering dimension (an introduction and definitions of these concepts can be found in Section \ref{sec:preliminaries}), $\underline{d}_M$ and $\overline{d}_M$, of a certain algebraic transformation of the set of output probability distributions of the channel: 
\begin{align}
    \frac14\underline{d}_M
    &\leq \dot{C}_{\text{DI}}(W)\,
    \leq \liminf_{n\rightarrow\infty} \frac{1}{n\log n}\log N
    \leq \frac12\underline{d}_M,\\
    \frac14\overline{d}_M
    &\leq \dot{C}_{\text{DI}}^{\text{opt}}(W) 
    \leq \limsup_{n\rightarrow\infty}\frac{1}{n\log n}\log N
    \leq \frac12\overline{d}_M.
\end{align}
When $d=0$, like in the case of the discrete memoryless channel, a more general and abstract theorem is provided to meaningfully bound the size of the code at the correct (slower) scaling \cite[Thm.~5.11]{CDBW:DI_classical}.

While the DI capacity is defined as the maximum achievable (superlinear) rate for reliable communication -- where the error probability can be made arbitrarily small with sufficiently large $n$ -- it does a priori not provide information on how quickly the errors vanish.
In contrast, it is well-known in the communication setting \cite{Shannon:TheoryCommunication,Gallager:ReliabilityBook} that at (linear scale) rates below capacity, $0\leq R < C$, the error probability decreases exponentially with the block length: $\lambda \sim 2^{-En}$. 
In the present paper we initiate the study of the performance of DI codes when $\lambda_1$ and $\lambda_2$ are exponentially small, in an attempt to better understand the superlinear rates at slowly vanishing errors.
As it turns out, we can rely on many ideas and tools from our previous work \cite{CDBW:DI_classical}, as it was written with an eye to finite block length. The present work extends the results from \cite{CDBW:DI_classical}, which are recovered from here in the limit of error exponents slowly going to $0$.
%probabilities.
%Also, for codes growing (single) exponentially with the block length, the rate is the exponent of the code size $N=2^{nR(n)}$, so we might as well describe any $(n,N,\lambda_1,\lambda_2)$-DI code by the triple $(R(n),E_1(n),E_2(n))$.

Anticipating our subsequent results, we associate to every $(n,N,\lambda_1,\lambda_2)$-DI code the triple $(R(n),E_1(n),E_2(n))$ of linear-scale message and error rates:
\[
  R(n) := \frac1n\log N,\,\,
  E_1(n) := \frac1n\log\frac{1}{\lambda_1},\,\,
  E_2(n) := \frac1n\log\frac{1}{\lambda_2}.
\]
%Defining these quantities in the non-asymptotic regime allows us to have information on the backoff from capacity when transmitting at a certain (finite) block length required to sustain some particular probabilities of error \cite{PPV:finite_blocklength}. 
%\pau{Add computability results where non-as ymptotics are important?}

We call $(R,E_1,E_2)$ an \emph{(asymptotically) achievable triple} if there exist deterministic identification codes with $(R(n),E_1(n),E_2(n))$ converging to $(R,E_1,E_2)$ for $n\rightarrow\infty$. The set of all achievable triples will be referred to as the \emph{achievable region} $\cR(W):=\{(R,E_1,E_2) \text{ achievable} \}$. The rate-reliability function is then
\[
R(E_1,E_2) := \sup_{E_1,E_2}R \quad \text{s.t.} \quad (R,E_1,E_2)\in\cR(W).
\]
%
Similarly, we can define an \emph{optimistic achievable region} $\overline{\cR}(W)\supseteq\cR(W)$ by allowing the convergence to happen only for a subsequence $n_k\rightarrow\infty$ of block lengths. That is to say, $\overline{\cR}(W)$ is the set of all triples $(R,E_1,E_2)$ such that there exist codes of block length $n_k\rightarrow\infty$ with $(R{(n_k)},E_1{(n_k)},E_2{(n_k)}) \rightarrow (R,E_1,E_2)$. The optimistic rate-reliability function is thus
\[
\overline{R}(E_1,E_2) := \sup_{E_1,E_2} R \quad \text{s.t.} \quad (R,E_1,E_2)\in\overline{\cR}(W).
\]
Note that both $\cR(W)$ and $\overline{\cR}(W)$ are so-called \emph{corners} in $\RR^3$: they are both actually contained in the positive orthant $\RR_{\geq 0}^3$, contain the origin, and with every point $(R,E_1,E_2)$ contain the entire box $[0;R]\times[0;E_1]\times[0;E_2]$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{di_vs_trans_plot.png}
%    \includegraphics[width=0.667\linewidth]{di_vs_trans_plot2.png}
    \caption{\small Schematic of the rate-reliability function for transmission and 
    %the present upper and lower bounds one the rate-reliability function for 
    DI. The rate-reliability function for transmission (blue line) is a monotonically non-increasing function with different regions depending on the optimum coding strategies (see the tendency change from linear to quadratic behaviour marked by the grey dotted line). For error exponents going to zero (very slow error decrease with the block length $n$), the linear rate converges to the transmission capacity $C(W)$; no higher rates are possible.
    Here, we provide bounds on the rate-reliability function for DI (red line) that show how the rates go to infinity logarithmically in the limit of error exponents $E\rightarrow 0$, corresponding to the superlinearity of the rates in that regime. Note that the DI curve must dominate the transmission curve everywhere, since an $(n,N,\lambda)$-transmission code is automatically an $(n,N,\lambda,\lambda)$-DI code.}
    \label{fig:T_vs_DI}
\end{figure}

In the rest of the paper, after giving the necessary technical tools in Section \ref{sec:preliminaries}, we present our rate-reliability bounds in Section \ref{sec:bounding}. We remark already here that while those bounds are stated for general exponents, they are most suitable for exponents below a certain threshold. After that, in Section \ref{sec:discussion} we discuss the bounds from three different angles: in Subsection \ref{ssec:linear_to_super} we show that for all (sufficiently small) $E_1,E_2>0$, the rate-reliability functions $R(E_1,E_2)$ and $\overline{R}(E_1,E_2)$ are bounded away from $0$ and $\infty$; and by letting $E_1(n), E_2(n)$ vanish slowly, we recover the main superlinear capacity results from \cite{CDBW:DI_classical} (see the schematic Figure \ref{fig:T_vs_DI}). In Subsection \ref{ssec:d=0} we show how to deal with channels characterised by dimension zero, where the previous bounds seem not to give the full picture of what is happening. In Subsection \ref{ssec:large_exponents} we discuss the regime of large error exponents (quickly vanishing errors).
Finally, we generalize the bounds to the quantum setting for classical-quantum channels, and general quantum channels under the restriction that only product states are used on the input (Subsection \ref{ssec:quantum}). We close with a brief discussion and conclusions in Section \ref{sec:conclusions}.













\section{Preliminaries}
\label{sec:preliminaries}
Our technical results rely on the methods developed in \cite{CDBW:DI_classical}. These are mainly based on metric studies of the output probability sets. The essential distance measures needed are defined next. The \emph{total variation distance} is a statistical distance measure which coincides with half the $L^1$ distance between the probability mass functions. Let $P$ and $Q$ be two probability distributions defined on a finite or countably infinite measurable space $\cL$, then the total variation distance is defined as
\[
  \frac{1}{2}\|P-Q\|_1 
    =\sum_{\ell\in\cL}\frac{1}{2} |P(\ell)-Q(\ell)|.
\]
The \emph{Bhattacharyya coefficient} (in quantum information called \emph{fidelity}) is given by $F(P,Q)=\sum_{\ell\in\cL}\sqrt{P(\ell)Q(\ell)}$, and it is related to the total variation distance by the following bounds:
\begin{equation}
  \label{eq:Classical_FvdG}
  1-F(P,Q) \leq \frac12 \|P-Q\|_1 \leq \sqrt{1-F(P,Q)^2}.
\end{equation}
Finally, following \cite[Sect.~3]{CDBW:DI_classical} we use a metric on a modified output set that comes out naturally for product distributions, and will allow us to work in a Euclidean space (enabling the use of the Minkowski dimensions defined above). For a probability distribution $P\in\cP(\cL)$, define the unit vector $\sqrt{P}:=(\sqrt{P(\ell)}:\ell\in\cL) \in \RR^{\cL}$. The image of $\cP(\cL)$ under the square root map is the non-negative orthant sector of the unit hypersphere in $\RR^{|\cL|}$, which we shall denote $S_+(|\cL|,1)$. 
This obeys the relation
\begin{equation}
  \label{eq:fidelity->euclidean}
  1-F(P,Q)^2 
    \leq \left| \sqrt{P}-\sqrt{Q} \right|_2^2 
    \leq 2\left(1-F(P,Q)^2\right).
\end{equation}
For a channel $W:\cX \rightarrow \cY$, we denote the output probability set $\widetilde{\cX} := W(\cX) \subset \cP(\cY)$, and hence have
\[
  \sqrt{\!\widetilde{\cX}} 
    = \left\{ \sqrt{W_x} : x\in\cX \right\} 
  \subset S_+(|\cY|,1).
\]

For the code construction, the \emph{(entropy) conditional typical set} in the output is used as the identification test $\cE_j:=\mathcal{T}_{u_j}^\delta\subset\cY^n$ of the code word $u_j$. For each $x^n\in\cX^n$ it is defined as
\begin{equation}
  \label{eq:entropy-typical-set}
  \mathcal{T}_{x^n}^\delta \! 
    := \left\{ y^n\in\mathcal{Y}^n \!: 
           \left| \log W_{x^n}(y^n) + H(W_{x^n}) \right| 
           \leq \delta\sqrt{n} \right\}.
\end{equation}
The nice properties exhibited by typical sets (they collect almost all the probability, which is almost evenly distributed among their elements, cf.~\cite[Lemmas~I.11~{\&}~I.12]{winter:PhDThesis}) allow us to bound the errors of first and second kinds (when choosing $\mathcal{T}_{u_j}^\delta$ as decision rule) through the following lemmas, which will be instrumental in the next section.

\begin{lemma}[{\cite[Lemma~2.1]{CDBW:DI_classical}}]
\label{lemma:error1}
For an arbitrary channel $W:\mathcal{X}\rightarrow\mathcal{Y}$, arbitrary block length $n$, $0<\delta\leq \sqrt{n}\log|\cY|$, and for any $x^n\in\mathcal{X}^n$, we have
\[
  W_{x^n}(\mathcal{T}_{x^n}^\delta) 
   \geq 1-2\exp\left(-\delta^2/36 K(|\mathcal{Y}|)\right),
\]
where $K(d) = (\log\max\{d,3\})^2$.
\end{lemma}

\begin{lemma}[{\cite[Lemma~3.1]{CDBW:DI_classical}}]\label{lemma:hyp_test_error2}
Let $x^n,{x'}^n \in \mathcal{X}^n$ be two input sequences such that the corresponding output probability distributions satisfy
$1-\frac12\left\|W_{x^n}-W_{{x'}^n}\right\|_1 \leq \epsilon$. Then,
\[\begin{split}
    W_{{x'}^n}(\mathcal{T}_{x^n}^\delta) 
    &\leq 2\exp\left(-\delta^2/36K(|\mathcal{Y}|)\right)\\
    &\phantom{\leq.} + \epsilon\left(1 + 2^{2\delta\sqrt{n}}2^{H(W_{x^n})-H(W_{{x'}^n})}\right).
\end{split}
\]
\end{lemma}

For both the coding and converse part, we will need the packing and covering of general sets in arbitrary dimensions. For a non-empty bounded subset $F$, the \emph{covering} problem consists in finding the minimum number $\Gamma_\delta(F)$ of closed balls of radius $\delta$ centered at points in $F$ such that their union contains $F$; in contrast, the \emph{packing} problem consists in finding the maximum number $\Pi_\delta(F)$ of pairwise disjoint open balls of radius $\delta$ centered at points in $F$. 
%Note that the centers of the balls in either case form a subset $F_0 \subset F$. For a covering, $F_0$ has to be such that for every $x\in F$ there exists $x_0\in F_0$ with $d(x,x_0)\leq\delta$, which is otherwise known as $\delta$-net. For a packing, the requirement is that for any $x_0\neq x_1\in F_0$, $d(x_0,x_1) \geq 2\delta$. A fundamental observation is that for every $\delta>0$ and $\eta>0$,
These packing and covering numbers are fundamental in geometry as they can be used to define the \emph{Minkowski dimension} (also known as covering, Kolmogorov, or entropy dimension) of a subset in Euclidean space as
\begin{equation}\label{eq:Minkowski_Dimension}
d_M(F) = \lim_{\delta\rightarrow0} \frac{\log \Gamma_\delta(F)}{-\log\delta} 
  = \lim_{\delta\rightarrow0} \frac{\log \Pi_\delta(F)}{-\log\delta}.
\end{equation}
While for smooth manifolds the topological and Minkowski dimensions coincide, the latter is especially relevant in theory of fractals, as it captures the geometric complexity of the set beyond the scope of the classical topological dimension, see the excellent textbook \cite{Falconer:fractal}, as well as \cite{Robinson:dimensions,Fraser:dimensions} for further insight. For even more general and irregular sets, it is quite common that the above limit [Eq.~\eqref{eq:Minkowski_Dimension}] does not exist. In that case, we define the \emph{upper} and \emph{lower Minkowski dimensions} through the limit superior and limit inferior, respectively: 
\begin{align}
 \overline{d}_M(F) &:= \limsup_{\delta\rightarrow0}\frac{\log \Gamma_\delta(F)}{-\log\delta} 
  = \limsup_{\delta\rightarrow0} \frac{\log \Pi_\delta(F)}{-\log\delta}, \\
 \underline{d}_M(F) &:= \liminf_{\delta\rightarrow0}\frac{\log \Gamma_\delta(F)}{-\log\delta}
  = \liminf_{\delta\rightarrow0} \frac{\log \Pi_\delta(F)}{-\log\delta}.\label{eq:lowerMinkowski}
\end{align}
The dimensions of the image of output probability set $\widetilde{\cX}$ and its square root $\sqrt{\!\widetilde{\cX}}$ are subject to the inequalities
$d_M(\widetilde{\cX}) 
 \leq d_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right) 
 \leq 2d_M(\widetilde{\cX})$.
This is because $\widetilde{\cX}$ is a Lipschitz-continuous image of $\sqrt{\!\widetilde{\cX}}$; and conversely, $\sqrt{\!\widetilde{\cX}}$ is a $\frac12$-H\"older-continuous image of $\widetilde{\cX}$, cf.~\cite{Robinson:dimensions,Fraser:dimensions}.








\section{Bounding the rate-reliability function}
\label{sec:bounding}
We divide this section into three parts. The first part (Subsection \ref{ssec:coding}) presents our lower bound on the rate-reliability function for deterministic identification, expressed in terms of the packing number of a transformed output probability set, where the radius is determined by the error exponents. This is followed by an analysis of this bound in the regime of small error exponents. Similarly, the second part (Subsection \ref{ssec:converse}) provides the upper bound on the rate-reliability function using a covering number, along with its analysis for small error exponents. Finally, in Subsection \ref{ssec:improved} we analyse the rate bounds when the error exponents are restricted to certain suitable subsets in the vicinity of $0$, raising our lower bound and decreasing our upper bound in the regime of small errors. Analogous to the results of \cite{CDBW:DI_classical}, this illuminates the separate roles of the lower and upper Minkowski dimension. 





\subsection{Lower bound on the rate-reliability function (coding)}
\label{ssec:coding}
We give our lower bound in Theorem \ref{thm:coding}, and analyse the regime of small error exponents in Corollary \ref{cor:coding}. In all the following sections we will use the shorthand $c=1/36K(|\cY|)$.

\begin{theorem}
\label{thm:coding}
For any $0<t<1$, and $E(n)>0$, there exists a DI code on block length $n$ with error exponents $E_1(n)\geq E(n)-\frac1n$ and $E_2(n)\geq E(n)-\frac3n$, and its rate lower-bounded 
\begin{equation}
\label{eq:coding_rate_final}
\begin{split}
    R(n) &\geq (1-t)\log\left[\Pi_{\sqrt[4]{\frac{6E(n)}{ct^2}}} \left(\!\sqrt{\!\widetilde{\cX}}\right)\right]\\
    &\phantom{\geq.}-H(t,1-t)-O\left(\frac{\log n}{n}\right).
\end{split}
\end{equation}
\end{theorem}
\begin{proof}
Inspired by the code construction in \cite{CDBW:DI_classical}, we use the typical sets of input strings as decoding elements, for which we know that Lemmas \ref{lemma:error1} and \ref{lemma:hyp_test_error2} give us upper bounds on the errors of first and second kind. We want to impose exponentially decreasing errors, construct a code, and then analyze the resulting rate. Looking at Lemma \ref{lemma:error1} it is immediate to see that for the error of first kind, the exponential decrease follows by letting $\delta:=\tau\sqrt{n}$, so that
\begin{equation}
  \label{eq:bound_1error}
  \lambda_1 \leq 2\exp(-c\tau^2 n).
\end{equation}

To bound the error of second kind, additional effort is required. We want to use Lemma \ref{lemma:hyp_test_error2}, which requires a minimum separation $1-\frac12\|W_{u_j}-W_{u_k}\|_1 \leq \epsilon$ between any two different code words $u_j\neq u_k\in\cX^n$. That is, the output probability distributions generated by code words need to form a packing with pairwise total variation distance being almost exponentially close to $1$. 
%For this reason and also because of
Following \cite{CDBW:DI_classical}, starting from
\begin{equation}
  \label{eq:fidelity-bound}
  1-\frac12\|W_{x^n}-W_{{x'}^n}\|_1
   \leq F\left(W_{x^n},W_{{x'}^n}\right)
   =    \prod_{i=1}^n F\left(W_{x_i},W_{x_i'}\right),
\end{equation}
we study the negative logarithm of the left hand side. Letting $u_j=x^n=x_1\dots x_n$ and $u_k=x'^n=x'_1\dots x'_n$, we have
\begin{equation}\begin{split}
  \label{eq:coding_TVD_to_Euc}
  -\ln\left(\! 1-\frac12\|W_{x^n}-W_{{x'}^n}\|_1\! \right)\! 
   &\geq \sum_{i=1}^n -\ln F\left(W_{x_i},W_{x_i'}\right) \\
   &%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
    =    \frac12 \sum_{i=1}^n -\ln F\left(W_{x_i},W_{x_i'}\right)^2 \\
   &%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
    \geq \frac12 \sum_{i=1}^n 1-F\left(W_{x_i},W_{x_i'}\right)^2 \\
   &%\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
    \geq \frac14 \sum_{i=1}^n \left| \sqrt{W_{x_i}}-\sqrt{W_{x_i'}}\right|_2^2. \\
   %&=    \frac14 \left| \bigoplus_{i=1}^n \sqrt{W_{x_i}} - \bigoplus_{i=1}^n \sqrt{W_{x_i'}} \right|_2^2.
\end{split}\end{equation}
%Despite this process might seem unnecessarily abstract at first glance, 
Notice the important simplification that this achieves: before, we had to construct and analyze a packing of full sequences of length $n$ in total variation distance, and now we just need a letter-wise Euclidean packing to ensure a distance between output probability distributions exponentially close to 1. Let $\cX_0$ be a maximum-size Euclidean packing in $\sqrt{\!\widetilde{\cX}}$ of radius $\beta$ and $\Pi_{\beta} := \Pi_{\beta}\left(\!\sqrt{\!\widetilde{\cX}}\right) = |\cX_0|$ the corresponding packing number.

Next we choose a maximum-size code $\cC_t\subset\cX_0^n$ such that the letters are elements of the packing and there is a minimum Hamming distance $d_H(x^n,x'^n)>tn$ between code words $x^n\neq x'^n\in\cC_t$. Then, in the last sum in Eq.~\eqref{eq:coding_TVD_to_Euc} at least $tn$ terms are $\geq 4\beta^2$ (and could be $0$ in the others), therefore
\begin{align*}
    -\ln\left( 1-\frac12\|W_{x^n}-W_{{x'}^n}\|_1 \right) 
    &\geq \frac14 \sum_{i=1}^n \left| \sqrt{W_{x_i}}-\sqrt{W_{x_i'}}\right|_2^2\\
    &\geq tn\beta^2.
  \label{eq_coding_epsilon_distance}
\end{align*}
%\aw{(I have removed a $\frac14$ in the last equation, please check that this and the rest of the proof are consistent with the calculation in the ICC version. I think there I had removed the O(1) placeholders in the error bounds and replaced them with explicit constants, cf. the statement of the Theorem.)}
Thus we can choose $\epsilon = \exp(-t\beta^2 n)$. Using simple combinatorics, it is evident that the Hamming ball around any point in $\cX_0^n$ with radius $tn$ contains at most $\leq \binom{n}{tn} \Pi_\beta^{tn}$ elements. Therefore, any maximal code with a Hamming distance of $tn$ must have at least the following number of code words, which is the ratio of the total number of elements to the size of the Hamming ball (otherwise, the code could be extended):
\[
  |\cC_t|\geq\Pi_\beta^{n(1-t)} 2^{-nH(t,1-t)}.
\]
The Gilbert-Varshamov bound \cite{Gilbert:combinatorics,Var:combinatorics} provides a similar (asymptotically equivalent) result, by constructing a linear code over the prime field $\mathbb{F}_p$, after reducing $\cX_0$ to the nearest smaller prime cardinality $p\geq \frac12|\cX_0|$ (Bertrand's postulate).
%
It only remains to bound the entropy difference $H(W_{x^n})-H(W_{x'^n})$ to apply Lemma \ref{lemma:hyp_test_error2}. As these entropies are in the interval $[0;n\log|\cY|]$, we can just partition the code into $S=\lceil n\log|\cY|\rceil$ parts $\cC_t^{(s)}$ ($s=1,\ldots,S$) in such a way that all $j\in\mathcal{C}_t^{(s)}$ have $H(W_{u_j})\in[s-1;s]$, meaning that any two $j,k\in\cC_s$ satisfy $|H(W_{u_j})-H(W_{u_k})|\leq 1$. We can now define $\cC$ as the largest $\cC_t^{(s)}$, which by the pigeonhole principle has $|\cC| \geq| \cC_t|/\lceil n\log|\cY|\rceil$ elements. 
We thus get the rate of the code as 
\begin{equation}
\label{eq:coding_rate_1}
\begin{split}
    R(n) 
    &=    \frac1n{\log|\cC|} 
    \geq \frac1n{\log|\cC_t|} 
           - \frac1n{\log \left\lceil n\log|\cY| \right\rceil} \\
    %&\geq(1-t)(\underline{d}_M-\phi)\log\left(\frac{K}{\beta}\right)\\
    &\geq (1-t)\log\Pi_\beta - H(t,1-t) 
          -O\left(\frac{\log n}{n}\right).
    %\phantom{=}-H(t,1-t)-O\left(\frac{\log n}{n}\right).
\end{split}
\end{equation}
To this code, we finally apply Lemma \ref{lemma:hyp_test_error2} to bound the error of second kind, and obtain
\[\begin{split}
\lambda_2
%=W_{{x'}^n}(\mathcal{T}_{x^n}^{\tau \sqrt{n}}) 
%   &\leq 2\exp(-c\tau^2 n)
%   + 6 \exp(2n\tau) \exp(-t\beta^2 n)\\
   &\leq 2\exp(-c\tau^2 n)
   + 3 \exp(2\tau n - t\beta^2 n)\\
   &\leq 5 \exp(-c\tau^2 n),
\end{split}\]
where the last inequality is true if $-c\tau^2 \geq 2\tau - t\beta^2$. The two error exponents in the present construction are: $E_1(n) \geq c\tau^2 - \frac1n$ and $E_2(n) \geq c\tau^2 - \frac3n$. Solving the quadratic constraint we obtain
\[
  \tau \leq \frac{-2+\sqrt{4+4ct\beta^2}}{2c}
       =    \frac{\sqrt{1+ct\beta^2}-1}{c}.
%       \leq \frac12 t\beta^2,
\]
We may assume w.l.o.g.~$ct\beta^2 \leq 1$ (see the definition of $c$, the range of $t$ and the meaningful range of $\beta$; otherwise our rate lower bound is trivial), hence by the concavity of the square root the above condition will hold for 
$\tau = \left(\sqrt{2}-1\right)t\beta^2$. 
Then, the error exponents are as claimed in the theorem, with $E(n) = \frac16 ct^2\beta^4$. To conclude, we solve for $\beta$ and substitute the solution $\beta = \sqrt[4]{\frac{6 E(n)}{ct^2}}$ into Eq.~\eqref{eq:coding_rate_1}.
\end{proof}

\begin{corollary}
\label{cor:coding}
Given any $\eta>0$ and $0<t<1$, and small enough $E(n)>0$. Then for all sufficiently large $n$ there exists a DI code with $E_1(n),\, E_2(n) \geq E(n)-\frac3n$ and the following lower bound on the rate in terms of the lower Minkowski dimension:
\begin{equation}
 \label{eq:coding_corollary}
\begin{split}
   R(n) &\geq \frac{1-t}{4} \left[\underline{d}_M\left(\!\sqrt{\!\widetilde{\cX}}\right)-\eta\right] \log\frac{ct^2}{6E(n)} \\
   &\phantom{\geq.}
     - H(t,1-t)-O\!\left(\frac{\log n}{n}\right).
\end{split}
\end{equation}
\end{corollary}
\begin{proof}
By definition of the lower Minkowski dimension [Eq.~\eqref{eq:lowerMinkowski}], for any $\eta>0$ there is an $E_0>0$ such that for all $E(n)\leq E_0$ we have
\[
  \log\left[\Pi_{\sqrt[4]{\frac{6 E(n)}{ct^2}}} \left(\!\sqrt{\!\widetilde{\cX}}\right)\right]
    \geq (\underline{d}_M-\eta)\log \sqrt[4]{\frac{ct^2}{6E(n)}}.
\]
The proof is completed by direct substitution of the above expression into Eq.~\eqref{eq:coding_rate_final} and taking the fourth square root out of the logarithm.
\end{proof}

\begin{remark}\label{remark:coding}
While the lower bound in Theorem \ref{thm:coding} is well suited for small error exponents 
%(even allowing Corollary \ref{cor:coding} when they are small enough)
, it becomes trivial for bigger exponents. Clearly, for $E(n)\geq 2ct^2/3$ the packing radius $\beta\geq\sqrt{2}$ is at least the maximum possible distance between any two elements in $\sqrt{\!\widetilde{\cX}}$, so the packing number can only be 1. This results on a non-positive (trivial) lower bound on the rate in \eqref{eq:coding_rate_final}.
The reason behind this behaviour and further remarks on the regime of large error exponents can be found in Section \ref{ssec:large_exponents}. 
However, for the main part of the discussion, where the small exponent regime is the relevant one, this will not be a problem.
\end{remark}








\subsection{Upper bound on the rate-reliability function (converse)}
\label{ssec:converse}
Following the structure of the previous section, we start presenting our general upper bound in Theorem \ref{thm:converse}, and analyse the regime of small error exponents in Corollary \ref{cor:converse}.
\begin{theorem}
\label{thm:converse}
For any DI code on block length $n$ with positive error exponents, $E_1(n),\, E_2(n) \geq E(n) > 0$, the rate is upper-bounded
\begin{equation}
  \label{eq:converse_rate}
  R(n) \leq \log\left[\Gamma_{\frac12{\sqrt{1-e^{-E(n)/2}}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right].
\end{equation}
%where $E(n)=\min\{E_1(n),E_2(n)\}$. 
%Thus, the optimistic maximum rate is finite: $\bar{R}(E_1,E_2) < \infty$.
\end{theorem}

\begin{proof}
Any DI code satisfies $\frac12\|W_{u_j}-W_{u_k}\|_1 \geq 1-\lambda_1-\lambda_2$ for any two different code words $u_j\neq u_k\in\cX^n$, thus
\[
  1-\frac12\|W_{u_j}-W_{u_k}\|_1 \leq 2e^{-E(n)n}.
\]
%where for readability we are omitting the dependence of $E$ on $n$ on the exponents in the proof steps. 
Using the relations between fidelity and total variation distance in Eq.~\eqref{eq:Classical_FvdG}, this yields
\begin{equation}
\label{eq:converse_maximum_F}
%\begin{split}
    F(W_{u_j},W_{u_k})^2 \leq 1-\left(1-2e^{-E(n)n}\right)^2
%    &=4e^{-E(n)n}\left(1-e^{-E(n)n}\right)\\
    < 4e^{-E(n)n}.
%\end{split}
\end{equation}
This is the maximum possible fidelity between the output distributions of any two input code words of a good identification code. Our strategy consists in creating a covering such that elements in the same ball have a fidelity higher than the limit stipulated by Eq.~\eqref{eq:converse_maximum_F}. In other words, we will ensure that elements in the same ball cannot be well-distinguishable code words, reducing our problem to upper-bounding the cardinality of the covering. 
%As we do not know the particular size of the needed covering, we leave it as a parameter $\delta$. 

We start with a Euclidean $\frac{\delta}{2}$-covering $\cX_0$ of the square root output probability set $\sqrt{\!\widetilde{\cX}}$ having $|\cX_0|=\Gamma_{\delta/2}\!\left(\!\sqrt{\!\widetilde{\cX}}\right) =: \Gamma_{\delta/2}$ elements,  meaning that for every $x\in\cX$ there is a $\xi\in\cX_0$ such that
\[
  \left| \sqrt{W_x}-\sqrt{W_\xi} \right|_2 \leq \frac{\delta}{2}.
\]
Hence, our input set $\cX$ is partitioned according to the $\frac{\delta}{2}$-balls around each $\sqrt{W_\xi}$ with $\xi\in\cX_0$. That is, to each ball corresponds a set of the input sequences that generate only probability distributions in that ball: $\cX=\dot{\bigcup}_{\ell=1}^{|\cX_0|}\cX_\ell$, such that for all $\ell$ and for any two $x,x'\in\cX_\ell$ we have $|\sqrt{W_x}-\sqrt{W_{x'}}|_2 \leq \delta$. We know from Eq.~\eqref{eq:fidelity->euclidean} that $1-F(W_x,W_{x'})^2\leq|\sqrt{W_x}-\sqrt{W_{x'}}|_2^2$. Then, for each $\ell$ and for any two $x,x'\in\cX_\ell$ we have:
\[
F(W_x,W_{x'})^2\geq1-\delta^2.
\]
In block length $n$, this gives rise to the partition $\cX^n = \dot{\bigcup}_{\ell^n} \cX_{\ell^n}$, where $\cX_{\ell^n} := \cX_{\ell_1}\times\dots\times\cX_{\ell_n}$ and $\ell^n\in[|\cX_0|]^n$ is the sequence that characterizes the previously defined ball for each letter.  This has the property that for all $x^n,x'^n\in\cX_{\ell^n}$, 
%(i.e.~for all input sequences which share the same ball on each letter):
\[
  F(W_{x^n},W_{x'^n})^2 \geq (1-\delta^2)^n.
\]
If we now choose $1-\delta^2=e^{-E(n)/2}$, we find that as long as $e^{-E(n)n/2}\geq 4e^{-E(n)n}$ (i.e.~$E(n)n/2 \geq \ln 4$) the fidelity between elements of the same $\cX_{\ell^n}$ is bigger than the maximum possible fidelity that the DI code allows. In other words, there cannot be two code words in the same $\cX_{\ell^n}$. So we can upper bound the number of elements in our code by counting the cardinality of the index $[|\cX_0|]^n$: 
$N\leq |\cX_0|^n = \left(\Gamma_{\delta/2}\right)^n$. Finally, as $\delta=\sqrt{1-e^{-E(n)/2}}$,
\[
    R(n)=    \frac{1}{n}\log N 
        \leq \log\left[\Gamma_{\frac12{\sqrt{1-e^{-E(n)/2}}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right],
        %\leq (\overline{d}_M+\epsilon) \log\left(\frac{2K}{\sqrt{1-e^{-E/2}}}\right).
\]
concluding the proof.
\end{proof}

\begin{corollary}
\label{cor:converse}
For any $\eta>0$ and all sufficiently large $n$, for small $E(n)>0$ in the above theorem we can upper-bound the rate of any DI code with error exponents $E_1(n),\,E_2(n) \geq E(n)$ in terms of the upper Minkowski dimension $\overline{d}_M$:
\begin{equation}
  \label{eq:converse_corollary}
  R(n)\leq \frac12\left[\overline{d}_M\left(\!\sqrt{\!\widetilde{\cX}}\right)+\eta\right] \log\frac{8}{E(n)} + O\left( E(n) \right).
\end{equation}
\end{corollary}
\begin{proof}
%As we have chosen $\delta=\sqrt{1-e^{E/2}}$, the error exponents sequence $(E_k)$ automatically yields another one with elements $\delta_k>0$ also converging to zero. 
By the definition of the upper Minkowski dimension, for all $\eta>0$ there exists an $E_0>0$ such that for all $E(n) \leq E_0$ it holds 
\begin{equation}\label{eq:plotted_upper_bound}
\log\left[\Gamma_{\frac12\sqrt{1-e^{-E(n)/2}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right]
\!\leq\! \left(\overline{d}_M+\eta\right) 
\log\frac{2}{\sqrt{1-e^{-E(n)/2}}}.
\end{equation}
For $E(n)$ small (close to zero) we can expand the log as follows,
%generalized Puiseux series
\begin{equation}
  \label{eq:Puiseux_series}
  \log \frac{2}{\sqrt{1-e^{-E(n)/2}}}
    \leq \frac12\log\frac{8}{E(n)} + O\left(E(n)\right).
\end{equation}
%Then, the logarithm of the covering number is
%\[
%\log\left[\Gamma_{\frac12{\sqrt{1 -e^{E/2}}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right]
%\leq\frac12(\overline{d}_M+\eta)\log\left(\frac{8}{E(n)}\right)+O(E).
%\]
The proof is completed by substituting the above expressions into Eq.~\eqref{eq:converse_rate}. 
\end{proof}








\subsection{Improved bounds for subsets of error exponents}
\label{ssec:improved}
In Corollaries \ref{cor:coding} and \ref{cor:converse} we have lower- and upper-bounded the rate-reliability function in terms of the lower and upper Minkowski dimensions, respectively. These bounds are universal in that they hold for all error exponents $E$ smaller than some threshold $E_0=E_0(\eta)$ that decreases with the parameter $\eta>0$. 
We might however be interested only in certain (infinitely many) sufficiently small values of $E$. To capture this, we consider subsets $\cE \subset (0;+\infty)$ that have $0$ as an accumulation point (meaning that $\cE$ contains sequences converging to $0$). 

Let us start with a subset $\cE_g$ such that
\[
  \lim_{\cE_g \ni E\rightarrow 0} \frac{\log\left[\Pi_{\sqrt[4]{6E/ct^2}} \left(\!\sqrt{\!\widetilde{\cX}}\right)\right]}{-\log\sqrt[4]{6E/ct^2}}
  = \overline{d}_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right).
\]
This is a ``good'' subset of error exponents, in the sense that it implies the convergence of $(\log\Pi_\delta)/(-\log\delta)$ to the best possible value (the limsup) as $\delta\rightarrow 0$. Furthermore, due to the properties of $\cE_g$, given any $\eta>0$ and its threshold $E_0>0$, there exist values $\cE_g \ni E \leq E_0$, and for those we have
\begin{equation}
  \label{eq:improvement_coding}
  \log\left[\Pi_{\sqrt[4]{\frac{6E}{ct^2}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right]
  \geq \frac{1}{4}(\overline{d}_M-\eta)\log\frac{ct^2}{6E}.
\end{equation}
Substituting the above expression into Eq.~\eqref{eq:coding_rate_final}, we obtain an improved achievability bound in Corollary \ref{cor:coding}, but now with the upper Minkowski dimension and under the added condition that the error exponents $E(n) \in \cE_g$.

We could similarly choose a ``bad'' subset $\cE_b\subset (0;+\infty)$ such that the covering numbers converge to the lower Minkowski dimension:
\[
  \lim_{\cE_b \ni E \rightarrow 0} \frac{\log\left[\Gamma_{\frac12\sqrt{1-e^{-E/2}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right]}{-\log\left(\frac12\sqrt{1-e^{-E/2}}\right)}
  = \underline{d}_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right).
\]
As before, for all $\eta>0$ and its associated threshold $E_0$, there exist values $\cE_b \ni E \leq E_0$, for all of which
\begin{equation}
  \label{eq:improvement_converse}
  \log\left[\Gamma_{\frac12\sqrt{1-e^{-E/2}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right]
   \leq \frac12(\underline{d}_M+\eta)\log\frac{8}{E}+O(E),
\end{equation}
where we have followed the last steps in the proof of Corollary \ref{cor:converse}, using again the expansion in Eq.~\eqref{eq:Puiseux_series}. Plugging the above expression into Eq.~\eqref{eq:converse_rate}, we obtain an improved upper bound on the rate-reliability function in Corollary \ref{cor:converse}, but now with the lower Minkowski dimension and under the added condition that the error exponents $E(n) \in \cE_b$.





\section{Discussion: constant vs vanishing exponents, dimension zero, and quantum channels}
\label{sec:discussion}
In this section, we demonstrate how the new reliability framework developed in this study aligns with the established results in \cite{CDBW:DI_classical}. We begin by deriving bounds for the rate-reliability functions in the asymptotic regime ($n \rightarrow \infty$), showing how these bounds allow us to recover the main capacity results from \cite{CDBW:DI_classical}. Next, we consider the special case where the Minkowski dimension of the output probability set is zero -- a scenario where Corollaries \ref{cor:coding} and \ref{cor:converse} do not fully capture the underlying behavior meaningfully. We illustrate an approach to handle such cases with a straightforward example. Finally, we extend our results to both classical-quantum and (fully) quantum channels, under the restriction that encoding is limited to product states. Together, these sections encompass the contents of \cite{CDBW:DI_classical} through reliability arguments, offering a new perspective on deterministic identification.

\subsection{From linear to superlinear DI rates}
\label{ssec:linear_to_super}
The results from the preceding section tell us that the superlinear rates of DI codes discussed in the introduction are lost when imposing exponentially vanishing errors. Indeed, when $E_1,\, E_2 > 0$, then Corollary \ref{cor:converse} shows for every $\eta>0$, letting $E = \min\{E_1,E_2,E_0(\eta)\}$, that 
\begin{equation}
  \label{eq:rate-reliabiliyty-outerbound}
  \overline{R}(E_1,E_2) 
   \leq \frac12 \left(\overline{d}_M + \eta\right)
                \log\frac{8}{E} + O\left(E\right) 
   < \infty.
\end{equation}
This is because we can take the limit in Eq.~\eqref{eq:converse_corollary} for every parameter $\eta>0$, as long as $E$ is below the cutoff exponent $E_0(\eta)$; at the same time, the DI rate can only increase if we lower $E$.

Conversely, and similarly, for every $\eta>0$ and $0<t<1$, and $E > 0$ sufficiently small, Corollary \ref{cor:coding} implies for all $E_1,\,E_2 \leq E$ that 
\begin{equation}
  \label{eq:rate-reliabiliyty-innerbound}
  R(E_1,E_2) 
   \geq \frac{1}{4} (1\!-\!t)(\underline{d}_M-\eta) 
               \log\frac{ct^2}{6E} - H(t,1\!-\!t), 
\end{equation}
which is positive for sufficiently small $E>0$, and indeed diverges to infinity as $E\rightarrow 0$.

On the other hand, the general superlinear scaling of DI codes featured in \cite{CDBW:DI_classical} does reappear when we consider (slowly) vanishing error exponents $\lim_{n\rightarrow\infty} E_1(n) = \lim_{n\rightarrow\infty} E_2(n) = 0$. 
Evidently, we still need small errors $\lambda_1,\,\lambda_2 \ll 1$ as $n\rightarrow \infty$ to have a good code, so the $n E_i(n)$ must remain bounded away from $0$; more precisely, we need $E_1(n),\, E_2(n) \geq \omega(1/n)$ for the coding part and weak converse, and $E_1(n),\, E_2(n) \geq \Omega(1/n)$ for the strong converse. By direct substitution of these two ``settings'' into the results above we can recover the results from \cite{CDBW:DI_classical}. We point out that those results are bounds on the superlinear capacity (the maximum asymptotic rate), which we can recover as follows in the present notation: 
\[
  \dot{C}_\text{DI}(W) 
    = \sup_{\myfrac{E_i(n)\rightarrow 0,}{nE_i(n)\rightarrow\infty}} 
      \left( \liminf_{n\rightarrow\infty} \frac{1}{\log n} \max_{\myfrac{\text{errors }\lambda_i\text{ s.t.}}{\log \lambda_i \leq -nE_i(n)}} R(n) \right),
\]
where the inner maximum is over all DI codes with errors of first and second kind bounded as $\lambda_i \leq 2^{-nE_i(n)}$, and the outer supremum is over functional dependencies of the exponents on $n$ going to $0$ sufficiently slowly: $\omega(1/n) \leq E_i(n) \leq o(1)$.

\begin{theorem}[{Cf.~\cite[Thm.~5.7]{CDBW:DI_classical}}]
\label{thm:recover_pessimistic}
The superlinear DI capacity $\dot{C}_{\text{DI}}(W)$ for the channel $W:\cX\rightarrow\cY$ is bounded in terms of the lower Minkowski dimension of the set $\sqrt{\!\widetilde{\cX}}$ as follows (for slowly vanishing error exponents $\displaystyle\lim_{n\rightarrow\infty} E_i(n) = 0$):
\[
  \frac14\underline{d}_M \!\left(\!\sqrt{\!\widetilde{\cX}}\right)
    \leq \dot{C}_{\text{DI}}(W)
    \leq \frac12\underline{d}_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right).
\]
\end{theorem}
\begin{proof}
We start with the coding part taking $E_1(n) = E_2(n) = \frac{\log n}{n}$ in Eq.~\eqref{eq:coding_corollary} (Corollary \ref{cor:coding}):
\[\begin{split}
R(n) 
    &\geq \frac{1-t}{4} \left[\underline{d}_M \left(\!\sqrt{\!\widetilde{\cX}}\right)-\eta\right] \log\frac{ct^2n}{6\log n}\\
    &\phantom{\geq.}-H(t,1\!-\!t)-O\!\left(\frac{\log n}{n}\right).
\end{split}\]
We can maximize this rate choosing the parameter $t=t(n)>0$ going to zero slowly enough for increasing $n$, such that $\lim_{n\rightarrow\infty} \frac{\log(t^2n/\log n)}{\log n} = 1$ (we can for example choose $t=1/\log n$), and $\eta=\eta(n)>0$ also going to zero as $n\rightarrow\infty$. Then, when taking the limit of large $n$ we find
\[
  \dot{C}_{\text{DI}}(W)
    \geq \liminf_{n\rightarrow\infty} \frac{1}{\log n} R(n)
    \geq \frac14 \underline{d}_M\left(\!\sqrt{\!\widetilde{\cX}}\right),
\]
completing the direct part.

Similarly, for the upper bound we need to take $\cE_b\ni E_1(n) = E_2(n) \geq \frac{C}{n}$ in Eq.~\eqref{eq:improvement_converse}, the (pessimistic) upper bound for bad sequences of errors. Choosing also $\eta=\eta(n)$ going slowly enough towards $0$ for increasing $n$, such that the error exponents, which are upper bounded by the threshold $E_0\!\left(\eta(n)\right)$, can also go to $0$ slowly, we find
\[\begin{split}
  \dot{C}_{\text{DI}}(W)
    &\leq \liminf_{n\rightarrow\infty} \frac{1}{2\log n} \underline{d}_M\left(\!\sqrt{\!\widetilde{\cX}}\right) \log\frac{8n}{C} + O\left(\frac1n\right)\\
    &= \frac12 \underline{d}_M\left(\!\sqrt{\!\widetilde{\cX}}\right),
\end{split}\]
and we are done.
\end{proof}

The capacity $\dot{C}_{\text{DI}}$ has to be approach by all sufficiently large values of $n$, i.e. we take the lowest convergence value of the limit (the inferior limit), this is why it is commonly referred to as a \textit{pessimistic capacity}. It might be possible to find a subsequence of block lengths $n_k\rightarrow\infty$ for which the rate converges to a higher number. This leads to the definition of the \textit{optimistic capacity}, which provides information about the best achievable rates (though only for specific block lengths) through the superior limit:
\[
  \dot{C}_{\text{DI}}^\text{opt}(W) 
  = \sup_{\myfrac{E_i(n)\rightarrow 0,}{nE_i(n)\rightarrow\infty}} 
      \left( \limsup_{n\rightarrow\infty} \frac{1}{\log n} \max_{\myfrac{\text{errors }\lambda_i\text{ s.t.}}{\log \lambda_i \leq -nE_i(n)}} R(n) \right),
\]
with the maximum inside and the supremum outside ranging over the same objects as in the formula for the pessimistic capacity. 

We refer the reader to \cite[Sect.~7.1]{CDBW:DI_classical} for an extensive discussion on the origins, comparison, and interpretation of the optimistic and pessimistic capacities, or else \cite{Ahlswede2006} which inspired it.
We can recover the general superexponential optimistic capacity bounds in \cite{CDBW:DI_classical} with similar methods as in the previous theorem.

\begin{theorem}[{Cf.~\cite[Thm.~5.9]{CDBW:DI_classical}}]\label{thm:recover_optimistic}
The optimistic superlinear DI capacity $\dot{C}_{\text{DI}}^{\text{opt}}(W)$ of a channel $W:\cX\rightarrow\cY$ is bounded in terms of the upper Minkowski dimension of $\sqrt{\!\widetilde{\cX}}$ as follows (for slowly vanishing error exponents $\displaystyle\lim_{n\rightarrow\infty} E_i(n) = 0$):
\[
  \frac14\overline{d}_M\! \left(\!\sqrt{\!\widetilde{\cX}}\right)
    \leq \dot{C}_{\text{DI}}^{\text{opt}}(W) 
    \leq \frac12\overline{d}_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right).
\]
\end{theorem}
\begin{proof}
We follow the same steps as in the previous proof. For the direct part we need to take $\cE_g\ni E_1(n),\, E_2(n) \geq \frac{C}{n}$ in the (optimistic) lower bound for good sequences of error exponents [Eq.~\eqref{eq:improvement_coding}], and similarly choose $t=t(n)$ and $\eta=\eta(n)$ going to $0$ simultaneously and sufficiently slowly as $n\rightarrow\infty$. Then,
\[\begin{split}
  \dot{C}_{\text{DI}}^{\text{opt}}(W)
    &\geq \limsup_{n\rightarrow\infty} \frac{1}{4\log n} \left[\overline{d}_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right)-\eta(n)\right]\log\frac{ct^2n}{6C}\\
    &= \frac14\overline{d}_M\! \left(\!\sqrt{\!\widetilde{\cX}}\right).
\end{split}\] 

For the converse we take $E_1(n) = E_2(n) = \frac{\log n}{n}$ in Eq.~\eqref{eq:converse_corollary} and $\eta=\eta(n)$ going to zero slowly enough, and find
\[\begin{split}
  \dot{C}_{\text{DI}}^{\text{opt}}(W)
    &\leq \limsup_{n\rightarrow\infty} \frac{1}{2\log n} \left[\overline{d}_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right)+\eta(n)\right] \log\frac{8n}{\log n}\\
    &\phantom{\leq.}+ O\left( \frac1n \right) = \frac12\overline{d}_M\! \left(\!\sqrt{\!\widetilde{\cX}}\right),
\end{split}\]
completing the proof.
\end{proof}

We have just seen that we can recover the capacity bounds in \cite{CDBW:DI_classical} from the rate-reliability bounds derived above. However, we have to impose a slow decrease of the errors in block length. The convergence of the rate bounds towards the capacity is also very slow, as shown in Figure~\ref{Fig:finite_n}.
%\aw{How were these plots made, and using which bounds?}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{Finite_n.png}
    \caption{\small Tendency of the upper and lower rate bounds towards the capacity upper and lower bounds for increasing block length, in an example case with $d_M=1$ and $E_1=E_2=E(n)= 1/n$. The upper bound (blue line) is a plot of Eq.~\eqref{eq:plotted_upper_bound} with $\eta(n)=1/(\log n)$, and the lower bound (black line) a plot of Eq.~\eqref{eq:coding_corollary} with $t(n)^2={3}/{(c\log n)}$ and $\eta(n)=1/n$, following the conditions described in the proofs of Theorems \ref{thm:recover_pessimistic} and \ref{thm:recover_optimistic} above.}
    \label{Fig:finite_n}
\end{figure}

%\aw{The last paragraph needs to be expanded and the substitutions made explicit; also the final results should be stated, matching the theorems from \cite{CDBW:DI_classical}. The above was short in the ICC version but there is no reason for such brevity here.}
%\pau{What do you think about this style? Showing that we recover the results by proving the theorems through the tools and results of the present paper. There are still some things that should be brushed in this version, like the asymptotic limits on the improved bounds which work for good and bad sequences of errors, which should be explicit on the proof.}











\subsection{Zero-dimensional output probability sets}
\label{ssec:d=0}
%\aw{Caution! We do not actually define a non-asymptotic rate-reliability function. I hope I have explained sufficiently already why that is a bad idea, especially for $n$-dependent $E_1$ and $E_2$. For the following I recommend we consider $E(n) \equiv E$ constant, and write $\max R(n)$ (maximum over the codes with prescribed errors) rather than $R(n)$, which is a function of the code.}

In Corollaries \ref{cor:coding} and \ref{cor:converse}, and in Section~\ref{ssec:improved}, we have demonstrated how to bound the rate $R(n)$ of a code with small error exponents using the lower and upper Minkowski dimensions featured in the main pessimistic and optimistic capacity results of \cite{CDBW:DI_classical}. 
%While in that previous work the Minkowski dimensions appeared naturally, we observe now that they only manifest given small enough error exponents. As a matter of fact, in the last Section \ref{ssec:linear_to_super}, we have seen that the old capacity results from \cite{CDBW:DI_classical} can only be recovered for smallest possible error exponents such that the errors vanish for increasing $n$. 
This could cause some confusion on how to approach channels for which the output probability set (equivalently its square-root set) has Minkowski dimension $d_M\!\left(\!\sqrt{\!\widetilde{\cX}}\right)=0$, particularly in the regime of small error exponents, where the aforementioned corollaries apply. Indeed, in such a case, the lower bound given in Corollary \ref{cor:coding} becomes trivial, and the upper bound only tells us that $R(n) \leq o\left(\log\frac{1}{E(n)}\right)$ for small $E(n)\leq E_1(n),\,E_2(n)$. 
However, we can obtain much more meaningful information if we go back to Theorems \ref{thm:coding} and \ref{thm:converse} (for the lower and upper bounds respectively) and impose the regime of small errors only at an appropriate point.

Let us demonstrate this through two examples. To state the first, we recall the definition of the \emph{Bernoulli channel} $B:[0;1] \rightarrow \{0,1\}$ on input $x\in[0;1]$ (a real number), which outputs a binary variable distributed according to the Bernoulli distribution $B_x$ with parameter $x$:
\begin{equation*}
    \label{eq:BernoulliChannel}
     B(y|x) = B_x(y) = xy + (1-x)(1-y)
      = \begin{cases}
          x   & \text{for } y=1, \\
          1-x & \text{for } y=0.
        \end{cases}
\end{equation*}

\begin{example}[{Cf.~\cite[Subsect.~5.4]{CDBW:DI_classical}}]
\label{ex:Bernoulli}
For a real number $a>1$, let $\cX_a:=\{0\}\cup\{a^{-k}:0\leq k\in\mathds{N}_0\}$ be a set of numbers in the interval $[0,1]$. 
Then, the maximum rate $R(n)$ of a DI-code for $B\vert_{\cX_a}$, the Bernoulli channel restricted to inputs from $\cX_a$, for small and equal error exponents $E(n):=E_1(n)=E_2(n)>0$, is bounded as follows:
\[
  \log\log \frac{1}{E(n)} - O(1)
    \leq \max R(n)
    \leq \log\log\frac{1}{E(n)} + O(1).
%\[\begin{split}
%    &R(n) \geq (1-t)\log\log_a\left[\frac{t\sqrt{c}(\sqrt{a}-1)^2}{36\sqrt{6E(n)}}\right]-H(t,1-t)-O\left(\frac{\log n}{n}\right),\\
%    &R(n)
%    \leq \log\log_a\left[\frac{2a^2}{E(n)}\right]+O\left(\frac{E(n)}{-\log E(n)}\right).
%    \end{split}\]
%    \end{comment}
\]
The capacity of this channel in the suitably defined $n\log\log n$ scale is thus $\mathring{C}_\text{DI}(\cB|_{\cX_a})=1$.
\end{example}

\begin{proof}
As the set of all probability distributions with binary output $\cP(\{0,1\})$ (with total variation distance) is isometric to the interval $[0;1]$ (with the usual metric on real numbers), the covering (or packing) of the output probability set of a channel with binary output (like $B|_{\cX_a}$) corresponds to the covering (or packing) of its input, which is $\cX_a$. 
It is not hard to see that
\begin{equation}
  \label{eq:example_covering_bounds}
  \log_a\frac{a-1}{3\epsilon} 
    \leq \Gamma_{\epsilon}(\cX_a) 
    \leq \log_a\frac{a^2}{2\epsilon}.
\end{equation}
Indeed, the upper bound results from placing an interval of length $2\epsilon$ at $[0;2\epsilon]$, which covers all $k$ such that $a^{-k}\leq 2\epsilon$, plus a separate interval for each smaller $k$. The lower bound comes from the realization that if the gap between two consecutive points in $\cX_a$ is bigger than $2\epsilon$, then they cannot be covered by the same interval. Say, $2\epsilon < 3\epsilon \leq a^{1-k}-a^{-k} = (a-1)a^{-k}$, which is true for $k\leq \log\frac{a-1}{3\epsilon}$. This means that each such $a^{-k}$ needs its own interval. 

To prove the lower bound, we start from Theorem \ref{thm:coding}, we use the covering/packing relation $\Pi_{\epsilon}(F)\geq\Gamma_{2\epsilon}(F)$, and apply the lower bound in Eq.~\eqref{eq:example_covering_bounds} (noting that in this example $\sqrt{\cX_a}=\cX_{\sqrt{a}}$).
After some basic algebra we get that there exists a DI code with
\[\begin{split}
  R(n) 
   &\geq (1-t)\log\log_a \frac{t\sqrt{c}(\sqrt{a}-1)^2}{36\sqrt{6E(n)}}\\ 
   &\phantom{\geq.}- H(t,1-t) - O\left(\frac{\log n}{n}\right).
\end{split}\]
To optimise the right hand side, we choose $t=\sqrt[4]{E(n)}$, such that
\[\begin{split}
R(n) &\geq \left(1-\sqrt[4]{E(n)}\right) \log \log_a \frac{O(1)}{\sqrt[4]{E(n)}}\\
&\phantom{===}- H\left(\sqrt[4]{E(n)},1-\sqrt[4]{E(n)}\right) -O\left(\frac{\log n}{n}\right)\\
&=\log\log\frac{1}{E(n)} - O(1).
\end{split}\]

For the upper bound we start from Theorem \ref{thm:converse} and use the right-hand inequality in Eq.~\eqref{eq:example_covering_bounds} and the property $\sqrt{\cX_a}=\cX_{\sqrt{a}}$:
\[\begin{split}
  R(n) &\leq \log\left[\Gamma_{\frac12{\sqrt{1-e^{-E(n)/2}}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right]\\
  &\leq\log \log_{\sqrt{a}}\left(\frac{a}{\sqrt{1-e^{-E(n)/2}}}\right).
\end{split}\]
For small error exponents $E(n)>0$ we can expand the expression above as follows:
\[\begin{split}
  R(n)
%\leq\log\left[\log_a\left(\frac{2a^2}{E(n)}\right)+O\left(E(n)\right)\right]
  &\leq \log\log_a \frac{2a^2}{E(n)} + O\left(\frac{E(n)}{-\log E(n)}\right)\\
  &=\log\log\frac{1}{E(n)} +O(1).
\end{split}\]

Considering the bounds above and the arguments in Section \ref{ssec:linear_to_super}, where we took $E(n)=\omega(1/n)$ for the lower bound and $E(n)=\Omega(1/n)$ for the upper, we conclude that the DI capacity in this case ought be defined at $n\log\log n$ scale (cf.~\cite[Subsect.~5.4]{CDBW:DI_classical}), and that it evaluates to 
\[
  \mathring{C}_\text{DI}(\cB|_{\cX_a}) 
    \!:=\!\!\!\! \sup_{\myfrac{E_i(n)\rightarrow 0,}{nE_i(n)\rightarrow\infty}} \!\!\!\!
      \left( \liminf_{n\rightarrow\infty}\frac{1}{\log\log n} \!\!\!\!\!\!\max_{\myfrac{\text{errors }\lambda_i\text{ s.t.}}{\phantom{==}\log \lambda_i \leq -nE_i(n)}} \!\!\!\!\!\!\!\! R(n) \right)\!=\! 1,
\]
completing the proof.
\end{proof}

\begin{example}[Cf.~\cite{SPBD:DI_power}]
\label{ex:DMC}
Given small enough $E(n)>0$, for all sufficiently large $n$ there exists a DI code with $E_1(n),\,E_2(n) \geq E(n)-\frac3n$ for a DMC $W:\cX\rightarrow\cY$ with $|W(\cX)|$ pairwise different output probability distributions (equivalently: $N_{\text{row}}=|W(\cX)|$ distinct rows of the channel stochastic matrix) and rate lower-bounded
%\pau{Change the statement:} For any pair of sufficiently small error exponents $E_1(n),\,E_2(n)>0$, the maximum rate $R(n)$ of a DI code for the DMC $W$ with $|W(\cX)|$ pairwise different output probability distributions (equivalently: $N_{\text{row}}=|W(\cX)|$ distinct rows of the channel stochastic matrix), is bounded by:
\begin{equation}
  \label{eq:DMC-direct}
  R(n)\! \geq\! \log |W(\cX)| - O\left(-\sqrt{E(n)}\log E(n)\right) - O\left(\frac{\log n}{n}\right). 
\end{equation}
Conversely, for any DI code with $E_1(n),\,E_2(n) \geq E(n) > 0$ and sufficiently large $n$,
\begin{equation}\label{eq:DMC-converse}
  R(n) \leq \log |W(\cX)| - O\Bigl(-E(n)\log E(n)\Bigr).
\end{equation}
The linear capacity results for the DMC from \cite{AC:DI,SPBD:DI_power} are recovered from these two bounds in the limit of vanishing exponents: $C_{\text{DI}}(W) = \log |W(\cX)|$.
\end{example}

\begin{proof}
For the direct part [Eq.~\eqref{eq:DMC-direct}] we can reuse the proof of Theorem \ref{thm:coding} with slight modifications. There, we started by defining a packing on $\sqrt{\!\cX}$ of radius $\beta$, and worked after with the discrete set of $|\cX_0|$ packing elements. Since now we already have a finite set of output probability distributions $\widetilde{\cX} = W(\cX)$, we let 
\[
2\beta 
:= \min_{x\neq x'\in\cX}\left|\sqrt{W_x}-\sqrt{W_{x'}}\right|_2,
\] 
the minimum Euclidean distance between square roots of output probability distributions, and the rest of the proof is recycled. We will create a maximum-size code in block length $n$ with minimum Hamming distance between different code words $d_H(x^n,x'^n)\geq tn$ and partition the code into regions of similar entropy. Following the arguments, we can lower-bound the rate as [cf.~Eq.~\eqref{eq:coding_rate_1}]:
\[
 R(n) \geq (1-t)\log|W(\cX)| - H(t,1-t) - O\left(\frac{\log n}{n}\right),
\]
and the error exponents are as claimed in the theorem by requiring $E(n)=\frac16 ct^2\beta^4$. Solving for $t$ and substituting in the equation above we get that, for small error exponents:
\[
  R(n) \geq \log|W(\cX)| - O\left(-\sqrt{E(n)}\log E(n)\right) - O\left(\frac{\log n}{n}\right).
\]

For the converse [Eq.~\eqref{eq:DMC-converse}] we start by noticing that Eq.~\eqref{eq:converse_maximum_F} translates to a minimum Hamming distance between code words. 
Namely, letting $\frac{1}{\alpha} := \displaystyle \max_{x\neq x'\in\cX} F(W_x,W_{x'}) < 1$ be the maximum fidelity between different output probability distributions, we have for two distinct code words $u_j = x^n$ and $u_k = {x'}^n$:
\[
  \frac{1}{\alpha^{d_H(u_j,u_k)}}\leq 4e^{-nE(n)},
\]
which implies
\(
  d_H(u_j,u_k) \geq nE(n)\log_\alpha e - \log_\alpha 4.
\)
With the sphere packing (aka Hamming) bound and in the regime of small error exponents we obtain
\[
  R(n) \leq \log |W(\cX)| - O\left(-E(n)\log E(n)\right).
\]

The only thing left to show is that in the asymptotic limit, the rate bounds above yield the linear capacity results from \cite{SPBD:DI_power}. Repeating the argument in Section \ref{ssec:linear_to_super} we take $E(n)\geq\Omega(1/n)$ for the upper bound and $E(n)\geq\omega(1/n)$ for the lower, which translates into:
\[\begin{split}
\max R(n) &\geq \log |W(\cX)| - O\left(\frac{1}{\sqrt{n}}\log n\right)\\
\max R(n) &\leq \log |W(\cX)| - O\left(\frac1n\log n\right).
\end{split}\]
As both the $O(\,\cdot\,)$ terms go to zero in the asymptotic limit, the linear capacity is given by
\[
C_\text{DI}(W) 
    = \!\!\!\sup_{\myfrac{E_i(n)\rightarrow 0,}{nE_i(n)\rightarrow\infty}} \!
      \left( \liminf_{n\rightarrow\infty} \!\!\!\!\!\!\!\max_{\myfrac{\text{errors }\lambda_i\text{ s.t.}}{\phantom{==}\log \lambda_i \leq -nE_i(n)}}\!\!\!\!\!\!\!\! R(n) \right)=\log |W(\cX)|,
\]
and the proof is completed.
\end{proof}

Example \ref{ex:DMC} can be generalized to DMCs with an input power constraint. Given a cost function $\phi : \cX \rightarrow \RR_{\geq 0}$ (power per symbol), an input power constraint $A$ is an upper bound on the average power that admissible code words have to obey:
%\begin{equation}
%  \label{eq:power_constraint}
$\phi^n(x^n)=\frac1n\sum_{t=1}^n \phi(x_t)\leq A$.
%\end{equation}
From \cite{SPBD:DI_power} we know that the DI capacity of the DMC $W$ under the power constraint $A$ is 
\[
  C_\text{DI}(W;\phi,A) = \max_{p_X\,:\,\EE \phi(X) \leq A} H(X),
\]
after purging the channel of duplicate input symbols: if $W_x=W_{x'}$, and say $\phi(x') \geq \phi(x)$, remove the input with the larger cost ($x'$) from $\cX$ and keep the other one ($x$); in case of a tie decide arbitrarily. 
Example \ref{ex:DMC} above is recovered when the power constraint is irrelevant (if $A \geq \max_{x\in\cX} \phi(x)$), in which case the capacity becomes $\max_{p_X\in\cP(\cX)} H(X) = \log|\cX| = \log|W(\cX)|$. With the power constraint active, and for positive error exponents, we obtain lower and upper bounds on $R(n)$ as given in Eqs.~\eqref{eq:DMC-direct} and \eqref{eq:DMC-converse}, with $\log|W(\cX)|$ replaced by $\max_{p_X\,:\,\EE \phi(X) \leq A} H(X)$.



Despite not having a closed formula for the general case with Minkowski dimension zero, we show how to tackle such cases through the manipulation of Theorems \ref{thm:coding} and \ref{thm:converse}. It is possible that calculating the needed covering and packing numbers is difficult in some cases, but regardless, it is clear that we will find the rate-reliability function bounded in terms of some (sublinearly growing) function of $\log\frac{1}{E}$.









\subsection{Extension to quantum channels}
\label{ssec:quantum}
Here we prove how the main results in Section \ref{sec:bounding} can be extended to classical-quantum (cq)-channels and to general quantum channels under the restriction that we only use product states when encoding. We refer the reader to \cite[Sect.~6]{CDBW:DI_classical} for all the necessary quantum information tools (alternatively, see \cite{Wilde-book,holevo:stat-structure-book,CK:book2011}) needed for the following, as well as for a brief review on identification via quantum channels \cite{loeber:PhDThesis,AW:StrongConverse,Winter:QCidentification,Winter:QC-ID-randomness,Winter:Review,CDBW:ID-0-sim,CDBW:ID-0-sim:ICC}. 

Let us start studying the reliability functions for DI over cq-channels $W:\cX\rightarrow \cS(B)$, which have a classical letter (a number $x\in\cX$) as input and a quantum state $W_x\in\cS(B)$ as output. The $n$-extension $W^n:\cX^n\rightarrow\cS(B^n)$ maps input code words $x^n\in\cX^n$ to $W_{x^n}=W_{x_1}\ox\dots\ox W_{x_n}$. 

For the direct part, we just need to define an informationally complete POVM $T=(T_y:y\in\cY)$. Then, we can form the concatenated channel $\overline{W}=\cT\circ W$ of the cq-channel $W$ followed by the quantum-classical (qc)-channel $\cT(\sigma)=\sum_y(\Tr\sigma T_y)\ket{y}\bra{y}$, which effectively outputs the probability of each outcome $y$ when the POVM $T$ is used as measurement. Now, since that the channel $\overline{W}$ can be identified with the classical channel $\overline{W}(y|x)=\Tr W_xT_y$ and $\widehat{\cX}:=\overline{W}(\cX)\subset\cP(\cY)$ its image of output distributions we can reduce the problem directly to the setup of Theorem \ref{thm:coding} to get the following:

\begin{theorem}
\label{thm:cq-coding}
For any $0<t<1$, and $E(n)>0$, there exists a simultaneous DI code over the cq-channel $W:\cX\rightarrow\cS(B)$ (and its associated $\overline{W}$ as above) on block length $n$ with error exponents $E_1(n)\geq E(n)-\frac1n$ and $E_2(n)\geq E(n)-\frac3n$, and its rate lower-bounded 
\[\begin{split}\pushQED{\qed} 
    R(n) &\geq (1-t)\log\left[\Pi_{\sqrt[4]{\frac{6E(n)}{ct^2}}} \left(\!\sqrt{\!\widehat{\cX}}\right)\right]\\
    &\phantom{\geq.}-H(t,1-t)-O\left(\frac{\log n}{n}\right).\qedhere
\popQED\end{split}\]
\end{theorem}

With the same arguments, we can also extend Corollary \ref{cor:coding} and the improved lower bound in Eq.~\eqref{eq:improvement_coding} to cq-channels obtaining similar results, albeit changing the lower (for the corollary) or upper (for the improvement) Minkowski dimensions of $\sqrt{\!\widetilde{\cX}}$ to $d_M\!\left(\!\sqrt{\!\widehat{\cX}}\right)$. Notice also that these are always rates for \emph{simultaneous} deterministic identification because we measure the output quantum states with the fixed POVM $T^{\ox n}$.

Finally, we can also extend the results to general quantum channels $\cN:A\rightarrow B$ with an input restriction to a subset $\cX\subset\cS(A)$ which can be effectively reduced to a cq-channel $W:\cX\rightarrow\cS(B)$, $W_x=\cN(x)$. The resulting code is defined as a quantum DI $\cX$-code (see \cite[Sect.~6]{CDBW:DI_classical}) and the corresponding lower bound on the simultaneous rate-reliability function is given, thanks to the reduction from quantum to classical-quantum channels, by Theorem \ref{thm:cq-coding}.

For the converse part, we can also follow closely the proof of Theorem \ref{thm:converse}. We have to be careful though, as in the quantum setting, we can no longer use the total variation and Euclidean distances to calculate the separation between outputs. We use instead their well-known generalization to quantum states: the \emph{trace} and the \emph{Hilbert-Schmidt distances}, based on the Schatten-$1$ and -$2$ norms, respectively. Given two density matrices $\rho$ and $\sigma$, these distances are related as follows \cite[Lemma 6.2]{CDBW:DI_classical}:
%\aw{(I don't think we need these relations and so should skip them, or replace them by the ones involving the square roots from the long paper...)}:
\[
  \frac12\|\rho-\sigma\|_1
    \leq \left\|\sqrt{\rho}-\sqrt{\sigma}\right\|_2
    \leq \sqrt{2}\sqrt{\|\rho-\sigma\|_1},
\]
and the Fuchs-van-de-Graaf inequalities in Eq.~\eqref{eq:Classical_FvdG} also hold \cite{FVdG:ineq}. Notice that by conveniently defining the output quantum states as $W_x\in\cS(B)$ the notation of the classical stochastic distances or fidelities between probability distributions is equivalent to the corresponding ones in the quantum setting, and we can actually follow the converse proof step by step. We fix a maximum possible fidelity between output states corresponding to code words from a good DI code, create a covering of $\sqrt{\!\widetilde{\cX}}:=\left\{\sqrt{\rho}:\rho\in\widetilde{\cX}\right\}\subset\cS(B)$ such that any two states in the same ball have a fidelity higher than the initial requirement, and finally count the cardinality of such a covering. We obtain:

\begin{theorem}
\label{thm:cq-converse}
For any DI code over the cq-channel $W:\cX\rightarrow\cS(B)$ on block length $n$ with positive error exponents, $E_1(n),\, E_2(n) \geq E(n) > 0$, the rate is upper-bounded
\[\pushQED{\qed} 
  R(n) \leq \log\left[\Gamma_{\frac12{\sqrt{1-e^{-E(n)/2}}}}\left(\!\sqrt{\!\widetilde{\cX}}\right)\right].\qedhere
\popQED\]
\end{theorem}

Corollary \ref{cor:converse} and the improved bound in Eq.~\eqref{eq:improvement_converse} are similarly extended to the quantum setting using the reduction from general quantum channels with an input state restriction $\cX\subset\cS(A)$ to cq-channels, it is clear that the rate-reliability function of a quantum channel DI $\cX$-code is upper-bounded by Theorem \ref{thm:cq-converse}. 









\subsection{Large error exponent regime}\label{ssec:large_exponents}
The bounds in Section \ref{sec:bounding} have permitted an extensive analysis of the trade-off between rate and reliability in the regime of small error exponents. But, as already mentioned in Remark \ref{remark:coding}, our lower bound on the rate [Equation \eqref{eq:coding_rate_final}] does not behave well for bigger error exponents. In this section we will explain the reason why this happens and show that a bypass to the problem is not at all trivial. After that, we include a discussion on what do we know about the rate-reliability functions in the regime of large error exponents.

Let us start by understanding when and why is our analysis limited to the regime of small error exponents. We have to go back to the proof of Theorem \ref{thm:coding} where, in the second inequality of Equation \eqref{eq:coding_TVD_to_Euc}, we use the bound $-\ln F^2\geq 1-F$, with $F$ the fidelity between two output distributions. Notice that this bound is only a good approximation when the letterwise fidelity is close to 1. That is, when the probability distributions are similar, which can be related to a finer packing, so smaller error exponents. Conversely, large error exponents are related to wider packings, which result on fidelity values getting closer to 0, making the bound $-\ln F^2\geq 1-F$ grossly inaccurate. So, when applying this bound, we limit our analysis to small error exponents. 

The reader could wonder now why is it at all necessary to apply such a problematic bound. Well, in order to apply our proof idea, we need a metric so that a packing can be defined, but the negative logarithm of the fidelity [second line in Equation \eqref{eq:coding_rate_final}] is not a well defined distance, as it does not satisfy the triangle inequality. This is why we use the bound $-\ln F^2\geq 1-F$ together with the lower bound in Equation \eqref{eq:fidelity->euclidean}, conveniently obtaining an Euclidean metric. As we have already discussed, this step is a good approximation for the small exponent regime, which provides the framework for the superlinearity study and allows the recovery of the capacity results from \cite{CDBW:DI_classical} (which were the main objectives of this work). However, it is increasingly inexact for bigger error exponents making the bound inadequate in the regime of large error exponents.

The lack of a lower bound able to well approximate the $-\ln F^2$ in the range of small fidelities (for which $-\ln F^2$ grows to infinity) and from which a well-defined distance can be extracted, suggests that our proof will probably not be trivially generalised to the large error exponent regime. Different ideas are needed. However, we can make some comments about how the rate-reliability function behaves in the large exponent regime, which are included next.

We can easily upper-bound the rate for DI over any channel $W$ more meaningfully in the regime of large error exponents starting from a transmission code for $W$. It is clear that any transmission code without randomness on the encoder and error probability $\lambda=2^{-nE_\text{T}}$ is also a DI code with $2^{-nE_\text{DI}}=\lambda_1=\lambda_2=\lambda$. Hence the rate-reliability function for transmission is a lower bound on the rate-reliability function for DI: $E_\text{DI}(R) \geq E_\text{T}(R)$. Furthermore, by using a sequential decoding of the $N=2^{nR}$ transmission messages using an $(n,N,\lambda,\lambda)$-DI code (decoding a message by trying to identify each possible output), we can bound
\[
2^{-nE_\text{T}}=\lambda\leq2^{nR}2^{-nE_\text{DI}}=2^{-n(E_\text{DI}-R)},
\]
which directly implies the following upper bound on the rate-reliability function for DI: $E_\text{DI}(R) \leq E_\text{T}(R)+R$.







\section{Conclusions}
\label{sec:conclusions}
We have initiated the study of the rate-reliability tradeoff between the message length and the exponents of the two error probabilities occurring in DI. The main finding is that, unlike in the communication problem, imposing exponentially (in the block length $n$) small errors of first and second kind makes the slightly superlinear scaling of the message length disappear, and replaces it with a ``regular'' linear-scale rate. We do not have exact formulas for the rate-reliability function, whose derivation remains as the main open question for further research. But our results are strong enough to show that for sufficiently small error exponents a universal behavior manifests itself: the rate is essentially proportional to the Minkowski dimension of the set $\sqrt{\!\widetilde{\cX}}$ (up to a factor between $\frac14$ and $\frac12$) multiplied by the logarithm of the inverse exponent. 

Technically, we derive upper and lower bounds on the DI rate for fixed reliability exponents and at finite block length. While the proofs are mainly inspired by those in \cite{CDBW:DI_classical}, the analysis in the non-asymptotic regime allows us to separate the information theoretic from the geometric aspects of the problem, resulting in the bounding of the DI rate in terms of error-exponent-dependent packings (for the lower bound, Theorem~\ref{thm:coding}) and coverings (for the upper bound, Theorem~\ref{thm:converse}). The geometric inspection of the bounds in the regime of small errors yields the appearance of the lower and upper Minkowski dimensions featuring in the main results of \cite{CDBW:DI_classical}. There, these two aspects appeared intertwined, as the covering and packing radii were made to directly depend on the block length (which is treated asymptotically), perhaps obscuring the main ideas behind the proof. 
We also show how to approach the special case of channels with a vanishing Minkowski dimension of the output probability set through a couple of examples, and extend the reliability results to classical-quantum channels and quantum channels under some input restriction. 
We approach the case of big error exponents for which our lower bound becomes trivial, and comment on some characteristics that the reliability curve must have in that regime, leaving a more thorough study for future work.
These sections complete our first reliability study of deterministic identification making it a parallel to the capacity study \cite{CDBW:DI_classical}, in the sense that every capacity result there can now be mapped to a reliability result here, in this paper. 

%Considering the optimistic and pessimistic asymptotic maximum rate in the limit of $n\rightarrow\infty$, we proved that the characteristic superlinear scaling of DI code size scaling can only happen in the regime of slowly vanishing errors. On the other hand, for exponentially decreasing errors the single-exponential code size scaling is restored and we have derived bounds on the rate-reliability function. When instead allowing slowly vanishing error exponents, we recover the optimistic and pessimistic superexponential capacity bounds from \cite{CDBW:DI_classical}.

The present results explain somewhat the origin of the surprising superlinear performance characteristic of deterministic identification: here we see that it has its roots in a sufficiently slow convergence of the errors to zero. By decoupling the error exponents from the block length, we discover that they determine the geometric scale at which the packing and covering properties of $\sqrt{\!\widetilde{\cX}}$ determine the (linear) DI rate. Thus, the Minkowski dimension emerges in the regime of small error exponents. 
%
It remains as an open question to determine the DI code scaling when merely one of the two errors is exponentially small, i.e.~$E_1>0,\, E_2=0$ or $E_1=0,\, E_2>0$. This is an interesting case as it corresponds to the Stein's setting of asymmetric hypothesis testing: the alternative hypothesis, if correct, should be detected with decent probability; but the null hypothesis should be falsely rejected only with exponentially small probability (or the other way round).
In this scenario, our converse gives the previous upper bound of $\frac12 d_M\, n\log n$ on the message length, but our current code constructions only seem to attain linear scaling. 




%\section*{Acknowledgments}
\bigskip\noindent
\small




\renewcommand*{\bibfont}{\footnotesize}
\printbibliography

%\begin{IEEEbiographynophoto}{Pau Colomer} (Student Member, IEEE) received the Graduate degree in physics from Universitat Aut\`onoma de Barcelona (UAB), Bellaterra, Spain, in 2021; and the Master's degree from Universitat de Barcelona (UB), UAB, and Universitat Polit\`ecnica de Catalunya (UPC), Barcelona, Spain, in 2022.

%During 2022, he was part of Institut de Ci\`encies Fot\`oniques (ICFO), Castelldefels, Spain, and Grup d'Informaci\'o Qu\`antica (GIQ), Bellaterra, Spain, as an associate student researcher. Since 2023 he has been pursuing a Ph.D. at the Technical University of Munich (TUM), Munich, Germany.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Christian Deppe} (Member, IEEE) received his Dipl.-Math. degree in mathematics from the University of Bielefeld in 1996 and his Dr.-math. degree, also from the University of Bielefeld, in 1998. He then worked there until 2010 as a research associate and assistant at the Faculty of Mathematics, Bielefeld. In 2011, he took over the management of the project "Safety and Robustness of the Quantum Repeater" from the Federal Ministry of Education and Research at the Faculty of Mathematics, Bielefeld University, for two years. In 2014, Christian Deppe was funded by a DFG project at the Chair of Theoretical Information Technology, Technical University of Munich. At the Friedrich Schiller University in Jena, Christian Deppe took up a temporary professorship at the Faculty of Mathematics and Computer Science in 2015. Until 2023, he worked for six years at the Chair of Communications Engineering at the Technical University of Munich and since January 2024 has taken on new tasks at the Institute of Communications Engineering at the TU Braunschweig. He is project leader of several projects funded by the BMBF and the DFG.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Holger Boche}(Fellow, IEEE) received the Dipl.-Ing. degree in electrical engineering, the Graduate degree in mathematics, and the Dr.-Ing. degree in electrical engineering from the Technische Universität Dresden, Dresden, Germany, in 1990, 1992, and 1994, respectively, the master’s degree from Friedrich-Schiller Universität Jena, Jena, Germany, in 1997, and the Dr.Rer.Nat. degree in pure mathematics from Technische Universität Berlin, Berlin, Germany, in 1998. 
 
%In 1997, he joined the Fraunhofer Institute for Telecommunications, Heinrich-Hertz-Institute (HHI), Berlin. From 2002 to 2010, he was a Full Professor of mobile communication networks with the Institute for Communications Systems, Technische Universität Berlin. In 2003, he became the Director of the Fraunhofer German-Sino Laboratory for Mobile Communications, Berlin. In 2004, he became the Director of the Fraunhofer Institute for Telecommunications, HHI. He was a Visiting Professor with ETH Zürich, Zürich, Switzerland, from 2004 to 2006 (Winter), and with KTH Stockholm, Stockholm, Sweden, in 2005 (Summer). He joined the Institute of Theoretical Information Technology, Technical University of Munich (TUM), Munich, Germany, in October 2010, where he is currently a Full Professor. He has been a member and an Honorary Fellow of the TUM Institute for Advanced Study, Munich, since 2014. Since 2018, he has been the Founding Director of the Center for Quantum Engineering, TUM. Since 2021, he has been jointly leading the BMBF Research Hub 6G-Life with Frank Fitzek. Among his publications is the recent book, \textit{Information Theoretic Security and Privacy of Information Systems} (Cambridge University Press, 2017).

%Prof. Boche was elected as a member of the German Academy of Sciences (Leopoldina) in 2008 and the Berlin Brandenburg Academy of Sciences and Humanities in 2009. He is a member of the IEEE Signal Processing Society SPCOM and SPTM Technical Committees. He was a recipient of the Research Award “Technische Kommunikation” from the Alcatel SEL Foundation in October 2003, the “Innovation Award” from the Vodafone Foundation in June 2006, and the Gottfried Wilhelm Leibniz Prize from Deutsche Forschungsgemeinschaft (German Research Foundation) in 2008. He was a recipient of the 2007 IEEE Signal Processing Society Best Paper Award. He was a co-recipient of the 2006 IEEE Signal Processing Society Best Paper Award. He was the General Chair of the Symposium on Information Theoretic Approaches to Security and Privacy at IEEE GlobalSIP 2016.
%\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Andreas Winter} received a Diploma degree in Mathematics from Freie Universit\"at Berlin, Germany, in 1997, and a Ph.D. degree from Fakult\"at f\"ur Mathematik, Universit\"at Bielefeld, Germany, in 1999. He was Research Associate at the University of Bielefeld until 2001, and then with the Department of Computer Science at the University of Bristol, UK. In 2003, still with the University of Bristol, he was appointed Lecturer in Mathematics, and in 2006 Professor of Physics of Information. From 2007 to 2012 he was in addition a Visiting Research Professor with the Centre of Quantum Technologies at NUS, Singapore. Since 2012 he has been ICREA Research Professor with the Universitat Aut\`onoma de Barcelona, Spain. His research interests include quantum and classical Shannon theory, and discrete mathematics.

%He is recipient, along with Charles H. Bennett, Igor Devetak, Aram W. Harrow and Peter W. Shor, of the 2017 Information Theory Society Paper Award. In 2022, he received an Alexander von Humboldt Research Prize, a Hans Fischer Senior Fellowship of Technische Universit\"at M\"unchen, and one of three 2022 QCMC International Quantum Awards.
%\end{IEEEbiographynophoto}

\end{document}
