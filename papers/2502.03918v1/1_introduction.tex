\section{INTRODUCTION}

The state of the scene that defines the goal of a specific task allows several possible variations in object positions and their internal state; variations that can still be considered valid configurations for a task. 

Many robotic systems try to accurately copy and imitate the goal configuration from human demonstration \cite{dobbe_immitationLearning, whirl_immitationLearning, mime_immitationLearning}. But they often struggle to do so because of additional challenges on the manipulator kinematics and, by imitating, they cost unnecessary additional time for adjustments of positions and internal states of objects, e.g. a cup's content level. Existing geometric task models \cite{taskLearning_trajectories} need to be extended by additional ontology information about possible variations in their execution, which this paper addresses.

% Fixed values with absolute precision are rarely present in nature. A \underline{range} of temperature values in which water is liquid favored the emergence of life. There is a \underline{period} during the year when farmers plant seeds for new plants to grow, not a single second when they must be planted. Even humans have a \underline{range} of hunger levels that we can tolerate before we go and eat.

% Furthermore, we, humans, do not have accurate sensors: we can not pinpoint exactly how many millimeters apart two cups are in the cupboard, can not precisely determine the 3d location of a sound source by hearing it, or determine the exact temperature of a hot cup by touching it. Thus, we have come to live by ranges of values, not fixed ones.

In non-assembly tasks, e.g., household environments, it is rare that fixed values are desired for object states. 
When cleaning the table after dinner, the washed cutlery does not have a fixed 3d pose in the cutlery drawer. Rather, there is an assigned space, a zone, a range of values where the knives, forks, and spoons are located, but not fixed values. 
% Similarly, when requesting a cup of water, we do not think of the exact amount of water volume to be poured into the cup. We have an accepted threshold of the content level when we consider a cup to be full. 
Similarly, when we want to empty a cup, i.e. set its content to 0, we do not make sure there is no water atom left in the cup.

Thus, representing task goal states as (only) fixed values does not model reality. 

The option to model ranges of values is important for robots to make sense of the world: to learn task goal states from the persons with whom they interact and to execute skills to bring an environment into a particular goal state. This is a challenging problem, as illustrated by Figure \ref{fig:teaser}. The user, moving the cup on the front table, has a goal definition in mind that he is trying to achieve. As illustrated on the figure's right side, there are many possibilities for the intended goal. A method of disambiguation and navigation through the possibility space is required for task goal specification.

In this paper, we propose a way of representing task goal states not via fixed states but via value ranges of agent and object properties. This is based on a hierarchical, conceptual definition of objects and agents that includes (both intrinsic and extrinsic) properties with their respective co-domain, which we call \textit{ValueDomain} \cite{conceptHierarchyGeriatronicsSummit24}.

Following the structured definition of environment states, we define a task’s goal state as a subset of the whole environment \textit{ValueDomain}. We call a subset of a \textit{ValueDomain} a \textbf{variation}. This structured environment definition enables computing differences between values of the same \textit{ValueDomain} and between values and defined \textbf{variation}. This becomes important when trying to solve a task: i.e. determining the \skills\ for agents to execute to change the environment into the task’s goal state. Creating the \textbf{variation} of an environment is also important to the model. We propose an interactive method based on one user demonstration of a task. We create the task goal state by computing the differences between the pre- and post-demonstration environments and by asking the user questions to solve ambiguities in the \textbf{variation} selection process.

\begin{figure}[t!]
    %\vspace{-0.5cm}
    \centering
    \includegraphics[width=1\linewidth]{images/Teaser_3.png}
    \caption{Understanding, i.e. representing, the intended amount and type of variation in a task's goal state is paramount for task monitoring and execution planning.} \label{fig:teaser}
\end{figure}

Task goal modeling was developed as part of classical artificial intelligence (AI) tools. Initially, logical predicates and statements were used to define states and (fixed) goal conditions. This definition led to the development of constraint satisfaction problems \cite{ai_book_1}, in which variables could be constrained to ranges of values, not only fixed ones. The possible value ranges were also extended from Boolean to numerical values.

Version Spaces \cite{versionSpaceAlgebra} is another classic AI tool for modeling variable states. Though initially not developed for modeling task goals, Version Spaces represent a set of hypotheses explaining a supervised data set. It can represent a most general hypothesis, accepting any data, a most specific hypothesis, accepting only one data point, and all other possible combinations of data points. One can view Version Spaces as representing subsets of a set: the set itself, single-element sets, and all element combinations in between. Inspired by this set-like view on Version Spaces, we have developed the \textbf{variations} of \textit{ValueDomains}.

STRIPS \cite{strips} emerged as a way of representing changes to states, called actions, that have preconditions to be satisfied and effects on the entities on which the action is performed. PDDL \cite{pddl} is a language in which many planning problems are specified. In it, a domain, i.e. general knowledge about the modeled world, a problem, i.e. the task goal, and an initial state are defined. In the problem domain, an enumeration of actions is included; these actions are sequenced by solvers to reach the task goal defined in the problem. PDDL supports many logical and temporal constraints and also defining variable goal states. However, more complex data types, such as lists or maps like one would need to maintain the collection of entities in an environment, are not supported. A distinction between actions and skills was proposed in \cite{conceptHierarchyGeriatronicsSummit24}, in which actions represent the abstract change to be executed and skills, performed by agents in environments, execute the change represented by actions in the real world.

Recent approaches no longer model tasks but employ large language models (LLM) to create task plan execution from a textual task description \cite{taskModel_llm1, fltrnn, taskModel_llm2}. Most tasks specified in this format already include the steps, i.e. actions, for a robot to perform: e.g. "\underline{put} the mug in box" or "\underline{open} the drawer" from \cite{taskModel_llm1} or "\underline{open} bathroom window" from \cite{taskModel_llm2}. Also, in \cite{fltrnn}, the goal state of a task is fixed, not variable. The authors of \cite{task_onlyPose_withVariableGoal} propose a method for solving object arrangement tasks. While not modeling internal object states, just 3d poses, the tasks have non-fixed goal states, as expected from e.g. household dinner setups.

Task learning from observation approaches include behavioral cloning \cite{bco}, learning from demonstrations \cite{learningFromDemonstrations}, and interactive learning \cite{interactiveLearning}. We employ an interactive learning from demonstration method to construct our task goal model from one visual task demonstration.

KnowRob \cite{knowrob} and ConceptNet \cite{conceptNet} are ontologies, i.e. knowledge bases, to store knowledge about robot skills and natural language, respectively. We use the concept hierarchy presented in \cite{conceptHierarchyGeriatronicsSummit24} to model \textit{ValueDomains}, objects, agents, actions, skills, and their properties.
