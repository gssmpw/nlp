\section{RESULTS}

\begin{figure*}[t!]
    %\vspace{-0.5cm}
    \centering
    \includegraphics[width=1\linewidth]{images/SystemArchitecture_2.png}
    \caption{From a single user demonstration, the system extracts the desired task goal state with the help of user interaction to solve ambiguities. Using the created environment variation, the system computes a task execution plan to bring new environments into the goal state. It sends the plan to agents in the environment to execute.} \label{fig:system_architecture}
\end{figure*}

Figure \ref{fig:system_architecture} shows our proposed framework to define a task goal, i.e. an environment goals state, and to turn a given environment into this goal state. The system visually observes a task execution by a user and segments this \underline{single} demonstration into \skills. \actions\ and \skills\ are defined in \ref{ssec:actions_skills}. The demonstration changed one or several properties of entities in the environment; environment which is now in the goal state. This information and the differences in entity properties from the start and end environment states are used to represent the task goal state. More on that in \ref{ssec:exp_model_def}. To turn a new environment into the defined goal state, a planning problem must be solved. This entails computing the differences between the environment's current state and the goal state, finding \actions\ that solve these differences, instantiating \skills\ that implement the \actions\ in the environment, selecting the \skills\ to execute by minimizing a given metric, and finally, sending the \skills\ to the agents in the environment to execute. This process is detailed in \ref{ssec:exp_model_use}.

% To prove the usability of our model, we present experiments to create a new goal state and turn the current environment into an (already-defined) goal state.

\subsection{Actions and Skills}\label{ssec:actions_skills}
A change in the environment is modeled using \actions, i.e. \textbf{what} has happened, and \skills, i.e. \textbf{how} did the change happen \cite{conceptHierarchyGeriatronicsSummit24}. Like in STRIPS \cite{strips} and PDDL \cite{pddl}, we represent \actions\ by their effects on entity properties and \skills\ by their preconditions and effects. \actions\ do not need preconditions because they only describe the \textbf{what} part of a change, not which conditions must be satisfied to perform the change. Besides preconditions and effects, \skills\ have a list of checks that tell our system if the \skill\ is executed in the environment. These checks allow the creation of a \skill\ recognition program, like the one presented in \cite{conceptHierarchyGeriatronicsSummit24}.
%\todo{citation of Geriatronics summit paper or the journal/unsubmitted paper?}
Using the \skill\ recognition output, we capture the changes from a task demonstration.

A \skill\ is thus the physical enactment of an abstract \action\ in an environment. Hence, \skills\ are correlated with \actions\ via their effects. A \skill\ can have more effects than a corresponding \action. For example, the \skill\ of scooping jam from a jar with a spoon implements the \action\ of \textit{TransferringContents}, but it also \textit{Dirties} the spoon.

\subsection{How To Parameterize The Model}\label{ssec:exp_model_def}
Creating a new goal state should be easier than manually specifying all variations wanted from the goal state. Doing so requires programming knowledge, which should not be needed to define goal states. One can let the system, which knows how to represent goal states, question the user about the desired state of the environment. However, this tedious process requires many questions from the system, also leading to decreased system usability.

Therefore, our approach is to let the user turn a given environment into a desired goal state and analyze the differences between the initial and final environment state to create the goal state representation. This single demonstration highlights the entity property values that were not in the desired goal state before being changed by the user.

We capture the demonstration via an Intel Realsense 3D camera \cite{realsense}, analyze the human skeleton via the OpenPose human pose estimation method \cite{openpose}, and determine the 3d pose of objects with AprilTag markers \cite{aprilTag}.

One demonstration contains the initial environment, not in the task goal state, and the final environment, in the goal state. The final environment state alone is not enough to create the environment variation. Thus, additional questions, guided by the differences between the two environment values, are posed by the system to the user to determine the desired variation in the environment state.

In a demonstration in which the user pours milk into a bowl, as shown in the top of Figure \ref{fig:system_architecture}, the initial question posed to the user is which entities that have changed properties are relevant for the goal state. If the goal state is to have more milk in the bowl, the milk carton is irrelevant; it is a means to achieve the goal state but not relevant to the goal itself. The bowl is thus selected as a relevant entity. 

Next, the list of relevant modified properties must also be determined for each relevant entity. It could have happened that during pouring of the milk into the bowl, the bowl's location also changed, e.g. touched accidentally by the user. Thus, not all modified properties could be relevant to the task. After selecting the relevant properties, the system knows from the knowledge base \cite{conceptHierarchyGeriatronicsSummit24} their \textit{ValueDomain} and the list of implemented \textbf{variations} for that \textit{ValueDomain}. Thus, the user parametrizes a selected \textbf{variation} from the list: choosing either a fixed value, a \textit{ValueDomain}-specific \textbf{RangeVariation} that must be parametrized, a conjunction or disjunction of \textbf{RangeVariations}, or the whole \textit{ValueDomain}.

In the example above, the user chooses the \textit{contentLevel} property as relevant. The system knows this property's defined set of values: a non-negative real number, and the possible range variation types: an open interval, a closed interval, an open-closed or closed-open interval, an intersection or union of intervals, etc. The user chooses a closed interval of $[0.28, 0.32]$ around the final \textit{contentLevel} value of $0.3L$. The user also specifies a variation for the entity's concept. It is generalized from that specific bowl instance to a \textit{LiquidContainer}.

After each modified property of each entity has a represented \textbf{variation}, the system automatically collects the entities into a variation of type $A$, see \ref{ssec:variations}, which is the assigned \textbf{variation} for the collection of entities in the environment.

Thus, the environment variation is determined in $\mathcal{O}\left(n\times m \times p\right)$ questions to the user, where $n$ is the number of entities in the environment, $m$ is the maximal number of properties that an entity can have, and $p$ is the maximal number of parameters that a \textbf{RangeVariation} needs to be represented. In the example above, $10$ questions were necessary to determine the task goal state shown in Figure \ref{fig:system_architecture} of a \textit{LiquidContainer} with \textit{contentLevel} between $0.28$ and $0.32L$. Figure \ref{fig:task_goal_state} shows the internal JSON-like representation of the goal state as the environment variation.
% 1 question which entities are relevant -> just bowl
% 1 question which properties are relevant -> contentLevel and concept
% 1 question about concept values being the same; should create variation?
% 1 question which ConceptValue-variation to select -> ConceptValue in Environment
% 1 question: which generalized concept?
% 1 question -> add other range-variation
% 1 question which Number-variation to select -> Interval
% 1 question: min-bound?
% 1 question: max-bound?
% 1 question -> add other range-variation

\begin{figure}[t!]
    %\vspace{-0.5cm}
    \centering
    \includegraphics[width=1\linewidth]{images/TaskDefinition_7.png}
    \caption{The goal state is a \textbf{RangeVariation} of the environment, of type EnvironmentDataRangeEntityVariation, which contains a \textbf{variation} of entities. This sub-variation is a \textbf{RangeVariation} of type MapRangeInstanceSubset (\textbf{variation} of type $A$, see \ref{ssec:variations}) and contains one instance \textbf{RangeVariation} of type InstanceRangePropertiesVariation. It defines the instance's concept \textbf{RangeVariation}, a \textit{LiquidContainer} to be found in the environment, and the \textit{contentLevel} property \textbf{RangeVariation}, the closed interval $\left[0.28, 0.32\right]$.} \label{fig:task_goal_state}
\end{figure}

\subsection{How To Use The Model}\label{ssec:exp_model_use}
Assuming the representation of a task's goal state is given, i.e. an environment variation, we detail our procedure (see Figure \ref{fig:experiment_description}) to turn the current environment into the goal state.

First, a Comparison between the environment and the goal variation is computed. This leads, as described in \ref{ssec:comparisons}, to a list of reasons why the environment is not in the variation. These reasons, i.e. differences $\delta$ of concept properties $p$, must be fixed to turn the environment into the goal state.

% Computing the differences between an EnvironmentData and an EnvironmentData-Variation, that has a Collection-Variation of type $A$, see \ref{ssec:variations}, is done via a maximal matching algorithm, where an edge between an entity $e$ an an entity variation $v_e$ means $e \in v_e$. 
For an EnvironmentData-Variation $v_{env}$ that defines a Collection-RangeVariation of type $A$, see \ref{ssec:variations}, computing the Comparison between an EnvironmentData $env$ and this target $v_{env}$ leads to a list of reasons for each entity $e_{env}$ in the entity collection of $env$, why $e_{env} \not\in v, \forall v \in A$. This can be seen in Figure \ref{fig:experiment_description}, where for each entity of \textit{LiquidContainer} concept in the environment, there is a list of differences, i.e. Comparisons, created for why the respective entity does not match the defined variation on the top-right.

\begin{figure}[t!]
    %\vspace{-0.5cm}
    \centering
    \includegraphics[width=1\linewidth]{images/Experiment_DescriptionUsingVariations_2.png}
    \caption{The procedure to turn an environment into its goal state is divided into 5 steps: computing differences, finding abstract solutions (i.e. \actions), computing practical solutions for the abstract ones (i.e. \actions\ $\rightarrow$ \skills), selecting the best practical solution, and executing the solution.} \label{fig:experiment_description}
\end{figure}
The second step of the procedure is to turn the list of differences into a list of \actions\ that can fix them. In notation, \action\ $A_x$ solves a difference in the concept property $p_x$. The system knows which properties \actions\ modify by analyzing the definition of their effects. Thus, \actions\ are created (parametrized) to fix the differences in entity properties.

% Because multiple instances can fit the instance variation, the third step is to match instances with the variations. Our matching optimization criterion is to minimize the amount of \textit{Actions} needed to fix the instances' property differences. \todo{continue!}

In the third step, each \action\ $A_x$ is converted into an execution plan $P_x$ that implements solving the difference $\delta_{p_x}$ in the environment. It is also possible that there is no possibility to implement the \action\ $A_x$ in the environment; this is represented as an execution plan $P_x = \emptyset$. An execution plan $P_x$ is otherwise, in its simplest form, a set of \skill\ alternatives $\left\{S_y\right\}$, where the \skill\ $S_y$ implements the \action\ $A_x$. There is the case to consider that the \skill\ $S_y$ has preconditions that are not met. And so, before executing the skill $S_y$, a different execution plan $P_{S_y}$ has to be computed and executed to allow the \skill\ $S_y$ to solve the property difference $\delta_{p_x}$. It is also possible that one single \skill\  $S_y$ is not enough to implement the \action\ $A_x$. Consider the case where the environment contains three cups with $0.1L$ of water, and the goal is to have one cup with $0.3L$ of content. One single \textit{Pouring} \skill\ is not enough to fulfill the goal; two \textit{Pouring} \skills\ must be executed. Thus, in the most general form, an execution plan $P_x = \left[\left\{ S_{iy}, P_{S_{iy}} \right\}_i\right]$ is a list of skill alternatives $\left\{ S_{iy}, P_{S_{iy}} \right\}_i$, that possibly contain other execution plans $P_{S_{iy}}$ to solve the skill's preconditions.

Our procedure to parameterize the \skills\ $S_y$ that implement the \action\ $A_x$ is a custom solution for each property $p_x$. One could backtrack through all possible parameter values of all possible skills to create a general solution that works for all properties. Another idea is to invert \skill\ effects and thus guide the \skill\ parameter search from the target variation to the value. However, both approaches would be computationally intense and would not create execution plans in a reasonable time. 
% reinforcement learning with policy for each property

The procedure to solve an entity $e$'s \underline{contentLevel} property difference searches for other \textit{Container} object instances in the environment, sorts them according to their content volume, and iterates through them in ascending order if $e.contentLevel \le target.contentLevel$; otherwise, in descending order. If a \skill\ $S$ can be executed with the two objects, that reduces the difference between $e.contentLevel$ and $target.contentLevel$, the \skill\ is added to the execution plan. If, after checking all objects, $e.contentLevel \not\in target.contentLevel$, there is no solution to solve this property difference.

Thus, the result of the third step is an execution plan $P_x$ for each entity property difference.

Fourth, after having the execution plans $P_x$ per entity-variation and entity, a \underline{solution selector} scores all solutions according to defined metrics and then, via a maximal matching algorithm, selects the solutions to execute to satisfy all variations of the Collection-RangeVariation of type $A$. The edges in the maximal matching have the cost of the solution score. For this paper, the scoring metric by the \underline{solution selector} is the number of steps of the execution plan.

The fifth and final step is to pass the execution plan to the agent(s) to execute in the environment. Figure \ref{fig:data_flow} presents the flow of data through the five steps.
We have used the Franka Emika Panda robot in CoppeliaSim \cite{coppeliaSim} to perform the computed execution plan.
% Note that the approach is independent of the used robot; only when instantiating \skills\ must the robot's abilities, manipulability region, and workspace be considered. How the \skills\ are executed in the environment is separated from the modeling of what must be done.

\begin{figure}[t!]
    % \vspace{-0.2cm}
    \centering
    \includegraphics[width=1\linewidth]{images/Experiment_DescriptionUsingVariations_DataFlow_2.png}
    \caption{Data flow when transforming an environment into a given goal state. $\Delta$ are differences of entity properties $p$, $A$ are \actions, $P$ is an execution plan and $S$ are \skills.} \label{fig:data_flow}
\end{figure}

The experiments aim to compute solution plans for solving the difference of the \textbf{contentLevel} property of \textit{Container} objects. For this, we consider the following criteria. $C1$: \textbf{variation} type = $\left\{\text{fixed},\text{interval},\text{interval union}\right\}$. $C2$: target relative to content = \{$\left\{t < cL \le cV \right\}$, $\left\{cL < t < cV \right\}$, $\left\{cL < t \ni cV \right\}$, $\left\{cL \le cV < t \right\}$\}, where $t$ is the \textbf{variation} value and $cL$ and $cV$ are the \textit{contentLevel} and \textit{contentVolume} properties respectively. $C3$: achievable in environment $ = \left\{\text{yes}, \text{no}\right\}$. Figure \ref{fig:experiment_table} presents planning results for different environments and the criteria described above. The lower table shows cases where the computed solution does not match the actual solution. This only happens when multiple instance variations are defined. The reason is that the implemented procedure to turn the list of differences into an execution plan treats each difference independently. Thus, dependencies between two variations are not accurately solved.

In the upper table of Figure \ref{fig:experiment_table}, there are two solutions for $C1.3$, $C2.3$, $C3.1$: one with the bowl $B$ as the instance in the \textbf{variation} $V1$, the other with $M$. The solution when $B$ is the matched instance has three steps: 1) pouring $0.1L$ from $M$ into $B$, 2) pouring $0.1L$ from $C1$ into $B$, and, finally, 3) pouring  $0.02L$ from $C2$ into $B$. This plan is sent to the robot in simulation and is executed as shown in Figure \ref{fig:robot_plan_execution}.

% \begin{figure}[t!]
%     % \vspace{-0.2cm}
%     \centering
%     \includegraphics[width=1\linewidth]{images/Experiment_Table_1Variation_compressed.png}
%     \caption{$B$ is a bowl with $0.5L$ \textit{contentVolume}, $M$ is a milk carton with $1.0L$ \textit{contentVolume}, $C1$ and $C2$ are cups with $0.3L$ \textit{contentVolume} each. Times, in seconds, averaged across 10 runs. Criteria $C2.4$ and $C3.1$ are mutually exclusive (a solution does not exist to let a container have more \textit{contentLevel} than its \textit{contentVolume}); thus, they are not included in the table.} \label{fig:experiment_table}
% \end{figure}
\begin{figure}[t!]
    % \vspace{-0.2cm}
    \centering
    \includegraphics[width=1\linewidth]{images/Experiment_Table_Results.png}
    \caption{$B$ is a bowl with $0.5L$ \textit{contentVolume}, $M$ is a milk carton with $1.0L$ \textit{contentVolume}, $C1$ and $C2$ are cups with $0.3L$ \textit{contentVolume} each. Times, in seconds, averaged across 10 runs. Criteria $C2.4$ and $C3.1$ are mutually exclusive (a solution does not exist to let a container have more \textit{contentLevel} than its \textit{contentVolume}); thus, they are not included in the upper table. The lower table presents results for open intervals and multiple variations in the environment.} \label{fig:experiment_table}
\end{figure}

\begin{figure}[t!]
    %\vspace{-0.1cm}
    \centering
    \includegraphics[width=1\linewidth]{images/Robot_PouringInBowl_M_PC1_PC2.png}
    \caption{Robot executing plan to bring $B$, the bowl, into the goal state. Because no liquids were simulated, the pouring amount was associated with the pouring time via: $t_{pour} = 10 * amount_{pour}$.} \label{fig:robot_plan_execution}
\end{figure}