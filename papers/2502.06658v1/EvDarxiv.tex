\documentclass{article}
\usepackage{subcaption}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure} % defined already in icml package
\usepackage{booktabs} % for professional tables
\usepackage[svgnames]{xcolor}
\usepackage[many]{tcolorbox}
\usepackage{mdframed}
\usepackage{xcolor}

\usepackage{quoting} %

\quotingsetup{font={itshape, raggedright, noindent}, leftmargin=0.15in, rightmargin=0.1in, begintext=``\,, endtext=\,''}

\newtcolorbox{mybox}[1][]{
  drop shadow southeast, enhanced, colback=gray!5!white,colframe=gray!75!black,
  #1
}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
\usepackage{hyperref}
\usepackage{ulem}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025arxiv}

% If accepted, instead use the following line for the camera-ready submission:


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{varwidth}
%\usepackage[noadjust]{cite}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%% Author's packages and macros
\usepackage{mathrsfs}
\usepackage{arydshln}
\DeclareMathOperator*{\argmin}{\arg\,\min}
\DeclareMathOperator{\sign}{sign}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\dd}{\mathrm{d}\,}
\renewcommand{\vec}[1]{{\boldsymbol{\mathbf{#1}}}}
\newcommand{\ttheta}{{\boldsymbol{\theta}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Generating Samples to Question Trained Models}

\usepackage{comment}

\begin{document}

\include{sibcommands}


\twocolumn[
% What Does Your Model's Favorite Data Look Like
\icmltitle{Generating Samples to Question Trained Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{E. Mehmet Kıral}{mk}
\icmlauthor{Nurşen Aydın}{na}
\icmlauthor{Ş. İlker Birbil}{sib}
\end{icmlauthorlist}

\icmlaffiliation{mk}{Keio University}
\icmlaffiliation{na}{University of Warwick}
\icmlaffiliation{sib}{University of Amsterdam}

\icmlcorrespondingauthor{E. Mehmet Kıral}{erenmehmetkiral@protonmail.com}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Data generation, explanation, model validity}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

Machine learning models are widely used in today’s data-driven world, powering critical decision-making processes in sectors ranging from healthcare to human resources. Their widespread adoption in high-stakes scenarios raises important questions on aligning trained models with human values. Understanding how these models operate has become a critical aspect of addressing these concerns. Our quest along this line starts with the following inquiry: ``\textit{What kind of data can we generate to probe our trained models?}'' We aim to study the implicit data distribution favored by trained models, illuminating potential tendencies and paving the way for more adaptable transparent systems.

To this end, we use an inductive approach to understand the model by creating samples in the data domain that the trained model considers favorable for a specific task.  Traditional Machine Learning (ML) workflows emphasize data collection, cleaning, and model training. However, real-world applications present challenges such as ensuring alignment with human values and addressing issues in generating realistic representative data. For instance, a social benefit approval model may unfairly reject applications from underrepresented demographic groups \citep{syri2020}. Similarly, changes in data distributions over time can lead to model obsolescence, impacting predictions in critical areas like public health. Incorporating data generation that reveals the model's preferences to the ML workflow can help mitigate these issues, enabling early warning systems and augmenting datasets to improve robustness.

Our goal is to understand models by generating data samples that are easy to interpret and showcase how the model answers specific questions posed to the model, rather than relying on feature saliency \citep{Shrikumar2017} or surrogate model properties \citep{Ribeiro2016} to explain an individual prediction. These questions are customized to each situation and can be expressed mathematically through a loss function that evaluates the data based on a combination of data characteristics and model parameters. We consider the problem of understanding a model to be a more nuanced endeavor that requires exploration across multiple dimensions of questioning. This involves providing explanations, such as counterfactual \cite{Wachter2017} or prototypical \cite{Biehl2016} scenarios, shedding light not only on why a particular prediction was made, but going beyond it as well. For instance, insights into model behavior can be gained by generating parameter-sensitive data samples or contrasting competing models through differentiating data. These custom questions, and others, provide a qualitative understanding of the model. Additionally, users have the flexibility to ask custom queries by designing specific probing functions within the data space. 

\paragraph{Related Literature.} 
Our work complements extensive research in synthetic data generation. Synthetic data has been pivotal in addressing fairness, bias reduction, and robustness challenges in machine learning. Prior works have explored detecting bias in datasets \citep{Kusner17}, using de-biased synthetic data to mitigate biased outputs \citep{Xu18, Breugel21}, and employing synthetic data for dataset augmentation \citep{Wong16, Fawaz18}. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have been widely used to approximate original data distributions \citep{Goodfellow14, Xu18, Kingma14, Breugel24}, focusing on privacy, diversity, and fidelity as primary goals.
\newpage

Recent studies leveraged generative models for counterfactual generation and exploring underrepresented data regions. For example, \citet{Joshi19} proposed a framework for generating task-specific synthetic data, enhancing model explainability. Similarly, \citet{Redelmeier24} introduced an approach using autoregressive generative models to create counterfactuals, facilitating bias exploration and decision boundary analysis.

Energy-based models (EBMs) have also emerged as a promising framework, combining generative and discriminative modeling tasks. By treating classifier logits as an energy function, EBMs can model joint distributions over data and labels \citep{LeCun06, Duvenaud20}. Applications of EBMs include adversarial robustness, out-of-distribution detection, and data augmentation \citep{Zhao17, Liu20, Arbel21, Margeloiu24}. For instance, \citet{Duvenaud20} demonstrated improved out-of-distribution detection using a joint energy-based model, while \citet{Ma24} extended EBMs to tabular data for synthetic data generation.

The proposed framework draws inspiration from these works while introducing a distinct perspective. Our probing function can be seen as an energy function and leads to Gibbs distribution. However, rather than learning the energy function to capture the data distribution (conditioned on class), we create a probing function using trained models. This design allows the distribution to generate samples that address the specific posed question. Related works, such as \cite{Duvenaud20} and \cite{Ma24} mentioned above, adopt a similar approach by utilizing a trained classifier to obtain an energy function and using Langevin dynamics for sampling from the Gibbs distribution. However, their main objective is to mimic the true data distribution. In fact, the former paper combines training of the energy function and classifier. In contrast, we propose a flexible framework that allows for directing diverse queries to trained models via probing functions that reflect various objectives, such as identifying prediction-risky, parameter-sensitive, or model-contrastive data samples.

\textbf{Contributions.}
We contribute to the literature by introducing a novel inductive approach. This approach creates data samples using a probing function that engages a trained model. Through this framework, the generated samples provide answers to various questions posed to trained models. With our computational study, we support our approach by applying it to a range of classification and regression tasks, showcasing its effectiveness in generating data tailored to specific queries. Our numerical results showcase the flexibility of our approach in uncovering biases, facilitating model interpretability, and consequently, promoting alignment of model predictions and human preferences.


\section{The Mathematical Framework}

Before we introduce the proposed framework, let us give our notation. The labeled data lie in $\mathcal{X}\times \mathcal{Y}$, and the model defines a predictor function $f(\vec{\theta}, \cdot): \mathcal{X} \to \mathcal{Y}'$ for any given set of model parameters $\vec{\theta} \in \Theta$. Then, we obtain for a given sample $\vec{x} \in \mathcal{X}$, the predicted label $y_{\vec\theta}(\vec{x}) \in \mathcal{Y}$ by passing the predictor function through a transformation depending on regression or classification task. The cost function $\ell_F : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_{\geq 0}$ measures how far the predicted labels are from the true labels.

We propose a framework for probing a model with answers in the form of generated data. Our method is structured to be symmetric to the training process itself: instead of generating parameters given a data distribution, we generate data given a trained model parameter distribution, which may be a single set of parameters, \textit{i.e.}, a Dirac delta distribution. One can gain valuable insights into the trained model's behavior by analyzing the generated population statistically. Figure~\ref{fig:maindiagramalt} presents the overview of our framework.

\begin{figure}
\centering
\fbox{\includegraphics[width = 0.45\textwidth]{figures/overview.pdf}}
\caption{Overview of model probing by data generation. The vertical blue arrows (a) and (c) start from loss functions and lead to distributions on the same space by solving the Bayesian learning problem, which balances minimizing the expected loss with maximizing the entropy. The diagonal red arrow (b) obtains a loss function on the data space by integrating out the $\vec{\theta}$ dependence from a designed function via $q^*(\vec{\theta})$. Samples following $p^*(\vec{x})$ constitute an answer to the question posed by $G$.}
\label{fig:maindiagramalt}
\end{figure}

The standard construction of the parameter loss function is \begin{align*}
    F(\vec{\theta}) &= \int_{\mathcal{X}\times \mathcal{Y}} (\ell_F(y_{\boldsymbol{\theta}}(\mathbf{x}, y)  + R_F(\boldsymbol{\theta})) \mathrm{d}\nu(\mathbf{x}, y) 
    \\&=\frac{1}{N}\sum_{i = 1}^N \ell_F(y_{\vec{\theta}}(\vec{x}_i), y_i) + R_F(\vec{\theta}),
\end{align*}
which can be seen as an integration of the function $\ell_F + R_F$ above against the empirical distribution given by the training dataset $\{(\vec{x}_i, y_i)\}_{i = 1}^N \subseteq \mathcal{X} \times \mathcal{Y}$. Here, $R_F(\vec{\theta})$ is a regularizer term that depends only on $\vec{\theta}$.

\vspace{5mm}
Similarly, we form a loss function $G$ on the data space by integrating out the $\vec{\theta}$ dependence of a function of our design that depends both on the data and the model parameters. Depending on our choice, we end up probing the model with different questions. In Figure \ref{fig:maindiagramalt}, the endpoint of the red arrow (b) is the function $G$, which is a mathematical model of the question posed to $q^*(\vec{\theta})$ at the tail of the arrow. A general form of such $G$ functions is given in \eqref{eq:genericG} below.

%For example one choice is
%\begin{equation*}
 %  G(\mathbf{x}) = \int_{\Theta} \left(\ell_G(y_{\boldsymbol{\theta}}(\mathbf{x}), \widehat{y}(\mathbf{x})) +R_G(\boldsymbol{\theta}) \right)q^*(\boldsymbol{\theta}) \mathrm{d}\boldsymbol{\theta} 
%\end{equation*}
%Using the cost function $\ell_G : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_{\geq 0}$ and the distribution $q^*(\vec \theta)$. 
%This is an integral of $\ell_F(f(\vec{x}, \vec\theta), y) + R(\vec{\theta})$ on the space $\mathcal{X} \times \mathcal{Y}$ with respect to the empirical measure $ \frac{1}{N} \sum_{i = 1}^N \delta_{(\vec{x}_i, y_i)}$. Adding ---as we did--- a function to $\ell_F$ that depends only on $\vec{\theta}$ leads to a regularizer term in $F$.
The blue arrows (a) and (c) in Figure \ref{fig:maindiagramalt} represent solving the variational learning problem, which -instead of finding a single set of model parameters that minimizes a loss function- seeks for a distribution that balances the tasks of minimizing the expected loss and maximizing the entropy. In case of (a), we solve
\begin{equation}\label{eq:BLPforF}
    \argmin_{q \in \mathcal{Q}} \E_{q} [F] - \tau \mathcal{H}(q),
\end{equation}
where $\mathcal{Q}$ is a choice of candidate distributions on $\Theta$, and $\mathcal{H}(q) = -\int_{\Theta} q \log q$ is the entropy with respect to a basis measure. The problem can be interpreted as an implementation of the exploration-exploitation trade-off in the parameter space. The constant $\tau > 0$, called the temperature, controls the balance between these two objectives. Absent any restriction, \textit{i.e.}, if $\mathcal{Q}$ is the set of all probability distributions\footnote{All probability distributions which are absolutely continuous with respect to a given base measure, which in this case taken to be the Lebesgue measure $\dd\vec{\theta}$.}, then the Gibbs-Boltzmann distribution $q^*(\vec{\theta}) \propto e^{-\frac{1}{\tau} F(\vec{\theta})}$ is the unique solution to \eqref{eq:BLPforF}.

Additionally, we point out that the well-known stochastic gradient descent training is not a significant departure from this setup, and in fact, can be seen as a specialization. Given a very restrictive family $\mathcal{Q}$, such as the manifold of Dirac delta distributions\footnote{To eschew technicalities of infinities, instead of exactly using Dirac delta we may instead consider distributions which are supported everywhere, and highly concentrated around a point but with fixed variance} supported on a single $\vec\theta$, the entropy term becomes irrelevant, and we get the classical optimization problem of minimizing the loss function $F$. 

Completely symmetrically on the data space $\mathcal{X}$, the blue arrow denoted by (c) in Figure \ref{fig:maindiagramalt}, represents solving the variational learning objective associated with the function $G$ on the data space
\begin{equation}\label{eq:BLPforG}
    \argmin_{p \in \mathcal{P}} \E_p[G] - \tau \mathcal{H}(p).
\end{equation}
The expectation term encourages the solution $p^*(\vec{x})$ to concentrate its mass in regions where $G$ is minimized, but the entropy term encourages $p^*(\vec{x})$ to explore a wide variety of data samples. Here, $\mathcal{P}$ represents the family of candidate distributions, which can be chosen in several ways.   One option is to explicitly select $\mathcal{P}$ as a family of distributions depending on the nature of the data distribution. Finally, we consider samples from the distribution $p^*(\vec x)$ as answers to the questions put forth by $G$.

Alternatively, we can retain the full space of probability distributions but replace $G$ with $G\circ \varphi$ for some function $\varphi: \mathcal{Z} \to \mathcal{X}\times \mathcal{Y}$. In the latter case, $\mathcal{Z}$ can be the latent space and $\varphi$ can be the decoder function of a trained variational autoencoder. This procedure produces a distribution on $\mathcal{Z}$, which we can sample from and map to $\mathcal{X}$ via $\varphi$; thus, sampling data from the pushforward distribution. As a more mundane example, we may choose $\mathcal{Z} = \mathcal{X}$ with $\varphi(\vec{x}) = (\vec{x}, y')$ for a fixed label $y'$. If certain features of the data are considered immutable, then $\mathcal{Z}$ can be a certain subspace of $\mathcal{X}$ and $\varphi$ can be taken to map the missing features to predetermined fixed values. Alternatively, the label coordinate of $\varphi$ may also depend on $\vec{x}$ using a classifier. We provide such examples in Section \ref{sec:probing} and state precisely the questions posed to the model. As a last example of $\varphi$, if the data has been standardized to $[0,1]$, taking it to be the sigmoid function ensures that the answers to our questions come as data points with features in the admissible range.

We illustrate our framework with an explicit case. Consider a trained Linear Regression (LR) model. We probe this model by asking ``What kind of data is truly preferred by the model for a fixed output value?'' To model this question, the functions $\ell_F$ and $\ell_G$ are chosen as the squared difference.  In this case, each step in Figure \ref{fig:maindiagramalt} can be solved analytically and the solutions are provided in Figure
\ref{fig:maindiagramaltlinear}. The resulting preferred data distribution follows a normal distribution, where the formulas and the derivations for the mean $\widehat{\vec{f}}$ and covariance matrix $\Sigma$ are given explicitly in Appendix \ref{app:LR}. In the next section, we dive into other probing questions and elaborate on their implications.

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width = 0.45\textwidth]{figures/linear_overview.pdf}}
\caption{The proposed framework applied to a linear regression model with mean square error and no regularizers. Here, $y'$ is a fixed output value. The distribution of data points $\vec{x}$ which are preferred by the LR model for the prediction $y'$ is calculated to be a Gaussian distribution centered at a point which is shifted from the mean of given data by a certain amount depending on the desired output value $y'$. Details and derivation can be seen in Appendix \ref{app:LR}.}
\label{fig:maindiagramaltlinear}
\end{figure}



\section{Probing Trained Models} \label{sec:probing}

We start with a general structure of the loss function that will be used for questioning the trained models:
\begin{equation}
\label{eq:genericG}
    G(\vec{x}) = \int_{\Theta} \ell_G(y_{\vec{\theta}}(\vec{x}), \widehat{y}(\vec{x})) q^*(\vec{\theta})\dd\vec{\theta} + R_G(\vec{x}),
\end{equation}
where $\widehat{y}$ stands for a predictor and $R_G$ is a regularizer function that can be chosen to put additional soft constraints on the samples in addition to the hard constraints coming from the restriction $p \in \mathcal{P}$. To question the models, we next consider various scenarios with different loss functions, which are the special cases of the general structure in \eqref{eq:genericG}.

\paragraph{Fixed-label samples.} We probe the model for what it thinks are good data samples from the distribution $\mathcal{P}$ that fit the bill for $\widehat{y}(\vec{x}) \equiv y'$. This can be applied to a single set of model parameters $\vec{\theta}^*$, or to a general ensemble of models where the parameters are coming from a distribution $q^*(\vec{\theta})$.

%\begin{example}
In case of a single set of problem parameters, we obtain
\begin{equation}
\label{eq:case1a}
G(\vec{x}) = \ell_G(y_{\vec{\theta}^*}(\vec{x}), y') + R_G(\vec{x}).
\end{equation}
When parameters are coming from a distribution, the loss function becomes
\begin{equation}
\label{eq:case1b}
       G(\vec{x}) = \int_{\Theta} \ell_G(y_{\vec{\theta}}(\vec{x}), y') q^*(\vec{\theta})\dd\vec{\theta} + R_G(\vec{x}).
\end{equation}
Figure \ref{fig:maindiagramaltlinear} demonstrates the steps when $y_{\vec{\theta}}(\vec{x}) = \vec{x}\tr\vec{\theta}$ corresponds to linear regression, and both $\ell_F$ and $\ell_G$ are the mean squared errors. Recall for this special case that we obtain analytical solutions for all steps of our framework. The details of this observation are given in Appendix \ref{app:LR}.
%\end{example}

\paragraph{Prediction-risky samples.} Suppose that a model predicts probabilities such as in logistic regression and the logit layers of neural networks before thresholding. Assuming the predictions correspond directly to these probabilities, that is, $y_{\vec{\theta}}(\vx) = f(\vx, \vec{\theta})$, and $\widehat{y}(\vec{x}) \equiv \vec{\alpha}$ for some anchor probability value, we can probe the model by evaluating the probability spread using $r$-norm, \textit{i.e.}, $\ell_G(y,y') = \|y - y'\|_{r}$ for $r \geq 1$. We then seek data points with probabilities close to $\vec{\alpha}$. For a binary classification with the anchor value $0.5$, this corresponds to generating ``risky data points'' near the decision boundary. 
%Suppose that the samples with predicted values close to an anchor value of $\vec{\alpha}$ are considered risky samples. Then, we can probe the trained model by setting the loss function for $r \geq 1$ as
%\begin{equation}
%\label{eq:case2}
 %      G(\vec{x}) = \int_{\Theta} \|f(\vec{x}, \vec{\theta}) - \vec{\alpha} \|^r q^*(\vec{\theta})\dd\vec{\theta} + R_G(\vec{x}).
%\end{equation}
%\blue{[Bu şekilde yazınca doğru oluyor ama \eqref{eq:genericG} ile verdiğimiz genel yapıya uymuyor. Genel yapıdan vazgeçmek bir çözüm olabilir.]}
%A typical example is a binary classification model where the model returns the probabilities of belonging to a class and the anchor value is set to 0.5. In other words, we probe the model by asking for data samples that would be around the decision boundary.

\paragraph{Parameter-sensitive samples.} Given a set of parameters $\vec{\theta}^*$ and the distribution $q^*(\vec{\theta})$, we ask the model for data samples that are classified as one value, but would be classified as another if the model parameters were to (perhaps slightly) be perturbed. That is
\begin{equation}
\label{eq:case3}
       G(\vec{x}) = \int_{\Theta} \ell_G(y_{\vec{\theta}}( \vec{x}), 1-y_{\vec{\theta}^*}(\vec{x})) q^*(\vec{\theta})\dd\vec{\theta} + R_G(\vec{x}),
\end{equation}
where we take $\widehat{y}(\vec{x}) \equiv 1-y_{\theta^*}(\vec{x})$ denoting the flipped classification decision that contrasts the model's decision with a fixed set of parameters, $\vec{\theta}^*$. When $\mathcal{Q}$ is a restricted to a family of distributions, like Gaussians with fixed variance, sampling from $q^*(\vec{\theta})$ corresponds to sampling from the vicinity of $\vec{\theta}^*$.

There is a notable distinction between prediction-risky and parameter-sensitive samples. Prediction-risky samples tend to be generated near the decision boundary, while parameter-sensitive case has the flexibility to generate samples that can be near or far from the decision boundary. We further elaborate on this distinction in our computational study section.

\paragraph{Model-contrasting samples.}
We note that the predictor $\widehat{y}$ does not necessarily have to be derived from the current model. It can also be obtained from a different model that we are comparing our current model against. With the function
\begin{equation}
\label{eq:case4}
    G(\vec{x}) = \ell_{G}(y_{\vec{\theta^*}}(\vec{x}), 1 - \widehat{y}(\vec{x})) + R_G(\vec{x}),
\end{equation}
we are asking where our (\textit{e.g.}, linear regression) model differs from another (\textit{e.g.}, XGBoost) model.

\paragraph{Localized samples.} In all of the above cases, we can add a regularizer term $R_G(\vec{x}) = \|\vec{x} - \vec{x}_s\|_{r}$ for $r\geq 1$ to generate synthetic data that is similar locally to a given $\vec{x}_s$. In fact, different weightings can also be applied to different columns to enforce this more or less stringently for different features. 

\paragraph{Feature-restricted samples.} By restricting $\mathcal{P}$ to be supported on data with certain features fixed, such as those features corresponding to age, race, and so on, we can ask the model for all of the above questions but conditioning on certain immutable characteristics. This falls into the class of optimizations, where instead of $G$ we consider 
$G(\varphi(\vz))$ on some other (latent) space $\vz\in Z$. In case of image data, for example, to have our samples conform to the data manifold, $\varphi$ can be taken as the trained decoder module from a VAE. Then, starting with measures $\widetilde{p} \in \mathcal{P}(Z)$ on the latent space, their pushforwards $\varphi_* \widetilde{p}$ lie on the data manifold, \textit{i.e.}, sampling $\vz \sim \widetilde{p}$ and computing $\varphi(\vz)$ gives a data sample. 

\begin{comment}
\begin{enumerate}
    \item Given a constant class $y'$, the constant predictor $\hat{y}(\vec{x}) \equiv y'$: We probe the model for what it thinks are good data samples from the distribution class $\mathcal{P}$ that fit the bill for $y'$. This can be applied to both single set of model parameters or a general ensemble of models where the parameters are coming from a distribution $q^*$ (as opposed to a Dirac delta distribution).
    \item Given a set of parameters $\theta^*$ and its associated predictor $\widehat{y_{\theta^*}}$, and a distribution $q^*$ for which $\theta^*$ is the mode of, choosing $\widehat{y}(\vec{x}) = 1 - \widehat{y_{\theta^*}}(\vec{x})$ asks the model for data samples that are classified as one value, but would be classified as another if the model parameters were to (perhaps slightly) be perturbed.
    \item For the binary classification problem, choosing $y' = 0.5$ and the cost function $c(y, y') = |y - y'|^p$ (for $p \geq 1$), means asking the model for data samples that would be around the decision boundary for the model. 
    \item In all of these we can add a $R(\vec{x}) = \|\vec{x} - \vec{x}_0\|_{\ell_p}^p$ 
    regularizer term to generate syntethic data that is similar to a given $\vec{x}_0$, in fact different weightings can too be applied to different columns to enforce this more or less stringently for different features. 
    \item By restricting $\mathcal{P}$ to be supported on data with certain features fixed (such as those features corresponding to age, race etc.) we can ask the model for all of the above questions but conditioning on certain immutable characteristics.
    \item By restricting $\mathcal{P}$ to be supported on data with certain features fixed (such as those features corresponding to age, race etc.) we can ask the model for all of the above questions but conditioning on certain immutable characteristics.    
\end{enumerate} 
\end{comment}

\section{Computational Study}
\label{sec:numerical}

In this section, we conduct a set of experiments to evaluate the cases presented in Section~\ref{sec:probing}. Our experiments aim to evaluate the proposed framework by demonstrating its ability to generate data samples across various scenarios. We use well-known datasets from the literature, and their specifics are outlined in Appendix \ref{app:B}. The implementation details and code for reproducing these experiments are available on our GitHub repository.\footnote{https://github.com/sibirbil/EvD}

\paragraph{Fixed-label samples.}
We apply the probing function $G$ in \eqref{eq:case1b} to the Adult dataset \citep{adult_data} obtained from US census data that is widely used as a benchmark for the binary classification task with the binarized income column (giving whether the individual makes $>$\$$50$k annually or not) as the response variable. We train a logistic regression model on this dataset and examine the behavior of this model by constructing counterfactual samples. Specifically, we choose a data sample $(\vec{x}_0, y)$, and use the probing function \eqref{eq:case1b} with $y' \neq y$ and $R(\vec{x}) = \|\vx - \vx_0\|^2$. 

%This experiment demonstrates how the model responds to a probing question designed to explore the changes required for a factual instance to cross the decision boundary. Using the Adult dataset \cite{adult_data}, a widely recognized benchmark for binary classification tasks, we predict whether an individual earns more than \$50K per year. A logistic regression model is trained on this dataset to evaluate the framework's ability to generate counterfactual data samples for a specific factual instance.

For this experiment, the factual instance represents an unmarried Latin-American Black Female, currently predicted to earn less than \$50K. Through our framework, we pose the question:

\begin{quoting}
What changes in the input features would lead to this individual being classified as having an income greater than \$50K?
\end{quoting}

To address this question, the probing function is designed to generate counterfactual samples by balancing two key objectives: aligning the model's predictions with the desired target label and maintaining proximity to the factual instance. The cost function $\ell_G(y_{\vec{\theta}}(\vec{x}), y')$ given in (\ref{eq:case1b}) is formulated based on the cross-entropy loss with $y'=1$, and the regularizer term $R_G(\vec{x})$ penalizes large deviations between the counterfactual samples and the factual instance.

\begin{comment}
we formulate the probing function given in (\ref{eq:case1b}) as follows and generate 500 counterfactual samples:
\begin{align*}
       G(\vec{x}) = \int_{\Theta} -[y' f(\vec{x}, \vec{\theta}) -  \log(1 + e^{f(\vec{x}, \vec{\theta})} &)]  q^*(\vec{\theta})\dd\theta \\
       & + \|\vec{x} - \vec{x}_s\|^2,
\end{align*}
where $\vec{x}_s$ is our factual instance and $y' = 1$. 
\end{comment}

Figure~\ref{fig:fixedsamples} illustrates the distribution of the generated samples. The results provide insights into the model's classification process and the factors it deems influential in income predictions. While generating counterfactual samples, we impose limitations (lower and upper bounds) on the potential values of certain features, namely age, educational attainment, and weekly working hours. These bounds are integrated into the Langevin dynamics sampling process, which ensures that each step is clipped to remain within the specified ranges. Additionally, we note that in all our experiments, the generated samples are projected to stay within the feature value ranges observed in the original dataset ({\textit{i.e.}\ the given training and test set). In Figure~\ref{fig:fixedsamples}, the factual instance is marked in red, with a vertical dashed line for numerical features and a red text label for categorical features. A comparison between the factual instance and the distribution of counterfactual samples reveals significant categorical changes. For example, the majority of counterfactual samples indicate a change in gender from female to male, and the native country shifts from Latin America to Western Europe. Gender and native-country columns show implications for fairness and bias. Since we opted for logistic regression as our trained model, one may also directly investigate the coefficients associated with these features. However, the bias towards the male gender is more difficult to observe from the respective coefficients (female $\approx -1.375$ vs.\ male $\approx -1.243$) than from the generated data. More importantly, such coefficients are not readily available for more complex models like deep networks.

\begin{figure}
\centering
\includegraphics[width = 0.48\textwidth]{figures/FixedLabel_sample.png}
\caption{Feature distributions of generated counterfactual samples (blue shaded) with factual instance highlighted (red markers).}
\label{fig:fixedsamples}
\end{figure}

\paragraph{Prediction-risky samples.}
This experiment aims to explore data samples near the decision boundary, where model predictions are inherently uncertain. To guide this analysis, we pose the following question to our framework:  
\begin{quoting}
What kind of data samples are predicted to be risky due to being close to a specific anchor value?
\end{quoting}
For this experiment, we use the FICO dataset \citep{fico_data}, which consists of credit applications with features related to financial history and risk performance. A neural network (MLP) is trained to predict whether a customer belongs to the ``Good'' or ``Bad'' credit class. To identify boundary samples, we define the decision boundary as the region where the model's predicted probabilities are close to the anchor value of 0.5. Using our framework, we generate and analyze 500 boundary samples to gain insights into the characteristics of individuals who are borderline cases for classification. The mean probability of belonging to the ``Bad'' credit class for these generated prediction-risky samples is calculated as 0.525, with a standard deviation of 0.017. 

The density plots in Figure~\ref{fig:Risky} compare the distributions of two representative features in the original data and the generated prediction-risky samples. The feature \texttt{NumTrades60Ever2DerogPubRec} represents the number of past credit trades, where payments were delayed by at least 60 days, serving as a key indicator of past delinquency. As shown in Figure~\ref{fig:RiskyF2}, the distribution of risky samples follows the original data closely, particularly in the lower range. However, the generated samples exhibit a stronger peak around zero, indicating that the model considers individuals with few or no past delinquencies as borderline cases. This implies that, for individuals with little to no history of late payments, the model seems to find it more difficult to make a confident classification, likely due to a lack of strong negative or positive indicators. The feature \texttt{MSinceOldestTradeOpen} represents the number of months since a customer’s first credit line was opened, effectively capturing the length of their credit history. As seen in Figure~\ref{fig:RiskyF1}, the distribution of generated risky samples is highly concentrated around 400 months ($\sim$33 years), whereas the original data is spread over a much wider range. This behavior suggests that the model associates longer credit histories with greater uncertainty in classification. The sharp peak around this value indicates that the model fixates on long-established credit histories as an ambiguous factor when making predictions.
For additional comparison, the distributions of the remaining features can be found in Appendix \ref{app:C}.

\begin{comment}
\begin{figure}%[h!]
\centering
\includegraphics[width = 0.49\textwidth]{figures/RiskyFeatures.png}
\caption{Feature distributions in the original data and generated prediction-risky samples.}
\label{fig:sensitivesamples}
\end{figure}
\end{comment}

\begin{figure}[h!]
    \centering
    % Subfigure for Test Data
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/RiskyFeature2.png}
        \caption{Feature distribution in the original data.}
        \label{fig:RiskyF2}
    \end{subfigure}
    \hfill
    \hfill
    % Subfigure for Synthetic Data
    \begin{subfigure}[b]{0.35\textwidth}
        \vspace{10pt}
        \centering
        \includegraphics[width=\textwidth]{figures/RiskyFeature1.png}
        \caption{Feature distribution in the generated samples.}
        \label{fig:RiskyF1}
    \end{subfigure}

    \caption{Feature distributions in the original data and generated prediction-risky samples.}
    \label{fig:Risky}
\end{figure}

\paragraph{Parameter-sensitive samples.} This experiment investigates data samples that are sensitive to small perturbations in the model parameters. Unlike prediction-risky samples, which are concentrated near the decision boundary, parameter-sensitive samples may exist anywhere in the input space, as their classification changes when the model parameters shift slightly. To guide this analysis, we pose the following question to our framework:
\begin{quoting}
What kind of data samples would exhibit prediction instability under small perturbations of the model parameters?
\end{quoting}

For this experiment, we train an MLP model on the FICO dataset to identify parameter-sensitive samples and directly compare the findings with those obtained for prediction-risky samples. To generate parameter-sensitive samples, we perturb the model parameters by sampling from a Gaussian distribution centered at the original parameters with a fixed variance. Using the probing function in (\ref{eq:case3}), we generate and analyze 500 samples to understand which instances are most susceptible to changes in model parameters.

The density plots in Figure~\ref{fig:sensitivesamples} compare the distributions of four representative features (see Appendix \ref{app:C} for the remaining features) in the generated parameter-sensitive samples and prediction-risky samples. By comparing these two distributions, we gain insights into how the model perceives uncertainty from different perspectives. While the prediction-risky samples are associated with uncertainty near the decision boundary, the parameter-sensitive samples highlight regions in the feature space where small perturbations in the model’s parameters can lead to classification shifts. The features \texttt{AverageMinFile} (average observation period) and \texttt{NumTotalTrades} (total number of trades) exhibit similar distributions across both generated sample types. In contrast, the features \texttt{MSinceMostRecentTradeOpen} (months since most recent trade opened) and \texttt{NumInqLast6M} (the number of inquiries in the last six months) show a clear divergence. For instance, \texttt{NumInqLast6M} represents the number of times a customer has had their credit history checked in the last six months, which is often linked to recent credit-seeking behavior. For this feature, the prediction-risky samples cluster around lower values, suggesting that individuals with fewer recent inquiries are more likely to be classified as borderline cases. On the other hand, the parameter-sensitive samples exhibit a broader and more spread-out distribution, indicating that parameter shifts affect individuals across a wider range of credit-scoring inquiries. This may be because frequent inquiries can indicate diverse financial behaviors, making these samples more susceptible to instability when model parameters change. These findings suggest that certain features contribute more significantly to classification robustness against parameter variations, whereas others primarily influence boundary-sensitive classifications. 

\begin{figure}%[h!]
\centering
\includegraphics[width = 0.49\textwidth]{figures/parameter_sensitive.png}
\caption{Feature distributions in generated parameter-sensitive and prediction-risky samples.}
\label{fig:sensitivesamples}
\end{figure}


\paragraph{Model-contrasting samples.}
This experiment investigates the differences between two predictive models by probing the features that drive contrasting predictions for the same data. Through our framework, we pose: 
\begin{quoting}
Which specific features or input changes lead to disagreement between the two models' predictions?
\end{quoting} 

For this experiment, we use datasets that have two different modalities: tabular and visual. The tabular datasets include Housing \citep{housing_data} and FICO, while the visual dataset is MNIST \citep{mnist_data}.

In the Housing dataset experiment, we compare support vector regression (SVR) and LR models. We split the dataset into training-test sets and trained both models on the same training data. To generate data samples where the two models diverge in their predictions, we formulate the cost function given in (\ref{eq:case4}) as $\ell_{G}(y_1, 1-y_2) = \exp(- (y_1 - y_2)^2)$. 
%using an exponential function of the squared difference between the predictions of the SVR and LR models. 
Using our framework, we generate data samples to identify the regions of the input space where the models exhibit significant disagreement, likely due to their differing assumptions about feature interactions and predictive mechanisms. 

Figure \ref{fig:modelcontrast} presents a scatter plot comparing the predictions of the LR and SVR models. The blue points represent the predictions of the models in the test data, demonstrating that the two models generally produce highly similar outputs, with minimal differences observed. The green points, on the other hand, represent generated samples, highlighting instances where the models exhibit contrasting predictions. The zoomed-in inset further emphasizes these discrepant predictions, demonstrating that our framework effectively identifies and generates data points that maximize the divergence between the two models.

\begin{figure}[h!]
\centering
\includegraphics[width = 0.38\textwidth]{figures/Modelcontrast.png}
\caption{Comparison of SVR and LR predictions on test data and generated samples.}
\label{fig:modelcontrast}
\end{figure}

Figure \ref{fig:FeatureDist} compares the feature distributions between the synthetic dataset generated by our framework and the test data. The box plots represent the range of values for each feature, with blue corresponding to the test data and green representing the generated samples. This figure provides a clear visualization of how the generated data differs from the test data in terms of feature distributions. For instance, as the number of bathrooms and stories increases, the model predictions diverge. Additionally, hot water heating and air conditioning exhibit a distinct concentration in the synthetic data, with most generated samples clustering around higher values compared to the test data. This suggests that these features play a prominent role in distinguishing instances where the models behave differently. Overall, this figure offers insights into how the generated samples differ from the original dataset, highlighting key feature distributions that drive divergence in model predictions and providing a deeper understanding of how our framework probes model behavior.

\begin{figure*}
\centering
\includegraphics[width = 0.60\textwidth]{figures/featureMC.png}
\caption{Feature distribution in test data and generated samples that produce different predictions for SVR and LR.}
\label{fig:FeatureDist}
\end{figure*}

We can also investigate model divergence in cases where the comparison model is non-differentiable.  To demonstrate this, we train XGBoost -a non-parametric model- alongside logistic regression on the FICO dataset. This setup highlights the flexibility of our framework, as it allows us to probe differences between fundamentally different modeling approaches. Two models agree on 94.5\% of the predictions in the test data. However, we generate a set of samples where the models exhibit full disagreement, \textit{i.e.}, XGBoost predicts one class, while logistic regression predicts the opposite. Figure~\ref{fig:logregvsxgboost} presents the feature distributions for these discrepant samples, focusing on four representative features.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/LogReg_XGboost.png}
    \caption{The distributions of four representative features in the generated samples. Here, XGBoost predicts ``Bad'' for \texttt{RiskPerformance}, while logistic regression predicts ``Good''.}
    \label{fig:logregvsxgboost}
\end{figure}

Our framework can also be used to compare and contrast two models trained on image data. To demonstrate, we consider a Convolutional Neural Network (CNN) and an MLP, both trained on MNIST. The architectural details of these networks are provided in Appendix \ref{app:models}. To better capture the data manifold, we also train a VAE with a latent dimension of 10. The trained encoder module of the VAE is denoted by $z \mapsto \varphi(z)$. Further details on the VAE training process are provided in Appendix \ref{app:VAE}. In Figure~\ref{fig:latent_joint_prediction_walk}, we present an example computation illustrating how this setup works. Starting with a latent vector encoding an image with label `3', we sample from a distribution that prefers the label `8' jointly for both a trained CNN (LeNet5) and an MLP.

\begin{figure}
    \centering
    \includegraphics[width=0.43\textwidth]{figures/latent-distribution.pdf}
    \caption{Using Langevin dynamics in the latent space, we obtain a sequence of latent vectors that, when passed through the decoder $\varphi$, correspond to a walk on the data manifold. In this image, the function $G$ is the sum of cross entropy predictions of trained MLP and LeNet5 networks for the label `8' and for the data $\varphi(z)$.}
    \label{fig:latent_joint_prediction_walk}
\end{figure}

We use this setup to systematically compare the CNN and MLP models. In Figure \ref{fig:CNNvsMLP}, we showcase some samples generated by forcing functions $G$ that pull the data in incompatible directions, for example resulting in amorphous data points that exhibit characteristics of both `1' and `0'. The third column highlights cases where the label `8' is preferred (top: MLP, bottom: CNN) while remaining close to an actual MNIST image labeled `3', which is enforced through two-norm regularization. 

\begin{figure}
    \centering
    \includegraphics[width=0.40\textwidth]{figures/mnist_grid.png}
    \caption{Images in the first and second columns are generated to prefer a given label on an MLP model and another one on a CNN model. upper-left: CNN-`0' MLP-`1',  upper-middle: CNN-`1' MLP-`7', lower-left: CNN-`0' MLP-`8', lower-middle: CNN-`2' MLP-`5'. On the third column, the upper image prefers the label `8' for the MLP model whilst being close to a data sample with label `3', and the same for the lower image for the CNN model.}
    \label{fig:CNNvsMLP}
\end{figure}

\begin{comment}
    

Let's assume $f(\vec{x}, \theta) = \frac{1}{1+e^{-x\tr_i \theta}}$ is obtained from a logistic regression model.  The loss function is defined as 
\[
F(\theta) = \frac{1}{N} \sum_{i=1}^N [\log (1+e^{x\tr_i \theta}) - y_i x\tr_i \theta ].
\]

Case 1: We aim to generate instances that are close to the decision boundary, which represent uncertain or \emph{risky} cases for classification. Given the distribution of $\theta, q^{*}(\theta)$,  we can rewrite the second step loss function as 
\[
G(f(\vec{x}, \theta), \tilde{y}) = \int_{\theta} (f(\vec{x}, \theta) - \tilde{y})^2 dq^{*}(\theta),
\]
where we can set $\tilde{y} = 0.5$ to reflect the boundary condition. This loss function encourages the generation of instances that lie close to the boundary, allowing us to explore regions of higher classification uncertainty.

For Dirac, we have
\[
G(f(\vec{x}, \theta^*), \tilde{y}) = (f(\vec{x}, \theta^*) - \tilde{y})^2,
\]
where $\theta^* = \argmin_{\theta \in \Theta} F(\theta)$.

Case 2: Our objective is to generate instances that are similar to a specific instance $x_0$, while also maintaining good classification quality. This could be useful when we want to collect statistics or gain insights in the vicinity of a particular data point. In this case, we can rewrite the second step loss function as 
\[
G(f(\vec{x}, \theta), y) = (\vec{x}_0 - \vec{x})^2 + \lambda \int_{\theta} (\log (1+e^{\vec{x} \tr \theta}) - y \vec{x} \tr \theta) dq^{*}(\theta),
\]
where $(\vec{x}_0 - \vec{x})^2$ ensures generated instances remain close to 
$x_0$, and $\int_{\theta} (\log (1+e^{\vec{x} \tr \theta}) - y \vec{x} \tr \theta) dq^{*}(\theta)$ promotes accurate classification. The parameter $\lambda$ balances the trade-off between similarity to $x_0$ and classification accuracy.

For Dirac, we have
\[
G(f(\vec{x}, \theta^*), y) = (\vec{x}_0 - \vec{x})^2 + \lambda (\log (1+e^{\vec{x} \tr \theta^*}) - y \vec{x} \tr \theta^*).
\]

Şunları unutmayalım:
\begin{itemize}
    \item Model, yani $f(\vec{x}, \theta)$, kullanıcıdan alınacak.
    \item Ayrıca $F(\theta)$ fonksiyonuna da ihtiyaç var.
\end{itemize}
\end{comment}

\section{Conclusion}

In this work, we propose a mathematical framework for probing trained models by generating data samples tailored to specific queries. Our approach provides a novel way to understand model behavior beyond well-known interpretability methods, such as feature saliency or surrogate models. By formulating probing functions, we demonstrate how to generate samples under various scenarios such as prediction-risky, parameter-sensitivity, and contrasting models. Our computational study confirms the effectiveness of the proposed framework across classification and regression tasks on various datasets, providing insights into model decision boundaries and sensitivity to input perturbations.

Despite its strengths, our framework has certain limitations. First, scaling to larger models, particularly deep learning architectures with billions of parameters, poses computational challenges. The iterative optimization and sampling procedures may become prohibitively expensive in such settings. Furthermore, due to implicit constraints among the features, our method may generate samples that are not representative enough of the dataset, potentially leading to narrow conclusions. 

For future research, an interesting direction is incorporating constraints among features to ensure that the generated samples remain plausible and adhere to known data dependencies. For instance, enforcing domain-specific relationships, such as monotonic constraints among features, could enhance the interpretability and reliability of the generated samples. By addressing these aspects, we can further refine data-driven explainability techniques.

% \clearpage 
% \section*{Impact Statement}

% Our work's aim is understanding machine learning models with the intention of creating a positive societal impact in various areas where these models are utilized. The tools presented in this paper are designed to assist in the analysis and comprehension of trained models through sample generation.

\bibliography{EvDarxiv}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\onecolumn

\section{Linear Regression with Gaussian Data}
\label{app:LR}

\newcommand{\D}{\mathbb{D}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\XX}{\mathscr{X}}
\renewcommand{\d}{\mathrm{d}\,}
\newcommand{\xxi}{{\boldsymbol{\xi}}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\eeta}{{\boldsymbol{\eta}}}
\newcommand{\vvarepsilon}{{\boldsymbol{\varepsilon}}}
\newcommand{\mmu}{{\boldsymbol{\mu}}}

We start with $y_{\boldsymbol{\theta}}(\vec{x}) = \boldsymbol{\theta}^\top \vec{x}$ and $\ell_F(y, y') = \ell_G(y,y') = \frac{1}{2}(y - y')^2$. Given a dataset $\{(\vec{x}_i, y_i)\}_{i = 1}^N$, we construct the loss function $F(\ttheta)$ as the integral of $\ell_F(y_{\boldsymbol{\theta}}(\vec{x}), y)$, over the data distribution, which is approximated by the Dirac delta comb $\nu = \frac{1}{N}\sum_{i = 1}^N \delta_{(\vec{x}_i, y_i)}$:
\[
	F(\ttheta) = \int_{\mathcal{X}\times \mathcal{Y}} \!\!\!\! \ell_F(y_{\boldsymbol{\theta}}(\vec{x}), y) \d\nu(\mathbf{x},y) = \frac{1}{2N} \sum_{i = 1}^N |\vec{x}_i^\top \ttheta - y_i|^2.
\]
Assume, for convenience, that a constant feature of 1 is included as the last coordinate of $\vec{x}$, allowing us to explicitly represent the intercept. Using this notation, we define
\[
	\vec{x} = \begin{bmatrix}
	\vec{f}\\ 1
	\end{bmatrix}, \qquad \ttheta = \begin{bmatrix} \xxi & b \end{bmatrix}, \text{ so that } \vec{x}^\top \ttheta = \vec{f}^\top \xxi + b. 
\]
We write the design matrix as
\[
	D = \left[\begin{array}{ccc;{2pt/2pt}c}
	\cdots & \vec{x}_1^\top & \cdots  & 1 \\
	\cdots & \vec{x}_2^\top & \cdots & 1 \\
	& \vdots &  & \vdots\\
	\cdots & \vec{x}_N^\top &\cdots  & 1
	\end{array}\right] = \left[ \begin{array}{c;{2pt/2pt}c} X & \mathbf{1}\end{array}\right].
\]

The quadratic loss function can then be expressed as
\[
	F(\ttheta) = \frac{1}{2N}\|D \ttheta - \vec{y}\|^2,
\]
where $\vec{y} = \begin{bmatrix}
y_1 & y_2 & \cdots &y_N
\end{bmatrix}^\top$ is the label vector. We can reorder the terms so that
\begin{align*}
	F(\ttheta) &= \frac{1}{2N} (D\ttheta - y)^\top (D\ttheta - y) = \frac{1}{2N}\left(\ttheta^\top D^\top D \ttheta - 2\ttheta^\top X^\top \vec{y}\right) + \text{const.}\\
	&= \frac{1}{2} (\ttheta - \widehat{\ttheta})^\top \frac{D^\top D}{N} (\ttheta  - \widehat{\ttheta}) + \text{const}. 
\end{align*}
where $\widehat{\ttheta} = (D^\top D)\inv D^\top \vec{y}$. Note that this is precisely the ordinary least squares solution. 

Since the loss function is quadratic, we can explicitly write the Gibbs distribution (which is the unrestricted solution to the Bayesian Learning Problem with $F$) as the Gaussian distribution
\[
	q^*(\theta) \propto e^{- \beta F(\theta)} \propto e^{- \frac{1}{2}(\ttheta - \widehat{\ttheta} )^\top  \frac{D^\top D}{N/\beta} (\ttheta - \widehat{\ttheta})} \quad \text{thus } q^*(\vec{\theta}) = \mathcal{N}\left(\widehat{\ttheta}, \left(\frac{D^\top D}{N/\beta}\right)\inv\right).
\]
Here, the variable $\beta$ is the inverse temperature defined as $\beta = 1/\tau$. 

Next, we construct $G$, a loss function on $\mathcal{X} \times \mathcal{Y}$. By fixing the label, we may also consider $G$ as a loss function only on $\mathcal{X}$, from which we derive a distribution over $\mathcal{X}$. To avoid overusing $\vec{x}$ and $y$, we denote elements of the labeled dataset as $(\vec{z}, w) \in \mathcal{X} \times \mathcal{Y}$ with $\vec{z} = \left[\begin{smallmatrix} \vec{f} \\ 1\end{smallmatrix} \right]$. Using the first and second moments of Gaussians, we calculate
\begin{align*}
	G(\vec{z}, w) &= \int_{\Theta} |\vec{z}^\top \ttheta - w|^2 q^*(\ttheta) \d \theta \\
	&=\vec{z}^\top  \E_{q^*}[\ttheta \ttheta^\top]\vec{z} - 2w  \vec{z}^\top \E_{q^*}[\ttheta]+ \text{ const}\\
	&= \vec{z}^\top \left(\widehat{\ttheta}\widehat{\ttheta}^\top  + \left(\frac{D^\top D}{N\tau}\right)^{-1} \right) \vec{z} - 2w \vec{z}^\top \widehat{\ttheta} + \text{ const}.
\end{align*}
which is again a quadratic function in $\mathbf{z}.$
Let us now write this quadratic in terms of $\mathbf{f}$. We write $\widehat{\ttheta}= \left[\begin{smallmatrix} \widehat{\xxi} \\ \widehat{b} \end{smallmatrix}\right]$. 

First, a quick calculation gives the block diagonal form
\begin{align*}
	\left(\frac{D^\top D}{N\tau}\right)\inv 
	&= \tau \left[\begin{array}{c;{3pt/3pt}c} \frac{X^\top X}{N} & \overline{\vec{x}}\\\hdashline[3pt/3pt]
	\overline{\vec{x}}^\top & 1
	 \end{array}\right]\inv \\
	 &= \tau \left[ \begin{array}{c;{3pt/3pt}c} A\inv & -A\inv \overline{\vec{x}}\\ \hdashline[3pt/3pt]
	 -\overline{\vec{x}}^\top A\inv & * \end{array} \right],
\end{align*}
where $A = \frac{X^\top X}{N} - \overline{\vec{x}}\overline{\vec{x}}^\top$ is the Schur complement and $\overline{\vec{x}} = \frac{1}{N} \sum_{i = 1}^N \vec{x}_i$ is the mean data vector. 

We can write $G$ as a quadratic function of $\vec{f}$ (fixing $w$) as 
\begin{align*}
	G_w(\vec{f}) &= \vec{f}^\top \left( \tau A\inv + \widehat{\xxi}\widehat{\xxi}^\top \right) \vec{f} - 2\vec{f}^\top \left( \tau A\inv \overline{\vec{x}} - \widehat{\xxi}\hat{b} + w\hat{b} \right) + \text{const.}\\
	&= (\vec{f} - \widehat{\vec{f}}) \left( \tau A\inv + \widehat{\xxi}\widehat{\xxi}^\top \right) (\vec{f} - \widehat{\vec{f}}) + \text{const}. 
\end{align*}

Here, $\widehat{\vec{f}}$ is calculated as
\begin{align*}
	\widehat{\vec{f}} &= \left(  \tau A\inv + \widehat{\xxi}\widehat{\xxi}^\top \right)\inv \left(\tau A\inv \overline{\vec{x}}  + \widehat{\xxi}(w - \hat{b})\right)\\
	&= \left(A_\tau - \frac{A_\tau\widehat{\xxi} \widehat{\xxi}^\top A_\tau}{1 + \widehat{\xxi}^\top A_\tau \widehat{\xxi}} \right)\left( A_\tau\inv \overline{\vec{x}} + \widehat{\xxi}^\top (w - \hat{b})\right), 
\end{align*}
where $A_\tau = \frac{1}{\tau}A$ and the Sherman-Morrison formula is used for inverting the matrix.

Now expanding the product, we obtain
\[
	\widehat{\vec{f}} = \overline{\vec{x}} + A_\tau \widehat{\xxi}(w - \hat{b}) - \frac{A_\tau \widehat{\xxi} \widehat{\xxi}^\top \overline{\vec{x}}}{1 + \widehat{\xxi}^\top A_\tau \widehat{\xxi}} - A_\tau \widehat{\xxi} \frac{\widehat{\xxi}^\top A_\tau \widehat{\xxi}}{1 + \widehat{\xxi}^\top A_\tau \widehat{\xxi}} (w -\hat{b}).
\]
Note that if we denote the predictions of the linear model as $\vec{x}_i^\top \widehat{\xxi} + \widehat{b} = \hat{y}_i$, we can rewrite the above formula as follows: 
\begin{align*}
	\widehat{\vec{f}} &= \bar{\vec{x}} + A_\tau \widehat{\xxi} (w - \widehat{b})  \frac{1}{1 + \hat{\xxi}^\top A_\tau \hat{\xxi}} - A_\tau \hat{\xxi} \frac{\hat{\xxi}^\top \bar{\vec{x}}}{1 + \hat{\xxi}^\top A_\tau \hat{\xxi}}\\
	&= \bar{\vec{x}} + A_\tau\hat{\xxi}  \frac{(w - \hat{b}) }{1 + \hat{\xxi}^\top A_\tau \hat{\xxi}} - A_\tau \hat{\xxi} \frac{(\widehat{\overline{\vec{y}}}- \hat{b})}{1 + \hat{\xxi}^\top A_\tau \hat{\xxi}}\\
	&= \bar{\vec{x}} + A_\tau \hat{\xxi} \frac{w- \widehat{\overline{\vec{y}}}}{1 + \hat{\xxi}^\top  A_\tau \hat{\xxi}}.
\end{align*}
Here, we denoted the prediction of the average data by $\widehat{\overline{\vec{y}}} = \hat{\xi}^{\top }\bar{\vec{x}} = \frac{1}{N} \sum_{i = 1}^N \hat{y}_i$. 

Finally, let's rewrite $A_\tau\widehat{\xxi}$ and $\widehat{\xxi}^\top A_\tau \widehat{\xxi}$ in terms of interpretable statistical quantities. Recall that $A_\tau = \frac{1}{\tau} \left( \frac{X^\top X}{N} - \overline{\vec{x}} \overline{\vec{x}}^\top  \right)$. Using this, we compute
\begin{align*}
	A_\tau \widehat{\xxi} &= \frac{1}{\tau}\left( \frac{1}{N} \sum_{i = 1}^N \vec{x}_i  (\underbrace{\vec{x}_i^\top \widehat{\xxi}}_{= \hat{y}_i - \hat{b}})  - \overline{\vec{x}} \underbrace{\bar{\vec{x}}^\top \hat{\xxi}}_{=\frac{1}{N} \sum_{i = 1}^N \hat{y}_i - b}\right)\\
	&=   \frac{1}{N\tau }\sum_{i = 1}^N (\vec{x}_i - \overline{\vec{x}}) (\hat{y}_i - \hat{b})\\
	&=   \frac{1}{N\tau}\sum_{i = 1}^N (\vec{x}_i - \overline{\vec{x}}) (\hat{y}_i - \hat{\overline{\vec{y}}})\\
 &= \frac{1}{\tau} \operatorname{Cov}(X, \widehat{\vec{y}}).
\end{align*}
In the final expression, the term $\operatorname{Cov}(X, \widehat{\vec{y}})$ corresponds directly to the previous line. This covariance is a vector that averages data deviations, weighted by prediction deviations. In the line before last, we replaced $\widehat{b}$ with any constant since it is independent of $i$, and the first factor sums to the zero vector. Additionally, we leveraged a key property of linear models: the average of the predictions is the same as the prediction of the average.

A similar calculation yields,
\[
	\hat{\xxi}^\top A_\tau \hat{\xxi} = \frac{1}{\tau}\left(\frac{1}{N} \sum_{i = 1}^N \hat{y}_i^2 - \left(\frac{1}{N} \sum_{i = 1}^N \hat{y}_i\right)^2\right) =\frac{1}{\tau} \operatorname{Var}(\widehat{\vec{y}}).
\]

Therefore, we obtain an explicit quadratic formulation of the data loss function $G$ in terms of $\vec{f}$ at a fixed $w$.  This means that the data distribution $p^*(\vec{x})$, which solves the unrestricted Bayesian Learning Problem, follows a Gaussian distribution given as
\[
	p^*(\vec{f}) \propto e^{-G_w(\vec{f})} \propto \mathcal{N}(\widehat{\vec{f}}, \Sigma),
\]
where
\[
	\widehat{\vec{f}} = \overline{\vec{x}} + \frac{\operatorname{Cov}(X,\widehat{\vec{y}}) }{\tau + \operatorname{Var}(\widehat{\vec{y}})} \left(w  - \frac{1}{N}\sum_{i = 1}^N \widehat{y}_i\right),
\]
and 
\[
	\Sigma\inv = \left( \tau\left(\frac{X^\top X}{N} - \bar{\vec{x}}\bar{\vec{x}}^\top\right)\inv + \widehat{\xxi}\widehat{\xxi}^\top \right).
\]

The interpretation of the mean $\widehat{\vec{f}}$ is as follows: if you want to sample from a data distribution that will produce a given $\omega$, then you should not sample around $\bar{\vec{x}}$ (which would be the case without output restrictions). Instead, you shift $\bar{\vec{x}}$ in proportion to the difference between $\omega$ and the mean of the training label predictions, following the direction of the covariance between the training data and predicted labels.

\newpage
\section{Computational Setup}
\label{app:B}

\subsection{Datasets Used in the Experiments}
\label{app:datasets}
Our experiments are conducted using three numerical datasets and one visual dataset from the literature. The details of the datasets are provided below.

\paragraph{Adult.} The Adult dataset, derived from the 1994 Census database, comprises 48,842 observations with 14 features, including both continuous and categorical variables \citep{adult_data}. The primary objective is to classify individuals based on whether their annual income exceeds \$50,000~USD. Data preprocessing steps are applied to address missing values and handle categorical features. We applied one-hot encoding to transform the categorical features into a numerical format suitable for our framework.

\paragraph{FICO.} The FICO (HELOC) dataset consists of home equity line of credit applications submitted by homeowners \citep{fico_data}. It includes 10,459 records with 23 features, comprising both numerical and ordinal variables. The primary objective is to classify applications based on their risk performance, identifying whether an applicant is likely to meet payment obligations or become delinquent. Data preprocessing steps are applied to address missing values.

\paragraph{Housing.} The Housing dataset, sourced from Kaggle, includes information on various house attributes such as lot size, number of rooms, and number of stories \citep{housing_data}. The dataset contains 535 records and 12 features, comprising both numerical and ordinal variables. The primary objective is to predict housing prices based on these features.

\paragraph{MNIST.} The MNIST dataset is a widely used benchmark in computer vision, consisting of 70,000 grayscale images of handwritten digits (0–9), each represented as a 28×28 pixel matrix \citep{mnist_data}. The dataset is divided into 60,000 training samples and 10,000 test samples. The primary objective is to classify images based on the digit they represent. We normalized each of the images to be arrays of shape $(28,28,1)$ with FP32 values in the interval $[0,1]$.

\subsection{Experimental Setup}
\label{app:models}

For the parameter-sensitive and prediction-risky experiments on the FICO dataset, we trained an MLP with ReLU activation functions and layer widths of $128-32-8-2$. Dropout with a rate of 0.2 was applied after each activation layer to prevent overfitting. The model was trained using a batch size of $128$ for $10,000$ steps.

For the image experiments, we used an MLP with layer widths of $1024$-$128$-$10$, where each layer included a ReLU activation, followed by a dropout layer with a rate of 0.2. The CNN architecture consisted of two convolutional blocks with feature sizes $32-64$. Each block followed the structure: $\text{ Conv}\rightarrow\text{ReLU}\rightarrow\text{Conv}\rightarrow\text{ReLU}\rightarrow\text{ max\_pool}\rightarrow\text{Dropout}$, where the convolutional kernels had a size of $3 \times 3$, the max pooling window was $2 \times 2$, and the dropout rate was 0.2.

Both the CNN and MLP models were trained for 10,000 update steps using a batch size of 128 and the Adam optimizer. The learning rate followed an exponential decay schedule, starting with a maximum learning rate of 0.1, decaying by a rate of 0.9 every 100 steps.



\newpage

\section{Additional Numerical Results}
\label{app:C}

This appendix presents additional results that complement the findings discussed in Section \ref{sec:numerical}. These results provide further insights into the generated data distributions, feature variations, and model behavior under different probing scenarios. 

\paragraph{Fixed-label samples.} We now analyze a different factual instance from the original data to further investigate the model’s behavior. The factual instance considered represents a married Latin American white male who is predicted to earn more than \$50K. To explore the conditions under which the model would classify this individual as earning less than \$50K, we generate a set of counterfactual samples. Figure \ref{fig:appendixFL} presents the distribution of these generated counterfactual samples, highlighting the key feature variations that lead to a different classification outcome. In the generated counterfactual samples, while no categorical changes are observed, the numerical features age, educational attainment, and working hours exhibit lower values compared to the factual instance, implying that a reduction in these features leads to a shift in classification.

\begin{figure}[h]
\centering
\fbox{\includegraphics[width = 0.75\textwidth]{figures/appendix_FL.png}}
\caption{Feature distributions of generated counterfactual samples (blue shaded) with factual instance highlighted (red markers).}
\label{fig:appendixFL}
\end{figure}

\paragraph{Prediction-risky samples. } To further investigate data samples near the decision boundary, we present the distributions of all features in the original dataset and the generated prediction-risky samples in Figure~\ref{fig:appendixPR}. These density plots provide a comprehensive view of the differences between the generated samples and the original data across multiple features. By analyzing these distributions, we can observe how the model identifies borderline cases based on different financial attributes. Across multiple features, the generated prediction-risky samples exhibit a much narrower distribution compared to the original data. This suggests that the model focuses on a specific subset of feature values when identifying borderline cases.

\begin{figure}[h!]
\centering
\fbox{\includegraphics[width = 0.98\textwidth]{figures/appendixRisky2.png}}
\caption{Feature distributions in the original data and generated
prediction-risky samples.}
\label{fig:appendixPR}
\end{figure}

\clearpage

\paragraph{Parameter-sensitive samples.}To complement the findings presented in Section 4, we provide the full set of feature distributions comparing parameter-sensitive samples and prediction-risky samples in Figure~\ref{fig:appendixPS}. These density plots illustrate how the two types of generated samples differ. By analyzing these distributions, we observe that while some features exhibit similar trends across both sample types, others show notable divergences. Features with broader distributions in parameter-sensitive samples indicate that model perturbations impact a wider range of instances.



\begin{figure}[h!]
\centering
\fbox{\includegraphics[width = 0.98\textwidth]{figures/appendixParameter.png}}
\caption{Feature distributions in generated parameter-sensitive
and prediction-risky samples.}
\label{fig:appendixPS}
\end{figure}


\clearpage
\section{The use of VAEs}
\label{app:VAE}

A notable example of using pushforwards to obtain points on the data manifold comes from image datasets. We employ a VAE architecture with two convolutional layers each for the encoder and decoder submodules. Features in the convolutional layers are $32$ and $64$ with kernel sizes of (3,3) and a stride of (2,2). During training, the reconstruction loss is computed using bitwise entropy.

Figure \ref{fig:VAE} shows how this setup works for constructing loss functions $G$ on the latent space. One may use a combination of models, each precomposed with the decoder of the trained VAE. The resulting distribution on the latent space, after pushforwarding (\textit{i.e.}, passing the samples through the decoder), corresponds to a distribution on the data that is closer to the original data distribution.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/CombinedVAE.png}
    \caption{By precomposing with the decoder submodule of a trained neural network, we can define $G$ functions on the lower-dimensional latent space, while still leveraging networks designed for higher-dimensional image inputs.}
    \label{fig:VAE}
\end{figure}


\begin{comment}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{figures/VAE.pdf}
    \hspace{40pt}
    \includegraphics[width = 0.35\linewidth]{figures/VAE-classifer.pdf}
    \caption{By precomposing with the decoder submodule of a trained neural network, we can define $G$ functions on the lower-dimensional latent space, while still leveraging networks designed for higher-dimensional image inputs.}
    \label{fig:VAE}
\end{figure}



\begin{table}[h]
\caption{The details of the datasets used in the experiments}
\label{tab:datasets}%
\centering
\footnotesize
\begin{tabular}{ccccc}
\hline
\multicolumn{1}{l}{Name} &  \multicolumn{1}{c}{Num of Features}  & \multicolumn{1}{c}{Cat. Features} & \multicolumn{1}{c}{Num. of Instances} & \multicolumn{1}{c}{Type} \\
\hline
\hline
\multicolumn{1}{l}{Adult} & \multicolumn{1}{c}{15} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{49,000} & \multicolumn{1}{c}{Classification}\\  
\multicolumn{1}{l}{FICO} &\multicolumn{1}{c}{20} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{10,459} & \multicolumn{1}{c}{Classification} \\ %\hline
\multicolumn{1}{l}{Housing}  & \multicolumn{1}{c}{12} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{545} & \multicolumn{1}{c}{Regression} \\ %\hline
\hline
\end{tabular}%
\end{table}%
\vspace{0.1in}
\end{comment}

\end{document}

\clearpage

\textbf{TO-DO List}

\textbf{Computational Study}

\begin{enumerate}
    \item Fixed Label Samples (a.k.a counterfactual): Numeric degerleri clipping ile fix alip (ya da kisitlayip) categorical variablelar uzerinde degisimleri report edelim.

    \item Risky-Samples: Regularization olmayacak. Bu nedenle 10 farkli initial point ile 10 farkli chain elde edilecek ve her birinden sample alip ortalama sonuclar report edilecek.

    \item Parameter-edgy samples: Regularization olmayacak. Amac parameter degisiminden etkilenen sample'larin risky sample olmadigini gostermek. Bu nedenle parameter-edgy olanlarin prediction probabilitylerinin 0.5'ten yuksek ya da dusuk oldugunu gosterilecek.

    \item Model-contrasting Samples: Housing datasi ve regression ornegi kullanilacak (linear regression ve SVR karsilastirilabilir). Bu nedenle G fonksiyonunu seu sekilde alabiliriz: ($\ell_{G} = e^{-\|\hat{y}_1(\vec{x}) - \hat{y}_2(\vec{x})\|^2} $).

    Image icin CNN ve MLP'nin farkli tahmin verdigi orneklerden bir kacini grid tablosunda gosterebiliriz. Ayni ya da baska bir tabloda hangi modelin karsilik gelen figure icin ne tahmin ettigini yazabiliriz.
    
\end{enumerate}


\textbf{Main Text}
\begin{itemize}
    \item Introduction'in yeni yapiya gore yazilmasi. (İlker)
    \item Literature Review'in guncellenmesi. (Nurşen)
    \item Three-Step framework section'inindaki notasyonun kontrol edilmesi ve figurlerin guncellenmesi. (İlker)
    \item \sout{Three-Step framework: Her adimda cozulen modellerin ayrintilandirilmasi ve bir paragraf acip ``latent space trick'' fikrinin aciklanmasi. (Mehmet)} three-step framework yerine symmetric framework dedim. Tekrar yazdim section'i, bence daha şık oldu. 
    \item  Computational Study'nin Probing Your Model section'i ile baglantili olarak tasarlanmasi, modele sorulacak olan sorularin sekillendirilmesi.(Computational study senaryo based yazilacak icine not dustuk.) (Mehmet $\rightarrow$ Nurşen)
    \item Image datasi icin sonuclarin toparlanmasi. (Mehmet)
    \item \sout{Parametre'lere bagli degisim icin loss fonksiyonunun yazilmasi. (Mehmet $\rightarrow$ İlker)}
    \item \sout{Modele sorulacak olan sorularin olusturulmasi ve bununla ilgili genel formda loss fonksiyonlarinin yazilmasi. (İlker)}
    
\end{itemize}

\clearpage


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

\begin{comment}
Machine learning models are playing an essential role in decision-making across different sectors in our data-driven world today. Whether it is diagnosing diseases in healthcare or making hiring decisions in human resources, these models have a significant impact on people's lives. However, there is a growing need for explainable AI and regulatory compliance to address concerns about the fairness and transparency of these decision-making processes. As machine learning systems become more common in high-stakes scenarios, it is crucial to understand the data preferences of these models to help regulators ensure that AI is deployed fairly and transparently.

While traditional workflows emphasize data collection, cleaning, and model training, the ever-evolving nature of data introduces significant hurdles. For example, a common challenge in real-world applications is that models can exhibit bias. Consider an approval model for social benefit applications which might implicitly screen for applicants from one demographic group leading to unfair rejections [cite SyRI]. Another hurdle compounding these issues could be data drift because data is not static and it evolves over time, leading to changes in sample statistics. This drift can render previously reliable models obsolete. By continuously comparing incoming data to the model's preferred distribution, organizations can develop early warning systems for when their models are likely to become less effective. For example, in public health, a disease monitoring system could detect shifts in population health trends, allowing authorities to identify the early stages of an outbreak or the emergence of new health risks. Yet another hurdle is the difficulty of generating data that closely resembles the model's preferred distribution. By generating such synthetic data, decision-makers can augment their training datasets and test the performances of their models under various scenarios.

These concerns underscore a critical question that lies at the heart of our research: ``What kind of data does our trained model truly prefer?'' We aim at answering this question by studying the implicit data distribution favored by a trained model. We argue that understanding a model's ``ideal data" not only illuminates potential biases embedded within models and their training data but also offers a pathway to more robust and adaptable systems.

\paragraph{Related literature.}
Our work complements the extensive research in data generation. In this section, we review several related works on synthetic data generation and their relevance to our study. Synthetic data plays a crucial role in addressing fundamental challenges in machine learning, including fairness, bias reduction, and robustness. Extensive research has been conducted on detecting bias in datasets \citep{Kusner17}. To address the risk of biased model outputs resulting from biased training data, de-biased synthetic data is frequently used in model training \citep{Xu18, Breugel21}. Moreover, synthetic data is widely used for dataset expansion, or data augmentation, particularly in scenarios where real-world data is limited. It can act as a regularizer and improve model robustness against outliers \citep{Wong16, Fawaz18}. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are widely utilized to generate synthetic data by approximating the distribution of original data \citep{Goodfellow14, Xu18, Kingma14, Breugel24}. Research on synthetic data generation typically prioritizes three key goals: privacy, diversity, and fidelity. Privacy ensures that synthetic data does not directly replicate specific samples from the original dataset which is essential when dealing with sensitive or private information. While diversity addresses how well the synthetic data captures the variability in the original data distribution, fidelity refers to the extent to which synthetic data accurately reflects the characteristics of real data. 

Several studies employ generative models for creating counterfactuals or exploring underrepresented regions of the data. \cite{Joshi19} propose a framework that leverages generative models to produce task-specific synthetic data, which is particularly valuable for understanding model behavior and improving explainability. \cite{Redelmeier24} introduce an approach using autoregressive generative models with decision-tree-based conditionals to model the joint distribution of features and target decisions. By adopting Monte Carlo sampling, their method generates counterfactuals from this distribution. Such methods support a wide range of applications, including bias exploration, targeted data augmentation, and decision boundary analysis, thereby advancing explainability and robustness in machine learning systems.

Energy-based models (EBMs) have recently gained attention as a promising framework for combining generative and discriminative modeling tasks. EBMs reinterpret standard classifiers by treating their logits as an energy function, enabling the modeling of joint distributions over data and labels \citep{LeCun06, Duvenaud20}. Recent works demonstrate their effectiveness in adversarial robustness, out-of-distribution detection, and data augmentation \citep{Zhao17, Liu20, Duvenaud20, Arbel21, Margeloiu24}. By defining a probability distribution through the energy function, EBMs can generate synthetic data that captures the underlying properties of the original dataset. \cite{Duvenaud20} introduce a joint energy-based model demonstrating how classifier logits can be utilized to construct a joint probability model of data points and labels. This approach demonstrates significant improvements in out-of-distribution detection and adversarial robustness compared to standard image classifiers. \cite{Ma24} extend the application of energy-based models to tabular data by leveraging a pre-trained discriminative transformer framework. Their approach defines a class-conditional energy function and generates synthetic tabular data using stochastic gradient Langevin dynamics sampling. Experiments demonstrate its effectiveness in tasks such as data augmentation, class balancing, and imputation.

Our $G$ function below can also be considered an energy function which and the unconstrained solution to \eqref{eq:BLPforG} below gives the Gibbs distribution. But rather than learning the energy function to capture the (class conditioned) data distribution, we design the $G$ function using trained models so that the distribution generates samples answering the question posed by $G$ to the trained model. Works such as \cite{Duvenaud20} and \cite{Ma24} are similar in their approach of using a trained classifier to get an energy function and the use of Langevin dynamics to sample from the Gibbs distribution, but their goal remains the mimicking of true data distribution, and in fact the first paper melds the training of energy function and the classifier. We instead present designing various $G$'s as a general framework for probing trained models.

\paragraph{Contributions.}

\end{comment}


%%% ILKER: Backup for Section 2 and Section 3
\section{The Mathematical Framework}
\red{Notasyonu şu şekilde değiştirmeyi öneriyorum...}

\red{Before we introduce the proposed framework, let us introduce our notation. The labeled data lie in $\mathcal{X}\times \mathcal{Y}$, and the model defines a predictor function $f(\vec{\theta}, \cdot): \mathcal{X} \to \mathcal{Y}'$ for any given set of model parameters $\vec{\theta} \in \Theta$. Then, we obtain for a given sample $\vec{x} \in \mathcal{X}$, the predicted label $y_{\vec\theta}(\vec{x}) \in \mathcal{Y}$ by passing the predictor function through a transformation depending on regression or classification task. The cost function $\ell_F : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_{\geq 0}$ measures how far predicted labels are from the true labels.}

\red{Eğer bu şekilde yazmaya ikna olursanız, ilerleyen kısımda $f(\vec\theta, \vec x)$ yerine $y_{\vec\theta}(\vec{x})$ kullanabiliriz. Kısaca, label gerekince (fixed-label, model contrasting, parameter-sensitive) $y_{\vec\theta}(\vec{x})$, probability gerekince (prediction-risky) $f(\vec\theta, \vec x)$ kullanacağız.}

Before we introduce the proposed framework, let us introduce our notation. The labeled data lie in $\mathcal{X}\times \mathcal{Y}$, and the model defines a predictor function $f(\vec{\theta}, \cdot): \mathcal{X} \to \mathcal{Y}$ for any given set of model parameters $\vec{\theta} \in \Theta$. The cost function $\ell_F : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_{\geq 0}$ measures how far predicted labels are from the true labels. %Although it is possible to work with data and parameter spaces that are general differentiable manifolds, such generality is not usually needed in practice, and writing in full generality would only clutter the notation without adding much practical benefit. Therefore we take the data and parameter space to be open subsets of $\R^d$.

We propose a framework for probing a model, with answers in the form of generated data. Our method is structured to be symmetric to the training process itself: instead of generating parameters given (an empirical) data distribution, we generate data given trained model parameters. One can gain valuable insights into the model's behavior by analyzing the generated population statistically.

\begin{figure}
\centering
\fbox{\includegraphics[width = 0.45\textwidth]{figures/EvDSteps.png}}
\caption{The three steps to obtain the favorable data of the trained model.}
\label{fig:maindiagramalt}
\end{figure}

The mathematical symmetry reveals itself in the distributional/variational learning formulation for training as illustrated in Figure \ref{fig:maindiagramalt}. We start with a loss function $F$ on the model parameters through which we get a distribution $q^*(\vec{\theta})$ on $\Theta$. Note that a practitioner may have a single $\vec{\theta}^*$ that can be seen as the Dirac delta measure $\delta_{\vec{\theta}^*}$ supported on a singleton. Using $q^*$ next, we form a loss function $G$ on the data space, similar to how $F$ was formed. Note that the parameter loss $F$ is itself an average of the loss contributions over the training data, \textit{i.e.}, the function $F$ is derived from a distribution on the data (approximated as the empirical distribution of the training set) by integrating out the data variable in a loss pairing 
%$\omega_F(\vec{\theta}, (\vec{x}, y))$ usually
given in the form $\ell_F(f(\vec{\theta}, \vec{x}), y)$. In a totally analogous fashion, we can define a (possibly different) pairing $\omega_G$ and integrate out the parameter variable with the trained distribution $q^*$ to get $G$. Following the same logic applied to parameters, the variational learning objective associated with this function $G$ optimizes a distribution $p^*$ over the data space. We consider samples from this distribution $p^*$ as answers to the questions put forth by $G$. Next, we expand on this overview.

Variational learning --instead of finding a single set of model parameters that minimizes a loss function-- seeks a distribution that balances the tasks of minimizing the expected loss and maximizing the entropy:
\begin{equation}\label{eq:BLPforF}
    \argmin_{q \in \mathcal{Q}} \E_{q} [F] - \tau \mathcal{H}(q),
\end{equation}
where $\mathcal{Q}$ is a choice of candidate distributions on $\Theta$ and $\mathcal{H}(q) = -\int_{\Theta} q \log q$ is the entropy with respect to a basis measure. The problem can be interpreted as an implementation of the exploration-exploitation trade-off in the parameter space. The constant $\tau > 0$, called the temperature, controls the balance between these two objectives. Absent any restriction, \textit{i.e.}, if $\mathcal{Q}$ is the set of all probability distributions\footnote{All probability distributions which are absolutely continuous with respect to a given base measure, which in this case taken to be the Lebesgue measure $\dd\vec{\theta}$.}, then the Gibbs-Boltzmann distribution $q^*(\vec{\theta}) \propto e^{-\frac{1}{\tau} F(\vec{\theta})}$ is the unique solution to \eqref{eq:BLPforF}.

Similarly given a loss function $G$ defined on the data space, we can obtain a distribution by solving:
\begin{equation}\label{eq:BLPforG}
    \argmin_{p \in \mathcal{P}} \E_p[G] - \tau \mathcal{H}(p).
\end{equation}
Here, $\mathcal{P}$ represents the family of candidate distributions, which can be chosen in several ways. One option is to explicitly select $\mathcal{P}$ as a family of distributions depending on the nature of the data distribution. Alternatively, we can retain the full space of probability distributions but replace $G$ with $\tilde{G} = G\circ \varphi$ for some function $\varphi: \mathcal{Z} \to \mathcal{X}\times \mathcal{Y}$. In the latter case, $\mathcal{Z}$ can be the latent space and $\varphi$ can be the decoder function of a trained variational autoencoder. This procedure produces a distribution on $\mathcal{Z}$, which we can sample from and map to $\mathcal{X}$ via $\varphi$ -- thus, sampling data from the pushforward distribution. As a more mundane example, we may choose $\mathcal{Z} = \mathcal{X}$ with $\varphi(\vec{x}) = (\vec{x}, y')$ for a fixed label $y'$. If certain features of the data are considered immutable, then $\mathcal{Z}$ can be a certain subspace of $\mathcal{X}$ and $\varphi$ can be taken to map the missing features to predetermined fixed values. Alternatively, the label coordinate of $\varphi$ may also depend on $\vec{x}$ using a classifier. We provide such examples in Section \ref{sec:probing} and state precisely the questions being posed to the model. As a last example of $\varphi$, if the data has been standardized to $[0,1]$, taking it to be the sigmoid function ensures that the answers to our questions come as data points with features in the admissible range.


In general, a function of two variables $\omega : \mathscr{A} \times \mathscr{B} \to \R$ gives us a way to pass from a distribution on $\mathscr{A}$ to a function on $\mathscr{B}$ by integration: If $\mu$ is a measure on $\mathscr{A}$, then the integral $\int_{\mathscr{A}} \omega(a,b) \dd\mu(a)$ --when it converges-- is a function on $\mathscr{B}$.
The standard construction of the parameter loss function can be seen as a special case of this: Given data $\mathscr{D} = \{(\mathbf{x}_i, y_i)\}_{i = 1}^N \subseteq \mathcal{X} \times \mathcal{Y}$ and a model $f: \mathscr{X} \times \Theta\to \mathscr{Y}$ we construct the loss function
\[
    F(\vec{\theta}) = \frac{1}{N}\sum_{i = 1}^N \ell_F(f(\mathbf{x}_i, \vec{\theta}), y_i) + R(\vec{\theta}).
\]
This is an integral of $\ell_F(f(\vec{x}, \vec\theta), y) + R(\vec{\theta})$ on the space $\mathcal{X} \times \mathcal{Y}$ with respect to the empirical measure $ \frac{1}{N} \sum_{i = 1}^N \delta_{(\vec{x}_i, y_i)}$. Adding ---as we did--- a function to $\ell_F$ that depends only on $\vec{\theta}$ leads to a regularizer term in $F$. 

As a final note, we point out that the standard stochastic gradient descent training is not a significant departure from this setup, and in fact, can be seen as a specialization. Given a very restrictive family $\mathcal{Q}$, such as the manifold of Dirac delta distributions\footnote{To eschew technicalities of infinities, instead of exactly using Dirac delta we may instead consider distributions which are supported everywhere, and highly concentrated around a point but with fixed variance} supported on a single $\vec\theta$, the entropy term becomes irrelevant, and we get the classical optimization problem of minimizing the loss function $F$. 

% \begin{figure}
% \centering
% \includegraphics[width = 0.45\textwidth]{figures/diagram.png}
% \caption{Steps to obtain the favorite data of the trained model.}
% \label{fig:maindiagram}
% \end{figure}

\begin{figure}
\centering
\fbox{\includegraphics[width = 0.45\textwidth]{figures/EvDStepsLinear.png}}
\caption{The three steps for linear regression.}
\label{fig:maindiagramaltlinear}
\end{figure}

\begin{figure}
\centering
\fbox{\includegraphics[width = 0.45\textwidth]{figures/overview.pdf}}
\end{figure}


\section{Probing Your Model} \label{sec:probing}


A general class of loss functions for data can be given as follows given a model $f(\vec{x}, \theta)$ and a predictor $\widehat{y}$ we write
\begin{equation}\label{eq:genericG}
    G(\vec{x}) = \int_{\Theta} \ell_G(f(\vec{x}, \vec{\theta}), \widehat{y}(\vec{x})) q^*(\vec{\theta})\dd\vec{\theta} + R(\vec{x})
\end{equation}
where $R(\vec{x})$ is a regularizer function that can be chosen to put additional soft constraints on the samples (on top of the hard constraints coming from the restriction $p \in \mathcal{P}$). To illustrate this general structure, we next give several cases.

\paragraph{Fixed-label samples.} We probe the model for what it thinks are good data samples from the distribution $\mathcal{P}$ that fit the bill for $\hat{y}(\vec{x}) \equiv y'$. This can be applied to both single set of model parameters, $\theta^*$ or a general ensemble of models where the parameters are coming from a distribution $q^*$.

%\begin{example}
In case of a single set of problem parameters, \textit{i.e.}, $\theta^*$, we obtain
\begin{equation}
\label{eq:case1a}
G(\vec{x}) = \ell_G(f(\vec{x}, \vec{\theta}^*), y') + R(\vec{x}).
\end{equation}
When parameters are coming from a distribution, \textit{i.e.}, $q^*$, the loss function becomes
\begin{equation}
\label{eq:case1b}
       G(\vec{x}) = \int_{\Theta} \ell_G(f(\vec{x}, \theta), y') q^*(\theta)\dd\theta + R(\vec{x}).
\end{equation}
Figure \ref{fig:maindiagramaltlinear} demonstrates the steps when $f(\vec{x}, \theta)$ corresponds to linear regression, and both $\ell_F$ and $\ell_G$ give the squared errors. In this special case, we obtain analytical solutions at each of the three-step framework. The details of this observation are given in Appendix \ref{app:LR}.
%\end{example}

\paragraph{Prediction-risky samples.} Consider a binary classification model where the model returns the probabilities of belonging to a class and a threshold of 0.5 determines the class. If set the loss function as
\begin{equation}
\label{eq:case2}
       G(\vec{x}) = \int_{\Theta} \|f(\vec{x}, \theta) - 0.5\|^p q^*(\theta)\dd\theta + R(\vec{x}) \text{ for } p \geq 1,
\end{equation}
then we probe the model by asking for data samples that would be around the decision boundary. These can be considered as the risky samples. Within the general function \eqref{eq:genericG}, $\ell_G$ and $\widehat{y}(\vec{x})$ are simply set to the norm-$p$ loss and $0.5$, respectively.

\paragraph{Parameter-sensitive samples.} Given a set of parameters $\theta^*$ and its associated predictor $y_{\theta^*}$, and a distribution $q^*$ for which $\theta^*$ is the mode of, we ask the model for data samples that are classified as one value, but would be classified as another if the model parameters were to (perhaps slightly) be perturbed. That is
\begin{equation}
\label{eq:case3}
       G(\vec{x}) = \int_{\Theta} \ell_G(f(\vec{x}, \theta), 1-y_{\theta^*}(\vec{x})) q^*(\theta)\dd\theta + R(\vec{x}),
\end{equation}
where $\widehat{y}(\vec{x}) \equiv 1-y_{\theta^*}(\vec{x})$ denotes the flipped classification decision constrasting the model's decision with fixed set of parameters, $\theta^*$.

When $\mathcal{Q}$ is a restricted family of distributions such as Gaussians with fixed variance, sampling from $q^*$ corresponds to sampling from the vicinity of $\theta^*$.

\red{Risky sample'dan nasil farkli oldugunu buraya not dusmek gerekir. Sonrasinda da numerical resultlara refer edelim.}

\paragraph{Model-contrasting samples.}

The predictor $\hat{y}(\vec{x})$ does not need to come from the current model with parameters $\vec{\theta}$, but be a reference model to which we are contrasting our model. With the function
\begin{equation}
\label{eq:case4}
    G(\vec{x}) = \ell_{G}(f(\vec{x}, \theta^*), \hat{y}(\vec{x})) + R(\vec{x})
\end{equation}
we are asking where our (e.g. linear regression) model differs from another (e.g. neural network) model.

\paragraph{Localized samples.} In all of the above cases, we can add a regularizer term $R(\vec{x}) = \|\vec{x} - \vec{x}_s\|^p$ for $p\geq 1$ to generate synthetic data that is similar locally to a given $\vec{x}_s$, in fact, different weightings can too be applied to different columns to enforce this more or less stringently for different features. 

\paragraph{Feature-restricted samples.} By restricting $\mathcal{P}$ to be supported on data with certain features fixed, such as those features corresponding to age, race, and so on, we can ask the model for all of the above questions but conditioning on certain immutable characteristics. This falls into the class of optimizations, where instead of $G$ we consider 
\[
    \widetilde{G}(\vz) = G(\varphi(\vz)) 
\]
on some other (latent) space $\vz\in Z$. For example for image data in order to have our samples conform to the data manifold $\varphi$ can be taken as the trained decoder module from a VAE. Then, starting with measures $\widetilde{p} \in \mathcal{P}(Z)$ on the latent space, their pushforwards $\varphi_* \widetilde{p}$ lie on the data manifold, i.e. sampling $\vz \sim \widetilde{p}$ and computing $\varphi(\vz)$ gives a data sample. 

\begin{comment}
\begin{enumerate}
    \item Given a constant class $y'$, the constant predictor $\hat{y}(\vec{x}) \equiv y'$: We probe the model for what it thinks are good data samples from the distribution class $\mathcal{P}$ that fit the bill for $y'$. This can be applied to both single set of model parameters or a general ensemble of models where the parameters are coming from a distribution $q^*$ (as opposed to a Dirac delta distribution).
    \item Given a set of parameters $\theta^*$ and its associated predictor $\widehat{y_{\theta^*}}$, and a distribution $q^*$ for which $\theta^*$ is the mode of, choosing $\widehat{y}(\vec{x}) = 1 - \widehat{y_{\theta^*}}(\vec{x})$ asks the model for data samples that are classified as one value, but would be classified as another if the model parameters were to (perhaps slightly) be perturbed.
    \item For the binary classification problem, choosing $y' = 0.5$ and the cost function $c(y, y') = |y - y'|^p$ (for $p \geq 1$), means asking the model for data samples that would be around the decision boundary for the model. 
    \item In all of these we can add a $R(\vec{x}) = \|\vec{x} - \vec{x}_0\|_{\ell_p}^p$ 
    regularizer term to generate syntethic data that is similar to a given $\vec{x}_0$, in fact different weightings can too be applied to different columns to enforce this more or less stringently for different features. 
    \item By restricting $\mathcal{P}$ to be supported on data with certain features fixed (such as those features corresponding to age, race etc.) we can ask the model for all of the above questions but conditioning on certain immutable characteristics.
    \item By restricting $\mathcal{P}$ to be supported on data with certain features fixed (such as those features corresponding to age, race etc.) we can ask the model for all of the above questions but conditioning on certain immutable characteristics.    
\end{enumerate} 
\end{comment}



we formulate the probing function given in (\ref{eq:case4}) as follows:
\begin{equation*}
    G(\vec{x}) = e^{-\|f_{SVR}(\vec{x}, \vec{\theta_{SVR}^*}) - f_{LR}(\vec{x}, \vec{\theta_{LR}^*})\|^2}
\end{equation*}
where $f_{SVR}(\vec{x}, \vec{\theta_{SVR}^*})$ and $f_{LR}(\vec{x}, \vec{\theta_{LR}^*})$ represent the predictions from the SVR and linear regression models, respectively.
