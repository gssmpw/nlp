\section{Background}\label{sec:background}

\subsection{Self-Attention}

Consider an input matrix $\mathbf{Z} \in \mathbb{R}^{T \times D}$, where $T$ represents the sequence length and $D$ is the feature dimension. The multi-head self-attention mechanism facilitates learning from different representational subspaces by executing multiple attention computations in parallel. Query ($\mathbf{Q}$), Key ($\mathbf{K}$), and Value ($\mathbf{V}$) are derived from linear projections:
$\mathbf{Q} = \mathbf{Z} \mathbf{M}_Q$, $\mathbf{K} = \mathbf{Z} \mathbf{M}_K$, and $\mathbf{V} = \mathbf{Z} \mathbf{M}_V$, where $\mathbf{M}_Q, \mathbf{M}_K, \mathbf{M}_V \in \mathbb{R}^{D \times D_h}$ are trainable weight matrices. The attention weights are computed via scaled dot-product attention, as given in Eq. \ref{eq:attn_scaled}: 
\begin{equation}\label{eq:attn_scaled}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{D_h}}\right) \mathbf{V}.
\end{equation}
This process is repeated over $H$ heads, each utilizing distinct weight matrices $\mathbf{M}_Q^{(h)}, \mathbf{M}_K^{(h)}, \mathbf{M}_V^{(h)}$. The concatenated outputs from all heads are projected back to the original dimension $D$ using a learned weight matrix $\mathbf{M}_O \in \mathbb{R}^{H D_h \times D}$:
\begin{equation}
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) \mathbf{M}_O,
\end{equation}
where each attention head is defined as follows:
\begin{equation}
\text{head}_h = \text{Attention}(\mathbf{Q}^{(h)}, \mathbf{K}^{(h)}, \mathbf{V}^{(h)}).
\end{equation}

\subsection{Key-Value (KV) Cache} 
During autoregressive LLM inference, the tokens are generated sequentially. Without caching, Key ($\mathbf{K}$) and Value ($\mathbf{V}$) matrices are recomputed at each generation step for all preceding tokens. KV caching mitigates this inefficiency by storing computed $\mathbf{K}$ and $\mathbf{V}$ projections. Rather than recomputing these values, the model retrieves and appends the cached matrices to the current tokenâ€™s projections. The updated attention computation follows Eq. \ref{eq:kv_cache_attn}: 
\begin{equation}\label{eq:kv_cache_attn}
\text{Attention}(\mathbf{Q}t, [\mathbf{K}{1:t-1}; \mathbf{K}t], [\mathbf{V}{1:t-1}; \mathbf{V}t]),
\end{equation}
where $[;]$ denotes concatenation along the sequence axis, and cached values ${\mathbf{K}{1:t-1}, \mathbf{V}_{1:t-1}}$ are loaded from memory. Although KV caching reduces redundant computation, storing cached projections for each token demands substantial memory, growing linearly with sequence length. For a transformer with $L$ layers, $H$ heads, and sequence length $T$, memory consumption scales as $2 \times T \times L \times H \times 16$-bit.

\subsection{KV-Cache Eviction} 
KV-Cache eviction aims to eliminate less significant tokens from $\mathbf{K}{1:t-1}$ and $\mathbf{V}{1:t-1}$ using a function $f_{evict}$ that identifies and removes redundant elements. The eviction mechanism is depicted in Eqs. \ref{eq:kv_evict_k} and \ref{eq:kv_evict_v}, where the $m^{th}$ token is removed from the cache.  
\begin{multline}\label{eq:kv_evict_k}
f_{evict}(\mathbf{K}{1:t-1}) = \mathbf{K'}{1:t-1} \\ = [k_1, \dots, k_{m-1}, k_{m+1}, \dots, k_{t-1}]
\end{multline}
\begin{multline}\label{eq:kv_evict_v}
f_{evict}(\mathbf{V}{1:t-1}) = \mathbf{V'}{1:t-1} \\ = [v_1, \dots, v_{m-1}, v_{m+1}, \dots, v_{t-1}]. 
\end{multline}
After eviction, attention is computed using the reduced cache, as shown in Eq.~\ref{eq:kv_attn_evict}: 
\begin{equation}\label{eq:kv_attn_evict}
\text{softmax}\left(\frac{\mathbf{Q}t [\mathbf{K'}{1:t-1}; \mathbf{K}t]^\top}{\sqrt{D_h}}\right) [\mathbf{V'}{1:t-1}; \mathbf{V}_t].
\end{equation} 
