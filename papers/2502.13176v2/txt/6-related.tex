\section{Related Works}\label{sec:related}
In this section, we discuss previous work relevant to BaKlaVa in five main areas: KV-cache eviction policy, profiling for determining memory budget, KV-cache quantization, cache merge, and system-level optimizations. 

\subsection{KV Cache Eviction Policy}
StreamingLLM~\cite{streamingllm} discovered the 'attention sink' effect, where early sequence tokens play a crucial role in maintaining model performance through asymmetric attention weight accumulation. H2O~\cite{h2o} introduces an eviction strategy based on cumulative attention, retaining 'heavy-hitter' key-value pairs while allowing token positions to vary. Similarly, Scissorhands~\cite{scissorhands} develops an approach that evicts based on a 'pivotal' metric, adjusting eviction rates across layers using a persistence ratio. Keyformer~\cite{keyformer} addresses the issue of token removal distorting softmax probability distributions by implementing regularization techniques to mitigate these perturbations. 

\subsection{Profiling for Determining Memory Budget}
Squeezeattention~\cite{squeezeattention} employs a dynamic approach, measuring layer importance through cosine similarity of input prompt differences pre- and post-self-attention, subsequently categorizing layers and adjusting their KV budgets. PyramidInfer~\cite{pyramidinfer} introduces a pyramid-shaped allocation strategy, prioritizing tokens with high attention values and maintaining a set of significant tokens through attention-driven updates during decoding. In comparison, Ada-KV~\cite{adakv} offers an adaptive budget allocation method that improves utilization across individual attention heads, resulting in more effective cache eviction strategies.



\subsection{KV-Cache Quantization}
GEAR~\cite{gear} takes a different approach by compressing less important entries to ultra-low precision, using a low-rank matrix for residual error approximation, and utilizing a sparse matrix for outlier correction. MiKV~\cite{notokenleftbehind} introduces a mixed-precision KV-cache quantization method, allocating precision based on token importance. QAQ~\cite{qaq} proposes a dynamic, quality-adaptive quantization approach that determines bit allocation based on token importance and sensitivity. KVQuant~\cite{hooper2024kvquant} offers strategies for smooth quantization of keys and values, including pre-RoPE quantization for keys, per-token quantization for values, and isolation of outliers in a sparse format. These diverse techniques collectively contribute to significant improvements in model compression and efficiency while maintaining performance.

\subsection{Cache Merge}
MiniCache~\cite{minicache} leverages the high angular similarity observed in middle-to-deep layer KV caches, merging key and value pairs from adjacent similar layers into shared representations. KVSharer~\cite{yang2024kvsharer}, on the other hand, exploits the counterintuitive finding that sharing KV caches between significantly different layers during inference does not substantially impact performance, prioritizing dissimilar layers for sharing based on Euclidean distance calculations. In comparison,  CaM~\cite{cam} focuses on merging keys or values of multiple evicted tokens with retained tokens using attention scores, while KVMerger~\cite{wang2024model} employs a two-step process: first clustering consecutive tokens with high cosine similarity, then merging tokens within each set into a pivotal token chosen by the attention score, using Gaussian kernel weights to emphasize contextual relevance. 

\subsection{System-Level Optimizations}
FlexGen~\cite{flexgen} proposes an SSD-based method for managing key-value (KV) caches, effectively expanding the memory hierarchy across GPU, CPU, and disk storage. This approach utilizes linear programming to optimize tensor storage and access patterns, enabling high-throughput LLM inference on hardware with limited resources. Complementing this, ALISA~\cite{alisa} introduces a dual-level KV cache scheduling framework that combines algorithmic sparsity with system-level optimization. At the algorithmic level, ALISA employs a Sparse Window Attention mechanism to identify and prioritize crucial tokens for attention computation, while at the system level, it implements a three-phase token-level dynamic scheduler to manage KV tensor allocation and balance caching and recomputation.

