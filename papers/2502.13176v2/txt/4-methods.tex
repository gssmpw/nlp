\section{The BaKlaVa Method}\label{sec:methods}
Our method for optimizing KV-cache memory allocation consists of 3 main steps for a given LLM: (i) A one-time collection of profiling data for a given prompt(s) (Algorithm~\ref{alg:profiling} -- step 1);  (ii) Using a heuristic to estimate the `importance' of KV caches,  which is also a one-time calculation  (Algorithm~\ref{alg:profiling} -- step 2); and (iii) Performing a parameter search to allocate memory accordingly (Algorithms~\ref{alg:kv-mem-allocation} and~\ref{alg:parameter_search}). 

All three steps in BaKlaVa only need to be run once. The most time-consuming part currently is Step (iii), where a parameter search is performed for the target compression level. For this parameter search, we quickly evaluate each parameter combination using `perplexity', rather than running a long-context evaluation benchmark, since perplexity does not require autoregressive token generation and consequently is much faster and gives a good approximation of actual performance. This parameter search, for the models we evaluated (which contain 7 to 8 billion parameters), takes 10 to 20 minutes on 8x A100 GPUs for around 200 combinations of parameters on 98k tokens for a chosen compression ratio. The number of tokens can be decreased for a proportional decrease in runtime, though they should be at least as much as the maximum context length being evaluated.

Once the ideal parameters for an LLM are obtained, no additional computation is required.  To begin inference, we initialize our custom huggingface transformers' KV-cache object to use in inference.   




\begin{algorithm}
\caption{One-Time Profiling for KV-Cache Importance}
\begin{algorithmic}[1]
\REQUIRE LLM model $\mathcal{M}$, one or more prompts $\mathcal{P}$ of varying lengths
\ENSURE Values indicating relative KV-cache importance

\STATE \textbf{Step 1:} Collect profiling data from prompts $\mathcal{P} = \{p_1, p_2, \dots, p_n\}$ of different lengths
\FOR{each prompt $p_i \in \mathcal{P}$}
    \STATE Run inference on $\mathcal{M}$ with $p_i$
    \FOR{each layer $l \in \mathcal{L}$}
        \FOR{each attention head $h$ in layer $l$}
            \STATE Compute token-wise cosine similarity between attention head input $V$ and output $SoftMax(QK^T) V$
            \STATE Compute the average across all cosine similarities to obtain a single similarity value $s_{il} \in \mathcal{S}$
        \ENDFOR
    \ENDFOR
\ENDFOR

\STATE \textbf{Step 2:} Convert attention head similarities to KV-cache importance using the number of attention heads per KV-cache group $g$
\FOR{each layer $l \in \mathcal{L}$}
    \FOR{each group of $g$ heads, denoted by $s_{il}, s_{(i+1)l}, \dots, s_{(i+g-1)l} \in \mathcal{S}$}
        \STATE Obtain similarity for the current KV cache, $KVsim \gets mean(s_{il}, \dots, s_{(i+g-1)l})$ 
        \STATE KV cache importance $I_{li} \gets 1-KVsim$
    \ENDFOR
\ENDFOR

\end{algorithmic}
\label{alg:profiling}
\end{algorithm}



\begin{figure}[h]
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.8\linewidth]{figs/similarity_heuristic.pdf}   
    \vspace{-0.25in}  
    \caption{The attention-head similarity heuristic used in BaKlaVa. By taking the cosine similarity between the input and output, we can calculate how much change there is. The more change between the input and output of the attention head, the more important we assume it is.}
    %\vspace{-0.3in}
    \label{fig:dot_product_cos} 
\end{figure}

\begin{figure}[h]
    \centering
    %\includegraphics[width=1\linewidth]{figs/head_importances.pdf}   

    \includegraphics[width=1\linewidth]{figs/similarities/llama_xsum_similarity.png}
    %\vspace{-20pt}
    \includegraphics[width=1\linewidth]{figs/similarities/llama_lcc_similarity.png}
    %\vspace{-5pt}
    %\rule{\linewidth}{0.5pt} % Adds a thin horizontal line
    \hdashrule{\linewidth}{0.5pt}{1mm 1mm} % Dotted horizontal line
    %\vspace{-7pt}
    \includegraphics[width=1\linewidth]{figs/similarities/qwen_xsum_similarity.png}
    \includegraphics[width=1\linewidth]{figs/similarities/qwen_lcc_similarity.png}
    \caption{Cosine similarity heatmap for input and output of attention heads for two different prompts in LLaMA3-8B and Qwen2.5-7B. We chose three representative layers to illustrate that attention head consistency holds across different prompts. The X-axis shows the attention heads in a layer, Y-axis represents each token position in the prompt. Green and red outlines show the highest and lowest column similarity means per layer, that is, the most and least important attention heads respectively.  
    The order of average attention head similarities (the mean of each column, see Algorithm~\ref{alg:profiling}) stays highly consistent even across different prompts of different lengths, indicating that profiling an LLM one time is sufficient to make KV-cache importance estimations.}
    %\vspace{-0.3in}
    \label{fig:prompt_test} 
\end{figure}


\subsection{Determining KV-cache Importances}
\label{sec:determine_kv_importance}
\subsubsection{Head Importance Heuristic} 
To determine the significance of an individual attention head, we used several key observations to come up with a heuristic. The first is that the more change there is between the input and output of a structure in an LLM, the more important it is, as used in~\cite{ge2024model} to determine the type of tokens the individual attention heads focus on and in~\cite{squeezeattention, pyramidinfer} to determine the importance of LLM layers. Second, the attention matrix $softmax(QK^T)$ has been shown to be a high-quality heuristic that can determine individual token importances~\cite{h2o, scissorhands, keyformer} for KV-cache eviction. Lastly, the $V$ tensor contains key information about the tokens, which is not found in the attention score matrix~\cite{guo2024attention, devoto2024simple}. \textbf{Based on these observations, we propose the idea that the greater the change between the input} $V$ \textbf{and the output} $softmax(QK^T)V$ \textbf{ of this attention head, the more critical this attention head is for inference.}

As shown in Algorithm~\ref{alg:profiling}, to do this, we compare the input $V$ tokens (each token is a multidimensional vector) with the output of the attention head, $softmax(QK^T)V$, using cosine similarity, as shown in Figure~\ref{fig:dot_product_cos}. 
  
Each input and output token are individually compared using a cosine similarity value to determine the change in vector direction. To obtain a single value for each attention head, we first (i) get the mean of all tokens' cosine similarities within that head. The result of cosine similarity ranges from -1 to 1 and the maximum value is obtained when the two compared token vectors are identical (i.e., the attention score matrix is an identity matrix). We then (ii) normalize these values from range 0 to 1 to obtain a single similarity value, such that a value of 1 means identical input and outputs for all tokens in the attention head. Lastly, (iii) to obtain the `importance value', we take the complement of each mean similarity ($1 - similarity$). An importance value closer to 1 means a bigger difference between the input and output tokens, thus it has more importance and vice versa. 

We tested multiple token comparison methods, such as dot-product and KL divergence, and found that cosine similarity and dot-product both give similar results.  However, cosine similarity guarantees an output between -1 and 1, leading to simpler calculations. Therefore, we chose cosine-similarity for BaKlaVa. Note that cosine similarity measures the change in angle, but not magnitude, and thus another method that incorporates {\em both} magnitude and direction change may give better results. We left this for future work. 

\subsubsection{One-Time Profiling}
To determine the frequency of profiling needed, considering that attention head behavior can vary between inference steps and different prompts, 
we ran several experiments. We found that while the importance of individual tokens may change throughout inference, overall the importance value (calculated by taking the average of all token cosine similarities, see Algorithm~\ref{alg:profiling}, step 1) remains consistent, across different prompts. This is illustrated in Figure~\ref{fig:prompt_test}, where for two models, LlaMA3-8B and Qwen2.5-7B-Instruct, we profile two prompts of lengths around 350 and 2000 tokens from a text (XSum) and coding (LCC) dataset respectively, for three layers in both LLMs (not just to highlight these specific layers, but to illustrate that the consistency the attention head behavior holds across different layers). Each tile (in the heatmaps) represents the cosine similarity difference for a single token with the top 5 and bottom 5 importance heads outlined in green and red,  respectively. We can observe that the highest- and lowest-ranking attention heads stay highly consistent across prompts of different lengths and different types (i.e. text vs code). This suggests that, for a given LLM, a single profiling run with a sufficiently large prompt (i.e. few hundred tokens) is sufficient to determine the attention head importance values that can be applied for all future inferences.

\subsubsection{Grouped Query Attention} 
If the LLM employs Grouped Query Attention (GQA), an additional step is required before determining the KV cache memory budgets. Since our measurements assess changes at the level of individual attention heads rather than KV-caches, a direct assignment is not possible. In GQA, multiple attention heads \textit{share} the same KV-cache, meaning that memory budgets cannot be allocated separately for each head within the same group. To address this, we compute the mean of similarities across all attention heads within a group, obtaining a single similarity value per GQA group. This process is detailed in Step 2 of Algorithm~\ref{alg:profiling}



\subsubsection{Layer Importance Heuristic}
\label{sec:layer_heuristic}
The KV-cache importances we have found so far are used to allocate the GPU memory budget {\em within} a single layer in an LLM. Simply taking the average of all KV-cache importances does not find the correct layer importance, since our KV-cache importance heuristic is agnostic to several other important structures in an LLM (e.g., the feed-forward networks, layer normalization, etc). Based on our empirical testing results (see Section~\ref{sec:empirical}), we found that SqueezeAttention~\cite{squeezeattention} is a simple and low-overhead heuristic that closely, though not perfectly, matches the `ground truth' layer importances and use this heuristic to determine the layer-wise importance values in BaKlaVa. The SqueezeAttention heuristic takes the cosine similarity between the input and output of each LLM layer, thus capturing the total effect of {\em all} structures within the layer. 



\subsection{Assigning Memory Budgets to KV-Caches}

\subsubsection{Memory Allocation}
\label{sec:methods-memory-alloc} 

\begin{algorithm}
\caption{KV-Cache Memory Reallocation Based on Attention Head Importance}
\begin{algorithmic}[1]
\REQUIRE Importance scores $\mathbf{I} = \{I_1, I_2, ..., I_m\}$ for $m$ KV-caches, threshold $t$, reduction amount $r$
\ENSURE Adjusted KV-cache allocations

\STATE $\mathcal{L} \gets \{i \mid I_i < t\}$ \COMMENT{Identify KV-caches with low importance}
%\STATE $\mathcal{H} \gets \{i \mid I_i \geq t\}$ \COMMENT{Identify high-importance KV-caches}

\IF{$|\mathcal{L}| > m - 1$} 
    \STATE RETURN UNCHANGED KV-cache allocations 
    \COMMENT{If all KV-caches are low importance then do not do anything}
\ENDIF

\FOR{EACH $i \in \mathcal{L}$}
    \STATE REDUCE KV-cache size of $i$ by $r$
\ENDFOR

\STATE $n \gets |\mathcal{L}|$ \COMMENT{Number of KV-caches reduced}
\STATE $k \gets \min(n, m - n)$ \COMMENT{Limit reallocation up to the top n available high-importance caches}
\STATE $\mathcal{H} \gets$ TOP-$k$ ELEMENTS OF $\mathcal{H}$ BASED ON $I_i$ 
\STATE $\Delta r \gets \frac{n \times r}{k}$ \COMMENT{Compute adjusted increase per cache}

\FOR{EACH $j \in \mathcal{H}'$}
    \STATE INCREASE KV-cache size of $j$ BY $\Delta r$
\ENDFOR

\STATE RETURN updated KV-cache allocations

\end{algorithmic}
\label{alg:kv-mem-allocation}
\end{algorithm}

Once the importance values for each KV-cache and layer are obtained, the next step is to determine how to allocate memory budgets. 

Based on our observation of token similarities as shown in Figure~\ref{fig:prompt_test}, we find that low-importance attention heads are more consistent with how they change each individual token in a prompt (that is, the dot-product between input and output of the attention head has low variance), while other attention heads can display significant changes across tokens (that is, high variance in the token cosine similarity). Thus, to reduce the chances for decreasing the memory of KV-caches belonging to potentially critical attention heads, we take a conservative approach and only target KV-caches with an importance score below a threshold $t$ by a predetermined amount $r$, as shown in Algorithm~\ref{alg:kv-mem-allocation}. The freed memory is then assigned to up to top $n$ KV caches of highest importance (where $n$ is the number of low-importance KV-caches selected), in order to prioritize increasing memory for the most important KV-caches. 


\subsubsection{Parameter Search}
\label{sec:parameter_search}


\begin{algorithm}
\caption{Parameter Search Using Perplexity}
\begin{algorithmic}[1]
\REQUIRE Evaluation prompt $\mathcal{P}$, model $M$, context length $L$, list of parameter configurations $\mathcal{C}$, compression ratio $cmp$
\ENSURE Optimal parameters $p^*$

\STATE $best\_params \gets \emptyset$
\STATE $min\_loss \gets \infty$

\FOR {$\text{params} \in \mathcal{C}$}
    
    \STATE CACHE = $\text{MAKE\_CACHE}(\text{params}, cmp)$
    \STATE $losses \gets []$
    
    \FOR{$\text{chunk} \in \text{STEP}(\mathcal{P}, L)$} \COMMENT{Get chunks of tokens from prompt}
        \STATE $loss \gets \text{PERPLEXITY}(M, \text{chunk}, CACHE)$
        \STATE $losses.\text{append}(loss)$
        \STATE CACHE $\gets \text{RESET\_CACHE}(CACHE)$
    \ENDFOR

    \STATE $avg\_loss \gets \frac{\sum \text{losses}}{\text{len}(\text{losses})}$

    \IF{$avg\_loss < min\_loss$}
        \STATE $min\_loss \gets avg\_loss$
        \STATE $best\_params \gets \text{params}$
    \ENDIF

\ENDFOR

\STATE RETURN $best\_params$

\end{algorithmic}
\label{alg:parameter_search}

\end{algorithm}

To determine the optimal values for $r$ and $t$, we performed a parameter search over different compression values, as shown in Algorithm~\ref{alg:parameter_search}. We found that the ideal parameter `area' varies across different compressions. These results were calculated using perplexity, as it is much faster to find compared to LongBench and it is a good indicator of actual performance. Based on these observations, we chose the best-performing parameter pair for a given compression value when evaluating on LongBench. 

\subsection{Empirical Evaluation of Heuristics}
\label{sec:empirical}


To evaluate the effectiveness of our layer and KV-cache importance heuristics in comparison to the `true' importance, we conducted computationally intensive experiments. These tests empirically assessed the impact of individual layers
%and KV-caches 
on model performance by measuring the variation in benchmark scores resulting from modifications to each component. The underlying principle is that the greater the performance degradation caused by a change (e.g., memory reduction) in a layer or KV-cache, the more critical that component is to the model’s overall functionality. 


%\subsubsection{Evaluating Layer Heuristic}

To evaluate the layer importance heuristic (see Section~\ref{sec:layer_heuristic})  we tested our LLM on the \textit{triviaqa} LongBench dataset after reducing the memory allocated to different groups of layers. We selected a single dataset to minimize the computational cost of our empirical evaluation. \textit{triviaqa} was specifically chosen because it exhibits the widest range of scores, making it more sensitive to performance variations and thus a better candidate for detecting changes in output.

As described in algorithm~\ref{alg:layer_empirical}, we systematically reduced the memory budgets of layers within a sliding window of size 5, running a separate benchmark for each window position. Rather than evaluating individual layers in isolation, we compressed groups of 5 adjacent layers at a time. If crucial layers were arbitrarily scattered, rather than forming coherent clusters, it would suggest an unintuitive and unlikely distribution of importance. Additionally, testing each layer in isolation (i.e., using a window size of 1) yielded erratic results, indicating that individual layer evaluations do not capture meaningful patterns of layer importance. By considering contiguous groups, we aim to better approximate the true structure of importance within the model.




 



\begin{algorithm}
\caption{Empirical Evaluation of Layer Heuristic}
\begin{algorithmic}[1]
\REQUIRE LLM $M$, window size $W$, benchmark $BENCH$
\ENSURE Scores for each layer $S$, compression ratio $cmp$

\STATE $S \gets [\ ]$ \COMMENT{Initialize empty list for scores}

\FOR {$L \in \{0, \dots, \text{last\_layer}(M)\}$}
    \STATE $L_{\text{min}} \gets \max(0, L - \lfloor W/2 \rfloor)$
    \STATE $L_{\text{max}} \gets \min(\text{last\_layer}(M), L + \lfloor W/2 \rfloor)$

    \STATE $\text{reduce\_kv\_cache}(M, L_{\text{min}}, L_{\text{max}}, cmp)$ \COMMENT{Reduce KV-cache sizes for layers in window}

    \STATE $\text{score} \gets BENCH(M)$ \COMMENT{Run LLM and obtain benchmark score}

    \STATE $S.\text{append}(\text{score})$
\ENDFOR

\STATE RETURN $S$

\end{algorithmic}
\label{alg:layer_empirical}
\end{algorithm}

