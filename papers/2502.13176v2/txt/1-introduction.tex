
\section{Introduction}\label{sec:intro}


Large Language Models (LLMs) have achieved great success in recent years and have been successfully used in several natural language processing tasks such as chatbots, search engines, summarization, and customer service. This success has led to the development of LLMs with exponentially increasing parameter counts and context lengths (how much previous text an LLM can remember), with the latest models having more than a trillion parameters with more than a million context lengths~\cite{geminiteam2024, ren2023trillionparameter}. Although larger models with longer context lengths have improved performance, they come at the cost of significantly higher GPU memory usage during inference, posing challenges for efficient deployment.

LLMs generate text in an autoregressive manner -- given an input of any length, the model generates only a single token (word). To generate multiple tokens, the previously generated token is {\em appended} to the input, and the process repeats. This method of inference leads to significant `redundant computations', leading to quadratic time complexity. To mitigate this inefficiency, LLMs employ {\bf Key-Value (KV) caches} to store previous calculations -- key and value tokens for each attention head\footnote{An attention head is a key component of LLMs~\cite{vaswani2023attentionneed}, which allows the capture of the relationships between words. LLMs can have up to thousands of attention heads, each with their own KV-cache.} -- and remove these unnecessary computations. Yet, this comes at the cost of substantial GPU memory to hold these tokens, limiting how many tokens can be stored and, thus, a limit to how much an LLM can remember. This is currently one of the major bottlenecks in LLM scaling for long-context inference. 

Recent works have tried to address this challenge mainly by reducing the amount of data that an LLM needs to cache. However, many of these compression-based policies allocate memory {\em uniformly} across all KV caches, which is suboptimal. Recent research~\cite{squeezeattention, pyramidinfer, adakv}, has begun to explore the benefits of assigning heterogeneous memory budgets to different KV-caches. The key challenge in this approach lies in determining the optimal allocation of KV-cache memory, that is, which KV-caches in an LLM are more or less critical than the others to model performance.

In our work, {\bf BaKlaVa}, we demonstrate that different attention heads in an LLM have varying levels of importance, and therefore KV-cache memory should be allocated accordingly-- more important heads receiving larger (space for) KV-caches and less important ones receiving smaller allocations. To achieve this, we introduce a one-time `profiling' method, which does \textit{not} require fine-tuning of the LLM. Using a simple heuristic, our method estimates the importance of each attention head and optimally distributes a given KV-cache budget to maximize inference performance. We run multiple benchmarks with different KV-cache eviction and compression policies and show that our method can increase the inference quality up to an order of magnitude, without using additional memory or computation, and allows for near-baseline (a cache with maximum context length) performance for lower compression ratios.

BakLaVa is {\em complementary} to most existing KV-cache management and optimization methods, such as FlashAttention~\cite{flashattention2} and vLLM~\cite{pagedattention}, as well as various KV-cache compression, eviction, and offloading policies. We emphasize that our proposed method is {\em not} a policy for managing KV-cache memory or optimizing KV-cache calculations; rather, it is a method for \emph{allocating} memory budgets among existing KV-caches in an LLM.

\textbf{Contributions:} The main contributions of this paper can be summarized as follows:
\begin{itemize} 
    \item We introduce a heuristic to determine the relative importance of attention heads and KV caches in LLMs.
    \item We empirically show that the results of this heuristic remain consistent across various prompts for a given LLM. 
    \item We empirically validate that our heuristic {\em near optimally} ranks how important each KV cache is.
    \item We evaluate our proposed methodology on LongBench~\cite{bai2023longbench} and compare it against other KV-cache memory allocation strategies.
    \item Finally, we implement our method in HuggingFace as a custom KV-cache object.
\end{itemize}

Our empirical evaluations demonstrate that KV-cache and layer importance can be effectively estimated using heuristics with a high degree of accuracy. However, even with accurately identified importance values, determining the optimal memory allocation remains non-trivial, as the ideal strategy varies across different compression ratios. To address this challenge, we introduce a straightforward yet effective memory allocation approach that enables rapid parameter search, allowing us to efficiently determine the optimal memory distribution for various model architectures and compression ratios.