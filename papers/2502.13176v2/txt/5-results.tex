\section{Results}\label{sec:results} 
In this section, we report the results of BaKlaVa for KV-cache compression on the models LLaMA-3-8B and Qwen2.5-7B. %and Mistral-7B-v0.1. 
Qwen weights are quantized to 8 bits due to hardware limitations. Section~\ref{sec:longbench_results} shows the results of LongBench on different KV-cache reduction methods for different compression ratios. Section~\ref{sec:empirical_results} reports the results of empirically evaluating how well the heuristics used in BaKlaVa reflect actual layer and KV-cache importances.

\subsection{LongBench}
\label{sec:longbench_results}


\begin{figure*}[hbt!]
    \centering
    
    \includegraphics[width=\textwidth]{figs/LlaMA3-8B_results_small_total.pdf}
    \includegraphics[width=\textwidth]{figs/qwen2.5-7B-instruct_results_small_total.pdf}
    %\includegraphics[width=\textwidth]{figs/Mistral-7B_results_small.pdf}

    \caption{Comparison of BaKlaVa (varying memory budgets for both layers and KV-caches), SqueezeAttention (varying memory budgets for layers), and StreamingLLM (uniform memory budget for all KV-caches) on different LongBench tasks under various compression settings. The LongBench datasets shown include few-shot learning(triviaqa), coding (repobench-p), multi-document question answering (2wikimqa), and summarization (gov\_report). For BaKlaVa and SqueezeAttention, we conducted a parameter search using perplexity as a benchmark to determine the optimal settings for each compression ratio (detailed in Section~\ref{sec:parameter_search}).}

    \label{fig:longbench_results_small}
\end{figure*}

We used the LongBench~\cite{bai2023longbench} evaluation suite to test how our proposed approach works with real-life scenarios with significant KV-cache memory usage. LongBench contains 14 English (\textit{qmsum, multifieldqa\_en, triviaqa, hotpotqa, samsum, musique, multi\_news, 2wikimqa, gov\_report, trec,narrativeqa, passage\_count, passage \_retrieval\_en,} and \textit{qasper}) and 2 coding tasks (\textit{lcc} and \textit{repobench-p}), with average contexts for these tasks ranging between 5000 and 15000 tokens -- though the longest contexts exceed 32000 tokens.

Note that LongBench's default prediction script allows truncating prompts longer than a user-defined threshold (the $max\_length$ parameter) before being input into the LLM model, so that the instructions at the beginning or end of the prompt are not removed. We used the default LongBench configuration, where the prompt is truncated to a size below the maximum context length of the LLM. For our results, this equals a $max\_length$ of 7500 tokens if the context length is 8196 (LlaMA-3 8B) and 31500 tokens for a context length of 32768 (qwen-2.5 7B)% and Mistral 7B).



LongBench results for \textit{triviaqa, samsum} (few-shot learning), \textit{repobench-p} (coding), \textit{2wikimqa, hotpotqa} (multi-document Q\&A), \textit{multifieldqa\_en} (single-document Q\&A), \textit{gov\_report} (summarization) and the total aggregate scores of all 16 English and coding tasks are plotted in Figure~\ref{fig:longbench_results_small}. Higher values indicate better performance.  
\\
We evaluate three KV-cache eviction and memory allocation strategies: StreamingLLM~\cite{streamingllm}, SqueezeAttention~\cite{squeezeattention}, and BaKlaVa. Strea-mingLLM applies a token eviction policy once a KV-cache reaches capacity but assigns equal memory to all caches. SqueezeAttention extends this approach by varying memory allocation across layers, while BaKlaVa further refines memory distribution by adjusting allocations for both layers and individual KV-caches. Note that other token eviction or compression policies can be used with SqueezeAttention and BaKlaVa; we chose StreamingLLM for ease of implementation. 

In Figure~\ref{fig:longbench_results_small} the total aggregate score shows that BaKlaVa outperforms other KV-cache memory allocation methods (StreamingLLM and SqueezeAttention) on average; however, the results show distinctly different behaviors for LlaMA3-8B and Qwen2.5-7B models. This is due to the different context lengths of both models (8196 tokens for LlaMA3-8B and 32768 for Qwen2.5-7B) along with the prompt truncation behavior of LongBench we discussed above. In Qwen2.5-7B, at compression ratio 1.0 we start with the default context length of 32k tokens, which is significantly more than most prompts in LongBench. Thus, increasing KV-cache compression does not affect results up to a certain point (e.g., 0.5 compression ratio in \textit{multifieldqa\_en} for Qwen). Afterward, there is a gradual decrease in the score as long-context prompts lose critical information upon compression. 

On the other hand, for LlaMA3, at compression ratio 1.0 we start at the model's maximum context length of 8192 tokens. Since most prompts in LongBench are more than 8192 tokens, they get truncated in the middle, leaving the critical instructions at the beginning and/or end of the now shorter prompt. Thus, in LlaMA, the score starts decreasing rapidly upon increasing the KV compression, as the critical instructions at the beginning of the prompt get quickly evicted with the StreamingLLM window-based eviction policy. With BaKlaVa, we observe that these critical instructions are remembered until a much higher compression ratio. For example, 0.6 compression for all results in Figure~\ref{fig:longbench_results_small} for LlaMA. This behavior is also observed in Qwen, but for higher compression rations of 0.1 and 0.2, where BaKlaVa retains an order of magnitude larger score compared to the other methods. This can be seen in \textit{multifieldqa\_en, samsum, 2wikimqa}, etc. for Qwen. 

Finally, we see that on average SqueezeAttention performs better than StreamingLLM in LlaMA, but worse in Qwen. This is due to the layer importance heuristic used in SqueezeAttention closely matching the `true' importance values in LlaMA, but not in Qwen. See Section~\ref{sec:empirical_results} for details.
%This is explained in detail below.


\subsection{Empirical Evaluation of Heuristics}
\label{sec:empirical_results}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/empirical_layer.pdf}
    %\caption{Comparison of layer importance heuristic with empirical evaluation results. The least important layers based on heuristic and empirical test scores respectively are highlighted in green, while critical layers are highlighted in red. LlaMA3-8B and Mistral-7B-v0.1 show high similarity across both heuristic and empritical results, whereas Qwen2.5-7B does not. As a result, performing layer-wise KV-cache memory allocation using the layerwise heuristic on Qwen can lead to degradation of performance.}
    \caption{Comparison of layer importance heuristics with empirical evaluation results. Layers identified as least important by both heuristic and empirical test scores are highlighted in green, while critical layers are marked in red. LlaMA3-8B and Mistral-7B-v0.1 exhibit strong alignment between heuristic predictions and empirical findings, whereas Qwen2.5-7B shows significant discrepancies. Consequently, applying layer-wise KV-cache memory allocation based on the heuristic to Qwen2.5-7B may result in performance degradation.}
    \label{fig:empirical_layer_results}
\end{figure}



Figure~\ref{fig:empirical_layer_results} presents the empirical test results (detailed in Section~\ref{sec:empirical}) for the LlaMA3-8B, Qwen2.5-7B, and Mistral-7B models, compared against the layer importance heuristics used in BaKlaVa and SqueezeAttention for estimation of layer importance with low computational cost. The heuristic results are expressed as similarity scores, as defined in Section~\ref{sec:determine_kv_importance}, while the empirical results are reported as average LongBench scores on the \textit{triviaqa} dataset across compression ratios ranging from 0.5 to 0.95. Both heuristic and empirical results are visualized using a red-yellow-green color gradient, where less critical layers (i.e., those with high similarity scores or high empirical performance) are highlighted in green. It is important to note that the first and last two empirical test results exhibit a bias toward higher scores due to the sliding window approach. This overlap at the layer boundaries results in less cumulative compression across layers, thereby preserving more KV-cache memory overall.


Accounting for this bias, we observe that LlaMA3-8B and Mistral-7B exhibit highly similar layer importance patterns, as indicated by the alignment of green and red-shaded regions. In contrast, Qwen2.5-7B demonstrates a markedly different layer importance distribution. Consequently, applying layer-wise KV-cache memory allocation to Qwen leads to poorer performance compared to equal memory allocation, as evidenced by the Qwen results for SqueezeAttention and StreamingLLM in Figure~\ref{fig:longbench_results_small}.

