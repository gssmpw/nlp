\section{Conclusion and Future Work \label{sec:conclusion}}


In this work, we introduce \textbf{BaKlaVa}, a simple yet effective approach to optimize LLM inference through intelligent KV-cache memory allocation. By leveraging a heuristic-based method to estimate layer and KV-cache importance, BaKlaVa significantly improves memory efficiency while maintaining model performance across a range of compression ratios. Our empirical evaluations demonstrate that BaKlaVa outperforms existing KV-cache allocation strategies, such as uniform allocation (StreamingLLM) and allocating KV-cache memory with layer-wise granularity (SqueezeAttention), particularly in tasks where preserving long-range dependencies is crucial. Notably, BaKlaVa maintains near-baseline performance up to 70\% compression, surpassing alternative methods on long-context datasets by preserving essential information and achieving higher accuracy under high compression across multiple tasks

A key advantage of our method is its ability to adapt to different model architectures by \textit{dynamically}  adjusting memory allocation based on computationally inexpensive heuristics. Unlike prior approaches that apply uniform compression or coarse layer-wise compression, BaKlaVa efficiently distributes KV-cache memory to maximize performance under constrained budgets. These improvements highlight the potential of fine-grained memory allocation in enhancing the efficiency of LLM inference without requiring modifications to model architecture or training procedures.


In future work, our aim is to develop a generalized framework for adaptive KV-cache memory allocation, reducing the need for manual parameter tuning. Additionally, extending BaKlaVa to support additional KV eviction policies and dynamically adjusting memory budgets at runtime could further enhance its applicability to real-world deployments.

