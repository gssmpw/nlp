\documentclass[final,onefignum,onetabnum]{siamart220329}
\usepackage[margin=1.4in]{geometry}

%\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{subfigure}
\usepackage{amsmath,amssymb}
\usepackage{blkarray, bigstrut, bigdelim}
\usepackage{mathtools,mathrsfs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{xspace}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{matrix,calc}
\usepackage{algorithm}% 
\usepackage[noend]{algpseudocode}% 
\algrenewcommand\textproc{}

% some useful stylistic commands
\newcommand{\eg}{e.\,g.\xspace}
\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\norm}[2]{\left\Vert {#2} \right\Vert_{#1}}
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}
\newcommand{\given}{\left.\hspace*{-0.0833em}\middle|\hspace*{-0.0833em}\right.}

% variable commands
\newcommand{\bvec}[1]{\textbf{#1}}
\newcommand{\bvecS}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\design}{\bvec{e}} % design weight vector 
\newcommand{\params}{\bvec{m}} % inversion params
\newcommand{\refparam}{z} % ref param
\newcommand{\refparams}{\bvec{\refparam}} % ref params
\newcommand{\unitvec}{\bvec{u}} % 
\newcommand{\targetparam}{x}
\newcommand{\targetparams}{\bvec{\targetparam}} % params in target space
\newcommand{\ratioparams}{\bvec{w}} % params in ratio space
\newcommand{\data}{\bvec{y}} % data 
\newcommand{\noise}{\bvecS{\eta}} % noise 
\newcommand{\indx}{\mathcal{I}} % index set
\newcommand{\m}{\mathrm{m}}
\newcommand{\LISDF}{\bvec{U}} % Eigenvectors of data-free LIS matrices
\newcommand{\LISDD}{\bvec{V}} % Eigenvectors of data-dependent LIS matrices
\newcommand{\source}{S} % PAI source

% dimensions and spaces
\newcommand{\Ne}{n_{e}} % number of candidate designs, \Ne = \Ns*\Nf 
\newcommand{\Ns}{s} % number of candidate sensor locations 
\newcommand{\Nd}{n_{y}} % dimension of measured data for each experiment
\newcommand{\NdAll}{n_{A}} % dimension of measured data at all candidate designs, \NdAll = \Nd*\Ne
\newcommand{\Ntot}{n} % n = \Nm+\Nd
\newcommand{\Nf}{t} % number of rhs 
\newcommand{\Nm}{n_{m}} % discretized parameter space
\newcommand{\TTRank}{R} % ranks of TT cores
\newcommand{\LISdim}{r} % dimension of LIS 
\newcommand{\nSamps}{N} % number of MC samples 
\newcommand{\paramSpace}{\mathbb{R}^{\Nm}}
\newcommand{\paramSubspace}{\mathcal{M}}
\newcommand{\designSpace}{\mathcal{E}}
\newcommand{\dataSpace}{\mathcal{Y}}
\newcommand{\targetSpace}{\mathcal{X}}

% Maps 
\newcommand{\PtO}{\mathcal{G}} % param-to-obs map 
\newcommand{\discPtO}{\bvec{G}} % discretized PTO map 
\newcommand{\prPtO}{\widetilde{\PtO}} % prior preconditioned PtO map
\newcommand{\PtoState}{\mathcal{A}} % param-to-state map 
\newcommand{\ObsOp}{O} % state-to-obs map 
\newcommand{\I}[1]{\bvec{I}_{#1}}
\newcommand{\0}[1]{\bvec{0}_{#1}}
\newcommand{\Proj}{\bvec{P}} % projection operator
\newcommand{\TTCore}[2]{\bvec{F}_{#1}(#2)}
\newcommand{\pushforward}[1]{\mathop{#1_\sharp}}
\let\pullback\undefined
\newcommand{\pullback}[1]{\mathop{#1^\sharp}}
\newcommand{\cS}{\mathcal{S}} % transport map 
\newcommand{\cT}{\mathcal{T}} % transport map 
\newcommand{\cQ}{\mathcal{Q}} % ratio transport map
\newcommand{\Jac}{\nabla} % Jacobian 

% measures, probabilities, statistics commands etc... 
\newcommand{\mupr}{\mu_{0}} % prior measure 
\newcommand{\refD}{\rho} % reference density
\newcommand{\target}{\pi} % arbitrary target density
\newcommand{\targetTT}{\Pi} % target density with TT approx
\newcommand{\targetUnnorm}{p} % unnormalized target density
\newcommand{\likelihood}[2]{\mathcal{L}(#1\given#2)}
\newcommand{\Rlikelihood}[2]{\widetilde{\mathcal{L}}(#1\given#2)}
\newcommand{\ratio}[1]{q^{#1}} % ratio, pullback density
\newcommand{\Cnoise}{\bvecS{\Gamma}_{\noise}} % noise covariance matrix 
\newcommand{\CWnoise}{\bvec{\Gamma}_{\bvec{W}}} % inverse of weighted noise covariance, used in data-dependent likelihood 
\newcommand{\Cnoiseinv}[1]{{\Cnoise(#1)}^{-1}} % noise covariance matrix 
\newcommand{\prmean}{\params_{0}}
\newcommand{\Cpr}{\mathcal{C}_{0}}
\newcommand{\Cprinv}{\mathcal{C}_{0}^{-1}}
\newcommand{\postmean}{\params_{\data}}
\newcommand{\Cpost}{\mathcal{C}_{\params|\data}}

% OED objectives and related variables
\newcommand{\EIG}{\Psi} 
\newcommand{\distKL}[2]{\mathcal{D}_\textup{KL}{(}{#1}{\|}{#2}{)}}
\newcommand{\distH}[2]{\mathcal{D}_\textup{H}{(}{#1,\,#2}{)}}
\newcommand{\Hist}{\bvec{H}} % Design history vector 
\newcommand{\Hmat}{\mathcal{H}} % LIS matrix
\newcommand{\T}{\cT} %transport map
\newcommand{\Expect}[2]{\mathbb{E}_{#1}{[}{#2}{]}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Argmax}{Arg\,max}
\DeclareMathOperator{\DFLIS}{DF\-LIS}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\newtheorem*{remark}{Remark}

\begin{document}

\setlength\textfloatsep{12pt}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\allowdisplaybreaks

\title{Subspace accelerated measure transport methods for fast and scalable sequential experimental design, with application to photoacoustic imaging}

\def\addressC{Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University, Heidelberg, Germany}
\def\addressE{Institute for Mathematics, Heidelberg University, Heidelberg, Germany}
\def\addressD{School of Mathematics and Statistics, University of Sydney, Australia.}

\author{Tiangang Cui\footnotemark[1]
    \and Karina Koval\footnotemark[2]
     \and Roland Herzog\footnotemark[2] \footnotemark[3]
    \and Robert Scheichl\footnotemark[3] \footnotemark[2]}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\footnotetext[2]{\addressC}%\ (\email{karina.koval@iwr.uni-heidelberg.de}).}
\footnotetext[1]{\addressD}%\ (\email{tiangang.cui@sydney.edu.au}).}
\footnotetext[3]{\addressE}%\ (\email{r.scheichl@uni-heidelberg.de}).}
%\footnotetext[4]{\addressC\ (\email{roland.herzog@iwr.uni-heidelberg.de}).}

\maketitle

\begin{abstract}
We propose a novel approach for sequential optimal experimental design (sOED) for Bayesian inverse problems involving expensive models with large-dimensional unknown parameters. The focus of this work is on designs that maximize the expected information gain (EIG) from prior to posterior, which is a computationally challenging task in the non-Gaussian setting. This challenge is amplified in sOED, as the incremental expected information gain (iEIG) must be approximated multiple times in distinct stages, with both prior and posterior distributions often being intractable. To address this, we derive a derivative-based upper bound for the iEIG, which not only guides design placement but also enables the construction of projectors onto likelihood-informed subspaces, facilitating parameter dimension reduction. By combining this approach with conditional measure transport maps for the sequence of posteriors, we develop a unified framework for sOED, together with amortized inference, scalable to high- and infinite-dimensional problems. Numerical experiments for two inverse problems governed by partial differential equations (PDEs) demonstrate the effectiveness of designs that maximize our proposed upper bound. \footnote[0]{\textbf{Funding:} This work was funded by the Carl Zeiss Stiftung through the project ``Model-Based AI: Physical Models and Deep Learning for Imaging and Cancer Treatment''.}
\end{abstract}

\begin{keywords}
optimal experimental design, Bayesian inverse problems, measure transport, dimension reduction
\end{keywords}

\begin{AMS}
  62K05, 62F15, 65D40
\end{AMS}

\section{Introduction}

We consider the sequential design of a finite number of experiments for Bayesian inverse problems. 
%
In sequential optimal experimental design (sOED), experimental conditions are chosen in distinct stages (indexed by $k \in \mathbb{N}$) and guided by the inference results conditioned on data observed from previous experiments. This design setting naturally arises in a range of applications---including medical imaging, subsurface exploration, assessment of environmental hazards, and more---where practitioners need to dynamically adjust experimental conditions, such as sensor locations, based on feedback from previous estimations. 
%
In such settings, sOED's advantage over standard batch experimental design is its ability to further reduce the estimation uncertainty by adaptively tailoring experimental conditions towards the unknown ground truth. 
%
However, sOED tends to be more computationally challenging than batch OED due to the presence of a feedback loop that requires the state of knowledge about the model parameters to be iteratively updated after each experiment is conducted. 

To set up the problem, let $\params \in \mathbb{R}^{\Nm}$ denote the unknown model parameters we seek to estimate, $\design_k \in \designSpace_k \subset \mathbb{R}^{\Ne}$ denote the stage-$k$ experimental conditions, and $\data_k \in \dataSpace_k \subset \mathbb{R}^{\Nd}$ denote the corresponding data collected during the $k$-th experiment.
In addition, we denote the sequence of experimental conditions and the corresponding observed data in the previous $k-1$ experiments by $\Hist_{k-1} = [\design_1^*,\data_1^*,\ldots,\design_{k-1}^*,\data_{k-1}^*]$ (with $\Hist_0 = \emptyset$). At stage-$k$, we represent the posterior density of the Bayesian inverse problem based on all previous experiments $\Hist_{k-1}$ by $\target(\params \given \Hist_{k-1})$. This way, after conducting the $k$-th experiment, the posterior density can be recursively updated as
%
\begin{equation}
\target(\params \given \design_k,\data_k,\Hist_{k-1}) = \frac{\likelihood{\data_k}{\params, \design_k, \Hist_{k-1}} \,\target(\params \given \Hist_{k-1}) }{\target(\data_k \given \design_k, \Hist_{k-1})},
\label{eq:posterior_update}
\end{equation}
where $\target(\data_k \given \design_k, \Hist_{k-1})$ is the (typically unknown) stage-$k$ evidence at design $\design_k$, and the previous posterior density $\target(\params \given \Hist_{k-1})$ becomes the stage-$k$ prior. 
Before collecting any data, we initialize the first stage with a prior distribution, setting $\target(\params \given \Hist_0) = \target(\params)$. This initial stage prior is often chosen to have an analytically tractable form, such as a Gaussian distribution $\target(\params) \sim \mathcal{N}(\params_0,\Cpr)$.
The stage-$k$ likelihood, $\likelihood{\data_k}{\params, \design_k, \Hist_{k-1}}$, depends on the model and measurement process that relate the parameters and design to the noisy observed data. As an example, we consider an additive Gaussian noise model, 
\begin{equation}
\data_k(\design_k) = \PtO(\design_k,\params)+\noise_k(\design_k), 
\end{equation} 
where $\PtO \colon \designSpace_k \times \paramSpace \rightarrow \dataSpace_k$ is the forward map, and $\noise_k \sim \mathcal{N}(\bvec{0},\Cnoise(\design_k))$ is the Gaussian noise. 
Under these assumptions, the stage-$k$ likelihood is independent of previous experiments, \ie, 
\begin{equation}\likelihood{\data_k}{\params, \design_k, \Hist_{k-1}} = \likelihood{\data_k}{\params, \design_k} \propto \exp\left(-\frac{1}{2} \norm{\Cnoiseinv{\design_k}}{\PtO(\design_k,\params)-\data_k(\design_k)}^2\right).
\label{eq:likelihood}
\end{equation}
Of particular interest are problems where the forward map is nonlinear and costly to evaluate, such as when it is defined implicitly through a partial differential equation (PDE).
In these settings, $\params$ typically represents a discretized functional input to the PDE, making it high-dimensional.  
For simplicity, we assume constant dimensions for the design and observed data spaces across all experimental stages, and that both the forward map $\PtO$ and noise model remain the same, although these assumptions are not essential to our approach.

There are various sOED formulations, including~\cite{HuanMarzouk:2016:1,ShenHuan:2023:1,FosterIvanovaMalikRainforth:2021:1}, which involve some degree of ``lookahead'', taking future experiments into account when choosing the optimal design at each experimental stage.   
We focus on applications without a hard limit on the experimental budget, where a greedy or myopic approach to sOED is particularly effective.   
To this end, we consider the design of multiple experiments, where the conditions at each experimental stage are chosen to maximize the \emph{incremental expected information gain} (iEIG),
\begin{align}
\EIG_k(\design_k) &= \Expect{\data_{k}\given \design_{k},\Hist_{k-1}}{\distKL{\target(\cdot \given \design_k,\data_k,\Hist_{k-1})}{\target(\cdot \given \Hist_{k-1})}}
\label{eq:EIG_k}
\end{align}
where $$\distKL{\target(\cdot \given \design_k,\data_k,\Hist_{k-1})}{\target(\cdot \given \Hist_{k-1})} \coloneqq \int \log\left(\frac{\target(\params \given \design_k,\data_k, \Hist_{k-1})}{\target(\params \given \Hist_{k-1})}\right) \target(\params \given \design_k,\data_k,\Hist_{k-1}) \, \mathrm{d}\params$$ is the Kullback--Leibler (KL) divergence of the posterior from the prior.

The optimal design $\design_k^*$ that maximizes~\eqref{eq:EIG_k} typically lacks an analytical form, and thus must be approximated numerically. This presents a combination of challenges, as the iEIG does not have a closed-form expression outside specific cases, e.g., those involving linear parameter-to-observable maps, Gaussian priors, and additive Gaussian noise. 
%
The primary challenge arises in approximating nested expectations with respect to the densities \linebreak $\target(\params \given  \design_k, \data_k, \Hist_{k-1})$ and $\target(\data_k \given \design_k,\Hist_{k-1})$ in \eqref{eq:EIG_k}, and accessing the ratio $\frac{\target(\params \given \design_k, \data_k, \Hist_{k-1})}{\target(\params \given \Hist_{k-1})}$ in the KL divergence calculation. 
In the first stage, with a tractable prior $\target(\params \given \Hist_{0}) \coloneqq \target(\params)$ that can be directly sampled, the EIG is commonly approximated using a nested Monte Carlo estimator, or a Laplace approximation of the intractable posterior~\cite{BeckMasourEspathTempone:2020,HuanMarzouk:2013:1,WuChenGhattas:2023:1}. 
However, approximating the incremental EIGs at subsequent stages ($k > 1$) becomes increasingly complicated, as drawing samples from the intractable prior distribution $\target(\params \given \Hist_{k-1})$ in later stages is a known challenge---often requiring specifically designed Markov-chain-based samplers \cite{CuiLawMarzouk:2016:1,girolami2011riemann,martin2012stochastic}. Moreover, the dimensionality of the parameters will further aggravate the complexity of sampling \cite{cotter2013mcmc,mattingly2012diffusion,roberts2001optimal}.  
%
We propose a likelihood-informed, measure-transport-based approach to sequentially approximate optimal designs, which is computationally feasible and scalable with dimension, while simultaneously characterizing the posterior distribution of the inverse problem. 

\subsection{Related work}\label{subsec:relatedWork}
The most common approach for estimating the expected information gain involves the use of nested Monte Carlo estimators~\cite{Ryan:2003:1,HuanMarzouk:2013:1}. 
In this approach, the EIG is reformulated as an expectation of the difference between the log-likelihood and log-evidence. 
Estimating the EIG requires an outer Monte Carlo estimator for the expectation, along with an inner nested Monte Carlo loop to estimate the intractable evidence for each outer Monte Carlo sample. 
While the nested Monte Carlo estimator is asymptotically unbiased, its convergence is slower than that of standard Monte Carlo, and obtaining sufficiently accurate estimators can be computationally expensive. Recently, measure transport approaches to batch optimal experimental design have been explored in~\cite{CaoChenBrennanOLearyRoseberryMarzoukGhattas:2024:1,LiBaptistaMarzouk:2024:1,DongJacobsenKhalloufiAkramLLiuDuraisamyHuan:2025:1}. 
These approaches can generally be viewed as two-step estimation approaches, combining measure-transport-based density estimation with a Monte Carlo estimator of the expectation.

Our previous work~\cite{KovalHerzogScheichl:2024:1} also uses transport maps for sOED, but is limited to problems with low- to moderate-dimensional parameters. A few other sOED approaches are outlined in the review articles~\cite{HuanJagalurMarzouk:2024:1,RyanDrovandiMcGreePettitt:2016:1}. 
Many of these approaches are formulated for low-dimensional parameters and involve sequentially transforming samples between experimental stages using ratio function estimation or sequential Monte Carlo methods~\cite{DrovandiMcGreePettitt:2013:1,KleinegesseDrovandiGutmann:2020:1}. 
In contrast, we focus on problems where the unknown parameter $\params$ corresponds to a finite-dimensional discretization of a functional input of the forward map, which typically results in high dimension.

In the context of OED for large- or infinite-dimensional problems, common approaches typically involve some combination of Gaussian approximations to the posterior~\cite{WuChenGhattas:2023:1}, the use of derivative-informed neural networks~\cite{WuOLeary-RoseberryChenGhattas:2023:1}, or approaches that exploit the presence of low-dimensional structures~\cite{CaoBaptistaChenLiGhattasOdenMarzouk:2023:1,LiBaptistaMarzouk:2024:1}. 
The aforementioned works all focus on batch OED, though a combination of these approaches has also been used in sOED. 
Specifically,~\cite{GoChen:2024:1} employs Gaussian approximations to the posterior, constructed efficiently using dimension-reduced neural network surrogates, for the sequential selection of optimal observation times in Bayesian inverse problems involving dynamical systems. In contrast, our primary focus is on the optimal selection of sensor locations, which requires a different formulation of the sOED problem. 

\subsection{Our approach and contributions}
We propose a novel approach to sequential optimal experimental design by maximizing a sharp bound on the incremental expected information gain. This bound, originally derived for estimating the intrinsic dimensionality of Bayesian inverse problems~\cite{LiCuiLiMarzoukZahm:2024:1}, is extended here as a fast-to-evaluate surrogate to simplify the computationally demanding objective functions of sOED. A key component of constructing this bound in the sequential setting is the use of measure transport, which can be implemented using any suitable method. In this work, we utilize tensor-train methods to efficiently construct these transport maps. In particular, we leverage the likelihood-informed parameter dimension reduction offered by the same upper bound used in sOED to make our transport maps scalable to large- or infinite-dimensional parameters. Another highlight of our framework is the integration of conditional transport maps---an extension of standard measure transport---into the sOED process to facilitate amortized inference. This enables the construction of transport maps prior to data collection at each experimental stage, allowing for direct posterior inference in real time once data are available.
In addition, we introduce a restart strategy to further improve the approximation accuracy of measure transport maps in this sequential setting.
%
Finally, through numerical examples, we evaluate the effectiveness of our designs, comparing them to nested Monte Carlo estimators and assessing the performance of designs maximizing our proposed upper bound against those based on Gaussian approximations.

The rest of this paper is organized as follows. In~\cref{sec:background}, we summarize relevant background material on likelihood-informed parameter dimension reduction, density approximation via triangular measure transport, and functional tensor trains. In~\cref{subsec:iEIG_UB}, we derive an upper bound on the incremental expected information gain and discuss how this bound can be used in conjunction with measure transport to guide the sequential design of experiments. In~\cref{subsec:sOED_composite}, we discuss how to combine conditional transport maps with likelihood-informed parameter dimension reduction at each experimental stage to enable scalable amortized inference in the sOED procedure.
In~\cref{subsec:sOED_restart}, we improve transport map accuracy by incorporating a restart strategy, leading to our final sOED algorithm, which we use for numerical comparisons in~\cref{sec:num1} and~\cref{sec:num2}.

\section{Background}\label{sec:background}

Here we summarize data-dependent and data-free likelihood-informed subspaces, which are used for parameter reduction and to formulate an upper bound on the incremental EIG. We also review transport-based posterior estimation and its tensor train construction, which are elements of our sOED method.

\subsection{Likelihood-informed subspaces}\label{subsec:LIS} The efficiency of sOED algorithms crucially relies on the ability to efficiently characterize generally intractable posterior densities $\target(\params \given \data) \propto \likelihood{\data}{\params}\,\target(\params)$ for large-dimensional parameters $\params \in \paramSpace$. 
Sampling from posterior distributions of high-dimensional parameters presents significant computational challenges. To address this, efficient sampling strategies often take advantage of low-dimensional structures that may be present in the parameter-data interaction. These can be due to, \eg, the smoothing properties of the prior distribution and the forward model, as well as the typically incomplete, noisy nature of data. As a result, the data usually informs only a low-dimensional subspace of the parameter space, relative to the prior. A key step in various efficient, dimension-independent samplers \cite{BrennanBigoniZahmSpantiniMarzouk:2020:1,ConstantineKentBui-Thanh:2016:1,CuiLawMarzouk:2016:1,CuiMartinMarzoukSolonenSpantini:2014:1,cui2022prior} is identifying the directions in the posterior that show the greatest variation from the prior. 
These directions could be obtained in various ways, but we use the \emph{likelihood-informed subspace} (LIS) methods outlined in~\cite{CuiTong:2022:1,CuiZahm:2021:1,LiCuiLiMarzoukZahm:2024:1,zahm2022certified}.

LIS methods seek to identify a subspace $\mathcal{M}_{\LISdim} \subset \paramSpace$ of dimension $\LISdim \ll \Nm$ that contains the effective support of the likelihood $\likelihood{\data}{\params}$. 
Let $\Proj_{\LISdim}$ and $\Proj_{\perp}$ denote orthogonal projection operators onto the space $\paramSubspace_{\LISdim}$ and its orthogonal complement $\paramSubspace_{\perp}$, respectively.
Once the likelihood-informed subspace is identified, the parameter $\params$ can be decomposed as $\params = \params^{\LISdim} + \params^{\perp}$, with $\params^{\LISdim} = \Proj_{\LISdim}\params$ and $\params^{\perp} = \Proj_{\perp}\params$. 
Following this decomposition, the full-dimensional likelihood function is approximated by $\likelihood{\data}{\params} \approx \int \likelihood{\data}{\params^{\LISdim}+{\params^{\perp}}'} \target({\params^{\perp}}' \given \params^{\LISdim}) \,\mathrm{d}{\params^{\perp}}' \eqqcolon \Rlikelihood{\data}{\params^{\LISdim}},$
which has support on $\paramSubspace_{\LISdim}$. This results in an approximation to the full-dimensional posterior:%\vspace{-12pt}
\begin{equation}
    \target(\params\given \data) \approx \widetilde{\target}(\params \given \data ;\Proj_{\LISdim}) \propto \underbrace{\Rlikelihood{\data}{\params^{\LISdim}} \target(\params^{\LISdim})}_{\widetilde{\target}(\params^{\LISdim} \given \data)}\,{\target(\params^\perp \given \params^{\LISdim})},
    \label{eq:li_posterior}
\end{equation}
where, with a slight abuse of notation, $\target(\params^{\LISdim}) = \int_{\Proj_{\perp}} \target(\params^{\LISdim} + {\params^{\perp}}') \,\mathrm{d} {\params^{\perp}}'$ is the marginal prior and $\target(\params^{\perp}\given\params^{{\LISdim}}) = \frac{\target(\params^{\LISdim}+\params^{\perp})}{\target(\params^{\LISdim})}$ is the conditional prior. Note that $\widetilde{\target}(\params^{\LISdim} \given \data)$ is also the marginal posterior density of the reduced parameters. This decomposition facilitates an efficient two-step sampling algorithm: one can draw samples from the lower-dimensional marginal posterior density (\eg, using Markov chain Monte Carlo), followed by independent samples drawn from the conditional prior density $\target(\params^{\perp}\given\params^{\LISdim})$. With slight modification using either the pseudo-marginal principle or importance sampling, exact samples can also be obtained from the full posterior, see \cite{CuiTong:2022:1,cui2022prior,zahm2022certified} for details. 

Accurately identifying the LIS $\paramSubspace_{\LISdim}$ is key for these algorithms and there are several variations. Here, we outline a \emph{data-dependent} approach and a \emph{data-free} approach, both of which utilize derivative information of the likelihood function. Without loss of generality, we assume that the parameters are transformed so that the associated prior distribution is a standard multivariate Gaussian, $\target(\params) = \mathcal{N}(\bvec{0},\I{\Nm})$, see \cite{CuiMartinMarzoukSolonenSpantini:2014:1,martin2012stochastic} for examples of linear transformations and \cite{CuiTong:2022:1} for nonlinear transformations. 

\smallskip
\paragraph{Data-dependent LIS~\cite{CuiTong:2022:1}} Given some measured data, $\data \in \dataSpace$, the data-dependent LIS is defined as the subspace spanned by the first $\LISdim$ leading eigenvectors of the Gram matrix, 
\begin{equation}
\Hmat_{G}(\data) = \int \nabla_{\params} \log \likelihood{\data}{\params} \,\nabla_{\params} \log \likelihood{\data}{\params}^\top \target(\params \given \data) \, \mathrm{d}\params.
\label{eq:gram}
\end{equation}
Denoting the eigenvalues of $\Hmat_{G}(\data)$ as $\{\lambda_{i}(\Hmat_{G}(\data))\}_{i=1}^{\Nm}$ with $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_{\Nm}$, the approximation to the posterior resulting from a restriction to this data-dependent subspace (with corresponding projection operator $\Proj^{G}_{\LISdim}$) satisfies~\cite[Theorem 2.4]{CuiTong:2022:1}
\begin{equation}
\distH{\target(\cdot \given \data)}{\widetilde{\target}(\cdot \given \data; \Proj^{G}_{\LISdim})} \leq \frac{\sqrt{\kappa}}{2}\bigg( \sum_{k={\LISdim}+1}^{\Nm}\lambda_{k}(\Hmat_{G}(\data))\bigg)^{1/2}, 
\label{eq:DI_bound}
\end{equation}
where $\distH{\cdot}{\cdot}$ is the Hellinger distance and $\kappa$ is the subspace Poincar\'e constant of the prior, which is bounded under mild conditions (see~\cite[Assumption 2.1, Proposition 2.2]{CuiTong:2022:1} for details). 

\smallskip
\paragraph{Data-free LIS~\cite{CuiZahm:2021:1,LiCuiLiMarzoukZahm:2024:1}}\label{paragraph:DF-LIS} In contrast to the data-dependent LIS, the data-free LIS can be constructed before observing any data. 
This approach allows for the identification of directions in which the posterior distribution varies the most from the prior for the average data realization. 
Denoting the Fisher information matrix of the likelihood as 
\begin{equation} \mathcal{I}(\params) = \int \left(\nabla_{\params} \log \likelihood{\data}{\params} \, \nabla_{\params} \log \likelihood{\data}{\params}^\top\right) \likelihood{\data}{\params}\, \mathrm{d}\data, 
\label{eq:Fisher}
\end{equation}
the data-free LIS is defined through the dominant $r$-dimensional eigenspace of the matrix 
\begin{equation}
\Hmat_{I} = \int \mathcal{I}(\params) \, \target(\params) \, \mathrm{d}\params.
\label{eq:avgFisher}
\end{equation}
Similar to the data-dependent approach, the expected error (over the data distribution) in approximating the full-dimensional posterior density with~\eqref{eq:li_posterior} restricted to the subspace spanned by the leading eigenvectors of $\Hmat_I$ (with corresponding projection operator $\Proj^{I}_{\LISdim}$) can be bounded. Error bounds in the form of \eqref{eq:DI_bound} are given in \cite{CuiDolgovZahm:2023:1,CuiZahm:2021:1}. With a standard multivariate Gaussian prior, the error bound can be further sharpened~\cite[Theorem 9]{LiCuiLiMarzoukZahm:2024:1} as:
\begin{equation}
    \mathbb{E}_{\data}\left[ \distKL{\target(\cdot\given \data)}{\widetilde{\target}(\cdot \given \data ; \Proj^{I}_{\LISdim})}\right] \leq \frac{1}{2}\sum_{k=\LISdim+1}^{\Nm} \log \left(1+\lambda_k(\Hmat_{I})\right). 
    \label{eq:EIG_bound}
\end{equation}

To apply these approaches, one needs to numerically approximate the matrices $\Hmat_{G}$ and $\Hmat_{I}$, which typically involves Monte Carlo approximations to the expectation. As analyzed in~\cite{CuiTong:2022:1}, the data-dependent approach provides more accurate approximations to the informed subspace given a fixed instance of measured data. However, it can be computationally challenging due to the calculation of the expectations over the typically inaccessible posterior distributions. The data-free approach avoids this challenge and is advantageous when solving multiple Bayesian inverse problems with varying observed data. Furthermore, the average Fisher information matrix $\Hmat_{I}$ in~\eqref{eq:EIG_bound} immediately provides a sharp bound on the expected information gain, which offers a fast surrogate to guide our sOED.
Specifically, using~\eqref{eq:EIG_bound} with $r = 0$ bounds the EIG as
\begin{equation}
    \mathbb{E}_{\data}\left[ \distKL{\target(\cdot\given \data)}{{\target}(\cdot )}\right] \leq \frac{1}{2} \log \det \left(\I{\Nm}+\Hmat_{I}\right). 
    \label{eq:EIG_bound_full}
\end{equation}

\subsection{Knothe--Rosenblatt rearrangement}
Although the LIS methods can reduce the dimensionality of the parameters, the question of how to efficiently characterize and sample from the dimension-reduced posterior remains. Measure transport, see~\cite{BaptistaMarzoukZahm:2023:1,MoselhyMarzouk:2012:1,MarzoukMoselhyParnoSpantini:2016:1} for instance, offers a versatile solution to this, with applications widely found in Bayesian inference, rare event estimation, and optimal experimental design. Measure transport constructs an invertible transformation $\mathcal{F} \colon \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ between a tractable reference measure $\nu$ with density $\refD$ (\eg, a multivariate Gaussian) and the intractable target measure $\mu$ with density $\target$ (\eg, a posterior density). 
If $\mathbb{R}^{n} \ni \refparams = \mathcal{F}(\targetparams) \sim \refD$ for any $\mathbb{R}^n \ni \targetparams \sim \target$ then $\mathcal{F}$ is said to \emph{push forward} the target density $\target$ to the reference density $\refD$ (likewise, \emph{pull back} $\refD$ to $\target$). The pushforward and pullback operators are defined, respectively, as 
\begin{align}
\pushforward{\mathcal{F}}\target(\refparams) &= \left(\target \circ \mathcal{F}^{-1}\right)(\refparams) | \Jac\mathcal{F}^{-1} (\refparams)| = \refD(\refparams) \label{eq:pushforward} \\
\pullback{\mathcal{F}}\refD(\targetparams) &= \left(\refD \circ \mathcal{F}\right)(\targetparams) | \Jac\mathcal{F} (\targetparams)| = \target(\targetparams),
\label{eq:pullback}
\end{align}
where $\Jac\mathcal{F}$ denotes the Jacobian of $\mathcal{F}$.
In our setting, we assume both the reference and target measures are absolutely continuous with respect to the Lebesgue measure. 

For multi-dimensional random variables, there may exist infinitely many measure transport maps $\mathcal{F}$ satisfying~\eqref{eq:pushforward} and~\eqref{eq:pullback} and various numerical methods for building them, including normalizing flows~\cite{KruseDetommasoKoetheScheichl:2021:1,PapamakariosNalisnickRezendeMohamedLakshminarayanan:2021:1}, neural differential equations \cite{chen2018neural,onken2021ot}, and more. Our sOED approach is flexible to utilize various measure transport maps. In this work, we employ the \emph{Knothe-Rosenblatt} (KR) rearrangement:
\begin{equation}
	\refparams
	=
	\begin{bmatrix}
		\refparam_1
		\\
		\refparam_2
		\\
		\vdots
		\\
		\refparam_n
	\end{bmatrix}
	=
    \mathcal{F}(\targetparams) 
    =
	\begin{bmatrix*}[l]
		\mathcal{F}_{\targetparam_1}(\targetparam_1)
		\\
		\mathcal{F}_{\targetparam_2 \given \targetparam_1}(\targetparams_{1:2})
		\\
		\quad
		\vdots
		\\
		\mathcal{F}_{\targetparam_n \given \targetparams_{1:n-1}}(\targetparams)
	\end{bmatrix*}
	,
	\label{eq:KR_map_x}
\end{equation}
where $\targetparams_{1:k} = [\targetparam_1,\ldots,\targetparam_k]^\top$. 
%
Each component of the KR map, $\mathcal{F}_{\targetparam_k \given \targetparams_{1:k-1}}(\targetparams_{1:k-1},\targetparam_k)$, is monotone in the last input parameter $\targetparam_k$. 
Given a sample from the reference density, $\refparams \sim \refD$, a sample from the target density, $\targetparams \sim \target$, can be obtained via the inverse map $\cT \coloneqq \mathcal{F}^{-1}$.
The triangular structure of the KR map enables application of $\cT$ via sequential inversion of univariate functions. 
Additionally, the components of the KR map are defined through the marginal conditionals of the target density. Thus, the $k$-th component of $\mathcal{F}$ immediately grants access to the conditional target density $\target(\targetparam_k \given \targetparams_{1:k-1})$.
This feature makes the KR map particularly valuable for Bayesian inference and, as we will detail in~\cref{sec:sOED}, for sOED. 

\smallskip
\paragraph{Tensor-train-based construction} \label{subsec:DIRT} We employ the density approximation strategy presented in~\cite{CuiDolgov:2022:1,CuiDolgovZahm:2023:2,dolgov2020approximation,WestermannZech:2023;1}, particularly the squared tensor train construction of~\cite{CuiDolgov:2022:1}, to numerically implement KR maps. Consider a target density $\target = \frac{\targetUnnorm}{Z}$, where $\targetUnnorm$ is the unnormalized density that can be evaluated and $Z$ is the commonly unknown normalizing constant. We first approximate the square root of the unnormalized density, $p$, using a functional tensor train,  
\begin{equation}
    \sqrt{\targetUnnorm(\targetparams)} \approx \TTCore{1}{\targetparam_1} \cdots \TTCore{i}{\targetparam_i} \cdots \TTCore{n}{\targetparam_n}  \eqqcolon \widehat{f},
    \label{eq:FTT}
\end{equation}
where each $\TTCore{i}{\targetparam_i} \in \mathbb{R}^{{\TTRank_{i-1}}\times{\TTRank_i}}$ is a matrix-valued univariable function, with $\TTRank_{0} = \TTRank_{n} = 1$. 
The elements of each matrix-valued function, $[\TTCore{i}{\targetparam_i}]_{k,j}$ are represented as a linear combination of $M_i$ basis functions. 
Such tensor train approximations can be constructed efficiently using alternating-direction cross interpolation methods; see~\cite{GorodetskyKaramanMarzouk:2019:1,OseledetsTyrtyshnikov:2010:1} for details. 
Here we employ a functional extension of the rank-adaptive alternating minimal energy scheme as described in~\cite[Appendix B]{CuiDolgov:2022:1} to build tensor trains. 
Denoting $M {=} \max_{i} M_i$ and $r {=} \max_{i} \TTRank_i$, it requires $\mathcal{O}(nM\TTRank^2)$ density evaluations to construct such approximations.  
% 
Given the tensor train approximation $\widehat{f}$, the approximation to the normalized target density is defined as 
\begin{equation}
    \widehat{\target}(\targetparams) = \frac{\widehat{p}(\targetparams)}{\xi}, \quad \widehat{p}(\targetparams) = \widehat{f}(\targetparams)^2+\tau\refD(\targetparams), \quad \xi = \int \widehat{p}(\targetparams) \, \mathrm{d}\targetparams, 
\end{equation}
where $\refD(\targetparams) = \prod_{i=1}^n \refD_{i}(\targetparam_i)$ is a product-form reference density such that $\sup_\targetparams \frac{p(\targetparams)}{\refD(\targetparams)} < \infty$ and $\tau > 0$ is a constant. The ``defensive'' term $\tau \refD$ is added to ensure that the tensor train surrogate $\widehat{\target}$ can define importance sampling estimators satisfying the central limit theorem. As shown in~\cite[Theorem 1]{CuiDolgov:2022:1}, choosing tensor ranks $\TTRank_i$ to bound the approximation error by some threshold $\varepsilon$, i.e., $\|\widehat{f} - \sqrt{p}\| < \varepsilon$, and using a constant $\tau < \varepsilon$ ensures that the Hellinger error of the approximate posterior is bounded such that $\distH{\target}{\widehat{\target}} \leq \frac{2\varepsilon}{\sqrt{Z}}$. This, in turn, bounds errors in expectations computed over the approximate posterior.

Leveraging the separable structure of tensor trains, the marginal densities
\begin{equation}
\widehat{\target}_{\targetparams_{1:k}}(\targetparams_{1:k}) = \int_{\targetSpace_{k+1:n}} \widehat{\target}(\targetparams) \,\mathrm{d}\targetparams_{k+1:n}, \quad k = 1,\ldots,n-1,
\end{equation}
of the approximation $\widehat{\target}$ can be computed dimension-by-dimension with $\mathcal{O}(nM\TTRank^3)$ floating point operations. This naturally leads to a KR map, $S(\targetparams)$, with components 
\begin{align*}
    \mathcal{F}_{\targetparam_1}(\targetparam_1) &= \int_{-\infty}^{\targetparam_1} \widehat{\target}_{\targetparam_1}(\targetparam_1')\,\mathrm{d}{\targetparam_1'} \\
    \mathcal{F}_{\targetparam_k\given\targetparams_{1:k-1}}(\targetparam_k) &= \int_{-\infty}^{x_k} \widehat{\target}_{\targetparam_k \given \targetparams_{1:k-1}} \, \mathrm{d}\targetparam_k' = \int_{-\infty}^{x_k} \frac{\widehat{\target}_{\targetparams_{1:k}}(\targetparams_{1:k-1},\targetparam_k')}{\widehat{\target}_{\targetparams_{1:k-1}}(\targetparams_{1:k-1})} \, \mathrm{d}\targetparam_k',
\end{align*}
which defines a coupling of $\widehat{\target}$ with the standard uniform density $\refD_{\text{unif}}$ on the $n$-dimensional unit hypercube $[0,1]^n$, \ie, $\pushforward{\mathcal{F}}\widehat{\target} = \refD_{\text{unif}}$. 
Additionally, the product-form reference density $\refD$ admits a diagonal KR rearrangement, $\mathcal{R} = [\mathcal{R}_{1}(\refparam_1),\ldots,\mathcal{R}_n(\refparam_n)]^\top$ with components $\mathcal{R}_{i}(\refparam_{i}) = \int_{-\infty}^{\refparam_i} \refD_{i}(\refparam_i')\,\mathrm{d}\refparam_i'$, such that $\pushforward{\mathcal{R}}\refD = \refD_{\text{unif}}$. Thus, the composite map $\cT = \mathcal{F}^{-1} \circ \mathcal{R}$ defines a triangular map satisfying $\pushforward{\cT}\refD = \widehat{\target}$. We refer the readers to~\cite{CuiDolgov:2022:1} for technical details.

%
\smallskip
\paragraph{Deep approximation} 
For target densities that are highly concentrated within a small subdomain or exhibit highly nonlinear correlation structures, constructing sufficiently accurate tensor train surrogates may require very large tensor ranks and a large number of basis functions. Since the number of unnormalized density evaluations required is $\mathcal{O}(nM\TTRank^2)$, the efficiency of the algorithm is highly dependent on the maximum rank $\TTRank$ and the number of basis functions $M$, particularly for problems where evaluating the target density involves solving a PDE. The deep approximation approach developed in~\cite{CuiDolgov:2022:1} addresses this challenge by building a composite map, $\cT^{L} = \cQ^1\circ\cQ^2\cdots\circ\cQ^{L}$, where each layer $\cQ^\ell$ is easier to construct. Such a composition is constructed recursively, guiding by a sequence of bridging densities, $\{\target^{\ell}\}_{\ell=1}^{L}$ with $\target^{L} = \target$, that increasingly capture the complexity of the target. 
At layer $\ell$, given the previous composition $\cT^{\ell-1} =\cQ^1\circ\cQ^2\cdots\circ\cQ^{\ell-1}$ such that $(\cT^{\ell-1})_\sharp \refD \approx \target^{\ell-1}$, the pullback density of the next bridging density under $\cT^{\ell-1}$ yields an approximation 
\[
\pullback{(\cT^{\ell-1})}\target^\ell \approx \frac{\target^{\ell} \circ \cT^{\ell-1}}{\target^{\ell-1} \circ \cT^{\ell-1}} \refD,
\]
which is the perturbation of the reference density $\refD$ by the ratio $\frac{\target^{\ell} \circ \cT^{\ell-1}}{\target^{\ell-1} \circ \cT^{\ell-1}}$. This way, for appropriately chosen bridging densities, the pullback density $\pullback{(\cT^{\ell-1})}\target^\ell$ is easier to approximate than the target density itself in tensor train format, and can be represented to sufficient accuracy with lower tensor ranks. We then build the intermediate map $\cQ^{\ell}$ to couple the reference density with the pullback density, $\pushforward{(\cQ^{\ell})}\refD \approx \pullback{(\cT^{\ell-1})}\target^\ell$ and enrich the composition as $\cT^{\ell} = \cT^{\ell-1}\circ\cQ^{\ell}$. We refer the readers to~\cite{CuiDolgov:2022:1} for further details and stability analysis. 

\section{Fast Bayesian sOED and scalable amortized inference}\label{sec:sOED}
In this section, we first extend the upper bound in~\eqref{eq:EIG_bound_full} to bound the iEIG, which in turn we use to guide our sequential design. In addition to sOED, this bound naturally reduces parameter dimensions for the chosen experiment condition. We will integrate the resulting parameter reduction with a conditional KR map to offer scalable amortized inference so that posterior parameter estimation can be issued in real time for any newly observed data. We will also discuss a restart procedure to improve the approximation accuracy of our sequential design-inference procedure. 

\subsection{sOED via an iEIG bound}\label{subsec:iEIG_UB}
At the $k$-th experimental stage, sOED aims to maximize the iEIG~\eqref{eq:EIG_k} between the stage-$k$ prior---that is the previous posterior $\target(\params \given \Hist_{k-1})$---and the stage-$k$ posterior $\target(\params \given \design_k, \data_k,\Hist_{k-1})$. Let ${\cT^{k-1}} {\colon} \mathbb{R}^{\Nm} {\rightarrow} \mathbb{R}^{\Nm}$ denote a measure transport between $\target(\params \given \Hist_{k-1})$ and the standard Gaussian $\refD$ such that $\pullback{({{\cT^{k-1}}})}\target(\refparams \given \Hist_{k-1}) = \refD(\refparams)$. 
Since the KL divergence is invariant to invertible transforms, the iEIG can be rewritten as
\begin{align}
\EIG_k(\design_k) &= \Expect{\data_{k}\given \design_{k},\Hist_{k-1}}{\distKL{\target(\cdot \given \design_k,\data_k,\Hist_{k-1})}{\target(\cdot\given \Hist_{k-1}})} \nonumber \\
&= \Expect{\data_{k}\given \design_{k},\Hist_{k-1}}{\distKL{\pullback{({{\cT^{k-1}}})}\target(\cdot \given \design_k, \data_k,\Hist_{k-1})}{\pullback{({{\cT^{k-1}}})}\target(\cdot \given \Hist_{k-1})}} \\
&= \Expect{\data_{k}\given \design_{k},\Hist_{k-1}}{\distKL{\ratio{k}(\cdot \given \design_k, \data_k)}{\refD(\cdot)}}, \nonumber
\label{eq:iEIG_pullback}
\end{align}
where, using the definition of the pullback operator~\eqref{eq:pullback}, as well as the definitions of ${\cT^{k-1}}$ and $\target(\params \given \design_k,\data_k,\Hist_{k-1})$ in~\eqref{eq:posterior_update}, the stage-$k$ posterior in the transformed coordinates is
\begin{align}
\ratio{k}(\ratioparams \given \design_k,\data_k) = \pullback{({\cT^{k-1}})}\target(\ratioparams \given \design_k, \data_k,\Hist_{k-1}) &= \frac{\refD(\ratioparams)}{\target({\cT^{k-1}}(\ratioparams)\given \Hist_{k-1})}\,\target({\cT^{k-1}}(\ratioparams)\given \design_k,\data_k,\Hist_{k-1}) \nonumber \\
&\propto {\likelihood{\data_k}{{\cT^{k-1}}(\ratioparams),\design_k}\,\refD(\ratioparams)}.
\label{eq:ratioFun}
\end{align}
This implies that $\ratio{k}(\ratioparams \given \design_k,\data_k)$ can be viewed as the posterior density arising from solving the Bayesian inverse problem with a likelihood function $\likelihood{\data_k}{{\cT^{k-1}}(\ratioparams),\design_k}$ and a standard multivariate Gaussian prior.
%
With $\params = {\cT^{k-1}}(\ratioparams)$, the Fisher information matrix $\mathcal{I}(\design_k,\cT^{k-1},\ratioparams)$ for the transformed parameters can be expressed as 
\begin{align}
    \mathcal{I}(\design_k,\cT^{k-1},\ratioparams) &= \hspace{-4pt} \int \left( \nabla_{\ratioparams} \log \likelihood{\data_k}{{\cT^{k-1}}(\ratioparams),\design_k} \,\nabla_{\ratioparams} \log \likelihood{\data_k}{{\cT^{k-1}}(\ratioparams),\design_k}^\top \right) \likelihood{\data}{\params,\design_k}\, \mathrm{d}\data \nonumber \\
    & = \Jac{{\cT^{k-1}}(\ratioparams)}^\top \mathcal{I}(\design_k,\params) \, \Jac{{\cT^{k-1}}(\ratioparams)}
    \label{eq:avg_iFisher}
\end{align}
where
$
\mathcal{I}(\design_k,\params) = \int \nabla_{\params} \log \likelihood{\data_k}{\bvec{m},\design_k} \, \nabla_{\params} \log \likelihood{\data_k}{\params,\design_k}^\top \likelihood{\data}{\bvec{m},\design_k}\, \mathrm{d}\data
$
is the original Fisher information for experimental condition $\design_k$. Thus, we can apply the bound~\eqref{eq:EIG_bound_full} to the transformed posterior $\ratio{k}(\ratioparams \given \design_k,\data_k)$ to bound the iEIG as
% 
\begin{equation}
\EIG_k(\design_k) \leq \EIG_k^\mathrm{UB}(\design_k) := \frac{1}{2} \log \det \left( \I{\Nm} + \Hmat_{I}(\design_k,\cT^{k-1}) \right) , \label{eq:iEIG_uB}
\end{equation}
where $\Hmat_{I}(\design_k,\cT^{k-1}) = \int \mathcal{I}(\design_k,\cT^{k-1},\ratioparams) \, \refD(\ratioparams)\,\mathrm{d}\ratioparams$.

For a range of likelihood functions, the Fisher information matrix $\mathcal{I}(\design_k,\params)$ can be directly evaluated. However, the stage-$k$ average information matrix $\Hmat_{I}(\design_k,\cT^{k-1})$ typically needs to be estimated numerically using Monte Carlo. Using our example of additive Gaussian noise where the likelihood satisfies~\eqref{eq:likelihood}, the stage-$k$ average information matrix takes the form 
\begin{align}
     \Hmat_{I}(\design_k,\cT^{k-1}) &= \int \Jac{{\cT^{k-1}}(\ratioparams)}^\top\Jac\PtO(\design_k,\params)^\top \Cnoiseinv{\design_k}\,\Jac\PtO(\design_k,\params)\,\Jac{{\cT^{k-1}}(\ratioparams)}\refD(\ratioparams)\,\mathrm{d}\ratioparams,
\end{align}
where $\params = {\cT^{k-1}}(\ratioparams)$. Its Monte Carlo integration thus takes the form 
\begin{equation}
    \overline{\Hmat}_{I}(\design_k,\cT^{k-1}) = \frac{1}{N} \sum_{i=1}^N  \Jac{{\cT^{k-1}}(\ratioparams^{(i)})}^\top\Jac\PtO(\design_k,\params^{(i)})^\top\Cnoiseinv{\design_k}\,\Jac\PtO(\design_k,\params^{(i)})\,\Jac{{\cT^{k-1}}(\ratioparams^{(i)})}, \hspace{-.5em}
    \label{eq:MC_Fisher}
\end{equation}
where $\ratioparams^{(i)} \sim \refD$ and $\params^{(i)} = {\cT^{k-1}}(\ratioparams^{(i)})$ for $i = 1,\ldots,N$. 
The Jacobians of the forward map $\Jac\PtO(\design_k,\params^{(i)})$ at any design $\design_k$ and parameter $\params^{(i)}$ can be computed efficiently using adjoint-based methods.
At any given design and sample parameter, constructing the Jacobian explicitly requires one full (potentially nonlinear) PDE solve and $\Nd$ linear adjoint solves. However, for Jacobians with fast singular value decays, the number of adjoint solves can be potentially reduced using iterative and/or randomized methods. 

The upper bound~\eqref{eq:iEIG_uB} serves as a guide for approximating sequentially optimal designs. We propose choosing a design that maximizes the upper bound on the approximated iEIG, 
\begin{equation}
\design_k^* \in \Argmax_{\design_k} \overline{\EIG}_k^{\mathrm{UB}}(\design_k) := \Argmax_{\design_k} \frac{1}{2}\log \det \left( \I{\Nm} +  \overline{\Hmat}_{I}(\design_k,\cT^{k-1}) \right)  . 
\label{eq:iUB}
\end{equation} 
For general designs, this noisy objective function can be maximized using methods such as stochastic gradient ascent. For this paper, we focus on OED problems where the design consists of selecting an experimental condition from a candidate set $\designSpace_k = \{\bvec{E}_1,\ldots,\bvec{E}_{\Ne}\}$ with cardinality $\Ne$. 
In this context, it is convenient to redefine the forward map $\mathcal{G}\colon\mathbb{R}^{\Nm} \rightarrow \mathbb{R}^{\Ne\Nd}$ as a map from the unknown parameters to the observables on \emph{all} the candidate designs, \ie, 
\begin{equation}
\mathcal{G}(\params) = [\mathcal{G}(\bvec{E}_1,\params);\mathcal{G}(\bvec{E}_2,\params);\ldots;\mathcal{G}(\bvec{E}_{\Ne},\params)]^\top,
\label{eq:fullPtO}
\end{equation}
with $\mathcal{G}(\bvec{E}_j,\params) \in \mathbb{R}^{\Nd}$ for all $j = 1,\ldots,\Ne$. 
The observables corresponding to any design $\design_k \in \designSpace_k$ can be obtained using a row selection matrix $\bvec{W}(\design_k)$. 
Specifically, let $\design_k = \bvec{E}_{j}$, then $\bvec{W}(\design_k) = \unitvec^{\Ne}_j \otimes \I{\Nd}$ where $\unitvec^{\Ne}_j \in \{0,1\}^{1\times\Ne}$ is a row vector corresponding to the $j$-th canonical basis, i.e., its only non-zero entry is the $j$-th element, $\I{\Nd}$ is the $\Nd$-dimensional identity matrix, and $\otimes$ is the Kronecker product. 
With this notation, it is clear that $\PtO(\design_k,\params) = \bvec{W}(\design_k)\,\PtO(\params)$ at any design $\design_k$ and its Jacobian is $\Jac \PtO(\design_k,\params) = \bvec{W}(\design_k)\,\Jac \PtO(\params)$, where $\Jac \PtO(\params) \in \mathbb{R}^{\Ne\Nd \times \Nm}$. The average information matrix in~\eqref{eq:MC_Fisher}  then takes the form
\begin{equation}
\overline{\Hmat}_{I}(\design_k,\cT^{k-1}) = \frac{1}{N} \sum_{i=1}^N  \mathcal{J}({\cT^{k-1}},\ratioparams^{(i)})^\top \bvec{W}(\design_k)^\top \Cnoiseinv{\design_k}\bvec{W}(\design_k) \,\mathcal{J}({\cT^{k-1}},\ratioparams^{(i)}).
    \label{eq:MC_Fisher_design}
\end{equation}
where $\mathcal{J}({\cT^{k-1}},\ratioparams) := \Jac\PtO(\params)\Jac{{\cT^{k-1}}(\ratioparams)}$ with $\params = {\cT^{k-1}}(\ratioparams)$ is the Jacobian with respect to the transformed parameter $\ratioparams$.

In the above fixed candidate design case, at each stage $k$, we can pre-compute the Jacobian $\mathcal{J}({\cT^{k-1}},\ratioparams^{(i)})$ at a set of $N$ samples $\ratioparams^{(i)} \sim \refD$. Then, the upper bound $\overline{\EIG}_k^{\mathrm{UB}}(\design_k)$ can be evaluated at each candidate design $\design_k \in \designSpace_k$ by extracting the relevant rows of $\mathcal{J}$ using the selection matrix $\bvec{W}(\design_k)$, without additional PDE and adjoint solves.  
Maximizing $\overline{\EIG}_k^{\mathrm{UB}}$ then involves choosing a candidate design $\design^*$ that results in the largest value. As a by-product, the eigendecomposition of the average information matrix at the optimal design, $\overline{\Hmat}_{I}(\design^*,\cT^{k-1})$, naturally results in a basis of the data-free LIS, as detailed in~\cref{subsec:LIS}. The procedure is outlined in~\cref{Alg:DLIS}. 
%
In later sections, we use the resulting LIS to construct scalable conditional measure transport, enabling amortized inference and providing the required transport map ${\cT^{k-1}}$ for finding optimal designs in the new stage. 

\begin{algorithm}[h]
\caption{Optimization of the iEIG upper bound~\eqref{eq:iUB}, as well as construction of the data-free LIS basis, $\LISDF$, at the optimal design $\design^*$. Inputs are a transport map $\cT$, a forward map $\mathcal{G}$, the noise covariance $\Cnoise$, truncation tolerance $\epsilon_I$, and a Monte Carlo sample size $N$. \label{Alg:DLIS}}
\begin{algorithmic}[1]
\Function{iEIGUB}{$\cT,\mathcal{G},\Cnoise,\epsilon_I,N$}
\State Draw samples $\ratioparams^{(i)} \sim \refD$ and compute $\mathcal{J}(\cT,\ratioparams^{(i)})$ for $i=1,\ldots,N$
%
\State $\design^* \gets \Argmax_{\design \in \designSpace} \frac{1}{2}\log\det(\I{\Nm}+\overline{\Hmat}_I(\design,\cT))$, where $\overline{\Hmat}_I(\design,\cT)$ is given in~\eqref{eq:MC_Fisher_design}
\State Compute eigendecomposition $\overline{\Hmat}_{I}(\design^*,\cT) = \LISDF \bvecS{\Lambda} \LISDF^\top$
\State Set $\LISDF = \LISDF(:,1:\LISdim)$, with $\LISdim$ chosen to ensure $\frac{1}{2}\sum_{i=\LISdim+1}^{\Nm}\log(1+\lambda_i(\overline{\Hmat}_I(\design^*,\cT))) \leq \mathrm{\epsilon_I}$
\State \Return $\design^*,\LISDF$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Scalable amortized inference}\label{subsec:sOED_composite}
The goal of amortized inference is the construction of a conditional map, $\cS^k_{\params \given \data} \colon\mathbb{R}^{\Nd} \times \mathbb{R}^{\Nm} \rightarrow \mathbb{R}^{\Nm}$, which approximately couples the stage-$k$ posterior to some reference density for any observed data. Importantly, such a conditional map can be built while experiments are conducted, prior to data collection. Once the experiment is complete and the data is collected, this map enables \emph{rapid online approximation} of the posterior $\target(\params \given \Hist_{k})$. Here we choose a standard Gaussian reference density so that the conditional map $\cS^k_{\params \given \data}$ can also be used to construct the iEIG bound in the next experimental stage. We combine the data-free LIS and tensor trains to construct the conditional map recursively. 
 
\smallskip
\paragraph{Conditional map} At the start, we assume that an initial (non-conditional) measure transport $\cT^{0} \colon\mathbb{R}^{\Nm} \rightarrow \mathbb{R}^{\Nm}$ is given to couple the prior distribution $\target(\params)$ with a standard multivariate Gaussian, $\refD(\ratioparams)$. If the prior is a multivariate Gaussian, $\target(\params) = \mathcal{N}(\params_0,\Cpr)$, then we have a linear map $\cT^{0}(\cdot) = \params_0+\Cpr^{1/2}(\cdot)$. At stage $k$, suppose we have a measure transport $\cT^{k-1}$---which can be either the previous conditional map or other forms of transport maps---satisfying $\pushforward{{(\cT^{k-1})}} \refD(\params) = \widehat{\target}(\params\given \Hist_{k-1}) \approx \target(\params \given \Hist_{k-1})$, where $\widehat{\target}$ is the pushforward density that approximates the target $\target$. 
%
Once the experimental condition $\design_k^*$ is chosen as in~\cref{Alg:DLIS}, we build the conditional map under the deep approximation framework of~\cite{CuiDolgov:2022:1} (cf.~\cref{subsec:DIRT}) as follows. 
%
\begin{enumerate}[leftmargin=19pt]
    \item Define a product-form joint reference density $\refD(\refparams_y,\refparams_m) = \refD(\refparams_y)\refD(\refparams_m)$ and a \emph{joint precondition map}, 
    $\cS^{k-1}_{\data,\params}= [\refparams_y,{\cT^{k-1}}(\refparams_m)]^\top$, 
    such that $\pushforward{(\cS^{k-1})}\refD(\data,\params) = \refD(\data)\,\widehat{\target}(\params \given \Hist_{k-1})$.
    %
    \item Pull back the stage-$k$ joint density for the data and parameter random variables, 
\begin{equation}
\target(\data_k,\params\given \design_k^*,\Hist_{k-1}) = \likelihood{\data_k}{\params,\design_k^*}\,\target(\params\given\Hist_{k-1}),
\label{eq:joint_ratio}
\end{equation}
under the preconditioning map, $\cS^{k-1}_{\data,\params}$, which yields 
\begin{align}
\pullback{({\cS^{k-1}_{\data,\params}})}\target(\ratioparams_y,\ratioparams_{m}\given \design_k^*,\Hist_{k-1}) &= \frac{\likelihood{\ratioparams_y}{\cT^{k-1}(\ratioparams_m),\design_k^*}\,\target(\cT^{k-1}(\ratioparams_m)\given \Hist_{k-1})}{\refD(\ratioparams_y)\,\widehat{\target}(\cT^{k-1}(\ratioparams_m)\given \Hist_{k-1})}\refD(\ratioparams_y,\ratioparams_m) \nonumber \\
&\approx \likelihood{\ratioparams_y}{\cT^{k-1}(\ratioparams_m),\design_k^*}\,\refD(\ratioparams_m) \eqqcolon q^k_{\data,\params}(\ratioparams_y,\ratioparams_m). \label{eq:qMap}
\end{align}
    %
\item Approximate the joint density $q^k_{\data,\params}(\ratioparams_y,\ratioparams_m)$ using the squared tensor train as in~\cref{subsec:DIRT} with sufficiently large tensor train ranks to obtain a lower-triangular \emph{incremental joint map} 
\begin{equation}
\mathcal{Q}_{\data,\params}^k (\refparams_y,\refparams_m) = \begin{bmatrix*}[l]
		\mathcal{Q}^k_{\data}(\refparams_y)\vspace{2pt}
		\\
		\mathcal{Q}^k_{\params \given \data}(\refparams_y, \refparams_m)
	\end{bmatrix*}
    = 
    \begin{bmatrix*}[l]
		\ratioparams_y
		\\
	   \ratioparams_m
	\end{bmatrix*},\label{eq:incrementalMap}
\end{equation}
that approximately pushes forward the reference density $\refD(\refparams_y,\refparams_m)$ to $q^k_{\data,\params}(\ratioparams_y,\ratioparams_m)$.
%
\item The composite map, $\cS^k_{\data,\params} = \cS^{k-1}_{\data,\params} \circ \cQ^{k}_{\data,\params} $, which also takes a lower triangular form of 
\[
\cS_{\data,\params}^k (\refparams_y,\refparams_m) = \begin{bmatrix*}[l]
		\;\cQ^k_{\data}(\refparams_y)\vspace{2pt}
		\\
		(\cT^{k-1} \circ \cQ^k_{\params \given \data})(\refparams_y, \refparams_m)
	\end{bmatrix*}
    = 
    \begin{bmatrix*}[l]
		\data_k
		\\
	    \params
	\end{bmatrix*},\label{eq:jointMap}
\]
approximates the joint density $\target(\data_k,\params\given \design_k^*,\Hist_{k-1})$. The resulting \emph{conditional map}
\begin{equation}
\cS^{k}_{\params \given \data}(\data_k, \refparams_m) = \left(\cT^{k-1} \circ \cQ^k_{\params \given \data}\right) \left((\cQ^k_{\data})^{-1}(\data_k), \refparams_m\right), 
    \label{eq:conditionalMap}
\end{equation}
approximately pushes forward the reference density $\refD(\refparams_m)$ to the  conditional marginal density $\target(\params\given \data_k,\design_k^*,\Hist_{k-1})$, which is the posterior, for any data $\data_k$ observed at stage $k$.
\end{enumerate}
%
%
Given the observed data $\data_k$, we define $\cT^{k}(\refparams_m) := \cS^{k}_{\params \given \data}(\data_k, \refparams_m)$, which serves as an input to~\cref{Alg:DLIS} for finding optimal experimental conditions for the next stage. This approach enables the design of an algorithm that shifts the majority of the computational workload to the offline phase. While experiments are conducted, the new joint distribution can be sequentially approximated as described above.

A straightforward approach to constructing $\cQ^{k}_{\data,\params}$ in the above procedure is to use the deep approximation approach in~\cref{subsec:DIRT}.
However, as mentioned previously, the number of unnormalized density evaluations required by this method scales linearly with the combined dimension of the data and parameters and quadratically with the tensor train ranks, which may increase with the total dimension. Therefore, we propose to combine the dimension reduction offered by LIS with DIRT to build a dimension-robust measure transport. 

\smallskip
\paragraph{Subspace accelerated conditional map} 

Let $\LISDF_{k}$ be the $\LISdim_k$-dimensional matrix corresponding to the leading eigenvectors of the average Fisher information matrix $\overline{\Hmat}_k(\design_k^*,{\cT^{k-1}})$ (see~\cref{Alg:DLIS}) containing the basis for the data-free LIS. 
The range of $\LISDF_k$ corresponds to directions in which the transformed parameters $\ratioparams_m$ are most informed by the likelihood $\likelihood{\ratioparams_y}{\cT^{k-1}(\ratioparams_m),\design_k^*}$, relative to the reference $\refD$.
We decompose the parameter $\ratioparams_m = \ratioparams^{r_k}_{m}+\ratioparams_m^{\perp_k}$ into its likelihood-informed components ($\ratioparams^{r_k}_{m})$ and uninformed components ($\ratioparams_m^{\perp_k}$).
Following~\eqref{eq:li_posterior}, the joint density $q^k_{\data,\params}(\ratioparams_y,\ratioparams_m)$ can then be approximated as  
\begin{equation}
\begin{split}
q^k_{\data,\params}(\ratioparams_y,\ratioparams_m) \approx \widetilde{q}^k_{\data,\params}(\ratioparams_y,\ratioparams_m;\Proj_{k}) 
&= \underbrace{\Rlikelihood{\ratioparams_y}{\mathcal{T}^{k-1}(\ratioparams_m^{r_k}),\design_k^*}\,\refD(\ratioparams_m^{r_k})}_{\widetilde{q}^k_{\data,\params}(\ratioparams_y,\ratioparams_m^{r_k})}\,\refD(\ratioparams_m^{\perp_k}\given \ratioparams_m^{r_k}).
\end{split}
\end{equation}
The dimension $\LISdim_k$ for sufficiently accurate approximation of the ratio density $q^k_{\data,\params}$ is chosen based on the upper bound on the expected value of the KL divergence~\eqref{eq:EIG_bound}. 
Given a tolerance $\epsilon_I$, we choose $\LISdim_k$ to be the smallest integer $\LISdim$ such that 
\begin{equation}
\frac{1}{2}\sum_{j=\LISdim+1}^{\Nm} \log\left( 1 + \lambda_j\left(\overline{\Hmat}_I(\design_k^*,\mathcal{T}^{k-1})\right)\right) \leq \epsilon_I. 
\end{equation}
%
Once the data-free LIS is determined, let $\widetilde{\ratioparams}_m^k = \LISDF_k^\top\ratioparams_m \in \mathbb{R}^{\LISdim_k}$ be the coefficient associated with the LIS basis. Then one can approximate the reduced-dimensional joint density, $\widetilde{q}^k_{\data,\params}(\ratioparams_y,\ratioparams_m^{r_k}) \equiv \widetilde{q}^k_{\data,\params}(\ratioparams_y,\widetilde{\ratioparams}_m^k)$, to build a smaller KR map $\widetilde{\cQ}^k_{\data,\params}:\mathbb{R}^{\Nd+\LISdim_{k}} \rightarrow \mathbb{R}^{\Nd+\LISdim_k}$,
\[
\widetilde{\cQ}^k_{\data,\params}(\refparams_y,\refparams_m^k) = [\widetilde{\cQ}^k_{\data}(\refparams_y), \widetilde{\cQ}^k_{{\params}\given \data}(\refparams_y,\refparams_m^k)]^\top, \quad \refparams_m^k \in \mathbb{R}^{\LISdim_k}
\]
such that 
\begin{equation}
    \pushforward{({\widetilde{\cQ}^k_{\data,\params}})}\refD(\ratioparams_y,\widetilde{\ratioparams}_{m}^k) \approx \widetilde{q}^k_{\data,\params}(\ratioparams_y,\widetilde{\ratioparams}_{m}^k). 
    \label{eq:reducedQmap}
\end{equation}
Embedding the map $\widetilde{\cQ}^k_{\data,\params}$ into a linear map defined using $\LISDF_k$, we obtain the incremental map on the full space
\begin{align}
    \cQ^{k}_{\data,\params}(\refparams_y,\refparams_m) &= \begin{bmatrix*}[l]
		\widetilde{\cQ}^{k}_{\data}(\refparams_y) \vspace{2pt}
		\\
		{\cQ}^{k}_{\params \given \data}(\refparams_y,{\refparams_m})
	\end{bmatrix*},
    \label{eq:ratio_fullSpace}
\end{align}
where
\(
    {\cQ}^{k}_{\params \given \data}(\refparams_y,{\refparams_m}) = \LISDF_{k}\widetilde{\cQ}^{k}_{\params \given \data}(\refparams_y,\LISDF_{k}^\top\refparams_m)+(\I{\Nm}-\LISDF_{k}\LISDF_{k}^\top)\refparams_m. 
\)

%

\subsection{Overall algorithm with restart}\label{subsec:sOED_restart}

The method described so far can be effective for sOED problems with a small number of experimental stages.
However, several practical challenges arise when dealing with problems involving a large number of experiments. One such challenge is the incremental accumulation of error due to the recursive approximation, which can make it computationally expensive to maintain a sufficiently small approximation error across the experimental stages.
This challenge is compounded by the use of conditional transport maps, as they can only probabilistically guarantee sufficiently accurate approximation to the posterior~\cite[Appendix A.2]{CuiDolgovZahm:2023:1}. 
Additionally, relying solely on the data-free LIS approach may lead to an overestimation of the number of important directions. The continuous addition of layers to the composite map approximating the posterior increases the computational cost of sampling and of evaluating the Jacobian $\Jac {\cT^k}$ at each experimental stage. 

To address these issues, we incorporate a ``restart'' step into the conditional map construction. Specifically, every $\ell$ steps---after collecting data from previous experiments but before designing the experimental conditions for the next stage---we discard the previous conditional map $\cS^\ell_{\params\given\data}$ and rebuild a map $\cT^\ell$ from scratch by targeting the stage-$\ell$ posterior directly. This process produces a more accurate approximation of the current posterior. To ensure scalability, the approximation begins by constructing the data-dependent LIS, followed by the reduced-dimensional posterior approximation. Specifically, we define an importance sampling estimator using the previous conditional map $\cS^\ell_{\params\given\data}$ to compute the Gram matrix~\eqref{eq:gram} for building a data-dependent LIS. This procedure is summarized in~\Cref{Alg:DTLIS}.
 

\begin{algorithm}[h]
\caption{Construction of the data-dependent LIS ($\LISDD$) for a posterior $\target(\params \given \data) \propto p(\params\given\data) = \likelihood{\data}{\params}\refD(\params)$. Inputs are a transport map $\cS_{\params \given \data}$ such that $\pushforward{({\cS_{\params \given \data}})}\refD(\params) = \hat{\target}(\params \given \data) \approx \target(\params \given \data)$, a truncation tolerance $\epsilon_G$, and a sample sizes $N$. \label{Alg:DTLIS}}
\begin{algorithmic}[1]
\Function{DDLIS}{$\cS_{\params \given \data},\epsilon_G,N$}
        \State Draw sample $\refparams^{(i)}\sim \rho$ and evaluate $\params^{(i)} = {\cS_{\params \given \data}}(\refparams^{(i)})$ for $i = 1, \ldots, N$
        \State Compute the Gram matrix using self-normalized importance sampling \vspace{-12pt}$$\overline{\Hmat}_{G} = \textstyle \frac{1}{\sum_{i=1} W_i}\sum_{i=1}^N W_{i}\nabla\log\likelihood{\data}{\params^{(i)}}^\top\nabla\log\likelihood{\data}{\params^{(i)}}, \quad W_{i} = \frac{p(\params^{(i)}|\data)}{\hat{\target}(\params^{(i)}|\data)} \vspace{-12pt}$$
        \State Compute eigendecomposition $\overline{\Hmat}_{G} = \LISDD \bvecS{\Lambda} \LISDD^\top$
        \State Set $\LISDD = \LISDD(:,1:\LISdim)$, with $\LISdim$ chosen using~\eqref{eq:DI_bound} to ensure $\frac{\sqrt{\kappa}}{2}\sqrt{\sum_{k=r+1}^{\Nm}\lambda_k(\overline{\Hmat}_G))} \leq \epsilon_G$ 
        \State \Return $\LISDD$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

In each restart, the procedure for constructing the transport map with the data-dependent LIS closely follows the subspace acceleration approach described in the previous subsection. 
The key differences are that the LIS dimension is determined by the data-dependent bound~\eqref{eq:DI_bound} and a non-conditional transport map is built by directly approximating the fixed posterior. 
Assuming the stage $\ell$ requires a restart---\ie, an accurate approximation to the stage $\ell-1$ posterior is needed---the transport map using the restart is outlined as follows. 
%
Let $\LISDD_{\ell-1} \in \mathbb{R}^{\Nm \times \LISdim_{\ell-1}}$ denote a basis of the data-dependent LIS for the current posterior $\target(\params \given \Hist_{\ell-1})$, computed using~\cref{Alg:DTLIS}. The parameter $\params$ can be decomposed as $\params = \params^{\LISdim_{\ell-1}} + \params^{\perp_{\ell-1}}$. Let $\widetilde{\params}^{\ell-1} \in \mathbb{R}^{\LISdim_{\ell-1}}$ be the coefficients of the reduced parameter $\params^{\LISdim_{\ell-1}}$ associated with $\LISDD_{\ell-1}$. One can first construct a lower-dimensional map $\widetilde{\cT}^{\ell-1}\colon\mathbb{R}^{\LISdim_{\ell-1}} \rightarrow \mathbb{R}^{\LISdim_{\ell-1}}$ satisfying $\pushforward{(\widetilde{\cT}^{\ell-1})}\refD(\widetilde{\params}^{\ell-1}) \approx \widetilde{\target}(\LISDD_{\ell-1}\widetilde{\params}^{\ell-1} \given \Hist_{\ell-1})$, with the reduced-dimensional posterior $\widetilde{\target}(\cdot \given \Hist_{\ell-1})$ defined in~\eqref{eq:li_posterior}. Then, embedding the smaller map $\widetilde{\cT}^{\ell-1}$ into a linear map as in~\cref{subsec:sOED_composite}, we obtain the full-dimensional transport map 
\begin{equation}
\cT^{\ell-1}(\refparams) = \LISDD_{\ell-1}^{}\widetilde{\cT}^{\ell-1}(\LISDD_{\ell-1}^\top\refparams)+\left(\I{\Nm}-\LISDD_{\ell-1}^{}\LISDD_{\ell-1}^\top\right)\refparams, \quad \refparams \in \mathbb{R}^{\Nm},\label{eq:restartMap}
\end{equation}
which approximately couples $\refD = \mathcal{N}(\bvec{0},\I{\Nm})$ and ${\target}(\params \given \Hist_{\ell-1})$. The new map $\cT^{\ell-1}$ can then be used in~\cref{Alg:DLIS} for finding new optimal designs and building the data-free LIS for the amortized inference in the new experimental stage. The final algorithm we use for our numerical experiments is presented in~\cref{Alg:sOED_wRestart}. 

\begin{algorithm}[h!]
	\caption{Greedy sOED with restart. Inputs are the number of experimental stages $K$, error tolerances $\epsilon_G$ and $\epsilon_I$, a forward map $\PtO(\design,\params)$, an observation noise model $\noise \sim \mathcal{N}(\bvec{0},\Cnoise)$, and a sample size $N$. Without loss of generality, we set the initial prior $\refD \sim \mathcal{N}(\bvec{0},\I{\Nm})$.}
	\label{Alg:sOED_wRestart}
	\begin{algorithmic}[1]
		\State Initialize $\cT^0 = \mathcal{I}$
		\For{k=1, \ldots, K}
        \If{k $>$ 1}
        \State Get new data $\data_{k-1}$ and define $\cT^{k-1}(\cdot) = \cS^{k-1}_{\params \given \data}(\data_{k-1}, \cdot)$ for posterior estimation
        \If{restart}
        \State $\LISDD_{k-1} \leftarrow \mathrm{DDLIS}(\cS^{k-1}_{\params \given \data},\epsilon_G,N)$
        \State Build the reduced map $\widetilde{\cT}_{k-1}$ such that $\pushforward{({\widetilde{\cT}_{k-1}})}\refD(\widetilde{\params}^{k-1}) \approx \widetilde{\target}(\LISDD_{k-1}\widetilde{\params}^{k-1} \given \Hist_{k-1})$
        \State Build the full-dimensional map ${\cT^{k-1}}(\cdot)$ as in~\eqref{eq:restartMap} for posterior estimation
        \EndIf
        \EndIf
        \State $\design_k^*,\LISDF_{k} \leftarrow \mathrm{iEIGUB}({\cT^{k-1}},\mathcal{G},\Cnoise,\epsilon_I,N)$
        \State  Define the joint preconditioning map $\cS^{k-1}_{\data,\params}= [\refparams_y,{\cT^{k-1}}(\refparams_m)]^\top$
        \State Build the incremental joint map $\cQ^k_{\data,\params}$ as in~\eqref{eq:incrementalMap} 
        \State Define $\cS_{\data,\params}^{k} = \cS^{k-1}_{\data,\params} \circ \cQ^{k}_{\data,\params}$ and extract the new conditional map $\cS^{k}_{\params \given \data}$ as in~\eqref{eq:conditionalMap}
		\EndFor
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

The additional computational effort of the restart step is justified by the improved accuracy of posterior approximations. This, in turn, can enhance the stability of the approximations and potentially lower the tensor ranks in subsequent experimental stages. Furthermore, the computational cost of constructing transport maps using tensor train decompositions can be significantly reduced by utilizing surrogate models for the forward map. These surrogates can be built using neural networks or, as demonstrated in our numerical experiments, through proper orthogonal decomposition and the discrete empirical interpolation method~\cite{ChaturantabutSorensen:2010:1}.

\section{Numerical results} Here we demonstrate the effectiveness of designs
that maximize our proposed upper bound for two model problems.

\subsection{Problem 1: Optimal sensor selection for diffusivity field estimation}\label{sec:num1}
%
We first consider an inverse problem governed by an elliptic PDE defined in $\Omega = [0,1]^2$ with left, right, top and bottom boundaries denoted as $\Gamma_L, \Gamma_R, \Gamma_T, \Gamma_B$, respectively: 
%
\begin{align*}
-\nabla \cdot \left(\kappa(m) \nabla u \right) &= 0 && \hspace{-6em} \text{ in } \Omega, \\
\kappa(m)\nabla u \cdot n &= 0  && \hspace{-6em}  \text{ on } \Gamma_T \cup \Gamma_B, \\
u &= 1+y/2 && \hspace{-6em}  \text{ on } \Gamma_L, \\
u &= -\sin(2\target y)-1 && \hspace{-6em}  \text{ on } \Gamma_R.
\end{align*}
%
This PDE is commonly used to model the flow of a fluid through a porous medium. The inverse problem we consider is the inference of the log-diffusivity field $m = \log(\kappa)$ given measurements of the pressure $u$ at a finite set of locations in $\Omega$.
We assume $m$ follows a Gaussian prior, $m \sim \mathcal{N}(0,C_{\text{pr}})$ with $C_{\text{pr}}$ obtained via the covariance kernel $\exp(-\frac{1}{2\ell^2} \norm{}{x-z}^{2}),  $ and $\ell = \frac{1}{\sqrt{50}}$. 

For the sequential design problem, we fix $\Nd = 121$ equally-spaced candidate locations for measuring the pressure $u$ and assume the measurement at each candidate location is corrupted by independent mean-zero Gaussian noise with standard deviation $0.2$. In each experiment, we choose one location from this set that maximizes the incremental EIG bound~\eqref{eq:EIG_k}. The true diffusivity field used to synthesize the data at each experimental stage, as well as the corresponding PDE solution $u$ and the candidate sensor locations are visualized in~\cref{fig:Darcy_setup}.   
\begin{figure}
\vspace{-24pt}
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (a) at (0,0)
{\includegraphics[width=0.7\textwidth, trim={0 5em 0 5em}, clip]{Graphics/Poisson_setup.pdf}};
\node at (-3.3,2.1) {{$\kappa_{\text{true}}$}};
\node at (2.8,2.1) {{$u(\kappa_{\text{true}})$}};
\label{fig:Darcy_setup}
\end{tikzpicture}\vspace{-15pt}
\caption{The ``true'' diffusivity field used to synthesize data (left) and the corresponding pressure field $u$ (right). The $\Nd = 121$ candidate locations for the design problem are visualized as black dots in the right figure.}
\end{figure}

After finite element discretization with second-order Lagrange elements on a triangular mesh of size $h = \frac{1}{32}$ in each coordinate direction, the unknown log-diffusivity field is characterized by a vector of nodal coefficients $\bvec{m} \in \mathbb{R}^{\Nm}$ with $\Nm = 4225$. The forward solves as well as the adjoint-based construction of the Jacobians needed for the upper bound on the iEIG~\eqref{eq:iEIG_pullback} are performed in \texttt{FastFins}~\cite{Cui:2022}. 

\subsection*{Optimal designs and comparisons}
We use eight design stages, each selecting an optimal location for measuring $u$ using algorithm~\cref{Alg:sOED_wRestart} with $N=100$ samples and truncation tolerances of $\epsilon_G = 0.01, \epsilon_I = 0.02$, and transport maps constructed using the \texttt{deep-transport} package~\cite{Cui:2023}. 
To speed up the computation of the tensor train density approximations, we build a reduced-order model (ROM) restricted to the data-free LIS at all the candidate sensor locations.
Specifically, we use the discrete empirical interpolation approach combined with proper orthogonal decomposition as in~\cite{ChaturantabutSorensen:2010:1}, with samples drawn from the LIS to create the snapshots.
The LIS at all the candidate designs that satisfies~\eqref{eq:EIG_bound} with a more conservative tolerance of $0.01$ has dimension $71$, and solving the resulting ROM is approximately 40 times faster than solving the full model. 
The computational cost of choosing the optimal designs and approximating the posteriors, along with the approximation errors, is presented in~\cref{table:cost_darcy}. 

Direct comparisons between our approach and existing methods in the sOED literature are not straightforward.
To evaluate the effectiveness and accuracy of our approach, we adapt two widely used batch OED methods to the sequential setting. 
Specifically, we compare our method against a nested Monte Carlo estimator of the iEIG (as detailed in~\cref{appendix:NMC}), which is computationally expensive but serves as the gold standard for nonlinear Bayesian inverse problems.
Additionally, we compare against a method based on Gaussian approximations proposed in~\cite{WuChenGhattas:2023:1} (detailed in~\cref{appendix:Laplace}), which is more efficient but may be inaccurate for highly non-Gaussian posteriors.

We begin by assessing the quality of the upper bound. At each experimental stage $k$, we evaluate the upper bound~\eqref{eq:iEIG_uB} and a nested Monte Carlo estimator of the iEIG at each candidate location, which serves as the reference. . 
The reference nested Monte Carlo approximation (using $N=\num{10000}$ samples) to the incremental EIG and the upper bound at the first four experimental stages is visualized in the top two rows of~\cref{fig:Darcy_iEIG_comp}. From the figure, we observe that using 100 samples appears sufficient to capture the variation in the expected information gain. For this example, the upper bound appears quite sharp and the contours of the iEIG align closely. Most importantly, the optimal designs, beyond the first stage, agree between the two methods. In the first stage, while the nested Monte Carlo estimator of the EIG identifies a maximum at a different location than the upper bound, the region around this maximum appears relatively flat, suggesting that many designs may perform equally well. 

In~\cref{fig:Darcy_iEIG_compNMC}, we further evaluate the effectiveness of using the upper bound to select sequential optimal designs. Specifically, at each stage $k$, we fix the stage-$k$ prior to be the posterior derived from the previously chosen optimal designs and corresponding data. We then evaluate the iEIG using nested Monte Carlo with \num{10000} samples for the design that maximizes the upper bound on the iEIG, as well as for 50 randomly selected designs. The designs that maximize the upper bound outperform the randomly selected designs almost all the time. As more sensors are selected, the performance gap naturally decreases and is expected to shrink further with additional experiments as information accumulates.

Additionally, we compare our approach with an extension of the method based on Gaussian approximations proposed in~\cite{WuChenGhattas:2023:1}. 
Specifically, in this approach, the expected value of the KL divergence is approximated via a Monte Carlo average, where both the stage-$k$ posterior in the numerator and the stage-$k$ prior in the denominator are replaced with Gaussian approximations. For further details, see~\cref{appendix:Laplace}.
 A visualization of the incremental EIG obtained using this method for the first four stages is shown in~\cref{fig:Darcy_iEIG_comp}. For the first three stages, the optimal designs agree with those obtained using our approach, likely due to the posterior being well-approximated by a Gaussian.
 However, starting at the fourth design, the approaches begin to diverge. This divergence is further illustrated in~\cref{fig:Darcy_iEIG_compNMC}, where we evaluated the KL-divergence between the stage-$k$ posterior and prior at
both sets of designs.
 The KL divergence between the posterior $\target(\params \given \Hist_{k})$ and prior $\target(\params)$ given a history vector $\Hist_{k}$ is approximated using importance sampling as
 \begin{equation}
     \distKL{\target(\cdot \given \Hist_{k})}{\target(\cdot)} \approx \frac{1}{N} \sum_{i=1}^{N} w_i \log\left[\frac{\widehat{\target}(\params^{(i)} \given \Hist_{k})}{\target(\params^{(i)}}\right], 
 \end{equation}
 where $\widehat{\target}(\params \given \Hist_{k}) \approx \target(\params \given \Hist_{k})$ and is obtained using a subspace-accelerated KR-rearrangement as described in~\cref{subsec:sOED_restart}. 
 The sample parameters $\params^{(i)} \sim \widehat{\target}(\params \given \Hist_{k})$ for $i=1,\ldots,N$ with $N=2000$.
 In this example, the KL-divergence for the first three designs is identical up to numerical error introduced by the sample-based Monte Carlo approximation. However, after the third design, a gap emerges between the two approaches.
 These results suggest that a hybrid approach may be effective for certain problems --- using the Gaussian-based method for the initial stages while the posterior is close to the Gaussian prior, and switching to a transport-map-based approach once the posterior begins to deviate significantly from the prior. 
 Further exploration of this approach is reserved for future work. 
 
 
\begin{figure}
\vspace{-15pt}
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (a) at (0,0)
{\includegraphics[width=0.93\textwidth, trim = {0 2em 0 3em}, clip]{Graphics/uB_vs_NMC_vs_Laplace.pdf}};
\node at (-7.7,3.0) {{${\Phi}_{\text{UB}}$}};
\node at (-7.7,0) {{${\Phi}_{\text{MC}}$}};
\node at (-7.7,-3.0) {{${\Phi}_{\text{L}}$}};
\label{fig:Darcy_iEIG_comp}
\end{tikzpicture}\vspace{-10pt}
\caption{The upper bound on the incremental EIG for stages $k = 1,2,3,4$ (top row, from left to right) compared with a nested Monte Carlo estimator (middle row) and a linearization-based estimator (bottom row). The upper bound was computed using~\cref{Alg:DLIS} with $N = 100$ samples to approximate $\Hmat_{I}^k$ at each stage. The nested Monte Carlo estimates were computed using $N = \num{10000}$ samples from the joint $\likelihood{\data_k}{\params}\,\widehat{\target}(\params \given \Hist_{k-1})$.}
%
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (a) at (-3,0)
{\includegraphics[width=0.4\textwidth, trim = {0 1em 0 2em }, clip]{Graphics/uB_vs_Rand.pdf}};
\node[inner sep=0pt] (b) at (4,0)
{\includegraphics[width=0.4\textwidth, trim = {0 1em 0 2em }, clip]{Graphics/uB_vs_Laplace.pdf}};
\end{tikzpicture}\vspace{-6pt}
\label{fig:Darcy_iEIG_compNMC}
\caption{On the left, a comparison of the iEIG for stages $1-8$ using the designs maximizing the iEIG upper bound (pink diamonds) as well as 50 randomly chosen designs (black dots). The incremental EIG is computed using nested Monte Carlo as described in~\cref{appendix:NMC}. On the right, a comparison of the KL-divergence between the stage-$k$ posterior and prior at designs maximizing the upper bound (pink diamonds) and those maximizing the estimate of the EIG based on Gaussian approximations (blue squares). }
\end{figure}

\begin{table}
\footnotesize
\caption{The dimensions of the data-free and data-dependent LISs for each stage (top and bottom rows of column 2, respectively), as well as the Hellinger distance between the true and approximate stage-$k$ posterior (column 3) and the effective sample size per sample ($\text{ESS}/N$, column 4). The last column lists the total number of ROM solves required to approximate the posterior in each stage.} 
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{$k$}              & \textbf{\# LIS} & \textbf{$\mathcal{D}_{\text{H}}$} & \textbf{ESS/N}         & \textbf{\# ROMs}         \\ \hline
\multirow{2}{*}{\textbf{1}} & 15              & \multirow{2}{*}{0.127}            & \multirow{2}{*}{0.788} & \multirow{2}{*}{108,500} \\ \cline{2-2}
                            & 28              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{2}} & 14              & \multirow{2}{*}{0.211}            & \multirow{2}{*}{0.523} & \multirow{2}{*}{111,631} \\ \cline{2-2}
                            & 31              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{3}} & 13              & \multirow{2}{*}{0.176}            & \multirow{2}{*}{0.752} & \multirow{2}{*}{210,552} \\ \cline{2-2}
                            & 35              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{4}} & 16              & \multirow{2}{*}{0.253}            & \multirow{2}{*}{0.362} & \multirow{2}{*}{244,807} \\ \cline{2-2}
                            & 40              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{5}} & 12              & \multirow{2}{*}{0.274}            & \multirow{2}{*}{0.133} & \multirow{2}{*}{426,157} \\ \cline{2-2}
                            & 42              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{6}} & 13              & \multirow{2}{*}{0.231}            & \multirow{2}{*}{0.662} & \multirow{2}{*}{443,021} \\ \cline{2-2}
                            & 43              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{7}} & 15              & \multirow{2}{*}{0.249}            & \multirow{2}{*}{0.606} & \multirow{2}{*}{744,992} \\ \cline{2-2}
                            & 44              &                                   &                        &                          \\ \hline
\multirow{2}{*}{\textbf{8}} & 15              & \multirow{2}{*}{0.266}            & \multirow{2}{*}{0.542} & \multirow{2}{*}{762,755} \\ \cline{2-2}
                            & 45              &                                   &                        &                          \\ \hline
\end{tabular}\label{table:cost_darcy}
\end{table}

\subsection{Problem 2: Light source and pressure sensor placement for photoacoustic imaging}\label{sec:num2}

%
Here, we consider a design problem in photoacoustic imaging (PAI) with simplified physics. 
PAI is a hybrid medical imaging modality that aims to combine the high contrast of optical imaging with the high spatial resolution of ultrasound via the photoacoustic effect. 
In the PAI problem, laser-induced ultrasound waves are observed along the boundary of the tissue sample and used to reconstruct spatially varying optical properties of the tissue, such as absorption and scattering coefficients.
A detailed overview of the physical problem and the applications of PAI can be found in~\cite{CoxLauferArridgeBeard:2012:1,GrohlSchellenbergDreherMaier-Hein:2021:1,LutzweilerRazansky:2013:1}. Here, we focus only on the details relevant for our example, and highlight the simplifications we make. 
While various unknown parameters could be estimated in the PAI problem, we focus on inferring the absorption coefficient $\mu_a$ ($\m^{-1}$), which is related to the observed data via solution of two coupled PDEs. 

The first PDE represents the optical component of the PAI problem, relating tissue absorption $\mu_a$ to the initial pressure $p_0$ (Pa). Accurate modeling of light transport in tissues can be achieved using Monte Carlo simulations or radiative transfer equations (RTEs), though both are computationally intensive. In highly scattering media, a common simplification is to apply the diffusion approximation to the RTEs~\cite{TarvainenCox:2024:1}.
For a two-dimensional domain $\Omega = [0,5]\times[0,3] \subset \mathbb{R}^2$, given an illumination source $\source$ on the boundary $\Gamma$ and an absorption $\mu_a$ the initial pressure $p_0$ is related to the light fluence $\phi$, which satisfies the PDE
\begin{equation}
\begin{split}
\mu_a \phi - \nabla \cdot \left( \kappa(\mu_a) \nabla \phi \right) = 0 &\quad \mbox{ for } x \in \Omega \\
\frac{1}{\target}\phi+ \frac{1}{2}\kappa(\mu_a) \nabla \phi \cdot n = \source &\quad \mbox{ for } x \text{ on } \Gamma, \\
p_0 = \Gamma \mu_a \phi &\quad \mbox{ for } x \in \Omega.
\label{eq:optic_RB}
\end{split}
\end{equation}
The diffusion coefficient $\kappa(\mu_a) = \frac{1}{2(\mu_a+\mu_s')}$ ($\m)$ depends on $\mu_a$ as well as the reduced scattering coefficient, $\mu_s'$ ($\m^{-1}$). Here, we fix the reduced scattering coefficient to $\mu_s' \equiv 20 \, \mathrm{cm}^{-1}$. 
The initial pressure $p_0$ is proportional to the absorbed optical energy density $\mu_a\phi$ through the Gr\"uneisen parameter $\Gamma$, which is typically spatially-dependent and unknown in practice. 
For our model example, we fix $\Gamma \equiv 1$. 
While $\Gamma$ and $\mu_s'$ could also be treated as inference parameters, doing so would significantly increase the computational cost. Moreover, simultaneous reconstruction of all three parameters ($\mu_a$, $\Gamma$, $\mu_s'$) poses theoretical challenges~\cite{BalRen:2011:1}. An alternative approach is to treat $\Gamma$ and $\mu_s'$ as unknown nuisance parameters and formulate an uncertainty-aware likelihood, which could be achieved by marginalizing them out using the Bayesian approximation error approach~\cite{KaipioKolehmainen:2013:1}.

The second PDE is used to represent the acoustic component of the PAI problem.
Given the initial pressure $p_0$, the  pressure $p$ is obtained by solving the acoustic wave equation:
\begin{equation}
\begin{split}
\frac{1}{{c}^2}p_{tt} - \Delta p = 0 &\quad \mbox{ in } \Omega \times (0,T] \\
p(t=0) = p_0 &\quad \mbox{ in } \Omega\\
p_t(t=0) = 0 &\quad \mbox{ in } \Omega \\
\nabla p\cdot \bvec{n} - \frac{1}{c}p_t = 0 &\quad \mbox{ on } \Gamma\times (0,T],
\label{eq:AC}
\end{split}
\end{equation} 
where the sound speed $c$ is assumed to take a constant value of $1510 \frac{m}{s}$ for the simulation.

To ensure the absorption coefficient is nonnegative and to sufficiently capture tissue heterogeneity, we set $\mu_a(m) = \exp(m)$ and assume {\it a priori} that $m \sim \mathcal{N}(m_0,C_{\text{pr}})$ with $m_0 \equiv -4$ and $C_{\text{pr}}$ obtained via the covariance kernel $ \exp(-\frac{1}{2\ell^2} \norm{}{x-z}^{2}), $ with $\ell = \frac{1}{\sqrt{5}}$. 
Three sample absorption coefficients from this prior, as well as the corresponding light fluence and initial pressure, are shown in~\cref{fig:PAI_prSamps}.

\begin{figure}
\vspace{-15pt}
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (a) at (0,0)
{\includegraphics[width=0.9\textwidth, trim={0 28em 0, 1em}, clip]{Graphics/prior_samples.pdf}};
\node at (-4.5,2.7) {{$\mu_a$}};
\node at (0,2.7) {{$\phi(\design,\mu_a)$}};
\node at (4.5,2.7) {{$p_0(\design,\mu_a)$}};
\node at (-2.9,0.6) {{$\Omega$}};
\node at (-2.9,-2.0) {{$\Omega$}};
\node at (1.4,0.6) {{\color{white}{$\Omega$}}};
\node at (1.4,-2.0) {{\color{white}{$\Omega$}}};
\node at (5.6,0.6) {{\color{white}{$\Omega$}}};
\node at (5.6,-2.0) {{\color{white}{$\Omega$}}};
\label{fig:PAI_prSamps}
\end{tikzpicture}%\vspace{-15pt}
\caption{Two sample absorption coefficients from the prior (left), the corresponding light fluence $\phi$ solving~\eqref{eq:optic_RB} (middle) with light source on the top and bottom, respectively, and the initial pressure $p_0$ (right).}
%
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (a) at (0,0)
{\includegraphics[width=0.9\textwidth]{Graphics/utrues_disc.pdf}};
\node at (-4.6,3.6) {{$\mu_a^1$}};
\node at (-2.8,1.1) {{$\Omega$}};
\node at (-2.8,-2.85) {{$\Omega$}};
\node at (-4.6,-0.4) {{$\mu_a^2$}};
\node at (-1.2,2.9) {{$\tau_i$}};
\node at (-1.2,1.1) {{$\tau_i$}};
\node at (-1.2,-0.9) {{$\tau_i$}};
\node at (-1.2,-2.7) {{$\tau_i$}};
\node at (3,-4.2) {{$e_1$ (sensor number)}};
\label{fig:PAI_setup}
\end{tikzpicture}%\vspace{-15pt}
\caption{The two ``true'' absorption coefficients used to synthesize data (left column). The black dots in the left column indicate the $50$ candidate locations for sensor placement. The corresponding observed pressure wave at all the candidate locations and all observation times $(\tau_i, i = 1,\ldots,5)$ is visualized in the right column, with the data in the top and bottom rows corresponding to illumination at the top and bottom boundaries,  respectively.}
\end{figure}


In each experimental stage, the design involves selecting the location of the light source, which enters through the boundary condition for the optical PDE~\eqref{eq:optic_RB}, as well as the location of a single sensor where the pressure wave is measured. The sample can be illuminated from the top or bottom using a light source defined as
\begin{equation}
\source(e_1) = \left\{
     \begin{array}{@{}l@{\thinspace}l}
        3\exp\left[-\frac{1}{2}\frac{(x-2.5)^2}{25}\right] &\quad \text{if } y = e_1,\\
        0 &\quad \text{else},
     \end{array}
   \right.
\end{equation}
with $e_1 \in \designSpace_1 \coloneqq \{0,3\} $. 
The pressure $p$ is measured at a sensor located on the top or bottom boundary at five equally spaced observation times, starting from an initial time $\tau_0$, \ie, at $\tau_i = \tau_0+d_{\tau}(i-1)$ for $i = 1,\ldots,5$. 
We assume each measurement is corrupted with mean zero noise with standard deviation $\sigma_{\eta} = 0.1$. 
For simulations, we fix $\Ns = 50$ candidate locations for sensor placement (visualized in~\cref{fig:PAI_setup}) and enumerate them with an index $e_2 \in \designSpace_2 \coloneqq \{1,\ldots,50\}$.
With this setup, the sOED objective is to select a sequence of two-dimensional designs $\design = [e_1,e_2] \in \designSpace_1 \times \designSpace_2$, specifying both the laser location and the location of the pressure-reading sensor. 
%
The effect of the light illumination location on the light fluence and initial pressure is visualized in~\cref{fig:PAI_prSamps} for different absorption coefficients. 


After employing a finite element discretization with first-order Lagrange elements on a triangular mesh of size $h = \frac{1}{16}$ in each direction, the resulting discretized parameter $\params \in \mathbb{R}^{3969}$ corresponds to the nodal coefficients of the log absorption coefficient. Thus the discretized forward map, $\discPtO$ maps from the coefficient vector to the observations arising from both illumination choices at all the candidate locations, \ie, $\discPtO\colon\mathbb{R}^{3936} \rightarrow \mathbb{R}^{500}$.  The optical PDE is solved using the \texttt{FastFins} package, in which the observation operator defined by the acoustic wave equation is constructed using~\texttt{FEniCS}~\cite{AlnaesBlechtaHakeJohanssonKehletLoggRichardsonRingRognesWells:2015:1}. 

\subsection*{Optimal designs} In this example, we compute optimal sensor placements and illumination locations for five experimental stages. To examine how the unknown parameter influences sequential designs, we selected optimal designs for two different absorption coefficients. The ``true'' absorption coefficients used to synthesize the data are visualized in~\cref{fig:PAI_setup}. For this problem, the Fisher information matrix varies significantly with the parameters, thus a larger number of samples was required to achieve a sufficiently accurate approximation of the average Fisher information matrix and the upper bound~\eqref{eq:iEIG_uB}. For our experiments, we used $N = 500$ samples with truncation tolerances $\epsilon_G = 0.01, \epsilon_I = 0.02$. As in the first example, to speed up computation of the tensor train surrogates, we build a reduced order model restricted to the data-free likelihood-informed basis at all candidate designs using the discrete empirical interpolation method. In this case, $109$ basis vectors are sufficient using a tolerance of $0.01$ and solving the ROM is approximately $130$ times faster than the coupled PDEs. 

In~\cref{fig:PAI_designs}, for both absorption coefficients, we visualize the optimal laser and sensor locations for experimental stages 1-5, as well as the posterior mean corresponding to the data collected from these synthetic experiments. Our results indicate that, for our setup, it is optimal to illuminate and collect data at the same boundary. Additionally, the sequential designs appear to be highly dependent on the true parameter. For the first``true'' absorption coefficient $\mu_a^1$, where the main inclusion is located close to the top boundary, most of the optimal designs align on the top boundary. In contrast, for the second example $\mu_a^2$, where the inclusion is near the bottom boundary, illuminations from the bottom are preferred after the initial few experiments. To further assess the effectiveness of our upper bound, we evaluate a nested Monte Carlo estimate of the iEIG at the optimal designs and compare it to the iEIG at 50 randomly chosen designs. As shown in~\cref{fig:iEIG_PAI}, our designs consistently outperform these randomly chosen designs, suggesting that the bound provides a reliable guide for optimality. In the figure, designs with an approximated iEIG of 0 correspond to configurations where the sensor is placed on the opposite boundary from the laser.
The computational cost of choosing the optimal designs and approximating the posteriors, along with the approximation errors, is presented in~\cref{table:PAI_data}. 



\begin{figure}
\vspace{-15pt}
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (a) at (-3,0)
{\includegraphics[width=0.4\textwidth, trim = {0 1em 0 2em }, clip]{Graphics/iEIG_comp_truth1.pdf}};
\node[inner sep=0pt] (b) at (4,0)
{\includegraphics[width=0.4\textwidth, trim = {0 1em 0 2em }, clip]{Graphics/iEIG_comp_truth2.pdf}};
\label{fig:iEIG_PAI}
\end{tikzpicture}\vspace{-6pt}
\caption{A comparison of the iEIG for stages $1-5$ using the designs maximizing the iEIG upper bound (pink diamonds) as well as $50$ randomly chosen designs at each stage. The left figure corresponds to ``true'' absorption coeffient $\mu_a^1$, and the right to ``true'' absorption coefficient $\mu_a^2$.}
%
    \centering
    \includegraphics[width=0.9\linewidth]{Graphics/post_means_both_horizontal.pdf}
    \caption{The posterior means for data collected at the optimal sensor locations (black dots) using the optimal illumination locations (red line) for experimental stages $1,3,5$ for both of the absorption coefficients $\mu_a^1$ (top row) and $\mu_a^2$ (bottom row) used to synthesize the data~\cref{fig:PAI_setup}. }
    \label{fig:PAI_designs}
\end{figure}

\begin{table}[h]\label{table:PAI_data}
\footnotesize
\caption{The dimensions of data-free and data-dependent LISs (2nd column), the Hellinger error of the surrogate posterior (3rd column), ESS per sample (4th column) and the number of ROM solves for approximating the posterior (5th column) in each stage. The left table corresponds to $\mu_{a}^1$ and the right table to $\mu_a^2$.} 
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{$k$}              & \textbf{\# LIS} & \textbf{$\mathcal{D}_{\text{H}}$} & \textbf{ESS/N}         & \textbf{\# ROMs}           \\ \hline
\multirow{2}{*}{\textbf{1}} & 9               & \multirow{2}{*}{0.152}            & \multirow{2}{*}{0.845} & \multirow{2}{*}{282,875}   \\ \cline{2-2}
                            & 17              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{2}} & 14              & \multirow{2}{*}{0.161}            & \multirow{2}{*}{0.808} & \multirow{2}{*}{577,933}   \\ \cline{2-2}
                            & 20              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{3}} & 13              & \multirow{2}{*}{0.169}            & \multirow{2}{*}{0.755} & \multirow{2}{*}{661,044}   \\ \cline{2-2}
                            & 26              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{4}} & 9               & \multirow{2}{*}{0.394}            & \multirow{2}{*}{0.153} & \multirow{2}{*}{712,473}   \\ \cline{2-2}
                            & 46              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{5}} & 19              & \multirow{2}{*}{0.318}            & \multirow{2}{*}{0.333} & \multirow{2}{*}{1,182,650} \\ \cline{2-2}
                            & 46              &                                   &                        &                            \\ \hline
\end{tabular}
\quad
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{$k$}              & \textbf{\# LIS} & \textbf{$\mathcal{D}_{\text{H}}$} & \textbf{ESS/N}         & \textbf{\# ROMs}           \\ \hline
\multirow{2}{*}{\textbf{1}} & 9               & \multirow{2}{*}{0.023}            & \multirow{2}{*}{0.996} & \multirow{2}{*}{245,737}   \\ \cline{2-2}
                            & 10              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{2}} & 9               & \multirow{2}{*}{0.135}            & \multirow{2}{*}{0.693} & \multirow{2}{*}{339,605}   \\ \cline{2-2}
                            & 25              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{3}} & 16              & \multirow{2}{*}{0.139}            & \multirow{2}{*}{0.843} & \multirow{2}{*}{699,670}   \\ \cline{2-2}
                            & 33              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{4}} & 17              & \multirow{2}{*}{0.285}            & \multirow{2}{*}{0.341} & \multirow{2}{*}{802,001}   \\ \cline{2-2}
                            & 39              &                                   &                        &                            \\ \hline
\multirow{2}{*}{\textbf{5}} & 17              & \multirow{2}{*}{0.233}            & \multirow{2}{*}{0.533} & \multirow{2}{*}{1,426,682} \\ \cline{2-2}
                            & 43              &                                   &                        &                            \\ \hline
\end{tabular}
\end{table}

\appendix
\vspace{-4mm}
\section{Comparison method I --- Nested Monte Carlo}\label{appendix:NMC}
To evaluate the performance of our upper bound on the iEIG, we compare it with a nested Monte Carlo estimator. Note that for our likelihood model, using Bayes' law, we have $\target(\params\given\design_k,\data_k,\Hist_{k-1}) = \frac{\likelihood{\data_k}{\params,\design_k}\,\target(\params\given \Hist_{k-1})}{\target(\data_k \given \design_k,\Hist_{k-1})}$, and the incremental EIG can be written as follows: 
\begin{equation}
\begin{split}
    \EIG_k(\design_k) &= \Expect{\data_{k}\given \design_{k},\Hist_{k-1}}{\distKL{\target(\cdot \given \design_k,\data_k,\Hist_{k-1})}{\target(\cdot \given \Hist_{k-1})}} \\
    &= \int \int \log \left( \frac{\target(\params \given \design_k,\data_k,\Hist_{k-1})}{\target(\params\given \Hist_{k-1})}\right) \target(\params \given \design_k,\data_k,\Hist_{k-1}) \, \mathrm{d}\params\,  \target(\data_k \vert \design_k, \Hist_{k-1}) \, \mathrm{d}\data_k \\
    &= \int  \int \log \left( \frac{\likelihood{\data_k}{\params,\design_k}}{\target(\data_k \given \design_k, \Hist_{k-1})}\right) \likelihood{\data_k}{\params,\design_k}\, \target(\params\given \Hist_{k-1}) \, \mathrm{d}\params \, \mathrm{d}\data_k.
\end{split}
\end{equation}
Following~\cite{HuanMarzouk:2013:1}, $\EIG_k(\design_k)$ can be approximated at any design $\design_k \in \designSpace$ via the following double-loop or nested Monte Carlo estimator
\begin{equation}
    \EIG_k(\design_k) \approx \EIG_k^{\text{NMC}}(\design_k) = \frac{1}{N_{\text{out}}}\sum_{i=1}^{N_{\text{out}}} \left( \log(\likelihood{\data_k^{(i)}}{\params^{(i)},\design_k})-\log(\hat{\target}(\data_k^{(i)} \given \design_k, \Hist_{k-1})\right), 
\end{equation}
where $\params^{(i)}$ are drawn from the prior $\target(\params \given \Hist_{k-1})$ and $\data_k^{(i)}$ are drawn from the likelihood $\likelihood{\data_k}{\params^{(i)},\design_k}$. Since the evidence $\target(\data_k \given \design_k, \Hist_{k-1})$ typically does not have a closed-form expression, we estimate it using an inner Monte Carlo estimator: 
\begin{equation}
    \target(\data_k^{(i)} \given \design_k, \Hist_{k-1}) \approx \hat{\target}(\data_k^{(i)} \given \design_k, \Hist_{k-1}) = \frac{1}{N_{\text{in}}} \sum_{j=1}^{N_{\text{in}}} \likelihood{\data_k^{(i)}}{\params^{(i,j)},\design_k}, 
    \label{eq:NMC_evidence}
\end{equation}
where $\params^{(i,j)} \sim \target(\params\given \Hist_{k-1})$. 

To accelerate the nested Monte Carlo estimator, we also use transport maps, combined with importance sampling, to draw samples from $\target(\params\given \Hist_{k-1})$. Simulating data from the likelihood and evaluating its density involves the forward map, and thus computing the inner and outer loops for each candidate design $\design_k$ would require $O(N_{\text{in}}N_{\text{out}}\Nd)$ evaluations of the costly forward map, which can be prohibitively expensive. We mitigate the computational cost by reusing the posterior samples in two ways. First, we fix a sample set $\{\params^{(i)}\}_{i=1}^{N_{\text{out}}}$ and reuse it for each design $\design_k$. We additionally set $N_{\text{in}} = N_{{\text{out}}}$ and reuse these samples when approximating the evidence~\eqref{eq:NMC_evidence}. Sample reuse contributes to the bias of the nested Monte Carlo estimator, however, as stated in~\cite{HuanMarzouk:2013:1}, this effect is rather small. 

\section{Comparison method II --- Estimating iEIG using local Gaussian approximations} \label{appendix:Laplace}
We further evaluate our approach against an extension of the methods outlined in~\cite{WuChenGhattas:2023:1}. 
In particular, we extend the prior sample point approximation method described in section 4.4 of~\cite{WuChenGhattas:2023:1} to estimate the iEIG using successive Gauss-Newton approximations to the posterior. 
Employing a Monte Carlo approximation to the iEIG, we have
\begin{equation}
    \EIG_k(\design_k) \approx \EIG_k^{G}(\design_k) = \frac{1}{N}\sum_{i=1}^N\distKL{\target^G(\cdot \given \data_k^{(i)},\design_k,\Hist_{k-1})}{\target^G(\cdot \given \Hist_{k-1})},
\end{equation}
where $\target^G(\params \given \data_k,\design_k,\Hist_{k-1})$ and $\target^G(\params \given \Hist_{k-1})$ denote Gaussian approximations to the candidate stage-$k$ posterior and stage-$k$ prior, respectively. The data samples $\data_k^{(i)}$ for $i = 1,\ldots,N$ are obtained by drawing a sample $\params^{(i)}$ from the Gaussian approximation to the prior, and synthesizing the noisy data using the accurate forward map, $\PtO$. 

In particular, given a fixed history of observed data and experimental conditions $\Hist_{k-1}$, denote the corresponding maximum-a-posteriori (MAP) estimator as $\params_{k-1}^{\mathrm{MAP}}$. Under our assumption of i.i.d.\@ Gaussian noise $\noise \sim \mathcal{N}(\bvec{0},\sigma_{\eta}^2\I{})$, the Gauss-Newton approximation to the stage-$k$ prior is, 
\begin{align*}
\target^G(\params \given \Hist_{k-1}) &\sim \mathcal{N}\left(\params_{k-1}^{\mathrm{MAP}},\mathcal{C}_{k-1}(\params_{\mathrm{MAP}})\right),  \\
\mathcal{C}_{k-1}(\params_{\mathrm{MAP}}) &= \left({\mathcal{C}^{-1}_{k-2}}+\frac{1}{\sigma_{\eta}^2}\Jac\PtO(\params_{k-1}^{\mathrm{MAP}})^\top\,\Jac\PtO(\params_{k-1}^{\mathrm{MAP}})\right)^{-1}. 
\end{align*}

In each experimental stage, a Gaussian approximation needs to be constructed for every set of sample data $\data_{k}^{(i)}$. The standard Gauss-Newton approximation centers the Gaussian around the MAP estimator. However, this requires solving at least $N$ optimization problems in each experimental stage (depending on the approach used). To avoid this, we use the approach outlined in~\cite[section 4.4]{WuChenGhattas:2023:1}. In this setting, the KL divergence between $\target^G(\cdot \given \data_k^{(i)},\design_k,\Hist_{k-1})$ and $\target^G(\cdot \given \Hist_{k-1})$ can be estimated as
\begin{multline}
    \distKL{\target^G(\cdot \given \data_k^{(i)},\design_k,\Hist_{k-1})}{\target^G(\cdot \given \Hist_{k-1})} = \\ \frac{1}{2}\sum_{j=1}^{k_i}\left(\log \left(1+\lambda_j(\Hmat^{(i)}_D(\design_k))\right) - \frac{\lambda_j(\Hmat^{(i)}_D(\design_k))}{1+\lambda_j(\Hmat^{(i)}_D(\design_k))}\right)
    + \| \params^{(i)} - \params_{k-1}^{\mathrm{MAP}} \|_{\mathcal{C}^{-1}_{k-1}}
    \label{eq:DKL_Gaussians}
\end{multline}
where $\lambda_{j}(\Hmat^{(i)}_D(\design_k))$ are the nonzero eigenvalues of the matrix $$\Hmat^{(i)}_D(\design_k) = \frac{1}{\sigma_{\eta}^2}\Jac\PtO(\design_k,\params^{(i)})\,\mathcal{C}_{k-1}\Jac\PtO(\design_k,\params^{(i)})^\top.$$ 
Thus, since the last term in~\eqref{eq:DKL_Gaussians} is independent of the current design $\design_k$, the optimal design at stage $k$ can be obtained by solving the optimization problem
\begin{equation}
    \design_k^* \in \Argmax_{\design_k} \frac{1}{2N}\sum_{i=1}^N \sum_{j=1}^{k_i}\left(\log \left(1+\lambda_j(\Hmat^{(i)}_D(\design_k))\right) - \frac{\lambda_j(\Hmat^{(i)}_D(\design_k))}{1+\lambda_j(\Hmat^{(i)}_D(\design_k))}\right). 
\end{equation}

\bibliographystyle{abbrv}

\bibliography{biblio}

\end{document}

