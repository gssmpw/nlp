\section{Related work}
\label{subsec:relatedWork}
The most common approach for estimating the expected information gain involves the use of nested Monte Carlo estimators~\cite{Ryan:2003:1,HuanMarzouk:2013:1}. 
In this approach, the EIG is reformulated as an expectation of the difference between the log-likelihood and log-evidence. 
Estimating the EIG requires an outer Monte Carlo estimator for the expectation, along with an inner nested Monte Carlo loop to estimate the intractable evidence for each outer Monte Carlo sample. 
While the nested Monte Carlo estimator is asymptotically unbiased, its convergence is slower than that of standard Monte Carlo, and obtaining sufficiently accurate estimators can be computationally expensive. Recently, measure transport approaches to batch optimal experimental design have been explored in~\cite{CaoChenBrennanOLearyRoseberryMarzoukGhattas:2024:1,LiBaptistaMarzouk:2024:1,DongJacobsenKhalloufiAkramLLiuDuraisamyHuan:2025:1}. 
These approaches can generally be viewed as two-step estimation approaches, combining measure-transport-based density estimation with a Monte Carlo estimator of the expectation.

Our previous work~\cite{KovalHerzogScheichl:2024:1} also uses transport maps for sOED, but is limited to problems with low- to moderate-dimensional parameters. A few other sOED approaches are outlined in the review articles~\cite{HuanJagalurMarzouk:2024:1,RyanDrovandiMcGreePettitt:2016:1}. 
Many of these approaches are formulated for low-dimensional parameters and involve sequentially transforming samples between experimental stages using ratio function estimation or sequential Monte Carlo methods~\cite{DrovandiMcGreePettitt:2013:1,KleinegesseDrovandiGutmann:2020:1}. 
In contrast, we focus on problems where the unknown parameter $\params$ corresponds to a finite-dimensional discretization of a functional input of the forward map, which typically results in high dimension.

In the context of OED for large- or infinite-dimensional problems, common approaches typically involve some combination of Gaussian approximations to the posterior~\cite{WuChenGhattas:2023:1}, the use of derivative-informed neural networks~\cite{WuOLeary-RoseberryChenGhattas:2023:1}, or approaches that exploit the presence of low-dimensional structures~\cite{CaoBaptistaChenLiGhattasOdenMarzouk:2023:1,LiBaptistaMarzouk:2024:1}. 
The aforementioned works all focus on batch OED, though a combination of these approaches has also been used in sOED. 
Specifically,~\cite{GoChen:2024:1} employs Gaussian approximations to the posterior, constructed efficiently using dimension-reduced neural network surrogates, for the sequential selection of optimal observation times in Bayesian inverse problems involving dynamical systems. In contrast, our primary focus is on the optimal selection of sensor locations, which requires a different formulation of the sOED problem.