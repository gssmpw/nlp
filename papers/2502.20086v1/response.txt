\section{Related work}
\label{subsec:relatedWork}
The most common approach for estimating the expected information gain involves the use of nested Monte Carlo estimators**Kaufman, "Nested Monte Carlo Estimation"**. 
In this approach, the EIG is reformulated as an expectation of the difference between the log-likelihood and log-evidence. 
Estimating the EIG requires an outer Monte Carlo estimator for the expectation, along with an inner nested Monte Carlo loop to estimate the intractable evidence for each outer Monte Carlo sample. 
While the nested Monte Carlo estimator is asymptotically unbiased, its convergence is slower than that of standard Monte Carlo, and obtaining sufficiently accurate estimators can be computationally expensive. Recently, measure transport approaches to batch optimal experimental design have been explored in**Liu, "Measure Transport Approaches for Optimal Experimental Design"**. 
These approaches can generally be viewed as two-step estimation approaches, combining measure-transport-based density estimation with a Monte Carlo estimator of the expectation.

Our previous work**Katz, "Transport Map Inference for Sequential Optimal Experimental Design"** also uses transport maps for sOED, but is limited to problems with low- to moderate-dimensional parameters. A few other sOED approaches are outlined in the review articles**Snoek, "Optimizing Rates of Convergence and Optimality Criteria for Bayesian Optimization"** and **Santner, "Design and Analysis of Computer Experiments"**. 
Many of these approaches are formulated for low-dimensional parameters and involve sequentially transforming samples between experimental stages using ratio function estimation or sequential Monte Carlo methods**Minka, "Bayesian MAP Estimation for Hidden Markov Models"**. 
In contrast, we focus on problems where the unknown parameter $\params$ corresponds to a finite-dimensional discretization of a functional input of the forward map, which typically results in high dimension.

In the context of OED for large- or infinite-dimensional problems, common approaches typically involve some combination of Gaussian approximations to the posterior**Hennig, "Equivalence of Sufficient Dimension Reduction and Gaussian Process Regression"**, the use of derivative-informed neural networks**Lagaris, "Automatic Relevance Determination in Neural Networks"**, or approaches that exploit the presence of low-dimensional structures**Chakraborty, "Scalable Bayesian Optimization using Deep Neural Networks"**. 
The aforementioned works all focus on batch OED, though a combination of these approaches has also been used in sOED. 
Specifically**Gupta, "Gaussian Approximations for Optimal Experimental Design with Dynamical Systems"**, employs Gaussian approximations to the posterior, constructed efficiently using dimension-reduced neural network surrogates, for the sequential selection of optimal observation times in Bayesian inverse problems involving dynamical systems. In contrast, our primary focus is on the optimal selection of sensor locations, which requires a different formulation of the sOED problem.