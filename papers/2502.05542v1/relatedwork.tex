\section{Related Works}
\label{sec:related}
%This work is broadly related to works on neural network adversarial attacks, universal adversarial attacks and defenses against universal adversarial attacks.

\textbf{Adversarial attacks.} 
Neural networks are highly vulnerable to adversarial attacks, which are small, deliberately crafted perturbations to input data that can fool the model into making incorrect predictions. Such perturbation can be 1) image-specific where the attacker computes a perturbation for every clean input and 2) image-agnostic where a single perturbation will cause majority of clean samples to fool a given model. In recent years, many input-specific adversarial attacks are proposed to generate disruptive perturbations.~\citet{fgsm} introduce Fast Gradient Sign Method (FGSM) that generates adversarial examples. 
%by perturbing the input data in the direction of the gradient of the loss function w.r.t. input. 
Subsequently, Basic Iterative Method (BIM) is proposed~\citep{BIM} as an extension of FGSM which applies small FGSM steps iteratively aiming to generate higher quality perturbations.~\citet{pgd} propose PGD which optimizes the perturbation at each iteration based on gradient of loss function. 
%and projects the perturbation back on to the feasible set defined by a norm constraint.
Furthermore,~\citet{CW} propose C\&W attack which formulates the adversarial example generation as an optimization problem aiming at minimal perturbation. Together with many others, e.g.,~\citep{adv_speech,audio_cnn,deep_face,blackbox_face,fool_topic,fusion}, adversarial attacks pose a significant threat to real-world applications in different domains.

\textbf{Universal adversarial attacks.} Unlike per-instance perturbations, UAPs work for the majority of clean samples, i.e., adding a single perturbation to majority of clean samples, the neural network will response with incorrect predictions. Such attacks can be broadly classified in to noise-based and generator-based attacks. Noise-based attack methods directly train a UAP that can be applied to all inputs while generator-based methods train an extra generative model as a bridge to craft he perturbation indirectly~\citep{uap_survey}.~\citet{uap} first explore the existence of such input-agnostic adversarial perturbations. 
%Given a model and a set of clean samples, to compute such UAPs, the algorithm proposed in~\citep{uap} iteratively computes the perturbation of each sample to make an adversarial example across the decision boundary of the correct label. 
Furthermore,~\citet{singular} propose to craft UAPs by maximizing the difference between the activations of a hidden layer for clean and perturbed inputs. Later on, many noise-based methods are proposed with good performance~\citep{FFF,datafree}. 
%Furthermore, study in~\citep{FFF} reveals that misfiring the activation of each feature layer can cause the model to produce wrong predictions. Thus, Fast Feature Fool is proposed to generate UAP by maximizing the feature activation when feeding it to the target model. Zhang~\emph{et al.} further propose data free UAP (DF-UAP)~\citep{datafree} which crafts UAPs from a proxy dataset and achieves comparable performance to the methods based on original training set. 
On the other hand,~\citet{gap} firstly apply generative model for crafting UAPs. %Different loss functions are designed for targeted and non-targeted UAP attacks. 
NAG is proposed~\citep{nag} with a novel loss function for training the perturbation generator. 
%NAG further uses a diversity objective function to increase the fooling ratio and the cross-model transferability of the generated UAP.
%GM-TUAP is another generator-based method that incorporates the Fast Feature Fool method as an extra supervision for training the perturbation generator based on their finding that the feature representations in the first layer of various model architectures share similar feature maps (thus increasing adversarial energy in the first layer will improve transferability). 
Beyond above mentioned methods, there are many other UAP attacks, e.g.,~\citep{double,singular,cduap,face,speech}. Compared to input-specific perturbations, UAPs are more efficient in terms of computation cost and become a more significant threat in practice.

\textbf{Defense against adversarial attacks.} 
%The existence of adversarial perturbations poses a significant threat to the real-world applications of neural networks. 
%Understanding and defending against these adversarial attacks has become a critical topic. 
%To mitigate the effect of adversarial perturbations, a number of works have attempted to defend through various techniques. 
Defense against adversarial attacks can be grouped into six domains~\citep{sur_sees}: 1) adversarial training which augments the training data with adversarial examples to make the model more robust~\citep{fgsm,pgd,fast,trades,spgd,fat}, 2) modifying the training process which adjusts the training process to improve robustness~\citep{distill,uat,sat,ltd,score,uap_defense}, 3) use of supplementary networks which add extra networks on top of the original model to remove the effect of adversarial perturbations~\citep{defense_gan,hgd,er_classifier,dg,disco,uapdefensefeatregen}, 4) changing network architecture which modifies the architecture of the original model for robustness~\citep{fdenoise,nas_robust,adv_random,cnls,cfn,fns}, 5) performing network validation which validates and certifies the robustness of a given model~\citep{deep_explore,deep_gauge,sadl} and 6) adversarial purification which removes adversarial perturbations of input samples and recovers the clean image~\citep{irugd,ddpm,adv_proxy,diffpure,densepure}. Among them, there are multiple works proposed to defense against UAPs~\citep{uap,uap_defense,spgd,uapdefensefeatregen,cfn,fns}.
%Adversarial training is widely applied to defend against input-specific attacks.~\citet{fgsm} first introduce adversarial training by augmenting the training data with adversarial examples to make the model more robust. Similarly,~\citet{pgd} proposes robust optimization techniques to enhance the resistance of deep learning models against adversarial attacks. 
%Wong~\emph{et al.}~\citep{fast} proposes an efficient method for adversarial training that significantly reduces the computational cost compared to traditional methods. The authors introduce Fast Adversarial Training, which speeds up the process of generating adversarial examples during training. 
%There are multiple works proposed in recent years to protect neural networks against UAPs as well.~\citet{uap} first finetune a given model with pre-crafted UAPs which leads to marginal robustness improvements.~\citet{spgd} propose to generate perturbations on-the-fly instead of pre-computing which results is better defense performance.~\citet{uapdefensefeatregen} propose a feature-level defense method. 
%A resilient feature regeneration unit is first trained with clean and pre-crafted UAPs and then is inserted into the original model aiming to transform vulnerable features into a resilient ones. Furthermore, Cheng Yu~\emph{et al.} propose clipping feature norms (CFN)~\citep{cfn} and feature norm suppression (FNS)~\citep{fns} to suppress the generation of large norm deep feature vectors in order to weaken the impact of universal adversarial patches on the original model. CFN clamps large feature norms at a selected hidden layer and FNS further renormalize the feature norm on top of CFN. Both methods introduce a norm suppressing layer to be inserted to the original neural network. 
%Furthermore, there are other defense methods such as certified training~\citep{certified_training,certified2}, randomized smoothing~\citep{rs0,rs1,rs2} and some others~\citep{fns,cfn}.