\section{Related Works}
\label{sec:related}
%This work is broadly related to works on neural network adversarial attacks, universal adversarial attacks and defenses against universal adversarial attacks.

\textbf{Adversarial attacks.} 
Neural networks are highly vulnerable to adversarial attacks, which are small, deliberately crafted perturbations to input data that can fool the model into making incorrect predictions. Such perturbation can be 1) image-specific where the attacker computes a perturbation for every clean input and 2) image-agnostic where a single perturbation will cause majority of clean samples to fool a given model. In recent years, many input-specific adversarial attacks are proposed to generate disruptive perturbations.____ introduce Fast Gradient Sign Method (FGSM) that generates adversarial examples. 
%by perturbing the input data in the direction of the gradient of the loss function w.r.t. input. 
Subsequently, Basic Iterative Method (BIM) is proposed____ as an extension of FGSM which applies small FGSM steps iteratively aiming to generate higher quality perturbations.____ propose PGD which optimizes the perturbation at each iteration based on gradient of loss function. 
%and projects the perturbation back on to the feasible set defined by a norm constraint.
Furthermore,____ propose C\&W attack which formulates the adversarial example generation as an optimization problem aiming at minimal perturbation. Together with many others, e.g.,____, adversarial attacks pose a significant threat to real-world applications in different domains.

\textbf{Universal adversarial attacks.} Unlike per-instance perturbations, UAPs work for the majority of clean samples, i.e., adding a single perturbation to majority of clean samples, the neural network will response with incorrect predictions. Such attacks can be broadly classified in to noise-based and generator-based attacks. Noise-based attack methods directly train a UAP that can be applied to all inputs while generator-based methods train an extra generative model as a bridge to craft he perturbation indirectly____.____ first explore the existence of such input-agnostic adversarial perturbations. 
%Given a model and a set of clean samples, to compute such UAPs, the algorithm proposed in____ iteratively computes the perturbation of each sample to make an adversarial example across the decision boundary of the correct label. 
Furthermore,____ propose to craft UAPs by maximizing the difference between the activations of a hidden layer for clean and perturbed inputs. Later on, many noise-based methods are proposed with good performance____. 
%Furthermore, study in____ reveals that misfiring the activation of each feature layer can cause the model to produce wrong predictions. Thus, Fast Feature Fool is proposed to generate UAP by maximizing the feature activation when feeding it to the target model. Zhang~\emph{et al.} further propose data free UAP (DF-UAP)____ which crafts UAPs from a proxy dataset and achieves comparable performance to the methods based on original training set. 
On the other hand,____ firstly apply generative model for crafting UAPs. %Different loss functions are designed for targeted and non-targeted UAP attacks. 
NAG is proposed____ with a novel loss function for training the perturbation generator. 
%NAG further uses a diversity objective function to increase the fooling ratio and the cross-model transferability of the generated UAP.
%GM-TUAP is another generator-based method that incorporates the Fast Feature Fool method as an extra supervision for training the perturbation generator based on their finding that the feature representations in the first layer of various model architectures share similar feature maps (thus increasing adversarial energy in the first layer will improve transferability). 
Beyond above mentioned methods, there are many other UAP attacks, e.g.,____. Compared to input-specific perturbations, UAPs are more efficient in terms of computation cost and become a more significant threat in practice.

\textbf{Defense against adversarial attacks.} 
%The existence of adversarial perturbations poses a significant threat to the real-world applications of neural networks. 
%Understanding and defending against these adversarial attacks has become a critical topic. 
%To mitigate the effect of adversarial perturbations, a number of works have attempted to defend through various techniques. 
Defense against adversarial attacks can be grouped into six domains____: 1) adversarial training which augments the training data with adversarial examples to make the model more robust____, 2) modifying the training process which adjusts the training process to improve robustness____, 3) use of supplementary networks which add extra networks on top of the original model to remove the effect of adversarial perturbations____, 4) changing network architecture which modifies the architecture of the original model for robustness____, 5) performing network validation which validates and certifies the robustness of a given model____ and 6) adversarial purification which removes adversarial perturbations of input samples and recovers the clean image____. Among them, there are multiple works proposed to defense against UAPs____.
%Adversarial training is widely applied to defend against input-specific attacks.____ first introduce adversarial training by augmenting the training data with adversarial examples to make the model more robust. Similarly,____ proposes robust optimization techniques to enhance the resistance of deep learning models against adversarial attacks. 
%Wong~\emph{et al.}____ proposes an efficient method for adversarial training that significantly reduces the computational cost compared to traditional methods. The authors introduce Fast Adversarial Training, which speeds up the process of generating adversarial examples during training. 
%There are multiple works proposed in recent years to protect neural networks against UAPs as well.____ first finetune a given model with pre-crafted UAPs which leads to marginal robustness improvements.____ propose to generate perturbations on-the-fly instead of pre-computing which results is better defense performance.____ propose a feature-level defense method. 
%A resilient feature regeneration unit is first trained with clean and pre-crafted UAPs and then is inserted into the original model aiming to transform vulnerable features into a resilient ones. Furthermore, Cheng Yu~\emph{et al.} propose clipping feature norms (CFN)____ and feature norm suppression (FNS)____ to suppress the generation of large norm deep feature vectors in order to weaken the impact of universal adversarial patches on the original model. CFN clamps large feature norms at a selected hidden layer and FNS further renormalize the feature norm on top of CFN. Both methods introduce a norm suppressing layer to be inserted to the original neural network. 
%Furthermore, there are other defense methods such as certified training____, randomized smoothing____ and some others____.