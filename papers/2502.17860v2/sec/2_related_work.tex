

\section{Related Work}
\label{sec:related_work}

  \textbf{Multi-modal pretraining via contrastive learning}. Leveraging multi-modal data to pre-train modality-specific encoders via contrastive alignment has received considerable attention in recent years~\citep{radford2021learning, mu2022slip} due to its ability to leverage massive in-the-wild datasets of paired data. While early works have largely been focused on image-text data, promoting research on text-based image manipulation~\citep{patashnik2021styleclip}, open vocabulary object detection~\citep{gu2021open, gao2022open}, language grounding~\citep{li2022grounded} and zero-shot segmentation~\citep{xu2023open}, there has been an increasing focus on learning 3D representations lately~\citep{xue2023ulip, xue2023ulip2, zeng2023clip2, zhou2024uni3d}. These approaches largely follow the contrastive learning paradigm by adding a new 3D representation encoder and aligning it with the 2D and/or text modalities. However, due to the difficulty of collecting and constructing 3D representation data, current approaches leverage point clouds. 
  More specially, PointCLIP~\citep{zhang2022pointclip} and CLIP2Point~\citep{huang2023clip2point} extract depth maps from point clouds to obtain image-like data that can be leveraged for contrastive pretraining.
  Since these depth maps lose plenty of spatial information of the original point cloud data structure, CLIP$^2$~\citep{zeng2023clip2} instead learns a 3D encoder directly on point clouds, demonstrating robustness in real-world indoor and outdoor scenarios. However, the flexibility and simplicity of point clouds come at a cost as specific shape and texture information of the object surface is lost, leading to ambiguities. In this work, we, therefore, adopt 3DGS~\citep{kerbl20233d} to replace point clouds as the 3D representation to alleviate this loss of information.
    
    \noindent\textbf{Zero-shot/Open-world Learning in 3D.}
    Considerable progress has been made to project a point cloud into 3D voxels~\citep{shi2020pv, maturana2015voxnet, qi2017pointnet} and extract features that can be associated with semantic category information. PointNet++~\citep{qi2017pointnet++} proposes a hierarchical neural network to extract local features with increasing contextual scales and PointMLP~\citep{ma2022rethinking} proposes a pure residual MLP network while achieving competitive results.
    More recently, self-supervised learning~\citep{yu2022point, pang2022masked} and unsupervised learning~\citep{afham2022crosspoint, liang2021exploring, liu2022masked} approaches for 3D understanding have also shown promising performance.
    However, while the current supervision-based methods are restricted by the annotated training datasets and show poor performance on unseen categories, self-supervised and unsupervised-based methods can not directly be transferred to zero-shot tasks with open-world vocabularies due to the limited downstream annotations. Therefore, we construct a language-image-3D dataset for pretraining to learn transferable 3D representations that are aligned to an open-vocabulary language space to facilitate zero-shot transfer.

