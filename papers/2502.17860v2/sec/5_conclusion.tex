\vspace{-3mm}
\section{Conclusion}
\label{sec:conclusion}

In this paper, we proposed \name{}, which for the first time includes 3DGS in cross-modal learning as the universal 3D representation supplementing shape and texture information. To this end, a parallel structure with cross-attention is proposed to avoid knowledge conflicts and builds knowledge connections via the attention mechanism.
We demonstrate that our proposed \name{} achieves superior performance to state-of-the-art approaches and reveals the power and importance of 3DGS for 3D representation learning. 

\textbf{Limitations:} Despite the robust and effective performance of \name{} for 3D representation learning and downstream applications, its current version lacks performance validation of out-door scenarios and 3D understanding ability with Large Language Model (LLM), resulting in insufficient performance improvement and validation for downstream tasks. 
%
Moreover, at least one image with a camera pose is required for the optimization of 3DGS, and how to further consider a camera-pose-free approach (e.g., Image-to-3DGS) or a dataset of pure point clouds while maintaining performance, is another exciting direction for future work.

\textbf{Acknowledgment:} This work is supported by National Key Research and Development Program of China(2024YFE0203100), National Natural Science Foundation of China (NSFC) under Grants No.62476293 and Nansha Key R$\&$D Program under Grant No.2022ZD014. We thank the General Embodied AI Center of Sun Yat-sen University for support of this work.