\appendix
\newpage

\section{Appendix/supplemental material}
The outline of the Appendix is as follows:
\begin{itemize}
    \item More Implementation Details;
        \begin{itemize}
            \item More Implementation Details of ensemble datasets;
            \item More Implementation Details of training and evaluation;
        \end{itemize}
    \item More ablation study on the quality of 3DGS;
        \begin{itemize}
            \item Additional ablation study on objaverse zero-shot classification;
            \item Additional ablation study on ABO zero-shot classification;
        \end{itemize}
    \item More comparisons on large-scale data;
    \item More comparisons to state-of-the-art methods;
    \item More experiments with unfrozen fundamental Encoder;
    \item More experiments on generalization to 3DGS-driven methods;
    % \item More experiments on generalization to point clouds;
    \item More evaluations on zero-shot image-driven retrieval;
        \begin{itemize}
            \item Additional comparisons on zero-shot image-driven retrieval;
        \end{itemize}
    \item Future work.
    \item Social impact;
    \item Discussion
\end{itemize}

\section{Implementation Details}
Here we provide more implementation details on the ensemble of the Objaverse\cite{deitke2023objaverse}, ABO\cite{collins2022abo}, MVImgNet\cite{yu2023mvimgnet} and SUN RGBD\cite{song2015sun} datasets; as well as the training and evaluation details on the Text-driven retrieval, Zero-shot classification, and scene recognition tasks.

\subsection{Details of ensemble datasets}
Cap3D\cite{luo2024scalable} and InternLM-composer2\cite{dong2024internlm} is leveraged for object caption, which will be used for the Text-driven retrieval task. To better utilize the weights pre-trained on colored point clouds, the SH degree is set to 0 to ignore the view-dependent color effects, which also brings the additional advantage of reducing the computation and storage consumption. Moreover, the opacity reset interval is set to 501 to promote the stability of 3DGS. Note that the saving interval should not be a multiplier of the opacity reset interval, otherwise the retained results may become unstable. The optimization iteration for Objaverse, ABO, MVImgNet, and SUN RGBD datasets is set to 1500, 2000, 3000, and 500, respectively. All datasets can be successfully prepared on 6$\times$RTX4090 GPU within 2 days, where 15 scenes can be optimized simultaneously on each GPU.

\subsection{Details of training and evaluation}
We leverage the activation function $tanh(\cdot)$ to convert the features of 3DGS to the range [-1,1] and set the batch size of training and evaluation to 24 and 80, respectively. In terms of time consumption, the whole training process on Objaverse costs 12.5 hours with 6$\times$RTX4090 GPU, where UniGS is trained for 15 epochs on the Objaverse training set for the sufficient understanding and alignment of 3DGS representations, which will be used for Zero-shot classification and Text-driven retrieval in the inference stage. 

\textbf{Zero-shot classification. }
After training 15 epochs on Objaverse, UniGS is directly evaluated on the entire Objaverse-Lvis, ABO, and MVimgnet datasets. We reorganize each dataset to accelerate the entire evaluation, where Objaverse-Lvis, ABO, and MVImgNet are reorganized into 315, 23, and 95 categories, respectively.

\textbf{Text-driven retrieval. }
In Text-driven retrieval, ABO and MVImgNet will be split into training and testing sets, where the testing sets of Objaverse, ABO, and MVImgNet contain 1000, 433, and 1450 items, respectively.
UniGS will be further fine-tuned for 50 epochs on the training set to alleviate the impact of the text domain across different datasets. Next, 3DGS encoded by UniGS is used to compute similarity and calculate Top$k$ accuracy across texts of all the items in the testing set.

\textbf{Scene recognition.}
As for the scene recognition task on the SUN RGBD dataset, UniGS follows the basic evaluation pattern to directly train 50 epochs on the training set and finally be evaluated on the testing set. 

\section{Additional ablation study of the quality of 3DGS}
\begin{figure*}
    \centerline{\includegraphics[width=\textwidth]{rebuttal/retrive_ablation_more.png}}
    \caption{\textbf{Additional ablation study of the quality of 3DGS on the Text-driven retrieval task.} The accuracy of Text-driven retrieval on Objaverse under three optimization pipelines.}
    \vspace{-3mm}
    \label{fig:3dgs_retrive_ablation}
\end{figure*}

\begin{figure*}
    \centerline{\includegraphics[width=\textwidth]{rebuttal/classification_ablation_more.png}}
    \caption{\textbf{Additional ablation study of the quality of 3DGS on the Zero-shot classification task.} The accuracy of Zero-shot classification on ABO under three optimization pipelines.}
    \vspace{-3mm}
    \label{fig:3dgs_classification_ablation}
\end{figure*} 

As shown in \cref{fig:3dgs_retrive_ablation} and \cref{fig:3dgs_classification_ablation}, we considered two common optimization settings of 3DGS for the ablation of the number of images: (1) flexible (ours): load surface points as initialization with flexibility of 3DGS location, (2) original: the vanilla optimization from 3DGS.

As shown in the results in \cref{fig:3dgs_retrive_ablation}, with the increasing number of images, the overall accuracy generally shows an upward trend.  
Our "flexible" pipeline exhibits stronger robustness and better reconstruction capability to the number of input images. Notably, when the number of images is halved, the overall accuracy can be maintained within 10\% of the optimal level.
Moreover, as shown in the results in \cref{fig:3dgs_classification_ablation} of the rebuttal PDF, our additional ablation study on the ABO dataset reveals that 3DGS only requires a small number of images to achieve commendable results. More specifically, on simpler reconstruction tasks like ABO, our UniGS can maintain its performance even when the image count is drastically reduced from 36 to just 4.



%------------------------------------------------------------------------------------------------------------------------%
\section{Further comparisons on large-scale data}
\begin{table*}[t]
		\centering
  % \addtolength{\tabcolsep}{-0.5pt}
  \caption{\textbf{Experimental results of large scale training on Objaverse-LVIS zero-shot classification. Avg.: }the mean average classification accuracy. All methods are trained from scratch.}
  \setlength{\tabcolsep}{3pt}
 \begin{tabularx}{\textwidth}{ l | c c c | c c c | c c }
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Source} & \multirow{2}{*}{3D points} & \multirow{2}{*}{Backbone}  & \multicolumn{3}{c|}{Avg.} & Training & \multirow{2}{*}{Representation} \\
% \midrule
  & & & & Top1 & Top3 & Top 5 & Dataset & \\
  
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{Large-scale training}}} \\
    \midrule
Uni3D & \multirow{2}{*}{with Lvis} & \multirow{2}{*}{1024} & \multirow{2}{*}{EVA02-S} & 46.31 & 72.62 & 79.78 & \multirow{2}{*}{800k}  & \multirow{2}{*}{3DGS}\\
UniGS(Ours) & & & & \textbf{49.95} & \textbf{75.60} & \textbf{82.38} & & \\
\bottomrule
\end{tabularx}
    \vspace{-1mm}
  \label{tab:further_comparsions_on_large_scale_data}
  \vspace{-3mm}
\end{table*}

As shown in \cref{tab:further_comparsions_on_large_scale_data}, we supplement comparisons between UniGS and Uni3D when scaling up to the dataset used in Uni3D, which is a combination of Objaverse and ABO. As shown in the \emph{Large-scale training} part in Table, UniGS benefits from scaling up of the dataset and still outperforms Uni3D when considering the larger scale setting, clearly demonstrating the benefit of 3DGS over point clouds. Additional comparisons to the official state-of-the-art methods with 10000 3D points can be found in \cref{sec:appendix_further_comparisons_to_sota_methods}.

%------------------------------------------------------------------------------------------------------------------------%

\section{Further comparisons to state-of-the-art methods}
\label{sec:appendix_further_comparisons_to_sota_methods}
\begin{table*}[t]
		\centering
  \addtolength{\tabcolsep}{-0.5pt}
  \caption{\textbf{Comparisons to state-of-the-art methods with the same data on Objaverse-LVIS zero-shot classification. Avg.: }the mean average classification accuracy.}
 \begin{tabularx}{\textwidth}{ l | c c | c c c | c c | c }
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Source} & \multirow{2}{*}{Backbone}  & \multicolumn{3}{c|}{Avg.} & \multicolumn{2}{c|}{Dataset} & \multirow{2}{*}{Representation} \\
% \midrule
  & & & Top1 & Top3 & Top 5 & train & test & \\

 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{10000 3D points with official model}}} \\
    \midrule
TAMM & \multirow{5}{*}{with Lvis}  & Point-BERT & 50.70 & 73.20 & 80.60 & 800k & 46k & point clouds\\
ReCon++-B & & ViT-bigG & 53.20 & 75.30 & 81.50 & 800k & 46k & point clouds \\
Uni3D-S & & EVA02-S & 50.34 & 72.70 & 79.81 & 800k & 46k & point clouds \\
Uni3D-S & & EVA02-S & 49.87 & 72.39 & 79.70	& 800k & 6k & point clouds \\
UniGS-S(Ours) & & EVA02-S & 51.22 & 73.64 & 80.88 & 46k & 6k & 3DGS\\
\bottomrule
\end{tabularx}
  \label{tab:appendix_further_comparsions_to_sota}
  \vspace{-3mm}
\end{table*}

We supplement extra experiments on representations with 10000 3D points and evaluate UniGS on a no-bias mini testing set following setting of the "Ensembled (with LVIS)" from Uni3D. As shown in \cref{tab:appendix_further_comparsions_to_sota}, UniGS benefits from the increasing of 3D points, achieving similar Top 1 accuracy compared with Uni3D, TAMM and ReCon++ (regardless of backbones). Note that
TAMM and ReCon++ use larger model parameters than UniGS.

Therefore, as shown in \cref{tab:further_comparsions_to_sota} and \cref{tab:appendix_further_comparsions_to_sota}, with the dataset scaling up and the increasing of 3D poitns, UniGS have great potential to show superior performance over common point clouds, outperforming at the same data.

Moreover, UniGS proposed to leverage 3DGS as 3D representation and utilize Gaussian-Aware Guidance for better understanding, which can be applied at existing 3D understanding work with point clouds for improvement. Since we leverage Uni3D as our fundamental encoder, experiments will focus on the comparisons with Uni3D in the original paper to show the effectiveness of 3DGS representation and our Gaussian-Aware Guidance module.


%------------------------------------------------------------------------------------------------------------------------%

\section{Additional ablation study with unfrozen representation}
\begin{table*}[t]
		\centering
  \caption{\textbf{Additional comparisons to state-of-the-art methods on Zero-shot classification.} Avg. denotes the mean average classification accuracy. * denotes training from scratch and \dag denotes unfreezing the fundamental encoder.}
  \vspace{-2mm}
  % \addtolength{\tabcolsep}{5pt}
  % \setlength{\tabcolsep}
 \begin{tabular}{ c c | c c c | c c c}
\toprule
\multirow{2}{*}{Methods} & Ufrozen  & \multicolumn{3}{c}{\textbf{\emph{Objaverse}} Avg.} & \multicolumn{3}{c}{\textbf{\emph{ABO}} Avg.} \\
& fundamental encoder & Top1 & Top3 & Top 5 &  Top1 & Top3 & Top 5 \\
 \midrule  \midrule
TAMM* & - &  22.70 & 38.83 & 47.13 & 35.44 & 54.96 & 63.40 \\
ReCon* & - &  23.40 & 41.41 & 48.95 & 34.29 & 55.14 & 67.69	 \\
Uni3D* & - &  30.47 & 48.46 & 55.87 & 37.79 & 61.08 & 69.04	 \\
\midrule
UniGS(Ours)* & \ding{55} & 38.57 & 60.54 & 68.96 &46.97 & 69.91 & \textbf{79.38}	\\
UniGS(Ours)\dag & \ding{51} &  \textbf{38.74} & \textbf{62.89} & \textbf{71.88} &	\textbf{47.53} & \textbf{70.49} & 78.60	\\

\bottomrule

\end{tabular}
  \label{tab:unfrozen_uni3d}
  % \vspace{-8mm}
\end{table*}

We freeze the Uni3D encoder as our UniGS only leverages spatial information of point clouds for explicit feature learning. Therefore, we opt to freeze the Uni3D encoder, utilizing it solely for guidance, which leads to relationship modeling and feature understanding of the 3DGS explicit features, instead of directly training our point encoder on 3DGS.

Moreover, we provide additional experiments with unfrozen fundamental encoder in \cref{tab:unfrozen_uni3d}. Despite the doubled demand for computing resources, UniGS with an unfrozen fundamental encoder still achieves comparable performance.

%------------------------------------------------------------------------------------------------------------------------%

\section{Generalization to 3DGS-driven methods}

\begin{table*}[t]
		\centering
  \caption{\textbf{Zero-shot classification on Objaverse with other 3DGS-driven methods.} Avg. denotes mean average classification accuracy. Results illustrate the ability of UniGS to migrate to other 3DGS-driven methods. 2DGS denotes "\emph{2D Gaussian Splatting for Geometrically Accurate Radiance Fields}".
  }
  \vspace{-2mm}
  \addtolength{\tabcolsep}{5pt}
  % \setlength{\tabcolsep}
 \begin{tabular}{ c | c c c }
\toprule
Representation & \multicolumn{3}{c}{Avg.} \\
% \midrule
 Training \& Testing & Top1 & Top3 & Top 5 \\
 \midrule  \midrule
 3DGS & 38.57 & 60.54 & 68.96 \\
 \textbf{2DGS} & 38.11 \textcolor{blue}{(-0.46)} & 61.28 \textcolor{orange}{(+0.74)} & 70.32 \textcolor{orange}{(+1.36)}  \\
\bottomrule

\end{tabular}
  \label{tab:2dgs_classification}
  \vspace{-3mm}
\end{table*}

We provide more comprehensive results of the robustness to 3DGS-derived methods with UniGS trained on 2DGS representation. As shown in the Table above, UniGS pre-trained on 2DGS achieve similar performance compared with UniGS pre-trained on 3DGS, illustrating the power of our proposed Gaussian-Aware Guidance and the ability to migrate to other 3DGS-derived methods.

%------------------------------------------------------------------------------------------------------------------------%

\section{Zero-shot Image-driven retrieval}
\begin{table}[t]
		\centering
  \addtolength{\tabcolsep}{-0.5pt}
 \begin{tabularx}{\textwidth}{ l | c c c | c| c |c }
\toprule
\multirow{2}{*}{Methods}  & \multicolumn{3}{c|}{Avg.} & \multirow{2}{*}{Representation} & \multirow{2}{*}{Text-image Model} & \multirow{2}{*}{Embedding dim} \\
% \midrule
  & Top1 & Top3 & Top 5 & & \\
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{Objaverse-Lvis}}} \\
    \midrule
CLIP$^2$ & 28.83 & 51.43 & 63.57 & point cloud & \multirow{5}{*}{ViT-B-16} & \multirow{5}{*}{512}\\
CLIP$^2$ & 27.99 & 50.30 & 62.76 & 3DGS &  & \\
Uni3D* & 39.65 & 60.72 & 70.51 & point cloud &  & \\
Uni3D & 35.82 & 58.35 & 69.63 & 3DGS &  & \\
Uni3D* & 34.74 & 56.70 & 67.12 & 3DGS &  & \\
\midrule
\rowcolor{mygray}UniGS(Ours) & \textbf{41.78} & \textbf{62.50} & \textbf{72.24} & 3DGS & ViT-B-16 & 512\\

 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{ABO}}} \\
\midrule
 CLIP$^2$ & 15.29 & 31.74 & 42.74 & point cloud & \multirow{5}{*}{ViT-B-16} & \multirow{5}{*}{512}\\
CLIP$^2$ & 13.80 & 29.60 & 40.85 & 3DGS &  & \\
 Uni3D* & 18.25 & 35.26 & 45.29 & point cloud &  & \\
Uni3D & 21.14 & 38.88 & 49.38 & 3DGS &  & \\
 Uni3D* & 25.30 & 45.69 & 57.51 & 3DGS & & \\
\midrule
\rowcolor{mygray} UniGS(Ours) & \textbf{26.69} & \textbf{46.26} & \textbf{56.72} & 3DGS & ViT-B-16 & 512\\

 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{MVImgNet}}} \\
    \midrule
CLIP$^2$ & 6.900 & 17.65 & 26.16 & point cloud & \multirow{5}{*}{ViT-B-16} & \multirow{5}{*}{512}\\
 CLIP$^2$ & 2.160 & 6.330 & 10.12 & 3DGS &  & \\
 Uni3D* & 6.380 & 16.65 & 25.50 & point cloud &  & \\
 Uni3D & 1.410 & 4.260 & 6.840 & 3DGS &  & \\
Uni3D* & 7.940 & 18.86 & 27.20 & 3DGS &  & \\
\midrule
\rowcolor{mygray} UniGS(Ours) & \textbf{10.55} & \textbf{23.75} & \textbf{33.15} & 3DGS & ViT-B-16 & 512\\
\bottomrule
\end{tabularx}

		\caption{\textbf{Zero-shot Image-driven retrieval. Avg.: }the mean average retrieval accuracy. * denotes training from scratch. }
  \label{tab:image_driven_retrieval}
  \vspace{-6mm}
\end{table}

As shown in Table~\ref{tab:image_driven_retrieval}, we further evaluate UniGS with Zero-shot Image-driven retrieval in batch, which reveals the alignment between 3DGS and the image domain. Experiment results on ABO and MVImgNet in Table~\ref{tab:image_driven_retrieval} demonstrate the power of 3DGS in Image-3D alignment.

\section{Future work}

To further validate the scaling ability of UniGS for better performance and 3D understanding. The next step of UniGS is to conduct it on large 3DGS datasets with more 3D points and further explore model architectures for language-image-3D alignment. We will establish more pipelines that support the conversion of various datasets into 3DGS and further expand the scale of the 3DGS datasets. Another interesting future direction will be the model architecture exploration, where we plan to further explore the connection between fundamental encoder and advanced encoder, and attempt to support training on point cloud datasets for larger datasets and benchmarks.

\section{Social Impact}
\label{sec:social_impact}
While there is a wide range of application domains where 3D representation learning will be beneficial, such as in autonomous driving, augmented/virtual reality, and embodied AI, there are also potentially negative application scenarios. For instance, these approaches could be used in malicious contexts to obtain private image data through splatting with a specific decoder or for surveillance purposes. Consequently, UniGS is released as a research tool to benefit the academic field only.

\section{Discussion}
\label{sec:discussion}

\subsection{Compared to NeRF-based approaches}
\begin{table}
		\centering
  \addtolength{\tabcolsep}{-2pt}
 \begin{tabular}{ l | c c}
\toprule
Methods & 3D Representation & Avg.(\%)$\uparrow$ \\
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{ShapeNetRender}}} \\
    \midrule
CLIP(1 view) & --- & 73.60 \\
CLIP(16 view) & --- & 82.40 \\
nerf2clip & NeRF & 84.00 \\
nf2vec & NeRF & 87.30 \\
Uni3D & 3DGS location & 88.96 \\
\midrule
\rowcolor{mygray}UniGS(Ours) & 3DGS & \textbf{93.94} \\
\bottomrule
\end{tabular}

		\caption{\textbf{Zero-shot classification on ShapeNetRender. Avg.: } the mean average Top1 classification accuracy. Uni3D and UniGS is trained for 15 epoch on ShapeNetRender. }
        \label{tab:discussion_nerf_comparison}
\end{table}

As shown in \cref{tab:discussion_nerf_comparison}, we conduct additional comparisons with NeRF-based approaches and UniGS outperforms nerf2clip\citep{ballerini2024connecting} and nf2vec\citep{ramirez2023deep} with 9.93\% and 6.64\%, demonstrating significant improvement over NeRF-based approaches on cross-modalities learning.

\subsection{UniGS vs Uni3D}
\begin{figure*}[h]
    \centerline{\includegraphics[width=\textwidth]{figure/motivation_new.png}}
    \caption{\textbf{Left:} Uni3D, using 3D point clouds for text-image-3D pre-training. \textbf{Middle}: our \name{}, leveraging 3DGS as the 3D representation with better alignment with image modality. \textbf{Right:} our \name{} learns a more general and stronger multi-modal representation.}
    \label{fig:discussion_vs_uni3d}
\end{figure*}
As shown in \cref{fig:discussion_vs_uni3d}, we highlight the difference between UniGS and Uni3D. Our UniGS is also capable of utilizing a single model to unify 3D representations from different models, with better performance with 3DGS representation and proposed Gaussian-Aware Guidance.
Specifically, when using point clouds as a unified 3D representation, the main challenge is the divergence between the 3D representation and other modalities. 
In contrast, UniGS leverages 3DGS as the 3D representation, which effectively reconstructs the 3D target object as well as provides efficient correspondences between 3D and 2D images.


\begin{table*}[h]
		\centering
  \caption{\textbf{Comparison results with 10000 points dataset on Objaverse-Lvis zero-shot classification.} \dag ~denotes fine-tuning on 3DGS datasets.}
  \vspace{-2mm}
 \begin{tabular}{ c | c | c c c | c | c }
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Backbone} & \multicolumn{3}{c|}{Avg.}  & \multirow{2}{*}{Representation} & Augment w. \\
 & & Top1 & Top3 & Top 5 & & point clouds  \\
     \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{10000 3D points}}} \\
    \midrule
 Uni3D  & \multirow{3}{*}{EVA02-S-patch14} & 50.34 & 72.70 & 79.81 & point clouds & --- \\
   UniGS & & 52.44 & 75.37 & \textbf{82.71} & 3DGS & \cmark \\
    UniGS\dag & & \textbf{53.16} & \textbf{75.59} & 82.14  & 3DGS & \xmark \\
\bottomrule
\end{tabular}
\label{tab:vs_uni3d}
\end{table*}

Moreover, we conducted additional experiments to fine-tune UniGS using a pure 3DGS dataset. As shown in \cref{tab:vs_uni3d}, UniGS outperforms Uni3D when augmented with point clouds under the same settings as Uni3D. It also achieves higher Top-1 and Top-3 accuracy after fine-tuning on the pure 3DGS dataset. Experimental results in \cref{tab:classification} further demonstrate that Uni3D is not fully compatible with both point clouds and 3DGS. However, with our proposed Gaussian-Aware Guidance, UniGS exhibits the ability to effectively understand objects in both point clouds and 3DGS, achieving superior results after fine-tuning.

\subsection{Runtime analysis}

\begin{table}[h]
		\centering
  \addtolength{\tabcolsep}{-2pt}
  \caption{\textbf{Comparisons of forward computational cost on Objaverse-Lvis. }}
 \begin{tabular}{ l | c c | c }
\toprule
Methods & FLOPs(G) $\downarrow$ & Time(ms)$\downarrow$ & Top 1 Avg.\\
    \midrule
% ULIP(PointMLP) & 62.70 & 265 \\
CLIP$^2$ & 22.49 & 232 & 10.20\\
TAMM & 22.49 & 233 & 22.70 \\
Uni3D  & 47.85 & 113 & 30.47\\
UniGS(Ours) & 98.17 & 233 & \textbf{38.57}\\
\bottomrule
\end{tabular}
        \label{tab:computational_cost}
\end{table}

\begin{figure*}[h]
    \centerline{\includegraphics[width=\textwidth]{rebuttal/flops_modules_analysis.png}}
    \caption{\textbf{Additional ablation study on the FLOPs of UniGS.} (a) FLOPs of each module, (b) 3D Encoder of UniGS. 76.5\% of FLOPs are due to the CNN layers of the fundamental and the advanced encoder to extract 3D spatial features, while only 23.5\% of the FLOPs is spent for 3D understanding.}
    \label{fig:discussion_runtime_analysis}
\end{figure*} 

\begin{table}[h]
		\centering
  \addtolength{\tabcolsep}{-2pt}
 \begin{tabular}{ c c | c c | c c | c}
\toprule
\multicolumn{2}{c|}{Fundamental Encoder} & \multicolumn{2}{c|}{Advanced Encoder} & \multirow{2}{*}{Cross-Attn} & \multirow{2}{*}{Others} & \multirow{2}{*}{FLOPs} \\
 CNN layers & ViT blocks & CNN layers & ViT blocks & & & \\
\midrule

\cmark & \xmarkg & \xmarkg & \xmarkg & \xmarkg & \xmarkg & 36.67 \\
\cmarkg & \cmark & \xmarkg & \xmarkg & \xmarkg & \xmarkg & 47.60 \\
\cmarkg & \cmarkg & \cmark & \xmarkg & \xmarkg & \xmarkg & 84.31 \\
\cmarkg & \cmarkg & \cmarkg & \cmark & \xmarkg & \xmarkg & 95.24 \\
\cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmark & \xmarkg & 95.43 \\
\cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmarkg & \cmark & 95.94 \\

\bottomrule
\end{tabular}

		\caption{\textbf{Ablation study on the FLOPs of UniGS modules.} CNN encoder denotes the CNN layers to extract spatial information from 3D representation into features, and ViT blocks denotes the Transformer blocks understanding object from extracted features. Cross-Attn denotes the Cross-attention layers between Fundamental and Advanced Encoder.}
        \label{tab:computational_cost_ablation}
\end{table}

As shown in the \cref{tab:computational_cost}, we further evaluate the FLOPs and runtime of UniGS and compare them with state-of-the-art approaches. With a slight increase in runtime, UniGS achieves significant improvement over CLIP$^2$, TAMM, and Uni3D on Objaverse-Lvis zero-shot classification.

Moreover, as shown in the \cref{tab:computational_cost_ablation} and \cref{fig:discussion_runtime_analysis}, we present an additional ablation study of UniGS modules on FLOPs in \cref{fig:discussion_runtime_analysis}. Specifically, this helps us understand the difference as 76.5\% of the total FLOPs (73.38G) is due to the CNN layers of the 3D Encoder to extract 3D spatial features.

Fortunately, much progress is being made in compressing models\citep{yang2024t3dnet} and 3D representations\citep{fan2023lightgaussian}, and we expect these advances to facilitate the development of 3D understanding with 3DGS representation.

\subsection{Visual comparisons}
\begin{figure*}[h]
    % \setlength{\belowcaptionskip}{-15pt}
    \centerline{\includegraphics[width=\textwidth]{figure/image_to_3D_retrieval_comparisons.png}}
    \caption{\textbf{Visual comparisons to Uni3D on Image-to-3D retrieval.} With 3D Gaussian Splatting representation capturing image details and a powerful Gaussian-Aware Guidance module, UniGS outperforms Uni3D in 3D understanding of object color, shape, and texture.}
    \label{fig:discussion_visual_comparisons}
\end{figure*}

As shown in \cref{fig:discussion_visual_comparisons}, Uni3D may mistakenly retrieve another similar object due to the similarity in point cloud structure. In contrast, UniGS demonstrates a superior 3D understanding of object color, shape, and texture with 3DGS representation and proposed Gaussian-Aware Guidance, resulting in better Image-to-3D retrieval. 