\begin{figure*}%[ht]
    \centerline{\includegraphics[width=\textwidth]{figure/model_overview_new.png}}
    \caption{\textbf{The overview of UniGS.} UniGS is an innovative, unified, and scalable 3D pretraining framework designed for 3D representation learning. It offers versatile pipelines for various datasets, enabling the efficient 3DGS acquisition to enhance 3D representation learning with SoTA CLIP models. UniGS demonstrates exceptional performance across a broad range of benchmarks.}
    \vspace{-3mm}
    \label{fig:framework_overview}
\end{figure*}

\vspace{-3mm}
\section{Methodology}
\label{sec:methodology}
\vspace{-3mm}
In this section, we introduce our proposed \name{} in detail. We first review background information of 3DGS and next provide an overview of UniGS in \cref{sec:preliminaries}. 
The proposed cross-modal contrastive learning framework for multi-modal alignment is then presented in \cref{sec:CRA} before we introduce the details of the Gaussian-Aware Guidance in \cref{sec:GAG}. We further present details on scaling up and initializing \name{} in \cref{sec:S3B}. Finally, we present how we ensemble 3DGS datasets from existing datasets in \cref{sec:E3D}.


%%%------------------------------------------------------------%%%


\subsection{Preliminaries and Overview}
\label{sec:preliminaries}
\noindent\textbf{3D Gaussian splatting.}
3D Gaussian splatting~\citep{kerbl20233d} proposed an explicit model design and efficient differentiable rendering implementation, which enable faster training and high-quality performance. 3DGS employs a collection of anisotropic Gaussian primitives, denoted as \( G = \{g_1, g_2, \ldots, g_N\} \), to represent the scene. Each 3D Gaussian sphere $g$ can be parameterized by the following attributes: (1) located at 3D position $\mu\in\mathcal{R}^{3}$ (2) with color defined by SH $(3+c\in\mathcal{R}^{k})$, (3) opacity $\alpha\in[0,1]$ and a 3D shape decomposed into (4) a scaling factor $s\in\mathcal{R}^{3}_{+}$ and (5) a rotation quaternion $r\in\mathcal{R}^{4}$, where $k$ denotes the freedom of SH basis. Clearly, a Gaussian sphere can be characterized by a high-dimensional feature representation concatenating its position $\mu$, color $c$, opacity $\alpha$, scaling factor $s$, and rotation $R$, where
\begin{equation}
    g = (\mu,c,\alpha,s,R).
\end{equation}

In the rendering process, a splatting pipeline is utilized, projecting 3D Gaussians onto the 2D image plane. This projection splats the 3D Gaussians into 2D counterparts on the image plane, which are then blended using the $\alpha$-blending algorithm to determine the final color composition.

\begin{equation}
    \label{eq:3dgs4}
    \textbf{C}=\underset{i\in N}{\sum}\textbf{c}_{i}\alpha_i\underset{j=1}{\overset{i=1}{\prod}}(1-\alpha_i), \boldsymbol{\Sigma} = \textbf{R}\textbf{S}\textbf{S}^T\textbf{R}^T
\end{equation}

where $c_i$ denotes the color defined by spherical harmonics~(SH)~\citep{kerbl20233d} coefficients of each 3D Gaussian, \textbf{R} and \textbf{S} are the matrix representation of $R$ and $s$, $\alpha_i$ is calculated by the multiplication of a 2D Gaussian with covariance $\boldsymbol{\Sigma}$ and a learned per-point opacity~\citep{yu2021plenoctrees}.

\label{sec:Overview}
\textbf{Overview of UniGS.} \name{} facilitates 3D multi-modal representation learning by leveraging the informative and effective 3DGS representation and supports open-world learning. The overview of \name{} is depicted in~\cref{fig:framework_overview}. 
%
With Image Encoder and Text Encoder from pre-trained Text-image aligned model~\citep{radford2021learning} for a shared latent space, UniGS formulates a parallel-structure dual-branch encoder, which adopts a frozen pre-trained point cloud encoder as the fundamental encoder for priors and leverages another encoder as the advanced encoder modeling high-level information.

\subsection{Cross-modal Representation Alignment} 
\label{sec:CRA}

To align multi-modal representations of text, image, and 3D domains, \name{} adopts the pre-trained language-image model CLIP~\citep{radford2021learning} to provide a common language-image latent space that serves as the target latent space to align 3DGS representation to (see \cref{fig:framework_overview}). To facilitate the transferability of the learned representations and enable zero-shot/open-word recognition, the text and image encoders of the CLIP model which defines the common latent space are frozen. 
%
In particular, we take inspiration from the contrastive loss in~\citep{radford2021learning, zeng2023clip2} and propose $\textbf{Language-3DGS}$ and $\textbf{Image-3DGS Alignment}$ losses to bridge the domain gap among the different modalities.

\noindent\textbf{Language-3DGS Alignment.} 
Given a text-image-3DGS triplet, $\{X_T, X_I, X_G\}$, text features, $f^T\in\mathbb{R}^{C^T}$, image features, $f^I\in\mathbb{R}^{C^I}$, and 3DGS features, $f^G\in\mathbb{R}^{C^G}$ can be obtained through the corresponding modality encoders. The contrastive loss between the text and 3D modality is then utilized to align the text and 3DGS feature representations. Let $N$ denotes the batch size and $\tau$ the temperature coefficient, the Language-3DGS Alignment training objective $L(T,G)$ can be described as: 

\begin{equation}
    \label{eq:text-3dgs-loss}
    L(T,G) = \frac{1}{N}\underset{i\in N}{\sum}\mathcal{L}(i,T,G)=
     -\frac{1}{N}\underset{i\in N}{\sum}\log \frac{\exp(f_{i}^{T}\cdot f^{G}_{i}/\tau)}{\exp(f_{i}^{T}\cdot f^{G}_{i}/\tau)+\underset{j\in N,X_{i}^{T}\neq X_{j}^{T}}{\sum}\exp(f_{i}^{T}\cdot f^{G}_{j}/\tau)}
\end{equation}

\noindent\textbf{Image-3DGS Alignment.} Similarly, we apply the contrastive loss to align the image and 3DGS features. The Image-3DGS Alignment objective $L(I,G)$ is defined as:

\begin{equation}
    \label{eq:image-3dgs-loss}
    L(I,G) = \frac{1}{N}\underset{i\in N}{\sum}\mathcal{L}(i,I,G)= -\frac{1}{N}\underset{i\in N}{\sum}\log \frac{\exp(f_{i}^{I}\cdot f^{G}_{i}/\tau)}{\exp(f_{i}^{I}\cdot f^{G}_{i}/\tau)+\underset{j\in N,j\neq i}{\sum}\exp(f_{i}^{I}\cdot f^{G}_{j}/\tau)},
\end{equation}

Following \citep{zeng2023clip2}, the final cross-modal contrastive learning objective $L_{CM}(T,I,G)$ can be obtained by combining the text-3DGS and image-3DGS alignment objective, namely, $L(T,G)$ and $L(I,G)$:

\begin{equation}
    \label{eq:main-loss}
    L_{CM}(T,I,G) = \lambda_1 L(T,G) + \lambda_2 L(I,G),
\end{equation}

where both hyper-parameters, $\lambda_1$ and $\lambda_2$, are set to 0.5.

%%%------------------------------------------------------------%%%

\begin{figure*}%[ht]
    \centerline{\includegraphics[width=\textwidth]{figure/model_architecture_new.png}}
    \caption{\textbf{Model overview of UniGS.} Let $\mu, c, \alpha, s, R$ denote the location, color, opacity, scale, and rotation attribute of 3DGS. (a) Given a 3DGS input, the pre-trained and frozen branch takes 3DGS locations and color as input while the second branch, which is initialized from scratch, focuses on the 3DGS location and the remaining attributes. (b) shows the details of our 3D Encoder and how the prior is leveraged through cross-attention layers.}
    \vspace{-3mm}
    \label{fig:model_overview}
\end{figure*}

\subsection{Gaussian-Aware Guidance}
\label{sec:GAG}
Admittedly, projecting point clouds into 3D voxels with a 3D backbone~\citep{qi2017pointnet, qi2017pointnet++} can be helpful for understanding the relationships between global position and feature derived from non-positional information. However, we observe that the explicit feature of 3DGS will be ignored as the voxelization procedure loses the shape and texture information. To address this issue, a Transformer-based model pattern is adopted for feature-context learning and model scalability. 

For better modeling and understanding of the 3D domain represented by 3DGS, we further propose a Gaussian-aware Guidance module. Specifically, this module, as highlighted in the dashed box in Fig.~\ref{fig:model_overview}a, formulates a dual-branch 3D encoder, where the fundamental encoder $E_{fun}$ leverages the 3D ViT Encoder pre-trained on 3D point clouds from \citep{zhou2024uni3d} to model the low-level features $f_{fun}$ including spatial and color information  of 3DGS, while the advanced encoder $E_{adv}$ additionally models the high-level features $f_{adv}$, the relationship between spatial connections and 3DGS feature. 
As the number of Gaussian spheres allocated to an object or scene represented by 3DGS varies, a group divider is leveraged to process the input 3DGS into a fixed number of Gaussian spheres.
The CNN layers map low-dimensional raw features to a high-dimensional feature space, consistent with Uni3D. Moreover, cross-attention layers are built to extract guidance of embeddings from the fundamental encoder and improve the alignment of the 3DGS features with other domains. We denote the process as $CA$, where
\begin{equation}
    CA = softmax(\frac{Q_{fun}K_{adv}^T}{\sqrt{d_k}})V_{adv}.
\end{equation}
Finally, we concatenate the features of $f'_{fun}$ and $f'_{adv}$, respectively generated by the fundamental encoder and the advanced encoder, and then map it through an MLP to the same dimension as the pre-trained image-text model. 
Let SA be the acronym of self-attention and $f_{\theta}(\cdot)$ denote the process of group divider and CNN layers.
The process of encoding 3DGS features based on Gaussian-Aware Guidance can then be represented by:
\begin{equation}
    f_{fun} = (\mu,c), f_{adv} = (\mu,\alpha,s,R)
\end{equation}
\begin{equation}
    f'_{fun} = SA(f_{\theta}(f_{fun})),f'_{adv} = CA(f_{\theta}(f_{adv}))
\end{equation}
\begin{equation}
    f^{G} = MLP(concat(f'_{fun},f'_{adv}))
\end{equation}

\subsection{Scaling Up 3DGS Backbones}
\label{sec:S3B}

\textbf{Scaling Up UniGS.} 
Previous works have achieved good performance by designing specific model architectures and shown case effectiveness in various applications. However, these methods are either limited to a certain small-scale dataset or data sources like point clouds which are expensive to collect. Instead, with recent advances in large-scare multi-view datasets~\citep{yu2023mvimgnet}, our method adopts 3DGS representations and designs a ViT-based Encoder to encode the 3D modality. Therefore, our model can naturally solve the difficulties by simply scaling up the data and model size with well-studied unified scaling-up strategies.

\textbf{Initializing UniGS.}
Restricted by the scale of the 3D dataset, directly pre-training each 3D backbone for specific 3D tasks leads to expensive training costs and may suffer from difficulties in convergence or overfitting. To overcome this issue, we follow Uni3D~\citep{zhou2024uni3d} and adopt off-the-shelf pre-trained large models~\citep{sun2023eva, fang2023eva, radford2021learning, caron2021emerging} with ViT-based structure in the other modalities as the initialization of the 3D backbone to transfer their rich underlying representational abilities to language-image-3D pre-training
%
Different from Uni3D~\citep{zhou2024uni3d}, \name{} further establishes stable cross-modal contrastive learning through the Gaussian-Aware Guidance module, which introduces a new perspective of leveraging pre-trained priors for stabilizing the learning of large-scale 3D representations, and provide guidance on spatial information for understanding and aligning 3DGS representations with other modalities.  

\begin{table}[t]
\centering
  \addtolength{\tabcolsep}{-0.8pt}
 \begin{tabularx}{\textwidth}{  l | c c c | c  |c  | c }
\toprule
\multirow{2}{*}{Methods}  & \multicolumn{3}{c|}{Avg.} & \multirow{2}{*}{Representation} & \multirow{2}{*}{Text-image Model} & \multirow{2}{*}{Embedding dim} \\
% \midrule
 & Top1 & Top5 & Top 10 & & \\

 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{Objaverse (3D dataset with mesh)}}} \\
    \midrule

Uni3D & \textcolor{lightgray}{8.400} & \textcolor{lightgray}{17.50} & \textcolor{lightgray}{22.60} & point cloud & EVA02-E-14-plus & 1024\\
\midrule
 CLIP$^2$ & 7.400 & 22.20 & 32.50 & point cloud & ViT-B-16 & 512\\
  CLIP$^2$ & 6.400 & 20.20 & 30.90 & 3DGS & ViT-B-16 & 512\\
  Uni3D & 2.300 & 8.100 & 12.00 & 3DGS location & EVA02-E-14-plus & 1024\\
  Uni3D* & 16.70 & 37.10 & 48.10 & point cloud & ViT-B-16 & 512\\
  Uni3D & 10.40 & 26.20 & 36.40 & 3DGS & ViT-B-16 & 512\\
  Uni3D* & 15.80 & 35.60 & 47.20 & 3DGS & ViT-B-16 & 512\\
\midrule
\rowcolor{mygray} UniGS(Ours) & \textbf{21.00} & \textbf{39.80} & \textbf{53.50} & 3DGS & ViT-B-16 & 512\\
%
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{ABO (3D dataset with mesh)}}} \\
    \midrule
Uni3D & \textcolor{lightgray}{5.700} & \textcolor{lightgray}{19.19} & \textcolor{lightgray}{29.49} & point cloud & EVA02-E-14-plus & 1024\\
\midrule
CLIP$^2$ & 7.090 & 24.34 & 38.94 & point cloud & ViT-B-16 & 512\\
  CLIP$^2$ & 7.650 & 23.92 & 37.83 & 3DGS & ViT-B-16 & 512\\
  Uni3D & 1.670 & 6.400 & 11.27 & 3DGS location & EVA02-E-14-plus & 1024\\
  Uni3D* & 10.29 & 29.21 & 43.67 & point cloud & ViT-B-16 & 512\\

  Uni3D & 8.070 & 25.31 & 37.41 & 3DGS & ViT-B-16 & 512\\
 Uni3D* & 10.85 & 29.76 & 42.98 & 3DGS & ViT-B-16 & 512\\
 \midrule
 \rowcolor{mygray} UniGS(Ours) & \textbf{11.27} & \textbf{30.32} & \textbf{43.95} & 3DGS & ViT-B-16 & 512\\
%
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{MVImgNet (Multi-view dataset)}}} \\
    \midrule
 CLIP$^2$ & 9.560 & 31.00 & 42.19 & point cloud & ViT-B-16 & 512\\
 CLIP$^2$ & 12.12 & 31.93 & 45.45 & 3DGS & ViT-B-16 & 512\\
 Uni3D & 1.400 & 7.690 & 11.42 & 3DGS location & EVA02-E-14-plus & 1024\\
Uni3D* & 17.72 & 48.95 & 62.24 & point cloud & ViT-B-16 & 512\\
 Uni3D & 9.090 & 29.14 & 39.63 & 3DGS & ViT-B-16 & 512\\
 Uni3D* & 17.02 & 47.09 & 60.61 & 3DGS & ViT-B-16 & 512\\
 \midrule
 \rowcolor{mygray} UniGS(Ours) & \textbf{18.65} & \textbf{53.38} & \textbf{66.90} & 3DGS & ViT-B-16 & 512\\
\bottomrule
\end{tabularx}
		\caption{\textbf{Top1, Top5 and Top10 Text-3D retrieval accuracy. Avg.: }the mean average retrieval accuracy. * denotes training from scratch.}
  \label{tab:retrieve}
  \vspace{-5mm}
\end{table}

\vspace{-3mm}
\subsection{Ensembling 3DGS datasets}
\label{sec:E3D}
\vspace{-3mm}

To create 3DGS-text-image triplets for training, we over-sample points from the mesh surface uniformly to capture the details of each object. We random sample $N$ point clouds to initialize the 3D Gaussians, where $N$ is set to 1024 in Objaverse, ABO, and MVImgNet. 
%
Next, we use rendered images for 3DGS optimization. More specifically, for ABO~\citep{collins2022abo}, we uniformly render 72 images covering the whole shape, while we obtain the rendered image from \citep{liu2023zero} for the Objaverse dataset. Finally, we collect and clean the caption for each dataset. We clean the given caption of ABO and sort ABO objects into 23 classes with 7929 items, while human-verified and machine-generated high-quality captions from \citep{luo2024scalable, dong2024internlm} are utilized for Objaverse.
%
During the optimization of 3DGS, we control the number of 3D Gaussians by adjusting the 3DGS optimization scheme. In particular, after each 3DGS duplicating and pruning step, we sort the 3D Gaussians by opacity and only keep the top $N$. To facilitate better optimization results for the SUN RGB-D dataset, we do not restrict $N$ to 1024 as it is under a sparser setting with a single image provided for each scene.

