\section{Introduction}
\label{sec:intro}
 
The remarkable success of 2D Image-Text pre-training through modality alignment via contrastive learning~\citep{radford2021learning, sun2023eva, fang2023eva, schuhmann2022laion, qi2020imagebert, changpinyo2021conceptual, hong2021gilbert} has recently inspired a line of work pursuing 3D pre-training~\citep{xue2023ulip, xue2023ulip2, zeng2023clip2, zhou2024uni3d, liu2024openshape, zhang2022pointclip, huang2023clip2point, afham2022crosspoint}.
Leveraging diverse large-scale 3D datasets, recent works such as ~\citep{liu2024openshape, zhou2024uni3d} expand the traditional 2D task to text-image-3D pertaining by including point clouds as 3D representations, resulting in considerable improvements in 3D zero-shot/open-world object detection, classification and retrieval tasks.
While point clouds serve as a natural step towards 3D representations, there are inherent limitations when using them to represent 3D objects. 
In particular, as illustrated in Fig. \ref{fig:motivation}, point clouds consist of a discrete set of points and thus struggle to accurately capture fine-grained geometric details and surface textures of common objects, limiting the performance of 3D representation learning approaches.
Moreover, there exists a noticeable gap between the discrete points and the dense 2D pixels of images, which further hinders the learning of joint multi-modal representations.

On the other hand, 3D Gaussian Splatting (3DGS)~\citep{kerbl20233d} has recently revolutionized 3D scene representations, and offers a promising and more efficient alternative to facilitate 3D representation learning. Specifically, 3DGS models scenes as a set of 3D Gaussians, which effectively reconstruct the 3D target object as well as provide efficient correspondence between 3D and 2D images through the splatting rendering algorithm.
Furthermore, 3DGS offers the advantage of utilizing a more diverse range of data sources as it can leverage multi-view images or COLMAP data~\citep{yu2023mvimgnet, schonberger2016structure} for optimization with minimal overhead and collection time. Additionally, existing point cloud datasets can be used as the initialization for 3D Gaussian locations, further enhancing the capabilities of 3DGS.

However, simply retraining existing multi-modal frameworks like CLIP$^2$~\citep{zeng2023clip2} or Uni3D~\citep{zhou2024uni3d} to leverage 3DGS as the 3D representation is not effective. 
As the spatial connections of 3DGS may be insufficient to capture and express objects due to the fact that 3DGS are not necessarily distributed on the surface of objects.

To this end, we propose \name{}, which leverages 3DGS as the 3D representation for unified language-image-3D pre-training and enhances the performance of 3D understanding. 
To better model and understand the explicit features of 3DGS,
UniGS additionally proposes a novel Gaussian-Aware Guidance module. 
Specifically, UniGS utilizes a parallel-structure ViT as the 3D encoder consisting of a fundamental encoder and advanced encoder, where the fundamental encoder encodes spatial information together with color and the advanced encoder the spatial information with the remaining 3D Gaussian attributes, where pre-trained models can be leveraged for initialization of the fundamental encoder.
With priors extracted from the fundamental encoder, the advanced encoder aggregates priors through cross-attention layers for guiding the 3DGS feature learning, unlocking the superior performance that comes with leveraging 3DGS.
%
Through extensive experiments across the Objaverse~\citep{deitke2023objaverse}, ABO~\citep{collins2022abo}, MVImgNet~\citep{yu2023mvimgnet} and SUN RGBD~\citep{song2015sun} datasets and various tasks, we demonstrate the effectiveness of \name{} in learning a more general and stronger multi-modal representation.
%
Specifically, \name{} achieves state-of-the-art results across different 3D tasks with remarkable improvements, including zero-shot classification (+9.36\%), text-driven retrieval (+4.3\%), and open-world understanding (+7.92\%).
Our contributions can be summarized as follows: 
\begin{itemize}
    \item We propose \name{}, a novel unified text-image-3D pre-training framework, which leverages 3DGS as the 3D representation for learning a more general and stronger multi-modal representation.
    \item We propose a novel Gaussian-Aware Guidance module to leverage priors from pre-trained point clouds encoders to guide the learning of the Gaussian features for better 3D understanding.
    \item Our proposed approach achieves state-of-the-art performance on various challenging datasets, demonstrating the effectiveness in learning strong cross-model representations.
\end{itemize}
