\begin{table}[t]
		\centering
  \addtolength{\tabcolsep}{-0.5pt}
 \begin{tabularx}{\textwidth}{ l | c c c | c| c |c }
\toprule
\multirow{2}{*}{Methods}  & \multicolumn{3}{c|}{Avg.} & \multirow{2}{*}{Representation} & \multirow{2}{*}{Text-image Model} & \multirow{2}{*}{Embedding dim} \\
% \midrule
  & Top1 & Top3 & Top 5 & & \\
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{Objaverse-Lvis (3D dataset with mesh)}}} \\
    \midrule
Uni3D & \textcolor{lightgray}{38.17} & \textcolor{lightgray}{59.92} & \textcolor{lightgray}{67.18} & point cloud & EVA02-E-14-plus & 1024\\
\midrule
CLIP$^2$ & 12.35 & 24.62 & 32.91 & point cloud & ViT-B-16 & 512\\
CLIP$^2$ & 10.20 & 20.47 & 27.71 & 3DGS & ViT-B-16 & 512\\
Uni3D & 5.130 & 11.20 & 13.27 & 3DGS location & EVA02-E-14-plus & 1024\\
Uni3D* & 36.72 & 57.09 & 65.18 & point cloud & ViT- B-16 & 512\\
Uni3D & 18.48 & 34.39 & 43.31 & 3DGS & ViT-B-16 & 512\\
Uni3D* & 30.47 & 48.46 & 55.87 & 3DGS & ViT-B-16 & 512\\
\midrule
\rowcolor{mygray}UniGS(Ours) & \textbf{38.57} & \textbf{60.54} & \textbf{68.96} & 3DGS & ViT-B-16 & 512\\

 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{ABO (3D dataset with mesh)}}} \\
    \midrule
Uni3D & \textcolor{lightgray}{68.94} & \textcolor{lightgray}{90.49} & \textcolor{lightgray}{94.15} & point cloud & EVA02-E-14-plus & 1024\\
\midrule
 CLIP$^2$ & 22.58 & 43.83 & 54.56 & point cloud & ViT-B-16 & 512\\
CLIP$^2$ & 19.06 & 38.48 & 48.71 & 3DGS & ViT-B-16 & 512\\
 Uni3D & 13.34 & 28.28 & 42.20 & 3DGS location & EVA02-E-14-plus & 1024\\
 Uni3D* & 37.60 & 59.68 & 70.22 & point cloud & ViT-B-16 & 512\\
Uni3D & 27.57 & 50.60 & 63.69 & 3DGS & ViT-B-16 & 512\\
 Uni3D* & 37.79 & 61.08 & 69.04 & 3DGS & ViT-B-16 & 512\\
\midrule
\rowcolor{mygray} UniGS(Ours) & \textbf{46.97} & \textbf{69.91} & \textbf{79.38} & 3DGS & ViT-B-16 & 512\\

 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{MVImgNet (Multi-view dataset)}}} \\
    \midrule
CLIP$^2$ & 5.030 & 12.33 & 16.96 & point cloud & ViT-B-16 & 512\\
 CLIP$^2$ & 4.310 & 11.24 & 14.89 & 3DGS & ViT-B-16 & 512\\
 Uni3D & 3.680 & 10.72 & 14.92 & 3DGS location & EVA02-E-14-plus & 1024\\
 Uni3D* & 5.410 & 12.51 & 16.68 & point cloud & ViT-B-16 & 512\\
 Uni3D & 7.020 & 15.18 & 21.05 & 3DGS & ViT-B-16 & 512\\
Uni3D* & 4.920 & 12.42 & 15.70 & 3DGS & ViT-B-16 & 512\\
\midrule
\rowcolor{mygray} UniGS(Ours) & \textbf{7.650} & \textbf{16.96} & \textbf{22.48} & 3DGS & ViT-B-16 & 512\\
\bottomrule
\end{tabularx}

		\caption{\textbf{Zero-shot classification. Avg.: }the mean average classification accuracy. * denotes training from scratch. }
  \label{tab:classification}
  \vspace{-6mm}
\end{table}

\vspace{-3mm}
\section{Experiment}
\label{sec:experiment}
\vspace{-3mm}
In this section, we evaluate the proposed UniGS on a range of 3D understanding tasks, highlighting its ability to learn more expressive and informative 3D representations than prior approaches. In particular, we consider the tasks of object retrieval, zero-shot classification, and scene understanding. We further conduct detailed and comprehensive ablation studies to reveal the impact and power of our design for cross-modal learning.

\vspace{-3mm}
\subsection{Experimental Setup}
\vspace{-3mm}

\textbf{Baseline.} We compare UniGS with \citep{zeng2023clip2,zhou2024uni3d,zhang2024tamm,qi2023contrast,qi2024shapellm}. To further understand how the 3DGS features, the pre-trained weights, and the parallel structure impact 3D representation learning, we retrain the most relevant model (Uni3D) with different settings for a fair comparison. 
%
As for the baseline model, we further report the performance of the original Uni3D model trained with point clouds and the altered version trained with the 3D location attributes (mean value of Gaussians) of 3DGS instead of point clouds for completeness. Note that 3DGS does not necessarily exist on the surface of objects, so there is a certain difference between point clouds and the 3D location attributes of 3DGS.

\textbf{Implementation Details.} Following \cref{sec:E3D}, we collect 146000, 7929, 3483, and 61871 objects, optimized for Objaverse (including Objaverse-LVIS for evaluation only ), ABO, MVImgNet, and SUN RGBD datasets, respectively.
For the retrieval task, we randomly sample 1000 items to form the test set, and use the rest as training set.
We train UniGS with a learning rate of 1e-4 for 15 epochs for the retrieval task and 50 epochs for the zero-shot classification and scene recognition tasks.

\begin{table*}[t]
\footnotesize
		\centering
\addtolength{\tabcolsep}{-3.1pt}
 \begin{tabularx}{\textwidth}{l|c|c|ccccccccccc}
\toprule
Method & Rep. & Avg. & Bed & Bsf. & Chair & Desk & Sofa & Table & Toilet & Btub. & Dresser & NSd.\\
\midrule
Uni3D & point clouds & \textcolor{lightgray}{11.04} & \textcolor{lightgray}{56.74} & \textcolor{lightgray}{14.33} & \textcolor{lightgray}{19.58} & \textcolor{lightgray}{25.03} & \textcolor{lightgray}{22.93} & \textcolor{lightgray}{15.98} & \textcolor{lightgray}{24.62} & \textcolor{lightgray}{24.00} & \textcolor{lightgray}{0.000} & \textcolor{lightgray}{1.690} \\

\midrule

CLIP${^2}$ & point clouds & 41.39 & 1.840 & 14.00 & 68.02 & 30.98 & 45.44 & 7.460 & 13.85 & 0.000 & 15.00 & 3.390 \\

CLIP${^2}$ & 3DGS & 28.50 & 1.470 & 4.000 & 40.03 & 1.640 & 15.20 & 56.72 & 4.620 & 0.000 & 26.25 & 30.51 \\

Uni3D* & point clouds & 61.72 & 63.60 & \textbf{59.67} & 84.33 & \textbf{47.43} & \textbf{79.36} & \textbf{78.97} & 63.59 & 74.67 & 12.92 & 18.93 \\

Uni3D& 3DGS & 54.51 & 58.09 & 19.00 & 80.38 & 17.05 & 62.40 & 47.68 & 56.92 & 48.00 & 7.500 & 11.02\\

Uni3D*& 3DGS & 56.67 & 74.63 & 28.00 & 83.89 & 28.36 & 50.88 & 54.31 & 7.690 & 20.00 & 27.50 & 19.49 \\
\midrule
\rowcolor{mygray} UniGS(Ours) & 3DGS & \textbf{69.64} & \textbf{81.62} & 32.00 & \textbf{87.46} & 17.38 & \textbf{79.36} & 68.74 & \textbf{93.85} & \textbf{96.00} & \textbf{35.00} & \textbf{36.44} \\

\bottomrule
\end{tabularx}
		\caption{\textbf{Recognition on SUN RGBD (dataset with point clouds).} \textbf{Avg.: }the mean average Top1 accuracy across all categories. * denotes training from scratch.
	}
 \label{tab:recognization}
 \vspace{-5mm}
\end{table*}


\subsection{Comparisons to state-of-the-art}
\vspace{-2mm}

To demonstrate the effectiveness of our proposed method, we evaluate \name{} on the Text-3D retrieval, zero-shot classification, and scene understanding tasks and make comparisons with state-of-the-art methods~\citep{zeng2023clip2, zhou2024uni3d}. We further report the results of Uni3D~\citep{zhou2024uni3d} trained with 3DGS for completeness. Note that CLIP$^2$~\citep{zeng2023clip2} is retrained on our collected dataset for fair comparisons.

\textbf{Text-3D retrieval.}
With the learned multi-modal representations of UniGS, we can naturally retrieve 3D shapes from the query text or images. Here, we focus on Text-driven retrieval, due to its importance for 3D asset search and downstream applications. Specifically, we retrieve 3D shapes from the test set by calculating the cosine similarity between the embedding of the query text prompt and 3D shapes in the gallery. We report Top1, Top5, and Top10 accuracy. % of text-driven retrieval.

As shown in \cref{tab:retrieve}, UniGS outperforms the current state-of-the-art approaches across all datasets and improves the Top 1 retrieval accuracy of CLIP$^2$ and Uni3D on the Objaverse dataset by over 13.6\% and 5.2\%, respectively. 
%
For reference we also include the evaluation results of Uni3D on their larger dataset (in gray).

\textbf{Zero-shot classification.} We evaluate the zero-shot classification performance of UniGS on Objaverse-Lvis, ABO, and MVImgNet without accessing their training sets. We reorganized ABO into 44 major categories, then skipped those with items fewer than 50, and formed 23 categories ultimately for classification evaluation. Similarly, we reorganized the Objaverse and MVImgNet datasets into 318 and 95 categories, respectively.

As shown in \cref{tab:classification}, our UniGS significantly outperforms CLIP$^2$, improving performance by over 24\% for both the Objaverse and ABO datasets. Due to the lack of real-world data in the pre-training dataset, the Top1 accuracy of MVImgNet is relatively low. However, UniGS outperforms all the baselines on all datasets, revealing the power of 3DGS representations and the effectiveness of UniGS. 

\textbf{Scene recognition.} We leverage the SUN RGBD dataset~\citep{song2015sun} as the scene data and classify objects into 37 categories following the setting of \citep{song2015sun}. Since SUN RGBD only provides a single image for each scene, we load the point clouds as the initialization for 3D Gaussians and fix them during the entire 3DGS optimizations. Thus the 3DGS locations of SUN RGBD are equivalent to point clouds.

As shown in \cref{tab:recognization}, our UniGS outperforms both CLIP$^2$ and Uni3D, improving performance by 28.25\% and 7.92\% respectively, and achieving an increase of 12.97\% over directly modeling 3DGS. Moreover, the success of UniGS in SUN RGBD shows the robustness of 3DGS representation to the number of multi-view images.

\begin{table*}[t]
		\centering
  % \addtolength{\tabcolsep}{-0.5pt}
  \setlength{\tabcolsep}{3pt}
 \begin{tabularx}{\textwidth}{ l | c c c | c c c | c c }
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{Source} & \multirow{2}{*}{3D points} & \multirow{2}{*}{Backbone}  & \multicolumn{3}{c|}{Avg.} & Training & \multirow{2}{*}{Representation} \\
% \midrule
  & & & & Top1 & Top3 & Top 5 & Dataset & \\
  
 \midrule  \midrule
    \multicolumn{3}{l}{\textit{\textbf{Additional zero-shot comparisons}}} \\
    \midrule
Uni3D & \multirow{5}{*}{no Lvis} & \multirow{5}{*}{1024} & \multirow{5}{*}{EVA02-S} & 36.72 & 57.09 & 65.18 & \multirow{5}{*}{100k}  & point cloud\\
Uni3D & & & & 30.47 & 48.46 & 55.87	 & & 3DGS \\
TAMM & & & & 22.70 & 38.83 & 47.13	 & &  3DGS\\
ReCon & & & & 23.40 & 41.41 & 48.95	 & & 3DGS \\
UniGS(Ours) & & & & \textbf{38.57} & \textbf{60.57} & \textbf{68.96} & & 3DGS \\

\bottomrule
\end{tabularx}
    \vspace{-1mm}
		\caption{\textbf{Summary of the experimental results on Objaverse-LVIS zero-shot classification. Avg.: }the mean average classification accuracy. All methods are trained from scratch.}
  \label{tab:further_comparsions_to_sota}
  \vspace{-3mm}
\end{table*}

\textbf{Further comparisons to additional zero-shot 3D object understanding methods.} As shown in the \emph{Additional zero-shot comparisons} part of \cref{tab:further_comparsions_to_sota}, we supplement extra comparisons to TAMM~\cite{zhang2024tamm} and ReCon~\cite{qi2023contrast} and UniGS significantly outperform SOTA methods, emphasizing the effectiveness of 3DGS representation and proposed Gaussian-aware Guidance. 

\vspace{-3mm}
\subsection{Generalization to point clouds}
\vspace{-1mm}

\begin{table*}[t]
	\centering
  % \vspace{-2mm}
  \addtolength{\tabcolsep}{5pt}
  % \setlength{\tabcolsep}
 \begin{tabular}{ c | c c | c c c }
\toprule
Training & \multicolumn{2}{c|}{Initialization} & \multicolumn{3}{c}{Avg.} \\
% \midrule
 w/ point cloud  & opacity & scale \& rotation & Top1 & Top3 & Top 5 \\
 \midrule  \midrule
\ding{55} & - & - & 46.97 & 69.91 & \textbf{79.38} \\
\ding{51} & 0 & 0 & 46.29 & 68.67 & 76.79 \\
\ding{51} & 0.4 & 0 & 48.49 & 70.06 & 77.55 \\
\ding{51} & 0.4 & 0.4 & \textbf{49.62} & \textbf{70.61} & 77.50\\

\bottomrule

\end{tabular}
      \caption{\textbf{Zero-shot classification with point clouds on ABO.} Avg. denotes mean average classification accuracy. Results illustrate that properly converting point clouds into 3DGS format can improve performance.}
      \label{tab:point_clouds_training}
    \vspace{-6mm}
\end{table*}

To show the capability of UniGS to process vanilla point clouds, we conduct experiments where we convert point clouds into 3DGS to benefit the performance of modeling spatial information.As shown in \cref{tab:point_clouds_training}, we do this by initializing the scale, opacity, and rotation with a default value and show that by including this data in training, UniGS can achieve better performance than trained using 3DGS only. This highlights UniGS's potential to inherit capabilities of the point cloud encoder and that it can directly be applied to 3D point clouds. 

\subsection{Ablation Study.}

\textbf{Ablation study on the proposed modules of \name{}.} To further study the power of 3DGS representation and the effectiveness of UniGS, we further ablate and evaluate UniGS in zero-shot classification on ABO. We summarize all the experiments and conduct an ablation study on whether to leverage the 3DGS feature, ViT pattern, Pretrained weight (Pre.), Parallel structure (Par.), and Cross attention (Cro.) between Parallel structures. Therefore, as shown in \cref{tab:ablation-study-structure}, we can analyze and conclude that

\begin{minipage}{\textwidth}

\begin{minipage}[t]{0.57\textwidth}
\makeatletter\def\@captype{table}
   \addtolength{\tabcolsep}{1.8pt}
   \footnotesize
 \begin{tabularx}{1\textwidth}{ c | c c c c c | c }
    \toprule
     \multirow{2}{*}{ExP.} & \multirow{2}{*}{3DGS} & \multirow{2}{*}{ViT} & \multicolumn{3}{c|}{GAG} & \multirow{2}{*}{Avg.}\\
\cmidrule{4-6}
 & & & Pre. & Par. & Cro. & \\
    
\midrule
   1 & \xmarkg & \xmarkg & \xmarkg & \xmarkg & \xmarkg & 22.62  \\

2 & \colorbox{mycyan}{\cmark} &  \xmarkg & \xmarkg  &  \xmarkg & \xmarkg & 19.35 \\

 3 &  \cmarkg & \colorbox{mycyan}{\cmark} & \xmarkg & \xmarkg & \xmarkg & 38.38 \\

  4 & \cmarkg & \cmarkg & \colorbox{mycyan}{\cmark} & \xmarkg & \xmarkg & 27.43 \\

  5 & \cmarkg & \cmarkg & \cmarkg & \colorbox{mycyan}{\cmark} & \xmarkg & 37.58 \\

  6 & \cmarkg & \cmarkg & \colorbox{mypink}\xmarkg & \cmarkg & \cmarkg & 40.57 \\
  \midrule
 \rowcolor{mygray} 7 &  \cmarkg & \cmarkg & \cmarkg & \cmarkg & \colorbox{mycyan}{\cmark} & \textbf{46.97} \\
 
\bottomrule
\end{tabularx}
		\caption{\textbf{Ablation study on the proposed modules.} Avg.: the mean average Top1 accuracy across all categories. GAG denotes our Gaussian-Aware Guidance.}
  \label{tab:ablation-study-structure}
\end{minipage}
    \hspace{8pt}
\begin{minipage}[t]{0.37\textwidth}
\makeatletter\def\@captype{table}
 \addtolength{\tabcolsep}{-3pt}
 \footnotesize
 \begin{tabularx}{1\textwidth}{ c | c c c}
\toprule
\multicolumn{4}{c}{Data scaling up} \tabularnewline
\midrule
  Data & 10k & 50k & 100k \\
 \midrule
 Avg. & 27.67 & 43.06 & \textbf{46.97} \\
\midrule
\midrule
\multicolumn{4}{c}{Model scaling up} \tabularnewline
\midrule
 Model & UniGS-T & UniGS-S & UniGS-L \\
 \midrule
 Avg. & 42.67 & 46.97 & \textbf{52.30} \\
 
\bottomrule
\end{tabularx}
		\caption{\textbf{UniGS performance with scaling up. Avg.: }the mean average Top1 zero-shot classification accuracy on ABO.}
  \label{tab:ablation-study-data}
\end{minipage}
\end{minipage}
\vspace{4pt}

\begin{itemize}
    \item Comparisons between Exp1., Exp2., and Exp3. show that the convolution-based model, i.e. PointNet++, can not capture the explicit 3DGS feature while the ViT-based model, i.e. Uni3D, can successfully model feature relationships.
    \item Comparisons between Exp3., Exp4., and Exp7. show that loading pretrained weights has certain advantages, but without careful designs, it may not be possible to utilize the explicit features of 3DGS. On the contrary, training directly from scratch can better learn the feature information of 3DGS, revealing that incorrect model design can hinder subsequent learning due to prior knowledge.
    \item  Comparisons between Exp3., Exp4., Exp5., and Exp7. show that dividing the model into parallel structures to process color and other 3DGS features separately can improve alignment of 3DGS, but it is not enough to achieve superior performance.
    \item  Comparisons between Exp5. and Exp7. show the importance of cross attention to utilize the prior from point cloud encoding to guide 3DGS feature encoding.
    \item  Comparisons between Exp5., Exp6., and Exp7. show that the pretrained weights are beneficial for the final performance and that the key to UniGS performance lies in the Gaussian-Aware Guidance that reduced the difficulty of overall 3DGS learning and enhances 3DGS understanding through the cross attentions.
\end{itemize}

\textbf{Scaling Up.} 
We next explore the effectiveness of scaling up training data and model type in \cref{tab:ablation-study-data}. The performance under different data and model scales demonstrates that scaling up the training data and model size of UniGS can significantly improve the performance of 3D representation learning.

