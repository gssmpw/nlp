\begin{figure}[t]
    \centering
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_NYU_Langone_AUC.pdf} 
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_NYU_Long_Island_AUC.pdf}
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_RSNA_AUC.pdf}
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_CQ500_AUC.pdf}
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_NYU_Langone_AP.pdf}
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_NYU_Long_Island_AP.pdf}
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_RSNA_AP.pdf}
    \includegraphics[width=0.24\textwidth]{figures/fig4/pretrain_size_CQ500_AP.pdf}
    \caption{\textbf{Performance for Different Percentage of Pre-training Samples (Mean):} we compare the label efficiency in terms of different percentage of pre-training data for MAE vs. DINO. The $95\%$ CI are plotted in colour bands and the centre points of the bands indicate the mean value. We show that although DINO present higher label efficiency plot, both MAE and DINO efficiently scale up on downstream performance as more pre-training data is incorporated.}
    \label{fig:scaling_law}
\end{figure}
