
\begin{figure*}
    \centering
    \makebox[\textwidth][l]{%
        \hspace{0.10\textwidth}
        \textbf{NYU Langone} \hspace{0.08\textwidth} \textbf{NYU Long Island} \hspace{0.1\textwidth} \textbf{RSNA} \hspace{0.15\textwidth} \textbf{CQ500}
    } \\[0.2cm]
    \includegraphics[trim={0 0 0 0},clip, width=0.22\textwidth]{figures/abla_probing/AUC_NYU.pdf}
    \includegraphics[trim={0 0 0 0},clip, width=0.22\textwidth]{figures/abla_probing/AUC_Longisland.pdf}
    \includegraphics[trim={0 0 0 0},clip, width=0.22\textwidth]{figures/abla_probing/AUC_RSNA.pdf}
    \includegraphics[trim={0 0 0 0},clip, width=0.28\textwidth]{figures/abla_probing/AUC_CQ500.pdf}
    \\[0.2cm]
    \includegraphics[width=0.22\textwidth]{figures/abla_probing/AP_NYU.pdf} 
    \includegraphics[width=0.22\textwidth]{figures/abla_probing/AP_Longisland.pdf} 
    \includegraphics[width=0.22\textwidth]{figures/abla_probing/AP_RSNA.pdf}
    \includegraphics[width=0.28\textwidth]{figures/abla_probing/AP_CQ500.pdf}
    \caption{\textbf{Comparison of Different Downstream Training Methods.} This plot illustrates the downstream performance of models evaluated using fine-tuning and various probing methods across four datasets. In most cases, the DINO pre-trained model outperforms the MAE pre-trained model. All models were pre-trained on $100\%$ of the available pretraining data.}
    \label{fig:probing_comparison}
\end{figure*}
