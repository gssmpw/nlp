\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{float}
\usepackage{cleveref}
\usepackage{nameref}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{longtable}

\pdfobjcompresslevel=0
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\crefname{figure}{Fig.}{Figs.}         %
\crefname{appendix}{Supplementary}{Supplementary}

\title{3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography}

\author[1†]{Weicheng Zhu}
\author[1†]{Haoxu Huang}
\author[1]{Huanze Tang}
\author[2]{Rushabh Musthyala}
\author[1]{Boyang Yu}
\author[1]{Long Chen}
\author[3]{Emilio Vega}
\author[3,8]{Thomas O'Donnell}
\author[3]{Seena Dehkharghani}
\author[4]{Jennifer A. Frontera}
\author[4,5,6]{Arjun V. Masurkar}
\author[4]{Kara Melmed}
\author[3,7*]{Narges Razavian}
\affil[1]{New York University, Center for Data Science, New York, NY, 10001, USA}
\affil[2]{New York University, Courant Institute of Mathematical Sciences, New York, NY, 10001, USA}
\affil[3]{NYU Grossman School of Medicine, Department of Radiology, New York, NY, 10016, USA}
\affil[4]{NYU Grossman School of Medicine, Department of Neurology, New York, NY, 10016, USA}
\affil[5]{NYU Grossman School of Medicine, Department of Neuroscience and Physiology, New York, NY, 10016, USA}
\affil[6]{NYU Grossman School of Medicine, Neuroscience Institute, New York, NY, 10016, USA}
\affil[7]{NYU Grossman School of Medicine, Department of Population Health, New York, NY, 10016, USA}
\affil[8]{Siemens Healthineers, Malvern, PA, 19355, USA}

\affil[*]{\textbf{Corresponding author:}Narges.Razavian@nyulangone.org}

\affil[$\dagger$]{These authors contributed equally to this work}
 

\begin{abstract}
Head computed tomography (CT) imaging is a widely-used imaging modality with multitudes of medical indications, particularly in assessing pathology of the brain, skull, and cerebrovascular system. It is commonly the first-line imaging in neurologic emergencies given its rapidity of image acquisition, safety, cost, and ubiquity. 
Deep learning models may facilitate detection of a wide range of diseases. However, the scarcity of high-quality labels and annotations, particularly among less common conditions, significantly hinders the development of powerful models. To address this challenge, we introduce \textbf{FM-CT}: a \textbf{F}oundation \textbf{M}odel for Head \textbf{CT} for generalizable disease detection, trained using self-supervised learning. Our approach pre-trains a deep learning model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans without the need for manual annotations, enabling the model to learn robust, generalizable features. To investigate the potential of self-supervised learning in head CT, we employed both discrimination with self-distillation and masked image modeling, and we construct our model in 3D rather than at the slice level (2D) to exploit the structure of head CT scans more comprehensively and efficiently. The pre-training phase is followed by fine-tuning on smaller, annotated downstream datasets, thereby optimizing the model for specific diagnostic tasks, such as detecting hemorrhages, tumors, and other abnormalities. The model's downstream classification performance is evaluated using internal and three external datasets, encompassing both in-distribution (ID) and out-of-distribution (OOD) data. Our results demonstrate that the self-supervised foundation model significantly improves performance on downstream diagnostic tasks compared to models trained from scratch and previous 3D CT foundation models on scarce annotated datasets. Furthermore, the model maintains strong generalization across different datasets, indicating its potential for broad clinical applicability. This work highlights the effectiveness of self-supervised learning in medical imaging and sets a new benchmark for head CT image analysis in 3D, enabling broader use of artificial intelligence for head CT-based diagnosis.
\end{abstract} 
\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}


\section*{Introduction}
Head computed tomography (CT) is often the first step in diagnosing a wide range of neurological disorders, including head trauma, hemorrhages, hydrocephalus, and malignancies. Head CT scans are faster, more accessible, and generally less expensive than magnetic resonance imaging (MRIs), making them ideal for emergencies like traumatic brain injury (TBI) or suspected stroke. They are also effective in detecting bone fractures, or neurovascular pathologies such as arterial venous malformations. Despite its widespread use, CT lacks the contrast resolution and hence the sensitivity for many disorders dependent upon diagnosis by MRI, thus MRI is the imaging modality of choice for many neurologic diseases. MRI, however, is more costly, risks potential heating or displacement of indwelling implants, and suffers generally slower acquisition times, increasing patient discomfort and risking non-diagnostic examinations due to its greater sensitivity to motion-related artifacts. It is also more expensive than CT and is contraindicated in specific patients. Access to MRI is a major challenge in resource-limited countries. The timely and arduous determination of certain pathologies can delay appropriate medical and surgical treatment for patients.
There is significant potential to harness artificial intelligence (AI) algorithms to enhance the diagnostic and early detection capabilities of head CT, providing critical support in clinical decision-making and improving patient outcomes. Early and accurate diagnosis can potentially lead to more effective treatments, reduce complications, and improve patient survival.


Current research on AI-driven diagnosis using head CT is limited due to both lack of data availability and the complexity of model architectures. Although datasets such as RSNA~\cite{flanders_construction_2020} and CQ500~\cite{CQ500} provide publicly available head CT data, they remain small (RSNA includes approximately 10K samples and CQ500 approximately 1K), and their primary focus is on hemorrhage detection, which restricts broader applicability as a credible path to clinical decision support. Moreover, many existing models are designed with highly task-specific architectures that may not generalize well to diverse clinical applications. These models typically apply 2D convolution neural networks (CNN) to sequentially process 3D volumes slice-by-slice under the supervision of slice-level labels~\cite{wang_deep_2021,CQ500,yun_deep_2023}. %
Slice-level labels are often expensive to acquire, and models trained on 2D slices often struggle to generalize to conditions like neurodegenerative diseases, where slice-level labels are not easily defined. Developing models that can harness the information embedded within the 3D structure of CT images while requiring minimal slice-level labeled data can thus expand the impact of such approaches. To address these challenges, we have developed FM-CT: a Foundation Model for Head CT, and demonstrated robust performance across multiple tasks and datasets, which highlights our model's potential for broad clinical applicability.


Recent advancements in AI \textit{foundation models}, deep learning models pre-trained on extensive datasets in a self-supervised manner, have enabled rapid adaptation and robust performance across a wide range of tasks~\cite{radford2021learningtransferablevisualmodels,zhou2021ibot,oquab2023dinov2, rishi24foundation}. Multiple studies have shown that foundation models trained on large-scale medical data can enhance model performance in various medical imaging tasks, including chest X-rays~\cite{yao2024evaxfoundationmodelgeneral}, histopathology~\cite{wang_pathology_2024,huang_visuallanguage_2023,chen_towards_2024, Vorontsov2024},  retina imaging~\cite{zhou2023foundation}, fMRI~\cite{dong2024brainjepa} and more. Additionally, several generalist vision-language models show promise for multimodal medical applications~\cite{codella2024medimageinsight,yang2024advancingmultimodalmedicalcapabilities,zhang2024generalist}. Although some research has focused on CT scans~\cite{Tang_2022_CVPR,blankemeier2024merlinvisionlanguagefoundation,codella2024medimageinsight}, these studies remain limited to abdominal CTs and cannot generalize to other part of the body. While Google CT Foundation model has explored report generation for head CTs~\cite{yang2024advancingmultimodalmedicalcapabilities}, it uses default video encoders to interpret 3D head CTs and has only been subjectively evaluated on fewer than 100 samples, for which the generated reports were worse than original ones. Given these limitations, developing a dedicated vision foundation model for head CTs is essential to advance AI-driven diagnosis and facilitate early detection of cranial and neurological conditions.

In this work, we introduce FM-CT, a 3D foundation model for head CT scans, developed using self-supervised learning (SSL). While SSL has shown success in natural images~\cite{chen2020simple, he2020momentum, caron2020unsupervised, caron2021emerging, bao2022beit, He2021MaskedAA, zhou2021ibot, oquab2023dinov2, zbontar2021barlow, bardes2022vicreg} and in medical imaging~\cite{Liu_2023_CVPR, zhu2022interpretablepredictionlungsquamous, chen_towards_2024, zhou2023foundation, Huang2023, azizi21big, Vorontsov2024, huang2023radiology, huang21GLoRIA, chen23masked, Azizi2023}, training a robust 3D CT volume encoder presents distinct challenges, such as selecting appropriate pretext tasks, managing spatial normalization, and addressing high computational demands. To overcome these challenges, we developed a standardized pipeline that normalizes head CT scans from various protocols, producing consistent input for our foundation model. For pre-training, we adapted two SSL frameworks—self-distillation, inspired by DINO~\cite{caron2021emerging}, and masked prediction, inspired by MAE~\cite{He2021MaskedAA}. These methods were tailored to train a volumetric encoder based on a customized vision Transformer (ViT)\cite{dosovitskiy2020vit}. The full details of our design and choices are described in Section\hyperref[sec:methods]{``Method''}. pre-training was conducted on a large-scale dataset comprising 361,663 head CT scans from a major clinical institution.

To evaluate the foundation model, we systematically assessed its performance and generalizability across 10 downstream disease detection tasks using diverse internal and external datasets, as illustrated in \Cref{fig:overview}. Beyond commonly studied hemorrhages, our evaluation includes crucial yet less-explored tasks in head CT, such as identifying brain tumors, Alzheimer’s disease and related dementia (ADRD), edema, and hydrocephalus (HCP). For each downstream task, the foundation model was fine-tuned using task-specific labels. Given the scarcity of expert-annotated public datasets for these conditions, we leverage electronic health records (EHR) to acquire labels of each task. While EHRs may include missing data and suffer potential label-noise, they remain a valuable and practical source for large-scale patient status labeling that can be used to evaluate the performance of the foundation model.

Our results reveal substantial performance improvements enabled by our foundation model. Downstream models initialized with the pre-trained weights of foundation model achieve a $16.07\%$ improvement in macro-AUC over models trained from scratch with random initialization on internal NYU Langone data, and $20.86\%$ and $12.01\%$ improvements on external datasets from NYU Long Island (previously a separate hospital) and RSNA, respectively ($P<0.001$ for all comparisons). These findings underscore the potential of our foundation model to advance AI-based interpretation of head CT scans, supporting more accurate diagnosis and early disease detection. Furthermore, as described in Section~\hyperref[sec:results]{``Results''}, we demonstrate the model’s capabilities in out-of-distribution generalization (\Cref{fig:overview}), few-shot learning (\Cref{fig:fewshot}), and scalability (\Cref{fig:scaling_law}), highlighting the method’s potential in scenarios with limited annotated fine-tuning data, or scenarios such as federated learning which provide access to orders of magnitude more data. Overall, the experimental results on multiple datasets and tasks underscore the generalizability, adaptability and effectiveness of the model, and pave the way for significant impact in real-world clinical applications.




\input{plots/overview}

\section*{Results}
\label{sec:results}
\subsection*{Foundation model for disease detection with 3D head CT scans}

The key aim of the foundation model is to develop a single model that improves performance on a wide range of downstream tasks of detecting recognizable abnormalities from head CT scans. To evaluate the capability of the foundation model, we train classification models for multiple disease detection tasks by fine-tuning the foundation model separately per disease, and assessing the fine-tuned model's performance on held-out validation and external data sets. The selected downstream tasks include detecting various types of hemorrhages (intraparenchymal hemorrhage (IPH), intraventricular hemorrhage (IVH), subdural hemorrhage (SDH), epidural hemorrhage (EDH), subarachnoid hemorrhage (SAH), and intracranial hemorrhage (ICH)), brain tumors, hydrocephalus (HCP), edema, and Alzheimer's diseases and related dementia (ADRD). \Cref{fig:overview}a,b,c show the overview of our pre-training framework and included data, EHR-matching, and datasets used in pre-training, in-domain fine-tuning, and external validation. Overall N=361,663 scans were used during pre-training, and four distinct datasets from different sources were used for various forms of validation (NYU Langone N=26,487; NYU Long Island N=2,202; RSNA N=1,058; and CQ500 N=236). NYU Langone is a hospital system comprised of multiple geographically distinct hospitals including two Level 1 Trauma Centers and three Comprehensive Stroke Centers. NYU Long Island, a Level 1 Trauma Center/Comprehensive Stroke Center, is treated as an external dataset for the purposes of this study. 

The first two rows of \Cref{fig:overview}e report the task-specific AUCs for Vision Transformer (ViT) classifiers trained from scratch with random initialization, namely \textit{scratch}, versus those fine-tuned from the foundation model, namely \textit{fine-tuned} on NYU Langone data. The fine-tuned models consistently outperform the scratch model across all 10 disease detection tasks, achieving a macro-AUC of 0.852 --- a $16.07\%$ increase over the scratch model’s 0.734 ($P<0.001$). Additionally, in \Cref{fig:overview}f and Supplementary \Cref{fig:radar-comparison-merlin} we compared the foundation model with two other foundation model for 3D CT scans --- Merlin~\cite{blankemeier2024merlinvisionlanguagefoundation} and Google's CT Foundation\cite{yang2024advancing} model. Merlin outperforms the scratch model with a macro-AUC of $5.67\%$ while falling short compared to our foundation model with $7.51\%$ lower macro-AUC ($P<0.001$, illustrated in Supplementary \Cref{fig:radar-comparison-merlin}). Although Merlin is not directly comparable to our foundation model as it was pre-trained on abdominal CT, it still provides a valuable baseline. We compare our model to Google CT Foundation model with linear probing, because trainable weights for end-to-end fine-tuning are not provided for this model. We consistently observe improved model performance across the board (in \Cref{fig:overview}f and Supplementary \Cref{fig:probing-comparison-gemini}). %
These findings demonstrate that despite the progress in general domain multimodal models, specialized foundation model pre-trained on head CT data still significantly enhance the understanding of brain CTs. 

To assess our foundation model's generalization to out-of-distribution data, we compiled three external datasets from multiple institutions and sources: NYU Long Island, RSNA~\cite{flanders_construction_2020}, and CQ500~\cite{CQ500}, as shown in \Cref{fig:overview}c (NYU Langone and NYU Long Island are geographically separate and distinct institutions within the broader health system). The data in these external datasets has a different distribution than the data used for pre-training. We evaluate the generalization on external datasets via two common practices to utilize the foundation model: (1) in-domain fine-tuning on separated datasets and tasks, and (2) fully external validation of the disease detection models without any site-specific fine-tuning.

\input{plots/mae_few_shot}

For in-domain fine-tuning, the foundation model is fine-tuned on each external dataset’s training set and validated on held-out sets from the same source. The bottom four rows in \Cref{fig:overview}e report the tasks-level performances on NYU Long Island and RSNA datasets. The fine-tuned model yields a macro-AUC of 0.904 across the 10 tasks on NYU Long Island dataset and a macro-AUC of 0.923 for five types of hemorrhages on the RSNA dataset. In comparison, the scratch model results in macro-AUC scores of 0.748 and 0.824, respectively.  Moreover, the foundation model also significantly outperforms Merlin, as shown in Supplementary \Cref{fig:radar-comparison-merlin}. The superior performances on external datasets indicate the generalizability of the foundation model. Note that the limited data size of CQ500 forbids training a effective deep learning model from scratch, reinforcing the importance of the foundation model in label efficiency, which is further studied in Section \hyperref[sec:label_efficiency]{"Label efficiency"}. Interestingly, when comparing performances across different datasets, \Cref{fig:overview}e demonstrates that the AUCs of the in-domain fine-tuned model on the external dataset even exceed the AUCs achieved on the internal dataset. For instance, the fine-tuned models consistently obtained AUCs greater than 0.90 in all the hemorrhage detection tasks on RSNA dataset, surpassing the AUCs on NYU Langone data. This may be attributed to the higher label quality in radiologist-reviewed datasets, for which label noise may be better controlled by comparison to EHR-derived labels.

In the full external validation without any site-specific fine-tuning (illustrated in \Cref{fig:overview}c), we evaluated classification models fine-tuned on the NYU Langone training set, as-is, on the held-out validation sets from each external dataset. \Cref{fig:overview}f compares performance between external validation and in-domain fine-tuning. Results show that, for the NYU Long Island and RSNA datasets —where the training set used for fine-tuning includes a sufficient number of high-quality labeled samples— in-domain fine-tuning does enhance the model performance. However, on the CQ500 dataset, with only 1,120 training samples, the in-domain fine-tuned model performs worse than the model transferred from NYU Langone, especially for EDH and SDH, which have a greater class imbalance. These comparisons highlight two typical use cases for foundation models depending on the availability of labeled data for fine-tuning. Additionally, comparing the first row of \Cref{fig:overview}e and external validation in \Cref{fig:overview}f, the fine-tuned model on NYU Langone achieves similar AUC values on both internal and external datasets, indicating robust generalizability to external data.











\subsection*{Label efficiency of few-shot classification performance}
\label{sec:label_efficiency}

Another key advantage of the foundation model is its ability to facilitate transfer learning and fine-tuning tasks with minimal labeled data. For example, as shown in \Cref{fig:overview}c, the CQ500 dataset contains only 1,585 scans. Despite the small dataset size, fine-tuning our foundation model on CQ500 achieves promising results, with an AUC of 0.863. 

To systematically evaluate the label efficiency of our foundation model, we also assess the generalization capabilities of models on new tasks given a limited number of examples within the paradigm of few-shot learning, where only $K$ positive and negative samples each are used for training in each task. Since the quality of few-shot learning is largely determined by the sampled $K$-shots training data, we re-sampled and re-trained the model 5 times for calculating means and confidence intervals. As expected, \Cref{fig:fewshot} shows that performance improves as more data is used for training, with narrower confidence intervals. Surprisingly, even with a small number of examples (e.g., 512 total, with $K=256$), the model achieves performance comparable to training with the full dataset, which contains over at least 16 times more training examples in the RSNA. Notably, for tasks like detecting IVH in the RSNA dataset, the 8-shots model achieves an AUC above 0.90, a result that rivals full-data training. These findings suggest that our foundation model has learned diverse and expressive features/representations during SSL pre-training, making it highly effective for new tasks even when trained on small labeled datasets.

\input{plots/pretrain_percentage_comparison_mean}

\subsection*{Scaling up pre-training data}

Scaling laws have proven effective in enhancing the performance of foundation models by increasing the size of the training dataset~\cite{kaplan2020scalinglawsneurallanguage}. This phenomenon is not only observed in natural language and image domains~\cite{zhai22scalingvit, pmlr-v202-dehghani23a}, but also extends to medical imaging~\cite{zhou2023foundation, li2024well}. As shown in \Cref{fig:scaling_law}, scaling up the foundation model by incorporating more data during self-supervised pre-training significantly improves downstream tasks performances. We compared models pre-trained with varying proportions of the available data --- $10\%$, $30\%$, and $100\%$ (full dataset), observing that larger pre-training datasets consistently led to better downstream task performance. These findings highlight the potential of leveraging more data to achieve superior results, further suggesting the value of multi-institutional collaboration and federated approaches to aggregating larger datasets to enhance model quality. Noticeably, the performance for CQ500 does not change a lot from $10\%$ to $30\%$, but $100\%$ gives a sudden performance improvement, this indicates that for smaller datasets like CQ500, scaling up the data size is crucial for learning meaningful representations.

\subsection*{Visual Interpretation}
To gain insight into the features learned through self-supervised pre-training and supervised fine-tuning of the foundation model, we visualize the attention maps within the Vision Transformer (ViT), as shown in \Cref{fig:attention_interpretation}. These heatmaps highlight the regions where the ViT model focuses most strongly. In the second column, we see that the pre-trained foundation model captures generic brain features, with dark red indicating attention on abnormal ventricular shapes and green marking areas of hemorrhage. After fine-tuning on specific tasks, the ViT’s attention becomes more focused on patterns relevant to each disease. For instance, in the edema task (third column), the heatmap extends across most of the brain, reflecting generalized swelling. For ADRD (fourth column), the model emphasizes regions of ventricular enlargement and cerebral atrophy. Multiple hemorrhages are also present in this sample, with attention covering both the IPH in the dense central region (fourth column) and extending toward the left end of the ventricle where IVH appears (fifth column). In the case of SAH (sixth column), the attention map is less prominent due to the small, peripheral area of the SAH in the lower part of the slice, although the model still predicts it accurately.

The comparison between the pre-trained and fine-tuned ViT explains the performance difference between linear probing and fine-tuning (shown in Supplementary \Cref{fig:probing-comparison-perpath}, as end-to-end fine-tuning allows the model to learn task-specific features more effectively. Details on the computation of the visualized attention maps are provided in Section \hyperref[sec:methods]{“Methods”}.

\input{plots/interpretation}

\section*{Discussion}
Despite advances in disease detection using 3D head CT scans, current solutions are limited by the availability of annotated data and the complex, task-specific design requirements of network architectures. These constraints hinder the broader application of machine learning in clinical disease detection. To address this, we developed a foundation model, trained on a large unlabeled dataset, to enable fine-tuning for multiple tasks with minimal labeled data under a unified network architecture. 

Highly accurate detection of intracerebral hemorrhages without delay is a critical clinical issue for the diagnostic decision making and treatment in an emergency room \cite{Hemphill2015-yw,Qureshi2009-ve}. Our results indicate that 3D Head CT scans can also be used to help identify hemorrhage subtypes and, more interestingly, etiology. High performances and generalizability observed by our model in detecting intracerebral hemorrhage have a potential to greatly assist in pre-hospital and early hospital management of blood pressure. This is particularly important given that early blood pressure control is a key factor in preventing hematoma expansion and improving patient outcomes~\cite{Macellari2014-zj,Morotti2022-lc}.

This approach is also particularly valuable for extending detection capabilities to new diseases in CT imaging. For example, early detection of ADRD with deep learning has traditionally relied on MRI scans \cite{Li2019-jx, pmlr-v116-liu20a, Xue2024}. However, access to MRI machines is costly and often restricted by patients’ geographic location and socioeconomic status~\cite{https://doi.org/10.1002/neo2.10}. Head CT, in contrast, is fast, accessible, and is the first-line imaging test in emergency and diagnostic settings. Our foundation model enables more accessible ADRD detection using head CT scans. This advancement holds the potential for expanding early ADRD detection in common public health settings for the older population~\cite{lin_dementia_2020, kim2021racial}, such as emergency rooms, as well as in underserved communities nationally and internationally in which CT is more available than MRI. Similarly, our model could facilitate the development of detection tools for other conditions, such as cancers and neuroinfectious diseases, thus supporting population health on a broader scale.

Our study demonstrates that this pre-trained foundation model significantly outperforms models trained from scratch and other CT foundation models on the same labeled data. Moreover, it exhibits strong performance even with limited data, as shown in few-shot learning experiments, and suggests promising potential for scaling up with larger datasets. In clinical practice, head CT scans are typically acquired using heterogeneous protocols, including variations in slice thickness and scanner modalities. A robust foundation model for CT should generalize effectively across these diverse protocols. In this study, we utilized scans with slice thicknesses ranging from 0.5 mm to 5 mm and data from two major manufacturers (Siemens and Toshiba) to develop and assess the generalization capabilities of our foundation model. As illustrated in Supplementary \Cref{fig:batch_effect}, the embeddings produced by the foundation model show separability based on scanner manufacturer and slice thickness, likely reflecting variations in protocol distribution. However, by comparing the distribution of “All” patients to that of positive cases for each condition, we observe that the embeddings do not tend to collapse or bias towards a certain protocol. Supplementary \Cref{fig:thickness-ablation} further demonstrates that fine-tuned models achieve comparable performance across scanner protocols. Detailed per-task performance results are provided in Supplementary \Cref{fig:slice_thickness_per_pathology,fig:manufacturer_per_pathology}.  Additionally, in comparisons of Toshiba and Siemens scanners, we noted a systemically higher prevalence of positive cases across all tasks in Toshiba scans, leading to a modestly higher AUC in specific instances. Despite these variations, our foundation model demonstrates robust generalization capabilities across diverse CT protocols, highlighting its potential for broad clinical application.

However, our evaluation is limited by label noise in real-world datasets. Labels derived from electronic health records (EHRs) can suffer from missing or incomplete information. This issue is evidenced by the model’s lower performance on NYU Langone data compared to RSNA data, where labels were rigorously reviewed by radiologists. Another limitation is that, due to constraints on training samples and computational resources, our model does not yet fully explore the potential of scaling laws. The 361K scans used for pre-training represent the entirety of CT scans available from a single large clinical institution, highlighting the need for multi-institutional collaborations to enhance the dataset diversity and volume. With greater computational resources, we could also scale up the model’s size, resolution of image patches, and number of tokens used in the ViT architecture, potentially improving performance for detecting conditions with small spatial manifestations, such as subarachnoid hemorrhage (SAH).

While our current results primarily focused on disease detection, our foundation model holds significant potential for advancing disease prognosis analysis. For instance, the prediction of decompensation, particularly hemorrhagic expansion, is an important potential use of the foundation model and may lead to the development of novel hyperacute treatment strategies~\cite{hematoma_expansion}. Additionally, critical applications in acute ischemic stroke, such as predicting hemorrhagic transformation and the development of malignant edema can benefit from the foundation model. Beyond acute conditions, the foundation model can potentially also be used to predict the development of ADRD~\cite{Zhu2024-zd}.


\section*{Methods}
\label{sec:methods}
\subsection*{Datasets}
\subsubsection*{Dataset for pre-training foundation model}
We utilized a large-scale head CT scan dataset from NYU Langone, consisting of 499,084 scans across 203,665 patients, collected between 2009 and 2023. These scans were acquired using Siemens and Toshiba machines. We included all the non-contrast head CT scans with ranging from 0.5mm to 5mm, kVp values between 70 and 150, and convolution kernels Hr/Qr/J with sharpness levels of 35-45. We filtered out corrupted scan series with missing DICOM files and those containing less than 10 slices, resulting in 451,298 scans. We partitioned these scans by the patient IDs into training, validation, and held-out validation sets in an 8:1:1 ratio to avoid the leakage of scans from the sample patient. As illustrated in \Cref{fig:overview}a, this led to training, validation and held-out validation set with 361,663, 44,886 and 44,749 scans, respectively. The scans in the training set were used to train the foundation model.

\subsubsection*{Datasets for downstream tasks}
We evaluated our model using four datasets: one \emph{in-domain (ID)} dataset from NYU Langone and three \emph{out-of-domain (OOD)} datasets from NYU Long Island, the RSNA Challenge, and the public CQ500 dataset. Each dataset includes multiple head CT disease detection classes, with some classes abbreviated as follows: Hydrocephalus (HCP), Dementia (ADRD), Intraparenchymal Hemorrhage (IPH), Intraventricular Hemorrhage (IVH), Subdural Hemorrhage (SDH), Epidural Hemorrhage (EDH), Subarachnoid Hemorrhage (SAH), and Intracerebral Hemorrhage (ICH). These classes can have co-occur in the same Head CT scan. The characteristics of the patients are shown in \Cref{tab:characteristic}. We split all datasets by patients to avoid information leakage, Further dataset details of our dataset are provided below.

\paragraph{NYU Langone - 10 tasks} The NYU Langone main campus dataset serves as the internal ID dataset for downstream evaluation. As depicted in \Cref{fig:overview}b, patient health status was derived from Electronic Health Records (EHR) within a 3-month window centered around the scan date, with conditions defined by ICD-10 diagnostic codes and medications, outlined in Supplementary \Cref{tab:disease_definition}. This cohort includes 270,205 scans from 66,801 patients with valid EHRs, covering 10 classes: Tumor, HCP, Edema, ADRD, IPH, IVH, SDH, EDH, SAH, and ICH. This cohort was partitioned following the same split used used during pre-training: matched patients within the training, validation, and held-out subsets of the SSL pre-training phase were assigned to the corresponding sets of the supervised fine-tuning phase. This resulted in 217,109, 26,609, and 26,487 scans in the training, validation and test set, respectively.
\paragraph{NYU Long Island - 10 tasks} NYU Long Island data was acquired in Long Island hospital which used to be a separate hospital, severing as an OOD dataset. This dataset includes 22,158 samples with 10 classes, labeled similarly to the NYU Langone dataset using ICD-10 codes. It is partitioned into an 8:1:1 train-validation-test split.
\paragraph{RSNA - 5 tasks} The RSNA Head CT Challenge dataset~\cite{flanders_construction_2020} serves as a public external OOD dataset, collected from Stanford University, Thomas Jefferson University, Unity Health Toronto, and Universidade Federal de São Paulo (UNIFESP). The dataset, initially provided as 2D slices, was reorganized by subject ID, retaining subjects with complete slice data. After preprocessing, the dataset consists of 10,579 samples across five classes: Any (any hemorrhage type), IPH, IVH, SAH, and SDH. Dataset labels were assigned by 60 volunteers from the American Society of Neuroradiology (ASNR). We partitioned this cohort into an 8:1:1 train-validation-test split.
\paragraph{CQ500 - 10 tasks} The CQ500 Head CT dataset~\cite{CQ500} serves as another public external OOD dataset, collected from multiple centers in India. This dataset includes 1,585 samples including varying slice thickness across 10 selected classes: ICH, IPH, IVH, SDH, EDH, SAH, BleedLocation-Left, BleedLocation-Right, MidlineShift, MassEffect. Each scan was labeled by three senior radiologists, and the cohort was split into an 8:1:1 train-validation-test ratio.

\subsubsection*{Label acquisition from electronic health records}

As illustrated in \Cref{fig:overview}b, we labeled head CT scans from NYU Langone and Long Island Hospital using electronic health records (EHR). For each head CT, we retrieved an EHR snippet for the corresponding patient based on their Medical Record Number (MRN), starting from the time of the scan and covering a 90-day period. We then checked for the presence of any diagnosis codes (ICD-10 codes) and medication records, within this EHR snippet that matched the predefined definitions for each disease, allowing us to create binary labels for each condition. The complete list of ICD-10 codes and the medications used for disease definitions is provided in Supplementary \Cref{tab:disease_definition}.


\subsubsection*{Data preprocessing}
For the NYU Langone and Long Island datasets, we converted the DICOM files into NIfTI format using MRIcroGL dcm2nii~\cite{li_first_2016}, standardizing the file format with those from the RSNA and CQ500 datasets. Given the variability in scan protocols, which can result in differing orientation, resolutions and slice thicknesses, we applied spatial normalization to transform the volume orientation to right-anterior-superior (RAS) angle and resample with bicubic interpolation to the isotropic resolution ratio of $(1.0, 1.0, 1.0)$ in the world coordinate system. This ensures uniform pixel spacing across all scans and axes. 

Head CT scans use Hounsfield Units (HU) to represent various tissue types, which span a broad range of values. To better capture tissue characteristics, we applied three windowing ranges, each emphasizing specific tissue types: (40, 80) for soft tissue, (80, 200) for contrast-enhanced tissues and blood vessels, and (600, 2800) for bone. We then stacked the values from each window, producing a 3-channel 3D volume that enhances the representation of these key tissues. Similar strategy has been applied in Chilamkurthy \textit{et al.}~\cite{CQ500}.

To ensure compatibility with model input requirements, we transformed each volume into the desired size. We first padded or cropped each volume to a size of (224, 224, 224), preserving the whole brain across all axes. Then for training, we applied data augmentations detailed in Supplementary Section \hyperref[sec:dataaug_details]{``Data Augmentation details’’}; for evaluation, we center-cropped the volumes to (192, 192, 192). Finally, we resized each volume to (96, 96, 96) as the input size for the model.


\subsection*{Model architecture}
\label{sec:model_architecture}
Numerous studies have demonstrated that ViT can effectively learn high-quality representations for 2D medical images at scale~\cite{chen_towards_2024, zhou2023foundation, MedSAM, Vorontsov2024, Azizi2023}. Our study extends this by exploring whether representations of 3D medical images (specifically head CT scans) can also be effectively learned at scale through the direct compression of 3D patches as model input. We employ the Vision Transformer (ViT)\cite{dosovitskiy2020vit} as the volume encoder for our foundation model, as well as for baseline comparisons in all experiments. Our model uses a ViT-Base architecture with an embedding dimension of 768, 12 self-attention layers, 12 heads, and feed-forward layers with a hidden size of 3072. We apply sine-cosine absolute positional encoding\cite{NIPS2017_3f5ee243} across all pre-training and fine-tuning stages.

For the 3D input volume, instead of creating 196 patches of size $16 \times 16$ from a $224 \times 224$ image as in standard 2D ViT, we segment $96 \times 96 \times 96$ 3D volumes into 512 patches of size $12 \times 12 \times 12$ for ViT input. This customized patch design considers the trade-off between performance and computational cost. As shown in Supplementary \Cref{fig:patches-ablation}, our model outperforms a version using 216 patches of size $16 \times 16 \times 16$, indicating that smaller, more numerous patches enhance model performance. This supports the importance of capturing fine-grained features in 3D medical imaging, consistent with prior findings~\cite{Tang_2022_CVPR, li2024well}. However, computational costs increase significantly with respect to $s$ ($s$ defined as patch size reducing factor), at a rate of $O(s^{6})$, due to the cubic growth of patch numbers in 3D and the quadratic growth in self-attention computation (\Cref{apd:self_attention_rate}). To balance performance with computational efficiency, we adopt 512 patches of $12 \times 12 \times 12$ as the optimal input size for ViT in our foundation model.



\subsection*{Self-supervised pretaining}
Self-Supervised Learning recently has been widely adopted as learning framework for building medical foundation models~\cite{chen_towards_2024, zhou2023foundation, Huang2023, azizi21big, Vorontsov2024}. While previous works mainly focus on directly applying existing self-supervised learning algorithms on 2D medical images, we explore how to effectively leverage these algorithms with 3D medical images. Specifically, we explore two main branches of self-supervised learning framework for building our 3D foundation model --- discriminative with self-distillation (DINO) and masked image modeling (MAE).

\paragraph{Self-Distillation Modelling (DINO)}
DINO\cite{caron2021emerging, oquab2023dinov2} is a self-supervised learning method shown promising and robust downstream evaluation performance in previous studies on different areas~\cite{chen_towards_2024, Vorontsov2024}. DINO uses a student-teacher framework for learning meaningful representations. Both student and teacher networks share the same model architecture, while the teacher’s parameters are updated using an exponential moving average of the student’s parameters. Each input image is augmented multiple times to create different views as student and teacher networks input. Specifically, we applied random global and local crops, random flips, shifts in intensity and contrasts, and Gaussian blurs for augmented views. Then the student’s output is trained to match the teacher’s output using a distillation loss, ensuring similar representations for different views of the same image. We pre-trained the ViT in the DINO framework for $1000$ epochs with batch size at $64$ per GPU and an AdamW~\cite{loshchilov2018decoupled} optimizer ($\beta_1=0.9, \beta_2=0.95$, $0.05$ weight decay). A base learning rate $3\times10^{-4}$ was applied combined with cosine scheduling and a linear warmup on the first $5$ epochs. During pre-training, two global augmentations and three local augmentations were applied to enable ViT to learn both global and local features of the head CT. Because small region of brain is likely to be dissimilar, we observed cropping too small brain regions would cause unstable model training by making the learning task to be too challenging. Therefore, we first resample the input images to $224\times224\times224$. Subsequently, we perform multi-scale cropping by extracting both global and local crops regions, ranging from $112\times112\times112$ to $224\times224\times224$ for global crops and from $64\times64\times64$ to $112\times112\times112$ for local crops. After the cropping, all cropped regions are resampled to $96\times96\times96$. For training on $100\%$ data, convergence on the performance for downstream tasks is observed at around $300$ epochs, which took around one week on four 80GB NVIDIA A100 GPUs. 

\paragraph{Masked Image Modeling (MAE)}
MAE~\cite{He2021MaskedAA} is another self-supervised learning method for vision tasks, inspired by masked language modeling in Natural Language Processing (NLP). MAE is trained to reconstruct randomly-masked patches via an encoder-decoder architecture, where the encoder processes visible patches of an image, while the decoder reconstructs the image from encoded patches and mask tokens. Specifically, we randomly masked the patches from each volume with a probability of 0.75. Mean squared error (MSE) loss is optimized to minimize the difference between the reconstructed volume and the original volume. We pre-trained the ViT in MAE framework for $400$ epochs with batch size at 64 per GPU and an AdamW~\cite{loshchilov2018decoupled} optimizer ($\beta_1=0.9, \beta_2=0.95$, $0.05$ weight decay). A base learning rate $1.5\times10^{-3}$ was applied combined with cosine scheduling and a linear warmup on the first $5\%$ steps,  For training on $100\%$ data, convergence is observed at around $250$ epochs, which took around 4 days on four 80GB NVIDIA A100 GPUs for MAE. Similar to DINO, MAE has shown success in learning robust representations in many previous works~\cite{ravi2024sam2, tong2022videomae, gupta2023siamese, zhou23self, huang2022masked, cong2022satmae, chen23masked}, including the studies on both 2D and 3D data.

We compared the performance on downstream tasks between two versions of foundation models pre-trained using DINO and MAE, as shown in \Cref{fig:scaling_law} and Supplementary \Cref{fig:probing_comparison,fig:probing-comparison-perpath,fig:probing-comparison-perpath-dino}. The results indicate that DINO consistently outperforms MAE across all datasets. Based on this finding, we selected the DINO-pre-trained model as our final foundation model.


\subsection*{Evaluation setting}
\paragraph{Baseline comparisons}
Since no prior foundation models have been specifically trained on 3D Head CT for direct comparison, we benchmark our model against Merlin~\cite{blankemeier2024merlinvisionlanguagefoundation} and Google CT Foundation model~\cite{yang2024advancingmultimodalmedicalcapabilities} to highlight the advantages of our domain-specific foundation model. Merlin is a 3D Abdomen CT foundation model pre-trained on vision-language pairs with contrastive learning~\cite{radford2021learningtransferablevisualmodels} and ICD code prediction task, where 6+ million images from 15,331 CTs, 1.8+ million diagnostic ICD codes from EHR, and 6+ million tokens from radiology reports are used. Different from our model architecture, Merlin used ResNet-152 ($\sim60.4$M Parameters) as vision model with reshaped image size of $224\times224\times160$. The performance comparison between our model and Merlin is shown in Supplementary \Cref{fig:radar-comparison-merlin}, where our model shows substantial improvement across most datasets and diseases. Google CT Foundation model is trained on a comprehensive private dataset comprising 527,078 CT studies with associated radiology reports from 430,772 patients. The model is first trained by Contrastive Captioning with CoCa~\cite{yu2022coca} on 2D medical images and then adapting to CT by training on series of CT slices with VideoCoCa~\cite{yan2023videococavideotextmodelingzeroshot}. The performance comparison between our model and Google CT Foundation model is shown in Supplementary \Cref{fig:probing-comparison-gemini}, where our model shows a consistent improvement across the board. We additionally show comparison of our model against model trained from scratch in \Cref{fig:overview} and Supplementary \Cref{fig:radar-comparison-merlin}, where the overall significantly improved performance shows the effectiveness of our pre-training strategies on 3D Head CT images.

\paragraph{Fine-tuning and Probing classification evaluation}
We assessed pre-trained model performance through full fine-tuning (updating all weights) and various probing methods (updating only the classification layers). For both approaches, images were normalized to isotropic spacing, transformed to three HU interval channels, and reshaped to $3\times96\times96\times96$. The entire transformed 3D image was then input into the ViT model for feature extraction, followed by an additional classification layer for downstream tasks. Probing utilized two strategies: linear probing, which adds a linear layer atop the ViT backbone, and attentive probing, which incorporates an attention layer. Attentive probing is chosen since MAE does not use \texttt{[CLS]} token as the learning objective. Linear probing only relies on \texttt{[CLS]} token to perform classification and attentive probing explores the interaction among all tokens~\cite{Chen2024}. Given the imbalances of downstream task labels, we randomly sampled a balanced subset from the training set per epoch, consisting of 5,000 samples (when fine-tuning on the NYU Langone, NYU Long Island, and RSNA datasets), and 500 samples when fine-tuning on CQ500. We trained all methods using the AdamW~\cite{loshchilov2018decoupled} optimizer with a cosine learning rate scheduler, a learning rate of $1\times10^{-5}$ for backbone and $1\times10^{-3}$ for classification layers, cross-entropy loss, and a maximum of 10 epochs. The main evaluation result with linear probing is shown in \Cref{fig:overview} with fine-tuning and probing comparison shown in Supplementary \Cref{fig:probing_comparison} for average performance across all diseases and Supplementary \Cref{fig:probing-comparison-perpath} for per disease performance. The result indicates that probing achieves performance levels close to full fine-tuning, underscoring the high quality of learned representations in our model.

For fine-tuning model from scratch, as we observe more unstable model performance from different hyper-parameters across different datasets, we perform hyper-parameters sweep across different setting and report the best performance model. The sweeping hyper-parameters are lr=\{1e-3, 1e-4, 1e-5\}, weight decay=\{0.01, 0.05, 0.0001, 0.00001\}, epochs=\{10, 15, 30, 50\}, optimizer=\{SGD, Adam, AdamW\}.

\paragraph{Few-shots classification evaluation}
In order to evaluate the effectiveness of our model under scare label conditions, we applied few-shots learning where each class is only sampled $K$-times. Specifically, we chose $K=8, 16, 32, 64, 128, 256$, where the data is sampled such that positive and negative samples equal to $K$ for each disease. Few-shot training was performed using full fine-tuning with the same hyper-parameter settings. While we also attempted some other commonly used few-shots classification methods such as k-nearest neighbors (KNN), Simple Shots~\cite{want19simpleshot} and Prototypical Networks~\cite{jake17proto}, we did not observe performance improvement on our datasets over full fine-tuning. The main evaluation for few-shots classification is present in \Cref{fig:fewshot}, where we observed our model can already reach performance close to full fine-tuning with only $K=256$ samples. This demonstrates the effectiveness of our model under scare data training regime.

\subsection*{Visual Interpretation}
\label{sec:visual_interpretation}
Self-attention enables the Vision Transformer (ViT) to integrate information across the entire volume, even in its lowest layers. To analyze the relationships among different patches within the CT volumes, we calculate the average spatial distance over which information is integrated, using the attention weights. 

Let $\mathbf{A}^{(l,h)} \in \mathbb{R}^{N \times N}$  represent the attention weight matrix for the $h$th attention head in the $l$th layer of ViT and $N$ is the number of patches in a CT volume. $d(i, j)$ denotes the spatial distance between patch $i$ and patch $j$ within the 3D volume. The attention distance for each patch $i$ is computed as a weighted average distance to other patches, based on the attention weights:
\begin{equation}
    \centering
    D_i^{(l, h)} = \sum_{j=1}^{N} A^{(l, h)}_{ij}  d(i, j)
\end{equation}

We visualize the average attention distances across all heads and layers for every patch in the volume in \Cref{fig:attention_interpretation}. This “attention distance” serves as an estimate of the ViT’s receptive field within the CT volumes, indicating the regions of the brain that the model focuses on. This visualization helps illustrate how the model integrates information across spatial areas to capture meaningful patterns within the volume.

\subsection*{Statistical analysis}

In each experiment, we report the mean and confidence interval, calculated by bootstrapping the held-out validation set 100 times. For few-shot learning, where model variance is also influenced by the specific training data samples, we repeated the training and evaluation process five times with randomly sampled training data, reporting the mean and confidence interval of the resulting metrics. For all statistical significance (p-values) reported in this study, we used a two-sided paired permutation test with $1,000$ permutations to assess the performance difference of two compared models.

\subsection*{Computing Hardware Software}
All experiments are performed under Python (v3.8.11), PyTorch (v2.4.1), CUDA (12.1) and MONAI (v1.2.0). We extend ViT, MAE, DINO implementation from original their corresponding repositories (\url{https://github.com/facebookresearch/mae}, \url{https://github.com/facebookresearch/dino}) to match our need for 3D CT image encoding. For comparison with Merlin~\cite{blankemeier2024merlinvisionlanguagefoundation}, we integrated their original model weight checkpoints and model backbone code (\url{https://github.com/louisblankemeier/merlin}) to our downstream fine-tuning code base. ResNet50-3D~\cite{hara3dcnns} from (\url{https://github.com/kenshohara/3D-ResNets-PyTorch/tree/master}) is integrated to our code base for evaluation. All plots and figures were created by Matplotlib (v0.1.6) and Seaborn (v0.13.2). All downstream experiments were conducted on single 80 GB NVIDIA A100 GPU (graphics processing unit). All pre-training experiments were conducted on four 80 GB NVIDIA A100 GPUs.




\section*{Data availability}
The internal clinical data involved in the study is unavailable due to privacy concerns and institutional policy. Public dataset RSNA is available from \url{https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection}. Public dataset CQ500 is available from \url{https://www.kaggle.com/datasets/crawford/qureai-headct}.  The original data is provided as DICOM files. We converted each scan from DICOM to NIfTI files and removed the scans with missing slices for creating 3D imaging datasets in our evaluation. We use all slice thickness scan protocols in each scan (e.g. thin, plain thin, and plain scan) for CQ500, hence providing a more exhaustive evaluation on our model adaptability on different slice thickness for scan.

\section*{Code availability}
The code for pre-training, fine-tuning and evaluation of the foundation model is available on \url{https://github.com/NYUMedML/headCT_foundation}. Due to the possibility of inferring patient face from headCT data, the model weights are only available upon request after signing institutional agreement. Requests for model weight should be sent to the corresponding author and the NYU Langone Data Sharing Strategy Board (DSSB) Committee (DataSharing@nyulangone.org).

\section*{Acknowledgements}
W.Z., H.H., L.C., A.V.M. and N.R. were supported by the National Institute On Aging of the National Institutes of Health under Award R01AG085617. W.Z. H.H., B.Y. and L.C. received partial support from NSF Award 1922658. N.R. and A.V.M. were also partially supported by the National Institute On Aging of the National Institutes of Health under Awards R01AG079175 and P30AG066512.

\input{main.bbl}





\include{supplementary}


\end{document}
