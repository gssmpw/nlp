
\section{Appendix}

\subsection{Proof of \eqref{eq:infosedd}} We first break $\expected\left[\log\frac{\bp_T}{\bq_T}\bigg|\bproc_0\right]$ in $\expected\left[\log\bp_T\bigg|\bproc_0\right]-\expected\left[\log\bq_T\bigg|\bproc_0\right]$ and apply the Dynkin's formula separately. We start with $\expected\left[\log\bp_T\bigg|\bproc_0\right]$:

{\scriptsize\begin{align}
    \expected\left[\log\bp_T\bigg|\bproc_0\right] \nonumber&= \log\bp_0(\bproc_0)+\expected\left[\int_0^T\frac{\partial \log\bp_t}{\partial t}(\bproc_t,t)+\boperator[\log\bp_t](\bproc_t,t)dt\bigg|\bproc_0\right] \\
    & = \log\bp_0(\bproc_0)+\expected\left[\int_0^T\frac{\partial \log\bp_t}{\partial t}(\bproc_t,t)+\sum_{y\neq \bproc_t}\bratesp_t(y,\bproc_t)(\log\bp_t(y)-\log\bp_t(\bproc_t))dt\bigg|\bproc_0\right] \nonumber\\
    & = \log\bp_0(\bproc_0)+\expected\left[\int_0^T\frac{\partial \log\bp_t}{\partial t}(\bproc_t,t)+\sum_{y\neq \bproc_t}\bratesp_t(y,\bproc_t)\log\frac{\bp_t(y)}{\bp_t(\bproc_t)}dt\bigg|\bproc_0\right] \nonumber
\end{align}}

We now focus on the term $\frac{\partial \log\bp_t}{\partial t}$, which we can rewrite using the definition given in \eqref{eq:bctmc}:

\begin{align}
    \frac{\partial \log\bp_t}{\partial t} = \frac{\partial \bp_t}{\partial t}\bigg/ \bp_t = \frac{\bratesp_t\bp_t}{\bp_t}
\end{align}

Where the division between numerator and denominator in the last two terms denotes element-wise division. We focus now on simplifying the numerator, using the definition in \eqref{eq:ratesdef}. Recalling that

\begin{align}\label{eq:ratesdef}
    \bratesp_t(y,x) &= \left(\frac{\fp_{T-t}(y)}{\fp_{T-t}(x)} \fratesp_{T-t}(x,y)\right) (1-\delta(x,y))+\left(-\sum_{y\neq x}\bratesp_t(y,x)\right)\delta(x,x) \\ &= \left(\frac{\bp_{t}(y)}{\bp_{t}(x)} \fratesp_{T-t}(x,y)\right) (1-\delta(x,y))+\left(-\sum_{y\neq x}\bratesp_t(y,x)\right)\delta(x,x)
\end{align}

we compute the $x$-th element of $\bratesp_t\bp_t$:

\begin{align}
    [\bratesp_t\bp_t](x) &= \sum_y \bratesp_t(x,y)\bp_t(y) \\&= \sum_{y\neq x} \bratesp_t(x,y)\bp_t(y) + \bratesp_t(x,x)\bp_t(x)\nonumber\\
    &= \sum_{y\neq x} \fratesp_{T-t}(y,x)\frac{\bp_t(x)}{\bp_t(y)}\bp_t(y) - \sum_{y\neq x}\bratesp_t(y,x)\bp_t(x)\nonumber\\
    &= \sum_{y\neq x} \fratesp_{T-t}(y,x)\bp_t(x) - \sum_{y\neq x}\fratesp_{T-t}(x,y)\frac{\bp_t(y)}{\bp_t(x)}\bp_t(x)\nonumber\\
    &= \sum_{y\neq x} \fratesp_{T-t}(y,x)\bp_t(x) - \sum_{y\neq x}\fratesp_{T-t}(x,y)\bp_t(y)\nonumber
    \\
    &= \sum_{y\neq x} \fratesp_{T-t}(y,x)\bp_t(x) - \fratesp_{T-t}(x,y)\bp_t(y)\nonumber
\end{align}

Finally, if we divide by the denominator we obtain:

{\begin{align}
    \left[\frac{\bratesp_t\bp_t}{\bp_t}\right](x) = &\frac{\sum_{y\neq x} \fratesp_{T-t}(y,x)\bp_t(x) - \fratesp_{T-t}(x,y)\bp_t(y)}{\bp_t(x)} \\ = &  \sum_{y\neq x} \fratesp_{T-t}(y,x) - \fratesp_{T-t}(x,y)\frac{\bp_t(y)}{\bp_t(x)} \nonumber
\end{align}}

Moreover, if we notice that $\bratesp_t(y,\bproc_t)\log\frac{\bp_t(y)}{\bp_t(\bproc_t)}=\fratesp_{T-t}(\bproc_t,y)\frac{\bp_t(y)}{\bp_t(\bproc_t)}\log\frac{\bp_t(y)}{\bp_t(\bproc_t)}$, we can write the Dynkin's formula as:

{\begin{align}\label{eq:dynkinp}
    &\expected\left[\log\bp_T|\bproc_0\right] = \\ &\log\bp_0(\bproc_0)+ \nonumber\\ &\expected\left[\int_0^T\sum_{y\neq \bproc_t} \fratesp_{T-t}(y,\bproc_t) - \fratesp_{T-t}(\bproc_t,y)\frac{\bp_t(y)}{\bp_t(\bproc_t)}+\fratesp_{T-t}(\bproc_t,y)\frac{\bp_t(y)}{\bp_t(\bproc_t)}\log\frac{\bp_t(y)}{\bp_t(\bproc_t)}dt\bigg|\bproc_0\right] \nonumber
\end{align}}

Then, if we define $K(a)=a(\log a - 1)$ and group some terms we obtain:

\begin{align}
    \expected\left[\log\bp_T|\bproc_0\right] &= \\ \log\bp_0(\bproc_0)&+\expected\left[\int_0^T\sum_{y\neq \bproc_t} \fratesp_{T-t}(y,\bproc_t) + \fratesp_{T-t}(\bproc_t,y)K\left(\frac{\bp_t(y)}{\bp_t(\bproc_t)}\right)dt\bigg|\bproc_0\right] \nonumber
\end{align}

We now repeat similar calculations for $\expected\left[\log\bq_T\bigg|\bproc_0\right]$. Firstly, the term $\frac{\partial \log\bq_t}{\partial t}$ becomes:

\begin{equation}
    \left[\frac{\partial \log\bq_t}{\partial t}\right](x) = \sum_{y\neq x} \fratesp_{T-t}(y,x) - \fratesp_{T-t}(x,y)\frac{\bq_t(y)}{\bq_t(x)}
\end{equation}

Whereas the backward operator term $\boperator[\log\bq_t](\bproc_t,t)$ becomes:
\begin{equation}
    \bratesp_t(y,\bproc_t)\log\frac{\bq_t(y)}{\bq_t(\bproc_t)}=\fratesp_{T-t}(\bproc_t,y)\frac{\bp_t(y)}{\bp_t(\bproc_t)}\log\frac{\bq_t(y)}{\bq_t(\bproc_t)}
\end{equation}

Putting everything together gives:

{\begin{align}\label{eq:dynkinq}
    &\expected\left[\log\bq_T\bigg|\bproc_0\right] = \\ &\log\bq_0(\bproc_0)+ \nonumber\\&\expected\left[\int_0^T\sum_{y\neq \bproc_t} \fratesp_{T-t}(y,\bproc_t) - \fratesp_{T-t}(\bproc_t,y)\frac{\bq_t(y)}{\bq_t(\bproc_t)}+\fratesp_{T-t}(\bproc_t,y)\frac{\bp_t(y)}{\bp_t(\bproc_t)}\log\frac{\bq_t(y)}{\bq_t(\bproc_t)}dt\bigg|\bproc_0\right] \nonumber
\end{align}}

Finally, we can estimate $\expected\left[\log\frac{\bp_T}{\bq_T}\bigg|\bproc_0\right]$ by subtracting \eqref{eq:dynkinq} from \eqref{eq:dynkinp}:

{\scriptsize\begin{equation}
    \expected\left[\log\frac{\bp_T}{\bq_T}\bigg|\bproc_0\right] \approx \expected\left[\int_0^T\sum_{y\neq \bproc_t} \fratesp_{T-t}(\bproc_t,y)\left(K\left(\frac{\bp_t(y)}{\bp_t(\bproc_t)}\right)+\frac{\bq_t(y)}{\bq_t(\bproc_t)}-\frac{\bp_t(y)}{\bp_t(\bproc_t)}\log\frac{\bq_t(y)}{\bq_t(\bproc_t)}\right)dt\bigg|\bproc_0\right] \nonumber
\end{equation}}

By using the fact that $\expected\left[\expected\left[\log\frac{\bp_T}{\bq_T}\bigg|\bproc_0\right]\right]=\expected\left[\log\frac{\bp_T}{\bq_T}\right]$, $\bp_t=\fp_{T-t}$, $\bq_t=\fq_{T-t}$, $\bproc_t=\fproc_{T-t}$ and by setting $\tau=T-t$, we get:

\begin{align}
&\expected\left[\int_0^T\sum_{y\neq \fproc_\tau} \fratesp_{\tau}(\fproc_\tau,y)\left(K\left(\frac{\fp_\tau(y)}{\fp_\tau(\fproc_\tau)}\right)+\frac{\fq_\tau(y)}{\fq_\tau(\fproc_\tau)}-\frac{\fp_\tau(y)}{\fp_\tau(\fproc_\tau)}\log\frac{\fq_\tau(y)}{\fq_\tau(\fproc_\tau)}\right)d\tau\right]
\end{align}

recovering \Cref{eq:infosedd}.

\subsection{Proof $\frac{\fu_t(x)}{\fu_t(\absorb)}=\ratio$ for $x\neq\absorb$}\label{sec:absorb_ratio}

{\small\begin{align}
    \frac{\fu_t(x)}{\fu_t(\absorb)} = \frac{\sum_{x_0\in\support}\fu_t(x|x_0)\fu_0(x_0)}{\sum_{x_0\in\support}\fu_t(\absorb|x_0)\fu_0(x_0)}
    = \frac{\sum_{x_0\in\support}\delta(x,x_0)e^{-\cumnoise(t)}\frac{1}{N}}{\sum_{x_0\in\support}(1-e^{-\cumnoise(t)})\frac{1}{N}} \nonumber = \frac{e^{-\cumnoise(t)}}{N(1-e^{-\cumnoise(t)})} = \ratio
\end{align}}

\subsection{Proof of \Cref{eq:free_marginal}}\label{sec:free_marginal}

Consider $\fp_t(\fproc_t=\xbar,\fprocy_t=\absorb)$:

\begin{align}\label{eq:marg_score}
    &\fp_t(\fproc_t=\xbar,\fprocy_t=\absorb) = \sum_{x,y}\prob(\fproc_t=\xbar, \fprocy_t=\absorb, \fproc_0=x,\fprocy_0=y)\\ &= \sum_{x,y}\underset{\pjump}{\underbrace{\prob(\fprocy_s=\absorb\g\fproc_t=\xbar,\fproc_0=x,\fprocy_0=y)}}\prob(\fproc_t=\xbar\g\fproc_0=x,\fprocy_0=y)\prob(\fproc_0=x,\fprocy_0=y) \nonumber \\ &= \sum_{x,y}\pjump\prob(\fproc_t=\xbar\g\fproc_0=x)\prob(\fproc_0=x,\fprocy_0=y) \nonumber \\ &= \pjump\sum_x\prob(\fproc_t=\xbar\g\fproc_0=x)\underset{\prob(\fproc_0=x)}{\underbrace{\sum_y\prob(\fproc_0=x,\fprocy_0=y)}} \nonumber \\ & = \pjump\prob(\fproc_t=\xbar) \nonumber
\end{align}

\Cref{eq:marg_score} implies that $\frac{\fp_t(\fproc_t=x,\fprocy_t=\absorb)}{\fp_t(\fproc_t=\xbar,\fprocy_t=\absorb)} = \frac{\prob(\fproc_t=x)}{\prob(\fproc_t=\xbar)}$. This important property enables the estimation of mutual information without modifying the score network. 




\section{Experimental details}

\subsection{Dataset generation}\label{sec:isomorphisms}
In our experiments, we exploit the additivity of mutual information with independent random variables to generate complex datasets. By appending discrete noise random variables $Z_\rvx$, $Z_\rvy$ to the original random variables $\fproc_0$, $\fprocy_0$, we have $I(\fproc_0,\fprocy_0) = I([\fproc_0, Z_\rvx],[\fprocy_0, Z_\rvy])$. Pairing functions are isomorphisms that map $\sN\times\sN$ to $\sN$. They allow preserving mutual information through the Markov Chain:

\begin{equation}
    [\fproc_0,Z_\rvx] \to \hat{X}_0 \to \hat{Y}_0 \to [\fprocy_0,Z_\rvy] \to \hat{Y}_0 \to \hat{X}_0 \to [\fproc_0,Z_\rvx]
\end{equation}

Where, by the \textit{data processing inequality}, we have $I([\fproc_0,Z_\rvx],[\fprocy_0,Z_\rvy]) \geq I(\hat{X}_0,\hat{Y}_0) \geq I([\fproc_0,Z_\rvx],[\fprocy_0,Z_\rvy]) \implies I([\fproc_0,Z_\rvx],[\fprocy_0,Z_\rvy]) = I(\hat{X}_0,\hat{Y}_0) \implies I(\hat{X}_0,\hat{Y}_0) = I(\fproc_0,\fprocy_0)$.

In \Cref{sec:synthetic} we keep the same support dimension for both random variables, increasing it using the following procedure:
\begin{enumerate}
    \item We sample two binomial random variables $Z_\rvx,Z_\rvy$ with parameters $(n,p)$, %where $n$ represents the number of experiments for the binomial random variable and $p$ represents the probability of success of each experiment. 
    We vary $n$ for increasing the complexity of the experiment and we keep $p$ fixed to $0.5$.
    \item We concatenate $Z_\rvx$ and $Z_\rvy$ respectively to $\fproc_0$ and $\fprocy_0$, to form higher support versions $\hat{X}_0,\hat{Y}_0$.
    \item We map the noisy $\hat{X}_0,\hat{Y}_0$ versions to univariate random variables by applying Cantor's mapping.
\end{enumerate}

\subsection{Model and training setup}\label{sec:training_details} For our method, we use a Multi Layer Perceptron (MLP) with skip connection, based on the architecture used in \cite{franzese2023minde}, reworking the initial layer to include absolute positional embeddings. For training, we match the methodology used by \cite{lou2024discrete}, using the absorb configuration. Similarly, we follow prior work to train other models in the synthetic benchmark. At inference time, we always take the last valid validation step estimate of each model to avoid not-a-number values in our tables.


\subsubsection{Synthetic benchmark} In this section, we describe how we scaled the architectures with increasing complexity in the synthetic benchmark.
We benchmark our competitors using architectures from prior work \citep{franzese2023minde, belghazi2018mine}. Depending on the task, some architectures are forced to increase the number of parameters. For example, \textsc{Info-SEDD} is forced to increase the number of parameters with the support size. In order to maintain a fair comparison between different models, we make the other architectures match the number of parameters of the architecture which is forced to include more parameters by either increasing the number of layers or by increasing the layer width.

\paragraph{Big support experiments} For supports smaller than $256$, we use architectures with a comparable number of parameters, in the order of $20k$. Instead, after this support dimension we increase the number of parameters, with an order of $70k$ parameters for support dimension $256$ and an order of $300K$ for support dimension $1024$.

\paragraph{Representation length experiments}For vectors of length shorter than $512$, we use architectures with a comparable number of parameters, remaining in the order of $20k$. Instead, after this length we increase the number of parameters, with an order of $50k$ parameters for length $512$ and an order of $150k$ for length $2048$.

\paragraph{High mutual information experiments} In this case we keep the number of parameters fixed on the order of $20k$, as fixed representation length and fixed support dimension allow MINE and MINDE to keep the number of parameters constant. 

\subsubsection{Ising model experiments}\label{subsec:app_ising} 

The Ising model is a system consisting of particles arranged in a lattice. In our experiments, we consider a $L\times L$ square. A particle $i$ of the lattice is associated with a discrete value $\sigma_{i}\in\{-1,+1\}$ called spin and each pair of particles $ij$ is characterized by an interaction strength $J_{ij}$. With no external fields, these quantities determine the energy $E(\sigma)$ of the configuration $\sigma$: 

\begin{equation}
E(\sigma)=\sum_{i,j}J_{ij}\sigma_i\sigma_j
\end{equation}

In turns, the energy of a configuration determines its likelihood. In particular, the configurations of the Ising model follow a probability distribution $\fp_0$ parametrised by the temperature $T$, the Boltzmann constant $\boltzmann$ and the interaction strengths:

\begin{equation}
    \fp_0(\sigma) = \frac{e^{-\beta E(\sigma)}}{Z(T)}
\end{equation}

Where $Z(T)=\sum_{i}e^{-\beta E(\sigma_i)}$ and $\beta=(\boltzmann T)^{-1}$. In order to generate our dataset from $\fp_0$, we follow the Metropolis algorithm \citep{bhanot1988metropolis}:

\begin{algorithm}[H]
\caption{Metropolis Algorithm for 2D Ising Spin Glass}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Lattice size \( N \), interaction strengths \( J_{ij} \), temperature \( T \), number of iterations \( \text{iter\_max} \)
\STATE \textbf{Initialize:} Spin lattice \( \sigma \) with \( \sigma_{i,j} \in \{-1, +1\} \) randomly assigned
\FOR{iteration = 1 to \( \text{iter\_max} \)}
    \STATE Randomly select a lattice site \(i \)
    \STATE Compute the change in energy \( \Delta E \) if \( \sigma_{i,j} \) is flipped:
    \[
    \Delta E = 2 \, \sigma_i \sum_{j} J_{i,j} \, \sigma_{j}
    \]
    \STATE Generate a random number \( r \) uniformly distributed in \( [0, 1] \)
    \IF{ \( r < \exp\left(-\beta\Delta E\right) \) }
        \STATE Flip the spin: \( \sigma_i \leftarrow -\sigma_i \)
    \ENDIF
\ENDFOR
\STATE \textbf{Output:} Final spin configuration \( \sigma \)
\end{algorithmic}
\end{algorithm}

We compute the entropy of $\fp_0$ analytically, starting from the free energy $F$ per site of the lattice:

\begin{equation}\label{eq:free_energy}
    F(T) = -\boltzmann T\log\pf_T
\end{equation}
Where $\pf_T$ is the partition function, which depends on the interaction horizontal and vertical interaction strength. For simplicity, we consider the same interaction strength $J=1$ for all neighboring particles, while we set it to zero for non neighboring particles. Under these assumptions, we can calculate $\log\lambda$ with a double integral\citep{onsager1944crystal}:

{\footnotesize\begin{equation}
    \log\pf_T = \log 2 + \frac{1}{2\pi^2}\int_0^\pi\int_0^\pi\log(\cosh(2\beta J)\cosh(2\beta J)-\sinh(2\beta J)\cos(\theta_1)-\sinh(2\beta J)\cos(\theta_2))d\theta_1d\theta_2
\end{equation}}

For simplicity, we also set $\boltzmann=1$. From $F$, we can calculate the entropy $H$ using the thermodynamic relation $H=-\frac{\partial F}{\partial T}$. We compute the integral numerically using the \textit{SciPy} Python package \citep{2020SciPy-NMeth} and we approximate $H$ as $H\approx \frac{F(T+\Delta T)-F(T-\Delta T)}{2\Delta T}$, with $\Delta T = 10^{-4}$.

For what concerns the \textsc{Info-SEDD} architecture, we keep a single model configuration for all temperatures, both for the model, which contains around $90k$ parameters, and for the diffusion. To get the entropy per site, we divide the estimates of the model by the number of particles in the configurations (400).