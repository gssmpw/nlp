In this section, we numerically validate the performance of \infosedd\ on both synthetic and real-world datasets. Specifically, we evaluate the mutual information and entropy estimators presented in \Cref{alg:infosedd} and \Cref{alg:compute_entropy}, respectively, through the following experiments: (i) benchmarking on high-dimensional distributions where ground truth values are known by construction, and (ii) assessing the accuracy of our method in a \textit{real-world} application by estimating the entropy of spin glass configurations in the Ising model \citep{onsager1944crystal}.



\subsection{Synthetic benchmark}\label{sec:synthetic}


\input{synthetic}

\paragraph{Experimental Setup} To evaluate the performance of our mutual information estimator, \infosedd, we design three sets of controlled synthetic experiments, each isolating a key aspect of data complexity. In the first experiment, we fix the mutual information value at $0.5$ and the length of the random vector at $2$, meaning each of the two random variables has one dimension. We then increase the support size using Cantor's mapping (\Cref{sec:isomorphisms}). The second experiment maintains a mutual information value of $0.5$ and binary support for each element of the random vector, but instead increases the length of the representation vector. Finally, in the third experiment, we generate distributions with varying mutual information values while keeping the support binary for each variable. The length of the random vectors is fixed at $10$, and mutual information values are linearly spaced from $0$ to $5$. We compare the results of our proposed methodology against MINE \citep{belghazi2018mine}, a variational neural estimator, MINDE \citep{franzese2023minde}, a generative neural estimator and KSG \citep{kraskov2004estimating}, a \textit{classical} statistical estimator. For a fair comparison, we use \glspl{MLP} based architectures for the neural estimators and scale parameters appropriately when needed (see \Cref{sec:training_details}). When an estimate is not available, we fill the entry in the tables with $-$. We report all results in nats.

\input{tables_and_figure}

\paragraph{Results and Analysis} The results from these experiments, shown in \Cref{tab:mi_big_support_table,tab:mi_big_vector_table,tab:mi_estimators}, demonstrate that \infosedd\ consistently outperforms competing methods. Notably, our model excels in settings where an inherent property of discrete systems—such as support size, representation length, or mutual information—introduces increased complexity.

Although our competitors perform relatively well in some benchmarks, they fail in at least one experiment. KSG performs well with large support size (\Cref{tab:mi_big_support_table}), but it fails when considering higher dimensions (\Cref{tab:mi_big_vector_table}). MINE and MINDE, on the contrary, excels with higher dimensions but struggle with large support size. MINDE also struggles more with higher mutual information values (\Cref{tab:mi_estimators}). Overall, these experiments motivate the usage of adequate neural estimators when dealing with discrete distributions. 

%Neural information metrics estimators essentially rely on a neural network to decode the relevant information for this task. When the data generating process becomes more convoluted, which in our case means injecting increasingly more complex nuisance in the dataset, information is harder to decode and extracting insights from low density regions of the data distributions becomes more important. Generative modeling approaches offer adequate inductive bias to encourage the exploration of the data distribution, while other neural estimators like MINE restrict themselves to the available training data. As a consequence, MINE is not able to decode the Cantor's pairing function, with a poor performance in related experiments (\Cref{tab:mi_big_support_table}). Additionally, efficiently exploring discrete data distributions remains a daunting task. Score-based generative models like MINDE waste many training steps exploring zero density regions due to their continuous nature, leading to slow convergence to reach the right estimate in experiments with big support (\Cref{tab:mi_big_support_table}) and higher mutual information (\Cref{tab:mi_estimators}). Moreover, training objectives relying on noise prediction use the Euclidean distance to calculate their loss, which is inadequate in discrete settings, especially in the case of non-ordinal data. Info-SEDD, instead, efficiently explores discrete data distribution by exploiting CTMC, leading to a more accurate mutual information estimation. Overall, these experiments motivate the usage of adequate neural estimators when dealing with discrete distributions. 
%\vspace*{-1cm}
\subsection{Spin glasses experiments}
Entropy computation in Ising models enables insights on the thermodynamics properties of the system \citep{cincio2007entropy}, which can be used for scientific discovery in the domain where the Ising model is applied \citep{macy2024ising, schneidman2006weak, sherrington1975solvable}. 

\paragraph{Experimental setup} We consider a simplified Ising model applied to spin glasses \citep{sherrington1975solvable}. We do not include an external field, we set a unitary interaction strength for all the sites interactions, unitary Boltzmann's constant and a $20\times20$ square lattice. The entropy per site of this configuration can be computed in closed form \cite{onsager1944crystal}. We test our model by estimating the entropy per particle at linearly spaced temperatures from $1.0K$ to $4.0K$. We generate our dataset using the Metropolis-Hastings algorithm, with $10000$ samples for each temperature (see \Cref{subsec:app_ising}). We post-process the output of \infosedd\ by dividing the entropy estimate by $400$ to report the entropy per site.

\paragraph{Results and Analysis} Variational estimators cannot estimate large \glspl{KL} divergences reliably with limited samples sizes \citep{mcallester2020formal,song2019understanding}. In this scenario, instead, \infosedd\ accurately estimates large \glspl{KL} divergences (\Cref{fig:ising}), performing particularly well at low temperatures where we need to estimate large \glspl{KL} divergences.