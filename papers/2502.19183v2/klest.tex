In this Section, we explore the relationship between \glspl{CTMC} over discrete state spaces \citep{anderson2012continuous} and the computation of \gls{KL} divergences. First, in \Cref{subsec:ctmc}, we provide a brief introduction to the fundamentals of \glspl{CTMC}, emphasizing their time-reversal properties and parametric approximations \citep{lou2024discrete}. Then, in \Cref{subsec:klfor}, we demonstrate how these processes can be adapted for divergence estimation, specifically by analyzing two processes that share the same generator but differ in their initial conditions.

\subsection{Preliminaries}\label{subsec:ctmc}
Consider a \gls{CTMC} $\fproc_t, t \in[0,T]$, defined over a finite state space $\support = \left\{1,\hdots,N\right\}$ and  specified by the infinitesimal generators $\fratesp_t: [0,T]\rightarrow \mathbb{R}^{N\times N}$, where the diagonal entries statisfy $\fratesp_t(x,x)=-\sum_{x\neq y}\fratesp_t(x,y)$, with $\fratesp_t(x,y)\geq 0, x\neq y$. As established in \citep{anderson2012continuous}, the time evolution of the probability distribution $Pr(X_t=i)\defeq \fp_t(i)$, satisfies the following \gls{ODE} 
\begin{equation}\label{for_ode}
\fp_t =\fp_0+\int_0^t \fp_s\fratesp_s ds,
\end{equation}
where the initial conditions of the process $\fp_0$ determine the distribution $\fp_t$ at any time $t$. 

A key property of \glspl{CTMC} is that their time-reversed counterpart, recently utilized in generative modeling \citep{lou2024discrete}, also follows a \gls{CTMC} but with a different set of transition matrices. More precisely, defining the time-reversed process as $\bp_t\defeq\fp_{T-t}$, it evolves according to \citep{lou2024discrete,sun2023scorebasedcontinuoustimediscretediffusion}:
\begin{align}\label{eq:bctmc}
    \bp_t =\fp_T+\int_0^t \bratesp_s\bp_s ds,
\end{align}
where the reverse-time transition matrices $\bratesp_t$ relate to the forward transition matrices as follows:

\begin{align}\label{eq:ratesdef}
    \bratesp_t(y,x) &= \left(\frac{\fp_{T-t}(y)}{\fp_{T-t}(x)} \fratesp_{T-t}(x,y)\right) (1-\delta(x,y))+\left(-\sum_{y\neq x}\bratesp_t(y,x)\right)\delta(x,x)
\end{align}

Under appropriate technical conditions on $\fratesp_t$ \citep{lou2024discrete} the terminal distribution $\fp_T$ converges to a known reference distribution $\pi$, which is independent of the initial distribution $\fp_0$. This property enables sampling from $\fp_0$ by simulating a \gls{CTMC} with appropriately chosen generators \citep{sun2023scorebasedcontinuoustimediscretediffusion, kelly1981reversibility}. However, other than simple and uninteresting scenarios, exact knowledge of the quantities $\frac{\fp_{T-t}(y)}{\fp_{T-t}(x)}$ is out of reach. A practical solution is to substitute in this numerical integration a parametric function $\scorep(x,t)_y$, whose parameters are optimized according to \citet{lou2024discrete}:

{\small\begin{equation}\label{eq:lossfn}
    \mathcal{L}(\theta) = \expected\left[\int_0^T\sum_{y\neq \fproc_t}\fratesp_t(\fproc_t,y)\left(\scorep(\fproc_t,t)_y-\frac{\fp_t(y|\fproc_0)}{\fp_t(\fproc_t|\fproc_0)}\log\scorep(\fproc_t,t)_y+K\left(\frac{\fp_t(y|\fproc_0)}{\fp_t(\fproc_t|\fproc_0)}\right)\right)dt\right]
\end{equation}}

Where $\fp_t(\cdot|x_0)$ is a known perturbation kernel, obtained from \Cref{for_ode} with the deterministic initial distribution centered in $x_0$, and $K(a)=a(\log a-1)$. Whenever the context is clear, we simplify the notation for the parametric score in the remainder of the paper and denote 
$\scorep(\fproc_t)_y$ instead of $\scorep(\fproc_t,t)_y$.
 

\subsection{\gls{KL} Divergences via \glspl{CTMC}}\label{subsec:klfor}

In this work, we leverage the \gls{CTMC} framework to compute the KL divergence between two probability distributions $\fp_0$ and $\fq_0$ defined over the same support $\support$, expressed as $\KL{\fp_0}{\fq_0}$. To achieve this, we construct two Markov chains that differ only in their initial conditions: one initialized from $\fp_0$ whose time evolution follows \Cref{eq:bctmc}, and the other initialized from $\fq_0$, which evolves analogously by substituting $\fq_t$ for $\fp_t$ in \Cref{eq:bctmc}. Since the KL divergence satisfies $\KL{\fp_0}{\fq_0}=\expected\left[\log\frac{\fp_0}{\fq_0}(\fproc_0)\right]$, we can equivalently express it as
\begin{equation}\label{eq:kl_expressions}
    \expected\left[\log\frac{\fp_0}{\fq_0}(\bproc_T)\right] =\expected\left[\log\frac{\bp_T}{\bq_T}(\bproc_T)\right] = \expected \left[\expected\left[\log\frac{\bp_T}{\bq_T}(\bproc_T)\bigg|\bproc_0\right]\right].
\end{equation}

The last term in \eqref{eq:kl_expressions} can be rewritten using Dynkin's formula \cite{hanson2007applied}, which states that for a generic function \( f:\support\times[0,T]\to\bbR \), we have:

\begin{equation}\label{eq:dynkin}
    \expected\left[f(\bproc_T,T)\bigg|\bproc_0\right]- f(\bproc_0,0)=\expected\left[\int_0^T\frac{\partial f}{\partial t}(\bproc_t,t)+\boperator[f](\bproc_t,t)dt\bigg|\bproc_0\right]
\end{equation}

where \( \boperator \) is the \textit{backward operator}, defined in our setting as:

\begin{equation}
    \boperator[f](x,t) = \sum_{y\neq x}\bratesp_t(y,x)(f(y)-f(x)).
\end{equation}

By combining the result from \Cref{eq:dynkin} with \Cref{eq:kl_expressions}, we obtain the following expression for the KL divergence between discrete distributions \( \KL{\fp_0}{\fq_0} \):

\begin{align}\label{eq:infosedd}
&\expected\left[\int_0^T\sum_{y\neq \fproc_t} \fratesp_{t}(\fproc_t,y)\left(K\left(\frac{\fp_t(y)}{\fp_t(\fproc_t)}\right)+\frac{\fq_t(y)}{\fq_t(\fproc_t)}-\frac{\fp_t(y)}{\fp_t(\fproc_t)}\log\frac{\fq_t(y)}{\fq_t(\fproc_t)}\right)dt\right]
\end{align}

where \( K(a)=a(\log(a)-1) \). The missing term \( \expected\left[\log{\frac{\bp_0}{\bq_0}}(\bproc_0)\right] \) is omitted, as both \( \bp_0 \) and \( \bq_0 \) converge to \( \pi \) \citep{lou2024discrete}.

While \Cref{eq:infosedd} provides a complete formulation for estimating the divergence of interest, in practical applications—similar to those in generative modeling \citep{lou2024discrete, ren2024discrete, holderrieth2024generator}—the key quantities \( \frac{\fp_{t}(y)}{\fp_{t}(x)} \) and \( \frac{\fq_{t}(y)}{\fq_{t}(x)} \) are not directly accessible. To address this, we adopt the approach of \citet{lou2024discrete}, replacing these unknown ratios with parametric approximations optimized via \Cref{eq:lossfn}. This leads to the construction of the estimator:

\begin{align}\label{eq:infosedd-param} 
&\int_0^T\expected\left[\sum_{y\neq \fproc_{t}} \fratesp_{t}(\fproc_{t},y)\left(K\left(\scorep(\fproc_{t})_y\right)+\scoreq(\fproc_{t})_y-\scorep(\fproc_{t})_y\log\scoreq(\fproc_{t})_y\right)\right] dt
\end{align}

where \( \scorep(\bproc_t)_y \approx \frac{\bp_t(y)}{\bp_t(\bproc_t)} \) and \( \scoreq(\bproc_t)_y\approx\frac{\bq_t(y)}{\bq_t(\bproc_t)} \) serve as parametric approximations of the true ratios. Estimating \Cref{eq:infosedd-param} using Monte Carlo techniques is conceptually straightforward: we sample time instants \( t \) uniformly in \( [0,T] \), simulate the forward process \( X_t \), and compute the required quantities using the parametric scores. However, as we will discuss in the next Section, the estimator in its general form is not scalable for computing information metrics, which is the primary focus of this work.
