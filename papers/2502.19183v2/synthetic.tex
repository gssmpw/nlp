Real-world data distributions grow rapidly in complexity. Modern language models \citep{radford2019language,lou2024discrete} process sequences of thousands of tokens from vocabularies of tens of thousands values. These correspond to the structured discrete variables introduced earlier, where tokens map to sequence length $M$, and vocabulary size corresponds to the number of states per subcomponent $N$. As mutual information quantifies dependencies, its value increases rapidly with $N^M$, making estimation challenging.

To evaluate the feasibility of mutual information estimation in high-dimensional discrete settings, we design synthetic experiments that highlight the limitations of estimators originally developed for continuous variables. We control three key factors: support size ($N$), representation dimension ($M$), and mutual information value. By varying one factor at a time, we systematically assess whether continuous-variable estimators fail while \textsc{info-sedd} scales effectively.

We generate joint distributions for random variables $X, Y$ with user-defined mutual information and support sizes $\support_X, \support_Y$. Using an evolutionary strategy, we encode the joint distribution in a vector $g_a \in \mathbb{R}^{|\support_X||\support_Y|}$ and transform it into a valid probability distribution via normalization and reshaping: $\frac{g_a-\text{min}(g_a)\ones+\epsilon\ones}{\ones(g_a-\text{min}(g_a)\ones+\epsilon\ones)}$ where $\epsilon$ ensures full support. The mutual information, computable in closed form, serves as the selection criterion in the evolutionary process. For large $|\support_X|$ and $|\support_Y|$, the evolutionary strategy struggles. Instead, we generate high mutual information distributions by concatenating independent distributions, leveraging the additive property of mutual information. Additionally, isomorphisms like Cantorâ€™s pairing function, $\pi(x,y) = \frac{1}{2}(x+y)(x+y+1)+y$, enables support expansion without altering mutual information, aiding consistency across experiments. Full details are in \Cref{sec:isomorphisms}.