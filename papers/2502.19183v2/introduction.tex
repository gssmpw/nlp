Information theoretic quantities represent a powerful tool to understand non-linear relationships between random variables \citep{shannon1948mathematical, mackay2003information} and find wide range of applications in scientific fields \citep{Karbowski_2024, Eckford_2016}. Mutual information, in particular, has become an established metric in machine learning \citep{bell1995information, stratos2018mutual, belghazi2018mine, oord2019representation, hjelm2019learning}, both for training models \citep{alemi2019deep, chen2016infogan, zhao2018information} and at inference time \citep{alemi2019gilbo, huang2020evaluating}.

Estimating information theoretic quantities remains an open problem, and different paradigms for their estimation emerged. Classical parametric and non-parametric methods \citep{pizer1987adaptive, moon1995estimation, kraskov2004estimating, gao2015efficient} have been recently superseded by variational approaches \citep{barber2004algorithm, nguyen2007neurips, nowozin2016neurips, poole2019variational, wunder2021reverse, letizia2023variational, federici2023effectiveness} and neural estimators \citep{papamakarios2017masked, belghazi2018mine, oord2019representation, song2019, rhodes2020telescoping, letizia2022copula, brekelmans2023improving,franzese2023minde,butakov2024mutual}. Despite its practical importance, few estimators for high-dimensional \textbf{discrete} distributions have been proposed in the literature. While classical estimators for discrete random variables exist \citep{pinchas2024comparative}, their accuracy rapidly decrease with increasing dimensionality of the considered problem. Applications that would benefit from scalable estimators of mutual information quantities include, among others, DNA or peptide sequencing \citep{newcomb2021use, xiaadanovo}, text summarization \citep{darrin2024textttcosmicmutualinformationtaskagnostic} and neuroscience \citep{chai2009exploring}. Consequently, the development of new estimation techniques is of paramount importance for the broader scientific community.

The current approach to solve the problem of lack of a viable estimator for discrete, high dimensional scenarios, is to embed in a real space the discrete quantities and then adopt neural estimators conceived for continuous distributions. One recent example is \citep{lee2024benchmarksuiteevaluatingneural}, where it is showed that the embeddings of pretrained language models can provide meaningful representations to estimate information theoretic quantities in unstructured data. However such a process may not fully capture the discrete nature of the underlying data and might suffer from several limitations, such as the necessity to consider embeddings which are application specific.

One extremely promising class of estimators which has been recently been considered in the continuous state space settings has its roots in generative diffusion models \cite{song2021a,ho2020}. While these generative models have been successfully considered in continuous settings for estimating information metrics \citep{franzese2023minde,kong2023information,bounouas}, a discrete state space adaptation is currently not available. In this work we fill this important gap and present \infosedd, a novel method for estimating information theoretic quantities of discrete data using \glspl{CTMC} \citep{lou2024discrete}. These stochastic processes have recently saw a surge in popularity due to the associated generative modelling applications as direct counterpart of the continuous state space models \cite{song2021a,ho2020}. Their fundamental working principle is the reversal of a perturbation process which starts from clean data from a given distribution and converge to uninformative noise. The workhorse of these approaches is the \textit{score function}, which contains information about the probability distributions associated to the \glspl{CTMC} at different time instants. Our proposed method, \infosedd, builds upon the same fundamental mathematical framework, extending it via Dynkin's lemma \citep{hanson2007applied}, and leverages score functions to compute key information-theoretic metrics, such as mutual information between two random variables and the entropy of a given distribution. By carefully selecting perturbation processes, our approach requires training only a single parametric model to compute mutual information across arbitrary subsets of variables. Furthermore, \infosedd\ seamlessly integrates with pretrained networks, enabling the reuse of computational resources already expended in training generative models. 

To rigorously evaluate our method, we design a benchmark that presents challenges across three critical dimensions: (1) the support size of the random variables, which defines the range of possible values each variable can take; (2) the dimensionality of the variable representations, referring to the number of components each variable contains; and (3) the mutual information value, which captures the complexity of dependencies between variables. Our results demonstrate that \infosedd\ is both robust and consistently outperforms neural estimation methods that rely on embedding techniques.
Beyond synthetic benchmarks, we further assess the practical utility of our approach by applying it to the real-world problem of estimating the entropy of the Ising model \citep{onsager1944crystal}. The Ising model is a paradigmatic example of a complex system with broad applications in statistical physics, neuroscience, and machine learning. Crucially, it provides a well-characterized ground truth for entropy, making it an ideal test case for evaluating the accuracy of information-theoretic estimators in high-dimensional discrete distributions.%{\citet{nir2020machine} already developed a successful neural estimator for this problem. However, they did so by building on top of \citet{belghazi2018mine}, falling in the pitfalls highligted by \citet{mcallester2020formal}, and by assuming translationally invariant physical systems, which can be limiting for other applications in science.} 
Our method achieves precise entropy estimates in this challenging setting, reinforcing \infosedd\ as a promising and reliable estimator for complex discrete data distributions.


