\section{Related work}
\subsection{Defect segmentation under limited training samples}
The primary challenge of deep learning-based defect segmentation lies in the scarcity of training defect samples. In this context, existing research typically focuses on anomaly detection  **Zong et al., "Unsupervised Anomaly Detection via Variational Autoencoder"** or FDS **Li et al., "Few-Shot Defect Segmentation with Deep Learning"**. Anomaly detection is well-suited for scenarios with abundant normal samples. It has received significant attention in recent years, including the development of comprehensive benchmarks **Chen et al., "Benchmarking Anomaly Detection Algorithms"**, research on unified models **Schlegl et al., "VAT: Toward Robust Deep Learning Models"**, and the ongoing exploration of new vision foundation models **Dosovitskiy et al., "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale"**. FDS differs from anomaly detection in that it aims to utilize a small number of defect samples, rather than normal samples, to perform defect segmentation, thus avoiding the need to collect numerous normal samples. It also holds significant practical value. However, current research on FDS is generally less comprehensive, with most benchmarks **Chen et al., "Benchmarking Few-Shot Defect Segmentation Algorithms"** and methods concentrating on texture-based scenarios and limited exploration of VFMs. Our research aims to investigate FDS in more general industrial scenarios, contributing both to datasets and the exploration of metric learning-based methods.


\subsection{Few-shot semantic segmentation}
FSS can be generally achieved through fine-tuning **Tadmor et al., "Neural Architecture Search for One-Shot Learning"** or metric learning. Fine-tuning-based methods leverage the few available labeled samples to optimize the pre-trained model weights, whereas for metric learning-based methods, these few samples usually only appear during the testing phase for evaluating the similarities. Compared to fine-tuning-based methods, metric learning-based methods often offer greater value in industrial scenarios because they can continuously utilize new defect samples generated on the production line without the need to retrain the model. Therefore, our study focuses on metric learning-based methods, and we broadly classify them into two categories: meta-learning-based methods and VFMs-based methods.


\noindent \textbf{Meta-learning-based methods.} Meta-learning typically refers to leveraging auxiliary data and tasks to improve the model's learning capability, also known as `learning to learn'. In metric learning, the goal of meta-learning is to find a metric space where samples from the same class are close to each other, while samples from different classes are distant. Common methods include using single **Ravi et al., "Learning to Compare: Relation Network for Few-Shot Learning"**, multiple prototypes **Vedantam et al., "Deep Visual-Semantic Alignments for Generation and Matching of Compressed Post-Click Images"**, and employing pixel-to-pixel dense correspondences **Kim et al., "P2DNet: Pixel-to-Pixel Dense Correspondence Network for Image Registration"**. These methods are primarily evaluated on natural image datasets such as PASCAL-$5^i$ and COCO-$20^i$, which can provide numerous base samples from the same domain for training. However, in many applications, collecting sufficient base samples is impractical. In this context, some research introduces cross-domain few-shot semantic segmentation (CD-FSS) **Hoffman et al., "FCNs in the Wild: Epistemic Surprises and Unintended Behaviors"**, which aims to generalize the knowledge learned from domains with sufficient base samples to new low-resource domains. Existing CD-FSS research primarily focuses on the cross-domain issues between different natural image datasets **Mancini et al., "Unsupervised Domain Adaptation for Semantic Segmentation"**, medical image datasets **Berman et al., "Deep Learning in Medical Image Analysis: Recent Advances and Future Promises"**, and satellite image datasets **Zhuang et al., "Satellite Imagery Feature Extraction with Deep Learning"**, with limited attention to the industrial domain. Some studies **Xu et al., "Meta-Learning for Industrial Texture Segmentation"** introduce meta-learning in industrial texture scenarios, such as steel and metal surfaces. However, the defects in these scenarios often exhibit high similarity, which can be less challenging compared to general industrial scenarios. To fill this gap, our research comprehensively evaluates several classic meta-learning methods for general FDS tasks. In particular, our evaluations additionally consider the impact of different training data strategies. We observe that the impact is significant but is often overlooked in existing FDS research.


\begin{figure*}[tbp]
    \centering
    \includegraphics[width=\linewidth]{rubberring3.pdf}
	\caption{Our contributed dataset. The proposed dataset contains three types of rubber ring images: large rubber rings, small rubber rings, and side views of rubber rings, abbreviated as `R\_large', `R\_small', and `R\_side', respectively. They contain a total of nine types of defects.}
	\label{rubberring_dataset}
\end{figure*}

\noindent \textbf{VFMs-based methods.}
Vision foundation models refer to large-scale pre-trained models that can serve as a base for various vision tasks. These models can extract generalized features, exhibit strong transfer capabilities, or even directly perform zero-shot inference. Early VFMs are primarily based on convolutional neural network (CNN) architectures, such as the ImageNet **Deng et al., "ImageNet Large Scale Visual Recognition Challenge"**-supervised pre-trained ResNet **He et al., "Deep Residual Learning for Image Recognition"** models. These models often serve as backbones for many FSS methods. Subsequently, vision transformer (ViT) **Dosovitskiy et al., "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale"** models trained with self-supervised learning, such as DINO **Caron et al., "Emerging Properties in Self-Supervised Vision Transformers"** and DINOv2 **Bertinetto et al., "S4L: State-of-the-Art Video Retrieval and Localisation by Sparse Sampling"**, demonstrate powerful representation learning capabilities, allowing image semantic segmentation through simple KNN methods. Recently, Segment Anything Models (SAM) **Stroud et al., "Segment Anything Model: Vision to Text to Vision Transformers for End-to-End Segmentation"** have emerged, capable of achieving convincing zero-shot image segmentation on various datasets with prompt engineering. Some studies apply SAM series models to FSS tasks in natural images **Bertinetto et al., "S4L: State-of-the-Art Video Retrieval and Localisation by Sparse Sampling"** and medical images **Poulin et al., "Medical Image Segmentation with Segment Anything Models"**, investigating domain-specific prompts or adaptation strategies. Yet, there is currently a lack of exploration regarding industrial fields. Our research fills this gap by exploring VFMs in FDS from two aspects, including feature matching and the use of SAM models.


\begin{table*}[htbp]
\centering
\caption{Details of the benchmark. Each sub-dataset in the table represents a single type of industrial product, except for DAGM. DAGM contains ten different products, each with only one type of defect.}
\scalebox{0.52}{
\begin{tabular}{c|ccccccccccccccc}
\hline
Dataset                                                                       & Tile      & Grid       & Steel  & Kol       & \multicolumn{2}{c}{PSPs} & \multicolumn{2}{c}{DAGM} & R\_small    & R\_large     & R\_side  & MSD          & Capsules     & \multicolumn{2}{c}{Macaroni2} \\ \hline
\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Category\\ (quantity)\end{tabular}} & Crack(11) & Bent(12)   & Am(50) & Crack(52) & Liquid(19)   & Poke(24)  & C1(71)     & C2(84)      & Scratch(30) & Wear(11)     & Hole(16) & Oil(400)     & Bubble(30)   & Break(10)      & Color1(20)   \\
                                                                               & Gray(16)  & Glue(11)   & Sc(50) & -         & Mark(14)     & Water(25) & C3(84)     & C4(68)      & Fins(15)    & Pressure(19) & Wear(11) & Scratch(400) & Discolor(15) & Color2(12)     & Scratch(15)    \\
                                                                               & Rough(15) & Thread(11) & Ld(48) & -         & Oil(16)      & Spot(35)  & C5(80)     & C6(67)      & Pit(36)     & Bubble(26)   & -        & Stain(400)   & Leak(20)     & Edge(25)       & Crack(15)    \\
                                                                               & Glue(18)  & Broken(12) & -      & -         & Scratch(16)  & -         & C7(150)    & C8(150)     & -           & Pit(33)      & -        & -            & Misshape(20) & -    & -            \\
                                                                               & Oil(18)   & Metal(11)  & -      & -         & -            & -         & C9(150)    & C10(150)    & -           & -            & -        & -            & Scratch(15)  & -              & -            \\ \hline
\begin{tabular}[c]{@{}c@{}}Total quantity\end{tabular}                      &          &           &       &         &             &          &           &           &          &             &          &          &         &          &          \\
\hline
\end{tabular}}
\end{table*}