\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98\textwidth,trim={0pt 0pt 0pt 0pt}]{fig/design/overview.pdf}
    \caption{An overview figure for Mordal. Gray circles and blocks represent pretrained models and VLM candidates, respectively. White blocks represent inactive or eliminated candidates. Mordal first groups similar candidates into clusters, where each candidate consists of one VE and one LLM. During efficient evaluation, every cluster picks one candidate (i.e., inter-cluster) and Mordal evaluates them. Poor-performed clusters are eliminated. For each candidate in the remaining clusters (i.e., intra-cluster), Mordal fits a linear regression model and predicts its performance to select the best candidate.   
    % \insu{Clusters, where VLM candidates with similar performance are grouped as a cluster.}  
    % \insu{Briefly introduce the flow so that people can understand by just reading the caption.
    % \insu{Best candidate should not be just a box, but a box among all candidates while all the other candidates are eliminated as you did during evaluation.} 
    % \mosharaf{+1; this figure needs to be clearer; e.g., all models can't be circles.}
    }
    \label{fig:overview}
    % Describe each component in this section \textcolor{red}{highlight the part to reduce number of combinations and data sampling. Expand exploration engine to. Redraw in powerpoint. Two-layer}
    \vspace{-2mm}
\end{figure*}



\section{Mordal Design}

This section details the core components of \name's design.
The exhaustive search is expensive because it (1) needs to evaluate every candidate (large search space), and (2) needs to train each candidate with a full dataset to see its performance (high evaluation cost).
\name reduces search space by clustering the candidates based on their similarity and by introducing a two-step inter- and intra-cluster evaluation (\S\ref{sec:clustering}),
and reduces evaluation cost of each candidate with an early stopping mechanism and observational scaling laws (\S\ref{sec:prediction}).

% \textcolor{red}{change the flow by mentioning the differences between llm selection and pretrained model selection.}
% \insu{Match the number of subsections with the number of big boxes in the figure.}
% \insu{You also need to outline subsections here, not just about dimensions. Similar to your prelim presentation, it only introduces technical designs and does not introduce any expected effects. How clustering, exploration, and scaled prediction contribute to reducing GPU hours is necessary here.}

\subsection{Candidate Clustering}
\label{sec:clustering}

With the rapid increase in the number of pretrained models in popular model zoos (e.g., HuggingFace~\cite{huggingface_models_website}), evaluating every candidate combination is expensive.
Based on prior observations that similar models tend to have similar performance~\cite{hu2023hydro, yu2024language, lai2023modelkeeper}, \name clusters candidates and evaluates them in two steps: inter-cluster and intra-cluster, to reduce the search space.

\paragraph{Measuring similarity.}
Measuring the similarity of VLM candidates -- without training projectors between vision encoders and language models -- is challenging.
Parameter similarity~\cite{lai2023modelkeeper}, which has been used to measure similarity between model architectures, does not fully consider the data distribution pattern in the target task. Models with high parameter similarity could still show different performance on different tasks. Therefore, in Mordal, we consider \textit{representation similarity} between VLM candidates, which depends on the target task.

% =================================================
% previous writings below
% =================================================

% With the rapid increase in the number of pretrained models in popular model zoos (e.g., HuggingFace~\cite{huggingface_models_website}
% ), evaluating every candidate combination is impossible. Also, existing model selection methods, such as zero-shot performance, are not applicable without first training the projector. Fortunately, it is known that similar models tend to have similar performance, which benefits hyperparameter search~\cite{hu2023hydro} and training warmup~\cite{lai2023modelkeeper}. Previous methods compute the \textit{parameter similarity} for identical model architectures \cite{yu2024language} and different model architectures \cite{lai2023modelkeeper}. However, the similarity of parameters does not fully consider the data distribution pattern in downstream tasks. Models with high parameter similarity (e.g., Vicuna and LLaMA) could produce different results depending on the task. Therefore, we adopt \textit{representation similarity} since it closely captures the vision and text representations generated by vision encoders in VLM.

% Simply applying cosine similarity is impossible to measure representation similarity because the hidden states and output sequence length for each pretrained model are different. For example, the hidden state of CLIP-ViT-L/14 is 769, while the hidden state of SigLIP-so400m-patch14 is 1152. To resolve this problem, we adopt central kernel alignment~\cite{kornblith2019similarity}, which can effectively estimate the similarity of representations with different shapes.
% \insu{The entire two paragraphs above are more suitable in background. The main idea of \name's clustering comes too late.}


% \mosharaf{Why can representation similarity work when architecture similarity doesn't?}
Mordal employs centered kernel alignment (CKA)~\cite{kornblith2019similarity} to evaluate the similarity of representations between two VLM model structures. CKA has been proven to be an effective tool for understanding and comparing the information encoded across different layers of neural networks. Formally, CKA operates on two datasets by analyzing their corresponding activation matrices. The CKA score is defined as:
\begin{equation}
\text{CKA}(K, L) = \frac{\text{HSIC}(K, L)}{\sqrt{\text{HSIC}(K, K) \cdot \text{HSIC}(L, L)}}
\end{equation}
where K and L are the kernel matrices of activations of two models. $\text{HSIC}$ is the Hilbert-Schmidt Independence Criterion (HSIC) defined as:
\begin{equation}
    \text{HSIC}(K, L) = \text{Tr}(KHLH)
\end{equation}
where Tr$()$ is the trace of a matrix and H is the centering matrix $H = I - \frac{1}{n} \mathbf{1}\mathbf{1}^\top$. CKA is particularly useful in this context for two reasons. First, CKA can compare representations with differing shapes generated by different pretrained models, a task where traditional metrics such as cosine similarity fail. Second, as vision representations are commonly projected through MLP layers, this transformation does not compromise CKA's properties, making it robust and well-suited for such evaluations.



\begin{figure}[!t]
\centering
\subfloat[Clustering based on $C_{ve,1}$]{
\centering
\includegraphics[width=0.23\textwidth]{fig/design/cluster_c1.pdf}
}
\subfloat[Clustering based on $C_{ve,2}$]{
\centering
\includegraphics[width=0.23\textwidth]{fig/design/cluster_c2.pdf}
}

% \subfloat[Benchmark Performance]{
% \includegraphics[width=0.48\textwidth]{fig/design/cluster.pdf}
% }
\caption{An example showing language model clustering process with four VEs and two LLMs. Different VE clusters will lead to different LLM clusters.  
% \insu{Honestly I don't get the point of this figure. Introduce what would be an expected benefits.}\mosharaf{Me neither :(}
}

\label{fig:cluster}
\end{figure}

\textbf{Two-step clustering.} Computing the CKA score between each pair of candidates can be expensive, since $K$ and $L$ are activations from batched data input. To reduce the amount of pair-wise CKA computation, Mordal introduces a two-step VLM clustering strategy: (1) clustering vision encoders, (2) clustering language models based on a fixed vision representation. 
% This eliminates the need to compute CKA for VLM candidates with similar vision encoders.\mosharaf{I'm confused. Why aren't we clustering LLMs as well?}
% fixed in later text
We detail the clustering process as follows:
\begin{itemize}
    \item \textit{Vision encoder clustering.} Mordal computes the representation similarity between vision encoders using CKA. A distance matrix $Dist_{ve}$ is then constructed based on the dissimilarity values. The clustering function will take an input threshold $t_{ve}$ and output the vision encoder clusters $\mathcal{C}_{ve}$.
    \item \textit{Language model clustering.} As shown in \Cref{fig:cluster}, Mordal constructs different language model clusters based on each vision encoder cluster. Using the medoid vision encoder from each cluster, Mordal generates a fixed image embedding for the dataset and a warmed-up feature projector transforms the shape of image embeddings to match the LLM input shape. A distance matrix $Dist_{llm}$ will record the dissimilarity and language models are then clustered based on an input threshold $t_{llm}$. 
\end{itemize}
For each vision encoder cluster and the corresponding language model clusters, Mordal generates the candidate clusters by conducting the Cartesian product of the two clusters. The two-step clustering process reduces the computation costs by avoiding computing the similarity between candidates that have dissimilar vision encoders, which we show in~\Cref{sec:eval_ablation} that usually yield distinct performance.  


% The detailed steps are described in \Cref{algo:clustering}:

% % Besides, to reduce the noise introduced by the feature projector, we 

% \begin{enumerate}
%     \item \textbf{Vision Encoder Clustering.} 
%     % and a K-Means clustering algorithm groups them into K clusters. 
%     \item \textbf{LLM Clustering.} Using the medoid vision encoder from each cluster, Mordal generates image embeddings for the dataset and a warmed-up feature projector will transform the shape of image embeddings to match the LLM input shape. The distance matrix $Dist_{llm}$ records the dissimilarity and language models are then clustered with an input threshold $t_{llm}$. 
%     \item \textbf{Candidate Clustering.} Mordal generates the candidate clusters by combining the vision encoder and language model clusters. 
% \end{enumerate}


% To be specific, during exploration, we first pick one representation model candidate per cluster and evaluate them to filter out unpromising clusters (inter-cluster exploration).
% Then we evaluate all candidates in the remaining clusters to pick the best one (intra-cluster exploration).


\begin{figure}[!t]
\centering

% First subfigure
\subfloat[Inter-cluster evaluation: selects top-$K$ clusters. When $K=1$, $C_3$ is selected. 
    % \insu{If K-=1, C3 will be selected.}
    ]{
    \includegraphics[width=0.45\textwidth]{fig/design/inter_raw.pdf}
    \label{fig:inter_cluster_raw}
}

% \vspace{0.5cm} % Optional vertical spacing between subfigures
\vspace{-2mm}

% Second subfigure
\subfloat[Intra-cluster evaluation: identifies the best candidate. Among the remaining two candidates, $C_{3,2}$ is selected. 
% \textcolor{red}{which one is $C_{3,2}$?}
% \insu{Remove loss for C1 and C2. } \insu{C3,2 is the best, thus it will be chosen.}
]{
    \includegraphics[width=0.45\textwidth]{fig/design/intra_raw.pdf}
    \label{fig:intra_cluster_raw}
}


\caption{Inter- and intra-cluster evaluation with candidate clusters.
% \mosharaf{Shapes in the figure should be consistent with previous ones. Like, where did the circles go? I think I didn't understand what happened in clustering.}
% resolve previous comments.
}

% \mosharaf{Confused about what this caption is supposed to mean. The actual text in the paper makes sense, so this figure needs to be updated along with its caption.}
\vspace{-4mm}
\label{fig:two_stage}
\end{figure}

\paragraph{Inter- and intra-cluster evaluation.}
After grouping the candidates into clusters, \name finds the best candidate with two-step evaluation: inter-cluster evaluation and intra-cluster evaluation. The detailed process is shown in \Cref{fig:two_stage}. Given that candidates in the same cluster have similar performance, inter-cluster evaluation first picks medoid from each cluster as the \textit{representative candidate} and compares \textit{performance of each cluster} to eliminate poorly performing clusters. The remaining Top-$K$ clusters will be evaluated in the second step, where $K$ is a user-defined parameter. 

% \insu{Add how you choose a representative model.}
% \insu{Add how you choose which clusters should be eliminated.}

With the inter-cluster evaluation, many fewer candidates remain in consideration.
Intra-cluster evaluation goes back to candidate-granularity evaluation by aggregating candidates from the remaining Top-$K$ clusters.
It trains all of them on the given alignment dataset, and returns the one with the best performance to the user.




\subsection{Efficient Evaluation}
\label{sec:prediction}

Section~\ref{sec:clustering} reduces the search space; however, evaluating each candidate in both stages (i.e., inter- and intra-) still requires training each candidate with a full dataset to assess its performance.
\name introduces an early stopping mechanism that eliminates the clusters more aggressively but with high fidelity for inter-cluster exploration and an efficient scaling prediction algorithm based on an observational scaling law for intra-cluster exploration.

% \insu{It might be better to map early stopping as an inter-cluster evaluation optimization and scaling law as an intra-cluster optimization (this is not true, but may be easier to read).}




\paragraph{Early stopping mechanism.}
% \insu{The paper~\cite{kaplan2020scaling} already mentions early stopping. Can early stopping be merged with scaling law?}
% I think the early stopping in that paper is mentioning limited computation.

Some obviously poor-performing candidates can be eliminated at an early stage of inter-cluster evaluation to allocate resources to more promising candidates. \name applies a simple but efficient Successive Halving Algorithm (SHA)~\cite{jamieson2016non} to roughly filter out the candidates before observing the scaling law.
It ensures that computational resources are focused on the most promising clusters.

In \Cref{fig:inter_cluster_early}, SHA is conducted during inter-cluster evaluation, where all representative candidates are evaluated with the maximum data sample ratio $R$. It consists of multiple rounds, also known as \textit{rung}. In each round, SHA allocates a budget $b$ to each candidate, evaluates all of them, and keeps the top $1/\eta$ candidates. In the next round, SHA increases the budget to $b \times \eta$ per candidate. This repeats until representative candidates are converged or Top-$K$ candidates are determined. In cases where the number of remaining candidates is large, it is also possible to apply SHA again to intra-cluster evaluation. With SHA as the early stopping mechanism, Mordal \textit{eliminates} poor-performing candidates earlier. It optimizes evaluation time for each candidate, which is an orthogonal dimension to search space, and therefore, speeds up the evaluation process.   


\begin{figure}[!t]
\centering

% First subfigure
\subfloat[Inter-cluster evaluation with early stopping mechanism. Two candidates are early stopped and $C_3$ is selected.]{
    \includegraphics[width=0.45\textwidth]{fig/design/inter_early.pdf}
    \label{fig:inter_cluster_early}
}

% \vspace{0.5cm} % Optional vertical spacing between subfigures
\vspace{-2mm}

% Second subfigure
\subfloat[Intra-cluster evaluation with scaling prediction. $C_{3,2}$ is selected based on the predicted performance. ]{
    \includegraphics[width=0.45\textwidth]{fig/design/intra_scaling.pdf}
    \label{fig:intra_cluster_scaling}
}


\caption{Applying early stopping mechanism and scaling prediction to inter- and intra-cluster evaluation.}

% \mosharaf{Confused about what this caption is supposed to mean. The actual text in the paper makes sense, so this figure needs to be updated along with its caption.}

\vspace{-4mm}

\label{fig:two_stage_efficient}
\end{figure}

\paragraph{Observational scaling law.}
% \mosharaf{Shouldn't this have some figure like the slides?}
The scaling law has been explored in previous works~\cite{lin2024selecting, ruan2024observational}, but limited to LLM selection. In standard scaling laws~\cite{kaplan2020scaling}, the "scale" is defined by the compute resources allocated to training LLMs, such as the number of training FLOPs $C$, model parameters $N$, and training samples $D$. Scaling laws are typically formulated as a power-law relationship between LLMs' cross-entropy loss $L$ and their compute scale measures. A common functional form for transformer architecture \cite{hoffmann2022training} is shown as:  
\begin{equation}
    L(N, D) = \frac{a}{N^{\alpha}}+\frac{b}{D^{\beta}} +e
\end{equation} 
where parameters ${\alpha,\beta,a,b,e}$ are fitted by training LLMs across different compute scales, varying $N$ and/or $D$, and measuring their loss. With this formula, a hypothesized power-law relationship between $D$ and loss $L$ is that they are linearly correlated under the log-log scale. In Mordal, our focus differs from previous scaling laws in our goals â€“ existing scaling laws aim to understand the scaling properties of pretraining and finetuning LLMs. In contrast, Mordal is interested in scaling laws of VLM alignment performance for a fixed model parameter size. We find that an observational scaling law exists for VLMs and present the results in \Cref{sec:eval_ablation}. By fixing model parameters $N$ and changing training sample $D$, we may construct log-linear relationship for each candidate and predict its performance. 

With the observational scaling law, Mordal employs a scaling prediction algorithm that automatically detects the log-linear scaling with sampled alignment dataset to conserve resources. As shown in \Cref{algo:scaling}, for each candidate $c$ in remaining candidate list $C$, the algorithm starts from an initial data sample ratio $R$ (e.g., $\frac{1}{8}$). It evaluates a checkpoint trained on randomly sampled data with ratio $R$. The corresponding performance point $(Log(r), Log(Err))$ will be recorded in list $P$. After that, the algorithm reduces data sample ratio and repeats the above process until enough performance points (i.e., $p$) are collected and the log-linear relationship is observed for a given candidate $c$. Since data sample ratio $r$ is decreasing, we may effectively start the evaluation of $r/u$ from existing intermediate checkpoints to save computation costs. 

For each candidate $c$, Mordal observes a distinct log-linear relationship and fits a linear regression model $f_{c}$ based on $P$. It uses the fitted model $f$ to predict the candidate performance with full data alignment (i.e., $f(1)$) and chooses the best candidate. Note that scaling prediction is orthogonal to the two-stage evaluation. Although it cannot be applied to inter-cluster evaluation speculatively since observing the log-linear relationship requires training the candidate with some portion of alignment data, it can be used during intra-cluster evaluation and save evaluation time for promising candidates. 


% \theendnotes
\begin{algorithm2e}[!t]
\small
\caption{Scaling Prediction 
% \textcolor{red}{remove end}
}
\label{algo:scaling}
\SetKwInOut{input}{Input}
\SetKwInOut{output}{Output}
\SetKwProg{Def}{Def}{:}{}

\input{Maximum data sample ratio $R$, scaling ratio $u$, minimum required performance point $p$}
\output{Prediction result list $L$}

\Def{ScalingPrediction($C$)}{
    % Initialize full data prediction list $L=[]$\\
    \For{$c \in C$}{
        Initialize loss-size pair list $P=[]$ \\
        Initialize data sample ratio $r=R$ \\
        \While{True}{
            \textcolor{blue}{\CommentSty{/* Evaluate performance*/}} \\
            $Err=Evaluate(\mathcal{D}_{align}, \mathcal{D}_{task}, c, r)$ \\
            $P.append((log(r), log(Err)))$ \\
            \If{$|P| > p$}{
                Fit a linear regression model $f_{c}$ on $P$ \\
                break if $f_{c}$ fitting loss $> \delta$ 
            }
            \textcolor{blue}{\CommentSty{/* Reduce data samples*/}} \\
            $r = r / u$ \\
        }
        $L.append((c, f_{c}(1)))$
    }
    Return $L$
}
% \vspace{-8mm}
\end{algorithm2e}



% \insu{Describe the algorithm here.}

% Poor-performing candidates can be eliminated earlier online to speed up the search process~\cite{li2020system,hu2023hydro}. This can be achieved with a simple but efficient early-stopping method, Successive Halving Algorithm (SHA)~\cite{jamieson2016non}, where more resources will be allocated to promising candidates. The combination of Inter-Cluster and Intra-Cluster exploration, extending SHA, forms a robust strategy for narrowing down the vast space of VLM combinations. It ensures that computational resources are focused on the most promising clusters and candidates. By systematically reducing the search space and progressively refining resource allocation, Mordal ensures a balance between exploration depth and resource efficiency. 

% \paragraph{Full Data Training.} As the last step, mordal will leverage all resources to train the best-performing VLM candidates on full data samples. By default, Mordal only picks the best VLM candidate based on the scaling prediction results from $L$. However, users are also allowed to pick the top-K VLM candidates and perform full data training.  

