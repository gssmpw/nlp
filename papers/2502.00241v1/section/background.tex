% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.40\textwidth,trim={0pt 0pt 0pt 0pt}]{fig/general/vlm_structure.pdf}
%     \caption{A vision language model (VLM) includes a vision encoder (left) and a language model (top). The example is taken from the VQA dataset \cite{antol2015vqa}. \textcolor{red}{change banana figure.} \insu{I think the entire figure should be gone. 8-pages limit is not generous while your Section 3 is too short.}
%     }
%     \label{fig:vlm_structure}
% \end{figure}

\section{Background and Motivation}
\label{sec:background}

% \mosharaf{Update lead in. Give pointers to subsections.}
We start by outlining the architecture of typical VLMs. Following this, we highlight the challenges in pretrained model selection for VLMs and show that existing VLMs often do not pick ideal pretrained models for downstream tasks. Based on these observations, we present the limitations of potential solutions like grid search, which motivate Mordal's design.
% \textcolor{red}{TODOs: add more introduction text}


% \textcolor{red}{Too long.}
\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table*}[!ht]
\centering
\caption{Evaluation results of capability on tasks of Visual QA, Doc QA, and Knowledge for four VLMs with different pretrained models. CLIP-Vicuna has the same pretrained models and mode structure as LLaVA-1.5-7B~\cite{liu2023improved}. 
The best result for each scenario is in \textbf{bold} text.
There is no silver bullet.}
\label{table:vlm_motivation}
\scriptsize
\begin{tabular}{lcc|cc|cc|cc}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Vision Encoder} & \multirow{2}{*}{Language Model} & 
    \multicolumn{2}{c|}{Visual QA} & \multicolumn{2}{c|}{Doc QA} & \multicolumn{2}{c}{Knowledge}
    \\
    & & & GQA & VizWiz & ChartQA & DocVQA & ScienceQA  & AI2D \\
    \hline
    % InstructBLIP-Vicuna-7B & EVA-CLIP ViT-g/14 & Vicuna-1.1-7B & 49.2 & 34.5 & 12.6 & 13.9 & 60.5 & 33.8\\
    % Qwen-VL-Chat-7B & CLIP ViT-bigG/14 & Qwen-7B & 57.1 & 36.3 & \textbf{60.1} & 66.3 & 67.1 & 45.9\\
    CLIP-Vicuna (LLaVA-1.5) & CLIP-ViT-L/14 & Vicuna-1.5-7B & 61.5 & 41.2 & 18.2 & \textbf{27.6}  & 70.4 & 54.8 \\
    
    % LLaVA-NeXT-Vicuna-7B & CLIP-ViT-L/14 & Vicuna-1.5-7B & \textbf{64.2} & 60.6 & 54.8 & \textbf{74.4} & 70.1 & \textbf{66.6}\\
    % LLaVA-NeXT-Mistral-7B & CLIP-ViT-L/14 & Mistral-v0.2-7B & 54.9 & \textbf{63.7} & 38.8 & 72.2 & 28.5 & 60.8 \\
    % \hline
    % LLava-1.5-7B* & CLIP-ViT-L/14 & Vicuna-1.5-7B & 61.2 & 41.4 & 17.6 & 24.2 & 68.7 & 52.8
    % \\
    % LLaVA-1.6-7B-Mistral* & CLIP-ViT-L/14 & Mistral-v0.2-7B & 62.0 & 45.6 & 15.3 & 21.3 & 68.5 & 53.2
    % \\
    \hline
    SigLIP-Vicuna & SigLIP-so400m-patch14 & Vicuna-1.5-7B & \textbf{66.4} & \textbf{44.8} & \textbf{18.4} & 24.1 & 68.5 & 53.0 \\
    %LLaVA-1.5-7B-Llama-3 
    CLIP-Llama & CLIP-ViT-L/14 & Llama-3-8B & 55.8 & 37.9 & 13.3 & 17.3 & 75.7 & 58.2\\
    % 61.5 & \textbf{54.4} & 18.2 & 27.6  & 70.4 & 54.8 \\
    SigLIP-Llama &  SigLIP-so400m-patch14 & Llama-3-8B & 56.4 & 38.1 & 13.4 & 17.4 & \textbf{78.5} & \textbf{60.1}  \\
    
    % ES-GQA & SigLIP-so400m-patch14 & Vicuna-1.5-7B & \textbf{66.4} & 44.8 & 18.4 & 24.1 & 68.5 & 53.0 \\
    % ES-VizWiz & SigLIP-so400m-patch14 & Mistral-v0.2-7B & 64.2 & \textbf{46.9} & 16.4 & 21.6 & 68.1 & 52.5  \\
    % ES-ChartQA & CLIP-ViT-L/14 & Qwen2-7B & 60.8 & 42.7 & \textbf{20.2} & 22.1 & 71.8 & 52.0 \\
    % ES-DocVQA & SigLIP-so400m-patch14 & Qwen2-7B & 61.8 & 44.2 & 18.2 & \textbf{28.5} & 74.9 & \textbf{64.8} \\
    % ES-ScienceQA &  SigLIP-so400m-patch14 & Llama-3-8B & 56.4 & 38.1 & 13.4 & 17.3 & \textbf{78.5} & 60.1  \\
    % ES-AI2D & SigLIP-so400m-patch14 & Qwen2-7B & 61.8 & 44.2 & 18.2 & \textbf{28.5} & 74.9 & \textbf{64.8} \\
    
    % Mordal-Vicuna-CLIP & CLIP-ViT-L/14 & Vicuna-1.5-7B & 60.2 & 40.7 & 15.0 & 15.2 & 68.1 & 52.0 \\
    % Mordal-Vicuna-SigLIP & SigLIP-so400m-patch14 & Vicuna-1.5-7B & 63.4 & 44.6 & \textbf{18.6} & \textbf{30.6} & 67.6 & 50.8 \\ 
    % Mordal-Mistral-CLIP & CLIP-ViT-L/14 & Mistral-v0.2-7B & 62.5 & \textbf{47.3} & 16.4 & 28.7 & 70.5 & \textbf{55.4} \\ 
    % Mordal-Mistral-SigLIP & SigLIP-so400m-patch14 & Mistral-v0.2-7B & \textbf{64.0} & 45.1 & 15.4 & 26.6 & \textbf{71.9} & 53.2 \\
    
\bottomrule
\end{tabular}
\end{table*}%

\endgroup


\subsection{Vision Language Model}

% \paragraph{Model Architecture.} 
VLMs are compound AI models with three key components:
% Common VLM architectures, like LLaVA \cite{liu2023improved}, include a pretrained visual encoder to encode visual features, a pretrained language model (LM) to comprehend the user instructions and produce responses, and a vision-language cross-modal feature projector to align the vision encoder outputs to the language models: 
\begin{denseitemize}
    \item \emph{Vision Encoder (VE).} The vision encoder is responsible for processing input images and extracting relevant features. 
    Potential encoder options are CLIP \cite{radford2021learning}, SigLIP \cite{zhai2023sigmoid}, InternViT \cite{chen2024internvl} and DINOv2 \cite{oquab2023dinov2}, etc. % Different vision encoders are trained in different manners. CLIP and SigLIP create connections between text and images through a combination of a vision transformer ViT and a text encoder. By leveraging a large corpus of text-image pairs, they fine-tune the ViT using contrastive learning \cite{radford2021learning}, where text and images that match are considered positive examples and all others are negative. 
    \item \emph{Feature Projector (FP).} The vision-language cross-modal feature projector aims to align the encoded image features to the text token embedding space. 
    The feature projector can be achieved directly by a Linear Projector or Multi-Layer Perceptron (MLP), i.e., several linear projectors interleaved with non-linear activation functions \cite{llava}. % There are also some implementations of vision-text interactions like cross-attention \cite{alayrac2022flamingo} and Q-Former \cite{li2023blip}. However, these approaches are not widely adopted due to their complexity and computation overheads. 
    \item \emph{Language Model (LM).} The language model processes mixed embeddings generated from both user instructions in text as well as image inputs. 
    The commonly used LMs in VLMs are decoder-only LLMs, which include Llama \cite{touvron2023llama}, Vicuna \cite{vicuna2023} and Mistral \cite{jiang2023mistral}.
\end{denseitemize}

When serving a request, a VLM first processes an input image with an image processor and passes it to a vision encoder. 
The vision encoder outputs a sequence of raw vision embeddings (or patches). 
The feature projector will then map the raw vision embeddings to aligned vision embeddings, which will append to text prompt embedding and feed into the LM decoder to generate output text.


% \paragraph{Alignment.} 
% Before using a VLM, however, one must go through an \emph{alignment} process to ensure that the individual components of the VLM are well integrated. 
% Developers integrate pretrained LLMs and visual encoders, training the projector from scratch using visual alignment datasets like VQA \cite{antol2015vqa}. 
% During the alignment process, pretrained components may remain frozen or be further fine-tuned during alignment training. 
% % The projector and pretrained modules may or may not be trained in separate stages \cite{karamcheti2024prismatic}. 
% Recently, some proprietary models have employed end-to-end training without using any pretrained models \cite{bai2023qwen}, but it is not common due to the excessive training cost. 

\subsection{Pretrained Model Selection:~No~Silver~Bullet}

VLMs are versatile and powerful because most vision tasks can be formulated as next-token prediction. 
To train a VLM for a specific downstream task, developers usually \emph{cherry-pick} pretrained vision encoders and language models for alignment. 
However, different pretrained models have varying capacities, which affect VLM performance. To investigate the impact of different pretrained models, we conduct grid search on 49 VLM candidates (i.e., seven vision encoders and seven language models) and train each candidate with \textit{the same alignment data}. 
While complete evaluation is described in \Cref{sec:eval}, we present the performance of four representative VLM candidates on six datasets, across three dimensions: Visual QA (GQA~\cite{hudson2019gqa} and VizWiz~\cite{gurari2018vizwiz}), Doc QA (ChartQA~\cite{masry2022chartqa} and DocVQA~\cite{mathew2021docvqa}) and Knowledge (ScienceQA~\cite{lu2022learn} and AI2D~\cite{kembhavi2016diagram}). As shown in \Cref{table:vlm_motivation}, both pretrained vision encoders and language models have a significant impact on VLM performance. 
At the same time, these improvements are use case-specific.
Overall, \textit{there is no silver bullet pretrained model or model combination that reigns supreme for all tasks}. 

% \mosharaf{This jump isn't clean. A better way would've been saying that one may consider the difference is coming from data. So we fix the data and only change architectures. So on...}

% \mosharaf{It should also say that we do a grid search across N combinations. And only present the good ones, not randomly pick different combinations.}

Given these observations, one may naturally ask: \emph{how can we select the best combination of pretrained models for a specific task?} 
In this paper, we address the pretrained model selection problem in the context of VLMs. 
Given an alignment dataset and a target task, we aim to find the combination of a pretrained vision encoder and a language model that achieves the best performance on the target task after alignment.   



\subsection{Exhaustive Search is Prohibitively Expensive}

% \mosharaf{Now we can talk about the cost of exhaustive.}

Solving the pretrained model selection problem by relying on empirical experiences or intuitions, such as choosing the newest, largest or most well-known model, is unstable and unpredictable. For example, in \Cref{table:vlm_motivation}, a VLM candidate with Llama-3-8B does not have a better performance than a VLM candidate with Vicuna-1.5-7B.
Additionally, since the VLM is not aligned, it is impossible to directly evaluate its zero-shot performance \cite{brown2020language}. 
Most existing model selection methods are designed for classification, regression, or LLM-only generation tasks, and do not account for the vision encoder and the alignment process \cite{vu2020exploring,you2021logme,lin2024selecting}. 
This makes the selection problem particularly challenging, especially in resource-constrained environments.

While an exhaustive search strategy is feasible within a limited search space, it becomes impractical when dealing with a large number of pretrained models -- for reference, there are more than 150,000 LLMs on the HuggingFace platform as of Jan 2025.
Each candidate requires training the projector and potentially finetuning pretrained components, making it unrealistic to train all candidates before making a selection. 


% \begin{figure}[!t]
%     \centering
%     % \subfloat[Benchmark Performance]{
%     \includegraphics[width=0.45\textwidth]{fig/motivation/radar_new.pdf}
%     % }
%     \caption{Benchmark performance of five latest \textit{open-sourced} VLMs on six popular multimodal tasks. \textcolor{red}{original values.}
%     }
%     \label{fig:existing-vlm-performance}
% \end{figure}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=0.95\textwidth,trim={0pt 0pt 0pt 0pt}]{fig/general/overview.pdf}
%     \caption{An overview figure for Mordal. Grey blocks represent queued candidates. Blue and orange blocks represent selected candidates and eliminated candidates respectively. \textcolor{red}{too many colors. Too close to each other and have some white space. \insu{Why this is in background file? Move it to design. Also make sure you apply changes in your prelim presentation to here as well.}}
%     }
%     \label{fig:overview}
%     % Describe each component in this section \textcolor{red}{highlight the part to reduce number of combinations and data sampling. Expand exploration engine to. Redraw in powerpoint. Two-layer} 
% \end{figure*}

% As shown in \Cref{fig:training_curve}, 
Even using a pretrained 7B LLM as the backbone, training a single VLM can take over 100 GPU hours with the LLaVA-1.5 dataset. 
Indeed, manually generating suitable results for \Cref{table:vlm_motivation} cost us 1000+ GPU hours (many inferior combinations are not shown for brevity).
Furthermore, researchers are continuously developing new neural architectures and improving datasets to enhance VLM performance. 
This means that whenever a new encoder or LLM is released, developers must manually re-train VLMs with the new components to evaluate performance.

To address the inefficiencies of exhaustive search in pretrained model selection, we must reduce the search cost along two key dimensions: 
(1) reducing the number of candidates and 
(2) minimizing the time required to evaluate each candidate. 
By optimizing these two dimensions, the exhaustive search process can be replaced with a more efficient pipeline that balances time consumption and selection accuracy.

% \mosharaf{We are basically repeating the same thing at the end of 2 and beginning of 3.}
