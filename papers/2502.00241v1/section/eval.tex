\section{Evaluation}
\label{sec:eval}



\begingroup
\renewcommand{\arraystretch}{1.2}
\begin{table*}[!ht]
\centering
\caption{Summary of improvements. Search Time (h), Top-1 Model Quality (\%) and Kendall $\tau$ results for different datasets. Mordal significantly reduces the amount of training needed (i.e., GPU Saving) while successfully finding the Top-1 VLM candidate. Kendall $\tau$ represents the differences of top-performing candidates compared with the grid search baseline and \textit{larger $\tau$ is better}.}
\label{table:eval_perf}
\scriptsize
\begin{tabular}{cc|c||cc|cccc}

\toprule 
\multirow{2}{*}{Task}
& \multirow{2}{*}{Dataset} 
% & \multirow{2}{*}{Size}
% & \multirow{2}{*}{CLIP-Vicuna-1.5}
& CLIP-Vicuna
& \multicolumn{2}{c|}{\textbf{Grid Search}}
% & \multicolumn{3}{c|}{\textbf{ASHA \textcolor{red}{delete column}}}
& \multicolumn{4}{c}{\textbf{Mordal (ours)}}
\\
&   & (LLaVA-1.5) 
% & Time 
& Top-1 Score & Model Name & Time & Speedup & Top-1 Score & $\tau$ 
\\

\hline
 \multirow{2}{*}{Visual QA} 
    & GQA~\cite{hudson2019gqa}       
    % & 12,578    
    & 61.5  
    % & 5439  
    & 66.4   & SigLIP-Vicuna & 483 & 11.2$\times$ & 66.4  & 0.81 \\
\cline{2-9}
    & VizWiz~\cite{gurari2018vizwiz})    
    % & 4,319     
    & 41.2  
    % & 5439  
    & 46.9   & SigLIP-Mistral & 469 & 11.6$\times$ & 46.9 & 0.88 \\
\hline

 \multirow{2}{*}{Doc QA} 
    & ChartQA~\cite{masry2022chartqa}  
    % & 2,500     
    & 18.2  
    % & 5439  
    & 20.2  & CLIP-Qwen & 607 & 8.9$\times$ & 18.6 (DFN5B-Qwen)  & 0.76 \\
\cline{2-9}
    & DocVQA~\cite{mathew2021docvqa}    
    % & 5,349     
    & 27.6  
    % & 5439  
    & 28.5  & SigLIP-Qwen & 593 & 9.2$\times$ & 28.5  & 0.89 \\
\hline

 \multirow{2}{*}{Knowledge} 
    & ScienceQA~\cite{lu2022learn}     
    % & 2,017     
    & 70.4  
    % & 5439  
    & 78.5  & SigLIP-Llama & 472 & 11.5$\times$ & 78.5  & 0.96 \\
\cline{2-9}
    & AI2D~\cite{kembhavi2016diagram}          
    % & 3,088     
    & 54.8  
    % & 5439  
    & 65.2  & SigLIP-Qwen & 496 & 10.9$\times$ & 65.2  & 0.89 \\
\bottomrule
\end{tabular}
\vspace{-8mm}
\end{table*}%
\endgroup

We conducted extensive experiments to thoroughly evaluate Mordal's performance. These experiments assessed its effectiveness in pretrained model selection and performed an ablation study. The key findings are summarized as follows:
\begin{enumerate}
    \item Mordal identifies the optimal combination of vision encoder and LLM for the target task 8.9$\times$-11.6$\times$ faster than the grid search baseline, as shown in \Cref{sec:eval_performance}. For each target task, the best candidate surpasses the LLaVA-1.5-7B equivalent structure in accuracy while using the same alignment dataset. 
    % \insu{Absolute amount of time without how it is improved is meaningless. 500~600 or whatever means nothing here. Compared to the baseline, how Mordal reduces search time? Also link subsections here. (\$\ref{sec:performance_results})}
    \item We further validate the effectiveness of model similarity computation and observational scaling law in \Cref{sec:eval_ablation}. By conducting the ablation studies, we show that each component in Mordal helps reduce the total search time while ensuring that it can still identify the top-performing candidates.  
    % In average, Mordal reduces the total training time by over 90\% compared to the grid search baseline, with no significant loss in the resulting model's accuracy. \insu{This should be merged with the first one to represent Section performance results. List other findings from ablation studies here.}
\end{enumerate}



\subsection{Experimental Setup}
The experiments are conducted on six mainstream datasets across three domains with seven vision encoders and seven LLMs. We deployed Mordal on a set of VMs on a cluster with a total of 16 NVIDIA A40 GPUs. Each GPU has 48 GB GDDR6 memory. More training and hyperparameter settings are described in \Cref{sec:additional_eval}.

\paragraph{Dataset.} In experiments, we use LLaVA-1.5-Instruction datasets \cite{liu2023improved} for alignment. In practice, users may have their own alignment datasets. For target datasets, we pick six representative datasets, which cover three domains: Visual QA, Doc QA and Knowledge. These datasets are commonly used as benchmarks to assess a model's performance~\cite{zhang2024lmms}. 

\paragraph{Model zoo.} We select seven vision encoders and seven language models based on popularity and performance. The selected vision models include both \textit{language-supervised models} (e.g., OpenAI CLIP) and \textit{self-supervised models} (e.g., DinoV2). For LLMs, all models follow the decoder-only structure, and we pick the most used 7B LLMs from \textit{HuggingFace}. The complete list of our model zoo is shown in \Cref{table:model_zoo}.



\paragraph{Baseline and metric.} We consider grid search as the baseline, which fully trains and evaluates every VLM candidate. We measure the total evaluation time for each method to get the top-1 model for the given task. 
% Besides, to fully evaluate each method, we also pay attention to the rank of candidates. While grid search ranks candidates by comparing their performance, Mordal ranks candidates by their elimination order. 
Additionally, to provide a comprehensive assessment, we also pay attention to the rank of candidates. In grid search, candidates are ranked based on their performance, whereas in Mordal, candidates are ranked by their elimination order. Ideally, if $T_i$ represents the rank of candidate $i$ in grid search and $S_i$ represents the rank of candidate $i$ in Mordal, we expect \textit{$S_i$ is better than $S_j$ if $T_i$ is better than $T_j$}. This can be captured by Kendall's $\tau$ coefficient defined as:
\begin{equation}
    \tau = \frac{2}{M(M-1)}\sum_{1 \leq i < j \leq M} sgn(T_i - T_j) sgn(S_i - S_j)
\end{equation}
where $M$ is the total number of candidates and $sgn()$ is the sign function. A perfect ranking match results in $\tau=1$. To further focus on top-performing candidates, we adopt the weighted Kendall's coefficient $\tau_{w}$, which is previously used in \cite{you2021logme,vigna2015weighted}. The details for implementing $\tau_{w}$ can be found in SciPy~\cite{2020SciPy-NMeth}.



\begin{figure}[!t]
\centering
% \vspace{-10px}
\subfloat[Grid Search (GQA)]{
\centering
\includegraphics[width=0.23\textwidth]{fig/eval/Rank_GQA_grid_search.pdf}
}
\subfloat[Mordal (GQA)]{
\centering
\includegraphics[width=0.23\textwidth]{fig/eval/Rank_GQA_mordal.pdf}
}
% \\
\caption{Top-5 candidates from grid search and Mordal on GQA.}
% \vspace{-10px}
\label{fig:rank_GQA}
\end{figure}


\subsection{Performance Results}
\label{sec:eval_performance}
This section examines the total training time required by Mordal to identify the best VLM candidate compared to grid search. Using the LLaVA-1.5-Instruction dataset for alignment, we also evaluate candidate accuracy across six datasets in comparison with the VLM structure (i.e., CLIP-Vicuna) used for LLaVA-1.5-7B. Additionally, we compute Kendall's $\tau$ coefficient for Top-10 ground-truth candidates found by grid search and their order in Mordal. 

% It is worth noting that the top-1 model selected by grid search on VizWiz achieves lower accuracy than LLaVA-1.5. This discrepancy could be attributed to the inclusion of an additional 558K pretraining dataset in LLaVA-1.5~\cite{liu2023improved}, whereas our approach relies solely on the 665K instruction dataset.



\paragraph{Mordal is significantly faster than grid search.} 
\Cref{table:vlm_evaluation} presents a comprehensive comparison between Mordal and grid search across six datasets. In each task, grid search exhaustively evaluates 49 candidates, requiring \textbf{5439 GPU hours}. Since the same alignment dataset is used across all six tasks, the total search time for grid search remains constant. Notably, for most tasks, the best VLM candidates have different pretrained model combinations and all of them surpass the performance of the LLaVA-1.5-7B equivalent structure, highlighting the importance of pretrained model selection. Compared to grid search, Mordal identifies the top-performing VLM candidates significantly faster and successfully selects the best candidate for five out of six tasks. For example, on VizWiz, Mordal completes the search in just 469 training hours, identifying SigLIP-Mistral as the top-1 model with 46.9\% accuracy. The search time for Mordal varies depending on factors such as the number of clusters formed and the convergence speed of candidates across tasks. Nevertheless, it consistently achieves substantial speedups ranging from 8.9$\times$ to 11.6$\times$ compared to grid search.

% \Cref{table:vlm_evaluation} provides a detailed comparison of Mordal and grid search across six datasets. For each task, grid search exhaustively trains 49 candidates, which takes \textit{5439 GPU hours}. The search time for grid search remains constant across all six tasks as the same alignment dataset is used. For all six tasks, the best VLM candidate performs better than the LLaVA-1.5-7B equivalent structure, highlighting the importance of pretrained model selection. Mordal identifies the best VLM candidates much faster than grid search while successfully identifying the best candidate for most tasks. For instance, on VizWiz, Mordal completes the search in just 469 training hours and finds the Top-1 candidates SigLIP-Mistral with 46.9\% accuracy. While the search time varies depending on the number of clusters formed and candidate convergence speed for different tasks, Mordal achieves impressive speedups of \textit{8.9x-11.6x} over grid search.  





\begin{figure}[!t]
\centering
% \vspace{-10px}
\subfloat[Grid Search (AI2D)]{
\centering
\includegraphics[width=0.23\textwidth]{fig/eval/rank_AI2D_grid_search.pdf}
}
\subfloat[Mordal (AI2D)]{
\centering
\includegraphics[width=0.23\textwidth]{fig/eval/rank_AI2D_mordal.pdf}
}
\caption{Top-5 candidates from grid search and Mordal on AI2D. 
}
\vspace{-4mm}
% \vspace{-10px}
\label{fig:rank_AI2D}
\end{figure}

\paragraph{Mordal maintains a good performance in finding top-performing models.} 
Beyond selecting the Top-1 model, we assess Mordal's capability to identify the Top-5 and Top-10 VLM candidates. As shown in \Cref{fig:rank_GQA} and \Cref{fig:rank_AI2D}, Mordal consistently identifies four out of the Top-5 performing candidates when compared to grid search. However, some candidates (e.g., DINOv2-Vicuna) are overlooked due to two reasons: (1) these candidates belong to poorly performing clusters; or (2) they demonstrate suboptimal performance and are early stopped. In both scenarios, resources are reallocated to better-performing models. We further compute Kendall's $\tau$ values based on Top-10 candidates identified via grid search. Mordal achieves high $\tau$ values across all six datasets. For instance, Mordal achieves a $\tau$ value of 0.96 on ScienceQA, indicating that it correctly ranks eight out of ten models. 


% \begin{figure}[!t]
% \centering
% % \vspace{-10px}
% \subfloat[ScienceQA]{
% \centering
% \includegraphics[width=0.23\textwidth]{fig/motivation/scaling_scienceqa_img.pdf}
% }
% \subfloat[VizWiz]{
% \centering
% \includegraphics[width=0.23\textwidth]{fig/motivation/scaling_vizwiz_vqa_val.pdf}
% }
% \caption{Observational scaling law validation with Vicuna-1.5 being the pretrained LLM. The results are on SienceQA and VizWiz. 
% % \mosharaf{Fix font.} 
% % \textcolor{red}{enlarge legend font size.}
% % \textcolor{red}{TODOs: plot real figs}
% }
% \label{fig:scaling_law}
% \end{figure}






\subsection{Ablation Studies}
\label{sec:eval_ablation}
In this section, we first validate the model similarity method and observational scaling law in Mordal. Then we conduct a comprehensive ablation study to evaluate the individual components of Mordal and their contributions to overall performance. Specifically, we analyze the effects of candidate clustering, early stopping and scaling prediction. 

% Validation for clustering and scaling law for three datasets 
% Also show the performance when disabling each of them 

\begin{table}[!t]
\centering
\caption{Evaluation results of VLMs with different vision encoders and the same language model (i.e., Vicuna-1.5-7B). }
\label{table:vlm_evaluation}
\footnotesize
\begin{tabular}{lccc}
    \toprule
    Vision Encoder & ScienceQA & VizWiz \\
    \midrule
    CLIP-ViT-L/14 & 67.6 & 41.2\\
    SigLIP-so400m-patch14 & 67.7 & 44.8\\
    DFN5B-CLIP-ViT-H/14  & 62.3 & 34.4\\
    InternViT-300M & 56.9 & 30.7\\
\bottomrule
\end{tabular}
\label{table:model_similarity}
\vspace{-4mm}
\end{table}%
% \vspace{-10px}

\begin{figure}[!t]
\centering
% \vspace{-10px}
\subfloat[ScienceQA]{
\centering
\includegraphics[width=0.23\textwidth]{fig/motivation/sqa_heatmap.pdf}
}
\subfloat[VizWiz]{
\centering
\includegraphics[width=0.23\textwidth]{fig/motivation/vizwiz_heatmap.pdf}
}
\caption{Similarity scores between four vision encoders on ScienceQA and VizWiz. Higher is better. 
% \mosharaf{Font too small.}
}
\label{fig:model_similarity}
% \vspace{-10px}
\vspace{-4mm}
\end{figure}

\paragraph{Model similarity validation.} 
To validate the effectiveness of CKA, we train four VLM candidates with four vision encoders and the same LLM backend Vicuna-1.5-7B. The trained VLMs are evaluated on two different datasets: ScienceQA and VizWiz. As shown in \Cref{fig:model_similarity}, the image representation (i.e., outputs of projection heads) generated by different vision encoders show different levels of similarity to each other. For example, with input images from ScienceQA, CLIP, SigLIP, and DFN-CLIP generate similar representations. Meanwhile, DFN-CLIP and InternViT generate similar representations in VizWiz. 
From the results in \Cref{table:model_similarity}, the vision encoders with similar representations will have similar performance with the same language model. Although this observation does not allow us to directly predict the target task's performance, it helps reduce the number of VLM candidates by eliminating similar pretrained models.    

% (CLIP~\cite{radford2021learning}, SigLIP~\cite{zhai2023sigmoid}, DFN-CLIP~\cite{fangdata}, InternViT~\cite{chen2024internvl})

\paragraph{Observation scaling law validation.}
To validate the existence of scaling laws in VLM alignment, we train multiple VLM combinations with different vision encoders and the same language model background on a sampled alignment dataset. The trained VLMs are evaluated on two different datasets: GQA and AI2D . As shown in \Cref{fig:ablation_scaling}, we observe log-linear scaling across different VLMs, which supports the design of scaling prediction. However, the log-linear scaling will only appear after a certain number of training samples, which is consistent with the conclusion in previous work~\cite{lin2024selecting,ruan2024observational}.


\paragraph{Effect of candidate clustering.} Candidate clustering plays a vital role in Mordal as it enables inter- and intra-cluster evaluation. As illustrated in \Cref{fig:ablation_time}, inter- and intra-cluster evaluation (i.e., Mordal without efficient evaluation) significantly reduces training time while maintaining a high $\tau$ value. By grouping candidates with similar characteristics into clusters, Mordal evaluates representative candidates from each cluster first and eliminates candidates in poor-performed clusters.   

% Representative candidates from each cluster are evaluated first and candidates in poor-performed cluster will be eliminated. 
% By grouping models with similar characteristics into clusters, clustering ensures that candidates within the same cluster are not discarded too early, thus increasing the chances of selecting high-performing models compared to the naive successive halving approach.
% \textcolor{red}{add explanation.}

\paragraph{Effect of efficient exploration.} Early stopping mechanism prunes candidates during the early stage of training. While it significantly reduces the search time, applying it during the entire evaluation (i.e., Mordal without scaling prediction) will eliminate some promising candidates (e.g., SigLIP-Qwen on AI2D shown in \Cref{fig:ablation_scaling}). It evaluates candidates based on intermediate performance and leads to a low $\tau$ value. Mordal limits the usage of early stopping and introduces scaling prediction instead to predict the performance of promising candidates. As shown in \Cref{fig:ablation_tau}, this leads to a significant improvement in the $\tau$ value while further reducing the total training time. 

\begin{figure}[!t]
\centering
\subfloat[GQA]{
\centering
\includegraphics[width=0.18\textwidth]{fig/eval/scaling_gqa_lite.pdf}
}
\subfloat[AI2D]{
\centering
\includegraphics[width=0.18\textwidth]{fig/eval/scaling_ai2d_lite.pdf}
}
% \begin{minipage}[m]
% \centering
% \vfill
\raisebox{5mm}{
\includegraphics[width=0.08\textwidth]{fig/eval/scaling_legend.pdf}
}
% \vfill
% }
% \end{minipage}

\caption{Observational scaling law validation for five VLMs with different vision encoders and same language model Qwen2-7B. The results are on GQA and AI2D. 
% \textcolor{red}{legend}
}
\label{fig:ablation_scaling}
% \vspace{-10px}
\vspace{-4mm}
\end{figure}


\begin{figure}[!t]
\centering

\subfloat[Training Time]{
\centering
\includegraphics[width=0.16\textwidth]{fig/eval/ablation_time.pdf}
\label{fig:ablation_time}
}
\subfloat[$\tau$ Value]{
\centering
\includegraphics[width=0.16\textwidth]{fig/eval/ablation_tau.pdf}
\label{fig:ablation_tau}
}
\raisebox{3mm}{
\includegraphics[width=0.12\textwidth]{fig/eval/ablation_legend.pdf}
}
\caption{Ablation study results for efficient evaluation (EE), early stopping (ES) and scaling prediction (SP) on GQA and AI2D. 
% \textcolor{red}{Enlarge legend. Wrong label. Change font.}
% \textcolor{red}{TODOs: add more dataset results?}
}
\label{fig:ablation}
\vspace{-4mm}
\end{figure}






