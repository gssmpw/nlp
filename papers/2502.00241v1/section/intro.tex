\section{Introduction}


% paragraph 2: multimodal llm
% Recent studies on visual QA multimodal models are converging on a popular concept known as vision language models (VLMs), which bridge the gap between visual and language understanding. 
Vision Language Models (VLMs) bridge the gap between visual and language understanding, rising as the dominant approach to solving visual information-based tasks. Notably, GPT-4V \cite{openai2023gpt4}, a large-scale multimodal language model, demonstrates impressive vision reasoning capabilities by taking images as input and generating detailed natural language descriptions. Although the technical details behind GPT-4V remain undisclosed, researchers have proposed a number of publicly available VLMs (e.g., MiniGPT-4 \cite{chen2023minigpt}, LLaVA \cite{liu2023improved} and Qwen-VL \cite{bai2023qwen}) that aim to match GPT-4's capabilities. Many of these open-source VLMs share a similar architecture, in which a feature projector converts the image embeddings generated by a vision encoder and feeds it to a large language model (LLM) along with the text embeddings.

% Later studies focus on more efficient vision-text interactions and training methods \cite{gao2023llama}.  

% common VLM architectures augment existing large language models (LLMs) with a frozen visual encoder to align the visual features. The results are promising as popular open-source VLMs, LLaVA \cite{liu2023improved} and MiniGPT-4 \cite{chen2023minigpt}, can reproduce numerous advanced multimodal abilities shown by GPT-4. 


\begin{figure}[!t]
    \centering
    % \subfloat[Benchmark Performance]{
    \includegraphics[width=0.45\textwidth]{fig/motivation/radar_new.pdf}
    % }
    \caption{Benchmark performance of five latest \textit{open-source} VLMs on six multimodal tasks. 
    % \textcolor{red}{original values.} 
    % \insu{I think this is somewhat duplicated with table 1. Consider remove this, or move table 1 here if having some visualization in introduction is important in ML conference.}
    }
    \label{fig:existing-vlm-performance}
    \vspace{-6mm}
\end{figure}





To construct and train a VLM, the common approach starts from selecting an appropriate \textit{pretrained} vision encoder and language model. Thanks to the ever-growing ecosystem like HuggingFace \cite{huggingface_models_website}, developers are able to choose from countless pretrained models for their own VLMs. Unfortunately, despite the great number of available models, it is difficult to determine which pretrained models (i.e., vision encoders and language models) are the most appropriate ones. Given a user-specific downstream task, it is unclear which pretrained models can form the VLM that will meet the user's needs most effectively. Unfortunately, as shown in \Cref{fig:existing-vlm-performance}, no single VLM consistently outperforms the others in accuracy across all dimensions. Their capabilities vary significantly depending on their pretrained components~\cite{liu2023mmbench,xu2023lvlm}.

With significant constraints on available time and computation cost, it is unrealistic to try every pretrained model combination and train corresponding VLM candidates. Training a single VLM with vision-text alignment data could take more than 100 GPU hours \cite{liu2023improved,karamcheti2024prismatic}. It is also unreliable and unpredictable to rely on human ``intuitions'' to select pretrained models for the given downstream task, such as selecting the latest one or the most well-known one \cite{liu2023mmbench}. In addition, existing model selection methods \cite{brown2020language,you2021logme,lin2024selecting}, like evaluating the highest zero-shot performance, will fail in VLM scenario since they were designed for vision-only and LLM-only tasks. With an untrained feature projector, the LLM will not understand the image embeddings and can produce random outputs. This motivates the key question of this work: \textit{How to effectively find the best pretrained models in a VLM given a downstream task?}

To address this question, we formulate the \textit{pretrained model selection problem for VLMs} for the first time and model it as a resource-constrained task to predict the \textit{alignment} performance of a VLM; i.e., the performance on the downstream task after training the feature projector with the vision-text alignment data. We empirically show that existing VLMs fail to dominate all downstream tasks and a naive approach like grid search is infeasible in practice.  

We present \textbf{Mordal}, a novel pretrained model selection framework, which automatically and efficiently explores different pretrained model combinations in VLM.
Mordal builds on our observation that efficiently solving this problem needs jointly considering two optimization directions: (1) minimizing the number of VLM candidates, where each candidate has different pretrained vision encoders and LLMs; and (2) reducing the evaluation time for each candidate. Overall, we make the following contributions in this work: 
\begin{itemize}
    \item We introduce and define the pretrained model selection problem in the context of VLMs for the first time and demonstrate that off-the-shelf VLMs often do not contain the best pretrained components for a given downstream task. 
    \item We propose Mordal, an efficient pretrained model search framework, to find the best VLM for a given downstream task. Mordal clusters VLM candidates by their representation similarities while employing an early stopping mechanism and an observational scaling law to reduce evaluation time.   
    \item We implement Mordal and provide a flexible interface. Our extensive evaluations show that Mordal efficiently finds near-optimal VLMs with $8.9\times$--$11.6\times$ less computation time than grid search for a wide range of tasks.
\end{itemize}
\vspace{-3mm}
% \textcolor{red}{1.5 page}


% Mordal automatically clusters VLM candidates by their representation similarities and pick the near-optimal model with an order-of-magnitude smaller resource consumption than exhaustive grid search.

% This naturally draws parallels to the studies on model similarity \cite{kornblith2019similarity} and scaling law \cite{kaplan2020scaling}. 
% We conduct systematic evaluations on measuring pretrained model similarities and scaling behavior in the context of VLM alignment.