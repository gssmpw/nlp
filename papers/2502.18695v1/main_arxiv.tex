% !TeX program = pdflatex

\documentclass[manuscript, screen, review=false]{acmart}
\settopmatter{printacmref=false} % Remove footnote references
\renewcommand\footnotetextcopyrightpermission[1]{} % Remove ACM footnote
\pagestyle{plain} % No fancy headers

% \documentclass[manuscript,screen,review,anonymous]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\usepackage{hyperref}
\usepackage[american]{babel}
\usepackage[autostyle=true,threshold=0,autopunct=true]{csquotes}
\usepackage{booktabs} 

% \usepackage{graphicx} 
% \usepackage[dvipsnames, table]{xcolor}

\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{wasysym}
\usepackage{subcaption}
\usepackage{svg}
\usepackage{tabularx}
\usepackage{array}
\usepackage{soul}

% \usepackage[utf8]{inputenc}
\usepackage{geometry} % For page layout



\newcommand\TODO[1]{\textcolor{red}{#1}}


\begin{document}

\title{Policy-as-Prompt: Rethinking Content Moderation in the Age of Large Language Models}


\author{Konstantina Palla}
\email{konstantinap@spotify.com}
\affiliation{%
  \institution{Spotify}
  \city{London}
  \country{UK}
}

\author{Jos\'e Luis Redondo Garc\'ia}
\email{joseluisr@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{Spain}
}

\author{Claudia Hauff}
\email{claudiah@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{Netherlands}
  }

\author{Francesco Fabbri}
\email{francescof@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{Spain}
}

\author{Henrik Lindstr\"om}
\email{henok@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{Sweden}
}

\author{Daniel R. Taber}
\email{dtaber@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{US}
}

\author{Andreas Damianou}
\email{andreasd@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{UK}
}

\author{Mounia Lalmas}
\email{mounial@spotify.com}
\affiliation{%
  \institution{Spotify}
  \country{UK}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.

\renewcommand{\shortauthors}{Palla et al.}



%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Content moderation plays a critical role in shaping safe and inclusive online environments, balancing platform standards, user expectations, and regulatory frameworks. Traditionally, this process involves operationalising policies into guidelines, which are then used by downstream human moderators for enforcement, or to further annotate datasets for training machine learning moderation models. However, recent advancements in large language models (LLMs) are transforming this landscape. These models can now interpret policies directly as textual inputs, eliminating the need for extensive data curation. This approach offers unprecedented flexibility, as moderation can be dynamically adjusted through natural language interactions. This paradigm shift raises important questions about how policies are operationalized and the implications for content moderation practices.
 %
In this paper, we formalise the emerging policy-as-prompt framework and identify five key challenges across four domains: \textbf{Technical Implementation} (1. translating policy to prompts, 2. sensitivity to prompt structure and formatting), \textbf{Sociotechnical} (3. the risk of technological determinism in policy formation), \textbf{Organisational} (4. evolving roles between policy and machine learning teams), and \textbf{Governance} (5. model governance and  accountability). Through analysing these challenges across technical, sociotechnical, organisational, and governance dimensions, we discuss potential mitigation approaches. This research provides actionable insights for practitioners and lays the groundwork for future exploration of scalable and adaptive content moderation systems in digital ecosystems.
\end{abstract}




% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Content moderation involves the systematic monitoring and regulation of user-generated content in online platforms. This critical practice fosters safe, inclusive, and respectful digital spaces while aligning with the values of hosting organisations and adhering to regulatory requirements. 
As digital ecosystems grow and content creation surges, the demand for scalable and adaptive moderation systems has become increasingly urgent. Online platforms now face the dual challenge of managing vast, dynamic content streams and meeting evolving societal and regulatory expectations. 
With the internet continuing to transform how we access information, entertainment, and services, content moderation has become essential for effectively managing online platforms, bridging local and global perspectives to address societal needs and foster community trust. 

The architecture of moderation pipelines varies significantly across digital services, reflecting their unique operational needs. For instance, video streaming platforms often prioritise automated detection of copyrighted or harmful content, while community forums like Reddit~\cite{reddit_content_policy} emphasise moderating user interactions and discussions, often requiring human oversight alongside automated tools~\cite{Gorwa2020}. Despite these platform-specific differences, successful content moderation fundamentally relies on two interconnected processes: \emph{operationalisation} and policy \emph{enforcement} \cite{Singhal_2023}. 

\textit{Operationalisation} involves transforming abstract policy objectives into actionable protocols for consistent implementation by human moderators and algorithms. This process includes several critical components: developing comprehensive annotation guidelines, curating high-quality training datasets, setting precise model predictions thresholds, and conducting thorough annotator training. Through operationalisation, abstract policies evolve into specific rules supported by robust workflows and systems. 
%
Content moderation policies are implemented through various documented forms, which we refer to as policy artifacts. These artifacts operate at different levels of granularity within an organization to cover its moderation needs. At the highest level, platform-wide rules establish foundational principles that guide moderation strategies. These principles then inform more granular artifacts, such as product-specific policies and detailed annotation guidelines, which address domain-specific requirements. This hierarchical structure ensures policies remain broadly applicable while allowing contextual adaptability. 

\textit{Enforcement} puts these artifacts into practice, guiding decision-making processes for both human reviewers and algorithmic systems. During enforcement, content is systematically evaluated against context-appropriate policy artifacts, translating the groundwork laid during operationalisation into concrete moderation actions. Operationalisation provides the foundation for consistent, clear enforcement and works in tandem with the enforcement process. 
%
The interplay between operationalisation and enforcement is dynamic and interdependent, with changes or inconsistencies in one often impacting the other. Policy artifacts are regularly updated to address emerging content scenarios, refine enforcement nuances, or establish additional boundaries. For example, the appearance of new content types or edge cases may require adjustments to annotation guidelines, ensuring both moderators and algorithms remain aligned with overarching principles. 



Technology plays a crucial role in advancing both operationalisation and enforcement, enabling these processes to adapt and remain effective in response to evolving challenges. As the exponential growth of content volumes has driven unprecedented demands on moderation systems, significant technological advancements have emerged to meet these needs. Platforms increasingly rely on automated solutions, with machine learning algorithms becoming central to the moderation pipeline~\cite{Prem2024}. 
%
This work focuses on a transformative development in AI-assisted enforcement:
the ability to encode content moderation policies directly as natural language prompts in LLMs, a paradigm we term \textbf{``policy-as-prompt''} and show in Figure \ref{fig:policy_prompt_setup}. By eliminating the need for extensive manual annotation pipelines, this approach offers unparalleled flexibility and scalability for moderation systems,  enabling rapid policy iteration and fine-grained control over enforcement decisions. This emerging approach is gaining significant traction - enforcement strategies continue to vary across platforms, but the adoption of AI-driven methods, particularly those leveraging LLMs with injectable guidelines, is rapidly redefining the content moderation landscape~\citep{inan2023llama, Markov2023}. 

To the best of our knowledge, this work presents the first comprehensive exploration of the policy-as-prompt paradigm, examining its key challenges and implications across technical, sociotechnical, organisational, and regulatory domains. Beyond identifying these challenges, we systematically analyse their interdependencies and implications, providing a foundation for further inquiry into this transformative approach.
%To the best of our knowledge, this work offers the first comprehensive exploration of the emerging policy-as-prompt paradigm, framing key challenges and their implications across technical, sociotechnical, organizational, and regulatory domains. Our contributions go beyond identifying these challenges by systematically structuring of their interdependencies and implications. 
In Section \ref{sec:the_shift} we examine the technological evolution of content moderation, tracing its progression from basic pattern matching to modern LLM-driven approaches. We explore how this progression has fundamentally altered moderation architectures and introduced the emerging policy-as-prompt. In Sections \ref{sec:technical_implementation_challenges} to \ref{sec:governance_challenges} we break down five key challenges across four domains as outlined in Table~\ref{tab:area_challenges}; \textbf{Technical Implementation} (focusing on the challenges of 1. converting policies to prompts and 2. addressing prompt structure and format sensitivity, which we support with empirical findings), \textbf{Sociotechnical} (3. highlighting the impact of technological determinism in policy formation), \textbf{Organisational} (4. examining the evolving roles of policy-ML teams), and \textbf{Governance} (considering the 5. model governance and accountability). We provide a list of recommendations towards mitigating the challenges in Section \ref{sec:mitigations}. 
We discuss the limitations of this work along with  concluding remarks in Section \ref{sec:conclusion}.

% \input{commands}
\input{tables/areas_challenges}



\section{The Evolution of Content Moderation Technology}
%\section{Transforming Content Moderation: Technological and Architectural Shifts}
\label{sec:the_shift}
The evolution of content moderation, particularly within the text domain, has been closely tied to the exponential growth of user-generated content, driven by the rapid advancement of algorithmic capabilities. Early moderation efforts relied on basic automation, functioning like a digital ``find and replace’’ tool. Methods such as keyword filters and hash-matching were used to identify and remove prohibited content~\cite{engstrom2017limits}. While these first-generation tools were effective at detecting exact matches,  they were easily circumvented by minor alterations, underscoring their limitations in adapting to the dynamic nature of online expression~\cite{Reda2017}.

The rapid increase in user-generated content demanded more robust and scalable solutions, driving a shift towards machine learning-based approaches~\cite{Prem2024}. Early word embedding models like Word2Vec~\cite{Mikolov2013} and GloVe~\cite{pennington2014glove} enabled moderation approaches to capture semantic relationships between words, enhancing their ability to detect variations in harmful phrases and toxic language. This marked a pivotal step in moving beyond simple pattern matching to understanding the meaning behind online content. 

However, the true breakthrough came with the introduction of contextual embedding models~\cite{peters-etal-2018-deep} and the transformative Transformer architecture~\cite{Vaswani2017}. Models like BERT~\cite{devlin-etal-2019-bert} enhanced content moderation  by analysing content in context, enabling systems to improve detection of nuances such as sarcasm, coded language, and implicit bias that were previously undetectable. This advancement significantly improved the ability to  understand the intent and impact of online communication, specifically in the realm of textual content~\cite{belloni2023multilingual}. 

Today, advanced language models like GPT series~\cite{openai2023gpt4}, LLaMA series~\cite{touvron2023llama2} and Claude series~\cite{anthropic2023claude} are redefining the boundaries of content moderation. These models depart from traditional supervised learning--where systems learn from training examples-- and are able to interpret desired behaviour directly from textual instructions. Their natural language understanding and reasoning capabilities allow them to go beyond simply identifying harmful content—they can assess its context and potential impact. They showcase capabilities such as applying complex policy guidelines, engaging in nuanced dialogue with users, and adapting to evolving language and online trends~\cite{openai2024moderation, anthropic_content_moderation, desai2024genaiusersafetysurvey, kolla2024, kumar2024, mullick-etal-2023-content, aldahoul2024}. 
%
%This evolution marks the emergence of more comprehensive and contextually-aware moderation systems. This transition from specialized classification models to general-purpose reasoning systems represents a fundamental shift in content moderation design and deployment, offering the potential for more nuanced, adaptable and effective oversight.
Rather than relying solely on rigid classification models, content moderation now incorporates general-purpose reasoning systems that provide more flexible and context-sensitive oversight. This approach allows for greater adaptability in addressing emerging challenges, making moderation more effective and responsive to the complexities of online discourse.

By enabling the direct integration of  policy guidelines into prompts, LLMs are reshaping the structure of moderation workflows. In the next section, we explore  the details of this policy-as-prompt setup, examining its impact on workflow design and comparing it with traditional frameworks to provide a clearer context for this paradigm shift. 

\subsection{The Traditional Algorithmic Moderation Pipeline}

\begin{figure}[h]
  \centering
  % First subfigure
  \begin{subfigure}{0.45\linewidth}
    \centering
    % \includegraphics[width=0.4\linewidth]{figures/traditional_setup.png}
    \includegraphics[width=0.8\linewidth]{figures/traditional_setup_updated.png}
    \caption{}
    \label{fig:trad_setup}
  \end{subfigure}
  \hspace{0.01in}
  % Second subfigure
  \begin{subfigure}{0.5\linewidth}
    \centering
     % \includegraphics[width=.85\linewidth]{figures/emerging_setup.png} 
      \includegraphics[width=\linewidth]{figures/policy_prompt_setup_updated.png} 
    \caption{}
    \label{fig:policy_prompt_setup}
  \end{subfigure}
  % Overall caption
  \caption{Approaches to content moderation. (a) \textbf{Traditional pipeline}: policy guidelines inform human annotation, which produces training data for models. (b) \textbf{Policy-as-prompt}: policy guidelines are encoded directly as prompts, enabling LLMs to perform moderation without explicit annotation datasets. %While hybrid approaches exist in practice (e.g. an arrow added in (a) from policy to model), this figure highlights the two extremes to facilitate discussion. Arrows represent general dependencies and workflows, intended primarily as a visual aid to support discussion rather than precise mappings.
  }
  \label{fig:comparison_diagrams}
  \Description{}
\end{figure}
%
The adoption of the ML-based approaches required the development of extensive infrastructures for data collection, labelling, model training, and deployment. These requirements fundamentally transformed how platforms approached content moderation at an architectural level.  A standard framework has emerged that integrates human expertise with machine learning models, creating a scalable and widely adopted industry  (see Figure~\ref{fig:comparison_diagrams}a). At the core of this framework are human annotators and machine learning models trained on annotated datasets~\cite{Gillespie2018, Gorwa2020}. Human experts label content based on predefined guidelines, producing datasets that encapsulate these guidelines. 
%
These annotated datasets are then used to train machine learning models, enabling them to assess whether content violates platform policies. During training, the model adjusts its internal parameters--known as weights--to better align with the patterns identified in the annotated data. Once trained, the model can be deployed to evaluate new, real-world content, autonomously flagging potential policy violations. 

This process is inherently iterative, with models continuously refined based on feedback and new data. However, real-world applications bring additional complexities. Annotators may disagree on how specific content should be labelled, leading to inconsistencies or ambiguities within the dataset~\cite{sandri-etal-2023-dont, larimore-etal-2021-reconsidering}. Furthermore, guideline may undergo revisions, but retrospective relabelling of existing data is often impractical, complicating the training process. Despite these challenges, this integration of human expertise and machine learning has become a foundational approach to scalable content moderation.


\subsection{Policy-as-prompt}
While the annotated dataset approach has long been the industry standard in moderation pipelines with an algorithmic component, the emergence of LLMs has reimagined how guidelines can be integrated into the moderation process. LLMs build their knowledge through extensive pretraining on web-based text, leveraging the transformer architecture \cite{Vaswani2017}. This process employs self-supervised learning, where models predict subsequent words in incomplete sentences, enabling them to associate words and phrases with their typical contexts. As a result, LLMs acquire an internal knowledge of language that enables them to generate coherent and contextually relevant responses across diverse scenarios. Interaction with LLMs relies on prompts—input text that guides the model’s response. LLMs encode the prompt into a high-dimensional vector space, preserving semantic relationships between words and phrases. These representations inform the model’s output, drawing on patterns learned during pretraining. 

The quality of an LLM’s response is profoundly influenced by the prompt. A well-crafted prompt ensures that the model’s output aligns with the intended outcome, highlighting the prompt as a pivotal element in effective LLM interaction. Factors such as phrasing, specificity, and context within the prompt play a critical role in shaping the response. As a result, \emph{prompt engineering} has emerged as a systematic practice to extract knowledge from LLMs. This involves designing precise, task-specific instructions, that guide models toward accurate, relevant, and coherent outputs without retraining or altering the model’s parameters (weights)~\cite{anthropic2024prompt}.  Techniques range from simple instruction-based prompts to advanced methods like in-context learning, where illustrative examples are included in the prompt to steer the model towards the desired output~\cite{schulhoff2024, sahoo2024systematicsurveypromptengineering} (see Figure \ref{fig:prompts}). Other sophisticated techniques, such as Chain-of-Thought prompting (CoT), encourage the model to break down complex reasoning into step-by-step deductions~\cite{wei2022}, while Reason and Act (ReAct) prompts guide the model to interact with external tools or information sources to enhance its responses~\cite{yao2023react}.

In content moderation, prompts enable quick adaptation of LLMs to changing policies or criteria, offering a flexible approach that avoids the need for retraining (see Figure \ref{fig:policy_prompt_setup}). By simply re-engineering the prompt with updated policy guidelines, LLMs can swiftly respond to evolving societal norms, emerging risks, or platform-specific needs. In this setting, policy guidelines become an integral part of the prompt, allowing for seamless updates and adjustments to content moderation practices. 
%-----------

%Prompts enable the rapid adaptation of LLMs to new domains and tasks, making them a powerful tool in modern content moderation.
\begin{comment} OLD
Through \textit{prompt engineering}, practitioners can systematically extract knowledge from LLMs. This involves crafting natural language, task-specific instructions that guide the model toward desired outcomes~\cite{anthropic2024prompt}. Unlike earlier supervised models, prompt engineering does not necessarily require model training or modification of parameters (weights). Techniques range from simple instruction-based prompts to advanced methods like in-context learning, where illustrative examples are included in the prompt to steer the model towards the desired output~\cite{schulhoff2024, sahoo2024systematicsurveypromptengineering} (see Figure \ref{fig:prompts}). Other sophisticated techniques, such as Chain-of-Thought prompting (CoT), encourage the model to break down complex reasoning into step-by-step deductions~\cite{wei2022}, while Reason and Act (ReAct) prompts guide the model to interact with external tools or information sources to enhance its responses~\cite{yao2023react}.
 
Instead of  relying on training and annotated datasets that indirectly represent policy, LLMs enable the direct incorporation of policy guidelines into prompts (see Figure~\ref{fig:comparison_diagrams}b). This approach eliminates the need for extensive retraining cycles by embedding policies directly into the prompt, allowing for immediate updates.
%This approach offers significant advantages. LLMs can be easily adapted to new policies or moderation criteria without retraining, enabling platforms to respond quickly to evolving online safety needs.  
Consequently, LLMs can be instantly re-prompted to adapt to evolving societal norms, emerging threats, or platform-specific priorities, facilitating rapid adjustments and highly responsive content moderation practices. Furthermore, prompt engineering provides fine-grained control over model behaviour across diverse moderation scenarios, offering the flexibility to address a broad range of content-related challenges effectively.
\end{comment}

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\linewidth]{figures/policy-comparison-optimized.png}
  \caption{Example prompts demonstrating two approaches for algorithmic content moderation using policy guidelines. \textbf{Left}: A basic prompt where the policy text is provided to the model for direct review of content. \textbf{Right}: An enhanced prompt, following the `in-context learning' technique, that includes both the policy text and specific examples of violative and non-violative content, aiding the model in contextualizing its decisions. }
  \Description{}
  \label{fig:prompts}
\end{figure}

LLMs in content moderation currently operate in hybrid setups where human moderators collaborate with and oversee LLM-driven enforcement, but they have the potential to transform content moderation if given greater autonomy. To ensure this does not lead to harm, we need research to understand the complexities of this future. As organizations adopt this paradigm, they must navigate challenges spanning  technical, sociotechnical, organisational, and governance dimensions as we show in Table \ref{tab:area_challenges}. \textbf{Technical implementation} challenges emerge from the fundamental change in how policy guidelines are operationalized--moving from trained models based on annotated datasets to dynamic prompts that directly guide LLM behaviour. This shift introduces concerns around accurately converting policy to prompts and the sensitivity of LLMs to variations in prompt structure and formatting. 
%
At the \textbf{sociotechnical} level, challenges stem from the limitations of LLMs, which can constrain human judgment and social adaptability. Embedding policies directly into LLM prompts risks reinforcing technological determinism, where policies are rigidly interpreted based on the model’s design, and potentially overlooking the nuanced interplay between technical systems and the human contexts in which they operate. 
On an \textbf{organisational} front, this shift demands changes to workflows and expertise. Traditional content moderation relied on distinct roles between policy formulation and operationalisation, where collaboration was often limited. Policy experts defined guidelines, and machine learning teams operationalised them through annotated datasets. In contrast, policy-as-prompt approaches necessitate hybrid expertise and foster closer collaboration between policy experts and machine learning practitioners.
%
Finally, the model \textbf{governance} domain encompasses challenges related to transparency and accountability. Ambiguities in decision attribution, coupled with difficulties in tracking prompt modifications, hinder auditability and complicate responsible deployment.

%The direct incorporation of policy guidelines as model inputs reshapes not only the technical architecture but impacts other aspects of the content moderation ecosystem. It affects how policies are written and refined, introduces policy structure as a critical parameter for model performance, blurs traditional boundaries between policy authors and ML practitioners, and raises important questions about automated decision-making in regulatory contexts. Moreover, the shift towards machine-readable policy formats potentially influences which moderation practices become feasible or preferred, echoing historical concerns about how technological capabilities shape policy choices \cite{lessig1999code, winner1980artifacts}. These dimensions of change, each carrying significant implications for the future of content moderation, deserve detailed examination both from technical and broader societal perspectives as we explore below.

%The influence of technical capabilities on policy formation extends beyond just content moderation. Lessig's (1999) seminal work "Code is Law" established how technological architecture inherently regulates behavior and shapes what policies are practically enforceable in digital spaces. This concept has been particularly relevant in content moderation, where Gorwa et al. (2020) demonstrate how platforms' reliance on automated systems has led to policies that favor clear, binary distinctions over nuanced contextual evaluation.

%Lessig, L. (1999). Code and other laws of cyberspace. Basic Books.
%Lessig argues that "code is law," meaning that the architecture of technology can regulate behavior just as effectively as traditional legal systems. This is a foundational text for understanding the relationship between technology and governance.

%Winner, L. (1980). Do artifacts have politics? Daedalus, 109(1), 121-136.
%This classic article argues that technologies can embody specific forms of power and authority, shaping social and political arrangements.

%\section{Rethinking Policy and Practice in LLM-powered Content Moderation} \TODO{alternative title: Implications and Challenges of LLM-Driven Content Moderation}

\section{Technical Implementation Challenges}
\label{sec:technical_implementation_challenges}
%
\subsection{Converting Policies to Prompts}
\label{sec:technical_pol_to_prompt}
Traditionally, policy writing has been a human-centric endeavour, where subject matter experts meticulously crafted guidelines designed for clarity and human interpretation. These guidelines were refined through iterative feedback from human reviewers, including annotators, moderators, and domain experts, who brought contextual understanding, nuanced judgment, and collective experience to the process. Ambiguities were addressed through deliberation and testing. For instance, annotators would extensively test policies by creating hypothetical scenarios, exploring edge cases, and providing qualitative feedback, enhancing the robustness of the guidelines. The primary metric of success was the policy's clarity and actionability for human practitioners.

In the policy-as-prompt paradigm, the policy prompt becomes a distinct artifact. Policy writing now serves a dual purpose: it must remain accessible to human understanding, while also be supplemented with representations optimized for machine interpretation. This requires translating a policy's intent and rules into a format that is optimised for machine processing while faithfully preserving its meaning. However, verifying whether a prompt accurately conveys the policy's intent to the model is challenging. Such verification often relies on other signals, such as comparing LLM moderation decisions against human-labelled ground truth or expert judgments, to assess whether the model applies the policy as intended. 

In traditional supervised learning approach, policies were operationalized through labelled training data, embedding intent within the dataset. While this approach had its own challenges--such as human annotators occasionally misunderstanding policies or models generalized imperfectly--it benefited from redundancy in the dataset, which allowed for statistical smoothing through the sheer volume of labelled examples. In contrast, policy-as-prompt encodes intent directly into the prompt, requiring the  prompt to fully capture the nuances of the policy. Unlike human-centric policy development and enforcement, which relies on human interpretation, writing for machines relies on machine interpretation.
%, making it an experimental process guided by trial and error~\cite{bommasani2022opportunitiesrisksfoundationmodels}.
% Unlike human-centric policy writing, which draws on centuries of linguistic refinement to ensure clarity and understanding, writing for machines also involves the relatively new and still developing discipline of prompt engineering. 
The internal workings of LLMs remain largely opaque, leaving prompt engineering an experimental process guided by trial and error~\cite{bommasani2022opportunitiesrisksfoundationmodels}. This challenge is further compounded by the lack of shared contextual understanding between humans and machines; LLMs often misinterpret nuanced or implicit cues that are easily understood by human readers~\cite{blaise2022, amirizaniani2024llmsexhibithumanlikereasoning}. 

Additionally, as discussed in Section \ref{sec:policy_as_parameter}, the process is highly sensitive to formatting and phrasing. Seemingly minor changes--such as using bullet points instead of paragraphs or altering punctuation--can significantly influence model outputs. Lastly, unlike traditional approaches, there is no intermediary dataset to more exhaustively mitigate these sensitivities, making the translation of policies into prompts, a non-trivial and error-prone task.

%

%[RECHECK] -moved later --  Bridging this gap requires machine learning practitioners to work closely with policy experts, translating nuanced, human-centered guidelines into structured, model-interpretable rules \cite{dong-etal-2024-survey}. This alignment between human intent and algorithmic behaviour is crucial but also complex. Practitioners now find themselves optimizing policies – or more precisely, prompts – primarily through algorithmic performance metrics. This shift represents more than a mere technical adjustment; it signals a reconfiguration of how guidelines are conceived, evaluated, and refined.


%\TODO{[to revisit]} Policy authors are no longer optimizing solely for human understanding but must also account for how guidelines are parsed and utilized by machine learning models.  This introduces a new feedback loop: policy formation increasingly incorporates performance insights from the algorithms themselves] 

%\subsection{Policy Formatting as a Performance Parameter}
\subsection{Prompt Structure and Format Sensitivity}
\label{sec:policy_as_parameter} 
In LLM-based content moderation systems, the structure and format of policy guidelines embedded within prompts are critical factors influencing performance.  The presentation and organisation of policy text can directly affect moderation outcomes. Recent empirical studies highlight the significant, yet often overlooked, impact of prompt structure on LLM performance. For instance, \citet{levy-etal-2024-task} found that increasing input length can negatively impact performance, with significant drops occurring well before reaching the models’ maximum input capacity.
Similarly, \cite{liu-etal-2024-lost} showed that performance drops significantly when relevant information is repositioned, highlighting a lack of robustness in long-context processing.
\citet{he2024} demonstrated  that seemingly minor formatting choices--such as using bullet points instead of paragraphs--can substantially influence model performance across tasks. Perhaps most concerning, \citet{sclar2024quantifying} showed that LLMs exhibit unexpected sensitivities to superficial formatting elements like capitalisation and whitespacing, challenging common assumptions about their robustness. 

The precise details of why certain prompt structures work better than others is still an active area of research in AI. Related works have observed that these sensitivities likely emerge from fundamental aspects of LLM architecture and pretraining. The models process text through self-attention mechanisms that compute relationships between all tokens in the input sequence \cite{Vaswani2017}. While theoretically capable of handling arbitrary input structures, the effectiveness of these attention patterns is heavily influenced by how information is organized in the prompt. \citet{zhang2024attentioninstructionamplifyingattention} and \citet{li2024concentrateattentiondomaingeneralizableprompt} demonstrated that different prompt structures lead to distinctly different attention patterns, affecting how information flows through the model's layers. Additionally, \citet{kazemnejad2023the} showed that the positional encoding schemes used in transformer architectures can create inherent biases in how models process information at different positions in the sequence, potentially explaining the observed sensitivity to information positioning.

These findings underscore that the structural choices in presenting information to LLMs are far from superficial. Instead, they represent fundamental parameters that can significantly impact model behavior and reliability. This sensitivity has critical implications for applications like content moderation, where consistent and reliable performance is essential to ensure fairness, accuracy, and trust in automated decisions. Further, this structural sensitivity parallels broader concerns in machine learning regarding how seemingly minor design choices can produce significant downstream effects. Recent research shows that variations in machine learning pipeline components, even when they do not substantially affect overall accuracy metrics, can lead to distinct error patterns or divergent outcomes on specific data subsets. This phenomenon, known as~\textit{predictive multiplicity}, has been observed across various classification tasks and raises particular concerns in high-stakes applications like content moderation~\cite{gomez2024, simson2024}. Similarly, in the policy-as-prompt setting, variations in prompt formatted may result in inconsistent moderation decisions on specific pieces of content, even when overall performance metrics appear unchanged. 
 


%%%% THE OLD -->
% It is crucial to ensure policies prompts are robust to variations in phrasing, formatting. To ameliorate the above concerns, we suggest the use of rigorous testing and evaluation frameworks for prompts, i.e. stress testing with diverse policy prompts. \citet{sclar2024quantifying} suggest reporting a performance spread over a sufficient sample of plausible prompt formats/styles, instead of simply reporting the formatting used and its performance, as is currently standard.
% For instance, Rashomon sets can reveal how different policy phrasings and rule formulations that appear semantically equivalent to humans may lead to substantially different model behaviours despite achieving similar performance metrics \cite{gomez2024}, helping identify which policy variations are most robust and interpretable to LLMs. 
% \TODO{to add...}
% <----

%one approach is to develop frameworks that track and quantify the influence of technical constraints on policy decisions. 


%****** TO BE CONTINUED *** TO INCLUDE Mention THE FOLLOWING
%\TODO{[stopped here - to continue]}The structural elements of policy guidelines—including their formatting, organization, and presentation style—thus represent a form of implementation choice that could introduce unintended variability in moderation decisions. Just as randomization in model training can produce unexpected consequences, variations in how policies are formatted and presented to LLMs may lead to inconsistent interpretation and application of moderation guidelines, even when the underlying rules remain unchanged. These findings underscore the need for careful consideration of policy structure as a critical parameter in LLM-based content moderation systems, particularly given the high-stakes nature of content moderation decisions.

%[....] Predictive multiplicity captures arbitrariness in ML model development, where seemingly innocuous choices made during training, which do not meaningfully change the accuracy of the ML model3 , can nevertheless affect what the algorithm gets wrong. Predictive multiplicity has been recently documented in a range of classification and prediction tasks [37, 66] and can lead to disparatetreatment of individual data points [7, 15, 48, 60]. We demonstrate that predictive multiplicity is rampant in state-of-the-art language models that have been proposed for toxic text classification: multiple models can achieve similar average accuracy yet conflict in classifying individual sentences as toxic. These observations imply that content moderation decision made using ML models lead to outcomes that lack consistency, predictability, and adherence to established principles or logic [16]. To explore the impact of predictive multiplicity in state-of-the-art models for content moderation, below, we detail the research questions that our work sets out to answer along with our main contributions and findings https://dl.acm.org/doi/pdf/10.1145/3630106.3659036


%************** CHECK BEFORE REMOVNIG
%\paragraph{\TODO{NOTES - to consider before removing}}
%Performance metrics – accuracy, precision, recall – now play a pivotal role in policy refinement, potentially at the expense of the nuanced, contextual understanding that human reviewers traditionally provided. This raises fundamental concerns about the potential reductionism inherent in metric-driven policy formulation. Do these optimized policies risk oversimplifying complex ethical guidelines, reducing rich, contextual human judgment to narrow computational interpretations?


% need for extended evaluation (point in alg. arbitrariness):
% Ongoing risk assessment, testing, and monitoring will be key to successful, responsible rollout. Because of the complexity of genAI models, it can be especially challenging to identify why
% they may be behaving in unanticipated ways or having unintended consequences. Because of
% this, risk assessment, testing, and monitoring may need to be carried out in greater depth or with
% greater frequency than might be necessary for a traditional AI model.



%[...] arbitrariness in ML model development, where seemingly innocuous choices made during training, which do not meaningfully change the accuracy of the ML models , can nevertheless affect what the algorithm gets wrong. Predictive multiplicity has been recently documented in a range of classification and prediction tasks [37, 66] and can lead to disparate treatment of individual data points [7, 15, 48, 60]. 

\subsubsection{Demonstration}
\label{sec:prompt_structure_format}
To support our discussion, we empirically investigate how the structure, style and the format of policy prompts influence LLM-based content moderation performance. We also further investigate whether predictive multiplicity emerges in this scenario.
%While prior work has studied prompt engineering's general impact on LLM behaviour \TODO{[CITE]}, our analysis specifically focuses on content moderation tasks. 

\paragraph{Dataset} We collaborated with moderators from a large-scale music platform to create a dataset of 
$2,115$ text descriptions labelled across sixteen different moderation categories. We report in this study six categories, which are widely used in industry for content moderation~\cite{Singhal_2023}: `not violating', `Harm', `Self Harm', `Hate', `Sexual Content' and `jailbreak'. The dataset comprises user-generated requests for playlist creation, offering real-world data for studying moderation. Each entry was labelled as not violating or falling under one specific policy category from the list of considered ones.



\paragraph{Setup}
Our experimental architecture follows the policy-as-prompt framework depicted in Figure~\ref{fig:policy_prompt_setup}.  For the LLM component, we use GPT-4o-mini~\cite{openai2024gpt4o-mini}, for its balance between performance and cost while maintaining characteristics representative of industry-standard models. 
The model's \texttt{{\{\{ prompt \}\}}} included a dictionary with pairs of policy label and policy descriptions for each of the categories considered, %as a JSON list 
alongside the preamble instruction: `\texttt{You are a helpful content moderation assistant. You categorize user input into one of these categories depending on whether the input violates the category}'. We use constrained decoding to prevent the model from deviating from the set of considered categories. While this architecture reflects common industry practices, our specific implementation choices--such as model selection, prompt design, and policy structures--were tailored for research purposes and are not representative of production systems.

First, we examine how subtle variations in prompt structure and style influence model performance. To investigate this, in Figure~\ref{fig:pred_multi} we focus on cases when semantic differences between templates and the original policy text (the \textit{Baseline}) are minimal. The variations include: \textit{Punctuated}, altered punctuation to assess sensitivity to grammatical changes; \textit{Structured}, used bullet points and sections to enhance clarity and facilitate programmatic processing; \textit{Concise}, streamlined phrasing to test the impact of brevity; \textit{Verbose},  detailed and elaborate descriptions to examine the effect of information density; and \textit{Annotator-Friendly}, tailored for human comprehension, emphasising simplicity and clarity.  Detailed examples of these prompt variations are provided in Figure~\ref{fig:prompt-templates} in Appendix Section~\ref{sec:exp}. 
We further explore how differences in policy prompt formatting--such as plain text, XML, YAML and JSON--affect model performance in Figure~\ref{fig:policy_format}.

Figure~\ref{fig:pred_multi_a} reveals  variability in accuracy across different prompt types, despite the fact that all these variations include similar information. Structured prompts demonstrated the highest accuracy, showing model's preference towards clarity and organisation. In contrast, verbose prompts performed the worst, indicating that excessive detail may hinder the model's understanding.
``Punctuation'' and ``Concise'' prompts showed comparable performance, with average accuracies of 
0.7890 and 0.7897, respectively (\textit{p-value} = 0.631). Figure~\ref{fig:pred_multi_b} compares accuracy across policy categories for the two prompt types, revealing evidence of predictive multiplicity; despite similar overall performance, seemingly identical prompts produced different predictions for the same samples.  The results in Figure \ref{fig:policy_format} confirm that variations in policy formats lead to differences in performance..



Next, we explore how increasing the amount of policy text in the prompt impacts model performance. To do this, we incrementally add information in the policy descriptions, such as extra facts and examples. This process resulted in sequential snapshots of policies with varying levels of information, with which we analysed for model performance, as illustrated in Figure~\ref{fig:prompt_temporal_a}. This approach mirrors the evolution of policy guidelines on digital platforms, where trends and new boundaries emerge over time. On average, model performance across all policy categories improves as the policy prompt becomes more detailed (greater semantic coverage). However, when examining the model's performance for individual policy categories at each snapshot (Figure \ref{fig:prompt_temporal_b}), we observe variability, suggesting that different categories are influenced to varying degrees by the increase in prompt information.


%Overall, the results confirm the model's sensitivity to subtle variations in prompt design. %highlighting the importance for careful optimisation to ensure consistency and reliability in moderation tasks.


% \input{figures/prompt_style_fig}


% --- Second set

\begin{figure}[h]
  \centering
  % Subfigure 1
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/pred_multi_a.png}
    \caption{}
    \Description{}
    \label{fig:pred_multi_a}
  \end{subfigure}
  \hfill
  % Subfigure 2
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/pred_multi_b_acc.png}
    \caption{}
    \Description{}
    \label{fig:pred_multi_b}
  \end{subfigure}
  \caption{Effect of prompt design on accuracy: (a) Variation in accuracy across prompt types, averaging over five runs for each type and, (b) Performance differences across policy categories for the `Punctuation' and `Concise' prompt types.}
  \label{fig:pred_multi}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=.6\linewidth, trim=0 0 0 0, clip]{figures/prompt_format_new.png} % Adjust the width as needed
    \caption{Performance spread (accuracy) for modifications in the format in which the policy is plugged into the prompt. `Baseline' refers to the plain text format.}
    \Description{}
    \label{fig:policy_format}
\end{figure}

\begin{figure}[htbp]
  \centering
  % Subfigure 1
  \begin{subfigure}[t]{0.47\linewidth}
    \centering
    \includegraphics[width=0.94\linewidth]{figures/exp2_evolution_performance.png}
    \caption{Overall accuracy scores across each temporal snapshot}
    \Description{}
    \label{fig:prompt_temporal_a}
  \end{subfigure}
  \hspace{0.2in}
  % Subfigure 2
  \begin{subfigure}[t]{0.47\linewidth}
    \centering
    \includegraphics[width=0.94\linewidth]{figures/exp2_evol_categories.png}
    \caption{Per-category accuracy scores across each temporal snapshot}
    \Description{}
    \label{fig:prompt_temporal_b}
  \end{subfigure}
  \caption{Analysing both overall and per-category performance of the model when injecting different temporal snapshots of the policies}
  \label{fig:prompt_temporal_all}
\end{figure}




\section{Sociotechnical Challenge: Technological Determinism in Policy Formation}
\label{sec:sociotechnical_challenges}


Another challenge we identify in the policy-as-prompt approach is that of technological determinism. Technology significantly influences societal values and structures. The concept of technological determinism posits that technology, along with its design and inherent capabilities, can shape social structures, cultural norms, and even political systems~\cite{winner1980artifacts}. Algorithmic systems, particularly those used in content moderation, have long been susceptible to technological determinism due to limitations such as model bias and undertraining \cite{Gorwa2020}. 
%
Traditionally, human oversight in content moderation served as a vital feedback loop. Human annotators played a key role in shaping training data, ensuring that machine learning models were informed by social contexts and values. This interaction mitigated the influence of technology, enabling the development of nuanced rules that accounted for context and intent, incorporated ethical considerations, and provided mechanisms for human appeals to challenge automated decisions~\cite{roberts2019}. 


The advent of LLMs, particularly proprietary and opaque third-party models, is now shifting this balance. By enabling the direct use of policy guidelines in moderation workflows, thus bypassing human annotation, LLMs amplify the risks of technological determinism. As discussed in Section~\ref{sec:technical_implementation_challenges}, LLMs excel at processing structured, algorithmic-friendly guidelines. However, this capability may inadvertently push experts to prioritize machine clarity over the nuance and flexibility required for context-dependent rules. Structured formulations might be favoured even when they do not fully align with underlying policy objectives, creating an environment where complex social issues are reduced to simplistic, binary judgments. This echoes Lessig's ``code as law'' principle, where technical architecture becomes a primary regulator of online behaviour~\cite{lessig1999code}. Similarly, \citet{yeung2019algorithmic} describes ``algorithmic regulation'' as the process by which designing and deploying algorithms transforms into a form of governance, shaping both individual and collective behaviour. Finally, \citet{vandijck2013culture} further argues that platform governance is increasingly dictated by technical affordances rather than democratic deliberation. Consequently, there is a growing risk that content moderation will be driven primarily by what LLMs can efficiently process, rather than by what best serves the diverse needs of online communities.

%With the advent of LLMs, particularly proprietary and opaque third-party models, this balance is shifting. The direct use of policy guidelines in LLM-driven moderation, without human annotation, amplifies the risks of technical determinism. As discussed in Section \ref{sec:technical_implementation_challenges}, LLMs excel at interpreting structured, algorithmic-friendly guidelines. This may push experts to prioritize clarity for machines over nuance and context-dependent rules. Structured formulations may be preferred even when these do not fully align with underlying policy objectives, creating an environment where complex social issues are reduced to simplistic, binary judgments. This echoes Lessig's "code as law" principle, where technical architecture becomes a primary regulator of online behaviour \cite{lessig1999code}. \citet{yeung2019algorithmic} argues that ``algorithmic regulation" transforms the act of designing and deploying algorithms into a form of regulation itself, shaping individual and collective behaviour. In a similar vein, \citet{vandijck2013culture}  analyses how platform governance becomes increasingly shaped by technical affordances rather than democratic deliberation. Just as platforms are shaped by their technological architecture, we risk content moderation becoming primarily driven by what LLMs can effectively process, rather than what serves diverse community needs.

A broader concern is that LLM capabilities could become the dominant force  shaping policy decisions, overshadowing thoughtful considerations of platform values, user needs, and societal impact. The difficulty of encoding complex ethical considerations into LLM-interpretable rules may push platforms towards simpler, more easily enforceable policies, resulting in the homogenisation of content moderation practices. If LLMs struggle with highly nuanced or context-sensitive rules, platforms may opt for standardized policies to ensure consistent enforcement. This trend recalls the Television Code era of the 1950s and 1960s, when standardized broadcast guidelines homogenized content across local stations, forcing diverse communities to conform to mainstream cultural norms rather than addressing their unique need~\cite{barnouw1970image, macdonald1990one}. Similarly, LLM-driven content moderation risks prioritizing technological efficiency over the diverse and context-specific needs of global online communities, potentially undermining efforts to serve the varied requirements of different cultures and contexts.
%Furthermore, technical determinism can be exacerbated by issues already explored extensive in the literature. LLMs can perpetuate and amplify biases embedded in their training data \cite{bender2021, blodgett-etal-2020-language}. These models not only shape policies but also inherit existing societal biases, which can lead to unfair or discriminatory outcomes.
%sets can reveal how different policy phrasings and rule formulations that appear semantically equivalent to humans may lead to substantially different model behaviors despite achieving similar performance metrics

%The broader risk is that LLM capabilities might become the primary driver of policy decisions, rather than thoughtful consideration of platform values, user needs, and societal impact. This could lead to a homogenization of content moderation policies. The difficulty of encoding complex ethical considerations into LLM-interpretable rules may push platforms towards simpler, more easily enforceable policies, leading to convergence. If LLMs struggle with highly nuanced or context-dependent rules, platforms might opt for more standardized policies to ensure consistent enforcement. This dynamic parallels the Television Code era of the 1950s and 1960s, where standardized broadcast guidelines led to content homogenization across local stations, forcing diverse communities to conform to mainstream cultural norms rather than serving their specific needs \cite{barnouw1970image, macdonald1990one}.

%------------
\begin{comment}
\TODO{OLD}
[OLD]The capabilities and limitations of LLMs increasingly influence how content moderation policies are conceived and formulated, creating a form of technical determinism in policy development. This dynamic echoes \citet{winner1980artifacts} work on how technological artifacts embed political choices and social consequences. Just as they demonstrated how built environments could enforce social order, LLM architectures may influence the boundaries of permissible speech online in several ways.
The ability of LLMs to interpret certain types of guidelines more effectively than others may inadvertently shape policy writing. As discussed in Section \ref{sec:technical_implementation_challenges}, experts may find themselves favouring more structured, algorithmic-friendly formulations over nuanced, context-dependent guidelines, even when the latter might better serve the underlying policy objectives. This effect aligns with Lessig's \citep{lessig1999code} framework of ``code as law'', where technical architecture becomes a primary regulator of online behaviour. 

The broader risk is that LLM capabilities might become the primary driver of policy decisions, rather than thoughtful consideration of platform values, user needs, and societal impact. This technical determinism could lead to a homogenization of content moderation policies across platforms, as organizations optimize their guidelines for LLM interpretation rather than their unique community needs.  In a similar discussion, \citet{vandijck2013culture} analyses how platform governance becomes increasingly shaped by technical affordances rather than democratic deliberation.
Just as the author observes platforms being shaped by their technological architecture, we risk content moderation becoming primarily driven by what LLMs can effectively process and enforce, rather than what serves diverse community needs. 

\TODO{Bias} The pretraining of LLMs may also introduce significant biases into policy evaluation \cite{gallegos-etal-2024-bias}. These models, trained on internet-scale datasets, may carry embedded assumptions about content appropriateness that reflect historical biases in online discourse rather than carefully considered policy positions \cite{birhane2021multimodaldatasetsmisogynypornography}. 
As a result, they can  amplify existing social biases, potentially perpetuating discriminatory practices in content moderation \cite{content_moderation_bias}. This phenomenon builds on \citet{Gillespie2018}'s analysis of how platforms' technical choices in content moderation systems inherently shape the parameters of public discourse. 

The concerns above apply  broadly to any LLM-based component at any stage of the content moderation pipeline. However, the direct embedding of moderation guidelines into LLM prompts, combined with their deployment in automated policy enforcement, creates an even greater urgency to carefully consider these implications. This new paradigm, where LLMs both interpret and enforce policies, amplifies the potential impact of technical determinism on content moderation practices.

%The pretraining of LLMs also introduces subtle but significant biases into policy evaluation. These models, trained on internet-scale datasets, may carry embedded assumptions about content appropriateness that reflect historical biases in online discourse rather than carefully considered policy positions.
%For instance, an LLM's understanding of "harassment" or "hate speech" may be influenced by the predominant interpretations present in its training data, potentially perpetuating existing biases in content moderation.

% [ ...from a paaper] This implies that content moderation algorithms, which ultimately control a user’s right to freedom of expression, will inherit any limitations intrinsic to ML models. This implication is a growing concern in the law and policy literature [50, 56, 64], particularly in scenarios described as “algorithmic leviathans” [15, 43], where algorithms excessively control the exercise of freedoms and access to resources. Recent research showing that ML-based content moderation occurs with limited accountability and with policies applied indiscriminately across jurisdictions [30, 71] only adds to these concerns
\end{comment}
\section{Organisational Challenge: Converging Roles, Policy Authors and ML Practitioners}
\label{sec:organisational_practice_challenges}

The integration of promptable LLMs into content moderation workflows has introduced a significant  shift in roles and skill sets for both policy experts and machine learning practitioners. As LLMs are used to operationalise guidelines, policy authors are increasingly required to broaden their expertise beyond traditional policy development to include a working knowledge of machine learning principles, such as prompt engineering and the nuances of algorithmic behaviour, including bias, accuracy, and explainability~\cite{bommasani2022opportunitiesrisksfoundationmodels}. This expanded skill set enables policy experts to anticipate potential issues, assess LLM performance, and contribute to the creation of more robust and fair moderation systems. Conversely, machine learning practitioners are stepping into domains traditionally governed by policy experts. By designing and refining the prompts that guide LLMs, they play a pivotal role in shaping how policies are interpreted and enforced. However, without formal training in policy development or regulatory frameworks, their contributions may inadvertently introduce systemic biases or misinterpretations that diverge from the policy's original intent and potentially overlook critical ethical considerations. 

These emerging workflows are not simply technical translations of policy but complex ``contested spaces'' where different professional epistemologies and interpretive frameworks converge~\cite{green2021}. This intersection highlights the growing need for cross-disciplinary collaboration, where machine learning practitioners develop a deeper understanding of policy nuances, and policy experts acquire foundational machine learning literacy. In this evolving paradigm, content moderation would no longer rely on distinct, sequential contributions from policy experts and machine learning  practitioners. Instead, it would require hybrid expertise and continuous collaboration. 


A notable example of this shift is the adoption of red teaming in LLM evaluation~\cite{perez-etal-2022-red, ganguli2022redteaminglanguagemodels}. Traditional evaluation methods, which rely on bespoke testing procedures, often fall short in capturing the full spectrum of behaviours and risks associated with increasingly autonomous and capable LLMs. Red teaming addresses this challenge by borrowing concepts from security practices, where deliberate attack strategies are employed to stress-test systems and uncover vulnerabilities. This approach requires not only technical expertise to understand and exploit the model's intricacies but also insights from fields such as social science, ethics, and policy to identify risks beyond technical performance, such as misuse or harmful societal impacts.

This shift underscores the need for hybrid knowledge systems that blend technical, ethical, and policy expertise. In the future, content moderation may give rise to roles such as ``AI policy translators''--professionals skilled in bridging the gap between technical and policy teams. These individuals would play a pivotal role in ensuring that automated systems align with policy goals while leveraging the capabilities of LLMs. By fostering cross-disciplinary collaboration, they would contribute to the development of more robust, adaptable, and ethically sound moderation practices.



% Short-term solutions should prioritise structured collaboration frameworks where policy and ML teams work in tandem rather than sequentially, through regular joint working sessions, shared documentation practices, and established feedback loops. 
% %Knowledge-sharing mechanisms like cross-training workshops and shared glossaries can help build common ground, while procedural safeguards such as mandatory policy review checkpoints and mixed-team audits can help maintain policy fidelity during technical implementation. 
% These interim measures acknowledge that while the eventual goal may be unified roles, the immediate need is for better interfaces between existing specialist teams, allowing organizations to maintain effective content moderation while building toward more integrated expertise.
% %The challenge extends beyond simple knowledge transfer - it requires developing new frameworks for collaborative decision-making that bridge the gap between technical capabilities and policy objectives \cite{Liu2023}. This evolution may suggest a future where content moderation becomes an inherently socio-technical practice, requiring organizations to cultivate hybrid expertise and establish new professional roles that span both domains \cite{Tonmoy2024}. Such roles might include "AI policy translators" who can effectively mediate between technical and policy teams, ensuring that automated moderation systems remain aligned with intended policy outcomes while leveraging the full potential of LLM capabilities.

% ]

%further emphasizes the inherently socio-technical nature of content moderation

%[OLD]The integration of promptable large language models into content moderation workflows highlights also a critical shift in roles and skill sets for both policy experts and machine learning practitioners. As LLMs operationalize guidelines, policy authors are increasingly called to expand their expertise beyond traditional policy crafting to include a working understanding of ML principles, such as prompt engineering and the nuances of algorithmic behavior \cite{bommasani2022opportunitiesrisksfoundationmodels}. These emerging workflows are not merebly technical translations of policy, but complex "contested spaces" where different professional epistemologies and interpretive frameworks collide \cite{green2021}. Conversely, ML practitioners are stepping into domains traditionally governed by policy experts. By designing and refining the prompts that guide LLMs, they effectively shape how policies are interpreted and enforced. However, without formal training in policy or regulatory frameworks, their interventions may introduce systemic biases or misinterpretations that deviate from the policy's original normative intent. This intersection underscores a growing need for cross-disciplinary collaboration, where ML practitioners gain a deeper appreciation of policy nuances and policy experts acquire foundational ML literacy. 



\section{Governance Challenges}
\label{sec:governance_challenges}
% \subsection{Regulatory Implications of Enhanced Automation}

The rapid adoption of LLMs in automated decision-making systems has intensified discussions around model governance, focusing on issues such as transparency, accountability, and fairness in the deployment of AI-driven systems \cite{gebru2021datasheets, oecd2019, europeancommission2020}.
Rather than revisiting existing discussions on governance for automated decision-making systems, we specifically examine the implications unique to the policy-as-prompt configuration.


% \subsection{Accountability in Automated Decisions}

Decision attribution poses an significant accountability challenge in the policy-as-prompt setup. The lack of clear system boundaries create confusion when unwanted outcomes occur, making it difficult to  pinpoint their exact cause--whether they stem from the policy language in the prompt, the LLM's interpretation of that language, or the complex interaction between the two. This ambiguity directly undermines auditability – reconstructing the decision-making process and documenting the influence of various components becomes incredibly difficult. The involvement of multiple stakeholders, including policy writers, prompt engineers, and model providers, further complicates attribution by obscuring how their contributions influence these outcomes~\cite{diakopoulos2016}. 

A further challenge is determining which prompt adjustments warrant formal documentation. As discussed in Section \ref{sec:technical_implementation_challenges}, changes to enforcement can be made by modifying prompts, often with the semantic meaning of the underlying policies remaining unchanged. However, the sensitivity to prompt structure can lead to significant shifts in model behaviour, making it difficult to track how enforcement evolves over time. If every minor change requires extensive documentation, it adds unnecessary overhead. Yet, if changes are not sufficiently documented, it becomes increasingly difficult to monitor and understand the impact of these modifications on model performance. %Striking the right balance between over- and under-documentation is crucial to maintaining the transparency and accountability of evolving moderation systems.

%These challenges underscore the need for systems to track changes, analyze interactions, and establish standards for accountability and transparency in policy-as-prompt configurations.


 \begin{comment} OLD - not allowe dby Comms
 In this setup, the boundary between policy artifacts and enforcement mechanisms becomes increasingly blurred. Traditional content moderation systems maintain a clear separation between policy artifacts (designed for human reference)
and their technical implementation (through training data and model architecture). However, in the policy-as-prompt paradigm, policy guidelines act as
direct inputs for LLM enforcement, eroding this distinction. 
Policies essentially become executable code through LLM prompts, possibly complicating regulatory requirements for transparent documentation of enforcement mechanisms. This is particularly relevant in frameworks like the EU's Digital Services Act~\cite{eu_dsa}, which presumes a clear delineation between policy creation and implementation.

The ability to modify enforcement behaviour through prompt engineering introduces further 
regulatory challenges. In this paradigm, changes to enforcement can be made by adjusting the prompts guiding LLMs, without requiring formal updates to official policy documents themselves, as long as the semantic meaning remains unchanged. This raises critical questions about how such modifications should be documented, particularly in relation to regulations requiring disclosure of ``significant changes'' to content moderation systems~\cite{SantaClaraPrinciples2021, Mulligan2019ProcurementAP}. For instance, when platforms refine how policies are framed within prompts without altering the underlying policies, it becomes unclear whether these refinements qualify as reportable changes under regulatory requirements. 

The policy-as-prompt approach also complicates the distinction between substantive and technical changes. In this paradigm, the performance impact of a policy modification is often more closely tied to the model’s interpretation of the prompt rather than semantic changes in the policy content itself. This technical sensitivity introduces additional complexities for regulators, as even minor textual adjustments can result in significant shifts in enforcement patterns (as discussed in Section \ref{sec:policy_as_parameter}). These challenges highlight the need for updated regulatory frameworks that account for the dynamic and highly nuanced nature of LLM-driven content moderation systems.
OLD
\end{comment}



% \TODO{confusing...no...maybe take some bits for the SOLS part}
% The convergence of policy authorship and technical implementation (discussed in Section \ref{sec:policy_as_}) may also complicate regulation. As policy writing becomes increasingly intertwined with prompt engineering, regulatory frameworks designed around distinct policy and technical roles may need revision. This could include new requirements for documentation of how policy guidelines are formatted for LLM consumption, testing protocols to verify consistent interpretation of policy prompts, standards for evaluating the fidelity between written policies and their automated enforcement and requirements for maintaining human oversight over prompt engineering decisions.




%These challenges suggest the need for regulatory frameworks specifically designed for systems where policy guidelines serve as direct enforcement inputs. Such frameworks, as suggested by Mittelstadt et al. (2016) in "The Ethics of Algorithms," would need to balance the benefits of flexible, rapid policy implementation with appropriate oversight and accountability mechanisms.




%-------------------------
%+++++Beyond regulatory mandates, organizations face heightened accountability demands in this new paradigm. Prompt-driven systems require mechanisms that provide transparency into how models interpret and apply policies in real-time, ensuring decisions remain explainable and auditable. This setup also increases the complexity of offering users clear pathways for redress or dispute resolution, as the reasoning behind moderation decisions may depend on prompt-specific nuances rather than fixed guidelines. Addressing these challenges is crucial to maintaining trust and compliance in the rapidly evolving landscape of automated moderation.
%%% -------------

% The shift toward LLM-driven content moderation raises significant regulatory considerations, particularly as automated decision-making becomes more prevalent in content governance. The deployment of LLMs for direct policy enforcement introduces new challenges for compliance with existing and emerging regulatory frameworks.

% A key consideration is the intersection with automated decision-making regulations, such as the EU's AI Act and the Digital Services Act (DSA). These frameworks impose specific requirements around transparency, accountability, and human oversight of automated systems. The DSA, in particular, requires platforms to maintain human oversight of content moderation decisions and provide clear explanations for enforcement actions. This creates an interesting tension with LLM-based systems, where decision-making processes may be less transparent due to the complex nature of large language models.

% The rise of LLM-driven moderation also intersects with data protection regulations like GDPR, particularly around automated decision-making that produces legal or similarly significant effects. Platforms must carefully consider how to implement LLM-based moderation while ensuring compliance with requirements for explainability, right to human review, and right to contest decisions. This becomes particularly complex when LLMs are making nuanced interpretations of policy guidelines rather than applying clear-cut rules.

% Moreover, emerging regulatory frameworks are beginning to specifically address AI-driven content moderation. For instance, the EU's AI Act classifies content moderation systems as high-risk applications, subjecting them to enhanced requirements around documentation, testing, and human oversight. This classification acknowledges the significant impact these systems can have on fundamental rights like freedom of expression.


% \section{Mitigating Challenges in Policy-as-Prompt} 
\section{Recommendations} \label{sec:mitigations}
In this section, we build on the systematic breakdown of challenges presented earlier (see Table \ref{tab:area_challenges}) to propose strategic directions for addressing them. These strategies are informed by the insights gained from analysing the technical, sociotechnical, organisational and governance-related aspects of the challenges. Rather than offering a definitive set of solutions, this section emphasizes high-level, actionable pathways that can guide future research and practical efforts. By framing these strategies within the context of our earlier analysis, we aim to provide a foundational perspective for tackling the complexities associated with this technology.


\subsection{Enhanced Evaluation }
\label{sec:enhanced_eval}
A key mitigation strategy across several challenges is to do rigorous evaluation of the policy-as-prompt implementation. This evaluation must extend beyond traditional accuracy metrics to assess performance across critical dimensions. %Specifically, it should evaluate prompt robustness against variations in structure and language while ensuring  sociotechnical readiness, such that prompts effectively guides the model toward fair decisions that authentically reflect policy intentions.

To address \textit{technical} sensitivity to prompt structure and formatting, the evaluation process should include comprehensive sensitivity analysis during the prompt development phase. This would involve studying the impact of formatting, phrasing, and structural changes on model outputs. Stress testing with diverse policy prompts ensures that small adjustments, such as punctuation or formatting changes, do not unintentionally lead to misinterpretation by the model. \citet{sclar2024quantifying} recommend reporting performance accross a range of plausible prompt formats and styles rather that focusing solely on the performance of a single prompt format. Sensitivity analysis should also address subtle inconsistencies, such as those revealed by predictive multiplicity. For instance, Rashomon sets can identify cases where semantically similar policy phrasing elicit divergent model responses,  such as inconsistent handling of edge cases, despite achieving comparable accuracy metrics~\cite{gomez2024, marx20}. By analysing these variations, developers can identify policy formulations that are both effective and robust. 

At the \textit{sociotechnical} level, evaluation should move beyond conventional accuracy metrics to include measures of societal readiness and adaptability. Metrics like demographic fairness~\cite{mitchell2021, verma2018} can help identifying whether specific prompt formulations create disparities across demographic groups, ensuring equitable policy application. Additionally, case libraries can serve as a valuable tool for addressing potential simplifying tendencies of LLMs. These libraries, containing nuanced, real-world examples of moderation edge cases--such as region-specific cultural references or satire--can test how well a policy-as-prompt system manages societal complexity. Specifically for the prompted guidelines, case libraries become crucial in revealing where binary or reductive prompt designs break down when confronted with complex real-world scenarios. Insights from this edge case analysis can guide iterative refinements of prompts, enabling policy makers and practitioners to design policy-as-prompt configurations that balance technical clarity with societal needs.

% ------------
\subsection{Collaborative Prompt Engineering}
In Section \ref{sec:technical_pol_to_prompt} we discussed the complexity of converting policy guidelines to prompt, requiring careful attention to the ambiguities that can arise due to the inherent differences between human and machine interpretability of the policy prompt. To minimize machine misinterpretations, strategies must focus on crafting prompts that address multi-faceted aspects of content and encourage diverse perspectives. Already techniques like chain-of-thought reasoning \cite{wei2022} have shown promise in crafting prompts that encourage step-by-step reasoning, allowing models to consider multiple facets of a problem before arriving at a conclusion. 

Techniques like meta-prompting can dynamically integrate contextual ethical reasoning \cite{suzgun2024metapromptingenhancinglanguagemodels}, and multi-persona prompting \cite{wang-etal-2024-unleashing} can embed varied policy perspectives, enabling models to better navigate nuanced contexts.
Additionally, fostering collaborative environments where LLMs—trained on diverse data and architectures—contribute to policy interpretation helps mitigate biases and refine prompt designs. These LLMs can collaboratively suggest rewrites and identify gaps creating a feedback loop for continuous improvement \cite{liang2024cmatmultiagentcollaborationtuning}. %Together, these strategies build adaptable frameworks that align machine understanding more closely with human reasoning, enhancing the reliability and context-sensitivity of content moderation.
By treating prompt writing as an iterative, collaborative process of continuous refinement, practitioners can develop more nuanced and adaptive approaches to capturing the complex contextual understanding required in content moderation.
% ------------
\subsection{Traceability of Prompt Modifications}
%and support regulatory compliance and accountability through transparency.

%%% ---> to add""" organizations must adopt governance practices that emphasize documentation, explainability, and continuous monitoring of model behavior. 

At the model \textit{governance } level accountability concerns can be addressed by implementing mechanisms that enhance the transparency and traceability of prompt modifications. A ``prompt genealogy'' could record changes to prompt structures, formatting, and phrasing, along with the rationale behind each modification. Similar to version control systems used in software development~\cite{spinellis2005version} and drawing inspiration from tools like DVC (Data Version Control)~\cite{dvc2020} and Pachyderm~\cite{pachyderm2021}, which provide versioning and lineage tracking for datasets and machine learning pipelines, a prompt genealogy would extend these principles to prompt engineering. 
For instance, practitioners could generate contextual logs or metadata documenting key aspects of the system's behaviour, such as the inputs provided, the policy components referenced in the prompts, and the resulting outputs. While this approach does not reveal the internal workings of how the LLM processes or interprets prompts internally--a subject of ongoing research~\cite{singh2024rethinking}--it offers a structured way to analyse how prompt changes or input variations  correlate with differences in outcomes.

This transparent record would also provide organisations with a comprehensible audit trail, supporting accountability requirements and reproducibility.  It would also allow organisations to demonstrate alignment with intended policy goals and, where necessary, facilitate external reviews or assessments. Finally, in addition to this tool being useful for auditing, it can complement the sensitivity analysis idea mentioned in Section \ref{sec:enhanced_eval},  by providing data for offline tests.

% By enabling reproducibility and accountability in LLM-driven systems, ths frameowrk would improve visibility into the interaction between policy prompts and outputs.

% This would address regulatory requirements for documenting significant changes to moderation systems, particularly when minor prompt adjustments lead to substantial shifts in enforcement behaviour. 
% Such a system would help organisations trace how prompt modifications influence moderation outcomes %and demonstrate compliance with accountability-focused mandates in automated systems. 
 %




\subsection{Bridging Organisational Silos}
As discussed in Section~\ref{sec:organisational_practice_challenges}, integrating policy authors and machine learning practitioners into a cohesive workflow is essential. However, the transition may encounter practical challenges, similar to historical precedents in other technological paradigm shifts. For example, in the early 2000s the rise of data science in business required close collaboration between domain experts and data specialists~\cite{davenport2012data}. Similarly, the emergence of bioinformatics in the late 20th century necessitated collaboration between biologists and computer scientists before the field gave rise to specialized bioinformaticians~\cite{mitra2022}.

In the short-term, structured collaboration frameworks should be prioritized. These framework  can include regular joint working sessions, shared documentation practices, and established feedback loops, ensuring that policy and machine learning teams work in tandem rather than sequentially. While the long-term goal may be the development of unified roles, these interim measures focus on creating better interfaces between existing specialist teams. This approach enables organisations to maintain effective content moderation while gradually building toward more integrated expertise.

\begin{comment} OLD OLD
\subsection{Holistic Evaluation of Policy-as-Prompt}


We suggest the development of \textbf{holistic evaluation frameworks} for policy-as-prompt implementations as a core solution to address interconnected challenges across technical, sociotechnical, and governance domains. Such comprehensive frameworks should assesses how prompt-based policy configurations perform across multiple dimensions: prompt sensitivity, sociotechnical readiness (i.e. the prompt artifact guides the model towards fair decisions reflecting the policy intent rightly), and regulatory compliance and accountability (through increased transparency), while emphasizing the critical relationships between these aspects.
 

We consider the prompt to be a critical parameter in LLM-based policy-as-prompt content moderation configuration. 
To address the \textit{technical} sensitivity to the prompt structure and format, the evaluation framework should incorporate extensive sensitivity analysis into the development phase, i.e. the stage of the prompt crafting, to study the impact of different formatting, phrasing, and structural changes on model outputs. Stress testing with diverse policy prompts ensures that seemingly small adjustments, like punctuation or formatting, do not unintentionally lead to misinterpretation by the model. \citet{sclar2024quantifying} suggest reporting a performance spread over a sufficient sample of plausible prompt formats/styles, instead of simply reporting the formatting used and its performance, as is currently standard. Sensitivity analysis should also account for examining subtle (and hard to capture) performance inconsistencies such as in the case of predictive multiplicity. For instance, Rashomon sets can reveal how different policy phrasings that appear semantically equivalent to humans may trigger divergent responses from the models, e.g. inconsistent handling of edge cases, despite achieving similar performance metrics \cite{gomez2024, marx20}. By analysing the diversity of the policy-as-prompt configuration with approaches like Rashomon sets, developers can identify which policy formulations are not only effective but also robust. 


\subsection{Other}
As seen in Section \ref{sec:technical_pol_to_prompt}
translating human-readable policy guidelines into prompts that are not only understandable but also actionable for LLMs is challenging. We discussed how we require careful attention to the ambiguities that can arise due to the inherent differences between human and machine interpretability. Best practices for prompt engineering are still emerging \cite{willner_chakrabarti_2024, voronov-etal-2024-mind}. Techniques like few-shot prompting and chain-of-thought reasoning \cite{wei2022} have shown promise in crafting prompts that enhance model performance. In CoT, LLMs are prompted to explain their reasoning, potentially exposing ambiguities, contradictions, or edge cases in policy language.  In an ideal scenario, this feedback loop enables practitioners to diagnose inconsistencies in machine interpretations and iteratively improve the clarity of policy directives. The policy itself becomes a dynamic interface between human intent and algorithmic interpretation.  In the immediate term, solutions for the policy-as-prompt setting, we believe, must focus on creating prompts that minimize machine misinterpretations. Similarly to CoT,  LLMs can take on a more agentic role while retaining crucial human oversight. We suggest to foster a collaborative \textit{agentic} environment where multiple LLMs, each with potentially diverse training data and architectures,  work together on policy interpretation and refinement \TODO{[CITE]}. This collaborative framework could involve LLMs suggesting rewrites, identifying areas where additional examples are needed, or even proposing entirely new policy labels. The feedback loop created allows for iterative refinement of the policy prompt.  Furthermore, employing a diverse set of LLMs helps mitigate the risk of individual model biases. Ultimately, this collaborative approach, with human moderators providing crucial oversight, narrows the gap between human understanding and machine interpretation. As LLMs refine their suggestions, human moderators can compare successive iterations to understand how models evolve in their interpretation. While these approaches are critical for converting policies to prompts, it is essential to emphasize their connection to the holistic evaluation framework. Any advancements in prompt crafting should not operate in isolation. Instead, they must be continuously assessed and refined using insights derived from the comprehensive evaluation mechanisms outlined above. 



While the unified framework effectively addresses technical, sociotechnical, and governance challenges, \textit{organisational} challenges and the converging roles of experts present a distinct set of demands that require targeted solution. The integration of policy authors and ML practitioners into a cohesive workflow is pivotal, but the immediate transition faces practical constraints similar to historical precedents in other technological paradigm shifts. Just as the rise of data science in business required collaboration between domain experts and data specialists in the early 2000s \cite{davenport2012data}, the convergence of biologists and computer scientists in the field of bioinformatics during the late 20th century required similar collaboration before specialized bioinformaticians emerged \cite{mitra2022}. Short-term solutions should prioritise structured collaboration frameworks where policy and ML teams work in tandem rather than sequentially, through regular joint working sessions, shared documentation practices, and established feedback loops. 
These interim measures acknowledge that while the eventual goal may be unified roles, the immediate need is for better interfaces between existing specialist teams, allowing organizations to maintain effective content moderation while building toward more integrated expertise.
OLD OLD
\end{comment}


%Ongoing Vulnerabilities of LLMs: Risks like hallucinations, adversarial attacks, or difficulty moderating ambiguities in user-generated content are intrinsic to LLM-based solutions. These challenges persist irrespective of advancements in prompt engineering

\section{Conclusions}
\label{sec:conclusion}

% This research provides a starting point for investigating the "policy-as-prompt" paradigm. 
% We hope that the organisational structure we provide will promote discussion and make identification of additional challenges easier.

%summary
In this paper, we explored the complexities of the policy-as-prompt paradigm in content moderation through four dimensions: technical, sociotechnical, organizational, and governance. By presenting a structured overview of the challenges practitioners (would) face when transitioning from ``traditional'' machine learning or rule-based systems to the policy-as-prompt paradigm, we aim to support decision-making and underscore the importance of holistically considering the interplay between the technical capabilities of large language models, social and organizational contexts, and governance frameworks. Additionally, we propose potential strategies to mitigate some of these challenges, while acknowledging that many open questions remain regarding the implementation of effective content moderation systems under this paradigm.

Our discussion highlights challenges across these four dimensions without claiming to be exhaustive, and readers may identify additional issues beyond those covered in this paper. Furthermore, while we emphasize  the transformative potential of the policy-as-prompt paradigm, it is important to note that such systems are not currently implemented in a fully autonomous manner. Instead, they typically operate within hybrid setups, where human moderators work alongside and oversee LLM-driven enforcement mechanisms. However, even within this hybrid framework, the increasing adoption of the policy-as-prompt configuration introduces considerable complexities and potential risks that warrant further investigation. Addressing these challenges proactively will be critical to developing more effective hybrid moderation systems that thoughtfully integrate LLM capabilities with human oversight. 

Empirically, we focused on one specific issue--model sensitivity to prompt structure--to demonstrate the brittleness of current LLMs are. We recognize, however, that model performance is influenced by a broader set of parameters beyond prompt structure, including the extent of pre-training, the choice of fine-tuning techniques, model architecture and size, and other factors that were fixed in our experiments. A more comprehensive analysis that examines how these parameters interact would yield deeper insights into their collective impact on model behaviour and reliability in content moderation tasks.

% In terms of empirical support, we have focused on one specific issue (a model's sensitivity to prompt structure) which shows just how brittle current LLMs are. Going forward we expect many of these issues to be resolved as new LLM version become available. At the same time, with each release of a more capable LLM, users gain an additional tool to generate more complex content, which in turn requires a more powerful LLMs for moderation. It is thus an ongoing cycle of escalation that underscores the need for continuous innovation in moderation technologies.

Going forward, as LLM capabilities continue to improve, the policy-as-prompt has the potential to overcome many of its current limitations, enabling organizations to dynamically align enforcement with nuanced and evolving policy objectives.  Sustained exploration of these advancements will be essential in shaping the future of content moderation systems.

% We believe that future research should expand the scope by exploring diverse use cases, integrating multiple models, and examining how human oversight interacts with LLM-driven systems in order to further to refine and adapt the `policy-as-prompt' framework for real-world deployment.

% \section{Table Example}

% \input{commands}
% \input{tables/example-table}




%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}

% \bibliographystyle{plain}
\bibliography{main}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/policy_as_prompt_spider.png}
    \caption{\textbf{Challenges} (light orange) across different \textbf{areas} (green) in ‘policy-as-prompt’ implementation.}
    \Description{Diagram illustrating challenges in implementing the concept of 'policy-as-prompt,' organized into four domains: Technical Implementation, Sociotechnical, Organisational, and Governance. Each domain connects to specific challenges, such as 'Converting policies to prompts' and 'Prompt structure and format sensitivity' under Technical Implementation, 'Technical determinism in policy formation' under Sociotechnical, 'Evolving policy-ML team roles' under Organisational, and 'Model governance and accountability' under Governance. The central node, 'policy-as-prompt,' anchors the relationships between these areas.}
    \label{fig:spider}
\end{figure}

\section{Experiments}
\label{sec:exp}
\subsection{Prompt style variations}
\input{figures/prompt_style_fig}

% --- First set 



\begin{figure}[h]
  \centering
  % Subfigure 1
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/performance_across_categories.png}
    \caption{F-beta scores for the different textual variations considered, across some of the most important categories.}
    \Description{}
    \label{fig:performance_exp1}
  \end{subfigure}
  \hfill
  % Subfigure 2
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/exp1_sd_per_category.png}
    \caption{Standard Deviation for the performance of the main policies considered.}
    \Description{}
    \label{fig:sd_per_category}
  \end{subfigure}
  \caption{Comparison of performance metrics across categories and their standard deviations.}
  \label{fig:combined_std}
\end{figure}

%%% OLD  starts

We observe that model performance, as measured by the F-beta score, fluctuates across textual prompt variations (see Figure~\ref{fig:prompt-templates} for an example of the template variations and Figure~\ref{fig:performance_exp1} for the results). Categories such as Not Violating and Self-Harm maintain relatively high and stable scores, while more contentious categories like Jailbreak, Harm, and Illegal Goods display marked sensitivity. This disparity highlights that model performance is prone to degradation when subjected to structural changes in the policy prompt.  This directly supports the thesis of Section 3.2, highlighting the role of prompt design in shaping moderation outcomes. The variations in F-beta scores (Figure \ref{fig:performance_exp1}) and the high standard deviation (Figure \ref{fig:sd_per_category}) demonstrate predictive multiplicity in action. Even though the overall performance across prompt variations may appear statistically comparable, the conflicting behaviours observed in key categories (e.g., Jailbreak, Harm) point to a deeper underlying issue: different prompt structures lead to divergent model predictions for the same input samples.
%  OLD ends

\subsection{Policy Evolution Snapshots} 
To examine the role of policy specificity, we created temporal snapshots of policy descriptions, starting with minimal drafts (one-line descriptions) and progressively adding detail to arrive at comprehensive guidelines (as seen in Table \ref{tab:table_temporal} reversely).

\input{tables/temporal_prompts} 


\subsection{Model Size}


\begin{figure}[h]
  \centering
  % Subfigure 1
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/models.png}
    \caption{P}
    \Description{}
    \label{fig:model_size_variation}
  \end{subfigure}
  \hfill
  % Subfigure 2
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/topmodel_predictive_multiplicity.png}
    \caption{}
    \Description{}
    \label{fig:topmodel_per_category}
  \end{subfigure}
  \caption{Analysing the effect of model provider and size in sensitivity and predictive multiplicity. a) The performance distribution of different model sizes for the six prompt template variations. b) Accuracy across policy categories for the `Structured' and `Annotators' prompt types.}
  \label{fig:combined}
\end{figure}


To gain insight how models with different size respond to variations of the prompt, we ran experiments using models with different size from different providers. In Figure \ref{fig:model_size_variation} we plot the performance distribution and use `P\#' to indicate different model provider series. Middle sized models are between the 32B and the 64B parameters mark, while big ones surpass the 100B. Overall, while we observe that the sensitivity to prompt structure and style decreases with model size, it is still present but to a lesser degree. We also observe that different providers - thus different pre-training strategies- affect the model sensitivity. Still, even for the most capable models, predictive multiplicity arises  as can be seen in \ref{fig:topmodel_per_category}, where the variants \textit{Structured} and \textit{Annotator-Friendly} present significant performance divergences for certain categories, despite negligible differences in averaged accuracy. For example, accuracy scores for Hate are around five points higher for the \textit{Annotator-Friendly} variation. 

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
