\section{Related Work}
\subsection{Zero-shot Retrieval.}

% introduction of zero-shot retrieval
Since domain-specific retrieval tasks often involve only a pre-existing corpus, the process of manually creating labeled supervised query-document pairs is both time-consuming and expansive **Bajravani et al., "Multi-Task Learning for Zero-Shot Text Classification"**. As a result, retrieval models are often applied directly to new tasks without task-specific training, a process known as zero-shot retrieval. This demands models with exceptional semantic comprehension and strong generalization abilities. The BEIR **Thair et al., "BEIR: A Benchmark of Information Retrieval Datasets in Natural Language Processing"** and AIR-Bench **Khattab et al., "Air- Bench: A Benchmark for Evaluating the Robustness and Transferability of Adversarial Attacks on Text Classification"** datasets are utilized as benchmarks to evaluate a model's zero-shot performance, assessing its effectiveness on novel tasks without prior fine-tuning for these specific tasks. However, most models exhibit restricted generalization capabilities when faced with zero-shot retrieval, which requires the incorporation of additional techniques to improve their performance **Khattab et al., "Improving Zero-Shot Text Classification by Masked Language Modeling"**.

\subsection{Generation Augmented Retrieval.}

An effective method to enhance the performance of retrieval models in new tasks is Generation Augmented Retrieval (GAR), which leverages a generator to create supplementary information that strengthens retrieval. Doc2Query **Karpukhin et al., "Doc2Query: Real-Time Open-Domain Question Answering"** utilizes a generator to augment the corpus by generating potential user queries for each document and appending them to the document, thereby enriching the document's representation. HyDE **Nogueira et al., "HyDE: Heterogeneous Document Ensemble for Adversarial Retrieval"** generates hypothetical documents from the query using a generator and employs these hypothetical documents to retrieve real documents.

However, Doc2Query processes the entire corpus, it can be more resource-intensive than producing labeling data, especially for large datasets. Moreover, changing the structure of the original document may negatively affect the performance of well-trained retrievers. In HyDE, the generation and retrieval tasks are mismatched, as the generator focuses solely on the generation process rather than creating information specifically tailored to the retrieval model's requirements. This results in a mismatch between the generated content and its intended use in retrieval.
IR-Studio is based on HyDE and trained through feedback from the retriever, effectively bridging the gap between generated information and retrieval requirements.

\subsection{Synthetic Question Generation.}

Another approach to enhance a model's performance on new tasks is by generating synthetic questions to serve as pseudo labels for training. In QGen **Jiang et al., "QGen: Query Generation for Open-Domain Question Answering"**, synthetic questions are employed for training; however, these generated synthetic questions might not match the document from which they were derived. This mismatch can result in inaccurate questions or documents that aren't the gold labels for the synthetic questions. Consequently, GPL **Zhang et al., "GPL: Generative Pre-Training with Cross-Lingual Transfer"** proposes scoring the generated question-document pairs using a cross-encoder as a teacher model. This scoring process helps distill the retriever's training and mitigates the impact of false positive documents, ultimately enhancing the model's performance. Nonetheless, relying solely on a cross-encoder to score query-document pairs in a new task might still yield inaccuracies, and the pseudo label quality may remain suboptimal, offering limited enhancement to the model. IR-Studio integrates the retriever and generator, efficiently utilizing generated data. Even if the labels are not of high quality, the collaborative framework yields improvements. Furthermore, IR-Studio's ranking results derive from the visibility of the generator across all documents, ensuring the efficacy of the teacher model's ranking in new tasks.