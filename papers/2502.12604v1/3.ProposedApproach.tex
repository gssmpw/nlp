
\section{Proposed Approach}\label{sc3}

This section first explains the motivation for learning noise-resistant representations, then introduces the S2C learning framework, and subsequently details the proposed technologies including contrastive change learning, grid sparsity loss, and the change mapping algorithms. Finally, the S2C methodologies are expanded to address unsupervised MMCD.

\subsection{Noise-resistant Semantic Embedding}\label{sc3.A}

DL-based CD essentially learns to project multi-temporal RS images ${I_1, I_2}$ into a binary change map $\mathbf{y}_c$. Let $f_\theta$ denote an encoding function parameterized by $\theta$, and $g$ being a projection function. This process can be represented as:
\begin{equation}\label{eq.cd}
    \mathbf{y_1} = f_\theta(I_1), \mathbf{y_2} = f_\theta(I_2), \mathbf{y_c} = g(\mathbf{y_1}, \mathbf{y_2})
\end{equation}
where $\mathbf{y_1}, \mathbf{y_2} \in \mathbb{R}^{c \times h \times w}$ are the learned semantic latent, $\mathbf{y_c} \in \mathbb{R}^{h \times w}$ is a change probabilistic map, $s$ and $h, w$ are the channel and spatial dimensions, respectively.

Let us denote the imaging process as $\Phi$, ground semantics as $s$, and semantic changes as $\delta$. In an ideal case where there is no temporal noise, this process can be re-written as:
\begin{equation}\label{eq.imaging}
    \mathbf{y}_s = f_\theta[\Phi_1(s)], \mathbf{y}_{s+\delta} = f_\theta[\Phi_2(s+\delta)], \mathbf{y_c} = g(\mathbf{y}_s, \mathbf{y}_{s+\delta})
\end{equation}
Since $\mathbf{y}_s$ and $\mathbf{y}_{s+\delta}$ share a common semantic space, $g$ can be implemented using a simple linear transformation with normalization. However, when considering practical cases that involve temporal noise (refer Fig.\ref{fig.challenge}), there exist insignificant changes (denoted $\epsilon$), spatial variance (denoted $\Omega$), and sensor differences ($\Phi_1 \neq \Phi_2$). Consequently, $I_2$ is projected into a different space:
\begin{equation}\label{eq.noise}
    f_\theta(I_2)=f_\theta\{\Omega[\Phi_2(s+\delta+\epsilon)]\}=\mathbf{y^{\prime}}_{s+\delta+\epsilon}
\end{equation}

Thus, the distance between $\mathbf{y}_s$ and $\mathbf{y^{\prime}}_{s+\delta+\epsilon}$ is nonlinear, and optimizing subsequent change embedding with $g$ typically requires supervised learning with task-specific labels.

To achieve unsupervised change learning, we utilize spatial and spectral augmentations to simulate various types of temporal noise, thus training a noise-resistant $f_\theta$ and formulating a training-free $g$. First, we apply augmentations functions $\phi$ on $I_1$ simulating the noise in Eq.(\ref{eq.noise}):
\begin{equation}\label{eq.phi_I1}
    \phi(I_1)= \hat{\Omega}\{\hat{\Phi}_{1 \rightarrow 2}[\Phi_1(s)]\} = \hat{\Omega}[\hat{\Phi}_{1,2}(s)],
\end{equation}
where $\hat{\Omega}$ is a set of spatial augmentations, $\hat{\Phi}_{1 \rightarrow 2}$ is a set of spectral augmentations that adapt the spectral distribution of $I_1$ approaching $I_2$. The augmentations are performed with stochastic parameters to simulate the imagining process with random noise, generating diverse training pairs. Further utilizing CL techniques elaborated in Sec.\ref{sc3.CL}, we can train $f_\theta$ towards projecting $I_1$ and $\phi(I_1)$ into similar representations, i.e., learning noise-invariant semantic latent:
\begin{equation}
    f_\theta(I_1) = \mathbf{\hat{y}}_s, 
    f_\theta[\phi(I_1)] = f_\theta\{ \hat{\Omega}[\hat{\Phi}_{1,2}(s)] \} \approx \mathbf{\hat{y}}_{s}
\end{equation}
These operations are also symmetrically performed on $I_2$ as:
\begin{equation}\label{eq.phi_I2}
\begin{aligned}
    \phi(I_2) = \hat{\Omega}\{\hat{\Phi}_{2 \rightarrow 1}[\Phi_2(s+\delta+\epsilon)]\} \approx \hat{\Omega}[\hat{\Phi}_{1,2}(s+\delta)],\\
    f_\theta(I_2) = \mathbf{\hat{y}}_{s+\delta}, f_\theta[\phi(I_2)] \approx f_\theta\{ \hat{\Omega}[\hat{\Phi}_{1,2}(s)] \} \approx \mathbf{\hat{y}}_{s+\delta}
\end{aligned}
\end{equation}
where $\epsilon$ denoting the subtle changes can be diminished since $\hat{\Omega}$ includes spatial filtering operations.
This enables modeling the semantic differences between $\mathbf{\hat{y}}_s, \mathbf{\hat{y}}_{s+\delta}$ in a common latent space, enabling a training-free formulation of $g$:
\begin{equation}
    \mathbf{y_c} = g(\mathbf{\hat{y}}_s, \mathbf{\hat{y}}_{s+\delta})
\end{equation}

\subsection{Overview of the S2C framework}
The theoretical analysis in Sec.\ref{sc3.A} offers a simplified understanding of the optimizing goals. In the following, we elaborate on the S2C architecture that trains $f_\theta$ to exploit noise-invariant semantic representations.

As depicted in Fig.\ref{fig.flowchar}, S2C is a UCD framework consisting of distinct stages of training and inference. During the training phase, a set of augmentation operations are first performed on the input images, corresponding to $\phi(\cdot)$ in Eq.(\ref{eq.phi_I1})(\ref{eq.phi_I2}). The augmentations comprise a sequential combination of spectral and spatial operations executed randomly at each training iteration, including \textit{RGBshift}, \textit{PCA adaptation}, \textit{down-sampling} and \textit{random shifting}. \textit{PCA adaptation} blends the spectral distribution of $I_1$ and $I_2$. The spatial operations are performed to simulate spatial misalignment and imaging degradation/distortion, while the spectral operations replicate imaging and seasonal variations. Collectively, their combination simulate stochastic injection of diverse temporal noise.

Subsequently, utilizing the constructed pre- and after-augmentation image sequences, two self-supervised CL strategies are jointly employed to learn task-specific semantic features. Although this training process can be performed from scratch using common DNNs, employing VFM as feature encoders empirically leads to better accuracy. In addition, considering the limitations of VFM in processing RS images \cite{ji2024segment}, we employ a VFM augmented with additional parameters $w$, denoted as $f_{\theta + w}$. $w$ is a trainable parameter implementing LoRA \cite{hu2021lora} (with $rank=4$), a parameter-efficient technique extensively used to adapt VFMs to a particular domain of interest. $\theta$ is frozen to retain pre-trained visual knowledge, whereas $w$ are the LoRA weights trained with CL paradigms to exploit temporal semantic features. The VFM can be any off-the-shelf models, as its inner structure is not modified in our S2C architecture.

The training process is conducted within two CL paradigms to learn the CD-relevant semantic representations. The two CL paradigms are introduced to embed difference and consistency representations, respectively, which are elaborated in Sec.\ref{sc3.CL}. The associated loss functions are $\mathcal{L}_{tri}$ and $\mathcal{L}_{info}$, respectively. In addition, we further introduce a sparsity regularization objective to learn sparse and compact change representations, noted as $\mathcal{L}_{spa}$. The joint training objective is:
\begin{equation}\label{eq.losses}
    \mathcal{L} = \mathcal{L}_{tri} + \alpha \mathcal{L}_{info} + \beta \mathcal{L}_{spa}
\end{equation}
where $\alpha$ and $\beta$ are two weighting parameters.

In the inference phase, the semantic latent $\mathbf{y}_1$ and $\mathbf{y}_2$ are first mapped to a coarse change map, then refined using the VFM decoder and an IoU matching function. The details are elaborated in \ref{sc3.chg_map}.

\subsection{Contrastive Change Learning} \label{sc3.CL}

\begin{figure*}[t]
\centering
    \includegraphics[width=0.95\linewidth]{illu_pic/CL_pipelines.png}
    \caption{Comparison of CL paradigms in CD. (a) \textbf{Consistency regularization}: $f_\theta$ extracts stable representations across weak/strong perturbations; (b) \textbf{Spatial contrast}: $f_\theta$ distinguishes same/different regions; (c) Proposed \textbf{Consistency-regularized Temporal Contrast} (CTC): $f_\theta$ identifies temporal differences independent of spectral or seasonal variations, and (d) Proposed \textbf{Consistency-regularized Spatial Contrast} (CSC): $f_\theta$ distinguishes same/different regions despite perturbations.}
\label{fig.CL_paradigms}
\end{figure*}

Before introducing the proposed CL paradigms, let us first review the two typical CL paradigms in CD, and analyze their usage and limitations.

\textit{1) Consistency Regularization (CR).} As depicted in Fig.\ref{fig.CL_paradigms}(a), a DNN $f_{\theta}$ learns to improve the robustness and generalization of feature embeddings. An image $I$ is first augmented with weak and strong transformations, resulting in two copies $\Tilde{I}$ and $\Bar{I}$. Then a distance loss is calculated between the two copies to ensure consistency across perturbations.

Since this learning paradigm does not explicitly model differences/similarities, it is often adopted in semi-supervised \cite{bandara2022revisiting} or weakly-supervised \cite{zhao2024pixellevel} learning settings to extend the CD insights learned with limited samples.

\textit{2) Spatial Contrast (SC).} As illustrated in Fig.\ref{fig.CL_paradigms}(b), $f_{\theta}$ learns to differentiate between bitemporal image pairs $[I_1^i, I_2^i]$ of the same region $i$ and $[I_1^i, I_2^j]$ of different regions $i$ and $j$. This drives $f_{\theta}$ to learn consistent embeddings against temporal variations. Areas with high similarity are identified as $unchanged$, whereas their opposites are detected as $changed$ \cite{chen2022selfsupervised}.

However, we identify that there are several limitations in this paradigm: i) changes are identified through negative embedding of the similarities rather than through explicit modeling. This often causes sensitivity to noise. ii) $f_{\theta}$ focuses on the discriminative elements within a region, such as certain edges or corners, rather than effectively exploiting the local semantic context. %iii) the supervision signal is given at the image level (same/different pairs), thus only coarse similarity representations are obtained.

Considering these limitations, we introduce two novel CL paradigms specifically tailored to the context of CD.

\textbf{Consistency-regularized Temporal Contrast} (CTC). An RS image $I_1$ is first augmented with a transform function $\phi(\cdot)$, producing a copy $\Bar{I}_1$. Subsequently, $I_1$ is employed as an anchor for comparison with both a positive sample $\Bar{I}_1$ and a negative sample $I_2$. $\phi(\cdot)$ simulates spectral and spatial noise between multi-temporal observations, as illustrated in Fig.\ref{fig.flowchar}. Consequently, $f_\theta$ learns to exploit noise-invariant difference representations, i.e., the semantic changes. With greater details, Fig.\ref{fig.flowchar} illustrates the CTC paradigm with bi-directional comparisons within $[I_1, \Bar{I}_1, I_2]$ and $[I_2, \Bar{I}_2, I_1]$. 

A triplet training objective $\mathcal{L}_{tri}$ using cosine distance is utilized for comparisons within the triplets. This is to align with the cosine difference embedding during the inference stage. The calculations are as follows:
\begin{equation}
\begin{aligned}
    %\mathbf{y_1}, \mathbf{y_2} = f_{\theta + w}(I_1), f_{\theta + w}(I_2),\\
    %\mathbf{\Bar{y}_1}, \mathbf{\Bar{y}_2} = f_{\theta+w}[\phi(I_1)], f_{\theta+w}[\phi(I_2)],\\
    \mathcal{L}_{tri} = max[cos(\mathbf{y_1}, \mathbf{y_2})-cos(\mathbf{y_1}, \mathbf{\Bar{y}_1})+m, 0]\\
    + max[cos(\mathbf{y_2}, \mathbf{y_1})-cos(\mathbf{y_2}, \mathbf{\Bar{y}_2})+m, 0]
\end{aligned}
\end{equation}
where $m=1$ is a margin parameter to promote separation between the anchor and positive.

\textbf{Consistency-regularized Spatial Contrast} (CSC). This contrastive learning paradigm integrates CR into typical SC learning, thereby enhancing the embedding of spatial consistency against perturbations. CSC alleviates the vulnerability to noise inherent in the SC paradigm by incorporating transformation $\phi(\cdot)$. The transformations, particularly the spatial transformations, reduce dependence on high-frequency spatial details, thereby prompting the exploitation of local semantic patterns such as color and texture.

We have introduced an additional variation in CSC, i.e., the calculation of consistency at each spatial position. Given a batch consisting of $N$ paired RS images $\{[I_1^i, I_2^i], [I_1^j, I_2^j], ..., [I_1^k, I_2^k]\}$, we first apply $\phi(\cdot)$ on each of the temporal images, thus getting two sets of augmented images. These images are further encoded with $f_{\theta+w}$, resulting in 4 sets of features: $[\mathbf{y_1^i, y_1^j, ..., y_1^k}]$, $[\mathbf{y_2^i, y_2^j, ..., y_2^k}]$, $[\mathbf{\Bar{y}_1^i, \Bar{y}_1^j, ..., \Bar{y}_1^k}]$ and $[\mathbf{\Bar{y}_2^i, \Bar{y}_2^j, ..., \Bar{y}_2^k}]$. We then calculate the co-occurrences between them, resulting in two matrices each with $N \times N$ dimensions, as illustrated in Fig.\ref{fig.CL_paradigms}(d). We utilize an infoNCE loss function to effectively train $f_{\theta+w}$ for the differentiation of genuine image pairs. It is calculated across both temporal phases, represented as:
\begin{equation} \label{eq.info_loss}
\begin{aligned}
    \mathcal{L}_{info} = -\frac{1}{N} \sum_{u=1}^{N} \log \left[ \frac{\exp \left( \mathbf{y}_1^u \odot \mathbf{\Bar{y}}_2^u \right)}{\sum_{v=1}^{N} \exp \left( \mathbf{y}_1^u \odot \mathbf{\Bar{y}}_2^v \right)} \right]\\
    -\frac{1}{N} \sum_{u=1}^{N} \log \left[ \frac{\exp \left( \mathbf{y}_2^u \odot \mathbf{\Bar{y}}_1^u \right)}{\sum_{v=1}^{N} \exp \left( \mathbf{y}_2^u \odot \mathbf{\Bar{y}}_1^v \right)} \right]
\end{aligned}
\end{equation}
where $\odot$ denotes a spatial similarity function that we introduce in this study. Instead of pooling the spatial features into single vectors for similarity calculation \cite{chen2022selfsupervised}, we compute the similarity at each spatial patch $p$, denoted as:
\begin{equation} \label{eq.sim_calc}
    \mathbf{y} \odot \mathbf{\Bar{y}} = \frac{1}{w \times h} \sum_{p} \left( \frac{\mathbf{y}^p \cdot \mathbf{\Bar{y}}^p}{|\mathbf{y}^p||\mathbf{\Bar{y}}^p|} \right)
\end{equation}
Both $\mathcal{L}_{tri}$ and $\mathcal{L}_{info}$ are calculated based on cosine similarity. While $\mathcal{L}_{tri}$ embeds appearance-invariant temporal differences, $\mathcal{L}_{info}$ embeds noise-resilient temporal consistencies. Therefore, when a certain temporal consistency pattern is captured in CSC, it suppresses the difference representations of the same area in CTC.

\subsection{Grid Sparsity loss} \label{sc3.loss_sparse}

Changed objects are commonly sparsely distributed in RS images and each present as compact regions. In contrast, edges and points are often associated with noise. Although training objectives that promote sparse representations have been explored in the literature, they typically calculate and penalize the average value of $\mathbf{y_c}$ \cite{bandara2023deep}. However, this approach does not guarantee sparsity, as there exists a trivial solution of learning an additional bias term on $\mathbf{y_c}$.

Differently, we propose a novel grid sparsity loss where sparsity is assessed at the level of each local grid rather than at each pixel. Considering the frequency of changes along with spatial resolution in an RS image, first, we predefine a sparsity threshold $T$ as well as a grid size $d$. Subsequently, the average density of each grid $\mathbf{g}$ is calculated and ranked, while a {\footnotesize$1-T$} ratio of grids with the lowest density are selected for loss calculation as follows:
\begin{equation} \label{eq.sparse_loss}
\begin{aligned}
    \mathcal{L}_{spa} = max\{ \frac{1}{n} \sum^{n} [sort\uparrow(y^\mathbf{g})], 0\},\\
    y^\mathbf{g} = \frac{1}{d*d} \sum_{p \in \mathbf{g}} \mathbf{y}_c^p, 
    n = wh*(1-T)/d^2
\end{aligned}
\end{equation}

We empirically set $d=16$ for HR RS images. This regularization objective ensures that a proportion of less than {\footnotesize$1-T$} potential changes exhibit high values, whereas the insignificant change representations in other areas are minimized.

\subsection{Change mapping} \label{sc3.chg_map}

\begin{algorithm}[t]
    \caption{Algorithm for IoU Matching and Refinement}
    \label{AlgorithmD}  \label{Algorithm.IoU_matching}
    \begin{algorithmic}[1]
        \renewcommand{\algorithmicrequire}{\textbf{Input:}}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \REQUIRE Change probability map $\mathbf{y}_c$,\\
                 VFM-extracted bi-temporal object masks $M_1$, $M_2$,\\
                 Parameter: IoU threshold $T_{IoU}$;
        \ENSURE Refined Change Map $M_c$;
        \FORALL{$m_i \in M_1, m_j \in M_2$}
        \IF {$(m_i \cap m_j)/(m_i \cup m_j)>T_{IoU}$}
            \STATE $M_{1} \leftarrow M_{1} \setminus \{m_i\}$
            \STATE $M_{2} \leftarrow M_{2} \setminus \{m_j\}$
            %\STATE $M_{12} \leftarrow M_{12} \cup \{m_i, m_j\}$
        \ENDIF
        \ENDFOR
        \STATE $M_{12} \leftarrow M_{1} \cup M_{2}$
        \FORALL{$m \in M_{12}$}
        \IF {$m \odot \mathbf{y}_c / \sum_{p} m_p >T_{IoU}$}
            \STATE $M_c \leftarrow M_{c} \cup \{m\}$
        \ENDIF
        \ENDFOR
 \RETURN $M_c$ 
    \end{algorithmic}
\end{algorithm}

The use of VFM and CL methodologies aims to enhance the effective exploitation of semantic contexts across multi-temporal image domains. In the inference phase, the major challenge lies in accurately mapping fine-grained changes. We employ a coarse-to-fine refinement strategy. First, a coarse change probability map $\mathbf{y}_c$ is derived by projecting the negative cosine embedding of the bi-temporal semantic embeddings:
\begin{equation}
    \mathbf{y}_c = \sigma[-cos(\mathbf{y}_1, \mathbf{y}_2)*\eta]
\end{equation}
where $\sigma$ is a $sigmoid$ function and $\eta=ln(1/0.07)$ is a scaling factor defined following literature practice \cite{deuser2023sample4geo}.

Then, we employ a pretrained VFM decoder $g_{\gamma}$ to segment two groups of bi-temporal masks $M_1=\{m_1^1, m_1^2, ..., m_1^k\}$ and $\{M_2=m_2^1, m_2^2, ..., m_2^k\}$ using the spatial prompts generated on high-response regions in $\mathbf{y}_c$. Given the logic implication of \textit{change}, high-overlap objects in $M_1$ and $M_2$ can be inferred as false alarms. Therefore, we use an XOR-alike matching algorithm (denoted $\ominus$) to merge $M_1$ and $M_2$ while eliminating the objects with large overlaps:
\begin{equation}
    M_{12} = M_1 \ominus M_2
\end{equation}
We further implement an Intersection-over-Union (IoU) analysis between $\mathbf{y}_c$ and $M_{12}$ to match the VFM-generated masks with the high-confidence regions in $\mathbf{y}_c$. The matched objects replace their counterparts in $\mathbf{y}_c$ as the changed items. For more details, a pseudo-code of this IoU analysis and matching algorithm is provided in Algorithm \ref{Algorithm.IoU_matching}.

Using this change mapping algorithm, the coarse predictions derived from the DNN are refined into detailed CD results, aligning with the spatial details present in the HR imagery.

\subsection{S2C for Unsupervised MMCD}

\begin{figure}[t]
\centering
    \includegraphics[width=1\linewidth]{illu_pic/S2C_flowchart_het.png}
    \caption{Illustration of the application of the proposed S2C for UCD in multimodal RS images. This learning framework applies to not only optical and SAR data, but also other image modalities.}
\label{fig.Het_flowchart}
\end{figure}

RS images observed by specialized sensors such as Synthetic Aperture Radar (SAR) and Infrared (IR) scanners demonstrate a notable modality difference compared to standard optical images. This significant domain gap precludes VFMs from extracting meaningful semantic representations \cite{ji2024segment}. Leveraging the joint modeling of consistency and differences, the S2C framework is capable of modeling modality-invariant change representations over multi-temporal observations.

Fig.\ref{fig.Het_flowchart} depicts the adapted S2C training pipeline for multimodal RS images, where the UCD process is exemplified through a pair of optical and SAR images. The semantic-to-change mapping is achieved through alignment of latent semantics, making it invariant to specific imaging modalities. Let us denote $I_{rgb}^i$ and $I_{sar}^i$ as a pair of multimodal images pertaining to region $i$. Two independent encoders with parameters $\theta$ and $\zeta$ are trained to extract features from optical and SAR images, respectively. It is worth noting that the VFMs are typically not applicable to SAR data and, therefore are not utilized in $f_\zeta$. However, with the continuous development of RS VFMs \cite{guo2024skysense, hong2024spectralgpt}, certain models may demonstrate the efficacy of semantic embedding within a specific domain. While we did not identify an optimal VFM for common SAR data, readers are encouraged to explore emerging VFMs within the S2C framework.

After feature embedding, the CTC and CSC paradigms are utilized to learn domain-invariant differences and consistencies. An adjustment in implementing the CTC is that it is asymmetrically applied, unlike its use with dual optical images. Given that optical data possess more comprehensive spatial information, the calculation of $\mathcal{L}_{tri}$ is centric to the optical data, i.e.:

\begin{equation} \label{eq.het_triplet_loss}
    \mathcal{L}_{tri} = max[cos(\mathbf{y}_{rgb}, \mathbf{y}_{sar})-cos(\mathbf{y}_{rgb}, \mathbf{\Bar{y}}_{rgb})+m, 0]
\end{equation}
where $\mathbf{y}_{rgb}$, $\mathbf{y}_{sar}$ and $\mathbf{\Bar{y}}_{rgb}$ are the semantic features encoded from $I_{rgb}$, $I_{sar}$ and $\phi(I_{rgb})$, respectively. The augmentation operations outlined in $\phi$ correspond to those detailed in Sec.\ref{sc3.CL}.

The presence of a domain gap poses more challenges to learning temporal consistency. Consequently, CSC learning is conducted directly on the original image sets $[\mathbf{y}_{rgb}^i, \mathbf{y}_{rgb}^j, ..., \mathbf{y}_{rgb}^k]$ and $[\mathbf{y}_{sar}^i, \mathbf{y}_{sar}^j, ..., \mathbf{y}_{sar}^k]$, rather than using their augmented counterparts. The loss function is computed as:
\begin{equation} \label{eq.het_info_loss}
\begin{aligned}
    \mathcal{L}_{info} = -\frac{1}{N} \sum_{u=1}^{N} \log \left[ \frac{\exp \left( \mathbf{y}_{rgb}^u \odot \mathbf{y}_{sar}^u \right)}{\sum_{v=1}^{N} \exp \left( \mathbf{y}_{rgb}^u \odot \mathbf{y}_{sar}^v \right)} \right]\\
    -\frac{1}{N} \sum_{u=1}^{N} \log \left[ \frac{\exp \left( \mathbf{y}_{sar}^u \odot \mathbf{y}_{rgb}^u \right)}{\sum_{v=1}^{N} \exp \left( \mathbf{y}_{sar}^u \odot \mathbf{y}_{rgb}^v \right)} \right]
\end{aligned}
\end{equation}

The comprehensive training objective is consistent with that in Eq.\ref{eq.losses}. The simultaneous application of CTC and CSC learning drives $f_\theta$ and $f_\zeta$ to align semantic representations across different domains, thus mapping domain-invariant changes. In the inference stage, since typical VFM decoders are unable to segment SAR objects, the refining algorithm is omitted. The change probability maps $\textbf{y}_c$ is directly binarized to map the multimodal changes.
