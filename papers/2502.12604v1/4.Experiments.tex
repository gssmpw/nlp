\section{Experiments}\label{sc4}

\subsection{Datasets and Evaluation Metrics}
To test the effectiveness of the proposed method, experiments are conducted on three RS datasets with varied data distributions and semantic annotations. The experimental datasets include CLCD \cite{liu2022cnntransformer}, SECOND (binary) \cite{yang2022asymmetric} and Levir \cite{chen2020spatialtemporal}. In addition, MMCD experiments are conducted on the Wuhan dataset \cite{zhang2022domain}. Table \ref{Table.Datasets} presents an overview of each dataset. Since the experimental methods are unsupervised, we do not use any labels within the training set. Notably, the Levir dataset focuses solely on building changes, while CLCD, SECOND and Wuhan encompass various change types. Its larger spatial size and sparse change instances make it more challenging in the context of UCD. 

\begin{table}[ht]
    \centering
    \caption{Summary of the main characteristics of the experimental datasets.} \label{Table.Datasets}
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
        \multirow{2}*{Datasets} & \multirow{2}*{Platform} & \multirow{2}*{Resolution} & Image & Dataset & Change \\
        & & & Size & Size & Type \\
        \hline
        CLCD & satellite & 0.5-2m & 512×512 & 600 & agricultural \\
        SECOND & aerial & 0.5-3m & 512×512 & 4,662 & land cover \\
        Levir & satellite & 0.5m & 1024×1024 & 637 & building \\
        \hline
        \multirow{2}*{Wuhan} & satellite & vis: 10m & \multirow{2}*{256×256} & \multirow{2}*{600} & \multirow{2}*{land cover} \\
        & (RGB\&SAR) & sar: 3m & & & \\
    \bottomrule
    \end{tabular}}
\end{table}

We adopt the most commonly used accuracy metrics in CD \cite{ding2024samcd, chen2023exchange} and binary segmentation tasks \cite{ding2021adversarial}, including Overall accuracy (\textit{OA}), Precision (\textit{Pre}), Recall (\textit{Rec}), and $F_1$ score. In these metrics, \textit{Pre} indicates the proportion of true positives among classified positives, while \textit{Rec} is the measure of identifying true positives. $F_1$ is the harmonic mean of \textit{Pre} and \textit{Rec}, therefore is more comprehensive in assessing the accuracy.

\begin{comment}
i) CLCD dataset. This is a data set for monitoring agricultural land change. It consists of 600 pairs of satellite images, with 320 pairs used for training, 120 pairs for validation, and 120 pairs for testing. The bi-temporal images were collected by the Gaofen-2 satellite in 2017 and 2019, with a spatial resolution of 0.5 to 2 meters. Each set of samples comprises two 512×512 pixels and a corresponding binary change label. The main types of annotated change objects include buildings, roads, lakes, and bare land.

ii) SECOND dataset. This is a semantic CD dataset that comprises 4,662 pairs of aerial images collected from multiple platforms and sensors. The image pairs are distributed across various cities such as Hangzhou, Chengdu, and Shanghai. Each image has 512 x 512 pixels, with a spatial resolution of 0.5 to 2 meters. 6 primary land cover changes are annotated, including bare land, trees, low vegetation, water, buildings, and playgrounds.     
\end{comment}

\begin{table}[t]
\centering
    \caption{Quantitative results of ablation study (tested on CLCD).}
    \resizebox{1\linewidth}{!}{%
        \begin{tabular}{l|c|cccc}
        \toprule
            \multirow{2}*{Methods} & \multirow{2}*{Backbone} & \multicolumn{4}{c}{Accuracy (\%)}\\
            \cline{3-6}
            & & $OA$ & $Pre$ & $Rec$ & $F_1$ \\
            \hline
            effi.SAM + CVA & effi.SAM (vit-t) & 61.24 & 13.90 & 81.01 & 23.73 \\
            effi.SAM + SC & effi.SAM (vit-t) & 92.23 & 45.76 & 24.07 & 31.55 \\
            \hline
            S2C (CSC only) & effi.SAM (vit-t) & 90.93 & 36.55 & 29.81 & 32.84 \\
            S2C (CTC only) & effi.SAM (vit-t) & 85.98 & 28.25 & 57.40 & 37.86 \\
            S2C (CSC + $\mathcal{L}_{spa}$) & effi.SAM (vit-t) & 89.71 & 33.35 & 38.33 & 35.67\\ 
            S2C (CTC + $\mathcal{L}_{spa}$) & effi.SAM (vit-t) & 91.06 & 39.52 & 38.03 & 38.76 \\
            S2C (CSC + CTC) & effi.SAM (vit-t) & 87.35 & 31.01 & 57.12 & 40.19 \\
            S2C & effi.SAM (vit-t) & 90.57 & 39.04 & 47.65 & 42.92 \\
            S2C & effi.SAM (vit-s) & 89.58 & 37.51 & \textbf{60.21} & 46.22  \\
            S2C + \textit{IoU refine} & effi.SAM (vit-s) & \textbf{91.47} & \textbf{43.85} & 52.28 & \textbf{47.69} \\
            \hline
            S2C (w/o. VFM) & ResNet18 & 87.75 & 26.80 & 37.34 & 31.21 \\
            S2C (w/o. VFM) & ResNet34 & 86.28 & 28.41 & 55.56 & 37.60 \\
            S2C & fastSAM & 86.65 & 24.71 & 38.81 & 17.78 \\
            S2C & SAM (vit-b) & 85.41 & 22.23 & 38.44 & 28.17  \\
            S2C & effi.SAM (vit-t) & 90.57 & 39.04 & 47.65 & 42.92  \\
            S2C & effi.SAM (vit-s) & 89.58 & 37.51 & 60.21 & 46.22  \\
            S2C & Dino-v2 (vit-b) & 93.95 & 59.04 & \textbf{61.24} & 60.12 \\
            S2C + \textit{IoU refine} & Dino-v2 (vit-b) & \textbf{94.46} & \textbf{63.82} & 59.12 & \textbf{61.38} \\
        \bottomrule
        \end{tabular} \label{Table.Ablation} }
\end{table}

\subsection{Implementation Details}\label{sc4.implement}
The training of S2C is performed using cropped images of $512 \times 512$ pixels over 20 epochs. The trained weights with the highest accuracy on the validation set are saved for subsequent evaluation on the test set. The training batch size depends on the backbone to fit in GPU memory, usually exceeding 12 in our implementation. The learning rate $lr$ is initially set to 0.01, and is updated at each iteration as: $0.01*(1-iterations/total\_iterations)^{1.5}$. The optimization algorithm is the Stochastic Gradient Descent with Nesterov momentum. The weighting parameters in eq.\ref{eq.losses} are set to $\alpha=0.2, \beta=1$, while $\alpha$ can be adjusted across datasets to balance $\mathcal{L}_{tri}$ and $\mathcal{L}_{info}$ in training dynamics. An sensitivity analysis on these paramenters are provided in Sec.\ref{sc.ablation}. , The sparsity threshold $T$ in Eq.(\ref{eq.sparse_loss}) is set according to the change sparsity in different datasets: $T=0.2$ for the CLCD and Wuhan datasets, $T=0.2$ for the Levir dataset with very sparse changes, and $T=0.4$ for the SECOND dataset with more change instances.

Apart from \textit{strong augmentations} as detailed in Sec.\ref{sc3.CL}, random flipping operations are also introduced as \textit{weak augmentations}. These operations are executed on each of the two temporal images to enhance sample diversity, rather than induce spatial or spectral perturbations.

%For more details, readers are encouraged to visit our codes released at: \url{https://github.com/ggsDing/SCanNet}.

\subsection{Ablation Study}\label{sc.ablation}

\textbf{Quantitative Evaluation.} An ablation study is conducted through cumulative integration of the proposed methodologies, including the CTC, CSC, grid sparse loss ($\mathcal{L}_{spa}$), and IoU matching and refinement (\textit{IoU refine}). Given that the proposed method employs both VFM and CL, an intuitive strategy is to combine these two techniques as a baseline approach. However, VFM alone is not capable of UCD, and there is no existing literature approach (to the best of our knowledge) that integrates CL with VFM for CD. Therefore, we implement these two approaches as the baseline: i) applying CVA and clustering on the VFM-encoded semantic features for CD following the practice in \cite{zheng2024segment}, and ii) conducting CS-based CL using the VFM features. Due to computational constraints, we employ \textit{efficient-SAM}(vit-t) \cite{xiong2024efficientsam} as a frozen encoder ($\theta$) in the initial baseline, which is an efficient variant of SAM \cite{Kirillov2023Segment}.

The quantitative results are presented in Table \ref{Table.Ablation}. As indicated by the results of \textit{eff.SAM + CVA}, direct change analysis on VFM features leads to suboptimal accuracy. By contrast, the foundational method that integrates VFM with SC-based CL demonstrates significant accuracy. The proposed CTC and CSC further surpass the conventional SC-based CL paradigm. It is worth noting that the CTC alone outperforms both SC and CSC by a large margin, establishing it as the leading single CL paradigm for CD. Meanwhile, the proposed grid sparsity regularization also exhibits notable effectiveness. Its addition to each CL paradigm results in an average enhancement of $~2\%$ in $F_1$. Adding CSC improves the robustness of CTC against temporal noise, leading to an increase of over $3\%$ in $F_1$. The refining algorithm substantially enhances the precision of the results, providing an increase of $1.47\%$ in $F_1$ and approximately $2\%$ in $OA$.

\begin{figure}[t]
\centering
    \includegraphics[width=0.8\linewidth]{ablation/param_acc.png}
    \caption{$F_1$ (\%) obtained by S2C with different weighting parameters.}
\label{fig.param_acc}
\end{figure}

\textbf{S2C with different backbones.} While S2C is introduced as a methodology that integrates CL and VFM, the core technique employed is a UCD framework, which is adaptable to other types of DNNs. Table \ref{Table.Ablation} also presents an evaluation of S2C utilizing various different backbones, including a vanilla ResNet \cite{he2016resnet} and several other VFMs. Surprisingly, the implementation of S2C with a simple ResNet34 backbone still yields considerably high accuracy. This further confirms its efficacy as a general framework for UCD.

Compared to using conventional DNNs, employing VFMs as backbone greatly improves the $Pre$ of S2C. This can be attributed to the rich semantic contexts inherent to VFMs, which significantly facilitate the discrimination of semantic changes. The implemented VFMs include SAM \cite{Kirillov2023Segment}, fastSAM \cite{zhao2023fast}, efficient SAM \cite{xiong2024efficientsam} and Dino-v2\cite{oquab2024dinov2}. Among SAM and its variants, efficient SAM obtains the highest accuracy. Its parameter size is considerably reduced compared to the original SAM, thereby improving its convergence in the context of UCD. Employing Dino-v2 results in the highest accuracy, with an advantage of approximately $12\%$ compared to the other backbones. Therefore, Dino-v2 is selected as the backbone in the subsequent experiments.

\begin{figure*}[t]
\centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{>{\centering\arraybackslash}m{0.4cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}>{\centering\arraybackslash}m{1.8cm}}
        (a)&
        \includegraphics[width=1.8cm]{ablation/00576_imgA.png} &
        \includegraphics[width=1.8cm]{ablation/00576_imgB.png} &
        \includegraphics[width=1.8cm]{ablation/00576_label.png} &
        \includegraphics[width=1.8cm]{ablation/00576_SC.png} &
        \includegraphics[width=1.8cm]{ablation/00576_infoNCE_only.png} &
        \includegraphics[width=1.8cm]{ablation/00576_triplet_only.png} &
        \includegraphics[width=1.8cm]{ablation/00576_noSparse.png}&
        \includegraphics[width=1.8cm]{ablation/00576_S2C.png}&
        \includegraphics[width=1.8cm]{ablation/00576_S2C_refined.png}\\
        (b)&
        \includegraphics[width=1.8cm]{ablation/00569_imgA.png} &
        \includegraphics[width=1.8cm]{ablation/00569_imgB.png} &
        \includegraphics[width=1.8cm]{ablation/00569_label.png} &
        \includegraphics[width=1.8cm]{ablation/00569_SC.png} &
        \includegraphics[width=1.8cm]{ablation/00569_infoNCEonly.png} &
        \includegraphics[width=1.8cm]{ablation/00569_triplet_only.png} &
        \includegraphics[width=1.8cm]{ablation/00569_noSparse.png}&
        \includegraphics[width=1.8cm]{ablation/00569_S2C.png}&
        \includegraphics[width=1.8cm]{ablation/00569_S2C_refined.png}\\
        (c)&
        \includegraphics[width=1.8cm]{ablation/09555_imgA.png} &
        \includegraphics[width=1.8cm]{ablation/09555_imgB.png} &
        \includegraphics[width=1.8cm]{ablation/09555_GT.png} &
        \includegraphics[width=1.8cm]{ablation/09555_SC.png} &
        \includegraphics[width=1.8cm]{ablation/09555_infoNCE_only.png} &
        \includegraphics[width=1.8cm]{ablation/09555_triplet_only.png} &
        \includegraphics[width=1.8cm]{ablation/09555_no_sparse.png}&
        \includegraphics[width=1.8cm]{ablation/09555_S2C.png}&
        \includegraphics[width=1.8cm]{ablation/09555_refined.png}\\
        (d)&
        \includegraphics[width=1.8cm]{ablation/Levir_74_A.png} &
        \includegraphics[width=1.8cm]{ablation/Levir_74_B.png} &
        \includegraphics[width=1.8cm]{ablation/levir_74_label.png} &
        \includegraphics[width=1.8cm]{ablation/Levir_74_CLSC.png} &
        \includegraphics[width=1.8cm]{ablation/Levir_74_infoNCE_only.png} &
        \includegraphics[width=1.8cm]{ablation/Levir_74_triplet_only.png} &
        \includegraphics[width=1.8cm]{ablation/Levir_74_no_sparse.png}&
        \includegraphics[width=1.8cm]{ablation/Levir_74_S2C.png}&
        \includegraphics[width=1.8cm]{ablation/Levir_74_S2C_refine.png}\\
        & $T_1$ image & $T_2$ image & GT & SC (baseline) & S2C {\small(CSC only)} & S2C {\small(CTC only)} & S2C {\small(CSC+CTC)} & S2C & S2C {\small(Refined)}\\
    \end{tabular}
    \caption{Example of results from different methods in the ablation study. CTC highlights differences while introducing much noise, CSC enhances change representations and reduces certain false alarms, while $\mathcal{L}_{spa}$ further suppresses insignificant changes.}
    \label{Fig.ablation_vis}
\end{figure*}

\begin{comment}
    \begin{table}[t]
\centering
    \caption{Results obtained with different weighting parameters.}
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{c | cccc | ccc }
    \toprule
    \multirow{2}*{} & \multicolumn{4}{c|}{$\beta=1$} & \multicolumn{3}{c}{$\alpha=0.2$}  \\
    \cline{2-8}
    & $\alpha=0.1$ & $\alpha=0.2$ & $\alpha=0.5$ & $\alpha=1.0$ & $\beta=1$ & $\beta=1$ & $\beta=3$ \\
    \hline
    $F_1 (\%)$ &   \\
    \bottomrule
    \end{tabular}}
    \label{Table.Params}
\end{table}
\end{comment}

\textbf{Parameter Analysis.} In Eq.\ref{eq.losses} two weighting parameters $\alpha$ and $\beta$ are introduced to balance the different training objectives. The optimal weighting parameters are determined by a sensitivity analysis. Considering that $\mathcal{L}_{info}$ poses considerable optimization challenges and commonly exhibits high values, $\alpha$ is assigned small values within the range of $[0, 1]$. In contrast, $\mathcal{L}_{spa}$ is straightforward to optimize and has relatively low values, thus $\beta$ is assigned with values over 1. Fig.\ref{fig.param_acc} reports the accuracy in $F_1$ obtained with different values of $\alpha$ and $\beta$. To mitigate random variability, the reported accuracy is the mean value across three trials. The results show a strong correlation between $\alpha$ and accuracy, whereas the effect of $\beta$ appears inconsistent. This finding aligns with the underlying learning mechanism within S2C, where $\alpha$ balances between $\mathcal{L}_{tri}$ and $\mathcal{L}_{info}$, affecting the focus on mapping differences or similarities. The allocation of $\alpha =0.2, \beta =2$ yields the highest observed accuracy.

\begin{figure}[t]
\centering
    \includegraphics[width=1\linewidth]{ablation/acc_curve.png}
    \caption{Average $F_1$ (\%) of S2C versus sample volume.}
\label{fig.acc_curve}
\end{figure}

\begin{table}[t]
    \centering
    \caption{Average $F_1$ (\%) of S2C obtained with varying amount of unlabeled samples.}
    \resizebox{1\linewidth}{!}{%
        \begin{tabular}{r|cccccc}
        \toprule
            \multirow{2}*{Dataset} & \multicolumn{6}{c}{Number of samples}\\
            \cline{2-7}
            & 5 & 10 & 50 & 100 & 200 & 300 \\
            \hline
            CLCD & 53.39\textcolor{gray}{$\pm$4.9} & 53.16\textcolor{gray}{$\pm$3.5} & 57.01\textcolor{gray}{$\pm$1.2} & 57.42\textcolor{gray}{$\pm$1.4} & 57.64\textcolor{gray}{$\pm$2.2} & 58.36\textcolor{gray}{$\pm$1.1} \\
            SECOND & 47.15\textcolor{gray}{$\pm$3.5} & 45.46\textcolor{gray}{$\pm$0.9} & 46.94\textcolor{gray}{$\pm$1.0} & 47.15\textcolor{gray}{$\pm$0.4} & 48.62\textcolor{gray}{$\pm$1.2} & 48.49\textcolor{gray}{$\pm$1.5} \\
            Levir & 35.98\textcolor{gray}{$\pm$3.1} & 35.74\textcolor{gray}{$\pm$0.7} & 38.79\textcolor{gray}{$\pm$0.7} & 40.41\textcolor{gray}{$\pm$2.1} & 40.63\textcolor{gray}{$\pm$0.1} & 41.17\textcolor{gray}{$\pm$1.5} \\
        \bottomrule
        \end{tabular}
        }\label{Table.FewSample}
\end{table}


\textbf{Qualitative Assessment.} Fig.\ref{Fig.ablation_vis} presents some examples of the CD results obtained using the different techniques. These results are selected in the 3 datasets covering different scenes, including (a) cropland, (b) countryside (c) factories, and (d) residential blocks. One can observe that using either the CSC or CTC alone leads to much noise, while their collaborative employment greatly reduces the false alarms. The grid sparsity generalization further removes much insignificant change representation and leads to more smoothed CD results. After the post-processing of \textit{IoU refinement}, binary CD results matching the object boundaries are produced.

\textbf{Training S2C with Few Samples.} An additional advantage of incorporating consistency regularization in contrastive learning is the enhancement of generalization ~\cite{yang2023revisiting, bandara2022revisiting}. Throughout the experimental process, we observe that the accuracy of the S2C framework is not sensitive to the number of training samples (unlabeled). Consequently, additional experiments are conducted to investigate the requisite quantity of unlabeled image samples for effective training of the S2C framework.

We train the S2C model using varying amounts of unlabeled data, ranging from 300 to as few as 5 and subsequently evaluate the resulting accuracy on the entire test set. To increase the diversity of samples, additional random cropping and enlarging augmentations are added in the preprocessing operations. This is added into the \textit{weak augmentation} process as elaborated in Sec.\ref{sc4.implement}. This operation is crucial for deploying the S2C with very few samples: when there are insufficient samples to enable spatial comparisons within the CSC paradigm, cropped sub-regions can be alternatives. To mitigate variations due to sample selection, the experiments are conducted over three trials, each randomly selecting the specified sample volume.

The average $F_1 (\%)$ results of the experimental trials are reported in Table \ref{Table.FewSample}. Compared with the results obtained using 300 training samples, utilizing only 100 samples yields an almost negligible average $F_1$ decrease of 1\% across the three datasets. Employing a minimal set of merely 5 samples results in an average $F_1$ decrease of about 3.8\%. This low requirement on training samples suggests that the S2C framework can be easily deployed in real-world applications. To intuitively illustrate the results, a graph of accuracy versus sample volume is presented in Fig.\ref{fig.acc_curve}. By analyzing the figure, one can infer that on the considered datasets, employing a minimum of 5 samples is sufficient for the effective training of the S2C, while utilizing 100 samples is adequate for attaining near-optimal accuracy.


\subsection{Comparative Experiments}

\begin{table*}[t]
    \centering
    \caption{Quantitative accuracy (\%) evaluation of the proposed S2C and SOTA UCD methods on various benchmark datasets.}
    \resizebox{1\linewidth}{!}{%
        \begin{tabular}{r|c|cccc|cccc|cccc}
        \toprule
            \multirow{2}*{Methods} & \multirow{2}*{Publication} & \multicolumn{4}{c|}{CLCD} & \multicolumn{4}{c|}{SECOND} & \multicolumn{4}{c}{Levir} \\
            \cline{3-14}
            & & $OA$ & $Pre$ & $Rec$ & $F_1$ & $OA$ & $Pre$ & $Rec$ & $F_1$ & $OA$ & $Pre$ & $Rec$ & $F_1$ \\
            \hline
            \color{gray} SAM-CD &\color{gray} \textit{TGRS 2024} \cite{ding2024samcd} & \color{gray}96.26 & \color{gray}73.01 & \color{gray}78.84 & \color{gray}75.81 & \color{gray}88.56 & \color{gray}73.32 & \color{gray}66.00 & \color{gray}69.47 & \color{gray}99.17 & \color{gray}92.62 & \color{gray}91.04 & \color{gray}91.82\\
            \hline
            CVA & \textit{TGRS 2000} \cite{Bruzzone2000diff} & 71.01 & 8.49 & 29.62 & 13.20 & 59.17 & 20.55 & 37.34 & 26.51 & 66.50 & 5.80 & 36.59 & 10.02 \\
            %IRMAD \cite{Nielsen2007IRMAD} & 74.64 & 8.85 & 25.89 & 13.19 & 60.80 & 32.43 & 19.82 & 24.60 & 70.13 & 6.02 & 33.27 & 10.19\\
            ISFA & \textit{TGRS 2014} \cite{Wu2014Slow} & 74.37 & 8.60 & 25.39 & 12.85 & 60.23 & 20.13 & 34.24 & 25.35 & 69.32 & 6.03 & 34.45 & 10.27\\
            DCVA & \textit{TGRS 2019} \cite{saha2019unsupervised} & 53.91 & 11.35 & \textbf{76.26} & 19.76 & 55.51 & 25.63 & \underline{66.07} & 36.93 & 48.28 & 7.20 & 76.94 & 13.16\\
            DSFA & \textit{TGRS 2019} \cite{Du2019Unsupervised} & 52.09 & 8.53 & 55.94 & 14.80 & 48.10 & 19.06 & 50.28 & 27.65 & 60.29 & 5.99 & 46.22 & 10.60\\
            KPCA & \textit{TCYB 2022} \cite{wu2021unsupervised} & 53.47 & 13.84 & 44.52 & 21.11 & 54.69 & 20.44 &  44.87 & 28.09 & 54.97 & 5.61 & 49.52 & 10.08\\
            CDRL & \textit{CVPR 2022} \cite{noh2022unsupervised} & 64.74 & 7.75 & 34.27 & 12.64 & 62.65 & 37.78 & 22.90 & 28.51 & 62.59 & 5.32 & 37.74 & 9.32\\
            SiROC & \textit{TGRS 2022} \cite{Kondmann2022Spatial} & 82.38 & 18.93 & 41.65 & 26.03 & 69.80 & 27.92 & 33.59 & 30.49 & 64.82 & 7.38 & 51.14 & 12.90 \\
            I3PE & \textit{ISPRS 2023} \cite{chen2023exchange} & 91.12 & 32.42 & 17.79 & 22.97 & 74.09 & 34.53 & 35.03 & 34.78 & \underline{91.03} & 17.39 & 20.31 & 18.74\\
            FCD-GAN & \textit{TPAMI 2023} \cite{wu2023fully} & 83.93 & 22.06 & 45.79 & 29.77 & 68.36 & 29.47 & 43.41 & 35.11 & 83.42 & 8.87 & 24.29 & 12.99 \\
            AnyChange & \textit{NeurIPS 2024} \cite{zheng2024segment} & - & - & - & - & - & 30.5 & \textbf{83.2} & 44.6 & - & 13.3 & \textbf{85.0} & 23.0 \\
            \hline    
            \multicolumn{2}{r|}{S2C-Dinov2 (proposed)} & \underline{93.95} & \underline{59.04} & \underline{61.24} & \underline{60.12} & \textbf{84.55} & \textbf{67.15} & 42.41 & \underline{51.99} & 89.28 & \underline{29.42} & \underline{78.88} & \underline{42.86} \\ 
            \multicolumn{2}{r|}{S2C + \textit{IoU Refine} (proposed)} & \textbf{94.46} & \textbf{63.82} & 59.12 & \textbf{61.38} & \underline{84.45} & \underline{64.91} & 46.02 & \textbf{53.86} & \textbf{92.84} & \textbf{34.85} & 70.69 & \textbf{46.69} \\ 
        \bottomrule
        \end{tabular}
        }\label{Table.CompareSOTA}
\end{table*}

\begin{figure*}[t]
\centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{>{\centering\arraybackslash}m{0.4cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.5cm}}
        & & & & \multicolumn{7}{c}{\includegraphics[width=10cm]{Vis_SOTA/ColorBar.png}}\\
        (a)&
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00498_S2C_refined.png}\\
        (b)&
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00572_S2C_refined.png}\\
        (c)&
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/00581_S2C_refined.png}\\
        (d)&
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/04036_S2C_refined.png}\\
        (e)&
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/02803_S2C_refined.png}\\
        (f)&
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/05829_S2C_refined.png}\\
        (g)&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_14_S2C_refined.png}\\
        (h)&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_84_S2C_refined.png}\\
        (i)&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_im1.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_im2.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_GT.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_DCVA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_DSFA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_KPCA.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_CDRL.png} &
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_SiROC.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_FCDGAN.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_I3PE.png}&
        \includegraphics[width=1.5cm]{Vis_SOTA/test_34_S2C_refined.png}\\
        & $T_1$ image & $T_2$ image & GT & DCVA & DSFA & KPCA & CDRL & SiROC & FCD-GAN & I3PE & S2C {\small(Refined)}\\
    \end{tabular}
    \caption{Qualitative comparison between the proposed S2C framework and SOTA methods for UCD. The samples are selected from (a)-(c) CLCD, (d)-(f) SECOND, and (g)-(i) Levir datasets.}
    \label{Fig.vis_SOTA}
\end{figure*}

We conduct comparative experiments with various SOTA methods for UCD in RS. The compared methods include several non-parameter methods based on difference analysis, including the CVA-based methods \cite{Bruzzone2000diff}, ISFA \cite{Wu2014Slow}, DCVA \cite{saha2019unsupervised}, DSFA \cite{Du2019Unsupervised}, KPCA \cite{wu2021unsupervised} and SiROC \cite{Kondmann2022Spatial}. In addition, we also compare generative methods CDRL \cite{noh2022unsupervised} and FCD-GAN\cite{wu2023fully}, an augmentation-based method I3PE \cite{chen2023exchange} and a very recent VFM-based method AnyChange.\cite{zheng2024segment}. To facilitate a comprehensive assessment of accuracy, SAM-CD\cite{ding2024samcd} is also included in the comparison, representing the SOTA accuracy of supervised CD.

Table.\ref{Table.CompareSOTA} presents the quantitative results of this comparative study. Among the difference analysis-based methods, DCVA \cite{saha2019unsupervised} exhibits a notable advantage in $Rec$. It obtains the highest $Rec$ on CLCD and the second one on SECOND and Levir datasets. Among the methods presented in recent literature, I3PE achieves high $Pre$ across the three datasets. FCD-GAN \cite{wu2023fully} obtains superior $F_1$ on the CLCD dataset, and also achieves high $F_1$ on the SECOND dataset. AnyChange \cite{zheng2024segment}, a training-free approach leveraging VFM for CD, demonstrates satisfactory accuracy by achieving the highest $Rec$ on two datasets. However, it obtains a relatively low $Pre$, suggesting a considerable percentage of false alarms present in the results.

The proposed S2C yields substantial and consistent improvements in accuracy compared to the SOTA methods. The coarse prediction of S2C results in a sharp improvement of more than $30\%$ on the CLCD dataset, and approaching $20\%$ on the Levir dataset. Subsequent post-processing utilizing the IoU refinement further enhances its balance between $Pre$ and $Rec$. Notably, this refinement algorithm brings the greatest enhancement on the Levir dataset, where the coarse prediction is relatively blurred and has low $Pre$. The final results demonstrate an improvement of nearly $4\%$ in $F_1$ on the Levir dataset, which is recognized as a challenging benchmark for UCD algorithms given its building-focused annotation and the small number of changes. The improvements of the proposed S2C over the SOTA methods, after refinement, are quantified as $31\%$, $9\%$, and $23\%$ in $F_1$ for the respective datasets. The achieved level of accuracy significantly reduces the gap with fully-supervised methods, with observed reductions of $14\%$ and $16\%$ in $F_1$ on the CLCD and SECOND datasets, respectively. However, the Levir dataset still exhibits a large gap in $F_1$ between supervised and unsupervised methods, owing to its sparsity of change instances and building-focused annotations.

Fig.\ref{Fig.vis_SOTA} presents a qualitative analysis of the state-of-the-art (SOTA) methods. One can observe that most of the literature methods generate numerous false alarms due to the impact of temporal noise, such as spectral variations in Fig.\ref{Fig.vis_SOTA}(b)(c)(e)(g)(h), spatial misalignment in Fig.\ref{Fig.vis_SOTA}(d)(f), and radiometric insignificant changes in Fig.\ref{Fig.vis_SOTA}(a)(i). The results of FCD-GAN and I3PE have fewer false alarms but exhibit substantial limitations in comprehensively segmenting the major changes. In contrast, the proposed S2C demonstrates a marked reduction in errors. Its results effectively cover the majority of significant changes, although it still contains few false alarms.

\subsection{Unsupervised MMCD with S2C}

\begin{table}[t]
\centering
    \caption{Accuracy obtained by the proposed S2C with different encoders (Wuhan dataset).}
    \resizebox{1\linewidth}{!}{%
        \begin{tabular}{c|cc|c}
        \toprule
            Methods & $RGB$ Backbone & $SAR$ Backbone & $F_1$ (\%)\\
            \hline
            effi.SAM+SC & effi.SAM (vit-t) & effi.SAM (vit-t) & 9.76 \\
            effi.SAM+SC & Dino-v2 (vit-b) & Dino-v2 (vit-b) & 15.91 \\
            \hline
            S2C & effi.SAM (vit-t) & effi.SAM (vit-t) & 23.23 \\
            (dual VFMs) & Dino-v2 (vit-b) & Dino-v2 (vit-b) & 27.45 \\
            \hline
            \multirow{4}*{} & effi.SAM (vit-t) & ResNet18 & 32.04 \\
            S2C & effi.SAM (vit-s) & ResNet18 & 34.75 \\
            (single VFM)  & Dino-v2 (vit-b) & ResNet18 & 35.86 \\
             & fastSAM & ResNet34 & 38.26 \\
            \hline
            S2C & ResNet34 & ResNet34 & 41.28 \\
            (w/o. VFMs) & ResNet18 & ResNet18 & \textbf{41.96} \\
        \bottomrule
        \end{tabular} \label{Table.Het_Ablation} }
\end{table}

\begin{table}[t]
    \centering
    \caption{Quantitative evaluation of accuracy (\%) provided by SOTA unsupervised MMCD methods (Wuhan dataset).}
    \resizebox{1\linewidth}{!}{%
        \begin{tabular}{c|c|c|cccc}
        \toprule
            \multicolumn{2}{c|}{\multirow{2}*{Methods}} & \multirow{2}*{Reference} & \multicolumn{4}{c}{Accuracy}  \\
            \cline{4-7}
            \multicolumn{2}{c|}{} & &  $OA$ & $Pre$ & $Rec$ & $F_1$ \\  
            \hline
            \multirow{9}*{\rotatebox{90}{Homogeneous UCD}} & CVA & \textit{TGRS 2000} \cite{Bruzzone2000diff} & 59.81 & 13.91 & 36.12 & 20.09 \\
            %IRMAD \cite{Nielsen2007IRMAD} & 64.00 & 14.40 & 31.85 & 19.83\\
            & ISFA & \textit{TGRS 2014} \cite{Wu2014Slow} & 62.93 & 13.97 & 32.01 & 19.46\\
            & DCVA & \textit{TGRS 2019} \cite{saha2019unsupervised} & 47.10 & 14.29 & \underline{55.66} & 22.74\\
            & DSFA & \textit{TGRS 2019} \cite{Du2019Unsupervised} & 51.27 & 14.16 & 49.08 & 21.98\\
            & KPCA & \textit{TCYB 2021} \cite{wu2021unsupervised} & 53.47 & 13.84 & 44.52 & 21.11\\
            & CDRL & \textit{CVPR 2022} \cite{noh2022unsupervised} & 60.12 & 13.45 & 34.07 & 19.29\\
            & SiROC & \textit{TGRS 2022} \cite{Kondmann2022Spatial} & 70.09 & 15.05 & 24.52 & 18.65 \\
            & I3PE & \textit{ISPRS 2023} \cite{chen2023exchange} & 70.45 & 16.06 & 26.34 & 19.95\\
            & FCD-GAN & \textit{TPAMI 2023} \cite{wu2023fully} & \textbf{85.33} & 15.06 & 1.05 & 1.97\\
            \hline
            \multirow{4}*{\rotatebox{90}{MMCD}} & SR-GCAE & \textit{TGRS 2022} \cite{Chen2022Unsupervised} & 18.54 & 13.94 & \textbf{90.64} & 24.16\\
            & AGSCC & \textit{TNNLS 2022} \cite{Sun2022AGSCC}  & 75.48 & 10.31 & 9.79 & 10.04 \\
            & CAAE & \textit{TNNLS 2024} \cite{Luppino2024CAAE} & 75.43 & 13.71 & 14.30 & 14.00\\
            & LPEM & \textit{TNNLS 2024} \cite{sun2024LPEM} & 68.19 & \underline{19.70} & 41.43 & \underline{26.70} \\
            \hline    
            \multicolumn{2}{c|}{ S2C (w/o. VFMs) }& proposed & \underline{83.85} & \textbf{42.18} & 41.74 & \textbf{41.96} \\ 
        \bottomrule
        \end{tabular}
        }\label{Table.Compare_Het}
\end{table}

In this section, we present the experimental performance of the S2C for unsupervised MMCD. First, we conduct an ablation study to examine the efficacy of different feature encoders applied to the visible and SAR images. The results are presented in Table\ref{Table.Het_Ablation}. Due to substantial heterogeneity of SAR images relative to their training domains, employing two VFMs as feature extractors results in low accuracy. Consequently, we proceed to utilize a basic ResNet as $f_\zeta$, i.e., feature extractor for the SAR branch. Under this experimental setting, fastSAM outperforms Dino-v2 and effi.SAM as a more effective feature extractor for the optical branch. This may due to the low spatial resolution of the optical images in this dataset, where CNN-based backbones can better retain the spatial details. We proceed to examine the application of S2C without using any VFM. Surprisingly, training simple ResNet18 backbones as $f_\theta$ and $f_\zeta$ leads to the highest accuracy. This indicates that VFMs are also less effective in visible images, as this dataset has lower spatial resolution compared to the optical datasets in Table \ref{Table.Datasets}. Theoretically, utilizing RS FMs with effective generalization to both SAR and optical data could potentially enhance the accuracy of S2C. However, this hypothesis remains unexplored due to the absence of such effective FMs, particularly in the case of this specific dataset. Nonetheless, these experimental results affirm the efficacy of the S2C framework for UCD, despite the absence of the VFMs.

We further compare the accuracy of the S2C (with dual ResNet18 encoders) with the SOTA methods in Table \ref{Table.Het_Ablation}. The compared methods include not only those homogeneous UCD methods in Table \ref{Table.CompareSOTA}, but also several SOTA unsupervised MMCD methods with available implementations. The compared MMCD methods include two methods based on graph analysis: AGSCC \cite{Sun2022AGSCC} and LPEM \cite{sun2024LPEM}, a graph convolution-based method: SR-GCAE \cite{Chen2022Unsupervised} and a generative transcoding method: CAAE \cite{Luppino2024CAAE}. 

The results indicate that homogeneous UCD methods, when applied to multimodal data, typically demonstrate reduced efficacy, with accuracy levels falling below 23\% in $F_1$. Most of the compared MMCD methods exhibit sensitivity to hyper-parameters, and fail to generalize effectively on the Wuhan heterogeneous dataset when with a substantial number of testing samples. Among these methods, the LPEM achieves the highest accuracy as indicated in $Pre$ and $F_1$. In contrast, the proposed S2C effectively identifies multimodal changes, surpassing literature methods by a margin exceeding 15\% in $F_1$. Fig.\ref{Fig.vis_SOTA_Het} present several samples of the MMCD results. One can observe that the SR-GCAE and LPEM produce considerable number of false alarms. These observations align with their evaluation metrics in Table \ref{Table.Het_Ablation} as indicated by high $Rec$ over $Pre$. In contrast, the CAAE identifies only limited areas of the changes. The proposed S2C method, although still exhibits ambiguity in delineating the object boundaries, effectively detects most of the multimodal changes.


\begin{figure}[t]
\centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1.2cm}}
        \multicolumn{7}{c}{\includegraphics[width=8cm]{Vis_SOTA/ColorBar.png}}\\
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_rgb.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_sar.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_GT.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_SRGCAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_CAAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_LPEM.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1118_S2C.png}\\
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_rgb.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_sar.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_GT.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_SRGCAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_CAAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_LPEM.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1915_S2C.png}\\
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_rgb.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_sar.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_GT.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_SRGCAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_CAAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_LPEM.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_2094_S2C.png}\\
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_rgb.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_sar.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_GT.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_SRGCAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_CAAE.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_LPEM.png} &
        \includegraphics[width=1.2cm]{Vis_SOTA/Het_1976_S2C.png}\\
        $T_1$ & $T_2$ & \multirow{2}*{GT} & SR- & \multirow{2}*{CAAE} & \multirow{2}*{LPEM} & S2C \\
        image & image & & GCAE & & & (proposed)
    \end{tabular}
    \caption{Qualitative comparison between the proposed S2C framework and SOTA methods for unsupervised MMCD.}
    \label{Fig.vis_SOTA_Het}
\end{figure}