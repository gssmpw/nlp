Our team has extensive experience in the field of recommender systems, demonstrated by numerous publications in prestigious conferences such as WWW~\cite{lu2023differentiable, han2024efficient, wan2022cross}. 
We have also organized tutorials and workshops, such as the tutorial at RecSys'23~\cite{liu2023user} and the workshops at RecSys'23\footnote{\url{https://dlp4rec.github.io/}} and CIKM'23\footnote{\url{https://rgm-cikm23.github.io/}}. Notably, our recent work~\cite{yin2024dataset} was recognized with the KDD Best Student Paper Award for its innovative data-centric approach to enhancing sequential recommender systems. Additionally, our survey~\cite{wu2024survey} is the first comprehensive review of the integration of large language models (LLMs) in recommendation systems. Our technical report~\cite{guo2024scaling} offers pioneering insights into generative large recommendation models. We also explore the theoretical aspects of large recommendation models, examining the scaling laws between data compression rate, quality, and model performance~\cite{shen2024predictive}.
Our profound expertise and pioneering achievements equip us to deliver a thorough and insightful presentation on these cutting-edge developments~\cite{zhang2024learning,zhang2020context,zhang2019graph,zhang2022hierarchical,zhang2022cglb,yang2024exploring,li2024configure,yang2023lever,penglive} .

