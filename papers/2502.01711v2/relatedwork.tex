\section{Related Work}
Extensive research exists on coordination in multi-agent systems, particularly in zero-shot coordination. Methods like \citet{hu2020other, muglich2022equivariant} use Dec-POMDP symmetries to avoid incompatible policies, while \citet{hu2021off} rely on environment dynamics for grounded policies. Diversity-based approaches also leverage known symmetries and simulator access \citep{cui2023adversarial, lupu2021trajectory}. In contrast, ER symmetries can be learned from agent-environment interactions without privileged information, enabling grounded signaling and effective coordination in concurrent environments (see Sections \ref{sec:cat-dog-experiments} and \ref{sec:overcooked-v2-experiments}).

In single-agent settings, symmetry has been shown to reduce sample complexity in RL \citep{van2020mdp, zhu2022sample, nguyen2024symmetry}. In multi-agent systems, symmetries reduce policy space complexity and help agents identify equivalent strategies \citep{van2021multi, muglich2022equivariant}. However, many methods require explicit knowledge of symmetries, or rely on predefined groups \citep{abreu2023addressing, yu2024leveraging, nguyen2024symmetry}. Our work generalizes these approaches by introducing ER symmetries, which do not require prior symmetry knowledge or equivariant networks, and can be learned directly through environment interactions.

Our work relates to value-based abstraction, which groups states or observations with similar value functions. \citet{rezaei2022continuous} use lax-bisimulation to learn MDP homomorphisms, while \citet{grimm2021proper} learn a model of the underlying MDP for value-based planning. In contrast, we focus on symmetries in the policy space that preserve expected return. ER symmetries are conceptually related to $Q^*$-irrelevance abstractions \citep{li2006towards} in that both aim to preserve the optimal value function of an MDP. However, whereas $Q^*$-irrelevance abstractions reduce complexity by aggregating states, ER symmetries form a group that acts bijectively on the policy space, transforming optimal policies into other policies with the same expected return.