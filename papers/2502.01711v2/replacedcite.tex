\section{Related Work}
Extensive research exists on coordination in multi-agent systems, particularly in zero-shot coordination. Methods like ____ use Dec-POMDP symmetries to avoid incompatible policies, while ____ rely on environment dynamics for grounded policies. Diversity-based approaches also leverage known symmetries and simulator access ____. In contrast, ER symmetries can be learned from agent-environment interactions without privileged information, enabling grounded signaling and effective coordination in concurrent environments (see Sections \ref{sec:cat-dog-experiments} and \ref{sec:overcooked-v2-experiments}).

In single-agent settings, symmetry has been shown to reduce sample complexity in RL ____. In multi-agent systems, symmetries reduce policy space complexity and help agents identify equivalent strategies ____. However, many methods require explicit knowledge of symmetries, or rely on predefined groups ____. Our work generalizes these approaches by introducing ER symmetries, which do not require prior symmetry knowledge or equivariant networks, and can be learned directly through environment interactions.

Our work relates to value-based abstraction, which groups states or observations with similar value functions. ____ use lax-bisimulation to learn MDP homomorphisms, while ____ learn a model of the underlying MDP for value-based planning. In contrast, we focus on symmetries in the policy space that preserve expected return. ER symmetries are conceptually related to $Q^*$-irrelevance abstractions ____ in that both aim to preserve the optimal value function of an MDP. However, whereas $Q^*$-irrelevance abstractions reduce complexity by aggregating states, ER symmetries form a group that acts bijectively on the policy space, transforming optimal policies into other policies with the same expected return.