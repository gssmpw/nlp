\section{Model Training \& Experiments}
\label{sec:experiments}

This section describes the experimental setup, including model training and results of evaluating LeanProgress's Step Predictor: Prediction accuracy and search pass rate improvement compared to traditional best-first search via log probability.

\subsection{Language Model: Remaining Step Predictor}

Our approach uses a language model to predict the number of steps remaining to reach a \texttt{no\_goals} state (proof completion) given a current proof state. While the language model architecture can be varied, we employ a fine-tuned DeepSeek Coder 1.3B model \citep{DeepSeek-coder}. This model is trained to predict the number of remaining steps based on the current proof state and, optionally, the history of applied tactics.

The input format for our model is as follows: 

\texttt{[STATE\_BEFORE]state [STEPS\_TO\_NO\_GOALS]steps}

Here, \texttt{state} represents the current proof state, encoded as a string representing the current goals. The model is trained to generate the number of remaining steps after this prompt. This input format allows the model to focus specifically on the task of predicting the remaining steps, distinct from predicting the next tactic.

We fine-tuned the DeepSeek Coder 1.3B model on (state, remaining steps) pairs extracted from successful proof trajectories generated using BFS and the Reprover model as described in the previous subsection. The DeepSeek Coder model was chosen for its strong performance in code understanding tasks with less than 2B parameters so that personal computers can support inference locally, which we believe translates well to the task of predicting a numerical value representing the remaining steps.

The model was fine-tuned using a Mean Squared Error (MSE) loss function with the AdamW optimizer. The training was conducted batch size of 4 and a learning rate of $1e-5$. Other parameters are like betas (0.9, 0.999) with weight decay 0.01 and warmup ratio 0.03.

We experimented with other models, such as DeepSeek coder V2 or Prover V1.5, but the model size of 7B is not capable of being used on personal computers. We found that the DeepSeek Coder 1.3B model provided the best balance between performance and computational efficiency.

\subsection{Proof History Utilization}

We investigated the impact of incorporating proof history into the input for remaining step prediction. We compared the performance of our model when using only the current proof state (\textbf{state\_before}) as input against using both the current state and the preceding tactic sequence (\textbf{state\_proof}). 
The results, shown in Table~\ref{tab:proof_history_results}, demonstrate the importance of including proof history.
The prompt formats used for these two settings are as follows:

\begin{itemize}
    \item \textbf{state\_before}:
    \begin{verbatim}
--- STATE_BEFORE: {state_before}
--- STEPS_TO_NO_GOALS:
    \end{verbatim}
    \item \textbf{state\_proof}:
    \begin{verbatim}
--- STATE_BEFORE: {state_before}
--- PROOF: {proof}
--- STEPS_TO_NO_GOALS:
    \end{verbatim}
\end{itemize}
Where \texttt{\{state\_before\}} represents the current proof state, and \texttt{\{proof\}} represents the sequence of tactics applied so far.

\subsection{Evaluation}

We evaluate our model in two ways. First, we assess the accuracy of our step predictions directly on our generated dataset by calculating the Mean Absolute Error (MAE). Second, we investigate the potential of using our step predictions as a ranking score within a best-first search framework, comparing its performance against standard best-first search based solely on log probabilities.

\subsubsection{MAE Evaluation on Steps Dataset}

To evaluate the accuracy of our step predictions, we calculate the Mean Absolute Error (MAE) on our generated dataset $D$. Given a state $s_{i,j^*,l}$ in the selected proof trajectory for theorem $t_i$, our model predicts the number of remaining steps as $\hat{n}_{i,j^*,l} = f(s_{i,j^*,l})$. The actual number of remaining steps is $n_{i,j^*} - l$. The MAE is then calculated as:
$\text{MAE} = \frac{1}{|D|} \sum_{(s, n) \in D} |\hat{n} - n|$
where $|D|$ is the total number of (state, remaining steps) pairs in our dataset. This metric provides a direct measure of the average difference between our model's predictions and the true number of remaining steps.


\subsubsection{Proof History Helps Step Prediction}

We evaluate the prediction accuracy using Mean Absolute Error (MAE), which measures the average absolute difference between the predicted number of remaining steps and the actual number of remaining steps. A lower MAE indicates better prediction accuracy. Table~\ref{tab:proof_history_results} presents the MAE and accuracy results for both input formats across different ranges of proof lengths and overall. The table shows the total number of samples for each range, along with the accuracy and MAE achieved by each input format.

\begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Input & Range & Total Samples & Accuracy & MAE \\
        \midrule
        {\texttt{state}} & 1-5 & 2.82k & 71.1\% & 1.412 \\
        & 6-10 & 1.17k & 52.1\% & 2.920 \\
        & 11-15 & 567 & 47.7\% & 6.808 \\
        & 16-20 & 563 & 63.2\% & 4.915 \\
        & 21+ & 3.70k & 59.7\% & 8.648 \\
        & Overall & 8.82k & 61.8\% & 5.217 \\
        \midrule
        {\texttt{proof}} & 1-5 & 2.82k & 79.0\% & 1.066 \\
        & 6-10 & 1.17k & 61.5\% & 2.857 \\
        & 11-15 & 567 & 68.3\% & 4.341 \\
        & 16-20 & 563 & 77.1\% & 2.748 \\
        & 21+ & 3.70k & 76.7\% & 5.221 \\
        & Overall & 8.82k  & \textbf{75.1\%} & \textbf{3.290} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of MAE and Accuracy with and without Proof History.}
    \label{tab:proof_history_results}
\end{table}

From the results in Table~\ref{tab:proof_history_results}, we observe a significant performance drop when only the input state is used (\textbf{state\_before}). Including all previous tactics in the prompt (\textbf{state\_proof}) provides a "direction" for the proof, leading to better performance and more accurate predictions, as evidenced by the consistently lower MAE values across all ranges and overall. This improvement is likely due to the fact that proof history encodes information beyond the current state, such as consistent application of specific mathematical techniques (e.g., repeated use of exponentiation or logarithms) or the overall strategy being employed. This contextual information allows the model to make more informed predictions about the remaining steps.

\begin{figure*}[!h]
\centering
\begin{tabular}{p{0.25\textwidth}p{0.6\textwidth}}
\toprule
\textbf{Input (User)} & \texttt{theorem}
 \begin{lstlisting}[language=Lean]
 theorem lean_workbook_plus_74374 (m n : ℕ) : (m * n + m + n) % 6 = 4 → 12 | m * n   :=  by
 \end{lstlisting}\\
& \texttt{predict\_steps\_with\_suggestion} \\
\midrule
\textbf{Suggestion (LeanProgress)} & \texttt{Lean Infoview} \\

& \texttt{      Try these:} \\
& \begin{itemize}
    \item \texttt{rw [Nat.add\_comm]}
    \item \texttt{intro h}
    \item \texttt{rw [← Nat.mod\_add\_div m n]}
    \item \texttt{rw [Nat.dvd\_iff\_mod\_eq\_zero]}
    \item \texttt{omega}
\end{itemize} \\
& \texttt{      Steps remaining: 6} \\
\bottomrule
\end{tabular}
\caption{Simulated example showing the use of \texttt{predict\_steps\_with\_suggestion}. The tactic predicts 6 remaining steps and suggests 5 tactics. The first 3 tactics for this proof should be \texttt{intro h}, \texttt{have g := congr\_arg (· \% 6) h} and \texttt{simp at g}.}
\label{fig:qualitative_example}
\end{figure*}

\subsubsection{Combining Best-First Search with Steps Prediction}

Beyond direct prediction accuracy, we explore the potential of using our step predictions to guide proof search. We integrate our model into a best-first search framework by combining the predicted remaining steps with the log probabilities of the tactic sequence. Specifically, when selecting the next state to expand, instead of using only the cumulative log probability $L(s_i) = \sum_{j=0}^{i-1} \log p(a_j | s_j)$, where $(s_0, a_0), \dots, (s_{i-1}, a_{i-1})$ is the proof trajectory before state $s_i$ and $\log p(a_j | s_j)$ is the average log probability of the generated tokens for the tactic $a_j$ given state $s_j$, we use a combined score: $C(s_i) = \alpha N(s_i) + (1 - \alpha) P(s_i)$, where $\alpha \in [0, 1]$ is a hyperparameter that controls the relative importance of the normalized steps $N(s_i)$ and the log probability $P(s_i)$.

The normalized steps are calculated as $N(s_i) = -2\hat{n}_i / N_{\max}$, where $\hat{n}_i = f(s_i)$ is the predicted number of remaining steps for state $s_i$, and $N_{\max}$ is the maximum possible number of steps from all states in a proof.

We compare the performance of this combined approach(where $\alpha = 0.2$) with a standard best-first search using only log probabilities (equivalent to setting $\alpha = 0$). We evaluate both approaches by measuring the number of theorems solved within a fixed number of expansions and the average number of expansions required to find a proof. This comparison demonstrates the effectiveness of incorporating our step predictions into the search process.


Proof search requires a search algorithm and a method for interacting with Lean. So, we chose the best-first search for LeanDojo's implementation. Best-first search is parameterized by the maximum number of generated tactics, defined as the number of attempts × expansion size per iteration × maximum iterations, subject to a timeout. We use a 2-minute timeout and use beam search with a size of 1 × 32 due to memory constraints.


We compare our method, which combines predicted remaining steps with log probabilities, against standard best-first search using only log probabilities. We evaluate the LeanDojo v4 test dataset. The primary metric for evaluating proof search performance is the percentage of theorems solved within the timeout.

The results of this comparison are shown in Table~\ref{tab:search_results}, which demonstrate the effectiveness of incorporating our step predictions into the proof search process. 

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Method} & \textbf{Mathlib4-test} \\
        \midrule
        Original LogP  & 41.2\% \\
        Steps as Critic  & \textbf{45.0\%} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of Proof Search Performance: Pass rates on the Mathlib4-test dataset with Lean. This table shows the pass rates of
previous logP method and ours. In
sampling, we used a model with a temperature of 0.7; we sampled 32 examples once.
}
    \label{tab:search_results}
\end{table}
