\section{Introduction}
\label{sec:introduction}

Formal theorem proving \citep{avigad2023mathematicsformalturn} has emerged as a cornerstone of rigorous mathematical verification, providing machine-checked guarantees for proofs ranging from foundational results \citep{gowers2023conjecturemarton} to industrial applications \citep{liquid2022}. The Lean proof assistant \citep{lean}, built on dependent type theory, has witnessed remarkable adoption growth \citep{best_et_al:LIPIcs.ITP.2023.36}, fueled by collaborative efforts on large-scale formalization projects \citep{mathlib} and novel mathematical developments \citep{asgeirsson:LIPIcs.ITP.2024.6}. This collaborative paradigm shift underscores the urgent need for enhanced tooling to support mathematicians navigating increasingly complex proof environments.

The recent success of Large Language Models (LLMs) in code generation \citep{rozi√®re2024codellamaopenfoundation} and symbolic reasoning \citep{yu2024natural} has spurred innovations at the intersection of LLMs and formal verification. Some of the works that have been developed include LeanDojo \citep{yang2024leandojo} which provides an interactive environment for training LLMs on tactic-level interactions while LLMStep \citep{welleck2023llmstep} and {LeanCopilot} \citep{song2024largelanguagemodelscopilots} focuses on next-tactic suggestion through interface as a useful tool. {Lean Agent} \citep{kumarappan2024leanagentlifelonglearningformal} then combines neural suggestion with life-long learning while Lean Aide \citep{agrawal2022mathematicsformalisationassistantusing} and Lean-STaR\citep{lin2024leanstarlearninginterleavethinking} translates statements written in natural language in a doc-string like format to Lean types (including theorem statements) and bootstrapping thoughts. While these systems demonstrate impressive tactic-level accuracy \citep{10.1007/978-3-031-46002-9_25}, they primarily optimize for local correctness rather than global proof progress -- a critical limitation when navigating Lean's vast action space \citep{nawrocki_et_al:LIPIcs.ITP.2023.24}.

\begin{figure*}[h]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/1.pdf} 
    \caption{ \textbf{The visualization of LeanProgress.} LeanProgress is a lightweight framework that collects the number of remaining steps in proof trees and then balances the data distribution to train the language model. Then LeanProgress takes the proof state as input to generate the remaining steps for each state as a signal for search. LeanProgress also integrates the tactic \texttt{predict\_steps} in LeanCopilot as a user-friendly tool.}
    \label{fig:intro}
\end{figure*}

Reinforcement learning (RL) presents a theoretically appealing framework for automated theorem proving \citep{dong2024formaltheoremprovingrewarding}, where finding reward signals over proof trajectories is essential. However, the combinatorial explosion of tactic sequences in Lean \citep{10.1145/3573105.3575669} renders direct RL applications impractical \citep{setlur2024rewardingprogressscalingautomated}. Alphaproof \citep{alphaproof2024ai} has done RL for theorem proving but it's not open source and needs enormous compute. Current approaches mitigate this through hybrid architectures \citep{wang2023legoproverneuraltheoremproving} but remain fundamentally limited by the absence of reliable progress indicators to guide exploration -- a prerequisite for effective RL in mathematical domains \citep{gao2024designingeffectiverlreward}.

We address this critical gap with \textbf{LeanProgress} (Fig~\ref{fig:mdp_diagram}), a lightweight framework that predicts remaining proof steps through learned progress signals with search methods beyond log-probability based search \citep{song2024largelanguagemodelscopilots} or manual heuristics \citep{ringer2021proof}.

LeanProgress makes the following key contributions:

\textbf{Balanced Data Generation Pipeline:} We construct a balanced dataset of approximately 80k proof trajectories from Lean Workbook Plus and Mathlib4 by performing a tree search and selecting the shortest path as ground truth. We employ a data balancing strategy based on relative proof progress. Since the useful and non-trivial data of long proofs are long-tailed distributed in the original dataset, we fully utilize long proof data by assigning each state with a remaining step as a label.

\textbf{Model for Progress Prediction:} We fine-tune a DeepSeek Coder V1 1.3b base model to predict the remaining steps, achieving a Mean Absolute Error (MAE) of 3.29 and an overall prediction accuracy of 75.1\% on the test set with proof history. Unlike tactic suggestion tools, LeanProgress provides a global view of the proof process by predicting the remaining steps rather than the immediate next tactic.

\textbf{Progress-Guided Proof Search:} We integrate our step prediction model into a best-first search framework. A natural first step for using the progress predictor is 
combining the predicted remaining steps with the tactic generator's log probabilities to guide the search. In the future, we hope to use this, instead of just relying on the log probabilities, as a reward for RL.  We observe on Mathlib4 a significant improvement of 3.8\% with the baseline Reprover performance
of 41.2\%.

\textbf{Integration with LeanCopilot:} Based on the LeanCopilot framework, we provide a new built-in tactic \texttt{predict\_steps\_with\_suggestion} within the standard Lean user interface. It is a helpful tool that not only suggest tactics but also offer users immediate feedback on proof progress and potential next steps.
