\section{Data Generation for LeanProgress}
\label{sec:methods}

This section details the data generation and processing methodology used to train LeanProgress. We describe the process of generating proof trees using best-first search (BFS) and the Reprover model, the resulting dataset of proof trajectories, and the adjustments made to address the skewed distribution of proof lengths.

\subsection{Preliminaries: Tactic Prediction as an MDP}

Interactive Theorem Provers (ITPs) frame theorem proving as a search problem. As Fig~\ref{fig:mdp_diagram} shows, the initial theorem to be proven represents the initial state, and the application of tactics generates transitions to new states, each containing subgoals. The objective is to find a sequence of tactics that leads to a state where all subgoals are proven. This search process is central to automated theorem proving, and our work focuses on providing valuable information to guide this search within the Lean ITP.

\begin{figure*}[h]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/fig2.png}
    \caption{ \textbf{The visualization of extract proof tree} in theorem proving.}
    \label{fig:mdp_diagram}
\end{figure*}

The theorem-proving problem can be formalized as a Markov Decision Process (MDP), denoted as $(S, A, P_a, R_a)$, where $S$ represents the set of all possible proof states. $A$ represents the set of all available tactics (actions). $P_a$ represents the state transition probabilities after executing tactic $a$ in state $s$. $R_a$ represents the reward obtained by executing tactic $a$. From an MDP perspective, a proof process can be viewed as a trajectory of states, tactics, and rewards: $(s_i, a_i, r_i)$, where the proof assistant (e.g., Lean) provides the next state $s_{i+1}$ given the current state $s_i$ and the applied tactic $a_i$.

In typical tactic prediction, proving a theorem involves providing a proof state $s$ to a language model $L$, which then generates a tactic $a$, i.e., $\pi_L(a|s)$. Typically, final states (where the goal is proven) are assigned a reward of 1, indicating successful completion of the proof.

\subsection{Generating Proof Trees and Trajectories}

A common evaluation strategy for neural theorem provers is best-first search (BFS), as used in GPT-f and related research \citep{han2021proof}. This method explores the proof space by iteratively expanding the "best" state, determined by the maximum cumulative log probability of the preceding proof trajectory. Specifically, given a set of unexpanded states $s_i$, the "best" state to expand is chosen according to:
$\max_i \sum_{j=0}^{i-1} \log p(a_j, s_j),$
where $(s_0, a_0), \dots, (s_{i-1}, a_{i-1})$ is the proof trajectory before state $s_i$ and $\log p(a_j, s_j)$ is the average log probability of the generated tokens for the tactic $a_j$ in state $s_j$.

Our work utilizes BFS in conjunction with the Reprover model \citep{yang2024leandojo} to generate successful proof trees. By systematically applying Reprover to all reachable states within a certain depth in a best-first manner, we construct a tree of successful proofs. This approach allows us to collect a dataset of complete proof trajectories, which is then used to train our model to predict the number of remaining steps. This data generation process is crucial for training our model to understand the relationship between proof states and the number of steps required for completion. In particular, if multiple proofs are found for a theorem (i.e., multiple \texttt{no\_goals} nodes are reached), we select the proof with the minimum depth (the length of the path from the root node to the \texttt{no\_goals} node) to ensure the quality and consistency of the training data.

Formally, let $T = \{t_1, t_2, ..., t_M\}$ be the set of theorems in our dataset. For each theorem $t_i \in T$, we perform BFS using the Reprover model to generate a set of successful proof trajectories $P_i = \{p_{i,1}, p_{i,2}, ..., p_{i,k_i}\}$, where $k_i$ is the number of proofs found for theorem $t_i$. Each proof trajectory $p_{i,j}$ is a sequence of proof states: $p_{i,j} = (s_{i,j,1}, s_{i,j,2}, ..., s_{i,j,n_{i,j}})$, where $n_{i,j}$ is the length (number of steps) of the $j$-th proof for theorem $t_i$.

If $k_i > 1$, we select the proof trajectory $p_{i,j^*}$ with the minimum depth:

\[
j^* = \arg\min_j \{n_{i,j} \mid 1 \le j \le k_i\}
\]

Our training dataset $D$ is then constructed by extracting (state, remaining steps) pairs from the selected proof trajectories:

\[
D = \{(s_{i,j^*,l}, n_{i,j^*} - l) \mid t_i \in T, 1 \le l \le n_{i,j^*}\}
\]

where $s_{i,j^*,l}$ is the $l$-th state in the selected proof trajectory $p_{i,j^*}$ for theorem $t_i$, and $n_{i,j^*} - l$ represents the number of remaining steps from state $s_{i,j^*,l}$ to the end of the proof.

\begin{figure*}[t]
\centering
    \subfigure[Original dataset of steps distribution]{
        \includegraphics[width=0.45\textwidth]{figs/sft_0129_original_distribution.png}
        \label{fig:original_proof_length_distribution}
    }
    \subfigure[Adjusted dataset of steps distribution by balancing different ranges]{
        \includegraphics[width=0.45\textwidth]{figs/sft_0129_adjusted_distribution.png}
        \label{fig:adjusted_proof_length_distribution}
    }
    \caption{Distribution of proof lengths before and after adjustment. Fig~(a) shows Original dataset with average proof length of $L_{\text{original}} = 2.47$. We address the imbalance by assigning each range with different sample ratios. After adjustment the average proof length of $L_{\text{original}} = 10.1$. This dataset is balanced to keep more long proof paths.} 
\end{figure*}

\subsection{Data Balancing}

We evaluated our models on a dataset of Lean proofs extracted from Lean Workbook Plus and Mathlib4. The original dataset exhibited a skewed distribution of proof lengths, with an average proof length of $L_{\text{original}} = 2.47$. This distribution is shown in Figure~\ref{fig:original_proof_length_distribution}. We adjusted the data distribution based on relative progress within each proof to address this imbalance and ensure a more representative sample of different proof stages.

The adjustment was performed by assigning different sampling ratios to five ranges of proof lengths: 1-5 steps (Basic progress, 0.01 ratio), 6-10 steps (Intermediate progress, 0.3 ratio), 11-15 steps (Moderate progress, 0.5 ratio), 16-20 steps (Advanced progress, 0.7 ratio), and 21+ steps (Expert progress, 1.0 ratio). This strategy effectively upsamples longer proofs and downsamples shorter ones, resulting in a more balanced dataset. The resulting adjusted distribution has an average proof length of $L_{\text{adjusted}} = 10.1$, and the comparison is shown in Figure~\ref{fig:adjusted_proof_length_distribution}.

The dataset was then split into training, validation, and test sets. The test set contains 88,233 proof states. The training set contains $N_{\text{train}}$ proof states, and the validation set contains $N_{\text{val}}$ proof states. The dataset was partitioned randomly at the theorem level, meaning that all states from a given theorem belong to the same split (either training, validation, or test). This prevents data leakage between splits.
