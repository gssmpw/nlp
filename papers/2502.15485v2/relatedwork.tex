\section{Related Work}
\label{sec:related}
Recent advances in long-context modeling follow three primary paradigms:

\paragraph{Sparse Attention Mechanisms} Approaches like Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce quadratic complexity through localized attention patterns. While effective, these methods introduce handcrafted sparsity patterns that may not align with linguistic structures.

\paragraph{Memory-Augmented Networks} Transformer-XL \cite{dai2019transformerxl} introduces segment-level recurrence with memory reuse, enabling longer context retention. However, its fixed-length memory window limits adaptability to varying context requirements.

\paragraph{Linear Transformers} The Linear Transformer family \cite{katharopoulos2020transformers} replaces softmax attention with kernel approximations for linear complexity. RWKV \cite{peng2023rwkv} extends this approach through exponential decay mechanisms and channel-wise mixing.

Our work bridges the gap between static recurrence and dynamic attention by introducing learnable gating mechanisms inspired by recent work in adaptive attention spans \cite{liu2021adaptive}. Unlike previous approaches that modify attention patterns, we directly optimize the hidden state dynamics through differentiable routing.