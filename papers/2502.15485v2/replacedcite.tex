\section{Related Work}
\label{sec:related}
Recent advances in long-context modeling follow three primary paradigms:

\paragraph{Sparse Attention Mechanisms} Approaches like Longformer ____ and BigBird ____ reduce quadratic complexity through localized attention patterns. While effective, these methods introduce handcrafted sparsity patterns that may not align with linguistic structures.

\paragraph{Memory-Augmented Networks} Transformer-XL ____ introduces segment-level recurrence with memory reuse, enabling longer context retention. However, its fixed-length memory window limits adaptability to varying context requirements.

\paragraph{Linear Transformers} The Linear Transformer family ____ replaces softmax attention with kernel approximations for linear complexity. RWKV ____ extends this approach through exponential decay mechanisms and channel-wise mixing.

Our work bridges the gap between static recurrence and dynamic attention by introducing learnable gating mechanisms inspired by recent work in adaptive attention spans ____. Unlike previous approaches that modify attention patterns, we directly optimize the hidden state dynamics through differentiable routing.