@misc{O1,
  author = {OpenAI},
  year = {2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/},
  title = {Learning to reason with LLMs}
}

@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={AAAI},
  volume={38},
  number={16},
  pages={17682--17690},
  year={2024}
}

@article{chen2023measuring,
  title={Measuring and improving chain-of-thought reasoning in vision-language models},
  author={Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay},
  journal={arXiv preprint arXiv:2309.04461},
  year={2023}
}

@article{chen2024boosting,
  title={Boosting of thoughts: Trial-and-error problem solving with large language models},
  author={Chen, Sijia and Li, Baochun and Niu, Di},
  journal={arXiv preprint arXiv:2402.11140},
  year={2024}
}

@misc{deepseek,
  author = {DeepSeek},
  year = {2024},
  url = {https://api-docs.deepseek.com/news/news1120},
  title = {Deepseek-r1-lite-preview: Unleashing su-
percharged reasoning power}
}

@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@article{hu2024rankprompt,
  title={RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners},
  author={Hu, Chi and Ge, Yuan and Ma, Xiangnan and Cao, Hang and Li, Qiang and Yang, Yonghua and Xiao, Tong and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2403.12373},
  year={2024}
}

@inproceeding{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={NeurIPS},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{le2024visualcoder,
  title={VISUALCODER: Guiding Large Language Models in Code Execution with Fine-grained Multimodal Chain-of-Thought Reasoning},
  author={Le, Cuong Chi and Truong-Vinh, Hoang-Chau and Phan, Huy Nhat and Le, Dung Duy and Nguyen, Tien N and Bui, Nghi DQ},
  journal={arXiv preprint arXiv:2410.23402},
  year={2024}
}

@article{lei2023boosting,
  title={Boosting logical reasoning in large language models through a new framework: The graph of thought},
  author={Lei, Bin and Liao, Chunhua and Ding, Caiwen and others},
  journal={arXiv preprint arXiv:2308.08614},
  year={2023}
}

@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@inproceedings{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  booktitle={NeurIPS},
  volume={36},
  year={2024}
}

@article{mishra2022lila,
  title={Lila: A unified benchmark for mathematical reasoning},
  author={Mishra, Swaroop and Finlayson, Matthew and Lu, Pan and Tang, Leonard and Welleck, Sean and Baral, Chitta and Rajpurohit, Tanmay and Tafjord, Oyvind and Sabharwal, Ashish and Clark, Peter and others},
  journal={arXiv preprint arXiv:2210.17517},
  year={2022}
}

@article{pitis2023boosted,
  title={Boosted prompt ensembles for large language models},
  author={Pitis, Silviu and Zhang, Michael R and Wang, Andrew and Ba, Jimmy},
  journal={arXiv preprint arXiv:2304.05970},
  year={2023}
}

@misc{qwq,
  author = {Qwen},
  year = {2024},
  url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
  title = {QwQ: Reflect Deeply on the Boundaries of the Unknown}
}

@inproceedings{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  booktitle={NeurIPS},
  volume={36},
  year={2024}
}

@article{shum2023automatic,
  title={Automatic prompt augmentation and selection with chain-of-thought from labeled data},
  author={Shum, KaShun and Diao, Shizhe and Zhang, Tong},
  journal={arXiv preprint arXiv:2302.12822},
  year={2023}
}

@article{team2025kimi,
  title={Kimi k1. 5: Scaling Reinforcement Learning with LLMs},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

@misc{tinyzero,
author       = {Jiayi Pan and Junjie Zhang and Xingyao Wang and Lifan Yuan and Hao Peng and Alane Suhr},
title        = {TinyZero},
howpublished = {https://github.com/Jiayi-Pan/TinyZero},
year         = {2025}
}

@article{wang2023knowledge,
  title={Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-intensive question answering},
  author={Wang, Keheng and Duan, Feiyu and Wang, Sirui and Li, Peiguang and Xian, Yunsen and Yin, Chuantao and Rong, Wenge and Xiong, Zhang},
  journal={arXiv preprint arXiv:2308.13259},
  year={2023}
}

@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={NeurIPS},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{yao2024mulberry,
  title={Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search},
  author={Yao, Huanjin and Huang, Jiaxing and Wu, Wenhao and Zhang, Jingyi and Wang, Yibo and Liu, Shunyu and Wang, Yingjie and Song, Yuxin and Feng, Haocheng and Shen, Li and others},
  journal={arXiv preprint arXiv:2412.18319},
  year={2024}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@article{zhao2024marco,
  title={Marco-o1: Towards open reasoning models for open-ended solutions},
  author={Zhao, Yu and Yin, Huifeng and Zeng, Bo and Wang, Hao and Shi, Tianqi and Lyu, Chenyang and Wang, Longyue and Luo, Weihua and Zhang, Kaifu},
  journal={arXiv preprint arXiv:2411.14405},
  year={2024}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

