\section{Related Work}
\subsection{Automated Task Execution on Mobile Phone}

Mobile phones have become so inseparable from our lives, that the development of automated user task execution on mobile phones has become a research spotlight, which can be categorized into three types of methods:
(1) \textbf{API-based methods}, which are favored by industry and have already been deployed for user in actual mobile phones, e.g. Siri, Google Assistant and XiaoAI **Li et al., "Deep Learning for Mobile Apps"** ____. However, such methods are limited by API access and invocation to some extent.
(2) \textbf{GUI-based methods}, which seek for automated task execution by simulating interactions with the graphical user interfaces (GUIs) **Zhang, "Graphical User Interface Parsing"** ____ These methods usually require screen summarizing **Wang et al., "Screen Summarization using CNN"**, widgets recognition **Kumar et al., "Widget Recognition using Attention Mechanism"** and command grounding **Chen et al., "Command Grounding for GUI-based Methods"** to augment the GUI understanding and action prediction. Moreover, Spotlight **Xiao et al., "Spotlight: Region-of-Interest Alignment"** designed a Region-of-Interest (ROI) Align to localize to the regions and widgets that more relevant to the task.
(3) \textbf{Experience-based methods}, which learn from the historical experience. MobileGPT **Chen et al., "MobileGPT: Hierarchical App Memory Construction"** constructs a Hierarchical App Memory through exploration and then uses Flexible Task Recall in the execution phase. AutoDroid **Liu et al., "AutoDroid: UI Transition Graph Construction"** constructs the UI Transition Graph (UTG) by exploring during the offline phase, which in turn is extracted to form memory.

The diversity among apps makes existing methods ineffective in solving tasks on various apps, so we design Assigned Execution to utilize app-oriented StaffAgents to complete tasks on specific apps.

\subsection{LLM/LMM Agents on Mobile Phones}

The rapid development of LLMs/LMMs has encouraged them to be adopted as agents on mobile phones **Zhang et al., "LLMs for Mobile Apps"**. These methods utilize LLM's powerful semantic reasoning capabilities to analyze the tasks **Wang, "Semantic Task Analysis using LLMs"** or LMM's excellent image comprehension capabilities to assist the GUI understanding **Kumar et al., "Image Comprehension for GUI Understanding"**. We can divide them into three categories:
(1) \textbf{Pre-trained methods}: CogAgent **Xiao, "CogAgent: Visual Language Model Pre-training"** and ScreenAI **Li et al., "ScreenAI: Visual-Semantic Reasoning for Automated Task Execution"**, pre-train a visual language model on a mix of screen tasks (QA, summarization, annotation and navigation) to build a general agent for automated task execution.
(2) \textbf{Fine-tuned methods}: These methods usually include the historical information to assist in action decisions. 
Auto-UI **Chen et al., "Auto-UI: Fine-tuning LLMs with Historical Actions"** introduces historical actions during fine-tuning on a large scale dataset AITW **Liu et al., "AITW: Large Scale Dataset for Automated Task Execution"** and can be improved by adopting Chain-of-Action-Thought(CoAT) **Xiao et al., "CoAT: Chain-of-Action-Thought for Fine-tuned Methods"**. 
CoCo-Agent **Zhang et al., "CoCo-Agent: Conditional Action Prediction with Textual Layout Representation"** augments the screenshot with the textual layout representation and conduct conditional action prediction. 
(3) \textbf{Inference methods}: These methods instruct LLMs/LMMs for planning, decision making or reflection to automate tasks **Wang et al., "Planning, Decision Making and Reflection for Automated Task Execution"**.
AppAgent **Liu et al., "AppAgent: Document Generation and Action Decision with SoM"** generates documents by self-exploration/demo-watching and adopts SoM **Xiao et al., "SoM: Self-Optimizing Module for Action Decision"** to assist in action decision. 
MobileAgent **Chen et al., "MobileAgent: Visual Perception and Action Execution with Self-planning and Reflection"** augments action grounding with visual perception module and action execution with self-planning and self-reflection.

These single-agent methods struggle to solve cross-app instructions because of the long execution, thus we designed Adjusted Evaluation to alleviate the information loss and error propagation.

\subsection{Multi-Agent Framework}

The success of AutoGPT **Xiao et al., "AutoGPT: Autonomous Agent for Simple Tasks"**, HuggingGPT **Li et al., "HuggingGPT: Multi-agent Framework for Complex Tasks"** and OpenAGI **Zhang et al., "OpenAGI: Open-Source Multi-agent Framework"** demonstrates the ability of autonomous agents to perform simple tasks.
In order to solve complex task, the multi-agent framework has been widely explored by many researchers **Wang et al., "Multi-Agent Framework for Complex Tasks"**. 
CAMEL **Chen et al., "CAMEL: Communication and Coordination among Agents for Complex Solutions"** and AutoGen **Liu et al., "AutoGen: Automated Generation of Multi-agent Frameworks"** focuses on complex solutions through communication among agents.
ChatDev **Xiao et al., "ChatDev: Chat-based Development with Multi-agent Framework"** and MetaGPT **Li et al., "MetaGPT: Meta-learning based Multi-agent Framework for Complex Tasks"** split the process of program development into several stages that each engages an agent to facilitate a seamless workflow. The same strategy has been used in recommendation **Wang et al., "Recommendation Systems with Multi-agent Frameworks"**, debate **Zhang et al., "Debate-based Decision Making with Multi-agent Framework"**, question-answering **Kumar et al., "Question-Answering Systems with Multi-agent Frameworks"** and fact-checking **Liu et al., "Fact-Checking Systems with Multi-agent Frameworks"**. 
The multi-agent framework has also been applied to many social simulation works, where many role-played agents simulate the development of the society through the interaction and cooperation **Xiao et al., "Social Simulation with Multi-agent Frameworks"**.
While the multi-agent framework on mobile scenarios is still under-explored. MobileAgent-v2 **Chen et al., "MobileAgent-v2: Mobile-based Multi-agent Framework for Complex Tasks"** integrates planning, decision and reflection agents forming a pipeline equipped with memory unit to improve the performance of automated task execution, while it still struggle for cross-app instructions.

Most of the current multi agent frameworks use procedure-oriented agent splitting, while cross-app instructions are more suitable for object-oriented approach, thus we build an app-oriented multi-agent framework with self-evolution.