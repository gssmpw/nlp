\section{Related Work}
\subsection{Automated Task Execution on Mobile Phone}

Mobile phones have become so inseparable from our lives, that the development of automated user task execution on mobile phones has become a research spotlight, which can be categorized into three types of methods:
(1) \textbf{API-based methods}, which are favored by industry and have already been deployed for user in actual mobile phones, \eg Siri, Google Assistant and XiaoAI ~\cite{wen2023empowering}. However, such methods are limited by API access and invocation to some extent.
(2) \textbf{GUI-based methods}, which seek for automated task execution by simulating interactions with the graphical user interfaces (GUIs) ~\cite{li2020mapping, li2021learning, burns2022interactive, sun2022meta}. These methods usually require screen summarizing~\cite{wang2021screen2words, li2021screen2vec, zhang2021screen}, widgets recognition~\cite{li2020widget, chen2022towards} and command grounding~\cite{bai2021uibert, burns2022dataset} to augment the GUI understanding and action prediction. Moreover, Spotlight~\cite{li2023spotlight} designed a Region-of-Interest (ROI) Align to localize to the regions and widgets that more relevant to the task.
(3) \textbf{Experience-based methods}, which learn from the historical experience. MobileGPT~\cite{lee2023explore} constructs a Hierarchical App Memory through exploration and then uses Flexible Task Recall in the execution phase. AutoDroid~\cite{wen2024autodroid} constructs the UI Transition Graph (UTG) by exploring during the offline phase, which in turn is extracted to form memory.

The diversity among apps makes existing methods ineffective in solving tasks on various apps, so we design Assigned Execution to utilize app-oriented StaffAgents to complete tasks on specific apps.

\subsection{LLM/LMM Agents on Mobile Phones}

The rapid development of LLMs/LMMs has encouraged them to be adopted as agents on mobile phones ~\cite{wen2023droidbot}. These methods utilize LLM's powerful semantic reasoning capabilities to analyze the tasks~\cite{venkatesh2022ugif,wang2023enabling} or LMM's excellent image comprehension capabilities to assist the GUI understanding~\cite{yan2023gpt, zheng2024gpt}. We can divide them into three categories:
(1) \textbf{Pre-trained methods}: CogAgent~\cite{hong2024cogagent} and ScreenAI~\cite{baechler2024screenai}, pre-train a visual language model on a mix of screen tasks (QA, summarization, annotation and navigation) to build a general agent for automated task execution.
(2) \textbf{Fine-tuned methods}: These methods usually include the historical information to assist in action decisions. 
Auto-UI~\cite{zhan2023you} introduces historical actions during fine-tuning on a large scale dataset AITW~\cite{rawles2024androidinthewild} and can be improved by adopting Chain-of-Action-Thought(CoAT)~\cite{zhang2024android}. 
CoCo-Agent\cite{ma2024comprehensive} augments the screenshot with the textual layout representation and conduct conditional action prediction. 
(3) \textbf{Inference methods}: These methods instruct LLMs/LMMs for planning, decision making or reflection to automate tasks~\cite{deng2024mobile}.
AppAgent~\cite{yang2023appagent} generates documents by self-exploration/demo-watching and adopts SoM~\cite{yang2023set} to assist in action decision. 
MobileAgent~\cite{wang2024mobile} augments action grounding with visual perception module and action execution with self-planning and self-reflection.

These single-agent methods struggle to solve cross-app instructions because of the long execution, thus we designed Adjusted Evaluation to alleviate the information loss and error propagation.

\subsection{Multi-Agent Framework}

The success of AutoGPT~\cite{autogpt}, HuggingGPT~\cite{shen2024hugginggpt} and OpenAGI~\cite{ge2024openagi} demonstrates the ability of autonomous agents to perform simple tasks.
In order to solve complex task, the multi-agent framework has been widely explored by many researchers ~\cite{guo2024large}. 
CAMEL~\cite{li2023camel} and AutoGen~\cite{wu2023autogen} focuses on complex solutions through communication among agents.
ChatDev~\cite{qian2023communicative} and MetaGPT~\cite{hongmetagpt} split the process of program development into several stages that each engages an agent to facilitate a seamless workflow. The same strategy has been used in recommendation~\cite{wang2024multi, zhang2024generative}, debate~\cite{du2023improving,chan2023chateval}, question-answering ~\cite{sun2024harnessing} and fact-checking~\cite{kim2024can, liu2025bidev}. 
The multi-agent framework has also been applied to many social simulation works, where many role-played agents simulate the development of the society through the interaction and cooperation~\cite{park2023generative, zhang2023exploring, kaiya2023lyfe, liu2024skepticism, liu2024tiny}.
While the multi-agent framework on mobile scenarios is still under-explored. MobileAgent-v2~\cite{wang2024mobilev2} integrates planning, decision and reflection agents forming a pipeline equipped with memory unit to improve the performance of automated task execution, while it still struggle for cross-app instructions.

Most of the current multi agent frameworks use procedure-oriented agent splitting, while cross-app instructions are more suitable for object-oriented approach, thus we build an app-oriented multi-agent framework with self-evolution.