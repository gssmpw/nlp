\section{Related Work}
\label{sec:rw}
\textbf{Graph Transformers.} Graph transformers are a class of models designed to process graph-structured data by leveraging self-attention, which provides a graph-wide receptive field____, thereby avoiding common limitations of message-passing GNNs____. Despite their advantages, graph transformers have not yet become the dominant architecture in graph learning, primarily due to the lack of unified structural and positional encodings that generalize well across diverse graph tasks____. This is evident from the continued success of hybrid approaches that integrate message-passing with global self-attention____, highlighting ongoing opportunities for architectural improvements. Promising research directions include: more flexible spectral attention mechanisms____, models that mitigate the quadratic memory footprint of self-attention____, and hierarchical encodings that enhance structural representations____. \\
\textbf{Expressivity of Path-Based MPNNs.} The expressivity of message-passing neural networks (MPNNs) on paths has been studied in relation to the Weisfeiler-Leman (WL) isomorphism test____. Notably, ____ demonstrated that iteratively updating node features via message-passing along paths originating from these nodes is strictly more expressive than the 1-WL test. However, these works consider only short path lengths, limiting their applicability to more complex graph structures. \\
\textbf{Counting Paths and Cycles in Graphs.} The problem of counting paths and cycles in graphs has been extensively studied over the past decades____. ____ derived an explicit formula for counting paths of length up to 6 between two nodes, while ____ proposed an algorithm for counting paths of any length based on enumerating connected induced subgraphs. However, these methods become impractical for the large graphs and long path lengths considered in our work, necessitating the use of efficient approximate counting methods.