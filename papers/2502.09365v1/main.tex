\documentclass[twocolumn]{article}

\usepackage{arxiv}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage[table,dvipsnames]{xcolor} % Allows coloring of table rows and columns

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{natbib}


\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}



\newcommand{\best}[1]{\textcolor{cyan}{#1}}
\newcommand\second[1]{\textcolor{orange}{#1}}
% \newcommand{\second}[1]{\textcolor{orange}{#1}}
\newcommand{\third}[1]{\textcolor{RawSienna}{#1}}
%\newcommand{\best}[1]{\mathbf{#1}}
\newcommand{\default}[1]{\textcolor{red}{#1}}
\newcommand{\firstusage}{\textit{SPSE used in MPNN for node representation }}
\newcommand{\secondusage}{\textit{RW/SPSE used as edge feature in self-attention layers}}
\newcommand{\spsegrit}{\textbf{GRIT-SPSE}}
\newcommand{\rwsegrit}{GRIT-RWSE}
\newcommand{\spsecsa}{\textbf{CSA-SPSE}}
\newcommand{\rwsecsa}{CSA-RWSE}
\newcommand{\gps}{GPS}

\usepackage{authblk}

\newcommand{\cmark}{\textcolor{black}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{black}{\ding{55}}}%


\newcommand{\AL}[1]{\textcolor{cyan}{Antonio: #1}}
\usepackage[table]{xcolor}  % Enables coloring of table cells

\definecolor{mycustomcolor}{HTML}{fadbd8} % Use HTML color #FF5733
\definecolor{mycustomcolor2}{HTML}{d6eaf8} % Use HTML color #FF5733
\definecolor{mycustomcolor3}{HTML}{d5f5e3} % Use HTML color #FF5733

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


% notations and symbols
\usepackage{xspace}
\newcommand{\xgb}{\textsc{LightGBM}\xspace}

\usepackage{pifont}


\title{Simple Path Structural Encoding for Graph Transformers}

% \begin{icmlauthorlist}
% \icmlauthor{Louis Lastname1}{unitn}
% \icmlauthor{Antonio Longa}{unitn}
% \icmlauthor{Mattia Lastname3}{unitn}
% \icmlauthor{Andrea Passerini}{unitn}
% \icmlauthor{Firstname4 Lastname4}{unitn}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{unitn}{University of Trento, Trento, Italy}


\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}
% box is needed for correct spacing with authblk
% \newbox{\orcid}\sbox{\orcid}{\includegraphics[scale=0.06]{orcid.pdf}} 
\author[1]{%
Louis Airale%
}
\author[1]{%
Antonio Longa
}
\author[1]{%
Mattia Rigon
}
\author[1]{%
Andrea Passerini
}
\author[1]{%
Roberto Passerone
}
\affil[1]{University of Trento, Trento, Italy}
% \affil[2]{Department of Electrical Engineering, Mount-Sheikh University, Santa Narimana, Levand}

% \author{ \href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}David S.~Hippocampus}\thanks{Use footnote for providing further
% 		information about author (webpage, alternative
% 		address)---\emph{not} for acknowledging funding agencies.} \\
% 	Department of Computer Science\\
% 	Cranberry-Lemon University\\
% 	Pittsburgh, PA 15213 \\
% 	\texttt{hippo@cs.cranberry-lemon.edu} \\
% 	%% examples of more authors
% 	\And
% 	\href{https://orcid.org/0000-0000-0000-0000}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Elias D.~Striatum} \\
% 	Department of Electrical Engineering\\
% 	Mount-Sheikh University\\
% 	Santa Narimana, Levand \\
% 	\texttt{stariate@ee.mount-sheikh.edu} \\
% \else
% % Multiple affiliations variant of author block
% \usepackage{authblk}
% \renewcommand\Authfont{\bfseries}
% \setlength{\affilsep}{0em}
% % box is needed for correct spacing with authblk
% \newbox{\orcid}\sbox{\orcid}{\includegraphics[scale=0.06]{orcid.pdf}} 
% \author[1]{%
% 	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}David S.~Hippocampus\thanks{\texttt{hippo@cs.cranberry-lemon.edu}}}%
% }
% \author[1,2]{%
% 	\href{https://orcid.org/0000-0000-0000-0000}{\usebox{\orcid}\hspace{1mm}Elias D.~Striatum\thanks{\texttt{stariate@ee.mount-sheikh.edu}}}%
% }
% \affil[1]{Department of Computer Science, Cranberry-Lemon University, Pittsburgh, PA 15213}
% \affil[2]{Department of Electrical Engineering, Mount-Sheikh University, Santa Narimana, Levand}
% \fi

\begin{document}


\twocolumn[{
  \begin{@twocolumnfalse}

	\maketitle

	\begin{abstract}
        Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs.
        This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting.
        SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers. 
	\end{abstract}

  \end{@twocolumnfalse}
  \vspace{1em}
}]


\section{Introduction}\label{sec:intro}

    Graphs are pervasive across diverse domains, representing complex relationships in areas such as social networks\citep{otte2002social}, molecular structures\citep{quinn2017molecular}, and citation graphs\citep{radicchi2011citation}. Recent advances in graph neural networks (GNNs) have driven significant progress in learning from graph-structured data~\citep{kipf2017semisupervised,veličković2018graph,bodnar2022neural,lachi2024a,ferrini2024meta,duta2024sphinx}, yet these models often face challenges in capturing long-range dependencies and structural patterns due to their reliance on localized message passing~\citep{alon2021on,topping2022understanding}.

    Inspired by their success in vision and sequence learning tasks~\citep{vaswani2017attention,dosovitskiy2020image}, transformers have been extended to graph learning problems~\citep{yun2019graph,dwivedi2020generalization,ying2021transformers,kreuzer2021rethinking}. Unlike traditional GNNs, graph transformers leverage global self-attention, allowing each node to attend to all others within a graph, regardless of distance. This flexibility overcomes the limitations of message-passing approaches but introduces new challenges, particularly in designing suitable positional and structural encodings that capture the inherent irregularities of graphs.

    For directed acyclic graphs (DAGs), positional encodings (PEs) based on partial orderings can be directly applied~\citep{dong2022pace,luo2024transformers,hwang2024flowerformer}. However, for general undirected graphs, successful PEs often rely on eigendecompositions of the graph Laplacian, drawing inspiration from sinusoidal encodings in sequence transformers~\citep{dwivedi2020generalization,mialon2021graphit}. These approaches encode node-level information but fail to capture the full structural complexity of edge patterns in node neighborhoods.

    To address this limitation, several graph transformer architectures incorporate initial message-passing steps to encode local substructures~\citep{wu2021representing,mialon2021graphit,chen2022structure}. While effective, these methods focus solely on node representations and do not exploit the potential of injecting pairwise structural encodings directly into the self-attention mechanism. Recent studies have explored structural edge encodings in pure transformer architectures, based for instance on Laplacian eigenvectors, heat kernels, or shortest path distances~\citep{kreuzer2021rethinking,ying2021transformers,chen2023graph}. Despite their utility, these encodings are limited in expressivity, particularly for capturing local cyclic patterns or higher-order substructures.

    A promising alternative is random walk structural encoding (RWSE), which encodes richer structural information by considering random walk probabilities as edge features. RWSE has shown substantial improvements in the performance of state-of-the-art graph transformers~\citep{menegaux2023self,ma2023graph}. However, RWSE struggles to differentiate between distinct graph structures in certain cases. Meanwhile, parallel research on message-passing GNNs has demonstrated the benefits of simple paths (or self-avoiding walks) over random walks in enhancing model expressivity~\citep{michel2023path,graziani2023no}. These findings motivate the exploration of simple paths as a structural encoding mechanism in graph transformers.

    In this work, we introduce \textbf{Simple Path Structural Encoding (SPSE)}, a novel method for structural edge encoding that replaces RWSE in graph transformers. SPSE encodes graph structure by counting simple paths of varying lengths between node pairs, capturing richer structural information than random walks. To address the computational challenges of simple path counting, we propose an efficient algorithm based on successive DAG decompositions using depth-first search (DFS) and breadth-first search (BFS). This approach avoids the exponential memory costs of path enumeration, enabling scalability to long path lengths.
    \footnote{The Python code for the path counting algorithm is available on the project's page: \href{https://github.com/LouisBearing/Graph-SPSE-Encoding}{https://github.com/LouisBearing/Graph-SPSE-Encoding}.}

    We validate SPSE on extensive benchmarks, including molecular datasets from Benchmarking GNNs~\citep{dwivedi2023benchmarking}, Long-Range Graph Benchmarks~\citep{dwivedi2022long}, and Large-Scale Graph Regression Benchmarks~\citep{hu2021ogb}. SPSE consistently outperforms RWSE in graph-level and node-level tasks, demonstrating significant improvements in molecular and long-range datasets. 
    We also characterize limit cases of the algorithm, and identify situations in which the performance might be more sensitive to approximate path counts.

    The remainder of this paper is structured as follows. Section~\ref{sec:prelim} introduces key concepts and notations. Section~\ref{sec:rw_limits} analyzes the limitations of RWSE and motivates SPSE. The proposed path-counting algorithm and encoding method are detailed in Section~\ref{sec:method}. Finally, experimental results and related works are discussed in Sections~\ref{sec:expe} and~\ref{sec:rw}, respectively.
    
\section{Preliminaries}\label{sec:prelim}
    % In this section we introduce the basics used later in the article.
    % I think we do not need this sentence, are you fine with it?
\subsection{Graph Theory}
    % \noindent
    Let \(\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{X})\) be a graph, where \(\mathcal{V}\) is the set of nodes, \(\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}\) is the set of edges, and \(\mathbf{X} \in \mathbb{R}^{|\mathcal{V}| \times d}\) represents the node features of dimension $d$. The \textit{adjacency matrix} \(\mathbf{A} \in \{0, 1\}^{|\mathcal{V}| \times |\mathcal{V}|}\) is a square matrix that represents the connectivity of the graph, i.e. \(\mathbf{A}_{ij}\) is one if there is an edge between nodes \(i\) and \(j\), and zero otherwise. 
    The diagonal degree matrix \(\mathbf{D} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}\) is a square matrix where the diagonal element \(\mathbf{D}_{i,i}\) represents the degree of node $i$. Formally, $\mathbf{D}_{i,i} = \sum_j \mathbf{A}_{i,j}$
    and $\mathbf{D}_{i,j} = 0$ for $i\neq j$.
    
    \begin{definition}[Walk]
        Given a graph \(\mathcal{G}\), a \textbf{walk} is a finite sequence of nodes \(v_0, v_1, \dots, v_m\), where each consecutive pair of nodes \((v_i, v_{i+1})\) is connected by an edge, i.e., \((v_i, v_{i+1}) \in \mathcal{E}\). The number of edges in a walk is referred to as the \textbf{walk length}.
    \end{definition}
    
    \begin{definition}[Simple Path]
        A \textbf{simple path} (here indifferently called \textbf{path}), is a walk in which all nodes are distinct. The number of edges in a simple path is called the \textbf{simple path length} (or \textbf{path length}).
        Paths themselves constitute graphs called \textbf{path graphs}.
        The \textbf{distance} between two nodes is the length of the shortest path between these two nodes.
    \end{definition}
    % Diameter definition? Maximum length between two nodes in a graph.
    
    \begin{definition}[Cycle]
        A \textbf{cycle} is a walk where all nodes are distinct, except for the first and last ones which are identical.
        A graph composed of a single cycle is a \textbf{cycle graph}.%, formally $\mathcal{G}_{\mathcal{C}_n}$.
    \end{definition}
    
    \begin{definition}[Random Walk Matrix]
        Given a graph $\mathcal{G}$ and $k \in \mathbb{N}^*$, the $k$-hop \textbf{random walk matrix} $P_k \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|}$ gives the landing probabilities of random walks of length $k$ between all pairs of nodes. Its closed-form expression is $P_k=(D^{-1}A)^k$.
    \end{definition}
    
    
    \begin{definition}[Simple Path Matrix]
        Similarly, we define the $k$-hop simple path matrix $S_k \in \mathbb{N}^{|\mathcal{V}| \times |\mathcal{V}|}$ as the matrix where the ($i,j$)-th entry, $(S_k)_{ij}$, is the number of $k$-length simple paths from node 
        $i$ to node $j$.
    \end{definition}
    %Contrary to the $k$-hop random walk matrix $P_k$, the simple path matrix of undirected graphs is symmetric. \AL{true?}However, 
    Unlike $P_k$, $S_k$ does not have a closed-form solution, and computing it is computationally expensive~\citep{vassilevska2009finding}.
    In the following, we focus on the properties of pairs of nodes $(i,j) \in \mathcal{V} \times \mathcal{V}$ through different \textit{edge encoding} methodologies.
    To facilitate the comparison with the encoding of another pair of nodes $(i', j')$ in a second graph $\mathcal{G'}$, we introduce the following equivalence relation for $k \geq 1$:
    \begin{equation*}
    (i,j)^{\mathcal{G}} \overset{k}{=}_{\textsc{rw}} (i',j')^{\mathcal{G'}} \iff (P_k)_{ij} = (P'_k)_{i'j'},
    \end{equation*}
    where $P'_k$ is the $k$-hop random walk matrix of graph $\mathcal{G'}$.
    Note that this does not require $\mathcal{G'}$ to have the same number of nodes as $\mathcal{G}$.
    This can be generalized as:
    \begin{equation*}
    (i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}} \iff \forall k \in \mathbb{N}^*, (i,j)^{\mathcal{G}} \overset{k}{=}_{\textsc{rw}} (i',j')^{\mathcal{G'}}.
    \end{equation*}
    
    The same equivalence relations can be defined for simple paths (writing $\overset{k}{=}_{\textsc{sp}}$ and $=_{\textsc{sp}}$) by substituting $P_k$ and $P'_k$ with $S_k$ and $S'_k$.
    

    % \begin{figure*}[t!]
    %     \includegraphics[width=1.0\linewidth]{rw_vs_sp.jpg}
    %     \centering
    %     \caption{This figure illustrates different properties of RWSE and SPSE. \textbf{(Left)} RWSE encodes identically edges from very distinct graphs which are separated by SPSE (see the tree decompositions in appendix~\ref{appendix:tree_dec}). \textbf{(Right)} SPSE edge encoding of adjacent nodes characterizes the cycles to which the edge belongs. Thus bonds in the 6-atom cycles of (A) and (B) are encoded identically (they both belong to a single such cycle), and so are the (C=O) bonds of (C) and (D).
    %     }
    %     \label{fig:rw_vs_sp}
    % \end{figure*}
        %%%%%%%%%%%%%%

    \subsection{RWSE encoding in Graph Transformers}
    % \noindent
    % \AL{I think we have to define or at leas explain structural end positional encodings}
    Pure graph transformers, which do not use any MPNN layer, typically encode structural and positional information directly in the self-attention layer.
    To compute the RWSE matrix $E_{\textsc{rw}}$, all $k$-hop random walk matrices up to a maximum walk length $K$ are concatenated into a matrix $P=[P_1, \cdots, P_K] \in [0,1]^{|\mathcal{V}| \times |\mathcal{V}| \times K}$, which is then encoded through a shallow neural network $\phi_0$ that maps $K$ to a (usually larger) dimension $d$~\citep{menegaux2023self,ma2023graph}: $E_{\textsc{rw}} = \phi_0(P) \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}| \times d}$.
    $E_{\textsc{rw}}$ both acts as a relative positional encoding, since it contains the shortest path distance between nodes, and as a structural edge encoding, since walks encode information about the visited sub-structures.
    As a standard positional encoding, it biases the pair-wise alignment between nodes given by the attention matrix.
    Very generally, the resulting self-attention layer can be written as follows, where the precise implementation of $\phi_1$ and $\phi_2$ varies among different methods:
    \begin{align}
    &a_{ij} = \phi_1(W^Qx_i, W^Kx_j, (E_{\textsc{rw}})_{ij}) \label{eq:attn_1} \\
    &\alpha_{ij} = \frac{a_{ij}}{\sum_k a_{ik}} \\
    &y_i = \sum_j \alpha_{ij} \phi_2 (W^Vx_j, (E_{\textsc{rw}})_{ij}) \label{eq:attn_3} 
    \end{align}
    with $x_i$ and $x_j$ the features of nodes $i$ and $j$, $W^Q$, $W^K$ and $W^V$ the query, key and value matrices, and $y_i$ the output from the self-attention layer for node $i$.
    
    % \AL{is the shallow NN to convert P into $E_{rw}$ trained end-to-end? if it is the case, thane it is not clear form equation 1} 

\section{Theoretical Properties of RWSE and SPSE}\label{sec:rw_limits}
%Structural encodings shape the expressiveness of graph-based models. 
In this section, we highlight the limitations of RWSE in distinguishing different graph structures, as it can assign identical transition probabilities to edges in distinct topologies, potentially leading to suboptimal performance.
Conversely, we show that SPSE naturally captures cycle-related information. In particular, SPSE enables cycle counting, which is crucial in applications such as molecular chemistry (e.g., identifying functional groups like aromatic rings)~\citep{may2014efficient,agundez2023aromatic}, social network analysis (e.g., detecting communities)~\citep{radicchi2004defining,dhilber2020community}, and circuit design (e.g., analyzing feedback loops)~\citep{horowitz1989art}
    
    % The discussion centers on the properties of an edge $(i,j)$ in a graph $\mathcal{G}$ through different structural encoding methods (note that we do not require that the corresponding $A_{ij}$ is $1$), and how it compares to others edges $(i', j')$ in a second graph $\mathcal{G'}$. 
    % To that end, we introduce the following equality relation, where $P'_k$ is the $k$-hop random walk matrix of graph $\mathcal{G'}$:
    % \begin{align}
    % &(i,j)^{\mathcal{G}} \overset{k}{=}_{\textsc{rw}} (i',j')^{\mathcal{G'}} \iff (P_k)_{ij} = (P'_k)_{i'j'} \\
    % &(i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}} \iff \forall k \in \mathbb{N}, (i,j)^{\mathcal{G}} \overset{k}{=}_{\textsc{rw}} (i',j')^{\mathcal{G'}}.
    % \end{align}
    % It is then possible to define similar relations~$\overset{k}{=}_{\textsc{sp}}$ and~$=_{\textsc{sp}}$ by replacing $P_k$ with a matrix $S_k$, with $(S_k)_{ij}$ being the number of paths of length $k$ starting at $i$ and ending at $j$.
    
    % The following of this section is articulated around two results summarized in distinct propositions: RWSE is an ambiguous edge encoding method, and SPSE can be viewed as a cycle counting method for adjacent nodes.
    
    \subsection{Limitations of RWSE for Edge Encoding}
    

        
\begin{figure}[b!]
    \centering
    \includegraphics[width=0.8\linewidth]{cycle_path_fig.jpg}
    \caption{RWSE encodes identically edges from very distinct graphs which are separated by SPSE (see the tree decompositions in Appendix~\ref{appendix:tree_dec})}
    \label{fig:rw_vs_sp_cycle}
\end{figure}

    Random walk probabilities possess the great advantage of being computable in closed form.
    However, their use introduces certain ambiguities, as illustrated in \Cref{fig:rw_vs_sp_cycle}. 
    In these two examples, $\mathcal{G}_A$ and $\mathcal{G}_C$ are cycle graphs, $\mathcal{G}_B$ and $\mathcal{G}_D$ are path graphs, and yet the following two relations hold (see Appendix~\ref{appendix:tree_dec} for a graphical proof up to a depth of 5):
    \begin{align*}
        (0,1)^{\mathcal{G}_A} =_{\textsc{rw}} (0,1)^{\mathcal{G}_B}, \\
        (0,1)^{\mathcal{G}_C} =_{\textsc{rw}} (0,1)^{\mathcal{G}_D}.
    \end{align*}
    Although the edges belong to clearly different graphs, RWSE assigns them the same transition probabilities in the cycle and path graphs.
    These examples are in reality two special cases of the following result which links RWSE edge encodings in even-length cycle graphs and linear graphs (all proofs can be found in Appendix~\ref{appendix:proof}):
    
    \begin{proposition}
    \label{prop:cycles_path_graphs}
    Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be an even-length cycle graph, i.e. $|\mathcal{V}|=2n$ for some $n\in \mathbb{N}^*$, and $\mathcal{G'}= (\mathcal{V}', \mathcal{E}')$ a path graph such that $|\mathcal{V}'|=2n+1$.
    Then given any pair of nodes $(i,j)$ in $\mathcal{G}$, there exits a pair of nodes $(i',j')$ in $\mathcal{G'}$ such that $(i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}}$.
    \end{proposition}
    \noindent
    Each pair of nodes in an even-length cycle graph is thus equivalent, under RWSE encoding, to another pair of nodes in a linear graph.
    In other words, random walks transition probabilities cannot be used to distinguish between even-length cycles and paths when considering single node pairs.
    On the other end, it is easy to show that this does not apply to SPSE encoding (see \Cref{fig:rw_vs_sp_cycle}).
    This result illustrates how random walk-based structural encoding may fail to capture critical structural differences between graphs, hence possibly leading to suboptimal performances of the overall graph transformer model.

    It is also possible to prove a more general result about the ambiguities of RWSE by leveraging the fact that random walks measure probabilities rather than absolute counts.
    In particular, the following proposition can be easily proven by induction by considering central symmetries around destination node $j$.
    \begin{proposition}
    \label{prop:gen_res_rwse}
    Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph and $(i,j) \in \mathcal{V} \times \mathcal{V}$ a pair of nodes in $\mathcal{G}$.
    Then there exists a non-isomorphic graph $\mathcal{G'}= (\mathcal{V}', \mathcal{E}')$ and a pair of nodes $(i',j')\in \mathcal{V}' \times \mathcal{V}'$ such that $(i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}}$, i.e. $(i,j)$ and $(i',j')$ are equivalent under RWSE encoding.
    \end{proposition}
    \noindent
    The RWSE encoding of a pair of nodes (note that $i$ and $j$ need not be adjacent) is therefore never unique, and cannot be used to identify a graph.
    Of course this is also obviously true for SPSE encoding.
    However, proving this result for adjacent nodes, in the cases of simple paths, still requires preserving the set of paths that connect them.
    This hints at the fact that the information contained in simple path edge encodings may be used to distinguish certain local graph structures, as we discuss in the next section.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{cycle_count.jpg}
    \caption{SPSE edge encoding of adjacent nodes characterizes the cycles to which the edge belongs. Thus bonds in the 6-atom cycles of (A) and (B) are encoded identically (they both belong to a single such cycle), and so are the (C=O) bonds of (C) and (D).
    % \AL{Add C in figure + fix bad centering of node labels}
    }
    \label{fig:rw_vs_sp_cycle_count}
\end{figure}

    \subsection{SPSE through the Prism of Cycle Counting}
    
    The relation between path and cycle counting has been well studied~\citep{perepechko2009number,graziani2023no}.
    The following result, which was introduced by \citet{perepechko2009number}, connects SPSE encoding with cycle counting for adjacent nodes:
    \begin{proposition}
    \label{prop:cycle_counting}
    Let $(i,j)$ be two adjacent nodes in a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, i.e. $(i,j) \in \mathcal{E}$, and $S_k$ the $k$-hop simple path matrix of $\mathcal{G}$ for any $k \in \mathbb{N}^*$, such that $(S_k)_{ij} = m_k \in \mathbb{N}$.
    Then for $k \geq 2$, there are exactly $m_k$ cycles of length $k+1$ in $\mathcal{G}$ that admit $(i,j)$ as an edge.
    \end{proposition}
    \noindent
    The case of $k=1$ simply corresponds to the number of parallel edges between nodes $i$ and $j$.
    Note that this is not the same as giving the number of cycles that possess $i$ and $j$ as vertices.
    Two illustrations of this result are presented in \Cref{fig:rw_vs_sp_cycle_count}.
    Here, $(S_1)_{01} = (S_5)_{01} = 1$ unambiguously defines the edge $(0,1)$ as belonging to a six-atom cycle.
    In a second example, SPSE uniquely encodes the double carbon-oxygen bond of any carboxylic acid group.
    Note that Proposition~\ref{prop:cycle_counting} however does not hold if $(i,j) \notin \mathcal{E}$, and that no equivalent result exists for RWSE since the landing probabilities can be made arbitrarily low by the addition of new edges to a cycle's nodes.

    This analysis of structural encodings through the lens of cycles therefore showcases structures for which SPSE is provably more informative than RWSE encoding.


\section{Simple Path Structural Encoding}
    \label{sec:method}
% The increased expressive power of paths over walks was studied within the framework of MPNN~\cite{michel2023path,graziani2023no}, where message-passing follows short-length paths that are listed and stored in memory.
% Due to the combinatorial nature of the problem of enumerating all paths of non-trivial lengths, it is unrealistic to keep the corresponding node sequences in memory. Moreover, results from Section~\ref{sec:rw_limits} show that \textit{the total count of distinct paths} between any two nodes, while requiring significantly less memory to store, can overcome several limitations faced by random walk landing probabilities as en edge structural encoding method. This poses however several issues. First, although existing path counting methods propose efficient algorithms for  short-length paths~\cite{perepechko2009number,giscard2019general}, the graph topologies and path lengths considered here (between 15 and 23) require resorting to approximate methods.
% Second, as the number of paths between two nodes can grow very large (being only bounded by $\frac{(|\mathcal{V}| - 2)!}{(|\mathcal{V}| - k - 1)!}$ for paths of length $k$ when the graph is complete), one needs to devise an appropriate encoding function for such large numbers.
% This section addresses these two issues.
The superior expressivity of paths over walks in MPNNs~\citep{michel2023path,graziani2023no} has been studied, where message-passing follows short paths stored in memory. However, due to the combinatorial complexity of enumerating all paths, storing these sequences becomes impractical. Results from Section~\ref{sec:rw_limits} indicate that counting distinct paths between nodes, while requiring significantly less memory, still possesses theoretical advantages over random walk probabilities as an edge structural encoding method. This approach, however, introduces two challenges. 
First, while existing path-counting algorithms efficiently handle short paths~\citep{perepechko2009number,giscard2019general}, the graph topologies and path lengths considered here (15-23) necessitate approximate methods. Second, since the number of paths between two nodes can grow exponentially (bounded by $\frac{(|\mathcal{V}| - 2)!}{(|\mathcal{V}| - k - 1)!}$ for length-$k$ paths in a complete graph), an appropriate encoding function is required. 
This section addresses both challenges.

    
\begin{algorithm}[tb]
   \caption{Count paths between all pairs of nodes (simplified)}
   \label{algo:pathcount}
\begin{algorithmic}[1]
   \STATE {\bfseries Parameters:} Proportion of root nodes $R$, maximum length $K$, maximum DFS depth $D_\textsc{dfs}$, maximum trial number $N$
   \STATE {\bfseries Input:} Undirected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$
   \STATE {\bfseries Output:} Path count matrix $M \in \mathbb{N}^{|\mathcal{V}| \times |\mathcal{V}| \times K}$
   \STATE $M \gets \textbf{0}_{|\mathcal{V}| \times |\mathcal{V}| \times K}$ \COMMENT{\textit{Initialize count matrix}}
   \STATE $\textsc{Nodes} \gets$ \textsc{DrawNodes}($R$, $\mathcal{V}$)
   \COMMENT{\textit{Select $R \times |\mathcal{V}|$ nodes from $\mathcal{V}$}}
   \FOR{each $v$ {\bfseries in} \textsc{Nodes}}
       \STATE $\textsc{DAGs} \gets$ \textsc{DAGDecompose}($\mathcal{G}$, $v$, $D_\textsc{dfs}$, $N$) \COMMENT{\textit{Retrieve list of node permutations starting with $v$}}
       \FOR{each \textsc{DAG} {\bfseries in} \textsc{DAGs}}
           \STATE $M \gets$ \textsc{Update}($M$, \textsc{DAG}) \COMMENT{\textit{Update total path count}}
       \ENDFOR
   \ENDFOR
   \STATE {\bfseries Return:} $M$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tb]
    \caption{\textsc{DAGDecompose}: Decomposition of an input graph into multiple DAGs}
    \label{algo:dagdec}
 \begin{algorithmic}
    \STATE {\bfseries Parameters:} Maximum DFS depth $D_\textsc{dfs}$, maximum trial number $N$
    \STATE {\bfseries Input:} Graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, root node $r$, diameter $D_\textsc{max}$
    \STATE {\bfseries Output:} List of node orderings $\Pi$
    \STATE Initialize $\Pi \gets \text{EmptyList}$
    \FOR{$d_\textsc{dfs} = 0$ {\bfseries to} $D_\textsc{dfs}$}
       \FOR{$n = 1$ {\bfseries to} $N$}
          \STATE Initialize $\pi \gets \text{EmptyList}$
          \WHILE{$\pi \neq \mathcal{V}$}
             \FOR{$d = 1$ {\bfseries to} $D_\textsc{max}$}
                \IF{$d < d_\textsc{dfs}$}
                   \STATE $\pi \gets$ \textsc{DFS}($r$, $\mathcal{G}$, $\pi$) \COMMENT{\textit{Start with $d_\textsc{dfs}-1$ DFS steps}}
                \ELSIF{$d = d_\textsc{dfs}$}
                   \STATE $\pi \gets$ \textsc{partialBFS}($r$, $\mathcal{G}$, $\pi$)
                \ELSE
                   \STATE $\pi \gets$ \textsc{BFS}($r$, $\mathcal{G}$, $\pi$) \COMMENT{\textit{Continue with as many BFS steps as possible}}
                \ENDIF
             \ENDFOR
          \ENDWHILE
          \STATE $\Pi \gets \textsc{Add}(\Pi, \pi)$ \COMMENT{\textit{Append $\pi$ to $\Pi$}}
       \ENDFOR
    \ENDFOR
    \STATE {\bfseries Return:} $\Pi$
 \end{algorithmic}
 \end{algorithm}


    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%% Figure illustration algo
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{figure}
        \includegraphics[width=0.7\linewidth]{algo_illustr.jpg}
        \centering
        \caption{Discovering the two paths of length 3 between nodes $0$ and $1$ in this Circular Skip Link graph requires a partial BFS step in either $0$ or $1$ (not all child nodes are explored), followed by a BFS step.
        Nodes that are discovered concurrently are given an arbitrary order, allowing to travel back in the graph.}
        \label{fig:algo_illustr}
        \end{figure}
        %%%%%%%%%%%%%%

\subsection{Simple Path Counting}
The path counting method followed here relies on the extraction of node orderings from an input undirected graph.
Any such ordering can be used to turn the graph into a DAG, which allows to count paths by computing powers of the resulting adjacency matrix.
For paths of length $K$, this process incurs a computational complexity of $\mathcal{O}(K|\mathcal{V}|^3)$.
% The approximate path counting method followed here supposes to assign an ordering to all nodes in an input undirected graph and allow only directed edges towards higher order nodes, effectively turning the graph into a DAG.
% Counting paths then amounts to computing powers of the corresponding adjacency matrix up to a maximum distance $K$, which incurs a computational complexity of $\mathcal{O}(K|\mathcal{V}|^3)$.
Different DAG decompositions of the input undirected graph allow the exploration of different paths, leading to increasingly more accurate path counts.
We summarize the overall procedure in Algorithm~\ref{algo:pathcount}.
The detailed version of this algorithm can be found in Appendix~\ref{appendix:full_algo}.
It consists in updating running path counts for all pairs of nodes and path length $k$, by comparing the counts yielded by each DAG with the stored values.
The DAG mining function is presented in Algorithm~\ref{algo:dagdec}.
Starting from a root node, it decomposes a graph into a tree by combining DFS and BFS: a DFS search is initiated at a given root node, updating a list which is incremented with each newly visited node.
After it reaches a distance $D_\textsc{dfs}$ from the root node, the search switches to BFS until it cannot proceed further, and the whole process is repeated until all nodes are visited.
A DAG can be obtained from the node ordering by directing edges towards nodes of higher indices.
DFS allows long path discoveries but cannot simultaneously discover more than one path between nodes, while BFS alone will typically miss long paths, hence their use in combination.
We add a \textit{partial BFS} step between the two, which randomly explores a subset of the child nodes, thus allowing to travel through otherwise inaccessible paths (see Figure~\ref{fig:algo_illustr} for an illustration).
Additional parameters include a repetition number $N$ which accounts for random effects, such as the direction taken by the DFS, and a parameter $R$ controlling the proportion of root nodes.
The computational cost is dominated by the powers of the adjacency matrix which must be computed for each DAG, for a maximal complexity of $\mathcal{O}(KRD_\textsc{dfs}N|\mathcal{V}|^3)$.
The additional factor $RD_\textsc{dfs}N$ can be of the order of tens to hundreds depending on the input graph, which makes SPSE calculation significantly more expensive than RWSE.
This needs however to be computed only once as a pre-processing step, and it is also much less than the total number of possible DAG decompositions $2^{|\mathcal{E}|}$, highlighting the effectiveness of the tree decomposition approach.
We discuss the choice of hyperparameters and their importance regarding path counts in Section~\ref{subsec:abla_count} and provide actual values in Appendix~\ref{appendix:path_stats}.

\subsection{Path Count Encoding}

Compositions of logarithm functions are used to map the obtained path count matrix $S = [S_1, \cdots, S_K]$ to a manageable value range for subsequent neural networks, as total counts can grow very large.
Using superscript to denote the composition of a function with itself, we use the following mapping $f$ for a total count $x$:
\begin{equation}
\label{eq:spse_encoding}
    f: x\mapsto \alpha g^n (x) + \beta,
\end{equation}
with $g: x\mapsto \ln(1+x)$, and $\alpha$, $\beta$ and $n$ being hyperparameters to be adjusted for different graph collections.
Normalized path counts $f(S)$ can then be used in place of the random walk matrix $P$ as input to the edge encoding network of graph transformer models, yielding the SPSE matrix $E_\textsc{sp}$ which replaces $E_\textsc{rw}$ in equations~\ref{eq:attn_1} and \ref{eq:attn_3}.


\section{Experiments}\label{sec:expe}
We first validate \Cref{prop:cycle_counting} experimentally through a synthetic experiment (\Cref{subsec:synt_exp}), and demonstrate the empirical superiority of SPSE on real-world datasets (\Cref{subsec:real_exp}).
We then present an ablation study on the algorithm parameters (\Cref{subsec:abla_count}) and discuss limitations (\Cref{sec:limitations}).


%\subsection{Cycle Counting Synthetic Experiment}\label{subsec:synt_exp} 
% This first section aims to validate experimentally the superior ability of SPSE encoding over RWSE for cycle counting implied by Proposition~\ref{prop:cycle_counting}.


\subsection{Cycle Counting Synthetic Experiment}\label{subsec:synt_exp}
\paragraph{Synthetic Dataset.}
To validate \Cref{prop:cycle_counting}, we design a synthetic dataset consisting of 12,000 graphs. Each graph is generated by randomly adding cycles of lengths between 3 and 8 until the total count for each cycle length reaches a value between 0 and 14. This process results in graphs with an average of 149 nodes and 190 edges.
The dataset is split into training (10,000 graphs), validation (1,000 graphs), and test (1,000 graphs) sets. The objective is to determine the number of cycles for each of the six cycle lengths. We frame this as six simultaneous multiclass classification tasks and evaluate performance using mean accuracy. Examples of the generated graphs are provided in \Cref{appendix:synt_graph_example}.

% To achieve this, a synthetic dataset of 12,000 graphs is generated, comprising 
% This results in synthetic graphs composed of an average of 149 nodes and 190 edges.
% Examples of such graphs are represented in Fig.~\ref{fig:synthe_graphs}.
% The task is then to retrieve the precise cycle count for all six cycle length values.
% It is implemented as six simultaneous multiclass classification tasks, for which we monitor the mean accuracy.

% E_\textsc{rw}, E_\textsc{sp}
\paragraph{Models.}
We build upon two state-of-the-art graph transformer models that use RWSE as an edge encoding method: GRIT~\citep{ma2023graph} and CSA~\citep{menegaux2023self}. SPSE can seamlessly replace RWSE in these models by substituting the encoding matrix $E_\textsc{sp}$, which captures path counts, for $E_\textsc{rw}$ in equations~\ref{eq:attn_1} and \ref{eq:attn_3}.
To evaluate performance on the cycle counting task, we train these models using three hyperparameter configurations adopted from~\citep{menegaux2023self}. These correspond to the setups used for ZINC (config \#1), PATTERN (config \#2), and CIFAR10 (config \#3), covering a range of model complexities from 40 to 280 gigaflops. This provides a comprehensive assessment of the impact of the two edge encoding methods across diverse settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Synthetic Exp results 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t!]
    \includegraphics[width=1.0\linewidth]{synthe_res.jpg}
    \centering
    \caption{Cycle counting accuracies for three training configurations of CSA and GRIT with either RWSE or SPSE edge encoding.}
    \label{fig:synthe_res}
\end{figure}
%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Benchmarking GNN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t]
\caption{Test results on all eight benchmarks. GPS, CSA \& GRIT w/ and w/o SPSE are (re-)trained on 10 random seeds (hence variations from seminal works) on all datasets (one run for PCQM4Mv2). \underline{Underline} indicates a difference between a baseline and the SPSE model version, and \textbf{*} is used for significant gaps based on a two-sided t-test ($p$-value $\leq 0.05$). Highlighted are \best{best}, \second{second}, and \third{third} best scores.}
\label{tab:benchgnn}
\begin{center}
\resizebox{\textwidth}{!}{
%{\scriptsize
\begin{tabular}{l cccc|cc|cc}
\toprule
& \multicolumn{4}{c}{\cellcolor{mycustomcolor} \textbf{Molecular}} & \multicolumn{2}{c}{\cellcolor{mycustomcolor2}\textbf{SBM}} & \multicolumn{2}{c}{\cellcolor{mycustomcolor3}\textbf{Superpixel}} \\
\cmidrule(l{2pt}r{2pt}){2-5}
\cmidrule(l{2pt}r{2pt}){6-7}
\cmidrule(l{2pt}r{2pt}){8-9}
 \textbf{Model} & \textbf{ZINC} & \textbf{Peptides-func}& \textbf{Peptides-struct} & \textbf{PCQM4Mv2} & \textbf{PATTERN} & \textbf{CLUSTER} & \textbf{MNIST} & \textbf{CIFAR10} \\
 \cmidrule(lr){2-9}
% \midrule
 & $\text{\textbf{MAE}} \downarrow$ & $\text{\textbf{AP}} \uparrow$  & $\text{\textbf{MAE}} \downarrow$ & $\text{\textbf{MAE}} \downarrow$& $\text{\textbf{Accuracy}} \uparrow$ & $\text{\textbf{Accuracy}} \uparrow$ & $\text{\textbf{Accuracy}} \uparrow$ & $\text{\textbf{Accuracy}} \uparrow$\\
\midrule

GCN & $0.367 \pm 0.011$ &$ - $&$ - $& $0.1379$& $71.892 \pm 0.334$ & $68.498 \pm 0.976$ & $90.705 \pm 0.218$ & $55.710 \pm 0.381$ \\
GIN & $0.526 \pm 0.051$ &$ - $&$ - $& $0.1195$& $85.387 \pm 0.136$ & $64.716 \pm 1.553$ & $96.485 \pm 0.252$ & $55.255 \pm 1.527$  \\
GAT & $0.384 \pm 0.007$ & $ - $&$ - $& $-$  & $78.271 \pm 0.186$ & $70.587 \pm 0.447$ & $95.535 \pm 0.205$ & $64.223 \pm 0.455$ \\
GINE &$ - $& $0.5498 \pm 0.0079$ & $0.3547 \pm 0.0045$ &$ - $&$ - $&$ - $&$ - $&$ - $ \\
GatedGCN & $0.282 \pm 0.015$ &$ - $&$ - $&$ - $ & $85.568 \pm 0.088$ & $73.840 \pm 0.326$ & $97.340 \pm 0.143$ & $67.312 \pm 0.311$ \\
GIN-AK+ & $0.080 \pm 0.001$ &$ - $&$ - $&$ - $ & $86.850 \pm 0.057$ &$ - $&$ - $& $72.190 \pm 0.130$ \\

\midrule

SAN (+RWSE) & $0.139 \pm 0.006$ & $0.6439 \pm 0.0075$ & $0.2545 \pm 0.0012$ & $-$ & $86.581 \pm 0.037$ & $76.691 \pm 0.65$ & $-$ & $-$ \\
Graphormer & $0.122 \pm 0.006$& $-$ & $-$ & \third{$\textbf{0.0864}$} & $-$ & $-$ & $-$ & $-$ \\
EGT & $0.108 \pm 0.009$ & $-$ & $-$ & $-$ & $86.821 \pm 0.020 $ & \third{$\textbf{79.232} \pm \textbf{0.348}$} & $98.173 \pm 0.087$ & $68.702 \pm 0.409$ \\
GraphViT & $0.073 \pm 0.001$ & \second{$\textbf{0.6970} \pm \textbf{0.0080}$} & \second{$\textbf{0.2475} \pm \textbf{0.0015}$} & $-$ & $-$ & $-$ & $97.422 \pm 0.110$ & $73.961 \pm 0.330$ \\
Exphormer & $-$ & $0.6527 \pm 0.0043$ & $0.2481 \pm 0.0007$ & $-$ & $86.742 \pm 0.015$ & $78.071 \pm 0.037$ & \best{$\textbf{98.550} \pm \textbf{0.039}$} & \third{$\textbf{74.696} \pm \textbf{0.125}$} \\
Drew & $-$ & \best{$\textbf{0.7150} \pm \textbf{0.0044}$} & $0.2536 \pm 0.0015$ & $-$ & $-$ & $-$ & $-$ & $-$ \\

\midrule

 \multicolumn{4}{l}{\firstusage{}} &&&&& \\
 %\gps~\cite{rampavsek2022recipe}
\gps & $0.070 {\pm 0.003}$ & $0.6601 \pm 0.0061$  & $0.2509 \pm 0.0017$  & $0.0942$ & $86.688 {\pm 0.073}$ & $77.969 {\pm 0.161}$ & $98.064 {\pm 0.157}$ & $72.097 {\pm 0.475}$ \\
%\gps + RWSE & $0.072 {\pm 0.003}$ & $\best{\textbf{86.86} {\pm \textbf{0.02}}}$ & $\best{\textbf{78.99} {\pm \textbf{0.11}}}$ & $98.10 {\pm 0.11}$ & $71.88 {\pm 0.52}$ & $65.66 \pm 0.73$ & $0.2510 \pm 0.0013$ & $\best{0.0918}$ \\
\textbf{\gps + SPSE} \textit{(ours)} & \underline{$0.068 {\pm 0.003}$} & \underline{$0.6608 \pm 0.0063$} & \underline{$0.2506 \pm 0.0010$} & \underline{$0.0934$} & \underline{$86.834 {\pm 0.025}$}\textbf{*} & \underline{$78.440 {\pm 0.177}$}\textbf{*} & \underline{$98.105 {\pm 0.158}$} & \underline{$72.114 {\pm 0.462}$} \\

\midrule

 \multicolumn{4}{l}{\secondusage}  &&&&&  \\
%\rwsecsa~\cite{menegaux2023self}
\rwsecsa & $0.069 {\pm 0.003}$ & $0.6513 \pm 0.0061$ & \underline{$0.24862 \pm 0.0012$} & $0.0918$ & $87.008 {\pm 0.062}$ & \underline{$79.071 {\pm 0.120}$} & $98.127 {\pm 0.123}$ & $73.885 {\pm 0.348}$  \\
\spsecsa \xspace \textit{(ours)} & \second{\underline{$\textbf{0.061} {\pm \textbf{0.003}}$}}\textbf{*} & \underline{$0.6605 \pm 0.0096$}\textbf{*} & $0.24945 \pm 0.0018$ & \underline{$0.0911$}  & \third{\underline{$\textbf{87.064} {\pm \textbf{0.052}}$}}\textbf{*} & $78.940 {\pm 0.132}$ & $\third{\underline{\textbf{98.269} {\pm \textbf{0.078}}}}$\textbf{*} & \underline{$73.897 {\pm 0.524}$}  \\
\midrule
\rwsegrit  & $\third{\textbf{0.065} {\pm \textbf{0.005}}}$ & $0.6803 {\pm 0.0085}$ & \third{$\textbf{0.2480} {\pm \textbf{0.0025}}$} & \second{$\textbf{0.0838}$} &  \second{$\textbf{87.229} {\pm \textbf{0.056}}$} & $\best{\underline{\textbf{79.730} {\pm \textbf{0.189}}}}$ & $98.231 {\pm 0.197}$ & \second{$\textbf{76.246} {\pm \textbf{0.954}}$}  \\
\spsegrit \xspace \textit{(ours)}& $\best{\underline{\textbf{0.059} {\pm \textbf{0.001}}}}$\textbf{*} & \third{\underline{$\textbf{0.6945} {\pm \textbf{0.0113}}$}}\textbf{*} &$\best{\underline{\textbf{0.2449} {\pm \textbf{0.0018}}}}$\textbf{*}  &  $\best{\underline{\textbf{0.0831}}}$ &  $\best{\underline{\textbf{87.235} {\pm \textbf{0.040}}}}$ & \second{$\textbf{79.571} {\pm \textbf{0.122}}$} & \second{\underline{$\textbf{98.294} {\pm \textbf{0.147}}$}} & $\best{\underline{\textbf{77.022} {\pm \textbf{0.430}}}}$\textbf{*} \\


\bottomrule
\end{tabular}}
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%

\paragraph{Results.}

The test accuracy for both model architectures across the three training configurations is reported in \Cref{fig:synthe_res}, along with standard deviations computed over 10 runs with different random seeds.
In all but one case, SPSE encoding achieves significantly higher cycle counting accuracy than RWSE.
All models learn to count cycles almost perfectly under the third training configuration, which suggests that deep architectures can compensate for expressivity limitations in the edge encoding matrix (see \Cref{appendix:synt_graph_example} for details on configurations).
However, SPSE still performs significantly better than RWSE for CSA in this setting.

These results empirically validate the superior ability of simple paths to characterize cycles when used as an edge encoding method in graph transformers.


\subsection{Real-World Benchmarks}\label{subsec:real_exp}

\paragraph{Datasets.}
We conduct experiments on graph datasets from three distinct benchmarks, covering both node- and graph-level tasks. These include ZINC, CLUSTER, PATTERN, MNIST, and CIFAR10 from Benchmarking GNNs~\citep{dwivedi2023benchmarking}, Peptides-functional and Peptides-structural from the Long-Range Graph Benchmark~\citep{dwivedi2022long}, and the 3.7M-sample PCQM4Mv2 dataset from the Large-scale Graph Regression Benchmark~\citep{hu2021ogb}.
We shall see in~\Cref{subsec:abla_count} how constraints imposed by graph complexity and dataset sizes impact the path count precision in molecular datasets (ZINC, the two Peptides \& PCQM4Mv2), image superpixel (MNIST \& CIFAR10) and Stochastic Block Model (SBM) (PATTERN \& CLUSTER) benchmarks, justifying a differenciated treatment.

%Next, we analyze how graph complexity and dataset size affect path count precision across different domains: molecular datasets (ZINC, Peptides-functional, Peptides-structural, and PCQM4Mv2), image superpixels (MNIST and CIFAR10), and Stochastic Block Model (SBM) datasets (PATTERN and CLUSTER). These variations motivate the need for distinct treatment of each benchmark.

%Experiments are conducted on graph datasets originating from three distinct benchmarks, featuring both node- and graph-level tasks: ZINC, CLUSTER, PATTERN, MNIST, and CIFAR10 datasets, introduced in \textbf{Benchmarking GNNs}~\cite{dwivedi2023benchmarking}, Peptides-functional and Peptides-structural from the \textbf{Long-Range Graph Benchmark}~\cite{dwivedi2022long}, and the 3.7M samples PCQM4Mv2 dataset from the \textbf{Large-scale Graph Regression Benchmark}~\cite{hu2021ogb}. 
%We shall see now how constraints imposed by graph complexity and dataset sizes impact the path count precision in molecular datasets (ZINC, the two Peptides \& PCQM4Mv2), image superpixel (MNIST \& CIFAR10) and Stochastic Block Model (SBM) (PATTERN \& CLUSTER) collections, justifying a distinct treatment of these benchmarks.


\paragraph{Experimental Setup.}

As before we replace RWSE with SPSE in GRIT and CSA.
We also explore adding SPSE to the well-known \textbf{GraphGPS} model~\citep{rampavsek2022recipe}, in which case the edge encoding is restricted to the elements of $\mathcal{E}$, and is only used to produce node-level positional encodings in the MPNN layer.
In all cases, for better result robustness, \textit{we retrain the original model and the SPSE version on ten random seeds} (one seed for the large PCQM4Mv2) using the released training configurations, with two exceptions: the unstable CSA learning rate for ZINC is reduced by a factor 2, and configuration files are added to train CSA on Peptides.

It is important to emphasize that \emph{no hyperparameter tuning} is performed. This decision ensures a fair and unbiased comparison, isolating the contribution of SPSE as a drop-in replacement for RWSE, and demonstrating its effectiveness across different architectures without the need for task-specific adjustments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Figure ablation path count
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t!]
    \includegraphics[width=0.92\linewidth]{abla_count.jpg}
    \centering
    \caption{Proportion of discovered paths and computation time per sample when varying one hyperparameter at a time, across ZINC, PATTERN, and MNIST datasets. Solid lines (left y-axis) represent the proportion of discovered paths, while dashed lines (right y-axis) indicate computation time per graph sample.}
    \label{fig:abla_path_count}
\end{figure*}

Note that replacing walks by path count is done at \textit{no additional cost} as the number of trainable parameters remains unchanged.
Two-sided Student's t-tests are conducted to assess the significance of the obtained results.
Finally, we compare with the following GNN methods: GCN~\citep{kipf2016semi}, GIN~\citep{xu2018how}, GAT~\citep{velickovic2017graph}, GatedGCN~\citep{bresson2017residual} and GIN-AK+~\citep{zhao2021stars}, along with the graph transformers: SAN~\citep{kreuzer2021rethinking}, Graphormer~\citep{ying2021transformers}, EGT~\citep{hussain2022global}, GraphViT~\citep{he2023generalization}, Exphormer~\citep{shirzad2023exphormer} and Drew~\citep{gutteridge2023drew}.
Results are reported in Table~\ref{tab:benchgnn}.


\paragraph{Results and Discussion}
Replacing RWSE with SPSE improves performance in 21 out of 24 cases (underlined in \Cref{tab:benchgnn}), with variations depending on the benchmark and model. The most notable gains are observed for CSA and GRIT on molecular graphs, where SPSE achieves statistically significant improvements in 5 out of 6 cases (marked with "*").
Improvements are also evident on superpixel benchmarks, with two cases showing statistically significant differences, on both CSA and GRIT. These results indicate that leveraging the full structural encoding matrix in self-attention layers, rather than restricting it to $\mathcal{E}$ as done in GPS, is an effective strategy for enhancing performance.
The limited improvement for GPS is however also likely due to the constraint of leaving the total number of parameters unchanged. In practice, this amounts to reducing the dimensionality of existing edge embeddings when using SPSE, limiting its potential benefit. This explanation is further supported by the significant improvements observed on SBM benchmarks, where GPS does not incorporate any edge encoding.
In contrast, pure graph transformers do not benefit from SPSE on CLUSTER. A possible explanation is the increased difficulty of accurately counting paths in the densely connected SBM graphs, leading to underestimated counts. Further details on these limitations are provided in \Cref{sec:limitations}.


\subsection{Path Count Sensitivity to Hyperparameters}\label{subsec:abla_count}
In this section, we examine how the number of paths discovered by \Cref{algo:pathcount} is affected by variations in hyperparameters across different benchmarks.
% the interplay among the three main hyperparameters of \Cref{algo:pathcount}:
Those parameters are the proportion of root nodes $R$, the maximum DFS depth $D_\textsc{dfs}$ and the number of trials $N$ (path length $K$ is fixed to walk lengths).
% To ensure a representative evaluation across different domains, we select 
ZINC, PATTERN, and MNIST are used as representative benchmarks for molecular graphs, SBM, and superpixels, respectively.
We measure the average proportion of discovered paths relative to a canonical configuration (reported in \Cref{appendix:path_stats}) and the computation time per sample while varying a single hyperparameter.
% All other hyperparameters remain fixed to the values reported in \Cref{appendix:path_stats}.
Results are reported in \Cref{fig:abla_path_count}, with path count proportions as solid lines (left y-axis) and computation time as dashed lines (right y-axis).
% The results, reported in \Cref{fig:abla_path_count}, illustrate the path count proportion (solid lines, left y-axis) and computation time (dashed lines, right y-axis) for each setting.
The main parameter controlling the path count precision for ZINC and other molecular datasets is the root node proportion $R$ which can be set to its maximum value ($R=1$) due to its low computational cost.
% For ZINC, the most influential hyperparameter is the root node proportion ($R$), which can be set to its maximum value ($R=1$) due to its low computational cost. 
In contrast, DFS depth and the number of trials have little to no impact on path counts and can be kept at moderate or low values. 
MNIST benefits almost equally from increases in all three hyperparameters, though adjusting $D_\textsc{dfs}$ offers the lowest computational overhead. 
Notably, the trade-off setting of $N=2$, imposed by the large size of the dataset, results in approximately $40\%$ fewer discovered paths compared to $N=8$, potentially leading to suboptimal learning accuracy.
% However, a more in-depth exploration of path counts in superpixel graphs is left for future work.
For the PATTERN dataset, we prioritize increasing $N$ over other hyperparameters, as it provides the most computationally efficient way to improve path discovery.
In this case, however, we expect a non-negligible fraction of paths to be missed due to the highly connected nature of SBM graphs, which sets a limit on the number of discoverable paths (see \Cref{sec:limitations} for further discussion).
% However, due to the highly connected nature of SBM graphs, a substantial fraction of paths is inherently undiscoverable, setting a natural limit on path enumeration. Further discussion on the proposed approach is provided in \Cref{sec:limitations}.




%Figure~\ref{fig:abla_path_count} illustrates how the number of paths discovered by Algorithm~\ref{algo:pathcount} is affected by variations in hyperparameters and graph features.Here is reported, the average proportion of paths that are discovered compared to a canonical configuration (corresponding, up to minor variations, to the setting of Table~\ref{appendix:path_stats}) when varying one hyperparameter, along with the computation time per sample.The main parameter controlling the path count precision for ZINC and other molecular datasets is the root node proportion $R$. Owing to a short computation time per sample, this parameter can be set to its maximal value of $1$. On the contrary, DFS depth $D_\textsc{dfs}$ and trial number $N$ have little to no influence on path counts, and can take medium to small values. Superpixel graphs like MNIST benefit almost equally from increases in all parameters, although controlling $D_\textsc{dfs}$ comes at the lowest computational cost. It is clear that the trade-off value $N=2$ of the canonical configuration results in a large number of missed paths ($\sim 40\%$ less than for $N=8$) and possibly sub-optimal learning accuracies. However due to time constraints a more thorough exploration of path counts in superpixel graphs is left as future work. Finally, for the two SBM datasets, the emphasis is put on $N$ whose increase comes at the lowest computational price. In this case, we expect a non-negligible fraction of paths to be missed due to the geometry of SBM graphs which, being highly connected, set a limit on the number of discoverable paths (see Discussion below).


% this lets us foresee where gains are possible: mnist / cif but not pattern / clust



\subsection{Limitations}\label{sec:limitations}
% After \Cref{algo:dagdec} computes node orderings starting from a root node $r$, \Cref{algo:pathcount} selects the longest path between the root $r$ and other connected nodes.

In the proposed algorithm, the path count between two nodes is updated by comparing the value provided by each new DAG with the total currently stored in memory.
The latter is then replaced by the maximum of the two values.
This approach is necessary since paths are not enumerated because of memory constraints.
The obtained counts therefore constitute lower bounds on the exact number of paths (see line 16 of \Cref{algo:path_count_long} in \Cref{appendix:full_algo}). 
% Failing to distinguish between individual paths can however lead to failure cases.
Failure cases can however arise because individual paths are not distinguished.
% However, this simplification can lead to failures in certain cases. 
For instance, consider the input graph illustrated in \Cref{fig:algo_limits}. The total number of paths of length 4 between nodes 0 and 1 is two (blue and green). 
The only directed graphs that would allow to discover these paths simultaneously are not acyclic: our algorithm can simply count them one at a time.
% However, these paths cannot form a DAG rooted in node 0 because doing so would violate the acyclicity constraint of DAGs.

% This limitation becomes particularly evident in high-density graphs (see \Cref{tab:path_stats} in \Cref{appendix:path_stats}). In fact, for datasets like PATTERN and CLUSTER, where graph density is high, our method does not yield any statistically significant improvement.

This limitation is especially true in high-density graphs: in fact our method does not yield any statistically significant improvement on the dense CLUSTER dataset.


% Our \Cref{algo:pathcount} 
% Figure~\ref{fig:algo_limits} presents an example of a graph where counting paths exhaustively requires discovering nodes along cycles, which cannot be done by relying only on DAG decompositions.
% Such cases are expected to represent a non-negligible proportion of total path counts in CLUSTER and PATTERN.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Figure algo limitations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \includegraphics[width=0.9\linewidth]{algo_limit.jpg}
    \centering
    \caption{Discovering simultaneously the two paths of length 4 between nodes $0$ and $1$ requires the directed graph decomposition on the right, which violates the acyclic property.}
    \label{fig:algo_limits}
    \end{figure}
%%%%%%%%%%%%%


\section{Related Work}
\label{sec:rw}
\textbf{Graph Transformers.} Graph transformers are a class of models designed to process graph-structured data by leveraging self-attention, which provides a graph-wide receptive field~\citep{vaswani2017attention,ying2021transformers}, thereby avoiding common limitations of message-passing GNNs~\citep{topping2022understanding}. Despite their advantages, graph transformers have not yet become the dominant architecture in graph learning, primarily due to the lack of unified structural and positional encodings that generalize well across diverse graph tasks~\citep{rampavsek2022recipe}. This is evident from the continued success of hybrid approaches that integrate message-passing with global self-attention~\citep{wu2021representing,rampavsek2022recipe,he2023generalization,choi2024topology}, highlighting ongoing opportunities for architectural improvements. Promising research directions include: more flexible spectral attention mechanisms~\citep{bo2023specformer}, models that mitigate the quadratic memory footprint of self-attention~\citep{shirzad2023exphormer}, and hierarchical encodings that enhance structural representations~\citep{luo2024enhancing}. \\
\textbf{Expressivity of Path-Based MPNNs.} The expressivity of message-passing neural networks (MPNNs) on paths has been studied in relation to the Weisfeiler-Leman (WL) isomorphism test~\citep{michel2023path,graziani2023no}. Notably, \citet{graziani2023no} demonstrated that iteratively updating node features via message-passing along paths originating from these nodes is strictly more expressive than the 1-WL test. However, these works consider only short path lengths, limiting their applicability to more complex graph structures. \\
\textbf{Counting Paths and Cycles in Graphs.} The problem of counting paths and cycles in graphs has been extensively studied over the past decades~\citep{johnson1975finding,alon1997finding,flum2004parameterized}. \citet{perepechko2009number} derived an explicit formula for counting paths of length up to 6 between two nodes, while \citet{giscard2019general} proposed an algorithm for counting paths of any length based on enumerating connected induced subgraphs. However, these methods become impractical for the large graphs and long path lengths considered in our work, necessitating the use of efficient approximate counting methods.



\section{Conclusion}
This work introduced Simple Path Structural Encoding (SPSE), a novel structural encoding method for graph transformers that leverages path counts instead of random walk probabilities. By providing a more expressive and structurally informative edge encoding, SPSE improves performance across various graph learning benchmarks. Our theoretical and experimental study shows that SPSE mitigates the limitations of random walk-based encodings while maintaining computational feasibility through an efficient approximation algorithm. While promising, SPSE's applicability to extremely large-scale graphs and its interaction with different transformer architectures would require further exploration. Future research directions may also include optimizing its computational efficiency and extending its applicability to broader domains such as knowledge graphs and large social networks.


\section*{Impact Statement}
This paper presents Simple Path Structural Encoding (SPSE), a novel structural encoding method for graph transformers, aimed at enhancing the expressivity of self-attention mechanisms by capturing richer structural patterns than existing random walk-based encodings. By improving edge encodings with simple path counts, SPSE contributes to the broader field of graph representation learning, offering a more robust method for tasks involving molecular graphs, social networks, and long-range dependencies.

The societal impact of this work aligns with the broader advancements in Machine Learning and Graph Neural Networks, with potential applications in drug discovery, recommendation systems, and scientific knowledge extraction. However, as with any machine learning model, the deployment of SPSE-based architectures should be approached with considerations for fairness, interpretability, and robustness, particularly in high-stakes applications.

We do not foresee any immediate ethical concerns or risks associated with our contributions beyond those generally applicable to graph machine learning. Future work should consider potential biases in training data and energy efficiency of graph transformers, as large-scale models can have computationally intensive requirements.
%This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


% \bibliographystyle{icml2025}
\bibliographystyle{unsrtnat}
\bibliography{example_paper}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn




\section{Additional Path Count Statistics and Training Hyperparameters}
\label{appendix:path_stats}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% SPSE calculation stats
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Statistic, path counting hyperparameters and total counting time per dataset with the Python implementation of the algorithm.}
\label{tab:path_stats}
\begin{center}
% \resizebox{1.0\textwidth}{!}{
{
% {\small
\begin{tabular}{l c c c c c c c}
\toprule
 \textbf{Dataset}&ZINC & PATTERN & CLUSTER & MNIST & CIFAR10 & Peptides & PCQM4Mv2\\
 % \cmidrule(lr){2-6}
 \midrule
\multicolumn{5}{l}{\textit{Dataset Statistics}} \\
\# of graphs & 12,000 & 14,000 & 12,000 & 60,000 & 70,000 & 15,535 & 3,746,620 \\
Avg. \# of nodes per sample & 23.2 & 118.9 & 117.2 & 70.6 & 117.6 & 150.9 & 14.1 \\
Avg. \# of edges per sample & 24.9 & 3,039.3 & 2,150.9 & 564.5 & 941.1 & 307.3 & 14.6 \\
Avg. density                & 0.09 & 0.43   &0.36       & 0.23 & 0.14 & 0.27 & 0.16\\  
\midrule
\multicolumn{5}{l}{\textit{Path count}} \\
$R$ (\# of root nodes, \% of $|\mathcal{V}|$) & 100\% & 40\% & 40\% & 55\% & 55\% & 100\% & 100\% \\
$K$ (max length) & 20 & 16 & 16 & 17 & 17 & 23 & 15 \\
$D_\textsc{dfs}$ (max DFS depth) & 6 & 2 & 2 & 11 & 11 & 4 & 6 \\
$N$ (\# of trials per depth) & 1 & 7 & 7 & 2 & 2 & 1 & 1 \\
Time (hr) & 1 & 60 & 39 & 34 & 80 & 48 & 80 \\

\midrule
\multicolumn{5}{l}{\textit{Model training}} \\
$\alpha$ & 0.5 & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 0.5 \\
$\beta$ & 0 & -0.2 & -0.2 & -0.2 & -0.2 & -0.2 & 0 \\
$n$ & 1 & 3 & 3 & 3 & 3 & 2 & 1 \\

\bottomrule
\end{tabular}
}
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%

Typical hyperparameter values for path counts with the respective computing duration are reported in Table~\ref{tab:path_stats}.
The hyperparameter search was conducted on a reduced pool of graphs and root nodes to optimize the trade-off between exhaustivity and duration.
The reported figures should therefore not be viewed as optimal, but rather give an indication of how they affect the path count duration. 
The latter ranges from under one hour for ZINC to a bit more than three days for CIFAR10 and the large PCQM4Mv2 dataset which contains 3.7M graphs.
Once computed, path counts are stored and used for model training at no additional cost.
In Table~\ref{tab:path_stats} are also reported values of $\alpha$, $\beta$ and $n$ training hyperparameters (see equation~\ref{eq:spse_encoding}).

\section{Proofs}
\label{appendix:proof}

\subsection{Proof of Proposition~\ref{prop:cycles_path_graphs}}
\label{appendix:proof_cycle_path}

\paragraph{Proposition.}
\textit{Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be an even-length cycle graph, i.e. $|\mathcal{V}|=2n$ for some $n\in \mathbb{N}^*$, and $\mathcal{G'}= (\mathcal{V}', \mathcal{E}')$ a path graph such that $|\mathcal{V}'|=2n+1$.
Then given any pair of nodes $(i,j)$ in $\mathcal{G}$, there exits a pair of nodes $(i',j')$ in $\mathcal{G'}$ such that $(i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}}$.}

\begin{proof}
The proof is done by induction by considering landing probabilities of walks of length $k$ and $k+1$ for any given $k$ in $\mathbb{N^*}$.
Do do that, we use the following notations:
\begin{equation*}
    p_k(i,j) = (P_k)_{ij}
\end{equation*}
where $P_k$ is the $k$-hop random walk matrix of $\mathcal{G}$, and $(P_k)_{ij}$ is the random walk probability of length $k$ between nodes $i$ and $j$.
We also take advantage of the symmetries of the circular graph $\mathcal{G}$ to denote as $p_k(j)$ the $k$-hop walk probability between any two nodes distant of $j \leq n$.
As $\mathcal{G}'$ is a linear graph with an odd number of nodes, we also use the notation 
\begin{equation*}
    p'_k(i,j) = (P'_k)_{ij},
\end{equation*}
with $P'_k$ the $k$-hop random walk matrix of $\mathcal{G}'$, but use this time node indices to represent the distance from the central node of the graph.
Hence $p'_k(0, n)$ is the $k$-hop walk probability between the central node and any of the two extremity nodes.
Similarly, for all $1 \leq j \leq n$, $p'_k(j)$ refers to $k$-hop walks originating from the central node and ending on one of the two nodes distant of $j$.

The first part of the proof consists in showing that, for all $1 \leq j < n$ and all $k \in \mathbb{N^*}$, $p'_k(j)=p_k(j)$, and $p'_k(n)=\frac{1}{2}p_k(n)$.
The initialization, up to $k=j$, is straightforward as all nodes have exactly two neighbors, except from the extremity nodes in $\mathcal{G}'$ which have one.
Suppose that the previous statement holds for $k \geq 1$.
Then we have:
\begin{align*}
p'_{k+1}(j) &= p'_k(j-1) \times \frac{1}{2} + p'_k(j+1) \times \frac{1}{2} \\
&= p_{k+1}(j) \quad \textit{if} \ j < n-1,
\end{align*}
\begin{align*}
p'_{k+1}(n-1) &= p'_k(n-2)  \times \frac{1}{2} + p'_k(n) \\
&= p_k(n-2)  \times \frac{1}{2} + p_k(n)  \times \frac{1}{2} \\
&= p_{k+1}(n-1),
\end{align*}
\begin{align*}
p'_{k+1}(n) &= p'_k(n-1)  \times \frac{1}{2} \\
&= (2 \times p_k(n-1) \times \frac{1}{2}) \times \frac{1}{2}\\
&= \frac{1}{2} p_{k+1}(n).
\end{align*}
%TODO: Add distance to the definition
At this point, equivalent random walk edge encodings have been found in $\mathcal{G}'$ for any pair of nodes of $\mathcal{G}$ distant of less than $n$ from each other.
The last part of the proof consists in showing that $p_k(n) = p'_k(n,0)$ for all $k\geq 1$, or equivalently, from the previous result, that $p'_k(n,0) = 2 p'_k(n)$.

This is again done by induction, by proving the following statement for all $i,j \in \mathbb{N}^2$, $k \in \mathbb{N}^*$ such that $0 \leq i < j \leq n$:
\begin{align*}
    p'_k(i,j) &= p'_k(j, i)\quad \textit{if} \: j < n,  \\
    p'_k(i,n) &= \frac{1}{2}p'_k(n, i).
\end{align*}
As before, the initialization up to the distance between $i$ and $j$ is direct as all nodes have exactly two neighbors, except $n$ which has one.
Suppose now that the statement holds for some $k \leq 1$.
Then, if $j<n$:
\begin{align*}
    p'_{k+1}(i,j) &=  p'_1(i, i+1)p'_k(i+1, j) + p'_1(i, i-1)p'_k(i-1, j) \\
    &= p'_k(j, i+1)p'_1(i+1, i) + p'_k(j, i-1)p'_1(i-1, i) \\
    &= p'_{k+1}(j,i),
\end{align*}
and
\begin{align*}
    p'_{k+1}(i,n) &=  p'_k(i, n-1)p'_1(n-1,n)\\
    &= \frac{1}{2} \times p'_1(n,n-1)p'_k(n-1, i)\\
    &= \frac{1}{2} \times p'_{k+1}(n,i),
\end{align*}
which concludes the proof.

\end{proof}

\subsection{Proof of Proposition \ref{prop:gen_res_rwse}}
\paragraph{Proposition.}
\textit{Let $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ be a graph and $(i,j) \in \mathcal{V} \times \mathcal{V}$ a pair of nodes in $\mathcal{G}$.
Then there exists a non-isomorphic graph $\mathcal{G'}= (\mathcal{V}', \mathcal{E}')$ and a pair of nodes $(i',j')\in \mathcal{V}' \times \mathcal{V}'$ such that $(i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}}$, i.e. $(i,j)$ and $(i',j')$ are equivalent under RWSE encoding.}

\begin{proof}
We proceed by induction to show that given an edge $(i,j)$ in a graph $\mathcal{G}$, there always exists a non-isomorphic graph $\mathcal{G'}$ with an edge $(i', j')$ such that $(i,j)^{\mathcal{G}} =_{\textsc{rw}} (i',j')^{\mathcal{G'}}$.
Without loss of generality, we take $i=i'=0$ and $j=j'=1$, and denote probabilities in $\mathcal{G'}$ with a prime superscript. We write $p_{ij} = (P_1)_{ij}$, $p'_{i'j'}=(P'_1)_{i'j'}$ with $P_k$ and $P'_k$ the $k$-hop random walk matrices of $\mathcal{G}$ and $\mathcal{G'}$.
Finally, we also write, $\forall k\in \mathbb{N}$, $p_k(j) = (P_k)_{0j}$ and $p'_k(j')=(P'_k)_{0j'}$.

The proof relies on the introduction of a graph $\mathcal{G'}$ as the result of a central symmetry of $\mathcal{G}$ around node $1$.
Concretely, each node $i$ (respectively $j$) originally in $\mathcal{G}$ possesses a symmetric node $i'$ (respectively $j'$) in $\mathcal{G'}$ such that:
\begin{align}
  & \label{eq:prime1} \forall i,j \in \mathcal{G}, i \neq 1, \quad p'_{i'j'} = p'_{ij} = p_{ij} \\
  & \label{eq:prime2} \forall j \in  \mathcal{N}_1 \cap \mathcal{G}, \quad p'_{1j} = p'_{1j'} = \frac{1}{2} p_{1j},
\end{align}
where $\mathcal{N}_1$ is the neighborhood of node $1$ in $\mathcal{G'}$, $\mathcal{N}_1 \cap \mathcal{G}$ is its restriction to $\mathcal{G}$, and $1' = 1$.

The initialization is straightforward as, for such $\mathcal{G'}$ and for any $k$ lesser or equal to the shortest path distance between $0$ and $1$, it holds true that $p'_k(1) = p_k(1)$.
Suppose now that for a given $k$, we have:
\begin{equation}
    \forall l \leq k, \quad p'_l(1) = p_l(1). \label{eq:hypo}
\end{equation}
Let $p_k^*(i)$ designate the probability to travel from $0$ to $i$  in $k$ steps in $\mathcal{G}$ \textit{without passing through $1$}, $p_k^{\prime*}(i)$ the similar probability in $\mathcal{G'}$, and let $\hat{p}_k(i)$ and $\hat{p}^{\prime}_k(i)$ be respectively the probabilities to travel to $i$ in $\mathcal{G}$ and $\mathcal{G'}$ \textit{while visiting $1$ at least once}.
It is then possible to write $p'_{k + 1}(1)$ as follows:
\begin{equation}
     p'_{k + 1}(1) = 
    \underbrace{\underset{i\in \mathcal{N}_1}{\sum} p^{\prime*}_k(i) p'_{i1} }_{
    \substack{
    \text{
    %
    \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {1}}}
    %
    Walks that do not}
     \\
     \text{go through $1$}
     }
    }
    +
    \underbrace{\underset{i\in \mathcal{N}_1}{\sum} \hat{p}^{\prime}_k(i) p'_{i1} }_{
    \substack{
    \text{
    %
    \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {2}}}
    %
    Walks that travel}
     \\
     \text{through $1$ at least once}
     }
    }
\end{equation}
The first term rewrites simply:
\begin{align}
\underset{i\in \mathcal{N}_1}{\sum} p^{\prime*}_k(i) p'_{i1} &= \underset{i\in \mathcal{N}_1 \cap \mathcal{G}}{\sum} p^{\prime*}_k(i) p'_{i1} \\
    &= \underset{i\in \mathcal{N}_1 \cap \mathcal{G}}{\sum} p^{*}_k(i) p_{i1}
\end{align}
For the second term, we introduce $\mathcal{W}^{*l}_{i \rightarrow j}$ as the set of random walks of length $l$ between nodes $i$ and $j$ that do not go through $1$. Thus, an element $w$ of $\mathcal{W}^{*l}_{i \rightarrow j}$ is a multiset of $l$ nodes such that $w=(w_1, \dots, w_l)$ with $w_1=i$ and $w_l=j$.
We may now rewrite \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {2}}}, and in particular each $\hat{p}^{\prime}_k(i)$ in the sum  as follows:
% \begin{align}
%     \underset{i\in \mathcal{N}_1}{\sum} p'_k(i) p'_{i1} &= \underset{i\in \mathcal{N}_1}{\sum} \underset{l \leq k-1}{\sum} \underset{j\in \mathcal{N}_1}{\sum} \underset{w\in \mathcal{W}^{k-(l+1)*}_{j \rightarrow i}}{\sum} p'_k(1)p'_{1j}\underset{w_m \in w}{\prod}p'_{w_m}
% \end{align}
\begin{align}
    &\hat{p}^{\prime}_k(i) = \sum_{l \leq k-1} \: \sum_{j\in \mathcal{N}_1} \sum_{w\in \mathcal{W}^{*k-(l+1)}_{j \rightarrow i}}
    p'_l(1)p'_{1j} \prod_{m > 1}p'_{w_{m-1}w_m} \label{eq:l1} \\
    &= 2 \sum_{l \leq k-1} \sum_{j\in \mathcal{N}_1 \cap \mathcal{G}} \sum_{w\in \mathcal{W}^{*k-(l+1)}_{j \rightarrow i}}
    p_l(1)\frac{p_{1j}}{2} \prod_{m > 1}p_{w_{m-1}w_m} \label{eq:l2}\\
    &= \hat{p}_k(i),
\end{align}
where equations~\ref{eq:prime1}, \ref{eq:prime2} and~\ref{eq:hypo} are used for (\ref{eq:l1}) $\rightarrow$ (\ref{eq:l2}).
Putting everything back together, we have:
\begin{align}
         p'_{k + 1}(1) &= 
\sum_{i\in \mathcal{N}_1 \cap \mathcal{G}} p^{*}_k(i) p_{i1}
    +
    \sum_{i\in \mathcal{N}_1 \cap \mathcal{G}} \hat{p}_k(i) p_{i1} \\
    &= p_{k + 1}(1),
\end{align}
which concludes the proof.
\end{proof}

\subsection{Proof of Proposition \ref{prop:cycle_counting}}
\paragraph{Proposition.}
\textit{Let $(i,j)$ be two adjacent nodes in a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, i.e. $(i,j) \in \mathcal{E}$, and $S_k$ the $k$-hop simple path matrix of $\mathcal{G}$ for any $k \in \mathbb{N}^*$, such that $(S_k)_{ij} = m_k \in \mathbb{N}$.
Then for $k \geq 2$, there are exactly $m_k$ cycles of length $k+1$ in $\mathcal{G}$ that admit $(i,j)$ as an edge.}

\begin{proof}
This can be proved directly by noting that each cycle of length $k+1$ on which the edge $(i,j)$ lies is a walk of $k+2$ nodes of the form $i,...j,i$ where only the first node is repeated.
The restriction of the latter to its $k+1$ first nodes is itself a path of length $k$ between $i$ and $j$, which therefore increments $(S_k)_{ij}$.
Conversely, all paths counted in $(S_k)_{ij}$ can be completed by the edge $ji$ to form a cycle of length $k+1$, which proves the equality.

\end{proof}


\section{Tree Decompositions}
\label{appendix:tree_dec}

The formal proof of the equivalence of the RWSE encodings in circular and linear graphs is given in Appendix~\ref{appendix:proof_cycle_path}.
Here we give a graphical sketch of the proof for the two examples considered in Figure~\ref{fig:rw_vs_sp_cycle}.
The tree decompositions of the graphs in Figure~\ref{fig:rw_vs_sp_cycle} are presented in Figure~\ref{fig:tree_dec} for walks starting in node 0 and a maximum depth of 5. 
As nodes only have one or two neighbors, colors are used to represent the walk probabilities toward child nodes. 
Edges $(0,1)$ have the same random walk probabilities in (A) and (B) (Figure~\ref{fig:tree_dec}, top) on one hand, and in (C) and (D) (Figure~\ref{fig:tree_dec}, bottom) on the other hand, despite belonging to very different graphs.
It is straightforward to extend these results to any depth, hence verifying that $(0,1)^A =_{\textsc{rw}} (0,1)^B$, and $(0,1)^C =_{\textsc{rw}} (0,1)^D$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Figure Tree Dec
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\includegraphics[width=0.83\linewidth]{tree_dec.jpg}
\centering
\caption{
Tree decompositions for the graphs in Figure~\ref{fig:rw_vs_sp_cycle} for walks rooted at node 0, up to a maximum depth of 5. Colors denote different probabilities to reach a node at a given depth level $k$: black lines are associated with a probability of $0.5^k$, red lines $0.5^{k-1}$, and green lines $0.5^{k-2}$.
}
\label{fig:tree_dec}
\end{figure*}
%%%%%%%%%%%%%%

\section{Full Path Counting Algorithm}
\label{appendix:full_algo}

We present in Algorithm~\ref{algo:path_count_long} the detailed version of Algorithm~\ref{algo:pathcount}.
Once a list of node permutations is obtained from the \textsc{DAGDecompose} function, rows and columns of the adjacency matrix are first permuted according to each new node ordering, which then allows to retain only the elements that are above the diagonal (all edges are directed towards nodes of higher index).
Subsequently, $k$-hop path counts are easily obtained by computing powers of the resulting nilpotent matrix $\tilde{A}$, after which rows and columns are permuted back to the original ordering.
The transpose of the resulting matrix $A_k$ yields the opposite paths and is then added to $A_k$ before updating the running $k$-hop path count matrix $M_k$.
If $\mathcal{G}$ is instead directed, this step is skipped and only $A_k$ is used to update $M_k$.


\begin{algorithm}[tb]
   \caption{Count paths between all pairs of nodes}
   \label{algo:path_count_long}
\begin{algorithmic}[1]
   \STATE {\bfseries Parameters:} Proportion of root nodes $R$, maximum length $K$, maximum DFS depth $D_\textsc{dfs}$, maximum trial number $N$
   \STATE {\bfseries Input:} Undirected graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, adjacency matrix $A$
   \STATE {\bfseries Output:} Path count matrix $M \in \mathbb{N}^{|\mathcal{V}| \times |\mathcal{V}| \times K}$
   \STATE $M_1, \dots, M_K \gets \textbf{0}_{|\mathcal{V}| \times |\mathcal{V}|}$ \COMMENT{\textit{Initialize count matrices}}
   \FOR{$i=1$ {\bfseries to} $R \times |\mathcal{V}|$}
       \STATE $r_i \gets$ \textsc{SelectRootNode}($\mathcal{G}$, $i$)
       \STATE $\Pi \gets$ \textsc{DAGDecompose}($\mathcal{G}$, $r_i$, $D_\textsc{dfs}$, $N$) \COMMENT{\textit{Retrieve list of node permutations starting with $r_i$}}
       \FOR{each $\pi_j$ {\bfseries in} $\Pi$}
           \STATE $\tilde{A} \gets$ \textsc{Reorder}($A$, $\pi_j$) \COMMENT{\textit{Permute rows and columns according to $\pi_j$}}
           \STATE $\tilde{A} \gets$ \textsc{TriUp}($\tilde{A}$) \COMMENT{\textit{Keep elements above the diagonal, fill the rest with $0$}}
           \STATE $k \gets 1$
           \STATE $\tilde{A}_k \gets \tilde{A}$
           \WHILE{$k \leq K$ {\bfseries and} $\tilde{A}_k \neq \textbf{0}_{|\mathcal{V}| \times |\mathcal{V}|}$}
               \STATE $\tilde{A}_k \gets \tilde{A}_k \times \tilde{A}$
               \STATE $A_k \gets$ \textsc{Reorder}($\tilde{A}_k$, $\pi_j^{-1}$)
               \STATE $M_k \gets \max(M_k, A_k + A_k^\top)$
               \COMMENT{\textit{Add opposite paths and store new maximum counts}}
               \STATE $k \gets k + 1$
           \ENDWHILE
       \ENDFOR
   \ENDFOR
   \STATE {\bfseries Return:} $M_1, \dots, M_K$
\end{algorithmic}
\end{algorithm}



% \begin{algorithm}[tb]
% % \tag{\ref{algo:dagdec}
% \label{algo:dagdec}
%     \caption{\textsc{DAGDecompose}: Decomposition of an input graph into multiple DAGs}
%  \begin{algorithmic}
%     \STATE {\bfseries Parameters:} Maximum DFS depth $D_\textsc{dfs}$, maximum trial number $N$
%     \STATE {\bfseries Input:} Graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, root node $r$, diameter $D_\textsc{max}$
%     \STATE {\bfseries Output:} List of node orderings $\Pi$
%     \STATE Initialize $\Pi \gets \text{EmptyList}$
%     \FOR{$d_\textsc{dfs} = 0$ {\bfseries to} $D_\textsc{dfs}$}
%        \FOR{$n = 1$ {\bfseries to} $N$}
%           \STATE Initialize $\pi \gets \text{EmptyList}$
%           \WHILE{$\pi \neq \mathcal{V}$}
%              \FOR{$d = 1$ {\bfseries to} $D_\textsc{max}$}
%                 \IF{$d < d_\textsc{dfs}$}
%                    \STATE $\pi \gets$ \textsc{DFS}($r$, $\mathcal{G}$, $\pi$) \COMMENT{\textit{Start with $d_\textsc{dfs}-1$ DFS steps}}
%                 \ELSIF{$d = d_\textsc{dfs}$}
%                    \STATE $\pi \gets$ \textsc{partialBFS}($r$, $\mathcal{G}$, $\pi$)
%                 \ELSE
%                    \STATE $\pi \gets$ \textsc{BFS}($r$, $\mathcal{G}$, $\pi$) \COMMENT{\textit{Continue with as many BFS steps as possible}}
%                 \ENDIF
%              \ENDFOR
%           \ENDWHILE
%           \STATE $\Pi \gets \textsc{Add}(\Pi, \pi)$ \COMMENT{\textit{Append $\pi$ to $\Pi$}}
%        \ENDFOR
%     \ENDFOR
%     \STATE {\bfseries Return:} $\Pi$
%  \end{algorithmic}
%  \end{algorithm}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Figure Synthetic Graphs 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Synthetic Experiment}\label{appendix:synt_graph_example}

\Cref{fig:synthe_graphs} shows three instances of randomly generated graphs for experiments in \Cref{subsec:synt_exp}.
These graphs possess an average of 149 nodes and 190 edges.
We also detail the three configurations used for the synthetic experiments in \Cref{tab:synthe_model_configs}.

\begin{figure}[h!]
    \includegraphics[width=0.6\linewidth]{synthe_graphs.jpg}
    \centering
    \caption{Examples of synthetic graphs generated for the cycle counting experiment.}
    \label{fig:synthe_graphs}
    \end{figure}
%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% Synthetic experiment configs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
\caption{Model configurations used for the synthetic experiments.}
\label{tab:synthe_model_configs}
\begin{center}
% \resizebox{1.0\textwidth}{!}{
{
% {\small
\begin{tabular}{l c c c}
\toprule
 \textbf{Configuration}& \# 1 & \# 2 & \# 3 \\
 % \cmidrule(lr){2-6}
\midrule

Transformer layers & 3 & 6 & 10 \\
Self-attention heads & 4 & 4 & 8 \\
Hidden dimension & 52 & 64 & 64 \\
Learning rate & $10^{-3}$ & $5\times10^{-4}$ & $5\times10^{-4}$ \\
Epochs & 100 & 300 & 400 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%

\end{document}

