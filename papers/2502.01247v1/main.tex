%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{stmaryrd}

% additional
\usepackage{enumitem}
\usepackage{pgfplots}
\pgfplotsset{compat=1.7}
\pgfplotsset{
    every axis/.append style={
        width=0.85\linewidth,  % Optimal width for papers
        height=0.55\linewidth, % Balanced aspect ratio
        grid=major,           % Add major grid lines
        grid style={dotted,gray!50},  % Subtle grid for readability
        axis line style={thick,black},  % Bold axis lines
        tick style={black, thick},  % Thicker ticks
        xlabel style={at={(axis description cs:0.5,-0.1)}, anchor=north, font=\normalsize},  % Smaller axis labels for papers
        ylabel style={at={(axis description cs:-0.1,0.5)}, anchor=south, font=\normalsize},
        label style={font=\normalsize},  % Axis labels (xlabel, ylabel)        
        title style={font=\bfseries\footnotesize},  % Bold and small title
        legend style={align=left, legend cell align=left, draw=none, fill=none, font=\footnotesize},  % Clean legend
        tick label style={font=\normalsize},  % Tick labels small but readable
        cycle list name=Set1-7,  % High-contrast colors
    }
}
\usepgfplotslibrary{colorbrewer}
\pgfplotsset{cycle list/Set1-7}

\usepackage{xurl}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% If preprint:
%\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{property}[theorem]{Property}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learnable polynomial, trigonometric, and tropical activations}

\begin{document}

\twocolumn[
\icmltitle{Learnable polynomial, trigonometric, and tropical activations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ismail Khalfaoui-Hassani}{fzj}
\icmlauthor{Stefan Kesselheim}{fzj}
\end{icmlauthorlist}

\icmlaffiliation{fzj}{Forschungszentrum JÃ¼lich, Germany}

\icmlcorrespondingauthor{Ismail Khalfaoui-Hassani}{i.khalfaoui@fz-juelich.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % \icmlEqualContribution otherwise use the standard text.

\begin{abstract}
This paper investigates scalable neural networks with learnable activation functions based on orthogonal function bases and tropical polynomials, targeting ImageNet-1K classification and next token prediction on OpenWebText. Traditional activations, such as ReLU, are static. In contrast, learnable activations enable the network to adapt dynamically during training. However, stability issues, such as vanishing or exploding gradients, arise with improper variance management in deeper networks. To remedy this, we propose an initialization scheme that single-handedly preserves unitary variance in transformers and convolutional networks, ensuring stable gradient flow even in deep architectures. Extensive experiments demonstrate that networks with Hermite, Fourier, and Tropical-based learnable activations significantly improve over GPT-2 and ConvNeXt networks in terms of accuracy and perplexity in train and test, highlighting the viability of learnable activations in large-scale tasks. The activation functions developed here are the subject of a library coded entirely in pure PyTorch: torchortho\footnote{\href{https://github.com/K-H-Ismail/torchortho}{https://github.com/K-H-Ismail/torchortho}}.
\end{abstract}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figs/learnable_activations.png}}
\caption{Comparison of a classical MLP (linear + ReLU \cite{nair2010rectified} or GELU \cite{hendrycks2016gaussian}) and Basis-MLP (linear + learnable basis function) blocks.}
\label{fig:schema}
\end{center}
\vskip -0.2in
\end{figure}
%\cref{fig:schema}.
\section{Introduction}
\label{sec:intro}
Modern deep learning is largely built upon the Multi-Layer Perceptron (MLP) \cite{mcculloch1943logical, rosenblatt1958perceptron} and the gradient backpropagation algorithm \cite{rumelhart1986learning}. The MLP can be described as a combination of a multiplication by a matrix of learnable weights and the application of a nonlinear activation function. Gradient backpropagation, on the other hand, relies on the chain rule to compute partial derivatives necessary for optimizing weights through gradient descent.

In a deep neural network, \emph{preserving variance across layers} is critical to ensure stable training dynamics. 
\citet{he2015delving} were the first to consider a variance-preserving analysis for deep neural networks. %If we suppose that a deep neural network is a composition of MLP blocks defined by:
%\begin{definition} Let  \( x \) be the input vector of the MLP, $W$ the learnable weight tensor of inner dimension $C_{in} \in \mathbb{N}^*$, and $F$ an activation function, and $\cdot$ the usual inner product in $\mathbb{R}^{C_{in}}$, an MLP block is defined by
%\begin{equation}
%\label{eq:mlp}
%    \texttt{MLP}(x) = W \cdot F(x)
%\end{equation}
%\end{definition}

 The analysis shown in \cite{he2015delving} could be stated as \emph{the output signal of each MLP block should have the same variance as the input signal}. And since learning is performed with backpropagation, this same rule should apply to the gradients as well, meaning that \emph{the variance of the gradient of the input should also be equal to the variance of the gradient of the output of the MLP}. 

In this manner, \citet{he2015delving} demonstrated the methodology for initializing the weights of a deep neural network, thereby attaining performance on ImageNet classification that exceeds that of humans. This process entails the calculation of a forward gain relative to the employed activation function and a backward gain relative to the derivative of the said function. Remarkably, for the ReLU function, this gain is equal to 2 in both the forward and backward.

Recently, \citet{yang2024kolmogorov} employed the same principle to train learnable rational activations. However, they encountered a challenge: the second-order moment calculus has no closed formulation in the case of rational fractions. The authors' solution for ensuring the convergence of such rational activation networks consisted in initializing them by fitting a classical activation such as ReLU or SiLU \cite{ ramachandran2017searching, elfwing2018sigmoid}. This study proposes a solution to the aforementioned problem by employing orthogonal basis functions, specifically polynomial and trigonometric functions. Orthogonal basis functions, as will be elucidated in the subsequent sections, facilitate the calculation of the second-order moment integral, thereby yielding a closed and straightforward formula. Additionally, we demonstrate that rational functions are unnecessary, asserting that polynomial activation functions are sufficient.

More generally, the convergence of polynomial networks demonstrates theoretically and empirically that deep neural networks can be seen as multivariate polynomial mappings. Indeed, the successive layers of a feed-forward network activated by a polynomial activation can be seen as a composition of weighted sums of multivariate polynomials, ultimately resulting in a polynomials mapping. A parallel representation was made by \cite{zhang2018tropical} for ReLU-activated networks, demonstrating that they are tropical rational mappings. In a later section, we also explore tropical polynomial functions as activation functions.

The contributions of this paper are therefore multi-faceted between theoretical proofs, technical developments, and empirical confirmations, and can be summarized in the following list:
\setlist{nolistsep}
\begin{itemize}[leftmargin=*,noitemsep]
    \item A novel variance-preserving initialization method is introduced for orthogonal learnable activations in neural networks. Assuming an orthonormal function basis, this method ensures that the output variances are unitary and match those of the derivative, leading to stable training.
    \item Empirically showing that deep neural networks like ConvNeXt \cite{liu2022convnet} and GPT-2 \cite{radford2019language} can be trained using learnable activations for tasks like image classification on ImageNet1k \cite{deng2009imagenet} and language modeling on OpenWebText \cite{Gokaslan2019OpenWeb}, achieving better performance than networks using traditional activations like ReLU or GELU. The innovation eliminates the need for additional activation functions (e.g., ReLU, SoftSign) to maintain training stability.
    \item Proving that polynomially activated deep neural networks are polynomial mappings.%, and extending the argument to Tropical and trigonometric polynomials.
    \item Developing Hermite, Fourier, and Tropical polynomial functions along with methods to address floating-point challenges in finite precision and parallel algorithms and kernels to efficiently implement these activations.
    % \item Extension of the approach to more varied families of orthonormal (or non-orthonormal) functions, such as wavelets, rational and Tropical functions.
\end{itemize}
\section{Related Work}
The use of polynomial activations has long been denigrated, probably by the rise of works such as \cite{pinkus1999approximation} and \cite{leshno1993multilayer} which have mathematically demonstrated that the universal approximation property is equivalent to the use of a non-polynomial activation function. The classical Universal Approximation Theorem \cite{cybenko1989approximation,hornik1990universal} holds for neural networks of arbitrary width and bounded depth. However, recent work such as \cite{kidger2020universal} shows that in the framework of bounded width and arbitrary depth, every activation function is possible to use in practice, including polynomial activation functions. We show empirically in this work that polynomial activations can converge in the context of large-scale deep networks with large-scale tasks and datasets. The key to this success may lie in the fact that the coefficients of the latter are learnable and that adequate initialization is used.
The empirical demonstration of the effectiveness of polynomial activations made here was achieved without the use of other functions intended to regularize convergence, such as the SoftSign function borrowed from \cite{turian2009quadratic} and used in \cite{lokhande2020generating} for Hermite activations, or a ReLU function, or any normalization as recently done in \cite{zhuo2024polynomial}.
This confirmation that polynomial activations are practicable opens the way to representing deep neural networks as multivariate polynomial mappings. As in \cite{kileel2019expressive} and \cite{kubjas2024geometry}, which see that these types of networks have greater expressive potential, we show that deep polynomially activated neural networks are multivariate polynomial mappings.
The subject of learnable activation is a well-known one, but it has seen a resurgence thanks to the popularity enjoyed by the KAN article \cite{liu2024kan}. In Appendix~\ref{appendix:kan}, we'll digress for a while to explain how these are inspired by the Kolmogorov-Arnold theorem \cite{kolmogorov1957representation}. An extended  related work can be found in Appendix~\ref{appendix:related}.

\section{Methods}
\subsection{Variance Preserving Initialization}
The variance-preserving principle \cite{he2015delving} mentioned in the introduction, is expressed in the following.
Consider an input vector $x = (x_0, \dots, x_i, \dots, x_{C_{in}}) \in \mathbb{R}^{C_{in}}$, $C_{in} \in \mathbb{N}^*$, where all $x_i$ in   are mutually independent and uniformly distributed. Preserving the variance in an MLP layer with a learnable weight tensor $W$ of inner dimension $C_{in}$ and an activation function $F$ amounts to:%described in Eq.~\ref{eq:mlp} amounts to:
\begin{equation}
\operatorname{Var}[x]=C_{i n} \operatorname{Var}[W F(x)] 
\end{equation}
If we suppose that $x$ and $W$ are independent and of finite variance, we have:
\begin{equation}
\label{eq:variance}
    \operatorname{Var}[x]=C_{i n} \left(\operatorname{Var}[W] \mathbb{E}\left[F(x)^2\right] + \operatorname{Var}[F(x)] \mathbb{E}\left[W\right]^2\right)
\end{equation}
\begin{assumption}
We initialize $W$ such as $ \mathbb{E}\left[W\right] = 0$.
\label{ass:weight_zero mean}
\end{assumption}
Since we always assume that $W$ is initialized with a zero mean, Eq.~\ref{eq:variance} simplifies into:
\begin{equation}
    \operatorname{Var}[x]=C_{i n} \operatorname{Var}[W] \mathbb{E}\left[F(x)^2\right]
\end{equation}
Thus, to calculate the variance of the weights, we should calculate the following ratios:
\begin{definition}
\label{def:gain_forward} 
The forward gain of the MLP layer is:
\begin{equation}
    \alpha =  \frac{\operatorname{Var}[x]}{\mathbb{E}\left[F(x)^2\right]}
\end{equation}
\end{definition}
Similarly, and in a backward manner, 
\begin{definition}
\label{def:gain_backward} 
The backward gain is the gain  of the derivative of the activation with respect to $x$ and is defined as:
\begin{equation}
    \alpha' =  \frac{\operatorname{Var}[x]}{\mathbb{E}\left[F'(x)^2\right]}
\end{equation}
\end{definition}
Since a deep neural network is essentially a composition of MLP layers, an appropriate initialization method must avoid reducing or amplifying the input signals \cite{he2015delving}.
\begin{assumption}
    We'll assume from now on that both the input signal $x$ and its gradient $\Delta x$ follow a distribution of mean 0 and variance 1. %and unless otherwise stated, \( x \sim \mathcal{N}(0, \mathbf{1}) \) and \( \Delta x \sim \mathcal{N}(0, \mathbf{1}) \), where $\mathcal{N}$ is the normal distribution.
\end{assumption}
Therefore, calculating the gains $\alpha$ and $\alpha'$ in an MLP (or equivalently a convolution layer) involves calculating only the inverse of the second-order moments of the activation functions and their derivatives. 

Interestingly, for the ReLU function, we have $\alpha = \alpha' = 2$. Hence the scaling of the standard deviation of the weights $W$ in \cite{he2015delving} by a factor $\sqrt{2/C_{in}}$, more details can be found in Appendix \ref{appendix:relu}.

Given an arbitrary activation, equality of forward and backward gains is not always achieved by default as in ReLU. In the next section, we show the conditions for an activation function written in an orthonormal coordinate system to verify the forward-backward gain equality. To illustrate this point, we will calculate the second moment for Hermite and Fourier basis decompositions, given their compatibility with the normal and uniform distributions, respectively.

\subsection{Second Moment of the Hermite Activation Function and Its Derivative}
\begin{definition}
$\forall n \in \mathbb{N}$, the probabilist Hermite polynomials can be defined as follows: 
\begin{equation}
\label{eq:hermite_def}
    \operatorname{He}_n(x)=(-1)^n e^{\frac{x^2}{2}} \frac{d^n}{d x^n} e^{-\frac{x^2}{2}}
\end{equation}
\end{definition}
$n$ is called the degree of the Hermite polynomial and we have the first terms:
\begin{equation*}
\begin{aligned}
\mathrm{He}_0(x)&=1 & \mathrm{He}_1(x)&=x \\
\mathrm{He}_2(x)&=x^2-1 & \mathrm{He}_3(x)&=x^3-3 x 
\end{aligned}
\end{equation*}
Hermite polynomials constitute a suitable choice for calculating the moment of order 2 when $x$ follows a standard normal distribution $\mathcal{N}(0,1)$.

\begin{property}
$\forall m,n \in \mathbb{N}^2$, we have:
\begin{equation}
    \int_{-\infty}^{\infty} \operatorname{He}_m(x) \operatorname{He}_n(x) e^{-\frac{x^2}{2}} d x=\sqrt{2 \pi} n!\delta_{n m}
\end{equation}
With $\delta_{nm}$ the Kronecker delta function.
\label{property:orthonormal}
\end{property}
\begin{definition}
\label{def:hermite}
We define the Hermite activation $F \colon \mathbb{R}\to\mathbb{R}$ with its learnable coefficients $\forall k \in \llbracket 0, n \rrbracket$ $a_k \in \mathbb{R}$ as:
\begin{equation}
\label{eq:hermite}
     x \mapsto F(x)=\sum^{n}_{k=0} \frac{a_k}{\sqrt{k!}}\operatorname{He}_k(x)
\end{equation}
\end{definition} 
\begin{proposition}
\label{prop:hermite_forward_gain}
The second moment of this activation is:
\begin{align}
\label{eq:hermite_final}
\mathbb{E}\left[F(x)^2\right]=&\sum^{n}_{k=0} a_k^2
\end{align}
\begin{proof}
    The proof relies on the orthonormality property \ref{property:orthonormal} and is detailed in Appendix \ref{appendix:hermite}. 
\end{proof}
\end{proposition}
\begin{property}
The following recurrence property is derived directly from the equation \ref{eq:hermite_def}. $\forall k \in \mathbb{N}$ $\forall x \in \mathbb{R}$:
\begin{equation}
    \operatorname{He}_{k}^\prime(x) = x \operatorname{He}_{k}(x) - \operatorname{He}_{k+1}(x)
\end{equation}
\label{prop:hermite_deriv}
\end{property}
\begin{property}
\label{prop:hermite_deriv2}
This property is shown by induction and by using the previous property \ref{prop:hermite_deriv}. $\forall k \in \mathbb{N}^*$ $\forall x \in \mathbb{R}$:
\begin{equation}
    \operatorname{He}_k^\prime(x) = k \operatorname{He}_{k-1}(x)
\end{equation}
\end{property}
\begin{proposition}
Using the last property and by the linearity of the integral, the derivative of $F$ (Eq.~\ref{eq:hermite}), $F^\prime \colon  \mathbb{R} \to \mathbb{R} $ is written as follows:
\begin{equation}
     x \mapsto F^\prime(x)=\sum^{n}_{k=1} \frac{ka_k} {\sqrt{k!}}\operatorname{He}_{k-1}(x)
\end{equation}
\end{proposition}
\begin{remark}
    \label{remark:hermite_lip} 
    A first remark here is that $\forall n >2$: $F^\prime$ is unbounded ($\displaystyle{\lim_{x\to\infty} F^\prime(x) \to \infty}$). This means that $F$ is not Lipschitz continuous. Lipschitz continuity is often desired (or even required) when training a deep neural network using gradient backpropagation. However, by a suitable initial choice of the coefficients $(a_k)_{k \in  \llbracket 0,n \rrbracket}$ we can keep the Lipschitz constant under control. 
\end{remark}
\begin{proposition}
\label{prop:hermite_backward_gain}
The second moment of the derivative of the Hermite activation is:
 \begin{equation}
    \mathbb{E}\left[F^\prime(x)^2\right]=\sum^{n}_{k=1}ka_k^2
\end{equation}
\begin{proof}
    an orthonormality argument as for the proof in Appendix \ref{appendix:hermite} suffices, we conclude by noticing that: 
\begin{equation*}
    \mathbb{E}\left[F^\prime(x)^2\right]=\sum^{n}_{k=1} k^2a_k^2\int_{-\infty}^{+\infty}\frac{\operatorname{He}_{k-1}(x)^2}{k(k-1)!}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x
\end{equation*}
\end{proof}
\end{proposition}
\begin{proposition}
\label{prop:hermite_equality}
    Equality between propositions~\ref{prop:hermite_forward_gain} and \ref{prop:hermite_backward_gain} imposes that:
\begin{equation}
    a_0^2 = \sum^{n}_{k=1} (k-1)a_k^2
\end{equation}
\end{proposition}

To satisfy the forward-backward gain equality, we could initialize the coefficients $(a_k)_{k \in  \llbracket 0,n \rrbracket}$ such as $\forall n \in \mathbb{N}^*$:
\begin{equation}
     \forall k \in   \llbracket 1,n \rrbracket \ a_k = 1 \quad\text{and}\quad a_0 = \sqrt{\frac{n(n-1)}{2}}
\end{equation}
This initialization works in practice for small $n$. However, this choice does not scale well with the degree $n$, as the leading coefficient $a_0$ diverges to infinity with $n$. Instead, we could opt for the following initialization inspired by the limit case $n \rightarrow +\infty$:
\begin{theorem}
\label{thm:hermite}
Scalable variance-preserving coefficient initialization of Hermite activation. Let $p > 1$, and
\begin{equation}
     \forall k \in \llbracket 1,n \rrbracket \ a_k = \frac{1}{k^p} \ \text{and}\ a_0 = \sqrt{\zeta(2p-1) - \zeta(2p)}
\end{equation}
 with $\zeta$ the Riemann function $\forall x \in [1,+\infty] \colon \zeta(x) = {\sum_{k=1}^{\infty} \frac{1}{k^x}}$. Then in the limit case $n \to +\infty$, the gain for the activation and its derivative becomes the same and equals:
\begin{equation}
    \alpha = \alpha' = \frac{1}{\zeta(2p-1)}
\end{equation}
\begin{proof}
    In the limit case, by a simple injection of $a_k = \frac{1}{k^p}$ in Prop.~\ref{prop:hermite_equality} and then in Prop.~\ref{prop:hermite_backward_gain}, we obtain the result.
\end{proof}
\end{theorem}
\begin{corollary}
\label{coro:hermite}
    The coefficient initialization in Theorem \ref{thm:hermite} could be divided by a factor $\sqrt{\zeta(2p-1)}$ in order to have unitary forward and backward gains. $\forall k \in \llbracket 1,n \rrbracket:$
    \begin{equation}
      a_k = \frac{1}{k^p\sqrt{\zeta(2p-1)}} \ \text{and} \ a_0 = \sqrt{1- \frac{\zeta(2p)}{\zeta(2p-1)}}
    \end{equation}
\end{corollary}
\begin{example}
If we take $p=\frac{3}{2}$, by the corollary \ref{coro:hermite} we have:
\begin{equation}
     \forall k \in \llbracket 1,n \rrbracket \ a_k = \sqrt{\frac{6}{\pi^2 k^{3}}} \ \text{and}\ a_0 = \sqrt{1 - \frac{6\zeta(3)}{\pi^2}} \approx 0.519
\end{equation}
In practice, we will use $p=\frac{3}{2}$ in all our subsequent experiments where Hermite activations are involved.
\end{example}
The choice of an orthonormal family of functions depends on the input's probability distribution. For a normally distributed input, Hermite polynomials simplify the computation of second-order moments and related gains. For a uniform distribution over $[-\pi,\pi]$, trigonometric functions (Fourier series) are appropriate. If the input follows a Wigner semi-circle distribution (of measure $\sqrt{1-x^2}dx$), then the Chebyshev polynomials of the second kind are the suitable choice.

\subsection{Second Moment of the Fourier Activation Function and Its Derivative}
The forward and backward gains for a Hermite activation have been calculated under the assumption that the input $x$ follows a normal distribution, such that the initial coefficients provide equal gains. The subsequent analysis will establish the same result for a truncated Fourier series expansion of order $n \in \mathbb{N}$. 
\begin{assumption}
    The input \( x \) is assumed now to follow a uniform distribution on the interval \( [-\pi, \pi] \), denoted as $x \sim \mathcal{U}\left(-\pi, \pi\right)$.
\end{assumption}
\begin{definition}
\label{def:fourier}
We consider the following Fourier activation $F \colon  \mathbb{R} \to \mathbb{R}$:
\begin{equation}
\label{eq:fourier}
    F(x) \mapsto a_0 +
    \sum_{k=1}^{n} \left(a_k \cos(k x) + b_k \sin(k x)\right)
\end{equation}
where \( (a_k)_{k \in \mathbb{N}} \) and \( (b_k)_{k \in \mathbb{N}^*} \) are real learnable coefficients.
\end{definition}
\begin{proposition}
\label{prop:fourier_forward_gain}
The second moment of this activation is:
\begin{equation}
\label{eq:fourier_simple}
\mathbb{E}[F(x)^2] = a_0^2 +  \frac{1}{2}\sum_{k=1}^{n} \left( a_k^2 + b_k^2 \right)
\end{equation}
\begin{proof}
    The proof relies on the orthonormality property \ref{property:fourier_ortho} and is detailed in Appendix \ref{appendix:fourier}. 
\end{proof}
\end{proposition}
\begin{remark}
     For an input $x$ of distribution $x \sim \mathcal{U}(-\sqrt{3},\sqrt{3})$, which has a variance of $\operatorname{Var}[x]=1$ and which is more in line with deep neural networks that seek a unitary variance preserving property across layers, we could rescale the fundamental frequency given in the  definition of $F$ in Def.~\ref{def:fourier} by redefining it as:
\begin{equation}
\label{eq:fourier_final}
    F(x) \mapsto a_0 + \sum_{k=1}^{n} \left(a_k \cos(k\frac{\pi}{\sqrt{3}} x) + b_k \sin(k\frac{\pi}{\sqrt{3}} x)\right)
\end{equation}
The computation of the second moment stays the same. In what follows, we will consider $x \sim \mathcal{U}(-\sqrt{3},\sqrt{3})$ as well as the definition of $F$ as established in Eq.~\ref{eq:fourier_final}.% In general if $x \sim \mathcal{U}(-l,l)$, $l \in \mathbb{R_+^*}$, and if \( \omega \in \mathbb{Z} \) is the fundamental frequency, this last should be scaled by $\omega^\prime = \frac{\pi}{l}\omega$.
\end{remark}

\begin{proposition}
The derivative of the Fourier activation \( F^\prime \colon  \mathbb{R} \to \mathbb{R} \) from its definition in Eq.~\ref{eq:fourier_final} is given by:
\begin{equation}
    F^\prime(x) \mapsto \sum_{k=1}^{n} k\frac{\pi}{\sqrt{3}}\left(-a_k \sin(k\frac{\pi}{\sqrt{3}} x) + b_k \cos(k\frac{\pi}{\sqrt{3}} x)\right)
\end{equation}    
\end{proposition}
\begin{remark}
    Contrary to the remark in \ref{remark:hermite_lip}, $F^\prime$ is bounded. 
\begin{equation}
    \forall x \in \mathbb{R} \colon |F^\prime(x)| \leq \frac{\pi n (n+1)}{\sqrt{3}} \operatorname{max}(|a_k|,|b_k|)_{k \in \llbracket 1,n\rrbracket}
\end{equation}
This means that in the case of a Fourier activation, $F$ is Lipschitz continuous.
\end{remark} 
\begin{proposition}
\label{prop:fourier_backward_gain}
The second moment of the derivative of the Fourier activation is:
 \begin{equation}
    \mathbb{E}\left[F^\prime(x)^2\right]=\sum^{n}_{k=1}\frac{\pi^2}{6}k^2(a_k^2 + b_k^2)
\end{equation}
\begin{proof}
    an orthonormality argument as for the proof in Appendix \ref{appendix:fourier} suffices.
\end{proof}
\end{proposition}
\begin{proposition}
\label{prop:fourier_equality}
    Equality between \ref{prop:fourier_forward_gain} and \ref{prop:fourier_backward_gain} imposes that:
\begin{equation}
    a_0^2 = \frac{1}{2}\sum^{n}_{k=1} \left(\frac{\pi^2}{3}k^2-1\right)(a_k^2 + b_k^2)
\end{equation}
\end{proposition}

To satisfy the forward-backward gain equality, we could again initialize the coefficients such as $\forall n \in \mathbb{N}^*$:
\small
\begin{equation}
     \forall k \in   \llbracket 1,n \rrbracket \ a_k = b_k = 1 \ \text{and} \  a_0 = \frac{\sqrt{\pi^2 n(n+1)(2n+1) -18n}}{6} 
\end{equation}
\normalsize
 However, this choice does not scale well with the degree $n$. Instead, we opt for an initialization akin to the one shown in theorem~\ref{thm:hermite}. More details can be found in Appendix \ref{appendix:fourier}.
\begin{remark}
    In our implementation of Fourier activation, not only the coefficients \( (a_k)_{k \in \mathbb{N}} \) and \( (b_k)_{k \in \mathbb{N}^*} \)  were learnable, but also the frequencies that were initialized to $(f_k = k \frac{\pi}{\sqrt{3}})_{k \in \mathbb{N}^*}$, yielding to what is known as ``cosine basis" \cite{mallat2009wavelet} rather than Fourier series.
\end{remark}
\subsection{Tropical polynomial and rational activations}
\begin{definition}
The max-tropical semiring  $\mathbb{T}$ is the semiring  $\mathbb{T}=(\mathbb{R} \cup\{+\infty\}, \oplus, \otimes)$, with the operations, $\forall x,y \in {\mathbb{R}\cup\{+\infty\}}^2$: 
\begin{equation}
x \oplus y:=\max \{x, y\} \quad\text{and}\quad x \otimes y:=x+y
\end{equation}
Equivalently, we could define the min-tropical semiring by substituting the max operation in $\oplus$ with a $\min$ operation.
By extension, we define for all $a \in \mathbb{N}$ the tropical power of $x$ raised to $a$ as multiplying $x$ to itself $a$ times:
\begin{equation}
x^{\otimes a}:=x \otimes \cdots \otimes x=a \cdot x
\end{equation}
\end{definition}
\begin{definition}
The \emph{tropical polynomial activation} $F$ is defined as the \emph{tropicalization} of a polynomial of degree $n \in \mathbb{N}$ with $\forall k \in \llbracket 0, n \rrbracket$ $a_k \in \mathbb{R}$ the learnable coefficients:
    \begin{align}
    F \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
    F(x)  &\mapsto  \bigoplus_{k=0}^n a_k \otimes x^{\otimes k} := \max_{k=0}^{n} \left\{ a_k + kx \right\}
\end{align}
With $\max\limits_{k=0}^{n} \left\{ a_k + kx \right\}:= \max ( a_0, a_1 + x, \cdots,$ $ a_n + nx )$.
\end{definition}
\begin{remark}
In the following, we will only be interested in polynomial tropical activations, for which we will initialize all learnable coefficients to 1, a ``reasonable" initialization that empirically holds. The computation of the second-order moment of a tropical activation involves a generalized extreme value distribution, which we will not discuss in this article, but rather in a later work.
\end{remark}
\subsection{Deep Polynomially Activated Neural Networks are Multivariate Polynomial Mappings}
Deep MLPs are compositions of affine transformations and activation functions applied layer by layer. When the activation functions are polynomial, the entire network can be expressed as a polynomial mapping.
\begin{definition}
    Let $n, m \in \mathbb{N}$. A function $F: \mathbb{R}^n \to \mathbb{R}^m$ is called a \emph{polynomial mapping} if each component function $F_i: \mathbb{R}^n \to \mathbb{R}$, for $i = 1, \dots, m$, is a polynomial in $n$ variables. Explicitly, this means that for each $i$, $F_i$ has the form:
    \[
        F_i(x_1, \dots, x_n) = \sum_{|\alpha| \leq d_i} c_{i, \alpha} x_1^{\alpha_1} x_2^{\alpha_2} \cdots x_n^{\alpha_n},
    \]
    where the sum is taken over all multi-indices $\alpha = (\alpha_1, \dots, \alpha_n) \in \mathbb{N}^n$ such that $|\alpha| = \alpha_1 + \alpha_2 + \dots + \alpha_n \leq d_i$, $c_{i, \alpha} \in \mathbb{R}$ are real coefficients, and $d_i \in \mathbb{N}^n$.
\end{definition}



\begin{definition}
    A \emph{deep neural network} with $L$ layers, input dimension $n$, and output dimension $m$ is a function $F: \mathbb{R}^n \to \mathbb{R}^m$ of the form:
    \[
        F(x) = W_L \sigma ( W_{L-1} \sigma ( \cdots \sigma(W_1 x + b_1) \cdots ) + b_{L-1}) + b_L,
    \]
    where $\forall i \in \llbracket 1, L \rrbracket$ $C_i \in \mathbb{N}^*$. Each $W_i \in \mathbb{R}^{C_i \times C_{i-1}}$ is a weight matrix, $b_i \in \mathbb{R}^{C_i}$ is a bias vector, and $\sigma$ is an activation function applied element-wise.
\end{definition}

\begin{proposition}
\label{prop:mapping}
    Let $F: \mathbb{R}^n \to \mathbb{R}^m$ be a deep neural network with polynomial activation functions of degree $d$. Then $F$ is a polynomial mapping of degree at most $d^L$.
\end{proposition}
\begin{proof}
    The proof proceeds by induction on the number of layers $L$ and is detailed in appendix~\ref{appendix:mapping}.
\end{proof}
%An equivalent consideration for trigonometric polynomials can be established by approximation, but will not be covered here.
\subsection{Practical Implementation}
In what follows, we outline the considerations we have taken in order to implement Hermite, Fourier, and Tropical polynomial activations efficiently in PyTorch.

\textbf{Weight decay.} An important aspect of training learnable activations is that their learnable coefficients should be trained without weight decay as it could bias them toward zero. 

\textbf{Explicit Hermite formula.} We can show by induction that the following definition is equivalent to the one in Eq.~\ref{eq:hermite_def}:
\begin{equation}
\label{eq:hermite_explicit}
    \operatorname{He}_n(x)=n!\sum_{m=0}^{\left\lfloor\frac{n}{2}\right\rfloor} \frac{(-1)^m}{m!(n-2 m)!} \frac{x^{n-2 m}}{2^m}
\end{equation}
The term $\frac{n!}{m!(n-2 m)!2^m}$ could lead to numerical instabilities if computed as such. Instead, we could reformulate it using the log-gamma function. We can see that the formula~\ref{eq:hermite_explicit} can be parallelized, and is, therefore, the core of the algorithm we have developed in native PyTorch to compute Hermite activations (see Algorithm~\ref{alg:hermite_forward}).
%With $\ln\Gamma =\ln(|\Gamma|)$, with the $\Gamma$ function : $ \displaystyle \Gamma (z)=\int _{0}^{\infty }t^{z-1}e^{-t}{\text{ d}}t,\ \qquad \Re (z)>0\$

%Which generalizes the notion of factorial foe real and complex numbers. For every %positive integer $n$, we have
%\begin{equation}
%    \displaystyle \Gamma (n)=(n-1)!
%\end{equation}
%This solve the computability issue for large $n$ values.
%For numerical reasons linked to the maximum value representable in \texttt{float32}, we will therefore limit ourselves to polynomials of degree $33$ at most, because $33!\times 34 > 2^{127}$
\textbf{A dedicated Hermite kernel.}
Along with the parallel implementation of the Hermite activation, we developed a dedicated kernel that leverages the derivation established in \ref{prop:hermite_deriv2} for the backward pass exploiting the fact that the derivative of a polynomial is a polynomial of lower degree  and the following recurrence formula in the forward pass to optimize performance and memory usage (see Algorithm~\ref{alg:hermite_cuda_forward}):
\begin{equation}
    {\displaystyle\operatorname {He} _{n+1}(x) =x\operatorname {He} _{n}(x)-n\operatorname {He} _{n-1}(x)}
\end{equation}
\textbf{Stable power computation.} Computing high-degree polynomial activations can be challenging in floating-point arithmetic, especially with %fractional exponents that can lead to complex numbers, such as $(-1)^{0.5}$, resulting in \emph{NaN} values. The expression $|x|^{\alpha} \cdot  \operatorname{sign}(x) ^{\alpha}$ helps to mitigate this problem by ensuring that calculations remain in the real number domain. While an alternative solution would be to use complex number formats, this approach requires additional memory and computational resources, making it less practical. In addition, 
the use of lower precision formats such as \texttt{(b)float16} %introduces further numerical challenges that
. These problems can be mitigated by limiting the polynomial degree to a maximum of 3, ensuring both stability and efficiency in computation.

\textbf{Alternative Fourier formula.} The definition of Fourier activation given in \ref{def:fourier} is under the Sine-Cosine form. In practice, we use the following equivalent Amplitude-Phase formulation (see Algorithm~\ref{alg:fourier_forward}):
\begin{equation}
    F(x) \mapsto a_0 + \sum_{k=1}^{n} a_k \cos(f_k x - \phi_k)
\end{equation}
as it is less onerous in terms of FLOPs. The learnable parameters here are initialized as follows: $\forall k \in \mathbb{N}^* f_k = k \frac{\pi}{\sqrt{3}}, \phi_k = \frac{\pi}{4}$ and $a_k$ and $a_0$ initialized as in \ref{coro:hermite}.

\textbf{Initializing by fitting a classical activation Function.}
Using a family of orthonormal functions permits an easy calculation of the initialization gain without resorting to the trick of fitting a function to an activation whose gain is known or easy to calculate as in \cite{yang2024kolmogorov} with Safe PadÃ© activation \cite{molina2019pad}.
However, in some cases, such as continuing or fine-tuning a model that was pretrained with a classical activation, using one of the learnable activations presented here to fit a classical activation could still be relevant. By \emph{fitting} we mean performing a Lagrange interpolation. This could be accomplished via a direct method involving the inversion of a Vandermonde matrix (Lagrange, or Newtone methods), or by an iterated gradient descent method (Gauss-Jordan method).
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={$x$-axis}, 
            ylabel={F(x)}, 
            legend pos=north west
        ]
        
        \addplot[very thick, blue] table[x=x, y=GELU, col sep=space] {figs/hermite_3.txt};
        \addlegendentry{GELU}

        \addplot[very thick, orange] table[x=x, y=GELU_deriv, col sep=space] {figs/hermite_3.txt};
        \addlegendentry{GELU deriv.}
        
        \addplot[very thick, red, dashed] table[x=x, y=Hermite, col sep=space] {figs/hermite_3.txt};
        \addlegendentry{Hermite}

        \addplot[very thick, green, densely dashed] table[x=x, y=Hermite_deriv, col sep=space] {figs/hermite_3.txt};
        \addlegendentry{Hermite deriv.};

        \end{axis}
    \end{tikzpicture}
    \caption{Fiting a GELU with a Hermite Activation of degree 3.}
    \label{fig:gelu_vs_hermite_3}
\end{figure}

Two precautions need to be taken, however, when performing such interpolation. The first concerns the maximum degree that should be considered in order to fit the function on a given interval. Figure~\ref{fig:gelu_vs_hermite_3} shows how far a Hermite activation of degree 3 can be accurately fitted, while Figure \ref{fig:gelu_vs_hermite_8} shows the extent to which a Hermite activation of degree 8 can be accurately fitted.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={$x$-axis}, 
            ylabel={F(x)}, 
            legend pos=north west
        ]
        
        \addplot[very thick, blue] table[x=x, y=GELU, col sep=space] {figs/hermite_8.txt};
        \addlegendentry{GELU}

        \addplot[very thick, orange] table[x=x, y=GELU_deriv, col sep=space] {figs/hermite_8.txt};
        \addlegendentry{GELU deriv.}
        
        \addplot[very thick, red, dashed] table[x=x, y=Hermite, col sep=space] {figs/hermite_8.txt};
        \addlegendentry{Hermite}

        \addplot[very thick, green, densely dashed] table[x=x, y=Hermite_deriv, col sep=space] {figs/hermite_8.txt};
        \addlegendentry{Hermite deriv.};

        \end{axis}
    \end{tikzpicture}
    \caption{Fiting a GELU with a Hermite Activation of degree 8.}
    \label{fig:gelu_vs_hermite_8}
\end{figure}

The second precaution concerns the derivative of the activation with respect to the derivative of the target function to be interpolated. A Lagrange interpolation of a function is not always sufficient to fit its k-th derivatives. If we want to interpolate a function and its derivative(s) simultaneously, we refer to this as a Hermite interpolation. In the case of the Fourier activation, we observe in Figure~\ref{fig:gelu_vs_fourier_lagrange_6} that a Lagrange interpolation is not sufficient and that higher-order frequencies occur in the derivative approximation. This phenomenon can be likened to aliasing and can be circumvented by performing a simple Hermite interpolation instead of a Lagrange interpolation, as shown in Figure~\ref{fig:gelu_vs_fourier_6}. \citet{berrut2007fourier} examined the solutions to this last problem.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={$x$-axis}, 
            ylabel={F(x)}, 
            legend pos=north west
        ]
        
        \addplot[very thick, blue] table[x=x, y=GELU, col sep=space] {figs/fourier_lagrange_6.txt};
        \addlegendentry{GELU}

        \addplot[very thick, orange] table[x=x, y=GELU_deriv, col sep=space] {figs/fourier_lagrange_6.txt};
        \addlegendentry{GELU deriv.}
        
        \addplot[very thick, red, dashed] table[x=x, y=Fourier, col sep=space] {figs/fourier_lagrange_6.txt};
        \addlegendentry{Fourier}

        \addplot[thin, green] table[x=x, y=Fourier_deriv, col sep=space] {figs/fourier_lagrange_6.txt};
        \addlegendentry{Fourier deriv.};

        \end{axis}
    \end{tikzpicture}
    \caption{Lagrange interpolation of a GELU with a Fourier Activation of degree 6.}
    \label{fig:gelu_vs_fourier_lagrange_6}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={$x$-axis}, 
            ylabel={F(x)}, 
            legend pos=north west
        ]
        
        \addplot[very thick, blue] table[x=x, y=GELU, col sep=space] {figs/fourier_6.txt};
        \addlegendentry{GELU}

        \addplot[very thick, orange] table[x=x, y=GELU_deriv, col sep=space] {figs/fourier_6.txt};
        \addlegendentry{GELU deriv.}
        
        \addplot[very thick, red, dashed] table[x=x, y=Fourier, col sep=space] {figs/fourier_6.txt};
        \addlegendentry{Fourier}

        \addplot[very thick, green, densely dashed] table[x=x, y=Fourier_deriv, col sep=space] {figs/fourier_6.txt};
        \addlegendentry{Fourier deriv.};

        \end{axis}
    \end{tikzpicture}
    \caption{Hermite interpolation of a GELU with a Fourier Activation of degree 6.}
    \label{fig:gelu_vs_fourier_6}
\end{figure}

The success in fitting classical activations with PadÃ© approximants in \cite{yang2024kolmogorov} could be attributed to the fact that a PadÃ© approximant is by definition the rational function  that coincides with a function to be interpolated to the highest possible order, thus naturally achieving a Hermite interpolation.

A good fit of a non-convex function by a tropical polynomial activation is impossible since tropical polynomials are convex by definition. Therefore, in Appendix~\ref{appendix:tropical_rational} we show how rational tropical activations (an extension of tropical polynomials) could in principle achieve this fitting.

\section{Experiments}
\subsection{Vision Task: ConvNeXt-T Image Classification on ImageNet1k}
We evaluated the ConvNeXt-T model \cite{liu2022convnet} on the ImageNet1k dataset \cite{deng2009imagenet} for single class image classification. The baseline ConvNeXt-T model employed GELU as the activation function in its MLP blocks. To analyze the impact of our learnable activations, we replaced GELU with Hermite polynomial, Fourier trigonometric, and tropical polynomial activation functions. Each model was trained under identical conditions with fixed random seeds to ensure reproducibility and comparability. The evaluation metrics included: training loss, top-1 and top-5 validation accuracy. We report in Table~\ref{res:imagenet} the extremal values of these metrics reached by each of these activation trials. The experimental setup followed the approach and hyperparameter configuration detailed in \cite{liu2022convnet}. 
\begin{table*}[!htbp]
\caption{Training and validation results of ConvNeXt-T (28M) model on ImageNet-1k classification.}
\label{res:imagenet}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
\textbf{Activation} & \textbf{Deg.} & \textbf{Learnable?} & \textbf{Init. from} & \textbf{Train Loss} & \textbf{Val Top-1 (\%)} & \textbf{Val Top-5 (\%)}  \\
\midrule
GELU (baseline) & - & $\times$ & GELU & 2.819 & 82.04 & 96.01 \\
tropical & 6 & $\surd$ & all ones & 2.857 & 82.20 & 95.90 \\
fourier & 6 & $\surd$  & GELU  & \textbf{2.775} & 81.91 & 95.77 \\
hermite & 3 & $\surd$ & Thrm.~\ref{thm:hermite} & 2.790 & \textbf{82.34} & \textbf{96.03} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\textbf{Ablation Studies.} Additionally, ablation studies were performed on this vision task to establish the impact of the degree for the learnable activations (Table~\ref{res:ablation_degree}), the impact of our proposed initialization scheme (Table~\ref{res:ablation_init}), and if making the activation coefficients learnable was useful (Table~\ref{res:ablation_learnable}). Higher degrees generally improved performance, with all proposed activations showing consistent improvements in top-1 and top-5 accuracy as the degree increased. Furthermore, making activation coefficients learnable consistently resulted in better performance across all activation functions. Initialization with the proposed method led to improvements, especially for Hermite activation, where our derived initialization scheme outperformed GELU-based initialization.
\subsection{Language Task: GPT-2 (124M) Next Token Prediction on OpenWebText}
For the language modeling task, we trained the GPT-2 model \cite{radford2019language} on the OpenWebText dataset \cite{Gokaslan2019OpenWeb} for next-token prediction. The baseline GPT-2 used GELU activation, and we compared it against the best Hermite, Fourier, and Tropical activations configurations found using the ablation studies on the previous vision experiment. All models were trained with identical hyperparameters and initialization seeds to ensure consistent and reproducible comparisons. The evaluation metrics included: training and test losses and perplexities (which are simply the exponential of the loss).  We report in Table~\ref{res:gpt} the extremal values of these metrics reached by each of these activation trials and in Figure~\ref{fig:gpt} the overall validation loss. The experimental design followed the guidelines established in \cite{radford2019language} and the open source reproduction available at \cite{Karpathy2022}. We used a total batch size of $786,432$ with a context length of $1024$ tokens for a total of $210,000$ iterations.
\begin{table*}[!htbp]
\caption{Training and validation results for next-token prediction using GPT-2 (124M) model with different activations.}
\label{res:gpt}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
\textbf{Activation} & \textbf{Deg.} & \textbf{Learnable?} & \textbf{Init. from} & \textbf{Train PPL} & \textbf{Train Loss} & \textbf{Val PPL} & \textbf{Val Loss} \\
\midrule
GELU (baseline)   & - & $\times$ & GELU & 18.87 & 2.9379  & 19.24 & 2.9571 \\
% GPTTropicalRational & 18.89 & 2.9382  & 19.21 & 2.9553 \\
Tropical    & 6 & $\surd$ & all ones  & 18.56 & 2.9208  & 18.64 & 2.9256 \\
Fourier     & 6 & $\surd$ & GELU & 18.49 & 2.9171  & 18.72 & 2.9296   \\
Hermite     & 3 & $\surd$ & Thrm.~\ref{thm:hermite} & \textbf{18.17} & \textbf{2.9001}  & \textbf{18.39} & \textbf{2.9119} \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

Both experiments were conducted under fixed configurations to ensure that any observed differences were solely due to the choice of activation function, allowing for fair and reproducible comparisons\footnote{The code to reproduce the experiments, the training, along with the different model weights, is available at: \href{https://github.com/K-H-Ismail/torchortho}{torchortho}.}.
% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\subsection{Parameters, Memory, and Flops}
The number of additional parameters introduced by the activations presented here is marginal compared to the other network parameters. The additional number of parameters introduced by a Hermite or Tropical polynomial activation of degree $d$ are $d+1$ and by a Fourier activation of degree $d$ are $3d + 1$ per activation. 
In terms of memory and computational complexity, the choice of a learnable activation function can significantly impact the efficiency of the network. When evaluated using the explicit formula, the complexity of all activations is multiplied by a factor which is $\mathcal{O}(d^2)$ in terms of Flops and memory, which constitutes a limitation of this algorithm. In contrast, using the recursive formulation reduces this to a factor $\mathcal{O}(d)$ in Flops and memory, making it a more efficient alternative.
Despite the higher complexity of Hermite activations when using the explicit formulation, it is possible to mitigate these limitations through more advanced numerical techniques such as Horner's method, which reduces the number of required multiplications.
For the Fourier and the Tropical activation, the multiplicative complexity factor remains $\mathcal{O}(d)$ for both FLOPs and memory. 
Overall, while the computational overhead of these advanced activation functions can be significant, their expressive power may justify the trade-off in applications where traditional activation functions fall short.

\section{Discussion}

The results presented in this paper demonstrate the potential of using learnable activation functions based on orthogonal function bases and tropical polynomials in large-scale neural network tasks. Our experiments on ImageNet-1K and OpenWebText with deep models such as ConvNeXt and GPT-2 show for the first time that such activations can lead to significant improvements over traditional static functions like ReLU and GELU, both in terms of classification accuracy and language modeling perplexity. 

This challenges the long-standing notion that polynomial activations are inherently unsuitable for deep learning, as demonstrated by prior work. Our approach provides empirical evidence that, with appropriate initialization, polynomial activations can indeed be competitive.

One of the key takeaways from our findings is the effectiveness of our proposed variance-preserving initialization scheme. The choice of orthogonal functions plays an essential role in achieving a closed-form expression for the second-order moment. Furthermore, the use of tropical polynomials, which are not orthogonal, introduces a Flops-light alternative approach to polynomial activations.

While our approach shows promise, there are several avenues for future exploration. Extending the framework to other activation families, such as wavelets is straightforward.  Multiplying the Hermite activation presented in this work  by the term $e^{-\frac{x^2}{2}}$ gives what is known as Hermitian wavelets \cite{brackx2008hermitian, pandey2020continuous}, and applying the same to the Fourier activation yields the Morlet wavelet \cite{grossmann1984decomposition} (or Gabor wavelet \cite{gabor1946theory}). Wavelets retain good orthogonal properties with respect to the adequate scalar product and the calculation of the second moment is slightly modified to take account of the additional decaying exponential term.
Using wavelet activations instead of polynomials could enhance variance stability by providing finite function support, with potential bio-plausibility implications.

By expressing a Fourier series in its complex form, a network with Fourier activation can be viewed as a complex-valued neural network, offering a natural framework for modeling neuronal synchronization through the phase and amplitude relationships of oscillatory brain activity.

Extension to other non-orthogonal functions such as rational functions could be done for example by means of a Laplace transform of the Fourier activation.

\section{Conclusion}
In this work, we introduced a novel framework for integrating learnable activation functions based on orthogonal function bases and tropical polynomials into deep neural networks, addressing challenges like variance preservation and stable gradient flow. Extensive experiments with the ConvNeXt model on ImageNet1k and the GPT-2 model on OpenWebText demonstrated that learnable activations outperform traditional static functions on large-scale tasks, showcasing their practical viability and challenging conventional beliefs about polynomial activations in neural networks. Our results pave the way for representing deep neural networks as polynomial mappings, with future work focused on exploring a careful relaxation of these mappings.

\iffalse

%\subsection{Links between the PadÃ© approximant and Fourier series}
%A link between the Fourier series and the PaddÃ© approximant can be established using the Laplace transform, defined for a suitable function $f$ as follows:

%\begin{equation}
% write Laplace transform definition
%{\displaystyle {\mathcal {L}}\{f\}(s)=\int _{0}^{\infty }f(t)e^{-st}\,dt}
%\end{equation}

%Indeed, taking the definition of the Fourier activation established in equation \ref{eq:fourier}, we obtain the following activation:

%\begin{equation}
%\label{fourier_laplace}
% write Fourier-Laplace transform definition
%\end{equation}

%Note that the latter is âsafeâ in the sense that it has no poles (zeros in the denominator). Unlike the Safe-PadÃ© activation established in \cite{molina2019pad} and then adopted by \cite{yang2024kolmogorov}, the activation defined here doesn't enforce non-nullity of the denominator by a process (like the absolute value in \cite{molina2019pad}), but comes with a nonzero denominator intrinsically (via the square in the denominator resulting from the Laplace transform applied to the $\sin$ and $\cos$ functions). Nevertheless, our definition in \ref{eq:fourier_laplace} imposes a degree of the rational fraction equal to $-1$. However, in \cite{yang2024kolmogorov}, the authors find empirically that the best classification performance is obtained for rational fractions of degree $+1$ (degree $5$ in the nominator and degree $4$ in the denominator, the current case). We believe that a rational fraction of degree $+1$ and its derivative (of degree $0$) are asymptotically equivalent to a classical activation (ReLU type) and its derivative respectively, which could explain their success in drop-in replacement mode by fitting the approximant to a classical activation in deep architectures optimized for the latter as was done in \cite{yang2024kolmogorov}. %A rational function of degree greater than 1 will have as its gradient a rational function of degree greater than 0, and will therefore be more prone to gradient explosion phenomena. A rational function of degree lower than 1 will have a gradient of degree lower than 0, leading to vanishing gradient problems.

%\subsection{A note on bio-plausibility}
Neither the Hermite activation (nor any polynomial activation) has finite support. To make these activations of finite support, we propose here to multiply them by the term $e^{-\frac{x^2}{2}}$. For Hermite polynomials, this gives what are known as Hermitian wavelets \cite{brackx2008hermitian, pandey2020continuous}. Here, for $n \in \mathbb{N}$, we give the definition of a Hermitian mother wavelet $\Psi$ of order $n$.

\begin{equation}
{\displaystyle \Psi _{n}(x)=(2n)^{-{\frac {n}{2}}}c_{n}\operatorname {He} _{n}\left(x\right)e^{-{\frac {1}{2}}x^{2}}}
\end{equation}
With $\operatorname {He} _{n}$ denoting the ${\displaystyle n^{\textrm {th}}}$ probabilist's Hermite polynomial and $c_n$ a normalization term. For the particular degree of $n = 2$, the obtained wavelet is known as the Ricker wavelet.

In our case, we are interested instead in the Hermitian wavelet activation formed using the sum of Hermitian mother wavelets of degrees ranging from $0$ to $n \in \mathbb{N}^*$.
\begin{align}
    F \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
     & x \mapsto F(x)= \sum^{n}_{k=0} \frac{a_k}{\sqrt{k!}}\operatorname{He}_k(x) e^{-{\frac {1}{2}}x^{2}}
\end{align}
With $(a_k)_{k \in \llbracket 0,n \rrbracket }$ initialized as in \ref{thm:hermite}.

Hermitian wavelets retain good orthogonal properties with respect to the scalar product defined for a Gaussian random variable. The calculation of the second-order moment is slightly modified to take account of the additional decaying exponential term.


The Fourier activation with the coefficients defined in \ref{eq:fourier} has finite energy but infinite support. By the same procedure described above, we can impose finite support while preserving the orthonormality properties of its terms by multiplying by a factor $e^{-\frac{x^2}{2}}$. This gives what is known as the Morlet wavelet \cite{grossmann1984decomposition} (or Gabor wavelet \cite{gabor1946theory}).

Here again, we are interested instead in the wavelet activation formed using the sum of Gabor mother wavelets of degrees ranging from $0$ to $n \in \mathbb{N}^*$

\begin{equation}
% write def Morlet mother wavelet
\end{equation}

In short, whatever the orthogonal polynomial or trigonometric family of functions under consideration, this same procedure can always lead to an orthonormal family of wavelets with the same properties, but with finite support.

Polynomial piecewise functions (such as B-splines) and rational functions (such as the PadÃ© approximant) can exhibit finite support properties. On the other hand, these last lack the orthogonality property. Several works have aimed to formulate orthogonal splines \cite{mason1993orthogonal, alavi2023orthogonal} and orthogonal rational functions \cite{bultheel2001orthogonal}, or even a theory of spline wavelets \cite{chui1991cardinal} and rational wavelets \cite{zheng1999rational, choueiter2007implementation}.

%\textbf{Why is this bio-plausibility note mentioned?} In many neuro-physiological studies, wavelets are used to explain observed physiological phenomena \cite{daugman1985uncertainty, cohen2014analyzing, bernardino2005real}. The polynomial/trigonometric activations presented here may be overlooked by scientific communities concerned with the bio-plausibility of activation in neural networks. This paragraph is therefore, if not a tangible proof of the bio-plausibility of wavelet activations, at least an outline of how polynomial and trigonometric activations can be adapted. Our preliminary empirical results also confirm that wavelet activations remain competitive on image classification tasks despite a slight decrease in accuracy and an increase in the overall number of FLOPs.

By expressing a Fourier series in its complex form, a neural network with Fourier activation can be considered a complex-valued neural network, which provides a natural framework for modeling neuronal synchronization by capturing the phase and amplitude relationships of oscillatory brain activity.

Oscillatory brain activity, often studied through EEG and MEG using Fourier or wavelet analysis, can be effectively modeled using deep networks with Fourier activations, inherently representing brain oscillations as a Fourier series.
%\subsection{Fourier activations and complex neural networks}
%Any Fourier series under the form: 

%\begin{equation}
% write def Fourier series complex definition
%\end{equation}
%can be rewritten in the following complex form:

%\begin{equation}
% write def Fourier series complex definition
%\end{equation}

%We believe that with this equivalent definition, the neural network with Fourier activation meets the definition of a neural network with complex activation, which constitutes a wide field of research for neural networks \cite{bassey2021survey}. Indeed, some research suggests that neuronal synchronization, a key mechanism supporting cognitive and sensory processes, can be effectively modeled using complex-valued deep networks, as these networks intrinsically capture the phase and amplitude relationships of oscillatory neuronal activity, offering a natural framework for understanding and simulating the dynamics of synchronization between brain regions \cite{reichert2013neuronal, sheng2023global}.


%\subsection{Fourier activations, oscillatory processes, and traveling waves}

%The activity of a mammalian brain can be seen as an oscillatory process \cite{alamia2024traveling}. Indeed, the study of traveling waves is the main focus of the branch of neuroscience that studies spatiotemporal patterns of neural activity that propagate across brain regions. These traveling waves are often observed in Electroencephalogram (EEG) and magnetoencephalography (MEG). The demonstration here of a Fourier / wavelet activation model shows just how directly this model can be adapted to model oscillatory brain phenomena. Fourier / wavelet analysis is the operation of choice when studying oscillatory phenomena in the brain. Here, in the case of a deep network with Fourier activations, we believe this will facilitate the study of oscillatory phenomena in the brain, as the activations are no longer analyzed by Fourier analysis but are, in their very definition, a Fourier series.

\section{Conclusion}
In conclusion, we have shown for the first time that it is possible to scale deep neural networks with learnable polynomial, trigonometric, and tropical activations. This was only possible by carefully calculating the variance of the latter. In addition, we have shown that it is possible to initialize them to match known activations, which could be useful for transfer learning tasks. Finally, we have shown 

\noindent\textbf{Future work:} The Hermite and Fourier bases have been chosen here because they are orthogonal with respect to the scalar product associated with the most common distributions (the normal and uniform distributions respectively). In addition, these bases are easy to implement and interpret, especially the Fourier base. This approach can therefore be generalized to families of functions that can supposedly be more expressive, economical in terms of parameters, and offer higher accuracy.


ouverture wavelets 
summations: Fejer summ, Riesz mean, sigma approx -> Gibbs
summations: Borel summ
Learning polynomial degree

\fi
% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
In alphabetical order, we thank Emile de Bruyn, Jan Ebert, Jiangtao Wang, and Oleg Filatov for their helpful discussions and feedback on this manuscript. We also thank David Guo and @iiisak for helpful technical discussions. This work would not have been possible without financial and computational support. This research was supported by the German Federal Ministry for Economic Affairs and Climate Action through the project âNXT GEN AI METHODSâ. We gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the Supercomputers JUWELS and JURECA at JÃ¼lich Supercomputing Centre (JSC).
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Forward and Backward Second Moment calculation for the ReLU Activation Function}
\label{appendix:relu}
\subsection{Second Moment of the ReLU Activation Function}

The Rectified Linear Unit (ReLU) activation function \cite{nair2010rectified}, defined as: 
\begin{equation}
    \text{ReLU}(x) = \max(0, x)
\end{equation}
is commonly used in neural networks due to its simplicity and effective gradient propagation. When \( x \) is drawn from a standard normal distribution \( x \sim \mathcal{N}(0, 1) \), the second moment of the ReLU function is:
\begin{equation}
\mathbb{E}[\text{ReLU}(x)^2] = \int_{0}^{\infty} x^2 \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx = \frac{1}{2}
\end{equation}

%This result implies that, on average, ReLU halves the variance of its input. This knowledge is leveraged in initialization schemes such as Kaiming uniform or normal, where the weights are scaled by \( \sqrt{2/n} \) to compensate for this reduction in variance.
\subsection{Second Moment of the Derivative of ReLU}
The derivative of ReLU, given by:
\begin{equation}
\frac{d}{dx} \text{ReLU}(x) =
\begin{cases}
1, & x > 0, \\
0, & x \leq 0,
\end{cases}
\end{equation}
acts as a binary indicator of positive inputs. The second moment of this derivative when \( x \sim \mathcal{N}(0, 1) \) is:
\begin{equation}
\mathbb{E}\left[\left(\frac{d}{dx} \text{ReLU}(x)\right)^2\right] = \int_{0}^{\infty}  \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx = \frac{1}{2}
\end{equation}
This result matches the variance of the ReLU function itself and validates the gain of 2 for variance-preserving weight initialization with ReLU activations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of the Propositions~\ref{prop:hermite_forward_gain} and~\ref{prop:hermite_backward_gain}}
\label{appendix:hermite}
The orthonormality property~\ref{property:orthonormal} means that:
$\forall m,n \in \mathbb{N}^2$,
    \begin{equation}
    \label{eq:hermite_ortho1}
    \int_{-\infty}^{\infty} \frac{\operatorname{He}_n(x)^2}{n!} \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x=1
\end{equation}
and if $m \neq n $
\begin{equation}
    \label{eq:hermite_ortho2}
    \int_{-\infty}^{\infty} \operatorname{He}_m(x) \operatorname{He}_n(x) \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x=0
\end{equation}
Given the definition (Def.~\ref{def:hermite}) of a Hermite activation $F$ , we have:
\begin{align}
\mathbb{E}\left[F(x)^2\right]=&\int_{-\infty}^{+\infty} F^2(x) \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x \\
=&\int_{-\infty}^{+\infty}\left(\sum^{n}_{k=0} \frac{a_k}{\sqrt{k!}}\operatorname{He}_k(x)\right)^2  \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x
\end{align}
Using the orthogonal property Eq.~\ref{eq:hermite_ortho2}, the cross terms cancel out, and we have:
\begin{align}
\mathbb{E}\left[F(x)^2\right]=&\int_{-\infty}^{+\infty}\sum^{n}_{k=0} \frac{a_k^2}{k!}\operatorname{He}_k(x)^2  \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x\\
=&\sum^{n}_{k=0} \frac{a_k^2}{k!}\int_{-\infty}^{+\infty}\operatorname{He}_k(x)^2  \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x
\end{align}
Given the normality property Eq.~\ref{eq:hermite_ortho1}, we have
\begin{align}
\mathbb{E}\left[F(x)^2\right]=&\sum^{n}_{k=0} a_k^2\int_{-\infty}^{+\infty}\frac{\operatorname{He}_k(x)^2}{k!}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x\\
=&\sum^{n}_{k=0} a_k^2
\end{align}
Having designed the initialization gain for the activation $F$ (Eq.~\ref{eq:hermite}) so as it equals $1$, we now need to enforce this same gain for its derivative. Indeed, we are going to use the gradient descent algorithm to train our learnable activation networks, and having an activation gradient of high (respectively low) variance could lead to exploding (respectively vanishing) gradients, a nondesirable property for deep neural networks trained with gradient backpropagation.
The derivative of $F$ (Eq.~\ref{eq:hermite}) is written as follows:
\begin{align}
\label{eq:hermite_prime}
    F^\prime \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
     & x \mapsto F^\prime(x)=\sum^{n}_{k=0} \frac{a_k}{\sqrt{k!}}\operatorname{He}_k^\prime(x)
\end{align}
Knowing that $\forall k \in \mathbb{N}^*$ $\forall x \in \mathbb{R}$:
\begin{equation}
    \operatorname{He}_k^\prime(x) = k \operatorname{He}_{k-1}(x)
\end{equation}
The definition of $F^\prime$ becomes:
\begin{align}
    F^\prime \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
     & x \mapsto F^\prime(x)=\sum^{n}_{k=1} \frac{ka_k} {\sqrt{k!}}\operatorname{He}_{k-1}(x)
\end{align}
Thus, the second-order moment of $F'$ is:
\begin{align}
    \mathbb{E}\left[F^\prime(x)^2\right]&=\int_{-\infty}^{+\infty}\left(\sum^{n}_{k=1} \frac{ka_k} {\sqrt{k!}}\operatorname{He}_{k-1}(x)\right)^2 \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x
\end{align}
By the orthogonal property Eq.~\ref{eq:hermite_ortho2}, the cross terms cancel out, and we have:
\begin{align}
    \mathbb{E}\left[F^\prime(x)^2\right]&=\int_{-\infty}^{+\infty}\sum^{n}_{k=1} \frac{k^2a_k^2} {k!}\operatorname{He}_{k-1}(x)^2 \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x \\
    &=\sum^{n}_{k=1} k^2a_k^2\int_{-\infty}^{+\infty}\frac{\operatorname{He}_{k-1}(x)^2}{k(k-1)!}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x\\
    &=\sum^{n}_{k=1} ka_k^2\int_{-\infty}^{+\infty}\frac{\operatorname{He}_{k-1}(x)^2}{(k-1)!}\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} d x
\end{align}
By the normality property Eq.~\ref{eq:hermite_ortho1}, we finally have:
\begin{equation}
    \mathbb{E}\left[F^\prime(x)^2\right]=\sum^{n}_{k=1}ka_k^2
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of the Propositions~\ref{prop:fourier_forward_gain} and~\ref{prop:fourier_backward_gain}}
\label{appendix:fourier}
We consider a truncated Fourier series expansion of order $n \in \mathbb{N}^*$, \( F \) and investigate its second moment and the second moment of its derivative. The random variable \( x \) is assumed to follow a uniform distribution on the interval \( [-\pi, \pi] \), denoted as:
\begin{equation}
    x \sim \mathcal{U}\left(-\pi, \pi\right)
\end{equation}

\begin{property}
\label{property:fourier_ortho}
The equivalent of the \ref{property:orthonormal} property for trignometric functions is given by $\forall m,n \in \mathbb{Z}^2$:
\begin{align}
\left\{\begin{aligned}
    \int_{-\pi}^{\pi} \cos(m x) \cos(n x)  d x&=\pi\delta_{n m}\\
    \int_{-\pi}^{\pi} \sin(m x) \sin(n x)  d x&=\pi\delta_{n m}\\
    \int_{-\pi}^{\pi} \cos(m x) \sin(n x)  d x&=0\\
\end{aligned}\right.
\end{align}
With $\delta_{nm}$ the Kronecker delta function.
\end{property} 
Let us now consider the following activation function:
\begin{align}
    F \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
    F(x) &\mapsto a_0 +
    \sum_{k=1}^{n} \left(a_k \cos(k x) + b_k \sin(k x)\right)
\end{align}
where \( (a_k)_{k \in \mathbb{N}} \) and \( (b_k)_{k \in \mathbb{N}^*} \) are the real Fourier coefficients.

To compute the second moment of the Fourier activation \( F(x) \), we need to compute the expected value of \( F(x)^2 \):
\begin{equation}
    \mathbb{E}[F(x)^2] = \int_{-\pi}^{\pi} F(x)^2 p(x) \, dx
\end{equation}
where \( p(x) \) is the probability density function (PDF) of the uniform distribution:
\begin{equation}
    p(x) = \frac{1}{2\pi}, \quad x \in [-\pi, \pi]
\end{equation}
Taking the square of the definition in Eq.~\ref{eq:fourier} gives:
\begin{equation}
F(x)^2 = \left( a_0 + \sum_{k=1}^{n} \left(a_k \cos(k x) + b_k \sin(k x)\right) \right)^2
\end{equation}
Using the fact that for a uniform distribution \( x \sim \mathcal{U}(-\pi, \pi) \), the second moment of $F$ over this interval is:
\begin{align}
	\mathbb{E}[F(x)^2] = &\frac{1}{2\pi} \int_{-\pi}^{\pi} F(x)^2 \, dx  \\
	= &\frac{1}{2\pi} \int_{-\pi}^{\pi}\left( a_0 + \sum_{k=1}^{n} \left(a_k \cos(k x) + b_k \sin(k x)\right) \right)^2 \, dx 
\end{align}
Using the orthogonal property \ref{property:fourier_ortho} and the linearity of the integral, we have:
\begin{align}
	\mathbb{E}[F(x)^2] = & a_0^2 + \frac{1}{2\pi} \sum_{k=1}^{n} \int_{-\pi}^{\pi} a_k^2 \cos^2(k x) + b_k^2\sin^2(k x) \, dx  \\ 
    = &a_0^2 + \frac{1}{2\pi} \sum_{k=0}^{n} a_k^2\left(\frac{\sin(2\pi k)}{2k} + \pi\right)  + b_k^2\left(\pi-\frac{\sin(2\pi k)}{2k} \right)     
\end{align}
The second moment simplifies to:
\begin{equation}
\mathbb{E}[F(x)^2] = a_0^2 +  \frac{1}{2}\sum_{k=1}^{n} \left( a_k^2 + b_k^2 \right)
\end{equation}
Next, we compute the second moment of the derivative of the Fourier activation \( F^\prime \) from its definition in Eq.~\ref{eq:fourier_final}. The derivative of \( F \) is given by:
\begin{align}
    F^\prime \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
    F^\prime(x) &\mapsto \sum_{k=1}^{n} k\frac{\pi}{\sqrt{3}}\left(-a_k \sin(k\frac{\pi}{\sqrt{3}} x) + b_k \cos(k\frac{\pi}{\sqrt{3}} x)\right)
\end{align}
We suppose now that $x \sim \mathcal{U}(-\sqrt{3}, \sqrt{3})$.

By using the orthonormality argument~\ref{property:orthonormal} and adapting the formula found in \ref{eq:fourier_simple}, the second moment of $F^\prime$ becomes:
\begin{equation}
\label{eq:fourier_deriv_simple}
    \mathbb{E}[F^\prime(x)^2] = \sum_{k=1}^{n} k^2\frac{\pi^2}{6}\left( a_k^2 + b_k^2 \right)
\end{equation}
Equality between \ref{eq:fourier_simple} and \ref{eq:fourier_deriv_simple} imposes that:
\begin{equation}
    a_0^2 = \sum^{n}_{k=1} \left(\frac{\pi^2}{6}k^2-\frac{1}{2}\right)(a_k^2 + b_k^2)
\end{equation}

Thus, in the limit case $n \to +\infty$, by taking $\forall k \in  \llbracket 1,n \rrbracket \ a_k = b_k = \frac{1}{k^p}$ with $p>1$, we have $a_0= \sqrt{\frac{\pi^2}{3}\zeta(2p-2)-\zeta(2p)}$ and , with $\zeta$ the Riemann function $\forall x \in [1,+\infty] \colon \zeta(x) = \displaystyle{\sum_{k=1}^{\infty} \frac{1}{k^x}}$. 

If we choose $p=2$, this gives:
\begin{align}
    \forall k \in \llbracket 1,n \rrbracket \quad a_k = \frac{1}{k^2}\\
    a_0 = \sqrt{\frac{\pi^2}{3} \frac{\pi^2}{6} - \frac{\pi^4}{90}} = \frac{2\pi^2}{3\sqrt{10}}
\end{align}
%%%%%%%%%
\section{Proof of the Proposition~\ref{prop:mapping}}
\label{appendix:mapping}

    \textbf{Base case:} For $L = 1$, the network takes the form
    \[
        F(x) = W_1 \sigma(W_0 x + b_0) + b_1.
    \]
    Since $\sigma$ is a polynomial of degree $d$, applying it to the affine transformation $W_0 x + b_0$ yields a polynomial mapping of degree at most $d$. Therefore, $F(x)$ is a polynomial mapping of degree at most $d$.

    \textbf{Inductive step:} Assume the statement holds for $L-1$ layers, meaning the network $F_{L-1}(x)$ is a polynomial mapping of degree at most $d^{L-1}$. For the $L$-layer case, we have
    \[
        F(x) = W_L \sigma(F_{L-1}(x)) + b_L.
    \]
    Since $\sigma$ is a polynomial of degree $d$, applying it to $F_{L-1}(x)$ results in a polynomial of degree at most $d \cdot d^{L-1} = d^L$. Thus, by induction, the statement holds for all $L \geq 1$.


\begin{corollary}
    Any deep neural network with polynomial activation functions realizes a polynomial mapping whose degree grows exponentially with the number of layers.
\end{corollary}

\begin{remark}
    The total number of monomial terms in this mapping is $\binom{d^L + n}{d^L}$.
\end{remark}
%%%%%%%
\newpage
\section{Algorithms}
\label{appendix:algorithms}
\begin{algorithm}[!htbp]
   \caption{Initialization of Hermite Grid and Coefficients}
   \label{alg:init_grid}
\begin{algorithmic}
   \STATE {\bfseries Input:} Polynomial degree $n$
   \STATE {\bfseries Output:} Coefficients tensor $\text{coeffs}$, Grid of powers tensor $\text{grid}$
   \STATE 
   \STATE Initialize $\text{coeffs}$ and $\text{grid}$ as zero matrices of shape $[n+1, n//2 + 1]$
   \FOR{$i=0$ {\bfseries to} $n$}
      \FOR{$j=0$ {\bfseries to} $\frac{n}{2}$}
         \IF{$j \leq \frac{i}{2}$}
            \STATE $\text{coeffs}[i][j] \leftarrow (-1)^j e^{\left(0.5 \log(i!) - \log(j!) - \log((i - 2 j)!)  - j \log(2)\right)} $
            \STATE $\text{grid}[i][j] \leftarrow i - 2 j$
         \ELSE
            \STATE $\text{coeffs}[i][j] \leftarrow 0$
            \STATE $\text{grid}[i][j] \leftarrow 0$
         \ENDIF
      \ENDFOR
   \ENDFOR
   \STATE \textbf{return} $\text{coeffs}, \text{grid}$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htbp]
   \caption{Hermite Activation Function Forward Pass}
   \label{alg:hermite_forward}
   \begin{algorithmic}
      \STATE \textbf{Input:} Input tensor $x$, polynomial degree $n$
       \STATE {\bfseries Parameters:} Learnable polynomial coefficients $A \in \mathbb{R}^n$      
      \STATE \textbf{Output:} Output tensor after applying Hermite activation function
      \STATE 
      \STATE $\text{coeffs}, \text{grid} \leftarrow  \text{Initialize\_coeffs\_grid()}$
      \STATE \textbf{Procedure} Forward($x$):
      \STATE \quad $x \leftarrow x.\text{repeat($n+1$).repeat($n//2+1$)}$
      \STATE \quad $x \leftarrow |x|^{\text{grid}} \odot \text{sign}(x)^{\text{grid}}$
      \STATE \quad $x \leftarrow x @ \text{coeffs}$
      \STATE \quad $x \leftarrow x @ A$
      \STATE \quad \textbf{return} $x$
      \STATE \textbf{End Procedure}
   \end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htbp]
   \caption{Hermite Forward CUDA Kernel}
   \label{alg:hermite_cuda_forward}
   \begin{algorithmic}
      \STATE \textbf{Input:} Input tensor $x$, degree $n$, output tensor $\text{out}$
      \STATE \textbf{Output:} Computed Hermite polynomials up to degree $n$
      \STATE
      \STATE \textbf{Procedure} HermiteForwardCUDA($x, n, \text{out}$):
      \STATE \quad \textbf{for} $i$ in parallel index $\text{size}(x)$:
      \STATE \quad \quad $\text{out}[i \cdot n] \leftarrow 1.0$
      \STATE \quad \quad \text{if } $n > 1$: $\text{out}[i \cdot n + 1] \leftarrow x[i]$
      \STATE \quad \quad \text{for } $k = 2$ to $n$:
      \STATE \quad \quad \quad $\text{out}[i \cdot n + k] \leftarrow x[i] \cdot \text{out}[i \cdot n + k - 1] - (k - 1) \cdot \text{out}[i \cdot n + k - 2]$
      \STATE \textbf{End Procedure}
   \end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htbp]
   \caption{Hermite Backward CUDA Kernel}
   \label{alg:hermite_cuda_backward}
   \begin{algorithmic}
      \STATE \textbf{Input:} Input tensor $x$, degree $n$, output tensor $\text{out}$, gradient tensor $\text{grad\_out}$
      \STATE \textbf{Output:} Computed gradients for Hermite polynomials
      \STATE
      \STATE \textbf{Procedure} HermiteBackwardCUDA($x, n, \text{out}, \text{grad\_out}$):
      \STATE \quad \textbf{for} $i$ in parallel index $\text{size}(\text{grad\_out})$:
      \STATE \quad \quad $\text{grad} \leftarrow 0.0$
      \STATE \quad \quad \text{for } $k = 1$ to $n$:
      \STATE \quad \quad \quad $\text{grad} \leftarrow \text{grad} + x[i \cdot n + k] \cdot k \cdot \text{out}[i \cdot n + k - 1]$
      \STATE \quad \quad $\text{grad\_out}[i] \leftarrow \text{grad}$
      \STATE \textbf{End Procedure}
   \end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htbp]
   \caption{Fourier Activation Function Forward Pass}
   \label{alg:fourier_forward}
   \begin{algorithmic}
      \STATE \textbf{Input:} Input tensor $x$, degree $n$
      \STATE {\bfseries Parameters:} Learnable coefficients $A \in \mathbb{R}^n$, fundamental $a \in \mathbb{R}$, phases $P \in \mathbb{R}^n$, frequencies $F \in \mathbb{R}^n$,      
      \STATE \textbf{Output:} Output tensor after applying Fourier activation function
      \STATE
      \STATE \textbf{Procedure} FourierActivation($x$):
      \STATE \quad $x \leftarrow x.\text{repeat($n+1$)}$
      \STATE \quad $x \leftarrow F \odot x - P$
      \STATE \quad $x \leftarrow \cos(x)$
      \STATE \quad $x \leftarrow x @ A$
      \STATE \quad $x \leftarrow x + a$
      \STATE \quad \textbf{return} $x$
      \STATE \textbf{End Procedure}
   \end{algorithmic}
\end{algorithm}
\begin{algorithm}[!htbp]
   \caption{Tropical Activation Function Forward Pass}
   \label{alg:tropical_forward}
   \begin{algorithmic}
      \STATE \textbf{Input:} Input tensor $x$, degree $n$
      \STATE {\bfseries Parameters:} Learnable coefficients $A \in \mathbb{R}^n$       
      \STATE \textbf{Output:} Output tensor after applying Tropical activation function
      \STATE
\STATE $\text{powers} \leftarrow  \text{range(0,n+1)}$      
      \STATE \textbf{Procedure} Forward($x$):
      \STATE \quad $x \leftarrow x.\text{repeat($n+1$)}$
      \STATE \quad $x \leftarrow \max(x\odot \text{powers} + A, \text{dim}=-1)$
      \STATE \quad \textbf{return} $x$
      \STATE \textbf{End Procedure}
   \end{algorithmic}
\end{algorithm}
%%%%%%
\newpage
\section{Rational Tropical Activation}
\label{appendix:tropical_rational}
\begin{definition}
The tropical quotient $\oslash$ of $x$ over $y$ is defined as:
\begin{equation}
    x \oslash y:=x-y
\end{equation}
\end{definition}
\begin{definition}
The \emph{tropical rational activation} $F$ is defined as the \emph{quotient} of two tropical polynomials $F_1$ and $F_2$ of degree $m,n \in \mathbb{N}^2$ respectively.
    \begin{align}
    F \colon & \mathbb{R} \to \mathbb{R}  \nonumber \\
    F(x)  &\mapsto  F_1(x) \oslash F_2(x) :=  F_1(x) - F_2(x)
\end{align}
\end{definition}
An example of fitting a classical activation (GELU) with a rational tropical activation is shown in Figure~\ref{fig:gelu_vs_tropical}. Rational tropical activation is understood here in the general sense, i.e. with real powers.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={$x$-axis}, 
            ylabel={F(x)}, 
            legend pos=north west
        ]
        
        \addplot[very thick, blue] table[x=x, y=GELU, col sep=space] {figs/tropical_rational_6.txt};
        \addlegendentry{GELU}

        \addplot[very thick, orange] table[x=x, y=GELU_deriv, col sep=space] {figs/tropical_rational_6.txt};
        \addlegendentry{GELU deriv.}
        
        \addplot[very thick, red, dashed] table[x=x, y=Tropical_rational, col sep=space] {figs/tropical_rational_6.txt};
        \addlegendentry{Tropical Rat.}

        \addplot[very thick, green, densely dashed] table[x=x, y=Tropical_rational_deriv, col sep=space] {figs/tropical_rational_6.txt};
        \addlegendentry{Tropical Rat. deriv.};

        \end{axis}
    \end{tikzpicture}
    \caption{Hermite interpolation of a GELU with a Tropical Rational Activation of degree 6 in both the nominator and the denominator.}
    \label{fig:gelu_vs_tropical}
\end{figure}
%%%%%%%%
\newpage
\section{Ablation Studies}
\label{appendix:ablations}
\begin{table*}[!htbp]
\caption{Ablation studies for the degree of the activation on ConvNeXt-T model.}
\label{res:ablation_degree}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
\textbf{Activation} & \textbf{Degree} & \textbf{Train Loss} & \textbf{Val Top-1 (\%)} & \textbf{Val Top-5 (\%)}\\
\midrule
tropical & 1 & 2.925 & 81.60 & 95.73 \\
tropical & 3 & 2.866 & 82.01 & 95.91 \\
tropical & 5 & 2.863 & 82.18 & 96.00 \\
tropical & 6 & 2.857 & 82.20 & 95.90 \\
\midrule
fourier & 1 & 2.872 & 80.29 & 95.03 \\
fourier & 3 & 2.850 & 80.61 & 95.26 \\
fourier & 5 & 2.844 & 80.69 & 95.41 \\
fourier & 6 & 2.837 & 80.93 & 95.44 \\
\midrule
hermite & 2 & 2.833 & 81.66 & 95.71 \\
hermite & 3 & 2.790 & 82.34 & 96.03 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\begin{table*}[!htbp]
\caption{Ablation studies for the initialization of the activation on ConvNeXt-T model.}
\label{res:ablation_init}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\textbf{Activation} & \textbf{Degree} & \textbf{Initialized from}  & \textbf{Train Loss} & \textbf{Val Top-1 (\%)} & \textbf{Val Top-5 (\%)}\\
\midrule
fourier & 6 & GELU  & 2.775 & 81.91 & 95.77 \\
fourier & 6 & Thrm.~\ref{thm:hermite} & 2.837 & 80.93 & 95.44 \\
\midrule
hermite & 3 & GELU & 2.809 & 82.04 & 95.91 \\
hermite & 3 & Thrm.~\ref{thm:hermite} & 2.790 & 82.34 & 96.03 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\begin{table*}[!htbp]
\caption{Ablation studies for the learnability of the parameters of the activation on ConvNeXt-T model.}
\label{res:ablation_learnable}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\textbf{Activation} & \textbf{Degree} & \textbf{Learnable?}  & \textbf{Train Loss} & \textbf{Val Top-1 (\%)} & \textbf{Val Top-5 (\%)}\\
\midrule
tropical & 6 & $\times$ & 3.560 & 76.31 & 93.09 \\
tropical & 6 & $\surd$ & 2.857 & 82.20 & 95.90 \\
\midrule
fourier & 6 & $\times$ & 3.181 & 79.51 & 94.60 \\
fourier & 6 & $\surd$ & 2.837 & 80.93 & 95.44 \\
\midrule
hermite & 3 & $\times$ & 3.411 & 78.48 & 94.20 \\
hermite & 3 & $\surd$ & 2.790 & 82.34 & 96.03 \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\begin{table*}[!htbp]
\caption{Ablation studies for the clamping in the Hermite activation on ConvNeXt-T model.}
\label{res:ablation_clamp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\textbf{Activation} & \textbf{Degree} & \textbf{Clamped?}  & \textbf{Train Loss} & \textbf{Val Top-1 (\%)} & \textbf{Val Top-5 (\%)}\\
\midrule
hermite & 3 & $\surd$ & 2.772 & 81.98 & 95.81 \\
hermite & 3 & $\times$ & 2.790 & 82.34 & 96.03 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\section{Extended Related Work}
\label{appendix:related}
The subject of learnable activation is a well-known one, but it has seen a resurgence thanks to the popularity enjoyed by the KAN article \cite{liu2024kan}. Examples of works in which the main theme is learning the activation function include \cite{houlsby2019parameter, goyal2019learning, tavakoli2021splash, moosavi2022adaptable, fang2022transformers, bodyanskiy2023learnable, pishchik2023trainable}.

Earlier works exploring polynomial activations in deep neural networks trained using the backpropagation algorithm include \cite{zhou2019polynomial} and \cite{chrysos2020p}, which empirically demonstrate that polynomially activated neural networks, even without non-linear activation functions, can perform well across multiple tasks. Building on this, \cite{chrysos2023regularization} sought to regularize such networks to compete with deep ReLU networks.

More recently, \cite{nebioglu2023higher} investigated the use of Chebyshev and Hermite orthogonal polynomials as activation functions, demonstrating that Chebyshev activations are computationally efficient but sensitive to problem types, while Hermite activations exhibit greater robustness and generalization. Additionally, \cite{xiao2024hope} introduced HOPE (High-order Polynomial Expansion), a novel method that represents neural networks as high-order Taylor polynomials, enabling improved interpretability, low computational complexity, and applications such as function discovery, fast inference, and feature selection.

%, using  Hermite polynomials \cite{beliczynski2007properties} 
Other recent works utilizing Chebyshev activation include \cite{deepthi2023development} and \cite{heidari2024single}, which employed single-layer shallow networks. \citet{seydi2024exploring} conducted a comparative study of exotic polynomial activations on the MNIST dataset, while \citet{cooley2024polynomial} applied polynomial-augmented neural networks for approximating solutions to partial differential equations.
 
On the rational activation front, notable works include \cite{trefethen1987pade}, which introduced stable-PadÃ© and Chebyshev-PadÃ© approximators, and \cite{molina2019pad}, which proposed the Safe-PadÃ© activation by ensuring the denominator of the rational activation remains nonzero. An orthogonal variant of the PadÃ© approximant was presented in \cite{biswas2021orthogonal}, while Chebyshev rational functions \cite{castellanos1993rational} and Fourier rational functions \cite{geer1995rational} were explored in subsequent studies. More recently, advancements in rational activation using general Jacobi functions were introduced in \cite{aghaei2024rkan, aghaei2024fkan}.

Polynomial piecewise functions (such as B-splines) and rational functions (such as the PadÃ© approximant) can exhibit finite support properties. On the other hand, these last lack the orthogonality property. Several works have aimed to formulate orthogonal splines \cite{mason1993orthogonal, alavi2023orthogonal} and orthogonal rational functions \cite{bultheel2001orthogonal}, or even a theory of spline wavelets \cite{chui1991cardinal} and rational wavelets \cite{zheng1999rational, choueiter2007implementation}.
%multiple rational Fourier \cite{khachar2022note},

Learning with a periodic function or a Fourier series has also been the subject of many anterior works such as \cite{sitzmann2020implicit}, and more recently \cite{mehrabian2024implicit}, and \cite{martinez2024enn} using a Discrete Cosine Transform (DCT).

In the context of tropical activations, prior work has been done to establish connections between tropical geometry and neural networks. For instance, \cite{zhang2018tropical} demonstrated that feedforward neural networks with ReLU activation can be interpreted as tropical rational maps, relating their decision boundaries to tropical hypersurfaces and showing how deeper networks leverage zonotopes to achieve exponentially greater expressiveness. Building on this geometric foundation, \cite{smyrnis2019tropical} introduced tropical polynomial division, an approach inspired by the max-plus semiring, and applied it to neural networks with ReLU activation.

\section{A brief digression on Kolmogorov Arnold Networks (KANs)}
\label{appendix:kan}
What is left of the recently famous Kolmogorov-Arnold networks (KAN) \cite{liu2024kan}?

Kolmogorov-Arnold networks have been presented as a potential alternative to Multilayer-Perceptrons (MLPs), promoting several merits such as greater accuracy, fewer learnable parameters, and better interpretability. While the first two advantages could only be demonstrated for simple cases in the \cite{liu2024kan} article, the third benefit is more straightforward, as these networks overcome the âblack-boxâ aspect of traditional non-linear activations MLPs by allowing the activation to be polynomial, piece-wise polynomial or rational, as in \cite{yang2024kolmogorov}. From there, having learned the weights of the network and those of the activation, it becomes clear what approximation these functions (polynomial, rational, or trigonometric ) have converged to.

Presenting themselves as heirs to the celebrated Kolmogorov-Arnold representation theorem (KART) \cite{kolmogorov1957representation, arnold1959predstavlenii},
the use made of this theorem in the recent article KAN \cite{liu2024kan} is to be understood figuratively. This is merely an inspiration, as the Kolmogorov-Arnold representation theorem, cited below, states that any continuous multivariate function $f \colon [0,1]^{n}\to \mathbb {R}$ can be represented as a composition of addition and some functions of one variable denoted by $\psi_{q,p}$ and $\Phi_q$:

\begin{theorem}
\label{thm:KART}
(\citet{arnold2009representation, arnold2009functions}) Let $f: \mathbb{I}^n:=[0,1]^n \rightarrow \mathbb{R}$ be an arbitrary multivariate continuous function. Then it can be represented as follows:
\begin{equation}
\label{eq:KART}
    f\left(x_1, \ldots, x_n\right)=\sum_{q=0}^{2 n} \Phi_q\left(\sum_{p=1}^n \psi_{qp}\left(x_p\right)\right)
\end{equation}
with continuous one-dimensional functions $\Phi_q \colon \mathbb {R} \to \mathbb {R}$ and $\psi_{q, p} \colon [0,1] \to \mathbb {R}$. $\Phi_q$ are called outer funcions and $\psi_{q, p}$ are called inner functions. The inner functions $\psi_{q, p}$ are independent of the function $f$.
\end{theorem} 
This is a far cry from KAN's formulation \cite{liu2024kan}, where the outer functions disappear, the inner functions are replaced by a weighted sum of a SiLU MLP \cite{elfwing2018sigmoid} and a B-spline, and the networks are a composition of multiple feed-forward layers to accommodate recent neural network architectures.

Since the KART proof is not constructible, and is essentially based on Baire's theorem \cite{kahane1975theoreme}, the first efforts to implement a constructive proof of the KART were made by Sprecher in \cite{sprecher1996numerical, sprecher1997numerical}. These latest works are based on a more economical variant of the KART in terms of the number of outer and inner functions due to both  \citet{sprecher1965structure} and \citet{lorentz1966approximation}. %who presented a variant of KART that reduces the number of outer functions $\Phi_q$ to a single continuous one $\Phi$, and on the number of inner functions due to \citet{sprecher1965structure} which further reduces the number of inner functions $\psi_{qp}$ to a single $\psi$ function.

%\textbf{Theorem.} (\citet{sprecher1965structure}). Let $n \geq 2$ and $m \geq 2 n$ be given integers. Then, for any arbitrary continuous function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, there exist $m+1$ continuous functions $\Phi_q: \mathbb{R} \rightarrow \mathbb{R}, q=0, \ldots m$, such that:
%\begin{equation}
%f(\mathbf{x})=\sum_{q=0}^m \Phi \left(\sum_{p=0}^n \lambda_{pq} \psi(x_p + aq) \right),
%\end{equation}
%with $\forall p,q \in \llbracket 0, n \rrbracket \times \llbracket 0, m \rrbracket \ \lambda_{pq} \in \mathbb{R}$ and $a \in \mathbb{R}$ .
 %\cite{liu2015kolmogorov}.
This was followed by the first article on the practical training of this type of network by \citet{koppen2002training} pointing out at the same time that the inner function $\psi$ constructed in this theorem was continuous but fractal! Which limited its use in gradient-based learning algorithms. \citet{braun2009constructive} gave rigorous proof of termination, continuity, and monotonicity for the construction of the inner and the outer functions given by \citet{sprecher1997numerical}.

%David Sprecher ${ }^{[10]}$ replaced the inner functions $\phi_{q, p}$ by one single inner function with an appropriate shift in its argument. He proved that there exist real values $\eta, \lambda_1, \ldots, \lambda_n$, a continuous function $\Phi: \mathbb{R} \rightarrow \mathbb{R}$, and a real increasing continuous function $\phi:[0,1] \rightarrow[0,1]$ with $\phi \in \operatorname{Lip}(\ln 2 / \ln (2 N+2))$, for $N \geq n \geq 2$, such that

%$$
%f(\mathbf{x})=\sum_{q=0}^{2 n} \Phi\left(\sum_{p=1}^n \lambda_p \phi\left(x_p+\eta q\right)+q\right)
%$$

%\textbf{Theorem.} (\citet{sprecher1965structure}). Let $n \geq 2$ and $m \geq 2 n$ be given integers. Then, for any arbitrary continuous function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, there exist $m+1$ continuous functions $\Phi_q: \mathbb{R} \rightarrow \mathbb{R}, q=0, \ldots m$, such that:
%\begin{equation}
%f(\mathbf{x})=\sum_{q=0}^m \Phi \left(\sum_{p=0}^n \lambda_{pq} \psi(x_p + aq) \right),
%\end{equation}
 %\cite{liu2015kolmogorov}.

%This was followed by the first article on the training of this type of network by \citet{koppen2002training}

%David Sprecher ${ }^{[10]}$ replaced the inner functions $\phi_{q, p}$ by one single inner function with an appropriate shift in its argument. He proved that there exist real values $\eta, \lambda_1, \ldots, \lambda_n$, a continuous function $\Phi: \mathbb{R} \rightarrow \mathbb{R}$, and a real increasing continuous function $\phi:[0,1] \rightarrow[0,1]$ with $\phi \in \operatorname{Lip}(\ln 2 / \ln (2 N+2))$, for $N \geq n \geq 2$, such that

%$$
%f(\mathbf{x})=\sum_{q=0}^{2 n} \Phi\left(\sum_{p=1}^n \lambda_p \phi\left(x_p+\eta q\right)+q\right)
%$$

As acknowledged by both \cite{liu2024kan} and \cite{yang2024kolmogorov}, the original ``KAN" layer defined in \cite{liu2024kan} could be seen as a sum of a SiLU MLP  and a weighted B-Spline combination. Let us define a linear function $\mathcal{L}_W \colon x \mapsto Wx$, with $W$ a learnable weight matrix. The ``KAN" layer \cite{liu2024kan}  is then defined as follows:
\begin{equation}
    \texttt{KAN}_\texttt{Liu}(x)=\mathcal{L}_{W_b}(\texttt{SiLU}(x)) + \mathcal{L}_{W_s} \left(\sum_i c_i B_i(x)\right)
\end{equation}
With  $W_b$ and $W_s$ two learnable weight matrices,  $(B_i)_{i \in \llbracket 0,d\rrbracket}$ a family of B-spline functions of order $d+1$, $(c_i)_{i \in \llbracket 0,d\rrbracket}$ the learnable spline weights and $\texttt{SiLU} \colon x \mapsto \frac{x}{1 + e^{-x}}$.

Indeed, if we follow the line of thought set out in KAN \cite{liu2024kan}, an MLP with learnable activation, or equivalently a learnable activation network (LAN) would be a sort of KART formulation, with the $\psi_{qp}$ inner functions being a linear combination of ReLU functions. However, this is not what the KART theorem suggests. Constructing a Kolmogorov-Arnold superposition requires a maximum of two layers formulated by inner and outer functions as in theorem~\ref{thm:KART} \citep{ismailov2024addressing}. 

It is worth noting that the concept of using splines to approximate inner functions in a Kolmogorov-Arnold network or more generally as a representation of an activation function isn't entirely new. The analogy between KANs and MLPs has been noticed since \cite{hecht1987kolmogorov} and \cite{kuurkova1992kolmogorov}. Earlier research, such as \cite{igelnik2003kolmogorov}, introduced Kolmogorov's Spline Network, which employed splines for flexible function approximation. In his PhD thesis, \citet{braun2009application} corrected the constructive proof of the KAT and gave practical examples using B-splines. Further developments in this area include \cite{bohra2020learning} and \cite{fakhoury2022exsplinet}, who focused on learning adaptive activation functions through splines, thus enhancing the network's expressiveness.

Additionally, the use of the Kolmogorov superposition theorem to tackle high-dimensional problems has been explored by \cite{laczkovich2021superposition} and \cite{lai2021kolmogorov}, who showed its potential in overcoming the curse of dimensionality. Similarly, \cite{montanelli2019deep} demonstrated how structured networks like Deep ReLU models can efficiently approximate bandlimited functions, thus expanding the practical applications of spline-based methodologies in neural networks.

With an equivalent number of parameters or FLOPs, \citet{yu2024kan} observed that KAN surpasses MLP solely in symbolic formula representation, while it falls short of MLP in other machine learning tasks, including computer vision, NLP, and audio processing. \citet{cang2024can} confirmed the same finding.% When we replace the matrix multiplication (channel mixing) in the MLPs of the ConvNeXt-T model by the addition operation, and the subsequent activations by our learnable activations presented in this paper, we observe the same result on image classification on ImageNet1k, as depicted by the following figure~\ref{fig:kan}:

%\begin{figure}[!htbp]
%    \centering
%    \includegraphics[width=0.8\linewidth]{figs/plot_kan.png}
%    \caption{Top1 accuracy of ConvNeXt-T (28M) model on %ImageNet-1k classification with respect to a KAN model.}
%    \label{fig:kan}
%\end{figure}

Nevertheless, KANs have had the merit of rekindling interest in learnable activations in neural networks, among them polynomial and trigonometric activations.

Since the interest in KANs began, numerous researchers have proposed a multitude of learnable functions for activations, spanning a diverse range of mathematical functions, including splines, classical orthogonal polynomials, rational functions, Fourier bases, and wavelets... Despite this, in some instances, the safety of these operations, the boundedness of their gradients, their initialization, and their computational properties in the context of gradient descent have been overlooked. Instead, the focus has been on demonstrating the potential of these techniques to achieve acceptable accuracy on small datasets, such as the MNIST dataset \cite{lecun1998mnist}.

A wealth of literature can be found on this subject. Yet, a common criticism of these papers is that they focus on a specific type of interpolation function and then attempt to adapt it through trial and error or speculation to fit a shallow neural network. In the latter case, small-scale datasets are employed (such as the MNIST dataset, for example), and hasty conclusions that may be erroneous are drawn from these experiments. These erroneous interpretations result from the fact that the majority of the functions in question could achieve a test classification accuracy exceeding 97\% on MNIST with networks of depth not exceeding three layers. This makes it impossible to discern which are the best performers, as they all manage to saturate the MNIST test set.
\section{Line plots}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/plot_gpt2.png}
   \caption{Comparison of the validation losses of the GPT2 model (124M) with GELU, Hermite, Fourier, and Tropical activations.}
    \label{fig:gpt}
\end{figure}
\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
