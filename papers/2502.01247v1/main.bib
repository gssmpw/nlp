@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  pages={115--133},
  year={1943},
  publisher={Springer}
}%

@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}%

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}%

@article{liu2024kan,
  title={Kan: Kolmogorov-arnold networks},
  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
  journal={arXiv preprint arXiv:2404.19756},
  year={2024}
}%

@article{yang2024kolmogorov,
  title={Kolmogorov-Arnold Transformer},
  author={Yang, Xingyi and Wang, Xinchao},
  journal={arXiv preprint arXiv:2409.10594},
  year={2024}
}%

@inproceedings{kolmogorov1957representation,
  title={On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition},
  author={Kolmogorov, Andrei Nikolaevich},
  booktitle={Doklady Akademii Nauk},
  volume={114},
  pages={953--956},
  year={1957},
  organization={Russian Academy of Sciences}
}%

@article{kahane1975theoreme,
  title={Sur le Théorème de Superposition de Kolmogorov},
  author={Kahane, Jean-Pierre},
  journal={Journal of Approximation Theory},
  year={1975}
}%

@article{arnold2009functions,
  title={On functions of three variables},
  author={Arnold, Vladimir I},
  journal={Collected Works: Representations of Functions, Celestial Mechanics and KAM Theory, 1957--1965},
  pages={5--8},
  year={2009},
  publisher={Springer}
}%

@article{arnold2009representation,
  title={On the representation of functions of several variables as a superposition of functions of a smaller number of variables},
  author={Arnold, Vladimir I},
  journal={Collected works: Representations of functions, celestial mechanics and KAM theory, 1957--1965},
  pages={25--46},
  year={2009},
  publisher={Springer}
}%
%O predstavlenii nepreryvnykh funktsii trekh peremennykh superpozitsiiami nepreryvnykh funktsii dvukh peremennykh [
@article{arnold1959predstavlenii,
  title={On the presentation of continuous functions of three variables by superpositions of continuous functions of two variables},
  author={Arnold, VI},
  journal={Sbornik: Mathematics},
  volume={48},
  number={90},
  pages={3--74},
  year={1959}
}%

@article{sprecher1965structure,
  title={On the structure of continuous functions of several variables},
  author={Sprecher, David A},
  journal={Transactions of the American Mathematical Society},
  volume={115},
  pages={340--355},
  year={1965}
}%

@article{sprecher1996numerical,
  title={A numerical implementation of Kolmogorov's superpositions},
  author={Sprecher, David A},
  journal={Neural networks},
  volume={9},
  number={5},
  pages={765--772},
  year={1996},
  publisher={Elsevier}
}%

@article{sprecher1997numerical,
  title={A numerical implementation of Kolmogorov's superpositions II},
  author={Sprecher, David A},
  journal={Neural networks},
  volume={10},
  number={3},
  pages={447--457},
  year={1997},
  publisher={Elsevier}
}%

@article{lorentz1966approximation,
  title={Approximation of functions, athena series},
  author={Lorentz, GG},
  journal={Selected Topics in Mathematics},
  year={1966}
}%

@article{doss1976representations,
  title={Representations of continuous functions of several variables},
  author={Doss, Raouf},
  journal={American Journal of Mathematics},
  volume={98},
  number={2},
  pages={375--383},
  year={1976},
  publisher={JSTOR}
}

@inproceedings{koppen2002training,
  title={On the training of a Kolmogorov Network},
  author={K{\"o}ppen, Mario},
  booktitle={Artificial Neural Networks—ICANN 2002: International Conference Madrid, Spain, August 28--30, 2002 Proceedings 12},
  pages={474--479},
  year={2002},
  organization={Springer}
}%

@inproceedings{hecht1987kolmogorov,
  title={Kolmogorov’s mapping neural network existence theorem},
  author={Hecht-Nielsen, Robert},
  booktitle={Proceedings of the international conference on Neural Networks},
  volume={3},
  pages={11--14},
  year={1987},
  organization={IEEE press New York, NY, USA}
}%

@article{kuurkova1992kolmogorov,
title = {Kolmogorov's theorem and multilayer neural networks},
journal = {Neural Networks},
volume = {5},
number = {3},
pages = {501-506},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(92)90012-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608092900128},
author = {Věra Kůrková},
keywords = {Feedforward neural networks, Multilayer perceptron type networks, Sigmoidal activation function, Approximations of continuous functions, Uniform approximation, Universal approximation capabilities, Estimates of number of hidden units, Modulus of continuity},
abstract = {Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal approximation capabilities of perceptron type networks with two hidden layers. From our proof, we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approximation.}
}%

@phdthesis{braun2009application,
  title={An application of Kolmogorov's superposition theorem to function reconstruction in higher dimensions},
  author={Braun, J{\"u}rgen},
  year={2009},
  school={Universit{\"a}ts-und Landesbibliothek Bonn}
}%

@phdthesis{liu2015kolmogorov,
  title={Kolmogorov superposition theorem and its applications},
  author={Liu, Xing},
  year={2015},
  school={Imperial College London}
}%

@article{braun2009constructive,
  title={On a constructive proof of Kolmogorov’s superposition theorem},
  author={Braun, J{\"u}rgen and Griebel, Michael},
  journal={Constructive approximation},
  volume={30},
  pages={653--675},
  year={2009},
  publisher={Springer}
}%

% Universal approx
@article{leshno1993multilayer,
  title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  journal={Neural networks},
  volume={6},
  number={6},
  pages={861--867},
  year={1993},
  publisher={Elsevier}
}%

@article{pinkus1999approximation,
  title={Approximation theory of the MLP model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}%

@article{hornik1990universal,
  title={Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={3},
  number={5},
  pages={551--560},
  year={1990},
  publisher={Elsevier}
}%

@inproceedings{stinchcombe1990approximating,
  title={Approximating and learning unknown mappings using multilayer feedforward networks with bounded weights},
  author={Stinchcombe, Maxwell and White, Halbert},
  booktitle={1990 IJCNN International Joint Conference on Neural Networks},
  pages={7--16},
  year={1990},
  organization={IEEE}
}%

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}%

@inproceedings{kidger2020universal,
  title={Universal approximation with deep narrow networks},
  author={Kidger, Patrick and Lyons, Terry},
  booktitle={Conference on learning theory},
  pages={2306--2327},
  year={2020},
  organization={PMLR}
}%

@article{laczkovich2021superposition,
  title={A superposition theorem of Kolmogorov type for bounded continuous functions},
  author={Laczkovich, Mikl{\'o}s},
  journal={Journal of Approximation Theory},
  volume={269},
  pages={105609},
  year={2021},
  publisher={Elsevier}
}%


@article{igelnik2003kolmogorov,
  title={Kolmogorov's spline network},
  author={Igelnik, Boris and Parikh, Neel},
  journal={IEEE transactions on neural networks},
  volume={14},
  number={4},
  pages={725--733},
  year={2003},
  publisher={IEEE}
}%

@article{montanelli2019deep,
  title={Deep ReLU networks overcome the curse of dimensionality for bandlimited functions},
  author={Montanelli, Hadrien and Yang, Haizhao and Du, Qiang},
  journal={arXiv preprint arXiv:1903.00735},
  year={2019}
}%

@article{bohra2020learning,
  title={Learning activation functions in deep (spline) neural networks},
  author={Bohra, Pakshal and Campos, Joaquim and Gupta, Harshit and Aziznejad, Shayan and Unser, Michael},
  journal={IEEE Open Journal of Signal Processing},
  volume={1},
  pages={295--309},
  year={2020},
  publisher={IEEE}
}%

@article{fakhoury2022exsplinet,
  title={ExSpliNet: An interpretable and expressive spline-based neural network},
  author={Fakhoury, Daniele and Fakhoury, Emanuele and Speleers, Hendrik},
  journal={Neural Networks},
  volume={152},
  pages={332--346},
  year={2022},
  publisher={Elsevier}
}%

@article{lai2021kolmogorov,
  title={The kolmogorov superposition theorem can break the curse of dimensionality when approximating high dimensional functions},
  author={Lai, Ming-Jun and Shen, Zhaiming},
  journal={arXiv preprint arXiv:2112.09963},
  year={2021}
}%

@article{ismailov2024addressing,
  title={Addressing Common Misinterpretations of KART and UAT in Neural Network Literature},
  author={Ismailov, Vugar},
  journal={arXiv preprint arXiv:2408.16389},
  year={2024}
}%


% Fourier and rational
@article{castellanos1993rational,
  title={Rational chebyshev approximations of analytic functions},
  author={Castellanos, D and Rosenthal, William E},
  journal={The Fibonacci Quarterly},
  volume={31},
  number={3},
  pages={205--215},
  year={1993}
}%

@article{geer1995rational,
  title={Rational trigonometric approximations using Fourier series partial sums},
  author={Geer, James F},
  journal={Journal of Scientific Computing},
  volume={10},
  pages={325--356},
  year={1995},
  publisher={Springer}
}%

@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}%

@article{khachar2022note,
  title={A note on multiple rational Fourier series},
  author={Khachar, HJ and Vyas, RG},
  journal={Periodica Mathematica Hungarica},
  pages={1--11},
  year={2022},
  publisher={Springer}
}%

@article{mehrabian2024implicit,
  title={Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks},
  author={Mehrabian, Ali and Adi, Parsa Mojarad and Heidari, Moein and Hacihaliloglu, Ilker},
  journal={arXiv preprint arXiv:2409.09323},
  year={2024}
}%

@article{aghaei2024rkan,
  title={rKAN: Rational Kolmogorov-Arnold Networks},
  author={Aghaei, Alireza Afzal},
  journal={arXiv preprint arXiv:2406.14495},
  year={2024}
}%

@article{aghaei2024fkan,
  title={fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions},
  author={Aghaei, Alireza Afzal},
  journal={arXiv preprint arXiv:2406.07456},
  year={2024}
}%

% hermite
@inproceedings{beliczynski2007properties,
  title={Properties of the Hermite activation functions in a neural approximation scheme},
  author={Beliczynski, Bartlomiej},
  booktitle={International Conference on Adaptive and Natural Computing Algorithms},
  pages={46--54},
  year={2007},
  organization={Springer}
}%

@inproceedings{lokhande2020generating,
  title={Generating accurate pseudo-labels in semi-supervised learning and avoiding overconfident predictions via hermite polynomial activations},
  author={Lokhande, Vishnu Suresh and Tasneeyapant, Songwong and Venkatesh, Abhay and Ravi, Sathya N and Singh, Vikas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11435--11443},
  year={2020}
}%

% soft sign
@inproceedings{turian2009quadratic,
  title={Quadratic features and deep architectures for chunking},
  author={Turian, Joseph and Bergstra, James and Bengio, Yoshua},
  booktitle={Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers},
  pages={245--248},
  year={2009}
}%

@article{biswas2021orthogonal,
  title={Orthogonal-Padé Activation Functions: Trainable Activation functions for smooth and faster convergence in deep networks},
  author={Biswas, Koushik and Banerjee, Shilpak and Pandey, Ashish Kumar},
  journal={arXiv preprint arXiv:2106.09693},
  year={2021}
}%

% Padé

@article{molina2019pad,
  title={Pad$\backslash$'e activation units: End-to-end learning of flexible activation functions in deep networks},
  author={Molina, Alejandro and Schramowski, Patrick and Kersting, Kristian},
  journal={arXiv preprint arXiv:1907.06732},
  year={2019}
}%

%% polys
@article{zhuo2024polynomial,
  title={Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models},
  author={Zhuo, Zhijian and Wang, Ya and Zeng, Yutao and Li, Xiaoqing and Zhou, Xun and Ma, Jinwen},
  journal={arXiv preprint arXiv:2411.03884},
  year={2024}
}%

@article{xiao2024hope,
  title={HOPE: High-Order Polynomial Expansion of Black-Box Neural Networks},
  author={Xiao, Tingxiong and Zhang, Weihang and Cheng, Yuxiao and Suo, Jinli},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}%

@inproceedings{chrysos2023regularization,
  title={Regularization of polynomial networks for image recognition},
  author={Chrysos, Grigorios G and Wang, Bohan and Deng, Jiankang and Cevher, Volkan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16123--16132},
  year={2023}
}%

@article{cooley2024polynomial,
  title={Polynomial-Augmented Neural Networks (PANNs) with Weak Orthogonality Constraints for Enhanced Function and PDE Approximation},
  author={Cooley, Madison and Zhe, Shandian and Kirby, Robert M and Shankar, Varun},
  journal={arXiv preprint arXiv:2406.02336},
  year={2024}
}%

@article{nebioglu2023higher,
  title={Higher order orthogonal polynomials as activation functions in artificial neural networks},
  author={Nebioglu, Burak and Iliev, Alexander I},
  journal={Serdica Journal of Computing},
  volume={17},
  number={1},
  pages={1--16},
  year={2023}
}%

@inbook{trefethen1987pade,
author = {Trefethen, L. N. and Gutknecht, M. H.},
title = {Padé, stable Padé, and Chebyshev-Padé approximation},
year = {1987},
isbn = {0198536127},
publisher = {Clarendon Press},
address = {USA},
booktitle = {Algorithms for Approximation},
pages = {227–264},
numpages = {38}
}%

@article{seydi2024exploring,
  title={Exploring the Potential of Polynomial Basis Functions in Kolmogorov-Arnold Networks: A Comparative Study of Different Groups of Polynomials},
  author={Seydi, Seyd Teymoor},
  journal={arXiv preprint arXiv:2406.02583},
  year={2024}
}%

@article{zhou2019polynomial,
  title={Polynomial activation neural networks: Modeling, stability analysis and coverage BP-training},
  author={Zhou, Jun and Qian, Huimin and Lu, Xinbiao and Duan, Zhaoxia and Huang, Haoqian and Shao, Zhen},
  journal={Neurocomputing},
  volume={359},
  pages={227--240},
  year={2019},
  publisher={Elsevier}
}%

@article{martinez2024enn,
  title={ENN: A Neural Network with DCT Adaptive Activation Functions},
  author={Martinez-Gost, Marc and P{\'e}rez-Neira, Ana and Lagunas, Miguel {\'A}ngel},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2024},
  publisher={IEEE}
}%

%Cheby
@article{deepthi2023development,
  title={Development of a novel activation function based on Chebyshev polynomials: an aid for classification and denoising of images},
  author={Deepthi, M and Vikram, GNVR and Venkatappareddy, P},
  journal={The Journal of Supercomputing},
  volume={79},
  number={18},
  pages={20515--20531},
  year={2023},
  publisher={Springer}
}%

% cheby 1 layer
@article{heidari2024single,
  title={Single-Layer Learnable Activation for Implicit Neural Representation ({SL}$^{2}$ {A}-{INR})},
  author={Heidari, Moein and Rezaeian, Reza and Azad, Reza and Merhof, Dorit and Soltanian-Zadeh, Hamid and Hacihaliloglu, Ilker},
  journal={arXiv preprint arXiv:2409.10836},
  year={2024}
}%

%learnable activation
@article{goyal2019learning,
  title={Learning activation functions: a new paradigm of understanding neural networks. arXiv 2019},
  author={Goyal, Mohit and Goyal, Rajan and Lall, Brejesh},
  journal={arXiv preprint arXiv:1906.09529},
  year={2019}
}%

@article{tavakoli2021splash,
  title={Splash: Learnable activation functions for improving accuracy and adversarial robustness},
  author={Tavakoli, Mohammadamin and Agostinelli, Forest and Baldi, Pierre},
  journal={Neural Networks},
  volume={140},
  pages={1--12},
  year={2021},
  publisher={Elsevier}
}%

@article{bodyanskiy2023learnable,
  title={Learnable Extended Activation Function for Deep Neural Networks},
  author={Bodyanskiy, YEVGENIY and Kostiuk, SERHII},
  journal={International Journal of Computing (Oct. 2023)},
  pages={311--318},
  year={2023}
}%

@article{fang2022transformers,
  title={Transformers with learnable activation functions},
  author={Fang, Haishuo and Lee, Ji-Ung and Moosavi, Nafise Sadat and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2208.14111},
  year={2022}
}%

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}%

@article{moosavi2022adaptable,
  title={Adaptable adapters},
  author={Moosavi, Nafise Sadat and Delfosse, Quentin and Kersting, Kristian and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2205.01549},
  year={2022}
}%

@article{pishchik2023trainable,
  title={Trainable activations for image classification},
  author={Pishchik, Evgenii},
  year={2023},
  publisher={Preprints},
  journal={Preprints}
}%


% Classical 
@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}%

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}%

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	url={http://Skylion007.github.io/OpenWebTextCorpus}, 
	year={2019}
}%

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI Blog},
  year={2019}
}%

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}%

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}%

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}%

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}%

@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}%

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@misc{lecun1998mnist,
  author={LeCun, Yann and Cortes, Corinna and Burges, Christopher},
  title={The MNIST database of handwritten digits},
  year={1998}
}%

@inproceedings{wu2022tinyvit,
  title={Tinyvit: Fast pretraining distillation for small vision transformers},
  author={Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu},
  booktitle={European conference on computer vision},
  pages={68--85},
  year={2022},
  organization={Springer}
}

% fourier series with Hermite interpolation
@incollection{priyanka2016introduction,
title = {Chapter 4 - Time and Frequency Analysis},
editor = {Priyanka A. Abhang and Bharti W. Gawali and Suresh C. Mehrotra},
booktitle = {Introduction to EEG- and Speech-Based Emotion Recognition},
publisher = {Academic Press},
pages = {81-96},
year = {2016},
isbn = {978-0-12-804490-2},
doi = {https://doi.org/10.1016/B978-0-12-804490-2.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012804490200004X},
author = {Priyanka A. Abhang and Bharti W. Gawali and Suresh C. Mehrotra},
keywords = {EEG signals, Fourier transformation, Gabor transformation, Sliding window, Speech signals, Uncertainty principle, Wavelet transformation},
abstract = {Various transformations have been widely used to extract frequency-dependent information from time-varying signals. The Fourier transformation is a fundamental technique for transforming a time domain to a frequency domain, but is applicable only to stationary signals. Sliding-window-based transformations have been used, where the requirement for time and frequency resolutions is not critical. Due to the uncertainty principle, time and frequency resolutions have to be optimized. In such cases, wavelet transformations are very popular to get salient features in time-dependent signals. Multiresolution analysis and selection of mother functions are also discussed. Some relevant examples from electroencephalograms and speech signals are used to illustrate the significance of these transformations in time-frequency analysis.}
}

@incollection{mallat2009wavelet,
title = {CHAPTER 4 - Time Meets Frequency},
editor = {Mallat Stéphane},
booktitle = {A Wavelet Tour of Signal Processing (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {89-153},
year = {2009},
isbn = {978-0-12-374370-1},
doi = {https://doi.org/10.1016/B978-0-12-374370-1.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123743701000082},
author = {Mallat, Stéphane}
}%
@article{berrut2007fourier,
  title={Fourier and barycentric formulae for equidistant Hermite trigonometric interpolation},
  author={Berrut, Jean-Paul and Welscher, Annick},
  journal={Applied and Computational Harmonic Analysis},
  volume={23},
  number={3},
  pages={307--320},
  year={2007},
  publisher={Elsevier}
}%

% Deep Learning Alternatives of the Kolmogorov Superposition Theorem
@article{guilhoto2024deep,
  title={Deep Learning Alternatives of the Kolmogorov Superposition Theorem},
  author={Guilhoto, Leonardo Ferreira and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2410.01990},
  year={2024}
}

@article{basina2024kat,
  title={KAT to KANs: A Review of Kolmogorov-Arnold Networks and the Neural Leap Forward},
  author={Basina, Divesh and Vishal, Joseph Raj and Choudhary, Aarya and Chakravarthi, Bharatesh},
  journal={arXiv preprint arXiv:2411.10622},
  year={2024}
}

@article{cang2024can,
  title={Can KAN Work? Exploring the Potential of Kolmogorov-Arnold Networks in Computer Vision},
  author={Cang, Yueyang and Shi, Li and others},
  journal={arXiv preprint arXiv:2411.06727},
  year={2024}
}%

% kuramoto for complex
@article{miyato2024artificial,
  title={Artificial Kuramoto Oscillatory Neurons},
  author={Miyato, Takeru and L{\"o}we, Sindy and Geiger, Andreas and Welling, Max},
  journal={arXiv preprint arXiv:2410.13821},
  year={2024}
}

@article{gabor1946theory,
  title={Theory of communication. Part 1: The analysis of information},
  author={Gabor, Dennis},
  journal={Journal of the Institution of Electrical Engineers-part III: radio and communication engineering},
  volume={93},
  number={26},
  pages={429--441},
  year={1946},
  publisher={IET}
}%

@article{grossmann1984decomposition,
  title={Decomposition of Hardy functions into square integrable wavelets of constant shape},
  author={Grossmann, Alexander and Morlet, Jean},
  journal={SIAM journal on mathematical analysis},
  volume={15},
  number={4},
  pages={723--736},
  year={1984},
  publisher={SIAM}
}%

@article{mason1993orthogonal,
  title={Orthogonal splines based on B-splines—with applications to least squares, smoothing and regularisation problems},
  author={Mason, JC and Rodriguez, Giuseppe and Seatzu, Sebastiano},
  journal={Numerical Algorithms},
  volume={5},
  pages={25--40},
  year={1993},
  publisher={Springer}
}%

@article{alavi2023orthogonal,
  title={Orthogonal cubic splines for the numerical solution of nonlinear parabolic partial differential equations},
  author={Alavi, Javad and Aminikhah, Hossein},
  journal={MethodsX},
  volume={10},
  pages={102190},
  year={2023},
  publisher={Elsevier}
}%

@article{bultheel2001orthogonal,
  title={Orthogonal rational functions and continued fractions},
  author={Bultheel, Adhemar and Gonzalez-Vera, Pablo and Hendriksen, Erik and Nj{\aa}stad, Olav},
  journal={Special Functions 2000: Current Perspective and Future Directions},
  pages={87--109},
  year={2001},
  publisher={Springer}
}%

@article{chui1991cardinal,
  title={A cardinal spline approach to wavelets},
  author={Chui, Charles K and Wang, Jian-zhong},
  journal={Proceedings of the American Mathematical Society},
  volume={113},
  number={3},
  pages={785--793},
  year={1991}
}%

@article{zheng1999rational,
  title={Rational filter wavelets},
  author={Zheng, Kuang and Minggen, Cui},
  journal={Journal of mathematical analysis and applications},
  volume={239},
  number={2},
  pages={227--244},
  year={1999},
  publisher={Elsevier}
}%

@article{choueiter2007implementation,
  title={An implementation of rational wavelets and filter design for phonetic classification},
  author={Choueiter, Ghinwa F and Glass, James R},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={15},
  number={3},
  pages={939--948},
  year={2007},
  publisher={IEEE}
}%

@inproceedings{bernardino2005real,
  title={A real-time gabor primal sketch for visual attention},
  author={Bernardino, Alexandre and Santos-Victor, Jos{\'e}},
  booktitle={Iberian Conference on Pattern Recognition and Image Analysis},
  pages={335--342},
  year={2005},
  organization={Springer}
}%

@book{cohen2014analyzing,
  title={Analyzing neural time series data: theory and practice},
  author={Cohen, MX},
  year={2014},
  publisher={MIT press}
}%

@article{daugman1985uncertainty,
  title={Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters},
  author={Daugman, John G},
  journal={JOSA A},
  volume={2},
  number={7},
  pages={1160--1169},
  year={1985},
  publisher={Optica Publishing Group}
}%

@article{bassey2021survey,
  title={A survey of complex-valued neural networks},
  author={Bassey, Joshua and Qian, Lijun and Li, Xianfang},
  journal={arXiv preprint arXiv:2101.12249},
  year={2021}
}%

@article{reichert2013neuronal,
  title={Neuronal synchrony in complex-valued deep networks},
  author={Reichert, David P and Serre, Thomas},
  journal={arXiv preprint arXiv:1312.6115},
  year={2013}
}%

@article{sheng2023global,
  title={Global synchronization of complex-valued neural networks with unbounded time-varying delays},
  author={Sheng, Yin and Gong, Haoyu and Zeng, Zhigang},
  journal={Neural Networks},
  volume={162},
  pages={309--317},
  year={2023},
  publisher={Elsevier}
}%

@article{alamia2019alpha,
  title={Alpha oscillations and traveling waves: Signatures of predictive coding?},
  author={Alamia, Andrea and VanRullen, Rufin},
  journal={PLoS Biology},
  volume={17},
  number={10},
  pages={e3000487},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}%

@article{alamia2024traveling,
  title={A traveling waves perspective on temporal binding},
  author={Alamia, Andrea and VanRullen, Rufin},
  journal={Journal of Cognitive Neuroscience},
  volume={36},
  number={4},
  pages={721--729},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}%

@article{brackx2008hermitian,
  title={Hermitian Clifford-Hermite wavelets: an alternative approach},
  author={Brackx, Fred and De Schepper, Hennie and De Schepper, Nele and Sommen, Franciscus},
  journal={Bulletin of the Belgian mathematical Society-Simon Stevin},
  volume={15},
  number={1},
  pages={87--107},
  year={2008},
  publisher={The Belgian Mathematical Society}
}%

@article{pandey2020continuous,
  title={Continuous and discrete wavelet transforms associated with hermite transform},
  author={Pandey, CP and Phukan, Pranami},
  journal={International Journal of Analysis and Applications},
  volume={18},
  number={4},
  pages={531--549},
  year={2020}
}%



% init random matrix

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{mezzadri2006generate,
  title={How to generate random matrices from the classical compact groups},
  author={Mezzadri, Francesco},
  journal={arXiv preprint math-ph/0609050},
  year={2006}
}

@article{livan2018introduction,
  title={Introduction to random matrices theory and practice},
  author={Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
  journal={Monograph Award},
  volume={63},
  pages={54--57},
  year={2018}
}

@inproceedings{pennington2017geometry,
  title={Geometry of neural network loss surfaces via random matrix theory},
  author={Pennington, Jeffrey and Bahri, Yasaman},
  booktitle={International conference on machine learning},
  pages={2798--2806},
  year={2017},
  organization={PMLR}
}

@article{daneshmand2021batch,
  title={Batch normalization orthogonalizes representations in deep random networks},
  author={Daneshmand, Hadi and Joudaki, Amir and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4896--4906},
  year={2021}
}

@article{pennington2017nonlinear,
  title={Nonlinear random matrix theory for deep learning},
  author={Pennington, Jeffrey and Worah, Pratik},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


% https://github.com/Tramac/awesome-semantic-segmentation-pytorch

% https://community.wandb.ai/t/how-can-i-watch-layer-activations/5338/8

% https://github.com/lucidrains/electra-pytorch

% https://en.wikipedia.org/wiki/Geometric_programming



% https://en.wikipedia.org/wiki/Posynomial

@article{kileel2019expressive,
  title={On the expressive power of deep polynomial neural networks},
  author={Kileel, Joe and Trager, Matthew and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}%

@article{kubjas2024geometry,
  title={Geometry of Polynomial Neural Networks},
  author={Kubjas, Kaie and Li, Jiayi and Wiesmann, Maximilian},
  journal={arXiv preprint arXiv:2402.00949},
  year={2024}
}%

@inproceedings{chrysos2020p,
  title={P-nets: Deep polynomial neural networks},
  author={Chrysos, Grigorios G and Moschoglou, Stylianos and Bouritsas, Giorgos and Panagakis, Yannis and Deng, Jiankang and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7325--7335},
  year={2020}
}%

@article{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen},
  journal={Cambridge UP},
  year={2004}
}

@article{boyd2007tutorial,
  title={A tutorial on geometric programming},
  author={Boyd, Stephen and Kim, Seung-Jean and Vandenberghe, Lieven and Hassibi, Arash},
  journal={Optimization and engineering},
  volume={8},
  pages={67--127},
  year={2007},
  publisher={Springer}
}

@article{pei2005mapping,
  title={Mapping polynomial fitting into feedforward neural networks for modeling nonlinear dynamic systems and beyond},
  author={Pei, Jin-Song and Wright, Joseph P and Smyth, Andrew W},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={194},
  number={42-44},
  pages={4481--4505},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{zhang2018tropical,
  title={Tropical geometry of deep neural networks},
  author={Zhang, Liwen and Naitzat, Gregory and Lim, Lek-Heng},
  booktitle={International Conference on Machine Learning},
  pages={5824--5832},
  year={2018},
  organization={PMLR}
}%

@article{speyer2009tropical,
  title={Tropical mathematics},
  author={Speyer, David and Sturmfels, Bernd},
  journal={Mathematics Magazine},
  volume={82},
  number={3},
  pages={163--173},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{smyrnis2019tropical,
  title={Tropical polynomial division and neural networks},
  author={Smyrnis, Georgios and Maragos, Petros},
  journal={arXiv preprint arXiv:1911.12922},
  year={2019}
}%

@article{zhao2023taylornet,
  title={TaylorNet: A Taylor-Driven Generic Neural Architecture},
  author={Zhao, Hongjue and Chen, Yizhuo and Sun, Dachun and Hu, Yingdong and Liang, Kaizhao and Mao, Yanbing and Sha, Lui and Shao, Huajie},
  year={2023}
}



% https://en.wikipedia.org/wiki/Polynomial_mapping
% https://en.wikipedia.org/wiki/Posynomial
% https://en.wikipedia.org/wiki/Signomial


% https://arxiv.org/pdf/2411.03884
https://openreview.net/forum?id=CbpWPbYHuv



@misc{Karpathy2022,
  author = {Andrej Karpathy},
  title = {\text{NanoGPT}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  url={https://github.com/karpathy/nanoGPT},
}%

@article{yu2024kan,
  title={Kan or mlp: A fairer comparison},
  author={Yu, Runpeng and Yu, Weihao and Wang, Xinchao},
  journal={arXiv preprint arXiv:2407.16674},
  year={2024}
}%



