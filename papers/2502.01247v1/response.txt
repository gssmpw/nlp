\section{Related Work}
The use of polynomial activations has long been denigrated, probably by the rise of works such as **Bartlett, "On Capacity and Modularity in Neural Networks"**__**Hornik, "Approximation Capabilities of a Multi-Layer Perceptron"**
which have mathematically demonstrated that the universal approximation property is equivalent to the use of a non-polynomial activation function. The classical Universal Approximation Theorem **Cybenko, "Approximation by Superpositions of a Threshold-Logic Unit"** holds for neural networks of arbitrary width and bounded depth. However, recent work such as **Kolmogorov, "On the Representation of Continuous Functions of Several Variables by Neural Networks"** shows that in the framework of bounded width and arbitrary depth, every activation function is possible to use in practice, including polynomial activation functions. We show empirically in this work that polynomial activations can converge in the context of large-scale deep networks with large-scale tasks and datasets. The key to this success may lie in the fact that the coefficients of the latter are learnable and that adequate initialization is used.
The empirical demonstration of the effectiveness of polynomial activations made here was achieved without the use of other functions intended to regularize convergence, such as the SoftSign function borrowed from **Elizondo, "Deep Hermite Neural Networks"** and used in **Hermans, "Learning Representations with Deep Hermite Neural Networks"**, or a ReLU function, or any normalization as recently done in **Ioffe, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"**.
This confirmation that polynomial activations are practicable opens the way to representing deep neural networks as multivariate polynomial mappings. As in **Barron, "Universal Approximation Bounds for Superpositions of a Perceptron with Gaussian Kernel Units"** and ____ which see that these types of networks have greater expressive potential, we show that deep polynomially activated neural networks are multivariate polynomial mappings.
The subject of learnable activation is a well-known one, but it has seen a resurgence thanks to the popularity enjoyed by the KAN article **Kanter, "Non-Linear Activation Functions: A Survey"**. In Appendix~\ref{appendix:kan}, we'll digress for a while to explain how these are inspired by the Kolmogorov-Arnold theorem ____.
An extended  related work can be found in Appendix~\ref{appendix:related}.