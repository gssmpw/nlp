\section{Related Work}
The use of polynomial activations has long been denigrated, probably by the rise of works such as ____ and ____ which have mathematically demonstrated that the universal approximation property is equivalent to the use of a non-polynomial activation function. The classical Universal Approximation Theorem ____ holds for neural networks of arbitrary width and bounded depth. However, recent work such as ____ shows that in the framework of bounded width and arbitrary depth, every activation function is possible to use in practice, including polynomial activation functions. We show empirically in this work that polynomial activations can converge in the context of large-scale deep networks with large-scale tasks and datasets. The key to this success may lie in the fact that the coefficients of the latter are learnable and that adequate initialization is used.
The empirical demonstration of the effectiveness of polynomial activations made here was achieved without the use of other functions intended to regularize convergence, such as the SoftSign function borrowed from ____ and used in ____ for Hermite activations, or a ReLU function, or any normalization as recently done in ____.
This confirmation that polynomial activations are practicable opens the way to representing deep neural networks as multivariate polynomial mappings. As in ____ and ____, which see that these types of networks have greater expressive potential, we show that deep polynomially activated neural networks are multivariate polynomial mappings.
The subject of learnable activation is a well-known one, but it has seen a resurgence thanks to the popularity enjoyed by the KAN article ____. In Appendix~\ref{appendix:kan}, we'll digress for a while to explain how these are inspired by the Kolmogorov-Arnold theorem ____. An extended  related work can be found in Appendix~\ref{appendix:related}.