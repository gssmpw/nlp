\section{Related Work}
\label{section:related}
\textbf{Control Barrier Function.} 
Control barrier functions (CBF) **Lee, "Barrier Certificates for Safe Exploration of Learning-Based Systems"** aim to ensure control safety in dynamical systems by imposing value-landscapes to render the safe set forward invariant. 
The key point is to enforce the derivative of the CBF to satisfy Lyapunov conditions**Huang, "Learning Control Barrier Functions from Demonstrations"**.
Traditional CBFs are manually designed based on domain-specific knowledge of the system, making them unsuitable for systems with complex dynamics or high uncertainty**Leung, "Data-Driven Construction of Control Barrier Functions for Complex Systems"**.
% In **Xiao et al., "Application of Pre-defined Control Barrier Functions in Adaptive Cruise Control"**, the authors apply the pre-defined CBFs on adaptive cruise control by solving Quadratic Programs to present safety considerations.
% % They derive the CBFs beforehand based on system dynamics as well as constraint specifications.
% **Zhu, "Simplified Construction of Control Barrier Functions for Complex Systems"** proposes new classes of CBFs to simplify the process of constructing CBF candidates,
% and perform evaluations in simple simulation tasks.
% Nevertheless, it remains challenging to construct valid CBFs for systems with complex or uncertain dynamics.

% \textbf{Learning-based CBFs.}
Learning-based methods have been introduced to construct data-driven CBF candidates**Li et al., "Data-Driven Construction of Control Barrier Functions using Neural Networks"**
from data.
% using linear functions**Wang, "Linear Control Barrier Functions for Safe Exploration"**, support vector machines**Liu, "Support Vector Machine-based Control Barrier Functions for Complex Systems"**, and neural networks**Chen, "Neural Network-based Control Barrier Functions for High-Dimensional Systems"**.
% They provide an alternative to the manually-designed CBFs especially for the systems with complex or uncertain dynamics.
% In recent, learning-based CBFs have been extended to handle challenging safety-critical tasks such as robot manipulations**Tan et al., "Robot Manipulation using Learning-based Control Barrier Functions"**, obstacle avoidance**Zhang et al., "Obstacle Avoidance using Learning-based Control Barrier Functions"**, [find one more], and multi-agent control**Wang et al., "Multi-Agent Control using Learning-based Control Barrier Functions"** etc.
Online algorithms learn CBFs by interacting with, or sampling from, the controlled system.
In **Chen et al., "Adaptive Sampling for Safe Exploration of Unknown Control-Affine Systems"**, the authors learn barrier certificates to derive the safe region of an unknown control-affine system. 
They propose an adaptive sampling algorithm to iteratively refine the CBF candidate on the states that have high uncertainty.
% The algorithm stops when the safe region can be no longer expanded.
% **Li, "Uncertainty Estimation for Unknown Dynamical Systems using Control Barrier Functions"** proposes to estimate model uncertainty for an unknown dynamical system using CBFs, in order to estimate both uncertainties due to parametric errors and unmodeled dynamics.  
% They use an episodic learning approach which alternates between collecting data using a QP-based controller synthesized by the CBF, and improving the CBF estimates.
**Zhu et al., "Multi-Agent Control using Learning-based Control Barrier Functions"** studies the multi-agent control problem. They jointly learn the barrier certificates alongside the multi-agent control policy, while regulating the policy based on CBF.
**Huang et al., "Learning Control Lyapunov Barrier Functions for Safe Exploration"** develops a model-based approach to learn control Lyapunov barrier functions based on stability and safety specifications.
The training state are sampled uniformly from the state space.
% This idea of certifying policy safety with learned CBFs is frequently adopted by many safe reinforcement learning methods**Xu et al., "Safe Reinforcement Learning using Learning-based Control Barrier Functions"**.
Offline algorithms learn CBFs without new data during the learning.
**Wang, "Incremental Learning of Linear Parametric Control Barrier Functions from Human Demonstrations"** proposes an incremental learning of a set of linear parametric CBFs from human demonstrations.
% in order to capture the state constraint for the motion trajectory.
In **Chen et al., "Synthesis of Local Valid Control Barrier Functions for Control-Affine Systems with Nonlinear Dynamics"**, the authors present an approach to synthesize local valid CBFs for control-affine systems with known but nonlinear dynamics.
The expert demonstrations contain only safe trajectories collected with a fixed nominal controller.
% **Li et al., "Simultaneous Learning of Policy and Barrier Models using Modified Stochastic Gradient Descent"** adopts the modified stochastic gradient descent to simultaneously learn the policy and barrier models from offline demonstrations.
% In this paper, we present an offline algorithm for learning neural CBF models.
% Our algorithm differs from the prior methods in two aspects.
% First,
% we allow the training data to contain unlabeled demonstrations which consist of states with uncertain safety. 
% We achieve this via novel annotation procedures based on out-of-distribution analysis**Zhang et al., "Out-of-Distribution Analysis for Safe Exploration"**. 
% Second, we alleviate the assumption that the training data must be generated with a fixed performative controller. 
% We achieve this by improving the optimization of CBF's Lie-derivative condition to concern only the maximally-safe controls captured by the proposed actor model.

\textbf{Out-of-distribution Analysis.}
Out-of-distribution (OOD) analysis is an emerging topic of machine learning that examines the distribution shifts where test data diverges from the training data distribution **Srivastava et al., "Understanding Out-of-Distribution Behavior in Machine Learning"**.
% aiming to improve the reliability and safety of machine learning systems **Saha, "Reliability and Safety of Machine Learning Systems"**.
% OOD methods can be broadly classified into two categories.
Unsupervised representation learning methods focus on learning domain-agnostic features from unlabeled data**Chen et al., "Domain-Agnostic Feature Learning for Out-of-Distribution Detection"**.
% capturing key attributes that enable OOD detection against OOD data.
However, these methods can introduce bias, if the OOD domain distributions overlap with the unlabeled data distribution**Sinha et al., "Bias in Unsupervised Representation Learning Methods"**.
% A sub-domain of unsupervised methods is the self-supervised approaches that typically involve generating synthesized OOD distribution in order to 
Supervised learning methods incorporate implicit domain labels from both in-distribution and OOD data**Patel et al., "Domain Labeling for Supervised Out-of-Distribution Detection"**.
While these methods are often more accurate due to the additional information, they may not generalize well to OOD examples that differ significantly from those seen during training.
% identifying and rejecting OOD samples.
% This unsupervised OOD algorithm explores the relationship between classification with rejection and cost-sensitive classification, with the goal of detecting OOD samples that deviate from the training data.