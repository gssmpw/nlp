\section{Related Work}
\label{section:related}
\textbf{Control Barrier Function.} 
Control barrier functions (CBF) \cite{ames2019control} aim to ensure control safety in dynamical systems by imposing value-landscapes to render the safe set forward invariant. 
The key point is to enforce the derivative of the CBF to satisfy Lyapunov conditions~\cite{lyapunov1992general}.
Traditional CBFs are manually designed based on domain-specific knowledge of the system, making them unsuitable for systems with complex dynamics or high uncertainty~\cite{ames2014control, ames2016control}.
% In \cite{ames2014control}, the authors apply the pre-defined CBFs on adaptive cruise control by solving Quadratic Programs to present safety considerations.
% % They derive the CBFs beforehand based on system dynamics as well as constraint specifications.
% \cite{ames2016control} proposes new classes of CBFs to simplify the process of constructing CBF candidates,
% and perform evaluations in simple simulation tasks.
% Nevertheless, it remains challenging to construct valid CBFs for systems with complex or uncertain dynamics.

% \textbf{Learning-based CBFs.}
Learning-based methods have been introduced to construct data-driven CBF candidates~\cite{{wang2018safe, taylor2020learning,qin2021learning,dawson2022safe,robey2020learning,yu2023sequential,zhang2023neural,yu2023learning}}
from data.
% using linear functions~\cite{saveriano2019learning}, support vector machines~\cite{srinivasan2020synthesis}, and neural networks~\cite{wang2018safe, robey2020learning, taylor2020learning,qin2021learning,dawson2022safe,yu2023sequential,zhang2023neural,yu2023learning}.
% They provide an alternative to the manually-designed CBFs especially for the systems with complex or uncertain dynamics.
% In recent, learning-based CBFs have been extended to handle challenging safety-critical tasks such as robot manipulations~\cite{yu2023learning}, obstacle avoidance~\cite{yu2023sequential}, [find one more], and multi-agent control~\cite{qin2021learning, zhang2023neural,yu2023learning} etc.
Online algorithms learn CBFs by interacting with, or sampling from, the controlled system.
In \cite{wang2018safe}, the authors learn barrier certificates to derive the safe region of an unknown control-affine system. 
They propose an adaptive sampling algorithm to iteratively refine the CBF candidate on the states that have high uncertainty.
% The algorithm stops when the safe region can be no longer expanded.
% \cite{taylor2020learning} proposes to estimate model uncertainty for an unknown dynamical system using CBFs, in order to estimate both uncertainties due to parametric errors and unmodeled dynamics.  
% They use an episodic learning approach which alternates between collecting data using a QP-based controller synthesized by the CBF, and improving the CBF estimates.
\cite{qin2021learning} studies the multi-agent control problem. They jointly learn the barrier certificates alongside the multi-agent control policy, while regulating the policy based on CBF.
\cite{dawson2022safe} develops a model-based approach to learn control Lyapunov barrier functions based on stability and safety specifications.
The training state are sampled uniformly from the state space.
% This idea of certifying policy safety with learned CBFs is frequently adopted by many safe reinforcement learning methods~\cite{cheng2019end, marvi2021safe}.
Offline algorithms learn CBFs without new data during the learning.
\cite{saveriano2019learning} proposes an incremental learning of a set of linear parametric CBFs from human demonstrations.
% in order to capture the state constraint for the motion trajectory.
In \cite{robey2020learning}, the authors present an approach to synthesize local valid CBFs for control-affine systems with known but nonlinear dynamics.
The expert demonstrations contain only safe trajectories collected with a fixed nominal controller.
% \cite{zhao2021learning} adopts the modified stochastic gradient descent to simultaneously learn the policy and barrier models from offline demonstrations.
% In this paper, we present an offline algorithm for learning neural CBF models.
% Our algorithm differs from the prior methods in two aspects.
% First,
% we allow the training data to contain unlabeled demonstrations which consist of states with uncertain safety. 
% We achieve this via novel annotation procedures based on out-of-distribution analysis~\cite{}. 
% Second, we alleviate the assumption that the training data must be generated with a fixed performative controller. 
% We achieve this by improving the optimization of CBF's Lie-derivative condition to concern only the maximally-safe controls captured by the proposed actor model.

\textbf{Out-of-distribution Analysis.}
Out-of-distribution (OOD) analysis is an emerging topic of machine learning that examines the distribution shifts where test data diverges from the training data distribution \cite{yang2024generalized}.
% aiming to improve the reliability and safety of machine learning systems \cite{yang2024generalized}.
% OOD methods can be broadly classified into two categories.
Unsupervised representation learning methods focus on learning domain-agnostic features from unlabeled data~\cite{mahajan2021domain,zhang2022towards,harary2022unsupervised,charoenphakdee2021classification}.
% capturing key attributes that enable OOD detection against OOD data.
However, these methods can introduce bias, if the OOD domain distributions overlap with the unlabeled data distribution~\cite{yu2024rethinking}.
% A sub-domain of unsupervised methods is the self-supervised approaches that typically involve generating synthesized OOD distribution in order to 
Supervised learning methods incorporate implicit domain labels from both in-distribution and OOD data~\cite{arjovsky2019invariant, wu2022discovering}.
While these methods are often more accurate due to the additional information, they may not generalize well to OOD examples that differ significantly from those seen during training.
% identifying and rejecting OOD samples.
% This unsupervised OOD algorithm explores the relationship between classification with rejection and cost-sensitive classification, with the goal of detecting OOD samples that deviate from the training data.