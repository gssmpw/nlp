\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}



% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}




%%%%%%%%%%%Evi Added%%%%%%%%%%
% \usepackage{amsmath,bm}
\usepackage{xfrac}
% \usepackage{amsthm}
\usepackage{xcolor}

\usepackage{bbm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{cleveref}
\usepackage{natbib}



\title{Computing Voting Rules with Improvement Feedback}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{ {Evi Micha} \\
	University of Southern California\\
	\texttt{evi.micha@usc.edu} \\
	%% examples of more authors
	\And {Vasilis Varsamis} \\
	University of Southern California\\
	\texttt{varsamis@usc.edu} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
%\renewcommand{\shorttitle}{Computing Voting Rules With Improvement Feedback}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Computing Voting Rules with Improvement Feedback},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Evi Micha, Vasilis Varsamis},
pdfkeywords={First keyword, Second keyword, More},
}

\DeclareMathOperator*{\argmax}{arg\,\max}
\DeclareMathOperator*{\argmin}{arg\,\min}
% \newtheorem{lemma}{Lemma}
% \newtheorem{theorem}{Theorem}
% \newtheorem{definition}{Definition}
% \newtheorem{example}{Example}
% \newtheorem{proposition}{Proposition}
% \newtheorem{claim}{Claim}
% \newtheorem{corollary}{Corollary}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\ceil}[1]{ \left\lceil #1 \right\rceil }
\newcommand{\floor}[1]{ \left\lfloor #1 \right\rfloor }
\newcommand{\ind}[1]{ \mathbb{1}\{ #1 \} }

\newcommand{\dist}{D}
\newcommand{\swapdist}{D^{a\leftrightarrow b}}
\newcommand{\hatdist}{\hat{D}}
\newcommand{\hatswapdist}{\hat{D}^{a\leftrightarrow b}}
\newcommand{\Sab}{S_{-ab}}

\newcommand{\sunif}{P^t}
\usepackage{graphicx}
\usepackage{caption} 
\usepackage{subcaption}


\usepackage{color-edits}
\addauthor[Evi]{evi}{red}
%%%End of Evi Added%%%%
\addauthor[Vasilis]{vv}{blue}

\begin{document}
\maketitle

\begin{abstract}
    Aggregating preferences under incomplete or constrained feedback is a fundamental problem in social choice and related domains. While prior work has established strong impossibility results for pairwise comparisons, this paper extends the inquiry to improvement feedback, where voters express incremental adjustments rather than complete preferences. We provide a complete characterization of the positional scoring rules that can be computed given improvement feedback. Interestingly, while plurality is learnable under improvement feedback—unlike with pairwise feedback—strong impossibility results persist for many other positional scoring rules. Furthermore, we show that improvement feedback, unlike pairwise feedback, does not suffice for the computation of any Condorcet-consistent rule. We complement our theoretical findings with experimental results, providing further insights into the practical implications of improvement feedback for preference aggregation.
\end{abstract}
\allowdisplaybreaks.

% keywords can be removed
%\keywords{Preference Aggregation \and Incomplete Preferences \and Improvement Feedback \and Positional Scoring Rules \and Condorcet-Consistent Rules \and Computational Social Choice \and Improvement Feedback}


\section{Introduction}
Classical social choice theory assumes that voters provide complete rankings of all candidates, which are then aggregated by a voting rule to select a winner~\citep{brandt2016handbook}. However, this assumption becomes impractical in many real-world scenarios involving a large number of candidates, as voters may be unable or unwilling to rank all of them. For example, in deliberation platforms like Polis~\citep{small2021polis}, users provide pairwise comparisons over a limited subset of opinions rather than full rankings. Similarly, in Reinforcement Learning from Human Feedback (RLHF)—a methodology widely used to fine-tune large language models (LLMs)—feedback often takes the form of comparison queries between pairs of outputs~\citep{instructgpt,christiano2017deep}.

Despite their widespread use,  pairwise or 
$t$-wise comparisons—rankings of 
$t$ candidates—face fundamental limitations. Recent work by~\citet{halpern2024computing} shows that, even under ideal conditions where the preferences of the population over every $t$-wise query are fully known (i.e., for every ranking of $t$ candidates, the proportion of the population that agrees with it is known), the winner under common voting rules cannot be  determined. For example, the plurality winner cannot be reliably identified, and even randomized algorithms fail to perform better than random guessing.

Motivated by these limitations, we explore an alternative type of feedback known as \emph{improvement feedback}. Unlike elicitation methods based on rankings or pairwise comparisons, improvement feedback enables agents to iteratively refine an initial suggestion. For instance, in RLHF applications, improvement feedback could involve a user modifying a draft generated by an LLM to better align with their preferences~\cite{schick2022peer,jin2023data}. Similarly, in robotics, users might iteratively adjust a robot’s trajectory or behavior to achieve their desired outcome~\cite{bajcsy2018learning, yang2024trajectory}. In this setting, users typically refine queried options through targeted adjustments rather than identifying the optimal candidate outright~\citep{shivaswamy2015coactive,tucker2024coactive}.

A simple observation is that improvement feedback offers a promising pathway to address some challenges posed by 
$t$-wise comparisons. For example, unlike pairwise feedback—which struggles to compute the plurality winner—improvement feedback enables users to iteratively refine a suggested candidate. As users provide targeted adjustments, they gradually reveal their top preferences, ultimately enabling the identification of the plurality winner through this iterative process. This raises the following  questions: \begin{quote}
Can other voting rules be computed using improvement feedback? For which voting rules is improvement feedback more effective than pairwise comparisons, and vice versa?
\end{quote}

\subsection{Our Contribution}

We model improvement feedback as a process in which, when a user or agent is queried with a candidate ranked at position \(i\) in their preference order or ranking, they return, with some probability, a candidate ranked at position \(j\), where \(j < i\). The likelihood of returning the candidate at position \(j\) decreases as the distance \(i - j\) increases, reflecting the agent’s tendency to provide localized improvements. To formalize this, we introduce the concept of \emph{\(t\)-improvement feedback}, where a user refines a queried option by selecting a better candidate from the \emph{\(t\)-above neighborhood} of the queried candidate in their preference ranking—i.e., a candidate within the \(t\) positions above the queried candidate. The \(t\)-\textit{improvement feedback distribution} specifies the probabilities of selecting a candidate from this neighborhood. The parameter \(t\) defines the size of this neighborhood, capturing how much better the returned candidate can be relative to the queried one. In practice, \(t\) is typically much smaller than the total number of candidates, \(m\), reflecting the limited cognitive and practical effort users are willing to expend.

Our goal is to investigate whether winners under various voting rules can be identified using \(t\)-\emph{improvement feedback queries} over the underlying preferences of the agents, which are unknown to the algorithm. We consider an idealized setting, similar to~\cite{halpern2024computing}, where the algorithm has access to the full statistical distribution of feedback responses. Specifically, with a sufficiently large number of \(t\)-improvement feedback queries, the algorithm knows the exact probability of receiving candidate \(b\) as feedback when querying candidate \(a\), given the population’s preferences and the underlying \(t\)-improvement feedback distribution. This idealized assumption strengthens our impossibility results and models scenarios where such statistical information is available or can be effectively estimated.

In~\Cref{sec:positional-scoring-rules}, we study the well-known family of positional scoring rules and provide a complete characterization of the rules that are learnable under \(t\)-improvement feedback queries, for all practical values of \(t\). We show that beyond plurality and a specific positional scoring rule uniquely determined by the \(t\)-improvement feedback distribution, as well as any linear combination of the two, no other positional scoring rule is learnable using \(t\)-improvement feedback queries for \emph{any} value of \(t \leq \sfrac{m}{2} - 2\). In fact, we show that even randomized algorithms cannot reliably identify the correct winner with a probability greater than \(1/m\) in this case. As discussed in the introduction, in practical settings \(t \ll m\), making these findings particularly relevant. We also extend these negative results for every \(t \in [m-1]\) under the uniform \(t\)-improvement distribution, where when a candidate $a$ is queried, a candidate from its  $t$-above neighborhood is returned uniformly at random.

In~\Cref{sec:condorcet-consistent-rules}, we turn our attention to Condorcet-consistent rules, which select the Condorcet winner whenever one exists. While pairwise comparisons suffice to identify the Condorcet winner, we  surprisingly show that under \(t\)-improvement feedback, no (randomized) algorithm can determine the Condorcet winner with probability greater than \(1/m\). This result applies under the uniform \(t\)-improvement feedback model for all values of \(t\) and extends to all \(t \leq \sfrac{m}{2} - 2\) for almost every \(t\)-improvement feedback distribution. We also discuss the one specific exception to this result in detail.

These theoretical results indicate that while \(t\)-improvement feedback is particularly effective for identifying the plurality winner, it is insufficient for computing many other widely studied rules in social choice theory, such as Kemeny, Copeland, or Borda, which, by contrast, can be identified using pairwise comparison feedback.

Lastly, in~\Cref{sec:experiments}, we compare the two types of feedback through simulations. Interestingly, contrary to the theoretical results, \(t\)-improvement feedback queries turn to  be more efficient in some cases for implementing rules—such as Copeland or Borda—that are learnable from pairwise comparison feedback but are not implementable by \(t\)-improvement feedback in the theoretical worst case.

\subsection{Related Work}

Our work contributes to the growing body of research on decision-making with partial access to votes.  \citet{filmus2014efficient} and \citet{oren2013efficient} studied \(t\)-top queries, where each agent reports their top \(t\)-candidates, and investigated how large \(t\) must be to reliably identify the correct winner under various voting rules. Similarly, \citet{bentert2020comparing} examined \(t\)-wise comparison queries, analyzing which voting rules can be implemented with this feedback. More recently, \citet{halpern2024computing} provided a complete characterization of positional scoring rules that can be computed using \(t\)-wise comparison feedback.  

These works build on foundational results regarding pairwise comparisons, which are often represented as (weighted) tournament graphs. Tournament graphs serve as the primary input for many well-known voting rules, including Borda count and several Condorcet-consistent rules, such as Copeland, Kemeny, and Minimax~\cite{brandt2016handbook}. Our work builds on and extends the results of \citet{halpern2024computing}, which we discuss in greater detail later. To the best of our knowledge, no prior work has studied improvement feedback query for identifying the correct winner under different voting rules. 

The query complexity of identifying the correct candidate has been also studied over different queries. For example, the query complexity of tournament graphs has been studied in the context of identifying Condorcet winners~\citep{procaccia2008note}. Other works have explored the query complexity of complete rankings, focusing either on identifying the winner~\cite{dey2015sample} or recovering the full ranking~\cite{micha2020can} of different voting rules. In contrast, our work focuses on information-theoretic impossibilities, assuming perfect access to the feedback model under consideration, and does not analyze the sample complexity of learning voting rules.

Our work is also related to frameworks like coactive learning~\citep{shivaswamy2015coactive} and interactive preference learning~\citep{tucker2024coactive}, which focus on scenarios where users provide incremental improvements rather than global rankings. However, these frameworks aim to minimize regret and learn a good candidate for a single user through iterative interaction. In contrast, we investigate whether improvement feedback is sufficient to identify the correct candidate when preferences are heterogeneous across a population.

\section{Model}

\par For $k \in \mathbb{N}$, let $[k] = \{1,2,\dots,k\}$. We consider a set $M = \{a_1, \dots, a_m\}$ of $m$ candidates. A ranking or preference $\sigma$ over the candidates is a bijection $\sigma: [m] \rightarrow  M$, where $\sigma(i)$ returns the candidate that is the $i$-th most preferred candidate in $\sigma$ and $\sigma^{-1}(a)$ returns the position of candidate $a$ in $\sigma$. We denote by $\mathcal{L}(M)$ the set of all  $m!$ possible rankings over $M$. We use $a \succ_{\sigma} b$ to denote that candidate $a$ is ranked above candidate $b$ under ranking $\sigma$.

Let $\Delta(L(M))$ be the set of probability distributions over $L(M)$.  A preference profile corresponds to a distribution $\dist\in \Delta(L(M))$  which represents the proportion of users in the population that have each ranking. For example, if $\Pr_{\sigma \sim \dist}[\sigma=a_1\succ \ldots \succ a_m]=1/3$ this means that $1/3$ of the population holds the ranking $a_1\succ \ldots \succ a_m$.

Given a permutation over the candidates \(\pi : M \to M\), 
we define \(\pi \circ \sigma\) as the ranking obtained by permuting the candidates in \(\sigma\) according to \(\pi\). The special case \(\pi_{ab}\) swaps only candidates \(a\) and \(b\), such that \(\pi_{ab}(a) = b\), \(\pi_{ab}(b) = a\), and \(\pi_{ab}(c) = c\) for all \(c \neq a, b\). For preference profiles, we denote by \(\pi \circ \dist\) the preference profile induced by sampling \(\sigma \sim \dist\) and outputting \(\pi \circ \sigma\). Lastly, for swapping \(a\) and \(b\), \(\swapdist = \pi_{ab} \circ \dist\).

\paragraph{Voting Rules.}

A voting rule is a function \(f:\Delta(L(M))\rightarrow M\) that takes as input a preference profile and outputs a candidate as the winner. We call the winner of \(f\) the \(f\)-winner. In this work, we focus on two main families of voting rules.



The first family is the family of \emph{positional scoring rules}, which are defined using a scoring vector \(\vec{s} = \left(s_1, s_2, \dots, s_m\right) \in \mathbb{R}^m\) with $s_i\geq s_{i+1}$ for all $i\in [m-1]$ and $s_1>s_m$. Without loss of generality, we usually assume $s_m=0$. The score of a candidate \(a \in M\) under a positional scoring rule \(f_{\vec{s}}\), parametrized by a scoring vector \(\vec{s}\), given a preference profile \(D\in \Delta(L(M))\), is defined as follows:
\[
sc_{\vec{s}}(a,D) = \sum_{i = 1}^{m}\Pr_{\sigma \sim D}[\sigma^{-1}(a)=i]\cdot s_i.
\]
The rule \(f_{\vec{s}}\) then returns the candidate with the highest score as the winner, breaking ties arbitrarily. Some well-known positional scoring rules are plurality, parametrized by \(\vec{s}_{plu}=(1,0,\ldots, 0)\); veto, parametrized by \(\vec{s}_{veto}=(1,\ldots, 1, 0)\); and Borda count, parametrized by \(\vec{s}_{Borda}=(m-1, m-2, \ldots, 0)\).



The second family  is the family of \emph{Condorcet-consistent rules}. A candidate \(a \in M\) is called the \emph{Condorcet winner} if they beat every other candidate in pairwise majority comparisons, i.e., \(\Pr_{\sigma \sim D}[a \succ_\sigma b] > 0.5\) for all \(b \in M \setminus \{a\}\). A Condorcet-consistent voting rule \(f\) selects the Condorcet winner whenever one exists. 
Famous examples of Condorcet-consistent rules include Copeland's Rule, Kemeny’s Rule,  the Minimax Rule,  Ranked pairs and many others~\cite{brandt2016handbook}.

\paragraph{Improvement Feedback Distribution.}

In the improvement feedback setting, when an agent is queried with a candidate \(a \) ranked at position \(i\) in the agent’s preference ranking \(\sigma\), the agent returns an improved candidate \(b \) ranked at position \(j\), where \(j < i\). The candidates ranked in positions \(j \in [\max(i - t, 1), i - 1]\) are referred to as the \emph{\(t\)-above neighborhood} of \(\sigma(i)\), representing the set of candidates that are strictly preferred but within \(t\) positions above the queried candidate. The only exception occurs when \(a\) is the agent’s first choice (i.e., \(i = 1\)), in which case no improvement is provided, and the agent returns \(a\).

More formally, for a fixed parameter \(t \leq m-1\), the \emph{\(t\)-improvement feedback distribution} defines the probability of returning a candidate \(\sigma(j)\) when querying \(\sigma(i)\), denoted as \(p^t_{i,j}\). The distribution satisfies the following properties:
\begin{itemize}
    \item \(p^t_{i,j} > 0\) only if \(j < i\), ensuring that the returned candidates are strictly preferred to the queried candidate. The only exception is \(p^t_{1,1} = 1\), reflecting that no improvement is possible when querying the top-ranked candidate.
    \item The probabilities sum to 1 over the candidates in the \emph{\(t\)-above neighborhood}:
    \[
    \sum_{j = \max(i - t, 1)}^{i-1} p^t_{i,j} = 1, \quad \forall i > 1.
    \]
\end{itemize}


We pay special attention to the \textit{uniform \(t\)-improvement feedback distribution}, where the agent selects an improved candidate uniformly at random from the \(t\)-above neighborhood:
\[
p^t_{i,j} =
\begin{cases} 
\frac{1}{\min(t, i-1)}, & \text{if } 0 < i - j \leq t, \\
0, & \text{otherwise}.
\end{cases}
\]
We also denote \( P^t_{i} \) as the cumulative probability that a candidate ranked at position \( i \) is returned as an improvement when querying a candidate ranked at any position below \( i \), i.e.,  
\[
P^t_{i} = \sum_{j=i+1}^{\min(m,i+t)} p^t_{j,i}.
\]
Intuitively, \( P^t_{i} \) represents the overall likelihood that a candidate at position \( i \) is selected through the improvement feedback process, summing over the probabilities of being reached from any candidate  ranked below it. Notably, \( P_m^t = 0 \) since there is no candidate ranked below the last position \( m \) that could return it as an improvement.


\paragraph{Improvement Feedback Queries.}
We consider algorithms that  make \(t\)-improvement feedback queries over the preference profile \(D\). Specifically, we assume that an algorithm has access to the exact probability of observing \(a \in M\) when querying \(b \in M\), denoted by \(\Pr_{\sigma \sim D}[q^t_{\sigma}(b) = a]\), and given by:

\begin{align*}
\Pr_{\sigma \sim D}[q^t_{\sigma}(b) = a]=
 \sum_{i=1}^{m-1} \sum_{j=i+1}^{\min(m, i+t)} p^t_{j,i} \cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i, \sigma^{-1}(b)=j].
\end{align*}
This expression considers all possible positions of \( a \) and \( b \) in the ranking, weighting the feedback probability \( p^t_{j,i} \) by the likelihood that \( a \) and \( b \) occupy these positions under the population distribution \( D \). 
Additionally, we denote by \( \Pr_{\sigma \sim D}[q^t(a) = a] \) the probability of not returning any improvement, as \( a \) is already ranked first. This probability reflects the likelihood that \( a \) is the top-ranked candidate in the preference profile. Therefore, as discussed in the introduction, when this information is available, the plurality winner can be directly determined.




Having direct access to this distribution provides strictly more information than an approximation based on random or adjusted queries (where, upon querying  $a$, the algorithm observes  $b$ with some probability). Since our impossibility results hold even under this idealized setting, they provide fundamental limits that also apply to real-world scenarios with finite queries.



\paragraph{\(t\)-Indistinguishable Preference Profiles.}  
Two preference profiles \(\dist_1, \dist_2 \in \Delta(L(M))\) are said to be \(t\)-indistinguishable if, for every pair of candidates \(a, b \in M\), the probability of receiving \(a\) as feedback when querying \(b\) is the same under both profiles. Formally,  
\[
\Pr_{\sigma \sim \dist_1}[q^t_{\sigma}(b) = a] = \Pr_{\sigma \sim \dist_2}[q^t_{\sigma}(b) = a].
\]  
Thus, no algorithm with access only to \(t\)-improvement feedback can distinguish between \(\dist_1\) and \(\dist_2\).

In our results, we will focus on the \(t\)-indistinguishability between a preference profile \(\dist\) and its swapped version \(\swapdist\). In this case, when the feedback probabilities satisfy  
\begin{align*}
\Pr_{\sigma \sim \dist}[q^t_{\sigma}(x) = y] &= \Pr_{\sigma \sim \swapdist}[q^t_{\sigma}(x) = y]
= \Pr_{\sigma \sim \dist}[q^t_{\sigma}(\pi_{ab}(x)) = \pi_{ab}(y)],
\end{align*}
for all \(x, y \in M\), then the profiles are \(t\)-indistinguishable.

\section{Main  \texorpdfstring{$t-$}{t-}Indistinguishable  Preference  Profiles}


We begin by constructing a family of preference profiles, denoted \( D_{i,j,\ell} \), which will serve as the foundation for our impossibility results. 

\begin{lemma}\label{lem:main-construction}
Let \( D_{i,j,\ell} \) be  a preference profile  defined with respect to two candidates \( a \) and \( b \), as follows:
\begin{enumerate}
    \item With probability \( p \), candidate \( a \) is fixed at position \( i \) (where \( i > 1 \)), and candidate \( b \) is fixed at position \( j \) (where \( j - i > t \)).
    \item With probability \( 1-p \), candidate \( a \) is fixed at position \( m \), and candidate \( b \) is fixed at position \( \ell \), where \( 1 < \ell < m - t \).
    \item Select a uniformly random ranking \( \tau^{\Sab} \) of all remaining candidates (excluding \( a \) and \( b \)). This ranking determines their relative order, and they are then assigned to the unoccupied positions.
\end{enumerate}
   For any $m\geq 4$ and any \( t \in [m-1] \), the preference profiles \( \dist_{i,j,\ell} \) and \( \swapdist_{i,j,\ell} \) are \( t \)-indistinguishable when \( p = \frac{P^t_{\ell}}{P^t_{i} - P^t_{j} + P^t_{\ell}} \).
\end{lemma}

\begin{proof}
    Let $a,b$ be the candidates of the statement. %For showing that $\dist_{i,j,\ell}$ and $\swapdist_{i,j,\ell}$ are $t$-indistinguishable, we will show that for every $x,y \in M$ it holds that $Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = y] = Pr_{\sigma \sim \swapdist_{i,j,\ell}}[q_\sigma^t(x) = y]$. 
    
    Since $j - i >t$ and $\ell < m-t$, $a,b$ are more than $t$ positions apart in both rankings. Therefore, it holds that $Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(a) = b] = Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = a] = 0$.
         Moreover, since $ i >1$ and $\ell>1$, it holds that $Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(a) = a] = Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = b] = 0$. 
    
    
    
    Furthermore, for every $x \in \Sab$, the following holds:

\begin{align*}
    Pr_{\sigma \sim \dist_{i,j,\ell}}[q^t_\sigma(a) = x] &= p \cdot \sum_{k = \max(i-t, 1)}^{i-1} Pr [\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x] 
    \\
    &+ (1-p) \cdot \sum_{ k = m-t-1}^{m-2} Pr [\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k = \max(i-t, 1)}^{i-1} Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
    &+ (1-p) \cdot \frac{1}{m-2}  \cdot \sum_{k = m-t-1}^{m-2}Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k = \max(i-t, 1)}^{i-1} p_{i,k}^t  \\
    &+ (1-p) \cdot \frac{1}{m-2} \cdot   \sum_{ k = m-t-1}^{m-2} p_{m,k+1}^t \\
    &= p \cdot \frac{1}{m-2} + (1-p) \cdot \frac{1}{m-2} = \frac{1}{m-2},
\end{align*}
    where the second equality holds because $\tau^{\Sab}$ is chosen uniformly at random and the second
    last equality holds because the sum of probabilities of returning a candidate ranked above the queried candidate, over all valid positions within the  $t$-above neighborhood, equals $1$.
    
   

Similarly,

\begin{align*}
    Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = x] &= 
    p \cdot \sum_{k = \max( j-t, 1)-1}^{j-2} Pr [\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x] \\
    &+ (1-p) \cdot \sum_{k = \max( \ell-t, 1)}^{\ell - 1} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x]\\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k =\max( j-t, 1)-1}^{j-2} Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x]\\
    &+ (1-p) \cdot \frac{1}{m-2} \cdot \sum_{k = \max( \ell-t, 1)}^{\ell-1} Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x]\\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k = \max( j-t, 1)-1}^{j-2} p^t_{j,k+1} \\
    &+ (1-p) \cdot \frac{1}{m-2} \cdot \sum_{k = \max( \ell-t, 1)}^{\ell - 1} p^t_{\ell,k}\\
    &= p \cdot \frac{1}{m-2} + (1-p) \cdot \frac{1}{m-2} = \frac{1}{m-2}.
\end{align*}

Next, we prove that for $p = \frac{\sunif_\ell}{\sunif_i - \sunif_j + \sunif_\ell}$, and every $x \in \Sab$, it holds that $Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = a] = Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = b]$. Note that

\begin{align*}
Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = a] &= p \cdot \sum_{k = i}^{i+t-1}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
&= p \cdot \frac{1}{m-2}\cdot \sum_{k = i}^{i+t-1} Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
&= p \cdot \frac{1}{m-2}\cdot \sum_{k = i}^{i+t-1} p_{k+1,i}^t \\
&= \frac{p}{m-2}\cdot \sunif_i
\end{align*}
where the last equality is by definition of $\sunif_i$.

Similarly,
\begin{align*}
Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = b] &= p \cdot \sum_{k = j-1}^{j+t-2}Pr[\tau^{\Sab}(k) = x] Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x]\\
&+ (1-p) \cdot \sum_{k = \ell}^{\ell+t-1}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x]\\
&= \frac{p}{m-2}\cdot \sum_{k = j-1}^{j+t-2} Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x]\\
&+ \frac{1-p}{m-2} \cdot \sum_{k = \ell}^{\ell+t-1} Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x]\\
&= \frac{p}{m-2}\cdot \sum_{k = j-1}^{j+t-2}  p_{k+2,j}^t+ \frac{1-p}{m-2} \cdot \sum_{k = \ell}^{\ell+t-1} p_{k+1,\ell}^t\\
&= \frac{p}{m-2}\cdot \sunif_j + \frac{1-p}{m-2} \cdot \sunif_\ell.
\end{align*}

Observe that we can make the two probabilities above equal by setting $p = \frac{\sunif_\ell}{\sunif_i - \sunif_j + \sunif_\ell} $. Lastly, for any $x,y \in \Sab$ it holds that $Pr_{\sigma \sim \dist_{i,j,\ell}}[q_\sigma^t(x) = y] = Pr_{\sigma \sim \swapdist_{i,j,\ell}}[q_\sigma^t(x) = y]$, since the two preference profiles differ only with respect to $a,b$. From all the above, we get that  $\dist_{i,j,\ell}$ and $\swapdist_{i,j,\ell}$ are $t$-indistinguishable by setting $p$ as stated in the lemma.

\end{proof}


We will also rely on the following technical lemma to establish our impossibility results. Intuitively, this lemma suggests that if there exist \(m\) distinct preference profiles that are \(t\)-indistinguishable from one another, and each has a different \(f\)-winner under some voting rule \(f\), then no randomized algorithm can identify the correct winner with a probability greater than \(\sfrac{1}{m}\). This result corresponds to Lemma 4.2 of~\cite{halpern2024computing}, but we restate and prove it here for completeness.

\begin{lemma}\label{lem:random-alg}
    Fix any voting rule $f$ and suppose there is a family of $m$  preference profiles that are all $t$-indistinguishable from one another, but each has a distinct singleton $f$-winner. Then, for all (possibly randomized) algorithms $A$ that have access to $t$-improvement  feedback, there is a preference profile with a unique $f$-winning candidate $a$, such that $A$ outputs $a$  with probability at most $1/m$. 
\end{lemma}
\begin{proof}
    We denote with $\{\dist^c\}_{c\in M}$  the set of $m$ preference profiles that are all $t$-indistinguishable and $c$ is the unique winner in $D^c$ under $f$.  When the algorithm is applied to any of this preference profiles, it will receive the exact same feedback, and therefore the output should be the same across them. Due to pigeonhole principle, there must be a candidate $a$ that is returned as the winner with probability at most $1/m$, and hence $D^a$ satisfies the desired property. 
\end{proof}

\section{Positional Scoring Rules}\label{sec:positional-scoring-rules}

In this section, we consider the family of positional scoring rules and provide a complete characterization of the rules that are learnable under \(t\)-improvement feedback queries. For general \(t\)-improvement feedback distributions, this characterization applies for all \(t \leq \sfrac{m}{2} - 2\), while for the uniform \(t\)-improvement feedback distribution, it extends to all \(t \in [m - 1]\).

In~\Cref{subsection:positional-general}, we show that for \emph{any} value of \(t\leq \sfrac{m}{2}-2\), the only learnable scoring vectors are linear combinations of \(\vec{s}_t^*\) and \(\vec{s}_{\text{plu}}\), where \(\vec{s}_t^* = (P_1^t, \ldots, P_m^t)\). In fact, we prove that even randomized algorithms cannot identify the correct winner for any scoring vector \(\vec{s}\) outside the  \(\operatorname{span}(\vec{s}_t^*, \vec{s}_{\text{plu}})\) with a probability greater than \(1/m\). In~\Cref{subsection:positional-uniform}, we extend these negative results to cover all values of \(t\) under the uniform \(t\)-improvement feedback distribution.


For showing our negative results, we start with the following lemma, which establishes the conditions under which a family of \(t\)-indistinguishable profiles, as described in~\Cref{lem:random-alg}, can be constructed. This lemma closely resembles Lemma 4.1 of~\citet{halpern2024computing}.

\begin{lemma} \label{lem:main-scoring-rules}
Fix a scoring vector \(\vec{s}\). Let \(\dist\) and \(\swapdist\) be two indistinguishable preference profiles and let \(a, b \in M\) be two candidates such that \(sc_{\vec{s}}(a, \dist) \neq sc_{\vec{s}}(b, \dist)\). Then, there exists a family of preference profiles \(\{D^c\}_{c \in M}\) such that all profiles are \(t\)-indistinguishable from one another, but each candidate \(c \in M\) uniquely maximizes \(sc_{\vec{s}}(c, \dist^c)\).
\end{lemma}

\begin{proof} 
Given \(D\), where, without loss of generality,  \(sc_{\vec{s}}(a,\dist) >  sc_{\vec{s}}(b,\dist)\), we construct a family of preference profiles \(\{D^c\}_{c \in M}\) in the same way as in Lemma 4.1 of \citet{halpern2024computing}. Briefly, we sample a permutation \(\pi\) over the candidates uniformly at random.  If \(\pi(b) = c\), we sample a ranking from \(\pi \circ \swapdist\); otherwise, if \(\pi(b) \neq c\), we sample a ranking from \(\pi \circ \dist\). When \(sc_{\vec{s}}(a,\dist) >  sc_{\vec{s}}(b,\dist)\), \citet{halpern2024computing} show in Lemma 4.1 that \(c\) is the unique score maximizer in \(D^c\).  

It remains to show that \(\{\dist^c\}_{c\in M}\) are \(t\)-indistinguishable from one another. Note that if \(\dist\) and \(\swapdist\) are \(t\)-indistinguishable, then \(\pi\circ\dist\) and \(\pi\circ\swapdist\) are also \(t\)-indistinguishable for all \(\pi\). Consequently, for every \(D^c\) and \(D^{c'}\) and for all \(x,y\in M\), we have:

\begin{align*}
    \Pr_{\sigma \sim D^c}[q^t_\sigma(x)=y] &= \sum_{\pi \in L(M): \pi(b)\neq c} \Pr_{\sigma \sim\pi \circ \dist}[q^t_\sigma(x)=y] \cdot \frac{1}{m!}  + \sum_{\pi \in L(M): \pi(b)=c} \Pr_{\sigma \sim\pi \circ \swapdist}[q^t_\sigma(x)=y] \cdot \frac{1}{m!} \\
    &= \frac{1}{m!} \sum_{\pi \in L(M)} \Pr_{\sigma \sim\pi \circ \dist}[q^t_\sigma(x)=y] \\
    &= \sum_{\pi \in L(M): \pi(b)\neq c'} \Pr_{\sigma \sim\pi \circ \dist}[q^t_\sigma(x)=y] \cdot \frac{1}{m!} + \sum_{\pi \in L(M): \pi(b)=c'} \Pr_{\sigma \sim\pi \circ \swapdist}[q^t_\sigma(x)=y] \cdot \frac{1}{m!}  \\
    &= \Pr_{\sigma \sim D^{c'}}[q^t_\sigma(x)=y],
\end{align*}
where the first and third equalities follow because $\pi$ is chosen uniformly at random from $L(M)$ and
the second and fourth equalities follow from the fact that \(\pi\circ\dist\) and \(\pi\circ\swapdist\) are \(t\)-indistinguishable, and the lemma follows.

\end{proof}

\subsection{General \texorpdfstring{$t-$}{t-}Improvement Feedback Distribution} \label{subsection:positional-general}


Here, we consider general \(t\)-improvement feedback distributions. To provide a complete characterization of the scoring rules that are learnable under \(t\)-improvement feedback queries, we first prove the following lemma.

\begin{lemma}\label{lem:positional-dist-general}  
For any $m\geq 6$, any \(t \leq \sfrac{m}{2} - 2\), any pair of candidates \(a, b \in M\) and any scoring vector \(\vec{s} \not \in \operatorname{span}(\vec{s}_{\text{plu}}, \vec{s}_{t}^*)\), there exists a preference profile \(\dist\) such that \(sc_{\vec{s}}(a, \dist) \neq sc_{\vec{s}}(b, \dist)\), and \(\dist\) and \(\swapdist\) are \(t\)-indistinguishable.  
\end{lemma}

\begin{proof}
Consider the preference profile \(\dist_{i, m, i+1}\) as defined in~\Cref{lem:main-construction}. From~\Cref{lem:main-construction}, we have that for all \(i \in \{2, \dots, m - t - 2\}\), \(\dist_{i, m, i+1}\) and \(\swapdist_{i, m, i+1}\) are \(t\)-indistinguishable for every \(t \leq m - 4\), when  
\[
p = \frac{\sunif_{i+1}}{\sunif_i - \sunif_m + \sunif_{i+1}} = \frac{\sunif_{i+1}}{\sunif_i + \sunif_{i+1}}.
\] 
If it holds that \(sc_{\vec{s}}(a, \dist_{i, m, i+1}) \neq sc_{\vec{s}}(b, \dist_{i, m, i+1})\) for some \(i \in \{2, \dots, m - t - 2\}\), then setting \(\dist = \dist_{i, m, i+1}\) satisfies the lemma. 

Now, suppose that for every \(i \in \{2, \dots, m - t - 2\}\), we have  
\[
sc_{\vec{s}}(a, \dist_{i, m, i+1}) = sc_{\vec{s}}(b, \dist_{i, m, i+1}).
\] 
This implies that  
\[
p \cdot s_i = (1 - p) \cdot s_{i+1} \implies p = \frac{s_{i+1}}{s_i + s_{i+1}}.
\] 
Thus, we have  
\begin{align}\label{eq:lemma-positional-general-1}
    p = \frac{\sunif_{i+1}}{\sunif_i + \sunif_{i+1}} = \frac{s_{i+1}}{s_i + s_{i+1}} \implies \frac{s_{i+1}}{\sunif_{i+1}} = \frac{s_i}{\sunif_i} = \lambda,
\end{align}
for all \(i \in \{2, \dots, m - t - 2\}\).

Assuming \(\sfrac{s_i}{\sunif_i} = \lambda\) for all \(i \in \{2, \dots, m - t - 1\}\), we next consider the preference profile \(\dist_{2, i, 2}\) as defined in~\Cref{lem:main-construction}. From~\Cref{lem:main-construction}, we have that for all \(i \in \{m - t, \dots, m - 1\}\), \(\dist_{2, i, 2}\) and \(\swapdist_{2, i, 2}\) are \(t\)-indistinguishable for every \(t \leq \sfrac{m}{2} - 2\), when  
\[
p = \frac{\sunif_{2}}{2\sunif_2 - \sunif_{i}}.
\] 
If it holds that \(sc_{\vec{s}}(a, \dist_{2, i, 2}) \neq sc_{\vec{s}}(b, \dist_{2, i, 2})\) for some \(i \in \{m - t, \dots, m - 1\}\), then setting \(\dist = \dist_{2, i, 2}\) satisfies the lemma. 

Now, suppose that for every \(i \in \{m - t, \dots, m - 1\}\), we have  
\[
sc_{\vec{s}}(a, \dist_{2, i, 2}) = sc_{\vec{s}}(b, \dist_{2, i, 2}).
\] 
This implies  
\[
p \cdot s_2 = (1 - p) \cdot s_2 + p \cdot s_i \implies p = \frac{s_2}{2s_2 - s_i}.
\] 
Thus, we obtain  
\[
p = \frac{\sunif_{2}}{2\sunif_2 - \sunif_{i}} = \frac{s_2}{2s_2 - s_i}.
\] 
From~\Cref{eq:lemma-positional-general-1}, we know that \(\frac{s_2}{P_2^t} = \lambda\). Then by combining~\Cref{eq:lemma-positional-general-1} and the above equality, we get  that \(\frac{s_i}{P_i^t} = \lambda\) for all \(i \in \{2, \dots, m - 1\}\). This means that, unless \(\frac{s_i}{P_i^t} = \lambda\) for all \(i \in \{2, \dots, m - 1\}\), which implies that \(\vec{s} \in \operatorname{span}(\vec{s}_{\text{plu}}, \vec{s}_{t}^*)\), there exists a preference profile \(\dist\) satisfying the conditions of the lemma and the lemma follows.
\end{proof}




Next, we show that if $\vec{s}\in span(\vec{s}^*_t, \vec{s}_{plu})$, then the scores of all the candidates under $\vec{s}$ can be computed with access to $t$-improvement feedback queries. 


\begin{lemma} \label{lemma:positional-learnable}
For any $t\in [m-1]$ and any $\vec{s}\in span(\vec{s}_{\text{plu}},\vec{s}_{t}^*)$, given $a\in M $ and $D\in \Delta(L(M))$, it is possible to calculate $sc_{\vec{s}}(a,D)$ with $t$-improvement feedback queries. 
\end{lemma}

\begin{proof}
    First, we note that $sc_{s_{plu}}(a,D)=\Pr_{\sigma \sim D}[\sigma^{-1}(a)=1]= \Pr_{\sigma\sim D}[q^t(a)=a]$, where the second equality is derived from the fact that when $a$ is the top choice of an agent the agent just returns this candidate. Since the algorithm knows $\Pr_{\sigma\sim D}[q^t(a)=a]$, the plurality score of candidate $a$ is computable. 

    Second, note that  
\begin{align*}
&\sum_{b\in M\setminus \{a\}} \Pr_{\sigma \sim D}[q^t_{\sigma}(b) = a] =\\
&=\sum_{b\in M\setminus \{a\}}\sum_{i=1}^{m} \sum_{j=i+1}^{\min(m, i+t)} p^t_{j,i} \cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i, \sigma^{-1}(b)=j]\\
&=\sum_{i=1}^{m} \sum_{j=i+1}^{\min(m, i+t)} \sum_{b\in M\setminus \{a\}} p^t_{j,i} \cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i, \sigma^{-1}(b)=j]\\
&=\sum_{i=1}^{m} \sum_{j=i+1}^{\min(m, i+t)} \sum_{b\in M\setminus \{a\}} p^t_{j,i} \cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i\mid \sigma^{-1}(b)=j]\cdot \Pr_{\sigma \sim D}[\sigma^{-1}(b)=j]\\
&=\sum_{i=1}^{m} \sum_{j=i+1}^{\min(m, i+t)}  p^t_{j,i} \cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i]\\
&=\sum_{i=1}^{m} P_i^t\cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i]= sc_{\vec{s}^*_t}(a,D).
 \end{align*}
    where the first equality follows from the definition of $ \Pr_{\sigma \sim D}[q^t_{\sigma}(b) = a]$, the second equality follows by rearranging the summations, the second last equality follows from the definition of $P^t_i$ and the last equality follows from the definition of $\vec{s}^*_t$. Since the algorithm knows the quantity $Pr_{\sigma \sim D}[q^t_{\sigma}(b) = a]$ for all $a,b\in M$, it can learn $sc_{\vec{s}^*_t}(a,D)$ for every $a\in M$. 

    The lemma follows by noticing that for each $\vec{s}=\lambda_1\cdot\vec{s}_{plu}+ \lambda_2\cdot\vec{s}^*_t$, from linearity of expectation we have that $sc_{\vec{s}}(a,D)= \lambda_1  \cdot sc_{\vec{s}_{plu}}(a,D)+ \lambda_2\cdot sc_{\vec{s}^*_t}(a,D)$ for any $\lambda_1$ and $\lambda_2$. 
\end{proof}

Now, we are ready to prove the following lemma.


\begin{theorem} \label{ther:positional-general}
For any \( m \geq 6 \), any \( t \leq \sfrac{m}{2} - 2 \), and any scoring vector \( \vec{s} \):
\begin{enumerate}
    \item If \( \vec{s} \in \operatorname{span}(\vec{s}^*_t, \vec{s}_{plu}) \), then using \( t \)-improvement feedback queries, the candidate that maximizes the score under \( \vec{s} \) for any  input profile \( D \) can be found.
    \item If \( \vec{s} \not\in \operatorname{span}(\vec{s}^*_t, \vec{s}_{plu}) \), then no randomized algorithm with access to \( t \)-improvement feedback queries can output the candidate with the maximum score under \( \vec{s} \) for any input profile \( D \) with probability greater than \( 1/m \).
\end{enumerate}
\end{theorem}

\begin{proof}
    The first part of the lemma immediately follows from~\Cref{lemma:positional-learnable}. 

 For the second part, since in~\Cref{lem:positional-dist-general} we show the existence of a preference profile $\dist$ that is indistinguishable from $\swapdist$ and has the desired properties, we can apply~\Cref{lem:main-scoring-rules} for constructing a family of preference profiles that are $t$-indistinguishable and each of them has a different candidate as a winner under a scoring rule not in $span(\vec{s}^*_t, \vec{s}_{plu})$. Then, by applying~\Cref{lem:random-alg}, the theorem follows. 
\end{proof}


An interesting observation arises when \( t = 1 \). In this case, \( P_i^t = 1 \) for all \( i \in [m - 1] \), since each candidate ranked at position \( i \) can only be returned as an improvement of a candidate ranked at position \( i + 1 \), and when a candidate at position \( i + 1 \) is queried, the agent always returns the candidate at position \(i\) with probability 1. As a result, the scoring vector \( \vec{s}^*_t = (1, \ldots, 1, 0) \), which corresponds to the veto scoring rule. Therefore, when \( t = 1 \), we can learn all rules within the span of plurality and veto. However, for larger values of \( t \), the veto scoring rule is no longer learnable under \( t \)-improvement feedback queries.

\subsection{Uniform \texorpdfstring{$t-$}{t-}Improvement Feedback Distribution} \label{subsection:positional-uniform}
Here, we turn our attention to the uniform \( t \)-improvement feedback distribution. In this setting, we extend~\Cref{lem:positional-dist-general} to cover all values of \( t \). To derive this result, we introduce an additional set of preference profiles, distinct from those in~\Cref{lem:main-construction}, and carefully analyze the resulting cases.

\begin{lemma}\label{lem:positional-dist-uniform}  
Under uniform $t$-improvement feedback distribution, for any \(t \in [m-1]\), any pair of candidates \(a, b \in M\) and any scoring vector \(\vec{s} \not \in \operatorname{span}(\vec{s}_{\text{plu}}, \vec{s}_{t}^*)\), there exists a preference profile \(\dist\) such that \(sc_{\vec{s}}(a, \dist) \neq sc_{\vec{s}}(b, \dist)\), and \(\dist\) and \(\swapdist\) are \(t\)-indistinguishable.  
\end{lemma}


From the above lemma, we immediately derive the following theorem in a manner similar to~\Cref{ther:positional-general}.
\begin{theorem} \label{ther:positional-uniform}
Under uniform $t$-improvement feedback distribution, for any \( m \geq 6 \), any \( t \in [m-1] \), and any scoring vector \( \vec{s} \):
\begin{enumerate}
    \item If \( \vec{s} \in \operatorname{span}(\vec{s}^*_t, \vec{s}_{plu}) \), then using \( t \)-improvement feedback queries, the candidate that maximizes the score under \( \vec{s} \) for any  input profile \( D \) can be found.
    \item If \( \vec{s} \not\in \operatorname{span}(\vec{s}^*_t, \vec{s}_{plu}) \), then no randomized algorithm with access to \( t \)-improvement feedback queries can output the candidate with the maximum score under \( \vec{s} \) for any input profile \( D \) with probability greater than \( 1/m \).
\end{enumerate}
\end{theorem}

\section{Condorcet Consistent Rules}\label{sec:condorcet-consistent-rules}

In the previous section, we showed that \(t\)-improvement feedback queries do not help us overcome the impossibility results associated with positional scoring rules and pairwise comparison queries, except for plurality and a specific positional scoring rule determined by the \(t\)-improvement feedback distribution. In this section, we extend these negative results to the family of Condorcet-consistent rules.

In~\texorpdfstring{\Cref{subsection:condorcet-uniform}}{Section}, we demonstrate that for every \(t \in [m - 1]\), no algorithm can reliably identify the Condorcet winner using uniform \(t\)-improvement feedback queries. In fact, even randomized algorithms cannot identify the correct candidate with a probability greater than \(\sfrac{1}{m}\). In~\texorpdfstring{\Cref{subsection:condorcet-general}}{Section}, we extend this negative result to \emph{any} \(t\)-improvement feedback distributions for \(t \leq \sfrac{m}{2} - 2\), with only one exception, which we discuss in detail. These results contrast sharply with pairwise comparison queries, which are sufficient for identifying the Condorcet winner whenever one exists.



 
As in the case of positional scoring rules, to establish these negative results,  we start with the following lemma, which establishes the conditions under which a set of $m$ \(t\)-indistinguishable profiles, as described in~\Cref{lem:random-alg}, can be constructed. Again, this lemma builds upon Lemma 4.1 of \citet{halpern2024computing}, employing the same method for constructing the set of distinct preference profiles. However, the condition that the initial preference profile must satisfy and the arguments used in the proof are significantly different.

\begin{lemma} \label{lem:main-condorcet}
Suppose there exist \(a, b \in M\) and a preference profile \(\dist\) such that \(\dist\) and \(\swapdist\) are \(t\)-indistinguishable, and it holds that
\[
\sum_{x \in M \setminus \{a\}} \Pr_{\sigma \sim D }[a \succ_\sigma x] \neq \sum_{x \in M \setminus \{b\}} \Pr_{\sigma \sim D }[b \succ_\sigma x].
\]
Then, there exists a family of preference profiles \(\{D^c\}_{c \in M}\) such that all preference profiles in the family are \(t\)-indistinguishable from one another, and each preference profile \(D^c\) has \(c\) as the Condorcet winner.
\end{lemma}

\begin{proof} We use the same construction as in Lemma 4.1 by \citet{halpern2024computing}, which we briefly restate here for completeness. For each $c \in M$ we define $\dist^c$ as following: we pick a permutation $\pi$ uniformly at random. If $ \pi(b) = c$ then we sample a ranking $\sigma \sim \pi \circ \swapdist$. If $ \pi(b) \neq c $ then we sample a ranking $\sigma \sim \pi \circ \dist$. We need to show that the two following statements are true: (1) $ Pr_{\sigma \sim \dist^c}[c \succ_\sigma x] > Pr_{\sigma \sim \dist^c}[x \succ_\sigma c]$, for all  $x \in  M \setminus \{c\}$ and (2) $\{\dist^c\}_{c \in M}$ consists of $m$ t-indistinguishable preference profiles. Firstly, we turn our attention to (1).



Fix any $x \in M \setminus \{c\}$.  
First note that,
\begin{align}
   &\sum_{z \in \{c,x\}} \Pr_{\sigma \sim \dist^c}[c \succ_\sigma x \mid \pi(b) = z] \cdot Pr[\pi(b) = z] \nonumber\\
   &= \frac{1}{m} \cdot \left( Pr_{\sigma \sim D}[a \succ_\sigma \pi^{ab}(\pi^{-1}(x))\mid \pi(b) = z] 
      + Pr_{\sigma \sim  D}[\pi^{-1}(c) \succ_\sigma b\mid \pi(b) = z] \right) \nonumber\\
   &= \frac{1}{m} \Bigg( 
       \sum_{y \in \Sab} Pr_{\sigma \sim D}[a \succ_\sigma y \mid \pi^{-1}(x) = y, \pi(b) = z] \cdot Pr[\pi^{-1}(x) = y\mid \pi(b) = z]  \nonumber\\
       &  \quad \quad + Pr_{\sigma \sim D}[a \succ_\sigma b \mid \pi^{-1}(x) = a,\pi(b) = z] \cdot Pr[\pi^{-1}(x) = a\mid \pi(b) = z] \nonumber\\
       &  \quad \quad  + \sum_{y \in \Sab} Pr_{\sigma \sim D}[y \succ_\sigma b \mid \pi^{-1}(c) = y, \pi(b) = z] \cdot Pr[\pi^{-1}(c) = y\mid \pi(b) = z] \nonumber \\
   & \quad \quad  
       + Pr_{\sigma \sim D}[a \succ_\sigma b \mid \pi^{-1}(c) = a, \pi(b) = z] \cdot Pr[\pi^{-1}(c) = a\mid \pi(b) = z] \Bigg) \nonumber\\
   &= \frac{1}{m\cdot (m-1)} \left( \sum_{x \in M \setminus \{a\}} Pr_{\sigma \sim \dist}[a \succ_\sigma x] 
       + \sum_{x \in M \setminus \{b\}} Pr_{\sigma \sim \dist}[x \succ_\sigma b] \right)
      \nonumber \\
   &= \frac{1}{m\cdot (m-1)} \left( \sum_{x \in M \setminus \{a\}} Pr_{\sigma \sim \dist}[a \succ_\sigma x] 
       + \sum_{x \in M \setminus \{b\}} \left( 1- Pr_{\sigma \sim \dist}[b \succ_\sigma x] \right) \right), \label{eq:condorcet-main-1} 
\end{align}
where the second equality holds, since $\pi$ is a permutation selected uniformly at random and  conditioned on $\pi(b) = c$, $D^c = \pi \circ \swapdist$ and conditioned on $\pi(b) = x$,  $D^c = \pi \circ \dist$. The second last equality holds again from the fact that $\pi$ is a permutation selected uniformly at random and that  $\dist$ is independent of $\pi$. 



By the exact same reasoning we can also argue that,
\begin{align}
   &\sum_{z \in \{c,x\}} \Pr_{\sigma \sim \dist^c}[x \succ_\sigma c \mid \pi(b) = z] \cdot Pr[\pi(b) = z] \nonumber  \\
   &=\quad \quad \frac{1}{m\cdot (m-1)} \left( \sum_{x \in M \setminus \{a\}} Pr_{\sigma \sim \dist}[x \succ_\sigma a] + \sum_{x \in M \setminus \{b\}} Pr_{\sigma \sim \dist}[b \succ_\sigma x] \,\, \right) \nonumber \\
   &=\quad\quad\frac{1}{m\cdot (m-1)} \left( \sum_{x \in M \setminus \{a\}} \left(1- Pr_{\sigma \sim \dist}[a \succ_\sigma x] \right) + \sum_{x \in M \setminus \{b\}} Pr_{\sigma \sim \dist}[b \succ_\sigma x] \,\, \right).  \label{eq:condorcet-main-2} 
\end{align}
By combining \Cref{eq:condorcet-main-1} and  \Cref{eq:condorcet-main-2}, and the fact that $\sum_{x \in M \setminus \{a\}} Pr_{\sigma \sim \dist}[a \succ_\sigma x] > \sum_{x \in M \setminus \{b\}} Pr_{\sigma \sim \dist}[b \succ_\sigma x]$, we conclude that 
\begin{align}
\sum_{z \in \{c,x\}} \Pr_{\sigma \sim \dist^c}[c \succ_\sigma x \mid \pi(b) = z] \cdot Pr[\pi(b) = z]
      >\sum_{z \in \{c,x\}} \Pr_{\sigma \sim \dist^c}[x \succ_\sigma c \mid \pi(b) = z] \cdot Pr[\pi(b) = z] \label{eq:condorcet-main-3}.
\end{align}

Next, note that
\begin{align}
   &\sum_{z \in M \setminus \{c,x\}}Pr_{\sigma \sim D^c}[c \succ_\sigma x \mid \pi(b) = z] \cdot Pr[\pi(b) = z] \nonumber\\
    &\quad \quad = \sum_{z \in M \setminus \{c,x\}}Pr_{\sigma \sim D}[\pi^{-1}(c) \succ_\sigma \pi^{-1}(x)\mid \pi(b) = z] \cdot \frac{1}{m} \nonumber\\
    &\quad \quad = \frac{1}{m} \cdot \Bigg( 
        \sum_{y, y' \in M \setminus \{a\}} Pr_{\sigma \sim D}[y \succ_\sigma y'\mid \pi^{-1}(c) = y,  \pi^{-1}(x) = y', \pi(b) = z ] \nonumber \\
       &\quad \quad \quad \quad   \cdot Pr[\pi^{-1}(c) = y, \pi^{-1}(x) = y'\mid \pi(b) = z]  \nonumber\\
    &\quad \quad \quad + \sum_{y \in M \setminus \{a\}} Pr_{\sigma \sim D}[y \succ_\sigma a\mid \pi^{-1}(c) = y, \pi^{-1}(x) = a, \pi(b) = z ] \cdot Pr[\pi^{-1}(c) = y,  \pi^{-1}(x) = a\mid  \pi(b) = z ] \nonumber\\
    &\quad \quad \quad + \sum_{y \in M \setminus \{a\}} Pr_{\sigma \sim D}[a \succ_\sigma y\mid \pi^{-1}(c) = a, \pi^{-1}(x) = y, \pi(b) = z ] \cdot Pr[\pi^{-1}(c) = a, \pi^{-1}(x) = y \mid \pi(b) = z ]
    \Bigg) \nonumber\\
    &\quad \quad = \frac{1}{m (m-1) (m-2)} \cdot \Bigg( 
        \sum_{y, y' \in M \setminus \{a\}} Pr_{\sigma \sim D}[y \succ_\sigma y'] 
        + \sum_{y \in M \setminus \{a\}} Pr_{\sigma \sim D}[y \succ_\sigma a] 
        + \sum_{y \in M \setminus \{a\}} Pr_{\sigma \sim D}[a \succ_\sigma y] 
    \Bigg) \label{eq:condorcet-main-4}
\end{align}
where the second equality holds, since $\pi$ is a permutation selected uniformly at random and  conditioned on $\pi(b) \not \in  \{c,x\}$, $D^c = \pi \circ \dist$. The last equality holds again from the fact  $\pi$ is a permutation selected uniformly at random and that  $\dist$ is independent of $\pi$.





 By the exact same reasoning, we get that
\begin{align}
    &\sum_{z \in M \setminus \{c,x\}}Pr_{\sigma \sim D^c}[x \succ_\sigma  c\mid \pi(b) = z] \cdot Pr[\pi(b) = z] \nonumber \\
    & \quad\quad = \frac{1}{m(m-1)(m-2)} \cdot \left(\sum_{y,y' \in M \setminus\{a\}} Pr_{\sigma \sim D}[y \succ_\sigma y' ] 
    + \sum_{y \in M \setminus \{a\}} Pr_{\sigma \sim D}[a \succ_\sigma y ]
    + \sum_{y \in M \setminus \{a\}} Pr_{\sigma \sim D}[y \succ_\sigma a ] \right). \label{eq:condorcet-main-5}
\end{align}
From \Cref{eq:condorcet-main-4} and 
 \Cref{eq:condorcet-main-5}, we get
 \begin{align}
   &\sum_{z \in M \setminus \{c,x\}}Pr_{\sigma \sim D^c}[c \succ_\sigma x \mid \pi(b) = z] \cdot Pr[\pi(b) = z]= \sum_{z \in M \setminus \{c,x\}}Pr_{\sigma \sim D^c}[x \succ_\sigma c \mid \pi(b) = z] \cdot Pr[\pi(b) = z]. \label{eq:condorcet-main-6}
 \end{align}
Lastly, by combining \Cref{eq:condorcet-main-3} and  \Cref{eq:condorcet-main-6}, we get 
  $ Pr_{\sigma \sim \dist^c}[c \succ_\sigma x] > Pr_{\sigma \sim \dist^c}[x \succ_\sigma c]$ as desired. 

 
 

 As for (2), we already have shown in \Cref{lem:main-scoring-rules} that the family of $\{\dist^c\}_{c \in M}$ consists of preference profiles that are all $t$-indistinguishable from one another. Hence, the lemma follows.

 \end{proof}

 

\subsection{Uniform \texorpdfstring{$t-$}{t-}Improvement Feedback Distribution} \label{subsection:condorcet-uniform}
Here, we prove that there is a preference profile that satisfies the conditions of~\Cref{lem:main-condorcet}, when the algorithm has access to uniform $t$-improvement feedback queries, for $t\in [m-1]$.
\begin{lemma} \label{lem:condorcet-dist-uniform} 
For any $m\geq 13 $ and any $t\in [m-1]$, and pair of candidates $a, b \in M$,   there is a preference profile $\dist$ such that 
     \begin{align*}
        \sum_{x\in M\setminus \{a\}} \Pr_{\sigma \sim D } [a\succ_\sigma x]\neq    \sum_{x\in M\setminus \{b\}} \Pr_{\sigma \sim D } [b\succ_\sigma x],
     \end{align*}
and   $D$ and   $\dist^{a \leftrightarrow b}$ are $t$-indistinguishable under the uniform \(t\)-improvement feedback distribution.
\end{lemma}


Since in~\Cref{lem:condorcet-dist-uniform} we show the existence of a preference profile $\dist$ that is indistinguishable from $\swapdist$ and has the desired properties, we can apply~\Cref{lem:main-condorcet} for constructing a family of preference profiles that are $t$-indistinguishable from one another and each of them has a different candidate as the Condorcet winner. Then, by applying~\Cref{lem:random-alg} we get the following theorem. 


\begin{theorem}\label{thm:main-impossibility-condorcet-uniform}
For \(m \geq 13\) and any \(t \in [m-1]\), no randomized algorithm $A$ that has access to uniform \(t\)-improvement feedback queries outputs  the unique Condorcet winner with probability more than  \(\sfrac{1}{m}\).
\end{theorem}





\subsection{General \texorpdfstring{$t-$}{t-}Improvement Feedback Distribution}\label{subsection:condorcet-general}
Now, we extend the negative results  to \emph{every} $t$-improvement feedback distribution when $t \leq \sfrac{m}{2} - 2$, with the exception of the special case where $\sfrac{P_i^t}{P_{i+1}^t} = \sfrac{(m-i)}{(m-i-1)}$ for all $i \in \{2, \ldots, m-1\}$. In~\Cref{appendix:condorcet-deterministic}, we discuss  in detail why this result cannot be applied in this case.  We also show that for \(t \leq m - 4\), no deterministic rule can identify the Condorcet winner under \emph{any} \(t\)-improvement feedback queries. Thus, while the negative result does not extend to randomized algorithms for this specific \(t\)-improvement feedback distribution, it still holds for deterministic algorithms across all distributions and  all values of \(t\leq m-4\).


To show the desired result,  we use the following lemma. 
\begin{lemma} \label{lem:condorcet-dist-general} 
Unless $\sfrac{P_i^t}{P_{i+1}^t}= \sfrac{(m-i)}{(m-i-1)}$ for all $i\in \{2, \ldots, m-2\}$, then for any $m\geq 6 $ and $t\leq \sfrac{m}{2}-2$, and pair of candidates $a, b \in M$,   there is a preference profile $\dist$ such that 
     \begin{align*}
        \sum_{x\in M\setminus \{a\}} \Pr_{\sigma \sim D } [a\succ_\sigma x]\neq      \sum_{x\in M\setminus \{b\}} \Pr_{\sigma \sim D } [b\succ_\sigma x],
     \end{align*}
and   $D$ and   $\dist^{a \leftrightarrow b}$ are $t$-indistinguishable. 
\end{lemma}

\begin{proof}
    We start by considering the preference profile $\dist_{i,m,i+1}$ from~\Cref{lem:main-construction} for any $i\in \{2,\ldots, m-t-2\}$.   From~\Cref{lem:main-construction}, we get that $\dist_{i,m,i+1}$ and $\swapdist_{i,m,i+1}$ are $t$-indistinguishable when $p=\sfrac{P_{i+1}^t}{(P_i^t+P_{i+1}^t)}$. Now notice that
    \begin{align*}
        \sum_{x \in M \setminus \{a\}}Pr_{\sigma \sim D_{i,m,i+1}}[a\succ_\sigma x]=&
        \sum_{x \in M \setminus \{b,a\}}\sum_{r=i}^{m-2}Pr[ \tau^{\Sab} (r)=x]\cdot p + Pr_{\sigma \sim D_{i,m,i+1}}[a\succ_\sigma b]\\
     =& \sum_{x \in M \setminus \{b,a\}} \frac{m-2-i+1}{m-2} \cdot p + p = (m-i)\cdot p,
    \end{align*}
and   
    \begin{align*}
        \sum_{x \in M \setminus \{b\}}Pr_{\sigma \sim D_{i,m,i+1}}[b\succ_\sigma x]=&
        \sum_{x \in M \setminus \{b,a\}}\sum_{r=i+1}^{m-2}Pr[ \tau^{\Sab} (r)=x]\cdot (1-p) + Pr_{\sigma \sim D_{i,m,i+1}}[b\succ_\sigma a]\\
     =& \sum_{x \in M \setminus \{b,a\}} \frac{m-2-(i+1)+1}{m-2} \cdot (1-p) +(1-p) = (m-i-1)\cdot p.
    \end{align*}

If for some  $i\in\{2,\ldots, m-t-2\}$, we have that $(m-i)\cdot p\neq  (m-i-1)\cdot (1-p)$, since $\dist_{i,m,i+1}$ and $\swapdist_{i,m,i+1}$ are $t$-indistinguishable,  by utilizing \Cref{lem:main-condorcet}, we can construct a family of preference profiles $\{D^c\}_{c\in M}$ with $m$ distinct Condorcet winners that are $t$-indistinguishable. Then, by applying~\Cref{lem:random-alg}, we get the desired result. 
    

Now suppose that $(m-i)\cdot p= (m-i-1)\cdot(1-p)$ for all $i\in\{2,\ldots, m-t-2\}$.  Then,  since  $p=\sfrac{P_{i+1}^t}{(P_i^t+P_{i+1}^t)}$, we conclude that 

\begin{align}
   \frac{P^t_i}{P^t_{i+1}} = \frac{m-i}{m-i-1}, \quad \quad  \forall  i=\{2,\ldots, m-t-2\} \label{eq:cond-all-1}
\end{align}

 

Next, we consider the preference profile $\dist_{2,m-i,i+2}$ for any $i \in \{1,\ldots, t+1\}$.   Since $t\leq \sfrac{m}{2}-2$, , from~\Cref{lem:main-construction}, we get  that  $\dist_{2,m-i,i+2}$ and $\swapdist_{2,m-i,i+2}$ are  $t$-indistinguishable   when $p=\sfrac{P_{i+2}^t}{(P_2^t-P_{m-i}^t + P^t_{i+2})}$. Next, we see that      
\begin{align*}
        \sum_{x \in M \setminus \{a\}}Pr_{\sigma \sim \dist_{2,m-i,i+2}}[a\succ_\sigma x]=&
        \sum_{x \in M \setminus \{b,a\}}\sum_{r=2}^{m-2}Pr[ \tau^{\Sab} (r)=x]\cdot p + Pr_{\sigma \sim \dist_{2,m-i,i+2}}[a\succ_\sigma b]\\
     =& \sum_{x \in M \setminus \{b,a\}} \frac{m-2-2+1}{m-2} \cdot p + p = (m-2)\cdot p,
\end{align*}
and   
\begin{align*}
   & \sum_{x \in M \setminus \{b\}}Pr_{\sigma \sim\dist_{2,m-i,i+2}}[b\succ_\sigma x]
   \\&=
    \sum_{x \in M \setminus \{b,a\}}\sum_{r=m-i-1}^{m-2}Pr[ \tau^{\Sab} (r)=x]\cdot p \\
  & \quad \quad + \sum_{x \in M \setminus \{b,a\}}\sum_{r=i+2}^{m-2}Pr[ \tau^{\Sab} (r)=x]\cdot (1-p) + Pr_{\sigma \sim \dist_{2,m-i,i+2}}[b\succ_\sigma a]\\
  &  =\sum_{x \in M \setminus \{b,a\}} \frac{m-2-(m-i-1)+1}{m-2} \cdot p + \sum_{x \in M \setminus \{b,a\}} \frac{m-2-(i+2)+1}{m-2} \cdot (1-p) + (1-p)  \\
   & =  i \cdot p + (m-i-2) \cdot (1-p).
\end{align*}



As before, if for some   $i\in\{1,\ldots, t+1\}$, we have that $(m-2)\cdot p \neq  i \cdot p + (m-i-2) \cdot (1-p)$,   by utilizing \Cref{lem:main-condorcet}, we can construct a family of preference profiles $\{D^c\}_{c\in M}$ with $m$ distinct Condorcet winners that are $t$-indistinguishable and  by applying~\Cref{lem:random-alg}, we get the desired result. 

Now suppose that $(m-2)\cdot p=  i \cdot p + (m-i-2) \cdot (1-p)$ for all $i \in \{1,\ldots, t+1\}$. This means that $p=\sfrac{1}{2}$, and since $p=\sfrac{P_{i+2}^t}{(P_2^t-P_{m-i}^t + P^t_{i+2})}$, we get that $P^t_{m-i}= P^t_2-P_{i+2}$. Since $t\leq m/2-2$ and $\sfrac{P_i^t}{P_{i+1}^t}= \sfrac{(m-i)}{(m-i-1)}$ for all $i \in\{2,\ldots, m-t-2\}$, we conclude that 
\begin{align}
   P^t_{m-i}= P^t_2-\frac{m-(i+2)}{m-2} P^t_2= \frac{i}{m-2} P^t_2, \quad \quad \forall i \in \{1,\ldots, t+1\} \label{eq:cond-all-2} 
\end{align}

Then, from \Cref{eq:cond-all-1} and \Cref{eq:cond-all-2}, we get that the only case that we cannot construct two profiles with the desired property is when $\sfrac{P_i^t}{P_{i+1}^t}= \sfrac{(m-i)}{(m-i-1)}$ for all $i\in \{2, \ldots, m-2\}$, and this concludes the lemma.    
\end{proof}

 
As before, by combining~\Cref{lem:condorcet-dist-general}, ~\Cref{lem:main-condorcet} and~\Cref{lem:random-alg}, we get the following theorem. 


\begin{theorem}\label{thm:main-impossibility-condorcet-general}
For \(m \geq 6\) and \(t \leq \sfrac{m}{2} - 2\), unless the \(t\)-improvement feedback distribution satisfies \(\sfrac{P_i^t}{P_{i+1}^t} = \sfrac{(m-i)}{(m-i-1)}\) for all \(i \in \{2, \ldots, m-1\}\), no randomized algorithm \(A\) with access to \(t\)-improvement feedback queries can identify the unique Condorcet winner with a probability exceeding \(\sfrac{1}{m}\).
\end{theorem}

\section{Experiments}\label{sec:experiments} 








\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - IC }
        \label{fig:subfig1}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_IC_borda.png}
    \end{subfigure}\hspace{0.02\textwidth}%    
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - PL Model}
        \label{fig:subfig2}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_plackett_borda.png}
    \end{subfigure}\hspace{0.02\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - Mallows Model}
        \label{fig:subfig3}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_mallows_phi=0.3_borda.png}
    \end{subfigure}

    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - IC }
        \label{fig:subfig4}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_IC_copeland.png}
    \end{subfigure}\hspace{0.02\textwidth}%    
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - PL Model}
        \label{fig:subfig5}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_plackett_copeland.png}
    \end{subfigure}\hspace{0.02\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - Mallows Model}
        \label{fig:subfig6}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_mallows_phi=0.3_copeland.png}
    \end{subfigure}

    \caption{Approximation ratio of the optimal Borda score (top) and the optimal Copeland score (bottom) for different numbers of agents and various underlying ranking distributions, with \( t = 5 \) under the uniform improvement feedback distribution.}
    \label{fig:main-text}
\end{figure*}




In the previous sections, we show that improvement feedback queries are not sufficient in the worst case to identify the winner under most positional scoring rules and any Condorcet-consistent rule. In this section, we empirically evaluate the effectiveness of \(t\)-improvement feedback queries compared to pairwise comparison queries  over one positional scoring rule, namely Borda count and one Condorcet consistent rule, namely Copeland’s rule.  Copeland’s rule calculates the number of candidates each candidate defeats in pairwise comparisons and returns the candidate with the most wins. For both rules access to the weighted majority graph suffices for determining the winner.

We evaluate the performance of the feedback mechanisms under three different distributions of rankings. The first is the impartial culture (IC), where every ranking in \(L(M)\) is equally likely to appear, providing a uniform distribution over rankings. The second distribution is derived from the Mallows model, which is parameterized by a central ranking \(\sigma^*\) and a noise parameter \(\phi \in [0,1]\). The probability of a ranking \(\sigma\) is proportional to \(\phi^{d(\sigma, \sigma^*)}\), where \(d(\sigma, \sigma^*)\) is the Kendall tau distance between \(\sigma\) and \(\sigma^*\). As \(\phi\) approaches 0, the distribution concentrates most of the probability mass on \(\sigma^*\), while as \(\phi\) approaches 1, the distribution converges to the impartial culture. In our experiments, we set $\phi=1/3$. The third distribution is the Plackett-Luce (PL) model, which generates rankings through a sequential selection process where each candidate \(a \in M\) is assigned a utility \(u_a > 0\). The probability of selecting a candidate at any step is proportional to its utility. For our experiments, we assign utilities uniformly at random from the interval \([0, 1]\).


We consider three different \(t\)-improvement feedback distributions: (1) \textit{Uniform distribution}: each candidate in the \(t\)-above neighborhood of the queried candidate is returned with equal probability, (2) \textit{Linear decay distribution}: the probability of returning a candidate in the \(t\)-above neighborhood  decreases linearly with the distance from the queried candidate and (3) \textit{Exponential decay distribution}: the probability of returning a candidate in the \(t\)-above neighborhood  decreases exponentially with the distance from the queried candidate.   


We construct the weighted majority graph using the two types of feedback by making one random query per user. Specifically, for the \(t\)-improvement feedback queries, upon randomly querying a candidate \(a\), if the agent returns an alternative \(b\) from \(a\)’s \(t\)-above neighborhood, we update our pairwise comparison estimates by adding the pair \((b, a)\). If the agent returns the queried candidate \(a\) (indicating that it is their top choice), we update our estimates by adding all pairs \((a, c)\) for each \(c \in M \setminus \{a\}\). For pairwise comparison queries, we randomly sample a pair of candidates \((a, b)\) and ask the agent to compare them, directly updating based on the comparison outcome.
Using this information, we calculate the Borda and Copeland winners. We then compare the score of the Borda (resp. Copeland) winner that is returned under the two different kinds of feedback with the optimal Borda (resp. Copeland) score.

For all the experiments,  we set the number of candidates \(m = 20\) and vary the number of agents from \(50\) to \(1000\) in increments of $50$. Here, we illustrate the results with respect to the uniform improvement feedback distribution, and in~\Cref{appendix:experiments}, we surprisingly show that all the three improvement feedback distributions  behave identically.  We also set \(t = 5\), and in~\Cref{appendix:experiments}, we show that the results are quantitatively similar for different values of \(t\). For each experiment,  we iterate over 500 iterations.


In~\Cref{fig:main-text}, we plot the average approximation ratio along with standard deviations. We notice that under the IC model, the two types of feedback—$t$-improvement feedback queries and pairwise comparison queries—behave identically. We believe this is because rankings in IC are sampled uniformly at random, meaning there is no underlying structure or consistent patterns for either feedback mechanism to exploit.  On the other hand, under the PL model, pairwise comparison queries provide a better approximation to the optimal score than $t$-improvement feedback queries, while under the Mallows model, the opposite is true. A positive explanation for these phenomena lies in the differences in how the models generate rankings. In the Mallows model, as the  parameter $\phi$ decreases, the rankings converge more closely to the underlying ranking, making it more likely that the Borda and Copeland winners coincide with the plurality winner. Since $t$-improvement feedback is highly effective at identifying the top alternative—each time we query  candidate $a$ and receive $a$ as a response (indicating it is the top choice) we directly increase $a$’s score relative to all other candidates—it excels in this setting. On the other hand, under the PL model, utilities for candidates are drawn uniformly at random, which implies that the relative probabilities of candidates being ranked higher or lower can exhibit more variance compared to the Mallows model, especially when the PL parameters are widely spread out. As a result, the probability that two candidates are frequently ranked closely to each other (or even swapped in rank) is higher under the PL model. This increased variance in relative rankings benefits pairwise comparison queries because they directly compare the probabilities of individual pairs and can more effectively distinguish small differences in candidates’ utilities.  In summary, it seems that the effectiveness of each  type of feedback  is inherently tied to the structure of the underlying preference profile. 



\section{Discussion}
An  immediate question that is left open is whether our negative results extend to all values of \(t\) under general \(t\)-improvement feedback distributions. Additionally, it would be interesting to explore how the results change when agents are characterized by different \(t\) values, reflecting varying levels of effort in providing feedback. Another promising direction involves hybrid feedback mechanisms that combine \(t\)-improvement feedback with other methods, such as pairwise comparisons or partial rankings, potentially overcoming the limitations highlighted in our work by leveraging the complementary strengths of different feedback types.

\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix






%\section{Proof of~\Cref{lem:positional-dist-%uniform} }
\section{Proof of~\texorpdfstring{\Cref{lem:positional-dist-uniform}}{Proof of Lemma}}
\begin{proof}
    Let $a,b$ be any two candidates and $\vec{s} = (s_1, \dots, s_m)$ be any positional scoring rule. 
    
    
    We will first define an additional family of preference profiles, denoted $\hatdist_i$, which will be used to prove impossibility results for the uniform $t$-improvement distribution. 

\begin{lemma} \label{lem:main-construction-uniform}
Let $\hatdist_i$ be preference profiles that are defined with respect to two candidates $a$ and $b$, as follows:
\begin{enumerate}
    \item With probability \( p \), candidate \( a \) is fixed at position \( i \), and candidate \( b \) is fixed at position \( i+1 \), where  $max(2, m-t) \leq i < m-1$.
    \item With probability \( 1-p \), candidate \( a \) is fixed at position \( m \), and candidate \( b \) is fixed at position \( m-1 \).
    \item Select a uniformly random ranking \( \tau^{\Sab} \) over the set of all remaining candidates (excluding \( a \) and \( b \)). This ranking determines the relative order of the remaining candidates, which are then placed in the positions not occupied by \( a \) or \( b \).
\end{enumerate}
For any $t \in [m-1]$,  in the uniform  $t$-improvement feedback distribution, the preference profiles $\hatdist_i$ and $\hatswapdist_i$ are $t$-indistinguishable, when  $p=\frac{min(i,t)}{t + min(i,t)}$.
\end{lemma}

\begin{proof} 
Let $a,b$ be the candidates of the statement.



Firstly, we need to show that \(\Pr_{\sigma \sim \hatdist_i}[q_\sigma^t(a) = b] = \Pr_{\sigma \sim \hatdist_i}[q_\sigma^t(b) = a]\). Under the uniform \(t\)-improvement feedback distribution, with probability \(p\), candidate \(a\) appears immediately above candidate \(b\) in the ranking. In this case, candidate \(b\) returns candidate \(a\) with uniform probability over the  \(t\)-above neighborhood positions, which is \(\frac{1}{\min(i, t)}\). Similarly, with probability \(1-p\), candidate \(b\) appears immediately above candidate \(a\) in the ranking. In this case, candidate \(a\) returns candidate \(b\) over the  \(t\)-above neighborhood positions,  which is \(\frac{1}{t}\). By construction of \(\hatdist_i\), these two probabilities are equal for the choice of \(p = \sfrac{\min(i, t)}{(t + \min(i, t))}\). 

Now, notice that when querying a candidate $x\in \Sab$, the  agent can return $a$ or $b$ only if $x$ is ranked at some $j>i+1$ when $a$ is at position $i$ and $b$ at position $i+1$.
Since \(i \geq \max(2, m-t)\), under the uniform \(t\)-improvement feedback distribution, querying any candidate \(x \in \Sab\) ranked at position \(j > i + 1\) results in the agent returning  \(a\) or \(b\) with equal probability, which is \(\sfrac{1}{\min(j-1, t)}\). Hence, 
$\Pr_{\sigma \sim \hatdist_i}[q_\sigma^t(x) = a] = \Pr_{\sigma \sim \hatdist_i}[q_\sigma^t(x) = b]$.






Moreover,
\begin{align*}
    Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(b) = x] 
    &= p\cdot  \sum_{k=i-min(i,t) + 1}^{i-1}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(b) = x \mid \tau^{\Sab} (k)=x] \\
    &+ (1-p) \cdot \sum_{k = m-t-1}^{m-2} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(b) = x \mid \tau^{\Sab}(k) = x] \\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k=i-min(i,t) + 1}^{i-1} Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(b) = x \mid \tau^{\Sab} (k)=x] \\
    &+ (1-p) \cdot \frac{1}{m-2} \cdot \sum_{k = m-t-1}^{m-2} Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(b) = x \mid \tau^{\Sab}(k) = x] \\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k=i-min(i,t) + 1}^{i-1} p^t_{i+1,k} + (1-p) \cdot \frac{1}{m-2} \cdot \sum_{k = m-t-1}^{m-2} p^t_{m-1,k} \\
     &= p \cdot \frac{1}{m-2} \cdot\frac{min(i,t) - 1}{min(i,t)}  + (1-p) \cdot \frac{1}{m-2}  \\
\end{align*}
where the second equality holds because $\tau^{\Sab}$ is chosen uniformly at random and the 
last equality holds since each agent returns every candidate in the $t$-above neighborhood with the same probability. 

Similarly, 
\begin{align*}
Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(a) = x] 
    &= p \cdot \sum_{k=i-min(i-1,t)}^{i-1}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(a) = x \mid \tau^{\Sab} (k)=x] \\
    &+ (1-p) \cdot \sum_{k = m-t}^{m-2} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(a) = x \mid \tau^{\Sab}(k) = x]\\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k=i-min(i-1,t)}^{i-1}Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(a) = x \mid \tau^{\Sab} (k)=x] \\
    &+ (1-p) \cdot \frac{1}{m-2}  \cdot \sum_{k = m-t}^{m-2}Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(a) = x \mid \tau^{\Sab}(k) = x] \\
    &= p \cdot \frac{1}{m-2} \cdot \sum_{k=i-min(i-1,t)}^{i-1}p^t_{i,k} + (1-p) \cdot \frac{1}{m-2} \cdot \sum_{k = m-t}^{m-2} p^t_{m,k} \\
     &= p \cdot \frac{1}{m-2}  + (1-p) \cdot \frac{1}{m-2} \cdot \frac{t-1}{t}.  \\
\end{align*}
Therefore, we have $Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(a) = x]=Pr_{\sigma \sim \hatdist_i}[q_{\sigma}^t(b) = x]$,  for the choice of \(p = \sfrac{\min(i, t)}{(t + \min(i, t))}\).  

 Lastly, for any $x,y \in \Sab$ it holds that $Pr_{\sigma \sim \hatdist_i}[q_\sigma^t(x) = y] = Pr_{\sigma \sim \hatswapdist_i}[q_\sigma^t(x) = y]$, since the two preference profiles differ only with respect to $a,b$. From all the above, we get that  $\hatdist_{i}$ and $\hatswapdist_{i}$ are $t$-indistinguishable by setting $p$ as stated in the lemma.
\end{proof}

We will continue with the following three lemmas. 



\begin{lemma}\label{lem:positional-dist-uniform-auxiliary-1}  
If \( s_i / P^t_i = \lambda \) does not hold for all \( i \in \{2, \ldots, m - t - 1\} \) for some \( \lambda \), then there exists a preference profile \( \dist \) such that \( sc_{\vec{s}}(a, \dist) > sc_{\vec{s}}(b, \dist) \), and \( \dist \) and \( \swapdist \) are \( t \)-indistinguishable, for every $t\in [m-4]$.  
\end{lemma}
\begin{proof}
    Consider the preference profile  $\dist_{i,m,i+1}$ as stated in~\Cref{lem:main-construction}. From~\Cref{lem:main-construction}, we get that for every $i \in \{2, \dots, m-t-2\}$, $\dist_{i,m,i+1}$ and $\swapdist_{i,m,i+1}$ are $t$-indistinguishable for every value of $t\leq m-4$, if  $p = \frac{\sunif_{i+1}}{\sunif_i - \sunif_m + \sunif_{i+1}} = \frac{\sunif_{i+1}}{\sunif_i + \sunif_{i+1}}$. If  it holds that $sc_{\vec{s}} (a, \dist_{i,m,i+1})> sc_{\vec{s}}(b, \dist_{i,m,i+1})$, for some  $i \in \{2, \dots, m-t-2\}$,  then by setting $\dist=\dist_{i,m,i+1}$, the statement of the lemma is satisfied. Now, suppose that for every  
    $i \in \{2, \dots, m-t-2\}$, it holds that $sc_{\vec{s}} (a, \dist_{i,m,i+1})= sc_{s}(b, \dist_{i,m,i+1})$. This means that
$$ p \cdot s_i = (1-p) \cdot s_{i+1} \implies p = \frac{s_{i+1}}{s_i + s_{i+1}}.$$
Therefore, we   get
$$p = \frac{\sunif_{i+1}}{\sunif_i + \sunif_{i+1}} = \frac{s_{i+1}}{s_i + s_{i+1}} \implies \frac{s_{i+1}}{\sunif_{i+1}} = \frac{s_i}{\sunif_i}=\lambda  $$ 
 and as a result: $\sfrac{s_i}{\sunif_i} = \lambda$ for all $i \in \{2, \dots, m-t-1\}$  and the lemma follows.
\end{proof}



\begin{lemma}\label{lem:positional-dist-uniform-auxiliary-2}  
If \( s_i / P^t_i = \mu \) does not hold for all \( i \in \{\max(2, m - t), \ldots, m - 1\} \) for some \( \mu \), then there exists a preference profile \( \dist \) such that \( sc_{\vec{s}}(a, \dist) > sc_{\vec{s}}(b, \dist) \), and \( \dist \) and \( \swapdist \) are \( t \)-indistinguishable, for every $t\in [m-1]$.  .  
\end{lemma}
\begin{proof}
     Consider the preference profile  $\hatdist_{i}$ as stated in~\Cref{lem:main-construction-uniform}. From~\Cref{lem:main-construction-uniform}, we get that for {\em every} $i \in \{ max(2, m-t), \dots, m-2\}$, $\hatdist_{i}$ and $\hatswapdist_{i}$ are $t$-indistinguishable for every value of $t$, if  $p = \frac{min(i,t)}{t + min(i,t)}$.   Under uniform $t$-improvement feedback distribution,  we know that $\sunif_{m-1} = \frac{1}{t}$ and $\sunif_i - \sunif_{i+1} = \frac{1}{min(i,t)}$, for all $i \in \{max(2,m-t), \cdots, m-2\}$. Hence, we have
    \[  \frac{\sunif_{m-1}}{\sunif_i - \sunif_{i+1} + \sunif_{m-1}} = \frac{\sfrac{1}{t}}{\sfrac{1}{t} + \sfrac{1}{min(i,t)}} = \frac{min(i,t)}{t + min(i,t)}=p.\]
     If for some  $i \in \{ max(2, m-t), \dots, m-2\}$, $sc_{\vec{s}}, (a, \hatdist_{i})> sc_{\vec{s}}(b, \hatdist_{i})$,  then by setting $D=\hatdist_i$, the statement of the lemma is satisfied. Now, suppose that for every 
    $i \in \{ max(2, m-t), \dots, m-2\}$, $sc_{\vec{s}} (a, \dist_{i,m,i+1})= sc_{\vec{s}}(b, \dist_{i,m,i+1})$. This means that
\[ p \cdot s_i  = p \cdot s_{i+1} + (1-p) \cdot s_{m-1} \implies
    p = \frac{s_{m-1}}{s_i - s_{i+1} + s_{m-1}}.\]
Therefore, we   get
$$p = \frac{\sunif_{m-1}}{\sunif_i - \sunif_{i+1} + \sunif_{m-1}} = \frac{s_{m-1}}{s_i - s_{i+1} + s_{m-1}} \implies\frac{s_i - s_{i+1}}{s_{m-1}} = \frac{ \sunif_i- \sunif_{i+1} }{\sunif_{m-1}}. $$ 
Now, observe that, substituting $i = m-2$ in the above,  we get $\frac{s_{m-2} - s_{m-1}}{s_{m-1}} = \frac{ \sunif_{m-2}- \sunif_{m-1} }{\sunif_{m-1}}$ which implies that $\frac{s_{m-2}}{\sunif_{m-2}} = \frac{s_{m-1}}{\sunif_{m-1}}$. By induction on $i$, we get for all $ i \in \{ max(2, m-t), \dots, m-1\}$, that $ \mu = \frac{s_i}{\sunif_i}$ and the lemma follows. 
\end{proof}



\begin{lemma}\label{lem:positional-dist-uniform-auxiliary-3}  
If \( s_i / P^t_i = \lambda \) for all \( i \in \{2, \ldots, m - t - 1\} \) for some \( \lambda \) and \( s_i / P^t_i = \mu \) for all \( i \in \{\max(2, m - t), \ldots, m-1\} \) for some \( \mu \), and \( \mu \neq \lambda \), then there exists a preference profile \( \dist \) such that \( sc_{\vec{s}}(a, \dist) > sc_{\vec{s}}(b, \dist) \), and \( \dist \) and \( \swapdist \) are \( t \)-indistinguishable.  
\end{lemma}

\begin{proof}
    
First, we note that when $t=m-1$ or $t=m-2$, from the hypothesis of the lemma we directly get that $s_2^t/P^t_2=\ldots=s_{m-1}^t/P^t_{m-1}=\mu$. Therefore for these two cases the lemma directly follows.  Next, we focus on the case where $t\leq m-3$.  



%Now, we need to prove that $\mu = \lambda$. To do that we distinguish into two cases. For $t \in \{m-2, m-1\}$ it already holds by \Cref{cor:larget} that $\frac{s_2}{\sunif_2} = \dots \frac{s_{m-1}}{\sunif_{m-1}}$. 
    
%     \textbf{Case I: $t < m-3$. }\textcolor{red}{Do we really need this case? I think the next case covers both} Let $\dist_{2,m-1,3}$ be the preference profile from~\Cref{lem:condorcet-dist-general}. Since $t<m-2$, we get that $\dist_{2,m-1,3}$ and $\swapdist_{2,m-1,3}$ are $t$-indistinguishable when  $p = \frac{\sunif_3}{\sunif_2 + \sunif_3 - \sunif_{m-1}}$. If  it holds that $sc_{\vec{s}} (a, \dist_{2,m-1,3})> sc_{\vec{s}}(b, \dist_{2,m-1,3})$,   then by setting $\dist=\dist_{i,m,i+1}$, the statement of the lemma is satisfied. Now, suppose that $sc_{\vec{s}} (a, \dist_{2,m-1,3})> sc_{\vec{s}}(b, \dist_{2,m-1,3})$ This means that
%     \begin{align*}
%          p \cdot s_2  = (1-p) \cdot s_3 + p \cdot s_{m-1}
%         \implies p = \frac{s_3}{s_2 + s_3 - s_{m-1}}=\frac{\sunif_3}{\sunif_2 + \sunif_3 - \sunif_{m-1}}.
%     \end{align*}
% By the statement of the lemma, since $t<m-3$, we have that $\sfrac{s_2}{P_2^t}=\sfrac{s_3}{P_3^t}=\lambda$ and $\sfrac{s_{m-1}}{P_{m-1}^t}=\lambda$. Then, from the above equality we get $\mu=\lambda$ and the lemma follows. 



%\textbf{Case II: $t = m-3$.} 

Here we consider the following preference profile, denoted by  $\dist$: 
\begin{enumerate}

    \item With probability \( p \), candidate \( a \) is fixed at position \( 2 \), and candidate \( b \) is fixed at position \( m \).
    \item With probability \( \sfrac{(1-p)}{2} \), candidate \( a \) is fixed at position \( 1 \), and candidate \( b \) is fixed at position \( m-1 \).
     \item With probability \( \sfrac{(1-p)}{2} \), candidate \( a \) is fixed at position \( m \), and candidate \( b \) is fixed at position \( 1 \).
    \item Select a uniformly random ranking \( \tau^{\Sab} \) over the set of all remaining candidates (excluding \( a \) and \( b \)). This ranking determines the relative order of the remaining candidates, which are then placed in the positions not occupied by \( a \) or \( b \).


\end{enumerate}
First, we will show that $\dist$ and $\swapdist$ are $t$-indistinguishable. 
Since in all the cases $a,b$ are more than $m-3$ positions apart, it holds that $Pr_{\sigma \sim \dist}[q_{\sigma}^t(b) = a ] = Pr_{\sigma \sim \dist}[q_{\sigma}^t(a) = b ] = 0 $. Furthermore, $Pr_{\sigma \sim \dist}[q_{\sigma}^t(b) = b] = Pr_{\sigma \sim \dist}[q_{\sigma}^t(a) = a] = \frac{1-p}{2}$. Next we  note that  for any $x \in \Sab$, it holds that
\begin{align*}
    Pr_{\sigma \sim D}[q_\sigma^t(a) = x] &= 
    p \cdot Pr[\tau^{\Sab}(1) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid\tau^{\Sab}(1) = x]\\ 
    &+ \frac{(1-p)}{2} \cdot \sum_{k = m-t-1}^{m-2} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid\tau^{\Sab}(k) = x] \\
    & = p \cdot \frac{1}{m-2} \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid\tau^{\Sab}(1) = x]\\ 
    &+ \frac{(1-p)}{2} \cdot \frac{1}{m-2} \sum_{k = m-t-1}^{m-2} Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid\tau^{\Sab}(k) = x] \\
    &= p \cdot p_{2,1}^t \cdot \frac{1}{m-2} + \frac{(1-p)}{2} \cdot \frac{1}{m-2} \cdot \sum_{k=m-t-1}^{m-2} p^t_{m,k+1}\\
    &= p \cdot \frac{1}{m-2} + \frac{(1-p)}{2(m-2)} = \frac{p+1}{2(m-2)}
\end{align*}
where the second equality comes from the fact that $\tau^{\Sab}$ is picked uniformly at random and the third equality by the fact that each agent returns a candidate from the $t$-above neighborhood. 


Similarly, for any $x \in \Sab$, it holds that
\begin{align*}
    Pr_{\sigma \sim D}[q_\sigma^t(b) = x] &= 
    p \cdot \sum_{k = m-t-1}^{m-2} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid\tau^{\Sab}(k) = x]\\ 
    &+ \frac{(1-p)}{2} \cdot \sum_{k = m-t-2}^{m-3} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}Pr[q_\sigma^t(b) = x \mid\tau^{\Sab}(k) = x] \\
    & = p \cdot \frac{1}{m-2} \cdot \sum_{k = m-t-1}^{m-2} Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid\tau^{\Sab}(k) = x]\\ 
    &+ \frac{(1-p)}{2} \cdot \frac{1}{m-2} \sum_{k = m-t-2}^{m-3} Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid\tau^{\Sab}(k) = x] \\
    & = p \cdot \frac{1}{m-2} \cdot \sum_{k = m-t-1}^{m-2} p^t_{m,k+1}
    + \frac{(1-p)}{2} \cdot \frac{1}{m-2} \sum_{k = m-t-2}^{m-3} p^t_{m-1,k+1} \\
    &= p \cdot \frac{1}{m-2} + \frac{(1-p)}{2(m-2)} = \frac{p+1}{2(m-2)}.
\end{align*}


Therefore, we have that $Pr_{\sigma \sim D}[q_\sigma^t(a) = x]=Pr_{\sigma \sim D}[q_\sigma^t(b) = x]$ for all $x\in \Sab$. 
Next, we see that
\begin{align*}
     Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a] &=
     p \cdot \sum_{k = 2}^{t+1}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim D}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
     &+ \frac{1-p}{2} \cdot \sum_{k=1}^{t}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim D}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x]\\
     &= p \cdot \frac{1}{m-2} \cdot \sum_{k = 2}^{t+1}\cdot Pr_{\sigma \sim D}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
     &+\frac{1-p}{2} \cdot \frac{1}{m-2} \cdot \sum_{k=1}^{t} \cdot Pr_{\sigma \sim D}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
     &= p \cdot \frac{1}{m-2} \sum_{k=2}^{t+1}p_{k+1,2}^t + \frac{1-p}{2} \cdot \frac{1}{m-2} \cdot \sum_{k=1}^{t} p_{k+1,1}^t \\
     & =p \cdot \frac{1}{m-2} \cdot \sunif_2 + \frac{1-p}{2} \cdot \frac{1}{m-2} \sunif_1 
\end{align*}
where the second equality comes from the fact that $\tau^{\Sab}$ is picked uniformly at random and the last equality by definition of $\sunif_i$. Similarly,
\begin{align*}
     Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b] &=
     \frac{1-p}{2} \cdot \sum_{k = 1}^{t}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim D}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
     &+ \frac{1-p}{2} \cdot Pr[\tau^{\Sab}(m-2) = x] \cdot Pr_{\sigma \sim D}[q_\sigma^t(x) = b \mid \tau^{\Sab}(m-2) = x]\\
     &= \frac{1-p}{2} \cdot \frac{1}{m-2} \cdot \sum_{k = 1}^{t}Pr_{\sigma \sim D}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
     &+\frac{1-p}{2} \cdot \frac{1}{m-2} Pr_{\sigma \sim D}[q_\sigma^t(x) = b \mid \tau^{\Sab}(m-2) = x] \\
     &= \frac{1-p}{2} \cdot \frac{1}{m-2} \sum_{k=1}^{t}p_{k+1,1}^t + \frac{1-p}{2} \cdot \frac{1}{m-2} \cdot p_{m,m-1}^t \\
     & =\frac{1-p}{2} \cdot \frac{1}{m-2} \cdot \sunif_1 + \frac{1-p}{2}\cdot \frac{1}{m-2} \sunif_{m-1}
\end{align*}
Therefore, we get  $Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a] = Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b]$ when $p=\sfrac{P_{m-1}^t}{(2P_2^t+P_{m-1}^t)}$ and then $\dist$ and $\swapdist$ become $t$-indistinguishable.

If  it holds that $sc_{\vec{s}} (a, \dist)\neq sc_{\vec{s}}(b, \dist)$,   then  the statement of the lemma is satisfied. Now, suppose that $sc_{\vec{s}} (a, \dist) =sc_{\vec{s}}(b, \dist)$.  This means that
    \begin{align*}
         p \cdot s_2 + \frac{1-p}{2} \cdot s_1  = \frac{1-p}{2} \cdot s_1 + \frac{1-p}{2} \cdot  s_{m-1}
        \implies p = \frac{s_{m-1}}{2\cdot s_2 + s_{m-1}}=\frac{\sunif_{m-1}}{2\cdot \sunif_2 + \sunif_{m-1}}.
    \end{align*}

For $t \leq m-4 $ we know that $ \sfrac{s_2}  {\sunif_2} = \lambda$ and $\sfrac{s_{m-1}} {\sunif_{m-1}} = \mu$. Then from the above equality we get $\mu = \lambda$. For $t = m-3$ we know that $ \sfrac{s_3}{\sunif_3} = \ldots = \sfrac{s_{m-1}}{\sunif_{m-1}} = \mu$ and the above implies that $ \sfrac{s_2}{\sunif_2} = \sfrac{s_3}{\sunif_3} = \ldots =  \sfrac{s_{m-1}}{\sunif_{m-1}} = \mu$, and the lemma follows.





%\textbf{Case III: $t > m-3$.}  When $t=m-1$, from the hypothesis of the lemma we get that $s_2^t/P^t_2=\ldots=s_m^t/P^t_m=\mu$. When $t=m-2$, from the hypothesis of the lemma we get that $s_2=P^2_t=\ell$ when $i=m-t-1$ and  $s_2/P^2_t=\ell$ and  $s_2/P^2_t=\mu$ and the lemma follows. 
\end{proof}

From the above lemma we get that, unless \(\frac{s_i}{P_i^t} = \lambda\) for all \(i \in \{2, \dots, m - 1\}\), which implies that \(\vec{s} \in \operatorname{span}(\vec{s}_{\text{plu}}, \vec{s}_{t}^*)\), there exists a preference profile \(\dist\) satisfying the conditions of the lemma and the lemma follows.

\end{proof}





%\section{Proof of~\Cref{lem:condorcet-dist-uniform}}
\section{Proof of ~\texorpdfstring{\Cref{lem:condorcet-dist-uniform}}{Proof of Lemma}}
\begin{proof}
We will distinguish into the following cases:

\paragraph{Case I: $2<t<m-3 $. }
Let \(\dist_{3,m,2}\) be the preference profile defined in \Cref{lem:main-construction}. Since \(2 < t < m - 3\), for \(p = \frac{\sunif_2}{\sunif_2 + \sunif_3}\), the preference profiles \(\dist_{3,m,2}\) and \(\swapdist_{3,m,2}\) are \(t\)-indistinguishable.

Under \(\dist_{3,m,2}\), we have
$\Pr_{\sigma \sim \dist_{3,m,2}}[a \succ_\sigma b] = p \quad$ and $\quad \Pr_{\sigma \sim \dist_{3,m,2}}[b \succ_\sigma a] = 1 - p$.
Additionally, for each \(x \in \Sab\), we have that
$\Pr_{\sigma \sim \dist_{3,m,2}}[a \succ_\sigma x] = p \cdot \frac{m - 4}{m - 2}$,
since with probability $p$, \(a\) appears at position $3$, followed by \(b\) at position \(m\), and the remaining \(m - 2\) candidates are uniformly distributed across the remaining positions and with probability $(1-p)$, $a$ appears at the last position. Similarly, we have
$\Pr_{\sigma \sim \dist_{3,m,2}}[b \succ_\sigma x] = (1 - p) \cdot \frac{m - 3}{m - 2}$,
since with probability $(1-p)$, \(b\) appears at position $2$, followed by \(a\) at position \(m\), and the remaining candidates are distributed uniformly, and with probability $p$, $b$ appears last.

Thus, we have that
\[
\sum_{x \in M \setminus \{a\}} \Pr_{\sigma \sim \dist}[a \succ_\sigma x] = \sum_{x \in \Sab} \Pr_{\sigma \sim \dist}[a \succ_\sigma x] + \Pr_{\sigma \sim \dist}[a \succ_\sigma b]=\sum_{x \in \Sab} \frac{m - 4}{m - 2} \cdot p + p = (m - 3) \cdot p,
\]
and similarly, for \(b\), we have:
\[
\sum_{x \in M \setminus \{a\}} \Pr_{\sigma \sim \dist}[b \succ_\sigma x] = \sum_{x \in \Sab} \Pr_{\sigma \sim \dist}[b \succ_\sigma x] + \Pr_{\sigma \sim \dist}[b \succ_\sigma a]=\sum_{x \in \Sab} \frac{m - 3}{m - 2} \cdot (1 - p) + (1 - p) = (m - 2) \cdot (1 - p).
\]


Next, we show that
\[
p = \frac{\sunif_2}{\sunif_2 + \sunif_3} = \frac{\sunif_2}{2\sunif_2 - \frac{1}{2} + \frac{1}{t}} > \frac{m - 2}{2m - 5},
\]
where the inequality follows from the fact that, under the uniform \(t\)-improvement feedback distribution, for \(t > 1\), we have that
$\sunif_3 = \sunif_2 - \frac{1}{2} + \frac{1}{t}$.
Then, this implies that$
\sum_{x \in M \setminus \{a\}} \Pr_{\sigma \sim \dist}[a \succ_\sigma x] > \sum_{x \in M \setminus \{a\}} \Pr_{\sigma \sim \dist}[b \succ_\sigma x]$,
as desired.

To ensure that \(p > \frac{m - 2}{2m - 5}\), we need:

\begin{align*}
\frac{\sunif_2}{2\sunif_2 - \frac{1}{2} + \frac{1}{t}} &> \frac{m-2}{2m-5} \\
\implies 2m \cdot \sunif_2 - 5 \sunif_2 &> (m-2)(2\sunif_2 - \sfrac{1}{2} + \sfrac{1}{t})\\ 
\implies 2m \cdot \sunif_2 - 5 \sunif_2 &> 2m \cdot \sunif_2 - \sfrac{m}{2} + \sfrac{m}{t} - 4\sunif_2 + 1 -\sfrac{2}{t}\\
\implies \sunif_2 &< \frac{m}{2} - \frac{m}{t} - 1 + \frac{2}{t} = (m-2)(\frac{1}{2} - \frac{1}{t})
\end{align*}


Under the uniform \(t\)-improvement feedback distribution, we have:
\[
P_2^t = \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{t - 1} + \frac{2}{t} = H_{t - 1} - 1 + \frac{2}{t},
\]
where \(H_k\) is the \(k\)-th harmonic number. Using the inequality \(H_k \leq \ln(k) + 1\), we get:
\[
P_2^t \leq \ln(t - 1) + \frac{2}{t}.
\]

Since, $\sunif_2  \leq \ln(t-1) + \frac{2}{t}$, it suffices to prove that: $\ln(t-1) + \frac{2}{t} < (m-2)(\frac{1}{2} - \frac{1}{t})$ or $\ln(t-1) + \frac{m}{t} < \frac{m-2}{2}$.

We prove that for any value of $m \geq 13$ and $3 \leq t < m-3$, this is true, as follows:

We have that $\ln(t-1) + \frac{m}{t} < \ln(t) + \frac{m}{t}$. Let $f(t) = \ln(t) + \frac{m}{t}$. Then $f'(t) = \sfrac{1}{t} - \sfrac{m}{t^2}$. Hence, $f'(m) = 0 $ and for every $0 < t < m$, $f'(t) < 0 $. Therefore, $f$ is monotone decreasing in $(0,m]$. As a result, $f(t) < f(3) = \ln(3) + \sfrac{m}{3}$. Now we need to find $m$ such that $\ln(3) + \frac{m}{3} < \frac{m-2}{2}$, or $m > 6 \cdot \ln(3) + 6$, or $m \geq 13$. This concludes the proof of the case.





\paragraph{Case II: $t \in \{1,2\} $. }
We define the preference profile $\dist$ as following:

\begin{enumerate}
    \item With probability \( p \), candidate \( a \) is fixed at position \( 2 \), and candidate \( b \) is fixed at position \( 5 \).
    \item With probability \( 1-p \), candidate \( a \) is fixed at position \( 6 \), and candidate \( b \) is fixed at position \( 3 \).
    \item Select a uniformly random ranking \( \tau^{\Sab} \) over the set of all remaining candidates (excluding \( a \) and \( b \)). This ranking determines the relative order of the remaining candidates, which are then placed in the positions not occupied by \( a \) or \( b \).
\end{enumerate}

First, we show that $\dist$ and $\swapdist$ are $t$-indistinguishable.  Note that since $t\le 2$, we get that $Pr_{\sigma \sim D}[q_\sigma^t(a) = b]= Pr_{\sigma \sim D}[q_\sigma^t(b) = a] =0$, since $a,b$ are more than $t$ positions apart. Furthermore, for each $x\in \Sab$, we have:

\begin{align*}
&Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x] 
\\&= p \cdot Pr[\tau^{\Sab}(1) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(1) = x]\\
&\quad \quad +(1-p) \cdot \sum_{k=5-t}^4 Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
&= p \cdot \frac{1}{m-2} \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(1) = x] + \frac{1-p}{m-2} \cdot \sum_{k=5-t}^4 Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
&= p \cdot \frac{1}{m-2} \cdot p_{2,1}^t + \frac{1-p}{m-2} \cdot \sum_{k=5-t}^{4}p_{6,k+1}^t \\
&= \frac{p}{m-2} + \frac{1-p}{m-2} = \frac{1}{m-2},
\end{align*}
where the first equality is due to the fact that $\tau^{\Sab}$ is sampled uniformly at random and the last equality is due to the fact that each agent returns a candidate in the $t$-above neighborhood of the suggested candidate. 

Similarly,
\begin{align*}
& Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x] \\
&= p \cdot \sum_{k=4-t}^{3}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x] \\
& \quad \quad + (1-p) \cdot \sum_{k=3-t}^{2} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x]\\
&=\frac{p}{m-2} \cdot \sum_{k=4-t}^{3}Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x] + \frac{1-p}{m-2} \cdot \sum_{k=3-t}^{2}Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x] \\
&=\frac{p}{m-2} \cdot \sum_{k=4-t}^{3}p_{5,k+1}^t + \frac{1-p}{m-2} \cdot \sum_{k=3-t}^{2}p^t_{3,k}\\
&= \frac{p}{m-2} + \frac{1-p}{m-2} = \frac{1}{m-2}.
\end{align*}

Furthermore,
\begin{align*}
&Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a] \\
&= p \cdot \sum_{k=2}^{1+t} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
&\quad \quad + (1-p) \cdot \sum_{k=5}^{min(m,4+t)} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
& = \frac{p}{m-2} \sum_{k=2}^{1+t}Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] + \frac{1-p}{m-2} \sum_{k=5}^{min(m,4+t)}Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x]\\
&= \frac{1}{m-2} \cdot \left(  p \cdot \sunif_2 + (1-p) \cdot \sunif_6 \right),
\end{align*}
where the last equality is by definition of $\sunif_2$ and the second to last is from the fact that $\tau^{\Sab}$ is picked uniformly at random.

Similarly,
\begin{align*}
Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b] &= p \cdot \sum_{k=4}^{min(3+t,m)} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
&+ (1-p) \cdot \sum_{k=3}^{2+t} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
& = \frac{p}{m-2} \sum_{k=4}^{min(3+t,m)}Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
&+ \frac{1-p}{m-2} \sum_{k=3}^{2+t}Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x]\\
&= \frac{1}{m-2}\cdot \left( p \cdot \sunif_5 + (1-p) \cdot \sunif_3\right).
\end{align*}

For getting $Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a] = Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b] $, we need to prove that there exists a value of $p$ such that:
$p\cdot \sunif_2 + (1-p) \cdot \sunif_6 = (1-p) \cdot \sunif_3 + p \cdot \sunif_5$.

Observe that for $t = 1$ and any $m \geq 7$ and for $t = 2$ and any $m \geq 8$ it holds that: $\sunif_2 = \sunif_3 = \sunif_5 = \sunif_6$, for any value of $p$.
If $m = 7$ and $t = 2$, then $\sunif_2 = \sunif_3 = \sunif_5 = 1$ and $\sunif_6 = \sfrac{1}{2}$ and the above holds for $p = 1$. Now, we focus on the case of $m =6$. In this case, if $t = 1$, $\sunif_2 = \sunif_3 = \sunif_5 = 1 $ and $\sunif_6 = 0$, and the above is satisfied for $p = 1$. If $t = 2$, then $\sunif_2 = \sunif_3 = 1$, $\sunif_5 = \sfrac{1}{2}$ and $\sunif_6 = 0$, and the above is satisfied for $p = \frac{2}{3}$. Thus, $\dist$ and $\swapdist$ are $t$-indistinguishable.

 We have shown above that in any case, there is a value of $p > \sfrac{1}{2}$ such that $Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a] = Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b]$. This immediately implies that, $Pr_{\sigma \sim \dist}[a \succ_\sigma b] = p > Pr_{\sigma \sim \dist}[b \succ_\sigma a] = 1-p$. Furthermore, for $x \in \Sab$ it holds that $Pr_{\sigma \sim \dist}[a \succ_\sigma x] = \frac{m-3}{m-2} \cdot p + \frac{m-6}{m-2} \cdot (1-p)$ and $Pr_{\sigma \sim \dist}[b \succ_\sigma x] = \frac{m-5}{m-2} \cdot p + \frac{m-4}{m-2} \cdot (1-p)$. This is due to the fact that with probability $p$, candidate $a$ is at position $2$ and $b$ is at position $5$, so a candidate $x$ below $a$ is sampled uniformly at random for the remaining $m-3$ positions. Similarly, with probability $1-p$, candidate $a$ is at position $6$, so a candidate $x$ that is ranked below $x$, is sampled uniformly at random from the remaining $m-6$ positions. Similarly, with probability $p$, candidate $b$ is at position $5$, so $x$ is sampled uniformly at random for the remaining $m-5$ positions. Furthermore, with probability $1-p$, candidate $b$ is at position $3$, with $a$ at position $6$, so $x$ is sampled uniformly at random for the remaining $m-4$ positions. Now for the final step of the proof we need the follwoing inequality to be true:

\begin{align*}
\sum_{x \in \Sab}Pr_{\sigma \sim \dist}[a \succ_\sigma x] &>\sum_{x \in \Sab} Pr_{\sigma \sim \dist}[b \succ_\sigma x] \\
\implies \sum_{x \in \Sab} \frac{m-3}{m-2} \cdot p + \frac{m-6}{m-2} \cdot (1-p) &> \sum_{x \in \Sab} \frac{m-5}{m-2} \cdot p + \frac{m-4}{m-2} \cdot (1-p)\\
\implies
(m-3) \cdot p + (m-6)\cdot(1-p) &> (m-5)\cdot p + (m-4) \cdot (1-p) \\
\implies 2\cdot p & > 2\cdot(1-p) \\
\implies p &> \sfrac{1}{2}
\end{align*}
As we have already shown, in any case, $p > \sfrac{1}{2}$. This combined with the fact that $Pr_{\sigma \sim \dist}[a \succ_\sigma b] > Pr_{\sigma \sim \dist}[b \succ_\sigma a] $, yields $\sum_{x \in M \setminus \{a\}}Pr_{\sigma \sim \dist}[a \succ_\sigma x] >\sum_{x \in M \setminus \{b\}} Pr_{\sigma \sim \dist}[b \succ_\sigma x]$, which concludes the proof of the case.








\par








\paragraph{Case III: $t=\{m-1,m-2,m-3\}$.}
Let $i = max(2,m-t)$. We set preference profile $\dist = \hatdist_i$, where $\hatdist_i$ is the preference profile from \Cref{lem:main-construction-uniform}. Hence for $p = \frac{i}{t+i} < \sfrac{1}{2}$, which holds for $t \geq m-3$ and $m > 6$,  $\dist$ and $\swapdist$ are $t$-indistinguishable. Observe that  $Pr_{\sigma \sim \dist}[b \succ_\sigma a] = 1-p$ and $Pr_{\sigma \sim \dist}[a \succ_\sigma b] = p$. Moreover, for each $x \in \Sab$, $Pr_{\sigma \sim \dist}[a \succ_\sigma x] = p \cdot \frac{m-i-1}{m-2}$, as, with probability $p$, $a$ appears at position $i$, $b$ appeats at position $i+1$,  and then all the remaining candidates occupy the remaining $m-i-1$ positions uniformly at random. Furthermore, with probability $1-p$ candidate $a$ appears at position $m$. Similarly, $Pr_{\sigma \sim \dist}[b \succ_\sigma x] = p \cdot \frac{m-i-1}{m-2}$. Thus, we get that
\begin{align*}
    \sum_{x \in M \setminus \{a\}}Pr_{\sigma \sim \dist}[a \succ_\sigma x] &= \sum_{x \in \Sab}Pr_{\sigma \sim \dist}[a \succ_\sigma x] + Pr_{\sigma \sim \dist}[a \succ_\sigma b]= \sum_{x \in \Sab} \frac{m-i-1}{m-2} \cdot p + p,
\end{align*}
and
\begin{align*}
    \sum_{x \in M \setminus \{b\}}Pr_{\sigma \sim \dist}[b \succ_\sigma x ] &= \sum_{x \in \Sab}Pr_{\sigma \sim \dist}[b \succ_\sigma x] + Pr_{\sigma \sim \dist}[b \succ_\sigma a]= \sum_{x \in M \setminus\{a,b\}} \frac{m-i-1}{m-2} \cdot p + 1-p.
\end{align*}

Now, since $p < \sfrac{1}{2}$, it follows that        $\sum_{x \in M \setminus \{a\}}Pr_{\sigma \sim \dist}[b \succ_\sigma x] > \sum_{x \in M \setminus \{b\}}Pr_{\sigma \sim \dist}[a \succ_\sigma x ]$ and this concludes the proof of the case.
\newline
\newline
\end{proof}

\section{Condorcet winner and Deterministic Algorithms}\label{appendix:condorcet-deterministic}


First, we discuss in detail why  the negative result of randomized rules cannot be extended in the case  where $\sfrac{P_i^t}{P_{i+1}^t} = \sfrac{(m-i)}{(m-i-1)}$ for all $i \in \{2, \ldots, m-2\}$. 

It is well-known that the Borda score of a candidate $c \in M$ is given by $\sum_{x \in M \setminus \{c\}} \Pr_{\sigma \sim D}[c \succ_\sigma x]$~\cite{brandt2016handbook}. A candidate $c$ is the Condorcet winner if $\Pr_{\sigma \sim D}[c \succ_\sigma x] > \Pr_{\sigma \sim D}[x \succ_\sigma c]$ for all $x \in M \setminus \{c\}$. This implies that when $c$ is a Condorcet winner, then $\Pr_{\sigma \sim D}[c \succ_\sigma x] > 1/2$ for all $x \in M \setminus \{c\}$, and therefore, 
the Borda score of $c$ must be strictly greater than $\frac{m-1}{2}$.

Now, assume for contradiction that we have a set of $m$ preference profiles $\{\dist^c\}_{c \in M}$ that are $t$-indistinguishable from one another, and suppose $c$ is the Condorcet winner in $D^c$. This means the score of $c$ in $D^c$ must be strictly greater than $\frac{m-1}{2}$. When $\vec{s}^*_t = (P_1^t, P_2^t, \ldots, P_m^t)$ where $\sfrac{P_i^t}{P_{i+1}^t} = \sfrac{(m-i)}{(m-i-1)}$ for all $i \in \{2, \ldots, m-2\}$, then we have that $\vec{s}_{Borda} \in span(\vec{s}_{plu},\vec{s}_{t}^*)$. Therefore, from~\Cref{ther:positional-general} we get that we can learn the Borda score of all the candidates. With similar arguments, as in the proof of~\Cref{lemma:positional-learnable}, we can get that 


\begin{align*}
sc_{\vec{s}_{Borda}}(a,D)= &\sum_{i=1}^{m} P_i^t\cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=i] + (m-1-P_1^t) \cdot \Pr_{\sigma \sim D}[ \sigma^{-1}(a)=1]  \\
=&\sum_{b\in M\setminus \{a\}} \Pr_{\sigma \sim D}[q^t_{\sigma}(b) = a] + (m-1-P_1^t)\cdot \Pr_{\sigma \sim D}[q^t_{\sigma}(a) = a].
 \end{align*}
From the definition of indistinguishability, we have that $\Pr_{\sigma \sim D^c}[q^t_{\sigma}(b) = a]= \Pr_{\sigma \sim D^{c'}}[q^t_{\sigma}(b) = a]  $  for any $a,b, c, c'\in M$ and therefore from the above equality, we get that that each candidate $c$ must have the same Borda score across all $m$ preference profiles. Consequently, in any preference profile, the total sum of scores for all candidates would be strictly larger than $m \cdot \frac{m-1}{2}$. However, this is a contradiction because the sum of Borda scores of all candidates in any preference profile is exactly equal to $m \cdot \frac{m-1}{2}$. Therefore, such a family of preference profiles cannot exist for this case.


Next, we  show that for \(t \leq m - 4\), no deterministic rule can identify the Condorcet winner under \emph{any} \(t\)-improvement feedback queries. 

\begin{theorem}
For any \(m \geq 6\) and any \(t \leq m-4\),  no  algorithm  with access to \(t\)-improvement feedback queries can identify the unique Condorcet winner.
\end{theorem}


\begin{proof}
Consider the following preference profile \( D \):
\begin{itemize}
    \item With probability \( \frac{1}{3} \), we select \( D_{2,m,3} \) from~\Cref{lem:main-construction}.
    \item With probability \( \frac{1}{3} \), we have \( a \succ b \succ \tau^{\Sab} \), where \( \tau^{\Sab} \) is a uniformly random ranking over all remaining candidates.
    \item With probability \( \frac{1}{3} \), we have \( b \succ a \succ \tau^{\Sab} \), where \( \tau^{\Sab} \) is a uniformly random ranking over all remaining candidates.
\end{itemize}

Since \( t \leq m - 4 \), by~\Cref{lem:main-construction} and the construction of \( \dist \), we have that \( \dist \) and \( \swapdist \) are \( t \)-indistinguishable when \( p = \frac{P_3^t}{P_2^t + P_3^t} \) in the construction of \( D_{2,m,3} \). Note that for each \( x \in \Sab \), we have \( \Pr_{\sigma \sim \dist}[a \succ_\sigma x] >\frac{2}{3} \) and \( \Pr_{\sigma \sim \dist}[b \succ_\sigma x] > \frac{2}{3} \).

If \( p \neq \frac{1}{2} \), then \( \Pr_{\sigma \sim \dist}[a \succ_\sigma b] \neq \Pr_{\sigma \sim \dist}[b \succ_\sigma a] \), implying that one of \( a \) or \( b \) is the Condorcet winner. However, since \( \dist \) and \( \swapdist \) are \( t \)-indistinguishable, no deterministic algorithm can always output the Condorcet winner.

On the other hand, if \( p = \frac{1}{2} \), we have \( \Pr_{\sigma \sim \dist}[a \succ_\sigma b] = \Pr_{\sigma \sim \dist}[b \succ_\sigma a] \). However, \( \Pr_{\sigma \sim \dist}[a \succ_\sigma x] > \Pr_{\sigma \sim \dist}[b \succ_\sigma x] \) for all $x\in \Sab$, because, in \( D_{2,m,3} \), with probability \( \sfrac{1}{2}
 \cdot \sfrac{1}{3} = \sfrac{1}{6} \), \( a \) appears in the second position and \( b \) in the last position, while with probability also \( \sfrac{1}{6} \), \( b \) appears in the third position and \( a \) again in the last position, with all other candidates arranged uniformly at random. Hence there is one position (namely, position 3) where $x$ can be below $a$ but not $b$. In this case,  by utilizing~\Cref{lem:main-condorcet} and~\Cref{lem:random-alg}, we can show that even a randomized algorithm cannot find the Condorcet winner with probability greater than \( \sfrac{1}{m} \). Therefore, no deterministic rule can always find the Condorcet winner, and the theorem follows.
\end{proof}

\section{More Experiments}\label{appendix:experiments}
In~\Cref{fig:different-distributions}, we present the approximation ratio of the Borda and Copeland scores across the three different ranking distributions and under the three different \(t\)-improvement feedback distributions. We observe that in all cases, the approximation ratio is similar.

In~\Cref{fig:different-t}, we illustrate the approximation ratio of the Borda and Copeland scores across the three different ranking distributions for varying values of \(t\) and $n=500$. Once again, we observe that the results do not significantly change for different values of $t$. However, in some cases, we observe that the approximation ratio improves as $t$ increases. 

For our experiments, we used the Python package \href{https://pref-voting.readthedocs.io/en/latest/}{pref-voting}.


%We conducted our experiments on a server with 32 cores / 64 threads at 4.2 GHz and 128 GB of RAM.


\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - IC }
        \label{fig:subfig7}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_IC_borda_all.png}
    \end{subfigure}\hspace{0.02\textwidth}%    
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - PL Model}
        \label{fig:subfig8}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_plackett_borda_all.png}
    \end{subfigure}\hspace{0.02\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - Mallows Model}
        \label{fig:subfig9}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_mallows_phi=0.3_borda_all.png}
    \end{subfigure}

    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - IC }
        \label{fig:subfig10}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_IC_copeland_all.png}
    \end{subfigure}\hspace{0.02\textwidth}%    
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - PL Model}
        \label{fig:subfig11}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_plackett_copeland_all.png}
    \end{subfigure}\hspace{0.02\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - Mallows Model}
        \label{fig:subfig12}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_mallows_phi=0.3_copeland_all.png}
    \end{subfigure}

    
    \caption{Different $t$-improvement feedback distributions}
    \label{fig:different-distributions}
\end{figure}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - IC }
        \label{fig:subfig13}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_IC_two_t_borda.png}
    \end{subfigure}\hspace{0.02\textwidth}%    
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - PL Model}
        \label{fig:subfig14}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_plackett_two_t_copeland.png}
    \end{subfigure}\hspace{0.02\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Borda - Mallows Model}
        \label{fig:subfig15}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_mallowsphi033_two_t_borda.png}
    \end{subfigure}

        \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - IC }
        \label{fig:subfig16}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_IC_two_t_copeland.png}
    \end{subfigure}\hspace{0.02\textwidth}%    
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - PL Model}
        \label{fig:subfig17}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_plackett_two_t_copeland.png}
    \end{subfigure}\hspace{0.02\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \caption{Copeland - Mallows Model}
        \label{fig:subfig18}
        \includegraphics[width=\textwidth]{icml2025/fig/plot_mallowsphi033_two_t_copeland.png}
    \end{subfigure}
    \caption{Varying values of $t$}
\label{fig:different-t}
\end{figure}


% Select a uniformly random ranking \( \tau^{\Sab} \) over the set of all remaining candidates (excluding \( a \) and \( b \)). This ranking determines the relative order of the remaining candidates, which are then placed in the positions not occupied by \( a \) or \( b \).



% Let $p_1 < p_2 < \sfrac{1}{2}$
% We define preference profile $\dist$ as following:

% \begin{enumerate}
% \item With probability $\sfrac{(1-p_1-p_2)}{2}$, candidate $a$ is fixed at position $1$, and candidate $b$ is fixed at position $2$.
% \item With probability $\sfrac{(1-p_1-p_2)}{2}$, candidate $a$ is fixed at position $2$, and candidate $b$ is fixed at position $1$.
% \item With probability $p_1$, $a$ is fixed at position $m$ and $b$ is fixed at position $2$.
% \item With probability $p_2$, $a$ is fixed at position $3$ and $b$ is fixed at position $m$
% \end{enumerate}

% Firstly, $Pr_{\sigma \sim \dist}[q_\sigma^t(a) = b] =
% Pr_{\sigma \sim \dist}[q_\sigma^t(b) = a] = \frac{1-p_1-p_2}{2}$, since $t < m-3$ and in the other cases $a,b$ are more than $t$ positions apart.

% Furthermore, for any $x \in \Sab$ it holds that:

% \begin{align*}
% Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x] &= p_1 \cdot \sum_{k=m-t}^{m-1}Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x] \\
% &+ p_2 \cdot \sum_{k=min(1,3-t)}^{2} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
% &=p_1 \cdot \frac{1}{m-2} \cdot \sum_{k=m-t}^{m-1} Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
% &+ p_2 \cdot \frac{1}{m-2} \cdot \sum_{k=min(1,3-t)}^{2}Pr_{\sigma \sim \dist}[q_\sigma^t(a) = x \mid \tau^{\Sab}(k) = x]\\
% &= p_1 \cdot \frac{1}{m-2} \cdot \sum_{k=m-t}^{m-1} p_{m,k} + p_2 \cdot \frac{1}{m-2} \cdot \sum_{k=min(1,3-t)}^{2}p_{3,k} \\
% &= \frac{p_1 + p_2}{m-2}
% \end{align*}
% where the second equality is due to the fact that $\tau^{\Sab}$ is picked uniformly at random from a set of $m-2$ candidates and the last equality is due to the fact that each agent returns a candidate from the $t$ - above neighborhood.
% \end{proof}

% Similarly,

% \begin{align*}
% Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x] &= p_1 \cdot Pr[\tau^{\Sab}(1) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(1) = x] \\
% &+ p_2 \cdot \sum_{k=m-t}^{m-1} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x]\\
% &=p_1 \cdot \frac{1}{m-2} \cdot  Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(1) = x]\\
% &+ p_2 \cdot \frac{1}{m-2} \cdot \sum_{k=m-t}^{m-1}Pr_{\sigma \sim \dist}[q_\sigma^t(b) = x \mid \tau^{\Sab}(k) = x]\\
% &= p_1 \cdot \frac{1}{m-2} \cdot p_{2,1} + p_2 \cdot \frac{1}{m-2} \cdot \sum_{k=m-t}^{m-1}p_{m,k} \\
% &= \frac{p_1 + p_2}{m-2}
% \end{align*}

% Observe that with probability $1-p_1-p_2$, any $x \in \Sab$ reaches both $a$ and $b$, since in that case $a$ is at position $1$ and b at position $2$ or $a$ is at position $2$ and $b$ at position $1$. Hence, we only need to argue for what happens with probability $p_1 + p_2$. In that case, we only examine the rankings we get with probability $p_1$ and $p_2$. Note that,

% \begin{align}
% Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a] &= p_2 \cdot \sum_{k=4}^{3+t} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
% &=p_2 \cdot \frac{1}{m-2} \cdot \sum_{k=4}^{3+t} Pr_{\sigma \sim \dist}[q_\sigma^t(x) = a \mid \tau^{\Sab}(k) = x] \\
% &=p_2 \cdot \sunif_3
% \end{align}
% where the last equality is by definition of $\sunif_i$
% Similarly,

% \begin{align}
% Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b] &= p_1 \cdot \sum_{k=3}^{2+t} Pr[\tau^{\Sab}(k) = x] \cdot Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
% &=p_1 \cdot \frac{1}{m-2} \cdot \sum_{k=3}^{2+t} Pr_{\sigma \sim \dist}[q_\sigma^t(x) = b \mid \tau^{\Sab}(k) = x] \\
% &=p_1 \cdot \sunif_2
% \end{align}

% Hence we require that $p_1 \cdot \sunif_2 = p_2 \cdot  \sunif_3$, or $\frac{p_1}{p_2} = \frac{\sunif_3}{\sunif_2}$. Observe that the only condition we want $P_i$ to satisfy is $\sunif_2 \neq \sunif_3$. If $\sunif_3 < \sunif_2$ then $p_1 < p_2$ as required. If $\sunif_2 < \sunif_3$ then we can swap the output when $p_1$ with the output when $p_2$ to get the desired result.



% Observe that because $a,b$ appear at first and second position with probability $1-p_1-p_2$ which by assumption is greater than $\sfrac{1}{2}$ we only have two possible Condorcet winners, namely $a$ or $b$. Furthermore, Since $p_1 < p_2$ it holds that $Pr_{\sigma \sim \dist}[a \succ b] = p_2 > Pr[b \succ a] = p_1$. Hence, $a$ is unique Condorcet winner.












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



 











\end{document}
