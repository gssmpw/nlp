\section{Related work}
Graph learning-driven failure predictions have been gaining attention due to their effectiveness in tackling the massive volumes of data generated by 5G networks and their ability to understand the complex structural characteristics of these networks. This section explores three areas of research: one focusing on learning-based 5G failure prediction, imbalanced learning techniques, and Graph Fourier transform (GFT) to capture the complex structure of NDTs.

\subsubsection{Learning-based failure prediction}

The primary advantage of applying deep graph learning to network modeling \cite{habibi2019comprehensive} lies in its data-driven approach, which enable it to capture the complex nature of the real-world networks. Most existing research in \cite{ding2022data} and \cite{said2020network} uses fully connected conventional neural networks. However, the primary constraint of these methods is their non-generalization to alternative network topologies and configurations, such as routing. In this context, more recent studies have proposed advanced neural network models, such as GNNs \cite{al2024comparative, soltanzadeh2021rcsmote}, convolutional neural networks \cite{said2020network}, and variational autoencoders \cite{ding2018opportunities} for robustness in imbalanced datasets common to cybersecurity domains. Despite their advancements, these models have different objectives and overlook critical aspects of real-world networks from the twin perspective. The concept of broadly applying neural networks to graphs has gained significant attention. For example, convolutional networks have been extended to graphs in the spectral domain \cite{bruna2013spectral}, where filters are applied to frequency modes using GFT. The eigenvector matrix of a graph Laplacian must be multiplied to achieve this transformation. One study \cite{defferrard2016convolutional} parameterized the spectrum filters as Chebyshev polynomials of eigenvalues, resulting in efficient and localized filters and reducing the computational burden. However, a drawback of these spectral formulations are disadvantageous is that they are limited to graphs with a single structure since they depend on the fixed spectrum of the graph Laplacian. In contrast, spatial formulations are not constrained by a specific graph topology. Generalizing neural networks to graphs is studied in two ways: a) given a single graph structure, or labels of individual nodes \cite{scarselli2008graph, bruna2013spectral, lei2017deriving, li2015gated, defferrard2016convolutional, kipf2016semi} b) given a set of graphs with different structure and sizes, the goal is learn predictions of the class labels of the graphs are learned \cite{duvenaud2015convolutional, atwood2016diffusion, niepert2016learning}.  Inspired by the ITU Challenge "ITU-ML5G-PS-008: Network Failure Classification Model Using the Digital Twin Network" \cite{Junichi:2023}, our research aims to leverage GNNs, enhanced with the CF-GNN, to analyze interconnected systems and networks in an end-to-end manner. By integrating class-oriented spectral filtering, the proposed CF-GNN framework enables precise classification of network failure types and accurate identification of failure points. The approach improves accuracy and adaptability compared to conventional deep learning models, effectively capturing both global consistency and localized variations within complex 5G network structures.

\subsubsection{Imbalance learning technique}
    Data-level and algorithm-level methods are the two main categories of class-imbalanced learning techniques. Before building classifiers, data-level approaches preprocess training data to reduce inequality \cite{li2021novel}. These strategies include undersampling majority classes and oversampling minority classes. On the other hand, algorithm-level approaches address the issue of class imbalance by modifying the model's fundamental learning decision-making process. Algorithm-level techniques can be broadly classified into three categories: threshold moving, cost-sensitive learning, and new loss functions \cite{johnson2019survey}. Several approaches have been proposed at the data level. SMOTE \cite{chawla2002smote} is one such technique that generates synthetic minority samples in the feature space by interpolating existing minority samples and their nearest minority neighbors. However, one of the primary limitations of SMOTE is that it creates new synthetic samples without considering the neighborhood samples of the majority classes, which can lead to increased class overlapping and extra noise \cite{koziarski2019radial}. Based on the SMOTE principle, many variations have been proposed, such as Borderline-SMOTE \cite{han2005borderline} and Safe-Level-SMOTE \cite{bunkhumpornpat2009safe}, which enhance the original approach by taking majority class neighbors into account. At the same time, Safe-Level-SMOTE creates safe zones to prevent oversampling in overlapping or noisy regions, whereas Borderline-SMOTE only samples the minority samples close to the class borders. To address class-imbalanced challenges, \cite{gilmer2017neural} modified the parameters of the model learning process that favored classes with fewer samples. For example, Focal Loss, introduced in \cite{lin2017focal}, allows minority samples to contribute more to the loss function. Additionally, a novel loss function called Mean Squared False Error (MSFE) was proposed in \cite{wang2016training} to train deep networks on imbalanced datasets.
    
    This study enables us to handle the class-imbalanced problem on graph data, two new models have been developed recently: We employ some of the settings in GATE-GNN \cite{fofanah2024addressing} dynamically modifies the weights of various GNN modules and Reweight \cite{he2024gradient} that focuses on class-incremental learning on dynamic real-world environments on the dual imbalances.

The paper was structured as follows: The notation and preliminaries are introduced in Section (\ref{2}). In Section (\ref{3}), we investigate the graph filtering mechanism of the proposed method and the 5G network digital twin datasets. The experiment and evaluation are provided in Section (\ref{4}). details the datasets and results. We finally concluded in Section (\ref{5}).