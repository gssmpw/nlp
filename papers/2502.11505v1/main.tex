%% 
%% Copyright 2007-2025 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage[ruled,vlined]{algorithm2e}
%\usepackage{multirow}
\usepackage{longtable} % to produce long table
\usepackage{booktabs} % to use top rule, bottom rule, ....in table
\usepackage{array}
\PassOptionsToPackage{colorlinks=true, linkcolor=blue}{hyperref}
\usepackage{hyperref}
\usepackage{graphicx} % For \includegraphics
%\usepackage{algorithm}
\usepackage{algorithmic}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage{lineno}

\journal{Internet of Things}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{A GNN-based Spectral Filtering Mechanism for Imbalance Classification in Network Digital Twin} %% Article title


\author[1,3]{Abubakar Isah} %% Author name
\ead{abubakarisah@jnu.ac.kr}
\author[1]{Ibrahim Aliyu}
\author[1]{Sulaiman Muhammad Rashid}
\author[1]{Jaehyung Park}
\author[4]{Minsoo Hahn}
\author[1]{Jinsul Kim\corref{cor1}} %% Author name
\ead{jsworld@jnu.ac.kr}
\cortext[cor1]{Corresponding authors}
%% Author affiliation

\affiliation[1]{organization={Department of Intelligent Electronics and Computer Engineering, Chonnam National University},%Department and Organization
            addressline={Buk-gu}, 
            postcode={61186}, 
            state={Gwangju},
            country={South Korea}}

%\affiliation[2]{organization={ Electronics
%and Telecommunications Research Institute (ETRI)},%Department and Organization
            %postcode={34129}, 
            %state={Daejeon},
            %country={South Korea}}

\affiliation[3]{organization={Department of Computer Science, Ahmadu Bello University},%Department and Organization
            postcode={810241}, 
            state={Zaria},
            country={Nigeria}}

\affiliation[4]{organization={Department of Computational and Data Science, Astana IT University, Astana, Kazakhstan}}


%% Abstract
\begin{abstract}
%% Text of abstract
Graph Neural Networks are gaining attention in Fifth-Generation (5G) core network digital twins, which are data-driven complex systems with numerous components. Analyzing these data can be challenging due to rare failure types, leading to imbalanced classification in multiclass settings. Digital twins of 5G networks increasingly employ graph classification as the main method for identifying failure types. However, the skewed distribution of failure occurrences is a major class imbalance issue that prevents effective graph data mining. Previous studies have not sufficiently tackled this complex problem. In this paper, we propose Class-Fourier Graph Neural Network (CF-GNN) introduces a class-oriented spectral filtering mechanism that ensures precise classification by estimating a unique spectral filter for each class. We employ eigenvalue and eigenvector spectral filtering to capture and adapt to variations in the minority classes, ensuring accurate class-specific feature discrimination, and adept at graph representation learning for complex local structures among neighbors in an end-to-end setting. Extensive experiments have demonstrated that the proposed CF-GNN could help with both the creation of new techniques for enhancing classifiers and the investigation of the characteristics of the multi-class imbalanced data in network digital twin system.
 
\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
\includegraphics[width=1.0\textwidth]{Images/CF-GNN.pdf}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item We proposed a novel CF-GNN to address class imbalance tasks in multiclass 5G network digital twins.

\item CF-GNN employs the concepts of eigenvalues and eigenvectors to identify minority classes in a multiclass failure state.

\item Incorporating spectral filtering influenced by the global graph structure and local neighborhood information during the filtering process.

\item The proposed model showcased the robust performance of the network digital twin-unbalanced datasets.
\end{highlights}

%% Keywords
\begin{keyword}
Graph neural network \sep Multi-class imbalanced \sep Eigenvalue \sep Graph representation learning \sep Network Digital Twin
\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section
\section{Introduction} \label{Intro I}
The emergence of fifth-generation (5G) networks, and robust core network systems are paramount for the evolution to Industry 5.0. According to recent studies, 5G connections are projected to increase over 100-fold, from approximately 13 million in 2018 to 2.6 billion by 2025 \cite{Ericsson:2025}. This growth is expected to continue with sixth-generation (6G) networks and the advent of the Industry 5.0 era, characterized by the vision of the Internet of Everything. In this context, network failure classification for 5G and beyond is essential to optime the network performance. As the backbone of communication infrastructure, reliable and resilient networks are essential for seamless operation in diverse applications in environments ranging from daily life to industrial settings. The core network management center can identify and classify network failures by applying deep learning algorithms and advanced analytics, enabling preemptive measures to ensure uninterrupted service delivery \cite{wang2022spatial}.

The digital twin (DT) is a powerful technology that has emerged as a promising solution for connecting physical spaces with digital systems \cite{grieves2014digital, shin2024improving}. This technology uses historical data and real-time operational information to create digital replicas of physical entities, such as devices, machines, and objects. Research on DTs \cite{yu2023digital} and \cite{isah2023digital} has garnered significant attention, with numerous researchers exploring its applications in such fields as aviation, 6G networks, intelligent manufacturing, and data generation \cite{wu2021digital, isah2023towards}. Additionally, DT has been recognized as one of the 12 representative use cases for future networks by the  International Telecommunication Union Network 2030 focus group \cite{cases2020key}. However, setting up a replica of a network event to mimic every potential pattern in network DTs is challenging.

Numerous tasks related to graphs classification have been extensively studied, including node classification \cite{maurya2022simplifying}, graph classification \cite{yuan2020interpreting}, and link predictions \cite{olah2017feature}. Moreover, a variety of advanced GNN methodologies, such as graph pooling \cite{wang2024graph}, graph convolution \cite{wang2020icapsnets}, and graph attention mechanisms \cite{velickovic2017graph}, have been suggested to enhance the performance of GNNs. Despite these advancements, there remains a scarcity of graph models for communication network domains \cite{isah2024graph}, which is crucial for understanding GNNs. Several methods, such as RouteNet \cite{ferriol2023routenet} and RouteNet-Fermi \cite{galmes2022routenet}, have been introduced to model the NDT for network performance. The benefit of data-driven models \cite{isah2023datadriven} is their ability to accurately capture a wide range of complex network nodes with unprecedented precision, as they are trained on real-world data \cite{zhang2024knowledge}. Nevertheless, none of these recent studies have focused on data-driven network digital twin approach to tackle the imbalance node classification tasks in the NDT domain.

Class-Imbalanced across many application fields, including the Digital Twin \cite{ming2025digital}, Fraud Detection \cite{liu2021pick}, for computing \cite{sharief2024multi}, and recently the synthetic oversampling  \cite{yan2025synthetic}. GNN models that are robust to class imbalance must be developed to avoid biases towards majority classes while maintaining the ability to generalize over minority classes. Traditional techniques for addressing the class imbalance, including oversampling \cite{chawla2002smote}, and sampling the classifiers \cite{hoens2013imbalanced}, have limitations when applied to graph-structured data because they do not account for the inherent graph structure. Thus, many scenarios have been developed to tackle class imbalance within semi-supervised node classification. Node classification with synthetic oversampling methods, GraphSMOTE \cite{zhao2021graphsmote, zhao2024imbalanced}, and GATE-GNN \cite{fofanah2024addressing} generates new nodes and employ a variety of methods to connect them to the existing graph. Furthermore, these methods often fail to fully utilize the untapped potential of the abundant labeled and unlabeled nodes in the graph, which limits their effectiveness in real-world applications.

In this study, we present a novel Class-Fourier Graph Neural Network filtering (CF-GNN) framework designed to address the challenges posed by Imbalanced node classification tasks. Our proposed method leverage node-specific spectral filtering to ensure that minority-class nodes receive attention without being overtaken by majority-class nodes. By enabling the filter to focus on particular nodes. This helps to emphasize overshadowed classes, which is particular beneficial for imbalanced datasets. the localized filtering concentrates on the distinct features of nodes. This ensures that class-specific changes can be applied to nodes that belong to minority classes, enhancing their representation in training and prediction. Additionally, the filtering process, being aware of class-specific characteristics in the spectral domain, reduces the likelihood of minority-class nodes being overwhelmed by dominant classes. During training, the model employs class-weighted loss functions, which penalize the misclassification of minority-class samples more heavily. This forces the network to learn better representations of underrepresented classes. The spectral filtering procedure of the CF-GNN explicitly integrates a multiclass structure, transforming features in a class-aware way by the use of spectral coefficients and eigenvalue-based distinctions between classes. Our experimental evaluations on two 5G NDT datasets demonstrate how well the proposed CF-GNN framework compared to state-of-the-art GNN techniques designed to mitigate class imbalance problems.

The contributions of this work and the proposed CF-GNN include the following:

\begin{itemize}
     \item The proposed CF-GNN employs a spectral attention technique in the Fourier domain to extract complex structural components that conventional spatial-based GNN may overlook.
     
    \item CF-GNN utilizes class-specific spectral filtering to enhance classification accuracy by highlighting significant feature components associated with certain classes. By iteratively improving the eigenvector estimates, this procedure ensure accurate spectral analysis of the graph structure.

    \item Introduces the theoretical Twin-GFT for multidimensional spectral representation, enabling scalable failure detection and improving prediction robustness through ensemble classification.
    
\end{itemize}

\subsection{Related work}
Graph learning-driven failure predictions have been gaining attention due to their effectiveness in tackling the massive volumes of data generated by 5G networks and their ability to understand the complex structural characteristics of these networks. This section explores three areas of research: one focusing on learning-based 5G failure prediction, imbalanced learning techniques, and Graph Fourier transform (GFT) to capture the complex structure of NDTs.

\subsubsection{Learning-based failure prediction}

The primary advantage of applying deep graph learning to network modeling \cite{habibi2019comprehensive} lies in its data-driven approach, which enable it to capture the complex nature of the real-world networks. Most existing research in \cite{ding2022data} and \cite{said2020network} uses fully connected conventional neural networks. However, the primary constraint of these methods is their non-generalization to alternative network topologies and configurations, such as routing. In this context, more recent studies have proposed advanced neural network models, such as GNNs \cite{al2024comparative, soltanzadeh2021rcsmote}, convolutional neural networks \cite{said2020network}, and variational autoencoders \cite{ding2018opportunities} for robustness in imbalanced datasets common to cybersecurity domains. Despite their advancements, these models have different objectives and overlook critical aspects of real-world networks from the twin perspective. The concept of broadly applying neural networks to graphs has gained significant attention. For example, convolutional networks have been extended to graphs in the spectral domain \cite{bruna2013spectral}, where filters are applied to frequency modes using GFT. The eigenvector matrix of a graph Laplacian must be multiplied to achieve this transformation. One study \cite{defferrard2016convolutional} parameterized the spectrum filters as Chebyshev polynomials of eigenvalues, resulting in efficient and localized filters and reducing the computational burden. However, a drawback of these spectral formulations are disadvantageous is that they are limited to graphs with a single structure since they depend on the fixed spectrum of the graph Laplacian. In contrast, spatial formulations are not constrained by a specific graph topology. Generalizing neural networks to graphs is studied in two ways: a) given a single graph structure, or labels of individual nodes \cite{scarselli2008graph, bruna2013spectral, lei2017deriving, li2015gated, defferrard2016convolutional, kipf2016semi} b) given a set of graphs with different structure and sizes, the goal is learn predictions of the class labels of the graphs are learned \cite{duvenaud2015convolutional, atwood2016diffusion, niepert2016learning}.  Inspired by the ITU Challenge "ITU-ML5G-PS-008: Network Failure Classification Model Using the Digital Twin Network" \cite{Junichi:2023}, our research aims to leverage GNNs, enhanced with the CF-GNN, to analyze interconnected systems and networks in an end-to-end manner. By integrating class-oriented spectral filtering, the proposed CF-GNN framework enables precise classification of network failure types and accurate identification of failure points. The approach improves accuracy and adaptability compared to conventional deep learning models, effectively capturing both global consistency and localized variations within complex 5G network structures.

\subsubsection{Imbalance learning technique}
    Data-level and algorithm-level methods are the two main categories of class-imbalanced learning techniques. Before building classifiers, data-level approaches preprocess training data to reduce inequality \cite{li2021novel}. These strategies include undersampling majority classes and oversampling minority classes. On the other hand, algorithm-level approaches address the issue of class imbalance by modifying the model's fundamental learning decision-making process. Algorithm-level techniques can be broadly classified into three categories: threshold moving, cost-sensitive learning, and new loss functions \cite{johnson2019survey}. Several approaches have been proposed at the data level. SMOTE \cite{chawla2002smote} is one such technique that generates synthetic minority samples in the feature space by interpolating existing minority samples and their nearest minority neighbors. However, one of the primary limitations of SMOTE is that it creates new synthetic samples without considering the neighborhood samples of the majority classes, which can lead to increased class overlapping and extra noise \cite{koziarski2019radial}. Based on the SMOTE principle, many variations have been proposed, such as Borderline-SMOTE \cite{han2005borderline} and Safe-Level-SMOTE \cite{bunkhumpornpat2009safe}, which enhance the original approach by taking majority class neighbors into account. At the same time, Safe-Level-SMOTE creates safe zones to prevent oversampling in overlapping or noisy regions, whereas Borderline-SMOTE only samples the minority samples close to the class borders. To address class-imbalanced challenges, \cite{gilmer2017neural} modified the parameters of the model learning process that favored classes with fewer samples. For example, Focal Loss, introduced in \cite{lin2017focal}, allows minority samples to contribute more to the loss function. Additionally, a novel loss function called Mean Squared False Error (MSFE) was proposed in \cite{wang2016training} to train deep networks on imbalanced datasets.
    
    This study enables us to handle the class-imbalanced problem on graph data, two new models have been developed recently: We employ some of the settings in GATE-GNN \cite{fofanah2024addressing} dynamically modifies the weights of various GNN modules and Reweight \cite{he2024gradient} that focuses on class-incremental learning on dynamic real-world environments on the dual imbalances.

The paper was structured as follows: The notation and preliminaries are introduced in Section (\ref{2}). In Section (\ref{3}), we investigate the graph filtering mechanism of the proposed method and the 5G network digital twin datasets. The experiment and evaluation are provided in Section (\ref{4}). details the datasets and results. We finally concluded in Section (\ref{5}).


\section{Notations and Preliminaries} \label{2}
We formulated the network failure injection as a classification problem, focusing on learning meaningful embeddings from the generated datasets to increase detection accuracy and identify different failure types. In the network structure, we define a graph $G=(V,E)$ with an adjacent matrix $A = \{e_{vu} | \forall v, u \in V\} \in \mathbb{R}^{N \times N}$, where $N$ represents the number of nodes, letting $G = (V, E)$ represent an input graph with a set of nodes $V$. If $e_{vu} = 1$, then $(v, u) \in E$; otherwise, $(v, u) \in E'$. We assume the set of feature vectors associated with each network component $v$ are $D$-dimensional represented by $F = \{f_v | v \in V\} \in \mathbb{R}^D$. We considered the failure points as nodes in the network graph for network failure classification. Each node is associated with various features, such as performance metrics or operational data, reflecting its behavior within the network. The edges connecting pairs of nodes represent the dependencies and relationships between them.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Images/CF-GNN.pdf}
    \caption{The architectural framework of the proposed technique for imbalanced classification of 5G network of digital twin}
    \label{fig: DTN 5G}
\end{figure}

The proposed framework in Figure \ref{fig: DTN 5G} illustrates the proposed model. We applied CF-GNN to examine individual nodes with different types of features (e.g., phys-address, oper-status, etc.). The GNN takes the provided NDT data as input. The GNN retrieves fine-grained information about node interactions and local graph structures, while the Class-Fourier Transform (CFT) efficiently denoizes features while capturing global network attributes. The graph module then processes these inferred relationships to create a multi-layer graph representation. The feature extraction module applies Fourier transform algorithm to convert 5G network data into useful information. To achieve accurate multi-class classification results, the proposed CF-GNN integrate the extracted features with the learned graph structure.


\begin{table}[htbp]
\centering
\caption{Explanation of Graph and Laplacian Variables}
\label{tab:graph_variables}
\begin{tabular}{c|p{6cm}}
\hline
\textbf{Variable} & \textbf{Explanation} \\ 
\hline
$G = (V,E)$ & Graph representing the NDT system \\ 
\hline
$V_n$ & Set of nodes in graph $G_n$ \\ 
\hline
$N$ & Total number of nodes \\ 
\hline
$N_n$ & Number of nodes in $G_n$ \\ 
\hline
$L_n$ & Laplacian matrix of $G_n$ \\ 
\hline
$\lambda^{(n)}_k$ & $k$-th eigenvalue of $\mathcal{L}^{(n)}$ \\ 
\hline
$\phi^{(n)}_k$ & Eigenfunctions of the Laplacian \\ 
\hline
$\mathcal{G}_1 \times \mathcal{G}_2$ & Product graph with node features \\ 
\hline
$\sigma(L_1 \oplus L_2)$ & Eigenvalues of $L_1 \oplus L_2$ \\ 
\hline
$(i_1, i_2)$ & Node indices in $\mathcal{G}_1$ and $\mathcal{G}_2$ \\ 
\hline
$(k_1, k_2)$ & Indices for eigenvalues and eigenfunctions \\
\hline
$y_{ij}$ & One-hot label component for node $v_i$ \\ 
\hline
$f_{ij}$ & Predicted probability for node $v_i$ in class $j$ \\ 
\hline
$W_{fc1}, b_{fc1}$ & Parameters of the GFT linear layer \\ 
\hline
$W_{mp1}, b_{mp1}$ & Parameters of the message passing layer \\ 
\hline
$h_i$ & Hidden representation at layer $i$ \\ 
\hline
$\hat{m}, \hat{v}$ & Adam optimizer moment estimates \\ 
\hline
$\eta$ & Learning rate \\ 
\hline
$y$ & Output vector of size $(N, C)$ \\ 
\hline
$\theta$ & Parameters of the GNN model \\ 
\hline
$f^{(l)}$ & Aggregation function at layer $l$ \\ 
\hline
$N(v_i)$ & Neighbors of node $v_i$ \\ 
\hline
$h_i^{(0)} = x_i$ & Initial feature vector for node $v_i$ \\ 
\hline
\hline
\end{tabular}
\end{table}

\subsection{ Graph Neural Network}
The GNN can transform the graph structure data into standard representations, making them suitable for input into neural networks for training. We employ the concept of class spectral filtering introduced in \cite{yi2024fouriergnn} and \cite{guo2023graph} to enhance the learning process. This approach allows the GNN to efficiently propagate the information from the nodes and edges to their neighboring nodes. To achieve this, we use the neighbor aggregation approach in \cite{lessan2019hybrid}.
The network infrastructure is modeled as a graph $G = (V,E)$, where $V$ represents the set of nodes (network components) and $E$ represents the set of edges (connections). Each node $v_i$ in the graph is associated with a feature vector $x_i$, which captures relevant information about the corresponding network component. The graph can be represented using an adjacency matrix $A$, where $A_{ij} = 1$ if a connection exists between nodes $v_i$ and $v_j$, and $A_{ij} = 0$ otherwise. Node features are stored in the feature matrix $X$, where $X$ has the dimensions $|V| \times D$, with $|V|$ represents the number of nodes, and $D$ denotes the dimensionality of the feature vectors $X = [x_1, x_2, \ldots, x_{|V|}]$. 

Many network components depend upon one another due to their interconnectivity \cite{ferriol2023routenet}. The GNN architecture consists of multiple layers, each processing information from neighboring vertices to extract hierarchical representations. We let $h_i^{(l)}$ denote the representation of vertex $v_i$ at layer $l$ of the GNN. Information is aggregated from the neighboring vertices and transformed using learnable parameters. The update equation for $h_i^{(l)}$ can be expressed as follow:

\begin{equation}
h_i^{(l)} = f^{(l)} \left( h_i^{(l-1)}, \{ h_j^{(l-1)} \}_{j \in N(v_i)} \right),
\end{equation}
where \( N(v_i) \) represents the set of neighboring vertices of \( v_i \).

One of the biggest challenges is creating network digital twin datasets that accurately replicate the real-world failure scenario. This difficulty arises to several factors, such as the limited availability of real-world system data and the inherent complexity of simulating diverse failure types.

\subsection{Graph Fourier Transform}
We considered an undirected weighted graph \( G = (V, E, w) \), where \( V = \{0, 1, \ldots, N-1\} \) is the vertex set, \( E \) is the set of edges, and \( w(i,j) \) is the weight function satisfying \( w(i,j) = 0 \) for any \( (i,j) \) $\notin$ \( E \). We assume all graphs are simple, meaning they have no loops or multiple edges.

Three matrices associated with \( G \) are important: the adjacency matrix \( W \), a degree matrix \( D \), and a Laplacian matrix \( L = D - W \). The Laplacian matrix is essential for Graph Fourier Transforms (GFTs).

The Laplacian matrix \( L \) is real, symmetric, and positive-semidefinite, thus, it possesses nonnegative eigenvalues \( \lambda_0, \ldots, \lambda_{N-1} \) and the corresponding orthonormal eigenfunctions \( u_0, \ldots, u_{N-1} \). These eigenfunctions satisfy the following equation:

\begin{equation}
L \begin{bmatrix}
u_k(0) \\
\vdots \\
u_k(N-1)
\end{bmatrix} = \lambda_k \begin{bmatrix}
u_k(0) \\
\vdots \\
u_k(N-1)
\end{bmatrix}
\end{equation}


For \( k = 0, \ldots, N-1 \), orthonormality implies that the sum of products of corresponding eigenfunctions is equal to the Kronecker delta function \(\delta(i,j)\), where \(\delta(i,j)\) equals 1 if \(i = j\) and is zero otherwise. This study assumes that eigenvalues are arranged in ascending order, such that \(\lambda_0 \leq \ldots \leq \lambda_{N-1}\). In addition, \(\lambda_0\) is strictly zero because the sum of rows in \(L\) equals zero. The spectrum of the matrix, denoted as \(\{ \lambda_k \}_{k=0}^{N-1}\), is represented as \(\sigma(L)\).
The GFT in \cite{kurokawa2017multi} of a graph feature \( f : V \rightarrow \mathbb{R} \) is defined as \( \hat{f} : \sigma(L) \rightarrow \mathbb{C} \), where \( \sigma(L) \) represents the spectrum of the graph Laplacian \( L \). It is expressed as follows:


\begin{equation}
    \hat{f}(\lambda_k) = \langle f, u_k \rangle = \sum_{i=0}^{N-1} f(i) u_k(i),
\end{equation}

for \( k = 0, \ldots, N-1 \). where, \( u_k \) denotes the orthonormal eigenfunctions of \( L \), and the GFT represents a feature expansion using these eigenfunctions. The inverse GFT is given by,

\begin{equation}
    f(i) = \sum_{k=0}^{N-1} \hat{f}(\lambda_k) u_k(i),
\end{equation}

which reconstruct the original function $f(i)$ from its spectral components.
Interestingly, the GFT's basic function in spectral graph research is highlighted by the fact that it is equal to the Discrete Fourier Transform (DFT) on cycle graphs.

When a graph Laplacian has non-distinct eigenvalues, the functions generated by the GFT may not be well-defined, resulting in multi-valued functions. For instance, if two orthonormal eigenfunctions, \( u \) and \( u_0 \) correspond to the same eigenvalue \( \lambda \), then the spectral component \( \hat{f}(\lambda) \) can have two distinct values: \( \langle f,u \rangle \) and \( \langle f,u_0 \rangle \).


\subsection{Digital Twin Spectral Filtering}

The Cartesian product \( G_1 \square G_2 \) of graphs \( G_1 = (V_1, E_1, w_1) \) and \( G_2 = (V_2, E_2, w_2) \) is a graph with the vertex set \( V_1 \times V_2 \) and the edge set \( E \) defined as follows. For any \((i_1, i_2)\) and \((j_1, j_2)\) in the vertex set \( V_1 \times V_2 \), these are connected by an edge if either \((i_1, j_1)\) is in \( E_1 \) and \(i_2 = j_2\) or \(i_1 = j_1\) and \((i_2, j_2)\) is in \( E_2 \). The weight function \( w \) is defined as follows:

\begin{equation}
    w((i_1, i_2), (j_1, j_2)) = w_1(i_1, j_1) \delta(i_2, j_2) + \delta(i_1, j_1) w_2(i_2, j_2)
\end{equation}


where, \( \delta \) denotes the Kronecker delta function. The graphs \( G_1 \) and \( G_2 \) are referred to as the factor graphs of \( G_1 \square G_2 \).

It has been established in \cite{zheng2015redundant} that the adjacency, degree, and Laplacian matrices of Cartesian product graph can be derived form those of its factor graphs. Two factor graphs, \( G_1 \) and \( G_2 \), with vertex sets \( V_1 = \{0, 1, \ldots, N_1 - 1\} \) and \( V_2 = \{0, 1, \ldots, N_2 - 1\} \) respectively are considered. Each with adjacency matrix \( W_1 \) and \( W_2 \), degree matrix \( D_1 \) and \( D_2 \), and Laplacian matrix \( L_1 \) and \( L_2 \). When the vertices of the Cartesian product graph are ordered lexicographically, such as \((0, 0), (0, 1), (0, 2), \ldots, (N_1 - 1, N_2 - 1)\), the adjacency, degree, and Laplacian matrices of \( G_1 \square G_2 \) can be expressed as \( W_1 \oplus W_2 \), \( D_1 \oplus D_2 \), and \( L_1 \oplus L_2 \), respectively, where operator \( \oplus \) denotes the Kronecker sum.

\textbf{Definition 1:} The Kronecker sum is defined by \( A \oplus B = A \otimes I_n + I_m \otimes B \) for matrices \( A \in \mathbb{R}^{m \times m} \) and \( B \in \mathbb{R}^{n \times n} \), where \( I_n \) represents the identity matrix of size \( n \).

The Kronecker sum has a valuable characteristic that decomposes an eigenproblem involving the Laplacian matrix of a product graph into eigenproblems of Laplacian matrices of the factor graphs. We assumed that the Laplacian matrix \( L_n \) of each factor graph has nonnegative eigenvalues \(\{\lambda^{(n)}_k\}_{k=0}^{N_n-1}\) and orthonormal eigenfunctions \(\{u^{(n)}_k\}_{k=0}^{N_n-1}\) for \(n=1,2\). In this case, the Kronecker sum \( L_1 \oplus L_2 \) yields an eigenvalue of \(\lambda^{(1)}_k + \lambda^{(2)}_k\) and the corresponding eigenfunction \(u^{(1)}_k \otimes u^{(2)}_k : V_1 \times V_2 \rightarrow \mathbb{C}\), where \(\oplus\) denotes the Kronecker sum. This eigenfunction satisfies the following:

\begin{align}
& (L_1 \oplus L_2) \left[ \begin{array}{c} u^{(1)}_k(0) u^{(2)}_k(0) \\ u^{(1)}_k(0) u^{(2)}_k(1) \\ \vdots \\ u^{(1)}_k(N_1-1) u^{(2)}_k(N_2-1) \end{array} \right] \\
& = (\lambda^{(1)}_k + \lambda^{(2)}_k) \left[ \begin{array}{c} u^{(1)}_k(0) u^{(2)}_k(0) \\ u^{(1)}_k(0) u^{(2)}_k(1) \\ \vdots \\ u^{(1)}_k(N_1-1) u^{(2)}_k(N_2-1) \end{array} \right]
\end{align}

This property holds for any \(k_1 = 0, \ldots, N_1-1\) and \(k_2 = 0, \ldots, N_2-1\). The resulting eigenvalues \(\{\lambda^{(1)}_k + \lambda^{(2)}_k\}\) are nonnegative, and the corresponding eigenfunctions \(\{u^{(1)}_k \otimes u^{(2)}_k\}\) are orthonormal. These conclusions can be straightforwardly derived from the fundamental properties of the Kronecker product.

A similar approach applies to decomposing an eigenproblem related to an adjacency matrix of a product graph. If the adjacency matrix $W_n$ has eigenvalues $\{\mu^{(n)}_k\}_{k=0}^{N_n-1}$ and orthonormal eigenfunctions $\{\mathbf{v}^{(n)}_k\}_{k=0}^{N_n-1}$ for $n=1,2$, the Kronecker sum $W_1 \oplus W_2$ yields the eigenvalues $\{\mu^{(1)}_k + \mu^{(2)}_l\}$ and corresponding eigenfunctions $\{\mathbf{v}^{(1)}_k \otimes \mathbf{v}^{(2)}_l\}$ defined on $V_1 \times V_2$.

The Cartesian product graph is represented as $G_1 \times G_2$, where $G_n$ for $n=1,2$ is an undirected weighted graph with a vertex set $V_n = \{0,1,\dots,N_n-1\}$. Assuming its graph Laplacian $L_n$ has eigenvalues $\{\lambda_k^{(n)}\}_{k=0}^{N_n-1}$ and corresponding orthonormal eigenfunctions $\{u_k^{(n)}\}_{k=0}^{N_n-1}$. The GFT of a graph feature $f:V_1 \times V_2 \rightarrow \mathbb{R}$ on the product graph $G_1 \times G_2$ is represented by:

\begin{equation}
    \hat{f} : (\sigma(L_1) \otimes \sigma(L_2)) \rightarrow \mathbb{C}
\end{equation}


\begin{equation} \label{Eq:9}
\hat{f}(\lambda^{(1)}_{k_1} + \lambda^{(2)}_{k_2}) = \sum_{i_1=0}^{N_1-1} \sum_{i_2=0}^{N_2-1} f(i_1,i_2) \cdot u^{(1)}_{k_1}(i_1) \cdot u^{(2)}_{k_2}(i_2)
\end{equation}

for $k_1=0,\dots,N_1-1$ and $k_2=0,\dots,N_2-1$, and its inverse is:

\begin{equation}\label{Eq:10}
f(i_1,i_2) = \sum_{k_1=0}^{N_1-1} \sum_{k_2=0}^{N_2-1} \hat{f}(\lambda^{(1)}_{k_1} + \lambda^{(2)}_{k_2}) \cdot u^{(1)}_{k_1}(i_1) \cdot u^{(2)}_{k_2}(i_2)
\end{equation}

Thus, we interpret features on a Cartesian product graph as 2D features and propose a Twin GFT, enabling multiclass failure classification in Network Digital Twin environments.

\textbf{Definition 1} The Twin Graph Fourier Transform (Twin-GFT) of a function $f: V_1\times V_2\rightarrow \mathbb{R}$ on a Cartesian product graph $G_1 \square G_2$ is a spectral representation $\hat{f}:\sigma(L_1)\times \sigma(L_2) \rightarrow \mathbb{C}$ defined as follows:


\begin{equation}
    \hat{f}(\lambda_{11},\lambda_{22}) = \sum_{i_1=0}^{N_1-1} \sum_{i_2=0}^{N_2-1} f(i_1,i_2) \cdot u_{k_{11}}^{(1)}(i_1) \cdot u_{k_{22}}^{(2)}(i_2)
\end{equation}

for $k_{11}=0,\dots,N_1-1$ and $k_{22}=0,\dots,N_2-1$, and its inverse is given by:

\begin{equation}
    f(i_1,i_2) = \sum_{k_1=0}^{N_1-1} \sum_{k_2=0}^{N_2-1} \hat{f}(\lambda_{k_{11}}^{(1)} + \lambda_{k_{22}}^{(2)}) \cdot u_{k_{11}}^{(1)}(i_1) \cdot u_{k_{22}}^{(2)}(i_2)
\end{equation}

for $i_1=0,\dots,N_1-1$ and $i_2=0,\dots,N_2-1$.

The twin-GFT can be represented as a series of matrix-matrix multiplications. Using $N_1 \times N_2$ matrices $F=(f(i_1,i_2))_{i_1,i_2}$ and $\hat{F}=(\hat{f}(\lambda_{k_{11}}^{(1)} + \lambda_{k_{22}}^{(2)}))_{k_{11},k_{22}}$, the twin-GFT applied to the features $f$ is expressed as follows:

\begin{equation}
\hat{F} = U_1^* FU_2,
\end{equation}

where $U_n$ denotes an $N_n \times N_n$ unitary matrix with $(i,k)$-th element $u_{k}^{(n)}(i)$ for $n=1,2$. Then its inverse transform is given by:

\begin{equation}
    F = U_1 \hat{F}^2 U_2^*,
\end{equation}

The twin GFT has connections to existing transformations. First, when both factor graphs are cycle graphs, the twin GFT can be equivalent to the 2D-Discrete Fourier Transform and the 2D-GFT. Second, when a factor graph is a cycle graph, some cycles might be nested within other cycles, creating a complex network structure. The twin GFT is known as a joint graph and temporal Fourier transform which generalizes these existing transformations. 

\section{PROPOSED CF-GNN} \label{3}
Class-Fourier graph Neural Networks (CF-GNNs) perform well with both homophilic and heterophilic graphs and have shown good expressive capacities. However, current methods often lack adaptability. In particular, conventional approaches use a single, globally shared spectral filter $\hat{g}$, trained across the entire graph, with fixed frequency coefficients $\{\gamma_k\}_{k=0}^K$. Node-specific changes are not considered in the global filter, which is unable to distinguish between the distinct local structures connected to each node during the filtering process. Although polynomial-parameterized spectral filter learning offers a certain level of localization, it might not adequately capture subtle and diverse local structural patterns. It is intuitively reasonable to train a class-specific filter $\hat{g}_i(\lambda_l)$ rather than depending on a globally consistent spectral filter $\hat{g}(\lambda_l)$. A more efficient method is provided by $\hat{g}_i(\lambda_l)$, tailored for each node $i$ to adaptively capture its local patterns. To mitigate this limitation, we revisit globally consistent spectral graph transforms in this section and propose a spectral filter learning strategy in the context of CF-GNNs.

\subsection{Class-Fourier Transform Filtering}
We present an adaptive localized spectral filtering method designed for Class-Fourier Transform Filtering on graph \( G \), which the generalized translation operator inspires in Graph Signal Processing (GSP) in \cite{zheng2023node}. By leveraging polynomial-parameterized spectral filtering from \cite{guo2024rethinking}, this technique ensures a more accurate and flexible representation of local patterns by fully accounting for the unique influence of the node at which the filter is applied. We introduce a generalized translation operator \( T_i \) for any signal \( g \in \mathbb{R}^n \) defined on a given graph \( G \) and any \( i \in \{0, 1, \dots, n-1\} \): \(T_i : \mathbb{R}^n \to \mathbb{R}^n\)

Using a Kronecker delta function in (\ref{Eq:9}) and (\ref{Eq:10}) \( \delta_i \) centered at the \( i \)-th node \( v_i \), we express generalized convolution as:

\begin{equation}
    T_i(g) := \sqrt{N} \, (g \ast \delta_i) = \sqrt{N} \sum_{l=1}^{n} u_l u_l^\top(i) \hat{g}(\lambda_l)
\end{equation}

where \( u_l^\top(i) \) denotes the \( i \)-th element of the eigenvector \( u_l \) corresponding to eigenvalue \( \lambda_l \), and \( \hat{g}(\lambda_l) \) represents the spectral domain representation of the signal \( g \).

In the context of Class-Fourier Transform Filtering, adaptive local filtering is achieved by first centering the filter signal \( g \) at the target node \( v_i \) using the generalized translation operator \( T_i \), and then performing spectral convolution with \( x \) as follows:

\begin{equation} \label{Eq:16}
    x \ast G T_i(g) = \sum_{l=1}^N u_l \, x^{(\lambda_l)} u_l^\top(i) g^{(\lambda_l)}
\end{equation}

where \( x^{(\lambda_l)} \) is the spectral domain representation of the input signal \( x \), and \( u_l^\top(i) \) is the \( i \)-th element of the eigenvector \( u_l \), which corresponds to the eigenvalue \( \lambda_l \).

Establishing that: \( \hat{g}_i(\lambda_l) = \sqrt{N} \, u_l(i) \, \hat{g}(\lambda_l) \) in \ref{Eq:16}, the equation above becomes:

\begin{equation}
    x \ast G T_i(g) = \sum_{l=1}^N u_l \, x^{(\lambda_l)} g_i^{(\lambda_l)}
\end{equation}

As evident from the definition of \( \hat{g}_i(\lambda_l) \), \( \hat{g}_i \) is a function associated with \( U \). Thus, by approximating \( U \), we can further establish the relationship between \( \hat{g}_i \) and \( x \).
Recall the Fourier transform of an inverted graph, which allows us to deduce that \(x_i = U_{i:} \cdot \hat{x},\) where \( U_{i:} \) denotes \( U \)'s \( i \)-th row. Therefore, \( U_{i:} \) can be approximately from \( x_i \) using \(U_{i:} \approx x_i q,\)where \( \hat{x} \)'s pseudoinverse is \( q = \text{pinv}(\hat{x}) \in \mathbb{R}^n \). Meanwhile, take notice that \( u_l(i) = \tilde{u}_l(i) \), which implies that \( \hat{g}_i(\lambda_l) \) can be approximately expressed as \(\hat{g}_i(\lambda_l) \approx \ldots\).




\begin{equation} \label{Eq:18}
    \hat{g}_i(\lambda_l) \approx \tilde{g}_i(\lambda_l) = \sqrt{N} (x_i q_l) \hat{g}(\lambda_l)
\end{equation}

Where the \( q_l \) denotes the \( l \)-th element of \( \mathbf{q} \). Consequently, a particular filter \( \tilde{g}_i \) corresponding to node \( v_i \), can approximate \( \hat{g}_i \) while simultaneously incorporating the influence of feature \( x_i \) associated with to node \( v_i \) in the filter coefficient estimation process.

Without loss of generality, the \( K \)-order polynomial approximation \( \hat{P}_K(\cdot) \) can be utilized to directly parameterize \( \tilde{g}_i \) without sacrificing generality; that is,\(\tilde{g}_i = \sum_{k=0}^{K} \eta_{i,k} \hat{p}_k(\Lambda),\) where \( \eta_{i,k} \) represents the coefficient of \( \hat{p}_k(\Lambda) \) for the \( \tilde{g}_i \) filter. We only pay attention to the convolution value of the filter placed at each node is our focus, i.e.,\(z_i = \delta_i (U \tilde{g}_i U^\top x).\) In this case, we utilize \( \delta_i \) as an indicator vector \(\delta_i = [0, \ldots, 1_i, \ldots, 0] \in \mathbb{R}^n,\) which represents a row vector with only the \( i \)-th element being 1 and the remaining elements being zeros. Therefore, the following is a formulation of class-oriented filtering:

\[
z_i = \delta_i \big( U \tilde{g}_i U^\top x \big) = \delta_i U 
\sum_{k=0}^{K} \eta_{i,k} \hat{p}_k(\Lambda) U^\top x
\]
\begin{equation} \label{Eq:19}
    = \delta_i \sum_{k=0}^{K} \Psi_{i,k} \hat{p}_k(L)x
\end{equation}

Where the trainable coefficient matrix is \( \Psi = [\eta_{i,k}] \in \mathbb{R}^{n \times (K+1)} \) . In practice, 

\begin{equation} \label{Eq:20}
    Z_i = \delta_i \sum_{k=0}^{K} \Psi_{i,k} \hat{p}_k(L) X
\end{equation}

Eq (\ref{Eq:20}) can still be applied to the feature matrix \( X \).


\begin{algorithm}[H]
\caption{CF-GNN: Class Fourier Graph Neural Network}
\KwIn{Symmetric matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$}
\KwIn{Eigenvalues $\lambda_i$ and eigenvectors $\mathbf{v}_i$}
\KwOut{Classification result $C(v), v \in V_{\text{train}}$}

// \textit{Initialization}

Set initial eigenvector guess: $\mathbf{v}^{(0)} \gets \text{random unit vector}$ \\
Set tolerance $\epsilon > 0$ \\
Set iteration count $t \gets 0$ \\

// \textit{Train CF-GNN}
\For{$l = 1, 2, \dots, L$}{  
    \For{$e = 1, 2, \dots, E$}{  
        // \textbf{GNN training}  \\
        $h_v^{(l)} \gets$ \text {Fourier transform on node embeddings};  \\
        $\mathcal{L}_{\text{GNN}}$ using spectral convolution;  \\
        $\mathcal{F}(v)$ Compute Fourier coefficients;  \\
        
        // \textbf{Feature embedding transformation}  \\
        $X^{(l)} \gets$ Transformed Fourier domain features;  \\
        $p_k^{(l)}(v) \gets$ Compute probability vector;  \\
        Calculate feature update loss $\mathcal{L}_{\text{MLP}}$;  \\
        Compute overall loss of CF-GNN $\mathcal{L}_{\text{CF-GNN}}$;  \\
        
        // \textbf{Eigenvalue and eigenvector computation} \\
        \While{not converged}{
            Compute $\mathbf{w}^{(t)} \gets \mathbf{A} \mathbf{v}^{(t)}$ \\
            Compute eigenvalue estimate: $\lambda^{(t)} \gets \frac{\mathbf{w}^{(t)} \cdot \mathbf{v}^{(t)}}{\|\mathbf{v}^{(t)}\|}$ \\
            Normalize eigenvector: $\mathbf{v}^{(t+1)} \gets \frac{\mathbf{w}^{(t)}}{\|\mathbf{w}^{(t)}\|}$ \\
            Check convergence: \textbf{if} $\|\mathbf{v}^{(t+1)} - \mathbf{v}^{(t)}\| < \epsilon$ \textbf{then stop} \\
            Update iteration count: $t \gets t + 1$
        }
        
        // \textbf{Class-specific spectral filtering} \\
        For each node $v_i$, \\
        compute Eq (\ref{Eq:18}) spectral filter $\tilde{g}_i(\lambda_l)$
        
        // \textbf{Polynomial approximation of localized filter} \\
        Approximate $\tilde{g}_i$ using a $K$-order polynomial in Eq(\ref{Eq:19}):        
        // \textbf{Class-oriented filtering} \\
        Compute filtered output (\ref{Eq:20}) for node $v_i$:
    }
}
\textbf{Ensemble classification result} $H(v) \gets$ from all weak classifiers;  

\end{algorithm}


\subsection{Experimental Settings} \label{Experiment}

\subsection{Datasets and preprocessing}

\textit{1) KDDI datasets}: The KDDI dataset is based on the ITU Challenge "ITU-ML5G-PS-008: Network Failure Classification Model Using network digital twin" \cite{Junichi:2023}. The datasets consist of the following: a dataset (Domain A) derived from network digital twin simulated environments and a dataset (Domain C) generated from a real network. Each domain dataset comprises 4121 features, with 16 failure classes per failure sample. However, the failure-type classes consist of a "normal" class and 15 other classes, with over 67\% of the data belonging to the "normal" class. In comparison, each of the remaining 15 failure classes comprises only 2.2\% of the datasets. Table \ref{tab:a1} represents the distribution table for the two datasets 

\textit{2) Pre-processing:} This section details the data preprocessing stage of the system. This stage consists of two subprocesses: one-hot encoding for categorical classes, and feature scaling for normalization.
The system transforms nominal attributes into one-hot vectors. Each nominal (categorical) attribute is represented as a binary vector with a size equal to the number of attribute values. In this binary vector, only one point corresponds to the expressed value, which is assigned a value of 1, whereas all other points are assigned a value of 0. For instance, for the failure injection attribute commonly used in network failure classification, with the failure points pf the process and mobility management function “amf,” authentication server function “ausf,” and unified data management “udm,” the attribute is transformed into a binary vector of length 3. The attribute values are converted to [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively.

The system scales the numeric attributes alongside the one-hot encoding process. Normalization methods, such as normalization (e.g., \cite{ieracitano2020novel}) and standardization (e.g., \cite{gao2020omni}) can be considered for scaling numeric features. We adopted the min-max normalization method. The normalization function $\mathcal{f}_A(\cdot)$ for a numeric attribute $A$ that maps every $x$ in $A$ into the range [0, 1] can be defined as follows:
\begin{equation}
f_A(x_i) = \tilde{x}_i = \frac{\text{max}(x_j) - \text{min}(x_j)}{x_i - \text{min}(x_j)},
\end{equation}

where, $x_i$ denotes the $i$th attribute value in attribute $A$.


\begin{table}[h]
    \centering
    \caption{Dataset Distribution Table}
    \label{tab:a1}
    \begin{tabular}{c|c|c|c}
        \hline
        \textbf{Dataset} & \textbf{Failure events} & \textbf{Features} &\textbf{IR} \\
        \hline
        \multirow{2}{*}{Domain A (Training)} & Normal = 2456 
        &\multirow{2}{*}{4121} 
        &\multirow{2}{*}{0.4886} \\
        & Failure = 1186 & \\
        \hline
        \multirow{2}{*}{Domain C (Training)} & Normal = 588 
        &\multirow{2}{*}{4121} 
        &\multirow{2}{*}{0.4847} \\
        & Failure = 285 & \\
        \hline
       
    \end{tabular}
\end{table}

\subsection{Comparison with the baseline methods}
The CF-GNN presented in this article is fundamentally a GNN model designed to identify failure classes in a network digital twin setting. The primary scientific problem addressed is graph representation learning for imbalanced graphs. We compared our results with cutting-edge baseline techniques that include: 1) general GNN models GCN \cite{kipf2016semi} and GAT \cite{velickovic2017graph}; 2) Four imbalance-based models, including PC-GNN \cite{liu2021pick}, Reweight \cite{he2024gradient}, GraphSMOTE \cite{zhao2021graphsmote}, and GATE-GNN \cite{fofanah2024addressing} as state-of-art GNN imbalance classification models.

\begin{itemize}
    
    \item \textit{GCN}: GCN aggregates information and updates its presentation of data from the node's neighbors, repeating this process across several layers.
    \item \textit{GAT}: GAT offers more accurate and nuanced representations of the graph data compared to earlier GNN by enabling the network to learn which neighbors are most significant for each node.
    \item \textit{Rewight}: The core idea of gradient reweighting is to mitigate the effects of class imbalance by adjusting the gradient updates during training.
    \item \textit{GraphSMOTE}: GraphSMOTE is a technique that generates synthetic nodes while preserving the graph structure to address the challenge of imbalanced node classification in graphs.
    \item \textit{PC-GNN}: The model is exposed to a more balanced training input by sampling balanced sub-graphs, which keeps the majority classes dominating the learning process.
    \item \textit{GATE-GNN}: GATE-GNN model for imbalanced node classification. It creates dynamic node interaction within the GATE architecture using learnable weight parameters.
    \item \textit{CF-GNN}: Our proposed CF-GNN model improves performance, particularly for minority classes in imbalanced graph classification, by leveraging the Fourier transform to examine class-specific frequency features.
\end{itemize}

\textit{Evaluation Metrics}: The selection of evaluation metrics is essential when addressing imbalanced datasets. These metrics must not only provide a fair assessment of classification results across all classes but also pay special attention to the performance of minority classes \cite{isahgft}. To ensure an objective evaluation, we have chosen five widely used metrics: Precision, Recall, F1-score, G-Mean, and MCC.

\begin{equation}
         \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
    \end{equation}
   
    
    \begin{equation}
         \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
   
    
    \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    
  
    \begin{equation}
         \text{F1-score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}

where TP, TN, FN, and FP denote the true positive, true negative, false negative, and false positive values, respectively. In the CF-GNN experiments, we evaluated each model using standard performance metrics. The precision, recall, and F1-score metrics vary significantly across classes, indicating differences in how well the model predicts each class. This heterogeneity in performance suggests that the model may perform well for some classes and less effectively for others.

Geometric-Mean (G-Mean) is computed using a True Positive Rate
(TPR) and True Negative Rate (TNR).

\[
\text{G-Mean} = \sqrt{\text{TPR} \cdot \text{TNR}} = \sqrt{\frac{\text{TP}}{\text{TP} + \text{FN}} \cdot \frac{\text{TN}}{\text{TN} + \text{FP}}}.
\]

A higher score for the aforementioned criteria indicates better model performance on imbalanced problems.

The Matthews Correlation Coefficient (MCC) generalizes the confusion matrix for multiclass classification. The generalized equation is:

\[
\text{\scriptsize{
\(\text{MCC} = \frac{\sum_{k} \sum_{l} \sum_{m} C_{kk} C_{lm} - C_{kl} C_{mk}}{\sqrt{\left( \sum_{k} \left( \sum_{l} C_{kl} \right) \left( \sum_{l \neq k} C_{lk} \right) \right) \cdot \left( \sum_{k} \left( \sum_{l} C_{lk} \right) \left( \sum_{l \neq k} C_{kl} \right) \right)}}\)
}}
\]
where \( k, l, m \) are indices corresponding to certain classes and \(C_{kl}\) are elements of the confusion matrix. The MCC is particularly suitable for multiclass imbalanced situations because it ensures a balanced assessment across all classes.

The Classification Mean Accuracy (cmA) computes per-class recall and averages it over all $K$ classes.

\begin{equation} \label{cmA}
    \text{cmA} = \frac{1}{K} \sum_{k=1}^{K} \frac{\text{TP}_k}{\text{TP}_k + \text{FN}_k}
\end{equation}

\section{Experiments} \label{4}

This section provides an empirical analysis of the CF-GNN using two 5G network digital twin datasets. It begins with a description of the experimental setup, including the datasets and evaluation metrics, followed by a detailed evaluation and analysis of the experimental results.


\begin{table}[ht]
    \centering
    \caption{Training Parameters}
    \label{tab: Param}
    \begin{tabular}{>{\centering\arraybackslash} p{3cm}|c} % Adjust the width as needed
        \toprule
        Parameters & Value \\
        \midrule
        Dimension & Hidden\_dim=64 \\ \hline
       Learning rate & lr = 0.001  \\ \hline
       Optimizer & Adam optimizer \\ \hline
       Loss function &Cross-entropy loss \\ \hline
       Number of epochs & num\_epochs = 500 \\ \hline
       Dimension & Hidden\_dim \\ \hline
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Overall performance comparison}

In this section, we evaluate the effectiveness of the proposed CF-GNN technique, its performance was compared with the state-of-the-art models using the class balanced accuracy metric (cmA) for handling imbalanced classification tasks. The results shown in Figure \ref{fig: Perform a} and \ref{fig: Perform c} demonstrate that conventional models such as GCN and GAT, struggled with imbalanced datasets, often plateauing at lower accuracy levels. For example, GAT exihibits fluctuations during training, indicating instability, while GCN reaches an early performance maximum and fails to adapt effectively to minority classes. In contrast, other methods like PC-GNN and GraphSMOTE achieve better performance due to their enhanced node representations and oversampling approaches. However, techniques such as Reweight show significant instability, particularly in the initial stages of training, suggesting that merely adjusting loss functions is insufficient to address class imbalance effectively. Although GATE-GNN, which aggregates node features using adaptive weights, performs competitively, it still exhibits some inconsistencies during the training.

\begin{figure}[htbp]
        \centering
        \includegraphics[width=10cm]{Images/Performance_A.png}
        \caption{Classification Mean Accuracy (cmA) from (Domain C) models performance comparison}
        \label{fig: Perform a}
    \end{figure}

\begin{figure}[htbp]
        \centering
        \includegraphics[width=10cm]{Images/Performance_C.png}
        \caption{Classification Mean Accuracy (cmA) from (Domain C) models performance comparison}
        \label{fig: Perform c}
    \end{figure}

On the other hand, CF-GNN (ours) consistently outperforms all other models across every dataset. It avoids the erratic fluctuations observed in Reweight and other baselines, achieving a smoother and more stable convergence. The main advantages of CF-GNN lies in its class-oriented filtering mechanism, where the algorithm iterates over classes to ensure the filter learns representations for each class. Moreover, this method ensures that the spectral filtering process focuses on class-dependent graph structure and features, leading to better generalization across tasks and improved classification accuracy, particularly for minority classes. The performance results demonstrate that CF-GNN outperforms the ebst baseline models by 1-3\% in cmA, highlighting its effectiveness in addressing the challenges posed by long-tail distributions. This enhancement positions CF-GNN as a reliable and scalable solution for practical network digital twin applications, underscoring the importance of integrating spectral approaches with graph-based imbalance classification.


\subsection{Imbalanced ratio influence}

The models' performance across a range of imbalance ratios provides valuable insights into their reliability and efficiency in handling network digital twin datasets. The percentage of minority class samples to the majority class, or the imbalance ratio, has a direct effect on how well the models generalize and categorize instances of minority classes. The findings demonstrate how sensitive certain models are to class imbalance, with some maintaining steady performance across a range of imbalance ratios while others show variations.

CFGNN demonstrates a high degree of adaptability in various imbalance ratios on real-network datasets.At an imbalance ratio of 0.1, CF-GNN achieves an accuracy of 79.07\%, which is 7.74\% higher than GCN 68.74\% but lower than PC-GNN (93.13\%), Reweight (80.69\%), and GraphSMOTE (84.36\%). The superior performance of PC-GNN, which outperforms CF-GNN by 13.92\%, can be attributed to its strategy of assigning probabilities to each node inversely proportional to their label frequencies. As the imbalance ratio increases to 0.3, CF-GNN (93.38\%) shows a significant improvement of 36.44\% over GCN (68.43\%) and outperforms GAT (70.29\%) by 32.81\%, demonstrating the effectiveness of its spectral filtering capabilities in moderately imbalanced scenarios. However, GraphSMOTE (86.98\%) and Reweight Model (86.03\%) remain competitive due to their reweighting strategies, while PC-GNN (86.75\%) and GATE-GNN (92.05\%) also perform well.


At an imbalance ratio of 0.5, CF-GNN achieves an accuracy 95.38\%, significantly surpassing GCN (68.13\%) by 39.96\% and outperforming GAT 87.25\% by 9.30\%. PC-GNN 94.07\% and GraphSMOTE 88.79\% remain competitive at this level, while GATE-GNN 77.36\% lags behind. This indicates that CF-GNN remains robust even in moderately imbalanced settings. However, as the imbalance ratio increases to 0.7, CF-GNN exhibits a slight decline, achieving 89.87\%. Despite this, it still outperforms GCN 67.83\% by 32.44\%. PC-GNN 94.53\% and GATE-GNN 93.87\% continue to perform strongly, while GraphSMOTE 86.65\% outperforms CF-GNN by 3.58\%, highliting the benefits of oversampling techniques can be beneficial in highly imbalanced ratio. At an imbalance ratio of 0.9, CF-GNN achieves 90.85\%, significantly surpassing GCN 67.54\% by 34.55\% and outperforming GAT 85.84\%. However, Reweight Model 93.46\% and PC-GNN 81.26\% remain competitive. This suggests that CF-GNN's spectral filtering strategy remains effective, even in extreme imbalance scenarios.

\begin{table}[htbp]
    \centering
    \caption{Real-network dataset results on different imbalance ratios (bold: best, underline: runner-up)}
    \label{tab:4}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Methods} & \textbf{0.1} & \textbf{0.3} & \textbf{0.5} & \textbf{0.7} & \textbf{0.9} \\
        \midrule
        GCN & 0.6874 & 0.6843 & 0.6813 & 0.6783 & \underline{0.6754} \\
        GAT & 0.8847 & 0.7029 & 0.8725 & 0.8512 & 0.8584 \\
        GraphSMOTE & 0.8436 & \underline{0.8698} & \underline{0.8879} & 0.8665 & \textbf{0.8126} \\
        PC-GNN & \textbf{0.9313} & 0.8675 & 0.9407 & 0.9453 & \underline{0.8126} \\
        Reweight & \underline{0.8069} & 0.8603 & 0.8879 & 0.7593 & \textbf{0.9346} \\
        GATE-GNN & 0.8049 & \textbf{0.9205} & 0.7736 & \underline{0.9387} & 0.8802 \\
        CF-GNN & 0.7406 & \underline{0.9338} & \textbf{0.9538} & \textbf{0.8987} & 0.9085 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{NDT dataset results on different imbalance ratios (bold: best, underline: runner-up)}
    \label{tab:5}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Methods} & \textbf{0.1} & \textbf{0.3} & \textbf{0.5} & \textbf{0.7} & \textbf{0.9} \\
        \midrule
        GCN & 0.6874 & 0.6843 & 0.6813 & 0.6783 & 0.6753 \\
        GAT & 0.7825 & 0.7029 & 0.7906 & \textbf{0.8512} & 0.8584 \\
        GraphSMOTE & 0.6874 & 0.6843 & 0.6813 & \underline{0.8053} & 0.6753 \\
        PC-GNN & \textbf{0.8847} & 0.7891 & \underline{0.7908} & 0.7561 & 0.7707 \\
        Reweight & \underline{0.8055} & 0.7177 & 0.7373 & 0.7302 & \textbf{0.7763} \\
        GATE-GNN & 0.7548 & \textbf{0.7855} & 0.7517 & 0.6783 & 0.7261 \\
        CF-GNN & \underline{0.7907} & \underline{0.7871} & \textbf{0.8725} & 0.7564 & \underline{0.7996} \\
        \bottomrule
    \end{tabular}
\end{table}

CF-GNN demonstrates a distinct performance pattern on network digital twin data across various imbalance ratios. At an imbalance ratio of 0.1, CF-GNN achieves an accuracy 79.07\%, surpassing GraphSMOTE 68.74\% and GCN 68.74\% by 15.01\%. This demonstrates the effectiveness of spectral filtering even in the absence of data augmentation. However, PC-GNN 88.47\% and Reweight Model 80.55\% perform better, suggesting that adaptive reweighting techniques and neighbor sampling strategies, which select neighboring nodes that are inversely proportional to their label frequencies, are more effective in these scenarios. As the imbalance ratio increases to 0.3, CF-GNN maintains strong performance at 78.71\%, outperforming GCN 68.43\% by 15.09\% and closely matching PC-GNN 78.91\%. However, GATE-GNN 78.55\% slightly outperforms CF-GNN, while Reweight Model 71.77\% underperformed. This indicates that attention-based models like GAT 70.29\% might be better suited for learning complex dependencies in network digital twin data.

At an imbalance ratio of 0.5, CF-GNN achieves 87.25\%, surpassing GAT 79.06\% by 10.37\% and outperforming GCN 68.13\% by 28.10\%. CF-GNN also exceeds PC-GNN (79.08\%), making it the top performer at this level. However, GraphSMOTE 68.13\% and GATE-GNN 75.17\% struggle, suggesting that synthetic oversampling is less effective for network digital twin compared to real-world network data. As the imbalance ratio increases to 0.7, CF-GNN sees a slight decline, achieving 75.64\%. Despite this, it remains competitive with PC-GNN 75.61\% and outperforms GCN 67.83\% by 11.50\%. Interestingly, GraphSMOTE 80.53\% performs better than CF-GNN, suggesting that oversampling techniques become more beneficial at higher imbalance levels in network digital twin datasets. However, GATE-GNN (67.83\%) falls behind, showing that its approach does not generalize well in highly imbalanced conditions.

At an extreme imbalance ratio of 0.9, CF-GNN maintains an accuracy of 79.96\%, surpassing GCN 67.53\% by 18.44\%. However, it falls behind Reweight Model 77.63\% and PC-GNN 77.07\%. While CF-GNN remains one of the top-performing models, GraphSMOTE 67.53\% performs the worst, indicating that synthetic oversampling struggles in severely imbalanced network digital twin data. Overall, CF-GNN proves to be highly adaptable across different imbalance scenarios, excelling particularly on real network data while maintaining competitive results on network digital twin data. However, GATE-GNN and reweighting techniques demonstrate better performance at extreme imbalance levels, highlighting their effectiveness in handling severe class imbalances.


\subsection{Multi-class classification results}


\begin{table*}[htbp]
    \centering
    \caption{Classification Results for the training in a Network Digital Twin Environment (Domain A) and Real Network Data (Domain C)}
    \label{tab A}
    \begin{tabular}{c|c|ccc|ccc}
            \hline \hline
            & \multicolumn{1}{c|}{Datasets} & \multicolumn{3}{c}{Domain A} & \multicolumn{3}{c}{Domain C} \\  \cline{1-8}
              Classes & Sub-Types & Pre  & Recall & F1 & Pre & Recall & F1 \\
             \hline
0 & amfx1\_bridge-delif & 0.99 & 0.86 & 0.92 & 0.88 & 0.70 & 0.78 \\
1 & amfx1\_ens5\_inter-down & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
2 & amfx1\_ens5\_inter-loss-70 & 0.91 & 0.87 & 0.89 & 1.00 & 0.90 & 0.95 \\
3 & amfx1\_memory-stress-start & 1.00 & 0.99 & 0.99 & 1.00 & 1.00 & 1.00 \\

4 & amfx1\_vcpu-overload-start & 0.95 & 0.91 & 0.93 & 1.00 & 0.90 & 0.95 \\
5 & ausfx1\_bridge-delif & 0.68 & 0.52 & 0.59 & 0.82 & 0.90 & 0.86 \\
6 & ausfx1\_ens5\_inter-down & 1.00 & 1.00 &  1.00 & 1.00 & 1.00 & 1.00 \\
7 & ausfx1\_ens5\_inter-loss-70 & 0.69 & 0.25 & 0.37 & 1.00 & 1.00 & 1.00 \\
8 & ausfx1\_memory-stress-start & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
9 & ausfx1\_vcpu-overload-start & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
10 & normal & 0.94 & 1.00 & 0.97 & 0.97 & 1.00 & 0.98 \\
11 & udmx1\_bridge-delif & 0.92 & 0.58 & 0.71 & 0.83 & 0.50 & 0.62 \\
12 & udmx1\_ens5\_inter-down & 1.00 & 1.00 & 1.00 & 1.00 & 0.90 & 0.95 \\
13 & udmx1\_ens5\_inter-loss-70 & 0.95 & 0.25 & 0.40 & 1.00 & 0.80 & 0.89 \\
14 & udmx1\_memory-stress-start & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
15 & udmx1\_vcpu-overload-start & 0.92 & 1.00 & 0.96 & 1.00 & 1.00 & 1.00 \\
             \hline
      \hline  
        & \textbf{Macro avg}  & 0.99 & 0.83 & 0.86 & 0.97 & 0.91 & 0.94 \\
      &  \textbf{weighted avg}  & 0.99 & 0.96 & 0.97 & 0.97 & 0.97 & 0.97 \\
    \hline
    \end{tabular}
\end{table*}

The classification reports provide the precision, recall, and F1-score for every class in the Domain A and Domain C datasets, offering a comprehensive performance analysis across domains. In Table \ref{tab A}, the performance on the training data from the network digital twin environment in Domain A is slightly lower compared to the real network environments Domain C, with an average F1-score of 0.93. Precision and recall values are also marginally lower, around 0.94, indicating that the model performs well but may struggle to identify certain classes accurately. Precision measures the proportion of TP predictions out of all positive predictions (TP and FP) made by the model. In the Domain A dataset, Classes 1, 3, 6, 8, 9, 12, and 14 achieved perfect precision (1.00), indicating that the model correctly identified all instances belonging to these classes without any false positives. However, Classes 5, 7, and 13 have relatively lower precision values, suggesting that the model's predictions for these classes include more false positives.
In the Domain C dataset, as shown in Figure \ref{fig: Network of Digital Twin (Domain A)}, most classes achieved high precision values, with Classes 1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, and 15 achieving perfect precision (1.00). However, Class 0 has a lower precision value, indicating that the model's predictions for this class include more false positives.

%================ Confusion Matrix ===================


\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm]{Images/Domain_A.png}
    \caption{Confusion matrix of the network of digital twin environment (Domain A)}
    \label{fig: Network of Digital Twin (Domain A)}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm]{Images/Domain_C.png}
    \caption{Confusion matrix obtained from the real network (Domain C)}
    \label{fig: Real Network Data (Domain C)}
\end{figure}


Recall, also known as sensitivity, measures the proportion of true positive (TP) predictions out of all actual positive instances (TP and false negatives, FN) in the dataset. In Figure \ref{fig: Real Network Data (Domain C)}, classes 1, 3, 6, 8, 9, 12, 14, and 15 achieved perfect recall (1.00), indicating that the model correctly identified all instances of these classes. However, classes 0, 2, 4, 5, 7, 11, and 13 have lower recall values, suggesting that the model missed some instances of these classes. The F1-score, which is the harmonic mean of the precision and recall, provides a balanced measure of the two metrics. This is particularly useful for datasets with class imbalance. The weighted average F1-score for the Domain A dataset is 0.93, indicating good overall performance across all classes. The weighted average F1-score for the Domain C dataset is 0.97, reflecting excellent performance across all classes. 

Overall, both datasets demostrate strong performance in terms of precision, recall, and F1-score results for most classes, indicating that the models effectively classify instances across different categories. The results suggest that the model accurately identifies and classifies the majority of data points. However, Class 0 has the lowest recall (0.63) compared to the others, meaning the model might be missing a significant portion of actual Class 0 instances, misclassifying them into other categories. Similarly, Class 5 has a lower recall (0.68) and F1 score (0.81), and class 13 has a slightly lower recall (0.95) compared to others. Further investigation into Classes 0, 5, and 13 is recommended to improve the classification of these classes and enhance the overall model effectiveness of the data.

\begin{figure}[htbp]
        \centering
       \includegraphics[width=12cm]{Images/Domain_A_ROC.png}
        \caption{ROC Curve analysis obtained from a network of digital twin}
        \label{fig: ROC Curve analysis obtained from a network of digital twin}
    \end{figure}
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=12cm]{Images/Domain_C_ROC.png}
        \caption{ROC curve analysis obtained from real network (Domain C)}
        \label{fig: ROC Curve analysis obtained from real network dataset}
    \end{figure}
 
    

The confusion matrices in Figure \ref{fig: Network of Digital Twin (Domain A)} and \ref{fig: Real Network Data (Domain C)} illustrate the training performance of each failure class in Domains A and Domain C. A high precision indicates that the model makes fewer false positive predictions. In Figure \ref{fig: Network of Digital Twin (Domain A)}, precision varies across classes, with some classes achieving high precision (e.g., Classes 0, 8, 9, 12, 14, 15) and others having lower precision (e.g., Classes 5, 7, 11, 13). Similarly, in Figure \ref{fig: Real Network Data (Domain C)}, precision varies across classes, but overall, it remains high for most classes.

However, the precision, recall, and F1-score remain consistently high at 97\% for the real network dataset, demonstrating robust performance.  In contrast, the performance on the network digital twin dataset (Domain A) is slightly lower than in Domain C, with an accuracy of 94.27\%. The precision, recall, and F1-score also show a slight decline, averaging 94\%, indicating that the model in the simulated environment (Domain A) struggled to classify certain failure types.

\section{Conclusion and Future work} \label{5}
In this paper, we propose a novel CF-GNN model as a solution to improve imbalanced classification for 5G network digital twins. We introduce an adaptive spectral filtering method specifically designed for multi-class classification datasets. A critical analysis of the literature highlights the challenges in training datasets, which often lead to class overlapping, particularly in relation to the imbalance ratio. This issue significantly complicates the identification of minority classes, especially as the number of classes increases. Experiment with the state-of-the-art GNN classification techniques have been conducted on real-network and network digital twin datasets to test the effectiveness of our proposed method in filtering the minority classes.

Future work shall continue to expand on our analysis of the effects of multiclass data difficulty for node classification. It might be investigated how the failure classes can be categorized into sub-types and whether there are small clusters of uncommon minority classes. We also intend to introduce efficient GNN designs data level-based node classification models for class-imbalanced graph data to improve classification performance.

\section{CRediT authorship contribution statement}
\textbf{Abubakar Isah}: Conceptualization, Writing-original draft, Formal analysis, Software, Data curation. \textbf{Ibrahim Aliyu:} Methodology and Review-writing. \textbf{Sulaiman Muhammad Rashid:} Writing-review and editing. \textbf{Jaehyung Park:} Supervision, Review and Editing. \textbf{Minsoo Hahn:} Methodology, Writing and Review. \textbf{Junsul Kim:} Writing-review and editing, Supervision, Conceptualization

\section{Declaration of competing interest}
The authors declare that they have no known competing financial interests or personal relationships that could have influenced the work reported in this study.

\section{Data availability}
Data will be made available on request.

\section{Acknowledgement}
This work was supported by the Institute of Information and Communications Technology Planning and Evaluation (IITP) grant, funded by the Korean government (MSIT), under grant number (RS-2024-00345030), Development of Digital Twin-Based Network Failure Prevention and Operation Management Automation Technology.


%%  \bibliographystyle{elsarticle-num-names} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

\bibliographystyle{elsarticle-num}
\bibliography{main}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
