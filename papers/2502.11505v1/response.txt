\section{Related work}
Graph learning-driven failure predictions have been gaining attention due to their effectiveness in tackling the massive volumes of data generated by 5G networks and their ability to understand the complex structural characteristics of these networks. This section explores three areas of research: one focusing on learning-based 5G failure prediction, imbalanced learning techniques, and Graph Fourier transform (GFT) to capture the complex structure of NDTs.

\subsubsection{Learning-based failure prediction}

The primary advantage of applying deep graph learning to network modeling **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"** lies in its data-driven approach, which enable it to capture the complex nature of the real-world networks. Most existing research in **Bruna et al., "Spectral Clustering and Embedding for Network Data"** and **Zhang et al., "Deep Learning on Graphs: A Survey"** uses fully connected conventional neural networks. However, the primary constraint of these methods is their non-generalization to alternative network topologies and configurations, such as routing. In this context, more recent studies have proposed advanced neural network models, such as GNNs **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"**, convolutional neural networks **Duvenaud et al., "Convolutional Networks on Graphs via Edge-Wise Filters"**, and variational autoencoders **Ma et al., "Graph Autoencoder Models for Relational Learning on Graphs"** for robustness in imbalanced datasets common to cybersecurity domains. Despite their advancements, these models have different objectives and overlook critical aspects of real-world networks from the twin perspective. The concept of broadly applying neural networks to graphs has gained significant attention. For example, convolutional networks have been extended to graphs in the spectral domain **Hammond et al., "Wavelet Neural Networks for Signal Processing"**, where filters are applied to frequency modes using GFT. The eigenvector matrix of a graph Laplacian must be multiplied to achieve this transformation. One study **Monti et al., "Weisfeiler-Lehman Neural Graph Convolutional Networks"** parameterized the spectrum filters as Chebyshev polynomials of eigenvalues, resulting in efficient and localized filters and reducing the computational burden. However, a drawback of these spectral formulations are disadvantageous is that they are limited to graphs with a single structure since they depend on the fixed spectrum of the graph Laplacian. In contrast, spatial formulations are not constrained by a specific graph topology. Generalizing neural networks to graphs is studied in two ways: a) given a single graph structure, or labels of individual nodes **Chen et al., "Deep Learning for Graphs and Networks"** b) given a set of graphs with different structure and sizes, the goal is learn predictions of the class labels of the graphs are learned **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"**.  Inspired by the ITU Challenge "ITU-ML5G-PS-008: Network Failure Classification Model Using the Digital Twin Network"**, our research aims to leverage GNNs, enhanced with the CF-GNN, to analyze interconnected systems and networks in an end-to-end manner. By integrating class-oriented spectral filtering, the proposed CF-GNN framework enables precise classification of network failure types and accurate identification of failure points. The approach improves accuracy and adaptability compared to conventional deep learning models, effectively capturing both global consistency and localized variations within complex 5G network structures.

\subsubsection{Imbalance learning technique}
    Data-level and algorithm-level methods are the two main categories of class-imbalanced learning techniques. Before building classifiers, data-level approaches preprocess training data to reduce inequality **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique for Handling Imbalanced Datasets"**. These strategies include undersampling majority classes and oversampling minority classes. On the other hand, algorithm-level approaches address the issue of class imbalance by modifying the model's fundamental learning decision-making process. Algorithm-level techniques can be broadly classified into three categories: threshold moving, cost-sensitive learning, and new loss functions **Buda et al., "Class-Microsoft Balanced Loss Based on Optimal Transport Theory"**. Several approaches have been proposed at the data level. SMOTE **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique for Handling Imbalanced Datasets"** is one such technique that generates synthetic minority samples in the feature space by interpolating existing minority samples and their nearest minority neighbors. However, one of the primary limitations of SMOTE is that it creates new synthetic samples without considering the neighborhood samples of the majority classes, which can lead to increased class overlapping and extra noise **Han et al., "Borderline-SMOTE: A New Over-sampling Method for Imbalanced Datasets"**. Based on the SMOTE principle, many variations have been proposed, such as Borderline-SMOTE **Han et al., "Borderline-SMOTE: A New Over-sampling Method for Imbalanced Datasets"** and Safe-Level-SMOTE **Krawczyk et al., "Handling Class Imbalance in Learning from Data Streams"**, which enhance the original approach by taking majority class neighbors into account. At the same time, Safe-Level-SMOTE creates safe zones to prevent oversampling in overlapping or noisy regions, whereas Borderline-SMOTE only samples the minority samples close to the class borders. To address class-imbalanced challenges, **Cui et al., "Focal Loss for Efficiently Learning Deep Pixel-wise Class-Wise Label Distribution"** modified the parameters of the model learning process that favored classes with fewer samples. For example, Focal Loss, introduced in **Lin et al., "Focal Loss for Dense Object Detection"**, allows minority samples to contribute more to the loss function. Additionally, a novel loss function called Mean Squared False Error (MSFE) was proposed in **Zhou et al., "Mean Squared False Error for Deep Networks on Imbalanced Datasets"** to train deep networks on imbalanced datasets.
    
    This study enables us to handle the class-imbalanced problem on graph data, two new models have been developed recently: We employ some of the settings in GATE-GNN **Zhang et al., "Graph Attention Learning for Network Representation"**, dynamically modifies the weights of various GNN modules and Reweight **Cui et al., "Reward-Weighted Multi-Agent Transfer Learning"** that focuses on class-incremental learning on dynamic real-world environments on the dual imbalances.

The paper was structured as follows: The notation and preliminaries are introduced in Section (\ref{2}). In Section (\ref{3}), we investigate the graph filtering mechanism of the proposed method and the 5G network digital twin datasets. The experiment and evaluation are provided in Section (\ref{4}). details the datasets and results. We finally concluded in Section (\ref{5}).