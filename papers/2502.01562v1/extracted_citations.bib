@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{gao2023pal,
  title={{PAL}: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@inproceedings{hernandez2024recursive,
    title={Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning},
    author={Sergio Hern{\'a}ndez-Guti{\'e}rrez and Minttu Alakuijala and Alexander V Nikitin and Pekka Marttinen},
    booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS},
    year={2024},
}

@article{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      journal={arXiv preprint arXiv:1503.02531}, 
}

@article{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023}
}

@inproceedings{kim_comparing_2021,
  title     = {Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation},
  author    = {Kim, Taehyeon and Oh, Jaehoon and Kim, Nak Yil and Cho, Sangwook and Yun, Se-Young},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  pages     = {2628--2635},
  year      = {2021},
}

@article{kujanpaa2024knowledge,
  title={Knowledge Injection via Prompt Distillation},
  author={Kujanp{\"a}{\"a}, Kalle and Valpola, Harri and Ilin, Alexander},
  journal={arXiv preprint arXiv:2412.14964},
  year={2024}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{nguyen_dynasaur_2024,
  title={DynaSaur: Large language agents beyond predefined actions},
  author={Nguyen, Dang and Lai, Viet Dac and Yoon, Seunghyun and Rossi, Ryan A and Zhao, Handong and Zhang, Ruiyi and Mathur, Puneet and Lipka, Nedim and Wang, Yu and Bui, Trung and others},
  journal={arXiv preprint arXiv:2411.01747},
  year={2024}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{qi2025context,
  title={In-Context Editing: Learning Knowledge from Self-Induced Distributions},
  author={Qi, Siyuan and Yang, Bangcheng and Jiang, Kailin and Wang, Xiaobo and Li, Jiaqi and Zhong, Yifan and Yang, Yaodong and Zheng, Zilong},
  journal={The Thirteenth International Conference on Learning Representations, {ICLR}},
  year={2025}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{snell2022learning,
  title={Learning by distilling context},
  author={Snell, Charlie and Klein, Dan and Zhong, Ruiqi},
  journal={arXiv preprint arXiv:2209.15189},
  year={2022}
}

@inproceedings{song2024agentbank,
    title = "{A}gent{B}ank: Towards Generalized {LLM} Agents via Fine-Tuning on 50000+ Interaction Trajectories",
    author = "Song, Yifan  and
      Xiong, Weimin  and
      Zhao, Xiutian  and
      Zhu, Dawei  and
      Wu, Wenhao  and
      Wang, Ke  and
      Li, Cheng  and
      Peng, Wei  and
      Li, Sujian",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    pages = "2124--2141",
}

@article{wang2024executable,
  title={Executable code actions elicit better llm agents},
  author={Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2402.01030},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{xu2023rewoo,
      title={Re{WOO}: Decoupling Reasoning from Observations for Efficient Augmented Language Models}, 
      author={Binfeng Xu and Zhiyuan Peng and Bowen Lei and Subhabrata Mukherjee and Yuchen Liu and Dongkuan Xu},
      year={2023},
    journal={arXiv preprint arXiv:2305.18323},
}

@inproceedings{yao2023react,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {Re{A}ct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR}},
  year         = {2023},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2024examination,
      title={An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models}, 
      author={Yizhou Zhang and Lun Du and Defu Cao and Qiang Fu and Yan Liu},
      year={2024},
      journal={arXiv preprint arXiv:2402.05359},
}

@inproceedings{zhangoffline,
  title={Offline Training of Language Model Agents with Functions as Learnable Weights},
  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@inproceedings{zhao2024expel,
    author       = {Andrew Zhao and Daniel Huang and Quentin Xu and Matthieu Lin and Yong-Jin Liu and Gao Huang},
    title        = {ExpeL: LLM Agents Are Experiential Learners},
    booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence},
    year         = {2024},
    pages        = {19632--19642},
}

@article{zhou2023language,
  title={Language agent tree search unifies reasoning acting and planning in language models},
  author={Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
  journal={arXiv preprint arXiv:2310.04406},
  year={2023}
}

@article{zhou2023least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={The Eleventh International Conference on Learning Representations,
                  {ICLR}},
  year={2023}
}

