@inproceedings{yao2023react,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {Re{A}ct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR}},
  year         = {2023},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zhuang2023toolqa,
  title={Tool{QA}: A dataset for {LLM} question answering with external tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50117--50143},
  year={2023}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{song2024agentbank,
    title = "{A}gent{B}ank: Towards Generalized {LLM} Agents via Fine-Tuning on 50000+ Interaction Trajectories",
    author = "Song, Yifan  and
      Xiong, Weimin  and
      Zhao, Xiutian  and
      Zhu, Dawei  and
      Wu, Wenhao  and
      Wang, Ke  and
      Li, Cheng  and
      Peng, Wei  and
      Li, Sujian",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    pages = "2124--2141",
}

@article{kujanpaa2024knowledge,
  title={Knowledge Injection via Prompt Distillation},
  author={Kujanp{\"a}{\"a}, Kalle and Valpola, Harri and Ilin, Alexander},
  journal={arXiv preprint arXiv:2412.14964},
  year={2024}
}

@inproceedings{kim_comparing_2021,
  title     = {Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation},
  author    = {Kim, Taehyeon and Oh, Jaehoon and Kim, Nak Yil and Cho, Sangwook and Yun, Se-Young},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  pages     = {2628--2635},
  year      = {2021},
}

@article{nguyen_dynasaur_2024,
  title={DynaSaur: Large language agents beyond predefined actions},
  author={Nguyen, Dang and Lai, Viet Dac and Yoon, Seunghyun and Rossi, Ryan A and Zhao, Handong and Zhang, Ruiyi and Mathur, Puneet and Lipka, Nedim and Wang, Yu and Bui, Trung and others},
  journal={arXiv preprint arXiv:2411.01747},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A and Shazeer, N and Parmar, N and Uszkoreit, J and Jones, L and Gomez, A and Kaiser, L and Polosukhin, I},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{zhou2023language,
  title={Language agent tree search unifies reasoning acting and planning in language models},
  author={Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
  journal={arXiv preprint arXiv:2310.04406},
  year={2023}
}

@article{zhou2023least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={The Eleventh International Conference on Learning Representations,
                  {ICLR}},
  year={2023}
}

@inproceedings{
padmanabhan2023propagating,
title={Propagating Knowledge Updates to {LM}s Through Distillation},
author={Shankar Padmanabhan and Yasumasa Onoe and Michael JQ Zhang and Greg Durrett and Eunsol Choi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@article{qi2025context,
  title={In-Context Editing: Learning Knowledge from Self-Induced Distributions},
  author={Qi, Siyuan and Yang, Bangcheng and Jiang, Kailin and Wang, Xiaobo and Li, Jiaqi and Zhong, Yifan and Yang, Yaodong and Zheng, Zilong},
  journal={The Thirteenth International Conference on Learning Representations, {ICLR}},
  year={2025}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@inproceedings{zhangoffline,
  title={Offline Training of Language Model Agents with Functions as Learnable Weights},
  author={Zhang, Shaokun and Zhang, Jieyu and Liu, Jiale and Song, Linxin and Wang, Chi and Krishna, Ranjay and Wu, Qingyun},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@inproceedings{gao2023pal,
  title={{PAL}: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@article{deepseekai2024deepseekv3technicalreport,
  title={Deep{S}eek-{V}3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{zhang2024examination,
      title={An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models}, 
      author={Yizhou Zhang and Lun Du and Defu Cao and Qiang Fu and Yan Liu},
      year={2024},
      journal={arXiv preprint arXiv:2402.05359},
}

@inproceedings{hernandez2024recursive,
    title={Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning},
    author={Sergio Hern{\'a}ndez-Guti{\'e}rrez and Minttu Alakuijala and Alexander V Nikitin and Pekka Marttinen},
    booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS},
    year={2024},
}

@article{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bai2022constitutional,
  title={Constitutional {AI}: Harmlessness from {AI} feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{grattafiori2024llama3herdmodels,
  title={The {Llama} 3 herd of models},
  author={Grattafiori, Aaron and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      journal={arXiv preprint arXiv:1503.02531}, 
}

@inproceedings{zhou_webarena_2023,
	title = {{WebArena}: {A} {Realistic} {Web} {Environment} for {Building} {Autonomous} {Agents}},
	shorttitle = {{WebArena}},
	url = {https://openreview.net/forum?id=oKn9c6ytLx},
	abstract = {With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than the human performance of 78.24\%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that {\textbackslash}ours can be used to measure such progress.{\textbackslash}footnote\{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.\}},
	language = {en},
	urldate = {2025-01-11},
	author = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
	month = oct,
	year = {2023},
	annote = {Proposes a benchmark for AI agents centered around web interaction. Features a set of practice web pages that closely mimic real-world sites along tasks (or “intents”) designed by human annotators, evaluation methods for those tasks, and baseline performance figures using GPT-4. The baseline agents are given a full list of available actions at each step.
},
	file = {Full Text PDF:/Users/yura/Zotero/storage/7HICHFZB/Zhou et al. - 2023 - WebArena A Realistic Web Environment for Building Autonomous Agents.pdf:application/pdf},
}

@article{openai2024gpt4technicalreport,
      title={{GPT}-4 Technical Report}, 
      author={OpenAI},
    journal={arXiv preprint arXiv:2303.08774},
    year={2023}
}

@article{openai2024gpt4ocard,
  title={{GPT}-4o system card},
  author={OpenAI},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{shinn_reflexion_nodate,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	language = {en},
	author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	annote = {Proposes a reflexive memory system that helps AI agents learn from unsuccessful trajectories. Might make sense to compare this we boosting, as both utilize past experience to improve agent performance. Experiments are based around ReAct and a few existing benchmarks. 
},
	file = {PDF:/Users/yura/Zotero/storage/ISXRQPPG/Shinn et al. - Reflexion Language Agents with Verbal Reinforcement Learning.pdf:application/pdf},
}

@article{xu2023rewoo,
      title={Re{WOO}: Decoupling Reasoning from Observations for Efficient Augmented Language Models}, 
      author={Binfeng Xu and Zhiyuan Peng and Bowen Lei and Subhabrata Mukherjee and Yuchen Liu and Dongkuan Xu},
      year={2023},
    journal={arXiv preprint arXiv:2305.18323},
}

@inproceedings{zhao2024expel,
    author       = {Andrew Zhao and Daniel Huang and Quentin Xu and Matthieu Lin and Yong-Jin Liu and Gao Huang},
    title        = {ExpeL: LLM Agents Are Experiential Learners},
    booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence},
    year         = {2024},
    pages        = {19632--19642},
}

@article{wang2024executable,
  title={Executable code actions elicit better llm agents},
  author={Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2402.01030},
  year={2024}
}

@article{snell2022learning,
  title={Learning by distilling context},
  author={Snell, Charlie and Klein, Dan and Zhong, Ruiqi},
  journal={arXiv preprint arXiv:2209.15189},
  year={2022}
}

@article{hu2021lora,
  title={Lo{RA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={627--635},
  year={2011},
  organization={JMLR}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
