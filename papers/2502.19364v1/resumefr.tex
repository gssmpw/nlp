%----------------------------------------------------------------------------------------
%
% Résumé des chapitres
%
%----------------------------------------------------------------------------------------
\addchap{Résumé des chapitres}



%----------------------------------------------------------------------------------------
%
% Chapitre 1
%
%----------------------------------------------------------------------------------------
\section*{Chapitre 1 : État de l'art pour l'analyse des séries temporelles : Apprentissage supervisé et non supervisé}
\addcontentsline{toc}{section}{Chapitre 1 : État de l'art pour l'analyse des séries temporelles : Apprentissage supervisé et non supervisé}

Ce chapitre traite de l'état de l'art dans l'analyse des séries temporelles, 
avec un focus sur deux types d'apprentissage: supervisé et non supervisé. 
L'analyse des séries temporelles est essentielle pour les scientifiques de données, 
car elle permet d'extraire des informations utiles à partir de données qui évoluent 
dans le temps. Il existe de nombreuses techniques dans ces deux approches, chacune 
adaptée à des contextes et des objectifs différents. Le chapitre commence par une 
introduction générale de ces concepts et passe ensuite en revue les méthodes les 
plus avancées dans ce domaine.

Dans l'apprentissage supervisé, l'objectif principal est de prédire des valeurs 
futures en se basant sur des observations passées. Cela inclut des tâches comme 
la régression extrinsèque, où l'on cherche à prédire des valeurs continues, par 
exemple le prix futur des actions, ou la classification, où l'on catégorise 
des événements futurs en fonction des observations historiques. Un 
exemple serait de classifier des emails comme "spam" ou "non-spam" 
en analysant les emails précédents. En revanche, l'apprentissage 
non supervisé consiste à découvrir des structures ou des motifs 
cachés dans les données sans utiliser de labels ou de catégories 
prédéfinis. Deux des tâches principales en apprentissage non supervisé 
sont le regroupement (clustering) et la détection d'anomalies. Ces 
méthodes sont utilisées pour identifier des comportements inhabituels 
ou des groupes de données similaires dans divers domaines, comme la 
segmentation des clients ou la détection de fraudes financières.

Le chapitre présente ensuite les techniques de classification des 
séries temporelles (\emph{Time Series Classification, TSC}), qui ont beaucoup 
évolué ces dernières décennies. Parmi les approches traditionnelles, 
on trouve les méthodes basées sur la distance (Plus Proche Voisin), qui utilisent des mesures 
de similarité comme le \emph{Dynamic Time Warping (DTW)}, une méthode capable 
de comparer deux séries temporelles en tenant compte des décalages 
temporels entre elles. Cette technique est très utile dans des domaines 
variés, allant de la reconnaissance d'activités humaines à la communication sans 
fil. Par exemple, dans une série de données temporelles, DTW permet de trouver 
la meilleure correspondance entre deux séries, même si elles ne sont pas parfaitement 
alignées dans le temps. C'est une avancée par rapport à des mesures de distance 
plus simples comme la distance euclidienne, qui ne tient pas compte des différences temporelles.

Un autre domaine important est celui des ``shapelets''~\cite{shapelets}. Ce sont des sous-séquences 
discriminatives d'une série temporelle qui permettent de distinguer différentes 
classes. Les shapelets sont souvent utilisées dans des domaines où la lisibilité 
et l'interprétabilité des modèles sont importantes, comme en médecine, 
où il est essentiel de comprendre pourquoi un certain diagnostic est fait 
à partir des données. Ces shapelets capturent des motifs locaux dans les séries 
temporelles qui sont représentatifs d'une classe spécifique.

Les approches basées sur des dictionnaires~\cite{boss} sont également abordées dans 
ce chapitre. Elles transforment les séries temporelles en séquences de symboles, 
ce qui permet de repérer des motifs répétés dans les données. L'une des techniques 
les plus connues dans ce domaine est SAX (\emph{Symbolic Aggregate approXimation})~\cite{sax}, 
qui simplifie les séries temporelles en les représentant par une séquence de symboles, 
rendant plus facile la détection de motifs répétitifs. Des méthodes dérivées comme 
BOSS (\emph{Bag-of-SFA-Symbols})~\cite{boss} permettent d'améliorer cette 
approche en analysant les séries temporelles sous forme de fenêtres qui se 
chevauchent, et en créant des ensembles de mots qui facilitent la 
classification des séries temporelles.

le chapitre aborde les méthodes hybrides qui combinent plusieurs approches pour 
améliorer les performances des modèles d'analyse de séries temporelles. Un modèle 
hybride célèbre est HIVE-COTE~\cite{hive-cote}, qui combine des techniques basées sur les distances, 
les dictionnaires, les shapelets et les intervalles, en les utilisant ensemble pour 
créer un modèle d'ensemble puissant et robuste. Ces approches combinées permettent 
souvent d'obtenir des résultats plus précis que l'utilisation d'une seule méthode, 
en exploitant les forces de chaque approche.

Les modèles plus récents pour la classification des séries temporelles 
sont de plus en plus basés sur l'apprentissage profond, en particulier les 
réseaux neuronaux convolutifs (\emph{Convolutional Neural Networks, CNN}). Ces 
réseaux utilisent des filtres qui capturent des motifs dans les données, 
comme des relations temporelles complexes, en parcourant les séries de 
manière glissante. Ils ont prouvé leur efficacité dans la reconnaissance 
de motifs dans les séries temporelles, souvent surpassant les méthodes 
traditionnelles.
Le chapitre consacre également une section importante à l'apprentissage profond 
pour l'analyse des séries temporelles. Les réseaux neuronaux, notamment les CNN 
et les réseaux récurrents (RNN), sont de plus en plus utilisés dans ce domaine.
Une particularité des réseaux neuronaux est leur capacité à extraire des 
caractéristiques complexes des séries temporelles sans nécessiter une étape 
explicite de sélection de caractéristiques, ce qui simplifie le processus 
d'apprentissage. Ces modèles peuvent paralléliser les calculs, ce qui 
les rend plus rapides que les approches traditionnelles pour les grands 
volumes de données. Les CNN, en particulier, sont efficaces pour la 
classification des séries temporelles, car ils permettent de détecter 
des motifs locaux importants dans les données.

La régression extrinsèque est une tâche importante de l'apprentissage 
supervisé, où l'objectif est de prédire une valeur continue à partir 
d'une série temporelle. Contrairement à la classification, où les résultats 
sont des catégories discrètes, la régression vise à estimer des valeurs 
numériques précises. Par exemple, elle peut être utilisée pour prédire la 
température d'une région en fonction des données climatiques historiques 
ou pour estimer les ventes futures en fonction des tendances des ventes passées.
L'approche traditionnelle consiste à utiliser des modèles de régression linéaire 
ou d'autres techniques classiques, mais avec les récentes avancées, les méthodes 
basées sur l'apprentissage profond gagnent en popularité. Les réseaux neuronaux 
peuvent capturer les relations non linéaires entre les observations passées et futures.

L'apprentissage non supervisé pour les séries temporelles ne vise pas à 
prédire des valeurs futures ou à classer des événements, mais plutôt à 
découvrir des structures cachées dans les données. Cette partie du chapitre 
se concentre sur troix techniques principales: le clustering, le prototypage
et l'apprentissage auto-supervisé. Ces méthodes sont particulièrement utiles 
lorsqu'on n'a pas, ou peu, de labels pour les données, ou lorsque les séries temporelles 
sont complexes et contiennent des motifs que l'on souhaite analyser sans 
définir de catégories précises à l'avance.

Le clustering (ou regroupement) est une méthode utilisée pour regrouper 
des séries temporelles similaires en fonction de leurs caractéristiques. 
Il permet d'organiser les données en groupes homogènes (\emph{clusters}) où les 
membres d'un même groupe partagent des propriétés communes. Par exemple, 
dans une analyse de séries temporelles de données clients, le clustering 
peut être utilisé pour segmenter les clients en fonction de leurs comportements 
d'achat, afin d'identifier différents types de consommateurs. Le chapitre 
présente plusieurs algorithmes pour le clustering, comme les méthodes 
basées sur la distance (ex : $k$-means), et explique comment ces techniques 
peuvent être adaptées aux séries temporelles.

L'un des défis du clustering dans les séries temporelles est de choisir 
la bonne mesure de similarité. Alors que les mesures comme la distance 
euclidienne sont simples à implémenter, elles ne prennent pas en compte 
les décalages dans le temps. Le \emph{Dynamic Time Warping (DTW)}, déjà mentionné 
dans la section sur la classification, est souvent utilisé pour cette raison. 
En outre, le chapitre mentionne des méthodes plus récentes qui combinent 
les approches traditionnelles avec des techniques d'apprentissage profond~\cite{deep-tscl-bakeoff} 
pour améliorer la précision du clustering.

Le prototypage~\cite{eamonn-prototyping} est une autre technique utile dans l'analyse non supervisée 
des séries temporelles, où l'objectif est de représenter un ensemble de séries 
temporelles par une série représentative appelée prototype. Cela permet de 
résumer efficacement de grands ensembles de données en une série qui capture 
l'essence des données. Ces prototypes peuvent être utilisés pour visualiser 
ou comprendre des groupes de séries temporelles, ou pour simplifier des modèles 
d'analyse complexes.

L'approche classique consiste à calculer une moyenne ou une médiane pour 
les séries temporelles d'un groupe, mais cette méthode simple peut échouer 
à capturer des motifs subtils. C'est pourquoi des méthodes plus sophistiquées, 
comme le $k$-means~\cite{kmeans-paper} avec le DTW ou d'autres mesures de similarité adaptées aux 
séries temporelles, sont utilisées pour obtenir des prototypes plus précis.

Le chapitre aborde également l'apprentissage auto-supervisé, qui est une méthode émergente 
dans l'analyse des séries temporelles. Contrairement à l'apprentissage supervisé 
classique où les modèles sont entraînés avec des labels, l'apprentissage auto-supervisé 
permet aux modèles d'apprendre à partir des données elles-mêmes, sans avoir 
besoin de labels explicites. Une technique commune consiste à créer des tâches 
d'apprentissage auxiliaires (par exemple, prédire une partie manquante de la 
série temporelle ou réorganiser des segments) pour permettre au modèle 
d'apprendre des représentations utiles des données. Ces représentations 
peuvent ensuite être réutilisées pour d'autres tâches comme la classification 
ou la régression.

L'apprentissage supervisé est particulièrement utile dans les situations où les 
labels sont rares ou coûteux à obtenir. Il permet d'exploiter efficacement 
les grands ensembles de données non étiquetés, ce qui est souvent le cas 
dans des secteurs comme la surveillance de la santé ou la gestion de grandes 
infrastructures industrielles.

Le chapitre conclut sur l'importance des progrès récents dans l'analyse des 
séries temporelles, que ce soit dans le domaine supervisé ou non supervisé. 
Il met en avant la richesse des méthodes disponibles et souligne que les 
techniques modernes, notamment celles basées sur l'apprentissage profond 
et l'apprentissage supervisé, sont essentielles pour résoudre les problèmes 
complexes que posent les séries temporelles aujourd'hui. Ces méthodes 
permettent de traiter des volumes de données de plus en plus importants, 
tout en offrant des prédictions plus précises et une meilleure capacité à 
identifier des comportements cachés ou inhabituels dans les données.

En résumé, le chapitre offre un panorama complet des techniques actuelles 
et des avancées dans l'analyse des séries temporelles. Que ce soit pour des 
tâches supervisées comme la classification et la régression, ou non supervisées 
comme le clustering et le prototypage, il souligne que la combinaison des 
approches traditionnelles avec les nouvelles techniques d'apprentissage profond 
offre des résultats prometteurs dans de nombreux domaines.

\section*{Chapitre 2 : Évaluation comparative des modèles d'apprentissage automatique sur les données de séries temporelles}
\addcontentsline{toc}{section}{Chapitre 2 : Évaluation comparative des modèles d'apprentissage automatique sur les données de séries temporelles}

L'évaluation comparative des modèles de machine learning est une pratique 
cruciale pour évaluer et améliorer les algorithmes. Elle permet de comparer 
la performance des modèles sur plusieurs jeux de données afin d'identifier 
les méthodes les plus performantes et de mieux comprendre les forces et 
faiblesses des différents modèles. Les méthodes traditionnelles comme le 
\emph{Wilcoxon signed-rank test}~\cite{wilcoxon-paper} et le \emph{Nemenyi test}~\cite{nemenyi-paper}
sont souvent utilisées, mais elles 
présentent des limites, par exemple elles peuvent être manipulées et ne donnent pas 
toujours une image complète des différences entre les modèles. Des méthodes plus récentes, 
comme les approches bayésiennes, sont proposées comme alternatives plus fiables pour les 
comparaisons multiples.

Dans ce chapitre, on se concentre sur les méthodes actuelles d'évaluation comparative, 
en particulier sur la tâche de classification des séries temporelles. L'un des outils 
les plus utilisés pour ces comparaisons est le
\emph{Critical Difference Diagram (CDD)}~\cite{demsar-cdd-paper,cdd-benavoli-paper}. 
Cependant, ce diagramme présente des limites importantes, comme l'instabilité dans les 
classements et la possibilité de manipulation. Une méthode nouvelle, appelée 
\emph{Multiple Comparison Matrix (MCM)}, est introduite pour offrir des comparaisons 
plus robustes et interprétables des modèles.

Le CDD résume les performances des modèles en les classant 
sur plusieurs jeux de données, comme les 128 de l'archive UCR~\cite{ucr-archive}.
Cependant, cette méthode 
ignore la magnitude des différences et peut être instable. Par exemple, ajouter ou retirer 
un modèle peut changer les conclusions sur les différences significatives.
Le \emph{Friedman test}~\cite{friedman-paper}
et le \emph{Nemenyi test}~\cite{nemenyi-paper} sont souvent utilisés,
mais ils ont des faiblesses similaires.

Le CDD simplifie les comparaisons, mais il présente trois problèmes majeurs :

\begin{itemize}
    \item \textbf{Instabilité des classements}: Le classement change quand on ajoute ou 
    retire des modèles.
    \item \textbf{Ignorance de la magnitude}: Le CDD ne prend pas en compte
    l'ampleur des gains ou pertes de performance.
    \item \textbf{Problèmes avec les tests statistiques}: Les \emph{p-values}
    ne reflètent pas toujours les différences réelles entre les modèles.
\end{itemize}

Pour résoudre ces problèmes, la \emph{Multiple Comparison Matrix (MCM)} est proposée 
dans ce chapitre 
comme une méthode plus fiable pour comparer les modèles. Cette méthode se concentre 
sur les comparaisons par paires des modèles, et elle garantit que les comparaisons 
restent invariantes à l'ajout ou au retrait d'autres modèles. Elle offre une vue 
plus détaillée des performances de chaque modèle avec des informations comme:

\begin{itemize}
    \item La différence moyenne de performance entre deux modèles.
    \item Un compte des victoires, égalités et défaites pour chaque paire de modèles.
    \item Une p-value issue d'un test \emph{Wilcoxon}.
\end{itemize}

En conclusion, l'évaluation comparative des modèles de machine learning joue un rôle 
central dans l'amélioration continue des algorithmes, en permettant d'identifier les 
meilleures méthodes et de mieux comprendre leurs forces et faiblesses. Bien que des 
approches traditionnelles comme les tests de Wilcoxon et de Nemenyi soient couramment 
utilisées, elles présentent des limites, notamment une instabilité dans les classements 
et une faible prise en compte des différences de magnitude entre les performances des 
modèles. Ces méthodes peuvent aussi être influencées par l'ajout ou le retrait de 
modèles, rendant les résultats moins fiables.

C'est dans ce contexte que des alternatives plus récentes, comme les approches 
bayésiennes ou la \emph{Multiple Comparison Matrix (MCM)}, ont été proposées 
pour offrir des comparaisons plus robustes et pertinentes. La MCM, en particulier, 
se concentre sur des comparaisons par paires entre modèles et garantit une 
meilleure stabilité des résultats, indépendamment des changements dans 
l'ensemble des modèles testés. Elle fournit aussi des informations plus détaillées 
et descriptives, notamment sur les différences de performance moyennes, ainsi 
que sur les victoires et défaites entre chaque paire de modèles.

Ce chapitre a mis en évidence les limites des outils traditionnels comme le 
\emph{Critical Difference Diagram (CDD)}, tout en proposant des solutions 
complémentaires avec la MCM pour pallier ces faiblesses. Ainsi, l'approche 
MCM permet d'offrir une vision plus précise et nuancée des différences entre 
les modèles, en garantissant des comparaisons plus cohérentes et informatives. 
Cela souligne l'importance de poursuivre l'exploration de méthodes d'évaluation 
robustes pour améliorer l'efficacité et la fiabilité des modèles de machine 
learning appliqués aux séries temporelles.

\section*{Chapitre 3 : Vers la recherche de modèles de fondation pour la classification des séries temporelles}
\addcontentsline{toc}{section}{Chapitre 3 : Vers la recherche de modèles de fondation pour la classification des séries temporelles}


Dans le domaine dynamique de la classification des séries temporelles
(\emph{Time Series Classification, TSC}), l'un des principaux défis consiste à développer 
des modèles robustes et adaptables à des jeux de données variés. Les modèles de fondation, qui 
sont de grands modèles pré-entraînés capables de généraliser sur plusieurs tâches, 
offrent une solution prometteuse. L'intérêt de ces modèles est qu'ils simplifient 
et accélèrent le processus d'ajustement pour des tâches spécifiques. Cela s'avère 
crucial dans des domaines comme la médecine, par exemple avec les signaux ECG, ou 
la gestion du trafic, où l'entraînement de modèles à partir de zéro est coûteux et 
long. Les modèles de fondation offrent un point de départ pré-entraîné qui comprend déjà
les motifs fondamentaux, ce qui permet de gagner en temps et en précision lors de l'ajustement 
à des jeux de données particuliers.

Ce chapitre présente deux contributions principales pour progresser vers 
les modèles de fondation profonds:

\begin{enumerate}
    \item La création de filtres convolutifs faits main pour améliorer 
    la généralisation des modèles.
    \item L'utilisation d'une méthodologie de pré-entraînement pour ajuster (\emph{fine tune})
    ces modèles à des tâches de classification spécifiques.
\end{enumerate}

Ces filtres sont conçus pour se concentrer sur des caractéristiques 
générales des données, indépendamment du domaine spécifique, 
afin d'améliorer l'adaptabilité des modèles sur diverses tâches.

Les modèles de deep learning pour la TSC sont souvent confrontés à des problèmes tels que:

\begin{itemize}
    \item Surapprentissage (\emph{overfitting}), où les modèles deviennent trop spécialisés 
    sur les données d'entraînement
    \item Complexité computationnelle
    \item Apprentissage de filtres redondants
\end{itemize}

Les réseaux neuronaux convolutifs (\emph{Convolutional Neural Networks, CNNs}) traditionnels 
apprennent les filtres grâce à la rétropropagation (\emph{backpropagation}), 
mais ce processus peut mener à 
des filtres trop spécifiques, manquant de généralité. Une solution consiste à créer des 
filtres manuellement, qui détectent des motifs génériques dans les données. Avant 
d'adopter cette approche, il faut supposer que les modèles CNN peuvent apprendre 
des filtres génériques communs à travers différents jeux de données.

Pour tester cette hypothèse, une analyse a été réalisée sur l'espace des filtres 
appris par les modèles CNN sur plusieurs ensembles de données ECG. L'analyse a 
montré qu'un certain nombre de filtres convolutifs coïncident dans l'espace, 
suggérant que certains filtres peuvent être partagés entre différents jeux de données.

Trois types de filtres faits main sont proposés pour capturer des motifs spécifiques 
dans les séries temporelles:

\begin{itemize}
    \item Filtre de détection de tendance croissante : construit pour détecter les 
    segments de séries temporelles où les valeurs augmentent
    \item Filtre de détection de tendance décroissante : construit pour 
    détecter les segments où les valeurs diminuent.
    \item Filtre de détection de pics : construit pour détecter les 
    changements brusques dans une série 
    temporelle, comme une augmentation suivie d'une diminution rapide.
\end{itemize}

Ces filtres ne sont pas ajustés pendant l'entraînement, permettant ainsi au modèle de se 
concentrer sur l'apprentissage de motifs plus complexes et nuancés. Ces filtres sont 
similaires à ceux utilisés en vision par ordinateur, tels que les filtres
Sobel~\cite{sobel-paper1,sobel-paper2}.

Pour évaluer l'impact des filtres faits main, trois architectures adaptées sont proposées :

\begin{itemize}
    \item CO-FCN (\emph{Custom Only-Fully Convolutional Network}): la premiere couche 
    de convolution dans FCN~\cite{fcn-resnet-mlp-paper} est 
    remplacé entièrement par les trois filtres créés manuellement.
    \item H-FCN (\emph{Hybrid-Fully Convolutional Network}): combine les filtres 
    créés manuellement et des filtres appris dans la première couche de convolution de FCN.
    \item H-Inception (\emph{Hybrid-Inception}): intègre les filtres faits main dans l'architecture 
    Inception~\cite{inceptiontime-paper}, qui est connue pour ses performances sur la TSC.
\end{itemize}

Les résultats expérimentaux sur 128 jeux de données de l'archive
UCR~\cite{ucr-archive} montrent que les modèles avec 
filtres faits main surpassent souvent leurs versions originales. Par exemple, 
CO-FCN a mieux performé que le modèle FCN d'origine sur la plupart des jeux de 
données. De même, les versions hybrides H-FCN et H-InceptionTime montrent 
des améliorations significatives.

Dans le reste de ce chapitre, nous abordons le développement d'un modèle de fondation pour 
la classification des séries temporelles en utilisant une tâche prétexte. 
L'objectif est de pré-entraîner un modèle sur une tâche générique avant de 
l'adapter à des jeux de données spécifiques pour des tâches de classification 
particulières. Cela permet non seulement de réduire le temps d'entraînement, 
mais aussi d'améliorer la capacité de généralisation du modèle. Le modèle de 
fondation pré-entraîné tire parti de l'architecture H-Inception et intègre des 
filtres convolutifs faits main, qui se sont révélés utiles pour capturer 
des motifs généraux dans les séries temporelles.

Dans de nombreuses applications du monde réel, comme la médecine ou la gestion 
industrielle, il est coûteux et laborieux de collecter et d'étiqueter de grandes 
quantités de données pour entraîner un modèle d' apprentissage profond à partir de zéro. 
Par exemple, pour la détection des maladies cardiaques à partir de signaux ECG, 
il est nécessaire de disposer d'énormes quantités de données annotées par des 
professionnels de santé, ce qui n'est souvent pas réalisable. De même, dans des 
domaines comme la maintenance prédictive, la collecte de données de capteurs 
nécessite un suivi à long terme et l'expertise d'ingénieurs pour annoter les 
modes de défaillance.

L'idée des modèles de fondation est de pré-entraîner un modèle 
robuste sur un large éventail de jeux de données similaires, ce qui permet 
ensuite de l'adapter plus facilement et plus efficacement à de nouveaux jeux 
de données spécifiques. Ce processus de \emph{fine tuning} est beaucoup plus rapide 
et permet d'éviter le risque de surapprentissage, un problème 
fréquent lorsque l'on travaille avec des ensembles de données de petite taille.

La méthode proposée dans ce chapitre repose sur une tâche prétexte construite pour 
entraîner un modèle à reconnaître les motifs généraux dans des séries temporelles 
à partir de différents jeux de données. Cette tâche prétexte consiste à apprendre 
au modèle à prédire le jeu de données d'origine de chaque échantillon de série 
temporelle. Ce processus permet au modèle de capturer des caractéristiques 
génériques applicables à plusieurs ensembles de données. Une fois ce modèle 
pré-entraîné, il est ensuite ajusté sur des tâches de classification propores
à chaque jeu de données.

L'architecture choisie pour ce modèle est basée sur H-Inception, proposé dans la premiere 
contribution de ce chapitre. Le processus global peut être résumé en deux étapes principales:
\begin{itemize}
    \item Étape 1: Pré-entraînement du modèle sur une tâche générique où il 
    doit prédire l'origine des séries temporelles.
    \item Étape 2: Ajustement du modèle sur des jeux de données 
    spécifiques pour des tâches de classification précises.
\end{itemize}

Les résultats expérimentaux montrent que le modèle proposé PHIT (\emph{Pre-trained H-InceptionTime}) 
dépasse la performance des approches traditionnelles d'ajustement. Le modèle a été 
comparé à des approches de pointe sur l'ensemble des 88 jeux de données de séries temporelles.

Une comparaison directe entre notre approche avec pré-entraînement et un modèle 
sans pré-entraînement a été réalisée. Les résultats montrent que PHIT 
offre de meilleures performances que le modèle de base dans $48$ jeux de données, 
tandis que le modèle de base n'en surpasse que $23$. L'analyse statistique de ces 
résultats à l'aide du \emph{Wilcoxon Signed-Rank Test}~\cite{wilcoxon-paper}
indique que l'amélioration apportée 
par PHIT est significative avec une p-value de $0,021$ (inférieure au seuil de $0,05$).

En conclusion, ce chapitre propose des avancées majeures dans la classification des 
séries temporelles en introduisant les modèles de fondation, qui permettent une 
généralisation plus efficace sur différentes tâches. Ces modèles pré-entraînés 
offrent un gain de temps et de précision, en particulier dans des domaines comme 
la médecine ou la gestion du trafic, où l'entraînement à partir de zéro est coûteux et complexe.

Les deux contributions principales de ce travail sont, d'une part, la 
création de filtres convolutifs créés manuellement, construits pour détecter des 
motifs génériques et améliorer la robustesse des modèles, et, d'autre 
part, l'utilisation d'une méthodologie de pré-entraînement. Cette dernière 
permet d'affiner les modèles sur des tâches spécifiques, en les rendant
plus adaptés à chaque jeux de données sans nécessiter un long processus 
d'apprentissage.

Les résultats expérimentaux montrent que les architectures telles que CO-FCN 
et H-Inception, intégrant ces filtres faits main, surpassent souvent les 
versions classiques sur un large éventail de jeux de données. De plus, 
la méthode de pré-entraînement basée sur une tâche générique améliore 
significativement les performances des modèles sur des tâches spécifiques, 
comme l'a montré le modèle PHIT.

En résumé, ce chapitre démontre l'efficacité des modèles de fondation dans la 
classification des séries temporelles, en offrant une meilleure capacité 
de généralisation et une réduction du 
temps d'entraînement, ce qui les rend particulièrement utiles pour des 
applications nécessitant des solutions rapides et performantes.

\section*{Chapitre 4 : Réduire la complexité des modèles d'apprentissage profond pour la classification des séries temporelles}
\addcontentsline{toc}{section}{Chapitre 4 : Réduire la complexité des modèles d'apprentissage profond pour la classification des séries temporelles}

Ce chapitre explore la réduction de la complexité des modèles d'apprentissage profond 
dans le cadre de la classification des séries temporelles. Traditionnellement, 
les modèles plus complexes et volumineux, comme InceptionTime~\cite{inceptiontime-paper} 
avec ses $2,1$ millions 
de paramètres, ont montré de bonnes performances. Cependant, leur complexité présente 
des défis lorsqu'il s'agit de les déployer dans des environnements à ressources 
limitées, comme les dispositifs embarqués ou mobiles.

Pour répondre à ces besoins, ce chapitre introduit le modèle LITE, qui vise à maintenir 
des performances compétitives tout en réduisant considérablement la taille et la complexité 
du modèle. LITE se base sur une version allégée de l'architecture Inception et intègre 
des techniques de boosting pour améliorer la capacité de généralisation sur divers jeux 
de données, tout en restant rapide à entraîner et économe en ressources.

Les principaux objectifs du modèle LITE incluent:

\begin{itemize}
    \item Architecture Inception allégée: réduction du nombre de paramètres et 
    de la complexité sans sacrifier la performance
    \item Techniques de boosting: intégration de techniques qui améliorent la 
    généralisation, réduisent le surapprentissage, et augmentent la précision
    \item Efficacité et adaptabilité: offrir un modèle adapté à des environnements 
    contraints en termes de ressources, tout en maintenant des performances 
    élevées avec un faible coût computationnel
\end{itemize}

L'architecture LITE est une version simplifiée du réseau Inception, construit pour maximiser 
l'efficacité tout en minimisant la complexité. Ses principaux éléments sont:

\begin{itemize}
    \item Filtres créés manuellement: Reprenant les contributions du chapitre précédent, 
    ces filtres sont utilisés dans la première couche pour capturer des motifs 
    génériques dès le début, tout en évitant un surapprentissage. Ils fonctionnent 
    en parallèle avec des convolutions apprises pour maximiser l'efficacité du modèle
    \item Multiplexing de convolution: Plusieurs convolutions avec différentes tailles 
    de filtres sont appliquées en parallèle dans les premières couches. Cela permet au 
    modèle de capturer divers motifs dans les séries temporelles, optimisant ainsi 
    l'extraction de caractéristiques importantes dès le début
    \item \emph{DepthWise Separable Convolutions(DWSC)}: Ces convolutions sont utilisées 
    dans les couches profondes de l'architecture. Elles permettent de réduire drastiquement 
    le nombre de paramètres et la charge computationnelle tout en conservant une forte 
    capacité d'extraction de caractéristiques pertinentes~\cite{mobilenets}
    \item Convolutions dilatées: Les couches profondes utilisent des convolutions 
    dilatées pour augmenter le champ réceptif du modèle sans ajouter de nouveaux 
    paramètres, ce qui permet au modèle d'apprendre des dépendances à plus long 
    terme dans les données~\cite{rocket}
    \item Pooling global: Un pooling global est appliqué dans les dernières 
    couches pour réduire la dimension des données avant la classification 
    finale, comme cela se fait dans les modèles classiques tels que
    FCN et ResNet~\cite{fcn-resnet-mlp-paper}
\end{itemize}

LITETime est une version ensemble du modèle LITE, similaire à
InceptionTime~\cite{inceptiontime-paper}, qui 
regroupe plusieurs modèles LITE pour améliorer les performances globales.

Les résultats expérimentaux montrent que LITE, avec ses moins de $10 000$ paramètres, 
surpasse des modèles bien plus volumineux comme FCN ($264 000$ paramètres) et ResNet 
($504 000$ paramètres). Il atteint une précision de $0,8304$, proche de celle des 
modèles plus complexes comme Inception ($0,8393$), tout en utilisant beaucoup 
moins de ressources et en étant beaucoup plus rapide à entraîner.

Lors des tests sur l'archive UCR avec 128 jeux de données~\cite{ucr-archive}, LITETime atteint 
une précision moyenne de 0,8462, tout en restant bien plus petit que InceptionTime. 
Il utilise seulement $2,34\%$ des paramètres d'InceptionTime, ce qui en fait une 
option beaucoup plus légère et efficace pour des environnements à ressources limitées.

Dans de nombreux cas réels, les séries temporelles sont multivariées, ce qui signifie 
qu'elles comportent plusieurs canaux de données (comme dans la santé, où des données 
ECG sont mesurées sur plusieurs axes). Pour s'adapter à cela, l'architecture LITEMV
a été développée, reposant sur la base du modèle LITE mais modifiée pour mieux gérer 
les séries temporelles multivariées.
LITEMV remplace les convolutions standards dans les premières couches par des convolutions 
\emph{DepthWise}, permettant de traiter chaque canal indépendamment avant de les 
combiner efficacement. Cela permet au modèle de mieux capturer les interactions 
entre les différents canaux de données.

Lors des tests sur 30 jeux de données multivariées de l'archive UEA~\cite{uea-archive}, LITEMVTime, 
l'ensemble de modèles basé sur LITEMV, a surpassé des modèles de pointe comme 
InceptionTime~\cite{inceptiontime-paper} et Disjoint-CNN~\cite{disjoint-cnn-paper}.
Dans certains cas, comme sur le jeu de données 
EigenWorms, LITEMVTime a obtenu une précision impressionnante de $93,89\%$, 
contre seulement $59,34\%$ pour ConvTran~\cite{convtran-paper},
un autre modèle de pointe pour la classification multivariée.

LITE et LITEMV représentent des avancées majeures dans la classification des séries temporelles. 
Grâce à leur faible complexité et leur efficacité énergétique, ces modèles sont parfaitement 
adaptés à des environnements à ressources limitées, tout en maintenant des performances 
compétitives face à des modèles bien plus complexes. De plus, l'approche d'ensemble, 
avec LITETime et LITEMVTime, montre que ces architectures peuvent offrir une précision 
encore plus élevée sans compromettre leur légèreté.

\section*{Chapitre 5 : Apprentissage semi-supervisé et auto-supervisé pour les données de séries temporelles avec un manque de labels}
\addcontentsline{toc}{section}{Chapitre 5 : Apprentissage semi-supervisé et auto-supervisé pour les données de séries temporelles avec un manque de labels}

Ce chapitre aborde les défis liés à la classification des séries temporelles dans les 
cas où les données annotées sont rares. Les méthodes traditionnelles de classification 
supervisée nécessitent des données largement annotées, ce qui est souvent difficile 
à obtenir en raison de la complexité et du besoin d'expertise pour annoter ces données. 
En réponse à cela, l'apprentissage semi-supervisé et l'apprentissage auto-supervisé 
émergent comme des solutions prometteuses. Ces techniques exploitent des données non 
annotées pour améliorer les performances des modèles.

L'approche proposée, nommée TRILITE (\emph{TRIplet Loss In TimE}), repose sur le concept 
de perte de triplet (\emph{triplet loss})~\cite{triplet-loss-paper}, 
une technique utilisée pour apprendre des 
représentations discriminatives à partir de données non annotées. TRILITE emploie 
une méthode d'augmentation de données spécifiquement adaptée aux séries temporelles, 
permettant au modèle d'apprendre des caractéristiques utiles sans avoir besoin de 
beaucoup de données annotées. Deux cas d'utilisation sont explorés:

\begin{itemize}
    \item L'amélioration des performances d'un classificateur supervisé 
    avec peu de données annotées
    \item Un contexte d'apprentissage semi-supervisé où une partie des 
    données est étiquetée et l'autre non
\end{itemize}

TRILITE est un modèle auto-supervisé qui utilise la perte de triplet pour apprendre 
des représentations significatives à partir de séries temporelles. Le modèle se compose 
de trois encodeurs partageant les mêmes poids, traitant les triplets d'entrée 
(référence, positif, négatif). Le mécanisme de triplet loss vise à rapprocher les 
échantillons similaires tout en éloignant les échantillons dissemblables~\cite{facenet-paper}.

TRILITE a été testé sur 85 jeux de données de l'archive UCR~\cite{ucr-archive}. 
Les expériences ont montré 
que TRILITE améliore les performances des classificateurs dans les deux scénarios 
explorés. Dans le cas de données annotées en faible quantité, TRILITE a aidé à fournir
des représentations complémentaires qui, combinées à des modèles supervisés comme 
FCN~\cite{fcn-resnet-mlp-paper}, 
améliorent significativement la précision. De plus, dans un contexte semi-supervisé, 
TRILITE, en utilisant à la fois des données annotées et non annotées, a surpassé les 
approches traditionnelles sur plusieurs jeux de données.

Ce chapitre montre que l'apprentissage auto-supervisé et semi-supervisé, via des 
approches comme TRILITE, offre des solutions efficaces lorsque les données annotées 
sont limitées. TRILITE utilise les données non annotées pour générer des 
représentations utiles, améliorant ainsi la performance des modèles de classification 
de séries temporelles. Ces résultats ouvrent la voie à des méthodes plus efficaces 
et moins dépendantes de l'annotation manuelle des données.

\section*{Chapitre 6 : Analyse de séries temporelles pour les données de mouvement humain}
\addcontentsline{toc}{section}{Chapitre 6 : Analyse de séries temporelles pour les données de mouvement humain}

L'analyse des mouvements humains à partir de données de squelettes est devenue une 
technique couramment utilisée dans des domaines variés, tels que la reconnaissance 
d'actions humaines~\cite{human-motion-example-paper}, la réhabilitation~\cite{kimore-paper}, 
et la génération de séquences de 
mouvements réalistes~\cite{action2motion-paper}. 
Ces données sont généralement capturées à l'aide de technologies 
comme Microsoft Kinect~\cite{kinect-paper} et les systèmes de capture de mouvement 
(MoCap)~\cite{mocap-paper}, qui 
enregistrent les positions des articulations du corps humain dans un espace 
tridimensionnel. Chaque articulation est représentée par des coordonnées X, Y, 
et Z dans le temps, formant une série temporelle multivariée (MTS).

Les MTS capturant des mouvements humains présentent un intérêt particulier car 
elles permettent d'extraire et d'analyser des caractéristiques spatiales et 
temporelles simultanément. Par exemple, dans la réhabilitation, il est 
essentiel de comprendre non seulement le déplacement des articulations 
individuelles au fil du temps, mais aussi la manière dont ces articulations se 
coordonnent pour réaliser des mouvements complexes.

Les données de mouvements humains présentent plusieurs avantages pour 
l'analyse de séries temporelles:

\begin{itemize}
    \item Elles sont souvent bien structurées et capturent les 
    dynamiques des articulations en mouvement
    \item Elles peuvent être directement utilisées dans de nombreux 
    algorithmes de l'apprentissage automatique et l'apprentissage profond 
    pour des tâches comme la classification, la régression et la génération.
    \item Elles sont particulièrement adaptées aux modèles qui exploitent la 
    relation entre les différentes dimensions des séries temporelles, 
    comme les réseaux neuronaux convolutifs (CNN) et les réseaux neuronaux récurrents (RNN).
\end{itemize}

Les modèles d'apprentissage profond, en particulier, se sont avérés être 
des outils puissants pour traiter et analyser les séries temporelles 
multivariées provenant de mouvements humains. Les architectures de 
réseaux neuronaux convolutifs ont montré leur efficacité dans l'extraction 
automatique des caractéristiques complexes des séries temporelles, offrant ainsi 
des performances supérieures à celles des méthodes manuelles traditionnelles. 
Ce chapitre se concentre sur plusieurs méthodes avancées d'analyse des MTS, 
notamment dans le domaine de la réhabilitation et de la génération de mouvements.

L'un des domaines d'application les plus importants pour l'analyse des mouvements 
humains est la réhabilitation. Dans ce contexte, les données de séries temporelles 
issues des mouvements humains peuvent être utilisées pour évaluer la progression 
des patients au cours de leurs séances d'exercices physiques. Traditionnellement, 
cette évaluation est réalisée par des experts humains, qui observent et notent 
la qualité des mouvements. Cependant, ce processus peut être subjectif, coûteux 
et manquer de précision. Les modèles d'apprentissage profond, en revanche, 
offrent une solution pour automatiser cette évaluation, fournissant des 
résultats rapides, cohérents et basés sur des données objectives.

Le modèle LITEMVTime, proposé dans chapitre 4 a été testé sur le jeu de données 
Kimore~\cite{kimore-paper}, un ensemble de données capturant des séquences 
de mouvements humains pendant des exercices de réhabilitation. Le jeu de données 
contient des enregistrements de patients sains et malades, chacun effectuant 
plusieurs exercices physiques sous la supervision d'experts humains. Chaque mouvement 
est annoté avec un score de qualité allant de $0$ (très mauvaise performance) à $100$ 
(excellente performance), attribué par des professionnels de la réhabilitation.

Le modèle LITEMVTime a été entraîné pour classer la qualité des mouvements en ``bon'' 
ou ``mauvais'' en utilisant les annotations d'experts comme vérité de terrain. 
Les résultats expérimentaux montrent que LITEMVTime surpasse d'autres architectures 
d'apprentissage profond telles que FCN, ResNet~\cite{fcn-resnet-mlp-paper}
et InceptionTime~\cite{inceptiontime-paper}, à la fois en termes de précision et 
de vitesse d'exécution. Grâce à sa conception légère, LITEMVTime peut être facilement 
intégré dans des systèmes cliniques en temps réel, fournissant ainsi des retours 
immédiats aux patients et aux cliniciens pendant les sessions de réhabilitation.

L'un des plus grands défis dans l'entraînement des modèles de deep learning sur des 
données médicales est le manque de données annotées. Les mouvements humains capturés 
pour des études médicales sont souvent limités, et leur annotation nécessite des experts 
spécialisés, ce qui en fait une ressource rare et coûteuse. Ce manque de données annotées 
peut entraîner des problèmes de surapprentissage, où les modèles d'apprentissage profond 
deviennent trop spécialisés sur les données d'entraînement et ne parviennent pas à 
bien généraliser sur de nouvelles données.
Pour répondre à ce problème, ce chapitre propose une méthode de prototypage de 
séries temporelles appelée ShapeDBA (\emph{Shape Dynamic Time Warping Barycenter Averaging}). 
Cette méthode permet de créer des prototypes qui représentent des moyennes barycentriques 
des séries temporelles, à partir desquelles de nouvelles séquences synthétiques peuvent 
être générées. Ces séquences synthétiques, qui conservent les propriétés essentielles 
des mouvements humains capturés, peuvent être ajoutées aux jeux de données 
d'entraînement pour augmenter artificiellement la taille du jeu de données 
et améliorer ainsi la généralisation des modèles.

Le prototypage de séries temporelles est une technique précieuse, en particulier 
pour les applications où les données réelles sont limitées. En créant des prototypes 
barycentriques, il est possible de générer des mouvements synthétiques qui imitent 
les mouvements réels des patients tout en offrant une plus grande diversité. Cela 
permet aux modèles de deep learning d'apprendre des motifs plus robustes et de 
mieux se généraliser à de nouveaux patients et à de nouvelles tâches de réhabilitation.

Les expérimentations menées sur le jeu de données Kimore~\cite{kimore-paper} montrent 
que l'ajout de données synthétiques générées par ShapeDBA améliore considérablement 
la performance des modèles d'apprentissage supervisé utilisés pour évaluer la qualité 
des mouvements des patients. Les modèles, lorsqu'ils sont entraînés à la fois sur 
des données réelles et synthétiques, produisent des prédictions plus précises sur 
la qualité des mouvements, réduisant les erreurs de prédiction mesurées par la MAE 
(erreur absolue moyenne) et la RMSE (erreur quadratique moyenne). Cela montre 
que ShapeDBA est une méthode efficace pour augmenter les jeux de données limités 
et améliorer les performances globales des modèles de régression.

Si le prototypage de séries temporelles est une méthode efficace pour étendre 
les jeux de données de manière synthétique, il existe une autre approche complémentaire, 
basée sur l'utilisation des modèles génératifs profonds. Les modèles génératifs, tels que 
les Auto-Encodeurs Variationnels (\emph{Variational Auto-Encoder, VAE})~\cite{vae-paper} et 
les Réseaux adverbiaux génératifs (\emph{Generative Adversarial Networks, GAN})~\cite{gan-paper}. 
Ces modèles peuvent apprendre des 
distributions complexes de mouvements et ensuite générer de nouvelles 
séquences qui ressemblent aux données d'entraînement d'origine.

Dans ce chapitre, on explore l'utilisation des VAE pour la génération 
de mouvements humains. Les VAE sont une classe de modèles génératifs qui apprennent 
à encoder des données d'entrée dans un espace latent de faible dimension, 
à partir duquel de nouvelles données peuvent être générées. Dans le cas des mouvements 
humains, les VAE peuvent capturer les dynamiques des articulations et générer 
des mouvements réalistes qui imitent les séquences observées dans les données d'entraînement.

Dans ce chapitre on propose le SVAE (\emph{Supervised Variational Autoencoder}) 
est une amélioration par rapport au VAE classique, car il intègre une tâche de classification 
dans l'espace latent du modèle. Cela permet au modèle de non seulement générer des 
séquences de mouvements humains réalistes, mais aussi de les classifier selon des 
catégories prédéfinies, ce qui renforce à la fois ses capacités génératives et discriminatives.

L'architecture du SVAE se compose de trois parties principales : l'encodeur, l'espace 
latent, et le décodeur, similaires à un VAE classique:
\begin{itemize}
    \item Entrée (séquence de squelettes): Les données d'entrée sont des 
    séquences de mouvements humaines capturées sous forme de coordonnées 
    3D des articulations squelettiques.
    \item Encodeur: L'encodeur prend les séquences de mouvements 
    comme entrée et apprend une représentation latente sous la 
    forme d'une distribution gaussienne (paramétrée par une moyenne et une variance). 
    L'objectif est d'apprendre une distribution latente compacte qui capture les 
    caractéristiques essentielles des mouvements humains.
    \item Supervision dans l'espace latent: Contrairement au VAE traditionnel, 
    le SVAE introduit une tâche de classification dans l'espace latent. Cette 
    supervision permet au modèle d'apprendre une séparation plus claire entre 
    les différentes actions (par exemple, marcher, courir, lever les bras). 
    Cela permet d'améliorer la cohérence entre la génération de séquences et 
    la classe d'action correspondante.
    \item Décodeur: Le décodeur prend un échantillon de l'espace latent et 
    reconstruit la séquence originale. En même temps, un classificateur est 
    intégré dans le modèle pour reconnaître l'action à partir de la représentation latente.
\end{itemize}

Le modèle SVAE permet ainsi de réaliser deux tâches simultanées :
\begin{itemize}
    \item Reconnaissance d'action: Prédire l'action associée à une séquence de mouvement.
    \item Génération de séquences réalistes: Créer des séquences de mouvements 
    réalistes en générant des exemples à partir de l'espace latent.
\end{itemize}

Ce double usage améliore à la fois les capacités génératives du modèle 
(production de nouvelles séquences de mouvements) et ses capacités discriminatives 
(classement des séquences dans la bonne catégorie).

Les expériences se basent sur le jeux de données de reconnaissance d'action
HumanAct12~\cite{action2motion-paper}. Ce jeux de données contient des 
séquences de mouvements humains où les positions des articulations sont 
enregistrées en 3D sur plusieurs frames.

Le modèle est testé sur sa capacité à générer des séquences de mouvements réalistes 
et variés à partir de l'espace latent. La qualité et diversité des séquences générées est mesurée 
à l'aide de metrics telles que la \emph{Fréchet Inception Distance (FID)}
et la \emph{Average Paired Distance (APD)}, 
qui évalue la similitude entre les distributions des données réelles et des données générées.

Les séquences générées par le SVAE sont plus réalistes que celles générées par des 
modèles VAE traditionnels. Le SVAE capture mieux les variations dans les mouvements 
humains et évite les artefacts communs des méthodes traditionnelles. Cela est reflété 
par des scores FID plus bas, indiquant une plus grande similarité entre les séquences 
générées et réelles.
Le modèle génère non seulement des séquences réalistes, mais aussi diversifiées. 
Cela est important pour les applications où des variations réalistes de mouvements 
sont requises, comme dans les jeux vidéo ou la réhabilitation médicale.

Ce chapitre a exploré plusieurs techniques avancées pour l'analyse des séries 
temporelles appliquées aux mouvements humains. Les modèles comme LITEMVTime ont 
montré leur efficacité pour évaluer la qualité des mouvements dans des contextes 
de réhabilitation, tandis que des approches comme ShapeDBA et le SVAE ont permis 
de surmonter les limites liées à la rareté des données annotées en générant des 
données synthétiques. Ces avancées offrent des perspectives prometteuses pour des 
applications en temps réel, non seulement dans le domaine médical, mais aussi dans 
des domaines créatifs comme le cinéma et les jeux vidéo.

Les technologies décrites dans ce chapitre démontrent que les données 
de mouvements humains capturées via des capteurs comme le Kinect ont 
le potentiel de révolutionner de nombreux domaines, en combinant l'analyse de 
séries temporelles avec des modèles d'apprentissage profond performants et des 
méthodes de génération de données synthétiques.

\section*{Chapitre 7 : Métriques d'évaluation pour la génération de mouvement humain}
\addcontentsline{toc}{section}{Chapitre 7 : Métriques d'évaluation pour la génération de mouvement humain}

Ce chapitre aborde les métriques d'évaluation des modèles génératifs appliqués à 
la génération de mouvements humains. Contrairement aux modèles discriminatifs, 
où la comparaison avec des données réelles est directe, les modèles génératifs 
posent un défi plus complexe~\cite{reliable-fidelity-diversity}, 
car il faut évaluer la fidélité des échantillons 
générés en fonction de leur ressemblance avec des données réelles et leur diversité. 
L'évaluation repose donc sur deux dimensions clés: la fidélité et la diversité. La 
fidélité mesure à quel point les données générées sont proches des données réelles, 
tandis que la diversité s'assure que le modèle génératif peut produire une variété 
d'échantillons.

Les méthodes traditionnelles d'évaluation, comme le \emph{Mean Opinion Scores (MOS)}~\cite{mos-paper}, 
ne sont pas adaptées aux modèles génératifs, car elles présupposent une perception 
uniforme de l'utilisateur, ce qui est souvent irréaliste. Par conséquent, 
l'évaluation quantitative devient essentielle pour juger la performance des 
modèles génératifs. Le chapitre souligne qu'il est difficile de trouver une 
métrique unique pour évaluer à la fois la fidélité et la diversité, d'où la 
nécessité d'un cadre unifié d'évaluation.

Un aspect crucial des données de mouvement humain est leur dépendance temporelle. 
Les distorsions temporelles, telles que les changements de fréquence ou les décalages 
dans le temps, jouent un rôle important dans l'évaluation des séquences de mouvements. 
Pourtant, de nombreuses métriques d'évaluation ne tiennent pas compte de ces aspects 
temporels, se concentrant davantage sur les caractéristiques latentes. Pour remédier 
à ce problème, une nouvelle métrique, appelée \emph{Warping Path Diversity (WPD)}, est 
introduite. Cette métrique permet de mesurer la diversité des distorsions 
temporelles dans les données réelles et générées, offrant ainsi une évaluation 
plus précise des modèles génératifs de séquences temporelles.

Les métriques de fidélité décrites dans ce chapitre incluent la \emph{Fréchet Inception 
Distance (FID)}~\cite{fid-original-paper}, qui évalue la différence entre les distributions des données réelles 
et générées. Plus la FID est basse, plus les données générées ressemblent aux données 
réelles. Une autre métrique importante est l'\emph{Accuracy on Generated (AOG)}, qui mesure la 
capacité du modèle à générer des échantillons conformes aux étiquettes de classes 
définies (par exemple, générer des mouvements de course lorsque la classe ``courir''  
est donnée). Enfin, la métrique de \emph{Density}~\cite{reliable-fidelity-diversity} évalue combien d'échantillons 
générés correspondent aux données réelles en mesurant la proximité entre ces 
deux ensembles dans l'espace des caractéristiques.

Les métriques de diversité permettent d'évaluer à quel point les données générées 
sont variées. La \emph{Average Pair Distance (APD)}~\cite{action2motion-paper}, 
par exemple, mesure la distance moyenne 
entre des paires d'échantillons générés, indiquant si le modèle évite la production de 
résultats trop similaires (un problème appelé mode collapse).
La \emph{Coverage}~\cite{reliable-fidelity-diversity} est une autre 
métrique qui mesure la proportion d'échantillons réels couverts par les échantillons 
générés, assurant que les données générées couvrent bien l'ensemble de l'espace des 
données réelles.

Un autre concept important introduit est celui de la Mean Maximum Similarity (MMS)~\cite{msm-paper}. 
Cette métrique évalue la nouveauté des échantillons générés en mesurant la distance 
entre les échantillons générés et les plus proches voisins dans l'ensemble des données 
réelles. Une valeur élevée de MMS indique que les échantillons générés sont non 
seulement variés, mais aussi nouveaux par rapport aux données d'entraînement.

Le \emph{Warping Path Diversity (WPD)}, une nouvelle métrique qu'on propose, est présentée 
pour évaluer les distorsions temporelles. Utilisant l'algorithme 
\emph{Dynamic Time Warping (DTW)}~\cite{dtw-paper}, cette métrique mesure comment 
les séquences générées diffèrent temporellement des séquences réelles. Par exemple, 
dans une séquence de mouvements comme ``boire avec la main gauche'', les échantillons 
réels peuvent commencer à différents moments, tandis que les échantillons générés 
peuvent ne pas varier suffisamment en termes de timing. WPD quantifie cette diversité 
dans les distorsions temporelles, offrant ainsi une évaluation plus fine.

Les expériences menées dans ce chapitre reposent sur l'utilisation de modèles \emph{Conditional 
Variational Auto-Encoders (CVAE)} pour la génération de mouvements humains. 
Ces modèles sont évalués sur plusieurs métriques en fonction de leurs architectures 
(CNN, RNN ou Transformer) et de différents hyperparamètres. Les résultats montrent 
que certains modèles, comme le CConvVAE (CVAE base sur les CNN), excellent en termes 
de fidélité et de 
diversité, mais les performances dépendent fortement des configurations de paramètres. 
Par exemple, le CConvVAE obtient les meilleurs résultats en fidélité (mesurée par la FID) 
dans plusieurs cas, mais les résultats peuvent varier lorsque l'on change les 
paramètres d'entraînement ou l'architecture.

L'analyse des résultats montre qu'il est impossible de trouver un modèle unique 
qui surpasse tous les autres sur toutes les métriques. Chaque métrique capture 
un aspect différent de la qualité des échantillons générés, et en fonction 
des besoins (diversité dans les jeux vidéo ou fidélité dans la réhabilitation médicale), 
on peut être amené à privilégier une métrique sur une autre.

En conclusion, ce chapitre propose un cadre d'évaluation unifié pour les modèles 
génératifs appliqués à la génération de mouvements humains, avec plusieurs métriques 
permettant d'évaluer à la fois la fidélité et la diversité des modèles. Le 
\emph{Warping Path Diversity (WPD)} ajoute une dimension temporelle essentielle à 
cette évaluation, en tenant compte des distorsions temporelles dans les séquences 
générées. Ce cadre d'évaluation contribue à améliorer la comparaison entre différents 
modèles génératifs et facilite l'avancement de la recherche dans le domaine de la 
génération de mouvements humains.

\section*{Chapitre 8 : Recherche reproductible}
\addcontentsline{toc}{section}{Chapitre 8 : Recherche reproductible}

Ce chapitre traite de l'importance de la reproductibilité dans la recherche 
scientifique, en particulier dans le contexte de l'analyse des séries temporelles et de 
l'apprentissage profond. La reproductibilité garantit que les travaux peuvent 
être reproduits et adaptés par d'autres chercheurs, renforçant ainsi la confiance 
dans les résultats et favorisant l'innovation future. Ce chapitre met en lumière 
les efforts entrepris pour assurer que les travaux présentés dans cette thèse 
respectent les normes les plus élevées en matière de reproductibilité.

Un élément clé de ce chapitre est l'introduction du paquet 
\emph{aeon}~\cite{aeon-paper}, une bibliothèque 
open-source en Python construit pour effectuer diverses tâches d'apprentissage 
automatique sur les séries temporelles. Le développement de ce paquet a permis 
d'intégrer les contributions issues de cette recherche dans une plateforme 
accessible à la communauté scientifique. En rendant ces outils disponibles à 
tous, le projet encourage la reproductibilité et l'utilisation plus large des 
méthodes développées au cours de ce travail.

La documentation détaillée et le code ouvert jouent un rôle crucial dans 
la reproductibilité. Toutes les expériences décrites dans les chapitres 
précédents sont soutenues par du code public, permettant ainsi aux chercheurs 
de reproduire les expériences, de valider les résultats et de construire de 
nouveaux modèles basés sur ce travail. Ce code est accompagné de descriptions 
claires, facilitant la prise en main et l'adaptation du projet par d'autres 
chercheurs. En fournissant des instructions détaillées et en tenant compte des 
commentaires de la communauté, l'objectif est d'améliorer constamment la 
reproductibilité et la fiabilité des recherches.

Le paquet \emph{aeon}~\cite{aeon-paper} est au cœur de cet effort, 
offrant des outils pour diverses tâches comme la classification, la 
régression, la détection d'anomalies, et la segmentation des séries 
temporelles. En tant que développeur principal, j'ai contribué 
à la conception et à l'extension de ce paquet pour intégrer des modèles 
d'apprentissage profond, notamment ceux utilisés dans la classification 
des séries temporelles. Les modèles tels que InceptionTime, H-InceptionTime, 
et LITETime ont été inclus, ainsi que de nouveaux modules en développement 
pour des tâches comme le clustering des séries temporelles.

Les efforts pour garantir la reproductibilité ne se limitent pas à la mise à 
disposition du code. Le maintien de ce cadre logiciel implique également la 
correction des bugs, l'amélioration de la documentation, et l'ajout de nouvelles 
fonctionnalités pour répondre aux besoins évolutifs de la communauté scientifique. 
Par ailleurs, l'utilisation des tests unitaires permet de s'assurer que les 
nouveaux développements n'affectent pas la performance du code existant.

Une section clé du chapitre concerne les principes fondamentaux d'un travail 
reproductible. Cela inclut une documentation soignée, la fourniture des dépendances 
nécessaires, et une architecture de code claire et modulaire. Le code doit être facile 
à comprendre et à modifier, permettant ainsi à d'autres chercheurs de l'étendre 
pour ajouter de nouveaux modèles ou fonctionnalités. Des bonnes pratiques telles que 
l'utilisation de noms de variables explicites et une organisation efficace des fichiers 
sont également mises en avant pour améliorer la lisibilité et la maintenabilité du code.

On souligne également l'importance d'utiliser des outils comme Docker pour 
faciliter la gestion des environnements de développement et garantir que 
le code fonctionne de manière cohérente sur différentes machines. L'utilisation 
de conteneurs Docker permet de simplifier l'intégration des dépendances, notamment 
les configurations CUDA nécessaires pour l'utilisation des GPU, assurant ainsi 
une reproduction facile des expériences dans des environnements informatiques complexes.

Un autre aspect abordé dans le chapitre concerne la visualisation et la publication 
des résultats sous forme d'outils interactifs, tels que des pages web et des figures 
dynamiques. Ces outils permettent de mieux comprendre les résultats obtenus et 
d'interagir avec les données générées par les modèles. Par exemple, des visualisations 
du chemin de distorsion temporelle et de l'espace des filtres convolutifs ont été mises 
à disposition pour aider les chercheurs à mieux analyser les modèles de classification 
des séries temporelles.

En conclusion, ce chapitre met l'accent sur la nécessité de garantir la 
transparence et la reproductibilité dans la recherche scientifique. En 
publiant le code, en documentant les processus et en fournissant des outils 
interactifs, je contribue à renforcer la fiabilité de la recherche et 
à encourager la collaboration au sein de la communauté scientifique. Le 
développement continu de la plateforme \emph{aeon} et la mise en place 
de ressources accessibles montrent un engagement fort envers la création 
d'un écosystème de recherche ouvert et reproductible. Ces efforts garantissent 
que le travail présenté dans cette thèse peut servir de base solide pour de 
futures avancées dans le domaine de l'analyse des séries temporelles.