\chapter{Reducing Complexity in Deep Learning Models for Time Series Classification}\label{chapitre_4}

\section{Introduction}\label{sec:lite-intro}

In Chapter~\ref{chapitre_3}, we explored the significant impact of integrating hand-crafted filters into small, 
non-complex models in terms of number of parameters (FCN), demonstrating that such models can outperform more complex,
state-of-the-art models (ResNet), as presented in Section~\ref{sec:hfc-between-models-compare}. 
This finding challenges the conventional wisdom that increasing model complexity and the number of parameters 
inherently leads to better performance. Instead, it suggests that strategic simplicity and careful feature engineering 
can yield superior results.
For instance, the groundbreaking work presented in~\cite{inceptiontime-paper} introduced an innovative deep learning model,
InceptionTime (Chapter~\ref{chapitre_1} Section~\ref{sec:tsc-deep}), for 
TSC, significantly advancing the role of Convolutional Neural Networks (CNNs) in this field. Despite its 
impressive performance, InceptionTime, with nearly 2.1 million parameters distributed across five Inception models, 
exemplifies a large and complex architecture.
This complexity poses challenges for deployment in real-world applications, 
particularly those requiring small, resource-constrained devices.

Building on these foundations, this chapter introduces and looks into the innovative approach presented in here. 
% ``\emph{LITE: Light Inception with boosTing tEchniques for Time Series Classification}''~\cite{lite-paper}. 
The core motivation behind this work is to develop a more efficient and effective model, LITE, for TSC
by leveraging lightweight Inception-based architectures and boosting techniques.
This results in a model that is not only powerful but also efficient and adaptable.

The key contributions of the LITE approach are multifaceted:

\begin{itemize}
    \item \textbf{Lightweight Inception Architecture}: The LITE model employs a streamlined version of the
    Inception (Figure~\ref{fig:inception}) architecture, designed to reduce computational complexity without
    compromising performance. This makes the model more accessible for applications with limited computational resources.
    \item \textbf{Boosting Techniques}: To further enhance performance, LITE integrates boosting techniques that 
    improve the model's ability to generalize across diverse datasets. Boosting helps in mitigating overfitting,
    enhancing its predictive accuracy.
    \item \textbf{Efficiency and Adaptability}: The combination of a lightweight architecture and boosting 
    techniques results in a model that is both efficient and adaptable, capable of performing well across 
    various TSC datasets with reduced training times, lower computational demands and carbon footprint.
\end{itemize}

In real-world scenarios, the LITE model has significant implications. For example, in the healthcare sector, 
it can be utilized for rapid and accurate diagnosis of heart conditions from ECG signals, even in resource-constrained 
environments. In traffic management, LITE can be deployed to predict congestion patterns and optimize traffic flow 
with minimal computational overhead.
\begin{figure}
    \centering
    \caption{
        Difficulties in deploying a high-parameter deep learning model, such as FCN, on a Sony 
        robot for ground type classification. The extensive computational resources and memory 
        required by FCN present significant challenges for resource-constrained devices.
    }
    \label{fig:lite-deployment}
    \includegraphics[width=\textwidth]{Figures/chapter_4/deployment.pdf}
\end{figure}
Figure~\ref{fig:lite-deployment} illustrates a real-world scenario where deploying a model with over $200,000$ 
parameters, such as FCN, on a Sony robot for ground type classification encounters significant difficulties. 
The high parameter count not only demands substantial computational resources but also strains the device's 
memory and processing capabilities. To address this issue, we propose constructing a lightweight model: LITE.

This chapter will provide an in-depth exploration of the LITE model, discussing its architecture, the integration 
of boosting techniques, and its performance across different TSC tasks. By building on the insights gained from the 
previous chapter regarding the efficacy of hand-crafted filters in simplifying and enhancing model performance, 
we will see how the LITE approach takes these principles further to achieve state-of-the-art results in a 
lightweight and efficient manner.
We also propose an adaptation of the proposed LITE network specifically for the case of multivariate time series,
LITE MultiVariate (LITEMV).
We support the findings of this chapter with extensive experiments on both the UCR and UEA
archives~\cite{ucr-archive,uea-archive}.

\section{The LITE Architecture}\label{sec:lite-archis}

The LITE architecture is a streamlined version of the Inception network (Figure~\ref{fig:inception}), designed to maintain high 
performance while significantly reducing computational overhead. The LITE
architecture, presented in Figure~\ref{fig:lite} with a detailed parametric view, includes:

\begin{figure}
    \centering
    \caption{The proposed LITE architecture for Time Series Classification.}
    \label{fig:lite}
    \includegraphics[width=\textwidth]{Figures/chapter_4/archis/LITE.pdf}
\end{figure}

\begin{itemize}
    \item \textbf{Hand-Crafted Convolution Filters}: Recognizing the significant impact of hand-crafted filters 
    (Chapter~\ref{chapitre_3} Section~\ref{sec:construction-hcf}) on the performance of the simple FCN network,
    we employ these filters in the first layer of LITE. This approach mirrors the implementations
    in H-FCN and H-Inception (Chapter~\ref{chapitre_3} Section~\ref{sec:hcf-archis}).
    This is the \textbf{first} boosting technique used by LITE.

    \item \textbf{Multiplexing Convolution}: The core of the LITE architecture is built upon Inception modules,
    which apply multiple 
    convolution operations with different filter sizes simultaneously. This allows the model to capture various 
    types of patterns within the time series data. This is referred to as multiplexing convolution, the \textbf{second} boosting 
    technique used in LITE, and it is used only on the raw data and not in the rest of the network such as in
    Inception~\cite{inceptiontime-paper}.
    The convolution layers used in this part of the network are the standard convolution operations.
    The output convolution of these three layers, as well as the hand-crafted filters (with an activation),
    are concatenated on the channels axis and go through a Batch Normalization layer and an activation layer.

    \item \textbf{Efficient Convolutions}: The architecture employs efficient convolution techniques, DWSCs
    (Chapter~\ref{chapitre_1} Section~\ref{sec:tsc-deep}), in the second and third layers of the LITE network. 
    This approach drastically reduces the computational cost and memory footprint 
    while maintaining the model's ability to extract meaningful features from the data.
    It is important to notice that for the first layer, standard convolutions are used instead of DWSC. This is due to the
    fact that as the input time series is univariate, DWSC will learn only one filter.
    The output of these DWSC layers go through a Batch Normalization layer and an activation layer.

    \item \textbf{Dilation}: The LITE network uses a dilation rate for the DWSC 
    layers in the second and third depths. This increases the receptive field without increasing the kernel size, 
    thus reducing parameters, unlike Inception, which does not use dilation. This is the 
    \textbf{third} boosting technique used by LITE.
    Notably, dilation is not used in the first standard convolution layers to avoid missing crucial 
    input data.
    % By applying dilation in later layers, LITE expands the receptive field while preserving initial 
    % data integrity.

    \item \textbf{Global Average Pooling}: Similar to the state-of-the-art networks, e.g. FCN, ResNet and Inception,
    the LITE network applies a GAP operation over the last activation layer, transforming the output MTS to a vector,
    before being fed to the classification FC layer.

\end{itemize}

Table~\ref{tab:lite-params} highlights how much LITE is less complex than the three state-of-the-art networks: FCN, ResNet and 
Inception, in terms of both number of trainable parameters and the number of FLoat-point Operation Per Second (FLOPS).
The number of parameters shown is the number of trainable parameters of the architecture without the last
classification Fully Connected layer because it depends on each dataset (number of classes).
The table shows that the smallest model in terms of number of parameters is the LITE with $9, 814$
parameters. This is mainly due to the usage of DWSC instead of standard ones.

\subsection{LITETime: An Ensemble Approach}\label{sec:litetime}

Similar to InceptionTime, which is an ensemble of five Inception models, we propose LITETime, an ensemble 
of five LITE models. The goal of an ensemble is to reduce the variance in the model's performance. Thus, 
the more sensitive a model is, the greater the impact an ensemble will have.

Given the compact architecture of LITE, with approximately $10k$ parameters compared to the nearly $400k$ parameters 
of Inception, we believe that ensembling multiple LITE models will have a significantly higher impact. 
This approach leverages the efficiency and simplicity of LITE, amplifying its performance through ensembling 
to achieve robust and reliable results.

\begin{table}
    \centering
    \caption{Comparing LITE, FCN, ResNet and Inception in terms of number of trainable parameters and number of 
    FLoat-point Operation Per Second (FLOPS).}
    \label{tab:lite-params}
    \begin{tabular}{c|c|c|c|c|}
    \cline{2-5}
     & FCN & ResNet & Inception & LITE \\ \hline
    \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Number\\ of trainable\\ parameters\end{tabular}} &
    264,704 & 504,000 & 420,192 & 9,814 \\ \hline
    \multicolumn{1}{|c|}{FLOPS} & 266,850 & 507,818 & 424,414 & 10,632 \\ \hline
    \end{tabular}
    \end{table}

\subsection{Experimental Setup}

We utilize the 128 datasets of the UCR archive~\cite{ucr-archive} to evaluate the performance of LITE and LITETime 
compared to existing deep learning models.
All datasets were z-normalized prior to training and testing.

% Our experiments were conducted using a GTX 1080 GPU with 8GB of VRAM.
We meticulously measured the 
training time, inference time, CO2 emissions, and energy consumption using the CodeCarbon python package~\cite{codecarbon}.
The best-performing model during training, 
determined by monitoring the training loss, was selected for testing. The Adam optimizer with Reduce on Plateau 
learning rate decay method was employed, using TensorFlow's~\cite{tensorflow-paper} default parameter settings.
Each LITE model in the LITETime ensemble was trained with a batch size of 64 for 1500 epochs, similarly to Inception.

% The code supporting this work is publicly available here: \url{https://github.com/MSD-IRIMAS/LITE}.

\subsection{Experimental Results}

In this section, we present the experimental results of LITE in terms of performance and efficiency 
compared to other complex deep learning models, notably FCN, ResNet, and Inception.

\subsubsection{Comparing To State-Of-The-Art}\label{sec:litetime-vs-deep}

\begin{figure}
    \centering
    \caption{MCM (Chapter~\ref{chapitre_2}) showing the comparison between (LITE, LITETime) and the rest of the state-of-the-art 
    deep learning models for time series classification.}
    \label{fig:lite-mcm}
    \includegraphics[width=\textwidth]{Figures/chapter_4/results/mcm_row.pdf}
\end{figure}

Our proposed LITE model and its ensemble, LITETime, demonstrate competitive performance on the UCR archive, 
particularly when considering their significantly smaller size compared to other deep learning models. 
The LITE model, with approximately $10k$ parameters, and LITETime, which ensembles five LITE models, 
show impressive results in terms of accuracy, as presented in Figure~\ref{fig:lite-mcm}.

LITE achieves a mean accuracy of 0.8304, outperforming traditional models such as ResNet (0.8066) and FCN (0.7883), 
which have considerably more parameters. Notably, LITETime further improves this performance, reaching a mean 
accuracy of 0.8462. While slightly below the performance of InceptionTime (0.8491), the advantage of LITE and 
LITETime lies in their efficiency and lower computational requirements.

Examining the significance in performance differences, it is noteworthy that LITE alone significantly 
outperforms FCN and shows no significant difference in performance compared to ResNet. This is a revolutionary 
finding given that LITE has a significantly smaller number of parameters compared to these two models.
Furthermore, LITETime presents no significant difference in performance compared to InceptionTime, despite 
having only $2.34\%$ of the parameters of InceptionTime. This underscores the efficiency and effectiveness 
of LITE and LITETime, demonstrating that smaller, well-optimized models can achieve competitive results 
with much lower computational requirements.

The smaller parameter size of LITE compared to InceptionTime, which has nearly 400,000 parameters, highlights 
the effectiveness of our approach. The compact architecture of LITE, combined with boosting techniques and the 
efficiency of ensembling in LITETime, allows for robust performance with reduced computational costs. 
This makes LITE and LITETime particularly suitable for deployment in resource-constrained environments, 
where model size and inference time are critical factors.

Overall, the results validate the hypothesis that smaller, well-optimized models like LITE can achieve 
high performance comparable to larger models, offering a viable and efficient alternative for TSC tasks.
An example showcasing the trade off between performance, FLOPS and number of parameters can be seen in
Figure~\ref{fig:lite-tradeoff}.
In this figure we present the performance of LITE, FCN, ResNet and Inception over the test set of the 
FreezerSmallTrain dataset~\cite{ucr-archive}, as well as the number of FLOPS needed for one inference 
of each model per sliding window.
We also present each model in a form of circle where its radius represents the number of trainable parameters 
of each model.
It can be seen form Figure~\ref{fig:lite-tradeoff} that LITE is the most accurate model in terms of performance as well 
as the most efficient in terms of FLOPS and number of parameters, with a large gap in difference of efficiency with 
the other deep learning models.

\begin{figure}
    \centering
    \caption{
        For each model, the y-axis shows accuracy on the FreezerSmallTrain dataset, and the x-axis shows FLOPS in 
        a~$\log_{10}$ scale. Circle diameter represents the number of trainable parameters. The smallest model, 
        LITE (ours), has~$10k$ parameters and the lowest FLOPS (4 in $\log_{10}$ scale), while achieving the 
        highest test accuracy.
    }
    \label{fig:lite-tradeoff}
    \includegraphics[width=0.6\textwidth]{Figures/chapter_4/results/summary_with_flops-1.pdf}
\end{figure}

\subsubsection{Efficiency Comparison}\label{sec:lite-efficiency}


\begin{table}[h!]
    \hspace*{-1.6cm}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        Models &
        \begin{tabular}[c]{@{}c@{}}Number of\\ parameters\end{tabular} &
        FLOPS &
        Training Time &
        Testing Time &
        CO2 (g) &
        Energy (Wh) \\ \hline
    Inception &
      420,192 &
      424,414 &
      \begin{tabular}[c]{@{}c@{}}145,267 seconds\\ 1.68 days\end{tabular} &
      \begin{tabular}[c]{@{}c@{}}81 seconds\\ 0.0009 days\end{tabular} &
      0.2928 g &
      0.6886 Wh \\ \hline
      ResNet &
      504,000 &
      507,818 &
      \begin{tabular}[c]{@{}c@{}} 165,089 seconds\\  1.91 days\end{tabular} &
      \begin{tabular}[c]{@{}c@{}} 62 seconds\\  0.0007 days\end{tabular} &
      0.3101 g &
      0.7303 Wh \\ \hline
    FCN &
    264,704 &
    266,850 &
    \begin{tabular}[c]{@{}c@{}}149,821 seconds\\ 1.73 days\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}27 seconds\\ 0.00031 days\end{tabular} &
    0.2623 g &
    0.6176 Wh \\ \hline
    \textbf{LITE} &
    \textbf{9,814} &
    \textbf{10,632} &
    \begin{tabular}[c]{@{}c@{}}\textbf{53,567 seconds}\\ \textbf{0.62 days}\end{tabular} &
    \begin{tabular}[c]{@{}c@{}}\textbf{44 seconds}\\ \textbf{0.0005 days}\end{tabular} &
    \textbf{0.1048 g} &
    \textbf{0.2468 Wh} \\ \hline
\end{tabular}
\caption{Comparison between the proposed methods with FCN, ResNet and Inception without ensemble.}
\label{tab:lite-efficiency}
\end{table}

Table~\ref{tab:lite-efficiency} summarizes the number of parameters, the number of
(FLOPS), training time, inference time, CO2, and power consumption using CodeCarbon~\cite{codecarbon}
across the 128 datasets of the 
UCR archive~\cite{ucr-archive}. The values are aggregated over the 128 datasets and averaged over five different runs.

Compared to FCN, ResNet, and Inception, LITE has only $3.7\%$, $1.95\%$, and $2.34\%$ of their 
respective number of parameters. Moreover, LITE is the fastest model during the training phase, 
with a training time of $0.62$ days. This makes LITE $2.79$, $3.08$, and $2.71$ times faster than 
FCN, ResNet, and Inception, respectively.

Additionally, LITE consumes the least amount of CO2 and energy, at $0.1048$ g and $0.2468$ Wh, respectively. 
This demonstrates that LITE is not only the fastest but also the most environmentally friendly model for 
TSC compared to FCN, ResNet, and Inception. Given these factors, we believe that LITE is highly suitable 
for deployment in small devices, such as mobile phones.

\subsection{Ablation Studies}

In this section, we present an extensive study on the various features of the LITETime classifier. 
LITETime leverages three key features: (1) boosting techniques in each LITE model, (2) Depthwise 
Separable Convolution layers, and (3) the ensemble approach. In the following three sections, 
we examine the significance of each of these features, providing a detailed analysis of their 
contributions to the overall performance of LITETime.

\subsubsection{Impact Of Boosting Techniques}

\begin{figure}
    \centering
    \caption{
        The Heat Map shows the one-vs-one comparison between the Striped-LITE and the three variants: (1) Add-Custom-Filters, (2) Add-Multiplexing-
Convolution and (3) Add-Dilated-Convolution. The colors of the Heat Map follow the value of the first line in each cell. This value is the difference between
the value of the first line (average accuracy when winning/losing). The second line represents the Win/Tie/Loss count between the models in question (wins
for the column model). The last line is the statistical P-Value between the two classifier using the Wilcoxon Signed Rank Test
    }
    \label{fig:lite-ablation}
    \includegraphics[width=\textwidth]{Figures/chapter_4/results/results_ablation_study_lite.pdf}
\end{figure}

The LITE architecture leverages several advanced techniques to boost its performance. To demonstrate the 
individual impact of each technique, we conduct a comprehensive ablation study.

\begin{figure}
    \centering
    \caption{The stripped version of the LITE architecture for Time Series Classification.}
    \label{fig:stripped-lite}
    \includegraphics[width=\textwidth]{Figures/chapter_4/archis/StrippedLITE.pdf}
\end{figure}

Initially, we strip the LITE model of its three key techniques: dilation, multiplexing, and hand-crafted filters. 
In the multiplexing convolutions performed in the first layer, there are three layers with $32$ filters, 
resulting in the stripped-down LITE learning a total of $96=3$x$32$ filters for the first depth. The remaining 
architecture is kept the same, utilizing DWSCs without dilation.
The detailed architecture of the Striped-LITE is presented in Figure~\ref{fig:stripped-lite}.

Following this, we reintroduce each boosting technique one at a time to the stripped LITE model and evaluate 
its performance. The results of this ablation study are illustrated in Figure~\ref{fig:lite-ablation} using an MCM.
The results demonstrate that integrating hand-crafted filters in the first layer and utilizing multiplexing convolutions 
significantly enhance the LITE model's performance. The MCM shows that hand-crafted filters positively impact 
average accuracy, although they do introduce additional parameters. In contrast, multiplexing convolutions achieve 
minor performance gains.
The minor average impact of $0.34\%$ is outweighed by the benefits, as multiplexing significantly reduces the 
number of parameters and consistently outperforms across the majority of datasets.

While the addition of 
dilated convolutions does not yield statistically significant improvements (p-value $> 0.05$), the average 
accuracy differences indicate that dilated convolutions generally enhance performance. This is particularly 
relevant for large datasets, as dilation expands the receptive field without increasing parameter count. 
Occasionally, dilation may negatively affect performance on datasets that do not require an extensive receptive field.

In summary, the LITE model, equipped with these boosting techniques, features fewer parameters than the 
stripped-down version while delivering performance on par with state-of-the-art models. The reduction in 
parameter count is primarily due to multiplexing, which offsets the additional parameters introduced by hand-crafted filters.

Figure~\ref{fig:lite-ablation} also presents the average rank of the models, similar to the CD Diagram. 
The model with custom filters ranks the highest, indicating the best performance, while the stripped LITE 
ranks the lowest, marking it as the least effective. Thus, the Stripped-LITE model, without any boosting techniques,
is the weakest among the configurations shown in the MCM.

\subsubsection{Impact Of DepthWise Separable Convolutions}\label{sec:lite-impact-dwsc}

\begin{figure}
    \centering
    \caption{One-vs-one comparison between LITETime and
    LITETime with Standard convolutions over the 128
    datasets of the UCR archive~\cite{ucr-archive}.}
    \label{fig:lite-dwsc-impact}
    \includegraphics[width=0.5\textwidth]{Figures/chapter_4/results/LITETime_vs_Standard_Conv_LITETime.pdf}
\end{figure}

To further investigate the impact of DepthWise Separable Convolutions (DWSC), we replaced them with standard 
convolutions followed by a BottleNeck layer.
The reason we add a bottleneck layer is it simulates the reduction of number of filters needed to learn, 
as motivated from the Inception architecture~\cite{inceptiontime-paper}.
To ensure a fair and accurate comparison, we employed ensemble 
techniques, which are crucial in this context due to the significant disparity in the number of parameters, LITE 
has only about $11\%$ of the parameters compared to the alternative model.
Since the alternative model has about $85,000$ parameters, LITE would have around 9,350 parameters ($11\% of 85,000$).
% which possesses approximately $85,000$ parameters.

Figure~\ref{fig:lite-dwsc-impact} showcases a one-on-one comparison between LITETime and LITETime utilizing standard convolutions. 
The findings reveal that incorporating DWSC does not substantially influence performance, as indicated 
by a high P-Value of 0.4556. This high p-value suggests that the performance difference is not statistically 
significant, highlighting that DWSC can achieve comparable results with a significantly reduced parameter count.

\subsubsection{Impact Of Number of LITE Models in the Ensemble}\label{sec:lite-number-heads}

\begin{figure}
    \centering
    \caption{A Critical Difference diagram showcasing the
    comparison of performance of LITETime when more or less
    LITE models are used in the ensemble.}
    \label{fig:lite-ensembles-cdd}
    \includegraphics[width=\textwidth]{Figures/chapter_4/results/ensemble-study-LITETime.pdf}
\end{figure}

\begin{figure}
    \centering
    \caption{A Multi-Comparison Matrix (Chapter~\ref{chapitre_2}) showcasing the
    comparison of performance of LITETime-10,9 and 8 with other LITETime models with varying
    number of LITE heads.}
    \label{fig:lite-ensembles-mcm}
    \includegraphics[width=\textwidth]{Figures/chapter_4/results/mcm_row_ensembles.pdf}
\end{figure}

In previous experiments, we used five LITE models in LITETime to ensure a 
fair comparison with InceptionTime, which is an ensemble of five Inception models. The original work of
InceptionTime~\cite{inceptiontime-paper} has 
shown that no significant performance improvement is observed on the UCR archive when the ensemble size of 
InceptionTime exceeds five models. However, due to the smaller and more lightweight architecture of LITE, 
it may exhibit greater variance and reduced robustness compared to Inception.

To address this, we believe that increasing the number of LITE models in the LITETime ensemble could enhance performance. 
To test this hypothesis, we trained ten different LITE models on the UCR archive and constructed ensembles of 
varying sizes (from 1 to 10 models) by averaging all possible ensemble combinations. For example, to create LITETime-3 
(an ensemble of three LITE models), we combined all possible sets of three models from the ten trained models. 
The results are displayed in the CD diagram in Figure~\ref{fig:lite-ensembles-cdd} and the detailed MCM in 
Figure~\ref{fig:lite-ensembles-mcm}.

As illustrated in Figure~\ref{fig:lite-ensembles-cdd}, LITETime-5 (LITETime)
(an ensemble of five LITE models) is not the optimal limit for LITE. 
Instead, LITETime-7 proves to be more effective. This is due to LITE's compact size, approximately $42$ times 
smaller than Inception, which allows for greater scalability within the ensemble framework. Consequently, 
LITETime can enhance accuracy while maintaining significantly lower complexity compared to InceptionTime. 
Specifically, LITETime-5, with five models, utilizes only about $2.34\%$ of InceptionTime's trainable parameters, 
and LITETime-7 increases this to just $3.27\%$.
However, the MCM in Figure~\ref{fig:lite-ensembles-mcm} demonstrates that LITETime-9 significantly outperforms
LITETime-8, which contrasts with the conclusions drawn from the CD diagram in Figure~\ref{fig:lite-ensembles-cdd}.
Additionally, LITETime-10 does not show a significant difference compared to LITETime-9, indicating that the optimal
ensemble size for LITETime lies between nine and ten models. Remarkably, even with ten models, LITETime-10
remains approximately 21 times smaller than the ensemble InceptionTime.

Moreover, Figure~\ref{fig:lite-ensembles-beef} provides a concrete example using the Beef dataset from the UCR archive,
illustrating how the 
performance on unseen data varies with the number of models in both the LITETime and InceptionTime ensembles. 
This figure underscores the scalability and efficiency of the LITETime approach, highlighting its potential 
for superior performance with minimal computational overhead.

\begin{figure}
    \centering
    \caption{A Comparison on the Beef dataset of the UCR
    archive~\cite{ucr-archive} between the ensemble of LITE and Inception models.
    The $x$-axis represents the number of models used in
    each ensemble and the $y$-axis the performance of the ensemble on the test set of the Beef dataset.}
    \label{fig:lite-ensembles-beef}
    \includegraphics[width=0.7\textwidth]{Figures/chapter_4/results/Beef_ensemble_study-1.pdf}
\end{figure}

\paragraph*{LITETime for Multivariate Time Series?}

Given the groundbreaking findings of this study with LITETime, its suitability for real-world applications becomes 
evident due to its remarkably small parameter count and memory footprint. However, practical applications often 
involve multivariate TSC tasks. This raises an important question: Can LITETime be effectively 
applied to multivariate scenarios, or does it require adaptation?

In the following section, we address this question by introducing a novel multivariate deep learning model, LITEMV 
(LITE MultiVariate). This new model is specifically designed to extend the capabilities of LITETime to handle the 
complexities of multivariate TSC, ensuring its applicability across a broader range of real-world tasks.

\section{LITEMV: Addressing Multivariate Time Series Classification}

As we explore the potential applications of the LITE architecture, it becomes evident that many real-world 
scenarios involve multivariate time series data and not only univariate data. Examples include medical diagnostics 
using multiple biosignals, financial forecasting with various economic indicators, and industrial monitoring with 
multiple sensor readings. Efficient and effective handling of multivariate data is essential for advancing the 
applicability of TSC models where channel dependency is crucial for capturing discriminative patterns.

While the LITE model has demonstrated exceptional performance and efficiency for univariate TSC, 
its design requires adaptation to fully leverage the information contained in multivariate datasets. The standard 
convolution approach used in the first layer of LITE for univariate data does not fully exploit the potential of 
multivariate inputs, where interactions between different channels can provide critical insights.

The proposed LITEMV (LITE MultiVariate) architecture is designed to address these challenges. By learning 
a filter per channel and combining them effectively, LITEMV ensures that the unique contributions of 
each channel are preserved and utilized to enhance classification performance. This adaptation allows 
LITEMV to maintain the efficiency and performance advantages of LITE while being optimized for the 
complexities of multivariate time series data.

\subsection{Model Adaptation For Multivariate Time Series}\label{sec:litemv-archi}

To address the issue of effectively handling MTS data, we propose adapting the LITE 
architecture by replacing the three standard convolution layers at the beginning of the network with 
DWSC layers. DWSCs allow each channel to be processed independently, 
preserving the unique information in each channel before combining them.

Additionally, the hand-crafted convolution filters used at the beginning of the network, originally 
implemented as standard convolutions, are also replaced with DepthWise Convolutions
(DWCs, Chapter~\ref{chapitre_1} Section~\ref{sec:tsc-deep}). While we continue 
to use the same hand-crafted filters, the outputs of these DWCs are concatenated 
rather than summed. This ensures that the information from each channel is retained and effectively 
utilized in the subsequent layers of the network.

This adaptation enhances the model's ability to manage the complexities of multivariate time series data, 
leading to improved overall performance and accuracy. We refer to this enhanced multivariate version of 
LITE as LITEMV and its ensemble version LITEMVTime.
% , which leverages
% DWSCs and refined hand-crafted filters to achieve superior results in some
% MTS classification tasks where channel dependency is crucial.

In what follows we present extensive experiments to highlight the contribution of this adaptation
and its placement compared to the state-of-the-art deep learning models for multivariate TSC tasks, notably
ConvTran~\cite{convtran-paper} and Disjoint-CNN~\cite{disjoint-cnn-paper} (Chapter~\ref{chapitre_1} Section~\ref{sec:tsc-deep}).

\subsection{Experimental Setup}

We utilize the 30 datasets of the UEA archive~\cite{uea-archive} to evaluate the
performance of LITETime and LITEMVTime compared to existing deep learning models for multivariate TSC.
All datasets were z-normalized prior to training and testing independently on each channel.
% Our experiments were conducted using a GTX 4090 GPU with 24GB of VRAM.
The best-performing model during training, determined by monitoring the training
loss, was selected for testing. The Adam optimizer with a Reduce on Plateau learning
rate decay method was employed, using TensorFlow's~\cite{tensorflow-paper} default
parameter settings. Similar to LITE, each LITEMV model in the LITEMVTime ensemble was trained with
a batch size of 64 for 1500 epochs.

\subsection{Experimental Results}

\begin{table}
    \centering
    \caption{Accuracy performance in $\%$ of LITEMVTime (LMVT), LITETime (LT), ConvTran (CT), InceptionTime (IT), Disjoint-CNN (D-CNN), FCN and ResNet on $30$ datasets of the UEA archive. The datasets are ordered by their average number of training samples per class. The accuracy of the best model for each dataset is presented in bold and of the second best is underlined.}
    \label{tab:lite-results-uea}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    Dataset &
      \begin{tabular}[c]{@{}c@{}}Train Size\\ per Class\end{tabular} &
      LMVT &
      LT &
      CT &
      IT &
      D-CNN &
      FCN &
      ResNet \\ \hline
    FaceDetection             & 2945 & 61.01          & {\ul 62.37}    & \textbf{67.22} & 58.85          & 56.65          & 50.37          & 59.48          \\ \hline
    InsectWingbeat            & 2500 & 61.72          & 39.79          & \textbf{71.32} & {\ul 69.56}    & 63.08          & 60.04          & 65.00          \\ \hline
    PenDigits                 & 750  & \textbf{98.86} & {\ul 98.83}    & 98.71          & 97.97          & 97.08          & 98.57          & 97.71          \\ \hline
    SpokenArabicDigits        & 660  & 98.59          & {\ul 98.77}    & \textbf{99.45} & 98.72          & 98.59          & 98.36          & 98.32          \\ \hline
    LSST                      & 176  & \textbf{66.42} & {\ul 62.85}    & 61.56          & 44.56          & 55.59          & 56.16          & 57.25          \\ \hline
    FingerMovements           & 158  & \textbf{56.00} & 44.00          & \textbf{56.00} & \textbf{56.00} & {\ul 54.00}    & 53.00          & {\ul 54.00}    \\ \hline
    MotorImagery              & 139  & 53.00          & 51.00          & \textbf{56.00} & 53.00          & 49.00          & {\ul 55.00}    & 52.00          \\ \hline
    SelfRegulationSCP1        & 134  & 73.04          & 75.09          & \textbf{91.80} & 86.34          & {\ul 88.39}    & 78.16          & 83.62          \\ \hline
    Heartbeat                 & 102  & 61.46          & 67.80          & \textbf{78.53} & 62.48          & 71.70          & 67.80          & {\ul 72.68}    \\ \hline
    SelfRegulationSCP2        & 100  & {\ul 55.00}    & 53.89          & \textbf{58.33} & 47.22          & 51.66          & 46.67          & 50.00          \\ \hline
    PhonemeSpectra            & 85   & 15.81          & 17.45          & \textbf{30.62} & 15.86          & {\ul 28.21}    & 15.99          & 15.96          \\ \hline
    CharacterTrajectories     & 72   & \textbf{99.58} & {\ul 99.51}    & 99.22          & 98.81          & 99.45          & 98.68          & 99.45          \\ \hline
    EthanolConcentration      & 66   & \textbf{69.20} & {\ul 67.30}    & 36.12          & 34.89          & 27.75          & 32.32          & 31.55          \\ \hline
    HandMovementDirection     & 40   & 35.14          & 21.62          & {\ul 40.54}    & 37.83          & \textbf{54.05} & 29.73          & 28.38          \\ \hline
    PEMS-SF                   & 39   & 79.19          & 82.66          & 82.84          & \textbf{89.01} & \textbf{89.01} & {\ul 83.24}    & 73.99          \\ \hline
    RacketSports              & 38   & 73.68          & 78.29          & \textbf{86.18} & 82.23          & {\ul 83.55}    & 82.23          & 82.23          \\ \hline
    Epilepsy                  & 35   & \textbf{99.28} & {\ul 98.55}    & {\ul 98.55}    & \textbf{99.28} & 88.98          & \textbf{99.28} & \textbf{99.28} \\ \hline
    JapaneseVowels            & 30   & 96.49          & 97.30          & \textbf{98.91} & 97.02          & {\ul 97.56}    & 97.30          & 91.35          \\ \hline
    NATOPS                    & 30   & 90.00          & 88.89          & \textbf{94.44} & 91.66          & {\ul 92.77}    & 87.78          & 89.44          \\ \hline
    EigenWorms                & 26   & {\ul 93.89}    & \textbf{95.42} & 59.34          & 52.67          & 59.34          & 41.98          & 41.98          \\ \hline
    UWaveGestureLibrary       & 15   & 84.68          & 85.00          & {\ul 89.06}    & \textbf{90.93} & {\ul 89.06}    & 85.00          & 85.00          \\ \hline
    Libras                    & 12   & {\ul 89.44}    & 87.78          & \textbf{92.77} & 87.22          & 85.77          & 85.00          & 83.89          \\ \hline
    ArticularyWordRecognition & 11   & 97.33          & 97.67          & {\ul 98.33}    & \textbf{98.66} & \textbf{98.66} & 98.00          & 98.00          \\ \hline
    BasicMotions &
      10 &
      \textbf{100.0} &
      {\ul 95.00} &
      \textbf{100.0} &
      \textbf{100.0} &
      \textbf{100.0} &
      \textbf{100.0} &
      \textbf{100.0} \\ \hline
    DuckDuckGeese             & 10   & 18.00          & 24.00          & \textbf{62.00} & 36.00          & {\ul 50.00}    & 36.00          & 24.00          \\ \hline
    Cricket                   & 9    & {\ul 98.61}    & 97.22          & \textbf{100.0} & {\ul 98.61}    & 97.72          & 93.06          & 97.22          \\ \hline
    Handwriting               & 6    & \textbf{40.00} & 36.82          & 37.52          & 30.11          & 23.72          & {\ul 37.60}    & 18.00          \\ \hline
    ERing                     & 6    & 84.44          & 89.63          & \textbf{96.29} & {\ul 92.96}    & 91.11          & 90.37          & {\ul 92.96}    \\ \hline
    AtrialFibrillation        & 5    & 13.33          & 06.67          & \textbf{40.00} & 20.00          & \textbf{40.00} & {\ul 33.33}    & {\ul 33.33}    \\ \hline
    StandWalkJump             & 4    & \textbf{66.67} & {\ul 60.00}    & 33.33          & 40.00          & 33.33          & 40.00          & 40.00          \\ \hline
    \end{tabular}
    }
\end{table}

\begin{figure}
    \centering
    \caption{A Multi-Comparison Matrix (MCM) showcasing
    the performance of LITEMVTime, LITETime, InceptionTime,
    Disjoint-CNN and ConvTran on the 30 datasets of the UEA
    archive.}
    \label{fig:lite-uea-mcm}
    \includegraphics[width=\textwidth]{Figures/chapter_4/results/mcm_litemv_uea.pdf}
\end{figure}

In Table~\ref{tab:lite-results-uea}, we compare the accuracy performance of LITEMVTime, LITETime, and five other
leading models: ConvTran, 
InceptionTime, Disjoint-CNN, FCN, and ResNet. The accuracy metrics for these competitors are sourced from the 
ConvTran paper~\cite{convtran-paper}. LITEMVTime shows a significant performance edge when it surpasses other models,
as evidenced 
by the substantial gaps in accuracy. This is further illustrated in the MCM plot in Figure~\ref{fig:lite-uea-mcm},
where LITEMVTime's 
performance is benchmarked against the other models.

LITEMVTime ranks second in overall performance, outperforming LITETime, Disjoint-CNN and InceptionTime. 
Although LITEMVTime does not exceed ConvTran on more than eight datasets, a detailed examination 
of Table~\ref{tab:lite-results-uea} reveals that its victories are often by a wide margin. For example, on the EigenWorms 
dataset, ConvTran achieves a top accuracy of $59.34\%$, whereas LITEMVTime attains an impressive $93.89\%$, 
and LITETime achieves an even higher accuracy of $95.42\%$.
In the following section, we dig into the analysis into the common characteristics of the datasets where LITEMVTime 
wins with a significant margin compared to ConvTran.
% It is important to note that LITEMVTime reduces the number of learned parameters in LITE, thanks to the use of 
% DWSCs in the first layer.
% This is a significant benefit of the adaptations made for handling multivariate data.
Moreover, a crucial limitation of ConvTran is its sensitivity to the length of the input series. As a Self-Attention 
based network, ConvTran must store an attention score matrix of size $(L,L)$, where $L$ is the length of the 
input series, with a runtime complexity of $\mathcal{O}(L^2)$. This can create issues for long time series, 
as the model may face out-of-memory errors during training despite its architectural simplicity.

\subsection{Analysis On Dataset Characteristics When Comparing LITEMVTime and ConvTran}

\begin{figure}
    \centering
    \caption{Difference of performance between LITEMVTime and ConvTran with respect to the number
    of training samples in log scale.}
    \label{fig:litemv-train-size}
    \includegraphics[width=0.8\textwidth]{Figures/chapter_4/results/LITEMVTime_vs_ConvTran_per_Train-size-per-class.pdf}
\end{figure}

To gain a deeper understanding of the scenarios where LITEMVTime significantly outperforms ConvTran, 
we analyzed the performance differences between these two models in relation to the number of training 
samples per class. This analysis aims to uncover any patterns or commonalities in the datasets where 
LITEMVTime demonstrates superior performance.

Figure~\ref{fig:litemv-train-size} illustrates the performance gaps between LITEMVTime and
ConvTran. The most pronounced differences 
are observed in three datasets: StandWalkJump, EigenWorms, and EthanolConcentration. These datasets exhibit 
considerable variation in the number of training examples. For instance, StandWalkJump has a small training 
set with only 4 samples per class and a total of 12 training samples. On the other hand, the EthanolConcentration 
dataset contains 66 training samples per class, amounting to 261 samples in total.

This analysis indicates that LITEMVTime's enhanced performance is not confined to datasets with a specific 
size but extends across datasets with varying numbers of training examples. This highlights LITEMVTime's 
robustness and versatility in handling diverse dataset conditions, making it a reliable choice for a wide range 
of time series classification tasks.

\begin{figure}
    \centering
    \caption{Difference of performance between LITEMVTime and ConvTran with respect to the number
    of channels in log scale.}
    \label{fig:litemv-channels}
    \includegraphics[width=0.8\textwidth]{Figures/chapter_4/results/LITEMVTime_vs_ConvTran_per_dim.pdf}
\end{figure}

To further investigate why LITEMVTime performs better than ConvTran on certain datasets, we can analyze the 
impact of the number of dimensions in multivariate time series data. Figure~\ref{fig:litemv-channels}
illustrates the same performance 
differences shown in Figure~\ref{fig:litemv-train-size}, but this time as a function of the number of dimensions in the datasets.
The analysis highlights that the datasets where LITEMVTime shows significant superiority,StandWalkJump, 
EigenWorms, and EthanolConcentration,all have a relatively small number of dimensions. However, 
ConvTran consistently outperforms LITEMVTime as the number of dimensions increases.

Figures~\ref{fig:litemv-train-size} and~\ref{fig:litemv-channels} illustrate that LITEMVTime is more suitable
for scenarios with small training data
and a limited number of channels. This limitation of ConvTran was also discussed in the original paper~\cite{convtran-paper}.


\section{Discussion Over Limitations of LITE and LITEMV}

LITE and LITEMV are designed with low complexity, making them efficient compared to other architectures. However, 
this simplicity may pose a limitation when dealing with very large datasets. For instance, these models might not 
perform optimally with training sets comprising millions of samples. This limitation can be mitigated by increasing 
the number of filters in the DWSC layers, which, thanks to the efficient convolution application of DWSCs, would 
result in only a slight increase in computational cost.
Assuming an input MTS of $M$ channels and the output target dimension we want is $M^{'}$ produce by a convolution layer 
with kernel size $K$, then the ratio between number of parameters needed in the cases of standard and DWSC layers is:
\begin{equation}\label{equ:lite-std-vs-dwsc-params}
\begin{split}
    Ratio &= \dfrac{number~of~parameters~standard}{number~of~parameters~DWSC}\\
    &= \dfrac{M.M^{'}.K}{M.K + M.M^{'}}\\
    &= \dfrac{M^{'}.K}{K+M^{'}}
\end{split}
\end{equation}

\begin{figure}
    \centering
    \caption{Number of parameters of Standard Convolutions and DWSCs in function of number of convolution filters 
    to learn and their kernel size.}
    \label{fig:params-convolution-ctr}
    \includegraphics[width=\textwidth]{Figures/chapter_4/dwsc_params.pdf}
\end{figure}

As seen in Eq.~\ref{equ:lite-std-vs-dwsc-params} and illustrated in Figure~\ref{fig:params-convolution-ctr},
independently of the number of input channels $M$, the number of parameters 
of the Standard Convolutions increases much faster compared to DWSCs.
For instance, the number of parameters in Standard Convolutions increases quadratically, while Depthwise Separable
Convolutions (DWSCs) exhibit a linear increase.

A second limitation common to all architectures discussed in this work relates to the length of the time series 
samples. This can be addressed by enhancing the CNN's Receptive Field (RF), which determines the length of 
the input visible to the CNN at the last layer. For a CNN with $\Lambda$ convolution layers, each with kernel size $K_i$
and dilation rate $d_i$ where $i~\in~[1,\Lambda]$, the RF is calculated as:

\begin{equation}\label{equ:receptive-field}
    RF = 1+\sum_{i=1}^{\Lambda}~d_i.(K_i-1)
\end{equation}

The RF varies between different CNN models. For example, the RF for FCN~\cite{fcn-resnet-mlp-paper}
is $14=1+7+4+2$, which is relatively small 
compared to the time series lengths in the UCR archive. In contrast, ResNet has an RF of $40$, and Inception extends 
this to $235$. For LITE and LITEMV, the RF is $114$, which is sufficient for state-of-the-art performance on the UCR archive. 
However, for datasets with much longer time series, this RF needs to be increased.

The RF can be expanded by either increasing the filter lengths or adding more layers to deepen the model. 
While this typically leads to a substantial increase in network complexity for conventional CNNs, LITE and 
LITEMV can accommodate these adjustments without significant complexity increases, making them well-suited for 
handling longer time series while maintaining their efficiency and performance.

\section{Conclusion}

In this chapter we introduced LITE: a lightweight, Inception-based architecture enhanced with boosting techniques for TSC.
Through rigorous experimentation and analysis, we demonstrated that LITE achieves competitive 
performance while maintaining a significantly lower number of parameters compared to more complex models like
Inception, FCN, and ResNet.

The ensemble approach, LITETime, further capitalizes on the strengths of LITE, reducing variance and enhancing robustness. 
We explored the impact of boosting techniques such as hand-crafted convolution filters, multiplexing convolutions, and dilated 
convolutions through comprehensive ablation studies, confirming their contributions to the model's overall efficacy.

Moreover, recognizing the importance of multivariate time series classification in real-world applications, 
we introduced LITEMV. This adaptation preserves the efficiency of LITE while extending its capabilities to
handle the complexities of multivariate data. Our experiments confirmed that LITEMV performs exceptionally 
well in its ensemble version LITEMVTime, often surpassing more memory-intensive models like ConvTran in various scenarios.

We also addressed the limitations of our proposed architectures, particularly in handling extremely large 
datasets. We discussed potential solutions such as increasing the number of filters in 
DWSC layers and expanding the receptive field, which LITE and LITEMV can achieve with minimal increase in complexity.

In summary, LITE and LITEMV present a significant advancement in time series classification, offering a 
balance of efficiency, performance, and adaptability. These models are well-suited for deployment in 
resource-constrained environments, making them practical for a wide range of real-world applications.