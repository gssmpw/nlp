\chapter{Evaluation Metrics For Human Motion Generation}\label{chapitre_7}

\section{Introduction}

Evaluating generative models presents unique challenges~\cite{reliable-fidelity-diversity}
that are not as prevalent in discriminative models, where comparisons to 
ground truth data are straightforward. For generative models, 
the evaluation involves measuring the validity of generated samples 
against real ones. Traditional human judgment metrics, such as 
Mean Opinion Scores (MOS)~\cite{mos-paper}, often fall short as they assume a 
uniform user perception of ideal generation, which is unrealistic. 
Therefore, quantitative evaluation is essential, focusing on two 
key dimensions: fidelity and diversity. Fidelity assesses the 
similarity between the distributions of real and generated data, 
while diversity measures the variety within the generated samples, 
ensuring they reflect the range present in real data sets.

In the previous chapter, we reviewed existing works on human motion generation, 
which typically use standard evaluation methods and sometimes introduce new metrics. 
The main challenge is defining fidelity and diversity metrics, as no single optimal 
solution exists, \textit{No Metric To Rule Them All}.
Consequently, numerous approaches and novel metrics have been developed 
to address this issue. However, evaluating these metrics alone is insufficient, 
and inconsistencies in generation setups and frameworks post-training across 
different studies make model comparisons problematic. This complexity underscores 
the need for a unified and detailed evaluation framework.

A crucial aspect of evaluating human motion data is its temporal dependency~\cite{ucr-archive}.
Temporal 
distortion, which includes time shifts, frequency changes, and warping, is vital for 
assessing multivariate time series~\cite{uea-archive}
such as human motion sequences. Existing metrics often 
overlook this, focusing instead on latent representations. To address this gap, we 
introduce a novel metric called Warping Path Diversity (WPD). WPD measures the diversity 
of temporal distortions in both real and generated data, scoring models based on their 
ability to produce varied temporal sequences, thus ensuring a more comprehensive evaluation.

Unifying the evaluation process is essential. This work consolidates evaluation metrics 
from the literature into a unified framework for fair comparisons,
providing a helpful 
resource for newcomers. Our experiments highlight the difficulty of identifying a 
universally superior model, as small changes in architecture and hyperparameters 
can significantly impact metric values. We conduct detailed experiments with three 
CVAE model variants on the same dataset, offering an in-depth analysis of each metric.

\begin{figure}
    \centering
    \caption{
        The evaluation metrics for human motion generation in this work are 
        divided into two groups: \protect\mycolorbox{0,117,185,0.64}{fidelity metrics}
        and \protect\mycolorbox{0,150,59,0.64}{diversity metrics}.
        These metrics are further categorized based on their evaluation 
        criteria, such as FID being a distribution-based metric.
    }
    \label{fig:metrics-summary}
    \includegraphics[width=0.7\textwidth]{Figures/chapter_7/summary.pdf}
\end{figure}

Figure~\ref{fig:metrics-summary} presents a brief summary 
of all the metrics used in this work, categorized 
into fidelity and diversity. The metrics are further organized into 
sub-categories based on their evaluation approach: accuracy-based, distribution-based, 
distance-based, neighbor-based, neighbor/distance-based, and warping-based.

In this chapter, we propose a clear, user-friendly evaluation framework for 
newcomers to the field. By establishing 
standardized practices, we aim to facilitate more consistent and meaningful 
comparisons of generative models, ultimately contributing to the advancement 
of human motion generation research.

\section{Generative Models Metrics}

The evaluation metrics for generative models are categorized into fidelity and diversity. 
Fidelity metrics evaluate how well generated samples mirror the real distribution, 
making it harder to distinguish between real and generated data, thus ensuring 
reliability. Diversity metrics assess the variation among generated samples, 
indicating the model's ability to produce a wide range of outputs. Higher diversity 
means the model isn't limited to a narrow segment of the real distribution. 

This section first defines key concepts, then thoroughly reviews the main metrics 
used for assessing both fidelity and diversity.

\subsection{Definitions}

To understand the metrics that follow, it's necessary to establish some definitions:

\mydefinition Real set of samples: a set of $N$ real samples is
referred to as $\mathcal{X}~=~\{\textbf{x}_i\}_{i=1}^{N}$ and follows a distribution \pr;
where $\textbf{x}_i$ is an MTS of length $L$ and dimensions $M=JxD$, with $J$ being the number 
of skeleton joints and $D$ the dimension of each joint.

\mydefinition Generated set of samples: a set of $G$ generated samples is referred to
as $\hat{\mathcal{X}}~=~\{\hat{\textbf{x}}_j\}_{j=1}^{G}$ and follows a distribution \pg.

\mydefinition A pre-trained deep learning model $\mathcal{G}\circ\mathcal{F}(.)$ 
is made of a feature extractor $\mathcal{F}$ and a last layer $\mathcal{G}$ 
achieving the desired task (e.g. classification).

\begin{figure}
    \centering
    \caption{
        Before calculating evaluation measures, two steps are followed. First, a 
        \protect\mycolorbox{0,194,200,0.47}{model} is trained on a 
        \protect\mycolorbox{156,120,255,0.6}{supervised task} using only
        \protect\mycolorbox{0,50,255,0.6}{real data} and not the 
        \protect\mycolorbox{255,165,0,0.6}{generated data}. Second, the 
        pre-trained encoder's \protect\mycolorbox{0,125,0,0.6}{latent representation of the
        real data}
        is extracted, as well as the \protect\mycolorbox{255,30,0,0.5}{latent 
        representation of the generated samples}.
        The metrics are then 
        computed based on this latent representation.
    }
    \label{fig:metrics-latent-space}
    \includegraphics[width=\textwidth]{Figures/chapter_7/latent_space_encoding.pdf}
\end{figure}

To compute most metrics, we first train a deep learning model $\mathcal{G}\circ\mathcal{F}$
on a specific 
task, typically classification, using real data. Here, $\mathcal{G}$ is a $softmax$ layer. 
The feature extractor is then used to project both real data $\mathcal{X}$ and generated 
data $\hat{\mathcal{X}}$ into a latent space, enabling metric calculations within this space. 
This process involves two steps, as illustrated in Figure~\ref{fig:metrics-latent-space}:
training the model on real data and using the 
feature extractor (excluding the final layer) to encode both real and generated 
samples into latent spaces $\textbf{V}$ and $\hat{\textbf{V}}$, respectively, such as:
\begin{equation}\label{equ:metrics-latent-space}
    \textbf{V} = \mathcal{F}(\mathcal{X}) \;\; \text{and} \;\; \hat{\textbf{V}} = \mathcal{F}(\hat{\mathcal{X}})
\end{equation}
\noindent where both $\textbf{V}$ and $\hat{\textbf{V}}$ are two-dimensional
matrices, corresponding to the number of examples in $\mathcal{X}$ and $\hat{\mathcal{X}}$
respectively, with each dimension representing features $f$.

For each metric discussed in this section, we compute two versions: one using 
generated samples (and real samples, as applicable) and one using only real 
samples. This approach provides a reference metric value for \pr. We achieve 
this by randomly splitting $\textbf{V}$ into two subsets, $\textbf{V}_1$ and 
$\textbf{V}_2$. Metrics are then calculated by treating $\textbf{V}_1$ as the 
latent space for real samples and $\textbf{V}_2$ as the latent space for generated samples.

\subsection{Fidelity Metrics}

The fidelity metrics in the literature are either
distribution based, neighbor based or accuracy based.

\subsubsection{Fréchet Inception Distance (FID)}\label{sec:metrics-fid}

In the previous chapter, we utilized the Fréchet Inception Distance (FID) due to 
its widespread use in evaluating generative models. In this section, we present 
the history and background of this metric.
Introduced by~\cite{fid-original-paper}, the Fréchet Inception Distance (FID) 
is a popular metric for evaluating generative models. It builds on the Inception Score 
(IS)~\cite{is-paper}, which assesses generated samples using
a pre-trained Inception model, indirectly 
considering the real distribution \pr. Unlike IS, FID quantifies the difference 
between the real distribution \pr~and the generated distribution \pg. It does 
this by calculating the Fréchet Distance~\cite{fd-paper} between two Gaussian distributions in the 
Inception model's latent space of both $\mathcal{X}$ and $\hat{\mathcal{X}}$.

The Fréchet Distance (FD)~\cite{fd-paper} measures the similarity between two continuous curves. 
To understand what FD
measures, a famous example goes as follows: \textit{Imagine a person and their dog, each 
wanting to traverse a different finite curved
path. The speed of the person and the dog can vary but they are not 
allowed to go backward on the path. The FD between these
two curves is the length of a leash, small enough so that both the person and the 
dog can traverse the whole finite curve}. For probability distributions, FD is 
calculated between their 
Cumulative Distribution Functions (CDFs)~\cite{fd-dist-paper,}. For multidimensional Gaussian
distributions~\cite{fid-gaus-paper} $\mathcal{P}_1 \sim \mathcal{N}(\mu_1,\Sigma_1)$ 
and $\mathcal{P}_2\sim\mathcal{N}(\mu_2,\Sigma_2)$, both of dimension $f$, the FD 
is calculated as follows:
\begin{equation}\label{equ:metrics-fid}
    FD(\mathcal{P}_1,\mathcal{P}_2)^2 = \textit{trace}(\Sigma_1+\Sigma_2-2(\Sigma_1.\Sigma_2)^{1/2}) + \sum_{i=1}^{f}(\mu_{1,i}-\mu_{2,i})^2 
\end{equation}
\noindent where the values of $FD$ (or $FID$) range from $0$ to $+\infty$.

\paragraph*{Setup for generative models}
First, we empirically estimate the mean vectors $\boldsymbol{\mu}$ and $\hat{\boldsymbol{\mu}}$
for both $\textbf{V}$ and $\hat{\textbf{V}}$, along with their covariance
matrices $\Sigma$ and $\hat{\Sigma}$.
Second, we compute the Fréchet Distance (FD) using Eq.~\ref{equ:metrics-fid}.
For consistency with the literature, we refer to this metric as FID throughout this 
work, even though the Inception network is not used for human motion.

\begin{figure}
    \centering
    \caption{
        On the left, we illustrate the energy (FID) needed to transform a 
        \protect\mycolorbox{0,100,255,0.6}{standard 
        Gaussian distribution} into another \protect\mycolorbox{255,30,0,0.6}{Gaussian
        distribution with a 
        higher mean and variance}. On the right, the plot shows that as 
        the mean ($\mu$) and variance ($\sigma^2$) of the target distribution increase, 
        the required transformation energy (FID) also increases progressively.
    }
    \label{fig:metrics-fid}
    \includegraphics[width=\textwidth]{Figures/chapter_7/fid-change-vs-gauss.pdf}
\end{figure}

\paragraph*{Interpretation}
The FID represents the amount of energy or effort needed to transform one Gaussian 
distribution into another. Figure~\ref{fig:metrics-fid} illustrates this concept, showing the 
probability density functions of two Gaussian distributions. The energy 
required to change  $\mathcal{N}(\mu=0.0, \sigma^2=1.0)$ to $\mathcal{N}(\mu=1.2, \sigma^2=1.8)$
increases with the mean and variance differences. Since no energy is needed to 
transform a distribution into itself, the starting point on the plot 
(right side of Figure~\ref{fig:metrics-fid}) is $0.0$. This aligns with the
FID's definition as a distance metric.

Many studies claim that a lower FID indicates higher fidelity in generated 
samples. However, this can be misleading. For instance, if a generative model 
merely replicates real samples, it would achieve a perfect FID of 0, showing 
no new value. Therefore, to accurately compare two generative models using the 
FID metric, we propose following this principle:
\begin{theorem}[Fréchet Inception Distance Interpretation]\label{the:metrics-fid}
    A generative model $Gen_1$ is considered more fidelitous than another 
    model $Gen_2$ on the $FID$ metric if $FID_{gen1} < FID_{gen2}$ while 
    respecting the following constraint:
    \begin{equation*} 
        \forall~\epsilon>0, \hspace{1cm} FID_{gen} = FID_{real} + \epsilon
    \end{equation*}
\end{theorem}

\subsubsection{Accuracy On Generated (AOG)}\label{sec:metrics-aog}

Generative models can incorporate relevant characteristics of each sample for better 
control. For labeled datasets, this might include discrete labels, continuous values, 
or text descriptions. This conditioning, as mentioned in Chapter~\ref{chapitre_6}
enhances the precision of generated outputs. 
For example, a model generating human motion sequences can be conditioned to produce 
specific actions like ``running'' or ``jumping'' ensuring alignment with the desired activity.

To evaluate the conditioning capability of a generative model, we can use the score 
of a classifier pre-trained on real samples $\mathcal{X}$,
treating the generated set $\hat{\mathcal{X}}$ as unseen data.
The classifier, $\mathcal{G}\circ\mathcal{F}$,
helps measure this capability. We refer to this metric as Accuracy On Generated (AOG), adapted from the Accuracy 
metric of~\cite{action2motion-paper}, 
formulated as follows:
\begin{equation}\label{equ:metrics-aog}
    AOG(\hat{\mathcal{X}},~\hat{Y},~\mathcal{G}\circ\mathcal{F}) = \dfrac{1}{G}\sum_{i=1}^{G}\mathds{1}\{\mathcal{G}\circ\mathcal{F}(\hat{\mathcal{X}}_i) == \hat{Y}_i\}
\end{equation}
\noindent where $\hat{Y}$ represents the set of labels employed to condition 
the generation process, serving as ground truth labels that 
$\mathcal{G}\circ\mathcal{F}$ is expected to predict.
Additionally, $\mathds{1}$ denotes the indicator function defined as:
\begin{equation}
    \mathds{1}\{condition\}=
    \begin{cases}
        1 & if\text{ }condition\text{ }is\text{ }True\\
        0 & if\text{ }condition\text{ }is\text{ }False
    \end{cases}
\end{equation}
\noindent where the values of AOG in Eq.~\ref{equ:metrics-aog} range from $0$ to $1$.

\paragraph*{Setup for generative models}
The AOG metric is calculated by comparing the ground truth labels $\hat{Y}$
with the predictions made by $\mathcal{G}\circ\mathcal{F}$ using Eq.~\ref{equ:metrics-aog}.

\begin{figure}
    \centering
    \caption{
        This example demonstrates the AOG metric for two generative models.
        \protect\mycolorbox{0,175,105,0.53}{$Model1$} achieves a perfect AOG of $100\%$
        by accurately generating samples conditioned on three classes 
        \protect\mycolorbox{255,50,0,0.6}{red triangles},
        \protect\mycolorbox{0,100,255,0.6}{blue squares} and 
        \protect\mycolorbox{0,140,0,0.6}{green circles}. 
        In contrast, \protect\mycolorbox{176,175,105,0.53}{$Model2$} only achieves $50\%$,
        indicating it struggles with correct conditional generation. The AOG metric
        reflects the classification accuracy of generated samples compared to ground
        truth labels $\hat{Y}$. 
    }
    \label{fig:metrics-aog}
    \includegraphics[width=\textwidth]{Figures/chapter_7/aog.pdf}
\end{figure}

\paragraph*{Interpretation}
The AOG metric is a strong indicator of a generative model's conditioning capability, 
but it must be interpreted carefully. A very low AOG might suggest the model 
generates samples from a narrow set of labels. Conversely, a perfect AOG score 
doesn't necessarily mean high fidelity to \pr; the generated samples could contain 
residual noise that doesn't affect the classifier's performance. Therefore, a high 
AOG might give a false impression of generation quality. Figure~\ref{fig:metrics-aog}
demonstrates this nuanced behavior of the AOG metric.

\subsubsection{Density and Precision}\label{sec:metrics-density}

Using a single metric to evaluate generative models is often insufficient due to 
the complexity of assessing both fidelity and diversity.~\cite{precision-recall-paper}
highlighted that two models might share the same FID score but differ in these 
qualities. To address this,~\cite{precision-recall-paper}
introduced precision and recall metrics to 
evaluate fidelity and diversity separately.~\cite{improved-precision-recall-paper} refined 
these metrics by incorporating the $k$-nearest neighbor algorithm to better 
estimate density functions. In particular, precision represents the portion 
of samples generated by \pg~that can be sampled from \pr~as well., and 
its improved formulation (referred to as precision in the rest of this work) is as follows:
\begin{equation}\label{equ:metrics-precision}
    precision(\textbf{V},\hat{\textbf{V}},k) = \frac{1}{G}\sum_{j=1}^{G}\mathds{1}(\hat{\textbf{V}}_j\in manifold(\textbf{V}_1,\ldots,\textbf{V}_N))
\end{equation}
\noindent where $manifold(\{a_1,a_2,\ldots,a_n\}) = \bigcup_{i=1}^{n}B(a_i,NND_k(a_i))$, 
$B(c,r)$ is a sphere in $\mathds{R}^{dimension(a_i)}$ of center $c$ and radius $r$, 
and $NND_k(a_i)$ is the distance from $a_i$ to its $k_{th}$ nearest neighbor in the 
set $\{a_j\}_{j=1,j\neq i}^{N}$. The values of precision range from $0$ to $1$.

\cite{reliable-fidelity-diversity} identified key limitations in the precision and 
recall metrics for evaluating generative models. They proposed density and coverage 
as new metrics for fidelity and diversity, respectively. This section addresses fidelity, 
with diversity discussed in Section~\ref{sec:metrics-diversity}.
The precision metric has two main issues: 
(1) no closed formulation for the expected precision when real and generated samples 
follow the same distribution (i.e. \pr~and \pg~are identical), and
(2) outliers in the real data $\mathcal{X}$ can produce misleading 
precision values, suggesting good performance even when it is not accurate.

The mathematical formulation of the density metric is as follows:
\begin{equation}\label{equ:metrics-density}
    density(\textbf{V},\hat{\textbf{V}},k) = \frac{1}{k.G}\sum_{j=1}^{G}\sum_{i=1}^{N}\mathds{1}(\hat{\textbf{V}}_j\in B(\textbf{V}_i,NND_k(\textbf{V}_i)))
\end{equation}
\noindent where the values of $density$ range from $0$ to $\dfrac{N}{k}$.

\cite{reliable-fidelity-diversity} showed that when real and generated distributions (\pr~and~\pg) 
are identical, the expected value of density is $1$. This means that with enough samples 
and a high number of neighbors, the density metric converges to $1$, accurately 
reflecting the fidelity of generated samples. Additionally, density has an advantage 
over precision by detecting outliers in the real distribution.

\begin{figure}
    \centering
    \caption{
        This example demonstrates the computation of density and precision metrics on a 
        synthetic dataset. The left side shows the latent representation of: 
        \protect\mycolorbox{0,100,255,0.6}{real data},
        \protect\mycolorbox{255,30,0,0.6}{the real outlier}, 
        generated samples \protect\mycolorbox{0,0,0,0.2}{near the outlier} 
        and \protect\mycolorbox{0,127,0,0.6}{near non-outliers}.
        For each real sample we represent its neighborhood area. 
        The right side depicts the original data series. The density 
        metric, unlike precision, correctly identifies the outlier, 
        giving a score of $1.25$ instead of $1$. This illustrates how density 
        better reflects fidelity by accounting for outliers. Both metrics use $2$ neighbors.
    }
    \label{fig:metrics-density}
    \includegraphics[width=\textwidth]{Figures/chapter_7/density-nn.pdf}
\end{figure}

Figure~\ref{fig:metrics-density} illustrates an outlier scenario with a
synthetic dataset. On the left, 
it shows the latent space of five real samples and four generated samples, 
highlighting an outlier in red. The generated samples correctly cluster around 
the four real samples, with two near the outlier. Precision fails here, giving a 
misleading perfect score of $1$, which suggests high fidelity. However, the density 
metric reveals the issue, as it identifies the two generated samples influenced by 
the outlier, resulting in a more accurate, non-perfect score of $1.25$.

\paragraph*{Setup for generative models}
First, we determine the distance from each sample in $\textbf{V}=\mathcal{F}(\mathcal{X})$
to its $k_{th}$ nearest neighbor within $\hat{\textbf{V}}=\mathcal{F}(\hat{\mathcal{X}})$.
Next, we compute the precision and density metrics using Eqs.~\ref{equ:metrics-precision}
and~\ref{equ:metrics-density}, respectively.

\paragraph*{Interpretation}
The precision metric measures the number of generated samples that fall within at 
least one real sample's neighboring sphere. Conversely, the density metric counts 
how many neighboring spheres each generated sample occupies. While both metrics 
assess how well generated samples match real ones, density provides a more detailed 
analysis by considering each real-generated pair individually. Precision, however, 
overlooks potential biases from real outliers by focusing only on the union of neighboring 
spheres, missing the nuanced fidelity captured by density.

\subsection{Diversity Metrics}\label{sec:metrics-diversity}

Evaluating fidelity alone does not fully ensure the reliability of generated 
samples; hence, diversity measures are necessary. Next, we introduce the 
diversity metrics commonly used in the literature, including both distance-based 
and neighbor-based approaches.

\subsubsection{Average Pair Distance (APD)}\label{sec:metrics-apd}

Another common metric we used in the previous chapter is the Average Pair Distance (APD). 
Originally proposed by~\cite{apd-original-paper} for measuring distances between images, 
it was adapted by~\cite{action2motion-paper} for evaluating the diversity of human motion 
generative models. APD calculates the average Euclidean Distance, in the latent space of the pre-trained encoder,
between randomly 
selected sample pairs, repeated $R$ times over $S_{apd}$ pairs. The final APD value is the average 
result of these experiments. This metric can evaluate the diversity of any dataset, 
not just generated samples.
The APD metric calculated on the generated set of samples, for one
random selection $r~\in~\{1,2,\ldots,R\}$ is formulated as follows:
\begin{equation}\label{equ:metrics-apd}
    APD_r(\mathcal{S},\mathcal{S}^{'}) = \dfrac{1}{S_{apd}}\sum_{i=1}^{S_{apd}} \sqrt{\sum_{j=1}^{f} (\mathcal{S}_{i,j} - \mathcal{S}^{'}_{i,j})^2} 
\end{equation}
\noindent where $\mathcal{S}$ and $\mathcal{S}^{'}$ are two randomly 
selected subsets of $\hat{\textbf{V}}=\mathcal{F}(\hat{\mathcal{X}})$, 
i.e. $\mathcal{S},\mathcal{S}^{'}~\subset \hat{\textbf{V}}$.

The APD metric is then calculated by averaging over the $R$ random experiments:
\begin{equation}\label{equ:metrics-apd-all}
APD(\hat{\mathcal{X}}) = \dfrac{1}{R}\sum_{r=1}^{R} APD_r(\mathcal{S}^r,\mathcal{S}^{'r})
\end{equation}

\noindent where $APD_r(\hat{\mathcal{X}})$ is calculated using Eq.~(\ref{equ:metrics-apd}).
An illustration of the APD metric is represented in Figure~\ref{fig:metrics-apd} 
highlighting the procedure to calculate APD on real and generated data separately.

\begin{figure}
    \centering
    \caption{
        This example demonstrates the calculation of the APD metric for both
        \protect\mycolorbox{0,100,255,0.6}{real samples} 
        and \protect\mycolorbox{255,30,0,0.6}{generated samples}. 
        For each latent representation, two sets $\mathcal{S}$ and $\mathcal{S}^{'}$
        of randomly selected values, 
        each of size $S_{apd}$, are created. The APD metric is then computed 
        between these sets. This process is repeated RR times, and the final APD 
        value is the \protect\mycolorbox{0,128,0,0.6}{average of all computed APD values}.
    }
    \label{fig:metrics-apd}
    \includegraphics[width=\textwidth]{Figures/chapter_7/apd_only_latent.pdf}
\end{figure}

\paragraph*{Setup for generative models}
We calculate the APD metric using Eqs~\ref{equ:metrics-apd} and~\ref{equ:metrics-apd-all}.
This is done for both real and generated datasets independently of each other, with only the encoder, pre-trained 
on real data, used for both computations.

\paragraph*{Interpretation}
The APD metric assesses whether a generative model can avoid mode collapse, 
where it generates the same outcome repeatedly. Ideally, the APD metric should 
be as high as possible to indicate diverse outputs. However, a potential issue 
arises if APD$_{gen}>APD_{red}$, as it suggests that the generated space is more 
diverse than the real one, which is an implausible outcome.
To address this,~\cite{action2motion-paper}
provided further interpretation, leading to the following theorem:

\begin{theorem}[Average Pair Distance Interpretation]\label{the:metrics-apd}
    A generative model $Gen_1$ is considered more diverse
    than another model $Gen_2$ if $APD_{gen1} > APD_{gen2}$,
    while respecting the following constraint:
    \begin{equation*} 
        \forall~\epsilon>0, |APD_{gen1} - APD_{real}| < \epsilon 
    \end{equation*}
\end{theorem}

In simpler terms, a generative model's APD diversity should not surpass that of the 
real distribution \pr. To illustrate, if real data has an APD$_{real}$ of $5$
and a generative model is randomly initialized without training, the APD$_{gen}$
could exceed $5$. However, this higher diversity would be due to random generation,
not a meaningful correlation with \pr. This demonstrates that exceeding too much the real 
data's diversity doesn't necessarily reflect a well-trained model.

\subsubsection{Average per Class Pair Distance (ACPD)}\label{sec:metrics-acpd}

The Average per Class Pair Distance (ACPD)~\cite{action2motion-paper}
metric, like APD, evaluates the 
diversity of generated samples but at a more detailed level. While APD 
measures diversity across the entire distribution \pr, ACPD focuses on individual 
sub-clusters within \pr. This allows for a more nuanced assessment of how well 
the model captures diversity within specific categories. ACPD computes the 
average APD for each sub-cluster, formed using the class labels, providing insights into class-specific diversity. 
The mathematical formulation of ACPD is as follows:
\begin{equation}\label{equ:metrics-acpd}
    ACPD_r(\mathcal{S},\mathcal{S}^{'}) = \dfrac{1}{C.S_{acpd}}\sum_{i=1}^{S_{acpd}}\sqrt{\sum_{j=1}^{f} (\mathcal{S}_{c,i,j} - \mathcal{S}^{'}_{c,i,j})^2} 
\end{equation}
\noindent where $C$ is the total number of classes in \pr, 
$\mathcal{S}_c,\mathcal{S}^{'}_c$ are randomly selected 
subsets from $\hat{\textbf{V}}[\hat{Y} = c]$, and $\hat{Y}$ 
are the labels used to generated $\hat{\mathcal{X}}$.

Similar to APD, due to the randomness involved in ACPD, the experiment is 
repeated $R$ times to calculate ACPD$_r$ for $r~\in~\{1,2,\ldots,R\}$.
The final ACPD value is then obtained by averaging these repeated calculations:
\begin{equation}\label{equ:metrics-acpd-all}
    ACPD(\hat{\mathcal{X}}) = \dfrac{1}{R}\sum_{r=1}^{R}ACPD_r(\mathcal{S}^r,\mathcal{S}^{'r})
\end{equation}
It is important to note that this metric is restricted only to labeled datasets where 
labels are discrete, e.g. classification.

\paragraph*{Setup for generative models}
We calculate ACPD using Eqs.~\ref{equ:metrics-acpd} and~\ref{equ:metrics-acpd-all}.
This is done for both real and generated datasets independently of each other, with only the encoder, pre-trained 
on real data, used for both computations.

\paragraph*{Interpretation}

The ACPD metric assesses the diversity of generated samples within each sub-cluster 
of \pr, ensuring that the model doesn't over-focus on a single cluster. 
This addresses the common issue of imbalanced labeled data in machine 
learning, where some classes may have more diversity than others.
Similar to the APD metric, ACPD is calculated for both real and generated samples, 
resulting in ACPD$_{real}$ and ACPD$_{gen}$.
A generative model is deemed class diverse when ACPD$_{gen}$
closely matches ACPD$_{real}$, indicating balanced generation across all categories.

\subsubsection{Coverage and Recall}\label{sec:metrics-coverage}

As mentioned in Section~\ref{sec:metrics-density},~\cite{reliable-fidelity-diversity}
proposed new metrics to replace the improved precision and recall metrics 
introduced by~\cite{precision-recall-paper,improved-precision-recall-paper}.
Their recall metric measures how well the generated distribution \pg~can
sample real examples from \pr. This is achieved by counting the number 
of real samples that appear in at least one neighborhood of a generated sample. 
The recall metric is formulated as follows:
\begin{equation}\label{equ:metrics-recall}
    recall(\textbf{V},\hat{\textbf{V}},k) = \dfrac{1}{N}\sum_{i=1}^{N}\mathds{1}(\textbf{V}_i~\in~manifold(\hat{\textbf{V}}_1,\hat{\textbf{V}}_2,\ldots,\hat{\textbf{V}}_M)) 
\end{equation}
\noindent where $manifold(.)$ and $\mathds{1}(.)$ follow the same definition 
detailed in Section~\ref{sec:metrics-density}. The recall metric is bounded between $0$ and $1$.

\cite{reliable-fidelity-diversity} identified several limitations of
the recall metric, summarized as follows:
\begin{enumerate}
    \item Defining neighborhood areas based on generated samples can lead 
    to misinterpretations, as outliers are more likely to be 
    sampled by \pg~than by~\pr.
    \item There is no closed-form solution for the expected value of the recall 
    metric when \pr~and \pg~are identical distributions, complicating the evaluation process.
\end{enumerate}

To overcome the limitations of the recall metric,~\cite{reliable-fidelity-diversity}
proposed the coverage metric. This metric focuses on neighborhood areas around 
the real samples $\mathcal{X}$. It counts how many real samples include at 
least one generated sample from $\hat{\mathcal{X}}$. The coverage metric provides 
a more accurate representation by measuring the presence of generated samples 
within the vicinity of real samples. The formulation of the coverage metric is as follows:
\begin{equation}\label{equ:metrics-coverage}
    coverage(\textbf{V},\hat{\textbf{V}},k)=\dfrac{1}{N}\sum_{i=1}^{N}\mathds{1}(\exists~j~s.t.~\hat{\textbf{V}}_j~\in~B(\textbf{V}_i,NND_k(\textbf{V}_i))) 
\end{equation}
    
\noindent where $B(.,.)$ and $NND_k(.)$ follow the same definitions detailed 
in Section~\ref{sec:metrics-density}. The coverage metric is bounded between $0$ and $1$.

\begin{figure}
    \centering
    \caption{
        The computation of coverage and recall metrics over a synthetic dataset is 
        shown on the left and right side of the figure.
        The figure's left and right sides depict the latent space with 
        \protect\mycolorbox{0,100,255,0.6}{real samples} and
        generated samples both \protect\mycolorbox{0,128,0,0.6}{reliable},
        and \protect\mycolorbox{255,30,0,0.6}{outliers}. 
        The middle shows the original series space, highlighting the differences
        between the three spaces. 
        The left plot shows neighbor areas around generated samples (recall metric), 
        while the right plot shows neighbor areas around real samples (coverage metric). 
        The coverage metric correctly identifies outliers, resulting in a non-perfect 
        measure, unlike the recall metric. Both metrics use $2$ neighbors.
    }
    \label{fig:metrics-coverage}
    \includegraphics[width=\textwidth]{Figures/chapter_7/coverage-nn-Synthetic.pdf}
\end{figure}

The coverage metric addresses the recall metric's limitation by avoiding the use of 
generated sample neighborhoods. Figure~\ref{fig:metrics-coverage} illustrates this 
with a synthetic example. 
The recall metric falsely shows perfect diversity due to over-estimated 
neighborhoods caused by outliers (left scatter plot). In contrast, the 
coverage metric accurately differentiates between valid and outlier samples
by focusing on the neighborhoods of real samples. This results in a more 
reliable assessment of the generative model's diversity.

The coverage metric also resolves the second limitation of recall, 
as demonstrated by~\cite{reliable-fidelity-diversity}. They showed that 
for identical distributions \pr~and~\pg, the expected value of coverage 
has a simplified closed-form solution, as such:
\begin{equation}\label{equ:metrics-expected-coverage}
    \mathds{E}[coverage] = 1-\dfrac{(N-1)\ldots(N-k)}{(G+N-1)\ldots(G+N-k)}
\end{equation}
\noindent which reduces to, in the case where both $N$ and $G$ are high enough:
\begin{equation}\label{equ:metrics-expected-coverage-high-nm}
    \mathds{E}[coverage] = 1 - \dfrac{1}{2^k} .
\end{equation}

\paragraph*{Setup for generative models}
First, we calculate the recall metric by determining the distance of each 
sample in $\hat{\textbf{V}}$ to its $k_{th}$ nearest neighbor in $\hat{\textbf{V}}$
and applying Eq.~\ref{equ:metrics-recall}. Second, we compute the coverage 
metric by finding the distance of each sample in $\textbf{V}$ to its $k_{th}$ 
nearest neighbor in $\textbf{V}$ and using Eq.~\ref{equ:metrics-coverage}.

\paragraph*{Interpretation}
The recall metric calculates the proportion of real samples that fall within 
the neighborhood of at least one generated sample. Conversely, the coverage metric 
determines the proportion of real samples that have at least one generated 
sample in their neighborhood. While both metrics assess diversity, 
the coverage metric is more reliable as it is based on real sample 
neighborhoods. With an expected value of $1-\dfrac{1}{2^k}$
for identical \pr~and~\pg, the interpretation of high diversity for the coverage 
metric depends on the chosen number of neighbors $k$.

\subsubsection{Mean Maximum Similarity (MMS)}\label{sec:metrics-mms}

Originally proposed in~\cite{mms-paper}, the Mean Maximum Similarity (MMS) 
metric evaluates the novelty of generated data. Generative models 
can sometimes produce data almost identical to the training set, 
mimicking its diversity without solving the intended task. MMS 
addresses this by quantifying the novelty of generated samples, which 
we interpret as a measure of diversity in this context. This ensures 
that the generated data is not just varied but also distinct from the training set.

The MMS quantifies novelty/diversity by averaging the distances of each
of the generated samples to its real nearest samples.
It is given by:
\begin{equation}\label{equ:metrics-mms}
    MMS(\textbf{V},\hat{\textbf{V}}) = \dfrac{1}{G}\sum_{j=1}^{G}\sqrt{\sum_{d=1}^{f}(\hat{\textbf{V}}_{j,d} - \textbf{V}_{NN_j,d})^2} 
\end{equation}
    
\noindent where $\textbf{V}_{NN_j}$ (from the real set) is 
the nearest neighbor to $\hat{\textbf{V}}_j$ (from the generated set).
A visual representation of the $MMS$ metric is shown in Figure~\ref{fig:metrics-mms}.
\begin{figure}
    \centering
    \caption{
        This example illustrates the MMS metric computation on a synthetic dataset. 
        The left side shows the latent representation of
        \protect\mycolorbox{0,100,255,0.6}{real samples}
        and \protect\mycolorbox{0,128,0,0.6}{generated samples}. The right side displays the 
        original series space. First, each generated point's 
        nearest neighbor in the real set is identified using the 
        \protect\mycolorbox{255,30,0,0.6}{Euclidean Distance}. 
        Second, the MMS metric is obtained by averaging all these distances.
    }
    \label{fig:metrics-mms}
    \includegraphics[width=\textwidth]{Figures/chapter_7/mms.pdf}
\end{figure}

\paragraph*{Setup for generative models}
First, for each sample in $\hat{\textbf{V}}$, calculate its distance to the 
nearest neighbor in $\textbf{V}$ and average these distances to obtain MMS$_{gen}$.
Second, for each sample in $\textbf{V}$, calculate the distance to the second 
nearest neighbor within $\textbf{V}$ and average these to determine MMS$_{real}$.
We adhere to the method outlined in~\cite{msm-paper} and do not use 
the $\textbf{V}_1$ and $\textbf{V}_2$ sets for the MMS metric calculation.

\paragraph*{Interpretation}
\cite{msm-paper} suggested that MMS$_{gen}$ should always be higher than
MMS$_{real}$ to signify high novelty. However, this metric has limitations, 
particularly when the model generates random samples far from the real set, 
leading to an overestimation of novelty. Thus, relying solely on MMS 
for evaluating a generative model's performance is insufficient. Additionally, 
MMS$_{real}$ is calculated within the entire real sample set, not between 
two subsets, which differs from the approach used for other metrics.

\section{Proposed Metric: Warping Path Diversity (WPD)}\label{sec:metrics-wpd}

\begin{figure}
    \centering
    \caption{
        This figure demonstrates the need for a temporal distortion 
        diversity metric. On the left side, three \protect\mycolorbox{0,100,255,0.6}{real (top)}
        and three \protect\mycolorbox{255,30,0,0.6}{generated (bottom)} 
        human motion sequences performing the "drink-with-left-hand" 
        action are presented. In the middle, the $y$-axis projection of the 
        subject's left-hand motion is displayed
        \protect\mycolorbox{30,117,179,0.6}{for the}
        \protect\mycolorbox{255,125,12,0.6}{three}
        \protect\mycolorbox{43,158,43,0.6}{samples} from both real and generated spaces. 
        The real samples show variability in the starting frame, 
        while the generated samples start consistently. On the right, 
        the latent representation of \protect\mycolorbox{0,100,255,0.6}{real}
        and \protect\mycolorbox{255,30,0,0.6}{generated} samples using a pre-trained 
        GRU classifier reveals that the model does not account for 
        temporal distortion diversity.
    }
    \label{fig:metrics-warping}
    \includegraphics[width=0.9\textwidth]{Figures/chapter_7/do-we-need-warping.pdf}
\end{figure}

Each diversity metric in Section~\ref{sec:metrics-diversity}
relies on a pre-trained encoder $\mathcal{F}$ to extract latent 
features, assuming an input latent space. However, for temporal 
data like human motion sequences, some temporal distortions exist, such as shifting and frequency changes. 
For instance, as presented in Figure~\ref{fig:metrics-warping},
real samples from the HumanAct12 dataset show the 
action of drinking starting at different frames, while generated 
samples lack this variability. The pre-trained encoder $\mathcal{F}$ 
fails to account for these distortions, affecting metrics 
like APD, which measure Euclidean distances in latent space. 
To address this, we propose a new metric that uses Dynamic 
Time Warping (DTW)~\cite{dtw-paper} (see Chapter~\ref{chapitre_1}) to capture and quantify 
temporal distortions between sequences.

\begin{figure}
    \centering
    \caption{
        The distance matrix between \protect\mycolorbox{0,100,255,0.6}{two}
        \protect\mycolorbox{255,30,0,0.6}{time series} is shown in a 
        heat map where each point represents the squared difference 
        between corresponding time stamps. The optimal
        \protect\mycolorbox{0,255,255,0.6}{Dynamic Time Warping 
        (DTW) path}, captures the 
        temporal distortion between the series. The
        \protect\mycolorbox{144,238,144,1.0}{connections}
        between the warping path and the 
        \protect\mycolorbox{0,128,0,0.6}{diagonal} 
        indicate how much the two series deviate from having 
        no temporal distortion.
    }
    \label{fig:metrics-warping-example}
    \includegraphics[width=0.5\textwidth]{Figures/chapter_7/dtw.pdf}
\end{figure}

To simplify, we assume both sequences xx and yy have the same length $L$. 
For the sequences in Figure~\ref{fig:metrics-warping-example},
three scenarios can occur:

\begin{enumerate}
    \item \textbf{Worst-case scenario}: The sequences are poorly aligned, 
    with the DTW path running along the matrix edges, resulting in a 
    path length of $L_{\pi}=2L$.
    \item \textbf{Best-case scenario}: The sequences are perfectly 
    aligned along the diagonal, making DTW equivalent to Euclidean Distance.
    \item \textbf{Intermediate scenario}: Temporal distortions cause the path
    to deviate from the diagonal but remain shorter than the maximum value;
    $L_{\pi}<2L$.
\end{enumerate}

\begin{figure}
    \centering
    \caption{Mathematical basis of the WPD metric:
    For \protect\mycolorbox{255,30,0,0.6}{each point} on 
    the \protect\mycolorbox{0,128,0,0.6}{warping path}, 
    the \protect\mycolorbox{255,100,255,0.6}{corresponding triangle} is always a right 
    isosceles triangle, given that the series are of equal length.
    Hence the \protect\mycolorbox{0,100,255,0.6}{distance} 
    from the \protect\mycolorbox{255,120,0,0.6}{point}
    to the \protect\mycolorbox{220,220,220,1.0}{diagonal} can be easily calculated 
    with the Pythagorean theorem~\cite{pythagorean-theorem-paper}.}
    \label{fig:metrics-dtw-triangle}
    \includegraphics[width=0.6\textwidth]{Figures/chapter_7/dtw_right_isosceles_triangle.pdf}
\end{figure}

To measure the diversity of temporal distortions (warping) between two 
sequences, we propose quantifying the distance of the warping path 
from the diagonal. This involves summing the distances from each point 
on the warping path to the diagonal, as illustrated in 
Figure~\ref{fig:metrics-dtw-triangle}. Each point on the path is 
considered within an integer coordinate space with axes ranging 
from $1$ to $L$. For equal-length sequences, the triangle at each 
warping path point is a right isosceles triangle, making the hypotenuse's 
median half its length.

\begin{theorem}[Warping Path Diversity's Distance To Diaginal Computation]\label{the:metrics-distance-to-diagonal}
    Given two sequences $\textbf{x}$ and $\textbf{y}$ both of length $L$,
    with warping path $\pi$ of length $L~\leq~L_{\pi}~\leq~2L$, the distance 
    from the $t_{th}$ point of the path $\pi$,
    where $t~\in~\{1,2,\ldots,L_{\pi}\}$,
    to the diagonal (perfect alignment) is defined as follows:
    \begin{equation*}
        distance(\pi_t,diagonal) = \dfrac{\sqrt{2}}{2}|t_1-t_2|
    \end{equation*}
\end{theorem}

\textit{proof}: 
Using the annotations of $a,~b,~c$ and $d$ in
Figure~\ref{fig:metrics-dtw-triangle} as the summit of points
$\pi_t$ on the warping path $\pi$, then $d=~distance(\pi_t,diagonal)$ is
calculated as follows:
\begin{equation}\label{equ:metrics-proof-wpd}
    \begin{split}
        d &= \dfrac{1}{2}\sqrt{c^2} = \dfrac{1}{2}\sqrt{a^2+b^2} = \dfrac{1}{2}\sqrt{2 * a^2}\\
        &= \dfrac{1}{2}\sqrt{2*(t_1-t_2)^2} = \dfrac{\sqrt{2}}{2} |t_1-t_2|
    \end{split}
\end{equation}

The WPD value between $\textbf{x}$ and $\textbf{y}$ is the average distance 
of all points on the warping path to the diagonal, as follows:
\begin{equation}\label{equ:wpd-d}
    WPD_d(\textbf{x},\textbf{y}) = \dfrac{\sqrt{2}}{2.L_{\pi}}\sum_{t=1}^{L_{\pi}}|t_1-t_2|
\end{equation}

Finally, the WPD metric of a generative model is calculated, 
like for the APD metric, between random subsets of samples 
from both real and generated samples:
\begin{equation}\label{equ:wpd-r}
WPD_r(\mathcal{S},\mathcal{S}^{'}) = \dfrac{1}{S_{wpd}}\sum_{i=1}^{S_{wpd}}WPD_d(\mathcal{S}_i,\mathcal{S}^{'}_i)
\end{equation}

\noindent where $\mathcal{S}$ and $\mathcal{S}^{'}$ are two randomly 
selected subsets, of size $S_{wpd}$, from
$\hat{\textbf{V}} = \mathcal{F}(\hat{\mathcal{X}})$, 
i.e., $\mathcal{S},\mathcal{S}^{'}~\subset~\hat{\textbf{V}}$ and 
$r~\in~\{1,2,\ldots,R\}$ is the number of repetitions of this 
random experiment to avoid the bias of a random selection.
The final $WPD$ metric is calculated as:
\begin{equation}\label{equ:wpd}
WPD(\hat{\mathcal{X}}) = \dfrac{1}{R}\sum_{r=1}^{R}WPD_r(\mathcal{S}^r,\mathcal{S}^{'r}) 
\end{equation}

\noindent with $WPD$ bounded between $0$ and 
$\dfrac{\sqrt{2}}{4}(L+1)$.

The same methodology is follows to calculate WPD on the real set of samples 
$\mathcal{X}$.
The characteristics of our proposed WPD metric 
are summarized in Table~\ref{tab:metrics-summmary}
along with all the other metrics presented in this study.

\begin{table}
    \centering
    \caption{Summary of the Generative Models Metrics in this study.}
    \label{tab:metrics-summmary}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
      \hline
      Metric & Category & Space & Hyperparameters & Bounds & Interpretation & Better Version & Used in Study \\
      \hline
      $FID$ & fidelity & latent & None & 
      \begin{tabular}{c}
        $0\leq FID < \infty$
      \end{tabular} & 
      \begin{tabular}{c}
        Higher but close \\ to $FID_{real}$
      \end{tabular} & None & yes \\
      \hline
      $AOG$ & fidelity/accuracy & latent & None & 
      \begin{tabular}{c}
        $0\leq AOG \leq 1$
      \end{tabular} & 
      \begin{tabular}{c}
        Close to 1 \\ (100\% accuracy)
      \end{tabular} & None & yes \\
      \hline
      $density$ & fidelity & latent & 
      \begin{tabular}{c}
        $k$: number \\ of neighbors
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq density \leq N/k$ \\ $N$ being the number \\ of real samples
      \end{tabular} & 
      \begin{tabular}{c}
        closer to $density_{real}$ \\ which is close to 1
      \end{tabular} & None & yes \\
      \hline
      $precision$ & fidelity & latent & 
      \begin{tabular}{c}
        $k$: number \\ of neighbors
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq precision \leq 1$
      \end{tabular} & 
      \begin{tabular}{c}
        closer to 1
      \end{tabular} & $density$ & no \\
      \hline
      $coverage$ & diversity & latent & 
      \begin{tabular}{c}
        $k$: number \\ of neighbors
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq coverage \leq 1$
      \end{tabular} & 
      \begin{tabular}{c}
        closer to $coverage_{real}$ \\ which is close to $1-1/2^k$
      \end{tabular} & None & yes \\
      \hline
      $recall$ & diversity & latent & 
      \begin{tabular}{c}
        $k$: number \\ of neighbors
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq recall \leq 1$
      \end{tabular} & 
      \begin{tabular}{c}
        closer to 1
      \end{tabular} & $coverage$ & no \\
      \hline
      $APD$ & diversity & latent & 
      \begin{tabular}{c}
        $S_{apd}$: size of \\ random subset \\ $R$: number of \\ random experiments
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq APD < \infty$
      \end{tabular} & 
      \begin{tabular}{c}
        Lower but close \\ to $APD_{real}$
      \end{tabular} & None & yes \\
      \hline
      $ACPD$ & diversity & latent & 
      \begin{tabular}{c}
        $S_{acpd}$: size of \\ random subset \\ $R$: number of \\ random experiments
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq ACPD < \infty$
      \end{tabular} & 
      \begin{tabular}{c}
        Lower but close \\ to $ACPD_{real}$
      \end{tabular} & None & yes \\
      \hline
      $MMS$ & diversity/novelty & latent & None & 
      \begin{tabular}{c}
        $0\leq MMS < \infty$
      \end{tabular} & 
      \begin{tabular}{c}
        Higher but close \\ to $MMS_{real}$
      \end{tabular} & None & yes \\
      \hline
      $WPD$ (\textbf{ours}) & diversity/warping & raw & 
      \begin{tabular}{c}
        $S_{wpd}$: size of \\ random subset \\ $R$: number of \\ random experiments
      \end{tabular} & 
      \begin{tabular}{c}
        $0\leq WPD \leq \dfrac{\sqrt{2}}{4}(L+1)$ \\ $L$ being the length \\ of the sequence
      \end{tabular} & 
      \begin{tabular}{c}
        Depends on the \\ application
      \end{tabular} & None & yes \\
      \hline
    \end{tabular}}
\end{table}

\section{Experimental Setup}

\begin{figure}
    \centering
    \caption{
        In our experiments, the CVAE undergoes two phases: training 
        and generation.\textbf{ During training}, the \protect\mycolorbox{0,194,0,0.47}{Encoder} 
        and \protect\mycolorbox{0,194,171,0.47}{Decoder} 
        are trained simultaneously. The \protect\mycolorbox{0,194,0,0.47}{Encoder}
        extracts features 
        from input sequences and projects them into a Gaussian latent 
        space with a learned \protect\mycolorbox{234,194,139,0.47}{mean} 
        and \protect\mycolorbox{234,112,139,0.47}{variance}, conditioned 
        on the \protect\mycolorbox{255,30,1309,0.6}{action label}. The
        \protect\mycolorbox{0,194,171,0.47}{Decoder}  then 
        reconstructs the input sequence from a
        \protect\mycolorbox{0,100,255,0.6}{sample in this space}. 
        \textbf{In the generation phase}, a \protect\mycolorbox{160,100,255,0.6}{random
        sample from a Normal 
        distribution} is fed to the \protect\mycolorbox{0,194,171,0.47}{Decoder}
        to generate a new sequence, 
        also conditioned on the desired \protect\mycolorbox{255,30,1309,0.6}{action label}.
    }
    \label{fig:metrics-cvae}
    \includegraphics[width=\textwidth]{Figures/chapter_7/cvae.pdf}
\end{figure}

To analyze the behavior of each metric during evaluation, we 
conduct an experiment using Conditional Variational Auto-Encoders 
(CVAE) to generate human motion sequences. The conditioning feature of 
the CVAE allows precise control over the generated actions by specifying 
the action class. Figure~\ref{fig:metrics-cvae} illustrates
the training and generating 
phases of a CVAE with human motion sequences, providing a visual 
representation of our experimental setup. This approach enables a 
comprehensive assessment of how different metrics respond to the 
generated data.

\subsection{Backbone Architectures}

The CVAE model employs an encoder-decoder architecture using three 
well-known neural network backbones: Convolutional Neural Networks 
(CNNs), Recurrent Neural Networks (RNNs), and Transformer Networks. 
The CNN-based CVAE (CConvVAE) uses symmetrical convolution and 
de-convolution blocks. The RNN-based CVAE (CGRUVAE) uses stacked 
Gated Recurrent Units (GRUs) for both encoding and decoding, 
repeating the input for sequence generation. The Transformer-based 
CVAE (CTransVAE) features self-attention mechanisms in both encoder 
and decoder, with matching layers and parameters. Each architecture 
maintains symmetry between its encoder and decoder components.

\subsection{Implementation Details}

The three CVAE variants in this work are implemented using 
\textit{tensorflow}~\cite{tensorflow-paper} Python package
and trained for $2,000$ epochs with a batch size of $32$, 
utilizing a learning rate decay method. The CConvVAE employs 
three convolution and three de-convolution blocks with $128$
filters and kernel sizes of $40$, $20$, and $10$. 
The CGRUVAE has two GRU layers in both the encoder and decoder, 
with a hidden state size of $128$. The CTransVAE uses convolution 
embedding followed by two Multi-Head Attention layers in both the 
encoder and decoder, with $128$ filters and a head size of $32$. 
All three variants have a latent space dimension of $16$.
To train the generative models, we utilize a publicly available 
action recognition dataset, HumanAct12~\cite{action2motion-paper}.
Prior to
training, we normalize all sequences in the dataset using a 
$\min-\max$ scalar on each of the $x~-~y-~z$ dimensions independently.
It is important to note that for all the metrics used in this
work, no prior train-test splits are required, instead all the dataset can
be used.

\subsection{Training on Different Loss Parameters}
In this experimental work, we assess how slight changes in a model's parameters may affect
the interpretation of 
evaluation metrics by experimenting with the model's loss parameters. 
We train a CVAE model to optimize a weighted sum of two losses: 
reconstruction loss and Kullback-Leibler (KL) divergence loss.
The total loss is defined as:
\begin{equation}\label{equ:metrics-total-loss}
    \mathcal{L} = \alpha.\mathcal{L}_{rec} + \beta.\mathcal{L}_{KL}
\end{equation}
    
\noindent where $\alpha$ and $\beta$ are scalar weights between 
$0$ and $1$ for each of the reconstruction and KL loss respectively.
In the ideal case, it is preferable to maintain the following constraint:
\begin{equation}\label{equ:metrics-alpha-beta}
    \alpha + \beta = 1 
\end{equation}
As seen in chapter~\ref{chapitre_1}, $\alpha$ is set to $1-\beta$ to preserve convexity.
Instead of selecting a specific $(\alpha,\beta)$ pair, we experiment 
with various values: $(1E^{-1},9E^{-1})$, $(5E^{-1},5E^{-1})$, 
$(9E^{-1},1E^{-1})$, $(9.9E^{-1},1E^{-2})$, $(9.99E^{-1},1E^{-3})$, 
$(9.999E^{-1},1E^{-4})$. This approach highlights how different weightings 
influence model performance and metric interpretation.

\subsection{Class Imbalanced Generation Setup}

To unify the evaluation method and ensure fairness,
we propose a generation setup that addresses the class 
imbalance problem in training datasets, relevant to any supervised 
generative model, including those used for human action recognition. 
Given that all metrics compare real and generated distributions, 
it's crucial to ensure fair evaluation. To do this, we match the 
label distribution of generated samples with that of the training 
set, preventing the model from over-representing majority classes. 
When generating more samples than available in the training set, a
proportional factor is applied to maintain the original label distribution, 
ensuring balanced and unbiased sample generation.

\section{Results and Analysis}

\begin{figure}
  \centering
  \caption{
    Radar charts compare the performance of three CVAE variants 
    across eight metrics. Each chart, labeled from \textbf{a} to
    \textbf{d}, represents 
    a different $(\alpha,\beta)$ parameter set. The charts 
    feature four polygons: one for \protect\mycolorbox{255,102,138,0.25}{each}
    \protect\mycolorbox{230,171,2,0.25}{CVAE}
    \protect\mycolorbox{117,112,179,0.25}{variant} and one for 
    \protect\mycolorbox{27,158,119,0.25}{real data metrics}.
    For all metrics except FID, a higher summit 
    indicates better performance. For FID, a higher summit means 
    worse performance.
  }
  \label{fig:metrics-radar}
  \includegraphics[width=\textwidth]{Figures/chapter_7/radar-plots-cvae.pdf}
\end{figure}

To evaluate our models, we use radar charts due to the varying ranges 
of our metrics, which offers a clearer comparison than simply stating 
numerical differences. For each $(\alpha,\beta)$ pair, a radar chart displays 
four polygons (see Figure~\ref{fig:metrics-radar}):
one for each CVAE variant and one for real samples. 
Metrics are normalized between 0 and 1 and transformed for comparison, 
with a summit lower than the real polygon indicating 
$metric_{gen} < metric_{real}$, except for FID where it 
indicates $FID_{gen} > FID_{real}$.
Optimal performance is shown by generative model 
polygons closely matching the real polygon, except 
for the MMS metric where higher generative values are better.

Figure~\ref{fig:metrics-radar} illustrates the difficulty 
in finding a generative model that excels across all metrics simultaneously. 
Changes in the backbone architecture or loss parameters can significantly 
impact metric values. Our experiments with the HumanAct12 dataset 
reveal that selecting the best model, CConvVAE, is only feasible 
for a specific set of $(\alpha,\beta)$ values, as shown in
Figure~\ref{fig:metrics-radar}-d. However, this parameter search 
is not always practical, making it challenging to identify 
the best model across all metrics. Therefore, depending on 
the application, we may need to prioritize specific metrics
or even a single metric.

We now present an analysis of the results by comparing three 
CVAE models on each metric individually. For each metric, we 
explain what it means for a model to perform best.

\begin{itemize}
  \item \textbf{FID}: In certain $(\alpha,\beta)$ configurations,
  Figure~\ref{fig:metrics-radar} demonstrates that CConvVAE 
  achieves the lowest FID value, closely approaching FID$_{real}$.
  This suggests that CConvVAE generates samples with superior 
  fidelity compared to CGRUVAE and CTransVAE. The model efficiently 
  learns a distribution \pg~that is easier to align with \pr~than 
  the distributions produced by the other models. However, 
  despite having the smallest and most accurate FID, there are 
  cases where the gap between CConvVAE and FID$_{real}$ is still 
  considerable (Figures~\ref{fig:metrics-radar}-a-b-c).
  
  \item \textbf{Density}: Just as with the FID metric, for 
  certain $(\alpha,\beta)$ pairs, CConvVAE produces a Density 
  value that is closer to Density$_{real}$ than the other CVAE 
  variants. This suggests that CConvVAE is more likely to 
  generate samples resembling \pr~than CGRUVAE and CTransVAE. 
  However, even though CConvVAE often comes closest to 
  matching Density$_{real}$, a notable gap between Density$_{gen}$ 
  and Density$_{real}$ can still be observed in some instances.

  \item \textbf{AOG}: The AOG metric is crucial for evaluating a 
  generative model's conditional effectiveness. Factors like 
  data scarcity, underfitting, or poor hyperparameters can 
  impact performance. Figure~\ref{fig:metrics-radar} shows CConvVAE often 
  leads in this metric, but not always successfully; 
  for instance, in Figure~\ref{fig:metrics-radar}-f, CConvVAE's
  AOG value diverges 
  significantly from AOG$_{real}$. Meanwhile, CTransVAE and 
  CGRUVAE consistently fail to manage sub-classes across 
  all settings, indicating persistent issues with their 
  conditional mechanisms.

  \item \textbf{APD}: Regarding the APD diversity metric, 
  CConvVAE stands out by consistently achieving values near 
  APD$_{real}$ across various hyperparameter settings, 
  particularly excelling in Figure~\ref{fig:metrics-radar}-d. 
  However, CGRUVAE also shows strong performance on the APD 
  metric in some configurations. This demonstrates that while 
  CConvVAE may significantly outperform CGRUVAE in one aspect, 
  such as FID, CGRUVAE can still excel in other metrics, 
  highlighting its overall competence.

  \item \textbf{ACPD}: The ACPD metric evaluates diversity per 
  sub-class, while the APD metric assesses overall diversity. 
  Figure~\ref{fig:metrics-radar} shows that nearly half the 
  hyperparameter settings have all three CVAE variants outperforming 
  the real data on ACPD, indicating greater sub-class diversity. 
  However, this can result from overfitting, instability, and 
  class imbalance. Excelling in one metric doesn't imply overall 
  superiority. For example, CGRUVAE outperforms CConvVAE in ACPD 
  in Figure~\ref{fig:metrics-radar}-a but has a much higher FID, 
  indicating unreliable results. This highlights the need for 
  multiple evaluation metrics.
  
  \item \textbf{Coverage}: For most hyperparameter settings, 
  CConvVAE surpasses other CVAE variants in terms of coverage, 
  indicating it generates a greater number of samples that align 
  with the real distribution \pr~compared to CGRUVAE and 
  CTransVAE. However, in Figures~\ref{fig:metrics-radar}-a-b, CTransVAE 
  outperforms CConvVAE in coverage, showing that despite 
  CConvVAE's strong APD diversity, it can perform poorly 
  in coverage under certain conditions.

  This raises the question: What is the difference between APD and 
  coverage if they both quantify diversity? Both metrics use a latent space 
  and Euclidean Distance, however APD measures the distance between 
  randomly selected pairs to measure the volume of space occupied 
  by \pr~and~\pg, independently. Conversely, coverage evaluates the nearest 
  neighbor relationships between real and generated samples to 
  quantify how much of~\pr's space is occupied by the generated samples.

  \item \textbf{MMS}: The MMS metric measures diversity by 
  evaluating the novelty of generated samples, ideally with 
  MMS$_{gen}$ values higher but close to MMS$_{real}$. This metric 
  can be more challenging to interpret. CConvVAE demonstrates 
  the most stable MMS values among the three variants, maintaining 
  a higher yet comparable level to MMS$_{real}$. In contrast, 
  CTransVAE's MMS values exceed the radar plot's limits. 
  This indicates that CConvVAE excels at producing novel human 
  motion sequences.

  \item \textbf{WPD}: The WPD metric assesses temporal diversity 
  by measuring warping and distortions between samples. 
  There are three interpretations for WPD:
  \begin{enumerate}
    \item $|WPD_{real} - WPD_{gen}| < \epsilon$ (where $\epsilon$
    is very small): This signifies a perfect replication 
    of all temporal distortions from \pr~to~\pg.
    \item $WPD_{gen} >>> WPD_{real}$: This indicates that the 
    generative model has identified and created similar but new 
    temporal distortions.
    \item $WPD_{gen} <<< WPD_{real}$: This implies the model fails 
    to replicate temporal distortions, generating consistent but 
    limited distortions.
  \end{enumerate}
  Figure~\ref{fig:metrics-radar} shows that, apart from CTransVAE 
  in Figure~\ref{fig:metrics-radar}-d, all WPD values are 
  less than WPD$_{real}$. This means most models can re-create some 
  temporal distortions, though not all. Notably, in 
  Figure~\ref{fig:metrics-radar}-d, the minimal gap between 
  WPD$_{real}$ and WPD for CConvVAE and CGRUVAE suggests 
  a near perfect replication of temporal distortions in \pr.
\end{itemize}

\section{Conclusion}

In this chapter, we provided a comprehensive review of evaluation metrics 
used to assess the reliability of generative models for human 
motion generation. Recognizing that human motion data are 
temporal and represented as multivariate time series, we 
introduced a novel metric to evaluate diversity in terms 
of temporal distortion. We proposed a unified evaluation 
framework, with eight metrics measuring fidelity and diversity,
that allows for fair comparisons between different 
models. Our experiments with three generative model variants 
on a publicly available dataset demonstrated that no single 
metric can universally determine model superiority. Instead, 
a combination of different metrics is often necessary to 
accurately evaluate model reliability.

Our findings indicate that the CConvVAE model outperforms others 
on the highest number of metrics, which can give the impression 
of being the best overall model. However, minor hyper-parameter adjustments 
can significantly impact its performance across various metrics. 
This underscores the difficulty of identifying the \textit{best model},
the challenge of finding \textit{The One Metric To Rule Them All},
and highlights the importance of tailoring model selection to 
specific applications. For instance, in gaming, where generating 
diverse actions is crucial, diversity metrics are more important 
than fidelity metrics like FID. Conversely, in medical research, 
where precise replication of movements is critical, fidelity takes 
precedence.

Additionally, we offer publicly available, user-friendly code for calculating 
all the metrics we used, applicable to any generative model with any parameterization. 
We hope this work serves as a valuable starting point for newcomers to the field 
of human motion generation and helps establish a clear framework for unified evaluation. 
However, we acknowledge that the metrics discussed are not exhaustive, as the field is 
rapidly evolving. It is also important to adapt metrics when labels in the real data 
are unavailable.
