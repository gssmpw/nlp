%----------------------------------------------------------------------------------------
%
% Exploiter des niveaux de grossissement des images de lames
%
%----------------------------------------------------------------------------------------
\chapter{Benchmarking Machine Learning Models on Time Series Data}
\label{chapitre_2}


\section{Introduction}~\label{sec:chap-mcm-intro}

Benchmarking machine learning models is a critical practice in the field 
of computer science and machine learning. This process involves comparing 
the performance of various algorithms across multiple datasets to determine 
their relative effectiveness and identify the state-of-the-art methods. 
Effective benchmarking is essential for understanding the strengths and 
weaknesses of different models, guiding future research, and improving 
algorithm design.
In this chapter, we will explore the methodologies for benchmarking 
machine learning models on time series data, with a focus on 
the classification task.

A crucial aspect of benchmarking involves hypothesis testing 
and the use of post-hoc tests for p-values and Null Hypothesis 
Significance Testing (NHST). Traditional methods like the Wilcoxon 
signed-rank test~\cite{wilcoxon-paper} and the Nemenyi test~\cite{nemenyi-paper}
are commonly used but have 
significant limitations~\cite{pvalues-pitfal-paper},
such as being prone to manipulation and 
providing limited insight into the true differences between model.
Recent studies advocate for Bayesian methods~\cite{bayseian-benavoli-paper}
as more reliable alternatives for multiple comparisons.

We explore in this chapter the current method used for benchmarking, 
such as the Critical Difference Diagram (CDD) introduced by~\cite{demsar-cdd-paper}, 
and discuss its evolution~\cite{demsar-extension-paper,cdd-benavoli-paper}
and limitations. These traditional methods often suffer 
from issues such as instability in ranking and susceptibility to 
manipulation. To address these concerns, we will introduce the Multiple 
Comparison Matrix (MCM), a novel approach designed to provide more robust 
and interpretable comparisons.

\section{Background and Current Benchmarking Methods}\label{sec:background-benchmark}

Following our exploration of the limitations inherent in traditional benchmarking methods, 
this section explores the current approaches used to evaluate classifiers.
We address the challenge of summarizing the evaluation outcomes of $m$ 
comparates $\mathcal{C} = \{c_1, \ldots, c_m\}$ across multiple datasets 
$\mathcal{D} = \{d_1, \ldots, d_n\}$ using a single performance measure 
$\gamma: \mathcal{C} \times \mathcal{D} \rightarrow \mathbb{R}$, that 
assesses the performance of a comparate $c \in \mathcal{C}$ on a task 
$d \in \mathcal{D}$. Each task (dataset) involves training a classifier $\lambda$ on a 
time series training set, with performance measured by the accuracy 
of $\lambda$ on a corresponding time series test set. 
In this context, the comparates $\mathcal{C}$ are the time series 
classifiers, the tasks $\mathcal{T}$ are the classification problems 
derived from 128 datasets in the UCR archive~\citep{ucr-archive}, and 
the performance measure $\gamma$ is the classification accuracy.

\subsection{Ranking Comparates}\label{sec:average-rank}

A common approach to summarize such evaluation is through ranking each of the comparate independently for each dataset
for which we can produce an average rank per comparate overall datasets.
The Critical Difference Diagram (CDD)~\citep{demsar-cdd-paper} is currently the primary method 
used for multi-comparate and multi-dataset benchmarking. This diagram provides 
two types of comparisons: (1) a group-wise comparison using the mean rank 
of each comparate, and (2) a pairwise comparison indicating which pairs 
of comparates show significant performance differences. An example is 
shown in Figure~\ref{fig:cdd-example} where each comparate is assigned an average rank overall datasets
and if a clique (straight line) exists amongst a set of comparates it highlights that no conclusion can be found 
on the statistical significance in difference of performance between these comparates.

\begin{figure}
    \centering
    \caption{Critical Difference Diagram between five state-of-the-art models of 
    TSC evaluated on 128 datasets 
    of the UCR archive highlighting the \protect\mycolorbox{0,0,255,0.7}{\textcolor{white}{average rank}}
    of each model over all datasets. If a \protect\mycolorbox{255,0,0,0.7}{\textcolor{white}{clique}} 
    is formed between two models it means that no conclusion can be made 
    on the statistical significance in difference of performance between this pair of 
    models.}
    \label{fig:cdd-example}
    \includegraphics[width=\textwidth]{Figures/chapter_2/cdd/cd-diagram.pdf}
\end{figure}

The mean rank is computed as follows. Each comparate $c_i$ is assigned a 
rank $R_{c_i}^{d_k}$ on each task $d_k$ with $k~\in~[1,n]$ based on its relative performance score 
$\gamma$:
\begin{equation}
    R_{c_i}^{d_k} = 1 + \left|\{c_j \in \mathcal{C} \setminus c_i : \gamma(c_j, d_k) \succ \gamma(c_i, d_k)\}\right| + \tfrac{1}{2} \cdot \left|\{c_j \in \mathcal{C} \setminus c_i : \gamma(c_i, d_k) = \gamma(c_j, d_k)\}\right|,
\end{equation}

\noindent
where $\succ$ means \emph{better than}. For example, if $\gamma$ represents 
the accuracy on the test set, then a higher accuracy means a \emph{better}
model, however if $\gamma$ represents the error rate, then a lower error 
rate means a \emph{better} model.
Each comparate is then assigned an 
Average Rank (AR) by averaging its ranks over all $n$ datasets in 
$\mathcal{D}$,
\begin{equation}
    \mathrm{AR}^{\mathcal{D}}_{c_i} = \frac{\sum_{k=1}^n R_{c_i}^{d_k}}{n}.
\end{equation}

% The lower the AR, the better the assessment of the summarized performance 
% relative to the set of competitor comparates.
The lower the AR value, the better the performance assessment relative to competing comparators.
The placement of the comparates in the diagram in Figure~\ref{fig:cdd-example}
follows their AR.


\subsection{Pairwise Comparisons with the CDD}

Originally, the CDD proposed 
in~\cite{demsar-cdd-paper} also emphasizes the significance of 
performance differences between each pair of comparates. 
This significance test is crucial for determining whether it is 
necessary to adopt the \emph{better} model with the lowest AR (AR$_1$).
However, there may be another model with an AR 
(AR$_2$) very close to AR$_1$ (i.e., AR$_2 = $ AR$_1 + \epsilon$, 
where $0 < \epsilon << 1$) that is significantly faster and less 
complex than the \emph{winning} model (see example between InceptionTime and ROCKET in Figure~\ref{fig:cdd-example}).

The method to assess this statistical significance in difference of performance 
has been changed throughout the years.
For instance, the original CDD utilized the Nemenyi test~\cite{nemenyi-paper},
based on the actual values of the AR.
However,~\cite{cdd-benavoli-paper} argued that using a post-hoc test following
the AR values may be miss-leading and proposed the usage of the Wilcoxon Signed 
Rank Test for pairwise significance comparison and the Friedman test~\cite{friedman-paper}
for group-wise significance comparison.

\subsubsection{Friedman Test}\label{sec:friedman}

The Friedman test~\cite{friedman-paper} is a non-parametric statistical test used to detect differences in 
treatments across multiple test attempts. It is particularly useful for comparing multiple 
algorithms over multiple datasets. Given $m$ algorithms and $n$ datasets, this test ranks the performance of algorithms for each 
dataset and then analyzes these ranks to determine if there are statistically significant 
differences between the algorithms.

% Given $m$ algorithms and $n$ datasets, the Friedman test ranks the algorithms for each dataset. 
Let $R_{c_i}^{d_k}$ be the rank of the $i_{th}$ comparate $c_i$ on the $k_{th}$ dataset $d_k$.
The test's objective is to determine if at least one of the $m$ comparates performs 
significantly better than all other comparates.
This is determined by following these steps below:

\begin{enumerate}
    \item Compute the rank sums for each comparate:
    \begin{equation}\label{equ:friedman-step1}
        R_{c_i} = \sum_{k=1}^{n}R_{c_i}^{d_k}
    \end{equation}
    \item Calculate the Friedman test statistic:
    \begin{equation}\label{equ:friedman-step2}
        \chi_{F}^2 = \dfrac{12}{n.m.(m+1)}\sum_{i=1}^{m}R_{c_i}^2~ - 3n.(m+1)
    \end{equation}
    \item Determine the p-value:
    The test statistic $\chi_{F}^2$ approximately follows a chi-squared distribution with 
    $m.(m+1)$ degrees of freedom. The p-value is computed based on this distribution. 
    A low p-value (typically less than $\alpha=0.05$) indicates that at least one of the algorithms 
    performs significantly differently from the others.
\end{enumerate}

While the Friedman test is useful for identifying differences between multiple algorithms,
it has several limitations:

\begin{itemize}
    \item \textbf{Magnitude Ignored}: The test only considers the ranks of the algorithms,
    not the magnitude of the differences in performance. As a result, small and large
    differences are treated equally.
    \item \textbf{Instability}: The results of the Friedman test can be sensitive to the set of 
    comparates included in the analysis. Adding or removing an algorithm can change the 
    conclusions about the relative performance of the remaining algorithms.
    \item \textbf{Post-hoc Analysis Needed}: To determine which specific algorithms differ from each
    other, a post-hoc test (such as the Nemenyi test) is required, adding complexity to 
    the analysis.
\end{itemize}

\subsubsection{Nemenyi Test}\label{sec:nemenyi}

The Nemenyi test~\cite{nemenyi-paper} is a post-hoc statistical test used to determine whether 
the performance differences between pairs of algorithms are statistically 
significant. The Nemenyi test is typically applied after conducting a Friedman 
test, which assesses whether there are any overall differences among 
multiple algorithms across multiple datasets.
The Nemenyi test compares the mean 
ranks of all pairs of algorithms and determines if the differences 
in their ranks exceed a critical value, which would indicate a 
statistically significant difference in performance.
Mathematically, the Critical Difference (CD) for the Nemenyi 
test is calculated as follows:

\begin{equation}
    CD = q_{\alpha} \sqrt{\frac{m(m+1)}{6n}}
\end{equation}
\noindent where $q_{\alpha}$ is the critical value from the Studentized range distribution, 
which depends on the desired significance level $\alpha$, usually set to 
$0.05$ and the number of comparates $m$.
% \begin{itemize}
%     \item $m$ and $n$ are the number of comparates and number of datasets
%     respectively
%     \item $q_{\alpha}$ is the critical value from the Studentized range distribution, 
%     which depends on the desired significance level $\alpha$, usually set to 
%     $0.05$ and and the number of comparates $m$
% \end{itemize}

Two algorithms $c_i$ and $c_j$ are considered to have a statistically 
significant difference in performance if the absolute difference 
in their ARs exceeds the critical difference:

\begin{equation}
|AR_{c_i}^{\mathcal{D}} - AR_{c_j}^{\mathcal{D}}| > CD
\end{equation}
\noindent where $AR_{c_i}^{\mathcal{D}}$ and $AR_{c_j}^{\mathcal{D}}$
are the ARs of both comparates $c_i$ and $c_j$ respectively on all datasets $\mathcal{D}$.
If this condition is met, it can be concluded that the performance 
of the two algorithms differs significantly at the given significance level 
$\alpha$. However, if the condition is not met, it does not imply 
that the two comparates are equivalent in performance. Rather, it 
suggests that, given the set of $n$ datasets $\mathcal{D}$, there is insufficient 
evidence to conclude a statistically significant difference in 
performance between the two comparates.

However, this test has several limitations~\cite{cdd-benavoli-paper}:
\begin{itemize}
    \item \textbf{Rank-Only Consideration}: It only considers the ranks of the comparates, 
    focusing solely on the number of tasks where one comparate performs better or worse than others.
    \item \textbf{Ignoring Magnitude Differences}: The test does not take into account the magnitude of performance 
    differences, treating small and large differences equally.
    \item \textbf{Results Instability}: The test's results are unstable with respect to 
    the set of comparates included in the evaluation.
    \item \textbf{Sensitivity to Comparate Changes}: The inclusion or exclusion of a single comparate can significantly 
    alter the pairwise conclusions drawn for the remaining comparates.
\end{itemize}
% It only considers the ranks of the comparates, focusing solely on 
% the number of tasks where one comparate performs better or worse 
% than others, without taking into account the magnitude of these 
% differences. This means it treats small performance differences 
% the same as large ones. Additionally, the test's results are unstable 
% with respect to the set of comparates included in the evaluation. 
% Finally, the inclusion or exclusion of a single comparate can alter the 
% pairwise conclusions drawn for the remaining comparates.

\subsubsection{Wilcoxon Signed-Rank Test}\label{sec:wilcoxon}

The Wilcoxon signed-rank test is a non-parametric statistical test used to compare two related samples, 
matched samples, or repeated measurements on a single sample. It is the non-parametric alternative to the 
Nemenyi test (Section~\ref{sec:nemenyi}), where~\cite{cdd-benavoli-paper} questions the following:
\emph{``Should we really use post-hoc tests based on mean-ranks?''}. This test was proposed in~\cite{wilcoxon-paper} 
as a means to test for differences in the median values of two related groups without assuming that the
differences follow a normal distribution.

Given $n$ datasets, the Wilcoxon signed-rank test can be used to compare the performance
of two specific comparates, $c_i$ and $c_j$ evaluated across these $n$ datasets. The steps are as follows:

\begin{enumerate}
    \item Compute the differences between the performance of the two comparates for each dataset:
    \begin{equation}\label{equ:wilcoxon-step1}
        D_k = \gamma(c_i,d_k) - \gamma(c_j,d_k) \hspace{1cm} for~k~\in~[1,n]
    \end{equation}
    \noindent where $\gamma(c_i,d_k)$ and $\gamma(c_j,d_k)$ are the performance of both 
    comparates $c_i$ and $c_j$ evaluated on the test set of 
    the $k_{th}$ dataset $d_k~\in~\mathcal{D}$.
    \item Rank the absolute values of the differences, ignoring the signs, and assign ranks $RD_k$.
    If there are ties, assign the average rank to both values.
    \item Restore the signs to the ranks, resulting in signed ranks $sRD_i$
    \item Calculate the test statistic $W^{+}$, which is the sum of the positive ranks:
    \begin{equation}\label{equ:wilcoxon-step2}
        W^{+} = \sum_{k:~sRD_k > 0}~|sRD_k|
    \end{equation}
    \item Calculate the test statistic $W^{-}$, which is the sum of the negative ranks:
    \begin{equation}\label{equ:wilcoxon-step3}
        W^{-} = \sum_{k:~sRD_k < 0}~sRD_k
    \end{equation}
    \item Use the smaller of the two sums, $W=\min(W^{+},W^{-})$ as the test statistic
    \item Determine the p-value:
    The p-value is calculated based on the distribution of $W$. For large sample sizes (typically $n > 30$),
    the distribution of $W$ approaches a normal distribution, and a z-score can be used. For smaller samples,
    exact tables or software can be used to find the p-value.
    If this p-value is lower than a threshold $\alpha$ (usually set to $0.05$), then the difference of performance between 
    both comparates $c_i$ and $c_j$ on the $n$ datasets is statistically significant.
    However if the p-value is higher than $\alpha$, then the $n$ datasets are not enough to find a conclusion
    on the statistical significance in difference of performance between $c_i$ and $c_j$.
\end{enumerate}

While the Wilcoxon signed-rank test is robust and widely applicable, it has several limitations:
\begin{itemize}
    \item \textbf{Sensitivity to Outliers}: Like many non-parametric tests, the Wilcoxon signed-rank test can be sensitive
    to outliers, which can disproportionately influence the results.
    \item \textbf{Dependent Observations}: The test assumes that the pairs of observations are independent of each other.
    If there is dependence, the results may not be valid.
    Such dependency exists when two comparates are the same approach but one being a weaker version than the other.
\end{itemize}

\paragraph*{Holm Correction for Multiple Pairwise Comparison}
When we need to find the p-values for multiple paired comparates among $m$ comparates, the Wilcoxon
signed-rank test is often used in conjunction with the Holm correction~\cite{holm-correction-paper}.
The Holm correction is a method for controlling the family-wise error rate when performing multiple comparisons.
It adjusts the significance levels for each hypothesis test to account for the multiple comparisons being made.

Given $m$ comparates and the significance threshold $\alpha$, we have $\hat{m}=\dfrac{m.(m-1)}{2}$ p-values between all possible pairs of comparates:
$p_1,p_2,\ldots,p_{\hat{m}}$.
The Holm correction is applied as follows:
\begin{enumerate}
    \item Order the p-values from smallest to largest
    \item Compute the adjusted significance level for each ordered p-value $p_v$ as such:
    \begin{equation}\label{equ:holm-adjust}
        \alpha_v = \dfrac{\alpha}{\hat{m}-v+1}
    \end{equation}
    \item Compare each ordered p-value $p_v$ to its corresponding adjusted significance level $\alpha_v$:
    \begin{itemize}
        \item Reject the null hypothesis for $p_v$ if $p_v\leq\alpha_v$
        \item Stop testing as soon as you fail to reject a null hypothesis (i.e. when $p_v>\alpha_v$)
    \end{itemize}
\end{enumerate}

\section{Limitations of the CDD}~\label{sec:limitations-cdd}

A significant benefit of the CDD is its ability to distill a large volume of information into a format that
is easy to understand. However, this simplification introduces several shortcomings. We will discuss three key 
issues with the CDD: (1) the inconsistency of the average rank, (2) the failure to adequately account 
for the magnitude of performance differences and (3) the adverse effects of applying multiple testing corrections.

\subsection{Instability of the Average Rank}~\label{sec:instability-average-rank}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/chapter_2/cdd/swap_eg_1b.pdf}
    \caption{Manipulation of the ranks of DrCIF and InceptionTime and the statistical 
    significance of their 
    pairwise differences by inclusion of similar comparates. When \protect\mycolorbox{44,160,44,0.7}{ResNet} is 
    replaced by \protect\mycolorbox{214,39,40,0.7}{STSF}, DrCIF moves from 
    a ``\protect\mycolorbox{31,119,180,0.7}{worse}'' to a ``\protect\mycolorbox{255,127,14,0.7}{better}''
    rank, and the pairwise differences between DrCIF and InceptionTime change from being 
    not statistically significant to statistically significant.}
    \label{fig:cdd-swap-1}
\end{figure}

The CDD arranges comparates based on their average rank (see Section~\ref{sec:average-rank}). However, this average 
rank can vary with the addition or removal of comparates. The relative ranking of a group of comparates
$\mathcal{C}$, can 
shift when one or more comparates are added or removed, as illustrated in Figures~\ref{fig:cdd-swap-1} and~\ref{fig:cdd-swap-2}. 

In particular, Figure~\ref{fig:cdd-swap-1} demonstrates that by replacing ResNet with STSF in the set of comparates, 
DrCIF changes its rank relative to InceptionTime,from a worse to a better rank. This shift also alters the pairwise 
significance between DrCIF and InceptionTime from not significant to significant. (In this scenario, ResNet is a 
weaker deep learning algorithm compared to InceptionTime, and STSF is a weaker interval method compared to DrCIF.)

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/chapter_2/cdd/swap_eg_2b.pdf}
    \caption{Manipulation of the ranks of DrCIF and TS-CHIEF and the 
    statistical significance of their pairwise 
    differences by inclusion of weakened comparates. When 
    a weakened variant of TS-CHIEF (\protect\mycolorbox{44,160,44,0.7}{TS-CHIEF*})
    is replaced by a weakened 
    variant of DrCIF (\protect\mycolorbox{214,39,40,0.7}{DrCIF*}), 
    DrCIF moves from a 
    ``\protect\mycolorbox{31,119,180,0.7}{worse}'' to a ``\protect\mycolorbox{255,127,14,0.7}{better}'' 
    rank and the pairwise differences between DrCIF 
    and TS-CHIEF change from being not statistically 
    significant to statistically significant.}
    \label{fig:cdd-swap-2}
\end{figure}

Figure~\ref{fig:cdd-swap-2} shows how adding a weaker version of a comparate (e.g., a different version of a classifier 
with fewer parameters or different hyperparameter tuning) can influence the mean rank and pairwise statistical significance.
Adding a weaker version of a 
comparate can significantly elevate the rank of the original comparate and change the statistical significance of 
pairwise differences between comparates. The original comparate, being more accurate than its weaker version, will 
have a better mean rank on many tasks, resulting in a lower p-value in the Wilcoxon signed-rank test, which can 
shift pairwise differences to become statistically significant under the Holm correction.

Therefore, we suggest that comparates should be ordered using a statistical measure that remains stable regardless 
of the addition or removal of other comparates. This approach ensures that the relative ranking of different comparates 
is consistent across studies and not susceptible to manipulation, either intentionally or unintentionally.

\subsection{Insufficient Attention to the Magnitude of Wins and Losses}~\label{sec:wtl-not-sufficient}

The mean rank evaluates the frequency with which a comparate outperforms or underperforms others across multiple tasks, 
such as achieving higher or lower classification accuracy. It does not consider the extent of these performance 
differences. Therefore, a comparate might achieve a low average rank through several minor wins while also experiencing 
significant losses. For instance, if a comparate $c_i$ has a $90\%$ likelihood of a slight loss against $c_j$ but a $10\%$ 
likelihood of a substantial gain, most would prefer $c_i$. Nevertheless, mean rank would heavily favor $c_j$.

While the Wilcoxon test somewhat considers the magnitude of wins, it does not fully resolve the issue when comparisons 
are primarily based on rank, as illustrated in the CDD of Figures~\ref{fig:cdd-swap-1} and~\ref{fig:cdd-swap-2}.

\subsection{Null Hypothesis Significance Testing}~\label{sec:issue-nhst}

The practice of using statistical significance tests to evaluate performance on benchmark tasks is facing growing
criticism.~\cite{bayseian-benavoli-paper} asserts that null hypothesis significance testing (NHST), such as the Wilcoxon test, 
is not optimal for benchmarking scenarios involving multiple comparates and multiple tasks for four main reasons. 
This viewpoint is also echoed by~\cite{pvalues-pitfal-paper}.

\textbf{First}, Null hypothesis significance testing (NHST) does not provide the probability of the alternative hypothesis 
(the hypothesis that there is a difference between comparates given the observed results). Instead, NHST calculates the 
likelihood of observing the outcomes $O$ assuming the null hypothesis $H_0$ is true, 
represented as $p(O|H_0)$, meaning there is no difference between comparates.~\cite{bayseian-benavoli-paper} suggests that 
a more meaningful assessment would be $p(H_0|O)$, the probability of the null hypothesis being \textit{true}
given the observed outcomes.

\textbf{Second},~\cite{bayseian-benavoli-paper} highlights that the null hypothesis 
can always be rejected by simply increasing the number of 
examples, a point we revisit in Section~\ref{sec:holm-manipulation}.

\textbf{Third},~\cite{bayseian-benavoli-paper} claims that NHST fails to indicate the 
magnitude of differences between comparates,
regardless of how small the p-value might be. For instance, if one comparate's accuracy is only $10^{-4}$ 
higher than another's on most benchmark tasks, the p-value (e.g., from a Wilcoxon test) could be extremely low, 
misleadingly suggesting a significant difference in accuracy. However, the p-value does not provide any insight 
into the actual magnitude of the performance difference between comparates. This problem is related to, but separate from, 
the issue discussed in Section~\ref{sec:wtl-not-sufficient}, where ranks do not reflect the magnitude of performance 
differences between comparates.

\textbf{Finally}, because the p-value indicates the likelihood of observing the given outcomes under the assumption 
that the null hypothesis is true, it does not directly reflect the probability that the null hypothesis itself 
is true. Therefore, a large p-value (which might suggest that differences are not statistically significant) 
does not actually confirm or refute the null hypothesis~\cite{lecoutre2014significance}.

\subsubsection{Inferential vs Descriptive Statistics}
The p-value derived from a statistical hypothesis test is often utilized as an inferential statistic rather than 
a descriptive one. Descriptive statistics accurately summarize the empirical properties of the results, whereas 
inferential statistics attempt to draw conclusions about the population from which the data was sampled. Practically, 
inferential statistics aim to predict or quantify how a comparate would perform on new or unseen data randomly drawn 
from the same source. Inferential statistics make stronger claims about the results by relying on stronger assumptions.

For instance, when applying the Wilcoxon test to two comparates $c_i$ and $c_j$ over $n$ tasks 
$\mathcal{D} = \{d_1, \ldots, d_n\}$, we obtain performance measures $\gamma(c_i,d_1), \ldots, \gamma(c_i,d_n)$ 
and $\gamma(c_j,d_1), \ldots, \gamma(c_j,d_n)$. The two-tailed Wilcoxon test returns the probability $p$ that the 
differences $\gamma(c_i,d_1) - \gamma(c_j,d_1), \ldots, \gamma(c_i,d_n) - \gamma(c_j,d_n)$ 
would be observed if these values were an independent and identically 
distributed (iid) sample from a distribution $\Omega$ that is symmetric around zero. 
As an inferential statistic, we reject the null hypothesis that the distribution of these differences is symmetric 
if $p-value \leq \alpha$, where $\alpha$ is the chosen significance level. The p-value represents the probability of 
obtaining a test statistic as extreme as, or more extreme than, the one observed, purely by chance, if the comparates 
were selected without prior knowledge of the data.

In traditional scientific experiments, this approach is feasible because data is typically collected freshly for each 
experiment. However, in many benchmarking scenarios, researchers select comparates based on algorithms and hyper-parameters 
that perform well on the given benchmark. They do not collect new benchmark data by re-sampling from the problems that 
define the benchmark. Therefore, it is difficult to identify a meaningful distribution from which the performance scores 
could be considered an iid sample, making the use of the test statistic and p-value for inferential purposes problematic.

Despite these issues, test statistics and p-values are valuable for measuring divergence between two or more sets 
of data points (e.g., the classification accuracies of two comparates). They provide a quantitative descriptive measure 
of performance differences between comparates.

\subsection{The Use of Multiple Test Corrections}~\label{sec:holm-manipulation}

When performing multiple tests, such as Wilcoxon tests between all pairs of comparates, it is generally accepted that the 
significance level (typically $0.05$) should be adjusted 
\cite{demsar-cdd-paper,bayseian-benavoli-paper,pvalues-pitfal-paper}.
Multiple test corrections aim to manage the risk of incorrectly rejecting any null hypothesis when several null 
hypotheses are tested simultaneously. The Holm correction~\cite{holm-correction-paper} is a widely used method for this 
purpose (see Section~\ref{sec:wilcoxon} for details on the Holm correction).

We identify that it is more crucial to control the risk for each pair of comparates rather than for the entire 
study. We also show that multiple testing corrections can introduce undesirable effects, potentially leading 
to the manipulation of benchmark results. This undermines efforts to provide a consistent and stable comparison 
of multiple comparates across tasks.

A significant issue is that the number of comparates in a study and the specific comparates included can affect the 
likelihood of one comparate outperforming another~\cite{bayseian-benavoli-paper}.
This enables the manipulation of the statistical significance of differences between comparates, whether
intentionally or unintentionally, by including or excluding certain comparates from the comparison.

\begin{figure}[ht]
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=0.995\linewidth]{Figures/chapter_2/cdd/pairwise_eg_3.pdf}
        \caption{\null}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=0.995\linewidth]{Figures/chapter_2/cdd/pairwise_eg_2.pdf}
        \caption{\null}
    \end{subfigure}
    \caption{
        Two examples demonstrate the instability of pairwise significance under the Holm correction: (a) for the 
        comparates BOSS, Catch22, TSF, and WEASEL; and (b) for the comparates DrCIF, HC2, Hydra, and MultiRocket. 
        The statistical significance of pairwise differences between comparates is influenced by the additional 
        comparates included in the comparison. In each case, four different patterns of statistically significant 
        pairwise differences,(i), (ii), (iii), and (iv),are shown in the left column (pairs with non-significant 
        differences according to the Wilcoxon signed-rank test with Holm correction are connected by a black line). 
        Randomly chosen examples of additional comparates that result in the given pattern of statistically 
        significant pairwise differences are shown in the right column.
    }
    \label{fig:stability-holm}
\end{figure}

To highlight the instability of pairwise significance when using the Holm correction, we illustrate how different sets 
of comparates influence the results. Figure~\ref{fig:stability-holm} shows two examples of this instability, demonstrating how the statistical 
significance of pairwise differences can change with different sets of comparates. In each example, we begin with a 
core set of four comparates and repeatedly combine this core set with different sets of four additional comparates. 
For each combination, the Wilcoxon test with Holm correction was applied to all pairs, noting which pairwise differences 
within the core set were statistically significant. Non-significant pairwise differences within the core set are indicated 
by lines connecting the comparates.

Figure~\ref{fig:stability-holm} presents results for core sets consisting of (a) BOSS, Catch22, TSF, and WEASEL; and (b) DrCIF, 
HC2, Hydra, and MultiRocket. These examples are based on results for 23 different comparates across 108 datasets from 
the UCR archive. For instance, in Figure~\ref{fig:stability-holm}(a), combining comparates BOSS, Catch22, TSF, and WEASEL 
with any of the five sets of comparates listed for pattern (ii),such as Arsenal, MultiRocket, ResNet, and Rocket,produces 
the pattern shown on the left, where the pairwise differences between BOSS and WEASEL are not statistically significant. 
However, combining the same core comparates with any of the sets listed in (i), (iii), or (iv) results in different 
patterns of statistical significance.


The examples in Figure~\ref{fig:stability-holm} illustrate that the statistical significance of pairwise differences can 
be influenced by adding or removing comparates, whether intentionally or unintentionally. In many instances, various
different sets of additional comparates can result in the same pattern of statistical significance for pairwise differences. 
For instance, in Figure~\ref{fig:stability-holm}(b), there are 123 different sets of additional comparates that produce pattern 
(i), 1,876 sets that produce pattern (ii), 680 sets that produce pattern (iii), and 1,197 sets that produce pattern (iv). 
For simplicity, only five randomly selected combinations of additional comparates are shown for each pattern in Figure~\ref{fig:stability-holm}.


This issue occurs because a multiple testing correction, such as the Holm correction, adjusts the threshold for statistical 
significance based on the p-values of all pairs of comparates. Consequently, the significance of pairwise differences for a given 
pair of comparates depends on the p-values of all other pairs. Adding or removing comparates with small 
p-values can shift pairwise differences above or below the threshold for significance. For a given pair of comparates, 
including or excluding other comparates can turn an otherwise significant difference into a non-significant one, and vice versa.


Another issue is that multiple test corrections aim to prevent any algorithm from being incorrectly found superior by chance. 
However, this comes at the expense of increasing the risk of overlooking true findings of superiority. It is debatable why one 
risk should be prioritized over the other. The ability to claim that a new algorithm is not significantly less effective 
than the current state-of-the-art allows proponents to add enough algorithms to a comparison to achieve such a claim.

To avoid the ``data dredging'' problem, the adjustment for multiple testing should ideally consider the total number of 
comparates, including all variations of an algorithm that were tested and discarded during development. However, this is 
often impractical, suggesting that such efforts may be futile.

Moreover, different studies might produce varying results for pairwise comparisons of the same two competitors depending 
on the number of comparates included. A study with fewer comparates might reject the null hypothesis and find a significant 
difference between comparates $c_i$ and $c_j$, whereas a study with more comparates might fail to reject the null hypothesis, 
finding no significant difference based on the same evidence. This inconsistency undermines the reliability of such evidence 
bases.

\section{An Alternative Approach}

As previously mentioned, recent studies have attempted to tackle some of these issues, especially regarding the statistical 
significance testing of pairwise differences between comparates. Notably, we emphasize the approach proposed
by~\cite{bayseian-benavoli-paper}.

\cite{bayseian-benavoli-paper} argued that the Wilcoxon test, or similar tests, should be replaced by a new Bayesian test 
modeled after the Wilcoxon test. For comparing two comparates $c_i$ and $c_j$, the Bayesian signed rank test generates 
a probability distribution indicating the likelihood that $c_i$ is significantly better than $c_j$, $c_j$ is significantly 
better than $c_i$, or $c_i$ and $c_j$ are not significantly different. 

Let $\textbf{z} = [z_0, z_1, \dots, z_i, \dots, z_q]$ be the vector of performance differences between $c_i$ and $c_j$ 
across $q$ tasks, including a pseudo observation $z_0$, a hyperparameter of the Dirichlet Process (DP). Assuming 
$\textbf{z}$ follows a DP, the resulting probability distribution is given by:

\begin{equation}
    \begin{split}
        \theta_l &= \sum_{i=0}^q\sum_{j=0}^q \omega_i \omega_j \mathbf{I}_{(-\infty,-2r)}(z_i+z_j)\\
        \theta_e &= \sum_{i=0}^q\sum_{j=0}^q \omega_i \omega_j \mathbf{I}_{(-2r,2r)}(z_i+z_j)\\
        \theta_r &= \sum_{i=0}^q\sum_{j=0}^q \omega_i \omega_j \mathbf{I}_{(2r,\infty)}(z_i+z_j),
    \end{split}
\end{equation}

\noindent
where $\mathbf{I}_A(x) = 1$ if $x \in A$, the weights $\omega_i$ follow a Dirichlet distribution $D(s,1,1,\ldots,1)$ and 
$r$ is the ``rope'' value that sets the interval of which the two classifiers have no significant difference.
Given that the distribution $\theta_l$, $\theta_e$,and $\theta_r$ does not have a closed form solution, the probability 
distribution is generated using a Monte Carlo sampling on the weights $\omega_i$.

An example of this Bayesian test is shown in Figure~\ref{fig:bayesian_triangle}, comparing InceptionTime~\cite{inceptiontime-paper} 
and ROCKET~\cite{rocket} on 108 datasets from the UCR archive~\cite{ucr-archive}. The triangle illustrates that there is a 
$77.7\%$ probability that ROCKET outperforms InceptionTime, with only a $17.3\%$ probability that the classifiers are not 
meaningfully different.


\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{Figures/chapter_2/bayesian_ROCKET_vs_InceptionTime.pdf}
    \caption{A visualization of the Bayesian Signed Rank Test proposed in~\cite{bayseian-benavoli-paper} as a 
    replacement for the Wilcoxon Signed Rank Test is provided. As illustrated, the Bayesian test offers information 
    on the probability that the null hypothesis is true, given the performance metrics of both InceptionTime and 
    ROCKET on the 108 tasks from the UCR archive.
    }
    \label{fig:bayesian_triangle}
\end{figure}

Considering the limitations discussed in this chapter, there is a clear need for an alternative approach instead 
of the CDD.
The method proposed by~\cite{bayseian-benavoli-paper} highlights the importance of measuring the statistical 
significance of pairwise differences between classifiers as an alternative to the Wilcoxon signed-rank test. 
This Bayesian approach is not mutually exclusive with the method described in the following section; instead, 
we consider it complementary. These efforts are geared towards enhancing the robustness of statistical testing 
for differences between comparates. Instead of relying on p-values derived from the Wilcoxon signed-rank test, 
the probabilities computed via the Bayesian signed-rank test could be utilized in the alternative approach 
detailed in the subsequent section.

\section{The Multi-Comparison Matrix}\label{sec:MCM}

Our goal is to develop methods for assessing $m$ comparates $\mathcal{C}$ across multiple datasets $\mathcal{D}$
using a single performance measure $\gamma$ and pairwise comparison measure $\delta$ that:

\begin{itemize}
    \item prioritizes pairwise comparisons between comparates;
    \item focuses on descriptive statistics over statistical hypothesis testing;
    \item ensures that pairwise comparisons $\delta(c_i,c_j)$ between any two comparates $c_i\in\mathcal{C}$ and 
    $c_j\in\mathcal{C}$ are invariant to $\mathcal{C}\backslash\{c_i,c_j\}$ (i.e., no pairwise comparison will 
    change with the addition or deletion of other comparates, maintaining consistency across studies);
    \item orders comparates such that the relative order of any two comparates $c_i\in\mathcal{C}$ and $c_j\in\mathcal{C}$ 
    is invariant to $\mathcal{C}\backslash\{c_i,c_j\}$ (i.e., the order of $c_i$ and $c_j$ will not change with the addition 
    or deletion of other comparates, ensuring stability across studies);
    \item provides a good balance between the amount of information presented and the informativeness of that information.
\end{itemize}


To achieve this, we propose a grid of pairwise comparison statistics, as shown in
Figures~\ref{fig:mcm-example} and~\ref{fig:mcm-example-row}. 
% We offer an open-source implementation at \url{https://github.com/MSD-IRIMAS/Multi_Comparison_Matrix}.
The proposed Multi-Comparison Matrix (MCM) maintains the pairwise comparisons between each pair of comparates 
$c_i$ and $c_j$, ordering the comparates by default based on the average performance measure $\gamma$. 
Each cell of this matrix contains three pairwise statistics between $c_i$, the comparate for the row, and $c_j$, 
the comparate for the column. 
These three statistics are:

%
\begin{itemize}
    \item The mean of $\gamma(c_i,d)-\gamma(c_j,d)$ over all $d\in\mathcal{D}$;
    \item A Win Tie Loss count for $c_i$ against $c_j$ over all the tasks (datasets) in $\mathcal{D}$;
    \item A p-value ($p$) for a Wilcoxon Signed Rank Test ($\delta$).
\end{itemize}
Note that despite concerns about using statistical significance testing for benchmarking, we continue to use 
the Wilcoxon test and the associated $p$ value. While there are arguments for excluding formal statistical hypothesis tests, 
we recognize that this may be too drastic for many. Therefore, we prioritize descriptive statistics. However, considering 
the issues discussed above, we encourage interpreting the $p$ value as a \textit{descriptive} statistic, a measure of the 
"strength" of the difference between comparates, rather than as an inferential statistic. In other words, the $p$ value 
should not be viewed as indicating the likelihood or probability of observing a similar difference in accuracy between 
comparates on new or unseen data (i.e., ``out of benchmark'').

By default, the MCM is generated with all comparates present in both the rows and the columns,
resulting in $m \times (m-1) / 2$ comparisons. Alternatively, separate lists of comparates for the rows and columns
can be specified, $\mathcal{C}_{\mathrm{row}}$ and $\mathcal{C}_{\mathrm{col}}$. In this case there are
$|\mathcal{C}_{\mathrm{row}}|\times|\mathcal{C}_{\mathrm{col}}|-|\mathcal{C}_{\mathrm{row}}\cap\mathcal{C}_{\mathrm{col}}|$
comparisons.

\subsection{MCM Examples}\label{sec:heatmap}
By default, all pairwise comparisons between comparates are displayed. The average performance 
measure $\gamma$ (e.g., classification accuracy) across all tasks $\mathcal{D}$ is shown next to each 
comparate label. An example illustrating the results for five comparates over 108 datasets from the UCR 
archive~\cite{ucr-archive} is provided in Figure~\ref{fig:mcm-example}. In the Heat Map, colors represent 
the mean difference in $\gamma$. A positive difference (red) indicates that the row comparate outperforms 
the column comparate on average. For instance, in Figure~\ref{fig:mcm-example}, the top right cell is red, 
demonstrating that MultiROCKET (row) is generally more accurate than ResNet (column). Conversely, a 
negative difference (blue) means that the column comparate outperforms the row comparate on average. 
Text in each cell is in \textbf{BOLD} if the $p$ value is below a specified threshold (e.g., $0.05$).


This format of the MCM is highly effective for presenting comparisons in a benchmark review, such as in~\cite{dl4tsc}. 
In such reviews, detailed information on all pairwise comparisons is essential to highlight the strengths and weaknesses 
of each comparate. For example, this MCM format can illustrate how a comparate that performs poorly against the winning 
comparate might still excel on certain datasets compared to other state-of-the-art comparates.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/chapter_2/mcm/mcm_example.pdf}
    \caption{MCM showing all pairwise comparisons between MultiROCKET, ROCKET, InceptionTime, ResNet, and
    FCN on the 128 datasets of the UCR archive. In this setup, the full pairwise comparison is presented.}
    \label{fig:mcm-example}
\end{figure}

However, if a study focuses on a few specific comparates, such as when introducing a new algorithm, it is often more beneficial 
to highlight comparisons between these few and many existing alternatives. In these scenarios, the MCM can be adjusted to display 
only the necessary results for comparing the proposed comparates with the current state-of-the-art. This allows the reader 
to focus on the most pertinent comparisons. The proposed comparates are listed in either a row or a column of the matrix, 
as illustrated in Figure~\ref{fig:mcm-example-row}.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/chapter_2/mcm/mcm_row.pdf}
    \caption{MCM showing pairwise comparisons between InceptionTime and each of ROCKET, 
    MultiROCKET, FCN and ResNet on the 128 datasets of the UCR archive.}
   \label{fig:mcm-example-row}
\end{figure}

\section{Conclusion}

In this chapter, we have explored the critical aspects of benchmarking machine learning models on time series data. 
The discussion began with an examination of the significance of rigorous benchmarking and the various challenges 
associated with it, including the instability of pairwise comparisons and the inherent limitations of traditional 
statistical significance tests. These challenges highlight the necessity for developing more robust and reliable 
benchmarking techniques.

We detailed the Critical Difference Diagram (CDD), a widely utilized tool for visualizing the performance of 
multiple comparates across various tasks. While the CDD provides valuable insights, it also has significant 
limitations, such as its sensitivity to the addition or removal of comparates and its reliance on average ranks that 
may not adequately reflect performance differences.

To address these limitations, we proposed the Multi-Comparison Matrix (MCM) as a novel approach for presenting 
pairwise comparisons between comparates. The MCM preserves the integrity of pairwise comparisons and ensures that 
these comparisons remain consistent regardless of the inclusion or exclusion of other comparates. This method offers 
a more stable and comprehensive view of comparative performance, enabling researchers to make more informed decisions 
about model selection and evaluation.

The usage of MCM is not intended to act as a ``replacement'' for the CDD, as the CDD remains and will 
continue to be more ``attractive'' due to its simplicity. Instead, we propose that the MCM should serve as a complementary 
analysis tool alongside the CDD in future research, such as we do in the following Chapter.
