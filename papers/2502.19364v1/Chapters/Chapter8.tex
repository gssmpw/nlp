\chapter{Reproducible Research}\label{chapitre_8}

\section{Introduction}

Reproducibility is a fundamental aspect of scientific research, as it ensures that our 
work can be replicated and adapted by other researchers for their own applications. In this chapter, we 
underscore the significance of reproducibility in research and outline the measures 
taken to guarantee that the work presented in this thesis aims to adhere to high 
standards of reproducibility.

A notable feature of this thesis is that almost all of the contributions have been 
integrated into an open-source Python package called \textit{aeon}~\cite{aeon-paper},
for which I am a 
core developer. This package serves as a centralized repository for the tools and
methods developed during this research, ensuring their accessibility and usability 
for the broader scientific community. By packaging our contributions in this way, 
we not only promote reproducibility but also encourage the wider use and continuous 
improvement of our work.

A key aspect of this thesis is that all the work detailed in the previous chapters 
is supported by publicly available code. This transparency allows other researchers 
to replicate our experiments, validate our findings, and build upon our work with 
confidence. By making our code accessible, we contribute to a more open and collaborative 
scientific community.

In this chapter, we will detail the specific requirements we follow to define good 
reproducible work. These requirements include clear documentation and adherence to 
best practices in software development, to the best of our capabilities.
We believe that by following these standards 
and taking feedback from the community, we can continually improve the reproducibility 
and reliability of our research.

Finally, every program used locally to produce figures or assist in analysis has been 
made available. While these tools may not be directly associated with any particular 
paper, they are crucial to the overall research process. We will present these programs 
in this chapter, highlighting their roles and functionalities.

In summary in this chapter we:

\begin{itemize}
    \item introduce the \textit{aeon} Python package, which encapsulates the contributions of this thesis.
    \item highlight the importance of reproducibility in research, by outlining
    the requirements and practices that define a reproducible work.
    \item present the local programs used for figure generation and analysis.
\end{itemize}

By committing to these principles of reproducibility and actively seeking feedback from 
the community, we aim to enhance the transparency, reliability, and impact of our research.

\section{Time Series Analysis With \textit{aeon}}

The \textit{aeon} Python package is a versatile open-source library developed to facilitate 
various time series machine learning tasks. It provides tools for classification, clustering,
transformations, regression, forecasting, anomaly detection, similarity search and segmentation,
making it a comprehensive solution for handling time series data.

\textit{aeon} is designed with ease of use and extensibility in mind, offering clear documentation 
and an intuitive interface. As a core developer of this package, the contributions from 
this thesis have been integrated into \textit{aeon}, ensuring that the methods and tools developed 
are accessible to the broader scientific community. This section will introduce \textit{aeon}, 
showcasing its capabilities and demonstrating its application in diverse time series 
machine learning scenarios.

\subsection{Deep Learning For Time Series With \textit{Aeon}}

My involvement in the \textit{aeon} project began with the task of defining 
the deep learning framework from the ground up. This framework 
had to be developed with several key principles in mind: ease of 
contribution for new developers, clarity of code, straightforward 
functionality, and comprehensive documentation. Ensuring that new 
contributors could easily understand and extend the code was paramount. 
As a result of these efforts, all the models reviewed in the 
deep learning for TSC~\cite{dl4tsc} are now implemented in \textit{aeon} for both 
classification and regression tasks. Additionally, new models 
such as InceptionTime~\cite{inceptiontime-paper},
our own H-InceptionTime (Chapter~\ref{chapitre_3}),
and LITETime (Chapter~\ref{chapitre_4}) 
have been incorporated, further enhancing the package's 
capabilities and robustness.

The maintenance of this framework is ongoing and 
involves several critical activities:

\begin{itemize}
    \item \textbf{Bug Fixes}: Regular updates are made to 
    identify and resolve bugs promptly, ensuring the stability 
    and reliability of the framework.
    \item \textbf{Documentation Improvement}: Continuous efforts 
    are made to enhance the documentation, making it clearer 
    and more comprehensive for users and contributors.
    \item \textbf{Feature Enhancement}: New features and capabilities 
    are regularly added to the framework, expanding its functionality 
    and keeping it at the forefront of time series machine learning 
    research.
    \item \textbf{Unit Testing}: Rigorous unit testing is conducted 
    to ensure the code remains robust and compatible with updates to 
    \textit{tensorflow}~\cite{tensorflow-paper}
    and \textit{keras}~\cite{keras-website}. This testing helps 
    maintain the integrity and performance of the framework as 
    the underlying libraries evolve.
\end{itemize}

By committing to these maintenance activities, we ensure that the \textit{aeon} 
package remains a valuable and reliable tool for the scientific community.

Recently, I started working on the deep learning for time series 
clustering module in \textit{aeon}, building upon the foundational work 
established in the original review~\cite{deep-tscl-bakeoff}.
This module is still under 
development, aiming to provide robust and efficient tools for 
clustering time series data using deep learning techniques. So 
far, I have included two original models from the review into 
the module, Auto-Encoder based deep learning models with FCN and ResNet
backbone networks, laying the groundwork for further expansion. In 
addition, I have been actively involved in mentoring a Google 
Summer of Code 
internship~\footnote{\url{https://summerofcode.withgoogle.com/programs/2024/projects/Hvd0DfkD}}, 
guiding the intern to enhance and 
develop more models for this module. This collaborative effort 
aims to accelerate the development process and ensure the 
inclusion of state-of-the-art deep clustering models in \textit{aeon}.

The upcoming work in deep learning within the \textit{aeon} package includes 
developing modules for time series anomaly
detection~\cite{anomaly-detection-review},
averaging~\cite{deep-averaging-paper}, 
and domain adaptation~\cite{deep-domain-adaptation-review}.
These enhancements will broaden \textit{aeon}'s 
capabilities, enabling it to tackle a wider range of time series 
challenges and better serve the research community.

\subsection{Other Tasks With \textit{Aeon}}

My involvement with the \textit{aeon} project extends beyond deep learning. 
In the distances module, I contributed to and continue to 
maintain the ShapeDTW similarity measure~\cite{shape-dtw-distance},
which is integral to my development of the ShapeDBA (Chapter~\ref{chapitre_6})
method in the averaging module. Additionally, I significantly 
optimized the runtime complexity of the PAA~\cite{paa} and SAX~\cite{sax} 
representation codes, making them much faster and more efficient. 
Beyond code contributions, I have also been actively involved in 
improving general documentation, creating example notebooks, 
and co-authoring the open-source software paper~cite{aeon}
with the rest of the core developers.

We believe that these efforts collectively enhance reproducibility 
within the research community, making tools more accessible and 
reliable for all users.

\section{What Makes A Work Reproducible~?}\label{sec:guide-coding}

Reproducibility is crucial in scientific research, enabling others 
to verify and build upon previous work. In this section, I will 
outline the key requirements we followed to ensure our research 
is reproducible. A reproducible codebase must be well-documented, 
extendable, easily modifiable, and well-architected. These practices cover 
technical aspects like code development and data management, as well 
as broader principles like clear documentation and transparency. 
By adhering to these standards, we aim to make our research both 
rigorous and accessible.

\subsection{Code Documentation}

While providing a GitHub repository might seem sufficient for 
ensuring code reproducibility, it is crucial to provide clear 
instructions for users who wish to re-run the experiments 
associated with the paper.

\subsubsection{Dependencies}

A key component of code documentation is a complete 
list of dependencies that the paper's code 
relies on is essential. Without this list, a new user would need to 
manually inspect all the code files to determine which dependencies 
are required, and in some cases, they might also need to identify 
the specific versions, especially if the code relies on older versions 
of these dependencies.

For this reason, we ensured that every project published during 
this thesis included a complete list of dependencies, thereby 
supporting better open-source reproducible research.

\subsubsection{Adapting Configuration To The User's Side}

Often, users may need to make minor adjustments to the code in 
order to successfully re-run it. This does not diminish the code's 
reproducibility, but it does make it essential for the repository 
to include a detailed, step-by-step guide. Such a guide should 
specify which variables need to be modified, where to make these 
changes, and how to do so. These variables might include the root 
directory for datasets. Some instructions can be for downloading 
datasets from a provided link etc.

Providing this information in the code's documentation is crucial; 
without it, users may encounter errors that prevent the code 
from running properly.

\subsection{Extendibility}

Ensuring that code is easily executable by others is a crucial 
first step towards creating a reproducible repository. However, 
research often involves incremental contributions, which means 
the original work must be designed to be extendable. For example, 
consider the deep learning for TSC review by~\cite{dl4tsc},
if a researcher wanted to add new models or datasets on top of 
this work, it should be easily achievable. If the code is not 
designed to allow such modifications, it cannot be considered 
truly extendable or modifiable.

\subsection{Code Architecture}

When the code from a research paper is intended to be studied 
by students, employees, or other researchers, it is essential 
that the code be well architected. A poorly structured codebase 
makes it difficult, if not impossible, for others to understand 
and build upon the work. Clear and thoughtful architecture is 
fundamental for ensuring that the techniques, algorithms, and 
overall project can be effectively comprehended and utilized by others.

\begin{enumerate}
    \item \textbf{Variable naming}: One critical element of 
    well-architected code is the use of meaningful variable 
    names. For instance, vague or arbitrary names like 
    ``$X = Y + Z.dot(alpha\_xyz)$'' should be avoided. 
    Instead, variables should be named descriptively, 
    especially when they represent parameters or concepts 
    from the paper's algorithm. For example, if the method 
    includes a parameter for the number of filters, the code 
    should use a name like ``$n\_filters$'' rather than something 
    ambiguous like ``$f$''.

    \item \textbf{File management}:
    Proposing a repository where a single file contains over 
    $10~000$ lines of code is simply unacceptable. Code should 
    be organized in a way that allows users to easily locate 
    specific functionalities. For example, if the project includes 
    multiple classifiers and normalization functions, each 
    classifier should be placed in its own file within a 
    dedicated sub-folder, while all normalization functions 
    could be grouped together in a separate file, as these 
    are typically concise. The choice of file management 
    structure doesn't follow a rule of thumb; it should be 
    tailored to the specific needs of the project.

    \item \textbf{Code Structure}:
    While there is no one-size-fits-all approach to file management, 
    the choice of code structure often follows certain best practices. 
    For instance, if the code does not require defining objects 
    with multiple functionalities, using only functions is sufficient, 
    and introducing classes would be unnecessary. Conversely, if 
    the code benefits from encapsulating behavior within objects, 
    then using classes is more appropriate.
    Furthermore, if several classes share common code, 
    it's not good practice to copy and paste these functionalities 
    across different classes. Instead, defining a base class that 
    these classes can inherit from is a better approach. 
    This also applies when working with abstract methods, 
    class methods, and similar concepts.
\end{enumerate}

\subsection{How To Check For All These Requirements}

When serving as the main developer of a project, it's often not 
ideal to be the one assessing whether all the necessary requirements 
for reproducibility and code quality are met. This is due to inherent 
human bias, as developers tend to view their own projects as being in 
an optimal state. To mitigate this bias, a better practice is to seek 
feedback from a fellow researcher, student, PhD candidate or a 
thesis supervisor. For example, you could ask one of these peers to 
try adding new functions to your code and provide feedback on how 
easily your code can be extended.

\section{Hardware Utilization and Accessible Code Repositories}

Throughout this thesis, I utilized a diverse set of hardware to support the computational demands 
of deep learning models, including a GTX1080ti with 8GB of VRAM, an RTX3090 with 24GB of VRAM, an 
RTX4090 with 24GB of VRAM, and the Mesocentre High Performance Computing Center 
of the University of Strasbourg. Initially, setting 
up the GPUs involved manually configuring the necessary CUDA tools, which required significant time 
and effort to ensure compatibility and performance. However, as the work progressed, I transitioned 
to using Docker containers, which provided several advantages, including simplified environment 
management, enhanced portability, and the ability to encapsulate all dependencies within a container. 
This ensured that the code ran consistently across different machines and facilitated easier scaling 
and deployment.

For each published article, I ensured that the community has access to all source codes and resources 
necessary to reproduce the work, as outlined previously in Table~\ref{tab:intro-contributions}.
The provided Dockerfiles encapsulate the code and configure the environment 
for seamless GPU integration, allowing users to execute the code efficiently with just two to three commands, 
leveraging GPU acceleration without the need for complex manual configurations. Additionally, all codes were 
developed following the guidelines outlined in the previous section, ensuring consistency, reliability, and 
ease of use for others in the community.

\section{Published Work Serving For Analysis And Reproducibility}

A central goal of this thesis has been to ensure that all tools 
used in our analysis are accessible to the broader community. 
By making these resources available, we enhance the community's 
ability to understand, replicate, and build upon our work, thereby 
increasing its impact and clarity for future readers.

We accomplished this through two main strategies: first, by publishing 
public repositories that contain the code used to generate visualizations 
and support our research analysis; and second, by creating web pages 
featuring interactive tools that go beyond what can be presented in a 
paper or GitHub repository. These efforts aim to deepen the community's 
understanding and facilitate wider engagement with our research.

\subsection{Elastic Warping Visualization}

A fundamental approach to analyzing time series data involves 
assessing how sensitive the samples are to temporal distortions. 
While there are various methods to achieve this, the most effective 
way is often through direct visualization. To facilitate this, we 
developed a public GitHub repository~\cite{ismail-fawaz2024elastic-vis} 
available here:
\url{https://github.com/MSD-IRIMAS/Elastic_Warping_Vis}.
This repository allows users to visualize the warping path and 
temporal distortions between any two time series samples, with 
outputs available in either PDF format, as shown in 
Figure~\ref{fig:example-warping-path-github}, or as MP4 
format~\footnote{\url{https://github.com/MSD-IRIMAS/Elastic\_Warping\_Vis/blob/main/exps/dtw-vis/ECGFiveDays/dtw.mp4}}.
\begin{figure}
    \centering
    \caption{Warping path example between two time series from 
    the ECGFiveDays dataset.}
    \label{fig:example-warping-path-github}
    \includegraphics[width=0.6\textwidth]{Figures/chapter_8/dtw.pdf}
\end{figure}

The code leverages the \textit{aeon} Python package for computing the warping 
path and distance matrix, and uses \textit{matplotlib} for 
visualization~\cite{matplotlib-paper}.
Users can apply any of the distance functions implemented in \textit{aeon}~\footnote{
\url{https://www.aeon-toolkit.org/en/stable/api_reference/distances.html}}.

The repository not only meets all the requirements discussed in
Section~\ref{sec:guide-coding} for public availability on 
GitHub but is also conveniently 
installable via PyPi~\footnote{\url{https://pypi.org/}}.

\subsection{Convolutional Filter Space Visualization}\label{sec:filters-code}

Training a CNN often requires post-training analysis of the convolutional 
filter space. For example, in Chapter~\ref{chapitre_3}
this type of analysis was conducted to ensure that the model was not relearning hand-crafted filters, as 
illustrated in Figure~\ref{fig:hcf-tsne_filters}.
After publishing this work,
we developed a GitHub repository that provides code to 
generate similar figures for any pre-trained model.
This tool~\footnote{\url{https://github.com/MSD-IRIMAS/filter1D\_visualization}}
produces visualizations in both PDF format, 
like Figure~\ref{fig:hcf-tsne_filters}
and in a web-friendly format using \textit{bokeh}~\cite{bokeh}, 
a visualization tool for web 
interfaces~\footnote{\url{https://maxime-devanne.com/pages/filter1D\_visualization/}}.
This resource will assist researchers in better analyzing 
the convolutional filter space when working with CNNs.

\subsection{Augmenting Time Series Classification Datasets With Elastic Averaging}

Since the publication of~\cite{weighted-dba-paper}, research on data augmentation for 
time series data has seen a significant increase in both quantity and diversity. 
The work in \cite{weighted-dba-paper} focuses on using weighted elastic averaging, specifically 
DBA~\cite{dba-paper}, for data augmentation to enhance 
the performance of TSC models. Although the original
implementation was written in Java, the method continues to attract attention and 
further development. In response, we proposed an open-source Python implementation 
of this approach~\cite{Ismail-Fawaz2023weighted-ba}~\footnote{\url{https://github.com/MSD-IRIMAS/Augmenting-TSC-Elastic-Averaging}}, 
utilizing the \textit{aeon}~\cite{aeon-paper} library as the backend for similarity and 
averaging computations. Our code adheres to open-source standards and offers flexibility, 
allowing users to parameterize the augmentation function with any similarity 
measure they choose.

\subsection{KAN It Work For Time Series Classification ?}

Following the publication of a new approach to neural networks 
using Kolmogorov-Arnold Networks (KANs)~\cite{kan-paper}, which are designed to 
effectively model complex relationships within tabular data by leveraging the 
Kolmogorov-Arnold representation theorem, we were interested in testing their 
applicability to time series data. Given their success with tabular data, we 
proposed a method that involves extracting Catch22~\cite{catch22} features from time 
series data before feeding them into the KAN model for classification tasks. Our goal 
was to provide an open-source repository~\cite{Ismail-Fawaz2023kan-c22-4-tsc}~\footnote{\url{https://github.com/MSD-IRIMAS/Simple-KAN-4-Time-Series}}
for this work, encouraging the time series community to engage with KAN 
models and fostering continued research and innovation in this area.

\subsection{Published Webpages With Associated Papers}

Developing webpages can be essential when visualizations, such as videos or interactive 
tools, cannot be effectively presented in a paper. For the hand-crafted convolution 
filters contribution, we created a 
webpage~\footnote{\url{https://msd-irimas.github.io/pages/HCCF-4-tsc/}}
featuring an interactive tool, utilizing the code discussed in Section~\ref{sec:filters-code}.
Similarly, for the ShapeDBA paper, we developed a 
webpage~\footnote{\url{https://msd-irimas.github.io/pages/ShapeDBA/}}
showcasing video visualizations of the ShapeDTW alignment, 
highlighting the differences between DBA and ShapeDBA in 
terms of the underlying similarity measure.

\subsection{Deep Learning For Time Series Classification: A Webpage}

Since the publication of the deep learning for TSC review~\cite{dl4tsc}, the research community 
has shown significant interest in this domain. However, with the increasing number 
of models,some of which are introduced in this thesis,revisiting the review with 
another paper wasn't ideal. Instead, we developed a 
webpage~\footnote{\url{https://msd-irimas.github.io/pages/dl4tsc/}} that features a detailed 
overview of each model from the 2019 review, along with newer models. This includes 
information on the number of parameters, FLOPS, and links to the original papers. 
The webpage also features a CD diagram~\footnote{\url{https://github.com/hfawaz/cd-diagram}} 
and an MCM from Chapter~\ref{chapitre_2} to present the results across the UCR archive~\cite{ucr-archive}.

Additionally, we created \textit{bokeh}~\cite{bokeh} based 1v1 scatter plots, allowing users to compare 
models of their choice. Furthermore, we included an interactive \textit{bokeh} plot to 
showcase the average performance of all models concerning their FLOPS and 
parameter counts across all UCR archive datasets. Users can also select specific 
datasets instead of viewing the average performance.

We believe that this webpage, with ongoing maintenance, will provide the research 
community with easy access to comprehensive details, results, and comparisons 
needed for deep learning in the TSC task.

\section{Conclusion}

In this chapter, we emphasized the critical role of reproducibility in scientific 
research, particularly within the context of time series analysis and deep learning.
Ensuring that research is transparent and replicable allows others to confidently 
build upon existing work. To this end, we have meticulously documented our code and 
methodologies, providing clear instructions providing clear instructions that facilitate 
accurate reproduction of our experiments. Additionally, we have focused 
on creating a well-structured and organized codebase, adhering to best practices in 
software engineering. This approach not only ensures the code's functionality but also 
makes it easier for other researchers to extend and adapt our work.

Moreover, we have made significant efforts to share the tools and resources 
developed during this thesis with the broader research community. By publishing 
our work on GitHub and integrating it into the \textit{aeon} Python package, we have 
made these resources widely accessible, fostering a collaborative environment 
for further innovation. The development of interactive web tools further enhances 
the accessibility and understanding of our work, allowing users to explore complex 
data and results dynamically. Through these efforts, we aim to contribute to a 
culture of reproducibility in research, ensuring that our work serves as a reliable 
foundation for future advancements in time series analysis.