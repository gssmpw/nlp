\chapter{Time Series Analysis For Human Motion Data}\label{chapitre_6}

\section{Introduction}

Human motion data, particularly skeleton-based data, plays a crucial role in various applications such as 
action recognition~\cite{human-motion-example-paper}, rehabilitation assessment~\cite{kimore-paper},
prediction/forecasting~\cite{hm-prediction-paper}, and generation~\cite{actor-paper}
for cinematic and gaming systems. 
This chapter digs into the unique aspects of Multivariate Time Series (MTS) analysis when applied to 
skeleton-based human motion sequences.

Skeleton-based human motion data is primarily extracted using advanced sensing technologies like the Microsoft 
Kinect~\cite{kinect-paper,kinect-survey-paper}, which captures 3D spatial coordinates
of body joints. This data provides a simplified yet informative 
representation of human motion by tracking the positions of key skeletal joints over time. Other technologies, 
such as motion capture systems~\cite{mocap-paper},
also offer detailed skeletal data by using markers placed on the body to record 
joint movements with high precision. These datasets are invaluable for various tasks due to their ability to 
encapsulate the complexity of human motion in a structured format.
One significant advantage of Kinect sensors over traditional MoCap technologies is their inherent synchronization. 
With Kinect, all joints are detected through a single camera operating at a consistent sampling frame rate, 
ensuring uniformity and temporal coherence in the captured data. In contrast, MoCap systems assign individual 
sensors to each joint, which can lead to potential desynchronization issues between the sensors. Moreover, Kinect 
sensors offer the benefits of being both low-cost and non-intrusive, enhancing their accessibility and ease of use. 
These attributes make Kinect an ideal choice for our study, and hence, we have exclusively utilized Kinect-based data 
in this chapter.
An example of such sequences are presented in Figure~\ref{fig:example-humanact}.

\begin{figure}
    \centering
    \caption{Example of one sample per action (12 total) taken from the HumanAct12 
    action recognition
    skeleton based dataset~\cite{action2motion-paper}. Each skeleton is made of five body parts,
    \protect\mycolorbox{161,135,88,1.0}{right arm},
    \protect\mycolorbox{203,182,126,1.0}{left arm},
    \protect\mycolorbox{108,158,157,1.0}{spine \& neck},
    \protect\mycolorbox{125,116,130,1.0}{right leg} and
    \protect\mycolorbox{178,168,183,1.0}{left leg}.
    }
    \label{fig:example-humanact}
    \includegraphics[width=\textwidth]{Figures/chapter_6/intro/example_human_act.pdf}
\end{figure}

The versatility of skeleton-based human motion data lends itself to numerous tasks:

\begin{itemize}
    \item \textbf{Action Recognition}: Identifying specific actions or activities performed by individuals.
    \item \textbf{Motion Assessment}: Monitoring and analyzing patient movements to aid in physical therapy and recovery.
    \item \textbf{Prediction}: Forecasting future movements based on past motion patterns.
    \item \textbf{Generation}: Assessment Creating realistic human movements for use in cinematic productions, gaming environments,
    medical research etc.
\end{itemize}

\begin{figure}
    \centering
    \caption{Example of one sample taken from the HumanAct12 action recognition
    skeleton based dataset~\cite{action2motion-paper} represented as a Multivariate
    Time Series.
    For the sake of visualization we consider only five joints:
    \protect\mycolorbox{0,30,255,0.6}{head}, 
    \protect\mycolorbox{0,0,0,0.3}{left wrist},
    \protect\mycolorbox{255,163,0,0.6}{right wrist},
    \protect\mycolorbox{255,30,0,0.6}{left ankle},
    and \protect\mycolorbox{0,127,0,0.6}{right ankle}, 
    each in a 3D space, resulting in an MTS of $15$ dimensions.}
    \label{fig:example-humanact-to-mts}
    \includegraphics[width=\textwidth]{Figures/chapter_6/intro/skeleton-to-mts.pdf}
\end{figure}

Skeleton-based human motion sequences can be effectively represented as MTS. Typically, 
a skeleton sequence has a shape of $(time, number~of~joints, dimensions~per~joint)$. For instance, with 25 
joints tracked in 3D space ($x$, $y$, $z$ coordinates), each time step is characterized by a $75$-dimensional 
vector. This structure can be transformed into a more conventional MTS format of $(time, 75)$. This transformation enables 
the application of standard time series analysis techniques to the data.
An example of this representation is illustrated in Figure~\ref{fig:example-humanact-to-mts}.

In previous chapters, we have explored various deep learning methodologies for time series analysis. 
This chapter will extend that exploration to the domain of skeleton-based human motion data. We will 
assess the effectiveness of deep learning models in addressing specific tasks related to this type of data.

In this chapter, we will explore several key contributions related to the analysis of skeleton-based human motion data.
\textbf{First}, we will investigate the use of deep learning models to assess the quality of
a patient's movement for rehabilitation exercises.
\textbf{Second}, to mitigate overfitting, we will explore techniques to prototype and extend medical datasets, 
ensuring robust model performance. This will be done through a novel Time Series Prototyping (TSP) approach, notably 
ShapeDBA (ShapeDTW Barycenter Average).
% , the state-of-the-art Elastic Barycenter
% Averaging (EBA) approach.
% , which we will demonstrate as 
% the state-of-the-art through time series clustering, as seen in the literature of prototyping.
\textbf{Third}, the chapter will cover the use of deep generative models to create new, realistic 
motion sequences, expanding the potential applications in action recognition tasks
by proposing a novel CNN-based VAE model.

We will use two publicly available datasets for all our work in this chapter, notably the HumanAct12~\cite{action2motion-paper}
dataset for action recognition and the Kimore~\cite{kimore-paper} dataset for the medical rehabilitation assessment.

\section{Advancing Human Motion Rehabilitation Assessment with LITEMVTime}

In the domain of rehabilitation assessment, accurately evaluating a patient's performance 
during physical exercises is paramount. Traditional methods often rely on subjective 
judgments or handcrafted features, which can be both time-consuming and inconsistent. 
The advent of deep learning models, particularly for time series classification, 
has opened new avenues for enhancing the precision and efficiency of these assessments.

Deep learning techniques, such as Convolutional Neural Networks (CNNs) and Recurrent Neural 
Networks (RNNs), have shown remarkable success in classifying time series data, including 
human motion sequences captured via 3D skeleton tracking~\cite{sensors-activity-recog}. 
These methods leverage the sequential and spatial characteristics of the data, providing a 
more nuanced understanding of the movements.

In this section, we utilize LITEMVTime, the multivariate extension of the previously 
developed LITETime model (Chapter~\ref{chapitre_4}). LITEMVTime has been specifically
designed to address MTS data for classification tasks, such as the task of 
human motion rehabilitation assessment. It is lightweight in terms of 
both the number of parameters and computational requirements, making it highly 
suitable for real-time applications in medical settings.
This efficiency is crucial for deployment in clinical environments, 
where timely and accurate feedback is essential for both patients and 
medical practitioners.

We showcase in this section that LITEMVTime outperforms other architectures on this task.
The model's superior performance is attributed to its innovative architecture, which 
effectively captures the temporal dynamics and spatial configurations of the 
human skeleton during rehabilitation exercises. Unlike conventional models that 
may require extensive computational resources, LITEMVTime's streamlined design 
ensures it can operate on standard medical clinic hardware without compromising on performance.

Furthermore, the model's efficiency ensures that it can be integrated into existing 
clinical workflows without the need for extensive computational infrastructure. 
This integration can enhance the overall quality of care by enabling more frequent 
and detailed assessments, ultimately contributing to better patient 
outcomes~\cite{gait-analysis-wearable,activity-recog-wearable}.

A crucial aspect of deploying machine learning models in medical applications is 
the explainability of their decisions. Medical doctors often pose the question, 
``\emph{Why should I trust you?}''~\cite{lime-paper} when presented with automated assessment results. To 
address this concern, we have integrated Class Activation Maps (CAMs)~\cite{original-cam-paper} into our 
framework. CAMs help in visualizing the regions of the input data that are most 
influential in the model's decision-making process, thereby providing insights 
into which features are most impactful for a given prediction. This is particularly 
important for our CNN-based LITEMVTime model.

\subsection{Experimental Setup \& Dataset Preprocessing}\label{sec:kimore-explain}

For this experiment, we utilized the Kimore dataset~\cite{kimore-paper},
which includes video sequences of 
patients performing rehabilitation exercises, captured and converted into numerical 
3D sequences using Kinect v2 sensors~\cite{kinect-paper}. The dataset comprises recordings 
from both healthy and unhealthy subjects executing five distinct rehabilitation exercises. 
These exercises are the following: (1) lifting of the arms, 
(2) lateral tilt of the trunk with the arms in extension,
(3) trunk rotation, (4) Pelvis rotations on the transverse plane
and (5) Squatting.
An example sequence of each exercise is represented in Figure~\ref{fig:example-kimore}.
The skeletons contain $18$ joints each in a three dimensional space $x$, $y$ and $z$.

\begin{figure}
    \centering
    \caption{Visualization of one sample from the Kimore skeleton based human rehabilitation
    dataset~\cite{kimore-paper}, per exercise.}
    \label{fig:example-kimore}
    \includegraphics[width=\textwidth]{Figures/chapter_6/kimore_cls/example_kimore.pdf}
\end{figure}

Each sequence of patient's movement in every exercise is evaluated by a human expert,
who assigns a quality score ranging from $0$
(poor performance) to $100$ (excellent performance).
The dataset comprises $71$ subjects, with $40$ being healthy and $31$ unhealthy. 
Each subject performs at least five repetitions of each exercise, and all repetitions 
are recorded as individual samples, resulting in $71$ samples per exercise.

\begin{figure}
    \centering
    \caption{
        The distribution of the scores given by experts to
        \protect\mycolorbox{0,30,255,0.6}{healthy}
        and \protect\mycolorbox{255,30,0,0.6}{unhealthy}
        patients when performing each of the five different exercises.
        The \protect\mycolorbox{0,128,0,0.6}{threshold} set to discretize 
        these scores is chosen to be the middle point
        posed at $50$.
    }
    \label{fig:kimore-scores}
    \includegraphics[width=\textwidth]{Figures/chapter_6/kimore_cls/kimore_scores_distribution.pdf}
\end{figure}

Figure~\ref{fig:kimore-scores} illustrates the distribution of
performance scores for each exercise, differentiated 
between healthy and unhealthy subjects. Typically, unhealthy subjects tend to receive 
lower scores, whereas healthy subjects achieve higher scores.
However, this is not always the case, as seen in Figure~\ref{fig:kimore-scores}.
This discrepancy arises because even if some subjects are considered as unheathy
in the dataset, their injury could not limit to perform some exercise
% a subject may be considered unhealthy for one type of 
% exercise but not for another. For instance, if a patient has issues with their leg, 
% they will be considered unhealthy for exercises that require leg movement but may be 
% considered healthy for exercises that do not involve leg movement.
Despite the regression 
nature of the dataset, we reframed the task to evaluate
the performance of subjects irrespective of their health status. The evaluation criteria 
are defined as follows:

\begin{itemize}
    \item Scores below $50$ indicate a poorly performed exercise.
    \item Scores above $50$ indicate a well-performed exercise.
\end{itemize}

The dataset features sequences of varying lengths, which necessitated resampling all 
sequences to a common length, determined to be 748 frames (the average length) using
the Fourier resampling method in \textit{scipy} Python package~\cite{scipy-paper}.
This is due to the fact that skeleton-based motion sequences extracted using 
kinect cameras are sampled using a uniform sample rate, which is a condition 
to be able to use this resampling approach.
% Each sequence contains 
% data on 18 human joints in 3D space. The dataset includes 71 examples per exercise, 
% with each example representing a single subject's recording. 
We split the dataset 
into an $80\%-20\%$ train-test set, ensuring stratification to maintain a balanced 
representation of good and bad performances in both sets.

Each 3D human motion sequence is transformed to an MTS and each exercise is utilized 
as an independent dataset, given that one score value does not represent the same 
thing from one exercise to another.
The dataset were z-normalized prior to training and testing independently on each channel.
% Our experiments were conducted using a GTX 4090 GPU with 24GB of VRAM.
The best-performing model during training, determined by monitoring the training
loss, was selected for testing. The Adam optimizer with a Reduce on Plateau learning
rate decay method was employed, using TensorFlow's~\cite{tensorflow-paper} default
parameter settings.
The same parameters are used as presented in Chapter~\ref{chapitre_4} for LITEMV,
as well as its ensemble version LITEMVTime.

\subsection{Competitor Models}

We evaluated the performance of various deep learning models on this dataset, 
including Fully Convolutional Networks (FCN), ResNet, and InceptionTime, 
along with our proposed model, LITEMVTime. Additionally, we included a baseline classifier, 
1-Nearest Neighbor Dynamic Time Warping (1-NN-DTW) following~\cite{bakeoff-tsc-2}.
For all the competitors we utilize the same parameter setup used in~\cite{dl4tsc}.

\subsection{Experimental Results}

\begin{table}
    \caption{
    Accuracy of the baseline, 1-NN-DTW, three state-of-the-art deep learning models, 
    FCN ResNet and InceptionTime compared to \textbf{our} LITEMVTime on the Kimore 
    human rehabilitation exercise.
    We present for each of the five exercises the accuracy of the models on the test unseen split.
    The performance of the winning model for each exercise is shown in \textbf{bold}
    and the second best is shown in \underline{underline}.}
    \label{tab:kimore-cls-results}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|ccccc}
    Kimore Exercise & \textbf{1-NN-DTW} & \textbf{FCN} & \textbf{ResNet} & \textbf{InceptionTime} & \textbf{LITEMVTime} \\ \hline
    \textbf{Exercise 1}       & 60.00          & 84.00          & {\ul 85.33} & 78.67          & \textbf{86.67} \\
    \textbf{Exercise 2}       & 46.67          & 72.00          & 69.33       & {\ul 78.67}    & \textbf{80.00} \\
    \textbf{Exercise 3}       & 86.67          & \textbf{92.00} & 86.67       & {\ul 88.00}    & 86.67          \\
    \textbf{Exercise 4}       & \textbf{66.67} & {\ul 65.33}    & 60.00       & 57.33          & \textbf{66.67} \\
    \textbf{Exercise 5}       & 73.33          & 66.67          & {\ul 81.33} & \textbf{84.00} & 80.00          \\ \hline
    \textbf{Average Accuracy} & 66.67          & 76             & 76.53       & {\ul 77.33}    & \textbf{80.00} \\ \hline
    \textbf{Average Rank}     & 3.6            & 2.8            & 2.8         & {\ul 2.6}      & \textbf{1.8} 
    \end{tabular}
    }
\end{table}

The results, summarized in Table~\ref{tab:kimore-cls-results},
indicate that LITEMVTime outperforms the other models 
in both average performance and average rank across all exercises. This demonstrates 
that LITEMVTime, despite its small size, is highly effective in classifying the 
quality of exercise performance based on recorded sequences. Consequently, 
LITEMVTime proves to be a valuable tool for assessing patient rehabilitation 
exercises, providing reliable classifications that can support clinical decision-making.

\subsection{Enhancing Trust and Transparency in Human Rehabilitation Assessment With LITEMV}

In the field of rehabilitation assessment, achieving high performance with deep learning 
models is essential. However, the ability to understand the decision-making process of 
these models is equally important, especially in medical applications where the stakes 
are high. Over the past decade, there has been a significant focus on model interpretability, 
particularly in Time Series Classification (TSC) over the last five
years~\cite{explainability-tsc}.

Class Activation Maps (CAM) are a powerful technique for interpreting the decisions of 
deep Convolutional Neural Networks (CNNs), which are often perceived as black-box models. 
Initially introduced by~\cite{original-cam-paper} for image data, CAMs have since been adapted 
for time series data, providing a way to visualize which parts of the input data 
contribute most to the model's decisions.
This technique got first adapted to time series classification in~\cite{fcn-resnet-mlp-paper}.

CAMs require a global representative layer before the softmax classification layer, 
such as Global Average Pooling (GAP). This setup is used in various architectures 
including FCN, ResNet, Inception, LITE, and LITEMVTime. In the context of TSC, the 
output of a CAM is a univariate time series where each timestamp indicates the importance 
of that specific point in the input series for the model's decision.

Mathematically, CAM is defined as follows:

\begin{itemize}
    \item Let $\textbf{O}(t)=\{\textbf{o}^1(t),\textbf{o}^2(t),\ldots,\textbf{o}^M(t)\}$
    represent the output of the last convolutional layer, 
    an $MTS$ with $M$ variables (the number of filters). 
    Thus, $\textbf{o}^m(t)$ is the output univariate time series of filter $m~\in~[1,M]$.
    \item Let $\textbf{w}^{c}=\{w_{1}^{c},w_{2}^{c},\ldots,w_{M}^{c}\}$
    be the weight vector connecting the GAP output to the neuron of the
    winning class $c$ (the class with the highest probability value).
\end{itemize}

The CAM output is then:
\begin{equation}\label{equ:CAM}
    CAM(t) = \sum_{m=1}^M~w_m^c.\textbf{o}^m(t)
\end{equation}

This output is normalized using min-max normalization. For two given timestamps, 
the one with the highest CAM score has contributed more significantly to the 
decision of the black-box model.

\begin{figure}
    \centering
    \caption{
        Explainability of the LITEMV model using the Class Activation Map (CAM) on the 
        feature of the last DWSC layer.
        The colorbar values represent the normalized (between $0$ and $1$) 
        scores of the CAM.
        Five samples each from one of the five exercises are presented with 
        the CAM scores on different time stamps.
        A higher CAM score indicates the importance of a time stamp for 
        the decision making of LITEMV.
    }
    \label{fig:kimore-cam-examples}
    \includegraphics[width=\textwidth]{Figures/chapter_6/kimore_cls/frames_kimore_example_with_cam.pdf}
\end{figure}

In our study, we apply CAM to the LITEMVTime model to interpret its decisions 
on the Kimore dataset, which includes human rehabilitation exercises. 
We analyze five different examples from each exercise and generate CAM 
outputs using a LITEMVTime model trained for each exercise 
classification task presented in Figure~\ref{fig:kimore-cam-examples}.

\begin{figure}
    \centering
    \caption{
        Explainability of the LITEMV model using the Class Activation Map (CAM) 
        on the feature of the last DWSC layer.
        Two samples from the test split of the same exercise are presented, the first 
        (top) having a ground truth of class 1, and the second (bottom) having a ground 
        truth of class 0.
        LITEMV correctly classifies the first sample but incorrectly the second.
        It can be seen that the important time stamp in the case of the correctly classified 
        sample has higher color intensity, so higher CAM score, compared to the same time stamp 
        from the incorrectly classified sample.
    }
    \label{fig:kimore-cam-diff}
    \includegraphics[width=\textwidth]{Figures/chapter_6/kimore_cls/frames_kimore_example_with_cam_diff.pdf}
\end{figure}

Given that human skeleton data forms a multivariate time series, the CAM values 
represent the temporal axis, with each timestamp's CAM score indicating the 
contribution of that particular pose to the classification. To further explore 
the variability in CAM scores, we compare two CAM explanations for two samples 
of the same exercise in Figure~\ref{fig:kimore-cam-diff}:
one correctly classified as class 1 ($score > 50$) and 
another incorrectly classified as class 1 when it should be class 0 ($score < 50$). 
The higher intensity of CAM colors for the correctly classified sample indicates 
higher contribution of important timestamps, while the misclassified sample shows 
lower scores, reflecting the influence of the incorrect class weights.

By utilizing CAM, we can provide clear explanations for the LITEMVTime model's 
decisions, demonstrating the specific data points that influenced its classifications. 
This transparency is crucial for integrating deep learning models into clinical 
workflows, ensuring that healthcare professionals can rely on these tools with confidence.

\section{Extending Human Motion Rehabilitation Data With Time Series Prototyping}

Human motion rehabilitation data is inherently sensitive and challenging to acquire, particularly 
due to privacy concerns and the complex nature of medical data. This scarcity often results in 
limited datasets that can lead to overfitting in machine learning models used for patient assessment, 
whether for classification or regression tasks. To mitigate this issue, we propose generating synthetic 
data using time series prototyping techniques~\cite{dba-paper}.
% Publicly available datasets are rare, and those that exist are often small and not
% sufficiently diverse to train robust models. Overfitting becomes a significant risk,
% where models perform well on training data but fail to generalize to new, unseen data.
% To address this problem, we propose the use of synthetic data generation through time 
% series prototyping.
By creating synthetic data, we can extend existing datasets, making models 
more resilient to overfitting and improving their generalization.

One effective approach for generating synthetic data is through time series prototyping. 
We introduce an innovative approach called ShapeDBA (Shape Dynamic Time Warping Barycenter Average)
for time series prototyping (Chapter~\ref{chapitre_1} Section~\ref{sec:ts-prototyping}).
Following its application in standard prototyping, we leverage ShapeDBA 
for advanced weighted prototyping. This method builds upon the sophisticated principles of weighted 
elastic averages as delineated by \cite{weighted-dba-paper}. This method involves creating prototypes 
that capture the essential characteristics of a set of time series data, which can then be 
used to generate new, synthetic sequences.

In the following sections, we will present the ShapeDBA method in detail and provide extensive 
experimental results to demonstrate that it is now the state-of-the-art prototyping method
for time series data. Subsequently, we will introduce the weighted ShapeDBA setup tailored for 
regression tasks, specifically focusing on human rehabilitation assessment using the Kimore dataset~\cite{kimore-paper}.
We will extend the Kimore dataset with synthetic data generated by the ShapeDBA method and demonstrate 
that this augmented dataset significantly enhances the performance of deep supervised regression models 
compared to using the original dataset alone.

\subsection{Generating Effective Time Series Prototypes With ShapeDBA}

Prototyping time series data is a critical task in various domains, including medical diagnostics, 
human motion analysis, and satellite imagery interpretation. Traditional methods for generating time 
series prototypes often fall short in preserving the inherent patterns and nuances of the data, leading 
to out-of-distribution artifacts. This discrepancy is primarily due to the reliance on conventional 
DTW measure~\cite{dba-paper}, which emphasize absolute similarities over neighborhood similarities. 
These artifacts can significantly impact the accuracy and reliability of subsequent analyses, such as 
clustering~\cite{elastic-clustering-review}, classification~\cite{neares-centroid-dba-paper}
or explainability~\cite{prototyping-explainability}.
Therefore, there is a pressing need for a more robust and representative 
method to generate prototypes that faithfully capture the underlying data distribution, ensuring more 
accurate and meaningful insights from time series analysis.

\subsubsection{ShapeDBA Methodology}

Our proposed prototyping method, ShapeDBA (Shape Dynamic Time Warping Barycenter
Averaging), is an advanced method 
designed to generate more accurate and representative time series prototypes. 
The key innovation in ShapeDBA is the integration of the ShapeDTW~\cite{shape-dtw-distance}
(Chapter~\ref{chapitre_1} Section~\ref{sec:tsc-distance})
measure, which considers the structural similarities within the neighborhoods of 
time series data points, thus overcoming the limitations of traditional DTW methods.

The ShapeDBA algorithm follows these steps:

\begin{enumerate}
    \item \textbf{Initialization}: Start with an initial average time series, which can be 
    randomly selected from the dataset.
    \item \textbf{Alignment}: For each time stamp in the average time series, find the aligned 
    points (using ShapeDTW measure) in all the time series samples.
    This involves creating a set of associated 
    time stamps $assoc_t$ for each time stamp $t$ in the average series.
    \item \textbf{Averaging}: Calculate the barycenter for each time stamp $t$ by averaging 
    all the aligned points in $assoc_t$. The barycenter is computed as:
    \begin{equation}\label{equ:shape-dba-barycenter}
        ShapeDBA_barycenter(assoc_t) = \dfrac{1}{|assoc_t|}\sum_{i=1}^{|assoc_t|}assoc_t^i
    \end{equation}
    \item \textbf{Iteration}: Repeat steps 2 and 3 until convergence, i.e.
    until the changes in the average time series are minimal.
\end{enumerate}

\subsubsection{Reach Value Control}

The ``reach'' hyperparameter in ShapeDTW defines the neighborhood size around each 
time stamp for alignment purposes. By adjusting this parameter, ShapeDTW can 
emulate different similarity measures. When the reach is set to 1, ShapeDTW 
operates like traditional DTW, focusing solely on individual time stamps. 
However, when the reach is set to a very large value, it behaves similarly 
to the Euclidean distance, as the neighborhood extends across the entire 
time series. This flexibility allows ShapeDTW, and consequently
ShapeDBA, to balance between local and 
global alignment, adapting to the specific requirements of the data.

\subsubsection{Qualitative Evaluation}

\begin{figure}
\centering
\caption{A qualitative evaluation of the proposed average technique compared to
other approaches on a GunPoint dataset. The ShapeDBA algorithm is the only
approach to not generate out-of-distribution artifacts.}
\label{fig:shape-dba-averaging-compare}
\includegraphics[width=\textwidth]{Figures/chapter_6/shapedba/averages_comparison.pdf}
\end{figure}

In the literature, the artimetic mean and two notable TSP approaches with elastic measures,
DBA~\cite{dba-paper} and SoftDBA~\cite{soft-dtw-distance}.
Figure~\ref{fig:shape-dba-averaging-compare}
compares prototype calculations using these methods and our ShapeDBA 
on the GunPoint dataset of the UCR archive~\cite{ucr-archive}.
The figure reveals that the arithmetic mean is 
unsuitable for temporal data, particularly when samples are shifted. 
It introduces spatial and temporal artifacts; the temporal placement of the prototype 
skews toward the most frequent occurrence, and the amplitude values become out-of-distribution 
due to averaging misaligned values. DBA and SoftDBA improve temporal alignment but still 
produce peak artifacts because of the rigid point-to-point alignment of DTW and SoftDTW, 
which do not account for amplitude differences. ShapeDBA, however, offers the best 
of both worlds. It leverages DTW's temporal alignment and ShapeDTW's neighborhood 
alignment~\cite{shape-dtw-distance}, avoiding point-to-point issues.
Consequently, the prototype generated by 
ShapeDBA in Figure~\ref{fig:shape-dba-averaging-compare} is free from peak 
out-of-distribution artifacts.

\subsubsection{Quantitative Evaluation}

To quantitatively evaluate the effectiveness of ShapeDBA in time series prototyping, 
we coupled ShapeDBA and ShapeDTW as the averaging method and distance measure in 
the $k$-means algorithm for clustering. Clustering, particularly using the $k$-means 
algorithm, serves as a robust evaluation metric for TSP methods based on 
elastic similarity measures~\cite{elastic-clustering-review}.

We compared ShapeDBA against four distance-based methods: (1) $k$-means with 
the default setup (arithmetic mean and Euclidean Distance), referred to as 
MED; (2) $k$-means with SoftDBA and SoftDTW; (3) $k$-means with DBA and DTW; and (4) $k$-shape.
Given that all other methods iteratively find prototypes, we applied the same iterative 
approach to the MED method. Instead of using a simple arithmetic mean of all samples, 
we iteratively calculated the mean over aligned points for each time stamp in the 
prototype, similar to DBA. However, for MED, we assumed ideal alignment without any warping.

\paragraph{Experimental Setup}

We conducted our experiments on 123 datasets from the UCR archive~\cite{ucr-archive}. Out of 
the 128 available datasets since 2018, five were excluded due to their 
high time series length, which would have been computationally prohibitive 
given the quadratic time complexity of the considered algorithms. All samples in each dataset 
were z-normalized to ensure a zero mean and unit standard deviation. The clustering 
algorithms were trained on the combined train-test splits of these 123 datasets. 
While some UCR datasets are merely different train-test splits of the same original dataset, 
this occurs infrequently, so the same data might be clustered multiple times.
% For 
% transparency and reproducibility, the source code used in this study is publicly accessible here:
% \url{https://github.com/MSD-IRIMAS/ShapeDBA}.
We set the value of the ``reach'' hyperparameter to $15$ resulting in a sliding
window size of $31$, following the original work of ShapeDTW~\cite{shape-dtw-distance}.

In machine learning, non-deterministic estimators often suffer from performance biases 
related to their initial setup, such as the initialization of weights in deep learning 
models. This bias is particularly relevant in clustering tasks, where the starting 
positions of clusters can significantly influence the results. To address this in 
our experiments, we ran each clustering algorithm five times, each with different 
initial cluster configurations, and averaged the results. However, using different 
initial clusters for each method could introduce another layer of bias. To ensure a 
fair comparison, we used the same set of five initial clusters across all clustering 
algorithms for each dataset. This approach eliminated variability due to initial cluster 
selection and allowed us to present unbiased average performance metrics, accurately 
reflecting the effectiveness of each clustering method.

\paragraph{Evaluation Metric: Adjusted Rand Index ($ARI$)}

The Adjusted Rand Index ($ARI$)~\cite{ari-paper} is an enhanced version of the Rand Index ($RI$), 
addressing the limitations of the original metric. The $RI$ 
measures the similarity between true labels $\textbf{y}$ and predicted labels $\hat{\textbf{y}}$
from a clustering algorithm using the formula:

\begin{equation}\label{equ:ri}
    RI(\textbf{y},\hat{\textbf{y}}) = \dfrac{TP+TN}{TP+FP+FN+TN}
\end{equation}

Here, $TP$ (True Positive) and $TN$ (True Negative) denote correctly clustered pairs, 
while $FP$ (False Positive) and $FN$ (False Negative) denote incorrectly clustered pairs. 
The $RI$ calculates the proportion of pairwise agreements between the true and predicted clusters.
However, the $RI$ can be misleading because it may indicate high similarity for 
clusters that are randomly generated, particularly when the number of clusters is large. 
This occurs because the expected $RI$ value varies between random clusters.

To overcome this, the $ARI$ adjusts the $RI$ to account for chance, normalizing the score so 
that random clustering yields an $ARI$ of $0.0$. The $ARI$ is defined as:

\begin{equation}\label{equ:ari}
    ARI(\textbf{y},\hat{\textbf{y}}) = \dfrac{RI(\textbf{y},\hat{\textbf{y}}) - E[RI]}{1.0 - E[RI]}
\end{equation}

where $E[RI]$ is the expected value of the $RI$ for random clustering. The $ARI$ ranges from
$-0.5$ (indicating no similarity) to $1.0$ (indicating perfect agreement),
providing a more reliable measure of clustering performance by correcting for random chance.

\paragraph{Implementation Efficiency}

The ShapeDTW algorithm modifies the original DTW similarity measure by transforming 
the input time series into a multivariate format. In the univariate case with the 
``identity'' descriptor, each time stamp's neighborhood is converted into a Euclidean 
vector, creating a multivariate time series. Applying DTW to this transformed series 
involves computing the Euclidean distance between the channel vectors of paired time 
stamps, which can lead to computational inefficiency when sliding the reach window, 
as depicted in Figure~\ref{fig:shapedtw-recompute}.
This inefficiency is specific to the identity transformation.

\begin{figure}
    \centering
    \caption{Calculation of the ShapeDTW measure between two
    \protect\mycolorbox{255,128,255,0.6}{time} \protect\mycolorbox{0,30,255,0.6}{series}.
    The \protect\mycolorbox{135,86,51,0.49}{overlapping area} between the 
    \protect\mycolorbox{194,224,194,0.84}{two} 
    \protect\mycolorbox{255,165,0,0.2}{sliding windows} is recomputed.}
    \label{fig:shapedtw-recompute}
    \includegraphics[width=\textwidth]{Figures/chapter_6/shapedba/re-computed_EDs.pdf}
\end{figure}

To optimize this process, we first calculate the Euclidean pairwise distances 
between the two time series, resulting in a distance matrix. This matrix is 
then padded with edge values equal to half the reach. A window, with dimensions 
matching the length of the time series, slides diagonally across the distance matrix. 
The results are accumulated into a zero-initialized matrix. The DTW algorithm is 
subsequently applied to this new matrix, thereby avoiding unnecessary computations 
and improving efficiency. Figure~\ref{fig:shape-dtw-efficient}
illustrates this streamlined implementation of ShapeDTW.

\begin{figure}
    \centering
    \caption{A more efficient implementation of the ShapeDTW measure with the 
    identity descriptor involves sliding a window over the time stamp pairwise Euclidean
    matrix between the two time series, instead of applying DTW directly 
    on their multivariate transformation. The data from each window position 
    is collected into a zero-initialized matrix, which is then processed using 
    the DTW algorithm, significantly reducing computational overhead.}
    \label{fig:shape-dtw-efficient}
    \includegraphics[width=\textwidth]{Figures/chapter_6/shapedba/sliding_window.pdf}
\end{figure}

\paragraph{Experimental Results}

\begin{figure}
    \centering
    \caption{An MCM comparing ShapeDBA to other averaging approaches, coupled with $k$-means and 
    their associated similarity measure, and $k$-shape, on the ARI metric.}
    \label{fig:shapedba-mcm-ari}
    \includegraphics[width=\textwidth]{Figures/chapter_6/shapedba/mcm-ari.pdf}
\end{figure}

In Figure~\ref{fig:shapedba-mcm-ari} we present the MCM (Chapter~\ref{chapitre_2}) between ShapeDBA, $k$-shape
and its competitors when coupled with $k$-means following the ARI metric on $123$ datasets of 
the UCR archive~\cite{ucr-archive}.
The MCM showcases that our proposed ShapeDBA outperforms MED and the original DBA
work~\cite{dba-paper},
significantly, as well as significantly outperforming $k$-shape, the fastest TSCL algorithm
in the literature. The winning approach in terms of average performance ranking is
SoftDBA~\cite{soft-dtw-distance}.
However, comparing ShapeDBA and SoftDBA showcases that no conclusion can be made between 
the performance of both algorithms given the high p-value.
We show however in Section~\ref{sec:shapedba-runtime} that ShapeDBA is way faster than
SoftDBA.

\begin{figure}
    \centering
    \subfloat[\centering \label{fig:shapedba-1v1-med} 1V1 with Mean Euclidean Distance]{\includegraphics[width=0.4\textwidth]{Figures/chapter_6/shapedba/ShapeDBAShapeDTW-vs-MED.pdf}}
    \subfloat[\centering \label{fig:shapedba-1v1-dbadtw} 1V1 with DBA using DTW as a metric]{\includegraphics[width=0.4\textwidth]{Figures/chapter_6/shapedba/ShapeDBAShapeDTW-vs-DBADTW.pdf}}\\
    
    \subfloat[\centering \label{fig:shapedba-1v1-kshape} 1V1 with $k$-shape]{\includegraphics[width=0.4\textwidth]{Figures/chapter_6/shapedba/ShapeDBAShapeDTW-vs-KShape.pdf}}
    \subfloat[\centering \label{fig:shapedba-1v1-softdbasoftdtw} 1V1 with SoftDBA using SoftDTW as a metric]{\includegraphics[width=0.4\textwidth]{Figures/chapter_6/shapedba/ShapeDBAShapeDTW-vs-SoftDBASoftDTW.pdf}}
    \caption{1v1 Comparison between using $k$-means with ShapeDBA-ShapeDTW and other
    approaches from the literature using the Adjusted Rand Index clustering metric.}
    \label{fig:shapedba-1v1}
\end{figure}

\begin{figure}
    \centering
    \caption{The ECGFiveDays dataset from the UCR archive provides
    \protect\mycolorbox{0,128,0,0.6}{two} \protect\mycolorbox{255,30,0,0.6}{examples} 
    from each class. In this dataset, the majority of the time stamps are 
    \protect\mycolorbox{255,106,55,0.6}{noisy}, 
    with the critical information localized in the 
    \protect\mycolorbox{0,30,255,0.6}{central section of the time series}.
    ShapeDBA does not perform well on this dataset, with an ARI score of almost $0.042$.}
    \label{fig:shape-dba-ecgfivedays}
    \includegraphics[width=\textwidth]{Figures/chapter_6/shapedba/ECGFiveDays-example.pdf}
\end{figure}

A detailed 1v1 scatter plot of the comparisons between ShapeDBA and the three other comparates
is presented in Figure~\ref{fig:shapedba-1v1}.
Certain outliers in the One-vs-One scatter plots distinctly favor either ShapeDBA 
or the other methods. For example, ShapeDBA shows lower performance (low $ARI$) 
compared to $k$-shape on the ShapeletSim and ECGFiveDays datasets. The ShapeletSim 
dataset, being a simulation of random data, does not provide meaningful conclusions. 
However, the ECGFiveDays dataset, as shown in Figure~\ref{fig:shape-dba-ecgfivedays},
uniquely illustrates a  limitation of ShapeDBA.
The ECGFiveDays dataset consists mainly of noisy time stamps, with critical 
information compressed into the middle segments of the time series, as depicted 
in Figure~\ref{fig:shape-dba-ecgfivedays}.
This noise introduces challenges during the optimization steps of 
ShapeDTW. On the other hand, ShapeDBA significantly outperforms $k$-shape on the 
SonyAIBORobotSurface1 dataset, with an ARI difference of nearly $0.6$. However, 
it is important to note that this might reflect $k$-shape's underperformance since 
MED, DBA, and SoftDBA also show better results on this dataset.
When comparing ShapeDBA to DBA, ShapeDBA exhibits a distinct advantage on the 
DiatomSizeReduction dataset, which struggles with having only four samples per 
class label. This highlights ShapeDBA's effectiveness in handling datasets with 
sparse training data.

\paragraph{Computational Runtime}\label{sec:shapedba-runtime}

\begin{figure}
    \centering
    \caption{An MCM (Chapter~\ref{chapitre_2}) showing ShapeDBA's (ours) duration
    (in seconds) compared to other approaches to finalize the clustering task on $123$
    datasets of the UCR archive~\cite{ucr-archive}.}
    \label{fig:shape-dba-mcm-time}
    \includegraphics[width=\textwidth]{Figures/chapter_6/shapedba/mcm-time.pdf}
\end{figure}

All experiments were conducted on the same machine and under identical conditions, 
ensuring a fair comparison of computational times. We recorded the total computation 
time for each clustering method, averaging the results over five initializations. This 
approach allowed us to apply the same comparison techniques as for the ARI.
In Figure~\ref{fig:shape-dba-mcm-time}, 
the MCM from Chapter~\ref{chapitre_2} illustrates the computational runtime comparison between ShapeDBA and the 
other comparates. To maintain 
consistency, we inverted the values (multiplying by -1) since lower times are preferable. 
The MCM reveals that $k$-shape is the fastest method, primarily due to its use of the Fast 
Fourier Transform (FFT) for cross-correlation, while SoftDBA is the slowest because of its 
computational gradient based optimization step.
Across 123 datasets, ShapeDBA is on average 1.7 times faster than SoftDBA, 
with 109 wins in terms of computational runtime.
Given that no definitive conclusion can be drawn regarding the performance difference 
between ShapeDBA and SoftDBA, and considering that ShapeDBA is significantly faster, 
these extensive experiments highlight ShapeDBA as the more suitable state-of-the-art 
method for the task of TSP.

In the following section, we propose using ShapeDBA in a weighted setup to generate 
new synthetic samples. In this approach, weights determine the amount of information 
drawn from each sample. This method will be applied to human motion data, specifically 
in the medical field of human rehabilitation, with the goal of extending a regression dataset.

\subsection{Weighted Average of Human Motion Sequences
for Improving Rehabilitation Assessment}

The collection and annotation of rehabilitation sequences~\cite{kimore-paper}
are complex, time-consuming, 
and require clinical expertise, which limits the size of available datasets. 
Conventional data augmentation methods for human motion data, simply adding noise,~\cite{human-motion-data-aug-noise},
although beneficial in other areas, 
tend to produce unrealistic motion sequences that do not capture the intricate 
temporal dynamics of human movement. This inadequacy necessitates innovative 
approaches to data generation that can create meaningful and representative synthetic sequences.

Another approach to generating synthetic sequences is the use of deep generative
models~\cite{actor-paper}, 
which have shown success in creating realistic sequences for human motion tasks. However, 
these models face a significant challenge: the lack of sufficient data to train a deep 
supervised model also means inadequate data for training a deep generative model. 
To address this issue, researchers in the time series domain often turn to prototyping 
techniques~\cite{weighted-dba-paper}.
These techniques create representative average sequences from existing 
training samples, offering a practical solution for augmenting datasets when data 
availability is limited.

Incorporating our previously proposed prototyping method, we propose to utilize
a weighted version of ShapeDBA
presented in the previous section, tailored to multivariate time series representing 
rehabilitation motions. By incorporating weights, this method ensures that 
the generated synthetic sequences maintain the essential characteristics of 
the original data, enhancing the realism and variability of the dataset. Our 
approach not only compensates for the limited data but also improves the 
generalization capability of models trained for rehabilitation assessment. 
This study utilizes the Kimore regression dataset~\cite{kimore-paper}
to validate our method, highlighting 
its effectiveness in producing coherent synthetic data that can significantly 
aid in the evaluation and personalization of rehabilitation treatments.

Moreover, we employ a weighted version of the ShapeDBA approach to generate 
diverse synthetic average sequences, subsequently used to enhance the training phase 
of deep learning models for downstream tasks. Due to the time-consuming nature of this 
averaging method, it cannot be computed at each epoch of the training phase, 
as is common in data augmentation. Instead, we generate several average 
sequences beforehand to expand the original training dataset, a process 
we refer to as data extension to distinguish it from traditional data augmentation.

Finaly, we address the challenge of rehabilitation assessment, which is 
often approached as an extrinsic regression problem. The goal is to predict a 
continuous performance score for each rehabilitation sequence. Current state-of-the-art 
methods primarily use data augmentation or data extension for classification tasks, 
where synthetic samples are given discrete labels matching the original samples. 
However, assigning continuous labels to synthetic data presents a more complex problem. 
To overcome this, we propose a novel approach that employs a weighting strategy to 
calculate weighted continuous labels based on the true labels of a set of samples. 
This method enhances the realism and accuracy of the generated data, as detailed in
Figure~\ref{fig:w-sdba-summary}, for which we detail in the following section.

\subsubsection{Methodology}

\begin{figure}
    \centering
    \caption{
        We determine the $N$ nearest neighbors for each reference in 
        the dataset using Dynamic Time Warping (DTW). Each neighbor 
        is assigned a weight according to its DTW distance from the 
        reference. These weights are then utilized to compute a weighted 
        average sequence through ShapeDBA. Subsequently, a weighted score 
        is calculated, allowing us to expand the regression training dataset 
        with this new synthetic sample.
    }
    \label{fig:w-sdba-summary}
    \includegraphics[width=\textwidth]{Figures/chapter_6/w-sdba/summary.pdf}
\end{figure}

Since rehabilitation motion sequences can be sparse, using the original ShapeDBA 
method to compute an average may lead to meaningless or incoherent results. To 
better capture the distribution of these sequences, a weighted average is more 
suitable~\cite{weighted-dba-paper}.

For each reference motion sequence, we generate a synthetic version by considering 
a neighborhood of $N$ motion sequences. The reference sequence $S_{ref}$ is given 
a weight of $1$, while each neighboring sequence $S_i$ is weighted based on its 
similarity to the reference. The weight $w_i$ for each neighbor is calculated as follows:

\begin{equation}\label{equ:w-sdba-weights}
    w_i = e^{ln(0.5).\dfrac{DTW(S_i,S_{ref})}{d_{NN}}}
\end{equation}
\noindent where $d_{NN}$ is the DTW distance between $S_{ref}$ and its nearest neighbor.
This weighting emphasizes the influence of sequences that are more similar to the reference.

\begin{figure}
    \centering
    \caption{
        Averaging six human motion sequences
        (\protect\mycolorbox{0,125,0,0.6}{Examples $0$ to $5$} shown on the top left/right) 
        with uniformly distributed weights results in an unrealistic example (bottom right 
        skeleton sequence). The 2D t-SNE~\cite{t-sne-paper}
        projection of these sequences (bottom left) 
        illustrates that the \protect\mycolorbox{255,30,0,0.6}{averaged sequence} 
        falls outside the \protect\mycolorbox{0,30,255,0.6}{manifold} of 
        the \protect\mycolorbox{0,125,0,0.6}{original sequences}.
    }
    \label{fig:w-sdba-sparse}
    \includegraphics[width=\textwidth]{Figures/chapter_6/w-sdba/why-we-need-weighted-avg.pdf}
\end{figure}

Figure~\ref{fig:w-sdba-sparse} demonstrates the significance of
using a weighted average in cases where 
the data manifold is sparse and the distribution is non-spherical.

For each reference sequence $S_{ref}$, the weighted ShapeDBA computation produces 
a corresponding synthetic sequence $\hat{S}$. To assign a continuous label (score) 
to the synthetic sequence, the same weights are normalized using $\min-\max$ normalization, 
ensuring the labels remain within the original range of $0$ to $1$. The continuous label 
$\hat{y}$ for the synthetic sequence $\hat{S}$ is calculated as:

\begin{equation}\label{equ:w-sdba-weighted-score}
    \hat{y} = \sum_{i=1}^{N+1} \bar{w}_i.y_i
\end{equation}
\noindent where $\bar{w}_i$ is the normalized weight of the $i_{th}$ sequence in the set 
of the $N+1$ samples (including the reference and its $N$ nearest neighbor sequences).

\subsubsection{Experimental Evaluation}

\paragraph{Experimental Setup}

We conducted a two-fold evaluation of our proposed approach. First, we examined the 
coherence of the synthetic sequences through both qualitative and quantitative 
analyses. Second, we investigated the utility of these synthetic sequences in 
improving a deep learning model's performance in extrinsic regression, aiming 
to predict continuous scores for rehabilitation sequences.

\paragraph*{Dataset}

We utilize the Kimore dataset~\cite{kimore-paper}, for which the pre-processing of the sequences 
are presented in Section~\ref{sec:kimore-explain}, however we use the regression task here.
We implemented a 5-fold cross-validation protocol with a unique adaptation to more 
accurately reflect real-world scenarios by including only unhealthy subjects in 
the test phase. Unlike the standard approach, we divided the sequences of unhealthy 
subjects into 5 folds. For each iteration, all sequences from healthy subjects and 4 
folds of unhealthy subjects were used for training, while the remaining fold of unhealthy 
subjects was reserved for testing. This method ensures that the evaluation is focused 
on the performance for unhealthy subjects, providing a more realistic assessment. 

\paragraph*{Comparative Sets of Rehabilitation Sequences}

Our comparative study involves several sets of rehabilitation sequences. The reference set, 
consisting of $D$ sequences, includes the original sequences from the Kimore dataset. 
As a baseline, we introduced random noise $\mathcal{N}(0,0.1)$ to each reference sequence, 
creating a noisy set of the same size. Beyond this, we generated five additional 
sets of synthetic sequences using our proposed weighted ShapeDBA method. For each 
reference sequence, we created synthetic versions by applying weighted ShapeDBA 
with neighborhood sizes ranging from $N = 1$ to $N = 5$. This process resulted in five 
distinct sets, each containing $D$ sequences, which we labeled as $ShapeDBA NN1$, 
$ShapeDBA NN2$, $ShapeDBA NN3$, $ShapeDBA NN4$, and $ShapeDBA NN5$.

\paragraph{Evaluation of Synthetic Data Coherence}\label{sec:evaluation-wsdba}

Evaluating generative models typically requires multiple methods. For human motion data, 
one way is to visually inspect the realism of the generated sequences. However, 
visual inspection alone is not enough, as it lacks quantitative objectivity,
as argued in~\cite{reliable-fidelity-diversity}. Thus, 
it is crucial to use specific metrics to assess the reliability of the generated 
samples. In this section, we introduce a comprehensive evaluation strategy that 
includes both visual and numerical assessments to ensure a thorough analysis of 
the generated sequences.

\paragraph*{Qualitative Analysis: Visualizing Real vs Generated}

\begin{figure}
    \centering
    \caption{
        Visualization of three examples: the top row shows a real sample, the 
        middle row displays a noisy sample, and the bottom row features a 
        sample generated using the weighted ShapeDBA method. Each sequence 
        is represented by 10 frames, arranged sequentially from left to right.
    }
    \label{fig:w-sdba-vs-noisy}
    \includegraphics[width=\textwidth]{Figures/chapter_6/w-sdba/seeing-generation.pdf}
\end{figure}

Figure~\ref{fig:w-sdba-vs-noisy} showcases three sequences:
the first is an original sequence from the Kimore 
dataset, the second is created by adding random noise to this original sequence, 
and the third is generated using our weighted ShapeDBA method. 
Visually, the noisy sequence appears less realistic compared to the sequence produced 
by the weighted ShapeDBA approach.

\begin{figure}
    \centering
    \caption{
        Visualization of two real sequences (top and bottom) and their corresponding 
        weighted ShapeDBA sequence (middle). The generated average sequence preserves 
        the temporal alignment of the top sequence, which has a higher weight, while 
        also incorporating features like the patient's height from the bottom sequence.
    }
    \label{fig:w-sdba-generation}
    \includegraphics[width=\textwidth]{Figures/chapter_6/w-sdba/average-with-real.pdf}
\end{figure}

To illustrate the effectiveness of weighted ShapeDBA, Figure~\ref{fig:w-sdba-generation}
presents two sequences 
and their resulting weighted ShapeDBA sequence. The higher weight given to the first 
sequence (the reference) ensures that the temporal motion aligns closely with it. 
Simultaneously, the weighted ShapeDBA sequence incorporates attributes from the second 
sequence, such as the patient's height and form, effectively blending characteristics 
from both sequences to create a more coherent and realistic synthetic sequence.

\paragraph*{Quantitative Analysis: Fidelity and Diversity}

When extending a dataset, even using non-deep learning methods, it is crucial to 
evaluate both the fidelity and diversity of the generated samples. Fidelity metrics 
determine how closely the generated samples match real ones, with higher fidelity 
indicating more reliable samples for real-world use. Diversity metrics, on the other 
hand, assess the variability among samples, ensuring that both real and generated 
samples are distinct from one another. The goal of a generative model is typically 
to create a generated space that is as diverse as the original data.

In this study, we used two common metrics to evaluate fidelity and diversity: the 
Fréchet Inception Distance (FID) and the Average Pair Distance (APD), define below:

\begin{itemize}
    \item \textit{Fréchet Inception Distance} (FID): The FID metric evaluates the similarity 
    between the distributions of real and generated samples. To compute FID, both real 
    and generated samples first undergo latent feature extraction using a pre-trained 
    deep learning model $\mathcal{F}$ (in this study, trained on a regression task). The mean and 
    covariance of these latent features are then calculated for both sets of samples. 
    FID measures the distance between these distributions by assuming they follow a 
    Gaussian distribution. The mathematical formula for FID is:
    \begin{equation}\label{equ:fid}
        FID(\mathcal{P}_1,\mathcal{P}_2)^2 = \textit{trace}(\Sigma_1+\Sigma_2-2(\Sigma_1.\Sigma_2)^{1/2}) + \sum_{i=1}^{f}(\mu_{1,i}-\mu_{2,i})^2
    \end{equation}
    \noindent where, $\mathcal{P}_1$ and $\mathcal{P}_2$ represent the distributions of 
    real and generated samples, respectively. $f$ is the dimension of the latent space of 
    $\mathcal{F}$. $\mu_1$ and $\mu_2$ are vectors of dimension $f$, representing
    the means of the real and 
    generated samples' feature spaces, respectively. $\Sigma_1$ and $\Sigma_2$ are the covariance 
    matrices of dimensions $(f,f)$ for the real and generated samples, respectively.

    \item \textit{Average Pair Distance} (APD): The APD metric evaluates the average Euclidean Distance 
    (ED) between randomly selected samples from both real and generated data within the 
    feature space, utilizing $\mathcal{F}$ as the latent feature extractor. To compute APD, two 
    randomly selected sets of samples, $\mathcal{S}_1$ and $\mathcal{S}_2$,
    each containing $S_{apd}$ samples, 
    are defined. The average distance between these sets is calculated as:
    \begin{equation}\label{equ:apd}
        APD(\mathcal{S}_1,\mathcal{S}_2) = \dfrac{1}{S_{apd}}\sum_{i=1}^{S_{apd}} \sqrt{\sum_{j=1}^{f} (\mathcal{S}_{i,j} - \mathcal{S}^{'}_{i,j})^2} ,
    \end{equation}
    This process is repeated across multiple random sets of $\mathcal{S}_1$ and $\mathcal{S}_2$ 
    where the final APD value is the average of these calculations, which helps 
    to eliminate bias from set selection.

In this study, we utilized open-source software to compute the FID and APD metrics. 
The results were averaged over different initializations of our pre-trained deep 
regression model, which was trained solely on the training set. For the APD metric, 
we used $S_{apd} = 20$ for the size of the randomly selected sets.
\end{itemize}

\begin{table}
    \centering
    \caption{The FID values of different augmentation methods and the real dataset, over different resamples of each exercise.
    The presented FID values include the average and standard deviation over all resamples per exercise and different initialization of the pre-trained feature extractor.}
    \label{tab:w-sdba-fid}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c}
                 & \textbf{Exercise 1}                 & \textbf{Exercise 2}                 & \textbf{Exercise 3}                 & \textbf{Exercise 4}                 & \textbf{Exercise 5}                 \\ \hline
    Real         & 02.18$E^{-6}$~$\pm$~06.66$E^{-7}$ & 02.48$E^{-6}$~$\pm$~09.82$E^{-7}$ & 02.19$E^{-6}$~$\pm$~08.18$E^{-7}$ & 05.00$E^{-6}$~$\pm$~02.09$E^{-6}$ & 03.06$E^{-6}$~$\pm$~01.30$E^{-6}$ \\ \hline
    Noisy        & \textbf{00.07~$\pm$~00.03}             & \textbf{00.08~$\pm$~00.03}             & \textbf{00.09~$\pm$~00.03}             & \textbf{00.24~$\pm$~00.10}             & \textbf{00.07~$\pm$~00.03}             \\
    ShapeDBA NN1 & {\ul 01.94~$\pm$~00.81}             & {\ul 04.12~$\pm$~01.86}             & {\ul 02.28~$\pm$~01.10}             & {\ul 04.19~$\pm$~02.95}             & {\ul 03.39~$\pm$~01.92}             \\
    ShapeDBA NN2 & 03.15~$\pm$~01.06             & 06.62~$\pm$~02.56             & 04.01~$\pm$~02.70             & 05.95~$\pm$~04.07             & 05.75~$\pm$~02.34             \\
    ShapeDBA NN3 & 03.77~$\pm$~01.21             & 07.98~$\pm$~02.59             & 05.16~$\pm$~03.39             & 07.16~$\pm$~04.51             & 07.34~$\pm$~02.91             \\
    ShapeDBA NN4 & 04.31~$\pm$~01.24             & 09.38~$\pm$~03.05             & 05.96~$\pm$~03.52             & 08.01~$\pm$~04.64             & 08.51~$\pm$~03.76             \\
    ShapeDBA NN5 & 04.62~$\pm$~01.28             & 10.21~$\pm$~03.34             & 06.45~$\pm$~03.69             & 08.90~$\pm$~05.07             & 09.23~$\pm$~04.12            
    \end{tabular}%
    }
\end{table}

\begin{table}
    \centering
    \caption{The APD values of different augmentation method and the real dataset, over different resamples of each exercise.
    The presented APD values include the average and standard deviation over all resamples per exercise, different randomly selected sets of size $S_{apd}=20$ and different initialization of the pre-trained feature extractor.}
    \label{tab:w-sdba-apd}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c}
     & \textbf{Exercise 1} & \textbf{Exercise 2} & \textbf{Exercise 3} & \textbf{Exercise 4} & \textbf{Exercise 5} \\ \hline
    Real & 06.09~$\pm$~00.63 & 06.61~$\pm$~00.83 & 05.95~$\pm$~00.78 & 08.24~$\pm$~01.24 & 07.10~$\pm$~01.10 \\ \hline
    Noisy & \textbf{06.05~$\pm$~00.57} & \textbf{06.55~$\pm$~00.79} & \textbf{05.90~$\pm$~00.78} & \textbf{08.14~$\pm$~01.18} & \textbf{07.00~$\pm$~01.03} \\
    ShapeDBA NN1 & {\ul 05.21~$\pm$~00.68} & {\ul 05.32~$\pm$~00.63} & {\ul 05.06~$\pm$~00.68} & {\ul 07.14~$\pm$~01.12} & {\ul 06.11~$\pm$~01.08} \\
    ShapeDBA NN2 & 04.93~$\pm$~00.69 & 04.96~$\pm$~00.66 & 04.77~$\pm$~00.65 & 06.90~$\pm$~01.16 & 05.70~$\pm$~00.98 \\
    ShapeDBA NN3 & 04.78~$\pm$~00.75 & 04.64~$\pm$~00.54 & 04.53~$\pm$~00.66 & 06.84~$\pm$~01.20 & 05.52~$\pm$~00.96 \\
    ShapeDBA NN4 & 04.67~$\pm$~00.74 & 04.47~$\pm$~00.55 & 04.34~$\pm$~00.63 & 06.59~$\pm$~01.16 & 05.35~$\pm$~00.91 \\
    ShapeDBA NN5 & 04.61~$\pm$~00.67 & 04.35~$\pm$~00.53 & 04.30~$\pm$~00.61 & 06.50~$\pm$~01.13 & 05.31~$\pm$~00.90
    \end{tabular}%
    }
\end{table}

For each augmentation method—noisy and weighted ShapeDBA, we present the average FID 
and APD values along with their standard deviations. These metrics are calculated over 
various resamples of the dataset for each exercise, as well as different initializations
of the pre-trained deep regression model.

Table~\ref{tab:w-sdba-fid} displays the FID values
for each exercise using the noisy augmentation method and 
five variations of our weighted ShapeDBA (with neighborhood sizes ranging from $1$ to $5$). 
The FID for real samples is also included as a baseline. Ideally, the FID of any generative 
method should be close to, but slightly higher than, the FID of the real data, as generated 
samples cannot exceed the fidelity of the actual data. As seen in Table~\ref{tab:w-sdba-fid},
the noisy method has a lower FID compared 
to the weighted ShapeDBA method. On one hand, this difference can be explained by the fact that the dataset exists 
in a sparse space. Adding a small amount of noise generates a point in the encoder's latent space that 
is very close to the real sample. On the other hand, generating data with ShapeDBA introduces new points 
into this sparse space. Given the small amount of data, the FID metric may appear ``better'' 
with noise augmentation.
Despite this, Figure~\ref{fig:w-sdba-vs-noisy}
clearly 
shows that the noisy augmentation produces unrealistic sequences. This highlights the 
importance of using multiple evaluation methods to draw comprehensive conclusions about 
generative models, as there is no single best approach.

The same trend is observed with the APD values in Table~\ref{tab:w-sdba-apd}. The noisy method has an 
APD closest to that of real samples, outperforming the weighted ShapeDBA method 
in quantitative evaluations.


\paragraph{Evaluation of Synthetic Sequences for Data Extension}

In our second experiment, we assess the effectiveness of the proposed method as 
a data extension technique for rehabilitation assessment. This task is framed 
as an extrinsic regression problem, where the aim is to predict a continuous 
performance score associated with each rehabilitation sequence.
We use the Fully Convolutional Network (Chapter~\ref{chapitre_1} Figure~\ref{fig:fcn})
as our backbone model (same used as feature extractor
for generation metrics in previous section).

To compare the predicted scores from our models with the clinical scores provided by 
experts, we used two metrics as outlined by~\cite{regression-metrics-paper}:
the Root Mean Square Error (RMSE) 
and the Mean Absolute Error (MAE). Given two sets of $N$ scores, $\textbf{y}$ (the ground truth) 
and $\hat{\textbf{y}}$ (the predictions), the MAE and RMSE are calculated as follows:
\begin{equation}\label{equ:rmse}
    MAE(\textbf{y},\hat{\textbf{y}}) = \dfrac{1}{N}~\sum_{i=1}^N~|y_i-\hat{y}_i|,
\end{equation}
\begin{equation}\label{equ:mae}
    RMSE(\textbf{y},\hat{\textbf{y}}) = \sqrt{\dfrac{1}{N}~\sum_{i=1}^N~(y_i-\hat{y}_i)^2}.
\end{equation}

\begin{table}
    \caption{MAE and RMSE errors obtained for all compared approaches on each exercise separately. Best values are emphasized in bold, while second best values are underlined.}
    \centering
    \label{tab:w-sdba-mae-rmse}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}
            {c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c@{\quad}|@{\quad}c}
            \multicolumn{1}{c|}{Training Set} & \multicolumn{1}{c|}{Exercise 1} & \multicolumn{1}{c|}{Exercise 2} & \multicolumn{1}{c|}{Exercise 3} & \multicolumn{1}{c|}{Exercise 4} & Exercise 5 \\ \hline
            \multicolumn{6}{c}{MAE} \\ \hline
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref.} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.206 $\pm$ 0.069} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.202 $\pm$ 0.037} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.204 $\pm$ 0.055} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.184 $\pm$ 0.068} & {\ul 0.224 $\pm$ 0.058} \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + Noise} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.186 $\pm$ 0.065} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.172 $\pm$ 0.040}} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.203 $\pm$ 0.045} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.185 $\pm$ 0.073} & 0.229 $\pm$ 0.069 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN1} & \multicolumn{1}{@{\quad}c@{\quad}|}{ {\ul 0.167 $\pm$ 0.070}} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.175 $\pm$ 0.030}} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.182 $\pm$ 0.051}} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.141 $\pm$ 0.062}} & \textbf{0.208 $\pm$ 0.079} \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN2} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.169 $\pm$ 0.057} & \multicolumn{1}{@{\quad}c@{\quad}|}{ 0.177 $\pm$ 0.041} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.194 $\pm$ 0.041}} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.168 $\pm$ 0.056}} & 0.226 $\pm$ 0.066 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN3} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.173 $\pm$ 0.063} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.183 $\pm$ 0.047} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.199 $\pm$ 0.058} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.168 $\pm$ 0.083} & 0.225 $\pm$ 0.055 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN4} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.168 $\pm$ 0.059} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.179 $\pm$ 0.043} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.199 $\pm$ 0.043} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.180 $\pm$ 0.080} & 0.231 $\pm$ 0.060 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN5} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.166 $\pm$ 0.067}} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.185 $\pm$ 0.043} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.201 $\pm$ 0.050} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.182 $\pm$ 0.089} & 0.226 $\pm$ 0.061 \\ \hline        
            
            \multicolumn{6}{@{\quad}c@{\quad}}{RMSE} \\ \hline
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref.} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.251 $\pm$ 0.083} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.247 $\pm$ 0.045} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.248 $\pm$ 0.065} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.230 $\pm$ 0.083} & {\ul 0.267 $\pm$ 0.073} \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + Noise} & \multicolumn{1}{@{\quad}c@{\quad}|}{ 0.203 $\pm$ 0.078} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.226 $\pm$ 0.043}} & \multicolumn{1}{@{\quad}c@{\quad}|}{ 0.238 $\pm$ 0.046} & \multicolumn{1}{@{\quad}c@{\quad}|}{ 0.227 $\pm$ 0.090} & 0.274 $\pm$ 0.092 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN1} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.199 $\pm$ 0.087}} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.226 $\pm$ 0.036}} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.214 $\pm$ 0.054}} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.178 $\pm$ 0.074}} & \textbf{0.251 $\pm$ 0.094} \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN2} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.203 $\pm$ 0.075} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.232 $\pm$ 0.052} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.226 $\pm$ 0.044}} & \multicolumn{1}{@{\quad}c@{\quad}|}{{\ul 0.210 $\pm$ 0.074}} & 0.268 $\pm$ 0.083 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN3} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.205 $\pm$ 0.082} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.235 $\pm$ 0.050} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.240 $\pm$ 0.062} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.214 $\pm$ 0.105} & 0.268 $\pm$ 0.066 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN4} & \multicolumn{1}{@{\quad}c@{\quad}|}{\textbf{0.198 $\pm$ 0.071}} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.235 $\pm$ 0.050} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.234 $\pm$ 0.048} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.230 $\pm$ 0.105} &  0.279 $\pm$ 0.070 \\
            
            \multicolumn{1}{@{\quad}c@{\quad}|}{Ref. + ShapeDBA NN5} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.202 $\pm$ 0.079} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.230 $\pm$ 0.049} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.244 $\pm$ 0.057} & \multicolumn{1}{@{\quad}c@{\quad}|}{0.231 $\pm$ 0.109} & \multicolumn{1}{@{\quad}c@{\quad}}{0.280 $\pm$ 0.080}         
            \end{tabular}
    }
\end{table}

We evaluated the performance of the FCN model for extrinsic regression by training 
it on various sets of rehabilitation sequences 
combined with the reference set. The average MAE and RMSE errors ($\pm$ standard deviation) 
are reported in Table~\ref{tab:w-sdba-mae-rmse}.

Our first observation is that adding noisy sequences to the training set 
($Ref. + Noise$) generally leads to better performance compared to using only 
the original training set ($Ref.$). This suggests that the FCN model tends to 
overfit the original data, and incorporating noisy sequences helps to improve 
its generalization capabilities.

Moreover, Table~\ref{tab:w-sdba-mae-rmse} highlights that the best
error values for both metrics are achieved 
when the FCN model is trained on the rehabilitation set extended with ShapeDBA-generated 
averages. This indicates that synthetic average sequences not only mitigate overfitting 
but also capture realistic rehabilitation motion patterns, allowing the FCN model to 
more effectively learn the variations in rehabilitation exercises.

Finally, we note that for three out of five exercises, the optimal performance is 
obtained when the training data includes average sequences computed using a single 
neighbor ($Ref. + ShapeDBA NN1$). This suggests that using a larger neighborhood may 
sometimes result in less coherent average sequences, as the neighbors might not 
reside within a continuous subspace.

The experiments demonstrated that incorporating synthetic sequences generated by the 
weighted ShapeDBA method significantly enhances the performance of rehabilitation 
assessment models. This approach mitigates overfitting and captures realistic motion 
patterns, leading to more accurate predictions. However, the reliance on real data 
distribution and the computational expense of finding nearest neighbors and applying 
ShapeDBA can be problematic in real-world scenarios where instant generation is required.
In the next section, we address this challenge by exploring the use of deep learning models 
for generation tasks, aiming to mitigate the limitations of dataset distribution and provide 
a more efficient solution.

\section{Exploring Deep Generative Models for Human Motion Generation}

In this section, we investigate the application of deep generative models 
for the task of human motion generation. The need for generating 
realistic and diverse motion sequences efficiently has spurred 
significant interest in this domain. Traditional prototyping 
methods, such as ShapeDBA, though effective, can be computationally 
intensive and time-consuming, particularly during the inference 
(generation) phase. This limitation makes them less suitable for 
real-time applications where quick data generation is crucial. 
Deep generative models, on the other hand, offer a substantial 
advantage in terms of speed and efficiency during the inference 
phase, making them highly suitable for these scenarios.

Generative models can revolutionize various fields, including the 
cinematic and gaming industries, by enabling the creation of 
lifelike and varied character
animations~\cite{gta-dataset-paper,generating-cut-scenes-paper}.
In medical rehabilitation~\cite{rehab-generation-paper}, 
these models can assist in creating realistic motion sequences for 
better patient assessment and treatment planning. The flexibility 
and scalability of deep generative models allow them to generate 
large volumes of high-quality motion sequences, essential for these 
applications. Additionally, fine-tuning these models~\cite{motiongpt-fine-tune-paper}
on small 
datasets helps avoid overfitting, ensuring the generated sequences 
are diverse and realistic without requiring extensive training data.

\begin{figure}
    \centering
    \caption{
        Summary of our proposed Supervised VAE (SVAE) architecture: The 
        \protect\mycolorbox{0,30,255,0.6}{input 
        skeleton sequences} are processed by the \protect\mycolorbox{102,220,230,0.6}{encoder} 
        to learn a \protect\mycolorbox{255,165,0,0.6}{Gaussian 
        distribution in the latent space}. From this distribution, a
        \protect\mycolorbox{255,30,0,0.6}{random sample} 
        is drawn and utilized in two ways: it is fed into a
        \protect\mycolorbox{255,100,255,0.6}{classifier} for the 
        action recognition task and also into the
        \protect\mycolorbox{48,255,0,0.6}{decoder} to reconstruct the 
        \protect\mycolorbox{0,30,255,0.6}{original sequence}. 
        This dual functionality enhances both the generative 
        and discriminative capabilities of the model.
    }
    \label{fig:svae-summary}
    \includegraphics[width=0.6\textwidth]{Figures/chapter_6/svae/svae-summary.pdf}
\end{figure}

Convolutional Neural Networks (CNNs) have proven highly effective in TSC,
as seen in chapters~\ref{chapitre_3} and~\ref{chapitre_4},
yet their application to human motion generation remains relatively 
unexplored. Despite this, CNNs offer distinct advantages over Recurrent Neural 
Networks (RNNs)~\cite{action2motion-paper} and Transformers~\cite{actor-paper},
particularly regarding inference time and computational 
efficiency (measured in FLOPS). To harness these benefits, we propose a CNN-based 
Variational Auto-Encoder (VAE) generative model tailored for human motion data, 
specifically for action recognition tasks as it is directly conditioned with labels.
The traditional approach to incorporating a conditioning aspect into a VAE model is 
by simply concatenating a one-hot encoded vector, representing the action label, with 
the latent space before feeding it to the decoder. However, this technique may have 
limitations in terms of sensitivity, as the model is not explicitly trained to recognize 
that the action label is significantly different from another.
To improve the conditioning and effectively separate the latent space, we introduce a 
classification task within the latent space by proposing a Supervised VAE (SVAE),
summarized in Figure~\ref{fig:svae-summary}.
Our model demonstrates competitiveness with state-of-the-art methods in terms of 
fidelity and diversity metrics. Additionally, we address the challenge of balancing 
training datasets concerning class label distribution, thereby enhancing downstream 
classification performance.

In the following sections, we detail some of the background work on human motion 
generation using deep learning models, the proposed architecture and its specific 
features, followed by the experimental results using the publicly available 
HumanAct12 dataset~\cite{action2motion-paper} (Figure~\ref{fig:example-humanact}).

\subsection{Background Work}

Recent advancements in deep generative models have brought significant 
improvements in the generation of human motion sequences. This 
subsection provides an overview of the key state-of-the-art models, 
highlighting their unique contributions and advantages.
The current state-of-the-art in this field lies within the 
capabilities of VAEs, GANs, and diffusion models.

\subsubsection{Generative Adversarial Networks (GANs)}

Generative Adversarial Networks (GANs)~\cite{gan-paper} employ a
dual-network structure consisting of 
a generator and a discriminator. The generator creates synthetic data, while the 
discriminator evaluates the authenticity of the generated data against real data. 
Through this adversarial process, GANs learn to produce highly realistic data that 
can often be indistinguishable from real samples. The ability to generate 
high-quality data quickly makes GANs especially appealing for applications 
requiring rapid generation of human motion sequences, such as in real-time 
game character animation or virtual reality environments.
Before the availability of 3D human motion datasets, researchers addressed the motion 
generation problem at a 2D level. One notable approach is the two-stage Generative 
Adversarial Network (GAN) introduced in~\cite{two-stage-gan-paper},
which generates 2D videos of human motion. 
This model works in two phases: first, it creates a human skeleton from random noise, 
and then it transforms this skeleton into an image, repeating this for multiple 
frames to produce a coherent video sequence. Another significant model is
MoCoGAN~\cite{mocogan-paper}, 
a GAN-based approach that generates motion by conditioning on specific content, learning 
two distinct spaces—one for content representation and the other for frame sequences. 
These innovative models, Two-Stage GAN and MoCoGAN, were later adapted for 3D human motion 
sequences, as demonstrated in~\cite{action2motion-paper},
paving the way for more advanced and realistic 3D human 
motion generation techniques.

\subsubsection{Variational Auto-Encoders (VAEs)}

Variational Auto-Encoders (VAEs)
(Chapter~\ref{chapitre_1} Section~\ref{sec:tscl})
learn to encode data into a Gaussian latent 
space and then decode it back into the data space, effectively 
capturing the underlying distribution of the training data. 
This allows VAEs to generate new data points that are coherent 
and representative of the original data distribution. 
The capability of VAEs to produce high-quality, diverse 
data points makes them particularly useful in applications 
such as the cinematic and gaming industries, where creating 
lifelike and varied character animations is essential.
With the availability of 3D human motion datasets, researchers in~\cite{action2motion-paper}
highlighted the 
limitations of existing datasets and introduced HumanAct12, a new dataset derived from 
the PHSPD dataset~\cite{PHSPD-dataset-paper,PHSPD-dataset-paper},
which utilizes a polarization camera and three Kinect v2 cameras~\cite{kinect-paper}.
Additionally, the authors in~\cite{action2motion-paper}
proposed Action2Motion, a VAE model, called Action2Motion,
designed to generate skeleton-based human motion sequences. This auto-regressive VAE 
consists of two encoders, a prior and a posterior encoder, and one decoder. The model approximates 
a latent representation of the prior and posterior time frames while minimizing the 
Kullback-Leibler (KL) divergence between their distributions to ensure regularization. 
Importantly, Action2Motion is a conditional VAE, with conditioning based on the action label
and the timestamp to differentiate between prior and posterior pose frames.
Other researchers have also explored conditional VAE modeling. For example~\cite{LCP-VAE-paper}
introduced a conditional VAE that learns a latent representation of the condition
instead of adding the label directly. This model comprises two VAEs, CS-VAE and 
LCP-VAE, one encoding the prior knowledge (condition) and the other encoding the 
future sequence (posterior knowledge). The model can be applied in two ways: using 
the action label as prior knowledge or using a part of the training sequence as 
prior knowledge. The latter approach has shown better performance, as using a real 
human motion sequence as prior knowledge increases the diversity of the generated samples.
Transformers have recently shown significant impact in translation
models~\cite{attention-all-you-need}
and image recognition~\cite{vision-transformer-paper}.
Building on this, the authors in~\cite{actor-paper} developed ACTOR, 
a Transformer-based VAE model to generate 3D human motion sequences.
Moreover,~\cite{posegpt-paper} introduced a Quantized VAE~\cite{vq-vae-paper}
to generate 3D human motion sequences. 
This model employs a Generative Pre-trained Transformer (GPT)-like model~\cite{gpt-1-paper}
in the 
latent space after quantization to predict latent indices. More recently, 
researchers in~\cite{um-cvae-paper} proposed a VAE that encodes the input human motion sequence 
in two streams simultaneously, resulting in action-agnostic and action-aware 
representations, known as UM-CVAE.

\subsubsection{Denoising Diffusion Probabilistic Models (DDPMs)}

Recently, the advancement of Denoising Diffusion Probabilistic Models
(DDPMs)~\cite{ddpm-paper}
has added another dimension to the 
field of generative models. DDPMs gradually transform simple 
initial data, such as Gaussian noise, into complex data distributions through 
a series of steps. This approach allows for high-quality data generation and 
has shown impressive results in generating detailed and realistic motion sequences. 
Diffusion models have advanced the field by providing a robust framework for 
generating complex data, further enhancing the ability of generative models to
create lifelike human motion sequences efficiently.
For instance,~\cite{motion-diffuse-paper} introduced MotionDiffuse, a
DDPM for human motion generation. This diffusion model employs an attention 
mechanism architecture for the denoising process, incorporating residual 
connections instead of the U-Net~\cite{u-net-paper}
architecture used in the original DDPM model~\cite{ddpm-paper}.

\subsection{Proposed Model}

\begin{figure}
    \centering
    \caption{
        The Variational Auto-Encoder (VAE) model for human motion generation 
        leverages an FCN backbone in both the Encoder and Decoder. In the Decoder, 
        \protect\mycolorbox{0,230,130,1.0}{standard one-dimensional convolutions} 
        are replaced with \protect\mycolorbox{255,199,20,1.0}{transposed convolutions}. 
        Each convolution layer is followed by batch normalization and a ReLU activation 
        function. The SVAE model further includes
        \protect\mycolorbox{0,158,255,0.57}{Fully Connected layers} 
        to enhance its functionality.
    }
    \label{fig:svae-detailed}
    \includegraphics[width=0.75\textwidth]{Figures/chapter_6/svae/VAE_SVAE.pdf}
\end{figure}

In this section, we introduce our proposed Supervised VAE (SVAE) architecture, 
which builds upon the original Variational Auto-Encoder (VAE) model by 
incorporating an associated classification task. Both models utilize a 
Fully Convolutional Network (FCN) architecture~\cite{fcn-resnet-mlp-paper} as the backbone. 
The encoder employs one-dimensional convolutions for down-sampling, 
while the decoder uses transposed one-dimensional convolutions for up-sampling.
Below, we present the original VAE and the Supervised VAE, both based on 
CNN architecture, as illustrated in Figure~\ref{fig:svae-detailed}.

\subsubsection{Original Variational Auto-Encoder (VAE)}

The VAE architecture, initially proposed in~\cite{vae-paper}, has been adapted for time series data, 
specifically targeting human motion sequences. The optimizer in this model minimizes 
two key losses: the reconstruction loss and the Kullback-Leibler (KL) divergence, as 
detailed in Chapter~\ref{chapitre_1} Section~\ref{sec:tscl}.
Notably, the reconstruction loss deviates from the standard Mean Squared Error 
loss by excluding the averaging over the time dimension, as shown below:
\begin{equation}\label{equ:svae-rec-loss}
    \mathcal{L}_{rec} (\textbf{x}, \hat{\textbf{x}}) = \dfrac{1}{J.D} \sum_{t=1}^{L}\sum_{m=1}^{J.D} (x_t^m - \hat{x}_t^m)^2,
\end{equation}
\noindent where $\textbf{x}$ and $\hat{\textbf{x}}$ are the MTS representation 
of the input skeleton sequence and the reconstructed one, both of length $L$ and $JxD$ dimensions,
and $J$ is the number of joints on the recorded skeleton each in a space of $D$ dimensions.
This modification prevents the model from converging to the average sequence during 
training, which would lead to underfitting due to reduced dimensionality in the 
optimization problem.

The KL divergence loss aligns the Gaussian distribution in the latent space to a standard
normal distribution with zero mean and unit variance, such as Eq.~\ref{equ:vae-kl} in
Chapter~\ref{chapitre_1}:

\begin{equation}\label{equ:vae-kl-human-motion}
\begin{split}
    \mathcal{L}_{KL} &= D_{KL}(q_{\theta}(\textbf{z}|\textbf{x}),\mathcal{N}(0,1))\\
    &= -\dfrac{1}{2}~\sum_{d=1}^\textbf{d}(1+\log\sigma_d^2-\mu_d^2-\sigma_d^2)
\end{split}
\end{equation}
\noindent where $\textbf{d}$ is the dimension of the latent space, specific to the architecture 
of the VAE, $\boldsymbol{\mu}$ and $\log~\boldsymbol{\sigma}^2$
are the mean and $\log$ variance vectors of the latent space features.

\subsubsection{Supervised Variational Auto-Encoder (SVAE)}

To enhance the VAE with conditioning capabilities, we introduce a classification 
task within the latent space. The SVAE model integrates an MLP based classifier~\cite{fcn-resnet-mlp-paper}
within the latent space. Both the classifier and the decoder utilize a randomly 
sampled vector from the Gaussian distribution produced by the encoder. This
additional classification task brings a Cross Entropy (CE) loss into the optimization 
process, alongside the reconstruction and KL divergence losses:
\begin{equation}\label{equ:cross-entropy-human-motion}
    \mathcal{L}_{CE}(\textbf{y},\hat{\textbf{y}}) = -\sum_{c=1}^{C}y_{c}.\log_{2}(\hat{y}_{c})
\end{equation}
\noindent where $\textbf{y}$ and $\hat{\textbf{y}}$ represent the true and predicted 
class distributions, respectively, and $C$ denotes the number of classes.

Inspired by the $\beta$-VAE~\cite{beta-vae-paper}, each loss in the SVAE model is weighted.
In $\beta$-VAE, the total loss is modulated by a hyperparameter $\beta$:
\begin{equation}\label{equ:beta-vae-total-loss}
    \mathcal{L}_{\beta-vae} = (1-\beta).\mathcal{L}_{mse} + \beta.\mathcal{L}_{KL}
\end{equation}
The $\beta$ parameter balances the emphasis between reconstruction quality and 
latent space disentanglement. Higher $\beta$ values prioritize learning 
disentangled latent features, while lower values focus on achieving 
better reconstruction quality. In our SVAE model, the total loss is calculated as:
\begin{equation}\label{equ:svae-loss-weights}
    \mathcal{L}_{total} = \lambda_{rec} \cdot \mathcal{L}_{rec} + \lambda_{KL} \cdot \mathcal{L}_{KL} + \lambda_{CE} \cdot \mathcal{L}_{CE}.
\end{equation}
For the original VAE architecture, the CE loss weight $\lambda_{CE}$
is set to zero. The optimal values for these weights were determined through
extensive experiments, detailed in the experimental section.

\subsection{Experimental Setup}

\subsubsection{Dataset}

To evaluate our proposed model, we employ the open-source HumanAct12 dataset 
as described in~\cite{action2motion-paper}. This dataset consists of $1,191$ human motion sequences, 
each depicting one of $12$ different actions. Each frame in these sequences 
represents a 3D skeleton composed of $24$ joints, for a total of $72$ dimensions.
To manage the variation in 
sequence lengths, we apply a resampling algorithm
that adjusts all sequences 
to a uniform target length of $75$, which corresponds to the average length of 
the input sequences. This resampling process is based on the Fourier Transform 
and is implemented using the SciPy Python module~\cite{scipy-paper}.
Prior to training, we normalize the skeleton sequences using $\min-\max$ normalization, 
which is applied separately to each dimension. Let $\mathcal{S}$ represent the dataset of skeleton 
sequences, organized into four dimensions: number of samples, sequence length, number 
of joints, and dimension of each joint. The normalization process is defined as follows:
\begin{equation}\label{equ:svae-normalization}
    \mathcal{S}[:,:,:,d] = \dfrac{\mathcal{S}[:,:,:,d] - \min(\mathcal{S}[:,:,:,d])}{\max(\mathcal{S}[:,:,:,d]) - \min(\mathcal{S}[:,:,:,d])},
\end{equation}
In scenarios involving a train/test split, the test set is normalized using the 
$\min$ and $\max$ values derived from each dimension of the training set. This ensures 
consistency in the data preprocessing steps and enhances the model's performance 
by standardizing the input data.

\subsubsection{Model Architecture}

For the Encoder and Decoder architecture in our model (Figure~\ref{fig:svae-detailed}), 
we have employed the following configurations:

\begin{itemize}
    \item Each convolutional layer in the network uses $128$ filters.
    \item The kernel sizes for the filters are $40$, $20$, and $10$ for the three layers
    in the Encoder, respectively, and these sizes are reversed in the Decoder.
    \item The dimension of the latent space is set to $16$.
\end{itemize}

The classifier architecture within the SVAE model comprises an FC layer 
with $8$ units, followed by a $50\%$ dropout, and then 
two fully connected layers with $C$ units each, where $C$ represents 
the number of classes. The generative models are trained for $2000$ epochs
with a batch size of $32$, using the 
Adam optimizer with a learning rate decay. We monitor the training 
loss to select the best model for evaluation.

\subsubsection{Evaluation Metrics}

For the evaluation metrics, we utilize the FID and APD fidelity and diversity 
metrics, detailed in the previous Section~\ref{sec:evaluation-wsdba}.
For feature extraction during evaluation, we adhere to the methodology in~\cite{action2motion-paper},
employing a GRU-based classifier.
% The code for this work is publicly available here:
% \url{https://github.com/MSD-IRIMAS/SVAE-4-HMG}.

\subsection{Experimental Results}

In this section, we present the experimental results on the HumanAct12 dataset. 
We start with a parameter search to determine the optimal weights for the different 
losses in SVAE while comparing its performance to the standard VAE model and the CVAE.
Next, we compare our SVAE model with state-of-the-art models. 
Finally, we conduct an experiment on label distribution balancing using data generation.

\subsubsection{Weight Losses Parameter Search}

\begin{table}
\centering
\caption{
The best loss function weights for the VAE, CVAE and SVAE models are as follows. For the VAE and CVAE, 
the classification loss weight ($W_{cls}$) is set to $0$ due to the absence of a 
classifier. The generated samples maintain the same label distribution as the 
training set but include more samples to better estimate the statistical metrics.
The best performing setup for each of the three variants is highlighted in \textbf{bold}.
}
\label{tab:svae-search}
\begin{tabular}{cccccc}
    \hline
    Model & W\_rec & W\_kl & W\_cls & FID & APD \\ 
    \hline \hline
    VAE   & 0.99900      & 1E-03     & 0.00000          & 01.38\textsuperscript{+-0.37}  & 06.42\textsuperscript{+-0.10} \\
    \textbf{VAE}   & \textbf{0.99990}     & \textbf{1E-04}    & \textbf{0.00000}          & \textbf{01.20\textsuperscript{+-0.10}}    & \textbf{06.46\textsuperscript{+-0.10}}  \\
    VAE   & 1.00000          & 1E+00         & 0.00000          & 45.03\textsuperscript{+-1.90}  & 00.07\textsuperscript{+-0.07} \\
    VAE   & 0.99999    & 1E-05     & 0.00000          & 01.23\textsuperscript{+-0.09}  & 06.45\textsuperscript{+-0.10}  \\
    VAE   & 0.99000       & 1E-02      & 0.00000          & 01.80\textsuperscript{+-0.38}   & 06.39\textsuperscript{+-0.10}  \\ \hline
    CVAE   & 0.99900      & 1E-03     & 0.00000          & 00.55\textsuperscript{+-0.13}  & 06.62\textsuperscript{+-0.11} \\
    CVAE   & 0.99990     & 1E-04    & 0.00000          & 01.35\textsuperscript{+-0.22}    & 06.39\textsuperscript{+-0.13}  \\
    CVAE   & 1.00000          & 1E+00         & 0.00000          & 05.83\textsuperscript{+-1.95}  & 06.15\textsuperscript{+-0.20} \\
    CVAE   & 0.99999    & 1E-05     & 0.00000          & 04.65\textsuperscript{+-1.85}  & 05.84\textsuperscript{+-0.23}  \\
    \textbf{CVAE}   & \textbf{0.99000}       & \textbf{1E-02}      & \textbf{0.00000}          & \textbf{00.38\textsuperscript{+-0.14}}   & \textbf{06.71\textsuperscript{+-0.11}}  \\ \hline
    SVAE  & 0.70000        & 1E-04    & 0.29000       & 00.59\textsuperscript{+-0.11}  & 06.64\textsuperscript{+-0.10}  \\
    \textbf{SVAE} & \textbf{0.49950} & \textbf{1E-03} & \textbf{0.49950} & \textbf{00.53\textsuperscript{+-0.15}} & \textbf{06.65\textsuperscript{+-0.11}} \\
    SVAE          & 0.49995         & 1E-04         & 0.49995         & 00.72\textsuperscript{+-0.23}          & 06.62\textsuperscript{+-0.11}          \\
    SVAE  & 1.00000          & 1E+00         & 1.00000          & 45.56\textsuperscript{+-1.85} & 00.05\textsuperscript{+-0.07} \\
    SVAE  & 0.29000       & 1E-04    & 0.70000        & 00.64\textsuperscript{+-0.09}  & 06.64\textsuperscript{+-0.10}  \\
    SVAE  & 0.30000        & 1E-02     & 0.70000        & 00.86\textsuperscript{+-0.30}   & 06.62\textsuperscript{+-0.12} \\
    SVAE  & 1.00000          & 1E-04    & 1.00000          & 00.79\textsuperscript{+-0.15}  & 06.59\textsuperscript{+-0.11} \\
    SVAE  & 0.70000        & 1E-04    & 0.30000        & 00.80\textsuperscript{+-0.22}   & 06.61\textsuperscript{+-0.10}  \\
    SVAE  & 0.29000       & 1E-02      & 0.70000        & 00.73\textsuperscript{+-0.27}  & 06.63\textsuperscript{+-0.13} \\
    SVAE  & 0.30000        & 1E-04    & 0.70000        & 00.68\textsuperscript{+-0.13}  & 06.63\textsuperscript{+-0.10}  \\ \hline
\end{tabular}
\end{table}

To determine the optimal set of hyperparameters for the loss function, we 
performed an ablation study using a grid search. During this process, we 
tracked the FID and APD metrics to evaluate performance for the VAE, CVAE and SVAE models.
The results 
of this ablation study are detailed in Table~\ref{tab:svae-search}, with the best-performing model 
highlighted in bold.

In this study, the SVAE model achieved optimal performance with loss function 
weights of $0.4995$ for the reconstruction loss, $0.001$ for the KL loss, and $0.4995$ for 
the CE loss. Reflecting the principles of the $\beta$-VAE model, this configuration minimizes 
the weight of the KL loss while equally weighting the reconstruction and classification 
losses. This setup ensures that the SVAE model strikes a balance between achieving 
an optimal latent space representation, maintaining high reconstruction quality, 
and effectively separating classes in the latent space.

The SVAE model outperforms the traditional VAE model, though it does not surpass the best setup 
of the CVAE model. However, the SVAE model is less sensitive to changes in the loss parameters. 
This stability is due to the explicit training setup, which ensures that the action labels 
effectively distinguish between samples in the latent space.
In order to highlight such ability, a 2D visualization of the 
training samples' latent space, created using Principal Component Analysis (PCA), 
is shown in Figure~\ref{fig:svae-pca} for the three models VAE, CVAE and SVAE.
This visualization highlights the SVAE model's ability to 
clearly separate multiple classes within the latent space, providing better control 
over generation conditioned on the label, which is not provided by both the VAE and the CVAE models.
A visualization of several generated samples is shown in Figure~\ref{fig:svae-generations},
demonstrating that the quality of the generated sequences appears visually satisfactory.
However, visual inspection alone is not sufficient to fully assess the model's
performance. Therefore, quantitative evaluations are necessary to objectively measure
the fidelity and diversity of the generated samples.

\begin{figure}
    \centering
    \caption{
        The 2D projection of the latent space (using PCA) for the VAE, CVAE and SVAE models 
        on the training samples shows distinct separation of latent points 
        corresponding to each action label in the case of SVAE compared to no separation in the cases of 
        VAE and CVAE.
    }
    \label{fig:svae-pca}
    \includegraphics[width=0.85\linewidth]{Figures/chapter_6/svae/pca_latent_space.pdf}
\end{figure}

\begin{figure}
    \centering
    \caption{
        Generated samples from the proposed SVAE models are conditioned on four 
        different actions: \protect\mycolorbox{0,30,255,0.6}{Warm Up}, 
        \protect\mycolorbox{255,30,0,0.6}{Drink}, 
        \protect\mycolorbox{255,165,0,0.6}{Lift Dumbbell}
        and \protect\mycolorbox{0,128,0,0.6}{Sit}. For each action, 
        we present two distinct generated examples to showcase the diversity and 
        robustness of the SVAE model.
    }
    \label{fig:svae-generations}
    \includegraphics[width=\textwidth]{Figures/chapter_6/svae/generation-examples.pdf}
\end{figure}

\subsubsection{Comparison With State-Of-The-Art}

\begin{table}
\centering
\caption{
        We compared our proposed model, the proposed SVAE,
with other approaches from the literature using the FID and Diversity metrics. 
For each method, we reported the average $\pm$ standard deviation of the FID and 
Diversity scores. For the real samples, a random split into two sets was used 
to calculate the FID metric. This comparison allows us to assess the effectiveness 
of our SVAE model in generating diverse and high-fidelity samples relative to existing methods.
}
\label{tab:svae-sota}
\begin{tabular}{cccccc}
    \hline
    Model         &  &  & FID $\downarrow$                             &  & Diversity $\uparrow$                  \\ %\cline{2-6} 
    \hline \hline
    Real          &  &  & 00.030\textsuperscript{+-0.005} &  & 06.860\textsuperscript{+-0.070}    \\ 
    \hline
    Two-stage GAN~\cite{action2motion-paper} &  &  & 10.480\textsuperscript{+-0.089}  &  & 05.960\textsuperscript{+-0.049}   \\
    Act-MoCoGAN~\cite{action2motion-paper}   &  &  & 05.610\textsuperscript{+-0.113}  &  & 06.752\textsuperscript{+-0.071}   \\
    Action2Motion~\cite{action2motion-paper} &  &  & 02.458\textsuperscript{+-0.079}  &  & 07.032\textsuperscript{+-0.038}   \\
    ACTOR~\cite{actor-paper}         &  &  & 00.120\textsuperscript{+-0.000}    &  & 06.840\textsuperscript{+-0.030}     \\
    PoseGPT~\cite{posegpt-paper}&  &  & 00.080\textsuperscript{+-*.***}                            &  & 06.850\textsuperscript{+-*.***}                             \\
    UM-CVAE~\cite{um-cvae-paper}       &  &  & 00.090\textsuperscript{+-0.000}     &  & 06.810\textsuperscript{+-0.020}       \\
    MotionDiffuse~\cite{motion-diffuse-paper} &  &  & 00.070\textsuperscript{+-0.000}    &  & 06.850\textsuperscript{+-0.020}     \\ 
    \hline
    SVAE (\textbf{ours})   &  &  & 00.560\textsuperscript{+-0.170}
      &  & 06.640\textsuperscript{+-0.100} \\ 
    \hline
\end{tabular}
\end{table}

To evaluate our proposed SVAE architecture against state-of-the-art models 
on the HumanAct12 dataset, we present the results in Table~\ref{tab:svae-sota}, focusing on 
our best SVAE model. To ensure fair metric evaluation, the generated samples 
maintain the same label distribution as the training set, for instance if the 
real data contains $10$ samples of class 1 and $5$ samples of class 2, 
then the generated data should follow the same label distribution.
This is followed by extracting the features of both real and generated samples using the GRU classifier,
i.e. extracting the output of the layer before the classification one.
The results reveal 
that our SVAE model surpasses the Action2Motion model in fidelity (FID) and 
performs closely to the leading state-of-the-art models. Moreover, the 
diversity of samples generated by the SVAE is similar to that of the real 
samples, indicating that the model effectively preserves diversity while 
maintaining high fidelity.

The results demonstrate that the SVAE model can generate high fidelity 
human motion sequences, as illustrated by the examples in Figure~\ref{fig:svae-generations}. 
This indicates that the convolution filters effectively extract temporal 
features from the input sequences. Consequently, this confirms that human 
motion skeleton sequences can be accurately represented as Multivariate Time Series (MTS).

\subsubsection{Fixing Labels Distribution With SVAE's Generations}

\begin{table}
\centering
\caption{Test accuracy values are reported for four different train/test splits: 
(first row) training on real samples; (second row) training on generated samples 
(following the same distribution as the real samples); and (third row) training 
on augmented samples (a combination of real and generated samples to achieve a
uniform label distribution).}
\label{tab:svae-accuracy}
\begin{tabular}{c|cccc}
    \hline
    Train On & Split 0 & Split 1 & Split 2 & Split 3 \\ 
    \hline \hline
    real & \textbf{87.77\textsuperscript{+-1.55}} & \textbf{76.47\textsuperscript{+-2.23}} & 87.78\textsuperscript{+-1.40} & 77.43\textsuperscript{+-2.54} \\
    generated & 72.47\textsuperscript{+-2.91} & 64.39\textsuperscript{+-2.42} & 77.37\textsuperscript{+-4.32} & 69.75\textsuperscript{+-7.31} \\
    augmented & 86.07\textsuperscript{+-2.10} & 73.51\textsuperscript{+-1.32} & \textbf{89.34\textsuperscript{+-1.77}} & \textbf{78.71\textsuperscript{+-2.53}} \\ 
    \hline
\end{tabular}
\end{table}

As previously discussed, utilizing data extension with deep generative models 
can present a paradoxical challenge. In this section, we aim to address the impact 
of the class balancing problem. By examining the effects of balancing class distributions 
in the training data, we seek to understand how this approach influences model 
performance and the quality of generated samples.
We implemented our method by balancing the label distribution in the training set,
after applying four different train/test splits with a cross subject setup.
Specifically, for each training set per split, we generated new human motion sequences using 
our SVAE model to augment each class, aiming for a uniform label distribution while 
keeping the most populated class unchanged. Additionally, we trained the model on 
generated samples that maintained the same number of samples and label distribution 
as the training set. The results, presented in Table~\ref{tab:svae-accuracy},
indicate that the performance 
is comparable to training on real samples, showing no significant differences. 
This demonstrates that the generated samples possess sufficient quality. Furthermore, 
using data generation to balance the label distribution enhances the model's performance.

\begin{figure}
    \centering
    \caption{
        The confusion matrix compares performance when training on real samples (left) 
        versus augmented samples with a fixed label distribution (right). 
        The matrix consists of $12$ rows and $12$ columns, corresponding to the 
        $12$ action labels. Each row represents the actual class of the samples 
        used for prediction, and each class is ranked in descending order based 
        on their population in the test set. The results show that when the label 
        distribution is fixed (right), the test set performance is significantly 
        affected in classes $4$, $7$, and $11$, which have higher ranks.
    }
    \label{fig:svae-confusion}
    \includegraphics[width=\textwidth]{Figures/chapter_6/svae/confusion_matrix_split3_v2.pdf}
\end{figure}

To further examine the fourth split and understand the impact on each action class, 
we present a confusion matrix in Figure~\ref{fig:svae-confusion}.
The row ticks include the rank of each 
action class, which represents the population of each class in the test set, listed 
in descending order. The confusion matrix reveals that our data generation for label 
distribution balancing 
technique effectively boosted the performance of some intermediate and low-rank 
action classes. This demonstrates that the quality of the generated samples is 
sufficient to enhance the performance of underrepresented classes.

\section{Conclusion}

In this chapter, we explored several advanced methodologies for improving 
human motion analysis represented as time series data.

Firstly, we discussed the usage of LITEMVTime, a powerful model designed for human 
rehabilitation assessments. LITEMVTime not only enhances performance but also offers 
explainability, making it highly suitable for clinical applications where understanding 
model decisions is crucial.

Next, we introduced ShapeDBA, a novel prototyping method. ShapeDBA was used in 
a weighted setup to perform data extension for human rehabilitation, effectively 
boosting regression models. This approach demonstrated significant improvements 
in handling sparse training data, enhancing the overall quality and robustness of 
the rehabilitation assessments.

Additionally, we proposed a deep generative model, the Supervised Variational 
Auto-Encoder (SVAE), for human motion generation. This CNN-based model 
competes well with state-of-the-art models. The SVAE model integrates 
classification tasks within the latent space, achieving a balance between 
reconstruction quality and class separation, ultimately generating high fidelity 
and diverse human motion sequences.

Throughout this chapter and all previous chapters, we have extensively discussed discriminative tasks 
such as classification, regression, and clustering, noting that the evaluation 
methods for these tasks are now well-established and widely adopted by the community. 
In contrast, the evaluation of generative models is more complex and diverse, 
with metrics like FID and APD being commonly used, but many new metrics being 
proposed each year. It is common in generative model studies, especially in 
applications like human motion generation, to use fewer than five datasets for 
evaluation. This limitation makes it impractical to use techniques such as the 
Critical Difference Diagram and the Multi-Comparison Matrix. 
Currently, each paper often proposes a new metric with different settings, leading 
to inconsistent and unfair comparisons. This inconsistency is due to the lack of a 
standardized framework, which the next chapter aims to address.
The next chapter 
addresses the need to unify the evaluation metrics for human motion generation.